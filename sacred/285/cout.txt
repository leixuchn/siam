INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "285"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-15 10:15:50.035488: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:15:50.035528: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:15:50.035534: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:15:50.035539: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:15:50.035543: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:15:50.378728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-15 10:15:50.378765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-15 10:15:50.378772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-15 10:15:50.378781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-15 10:15:54.764355: step 0, loss = 2.28, batch loss = 2.23 (2.5 examples/sec; 3.170 sec/batch; 292h:49m:32s remains)
2017-12-15 10:15:55.323718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4195271 -4.4194989 -4.4194756 -4.4194517 -4.4194431 -4.4194627 -4.4194937 -4.4195285 -4.4195619 -4.4195843 -4.4195933 -4.4195852 -4.4195752 -4.4195681 -4.4195657][-4.4195666 -4.4195418 -4.4195256 -4.4195113 -4.4195094 -4.4195256 -4.4195471 -4.4195681 -4.4195848 -4.4195948 -4.4195948 -4.4195862 -4.4195814 -4.4195795 -4.4195795][-4.41958 -4.4195633 -4.4195561 -4.41955 -4.4195509 -4.4195604 -4.4195724 -4.4195819 -4.4195862 -4.4195843 -4.4195766 -4.4195681 -4.4195666 -4.4195652 -4.419559][-4.419558 -4.41955 -4.419549 -4.4195423 -4.4195356 -4.4195342 -4.419539 -4.4195461 -4.419549 -4.4195457 -4.4195385 -4.4195337 -4.4195361 -4.4195294 -4.4195051][-4.4195008 -4.4195013 -4.4195027 -4.4194875 -4.4194622 -4.4194412 -4.4194388 -4.4194527 -4.4194694 -4.4194794 -4.4194832 -4.4194884 -4.4194946 -4.4194822 -4.4194374][-4.4194279 -4.4194374 -4.4194412 -4.4194169 -4.4193707 -4.4193258 -4.4193187 -4.41935 -4.4193892 -4.41942 -4.4194407 -4.4194613 -4.4194779 -4.4194703 -4.4194274][-4.4194007 -4.4194236 -4.4194341 -4.4194121 -4.4193635 -4.4193115 -4.4193039 -4.4193387 -4.419385 -4.4194241 -4.4194536 -4.4194822 -4.4195085 -4.4195151 -4.4194913][-4.4194264 -4.4194613 -4.4194794 -4.4194684 -4.4194345 -4.4193916 -4.4193778 -4.4193959 -4.4194293 -4.4194655 -4.4194946 -4.4195218 -4.4195495 -4.4195619 -4.4195514][-4.4194632 -4.4194961 -4.4195108 -4.4195032 -4.4194779 -4.4194427 -4.4194236 -4.4194293 -4.4194555 -4.4194875 -4.4195132 -4.4195342 -4.4195576 -4.4195704 -4.4195695][-4.4194837 -4.4195056 -4.4195108 -4.419498 -4.4194727 -4.4194412 -4.4194188 -4.4194202 -4.4194474 -4.4194803 -4.4195051 -4.4195247 -4.4195452 -4.419558 -4.4195657][-4.4194794 -4.4194932 -4.4194932 -4.4194794 -4.4194551 -4.4194255 -4.4194026 -4.4194021 -4.4194336 -4.4194679 -4.4194922 -4.4195123 -4.4195318 -4.4195457 -4.419559][-4.419466 -4.4194732 -4.4194736 -4.4194632 -4.4194412 -4.4194145 -4.4193912 -4.4193864 -4.4194164 -4.4194503 -4.4194794 -4.4195037 -4.4195232 -4.4195395 -4.4195542][-4.4194589 -4.4194641 -4.4194684 -4.419467 -4.4194551 -4.4194365 -4.4194179 -4.4194098 -4.4194312 -4.4194593 -4.4194875 -4.4195132 -4.4195328 -4.4195466 -4.4195561][-4.4194608 -4.4194694 -4.4194841 -4.4194989 -4.4195018 -4.4194937 -4.4194846 -4.4194803 -4.419497 -4.4195209 -4.4195423 -4.419559 -4.41957 -4.4195738 -4.41957][-4.419477 -4.4194918 -4.419518 -4.4195442 -4.4195542 -4.41955 -4.4195461 -4.4195452 -4.4195566 -4.4195752 -4.4195905 -4.4196 -4.4196053 -4.4196019 -4.4195881]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:16:02.264113: step 10, loss = 0.92, batch loss = 0.86 (13.5 examples/sec; 0.593 sec/batch; 54h:44m:39s remains)
INFO - root - 2017-12-15 10:16:08.245146: step 20, loss = 1.00, batch loss = 0.94 (13.6 examples/sec; 0.587 sec/batch; 54h:13m:11s remains)
INFO - root - 2017-12-15 10:16:14.169716: step 30, loss = 1.08, batch loss = 0.98 (13.5 examples/sec; 0.593 sec/batch; 54h:44m:39s remains)
INFO - root - 2017-12-15 10:16:20.125967: step 40, loss = 0.84, batch loss = 0.70 (13.2 examples/sec; 0.604 sec/batch; 55h:48m:36s remains)
INFO - root - 2017-12-15 10:16:26.116114: step 50, loss = 0.83, batch loss = 0.68 (13.4 examples/sec; 0.599 sec/batch; 55h:19m:21s remains)
INFO - root - 2017-12-15 10:16:32.040026: step 60, loss = 0.71, batch loss = 0.54 (13.3 examples/sec; 0.600 sec/batch; 55h:26m:13s remains)
INFO - root - 2017-12-15 10:16:38.014422: step 70, loss = 0.75, batch loss = 0.57 (13.3 examples/sec; 0.602 sec/batch; 55h:33m:50s remains)
INFO - root - 2017-12-15 10:16:44.042708: step 80, loss = 0.70, batch loss = 0.51 (13.1 examples/sec; 0.610 sec/batch; 56h:20m:14s remains)
INFO - root - 2017-12-15 10:16:50.033046: step 90, loss = 0.73, batch loss = 0.54 (13.6 examples/sec; 0.587 sec/batch; 54h:12m:32s remains)
INFO - root - 2017-12-15 10:16:56.024687: step 100, loss = 0.71, batch loss = 0.52 (13.5 examples/sec; 0.592 sec/batch; 54h:40m:32s remains)
2017-12-15 10:16:56.475801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9487524 -0.82065058 -0.73669791 -0.54885793 -0.39474583 -0.16344666 0.057890415 0.25926709 0.38448453 0.31159234 0.029329538 -0.44964194 -0.91590738 -1.3196082 -1.6109123][-0.43796992 -0.23182273 -0.073381186 0.1005969 0.20632863 0.47344136 0.71027493 0.91320539 1.0297716 0.93939948 0.64725804 0.1053431 -0.42160177 -0.8811121 -1.2142024][-0.14473319 0.05005908 0.20107245 0.42858434 0.65854335 0.94705415 1.1557124 1.3508322 1.4797361 1.3592603 1.0626447 0.48126149 -0.094306469 -0.61149478 -1.0203414][0.098959923 0.28665042 0.39073348 0.5811069 0.777653 1.0846937 1.3347719 1.5184338 1.6625288 1.5032513 1.145766 0.56468177 -0.0012729168 -0.50281191 -0.96025085][0.090295792 0.19166636 0.22365642 0.35349059 0.57682729 0.85418391 1.1139224 1.3234885 1.5047328 1.3524115 0.9138701 0.29314876 -0.32231331 -0.85741234 -1.3456056][-0.19556665 -0.16215467 -0.072187662 0.031590939 0.17331386 0.35883737 0.63955 0.87623668 1.0920942 0.95685315 0.53859496 -0.075425386 -0.68953156 -1.2488718 -1.7678666][-0.54810429 -0.53354907 -0.522233 -0.41931605 -0.2648766 -0.13917923 -0.039146423 0.1280458 0.29274726 0.1609962 -0.19971681 -0.72986245 -1.2851627 -1.8051782 -2.3015921][-0.78896403 -0.72808003 -0.67411613 -0.601985 -0.51389575 -0.44195032 -0.3958621 -0.32378197 -0.24801421 -0.40688324 -0.72703362 -1.136512 -1.5553455 -1.96267 -2.381834][-0.76685262 -0.66580033 -0.61832047 -0.59981227 -0.59999442 -0.64796114 -0.72463751 -0.77221894 -0.77469134 -0.90709591 -1.1554573 -1.4313533 -1.6859534 -1.960969 -2.2640533][-0.93437719 -0.80023026 -0.7383759 -0.73673725 -0.77262378 -0.88283372 -1.0389712 -1.2010217 -1.2912033 -1.4227366 -1.5981534 -1.7387831 -1.84042 -1.9563448 -2.1374693][-1.1302006 -0.9994812 -0.95469189 -0.98804903 -1.0551121 -1.204757 -1.4239266 -1.6814587 -1.8592039 -2.0040696 -2.1468608 -2.2230136 -2.2242608 -2.198977 -2.2109482][-1.3766043 -1.2793815 -1.2692809 -1.3206809 -1.3951647 -1.5491486 -1.7609148 -2.0226595 -2.2450159 -2.4032998 -2.5371542 -2.5521684 -2.4892659 -2.3829474 -2.3184924][-1.9236995 -1.8746892 -1.8801922 -1.9325709 -1.98473 -2.086868 -2.1988659 -2.3563557 -2.4977722 -2.5721188 -2.6247287 -2.5777826 -2.501786 -2.4381435 -2.4086964][-2.6215076 -2.5854425 -2.6020455 -2.622932 -2.6334953 -2.6801364 -2.7083831 -2.7347636 -2.7426972 -2.7325649 -2.7456751 -2.6968641 -2.6109424 -2.5218675 -2.4996696][-3.2297649 -3.143002 -3.0999517 -3.0694752 -3.0259907 -2.9895515 -2.9342191 -2.8957083 -2.8646345 -2.8341286 -2.7868056 -2.7130265 -2.6328058 -2.5668609 -2.5478911]]...]
INFO - root - 2017-12-15 10:17:02.498405: step 110, loss = 0.67, batch loss = 0.48 (13.1 examples/sec; 0.609 sec/batch; 56h:16m:09s remains)
INFO - root - 2017-12-15 10:17:08.533780: step 120, loss = 0.67, batch loss = 0.47 (13.1 examples/sec; 0.610 sec/batch; 56h:18m:13s remains)
2017-12-15 10:17:09.745449: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 57613 get requests, put_count=57600 evicted_count=1000 eviction_rate=0.0173611 and unsatisfied allocation rate=0.0193186
2017-12-15 10:17:09.745599: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-15 10:17:14.591793: step 130, loss = 0.64, batch loss = 0.44 (13.3 examples/sec; 0.600 sec/batch; 55h:24m:38s remains)
INFO - root - 2017-12-15 10:17:20.642721: step 140, loss = 0.65, batch loss = 0.46 (12.6 examples/sec; 0.635 sec/batch; 58h:37m:54s remains)
INFO - root - 2017-12-15 10:17:26.825107: step 150, loss = 0.65, batch loss = 0.45 (13.1 examples/sec; 0.609 sec/batch; 56h:14m:09s remains)
INFO - root - 2017-12-15 10:17:33.134081: step 160, loss = 0.63, batch loss = 0.43 (12.8 examples/sec; 0.624 sec/batch; 57h:36m:31s remains)
INFO - root - 2017-12-15 10:17:39.334403: step 170, loss = 0.70, batch loss = 0.51 (13.0 examples/sec; 0.614 sec/batch; 56h:41m:33s remains)
INFO - root - 2017-12-15 10:17:45.566624: step 180, loss = 0.66, batch loss = 0.46 (13.0 examples/sec; 0.614 sec/batch; 56h:38m:48s remains)
INFO - root - 2017-12-15 10:17:51.848263: step 190, loss = 0.61, batch loss = 0.42 (12.8 examples/sec; 0.626 sec/batch; 57h:45m:58s remains)
INFO - root - 2017-12-15 10:17:58.191288: step 200, loss = 0.68, batch loss = 0.49 (12.7 examples/sec; 0.632 sec/batch; 58h:20m:26s remains)
2017-12-15 10:17:58.711396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.96273518 -0.89174294 -0.97625422 -0.89287233 -0.822809 -0.8311677 -0.84229636 -0.89173865 -0.8982892 -0.91643023 -1.0637498 -1.0936732 -1.0725243 -1.3169641 -1.615942][-0.98373294 -0.87222743 -0.84960079 -0.84712029 -0.79825711 -0.72725463 -0.66462922 -0.64085507 -0.60189819 -0.69511008 -0.90073276 -0.94970632 -0.94025421 -1.0310092 -1.3359101][-1.1135969 -0.9734292 -0.887393 -0.81023955 -0.69705296 -0.64109325 -0.564347 -0.46052194 -0.34539676 -0.37086821 -0.52774096 -0.61438394 -0.61322975 -0.6893785 -1.001296][-0.94416189 -0.93673992 -0.90060091 -0.74039674 -0.61205554 -0.54378343 -0.4389019 -0.32107735 -0.21337295 -0.163306 -0.25460577 -0.31253529 -0.3113203 -0.48130774 -0.82170105][-0.87636662 -0.855839 -0.78333926 -0.69650984 -0.56711459 -0.3865428 -0.1981926 -0.053901672 0.063435793 0.076501846 -0.089643955 -0.1550889 -0.16754699 -0.38488483 -0.78078032][-1.0207477 -0.94967651 -0.84003353 -0.73701143 -0.59377193 -0.39762735 -0.14872289 0.098407269 0.30237389 0.28738213 0.077674866 -0.036664963 -0.056325436 -0.25759888 -0.73298311][-1.1003618 -1.0295765 -0.8959341 -0.76147485 -0.60776591 -0.39907932 -0.14981961 0.11861801 0.34840918 0.35178566 0.12603045 -0.017473459 -0.013734818 -0.23058057 -0.71077037][-1.0806079 -1.0560603 -0.97092366 -0.80220246 -0.5689559 -0.33927321 -0.1312561 0.1161809 0.34672832 0.33834743 0.11109114 -0.040731668 -0.11628437 -0.36504364 -0.71208596][-0.95960283 -0.88785434 -0.79501748 -0.63910532 -0.43243003 -0.14647269 0.14366007 0.36212111 0.48775578 0.48077679 0.28309917 0.047937393 -0.17453194 -0.49552798 -0.940573][-1.1750088 -1.0178781 -0.819515 -0.63017845 -0.44552422 -0.1967876 0.0572536 0.31472349 0.48556042 0.44756413 0.25817442 0.057315111 -0.16641927 -0.52946973 -1.1001539][-1.8491771 -1.6479285 -1.3963003 -1.0674896 -0.73379326 -0.49282598 -0.28381491 -0.042680502 0.12758875 0.15552521 0.066855192 -0.10213518 -0.28155398 -0.62594342 -1.2050266][-2.4643703 -2.2724795 -2.0400829 -1.6770947 -1.2984917 -0.92576575 -0.599596 -0.39544511 -0.2696135 -0.18993235 -0.23495102 -0.38779855 -0.51743388 -0.74149394 -1.1619995][-2.7504733 -2.6473188 -2.5003748 -2.2193856 -1.9209769 -1.4971106 -1.1016736 -0.78262234 -0.5571146 -0.45495081 -0.46522212 -0.55224752 -0.6845417 -0.90362191 -1.1985445][-2.8725958 -2.6961012 -2.4750206 -2.2769971 -2.0491776 -1.7276545 -1.4535267 -1.0821848 -0.78675628 -0.608258 -0.52248693 -0.50848794 -0.5991137 -0.87843156 -1.2182302][-3.1542232 -2.9400225 -2.6776657 -2.4072413 -2.1245065 -1.897862 -1.6939383 -1.4639266 -1.3051794 -1.0448449 -0.86493182 -0.80741096 -0.79866362 -0.83509469 -1.072612]]...]
INFO - root - 2017-12-15 10:18:04.920907: step 210, loss = 0.61, batch loss = 0.42 (12.9 examples/sec; 0.618 sec/batch; 57h:03m:31s remains)
INFO - root - 2017-12-15 10:18:11.162310: step 220, loss = 0.65, batch loss = 0.46 (13.1 examples/sec; 0.611 sec/batch; 56h:21m:42s remains)
INFO - root - 2017-12-15 10:18:17.363124: step 230, loss = 0.62, batch loss = 0.43 (12.9 examples/sec; 0.619 sec/batch; 57h:09m:33s remains)
INFO - root - 2017-12-15 10:18:23.611394: step 240, loss = 0.58, batch loss = 0.39 (12.6 examples/sec; 0.633 sec/batch; 58h:26m:36s remains)
INFO - root - 2017-12-15 10:18:29.924660: step 250, loss = 0.61, batch loss = 0.41 (12.7 examples/sec; 0.631 sec/batch; 58h:11m:33s remains)
INFO - root - 2017-12-15 10:18:36.256474: step 260, loss = 0.67, batch loss = 0.48 (12.7 examples/sec; 0.631 sec/batch; 58h:11m:56s remains)
INFO - root - 2017-12-15 10:18:42.665864: step 270, loss = 0.61, batch loss = 0.42 (12.5 examples/sec; 0.643 sec/batch; 59h:17m:56s remains)
INFO - root - 2017-12-15 10:18:49.036714: step 280, loss = 0.73, batch loss = 0.54 (12.8 examples/sec; 0.627 sec/batch; 57h:53m:50s remains)
INFO - root - 2017-12-15 10:18:55.483211: step 290, loss = 0.60, batch loss = 0.42 (12.2 examples/sec; 0.658 sec/batch; 60h:41m:44s remains)
INFO - root - 2017-12-15 10:19:01.936509: step 300, loss = 0.58, batch loss = 0.39 (12.6 examples/sec; 0.637 sec/batch; 58h:48m:49s remains)
2017-12-15 10:19:02.452869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.230196 -1.3976028 -1.1564634 -0.6543932 -0.10209727 0.39747691 0.9205277 1.3083799 1.4919126 0.80148053 -0.38043761 -1.9876219 -3.444881 -4.7591481 -5.7675152][-1.0695729 -1.2355049 -1.0371618 -0.52021646 0.097028494 0.66525388 1.1326153 1.4345124 1.5643408 0.87920594 -0.32366133 -1.9564369 -3.4354815 -4.7645149 -5.792623][-1.0118184 -1.1070807 -0.84333205 -0.31070375 0.31065917 0.9054749 1.3929207 1.6929729 1.7889659 1.0433347 -0.22941375 -1.8795946 -3.3729131 -4.7325773 -5.7810917][-1.0478785 -1.0678039 -0.74237013 -0.1584785 0.48950839 1.1117208 1.6453097 1.9675786 2.0633557 1.2701347 -0.066008091 -1.7635839 -3.2904396 -4.654129 -5.7097626][-1.1820445 -1.1147852 -0.7378962 -0.10452223 0.611326 1.3205097 1.9339273 2.305402 2.416291 1.6198728 0.22865129 -1.5793018 -3.2060852 -4.5821633 -5.6471858][-1.263941 -1.171891 -0.77257061 -0.12259126 0.64268756 1.427922 2.1447542 2.6215971 2.81167 2.0061691 0.55043435 -1.3029425 -3.0042496 -4.4855266 -5.6124177][-1.2310338 -1.169589 -0.79880548 -0.16099334 0.65848851 1.531966 2.3200696 2.8521922 3.0650108 2.2488868 0.7527492 -1.2000186 -2.9583452 -4.435092 -5.5384674][-1.1314001 -1.1112428 -0.83783484 -0.2440064 0.61564374 1.5719306 2.4404385 3.0470178 3.2630236 2.4132559 0.87893081 -1.1110482 -2.9162681 -4.4220572 -5.5185409][-1.0250034 -1.0526578 -0.85745668 -0.33776855 0.48444438 1.4379666 2.3410871 3.0256827 3.2947109 2.4796026 0.925864 -1.0779669 -2.8809526 -4.4278479 -5.571825][-1.4807668 -1.4779015 -1.3207712 -0.90955424 -0.19602489 0.69666076 1.559552 2.1900609 2.4366534 1.7128108 0.31854892 -1.5397589 -3.2482221 -4.7018023 -5.72831][-1.8621721 -1.8745461 -1.72296 -1.3398883 -0.7048285 0.028435469 0.74153924 1.3136227 1.5289357 0.88810468 -0.3571465 -1.972158 -3.4569473 -4.7640014 -5.7320771][-2.9558299 -2.9209487 -2.7290156 -2.3629003 -1.823832 -1.2006192 -0.64710546 -0.29222512 -0.20525026 -0.77037239 -1.8612301 -3.1850493 -4.3111677 -5.2656674 -5.9182167][-3.9675341 -3.8586926 -3.6428428 -3.3275242 -2.8905656 -2.4213638 -2.0040343 -1.7416794 -1.732008 -2.1697803 -2.9502649 -3.9388709 -4.8523588 -5.6027565 -6.029891][-4.6863046 -4.5059342 -4.2792063 -4.0806088 -3.8651226 -3.6038725 -3.3109446 -3.0697529 -3.0285964 -3.2767351 -3.7378371 -4.2788658 -4.8630009 -5.5055676 -5.9650154][-5.1964841 -5.0124846 -4.8194618 -4.67602 -4.5674458 -4.4270415 -4.2315817 -4.0721908 -4.0669093 -4.2367649 -4.5824232 -5.0125141 -5.4075785 -5.7642174 -6.038105]]...]
INFO - root - 2017-12-15 10:19:08.905896: step 310, loss = 0.64, batch loss = 0.46 (12.5 examples/sec; 0.637 sec/batch; 58h:49m:23s remains)
INFO - root - 2017-12-15 10:19:15.255490: step 320, loss = 0.56, batch loss = 0.37 (12.6 examples/sec; 0.636 sec/batch; 58h:42m:15s remains)
INFO - root - 2017-12-15 10:19:21.483484: step 330, loss = 0.76, batch loss = 0.58 (13.0 examples/sec; 0.613 sec/batch; 56h:34m:14s remains)
INFO - root - 2017-12-15 10:19:27.778868: step 340, loss = 0.67, batch loss = 0.48 (12.9 examples/sec; 0.622 sec/batch; 57h:20m:58s remains)
INFO - root - 2017-12-15 10:19:34.099926: step 350, loss = 0.58, batch loss = 0.39 (12.7 examples/sec; 0.630 sec/batch; 58h:05m:40s remains)
INFO - root - 2017-12-15 10:19:40.421557: step 360, loss = 0.65, batch loss = 0.46 (12.8 examples/sec; 0.624 sec/batch; 57h:33m:17s remains)
INFO - root - 2017-12-15 10:19:46.786043: step 370, loss = 0.65, batch loss = 0.46 (12.6 examples/sec; 0.632 sec/batch; 58h:21m:11s remains)
INFO - root - 2017-12-15 10:19:53.179369: step 380, loss = 0.61, batch loss = 0.42 (12.7 examples/sec; 0.629 sec/batch; 57h:59m:04s remains)
INFO - root - 2017-12-15 10:19:59.525838: step 390, loss = 0.60, batch loss = 0.42 (12.6 examples/sec; 0.635 sec/batch; 58h:33m:16s remains)
INFO - root - 2017-12-15 10:20:05.859824: step 400, loss = 0.54, batch loss = 0.35 (12.7 examples/sec; 0.628 sec/batch; 57h:55m:00s remains)
2017-12-15 10:20:06.447790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6344757 -1.7825787 -1.669251 -1.3426039 -0.87394881 -0.365062 0.20432711 0.68300867 0.9767189 0.55911636 -0.38878202 -1.7336226 -3.1226778 -4.3087931 -5.0488505][-2.0829523 -2.1942692 -2.0156875 -1.6717479 -1.2046754 -0.67197561 -0.079061031 0.41778803 0.71067333 0.33481836 -0.59149623 -1.8888834 -3.2594206 -4.4450994 -5.2005234][-2.0088794 -2.0476012 -1.8502862 -1.5441933 -1.1294384 -0.59415436 -0.023295403 0.42296886 0.66995621 0.31941795 -0.56399012 -1.8555081 -3.2505386 -4.4997931 -5.3237691][-1.5449502 -1.5105665 -1.3217053 -1.0339785 -0.61990547 -0.11976075 0.33161736 0.64161634 0.80779696 0.41343737 -0.51992893 -1.8266811 -3.2146637 -4.4452691 -5.2905364][-0.99720359 -0.960438 -0.78345776 -0.48161817 -0.067771435 0.35261822 0.69038868 0.90183544 0.99228144 0.56627083 -0.37862229 -1.7238567 -3.1702104 -4.4364576 -5.3093715][-0.5600183 -0.48463726 -0.26931596 0.058867931 0.46398163 0.81690025 1.076839 1.2172976 1.2701674 0.76988506 -0.26241016 -1.6536324 -3.1058259 -4.4042296 -5.3396673][-0.21420074 -0.12037539 0.10477924 0.429379 0.82860661 1.1959515 1.5038304 1.6736641 1.6793675 1.088037 -0.060081482 -1.5491102 -3.0822084 -4.4257121 -5.3745651][-0.091857672 0.0087213516 0.25036621 0.59285641 1.0385022 1.4841743 1.8397355 2.0582891 2.0742435 1.4182782 0.17906094 -1.4351783 -3.0449822 -4.4756417 -5.4612112][-0.11798239 -0.022832632 0.20897436 0.63736677 1.1392159 1.6612406 2.0785532 2.2712655 2.2090888 1.5039959 0.23786926 -1.3753276 -3.004288 -4.4474206 -5.4653339][-0.65003467 -0.57714581 -0.35389042 0.054047585 0.52673054 1.0038457 1.3849602 1.555244 1.4444585 0.80982161 -0.31248927 -1.7570348 -3.2474508 -4.5741396 -5.493125][-1.6831632 -1.667402 -1.5212193 -1.1652744 -0.7908721 -0.39406776 -0.12160945 -0.014989138 -0.12779808 -0.66261768 -1.5480137 -2.6997595 -3.919343 -5.0247221 -5.7385058][-3.0890236 -3.1236191 -3.016674 -2.7562528 -2.4560027 -2.1857738 -1.9873044 -1.9610507 -2.0727673 -2.4546275 -3.063864 -3.8582368 -4.7220831 -5.5027266 -5.9462762][-4.12597 -4.2125654 -4.11631 -3.9919186 -3.7937872 -3.6538737 -3.5795159 -3.5878971 -3.6771965 -3.8877931 -4.2028041 -4.658638 -5.1901135 -5.6673164 -5.9322591][-5.1913223 -5.2608271 -5.2136588 -5.1704969 -5.0727596 -5.0029879 -4.9695282 -4.9830384 -5.0407763 -5.2016973 -5.4275522 -5.6600227 -5.845376 -5.9716682 -6.0152416][-5.9393349 -5.9492116 -5.8787117 -5.8788133 -5.8262062 -5.81252 -5.8205442 -5.8436661 -5.8870583 -5.9387846 -5.9743872 -5.9998803 -6.0098186 -5.995018 -5.9789381]]...]
INFO - root - 2017-12-15 10:20:12.869229: step 410, loss = 0.57, batch loss = 0.38 (12.2 examples/sec; 0.658 sec/batch; 60h:42m:53s remains)
INFO - root - 2017-12-15 10:20:19.319340: step 420, loss = 0.56, batch loss = 0.37 (12.2 examples/sec; 0.655 sec/batch; 60h:26m:17s remains)
INFO - root - 2017-12-15 10:20:25.677984: step 430, loss = 0.63, batch loss = 0.44 (12.8 examples/sec; 0.624 sec/batch; 57h:31m:17s remains)
INFO - root - 2017-12-15 10:20:32.043370: step 440, loss = 0.58, batch loss = 0.39 (13.0 examples/sec; 0.617 sec/batch; 56h:56m:17s remains)
INFO - root - 2017-12-15 10:20:38.408975: step 450, loss = 0.61, batch loss = 0.43 (12.0 examples/sec; 0.668 sec/batch; 61h:36m:54s remains)
INFO - root - 2017-12-15 10:20:44.720778: step 460, loss = 0.53, batch loss = 0.34 (12.6 examples/sec; 0.634 sec/batch; 58h:29m:48s remains)
INFO - root - 2017-12-15 10:20:51.122166: step 470, loss = 0.75, batch loss = 0.56 (12.3 examples/sec; 0.650 sec/batch; 59h:57m:08s remains)
INFO - root - 2017-12-15 10:20:57.473888: step 480, loss = 0.61, batch loss = 0.43 (12.4 examples/sec; 0.645 sec/batch; 59h:28m:47s remains)
INFO - root - 2017-12-15 10:21:04.015943: step 490, loss = 0.61, batch loss = 0.43 (12.2 examples/sec; 0.654 sec/batch; 60h:21m:20s remains)
INFO - root - 2017-12-15 10:21:10.356520: step 500, loss = 0.56, batch loss = 0.38 (12.6 examples/sec; 0.633 sec/batch; 58h:24m:22s remains)
2017-12-15 10:21:10.920545: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028817654 -0.027956963 -0.10640812 -0.1201303 -0.0655849 0.0022153854 0.094585419 0.18379307 0.23005295 -0.14021707 -0.966851 -2.0081563 -3.0349019 -3.9629815 -4.5871043][-0.036432743 -0.096096277 -0.13466597 -0.16254592 -0.14571738 -0.060404539 0.060758114 0.17333317 0.21244144 -0.16150379 -1.0049016 -2.0488536 -3.0685408 -3.9838622 -4.5970783][-0.31441355 -0.34727383 -0.39192271 -0.43449306 -0.44619465 -0.41062379 -0.32885242 -0.22458315 -0.11700463 -0.37278485 -1.110395 -2.128696 -3.1520715 -4.0759444 -4.7293324][-0.55541062 -0.59376073 -0.62491512 -0.63621187 -0.64270544 -0.62423635 -0.56113029 -0.44808483 -0.33583093 -0.536021 -1.2013028 -2.1644533 -3.1506972 -4.104845 -4.7914443][-0.40699983 -0.42163134 -0.46312642 -0.50637341 -0.54502845 -0.57081819 -0.55836415 -0.49911046 -0.40757608 -0.63731456 -1.3319082 -2.258848 -3.2065475 -4.1279087 -4.8088903][-0.17043614 -0.19504786 -0.21812844 -0.22650528 -0.26388764 -0.29060483 -0.28260326 -0.23133826 -0.14267063 -0.39024329 -1.1002235 -2.1003966 -3.1271052 -4.0934072 -4.8072529][0.20201397 0.1650486 0.14170504 0.10403252 0.069525719 0.049292088 0.068003178 0.12386847 0.20353937 -0.093264818 -0.85946035 -1.9019361 -2.9828343 -4.0301867 -4.8113775][0.61190367 0.5746603 0.57442045 0.55639744 0.52842522 0.5053401 0.51081276 0.55217171 0.6246686 0.26199055 -0.57816029 -1.7208147 -2.9107356 -4.010376 -4.8111358][1.0233097 1.0423708 1.0829349 1.0917883 1.0905261 1.0803418 1.0725446 1.0838175 1.1106081 0.66937685 -0.25807118 -1.4735198 -2.7178044 -3.8934908 -4.746604][0.47740364 0.56211948 0.62438965 0.676126 0.69059896 0.68478394 0.67715931 0.67998219 0.7005434 0.28413486 -0.58123541 -1.7302344 -2.9180803 -4.0039573 -4.7421026][-0.77903271 -0.72462726 -0.65672016 -0.59002328 -0.56787205 -0.57159877 -0.57800388 -0.56904244 -0.55331492 -0.90536165 -1.6364551 -2.5452614 -3.4330161 -4.19987 -4.7429795][-1.8600121 -1.784343 -1.7004473 -1.6468134 -1.6309543 -1.6415737 -1.6498289 -1.6395729 -1.6169596 -1.8381598 -2.3181875 -2.9689214 -3.6661649 -4.3120885 -4.77594][-3.0570593 -3.0066872 -2.9251266 -2.8676863 -2.8543367 -2.8514457 -2.8422985 -2.8301077 -2.8122828 -2.972415 -3.332746 -3.7807312 -4.2060452 -4.6001711 -4.8963203][-4.1263905 -4.0602446 -3.9829841 -3.9278672 -3.9039495 -3.9016924 -3.8940527 -3.8696051 -3.8417737 -3.9229679 -4.11602 -4.3385077 -4.5561185 -4.788928 -4.9561191][-5.6482248 -5.58467 -5.5134139 -5.4582205 -5.4315405 -5.4102426 -5.3821535 -5.3527942 -5.3223076 -5.2825203 -5.2481694 -5.2178082 -5.1985235 -5.190403 -5.1630626]]...]
INFO - root - 2017-12-15 10:21:17.270017: step 510, loss = 0.60, batch loss = 0.41 (12.9 examples/sec; 0.619 sec/batch; 57h:05m:20s remains)
INFO - root - 2017-12-15 10:21:23.552438: step 520, loss = 0.62, batch loss = 0.44 (12.9 examples/sec; 0.620 sec/batch; 57h:13m:03s remains)
INFO - root - 2017-12-15 10:21:29.971684: step 530, loss = 0.68, batch loss = 0.49 (12.5 examples/sec; 0.643 sec/batch; 59h:14m:53s remains)
INFO - root - 2017-12-15 10:21:36.505199: step 540, loss = 0.63, batch loss = 0.45 (12.5 examples/sec; 0.642 sec/batch; 59h:14m:32s remains)
INFO - root - 2017-12-15 10:21:42.865744: step 550, loss = 0.69, batch loss = 0.50 (12.4 examples/sec; 0.643 sec/batch; 59h:16m:22s remains)
INFO - root - 2017-12-15 10:21:49.190580: step 560, loss = 0.72, batch loss = 0.53 (12.7 examples/sec; 0.628 sec/batch; 57h:51m:42s remains)
INFO - root - 2017-12-15 10:21:55.518516: step 570, loss = 0.58, batch loss = 0.39 (12.3 examples/sec; 0.649 sec/batch; 59h:47m:41s remains)
INFO - root - 2017-12-15 10:22:01.907500: step 580, loss = 0.59, batch loss = 0.40 (12.8 examples/sec; 0.625 sec/batch; 57h:37m:17s remains)
INFO - root - 2017-12-15 10:22:08.282247: step 590, loss = 0.54, batch loss = 0.35 (12.4 examples/sec; 0.647 sec/batch; 59h:40m:58s remains)
INFO - root - 2017-12-15 10:22:14.703432: step 600, loss = 0.55, batch loss = 0.36 (12.7 examples/sec; 0.630 sec/batch; 58h:04m:16s remains)
2017-12-15 10:22:15.214164: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.90129471 0.89559793 0.94241285 1.0576034 1.1494722 1.2389646 1.3330464 1.4434924 1.4878159 1.0015759 0.16138029 -1.0564399 -2.279887 -3.8197205 -5.1256175][0.51935053 0.48284149 0.51971149 0.62017059 0.72750187 0.80207586 0.90498257 0.9640708 0.96724987 0.4506278 -0.44658971 -1.6103184 -2.8123832 -4.3006177 -5.5380864][0.24424267 0.20266104 0.21129799 0.29187775 0.28905821 0.37643433 0.49069309 0.58590317 0.65076637 0.2206111 -0.6073575 -1.7866127 -3.0453444 -4.5435362 -5.8377647][0.22143078 0.17082071 0.24375868 0.27798319 0.30561876 0.37359524 0.47898197 0.56187916 0.60664606 0.17182875 -0.66294718 -1.815953 -3.0421615 -4.5518737 -5.8662767][0.31478786 0.25002193 0.25221539 0.24133968 0.23168516 0.30370474 0.41229057 0.48707008 0.52345181 0.090767384 -0.74129581 -1.898144 -3.1397839 -4.6258698 -5.9152293][0.30033493 0.28394413 0.31499338 0.27892542 0.29395866 0.36488724 0.47044325 0.54100943 0.56787062 0.14315987 -0.68578792 -1.8389614 -3.0715466 -4.5517073 -5.8449159][0.31283855 0.26969337 0.2574048 0.26651573 0.25502825 0.3517518 0.44604349 0.50481319 0.53194618 0.12989902 -0.68360639 -1.8219581 -3.0394144 -4.4947457 -5.7789574][0.23533583 0.19260931 0.22227716 0.20248795 0.18222332 0.26200962 0.33817959 0.39692116 0.43645859 0.062294483 -0.72423673 -1.8218701 -3.0100698 -4.4394846 -5.7021513][0.15524006 0.10061979 0.091373444 0.062935352 0.022400379 0.096278667 0.16418934 0.22354555 0.26943636 -0.074212074 -0.8186202 -1.869488 -3.0142543 -4.3948493 -5.6064072][-0.017259598 -0.14041138 -0.19758487 -0.2939446 -0.292449 -0.24821663 -0.19659591 -0.14199352 -0.089314461 -0.38935828 -1.0695081 -2.0554857 -3.155741 -4.4778724 -5.5636182][-1.198123 -1.3663027 -1.4886591 -1.5561302 -1.5735321 -1.5566332 -1.5263441 -1.4570282 -1.3793998 -1.6221082 -2.2049844 -3.0324054 -3.9287431 -4.9959397 -5.8287487][-2.6841245 -2.8739688 -2.93683 -3.0264692 -3.0364127 -3.0210848 -2.9753566 -2.9052491 -2.8211377 -2.9730136 -3.3896585 -3.9715116 -4.6025486 -5.3938046 -6.0026426][-4.2021184 -4.332932 -4.40616 -4.4183979 -4.4137192 -4.4037232 -4.3786306 -4.322073 -4.2559385 -4.3253307 -4.5806108 -4.9706812 -5.4078979 -5.921185 -6.2711916][-5.0018015 -5.0982742 -5.1017118 -5.1082911 -5.122653 -5.1193795 -5.0916038 -5.0477605 -4.991981 -5.026875 -5.2050867 -5.474895 -5.7893977 -6.1136532 -6.334116][-6.2702589 -6.2615776 -6.2844315 -6.2817955 -6.2733841 -6.2655306 -6.2384253 -6.1982942 -6.1579928 -6.1307893 -6.1367068 -6.194387 -6.2725849 -6.3413253 -6.4038091]]...]
INFO - root - 2017-12-15 10:22:21.543104: step 610, loss = 0.59, batch loss = 0.41 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:50s remains)
INFO - root - 2017-12-15 10:22:27.888616: step 620, loss = 0.67, batch loss = 0.48 (12.7 examples/sec; 0.629 sec/batch; 57h:56m:41s remains)
INFO - root - 2017-12-15 10:22:34.171285: step 630, loss = 0.69, batch loss = 0.50 (12.7 examples/sec; 0.631 sec/batch; 58h:10m:31s remains)
INFO - root - 2017-12-15 10:22:40.575159: step 640, loss = 0.66, batch loss = 0.48 (12.2 examples/sec; 0.654 sec/batch; 60h:15m:45s remains)
INFO - root - 2017-12-15 10:22:46.948419: step 650, loss = 0.56, batch loss = 0.37 (12.6 examples/sec; 0.633 sec/batch; 58h:19m:28s remains)
INFO - root - 2017-12-15 10:22:53.402644: step 660, loss = 0.64, batch loss = 0.46 (12.4 examples/sec; 0.646 sec/batch; 59h:32m:01s remains)
INFO - root - 2017-12-15 10:22:59.837291: step 670, loss = 0.58, batch loss = 0.40 (12.2 examples/sec; 0.655 sec/batch; 60h:21m:41s remains)
INFO - root - 2017-12-15 10:23:06.265408: step 680, loss = 0.60, batch loss = 0.42 (12.2 examples/sec; 0.657 sec/batch; 60h:35m:09s remains)
INFO - root - 2017-12-15 10:23:12.795627: step 690, loss = 0.56, batch loss = 0.38 (12.1 examples/sec; 0.659 sec/batch; 60h:42m:52s remains)
INFO - root - 2017-12-15 10:23:19.183124: step 700, loss = 0.55, batch loss = 0.36 (12.6 examples/sec; 0.637 sec/batch; 58h:44m:01s remains)
2017-12-15 10:23:19.745509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3316126 -3.3881671 -3.3727877 -3.2727461 -3.1487489 -2.883388 -2.3943841 -1.8238416 -1.1796379 -1.4293954 -2.4247975 -3.5378437 -4.6018391 -5.3348379 -5.7305679][-3.372206 -3.4258146 -3.4534924 -3.3978794 -3.3015218 -3.0316167 -2.5259418 -1.9084146 -1.2169027 -1.4076862 -2.3842454 -3.5201938 -4.6410584 -5.4901204 -5.7898984][-2.4179783 -2.5466044 -2.669796 -2.6611993 -2.5909653 -2.3690531 -1.8567111 -1.199311 -0.51922512 -0.75847125 -1.8145561 -3.1564987 -4.4755812 -5.4256711 -5.8207607][-1.5648048 -1.5952642 -1.6433027 -1.6703007 -1.6289742 -1.4067574 -0.90206051 -0.23910832 0.43023396 0.15373898 -0.97916317 -2.4462829 -3.8615437 -4.9831486 -5.545917][-0.41294456 -0.35136032 -0.33280873 -0.25357509 -0.14389133 0.080950737 0.5102787 1.0514598 1.5795689 1.2305622 -0.018270016 -1.6720548 -3.21235 -4.3916154 -5.0037231][0.68038893 0.83182764 1.0358086 1.2401118 1.4779181 1.8303638 2.28905 2.7784009 3.2140474 2.7243042 1.3266082 -0.50751305 -2.2179379 -3.629353 -4.5103741][1.3578849 1.6347771 1.8754845 2.2377586 2.6491666 3.1334925 3.6487565 4.0503969 4.3486395 3.7574983 2.2821259 0.33596134 -1.4859338 -3.0223632 -4.0677218][1.5814795 1.9538007 2.3649907 2.7890458 3.2451649 3.6986637 4.1119118 4.462328 4.7231112 4.0039434 2.3893719 0.36742735 -1.4513855 -2.9458368 -3.9474156][1.3856616 1.7440443 2.1011772 2.4925556 2.930645 3.3740945 3.7400265 3.9270697 4.0286188 3.3398805 1.8166537 -0.1761353 -1.9929469 -3.4517045 -4.3406844][-0.073327541 0.12538242 0.38595915 0.68350887 0.997324 1.3095269 1.5580835 1.6738014 1.7381577 1.0284657 -0.34397435 -1.9618766 -3.4403067 -4.5378046 -5.0151949][-1.7560401 -1.6165898 -1.4985361 -1.2891431 -1.0814071 -0.98202562 -0.88447213 -0.82225204 -0.80696487 -1.35657 -2.3518617 -3.4937797 -4.4576707 -5.1688938 -5.5249314][-3.0780251 -3.0416956 -2.9327588 -2.8343315 -2.740355 -2.6921804 -2.6950235 -2.7798746 -2.8311462 -3.2777672 -4.07287 -4.9347534 -5.569191 -5.9475603 -6.0213881][-4.7715468 -4.7558951 -4.660151 -4.5714507 -4.4885406 -4.5124006 -4.54929 -4.640696 -4.7335739 -4.997139 -5.4654655 -5.9900732 -6.3987575 -6.5235176 -6.4363027][-5.40365 -5.3765736 -5.2642517 -5.2188759 -5.1861587 -5.1851492 -5.2013569 -5.2642913 -5.3165851 -5.4999323 -5.7992206 -6.0742884 -6.2988443 -6.3723769 -6.3586631][-6.8292923 -6.722712 -6.59118 -6.535491 -6.5199866 -6.5382643 -6.56485 -6.600152 -6.629611 -6.6507559 -6.6595745 -6.6090631 -6.5579524 -6.4091892 -6.1812725]]...]
INFO - root - 2017-12-15 10:23:26.098887: step 710, loss = 0.76, batch loss = 0.58 (12.9 examples/sec; 0.622 sec/batch; 57h:21m:39s remains)
INFO - root - 2017-12-15 10:23:32.537333: step 720, loss = 0.54, batch loss = 0.36 (12.4 examples/sec; 0.647 sec/batch; 59h:39m:49s remains)
INFO - root - 2017-12-15 10:23:38.986939: step 730, loss = 0.47, batch loss = 0.29 (12.6 examples/sec; 0.636 sec/batch; 58h:38m:53s remains)
INFO - root - 2017-12-15 10:23:45.347568: step 740, loss = 0.55, batch loss = 0.37 (13.0 examples/sec; 0.613 sec/batch; 56h:29m:41s remains)
INFO - root - 2017-12-15 10:23:51.788044: step 750, loss = 0.55, batch loss = 0.37 (12.2 examples/sec; 0.654 sec/batch; 60h:15m:13s remains)
INFO - root - 2017-12-15 10:23:58.295681: step 760, loss = 0.62, batch loss = 0.44 (12.6 examples/sec; 0.637 sec/batch; 58h:43m:23s remains)
INFO - root - 2017-12-15 10:24:04.748746: step 770, loss = 0.71, batch loss = 0.53 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:29s remains)
INFO - root - 2017-12-15 10:24:11.173763: step 780, loss = 0.66, batch loss = 0.48 (12.5 examples/sec; 0.638 sec/batch; 58h:49m:45s remains)
INFO - root - 2017-12-15 10:24:17.570058: step 790, loss = 0.55, batch loss = 0.37 (12.6 examples/sec; 0.637 sec/batch; 58h:40m:11s remains)
INFO - root - 2017-12-15 10:24:24.007421: step 800, loss = 0.67, batch loss = 0.49 (12.6 examples/sec; 0.635 sec/batch; 58h:31m:23s remains)
2017-12-15 10:24:24.554380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2415497 -2.32428 -2.2660768 -2.1558788 -1.9712021 -1.8763189 -1.7364528 -1.6427984 -1.5393293 -1.6716344 -2.1370745 -2.7424452 -3.3074107 -3.7667663 -4.2242041][-2.3405142 -2.4903245 -2.4934597 -2.2801771 -2.0077815 -1.7999303 -1.5536382 -1.343389 -1.1870241 -1.3951011 -1.9268718 -2.6170459 -3.2600362 -3.748419 -4.20766][-2.1901841 -2.2540512 -2.1182106 -1.8904924 -1.5697801 -1.2977505 -0.98059177 -0.6797998 -0.48846936 -0.69825339 -1.2735612 -2.0891364 -2.8554759 -3.4457455 -3.9406166][-1.94965 -1.9436746 -1.7268655 -1.3207016 -0.87059927 -0.52554893 -0.20423031 0.00076198578 0.1183362 -0.14412642 -0.75738 -1.5996075 -2.363946 -2.9835413 -3.5362232][-1.2503862 -1.2350168 -0.98431158 -0.60941553 -0.17583227 0.22913361 0.62508345 0.94720554 1.1091137 0.75564194 0.061897755 -0.81463885 -1.6432807 -2.3514652 -2.9581857][-0.40896845 -0.34349227 -0.079781055 0.33069324 0.78122854 1.105155 1.4314842 1.733758 1.8934546 1.5777063 0.87897396 -0.046670914 -0.87101316 -1.5931077 -2.2902665][0.33321333 0.48721361 0.72622919 1.0542765 1.4517832 1.8084273 2.1260543 2.2497182 2.2172427 1.8221841 1.0927758 0.17689943 -0.61034918 -1.3266442 -1.9697618][0.66716051 0.89739513 1.1814022 1.466568 1.7800913 2.0197792 2.2181363 2.3415356 2.2455497 1.6903496 0.83223104 -0.10343218 -0.86744595 -1.5593834 -2.1610012][0.77443838 0.96459723 1.2119799 1.5037751 1.7762675 1.9156256 1.9627237 1.896646 1.6688972 1.1445913 0.29184532 -0.68142772 -1.4954963 -2.0917382 -2.6238456][-0.16927052 0.10817051 0.36135721 0.62299967 0.84420156 0.93096495 0.92476606 0.75957394 0.48800325 -0.030882359 -0.78355336 -1.6536107 -2.3735247 -2.9224386 -3.3514278][-1.2637472 -1.0740812 -0.87192941 -0.66389513 -0.4607079 -0.40237546 -0.4756732 -0.62501121 -0.86082244 -1.3439264 -2.0393164 -2.8475161 -3.4217794 -3.8071964 -4.0521269][-2.9809074 -2.8576789 -2.7132492 -2.5881915 -2.4464042 -2.4553821 -2.5162635 -2.6627054 -2.8885946 -3.1675453 -3.5150056 -3.9750109 -4.3703346 -4.5649915 -4.5939856][-4.1852546 -4.08808 -3.9896238 -3.9310915 -3.8404288 -3.8724327 -3.9460421 -4.0191636 -4.0951309 -4.235672 -4.4341545 -4.6629372 -4.8037481 -4.8622456 -4.8412752][-5.0937486 -4.9833846 -4.88708 -4.7860365 -4.6650991 -4.7105274 -4.7168865 -4.7211089 -4.74173 -4.7714396 -4.8528461 -4.97028 -5.0247793 -4.9737716 -4.8992972][-6.0246391 -5.9624615 -5.9485784 -5.8657713 -5.7565994 -5.7707934 -5.7506375 -5.7223749 -5.65333 -5.5482578 -5.4254279 -5.3351521 -5.2472124 -5.1034908 -4.9359746]]...]
INFO - root - 2017-12-15 10:24:30.962404: step 810, loss = 0.57, batch loss = 0.39 (12.8 examples/sec; 0.627 sec/batch; 57h:47m:12s remains)
INFO - root - 2017-12-15 10:24:37.356907: step 820, loss = 0.59, batch loss = 0.41 (12.6 examples/sec; 0.635 sec/batch; 58h:29m:07s remains)
INFO - root - 2017-12-15 10:24:43.725563: step 830, loss = 0.58, batch loss = 0.40 (12.8 examples/sec; 0.623 sec/batch; 57h:22m:52s remains)
INFO - root - 2017-12-15 10:24:50.126050: step 840, loss = 0.53, batch loss = 0.35 (12.5 examples/sec; 0.640 sec/batch; 58h:55m:59s remains)
INFO - root - 2017-12-15 10:24:56.524262: step 850, loss = 0.53, batch loss = 0.35 (12.4 examples/sec; 0.647 sec/batch; 59h:35m:12s remains)
INFO - root - 2017-12-15 10:25:02.872070: step 860, loss = 0.53, batch loss = 0.35 (12.6 examples/sec; 0.635 sec/batch; 58h:30m:51s remains)
INFO - root - 2017-12-15 10:25:09.266003: step 870, loss = 0.61, batch loss = 0.43 (12.1 examples/sec; 0.663 sec/batch; 61h:02m:54s remains)
INFO - root - 2017-12-15 10:25:15.687418: step 880, loss = 0.53, batch loss = 0.35 (12.5 examples/sec; 0.640 sec/batch; 58h:55m:13s remains)
INFO - root - 2017-12-15 10:25:22.118193: step 890, loss = 0.64, batch loss = 0.47 (11.9 examples/sec; 0.673 sec/batch; 61h:57m:42s remains)
INFO - root - 2017-12-15 10:25:28.524821: step 900, loss = 0.49, batch loss = 0.31 (12.5 examples/sec; 0.640 sec/batch; 58h:56m:07s remains)
2017-12-15 10:25:29.009127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086664677 -0.029381275 -0.067780972 0.014446735 0.00959444 -0.0044827461 0.018570423 -0.013873577 -0.034630775 -0.27961588 -0.74342489 -1.2484641 -1.6082633 -1.8253 -1.9868677][-0.24339581 -0.04133606 0.16427898 0.014819622 -0.086390972 0.16299963 0.359437 0.46105242 0.55588961 0.31314135 -0.25676584 -0.67406893 -1.0084713 -1.2445598 -1.4557192][0.095710754 0.053221226 -0.14270258 -0.28461409 -0.089231968 0.11202097 0.41114283 0.74196434 0.95351791 0.81245184 0.30024004 -0.41478491 -1.0055025 -1.3514721 -1.5654845][-0.20545578 0.054721355 0.18681192 0.14538908 0.27276897 0.51247931 0.93361473 1.1297131 1.3225527 1.0453129 0.38976955 -0.23633289 -0.80910254 -1.2094572 -1.5978858][-0.27678776 -0.052022934 0.10493231 0.22064781 0.23882294 0.59561968 1.0834312 1.5410824 1.8434191 1.4730654 0.79257822 -0.13722277 -0.83600211 -1.2466011 -1.6422486][-0.3100605 -0.1021409 0.057580471 0.1766572 0.46176338 0.7313633 0.96645832 1.3194675 1.4792986 1.2424364 0.56438446 -0.4166224 -1.078511 -1.6170058 -2.0368652][-0.27380514 0.079589367 0.34207582 0.35540485 0.52466869 0.54404831 0.64256048 0.79040956 0.86182833 0.64191961 -0.2103796 -1.0285938 -1.6767538 -2.30262 -2.8007045][-0.50129843 -0.10146713 0.13482904 0.44512653 0.72707748 0.87905931 0.98623991 0.9683938 0.99973774 0.34758329 -0.69413471 -1.5788505 -2.3761766 -2.9750881 -3.398093][-0.19116735 0.15716982 0.39353657 0.56822014 0.78212547 0.85389519 0.81915283 0.88182783 0.88333559 0.29329348 -0.59320521 -1.7535899 -2.6608653 -3.2849517 -3.7915156][-0.60302997 -0.34889746 -0.19086361 -0.13568068 -0.1102953 -0.12794304 -0.073098183 -0.0034809113 -0.074635029 -0.589463 -1.5373228 -2.5326722 -3.1367049 -3.7290843 -4.1306152][-1.5452437 -1.3036041 -1.2113369 -1.2989242 -1.3430345 -1.4227071 -1.4491251 -1.4087789 -1.3954613 -1.7982047 -2.5438805 -3.2935174 -3.8224437 -4.1707225 -4.3991108][-2.5181651 -2.42627 -2.3327346 -2.3389459 -2.43813 -2.5292823 -2.5207956 -2.517401 -2.5966008 -3.0028095 -3.6158271 -4.1980252 -4.6201773 -4.7905841 -4.737875][-3.2107077 -3.1448922 -3.0864494 -3.1140959 -3.0865641 -3.1294432 -3.0411007 -2.9607425 -3.0214672 -3.2918787 -3.6793785 -4.258287 -4.752738 -5.0156207 -5.0491643][-3.808398 -3.8412304 -3.7384191 -3.7547591 -3.7268953 -3.702796 -3.6430421 -3.6296442 -3.639451 -3.9045079 -4.2739358 -4.685432 -5.0776644 -5.2993364 -5.3539815][-4.6781774 -4.74619 -4.7107754 -4.7382956 -4.7263293 -4.7665896 -4.7436085 -4.7806339 -4.8888879 -5.0907397 -5.2689443 -5.374042 -5.4188986 -5.5170712 -5.5566282]]...]
INFO - root - 2017-12-15 10:25:35.409273: step 910, loss = 0.58, batch loss = 0.40 (12.7 examples/sec; 0.632 sec/batch; 58h:11m:00s remains)
INFO - root - 2017-12-15 10:25:41.853946: step 920, loss = 0.55, batch loss = 0.37 (12.5 examples/sec; 0.641 sec/batch; 59h:03m:47s remains)
INFO - root - 2017-12-15 10:25:48.217499: step 930, loss = 0.52, batch loss = 0.34 (12.0 examples/sec; 0.668 sec/batch; 61h:30m:39s remains)
INFO - root - 2017-12-15 10:25:54.659345: step 940, loss = 0.53, batch loss = 0.36 (12.1 examples/sec; 0.661 sec/batch; 60h:54m:21s remains)
INFO - root - 2017-12-15 10:26:01.032087: step 950, loss = 0.56, batch loss = 0.38 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:06s remains)
INFO - root - 2017-12-15 10:26:07.395462: step 960, loss = 0.53, batch loss = 0.35 (12.0 examples/sec; 0.666 sec/batch; 61h:18m:03s remains)
INFO - root - 2017-12-15 10:26:13.740797: step 970, loss = 0.62, batch loss = 0.44 (13.2 examples/sec; 0.605 sec/batch; 55h:43m:08s remains)
INFO - root - 2017-12-15 10:26:20.114634: step 980, loss = 0.56, batch loss = 0.38 (12.5 examples/sec; 0.639 sec/batch; 58h:52m:59s remains)
INFO - root - 2017-12-15 10:26:26.510787: step 990, loss = 0.60, batch loss = 0.43 (12.8 examples/sec; 0.624 sec/batch; 57h:25m:41s remains)
INFO - root - 2017-12-15 10:26:32.959731: step 1000, loss = 0.62, batch loss = 0.44 (12.5 examples/sec; 0.642 sec/batch; 59h:04m:54s remains)
2017-12-15 10:26:33.475939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8754706 -1.8099716 -1.773895 -1.6776533 -1.5647068 -1.2859745 -0.92605662 -0.51447845 0.074170589 0.44637489 0.49276352 0.28374863 -0.16616488 -1.1889803 -2.1309459][-0.88304496 -0.47330666 -0.090346336 0.10128069 0.26002026 0.35223579 0.36953402 0.6873455 1.354208 1.6782913 1.7152882 1.4206147 0.84524393 -0.0846529 -1.0996652][0.26276827 0.66690254 1.0783515 1.5463238 1.917171 2.1221285 2.0989947 2.2542896 2.6911678 2.8439236 2.6256824 2.0360827 1.4386668 0.39041758 -1.062572][0.48712206 1.1321893 1.651495 2.1569519 2.552814 2.9798808 3.0481353 3.1173363 3.4461532 3.307416 2.7826505 1.9356542 1.1983724 0.18319798 -0.9526825][-0.281662 0.1619215 0.68158627 1.25699 1.471693 1.7724566 2.0364046 2.3675632 2.7271156 2.2205467 1.5058427 0.89064026 0.356246 -0.49921441 -1.2815394][-0.28138542 0.23501348 0.59543753 0.84967422 1.1853371 1.5389967 1.705606 1.8044581 1.9755564 1.6289315 0.95026445 0.029547691 -0.58342433 -1.0488765 -1.3694181][-0.48219228 0.17808962 0.61444235 1.0268502 1.4636259 1.9518366 2.3625789 2.5169234 2.4435968 1.7940359 0.846714 -0.12520075 -0.83782363 -1.4177651 -1.7427778][0.11952734 0.63280582 1.185236 1.6975508 2.2118745 2.5943131 3.0018978 3.3811316 3.4015703 2.7196364 1.6367955 0.49564409 -0.41154432 -0.77042675 -1.0053949][0.45092821 1.103128 1.6547928 2.0996737 2.7748632 3.3866563 3.7961993 4.0109305 4.0131993 3.6564317 2.934567 1.7124119 0.6743331 0.22540808 0.055738926][0.2084918 0.77442455 1.1101346 1.563076 2.0737009 2.2848468 2.5902019 2.8907909 2.9149961 2.7739329 2.5053849 1.7444048 0.89350367 0.12634706 -0.24952316][-0.4013648 0.07742548 0.68034554 1.3543715 1.7767577 1.9353123 2.1124239 2.1366315 1.9640279 1.9235907 1.771162 1.3245454 0.7978878 0.16499996 -0.40352011][-0.87668395 -0.37949276 0.2013917 0.78190613 1.3773718 1.7710681 2.0937214 2.2414875 2.2024288 1.9159431 1.5219288 0.97189903 0.53327227 0.41747761 -0.02756834][-1.3827841 -1.0055902 -0.46747351 0.06490469 0.8622098 1.4508977 2.1105747 2.4919624 2.5246258 2.0938873 1.6020341 0.95639038 0.42310238 0.0695281 -0.29149866][-2.1162672 -1.7970576 -1.3326087 -0.95231819 -0.40765262 -0.0056462288 0.54721737 1.0837674 1.3077459 1.1419425 0.90748787 0.43772221 -0.15205336 -0.79421711 -1.1371443][-3.793926 -3.3911462 -2.9996631 -2.6953049 -2.2746382 -1.9370644 -1.3398669 -0.91345024 -0.77350879 -0.83032632 -0.8196187 -0.83291674 -1.0809309 -1.4680443 -1.9268763]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:26:39.916672: step 1010, loss = 0.58, batch loss = 0.40 (12.3 examples/sec; 0.652 sec/batch; 60h:04m:21s remains)
INFO - root - 2017-12-15 10:26:46.369719: step 1020, loss = 0.65, batch loss = 0.47 (12.6 examples/sec; 0.633 sec/batch; 58h:19m:37s remains)
INFO - root - 2017-12-15 10:26:52.892359: step 1030, loss = 0.56, batch loss = 0.38 (12.8 examples/sec; 0.624 sec/batch; 57h:30m:02s remains)
INFO - root - 2017-12-15 10:26:59.317078: step 1040, loss = 0.65, batch loss = 0.47 (12.7 examples/sec; 0.629 sec/batch; 57h:56m:05s remains)
INFO - root - 2017-12-15 10:27:05.784035: step 1050, loss = 0.60, batch loss = 0.42 (12.5 examples/sec; 0.639 sec/batch; 58h:49m:13s remains)
INFO - root - 2017-12-15 10:27:12.150116: step 1060, loss = 0.61, batch loss = 0.44 (12.7 examples/sec; 0.631 sec/batch; 58h:08m:23s remains)
INFO - root - 2017-12-15 10:27:18.496300: step 1070, loss = 0.63, batch loss = 0.45 (12.5 examples/sec; 0.639 sec/batch; 58h:51m:30s remains)
INFO - root - 2017-12-15 10:27:24.906705: step 1080, loss = 0.53, batch loss = 0.36 (12.8 examples/sec; 0.627 sec/batch; 57h:40m:56s remains)
INFO - root - 2017-12-15 10:27:31.393121: step 1090, loss = 0.58, batch loss = 0.40 (12.8 examples/sec; 0.626 sec/batch; 57h:36m:36s remains)
INFO - root - 2017-12-15 10:27:37.838257: step 1100, loss = 0.64, batch loss = 0.46 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:56s remains)
2017-12-15 10:27:38.419382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7976182 -2.7014806 -2.6691928 -2.6441669 -2.3362229 -1.7870481 -1.1113703 -0.53890753 -0.19926167 -0.47886395 -1.4064794 -2.4312465 -3.3212297 -4.081213 -4.6586466][-2.9981835 -3.0143604 -3.0372481 -2.8964639 -2.4462731 -1.8187137 -1.0792551 -0.53110933 -0.12945986 -0.33695173 -1.3367107 -2.6998444 -3.7191849 -4.4975724 -5.1553135][-2.0879991 -2.0649531 -2.1089959 -1.9868383 -1.5281606 -0.83024979 -0.077262878 0.41827965 0.70597792 0.31504631 -0.747556 -2.3688729 -3.8330059 -4.9839272 -5.7269826][-1.3621325 -1.2041535 -1.0847208 -0.93844008 -0.52773046 0.0186162 0.55614567 0.98093462 1.2046547 0.81911373 -0.28826141 -1.9082818 -3.3380628 -4.7018037 -5.7819796][-0.65056872 -0.26175642 0.023517609 0.33388329 0.848845 1.5939598 2.2719936 2.7155991 2.8337464 2.0318918 0.45812178 -1.2862406 -2.7769294 -4.2099533 -5.3258586][-0.069966793 0.48060942 1.0409389 1.6230397 2.3199062 3.0001884 3.5067267 3.8650641 3.9791312 3.2747073 1.7042489 -0.4975183 -2.3550172 -3.7275813 -4.7801967][0.08671999 0.7188592 1.368094 2.1430554 3.0411234 3.89115 4.6249528 4.9818568 5.0483823 4.248611 2.5309119 0.33031559 -1.7327919 -3.4622879 -4.6711297][0.22170782 0.90332174 1.5531712 2.4017305 3.3436604 4.220984 5.0418468 5.5405717 5.6872983 4.8375716 3.1788325 1.0082049 -1.1560285 -2.9506493 -4.3730583][0.18539 0.86132288 1.5488329 2.1662512 2.6658254 3.2365422 3.7887959 4.1352792 4.216989 3.575665 2.3120327 0.38505602 -1.4840512 -3.0241539 -4.2995396][-1.4052644 -0.91776586 -0.37447691 0.071200848 0.431211 0.82018471 1.2627106 1.442421 1.4142838 0.87914991 -0.10525846 -1.4402688 -2.67414 -3.6522069 -4.4791927][-2.5688574 -2.2165794 -1.8946419 -1.3974481 -1.0796554 -0.98675704 -0.77172351 -0.68157029 -0.6540308 -0.97944117 -1.6176155 -2.689198 -3.7542124 -4.5882688 -5.2275214][-4.0592766 -3.8798709 -3.6031189 -3.4379888 -3.2773857 -3.0477872 -2.8154736 -2.6528053 -2.5296841 -2.6555653 -3.018378 -3.6617956 -4.2883229 -4.9455924 -5.4188228][-4.8844614 -4.7882957 -4.6182837 -4.439568 -4.345367 -4.1844091 -3.9836044 -3.8749332 -3.7535467 -3.7225945 -3.91857 -4.3086033 -4.7651076 -5.0713458 -5.3367248][-5.5916076 -5.459229 -5.2676759 -5.0957 -5.0535355 -4.9806495 -4.8845859 -4.712955 -4.5059938 -4.4767256 -4.3997097 -4.566689 -4.9129381 -5.1040277 -5.2408867][-6.4031191 -6.3585176 -6.286427 -6.1672225 -6.0037074 -5.9419208 -5.883903 -5.7370877 -5.6522503 -5.5098882 -5.3245029 -5.376533 -5.3694353 -5.304718 -5.1570687]]...]
INFO - root - 2017-12-15 10:27:44.784360: step 1110, loss = 0.55, batch loss = 0.38 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:47s remains)
INFO - root - 2017-12-15 10:27:51.161945: step 1120, loss = 0.55, batch loss = 0.37 (12.1 examples/sec; 0.660 sec/batch; 60h:46m:28s remains)
INFO - root - 2017-12-15 10:27:57.568306: step 1130, loss = 0.58, batch loss = 0.40 (13.1 examples/sec; 0.611 sec/batch; 56h:12m:55s remains)
INFO - root - 2017-12-15 10:28:03.941134: step 1140, loss = 0.59, batch loss = 0.42 (12.5 examples/sec; 0.642 sec/batch; 59h:04m:04s remains)
INFO - root - 2017-12-15 10:28:10.330599: step 1150, loss = 0.58, batch loss = 0.41 (12.5 examples/sec; 0.638 sec/batch; 58h:43m:45s remains)
INFO - root - 2017-12-15 10:28:16.731779: step 1160, loss = 0.56, batch loss = 0.39 (12.5 examples/sec; 0.642 sec/batch; 59h:07m:22s remains)
INFO - root - 2017-12-15 10:28:23.137180: step 1170, loss = 0.50, batch loss = 0.33 (12.3 examples/sec; 0.649 sec/batch; 59h:41m:19s remains)
INFO - root - 2017-12-15 10:28:29.603835: step 1180, loss = 0.50, batch loss = 0.33 (12.6 examples/sec; 0.636 sec/batch; 58h:33m:53s remains)
INFO - root - 2017-12-15 10:28:35.944408: step 1190, loss = 0.53, batch loss = 0.36 (12.6 examples/sec; 0.636 sec/batch; 58h:30m:30s remains)
INFO - root - 2017-12-15 10:28:42.425754: step 1200, loss = 0.62, batch loss = 0.44 (12.5 examples/sec; 0.638 sec/batch; 58h:40m:19s remains)
2017-12-15 10:28:42.990745: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.585876 0.61004543 0.47660446 0.22647619 0.0034737587 -0.24666595 -0.21330976 0.065423489 0.20862722 -0.3129034 -1.5736625 -2.5872598 -3.1823282 -3.5126975 -3.7392926][0.43159962 0.47757864 0.46103764 0.66046476 0.57793045 -0.039854527 -0.21817398 0.021128178 0.31055832 0.00069046021 -0.88006473 -2.1170013 -3.0994177 -3.4814577 -3.6507967][0.69888926 0.41438532 0.33521032 0.29449177 0.080665588 0.24040031 0.34506416 0.098972321 0.20454931 0.018177032 -0.74654388 -1.8830819 -2.8747015 -3.6346073 -4.2259469][-0.024400711 -0.21290588 -0.41921186 -0.51538754 -0.61199784 -0.72242904 -0.66221523 -0.10712051 0.32289553 -0.25914288 -1.2149482 -2.2994447 -3.1703095 -3.73249 -4.2922482][-0.80613256 -0.78739715 -0.5587163 -0.5848577 -0.83078313 -1.0385878 -0.97040892 -0.57497048 -0.14116144 -0.099496365 -0.67727923 -2.1803625 -3.2984085 -3.8448949 -4.2719641][-0.72610259 -0.69909549 -0.48464012 -0.34684515 -0.030029774 -0.097085476 -0.33915377 -0.42145252 -0.26871824 -0.36137533 -0.99700093 -1.8836956 -2.7555208 -3.6752505 -4.3205023][-0.91813064 -0.79631853 -0.4992137 -0.15706491 0.077851772 0.18695879 0.5116787 0.55759144 0.367414 -0.16241932 -0.9502244 -1.9936812 -2.8706341 -3.1800489 -3.5546792][-0.91548514 -0.6994586 -0.24584103 0.17776823 0.631258 0.95171165 1.0283504 1.0149312 1.2491188 0.90247583 -0.16971493 -1.6066356 -2.66626 -3.0056896 -3.3298388][-0.92341685 -0.78956771 -0.2104435 0.38850832 0.77748919 1.08639 1.2318883 1.40557 1.4205632 0.89203119 0.23617935 -0.99034071 -2.0553887 -2.6820078 -3.1583529][-1.8524022 -1.6356668 -1.1182156 -0.79211617 -0.39786673 -0.16882038 -0.15266848 -0.17320442 -0.0811944 -0.23009777 -0.72951841 -1.5390878 -2.0805745 -2.6416564 -3.1687388][-2.0370903 -1.838829 -1.3209126 -0.97557259 -0.64983916 -0.52216578 -0.55821848 -0.6648221 -0.6396215 -0.7823422 -1.043556 -1.7172284 -2.2917788 -2.5437186 -2.6671331][-2.8344755 -2.5781436 -2.2851126 -2.0958724 -1.8297081 -1.7667036 -1.8312805 -1.8092284 -1.7556703 -1.7656581 -1.8930502 -2.412816 -2.6919582 -2.8532822 -2.8953791][-3.199218 -3.0147552 -2.7659273 -2.6963558 -2.5460553 -2.5024366 -2.4885669 -2.5477526 -2.5413892 -2.4994364 -2.6696796 -2.9564886 -3.1980128 -3.1006551 -3.1476369][-3.8473909 -3.7595437 -3.6892712 -3.72495 -3.6382627 -3.6239443 -3.6264105 -3.5682759 -3.5232787 -3.5988853 -3.6375747 -3.8306623 -3.8391781 -3.9064286 -3.8047347][-4.7606025 -4.6219816 -4.6762538 -4.8749542 -5.0079374 -5.1399007 -5.2575045 -5.293159 -5.2814531 -5.2270393 -5.0894146 -4.8895812 -4.8579659 -4.81679 -4.5994372]]...]
INFO - root - 2017-12-15 10:28:49.345411: step 1210, loss = 0.55, batch loss = 0.38 (12.6 examples/sec; 0.637 sec/batch; 58h:35m:09s remains)
INFO - root - 2017-12-15 10:28:55.807885: step 1220, loss = 0.53, batch loss = 0.36 (12.2 examples/sec; 0.658 sec/batch; 60h:31m:02s remains)
INFO - root - 2017-12-15 10:29:02.169535: step 1230, loss = 0.57, batch loss = 0.40 (12.5 examples/sec; 0.640 sec/batch; 58h:50m:47s remains)
INFO - root - 2017-12-15 10:29:08.665108: step 1240, loss = 0.55, batch loss = 0.38 (12.6 examples/sec; 0.634 sec/batch; 58h:17m:49s remains)
INFO - root - 2017-12-15 10:29:15.066271: step 1250, loss = 0.50, batch loss = 0.33 (12.6 examples/sec; 0.635 sec/batch; 58h:23m:20s remains)
INFO - root - 2017-12-15 10:29:21.442761: step 1260, loss = 0.60, batch loss = 0.43 (12.6 examples/sec; 0.637 sec/batch; 58h:34m:53s remains)
INFO - root - 2017-12-15 10:29:27.870112: step 1270, loss = 0.55, batch loss = 0.38 (12.4 examples/sec; 0.643 sec/batch; 59h:07m:30s remains)
INFO - root - 2017-12-15 10:29:34.340059: step 1280, loss = 0.49, batch loss = 0.32 (12.5 examples/sec; 0.639 sec/batch; 58h:48m:30s remains)
INFO - root - 2017-12-15 10:29:40.794413: step 1290, loss = 0.54, batch loss = 0.37 (12.2 examples/sec; 0.655 sec/batch; 60h:17m:57s remains)
INFO - root - 2017-12-15 10:29:47.144258: step 1300, loss = 0.55, batch loss = 0.38 (12.6 examples/sec; 0.633 sec/batch; 58h:16m:39s remains)
2017-12-15 10:29:47.653007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.718605 -1.4657097 -1.279566 -1.232631 -1.2582028 -1.3179338 -1.2413151 -1.1522272 -0.99921727 -1.1096878 -1.6087828 -2.2836986 -2.7851961 -3.205878 -3.548821][-1.3294744 -1.1526754 -1.0401525 -1.0653093 -1.112241 -1.10851 -1.0397842 -0.86259842 -0.48557663 -0.51594734 -1.020462 -1.7593915 -2.4072759 -3.0028648 -3.4309821][-0.91018677 -0.73244047 -0.66829586 -0.71446443 -0.69939852 -0.6320281 -0.55799818 -0.35147905 0.063223839 0.099569321 -0.22424078 -0.973073 -1.7831521 -2.3736634 -2.9452131][-0.80080724 -0.60931492 -0.52592993 -0.55988121 -0.524436 -0.46398306 -0.31361198 -0.14225054 0.23204088 0.2131381 -0.15696526 -0.99471307 -1.7114928 -2.2115276 -2.7579508][-0.26422405 -0.14555454 -0.011719227 -0.01039505 -0.023605347 -0.03661108 0.0081868172 0.20166636 0.54672909 0.43298435 -0.016602039 -0.81487608 -1.5396447 -2.1441717 -2.6719568][0.14526701 0.21425104 0.33991432 0.34126139 0.36590958 0.32304525 0.3272543 0.42445135 0.70934629 0.6412611 0.23781872 -0.62641287 -1.4335976 -1.9293778 -2.3778098][0.19419384 0.12740469 0.12608528 0.16602707 0.22385597 0.16367722 0.15558863 0.29664993 0.62279224 0.54220629 0.17829084 -0.66318512 -1.455364 -1.9769392 -2.4459548][0.12736893 0.05441618 0.032799244 -0.010691166 0.016675949 -0.092681408 -0.22102022 -0.093764305 0.28786278 0.31586123 -0.011812687 -0.79553771 -1.5694847 -2.1247954 -2.6702833][0.07855463 0.054152966 0.075912476 0.091392994 0.0850935 -0.099121571 -0.22135925 -0.091885567 0.32009172 0.2848773 -0.065030575 -0.89172125 -1.6907 -2.3041458 -2.9475167][-0.425591 -0.35974503 -0.31847477 -0.42456961 -0.57768536 -0.8200407 -0.89311171 -0.78525448 -0.53247547 -0.45862961 -0.62517357 -1.3527431 -2.0771482 -2.6086113 -3.1845615][-1.2436671 -1.1182165 -1.0320115 -1.064379 -1.1649418 -1.3437862 -1.3364608 -1.2462897 -1.1019678 -0.99040842 -1.0976379 -1.6159177 -2.2738128 -2.8670621 -3.4310994][-2.1479564 -1.9916883 -1.8487263 -1.8340847 -1.8908536 -2.0227771 -1.9811993 -1.8367951 -1.5281804 -1.3922741 -1.4402254 -1.7755058 -2.2717478 -2.8014128 -3.258368][-3.1300473 -2.9925647 -2.8781781 -2.8590899 -2.857064 -2.8736708 -2.7388198 -2.5162601 -2.2509074 -1.978446 -1.8148947 -1.9122632 -2.1469996 -2.4801741 -2.9054337][-3.7412028 -3.6388805 -3.5846322 -3.634537 -3.6449265 -3.6045904 -3.4448843 -3.2239749 -2.882169 -2.4694173 -2.1450579 -1.9434872 -1.8876927 -2.1478276 -2.5865276][-4.9695754 -4.7844949 -4.5871978 -4.642643 -4.7066245 -4.6618671 -4.51563 -4.2651248 -3.8458283 -3.4467745 -2.9494524 -2.4806743 -2.2531412 -2.3267474 -2.6319902]]...]
INFO - root - 2017-12-15 10:29:54.036537: step 1310, loss = 0.62, batch loss = 0.45 (12.6 examples/sec; 0.637 sec/batch; 58h:38m:27s remains)
INFO - root - 2017-12-15 10:30:00.423052: step 1320, loss = 0.59, batch loss = 0.42 (12.7 examples/sec; 0.629 sec/batch; 57h:50m:44s remains)
INFO - root - 2017-12-15 10:30:06.900223: step 1330, loss = 0.53, batch loss = 0.36 (12.0 examples/sec; 0.666 sec/batch; 61h:15m:47s remains)
INFO - root - 2017-12-15 10:30:13.329191: step 1340, loss = 0.56, batch loss = 0.39 (12.4 examples/sec; 0.648 sec/batch; 59h:33m:50s remains)
INFO - root - 2017-12-15 10:30:19.740218: step 1350, loss = 0.49, batch loss = 0.32 (12.4 examples/sec; 0.647 sec/batch; 59h:29m:33s remains)
INFO - root - 2017-12-15 10:30:26.203771: step 1360, loss = 0.62, batch loss = 0.45 (12.4 examples/sec; 0.645 sec/batch; 59h:18m:23s remains)
INFO - root - 2017-12-15 10:30:32.624602: step 1370, loss = 0.53, batch loss = 0.36 (12.7 examples/sec; 0.631 sec/batch; 58h:02m:58s remains)
INFO - root - 2017-12-15 10:30:39.057113: step 1380, loss = 0.50, batch loss = 0.33 (12.6 examples/sec; 0.635 sec/batch; 58h:24m:17s remains)
INFO - root - 2017-12-15 10:30:45.465225: step 1390, loss = 0.57, batch loss = 0.40 (12.6 examples/sec; 0.635 sec/batch; 58h:25m:11s remains)
INFO - root - 2017-12-15 10:30:51.867280: step 1400, loss = 0.52, batch loss = 0.35 (12.8 examples/sec; 0.627 sec/batch; 57h:39m:10s remains)
2017-12-15 10:30:52.368756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7651236 -1.9807281 -2.1491916 -1.7802804 -1.0053039 -0.37773466 0.13262463 0.52174377 0.7767911 0.36125803 -0.66719961 -2.0004396 -3.2775559 -4.2395082 -4.7355466][-1.9728346 -2.3395319 -2.4284098 -1.9763327 -1.2091241 -0.44636059 0.26074839 0.75671768 0.96104956 0.57472897 -0.4474206 -1.9203174 -3.3660398 -4.4743805 -5.1000824][-1.6734533 -2.1725593 -2.3038869 -1.8895988 -1.1461022 -0.34290743 0.40007257 1.0179105 1.3994503 1.0668969 0.1192894 -1.4490273 -3.0631142 -4.2759452 -5.1052809][-1.7342267 -2.1101916 -2.1139061 -1.6687155 -0.82309484 0.054062366 0.75857162 1.2662573 1.5420065 1.1803665 0.159801 -1.4939451 -3.128758 -4.3345723 -5.1562777][-2.025677 -2.1786764 -2.0607917 -1.4344895 -0.52567148 0.43959141 1.2255268 1.7139292 1.909759 1.4708099 0.4278121 -1.3099399 -2.991117 -4.2456021 -5.1094537][-2.2955797 -2.428757 -2.2704985 -1.6073048 -0.70763636 0.36976051 1.3475089 2.0139871 2.3049412 1.9342289 0.80850887 -0.99663687 -2.7863953 -4.2030706 -5.09126][-2.461642 -2.4903362 -2.2970302 -1.524689 -0.54623842 0.48462772 1.5576954 2.3887472 2.8289428 2.4661713 1.1811576 -0.75344324 -2.651305 -4.1387529 -5.1199093][-2.6144211 -2.4608166 -2.1178064 -1.322341 -0.18526983 0.76323414 1.7406497 2.6963806 3.2886119 2.87668 1.5195427 -0.57777214 -2.5117028 -3.9812937 -4.9809747][-2.612201 -2.3931389 -2.0382824 -1.2803035 -0.075173378 1.0628872 2.0512514 2.9427848 3.6381936 3.3387523 1.9524021 -0.23122311 -2.2400856 -3.8115964 -4.8414383][-2.6545084 -2.354543 -1.9637043 -1.3253667 -0.29247046 1.0152364 2.11662 2.8375516 3.3784308 3.1962051 2.0708132 0.096219063 -1.9906983 -3.6746674 -4.7130847][-3.1444268 -2.8812356 -2.5198054 -1.8691876 -1.0620031 0.026865482 1.1482506 1.917685 2.1955104 1.9414272 1.0615153 -0.50366163 -2.3413284 -3.8351784 -4.8030357][-4.05605 -3.8116045 -3.4702616 -2.770062 -1.956068 -1.1736114 -0.30291414 0.34367704 0.45737839 -0.010723114 -0.96196628 -2.1981189 -3.600657 -4.6534414 -5.3307538][-4.1254539 -3.7892191 -3.2461429 -2.5687418 -1.8779526 -1.273751 -0.79467225 -0.52888346 -0.49689102 -0.97611666 -1.9559982 -3.06389 -3.9725163 -4.7623119 -5.3409977][-4.1368661 -3.6615529 -3.052228 -2.4933872 -1.9733496 -1.5391417 -1.2998111 -1.3301983 -1.5810261 -2.1444855 -2.9845953 -3.9459844 -4.7076955 -5.072247 -5.2278924][-4.8378072 -4.2294731 -3.5750775 -2.9772859 -2.563782 -2.3110051 -2.2319431 -2.4410143 -2.8936129 -3.5427079 -4.3346577 -5.095861 -5.5406909 -5.7164283 -5.4427233]]...]
INFO - root - 2017-12-15 10:30:58.771217: step 1410, loss = 0.55, batch loss = 0.38 (12.7 examples/sec; 0.628 sec/batch; 57h:44m:49s remains)
INFO - root - 2017-12-15 10:31:05.088810: step 1420, loss = 0.58, batch loss = 0.41 (12.6 examples/sec; 0.635 sec/batch; 58h:25m:52s remains)
INFO - root - 2017-12-15 10:31:11.558007: step 1430, loss = 0.64, batch loss = 0.47 (12.5 examples/sec; 0.639 sec/batch; 58h:43m:10s remains)
INFO - root - 2017-12-15 10:31:17.992961: step 1440, loss = 0.60, batch loss = 0.43 (12.6 examples/sec; 0.637 sec/batch; 58h:37m:01s remains)
INFO - root - 2017-12-15 10:31:24.341286: step 1450, loss = 0.53, batch loss = 0.36 (12.6 examples/sec; 0.636 sec/batch; 58h:30m:01s remains)
INFO - root - 2017-12-15 10:31:30.638942: step 1460, loss = 0.62, batch loss = 0.45 (12.7 examples/sec; 0.630 sec/batch; 57h:57m:17s remains)
INFO - root - 2017-12-15 10:31:37.041491: step 1470, loss = 0.58, batch loss = 0.41 (12.4 examples/sec; 0.644 sec/batch; 59h:10m:45s remains)
INFO - root - 2017-12-15 10:31:43.496840: step 1480, loss = 0.50, batch loss = 0.34 (12.3 examples/sec; 0.650 sec/batch; 59h:48m:42s remains)
INFO - root - 2017-12-15 10:31:49.947308: step 1490, loss = 0.71, batch loss = 0.54 (12.5 examples/sec; 0.642 sec/batch; 59h:01m:12s remains)
INFO - root - 2017-12-15 10:31:56.350312: step 1500, loss = 0.50, batch loss = 0.34 (12.8 examples/sec; 0.625 sec/batch; 57h:28m:04s remains)
2017-12-15 10:31:56.859829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3730855 -3.4188998 -3.2777431 -3.0610414 -2.5184221 -1.8206279 -1.0503058 -0.37276554 0.051599026 -0.16391325 -0.848217 -1.9385135 -3.0137146 -3.95471 -4.6322079][-3.246985 -3.3994584 -3.3619719 -2.9439037 -2.1789207 -1.3249049 -0.489048 0.14352369 0.48531246 0.20736265 -0.56067514 -1.8621974 -3.0054717 -3.9055738 -4.6151175][-2.9854813 -3.1134129 -2.9230137 -2.4226894 -1.6730254 -0.78152227 0.030721188 0.63000107 0.96993351 0.6877284 -0.1007781 -1.4387555 -2.814522 -3.9392803 -4.7278814][-2.7595158 -2.7763557 -2.4454253 -1.79265 -1.0794678 -0.32569027 0.41649532 1.0211458 1.341516 1.0693989 0.29534769 -1.1454895 -2.6102679 -3.7651696 -4.6343236][-2.310358 -2.1128361 -1.6856956 -1.0550137 -0.3297596 0.43776846 1.1313047 1.6252694 1.9093409 1.525177 0.61640835 -0.96355629 -2.5634334 -3.897078 -4.7334051][-1.8181701 -1.4766343 -0.95288873 -0.24319315 0.56085539 1.2521052 1.9102221 2.3044677 2.3376379 1.7257004 0.63610315 -1.0791123 -2.7657061 -4.1144838 -4.9783626][-1.3677485 -0.891088 -0.19566488 0.5172205 1.2240386 1.8148918 2.347445 2.6574364 2.587038 1.7931147 0.55583286 -1.2575464 -3.0109639 -4.3515368 -5.1457767][-1.019486 -0.45998621 0.19542551 0.92362118 1.636487 2.2135296 2.6686225 2.8372512 2.6488142 1.7350049 0.34388494 -1.4749756 -3.205009 -4.557723 -5.3425994][-0.88550878 -0.34744358 0.30619144 1.0516539 1.7777367 2.2570372 2.5703802 2.6625433 2.4190822 1.4730239 0.13492012 -1.7327862 -3.4662752 -4.6947684 -5.3501768][-1.9274263 -1.3583145 -0.69192362 0.004884243 0.55440474 0.98527193 1.219677 1.1312585 0.855042 0.062266827 -1.1279917 -2.7758896 -4.2229977 -5.2410192 -5.643713][-2.7513044 -2.2644258 -1.6746435 -1.0722022 -0.63177347 -0.41203785 -0.33883047 -0.39484978 -0.66791391 -1.3958824 -2.3330846 -3.6189029 -4.7791309 -5.5541673 -5.8658915][-3.7114339 -3.3283856 -2.8400674 -2.3140466 -2.0094631 -1.8801885 -1.8258014 -1.9447246 -2.1434121 -2.5823693 -3.2718449 -4.2024183 -4.9707694 -5.4847345 -5.6940107][-4.7037082 -4.4220734 -4.1185822 -3.745995 -3.5210907 -3.4678044 -3.5328274 -3.6072097 -3.6594949 -3.9291654 -4.3413763 -4.9761572 -5.4836397 -5.8059545 -5.8194542][-5.5899124 -5.4014883 -5.0770144 -4.7616243 -4.6333508 -4.5535307 -4.5230236 -4.6191812 -4.7316446 -4.874485 -5.0730405 -5.4575453 -5.7921019 -5.9501438 -5.8530011][-6.9487529 -6.7977204 -6.5498724 -6.2567677 -6.1068125 -6.1090307 -6.1243625 -6.1520166 -6.1687856 -6.2071681 -6.2720041 -6.2918186 -6.2461948 -6.1493969 -5.8520408]]...]
INFO - root - 2017-12-15 10:32:03.270467: step 1510, loss = 0.55, batch loss = 0.39 (12.6 examples/sec; 0.636 sec/batch; 58h:30m:25s remains)
INFO - root - 2017-12-15 10:32:09.850222: step 1520, loss = 0.56, batch loss = 0.39 (12.4 examples/sec; 0.645 sec/batch; 59h:18m:21s remains)
INFO - root - 2017-12-15 10:32:16.262230: step 1530, loss = 0.50, batch loss = 0.33 (12.7 examples/sec; 0.628 sec/batch; 57h:42m:02s remains)
INFO - root - 2017-12-15 10:32:22.593503: step 1540, loss = 0.64, batch loss = 0.48 (12.5 examples/sec; 0.641 sec/batch; 58h:56m:51s remains)
INFO - root - 2017-12-15 10:32:29.105221: step 1550, loss = 0.55, batch loss = 0.38 (12.3 examples/sec; 0.648 sec/batch; 59h:36m:24s remains)
INFO - root - 2017-12-15 10:32:35.494387: step 1560, loss = 0.54, batch loss = 0.37 (12.1 examples/sec; 0.659 sec/batch; 60h:34m:14s remains)
INFO - root - 2017-12-15 10:32:41.981362: step 1570, loss = 0.56, batch loss = 0.40 (12.3 examples/sec; 0.648 sec/batch; 59h:34m:29s remains)
INFO - root - 2017-12-15 10:32:48.376143: step 1580, loss = 0.51, batch loss = 0.35 (12.4 examples/sec; 0.644 sec/batch; 59h:11m:40s remains)
INFO - root - 2017-12-15 10:32:54.760114: step 1590, loss = 0.55, batch loss = 0.38 (12.8 examples/sec; 0.625 sec/batch; 57h:26m:49s remains)
INFO - root - 2017-12-15 10:33:01.134920: step 1600, loss = 0.56, batch loss = 0.40 (12.4 examples/sec; 0.645 sec/batch; 59h:18m:18s remains)
2017-12-15 10:33:01.662061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5338306 -2.4234862 -2.2699513 -2.0908082 -1.945559 -1.8791349 -1.9611266 -1.9069755 -1.7986619 -1.8983662 -2.391602 -3.171279 -3.9556389 -4.8253369 -5.4321165][-1.6153309 -1.3537369 -1.2781525 -1.2514262 -1.0815046 -0.99940586 -1.0102344 -1.038029 -1.1560197 -1.3727074 -1.8416626 -2.6182027 -3.5872321 -4.5010438 -5.1179104][-1.2305 -0.84659529 -0.37587214 -0.13465738 0.037255764 -0.093552113 -0.21929455 -0.41994429 -0.73248434 -1.0289059 -1.4595792 -2.2433078 -3.103415 -4.1963878 -4.8426318][-0.14311504 0.018799305 0.14789867 0.64699888 0.84888887 0.51653242 0.26329565 0.024950981 -0.39744806 -0.88009119 -1.237431 -2.0004964 -3.1092868 -4.1500516 -4.72306][0.38306427 0.56050873 0.69866133 0.95629787 1.080636 0.81097364 0.32366228 -0.042224407 -0.26589966 -0.5723567 -1.0832849 -1.9234977 -2.8246379 -3.8696096 -4.5739489][0.81127834 1.2440901 1.2405353 1.0301313 0.85313654 0.58237123 0.21817064 -0.18752337 -0.55977058 -0.92927742 -1.2556005 -2.1083565 -2.9547191 -3.7365894 -4.3589692][0.97098351 1.5295978 1.8404231 1.5182571 1.0977764 0.47921991 -0.045699596 -0.42332506 -0.64142275 -0.91583681 -1.3966446 -2.1947682 -3.0166197 -3.9566998 -4.5767255][1.3606253 1.7939477 1.7799048 1.4526691 0.94790411 0.35953426 -0.037499905 -0.37477684 -0.67848921 -1.0279713 -1.4019639 -2.1495502 -3.0531745 -3.946162 -4.5817304][0.59027481 1.061048 1.2743702 1.2170796 0.86973143 0.3696022 -0.027747154 -0.3116436 -0.50161219 -0.84525585 -1.3264952 -2.1367373 -2.969692 -3.8846049 -4.5763726][-0.8737545 -0.36840057 -0.05385685 0.073762417 0.072016716 -0.25764942 -0.61534929 -0.86572409 -1.093086 -1.4974339 -1.8970706 -2.6519623 -3.4546361 -4.16947 -4.6863832][-2.5562017 -1.9626999 -1.5680664 -1.4278936 -1.4563129 -1.6997902 -1.9183638 -2.1181216 -2.2304113 -2.4279199 -2.7075377 -3.2900321 -3.8018794 -4.25185 -4.5875092][-3.26442 -2.7657869 -2.4524865 -2.2883925 -2.2630103 -2.5296197 -2.7630007 -2.8715951 -2.9701424 -3.1589379 -3.3016448 -3.5911036 -3.8692646 -4.2887945 -4.6291466][-3.6340511 -3.2071016 -2.842591 -2.6640334 -2.6419389 -2.9005983 -3.0348015 -3.1278813 -3.1543872 -3.3504119 -3.5645785 -3.8984623 -4.1917934 -4.4425211 -4.6728163][-5.0838327 -4.9019828 -4.6958385 -4.4577165 -4.3203073 -4.4138803 -4.5207667 -4.591465 -4.5459085 -4.5342188 -4.5485072 -4.746562 -4.8813782 -4.9762049 -4.9440827][-6.1828985 -5.9575195 -5.6824307 -5.4915223 -5.34868 -5.4304962 -5.4855356 -5.4569411 -5.45082 -5.4549632 -5.3544292 -5.186018 -5.1038766 -5.062036 -4.954484]]...]
INFO - root - 2017-12-15 10:33:08.009895: step 1610, loss = 0.62, batch loss = 0.45 (12.5 examples/sec; 0.642 sec/batch; 58h:59m:02s remains)
INFO - root - 2017-12-15 10:33:14.445460: step 1620, loss = 0.49, batch loss = 0.32 (12.4 examples/sec; 0.647 sec/batch; 59h:27m:33s remains)
INFO - root - 2017-12-15 10:33:20.834333: step 1630, loss = 0.50, batch loss = 0.33 (12.6 examples/sec; 0.636 sec/batch; 58h:24m:49s remains)
INFO - root - 2017-12-15 10:33:27.162130: step 1640, loss = 0.47, batch loss = 0.30 (13.0 examples/sec; 0.613 sec/batch; 56h:22m:33s remains)
INFO - root - 2017-12-15 10:33:33.525813: step 1650, loss = 0.56, batch loss = 0.39 (12.7 examples/sec; 0.629 sec/batch; 57h:49m:20s remains)
INFO - root - 2017-12-15 10:33:39.975518: step 1660, loss = 0.66, batch loss = 0.50 (12.2 examples/sec; 0.655 sec/batch; 60h:12m:31s remains)
INFO - root - 2017-12-15 10:33:46.405079: step 1670, loss = 0.56, batch loss = 0.39 (12.5 examples/sec; 0.638 sec/batch; 58h:36m:25s remains)
INFO - root - 2017-12-15 10:33:52.830173: step 1680, loss = 0.54, batch loss = 0.37 (12.4 examples/sec; 0.644 sec/batch; 59h:11m:05s remains)
INFO - root - 2017-12-15 10:33:59.281326: step 1690, loss = 0.55, batch loss = 0.38 (12.5 examples/sec; 0.639 sec/batch; 58h:43m:30s remains)
INFO - root - 2017-12-15 10:34:05.603927: step 1700, loss = 0.51, batch loss = 0.35 (12.6 examples/sec; 0.637 sec/batch; 58h:31m:21s remains)
2017-12-15 10:34:06.125735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1242702 -1.0158293 -0.94805908 -0.94735 -0.72276306 -0.6424346 -0.52955055 -0.34474325 -0.24796104 -0.35832024 -0.83777618 -1.5991945 -2.4564035 -3.3853226 -4.1222591][-0.67850971 -0.49788857 -0.33663845 -0.29107332 -0.10471869 -0.095161438 -0.076622009 0.036386013 0.1268425 -0.031165123 -0.47920132 -1.3302367 -2.2742405 -3.2893374 -4.1908846][0.10201836 0.31675005 0.53296185 0.661046 0.821784 0.74599743 0.69430256 0.71364546 0.7599268 0.573462 0.17965746 -0.79610109 -1.8477302 -2.9696343 -4.0287609][0.20022392 0.45898962 0.69010305 0.8340764 0.99721479 0.86115313 0.77617741 0.74057007 0.7556963 0.57557058 0.21135664 -0.73363495 -1.7618563 -2.8933694 -4.0096259][0.34695005 0.59139538 0.80626059 0.97161341 1.1365089 0.99465609 0.90889072 0.87178183 0.91705847 0.772789 0.43327284 -0.52806282 -1.584445 -2.7523611 -3.8971176][0.45836926 0.69722748 0.92438316 1.1014266 1.289814 1.1666851 1.1099753 1.1276278 1.2163882 1.0933943 0.76183939 -0.21231222 -1.2972729 -2.5312591 -3.7457137][0.44665051 0.69811296 0.92291307 1.0964112 1.292697 1.2195525 1.2354674 1.3188858 1.4615555 1.3747234 1.0517473 0.049625397 -1.0873821 -2.3593795 -3.5925217][0.475492 0.7497592 1.0072889 1.2233233 1.4767699 1.4327078 1.4886951 1.5902462 1.7407789 1.642283 1.3040137 0.26735067 -0.90408278 -2.1948259 -3.4519329][0.46996784 0.74331474 1.0267963 1.2553849 1.5368428 1.4956951 1.5583153 1.6663728 1.8208156 1.7259912 1.3917098 0.34580946 -0.83207607 -2.1281807 -3.3928919][-0.26086664 0.052616119 0.37186813 0.60300064 0.85887671 0.83311081 0.87854958 0.97665834 1.1019335 1.0056973 0.68334866 -0.27401447 -1.3245435 -2.4321687 -3.518024][-0.77555513 -0.43595695 -0.053489208 0.17122841 0.42218542 0.38450575 0.41569996 0.48429585 0.57778597 0.431602 0.086506844 -0.83090162 -1.826529 -2.8539705 -3.90594][-2.2311165 -1.94999 -1.6378865 -1.4184031 -1.1710978 -1.2173882 -1.2049675 -1.1557488 -1.0838349 -1.1858044 -1.4461803 -2.0971069 -2.7704091 -3.5584257 -4.3789492][-3.5989144 -3.4013677 -3.0818756 -2.8524237 -2.6177945 -2.6693957 -2.6839151 -2.647543 -2.5954373 -2.7119863 -2.9403195 -3.4107606 -3.9478569 -4.4546995 -4.934607][-4.8807106 -4.6813488 -4.3416147 -4.1369386 -3.9466352 -3.9668007 -3.9752462 -3.9445879 -3.9072976 -3.9382889 -4.0518937 -4.3696432 -4.6839986 -4.9445829 -5.2372961][-6.6605844 -6.4811707 -6.1862907 -6.0419145 -5.8589754 -5.8708038 -5.8760967 -5.8418841 -5.804338 -5.7678795 -5.7484231 -5.7170606 -5.7054305 -5.6419415 -5.6719928]]...]
INFO - root - 2017-12-15 10:34:12.412183: step 1710, loss = 0.52, batch loss = 0.35 (12.7 examples/sec; 0.628 sec/batch; 57h:43m:19s remains)
INFO - root - 2017-12-15 10:34:18.846582: step 1720, loss = 0.55, batch loss = 0.38 (12.4 examples/sec; 0.644 sec/batch; 59h:08m:53s remains)
INFO - root - 2017-12-15 10:34:25.211432: step 1730, loss = 0.53, batch loss = 0.37 (12.3 examples/sec; 0.650 sec/batch; 59h:44m:49s remains)
INFO - root - 2017-12-15 10:34:31.579397: step 1740, loss = 0.54, batch loss = 0.38 (12.4 examples/sec; 0.644 sec/batch; 59h:08m:00s remains)
INFO - root - 2017-12-15 10:34:38.026736: step 1750, loss = 0.50, batch loss = 0.34 (12.5 examples/sec; 0.638 sec/batch; 58h:34m:52s remains)
INFO - root - 2017-12-15 10:34:44.398796: step 1760, loss = 0.61, batch loss = 0.44 (12.5 examples/sec; 0.642 sec/batch; 58h:59m:56s remains)
INFO - root - 2017-12-15 10:34:50.790044: step 1770, loss = 0.58, batch loss = 0.42 (12.7 examples/sec; 0.632 sec/batch; 58h:02m:07s remains)
INFO - root - 2017-12-15 10:34:57.152891: step 1780, loss = 0.58, batch loss = 0.42 (12.8 examples/sec; 0.625 sec/batch; 57h:24m:22s remains)
INFO - root - 2017-12-15 10:35:03.598870: step 1790, loss = 0.54, batch loss = 0.38 (12.3 examples/sec; 0.653 sec/batch; 59h:59m:28s remains)
INFO - root - 2017-12-15 10:35:10.074786: step 1800, loss = 0.46, batch loss = 0.30 (12.4 examples/sec; 0.647 sec/batch; 59h:28m:30s remains)
2017-12-15 10:35:10.566675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3305345 -3.1976588 -3.0178688 -2.8326745 -2.6692219 -2.6506078 -2.4735017 -2.2570128 -1.9448953 -1.9537795 -2.3110065 -2.8202083 -3.494194 -3.9830651 -4.404707][-3.318084 -3.2478693 -3.1286824 -3.040272 -2.9890447 -3.0240006 -2.8419452 -2.5439661 -2.1816123 -2.1741745 -2.3594186 -2.8064501 -3.4094627 -3.901228 -4.4082804][-3.215595 -3.2714667 -3.2779665 -3.2957597 -3.3496814 -3.475204 -3.3815827 -3.1465526 -2.7958391 -2.700747 -2.7720711 -3.1177073 -3.536391 -3.9565678 -4.5101795][-2.9323628 -3.0857677 -3.2178974 -3.3944273 -3.5284266 -3.72333 -3.7060928 -3.5576775 -3.3344173 -3.3053679 -3.4345412 -3.7785509 -4.1326389 -4.4763422 -4.8841524][-2.2352674 -2.1732059 -2.0760541 -2.0945437 -2.1846573 -2.3713915 -2.4311662 -2.3571029 -2.2694411 -2.4583943 -2.8643031 -3.4374743 -3.9477718 -4.5040326 -5.0292864][-1.5041966 -1.2249007 -0.97919178 -0.73966169 -0.61924982 -0.5696187 -0.47620869 -0.30929995 -0.22850943 -0.55118895 -1.1692955 -2.0392196 -2.9555218 -3.8502135 -4.5818396][-1.0189867 -0.77616167 -0.49970484 0.00023126602 0.44895649 0.72258377 1.0025463 1.3651838 1.5618782 1.217133 0.49670362 -0.58734512 -1.7822833 -2.9453475 -4.0064888][-0.59924412 -0.35309219 -0.12210417 0.50134706 1.0123539 1.3489623 1.7019434 2.114234 2.4069815 2.1111369 1.3935962 0.29407072 -0.94187593 -2.1930516 -3.411149][-0.77765751 -0.47855234 -0.21692419 0.23112059 0.54331875 1.0511961 1.5955305 2.10706 2.5696082 2.2366972 1.4041286 0.22518587 -1.0859766 -2.3968527 -3.5639186][-1.409411 -1.2251902 -0.91328 -0.61233711 -0.35753202 0.0013918877 0.34233904 0.86193752 1.4295688 1.2944613 0.72947788 -0.19141197 -1.2840338 -2.3688734 -3.3558574][-1.8319349 -1.6542392 -1.429883 -1.215425 -1.0091147 -0.9040966 -0.76836586 -0.30386686 0.17336845 -0.01651907 -0.44856691 -1.1326022 -1.9027998 -2.7241497 -3.544661][-3.3906939 -3.2948627 -3.125421 -2.996701 -2.8399119 -2.850174 -2.7820833 -2.5451248 -2.3219969 -2.2848792 -2.3238554 -2.5518007 -2.7784789 -3.1426206 -3.6834722][-4.05619 -3.9887788 -3.8422694 -3.7568254 -3.6679292 -3.7429185 -3.7506642 -3.6681342 -3.5900779 -3.6222458 -3.6606433 -3.8415008 -3.9305921 -4.1458578 -4.4469914][-5.3637552 -5.3552885 -5.2780228 -5.2205896 -5.1627388 -5.2428823 -5.2934732 -5.3042126 -5.2676988 -5.2211075 -5.1950717 -5.0978208 -4.9914875 -5.0289268 -5.1047325][-6.4204264 -6.42362 -6.3313665 -6.2495928 -6.2132912 -6.3083577 -6.3187423 -6.3242764 -6.3237286 -6.3107376 -6.2238283 -6.1921983 -6.1609993 -5.9649572 -5.7355156]]...]
INFO - root - 2017-12-15 10:35:16.988885: step 1810, loss = 0.50, batch loss = 0.34 (12.6 examples/sec; 0.636 sec/batch; 58h:25m:49s remains)
INFO - root - 2017-12-15 10:35:23.328464: step 1820, loss = 0.63, batch loss = 0.47 (12.5 examples/sec; 0.639 sec/batch; 58h:44m:03s remains)
INFO - root - 2017-12-15 10:35:29.708712: step 1830, loss = 0.50, batch loss = 0.34 (12.2 examples/sec; 0.654 sec/batch; 60h:04m:40s remains)
INFO - root - 2017-12-15 10:35:36.120828: step 1840, loss = 0.50, batch loss = 0.34 (12.4 examples/sec; 0.643 sec/batch; 59h:02m:18s remains)
INFO - root - 2017-12-15 10:35:42.508523: step 1850, loss = 0.51, batch loss = 0.35 (12.6 examples/sec; 0.637 sec/batch; 58h:30m:57s remains)
INFO - root - 2017-12-15 10:35:48.941107: step 1860, loss = 0.47, batch loss = 0.31 (12.8 examples/sec; 0.627 sec/batch; 57h:34m:57s remains)
INFO - root - 2017-12-15 10:35:55.358225: step 1870, loss = 0.59, batch loss = 0.43 (12.6 examples/sec; 0.635 sec/batch; 58h:18m:29s remains)
INFO - root - 2017-12-15 10:36:01.703185: step 1880, loss = 0.49, batch loss = 0.33 (12.8 examples/sec; 0.623 sec/batch; 57h:10m:38s remains)
INFO - root - 2017-12-15 10:36:08.126032: step 1890, loss = 0.49, batch loss = 0.33 (12.5 examples/sec; 0.642 sec/batch; 58h:57m:52s remains)
INFO - root - 2017-12-15 10:36:14.435897: step 1900, loss = 0.51, batch loss = 0.35 (12.4 examples/sec; 0.644 sec/batch; 59h:09m:02s remains)
2017-12-15 10:36:14.975357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1325445 -2.8026743 -2.4067667 -1.9866185 -1.5666251 -1.1729574 -0.71594143 -0.14644194 0.48086691 0.7188015 0.34700346 -0.52380133 -1.5230355 -2.6914332 -3.7774477][-3.0476909 -2.571512 -2.0925307 -1.6385317 -1.2872062 -0.88348246 -0.32490158 0.36358356 0.88292313 1.0077348 0.72138786 -0.20043564 -1.3051255 -2.5636935 -3.6649566][-3.0585728 -2.3997903 -1.7043226 -1.1455936 -0.59990597 -0.11122322 0.39661169 0.92141438 1.3633289 1.48557 1.2479076 0.07613945 -1.1721246 -2.4766285 -3.711235][-2.1488221 -1.5268147 -0.82782459 -0.3228178 0.20510054 0.54077578 0.94001245 1.3671484 1.6510248 1.5527201 1.175694 0.06132412 -1.2401094 -2.6100028 -3.6905422][-0.64779711 -0.14406681 0.39354658 0.75073671 1.0368843 1.2700334 1.5479255 1.7959256 1.9490008 1.8804154 1.4929566 0.18539095 -1.1021848 -2.490381 -3.57687][0.39707184 0.64259338 0.93849564 1.4033737 1.7785826 1.9743686 2.1019158 2.2168708 2.2604108 1.9530683 1.4138002 0.12283373 -1.1663818 -2.5474038 -3.6659579][0.72121191 0.98058844 1.2614379 1.6602936 1.9760642 2.2139864 2.3941875 2.4176316 2.3492231 1.9388623 1.3078866 -0.11816168 -1.4290802 -2.7021315 -3.8163886][1.0575566 1.3683209 1.6630859 2.0412245 2.4000397 2.60811 2.6753583 2.6106548 2.4942808 1.9983454 1.2884393 -0.1457243 -1.426507 -2.7186716 -3.8125181][1.3186617 1.7312255 2.2049246 2.4575858 2.6345134 2.7374325 2.7255592 2.6127486 2.4762712 1.9958711 1.3224978 -0.074397087 -1.2934635 -2.5390489 -3.5740926][0.46147823 0.94956875 1.3393879 1.643939 1.937408 1.9403434 1.7891564 1.686439 1.5562096 1.0945845 0.47506094 -0.79052687 -1.8892903 -2.8956826 -3.6751971][-1.2722721 -0.80299854 -0.41071606 -0.038855076 0.14555883 0.27966118 0.34393597 0.24832869 0.073908806 -0.36389112 -0.97613907 -2.0992208 -2.9714446 -3.6678104 -4.1746912][-2.7633307 -2.37926 -2.2105904 -1.9093008 -1.6227648 -1.4624586 -1.4711902 -1.4863849 -1.4584796 -1.8037856 -2.3436639 -3.205143 -3.8797665 -4.4519897 -4.7809453][-4.1042352 -3.9064567 -3.702394 -3.3551722 -3.0996108 -3.0240076 -3.0701849 -3.0937946 -3.0719404 -3.2388277 -3.5057979 -3.9467969 -4.2969055 -4.6930947 -4.9721208][-5.2857404 -5.154933 -5.011816 -4.8437233 -4.6429644 -4.5653725 -4.5547571 -4.5646739 -4.5493121 -4.5314932 -4.5883141 -4.8394361 -5.0517254 -5.0636649 -4.9488993][-6.4140611 -6.4130497 -6.3680491 -6.2464552 -6.1250467 -6.0763607 -5.909481 -5.7963166 -5.6711287 -5.5448737 -5.4659557 -5.3186693 -5.24849 -5.17733 -5.1562266]]...]
INFO - root - 2017-12-15 10:36:21.532277: step 1910, loss = 0.56, batch loss = 0.39 (12.7 examples/sec; 0.629 sec/batch; 57h:44m:17s remains)
INFO - root - 2017-12-15 10:36:27.893128: step 1920, loss = 0.50, batch loss = 0.33 (12.4 examples/sec; 0.645 sec/batch; 59h:14m:36s remains)
INFO - root - 2017-12-15 10:36:34.354060: step 1930, loss = 0.63, batch loss = 0.47 (11.9 examples/sec; 0.675 sec/batch; 61h:57m:27s remains)
INFO - root - 2017-12-15 10:36:40.732215: step 1940, loss = 0.61, batch loss = 0.45 (12.5 examples/sec; 0.642 sec/batch; 58h:56m:02s remains)
INFO - root - 2017-12-15 10:36:47.069978: step 1950, loss = 0.57, batch loss = 0.41 (12.6 examples/sec; 0.637 sec/batch; 58h:30m:33s remains)
INFO - root - 2017-12-15 10:36:53.471208: step 1960, loss = 0.60, batch loss = 0.44 (12.5 examples/sec; 0.637 sec/batch; 58h:31m:56s remains)
INFO - root - 2017-12-15 10:36:59.870558: step 1970, loss = 0.53, batch loss = 0.37 (12.1 examples/sec; 0.663 sec/batch; 60h:52m:13s remains)
INFO - root - 2017-12-15 10:37:06.320277: step 1980, loss = 0.54, batch loss = 0.38 (12.4 examples/sec; 0.643 sec/batch; 59h:01m:30s remains)
INFO - root - 2017-12-15 10:37:12.703923: step 1990, loss = 0.48, batch loss = 0.32 (12.7 examples/sec; 0.630 sec/batch; 57h:49m:44s remains)
INFO - root - 2017-12-15 10:37:19.119364: step 2000, loss = 0.52, batch loss = 0.36 (12.1 examples/sec; 0.664 sec/batch; 60h:55m:12s remains)
2017-12-15 10:37:19.632638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.95179605 -1.1502438 -1.0432348 -0.592144 -0.48845577 -0.25765514 -0.071093559 -0.041932583 0.1643219 0.30064774 0.19025993 -0.41327477 -1.222578 -2.1744692 -3.1937692][-1.4180295 -1.1447229 -1.0715289 -1.0637193 -1.016078 -0.70914268 -0.47141027 -0.24003506 -0.0014395714 -0.10753679 -0.35998535 -0.64025784 -1.1136317 -2.1297262 -3.0900688][-1.6131461 -1.5188637 -1.381772 -1.3089106 -1.2854292 -1.1043992 -0.89969587 -0.6030097 -0.22465038 -0.23994398 -0.38617849 -1.0381918 -1.5994971 -2.0645483 -2.67605][-1.3018527 -1.4108522 -1.6097775 -1.5021422 -1.3572001 -1.4007294 -1.4456666 -1.3807545 -1.160172 -1.0661716 -1.1440148 -1.8072691 -2.3869061 -3.1786749 -3.6640859][-1.560555 -1.3290043 -1.2362409 -0.92664051 -0.70826244 -0.595603 -0.66727543 -0.9429822 -1.2493105 -1.7492588 -2.1031148 -2.3793416 -2.6203473 -3.1372087 -3.5710919][-1.5603735 -1.3731394 -1.0233717 -0.71745968 -0.51138639 -0.10226297 0.19138193 0.14190817 -0.18898058 -0.90090227 -1.6492198 -2.470226 -3.0920727 -3.4245186 -3.6610448][-1.427753 -1.2977221 -1.0879245 -0.76242161 -0.37956905 -0.2097044 -0.1742363 -0.059864998 -0.074079037 -0.52190638 -1.1450214 -1.8540835 -2.6027722 -3.300559 -3.7817266][-1.5121028 -1.3684397 -1.1498737 -0.79812145 -0.46075106 -0.18368816 0.010343075 0.069442272 -0.057139397 -0.44374704 -0.93137407 -1.7124236 -2.4325306 -2.9952378 -3.5515666][-1.912128 -1.7488012 -1.328495 -0.9625349 -0.5758338 -0.26294041 -0.023488045 0.032835484 -0.032063484 -0.34262705 -0.8119998 -1.5967181 -2.2396493 -2.8694565 -3.4772701][-2.3250325 -2.462168 -2.271275 -1.8501019 -1.2722297 -0.79135036 -0.43518019 -0.31581211 -0.40382624 -0.69590044 -1.1228476 -1.8667848 -2.6243608 -3.2197723 -3.7453012][-2.3605366 -2.4183652 -2.3063128 -2.0642686 -1.6201792 -1.1009183 -0.63725328 -0.32452059 -0.20782614 -0.42672443 -0.85336637 -1.6462793 -2.4619722 -3.1094482 -3.6483126][-3.0013671 -3.0524483 -2.9906611 -2.8452241 -2.5259132 -2.2678037 -1.804091 -1.2955656 -0.97720718 -1.0210214 -1.2740352 -1.8236516 -2.4605229 -3.15069 -3.8405385][-3.5690107 -3.6224861 -3.5814233 -3.393163 -3.2107956 -2.9867876 -2.7742987 -2.5779059 -2.3510089 -2.2119675 -2.2695675 -2.5389137 -2.8189516 -3.259366 -3.962934][-4.0649939 -4.1104932 -4.0738616 -4.0110784 -3.860465 -3.6705391 -3.4379377 -3.2926998 -3.2328625 -3.1933451 -3.3474913 -3.537641 -3.7225218 -3.826447 -4.0086093][-5.0085087 -4.9415116 -4.9588933 -4.9274759 -4.9012647 -4.9056411 -4.8596644 -4.9101105 -4.9137421 -4.9303851 -4.8479156 -4.8707986 -4.9494715 -4.8981838 -4.7834992]]...]
INFO - root - 2017-12-15 10:37:26.122927: step 2010, loss = 0.50, batch loss = 0.34 (12.6 examples/sec; 0.637 sec/batch; 58h:28m:58s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:37:32.519508: step 2020, loss = 0.53, batch loss = 0.37 (12.6 examples/sec; 0.633 sec/batch; 58h:08m:23s remains)
INFO - root - 2017-12-15 10:37:38.871491: step 2030, loss = 0.59, batch loss = 0.43 (12.1 examples/sec; 0.662 sec/batch; 60h:43m:31s remains)
INFO - root - 2017-12-15 10:37:45.342546: step 2040, loss = 0.53, batch loss = 0.37 (12.0 examples/sec; 0.669 sec/batch; 61h:24m:45s remains)
INFO - root - 2017-12-15 10:37:51.806430: step 2050, loss = 0.51, batch loss = 0.35 (12.4 examples/sec; 0.645 sec/batch; 59h:11m:24s remains)
INFO - root - 2017-12-15 10:37:58.283856: step 2060, loss = 0.46, batch loss = 0.30 (12.1 examples/sec; 0.663 sec/batch; 60h:51m:04s remains)
INFO - root - 2017-12-15 10:38:04.627563: step 2070, loss = 0.54, batch loss = 0.38 (13.1 examples/sec; 0.611 sec/batch; 56h:05m:18s remains)
INFO - root - 2017-12-15 10:38:10.956310: step 2080, loss = 0.51, batch loss = 0.35 (12.5 examples/sec; 0.638 sec/batch; 58h:31m:34s remains)
INFO - root - 2017-12-15 10:38:17.248624: step 2090, loss = 0.62, batch loss = 0.46 (13.1 examples/sec; 0.612 sec/batch; 56h:12m:12s remains)
INFO - root - 2017-12-15 10:38:23.609934: step 2100, loss = 0.78, batch loss = 0.62 (12.6 examples/sec; 0.634 sec/batch; 58h:12m:54s remains)
2017-12-15 10:38:24.169266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7662022 -1.7251146 -1.6831956 -1.70873 -1.7081666 -1.8007262 -2.0270419 -2.2223299 -2.1698887 -2.2302024 -2.7477212 -3.5545015 -4.1740837 -4.4398012 -4.9390488][-1.3565495 -1.2708926 -1.2723088 -1.3018775 -1.2283545 -1.2027645 -1.3510251 -1.6048932 -1.7460003 -1.8275025 -2.3321962 -3.0487683 -3.6478834 -4.0684395 -4.4048452][-0.81169987 -0.57444429 -0.32053041 -0.25160456 -0.20154428 -0.1776638 -0.31834841 -0.64795017 -0.93251467 -1.2013216 -1.8419604 -2.6477168 -3.3102543 -3.7142491 -4.0823469][-0.57980824 -0.27119112 -0.01504755 0.16821718 0.34000587 0.31676149 0.14459372 -0.15093946 -0.41650152 -1.00349 -1.8395617 -2.6720166 -3.3181727 -3.8487413 -4.2989225][-0.35032177 -0.043370247 0.36056566 0.52528095 0.59872389 0.64607382 0.54059839 0.27152824 0.0070004463 -0.5864706 -1.4873943 -2.5265722 -3.3832135 -3.8601339 -4.1195521][-0.11910439 0.19168472 0.57965326 0.7640872 0.82992792 0.90133715 0.79279518 0.5883646 0.35770226 -0.24743605 -1.2115598 -2.3244777 -3.2714822 -3.7591288 -4.0766664][0.32595634 0.6668539 0.96857357 1.1460671 1.2209163 1.1639433 1.0095987 0.78323078 0.470963 -0.31287479 -1.4775465 -2.6202998 -3.4757791 -3.8695121 -4.0795546][0.51667881 0.74864864 1.0494533 1.3488955 1.4445558 1.385983 1.2363176 0.97995758 0.68188667 -0.0706954 -1.1734123 -2.4580557 -3.330318 -3.6206062 -3.8394384][0.65109205 0.89594555 1.0147395 1.2006497 1.1675038 1.1780949 1.1449656 1.0193148 0.8412714 0.13290787 -1.0614314 -2.1585772 -2.9334126 -3.3587923 -3.6014836][-0.54673624 -0.2259407 0.091067314 0.20405626 0.13084745 0.11389732 0.057549 -0.027350426 -0.13243723 -0.67293358 -1.3899503 -2.3228967 -3.1937594 -3.458529 -3.6573088][-1.8436334 -1.5948021 -1.2931013 -1.1992855 -1.1828213 -1.1214342 -1.0981379 -1.1475458 -1.2422128 -1.581059 -2.0591178 -2.8451343 -3.3593659 -3.544878 -3.8546872][-2.923945 -2.6430631 -2.4653959 -2.3357818 -2.3032598 -2.3015721 -2.2863257 -2.2446353 -2.2520025 -2.5983081 -3.0121162 -3.3287435 -3.5769048 -3.8459678 -4.0500631][-3.5307364 -3.4071119 -3.23544 -3.046263 -2.9418371 -2.9386773 -2.9293392 -2.9665251 -3.0093277 -3.0964699 -3.2636964 -3.7710657 -4.1548438 -4.2470665 -4.3501854][-4.2122555 -4.1028509 -3.9866428 -3.9433055 -3.8737159 -3.8637245 -3.8525572 -3.8892765 -3.9037783 -3.9980426 -4.13182 -4.3541007 -4.5691385 -4.6473093 -4.7068486][-5.4455972 -5.29923 -5.2238584 -5.2263908 -5.1622391 -5.1916275 -5.107039 -5.0953131 -5.1239977 -5.1043987 -5.0298829 -4.9211755 -4.8715172 -4.8763442 -4.8300977]]...]
INFO - root - 2017-12-15 10:38:30.534314: step 2110, loss = 0.49, batch loss = 0.33 (12.6 examples/sec; 0.636 sec/batch; 58h:21m:21s remains)
INFO - root - 2017-12-15 10:38:37.008901: step 2120, loss = 0.44, batch loss = 0.29 (12.4 examples/sec; 0.645 sec/batch; 59h:09m:23s remains)
INFO - root - 2017-12-15 10:38:43.422603: step 2130, loss = 0.57, batch loss = 0.41 (12.1 examples/sec; 0.661 sec/batch; 60h:38m:03s remains)
INFO - root - 2017-12-15 10:38:49.780000: step 2140, loss = 0.53, batch loss = 0.38 (12.4 examples/sec; 0.644 sec/batch; 59h:06m:20s remains)
INFO - root - 2017-12-15 10:38:56.105204: step 2150, loss = 0.46, batch loss = 0.30 (12.4 examples/sec; 0.645 sec/batch; 59h:11m:53s remains)
INFO - root - 2017-12-15 10:39:02.573884: step 2160, loss = 0.54, batch loss = 0.38 (12.2 examples/sec; 0.654 sec/batch; 60h:00m:27s remains)
INFO - root - 2017-12-15 10:39:08.993289: step 2170, loss = 0.46, batch loss = 0.30 (12.6 examples/sec; 0.633 sec/batch; 58h:03m:35s remains)
INFO - root - 2017-12-15 10:39:15.342355: step 2180, loss = 0.53, batch loss = 0.37 (12.7 examples/sec; 0.630 sec/batch; 57h:51m:04s remains)
INFO - root - 2017-12-15 10:39:21.782609: step 2190, loss = 0.50, batch loss = 0.35 (12.7 examples/sec; 0.630 sec/batch; 57h:50m:35s remains)
INFO - root - 2017-12-15 10:39:28.157646: step 2200, loss = 0.55, batch loss = 0.40 (13.0 examples/sec; 0.617 sec/batch; 56h:38m:51s remains)
2017-12-15 10:39:28.607131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3288159 -2.270036 -2.3992028 -2.3630424 -2.218147 -1.959372 -1.6950307 -1.5615981 -1.5213251 -1.8109043 -2.4900098 -3.2822783 -3.8042212 -4.199749 -4.5329208][-2.4150288 -2.4620984 -2.5245185 -2.3244686 -2.0718985 -1.8011405 -1.4275374 -1.0968204 -0.93485737 -1.3166342 -2.0948277 -2.9915459 -3.6330819 -4.2556992 -4.7370968][-2.6024709 -2.7090943 -2.8020737 -2.6851346 -2.2903366 -1.8466458 -1.3985081 -1.01373 -0.7147646 -0.90404081 -1.553746 -2.6497266 -3.4874666 -4.2538443 -4.8068352][-2.9101152 -3.0623395 -3.1948669 -3.0983047 -2.7175455 -2.2622533 -1.7230844 -1.2132125 -0.90126753 -0.98331738 -1.5092361 -2.546844 -3.4125807 -4.2451806 -4.9262953][-2.8489749 -2.8767188 -2.9263628 -2.802979 -2.4590282 -2.0288875 -1.4938183 -1.0499597 -0.79432678 -0.91393232 -1.4432769 -2.4204559 -3.2855134 -4.1400805 -4.8190846][-2.5728343 -2.5756674 -2.4604936 -2.1653976 -1.6782582 -1.2440839 -0.85071373 -0.48431873 -0.29051018 -0.62902021 -1.3982511 -2.4948995 -3.3460331 -4.1636391 -4.7858434][-2.2692196 -2.2423801 -2.0990849 -1.6731982 -1.0603528 -0.45566893 0.13133335 0.50763035 0.51010084 -0.040244102 -1.0444646 -2.485924 -3.5797887 -4.3925242 -4.9616079][-1.9684775 -1.873522 -1.7269115 -1.2744093 -0.58976126 0.084820271 0.7201829 1.1701999 1.2438445 0.61848593 -0.57809162 -2.1523852 -3.3660281 -4.4112654 -5.1019983][-1.8511434 -1.695899 -1.4206953 -0.94850111 -0.35451508 0.24813604 0.89644957 1.3235822 1.3663373 0.82157326 -0.26478004 -1.8616235 -3.1068652 -4.2013359 -5.0492268][-2.4262943 -2.2208652 -1.9689598 -1.5164361 -0.93055677 -0.39851141 0.090101719 0.4014163 0.43637228 -0.078237057 -0.99391556 -2.3165472 -3.3807638 -4.2707281 -4.9102163][-2.7553592 -2.5928652 -2.3778863 -2.0108881 -1.5620747 -1.1858468 -0.8086462 -0.56597996 -0.50802755 -0.85376549 -1.5659771 -2.7315314 -3.7229252 -4.5471973 -5.0650177][-3.3026519 -3.2047842 -3.0672729 -2.8083906 -2.5043764 -2.3052649 -2.0799305 -1.9427443 -1.8995039 -1.9898574 -2.40111 -3.2814744 -4.0816569 -4.728858 -5.1076393][-3.8717408 -3.7765932 -3.6696954 -3.4916222 -3.3050272 -3.2470989 -3.1868761 -3.1441138 -3.1374316 -3.209023 -3.4639053 -3.8615379 -4.1890459 -4.594871 -4.8601837][-4.8649364 -4.7768178 -4.6818223 -4.5548992 -4.4111781 -4.3691473 -4.3828077 -4.4337411 -4.49736 -4.5146341 -4.6032767 -4.815094 -4.92919 -5.0566378 -5.0093145][-5.9304857 -5.8772149 -5.8212118 -5.6555438 -5.5137587 -5.5492253 -5.5698705 -5.5986133 -5.6729622 -5.7008219 -5.6536469 -5.62605 -5.6405611 -5.5491385 -5.3998566]]...]
INFO - root - 2017-12-15 10:39:35.004173: step 2210, loss = 0.52, batch loss = 0.36 (12.6 examples/sec; 0.637 sec/batch; 58h:25m:28s remains)
INFO - root - 2017-12-15 10:39:41.436926: step 2220, loss = 0.53, batch loss = 0.38 (12.7 examples/sec; 0.631 sec/batch; 57h:50m:45s remains)
INFO - root - 2017-12-15 10:39:47.739607: step 2230, loss = 0.48, batch loss = 0.33 (12.6 examples/sec; 0.633 sec/batch; 58h:02m:43s remains)
INFO - root - 2017-12-15 10:39:54.096815: step 2240, loss = 0.50, batch loss = 0.34 (12.5 examples/sec; 0.638 sec/batch; 58h:30m:00s remains)
INFO - root - 2017-12-15 10:40:00.437673: step 2250, loss = 0.58, batch loss = 0.43 (12.8 examples/sec; 0.626 sec/batch; 57h:25m:28s remains)
INFO - root - 2017-12-15 10:40:06.806262: step 2260, loss = 0.55, batch loss = 0.40 (12.7 examples/sec; 0.631 sec/batch; 57h:53m:26s remains)
INFO - root - 2017-12-15 10:40:13.207685: step 2270, loss = 0.57, batch loss = 0.42 (12.9 examples/sec; 0.622 sec/batch; 57h:02m:48s remains)
INFO - root - 2017-12-15 10:40:19.612034: step 2280, loss = 0.57, batch loss = 0.41 (12.7 examples/sec; 0.631 sec/batch; 57h:55m:25s remains)
INFO - root - 2017-12-15 10:40:25.970548: step 2290, loss = 0.43, batch loss = 0.27 (12.9 examples/sec; 0.622 sec/batch; 57h:00m:31s remains)
INFO - root - 2017-12-15 10:40:32.321825: step 2300, loss = 0.45, batch loss = 0.30 (12.8 examples/sec; 0.624 sec/batch; 57h:16m:13s remains)
2017-12-15 10:40:32.817720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.942565 -1.8064916 -1.6247504 -1.5076239 -1.4167504 -1.216156 -0.94712257 -0.81403112 -0.7909832 -1.0147691 -1.4638281 -1.7585497 -2.1076746 -2.7512546 -3.5788679][-1.8466933 -1.7499774 -1.6848431 -1.613368 -1.5428815 -1.3644953 -1.0120654 -0.77775288 -0.57000065 -0.74838161 -1.1122036 -1.7007475 -2.1325033 -2.3686445 -2.8130484][-1.3117776 -1.0230603 -0.85461426 -1.0086403 -1.1782832 -1.0733738 -0.87143755 -0.9809804 -1.1855555 -1.3957777 -1.5319188 -1.8070445 -2.1623545 -2.7093344 -3.2213364][-1.836333 -1.6904118 -1.4939532 -1.4239244 -1.5581052 -1.7497966 -1.7053468 -1.5674794 -1.6393659 -2.0310838 -2.4199946 -2.6561623 -2.8726802 -3.1784854 -3.6705122][-2.3039768 -2.1206179 -2.0129609 -1.8318055 -1.7401962 -1.826339 -1.7758386 -1.7692218 -1.8537631 -2.2221937 -2.5584726 -2.8517621 -3.0203559 -2.9784613 -3.2177265][-2.5139403 -2.2679193 -2.0356884 -1.7660341 -1.6251218 -1.4563704 -1.3282862 -1.4080272 -1.7319934 -2.0269017 -2.233269 -2.53215 -2.6863554 -2.7159424 -3.0515244][-2.4138455 -2.2694273 -2.0564613 -1.5584168 -0.94679689 -0.78551912 -0.78796911 -0.99517584 -1.3652911 -1.6200702 -1.804251 -1.9302044 -2.0269656 -2.0820198 -2.3768556][-2.61635 -2.4468431 -2.3077435 -1.6707685 -0.93646622 -0.49810743 -0.21405172 -0.21620274 -0.2513485 -0.44001245 -0.69530439 -0.96085358 -1.1368294 -1.2918973 -1.8346994][-2.608768 -2.266089 -1.9246278 -1.5002162 -0.83669329 -0.34015512 -0.10749722 -0.012609005 -0.012102127 -0.14005852 -0.34410906 -0.6967802 -0.84483194 -1.1024804 -1.8327043][-2.0226617 -1.9300368 -1.8284831 -1.2403593 -0.53484392 -0.27784348 -0.1597333 -0.099330425 -0.13551331 -0.24775743 -0.43704271 -0.8319521 -1.1454072 -1.4743028 -2.1068237][-2.4424255 -2.1062913 -1.8770561 -1.6521466 -1.2610641 -0.92561293 -0.785696 -0.60892773 -0.38625336 -0.38241911 -0.5860281 -0.86266422 -1.0170112 -1.2811074 -1.8117845][-3.2758296 -3.0991669 -2.9839761 -2.7342463 -2.3777568 -2.2532468 -2.2795839 -2.1929286 -1.916821 -1.6293926 -1.6628802 -1.9322855 -2.0952957 -2.2090511 -2.6044502][-3.5428479 -3.3038614 -3.1442006 -2.9559958 -2.6762114 -2.7109404 -2.916147 -2.8996761 -2.7360916 -2.6400285 -2.6954727 -3.0080419 -3.3232541 -3.3253999 -3.3480952][-4.160223 -3.836612 -3.6094465 -3.5675888 -3.4354188 -3.2331421 -3.1947446 -3.2861605 -3.2685907 -3.1976492 -3.1806641 -3.3927991 -3.5232525 -3.6200271 -3.7136061][-5.4996672 -5.2262268 -5.0972137 -4.9632235 -4.739049 -4.6891093 -4.7669969 -4.7928619 -4.7296262 -4.5225515 -4.3335071 -4.3117437 -4.3487449 -4.3689785 -4.2470546]]...]
INFO - root - 2017-12-15 10:40:39.309180: step 2310, loss = 0.55, batch loss = 0.40 (12.6 examples/sec; 0.637 sec/batch; 58h:25m:14s remains)
INFO - root - 2017-12-15 10:40:45.650865: step 2320, loss = 0.52, batch loss = 0.36 (12.9 examples/sec; 0.618 sec/batch; 56h:41m:44s remains)
INFO - root - 2017-12-15 10:40:52.016273: step 2330, loss = 0.48, batch loss = 0.33 (12.2 examples/sec; 0.654 sec/batch; 60h:00m:41s remains)
INFO - root - 2017-12-15 10:40:58.453872: step 2340, loss = 0.55, batch loss = 0.39 (12.6 examples/sec; 0.633 sec/batch; 58h:03m:20s remains)
INFO - root - 2017-12-15 10:41:04.787266: step 2350, loss = 0.47, batch loss = 0.32 (12.8 examples/sec; 0.627 sec/batch; 57h:31m:03s remains)
INFO - root - 2017-12-15 10:41:11.287487: step 2360, loss = 0.47, batch loss = 0.32 (11.9 examples/sec; 0.671 sec/batch; 61h:31m:54s remains)
INFO - root - 2017-12-15 10:41:17.708397: step 2370, loss = 0.50, batch loss = 0.35 (12.5 examples/sec; 0.638 sec/batch; 58h:30m:12s remains)
INFO - root - 2017-12-15 10:41:24.044450: step 2380, loss = 0.50, batch loss = 0.35 (12.6 examples/sec; 0.637 sec/batch; 58h:23m:30s remains)
INFO - root - 2017-12-15 10:41:30.403264: step 2390, loss = 0.51, batch loss = 0.36 (12.9 examples/sec; 0.620 sec/batch; 56h:49m:05s remains)
INFO - root - 2017-12-15 10:41:36.784171: step 2400, loss = 0.46, batch loss = 0.31 (12.4 examples/sec; 0.646 sec/batch; 59h:16m:24s remains)
2017-12-15 10:41:37.261402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6358566 -2.4333358 -2.3317416 -2.089011 -1.8748679 -1.6711607 -1.3217268 -1.0876245 -0.96733522 -1.2074175 -1.7545092 -2.6887884 -3.4141703 -3.9196887 -4.3628936][-2.5138426 -2.2929597 -1.8818557 -1.6290622 -1.4796796 -1.1913753 -0.91959906 -0.59537411 -0.50178766 -0.68511486 -1.1606565 -2.1241539 -2.9718091 -3.6634874 -4.0831585][-2.4107413 -2.3864055 -1.9304187 -1.513423 -1.2133093 -0.92262745 -0.68494415 -0.34875727 -0.13914728 -0.33323288 -0.9535923 -1.8733196 -2.663095 -3.4060526 -3.9673462][-2.304518 -2.1468322 -1.8802118 -1.4665194 -1.045414 -0.77384567 -0.50422144 -0.35635805 -0.18238401 -0.32682896 -0.8787818 -1.8834243 -2.6045842 -3.3810155 -3.7630084][-1.9036036 -1.7244723 -1.4904156 -1.1615233 -0.88209915 -0.66904688 -0.48304844 -0.27092409 -0.1980114 -0.58217144 -1.1546545 -2.0177484 -2.7200122 -3.3676383 -3.7058344][-1.2108874 -1.1231833 -1.008779 -0.911005 -0.85689116 -0.754364 -0.68578768 -0.44820976 -0.37419176 -0.57231331 -0.95786953 -1.9520781 -2.6512678 -3.3249328 -3.7029467][-1.0821433 -1.1487589 -1.0870252 -1.0145121 -1.0038896 -0.9728756 -0.95106792 -0.83316565 -0.7594986 -0.95775461 -1.3974156 -2.1989143 -2.7898388 -3.5526748 -4.0354929][-0.97215605 -1.0471554 -0.9949069 -1.0238686 -1.0223022 -1.0330057 -1.012857 -0.9443326 -0.87555552 -1.0725307 -1.4593043 -2.2823346 -3.0086763 -3.7209363 -4.10897][-0.95578957 -0.84319735 -0.805696 -0.75773096 -0.64295006 -0.57862616 -0.492908 -0.5204196 -0.47150373 -0.85423613 -1.4493346 -2.2869127 -2.9887333 -3.649816 -4.1192541][-1.6483004 -1.5884163 -1.4630241 -1.3564558 -1.3263521 -1.1895895 -1.1220202 -1.1183209 -1.0450826 -1.4037375 -1.73314 -2.4081802 -3.0528586 -3.6644268 -4.16086][-2.8182678 -2.71418 -2.5494826 -2.4515676 -2.3929861 -2.1972873 -2.0346975 -1.8680911 -1.7086241 -1.824116 -2.1420431 -2.5718045 -2.8657112 -3.3499808 -3.6704278][-3.547096 -3.4511378 -3.3441088 -3.2835245 -3.2864954 -3.1048734 -2.7893054 -2.4263442 -2.140944 -1.9403143 -1.9604132 -2.3246684 -2.6271322 -3.0217457 -3.3351257][-4.3732576 -4.3485775 -4.2253718 -4.1335073 -4.0432029 -3.8315313 -3.516427 -2.9350951 -2.5260267 -2.1911776 -2.0236442 -1.9931796 -2.3499472 -2.6538496 -2.8816109][-5.1339483 -5.2120581 -5.2685204 -5.1830535 -5.0965309 -4.8352032 -4.3855839 -3.8370957 -3.2018893 -2.7605262 -2.3627088 -2.1659651 -2.3083324 -2.3953555 -2.6650643][-5.9539638 -6.1153622 -6.2742362 -6.324687 -6.2592421 -6.0359097 -5.5803719 -5.0663161 -4.4029636 -3.7169189 -3.131156 -2.7844632 -2.5682011 -2.5165441 -2.4907768]]...]
INFO - root - 2017-12-15 10:41:43.703726: step 2410, loss = 0.50, batch loss = 0.34 (12.6 examples/sec; 0.636 sec/batch; 58h:17m:13s remains)
INFO - root - 2017-12-15 10:41:50.035262: step 2420, loss = 0.46, batch loss = 0.30 (12.6 examples/sec; 0.636 sec/batch; 58h:18m:42s remains)
INFO - root - 2017-12-15 10:41:56.418795: step 2430, loss = 0.48, batch loss = 0.32 (12.6 examples/sec; 0.637 sec/batch; 58h:24m:21s remains)
INFO - root - 2017-12-15 10:42:02.893658: step 2440, loss = 0.55, batch loss = 0.40 (12.8 examples/sec; 0.624 sec/batch; 57h:14m:13s remains)
INFO - root - 2017-12-15 10:42:09.403398: step 2450, loss = 0.52, batch loss = 0.37 (12.3 examples/sec; 0.652 sec/batch; 59h:46m:10s remains)
INFO - root - 2017-12-15 10:42:15.779252: step 2460, loss = 0.51, batch loss = 0.36 (13.0 examples/sec; 0.616 sec/batch; 56h:26m:52s remains)
INFO - root - 2017-12-15 10:42:22.204982: step 2470, loss = 0.54, batch loss = 0.39 (12.7 examples/sec; 0.630 sec/batch; 57h:45m:56s remains)
INFO - root - 2017-12-15 10:42:28.575445: step 2480, loss = 0.56, batch loss = 0.41 (12.4 examples/sec; 0.644 sec/batch; 59h:00m:50s remains)
INFO - root - 2017-12-15 10:42:34.963838: step 2490, loss = 0.43, batch loss = 0.28 (12.3 examples/sec; 0.649 sec/batch; 59h:28m:58s remains)
INFO - root - 2017-12-15 10:42:41.416287: step 2500, loss = 0.54, batch loss = 0.39 (12.1 examples/sec; 0.659 sec/batch; 60h:25m:50s remains)
2017-12-15 10:42:41.871562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0062227 -1.2443914 -1.5666804 -1.384583 -0.92893124 -0.4426837 0.13667011 0.69708586 1.2396984 1.3450503 0.6129179 -0.80332327 -1.9931824 -2.9592772 -3.5611691][-1.1148696 -1.4420366 -1.6440511 -1.4859881 -1.0366664 -0.65787888 -0.236907 0.14436865 0.5422287 0.56935215 0.047751904 -1.2369227 -2.3044999 -3.3065264 -4.0129185][-1.4155021 -1.7542372 -1.8621836 -1.6041903 -1.0175037 -0.5248127 -0.055162907 0.19597578 0.46424007 0.44250441 -0.098372936 -1.4565883 -2.6068511 -3.4758177 -4.1485662][-1.5929537 -1.7058344 -1.6537437 -1.3367424 -0.83479977 -0.44153214 -0.09043169 0.035103321 0.24112606 0.10296297 -0.40273428 -1.6636348 -2.828378 -3.6653633 -4.3020554][-1.6071811 -1.6150894 -1.3071761 -0.81756163 -0.38192892 0.020967007 0.28988409 0.27249527 0.38527346 0.30924797 -0.18334198 -1.368145 -2.575253 -3.4519694 -4.1626682][-1.4830079 -1.4211373 -1.0334425 -0.3197217 0.23278666 0.51841021 0.74931908 0.82695627 0.84690142 0.77516842 0.30954218 -0.81081724 -1.9808996 -2.8923028 -3.7056994][-1.5142622 -1.2134881 -0.6933136 0.020895481 0.64490557 1.0149035 1.0979943 1.1884532 1.3711729 1.3452268 0.94861412 -0.098957539 -1.1961679 -2.098068 -2.9073751][-1.6208367 -1.1379442 -0.43200302 0.37935257 0.96722841 1.3452611 1.4430704 1.391922 1.5748448 1.8051138 1.5937657 0.69513178 -0.3145709 -1.2776513 -2.2124448][-1.8209124 -1.2745905 -0.53535271 0.26381159 0.86420012 1.2354512 1.3380747 1.3543983 1.5423293 1.8439808 1.8481841 1.1896505 0.32461309 -0.52786016 -1.4785695][-2.1392906 -1.6752803 -1.0170078 -0.26633024 0.13884687 0.46182632 0.60989 0.68817711 0.92700911 1.3302989 1.5382276 1.1002412 0.50676727 -0.18664169 -0.97101974][-2.5790279 -2.0751822 -1.5202861 -0.91695023 -0.55973434 -0.38013363 -0.29865503 -0.25048923 0.02721405 0.43775225 0.78131056 0.44864035 0.033071518 -0.53435373 -1.1754885][-3.0868142 -2.66951 -2.1403596 -1.7477472 -1.4699445 -1.1517873 -1.0142322 -1.0026374 -0.80794477 -0.57062006 -0.37310362 -0.68419838 -1.0375423 -1.5446377 -1.9305131][-3.3166931 -2.9534018 -2.401144 -2.0443823 -1.8366826 -1.7062984 -1.4728856 -1.2387276 -1.0922656 -1.0100255 -1.0173388 -1.5113635 -1.9221814 -2.3150983 -2.6545875][-3.5885909 -3.0974233 -2.6752057 -2.4190025 -2.1695917 -1.9615855 -1.7204309 -1.5572581 -1.3414421 -1.3531318 -1.4945297 -1.9947901 -2.5684917 -3.1608849 -3.4534194][-3.8911433 -3.3391526 -2.8507159 -2.7194126 -2.6517515 -2.5798335 -2.4086726 -2.2906041 -2.0305772 -1.9755332 -2.128233 -2.6748269 -3.1865675 -3.68718 -4.0779514]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 10:42:49.115743: step 2510, loss = 0.48, batch loss = 0.32 (12.2 examples/sec; 0.654 sec/batch; 59h:56m:42s remains)
INFO - root - 2017-12-15 10:42:55.567739: step 2520, loss = 0.45, batch loss = 0.29 (12.1 examples/sec; 0.663 sec/batch; 60h:47m:13s remains)
INFO - root - 2017-12-15 10:43:01.994914: step 2530, loss = 0.50, batch loss = 0.35 (12.7 examples/sec; 0.632 sec/batch; 57h:56m:56s remains)
INFO - root - 2017-12-15 10:43:08.341426: step 2540, loss = 0.45, batch loss = 0.30 (12.5 examples/sec; 0.641 sec/batch; 58h:44m:05s remains)
INFO - root - 2017-12-15 10:43:14.747770: step 2550, loss = 0.44, batch loss = 0.29 (12.5 examples/sec; 0.640 sec/batch; 58h:38m:30s remains)
INFO - root - 2017-12-15 10:43:21.117506: step 2560, loss = 0.44, batch loss = 0.29 (12.6 examples/sec; 0.636 sec/batch; 58h:19m:58s remains)
INFO - root - 2017-12-15 10:43:27.435261: step 2570, loss = 0.46, batch loss = 0.31 (12.8 examples/sec; 0.624 sec/batch; 57h:10m:50s remains)
INFO - root - 2017-12-15 10:43:33.850075: step 2580, loss = 0.51, batch loss = 0.36 (11.9 examples/sec; 0.673 sec/batch; 61h:38m:28s remains)
INFO - root - 2017-12-15 10:43:40.301199: step 2590, loss = 0.51, batch loss = 0.36 (12.5 examples/sec; 0.638 sec/batch; 58h:30m:43s remains)
INFO - root - 2017-12-15 10:43:46.747364: step 2600, loss = 0.48, batch loss = 0.33 (12.4 examples/sec; 0.645 sec/batch; 59h:07m:11s remains)
2017-12-15 10:43:47.281738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4179885 -2.1704073 -2.1082723 -1.8086615 -1.486176 -1.4672375 -1.5139246 -1.6168838 -1.9500008 -2.3133206 -2.7882338 -3.5515897 -4.0780892 -4.9913063 -5.6358848][-2.784128 -2.5531268 -2.1214128 -1.7551796 -1.2166629 -1.1658721 -1.0100861 -0.782341 -1.0581837 -1.6893401 -2.6164398 -3.5367136 -4.09282 -5.0110288 -5.7943182][-2.6086912 -2.6221538 -1.9909523 -1.581439 -1.0105624 -0.77322674 -0.47053385 -0.23213387 -0.4470787 -1.0374837 -1.9090335 -3.002249 -3.907897 -4.9169245 -5.5671077][-2.3865385 -2.1826329 -1.4317875 -0.87756824 -0.052269459 -0.1405735 -0.094154835 -0.14575052 -0.37721205 -0.69476795 -1.5637336 -2.7235858 -3.6775787 -4.5327263 -5.3486028][-1.8728569 -1.2535806 -0.44934082 0.33218956 1.0040646 1.089994 1.006566 0.43974495 -0.19442225 -0.86516762 -1.5176077 -2.3409324 -3.2440155 -4.0819874 -4.8836117][-1.6956062 -0.59841394 0.42871475 1.2645421 2.2127886 2.5225878 2.0962834 1.6876855 0.90712023 -0.096390724 -1.0181251 -2.0760226 -2.9559026 -3.7349169 -4.4690471][-0.82499743 0.035562515 1.3258843 2.4885759 3.2133484 3.1175909 3.0295181 2.58502 1.7385011 0.86704016 -0.19690847 -1.5542464 -2.5757122 -3.4943979 -4.2624035][-0.19866323 1.1844735 1.9358253 2.7576475 3.2982178 3.4615755 3.2118969 2.3775206 1.9047103 1.1535606 0.37044621 -0.76251078 -1.9685037 -3.127655 -3.9802895][0.22399712 1.1973009 2.3241911 3.195014 3.4444733 3.0352392 2.7674694 2.5667038 1.9395428 1.0527987 0.33922911 -0.58941126 -1.5710311 -2.7044222 -3.8361359][-0.51201534 0.46753311 1.4605956 2.1902909 2.807929 2.4833889 2.1894817 1.5405793 1.0594511 0.48148012 -0.4063797 -1.3001795 -1.988436 -2.921438 -3.70514][-1.4909916 -0.725801 0.1535368 1.085134 1.5976496 1.4794903 1.1909056 0.70449114 0.1701746 -0.67820311 -1.5238194 -2.3298235 -2.9617147 -3.4087367 -3.99163][-2.5827234 -1.9217057 -1.1203098 -0.62432814 -0.48867702 -0.84168863 -1.0560832 -1.2465758 -1.6515613 -2.142195 -2.8752055 -3.3595803 -3.8873534 -4.190794 -4.5672708][-2.8965061 -2.5373609 -2.0467539 -2.057512 -2.0735228 -2.4053125 -2.6851041 -3.0398376 -3.4461741 -3.6650858 -4.1516118 -4.1905355 -4.5186806 -4.6270466 -4.7488585][-3.575202 -3.5371244 -3.5627661 -3.4650798 -3.2212687 -3.5143535 -3.7067988 -3.9715261 -4.4457951 -4.5941229 -5.0460296 -5.11102 -5.2804437 -5.1764507 -5.0459495][-4.1499252 -4.2150221 -3.8718419 -3.9532871 -3.9581594 -4.3340425 -4.3686156 -4.510355 -4.8495574 -5.2024879 -5.7400908 -5.6746421 -5.6599431 -5.6180758 -5.3527946]]...]
INFO - root - 2017-12-15 10:43:53.710010: step 2610, loss = 0.52, batch loss = 0.37 (12.4 examples/sec; 0.643 sec/batch; 58h:53m:16s remains)
INFO - root - 2017-12-15 10:44:00.092519: step 2620, loss = 0.53, batch loss = 0.38 (12.3 examples/sec; 0.649 sec/batch; 59h:27m:29s remains)
INFO - root - 2017-12-15 10:44:06.608191: step 2630, loss = 0.46, batch loss = 0.31 (13.1 examples/sec; 0.609 sec/batch; 55h:45m:57s remains)
INFO - root - 2017-12-15 10:44:13.013293: step 2640, loss = 0.49, batch loss = 0.34 (12.1 examples/sec; 0.664 sec/batch; 60h:48m:12s remains)
INFO - root - 2017-12-15 10:44:19.475365: step 2650, loss = 0.49, batch loss = 0.34 (12.5 examples/sec; 0.641 sec/batch; 58h:42m:08s remains)
INFO - root - 2017-12-15 10:44:25.847742: step 2660, loss = 0.51, batch loss = 0.36 (13.0 examples/sec; 0.618 sec/batch; 56h:34m:49s remains)
INFO - root - 2017-12-15 10:44:32.222626: step 2670, loss = 0.49, batch loss = 0.34 (12.1 examples/sec; 0.663 sec/batch; 60h:43m:33s remains)
INFO - root - 2017-12-15 10:44:38.647542: step 2680, loss = 0.49, batch loss = 0.34 (12.4 examples/sec; 0.645 sec/batch; 59h:06m:32s remains)
INFO - root - 2017-12-15 10:44:45.049642: step 2690, loss = 0.51, batch loss = 0.36 (13.0 examples/sec; 0.617 sec/batch; 56h:32m:34s remains)
INFO - root - 2017-12-15 10:44:51.358153: step 2700, loss = 0.51, batch loss = 0.36 (12.7 examples/sec; 0.628 sec/batch; 57h:33m:37s remains)
2017-12-15 10:44:51.844902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6662683 -4.316864 -4.0343342 -3.8602252 -4.0011964 -4.11039 -4.0426512 -3.53301 -2.8543289 -2.7712433 -3.1158376 -4.0893517 -4.7799621 -5.2946925 -5.8655939][-4.290976 -3.9419465 -3.6911955 -3.5576646 -3.6025434 -3.6533964 -3.466186 -3.0720432 -2.6209371 -2.426712 -2.6608951 -3.492856 -4.2104731 -5.022779 -5.7952886][-4.1650176 -3.7479062 -3.2737286 -2.9182875 -2.7427356 -2.7200015 -2.5838714 -2.1153984 -1.6147671 -1.4789677 -1.8928308 -2.7882788 -3.5722532 -4.518795 -5.6317329][-4.0347648 -3.6773927 -3.045155 -2.2917428 -1.5929875 -1.3315473 -1.055542 -0.6515708 -0.23539972 -0.16694593 -0.71947 -1.8527825 -2.9289157 -4.2066574 -5.2993975][-2.7007165 -2.3466396 -1.7475343 -0.92535114 -0.2532115 0.33816338 1.0094619 1.3914447 1.6755862 1.4162836 0.49022865 -1.0598626 -2.4952056 -3.7610526 -4.85045][-1.8782697 -1.2496324 -0.82997704 -0.27810907 0.40750456 1.0637732 1.6155648 2.11722 2.3417578 1.811451 0.56889391 -1.1245322 -2.4188225 -3.7065601 -4.6642818][-1.3995566 -0.8651433 -0.19748497 0.5565691 1.2328858 1.6036863 2.2122545 2.8212872 3.2602358 2.5942907 1.3260279 -0.66635942 -2.2875402 -3.8038287 -4.8853135][-1.5009212 -0.84841251 -0.049256325 0.75372028 1.4818506 1.9469829 2.445549 2.8644524 3.1450891 2.6250715 1.7735639 0.18264103 -1.4158735 -3.1467733 -4.4457216][-1.7878666 -1.1421227 -0.48411751 0.17783642 0.92576647 1.6396461 2.1908107 2.5223341 2.8853102 2.3384237 1.1252637 -0.42764091 -1.7970638 -3.0148959 -3.9728801][-3.2009308 -2.9563088 -2.4700525 -1.7378182 -1.1316056 -0.52615881 0.085447788 0.72952795 1.0590181 0.76385927 0.052802563 -1.0987139 -2.3204825 -3.4672725 -4.3545866][-3.8681543 -3.7514143 -3.4525568 -3.0527725 -2.5483458 -1.8603902 -1.3509116 -0.73330832 -0.46558666 -0.2896204 -0.44665766 -1.5506563 -2.5519853 -3.9035482 -4.9229288][-5.020164 -4.8345017 -4.5200343 -4.3004789 -4.054184 -3.906944 -3.6496112 -3.3149226 -2.6081369 -2.1326754 -2.1093814 -2.7691872 -3.5613539 -4.5012164 -5.3371925][-5.6060543 -5.5560708 -5.3575745 -5.1402478 -4.8828139 -4.7089195 -4.483695 -4.4415364 -4.2243037 -3.8285527 -3.5630879 -3.569792 -3.9755931 -4.7242494 -5.4570608][-5.6808534 -5.5717349 -5.533349 -5.4935994 -5.443284 -5.2213049 -4.9422579 -4.7641068 -4.6416826 -4.7197428 -4.6995616 -4.5326934 -4.4356489 -4.6412659 -4.9427929][-6.2850804 -6.2166672 -6.1890192 -6.1911373 -6.1789703 -6.0719614 -5.9411378 -5.7392144 -5.5088105 -5.2977219 -5.3359113 -5.5132651 -5.5564895 -5.5727611 -5.5105047]]...]
INFO - root - 2017-12-15 10:44:58.275761: step 2710, loss = 0.48, batch loss = 0.33 (12.7 examples/sec; 0.631 sec/batch; 57h:47m:57s remains)
INFO - root - 2017-12-15 10:45:04.698906: step 2720, loss = 0.46, batch loss = 0.31 (12.0 examples/sec; 0.667 sec/batch; 61h:07m:21s remains)
INFO - root - 2017-12-15 10:45:11.110031: step 2730, loss = 0.42, batch loss = 0.27 (12.8 examples/sec; 0.626 sec/batch; 57h:20m:20s remains)
INFO - root - 2017-12-15 10:45:17.493670: step 2740, loss = 0.54, batch loss = 0.39 (12.1 examples/sec; 0.664 sec/batch; 60h:47m:51s remains)
INFO - root - 2017-12-15 10:45:23.878817: step 2750, loss = 0.42, batch loss = 0.27 (12.4 examples/sec; 0.644 sec/batch; 58h:57m:41s remains)
INFO - root - 2017-12-15 10:45:30.272469: step 2760, loss = 0.50, batch loss = 0.35 (12.5 examples/sec; 0.638 sec/batch; 58h:26m:36s remains)
INFO - root - 2017-12-15 10:45:36.802418: step 2770, loss = 0.49, batch loss = 0.34 (12.4 examples/sec; 0.645 sec/batch; 59h:04m:54s remains)
INFO - root - 2017-12-15 10:45:43.290112: step 2780, loss = 0.53, batch loss = 0.38 (12.5 examples/sec; 0.638 sec/batch; 58h:27m:42s remains)
INFO - root - 2017-12-15 10:45:49.685072: step 2790, loss = 0.48, batch loss = 0.33 (12.7 examples/sec; 0.629 sec/batch; 57h:36m:24s remains)
INFO - root - 2017-12-15 10:45:56.133912: step 2800, loss = 0.48, batch loss = 0.33 (12.0 examples/sec; 0.669 sec/batch; 61h:18m:33s remains)
2017-12-15 10:45:56.608964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.70623255 -0.42268133 -0.18936443 0.10736322 0.49333906 0.84349108 1.2394433 1.384757 1.476717 1.1508021 0.49576473 -0.867085 -2.2163858 -3.407109 -4.4645858][-0.95403719 -0.75008106 -0.51894331 -0.23661566 0.1080761 0.46570015 0.8802228 1.2576971 1.5839024 1.3998528 0.88794041 -0.52004337 -1.9364834 -3.2578204 -4.436419][-1.0963907 -0.8279624 -0.53368378 -0.29331255 0.092708111 0.43420124 0.8580184 1.2128143 1.4939094 1.3260536 0.83154917 -0.57643414 -1.9714694 -3.2575071 -4.4127707][-0.71604824 -0.43323421 -0.1354723 0.15706348 0.54532528 0.85108662 1.2174029 1.541882 1.7771282 1.5592756 1.0040827 -0.45305872 -1.8802888 -3.1878819 -4.3675766][-0.23179293 0.0933857 0.40395021 0.66527843 1.012887 1.2884617 1.6376514 1.9596486 2.1766515 1.8957229 1.2833104 -0.20659018 -1.6542273 -2.9783571 -4.1398454][0.14335442 0.54658318 0.88033772 1.2265725 1.6232047 1.9015894 2.2267184 2.5276494 2.7101846 2.3511472 1.6489387 0.051132679 -1.4698429 -2.8190353 -3.9648256][0.30398321 0.68654871 1.0693088 1.4306583 1.8575501 2.2148366 2.5826488 2.8743367 3.0368018 2.6781812 1.9783654 0.37488604 -1.1850595 -2.6172791 -3.8154995][0.28033209 0.6108489 0.92711878 1.3594327 1.7779913 2.1890864 2.6507049 3.0143256 3.1836667 2.8304944 2.13172 0.54295349 -1.0170898 -2.427628 -3.64903][0.13145113 0.37986994 0.60109949 0.98975468 1.3946738 1.8389297 2.3567119 2.827745 3.1183333 2.8096108 2.1335816 0.54998446 -1.0169897 -2.4499445 -3.6587541][-0.56914568 -0.38609076 -0.22560263 0.088016033 0.50830841 0.91573524 1.3963699 1.8191895 2.1329122 1.8959494 1.3139939 -0.1224618 -1.5560417 -2.8472741 -3.9235902][-1.4713697 -1.4421034 -1.328578 -1.1318922 -0.80521107 -0.50706148 -0.11778545 0.22358704 0.494442 0.28299093 -0.20642471 -1.4399862 -2.6352561 -3.6438243 -4.4905286][-2.49034 -2.420104 -2.3835821 -2.3298404 -2.2064939 -2.0324161 -1.7682157 -1.5259562 -1.2995286 -1.3991446 -1.7085891 -2.6079462 -3.495604 -4.2526827 -4.9236064][-4.4289622 -4.3697352 -4.3017826 -4.2314134 -4.2138858 -4.1882038 -4.0841274 -3.9943962 -3.8425269 -3.8983769 -4.0433912 -4.4559546 -4.8797526 -5.1683135 -5.38567][-5.7578 -5.6694589 -5.5526719 -5.5159764 -5.489634 -5.5411434 -5.5436697 -5.55569 -5.4821043 -5.5592937 -5.6256528 -5.7805963 -5.8538876 -5.8666959 -5.8258181][-6.6905179 -6.5923829 -6.4680109 -6.420186 -6.3850851 -6.4934678 -6.5723033 -6.6252575 -6.6097984 -6.6824784 -6.676558 -6.5730934 -6.4151425 -6.212193 -5.9507151]]...]
INFO - root - 2017-12-15 10:46:02.974134: step 2810, loss = 0.47, batch loss = 0.32 (12.7 examples/sec; 0.631 sec/batch; 57h:44m:44s remains)
INFO - root - 2017-12-15 10:46:09.480520: step 2820, loss = 0.51, batch loss = 0.36 (12.2 examples/sec; 0.655 sec/batch; 59h:58m:27s remains)
INFO - root - 2017-12-15 10:46:15.861599: step 2830, loss = 0.47, batch loss = 0.32 (12.4 examples/sec; 0.646 sec/batch; 59h:09m:59s remains)
INFO - root - 2017-12-15 10:46:22.166788: step 2840, loss = 0.49, batch loss = 0.34 (13.0 examples/sec; 0.615 sec/batch; 56h:18m:10s remains)
INFO - root - 2017-12-15 10:46:28.570838: step 2850, loss = 0.47, batch loss = 0.32 (12.8 examples/sec; 0.625 sec/batch; 57h:16m:20s remains)
INFO - root - 2017-12-15 10:46:34.878533: step 2860, loss = 0.48, batch loss = 0.34 (12.5 examples/sec; 0.640 sec/batch; 58h:35m:16s remains)
INFO - root - 2017-12-15 10:46:41.299109: step 2870, loss = 0.50, batch loss = 0.35 (12.8 examples/sec; 0.626 sec/batch; 57h:16m:46s remains)
INFO - root - 2017-12-15 10:46:47.700471: step 2880, loss = 0.50, batch loss = 0.36 (12.8 examples/sec; 0.625 sec/batch; 57h:15m:57s remains)
INFO - root - 2017-12-15 10:46:54.142244: step 2890, loss = 0.47, batch loss = 0.32 (12.5 examples/sec; 0.640 sec/batch; 58h:35m:27s remains)
INFO - root - 2017-12-15 10:47:00.549897: step 2900, loss = 0.46, batch loss = 0.31 (12.7 examples/sec; 0.632 sec/batch; 57h:49m:15s remains)
2017-12-15 10:47:01.041819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82921982 -1.0011015 -0.88925743 -0.40130758 0.082724571 0.49800777 0.81422281 0.73055887 0.5235157 -0.22281075 -1.1558857 -2.6122437 -3.8482258 -4.7936282 -5.7182851][-1.2928605 -1.2913723 -1.0712729 -0.69974518 -0.11088181 0.25080538 0.32391548 0.28036308 -0.011433601 -0.780622 -1.6060514 -2.9811726 -4.0544276 -4.9238381 -5.7184048][-1.846622 -1.9079103 -1.5165343 -0.74042845 0.043932438 0.61742258 0.92478514 0.76157236 0.35742664 -0.572875 -1.5558019 -2.8944473 -3.9167197 -4.6296406 -5.3254519][-1.8881178 -1.7019477 -1.2222443 -0.47571993 0.41676569 1.0648732 1.366785 1.3036685 0.82847261 -0.11658955 -1.1056104 -2.5416312 -3.6494749 -4.4764376 -5.1390696][-1.526576 -1.2845259 -0.81706142 -0.069930553 0.79661131 1.5253468 1.9429874 1.7981386 1.3317118 0.35600138 -0.76172733 -2.2194204 -3.4054601 -4.2629185 -4.9935136][-1.3335347 -0.88598347 -0.36787033 0.29174376 1.0996404 1.7058887 1.9910259 1.9295454 1.5379138 0.547977 -0.51153612 -1.8738561 -2.9728179 -3.7485192 -4.437252][-0.7733016 -0.45125961 -0.075301647 0.52647877 1.2335806 1.888483 2.265676 2.1107717 1.6260557 0.69943094 -0.31623936 -1.6352324 -2.6637437 -3.4849904 -4.2254424][-0.40435266 -0.1401329 0.15126276 0.61412573 1.275598 1.8859901 2.19739 2.1086802 1.7397451 0.81974268 -0.16383886 -1.483253 -2.577672 -3.435606 -4.1786833][-0.0676465 0.10067368 0.29203987 0.7680254 1.3359737 1.9234414 2.3041091 2.2292466 1.815722 0.9860754 -0.0048828125 -1.2880921 -2.3743269 -3.2850645 -4.040082][-0.19489145 -0.060373306 0.11131334 0.44239187 0.90514088 1.4822497 1.8079581 1.8455772 1.5685577 0.73114967 -0.19492197 -1.4138074 -2.4439118 -3.2251804 -3.932709][-0.97362804 -0.98202038 -0.9033947 -0.62701845 -0.29158592 0.11386824 0.47380781 0.54554796 0.37258673 -0.21099138 -0.96544218 -1.9713583 -2.8271151 -3.4263508 -3.9513469][-1.9836681 -2.0361309 -2.0237799 -1.9343071 -1.718255 -1.3935385 -1.0999374 -0.96635103 -0.9902215 -1.332386 -1.8423634 -2.5996969 -3.252218 -3.7948253 -4.2611113][-3.0653579 -3.0679808 -3.0470631 -3.0127716 -2.8893688 -2.7034411 -2.5298676 -2.3653204 -2.2780817 -2.4055479 -2.7240698 -3.3013914 -3.7987008 -4.1904192 -4.5590949][-4.3314915 -4.2455249 -4.1643357 -4.1318617 -4.053834 -3.9676929 -3.8602858 -3.7381124 -3.6464343 -3.7181041 -3.897718 -4.099102 -4.3089142 -4.5478597 -4.7749362][-5.7991915 -5.6785512 -5.5718946 -5.5692835 -5.5186453 -5.4895191 -5.444088 -5.3201761 -5.1625652 -5.10818 -5.1015372 -5.0926042 -5.0732632 -5.0212355 -5.0108404]]...]
INFO - root - 2017-12-15 10:47:07.418871: step 2910, loss = 0.49, batch loss = 0.34 (12.1 examples/sec; 0.658 sec/batch; 60h:17m:06s remains)
INFO - root - 2017-12-15 10:47:13.831389: step 2920, loss = 0.43, batch loss = 0.28 (12.5 examples/sec; 0.640 sec/batch; 58h:36m:36s remains)
INFO - root - 2017-12-15 10:47:20.289423: step 2930, loss = 0.50, batch loss = 0.35 (12.7 examples/sec; 0.630 sec/batch; 57h:39m:19s remains)
INFO - root - 2017-12-15 10:47:26.678071: step 2940, loss = 0.46, batch loss = 0.31 (12.7 examples/sec; 0.632 sec/batch; 57h:51m:39s remains)
INFO - root - 2017-12-15 10:47:33.106308: step 2950, loss = 0.67, batch loss = 0.53 (12.6 examples/sec; 0.633 sec/batch; 57h:54m:42s remains)
INFO - root - 2017-12-15 10:47:39.426878: step 2960, loss = 0.55, batch loss = 0.40 (12.6 examples/sec; 0.636 sec/batch; 58h:14m:21s remains)
INFO - root - 2017-12-15 10:47:45.870381: step 2970, loss = 0.44, batch loss = 0.30 (12.5 examples/sec; 0.641 sec/batch; 58h:39m:52s remains)
INFO - root - 2017-12-15 10:47:52.217213: step 2980, loss = 0.46, batch loss = 0.32 (12.2 examples/sec; 0.654 sec/batch; 59h:49m:53s remains)
INFO - root - 2017-12-15 10:47:58.583059: step 2990, loss = 0.46, batch loss = 0.31 (12.9 examples/sec; 0.621 sec/batch; 56h:51m:08s remains)
INFO - root - 2017-12-15 10:48:04.923400: step 3000, loss = 0.47, batch loss = 0.33 (12.4 examples/sec; 0.643 sec/batch; 58h:50m:33s remains)
2017-12-15 10:48:05.438804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.84462404 -1.5262599 -1.9315758 -2.2220969 -2.4027474 -2.3721907 -2.2839334 -2.280467 -1.9350448 -1.9336166 -2.3773425 -3.1954539 -4.0837965 -4.94843 -5.4244165][-1.2454114 -2.082408 -2.5031955 -2.7673609 -2.7449892 -2.5977855 -2.3326626 -1.9488511 -1.6812901 -2.1268318 -2.4755731 -3.2175639 -4.0930634 -5.2233725 -6.0424523][-1.6535635 -2.2792943 -2.8340292 -3.2228675 -3.3642082 -2.8439937 -2.2639463 -1.905972 -1.4564075 -1.5560498 -2.0407093 -2.9362221 -4.0490413 -5.1554375 -6.1005874][-1.739378 -2.3697805 -2.8012743 -3.1150415 -3.1767035 -2.814059 -2.1497743 -1.4781995 -0.8624239 -1.115139 -1.469295 -2.3292987 -3.2549841 -4.5667286 -5.5747085][-1.7506447 -2.427846 -2.7151971 -2.8604267 -2.7872884 -2.4195213 -1.8501163 -0.99234724 -0.2011528 -0.58174086 -1.3073535 -2.4143851 -3.3865888 -4.3793392 -5.2499547][-1.6685495 -2.0853524 -2.403887 -2.4935243 -2.1005976 -1.5300512 -1.1795607 -0.42054844 0.36889076 -0.0447073 -0.83849096 -1.9886208 -3.1884804 -4.4959493 -5.202044][-1.2140727 -1.5289764 -1.7419915 -1.5006986 -0.93819046 -0.39432669 -0.10804367 0.079923153 0.57483959 0.38971949 -0.3617959 -1.8095112 -3.1302836 -4.3860626 -5.2605057][-1.1506357 -1.0054383 -1.1076312 -0.92632341 -0.16372013 0.63709593 0.85834551 1.069531 1.2391934 0.54519129 -0.40475225 -1.8117356 -3.241291 -4.4963603 -5.1417141][-1.0633821 -0.86557674 -0.693748 -0.23307133 0.42908621 1.038528 1.5752158 1.732357 1.6478181 0.54494143 -0.65744543 -2.1446652 -3.4477983 -4.5469074 -5.2597008][-1.4729123 -1.3549027 -1.0891705 -0.90275431 -0.48531389 0.087402344 0.40949917 0.53608894 0.6462965 -0.11911154 -1.2480764 -2.5818334 -3.698631 -4.6820107 -5.2226634][-1.5357757 -1.6557336 -1.7838368 -1.6865149 -1.4654269 -1.1699357 -1.023088 -1.0862694 -1.2196665 -1.690793 -2.2992814 -3.3047273 -4.2301035 -4.8987412 -5.4119892][-2.3142321 -2.4419637 -2.6910248 -2.898288 -2.9521718 -2.8714759 -2.9365404 -3.0272455 -2.9504111 -3.2407224 -3.6845131 -4.3348565 -4.9550776 -5.489758 -5.73986][-3.6950307 -3.7180068 -3.9859107 -4.261096 -4.4980011 -4.665607 -4.8750248 -5.1264353 -5.0546565 -5.1061778 -5.1870465 -5.5203671 -5.7244005 -5.9712596 -6.1098309][-5.2308741 -5.1668868 -5.2762318 -5.3951588 -5.4163389 -5.5886788 -5.8280263 -6.2093863 -6.3395677 -6.4009266 -6.3509321 -6.249485 -6.1837306 -6.1687117 -6.1249285][-6.1245551 -6.1429939 -6.3601727 -6.4753561 -6.5167937 -6.5694971 -6.534997 -6.6573834 -6.7421994 -6.7817674 -6.692605 -6.5532537 -6.2800636 -6.0811439 -5.7141609]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:48:11.853177: step 3010, loss = 0.56, batch loss = 0.41 (12.8 examples/sec; 0.626 sec/batch; 57h:16m:42s remains)
INFO - root - 2017-12-15 10:48:18.263525: step 3020, loss = 0.54, batch loss = 0.40 (12.6 examples/sec; 0.635 sec/batch; 58h:04m:19s remains)
INFO - root - 2017-12-15 10:48:24.722895: step 3030, loss = 0.43, batch loss = 0.28 (12.0 examples/sec; 0.667 sec/batch; 61h:03m:55s remains)
INFO - root - 2017-12-15 10:48:31.055458: step 3040, loss = 0.39, batch loss = 0.24 (12.7 examples/sec; 0.632 sec/batch; 57h:47m:34s remains)
INFO - root - 2017-12-15 10:48:37.464655: step 3050, loss = 0.44, batch loss = 0.29 (12.5 examples/sec; 0.642 sec/batch; 58h:46m:27s remains)
INFO - root - 2017-12-15 10:48:43.819804: step 3060, loss = 0.46, batch loss = 0.31 (12.4 examples/sec; 0.647 sec/batch; 59h:11m:52s remains)
INFO - root - 2017-12-15 10:48:50.231575: step 3070, loss = 0.41, batch loss = 0.26 (12.6 examples/sec; 0.635 sec/batch; 58h:05m:59s remains)
INFO - root - 2017-12-15 10:48:56.657004: step 3080, loss = 0.48, batch loss = 0.34 (12.5 examples/sec; 0.639 sec/batch; 58h:30m:08s remains)
INFO - root - 2017-12-15 10:49:03.062899: step 3090, loss = 0.46, batch loss = 0.31 (12.8 examples/sec; 0.627 sec/batch; 57h:19m:39s remains)
INFO - root - 2017-12-15 10:49:09.441969: step 3100, loss = 0.48, batch loss = 0.33 (12.4 examples/sec; 0.644 sec/batch; 58h:54m:26s remains)
2017-12-15 10:49:09.884972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8939471 -2.21574 -2.6201742 -2.7375486 -2.6367543 -2.3545384 -2.0718222 -2.0231218 -1.9621887 -1.9870481 -2.2567992 -2.3783143 -2.3976991 -2.8184326 -3.3074648][-2.5603783 -2.5060668 -2.532654 -2.6392717 -2.6426048 -2.5381517 -2.3577821 -2.081315 -1.840888 -2.0515966 -2.2686284 -2.5894628 -2.8012486 -3.2514434 -3.6313975][-2.0621333 -2.1664543 -2.3255138 -2.3253036 -2.2252624 -2.0938387 -1.9904256 -1.82656 -1.6087065 -1.8292875 -2.0984476 -2.5268095 -2.8651268 -3.2088244 -3.5876122][-1.8328052 -1.7422495 -1.9184918 -2.0308647 -2.0501747 -1.8837609 -1.6213217 -1.4650431 -1.3414407 -1.7481146 -2.1114261 -2.6195204 -3.0624173 -3.4942825 -3.8929517][-2.594897 -2.1993616 -1.9473524 -1.8747578 -1.8344593 -1.6224694 -1.4295707 -1.348124 -1.4017053 -1.8338466 -2.2854192 -2.9223132 -3.3653431 -3.8556879 -4.1322722][-2.1033075 -1.6883645 -1.4543285 -1.4218674 -1.3146029 -1.1552925 -1.0816555 -1.1229434 -1.2202339 -1.8850245 -2.4456224 -3.047425 -3.4278378 -3.7337596 -3.9289486][-2.2614031 -1.8712721 -1.6395626 -1.3208694 -1.0708256 -0.90599203 -0.76807737 -1.0811338 -1.4043236 -2.1001844 -2.6335008 -3.3535266 -3.90451 -4.322772 -4.5227375][-2.1722438 -1.8705106 -1.6699495 -1.4905744 -1.346715 -1.1298237 -0.92558956 -1.1046381 -1.3719406 -2.1914535 -2.8529339 -3.6797161 -4.2761288 -4.7169304 -4.9141893][-2.0162444 -1.6414514 -1.384716 -1.3543272 -1.3012347 -1.1009045 -1.0139217 -1.1210661 -1.241601 -2.0136552 -2.7466779 -3.5784128 -4.1237803 -4.6647711 -4.890389][-2.4557815 -2.1662593 -2.0995758 -2.0875459 -2.0193696 -1.8189397 -1.7279797 -1.7680073 -1.8668165 -2.3797536 -2.9569716 -3.8045857 -4.401288 -4.8217282 -5.0257111][-3.2803602 -3.0219414 -2.9756932 -2.8390315 -2.7566798 -2.7431257 -2.7097836 -2.8168626 -2.9160805 -3.249074 -3.6027186 -4.176878 -4.630455 -5.0181589 -5.3596706][-4.1985073 -3.9645586 -3.789721 -3.6466658 -3.6272345 -3.5389965 -3.5016878 -3.5879803 -3.7196956 -4.0179372 -4.1848254 -4.5724373 -4.7934647 -5.1027493 -5.2915797][-4.7950225 -4.5507617 -4.3466177 -4.1773653 -4.0663829 -4.0836945 -4.2557268 -4.4005709 -4.5089693 -4.65438 -4.7481265 -4.9964218 -5.2121148 -5.4172325 -5.57443][-4.9213181 -4.696774 -4.5674171 -4.449492 -4.3782272 -4.4200354 -4.5305367 -4.6944294 -4.8206253 -4.974606 -5.0913777 -5.2771597 -5.5013504 -5.6784291 -5.833786][-5.5657969 -5.4340715 -5.3932056 -5.3606048 -5.3589225 -5.3074393 -5.2860241 -5.4357414 -5.5424891 -5.6469254 -5.7818809 -5.9135141 -5.9863787 -6.0286927 -6.1221795]]...]
INFO - root - 2017-12-15 10:49:16.239682: step 3110, loss = 0.46, batch loss = 0.32 (12.6 examples/sec; 0.635 sec/batch; 58h:04m:27s remains)
INFO - root - 2017-12-15 10:49:22.580083: step 3120, loss = 0.47, batch loss = 0.33 (12.4 examples/sec; 0.644 sec/batch; 58h:54m:00s remains)
INFO - root - 2017-12-15 10:49:28.932842: step 3130, loss = 0.50, batch loss = 0.36 (12.6 examples/sec; 0.637 sec/batch; 58h:14m:05s remains)
INFO - root - 2017-12-15 10:49:35.375822: step 3140, loss = 0.42, batch loss = 0.28 (12.8 examples/sec; 0.625 sec/batch; 57h:09m:43s remains)
INFO - root - 2017-12-15 10:49:41.716147: step 3150, loss = 0.43, batch loss = 0.29 (13.0 examples/sec; 0.617 sec/batch; 56h:24m:14s remains)
INFO - root - 2017-12-15 10:49:48.172913: step 3160, loss = 0.45, batch loss = 0.31 (12.8 examples/sec; 0.623 sec/batch; 56h:57m:22s remains)
INFO - root - 2017-12-15 10:49:54.524354: step 3170, loss = 0.48, batch loss = 0.34 (12.8 examples/sec; 0.627 sec/batch; 57h:22m:08s remains)
INFO - root - 2017-12-15 10:50:00.849682: step 3180, loss = 0.45, batch loss = 0.31 (12.6 examples/sec; 0.634 sec/batch; 57h:57m:17s remains)
INFO - root - 2017-12-15 10:50:07.260143: step 3190, loss = 0.51, batch loss = 0.37 (12.6 examples/sec; 0.633 sec/batch; 57h:53m:08s remains)
INFO - root - 2017-12-15 10:50:13.559457: step 3200, loss = 0.48, batch loss = 0.34 (12.7 examples/sec; 0.630 sec/batch; 57h:38m:47s remains)
2017-12-15 10:50:14.123104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7562127 -3.8092704 -3.8627625 -3.9136946 -3.9192157 -4.0038714 -3.9121766 -3.7382 -3.4715431 -3.841018 -4.4179993 -5.0215163 -5.483336 -5.7513833 -5.7519608][-3.3776224 -3.4547081 -3.5583889 -3.5768988 -3.6558259 -3.5693069 -3.4786844 -3.5551867 -3.5895877 -3.9329417 -4.3781986 -5.0039864 -5.3816996 -5.6218734 -5.7627439][-3.1578288 -2.8959329 -2.7562425 -2.7972648 -2.8251872 -2.8269284 -3.0061443 -3.0117867 -3.0510292 -3.4637229 -3.9645686 -4.6359706 -5.0871882 -5.3362875 -5.5315709][-2.7559564 -2.5058782 -2.0978432 -1.8272696 -1.9304953 -2.0406117 -2.1245565 -2.3737257 -2.6326306 -3.1896241 -4.0024948 -4.5239091 -4.8217039 -5.073422 -5.2273254][-2.0783505 -1.6216779 -1.1800132 -1.1140304 -1.0620818 -1.2363234 -1.5630035 -1.9647079 -2.3036165 -2.9286902 -3.5125062 -4.2378349 -4.8494353 -5.0972047 -5.172236][-1.9063787 -1.3344307 -0.81790829 -0.51217222 -0.46712828 -0.715302 -0.97572756 -1.4382119 -1.8451829 -2.5858629 -3.3083317 -4.0248594 -4.4827924 -4.7312007 -4.8585062][-1.3774309 -0.89938593 -0.62214947 -0.25437784 -0.22432041 -0.39436054 -0.68156052 -1.0661974 -1.4630547 -2.2457581 -3.1012611 -3.9151406 -4.2509661 -4.4942374 -4.7270384][-1.3662682 -1.0522861 -0.687305 -0.567739 -0.52538109 -0.6833992 -0.87537813 -1.2211947 -1.5159025 -2.0473151 -2.7605472 -3.5444453 -4.0347624 -4.1947269 -4.2117791][-1.8762231 -1.5603023 -1.2172174 -1.2968884 -1.2336249 -1.0681458 -0.93835783 -1.1326904 -1.3833876 -2.1476412 -2.8471258 -3.4877515 -3.8530674 -4.1391845 -4.2956][-2.5300169 -2.2260535 -1.9445705 -1.779624 -1.55688 -1.5524907 -1.5859098 -1.6251945 -1.821034 -2.0983167 -2.558027 -3.2559702 -3.8253071 -4.15957 -4.2079291][-3.0372524 -2.5649991 -2.2075095 -2.1089921 -1.9215093 -1.6318283 -1.5449548 -1.6178865 -1.5861897 -1.8483639 -2.4993968 -3.1698422 -3.69742 -3.8523507 -4.0926185][-3.7234924 -2.96195 -2.4079368 -2.1945438 -2.0976272 -1.9343038 -1.8068037 -1.6211276 -1.4173803 -1.7472501 -2.1046371 -2.6537039 -3.2129841 -3.7425623 -4.1767921][-4.1113253 -3.686784 -3.1223338 -2.6729031 -2.5162642 -2.3689189 -2.3126707 -2.3407428 -2.2930856 -2.3880329 -2.3672259 -2.578989 -2.8374505 -3.2599497 -3.5242715][-4.6077795 -3.9899895 -3.1864257 -2.8418934 -2.6864338 -2.6043882 -2.6599865 -2.7357244 -2.7181017 -2.6381676 -2.7562151 -2.9103317 -3.0941219 -3.2683592 -3.4688182][-4.9006195 -4.1741056 -3.4857273 -3.1192713 -2.8377836 -2.83893 -2.9580536 -3.174741 -3.320827 -3.1721172 -3.0902102 -3.0124257 -3.1468143 -3.421046 -3.69345]]...]
INFO - root - 2017-12-15 10:50:20.475957: step 3210, loss = 0.43, batch loss = 0.29 (12.5 examples/sec; 0.639 sec/batch; 58h:27m:31s remains)
INFO - root - 2017-12-15 10:50:26.931182: step 3220, loss = 0.41, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 57h:44m:41s remains)
INFO - root - 2017-12-15 10:50:33.254778: step 3230, loss = 0.39, batch loss = 0.25 (12.5 examples/sec; 0.639 sec/batch; 58h:27m:32s remains)
INFO - root - 2017-12-15 10:50:39.612448: step 3240, loss = 0.44, batch loss = 0.30 (12.3 examples/sec; 0.652 sec/batch; 59h:36m:09s remains)
INFO - root - 2017-12-15 10:50:45.942629: step 3250, loss = 0.50, batch loss = 0.36 (12.6 examples/sec; 0.636 sec/batch; 58h:07m:31s remains)
INFO - root - 2017-12-15 10:50:52.335405: step 3260, loss = 0.45, batch loss = 0.30 (12.2 examples/sec; 0.658 sec/batch; 60h:09m:20s remains)
INFO - root - 2017-12-15 10:50:58.671495: step 3270, loss = 0.40, batch loss = 0.25 (12.8 examples/sec; 0.626 sec/batch; 57h:13m:36s remains)
INFO - root - 2017-12-15 10:51:05.045094: step 3280, loss = 0.42, batch loss = 0.28 (12.3 examples/sec; 0.649 sec/batch; 59h:19m:10s remains)
INFO - root - 2017-12-15 10:51:11.496372: step 3290, loss = 0.46, batch loss = 0.32 (12.8 examples/sec; 0.623 sec/batch; 56h:59m:27s remains)
INFO - root - 2017-12-15 10:51:17.902444: step 3300, loss = 0.42, batch loss = 0.28 (12.6 examples/sec; 0.635 sec/batch; 58h:02m:05s remains)
2017-12-15 10:51:18.384966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0279436 -4.0429754 -3.9551694 -4.0461621 -4.361948 -4.5028858 -4.4823647 -4.6614151 -4.1693316 -3.7296963 -3.4884174 -3.264271 -3.3222322 -3.8320291 -4.5812564][-3.8943691 -4.1501861 -4.2148037 -4.3019662 -4.1107883 -4.0069342 -4.0925884 -4.1333895 -3.6340895 -3.3195767 -2.886307 -2.8978379 -3.1073046 -3.8032861 -4.3840666][-3.4465153 -3.3342764 -3.3041396 -3.3075609 -3.0889375 -3.0541515 -3.2535806 -3.1551147 -2.7044709 -2.4334013 -1.9159098 -1.7777476 -2.2693782 -3.0276904 -3.6326439][-3.193377 -3.0614405 -2.7347276 -2.2443056 -2.0248113 -1.6103716 -1.6828971 -1.8438625 -1.6020451 -1.4932723 -1.0945539 -1.3122072 -1.5158787 -1.9951 -2.7717311][-2.6168332 -2.542064 -2.1851783 -1.5231147 -1.2261548 -0.95739412 -1.0376053 -0.83827686 -0.43275928 -0.390594 -0.20232344 -0.73264503 -1.1861501 -1.8756609 -2.7100346][-2.0906744 -2.1357074 -1.8361273 -1.2332883 -0.59271955 -0.0680089 -0.045923233 -0.062465191 0.2512393 0.2236228 0.41343641 -0.0049962997 -0.60412455 -1.5694628 -2.6386523][-1.7356424 -1.6461883 -1.5868425 -0.84544468 -0.29272509 0.10009146 0.3127737 0.40657616 0.77940226 0.61952019 0.62279844 0.037314892 -0.61815357 -1.6383853 -2.8947525][-2.171495 -1.77527 -1.3771071 -0.95655155 -0.46536827 0.18166256 0.34254742 0.29753542 0.64684486 0.84495306 1.0175838 0.47826815 -0.56652546 -1.8307185 -3.1671789][-2.5978827 -2.6737409 -2.2275167 -1.618403 -0.83939075 -0.14129019 0.41342831 0.51182556 0.89587593 0.87877226 0.70147181 0.15840578 -0.94297028 -2.1539698 -3.2569962][-3.2454026 -2.8891332 -2.5799801 -2.0873652 -1.5437074 -1.0651388 -0.4523325 0.10571575 0.83679342 0.65217066 0.32333612 -0.48938036 -1.6165371 -2.6549232 -3.8586154][-3.6447167 -3.6006267 -3.1284006 -2.3404245 -1.7170181 -1.2648773 -1.0442867 -0.74630976 -0.26220465 -0.26315165 -0.48143816 -1.1769004 -1.9883671 -2.8862753 -3.809608][-4.278573 -3.8868876 -3.5836639 -3.2201576 -2.6364162 -2.2193284 -2.0028596 -1.7325211 -1.2801514 -1.6257138 -2.0993304 -2.4266303 -3.053853 -3.4583693 -3.9656932][-4.6064429 -4.3273726 -4.0344219 -3.6520121 -3.3325438 -3.1245396 -3.0181584 -2.8761096 -2.7470489 -2.9587543 -3.0832558 -3.4178295 -3.8221312 -4.1769123 -4.6180406][-5.1522384 -5.1364789 -4.5796309 -4.302464 -3.643914 -3.6018817 -3.663043 -3.6192408 -3.6178308 -3.9102814 -4.1407089 -4.333343 -4.4231563 -4.4781551 -4.4775119][-6.0525436 -5.6537166 -5.2462549 -5.260807 -5.0341821 -4.7452526 -4.7035275 -4.6133971 -4.5020037 -4.806159 -4.8669653 -5.0071645 -5.1093626 -4.8914495 -4.7111144]]...]
INFO - root - 2017-12-15 10:51:24.752706: step 3310, loss = 0.41, batch loss = 0.26 (12.7 examples/sec; 0.629 sec/batch; 57h:30m:00s remains)
INFO - root - 2017-12-15 10:51:31.212709: step 3320, loss = 0.46, batch loss = 0.32 (12.4 examples/sec; 0.643 sec/batch; 58h:48m:50s remains)
INFO - root - 2017-12-15 10:51:37.559539: step 3330, loss = 0.42, batch loss = 0.28 (12.7 examples/sec; 0.629 sec/batch; 57h:28m:23s remains)
INFO - root - 2017-12-15 10:51:43.921156: step 3340, loss = 0.39, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 58h:44m:23s remains)
INFO - root - 2017-12-15 10:51:50.269698: step 3350, loss = 0.45, batch loss = 0.31 (12.6 examples/sec; 0.634 sec/batch; 57h:58m:15s remains)
INFO - root - 2017-12-15 10:51:56.605465: step 3360, loss = 0.42, batch loss = 0.28 (13.0 examples/sec; 0.617 sec/batch; 56h:23m:40s remains)
INFO - root - 2017-12-15 10:52:02.947212: step 3370, loss = 0.47, batch loss = 0.33 (13.1 examples/sec; 0.611 sec/batch; 55h:49m:35s remains)
INFO - root - 2017-12-15 10:52:09.342942: step 3380, loss = 0.45, batch loss = 0.31 (12.4 examples/sec; 0.646 sec/batch; 59h:06m:07s remains)
INFO - root - 2017-12-15 10:52:15.781365: step 3390, loss = 0.39, batch loss = 0.25 (12.8 examples/sec; 0.624 sec/batch; 57h:01m:27s remains)
INFO - root - 2017-12-15 10:52:22.318735: step 3400, loss = 0.42, batch loss = 0.28 (12.3 examples/sec; 0.649 sec/batch; 59h:20m:08s remains)
2017-12-15 10:52:22.828223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3967228 -4.2415037 -4.1027718 -4.2993488 -4.44016 -4.4128537 -4.4174113 -4.3433213 -4.2622581 -4.4253855 -4.6728711 -4.7675529 -4.9886956 -5.2487068 -5.5486856][-3.5913115 -3.7971056 -4.0512409 -4.1479759 -4.0902605 -3.9712245 -3.6950259 -3.5044594 -3.4434319 -3.6708214 -3.9042304 -4.1644459 -4.4539475 -4.6117163 -4.8293447][-2.8423228 -2.7456903 -2.8730707 -3.1136925 -3.2620161 -2.9250968 -2.4877815 -2.4221983 -2.3577294 -2.6797318 -2.7494657 -2.9772568 -3.2917345 -3.7468047 -3.9783831][-1.7468143 -1.657783 -1.6588483 -1.7742033 -1.8743105 -1.7441111 -1.4276996 -1.2383733 -1.1027031 -1.4956923 -1.9005895 -2.5150924 -3.3021665 -3.6531889 -3.818573][-1.6209688 -1.2495275 -0.97828388 -0.86713266 -0.75726748 -0.5726862 -0.34177542 -0.20999718 -0.17618227 -1.0472531 -1.7613339 -2.4963875 -3.0745668 -3.6252317 -4.1452107][-1.4435468 -1.0069532 -0.78886414 -0.5368042 -0.39573812 -0.20571566 0.11584473 0.16189766 -0.0063152313 -0.85501766 -1.8267274 -2.7752807 -3.5704887 -4.1895285 -4.5876923][-1.7013178 -1.3823099 -1.1549163 -0.67387581 -0.29125071 -0.21952724 -0.17171669 -0.20256853 -0.2119112 -1.2988262 -2.4229395 -3.5122392 -4.2635508 -4.6994634 -4.9171238][-2.135047 -1.8948493 -1.6452942 -1.2124734 -0.77892256 -0.43085575 -0.15606833 -0.39162874 -0.53119326 -1.4072928 -2.3565383 -3.4726725 -4.3613687 -5.090673 -5.5311031][-2.4169645 -2.134037 -1.8649998 -1.6894336 -1.5930924 -1.2909112 -0.91088867 -1.0684214 -1.3128223 -2.1602917 -2.9157083 -3.7868872 -4.5186715 -4.9893656 -5.4596395][-2.8290634 -2.5200374 -2.3543768 -2.2613158 -2.2404675 -2.2390995 -2.3183546 -2.5206983 -2.5391359 -3.2635825 -3.963779 -4.6027441 -5.0583773 -5.5017061 -5.9643183][-3.9989762 -3.7724471 -3.5219293 -3.6782134 -3.8390372 -3.8531554 -3.7178106 -3.8355205 -4.149704 -4.7801347 -5.1346035 -5.4487824 -5.7614913 -6.0456443 -6.3704596][-4.9307771 -4.8026981 -4.7696466 -4.8198876 -4.914403 -5.0435987 -5.0578046 -5.3373795 -5.3316264 -5.1658959 -5.316174 -5.7898126 -6.11151 -6.1416545 -6.2126222][-5.4793692 -5.3683329 -5.2928672 -5.3380251 -5.4888611 -5.5270205 -5.3737731 -5.3084879 -5.3127418 -5.6595387 -5.9129639 -5.8233109 -5.8031759 -5.9422708 -6.1360679][-5.2381105 -5.2543378 -5.3087797 -5.2700362 -5.1844 -5.0670176 -4.9239736 -4.8424978 -4.7021384 -4.7285557 -5.0265846 -5.5318108 -5.840591 -5.7490587 -5.713563][-5.430512 -5.2556443 -5.121769 -5.0478911 -4.9927497 -4.8856573 -4.6651993 -4.5146418 -4.5244794 -4.6625495 -4.7651839 -4.84233 -5.0415173 -5.4223113 -5.6951852]]...]
INFO - root - 2017-12-15 10:52:29.191989: step 3410, loss = 0.45, batch loss = 0.31 (12.6 examples/sec; 0.633 sec/batch; 57h:52m:06s remains)
INFO - root - 2017-12-15 10:52:35.566579: step 3420, loss = 0.44, batch loss = 0.30 (12.7 examples/sec; 0.631 sec/batch; 57h:39m:00s remains)
INFO - root - 2017-12-15 10:52:41.899339: step 3430, loss = 0.50, batch loss = 0.36 (12.7 examples/sec; 0.631 sec/batch; 57h:41m:56s remains)
INFO - root - 2017-12-15 10:52:48.268525: step 3440, loss = 0.42, batch loss = 0.28 (12.0 examples/sec; 0.669 sec/batch; 61h:10m:46s remains)
INFO - root - 2017-12-15 10:52:54.639361: step 3450, loss = 0.38, batch loss = 0.25 (12.5 examples/sec; 0.641 sec/batch; 58h:32m:53s remains)
INFO - root - 2017-12-15 10:53:01.029962: step 3460, loss = 0.44, batch loss = 0.30 (12.6 examples/sec; 0.636 sec/batch; 58h:07m:49s remains)
INFO - root - 2017-12-15 10:53:07.451278: step 3470, loss = 0.47, batch loss = 0.33 (12.7 examples/sec; 0.632 sec/batch; 57h:45m:06s remains)
INFO - root - 2017-12-15 10:53:13.812688: step 3480, loss = 0.45, batch loss = 0.31 (12.4 examples/sec; 0.645 sec/batch; 58h:55m:18s remains)
INFO - root - 2017-12-15 10:53:20.225833: step 3490, loss = 0.46, batch loss = 0.32 (12.5 examples/sec; 0.641 sec/batch; 58h:35m:36s remains)
INFO - root - 2017-12-15 10:53:26.575639: step 3500, loss = 0.46, batch loss = 0.32 (12.5 examples/sec; 0.642 sec/batch; 58h:41m:08s remains)
2017-12-15 10:53:27.121643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2152276 -2.2259169 -2.1042361 -1.9622445 -1.7968144 -1.7063942 -1.7463036 -1.8689313 -1.8325887 -2.3014755 -2.9095287 -3.4216046 -3.8267808 -4.2252192 -4.3045621][-2.4523559 -2.5200634 -2.3464837 -2.0297246 -1.7397761 -1.4979262 -1.3373923 -1.2216034 -1.1520171 -1.5926032 -2.3239245 -2.9117939 -3.5125663 -4.0948973 -4.2551036][-2.7587035 -2.69865 -2.4452991 -2.1029577 -1.8576002 -1.5079684 -1.1628723 -1.0082779 -0.87476015 -1.2804909 -1.6356063 -2.1185231 -2.6725857 -3.2889628 -3.7300742][-3.2730429 -3.1240938 -2.8456604 -2.455627 -2.0346823 -1.6129088 -1.2182579 -0.87723923 -0.7157011 -1.1426563 -1.3142152 -1.671144 -1.9630771 -2.4862487 -2.8673034][-3.6818562 -3.4764838 -3.12575 -2.6899285 -2.150032 -1.6172743 -0.95954943 -0.50068235 -0.19618607 -0.4209981 -0.48511028 -0.96608639 -1.4171104 -1.8746824 -2.3735452][-3.2802074 -3.1514802 -2.8450122 -2.2204375 -1.6105585 -1.0364246 -0.34670258 0.19884443 0.62836933 0.41574907 0.34156656 -0.16256809 -0.67467117 -1.2832141 -1.9512591][-2.537914 -2.1955748 -1.9289823 -1.4367762 -0.71964836 0.18667507 0.84415865 1.3104587 1.8216033 1.5006409 1.3301725 0.65266705 -0.043274403 -0.80793238 -1.6668491][-1.6647844 -1.1995187 -0.6912632 -0.022218704 0.63001585 1.629807 2.1861148 2.6588273 3.2067809 2.7286458 2.2937684 1.3823037 0.54697657 -0.43012857 -1.6845675][-0.86826468 -0.34817886 0.26513577 1.1254845 1.9491248 2.7069154 3.3372684 3.7264705 3.9670367 3.2743468 2.69696 1.7746491 0.82493496 -0.24546432 -1.5402789][-1.0927839 -0.58844995 -0.051532269 0.7670083 1.5834613 2.0727572 2.6401067 3.1527286 3.2756982 2.5536075 2.0853705 1.252768 0.48132849 -0.29377317 -1.1741824][-1.9089518 -1.4088583 -1.0885634 -0.38595486 0.15012121 0.46232653 0.78461361 1.1282973 1.3272953 0.83792162 0.617311 0.34979677 -0.38471556 -1.1243033 -1.5331397][-3.0033867 -2.5283163 -2.3168511 -1.9229274 -1.6886973 -1.4883914 -1.516552 -1.3267274 -1.2287526 -1.6008954 -1.7576375 -1.6978459 -1.9585519 -2.1179266 -2.1184092][-4.1708212 -3.7012551 -3.5113575 -3.2445707 -3.0601244 -2.952352 -2.9012487 -3.052248 -3.0665913 -3.1295805 -3.1279852 -2.9846585 -2.9078486 -2.9508436 -2.8332877][-5.3415189 -4.9564838 -4.6730967 -4.4922123 -4.3497958 -4.2585874 -4.16706 -4.364151 -4.442872 -4.5240278 -4.5293183 -4.3984585 -4.2435541 -4.0098896 -3.7281015][-6.0004315 -5.5830145 -5.1492262 -5.1172309 -5.1566048 -5.0248489 -4.9953771 -5.1503654 -5.33347 -5.6512313 -5.6965837 -5.6589608 -5.625452 -5.5290637 -5.3623867]]...]
INFO - root - 2017-12-15 10:53:33.489251: step 3510, loss = 0.39, batch loss = 0.25 (12.9 examples/sec; 0.621 sec/batch; 56h:46m:12s remains)
INFO - root - 2017-12-15 10:53:39.839427: step 3520, loss = 0.45, batch loss = 0.31 (12.6 examples/sec; 0.634 sec/batch; 57h:54m:17s remains)
INFO - root - 2017-12-15 10:53:46.196228: step 3530, loss = 0.39, batch loss = 0.25 (12.9 examples/sec; 0.621 sec/batch; 56h:45m:40s remains)
INFO - root - 2017-12-15 10:53:52.669436: step 3540, loss = 0.44, batch loss = 0.30 (12.0 examples/sec; 0.668 sec/batch; 61h:04m:46s remains)
INFO - root - 2017-12-15 10:53:59.159013: step 3550, loss = 0.44, batch loss = 0.30 (12.4 examples/sec; 0.646 sec/batch; 59h:03m:42s remains)
INFO - root - 2017-12-15 10:54:05.595882: step 3560, loss = 0.40, batch loss = 0.26 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:39s remains)
INFO - root - 2017-12-15 10:54:12.062427: step 3570, loss = 0.42, batch loss = 0.28 (12.2 examples/sec; 0.655 sec/batch; 59h:49m:08s remains)
INFO - root - 2017-12-15 10:54:18.558510: step 3580, loss = 0.40, batch loss = 0.26 (12.6 examples/sec; 0.636 sec/batch; 58h:07m:16s remains)
INFO - root - 2017-12-15 10:54:24.965957: step 3590, loss = 0.42, batch loss = 0.29 (12.6 examples/sec; 0.637 sec/batch; 58h:12m:01s remains)
INFO - root - 2017-12-15 10:54:31.347304: step 3600, loss = 0.39, batch loss = 0.26 (12.6 examples/sec; 0.637 sec/batch; 58h:11m:37s remains)
2017-12-15 10:54:31.864168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1388614 -3.4191792 -3.0865548 -2.8276782 -2.8243744 -3.5017252 -3.9170015 -3.5234005 -3.3259404 -3.6234286 -3.8266909 -3.9906485 -3.8033388 -4.2880058 -5.2351012][-2.6567473 -2.7196627 -2.5488582 -2.2819438 -2.7492421 -2.9988554 -3.1924427 -3.5910747 -3.4736536 -2.7143843 -2.9267147 -3.540247 -3.8519642 -4.2135048 -4.3683729][-2.8213277 -2.9841263 -2.9473193 -2.5773053 -2.1825252 -2.2910476 -2.2738504 -2.1275997 -2.1745429 -2.2778563 -2.08352 -2.5673509 -3.6962168 -4.4968624 -4.9830694][-3.5074692 -3.554775 -3.095506 -2.2932463 -2.0227194 -1.8855643 -1.7295823 -2.0385547 -1.9734902 -1.7182021 -1.8671732 -2.6302419 -3.0385613 -4.3409052 -5.23724][-4.0044832 -3.059593 -2.5874434 -2.6554198 -1.7779717 -0.99595547 -0.69293833 -1.2070484 -1.1438255 -1.0943942 -0.97165012 -1.4355311 -2.1495113 -3.39449 -4.4791794][-3.0486588 -3.021138 -2.275888 -1.4738708 -1.1231642 -0.69348621 0.067450047 -0.35797215 -0.76061249 -1.0528378 -1.3611865 -1.7465205 -2.1444197 -2.8430686 -4.1741476][-2.8978307 -1.7508869 -0.9329586 -0.60667753 -0.62739134 -0.28818607 0.15819311 0.3746295 0.34251785 0.07067728 -1.1656303 -1.6799073 -2.3174191 -3.1421745 -4.1537938][-3.0959291 -1.5244927 -0.36414909 0.41276407 1.5093589 1.9439874 1.8009868 1.1600132 0.84577131 0.086112022 -1.0091748 -1.8931403 -2.5749278 -3.3925321 -4.3251181][-2.890743 -1.6475625 -0.41542435 1.2473655 1.8816009 1.6622443 1.8573442 1.7235827 1.7363286 0.92666769 -0.18656826 -1.5392127 -2.1592202 -3.1023006 -4.194397][-3.2484407 -2.3202362 -1.1261592 -0.19545412 1.4180846 1.8819079 1.6802382 1.0542097 0.81398869 0.5408144 -0.26705694 -0.76837111 -1.6116328 -2.9550881 -4.2274256][-3.9228175 -3.1122992 -2.1930056 -0.76533842 0.737977 1.1424026 1.231957 0.99423647 0.45047379 0.17178726 -0.61365414 -1.389658 -2.1257854 -2.9638214 -3.6493597][-3.7598886 -3.2326527 -3.2323921 -1.8262691 -0.87126732 0.44937134 0.85077381 0.60797882 -0.24268436 -1.0857496 -1.597611 -1.8967838 -2.7133167 -3.571635 -4.271759][-4.07095 -2.9365 -2.4177666 -2.5369205 -1.5989494 -0.70299149 -0.18014956 -0.11296129 -0.52987146 -0.94063425 -2.1600904 -2.38342 -2.4060392 -3.29471 -4.0818691][-5.178895 -4.3320103 -3.213593 -2.5439353 -2.2497711 -2.3424354 -1.8891873 -2.1554761 -2.1636209 -2.6114526 -3.0721729 -3.5092797 -4.276433 -4.1981583 -4.2208996][-5.7335672 -4.8534021 -4.0689344 -3.7108285 -2.7538974 -2.6347337 -2.8162546 -3.2328432 -3.8306108 -4.2362113 -4.46749 -4.4097867 -3.9725697 -4.427887 -4.99082]]...]
INFO - root - 2017-12-15 10:54:38.258087: step 3610, loss = 0.43, batch loss = 0.30 (12.4 examples/sec; 0.644 sec/batch; 58h:48m:27s remains)
INFO - root - 2017-12-15 10:54:44.693094: step 3620, loss = 0.46, batch loss = 0.33 (12.1 examples/sec; 0.659 sec/batch; 60h:12m:38s remains)
INFO - root - 2017-12-15 10:54:51.116557: step 3630, loss = 0.51, batch loss = 0.37 (12.6 examples/sec; 0.635 sec/batch; 58h:02m:43s remains)
INFO - root - 2017-12-15 10:54:57.506331: step 3640, loss = 0.41, batch loss = 0.27 (12.3 examples/sec; 0.651 sec/batch; 59h:27m:26s remains)
INFO - root - 2017-12-15 10:55:03.875154: step 3650, loss = 0.43, batch loss = 0.30 (12.9 examples/sec; 0.622 sec/batch; 56h:50m:10s remains)
INFO - root - 2017-12-15 10:55:10.313897: step 3660, loss = 0.39, batch loss = 0.26 (12.3 examples/sec; 0.649 sec/batch; 59h:19m:26s remains)
INFO - root - 2017-12-15 10:55:16.759555: step 3670, loss = 0.42, batch loss = 0.28 (12.3 examples/sec; 0.650 sec/batch; 59h:22m:32s remains)
INFO - root - 2017-12-15 10:55:23.157685: step 3680, loss = 0.41, batch loss = 0.27 (12.6 examples/sec; 0.637 sec/batch; 58h:12m:14s remains)
INFO - root - 2017-12-15 10:55:29.618162: step 3690, loss = 0.38, batch loss = 0.25 (12.3 examples/sec; 0.648 sec/batch; 59h:12m:08s remains)
INFO - root - 2017-12-15 10:55:36.021594: step 3700, loss = 0.46, batch loss = 0.33 (12.6 examples/sec; 0.635 sec/batch; 57h:59m:34s remains)
2017-12-15 10:55:36.550825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.056128 -4.4784741 -4.9658728 -5.2068157 -5.1704783 -4.9747887 -4.6772785 -4.3530445 -4.0974865 -4.0273619 -4.0779963 -4.3894281 -4.7333078 -4.7776227 -5.0837741][-3.9527061 -3.9923363 -4.4337893 -4.5852709 -4.4260764 -4.0791111 -3.8511508 -3.713629 -3.3854959 -3.4485402 -3.6010265 -3.9105976 -4.3378181 -4.5027204 -4.8960681][-3.9572027 -3.8477848 -3.9819782 -3.9240236 -3.6992984 -3.486994 -3.2430618 -2.8590455 -2.5674047 -2.7406929 -3.0649889 -3.3814561 -4.0435772 -4.4600687 -4.779891][-4.1092167 -3.6501567 -3.4225123 -3.1152794 -2.80907 -2.6564536 -2.2760758 -1.9987383 -1.7805581 -1.9135871 -2.2626963 -2.8159895 -3.4472353 -3.893378 -4.3161983][-3.1479948 -2.7869895 -2.6435833 -2.1633787 -1.4190841 -0.86336327 -0.42991972 -0.042649269 0.1651783 -0.33806038 -0.99784374 -1.7686634 -2.7844887 -3.2341161 -3.8374619][-2.1862831 -1.6562057 -1.2489715 -0.69170475 -0.21676159 0.58945131 1.5480709 2.148632 2.3692799 1.5423603 0.43137741 -0.3611145 -1.5348058 -2.2652016 -3.0323658][-2.0181417 -1.5836463 -0.95337296 -0.29272079 0.20425749 0.97451496 1.6181965 2.0381107 2.0720606 1.2286148 0.32743645 -0.68501568 -2.0150332 -2.5626974 -3.2566588][-1.9003482 -1.6585655 -1.3027363 -0.6355958 0.22103643 0.85375023 1.6908445 2.1031075 2.2033052 1.2942247 -0.09912014 -1.0363898 -1.8868399 -2.4007025 -3.176347][-2.157248 -1.431118 -1.1332488 -0.59159613 0.06088829 0.3309164 0.7805171 1.0825777 1.0508146 0.089581966 -1.1677208 -2.5008082 -3.4135754 -3.6503415 -4.0213661][-2.9122756 -2.3784208 -1.85779 -1.2565689 -0.7909236 -0.3773632 -0.20873833 -0.074264526 0.11965847 -0.69606733 -2.00037 -3.1920557 -3.9154572 -4.12644 -4.6411705][-4.3162665 -3.8662932 -3.2775762 -2.7933764 -2.3851123 -1.9170222 -1.7405648 -1.7839656 -1.9929924 -2.8220041 -3.5163169 -4.0407677 -4.6739955 -4.6943984 -4.8905778][-4.9087625 -4.6106935 -4.3526831 -3.9524999 -3.45141 -3.1961558 -2.9503682 -2.9758542 -3.1358685 -3.608186 -4.344646 -4.6780915 -4.8819904 -4.8865185 -4.9381132][-5.7613807 -5.4098511 -5.18165 -4.9179344 -4.5340366 -4.4045253 -4.1901484 -4.2001219 -4.1786127 -4.3646846 -4.6504736 -4.7864847 -5.0346837 -5.0327721 -4.9934063][-5.6700692 -5.5627103 -5.4635892 -5.2674241 -5.1437306 -5.0815344 -4.8473072 -4.7026262 -4.6541128 -4.7664089 -4.791801 -5.1002111 -5.2212853 -5.1595612 -5.1672349][-6.768672 -6.7483292 -6.66674 -6.6032028 -6.5526261 -6.4059968 -6.40905 -6.4273062 -6.3700256 -6.1985211 -5.9576359 -5.5571074 -5.2994022 -5.2474313 -5.1220727]]...]
INFO - root - 2017-12-15 10:55:42.969166: step 3710, loss = 0.42, batch loss = 0.29 (12.3 examples/sec; 0.649 sec/batch; 59h:18m:31s remains)
INFO - root - 2017-12-15 10:55:49.420204: step 3720, loss = 0.42, batch loss = 0.29 (12.4 examples/sec; 0.646 sec/batch; 58h:57m:10s remains)
INFO - root - 2017-12-15 10:55:55.777660: step 3730, loss = 0.45, batch loss = 0.32 (13.0 examples/sec; 0.614 sec/batch; 56h:06m:39s remains)
INFO - root - 2017-12-15 10:56:02.210735: step 3740, loss = 0.38, batch loss = 0.25 (12.2 examples/sec; 0.654 sec/batch; 59h:41m:46s remains)
INFO - root - 2017-12-15 10:56:08.820957: step 3750, loss = 0.41, batch loss = 0.27 (12.0 examples/sec; 0.668 sec/batch; 60h:58m:17s remains)
INFO - root - 2017-12-15 10:56:15.224556: step 3760, loss = 0.49, batch loss = 0.35 (12.7 examples/sec; 0.628 sec/batch; 57h:22m:11s remains)
INFO - root - 2017-12-15 10:56:21.587811: step 3770, loss = 0.38, batch loss = 0.25 (12.6 examples/sec; 0.637 sec/batch; 58h:11m:50s remains)
INFO - root - 2017-12-15 10:56:28.065783: step 3780, loss = 0.49, batch loss = 0.36 (12.2 examples/sec; 0.657 sec/batch; 59h:59m:17s remains)
INFO - root - 2017-12-15 10:56:34.451091: step 3790, loss = 0.44, batch loss = 0.31 (12.7 examples/sec; 0.629 sec/batch; 57h:26m:04s remains)
INFO - root - 2017-12-15 10:56:40.929826: step 3800, loss = 0.43, batch loss = 0.29 (12.1 examples/sec; 0.659 sec/batch; 60h:09m:59s remains)
2017-12-15 10:56:41.451629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5555062 -2.7526696 -2.885397 -3.0531712 -3.1189091 -2.95217 -2.6260223 -2.38942 -2.17664 -2.486558 -3.060133 -3.6248763 -4.1181221 -4.4045906 -4.6594057][-2.8010564 -2.9539974 -3.1143086 -3.2552242 -3.2619786 -3.1622667 -2.9106858 -2.5885854 -2.2732396 -2.6694603 -3.2493553 -3.8280168 -4.4815869 -4.8482685 -5.105516][-2.9946434 -3.14399 -3.2517927 -3.2780144 -3.2177839 -3.0828936 -2.8013048 -2.5158463 -2.2562141 -2.6685152 -3.3329959 -4.084734 -4.7849064 -5.1275191 -5.4567871][-3.2483187 -3.1652734 -3.0847747 -3.0295992 -2.8249221 -2.4867864 -2.0218649 -1.6024356 -1.240304 -1.6986036 -2.5675859 -3.5657229 -4.5099897 -5.1264815 -5.6673169][-3.3645973 -3.3065169 -3.0876412 -2.7481263 -2.2168941 -1.7098846 -1.1080418 -0.61670351 -0.24025202 -0.69607735 -1.6328001 -2.7877712 -4.0207682 -4.904521 -5.5908065][-3.4009109 -3.2957358 -3.0482605 -2.5400143 -1.7331204 -0.922771 -0.13102818 0.41441679 0.75661516 0.093212128 -0.95114994 -2.2467499 -3.6223946 -4.6342564 -5.39425][-3.2908332 -3.0437107 -2.6858006 -2.1007695 -1.1485691 -0.18305445 0.75074196 1.4835482 1.8381453 1.1422987 -0.089387894 -1.6508508 -3.1459258 -4.2127295 -5.1189394][-2.9269953 -2.7978466 -2.5468636 -1.8768578 -0.86837864 0.14586735 1.0401664 1.5816727 1.8770676 1.1913223 -0.061886311 -1.4514999 -2.8284214 -3.8962712 -4.7271166][-2.9631934 -2.8099141 -2.6152716 -2.0060272 -1.0931211 -0.18146372 0.60863876 1.0584874 1.176125 0.3509326 -0.74905014 -2.0456605 -3.3449359 -4.1896086 -4.8016958][-3.3623655 -3.21311 -3.0862939 -2.6395459 -2.0461264 -1.2819695 -0.56536674 -0.33820391 -0.20771694 -0.8304162 -1.8678498 -2.930532 -3.8649154 -4.6364713 -5.2156987][-3.7459774 -3.7289975 -3.6882727 -3.4544432 -3.1861315 -2.67244 -2.1615639 -1.9972425 -1.9034414 -2.3912797 -3.0790527 -3.8298423 -4.5243855 -5.0152459 -5.3381271][-4.5672808 -4.4238973 -4.28834 -4.1588554 -4.0181775 -3.7817504 -3.5629933 -3.3133478 -3.1431048 -3.622896 -4.1431241 -4.6049566 -5.0794916 -5.2703419 -5.5097303][-5.4846191 -5.4103384 -5.2079635 -5.0498524 -4.8280377 -4.61263 -4.4205632 -4.2281203 -4.0482988 -4.1883297 -4.38729 -4.8676577 -5.3194718 -5.452095 -5.6865516][-5.973444 -5.821084 -5.6543522 -5.5158978 -5.378274 -5.2047033 -5.0336919 -4.8826828 -4.7071705 -4.6618357 -4.7050428 -4.9946961 -5.2869139 -5.3796949 -5.5454569][-6.5124154 -6.14557 -5.9142632 -5.8057365 -5.6856418 -5.5834093 -5.503561 -5.3802295 -5.28354 -5.2885971 -5.2273035 -5.16559 -5.2332869 -5.2232952 -5.271059]]...]
INFO - root - 2017-12-15 10:56:47.808408: step 3810, loss = 0.44, batch loss = 0.31 (12.8 examples/sec; 0.625 sec/batch; 57h:06m:10s remains)
INFO - root - 2017-12-15 10:56:54.302512: step 3820, loss = 0.38, batch loss = 0.25 (11.8 examples/sec; 0.675 sec/batch; 61h:39m:14s remains)
INFO - root - 2017-12-15 10:57:00.728950: step 3830, loss = 0.44, batch loss = 0.31 (12.4 examples/sec; 0.646 sec/batch; 58h:58m:28s remains)
INFO - root - 2017-12-15 10:57:07.266591: step 3840, loss = 0.45, batch loss = 0.32 (12.2 examples/sec; 0.657 sec/batch; 59h:57m:50s remains)
INFO - root - 2017-12-15 10:57:13.673105: step 3850, loss = 0.39, batch loss = 0.26 (12.4 examples/sec; 0.648 sec/batch; 59h:06m:43s remains)
INFO - root - 2017-12-15 10:57:20.099674: step 3860, loss = 0.41, batch loss = 0.28 (12.6 examples/sec; 0.637 sec/batch; 58h:08m:35s remains)
INFO - root - 2017-12-15 10:57:26.539791: step 3870, loss = 0.42, batch loss = 0.29 (12.5 examples/sec; 0.642 sec/batch; 58h:38m:46s remains)
INFO - root - 2017-12-15 10:57:32.905236: step 3880, loss = 0.36, batch loss = 0.23 (12.6 examples/sec; 0.633 sec/batch; 57h:49m:25s remains)
INFO - root - 2017-12-15 10:57:39.316784: step 3890, loss = 0.49, batch loss = 0.36 (12.0 examples/sec; 0.664 sec/batch; 60h:37m:22s remains)
INFO - root - 2017-12-15 10:57:45.660683: step 3900, loss = 0.37, batch loss = 0.23 (12.4 examples/sec; 0.646 sec/batch; 58h:57m:41s remains)
2017-12-15 10:57:46.177690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5590019 -1.370667 -1.3550286 -1.2609878 -1.0934944 -0.75860834 -0.39363956 -0.097337246 0.11756134 -0.34328842 -0.68420362 -1.5327849 -2.5119929 -3.6015983 -4.4480524][-1.7971015 -1.5826907 -1.5521479 -1.4683332 -1.2878208 -1.0196786 -0.68719482 -0.33007956 -0.0085091591 -0.44309425 -0.73872709 -1.5181489 -2.4621325 -3.5860879 -4.46952][-1.5960121 -1.431397 -1.3981977 -1.295012 -1.1492767 -0.94549656 -0.62290812 -0.32166052 -0.034094334 -0.44542122 -0.71352148 -1.5391374 -2.5103579 -3.5339737 -4.3661065][-1.2880783 -1.1055365 -1.035429 -0.91554403 -0.68669748 -0.41445017 -0.082018852 0.090684414 0.26816368 -0.18995047 -0.52605104 -1.3974643 -2.3613071 -3.3663359 -4.2445364][-1.1287746 -0.87875843 -0.72045422 -0.50997257 -0.16279888 0.22482347 0.54683161 0.6788063 0.74129057 0.12204266 -0.34429741 -1.2457714 -2.2559505 -3.3098977 -4.2350626][-0.77661467 -0.59614229 -0.45492792 -0.10449934 0.35351515 0.72702074 0.98779964 1.1185789 1.1641407 0.37395573 -0.20985365 -1.1920862 -2.2399898 -3.3193111 -4.1541739][-0.57816076 -0.26795149 0.02486515 0.42803144 0.91779709 1.3791409 1.5794201 1.6029024 1.548769 0.70228767 0.033332825 -1.0874248 -2.2315207 -3.3777468 -4.2679234][-0.50397682 -0.06292057 0.30715179 0.8121686 1.3342428 1.744586 1.8839493 1.8416519 1.6850605 0.76779079 0.076705933 -1.0415053 -2.1931219 -3.4155219 -4.3889952][-0.75574827 -0.43179703 -0.075632572 0.49704838 1.0935397 1.4745293 1.5988483 1.616353 1.498847 0.60714436 -0.014858723 -1.0704551 -2.2293668 -3.465312 -4.399857][-1.2570229 -1.0481067 -0.76101112 -0.16213703 0.42757702 0.81494665 0.97657681 1.087729 1.0424881 0.21244049 -0.33856392 -1.4131031 -2.4481654 -3.5432329 -4.3917522][-1.7420635 -1.6346245 -1.426775 -0.96446896 -0.46548653 -0.06335783 0.13574362 0.21814728 0.20991945 -0.48086357 -0.93196917 -1.9038296 -2.8929038 -3.8911784 -4.657311][-2.6283965 -2.5446906 -2.3776817 -2.1283326 -1.8549905 -1.5658469 -1.3944211 -1.2585444 -1.1348662 -1.6523843 -1.9751234 -2.6563206 -3.4377377 -4.2150545 -4.73658][-3.4613559 -3.3305316 -3.1728365 -3.0565093 -2.9328184 -2.7721996 -2.6580033 -2.5820026 -2.4831576 -2.761693 -2.8823788 -3.3500195 -3.8736219 -4.4468203 -4.8803349][-4.7653542 -4.6010122 -4.4744344 -4.3288937 -4.2566614 -4.2121162 -4.24008 -4.2475681 -4.2140131 -4.4259129 -4.4986649 -4.6081452 -4.8049035 -4.9850779 -5.1576729][-6.162889 -5.9692349 -5.7678113 -5.6419973 -5.5908575 -5.5976815 -5.7367973 -5.8193879 -5.8831615 -5.8787165 -5.8190308 -5.7033815 -5.6109409 -5.5638561 -5.3848667]]...]
INFO - root - 2017-12-15 10:57:52.531721: step 3910, loss = 0.42, batch loss = 0.28 (12.6 examples/sec; 0.633 sec/batch; 57h:47m:45s remains)
INFO - root - 2017-12-15 10:57:58.965307: step 3920, loss = 0.46, batch loss = 0.33 (12.1 examples/sec; 0.660 sec/batch; 60h:13m:23s remains)
INFO - root - 2017-12-15 10:58:05.434158: step 3930, loss = 0.38, batch loss = 0.25 (12.6 examples/sec; 0.636 sec/batch; 58h:04m:45s remains)
INFO - root - 2017-12-15 10:58:11.793443: step 3940, loss = 0.44, batch loss = 0.30 (12.4 examples/sec; 0.644 sec/batch; 58h:48m:53s remains)
INFO - root - 2017-12-15 10:58:18.267667: step 3950, loss = 0.39, batch loss = 0.26 (12.7 examples/sec; 0.630 sec/batch; 57h:29m:27s remains)
INFO - root - 2017-12-15 10:58:24.701917: step 3960, loss = 0.47, batch loss = 0.33 (12.2 examples/sec; 0.655 sec/batch; 59h:48m:02s remains)
INFO - root - 2017-12-15 10:58:31.202477: step 3970, loss = 0.36, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 59h:41m:07s remains)
INFO - root - 2017-12-15 10:58:37.573920: step 3980, loss = 0.39, batch loss = 0.26 (12.3 examples/sec; 0.651 sec/batch; 59h:22m:44s remains)
INFO - root - 2017-12-15 10:58:43.995818: step 3990, loss = 0.39, batch loss = 0.26 (12.5 examples/sec; 0.639 sec/batch; 58h:20m:08s remains)
INFO - root - 2017-12-15 10:58:50.338897: step 4000, loss = 0.38, batch loss = 0.25 (12.4 examples/sec; 0.647 sec/batch; 59h:01m:09s remains)
2017-12-15 10:58:50.914471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5702639 -2.3460155 -2.4406857 -2.537529 -2.7725539 -2.8820269 -2.9620457 -2.972827 -2.8753591 -3.1676791 -3.7324572 -4.5153031 -5.1708765 -5.729022 -6.0238366][-2.4919095 -2.4838762 -2.4981942 -2.473814 -2.4054132 -2.4215074 -2.4986005 -2.5089474 -2.4759512 -2.9187708 -3.5408266 -4.1276426 -4.656044 -5.1844969 -5.6571183][-1.6969333 -1.4438472 -1.2746177 -1.2732801 -1.2499342 -1.1383533 -1.1311822 -1.1369481 -1.2098894 -1.8636813 -2.3614693 -3.0758688 -3.7035823 -4.2674828 -4.697382][-1.568275 -0.96464396 -0.64875746 -0.35105944 0.0027775764 0.065760136 0.12696028 0.064979553 -0.093262672 -0.96405411 -1.6140571 -2.3558159 -2.8806174 -3.435492 -3.8533249][-1.5475926 -0.89296865 -0.36352777 0.11159182 0.66735411 1.1705256 1.2733555 1.1959162 1.0867801 0.29733467 -0.20002317 -1.0003552 -1.7445207 -2.5611358 -3.1198268][-0.92323828 -0.32743788 0.030060768 0.63268614 1.2990284 1.9053297 2.2417507 2.1723232 2.0583916 1.2340603 0.64494228 -0.090626717 -0.73384333 -1.5571451 -2.360055][-0.78994131 0.039783955 0.70236778 1.2416344 1.6847916 2.3428383 2.6624365 2.7109661 2.65878 1.7947779 1.0912871 0.30873394 -0.34993315 -1.0968299 -1.8429108][-0.82605696 0.20922279 1.0998745 1.8674641 2.4130516 2.8779545 3.1386433 3.1597939 3.0575438 2.182436 1.4171486 0.47100687 -0.38598967 -1.0541506 -1.7526288][-1.2694035 -0.33739185 0.6038146 1.6407909 2.4588251 2.8342524 3.0641232 3.1168756 3.0285015 2.1808376 1.3604546 0.3110342 -0.57515 -1.4044423 -2.1833129][-2.2677946 -1.3331966 -0.58691931 0.15951109 0.94265079 1.3966961 1.7532873 1.8032799 1.710104 1.0415101 0.46906185 -0.26328564 -0.952158 -1.5377469 -2.3730593][-3.500757 -2.7147779 -2.0343485 -1.4227781 -1.0573688 -0.71814013 -0.42673874 -0.26484776 -0.21019459 -0.7505827 -1.2921472 -1.811439 -2.3980727 -2.82084 -3.0769868][-5.1652946 -4.4652152 -3.7864504 -3.265352 -2.8892896 -2.6385574 -2.5504842 -2.5697083 -2.53262 -2.9028621 -3.2210333 -3.5545828 -3.9621561 -4.0735531 -4.226984][-5.6183381 -5.5307016 -5.1956558 -4.6686869 -4.2936754 -3.9660642 -3.9029627 -3.9556243 -4.1608968 -4.5274706 -4.8042355 -4.900095 -5.0191 -5.0182505 -5.0030575][-6.1894417 -5.9204721 -5.7142797 -5.6322331 -5.5282393 -5.3060112 -5.2991505 -5.3359227 -5.465363 -5.655972 -5.8578396 -6.037704 -6.0108461 -5.7907305 -5.6933432][-6.396646 -6.4213247 -6.2945318 -6.2625327 -6.2368374 -6.2749481 -6.3675632 -6.5189109 -6.622582 -6.6292605 -6.6287594 -6.5195475 -6.3839593 -6.3891854 -6.2160134]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:58:57.358800: step 4010, loss = 0.59, batch loss = 0.46 (12.8 examples/sec; 0.626 sec/batch; 57h:08m:12s remains)
INFO - root - 2017-12-15 10:59:03.815942: step 4020, loss = 0.43, batch loss = 0.30 (12.6 examples/sec; 0.633 sec/batch; 57h:42m:51s remains)
INFO - root - 2017-12-15 10:59:10.253020: step 4030, loss = 0.42, batch loss = 0.29 (12.3 examples/sec; 0.650 sec/batch; 59h:20m:49s remains)
INFO - root - 2017-12-15 10:59:16.770024: step 4040, loss = 0.37, batch loss = 0.24 (12.6 examples/sec; 0.636 sec/batch; 58h:03m:30s remains)
INFO - root - 2017-12-15 10:59:23.177731: step 4050, loss = 0.43, batch loss = 0.30 (12.5 examples/sec; 0.638 sec/batch; 58h:12m:03s remains)
INFO - root - 2017-12-15 10:59:29.605031: step 4060, loss = 0.38, batch loss = 0.25 (12.5 examples/sec; 0.640 sec/batch; 58h:22m:23s remains)
INFO - root - 2017-12-15 10:59:35.993364: step 4070, loss = 0.47, batch loss = 0.34 (12.1 examples/sec; 0.659 sec/batch; 60h:05m:18s remains)
INFO - root - 2017-12-15 10:59:42.448830: step 4080, loss = 0.43, batch loss = 0.30 (12.3 examples/sec; 0.652 sec/batch; 59h:30m:06s remains)
INFO - root - 2017-12-15 10:59:48.908081: step 4090, loss = 0.34, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 58h:54m:36s remains)
INFO - root - 2017-12-15 10:59:55.328893: step 4100, loss = 0.37, batch loss = 0.23 (12.8 examples/sec; 0.627 sec/batch; 57h:11m:00s remains)
2017-12-15 10:59:55.836484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0817394 -2.1018124 -2.0835052 -1.9957952 -1.8133783 -1.7803931 -1.8652787 -1.9175634 -1.9038472 -2.6699142 -3.4211209 -4.3644114 -4.914834 -5.3997526 -5.4517794][-2.2363048 -2.4228458 -2.454421 -2.4166236 -2.0933743 -1.9568367 -1.9590383 -2.0692205 -2.1229749 -2.6938457 -3.1025908 -3.9692843 -4.7085381 -5.4123964 -5.6120877][-2.0904183 -2.2015429 -2.2444777 -2.2179346 -2.0789385 -1.7002678 -1.3912325 -1.4544816 -1.5497947 -2.2364488 -2.848947 -3.8539672 -4.5359821 -5.2787843 -5.7172213][-2.6426959 -2.5237274 -2.2702198 -2.0141759 -1.9153895 -1.610342 -1.3986907 -1.2032466 -0.888052 -1.2244387 -1.5724072 -2.7059231 -3.7187967 -4.9434223 -5.5186357][-2.4440351 -2.4218059 -1.883997 -1.2940087 -0.91064215 -0.64853811 -0.21033812 0.14697027 0.3077383 -0.052793026 -0.30503416 -1.5110526 -2.7729683 -4.3431549 -5.3353338][-1.9288487 -1.5630302 -1.1174221 -0.53971338 0.29864979 1.0256972 1.3114572 1.6260314 1.9334073 1.5942492 0.934886 -0.4094367 -1.5458026 -3.2614036 -4.4527426][-1.7429152 -1.2270284 -0.49495316 0.1510129 0.78655052 1.7049127 2.3811126 2.7695975 2.8357673 2.4459395 1.7863927 0.3307066 -1.00383 -2.6385422 -3.7769485][-1.3514733 -0.81087589 -0.046235561 0.79092169 1.4887204 2.0359855 2.3379912 2.5325751 2.6901622 2.4151368 1.799602 0.47030449 -0.61049318 -2.2812948 -3.673254][-1.0379992 -0.42516613 0.35888529 1.0075793 1.4301152 1.776814 2.0300212 1.981029 1.8926053 1.4706073 1.2412419 0.23038101 -0.64313078 -2.019248 -3.0305293][-1.2217813 -0.86602879 -0.24510145 0.065476418 0.34397078 0.66753912 0.66855431 0.75384665 0.77558756 0.2372365 -0.1958518 -1.1111503 -1.7068896 -2.8997726 -3.6081107][-2.017992 -1.8080029 -1.5193729 -1.2246699 -1.2767558 -1.4196897 -1.498383 -1.0960712 -0.92782545 -1.3243928 -1.6742668 -2.2723761 -2.7161384 -3.485646 -4.0940456][-3.8123965 -3.7015989 -3.6482399 -3.298444 -2.9677308 -2.8732119 -3.2596617 -3.3611312 -3.0958803 -3.2037334 -2.9809914 -3.4200811 -3.6462615 -4.0618367 -4.459239][-4.8921943 -4.7282157 -4.8418818 -4.8216352 -4.8134995 -4.7067518 -4.5794983 -4.5495214 -4.4766827 -4.4844155 -4.2005548 -4.3281279 -4.4397783 -4.787271 -4.7188272][-5.3692427 -5.294023 -5.2428832 -5.2129936 -5.2276173 -5.0875306 -5.0440335 -5.1113205 -5.1465292 -5.066371 -4.891017 -4.9011431 -4.8648777 -5.0240536 -5.0696273][-6.2491155 -6.0097413 -5.8272495 -5.6320291 -5.4661107 -5.442389 -5.3373833 -5.3582654 -5.4262838 -5.4724617 -5.7131257 -5.5708303 -5.394352 -5.2543082 -5.217278]]...]
INFO - root - 2017-12-15 11:00:02.331365: step 4110, loss = 0.40, batch loss = 0.27 (12.2 examples/sec; 0.656 sec/batch; 59h:50m:45s remains)
INFO - root - 2017-12-15 11:00:08.788171: step 4120, loss = 0.40, batch loss = 0.26 (12.1 examples/sec; 0.660 sec/batch; 60h:12m:59s remains)
INFO - root - 2017-12-15 11:00:15.236220: step 4130, loss = 0.44, batch loss = 0.31 (12.4 examples/sec; 0.643 sec/batch; 58h:37m:09s remains)
INFO - root - 2017-12-15 11:00:21.696148: step 4140, loss = 0.37, batch loss = 0.24 (12.4 examples/sec; 0.643 sec/batch; 58h:39m:35s remains)
INFO - root - 2017-12-15 11:00:28.087572: step 4150, loss = 0.36, batch loss = 0.23 (12.7 examples/sec; 0.630 sec/batch; 57h:27m:20s remains)
INFO - root - 2017-12-15 11:00:34.420693: step 4160, loss = 0.46, batch loss = 0.33 (12.8 examples/sec; 0.626 sec/batch; 57h:07m:27s remains)
INFO - root - 2017-12-15 11:00:40.786070: step 4170, loss = 0.40, batch loss = 0.27 (12.3 examples/sec; 0.653 sec/batch; 59h:31m:35s remains)
INFO - root - 2017-12-15 11:00:47.210296: step 4180, loss = 0.37, batch loss = 0.24 (12.3 examples/sec; 0.651 sec/batch; 59h:22m:28s remains)
INFO - root - 2017-12-15 11:00:53.574215: step 4190, loss = 0.48, batch loss = 0.35 (12.1 examples/sec; 0.661 sec/batch; 60h:16m:17s remains)
INFO - root - 2017-12-15 11:00:59.984307: step 4200, loss = 0.44, batch loss = 0.31 (12.0 examples/sec; 0.664 sec/batch; 60h:33m:05s remains)
2017-12-15 11:01:00.479722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14618 -3.5338047 -3.0414355 -2.6727238 -2.7948895 -2.8489771 -2.7342038 -2.4776397 -2.1637926 -2.4332342 -2.76962 -3.3627765 -3.5540111 -3.803406 -4.2023921][-2.7618341 -2.7877331 -2.6764879 -2.6905522 -2.7753506 -2.8396144 -2.8505754 -2.4768486 -2.0873055 -2.265378 -2.2252998 -2.6770558 -3.1741464 -3.7430058 -4.2313519][-2.5103488 -2.2587314 -2.1052713 -2.4549875 -2.3536849 -2.0448809 -1.6465216 -1.6048193 -1.476037 -1.5839305 -1.7380362 -2.3998866 -2.8386717 -3.4333813 -3.8988883][-1.5529923 -1.1602073 -0.93790817 -1.1080713 -1.0885615 -0.80099106 -0.38678122 -0.23327589 -0.094459057 -0.46749115 -0.66587591 -1.322598 -1.9231887 -2.6017809 -3.327455][-0.83947468 -0.39560556 0.16772842 0.11820316 0.18702745 0.15955544 0.14501476 0.29892588 0.54638958 0.095215321 -0.35617256 -1.1154585 -1.6986113 -2.365252 -3.1449525][-0.73895741 0.22059011 0.55216217 0.47009087 0.91678143 1.1543941 1.3100834 1.383956 1.4929285 0.741868 0.022330284 -0.81142044 -1.629396 -2.3772559 -3.0838819][-0.90116215 -0.061487198 0.48591852 0.67721224 0.83499241 0.99760962 1.1576529 1.2350693 1.2327147 0.2667737 -0.57840967 -1.665256 -2.5649729 -3.2222812 -3.8680131][-0.96754885 -0.52262878 -0.032612324 0.30593777 0.67371845 0.62661409 0.64912653 0.74568176 0.74803829 -0.30063772 -1.1111846 -2.2653823 -3.1551085 -4.03405 -4.7801352][-0.66636848 -0.45923138 -0.4898057 -0.37564659 -0.079287529 0.07118988 0.30663395 0.36762428 0.30545521 -0.84423923 -1.5410705 -2.6122828 -3.5014102 -4.325263 -5.1747174][-1.6360431 -1.4887872 -1.4715009 -1.45888 -1.399744 -1.1794062 -1.0545163 -1.2030234 -1.564126 -2.4588165 -2.9805574 -3.7687442 -4.5616083 -5.1956005 -5.3996062][-3.1269412 -2.6290998 -2.4387951 -2.4068971 -2.4051771 -2.4821668 -2.3739619 -2.612143 -2.9821892 -3.8672364 -4.2159877 -4.5677581 -4.9708948 -5.2935524 -5.4497986][-4.0050554 -3.5568278 -3.212013 -3.1124432 -3.1692069 -3.1645408 -3.3910289 -3.8495426 -4.0370674 -4.6821718 -4.8135872 -5.1551871 -5.4609637 -5.72138 -5.8364744][-4.7266579 -4.5035944 -4.13958 -3.8743327 -3.6036806 -3.6094892 -3.9693024 -4.4133797 -4.7459373 -5.14008 -5.163043 -5.2955961 -5.5801687 -5.8377028 -6.0264177][-5.2858386 -4.8635311 -4.729857 -4.481648 -4.095871 -4.088798 -4.4041495 -4.6035295 -4.6735325 -4.9876895 -5.1200809 -5.5135708 -6.0427728 -6.1813774 -6.0791907][-6.2113724 -5.6762815 -5.3553972 -5.1498489 -4.8878889 -4.8357582 -5.0941324 -5.3504581 -5.5221395 -5.5998659 -5.572629 -5.7573552 -5.8783045 -5.9487228 -5.96733]]...]
INFO - root - 2017-12-15 11:01:06.938299: step 4210, loss = 0.41, batch loss = 0.28 (12.2 examples/sec; 0.656 sec/batch; 59h:47m:47s remains)
INFO - root - 2017-12-15 11:01:13.327388: step 4220, loss = 0.43, batch loss = 0.30 (12.8 examples/sec; 0.623 sec/batch; 56h:47m:33s remains)
INFO - root - 2017-12-15 11:01:19.748739: step 4230, loss = 0.40, batch loss = 0.27 (12.6 examples/sec; 0.633 sec/batch; 57h:44m:58s remains)
INFO - root - 2017-12-15 11:01:26.193787: step 4240, loss = 0.45, batch loss = 0.32 (12.1 examples/sec; 0.664 sec/batch; 60h:30m:41s remains)
INFO - root - 2017-12-15 11:01:32.525142: step 4250, loss = 0.43, batch loss = 0.30 (12.7 examples/sec; 0.631 sec/batch; 57h:33m:43s remains)
INFO - root - 2017-12-15 11:01:38.849578: step 4260, loss = 0.45, batch loss = 0.32 (12.8 examples/sec; 0.625 sec/batch; 56h:57m:47s remains)
INFO - root - 2017-12-15 11:01:45.302131: step 4270, loss = 0.39, batch loss = 0.26 (12.2 examples/sec; 0.655 sec/batch; 59h:42m:22s remains)
INFO - root - 2017-12-15 11:01:51.765639: step 4280, loss = 0.43, batch loss = 0.30 (11.9 examples/sec; 0.674 sec/batch; 61h:25m:28s remains)
INFO - root - 2017-12-15 11:01:58.236835: step 4290, loss = 0.37, batch loss = 0.24 (12.3 examples/sec; 0.653 sec/batch; 59h:30m:44s remains)
INFO - root - 2017-12-15 11:02:04.622546: step 4300, loss = 0.40, batch loss = 0.28 (12.4 examples/sec; 0.645 sec/batch; 58h:49m:58s remains)
2017-12-15 11:02:05.218607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.026865 -1.1127095 -1.1784086 -1.1586161 -1.155232 -1.2328534 -1.4086013 -1.4744301 -1.6444373 -2.1173797 -2.5436687 -3.1946149 -3.8842785 -4.5011654 -5.0460224][-0.6601 -0.93116474 -1.2094479 -1.2529254 -1.3166313 -1.2918715 -1.2708669 -1.3909235 -1.3939304 -1.7038341 -2.2274389 -2.9960718 -3.5956163 -4.3058186 -5.0452719][-0.46494198 -0.6955471 -0.90620661 -0.95829678 -0.94769573 -0.80049276 -0.67842627 -0.62507105 -0.57798481 -0.87840557 -1.4068122 -2.1934781 -2.8854976 -3.698216 -4.4263849][-0.16900444 -0.14037466 -0.19092751 -0.23040485 -0.21498585 -0.0032815933 0.11730909 0.18909597 0.33892727 -0.023200989 -0.64912367 -1.5462375 -2.3413963 -3.2419372 -3.9689691][0.10633755 0.057257175 0.036158562 -0.0054244995 -0.063910007 0.23123121 0.56895542 0.70463705 0.864521 0.56944084 0.02801609 -0.87194538 -1.8255897 -2.9396424 -3.8942084][-0.0907011 0.04570961 0.28287745 0.45680809 0.48604631 0.7799468 1.1329532 1.2470908 1.3359056 0.88742256 0.26474762 -0.60567951 -1.5633483 -2.7144527 -3.7353923][-0.29054451 0.0070061684 0.36999369 0.65785313 0.90232992 1.1833978 1.5237966 1.6277757 1.6643119 1.1370149 0.43314362 -0.5510726 -1.5974298 -2.7951884 -3.8817844][-0.14323473 0.16494846 0.41674995 0.88048887 1.3656154 1.6584935 1.9306188 2.0053144 2.0004535 1.3744922 0.57728004 -0.47682333 -1.6240363 -2.9226017 -4.0551386][-0.16133642 0.12351322 0.48419762 1.0370193 1.5343375 1.8966765 2.1459146 2.1913314 2.1742406 1.4728217 0.56661749 -0.53237438 -1.6840019 -2.9909244 -4.0409651][-0.89058256 -0.42184544 -0.060309887 0.45541048 1.0233235 1.3858199 1.4936461 1.5667796 1.5527453 0.80380726 -0.11421156 -1.1055808 -2.1827631 -3.3415163 -4.2751255][-1.9841623 -1.4950171 -1.096467 -0.67929745 -0.14064407 0.25990105 0.30144358 0.35393095 0.32573271 -0.38589239 -1.2059259 -2.0631008 -2.8211122 -3.7353084 -4.4242859][-3.0868611 -2.586782 -2.1626945 -1.8009858 -1.3595777 -0.99113846 -0.98429537 -0.98585033 -1.0213852 -1.4662948 -2.0695224 -2.6629286 -3.2540412 -3.8839254 -4.3306875][-3.9927993 -3.5238931 -3.0600686 -2.7725549 -2.4724002 -2.1567397 -2.1934767 -2.22545 -2.2543731 -2.5446434 -2.8718462 -3.2902 -3.5921433 -4.0135512 -4.2699785][-4.8832264 -4.407877 -3.9760797 -3.7177846 -3.5216691 -3.3312438 -3.422219 -3.4880583 -3.55653 -3.6621692 -3.8128507 -3.9340892 -4.05777 -4.1165972 -4.1833649][-5.7797365 -5.2751303 -4.9145765 -4.7079144 -4.5846491 -4.4436741 -4.5766754 -4.7282596 -4.852375 -4.8836317 -4.8545175 -4.7290297 -4.5982533 -4.5136137 -4.3739862]]...]
INFO - root - 2017-12-15 11:02:11.743525: step 4310, loss = 0.43, batch loss = 0.31 (12.7 examples/sec; 0.628 sec/batch; 57h:12m:46s remains)
INFO - root - 2017-12-15 11:02:18.239851: step 4320, loss = 0.36, batch loss = 0.23 (12.6 examples/sec; 0.636 sec/batch; 57h:56m:28s remains)
INFO - root - 2017-12-15 11:02:24.709626: step 4330, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.639 sec/batch; 58h:13m:49s remains)
INFO - root - 2017-12-15 11:02:31.203478: step 4340, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 58h:11m:36s remains)
INFO - root - 2017-12-15 11:02:37.636544: step 4350, loss = 0.34, batch loss = 0.21 (12.3 examples/sec; 0.652 sec/batch; 59h:25m:24s remains)
INFO - root - 2017-12-15 11:02:44.022127: step 4360, loss = 0.40, batch loss = 0.27 (12.1 examples/sec; 0.659 sec/batch; 60h:06m:10s remains)
INFO - root - 2017-12-15 11:02:50.459776: step 4370, loss = 0.39, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 57h:31m:11s remains)
INFO - root - 2017-12-15 11:02:56.802448: step 4380, loss = 0.44, batch loss = 0.31 (12.7 examples/sec; 0.632 sec/batch; 57h:36m:05s remains)
INFO - root - 2017-12-15 11:03:03.159170: step 4390, loss = 0.48, batch loss = 0.35 (12.5 examples/sec; 0.640 sec/batch; 58h:21m:16s remains)
INFO - root - 2017-12-15 11:03:09.594231: step 4400, loss = 0.41, batch loss = 0.28 (12.5 examples/sec; 0.640 sec/batch; 58h:20m:22s remains)
2017-12-15 11:03:10.083816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2987385 -4.2474494 -4.2600408 -4.2067909 -4.0367613 -4.1902561 -4.3095574 -4.1635056 -3.7942939 -3.95085 -4.0176086 -4.8243427 -5.8606796 -6.4533706 -6.8429046][-3.4353702 -3.7733762 -4.0438652 -4.384079 -4.2668538 -4.0982976 -4.0538459 -4.1119719 -3.8489082 -4.0896721 -4.2992206 -4.921833 -5.5539155 -6.1564889 -6.7598429][-2.6316476 -2.6736436 -3.1573172 -3.6124775 -3.5571842 -3.4993563 -3.4253466 -3.1937211 -2.8462968 -3.1201103 -3.2668924 -4.1434746 -4.9559784 -5.4651432 -5.936202][-1.6578922 -1.811286 -2.3134003 -2.5846386 -2.5099235 -2.4055958 -2.2737517 -2.1824784 -2.0331888 -2.4310493 -2.7748113 -3.4080355 -4.1212111 -4.8404379 -5.3805113][-1.7577314 -1.7712412 -1.9279284 -1.8599119 -1.5645285 -1.0695281 -0.76091194 -0.570312 -0.3998661 -0.99728394 -1.5802398 -2.5593119 -3.5343623 -4.4688644 -5.1242609][-2.100924 -1.9267302 -1.8519392 -1.3470216 -0.60859632 0.24333715 0.84099913 1.1248999 1.2180223 0.34147406 -0.36404085 -1.4157138 -2.5510087 -3.6173108 -4.40023][-2.2643995 -1.7151217 -1.2981071 -0.59031916 0.21461058 1.1502905 1.93818 2.3192225 2.3612633 1.0171843 0.061448574 -1.1894889 -2.181808 -3.158092 -3.8948896][-2.5646234 -2.0393133 -1.5389099 -0.54750776 0.61986923 1.6341753 2.3007374 2.4596729 2.4360666 1.214488 0.22878361 -0.971261 -1.8378248 -2.7445145 -3.5620465][-3.1744111 -2.7721639 -2.3818893 -1.2777371 -0.13434267 0.92750549 1.6753645 1.7359371 1.6706209 0.40825415 -0.57435322 -1.5883107 -2.1601715 -2.950613 -3.5239758][-3.6498806 -3.4338977 -3.1047783 -2.4364648 -1.6385427 -0.71680593 -0.046884537 0.1213212 0.22727346 -0.80903387 -1.5784125 -2.4199343 -3.1097469 -3.6442225 -3.9080081][-4.258317 -3.9490969 -3.5991211 -3.2145104 -2.7937198 -2.2411981 -1.7446818 -1.4416795 -1.2659216 -2.1082673 -2.8646536 -3.61455 -4.0778995 -4.3045177 -4.610806][-4.7954044 -4.4799547 -4.2130327 -3.9911113 -3.8155522 -3.6216943 -3.310997 -3.1966131 -2.9559598 -3.5303309 -3.8994095 -4.2365222 -4.3352575 -4.4922829 -4.7303028][-5.143734 -4.8248787 -4.585886 -4.4939556 -4.3892918 -4.4082823 -4.2427263 -4.1593428 -4.0654268 -4.4120188 -4.4442191 -4.6416721 -4.8456917 -4.8563361 -4.6168528][-5.6948447 -5.4158807 -5.2481804 -5.1597137 -5.0089006 -4.9019938 -4.7042837 -4.6419525 -4.5103922 -4.6695318 -4.81495 -4.883215 -4.9793248 -5.1851821 -5.1333818][-6.1946874 -6.0463543 -5.936142 -5.778738 -5.5224915 -5.5203443 -5.3985949 -5.3473125 -5.3840442 -5.44468 -5.251955 -5.1495819 -5.2425318 -5.1086187 -4.9073391]]...]
INFO - root - 2017-12-15 11:03:16.529898: step 4410, loss = 0.40, batch loss = 0.28 (12.6 examples/sec; 0.633 sec/batch; 57h:39m:07s remains)
INFO - root - 2017-12-15 11:03:22.987771: step 4420, loss = 0.37, batch loss = 0.24 (11.9 examples/sec; 0.673 sec/batch; 61h:19m:32s remains)
INFO - root - 2017-12-15 11:03:29.502196: step 4430, loss = 0.41, batch loss = 0.29 (12.4 examples/sec; 0.646 sec/batch; 58h:50m:33s remains)
INFO - root - 2017-12-15 11:03:35.873715: step 4440, loss = 0.49, batch loss = 0.36 (12.6 examples/sec; 0.633 sec/batch; 57h:38m:23s remains)
INFO - root - 2017-12-15 11:03:42.284908: step 4450, loss = 0.48, batch loss = 0.36 (12.7 examples/sec; 0.627 sec/batch; 57h:10m:48s remains)
INFO - root - 2017-12-15 11:03:48.643834: step 4460, loss = 0.39, batch loss = 0.26 (12.4 examples/sec; 0.644 sec/batch; 58h:43m:40s remains)
INFO - root - 2017-12-15 11:03:55.017905: step 4470, loss = 0.36, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 59h:35m:08s remains)
INFO - root - 2017-12-15 11:04:01.412281: step 4480, loss = 0.41, batch loss = 0.28 (12.4 examples/sec; 0.647 sec/batch; 58h:57m:13s remains)
INFO - root - 2017-12-15 11:04:07.820226: step 4490, loss = 0.45, batch loss = 0.32 (12.2 examples/sec; 0.654 sec/batch; 59h:34m:05s remains)
INFO - root - 2017-12-15 11:04:14.300817: step 4500, loss = 0.45, batch loss = 0.33 (12.5 examples/sec; 0.638 sec/batch; 58h:08m:52s remains)
2017-12-15 11:04:14.788828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5516732 -3.6862097 -3.6421478 -3.8544035 -4.0874758 -3.8095205 -3.165844 -2.0614724 -1.375906 -1.5894165 -2.0662441 -2.4619069 -2.8986263 -3.8989921 -5.125][-4.2115293 -4.8866329 -5.0710888 -5.484056 -5.3911414 -4.4055347 -3.5949266 -2.9799981 -2.315835 -2.0387936 -2.2306089 -2.6995459 -3.5031023 -4.1031609 -4.7291708][-4.5025716 -4.054769 -3.6866639 -4.2917337 -4.4614983 -4.1056185 -3.3016975 -2.3521385 -1.5786743 -1.5174732 -1.7962136 -2.3096442 -2.7919044 -3.3247168 -4.1372757][-3.2260859 -3.5286989 -3.7354081 -3.4281907 -3.0493474 -2.6331115 -1.7138882 -1.2959695 -1.006073 -0.91424513 -1.2934594 -1.9441772 -2.752326 -3.2417104 -4.1594687][-3.1093736 -2.6285787 -1.932158 -2.1276164 -1.9797044 -1.2839036 -0.22879076 0.47836781 1.0685015 0.31653833 -0.84932947 -1.8614521 -2.93506 -3.5043945 -4.1794176][-2.4341216 -1.8889217 -1.3194818 -0.91554403 -0.22408581 0.62442017 1.6658239 1.9863958 2.2107038 1.274106 0.10152054 -1.3443222 -2.7981954 -3.7865481 -4.7085967][-2.7609386 -2.0693026 -1.4276433 -0.56801796 0.30597448 1.2677765 2.5599327 2.8061552 2.8398905 1.8923893 0.62572289 -0.87600279 -2.5447702 -3.7334757 -4.7329893][-3.3445644 -2.5435195 -1.6562037 -0.63957787 0.12232208 0.88937569 1.809083 2.4171944 2.8789158 1.5286064 0.13344765 -1.2305436 -2.6808414 -3.6878743 -4.836235][-3.9080219 -3.4791219 -3.1294923 -2.0097761 -1.028583 -0.14578009 0.7323637 1.0415058 1.3672295 0.75963306 -0.24307775 -1.6138425 -2.8452096 -3.6231165 -4.4187932][-4.6201811 -4.1556787 -3.7561309 -3.3839164 -2.9255381 -2.1338978 -1.3851337 -0.65151215 0.12036657 -0.91176081 -2.0398679 -2.8427138 -3.5116928 -4.3056717 -5.0560026][-4.8874087 -4.4089346 -4.2360191 -4.0289283 -3.6190298 -3.2382684 -2.8522019 -2.6051755 -2.5594392 -2.8066902 -3.1119428 -3.5484896 -4.2021875 -5.049108 -5.7902508][-5.201807 -4.62809 -4.2084484 -4.27657 -4.2615905 -4.3190365 -4.1910143 -4.0719738 -3.9166815 -4.5529146 -4.9805574 -5.010705 -5.1452923 -5.306181 -5.6098433][-4.8703241 -4.6107435 -4.533814 -4.4394746 -4.4278116 -4.5594091 -4.7529988 -4.8823566 -4.91049 -5.0217495 -5.3317451 -5.3668947 -5.5512123 -5.7454739 -5.9973164][-4.7307405 -4.1780634 -3.8814232 -3.8174455 -3.7475586 -3.8655555 -3.9852982 -4.2662024 -4.5322614 -5.0111771 -5.6004834 -5.8492942 -6.0762625 -5.8383007 -5.8379769][-5.128418 -4.7110558 -4.2440891 -4.06769 -3.9440339 -3.9341846 -4.1413755 -4.4705563 -4.8697824 -5.2468915 -5.5187473 -5.8158908 -6.159029 -6.5353804 -6.6369]]...]
INFO - root - 2017-12-15 11:04:21.188353: step 4510, loss = 0.45, batch loss = 0.32 (12.6 examples/sec; 0.633 sec/batch; 57h:42m:25s remains)
INFO - root - 2017-12-15 11:04:27.595191: step 4520, loss = 0.42, batch loss = 0.29 (12.4 examples/sec; 0.647 sec/batch; 58h:59m:17s remains)
INFO - root - 2017-12-15 11:04:34.071165: step 4530, loss = 0.34, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 57h:51m:03s remains)
INFO - root - 2017-12-15 11:04:40.512389: step 4540, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 58h:46m:59s remains)
INFO - root - 2017-12-15 11:04:46.962142: step 4550, loss = 0.40, batch loss = 0.28 (12.6 examples/sec; 0.637 sec/batch; 57h:59m:24s remains)
INFO - root - 2017-12-15 11:04:53.320900: step 4560, loss = 0.37, batch loss = 0.24 (12.4 examples/sec; 0.645 sec/batch; 58h:44m:04s remains)
INFO - root - 2017-12-15 11:04:59.722458: step 4570, loss = 0.42, batch loss = 0.29 (12.7 examples/sec; 0.628 sec/batch; 57h:12m:27s remains)
INFO - root - 2017-12-15 11:05:06.158518: step 4580, loss = 0.60, batch loss = 0.47 (12.6 examples/sec; 0.637 sec/batch; 58h:01m:16s remains)
INFO - root - 2017-12-15 11:05:12.701252: step 4590, loss = 0.36, batch loss = 0.23 (11.7 examples/sec; 0.685 sec/batch; 62h:21m:13s remains)
INFO - root - 2017-12-15 11:05:19.168639: step 4600, loss = 0.42, batch loss = 0.30 (12.4 examples/sec; 0.647 sec/batch; 58h:57m:51s remains)
2017-12-15 11:05:19.673913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8219566 -3.9250615 -4.0911818 -4.1350265 -4.2055759 -4.57689 -4.5549474 -4.2010365 -3.4770019 -3.1683536 -2.9194756 -3.5481918 -4.2683287 -4.5395517 -5.2262464][-3.7949979 -3.8321021 -3.9763989 -4.0829053 -4.46116 -4.8175077 -4.6844411 -4.5409389 -3.800185 -3.5263057 -3.5199761 -3.9281347 -4.3005834 -4.4563103 -5.0761943][-3.5149372 -3.7256019 -3.7680006 -3.7690425 -3.8413439 -3.7823532 -3.5349252 -3.24184 -2.8266306 -3.284688 -3.6771021 -3.9731586 -4.1692591 -4.1140051 -4.4040475][-2.6319919 -2.7936759 -2.9766297 -2.649497 -2.333632 -2.2957997 -2.1343613 -1.6953282 -1.4442501 -2.0775781 -2.6866913 -3.5737696 -4.2859888 -4.1904249 -4.4267311][-1.239697 -0.78733206 -0.81171227 -0.67653227 -0.41430759 -0.044927597 0.19233704 0.24967003 0.31797981 -0.19411516 -1.2296391 -2.6406765 -3.6141012 -4.1969242 -4.6754956][-0.52176428 -0.055286884 0.513989 0.57413387 0.65600967 1.2610078 1.8172307 1.5257602 1.1912351 0.13688803 -1.0520554 -2.3499279 -3.4709973 -4.1237049 -4.5869246][-0.49532843 0.55477285 1.2393918 1.3559909 1.8833032 2.286283 2.2390275 2.0024352 1.685452 0.27327013 -1.2334194 -2.5182166 -3.929354 -4.5561147 -5.0761423][-0.99153566 0.078199863 0.95486784 1.498611 2.0253654 2.383677 2.7878633 2.5391297 1.7028317 0.31436348 -1.2752185 -2.8099384 -4.1750841 -4.9041562 -5.4473081][-1.7637782 -1.4118848 -0.88952017 -0.10236502 0.70633411 1.5601935 2.1819625 1.8570409 1.4020391 0.19370222 -1.6678066 -3.018713 -4.051239 -4.6307859 -5.1163778][-4.1525974 -3.6535482 -3.1090174 -2.1757479 -1.2117519 -0.6118722 -0.12997818 0.43886328 0.42403316 -0.86084652 -2.5526052 -3.5904124 -4.5997963 -5.0955696 -5.2724175][-5.2798557 -4.824132 -4.2357845 -3.8032622 -3.3686383 -3.004724 -2.4177332 -1.8821993 -1.6384044 -2.3866191 -3.2265806 -3.6617496 -4.3218374 -4.9735031 -5.5723553][-5.9169307 -5.6758423 -5.3153257 -4.8993688 -4.4656129 -4.071 -3.5553906 -3.2327681 -3.0543733 -3.5707858 -3.9448125 -4.2384014 -4.695817 -4.9842744 -5.5175657][-5.8382015 -5.789896 -5.5195789 -5.0037441 -4.6069288 -4.2796936 -3.4329162 -2.8761692 -2.979084 -3.5347161 -3.9746692 -4.2589459 -4.9402881 -5.29586 -5.4774246][-5.0557637 -4.9967766 -4.9525385 -4.5903835 -4.2738733 -3.9247084 -4.0184803 -4.0175705 -3.7554345 -3.6355884 -3.8235097 -4.0735874 -4.7470255 -5.1964092 -5.7006607][-5.3705873 -4.8077106 -4.4945288 -4.2416744 -3.9170041 -3.9429367 -4.1235576 -4.5035944 -4.6233282 -4.7122955 -4.6853848 -4.9527435 -5.177453 -5.1827126 -5.3551607]]...]
INFO - root - 2017-12-15 11:05:26.083045: step 4610, loss = 0.36, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 58h:28m:22s remains)
INFO - root - 2017-12-15 11:05:32.574170: step 4620, loss = 0.34, batch loss = 0.22 (12.8 examples/sec; 0.623 sec/batch; 56h:46m:22s remains)
INFO - root - 2017-12-15 11:05:39.004306: step 4630, loss = 0.45, batch loss = 0.33 (12.0 examples/sec; 0.664 sec/batch; 60h:29m:07s remains)
INFO - root - 2017-12-15 11:05:45.409988: step 4640, loss = 0.42, batch loss = 0.29 (11.9 examples/sec; 0.673 sec/batch; 61h:16m:57s remains)
INFO - root - 2017-12-15 11:05:51.991522: step 4650, loss = 0.38, batch loss = 0.26 (12.5 examples/sec; 0.643 sec/batch; 58h:30m:47s remains)
INFO - root - 2017-12-15 11:05:58.414135: step 4660, loss = 0.43, batch loss = 0.31 (12.3 examples/sec; 0.649 sec/batch; 59h:04m:06s remains)
INFO - root - 2017-12-15 11:06:04.794065: step 4670, loss = 0.39, batch loss = 0.26 (12.5 examples/sec; 0.642 sec/batch; 58h:28m:48s remains)
INFO - root - 2017-12-15 11:06:11.245627: step 4680, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 58h:01m:56s remains)
INFO - root - 2017-12-15 11:06:17.695372: step 4690, loss = 0.45, batch loss = 0.33 (12.5 examples/sec; 0.641 sec/batch; 58h:22m:09s remains)
INFO - root - 2017-12-15 11:06:24.095375: step 4700, loss = 0.35, batch loss = 0.23 (12.3 examples/sec; 0.651 sec/batch; 59h:15m:18s remains)
2017-12-15 11:06:24.621986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5325708 -6.6006308 -8.3900232 -9.6778965 -10.406497 -9.9445772 -8.4114361 -6.3253446 -4.6240406 -4.5277319 -4.83677 -5.9978638 -5.986311 -6.4783468 -7.5287762][-5.3211408 -6.8172512 -8.120595 -8.6912813 -8.9151421 -8.3566742 -6.7474003 -5.6191897 -4.2037334 -4.2968369 -5.0042739 -5.3110838 -5.5539265 -6.4548206 -7.3377743][-4.050581 -5.282649 -6.7204041 -7.3651276 -7.3531175 -6.245429 -4.4579372 -2.9573221 -1.8681283 -2.7831817 -3.401968 -4.6630087 -4.8455954 -5.44917 -6.841929][-3.1459937 -4.0775843 -4.9883795 -5.7246103 -6.3197575 -5.6855588 -4.2279973 -2.4146523 -0.62473965 -0.920002 -2.1769285 -3.4541922 -4.1651669 -5.503685 -7.1170321][-2.7056055 -3.3297 -4.121942 -3.6840184 -3.7280645 -3.3744867 -2.7994933 -1.5839596 -0.32722378 -1.8177514 -2.9958625 -4.1081266 -5.17853 -6.1109152 -7.3657041][-3.5573099 -3.6651692 -3.1108747 -2.8706603 -1.8080249 -0.10227776 1.2367163 1.8229523 2.1798487 0.28132915 -1.7844586 -4.5896349 -6.1745372 -7.5106807 -8.3252172][-2.5547194 -1.7311339 -0.7094183 0.5988183 2.2247334 4.0356979 6.228404 6.6580696 7.0644865 4.0615349 0.34594536 -2.6023545 -4.7026072 -7.426414 -8.8606262][-2.3003922 -1.2166009 0.13430738 2.1789284 3.6778278 5.4918604 7.4282055 8.0294447 7.2895708 3.9308844 0.70435429 -2.6459303 -4.7392716 -6.8337455 -7.9768634][-1.3983521 -1.0966692 -0.58386993 1.2110338 2.6016674 3.5036745 4.9475536 5.2646656 5.1293278 2.6443033 -0.70910549 -3.9421251 -5.2985716 -6.7495856 -7.9386964][-2.9632511 -2.5847006 -2.1679482 -1.1601663 0.026515007 1.3694096 2.0219698 1.8686628 2.0250225 -0.6509943 -2.3714652 -4.7387228 -6.2832365 -7.0637326 -7.0178847][-5.5224686 -5.3627119 -4.7743568 -4.2643223 -3.1311712 -2.5866055 -2.0402713 -2.24189 -3.0294051 -4.9205465 -5.882287 -6.9699287 -7.7038035 -7.904592 -7.8044529][-7.4046316 -6.6396208 -5.6301618 -5.3799376 -5.1671906 -4.9731483 -4.3725939 -5.3558826 -6.0054951 -8.1819382 -9.3894987 -9.7607784 -9.9184923 -9.2593918 -9.2303171][-8.1007185 -7.18908 -6.2684774 -5.5145607 -5.4975438 -5.489368 -5.3997836 -6.1067386 -6.6344881 -7.65494 -8.1871243 -8.6872787 -9.3864336 -8.8238325 -8.9852295][-8.24771 -7.9386487 -7.5114818 -6.4841323 -6.0529513 -5.5859194 -5.6428876 -5.9815164 -7.0427079 -8.1486959 -9.03037 -8.643177 -8.5185747 -8.2848206 -8.5189762][-8.0974016 -8.0590611 -7.944726 -7.8830118 -7.5489578 -7.1648793 -7.12551 -7.0863013 -7.7040563 -8.4283609 -8.9499435 -8.8871384 -8.8558445 -8.1825495 -7.4867725]]...]
INFO - root - 2017-12-15 11:06:31.037082: step 4710, loss = 0.39, batch loss = 0.27 (12.2 examples/sec; 0.654 sec/batch; 59h:34m:15s remains)
INFO - root - 2017-12-15 11:06:37.418777: step 4720, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 59h:11m:13s remains)
INFO - root - 2017-12-15 11:06:43.857713: step 4730, loss = 0.37, batch loss = 0.25 (12.4 examples/sec; 0.643 sec/batch; 58h:32m:22s remains)
INFO - root - 2017-12-15 11:06:50.235279: step 4740, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 58h:05m:28s remains)
INFO - root - 2017-12-15 11:06:56.647998: step 4750, loss = 0.41, batch loss = 0.29 (12.7 examples/sec; 0.632 sec/batch; 57h:29m:39s remains)
INFO - root - 2017-12-15 11:07:03.065565: step 4760, loss = 0.33, batch loss = 0.21 (11.9 examples/sec; 0.673 sec/batch; 61h:16m:46s remains)
INFO - root - 2017-12-15 11:07:09.429208: step 4770, loss = 0.40, batch loss = 0.27 (12.3 examples/sec; 0.650 sec/batch; 59h:12m:25s remains)
INFO - root - 2017-12-15 11:07:15.856098: step 4780, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 58h:20m:30s remains)
INFO - root - 2017-12-15 11:07:22.308606: step 4790, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 57h:28m:28s remains)
INFO - root - 2017-12-15 11:07:28.763475: step 4800, loss = 0.46, batch loss = 0.34 (12.1 examples/sec; 0.659 sec/batch; 60h:00m:43s remains)
2017-12-15 11:07:29.212662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5763178 -2.3968039 -2.6480298 -2.9633355 -3.4135206 -3.2495508 -2.5542574 -2.1953239 -2.3278556 -2.7854786 -3.0595331 -4.0481873 -5.1409068 -5.3745642 -6.0108185][-3.0150833 -2.7610183 -2.9790287 -2.8328023 -2.7919126 -2.8624244 -2.5588613 -2.4225826 -1.8839593 -2.2755752 -2.9043679 -4.0659447 -4.9062452 -5.0831633 -5.6453552][-3.0039921 -2.6605053 -2.7964449 -2.5131226 -2.2332268 -1.8885994 -1.3132968 -1.192121 -1.0453897 -1.5286918 -1.9507647 -2.936285 -3.9663162 -4.4089251 -4.8756523][-2.1794891 -2.22328 -2.4208455 -2.3714056 -1.9693356 -1.11234 -0.29260874 -0.18875027 -0.029361725 -0.69250822 -1.2904005 -2.5491982 -3.6465967 -4.2598662 -5.2588711][-2.3005528 -1.8314719 -1.7226248 -1.4973407 -1.2871041 -0.74936056 0.064916134 0.89250326 1.4413342 0.5315876 -0.54228354 -2.1273451 -3.4651928 -4.0985432 -4.9746647][-2.2580156 -2.3040543 -2.0030112 -1.0950923 -0.15161324 0.64473629 1.6762767 2.2675471 2.4081273 1.7168174 1.1072869 -0.59010077 -2.6450381 -3.7654593 -4.6545286][-1.704607 -1.4723048 -1.059597 -0.36742783 0.70004511 1.8854547 2.6710505 3.2057757 3.5916076 2.9970975 2.1739535 0.38834333 -1.4903851 -2.7517438 -4.2854357][-1.6319952 -0.87464428 -0.33491564 0.66540575 1.5716987 2.3240685 3.3470111 3.7218375 3.690639 2.8140054 1.7610173 0.11934185 -1.9285975 -3.013968 -3.9871142][-1.4522023 -0.967381 0.036818981 0.94864225 1.5898223 2.2129436 2.7706532 2.9168572 2.8618827 1.9679284 1.0031819 -0.28364754 -2.4482913 -3.5163076 -4.73108][-1.4975739 -1.2539415 -0.67233849 -0.20914364 0.2462182 0.81638861 1.3775306 1.6893554 1.9329715 1.1095309 -0.083500385 -1.4758081 -3.1727839 -4.0075655 -5.131062][-2.4104214 -1.7305851 -1.5885692 -1.3519788 -1.1259437 -0.84391165 -0.69783115 -0.57219791 -0.2968421 -0.8491993 -1.5423441 -2.66506 -3.9604023 -4.7140493 -4.9587803][-3.3881924 -3.0688987 -2.8786397 -2.6499062 -2.4613986 -2.2422886 -2.1785102 -2.3658628 -2.4098382 -2.9971938 -3.6015923 -4.4049721 -5.5066962 -5.7514534 -5.7567673][-3.5549672 -3.77793 -3.9914341 -3.9365838 -3.6413865 -3.2965717 -3.2601547 -3.4696748 -3.5990653 -4.3682547 -5.0414991 -5.5328488 -5.9893141 -6.0244575 -6.100358][-4.3655148 -4.2446928 -4.6956167 -4.9569626 -4.7070265 -4.6236987 -4.4684925 -4.5475426 -4.7859364 -5.1718974 -5.3918176 -5.8359323 -6.3041325 -6.3873882 -6.3004351][-5.0299921 -5.2249985 -5.5348949 -5.4999967 -5.7390656 -5.7172589 -5.4890804 -5.6889987 -6.0196905 -6.1942577 -6.216485 -6.4649272 -6.5188107 -6.3089051 -6.0983939]]...]
INFO - root - 2017-12-15 11:07:35.565898: step 4810, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.630 sec/batch; 57h:19m:40s remains)
INFO - root - 2017-12-15 11:07:41.953985: step 4820, loss = 0.36, batch loss = 0.24 (12.3 examples/sec; 0.651 sec/batch; 59h:18m:02s remains)
INFO - root - 2017-12-15 11:07:48.310992: step 4830, loss = 0.31, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 56h:43m:51s remains)
INFO - root - 2017-12-15 11:07:54.698470: step 4840, loss = 0.36, batch loss = 0.24 (12.4 examples/sec; 0.646 sec/batch; 58h:49m:32s remains)
INFO - root - 2017-12-15 11:08:01.120166: step 4850, loss = 0.35, batch loss = 0.23 (12.2 examples/sec; 0.655 sec/batch; 59h:39m:10s remains)
INFO - root - 2017-12-15 11:08:07.522923: step 4860, loss = 0.36, batch loss = 0.24 (12.2 examples/sec; 0.658 sec/batch; 59h:55m:20s remains)
INFO - root - 2017-12-15 11:08:13.859658: step 4870, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 58h:06m:21s remains)
INFO - root - 2017-12-15 11:08:20.238964: step 4880, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 58h:46m:35s remains)
INFO - root - 2017-12-15 11:08:26.699373: step 4890, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.635 sec/batch; 57h:45m:11s remains)
INFO - root - 2017-12-15 11:08:33.162284: step 4900, loss = 0.36, batch loss = 0.24 (12.4 examples/sec; 0.643 sec/batch; 58h:29m:31s remains)
2017-12-15 11:08:33.655343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2162733 -1.4207802 -1.6010575 -1.7130189 -1.81393 -1.8701334 -1.8098941 -1.7729087 -1.5407019 -2.4006777 -2.8661909 -3.4629531 -4.2476578 -4.6439867 -5.2747126][-1.6452079 -1.7618699 -1.9291649 -2.2460461 -2.520793 -2.5562077 -2.4838891 -2.0533342 -1.5880723 -2.8482695 -3.5010958 -4.06822 -4.9949455 -5.2426939 -5.7277112][-2.1533403 -2.1349249 -2.2595625 -2.2010636 -1.9476275 -2.0241132 -2.0269141 -1.8734989 -1.7762265 -2.9105453 -3.4106064 -4.2837529 -5.4111667 -5.770072 -6.132977][-2.8907571 -2.9107828 -2.7993965 -2.359199 -1.8663321 -1.4177947 -0.89951086 -0.64369631 -0.52476311 -1.8458676 -2.7239618 -3.6377993 -4.9845309 -5.6617069 -6.1038623][-3.6155286 -3.1118517 -2.6647735 -2.16744 -1.4157677 -0.53290796 0.33267593 0.61667871 0.80478191 -0.46076918 -1.4031525 -2.6138644 -4.0871305 -4.9479437 -5.70547][-4.2601705 -3.5315592 -2.8516045 -1.9473438 -0.92203 0.301826 1.6052208 2.0244484 2.2211256 0.71735668 -0.49035788 -1.915081 -3.5746894 -4.4259977 -5.2552948][-4.0395637 -3.3276663 -2.5488415 -1.4552236 -0.07151556 1.3981876 2.7715235 3.2623653 3.6732597 1.8823128 0.26589727 -1.1158662 -2.6068368 -3.5001197 -4.3478928][-4.0578418 -3.0203547 -2.1372619 -0.90335894 0.47600079 1.6173687 2.81081 3.4959154 3.9568977 2.1434498 0.71599388 -0.74297667 -2.1173472 -2.7422137 -3.3967462][-3.8088331 -3.0479951 -2.2911811 -0.95413065 0.40506935 1.4295807 2.4950314 3.062953 3.3082323 1.5065632 0.11250114 -1.3001375 -2.5545945 -2.9164081 -3.4754753][-4.1202331 -3.3624945 -2.6044188 -1.6017618 -0.56562376 0.017444611 0.73586369 1.0026026 1.2342072 -0.17895508 -1.1362624 -2.1573081 -3.1599855 -3.4065623 -3.9824638][-4.9362369 -4.4054642 -3.8808985 -2.881959 -2.0449772 -1.3647079 -0.70323515 -1.1212459 -1.2792382 -2.3487515 -2.7579455 -3.2795014 -4.0539074 -4.2859879 -4.6936703][-5.5934935 -4.9332123 -4.4326544 -4.0769029 -3.6128149 -2.7943683 -2.2947636 -2.581285 -2.793716 -4.007184 -4.2072258 -4.1620307 -4.583096 -4.6923256 -5.0120759][-6.376255 -6.0119152 -5.4842215 -5.0349894 -4.643394 -4.3704596 -4.1268559 -4.1483378 -4.3890071 -5.4318419 -5.4882207 -4.9186907 -5.1436157 -5.0277681 -5.0756812][-6.3992233 -5.9630766 -5.4858532 -5.142334 -4.7442865 -4.4658747 -4.3540106 -4.5165081 -4.7703276 -5.4853363 -5.5507026 -5.2504425 -5.3104367 -5.113471 -5.0379915][-7.2425361 -6.7318835 -6.3771 -6.1327481 -5.7845044 -5.5671959 -5.5194516 -5.8170185 -6.1954517 -6.3734956 -6.2284155 -5.880702 -5.5853977 -5.2252073 -5.0551567]]...]
INFO - root - 2017-12-15 11:08:40.047231: step 4910, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 59h:56m:21s remains)
INFO - root - 2017-12-15 11:08:46.506488: step 4920, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 57h:51m:29s remains)
INFO - root - 2017-12-15 11:08:52.948147: step 4930, loss = 0.43, batch loss = 0.31 (12.5 examples/sec; 0.640 sec/batch; 58h:14m:27s remains)
INFO - root - 2017-12-15 11:08:59.468524: step 4940, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 57h:24m:36s remains)
INFO - root - 2017-12-15 11:09:05.857916: step 4950, loss = 0.36, batch loss = 0.24 (12.6 examples/sec; 0.636 sec/batch; 57h:52m:13s remains)
INFO - root - 2017-12-15 11:09:12.324089: step 4960, loss = 0.38, batch loss = 0.26 (12.2 examples/sec; 0.658 sec/batch; 59h:50m:57s remains)
INFO - root - 2017-12-15 11:09:18.724028: step 4970, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 58h:29m:49s remains)
INFO - root - 2017-12-15 11:09:25.120340: step 4980, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 58h:26m:26s remains)
INFO - root - 2017-12-15 11:09:31.491204: step 4990, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 57h:48m:20s remains)
INFO - root - 2017-12-15 11:09:37.930354: step 5000, loss = 0.41, batch loss = 0.29 (11.8 examples/sec; 0.678 sec/batch; 61h:41m:05s remains)
2017-12-15 11:09:38.491622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2454166 -3.0356121 -3.1466188 -3.0726185 -3.3487396 -3.6264296 -3.76795 -3.7514172 -3.645262 -4.6233568 -4.96473 -5.8329363 -6.2986054 -6.5334172 -6.7175865][-3.6418307 -3.4048719 -3.614543 -3.8162019 -4.2166815 -4.191853 -4.3508897 -4.5361385 -4.4411716 -5.2320242 -5.358243 -6.061439 -6.5486841 -6.8114514 -6.855361][-3.75042 -3.24197 -3.1086545 -2.9638367 -3.2704782 -3.1157017 -3.0900435 -3.3509884 -3.6055415 -4.644238 -4.952879 -5.6804628 -6.0947008 -6.5176597 -6.7412467][-3.8366005 -3.4646876 -3.2851095 -2.7525725 -2.5049615 -2.3002133 -2.2296195 -2.1206927 -2.1908226 -3.4220359 -3.8055346 -4.8332472 -5.4050455 -5.8009686 -6.2609363][-3.888324 -3.5263281 -3.3004575 -2.7175102 -2.1458912 -1.3171964 -0.73584127 -0.818223 -1.0747833 -2.2382975 -2.8785405 -4.1421494 -4.8441133 -5.3836603 -5.7207494][-4.027216 -3.6255054 -3.2532725 -2.6901274 -2.0852809 -1.0385246 -0.28213358 -0.049925804 -0.095905304 -1.2368841 -1.736558 -2.9540725 -3.823756 -4.5703974 -5.3174968][-3.8056319 -3.3579841 -2.9782662 -2.1673851 -1.3951254 -0.6017704 0.027449608 0.38704824 0.50105858 -0.63017988 -1.0669799 -2.1597538 -3.0766654 -3.9927998 -4.8787832][-3.448375 -3.3140721 -2.9955401 -2.1317706 -1.3448381 -0.48333216 0.19106817 0.42519712 0.52926826 -0.26055098 -0.51758051 -1.5864143 -2.519887 -3.3809385 -4.2926235][-3.1526194 -3.0377631 -2.8923678 -2.3122149 -1.6813116 -0.77192926 -0.10050631 0.11001158 0.12699938 -0.73173761 -1.1724319 -2.1277003 -2.8069382 -3.5003505 -4.1687012][-3.6447666 -3.0976868 -2.7687359 -2.2764721 -1.7459416 -1.1783042 -0.67498207 -0.37951851 -0.21128845 -1.3167291 -1.9077768 -2.7873726 -3.5397706 -4.0830865 -4.5675173][-4.5679445 -4.0261831 -3.5548828 -2.8746352 -2.1965384 -1.8531442 -1.5482926 -1.6416383 -1.5853338 -2.4457154 -2.8862534 -3.4862902 -4.0709219 -4.4129686 -4.592617][-5.1667547 -4.9918175 -4.6397495 -3.9521625 -3.2040143 -2.5740352 -2.0933089 -2.1387672 -2.1286898 -2.713769 -3.0866404 -3.7977448 -4.1786704 -4.6202717 -5.0372453][-6.2664242 -5.9809537 -5.5403748 -4.8913708 -4.226614 -3.724725 -3.1774874 -2.8631129 -2.6120682 -3.1383586 -3.3050895 -3.8792267 -4.2231665 -4.452107 -4.7218933][-6.8171749 -6.7502584 -6.2974458 -5.5805249 -4.69707 -4.0135593 -3.413053 -3.0651999 -2.7807765 -2.8035221 -2.9318724 -3.4056435 -3.7421482 -4.043499 -4.3327131][-7.3773847 -7.36196 -7.0684323 -6.4147973 -5.7178087 -5.02814 -4.8220596 -4.4193025 -4.1775246 -4.12253 -4.1126518 -4.3259153 -4.1100383 -4.2033839 -4.3655]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 11:09:46.130990: step 5010, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.644 sec/batch; 58h:34m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 11:09:52.557277: step 5020, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.630 sec/batch; 57h:20m:14s remains)
INFO - root - 2017-12-15 11:09:58.919600: step 5030, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.634 sec/batch; 57h:40m:14s remains)
INFO - root - 2017-12-15 11:10:05.402595: step 5040, loss = 0.34, batch loss = 0.22 (12.3 examples/sec; 0.653 sec/batch; 59h:21m:41s remains)
INFO - root - 2017-12-15 11:10:11.773167: step 5050, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 57h:47m:09s remains)
INFO - root - 2017-12-15 11:10:18.145741: step 5060, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.629 sec/batch; 57h:13m:26s remains)
INFO - root - 2017-12-15 11:10:24.545282: step 5070, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 58h:42m:58s remains)
INFO - root - 2017-12-15 11:10:30.943708: step 5080, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 57h:50m:52s remains)
INFO - root - 2017-12-15 11:10:37.331583: step 5090, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 56h:42m:56s remains)
INFO - root - 2017-12-15 11:10:43.785887: step 5100, loss = 0.34, batch loss = 0.22 (12.9 examples/sec; 0.619 sec/batch; 56h:19m:02s remains)
2017-12-15 11:10:44.300267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1798687 -4.0878711 -3.4736972 -3.1882105 -2.8936348 -2.7415009 -2.8112712 -2.8606539 -2.9434881 -3.5750959 -3.8223448 -4.0593839 -4.4710941 -5.8807569 -4.8729095][-5.0698848 -4.4296632 -3.5789704 -2.9685063 -2.7362609 -2.5253282 -2.5719519 -2.6418462 -2.6412463 -3.200624 -3.6558871 -4.0325513 -4.2601433 -5.2589769 -3.9360571][-4.3721371 -3.5072014 -2.7847042 -2.6196389 -2.2916794 -1.8825164 -1.938117 -2.0811648 -2.1802592 -2.7698994 -3.4060149 -4.0928211 -4.4524593 -5.40294 -4.2343764][-3.9587231 -2.8822241 -2.4443855 -1.979455 -1.8357205 -1.3010058 -0.69492769 -0.96786165 -0.79070568 -1.4449801 -2.3255897 -3.1235065 -4.0143728 -5.3178453 -4.2408662][-4.329587 -3.04182 -1.7283144 -1.1616311 -0.65328121 -0.41664028 -0.52208805 -0.62045 -0.43495178 -1.2039294 -2.2859111 -3.0881319 -3.7444398 -5.0029163 -4.0534391][-1.448801 -1.0584702 -0.64560413 0.35118198 1.1755724 1.7233105 1.8583517 1.3999043 0.9336071 0.0083880424 -1.6747336 -2.9728494 -3.5760233 -4.8824482 -3.8491786][-0.16756535 0.032474041 0.610302 1.1961412 1.5724754 2.7016869 2.7129602 2.5324917 2.3839159 1.0541515 -0.86858177 -2.4613504 -3.9600024 -5.5055604 -4.2671776][0.13422012 1.1822157 0.99149323 1.30264 1.9366846 2.1143837 2.0431423 2.3518429 1.9393644 0.96229267 -0.48632479 -2.0052381 -3.353919 -5.6355791 -5.1259913][0.88388634 0.94303417 1.7564116 1.8360682 1.7101908 2.0497217 1.914793 1.5611629 1.4299803 0.45031071 -1.2777009 -2.5576038 -3.4702287 -5.1719437 -4.5351057][0.046256542 0.95482445 1.3286724 1.0910912 1.6817608 1.2946739 1.0839739 1.2158194 1.0601616 -0.20646381 -1.1938262 -2.2537975 -4.1923828 -5.7520151 -4.5856991][-1.5679145 -0.39904785 0.00028419495 0.5291543 0.32884216 0.6419239 0.64877892 -0.13651228 -0.4745059 -1.2146869 -1.4690938 -2.1611328 -3.5313973 -5.6505032 -5.4055357][-3.8069932 -3.0963988 -1.9615059 -1.3305411 -0.65884924 -0.080436707 -0.49925184 -0.27807951 -0.92416286 -2.351964 -2.9869742 -3.371912 -3.8357556 -4.9469862 -4.5663967][-3.648705 -3.7851918 -3.792784 -3.2287354 -2.6091118 -1.8877263 -1.5234337 -1.4327383 -1.6743364 -1.7832022 -2.5557666 -4.0448785 -4.8041453 -5.4158378 -4.6080837][-4.4818888 -3.6658988 -3.5154481 -3.4906194 -3.6228945 -3.2407665 -3.1921549 -3.2423272 -3.0661855 -3.0380111 -3.3888116 -3.686491 -4.9823103 -5.9112921 -5.7789688][-6.1540666 -5.2501698 -4.7585735 -3.9720106 -3.7655723 -3.9272432 -4.2191958 -4.463438 -4.4334431 -4.5754337 -4.8310351 -5.0584917 -4.9830108 -5.6336789 -6.0601783]]...]
INFO - root - 2017-12-15 11:10:50.645663: step 5110, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 58h:40m:31s remains)
INFO - root - 2017-12-15 11:10:56.999023: step 5120, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 58h:31m:14s remains)
INFO - root - 2017-12-15 11:11:03.422963: step 5130, loss = 0.34, batch loss = 0.22 (12.3 examples/sec; 0.649 sec/batch; 59h:02m:44s remains)
INFO - root - 2017-12-15 11:11:09.901145: step 5140, loss = 0.32, batch loss = 0.20 (12.0 examples/sec; 0.666 sec/batch; 60h:32m:58s remains)
INFO - root - 2017-12-15 11:11:16.418322: step 5150, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 57h:54m:27s remains)
INFO - root - 2017-12-15 11:11:22.812873: step 5160, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 57h:04m:08s remains)
INFO - root - 2017-12-15 11:11:29.239451: step 5170, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 57h:41m:32s remains)
INFO - root - 2017-12-15 11:11:35.733204: step 5180, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 57h:10m:03s remains)
INFO - root - 2017-12-15 11:11:42.189208: step 5190, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 58h:10m:25s remains)
INFO - root - 2017-12-15 11:11:48.662426: step 5200, loss = 0.31, batch loss = 0.19 (11.7 examples/sec; 0.684 sec/batch; 62h:10m:03s remains)
2017-12-15 11:11:49.143168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6686053 -2.2818294 -2.1071529 -2.2703862 -2.3853769 -2.4386668 -2.3669534 -2.0055432 -1.6476598 -2.4131961 -3.3290067 -3.7732933 -4.2042356 -4.7776403 -5.1443443][-3.4476259 -3.1794424 -3.1065311 -2.8767438 -2.479589 -2.3183403 -1.9071293 -1.5257649 -1.2980552 -2.100863 -2.8263521 -3.2340431 -4.2193346 -4.9399109 -4.9774327][-4.0064449 -3.5079749 -3.2930965 -3.2258983 -2.8070769 -2.4484558 -2.0561919 -1.6774321 -1.2737575 -1.5829105 -2.1065741 -2.9621553 -3.863517 -4.5317869 -4.8864756][-3.4689324 -2.7663016 -2.5469937 -2.4350715 -2.2467546 -2.4126372 -2.1896658 -1.8992944 -1.3035102 -1.7800808 -2.4604959 -2.8242331 -3.0839329 -3.9447992 -4.5527439][-3.1809292 -2.4692993 -1.9167418 -1.2566519 -0.84355164 -0.75961447 -1.1073699 -1.0990906 -0.77182007 -1.4191589 -2.2465711 -3.1799235 -3.6908407 -4.0381002 -3.9608834][-2.1263561 -1.4226813 -0.93607378 -0.68780422 -0.51174164 -0.31663942 -0.32168531 -0.49016428 -0.40597296 -1.1035204 -1.6836071 -2.1476512 -2.9945927 -3.4801812 -3.4741883][-1.2313638 -0.77574778 -0.12300587 0.04285574 0.089075089 -0.0062026978 -0.1424017 -0.51918125 -0.68217421 -0.98862362 -0.97498417 -1.5381217 -2.5870953 -3.311686 -3.3351531][-0.46783733 -0.22826195 0.20732021 0.1131835 -0.10168791 0.077795982 0.11336088 0.14622211 0.12869787 -0.45059252 -0.90330839 -1.0369244 -2.2296405 -3.2410493 -3.533318][-0.2990756 -0.33729458 -0.263556 -0.20856047 0.030280113 0.28619957 0.52193737 0.50123835 0.29518795 -0.18859386 -0.83708763 -1.2680101 -2.3231449 -3.192585 -3.5176506][-1.4160209 -1.0543637 -1.0224419 -0.712523 -0.40695667 -0.11124086 0.31236649 0.50558186 1.1580796 -0.20110369 -1.1810102 -1.3758554 -2.4148979 -3.362411 -3.4940896][-3.0564008 -2.9258189 -2.4229412 -1.8014975 -1.136889 -0.20464802 0.53607845 0.62622976 0.41630745 -0.89783049 -2.0820661 -2.6415777 -3.2267323 -3.6692386 -3.8447616][-4.1673632 -3.6329386 -3.2368298 -3.2262678 -2.093617 -1.0831013 -0.53595161 -0.58651638 -0.38414 -1.8923678 -3.4393671 -3.8130362 -4.5922823 -5.0653906 -5.0618277][-5.1692796 -4.7498112 -4.0857153 -3.029892 -2.2778888 -1.9736009 -1.5547776 -1.7299294 -2.4989791 -3.513504 -5.2489786 -5.0944843 -5.3066559 -5.1411095 -5.0408258][-6.5273762 -5.7542238 -4.8834124 -4.1657705 -3.7945893 -2.9869175 -3.1762843 -3.7474327 -4.2967014 -4.7972851 -5.1683483 -5.1255436 -5.3040166 -5.0633817 -4.9624491][-6.6538405 -6.0460219 -5.9455996 -5.0895066 -4.7896142 -4.6675925 -5.20696 -5.7570729 -6.2223663 -6.4014583 -6.4264503 -6.2373323 -5.7710133 -5.5352297 -4.8705454]]...]
INFO - root - 2017-12-15 11:11:55.570996: step 5210, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 59h:53m:14s remains)
INFO - root - 2017-12-15 11:12:01.931877: step 5220, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 57h:56m:31s remains)
INFO - root - 2017-12-15 11:12:08.343675: step 5230, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.632 sec/batch; 57h:29m:54s remains)
INFO - root - 2017-12-15 11:12:14.701506: step 5240, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 57h:40m:25s remains)
INFO - root - 2017-12-15 11:12:21.111054: step 5250, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.614 sec/batch; 55h:46m:34s remains)
INFO - root - 2017-12-15 11:12:27.500675: step 5260, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 57h:06m:38s remains)
INFO - root - 2017-12-15 11:12:33.876168: step 5270, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.633 sec/batch; 57h:31m:54s remains)
INFO - root - 2017-12-15 11:12:40.310254: step 5280, loss = 0.34, batch loss = 0.22 (12.9 examples/sec; 0.622 sec/batch; 56h:29m:30s remains)
INFO - root - 2017-12-15 11:12:46.712740: step 5290, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 57h:51m:40s remains)
INFO - root - 2017-12-15 11:12:53.144763: step 5300, loss = 0.37, batch loss = 0.25 (12.4 examples/sec; 0.646 sec/batch; 58h:43m:09s remains)
2017-12-15 11:12:53.680569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5021324 -3.2913203 -1.8618598 -1.2105589 -0.66161585 -0.55122519 -0.3896203 -0.48002958 -0.90374231 -1.1254225 -2.6152496 -3.5044794 -4.4153228 -4.7554445 -5.6199622][-4.6128507 -3.3368292 -2.0453362 -1.2600923 -0.64136934 -0.5880208 -0.45630789 -0.61188412 -1.1776223 -1.8753529 -2.9694986 -4.0293913 -4.9422989 -4.8814764 -5.4732227][-4.0368681 -3.1568308 -2.2668786 -1.1899381 -0.17482471 0.40890455 0.36097717 0.029358387 -0.37737846 -1.3368864 -2.9561715 -4.2289639 -4.8616109 -4.7089305 -4.9620128][-3.638519 -2.8869958 -1.8306708 -1.0279484 -0.080140591 0.63017035 0.95207548 0.92421484 0.19495392 -0.39887571 -2.0813813 -3.6652658 -4.4096184 -4.4960489 -4.748765][-2.48553 -1.6130385 -0.88947153 0.089725018 0.78515768 0.76056242 1.0274062 0.96045446 0.28376341 -0.42735958 -2.0303421 -3.3586526 -3.9443266 -3.9579861 -4.2756186][-1.6055021 -0.87547731 0.082816124 0.78444147 1.0407014 0.733644 0.50888109 0.17546654 0.0474267 -0.1870265 -1.8434615 -3.1832309 -3.7133269 -3.51181 -3.8351581][-1.5907269 -0.29704475 0.9766345 1.3325047 1.1553845 0.82692003 0.52515793 0.057955265 -0.3714242 -0.8296814 -2.728117 -3.6144795 -3.9088891 -3.4875362 -3.7897346][-1.8688216 -0.73712826 0.34672403 1.075048 1.3611846 1.0023646 0.3666501 -0.034344196 -0.39263487 -0.80344391 -2.3124132 -3.5497608 -4.1255841 -3.8750954 -4.0857162][-2.4662066 -1.3953648 -0.37693977 0.36904335 0.58948374 0.72809553 0.61377192 0.30906105 0.0036482811 -0.87817717 -2.449863 -3.432951 -3.9777837 -3.426888 -3.6747692][-3.0977826 -2.2775788 -1.7793131 -1.2054219 -0.80904627 -0.92095804 -0.90344667 -0.93232584 -0.95008135 -1.2511206 -2.6060009 -3.3119664 -3.9590831 -3.8746943 -4.2782459][-4.2132154 -3.2619729 -2.7638149 -2.5251136 -2.4908013 -2.2254362 -2.236105 -1.9411249 -1.9850044 -2.7074437 -3.85536 -4.1024885 -4.2916889 -4.1367445 -4.3981352][-4.7775383 -4.1691856 -3.671875 -3.5589237 -3.576416 -3.449193 -3.3155513 -3.6083171 -3.3914714 -3.2039223 -3.7833381 -3.977416 -4.248374 -4.8418016 -5.0795889][-5.7142482 -5.4066496 -4.5949926 -4.1607375 -4.0234995 -4.1620855 -4.3770609 -4.1470203 -3.9087329 -3.8526027 -4.0600691 -4.1307974 -4.6624804 -4.6049452 -5.0740356][-6.3264794 -5.8375816 -5.3875847 -4.8006554 -4.0937734 -3.8913257 -4.1533289 -4.0894842 -4.0033426 -3.8360908 -3.7964551 -4.0116153 -4.3907 -4.5589504 -4.9868][-6.6304584 -5.9984565 -5.1280618 -4.7416468 -4.1327724 -4.1032572 -4.2002478 -4.2405438 -4.4074874 -4.1645751 -4.34099 -4.5627007 -4.8057318 -4.8440824 -5.0140719]]...]
INFO - root - 2017-12-15 11:13:00.042845: step 5310, loss = 0.36, batch loss = 0.24 (12.8 examples/sec; 0.623 sec/batch; 56h:37m:11s remains)
INFO - root - 2017-12-15 11:13:06.456843: step 5320, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 57h:17m:18s remains)
INFO - root - 2017-12-15 11:13:12.894669: step 5330, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 59h:33m:34s remains)
INFO - root - 2017-12-15 11:13:19.258787: step 5340, loss = 0.38, batch loss = 0.26 (12.6 examples/sec; 0.634 sec/batch; 57h:37m:31s remains)
INFO - root - 2017-12-15 11:13:25.645027: step 5350, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 58h:17m:25s remains)
INFO - root - 2017-12-15 11:13:32.004998: step 5360, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 58h:22m:47s remains)
INFO - root - 2017-12-15 11:13:38.412399: step 5370, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 57h:40m:27s remains)
INFO - root - 2017-12-15 11:13:44.867234: step 5380, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 57h:46m:19s remains)
INFO - root - 2017-12-15 11:13:51.290829: step 5390, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 58h:56m:42s remains)
INFO - root - 2017-12-15 11:13:57.698208: step 5400, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 57h:28m:55s remains)
2017-12-15 11:13:58.236543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2524996 -4.4575033 -4.5845289 -4.780664 -4.6816683 -4.3413687 -3.2723088 -2.2802005 -1.5545402 -1.9713755 -2.4639492 -2.7192283 -3.8132536 -4.9847689 -5.706367][-3.3692203 -3.661397 -3.750751 -3.9585977 -3.7721567 -3.4982934 -3.1250396 -2.2683635 -1.2919483 -1.5532355 -2.0505304 -2.3148375 -3.4435191 -4.4174309 -4.9294524][-2.943048 -2.487586 -2.4480786 -2.6120219 -2.47998 -2.7125306 -2.0503378 -1.0423427 -0.46579123 -0.89146423 -1.600235 -1.9136391 -3.0114679 -3.9677203 -4.5896459][-2.9758677 -2.7342596 -2.352366 -1.9901919 -1.7080169 -1.6194367 -1.2242532 -0.64235973 -0.12307358 -0.7509594 -1.4202194 -1.8811545 -3.1332316 -4.2204866 -4.7465582][-3.4315553 -2.50285 -1.6185131 -1.2076406 -0.59643316 -0.43706274 -0.021844864 0.19930172 0.53527021 -0.15080309 -1.0094543 -1.5204377 -2.661273 -3.7739258 -4.4278927][-3.4461482 -2.5444484 -1.6546984 -1.022419 -0.30401325 0.24525166 1.0040321 1.1687222 1.1832094 0.30361938 -0.60235548 -1.4154639 -2.4820642 -3.525224 -4.412735][-3.6881166 -2.9076152 -1.9996748 -1.3432388 -0.44879389 -0.0026082993 0.56030369 0.79177666 0.98044491 0.23372841 -0.63000488 -1.3173704 -2.5886884 -3.6976597 -4.6366472][-3.7298441 -3.044179 -2.4047785 -1.7986369 -1.03017 -0.45018673 0.22744036 0.47648525 0.67389393 0.113873 -0.8582716 -1.6408968 -2.7911706 -3.6667371 -4.4134135][-3.3544402 -2.4629626 -1.7488847 -1.459269 -1.0401611 -0.40219259 0.25854874 0.42081118 0.65827656 0.18693638 -0.94521952 -1.8126936 -2.818841 -3.3847098 -4.1165752][-2.7647681 -2.4546704 -2.1079926 -1.6608033 -1.0537739 -0.70657921 -0.14927578 0.13461494 0.27609921 -0.20766926 -1.1014624 -1.4020033 -2.500483 -2.6694794 -3.0537024][-4.2392812 -3.6980124 -3.1068296 -2.9511051 -2.5110054 -2.2615438 -1.7858348 -1.5401764 -1.3762717 -1.5908823 -2.0515161 -2.1010318 -2.5432382 -2.5569587 -2.7370119][-4.9467897 -4.4456253 -4.2099237 -4.2852259 -3.940726 -3.518939 -2.9898295 -2.8671713 -2.4149103 -2.617547 -2.6494231 -2.81603 -3.1196408 -3.1939254 -3.0681691][-5.2919855 -5.0717735 -4.8321323 -5.1288991 -4.9917107 -4.7367296 -4.203826 -4.1375537 -3.7369003 -3.4118657 -2.7311916 -2.775744 -3.2049403 -3.2509184 -2.9076986][-6.0649128 -5.9260268 -5.6844997 -6.0932417 -6.1990776 -6.1367688 -5.67325 -5.4303246 -5.0354214 -4.6073251 -4.0432091 -3.543977 -3.2768106 -3.4058523 -3.3922458][-6.1661973 -6.288547 -6.1577463 -6.4335828 -6.8247252 -7.0096588 -6.6219296 -6.2452574 -5.8815041 -5.5658836 -5.1658144 -4.8610945 -4.5003357 -4.0220013 -3.7553186]]...]
INFO - root - 2017-12-15 11:14:04.605411: step 5410, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 56h:56m:56s remains)
INFO - root - 2017-12-15 11:14:11.022183: step 5420, loss = 0.30, batch loss = 0.18 (11.8 examples/sec; 0.680 sec/batch; 61h:49m:29s remains)
INFO - root - 2017-12-15 11:14:17.367847: step 5430, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 57h:05m:35s remains)
INFO - root - 2017-12-15 11:14:23.815956: step 5440, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 59h:06m:38s remains)
INFO - root - 2017-12-15 11:14:30.204998: step 5450, loss = 0.39, batch loss = 0.27 (12.3 examples/sec; 0.650 sec/batch; 59h:05m:26s remains)
INFO - root - 2017-12-15 11:14:36.665213: step 5460, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 59h:35m:04s remains)
INFO - root - 2017-12-15 11:14:43.006827: step 5470, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 57h:16m:56s remains)
INFO - root - 2017-12-15 11:14:49.341211: step 5480, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 57h:48m:06s remains)
INFO - root - 2017-12-15 11:14:55.694358: step 5490, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 58h:00m:50s remains)
INFO - root - 2017-12-15 11:15:02.051818: step 5500, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 57h:59m:49s remains)
2017-12-15 11:15:02.616134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7656517 -4.8431849 -4.9087305 -4.9554138 -4.9385967 -4.3214765 -4.0920877 -3.98532 -3.4037991 -3.6037109 -3.9478548 -4.62037 -5.1230288 -6.0246086 -7.6166148][-4.5761895 -4.6592455 -4.6743927 -5.0001917 -5.1494193 -5.0477376 -5.270875 -5.0323458 -4.5556059 -4.7780056 -4.2647924 -4.734674 -5.5941906 -6.2217922 -7.4549217][-4.7438917 -4.7775812 -4.5066323 -4.5673161 -4.6076441 -4.525671 -4.5201049 -4.5050344 -4.3950748 -4.708322 -4.3472281 -4.6315994 -4.7137628 -5.5272985 -6.5363226][-4.7300267 -4.6536627 -4.597599 -4.3648767 -4.0140495 -3.7535522 -3.5624189 -3.0622811 -2.5811706 -3.5371511 -3.7628226 -3.7983222 -3.7107193 -4.8286905 -6.0563378][-5.0820532 -4.8258581 -4.0118766 -3.4188271 -2.9666443 -2.2972469 -1.9272552 -1.9744215 -1.4524827 -1.9191689 -2.1427975 -2.8010488 -2.9531717 -3.7193496 -5.16024][-4.9038143 -4.433661 -3.6528897 -2.6788163 -1.6477084 -0.57147837 0.19915676 0.11912537 0.32795906 -0.41283417 -0.44187593 -1.1773925 -1.9617968 -2.7199965 -3.3701539][-4.256299 -3.7557983 -3.075551 -1.8169847 -0.75432873 0.55259848 1.4142842 1.5772395 2.2947278 1.5677218 1.2647243 0.37181187 -1.1201258 -2.7108479 -3.8209555][-4.0745945 -3.5804486 -2.1670866 -1.2183113 -0.36542177 1.0292315 1.9600625 2.3226285 3.0347362 2.2014089 1.2191358 -0.092675209 -1.2820687 -2.9779792 -4.7260923][-3.7988539 -3.48169 -2.2605095 -1.0402517 -0.28794718 0.53498554 1.2339721 1.8792157 2.844624 2.2268806 0.95970869 -0.689857 -2.1279607 -3.6694331 -5.2298641][-3.5481346 -3.9839144 -3.3544202 -2.3149657 -0.9419589 0.20877552 0.56386328 0.90997934 1.6056046 1.1431499 0.6538558 -1.0185809 -2.72579 -4.4310822 -6.0549979][-5.5755076 -5.4233131 -4.9661136 -4.2061563 -3.2197881 -2.1109676 -0.9232254 0.012486458 -0.0041790009 -0.94144726 -1.3299513 -2.5567541 -3.5020063 -4.8275127 -6.3848062][-7.2413392 -6.42922 -5.654964 -5.5759974 -4.9850616 -4.2858372 -3.7171729 -2.6635842 -1.9505906 -2.3374696 -2.7372203 -3.1453376 -4.5862088 -5.5064583 -5.9086261][-7.7971587 -7.0813971 -6.3837991 -6.3189383 -5.7465186 -4.9024506 -4.2992907 -4.0135827 -3.6024828 -3.8241396 -3.4995055 -3.4471178 -4.2995253 -5.5465121 -6.466897][-7.3696685 -7.3485775 -6.6305933 -6.2212834 -5.7365923 -5.2772903 -4.4591565 -3.828326 -3.8415251 -4.2823267 -4.3825588 -3.9634628 -4.5205784 -5.2201643 -5.832232][-7.623281 -7.5305419 -7.3732677 -7.1491055 -7.0150266 -6.8847518 -6.2230954 -5.8896236 -5.4803109 -5.4093342 -5.3025761 -5.22506 -5.202631 -4.9920645 -5.2881784]]...]
INFO - root - 2017-12-15 11:15:09.021113: step 5510, loss = 0.32, batch loss = 0.20 (11.9 examples/sec; 0.674 sec/batch; 61h:11m:46s remains)
INFO - root - 2017-12-15 11:15:15.445853: step 5520, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 57h:07m:21s remains)
INFO - root - 2017-12-15 11:15:21.805784: step 5530, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 60h:45m:49s remains)
INFO - root - 2017-12-15 11:15:28.156807: step 5540, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 57h:04m:47s remains)
INFO - root - 2017-12-15 11:15:34.550266: step 5550, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 57h:55m:55s remains)
INFO - root - 2017-12-15 11:15:41.050534: step 5560, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 57h:24m:36s remains)
INFO - root - 2017-12-15 11:15:47.563016: step 5570, loss = 0.33, batch loss = 0.21 (12.8 examples/sec; 0.627 sec/batch; 56h:58m:02s remains)
INFO - root - 2017-12-15 11:15:53.950864: step 5580, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 57h:37m:36s remains)
INFO - root - 2017-12-15 11:16:00.223370: step 5590, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 57h:12m:36s remains)
INFO - root - 2017-12-15 11:16:06.680927: step 5600, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.637 sec/batch; 57h:53m:08s remains)
2017-12-15 11:16:07.259808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.906868 -2.730803 -3.1073346 -3.2908864 -3.2818017 -3.2671065 -2.6960487 -2.3618445 -2.2199244 -2.2938075 -2.8203468 -3.9926214 -4.134541 -4.8612728 -5.96767][-2.9533734 -3.3936253 -3.9050708 -4.2099319 -4.2495975 -4.3846006 -4.0947814 -3.5352952 -3.2525864 -3.0500431 -3.8656816 -4.6833744 -5.0155087 -5.4318328 -5.673553][-3.4844563 -3.211966 -3.3317165 -3.4283347 -3.6496577 -3.9003336 -3.7423618 -3.4701421 -3.261107 -3.5361166 -4.5533991 -5.1854835 -4.76486 -5.2196059 -6.0798807][-3.0359211 -3.0021896 -3.2485776 -3.313931 -3.4513297 -3.450377 -3.0114698 -2.7688107 -2.9987073 -3.3070307 -4.2882204 -5.1028838 -5.2342491 -5.1735229 -5.3577642][-3.5397875 -3.0416031 -2.5360136 -2.5613098 -2.4822106 -2.1987286 -1.9723353 -1.7864118 -1.6153989 -2.0718746 -3.1684198 -4.0515881 -4.1440034 -4.4371457 -5.0312071][-3.2985973 -2.5356832 -2.5903378 -2.16256 -1.4901028 -1.1521907 -0.51060438 -0.74801016 -0.87646818 -1.2591314 -1.7916574 -3.0858765 -3.2582521 -3.6957574 -4.2696729][-3.9022272 -3.0260472 -2.0838361 -1.5259991 -1.0502248 -0.41763496 -0.048874378 0.22624063 0.49459028 0.018037796 -1.0931616 -2.3509908 -3.1758394 -3.6153598 -4.1918554][-3.013217 -2.6013932 -2.548368 -1.5309229 -0.21453762 0.48297644 0.97111654 0.97829294 1.3163829 1.1372418 -0.18203449 -1.8830385 -2.3772459 -2.698523 -3.9075978][-3.5626645 -2.5593233 -1.7799163 -1.2079983 -1.0243869 -0.2856698 0.83758593 1.4082103 1.7333961 1.5248885 0.91602278 -1.0665383 -2.3931165 -2.8492608 -3.3660226][-4.002018 -3.3022079 -2.95085 -2.1525688 -1.15131 -0.17342329 0.36214113 0.43422318 1.1734252 0.47870874 -1.014709 -2.3770242 -2.7878227 -3.2388005 -4.1721554][-5.4751735 -4.9482222 -4.0550661 -3.5641437 -3.4316421 -2.7202182 -1.6179361 -1.3825417 -1.5542059 -2.0557308 -2.4229617 -3.3362989 -4.1675606 -4.5159636 -5.234951][-6.4462605 -6.1701775 -5.5952988 -4.8877974 -4.4207039 -4.228961 -3.9724762 -3.5297046 -2.9767065 -3.7672281 -4.5461645 -5.0702477 -5.3596706 -5.6154675 -5.191339][-7.2128539 -6.7522106 -6.51328 -6.3934274 -6.1491237 -5.8247204 -5.2034678 -5.3767939 -5.3867054 -5.3154411 -5.0624695 -5.377718 -5.71005 -5.930131 -6.0127773][-7.0883927 -7.0283895 -6.6393504 -6.4380426 -6.4779267 -6.5932775 -6.0971646 -6.0024829 -5.8939028 -5.9364319 -6.0770569 -6.1131649 -5.8808088 -5.4274511 -5.0218134][-6.1447854 -5.7584467 -6.3566365 -6.6305957 -6.477932 -6.4278135 -6.2570066 -6.3213196 -6.2128973 -6.1340137 -6.1170378 -6.0756817 -5.9088731 -5.8829536 -5.9278231]]...]
INFO - root - 2017-12-15 11:16:13.610521: step 5610, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 58h:24m:52s remains)
INFO - root - 2017-12-15 11:16:20.112041: step 5620, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 58h:17m:32s remains)
INFO - root - 2017-12-15 11:16:26.512004: step 5630, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 56h:51m:08s remains)
INFO - root - 2017-12-15 11:16:32.909094: step 5640, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 58h:09m:01s remains)
INFO - root - 2017-12-15 11:16:39.307798: step 5650, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 57h:27m:36s remains)
INFO - root - 2017-12-15 11:16:45.684343: step 5660, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 58h:42m:01s remains)
INFO - root - 2017-12-15 11:16:52.075622: step 5670, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 56h:21m:21s remains)
INFO - root - 2017-12-15 11:16:58.378459: step 5680, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 56h:38m:07s remains)
INFO - root - 2017-12-15 11:17:04.709446: step 5690, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 57h:54m:34s remains)
INFO - root - 2017-12-15 11:17:11.159701: step 5700, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.615 sec/batch; 55h:48m:49s remains)
2017-12-15 11:17:11.663405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0008945 -4.0517378 -3.8430834 -3.3659897 -2.955677 -2.6219244 -2.0459957 -1.7242975 -1.4792981 -2.1052618 -3.1094885 -4.7523055 -5.1796083 -6.2278652 -6.182292][-3.8927166 -4.1537066 -4.165143 -4.3451557 -3.951685 -2.9885669 -2.4885459 -2.1741366 -2.1350913 -3.0431795 -3.8976381 -5.5298405 -5.7764149 -6.6111832 -6.3518391][-3.3015685 -3.3916979 -3.4943867 -3.8723979 -3.7072 -3.1095805 -2.2614288 -1.5954309 -1.3473091 -1.9874158 -3.1524682 -4.8968029 -5.3277903 -6.3654914 -5.97819][-3.0647259 -3.2909327 -3.4834676 -3.0232124 -2.9646749 -2.5760989 -1.9223332 -1.2053237 -0.84451389 -1.4006691 -2.0487037 -3.7347035 -4.3214064 -5.572824 -5.7419443][-3.6542051 -3.8491118 -3.992456 -3.7196214 -2.5306239 -1.2118907 -0.63032722 -0.42432404 -0.29585552 -0.85890675 -1.8624582 -3.5373147 -3.8910887 -5.1372519 -5.1446152][-4.6332088 -4.2622643 -4.142436 -3.6680839 -2.6396961 -0.81413794 0.65782404 1.3983722 1.3755488 0.31621218 -1.1658292 -3.264153 -4.09639 -5.0538173 -4.9095144][-5.2945585 -5.0070562 -4.2275534 -3.4835665 -2.3372483 -0.28443098 1.4558597 2.6043315 2.8995566 1.6026559 -0.40900373 -3.31706 -4.4281211 -5.3721333 -5.2449732][-5.0532322 -4.5539618 -3.8620274 -3.0506206 -1.8992529 -0.068616867 1.7207398 2.7841697 2.9092679 1.9183354 0.1874795 -2.7543411 -4.7778339 -6.0601721 -5.2834549][-4.2902274 -3.4938743 -2.9242291 -1.9523191 -0.75363588 0.83302927 2.2109847 2.9360251 2.9597669 2.2426267 0.9113698 -1.7465968 -3.346983 -5.3583994 -5.4334445][-3.6657073 -3.1015072 -2.4993253 -1.8262892 -1.0415974 0.41132498 1.5915217 2.2153764 2.3606296 1.2731738 0.056780815 -2.2054291 -3.4026198 -4.956686 -5.3370304][-3.4380498 -2.6840239 -2.3188043 -1.6593022 -1.1104994 -0.1256094 0.7914958 0.82489824 0.71356821 -0.42656803 -1.327858 -3.1878891 -4.1626735 -4.7837229 -4.6957011][-2.9422941 -2.4206758 -1.9179626 -1.442059 -1.1164246 -0.59463692 -0.1940999 -0.62940073 -1.0887251 -2.0626955 -2.6739311 -4.1177077 -5.1737947 -5.891077 -5.2391415][-3.715492 -3.1092429 -2.2288117 -1.5707407 -1.3008022 -1.0915508 -1.2152433 -1.929347 -2.8209963 -3.7276986 -4.009243 -4.5665135 -5.1778407 -6.1891403 -5.9097295][-3.7701297 -3.1021366 -2.3732691 -1.8128519 -1.5163569 -1.5320716 -1.8331876 -2.5663857 -3.426743 -4.5221987 -4.5010891 -4.8224287 -5.1333132 -5.7251458 -5.7372255][-5.4107132 -4.9018083 -4.1220493 -3.0340257 -2.564846 -2.6899714 -3.2020454 -3.6761572 -4.3282442 -5.1519918 -5.6357412 -5.9608941 -5.7677593 -5.6766009 -5.7377386]]...]
INFO - root - 2017-12-15 11:17:18.052042: step 5710, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:52m:04s remains)
INFO - root - 2017-12-15 11:17:24.574047: step 5720, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 60h:45m:42s remains)
INFO - root - 2017-12-15 11:17:31.095181: step 5730, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 58h:11m:47s remains)
INFO - root - 2017-12-15 11:17:37.532490: step 5740, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 58h:23m:22s remains)
INFO - root - 2017-12-15 11:17:44.039349: step 5750, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 57h:53m:27s remains)
INFO - root - 2017-12-15 11:17:50.427071: step 5760, loss = 0.32, batch loss = 0.20 (12.0 examples/sec; 0.669 sec/batch; 60h:42m:06s remains)
INFO - root - 2017-12-15 11:17:56.996785: step 5770, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 57h:48m:44s remains)
INFO - root - 2017-12-15 11:18:03.454989: step 5780, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 57h:49m:29s remains)
INFO - root - 2017-12-15 11:18:09.933811: step 5790, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 58h:54m:16s remains)
INFO - root - 2017-12-15 11:18:16.358925: step 5800, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 58h:02m:21s remains)
2017-12-15 11:18:16.903176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3987389 -4.8743715 -4.2262583 -2.1931367 -2.1730895 -2.8574538 -3.1590405 -3.0741282 -2.7799735 -1.2503948 -4.4790421 -3.1739278 -4.1019974 -4.9543266 -5.0751076][-3.3068604 -3.7401345 -3.1260948 -2.5351114 -2.0955596 -2.2603674 -3.350224 -3.837363 -1.6379447 0.35352325 -2.2248921 -3.5207546 -5.2855978 -6.1255646 -5.6265111][-2.8775015 -3.7706387 -2.9839282 -1.2798615 -0.62326813 -1.2531166 -1.5801558 -1.2775812 -2.187458 -0.89473343 -2.1615033 -2.0046182 -3.6607645 -4.948133 -5.805872][-2.0709729 -2.2719173 -1.7356796 -1.2048965 -0.24535131 -0.96720123 -2.2274418 -2.0916305 -1.1221914 1.2919436 -1.9225812 -2.7346106 -2.8942461 -4.1700697 -4.9296803][-3.1958284 -2.4850588 -0.22591591 0.8479538 0.59427547 -0.77815819 -1.6476302 -2.5330086 -2.5263486 0.27984095 -0.96129322 -1.0059481 -3.0766926 -3.9604816 -4.617836][-0.89706087 -0.53502417 -0.005777359 0.51870012 1.0499392 0.78903961 0.52523041 -0.50832319 -0.79422855 1.3497391 -0.682919 -1.6425853 -3.0672307 -3.7536187 -4.7824149][-1.5834513 0.50817347 2.9818316 2.9802465 1.9388752 -0.18055105 1.2174397 0.85254574 1.1024504 2.0045948 -0.98020744 -1.1759257 -2.7008295 -3.6615176 -4.4346905][-1.4094539 -0.95778322 1.1467724 3.7715988 4.3454437 2.5271568 1.1895351 0.68066978 1.5367413 3.0942783 -0.85416126 -1.4795032 -2.6622844 -3.8931491 -5.315484][-0.066601753 0.26934814 1.0080967 2.8791742 3.95189 3.1151333 2.0392714 1.6792107 1.5607023 2.5573759 -0.53825474 -0.83451843 -2.0132051 -3.6617718 -4.6937551][-1.0458107 -0.99863958 0.80536747 1.610177 1.6348391 1.178441 0.98860264 1.2966213 2.2791567 2.7012081 -0.079569817 -0.98115778 -2.0815911 -2.614892 -4.00179][-3.2667446 -2.7543197 -1.1582842 -0.65864849 0.0166049 -0.1359334 -0.42495537 -0.47975206 -0.33170652 0.24654484 -1.6891446 -2.9284215 -3.4535894 -3.8036911 -3.9879472][-5.7804112 -5.5886292 -4.60566 -3.6143336 -2.8575554 -3.8367126 -3.362052 -2.5194273 -1.8193703 -1.7987218 -2.2527695 -2.9722304 -4.7077637 -5.7968173 -5.9953542][-4.7964725 -5.4423227 -5.342618 -5.8482642 -4.8274956 -3.6474695 -3.598928 -3.0609541 -2.9243984 -1.698626 -3.0375977 -2.9873295 -4.4285526 -5.2345767 -5.8734307][-5.6645403 -5.0928364 -4.4021139 -5.1891966 -5.4622984 -5.4309139 -4.96044 -3.5036621 -3.797194 -4.2302184 -3.9233696 -3.6260734 -4.9509473 -5.1707978 -5.8584614][-5.1570063 -5.1754479 -4.9281459 -4.9270611 -4.6632805 -5.4323359 -5.1363978 -5.2448325 -5.146419 -5.450376 -4.5839796 -4.920681 -5.1055288 -5.4731712 -5.3734436]]...]
INFO - root - 2017-12-15 11:18:23.376817: step 5810, loss = 0.28, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 60h:49m:36s remains)
INFO - root - 2017-12-15 11:18:29.712538: step 5820, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 57h:20m:19s remains)
INFO - root - 2017-12-15 11:18:36.027553: step 5830, loss = 0.38, batch loss = 0.26 (12.9 examples/sec; 0.621 sec/batch; 56h:22m:11s remains)
INFO - root - 2017-12-15 11:18:42.418716: step 5840, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 58h:42m:55s remains)
INFO - root - 2017-12-15 11:18:48.876722: step 5850, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 60h:03m:50s remains)
INFO - root - 2017-12-15 11:18:55.286771: step 5860, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 57h:14m:05s remains)
INFO - root - 2017-12-15 11:19:01.616346: step 5870, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.634 sec/batch; 57h:28m:51s remains)
INFO - root - 2017-12-15 11:19:08.063907: step 5880, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 57h:28m:59s remains)
INFO - root - 2017-12-15 11:19:14.433393: step 5890, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 60h:17m:39s remains)
INFO - root - 2017-12-15 11:19:20.765743: step 5900, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:51m:00s remains)
2017-12-15 11:19:21.268619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0421772 -5.1237311 -5.4289227 -5.9479156 -6.1910667 -6.4200354 -5.6777034 -4.9783697 -4.6434116 -4.7722921 -4.0637865 -5.4481955 -5.4842076 -5.9579916 -6.7610373][-4.7074814 -5.0380993 -5.5634842 -5.5329094 -5.9840302 -6.2734346 -5.4227295 -5.1569042 -4.4569931 -4.4848003 -4.2309332 -5.8268137 -5.6597996 -5.5391431 -6.6601853][-3.7954285 -3.9670115 -4.9363041 -4.9641304 -5.4530048 -5.0257397 -4.0781031 -3.8958704 -2.5717983 -2.7332196 -3.4152074 -4.9301372 -5.4357891 -6.3894982 -6.8333292][-2.6660309 -3.0149531 -4.1910582 -3.8830779 -4.7070503 -4.4173784 -3.0976377 -2.9295459 -1.9124517 -1.6791759 -1.6311502 -4.1262083 -5.1762519 -5.588541 -7.2326894][-2.363656 -2.7145343 -2.8455758 -3.0398211 -3.737849 -3.1718736 -2.2880244 -1.2203674 -0.935905 -2.1490407 -3.4524841 -4.5424976 -4.3824697 -6.0486813 -7.10461][-3.8362558 -2.4501395 -2.3887897 -1.5743685 -1.162529 -1.0525613 -0.48740005 0.11226702 1.2214808 0.081182957 -1.9591722 -5.0573125 -5.8950396 -5.9939909 -7.0869284][-2.6769843 -3.742157 -3.3471832 -1.6871581 -0.35040045 1.410665 2.8398242 3.0516696 3.2873931 2.4942641 0.680182 -3.3645477 -4.8860922 -6.2829037 -7.4026194][-2.1648688 -1.9376864 -2.446384 -2.1900039 -0.97988081 1.0141864 2.9603114 3.4741921 4.3083491 2.7400022 1.7738032 -1.2218628 -3.7816153 -5.8240361 -7.4145269][-0.94560146 -0.37659836 -0.751318 -1.2137055 -1.4095993 -0.4421649 1.2004247 2.3831801 2.965097 0.78621817 0.086503029 -2.2114339 -3.6667194 -4.9895916 -7.0019264][0.36890173 -0.32775831 -0.42759752 0.020067692 -0.18581963 -0.33217144 -0.52569675 0.12766409 1.1419597 0.39904451 -1.7755952 -4.8113084 -5.1488461 -5.7051473 -6.2063007][-0.75044346 -0.47366905 -0.56780958 -1.4669433 -1.6746626 -1.1879764 -1.3634863 -1.6349349 -1.5676479 -1.9118075 -1.7489696 -3.486764 -5.047647 -6.7101231 -6.8327007][-1.7116275 -1.9707088 -2.0167837 -2.1554632 -3.0863671 -3.3491588 -2.7625976 -2.6670828 -2.5434241 -3.2908282 -3.6585267 -4.6375003 -5.4282522 -6.3072257 -7.3514833][-1.4436765 -1.6695333 -2.7040253 -3.7270961 -4.1324978 -4.8554869 -5.1331091 -5.5373731 -5.4599333 -4.9713354 -4.3399091 -5.4251328 -5.5101104 -6.8292055 -7.6202788][0.64941072 -1.1360755 -2.7168732 -3.5709841 -4.5195751 -5.1558552 -5.3499622 -5.7261343 -6.39653 -6.5590343 -6.3245869 -6.625391 -6.4818573 -6.6111593 -7.378068][-2.6581483 -1.3557777 -2.1865911 -4.0823226 -4.9852619 -6.4540148 -6.8201461 -6.3051853 -6.0725703 -6.9894261 -7.5040412 -7.4294991 -7.3569727 -7.3783431 -7.4688511]]...]
INFO - root - 2017-12-15 11:19:27.699099: step 5910, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 59h:16m:57s remains)
INFO - root - 2017-12-15 11:19:34.087177: step 5920, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 57h:29m:07s remains)
INFO - root - 2017-12-15 11:19:40.507249: step 5930, loss = 0.35, batch loss = 0.23 (12.8 examples/sec; 0.624 sec/batch; 56h:35m:10s remains)
INFO - root - 2017-12-15 11:19:46.897089: step 5940, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 57h:56m:26s remains)
INFO - root - 2017-12-15 11:19:53.278804: step 5950, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 56h:00m:10s remains)
INFO - root - 2017-12-15 11:19:59.820324: step 5960, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 58h:44m:52s remains)
INFO - root - 2017-12-15 11:20:06.241734: step 5970, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 58h:44m:35s remains)
INFO - root - 2017-12-15 11:20:12.723667: step 5980, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 58h:36m:47s remains)
INFO - root - 2017-12-15 11:20:19.188238: step 5990, loss = 0.33, batch loss = 0.21 (11.8 examples/sec; 0.680 sec/batch; 61h:41m:35s remains)
INFO - root - 2017-12-15 11:20:25.756257: step 6000, loss = 0.33, batch loss = 0.21 (12.0 examples/sec; 0.667 sec/batch; 60h:28m:20s remains)
2017-12-15 11:20:26.275202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0625606 -2.5204959 -3.1206765 -3.2195916 -2.7978435 -2.4671082 -2.1132407 -1.7797685 -1.3806891 -1.5469332 -2.321579 -3.5308769 -4.5092344 -4.8844671 -5.3015738][-2.8296552 -2.997839 -3.0228572 -3.4174085 -3.7972054 -3.3534498 -2.7867904 -2.2763081 -1.9085135 -2.4463797 -3.3059206 -4.390831 -5.2378559 -5.6412573 -5.9502211][-3.6925063 -3.4667969 -3.2138376 -3.1035247 -3.1780086 -3.0032406 -2.6905894 -2.252316 -1.8259015 -2.6254611 -3.6964817 -4.9731417 -5.8937759 -6.5050473 -6.654161][-3.5349174 -3.2468467 -2.9637427 -2.7917614 -2.1162968 -1.6493783 -1.4520698 -1.3213983 -1.3404088 -2.2641811 -3.6044466 -5.0515547 -6.0402927 -6.6919479 -6.928091][-3.8077734 -3.3276052 -2.7705789 -1.8789849 -1.0075145 -0.35852909 0.11119747 0.31255579 0.26648283 -1.0464664 -2.8595715 -4.7539034 -6.0358052 -6.6094818 -7.0416627][-5.1738954 -4.4362245 -3.1086178 -1.58214 -9.5367432e-06 0.98310089 1.665781 1.9620581 1.9447803 0.45074606 -1.8023825 -4.2490578 -6.1483822 -6.9050627 -7.1375585][-5.0247393 -4.3655353 -3.2150607 -1.349648 0.59276772 1.9284821 3.4079027 3.7588415 3.6002569 2.2060928 -0.25927258 -2.7888474 -4.834857 -6.2284288 -6.7331018][-4.6394958 -3.6744533 -2.4632354 -0.96972942 0.7556324 3.128624 4.7095871 4.67688 4.4163332 2.9116154 0.77042389 -1.7946415 -3.8304703 -5.0373578 -5.8487988][-4.5834961 -3.9990971 -3.4073505 -1.8899136 -0.26776648 1.8091192 3.4886112 3.9369001 3.9768429 1.8494816 -0.66564131 -2.6487589 -4.2406778 -5.02099 -5.4452109][-5.6535087 -5.2274966 -4.362812 -3.1198926 -1.9305043 -1.1065807 0.01815176 0.89185333 1.4159288 -0.29443216 -2.3078537 -3.9153395 -4.8545346 -5.1265721 -5.428153][-6.4047318 -6.2606335 -5.6621046 -4.3216834 -3.1146345 -2.1779609 -1.481482 -1.3997812 -1.2439528 -2.138 -3.0908427 -4.6544647 -5.2980733 -5.2908344 -5.3966][-5.8264446 -5.9653969 -5.9189095 -5.6233325 -4.9632826 -3.7802656 -3.0965352 -3.1292849 -3.1029258 -3.5039163 -3.9773569 -5.0319529 -5.3263063 -5.3849487 -5.3509417][-7.1996775 -6.4829774 -6.0687275 -5.7925048 -5.5304637 -5.2291694 -4.8794837 -4.5135393 -4.4056659 -4.92455 -5.1146765 -5.4358592 -5.3202596 -5.4304018 -5.6521835][-7.549716 -7.3631735 -6.7064047 -5.8595543 -5.540041 -5.413847 -5.1457505 -5.3322916 -5.4176598 -6.0444155 -5.9922256 -5.8572712 -5.8216052 -5.8901896 -5.90467][-7.4987526 -7.9308724 -8.142272 -7.4531059 -6.5358067 -5.9064579 -5.9203506 -6.2167978 -6.301703 -6.46683 -6.2831221 -6.3238363 -6.2525654 -5.9548383 -5.6884942]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 11:20:32.809074: step 6010, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.664 sec/batch; 60h:13m:41s remains)
INFO - root - 2017-12-15 11:20:39.277086: step 6020, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.648 sec/batch; 58h:43m:27s remains)
INFO - root - 2017-12-15 11:20:45.658093: step 6030, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 57h:47m:23s remains)
INFO - root - 2017-12-15 11:20:52.133637: step 6040, loss = 0.34, batch loss = 0.22 (12.2 examples/sec; 0.658 sec/batch; 59h:39m:04s remains)
INFO - root - 2017-12-15 11:20:58.568470: step 6050, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 57h:38m:55s remains)
INFO - root - 2017-12-15 11:21:05.062814: step 6060, loss = 0.32, batch loss = 0.20 (11.7 examples/sec; 0.682 sec/batch; 61h:49m:54s remains)
INFO - root - 2017-12-15 11:21:11.439709: step 6070, loss = 0.30, batch loss = 0.18 (13.1 examples/sec; 0.612 sec/batch; 55h:31m:26s remains)
INFO - root - 2017-12-15 11:21:17.849417: step 6080, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 58h:05m:12s remains)
INFO - root - 2017-12-15 11:21:24.342937: step 6090, loss = 0.38, batch loss = 0.26 (12.1 examples/sec; 0.660 sec/batch; 59h:52m:46s remains)
INFO - root - 2017-12-15 11:21:30.847454: step 6100, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 59h:06m:34s remains)
2017-12-15 11:21:31.366230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7796054 -2.1139512 -2.3133836 -2.6200118 -3.0675125 -3.3736525 -3.6165776 -3.6800349 -3.4463596 -4.1442308 -4.7851667 -5.4305186 -5.7597885 -6.038765 -6.3110666][-1.9505944 -2.2499013 -2.4553332 -2.9861279 -3.5574572 -3.8901196 -4.1701975 -4.2912631 -4.1813555 -4.8493896 -5.3929563 -6.0129342 -6.335093 -6.736588 -6.9865632][-1.5483761 -1.7240791 -1.8365421 -2.2849131 -2.7208457 -2.9874525 -3.2691846 -3.5924363 -3.4619493 -4.2735167 -4.9667349 -5.7230711 -6.3206635 -6.7816033 -7.11307][-1.2075872 -1.4035378 -1.4443641 -1.6023431 -1.8251338 -1.9815254 -2.0791826 -2.1203179 -2.0629163 -2.98212 -3.8050036 -4.6919594 -5.4614153 -6.2641845 -6.9939284][-1.4533324 -1.391993 -1.2212276 -1.1449065 -0.94007587 -0.71210432 -0.57957506 -0.58099318 -0.46583271 -1.3248572 -2.3560162 -3.6178565 -4.6100445 -5.4982805 -6.2281933][-2.3520107 -1.7287703 -1.2674284 -0.85149956 -0.48445845 -0.09552002 0.31201077 0.56277657 0.95044136 -0.027531624 -1.0699511 -2.3691859 -3.7282691 -4.7646761 -5.650135][-2.6540232 -2.2160411 -1.4860811 -0.70132017 0.066842079 0.72662354 1.3242674 1.8557682 2.2974768 1.3080111 0.11913204 -1.2771025 -2.5144553 -3.7070773 -4.8152804][-2.9051051 -2.1678495 -1.6232519 -0.74629068 0.28796148 1.2205667 1.9390602 2.3883533 2.9798794 2.0581865 0.86339855 -0.66131115 -1.9229541 -2.9375577 -3.9582338][-2.924253 -2.4140692 -1.8909483 -1.1464314 -0.28751945 0.71214676 1.5491705 1.9205942 2.2336693 1.1840334 0.037739754 -1.2389383 -2.294373 -3.1834664 -4.0287352][-3.3762822 -2.8911963 -2.466898 -1.7570829 -1.0495381 -0.35424995 0.42578268 1.0104904 1.3292618 -0.10268259 -1.136251 -2.2707086 -3.1575065 -3.7900605 -4.4160213][-4.2088432 -3.8271754 -3.4357119 -2.782599 -2.2773924 -1.6880221 -0.98380947 -0.73833179 -0.64278889 -1.6559229 -2.6680436 -3.6258061 -3.9833758 -4.2757583 -4.6996737][-4.6961117 -4.417932 -4.019453 -3.6485298 -3.4533606 -2.8746982 -2.5122423 -2.5045485 -2.3570194 -3.2418866 -3.8709707 -4.4628458 -4.7227831 -4.8430719 -4.9300132][-5.5067415 -4.9844069 -4.5132427 -4.1927958 -3.9936767 -3.8074472 -3.7175336 -3.6007187 -3.713912 -4.3455777 -4.66175 -5.0134811 -5.2797155 -5.3052444 -5.1320286][-5.8859019 -5.4534397 -4.9764252 -4.5643568 -4.4290113 -4.2384424 -4.230916 -4.40786 -4.58245 -5.0364919 -5.0474906 -5.1754417 -5.2220144 -5.4524136 -5.2822332][-6.517055 -6.0673881 -5.6902733 -5.4320688 -5.2641206 -5.1153059 -5.1902843 -5.221158 -5.3406782 -5.4348822 -5.4491568 -5.5264945 -5.6357636 -5.5854149 -5.4583292]]...]
INFO - root - 2017-12-15 11:21:37.874341: step 6110, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 58h:59m:18s remains)
INFO - root - 2017-12-15 11:21:44.238784: step 6120, loss = 0.43, batch loss = 0.31 (12.8 examples/sec; 0.626 sec/batch; 56h:47m:30s remains)
INFO - root - 2017-12-15 11:21:50.698914: step 6130, loss = 0.32, batch loss = 0.20 (11.8 examples/sec; 0.676 sec/batch; 61h:14m:32s remains)
INFO - root - 2017-12-15 11:21:57.208691: step 6140, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 57h:47m:17s remains)
INFO - root - 2017-12-15 11:22:03.679389: step 6150, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:48m:58s remains)
INFO - root - 2017-12-15 11:22:10.092111: step 6160, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 58h:05m:57s remains)
INFO - root - 2017-12-15 11:22:16.475321: step 6170, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.620 sec/batch; 56h:10m:12s remains)
INFO - root - 2017-12-15 11:22:22.867281: step 6180, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 58h:12m:00s remains)
INFO - root - 2017-12-15 11:22:29.322597: step 6190, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.668 sec/batch; 60h:32m:55s remains)
INFO - root - 2017-12-15 11:22:35.813373: step 6200, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 59h:37m:57s remains)
2017-12-15 11:22:36.289749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8791952 -5.8332024 -6.3622646 -6.5656691 -5.9932313 -4.9134178 -4.0693941 -3.6522844 -3.186955 -3.9440768 -3.9764068 -4.0123172 -4.4936953 -4.8588495 -5.3085852][-4.9017868 -6.3918691 -7.3011837 -7.9873819 -8.1328287 -7.7498865 -7.0358877 -5.7425714 -4.7181416 -5.3021412 -5.6143703 -6.64601 -7.4733438 -7.5746522 -7.4306974][-5.4385405 -6.7021666 -7.8041954 -8.4471922 -8.9965868 -8.6431379 -7.9240704 -7.5292726 -6.7932086 -7.0257912 -7.2811017 -8.6578541 -9.4450312 -9.79776 -9.6319008][-6.6735783 -7.1477828 -7.4634414 -7.7282734 -8.0497923 -7.750308 -7.097918 -6.7200885 -6.2233553 -7.0889854 -7.6071916 -8.7964344 -10.215538 -10.829957 -11.02552][-7.0626855 -7.0277152 -5.8856039 -5.5623255 -5.9052057 -5.1323047 -4.7338991 -3.9453731 -3.3229079 -4.5739503 -5.452117 -6.9816504 -8.8908339 -10.348976 -10.857491][-7.1498718 -6.4782524 -5.7020063 -3.9518976 -2.4121671 -0.91245413 -0.13204861 0.4825635 0.89758015 -0.58979654 -2.2120028 -4.623229 -6.7335591 -8.49795 -9.042367][-5.0510721 -4.2305646 -3.1983819 -1.3976259 1.3388252 3.7443676 5.4712095 5.8776445 5.9893484 3.7260122 1.6182575 -1.0134454 -3.9129052 -6.1477251 -7.6657739][-3.9484236 -2.6608486 -1.0492454 1.2416658 3.8024635 5.9052019 7.495883 7.7217875 7.5872946 4.9032078 3.0398359 0.18433666 -2.5894456 -4.8255005 -6.6315656][-3.5418448 -3.1976194 -2.4213548 -0.036523342 1.7842674 3.865447 6.0834808 6.61001 6.897522 4.1608887 1.3910398 -0.85071516 -2.7941589 -4.8579884 -6.3715835][-6.2592726 -5.821497 -5.0877466 -3.0556746 -1.401782 0.21812487 2.1734486 2.2375526 2.3071556 -0.51920986 -2.832922 -4.6733713 -5.8640118 -6.5654278 -7.3848033][-8.9433851 -8.8022928 -7.947794 -6.718945 -5.5096016 -3.3974371 -1.4180689 -1.5816274 -0.98726845 -3.4210415 -4.8810115 -6.0714788 -7.9299564 -8.1651306 -8.689621][-9.7460833 -9.52679 -9.464695 -8.6227417 -7.7369518 -6.2310123 -4.73624 -4.4834232 -3.4330764 -4.7422066 -5.1923704 -6.0119009 -7.5980706 -7.871274 -8.33033][-9.7616653 -9.7865114 -9.86616 -9.766633 -9.3144894 -8.1437311 -7.0745664 -6.7653165 -6.5333681 -7.5035625 -7.4413319 -7.2872705 -7.4553394 -7.4033351 -7.4068127][-9.416173 -9.77271 -9.2810745 -9.5928926 -9.5936346 -9.0144024 -8.1511917 -7.992147 -8.1318464 -8.888689 -8.5745153 -8.4765263 -8.3591785 -6.9804029 -6.5268655][-8.15415 -8.41289 -8.8886137 -9.1239882 -9.1843843 -8.973423 -8.5537071 -8.5196447 -8.8590546 -8.78393 -8.72842 -8.6096191 -8.2608109 -7.67826 -7.2635355]]...]
INFO - root - 2017-12-15 11:22:42.680700: step 6210, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 56h:37m:29s remains)
INFO - root - 2017-12-15 11:22:49.038061: step 6220, loss = 0.34, batch loss = 0.22 (12.8 examples/sec; 0.624 sec/batch; 56h:34m:23s remains)
INFO - root - 2017-12-15 11:22:55.375789: step 6230, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 56h:39m:47s remains)
INFO - root - 2017-12-15 11:23:01.734396: step 6240, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 57h:29m:20s remains)
INFO - root - 2017-12-15 11:23:08.139014: step 6250, loss = 0.31, batch loss = 0.19 (11.8 examples/sec; 0.676 sec/batch; 61h:16m:21s remains)
INFO - root - 2017-12-15 11:23:14.614622: step 6260, loss = 0.31, batch loss = 0.19 (11.9 examples/sec; 0.673 sec/batch; 60h:58m:34s remains)
INFO - root - 2017-12-15 11:23:21.126865: step 6270, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 57h:27m:16s remains)
INFO - root - 2017-12-15 11:23:27.560944: step 6280, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 57h:18m:09s remains)
INFO - root - 2017-12-15 11:23:33.974037: step 6290, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.640 sec/batch; 58h:01m:58s remains)
INFO - root - 2017-12-15 11:23:40.392497: step 6300, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 57h:19m:59s remains)
2017-12-15 11:23:40.871842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.316133 -2.3361359 -2.7028532 -2.9390774 -2.7502198 -2.2687745 -2.0673804 -2.1007743 -2.2885184 -3.2241802 -3.6231437 -4.7510958 -5.14243 -5.0982819 -5.773643][-3.1913738 -3.3387809 -3.6407099 -3.5940211 -3.1359572 -2.5996113 -2.3103943 -2.0850186 -1.8903098 -2.9531436 -3.4100118 -4.5617018 -4.9429145 -5.2462845 -5.9679589][-3.5845153 -3.6965778 -3.7858515 -3.9041898 -3.6065738 -2.9494267 -2.344646 -2.1557279 -2.0194149 -2.5404844 -2.7434797 -3.5621779 -3.8717539 -4.2745314 -4.9223733][-3.4875464 -3.6185429 -3.5439415 -3.079134 -2.3308749 -1.7026982 -1.0203676 -0.9818716 -1.1739893 -2.0727239 -2.5597763 -3.4747272 -3.9373279 -4.1111126 -4.491549][-4.1824713 -4.1512175 -3.8597562 -3.0897021 -1.9006066 -0.87613773 0.0062417984 0.28353262 0.50899887 -0.40278149 -0.9568038 -2.3161268 -3.2167258 -3.550925 -4.0607419][-4.9951868 -4.8247566 -4.3493786 -3.1682067 -1.6830058 -0.26924324 0.85482216 1.2771091 1.7308197 1.0079336 0.17828417 -1.4329324 -2.2745714 -2.6586747 -3.2899294][-5.5404997 -4.9658403 -3.997848 -2.537127 -1.0796151 0.53403091 2.3090982 2.9056969 3.0636377 1.919013 1.0564184 -0.68651867 -1.8200645 -2.4094524 -3.2725835][-4.6113939 -4.03922 -3.2891717 -1.6363182 0.18335342 1.7974463 3.2078362 3.7988224 4.2174759 3.1662216 2.044631 0.015331268 -1.3588133 -2.2665253 -3.4457388][-3.9651461 -3.31842 -2.8111262 -1.4800186 -0.19490862 1.1851606 2.4245825 2.7227154 2.9422894 2.0722866 1.2153397 -0.3744297 -1.5753956 -2.5816789 -3.8097045][-4.175745 -3.7603495 -3.173068 -1.9257011 -0.98114824 -0.1036005 0.86192226 1.246274 1.5293446 0.33059978 -0.49201536 -1.9689426 -3.1387153 -3.5916519 -4.4460182][-4.4832411 -4.3364124 -4.1686373 -3.3556991 -2.606267 -1.7088146 -0.99099112 -0.73025084 -0.25340557 -0.938252 -1.6773243 -2.9458475 -4.0336823 -4.6469126 -5.5556822][-5.5346146 -5.0400171 -4.6677589 -4.231226 -4.043818 -3.6331241 -3.14955 -2.8511262 -2.5637493 -3.1054306 -3.6098511 -4.2343307 -4.6977568 -4.8933034 -5.2191973][-7.1468954 -6.58683 -6.0702963 -5.5522919 -5.3817167 -5.4720049 -5.29416 -5.2076316 -5.0304275 -5.2631264 -5.4031057 -5.6002593 -5.9955711 -6.0281749 -5.9352074][-7.5298352 -7.10536 -6.8339343 -6.3178463 -5.7479639 -5.6430283 -5.422616 -5.7033825 -5.9650207 -6.475564 -6.7020073 -6.6702752 -6.6364055 -6.1143665 -5.7488904][-8.9747353 -8.2735081 -7.5716634 -7.4469929 -7.3668504 -7.2010512 -6.8060379 -6.7360959 -6.8196507 -6.929997 -7.3928065 -7.6384964 -7.628674 -7.1222382 -6.5315084]]...]
INFO - root - 2017-12-15 11:23:47.346378: step 6310, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.648 sec/batch; 58h:42m:51s remains)
INFO - root - 2017-12-15 11:23:53.767388: step 6320, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:57m:36s remains)
INFO - root - 2017-12-15 11:24:00.232351: step 6330, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:58m:20s remains)
INFO - root - 2017-12-15 11:24:06.613607: step 6340, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 59h:04m:03s remains)
INFO - root - 2017-12-15 11:24:13.080929: step 6350, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 57h:35m:35s remains)
INFO - root - 2017-12-15 11:24:19.511415: step 6360, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:58m:42s remains)
INFO - root - 2017-12-15 11:24:25.963826: step 6370, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.627 sec/batch; 56h:50m:45s remains)
INFO - root - 2017-12-15 11:24:32.352865: step 6380, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 58h:06m:35s remains)
INFO - root - 2017-12-15 11:24:38.679941: step 6390, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 57h:41m:01s remains)
INFO - root - 2017-12-15 11:24:45.100760: step 6400, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 58h:29m:43s remains)
2017-12-15 11:24:45.585708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7194424 -6.2440729 -6.5646486 -6.8143435 -6.4627423 -5.0676584 -3.5878315 -2.7470284 -2.3493576 -2.9969611 -3.9726617 -4.7095728 -5.5311904 -6.3245697 -7.4993854][-6.2190022 -6.6707878 -6.6408563 -7.2709889 -7.6671114 -6.4542542 -5.0244942 -3.7647285 -2.690455 -3.1503868 -4.1287241 -4.7535992 -5.4353704 -5.7212243 -6.5677109][-3.4552016 -4.5172329 -5.3719206 -6.0197697 -5.5933213 -5.0045462 -4.4566984 -3.6432412 -3.0352969 -3.3345242 -3.659977 -4.2274323 -4.96717 -5.1797686 -5.9258351][-1.7845302 -2.0303497 -2.706409 -3.8589389 -4.4004292 -3.6410015 -2.4446044 -2.3072577 -2.4601345 -3.0935187 -3.573854 -3.84004 -4.4720612 -4.5341244 -4.97192][-1.0690293 -1.3049545 -1.5549803 -1.6599226 -1.909349 -1.6175647 -1.0889874 -0.90448141 -0.90512848 -1.8538961 -2.8112454 -3.4580073 -4.1269813 -4.1487226 -4.5638666][-1.6751809 -1.549202 -1.3651433 -0.89669895 -0.32815075 0.46119785 1.3741746 1.2369151 1.0473704 -0.30682611 -1.4017115 -2.412457 -3.1948161 -3.8060677 -4.6055465][-3.2542763 -2.6147995 -1.9583511 -0.7839222 0.32822466 1.1607385 1.9594321 2.2520957 2.290009 0.8453021 -0.68261719 -2.3898792 -3.5796292 -4.1825552 -4.966713][-3.1224728 -2.9592152 -2.2639174 -1.0538425 0.16108894 1.2827115 1.684176 1.9552741 2.6158853 1.6194358 0.3171525 -1.4644103 -2.6478076 -3.9491 -5.0647907][-4.2955775 -3.6134796 -2.9963651 -1.8913174 -0.92137766 0.45453024 1.4895806 1.602097 1.6345325 1.0221791 0.30000782 -1.0540438 -1.9058676 -2.7150998 -3.664526][-5.3670278 -4.7747741 -4.197957 -3.0187263 -1.9836583 -0.87042713 -0.15917253 0.34670019 1.26475 -0.026623249 -1.4090753 -2.2583299 -3.1460433 -3.614795 -4.0182638][-6.77339 -6.0439577 -5.2661219 -4.2825794 -3.2756829 -1.9889956 -1.0258436 -1.4148149 -1.8430877 -2.6807089 -3.0213099 -3.7951884 -4.4413128 -4.7534027 -5.2143173][-6.7450247 -6.0084977 -5.5117087 -4.7177963 -4.1213641 -3.4737358 -3.0059848 -2.9615831 -3.0385528 -4.2399921 -5.2336535 -5.855608 -6.10832 -6.0489225 -5.5808043][-6.008728 -5.6352911 -4.8612642 -4.3310976 -4.1993957 -3.8846259 -3.2557063 -3.8814988 -4.6026821 -5.158608 -6.137167 -6.5073056 -6.565136 -6.6580667 -6.528204][-4.5755291 -4.732336 -4.5243711 -3.8385782 -3.3882537 -3.073504 -2.9669647 -3.6355221 -4.2317009 -5.2314582 -6.5464797 -7.1509981 -7.0666475 -6.7505689 -6.3216686][-3.2892079 -3.84712 -3.9814527 -4.0556488 -3.8379941 -3.4827652 -3.2057757 -3.6341484 -4.4170928 -5.2092819 -5.7939634 -6.4002829 -6.9055843 -7.2497354 -7.0771995]]...]
INFO - root - 2017-12-15 11:24:52.055619: step 6410, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 58h:46m:47s remains)
INFO - root - 2017-12-15 11:24:58.456803: step 6420, loss = 0.37, batch loss = 0.26 (12.6 examples/sec; 0.633 sec/batch; 57h:20m:02s remains)
INFO - root - 2017-12-15 11:25:04.907262: step 6430, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 58h:13m:48s remains)
INFO - root - 2017-12-15 11:25:11.377096: step 6440, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.666 sec/batch; 60h:18m:31s remains)
INFO - root - 2017-12-15 11:25:17.796651: step 6450, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.631 sec/batch; 57h:11m:06s remains)
INFO - root - 2017-12-15 11:25:24.206455: step 6460, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 57h:55m:24s remains)
INFO - root - 2017-12-15 11:25:30.615503: step 6470, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.664 sec/batch; 60h:09m:54s remains)
INFO - root - 2017-12-15 11:25:37.021610: step 6480, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 57h:44m:12s remains)
INFO - root - 2017-12-15 11:25:43.545416: step 6490, loss = 0.38, batch loss = 0.27 (12.8 examples/sec; 0.624 sec/batch; 56h:33m:02s remains)
INFO - root - 2017-12-15 11:25:50.075060: step 6500, loss = 0.40, batch loss = 0.29 (12.7 examples/sec; 0.631 sec/batch; 57h:08m:59s remains)
2017-12-15 11:25:50.570350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.245306 -6.3122578 -6.2611675 -6.2381234 -6.1720285 -5.751605 -5.1566706 -4.6201043 -3.7884881 -3.5909441 -3.1818433 -4.1488953 -4.8712797 -5.5741634 -6.2069135][-5.9240313 -5.9077435 -6.1484332 -6.486052 -6.459867 -6.0875583 -5.4335032 -4.663342 -3.6087949 -3.448235 -3.1549869 -3.8011982 -4.3448014 -5.3183708 -5.9705696][-4.8906488 -4.69847 -4.5702672 -4.508914 -4.5638618 -4.2683172 -3.7376189 -3.2650294 -2.4091668 -2.440639 -2.2282972 -2.9666352 -3.8407938 -4.7054925 -5.3306732][-4.1622543 -3.4651747 -3.0991898 -2.62961 -2.1077776 -1.7042751 -1.1379042 -0.47575951 0.030819893 -0.72273684 -1.3888855 -2.5774283 -3.2110767 -3.9206741 -4.7197123][-2.4579697 -1.6593008 -0.92815781 -0.27087784 0.23370314 0.72343969 1.3034854 1.3409266 1.4882579 0.56467962 -0.46502876 -2.5136628 -3.5303922 -4.3286915 -5.0037823][-1.3049865 -0.64992237 -0.099514961 0.72155237 1.8069997 2.506721 2.848083 2.63341 2.5448642 1.3718524 0.35516357 -1.4175277 -2.8357859 -4.106472 -4.85169][-1.0045161 -0.021720409 0.89667082 1.3210568 1.6525722 2.4393563 3.3054967 3.3400884 3.1070724 1.6619382 0.022119522 -2.2585988 -3.9387434 -5.247736 -5.9598045][-1.4236684 -0.7434001 -0.15308809 0.82002592 1.5206656 2.1834321 2.925838 3.2194123 3.2131209 1.9723268 0.1413393 -2.2619867 -4.0518408 -5.3617773 -6.0838904][-2.9734774 -2.0368471 -1.152113 -0.025610924 0.62004423 1.1694312 1.588285 1.887856 2.040731 1.0691705 -0.55693674 -2.6847386 -3.9076526 -5.3427038 -5.8950167][-4.3195491 -3.5728703 -2.8044968 -1.8669066 -1.2114773 -0.48609686 0.02978754 0.51426077 0.77666616 -0.43080902 -1.8458614 -3.7382493 -4.8540115 -5.7089982 -6.0423074][-5.3511934 -4.619144 -4.1056037 -3.2694678 -2.4008198 -1.7149992 -1.2136321 -1.253159 -1.5026355 -2.1204376 -2.7490783 -4.2693148 -4.9838767 -5.5966063 -5.7338343][-6.2254734 -5.4412203 -4.7385359 -4.0399194 -3.3694682 -2.9018865 -2.3816557 -2.3629761 -2.5268383 -3.433228 -4.1957092 -4.277801 -4.3920593 -4.83261 -5.0022025][-6.0360255 -5.6495647 -5.1100569 -4.3476171 -3.8163667 -3.2884908 -3.0092587 -3.0173869 -3.0691881 -3.5522833 -4.06569 -4.1780949 -4.0764771 -4.1128731 -4.1382923][-5.6457238 -5.1270862 -4.9658003 -4.3312745 -3.5733435 -2.806078 -2.8083134 -2.8987565 -3.0837288 -3.09757 -3.4738984 -3.36769 -3.038095 -3.0881343 -3.1503181][-6.2458448 -5.77878 -5.3187 -4.5962362 -4.0674334 -3.3962784 -3.0210848 -3.2063437 -3.1364851 -3.3628244 -3.4026089 -3.0932612 -2.9955788 -3.0959196 -3.1145096]]...]
INFO - root - 2017-12-15 11:25:56.903240: step 6510, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 56h:15m:41s remains)
INFO - root - 2017-12-15 11:26:03.295174: step 6520, loss = 0.33, batch loss = 0.21 (12.8 examples/sec; 0.627 sec/batch; 56h:46m:41s remains)
INFO - root - 2017-12-15 11:26:09.617252: step 6530, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 56h:51m:35s remains)
INFO - root - 2017-12-15 11:26:16.053215: step 6540, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 59h:13m:41s remains)
INFO - root - 2017-12-15 11:26:22.465946: step 6550, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.647 sec/batch; 58h:33m:38s remains)
INFO - root - 2017-12-15 11:26:28.969719: step 6560, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 59h:24m:12s remains)
INFO - root - 2017-12-15 11:26:35.424031: step 6570, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 58h:18m:50s remains)
INFO - root - 2017-12-15 11:26:41.809739: step 6580, loss = 0.41, batch loss = 0.29 (12.9 examples/sec; 0.621 sec/batch; 56h:15m:56s remains)
INFO - root - 2017-12-15 11:26:48.200615: step 6590, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 58h:24m:16s remains)
INFO - root - 2017-12-15 11:26:54.719893: step 6600, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 57h:53m:29s remains)
2017-12-15 11:26:55.184453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8909159 -3.5808172 -3.6690035 -4.6210594 -4.1163607 -4.4170966 -3.4562387 -3.2870994 -2.9432945 -3.7319038 -5.0423703 -5.6507597 -6.2248363 -6.6107497 -6.5731173][-1.8934803 -1.8978758 -2.1163464 -2.6883554 -2.8301983 -3.49472 -3.554388 -4.3403358 -4.4238615 -4.3106194 -4.8989058 -5.0769262 -5.4402304 -5.73722 -6.49484][-4.2064543 -3.343215 -2.8008351 -2.4253983 -2.7395859 -2.9400182 -3.4413342 -4.0812693 -4.503582 -4.7032003 -6.4533539 -6.0594654 -6.2576675 -5.7743425 -6.3198118][-4.7956357 -4.7339296 -4.0416365 -3.3548899 -2.40066 -3.0366883 -2.7466965 -3.0320516 -3.6768374 -4.5146928 -6.1952043 -6.2166114 -6.8112354 -6.3569942 -7.3417358][-4.5963917 -3.8272164 -2.8568721 -2.3897152 -1.8598094 -1.6770492 -1.0275021 -1.6418943 -1.7564325 -2.0174217 -4.1424055 -5.5842781 -6.6186967 -7.0169678 -6.93676][0.055500031 -0.74239349 -1.5918474 -0.38954353 0.46087313 0.49196625 0.13927746 0.30953264 0.4812026 -0.28666306 -3.319706 -4.09478 -5.1995826 -6.6010008 -6.8085184][-0.18439817 0.028225899 0.64066696 0.17627335 0.13467455 0.81996346 0.98629475 0.92188931 0.0061426163 -0.023714066 -2.1601353 -3.5235014 -4.5406113 -5.2148786 -6.35983][1.0358105 0.46758795 -0.11149025 -0.31292772 0.24445534 0.33085203 0.91423035 0.8714571 0.36558437 -0.10267162 -2.8780842 -3.7441432 -4.5132971 -4.483438 -5.7380567][-0.48698616 -0.081098557 0.58941936 0.18340921 -0.44967937 0.35118675 1.4110336 1.1749 1.6271029 0.61568356 -2.2967129 -2.9258637 -4.2950845 -5.6451306 -5.998064][-1.9030895 -0.41396856 -1.0002265 -2.0234232 -0.71267939 -0.88841581 -1.3157744 -0.71704197 -0.1592536 -0.13228083 -2.0522852 -3.5292203 -5.7412629 -5.5159478 -6.251986][-4.0701547 -3.474854 -2.1885657 -2.7744417 -3.3621154 -3.7163646 -3.079854 -2.7826638 -3.4229484 -2.6915574 -3.7381825 -4.4879079 -4.8639269 -6.2117119 -7.1529388][-5.4192333 -5.9982748 -5.6729755 -5.3114843 -4.8044348 -5.2139292 -5.3177328 -5.5790391 -5.3671374 -4.5628476 -5.0478792 -5.0391445 -5.8725748 -6.204505 -6.2612729][-6.0577426 -6.1786804 -6.0730653 -5.7217054 -5.4140511 -5.1613865 -4.6177092 -5.7786903 -5.5832705 -5.6934953 -5.9519458 -5.2692213 -5.9459481 -6.2963381 -6.3809233][-5.1826687 -5.7027278 -5.2564597 -5.9518118 -5.8262877 -6.1752872 -6.1035995 -6.1881495 -6.1208038 -5.8074145 -6.4020896 -5.6976104 -6.5729961 -6.1724806 -5.892498][-4.4680953 -4.8099442 -5.4185371 -5.7237835 -5.6257296 -5.8458824 -6.3507071 -6.7941875 -7.5131164 -7.1472569 -7.3587813 -7.0685682 -6.9972181 -7.4230185 -7.5444965]]...]
INFO - root - 2017-12-15 11:27:01.567522: step 6610, loss = 0.40, batch loss = 0.28 (12.6 examples/sec; 0.635 sec/batch; 57h:30m:36s remains)
INFO - root - 2017-12-15 11:27:07.938442: step 6620, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 58h:18m:22s remains)
INFO - root - 2017-12-15 11:27:14.418941: step 6630, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 57h:10m:18s remains)
INFO - root - 2017-12-15 11:27:20.812946: step 6640, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 56h:18m:02s remains)
INFO - root - 2017-12-15 11:27:27.234585: step 6650, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 57h:33m:54s remains)
INFO - root - 2017-12-15 11:27:33.654123: step 6660, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 57h:33m:15s remains)
INFO - root - 2017-12-15 11:27:40.107921: step 6670, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 59h:26m:49s remains)
INFO - root - 2017-12-15 11:27:46.569293: step 6680, loss = 0.35, batch loss = 0.23 (12.4 examples/sec; 0.645 sec/batch; 58h:24m:56s remains)
INFO - root - 2017-12-15 11:27:52.962219: step 6690, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.636 sec/batch; 57h:31m:15s remains)
INFO - root - 2017-12-15 11:27:59.324032: step 6700, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 57h:33m:04s remains)
2017-12-15 11:27:59.857990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3462658 -3.8812225 -4.6737595 -4.7519841 -4.3390617 -3.8743963 -3.2115998 -2.4014063 -1.5442324 -1.2676916 -1.8617582 -2.4487257 -3.3893247 -4.8254023 -4.9712415][-1.4198685 -2.1014652 -3.4324055 -4.6728611 -5.1877193 -4.5971203 -3.7195225 -2.7449827 -2.1144042 -1.938344 -2.6512618 -2.9168096 -3.7652509 -5.3622885 -5.4678974][-1.1535487 -1.9927754 -3.2426519 -4.6616173 -5.44197 -5.7805748 -5.6414065 -4.8730817 -3.9746623 -3.2325015 -3.1573057 -3.514209 -4.7214108 -6.2139935 -6.4720135][-1.2417297 -2.5222735 -3.7578523 -4.7673836 -5.5195432 -4.8206878 -4.3710108 -3.7721317 -3.3727417 -3.4555731 -3.7876279 -3.9468968 -5.0388656 -6.5175285 -7.1462622][-1.1905675 -1.6938167 -2.6324639 -3.4730072 -3.3294468 -2.1448703 -1.0844951 -0.48264837 -0.38888597 -1.5186234 -3.3202767 -4.4130754 -5.5533538 -7.0774031 -7.3609934][-0.30235004 -0.50679159 -0.95045233 -1.4139972 -1.3302369 -0.45921707 0.45895004 1.5391645 1.8157864 0.56390858 -1.2293191 -2.9730997 -5.052269 -7.1864738 -7.7349567][-0.27263737 0.55852127 0.2487874 -0.15132523 0.86405563 1.8823118 3.0255127 3.9680805 3.6120481 2.2920904 0.47903013 -1.2077298 -3.708194 -6.4368243 -7.6288514][0.44538307 1.0920973 1.6861725 1.8961143 2.7450228 3.8391581 5.0453138 5.607646 4.8551741 3.3121328 1.1127491 -0.92043209 -3.0749617 -5.6890697 -6.9199672][0.30664539 0.72942066 1.4316864 1.8080111 2.4805727 3.4409389 4.1497374 4.709177 4.9354696 3.59828 0.93149185 -0.770751 -2.9956026 -5.4536285 -6.1245613][-0.66822863 -0.50853777 -0.32514 0.27552605 0.88684082 1.7733698 2.2757435 2.7008085 2.6712818 1.8645439 0.032168388 -1.5874171 -3.8257239 -5.8639612 -6.3353314][-3.4734254 -2.9568715 -2.66605 -2.5194201 -2.3254223 -1.6430593 -1.1949487 -1.3412004 -1.2780848 -2.1130524 -3.3189564 -4.1241326 -5.37296 -6.4649334 -6.6798687][-4.5337219 -4.517889 -4.2760096 -4.2630224 -4.4214206 -4.2279882 -4.1949506 -4.2013631 -4.0922709 -4.6390009 -4.9883118 -5.4374113 -6.0436912 -7.3059063 -7.4326072][-6.6290436 -6.2981963 -5.9616575 -6.1233339 -5.7281508 -5.5649619 -5.8056464 -5.8424392 -5.8656178 -6.3742313 -7.0948071 -7.408915 -7.601727 -7.6329584 -7.908442][-6.9765124 -7.1579037 -7.016036 -7.1482697 -6.8659792 -6.6618066 -6.37222 -6.6574278 -7.1779428 -7.2470255 -7.4487619 -7.3403912 -7.4460139 -7.7135291 -7.0531845][-7.1647382 -6.9845119 -6.8592682 -7.1689239 -6.8698473 -6.9137177 -7.1926332 -7.10906 -6.8513594 -6.7184372 -6.8034024 -6.8654881 -6.6736016 -6.384222 -6.4041982]]...]
INFO - root - 2017-12-15 11:28:06.332744: step 6710, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:43m:32s remains)
INFO - root - 2017-12-15 11:28:12.776814: step 6720, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 59h:26m:54s remains)
INFO - root - 2017-12-15 11:28:19.203259: step 6730, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 58h:58m:18s remains)
INFO - root - 2017-12-15 11:28:25.653990: step 6740, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 57h:50m:15s remains)
INFO - root - 2017-12-15 11:28:31.967466: step 6750, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.645 sec/batch; 58h:22m:40s remains)
INFO - root - 2017-12-15 11:28:38.323263: step 6760, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:52m:24s remains)
INFO - root - 2017-12-15 11:28:44.758955: step 6770, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 58h:13m:29s remains)
INFO - root - 2017-12-15 11:28:51.097323: step 6780, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 57h:55m:19s remains)
INFO - root - 2017-12-15 11:28:57.482232: step 6790, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 56h:50m:57s remains)
INFO - root - 2017-12-15 11:29:03.918300: step 6800, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 57h:27m:53s remains)
2017-12-15 11:29:04.453866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9303417 -5.6867418 -5.282774 -4.8210468 -4.8013196 -5.0645657 -5.2263317 -5.16326 -4.9928093 -4.78645 -5.7400703 -6.0637355 -7.0041037 -7.2045431 -7.2663236][-5.5069518 -5.2861867 -4.9399509 -4.9264846 -4.9553432 -5.043643 -5.3567924 -5.352212 -5.2289066 -4.9269209 -5.8492641 -5.8203726 -6.5456409 -6.852181 -6.9528718][-4.4256396 -4.2458143 -3.9900389 -4.0481462 -3.9334705 -4.2758694 -4.3647966 -4.1758761 -4.084012 -3.8605442 -5.00832 -4.7087975 -5.1939473 -5.7506847 -5.9448867][-3.699223 -3.4402714 -3.1325254 -2.9939137 -2.9022532 -3.1569195 -3.0270066 -2.9983439 -2.8837271 -2.5767975 -3.6051872 -3.55327 -4.5894127 -5.0993195 -5.0271764][-3.4681959 -2.96236 -2.2716122 -1.9858413 -1.8101635 -1.5386286 -1.2617359 -1.2388554 -0.96789694 -0.92117262 -2.2511239 -2.0277352 -2.8759947 -3.6950648 -3.9439023][-2.3339529 -2.0726876 -1.8178482 -1.1503239 -0.40240479 -0.04830122 0.23260069 0.4608469 0.6015954 0.39904737 -0.92225695 -1.1053619 -2.1022711 -2.5976529 -2.9935436][-2.824348 -1.9829922 -1.2627263 -0.81188631 -0.31586838 0.48689985 1.1683698 1.2818875 1.4134822 1.0797429 -0.28957653 -0.83689547 -2.120863 -2.9908981 -3.1397042][-2.3396888 -2.0883145 -1.7389989 -1.0220103 -0.16911077 0.64069033 1.2139144 1.4724507 1.8584733 1.4955363 -0.17513323 -0.9327116 -2.0290828 -3.0252066 -3.8177533][-2.1626649 -2.0021629 -1.3131051 -1.1390595 -0.3611784 0.5179038 1.218019 1.4980025 1.7939591 1.2440076 -0.42031765 -1.4820991 -2.4969 -3.5233338 -4.2259884][-3.5269723 -3.3154125 -2.635272 -2.2998872 -1.5069928 -0.5676055 0.30531597 0.53961134 0.71325636 0.018640518 -1.2251658 -1.9597163 -3.0527496 -4.1477981 -5.0639095][-5.835618 -5.0583563 -4.546598 -4.1761732 -3.4049802 -2.5620923 -1.4869514 -1.1344013 -1.0434284 -1.4225016 -2.2955222 -2.9707251 -3.8023334 -4.9027352 -5.56725][-6.9905691 -6.4210062 -5.5970283 -5.1812296 -4.8975363 -4.1300807 -3.3039026 -3.1885972 -2.9933367 -3.0687623 -3.4527617 -4.1666403 -4.7764349 -5.4754782 -5.7627378][-7.4548621 -7.2604485 -6.8474331 -6.544065 -5.9900279 -5.2810454 -4.4599285 -4.097764 -3.8774717 -4.1124668 -4.5682793 -4.5536346 -4.6868763 -5.082634 -5.33848][-7.0960383 -7.4197669 -7.3422813 -7.4028625 -7.0489631 -6.4417834 -5.7270374 -5.41535 -5.1059422 -4.9645758 -5.156045 -5.3947382 -5.4371438 -5.232563 -4.99919][-6.885819 -7.126307 -7.1595783 -7.4230862 -7.4023581 -7.3542438 -7.029151 -6.7573829 -6.6690106 -6.6662812 -6.4256339 -6.2216282 -6.2296076 -5.9788156 -5.7220569]]...]
INFO - root - 2017-12-15 11:29:10.874723: step 6810, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:51m:54s remains)
INFO - root - 2017-12-15 11:29:17.292621: step 6820, loss = 0.37, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 58h:06m:45s remains)
INFO - root - 2017-12-15 11:29:23.813587: step 6830, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 57h:45m:14s remains)
INFO - root - 2017-12-15 11:29:30.179028: step 6840, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 58h:26m:28s remains)
INFO - root - 2017-12-15 11:29:36.621906: step 6850, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 58h:56m:14s remains)
INFO - root - 2017-12-15 11:29:43.014627: step 6860, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 58h:30m:38s remains)
INFO - root - 2017-12-15 11:29:49.384318: step 6870, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 56h:32m:04s remains)
INFO - root - 2017-12-15 11:29:55.747100: step 6880, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 58h:26m:28s remains)
INFO - root - 2017-12-15 11:30:02.217476: step 6890, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 59h:09m:15s remains)
INFO - root - 2017-12-15 11:30:08.645563: step 6900, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 58h:28m:26s remains)
2017-12-15 11:30:09.230536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.44968 -4.7715273 -4.82274 -4.2536049 -3.4102659 -2.991962 -2.6491876 -2.8315892 -3.4971876 -4.9397964 -6.0868773 -7.029798 -7.7833338 -7.1210971 -7.2325244][-3.8154714 -4.1825361 -4.5225086 -4.1937895 -3.8853633 -3.3360238 -2.8289709 -2.6318021 -2.8499603 -3.901829 -5.758028 -7.0161748 -8.1780968 -8.1227808 -8.0395546][-4.5595851 -5.0067625 -5.0270085 -4.3019409 -3.4409518 -2.9839945 -2.7189069 -2.5643644 -2.5391345 -3.9718039 -5.1043191 -6.724246 -8.0326262 -7.9800186 -8.1484051][-4.8119793 -4.8199863 -4.5146775 -3.4456134 -2.3115788 -1.7809367 -1.6435161 -2.3689518 -2.7197576 -3.8639905 -5.0047016 -6.3699551 -7.2895422 -7.2996893 -7.6955419][-4.3720131 -3.4369512 -2.5903335 -1.9630132 -1.051414 -0.61043358 -0.50783253 -1.1694274 -2.1359415 -3.1559691 -4.542038 -5.724546 -6.8081136 -6.7737288 -6.8248787][-4.0829897 -2.2983794 -0.90164757 -0.031936646 0.5649085 1.1748805 1.3418889 0.21367121 -0.41751909 -2.1675196 -3.2233791 -4.4675617 -5.6515827 -5.8792133 -6.2874646][-3.1731853 -1.849956 -0.39097309 1.269855 2.8277164 3.0327258 2.9028096 2.1965404 0.96286249 -1.0442705 -2.5403366 -3.7708149 -4.6674061 -5.0424366 -5.5627179][-2.9961357 -1.9495587 -0.87115908 0.91571188 2.9903216 3.8020997 3.9975829 3.0698724 1.8274369 -0.40395164 -2.359067 -3.6616163 -4.6181192 -4.7546625 -5.0502043][-3.8013134 -2.7374377 -1.9326825 -0.46029902 0.70396948 1.9476628 2.854712 2.6730876 2.0320306 -0.23424625 -2.5928316 -4.4899864 -5.1975946 -4.9942746 -5.4859428][-4.3741331 -3.8060496 -3.2160215 -2.1591415 -1.111445 -0.60059738 0.218503 0.72060061 0.94858885 -0.88414383 -3.0622354 -4.7560439 -5.8880081 -5.8336463 -6.102026][-5.6175194 -5.2178659 -4.9476709 -4.1448135 -3.380589 -2.844429 -2.1268458 -2.0468459 -2.1116185 -3.3855762 -4.6488914 -5.5662127 -6.2963872 -6.4014368 -6.7638979][-6.7935686 -6.1698933 -5.6777973 -5.1015186 -4.7606659 -4.2680693 -3.8767188 -3.968184 -4.246438 -5.114974 -5.4058332 -5.8834257 -6.5609894 -6.8308587 -6.8807077][-7.6361775 -7.1190844 -6.671278 -6.2048187 -5.9788237 -5.8069839 -5.7021761 -5.550745 -5.5118937 -6.2971258 -6.5433092 -6.3140888 -6.5532384 -6.5777736 -6.6397219][-8.4136744 -7.7995534 -7.2117052 -7.0924721 -6.5172195 -6.2871842 -6.6478367 -6.6946173 -6.54165 -6.4662094 -6.5128775 -6.162354 -6.3592811 -6.1340837 -5.9763703][-8.2752485 -7.9958558 -8.0887556 -7.8849978 -7.6917596 -7.190762 -6.7905197 -7.0215731 -7.0693645 -7.2934761 -7.0972662 -6.3480406 -6.0716219 -5.8850117 -5.6495247]]...]
INFO - root - 2017-12-15 11:30:15.598600: step 6910, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 56h:08m:52s remains)
INFO - root - 2017-12-15 11:30:22.000043: step 6920, loss = 0.27, batch loss = 0.15 (13.1 examples/sec; 0.609 sec/batch; 55h:03m:02s remains)
INFO - root - 2017-12-15 11:30:28.418391: step 6930, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 57h:30m:23s remains)
INFO - root - 2017-12-15 11:30:34.758129: step 6940, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 57h:10m:48s remains)
INFO - root - 2017-12-15 11:30:41.100720: step 6950, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 58h:25m:26s remains)
INFO - root - 2017-12-15 11:30:47.507088: step 6960, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:41m:38s remains)
INFO - root - 2017-12-15 11:30:53.988323: step 6970, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 58h:59m:31s remains)
INFO - root - 2017-12-15 11:31:00.331288: step 6980, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 56h:47m:27s remains)
INFO - root - 2017-12-15 11:31:06.738029: step 6990, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 58h:01m:54s remains)
INFO - root - 2017-12-15 11:31:13.059414: step 7000, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.630 sec/batch; 56h:58m:54s remains)
2017-12-15 11:31:13.591410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2886019 -3.7416303 -3.280479 -2.7599044 -2.2286973 -1.7565513 -0.98073053 -0.65909958 -0.23898363 -1.4002628 -2.3391557 -3.7438002 -4.8258991 -5.18124 -6.4028258][-3.1338754 -2.5706124 -2.1357121 -1.3684721 -0.835845 -0.44745493 -0.017482758 0.49712992 0.67238855 -0.72690153 -1.6023488 -3.1434469 -4.3398972 -4.7476034 -6.2432227][-3.5401766 -2.3552318 -1.4956875 -0.67084074 0.1122303 0.53590059 0.943748 0.90261126 0.68078661 -0.85529661 -1.9860196 -3.6729693 -4.5557766 -4.7870955 -5.9053354][-4.5572472 -3.2150736 -2.2080207 -0.54938173 0.88970327 1.4779344 1.9302773 1.8637967 1.4531722 -0.40610456 -1.7065196 -3.7381239 -4.9083729 -5.062304 -5.8487864][-5.8443818 -3.4823136 -1.5256085 -0.15599537 1.0827794 1.8643098 2.4451995 2.5072055 2.3373122 0.10063887 -1.1856265 -3.247694 -4.441144 -4.8675585 -5.8003807][-6.2338276 -4.1268697 -2.5874467 -0.44482374 1.2125134 2.3929667 3.346149 2.7426019 2.0439725 0.29550791 -0.085884571 -2.0526056 -3.3029065 -3.9215994 -5.209527][-6.4277592 -4.4736471 -2.1403117 -0.42590237 1.0559449 1.9387031 2.416676 2.4720197 2.4600816 0.76202345 0.39877224 -1.2532954 -2.343029 -2.8847175 -3.6931076][-6.534832 -4.3407559 -2.4756837 -0.64518595 0.62751341 0.99246836 2.0239215 2.4441075 2.4292226 1.0034738 0.41378784 -0.76265335 -1.7467403 -2.1863556 -3.1784863][-6.6751781 -5.6762066 -4.0506644 -2.0666447 -0.13144112 0.64985895 0.65137339 0.79468584 1.2046714 -0.13565159 -0.39997339 -1.5694542 -2.2789807 -2.285182 -3.2427444][-7.8471928 -6.9976187 -5.7501078 -3.3084078 -1.4261074 -0.48084593 0.36578894 0.69942331 0.81381655 -0.81774569 -1.8565931 -2.4896517 -2.9388371 -3.2245412 -3.8567431][-8.82861 -8.6074467 -7.5991945 -5.6405134 -3.4947376 -1.8664379 -1.1718206 -0.94856262 -0.85982561 -2.0070906 -2.6382942 -3.7006004 -4.485393 -4.6015692 -5.3671937][-10.349259 -9.78822 -8.6891632 -7.4819388 -5.74212 -4.1928544 -2.8203473 -2.5153041 -2.5929785 -3.1650562 -4.3476992 -4.8595333 -5.4392281 -6.0641451 -7.1760607][-11.303054 -11.05862 -10.011003 -8.4263678 -6.722785 -5.4625373 -4.2857275 -3.5686679 -3.2125683 -4.274106 -4.7768455 -5.2075195 -5.7422357 -6.2066293 -7.2077117][-11.484039 -11.514921 -11.248662 -10.254761 -8.8474865 -7.4175797 -5.6354127 -5.091413 -4.7713041 -4.7253404 -5.0083036 -5.2275872 -5.4827061 -6.0692272 -6.7610955][-11.132026 -11.329374 -11.366136 -10.899409 -9.6396971 -8.3124609 -7.1533904 -6.4027691 -6.0289583 -6.1683717 -6.0582986 -5.7839327 -5.8460722 -6.2976227 -6.6617417]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 11:31:20.000629: step 7010, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 58h:33m:35s remains)
INFO - root - 2017-12-15 11:31:26.445958: step 7020, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.658 sec/batch; 59h:32m:05s remains)
INFO - root - 2017-12-15 11:31:32.814928: step 7030, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 56h:15m:07s remains)
INFO - root - 2017-12-15 11:31:39.209829: step 7040, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 57h:56m:54s remains)
INFO - root - 2017-12-15 11:31:45.658043: step 7050, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:52m:43s remains)
INFO - root - 2017-12-15 11:31:52.075088: step 7060, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 58h:28m:12s remains)
INFO - root - 2017-12-15 11:31:58.524584: step 7070, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:50m:36s remains)
INFO - root - 2017-12-15 11:32:04.920382: step 7080, loss = 0.31, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 56h:09m:15s remains)
INFO - root - 2017-12-15 11:32:11.428945: step 7090, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 58h:48m:39s remains)
INFO - root - 2017-12-15 11:32:17.831496: step 7100, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 57h:30m:14s remains)
2017-12-15 11:32:18.339193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2785721 -4.3950634 -5.6679697 -6.1211081 -6.156239 -6.2308969 -5.9817743 -4.971364 -3.8785644 -3.8911161 -4.5119033 -5.428268 -5.698843 -5.7695475 -5.5880222][-3.5028276 -3.6862123 -4.590013 -5.2848535 -5.6085482 -5.1970434 -4.6536927 -4.3083086 -4.2363958 -4.1851239 -4.5905104 -6.2119479 -6.5876904 -6.4774294 -6.2859354][-3.0075488 -3.7061555 -4.1424594 -4.2471266 -3.8804111 -3.80577 -3.5404248 -3.3358269 -3.512928 -4.0126858 -5.3061171 -6.7928867 -7.2008905 -7.6525126 -7.7530365][-3.4032884 -3.3422098 -3.4999566 -4.1223831 -4.0792332 -3.3158779 -2.6395993 -2.9417939 -2.8836694 -3.3843913 -4.5345721 -6.2898765 -7.2668428 -7.7267952 -8.1665974][-3.1323423 -3.6452563 -3.7921844 -3.2353868 -2.4318867 -1.1754875 -0.43561172 -0.62110424 -2.1317072 -3.2389846 -4.1707997 -5.8020315 -6.6207304 -7.0260229 -7.2506008][-4.0943327 -3.8102593 -3.4490275 -2.0096426 -0.6230588 0.77848005 1.9600549 1.5891089 0.13237524 -1.8838172 -4.1016111 -5.8345284 -6.1820707 -6.524889 -6.8227763][-3.7352223 -3.6200316 -2.7982469 -1.209125 1.0677638 3.0139041 4.3268733 3.9997401 3.1120877 0.55846548 -2.2428389 -4.9937134 -6.0842438 -6.0161324 -5.9866567][-2.486485 -2.4464049 -2.5244236 -0.55777788 2.3369231 4.368053 5.8009353 5.5931554 4.6898818 1.8517365 -1.2019 -4.0760083 -5.1866922 -5.5603218 -6.0805798][-3.6125484 -2.683568 -1.9011607 -0.63495588 0.98401308 2.8329349 4.5947843 4.256103 3.2657981 1.2528415 -1.5685191 -4.18713 -5.2660365 -5.5687289 -5.7051144][-5.5285606 -4.9889779 -4.5297832 -3.326622 -2.1224322 -0.16343021 1.4783587 1.611444 1.6336417 -0.70785809 -3.5432844 -5.7230916 -6.30116 -6.3724866 -6.6695895][-6.8490934 -6.9357748 -6.3779926 -5.1533284 -4.1060514 -2.809 -1.8807468 -1.4057779 -1.1550026 -2.4675989 -4.134 -6.2619848 -6.9532051 -6.8962703 -6.760838][-6.9402018 -6.5450935 -6.6425905 -6.1892586 -5.9378829 -5.2182212 -4.136579 -3.904578 -3.9537125 -4.8639965 -5.4016566 -6.9339895 -6.5943942 -6.6368141 -6.9636779][-7.4103384 -7.2410707 -7.061089 -6.4168963 -6.3123708 -6.4654508 -6.467308 -6.3592377 -5.7602882 -6.0791769 -6.21951 -7.237236 -7.516818 -7.7819242 -8.112154][-8.3426962 -8.031868 -6.7902217 -6.3794575 -6.6044517 -6.5403781 -6.8074594 -7.2222028 -7.6741357 -7.9715195 -7.5730934 -7.6858659 -7.4612741 -7.2174072 -7.30664][-8.6117935 -9.0819483 -9.3091679 -8.6991825 -7.6052871 -7.4618955 -7.3416882 -7.5247922 -8.0541687 -8.2768841 -8.31421 -7.79141 -7.06576 -6.9404683 -6.760386]]...]
INFO - root - 2017-12-15 11:32:24.664093: step 7110, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 59h:36m:04s remains)
INFO - root - 2017-12-15 11:32:31.122272: step 7120, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 60h:03m:13s remains)
INFO - root - 2017-12-15 11:32:37.543383: step 7130, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 58h:48m:43s remains)
INFO - root - 2017-12-15 11:32:43.953568: step 7140, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 59h:20m:04s remains)
INFO - root - 2017-12-15 11:32:50.315515: step 7150, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 58h:33m:02s remains)
INFO - root - 2017-12-15 11:32:56.700527: step 7160, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 56h:21m:11s remains)
INFO - root - 2017-12-15 11:33:03.163753: step 7170, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 59h:26m:41s remains)
INFO - root - 2017-12-15 11:33:09.495321: step 7180, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 58h:17m:47s remains)
INFO - root - 2017-12-15 11:33:15.943757: step 7190, loss = 0.37, batch loss = 0.26 (12.8 examples/sec; 0.623 sec/batch; 56h:18m:39s remains)
INFO - root - 2017-12-15 11:33:22.354231: step 7200, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 56h:05m:21s remains)
2017-12-15 11:33:22.906402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1288953 -1.2591481 -1.6829839 -2.0404372 -2.555624 -3.0161023 -2.7638469 -2.5476832 -2.2268834 -2.7264123 -3.1740198 -3.94955 -5.0242386 -5.6303558 -5.9724951][-1.7491527 -1.6987367 -2.0744739 -2.4136419 -2.9131093 -3.0892277 -2.8304315 -2.728682 -2.819345 -3.6332178 -3.6905651 -4.4703312 -5.6071453 -6.3098545 -6.67007][-2.0411844 -1.8002443 -1.6910157 -1.9552827 -2.1330647 -2.2459483 -2.00485 -1.9426889 -2.2106342 -3.04677 -3.3773932 -4.1283059 -5.0129023 -5.5663948 -6.0226789][-2.0905905 -1.8516459 -1.7010202 -1.6998248 -1.7253604 -1.5388417 -1.1609917 -1.2300658 -1.3921461 -2.4030318 -3.015132 -3.869117 -4.8147244 -5.294476 -5.6419549][-2.4344306 -2.114193 -1.8154559 -1.601047 -1.4062819 -0.92138624 -0.5277276 -0.43744516 -0.51010323 -1.4141374 -1.9054847 -2.8908367 -3.9414833 -4.6594386 -5.1106262][-2.8351812 -2.3106432 -1.8722134 -1.4607739 -1.0668759 -0.48616457 0.030623436 0.18577385 0.27609158 -0.64777136 -1.1464605 -1.9050598 -2.7983122 -3.6657736 -4.5821104][-2.7667522 -2.3242645 -1.9231567 -1.229476 -0.49148321 0.12432289 0.54898787 0.95380068 1.2638936 0.30935764 -0.23499966 -1.0485182 -2.0992389 -3.2171216 -4.3021688][-2.6569533 -2.1680102 -1.6083984 -0.90095234 -0.084332466 0.57515383 1.1071715 1.5029092 1.9051909 1.1059213 0.47818613 -0.58692694 -1.7913995 -2.9492693 -3.9322467][-2.5838919 -1.888772 -1.4616032 -0.8717885 -0.20929337 0.62539911 1.1313252 1.3295875 1.6097703 0.87783384 0.42632484 -0.61348438 -2.0148239 -3.1892638 -4.103385][-2.9098268 -2.5614257 -1.9270186 -1.211051 -0.71904945 -0.13816833 0.34086657 0.61956072 0.93253279 -0.046766281 -0.67429209 -1.6449437 -2.8392925 -3.788898 -4.536747][-3.8208358 -3.3835087 -2.8706255 -2.2707672 -1.6739097 -1.2593141 -1.033195 -0.83611012 -0.56525278 -1.5970445 -2.2546926 -2.9956222 -3.9777913 -4.6534557 -5.2752857][-4.6572113 -4.1174049 -3.8550708 -3.3282537 -2.8890209 -2.5075688 -2.3910298 -2.4881139 -2.4361334 -3.068121 -3.5985312 -4.2832851 -5.008626 -5.2528114 -5.5525751][-5.4695926 -5.1336632 -4.9920793 -4.6791048 -4.1894574 -4.01116 -3.9745362 -3.8507249 -3.7935119 -4.4951072 -4.6770926 -5.0628958 -5.5399446 -5.6271634 -5.381999][-5.9588561 -5.6757593 -5.3344207 -5.0911474 -4.7132049 -4.6740408 -4.6764846 -4.7080507 -4.7804337 -5.24356 -5.3464909 -5.21304 -5.5471387 -5.6091895 -5.35863][-6.8173823 -6.6726093 -6.3687134 -5.7633491 -5.4667282 -5.415822 -5.4462671 -5.5181823 -5.5231781 -5.6245823 -5.7673144 -5.7976804 -5.8911409 -5.869493 -5.7144823]]...]
INFO - root - 2017-12-15 11:33:29.306336: step 7210, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 57h:38m:05s remains)
INFO - root - 2017-12-15 11:33:35.726185: step 7220, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 58h:07m:56s remains)
INFO - root - 2017-12-15 11:33:42.107987: step 7230, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 57h:00m:01s remains)
INFO - root - 2017-12-15 11:33:48.638336: step 7240, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 58h:36m:27s remains)
INFO - root - 2017-12-15 11:33:55.023063: step 7250, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 57h:11m:08s remains)
INFO - root - 2017-12-15 11:34:01.480354: step 7260, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 57h:04m:06s remains)
INFO - root - 2017-12-15 11:34:07.880193: step 7270, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:47m:17s remains)
INFO - root - 2017-12-15 11:34:14.284290: step 7280, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 57h:21m:01s remains)
INFO - root - 2017-12-15 11:34:20.744016: step 7290, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:54m:13s remains)
INFO - root - 2017-12-15 11:34:27.148482: step 7300, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:46m:51s remains)
2017-12-15 11:34:27.636053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7978973 -8.3340864 -9.7666569 -10.945782 -10.856265 -10.399958 -10.107756 -9.6296825 -8.5667906 -8.025445 -7.0620832 -6.3656979 -5.6782346 -5.7765555 -5.8395033][-6.6244249 -8.88018 -10.783857 -12.144423 -13.125862 -13.31888 -12.576147 -11.222812 -9.6630306 -9.0319195 -8.7942858 -9.27655 -8.6586046 -7.5765147 -7.2420807][-5.5686159 -6.9528675 -9.2545128 -10.700705 -11.340247 -12.015594 -11.777261 -11.31394 -10.165567 -9.6605377 -9.6200619 -10.610861 -10.754989 -10.300055 -9.9794369][-5.7896614 -6.6676321 -7.3692384 -8.4427519 -9.3368053 -10.197398 -9.9061966 -9.59407 -8.8892975 -8.6462708 -9.0929279 -10.452128 -11.135974 -11.314116 -11.515204][-5.9603748 -5.6294026 -5.5774345 -6.3692293 -6.5071983 -6.1985483 -6.1039143 -5.8022876 -5.752315 -6.3448348 -7.0808506 -8.4460554 -10.345421 -10.733513 -11.321055][-6.3346872 -5.7897053 -4.9940367 -4.5152988 -3.1919184 -1.9681873 -0.13187599 0.56895113 0.37572002 -1.4009161 -3.1434851 -6.0740514 -7.8793859 -8.75656 -9.7883577][-5.489841 -4.8190241 -4.4539375 -2.5228972 -0.54952765 0.59426546 3.0886283 3.9385152 4.1026692 1.8931165 -0.6526041 -3.5209155 -5.9162507 -7.049273 -7.6707482][-6.05959 -5.2078347 -4.181509 -1.4123473 0.55850267 1.4486518 3.9055409 4.2391191 4.6312242 2.5865054 -0.31884098 -2.5823693 -4.7908926 -5.8685 -7.0709805][-6.1863084 -6.19838 -5.5785217 -3.8073416 -1.2754817 0.26247358 2.4515214 2.5056758 3.2519002 0.94464064 -1.3185229 -3.4348426 -5.0233307 -5.3489037 -6.2174211][-8.6684408 -7.7546396 -7.3037543 -6.3694868 -4.7732153 -2.8332996 -1.4777236 -0.84518051 -0.32711887 -3.12787 -4.7789917 -6.3009872 -6.9376864 -6.8624673 -7.0373721][-9.52985 -9.3581133 -9.2167177 -8.6757736 -7.583879 -5.4575438 -4.5960007 -4.6685934 -4.2894621 -5.5059967 -6.2181311 -7.6971354 -8.7011881 -8.92396 -9.26347][-9.9520922 -9.9733334 -9.5574656 -8.8652782 -8.5307446 -8.1339025 -7.1433725 -6.6140146 -6.3332176 -7.8482409 -7.7195778 -8.4684954 -8.9752722 -9.153882 -9.3903694][-9.90308 -9.838273 -9.7680454 -9.6254578 -9.2423916 -8.9765282 -8.480176 -8.13703 -7.755878 -8.3700352 -8.5632687 -9.0530071 -8.7619972 -8.6180429 -9.3525753][-8.2518415 -8.5815334 -9.4603586 -9.3344164 -9.0815611 -8.88358 -8.1879549 -8.2363663 -8.3147345 -8.0431786 -8.3982925 -8.8522129 -8.5134945 -8.0030661 -8.1000271][-6.6315155 -7.792738 -8.195364 -8.6382637 -8.93286 -8.5331392 -8.027215 -7.9207048 -7.7366419 -7.9857116 -7.8715267 -7.9043531 -8.38686 -8.4479818 -8.7394266]]...]
INFO - root - 2017-12-15 11:34:33.994062: step 7310, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 57h:00m:49s remains)
INFO - root - 2017-12-15 11:34:40.428401: step 7320, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 59h:03m:52s remains)
INFO - root - 2017-12-15 11:34:46.855094: step 7330, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 58h:24m:11s remains)
INFO - root - 2017-12-15 11:34:53.245010: step 7340, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 56h:53m:05s remains)
INFO - root - 2017-12-15 11:34:59.699823: step 7350, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 59h:50m:34s remains)
INFO - root - 2017-12-15 11:35:06.107076: step 7360, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 58h:53m:02s remains)
INFO - root - 2017-12-15 11:35:12.520200: step 7370, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:49m:35s remains)
INFO - root - 2017-12-15 11:35:18.948141: step 7380, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 58h:54m:36s remains)
INFO - root - 2017-12-15 11:35:25.344789: step 7390, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:48m:22s remains)
INFO - root - 2017-12-15 11:35:31.719297: step 7400, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 59h:14m:38s remains)
2017-12-15 11:35:32.299606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0533609 -5.808104 -5.4277411 -5.0836129 -4.8533731 -4.5831251 -4.1761045 -3.9538214 -3.7268946 -4.4379578 -4.951231 -6.321846 -7.03301 -7.2677531 -7.8109546][-6.0093012 -5.7800016 -5.7077794 -5.6936116 -5.4605894 -5.142417 -4.7577109 -4.434845 -3.9924557 -4.6467714 -5.2914305 -6.3569336 -6.8803554 -7.3861094 -8.1589851][-5.9171324 -5.7483706 -5.6597476 -5.6691871 -5.5153985 -5.1952662 -4.5728321 -4.0640526 -3.5800836 -4.1287069 -4.4553108 -5.4145703 -5.4783354 -5.723577 -6.7078381][-5.16493 -5.0585222 -4.939888 -4.5936785 -4.0775003 -3.5598276 -2.8917809 -2.3712258 -1.7955852 -2.2742314 -2.7386146 -3.4737778 -4.0093102 -4.3813362 -5.2483625][-5.3683071 -5.0672393 -4.4996166 -3.6663229 -2.6545315 -1.7815242 -0.74533749 -0.081003189 0.52878761 -0.049618721 -0.51069593 -1.6874332 -2.2449403 -3.056705 -4.311286][-5.9177132 -5.1472645 -4.0657043 -2.5383792 -0.93613815 0.28513622 1.3292751 1.7906504 2.0132551 1.1609039 0.60271358 -0.21761179 -0.67154455 -1.3641615 -3.0954175][-5.2582903 -4.5577383 -3.3989801 -1.7950993 0.089933872 1.9110851 3.4643106 3.9672842 3.8033762 2.3548346 1.2584362 -0.083293915 -0.71599531 -1.3993649 -2.676681][-4.2919197 -3.07473 -1.7592845 0.20453262 2.3991842 4.1950388 5.2541962 5.3110571 4.8493757 3.3886585 1.8726063 0.042109966 -1.1592293 -2.0307269 -3.5321345][-2.8684554 -2.0097461 -0.68241358 1.2885923 3.2042265 4.5371962 5.3095675 5.0380917 4.3606071 3.0800772 1.8204279 -0.051589012 -1.5281458 -2.5406003 -3.7847848][-3.5526276 -2.4402494 -1.4384527 -0.0435977 1.7186651 2.9992495 3.7083054 3.503231 3.1102819 1.997016 0.812809 -0.54541111 -1.9112611 -2.7148757 -3.7975402][-4.26314 -3.7369192 -3.2686586 -1.7262211 -0.46820498 0.60153008 1.5401077 1.7817764 1.820405 0.76043034 -0.44809437 -1.7869205 -3.0403795 -3.4467587 -3.9813659][-4.8626585 -4.2022371 -3.5122838 -2.4467769 -1.413568 -0.65187883 -0.19822073 0.22892475 0.41009855 0.20985603 -0.61708832 -1.9743848 -3.1108632 -3.7235997 -4.2380085][-5.3205681 -4.7327514 -4.1050282 -3.2113514 -2.2629156 -1.8442998 -1.6310697 -1.7650747 -1.6131549 -1.4517164 -1.5975876 -2.0137153 -2.4616165 -2.9341607 -3.4361892][-5.7071047 -5.2352686 -4.8670216 -3.9193742 -3.2700677 -3.0542455 -2.9890766 -3.0138712 -2.8063745 -2.6928568 -2.6141038 -2.6784964 -2.9003382 -3.1175814 -3.1817746][-6.4029331 -5.7667665 -5.0643358 -4.5132074 -4.2216945 -4.040554 -3.8229179 -3.7675488 -3.5744703 -3.1077585 -2.9623666 -2.7765608 -2.8435245 -2.9904108 -3.2123075]]...]
INFO - root - 2017-12-15 11:35:38.707729: step 7410, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 60h:00m:04s remains)
INFO - root - 2017-12-15 11:35:45.080932: step 7420, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 57h:36m:38s remains)
INFO - root - 2017-12-15 11:35:51.447020: step 7430, loss = 0.36, batch loss = 0.24 (12.4 examples/sec; 0.647 sec/batch; 58h:23m:15s remains)
INFO - root - 2017-12-15 11:35:57.865144: step 7440, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 56h:22m:06s remains)
INFO - root - 2017-12-15 11:36:04.351109: step 7450, loss = 0.32, batch loss = 0.21 (12.1 examples/sec; 0.660 sec/batch; 59h:36m:16s remains)
INFO - root - 2017-12-15 11:36:10.733535: step 7460, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 57h:19m:29s remains)
INFO - root - 2017-12-15 11:36:17.144596: step 7470, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 57h:00m:15s remains)
INFO - root - 2017-12-15 11:36:23.501674: step 7480, loss = 0.31, batch loss = 0.19 (11.9 examples/sec; 0.674 sec/batch; 60h:49m:49s remains)
INFO - root - 2017-12-15 11:36:29.956952: step 7490, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 58h:48m:32s remains)
INFO - root - 2017-12-15 11:36:36.366369: step 7500, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 56h:09m:06s remains)
2017-12-15 11:36:36.871062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7401576 -3.6269588 -4.4915214 -5.2423997 -5.7814174 -5.6919971 -5.1709414 -4.47398 -3.8231313 -3.9029562 -4.5115347 -6.2250991 -6.985168 -7.4626408 -7.5832629][-3.9869554 -5.3126564 -6.3642273 -7.3278985 -7.9784808 -8.0960178 -7.77496 -6.9044619 -5.79198 -5.6727781 -6.2650785 -7.09222 -7.2985253 -7.5881295 -7.4629397][-3.3180637 -4.4073768 -5.5141964 -6.7476392 -7.6256142 -7.863903 -7.3298292 -6.5506287 -5.8077726 -5.5901318 -5.8166962 -7.3026695 -7.5280042 -7.3663583 -7.4121885][-3.2015233 -3.6529524 -4.2596569 -5.0551844 -5.8074903 -5.67169 -4.9176378 -4.0529656 -3.1909781 -3.3961382 -4.3589373 -5.9994221 -6.7988591 -7.2710466 -6.9175234][-2.6388769 -2.5353141 -2.5210094 -2.7048082 -3.1348724 -2.8636341 -2.1636925 -1.1534095 -0.52780819 -1.1011324 -2.2593217 -4.2416706 -5.4811258 -6.1608963 -6.701004][-2.6111684 -2.2917709 -2.3369651 -1.9848566 -1.5228114 -0.42558432 0.88610506 1.7008309 2.3597875 1.4543557 -0.11721325 -2.5541482 -3.9919972 -4.9910049 -5.3923817][-2.2943621 -1.859715 -1.472928 -0.4210043 0.55222464 1.972528 3.2770534 4.0470424 4.3482251 2.8821273 0.72909021 -2.249527 -4.1260967 -5.2067232 -5.5135775][-2.9842262 -2.53861 -2.0246992 -1.1933999 0.18719149 2.0215278 3.5422807 4.2262111 4.4526572 2.9378715 0.76818228 -2.0025377 -3.8528013 -4.8325095 -5.3047533][-3.3632817 -2.7150273 -2.5471063 -1.8088179 -1.0605502 0.25235128 1.7744164 2.4694638 2.8948712 1.430222 -0.55348539 -3.1862788 -4.6297832 -5.1848149 -5.3456039][-4.2502079 -3.3582768 -2.2789941 -1.9380064 -1.5002408 -0.58691168 0.50409651 0.96045542 1.0730472 -0.31172895 -2.0157342 -4.1800957 -5.0982056 -5.4294381 -5.4474983][-5.710146 -5.3698311 -4.6694236 -4.0678792 -3.1215973 -2.3281174 -1.4085803 -0.84409809 -0.7978797 -2.0761223 -3.2452888 -4.7051735 -5.4530106 -5.3838186 -5.5643926][-5.4612274 -5.2008424 -5.3362355 -5.3502655 -4.8621378 -4.190732 -3.1832738 -2.712739 -2.2475171 -2.8708787 -3.423737 -4.1192231 -4.6572704 -5.12946 -5.3049927][-6.4671192 -6.175642 -6.1817861 -5.5832634 -4.8872752 -4.2316275 -3.4591212 -2.903975 -2.640276 -3.1605325 -3.9606736 -4.6091623 -5.107338 -4.8163815 -4.8078032][-5.5244427 -5.4574456 -5.3207865 -5.1228852 -4.87016 -4.0037928 -2.9757771 -2.4867787 -2.3175602 -2.6695228 -2.8848143 -3.57314 -4.1348777 -4.4611306 -4.7450156][-5.059761 -5.3021903 -5.902318 -5.8462644 -5.5297556 -5.5495534 -4.8470988 -4.0486631 -3.3781343 -3.3197694 -3.7811091 -4.350853 -4.8895597 -5.3501191 -5.3117638]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 11:36:44.124085: step 7510, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 58h:41m:36s remains)
INFO - root - 2017-12-15 11:36:50.477363: step 7520, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 56h:39m:22s remains)
INFO - root - 2017-12-15 11:36:56.890652: step 7530, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 58h:03m:51s remains)
INFO - root - 2017-12-15 11:37:03.339283: step 7540, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 60h:13m:33s remains)
INFO - root - 2017-12-15 11:37:09.690000: step 7550, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:48m:19s remains)
INFO - root - 2017-12-15 11:37:16.121039: step 7560, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 58h:48m:00s remains)
INFO - root - 2017-12-15 11:37:22.486243: step 7570, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 57h:03m:03s remains)
INFO - root - 2017-12-15 11:37:28.921269: step 7580, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 57h:36m:31s remains)
INFO - root - 2017-12-15 11:37:35.270786: step 7590, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:51m:51s remains)
INFO - root - 2017-12-15 11:37:41.699692: step 7600, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 56h:35m:26s remains)
2017-12-15 11:37:42.209264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3765583 -4.2623444 -4.4077244 -4.6732197 -5.0534849 -5.0320492 -4.9515629 -4.5472264 -3.8280184 -3.6004202 -4.2867217 -4.3867044 -5.0964031 -5.2628822 -5.751461][-4.323091 -4.3337131 -4.456605 -4.7768631 -5.0925732 -5.36808 -5.1832848 -4.843709 -4.3454652 -4.2233047 -4.82789 -4.9641066 -5.8581104 -5.611536 -5.8217711][-4.777102 -4.7170763 -4.6711831 -4.9863119 -5.1499553 -5.2926116 -5.1370621 -4.8795934 -4.7326441 -4.633009 -5.4487858 -5.5181 -6.1721129 -6.0383844 -6.0723276][-4.785429 -4.7265873 -4.594605 -4.7232418 -4.3906078 -4.1461687 -3.8565154 -3.6967025 -3.5208035 -3.9042561 -4.928246 -5.2736559 -6.2238793 -6.3072529 -6.6060858][-4.9232459 -4.4712615 -3.9324403 -3.8185546 -3.2177944 -3.0156403 -2.6250062 -2.4084463 -2.1875792 -2.5112553 -3.645736 -4.432723 -5.8538351 -6.1236768 -6.5742822][-5.0282655 -4.5276814 -3.8387651 -3.1046929 -1.9062252 -1.5239487 -1.008194 -0.95981789 -1.0754218 -1.2726822 -2.3108907 -3.3596597 -4.8029261 -5.4737458 -6.2141714][-4.7661414 -4.2694025 -3.7628279 -2.6660857 -1.3643613 -0.5854435 0.36411524 0.52372408 0.49527311 0.033840179 -1.5982018 -2.6561942 -3.9342492 -4.6179962 -5.4262981][-4.3335571 -3.7383871 -3.4073572 -2.2577281 -1.2759824 -0.36123896 0.625505 0.789907 1.1307464 0.54809809 -1.2689614 -2.1560936 -3.4754038 -3.940053 -4.6216712][-4.4184546 -3.8549943 -3.2665114 -2.4634733 -1.5773797 -0.69531918 -0.10373878 0.17329264 0.7284503 0.015409946 -1.6210227 -2.3937879 -3.8058476 -4.1456404 -4.5686545][-5.1542368 -4.9155645 -4.2567558 -3.702903 -2.7360339 -1.6968102 -1.3572955 -1.1466031 -0.87755203 -1.285943 -2.3406615 -2.8145962 -3.7781985 -4.360507 -5.1132212][-6.0425205 -5.7943587 -5.619668 -5.4621549 -4.6379695 -3.8295264 -3.1869202 -3.0130014 -3.0635009 -3.187243 -3.7374885 -4.295392 -4.6840734 -5.2421074 -5.8456664][-7.1375017 -6.7927518 -6.5253229 -6.4023972 -5.8446751 -5.4368696 -4.9437418 -4.692843 -4.4801364 -4.5874519 -5.0228529 -5.1200933 -5.0426316 -5.4695549 -5.782548][-7.8621082 -7.754355 -7.6780572 -7.3651891 -6.7816558 -6.3662577 -5.9687958 -5.8722143 -5.5700665 -5.2701378 -5.5581961 -5.6032109 -5.269207 -5.4302969 -5.6308742][-7.0408092 -7.1950712 -7.3238058 -7.1936626 -6.926271 -6.4825168 -6.0615983 -5.8869705 -5.7689109 -5.6895571 -5.6609745 -5.5964351 -5.3796039 -5.2881031 -5.2217045][-6.8819532 -6.7602277 -6.6875582 -6.7483964 -6.723968 -6.5157118 -6.4381905 -6.3855648 -6.2856565 -6.2029905 -6.1354337 -6.07458 -6.0408549 -6.0267024 -5.8869529]]...]
INFO - root - 2017-12-15 11:37:48.592612: step 7610, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 59h:26m:52s remains)
INFO - root - 2017-12-15 11:37:55.096791: step 7620, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 58h:32m:25s remains)
INFO - root - 2017-12-15 11:38:01.463345: step 7630, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 58h:04m:54s remains)
INFO - root - 2017-12-15 11:38:07.838502: step 7640, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 57h:45m:54s remains)
INFO - root - 2017-12-15 11:38:14.250921: step 7650, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 58h:37m:41s remains)
INFO - root - 2017-12-15 11:38:20.838095: step 7660, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 60h:06m:33s remains)
INFO - root - 2017-12-15 11:38:27.343952: step 7670, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 58h:45m:17s remains)
INFO - root - 2017-12-15 11:38:33.716232: step 7680, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 58h:13m:13s remains)
INFO - root - 2017-12-15 11:38:40.062745: step 7690, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 58h:00m:00s remains)
INFO - root - 2017-12-15 11:38:46.518895: step 7700, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 57h:38m:30s remains)
2017-12-15 11:38:47.101118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2321358 -3.8058562 -4.4536109 -4.7179461 -5.2927618 -5.7112837 -5.648478 -4.8519306 -4.1452723 -3.7135139 -4.287262 -6.2188196 -7.5744758 -8.4981337 -8.81827][-3.9489408 -4.6440282 -5.3672361 -5.8133216 -6.4148593 -5.7225881 -5.1655149 -4.3359208 -3.6776116 -4.0620632 -5.5333447 -7.3832016 -9.0436029 -9.5951395 -9.4003334][-4.1699052 -4.6696219 -5.4984732 -5.6787281 -5.5461183 -5.0047197 -3.9580684 -2.9862862 -2.1876864 -2.2522001 -4.1398926 -6.8468566 -8.9291573 -9.6349716 -9.8936577][-5.3065629 -5.0953865 -5.1669536 -5.1547213 -4.6207333 -3.3930421 -1.8453994 -0.91663074 0.014354706 -0.32016468 -2.3371005 -5.5415859 -8.0691595 -9.1531029 -9.1584625][-5.6258817 -5.392601 -4.7847109 -4.3084621 -3.2856894 -1.0669899 0.94021082 2.2196555 2.1700635 0.95179987 -0.83257151 -4.4194822 -7.4653177 -8.6083851 -8.9320974][-5.2340374 -5.0801878 -4.3248119 -3.4860129 -2.0909848 0.34274149 2.6436906 4.0955033 4.54806 2.9020581 -0.16629076 -3.5162296 -6.1022277 -7.4873924 -7.8259735][-5.6418381 -5.2883062 -4.3462291 -2.5762391 -0.77825642 1.3803258 3.5849471 5.1301885 5.9023862 4.6234937 1.4173141 -2.8620272 -6.3771462 -7.88388 -7.9145622][-5.1463728 -4.6963053 -3.8625836 -2.3203974 -0.26208258 2.1401772 4.4767756 5.3115506 5.5102324 4.4083905 1.6078897 -2.6670375 -6.0472794 -7.8412004 -7.9594612][-4.8977356 -4.6506419 -3.9753182 -2.4562302 -0.54298592 1.7328534 3.5920405 4.6692796 5.0044847 3.2362466 0.46285486 -3.2722764 -6.1682158 -7.7933111 -8.1054754][-5.4841871 -5.4545822 -5.0570593 -4.3201609 -2.9926066 -0.92242956 0.86903048 1.8624606 2.200398 0.85743189 -1.4900451 -4.7580028 -7.2726526 -8.4051895 -8.7941122][-5.7599134 -6.0300131 -6.413753 -5.8960538 -4.8078403 -3.305573 -1.8197808 -1.1195498 -0.69688368 -1.6750326 -3.735625 -6.080523 -7.9458842 -8.9512444 -8.9852371][-6.6065331 -6.7217207 -6.736145 -6.6287732 -6.384799 -5.5445805 -4.5299597 -4.0562973 -3.8812597 -4.0836678 -4.9664 -6.3274288 -7.4217615 -8.1687784 -8.4104042][-7.7629123 -7.7087674 -7.8974996 -7.7654123 -7.7016349 -7.5154843 -7.0420561 -6.8528047 -6.3871112 -5.968205 -6.6083274 -7.3341765 -7.9596615 -8.4159832 -8.2296743][-7.5057755 -7.7125144 -7.4786205 -7.1538835 -6.9169245 -6.5177112 -6.1264434 -6.5082664 -6.6246767 -6.2375011 -6.1924996 -6.3312163 -6.8544984 -7.2265253 -7.1449022][-7.3069382 -7.9838643 -8.5554276 -8.33187 -7.6574383 -7.254107 -6.7294035 -6.6741171 -7.1834474 -7.5937009 -7.6519551 -7.3905873 -6.9972677 -6.8136067 -6.5318875]]...]
INFO - root - 2017-12-15 11:38:53.634155: step 7710, loss = 0.34, batch loss = 0.22 (12.2 examples/sec; 0.658 sec/batch; 59h:20m:43s remains)
INFO - root - 2017-12-15 11:38:59.995045: step 7720, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 56h:55m:25s remains)
INFO - root - 2017-12-15 11:39:06.550016: step 7730, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 58h:02m:16s remains)
INFO - root - 2017-12-15 11:39:12.860410: step 7740, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 56h:58m:51s remains)
INFO - root - 2017-12-15 11:39:19.159508: step 7750, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 56h:24m:14s remains)
INFO - root - 2017-12-15 11:39:25.534344: step 7760, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 59h:20m:57s remains)
INFO - root - 2017-12-15 11:39:32.097934: step 7770, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 59h:02m:11s remains)
INFO - root - 2017-12-15 11:39:38.404443: step 7780, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 56h:57m:57s remains)
INFO - root - 2017-12-15 11:39:44.806197: step 7790, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 57h:26m:12s remains)
INFO - root - 2017-12-15 11:39:51.149855: step 7800, loss = 0.37, batch loss = 0.25 (12.7 examples/sec; 0.630 sec/batch; 56h:48m:00s remains)
2017-12-15 11:39:51.623259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2526312 -3.9287059 -4.7117548 -5.5537424 -6.4497337 -7.4359789 -8.3729286 -9.4923334 -10.281894 -11.176056 -12.116645 -11.765799 -11.058921 -9.46907 -8.7538576][-2.8110313 -3.6551833 -4.3934631 -5.3248563 -6.1424451 -7.074223 -8.1780529 -8.6360779 -8.7877865 -9.0858231 -9.5282078 -10.116206 -10.73041 -10.390415 -10.0673][-3.4708405 -4.0321536 -4.9359121 -5.1043539 -5.3962479 -5.6574936 -6.0570145 -6.6127625 -6.94701 -7.1230755 -7.6248531 -8.1092262 -8.87912 -9.4913225 -10.49744][-5.019166 -5.1014185 -5.2442431 -5.0990391 -4.526248 -3.54558 -2.9953527 -3.1117277 -3.4661059 -4.5222869 -5.7330427 -7.1849804 -7.7372274 -8.217741 -8.8320818][-7.2709274 -6.5664024 -6.0178862 -5.660769 -5.044693 -2.8302832 -0.84656143 -0.21651077 -0.45295477 -1.6648912 -3.6641903 -6.4051943 -7.7907705 -8.6950111 -9.4798613][-7.9643836 -7.3539896 -6.3519139 -4.8983536 -3.2225695 -1.0728064 0.86409473 2.1387634 2.461936 0.33906078 -2.3863811 -6.2241635 -8.4161606 -9.7691994 -10.104508][-7.998189 -7.0748019 -5.4376497 -2.5908747 0.46323967 2.9066706 4.9805117 5.316165 5.1876516 2.9460373 -0.46707249 -5.0019484 -7.6467161 -9.627306 -10.476618][-7.2687755 -7.2571206 -5.9036717 -3.0703182 0.41743326 3.1854744 5.3112564 6.2047825 6.4061251 3.95442 0.93207169 -2.8893237 -5.962534 -8.37116 -9.1722775][-8.0219851 -7.5260091 -6.640656 -4.0924616 -1.0544238 0.84111881 2.8249702 3.1443129 3.492074 2.336833 -0.51061583 -3.4583631 -5.2749491 -6.9035931 -8.098177][-8.6413231 -8.8492136 -8.0237856 -6.3158779 -3.8484805 -1.1146569 0.84400845 1.0638523 1.345088 -0.97874117 -3.8801794 -5.82669 -7.3450069 -8.001646 -8.2250147][-10.240166 -10.610517 -10.567975 -9.413 -7.8570356 -5.7553387 -3.6430109 -2.074224 -0.87114811 -1.8577251 -4.4389429 -6.6166744 -7.620625 -8.364768 -9.0197935][-9.7589645 -10.6661 -10.644748 -10.00841 -9.200141 -8.1373444 -7.0854006 -6.2367139 -4.7535214 -4.3657284 -5.0822506 -5.2880373 -6.3653183 -7.3168077 -7.8305788][-10.455474 -10.631788 -10.504422 -10.217252 -9.5914726 -8.9047823 -8.2800217 -7.9209971 -7.9080935 -8.1436815 -8.4372 -7.9116693 -7.5154872 -7.1174021 -6.7628107][-8.8929253 -9.1734838 -9.2814894 -8.9040613 -8.8041677 -8.2086554 -7.4291792 -7.2453871 -7.2973986 -7.5581722 -8.2361088 -8.1906509 -8.5417013 -8.5252171 -8.0726461][-9.5072556 -8.7906532 -8.388896 -8.2595091 -8.0875244 -8.0049276 -7.6520438 -7.5380282 -7.6461692 -7.7469006 -7.3789225 -7.2878122 -7.4398727 -7.44248 -7.1810827]]...]
INFO - root - 2017-12-15 11:39:58.068817: step 7810, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.664 sec/batch; 59h:53m:14s remains)
INFO - root - 2017-12-15 11:40:04.544682: step 7820, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 55h:50m:09s remains)
INFO - root - 2017-12-15 11:40:10.953504: step 7830, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 58h:14m:04s remains)
INFO - root - 2017-12-15 11:40:17.340588: step 7840, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 56h:09m:14s remains)
INFO - root - 2017-12-15 11:40:23.714657: step 7850, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 57h:40m:05s remains)
INFO - root - 2017-12-15 11:40:30.096308: step 7860, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 57h:17m:09s remains)
INFO - root - 2017-12-15 11:40:36.476876: step 7870, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 56h:22m:42s remains)
INFO - root - 2017-12-15 11:40:42.841894: step 7880, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 56h:16m:36s remains)
INFO - root - 2017-12-15 11:40:49.239320: step 7890, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 57h:39m:10s remains)
INFO - root - 2017-12-15 11:40:55.694841: step 7900, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 57h:50m:21s remains)
2017-12-15 11:40:56.178697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4901142 -3.2456703 -3.9097135 -3.9369848 -3.4074278 -2.9345384 -2.909153 -2.4731598 -2.137404 -2.7047615 -3.3331561 -4.7825937 -5.5499086 -5.4318409 -6.3526306][-3.0530148 -3.8289487 -4.5836825 -4.8138065 -4.5639548 -3.683048 -3.0247431 -2.9931717 -3.16854 -3.6830728 -4.2343311 -5.429317 -6.0998936 -6.1012335 -6.8013172][-3.6692812 -3.8527153 -4.2983828 -4.4332409 -4.3506603 -3.9938147 -3.5868945 -3.1842165 -2.7526851 -3.5971406 -4.5266156 -5.9226079 -6.5541239 -6.701643 -7.5221462][-4.1626539 -4.3167076 -4.4512329 -4.0172825 -3.4683738 -3.2232637 -2.3710032 -1.9627266 -1.8367167 -2.3591619 -3.2950644 -5.0890188 -6.1950779 -6.5846982 -7.1433172][-4.9379096 -4.7708349 -4.7541256 -4.2369652 -3.2196398 -2.303997 -1.1093593 -0.68536997 -0.46953678 -1.425941 -2.8027792 -4.640933 -5.7328477 -6.2761331 -7.2084265][-6.5278521 -6.1304226 -5.5172834 -4.3991575 -3.000329 -1.7331052 0.081321716 0.96051788 1.0250921 -0.77802134 -2.8013544 -5.1245155 -6.2495728 -6.4270658 -7.0246615][-6.4322443 -5.9938755 -5.6257443 -4.0060654 -1.7078495 -0.0542078 2.1782618 2.8884621 3.0813141 1.4141588 -0.93697262 -3.7449265 -5.463172 -5.7562904 -6.4568024][-5.7656841 -5.5945535 -5.0329094 -3.556119 -1.2918825 1.3281069 3.6091824 3.9249163 3.6665802 1.9329596 -0.021098137 -2.6474981 -4.2925348 -4.5505629 -5.1632643][-5.91603 -5.3315969 -5.1501055 -4.0677977 -2.1485281 0.27947092 2.7066727 3.2627525 3.2663488 1.2327404 -0.83264112 -3.2780981 -4.6502943 -4.7161307 -5.543664][-6.2710667 -5.8724194 -5.5872803 -4.51869 -3.1767378 -1.5693521 0.60912704 1.0215492 1.1637011 -0.78185368 -2.8058891 -4.882412 -5.7757397 -5.4979057 -6.2161779][-7.2947588 -6.8969073 -6.4020424 -5.4105568 -4.487937 -3.4760942 -2.3163652 -1.6523485 -1.3806162 -2.8449278 -3.8914821 -5.7153111 -6.5359359 -5.927341 -6.3376765][-7.4568753 -7.0248184 -6.653554 -5.807662 -5.1984615 -4.59379 -3.8982382 -3.9523032 -3.8511167 -4.0502534 -4.2954359 -5.4035339 -5.9425116 -5.8601418 -6.0724192][-8.4888592 -7.7206554 -7.0131507 -6.197351 -5.7341151 -5.5897331 -5.3027782 -5.2554474 -4.9508295 -5.6436095 -5.947938 -6.0764918 -6.0773716 -6.1061668 -6.3564324][-8.5303421 -7.8474789 -7.2348156 -6.4134903 -5.9089651 -5.98681 -5.5587406 -5.7693634 -5.7396488 -5.6192279 -5.845438 -6.0287695 -6.03119 -6.2074928 -6.273036][-8.7496862 -8.5448828 -8.048275 -7.2000713 -6.3847022 -6.4703875 -6.6901956 -6.5383315 -6.5012794 -6.3053689 -6.3037252 -6.15007 -6.007874 -5.8382072 -5.53314]]...]
INFO - root - 2017-12-15 11:41:02.888642: step 7910, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:43m:22s remains)
INFO - root - 2017-12-15 11:41:09.246298: step 7920, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 59h:05m:22s remains)
INFO - root - 2017-12-15 11:41:15.635514: step 7930, loss = 0.35, batch loss = 0.23 (12.8 examples/sec; 0.626 sec/batch; 56h:28m:11s remains)
INFO - root - 2017-12-15 11:41:22.036964: step 7940, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.653 sec/batch; 58h:50m:33s remains)
INFO - root - 2017-12-15 11:41:28.499708: step 7950, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 57h:29m:07s remains)
INFO - root - 2017-12-15 11:41:34.975093: step 7960, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 57h:57m:26s remains)
INFO - root - 2017-12-15 11:41:41.324963: step 7970, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 57h:54m:46s remains)
INFO - root - 2017-12-15 11:41:47.727442: step 7980, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 59h:15m:08s remains)
INFO - root - 2017-12-15 11:41:54.046246: step 7990, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:40m:36s remains)
INFO - root - 2017-12-15 11:42:00.376802: step 8000, loss = 0.29, batch loss = 0.17 (13.2 examples/sec; 0.608 sec/batch; 54h:49m:55s remains)
2017-12-15 11:42:00.865855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5517626 -3.944077 -4.0276728 -3.4295158 -2.7234898 -2.2265005 -1.7650671 -1.8713274 -2.1278734 -3.3085217 -3.9587595 -4.8482447 -5.4726372 -5.6483784 -5.8657007][-3.4152117 -4.0572014 -4.3375878 -3.7241936 -2.738625 -1.8588824 -1.409266 -1.4604306 -1.6715255 -3.69303 -4.6026754 -5.9627972 -6.681129 -6.7698674 -6.7740326][-4.1201277 -4.1909704 -4.4815569 -4.15481 -3.1172714 -2.544435 -2.0693755 -1.8076878 -2.1493182 -3.7722161 -4.9746418 -6.5664811 -7.4515367 -8.0184612 -8.3020182][-4.6432943 -4.6823368 -4.2789927 -3.6264133 -2.8156948 -2.207036 -1.7153864 -2.167438 -2.5066733 -3.9558091 -5.0860529 -6.5138726 -7.524056 -7.635406 -7.9588323][-4.7552328 -4.4687738 -4.0711489 -3.20612 -2.1243901 -1.3507662 -0.71814632 -1.2906127 -2.0581546 -3.7210267 -4.3627806 -5.216434 -5.7833748 -6.103529 -6.6343246][-5.1462727 -4.5784426 -4.2398825 -3.1040816 -1.8691759 -0.86892319 -0.3371563 -0.4811554 -1.1960635 -2.7853823 -3.3376598 -4.1431518 -4.2174234 -4.3079166 -5.1595864][-4.98685 -3.9990106 -3.547183 -2.1930447 -0.51397562 0.53494167 1.1334486 0.75039577 0.050045013 -1.6524897 -2.8040142 -4.0744634 -4.2716465 -3.7724431 -4.2033072][-4.489738 -3.3421364 -2.6486459 -1.0107493 0.81074429 2.4188242 3.0563669 2.5247679 1.9549475 -0.14300489 -1.5661674 -3.2207065 -4.1564703 -3.852967 -4.1269007][-4.3672104 -3.4243326 -2.7470231 -1.4741893 -0.0730958 1.5348606 2.5330086 2.8213787 2.7023125 0.68642521 -0.87590265 -2.5909624 -3.8406167 -4.2156458 -4.9665117][-4.8747797 -4.1794291 -3.7063529 -2.4504433 -1.1332736 0.15731764 1.2957306 1.9961872 2.317193 0.48020363 -0.7938633 -2.6632152 -3.7181156 -4.1310244 -5.1303024][-6.0494809 -5.3056092 -4.8886275 -3.6238747 -2.732049 -1.6957793 -0.72324753 -0.49184227 -0.19550896 -1.3129015 -2.3741579 -3.7021248 -4.6050739 -4.9903822 -5.6768126][-6.8956604 -6.0667591 -5.6615219 -4.96941 -4.3525562 -3.5373511 -2.74022 -2.3318009 -1.947978 -2.897634 -3.0803461 -3.9886031 -4.8981285 -5.332438 -5.9669495][-7.8354721 -7.1342983 -6.6453595 -5.9949861 -5.5561523 -5.3627396 -4.9168677 -4.1734858 -3.8162315 -4.6899309 -4.6082206 -4.6535788 -4.7789593 -4.8983555 -5.3072648][-8.21845 -7.6410666 -7.2584348 -6.5897341 -6.1782446 -6.0950336 -6.0962839 -6.1778016 -5.9800358 -6.1374669 -6.0242109 -5.8532152 -5.5771408 -5.4114857 -5.2911277][-8.3277607 -7.9563856 -7.6151657 -7.3405046 -7.08154 -6.9533153 -6.8773117 -7.1539278 -7.2009168 -7.2812767 -7.0472283 -6.7896557 -6.5658731 -6.1271858 -5.8168135]]...]
INFO - root - 2017-12-15 11:42:07.329502: step 8010, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 57h:32m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 11:42:13.751132: step 8020, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 56h:03m:29s remains)
INFO - root - 2017-12-15 11:42:20.222509: step 8030, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 57h:56m:31s remains)
INFO - root - 2017-12-15 11:42:26.612290: step 8040, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:39m:14s remains)
INFO - root - 2017-12-15 11:42:32.925268: step 8050, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 56h:34m:19s remains)
INFO - root - 2017-12-15 11:42:39.260037: step 8060, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 57h:21m:26s remains)
INFO - root - 2017-12-15 11:42:45.671118: step 8070, loss = 0.31, batch loss = 0.20 (13.0 examples/sec; 0.617 sec/batch; 55h:36m:14s remains)
INFO - root - 2017-12-15 11:42:52.124085: step 8080, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 57h:42m:20s remains)
INFO - root - 2017-12-15 11:42:58.519480: step 8090, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 56h:48m:59s remains)
INFO - root - 2017-12-15 11:43:04.931785: step 8100, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 57h:14m:55s remains)
2017-12-15 11:43:05.408320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3011522 -4.077827 -3.4645333 -3.8630226 -4.4430132 -4.8570523 -5.0890722 -5.1754479 -4.9449921 -6.2281485 -6.6545286 -6.9459338 -7.4102912 -7.9867415 -7.588244][-3.391078 -3.6599498 -3.9739661 -4.4129868 -4.6946549 -4.7858706 -5.0136495 -5.0124559 -4.9273348 -6.1759315 -6.4624128 -6.96395 -7.9234357 -8.3510723 -7.8290105][-3.5090504 -3.8939683 -3.763607 -3.6342573 -3.595027 -3.3935723 -3.273459 -3.17587 -3.0873389 -4.6286488 -5.253108 -5.674263 -6.6414423 -7.3294458 -7.2538552][-2.7601385 -2.8810306 -3.114037 -2.9955525 -2.4716244 -1.9108977 -1.52567 -1.3276186 -1.2275424 -2.9739375 -3.7717209 -4.766118 -6.2178206 -6.936183 -6.5453382][-3.5422711 -2.9120588 -1.7518134 -1.0436635 -0.50234652 0.011797428 0.33267689 0.54892778 0.75216627 -1.2284679 -2.3552637 -3.39882 -4.9151754 -6.1994638 -6.0457759][-2.5903358 -2.0563054 -1.4046316 -0.17669344 1.4396768 2.3049502 2.6316638 2.6101985 2.170609 0.078442574 -1.0632372 -2.2689571 -4.0665369 -5.1232252 -4.9214149][-2.5702543 -2.0090575 -0.84081888 0.72546816 2.3048568 3.0593104 3.4609199 3.3677278 3.0897298 0.51976442 -1.0290389 -2.46551 -4.2075834 -5.2271504 -5.5917454][-2.4536533 -1.7325525 -0.82666683 1.0928922 2.6303878 3.4711185 3.5604358 3.5152307 3.1618524 0.77649355 -0.65194607 -2.1175718 -4.0825834 -5.4852114 -5.6755023][-1.7538037 -1.545198 -0.45701456 1.3646474 2.4053531 2.6825442 2.5820384 2.5845046 2.3411613 0.16738081 -1.3758512 -2.6212072 -4.2336378 -5.6588106 -6.0969467][-1.4633999 -1.551733 -0.81864882 0.43415308 1.232203 1.4611163 1.3987594 1.0759091 0.70035887 -1.2161632 -2.0916824 -3.2901502 -4.7727866 -5.9541435 -6.3270864][-3.7215226 -3.2486739 -2.8189139 -1.7363524 -0.85545588 -0.29587984 -0.17784548 -0.45012426 -0.45482254 -2.1173105 -3.2296672 -4.0490041 -5.2016325 -6.1597948 -6.6277657][-4.847887 -4.4790936 -3.899945 -3.1686316 -2.4788804 -2.0279942 -1.8035169 -1.7525339 -1.7143087 -2.717329 -3.5493741 -4.1310949 -5.5679812 -6.4723454 -6.6264458][-5.2506852 -5.2468286 -5.2579541 -5.0249395 -4.7265053 -4.0551505 -3.5228829 -3.5410838 -3.3753729 -3.8651407 -4.805882 -5.0580053 -5.8284941 -6.2915621 -6.3089943][-5.4802618 -5.5816708 -5.6033168 -5.5386696 -5.1164603 -4.5333824 -4.3876147 -4.1793208 -3.6657286 -3.9440162 -4.3824849 -4.7776322 -5.3884182 -6.1770597 -6.410675][-6.3717861 -6.0871086 -6.0428333 -6.0237174 -5.9562092 -5.6210485 -5.2591019 -5.0800066 -4.9959793 -4.7467823 -5.1626563 -5.2996693 -5.4363747 -6.1205106 -6.7097006]]...]
INFO - root - 2017-12-15 11:43:11.798891: step 8110, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 58h:08m:21s remains)
INFO - root - 2017-12-15 11:43:18.318316: step 8120, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 57h:06m:04s remains)
INFO - root - 2017-12-15 11:43:24.660625: step 8130, loss = 0.32, batch loss = 0.20 (12.9 examples/sec; 0.620 sec/batch; 55h:53m:05s remains)
INFO - root - 2017-12-15 11:43:31.049050: step 8140, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 58h:50m:53s remains)
INFO - root - 2017-12-15 11:43:37.438988: step 8150, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 57h:12m:52s remains)
INFO - root - 2017-12-15 11:43:43.806189: step 8160, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:37m:58s remains)
INFO - root - 2017-12-15 11:43:50.193494: step 8170, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 56h:56m:06s remains)
INFO - root - 2017-12-15 11:43:56.663138: step 8180, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 56h:22m:17s remains)
INFO - root - 2017-12-15 11:44:02.978091: step 8190, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 56h:19m:14s remains)
INFO - root - 2017-12-15 11:44:09.447888: step 8200, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 58h:03m:22s remains)
2017-12-15 11:44:09.910850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5061665 -3.7111049 -4.8915176 -5.6253567 -6.2177038 -6.7194719 -6.7100592 -5.8566489 -4.7716866 -4.2044725 -5.0290213 -5.9425821 -6.26085 -6.1952825 -5.690196][-3.0388088 -3.5065408 -4.5522065 -5.5854177 -6.30105 -5.9919996 -5.3458567 -4.9961309 -4.4943452 -4.4378157 -5.3561954 -6.6361394 -6.8621268 -6.9931321 -6.3164797][-2.8120165 -3.4226604 -4.155447 -4.5317307 -4.6182604 -4.569891 -4.2454476 -3.8152328 -3.6375093 -4.1973829 -5.6449213 -6.9166226 -7.24382 -7.9504371 -7.485291][-3.2597556 -3.4089885 -3.696913 -3.8736112 -3.8051186 -3.4190211 -2.6112127 -2.4872794 -2.2442207 -2.471684 -3.9713604 -5.7812481 -6.8861656 -7.2933817 -7.3673997][-3.6181011 -4.2350717 -4.3316927 -3.5220022 -2.4670358 -1.1207333 -0.45137262 -0.49281836 -1.3481803 -2.3781376 -3.6896839 -5.1464119 -5.8287358 -6.3276029 -6.703866][-4.7786374 -4.8018379 -4.2933903 -3.1108022 -1.7305446 0.18621063 1.3794308 1.7465363 0.69522095 -1.1218624 -3.8600757 -5.6734633 -5.9602795 -6.5195732 -6.3628554][-4.8324852 -4.45468 -3.8447974 -2.4281478 0.015802383 2.1579914 3.4565039 3.7673893 3.1661081 1.1231098 -2.4848104 -5.34422 -5.9445229 -5.898994 -5.39163][-3.9276118 -3.4319448 -2.9209623 -1.5465322 1.389267 3.7837076 4.9217253 4.7366982 3.9499683 1.8429823 -1.625268 -4.3784504 -5.3233366 -5.8047419 -5.5021482][-4.7344432 -3.7961795 -2.7586713 -1.7074833 0.31295395 2.1437931 3.6578474 3.981637 3.1194687 0.89042664 -2.1568317 -4.7621021 -5.6777673 -5.9178152 -5.9126792][-5.8020544 -5.750464 -5.1314278 -3.7465947 -2.2856436 -0.89611912 0.66737461 1.2696152 1.3221159 -0.7813797 -4.2219553 -6.3145466 -7.1040683 -7.069356 -6.9643226][-7.5374832 -6.9831891 -6.4830408 -5.4509783 -4.4656658 -3.4621115 -2.3729773 -1.6861629 -1.3431954 -2.2833953 -4.2593188 -6.3710942 -6.9091673 -7.1043844 -6.8486562][-7.2805128 -7.2636967 -7.227056 -6.6548443 -6.3244123 -5.3579168 -4.4756842 -3.9930341 -3.6945741 -4.2923479 -5.1702437 -6.2381239 -6.5178785 -6.9399614 -6.8496227][-7.7549815 -7.3895135 -6.9964013 -6.5360026 -6.5907702 -6.5706267 -6.4860992 -6.1884885 -5.6630354 -6.2521653 -6.6077709 -7.3594437 -7.5717297 -7.6406827 -7.4130354][-8.4044371 -8.2458248 -7.4260488 -6.928288 -6.6868863 -6.6380997 -6.8380995 -7.5715346 -7.63793 -7.8745108 -7.4322519 -7.2743344 -7.3301516 -7.066853 -6.9869442][-9.2067986 -9.1394129 -8.673357 -7.9705682 -7.4629054 -7.2459106 -7.0903769 -7.3362546 -7.7905526 -7.8640451 -7.8993759 -7.4615731 -6.7159634 -6.3170924 -5.8683381]]...]
INFO - root - 2017-12-15 11:44:16.406786: step 8210, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 58h:08m:00s remains)
INFO - root - 2017-12-15 11:44:22.802654: step 8220, loss = 0.37, batch loss = 0.26 (11.9 examples/sec; 0.670 sec/batch; 60h:20m:34s remains)
INFO - root - 2017-12-15 11:44:29.168117: step 8230, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 57h:13m:37s remains)
INFO - root - 2017-12-15 11:44:35.568048: step 8240, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 56h:29m:55s remains)
INFO - root - 2017-12-15 11:44:41.903906: step 8250, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 57h:31m:36s remains)
INFO - root - 2017-12-15 11:44:48.266475: step 8260, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 57h:24m:30s remains)
INFO - root - 2017-12-15 11:44:54.689662: step 8270, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 56h:52m:37s remains)
INFO - root - 2017-12-15 11:45:01.107909: step 8280, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 56h:17m:14s remains)
INFO - root - 2017-12-15 11:45:07.554647: step 8290, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 59h:52m:58s remains)
INFO - root - 2017-12-15 11:45:13.976552: step 8300, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 56h:20m:06s remains)
2017-12-15 11:45:14.499927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4507189 -3.4322677 -3.5268712 -3.1150484 -2.704164 -2.2394991 -1.9667587 -1.8581977 -1.6471629 -3.5341525 -4.4701138 -5.2863774 -5.7672329 -6.0676775 -6.5549631][-3.0064626 -3.1807995 -3.4291148 -3.415122 -3.0298076 -2.4622588 -2.0268822 -1.6990261 -1.6774664 -3.6674731 -4.3855433 -5.1703577 -6.1727462 -6.5589752 -6.9982977][-2.2726851 -2.367805 -2.5452204 -2.4191561 -2.3288617 -2.1112752 -1.8103213 -1.7256098 -1.4395318 -3.4609118 -4.3987665 -5.1812325 -5.8606992 -6.2709055 -6.8996334][-1.3023558 -1.5117526 -1.5936804 -1.4025731 -1.0495887 -0.70864773 -0.43168545 -0.5549655 -0.58575916 -2.6968913 -3.9121003 -5.0072465 -5.9279251 -6.4029293 -6.8138251][-1.029357 -0.77315474 -0.45436335 -0.25468826 0.34382582 0.86639929 0.88286638 0.41800165 0.17700005 -2.298913 -3.5905685 -4.9018631 -6.0606847 -6.4321008 -6.789391][-1.9264598 -0.96148491 -0.13907003 0.57396078 1.3988175 2.1131673 2.4347911 2.0943475 1.4823985 -1.3625574 -2.8341751 -4.4164391 -5.8027506 -6.5001688 -6.9484215][-2.305068 -1.3458405 -0.16787624 1.1110291 2.3191791 3.0915875 3.4815116 3.0874562 2.5879464 -0.20902443 -2.1399264 -3.8541343 -5.3758554 -6.3329535 -6.9292545][-2.6767693 -1.4678588 -0.25260496 1.4230752 2.7023549 3.3992553 3.9079509 3.5588527 2.9931045 0.25879145 -1.5810585 -3.500226 -5.24041 -6.1330233 -6.7744284][-3.564177 -2.3972964 -1.0820136 0.50777864 1.6273074 2.2514834 2.6186261 2.581656 2.3128877 -0.14264917 -1.7933331 -3.4619188 -5.1872835 -6.292541 -6.8767109][-4.1646881 -3.1263452 -2.1347399 -0.78760433 0.14669895 0.55530787 1.0135341 1.158155 0.97325087 -1.3118587 -2.4736023 -3.7360747 -5.1306119 -6.0953989 -6.7980981][-5.1844263 -4.513196 -3.473702 -2.2549138 -1.679368 -1.249907 -1.0063505 -0.87854958 -0.73015547 -2.9396081 -4.1101756 -5.0527067 -6.115428 -6.691968 -6.9806027][-6.0452161 -5.3694677 -4.6868124 -3.903372 -3.1931019 -2.8924284 -2.6144762 -2.4512734 -2.459157 -4.0348549 -4.7929344 -5.6221 -6.4201279 -6.8437028 -6.8351383][-6.5969677 -6.1355157 -5.5429153 -5.0394011 -4.6738329 -4.2129536 -4.1415663 -3.9958415 -3.8521795 -5.1580167 -5.57759 -5.6716723 -6.2227125 -6.487402 -6.3109732][-6.9977078 -6.6028152 -6.3775892 -5.8883944 -5.4472866 -5.2706194 -5.1338844 -4.9187584 -4.6090722 -5.3396358 -5.58652 -5.8222528 -5.7582712 -5.8673172 -5.8717737][-7.3743224 -7.241972 -6.9342513 -6.7998991 -6.5978413 -6.5031981 -6.6913853 -6.72492 -6.3372326 -6.2020941 -6.0866051 -6.044776 -6.1360683 -5.9566336 -5.7299337]]...]
INFO - root - 2017-12-15 11:45:20.855865: step 8310, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 58h:13m:57s remains)
INFO - root - 2017-12-15 11:45:27.213023: step 8320, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 56h:53m:16s remains)
INFO - root - 2017-12-15 11:45:33.686055: step 8330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 57h:08m:23s remains)
INFO - root - 2017-12-15 11:45:40.025536: step 8340, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 56h:39m:29s remains)
INFO - root - 2017-12-15 11:45:46.395067: step 8350, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.618 sec/batch; 55h:38m:22s remains)
INFO - root - 2017-12-15 11:45:52.771050: step 8360, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 58h:58m:33s remains)
INFO - root - 2017-12-15 11:45:59.261197: step 8370, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 57h:04m:30s remains)
INFO - root - 2017-12-15 11:46:05.599987: step 8380, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 57h:52m:54s remains)
INFO - root - 2017-12-15 11:46:11.991107: step 8390, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 56h:27m:43s remains)
INFO - root - 2017-12-15 11:46:18.273820: step 8400, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 55h:27m:06s remains)
2017-12-15 11:46:18.769245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0382538 -5.0164995 -4.8274903 -4.9514117 -4.9294605 -5.2288337 -5.7908044 -6.2351174 -6.7650928 -7.65181 -8.163908 -8.3416147 -8.200017 -7.7014766 -7.8403416][-3.7393203 -3.4135871 -2.7051945 -2.6955414 -3.320827 -3.8727472 -3.6099627 -4.1993985 -5.3754492 -6.6042891 -6.8583531 -6.86796 -7.02253 -6.6083603 -6.5636959][-2.9069929 -2.6039858 -1.7785659 -0.98572969 -0.99893427 -1.5415969 -2.0692782 -2.6562681 -2.801856 -3.5950036 -4.6235571 -4.7430587 -5.0602865 -4.6893358 -5.209445][-4.3576269 -3.2206306 -1.6024594 -0.97437954 -0.96990347 -0.86517 -0.760561 -0.75705433 -1.2301059 -2.2975082 -2.6269975 -3.2185059 -4.3570242 -4.2231479 -4.9716539][-5.0000339 -3.8359737 -2.3959732 -1.2192526 -0.24350262 0.26000071 0.640079 0.65930033 0.74931765 -0.26845407 -1.5806842 -2.2955594 -2.7221565 -3.1672034 -4.5499187][-4.5578079 -3.4969749 -2.1418447 -0.31732273 1.0354323 1.8865209 2.576159 2.9267201 3.4773393 2.5303063 1.1827006 -0.54179239 -2.5046377 -2.788908 -3.5559421][-3.7023554 -2.9728355 -1.6666484 0.11165237 1.8836789 3.5016274 4.8198009 5.2599664 5.5320249 4.2194428 2.7157016 1.0579963 -0.84562683 -2.2347651 -3.9864759][-3.4300966 -2.66678 -1.2410011 0.2023139 2.1729312 3.552485 4.962646 5.6888127 6.1003747 4.675941 2.5627675 0.41072321 -1.1279545 -2.1914539 -3.6796334][-3.6121314 -2.9147849 -1.8837461 -0.44006634 1.0136409 2.4567752 3.9755931 4.4629741 4.8548589 3.3823104 1.6999154 -0.23442936 -2.1089616 -3.2356997 -4.2811575][-4.3728895 -3.7547619 -2.7517195 -1.3144832 0.079891205 1.1350703 2.066154 2.5386434 2.5946269 1.3655953 0.099364758 -1.5777078 -2.9663386 -4.0932636 -5.3628664][-3.9239273 -3.2070494 -2.7438736 -2.2686286 -1.1074305 -0.25572872 0.74044085 1.2524867 1.5202947 0.352468 -1.2582073 -2.2361469 -3.0758519 -3.8026192 -4.9587793][-5.3490009 -4.8361177 -4.3548775 -3.7420502 -3.2063475 -2.457602 -1.6165524 -1.3001442 -0.95180988 -1.4255676 -1.8370681 -2.5223904 -3.9671898 -4.20535 -4.4557905][-5.4490094 -5.3971858 -5.2213697 -4.8182936 -4.2368813 -3.6529005 -2.7019496 -2.3069205 -2.1402211 -2.544539 -3.0705438 -2.8907976 -3.2448745 -4.1605797 -5.1245146][-6.2957506 -5.8911171 -5.6904716 -5.5125751 -5.3448114 -5.1419382 -4.7500877 -4.2528267 -3.3560395 -3.2550836 -3.92419 -3.9858768 -4.556632 -4.5628514 -5.0906935][-7.4237547 -6.0456696 -5.1951504 -5.6639509 -5.6612082 -5.607193 -5.5180283 -5.4353986 -5.2196093 -4.9496279 -4.5325379 -4.7729449 -5.2515855 -5.7926683 -5.9489565]]...]
INFO - root - 2017-12-15 11:46:25.228513: step 8410, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 56h:52m:24s remains)
INFO - root - 2017-12-15 11:46:31.614493: step 8420, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 58h:31m:33s remains)
INFO - root - 2017-12-15 11:46:38.030430: step 8430, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 57h:13m:16s remains)
INFO - root - 2017-12-15 11:46:44.467255: step 8440, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 58h:37m:03s remains)
INFO - root - 2017-12-15 11:46:50.912567: step 8450, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.628 sec/batch; 56h:32m:21s remains)
INFO - root - 2017-12-15 11:46:57.264265: step 8460, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 57h:28m:51s remains)
INFO - root - 2017-12-15 11:47:03.679365: step 8470, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:25m:19s remains)
INFO - root - 2017-12-15 11:47:10.065448: step 8480, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 57h:38m:51s remains)
INFO - root - 2017-12-15 11:47:16.474451: step 8490, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 57h:50m:10s remains)
INFO - root - 2017-12-15 11:47:22.843349: step 8500, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 59h:31m:14s remains)
2017-12-15 11:47:23.416558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4107685 -5.6738005 -5.8209467 -5.4548879 -5.3710704 -4.9989634 -4.61617 -4.4093575 -4.0088654 -4.8707294 -4.8238382 -5.7458735 -6.1088209 -6.4815187 -7.4286366][-5.4738569 -5.5486298 -5.6855431 -5.8797646 -5.9100418 -5.5968285 -5.135653 -4.8726606 -4.4311085 -5.20677 -5.0413485 -5.8853831 -6.4123921 -6.9229336 -7.8945751][-5.6266651 -5.7236204 -5.7263708 -5.9699974 -5.8390188 -5.3886871 -4.7598619 -4.4131165 -3.9117477 -4.6308961 -4.2399855 -5.0651345 -5.2786183 -5.741354 -7.036654][-6.0522738 -6.0663815 -5.90687 -5.2972956 -4.7541881 -3.791985 -2.7830229 -2.2809372 -1.7540531 -2.7440276 -2.8260412 -3.8866303 -4.4757862 -5.3501463 -6.4599018][-6.1485767 -5.759181 -5.3594379 -4.12609 -2.82939 -1.4761486 -0.15538692 0.16670132 0.403049 -0.76200676 -1.0721002 -2.5144014 -3.5165162 -4.3504448 -5.7227283][-6.0694575 -5.3705592 -4.7027955 -3.0577092 -1.1120586 0.38391495 1.6642461 1.919826 2.0980477 0.60713053 -0.062560558 -1.3780489 -2.3542075 -3.4015265 -4.6790991][-5.4417286 -4.8729849 -4.0546246 -2.3866391 -0.3765173 1.4158111 2.9078794 3.4262147 3.7392402 2.0462642 0.850904 -0.74069262 -2.042953 -2.9551015 -4.2126265][-4.9405575 -4.1821809 -3.4468374 -1.5756717 0.57034731 2.2481103 3.6709476 4.1144652 4.0979686 2.4122443 0.96590853 -0.99060726 -2.355082 -3.3923249 -4.3916483][-4.263422 -3.7867069 -3.0499578 -1.5177302 0.17439938 1.5701814 2.7568812 3.1895118 3.3216529 1.6680369 0.41959429 -1.3392339 -2.7779312 -3.58177 -4.406146][-4.4864054 -4.0181484 -3.1571679 -2.1819153 -1.0980997 0.095999241 1.0905356 1.531939 1.772862 0.50187731 -0.29027414 -1.9206991 -2.8486395 -3.9033587 -5.0002441][-5.1611004 -4.7472992 -4.3966742 -3.1639433 -2.3909998 -1.7058582 -0.96033382 -0.50009012 -0.097321987 -0.99114132 -1.6179581 -2.8636727 -3.8923073 -4.4855103 -5.31581][-5.4212189 -4.9554033 -4.4303784 -3.7958896 -3.190208 -2.3780293 -1.9536524 -1.5423036 -1.0466943 -1.5061903 -1.5210986 -2.8625584 -3.5962124 -4.185585 -5.0928168][-6.3407826 -5.605299 -4.9004951 -4.2397518 -3.5639853 -2.9438405 -2.6124554 -2.5245028 -2.1409307 -2.0799923 -1.8864484 -2.5010118 -2.7073789 -3.2918267 -4.0932894][-6.5858054 -5.873971 -5.3487883 -4.4093704 -3.644537 -3.5765367 -3.7254434 -3.5345197 -3.1878791 -3.3813362 -2.8724003 -2.9458694 -3.3301759 -3.6838853 -4.13031][-6.6214485 -6.1347752 -5.5585079 -5.185627 -4.7906561 -4.7381105 -5.14602 -5.262567 -5.0760126 -4.8797235 -4.6322975 -4.3815212 -3.9259493 -4.2731719 -4.5356092]]...]
INFO - root - 2017-12-15 11:47:29.893422: step 8510, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 56h:08m:11s remains)
INFO - root - 2017-12-15 11:47:36.329394: step 8520, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 56h:17m:26s remains)
INFO - root - 2017-12-15 11:47:42.697895: step 8530, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 56h:52m:48s remains)
INFO - root - 2017-12-15 11:47:49.074916: step 8540, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 56h:14m:56s remains)
INFO - root - 2017-12-15 11:47:55.414484: step 8550, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 57h:19m:06s remains)
INFO - root - 2017-12-15 11:48:01.808000: step 8560, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 56h:48m:22s remains)
INFO - root - 2017-12-15 11:48:08.170794: step 8570, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 58h:40m:06s remains)
INFO - root - 2017-12-15 11:48:14.566475: step 8580, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 57h:23m:51s remains)
INFO - root - 2017-12-15 11:48:20.933524: step 8590, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 58h:36m:24s remains)
INFO - root - 2017-12-15 11:48:27.392244: step 8600, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 57h:00m:47s remains)
2017-12-15 11:48:27.867130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9468417 -3.5375814 -4.2397084 -4.7366815 -4.8295622 -4.3455734 -3.929441 -2.8736768 -1.7242332 -2.39501 -3.4250917 -4.8920369 -6.3978148 -8.0746412 -9.1554384][-3.1967936 -4.2907257 -5.5727606 -6.214365 -6.47592 -6.48694 -5.98412 -4.8934078 -3.8581684 -4.1637406 -4.4787655 -5.7984838 -7.0545368 -7.9542203 -8.6240206][-2.9733033 -3.3739767 -4.4733667 -5.0834389 -5.9215307 -6.2165146 -5.69339 -4.9413452 -4.3048944 -4.3187389 -4.497365 -5.7748022 -6.513442 -6.714169 -7.3101187][-3.2970943 -3.0551267 -3.4026895 -3.7568007 -4.3822131 -4.4602947 -3.9923024 -3.1430898 -2.7189274 -3.4711041 -3.9135501 -5.0088634 -5.5413213 -6.1545005 -6.3292661][-3.9951723 -3.4036055 -2.8648353 -2.3930759 -2.6008983 -2.0319419 -1.1413698 -0.65810633 -0.15166759 -0.98150158 -1.9220672 -3.6769464 -4.3101993 -5.1215172 -5.8864441][-3.7298567 -3.328217 -2.6438189 -1.9485259 -1.3318219 -0.03972435 0.985209 1.8987308 2.4431672 1.2225499 0.041221619 -1.6660738 -3.1471004 -4.2811403 -4.611414][-3.4744549 -2.8980851 -1.9408045 -0.81387663 0.32487917 1.4327693 2.2551818 2.9210687 3.6205401 2.2303481 0.642509 -1.6255331 -3.358377 -4.4796753 -5.620111][-3.0459251 -2.2463503 -1.7259016 -0.60450745 0.41575956 1.5344796 2.4014411 2.8905721 3.3150172 2.1989465 0.84673357 -1.753787 -3.5624776 -4.731926 -5.5301948][-2.9697208 -2.2463269 -1.8174186 -1.1414428 -0.056005478 0.6438117 1.1272235 1.6550403 2.0414424 0.70971441 -0.699924 -2.9810829 -4.5156088 -5.4279618 -5.8422971][-3.2793517 -2.7222791 -1.8996153 -1.2231503 -0.72768641 -0.28069878 0.41091871 0.36781931 0.58030653 -0.84976196 -2.4261575 -4.2093477 -5.7367344 -6.1278381 -6.2426095][-4.3040218 -3.8034902 -2.8623705 -2.4059415 -1.9598336 -1.6084366 -1.1180525 -1.3995314 -1.5784874 -2.9216862 -4.1721392 -5.4790392 -6.4442325 -6.3489161 -6.2218246][-5.2346997 -4.5993595 -4.1515937 -3.9374559 -4.0228353 -3.6732678 -3.254498 -3.2214179 -3.1843824 -4.0764475 -4.97554 -5.2854195 -5.7601686 -5.6413994 -5.4403276][-6.4677 -6.5528603 -6.5820847 -6.3792448 -5.9664412 -5.481432 -4.7028604 -3.7915778 -3.6699827 -4.2355819 -4.8700633 -5.0946288 -5.6514993 -5.3182678 -4.9509163][-6.0529628 -6.3064995 -6.4062862 -6.7717352 -6.5056825 -5.5266929 -4.0836287 -3.4609518 -2.9425139 -2.8270097 -3.5037203 -3.7693121 -4.0281491 -4.32611 -3.93817][-6.0984082 -6.8208547 -7.4843583 -7.9081807 -7.795526 -7.2999167 -6.1956997 -5.0237231 -4.4864349 -4.3754311 -4.183075 -4.3016176 -4.4830027 -4.4296813 -4.0094509]]...]
INFO - root - 2017-12-15 11:48:34.189547: step 8610, loss = 0.31, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 55h:53m:38s remains)
INFO - root - 2017-12-15 11:48:40.582638: step 8620, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:40m:23s remains)
INFO - root - 2017-12-15 11:48:47.043225: step 8630, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 56h:49m:12s remains)
INFO - root - 2017-12-15 11:48:53.407948: step 8640, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:46m:46s remains)
INFO - root - 2017-12-15 11:48:59.795034: step 8650, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 58h:06m:50s remains)
INFO - root - 2017-12-15 11:49:06.188285: step 8660, loss = 0.32, batch loss = 0.21 (12.9 examples/sec; 0.619 sec/batch; 55h:41m:55s remains)
INFO - root - 2017-12-15 11:49:12.596797: step 8670, loss = 0.36, batch loss = 0.24 (12.4 examples/sec; 0.648 sec/batch; 58h:14m:54s remains)
INFO - root - 2017-12-15 11:49:18.970269: step 8680, loss = 0.31, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 55h:59m:17s remains)
INFO - root - 2017-12-15 11:49:25.412246: step 8690, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 56h:12m:34s remains)
INFO - root - 2017-12-15 11:49:31.923914: step 8700, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 57h:07m:08s remains)
2017-12-15 11:49:32.423015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8344231 -3.0320258 -3.6278253 -3.6554 -3.6330168 -3.4134541 -3.1220136 -2.8894162 -2.5980325 -3.0197988 -3.1077132 -3.4403071 -4.5105724 -4.7707267 -5.9429584][-2.6088376 -2.7559366 -2.9374146 -3.3259144 -3.593081 -3.7461534 -3.7924922 -3.8805792 -3.6601644 -4.2177825 -4.2551389 -4.8166475 -5.9879217 -5.8426008 -7.1702003][-2.2319164 -1.9061346 -1.7783089 -1.9287972 -2.0637913 -2.3981614 -2.6251545 -3.088882 -3.40763 -3.9029481 -4.1026978 -4.5268011 -5.3380833 -5.7116666 -6.5990829][-1.1450067 -0.57932377 -0.33683395 -0.66887712 -1.1267495 -1.184485 -1.1593857 -1.6128888 -1.9002557 -2.6948652 -3.1848168 -3.5487428 -4.6924181 -4.7805033 -5.4885492][-0.49740934 0.21271372 0.55151606 0.44134188 0.17172861 0.21199989 0.41630697 -0.099495888 -0.58142567 -1.5599141 -2.118237 -2.5602422 -3.5274029 -3.9145505 -5.1275783][-0.10239697 0.6521945 0.83645773 0.99114943 0.96584272 0.90009069 1.0928082 0.71874094 0.42804289 -0.49189138 -0.77937317 -1.0834374 -1.889039 -2.4232192 -3.9525473][-0.21102333 0.70091867 1.2597985 1.4895616 1.7047458 1.7784343 1.8600659 1.4929128 1.3926225 0.31516027 -0.21906948 -0.59077263 -1.3812971 -1.8914185 -3.4989963][0.029911041 0.67117071 1.3996282 1.949429 2.2894702 2.3414559 2.2862315 2.0906539 2.0713506 0.87024164 0.32714939 -0.3701005 -1.213171 -1.804184 -3.4522767][0.028466225 0.67332315 1.0146508 1.3944364 1.8179116 1.7973542 1.651217 1.8556809 2.1579165 0.80001307 0.023970127 -0.79441786 -1.7588735 -2.7027378 -4.2745295][-1.0650239 -0.74216127 -0.31073427 0.067491055 0.38779879 0.8354125 0.73523378 1.109838 1.4881625 0.46080351 -0.30674076 -1.3415475 -2.4302759 -3.2922926 -4.7685485][-2.9649153 -2.5432262 -2.2670155 -1.5073481 -0.79124689 -0.48494148 -0.37650013 -0.36039829 -0.406909 -1.5074639 -2.2039504 -3.2092147 -4.1190586 -4.6488237 -5.7187338][-4.0762367 -3.7263727 -3.5698047 -3.4766641 -3.036623 -2.6770048 -2.4624515 -2.552928 -2.4162831 -3.7772346 -4.5284681 -4.8538713 -5.36238 -5.531003 -6.1113167][-6.1437778 -5.6812057 -5.3619924 -5.1949143 -5.1361752 -4.7152066 -4.6184587 -4.3659792 -4.2687006 -4.8153896 -5.0249643 -5.4399533 -5.4115181 -5.1145191 -5.091958][-7.0769715 -7.1292472 -7.2041259 -6.8370333 -6.2782588 -6.0124679 -5.9640694 -6.0249109 -6.0746012 -6.3842745 -6.7465281 -6.2536268 -5.5645881 -4.6553144 -4.0629282][-6.8972068 -7.1007166 -7.265327 -7.2890835 -7.4395742 -7.4254618 -7.5722003 -7.5747485 -7.4634976 -7.2264705 -7.2271824 -6.8575134 -6.4096875 -5.6218972 -4.7444835]]...]
INFO - root - 2017-12-15 11:49:38.934033: step 8710, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 58h:10m:28s remains)
INFO - root - 2017-12-15 11:49:45.295980: step 8720, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:33m:39s remains)
INFO - root - 2017-12-15 11:49:51.769178: step 8730, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 55h:59m:49s remains)
INFO - root - 2017-12-15 11:49:58.227023: step 8740, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 57h:39m:47s remains)
INFO - root - 2017-12-15 11:50:04.626757: step 8750, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 57h:30m:23s remains)
INFO - root - 2017-12-15 11:50:11.052121: step 8760, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.634 sec/batch; 56h:59m:29s remains)
INFO - root - 2017-12-15 11:50:17.428453: step 8770, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 57h:12m:31s remains)
INFO - root - 2017-12-15 11:50:23.875576: step 8780, loss = 0.37, batch loss = 0.25 (12.6 examples/sec; 0.637 sec/batch; 57h:18m:05s remains)
INFO - root - 2017-12-15 11:50:30.396764: step 8790, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:36m:25s remains)
INFO - root - 2017-12-15 11:50:36.890247: step 8800, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 58h:33m:42s remains)
2017-12-15 11:50:37.375458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5712147 -4.4999146 -5.1626625 -6.218545 -6.4492011 -6.3553667 -6.8623314 -7.164505 -6.9942088 -7.7085056 -7.1178527 -7.3201475 -7.3541741 -6.2825994 -4.4381924][-3.1716018 -3.5610628 -3.6973183 -4.5956144 -4.7957754 -5.0106764 -5.7109756 -6.3104439 -6.860136 -7.7101417 -7.0262675 -7.0976539 -6.2765946 -5.7343144 -4.7002907][-1.6276655 -2.3707542 -2.7862115 -3.4836226 -3.2852449 -2.8750129 -3.7193568 -4.3188381 -5.042182 -6.2411237 -5.6034846 -6.7685304 -7.2920451 -7.8042231 -7.6621437][-4.1980691 -3.5609179 -3.6442456 -3.7345915 -3.5071421 -3.6006246 -3.5424366 -3.4590793 -4.3282809 -5.8209372 -5.992743 -7.4388628 -7.866004 -7.943799 -8.0408115][-4.0090642 -3.7236266 -3.7206061 -2.9069653 -1.6538472 -1.0636382 -1.1437082 -1.0808296 -1.7708373 -3.472291 -3.499105 -5.1643472 -6.7112446 -7.6012974 -7.839138][-5.5295916 -4.7744341 -4.5309572 -3.0249877 -1.8044906 -1.0806837 0.077803135 0.96988297 0.8682909 0.052147388 -0.12898684 -1.7299533 -2.7160931 -4.2951555 -5.2094336][-4.8445296 -4.4820585 -3.9195867 -2.2116313 -0.61331415 0.95323849 2.3388615 2.3275642 2.082304 1.3905535 1.5154066 0.18982649 -1.8651567 -3.8835447 -4.3707094][-4.5308828 -3.9720852 -2.9273877 -0.87995148 0.95902824 2.9408741 4.5365629 4.7014732 4.4154053 2.4369402 2.2922611 1.8049498 0.37416363 -1.1347327 -2.487011][-4.9586043 -4.370266 -3.603631 -2.107687 -0.3022995 2.1298618 3.8533955 5.1592121 5.7414017 3.5432816 2.8858528 0.77201939 -1.4268665 -3.1925688 -4.2812462][-6.5619411 -5.8909483 -5.1959858 -3.7228127 -2.24965 -0.398005 1.0077724 2.5823765 3.138483 1.5532503 1.0111837 -1.3967886 -3.7948482 -5.1016874 -5.6618638][-7.9857831 -7.8494449 -7.1912255 -5.8408308 -4.5399418 -2.7898583 -1.7652879 -0.93060017 0.56199455 0.12590265 -0.25052214 -2.0299792 -4.2993193 -5.9389272 -6.79482][-8.26015 -7.9405394 -7.9797888 -7.834343 -6.8329453 -5.2513733 -4.3216853 -3.4544091 -2.3939342 -2.6092958 -2.3330221 -3.3371582 -4.7519388 -5.7501812 -6.8277335][-8.6396723 -8.0511055 -8.2915573 -8.1696835 -7.6691542 -6.7905831 -6.1878338 -5.86187 -5.2486763 -4.948802 -4.6437178 -4.8990455 -5.0125494 -5.3803306 -6.3672023][-8.2498531 -7.7304907 -7.6374779 -7.7107449 -7.6847186 -7.3303471 -7.1583476 -7.0892572 -6.80869 -6.7369628 -6.5348167 -6.6285682 -6.5236878 -5.7457476 -5.4035697][-9.0574913 -9.1139984 -9.2336922 -8.56089 -7.77997 -7.4527268 -7.3878174 -7.9247084 -8.0988054 -8.3242035 -8.1355124 -7.4287429 -7.1208243 -7.1682982 -6.6555629]]...]
INFO - root - 2017-12-15 11:50:43.773983: step 8810, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 57h:08m:55s remains)
INFO - root - 2017-12-15 11:50:50.301608: step 8820, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 58h:53m:50s remains)
INFO - root - 2017-12-15 11:50:56.796247: step 8830, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 58h:34m:40s remains)
INFO - root - 2017-12-15 11:51:03.254377: step 8840, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 58h:38m:54s remains)
INFO - root - 2017-12-15 11:51:09.569895: step 8850, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:33m:13s remains)
INFO - root - 2017-12-15 11:51:15.948516: step 8860, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:36m:57s remains)
INFO - root - 2017-12-15 11:51:22.376045: step 8870, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.626 sec/batch; 56h:15m:02s remains)
INFO - root - 2017-12-15 11:51:28.745169: step 8880, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 57h:17m:32s remains)
INFO - root - 2017-12-15 11:51:35.218517: step 8890, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 60h:14m:54s remains)
INFO - root - 2017-12-15 11:51:41.723544: step 8900, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 58h:46m:56s remains)
2017-12-15 11:51:42.240007: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.059208393 0.14383745 0.2717762 0.30115318 0.55119276 1.0822997 1.0546412 0.86860991 0.35173798 -0.38869286 -2.8634868 -6.4620867 -8.0842667 -9.1573143 -10.057335][0.9429698 0.25273705 -0.078830242 -0.022134304 0.17684269 0.664062 0.86893797 0.68468428 0.093934059 -0.67193937 -3.2845306 -6.8611097 -8.4359579 -9.3990669 -10.060546][2.1267076 2.0116258 1.6409421 1.3587308 1.4515119 1.7499356 1.6004214 1.2703738 0.88405371 0.16908646 -2.6330256 -5.5280514 -7.1892986 -8.1447859 -9.0905581][2.4385562 2.4963717 2.324141 2.4615378 2.3072276 2.6403203 2.4910226 1.7042356 1.0008988 0.29554558 -2.5225544 -5.9203672 -7.1469669 -7.8732061 -8.4036694][1.3223281 1.9905782 2.0337796 2.3132234 2.2664618 2.3708425 2.5283351 2.1478448 1.6620259 0.72429132 -1.8354254 -5.3665867 -6.8453817 -7.6934962 -8.2584934][-0.86320829 0.079095364 0.99718142 1.9569869 2.268168 2.4080224 2.5931153 2.2318616 1.7472405 0.96834135 -1.6344485 -4.7471523 -6.2379332 -7.324409 -7.5573034][-2.6375227 -1.6360188 0.031446457 0.83298063 1.3792682 1.3386321 1.4954429 1.1772132 0.81438494 0.077076912 -2.4794283 -5.61739 -6.8589759 -7.8103366 -8.1952095][-3.4239664 -2.0994945 -1.1866593 -0.23919296 0.17785025 0.22930098 0.43602133 0.35160255 -0.062390327 -0.45023203 -3.016531 -5.9352818 -7.1674504 -8.0244389 -8.3869143][-4.2701044 -3.01088 -1.9717264 -0.974679 -0.39298868 -0.17833471 -0.093121529 0.08409977 -0.064591408 -0.45460606 -2.6103997 -5.5128541 -6.783546 -7.9088488 -8.5311489][-4.8366151 -3.7884316 -3.0264916 -2.061142 -1.2514524 -0.80512238 -0.86353683 -0.87987614 -0.71073627 -1.2854867 -3.1062083 -5.3248134 -6.243319 -7.1584778 -8.1428146][-4.472271 -3.8601499 -3.0562644 -2.4286489 -1.7616291 -1.5015345 -1.3921933 -1.6186948 -1.7176862 -2.4268289 -3.8153472 -5.466445 -5.9193158 -6.6776776 -7.3340931][-4.2178659 -3.7104735 -2.9508543 -2.2249527 -1.9845166 -1.9976106 -2.0021195 -2.0745463 -2.4132843 -2.6539636 -4.0166121 -5.3403254 -5.9266543 -6.3067331 -7.0635285][-4.1168823 -3.351522 -2.6652083 -2.1214724 -2.19381 -2.5162487 -2.9628053 -3.2885466 -3.2342291 -3.3947973 -4.1211243 -5.1249175 -5.7878971 -5.9894295 -6.5552616][-4.6652164 -4.1456356 -3.5950332 -3.0951991 -3.0283375 -3.2626128 -3.6882 -4.2499924 -4.5660896 -4.5461688 -4.9949369 -5.2269793 -5.5636549 -5.6761909 -5.7169456][-6.2882705 -5.5125 -4.7491341 -4.5148573 -4.5544739 -4.22377 -4.6456852 -5.2780962 -5.8008728 -5.9149284 -6.0101094 -6.2503657 -5.9305773 -5.5754051 -5.51036]]...]
INFO - root - 2017-12-15 11:51:48.686571: step 8910, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:32m:25s remains)
INFO - root - 2017-12-15 11:51:55.076386: step 8920, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 56h:47m:54s remains)
INFO - root - 2017-12-15 11:52:01.510733: step 8930, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 56h:54m:59s remains)
INFO - root - 2017-12-15 11:52:08.089690: step 8940, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.678 sec/batch; 60h:57m:29s remains)
INFO - root - 2017-12-15 11:52:14.612716: step 8950, loss = 0.32, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 59h:57m:04s remains)
INFO - root - 2017-12-15 11:52:21.029058: step 8960, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:18m:28s remains)
INFO - root - 2017-12-15 11:52:27.462936: step 8970, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 58h:34m:49s remains)
INFO - root - 2017-12-15 11:52:33.925909: step 8980, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 57h:22m:06s remains)
INFO - root - 2017-12-15 11:52:40.450661: step 8990, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 57h:47m:45s remains)
INFO - root - 2017-12-15 11:52:46.891467: step 9000, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 58h:32m:27s remains)
2017-12-15 11:52:47.369882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1817007 -3.4340558 -3.5559373 -3.4627156 -2.9968119 -2.6253357 -2.1801162 -1.3123727 -0.79821491 -1.4591737 -2.694077 -5.28077 -6.2498951 -6.8266845 -7.0873246][-3.6246939 -3.8939307 -4.3590994 -5.0202541 -4.8783617 -4.4419327 -4.0838966 -3.5704017 -3.0240993 -3.6598763 -4.8615761 -7.0927677 -7.603056 -8.12911 -8.720787][-3.2271671 -3.5339704 -3.8103247 -4.0559888 -4.0819626 -3.90098 -3.5164089 -3.3038635 -3.4289322 -3.9785473 -5.0173159 -6.9917493 -7.7190604 -8.3945894 -8.4428883][-3.6233134 -3.744664 -3.7243254 -3.3368125 -3.0769019 -2.7589788 -2.2474422 -2.1338086 -2.3613195 -3.2285085 -4.4382887 -6.0242476 -6.6279907 -7.5583386 -7.9024472][-4.3143005 -4.538105 -4.7815189 -4.2241349 -3.0691195 -1.8913956 -0.93249083 -0.55776119 -0.83625412 -2.2978125 -4.0805569 -6.08854 -6.8118896 -7.5154819 -7.695416][-6.1375594 -5.7432823 -5.210165 -4.4519644 -3.2591825 -1.3342099 0.34430218 0.87617445 0.70831442 -0.69880056 -2.5829802 -4.9562387 -6.0064049 -6.4411192 -6.6287222][-6.5215521 -5.8940291 -5.2151365 -3.8483696 -1.9506059 0.27894974 2.1917052 2.9205804 2.942348 1.4276309 -0.85883427 -3.8861542 -5.4570885 -5.9764085 -6.3088336][-6.270649 -5.7028618 -4.564497 -3.1129432 -1.54986 0.78745127 2.9855704 3.4923434 3.113235 1.8407607 -0.16869497 -3.1294298 -5.0281959 -6.018342 -6.1015291][-4.4160662 -3.7218347 -3.09062 -1.6814823 -0.20663548 1.5295224 3.2469049 4.05426 4.1204877 2.4684672 0.28231859 -2.5664062 -4.1604481 -5.3938551 -5.8866072][-3.2906103 -2.5999494 -2.0756249 -1.1089196 -0.2154932 1.1021819 2.1240258 2.2520213 2.0188041 0.45074987 -1.1142302 -3.7706449 -4.9179683 -5.5360537 -5.60805][-2.2214069 -1.8825603 -1.6247754 -1.0537353 -0.51989555 0.21140003 0.91233587 0.49427271 -0.040482998 -1.4785409 -3.1383777 -5.4000578 -6.3970704 -6.3271308 -5.9330812][-1.5610642 -1.3874574 -1.1335173 -0.73916769 -0.72760725 -0.57813883 -0.49360895 -1.3278923 -2.2403297 -3.202033 -4.3210454 -6.0993738 -6.9473047 -7.0361915 -6.597393][-3.5520854 -3.2597079 -2.8127565 -2.440444 -2.4667726 -2.2888622 -2.3285718 -3.1741219 -3.9525919 -5.0093679 -5.5291739 -6.3253083 -6.7067642 -7.1269932 -6.802537][-4.4581232 -4.1265674 -3.5127792 -2.62318 -2.2613945 -1.9577041 -1.8310146 -2.5736122 -3.5258007 -4.6353941 -5.1184168 -5.4441843 -5.8043742 -6.2324 -5.7947721][-5.7236791 -5.5452223 -4.9271984 -3.9772558 -3.6119318 -3.4220986 -3.5263271 -3.6854589 -4.2183571 -4.8277788 -4.8171177 -5.2787638 -5.6504993 -5.9357533 -6.0016489]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 11:52:53.762394: step 9010, loss = 0.33, batch loss = 0.22 (12.2 examples/sec; 0.653 sec/batch; 58h:42m:46s remains)
INFO - root - 2017-12-15 11:53:00.163713: step 9020, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 57h:22m:44s remains)
INFO - root - 2017-12-15 11:53:06.600396: step 9030, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 57h:46m:21s remains)
INFO - root - 2017-12-15 11:53:13.074954: step 9040, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:36m:02s remains)
INFO - root - 2017-12-15 11:53:19.424321: step 9050, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 56h:56m:50s remains)
INFO - root - 2017-12-15 11:53:25.804039: step 9060, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:38m:16s remains)
INFO - root - 2017-12-15 11:53:32.207606: step 9070, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 56h:21m:22s remains)
INFO - root - 2017-12-15 11:53:38.658350: step 9080, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 58h:12m:30s remains)
INFO - root - 2017-12-15 11:53:45.113779: step 9090, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 57h:17m:24s remains)
INFO - root - 2017-12-15 11:53:51.537392: step 9100, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 59h:55m:45s remains)
2017-12-15 11:53:52.119207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3204 -6.0960703 -6.4685874 -6.3496742 -6.2560921 -5.97478 -4.9000683 -3.9284296 -2.9127107 -4.1397085 -4.7094841 -4.6228404 -5.3719559 -6.6441994 -6.8565884][-5.9439745 -6.3441572 -6.789207 -7.0168195 -7.13593 -6.8188457 -6.0574589 -5.0215435 -3.8244753 -5.1879435 -5.6854105 -5.63156 -6.5838675 -7.8106103 -8.2281523][-4.9827003 -5.2839665 -5.7373929 -5.8006873 -6.001224 -6.0223083 -5.520009 -4.6801224 -4.101193 -5.4177017 -5.9076185 -5.9400883 -6.8227544 -8.0200806 -8.5007687][-5.398901 -5.3564281 -5.555182 -5.3735161 -5.3315458 -4.5269785 -3.61231 -2.9668989 -2.4672556 -4.2685804 -5.3196774 -5.772069 -6.8887863 -8.029561 -8.3185][-6.7386594 -6.1999784 -5.2620687 -4.5101528 -4.0230503 -2.8194752 -1.7970123 -1.452805 -1.1715984 -3.2330804 -4.5601549 -5.3303962 -6.4045324 -7.5685143 -8.08223][-8.4728527 -7.3927436 -6.1192808 -4.0514259 -1.8510542 -0.23785639 0.98567104 1.2177849 1.0568142 -1.7444601 -3.885896 -4.7184906 -5.6836729 -7.3448677 -8.4566288][-8.7209892 -7.0189204 -5.0442266 -2.3772058 0.13094759 2.4074984 3.9497538 4.3666086 4.1369 0.35311747 -2.3757348 -3.6322267 -4.9939442 -6.4178581 -7.0849581][-7.7467957 -6.5059094 -5.1909389 -2.7063003 0.46857262 2.4984875 3.7687969 4.6944609 4.7090616 0.82482529 -2.2561932 -3.7749746 -4.6234641 -6.1130643 -6.7687664][-8.2532177 -7.7419381 -6.697504 -4.2065239 -1.3244605 0.67000771 2.358531 2.9162025 2.785533 -0.54718304 -3.3291636 -4.40395 -5.5323567 -6.4067936 -6.4085903][-9.6757689 -8.9767027 -7.6688557 -5.0023251 -2.1226969 -0.45412827 1.1320944 1.5911303 1.1265612 -2.5583992 -4.8765931 -6.016573 -7.1709032 -7.9742842 -7.6337848][-11.1355 -10.395173 -9.5828819 -7.8202772 -6.0185385 -3.9831522 -2.2520142 -2.4023685 -2.8914828 -5.5850019 -6.9644389 -7.4466267 -8.0333681 -9.1312046 -8.9844952][-11.059479 -10.229975 -9.272584 -8.4173307 -7.6117425 -6.6109414 -5.9911942 -5.4812145 -5.1652594 -7.3099308 -8.4553518 -8.78775 -9.1292038 -9.1988993 -8.585537][-11.389405 -10.766186 -10.263319 -9.3171539 -8.49436 -7.6468105 -6.9340086 -6.8159356 -7.0663133 -9.0820122 -9.8365784 -9.6406775 -9.9120722 -9.6800652 -9.0618477][-10.814957 -10.207341 -9.7457514 -9.2194834 -8.3185158 -7.5950646 -7.209311 -7.0637879 -7.1867561 -8.3895226 -8.8231945 -9.1884756 -9.58255 -9.6821089 -8.7991495][-10.411014 -9.8510666 -9.4451342 -8.8781195 -8.3854218 -7.778059 -7.1687517 -7.41653 -7.6032019 -7.7625189 -7.8054705 -7.9030471 -7.8353395 -7.7579031 -7.1776576]]...]
INFO - root - 2017-12-15 11:53:58.614006: step 9110, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 56h:11m:43s remains)
INFO - root - 2017-12-15 11:54:05.079503: step 9120, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 58h:04m:27s remains)
INFO - root - 2017-12-15 11:54:11.608995: step 9130, loss = 0.31, batch loss = 0.20 (11.8 examples/sec; 0.680 sec/batch; 61h:02m:30s remains)
INFO - root - 2017-12-15 11:54:18.069454: step 9140, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 56h:56m:33s remains)
INFO - root - 2017-12-15 11:54:24.512413: step 9150, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 56h:56m:57s remains)
INFO - root - 2017-12-15 11:54:31.061529: step 9160, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 59h:42m:15s remains)
INFO - root - 2017-12-15 11:54:37.521883: step 9170, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 57h:50m:00s remains)
INFO - root - 2017-12-15 11:54:43.882886: step 9180, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 56h:28m:10s remains)
INFO - root - 2017-12-15 11:54:50.294128: step 9190, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 58h:10m:49s remains)
INFO - root - 2017-12-15 11:54:56.801667: step 9200, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 56h:51m:11s remains)
2017-12-15 11:54:57.362051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0056033 -2.9803219 -2.6882877 -2.9467621 -3.7377095 -4.1749954 -4.4732985 -4.538703 -4.436861 -5.8908348 -6.3028297 -6.9298058 -8.05002 -8.9228325 -8.6873589][-2.8163548 -2.6362977 -2.8918881 -3.4785171 -3.7415173 -3.9617352 -3.9175534 -4.2452106 -3.9430234 -5.3025913 -5.8510852 -6.9687052 -8.1193771 -8.8740149 -8.9303417][-3.6236105 -3.4537325 -3.5892234 -3.5986891 -3.78529 -3.4086967 -3.2688227 -3.0698867 -2.5551362 -4.1132874 -4.6919088 -5.7929683 -7.1004214 -8.1101618 -7.9637671][-3.6855798 -3.4013124 -3.0714512 -2.8742533 -2.591311 -2.030766 -1.6518769 -1.5138493 -1.412077 -3.0494704 -3.8656909 -5.3556 -6.7219033 -7.3786168 -7.2972126][-4.2286882 -3.1473408 -2.0862803 -1.4057441 -0.62708712 0.24171066 0.67581749 0.65202427 0.73252392 -1.2250924 -2.3692131 -4.090786 -5.6283731 -6.5326672 -6.4897795][-3.8130655 -3.284265 -2.0951295 -0.769094 0.54750919 1.7644262 2.5649462 2.4809732 1.9335308 -0.27082872 -1.3765702 -3.0570679 -4.6734953 -5.7552671 -5.8355894][-3.6451375 -2.8955445 -1.236074 0.43217182 1.9611845 2.9892139 3.6802244 3.5731592 3.1353111 0.42957687 -1.1059828 -2.9700956 -4.5531511 -5.5207648 -5.9937673][-2.4625325 -1.2717657 0.0521636 1.8408909 2.980196 3.7388115 3.8803091 3.6426477 3.1172352 0.65401459 -1.0133586 -2.8748856 -4.6716881 -6.0438585 -6.2262907][-1.8353724 -1.0632043 0.39852524 1.7207432 2.4286842 2.8096781 2.6320782 2.3376102 1.7699776 -0.48423767 -1.9295373 -3.3645968 -5.269783 -6.47167 -6.8937559][-2.4605598 -1.896287 -0.37901402 0.9325943 1.3743582 1.4418736 1.4777193 1.2354889 0.59190845 -1.6259351 -2.5671206 -4.0290861 -5.5253572 -6.7710357 -7.1646762][-3.6605074 -2.9791603 -2.3649721 -1.2889585 -0.78570795 -0.58234453 -0.69445944 -1.0158658 -0.89136839 -2.6785259 -3.9489908 -5.0388002 -6.26119 -7.3298364 -7.41182][-4.5120039 -3.8420234 -3.3192773 -2.8993883 -2.4634743 -2.3583241 -2.5030179 -2.5232058 -2.4795971 -3.6448617 -4.7606049 -5.2621765 -6.2777462 -7.04131 -7.22649][-5.4750929 -5.3358045 -5.2353797 -4.54787 -4.3816404 -3.9602792 -3.6150775 -3.9054804 -3.9740918 -4.4443264 -5.2047143 -5.6723747 -6.2052641 -6.3605332 -6.4340639][-5.0623288 -5.4367418 -5.3927069 -5.0384979 -4.7319756 -4.283474 -4.2512236 -4.2321014 -3.9342911 -4.1649561 -4.4529018 -4.9194632 -5.4376726 -6.0010772 -6.0541935][-6.04078 -5.4401197 -5.2849951 -5.3525634 -5.3921766 -5.1498079 -4.8264 -4.807972 -5.0209141 -4.6634569 -4.8597326 -5.1172757 -5.2787247 -5.5550804 -5.8214631]]...]
INFO - root - 2017-12-15 11:55:03.770584: step 9210, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 56h:48m:21s remains)
INFO - root - 2017-12-15 11:55:10.110343: step 9220, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 55h:24m:00s remains)
INFO - root - 2017-12-15 11:55:16.521401: step 9230, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 58h:07m:51s remains)
INFO - root - 2017-12-15 11:55:22.927604: step 9240, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 56h:57m:55s remains)
INFO - root - 2017-12-15 11:55:29.382482: step 9250, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 58h:05m:35s remains)
INFO - root - 2017-12-15 11:55:35.817909: step 9260, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:14m:39s remains)
INFO - root - 2017-12-15 11:55:42.328988: step 9270, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 58h:01m:59s remains)
INFO - root - 2017-12-15 11:55:48.742707: step 9280, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:36m:46s remains)
INFO - root - 2017-12-15 11:55:55.194333: step 9290, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 56h:53m:32s remains)
INFO - root - 2017-12-15 11:56:01.558165: step 9300, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:38m:26s remains)
2017-12-15 11:56:02.059815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4366827 -4.875452 -4.6586027 -4.7841129 -4.4098291 -4.2289429 -4.5246725 -4.0270276 -4.114377 -4.59871 -4.9164343 -5.3831048 -6.3893895 -7.238698 -6.9812541][-4.804081 -4.6888 -4.9531412 -5.0605211 -5.4363794 -5.2827177 -5.0418386 -4.75424 -5.4466252 -6.0768471 -5.4166665 -5.7980971 -6.6130705 -8.2426434 -8.3047762][-3.4988966 -3.2957034 -4.151464 -4.7130375 -5.2616062 -5.3785524 -5.1769962 -4.9484286 -4.5312481 -5.3675213 -5.146863 -6.1381955 -7.3065491 -8.3449211 -8.1559315][-5.1416245 -4.5734234 -4.1437964 -4.010191 -3.6416881 -3.0324602 -2.8277678 -3.0995102 -3.2477231 -4.6031008 -4.413105 -4.7431583 -5.2622242 -6.289392 -6.7802396][-4.6722717 -4.4269695 -4.5039968 -3.6611452 -3.0550199 -2.3767695 -1.7053676 -1.4423218 -1.6957316 -2.7728519 -2.5074654 -3.103857 -3.7364905 -5.2947178 -5.5169649][-0.94191885 -1.773016 -2.0527763 -1.5736475 -1.2180843 -0.9441514 -0.84852648 -0.56437874 -0.53985834 -1.9836926 -1.7536283 -2.5669222 -2.8070159 -4.3469906 -4.7216172][-0.34380388 0.33333874 0.508276 0.20628691 0.37286568 0.64205933 0.9202013 0.57495117 0.29180145 -1.6109734 -1.3689728 -2.0017433 -2.8205223 -4.2577229 -4.4398918][0.40942383 -0.17473936 -0.066819191 0.62822914 0.78225708 1.5834608 1.8730803 1.4925833 1.31882 -0.47580719 -0.45261574 -1.7105932 -2.4270253 -4.3355246 -4.9170718][-0.086445332 0.38280106 0.24757814 0.58700752 0.75171947 0.64561272 1.0506649 1.3222666 1.1129322 -0.64164352 -0.71937609 -1.9158049 -2.975194 -4.4522057 -4.5616736][-0.95922089 -0.1942153 0.076972485 0.44394779 0.50246906 0.77329159 0.83560085 0.8568697 0.85653782 -0.7482748 -0.8645072 -2.5793443 -3.715986 -5.45873 -5.4371371][-3.6369925 -2.8643498 -2.9396458 -2.7965069 -2.8133245 -2.6539025 -2.6186957 -2.4831171 -1.9044085 -2.6989679 -2.9296732 -4.40948 -5.670495 -6.8356009 -6.7434435][-5.1366467 -4.8736591 -4.9721832 -4.5991535 -4.8358421 -5.0014162 -4.6976881 -4.3323507 -3.8932266 -4.2320557 -4.3691239 -5.1446495 -6.0821261 -7.1639028 -7.3422289][-6.608942 -6.2707005 -6.6237078 -6.8217106 -7.0334616 -6.76013 -6.5361519 -6.0032377 -5.5392737 -5.3979688 -5.81672 -6.2146416 -6.6283875 -7.0973191 -6.8709221][-6.5602531 -6.5300922 -6.8473144 -6.3480768 -6.4125142 -6.1568284 -5.704103 -5.4474411 -5.5812941 -5.8811407 -5.78438 -5.9440956 -6.0193367 -5.6561127 -5.3456306][-6.58853 -5.9993029 -6.6022158 -6.6844711 -6.0036883 -6.3319588 -6.2443857 -5.98556 -5.634007 -5.4547577 -5.9391809 -6.34705 -5.9537191 -5.7382164 -5.6753535]]...]
INFO - root - 2017-12-15 11:56:08.471366: step 9310, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 59h:06m:11s remains)
INFO - root - 2017-12-15 11:56:14.972262: step 9320, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 58h:53m:26s remains)
INFO - root - 2017-12-15 11:56:21.469307: step 9330, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 58h:44m:50s remains)
INFO - root - 2017-12-15 11:56:27.952878: step 9340, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 57h:30m:25s remains)
INFO - root - 2017-12-15 11:56:34.421806: step 9350, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 59h:25m:07s remains)
INFO - root - 2017-12-15 11:56:40.899986: step 9360, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 59h:40m:39s remains)
INFO - root - 2017-12-15 11:56:47.349028: step 9370, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 57h:14m:00s remains)
INFO - root - 2017-12-15 11:56:53.695625: step 9380, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 58h:13m:39s remains)
INFO - root - 2017-12-15 11:57:00.067154: step 9390, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 58h:45m:37s remains)
INFO - root - 2017-12-15 11:57:06.457135: step 9400, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 58h:41m:32s remains)
2017-12-15 11:57:06.987339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9512734 -5.1190481 -3.8682485 -4.1096373 -4.60734 -4.4133167 -4.4376535 -4.6152992 -4.8509932 -5.0157185 -6.2851257 -5.9741278 -6.4821858 -7.36554 -7.2662568][-5.1422234 -5.3943033 -5.6412177 -5.1645374 -4.0498409 -4.279933 -4.4641523 -4.0319843 -3.8111615 -3.9165308 -5.7745285 -6.1265507 -7.1721158 -7.4752049 -7.28138][-4.6519241 -4.04208 -3.9828589 -3.8635747 -3.175128 -3.3140264 -3.690902 -3.3330908 -3.032784 -3.093729 -4.9055934 -5.139945 -6.1094294 -6.4125953 -6.7165375][-4.6972742 -3.2282004 -2.8840213 -2.9705505 -2.2559891 -1.1557617 -1.3547978 -2.0170684 -2.1120896 -2.2166729 -4.185123 -4.3815374 -5.6208858 -6.2916265 -6.2095838][-4.2852783 -3.1271377 -1.9838881 -1.5286803 -1.4250212 -0.912662 -0.32106352 -0.5759306 -1.2176905 -1.8941951 -3.8605053 -3.7796805 -5.2480755 -6.2256427 -6.1023564][-3.8719356 -3.0208611 -2.2398067 -1.5115995 -0.89670706 -0.098031521 0.25456285 -0.29618454 -0.59720278 -1.0203867 -3.4335508 -3.2008572 -4.3252192 -5.1276817 -5.5648251][-3.7083051 -2.5628233 -1.4141784 -1.0120244 -0.52341795 0.064073086 0.426239 0.37831211 0.16677189 -0.74549675 -2.8978624 -2.6708374 -4.4168291 -5.6137271 -5.2935781][-3.1114612 -2.2105117 -1.1778097 -0.35717392 0.3215642 1.066658 1.1381645 0.52573681 0.21048641 -0.85303879 -2.7457376 -2.6793933 -4.1593542 -5.5392303 -5.5190415][-3.1860685 -2.6210842 -1.2698092 -0.66519213 -0.544538 -0.099298477 0.1850276 0.66574383 0.59498024 -0.56486034 -2.3386655 -2.4201446 -3.9547193 -5.4929113 -5.6860471][-2.5017586 -2.5600042 -2.6670008 -1.8338408 -1.0427227 -0.95659733 -1.2817826 -1.1125398 -0.70652866 -1.2984571 -2.9480076 -3.3054152 -4.3532925 -4.8596087 -5.1409931][-3.5976005 -3.6100798 -3.4936752 -3.147758 -2.8978977 -2.6362419 -2.7216964 -2.4163375 -2.6829023 -3.7375622 -4.1186409 -3.7279677 -4.9669938 -6.5417595 -6.1484833][-3.6818793 -3.6691709 -3.8595147 -3.8292065 -3.675108 -3.3964171 -3.0477271 -3.7059381 -4.1569386 -4.6870518 -5.9930191 -5.9389114 -6.0558739 -6.8666735 -6.87311][-4.6873374 -4.8573542 -4.6041889 -4.2651377 -4.5699644 -4.7586865 -4.2727795 -4.35373 -4.400218 -5.4312649 -7.109868 -6.2939568 -6.0558157 -7.6890445 -8.436801][-4.4523106 -4.3010769 -4.1887484 -4.2043533 -4.1383724 -3.7913935 -4.0469551 -4.3644037 -4.3488808 -4.1510944 -4.6921964 -5.7506447 -6.82878 -7.0652051 -6.929028][-6.3814154 -4.939024 -4.6443796 -5.1000261 -4.77722 -4.6871514 -4.8636007 -4.58407 -4.887805 -5.2476377 -5.3408017 -5.26235 -5.6615086 -6.4979396 -6.7301188]]...]
INFO - root - 2017-12-15 11:57:13.444371: step 9410, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 57h:46m:20s remains)
INFO - root - 2017-12-15 11:57:19.833090: step 9420, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 56h:37m:23s remains)
INFO - root - 2017-12-15 11:57:26.264802: step 9430, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.637 sec/batch; 57h:12m:27s remains)
INFO - root - 2017-12-15 11:57:32.651172: step 9440, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 57h:08m:07s remains)
INFO - root - 2017-12-15 11:57:39.105732: step 9450, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:36m:56s remains)
INFO - root - 2017-12-15 11:57:45.436956: step 9460, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 55h:55m:58s remains)
INFO - root - 2017-12-15 11:57:51.841589: step 9470, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 56h:25m:19s remains)
INFO - root - 2017-12-15 11:57:58.301268: step 9480, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 57h:00m:30s remains)
INFO - root - 2017-12-15 11:58:04.771157: step 9490, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 59h:13m:41s remains)
INFO - root - 2017-12-15 11:58:11.322915: step 9500, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 56h:49m:07s remains)
2017-12-15 11:58:11.857289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9698319 -4.9914904 -4.9300742 -5.7563267 -6.048995 -6.014771 -6.2686 -6.4160094 -6.8047686 -7.4860592 -7.76377 -7.6587572 -7.0649638 -6.39718 -4.8763437][-3.8494842 -3.7712955 -3.9294531 -4.1866922 -4.2841291 -4.5872421 -4.8789635 -5.1565924 -5.4213362 -5.9984913 -6.3894463 -6.48754 -6.0524521 -5.4641228 -3.9409225][-4.0481014 -3.6397595 -3.3910608 -3.2517052 -2.8637538 -2.7982025 -2.7728958 -3.4811468 -3.7652764 -4.6367035 -5.1991897 -5.3298159 -5.277185 -4.997242 -3.9437335][-4.924171 -4.3590059 -3.7034423 -2.832665 -2.0566368 -1.1243868 -0.51651 -0.632422 -0.76695919 -2.1031966 -2.9174013 -3.4012952 -4.0526333 -4.6288605 -3.8115664][-4.5506635 -3.5433574 -2.5879064 -1.4406695 -0.52458429 0.69633818 1.276732 1.4218764 1.5363059 0.48351908 -0.4652667 -1.9045877 -2.8345976 -4.0651679 -4.4699163][-3.4672174 -2.4003778 -0.99174118 0.28001833 1.5765681 2.6166253 3.077949 3.3534245 3.7263684 2.5715804 1.2887168 -0.16966343 -1.669445 -3.48182 -3.8821][-3.5408821 -1.7917209 -0.12542105 1.7431331 2.9810739 4.0866513 4.5716615 4.9195819 4.9392495 3.7353988 2.4644275 0.60475588 -1.017765 -3.2096529 -4.0658636][-2.565268 -1.8701987 -0.28181696 1.369391 2.9780679 3.8986363 4.6232953 4.8841958 5.0440946 3.8372388 1.513968 -0.32663965 -1.6646943 -3.5644393 -4.2899356][-2.8775363 -2.0436068 -1.229598 0.10016823 1.2185092 2.5904851 3.2705092 3.313416 3.2672915 1.9207911 0.52436018 -1.5126696 -2.9731998 -4.6624579 -4.7760382][-3.3590403 -2.9232831 -2.0636768 -1.3095946 -0.31464291 0.67194796 1.190846 1.6912951 1.7039981 0.15761518 -1.2335663 -2.6468468 -3.6302962 -4.7127123 -5.3405848][-4.3597136 -4.0599928 -3.79137 -2.8579144 -2.0051503 -1.1771851 -0.95839882 -1.2692709 -1.2531261 -1.6038442 -2.5278049 -3.439271 -4.1927853 -5.5544109 -5.479177][-5.3955932 -5.2182803 -4.7777691 -4.5572495 -4.0078058 -3.2735491 -2.7358685 -2.8131938 -2.852993 -3.4659142 -4.228478 -4.3764162 -4.9744749 -6.0202789 -6.116632][-6.046989 -5.930388 -5.8937297 -5.202127 -4.3743706 -4.1730623 -3.9373734 -3.8573723 -3.7719965 -3.8076806 -4.2745957 -4.7512174 -5.2687178 -6.0996876 -6.2007709][-5.96968 -6.1901183 -6.3305669 -6.08179 -5.4737444 -4.5885496 -4.0621772 -4.0645409 -4.0537882 -4.0785627 -4.6424627 -4.8891091 -5.668117 -6.7459416 -7.1081681][-7.752583 -7.1933675 -7.1782508 -6.8856673 -6.6819172 -5.8747554 -5.2275286 -4.8911791 -4.551259 -4.7072134 -5.0104647 -5.5792723 -6.1574883 -6.9531913 -7.4986897]]...]
INFO - root - 2017-12-15 11:58:18.225693: step 9510, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 57h:47m:41s remains)
INFO - root - 2017-12-15 11:58:24.611023: step 9520, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 57h:42m:42s remains)
INFO - root - 2017-12-15 11:58:31.046790: step 9530, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 58h:00m:28s remains)
INFO - root - 2017-12-15 11:58:37.493814: step 9540, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 56h:36m:11s remains)
INFO - root - 2017-12-15 11:58:43.998034: step 9550, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 57h:35m:14s remains)
INFO - root - 2017-12-15 11:58:50.357684: step 9560, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 57h:49m:08s remains)
INFO - root - 2017-12-15 11:58:56.751220: step 9570, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 57h:03m:59s remains)
INFO - root - 2017-12-15 11:59:03.220867: step 9580, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 59h:14m:00s remains)
INFO - root - 2017-12-15 11:59:09.706419: step 9590, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 56h:05m:51s remains)
INFO - root - 2017-12-15 11:59:16.202283: step 9600, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 58h:24m:56s remains)
2017-12-15 11:59:16.678682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3235855 -1.2560406 -1.1088123 -1.2471466 -1.7100339 -1.8821664 -2.4024014 -2.5865946 -2.8762975 -4.44089 -5.0954685 -6.7115278 -7.2883353 -7.9413252 -7.9670057][-1.4267459 -1.5604877 -1.79706 -1.815927 -2.4100952 -2.5014486 -2.6766539 -3.0310411 -3.5754962 -5.1392288 -5.5529118 -7.0950909 -7.7711935 -8.4049778 -8.2659149][-2.0223241 -1.9078188 -1.6864305 -1.7595415 -1.9775152 -2.3039789 -2.7040982 -2.9627013 -3.282268 -5.2502866 -5.7260246 -6.9090681 -7.6262622 -8.5098629 -8.4345179][-2.5289812 -2.3991151 -2.349422 -2.2495651 -2.0707612 -1.9735885 -2.0945563 -2.5605297 -2.8565803 -4.5793037 -5.1386237 -6.5669971 -6.9793959 -7.8228064 -8.077137][-2.9131269 -2.829525 -2.5158734 -2.1710114 -1.7638474 -1.3361578 -1.5466843 -2.0398188 -2.4438314 -4.2321897 -4.5278697 -5.9515929 -6.7435913 -7.2723351 -7.1959076][-3.6100445 -3.1694307 -2.7509155 -2.1936889 -1.3102393 -0.56953 -0.50813627 -0.89006138 -1.1923327 -3.1274118 -3.5378218 -5.1305513 -5.8806753 -6.7556386 -6.7884693][-4.0301294 -3.5136571 -3.1017289 -2.5340767 -1.7353363 -0.52718115 -0.11355639 -0.19832468 -0.25697517 -1.8276658 -2.3698974 -4.0878439 -4.7705054 -5.9973121 -6.3665876][-3.9860275 -3.6494517 -3.2111092 -2.3975816 -1.5074224 -0.52616835 0.049767971 0.52980614 0.75125408 -0.65218878 -0.81078291 -2.5749979 -3.9731505 -5.5628257 -5.9312158][-4.024601 -3.7923055 -3.7516115 -2.868566 -1.6065369 -0.62613249 0.17529726 0.83735561 1.3851957 -0.0017552376 -0.19994783 -1.8724213 -3.3990011 -5.0411024 -5.6634288][-4.376936 -3.9043767 -3.8244209 -3.1796303 -2.3574491 -1.2334423 -0.32081604 0.075170517 0.78469372 -0.072244644 -0.12713814 -2.0132709 -3.2454181 -4.6825461 -5.8618183][-5.3306246 -4.9393072 -4.7440109 -3.9986165 -3.252656 -2.3883033 -1.7189164 -1.1472921 -0.39506865 -1.4401393 -1.4261117 -2.535852 -3.9480968 -5.3712831 -5.7937436][-5.8844147 -5.4387503 -5.1389589 -4.3857541 -3.5745826 -2.7246776 -2.27676 -1.9823565 -1.5509825 -2.4579945 -2.4484353 -3.4326324 -4.3091736 -5.2417822 -5.9664283][-6.4098654 -6.2900333 -5.7273974 -5.0733013 -4.6210089 -4.1205812 -3.606863 -3.7947471 -3.6239038 -3.9937267 -3.8914988 -4.3164 -4.6827755 -5.19201 -5.4328194][-6.8808646 -6.4815731 -6.1444507 -5.4813924 -4.7960854 -4.4560289 -4.5961781 -4.5660214 -4.443202 -4.933063 -4.82337 -4.508028 -4.4642382 -4.9586134 -5.1223655][-7.5523748 -7.2726631 -6.7995992 -6.4084091 -6.1792226 -6.0091414 -5.9013929 -6.0092068 -5.9336863 -6.0279312 -5.9844732 -5.6353807 -5.3169451 -5.4049339 -5.5674715]]...]
INFO - root - 2017-12-15 11:59:23.136830: step 9610, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 59h:21m:36s remains)
INFO - root - 2017-12-15 11:59:29.584601: step 9620, loss = 0.38, batch loss = 0.27 (12.2 examples/sec; 0.658 sec/batch; 59h:00m:21s remains)
INFO - root - 2017-12-15 11:59:35.973876: step 9630, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 56h:53m:39s remains)
INFO - root - 2017-12-15 11:59:42.347023: step 9640, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 57h:03m:27s remains)
INFO - root - 2017-12-15 11:59:48.712449: step 9650, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 57h:38m:31s remains)
INFO - root - 2017-12-15 11:59:55.103502: step 9660, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 59h:56m:58s remains)
INFO - root - 2017-12-15 12:00:01.493165: step 9670, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 56h:55m:39s remains)
INFO - root - 2017-12-15 12:00:07.939912: step 9680, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 58h:08m:03s remains)
INFO - root - 2017-12-15 12:00:14.324485: step 9690, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 57h:05m:26s remains)
INFO - root - 2017-12-15 12:00:20.677744: step 9700, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 56h:46m:17s remains)
2017-12-15 12:00:21.153279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4622331 -6.4380293 -6.8793855 -7.0136957 -6.474793 -5.47794 -4.384809 -3.3962908 -2.3455677 -2.2012219 -2.9612465 -4.2857637 -5.2105055 -6.0279784 -6.8869267][-5.9564838 -6.9978495 -7.7929649 -8.02081 -7.4505482 -6.6256952 -5.6824694 -5.0544529 -4.3636827 -4.1625514 -4.8728857 -5.9063311 -6.9821672 -7.3957086 -7.8618035][-7.2026463 -7.0706367 -6.4645553 -6.7214813 -6.5067439 -6.2065396 -5.5807109 -5.34249 -4.865325 -4.7426872 -5.497303 -6.6303596 -7.0867124 -7.1484756 -7.684617][-6.3198934 -6.1786242 -5.8961773 -5.2143297 -4.8465014 -4.1598883 -3.4498448 -3.3872476 -3.4109201 -3.4388776 -4.5216618 -6.2107611 -6.85671 -7.4515066 -7.4293575][-6.2348156 -5.1896868 -3.5427265 -2.4463577 -1.8190889 -1.5960131 -1.5725012 -1.8206458 -1.7457681 -1.8725929 -2.8613186 -4.5673823 -5.5433083 -6.4864144 -7.0824838][-4.5639434 -3.4600964 -1.9153619 -0.92411327 -0.44974661 0.04076004 0.22854757 0.13651037 0.21612501 -0.076214314 -1.0297689 -2.8371186 -3.9341712 -4.9011135 -5.5616093][-4.2234349 -2.8050804 -1.322053 0.0060300827 0.8670454 0.96075058 1.1488857 1.2086954 1.2282715 0.67015266 -0.42557669 -2.1432624 -3.308372 -4.3931856 -5.1059742][-3.5026035 -2.4528923 -1.0726686 0.361598 1.00313 1.5809717 1.7502718 1.6194258 1.7716398 1.1879711 -0.29999876 -2.1815257 -3.3749027 -4.1927109 -4.8581791][-3.7066019 -2.58117 -1.390275 -0.11023235 0.9826498 1.1342316 1.0768461 1.2471371 1.2590179 0.97767067 -0.2116127 -2.066999 -3.0165749 -3.6549244 -4.1176176][-4.30455 -3.1082988 -1.9993029 -0.9720602 -0.4301548 -0.1892724 -0.04429245 0.053585529 0.39752102 -0.3676753 -1.9938664 -2.9495931 -3.6408381 -4.4453583 -4.7602806][-5.9896879 -5.0852866 -4.1067948 -3.4863939 -2.7572594 -2.4567556 -2.1988502 -2.0395308 -1.9825101 -2.6165366 -3.3802185 -4.3007946 -4.9692011 -5.3411441 -5.5023861][-6.6892595 -5.9947896 -5.3133936 -4.8880844 -4.7650108 -4.4800496 -4.0245008 -3.7426803 -3.6122031 -4.11094 -4.9160361 -5.31406 -5.4409394 -5.4984522 -5.2566862][-6.56411 -6.5411024 -6.4409 -6.2862725 -6.0879211 -5.6668425 -5.4985628 -5.56931 -5.1263847 -4.717433 -5.10916 -5.2726049 -5.3652153 -5.2020435 -5.05182][-5.5155358 -5.5881786 -5.6091652 -6.0168962 -6.2129126 -6.0845823 -5.8685112 -5.7692881 -5.6911163 -5.5709238 -5.776618 -5.7057714 -5.2482147 -4.9450955 -4.91272][-5.245008 -4.8605595 -5.3252735 -5.6703014 -6.1409941 -6.4281354 -6.3993287 -6.44804 -6.3437748 -6.2487512 -6.0359893 -5.8340979 -5.5630221 -5.8098583 -5.74312]]...]
INFO - root - 2017-12-15 12:00:27.560935: step 9710, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 57h:48m:12s remains)
INFO - root - 2017-12-15 12:00:34.064167: step 9720, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 57h:55m:37s remains)
INFO - root - 2017-12-15 12:00:40.490460: step 9730, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 58h:52m:21s remains)
INFO - root - 2017-12-15 12:00:46.905504: step 9740, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 58h:47m:34s remains)
INFO - root - 2017-12-15 12:00:53.434297: step 9750, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 57h:39m:34s remains)
INFO - root - 2017-12-15 12:00:59.881409: step 9760, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:31m:05s remains)
INFO - root - 2017-12-15 12:01:06.221549: step 9770, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 56h:56m:50s remains)
INFO - root - 2017-12-15 12:01:12.754115: step 9780, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 57h:58m:03s remains)
INFO - root - 2017-12-15 12:01:19.115018: step 9790, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 58h:16m:12s remains)
INFO - root - 2017-12-15 12:01:25.564952: step 9800, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 56h:38m:23s remains)
2017-12-15 12:01:26.062459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4680533 -4.4618893 -4.4946432 -4.3402281 -3.7415485 -2.771462 -1.5239215 -0.36461544 0.40822506 -0.75914335 -1.6488643 -2.7629066 -3.4402308 -4.9505415 -6.4610195][-4.2968273 -4.52061 -4.7519994 -4.8334551 -4.7232594 -4.2216854 -3.290174 -1.9846668 -0.59533596 -1.3127894 -1.7281089 -2.4659686 -3.2027078 -4.6434288 -5.6712122][-4.9806442 -4.6972222 -5.2690134 -5.6771841 -5.923583 -5.6242361 -4.8138304 -3.9016597 -2.8226018 -3.3455205 -3.0881319 -3.3594298 -3.6849055 -4.3220468 -5.2770686][-5.2031326 -5.3810396 -5.692421 -5.5479131 -5.5827341 -5.4169741 -4.72091 -3.9580133 -3.2069826 -4.10851 -4.1966081 -4.4316282 -4.4553947 -5.0782824 -5.6719728][-6.0618777 -4.7244992 -4.1726503 -4.1188068 -4.1104026 -3.56434 -2.8437295 -2.3704 -1.7672725 -2.9520364 -3.4791007 -4.28738 -4.9761314 -5.964509 -6.6465054][-5.2685022 -4.0940065 -3.25839 -2.131959 -1.5891428 -0.852293 -0.16407299 0.011514664 0.35289 -1.2135477 -2.165278 -3.5296845 -4.5183039 -5.7936678 -6.9703093][-4.1601267 -2.7526383 -1.2850466 -0.24015999 0.85645008 2.3243523 3.269392 3.3498821 3.2514439 1.0692682 -0.47427845 -2.3429074 -3.5608611 -5.0499849 -6.2265763][-2.9806342 -1.6372519 -0.70418358 0.85002232 2.1610956 3.3231583 4.2761784 5.0353918 5.1747551 2.6829605 0.85577679 -1.0946994 -2.7312078 -4.5111337 -5.5869389][-2.951066 -1.5735264 -0.37053585 0.9780674 1.7324991 2.8643713 3.6535521 4.1188393 4.5388994 2.5715437 0.76326275 -1.4227138 -3.1865606 -4.6813583 -5.4412746][-3.64958 -2.307097 -1.2207384 0.18590117 1.1845322 2.2280502 2.7086029 3.0794716 2.976985 0.8522644 -0.45451212 -1.8587999 -3.4030533 -4.6450782 -5.3300133][-4.9959245 -3.842886 -3.1235843 -2.237885 -1.4494014 -0.30580139 0.32059097 0.18794727 -0.26360321 -1.9280233 -2.7592044 -3.8298385 -4.8767443 -5.5700579 -5.8125048][-6.7497525 -5.5688133 -4.6430259 -3.9546111 -3.5372539 -3.1171837 -2.7603078 -2.5101 -2.3110971 -3.7328823 -4.703527 -4.9452534 -5.4806447 -6.3775845 -6.4813008][-8.0590878 -7.7487612 -7.12607 -6.2994161 -5.7451658 -5.2467766 -4.6788435 -4.7191515 -4.750258 -5.3409519 -5.4528952 -5.41028 -5.8343825 -6.1758642 -6.17353][-8.3312187 -8.47646 -8.23901 -7.6734495 -7.1769614 -6.5712886 -6.0569372 -5.788991 -5.8649597 -6.087882 -6.142827 -5.516716 -5.0231624 -5.1710982 -5.4508648][-7.8816967 -7.6525655 -7.2982669 -7.3674178 -7.1629639 -6.6658149 -6.3901258 -6.4407706 -6.5620632 -6.2503505 -6.3153152 -6.2240577 -5.9011636 -5.5059843 -5.3970194]]...]
INFO - root - 2017-12-15 12:01:32.472457: step 9810, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 59h:36m:48s remains)
INFO - root - 2017-12-15 12:01:38.801197: step 9820, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 56h:29m:19s remains)
INFO - root - 2017-12-15 12:01:45.301331: step 9830, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 58h:31m:54s remains)
INFO - root - 2017-12-15 12:01:51.701865: step 9840, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 58h:07m:57s remains)
INFO - root - 2017-12-15 12:01:58.157461: step 9850, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.639 sec/batch; 57h:17m:23s remains)
INFO - root - 2017-12-15 12:02:04.541662: step 9860, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 57h:58m:21s remains)
INFO - root - 2017-12-15 12:02:10.933317: step 9870, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 55h:58m:56s remains)
INFO - root - 2017-12-15 12:02:17.382188: step 9880, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 56h:59m:39s remains)
INFO - root - 2017-12-15 12:02:23.828472: step 9890, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 57h:06m:53s remains)
INFO - root - 2017-12-15 12:02:30.185625: step 9900, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:19m:54s remains)
2017-12-15 12:02:30.735516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0459046 -3.5588856 -4.3214941 -5.1007595 -5.0951819 -4.6012845 -4.47243 -4.3206739 -4.3100538 -4.8188276 -5.3029528 -5.9280162 -5.410018 -4.3990932 -2.8835955][-0.3080101 -1.4323931 -2.4125776 -3.1268191 -3.9188907 -3.3837895 -3.2056298 -2.9590211 -3.1054754 -3.5094576 -3.6846325 -4.7268262 -4.527081 -4.1705952 -3.4354229][-0.21409893 -0.71563005 -1.5662093 -2.0151248 -2.4691586 -2.4991388 -2.782474 -2.7279606 -2.945621 -3.4132762 -3.5952673 -6.1767216 -7.3129845 -7.5123167 -7.8393273][-1.4444656 -2.3597512 -2.98632 -2.6604939 -2.1873355 -1.3736348 -1.1787739 -1.3578358 -2.0516491 -3.2254705 -3.6992295 -6.0729914 -6.8567328 -7.26836 -7.8884506][-2.5763221 -2.6591225 -3.6247277 -2.7132273 -1.6362524 -1.2460756 -0.80377674 -0.49302006 -0.76519156 -1.2337413 -1.2682719 -2.8086796 -4.2025042 -5.3294621 -5.8584146][-3.9190769 -3.1354423 -2.8061523 -2.4372807 -1.1999364 0.057390213 1.328649 1.4185395 1.2545695 1.504643 1.9291844 0.6330533 0.0795455 -1.8246393 -4.1302743][-2.9064684 -2.5034595 -1.8906808 -0.83257484 0.58107042 2.8550305 5.0362468 5.02543 4.4254727 4.3134713 4.3660588 2.1167703 1.1576819 -0.4530549 -2.6434031][-2.334497 -2.0378151 -1.1850715 0.45266867 2.4051433 4.01586 5.5909019 6.6110997 7.4671807 6.5474887 5.8132281 3.8223796 2.2816157 0.57629538 -1.4357505][-2.5253472 -2.550035 -1.8577681 -0.1445632 1.685967 3.0923266 4.0082393 5.078218 5.9756074 5.9696193 5.7954965 2.2738376 -0.12283611 -1.9243464 -4.0991421][-4.4529505 -4.3816004 -4.0085611 -2.3675075 -0.79060936 0.9035821 2.7149453 3.1100535 2.8469748 2.5184751 2.7588439 -0.32941961 -3.3834195 -4.9144154 -5.838984][-6.65495 -6.7372737 -6.9608536 -6.0311365 -5.0278859 -3.456687 -1.4147892 -0.56188774 0.47318888 -0.29799223 -1.0243893 -2.9311686 -5.5106621 -6.5972443 -7.6828637][-7.0649576 -6.959784 -7.37154 -7.2682419 -6.7048597 -5.6269388 -4.9104671 -4.2324486 -2.8996453 -3.2636914 -3.1275396 -3.8934209 -5.3118267 -6.2039 -7.4230537][-8.3808947 -8.5911331 -9.1133413 -8.8133011 -8.7544441 -8.4028254 -7.9515514 -7.4950538 -7.1041703 -7.0272942 -6.091692 -6.1202602 -6.8067393 -6.8123722 -7.0530691][-7.3935175 -7.251133 -8.0262079 -8.01595 -7.9400616 -7.631918 -7.4052873 -7.6416864 -7.2402205 -7.4164095 -7.2586474 -6.9217997 -6.77974 -5.657536 -5.0460644][-8.4693623 -8.2885313 -8.8545036 -8.4271259 -7.729856 -7.5147848 -7.3790922 -7.586689 -7.3982768 -7.4243364 -7.4214687 -6.9309874 -6.3686185 -6.146791 -5.8054671]]...]
INFO - root - 2017-12-15 12:02:37.014615: step 9910, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 56h:10m:35s remains)
INFO - root - 2017-12-15 12:02:43.526249: step 9920, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 58h:32m:01s remains)
INFO - root - 2017-12-15 12:02:49.915253: step 9930, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 56h:00m:54s remains)
INFO - root - 2017-12-15 12:02:56.348385: step 9940, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 57h:50m:21s remains)
INFO - root - 2017-12-15 12:03:02.715883: step 9950, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 58h:16m:42s remains)
INFO - root - 2017-12-15 12:03:09.145334: step 9960, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 56h:17m:59s remains)
INFO - root - 2017-12-15 12:03:15.640266: step 9970, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 56h:46m:46s remains)
INFO - root - 2017-12-15 12:03:22.033343: step 9980, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 57h:53m:22s remains)
INFO - root - 2017-12-15 12:03:28.473050: step 9990, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.619 sec/batch; 55h:29m:15s remains)
INFO - root - 2017-12-15 12:03:34.904155: step 10000, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 57h:39m:25s remains)
2017-12-15 12:03:35.407628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1897798 -2.1318073 -3.0828991 -3.4646392 -3.3264713 -3.1074419 -2.7530956 -2.0425806 -1.4472475 -2.1659894 -2.4005756 -4.4575748 -5.6364512 -6.2556891 -7.2554832][-1.9231672 -2.4580216 -3.5331783 -4.5674562 -4.9471321 -4.1060295 -2.9100285 -2.3555412 -2.1444917 -3.26151 -3.6702862 -5.7731051 -7.0183153 -7.5849013 -8.243577][-2.8736672 -3.1716876 -4.0689058 -4.45005 -4.7786913 -4.7898436 -3.9365389 -3.1088958 -2.4977698 -4.0121565 -4.9879146 -6.7805204 -7.5465031 -8.2185707 -8.9614305][-3.551764 -3.9102943 -4.3865976 -4.2933764 -3.9209752 -3.0277834 -1.8095284 -1.3411226 -1.2390699 -2.8726749 -3.89564 -6.3515639 -7.6791658 -8.1413517 -8.5084095][-4.6084347 -4.603281 -5.0759521 -4.6199083 -4.0310388 -2.4067779 -0.33001328 0.55042934 0.6784687 -1.3161788 -3.0629678 -5.7607222 -7.2748003 -8.2064867 -8.65484][-6.2874017 -6.0935869 -5.8822074 -4.5922346 -3.1273766 -1.0797973 1.3769407 2.5215826 2.7295828 0.2063961 -2.2047105 -5.3473878 -7.3853936 -8.2308969 -8.8909273][-5.8721857 -5.874537 -5.9631429 -4.3205442 -1.7546244 1.1783819 4.1864624 5.643034 6.3607521 3.8399 0.93748093 -3.5350709 -6.2879128 -7.4463344 -8.0623446][-5.15167 -4.7941761 -4.4004669 -3.1582522 -1.4747062 2.0243254 5.3821697 6.7361031 7.349679 5.2559147 2.6402283 -1.7198758 -4.9423923 -6.7583685 -7.4828925][-5.5958967 -4.6119475 -4.4046755 -3.2340341 -1.7818227 0.52274323 3.2230368 5.0873337 6.2147837 4.1054544 1.7636404 -1.8598475 -4.4100552 -6.2141304 -7.4299717][-5.7648249 -5.07889 -4.5830445 -3.5132675 -2.6583681 -1.387012 0.46259689 1.8833265 2.9695616 0.93579578 -0.65063667 -3.6739888 -5.3006363 -6.1280432 -7.1463642][-6.5251532 -6.1452041 -6.0392213 -4.9666243 -4.2171 -3.4811859 -2.9022942 -2.2788739 -1.2105951 -2.3689084 -3.4110126 -5.7616258 -6.5168552 -6.5997906 -6.6096444][-6.77073 -6.5631013 -6.5463867 -5.7909641 -5.3918343 -4.7732134 -4.1356297 -4.276432 -3.7260273 -4.0312161 -4.3394718 -5.8243895 -6.4080215 -7.2229648 -7.49768][-7.9731784 -7.441946 -7.1608539 -6.534668 -5.9737186 -5.7431564 -5.5081005 -5.4152308 -5.1908412 -6.1268606 -5.8664331 -6.0472946 -6.5619783 -7.3184366 -7.5619068][-7.5647364 -7.3724704 -6.9302459 -6.1822309 -5.6823683 -5.0618477 -4.5375805 -5.0807071 -5.4314814 -6.0742178 -5.9716306 -5.9354153 -6.107666 -6.4447479 -6.6128864][-8.1505842 -8.4010124 -8.5150423 -7.5948558 -6.7337108 -6.4053669 -6.2437553 -6.3621073 -6.46644 -6.784811 -6.9234614 -6.6727762 -6.2256804 -5.9001808 -5.8080282]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:03:42.951298: step 10010, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 58h:29m:30s remains)
INFO - root - 2017-12-15 12:03:49.401889: step 10020, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 57h:54m:33s remains)
INFO - root - 2017-12-15 12:03:55.791362: step 10030, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:30m:24s remains)
INFO - root - 2017-12-15 12:04:02.235011: step 10040, loss = 0.39, batch loss = 0.27 (12.5 examples/sec; 0.640 sec/batch; 57h:22m:02s remains)
INFO - root - 2017-12-15 12:04:08.646193: step 10050, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 57h:39m:07s remains)
INFO - root - 2017-12-15 12:04:15.084345: step 10060, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:28m:39s remains)
INFO - root - 2017-12-15 12:04:21.437843: step 10070, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 57h:28m:33s remains)
INFO - root - 2017-12-15 12:04:27.820162: step 10080, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:20m:58s remains)
INFO - root - 2017-12-15 12:04:34.267823: step 10090, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 58h:52m:13s remains)
INFO - root - 2017-12-15 12:04:40.682479: step 10100, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 57h:12m:08s remains)
2017-12-15 12:04:41.203833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5795665 -4.2443824 -4.5059624 -4.2982912 -3.8533797 -3.6425972 -2.8540654 -2.0428405 -1.0156307 -1.8597946 -2.4600658 -4.5711627 -5.3469715 -6.51968 -6.864027][-3.6161542 -4.0915775 -4.3954806 -4.7485609 -4.4858723 -3.7249358 -2.9682417 -2.0871353 -1.6824408 -2.1394405 -2.8033981 -5.446063 -6.3442535 -7.5136929 -7.373394][-3.8048093 -3.7925773 -3.7964211 -3.790765 -3.7286031 -3.1581078 -2.106369 -1.6302924 -1.499958 -2.0471821 -2.3966722 -4.7685885 -5.5597782 -6.9477558 -7.0620852][-3.9095998 -4.2585564 -4.194447 -3.5381861 -2.7829909 -1.7673407 -0.98550272 -0.31698561 -0.17621326 -1.3016644 -2.5405583 -4.7678766 -5.4780631 -6.8019629 -7.6056347][-4.7660804 -4.7150717 -4.7376218 -4.1256371 -2.7087107 -1.2924194 -0.026540756 0.59680748 0.50668907 -0.79729843 -2.453763 -4.8731222 -5.5705161 -6.3638945 -6.4941206][-6.1660976 -5.9035606 -5.0307159 -3.8341174 -2.2956276 0.00098562241 1.7217855 2.1419554 1.8201637 -0.082772732 -1.6403909 -4.3518963 -5.6030588 -6.4359403 -6.6110568][-6.6113639 -5.7047338 -5.1066923 -3.8876126 -1.7495108 0.8546133 3.1289711 3.9641705 3.6517038 1.616169 -0.55249739 -3.9866455 -5.8180861 -6.7451429 -6.6750431][-6.4752283 -5.6669097 -4.5333176 -2.9814167 -1.4326243 1.0267391 3.1585655 3.7394543 3.4075222 1.5642805 -0.17220354 -3.0005951 -4.5238152 -5.9557939 -6.407815][-4.7097039 -4.1237316 -3.5922823 -2.1626506 -0.59664774 1.4462957 3.2260981 3.9938679 3.5235243 1.7141829 -0.010756493 -2.9093199 -4.26604 -5.1706581 -5.3609838][-3.3885493 -2.7584205 -2.4727788 -1.7276511 -0.88470554 0.639698 1.9411221 2.2860336 2.2644186 0.43479633 -1.0393386 -3.7888112 -4.9510937 -5.7365975 -5.6928177][-3.09374 -2.4910388 -2.0174975 -1.5910864 -0.92067909 0.0651803 0.52706337 0.22274971 -0.32784271 -1.9757557 -3.2529182 -5.2191963 -5.8523159 -6.3805656 -5.7293124][-2.5566187 -2.3023744 -1.7779741 -0.92543983 -0.68312073 -0.26355886 -0.45248652 -1.4537272 -2.3423891 -3.0950913 -3.8789074 -5.7960653 -6.6101241 -6.8894334 -6.4121246][-3.4675407 -2.6476355 -1.945632 -1.2509503 -1.0508208 -0.79361963 -1.5221486 -2.593194 -3.5089312 -4.560853 -4.6503577 -5.6230812 -6.0752506 -7.050602 -7.0410337][-4.7103057 -3.4293332 -2.3614554 -1.5148268 -1.2426009 -1.1580801 -1.3922539 -2.2622175 -3.1418891 -4.6147728 -4.6798487 -4.99606 -5.0993061 -5.9884429 -6.0558214][-5.6699605 -4.6404419 -3.3953304 -2.2227106 -2.0573688 -2.3133469 -3.0813618 -3.3259215 -3.5517349 -4.2780981 -4.6105824 -5.4801297 -5.8998938 -6.1369123 -5.857173]]...]
INFO - root - 2017-12-15 12:04:47.576842: step 10110, loss = 0.38, batch loss = 0.27 (12.7 examples/sec; 0.630 sec/batch; 56h:27m:00s remains)
INFO - root - 2017-12-15 12:04:53.954657: step 10120, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 58h:48m:09s remains)
INFO - root - 2017-12-15 12:05:00.408988: step 10130, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 59h:01m:35s remains)
INFO - root - 2017-12-15 12:05:06.781999: step 10140, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 56h:49m:20s remains)
INFO - root - 2017-12-15 12:05:13.155105: step 10150, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 55h:49m:35s remains)
INFO - root - 2017-12-15 12:05:19.717762: step 10160, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 56h:55m:06s remains)
INFO - root - 2017-12-15 12:05:26.110535: step 10170, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 58h:44m:46s remains)
INFO - root - 2017-12-15 12:05:32.529990: step 10180, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 57h:42m:09s remains)
INFO - root - 2017-12-15 12:05:38.987785: step 10190, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 57h:38m:51s remains)
INFO - root - 2017-12-15 12:05:45.362937: step 10200, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 56h:36m:24s remains)
2017-12-15 12:05:45.865436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5741096 -4.757144 -3.9089773 -3.5296865 -3.2546234 -2.6929116 -2.7022686 -2.7809944 -2.578464 -3.8143945 -4.7875519 -5.6530385 -6.7296219 -7.4799795 -7.8294306][-4.7187967 -4.1553164 -3.5337405 -3.073719 -2.7045097 -2.5986533 -2.6724367 -2.8465948 -2.9298925 -4.3356972 -5.2692642 -5.8181472 -6.5942335 -7.1880918 -7.3988981][-3.9311259 -3.3666744 -3.0130472 -2.9309602 -2.8758302 -2.5590944 -2.3828731 -2.3884907 -2.3961477 -3.9331334 -5.0348158 -5.5692058 -6.4112859 -7.0212116 -7.1997252][-3.3681927 -3.0780602 -2.8478699 -2.3573575 -2.1037178 -2.0015521 -1.7513661 -1.7650852 -1.7693906 -3.2117219 -4.2194176 -4.9725695 -5.9330397 -6.5137811 -6.7658906][-3.5648527 -2.7880802 -1.8111572 -1.5361009 -1.4289408 -1.1090822 -0.76385736 -0.8399291 -1.0111895 -2.6331897 -3.6267333 -4.3351917 -5.4136877 -6.2457018 -6.2490435][-3.2334061 -2.895555 -2.5511456 -1.7012224 -0.64377356 -0.070729733 0.13359356 0.079496861 0.070991516 -1.669271 -2.9337668 -3.8894055 -5.0702124 -5.7788448 -6.1289182][-3.3171682 -2.962924 -2.2347851 -1.4523611 -0.57989931 0.33390474 0.98907042 1.1791644 1.1868844 -0.59530973 -1.9442506 -3.1058273 -4.4795327 -5.4291515 -5.8729835][-2.6571908 -2.1008549 -1.8888955 -1.0790696 0.20784855 0.96598196 1.2688422 1.7091508 2.1366343 0.42147589 -1.0356808 -2.4715939 -4.1287537 -5.3571415 -5.8369823][-2.9092073 -2.6146154 -1.3835726 -0.21022844 0.43126822 1.0046496 1.34586 1.5806565 1.8699594 0.1172657 -1.3282909 -2.6714492 -4.2777019 -5.5032392 -6.114574][-3.1174908 -2.3897676 -1.7628574 -0.68293476 0.52170038 1.1405253 1.1465917 1.1950984 1.2514405 -0.65907526 -2.0524688 -3.6322284 -5.1291766 -6.1448832 -6.58178][-3.8262463 -3.191227 -2.5875931 -1.5132747 -0.82915211 -0.44598722 -0.18287897 -0.15963697 -0.20198488 -2.0511565 -3.6146717 -4.8981094 -6.1953077 -7.007123 -7.1842546][-4.2444677 -3.5125856 -2.7561398 -2.0114422 -1.3789182 -0.86539936 -0.89544106 -1.0914149 -1.3800077 -2.9493871 -4.1483006 -5.1902285 -6.5707955 -7.2300553 -7.4277735][-4.810236 -4.6132722 -4.2518744 -3.483912 -2.9088879 -2.5543332 -2.3850079 -2.5270538 -2.8772254 -3.9095204 -4.949296 -5.6035709 -6.3273611 -6.6659226 -6.7187643][-4.9726782 -4.5456972 -4.3923774 -4.1961288 -4.1080494 -3.9298532 -3.7346592 -3.8196442 -3.9460251 -4.5287147 -4.9245238 -5.1946087 -5.7841907 -6.1229162 -5.9496865][-5.7700472 -5.8346181 -5.4739237 -5.2963877 -5.3836288 -5.2425184 -5.3225269 -5.2600546 -5.5150256 -5.4836531 -5.329164 -5.5784211 -5.5683966 -5.5658541 -5.4292846]]...]
INFO - root - 2017-12-15 12:05:52.253945: step 10210, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 58h:14m:54s remains)
INFO - root - 2017-12-15 12:05:58.708709: step 10220, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 57h:28m:45s remains)
INFO - root - 2017-12-15 12:06:05.066513: step 10230, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 57h:31m:22s remains)
INFO - root - 2017-12-15 12:06:11.432558: step 10240, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 55h:57m:13s remains)
INFO - root - 2017-12-15 12:06:17.902143: step 10250, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 55h:25m:31s remains)
INFO - root - 2017-12-15 12:06:24.347382: step 10260, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 56h:38m:56s remains)
INFO - root - 2017-12-15 12:06:30.729156: step 10270, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 57h:27m:56s remains)
INFO - root - 2017-12-15 12:06:37.112210: step 10280, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 58h:51m:18s remains)
INFO - root - 2017-12-15 12:06:43.558540: step 10290, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 55h:56m:00s remains)
INFO - root - 2017-12-15 12:06:49.949715: step 10300, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 58h:13m:48s remains)
2017-12-15 12:06:50.436328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9423983 -4.6037474 -5.06763 -5.4496374 -5.9021983 -5.9002028 -5.2322121 -5.0856161 -4.6489592 -4.5749683 -5.9576941 -7.7894659 -9.4969826 -10.537102 -11.007336][-2.7946224 -3.9351118 -5.5544305 -6.6103249 -7.3770866 -7.5667462 -7.1826267 -6.907083 -5.8151445 -5.3121977 -6.3268719 -7.3827667 -8.9627132 -10.256307 -11.031876][-3.1703129 -3.4925203 -4.0292006 -5.3024912 -6.5111279 -7.22841 -7.3941965 -6.6726336 -5.47666 -4.9897165 -5.3905973 -6.6490793 -8.4407282 -9.3415527 -10.20804][-4.415349 -3.7212481 -3.64042 -4.0601883 -4.3913612 -4.6194963 -4.7014141 -4.9851532 -4.3458672 -3.8081813 -4.6655407 -5.6007557 -7.4899421 -8.5743608 -9.4603214][-4.2825017 -3.8022008 -3.0055904 -3.1755004 -3.1981335 -2.4416976 -1.6378264 -1.3599253 -1.1023626 -2.0293403 -3.8890128 -5.4229207 -7.2546363 -8.5577335 -9.2929659][-3.6612749 -3.4783669 -3.1390238 -2.696559 -1.8262858 -0.900692 0.18387938 0.83403492 1.1736574 0.087050915 -2.2736144 -4.4548111 -6.9300537 -7.69381 -8.5474167][-3.776041 -3.5995903 -3.3887472 -2.0918713 -0.45652866 0.63605404 1.9014845 2.8286629 3.2517118 2.2141314 -0.36377287 -3.4730248 -6.3733587 -7.5066104 -8.5145731][-4.2378483 -3.4698062 -2.5198789 -1.561655 0.14809418 1.5405693 3.211031 3.7205076 3.841691 3.0041962 -0.066683292 -2.8265195 -5.1242838 -6.9534416 -8.0604315][-4.3516903 -4.11384 -3.5162511 -2.3312259 -0.76404238 0.81713009 2.395153 2.9355431 3.480732 1.8662806 -1.200273 -3.5732298 -5.9336672 -6.8763514 -7.7055035][-5.4488153 -5.4698677 -4.99753 -3.9936619 -2.7763128 -1.8439655 -0.50452042 0.42529011 0.86157417 -0.74639177 -2.9454985 -4.7711892 -6.5613813 -7.3277264 -7.8949842][-8.2990236 -8.1077719 -7.733923 -6.6746197 -5.4245567 -4.5456734 -2.9499216 -2.6363153 -2.5891032 -3.2957058 -5.2984467 -6.2386651 -7.7420039 -8.2349758 -8.5660181][-8.1297016 -8.3223743 -8.0236721 -7.5882 -6.937623 -6.0177054 -4.9538674 -4.9316616 -4.3727169 -5.035265 -6.3529572 -6.9678321 -7.5443835 -7.9483418 -7.5068774][-9.0656862 -9.4377508 -8.7456427 -8.3328342 -7.542316 -7.0001931 -6.3908858 -6.3829412 -6.3719234 -6.51681 -6.9966521 -6.7499971 -6.9872084 -7.095253 -7.0550814][-9.0574322 -8.9189405 -8.4063807 -8.1287651 -7.7083058 -7.0477109 -6.2402039 -6.3834519 -6.2406683 -6.8237176 -7.0713253 -6.6547909 -6.5381203 -6.5761838 -6.4204388][-9.0012436 -9.2728338 -9.3164339 -9.1801243 -8.8411293 -8.6621675 -7.9815273 -7.6500545 -7.4564672 -7.6394305 -7.7927413 -7.5539222 -7.5491681 -6.9914355 -6.4418173]]...]
INFO - root - 2017-12-15 12:06:56.891729: step 10310, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 58h:38m:00s remains)
INFO - root - 2017-12-15 12:07:03.373639: step 10320, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:15m:31s remains)
INFO - root - 2017-12-15 12:07:09.871188: step 10330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.632 sec/batch; 56h:36m:07s remains)
INFO - root - 2017-12-15 12:07:16.281398: step 10340, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 57h:34m:45s remains)
INFO - root - 2017-12-15 12:07:22.758161: step 10350, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 58h:13m:40s remains)
INFO - root - 2017-12-15 12:07:29.250975: step 10360, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:18m:24s remains)
INFO - root - 2017-12-15 12:07:35.677748: step 10370, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 57h:51m:16s remains)
INFO - root - 2017-12-15 12:07:42.104940: step 10380, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 55h:49m:02s remains)
INFO - root - 2017-12-15 12:07:48.470361: step 10390, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 56h:57m:33s remains)
INFO - root - 2017-12-15 12:07:54.982977: step 10400, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 58h:19m:00s remains)
2017-12-15 12:07:55.477483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9401369 -4.0431986 -4.2938247 -4.8190112 -4.8365412 -5.1974521 -5.457077 -5.7157235 -5.8915339 -6.46278 -7.5121088 -7.65031 -8.0147533 -9.0244951 -8.583333][-3.2643542 -3.1400228 -3.71838 -4.4779296 -5.2302494 -5.8274169 -6.0091171 -5.8213358 -6.0410204 -6.6795211 -7.2183957 -7.3362956 -7.941309 -8.8971252 -8.8427534][-1.8028898 -1.5605707 -2.5806174 -3.3227587 -4.2452617 -4.7843914 -5.2043538 -5.1245809 -4.810019 -5.2721782 -5.9622431 -6.3852363 -6.8163877 -7.74453 -7.97142][-1.5941429 -1.3566966 -1.2356095 -1.8442278 -2.7289786 -3.030859 -3.8437269 -4.2134476 -4.1087379 -4.1462173 -4.4023218 -4.9761477 -5.6999459 -7.0891428 -7.1585941][-2.6755228 -1.9355969 -1.9231796 -1.9840565 -1.9588504 -1.7062249 -1.8583431 -2.0623403 -2.0676908 -2.2886896 -2.9847069 -3.7793305 -4.3061771 -5.6251712 -6.0789576][-2.706573 -2.4413576 -1.8776035 -1.7695208 -1.6102204 -0.55675554 -0.19892216 0.10254908 0.51138306 -0.13589907 -1.0163746 -2.2940993 -3.2615671 -4.9670792 -5.3019323][-2.7107344 -2.1145725 -1.6923065 -1.4068966 -0.8137784 0.43963146 1.2098389 1.3881893 1.8111343 1.1948023 0.35736847 -1.0048184 -2.3193889 -4.4396372 -5.0535936][-1.7032924 -1.8173285 -1.6491761 -1.2115679 -0.37893534 0.76921844 1.7618189 2.1064482 2.2773705 1.5615196 0.70336628 -0.53617096 -2.0106535 -4.4261775 -4.9596281][-1.9559102 -1.4410324 -1.0213227 -0.79738 -0.57221174 0.51079845 1.5324249 1.9302235 2.0626745 1.3150101 0.17953396 -1.2562404 -2.5865602 -4.8027225 -5.2372065][-2.6058478 -1.8049517 -0.47688866 -0.49535751 -0.74345112 -0.02948904 0.9487648 1.3283043 1.2428389 -0.23063707 -1.549005 -2.294168 -3.3683176 -5.2923613 -5.6807818][-3.3288703 -3.2461681 -2.5597014 -2.1003008 -2.7753363 -2.3879905 -1.640234 -1.3735266 -1.2285013 -2.3805833 -3.9253221 -4.3864527 -4.9887638 -6.5618811 -6.9763961][-4.2091866 -4.7481146 -4.904768 -4.138607 -3.9843256 -3.8290887 -3.4815841 -3.2954741 -3.327364 -4.5147476 -5.4822617 -5.5284529 -5.7724948 -6.9236641 -7.29119][-4.8977346 -5.1948423 -5.1789026 -5.3568554 -5.2673135 -5.1928167 -4.9795504 -4.7870855 -4.4168177 -5.171752 -5.8958116 -5.831521 -5.7771859 -5.9628682 -5.9490356][-5.2500763 -5.1154051 -5.4246736 -5.3570271 -5.1342831 -5.5137124 -5.0246639 -4.8666162 -4.6421337 -5.0376558 -5.3765097 -5.4178872 -5.1969538 -4.9838085 -4.9274955][-5.3491116 -5.9024572 -5.759716 -5.7018576 -5.6112142 -5.2473636 -5.3180828 -5.0944595 -4.8200293 -4.746263 -4.8569379 -5.3331127 -5.6723628 -5.4667034 -5.3614931]]...]
INFO - root - 2017-12-15 12:08:01.820421: step 10410, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 55h:43m:52s remains)
INFO - root - 2017-12-15 12:08:08.231419: step 10420, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 56h:11m:55s remains)
INFO - root - 2017-12-15 12:08:14.599636: step 10430, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 55h:38m:18s remains)
INFO - root - 2017-12-15 12:08:21.012303: step 10440, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 56h:36m:36s remains)
INFO - root - 2017-12-15 12:08:27.351323: step 10450, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 56h:47m:57s remains)
INFO - root - 2017-12-15 12:08:33.798198: step 10460, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 58h:56m:22s remains)
INFO - root - 2017-12-15 12:08:40.262609: step 10470, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 56h:49m:24s remains)
INFO - root - 2017-12-15 12:08:46.598061: step 10480, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:20m:27s remains)
INFO - root - 2017-12-15 12:08:52.943207: step 10490, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 56h:43m:02s remains)
INFO - root - 2017-12-15 12:08:59.314019: step 10500, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 58h:02m:45s remains)
2017-12-15 12:08:59.836115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8453484 -5.31665 -3.7844651 -3.7874031 -3.5049887 -3.4233394 -3.5634956 -3.6901929 -3.2612591 -4.5974064 -5.4172359 -5.3291197 -7.0337529 -7.2767053 -7.3990803][-4.8645573 -4.9247694 -4.5856457 -3.7245281 -2.6771126 -2.408092 -2.1875315 -2.2493176 -2.6556349 -3.6572046 -4.4464979 -5.7592707 -7.2486534 -7.3380303 -7.5235991][-4.7302532 -3.3851418 -2.709197 -2.6518002 -1.8111477 -0.96398211 -0.69698858 -0.69852924 -0.5192647 -2.1399531 -3.9912739 -4.442296 -6.0408316 -6.6176777 -6.9034286][-2.882638 -2.5965061 -1.9208784 -0.72544527 0.41150904 1.0177188 1.3942876 1.3770938 1.0432506 -0.74929285 -2.4724665 -3.5170321 -5.3502245 -5.3894639 -5.9257588][-3.745369 -2.0875635 -0.287385 0.22155905 1.45752 2.3637013 2.7416253 2.7565894 2.3325934 -0.11948824 -1.9627342 -2.777513 -4.5723038 -5.0064468 -5.6154776][-2.9035473 -1.7287784 -0.79747868 0.63494062 2.3998675 3.2244906 4.0013537 3.5880942 2.8435788 0.59259558 -1.3688354 -2.3759942 -4.4499254 -4.8186736 -5.1738577][-2.5038047 -1.243011 -0.15601063 1.0285077 2.4187474 3.5566525 4.4384456 4.0644479 3.6607089 1.2395864 -1.1880031 -2.552906 -4.4291143 -4.764617 -5.2529411][-2.4860492 -1.6516309 -0.76274776 0.91085958 2.8785625 3.7714629 4.4538236 4.2448335 3.888391 1.0819879 -1.313899 -2.7065959 -4.713192 -5.0095568 -5.2960181][-2.6128802 -2.1983275 -0.89060545 -0.11696005 1.0896621 2.369206 3.0610881 3.1934218 3.1360335 0.69190454 -1.4202199 -2.5698042 -4.7566452 -5.4837284 -5.7418437][-4.0166292 -3.7135074 -3.0875525 -2.216404 -1.2383966 -0.75093746 -0.033921242 0.31534529 0.077243805 -2.1158242 -3.1832705 -4.1172905 -6.1329293 -6.08394 -6.0180974][-4.8933764 -4.6825881 -4.1299338 -3.7639124 -3.9133222 -3.3841791 -2.6793847 -2.8658338 -3.2708259 -5.0225821 -6.3871765 -6.0713987 -6.8298678 -7.2952137 -7.4320822][-5.75229 -5.2363739 -5.33827 -5.1079216 -3.9602084 -3.8039873 -3.9617531 -4.2106676 -4.625248 -6.0956469 -7.4663396 -7.671433 -8.1791935 -7.7557392 -7.6063657][-5.5030069 -5.6589241 -5.5615163 -5.0552654 -4.9092112 -4.3154888 -3.9277434 -4.3777924 -4.9924259 -5.8909092 -7.0691366 -6.85819 -7.6757188 -7.6022258 -7.2476068][-4.8433933 -4.6190977 -5.0833979 -4.8678856 -4.3099656 -4.2903166 -4.4950666 -4.204987 -3.821342 -4.7473216 -5.2251158 -5.7826004 -6.688271 -6.8125067 -6.7641735][-5.4950542 -4.9019952 -3.877104 -4.5948229 -5.6764078 -5.4384117 -4.6826444 -4.7906227 -5.2936654 -4.7727003 -4.4102631 -5.1429796 -6.0140638 -6.2569656 -6.5903416]]...]
INFO - root - 2017-12-15 12:09:06.265646: step 10510, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 56h:16m:51s remains)
INFO - root - 2017-12-15 12:09:12.753255: step 10520, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.673 sec/batch; 60h:12m:48s remains)
INFO - root - 2017-12-15 12:09:19.161631: step 10530, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 56h:54m:00s remains)
INFO - root - 2017-12-15 12:09:25.561107: step 10540, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 58h:13m:38s remains)
INFO - root - 2017-12-15 12:09:32.065393: step 10550, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 57h:01m:25s remains)
INFO - root - 2017-12-15 12:09:38.536370: step 10560, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 55h:59m:20s remains)
INFO - root - 2017-12-15 12:09:44.886671: step 10570, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 58h:50m:30s remains)
INFO - root - 2017-12-15 12:09:51.214830: step 10580, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 55h:50m:19s remains)
INFO - root - 2017-12-15 12:09:57.628595: step 10590, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 57h:59m:40s remains)
INFO - root - 2017-12-15 12:10:04.144296: step 10600, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 57h:08m:51s remains)
2017-12-15 12:10:04.641056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.695086 -5.1296864 -4.4788675 -4.0852795 -4.0391469 -3.9357657 -3.5351677 -3.1988378 -2.6529388 -2.8546276 -3.0986919 -3.8746593 -4.6846857 -5.2279444 -5.91776][-4.4448438 -4.46393 -4.4477444 -4.4295807 -4.403491 -4.2628632 -3.6391978 -2.9832525 -2.2287083 -2.3903079 -2.8434691 -3.4940777 -4.3238783 -5.1270428 -5.907701][-3.7089934 -3.8469985 -3.9652689 -4.0700064 -4.0421371 -3.7144265 -3.1449075 -2.7246051 -2.2557034 -2.4545283 -2.7922311 -3.5220537 -3.8969464 -4.4962721 -5.4376526][-2.9175844 -2.7878847 -2.809566 -2.4967017 -2.2911477 -2.2461171 -2.026742 -1.7770629 -1.2331028 -1.4383807 -2.1433744 -3.1111908 -3.7785459 -4.2103634 -5.1965747][-4.0926371 -3.2123141 -2.424727 -1.9324293 -1.3852406 -1.0364103 -0.52933455 -0.25954819 0.021945 -0.84146929 -1.6012988 -2.9292331 -4.070601 -4.7453947 -5.8163362][-3.8032136 -3.3883538 -2.7723842 -1.6709504 -0.83834982 -0.091093063 1.120832 1.4745069 1.3834224 -0.001531601 -1.7550826 -3.4275746 -4.2086778 -4.7717533 -5.5296049][-3.3752904 -2.7227893 -1.9751968 -0.67308331 0.27746439 1.3809724 2.6344571 2.8432841 2.8847489 1.4153285 -0.49981308 -1.9565892 -2.8901391 -3.8007178 -4.9317923][-3.0609665 -2.1592269 -1.3095465 0.13247395 1.3208852 2.2756534 3.1240945 3.2372394 3.6451192 2.7073054 1.387094 -0.1809001 -1.8689837 -3.4207273 -4.8715258][-3.1614919 -2.1016626 -1.4755902 -0.4674654 0.79794836 2.0678277 2.7675376 2.8626609 3.1408467 2.1050906 0.72425795 -0.65469503 -1.7491183 -3.1301498 -4.9817562][-3.016387 -2.5295787 -1.9445395 -0.91156292 0.16329765 1.1897836 1.6533742 1.2715268 1.2004361 0.4846034 -0.74012709 -2.1347675 -3.1349225 -3.5188398 -4.6672978][-3.9928253 -3.6462045 -3.0153365 -2.2817683 -1.7003646 -0.96295452 -0.42049456 -0.37771654 -0.083856583 -0.83089781 -1.781826 -2.8921318 -3.7811599 -4.2193794 -5.4091682][-4.8967385 -4.7229958 -4.4999905 -4.0648317 -3.7036905 -3.1877131 -2.3836236 -2.1357131 -1.9558067 -2.1070414 -2.6035113 -3.4873204 -4.0222654 -4.8436837 -5.8373489][-5.3760514 -5.2409096 -5.237381 -4.7269921 -4.4204245 -4.1926413 -3.4160452 -3.3465929 -3.4096918 -3.4863129 -3.8894713 -4.4696279 -4.6230326 -4.9307723 -5.9024506][-5.929256 -5.8054056 -5.6289444 -5.2315016 -5.110364 -4.9831152 -4.9162016 -5.0411406 -4.66625 -4.8319778 -4.7850151 -4.8852272 -4.8638706 -5.0593395 -5.9057708][-7.7791357 -7.386734 -6.9579263 -6.6736021 -6.680366 -6.3029318 -5.7423744 -5.8228693 -6.07975 -6.2333379 -5.8921556 -5.8733139 -5.8498249 -5.9020224 -6.1640196]]...]
INFO - root - 2017-12-15 12:10:11.131525: step 10610, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 58h:05m:25s remains)
INFO - root - 2017-12-15 12:10:17.596072: step 10620, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 58h:18m:52s remains)
INFO - root - 2017-12-15 12:10:24.000044: step 10630, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 57h:07m:19s remains)
INFO - root - 2017-12-15 12:10:30.393373: step 10640, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 57h:27m:15s remains)
INFO - root - 2017-12-15 12:10:36.807287: step 10650, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 57h:42m:29s remains)
INFO - root - 2017-12-15 12:10:43.266212: step 10660, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 58h:07m:19s remains)
INFO - root - 2017-12-15 12:10:49.618462: step 10670, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.627 sec/batch; 56h:05m:47s remains)
INFO - root - 2017-12-15 12:10:56.138806: step 10680, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 55h:51m:58s remains)
INFO - root - 2017-12-15 12:11:02.716173: step 10690, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 59h:53m:45s remains)
INFO - root - 2017-12-15 12:11:09.152051: step 10700, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 57h:14m:46s remains)
2017-12-15 12:11:09.695273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0639338 -1.7545958 -1.5068169 -1.2718196 -1.0883374 -0.86109304 -0.63869858 -0.50292683 -0.618865 -1.8521848 -2.5011668 -3.8272777 -4.8273325 -6.1887484 -7.2219934][-2.7182288 -2.322854 -2.0850878 -1.9063511 -1.9788566 -1.9232602 -2.0032873 -2.0806451 -2.3201408 -3.4735589 -4.0338678 -5.4720125 -6.2333779 -7.268002 -8.1231928][-3.8151281 -3.4629965 -3.435266 -3.1497126 -2.8590121 -2.5745149 -2.5234585 -2.6377974 -2.9118791 -4.0577135 -4.6694994 -5.7915936 -6.163084 -7.1076713 -7.88225][-5.1421623 -4.7155051 -4.3589039 -3.6357865 -3.346313 -3.01752 -2.6625009 -2.7943177 -2.806551 -3.9158823 -4.2278328 -5.3483667 -5.8467016 -6.6881652 -7.3237605][-5.2980156 -4.6704607 -4.0448694 -3.1085744 -2.5968084 -1.8846049 -1.7188439 -1.9260292 -1.8214355 -2.9062309 -3.2384849 -4.3595734 -4.5666409 -5.5888748 -6.3803353][-5.285007 -4.5469236 -3.9674325 -2.488925 -1.4036841 -0.79552269 -0.39083576 -0.15222216 0.13635015 -1.0152345 -1.3976398 -2.6553512 -3.2540073 -4.6652393 -5.6062407][-5.897737 -4.9340897 -3.7464938 -2.370666 -1.2114925 0.041577339 0.97054672 1.199873 1.3638783 0.59551525 0.39107418 -1.2143669 -2.0147824 -3.3977008 -4.6749268][-5.8421259 -4.4570775 -3.1497903 -2.1499529 -1.2407346 0.15196705 1.1195803 1.6097851 2.1524343 1.3515921 0.81416512 -0.76017523 -1.6836138 -3.0637026 -4.2199192][-4.1611519 -3.62186 -2.9222498 -1.6023803 -0.57334471 0.12371349 0.66491032 1.1335096 1.43433 0.85926914 0.32956409 -1.373641 -2.3101063 -3.5424643 -4.5193186][-3.7285683 -3.0422196 -2.7404013 -2.2183437 -1.4613476 -0.60233307 0.22039604 0.43171024 0.4620924 -0.40136814 -0.92322445 -2.3453031 -3.0782056 -4.3245344 -5.0614252][-3.1242075 -2.8767228 -3.0414319 -2.8980031 -2.5103812 -2.4448533 -2.3826566 -1.8006215 -1.3956537 -2.3884974 -3.0353627 -3.8716855 -4.0919542 -4.5257564 -5.0952768][-2.112349 -2.562016 -3.3458891 -4.0617123 -4.178195 -4.2608786 -4.2548571 -4.2651563 -4.3030977 -4.5875206 -4.750576 -5.5226736 -5.6307211 -5.9347997 -6.247221][-2.361342 -2.9306207 -3.8179729 -4.1499257 -4.4069052 -5.0279818 -5.3256192 -5.0111876 -4.9791727 -5.4111881 -5.4900093 -5.8591013 -6.0997715 -6.3138704 -6.435442][-3.2697806 -3.7005327 -4.25675 -4.7012863 -4.9534159 -5.4259653 -5.7924252 -5.5540638 -4.9906769 -5.1843996 -5.2730732 -5.6392956 -5.8234882 -6.1538119 -6.3771811][-6.5818338 -6.4423494 -6.0911245 -6.1726484 -6.3723412 -6.8176055 -7.3161712 -7.3934093 -7.2159891 -6.8170266 -6.5217881 -6.4193354 -6.5724907 -7.0280709 -7.3096733]]...]
INFO - root - 2017-12-15 12:11:16.105507: step 10710, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 60h:17m:34s remains)
INFO - root - 2017-12-15 12:11:22.469552: step 10720, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 56h:10m:18s remains)
INFO - root - 2017-12-15 12:11:29.013562: step 10730, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.643 sec/batch; 57h:25m:44s remains)
INFO - root - 2017-12-15 12:11:35.434052: step 10740, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.630 sec/batch; 56h:19m:37s remains)
INFO - root - 2017-12-15 12:11:41.865084: step 10750, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 55h:15m:30s remains)
INFO - root - 2017-12-15 12:11:48.239445: step 10760, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 56h:41m:32s remains)
INFO - root - 2017-12-15 12:11:54.561876: step 10770, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 56h:57m:23s remains)
INFO - root - 2017-12-15 12:12:00.974688: step 10780, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 58h:20m:41s remains)
INFO - root - 2017-12-15 12:12:07.335710: step 10790, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:15m:26s remains)
INFO - root - 2017-12-15 12:12:13.721309: step 10800, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 57h:34m:16s remains)
2017-12-15 12:12:14.209626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.721663 -2.6183963 -2.5670648 -3.0237412 -3.6279573 -4.030817 -4.4076567 -3.834322 -2.92028 -3.3813066 -3.1651092 -3.7445035 -4.3041716 -5.2726383 -6.56004][-3.7024155 -3.3336706 -3.2976694 -3.4908786 -3.6733449 -3.7805033 -4.1183634 -3.5445027 -2.679369 -3.022253 -2.8902674 -4.1853867 -4.7819881 -5.147294 -6.3926067][-3.8682024 -3.2816372 -3.2310019 -3.3003702 -3.1710529 -2.9869905 -3.0352287 -2.9407721 -2.5958028 -3.0797458 -3.2812648 -4.5621195 -4.692884 -4.8403234 -5.8463726][-3.895961 -3.6518779 -3.4822083 -2.7398787 -2.209281 -1.6306114 -1.1795626 -1.1311407 -0.792429 -1.3761091 -1.5192499 -2.8153853 -3.4796157 -4.5055256 -5.8548059][-4.7447968 -4.2303381 -3.4665742 -2.3299904 -1.5318007 -0.13318062 0.89090252 0.8378849 0.89717007 -0.028989792 -0.55013275 -2.1235242 -2.9521446 -4.1009073 -5.3034058][-4.1731572 -4.3064809 -4.2101107 -2.5147977 -0.97340059 0.81229877 2.3667736 2.759469 3.1049929 1.8640881 0.85171127 -1.0492477 -2.00883 -3.4325724 -4.5800676][-5.6335821 -5.3660212 -4.56278 -2.8998232 -1.4093456 0.96398735 3.2162237 3.8054848 4.5235786 3.5617561 2.4473772 0.4591589 -1.1038523 -2.8728771 -4.4699211][-5.6291485 -5.4625883 -4.4330559 -2.5908613 -0.44325876 1.6219263 3.4838905 4.5009432 5.4737425 4.4634809 3.0569277 0.900301 -0.89356852 -3.023623 -4.9184656][-5.1670828 -5.3085561 -4.6645374 -3.0681853 -1.5007939 0.86080456 2.9551563 3.9468203 4.7997541 3.8832064 2.6077051 0.53929996 -1.2703419 -3.2966652 -4.7862539][-5.8536892 -6.2252636 -5.7717571 -4.7912712 -3.2691593 -0.93046236 0.75340462 1.8860855 2.9613323 1.7980633 0.38977432 -0.95226145 -2.399127 -3.7021801 -5.0735331][-6.8243508 -7.1370912 -7.1341782 -6.38957 -5.3845143 -3.7668419 -2.4033966 -1.1412334 -0.18151951 -0.57192707 -1.5260191 -2.5480714 -3.7037835 -4.6644897 -5.6183004][-7.2097316 -7.2089958 -7.4464912 -7.1385584 -6.9373145 -5.7583213 -4.6171312 -3.7743397 -3.3897729 -3.5687451 -4.1717935 -4.544476 -4.8872385 -5.2563305 -5.7678132][-6.4736447 -6.9925203 -7.6554413 -7.7205024 -7.8046212 -7.3753138 -6.7237506 -6.0837078 -5.3999581 -5.1338663 -5.538538 -5.904593 -5.9453068 -5.4764957 -5.3534727][-6.3963251 -6.6496449 -6.76978 -6.8262124 -7.0957489 -7.0547085 -6.5956764 -6.5555611 -6.4294052 -6.3244953 -6.2940269 -6.081049 -5.503787 -5.4313297 -5.5149708][-6.9980149 -6.9466362 -6.9418511 -6.7734995 -7.1810918 -6.9619107 -6.569212 -6.7859144 -6.80495 -6.8740282 -6.9108543 -6.6196432 -6.2733994 -5.9280438 -5.4125352]]...]
INFO - root - 2017-12-15 12:12:20.594445: step 10810, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:18m:01s remains)
INFO - root - 2017-12-15 12:12:27.066745: step 10820, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 58h:47m:50s remains)
INFO - root - 2017-12-15 12:12:33.510467: step 10830, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 58h:56m:03s remains)
INFO - root - 2017-12-15 12:12:39.920111: step 10840, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 56h:36m:56s remains)
INFO - root - 2017-12-15 12:12:46.349684: step 10850, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 57h:33m:01s remains)
INFO - root - 2017-12-15 12:12:52.830039: step 10860, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 55h:58m:24s remains)
INFO - root - 2017-12-15 12:12:59.275592: step 10870, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 56h:50m:15s remains)
INFO - root - 2017-12-15 12:13:05.630906: step 10880, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 56h:11m:24s remains)
INFO - root - 2017-12-15 12:13:12.057019: step 10890, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 56h:39m:52s remains)
INFO - root - 2017-12-15 12:13:18.433630: step 10900, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 57h:18m:46s remains)
2017-12-15 12:13:18.907967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6169167 -5.48844 -5.4825068 -5.5801415 -5.0995312 -4.6551018 -3.7360852 -2.9996548 -2.5306563 -3.1700983 -4.4003429 -5.3755836 -6.7195573 -7.3454771 -7.9483633][-5.2745023 -5.4618196 -5.5238581 -5.0805678 -4.8649416 -4.526619 -4.2681227 -3.4863133 -2.4030304 -2.6095805 -3.5565028 -4.8733358 -6.4056754 -7.181807 -7.8880677][-4.8564091 -4.7739363 -4.6792049 -4.8443441 -4.5765944 -3.7770638 -3.1631813 -2.7371163 -2.5436144 -2.5180645 -3.05682 -4.2506819 -5.8052478 -6.8869147 -7.6957746][-4.809773 -4.3962317 -4.2752013 -3.8401256 -3.1720948 -2.4535193 -1.4989228 -1.0970359 -1.0893269 -1.7371416 -2.7432752 -3.9296489 -5.5611143 -6.4541435 -7.348947][-4.6551924 -3.8976159 -3.30511 -2.857698 -2.224299 -1.1992426 -0.24916363 0.2284236 0.41097403 -0.37006903 -1.8404908 -3.4693203 -5.1385174 -6.0955458 -6.9142618][-3.9556172 -3.6436553 -2.9699903 -2.2128115 -0.96978521 0.13044357 0.93530226 0.7662816 0.67962885 -0.20089436 -1.8855429 -3.2073278 -4.7010384 -5.6644573 -6.6814656][-2.7476692 -2.1165771 -1.67806 -0.98802233 -0.04618597 0.99309778 1.6601062 1.3970208 0.94817972 -0.07775259 -1.4962859 -3.1314631 -4.8993683 -5.630352 -6.33529][-1.4769206 -1.0669265 -0.54805756 0.20005989 1.0490546 1.6252208 2.1911597 2.0458808 1.4568572 0.095509052 -1.600636 -3.0209236 -4.4952888 -5.4240122 -6.1185522][-1.9075818 -1.1618233 -0.20017147 0.35078192 1.3253417 1.9225793 2.0063577 1.8611665 1.586287 0.27622652 -1.6023984 -3.0929198 -4.5460882 -5.2077723 -5.7565708][-2.9918308 -2.2770052 -1.3427215 -0.5819478 0.35853338 1.0856118 1.6838946 1.2397599 0.64034414 -0.42639446 -1.8089375 -3.2870526 -4.5198011 -5.3231263 -5.9362726][-4.7961464 -3.8812451 -2.9069033 -2.2986321 -1.5439754 -0.74336529 -0.058764935 -0.11526155 -0.45338154 -1.7510242 -3.2274275 -4.0731506 -5.0151272 -5.759057 -6.3868752][-4.8384714 -3.8745961 -3.7523229 -3.1124716 -1.9724784 -1.5654206 -1.4826655 -1.5052996 -1.4733415 -2.6000838 -3.7270746 -4.7305403 -5.827589 -6.4852362 -7.0264449][-5.3260651 -4.6121292 -3.8167539 -3.4510059 -3.393311 -2.9722929 -2.65691 -2.8833475 -3.09557 -3.9872615 -5.1369615 -5.9089293 -6.3658428 -7.0995712 -7.4661674][-4.9718385 -4.9196482 -4.541759 -3.661438 -3.0561934 -3.398705 -3.5519943 -3.6783354 -3.978543 -4.4915934 -5.2497168 -6.1484046 -6.8413396 -7.2017655 -7.3623118][-4.6353941 -4.6856146 -4.7473259 -4.2979708 -3.2762642 -3.1486826 -3.526104 -4.2052441 -4.4870539 -4.9946504 -5.850945 -6.3284917 -6.699481 -7.1625075 -7.4166269]]...]
INFO - root - 2017-12-15 12:13:25.453705: step 10910, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 57h:51m:33s remains)
INFO - root - 2017-12-15 12:13:31.897362: step 10920, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 57h:02m:10s remains)
INFO - root - 2017-12-15 12:13:38.172947: step 10930, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 55h:48m:40s remains)
INFO - root - 2017-12-15 12:13:44.529825: step 10940, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 56h:03m:51s remains)
INFO - root - 2017-12-15 12:13:50.994378: step 10950, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 57h:14m:31s remains)
INFO - root - 2017-12-15 12:13:57.386272: step 10960, loss = 0.29, batch loss = 0.17 (11.8 examples/sec; 0.676 sec/batch; 60h:21m:52s remains)
INFO - root - 2017-12-15 12:14:03.816688: step 10970, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 57h:56m:42s remains)
INFO - root - 2017-12-15 12:14:10.244807: step 10980, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.637 sec/batch; 56h:56m:07s remains)
INFO - root - 2017-12-15 12:14:16.652203: step 10990, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 58h:10m:42s remains)
INFO - root - 2017-12-15 12:14:23.072002: step 11000, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 56h:53m:07s remains)
2017-12-15 12:14:23.551508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5904808 -5.9256706 -6.8477879 -6.6446834 -5.506495 -4.6077132 -3.3220344 -2.3871722 -2.0711565 -3.0728841 -4.6259766 -6.4937491 -7.4593067 -7.5597024 -7.6813169][-3.5943751 -5.0279484 -6.8284383 -7.2997274 -6.8110161 -5.8700619 -4.285223 -3.4334078 -3.4935308 -4.3285127 -5.5626907 -6.8973603 -8.0178547 -8.6350317 -8.6437178][-3.6121864 -4.6333466 -5.8202152 -5.86154 -5.5230527 -4.8623371 -3.7680418 -3.420856 -3.5119123 -4.5454931 -5.7756777 -6.8600812 -7.6989946 -8.1631975 -8.482832][-3.7592652 -3.7474196 -4.2481194 -3.4030786 -2.5949769 -2.0102692 -1.1189008 -1.2777357 -1.7327833 -3.4295893 -4.8879671 -6.125916 -7.0277357 -7.3489089 -7.7145414][-3.8964863 -2.6448913 -2.2323418 -1.2573304 -0.11961365 0.39171648 0.83132219 0.51240969 -0.3855114 -1.9577899 -3.5339322 -5.1320391 -6.4041777 -6.9654665 -7.038559][-3.8523066 -2.2787657 -1.3934584 -0.27879429 0.5431056 1.8221917 2.3885522 1.3003821 0.45808458 -0.90763569 -2.4333959 -4.187521 -5.5307803 -6.2180409 -6.453989][-4.5805149 -3.1936841 -2.0366535 -0.58135939 0.89403772 2.3610978 3.2648005 2.277101 1.02741 -0.61048079 -2.2710495 -4.1384764 -5.5347281 -6.0044446 -6.3435059][-5.1375847 -4.0728922 -2.760385 -0.91666651 0.45161486 1.8231406 2.9693198 2.6324983 1.6061711 -0.64037752 -2.8229008 -4.7963696 -5.8348293 -6.3374228 -6.5106745][-5.6256819 -4.8678546 -4.2028675 -2.4112229 -1.3441315 -0.28815079 0.80199957 1.2291446 1.3027472 -0.68728542 -3.1457438 -5.4904251 -6.5251565 -6.6648932 -6.7980175][-6.3695807 -5.8557873 -5.4079967 -4.194046 -3.1658 -2.1639614 -1.3739567 -0.73379135 -0.18613815 -1.9205403 -3.7844787 -5.8433003 -7.1197152 -7.5740814 -7.736392][-8.3583269 -7.9423575 -7.41054 -6.5651455 -5.5353651 -4.7408895 -4.1045876 -3.9073625 -3.9527278 -5.0758991 -6.1187391 -7.0300164 -7.8232212 -8.3875256 -8.5342417][-9.2610474 -8.5920162 -8.3252106 -7.6237016 -6.9351192 -6.2768326 -5.7368984 -5.7754784 -5.78658 -6.785419 -7.24541 -7.6100717 -7.7061009 -8.1980867 -8.3232613][-10.835749 -10.105777 -9.55979 -8.9714947 -8.2613707 -7.66207 -7.117878 -7.312077 -7.1656909 -7.7239542 -8.1690207 -7.5453157 -7.1142607 -7.203403 -6.9676809][-10.307719 -9.6836624 -8.958601 -8.5039482 -8.0626307 -7.2789383 -6.4733882 -6.6470003 -6.90073 -7.4369211 -7.6091843 -6.9190807 -6.3949 -6.1347728 -5.8697977][-9.74097 -9.3862562 -9.522562 -9.1404772 -8.4983435 -7.7874136 -7.134316 -7.2432904 -7.28826 -7.5803742 -7.4699821 -6.6935983 -5.9729829 -5.7563882 -5.311533]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:14:29.945213: step 11010, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 56h:56m:35s remains)
INFO - root - 2017-12-15 12:14:36.418369: step 11020, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 55h:28m:52s remains)
INFO - root - 2017-12-15 12:14:42.912520: step 11030, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 57h:25m:58s remains)
INFO - root - 2017-12-15 12:14:49.291284: step 11040, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 57h:26m:45s remains)
INFO - root - 2017-12-15 12:14:55.641988: step 11050, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 56h:21m:04s remains)
INFO - root - 2017-12-15 12:15:02.050161: step 11060, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 58h:36m:50s remains)
INFO - root - 2017-12-15 12:15:08.541937: step 11070, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 58h:35m:36s remains)
INFO - root - 2017-12-15 12:15:14.951459: step 11080, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 58h:45m:38s remains)
INFO - root - 2017-12-15 12:15:21.415361: step 11090, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 56h:23m:15s remains)
INFO - root - 2017-12-15 12:15:27.793051: step 11100, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.655 sec/batch; 58h:29m:32s remains)
2017-12-15 12:15:28.342112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4078574 -6.5403113 -6.3814726 -6.0396461 -5.6213331 -5.0863686 -4.5201387 -4.0038123 -3.5367813 -4.3927946 -5.066062 -6.281157 -7.3968577 -7.8468895 -8.3507071][-5.048151 -5.5135603 -5.9351745 -6.4282656 -6.3973804 -5.4843683 -4.4795566 -3.7784789 -3.3047032 -4.6170559 -5.4796686 -6.7564688 -8.1490116 -8.6024771 -9.1488113][-3.4750352 -3.8476298 -4.0087242 -4.5949965 -4.7533245 -4.3975935 -3.4687672 -2.8938155 -2.1612883 -3.1721654 -4.0202055 -5.4806628 -6.4966989 -6.9846048 -7.1710219][-2.5249162 -2.9770718 -2.9811749 -3.0120797 -2.5096431 -1.5983505 -0.79618406 -0.58433533 -0.12222815 -1.4131169 -2.5609102 -4.3720937 -6.1720171 -6.7042637 -7.12343][-2.2860599 -2.0516076 -1.3401642 -0.65872335 0.11171484 1.0919328 1.6559377 1.5938716 1.4703193 -0.43508005 -1.4687457 -3.1393337 -4.7610579 -5.6210871 -6.8487291][-3.1629162 -2.0556178 -1.2111177 -0.23257446 1.1001906 2.5377269 3.59648 3.4693475 3.0513711 0.57363272 -1.1611309 -2.746139 -4.1911306 -4.8945379 -5.3872643][-4.2724824 -3.5320282 -2.1658206 -0.0090489388 1.7698922 3.0899863 3.9641671 3.7475495 3.1920409 0.62404394 -1.4903708 -3.5360451 -5.1429186 -5.7114739 -6.22396][-5.3194 -4.1605191 -2.4906597 -0.068843365 2.2644439 3.5504422 3.8450322 3.3362479 2.456511 -0.23821878 -2.2335882 -4.6655464 -6.1545258 -6.968215 -7.3775258][-5.7435236 -4.5780163 -2.8523712 -0.71686459 1.2918296 2.5717063 2.7794795 2.6161141 2.0827079 -0.77529526 -3.1315784 -5.1832428 -6.7512412 -7.4356294 -7.7210865][-7.0776782 -6.1751089 -4.4868608 -2.3382611 -0.57583237 0.39727354 0.70516729 0.41955137 0.13193941 -2.5540519 -4.4412255 -6.2001143 -7.4634695 -8.1667 -8.3961086][-7.9006429 -7.1191106 -5.758553 -3.7629838 -2.4380546 -1.8208399 -1.8555627 -2.1702876 -2.4804688 -4.7774553 -6.0747967 -7.4060345 -8.1131639 -8.5843792 -8.6913471][-7.6174374 -6.7640872 -5.622767 -4.5485477 -3.8036277 -3.3688588 -3.591167 -3.7171915 -3.8913674 -5.7183514 -6.4165125 -7.2157712 -7.7964664 -8.3333578 -8.54682][-7.7661781 -6.7197957 -5.7978468 -4.8518724 -4.3959112 -4.4223166 -4.31006 -4.8970323 -5.2816896 -6.2352295 -6.8687644 -7.357964 -7.7356234 -7.6589017 -7.6839476][-7.5892506 -6.5871878 -5.7205348 -5.1719923 -4.9347043 -5.187273 -5.4913154 -6.0814171 -6.3391294 -7.0646253 -6.991931 -6.8809047 -6.9624271 -6.8790288 -6.8027921][-7.4066935 -6.763401 -5.87272 -5.5613842 -5.7125597 -6.1896982 -6.7105026 -7.2302265 -7.5851817 -7.6065702 -7.2777166 -6.8863554 -6.5302525 -6.3321443 -6.1313343]]...]
INFO - root - 2017-12-15 12:15:34.735442: step 11110, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 57h:35m:04s remains)
INFO - root - 2017-12-15 12:15:41.213097: step 11120, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 58h:24m:25s remains)
INFO - root - 2017-12-15 12:15:47.625668: step 11130, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 57h:09m:01s remains)
INFO - root - 2017-12-15 12:15:53.990572: step 11140, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 59h:04m:40s remains)
INFO - root - 2017-12-15 12:16:00.426277: step 11150, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 58h:50m:35s remains)
INFO - root - 2017-12-15 12:16:06.845764: step 11160, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:10m:41s remains)
INFO - root - 2017-12-15 12:16:13.251515: step 11170, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 55h:51m:37s remains)
INFO - root - 2017-12-15 12:16:19.659273: step 11180, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 59h:05m:55s remains)
INFO - root - 2017-12-15 12:16:26.018438: step 11190, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 56h:08m:16s remains)
INFO - root - 2017-12-15 12:16:32.464705: step 11200, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 59h:47m:24s remains)
2017-12-15 12:16:32.972564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7535982 -3.2697854 -3.048152 -3.1370115 -3.305315 -3.0150566 -2.8432283 -2.6751132 -2.54425 -4.1920261 -5.2456703 -6.3392959 -7.399466 -8.3131914 -8.5081816][-3.7120819 -3.5682244 -3.7001307 -3.3430476 -3.0641026 -2.8730135 -2.7566032 -2.6897292 -2.4394932 -3.8791804 -4.674264 -5.6848149 -6.8083491 -7.1573067 -7.6604433][-5.5371447 -5.0053358 -4.4723463 -3.908982 -3.5273681 -3.1648126 -2.9317508 -2.9108958 -2.69419 -4.1979666 -4.8252435 -5.8429685 -6.7906246 -7.2811403 -7.6116009][-6.0556135 -5.6126261 -5.4622679 -4.0067453 -2.6007323 -1.9419088 -1.884974 -1.7972417 -1.6784539 -3.313467 -4.1854086 -5.3835449 -6.4963412 -6.9582224 -7.1607389][-6.0205936 -4.5344734 -3.2627296 -2.3534908 -1.9763269 -0.77199507 -0.22953463 -0.14084148 -0.065732956 -1.5412021 -2.3607388 -3.6306977 -5.1063538 -6.0492744 -6.5733013][-5.2896075 -3.9269474 -3.4121027 -1.8200741 -0.41140795 0.395998 0.98389244 1.0673342 0.7068491 -0.90612173 -1.571455 -2.7727137 -4.0601511 -5.0239162 -5.9691787][-4.9881792 -3.8305697 -2.0004148 -0.29440975 0.5230484 1.4022999 1.9305115 1.7122059 1.4587536 -0.33755541 -1.1732411 -2.07802 -2.97966 -4.0416265 -5.1754885][-3.6924403 -2.0000124 -1.1413631 0.33634281 2.0920057 2.5274248 2.8850584 2.8513708 2.6534195 0.39234924 -0.60247707 -1.5611324 -2.6267109 -3.6079106 -4.4133892][-3.4267988 -2.1388388 -0.14286518 1.0342054 1.6616316 2.1930437 2.404398 1.9770346 1.7854233 0.19408035 -0.54109144 -1.5848837 -2.9803243 -4.0943995 -5.0707388][-4.3756409 -2.9634514 -1.9086661 -0.18320465 1.0386581 1.2411661 1.2917738 1.0543327 0.90389729 -0.78190756 -1.6779137 -2.5347724 -3.5736737 -4.1914434 -5.0053167][-5.6179242 -5.1587181 -4.33417 -2.6493726 -1.4334755 -1.0358567 -0.737803 -1.0860887 -1.0141029 -2.4179363 -3.3187342 -3.7856255 -4.4963083 -4.8457441 -5.5100384][-8.4324741 -7.2131171 -6.049305 -4.7899175 -3.5079951 -2.8871894 -2.8038611 -3.0177417 -2.9537926 -3.8854566 -4.8375292 -5.0079603 -5.2841225 -5.23945 -5.82942][-9.1949081 -8.0534639 -7.3680696 -6.2809 -5.1337976 -4.3976669 -3.886322 -4.056735 -4.1602049 -4.6477418 -4.9726954 -5.0102148 -5.5739942 -5.4395914 -5.6194973][-8.032774 -7.7558846 -7.1211033 -6.3625188 -5.7151623 -5.3856411 -5.0381651 -4.7882605 -4.7888002 -5.0351834 -5.1630049 -5.0785327 -5.3709879 -5.4216909 -5.6962452][-7.8601074 -7.2301922 -6.8049212 -6.7685003 -6.5924935 -6.4371796 -6.1361895 -6.2096515 -6.4328189 -6.3894434 -6.5431709 -6.3609362 -6.2495556 -6.119946 -5.957922]]...]
INFO - root - 2017-12-15 12:16:39.294513: step 11210, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 55h:05m:04s remains)
INFO - root - 2017-12-15 12:16:45.704023: step 11220, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 57h:18m:57s remains)
INFO - root - 2017-12-15 12:16:52.108721: step 11230, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 55h:42m:49s remains)
INFO - root - 2017-12-15 12:16:58.505365: step 11240, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 58h:31m:09s remains)
INFO - root - 2017-12-15 12:17:04.955357: step 11250, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 58h:36m:50s remains)
INFO - root - 2017-12-15 12:17:11.466699: step 11260, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.668 sec/batch; 59h:33m:48s remains)
INFO - root - 2017-12-15 12:17:17.863080: step 11270, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:12m:30s remains)
INFO - root - 2017-12-15 12:17:24.286878: step 11280, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 57h:09m:47s remains)
INFO - root - 2017-12-15 12:17:30.666646: step 11290, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 57h:43m:17s remains)
INFO - root - 2017-12-15 12:17:37.030063: step 11300, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:13m:50s remains)
2017-12-15 12:17:37.497884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1794467 -2.7552419 -2.3509145 -1.7025843 -1.2339253 -1.0405569 -1.0384679 -1.0947151 -1.0775385 -2.8803787 -3.9042234 -5.2074423 -6.0447307 -6.6842842 -7.6544271][-2.9298468 -2.4577322 -2.1494956 -1.8304272 -1.5348215 -1.3668571 -1.3774815 -1.6167965 -1.8511586 -3.7341197 -4.4993148 -5.24597 -6.2563128 -7.2738771 -8.0788441][-3.1983032 -2.7340288 -2.6547093 -2.4722896 -2.0919704 -1.5005212 -0.95160627 -0.96146822 -1.0796399 -2.6368985 -3.3624187 -4.1609807 -4.9495668 -6.0157413 -7.0384159][-3.398633 -3.2041521 -2.9606752 -2.4621091 -1.905179 -1.1947775 -0.56709528 -0.51928759 -0.3450532 -1.8482122 -2.9134235 -3.6785536 -4.5978675 -5.7210078 -6.7496071][-3.9738102 -3.4048862 -2.7029266 -2.2594256 -1.7548285 -0.86826038 0.040434361 0.16450548 0.17720652 -1.3418221 -2.1915874 -3.1326084 -4.2602429 -5.2657619 -6.2491555][-4.0721397 -3.3873725 -2.8079691 -2.0080562 -1.1927204 -0.50777388 0.33914042 0.5308404 0.60819387 -0.87339878 -1.8078794 -2.7899251 -4.0736275 -5.1958957 -6.2200694][-3.8484538 -3.2265544 -2.3708735 -1.5561829 -0.814723 -0.04985857 0.89160109 1.2357526 1.4352298 -0.22356796 -1.3720717 -2.4449768 -3.7184827 -4.9169459 -5.9385705][-3.1736093 -2.2057357 -1.2187395 -0.25867319 0.74480104 1.1955543 1.7485061 1.834518 1.9291577 0.22102213 -1.2104959 -2.5517797 -3.7821598 -4.9481668 -5.8709278][-2.7667022 -1.7979274 -0.7604394 0.34425879 1.0839963 1.3208194 1.6392159 1.7687573 1.9180303 0.087469578 -1.4219499 -2.8775477 -4.3496804 -5.432292 -6.2425051][-2.0931525 -1.0228176 -0.26628447 0.53099203 1.2409873 1.3910604 1.522367 1.4057813 1.2039065 -0.76513433 -2.0916481 -3.6463923 -4.9007049 -5.8666668 -6.7329345][-2.8015065 -1.9215951 -1.2300706 -0.34370947 0.14572716 0.06682682 -0.12455416 -0.33306789 -0.4037919 -2.3624897 -3.7667127 -4.7231865 -5.5046959 -6.2206392 -6.8806629][-4.0904536 -3.3260641 -2.7153959 -1.6672201 -1.0794053 -1.1990132 -1.260201 -1.5635791 -1.8703828 -3.2712626 -4.4069071 -5.3078122 -5.8413506 -6.2819457 -6.6289334][-5.3811393 -5.090693 -4.652194 -3.8961587 -3.3395839 -3.2587624 -3.3787808 -3.5657415 -3.5040026 -4.1893549 -4.9754877 -5.3916163 -5.624557 -5.8420539 -6.0804906][-5.7686534 -5.5190477 -5.3570375 -4.5680227 -4.2231288 -3.9782476 -4.0457411 -4.0953722 -4.11648 -4.7717056 -5.0855513 -5.1142178 -5.0754976 -5.1836405 -5.3839951][-6.9392824 -6.5844545 -5.9951339 -5.6140704 -5.4552755 -5.337657 -5.52815 -5.728054 -5.7944832 -5.635674 -5.65432 -5.5667229 -5.2993402 -5.155159 -5.039907]]...]
INFO - root - 2017-12-15 12:17:43.921026: step 11310, loss = 0.37, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 58h:32m:57s remains)
INFO - root - 2017-12-15 12:17:50.324565: step 11320, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 56h:07m:03s remains)
INFO - root - 2017-12-15 12:17:56.746468: step 11330, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 56h:24m:12s remains)
INFO - root - 2017-12-15 12:18:03.130735: step 11340, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 58h:28m:55s remains)
INFO - root - 2017-12-15 12:18:09.627316: step 11350, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.629 sec/batch; 56h:04m:29s remains)
INFO - root - 2017-12-15 12:18:15.980818: step 11360, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 57h:46m:42s remains)
INFO - root - 2017-12-15 12:18:22.448689: step 11370, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 58h:53m:29s remains)
INFO - root - 2017-12-15 12:18:28.964483: step 11380, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 57h:01m:21s remains)
INFO - root - 2017-12-15 12:18:35.413684: step 11390, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 56h:39m:26s remains)
INFO - root - 2017-12-15 12:18:41.750336: step 11400, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 56h:59m:20s remains)
2017-12-15 12:18:42.253867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1350503 -6.3834748 -6.4964266 -6.4328785 -5.4797482 -4.4299154 -3.0006537 -1.3920255 -0.12890339 -0.56884146 -1.6710782 -4.1182852 -6.2192664 -8.3030844 -9.3046436][-5.0695982 -5.5615005 -6.3264389 -6.6410675 -6.4208412 -5.3391314 -3.6047635 -1.516706 -0.034159184 -0.0517807 -1.5976229 -3.8119087 -6.0772281 -8.2116537 -9.798707][-4.9786124 -5.2921104 -5.7566419 -5.4081459 -4.7010674 -3.8487663 -2.8444991 -1.5982175 -0.20368004 -0.48396349 -1.5458145 -3.6297693 -5.6301355 -7.173419 -8.4193707][-4.9073629 -4.4579525 -4.2863994 -3.9764006 -3.4847755 -2.5746269 -1.3718662 -0.62156916 0.1030674 -0.27446222 -1.1251659 -3.3937192 -5.1397548 -6.7208872 -7.5337467][-4.8805447 -4.1396694 -3.2793956 -2.5813622 -1.880619 -0.66831064 0.54038906 1.2085772 1.5192432 0.56866169 -0.92463112 -3.2137403 -4.9698372 -6.6173763 -7.45792][-4.4504619 -3.6649313 -2.3942833 -1.5098243 -0.37949181 0.79918385 2.4423847 2.9843607 3.1027775 1.9997473 0.17900562 -2.61229 -4.8292561 -6.2450571 -7.0421124][-4.0833659 -2.9070587 -1.6673703 -0.7383728 0.44523811 1.4805441 2.8378239 3.7184153 4.3040829 2.7974586 0.80809689 -2.1906662 -4.5868635 -6.2824621 -7.1608491][-3.6765327 -2.1922383 -0.67713547 0.23595285 1.3202791 1.8936281 2.538496 3.50673 4.0850096 3.0986748 1.1220675 -1.9274549 -4.4767671 -6.4732432 -7.4363265][-2.9936337 -1.8126931 -0.40926075 0.68875885 1.8839874 2.2414665 2.6610346 3.0110006 3.2835169 2.1047783 0.46635532 -1.9025359 -4.3081341 -6.4875813 -7.7544317][-2.2826695 -1.4475837 -0.094302177 0.70301056 1.6236544 2.0676508 2.6984959 2.8135881 2.8432646 1.0429611 -0.85530853 -3.1218848 -4.9440174 -6.3472443 -7.3339472][-2.0466652 -1.341507 -0.55355835 0.32774544 1.2649794 1.3754225 1.7119522 1.8127022 1.8415108 0.057634354 -2.2042713 -4.077879 -5.2784472 -6.686079 -7.5400715][-2.8711519 -1.8703427 -0.90902996 -0.056172848 0.521678 0.46473789 0.52720451 0.10438013 -0.45008612 -1.2358537 -2.4935517 -4.1872482 -5.8048863 -7.0158343 -7.7856627][-4.1015472 -3.1384058 -2.2104564 -1.1779304 -0.26784611 -0.28294611 -0.68807125 -1.1362028 -1.5609503 -2.4634461 -3.5920229 -4.6358991 -5.8627276 -6.6815767 -7.5024176][-6.0490522 -5.4713926 -4.2116594 -3.1052341 -1.9581065 -1.4083147 -1.4126043 -1.5637736 -2.1278129 -3.1095109 -4.4003611 -5.3968463 -6.3280487 -6.6818686 -6.5786014][-7.0897141 -6.5164804 -5.7876163 -5.4010687 -4.37351 -3.4968734 -3.0540934 -3.2422323 -3.9856582 -4.6224623 -5.7210116 -6.4671903 -6.6699152 -6.5863533 -6.5518103]]...]
INFO - root - 2017-12-15 12:18:48.602402: step 11410, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 55h:38m:39s remains)
INFO - root - 2017-12-15 12:18:55.046233: step 11420, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 56h:56m:19s remains)
INFO - root - 2017-12-15 12:19:01.454019: step 11430, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.639 sec/batch; 56h:58m:20s remains)
INFO - root - 2017-12-15 12:19:07.916620: step 11440, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 57h:11m:15s remains)
INFO - root - 2017-12-15 12:19:14.288697: step 11450, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 57h:58m:12s remains)
INFO - root - 2017-12-15 12:19:20.680592: step 11460, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.617 sec/batch; 55h:00m:27s remains)
INFO - root - 2017-12-15 12:19:27.042272: step 11470, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 57h:00m:05s remains)
INFO - root - 2017-12-15 12:19:33.474070: step 11480, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 57h:30m:37s remains)
INFO - root - 2017-12-15 12:19:39.855973: step 11490, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 59h:04m:36s remains)
INFO - root - 2017-12-15 12:19:46.295965: step 11500, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 57h:31m:03s remains)
2017-12-15 12:19:46.759495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2755013 -0.872211 -0.86439705 -0.97387552 -1.1503048 -1.7103739 -1.9648676 -2.4825602 -3.2217512 -4.359808 -5.1398416 -6.1722455 -7.0011797 -7.6789551 -8.6171551][-1.7913618 -1.5938864 -1.5019741 -1.1051235 -1.119946 -1.4213109 -2.2111979 -3.0855584 -3.1565142 -4.4175119 -5.5244093 -6.3497658 -7.0025415 -7.8919845 -9.1913071][-2.7253957 -2.4769726 -1.8173113 -1.5314841 -1.3096294 -1.3811369 -1.4424753 -1.7481074 -2.2608581 -2.921535 -3.7457819 -5.0066833 -6.35206 -6.9365897 -8.1938734][-3.0224862 -2.4441166 -2.1376433 -1.6087036 -0.99148035 -0.76245403 -0.73656178 -0.96493435 -1.1106744 -1.7994418 -2.6112771 -3.5085859 -4.4925566 -5.152688 -6.4133558][-4.6811352 -3.5877233 -2.2148218 -1.8282838 -1.2919445 -0.40843105 0.24174023 0.041686535 -0.038824081 -0.71387386 -1.3213425 -2.5615897 -3.780894 -4.5029373 -5.6853724][-5.0299921 -3.9698117 -3.185967 -2.0352621 -1.0900464 -0.30963087 0.65641117 1.2081184 1.5826311 0.68780327 -0.23168707 -1.5699515 -2.6616101 -3.6230412 -4.6167803][-4.900548 -4.3660607 -3.3364291 -2.04219 -1.3406734 -0.58671427 0.38551235 0.921649 1.58852 0.90339184 0.080816746 -1.6336179 -3.0040369 -3.3677497 -4.2847919][-4.8139434 -4.0850267 -3.0927844 -1.9737282 -0.85242224 -0.42122507 0.1689868 0.47102356 0.92323494 0.011144161 -0.95326805 -2.4328184 -3.2100759 -3.9203839 -4.7345486][-4.3002577 -3.8648903 -2.9494243 -1.8227763 -0.82884836 -0.2291441 0.72934628 0.5104332 0.3745203 -0.97335672 -2.2364669 -3.9452956 -5.0765619 -5.2945962 -5.8969374][-5.317935 -4.4605694 -3.4722328 -2.7643337 -1.9516377 -1.11303 0.0022611618 -0.16437483 -0.41210508 -2.0822892 -3.8729486 -5.2972283 -6.4187202 -7.0400753 -7.7718587][-6.0779181 -5.684762 -5.4095788 -4.4047308 -3.6565495 -2.9309387 -1.9201732 -1.8759289 -1.9657111 -3.4976459 -5.2225113 -6.2656722 -7.6345987 -8.0002518 -8.4419851][-7.2818904 -6.4898987 -6.1574087 -5.9109344 -5.6958671 -4.6974807 -3.5702505 -3.513998 -3.3511896 -4.5665441 -5.7919564 -6.8113666 -7.8331451 -7.848794 -8.7116289][-8.6322956 -7.920579 -7.39298 -7.0522838 -6.3999467 -5.7828941 -4.902894 -4.8465481 -4.8597107 -5.3129058 -5.9290838 -6.0878749 -6.8905635 -7.0947127 -7.5069208][-8.5601225 -8.4643612 -7.9700975 -7.2720575 -6.7106514 -6.0661535 -5.5111923 -5.5690374 -5.56581 -5.6733189 -5.7438164 -5.8270359 -6.2330084 -5.9388256 -6.3229938][-9.0590725 -9.0672131 -8.9121866 -8.43505 -7.8442745 -7.553937 -6.6160812 -6.755733 -6.8771358 -6.7385807 -6.926734 -6.4710674 -6.2808495 -6.310441 -5.9519453]]...]
INFO - root - 2017-12-15 12:19:53.239337: step 11510, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 59h:17m:59s remains)
INFO - root - 2017-12-15 12:19:59.736927: step 11520, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 58h:19m:37s remains)
INFO - root - 2017-12-15 12:20:06.244688: step 11530, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 58h:56m:41s remains)
INFO - root - 2017-12-15 12:20:12.642862: step 11540, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 57h:45m:55s remains)
INFO - root - 2017-12-15 12:20:19.020916: step 11550, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 56h:02m:37s remains)
INFO - root - 2017-12-15 12:20:25.382943: step 11560, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 56h:31m:16s remains)
INFO - root - 2017-12-15 12:20:31.727224: step 11570, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 56h:54m:23s remains)
INFO - root - 2017-12-15 12:20:38.167655: step 11580, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 56h:50m:25s remains)
INFO - root - 2017-12-15 12:20:44.601595: step 11590, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:09m:46s remains)
INFO - root - 2017-12-15 12:20:51.012088: step 11600, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 55h:45m:04s remains)
2017-12-15 12:20:51.494109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.1216011 -9.00703 -8.53431 -7.3532863 -6.2581377 -5.7852879 -5.5023346 -5.0831976 -4.5296803 -5.0393047 -5.5708427 -6.5191689 -7.9776888 -8.7860613 -9.786026][-8.3217812 -8.0396776 -7.9935374 -8.0204163 -7.5300717 -6.4433165 -6.1046991 -6.2368803 -6.3231959 -7.3164411 -7.5058355 -8.302865 -9.7878532 -10.51651 -11.569513][-7.6653857 -7.3045893 -7.0905104 -7.0307851 -6.5633388 -5.9973316 -5.6142306 -5.6852379 -5.7827926 -6.8755341 -7.4489083 -8.1896057 -9.2859364 -9.6844788 -10.198184][-6.4677105 -6.0476627 -5.436482 -5.261179 -4.8663621 -4.1441488 -3.6467481 -3.2290106 -2.8513708 -4.3560848 -4.925642 -5.9270754 -7.2023082 -7.8766289 -8.92849][-4.4137793 -4.1202364 -3.7701058 -3.857743 -3.5387268 -2.4044995 -1.3582454 -0.95994473 -0.64753866 -2.1866593 -2.656703 -3.6215305 -5.2369971 -6.1359019 -7.348268][-3.6400123 -3.2397361 -2.9420495 -2.5199056 -1.9952312 -1.2720304 -0.28746891 0.22810221 0.49519396 -0.99947309 -1.0723324 -1.9572673 -3.3232622 -4.5294142 -5.7470202][-4.684659 -4.1051483 -3.4683771 -2.3719325 -1.2204533 -0.24617052 0.58181047 1.1255889 1.3990579 -0.32206345 -0.743371 -1.6755123 -2.9473162 -4.1168532 -5.38794][-4.8080668 -3.8834636 -3.0960894 -2.1297016 -0.70499372 0.30210304 1.1165738 1.6921535 1.9909415 0.36450434 -0.0060210228 -0.95380735 -2.9016919 -4.33305 -5.4798069][-4.3974056 -3.8245163 -3.0599675 -2.0497537 -0.85580158 0.3557992 1.0812945 1.1050525 1.3933196 -0.14166594 -0.92961788 -1.8135262 -3.2516317 -4.4703474 -5.8013673][-4.4304996 -3.5283184 -2.7823153 -2.2094488 -1.2485743 -0.08897543 0.14408922 0.3068471 0.51883554 -1.6959929 -2.1112485 -2.8929286 -3.6286912 -4.4016418 -5.3821697][-5.0198822 -4.0908346 -3.4018874 -2.6551929 -1.9966168 -1.2718816 -1.1225076 -1.1825399 -1.192565 -3.0012937 -3.66369 -4.3273048 -4.71428 -4.3676796 -4.6765656][-5.8635731 -4.0083356 -2.7567382 -2.4814734 -2.0374928 -1.0313001 -0.990716 -1.3943644 -1.670156 -3.073103 -3.9239771 -4.5651135 -4.8426304 -4.9349623 -4.9697556][-6.0977168 -4.2648172 -2.747395 -1.7264457 -1.1099901 -0.32291222 -0.6716013 -1.2673264 -1.5104713 -2.4650922 -2.9718308 -3.9305797 -4.0122995 -3.6192904 -3.6130481][-6.0259838 -4.4180584 -2.7575226 -2.2656517 -2.0239382 -0.88501358 -0.80218649 -1.7112417 -2.642571 -3.5108385 -3.410181 -3.5405846 -3.5239391 -2.8336091 -2.3618636][-7.6512694 -6.0074916 -4.0183945 -2.9781384 -2.5705709 -1.9870062 -1.91786 -2.3531647 -3.2076864 -3.7239544 -3.6342573 -3.91767 -4.0700045 -3.4268475 -2.8851852]]...]
INFO - root - 2017-12-15 12:20:57.917762: step 11610, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 58h:22m:35s remains)
INFO - root - 2017-12-15 12:21:04.296098: step 11620, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 56h:56m:49s remains)
INFO - root - 2017-12-15 12:21:10.741315: step 11630, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 55h:19m:53s remains)
INFO - root - 2017-12-15 12:21:17.153872: step 11640, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 56h:12m:28s remains)
INFO - root - 2017-12-15 12:21:23.542233: step 11650, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 56h:27m:33s remains)
INFO - root - 2017-12-15 12:21:29.903372: step 11660, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 58h:01m:10s remains)
INFO - root - 2017-12-15 12:21:36.358308: step 11670, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 55h:59m:47s remains)
INFO - root - 2017-12-15 12:21:42.813780: step 11680, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 56h:15m:06s remains)
INFO - root - 2017-12-15 12:21:49.263173: step 11690, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 56h:12m:45s remains)
INFO - root - 2017-12-15 12:21:55.616967: step 11700, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.631 sec/batch; 56h:12m:16s remains)
2017-12-15 12:21:56.116482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3891516 -4.8091221 -4.8742085 -5.8936973 -6.1833825 -6.9155307 -7.6133342 -7.7260466 -6.9343076 -5.9789906 -5.6517453 -5.3659468 -5.2701759 -5.4995351 -5.6733055][-5.0031142 -4.7133207 -4.925128 -5.19097 -5.8186679 -7.0797176 -7.8931928 -8.3954983 -7.855196 -6.9961548 -6.7158909 -6.2779918 -6.3104429 -5.9729476 -6.100163][-6.2431488 -5.6062155 -4.9713087 -4.3202448 -4.2768087 -5.0137711 -5.9749236 -7.0131941 -6.9796085 -6.9682627 -6.9800172 -7.2919703 -7.5411887 -7.3399043 -6.9914832][-7.0858011 -6.0253277 -4.9048986 -3.2968507 -2.3058386 -2.1252565 -2.3729115 -3.2886887 -3.244709 -3.8358924 -4.6361961 -5.7256141 -6.715343 -7.4040241 -7.4625764][-6.7956543 -5.104301 -3.4619498 -1.6262689 -0.89562368 -0.00045490265 0.22117186 -0.11430216 -0.18170166 -0.54197216 -1.601799 -3.5584974 -5.179708 -6.5137262 -6.7894979][-6.1363049 -4.4798317 -2.6188221 -0.91484737 0.3946414 1.2639394 1.7089047 1.8849339 2.0840917 1.6414762 0.097308636 -2.3997869 -4.3051977 -5.6355314 -6.0581121][-5.5969639 -3.8894854 -1.8379292 -0.21630049 1.5452037 2.4263053 2.8628535 3.0876412 3.2567754 2.6055274 0.65301752 -1.6763487 -3.2110815 -4.8497791 -5.5768223][-5.0886359 -3.4008622 -2.0972128 0.068245411 1.6735349 2.500442 3.0587544 3.1109653 3.1292291 2.4603038 0.861979 -1.0230703 -2.7788134 -4.5880337 -5.61511][-4.6039252 -3.658287 -2.5918055 -1.2914109 -0.46409035 0.41678667 0.91520357 1.096992 1.3618979 0.51003027 -1.2145143 -2.8298554 -4.5752583 -5.871161 -6.5587492][-4.9656911 -4.7433214 -3.9704068 -3.0418653 -1.9219284 -1.4488854 -1.1816077 -0.62327623 -0.94101667 -1.8181486 -3.412324 -4.3716626 -6.0011945 -6.8288264 -7.6657372][-6.5817003 -6.5796967 -6.3758755 -5.2378869 -4.3437891 -3.8613005 -3.215332 -3.2554541 -3.8155732 -4.1443477 -5.4117613 -5.8222828 -6.9057512 -7.2907133 -7.3522716][-8.3160667 -8.1993685 -8.0256929 -7.2226338 -6.5344191 -5.7070327 -5.0654016 -5.3879089 -5.6094375 -5.8596773 -7.0768867 -7.5241113 -8.2590618 -8.0600586 -7.7801623][-8.6300564 -9.1669474 -9.3816233 -8.7074337 -7.86427 -6.7372541 -6.0055242 -5.9915433 -6.1828237 -6.23048 -7.3088508 -7.2491531 -7.7384992 -7.5895867 -7.6813092][-8.7891989 -9.576643 -9.2301273 -8.88546 -8.1853285 -6.9559779 -6.1714516 -5.7575235 -5.8042502 -5.6753788 -6.1403737 -5.9600286 -6.503449 -6.8896837 -6.5438862][-8.9549971 -9.0081758 -8.9553471 -8.7837543 -8.1588373 -7.2935109 -6.6156793 -6.4134536 -5.8724117 -5.6939783 -5.7229166 -5.7676649 -5.7003231 -5.9513507 -5.9074178]]...]
INFO - root - 2017-12-15 12:22:02.497713: step 11710, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 57h:13m:27s remains)
INFO - root - 2017-12-15 12:22:08.977531: step 11720, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 57h:15m:00s remains)
INFO - root - 2017-12-15 12:22:15.350160: step 11730, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 55h:56m:55s remains)
INFO - root - 2017-12-15 12:22:21.770068: step 11740, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 56h:31m:30s remains)
INFO - root - 2017-12-15 12:22:28.185435: step 11750, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 57h:40m:46s remains)
INFO - root - 2017-12-15 12:22:34.589653: step 11760, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 59h:28m:12s remains)
INFO - root - 2017-12-15 12:22:41.006881: step 11770, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 57h:28m:00s remains)
INFO - root - 2017-12-15 12:22:47.425762: step 11780, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 57h:10m:13s remains)
INFO - root - 2017-12-15 12:22:53.864571: step 11790, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 56h:51m:31s remains)
INFO - root - 2017-12-15 12:23:00.190762: step 11800, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 56h:44m:28s remains)
2017-12-15 12:23:00.735943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8515172 -5.9847503 -6.3344965 -6.8279552 -6.7227788 -6.3994837 -6.1688066 -5.4867816 -5.5811682 -6.2484341 -7.4012637 -7.856916 -8.0054665 -7.9940853 -7.5675826][-5.3898773 -6.7010078 -7.4452176 -8.1459217 -7.9886417 -7.7742705 -7.0419874 -5.6630383 -4.8971357 -5.4300609 -6.74891 -7.8994031 -9.1037083 -9.3600264 -9.1918154][-4.4334149 -5.570385 -6.5438356 -7.662199 -8.070776 -7.9466262 -6.8609648 -4.8432379 -3.3867121 -3.6515832 -4.1652322 -5.3039112 -6.3468919 -7.5254154 -7.6518927][-3.6474566 -4.8014135 -5.3601403 -5.7888565 -6.3186545 -5.5297832 -4.7474303 -3.3846235 -1.7659059 -2.1509018 -2.9850221 -4.5631762 -5.3794527 -6.5034976 -7.0631104][-5.1107688 -4.8976297 -3.7461228 -3.7081671 -3.7939305 -2.2617154 -0.95552206 0.47341204 0.94828558 -0.45258045 -1.5074754 -3.2469335 -4.2735043 -6.1715355 -6.9230738][-5.0511665 -4.5485106 -3.6587334 -2.3512859 -0.86263132 1.5979667 3.0364356 3.7073216 4.088697 1.6853242 -0.80744457 -3.4212074 -4.7412009 -6.1144609 -6.6808467][-4.8450956 -3.519949 -2.2075868 -0.30707073 1.9099793 4.1672826 5.54966 5.8999162 5.3545547 2.2012534 -0.89260912 -3.870971 -5.39557 -6.6172466 -6.1482277][-3.7455258 -2.6348062 -1.2449956 0.72621107 3.262259 5.8485589 7.2879395 7.4009185 6.949245 3.2971196 -0.47421932 -3.7619827 -5.68521 -7.194593 -7.047718][-4.1013274 -2.5409346 -2.4872913 -0.4898715 1.707396 3.5747066 4.9304433 5.5485225 5.4983735 2.4472852 -1.0238676 -4.0474482 -5.7141838 -7.1457248 -6.8254948][-4.6056552 -4.0181327 -2.9509435 -1.6472435 -0.86064386 0.683269 1.811789 2.3275084 2.2530074 -0.52272463 -3.0708961 -5.3660789 -7.2374692 -8.3876953 -7.7702727][-6.0554619 -5.6822948 -5.752667 -4.9835277 -4.0079756 -3.1024399 -2.2306232 -2.2566323 -2.1508431 -4.1645613 -6.2131882 -7.2304816 -8.4828091 -9.323514 -8.9908409][-6.9376764 -7.2133379 -7.2342768 -6.9220352 -6.6979933 -6.0573134 -5.73434 -5.4024258 -5.0834789 -6.382689 -6.7762947 -7.7741909 -8.9764185 -8.757803 -8.3962746][-8.01867 -8.3603249 -7.9919219 -7.3941445 -6.7837029 -6.3721485 -6.21963 -6.5650692 -7.2035294 -7.8592372 -8.3872471 -8.0674772 -8.1006641 -7.8516731 -7.4991708][-7.7870994 -8.361474 -7.8946757 -7.19295 -6.5617838 -5.7648983 -5.5533867 -5.3949795 -5.790822 -6.9386072 -7.4495611 -7.7282791 -8.01673 -7.4780169 -7.3500576][-9.0475512 -8.76147 -8.27015 -7.6492782 -7.1261349 -6.6516485 -6.306766 -5.9382768 -6.188724 -6.4414577 -6.4937091 -6.7571559 -6.9187012 -6.7726316 -6.6147661]]...]
INFO - root - 2017-12-15 12:23:07.191547: step 11810, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 56h:32m:20s remains)
INFO - root - 2017-12-15 12:23:13.630153: step 11820, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 55h:59m:27s remains)
INFO - root - 2017-12-15 12:23:20.046145: step 11830, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 56h:37m:46s remains)
INFO - root - 2017-12-15 12:23:26.572474: step 11840, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 56h:13m:27s remains)
INFO - root - 2017-12-15 12:23:32.948113: step 11850, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 55h:42m:54s remains)
INFO - root - 2017-12-15 12:23:39.304776: step 11860, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 58h:57m:50s remains)
INFO - root - 2017-12-15 12:23:45.721354: step 11870, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 56h:56m:00s remains)
INFO - root - 2017-12-15 12:23:52.138931: step 11880, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 57h:06m:54s remains)
INFO - root - 2017-12-15 12:23:58.525649: step 11890, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 56h:29m:11s remains)
INFO - root - 2017-12-15 12:24:04.945740: step 11900, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 57h:36m:01s remains)
2017-12-15 12:24:05.451216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3661885 -1.7508821 -1.9994631 -2.2403407 -2.3639054 -1.941174 -1.5021615 -1.3659606 -1.2516308 -2.1554589 -3.1931944 -4.7413588 -5.6877851 -6.6119084 -7.0570974][-2.3994293 -2.3230762 -2.5704522 -2.5380878 -2.6739163 -2.3670559 -2.0229945 -1.8788977 -1.7490363 -2.7359395 -3.58217 -5.1314907 -6.0542531 -6.97882 -7.3581324][-2.0634294 -1.7324915 -1.8118939 -1.7623048 -1.9653516 -1.3610048 -0.9089942 -0.94331312 -1.0134072 -2.0338349 -2.9018898 -4.5348654 -5.352047 -6.3088942 -6.7546239][-1.9929814 -1.7901087 -1.6252894 -1.0506601 -0.81847191 -0.10050631 0.53366661 0.56840134 0.20896435 -0.94187593 -2.1106005 -3.8838568 -4.9542742 -5.8973951 -6.4099236][-2.8058939 -2.3969817 -1.9706769 -1.3161154 -0.44803619 0.80456161 1.7194986 1.6501589 1.2832212 0.025668621 -1.5624242 -3.7050266 -4.7205873 -5.6936755 -6.335043][-3.2258921 -2.9423237 -2.6367331 -1.2548575 0.19990015 1.5503454 2.4073315 2.5075693 2.3716908 0.90424919 -0.80894232 -3.0029473 -4.4563951 -5.5116835 -6.2547903][-3.9718585 -3.309473 -2.4905238 -0.89216518 0.85736561 2.4844923 3.5560751 3.7500753 3.6175652 1.9663219 -0.34879017 -2.9370313 -4.3019056 -5.2815261 -5.8140531][-3.5072427 -2.9242511 -2.0958128 -0.30117989 1.5717916 2.9227104 3.5566244 3.5479403 3.3814564 1.8982334 -0.064193249 -2.3775053 -3.7533991 -5.0327883 -5.6806121][-2.8746061 -2.4732833 -2.0059614 -0.85147858 0.33403587 1.8770132 2.5605574 2.7379284 2.5931549 1.3370733 -0.22135019 -2.6832132 -3.9136934 -4.9415808 -5.3519306][-2.5443134 -2.1599593 -2.0047374 -1.2283359 -0.39799309 0.70026112 1.4725227 1.4691019 1.1992102 -0.047871113 -1.4911218 -2.9647679 -3.8081017 -4.5454931 -5.1366405][-3.2104144 -3.2055597 -2.6447992 -1.9109955 -1.3047552 -0.47746658 0.027145386 -0.12819719 -0.56433678 -1.455843 -2.6199493 -3.4270048 -4.0929017 -4.1398578 -4.232326][-4.2760849 -3.5784965 -3.0624337 -2.658175 -2.2620969 -1.8633595 -1.7163591 -1.815927 -2.025631 -2.7957511 -3.3550835 -4.2238817 -4.189043 -4.3975267 -4.4168358][-5.7334528 -5.4061213 -4.469758 -3.7794981 -3.4028959 -2.9578128 -2.9488468 -3.1086125 -3.6467223 -4.0603819 -4.5678434 -4.9324894 -4.9589691 -5.0612803 -4.5351849][-5.7364893 -5.2780619 -4.5398211 -3.9985731 -3.5677266 -3.0862045 -3.008472 -3.1856632 -4.01007 -4.6699581 -4.9347534 -5.0609531 -5.3649173 -5.0457144 -4.8562222][-5.9750032 -5.503871 -4.6504021 -4.3088188 -4.2558227 -4.0947876 -4.1158538 -4.3807383 -4.6333 -4.6070433 -4.7738719 -4.8242908 -4.961648 -5.2294455 -5.4682822]]...]
INFO - root - 2017-12-15 12:24:11.888625: step 11910, loss = 0.33, batch loss = 0.22 (12.2 examples/sec; 0.653 sec/batch; 58h:11m:17s remains)
INFO - root - 2017-12-15 12:24:18.412985: step 11920, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 57h:27m:42s remains)
INFO - root - 2017-12-15 12:24:24.852056: step 11930, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 56h:31m:51s remains)
INFO - root - 2017-12-15 12:24:31.255841: step 11940, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 58h:18m:45s remains)
INFO - root - 2017-12-15 12:24:37.677877: step 11950, loss = 0.31, batch loss = 0.20 (13.0 examples/sec; 0.615 sec/batch; 54h:44m:54s remains)
INFO - root - 2017-12-15 12:24:44.065371: step 11960, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 58h:00m:48s remains)
INFO - root - 2017-12-15 12:24:50.465618: step 11970, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 57h:03m:32s remains)
INFO - root - 2017-12-15 12:24:56.833915: step 11980, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 58h:27m:27s remains)
INFO - root - 2017-12-15 12:25:03.220810: step 11990, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:56m:23s remains)
INFO - root - 2017-12-15 12:25:09.624857: step 12000, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 57h:25m:48s remains)
2017-12-15 12:25:10.095548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9363825 -4.273941 -4.5414748 -4.7537613 -5.2836566 -5.4120936 -5.6712866 -5.0713949 -4.110949 -3.6630769 -3.0677848 -4.1349421 -4.8135519 -6.0560589 -7.0663815][-3.8250692 -4.0949678 -4.5896254 -4.96929 -5.7342052 -6.1089411 -6.3069944 -5.888907 -4.9763412 -4.2006078 -3.1030617 -3.8914156 -4.221467 -5.0410881 -5.9909697][-4.903379 -5.0956163 -5.1594181 -5.45853 -6.2311516 -6.4927607 -6.6440454 -6.5856352 -5.8512406 -5.4508824 -4.844368 -5.4223347 -5.4474125 -5.7892342 -6.4422345][-5.6945038 -5.5297918 -5.3278379 -4.9210949 -4.9139128 -4.9984665 -4.9865756 -4.9982653 -4.6909132 -4.764348 -4.6191273 -5.5449362 -5.8897219 -6.5038643 -7.0255866][-5.3616447 -4.4759083 -3.8046141 -3.1474485 -2.8763924 -2.5168228 -2.129077 -2.0481429 -1.7782207 -1.909328 -2.0187836 -3.4488134 -4.4041824 -5.748662 -6.6996422][-4.6580153 -3.874378 -3.0180607 -1.9202209 -0.94207048 -0.030698776 0.69042683 1.1218843 1.5711164 1.249568 0.43801022 -1.7159438 -3.2498899 -4.9553013 -6.1627293][-3.8306057 -2.6111932 -1.4743056 -0.40363455 0.46817684 1.7322836 2.6179113 3.0071621 3.3182707 2.825511 1.736578 -0.75201464 -2.4936142 -4.4044991 -5.8702369][-2.6658835 -1.7985525 -0.58111477 0.80030632 2.2522144 3.1582823 3.766345 4.3051796 4.6119375 3.729929 2.5867853 0.028669357 -1.7313242 -3.6909811 -5.1626587][-1.7230296 -1.1236477 -0.54842281 0.47871208 1.4423084 2.4987087 3.3304548 3.6761522 3.8997183 3.0545559 1.9335127 -0.67826557 -2.6331897 -4.4983606 -5.9537373][-2.9828453 -2.123826 -1.2776356 -0.082027912 0.61965084 1.1795044 1.3955612 1.5058317 1.7206812 0.64205647 -0.14063311 -2.2459755 -3.8550961 -5.4458179 -6.8256254][-4.0317574 -4.1342688 -3.8724372 -2.4925885 -1.4666486 -0.81300926 -0.43614149 -0.61611319 -0.84107018 -2.0768652 -2.6701159 -4.3043156 -5.5276475 -6.7238197 -7.6544929][-6.9645276 -7.0487585 -6.612711 -5.5265083 -4.5192995 -3.6120715 -3.0200639 -3.3006234 -3.5785651 -4.4854808 -4.9650888 -6.0373793 -7.036747 -7.8693528 -8.6302338][-9.00329 -9.6070824 -9.819726 -9.0875416 -8.22784 -7.0378718 -6.0005355 -5.9474883 -6.0845432 -6.5553222 -6.7863965 -6.9395227 -7.1245685 -7.5105271 -7.8434968][-8.8422089 -8.9383068 -9.04047 -9.1848574 -8.9169941 -8.0194235 -7.1792455 -6.7885795 -6.537631 -6.4701624 -6.2774224 -6.1458559 -5.9867711 -6.2171564 -6.5055966][-8.5706568 -8.2909365 -7.8933697 -8.0809155 -8.1329327 -7.6071739 -6.9839883 -6.6182904 -6.5978928 -6.428772 -5.9968534 -5.4696388 -4.9842205 -5.0236387 -5.2009249]]...]
INFO - root - 2017-12-15 12:25:16.565205: step 12010, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 56h:03m:14s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:25:22.939199: step 12020, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:02m:35s remains)
INFO - root - 2017-12-15 12:25:29.266912: step 12030, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 55h:12m:12s remains)
INFO - root - 2017-12-15 12:25:35.643376: step 12040, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 56h:09m:02s remains)
INFO - root - 2017-12-15 12:25:41.980957: step 12050, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 56h:28m:10s remains)
INFO - root - 2017-12-15 12:25:48.363573: step 12060, loss = 0.28, batch loss = 0.16 (13.1 examples/sec; 0.610 sec/batch; 54h:20m:22s remains)
INFO - root - 2017-12-15 12:25:54.856265: step 12070, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 57h:37m:42s remains)
INFO - root - 2017-12-15 12:26:01.244660: step 12080, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 57h:29m:50s remains)
INFO - root - 2017-12-15 12:26:07.649367: step 12090, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 56h:27m:16s remains)
INFO - root - 2017-12-15 12:26:14.036452: step 12100, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 55h:42m:17s remains)
2017-12-15 12:26:14.492430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5907912 -3.6625562 -3.5091171 -3.4574032 -3.4728527 -2.6543255 -2.2508197 -2.372169 -2.250638 -3.417541 -4.0735483 -5.0568085 -5.8000741 -6.3595371 -6.7780733][-4.364697 -4.9221854 -5.341753 -4.713171 -4.3583584 -4.329915 -4.1694212 -3.6898665 -3.2606959 -4.629077 -5.4813976 -6.5048895 -7.154706 -7.4529986 -7.8625231][-5.420886 -5.772707 -6.4272976 -5.9997878 -5.2633 -4.6878304 -4.2088509 -4.3259268 -4.51497 -5.5891466 -6.1883125 -7.413743 -8.3825455 -8.5408888 -8.3745022][-6.2344379 -6.2248311 -6.3200979 -5.171875 -4.4206219 -3.5469818 -2.6211553 -2.6584668 -2.9166555 -4.3691759 -5.2838907 -6.6604824 -7.6676702 -7.6453328 -7.5084071][-8.1050272 -6.9336629 -5.8114543 -4.6433282 -3.3288221 -1.6016293 -0.47458553 -0.59435177 -0.79335928 -2.3571739 -3.2465158 -5.0948696 -6.3294334 -6.6883078 -6.9216566][-8.68726 -8.2612848 -7.8246965 -5.2061634 -2.2829075 0.091331005 1.8114109 1.677742 1.1500692 -0.92321587 -2.3785596 -4.4415331 -5.9236789 -6.7551794 -7.0348825][-9.2800894 -7.8625932 -5.6483197 -3.5199924 -0.88984585 1.9063587 3.6869545 3.9847326 3.5734406 1.1059189 -0.23719454 -2.5446954 -4.2839689 -5.036623 -5.3019919][-8.1626434 -7.1048541 -6.182847 -2.3368344 1.4855242 3.8817654 6.0617428 6.0467262 5.2087574 2.631134 1.2752161 -1.0611906 -2.7038217 -3.5479183 -3.6684003][-8.4129486 -7.6114054 -6.1021633 -3.4691129 -1.1030445 1.0055323 3.188364 4.2848969 4.9813042 2.3801956 0.4496603 -1.8606749 -3.4671531 -4.15591 -4.2752972][-9.0117455 -8.0667133 -7.4889789 -4.9829535 -2.539443 -0.9013238 0.57059574 1.2711535 1.8492994 -0.38078403 -2.2070498 -3.8686035 -5.5405388 -5.698103 -5.1888576][-9.610815 -9.5924778 -9.535964 -7.9225216 -6.3075724 -3.9293582 -2.334723 -2.4681339 -2.2130446 -3.3430781 -3.8710167 -4.9217596 -6.5843067 -6.6928725 -6.2053747][-9.8824224 -9.7755032 -9.53341 -8.6727705 -7.7265954 -6.1747527 -5.0137262 -4.0927353 -3.2848949 -5.1686945 -5.6870074 -5.6612206 -6.3531427 -6.6880317 -7.1685829][-10.256561 -10.073648 -9.763752 -9.1815357 -8.3696165 -7.4261141 -6.8168926 -6.3324037 -5.9622431 -6.4946809 -6.5241718 -6.6487794 -6.4158535 -6.3397303 -6.2917137][-8.2889338 -8.7485619 -8.52411 -8.0687551 -7.4204469 -6.6582842 -6.0776629 -6.2300854 -6.4072032 -6.7046981 -6.7015057 -6.5751371 -6.4791927 -6.1822987 -5.8775072][-7.3820548 -7.436985 -8.1470547 -8.3196669 -7.7612429 -7.0442929 -6.4053097 -6.6728983 -6.7688475 -6.8169012 -7.3352156 -7.57097 -7.6379862 -7.2067661 -6.7528305]]...]
INFO - root - 2017-12-15 12:26:20.825853: step 12110, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 56h:33m:14s remains)
INFO - root - 2017-12-15 12:26:27.141409: step 12120, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 56h:14m:35s remains)
INFO - root - 2017-12-15 12:26:33.490227: step 12130, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 57h:11m:40s remains)
INFO - root - 2017-12-15 12:26:39.851631: step 12140, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 57h:15m:50s remains)
INFO - root - 2017-12-15 12:26:46.309034: step 12150, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 55h:24m:23s remains)
INFO - root - 2017-12-15 12:26:52.746243: step 12160, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 57h:42m:43s remains)
INFO - root - 2017-12-15 12:26:59.200274: step 12170, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 57h:20m:25s remains)
INFO - root - 2017-12-15 12:27:05.522437: step 12180, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:54m:39s remains)
INFO - root - 2017-12-15 12:27:11.884495: step 12190, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 56h:27m:58s remains)
INFO - root - 2017-12-15 12:27:18.199188: step 12200, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 56h:24m:32s remains)
2017-12-15 12:27:18.712280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0750303 -5.4141212 -6.0321026 -6.211802 -6.2164745 -5.910603 -5.36601 -4.8005724 -4.069334 -3.9438174 -5.0806971 -6.5607529 -7.9388361 -7.9324393 -7.9444461][-5.0122776 -5.3526793 -5.827733 -6.5242777 -6.929812 -6.7723379 -6.3240619 -5.5790787 -4.9744987 -4.5977497 -5.2610192 -7.053596 -8.6333265 -8.78215 -9.1898508][-2.6809573 -3.109005 -3.6846371 -4.479713 -4.7078114 -4.7627158 -4.8809156 -4.5220652 -4.1059637 -3.819067 -4.7262778 -6.3794832 -7.6224389 -7.5267653 -7.6558256][-1.7084293 -1.96731 -2.112649 -2.409296 -2.6844697 -2.8939309 -2.7366228 -2.6023259 -2.4290314 -2.2728243 -3.3210998 -5.4370794 -7.2491579 -7.229887 -7.4381905][-1.0501442 -1.2403574 -1.3167224 -1.1760454 -0.732285 -0.33790398 -0.078886509 -0.099748611 -0.323534 -0.73815918 -2.1704054 -4.3770738 -6.0756946 -6.1220922 -6.4999313][-2.2091637 -2.0731049 -1.7212119 -0.77948713 0.59326363 1.600378 2.3563671 2.7489614 2.7291784 1.8617115 -0.30291271 -2.9773097 -4.9297132 -4.9336424 -5.1131434][-3.5790849 -3.5689034 -2.8640199 -0.91808224 1.2651081 2.8708887 4.03067 4.1257277 3.8951111 2.8465691 0.23730898 -2.9721241 -5.1738553 -5.3879595 -5.6428871][-4.2845144 -3.7863421 -2.8245163 -1.1406536 1.0273247 3.3100863 4.8835268 5.2984543 4.7200403 3.2619066 0.72704887 -2.5578661 -5.1372094 -5.6779919 -6.1598449][-4.714066 -4.2271791 -2.8316827 -1.6338534 0.2755537 2.461421 3.9599752 4.604022 4.5675573 3.1792307 0.46223068 -2.834568 -5.3715973 -6.0244942 -6.372097][-6.1325293 -5.3654442 -4.2383566 -2.7638011 -1.2104635 0.59960556 2.1031036 2.3825998 2.1336842 0.98561764 -1.1832156 -4.1790504 -6.3415556 -6.7149057 -7.192585][-6.5642 -6.2851467 -5.1503744 -3.7160845 -2.3105536 -1.2609148 -0.30285835 0.097997189 -0.11888695 -1.3169584 -3.2761521 -5.9658208 -7.3909693 -7.4509377 -7.7945242][-6.7190695 -6.3520212 -5.5615082 -4.1821203 -3.1466279 -2.355289 -2.0931478 -2.1198549 -2.2484102 -2.7861824 -4.1519718 -6.2932863 -7.4653435 -7.4321227 -7.8379207][-7.2274942 -6.5307941 -5.5608597 -4.5135832 -3.7056625 -3.6071258 -3.4763546 -3.6720166 -4.0639849 -4.9049454 -5.9355097 -7.0654035 -7.6581092 -7.5625248 -7.6870766][-7.4436727 -6.5976706 -5.7963619 -4.7436008 -4.2017937 -4.2202311 -4.4736338 -5.224587 -5.6441479 -6.0747366 -6.6138864 -6.9772606 -7.4957395 -7.2019958 -7.0496006][-7.4879537 -7.1407127 -6.1691332 -5.4408531 -5.1809826 -5.1621161 -5.7344685 -6.3636122 -6.8777041 -7.3983307 -7.6475348 -7.5035291 -6.9509916 -6.7178664 -6.6084766]]...]
INFO - root - 2017-12-15 12:27:25.168926: step 12210, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 56h:18m:57s remains)
INFO - root - 2017-12-15 12:27:31.559344: step 12220, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 58h:10m:16s remains)
INFO - root - 2017-12-15 12:27:38.078272: step 12230, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 57h:10m:54s remains)
INFO - root - 2017-12-15 12:27:44.503950: step 12240, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 56h:59m:17s remains)
INFO - root - 2017-12-15 12:27:50.871725: step 12250, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 56h:18m:38s remains)
INFO - root - 2017-12-15 12:27:57.350663: step 12260, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 57h:11m:57s remains)
INFO - root - 2017-12-15 12:28:03.758174: step 12270, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 59h:53m:09s remains)
INFO - root - 2017-12-15 12:28:10.224042: step 12280, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 58h:14m:22s remains)
INFO - root - 2017-12-15 12:28:16.636360: step 12290, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 55h:59m:11s remains)
INFO - root - 2017-12-15 12:28:23.047434: step 12300, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:56m:23s remains)
2017-12-15 12:28:23.573945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0841732 -3.8978233 -3.8112404 -3.8572295 -3.8734226 -3.9970057 -3.6347027 -3.5704551 -3.3130536 -5.0572491 -5.3553429 -6.5983868 -7.0832977 -7.4132752 -7.8623285][-4.3505926 -4.13771 -3.9770832 -3.848892 -3.7714643 -3.9481869 -3.7939031 -3.8719532 -3.5420108 -4.8819623 -4.9615364 -6.1672678 -6.55526 -6.654264 -6.7670245][-5.5113811 -5.5246305 -4.9677076 -4.342073 -3.9630461 -4.0171208 -3.926414 -4.062891 -4.115016 -5.623333 -5.5876303 -6.5825963 -6.9135885 -7.4622831 -7.5148435][-6.3303146 -5.6973429 -5.4346142 -4.33762 -3.1800075 -2.4181037 -2.0006099 -1.8256578 -1.856236 -3.6417685 -4.2715149 -5.6948934 -6.3542156 -6.7929149 -6.9793725][-5.7726717 -4.5376177 -3.3841472 -2.4880104 -2.1719718 -0.92353773 0.069125652 0.3155036 0.28653669 -1.2183318 -1.7999887 -3.4221511 -4.3059192 -5.3314629 -6.1297126][-5.1230521 -3.5645323 -3.2578182 -1.7804003 -0.54597187 0.094384193 0.79991341 1.3873053 1.5104218 -0.052267551 -0.74684143 -2.3762836 -3.2994876 -4.2669954 -4.9841976][-4.6245661 -3.7255392 -2.4562674 -0.48568249 0.16058922 1.0212269 1.6779423 1.7202883 1.7667761 0.067314148 -0.51261187 -2.1398726 -2.89991 -3.8069487 -4.2900348][-3.9591904 -2.3345923 -1.4105272 -0.4006362 1.0665379 2.5820675 2.9561996 3.1547184 3.0114546 0.61400604 -0.75002718 -2.2744555 -2.9960651 -4.0756712 -4.17365][-2.8427291 -2.4950662 -1.71661 0.24125242 0.70834637 1.2914238 2.0460672 1.9739065 1.59655 0.13727093 -0.61916971 -2.5059962 -3.5437121 -4.3461194 -4.7724972][-4.0639381 -3.426271 -2.7889256 -1.5370154 -0.070653915 0.86359787 0.79346371 0.60664845 0.58759212 -1.1608777 -2.0968142 -3.5488405 -4.2784777 -5.1025915 -5.2994442][-5.5237865 -5.3878355 -5.1634665 -4.1824393 -3.2647042 -2.3867087 -1.4428649 -1.3407807 -1.6220922 -3.4654617 -3.9211526 -4.5347891 -4.9011941 -5.5556922 -5.6760635][-8.8515444 -8.063035 -7.2316031 -6.2320027 -5.2051539 -4.1359682 -3.7113185 -3.9247994 -3.9539845 -4.8076186 -5.2860417 -5.8774767 -5.73396 -5.7283678 -5.93096][-9.2926941 -8.88905 -8.4221277 -7.0717182 -6.3179092 -5.7019367 -5.0403423 -4.6339417 -4.6343784 -5.162662 -5.4034667 -5.7750874 -5.8610487 -5.7339044 -5.2281561][-6.8063087 -7.0308313 -7.1134048 -6.5013437 -5.6771221 -5.0459375 -4.9172077 -4.6151457 -4.6665945 -5.0412483 -5.1970034 -5.4809179 -5.4358721 -5.4890294 -5.2460418][-6.5640125 -6.0295234 -5.8036261 -5.9897776 -6.0114512 -5.9148183 -5.8820548 -5.9965358 -6.1259837 -6.2798586 -6.2889552 -6.2603316 -6.1543837 -6.1538033 -5.9017906]]...]
INFO - root - 2017-12-15 12:28:29.871134: step 12310, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 56h:00m:37s remains)
INFO - root - 2017-12-15 12:28:36.257618: step 12320, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 55h:31m:26s remains)
INFO - root - 2017-12-15 12:28:42.743550: step 12330, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 56h:17m:56s remains)
INFO - root - 2017-12-15 12:28:49.251998: step 12340, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 56h:11m:16s remains)
INFO - root - 2017-12-15 12:28:55.692601: step 12350, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 56h:32m:26s remains)
INFO - root - 2017-12-15 12:29:02.095161: step 12360, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 57h:50m:23s remains)
INFO - root - 2017-12-15 12:29:08.564159: step 12370, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 58h:08m:02s remains)
INFO - root - 2017-12-15 12:29:14.927330: step 12380, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 56h:08m:03s remains)
INFO - root - 2017-12-15 12:29:21.311769: step 12390, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 56h:06m:07s remains)
INFO - root - 2017-12-15 12:29:27.684661: step 12400, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 57h:08m:47s remains)
2017-12-15 12:29:28.178067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0986357 -4.0343442 -3.8045242 -3.6448822 -3.3234239 -2.6167507 -2.0307813 -1.5065398 -1.1140771 -2.711781 -3.8889818 -5.3332243 -6.30416 -7.4792647 -8.28402][-3.3502712 -3.6949346 -3.9217777 -3.9384489 -3.7516365 -3.230773 -2.6509137 -2.1189775 -1.7155938 -3.1846442 -4.3012018 -5.6931777 -6.7177286 -7.637661 -8.37029][-2.7263994 -2.6662059 -2.8768702 -2.7165527 -2.1871247 -1.7856302 -1.4333291 -1.0956564 -0.98599958 -2.838531 -4.2993011 -5.7514992 -6.6940131 -7.8267279 -8.6348829][-2.154377 -2.2207975 -2.172163 -1.6439123 -1.1198144 -0.571548 -0.06746912 0.043236732 -0.092699051 -2.1955891 -3.90392 -5.7724738 -6.9420156 -7.6923809 -8.3985682][-2.6810107 -1.8801332 -1.1383786 -0.63763618 -0.074570656 0.64577246 1.226213 1.1303802 0.80236959 -1.5522881 -3.4461427 -5.3037906 -6.5593557 -7.585218 -8.29845][-2.8698545 -2.2142735 -1.4744682 -0.38527536 0.64369059 1.7206798 2.3979125 2.1611753 1.7182126 -0.93426609 -3.0983396 -5.08381 -6.3439331 -7.416079 -8.2887068][-3.3673515 -2.4329858 -1.2976274 0.061894894 1.4241586 2.5974498 3.2174239 3.1525588 2.7030492 -0.27817106 -2.7795238 -4.9729528 -6.1767173 -7.3589168 -8.3607645][-3.324543 -2.6104188 -1.6637411 -0.029850006 1.5946746 2.6147914 2.99476 3.0577598 2.7070527 -0.028492928 -2.352499 -4.7226286 -6.1369019 -7.2410979 -7.9566851][-3.9549105 -3.0159364 -2.1361132 -0.77067995 0.74889517 1.6976066 1.9972215 1.9949222 1.8286576 -0.7183094 -2.9947691 -4.9303007 -6.067111 -7.1307979 -7.9658389][-4.4493604 -3.9115591 -3.1433291 -1.7430921 -0.31090784 0.57047606 0.79291582 0.91662836 0.91513586 -1.640667 -3.4096651 -5.1957932 -6.232893 -7.2484303 -8.0226727][-5.7346473 -5.173502 -4.342896 -3.2372446 -2.2498622 -1.4018297 -1.0241117 -1.3460321 -1.618392 -3.6387277 -5.0178046 -6.0632315 -6.5854297 -7.454802 -7.9693503][-6.8352194 -6.26834 -5.1890306 -4.3524823 -3.630682 -2.8734798 -2.8048539 -3.2013955 -3.5560546 -5.0945873 -5.8661404 -6.4073 -6.5845623 -7.2005806 -7.5263181][-7.0108852 -6.3668771 -5.4712391 -4.6965346 -4.1196985 -3.7333891 -3.7539458 -4.0951347 -4.61423 -5.8247566 -6.4881082 -6.5512433 -6.5465536 -6.6818533 -6.6652412][-7.036839 -6.665947 -5.772594 -5.1789484 -4.8103547 -4.6861649 -4.9171505 -5.1052551 -5.205472 -5.8321352 -6.0096359 -6.1346722 -6.0979257 -6.2827845 -6.1785488][-7.6392341 -7.069541 -6.46539 -6.2409925 -6.0871134 -6.0687056 -6.2202387 -6.5924 -7.0032158 -6.8576636 -6.5088558 -6.2164927 -5.9650688 -5.9714255 -5.8583694]]...]
INFO - root - 2017-12-15 12:29:34.594707: step 12410, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 58h:47m:34s remains)
INFO - root - 2017-12-15 12:29:40.953663: step 12420, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 56h:02m:15s remains)
INFO - root - 2017-12-15 12:29:47.451842: step 12430, loss = 0.35, batch loss = 0.24 (12.1 examples/sec; 0.661 sec/batch; 58h:47m:33s remains)
INFO - root - 2017-12-15 12:29:53.788088: step 12440, loss = 0.26, batch loss = 0.14 (13.1 examples/sec; 0.611 sec/batch; 54h:19m:10s remains)
INFO - root - 2017-12-15 12:30:00.172327: step 12450, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 56h:41m:30s remains)
INFO - root - 2017-12-15 12:30:06.605200: step 12460, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 56h:27m:27s remains)
INFO - root - 2017-12-15 12:30:12.958399: step 12470, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.652 sec/batch; 57h:59m:15s remains)
INFO - root - 2017-12-15 12:30:19.339133: step 12480, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 56h:31m:53s remains)
INFO - root - 2017-12-15 12:30:25.661014: step 12490, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 55h:13m:06s remains)
INFO - root - 2017-12-15 12:30:31.936472: step 12500, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.614 sec/batch; 54h:35m:16s remains)
2017-12-15 12:30:32.462526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.96947 -6.2985287 -6.6726975 -6.51616 -5.6399813 -4.7687869 -3.2455306 -1.661727 -0.15081644 -0.35032988 -1.2499957 -4.3413715 -6.1579547 -8.5657187 -10.032847][-4.8713412 -5.6801395 -6.3582644 -6.8533654 -6.5952029 -5.4732671 -3.6383405 -1.4577956 0.037716389 -0.11984539 -1.3425927 -3.7480609 -5.6802626 -8.1772223 -9.8899632][-4.5442109 -5.314703 -5.5994215 -5.3147812 -4.7450943 -4.0315557 -2.916328 -1.5634718 -0.036162853 -0.29862881 -1.3632226 -3.6042023 -5.0736527 -7.0319285 -8.5931168][-4.7345185 -4.6454935 -4.478879 -3.9924903 -3.3063879 -2.3457489 -1.2627563 -0.71389627 0.027251244 -0.36463356 -1.2016907 -3.6781487 -5.0553923 -6.5618315 -7.4212008][-5.006443 -4.4085531 -3.6004558 -2.8551855 -2.1187344 -0.76792526 0.49683619 1.1607242 1.5095792 0.7021699 -0.57983923 -3.2448683 -4.8590636 -6.5234518 -7.6909966][-4.7809792 -3.7606664 -2.4942994 -1.5459309 -0.35433245 0.90477896 2.295547 2.8708081 3.3142009 2.2507148 0.61300039 -2.3852177 -4.3481245 -6.0869846 -7.231792][-4.0823 -2.7669449 -1.6778436 -0.68043756 0.50978041 1.5550208 2.9009271 3.7247415 4.2792907 3.0433593 1.3894296 -1.9502926 -4.1492367 -6.0971875 -7.265677][-3.5257344 -2.148632 -0.80260229 0.088793278 1.1884646 1.7993569 2.50882 3.4305882 4.2036777 3.1931787 1.341826 -1.8557177 -4.1458473 -6.338151 -7.5634408][-2.9777169 -1.6306305 -0.4789114 0.5707202 1.7862706 2.0824218 2.4741302 2.8853669 3.2073846 2.0864625 0.7042613 -2.0214319 -4.2448063 -6.44534 -8.0988188][-2.5897737 -1.5888786 -0.19167042 0.66158056 1.5858626 2.0747008 2.6754165 2.7388682 2.838923 1.0175681 -0.85189629 -3.2956667 -4.8809977 -6.4988875 -7.6558971][-2.2920055 -1.5649867 -0.64395475 0.26590824 1.2684207 1.380939 1.823523 1.9569125 1.9797883 0.14613533 -1.8644462 -4.1996493 -5.2642469 -6.6339378 -7.5790396][-3.0947113 -2.100482 -1.0686402 -0.2347331 0.49227858 0.45935583 0.389071 0.12774277 -0.351264 -1.1784868 -2.4214969 -4.1996326 -5.5411816 -6.9328709 -7.6656041][-4.6569157 -3.6354737 -2.4407721 -1.5732565 -0.55530167 -0.47744322 -0.81805611 -1.2204752 -1.6738591 -2.4532909 -3.2364531 -4.2406545 -5.3909245 -6.3492594 -7.2524076][-6.3885789 -5.8075876 -4.4596109 -3.5539379 -2.4026313 -1.8067884 -1.5547104 -1.6257 -2.1861744 -3.074079 -4.3611269 -5.3763905 -5.852891 -6.3815479 -6.4117494][-7.2613668 -6.6632137 -5.851882 -5.5351181 -4.5828342 -3.7659793 -3.2326164 -2.8165336 -3.4508281 -3.9305656 -4.929441 -5.9966431 -6.4791365 -6.5750904 -6.4447165]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 12:30:39.995067: step 12510, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 55h:14m:36s remains)
INFO - root - 2017-12-15 12:30:46.438009: step 12520, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 57h:08m:50s remains)
INFO - root - 2017-12-15 12:30:52.941875: step 12530, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.615 sec/batch; 54h:40m:50s remains)
INFO - root - 2017-12-15 12:30:59.408493: step 12540, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 57h:10m:13s remains)
INFO - root - 2017-12-15 12:31:05.806814: step 12550, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 56h:43m:09s remains)
INFO - root - 2017-12-15 12:31:12.132832: step 12560, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 55h:35m:44s remains)
INFO - root - 2017-12-15 12:31:18.485587: step 12570, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 55h:31m:24s remains)
INFO - root - 2017-12-15 12:31:24.934870: step 12580, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 57h:21m:23s remains)
INFO - root - 2017-12-15 12:31:31.353840: step 12590, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 57h:20m:05s remains)
INFO - root - 2017-12-15 12:31:37.801784: step 12600, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 58h:10m:33s remains)
2017-12-15 12:31:38.365401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5859084 -4.3105006 -4.0667677 -3.9419212 -4.0582643 -4.0358834 -3.5708494 -2.2942076 -1.1849856 -2.6216278 -3.5570951 -5.5744009 -6.1816721 -7.5564795 -7.8187575][-4.8408613 -5.0739117 -5.4332108 -4.9405184 -4.791656 -4.4591694 -4.1848974 -3.7935739 -2.5919833 -3.0130095 -3.4937263 -5.2873707 -6.4547815 -7.5996723 -8.27592][-4.5684304 -4.2687063 -4.6778612 -4.5291958 -4.2313495 -3.3182878 -2.2191463 -1.6948357 -1.4800787 -2.4051347 -2.7243881 -5.0538354 -6.1447473 -7.4119096 -7.9394827][-4.8066459 -4.3542662 -3.8316014 -3.2857585 -2.5676794 -1.8723149 -1.19765 -0.42185497 -0.0058345795 -1.3732681 -2.5525951 -4.1004992 -4.6603394 -6.2348404 -6.8493805][-5.4076986 -4.3708272 -3.2017207 -2.6363778 -1.8405786 -0.62751341 0.078205585 0.44647503 0.29467821 -1.6999841 -2.902339 -5.0323753 -5.3248453 -6.1838737 -6.25245][-3.6819108 -3.2857642 -2.7467036 -1.6814933 -0.80932856 0.057352066 0.69131374 1.2474794 1.4389296 -0.94914055 -2.9913716 -5.6099052 -7.1207604 -7.6994104 -6.9704928][-2.414741 -1.83916 -1.604013 -0.66040182 -0.202322 0.60717106 1.4575577 1.921072 2.1161242 0.045974255 -2.0373535 -5.1444912 -6.7424045 -8.2309036 -8.3739967][-2.1488414 -1.5861588 -1.1793766 -0.44636011 0.29835796 0.97551441 1.3957195 1.896018 2.1498003 0.22951221 -1.5342832 -4.891633 -6.4596291 -7.9627104 -7.9126287][-2.1477404 -2.102067 -1.7850595 -0.95793724 -0.06087923 0.63747978 1.2264824 1.5557261 1.3812714 -0.32132387 -1.9412961 -4.5179329 -6.1326351 -7.696157 -7.558042][-3.4487329 -2.6838775 -2.1181068 -1.6222882 -1.1987514 -0.35173893 0.550292 0.97297764 1.2404118 -0.88369751 -2.5742974 -4.6526241 -6.1678905 -7.1287613 -7.1699781][-4.2875018 -4.6052265 -4.6558733 -3.7759583 -3.4455867 -2.6629071 -1.8233123 -1.130415 -1.086103 -2.5499921 -3.7891905 -5.5854397 -6.9337854 -7.5725241 -7.0450506][-5.9900746 -5.9320431 -5.7930446 -5.3847756 -4.78905 -4.5509586 -4.3838339 -3.8876307 -3.1291709 -3.9463181 -5.0223 -5.8274255 -6.71533 -6.919106 -6.629591][-7.3079696 -7.4642653 -7.2921185 -6.9250751 -6.5906143 -6.1351647 -5.2122016 -4.5996618 -4.8851986 -5.036696 -5.7906694 -6.2894526 -6.1146135 -6.5108156 -5.9777303][-7.5189495 -7.4533863 -7.3956962 -7.0095015 -6.5884581 -6.1657209 -6.3266382 -6.0741339 -5.550436 -5.385253 -5.3976688 -5.8150826 -6.076179 -6.0973177 -5.5041504][-8.0968618 -7.9150467 -7.5520926 -7.113657 -6.9331894 -6.6645536 -6.6231346 -6.4674325 -7.0292282 -7.0102024 -6.7617249 -6.7553811 -6.26347 -6.1621957 -5.9507179]]...]
INFO - root - 2017-12-15 12:31:44.720095: step 12610, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 55h:37m:37s remains)
INFO - root - 2017-12-15 12:31:51.051112: step 12620, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 56h:53m:20s remains)
INFO - root - 2017-12-15 12:31:57.401217: step 12630, loss = 0.26, batch loss = 0.14 (13.1 examples/sec; 0.611 sec/batch; 54h:17m:18s remains)
INFO - root - 2017-12-15 12:32:03.782012: step 12640, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 55h:07m:44s remains)
INFO - root - 2017-12-15 12:32:10.179513: step 12650, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:49m:53s remains)
INFO - root - 2017-12-15 12:32:16.488873: step 12660, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 55h:22m:01s remains)
INFO - root - 2017-12-15 12:32:22.823572: step 12670, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 57h:14m:24s remains)
INFO - root - 2017-12-15 12:32:29.253352: step 12680, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 54h:42m:35s remains)
INFO - root - 2017-12-15 12:32:35.601421: step 12690, loss = 0.35, batch loss = 0.23 (12.8 examples/sec; 0.625 sec/batch; 55h:30m:48s remains)
INFO - root - 2017-12-15 12:32:42.025268: step 12700, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 55h:26m:32s remains)
2017-12-15 12:32:42.505796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3618488 -6.7649283 -6.4892921 -6.0671167 -4.9886866 -4.1622438 -3.4250107 -2.4409161 -2.269125 -2.6155205 -4.9686623 -5.6123924 -6.7110968 -7.8807216 -7.3608069][-4.9026289 -5.2932315 -5.3028164 -4.6017838 -4.3913565 -3.9103894 -2.9333806 -2.4914093 -2.3689022 -2.374033 -4.7466531 -5.6067429 -6.3978682 -7.3952274 -7.2509971][-3.6998739 -3.9859881 -4.3113079 -3.675736 -2.8738327 -2.2799392 -1.9690585 -1.9958544 -1.8366785 -1.2208228 -3.5555897 -4.218111 -5.269577 -6.265749 -6.051517][-2.3674722 -2.6109753 -2.789278 -2.1198854 -1.1400623 -1.0806899 -0.32386923 0.12527657 -0.19510937 -0.22374964 -2.6734881 -3.5250964 -4.6766272 -5.543232 -5.7560287][-2.1911407 -1.7766833 -0.98070574 -0.54456425 -0.041768551 0.67091513 1.1945252 1.1313195 0.65624285 0.35672998 -1.8230667 -2.6483679 -3.8211651 -5.0312767 -5.1234908][-2.7510438 -1.799396 -0.97487974 -0.53793049 0.37804747 1.3430037 1.6597314 1.336453 1.0856738 0.68645811 -1.4346113 -1.7645593 -2.813664 -4.3273087 -4.6830821][-3.7614849 -2.7432284 -1.2956572 -0.36014748 0.79419374 1.2824616 1.7030988 1.7643132 1.4843869 1.0452037 -1.1732535 -1.5713968 -2.7403817 -4.5133371 -4.819057][-4.3368096 -3.1563106 -1.8217959 -0.47586584 0.4362092 1.108953 1.6500897 1.8319964 1.6280084 1.3551269 -0.81501627 -1.6816649 -3.1875005 -4.7416573 -4.8948178][-4.7107954 -3.9477494 -2.2987633 -1.2635345 -0.070300579 0.43193579 1.0271354 1.0635667 1.4568744 1.1662755 -1.2159791 -2.1364069 -3.4892335 -4.65059 -5.0302811][-4.7469068 -4.2386103 -3.0176582 -2.1084509 -1.3401675 -0.916029 -0.93305635 -0.60443354 -0.21766949 -0.52190495 -2.4116697 -3.3338318 -4.2199459 -5.0482779 -5.0018373][-6.3851209 -5.5911126 -5.4037147 -4.269917 -3.204906 -2.9863796 -3.0301766 -2.5254707 -2.4643865 -2.7433572 -3.6478786 -4.3224082 -4.8332505 -5.6820583 -5.7126551][-6.3773112 -6.017529 -5.1498556 -4.2997289 -3.3397865 -2.8819766 -2.5817256 -2.7095633 -3.2941675 -3.2570577 -4.0335255 -4.2298884 -4.7930131 -5.8260236 -6.5143023][-6.8121319 -6.2195206 -5.7551985 -4.8598576 -3.4141874 -2.2781739 -2.0108442 -2.5079441 -2.9890695 -3.0522332 -4.5172224 -4.233304 -3.8750308 -5.1286459 -5.9716282][-6.8843927 -6.2395144 -5.7858124 -5.4164057 -4.0820112 -2.7790155 -1.8991337 -1.8087058 -1.715662 -1.8619914 -3.0321445 -2.7950869 -3.250114 -4.1127968 -4.4339962][-6.6500368 -6.8423958 -6.1415167 -5.6279855 -5.1855359 -3.8465104 -2.9801068 -2.7698655 -2.4298434 -1.7802501 -1.8749247 -2.5904784 -3.6564445 -4.3130178 -4.8732214]]...]
INFO - root - 2017-12-15 12:32:49.018376: step 12710, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 57h:19m:15s remains)
INFO - root - 2017-12-15 12:32:55.433886: step 12720, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 58h:39m:41s remains)
INFO - root - 2017-12-15 12:33:01.763442: step 12730, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 56h:54m:56s remains)
INFO - root - 2017-12-15 12:33:08.156300: step 12740, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 56h:54m:01s remains)
INFO - root - 2017-12-15 12:33:14.509451: step 12750, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 56h:09m:18s remains)
INFO - root - 2017-12-15 12:33:20.847130: step 12760, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 56h:17m:45s remains)
INFO - root - 2017-12-15 12:33:27.276130: step 12770, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 57h:48m:10s remains)
INFO - root - 2017-12-15 12:33:33.692150: step 12780, loss = 0.23, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 58h:34m:21s remains)
INFO - root - 2017-12-15 12:33:40.061925: step 12790, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 57h:16m:13s remains)
INFO - root - 2017-12-15 12:33:46.403708: step 12800, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 56h:24m:27s remains)
2017-12-15 12:33:46.923826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9168935 -2.6956687 -2.6122632 -2.6460824 -3.0243173 -3.5446043 -3.6283889 -3.5593038 -3.5818315 -3.9110558 -4.9938793 -5.8956771 -6.6109695 -7.8127551 -8.4273577][-2.0380874 -2.2841606 -2.5356221 -2.5387049 -3.3470497 -4.0370703 -4.2879019 -4.5253291 -4.5758028 -4.4742641 -5.0810347 -5.7387209 -5.9783812 -6.9072928 -7.4392295][-1.1274529 -0.77150583 -1.0252075 -1.3330817 -1.8030534 -2.3669715 -3.207809 -3.5315185 -3.6876462 -3.8878586 -4.4970245 -5.0406647 -5.2213173 -5.97239 -6.2709036][-0.024841309 0.68955088 0.47540903 0.16404867 -0.43800116 -0.93352365 -1.5106578 -2.0110464 -2.618319 -2.91711 -3.4641843 -4.0556917 -4.3501148 -5.1197948 -5.7388358][-0.066504955 0.65107393 0.70169687 0.62337923 0.34821463 0.5091157 0.47778368 -0.0072445869 -0.19974756 -0.77715969 -1.6644964 -2.0941606 -2.6782446 -3.9035447 -4.9983521][-1.0080504 -0.59440279 -0.0734067 0.29099703 0.52072859 1.4951682 2.1919303 2.028049 1.9652638 1.4748111 0.55103922 -0.58347416 -1.6598997 -3.1132722 -4.4446011][-1.5647449 -1.2823024 -0.93430233 -0.52612257 0.28593111 1.4505429 2.6471057 3.1128011 3.3632331 2.4880328 1.1020513 -0.35452604 -1.5977159 -3.3744545 -4.2576056][-2.4147186 -1.8583665 -1.3531709 -1.0494704 -0.10134125 1.2537599 2.8029323 3.6738982 4.2899146 3.4405608 1.9472699 0.18404818 -1.4282627 -3.2933035 -4.5052767][-3.5882092 -2.6788774 -1.938684 -1.4603581 -0.80413485 0.51494074 1.9213443 2.9732137 3.8892255 3.1495671 1.2254729 -0.37208271 -1.6554966 -3.6742263 -4.8540778][-4.5448685 -3.884675 -2.9710875 -2.2999268 -1.6025405 -0.41592741 0.49533892 1.0843434 1.7347255 1.3206115 -0.1873703 -1.7919612 -3.3142281 -4.5499544 -5.5462046][-5.7088351 -5.4807768 -5.0917444 -4.2387977 -3.6609035 -2.6548266 -1.8794823 -1.4578648 -1.165874 -1.5679088 -2.8016305 -3.7869768 -4.9140129 -6.1326723 -6.9682593][-7.5849071 -7.2308831 -6.9530187 -6.4213357 -5.8523083 -5.1958895 -4.7754211 -4.4726419 -4.32465 -4.5149469 -5.1746445 -5.636055 -6.2788916 -7.1746559 -8.0477419][-7.8256707 -7.8025656 -7.6356893 -7.521564 -7.0584092 -6.3115621 -5.9254732 -5.9054484 -5.7761016 -5.6987309 -6.2280917 -6.4274945 -6.692256 -7.1063066 -7.5594282][-7.294529 -7.4018693 -7.6071863 -7.5702152 -7.3515291 -6.6812606 -6.2013993 -6.0708156 -6.0866833 -6.01015 -6.268343 -6.5637445 -6.5376468 -6.6661639 -6.9209728][-6.9880075 -7.1162186 -7.0722475 -7.1217909 -6.8966889 -6.1672788 -5.64524 -5.5703363 -5.5201845 -5.6456051 -5.66818 -5.7079639 -5.8681526 -6.0897822 -6.114531]]...]
INFO - root - 2017-12-15 12:33:53.349029: step 12810, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 56h:23m:21s remains)
INFO - root - 2017-12-15 12:33:59.783661: step 12820, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 57h:24m:16s remains)
INFO - root - 2017-12-15 12:34:06.216992: step 12830, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 58h:26m:36s remains)
INFO - root - 2017-12-15 12:34:12.613632: step 12840, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 58h:40m:31s remains)
INFO - root - 2017-12-15 12:34:18.977547: step 12850, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 58h:17m:11s remains)
INFO - root - 2017-12-15 12:34:25.377013: step 12860, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 56h:13m:26s remains)
INFO - root - 2017-12-15 12:34:31.746585: step 12870, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 56h:33m:59s remains)
INFO - root - 2017-12-15 12:34:38.164325: step 12880, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 55h:59m:39s remains)
INFO - root - 2017-12-15 12:34:44.494435: step 12890, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 55h:25m:13s remains)
INFO - root - 2017-12-15 12:34:50.908576: step 12900, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 56h:32m:49s remains)
2017-12-15 12:34:51.384108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4204054 -3.5143647 -4.2120862 -4.9256735 -6.4372449 -7.0445156 -7.030386 -7.3528895 -7.7494817 -9.2523308 -10.058122 -10.694761 -10.641635 -10.936706 -11.064113][-4.3032875 -4.8657222 -5.5283251 -5.5258522 -5.946651 -6.4376135 -7.563437 -8.1493206 -8.3703947 -9.2385483 -9.2472591 -10.005988 -10.053476 -10.108643 -10.067987][-3.5739813 -4.0293055 -4.8265667 -5.4591889 -6.1348624 -6.1181431 -6.1097245 -6.1021585 -6.0869789 -7.0066743 -7.0176477 -7.9547558 -8.2179747 -8.365612 -8.7755489][-3.316124 -3.7167213 -4.6972084 -4.9786782 -4.8825073 -4.7251086 -4.3223696 -4.1349025 -3.9919748 -4.6046915 -4.3951082 -5.1291542 -5.7850904 -6.49965 -7.3390551][-3.8126442 -2.930479 -2.4244304 -2.7984958 -3.5609689 -2.8564382 -1.7504234 -1.3100152 -0.5720911 -1.6722369 -1.9095201 -2.8740649 -3.7863839 -4.5913587 -5.488472][-3.4744167 -3.1901941 -2.8528543 -1.8115506 -0.51699543 0.65096521 1.4168143 1.7698293 2.2704492 0.91773367 0.364532 -1.3883071 -2.951848 -4.3622971 -5.6741638][-4.8088083 -4.0521393 -3.0226178 -1.4237037 0.16289854 2.1569486 3.9986825 4.7205853 5.4786124 3.5301661 1.791882 -0.67018509 -3.1441078 -4.5826507 -5.281846][-5.5370536 -4.4171953 -3.2482524 -1.3260059 1.1511464 3.1801648 4.6237741 5.6515737 6.4600873 4.321732 2.7643228 -0.090874672 -2.5402865 -3.9990234 -5.1315293][-5.4073839 -4.0789 -3.2366753 -1.7627096 0.1692524 2.1805778 4.2332654 4.9902444 5.1521134 3.0145612 1.4062495 -1.166151 -3.2955203 -4.6495209 -5.6507988][-6.3209181 -5.553381 -4.3375645 -3.058547 -1.5692911 0.081438065 1.4394374 2.2958951 2.4603667 -0.2509594 -2.1955881 -4.2389984 -5.5439854 -6.431355 -7.1650038][-8.6662941 -8.0503178 -7.2311559 -6.1372743 -4.6275558 -3.4514723 -2.4453964 -1.93923 -2.1251478 -3.5739117 -4.7203636 -6.3574128 -7.4367771 -7.9440346 -8.1358328][-8.6038227 -8.4317045 -7.9648428 -7.4128084 -6.68524 -5.7347088 -5.1565652 -4.8828087 -4.5466518 -5.7869473 -7.4943509 -8.258688 -8.8474417 -8.758275 -8.677702][-9.0219555 -9.0085087 -9.165554 -8.4355431 -7.5992551 -7.2386465 -7.080946 -7.0052457 -6.8417583 -7.4102349 -7.9547281 -8.524127 -9.1351681 -8.8321676 -8.0626192][-9.232193 -8.47312 -7.82153 -7.7800894 -7.9777465 -7.4278264 -7.0956392 -7.3535886 -7.48603 -7.9001417 -8.4237328 -8.286581 -8.3648024 -8.1623144 -7.8939581][-9.9569311 -9.1437874 -8.6265392 -7.8437095 -7.3354831 -7.2244143 -7.1514249 -7.2287254 -7.468132 -7.7729492 -7.8767176 -7.8198805 -7.8340774 -7.6829562 -7.4690981]]...]
INFO - root - 2017-12-15 12:34:57.779410: step 12910, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 56h:16m:34s remains)
INFO - root - 2017-12-15 12:35:04.119640: step 12920, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 56h:05m:19s remains)
INFO - root - 2017-12-15 12:35:10.512515: step 12930, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 56h:57m:04s remains)
INFO - root - 2017-12-15 12:35:16.830838: step 12940, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.627 sec/batch; 55h:41m:49s remains)
INFO - root - 2017-12-15 12:35:23.193343: step 12950, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 57h:06m:53s remains)
INFO - root - 2017-12-15 12:35:29.581837: step 12960, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 56h:47m:15s remains)
INFO - root - 2017-12-15 12:35:36.002276: step 12970, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 55h:09m:20s remains)
INFO - root - 2017-12-15 12:35:42.495232: step 12980, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.637 sec/batch; 56h:34m:53s remains)
INFO - root - 2017-12-15 12:35:48.831074: step 12990, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.627 sec/batch; 55h:36m:47s remains)
INFO - root - 2017-12-15 12:35:55.227789: step 13000, loss = 0.55, batch loss = 0.44 (12.8 examples/sec; 0.623 sec/batch; 55h:19m:20s remains)
2017-12-15 12:35:55.786133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4736557 -4.5834551 -3.3254614 -2.5113349 -2.2010517 -1.8990769 -1.6032634 -1.2914267 -1.1545253 -2.0154324 -3.7060928 -5.0669131 -6.9108472 -8.30138 -9.7622986][-6.2456551 -5.3990407 -5.3982325 -5.5758467 -5.2207422 -4.1316862 -3.4510641 -3.4907598 -2.9063311 -4.0090818 -4.9402218 -6.0502739 -8.04611 -9.1658573 -10.705206][-6.0409565 -5.0826454 -4.4634066 -4.5164666 -3.9189985 -3.7109921 -3.5177975 -3.299737 -3.4471917 -4.6884079 -5.9951868 -6.6509142 -7.8788118 -9.0826483 -10.123077][-5.7701955 -4.5015788 -3.3188524 -2.5005832 -1.9359789 -1.3868194 -0.71454716 -1.1534352 -1.4987073 -3.3573332 -5.0456858 -6.1143236 -7.8736639 -8.2619734 -8.832427][-3.6646514 -2.3778887 -1.0367236 -0.24230909 0.79909086 1.3633943 1.6105304 1.2510924 0.721539 -1.2189488 -3.0743947 -4.07944 -6.0533552 -6.78324 -7.5027461][-2.9782567 -1.9612103 -0.69480658 0.67288733 1.8712764 2.8903279 3.5598893 3.271646 2.9375978 0.85913038 -0.78588343 -1.6377783 -3.4834123 -4.2980113 -5.2672882][-2.902422 -1.6939034 -0.33012009 1.036427 2.6473136 3.8271842 4.3279386 4.1171346 3.7943664 1.8237691 0.30241013 -0.6616354 -2.5362792 -3.2518797 -4.0780835][-2.0096684 -0.87127304 0.52656412 1.916863 2.9923997 3.9792857 4.4690909 4.3162322 4.0945354 2.1545672 0.4649291 -0.35587978 -2.1349177 -3.3245373 -4.0848451][-0.47084427 0.23492193 1.0185981 1.4195027 2.1479964 2.8014913 3.0511279 3.2578702 3.2613196 1.6915774 0.91192675 0.0016713142 -1.9943748 -3.0810294 -4.2036495][0.13115358 0.70316267 0.84811163 1.0026088 1.4064918 1.782393 2.338243 2.6048484 2.6701951 1.2759957 0.050747871 -0.79869223 -2.2090449 -3.643528 -4.5195427][-0.61629772 0.04916954 0.587132 0.58366156 0.75655985 0.79761839 1.1771264 1.5573192 1.3186984 -0.39567757 -1.1330838 -2.3055897 -3.9482641 -4.7141962 -5.1762142][-0.24894905 -0.069708347 0.13261557 -0.13351107 -0.24714041 -0.2285285 -0.15108776 -0.33707237 -0.72348595 -2.0443649 -3.3309193 -4.6633539 -5.772501 -6.2393851 -6.323926][-1.5338721 -1.2751608 -1.4259062 -1.8657541 -1.9440532 -2.0059876 -1.8955245 -2.0137677 -2.4452214 -4.2123861 -5.7001524 -6.3799186 -7.2708497 -7.6312571 -7.0151277][-3.0216465 -2.6621232 -2.2286835 -2.172514 -2.0404325 -2.324368 -2.7571673 -3.2542267 -4.0385942 -5.5207605 -6.1876984 -7.0988421 -7.8610978 -8.0138817 -7.7733488][-4.51824 -3.9310679 -3.3563976 -3.0514812 -3.2178078 -3.922107 -4.3217983 -5.0646172 -5.7840257 -6.3637195 -7.0948133 -7.7034607 -8.3323593 -8.4555235 -7.9317613]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:36:02.248414: step 13010, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 57h:47m:51s remains)
INFO - root - 2017-12-15 12:36:08.738322: step 13020, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 56h:24m:09s remains)
INFO - root - 2017-12-15 12:36:15.175590: step 13030, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 56h:29m:51s remains)
INFO - root - 2017-12-15 12:36:21.663688: step 13040, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 56h:51m:11s remains)
INFO - root - 2017-12-15 12:36:28.143275: step 13050, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 59h:06m:54s remains)
INFO - root - 2017-12-15 12:36:34.634803: step 13060, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.638 sec/batch; 56h:35m:39s remains)
INFO - root - 2017-12-15 12:36:40.990222: step 13070, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 56h:34m:30s remains)
INFO - root - 2017-12-15 12:36:47.399834: step 13080, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 56h:38m:35s remains)
INFO - root - 2017-12-15 12:36:53.701013: step 13090, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 56h:58m:57s remains)
INFO - root - 2017-12-15 12:37:00.067358: step 13100, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 56h:36m:02s remains)
2017-12-15 12:37:00.577557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3785563 -1.4799552 -1.0063763 -1.0760064 -0.80125046 -0.72390032 -0.968359 -0.69663334 -0.42670536 -1.1725383 -2.36721 -3.5825915 -4.7212596 -5.567656 -6.723629][-2.9106951 -2.592505 -2.4466224 -2.1735349 -1.8521714 -1.529099 -1.2000923 -1.1319442 -1.0719323 -1.925539 -3.2116261 -4.3726616 -5.250308 -6.0070124 -7.0606551][-3.1868749 -2.6499515 -2.4289494 -2.0805969 -1.60011 -1.1924596 -1.0343547 -0.63611174 -0.29580164 -1.1608591 -2.6668825 -4.0331707 -4.9483333 -5.4934931 -6.428638][-1.4748783 -1.4807701 -1.0283685 -0.96916246 -0.64293957 -0.251853 0.24888897 0.39994431 0.16855669 -0.84784031 -2.4600673 -3.6272054 -4.7331953 -5.4956408 -6.6131897][-2.4279804 -1.2592249 -0.051990509 0.12631702 0.07634306 0.271513 0.74029827 1.011734 1.1601315 0.17643881 -1.0839386 -2.5747504 -3.8740723 -4.7490406 -5.9294124][-1.8147507 -0.73600721 0.037017822 0.78824329 1.2115927 1.6943378 2.1486959 2.0964394 2.0487471 1.1918888 -0.11315489 -1.3980885 -2.4187865 -3.3869147 -4.5427485][-2.1827703 -1.1206088 0.22294855 1.2128344 1.8027697 2.3400288 2.5606689 2.3832951 2.1305017 0.697196 -0.97103071 -2.3904762 -3.5920978 -4.5937147 -5.5098906][-2.1317382 -0.63276625 0.15993023 0.932147 1.6457396 2.0882025 2.1392117 1.8343973 1.2583771 -0.20263815 -1.8845568 -3.3977413 -4.6630826 -5.7619429 -6.9200788][-1.648211 -1.0404897 -0.17784834 0.953887 1.3046837 1.5727587 1.3900518 1.1802588 0.71259117 -0.59774446 -2.1723981 -3.5777559 -4.9126196 -6.0266738 -7.0829382][-2.7593074 -1.8466744 -1.6173005 -0.72717714 -0.073608875 0.28692198 0.42610931 0.15552568 -0.31661367 -1.4301357 -2.976418 -4.2471123 -5.52668 -6.2821403 -7.4187675][-3.9782956 -3.5997458 -2.7049208 -2.109683 -1.6911569 -0.92767906 -0.44428492 -0.84932375 -1.2326703 -2.4112768 -3.9131417 -4.7630749 -5.7108474 -6.3404989 -7.0952449][-5.7059536 -5.0404272 -4.5823746 -3.9467471 -3.2732954 -2.6941404 -2.2976646 -2.2798128 -2.4908881 -3.723743 -4.6602087 -5.19404 -5.7793741 -6.1266985 -6.8775749][-6.9764094 -6.8633347 -6.3037648 -5.5144463 -5.1732016 -4.7628603 -4.3874779 -4.6749477 -4.8733931 -5.6022925 -6.0037136 -6.0510836 -6.2523422 -6.5577488 -7.3309383][-7.6300483 -7.2092013 -6.543963 -6.0647244 -5.5750923 -5.538538 -5.4077129 -5.4275055 -5.4993482 -6.2591786 -6.2040844 -6.4232969 -6.4645767 -6.6590466 -7.0239277][-9.3279505 -8.9981546 -8.47408 -7.9496651 -7.7438917 -7.5445895 -7.2527614 -7.494576 -7.7543921 -7.7200322 -7.693265 -7.4222369 -7.3680172 -7.1185808 -7.0636005]]...]
INFO - root - 2017-12-15 12:37:06.973623: step 13110, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 57h:47m:23s remains)
INFO - root - 2017-12-15 12:37:13.357472: step 13120, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 56h:16m:27s remains)
INFO - root - 2017-12-15 12:37:19.845815: step 13130, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 58h:04m:35s remains)
INFO - root - 2017-12-15 12:37:26.312334: step 13140, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 56h:34m:14s remains)
INFO - root - 2017-12-15 12:37:32.732581: step 13150, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 56h:10m:14s remains)
INFO - root - 2017-12-15 12:37:39.074804: step 13160, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 56h:07m:21s remains)
INFO - root - 2017-12-15 12:37:45.435887: step 13170, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 57h:16m:30s remains)
INFO - root - 2017-12-15 12:37:51.833440: step 13180, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 56h:19m:39s remains)
INFO - root - 2017-12-15 12:37:58.353205: step 13190, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:20m:50s remains)
INFO - root - 2017-12-15 12:38:04.795062: step 13200, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 56h:59m:43s remains)
2017-12-15 12:38:05.364570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7307887 -7.4785004 -9.23198 -10.967396 -12.285295 -12.522528 -11.908051 -9.7216511 -7.9550056 -6.6822462 -6.5917597 -10.232871 -7.4768319 -7.9443226 -8.6995859][-6.9971571 -7.9063306 -9.4421072 -10.978209 -13.003624 -13.124166 -12.064747 -9.8295975 -8.8760386 -7.821269 -7.8569393 -11.439778 -9.6955538 -10.340666 -9.9317532][-6.8113117 -7.6338425 -9.0732183 -10.090233 -11.537477 -11.054177 -9.7234364 -8.5955086 -6.949852 -6.27848 -6.7875056 -10.963998 -9.7220917 -10.799562 -11.290872][-6.316277 -7.1131048 -8.2014351 -8.5568123 -9.9422131 -9.451745 -7.8676233 -6.5901351 -5.1690621 -5.692318 -5.3299007 -9.6332912 -8.6172543 -10.207446 -11.272428][-5.90025 -6.6103311 -7.3490915 -7.2745152 -7.7274294 -6.1202083 -4.8922215 -4.114048 -2.9548264 -4.1799335 -5.8333983 -11.027062 -9.1184387 -10.455814 -11.607925][-6.5823779 -8.0414314 -7.6202364 -6.4251881 -6.3990355 -4.3654814 -1.2674413 0.62358618 1.66606 -1.4632416 -3.8930068 -10.308422 -9.9954386 -11.841526 -11.836584][-6.0427923 -6.9963613 -7.1167736 -5.0298977 -3.957521 -0.788579 2.0360866 3.7748971 5.1442075 2.9285102 0.13065147 -8.2000246 -9.0544558 -11.747261 -12.037014][-6.0616703 -6.2544975 -5.9049335 -3.8385048 -2.3634787 1.3225093 4.5950055 5.1544805 4.9914279 3.1788898 1.5266643 -6.0426044 -7.3903179 -11.39544 -13.085911][-6.8016028 -6.6000977 -5.9942951 -3.9564683 -2.82346 -0.016558647 2.5919013 3.7247815 3.6611676 0.63977194 -1.9024444 -7.95452 -7.7976465 -10.935498 -12.378811][-8.3061047 -8.909874 -7.9289927 -5.78199 -4.5233688 -2.1820369 -0.2840848 1.1878839 1.4971642 -1.6515322 -4.6349387 -10.242666 -9.8424873 -11.417813 -11.531776][-9.0119371 -10.288342 -9.99755 -7.9792628 -6.486342 -4.3857765 -2.537622 -2.4124675 -1.9822378 -3.6449113 -5.4252605 -10.864745 -11.22326 -12.24692 -11.577781][-7.9987092 -9.3817816 -9.9177046 -9.2935781 -8.6212435 -6.4868422 -4.77841 -4.6033726 -4.9929123 -6.5029149 -6.8220005 -9.7710772 -10.507753 -11.367943 -10.966646][-9.0933418 -9.2412968 -8.3839283 -7.7425694 -7.7472086 -7.04039 -5.6026058 -5.0574331 -5.7761645 -7.1992421 -7.6286335 -9.0103712 -8.2255049 -8.9603148 -8.9820881][-8.0892258 -8.8275261 -8.2105093 -7.2010059 -6.6844735 -5.6157203 -5.5146265 -5.1177263 -5.4993615 -6.6607761 -7.3241811 -8.48077 -8.1430035 -7.1531205 -6.7325749][-7.5569468 -8.7887573 -8.778923 -8.2896233 -7.1401162 -6.1081233 -5.7763367 -5.476563 -6.1649451 -7.2902756 -7.8645625 -7.9550171 -7.4906511 -6.9078751 -6.0323954]]...]
INFO - root - 2017-12-15 12:38:11.788323: step 13210, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 55h:55m:11s remains)
INFO - root - 2017-12-15 12:38:18.175306: step 13220, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 59h:03m:41s remains)
INFO - root - 2017-12-15 12:38:24.538726: step 13230, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 56h:56m:31s remains)
INFO - root - 2017-12-15 12:38:30.851079: step 13240, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 55h:21m:07s remains)
INFO - root - 2017-12-15 12:38:37.189624: step 13250, loss = 0.29, batch loss = 0.18 (13.1 examples/sec; 0.609 sec/batch; 54h:01m:14s remains)
INFO - root - 2017-12-15 12:38:43.551544: step 13260, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 56h:31m:01s remains)
INFO - root - 2017-12-15 12:38:49.980481: step 13270, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 57h:56m:37s remains)
INFO - root - 2017-12-15 12:38:56.315017: step 13280, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:59m:21s remains)
INFO - root - 2017-12-15 12:39:02.661461: step 13290, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 57h:44m:25s remains)
INFO - root - 2017-12-15 12:39:09.011010: step 13300, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 57h:43m:31s remains)
2017-12-15 12:39:09.548307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3687553 -3.0253997 -2.6217275 -2.7032142 -2.9927082 -2.9164081 -2.6913662 -2.6707082 -2.3052812 -3.6908393 -4.362237 -5.1597462 -6.3719206 -7.1403637 -6.6297855][-3.9579411 -3.6892917 -3.4782619 -3.5296454 -3.2299557 -2.8535147 -2.6942654 -2.3819218 -1.7563276 -2.7865167 -3.3673449 -4.1961026 -5.744926 -6.6876416 -6.4429054][-4.7866278 -4.5269804 -4.0512295 -3.7208791 -3.3474264 -2.5881138 -2.0851865 -1.9568615 -1.7679238 -2.6664591 -3.0730472 -3.8026812 -5.1110783 -6.2800283 -6.4356604][-4.61576 -4.44693 -4.2916536 -4.0621815 -3.2884603 -2.3615074 -1.6832633 -1.2571793 -0.69249392 -1.9190955 -2.7343636 -3.5763483 -5.1170545 -6.312983 -6.2854452][-5.2685404 -4.5371165 -4.1664958 -3.8082063 -3.0582466 -1.8281827 -0.83959246 -0.38130188 0.22908115 -1.0119381 -1.8559766 -3.1444311 -5.0251017 -6.1705718 -6.2132692][-5.4703569 -5.0545969 -4.7717676 -3.919651 -2.4459481 -0.90945387 0.39626408 0.89250183 1.0691776 -0.57754183 -1.5290089 -2.8653283 -4.8216181 -6.1789103 -6.2756205][-5.191258 -4.9726038 -4.3146496 -3.2415695 -1.6605048 0.27755785 1.7374783 2.3413486 2.5968781 0.31141615 -1.0572147 -2.499835 -4.58169 -6.2871065 -6.4582872][-5.5577455 -4.7126465 -3.7380421 -2.452137 -0.70587492 1.1438398 2.5450916 3.2103634 3.2597227 0.9061327 -0.63272762 -2.2058945 -4.4301209 -6.1084881 -6.2535248][-5.2480927 -4.857183 -3.3651185 -1.7100644 -0.18739223 1.2080116 2.0476027 2.6692991 2.6208715 0.64801979 -0.66245985 -2.5799413 -4.7965069 -6.2860518 -6.44147][-5.0000939 -4.4843969 -3.8312128 -2.3987522 -0.91066217 0.37872219 1.4282751 1.6579866 1.7975025 -0.0888114 -1.3527279 -3.0465794 -4.9407239 -6.1623178 -6.2359171][-6.5846705 -6.0127997 -4.6107168 -3.4795079 -2.4774485 -1.1626062 -0.65875912 -0.52986526 -0.78587866 -2.8069253 -3.6406054 -4.9136758 -6.2363062 -6.9119148 -6.9140358][-7.4936061 -7.2376814 -6.6988373 -5.5597906 -4.4609642 -3.4670215 -2.8670769 -2.9117112 -3.3070006 -5.105114 -5.6692705 -6.1614733 -6.5130844 -6.8278527 -7.1391273][-7.926353 -7.8778944 -6.9333754 -6.0952187 -5.4292517 -4.8194656 -4.4948473 -4.544735 -4.7369103 -5.7190504 -5.8231034 -6.0896721 -6.3268852 -6.1102028 -5.613986][-6.6065574 -6.3457031 -6.293869 -5.6895027 -5.3839645 -5.2786694 -5.022316 -5.3808479 -5.348237 -5.668488 -5.6585593 -5.7339039 -5.8123732 -5.4312406 -5.0635834][-7.6251521 -7.3122182 -6.8788033 -6.8540258 -7.00484 -6.81438 -6.6955934 -7.0262747 -7.5324264 -7.2946839 -6.4657869 -6.0780778 -5.8219509 -5.2830091 -5.0174632]]...]
INFO - root - 2017-12-15 12:39:16.032156: step 13310, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 57h:43m:58s remains)
INFO - root - 2017-12-15 12:39:22.430086: step 13320, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 55h:43m:51s remains)
INFO - root - 2017-12-15 12:39:28.797632: step 13330, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 55h:02m:46s remains)
INFO - root - 2017-12-15 12:39:35.133806: step 13340, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 55h:31m:55s remains)
INFO - root - 2017-12-15 12:39:41.512463: step 13350, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 55h:44m:16s remains)
INFO - root - 2017-12-15 12:39:47.879795: step 13360, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 56h:02m:10s remains)
INFO - root - 2017-12-15 12:39:54.310722: step 13370, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 58h:11m:01s remains)
INFO - root - 2017-12-15 12:40:00.633632: step 13380, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:46m:17s remains)
INFO - root - 2017-12-15 12:40:07.131111: step 13390, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 56h:26m:54s remains)
INFO - root - 2017-12-15 12:40:13.503657: step 13400, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 56h:23m:17s remains)
2017-12-15 12:40:14.051918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7387915 -7.8389664 -7.5548315 -7.7258763 -7.8755722 -7.4213614 -5.6206293 -4.1806526 -2.7601552 -2.5836043 -3.0997143 -3.9950221 -4.6022348 -5.118659 -6.2135711][-5.80441 -6.3713174 -6.5460091 -7.1230125 -7.4936604 -7.3934526 -6.7640653 -5.3330364 -3.0960693 -2.8257499 -3.6359835 -4.9704537 -4.811821 -5.0317168 -6.4071026][-5.4488387 -5.7719364 -5.8930154 -6.9194083 -7.6418734 -7.906261 -7.3633552 -6.2152815 -5.262526 -5.03607 -5.2216272 -6.642354 -6.6998944 -6.8218751 -6.9524517][-5.6508956 -5.6460648 -5.4784575 -5.2820487 -6.0821958 -6.1415567 -5.6916432 -4.9105091 -4.1833467 -5.0556231 -6.4077187 -7.9469652 -7.5182815 -7.7205667 -8.3145781][-6.3177295 -5.3720922 -4.3164048 -3.534677 -3.0451894 -2.3954816 -1.703342 -1.071723 -0.62881517 -2.3648834 -4.4392381 -6.7633386 -7.2545447 -7.716886 -8.3862953][-5.0379648 -3.9078841 -3.5626287 -2.1156621 -0.66501188 0.62402391 1.9271913 2.411016 2.9229941 1.181953 -0.68258667 -3.7448649 -5.1759548 -6.3283939 -7.5748138][-5.1062489 -3.3357296 -1.5414419 -0.21222115 0.96125937 2.5290914 4.14407 4.2240329 4.4267755 2.600246 0.64531469 -1.785851 -3.2053628 -4.8876219 -6.8041272][-4.087678 -2.8885403 -2.0664968 -0.0673275 1.8483901 2.8586135 3.9750857 4.3153405 4.4486518 2.4579797 0.51267385 -1.980607 -3.0265799 -4.17338 -5.7802191][-4.3247318 -3.403913 -2.6746879 -0.76495361 0.79018164 1.9807982 2.7245784 2.5695176 3.0770888 1.3285699 -1.1984725 -3.11762 -3.5768986 -4.1783381 -5.5002003][-6.0176888 -5.1448674 -4.5040836 -2.8642888 -1.4004688 -0.30073404 -0.11914206 -0.33212662 0.030956268 -1.4072957 -2.6165419 -4.6717358 -5.5837479 -5.9453754 -6.5388756][-7.5574236 -7.3400044 -6.8068233 -6.2464261 -5.8251104 -4.207819 -3.0973682 -3.174705 -3.6715832 -5.2027388 -5.6261215 -7.1368618 -7.1580448 -7.6205068 -8.7599258][-9.5838223 -8.97883 -8.448885 -7.8892703 -7.4336715 -7.1187339 -6.9372063 -6.2024364 -5.6908045 -6.9290504 -7.2344432 -7.8776984 -7.352993 -7.6296487 -8.4769964][-9.9395971 -10.42041 -10.103037 -9.3036766 -8.5925331 -7.960104 -7.7050352 -7.9875689 -8.1699715 -8.6453848 -8.5789557 -8.5788431 -7.60015 -6.8689213 -7.4092646][-9.1071272 -9.3437557 -9.5245314 -9.1973505 -8.7371349 -8.5321083 -7.9234419 -7.7844324 -8.0651827 -8.49348 -8.4406347 -8.3573771 -8.0134077 -7.30442 -6.6026068][-7.3727145 -7.4067755 -7.6809907 -7.4853249 -7.7394176 -7.9478378 -7.633604 -7.8454933 -7.3015947 -7.3361058 -7.8564053 -7.7586365 -7.8034782 -7.399539 -6.8113217]]...]
INFO - root - 2017-12-15 12:40:20.527378: step 13410, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 57h:29m:47s remains)
INFO - root - 2017-12-15 12:40:26.933895: step 13420, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 58h:53m:23s remains)
INFO - root - 2017-12-15 12:40:33.413710: step 13430, loss = 0.26, batch loss = 0.14 (11.8 examples/sec; 0.679 sec/batch; 60h:10m:33s remains)
INFO - root - 2017-12-15 12:40:39.854495: step 13440, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 58h:07m:16s remains)
INFO - root - 2017-12-15 12:40:46.311030: step 13450, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 57h:57m:58s remains)
INFO - root - 2017-12-15 12:40:52.762091: step 13460, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 58h:57m:00s remains)
INFO - root - 2017-12-15 12:40:59.116029: step 13470, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:20m:37s remains)
INFO - root - 2017-12-15 12:41:05.563037: step 13480, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.664 sec/batch; 58h:52m:54s remains)
INFO - root - 2017-12-15 12:41:12.039163: step 13490, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 57h:34m:24s remains)
INFO - root - 2017-12-15 12:41:18.562457: step 13500, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:53m:20s remains)
2017-12-15 12:41:19.065170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6107807 -2.9995484 -3.2809906 -3.5099339 -3.7887921 -3.570343 -3.2212515 -2.6159534 -2.1332231 -3.3139777 -3.6985435 -4.9053764 -5.9748549 -7.1195536 -7.8943262][-3.1547551 -3.6057692 -3.7955809 -4.3900995 -4.7451053 -4.4746284 -4.1476736 -3.5004506 -2.8289485 -4.0644188 -4.51812 -5.4179277 -6.3961296 -7.3376164 -8.033186][-3.5075412 -3.6032748 -3.6584721 -3.7565584 -3.5483937 -3.3800039 -2.9603324 -2.6349306 -2.1839876 -3.459188 -3.9708002 -4.8215647 -5.7977667 -6.8505707 -7.4891052][-3.7677851 -3.5597043 -3.2119732 -2.8345523 -2.3965788 -2.1934104 -1.938436 -1.8876023 -1.7211976 -3.2554312 -3.8855882 -4.8162222 -5.5615969 -6.1472821 -6.7233572][-4.1059036 -3.3575511 -2.5282722 -1.9870801 -1.3920941 -0.718287 -0.38323927 -0.56999016 -0.38472891 -1.8798499 -2.5324955 -3.5185113 -4.5236244 -5.2703543 -5.9543328][-4.4329205 -3.4932709 -2.8296642 -1.580308 -0.55004311 0.18525553 0.67402363 0.71594334 0.83706093 -0.82407141 -1.5921311 -2.5743861 -3.6320682 -4.5295868 -5.3529167][-4.469758 -3.4812188 -2.6087756 -1.2564936 -0.11428738 0.85999107 1.4329691 1.4919519 1.559865 -0.072863579 -1.0682263 -2.2510338 -3.3377571 -4.3304324 -5.0175657][-4.8229628 -3.6426616 -2.4770637 -1.0700345 0.0814147 1.0662756 1.7041769 2.1942749 2.4689999 0.606617 -0.4624424 -1.83534 -3.0075412 -4.2054806 -4.9638634][-4.4342804 -3.8873618 -3.142221 -1.711441 -0.7135129 0.34218025 0.98235321 1.5761366 1.9219046 0.32063293 -0.67471218 -2.0181046 -3.1526222 -4.4171333 -5.2491326][-4.8101768 -4.15524 -3.4396906 -2.4035463 -1.3385825 -0.17235756 0.64157391 1.2457237 1.458581 -0.34566975 -1.4150929 -2.6486 -3.3755832 -4.4266748 -5.1423225][-5.8347688 -5.495575 -4.8547935 -3.9759431 -2.7899289 -1.7934737 -0.894495 -0.28773928 -0.0487566 -1.6855707 -2.8041711 -3.7550097 -4.2856407 -5.0928969 -5.369175][-6.861423 -6.5579281 -6.0133734 -5.3671179 -4.3546739 -3.3248577 -2.3902736 -2.1089292 -2.0293288 -3.2266874 -3.8544564 -4.6179504 -4.9684048 -5.55641 -5.7706666][-7.0406594 -6.9505153 -6.6288185 -5.9096813 -5.2498236 -4.4899812 -3.8368533 -3.6974075 -3.7937672 -4.4829855 -4.7847304 -4.8951216 -5.1015186 -5.228672 -5.2134447][-7.0985842 -6.9833293 -6.8713121 -6.2919388 -5.8033581 -5.1024742 -4.7024727 -4.7455006 -4.797966 -5.179637 -5.1807475 -5.0029407 -4.9333377 -5.0085077 -4.9812937][-7.8394222 -7.9564986 -7.498817 -7.2336268 -6.9443393 -6.2951365 -6.1987576 -6.290092 -6.2982545 -6.2620187 -6.22512 -6.1223431 -6.0816216 -5.7503848 -5.7162914]]...]
INFO - root - 2017-12-15 12:41:25.517299: step 13510, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 58h:15m:30s remains)
INFO - root - 2017-12-15 12:41:31.978031: step 13520, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.653 sec/batch; 57h:49m:20s remains)
INFO - root - 2017-12-15 12:41:38.494587: step 13530, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 57h:47m:16s remains)
INFO - root - 2017-12-15 12:41:44.926430: step 13540, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 56h:16m:16s remains)
INFO - root - 2017-12-15 12:41:51.334960: step 13550, loss = 0.37, batch loss = 0.26 (12.6 examples/sec; 0.633 sec/batch; 56h:05m:50s remains)
INFO - root - 2017-12-15 12:41:57.761420: step 13560, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 55h:47m:31s remains)
INFO - root - 2017-12-15 12:42:04.146041: step 13570, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 56h:53m:14s remains)
INFO - root - 2017-12-15 12:42:10.611573: step 13580, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 56h:20m:56s remains)
INFO - root - 2017-12-15 12:42:17.073934: step 13590, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 55h:36m:15s remains)
INFO - root - 2017-12-15 12:42:23.649747: step 13600, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 58h:18m:06s remains)
2017-12-15 12:42:24.150379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.52598238 -0.6018796 -0.77508593 -1.1365175 -1.4954286 -1.8082528 -1.8929276 -1.9619589 -2.07125 -3.3043752 -4.1745882 -5.7112894 -6.1389937 -7.3277 -7.4167733][-1.8025565 -1.7508516 -2.2246714 -2.5383611 -2.8857284 -2.8015985 -2.7777452 -3.0305943 -3.0457606 -4.475234 -5.3760381 -6.7154922 -7.0941935 -8.0506735 -7.90702][-3.4715137 -3.3142118 -3.5054588 -3.4735508 -3.4366837 -3.2306128 -3.1321735 -3.3213592 -3.4691133 -4.9792094 -5.7279348 -7.2824736 -7.876267 -8.9263544 -8.7888126][-4.9480505 -4.7123489 -4.3749685 -3.8105001 -3.3757248 -2.9194202 -2.5485711 -2.6100197 -2.705833 -4.1426497 -4.9990644 -6.7710695 -7.4697723 -8.4902678 -8.0989571][-6.0505161 -5.22386 -4.2961578 -3.3886471 -2.4303713 -1.5151653 -1.0390477 -1.0396333 -1.1191149 -2.4151392 -3.2170897 -4.9638767 -5.6660404 -7.0065846 -6.9983773][-6.1881084 -5.1580534 -4.348444 -2.835732 -1.4269629 -0.47946215 0.0087394714 0.19568777 0.18257141 -1.1968975 -2.0865369 -3.6971154 -4.2637572 -5.5799294 -6.0675468][-5.566906 -4.3084707 -3.0912094 -1.3383751 0.06402874 0.87032747 1.0198064 1.4012828 1.5337567 0.095516682 -0.83407211 -2.6261635 -3.4425693 -4.608438 -4.90415][-4.8272886 -3.2639103 -1.8722658 0.034285545 1.6377234 2.2909627 2.2992749 2.3254437 2.2078032 0.82088232 0.0048041344 -1.6771407 -2.3916383 -3.3529892 -3.61911][-3.6767077 -2.3028593 -1.0299568 0.6621089 1.54178 1.9039502 2.4807277 2.1787477 1.8234906 0.2911005 -0.21370316 -1.5289583 -2.0595579 -2.9739313 -3.163239][-4.2003708 -2.6219912 -1.3358288 -0.35614061 0.33927774 1.0587201 1.3040872 1.6146407 1.6102042 -0.61004782 -1.4677229 -2.4823523 -2.7937765 -3.6802483 -3.6969938][-5.9466767 -4.7055268 -3.8074353 -2.5878544 -2.0272536 -1.4385118 -0.8004365 -0.32452726 -0.31874943 -1.8714089 -3.0853825 -4.6701488 -4.6995554 -4.9959259 -4.501462][-7.7304921 -7.0605793 -5.9114203 -4.741776 -4.1699891 -3.582407 -2.9607682 -2.6961498 -2.5461473 -3.20719 -4.0430231 -5.2854366 -5.6314487 -6.1249113 -5.5848732][-8.89889 -8.2423325 -7.4746013 -6.6071973 -5.728086 -4.8055029 -4.3227649 -4.1302738 -3.8506224 -4.4809971 -4.9989948 -5.4569516 -5.9270115 -6.1764131 -5.5003214][-9.1727219 -8.7624216 -8.1258469 -7.2868385 -6.4028788 -5.4712911 -4.7463841 -4.745019 -4.7467337 -5.0902452 -5.497963 -5.7085705 -5.4920821 -5.6129012 -5.2671213][-9.2437716 -9.4393415 -9.0913439 -7.9978261 -6.9981594 -5.8572788 -5.5293541 -5.5170469 -5.7146711 -5.9709339 -6.0719666 -6.195497 -5.9832468 -5.7410564 -5.5377684]]...]
INFO - root - 2017-12-15 12:42:30.543035: step 13610, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 55h:57m:13s remains)
INFO - root - 2017-12-15 12:42:36.959978: step 13620, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.655 sec/batch; 58h:03m:21s remains)
INFO - root - 2017-12-15 12:42:43.415849: step 13630, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 58h:42m:54s remains)
INFO - root - 2017-12-15 12:42:49.923929: step 13640, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 58h:09m:19s remains)
INFO - root - 2017-12-15 12:42:56.340110: step 13650, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 57h:18m:40s remains)
INFO - root - 2017-12-15 12:43:02.809152: step 13660, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 57h:33m:34s remains)
INFO - root - 2017-12-15 12:43:09.284618: step 13670, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 57h:39m:10s remains)
INFO - root - 2017-12-15 12:43:15.735351: step 13680, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 57h:00m:51s remains)
INFO - root - 2017-12-15 12:43:22.225811: step 13690, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 55h:43m:20s remains)
INFO - root - 2017-12-15 12:43:28.653325: step 13700, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 57h:23m:07s remains)
2017-12-15 12:43:29.158814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1032848 -3.9622247 -3.9246409 -3.8568568 -4.0650368 -4.2401333 -4.4266267 -4.667861 -5.3458261 -6.0633149 -6.7920885 -8.1998835 -8.920454 -9.5015745 -9.6443071][-4.7391453 -4.3768544 -4.21667 -4.4022446 -4.83016 -4.9835367 -5.1232848 -5.4945335 -5.9118004 -6.5275226 -6.9409561 -7.9589052 -8.7478971 -9.909893 -10.188436][-3.8719947 -3.8912892 -3.7577868 -3.6844742 -3.71288 -3.6307559 -3.3456717 -3.60893 -3.8450181 -4.6882362 -4.8157759 -5.9997945 -7.317431 -8.8194647 -9.2423][-5.0892153 -4.7591467 -4.2747893 -3.8353286 -3.3831091 -2.7851968 -1.7806263 -1.6390147 -1.6813684 -2.7490144 -3.4407024 -4.6956434 -5.8158669 -7.4061956 -8.1961412][-5.4456396 -4.6846867 -4.0516005 -3.0911965 -2.229599 -1.2732053 -0.053460598 0.67867947 1.2495737 -0.10597515 -1.5692797 -3.8338809 -5.6399579 -7.1520743 -8.0695219][-5.5529585 -4.6362114 -3.4206762 -2.1287022 -1.1275964 0.230052 1.6433239 2.3231306 2.6700134 1.3476305 -0.47453117 -3.1488619 -5.5928226 -7.366487 -8.39995][-4.8937736 -4.3891821 -3.0548258 -1.3181243 0.14694118 1.4216013 2.5941505 3.4339857 3.8183184 2.2915754 0.42915535 -2.2639332 -5.0525351 -7.5662723 -8.9784842][-3.266284 -3.1897559 -2.326601 -1.1336384 0.36784267 1.7202349 2.6084156 3.0031195 3.2419415 1.8367004 0.25516272 -2.4007692 -4.8189278 -7.1595335 -8.7172956][-1.8232565 -1.9143486 -1.5047073 -1.1585102 -0.27395058 0.60641193 1.1702633 1.4629955 1.7458544 0.45968628 -0.8849535 -3.2926431 -5.5146708 -7.6457491 -8.849412][-2.4875631 -1.7697115 -1.1377945 -1.1708016 -0.70645618 -0.48808765 -0.061408997 0.18513918 0.49730778 -1.0363393 -2.2886305 -4.54082 -6.3162918 -8.1422939 -9.1801815][-4.2537313 -3.8502355 -3.0508904 -2.4676256 -1.997324 -1.8118548 -1.6040483 -1.6589727 -1.5800657 -2.9031634 -4.0074558 -6.0365353 -7.1168518 -8.193429 -8.433177][-5.119092 -4.7699614 -4.3680754 -4.0405765 -3.7073679 -3.2420201 -3.1104927 -3.1380239 -3.0624561 -4.1714392 -5.0544686 -6.4468837 -6.78384 -7.5009727 -7.7747893][-6.9415984 -6.1977158 -5.7093205 -5.387671 -5.0169854 -4.6249046 -4.3458509 -4.3101258 -4.2620678 -5.0947456 -5.7433043 -6.6077957 -6.9412546 -7.2258983 -6.8178725][-6.4980011 -5.667666 -5.1895752 -5.1773796 -4.9234362 -4.30929 -4.0278263 -4.3677626 -4.7647285 -5.5311437 -5.8131618 -5.8787422 -6.02702 -6.4796453 -6.4508781][-8.4607725 -8.02361 -7.1379285 -6.56377 -6.4267755 -6.1429582 -5.8546228 -5.7266197 -5.8776155 -6.3588252 -6.6445751 -6.3962135 -6.0749512 -5.8038378 -5.5455036]]...]
INFO - root - 2017-12-15 12:43:35.550565: step 13710, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.664 sec/batch; 58h:48m:44s remains)
INFO - root - 2017-12-15 12:43:41.939577: step 13720, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 55h:26m:49s remains)
INFO - root - 2017-12-15 12:43:48.325332: step 13730, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 56h:45m:39s remains)
INFO - root - 2017-12-15 12:43:54.634290: step 13740, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 56h:49m:10s remains)
INFO - root - 2017-12-15 12:44:00.994670: step 13750, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 57h:49m:28s remains)
INFO - root - 2017-12-15 12:44:07.426038: step 13760, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 56h:42m:21s remains)
INFO - root - 2017-12-15 12:44:13.881255: step 13770, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 57h:37m:16s remains)
INFO - root - 2017-12-15 12:44:20.298246: step 13780, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 55h:10m:23s remains)
INFO - root - 2017-12-15 12:44:26.690904: step 13790, loss = 0.39, batch loss = 0.28 (12.4 examples/sec; 0.644 sec/batch; 57h:01m:16s remains)
INFO - root - 2017-12-15 12:44:33.098846: step 13800, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 56h:33m:31s remains)
2017-12-15 12:44:33.618670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2420769 -3.4059014 -3.469748 -4.1757345 -5.1373358 -5.3484755 -5.7208567 -5.4784665 -5.0777168 -5.3375607 -6.2056675 -6.4267082 -6.8714314 -7.330987 -7.4074693][-2.6216002 -3.0266514 -3.3436737 -3.9339752 -4.6989279 -5.3953 -5.9900808 -5.5656013 -5.187964 -5.0753932 -5.5034914 -5.9444575 -7.4260192 -7.612298 -7.8630338][-2.2760983 -2.3572059 -2.7832632 -3.4914732 -3.7014832 -4.2370596 -4.1931438 -3.8368139 -3.3959641 -3.7581275 -4.5401964 -4.9493585 -5.7637434 -6.0901947 -6.4775763][-1.8200812 -2.732892 -3.1985617 -3.482317 -3.2382092 -2.9523096 -2.32651 -2.1679177 -1.3012056 -1.0560203 -2.0785613 -3.3737025 -4.5235615 -5.5238304 -5.9235849][-3.0866642 -2.5224748 -1.8670297 -1.9856591 -1.8402119 -1.2885742 -0.20874357 0.47145081 1.3166056 0.93087387 -0.65326738 -2.2326932 -4.1596231 -5.2778678 -5.9482307][-2.7007313 -2.9471779 -2.7446671 -1.7862992 -0.66271782 0.52860451 1.8404284 2.4481983 3.1269093 2.2945843 0.28671408 -1.663537 -3.7655485 -4.6525216 -5.5485268][-3.6733775 -3.1087332 -2.6412792 -1.6649828 -0.11705208 1.4042511 2.9964972 3.3810263 3.3268471 2.1826439 0.73133945 -0.73816681 -3.07303 -4.5748386 -6.0034304][-4.3070374 -4.0430155 -2.9860163 -1.5432267 0.18986654 1.8147516 3.0803604 3.3177433 4.0016718 2.9480267 0.67625237 -1.5564384 -3.6893182 -4.9020739 -6.2321968][-4.3621531 -4.1042509 -3.8457961 -2.8460598 -1.4659986 0.097458363 1.7573109 2.523921 2.5897665 1.405056 0.050838947 -1.6310711 -3.8974879 -5.2763481 -6.3820019][-5.85321 -4.9297762 -4.3819113 -4.04951 -3.0808067 -1.8633575 -0.99465036 -0.66475105 -0.024040699 -0.81319427 -2.6710815 -3.26714 -4.6272173 -5.6705976 -6.6596465][-5.8463793 -6.0561237 -5.1732645 -4.801846 -4.3810482 -4.1295662 -3.430079 -3.3114247 -3.567842 -4.0050097 -4.7962985 -5.3821459 -5.9394464 -6.0693903 -7.1686935][-6.120132 -6.0903983 -6.9127231 -6.9844027 -6.4892716 -6.1826286 -5.9554219 -5.8944077 -5.3493967 -5.7956161 -7.0134935 -7.1382117 -7.3369184 -7.5778852 -7.1720557][-6.4726758 -6.7854266 -6.12127 -5.9779596 -6.5516891 -6.1302156 -5.4202709 -5.7253623 -5.9564881 -5.9235783 -6.894125 -7.1191468 -7.3134608 -7.2427683 -7.6591206][-4.9629354 -5.43771 -6.014843 -5.8096924 -5.2292728 -5.4887466 -5.6240587 -5.5526972 -5.5879416 -5.647203 -6.6866403 -7.3122072 -7.0238619 -5.8247905 -5.4087772][-5.3874846 -5.2572131 -5.4309511 -6.3880281 -6.5855594 -5.7738404 -5.3893661 -5.2887955 -5.5251341 -5.5037718 -5.5540342 -6.0522389 -6.9484973 -7.9105425 -7.6546493]]...]
INFO - root - 2017-12-15 12:44:40.018147: step 13810, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 57h:18m:00s remains)
INFO - root - 2017-12-15 12:44:46.357639: step 13820, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 55h:41m:52s remains)
INFO - root - 2017-12-15 12:44:52.866935: step 13830, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 56h:17m:37s remains)
INFO - root - 2017-12-15 12:44:59.336917: step 13840, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 58h:55m:15s remains)
INFO - root - 2017-12-15 12:45:05.754249: step 13850, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 58h:55m:37s remains)
INFO - root - 2017-12-15 12:45:12.262232: step 13860, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 58h:45m:36s remains)
INFO - root - 2017-12-15 12:45:18.751694: step 13870, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 57h:12m:59s remains)
INFO - root - 2017-12-15 12:45:25.243481: step 13880, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 55h:10m:09s remains)
INFO - root - 2017-12-15 12:45:31.757617: step 13890, loss = 0.24, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 57h:48m:26s remains)
INFO - root - 2017-12-15 12:45:38.359981: step 13900, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 58h:42m:44s remains)
2017-12-15 12:45:38.879794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1466594 -4.6806717 -4.4500217 -4.7884932 -5.37153 -5.5284386 -6.124855 -6.7029696 -7.0013537 -7.4812202 -8.2544012 -8.8359671 -7.6985054 -7.4107995 -6.9815292][-5.3194094 -5.0809078 -5.0220585 -4.5871315 -4.4849749 -4.5979834 -5.0373306 -5.9279966 -7.1844397 -8.1545372 -9.3787689 -10.554764 -9.3901253 -8.4496775 -7.74214][-6.4180441 -5.3081393 -4.486414 -4.0045943 -3.7523303 -3.6570539 -3.5430799 -4.1308889 -5.6434507 -7.3844914 -9.2413607 -10.255537 -9.4412775 -8.9174356 -8.206871][-7.3663559 -6.1619129 -5.2298126 -3.5399356 -2.1592975 -1.4526281 -1.442059 -2.2279243 -2.8565564 -4.3034678 -6.0721755 -7.7441921 -7.677978 -7.49749 -7.5990524][-7.0372243 -5.3149118 -4.2169456 -3.0571694 -2.0712867 -0.33832359 0.64845991 0.52430391 0.64401674 -0.44360971 -2.0931911 -4.0627904 -4.671247 -5.5973673 -6.1152415][-6.4726791 -5.27999 -3.7150807 -1.6554708 -0.46012926 0.48239088 1.3829799 1.6522031 2.0054269 1.4339824 0.76114893 -0.89315271 -1.7956605 -2.8817019 -4.1229553][-5.0962572 -3.9307292 -2.5591178 -1.6212478 -0.6047349 1.1683488 2.0787416 2.5174098 2.9832454 2.1297278 1.3263021 -0.20299578 -1.1694479 -2.9700541 -4.2948618][-4.4867272 -3.5034285 -2.5629635 -0.087986946 1.4786601 1.2993808 2.0939984 2.6587358 2.4596953 1.4769254 0.44728613 -1.9694519 -2.5668354 -3.0906267 -3.8858271][-5.0909953 -4.2576342 -2.558176 -0.71557379 0.56159353 1.8754668 2.1058726 1.5460858 2.4592071 1.6247926 0.17222261 -2.2352962 -3.3823533 -4.3061514 -4.5848179][-5.3939695 -4.6795545 -4.43253 -3.2306151 -1.2087326 0.19608784 0.83750868 0.97402334 0.85119677 -0.94393349 -2.0767341 -3.5772958 -4.184895 -4.319562 -4.5860786][-6.2897396 -6.4416661 -6.0976334 -5.8309655 -5.6505351 -4.6173024 -3.500814 -2.9010615 -2.505868 -3.1739473 -4.4057159 -5.5813532 -5.3456554 -5.8654819 -6.1562862][-7.3054585 -7.3964081 -7.4553418 -7.0524545 -6.2031174 -5.677104 -5.4297514 -5.3593264 -5.3176308 -5.9178791 -6.4735122 -6.8433757 -7.1958103 -7.3492222 -6.9021087][-7.8636994 -7.8859334 -7.8883934 -7.4435329 -7.3441725 -6.7709436 -6.2689667 -6.4346523 -6.7027268 -6.7484303 -7.4603672 -7.57584 -7.013751 -6.314702 -6.5900726][-7.1320128 -7.2134542 -6.1080179 -5.6469941 -5.6089807 -5.2474341 -5.0786142 -5.1503634 -5.4498811 -5.6513734 -6.1738729 -6.6674719 -6.9631052 -6.4938259 -6.2294564][-7.4443502 -6.6406679 -6.4372592 -6.6485267 -6.2377019 -5.4682646 -4.8397379 -4.7104778 -4.9088917 -5.1994724 -5.474452 -5.5623455 -5.6244574 -5.8757982 -6.045239]]...]
INFO - root - 2017-12-15 12:45:45.317723: step 13910, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.637 sec/batch; 56h:24m:59s remains)
INFO - root - 2017-12-15 12:45:51.807406: step 13920, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 58h:05m:40s remains)
INFO - root - 2017-12-15 12:45:58.279558: step 13930, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 58h:56m:38s remains)
INFO - root - 2017-12-15 12:46:04.664173: step 13940, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 57h:03m:41s remains)
INFO - root - 2017-12-15 12:46:11.131729: step 13950, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 57h:29m:05s remains)
INFO - root - 2017-12-15 12:46:17.581714: step 13960, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 57h:02m:34s remains)
INFO - root - 2017-12-15 12:46:24.080358: step 13970, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 57h:29m:49s remains)
INFO - root - 2017-12-15 12:46:30.509375: step 13980, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 56h:42m:55s remains)
INFO - root - 2017-12-15 12:46:36.987048: step 13990, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 56h:03m:44s remains)
INFO - root - 2017-12-15 12:46:43.555277: step 14000, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:58m:56s remains)
2017-12-15 12:46:44.079465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6570988 -5.5526533 -5.4642897 -5.6576376 -6.1036053 -6.3204112 -6.3197818 -5.7228837 -4.8499455 -4.1188173 -5.050703 -5.6119795 -5.9506149 -7.1847086 -8.2949162][-5.6846695 -5.0478516 -4.771543 -4.8615761 -5.5595603 -5.9615231 -5.78607 -5.3349347 -4.3831053 -3.8809803 -5.2408419 -6.761857 -7.8889933 -7.9546928 -8.345089][-5.3590641 -4.4495277 -3.6785259 -3.2152772 -3.2038975 -3.17725 -3.3623476 -3.1782107 -3.0842462 -2.8913703 -4.0082159 -5.5721893 -6.4930725 -7.2311311 -7.6740694][-4.4975028 -3.6819134 -3.1672506 -2.2609029 -1.3864851 -0.95930624 -0.41198683 -0.28803873 -0.059689045 -0.72305107 -2.6619701 -5.1003904 -6.2581024 -6.6549425 -7.1876388][-4.7409153 -3.3480768 -1.8279705 -1.0658207 -0.097813129 1.0163927 1.7181711 1.9933662 2.0384703 1.09906 -1.0750542 -3.9621873 -5.8178158 -6.7676959 -7.39397][-4.33119 -3.4255438 -2.1011753 -0.6801486 0.92033672 2.391593 3.6570568 3.7732067 3.6646366 2.7298279 -0.25007296 -3.1158395 -5.7751789 -6.7230616 -7.2132964][-4.9269133 -3.5066519 -1.8618064 -0.13129377 1.7364016 3.3258762 4.5008888 4.9922724 4.9756985 3.2990618 0.10504532 -3.3037782 -6.3709335 -7.5174642 -7.9650054][-5.2481642 -4.0198908 -2.4889622 -0.53130531 1.2953196 2.9865036 4.2015371 4.4709654 4.4107952 3.1108828 0.069437027 -3.4439745 -6.5073662 -7.9557962 -8.3957853][-5.7324271 -4.928504 -3.5496287 -1.6543169 0.14044237 1.7855072 2.9466238 3.2589827 3.0097847 1.9960947 -1.1417837 -4.6424961 -7.0145068 -8.28051 -8.676548][-6.3390627 -6.2053103 -5.402071 -4.1063433 -2.4331422 -1.189404 -0.034266949 0.4077816 0.54105854 -0.4944067 -2.8837557 -5.6387005 -7.6428537 -8.3849449 -8.561986][-7.1192226 -7.4719057 -6.9887028 -6.0836973 -4.9597993 -3.74873 -2.7989244 -2.7908854 -2.4020004 -3.1708837 -4.8090935 -6.7197127 -8.0586853 -8.5764389 -8.9619932][-7.3033891 -7.358315 -7.1276445 -6.2778707 -5.7113161 -5.0576973 -4.5385761 -4.3505068 -3.9520524 -4.7131853 -5.9809656 -6.921001 -7.8092265 -7.8464584 -8.125288][-8.0435333 -8.0595179 -8.0466089 -7.1532855 -6.4874973 -5.7970152 -5.282732 -5.2415295 -5.1132059 -5.4181738 -6.3874531 -6.5326304 -7.5087624 -7.4545245 -7.3638887][-7.1167016 -7.42775 -6.8531413 -6.4950895 -6.0196433 -5.4680238 -4.9419422 -4.9196439 -5.0631833 -5.3605175 -5.9426794 -5.7340059 -6.1409693 -6.050611 -6.1685138][-8.2328615 -7.9396334 -7.8862047 -7.2437339 -6.6596513 -6.31921 -5.9423237 -6.3435497 -6.5264406 -6.5159683 -6.5511017 -6.3944778 -6.308403 -6.3934832 -6.1661739]]...]
INFO - root - 2017-12-15 12:46:50.571335: step 14010, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 55h:38m:55s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:46:57.058935: step 14020, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 57h:41m:18s remains)
INFO - root - 2017-12-15 12:47:03.596965: step 14030, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 55h:35m:51s remains)
INFO - root - 2017-12-15 12:47:10.099133: step 14040, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 56h:22m:48s remains)
INFO - root - 2017-12-15 12:47:16.672675: step 14050, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 58h:44m:43s remains)
INFO - root - 2017-12-15 12:47:23.071222: step 14060, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 55h:24m:44s remains)
INFO - root - 2017-12-15 12:47:29.451620: step 14070, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 55h:22m:43s remains)
INFO - root - 2017-12-15 12:47:35.785396: step 14080, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 55h:39m:05s remains)
INFO - root - 2017-12-15 12:47:42.199301: step 14090, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 56h:40m:39s remains)
INFO - root - 2017-12-15 12:47:48.552074: step 14100, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 55h:41m:58s remains)
2017-12-15 12:47:49.037147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3719797 -4.4210324 -3.6759529 -3.557373 -2.9717355 -1.8066583 -0.99808884 -0.34459782 0.42726707 -1.5343046 -2.9101009 -5.0647259 -6.3469357 -7.7575197 -7.8002772][-5.9860516 -5.6873536 -5.6243405 -4.8214025 -3.9376233 -2.9076672 -1.6522169 -1.1906214 -0.77238894 -2.5089574 -3.2963653 -5.1203756 -6.3981476 -8.2612114 -8.7275753][-6.6030197 -5.3510904 -4.4250689 -3.7052116 -2.7741761 -1.8822989 -0.80548334 -0.20012426 0.25796127 -1.8788733 -3.4803867 -5.3671637 -6.2758493 -7.5139666 -7.7552586][-5.3147421 -4.2774429 -3.3019481 -1.335062 0.052089691 0.85362816 1.7809744 1.5518684 1.2750216 -0.80351257 -2.2808595 -4.6546392 -6.2323136 -7.6247177 -7.7781329][-6.2243438 -4.1101389 -2.4798694 -0.87685347 0.734045 2.5136156 3.6691895 3.233139 2.6664648 0.104074 -1.5293231 -3.6610975 -4.8669538 -6.3393645 -6.8086691][-5.1229377 -3.8587286 -2.8290915 -0.616632 1.3819132 3.0001869 4.2720938 4.3793535 4.212059 1.8200836 0.22577333 -1.9745693 -3.0282001 -4.1181498 -4.3580179][-4.409842 -2.6554689 -0.86301851 0.8781414 2.3821144 4.057291 4.6489925 4.6247616 4.6289673 2.2356424 1.0185022 -1.1664195 -2.1597919 -3.7009757 -4.0046973][-2.2355814 -0.99335861 -0.40189409 0.94342327 2.5591431 3.8655024 4.3686638 4.1130819 3.4223394 0.79793358 0.0092782974 -1.6130729 -2.86685 -4.447217 -4.2136779][-2.5160332 -1.3029566 -0.10620308 0.709795 1.1686697 1.8848734 2.2273026 2.1098928 2.0846424 -0.10159779 -0.90243292 -2.2553015 -3.138833 -4.2135234 -4.3619204][-3.3188057 -2.6413102 -2.4462814 -1.6867189 -0.42213202 -0.059018612 -0.17024899 -0.15036392 0.10794449 -1.8943939 -2.5091038 -3.1471534 -3.6175981 -4.2607684 -3.3979592][-2.5415077 -3.0459023 -3.6756763 -3.219677 -2.8389053 -2.2636623 -1.8444276 -2.1293907 -2.4848032 -3.7190406 -3.6211085 -4.5858517 -5.673213 -5.6586409 -4.1939898][-4.6265182 -3.8920639 -3.8917444 -4.1466036 -4.2763491 -4.1688347 -4.5138073 -4.8501825 -4.7393069 -5.9485984 -6.2199192 -5.8696222 -5.4926596 -5.9444947 -5.5346179][-5.1754556 -5.0049191 -5.0147705 -5.0421343 -4.9723644 -5.1544771 -5.4876719 -5.8494639 -6.0430384 -6.9314651 -6.6440878 -6.2711573 -6.2147017 -5.9935102 -5.0196085][-5.4808221 -5.3345547 -4.8499346 -4.9150362 -4.7479296 -4.6314058 -4.7101803 -5.1306572 -5.4966354 -6.1578703 -6.4034491 -6.3508554 -5.9411077 -5.55607 -5.2589054][-5.2332096 -4.2435217 -4.7707849 -5.5772128 -5.5373287 -5.34991 -5.2990007 -5.6494551 -6.2977676 -6.4283056 -6.5784349 -6.4368606 -6.349184 -6.1383438 -5.8782697]]...]
INFO - root - 2017-12-15 12:47:55.386808: step 14110, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 55h:10m:25s remains)
INFO - root - 2017-12-15 12:48:01.809546: step 14120, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 56h:42m:45s remains)
INFO - root - 2017-12-15 12:48:08.279070: step 14130, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 58h:04m:21s remains)
INFO - root - 2017-12-15 12:48:14.759133: step 14140, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 57h:55m:11s remains)
INFO - root - 2017-12-15 12:48:21.122308: step 14150, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 56h:07m:16s remains)
INFO - root - 2017-12-15 12:48:27.587565: step 14160, loss = 0.37, batch loss = 0.26 (12.6 examples/sec; 0.636 sec/batch; 56h:12m:06s remains)
INFO - root - 2017-12-15 12:48:34.061277: step 14170, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 57h:21m:48s remains)
INFO - root - 2017-12-15 12:48:40.526646: step 14180, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 56h:37m:19s remains)
INFO - root - 2017-12-15 12:48:46.929073: step 14190, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 55h:36m:50s remains)
INFO - root - 2017-12-15 12:48:53.419276: step 14200, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 55h:49m:54s remains)
2017-12-15 12:48:53.949545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4827886 -3.7353711 -4.5713177 -4.698637 -4.9390936 -4.6869807 -4.8779469 -4.9282842 -5.008647 -5.2420325 -6.4248023 -6.7685571 -7.1942568 -7.9217825 -7.9943142][-3.0280175 -3.1255069 -3.6419468 -3.6999784 -3.8763561 -3.9642575 -4.2312436 -4.33753 -4.6816397 -5.1133261 -6.2562642 -6.9052973 -7.5889168 -8.1473732 -7.8817964][-2.709569 -2.6028361 -2.4587746 -2.6513839 -3.0742612 -2.70849 -2.6334033 -2.8041253 -3.0726652 -3.8363352 -5.4240007 -6.2115107 -6.7474275 -7.4719229 -7.4465108][-3.8146727 -3.1365218 -2.5083661 -2.29921 -2.1363373 -1.4075608 -0.68744993 -0.80885553 -1.0816054 -2.3775091 -3.9737341 -4.7425284 -5.5663052 -6.4580469 -6.753026][-4.3148603 -3.2797933 -2.4748702 -2.0561514 -1.7249184 -0.9161253 -0.078825951 0.0059776306 -0.12680864 -1.1823421 -2.4039183 -2.8526816 -3.7658153 -5.2769556 -6.0272875][-4.148447 -3.0219421 -1.589458 -0.8177495 -0.25385571 0.7071743 1.5097294 1.5270166 1.4670486 0.34365463 -0.85382652 -1.1337309 -2.1066723 -3.8136179 -4.9708319][-3.875246 -2.9901471 -1.3026729 0.11435747 1.1534996 1.9988508 2.9331293 3.0557594 2.92842 1.7287922 0.4836731 -0.43120432 -1.9743896 -3.8636234 -5.1954632][-3.2276831 -2.3083034 -1.255033 0.24251652 1.7909012 2.7985344 3.6872063 4.1013193 4.1619215 3.2529545 1.7893314 0.29275179 -1.6101122 -3.7155533 -5.2270794][-3.4020004 -2.6037765 -2.1674161 -0.63380957 1.1868668 2.2784452 2.9675188 3.77919 4.3061304 3.2751369 1.7191763 0.059238911 -1.9603024 -3.5229917 -4.6730609][-4.3301029 -4.1811914 -3.5164876 -2.1576619 -0.84879923 0.42894459 1.1745319 1.7779741 2.4732313 1.4692125 -0.39224768 -1.4447999 -2.8361506 -4.2563238 -5.4302435][-6.5686789 -5.9941149 -5.6264033 -4.8124638 -3.7476077 -2.6323767 -1.7652764 -1.5654187 -1.2295632 -1.9337182 -3.4897161 -4.0998583 -4.8062425 -5.8565049 -6.3146954][-7.7462087 -7.4771876 -7.1109114 -6.9353342 -6.3008485 -5.2614622 -4.4588814 -4.3522186 -3.8477328 -4.4761171 -5.5275326 -5.6641445 -5.5780172 -6.2836657 -6.8481722][-8.7959833 -8.8760519 -8.3571281 -8.0713768 -7.6689 -6.8952589 -6.2176585 -6.0287123 -5.6816835 -5.8773756 -6.7552609 -6.4097037 -5.8025846 -6.1111994 -6.3309932][-8.449585 -8.6231165 -7.8031788 -7.4955182 -7.0455341 -6.2993574 -5.9563656 -5.8974833 -5.8717179 -6.0793047 -6.3822007 -6.2017403 -5.8023434 -5.9507785 -5.4135084][-8.9776974 -8.6163082 -8.0319977 -7.6839895 -6.888576 -6.3499327 -6.2384605 -6.535768 -6.7298832 -6.7803535 -6.8282909 -6.8655148 -6.4665275 -5.9712229 -5.6487718]]...]
INFO - root - 2017-12-15 12:49:00.375429: step 14210, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 58h:44m:30s remains)
INFO - root - 2017-12-15 12:49:06.893298: step 14220, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 55h:37m:37s remains)
INFO - root - 2017-12-15 12:49:13.378322: step 14230, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 57h:17m:11s remains)
INFO - root - 2017-12-15 12:49:19.733964: step 14240, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 57h:48m:38s remains)
INFO - root - 2017-12-15 12:49:26.229749: step 14250, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 58h:01m:37s remains)
INFO - root - 2017-12-15 12:49:32.647817: step 14260, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 56h:37m:31s remains)
INFO - root - 2017-12-15 12:49:39.032484: step 14270, loss = 0.23, batch loss = 0.11 (12.9 examples/sec; 0.622 sec/batch; 54h:58m:05s remains)
INFO - root - 2017-12-15 12:49:45.437709: step 14280, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 55h:29m:20s remains)
INFO - root - 2017-12-15 12:49:51.925691: step 14290, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 56h:35m:59s remains)
INFO - root - 2017-12-15 12:49:58.355106: step 14300, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 57h:01m:55s remains)
2017-12-15 12:49:58.848034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1845713 -5.55122 -7.3723612 -9.47056 -9.7733126 -8.0921907 -6.8337812 -5.2718129 -4.3362789 -5.5737047 -5.0225878 -5.5378261 -6.2745829 -6.9772186 -7.0064983][-6.8118753 -7.098258 -7.2490554 -8.8038568 -9.6719923 -9.3936367 -8.955924 -7.694983 -6.54774 -7.2256646 -6.5972443 -6.6926994 -6.8860211 -6.6029205 -7.5069647][-6.0557456 -7.3671703 -8.1980438 -8.0629311 -6.9810181 -6.7549491 -6.678359 -6.8300009 -6.8525538 -7.46924 -7.1680193 -7.4344773 -7.8019929 -7.6874232 -7.2891068][-4.1668139 -5.8721018 -7.4117885 -7.4353552 -6.5519876 -4.4709487 -2.9422002 -2.9988451 -4.1697459 -5.1807051 -5.1128979 -5.6579638 -5.1527786 -5.7757258 -6.5213866][-2.2181931 -2.8883033 -3.9445808 -4.1525593 -3.4538035 -2.5203619 -1.8306732 -1.6424494 -1.8494925 -3.4534311 -4.1475716 -4.696095 -4.878499 -5.0130987 -5.109592][-2.4514365 -3.5324626 -3.865778 -2.7526007 -0.89836836 0.61871529 1.6213512 1.1577158 0.81387997 -0.47579527 -1.2851772 -2.6405878 -2.4442692 -3.2600079 -3.7548947][-3.2583518 -2.8341827 -2.9236698 -2.6274314 -1.0610175 1.0445471 1.9558821 2.2006006 2.1291656 0.7213459 -0.29705334 -1.620904 -2.4780154 -4.1654348 -4.2737322][-4.0604591 -3.935925 -3.3741798 -2.360518 -0.98260355 0.62384796 1.8206873 1.5051308 1.3262081 0.56482315 -0.36103678 -1.4496689 -2.9708219 -4.1917443 -4.918932][-5.3124752 -4.5504217 -3.8569303 -2.8986959 -1.5888486 -0.14866543 1.4317255 2.0034761 1.8672457 0.55044365 -1.1505809 -2.8034191 -4.3377976 -5.0967946 -5.6848178][-4.5664291 -4.0686164 -3.8361144 -3.0539265 -1.5967975 -1.0451441 -0.7221899 -0.72603559 -0.43222809 -1.3885193 -2.9218736 -4.2653213 -6.3714914 -6.8196979 -6.6637115][-6.7525578 -5.5634184 -4.4737778 -3.929718 -2.6225643 -1.3464646 -1.0543022 -1.382823 -1.1871166 -2.9761362 -4.7357726 -5.658721 -7.3958473 -7.1379538 -6.8858013][-6.3084702 -6.4783068 -6.7888112 -6.2414808 -4.9975228 -3.286377 -2.1051688 -2.8397894 -2.7461886 -4.7191057 -6.1608896 -5.640399 -6.2000027 -6.753643 -6.967988][-7.0158134 -5.6977582 -5.5594454 -6.115509 -6.8798351 -6.1778584 -5.740746 -5.8790717 -5.5584946 -6.5620122 -7.2734752 -6.4323144 -6.4323244 -7.3525906 -6.463563][-6.1686072 -5.3931341 -5.2613735 -5.3280725 -4.752913 -5.1880894 -5.6117315 -6.1506257 -6.8217163 -6.957314 -7.3464165 -6.8668523 -6.6985416 -7.1470165 -6.5123172][-6.4455109 -6.9235086 -6.8376255 -6.9459987 -6.7345653 -6.6033826 -6.5605216 -7.3377557 -7.5897746 -7.5043736 -7.2293735 -7.0844173 -6.4592109 -5.6985517 -5.6951823]]...]
INFO - root - 2017-12-15 12:50:05.255709: step 14310, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 59h:04m:50s remains)
INFO - root - 2017-12-15 12:50:11.796519: step 14320, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 57h:47m:29s remains)
INFO - root - 2017-12-15 12:50:18.304068: step 14330, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:43m:48s remains)
INFO - root - 2017-12-15 12:50:24.796189: step 14340, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 57h:56m:06s remains)
INFO - root - 2017-12-15 12:50:31.247992: step 14350, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 55h:58m:30s remains)
INFO - root - 2017-12-15 12:50:37.688999: step 14360, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 57h:06m:46s remains)
INFO - root - 2017-12-15 12:50:44.137340: step 14370, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 56h:18m:40s remains)
INFO - root - 2017-12-15 12:50:50.612561: step 14380, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 57h:46m:14s remains)
INFO - root - 2017-12-15 12:50:57.114117: step 14390, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 59h:00m:31s remains)
INFO - root - 2017-12-15 12:51:03.524519: step 14400, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 58h:19m:16s remains)
2017-12-15 12:51:04.054072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1766424 -3.4337802 -2.2172961 -2.5456891 -2.9086356 -2.6154981 -2.2962127 -1.8652158 -1.7316198 -2.8289752 -3.7286215 -4.8720064 -6.3345814 -6.2454028 -6.8257346][-5.0458488 -4.6300516 -4.4252243 -3.4735532 -2.0900044 -1.9254413 -2.0253463 -1.9950676 -1.7982283 -2.4353971 -3.0625257 -3.9505928 -5.1092997 -5.648469 -6.6192665][-5.4801226 -5.1536379 -4.9234023 -4.3936539 -3.6351714 -2.6004639 -1.4102216 -0.91990614 -0.77738476 -2.122551 -3.173461 -4.0899692 -5.5361252 -5.5811892 -6.2988329][-5.1685476 -4.9801025 -5.3611355 -4.6593933 -3.557539 -2.3474231 -1.4452252 -0.81860209 -0.069969177 -1.0190005 -2.1003323 -3.5848913 -5.4325466 -5.9280777 -6.7571063][-6.0311041 -4.937798 -3.8630595 -3.6410022 -3.0959482 -2.3886352 -1.3915234 -0.71008873 -0.1473527 -0.93452692 -1.8489633 -2.9509206 -4.6705008 -5.1718073 -6.5481315][-5.4884286 -5.3389869 -5.0576997 -3.5985355 -1.7499166 -0.82408953 0.0040402412 0.30714273 0.60826159 -0.63993406 -1.8102593 -3.217031 -5.2529464 -5.2736211 -6.0169344][-4.7135534 -4.2548418 -3.3869338 -2.535614 -1.1428895 -0.060042858 1.0624261 1.5085807 1.6070924 -0.16513443 -1.8438187 -3.3578076 -5.1105785 -5.2128143 -6.1816759][-4.8382263 -3.594852 -2.7066002 -1.5203447 -0.060268402 1.3084025 2.5153298 2.2734675 1.8876586 0.18079138 -1.4759898 -3.3641944 -5.574192 -5.5582161 -6.4961958][-3.9503806 -3.5807872 -2.4427233 -1.0006609 0.33850908 1.4676118 2.2429376 2.4062276 2.1256928 0.24230337 -1.912734 -3.684938 -5.6992445 -5.9513979 -6.6845188][-3.5068107 -2.9922385 -2.0911222 -1.1617713 -0.35151529 0.60209513 1.3126464 1.6798043 1.6727405 -0.25549603 -1.883224 -3.5642986 -5.3234668 -5.7434125 -6.7012639][-6.2254143 -5.05326 -3.7034621 -2.3437586 -0.93113947 -0.67820454 -0.47590065 -0.76478243 -1.0789089 -2.580483 -4.0571184 -5.2169051 -6.2101612 -6.2086763 -7.1226115][-7.1589246 -6.7294521 -6.1614141 -4.8594666 -3.6596909 -2.9510436 -2.7200236 -3.1795578 -3.475688 -4.7408466 -5.9124866 -6.5106525 -7.0664577 -6.7187681 -7.2806263][-7.1923909 -7.005177 -6.7264571 -6.3606815 -5.7380958 -4.6317363 -3.8952439 -4.32514 -4.4527521 -4.9720879 -5.7087669 -6.0170794 -6.3273468 -5.9056268 -6.30641][-7.5470686 -7.3411007 -7.0273318 -6.2804146 -5.5964718 -5.754179 -5.9345722 -5.6377306 -5.2565193 -5.4779167 -5.3385057 -5.7279587 -5.3918753 -5.306787 -5.6301489][-8.49395 -8.1166105 -7.44638 -7.4636025 -7.1848936 -6.8477063 -6.4040084 -6.865243 -7.4267688 -6.9824781 -6.2317576 -6.0137739 -5.8358812 -5.5881786 -5.3508978]]...]
INFO - root - 2017-12-15 12:51:10.589084: step 14410, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 56h:20m:21s remains)
INFO - root - 2017-12-15 12:51:17.038716: step 14420, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 56h:27m:14s remains)
INFO - root - 2017-12-15 12:51:23.454029: step 14430, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 56h:58m:43s remains)
INFO - root - 2017-12-15 12:51:29.933006: step 14440, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 57h:36m:34s remains)
INFO - root - 2017-12-15 12:51:36.284728: step 14450, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 56h:37m:51s remains)
INFO - root - 2017-12-15 12:51:42.780231: step 14460, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 58h:28m:57s remains)
INFO - root - 2017-12-15 12:51:49.243112: step 14470, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 55h:19m:35s remains)
INFO - root - 2017-12-15 12:51:55.671985: step 14480, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 55h:32m:06s remains)
INFO - root - 2017-12-15 12:52:02.053688: step 14490, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 56h:39m:46s remains)
INFO - root - 2017-12-15 12:52:08.509057: step 14500, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 57h:55m:21s remains)
2017-12-15 12:52:09.079858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6584992 -5.9721193 -6.0032334 -6.0224662 -6.524425 -6.5994782 -5.6825457 -3.8186345 -2.759614 -2.1461391 -2.7077885 -4.2581453 -5.0464315 -5.7209854 -6.1814909][-4.9362521 -4.987679 -5.1189661 -5.6661844 -6.4373221 -6.5058594 -6.2547979 -4.9701738 -3.4131017 -2.5077653 -3.1095886 -5.1825161 -5.9777493 -5.9348316 -6.0210524][-3.9114435 -4.4432459 -5.4020014 -5.5045395 -5.9886103 -6.4471602 -6.3039436 -5.1585722 -4.0963116 -3.7574346 -4.1217108 -6.4053774 -7.0367737 -7.3945661 -6.909327][-4.7241979 -4.4259863 -4.5309267 -4.1705136 -5.0225816 -4.8110027 -4.377882 -3.5926242 -2.8243666 -3.2845731 -4.5327549 -7.0053272 -7.9735966 -7.8741579 -7.7676764][-5.2013063 -3.9870665 -2.8304071 -2.1244884 -1.7057552 -0.76907873 -0.2991271 0.313879 0.74768353 -0.68722486 -2.7248764 -5.9709511 -7.5421853 -7.9366536 -7.7970772][-3.9692473 -2.3243308 -1.6944113 -0.40083742 1.0799103 2.2290716 3.5080605 4.0191154 4.6173563 2.8036413 0.58954525 -3.1276698 -5.338522 -6.6494617 -7.1800375][-3.8348036 -2.3519812 -0.75222397 0.85073471 2.0872574 3.3594866 4.2675667 4.7250929 4.8803139 3.3101406 1.6694469 -1.3970404 -3.9597602 -5.2070026 -6.2310772][-2.9139485 -1.769186 -0.81738329 0.50919914 1.9860792 2.950633 3.921174 4.1199703 3.6185722 2.7130623 1.2105131 -1.8187857 -3.4833527 -4.6643467 -5.8054147][-3.5592365 -2.5857067 -1.6453314 0.12193394 1.337594 2.059371 2.4616957 2.4376621 2.8087196 1.3921585 -0.67057562 -2.9491663 -3.8965254 -4.7934556 -5.2399187][-4.7005091 -4.0717249 -3.5509157 -2.1136146 -0.84125519 0.063983917 0.23582888 0.33665276 0.2021327 -1.2036881 -2.4810629 -4.4485316 -5.8441629 -5.8704133 -6.0052323][-6.8784628 -6.6993504 -6.2456269 -5.3673544 -4.7891068 -3.547883 -2.4699216 -2.5927477 -3.16818 -4.1328344 -5.1868715 -6.3608341 -7.03356 -7.467968 -7.8851113][-8.1721745 -8.300868 -7.8855944 -7.3909159 -6.9453187 -6.6294231 -6.2762704 -6.0343819 -5.3517089 -6.3261385 -6.6998162 -7.1301112 -7.6883793 -8.179903 -8.3998][-9.3706989 -9.5859671 -9.121192 -8.5272121 -7.7137475 -7.2318068 -7.4293914 -7.703784 -7.7634535 -7.9536767 -7.7404394 -7.1200094 -6.851109 -6.8579159 -7.1424971][-8.37504 -8.7678909 -9.0710077 -8.406353 -7.4215527 -7.0854239 -6.6036177 -6.916832 -7.1866145 -7.4014659 -7.7339087 -7.1618886 -7.0823517 -6.5880184 -6.2266531][-6.6942773 -7.021647 -7.5093155 -7.5189786 -7.5610962 -7.4395308 -6.9676547 -7.1153455 -6.5832458 -7.1146049 -7.7869339 -7.7140303 -7.5047522 -7.2965674 -6.6696844]]...]
INFO - root - 2017-12-15 12:52:15.499866: step 14510, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 54h:56m:45s remains)
INFO - root - 2017-12-15 12:52:21.943224: step 14520, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 58h:23m:14s remains)
INFO - root - 2017-12-15 12:52:28.392801: step 14530, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 54h:22m:15s remains)
INFO - root - 2017-12-15 12:52:34.820437: step 14540, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 56h:40m:54s remains)
INFO - root - 2017-12-15 12:52:41.258549: step 14550, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 57h:58m:29s remains)
INFO - root - 2017-12-15 12:52:47.738121: step 14560, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 59h:29m:21s remains)
INFO - root - 2017-12-15 12:52:54.122283: step 14570, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:53m:22s remains)
INFO - root - 2017-12-15 12:53:00.461201: step 14580, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 55h:26m:28s remains)
INFO - root - 2017-12-15 12:53:06.790751: step 14590, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 56h:51m:13s remains)
INFO - root - 2017-12-15 12:53:13.230112: step 14600, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.629 sec/batch; 55h:32m:52s remains)
2017-12-15 12:53:13.722049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2196636 -3.1032939 -3.395524 -3.7012053 -3.7326131 -3.5364604 -3.2448239 -2.9771442 -2.5330834 -3.7328298 -4.5209918 -5.1814547 -5.9904795 -6.2581019 -7.0226569][-3.6350465 -3.6599364 -3.7104216 -3.9706571 -3.8557327 -3.4683466 -3.2442875 -2.888093 -2.7979784 -3.9439421 -4.4440851 -5.4736395 -6.2421365 -6.7273583 -7.3749228][-2.3674359 -2.4531479 -2.5528998 -2.8875418 -2.816782 -2.5638757 -2.2508793 -1.997355 -1.7678981 -2.9254155 -3.593698 -4.43067 -5.4461508 -5.9766464 -6.83462][-2.2250209 -2.130981 -2.18707 -2.053411 -1.7124214 -1.2604656 -0.77469587 -0.65073967 -0.59296227 -2.0735612 -3.0342493 -4.1409168 -5.4122581 -5.8757992 -6.754591][-2.4696903 -1.9130788 -1.4437838 -1.0773654 -0.45828247 -0.005294323 0.46939087 0.49299717 0.3815527 -1.3071342 -2.3944945 -3.6844654 -5.0409441 -5.7390695 -6.6908908][-3.6052041 -2.9700027 -1.9580622 -1.2047749 -0.482903 0.38196564 1.175354 1.1497812 0.87996483 -1.0637507 -2.3449879 -3.8805308 -5.3474522 -6.0313034 -6.9694242][-3.8751428 -3.2430305 -2.4387236 -1.1551557 0.072247982 1.0911465 1.9094172 1.9000673 1.6634512 -0.55359507 -2.266933 -3.996237 -5.5964646 -6.433877 -7.4184194][-4.03735 -3.2199578 -2.0499897 -0.61556196 0.46059704 1.3006477 1.9780149 1.9457073 1.7027855 -0.47923183 -2.2194633 -4.0710497 -5.7315264 -6.5271034 -7.4107962][-4.6032863 -3.5373688 -2.6046004 -1.2133913 -0.0084428787 0.59438229 1.072011 1.3099308 1.3040934 -0.88520956 -2.5732517 -4.313839 -5.9949102 -6.9870381 -7.799469][-4.93947 -4.2371817 -3.1708074 -1.9515724 -0.902442 -0.57502031 -0.17816067 0.18410158 0.36582661 -1.8695531 -3.3179827 -5.0055346 -6.4196091 -7.3449149 -8.3111343][-6.2147274 -5.4355412 -4.4252381 -3.3056517 -2.7317653 -2.4869127 -2.2899132 -2.141439 -1.9243903 -3.8783405 -5.25527 -6.4716311 -7.4791451 -8.036622 -8.5890684][-6.3913927 -5.9068842 -5.1703672 -4.3114119 -3.899317 -3.7804885 -3.7947359 -3.6854968 -3.7077217 -4.9949751 -5.9104424 -6.8511052 -7.4093313 -7.9007945 -8.3329105][-6.9589181 -6.3073263 -5.7695389 -5.2326651 -4.7609777 -4.8489418 -4.8662052 -4.9621077 -5.0391264 -5.9112682 -6.6292653 -6.8522072 -7.3346062 -7.6360168 -7.6448407][-6.4734731 -6.1567159 -5.634963 -5.1864753 -4.8953638 -5.0983977 -5.2146926 -5.2997866 -5.4234505 -6.0953631 -6.157691 -6.253654 -6.2860103 -6.4641767 -6.6465645][-7.8284669 -7.4854393 -7.1207442 -6.5313449 -6.1666965 -6.2443452 -6.4023457 -6.71056 -6.8738956 -6.8136578 -6.8739443 -6.8147035 -6.6080985 -6.402575 -6.1982751]]...]
INFO - root - 2017-12-15 12:53:20.078871: step 14610, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 55h:17m:24s remains)
INFO - root - 2017-12-15 12:53:26.445643: step 14620, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 56h:26m:53s remains)
INFO - root - 2017-12-15 12:53:32.790720: step 14630, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 58h:30m:50s remains)
INFO - root - 2017-12-15 12:53:39.178701: step 14640, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 56h:23m:32s remains)
INFO - root - 2017-12-15 12:53:45.597289: step 14650, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 56h:19m:08s remains)
INFO - root - 2017-12-15 12:53:52.086667: step 14660, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 56h:51m:38s remains)
INFO - root - 2017-12-15 12:53:58.523789: step 14670, loss = 0.33, batch loss = 0.22 (12.2 examples/sec; 0.653 sec/batch; 57h:40m:00s remains)
INFO - root - 2017-12-15 12:54:04.920806: step 14680, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 56h:34m:37s remains)
INFO - root - 2017-12-15 12:54:11.451282: step 14690, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 58h:17m:36s remains)
INFO - root - 2017-12-15 12:54:17.860687: step 14700, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 57h:12m:46s remains)
2017-12-15 12:54:18.335154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.406889 -5.9986711 -7.1088591 -8.7212877 -9.16266 -8.5209112 -7.6367145 -6.501894 -5.1820488 -5.9175467 -5.8562269 -5.358345 -5.8046813 -7.0132418 -7.4636574][-5.1255016 -6.2123718 -8.0836287 -9.3842354 -10.122486 -9.5055943 -9.18717 -8.6617336 -7.2309561 -7.7202415 -7.446569 -7.1132216 -7.9635205 -9.1776867 -9.4217005][-4.0340786 -4.8899064 -6.8748364 -8.27608 -9.9749994 -9.1215153 -8.3766928 -8.0869474 -7.1689491 -7.9646721 -8.08825 -8.4726162 -8.8835487 -10.167897 -10.435433][-5.1746335 -5.30219 -5.7711 -6.5912652 -7.5625172 -6.5382662 -6.3372741 -6.0454049 -5.1937284 -6.5553565 -7.0466146 -7.9637671 -9.21294 -10.664759 -10.669725][-6.3484082 -6.8746891 -6.9361773 -6.9306555 -6.8921795 -5.5169606 -4.7408304 -3.9094188 -2.8046432 -4.4581213 -5.4356766 -6.7233634 -8.3322058 -10.318178 -11.205119][-7.9478717 -7.6638308 -7.3549843 -5.7955694 -4.0801735 -2.2109613 -1.042522 -0.26951265 0.67116451 -1.5956674 -3.3906617 -5.129302 -6.9318781 -9.2588377 -10.610394][-6.7781258 -6.205512 -5.3308425 -3.1831126 -0.66873884 1.9102335 4.449378 5.22799 5.1808138 2.171442 -0.1845603 -2.5242839 -4.7581444 -7.1701612 -8.53527][-6.072341 -5.4401417 -3.8748765 -0.94854355 2.3237 4.9057407 7.1117182 7.4790993 7.3370943 3.9314251 1.2996864 -0.78143883 -3.55094 -6.2119045 -7.226902][-6.2668753 -5.7263527 -4.7318277 -2.1036053 0.19084072 2.2942953 5.0060072 5.8250294 5.74403 2.0405474 -0.99457169 -2.4712443 -4.3250618 -6.7390633 -7.54101][-7.8072948 -6.7849131 -5.9475174 -3.9215741 -2.1671867 -0.2964015 1.9592371 2.199338 2.016036 -1.5986514 -4.222 -5.5737162 -7.1987429 -8.6074486 -8.4111357][-10.698138 -10.345689 -9.8282938 -8.78964 -7.4601774 -5.1827722 -3.0418911 -2.8304205 -2.1823893 -4.8673916 -6.4516239 -7.4301357 -8.5950212 -9.6526995 -9.3915052][-11.465408 -11.179503 -11.094858 -10.62096 -9.880434 -8.2588024 -6.9248838 -6.4959822 -5.5768261 -7.4017997 -7.500051 -7.5025611 -8.2391424 -8.8917351 -9.0017023][-10.707315 -11.157043 -11.485378 -11.343728 -10.961253 -9.6884041 -8.7492981 -8.1470537 -7.5608692 -8.9254475 -9.077486 -8.7729416 -8.534215 -8.5953989 -8.7317505][-9.0224285 -9.2493324 -9.2289619 -9.5054588 -8.8832932 -8.0747395 -7.6439648 -7.4753666 -7.2308259 -8.044054 -7.8397713 -7.9350705 -7.9518332 -7.4757781 -7.2675848][-8.8664331 -9.37948 -9.8188877 -10.172389 -9.3114281 -8.6136436 -8.0964108 -7.8750582 -7.6060619 -8.0067692 -7.8285251 -7.5263948 -7.350534 -7.30852 -7.1941857]]...]
INFO - root - 2017-12-15 12:54:24.800839: step 14710, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 55h:28m:18s remains)
INFO - root - 2017-12-15 12:54:31.225356: step 14720, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 56h:05m:50s remains)
INFO - root - 2017-12-15 12:54:37.658363: step 14730, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 56h:23m:48s remains)
INFO - root - 2017-12-15 12:54:44.088664: step 14740, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 57h:17m:39s remains)
INFO - root - 2017-12-15 12:54:50.504012: step 14750, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 55h:43m:41s remains)
INFO - root - 2017-12-15 12:54:56.887364: step 14760, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 56h:06m:22s remains)
INFO - root - 2017-12-15 12:55:03.423811: step 14770, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 58h:23m:10s remains)
INFO - root - 2017-12-15 12:55:09.826012: step 14780, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 56h:31m:07s remains)
INFO - root - 2017-12-15 12:55:16.316949: step 14790, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 55h:57m:19s remains)
INFO - root - 2017-12-15 12:55:22.737017: step 14800, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 55h:49m:38s remains)
2017-12-15 12:55:23.303446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2099724 -3.0035534 -2.7230182 -2.234663 -1.4582348 -1.4520659 -1.9301314 -1.383317 -0.912065 -2.606997 -3.0116711 -3.8925121 -5.2463088 -7.0343351 -8.01299][-2.6294966 -2.4730296 -2.3024549 -2.9577985 -3.1289897 -2.181284 -1.6991754 -1.8056912 -1.5973253 -2.9729443 -3.6165972 -4.631361 -5.2955742 -6.2930913 -6.9011059][-3.404573 -2.9509363 -2.8879414 -2.4430027 -1.5362711 -0.75854349 -0.6077528 -0.33230877 -0.24982405 -1.6735978 -2.2583857 -3.0664415 -3.8916051 -5.1311536 -5.8589015][-4.3233595 -3.3484526 -2.5705428 -1.9308281 -1.7619724 -0.74159765 0.30060673 0.2966156 0.30191755 -1.1204929 -1.8693442 -2.6441865 -3.7985435 -5.1233215 -5.2582188][-4.0405254 -2.6319146 -1.6206112 -1.1518774 -0.67486334 0.4271574 0.83760929 0.575407 0.58854485 -0.8655715 -1.2722573 -2.2837176 -3.6568241 -4.9489069 -5.0463505][-3.3284135 -2.2782598 -1.2774892 -0.048307896 0.89979744 1.592432 1.913538 1.6543331 1.2474966 -0.40855646 -0.89482117 -1.5286884 -2.4235992 -3.7123625 -4.1369448][-3.1877656 -1.7297134 -0.08382225 0.96600533 1.6480446 2.478282 2.9179926 2.7180824 2.3496199 0.40906715 -0.046983242 -1.325057 -2.5595884 -3.4700861 -3.9188497][-1.6676083 -0.3344965 1.1156483 2.9922609 3.9454947 4.1829185 3.9342756 3.3437471 2.9080963 0.72293949 0.17885971 -0.94539595 -2.1352029 -3.2803025 -3.8223462][0.27524376 1.0536537 2.2435598 2.79737 2.7909946 3.3545198 3.2227917 2.7087412 2.1868267 0.74053669 0.72661591 -0.73555231 -2.4698982 -3.5491514 -3.6784902][0.08736372 0.74736214 0.60817432 1.4400454 2.0217171 1.9965391 1.4846601 1.2655859 1.1459322 -0.35830832 -0.79429817 -0.9412508 -1.9795871 -3.2260575 -4.0917482][-0.75316858 -1.4770751 -1.5892224 -1.3475208 -1.4001966 -1.1151953 -0.777153 -0.36142349 0.014724731 -1.1719022 -1.2742047 -2.5533338 -4.0926228 -4.9196582 -5.4040422][-3.0503755 -3.2369318 -3.9643691 -4.2380152 -4.17792 -3.7896721 -3.0512772 -3.0978193 -2.7083945 -3.3461275 -3.6065116 -4.209177 -4.7979693 -5.6859446 -5.9164052][-3.2527337 -4.4950047 -5.5366764 -5.708303 -6.3150854 -6.06206 -5.5196543 -4.654439 -4.61518 -5.043262 -5.1839981 -5.4596567 -6.3050294 -6.8426542 -7.0162258][-4.5504208 -5.3661003 -6.0384197 -6.6118851 -6.7649837 -6.4034023 -6.1734838 -6.211215 -5.8590274 -5.5857744 -5.8163137 -6.3156633 -6.9609923 -7.272325 -7.8618536][-4.4102268 -6.0298052 -7.1075983 -7.6037726 -8.0018244 -7.8267412 -7.0762691 -6.5433598 -6.1235108 -6.4484882 -6.7477164 -6.9852548 -6.8255405 -7.2135205 -7.242384]]...]
INFO - root - 2017-12-15 12:55:29.748463: step 14810, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 56h:50m:07s remains)
INFO - root - 2017-12-15 12:55:36.120411: step 14820, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 56h:16m:50s remains)
INFO - root - 2017-12-15 12:55:42.561323: step 14830, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:41m:47s remains)
INFO - root - 2017-12-15 12:55:48.980102: step 14840, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 57h:09m:38s remains)
INFO - root - 2017-12-15 12:55:55.383112: step 14850, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 57h:30m:56s remains)
INFO - root - 2017-12-15 12:56:01.816398: step 14860, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 55h:49m:40s remains)
INFO - root - 2017-12-15 12:56:08.286443: step 14870, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 56h:39m:16s remains)
INFO - root - 2017-12-15 12:56:14.640766: step 14880, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 57h:23m:55s remains)
INFO - root - 2017-12-15 12:56:21.073026: step 14890, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 57h:27m:12s remains)
INFO - root - 2017-12-15 12:56:27.533328: step 14900, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 58h:11m:48s remains)
2017-12-15 12:56:28.039293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2380667 -4.4361973 -4.4420252 -4.6315641 -4.635026 -4.5177536 -4.206409 -3.8179412 -3.7832756 -3.8895402 -5.3141093 -7.00072 -8.3285437 -9.4660778 -10.364206][-3.9686036 -4.0089607 -4.20802 -5.14386 -5.5060511 -5.8167415 -6.0292187 -5.3825154 -4.8513541 -5.0154467 -6.2357988 -7.9006543 -9.3479929 -10.701873 -11.361775][-3.9526157 -3.5029168 -3.7636819 -4.4701033 -4.7281461 -5.0400023 -5.4441204 -5.0668983 -4.5389996 -4.1688423 -4.9586725 -6.3494129 -7.6022072 -8.9165821 -9.6793928][-4.1687274 -2.9021268 -2.9685912 -3.2490125 -3.3764772 -3.2420659 -3.3171954 -3.3918629 -3.0020905 -2.4726343 -3.3688884 -4.6312332 -5.8415828 -6.9837904 -7.8218336][-3.849194 -2.795867 -2.3285584 -1.9907818 -1.8730044 -1.4885826 -0.92433596 -1.0420837 -0.98292303 -0.9315114 -1.8652468 -3.1712098 -4.5645905 -5.5520244 -6.103116][-2.6269116 -2.3356481 -2.0182405 -1.6459537 -1.1048312 -0.10716391 0.693408 0.75600624 0.77079773 0.5003891 -0.6172924 -2.365859 -3.7105465 -4.7803187 -5.4825592][-2.70581 -2.1656723 -1.3942537 -0.53211641 0.35424042 1.3940687 1.9995937 1.7881308 1.6341658 0.72905636 -1.1234865 -3.0199666 -4.4511828 -5.5663085 -6.2464652][-2.8577948 -2.390286 -1.6129346 -0.14666986 1.1114159 2.1386623 2.7924786 2.5679703 2.0162811 0.65092182 -1.4162798 -3.8393304 -5.3555965 -6.3517075 -6.695581][-3.629025 -2.6626596 -1.3832426 -0.32219124 0.80198956 1.8329382 2.2390156 1.9563723 1.5039482 0.53247833 -1.434814 -3.8337054 -5.3762808 -6.2843456 -6.2503633][-3.8606684 -3.5378122 -2.7321081 -2.0867214 -1.5061393 -0.47674608 0.21417522 -0.084430218 -0.30852222 -0.98697329 -2.6643772 -4.2592392 -5.1377039 -6.3221626 -6.8987894][-5.1522274 -5.0737877 -4.849154 -4.3466463 -3.8927717 -3.403657 -3.0241289 -2.9598761 -3.0550766 -3.5599332 -4.6866198 -5.6477752 -6.4245896 -7.3665633 -7.7684903][-6.1150904 -5.7551475 -5.4226627 -5.0749846 -4.8823614 -4.9038143 -4.8832135 -4.8288636 -4.8601694 -5.82013 -7.1883125 -7.4402981 -7.9204054 -8.7455292 -8.7465858][-7.7364521 -7.4342489 -6.9043555 -6.1698351 -5.6081905 -5.30853 -5.5202847 -6.1389704 -6.3627453 -7.2517653 -8.6938286 -8.69479 -8.5186882 -8.0579777 -7.3521652][-6.6569619 -6.3174219 -5.9130225 -5.279479 -4.8686886 -4.26322 -4.0832944 -4.7130232 -5.4828258 -6.2555857 -6.9983091 -7.3168387 -7.2677054 -6.8505545 -6.234683][-5.5228133 -4.8862476 -5.0266418 -5.1497812 -4.6948233 -4.3158007 -4.253027 -4.2964869 -4.6296649 -5.0588021 -5.4533691 -5.9706039 -6.2790947 -6.4601483 -6.2440438]]...]
INFO - root - 2017-12-15 12:56:34.469795: step 14910, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 56h:28m:45s remains)
INFO - root - 2017-12-15 12:56:40.842074: step 14920, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 56h:39m:43s remains)
INFO - root - 2017-12-15 12:56:47.239462: step 14930, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 55h:59m:20s remains)
INFO - root - 2017-12-15 12:56:53.640721: step 14940, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 57h:01m:28s remains)
INFO - root - 2017-12-15 12:57:00.066875: step 14950, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 55h:43m:07s remains)
INFO - root - 2017-12-15 12:57:06.477197: step 14960, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:02m:11s remains)
INFO - root - 2017-12-15 12:57:12.800323: step 14970, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 55h:23m:30s remains)
INFO - root - 2017-12-15 12:57:19.259274: step 14980, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 55h:20m:46s remains)
INFO - root - 2017-12-15 12:57:25.669814: step 14990, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 55h:44m:53s remains)
INFO - root - 2017-12-15 12:57:32.057763: step 15000, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 56h:09m:16s remains)
2017-12-15 12:57:32.556852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3614626 -2.6355653 -2.733747 -2.7419987 -2.6896377 -2.6360078 -2.6676416 -2.5291023 -2.4195428 -3.7618911 -4.3089962 -5.196352 -5.8367772 -6.2943335 -7.1278396][-2.3552885 -2.5578866 -2.7042933 -2.8506193 -2.9305692 -2.7241306 -2.4414096 -2.143343 -2.0385294 -3.6378169 -4.4989624 -5.6011481 -6.3901443 -6.8166962 -7.5782838][-2.4427228 -2.1771736 -1.8811483 -2.0472541 -2.0379372 -1.8607125 -1.6154394 -1.3719821 -1.3075209 -2.5838432 -3.1062932 -4.1734276 -5.2046947 -5.9870763 -7.2046404][-1.1106734 -0.72077513 -0.5033474 -0.37056208 -0.22632313 0.01761961 0.25823832 0.42787504 0.63376856 -0.93286705 -1.7943935 -3.1838484 -4.5284653 -5.24107 -6.123497][-1.2184229 -0.39704561 0.082645416 0.20281649 0.4118247 0.69612074 0.98757887 1.1359668 1.330441 -0.1550808 -1.0446396 -2.334136 -3.6301413 -4.5008283 -5.7525878][-1.7032032 -0.78126097 -0.14421177 0.40348291 0.854074 1.2554135 1.7317071 1.9062486 1.9293113 0.098333836 -1.0340233 -2.3041425 -3.5732741 -4.2825623 -5.3827066][-2.1556754 -1.0915747 -0.46370029 0.38741827 1.1891294 1.6633496 2.0747752 2.2969518 2.4713969 0.67299986 -0.52162504 -1.941617 -3.432559 -4.2466297 -5.4769287][-2.3184638 -1.441505 -0.73755121 0.32063341 1.4213948 2.2483535 2.8110719 2.8373055 2.8762374 1.0532813 -0.12083292 -1.6115212 -3.2857189 -4.203105 -5.3457217][-2.6146379 -1.7628202 -0.85342646 0.15783215 0.9842267 1.9930158 2.8661389 3.0015273 3.0201173 0.88046312 -0.45631266 -1.7923107 -3.2741556 -4.0974326 -5.3258829][-3.1288157 -2.3985596 -1.7108755 -0.8453145 -0.083117962 0.82170725 1.6910033 1.879559 2.0420508 0.1378932 -0.92367268 -2.4210773 -3.9468107 -4.605042 -5.7925806][-4.155612 -3.419508 -2.9260139 -2.229085 -1.4715838 -0.55168819 0.2487216 0.36010695 0.42030954 -1.3888831 -2.2592559 -3.5819101 -4.6787558 -5.1343 -6.1473336][-5.5742946 -4.7815304 -4.3231649 -3.7450273 -3.1890869 -2.558423 -1.9876938 -1.7548995 -1.6029968 -2.9975362 -3.3976789 -4.3748035 -5.31555 -5.4878645 -6.1000333][-6.2035007 -5.6577787 -5.3429413 -4.9577484 -4.4601955 -4.0462184 -3.6539378 -3.5587287 -3.5632606 -4.2583113 -4.5406346 -5.3810368 -6.0144491 -6.2013297 -6.4691324][-6.6793566 -6.2177424 -5.8157029 -5.32918 -4.899622 -4.6632185 -4.5784321 -4.6972451 -4.69522 -5.232461 -5.3274422 -5.7170463 -6.0173335 -6.0235071 -6.356349][-7.7127147 -7.4254127 -7.30554 -6.7757559 -6.37408 -6.0887079 -5.7830658 -5.919436 -6.1722665 -6.3873734 -6.3638639 -6.2402086 -6.2982373 -6.3097649 -6.31119]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 12:57:39.800208: step 15010, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 55h:44m:50s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 12:57:46.235156: step 15020, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 55h:25m:32s remains)
INFO - root - 2017-12-15 12:57:52.701658: step 15030, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 56h:34m:13s remains)
INFO - root - 2017-12-15 12:57:59.257252: step 15040, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.656 sec/batch; 57h:53m:04s remains)
INFO - root - 2017-12-15 12:58:05.653722: step 15050, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 56h:57m:28s remains)
INFO - root - 2017-12-15 12:58:12.189621: step 15060, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 57h:03m:51s remains)
INFO - root - 2017-12-15 12:58:18.555573: step 15070, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 56h:27m:19s remains)
INFO - root - 2017-12-15 12:58:25.022241: step 15080, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 57h:10m:57s remains)
INFO - root - 2017-12-15 12:58:31.480320: step 15090, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 55h:57m:16s remains)
INFO - root - 2017-12-15 12:58:37.820700: step 15100, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 55h:21m:19s remains)
2017-12-15 12:58:38.407955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6747017 -4.6989479 -5.4783945 -5.7129722 -5.2696276 -4.9005661 -4.4206314 -4.1241903 -3.8882661 -4.1527038 -4.3921022 -4.4820557 -5.1586027 -7.3196239 -8.00224][-2.7826252 -3.6748576 -4.4796653 -4.8277607 -5.214879 -5.5104885 -5.6402335 -5.5532346 -5.0164204 -4.9648046 -5.1509356 -4.9424868 -5.4962597 -6.5468903 -6.6188459][-2.2265978 -2.4580264 -2.8109636 -3.7000411 -4.2957706 -4.207387 -4.2257338 -4.5633764 -4.8483 -5.3966951 -5.5636892 -4.9849129 -5.3073549 -6.4816861 -6.4489408][-2.1858377 -1.7675047 -1.6347599 -2.1530447 -2.3850374 -2.2900567 -2.4477315 -2.9171381 -3.2833052 -4.3127213 -4.8233418 -4.9105234 -5.6669626 -7.0574021 -6.8710661][-1.4589019 -1.2159014 -1.4299812 -1.7705331 -1.7683778 -1.2427216 -0.5785141 -0.22301769 -0.047148228 -1.5799813 -2.9731593 -3.2320714 -4.2272196 -6.1471825 -6.3947539][-1.9151883 -1.5785785 -1.2758408 -1.0668778 -0.017763615 0.92413139 1.4971762 2.1347523 2.6453257 1.6297588 0.354105 -0.75488758 -3.0902071 -5.642024 -6.6145873][-3.4322896 -2.7928576 -1.9459891 -1.0947313 -0.10567093 0.98029137 2.0302095 2.695426 3.3297558 2.6660376 1.5309963 0.63561153 -1.6309175 -4.8663645 -6.3808508][-4.0824089 -3.6636705 -3.1485848 -1.7811937 -0.026212692 1.3970842 2.465415 3.0292215 3.5447893 2.4689541 1.3902626 0.75481987 -1.014605 -3.9426503 -5.2250905][-3.0862374 -3.4171166 -3.5399513 -2.5188594 -1.1192055 0.78531265 2.5456047 3.1472292 3.6190195 2.4238539 0.82294369 -0.40798235 -2.570055 -5.3902912 -6.2549949][-5.0336475 -3.8029308 -2.8518548 -2.2798657 -1.4324126 -0.41233397 0.24234104 1.2583027 1.7562304 0.25035191 -1.1952295 -2.4946251 -4.6143184 -6.5865426 -7.46849][-4.953721 -4.7879925 -4.8474216 -3.8523726 -2.8453836 -2.3059688 -1.6512642 -1.5116014 -1.4503689 -1.9590411 -2.644083 -4.0844383 -5.9970155 -8.0581646 -8.5123062][-7.6054459 -6.7344213 -5.7548656 -5.2930593 -4.8565121 -3.858969 -3.3266292 -3.335712 -3.3629432 -4.1754408 -5.0173779 -6.1207461 -7.0680027 -7.9518766 -8.3163452][-8.2830992 -8.0893412 -7.4321742 -6.6775031 -5.9326391 -5.4580231 -5.1969051 -5.2458286 -5.4572611 -5.6283278 -6.0174341 -6.3008966 -6.7455792 -7.4726176 -7.5439768][-7.0925241 -6.9961023 -6.91634 -6.1908569 -5.5541625 -5.1986241 -5.0152006 -5.1445255 -5.2603159 -5.7262411 -6.1031079 -6.423862 -6.7449036 -6.7552309 -6.5619035][-6.8363976 -7.4405241 -7.9289842 -7.7190833 -7.0750813 -6.2631316 -5.89251 -6.1728959 -6.2913685 -6.3793321 -6.3848929 -6.5182314 -6.5168481 -6.4037442 -6.267108]]...]
INFO - root - 2017-12-15 12:58:44.883467: step 15110, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 56h:23m:09s remains)
INFO - root - 2017-12-15 12:58:51.230049: step 15120, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.618 sec/batch; 54h:27m:31s remains)
INFO - root - 2017-12-15 12:58:57.729829: step 15130, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 57h:57m:55s remains)
INFO - root - 2017-12-15 12:59:04.087453: step 15140, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 56h:01m:28s remains)
INFO - root - 2017-12-15 12:59:10.499230: step 15150, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:01m:13s remains)
INFO - root - 2017-12-15 12:59:16.901597: step 15160, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.660 sec/batch; 58h:08m:23s remains)
INFO - root - 2017-12-15 12:59:23.307767: step 15170, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 55h:40m:00s remains)
INFO - root - 2017-12-15 12:59:29.777833: step 15180, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.623 sec/batch; 54h:52m:22s remains)
INFO - root - 2017-12-15 12:59:36.263332: step 15190, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:01m:29s remains)
INFO - root - 2017-12-15 12:59:42.606398: step 15200, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.623 sec/batch; 54h:55m:33s remains)
2017-12-15 12:59:43.105965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4690194 -3.2481356 -3.5008736 -4.4072046 -4.8722053 -5.0288644 -5.3397007 -4.724472 -3.8793614 -4.0496044 -4.3029375 -6.0388622 -7.0347061 -7.7364759 -8.5459366][-4.3978815 -4.1318126 -4.3541107 -5.107954 -5.8547053 -5.982523 -6.2902174 -5.8263917 -5.3378363 -5.075079 -5.2948422 -6.9096227 -7.8827786 -8.3865328 -9.2169952][-4.3487387 -3.9461851 -4.286231 -4.416625 -4.7228308 -5.394382 -5.6636734 -5.3478866 -4.957294 -5.1832304 -5.592042 -7.4398265 -7.9964108 -8.4879541 -9.1305637][-4.27602 -3.7826705 -4.0329161 -3.9068763 -4.1308689 -3.9476295 -3.6312308 -3.3763914 -3.1448317 -3.4529629 -3.6959534 -5.5594912 -6.1697068 -6.9148173 -7.6227255][-5.02776 -4.3259983 -4.1499796 -3.4335709 -3.1806793 -2.5551186 -1.6332321 -1.1778064 -0.62174273 -1.1814089 -1.6444831 -3.4997973 -4.2054162 -5.107008 -5.880723][-4.3203487 -4.0437851 -3.8790777 -3.0415306 -2.0454955 -0.85521507 0.64200115 0.89296627 0.97983932 -0.068361282 -0.87842226 -2.4892783 -3.6585283 -4.4088225 -5.2444029][-4.7368212 -4.3887463 -3.5804791 -2.5178275 -1.135725 0.34112358 1.7567205 2.4608641 2.9533491 1.8136272 0.39160252 -1.5215621 -3.0342937 -3.8821971 -4.9202666][-3.9244211 -3.493907 -2.7304549 -1.6996069 -0.19195223 1.3558474 2.7435045 3.4847689 3.7398853 2.4361973 0.89231014 -1.0218 -2.425611 -3.1159897 -4.1662068][-4.9001875 -3.7690828 -2.4521551 -1.5067263 -0.15540838 1.2590933 2.5061617 2.9520226 3.2759094 2.3110332 0.95540905 -1.1839018 -3.067328 -3.8846796 -4.6075983][-6.0470943 -4.8277326 -3.4981327 -2.3053598 -1.1578021 0.18348598 1.30966 1.8234091 1.9389296 0.69035244 -0.67210579 -2.0286999 -3.5516109 -4.377635 -5.4183197][-8.3670864 -7.03441 -5.621017 -4.42432 -3.1778512 -2.3168039 -1.4396 -1.2654262 -1.4864197 -2.2032046 -3.1751585 -4.1016593 -5.2523842 -5.7534266 -6.4027705][-8.7508135 -7.9074383 -7.0279713 -5.7079983 -4.6985092 -4.0015626 -3.4543405 -3.2895603 -3.2933106 -3.8605957 -4.8275938 -5.2354741 -5.9390144 -6.3337193 -6.6450033][-8.7281322 -8.5993052 -8.4884176 -7.5268173 -6.4617996 -5.5814457 -4.9559684 -4.7475953 -4.5356603 -4.7430239 -5.4886551 -5.5233927 -5.838593 -6.1465626 -6.354794][-7.0616655 -6.9883156 -7.37026 -7.1130309 -6.679482 -5.9225273 -5.2441387 -4.9707394 -5.0512342 -5.1196527 -5.7347722 -5.7394543 -5.7782803 -5.4629087 -5.4631643][-6.1593304 -6.3545251 -6.4966011 -6.4473953 -6.0886283 -5.6271839 -5.2400694 -5.2428117 -5.3438587 -5.4359884 -5.5005412 -5.5359907 -5.562624 -5.6688662 -5.6943717]]...]
INFO - root - 2017-12-15 12:59:49.567254: step 15210, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 55h:47m:17s remains)
INFO - root - 2017-12-15 12:59:56.008491: step 15220, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.620 sec/batch; 54h:36m:26s remains)
INFO - root - 2017-12-15 13:00:02.421071: step 15230, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 58h:58m:32s remains)
INFO - root - 2017-12-15 13:00:08.874292: step 15240, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 57h:32m:12s remains)
INFO - root - 2017-12-15 13:00:15.340737: step 15250, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 55h:27m:01s remains)
INFO - root - 2017-12-15 13:00:21.732542: step 15260, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 57h:22m:24s remains)
INFO - root - 2017-12-15 13:00:28.233810: step 15270, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 57h:59m:31s remains)
INFO - root - 2017-12-15 13:00:34.633675: step 15280, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 56h:13m:12s remains)
INFO - root - 2017-12-15 13:00:41.044647: step 15290, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 55h:24m:30s remains)
INFO - root - 2017-12-15 13:00:47.491551: step 15300, loss = 0.24, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 59h:40m:19s remains)
2017-12-15 13:00:48.008451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5938144 -3.7178569 -3.7843018 -4.6056757 -4.7636509 -5.4509315 -5.4515834 -5.372983 -5.5695171 -6.2452664 -6.3906212 -6.4420004 -7.3161016 -7.9542313 -8.3279076][-2.9156065 -3.5675244 -3.5581107 -3.2136183 -3.0212188 -3.5915394 -3.7716234 -3.5476866 -3.9251902 -4.984581 -5.4533844 -5.546175 -6.5837011 -6.3235345 -6.2198229][-2.9588261 -1.7903857 -1.5296564 -1.8699083 -1.899961 -2.2013798 -2.031178 -1.2965498 -1.0322499 -2.2884865 -3.3729134 -3.4859376 -4.4228597 -4.9218249 -6.0854573][-2.7055488 -1.1088672 -0.17924404 0.18273449 0.42466545 0.63763046 0.76320362 1.1557541 0.96593 -0.38256931 -1.8358693 -2.3568382 -3.0420136 -3.7028053 -4.4985027][-3.2283545 -0.88853168 1.0167103 1.8360214 2.4045954 2.874156 2.9643917 2.9645176 2.9530258 1.4615841 0.16767073 -1.8198428 -3.1971807 -3.4646668 -3.9968185][-2.9125881 -1.3801374 -0.073881626 1.6313553 2.9293728 3.6328812 4.1626663 4.1104803 3.9528742 2.4385872 1.1328773 -0.84501171 -2.6246057 -3.5034161 -3.8569899][-3.1914444 -1.5848446 -0.28233004 0.80528927 1.7361002 2.9012737 3.4669838 3.6666517 3.7325935 1.5043993 0.016963959 -1.65621 -3.6489825 -4.5381446 -5.3293891][-3.8540821 -2.5071092 -0.97739267 0.5772438 1.4720373 2.1448421 2.7827206 3.302453 3.2597036 1.025013 -0.34468365 -2.0568566 -4.1557779 -5.3658209 -6.1445665][-3.4580364 -2.808331 -1.8492336 -0.061319351 1.0332518 1.5619612 1.9959555 2.2942524 2.6019764 0.576766 -1.3468213 -2.8951173 -4.8662958 -5.8344793 -6.6857843][-4.0615396 -3.2747583 -2.3910556 -1.1410756 0.46270943 1.5443907 1.7583656 1.8676662 1.7692204 -0.12231588 -1.7644348 -3.3238554 -5.0074806 -5.2835674 -5.8195124][-5.3450813 -4.6575928 -3.9878361 -3.0280757 -2.0055704 -0.42236614 0.87952232 1.2202282 1.1636076 -0.73380375 -2.96658 -3.7191854 -4.7009649 -5.0076075 -5.4884086][-8.1874275 -7.258275 -5.8199153 -4.6183767 -3.3533683 -2.2927227 -1.1484551 -0.65033293 -0.573256 -1.3802438 -2.553555 -3.3752794 -4.5746555 -5.0357151 -5.1780968][-9.760725 -9.22852 -8.7150154 -7.3821449 -6.1024337 -5.0385075 -3.8137653 -3.2517562 -2.9402533 -3.1787992 -4.1732259 -4.1239929 -4.2480969 -4.3187647 -4.2281933][-9.6895924 -9.4372015 -8.5778294 -7.8776169 -6.8916235 -5.9387064 -5.2160511 -4.4262476 -3.8180544 -3.4880481 -3.7593448 -3.9430978 -3.89955 -3.977119 -4.1850119][-10.478312 -9.7460022 -9.0176649 -8.72496 -8.4142981 -7.7083607 -6.7685671 -6.1535759 -5.5242825 -5.1250906 -5.1445932 -5.2739244 -5.0991182 -5.0755363 -4.6066332]]...]
INFO - root - 2017-12-15 13:00:54.456404: step 15310, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 55h:26m:44s remains)
INFO - root - 2017-12-15 13:01:00.822520: step 15320, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 55h:45m:47s remains)
INFO - root - 2017-12-15 13:01:07.278228: step 15330, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 56h:34m:36s remains)
INFO - root - 2017-12-15 13:01:13.790068: step 15340, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 57h:39m:32s remains)
INFO - root - 2017-12-15 13:01:20.210310: step 15350, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 55h:36m:39s remains)
INFO - root - 2017-12-15 13:01:26.724939: step 15360, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 58h:27m:31s remains)
INFO - root - 2017-12-15 13:01:33.177894: step 15370, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 56h:03m:19s remains)
INFO - root - 2017-12-15 13:01:39.587970: step 15380, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 55h:50m:15s remains)
INFO - root - 2017-12-15 13:01:45.995153: step 15390, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 56h:42m:46s remains)
INFO - root - 2017-12-15 13:01:52.441877: step 15400, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 57h:09m:58s remains)
2017-12-15 13:01:52.937626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.161252 -3.1318069 -3.8260632 -4.39708 -3.2256069 -3.8769948 -3.0009203 -2.544796 -2.2781467 -1.8794856 -5.1762619 -3.7978127 -6.1072335 -4.8915148 -4.5219207][-3.2599721 -2.2809176 -1.2536082 -2.2177277 -2.1081195 -2.59872 -3.17492 -3.4785619 -0.845376 0.029025555 -2.8291388 -4.3932877 -7.0029659 -4.1095734 -4.0471368][-1.6653137 -2.9555011 -2.3741899 -0.62272596 -0.1107707 -1.0063438 -0.035914898 -1.1315908 -0.93851662 -0.95386648 -2.8066039 -2.7216148 -6.2121115 -5.4450541 -5.0234289][-1.1221018 -0.53763103 -1.688498 -1.9143782 -0.0024266243 0.49951792 0.4251771 0.33186579 2.0382609 1.0931973 -2.2717357 -4.0632682 -5.5868196 -4.771245 -4.7785735][-2.6336422 -1.4278283 0.94532633 1.0383658 -0.0060620308 -0.66493368 0.16357565 -0.26827526 0.7263093 0.96833754 -1.6481876 -2.9833264 -5.7460394 -5.2134695 -4.6566663][-0.43466806 -1.0319471 -1.1858273 0.23837662 2.43992 2.4409766 2.278337 1.8328738 1.4233804 0.94189596 -1.7743597 -2.9192328 -5.2870483 -4.737987 -5.0407825][-1.6646476 -1.126637 1.0133147 0.83411264 1.3556285 2.1990104 3.9944701 4.5549207 4.1904073 2.4114308 -1.6673756 -2.8940177 -5.577775 -4.23312 -4.2387714][-0.32433558 -0.66200876 -0.44397449 2.0507178 3.2113261 3.0551171 3.5797629 3.67942 4.3368764 3.652844 -0.87760353 -2.384305 -5.0502806 -4.6951079 -4.6346865][-0.35773993 0.47325468 0.92106295 2.1597943 3.5112329 3.8254466 4.2358565 3.5369229 3.6128116 3.3799853 -0.62185383 -0.73183775 -3.7432282 -3.3250313 -4.1174636][-1.5366068 -1.7341857 -0.7684269 0.46368074 0.90856028 2.2735476 1.9819446 2.094583 2.7459712 2.1670356 -0.72590828 -1.2816243 -3.3806615 -2.8443155 -3.8894083][-5.2891154 -4.9885044 -3.8094392 -3.2559338 -2.4303584 -1.30973 -0.79415846 -1.7045088 -1.4685383 -1.0065351 -2.7050657 -2.9127698 -4.7609234 -4.2393889 -5.2242403][-6.4460526 -6.9376755 -5.6759233 -5.5400209 -4.1539469 -4.3223372 -4.0619526 -3.0837979 -3.1737905 -3.0182018 -3.9266505 -4.105691 -5.4941654 -5.0844874 -6.0311985][-5.5139074 -6.8219223 -6.6598969 -6.8531933 -5.61098 -4.7651329 -4.6868567 -4.5325689 -4.4550037 -3.4995666 -4.9883304 -4.8182435 -5.2616296 -5.5887718 -6.5647583][-6.2950993 -5.4013715 -5.1544685 -5.8809938 -5.8034954 -6.1652122 -4.4178267 -4.4996986 -4.18478 -4.4009185 -5.3158689 -5.1788778 -5.4934416 -5.6317005 -6.3837204][-5.5398259 -6.0982943 -5.7979612 -5.95245 -5.0347281 -6.3401756 -5.7784996 -6.20969 -5.2265139 -5.34366 -5.744689 -6.0647893 -6.66259 -6.1158075 -5.6092615]]...]
INFO - root - 2017-12-15 13:01:59.406625: step 15410, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 57h:18m:52s remains)
INFO - root - 2017-12-15 13:02:05.856369: step 15420, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 56h:33m:52s remains)
INFO - root - 2017-12-15 13:02:12.329723: step 15430, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 57h:41m:17s remains)
INFO - root - 2017-12-15 13:02:18.784888: step 15440, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 58h:47m:24s remains)
INFO - root - 2017-12-15 13:02:25.295780: step 15450, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 56h:37m:37s remains)
INFO - root - 2017-12-15 13:02:31.775470: step 15460, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 57h:05m:01s remains)
INFO - root - 2017-12-15 13:02:38.190675: step 15470, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 57h:12m:58s remains)
INFO - root - 2017-12-15 13:02:44.637672: step 15480, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 56h:03m:15s remains)
INFO - root - 2017-12-15 13:02:51.001949: step 15490, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 56h:04m:04s remains)
INFO - root - 2017-12-15 13:02:57.461934: step 15500, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 56h:24m:26s remains)
2017-12-15 13:02:57.931134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6351342 -6.5447989 -8.2086267 -9.4223528 -9.05844 -7.7155008 -6.3630772 -5.3681126 -4.57094 -5.1525269 -6.3598318 -7.2165084 -7.5823088 -8.2876511 -7.9935956][-4.9679008 -6.5807056 -9.1202011 -10.659383 -10.80639 -9.81613 -8.0023823 -6.53889 -5.7040854 -6.5621476 -7.9316607 -8.8071089 -9.0472183 -9.5580692 -9.51912][-5.6454649 -6.9931149 -9.4431372 -11.180344 -11.247231 -9.7362223 -7.6288605 -6.5395575 -5.7394891 -7.0667429 -8.5134153 -9.7299757 -10.023174 -10.598595 -9.93447][-7.0892115 -8.4441528 -9.9786949 -10.310747 -9.8276653 -8.0698071 -5.8876419 -4.5235696 -4.0887203 -5.7217617 -7.8124838 -9.7605181 -10.364278 -10.904449 -10.396292][-7.82137 -8.829608 -8.93046 -8.9825573 -7.4594917 -4.5674181 -2.2477217 -1.0755954 -1.1394987 -3.3505898 -5.6241446 -8.5052013 -9.8740625 -10.628519 -10.394837][-9.3702326 -9.3027058 -8.8168325 -7.2435479 -4.4012423 -1.0155063 1.8384194 3.0696597 2.6816459 -0.65551424 -4.2583022 -7.15052 -8.8928833 -10.008131 -10.161178][-9.266758 -8.7056742 -6.9589734 -4.4837103 -0.95367813 3.416903 6.4767756 6.8835311 5.6783919 1.8430667 -2.7815375 -6.3003769 -8.372612 -9.7734394 -9.3930416][-8.061492 -7.6614585 -6.704905 -3.4165936 1.0557513 5.3242555 7.7423787 8.1224937 7.2167344 3.1937337 -1.5132079 -5.4505653 -7.8346143 -9.0445251 -8.830162][-8.73409 -8.3282614 -7.8818111 -5.3484516 -1.2904778 2.4717889 5.4965482 5.7838178 4.6032281 1.7720189 -1.6877036 -5.1600957 -7.4590116 -8.96761 -9.247838][-9.405695 -9.8343477 -9.1466455 -6.8721175 -4.091774 -1.3439794 0.59487867 1.3771052 1.6416421 -1.4031219 -4.3905745 -7.1009355 -8.961998 -9.7226982 -9.2740555][-10.75184 -11.531083 -11.167873 -9.5981474 -7.5459833 -5.4891481 -3.7240756 -3.6034622 -3.6963124 -5.5474033 -7.91942 -9.3782339 -10.220922 -10.852554 -10.212862][-11.145783 -11.71936 -11.605998 -10.747614 -9.4758844 -7.5166545 -6.1775074 -6.1866779 -6.2613859 -7.7819133 -8.8511057 -9.7842541 -9.9442787 -9.8299065 -9.7424669][-11.391766 -11.484996 -10.882255 -10.216593 -9.9650621 -9.5492964 -8.4907408 -7.9683008 -7.6634684 -9.0663538 -10.036743 -9.7995491 -9.1961927 -9.3705368 -9.2021275][-9.86803 -9.9410686 -8.6689854 -8.3346357 -8.4170284 -7.9652147 -7.7286787 -7.603158 -7.3255162 -7.7119 -7.9998121 -8.0314035 -7.8737135 -8.3209906 -8.3803167][-8.7303972 -8.4589586 -8.0086069 -7.771225 -7.7655497 -7.8010726 -7.4897642 -7.3227329 -7.3083591 -7.0248852 -6.7703042 -6.9854956 -6.757226 -7.1711273 -7.619328]]...]
INFO - root - 2017-12-15 13:03:04.441018: step 15510, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 56h:18m:43s remains)
INFO - root - 2017-12-15 13:03:10.802644: step 15520, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 55h:38m:00s remains)
INFO - root - 2017-12-15 13:03:17.220966: step 15530, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 55h:35m:10s remains)
INFO - root - 2017-12-15 13:03:23.561096: step 15540, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 55h:27m:44s remains)
INFO - root - 2017-12-15 13:03:29.953762: step 15550, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 57h:28m:41s remains)
INFO - root - 2017-12-15 13:03:36.341631: step 15560, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:35m:32s remains)
INFO - root - 2017-12-15 13:03:42.767512: step 15570, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 55h:53m:58s remains)
INFO - root - 2017-12-15 13:03:49.158783: step 15580, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 54h:35m:26s remains)
INFO - root - 2017-12-15 13:03:55.535841: step 15590, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 55h:50m:30s remains)
INFO - root - 2017-12-15 13:04:01.975337: step 15600, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 57h:43m:28s remains)
2017-12-15 13:04:02.431008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2191882 -7.737154 -7.8818727 -7.8208175 -7.1798015 -6.1178746 -4.4036522 -2.6206427 -1.1736941 -1.0064669 -1.7414522 -2.6455331 -3.6235881 -4.9434867 -6.0417547][-6.3397565 -6.9435387 -7.3626113 -7.3801751 -7.1799564 -6.5868435 -5.7068191 -3.8988125 -2.4543643 -2.2639332 -2.4904394 -3.3558636 -4.4369955 -4.6765013 -6.1243668][-6.0616579 -6.6702547 -6.8170176 -6.9516888 -6.7829828 -6.50689 -5.8997631 -5.0974083 -3.9940836 -3.9099181 -4.0482321 -5.393837 -5.8276734 -5.7655368 -6.5170012][-6.07074 -5.9635515 -5.646987 -5.2676792 -5.3910737 -4.7263422 -4.1397305 -3.547152 -3.4238019 -4.3111911 -4.9996681 -6.1005416 -7.1156335 -7.1324868 -7.6405282][-6.1374884 -5.0230069 -4.0948229 -3.0333858 -2.6008701 -2.2589216 -1.4102588 -0.73346233 -0.36720896 -1.5602655 -3.3843613 -5.4047184 -6.6294589 -7.1572566 -7.9459963][-5.0652113 -3.5478482 -2.2124133 -0.63005781 0.74009514 1.2782221 1.5282316 1.471549 2.1076775 0.70780659 -1.0919495 -3.2263274 -5.1501303 -5.9558115 -6.9955397][-5.032424 -3.1642962 -1.0911379 0.60400391 2.4717894 3.5601292 4.2909756 3.8182554 3.6068506 1.9089222 0.11642122 -2.0868487 -3.6202898 -4.984808 -6.5653496][-4.2621393 -3.1890841 -1.4186544 0.44302654 2.1124573 3.2041855 4.2963343 4.5069675 4.3023405 2.0363569 -0.40847635 -2.6538978 -3.8733056 -4.6843157 -5.7895842][-5.0396204 -3.4284105 -2.2567534 -0.5306406 0.73964596 2.1005535 2.7836018 2.8334103 3.0155296 1.339592 -1.2575235 -3.3321233 -4.6067958 -5.2511158 -5.8366189][-6.101512 -5.0995169 -4.1716666 -2.6223054 -1.5528898 0.097987175 1.1026917 1.084547 0.86987972 -0.77144575 -2.2496266 -3.5164189 -4.9813004 -5.4863567 -6.1511292][-7.914669 -7.4396548 -6.7329912 -5.6587219 -5.0424972 -3.5298486 -2.1553888 -1.9633193 -2.1675911 -3.6665039 -4.3876076 -5.0089612 -5.6959167 -5.9873934 -7.2288704][-8.6288471 -8.3149176 -8.1109905 -7.2411466 -6.4240227 -5.7359395 -5.5268679 -4.9530334 -4.4887619 -5.1662383 -5.0516648 -5.2394638 -5.7547984 -5.988112 -6.6200509][-9.264452 -9.7653246 -9.3824024 -8.308157 -7.6357131 -7.07366 -6.4590645 -6.7445292 -6.9408383 -6.9316649 -6.438345 -5.7044458 -5.4790497 -5.0664043 -5.8917389][-8.5591564 -8.8640566 -9.0322828 -8.6193237 -8.1942167 -7.673861 -7.078434 -7.1976795 -7.4538178 -7.8648982 -7.5670838 -7.0788279 -6.4434347 -5.8970504 -5.4988194][-6.4514718 -6.5966682 -7.1321726 -6.9670925 -7.3093815 -7.4474206 -7.2929258 -7.5381618 -7.3284159 -7.23459 -7.6699505 -7.5393386 -7.5671024 -7.1062441 -6.779789]]...]
INFO - root - 2017-12-15 13:04:08.874876: step 15610, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-15 13:04:15.270161: step 15620, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 55h:16m:14s remains)
INFO - root - 2017-12-15 13:04:21.720955: step 15630, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 57h:12m:49s remains)
INFO - root - 2017-12-15 13:04:28.229248: step 15640, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 56h:32m:18s remains)
INFO - root - 2017-12-15 13:04:34.638298: step 15650, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 57h:26m:39s remains)
INFO - root - 2017-12-15 13:04:41.021566: step 15660, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 57h:23m:22s remains)
INFO - root - 2017-12-15 13:04:47.383851: step 15670, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 56h:08m:38s remains)
INFO - root - 2017-12-15 13:04:53.837006: step 15680, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 57h:09m:34s remains)
INFO - root - 2017-12-15 13:05:00.218884: step 15690, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:58m:13s remains)
INFO - root - 2017-12-15 13:05:06.724190: step 15700, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 56h:22m:47s remains)
2017-12-15 13:05:07.260568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7429829 -2.8968911 -3.0630546 -3.3135223 -3.8443356 -4.1282396 -4.2511029 -3.9750972 -3.5993309 -5.6295991 -6.21755 -6.8818159 -7.5335989 -8.1745052 -8.5904665][-3.4356842 -3.7313478 -4.1558294 -4.7058067 -5.334794 -5.4569473 -5.3945322 -5.2688918 -5.0231676 -6.7505732 -6.6595917 -7.1896758 -7.9614925 -8.53408 -8.8701][-3.223855 -3.3402386 -3.80075 -3.9741268 -4.2103548 -4.0588465 -3.7333105 -3.673738 -3.690474 -5.794158 -6.1048632 -6.5852475 -7.2909431 -7.7742033 -8.3473549][-4.0683537 -3.4866652 -3.0838833 -2.5606713 -2.3308654 -2.1232171 -1.8295588 -2.0211105 -2.1869965 -4.3401623 -5.0196362 -5.5487342 -6.2740135 -7.1719871 -8.1906719][-4.9242558 -3.8132529 -2.7889113 -1.9809461 -1.3881135 -0.35711145 0.30519438 0.068939686 -0.39915705 -2.9467268 -4.0425119 -5.2080469 -6.2757349 -6.7848463 -7.4880123][-4.5203633 -3.9867108 -3.2876477 -2.0004535 -0.92473793 0.10460043 1.0837245 1.2493911 0.87395811 -1.7810345 -2.8812571 -4.01 -5.3097191 -6.38246 -7.2761536][-3.669591 -2.9605722 -2.1984353 -1.3653283 -0.23775816 0.93784094 1.8460021 2.0081725 1.8952937 -0.51173782 -1.7980385 -3.1557188 -4.3490782 -5.6815176 -6.7775655][-2.5714507 -2.3007035 -1.8482051 -0.57063293 0.61261988 1.2598109 1.9151931 2.1660733 2.225225 -0.029893875 -1.2259421 -2.4280972 -3.892632 -5.221446 -6.4285016][-2.524013 -1.8093257 -1.0542326 -0.1959815 0.52549791 0.86961603 1.1432891 1.4000278 1.6220975 -0.4277463 -1.6569514 -2.9925613 -4.416811 -5.48224 -6.4345641][-2.7301807 -2.023231 -1.0616517 0.044887066 0.74013186 0.71334505 0.81885004 0.97377634 1.2544045 -1.0632482 -2.1117234 -3.204845 -4.4772491 -5.5552158 -6.7013364][-3.9280899 -3.0060315 -2.1254926 -1.1789751 -0.56666613 -0.17694378 -0.090063572 -0.33635664 -0.17417145 -2.0532088 -3.0755243 -4.1731014 -4.9422493 -5.523314 -6.4304118][-4.8627973 -4.0036306 -3.2037864 -1.8620911 -0.958467 -0.46660328 -0.32017326 -0.50001335 -0.55201149 -2.2923031 -3.2440619 -4.1549296 -5.2396765 -6.0419083 -6.762567][-6.1890249 -5.5742111 -4.6185908 -3.3698549 -2.1169152 -1.3479085 -0.94818497 -1.3549318 -1.3141789 -2.5985689 -3.209938 -3.8366818 -5.0115123 -5.1869864 -5.9937396][-6.2994032 -5.8411775 -5.2547235 -4.2353492 -3.4563551 -2.5848794 -1.8928685 -2.0354857 -1.7978673 -1.98037 -2.0895658 -2.5361037 -3.7869053 -4.3758879 -5.18304][-7.0736747 -6.5384054 -5.8465371 -5.5758543 -4.8788 -4.1449146 -3.7702758 -3.9624267 -3.9037182 -3.7257502 -3.2124338 -3.6044178 -4.160337 -4.3056006 -4.7470417]]...]
INFO - root - 2017-12-15 13:05:13.625202: step 15710, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 56h:17m:48s remains)
INFO - root - 2017-12-15 13:05:20.068674: step 15720, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 55h:38m:55s remains)
INFO - root - 2017-12-15 13:05:26.508875: step 15730, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 55h:53m:01s remains)
INFO - root - 2017-12-15 13:05:32.913280: step 15740, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 54h:49m:40s remains)
INFO - root - 2017-12-15 13:05:39.261359: step 15750, loss = 0.24, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 57h:14m:21s remains)
INFO - root - 2017-12-15 13:05:45.635694: step 15760, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.618 sec/batch; 54h:24m:32s remains)
INFO - root - 2017-12-15 13:05:52.091604: step 15770, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 55h:00m:55s remains)
INFO - root - 2017-12-15 13:05:58.573549: step 15780, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 55h:40m:25s remains)
INFO - root - 2017-12-15 13:06:04.978216: step 15790, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 56h:51m:23s remains)
INFO - root - 2017-12-15 13:06:11.441194: step 15800, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.676 sec/batch; 59h:30m:44s remains)
2017-12-15 13:06:11.951562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1572924 -5.7120461 -5.0166435 -4.3216047 -3.438067 -2.8871341 -2.7707462 -2.7508116 -2.8011642 -3.3954148 -4.5675573 -5.8150382 -6.24335 -6.1768336 -6.348321][-5.5586338 -4.6851768 -4.1273413 -3.5890827 -3.1717639 -3.0777798 -3.27138 -3.3044534 -3.2420635 -3.5491757 -4.6163673 -5.19219 -5.7176318 -5.7731819 -6.1765795][-6.1073351 -5.6804204 -4.6592894 -3.4881248 -2.8388734 -2.7418861 -2.6814404 -2.8238316 -3.1353083 -3.8868511 -5.1133027 -5.5000463 -5.8367162 -5.5787239 -6.3759766][-5.0051193 -4.3977661 -3.8082933 -3.4245338 -2.7810884 -2.34345 -1.8906598 -1.9922624 -2.0072498 -2.710866 -4.2380791 -5.2308855 -5.8129253 -5.37992 -5.9435821][-4.1308384 -3.3990126 -3.0127382 -2.0929842 -1.3682394 -1.1492796 -0.73136187 -0.55902529 -0.68289614 -1.3880982 -2.4657855 -3.9026411 -4.9231791 -5.567441 -6.3933916][-3.2885838 -2.6256561 -1.9832773 -1.3219891 -0.9230299 -0.32351017 -0.32444811 -0.12910223 0.11158609 -0.591424 -2.4163003 -3.6615944 -4.606967 -5.0420504 -5.7771864][-1.4098873 -1.3907986 -0.76876163 0.05839777 0.82818508 0.82675743 0.93712139 0.68086052 -0.026204586 -0.9125495 -2.6937351 -3.9069633 -4.6988831 -4.832231 -5.4963603][0.33986187 1.1032209 1.246151 1.3042059 1.6016226 1.7294626 1.9309893 1.5943851 1.4561481 0.388525 -1.9812245 -3.4868002 -4.5891242 -4.8858147 -5.5221162][-1.0694656 0.0032553673 1.1849861 1.6101685 1.8853626 2.0494041 2.0912647 1.6473188 1.2291069 0.10281181 -1.4841642 -3.2669196 -4.544939 -4.9135756 -5.9557128][-1.41888 -0.72483873 -0.10367918 0.76797962 1.2005634 1.4473972 1.5897903 1.2796097 0.83959293 -0.44120884 -2.2864714 -3.9167225 -4.4894581 -4.7281857 -5.5567255][-2.3956914 -1.990664 -1.385345 -0.6099577 -0.16683292 0.38143921 0.21736431 0.032365322 -0.35580635 -1.4423747 -2.8323092 -4.3192453 -4.5040264 -4.4234457 -5.2713685][-4.2763815 -3.368382 -2.8529463 -2.6616726 -2.1160712 -1.4987893 -1.1995144 -1.0235901 -1.1528172 -1.4999118 -3.3834772 -4.591538 -5.190311 -5.808157 -6.8318281][-5.2342281 -4.8309922 -4.5346503 -4.0883904 -3.3908582 -2.9858112 -2.5955749 -2.3373485 -2.447536 -2.4358654 -4.1698942 -4.9688568 -5.6440783 -6.1229925 -6.8648176][-5.7940154 -5.5012932 -5.4744959 -5.3545923 -4.8416548 -4.1653886 -3.6936381 -3.65909 -3.7374151 -3.5325994 -4.8573914 -5.4414234 -6.1691041 -6.5124674 -6.4497209][-6.7558494 -6.2145891 -5.7353172 -5.6322126 -5.8335352 -5.7330418 -5.5087614 -5.4420118 -4.97408 -5.3093696 -5.6555672 -5.80377 -6.2740593 -6.9265747 -7.210319]]...]
INFO - root - 2017-12-15 13:06:18.370143: step 15810, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 56h:43m:52s remains)
INFO - root - 2017-12-15 13:06:24.735750: step 15820, loss = 0.24, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 58h:08m:26s remains)
INFO - root - 2017-12-15 13:06:31.141765: step 15830, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 54h:32m:40s remains)
INFO - root - 2017-12-15 13:06:37.550016: step 15840, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 56h:59m:33s remains)
INFO - root - 2017-12-15 13:06:43.957896: step 15850, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 56h:54m:50s remains)
INFO - root - 2017-12-15 13:06:50.421580: step 15860, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 57h:51m:52s remains)
INFO - root - 2017-12-15 13:06:56.869807: step 15870, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 57h:51m:36s remains)
INFO - root - 2017-12-15 13:07:03.204485: step 15880, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 56h:34m:25s remains)
INFO - root - 2017-12-15 13:07:09.649453: step 15890, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 57h:19m:02s remains)
INFO - root - 2017-12-15 13:07:16.126004: step 15900, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 54h:54m:35s remains)
2017-12-15 13:07:16.607945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5646911 -3.7332325 -3.6573915 -3.1005397 -3.0987358 -2.936235 -2.3850718 -2.2216129 -1.832572 -3.2945695 -3.6508555 -4.5762682 -6.2123203 -6.7587471 -7.1404214][-4.1300864 -3.8581195 -4.3066 -4.2003665 -3.9900615 -3.6245465 -2.9197178 -2.7095523 -2.3613267 -4.2676525 -4.5565796 -4.960392 -6.2695837 -6.8478994 -7.962698][-3.0757008 -2.8967609 -2.9362383 -3.0205674 -2.6506352 -2.6626086 -1.8709078 -1.858685 -1.6516457 -3.1022348 -3.7082977 -4.2684216 -5.5481768 -6.277359 -7.0714493][-3.9530365 -3.2726917 -2.7120409 -1.8694682 -0.91616535 -0.36841822 0.021306515 -0.072077274 0.072943687 -2.1116352 -2.8013763 -3.3967795 -4.6790857 -5.1143 -6.0668755][-4.8487825 -3.2478509 -2.1916752 -1.2884307 -0.56374264 0.2858305 0.99861145 1.0097446 1.2300224 -0.82868958 -1.5581384 -2.72783 -4.2178063 -4.9416485 -6.2693443][-3.3132105 -2.43015 -1.2813458 0.078270912 1.0779228 1.8694582 2.5047493 2.711216 2.8466959 0.99666309 0.27978182 -0.92693996 -2.6957698 -3.6808219 -4.8491592][-3.2631536 -2.3806305 -1.4246283 0.13668346 1.624589 2.8473959 3.770421 3.6987171 3.8628216 2.068718 1.3769398 -0.048587322 -1.6143556 -2.5869088 -4.094388][-3.1614084 -1.5860295 -0.22145844 0.67391872 1.6201038 2.6786785 3.23388 3.4543581 3.7003117 1.6969223 1.1399422 0.057912827 -1.5796981 -2.7690325 -4.0491362][-1.914609 -1.0144787 -0.19296122 1.0032568 1.5688076 1.7790422 2.2961235 2.2543058 2.1652508 1.064146 1.1290379 -0.28716898 -1.9297142 -2.5795579 -3.8251073][-1.6169314 -1.1792731 -0.636518 0.01008749 0.4196043 1.0003309 1.466507 1.8488789 2.3956718 0.5189867 -0.11230612 -0.80367661 -2.2480664 -3.2329049 -4.1517367][-2.7943697 -2.7733583 -2.2127886 -1.6642675 -1.0212026 -0.58749819 0.082929611 0.34612465 0.2306757 -1.4292378 -1.7984729 -2.8380203 -3.6796508 -3.999212 -4.5916457][-3.9120398 -4.188158 -4.1876173 -3.5544381 -3.1460738 -2.9610996 -2.7917337 -2.590116 -1.9540863 -3.2926459 -3.9344063 -4.9715109 -5.624064 -5.5680146 -5.781642][-5.8764062 -6.0513639 -6.2578297 -5.6515293 -4.8649845 -4.7415266 -4.4004011 -3.9133432 -3.9448011 -4.9638739 -5.3108335 -5.5500121 -5.6063366 -5.9072671 -7.007967][-6.5937896 -6.8275232 -6.8140426 -6.3930006 -5.5591011 -5.0112314 -4.7470441 -4.4951205 -4.2820091 -4.5809669 -5.2172632 -5.8701525 -6.4146461 -6.3013954 -6.2388673][-7.5743256 -8.2246313 -8.2846394 -7.3735194 -6.8339734 -6.7932944 -6.8639097 -6.626965 -6.4354215 -6.2292233 -6.3160319 -6.5624886 -6.5263524 -6.2077389 -5.9002123]]...]
INFO - root - 2017-12-15 13:07:22.921019: step 15910, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 55h:35m:38s remains)
INFO - root - 2017-12-15 13:07:29.302594: step 15920, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 56h:01m:19s remains)
INFO - root - 2017-12-15 13:07:35.670940: step 15930, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 56h:04m:58s remains)
INFO - root - 2017-12-15 13:07:42.045930: step 15940, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 55h:30m:01s remains)
INFO - root - 2017-12-15 13:07:48.485288: step 15950, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 57h:19m:31s remains)
INFO - root - 2017-12-15 13:07:54.847590: step 15960, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 55h:04m:56s remains)
INFO - root - 2017-12-15 13:08:01.230713: step 15970, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 55h:12m:47s remains)
INFO - root - 2017-12-15 13:08:07.587284: step 15980, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 56h:22m:38s remains)
INFO - root - 2017-12-15 13:08:14.062544: step 15990, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 55h:23m:11s remains)
INFO - root - 2017-12-15 13:08:20.547533: step 16000, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 57h:50m:42s remains)
2017-12-15 13:08:21.061290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0137749 -6.0323796 -5.1144028 -4.4465113 -4.6289692 -4.2587676 -4.0898724 -4.5147357 -4.4039459 -4.9644632 -5.0569243 -5.97904 -6.7877979 -7.2160897 -7.971271][-6.55641 -6.2199292 -5.8658614 -4.8277521 -4.2599564 -3.9332268 -3.8443613 -4.1106281 -4.1629982 -5.3432765 -5.3527036 -6.00682 -6.349216 -6.1154461 -6.6701913][-5.2292285 -5.2504568 -4.7053204 -4.0137396 -3.473186 -2.9676909 -2.6905146 -2.960381 -3.078999 -4.2328825 -4.5631347 -5.5948629 -5.8995423 -5.5662045 -6.1255741][-3.8572612 -3.8443089 -3.8772743 -2.8978715 -1.7488532 -0.81101561 -0.24334288 -0.56102991 -1.1346436 -2.95198 -3.7550037 -4.9250994 -5.6150455 -5.6129351 -6.0990977][-4.339016 -3.6005096 -2.7642398 -1.6097794 -0.99261284 0.17665005 1.5109158 1.2453089 0.86489773 -0.7785964 -1.9738979 -3.774955 -4.9318171 -5.5782795 -6.3818779][-4.2329969 -3.4741769 -2.5804729 -1.4682298 -0.5984478 1.0171146 2.5631723 2.6305618 2.3835096 0.45684052 -1.0031714 -3.2793918 -4.9513664 -5.7580533 -6.8455315][-4.1625752 -2.3462667 -1.272634 -0.46218824 0.55282784 1.9900427 3.343255 2.9710484 2.8900194 0.99008656 -0.78156662 -3.4893126 -5.3799286 -6.3224483 -7.1610088][-4.1284475 -2.8984971 -1.4039369 -0.38929987 0.27418137 1.5685949 3.1230125 3.1396894 3.0081625 0.66347408 -0.99554777 -3.2857881 -5.24078 -6.2963343 -7.3302608][-4.7633462 -4.0127211 -2.9790511 -1.9476223 -1.1470823 -0.31026268 0.7241745 1.1050425 1.1213512 -0.75169182 -2.0614033 -4.0492134 -5.5916648 -6.5385327 -7.1490498][-5.5169792 -5.0071106 -5.2435369 -4.2003322 -2.9739857 -2.2787395 -1.9887652 -1.4479923 -1.0496302 -2.3701496 -3.6928446 -5.288764 -6.4131093 -6.5445852 -7.1272321][-6.563199 -6.5007505 -6.4072022 -5.6097708 -5.0473833 -4.0520334 -3.4977689 -3.5762424 -3.3908186 -4.2822556 -5.1666365 -6.0097408 -6.683713 -7.1001706 -7.8176494][-7.1082306 -6.4561138 -6.9555254 -6.2264948 -5.6502209 -5.0038767 -4.3530335 -4.3856192 -4.8005362 -5.8725986 -6.3749285 -6.9424734 -7.7026463 -7.8809719 -7.9036431][-6.1167412 -5.31748 -5.4753213 -5.1499567 -4.9480629 -4.2266207 -3.6077976 -3.815686 -3.9317775 -5.2443871 -6.9410095 -7.1403694 -7.1841259 -7.6508021 -8.3288469][-5.2039995 -4.5650206 -3.7123299 -2.868845 -2.67764 -2.2800226 -1.5792384 -2.2462578 -2.4005542 -3.3531489 -4.9836922 -6.1720095 -6.6795793 -6.8751264 -7.7127509][-4.7023168 -3.6822858 -2.9286809 -2.7634215 -2.6198468 -2.1238022 -2.1020775 -3.174943 -3.8169951 -4.358448 -4.9794106 -5.9973106 -6.9876227 -7.2884221 -7.2400289]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 13:08:27.421845: step 16010, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 56h:48m:52s remains)
INFO - root - 2017-12-15 13:08:33.758713: step 16020, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 55h:10m:30s remains)
INFO - root - 2017-12-15 13:08:40.198398: step 16030, loss = 0.31, batch loss = 0.20 (13.0 examples/sec; 0.614 sec/batch; 53h:56m:42s remains)
INFO - root - 2017-12-15 13:08:46.651605: step 16040, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 55h:56m:05s remains)
INFO - root - 2017-12-15 13:08:53.122446: step 16050, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 57h:44m:05s remains)
INFO - root - 2017-12-15 13:08:59.588498: step 16060, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 55h:26m:24s remains)
INFO - root - 2017-12-15 13:09:06.065639: step 16070, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 57h:26m:44s remains)
INFO - root - 2017-12-15 13:09:12.531908: step 16080, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.624 sec/batch; 54h:51m:46s remains)
INFO - root - 2017-12-15 13:09:18.912505: step 16090, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 57h:03m:34s remains)
INFO - root - 2017-12-15 13:09:25.482626: step 16100, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 55h:07m:00s remains)
2017-12-15 13:09:25.992998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5035114 -3.5350223 -3.3725829 -3.380393 -2.8858442 -2.1755004 -1.9868498 -1.660634 -1.2276516 -2.2138152 -2.9571962 -4.4607897 -5.5453262 -6.6866965 -7.7785249][-2.8369179 -2.9652457 -3.1350431 -3.2601285 -3.1873522 -2.9814811 -3.044117 -2.6207252 -2.1263294 -2.7000389 -3.3440928 -4.8861442 -6.0061278 -6.8023157 -7.6281805][-2.3153696 -2.3549275 -2.2961888 -2.4038053 -2.5191331 -2.3741999 -2.3114653 -1.8661819 -1.4545784 -2.0388889 -2.4890814 -3.8482294 -5.0376606 -5.9516315 -6.7749295][-1.6985059 -1.5542521 -1.8011374 -2.092103 -2.0274334 -1.4702992 -1.131721 -0.50015354 -0.073762894 -0.8725605 -1.6609859 -3.2472572 -4.5644031 -5.7092357 -6.6355724][-1.5676079 -1.0173879 -0.9216547 -1.2136517 -1.4047217 -0.65365267 -0.16995525 -0.054830551 0.087623596 -0.97669744 -1.5888443 -2.9659872 -4.2092619 -5.4103785 -6.4097404][-2.1518788 -1.4160986 -1.0201507 -0.71422482 -0.38897896 0.138906 0.65695524 0.72409105 0.86386824 -0.23859024 -1.0599179 -2.3191686 -3.4887013 -4.7728424 -5.9093938][-2.710855 -1.7587132 -1.2869468 -0.6657629 -0.30030727 0.30554819 0.96890879 1.0966659 1.3098073 -0.0659461 -1.0433483 -2.5621395 -3.8793452 -5.1478291 -6.0713134][-3.0537114 -2.1105976 -1.5673332 -0.58501625 0.095076084 0.74427843 1.3099446 1.3978238 1.551312 0.11353779 -1.0620627 -2.4756179 -3.8412313 -5.0311861 -5.8934817][-2.9222918 -1.8846898 -1.3528123 -0.59104729 0.23168087 0.95357847 1.5059848 1.3162036 1.2537074 -0.30947304 -1.493412 -2.8671331 -3.9395897 -4.8864288 -5.59284][-3.2887659 -2.145853 -1.3431072 -0.77373266 -0.10754824 0.49217081 1.0367818 0.80624819 0.81594419 -0.65129232 -1.7038741 -2.8670931 -3.8701718 -4.6205873 -5.2260923][-4.6625071 -3.5720806 -2.7788563 -2.0280352 -1.4161868 -0.78002167 -0.17800713 -0.395689 -0.62652969 -1.7379827 -2.508574 -3.2753639 -3.857908 -4.3802495 -4.7499247][-6.0154638 -4.8981714 -3.9600675 -3.4438891 -3.0341229 -2.3110461 -1.8445797 -1.9809399 -2.1357532 -2.9847054 -3.4992094 -3.8151448 -4.1900849 -4.3352661 -4.5610766][-6.5966644 -5.5088272 -4.7778096 -4.3247461 -3.9634774 -3.4015727 -2.9019074 -3.1226692 -3.4170132 -4.0543771 -4.21707 -4.3270621 -4.4740686 -4.3123426 -4.5596018][-6.6683531 -5.948215 -5.3503723 -5.3228097 -4.9644427 -4.1811275 -3.8543961 -3.8238437 -3.7794259 -4.1651793 -4.2067976 -4.2273989 -4.2565174 -4.2545977 -4.5401487][-7.8307023 -7.4095092 -6.5884309 -6.4916387 -6.5880575 -5.8481975 -5.3752794 -5.2525091 -5.3177586 -5.1847267 -4.8709507 -5.03189 -5.1365094 -5.2601805 -5.48256]]...]
INFO - root - 2017-12-15 13:09:32.434284: step 16110, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 58h:28m:29s remains)
INFO - root - 2017-12-15 13:09:38.896674: step 16120, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 58h:22m:35s remains)
INFO - root - 2017-12-15 13:09:45.361119: step 16130, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 57h:22m:23s remains)
INFO - root - 2017-12-15 13:09:51.759584: step 16140, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 55h:18m:18s remains)
INFO - root - 2017-12-15 13:09:58.177130: step 16150, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 54h:31m:39s remains)
INFO - root - 2017-12-15 13:10:04.493014: step 16160, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 54h:34m:37s remains)
INFO - root - 2017-12-15 13:10:10.912445: step 16170, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 54h:36m:34s remains)
INFO - root - 2017-12-15 13:10:17.334207: step 16180, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 57h:30m:13s remains)
INFO - root - 2017-12-15 13:10:23.711677: step 16190, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 56h:40m:55s remains)
INFO - root - 2017-12-15 13:10:30.190269: step 16200, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 57h:22m:57s remains)
2017-12-15 13:10:30.705109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.049881 -5.6663561 -5.5181732 -5.21553 -5.0674295 -3.9127998 -3.2606525 -2.8276043 -2.6574111 -3.1775532 -3.7421553 -4.0412574 -4.3142056 -5.7279186 -6.7846904][-4.8842411 -5.8423243 -6.0732527 -6.1682792 -6.0592046 -5.5328207 -5.0344925 -4.2513304 -3.4650898 -3.2911153 -3.25041 -4.1408877 -4.525692 -5.8325672 -6.6169195][-3.8614814 -4.7349172 -4.7682934 -4.82631 -5.439496 -5.5490093 -5.5045967 -5.0097504 -4.741086 -4.7494082 -4.5578904 -4.2770791 -4.7465668 -6.253624 -7.6843443][-3.8145719 -4.0223227 -4.1056747 -3.6975396 -3.9861915 -3.6156402 -3.7581725 -4.01515 -4.3173351 -4.8746967 -4.9319506 -5.2838154 -5.73578 -6.5318928 -7.64107][-3.5544639 -3.5629754 -3.3560171 -2.8550763 -2.2951269 -1.6958036 -1.3658438 -1.5254464 -1.8526421 -2.9977994 -4.0406885 -4.5807858 -5.3929291 -6.7468653 -8.0333014][-4.024929 -3.6150289 -3.3445783 -1.6342049 -0.59153557 0.21286392 0.70319319 0.46984339 0.50706053 -0.95491219 -2.0192275 -3.4137111 -5.243854 -6.5428934 -7.819819][-3.1848001 -2.9387593 -2.5720811 -1.614491 -0.079984188 1.1639419 2.0076137 1.8002591 2.0555196 1.0901837 -0.45252132 -1.7716413 -3.1726408 -4.9575377 -6.6481705][-2.490715 -1.743288 -0.81934834 0.20946312 1.2824798 2.0217624 2.4775681 2.3734775 2.7899537 2.0889649 0.86168146 -0.49776268 -1.9977036 -3.8203552 -5.8146453][-2.9455314 -2.0045304 -0.00055646896 0.62724066 1.553555 1.969954 2.4470696 2.1098418 2.0077558 1.5106406 0.57743788 -1.1058078 -2.876791 -4.7469172 -6.0591807][-3.186564 -2.1296554 -1.8633299 -0.78662252 0.34680128 1.4937196 1.8591571 1.4798026 1.648787 0.85455084 -0.59629488 -1.9160342 -3.5106044 -5.4524794 -7.2185793][-5.1301355 -4.3093386 -3.2652693 -2.2612219 -0.70833063 0.052888393 0.18620205 0.011918068 0.20088673 -0.18515635 -1.530416 -2.845253 -4.0710855 -6.0901294 -8.1040907][-6.7619591 -6.2916703 -5.4712029 -4.5345235 -3.6061368 -2.6237783 -2.2474337 -1.6113462 -1.196476 -2.0641241 -2.8571906 -3.757442 -4.409626 -6.0089993 -7.7776842][-7.9272542 -7.2768083 -6.8304858 -6.1295033 -5.7170482 -5.1175256 -4.4103079 -3.8560748 -3.372036 -3.8332319 -3.9777703 -4.5345697 -5.4911346 -6.5009508 -7.0392509][-8.5367012 -8.65207 -8.4192066 -7.4864178 -7.2118988 -6.3808832 -6.0330057 -5.5596037 -4.9114513 -4.5178356 -4.8350468 -5.1711903 -5.8040371 -6.19674 -6.7426462][-8.1483765 -8.6096439 -8.3121395 -7.87059 -7.4012828 -7.1788621 -7.0646505 -6.8617196 -6.7420306 -6.4165792 -5.9721074 -5.7062273 -6.0131617 -6.1817327 -6.3018751]]...]
INFO - root - 2017-12-15 13:10:37.161461: step 16210, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 56h:29m:24s remains)
INFO - root - 2017-12-15 13:10:43.484605: step 16220, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 54h:24m:51s remains)
INFO - root - 2017-12-15 13:10:49.841069: step 16230, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 56h:13m:40s remains)
INFO - root - 2017-12-15 13:10:56.210241: step 16240, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 55h:51m:01s remains)
INFO - root - 2017-12-15 13:11:02.644952: step 16250, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 56h:35m:07s remains)
INFO - root - 2017-12-15 13:11:09.152301: step 16260, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 57h:16m:46s remains)
INFO - root - 2017-12-15 13:11:15.619300: step 16270, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 57h:48m:46s remains)
INFO - root - 2017-12-15 13:11:22.112693: step 16280, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 55h:42m:50s remains)
INFO - root - 2017-12-15 13:11:28.526697: step 16290, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 55h:40m:22s remains)
INFO - root - 2017-12-15 13:11:34.885135: step 16300, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:11m:45s remains)
2017-12-15 13:11:35.381357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9154792 -6.6493092 -6.2280025 -5.3976679 -5.1450081 -5.5190139 -5.9035892 -5.7383652 -5.6552353 -5.4638233 -5.93954 -7.4045234 -8.2142715 -8.8911371 -9.7302933][-6.9535871 -7.1531029 -6.6893139 -6.5982695 -6.3040094 -6.1999388 -6.0118861 -6.3113132 -6.4205761 -6.576777 -7.455379 -8.1326771 -8.5892868 -9.0734177 -9.6774225][-6.7584734 -6.4109583 -6.2368922 -5.8400545 -5.7898207 -5.7504163 -5.3958497 -5.1528864 -4.9900193 -5.0126452 -6.1169128 -7.9587307 -8.591815 -8.8131256 -8.7309008][-5.9790559 -5.8232555 -5.4774055 -4.9076395 -4.3544226 -4.1338711 -3.4267707 -3.6866155 -3.42343 -3.3465447 -4.2764082 -6.0977697 -7.0279255 -8.0452108 -8.7209139][-4.7540612 -4.080287 -3.7811997 -2.9463892 -2.3199162 -1.7843294 -1.3620687 -1.2109056 -1.0089474 -1.3031898 -2.6955905 -4.7022657 -5.5112925 -6.06595 -6.8243957][-3.6974289 -3.649816 -2.8572278 -1.9761887 -0.99736452 0.30417442 1.2852292 1.157805 0.96560526 0.0096979141 -1.5657458 -3.0006804 -3.8316078 -4.7215652 -5.610158][-4.5312872 -4.4084244 -3.7962861 -2.6016302 -1.2589374 0.63490248 1.5931411 1.6710887 1.897449 1.112123 -0.72157145 -2.8573723 -3.9905334 -4.5609307 -5.6920085][-4.4404054 -4.1139917 -3.3647466 -1.9954591 -0.38502884 1.097837 1.7556539 2.2250075 2.5784373 1.8394113 0.055878162 -2.3208089 -3.6700153 -4.9792891 -5.7707429][-3.4603004 -3.3525839 -3.1277604 -2.2366652 -1.2811694 0.35371637 1.4595551 1.790772 1.5863128 1.0105863 -0.14966917 -2.1846409 -3.110014 -4.4693666 -5.6315994][-4.25641 -4.1821795 -3.7507529 -2.6400695 -1.798306 -0.67659235 0.0453825 0.064732552 -0.081029415 -0.5061307 -1.5801792 -3.5199256 -4.3689651 -4.7044163 -5.7827158][-4.7520456 -4.8058624 -4.2758455 -3.5514259 -2.8836994 -1.7470865 -1.13663 -0.86160851 -0.55706406 -0.97149515 -2.20543 -3.1254869 -3.9308174 -4.7211227 -6.008831][-4.2369142 -4.6449218 -4.4991531 -3.7225349 -3.4416165 -2.6550446 -2.5276608 -2.148984 -1.8337169 -1.8733168 -2.9482274 -3.9170108 -4.6199112 -5.5269346 -5.678277][-4.6674242 -4.6368179 -4.4283562 -4.6873684 -4.6105194 -4.3994703 -4.2417388 -4.1975746 -4.0929174 -4.5165606 -5.1257281 -5.5606208 -5.8696561 -6.6144304 -6.4950352][-5.6932058 -5.6175475 -5.079711 -4.3222647 -4.2362661 -4.4714246 -4.8018427 -5.181529 -5.6112137 -5.6306152 -6.4250035 -6.5858073 -6.7655511 -5.9886103 -5.96171][-7.31539 -6.4023619 -5.82602 -5.5602517 -5.5009732 -5.3934793 -5.7695909 -6.1178942 -6.191577 -6.40611 -6.5803523 -6.7590408 -6.6730313 -7.1615658 -6.5339766]]...]
INFO - root - 2017-12-15 13:11:41.808971: step 16310, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 58h:18m:06s remains)
INFO - root - 2017-12-15 13:11:48.202478: step 16320, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.637 sec/batch; 55h:59m:21s remains)
INFO - root - 2017-12-15 13:11:54.580890: step 16330, loss = 0.28, batch loss = 0.16 (13.1 examples/sec; 0.611 sec/batch; 53h:37m:57s remains)
INFO - root - 2017-12-15 13:12:00.956819: step 16340, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:37m:51s remains)
INFO - root - 2017-12-15 13:12:07.332790: step 16350, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 56h:03m:49s remains)
INFO - root - 2017-12-15 13:12:13.725935: step 16360, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 55h:46m:35s remains)
INFO - root - 2017-12-15 13:12:20.100188: step 16370, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 55h:53m:55s remains)
INFO - root - 2017-12-15 13:12:26.514435: step 16380, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 57h:08m:43s remains)
INFO - root - 2017-12-15 13:12:32.892024: step 16390, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.613 sec/batch; 53h:47m:51s remains)
INFO - root - 2017-12-15 13:12:39.328712: step 16400, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 57h:59m:46s remains)
2017-12-15 13:12:39.827478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8340034 -2.970891 -2.5990267 -2.5280681 -2.5923448 -2.9187961 -2.6776118 -2.1650705 -1.7326355 -1.875186 -3.732487 -5.5122671 -6.9760542 -7.8752646 -8.7911367][-4.0416918 -3.8006153 -3.3193278 -3.1734133 -3.4881282 -3.4091654 -3.4120169 -3.20676 -2.4777308 -2.3400664 -3.9094489 -5.4094248 -7.0466137 -8.2064056 -8.8640795][-3.3713923 -2.8368697 -2.7039142 -2.90843 -2.9095426 -2.8245602 -2.5783806 -2.2334332 -2.0547523 -2.1062822 -3.471046 -4.924108 -6.5246634 -7.6569219 -8.51287][-2.3531103 -1.950386 -1.5472159 -1.3444266 -1.5238705 -1.8542848 -1.1324601 -0.58830452 -0.32918358 -0.48386812 -2.1257892 -4.0366 -5.895638 -7.1408334 -8.1940241][-1.1396346 -0.61067629 -0.21139097 0.19444227 -0.14476967 -0.074007511 0.8004632 1.0723271 1.5475955 1.122025 -0.82475948 -2.8322215 -5.0326042 -6.5232391 -7.5649157][-0.67087078 -0.53942204 -0.14821625 0.38159895 0.8031106 1.4398141 2.2989326 2.8146033 3.1766019 2.1911645 -0.33163595 -2.7514281 -4.958086 -6.160512 -6.8496656][-1.4886975 -1.1035175 -0.3998332 0.40573168 1.2428651 2.2975411 3.190762 3.5521255 3.6823268 2.7658477 -0.38123655 -3.3749738 -5.6117454 -6.6784859 -7.1359835][-1.9457841 -1.6517949 -0.81687212 0.25395775 0.77376223 1.6900735 2.7823768 3.5096488 3.8170371 2.7078872 -0.41116047 -3.3002687 -5.7125344 -7.0320368 -7.6135926][-1.2618995 -0.9664855 -0.40898132 0.18039417 0.33098936 1.1296897 1.808557 2.1474948 2.2714438 1.3274064 -1.5196476 -3.9978356 -5.6941934 -6.5338473 -6.7566643][-2.221488 -1.4146523 -1.0135055 -0.80670309 -0.34628344 0.23270655 0.50825548 0.81000948 0.81243849 -0.38026094 -3.168128 -5.0545111 -6.4570045 -7.1235409 -7.0127597][-5.8998661 -5.2379031 -4.1190281 -3.3070207 -3.1157756 -2.4209485 -1.8356953 -1.7295575 -2.0349164 -2.7406263 -4.6933055 -6.2469044 -7.2285409 -7.5560517 -7.5827069][-6.4372969 -6.4800134 -5.92486 -5.2750378 -4.6294 -4.3466587 -4.1592026 -4.0169878 -4.0653505 -4.4846458 -5.8641162 -6.3569231 -6.8892765 -7.2269292 -7.2645378][-4.8019505 -4.913517 -5.1911974 -5.0591459 -4.8132653 -4.6599579 -4.4674387 -4.4143 -4.6271925 -4.7960415 -5.6568775 -5.7841754 -5.7009344 -5.5574532 -5.7204013][-4.2336483 -4.5037642 -4.8926868 -4.9718437 -4.6975656 -4.522644 -4.3136978 -4.4699745 -4.5802312 -4.4851789 -4.935503 -4.6401377 -4.6407089 -4.633153 -4.85746][-5.7086353 -5.1642447 -5.3174009 -5.6055827 -5.5211215 -5.0157251 -5.0859241 -5.3662634 -5.3245192 -5.5689397 -5.6120968 -5.2685671 -5.1893959 -5.30094 -5.3299923]]...]
INFO - root - 2017-12-15 13:12:46.188450: step 16410, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 56h:37m:54s remains)
INFO - root - 2017-12-15 13:12:52.481468: step 16420, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 55h:50m:07s remains)
INFO - root - 2017-12-15 13:12:58.845956: step 16430, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:32m:26s remains)
INFO - root - 2017-12-15 13:13:05.280851: step 16440, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 56h:42m:40s remains)
INFO - root - 2017-12-15 13:13:11.698956: step 16450, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 55h:13m:58s remains)
INFO - root - 2017-12-15 13:13:18.134155: step 16460, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 58h:36m:21s remains)
INFO - root - 2017-12-15 13:13:24.577590: step 16470, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 58h:38m:22s remains)
INFO - root - 2017-12-15 13:13:31.004763: step 16480, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 56h:36m:35s remains)
INFO - root - 2017-12-15 13:13:37.406611: step 16490, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 57h:52m:24s remains)
INFO - root - 2017-12-15 13:13:43.768117: step 16500, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 54h:39m:30s remains)
2017-12-15 13:13:44.291865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5369768 -4.703805 -5.4373493 -5.2096558 -4.3759503 -3.8682876 -3.0681248 -1.8827372 -1.4882269 -2.1459713 -2.8688765 -4.9627347 -6.6162844 -7.3948011 -7.5518107][-2.9186063 -3.9440203 -5.4771481 -6.3184133 -6.736659 -5.8039103 -4.3976331 -3.7369084 -3.6978264 -3.7420077 -4.4182734 -5.5403171 -6.6121035 -8.0312223 -8.8259411][-2.14854 -3.4125543 -5.2635207 -6.0729322 -6.3883357 -6.1600819 -5.0368662 -4.5706768 -4.4269061 -4.9416857 -5.2948761 -5.8339248 -6.6117744 -7.9036841 -8.4783611][-3.3209858 -3.4108305 -4.2907043 -4.4849191 -4.3845196 -3.5727115 -2.7350464 -2.5441294 -2.8918614 -3.8455138 -4.8744731 -5.6698627 -6.2837033 -7.224647 -7.7219377][-3.8402171 -3.3727431 -3.1344881 -2.2715139 -1.6895294 -0.53857517 0.30650616 -0.40272617 -0.86591482 -1.7546792 -2.7258902 -4.3043036 -5.7098165 -6.8319435 -7.1756325][-3.3796206 -2.0857244 -1.6061559 -0.76853943 -0.023941994 1.6776032 2.5231271 1.3978028 0.50901175 -0.58216858 -1.5531502 -2.9128313 -4.4740982 -5.8514571 -6.6871095][-3.2573094 -2.4205542 -1.4513912 -0.59714365 0.47356176 2.2467856 3.2741742 2.6360726 1.5308337 -0.26709795 -1.1740122 -2.6062045 -4.4140868 -5.9613972 -6.8510633][-3.3313503 -2.6180944 -1.981112 -0.82050085 0.6180768 1.9893651 3.0223317 3.1560454 2.4837966 0.62845373 -1.4729657 -3.3937716 -4.8471632 -6.4077282 -7.0975332][-4.2345462 -3.362658 -2.8292727 -1.7710075 -0.78298807 0.32372427 1.3651881 2.0757251 2.2919354 0.83303976 -1.0667477 -3.738035 -5.5272069 -6.6420789 -7.0801263][-4.5813127 -4.4357882 -4.3248763 -3.4540277 -2.7075534 -1.9275465 -0.83320141 0.020895958 0.52685213 -0.57568121 -2.2024083 -4.1386905 -5.7060423 -7.5416412 -8.2698479][-7.0663362 -6.7199697 -6.6231818 -5.8237052 -5.0396338 -4.2012711 -3.6012878 -3.1349411 -3.06491 -4.0969725 -4.9687948 -6.2273784 -7.3379021 -8.4774532 -8.927187][-7.9404612 -7.7028728 -7.7544475 -7.3032269 -6.6409607 -5.8865194 -5.2588639 -5.4574718 -5.5046091 -6.4879742 -7.0506287 -7.6065316 -7.8397217 -8.8637533 -9.0784922][-9.6632376 -9.6109028 -9.4107809 -8.7337437 -7.7967911 -6.8118944 -6.5198612 -7.0856218 -7.0094867 -7.5111713 -8.0180893 -8.0204172 -7.7886395 -8.0899057 -8.0250568][-9.9255342 -9.8206234 -9.3485708 -8.98417 -8.3802395 -7.6394329 -6.7458148 -6.5739818 -6.860898 -7.7975259 -7.8454003 -7.4514327 -7.2278204 -7.3777962 -6.9524007][-8.9376183 -8.36058 -8.7403889 -8.8325834 -8.746727 -8.54094 -7.500371 -7.0129194 -6.6467791 -7.0074697 -7.1743221 -6.6453977 -6.3231344 -5.8598971 -5.8122072]]...]
INFO - root - 2017-12-15 13:13:50.736427: step 16510, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 56h:45m:52s remains)
INFO - root - 2017-12-15 13:13:57.169733: step 16520, loss = 0.36, batch loss = 0.25 (12.6 examples/sec; 0.637 sec/batch; 55h:56m:22s remains)
INFO - root - 2017-12-15 13:14:03.640502: step 16530, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 57h:25m:40s remains)
INFO - root - 2017-12-15 13:14:10.039859: step 16540, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 57h:38m:35s remains)
INFO - root - 2017-12-15 13:14:16.443188: step 16550, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 55h:23m:36s remains)
INFO - root - 2017-12-15 13:14:22.831634: step 16560, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 54h:50m:23s remains)
INFO - root - 2017-12-15 13:14:29.266062: step 16570, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 55h:51m:21s remains)
INFO - root - 2017-12-15 13:14:35.601752: step 16580, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 55h:53m:24s remains)
INFO - root - 2017-12-15 13:14:41.975717: step 16590, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 56h:14m:04s remains)
INFO - root - 2017-12-15 13:14:48.387468: step 16600, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 55h:35m:42s remains)
2017-12-15 13:14:48.909090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4051085 -4.10428 -4.5231113 -4.5402451 -4.7686129 -4.4764752 -3.8192756 -3.110332 -2.3108239 -2.6043878 -2.6453967 -3.1400738 -3.6077638 -4.5388031 -5.9868917][-3.3934617 -3.5666428 -3.7481556 -4.043263 -4.1952772 -3.8335443 -3.3583398 -2.7399297 -1.9903097 -2.3710346 -2.5747433 -3.3075204 -4.0024214 -4.575191 -5.6610069][-3.9005287 -3.6204705 -3.6177292 -3.6219344 -3.5630999 -3.0812812 -2.6374764 -2.7017326 -2.5343761 -2.7632394 -3.1121693 -4.1107197 -4.7127018 -5.2068357 -6.1267309][-4.5838938 -3.6177387 -2.8265867 -2.5681114 -2.10717 -1.6515307 -1.3167605 -1.3478589 -1.1087284 -2.0088067 -3.0508966 -4.1960049 -4.6777878 -5.4040403 -6.6375146][-4.6397023 -3.5995855 -2.773982 -2.1156893 -1.5404377 -0.70131636 0.35417223 0.64527273 0.58179426 -0.89132929 -2.5476837 -3.8470759 -4.7660861 -5.7927837 -6.9457088][-5.3104186 -3.9829066 -2.4996514 -1.4756246 -0.50388384 0.48693514 1.2774014 1.6043487 1.4195361 -0.0031599998 -1.5562062 -3.0438428 -4.1365967 -5.3232174 -6.5825634][-4.5406857 -3.1738458 -2.2263827 -0.53068495 0.9380126 2.1413903 3.0526204 2.7901597 2.3868623 1.4136472 -0.040135384 -1.744535 -3.1577125 -4.6249228 -5.826663][-3.4172034 -1.9570446 -0.6648798 0.88669538 1.8042912 2.6455913 3.496017 3.2787805 3.398355 2.48696 1.0347648 -0.79065132 -2.5742912 -3.8953886 -5.1532211][-3.8157785 -2.7671494 -1.057158 0.6485734 1.5386062 2.1931491 2.2387338 2.4074683 2.6836591 1.9599605 0.75389433 -1.1224694 -2.6659474 -3.8290548 -4.9870462][-4.8355961 -4.0385442 -3.1086698 -0.89851 0.45517778 0.80911779 1.0843644 1.1291471 1.3169341 0.52713919 -0.75876713 -2.3149748 -3.2887611 -4.252439 -4.7675266][-6.8061867 -5.7411003 -4.9470739 -3.3410268 -2.2943382 -1.3693652 -1.0416989 -1.3596745 -0.92020512 -1.8007817 -2.934629 -3.8033295 -4.3184843 -5.2612257 -6.3131475][-8.4435749 -7.6968632 -6.5529532 -5.3591523 -4.6215348 -3.6529417 -3.2399778 -3.1144824 -2.7704926 -3.8226964 -4.6655812 -5.2311144 -5.5660858 -6.3597193 -6.9678183][-9.2658148 -8.4585 -7.4028492 -6.2887411 -5.2742848 -4.5685253 -4.0001645 -4.0555878 -4.1103468 -4.7348223 -5.3556652 -5.4626827 -5.6350431 -6.3006744 -6.6309342][-9.6805334 -8.9477243 -8.0870829 -6.9241 -5.9685507 -5.1388044 -4.3660417 -4.3088436 -4.5198593 -5.1804857 -5.5458717 -5.8984213 -6.5511765 -6.7492504 -6.6935515][-9.9334993 -9.3159571 -8.5728855 -7.7956266 -7.0750709 -6.6117005 -5.92334 -5.7068262 -5.8058796 -5.9845705 -6.1610222 -6.2045479 -6.2764235 -6.4831324 -6.2868829]]...]
INFO - root - 2017-12-15 13:14:55.350865: step 16610, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 57h:09m:37s remains)
INFO - root - 2017-12-15 13:15:01.860322: step 16620, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 56h:52m:29s remains)
INFO - root - 2017-12-15 13:15:08.265689: step 16630, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 57h:28m:48s remains)
INFO - root - 2017-12-15 13:15:14.695992: step 16640, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 55h:35m:30s remains)
INFO - root - 2017-12-15 13:15:21.005233: step 16650, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 54h:19m:16s remains)
INFO - root - 2017-12-15 13:15:27.407709: step 16660, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 55h:51m:47s remains)
INFO - root - 2017-12-15 13:15:33.814662: step 16670, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 55h:11m:00s remains)
INFO - root - 2017-12-15 13:15:40.196918: step 16680, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 55h:04m:18s remains)
INFO - root - 2017-12-15 13:15:46.593500: step 16690, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 56h:00m:01s remains)
INFO - root - 2017-12-15 13:15:52.997945: step 16700, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 56h:49m:56s remains)
2017-12-15 13:15:53.514617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6144729 -3.9556494 -4.3618708 -4.7725611 -4.65416 -4.1267061 -3.7090681 -3.2515821 -2.1248007 -2.3974071 -3.0185294 -3.0123792 -3.7174425 -5.0301819 -6.19119][-2.8268671 -3.1611958 -3.8191488 -4.0959163 -4.1605206 -4.1393814 -3.9725907 -3.4406681 -2.4689856 -2.6285977 -2.9504681 -2.9313788 -3.7049754 -4.7259946 -6.4774976][-3.0007052 -2.6376567 -2.8515239 -3.3558803 -3.729398 -4.0063076 -3.7351589 -3.6261802 -3.21942 -3.5832286 -3.7733293 -3.4842844 -4.3895597 -5.6739612 -6.7205272][-3.485116 -2.7740207 -2.6028428 -2.3658419 -2.0579748 -2.5525746 -2.8012266 -2.56384 -1.9382539 -3.2098536 -4.2367435 -3.8643675 -4.4305391 -5.89596 -6.9912958][-3.5309677 -2.8450532 -2.3003249 -1.6570859 -1.3251319 -1.3023295 -0.91645718 -0.61028385 -0.45391703 -1.6814232 -2.8762231 -3.4841475 -4.7959027 -5.8315072 -6.8594084][-3.5246563 -2.0863576 -1.3599291 -0.98714304 -0.34382534 0.43267918 0.96474552 0.75313759 0.83452034 -0.20364857 -1.6224651 -2.3989153 -3.9472234 -5.1434574 -6.4497962][-1.8301578 -1.1273556 0.072766304 0.89988327 1.4407911 2.1891336 3.3010168 3.1316357 2.7881594 0.9547472 -0.75509644 -1.6706953 -3.1685686 -4.7782421 -6.1615896][-0.60869169 -0.070203781 0.17988443 1.4593601 2.769865 3.0477676 3.6256418 3.9992886 4.2773762 2.0856943 -0.25939751 -1.6518793 -3.2183867 -4.5421915 -5.6076431][-1.1790843 -0.76527357 -0.039438725 1.1011839 1.9730892 2.3121347 2.6894073 3.1674633 3.5089445 1.6312141 -0.69782734 -2.2999005 -3.8452969 -4.8747044 -5.9938369][-2.3853831 -1.9093995 -1.0884905 -0.19501686 0.65455723 0.8627882 1.3180866 1.7008066 1.6705532 -0.029810429 -1.7132068 -3.4357047 -5.238555 -5.8673673 -6.6048932][-4.4047995 -4.1920819 -3.9249337 -3.1092286 -2.381505 -1.8502989 -1.1066251 -1.0599046 -0.99595642 -2.7823148 -4.2032294 -4.9385219 -6.2710371 -7.2180309 -7.6446595][-7.36582 -6.4092917 -5.8039517 -5.40003 -4.8006983 -4.4618874 -4.1952734 -3.8633132 -3.4857111 -4.8220911 -5.7360067 -5.5120611 -6.3402562 -7.7237945 -8.2208023][-8.1319695 -7.972878 -7.4899306 -6.7957606 -6.4420538 -5.9284339 -5.4789782 -5.5541906 -5.675869 -6.2225075 -6.7161512 -6.2360234 -6.5383463 -6.9800558 -7.6852875][-7.7470541 -7.9989142 -8.1522865 -7.8313327 -7.2429242 -6.9032507 -6.7868967 -6.640471 -6.58366 -7.065402 -7.0475736 -6.9890871 -6.9587612 -6.9126019 -6.7046804][-7.7176414 -7.3354139 -6.9323759 -7.3620319 -7.7208657 -7.3318267 -6.8641047 -7.0608835 -7.3128858 -7.2419834 -6.8811264 -7.0384011 -6.8668656 -6.7626476 -6.2485161]]...]
INFO - root - 2017-12-15 13:15:59.839262: step 16710, loss = 0.30, batch loss = 0.19 (13.2 examples/sec; 0.607 sec/batch; 53h:13m:17s remains)
INFO - root - 2017-12-15 13:16:06.197252: step 16720, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 56h:24m:37s remains)
INFO - root - 2017-12-15 13:16:12.583714: step 16730, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 54h:46m:38s remains)
INFO - root - 2017-12-15 13:16:18.973498: step 16740, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 58h:04m:27s remains)
INFO - root - 2017-12-15 13:16:25.521472: step 16750, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 58h:28m:14s remains)
INFO - root - 2017-12-15 13:16:31.996754: step 16760, loss = 0.36, batch loss = 0.24 (12.5 examples/sec; 0.639 sec/batch; 56h:01m:22s remains)
INFO - root - 2017-12-15 13:16:38.315546: step 16770, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.615 sec/batch; 53h:57m:47s remains)
INFO - root - 2017-12-15 13:16:44.744874: step 16780, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:45m:36s remains)
INFO - root - 2017-12-15 13:16:51.060703: step 16790, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 55h:00m:01s remains)
INFO - root - 2017-12-15 13:16:57.406938: step 16800, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 55h:14m:13s remains)
2017-12-15 13:16:57.917493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.402853 -2.9006286 -3.2372169 -3.4027214 -3.6859941 -3.9829991 -4.1549244 -4.2044811 -4.380724 -5.83757 -6.3692584 -7.3196297 -7.7065377 -8.0614958 -8.1688251][-2.564527 -2.9640746 -3.3862896 -3.9339917 -4.4435649 -4.6178265 -4.8322873 -5.1559787 -5.4394679 -6.7315178 -7.2768016 -8.2896223 -8.966774 -9.4662323 -9.6201782][-1.7484837 -2.060801 -2.3646684 -2.7660322 -3.0814409 -3.1711206 -3.3176732 -3.7891939 -4.3550491 -5.9783463 -6.6147866 -7.4600081 -8.0070639 -8.4460621 -8.6322279][-1.4017153 -1.6375599 -1.7124262 -1.5862961 -1.8702626 -1.9803896 -2.074934 -2.5043459 -2.8223543 -4.2694111 -5.1003261 -6.2217174 -6.9180937 -7.5890412 -8.09841][-1.57166 -1.2418857 -1.1762505 -1.1323462 -0.91776514 -0.75670862 -0.66857624 -0.86514711 -0.9338975 -2.3163815 -3.140728 -4.3984323 -5.3406858 -6.2894459 -7.0437603][-2.5865922 -1.9357524 -1.4365788 -1.0138016 -0.61356878 -0.21917915 0.21144104 0.27381611 0.49832392 -0.82679796 -1.6444721 -2.8631802 -3.9752584 -5.0641551 -5.9317484][-3.1323705 -2.4704752 -1.8006334 -0.95258522 -0.14007378 0.296278 0.69121408 1.1402459 1.6848664 0.34597731 -0.571455 -1.8320503 -2.6850581 -3.8824267 -5.0328455][-3.6334457 -2.8572054 -2.1694932 -1.0721741 0.0019583702 0.70702887 1.0962672 1.4943223 2.2187257 1.1500096 0.46058798 -0.85518217 -1.9628363 -3.1259456 -4.217732][-3.6465015 -2.9302764 -2.2910581 -1.4354196 -0.66147709 0.18891096 0.90195322 1.210856 1.6001287 0.3919816 -0.36824131 -1.6386766 -2.5380502 -3.5572233 -4.499157][-3.8804131 -3.4442515 -2.9048891 -2.10884 -1.3576946 -0.66736794 -0.053772926 0.45013285 0.88870668 -0.83992672 -1.7639585 -3.1431012 -4.1339417 -5.0222564 -5.6655159][-5.2526083 -4.658392 -4.0726423 -3.3315015 -2.6700006 -2.0435629 -1.5484924 -1.3291636 -1.0479407 -2.5964937 -3.5174565 -4.635376 -5.2081666 -5.8808908 -6.5013065][-5.3205032 -4.9772348 -4.5713177 -3.9968174 -3.5206118 -2.7595873 -2.3192983 -2.4089174 -2.3960562 -3.601759 -4.1993256 -5.276969 -5.8612871 -6.2189941 -6.459024][-6.1524363 -5.3890285 -5.0473452 -4.5601807 -3.9896953 -3.5684977 -3.3984332 -3.3553061 -3.306313 -4.181922 -4.5789771 -5.4184456 -6.1518831 -6.3728356 -6.1719422][-5.8142157 -5.18664 -4.6876345 -4.0217161 -3.6316037 -3.2392259 -3.1427336 -3.3566446 -3.534986 -4.3000059 -4.5749741 -4.9729853 -5.3899574 -5.8156314 -6.0717769][-6.8696408 -6.497992 -5.9308996 -5.4353127 -4.953866 -4.3695602 -4.2275686 -4.448885 -4.640871 -4.7412739 -4.8831906 -5.4097223 -5.9403758 -5.830029 -5.7535553]]...]
INFO - root - 2017-12-15 13:17:04.356186: step 16810, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 55h:52m:29s remains)
INFO - root - 2017-12-15 13:17:10.751610: step 16820, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 55h:57m:58s remains)
INFO - root - 2017-12-15 13:17:17.088650: step 16830, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:52m:42s remains)
INFO - root - 2017-12-15 13:17:23.557177: step 16840, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 55h:39m:21s remains)
INFO - root - 2017-12-15 13:17:29.954080: step 16850, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 57h:45m:57s remains)
INFO - root - 2017-12-15 13:17:36.316577: step 16860, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:52m:23s remains)
INFO - root - 2017-12-15 13:17:42.701290: step 16870, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 54h:45m:01s remains)
INFO - root - 2017-12-15 13:17:49.061057: step 16880, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 55h:10m:42s remains)
INFO - root - 2017-12-15 13:17:55.399501: step 16890, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:54m:25s remains)
INFO - root - 2017-12-15 13:18:01.799316: step 16900, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 57h:25m:14s remains)
2017-12-15 13:18:02.249697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0320368 -5.959404 -6.7387362 -7.1794028 -7.219656 -6.464838 -5.4782581 -4.3233032 -3.457479 -3.5743666 -4.4739933 -5.83712 -5.940208 -6.4529195 -6.5135994][-5.0770807 -6.0247917 -7.0281067 -7.1720724 -7.1623411 -6.5618978 -6.0409122 -5.1287718 -4.0884628 -4.5075779 -5.9033303 -7.7163825 -8.42747 -8.7628393 -8.2041235][-6.4304776 -6.5092974 -7.4342146 -7.3235497 -6.9349546 -6.0874271 -5.2676172 -5.0339851 -4.4642992 -4.8566341 -5.8194513 -7.5714235 -8.1112347 -8.97034 -8.9599237][-6.654335 -7.0496254 -7.6501751 -6.7448 -6.1026297 -4.8160152 -3.6024742 -3.2199645 -2.6016946 -3.2021708 -4.9189706 -7.1300092 -7.820982 -8.348341 -8.5075445][-8.2472105 -6.95279 -6.2395058 -5.5507765 -4.2571774 -2.2469592 -0.35733604 -0.099273682 0.12830019 -1.1658702 -3.0137539 -5.60517 -7.1052084 -8.2912436 -8.3972425][-7.2725658 -6.7298813 -5.6293144 -3.4992285 -1.5129895 0.67286777 2.5613108 3.3402548 3.3616123 1.3080282 -1.0629411 -4.2504187 -5.9509115 -7.2319369 -8.1088295][-6.4108272 -5.9937487 -4.7309971 -2.0959759 0.50416756 3.0005722 5.0208492 5.358243 5.0448933 2.7436132 -0.062697411 -3.4841723 -5.4215188 -7.0419631 -7.4105415][-6.9890113 -6.6719561 -5.0584354 -2.0300331 0.73878384 2.9512968 4.7601948 5.3528414 5.5267572 2.4718819 -1.0535235 -3.7632873 -5.1111064 -6.6495309 -6.9736729][-7.6388121 -7.4151011 -7.1300135 -4.8573275 -2.1283264 0.05737114 2.0057316 2.834487 3.4557056 0.6630764 -2.1451278 -4.8270092 -6.0391064 -6.92373 -7.0216422][-9.8550072 -9.4805813 -9.1087055 -7.5998778 -5.5543904 -3.5058475 -2.1246047 -1.7032495 -1.2509775 -3.3856215 -5.3900404 -7.3179841 -7.7277012 -8.0771952 -7.7825422][-11.291156 -11.574217 -11.692322 -10.853806 -9.5000381 -7.4239612 -5.3553791 -4.9995537 -5.1872606 -6.7160711 -7.7322626 -8.9895105 -9.4447155 -9.49758 -9.0356827][-12.481653 -12.489115 -12.088976 -11.197309 -10.267305 -8.9515076 -8.0220938 -7.5376797 -7.0354495 -7.9672875 -8.9242744 -9.6440382 -9.6670771 -9.4612045 -9.1631174][-11.723878 -11.866123 -11.596519 -10.964237 -10.07011 -9.1709833 -8.5663347 -8.6550026 -8.8717957 -9.6540623 -10.327894 -9.9570379 -9.318553 -8.8323288 -8.1186676][-10.480231 -9.8554535 -10.020276 -9.5017433 -9.0109415 -8.5837412 -7.9136167 -8.2291021 -8.4093609 -8.6629372 -9.34459 -9.34898 -9.0145216 -8.5725765 -7.6243811][-8.5479183 -8.5214176 -8.4777451 -8.532238 -8.9456606 -8.4928894 -8.0314693 -8.3184471 -8.3836918 -8.6843033 -8.8728495 -8.3028345 -7.978713 -8.20137 -8.21936]]...]
INFO - root - 2017-12-15 13:18:08.681428: step 16910, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 57h:07m:58s remains)
INFO - root - 2017-12-15 13:18:15.165635: step 16920, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 58h:22m:36s remains)
INFO - root - 2017-12-15 13:18:21.549108: step 16930, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 55h:56m:03s remains)
INFO - root - 2017-12-15 13:18:27.930214: step 16940, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 55h:33m:36s remains)
INFO - root - 2017-12-15 13:18:34.261296: step 16950, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 55h:48m:37s remains)
INFO - root - 2017-12-15 13:18:40.602371: step 16960, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:27m:42s remains)
INFO - root - 2017-12-15 13:18:46.966563: step 16970, loss = 0.32, batch loss = 0.20 (13.0 examples/sec; 0.616 sec/batch; 53h:58m:43s remains)
INFO - root - 2017-12-15 13:18:53.382112: step 16980, loss = 0.39, batch loss = 0.28 (12.5 examples/sec; 0.639 sec/batch; 56h:02m:38s remains)
INFO - root - 2017-12-15 13:18:59.726589: step 16990, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 56h:07m:27s remains)
INFO - root - 2017-12-15 13:19:06.138081: step 17000, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 56h:04m:34s remains)
2017-12-15 13:19:06.646493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75051212 -0.39494896 -0.58232832 -0.4933219 -0.25211859 -0.10132837 0.31192446 0.46669817 0.53085279 -1.1496205 -2.6924343 -4.4484634 -5.8551197 -7.1224647 -7.7979441][-2.5490069 -2.2518177 -2.4086061 -2.7066054 -2.9391088 -2.7732844 -2.3667035 -2.0532842 -2.0111351 -3.4196725 -4.6574559 -6.2174168 -7.4573922 -8.667901 -9.408905][-3.1510153 -3.0129371 -3.0088754 -2.9572778 -2.9836488 -2.7598629 -2.3968081 -2.0219417 -1.9605966 -3.5861406 -4.5569534 -5.9453726 -7.0283613 -8.0349674 -8.8403912][-4.1749492 -3.4461074 -3.0731683 -2.4600081 -1.9266996 -1.1912179 -0.37657785 0.0097589493 -0.05472374 -2.0621352 -3.23382 -4.928175 -6.1187162 -7.6577268 -8.2342434][-5.2547355 -4.3517876 -3.7691178 -2.8295822 -1.9242201 -0.80515909 0.13957882 0.28236103 0.12773323 -2.1040716 -3.3594937 -5.2489071 -6.5053272 -7.2365742 -7.6958547][-5.8722272 -4.8116961 -3.9433734 -2.6037154 -1.2491336 0.20748711 1.5578504 1.718801 1.4760451 -0.77747869 -2.2527776 -4.1232119 -5.3432732 -6.3647289 -6.6410389][-6.6607447 -5.6357403 -4.3685808 -2.7081041 -1.0769186 0.64144659 2.2254062 2.6927695 2.6516795 0.3013835 -1.3148422 -3.6717558 -5.3039308 -6.1837897 -6.3719106][-5.4112864 -4.1416349 -2.9909315 -1.1236281 0.46149397 2.2296195 3.8152394 4.1682038 4.1733956 1.6090331 -0.21355677 -3.0223498 -4.8054714 -6.0217614 -6.5343075][-4.0579615 -3.0614271 -2.0224037 -0.55007315 0.74051714 2.1316266 3.3258681 3.846148 4.0238204 1.7077298 -0.27173758 -2.5187869 -4.4224062 -5.6381946 -6.1424561][-3.8166845 -3.1043587 -2.4236875 -1.0513744 -0.038995266 1.1010537 2.1504636 2.6090665 2.5240836 0.5139575 -1.1044936 -3.2004242 -4.6694193 -6.0670109 -6.6587071][-5.1408854 -4.3035507 -3.573936 -2.6519008 -1.958807 -0.97787619 -0.0650568 0.30961514 0.42620516 -1.333302 -2.7827916 -4.5415459 -5.7017217 -6.3924971 -6.9477067][-6.0451188 -5.3994522 -4.8230124 -4.0030317 -3.5034609 -2.9485626 -2.5454617 -2.3283706 -2.1347795 -3.2826138 -4.110209 -5.2386227 -5.9907742 -6.767488 -7.2141981][-7.4565878 -7.2596736 -6.73728 -6.3333445 -5.960732 -5.1951556 -4.7878962 -4.58986 -4.5948391 -5.1138792 -5.5721426 -6.1559868 -6.3771834 -6.5646105 -6.5104094][-7.7735882 -7.3472128 -7.1888542 -6.7116766 -6.5202451 -6.1198707 -5.5282135 -5.4529285 -5.4822412 -5.9984756 -6.0456295 -6.3656559 -6.5768428 -6.5642266 -6.4900522][-8.1953754 -7.8910503 -7.8119707 -7.8653383 -7.8532653 -7.5346279 -6.9114838 -6.8641911 -6.8348036 -6.9161973 -7.12698 -7.1880693 -7.1033926 -6.984683 -6.8047147]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 13:19:13.130880: step 17010, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 54h:20m:19s remains)
INFO - root - 2017-12-15 13:19:19.450488: step 17020, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 55h:06m:58s remains)
INFO - root - 2017-12-15 13:19:25.853607: step 17030, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 56h:54m:13s remains)
INFO - root - 2017-12-15 13:19:32.262152: step 17040, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:28m:48s remains)
INFO - root - 2017-12-15 13:19:38.676733: step 17050, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 55h:31m:14s remains)
INFO - root - 2017-12-15 13:19:45.086722: step 17060, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 55h:58m:45s remains)
INFO - root - 2017-12-15 13:19:51.463412: step 17070, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 55h:13m:22s remains)
INFO - root - 2017-12-15 13:19:57.874264: step 17080, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 56h:06m:50s remains)
INFO - root - 2017-12-15 13:20:04.188284: step 17090, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 54h:01m:13s remains)
INFO - root - 2017-12-15 13:20:10.597944: step 17100, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 56h:23m:41s remains)
2017-12-15 13:20:11.073081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4951234 -4.4355249 -4.9135714 -5.2782831 -5.4525986 -5.0851021 -4.1133833 -3.386663 -2.3738098 -2.8719916 -3.5073156 -4.685421 -5.5561743 -6.8138752 -7.9510207][-4.2064714 -4.281002 -4.8549418 -5.2975016 -5.7754812 -5.578742 -4.4903479 -3.7573922 -2.6759024 -3.2728992 -4.1060734 -5.4736223 -6.21539 -7.1356692 -8.1381512][-3.5917025 -3.8207259 -4.4766493 -4.7705059 -5.0219669 -4.462606 -3.5680666 -3.2981634 -2.6015382 -3.4856768 -4.3661461 -5.7332773 -6.2425036 -7.024056 -7.650115][-3.3448 -2.9329057 -3.2866106 -3.3414869 -3.2178903 -2.5313616 -1.4968071 -0.8503294 -0.49165058 -2.0002885 -3.7287793 -5.7513423 -6.37011 -7.073894 -7.47784][-4.2850876 -3.4262481 -3.0050802 -2.6497908 -1.8769755 -0.4715066 0.99436855 1.459136 1.6853743 0.24495649 -1.808002 -4.1765642 -5.7220759 -6.7748017 -7.4037833][-4.3418841 -3.5155597 -3.1107316 -2.0343256 -0.59810734 1.0748606 2.7548895 3.4544878 3.4395761 1.4643879 -0.64751291 -3.0080495 -4.4412804 -5.8859468 -6.886508][-4.4087296 -3.1276045 -2.3362112 -0.965137 0.37781239 2.1075516 3.6515408 4.3320761 4.7406387 2.73386 0.24525642 -2.4845128 -3.9660888 -5.5318823 -6.3643703][-3.7248054 -2.8693976 -2.1548734 -0.695436 0.84566784 2.7361841 4.2773743 4.6036263 4.7696066 2.6884642 0.41183853 -2.3033795 -3.8819377 -5.4075832 -6.50542][-4.0941005 -3.1413627 -2.2491088 -0.7936964 0.12402487 1.6155577 3.193409 4.04854 4.5400848 2.4164371 0.54346371 -2.2119412 -3.9575105 -5.5376248 -6.6060224][-4.4614344 -3.8410497 -3.1207595 -1.9311819 -0.899796 0.61456966 1.9868135 2.4791822 2.7985325 0.72886848 -1.060832 -3.3160672 -4.3625317 -5.7903261 -7.0251193][-4.9657335 -4.7657509 -4.0051537 -2.7678466 -1.9291925 -0.81875515 0.2711277 0.810071 1.0321646 -0.72557831 -2.3785763 -4.2611485 -5.0609322 -6.1829467 -7.3129606][-5.5658312 -4.8829441 -4.0241909 -3.0453691 -2.2872353 -1.6324258 -0.92208958 -0.63315821 -0.55145168 -2.0143762 -3.3363218 -4.5660591 -4.9971318 -6.2627416 -6.9773922][-6.6741152 -5.771204 -4.8210239 -3.5549212 -2.6868095 -2.0028887 -1.7797794 -1.614193 -1.6827731 -2.3659043 -3.5657845 -4.1105289 -4.501555 -5.53642 -6.4611111][-6.8965287 -6.0747986 -5.0256567 -4.0402527 -3.5392833 -2.725101 -2.2756319 -2.5427332 -2.6594205 -3.0052567 -3.9550564 -4.5141821 -4.5964437 -5.2572451 -5.6100831][-7.6103477 -6.5991826 -5.6445637 -4.8534718 -4.4768257 -3.890002 -3.8087091 -3.9631364 -4.2162724 -4.2178078 -4.4951916 -4.52413 -4.5348682 -4.8778934 -5.2414541]]...]
INFO - root - 2017-12-15 13:20:17.511071: step 17110, loss = 0.26, batch loss = 0.14 (11.6 examples/sec; 0.690 sec/batch; 60h:29m:06s remains)
INFO - root - 2017-12-15 13:20:23.941103: step 17120, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 55h:19m:22s remains)
INFO - root - 2017-12-15 13:20:30.317440: step 17130, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.619 sec/batch; 54h:15m:23s remains)
INFO - root - 2017-12-15 13:20:36.818203: step 17140, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 57h:14m:43s remains)
INFO - root - 2017-12-15 13:20:43.251937: step 17150, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 55h:13m:33s remains)
INFO - root - 2017-12-15 13:20:49.613052: step 17160, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 55h:06m:22s remains)
INFO - root - 2017-12-15 13:20:56.076064: step 17170, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 57h:26m:49s remains)
INFO - root - 2017-12-15 13:21:02.534527: step 17180, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.663 sec/batch; 58h:04m:47s remains)
INFO - root - 2017-12-15 13:21:08.962764: step 17190, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 54h:59m:53s remains)
INFO - root - 2017-12-15 13:21:15.414849: step 17200, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 54h:37m:17s remains)
2017-12-15 13:21:15.920648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2859325 -7.4027905 -7.4542918 -7.3017182 -7.0229645 -6.3514428 -5.8779831 -5.4222794 -4.4913683 -4.0686579 -4.203548 -4.6963711 -5.1868076 -6.1770344 -6.9525566][-7.2211356 -7.3173275 -7.2679634 -7.6957636 -7.7535319 -6.7323489 -5.6762075 -5.1593323 -5.0214157 -4.567481 -4.8797307 -4.9765835 -5.2586808 -6.2385488 -7.2139821][-6.70281 -6.6477118 -6.5459 -6.1119151 -5.3316059 -4.5373726 -4.2758584 -4.12265 -3.8798726 -3.2473288 -3.4038033 -3.9156444 -4.6346951 -6.0949321 -6.9789085][-5.2883654 -5.3167629 -4.7004046 -3.9043219 -3.6315432 -2.6601796 -1.5588074 -1.5573716 -1.5834827 -1.9857483 -2.6313109 -3.6195374 -3.9885411 -5.0283065 -6.0413518][-4.4683719 -3.9915078 -3.1995664 -2.1844382 -1.2481284 -0.33546829 0.37541294 0.33942413 0.66926289 -0.23800516 -1.2356753 -2.8181314 -3.6851497 -5.2807088 -6.1540575][-2.7610445 -2.7131014 -1.5750594 -0.36470842 0.38290882 1.0946827 1.7734003 1.6732998 1.5693884 0.43932629 -0.84455967 -2.6170301 -3.957551 -5.5422754 -6.6833615][-2.4852672 -1.7258482 -0.33347607 0.17056704 0.78996658 1.5685387 2.3290272 2.3518009 2.2326403 0.9014101 -0.94846249 -3.0289717 -4.6991119 -6.4118977 -7.210885][-2.1491928 -1.8000436 -1.1171384 0.15056276 0.91513443 1.6205368 2.1148024 2.4317017 2.4106045 1.2212038 -0.73197222 -3.1756015 -5.1253753 -6.8744464 -7.7067118][-2.9804087 -2.3093433 -0.97252607 -0.23910618 0.070345879 1.0041151 1.6074171 1.7906895 1.7843838 0.86554909 -1.0212498 -3.0632625 -4.5302143 -6.5886555 -7.4834185][-3.5201545 -2.4148846 -1.7485342 -1.2844381 -0.90308237 -0.31559944 0.019555092 0.58706188 0.91027069 -0.088491917 -1.6637335 -4.0388346 -5.5539026 -6.8135557 -7.4388909][-5.1106949 -4.0859108 -3.2541151 -2.8181129 -2.5151181 -2.231071 -1.9083943 -1.8687959 -2.0301628 -2.38966 -3.1440697 -4.7827339 -5.576108 -6.445508 -6.9289207][-5.9013605 -4.7957468 -3.9458311 -3.6740327 -3.3827744 -2.9878516 -2.6315761 -2.5955462 -2.589829 -3.1171484 -3.8926497 -4.7259092 -5.27532 -5.4160504 -5.8074617][-5.9760938 -5.5518441 -4.6823645 -4.4385748 -3.7389662 -3.7972538 -3.6424246 -3.7234106 -3.9865863 -3.9299769 -4.2625461 -4.9394884 -5.1728907 -5.1044507 -5.0819798][-6.239253 -5.6235113 -5.0103774 -4.8400707 -4.460453 -4.2987103 -4.5271273 -4.921689 -4.9970026 -5.134851 -5.6608763 -4.9409642 -4.4694333 -4.2814026 -4.3108544][-7.5695043 -7.0773768 -6.1244221 -6.0584626 -5.7076344 -5.5251694 -5.7525392 -5.9091115 -6.2320328 -5.8053522 -4.922091 -4.5771017 -4.5127068 -4.1107178 -3.7941515]]...]
INFO - root - 2017-12-15 13:21:22.497770: step 17210, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 57h:04m:28s remains)
INFO - root - 2017-12-15 13:21:28.893772: step 17220, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 54h:52m:04s remains)
INFO - root - 2017-12-15 13:21:35.381991: step 17230, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 58h:31m:12s remains)
INFO - root - 2017-12-15 13:21:41.790021: step 17240, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 55h:13m:02s remains)
INFO - root - 2017-12-15 13:21:48.104361: step 17250, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.623 sec/batch; 54h:33m:08s remains)
INFO - root - 2017-12-15 13:21:54.524385: step 17260, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 55h:48m:31s remains)
INFO - root - 2017-12-15 13:22:00.930689: step 17270, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 56h:05m:50s remains)
INFO - root - 2017-12-15 13:22:07.286743: step 17280, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:47m:45s remains)
INFO - root - 2017-12-15 13:22:13.726562: step 17290, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 57h:59m:41s remains)
INFO - root - 2017-12-15 13:22:20.164707: step 17300, loss = 0.23, batch loss = 0.12 (12.7 examples/sec; 0.630 sec/batch; 55h:07m:11s remains)
2017-12-15 13:22:20.680869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7993321 -4.826901 -4.55324 -4.3464909 -4.4778409 -4.1303935 -4.334157 -4.0865326 -3.862021 -2.9435096 -5.5363355 -5.0935497 -5.9895091 -5.9364777 -5.7794666][-4.1254687 -4.8602571 -3.7316785 -4.2087679 -3.7714636 -3.386837 -3.0824914 -2.8485456 -3.5581493 -2.8843241 -6.2046995 -4.94545 -5.9232979 -6.399848 -6.370749][-5.2303352 -5.2293792 -4.4794636 -4.2374744 -3.9649923 -3.7822213 -2.8927488 -3.0623183 -2.8059516 -2.2170877 -5.9897785 -5.323349 -7.51256 -7.13281 -7.2898855][-5.4966726 -5.74541 -5.2504473 -3.8409483 -2.387567 -1.6793361 -1.7312403 -2.5634289 -1.7338719 -1.3723168 -4.737422 -4.41036 -6.3605623 -6.7664142 -7.2172384][-6.4063597 -5.0876136 -3.3943768 -2.6586251 -2.0852833 -1.5453453 -1.4012704 -1.8251834 -1.3527737 -1.2391562 -3.2053065 -2.8895178 -5.365427 -6.1353831 -7.264924][-3.7666061 -3.8633535 -3.384511 -1.8455491 0.39857006 1.5457048 1.7012873 0.77331638 -0.49082422 -0.62798738 -3.0196834 -2.9619699 -4.1576862 -4.5586185 -5.6261106][-3.1941676 -1.5166268 0.48580933 1.4059954 1.9883118 2.4512463 3.4910221 3.2653418 1.6016502 0.51461315 -3.2875009 -3.1215115 -3.9295971 -4.6880331 -5.4782844][-1.4710498 -1.0299931 -0.58511305 0.48114014 2.8564167 3.8685884 4.4485865 3.8494568 4.0265551 2.8147774 -2.4441652 -3.3789072 -6.1528139 -6.7900958 -6.0994687][-2.8409266 -1.5185046 -0.106493 1.8670683 2.3195181 2.3009863 2.6681137 2.9493637 3.0041628 2.0131655 -1.5725694 -2.8433695 -5.3409252 -6.2451506 -7.2007365][-5.6466084 -4.3272061 -2.7788329 -1.531837 -0.38849306 0.88316631 0.6918726 0.025661469 -0.1793251 -0.68516779 -2.4871554 -4.0875468 -5.7457461 -6.9912982 -7.2851067][-6.5967693 -6.6769276 -5.8735852 -4.5894027 -3.5111055 -3.6070418 -3.4992752 -3.541584 -3.7943385 -4.7091646 -6.5385761 -6.5467987 -7.0442686 -7.7371631 -7.8038387][-7.4290843 -7.2672338 -6.865 -6.5367622 -5.566184 -4.7263184 -4.2904806 -4.3481846 -4.6672144 -4.3554506 -6.070899 -6.6080308 -7.3384504 -7.6527767 -7.7633615][-7.9690228 -7.6397943 -6.7041745 -6.3081188 -5.5653534 -5.1848793 -4.0880785 -3.806124 -3.8544722 -4.4425573 -5.3392248 -6.048049 -6.5397806 -6.5479732 -6.3119965][-8.2143736 -8.2384129 -8.4836054 -7.6816449 -6.571342 -5.5096188 -4.411706 -3.8452706 -3.3059688 -3.3992152 -4.0834064 -4.7299128 -5.1999006 -6.1116 -6.470376][-8.084836 -8.17111 -8.25473 -6.8986998 -6.7350736 -6.420063 -6.386137 -5.6980877 -5.259038 -5.3950958 -5.2008467 -5.8371415 -6.7411408 -7.0615869 -6.6506705]]...]
INFO - root - 2017-12-15 13:22:27.038033: step 17310, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 55h:52m:00s remains)
INFO - root - 2017-12-15 13:22:33.469984: step 17320, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 56h:29m:13s remains)
INFO - root - 2017-12-15 13:22:39.888687: step 17330, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 56h:43m:29s remains)
INFO - root - 2017-12-15 13:22:46.304764: step 17340, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 55h:56m:42s remains)
INFO - root - 2017-12-15 13:22:52.750189: step 17350, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 55h:35m:00s remains)
INFO - root - 2017-12-15 13:22:59.123182: step 17360, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 57h:12m:59s remains)
INFO - root - 2017-12-15 13:23:05.688555: step 17370, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 56h:02m:06s remains)
INFO - root - 2017-12-15 13:23:12.147083: step 17380, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.639 sec/batch; 55h:55m:06s remains)
INFO - root - 2017-12-15 13:23:18.535172: step 17390, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 57h:17m:57s remains)
INFO - root - 2017-12-15 13:23:24.953465: step 17400, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 55h:46m:58s remains)
2017-12-15 13:23:25.443313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8653274 -6.9788456 -6.665597 -6.6421251 -6.4064541 -5.9156537 -5.3134317 -5.0129967 -4.109951 -4.6251369 -4.9011707 -5.8533158 -6.5227189 -7.3667989 -8.52236][-7.8311749 -7.5311446 -7.261816 -7.3758106 -7.2249479 -6.8052535 -6.4164062 -5.6235132 -4.7299061 -5.0099468 -5.110014 -5.8599281 -6.6588907 -7.1435337 -7.5498228][-7.3841352 -7.0377378 -6.8467355 -6.6455555 -6.5842967 -6.6738477 -6.202251 -5.4593592 -4.7054253 -5.0825863 -5.3397913 -6.1836605 -6.6723814 -7.1290455 -7.2454276][-7.6179638 -6.6238022 -6.0659103 -5.345088 -4.8881521 -4.3246145 -4.0803518 -3.4843636 -2.7865295 -3.7762845 -4.5586128 -5.5585928 -6.6191654 -7.2538967 -7.638845][-6.8925762 -6.0446172 -5.54434 -4.539979 -3.7821612 -2.680934 -1.8698492 -1.5719643 -1.0841274 -2.0938172 -3.2284427 -4.8194151 -6.2007608 -7.2449803 -7.6351643][-6.2309637 -5.1732988 -4.4038 -3.3573198 -2.1958618 -0.59778786 0.61349344 0.86538076 0.88348627 -0.50240278 -1.47995 -3.6131573 -5.1473827 -6.7332935 -7.4987659][-5.566493 -4.4925027 -3.3525457 -2.1167798 -1.0113988 0.62296534 1.8520026 1.8759704 1.9491086 0.77635431 -0.84387541 -2.8947825 -4.5375862 -6.2705326 -7.2305574][-5.0434237 -3.8658118 -2.786582 -1.3775845 -0.05350399 1.5843768 2.8306251 2.8283734 2.6828914 1.1200576 -0.37687159 -2.582984 -4.3597994 -5.9876771 -6.738905][-5.0126982 -3.9910893 -2.8842268 -1.661386 -0.41735697 0.91866732 1.9853663 2.1568542 2.1417222 0.69004107 -0.65483332 -2.5870981 -4.1714582 -5.5560894 -6.3905125][-4.6140327 -4.0752058 -3.0476394 -1.9964013 -0.82321453 0.055680752 0.60274744 0.43355703 0.33622885 -0.75661945 -1.8678694 -2.7321343 -3.4908819 -4.8829985 -6.28939][-5.1468325 -4.352767 -3.5335846 -2.6046157 -1.6364679 -0.85758734 -0.78133583 -1.2283401 -1.0013638 -1.7760048 -2.5667877 -3.0993285 -3.9252443 -4.3793054 -4.9982772][-5.6279664 -4.90679 -4.1637774 -3.2954278 -2.6383452 -2.2102981 -1.7190194 -2.1180038 -1.9352684 -2.2684722 -2.6183553 -3.0609622 -4.1469736 -5.018281 -6.0745425][-6.4011889 -5.82323 -4.9291105 -4.3937035 -3.4826307 -3.0125737 -3.1189933 -3.2951336 -3.4419537 -3.5442877 -3.6746449 -3.6611772 -4.4017048 -4.8060155 -5.4095926][-5.8989096 -5.8742085 -5.53621 -5.0287066 -4.5685039 -4.2011647 -3.7196627 -4.19502 -4.4473534 -4.1995449 -4.3568873 -4.0828533 -4.6151047 -4.851222 -5.1319504][-6.920373 -6.2630682 -5.9453492 -5.9752035 -5.796308 -5.7118545 -5.8353195 -5.8165951 -6.0174837 -6.1839519 -5.8842831 -5.7052393 -6.247304 -6.063571 -5.7183762]]...]
INFO - root - 2017-12-15 13:23:31.830706: step 17410, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 54h:53m:27s remains)
INFO - root - 2017-12-15 13:23:38.295044: step 17420, loss = 0.25, batch loss = 0.14 (11.6 examples/sec; 0.692 sec/batch; 60h:33m:08s remains)
INFO - root - 2017-12-15 13:23:44.944786: step 17430, loss = 0.29, batch loss = 0.18 (11.7 examples/sec; 0.684 sec/batch; 59h:49m:20s remains)
INFO - root - 2017-12-15 13:23:51.419233: step 17440, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 55h:41m:19s remains)
INFO - root - 2017-12-15 13:23:57.888235: step 17450, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 57h:10m:07s remains)
INFO - root - 2017-12-15 13:24:04.383345: step 17460, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:41m:53s remains)
INFO - root - 2017-12-15 13:24:10.867863: step 17470, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 56h:55m:25s remains)
INFO - root - 2017-12-15 13:24:17.227435: step 17480, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 55h:27m:45s remains)
INFO - root - 2017-12-15 13:24:23.710121: step 17490, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.632 sec/batch; 55h:20m:30s remains)
INFO - root - 2017-12-15 13:24:30.098686: step 17500, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 55h:08m:33s remains)
2017-12-15 13:24:30.614182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1743498 -1.789793 -1.6056538 -1.0215693 -0.64157915 -0.77317953 -1.2866402 -1.3826289 -1.5224972 -3.2556581 -3.2202559 -4.2535582 -5.10773 -6.2011275 -7.2890291][-2.294673 -1.9363322 -1.4636865 -0.68487167 -0.18044281 -0.23184872 -0.6411972 -1.2835507 -1.8841419 -3.4944654 -3.8913362 -5.1517687 -5.8698058 -6.7501335 -7.5530181][-3.3349252 -2.7436419 -2.5374079 -1.7833095 -1.2505064 -0.90122557 -1.1357541 -1.7570052 -2.1276522 -3.5391512 -4.0601692 -5.5172119 -6.2908454 -7.3274393 -8.3520107][-4.4943347 -3.8885472 -3.5044451 -2.5912843 -1.6555147 -1.2733417 -1.3502064 -1.5383773 -1.6319895 -3.0387082 -3.2866573 -4.6728067 -5.6840658 -6.7488518 -8.0131626][-4.8412938 -3.9840119 -3.4293323 -2.3743839 -1.5247331 -0.7113409 0.0096092224 -0.55452156 -0.94298887 -2.2546849 -2.5494328 -3.8893657 -4.8513527 -5.6897593 -6.7863665][-4.5714521 -3.9826858 -3.7725089 -2.5840039 -1.3172579 -0.54454279 -0.3431654 -0.85792685 -1.0319476 -2.0802875 -1.8152971 -3.0745196 -3.7466888 -4.6496611 -5.8706789][-3.5834064 -3.4630208 -3.3130889 -1.9596977 -0.6813612 0.40919781 0.8880434 0.26570606 -0.319551 -1.8558712 -1.6528134 -2.3275118 -2.423595 -3.2156434 -4.2813826][-3.3003068 -2.3903193 -2.0229654 -0.6095562 0.64071655 1.8531618 2.5318451 2.3562517 1.914938 -0.0936718 -0.5669651 -1.5413361 -1.9561925 -2.2607856 -2.8987861][-4.0747423 -3.088964 -2.5265689 -1.2380304 0.42381096 1.7745962 2.5729666 2.8268452 2.9092131 1.0486116 0.16096401 -0.91598749 -1.8245764 -2.1690955 -2.931428][-5.5524263 -4.83629 -3.9445 -2.7895765 -1.6488938 -0.34689569 1.0968237 1.3353262 1.5886021 -0.21107244 -0.72549009 -1.4146051 -2.5933366 -3.1141696 -3.9214602][-6.8761454 -6.3933725 -5.9609122 -4.9228568 -3.737057 -2.5287657 -1.605382 -0.97437525 -0.57288265 -2.2798419 -2.8348794 -3.8960986 -4.9359884 -5.3990879 -5.6540055][-8.4923916 -7.6937871 -7.350482 -6.8166256 -5.6882234 -4.3805876 -3.6355329 -3.4209065 -3.5450587 -4.6665568 -4.5929308 -5.4311872 -6.0138469 -6.6139374 -7.0805821][-8.7415857 -8.4135466 -7.986331 -7.2112846 -6.7447281 -6.034955 -5.1152449 -4.6093841 -4.4710913 -5.3976612 -5.6341949 -6.0836711 -5.9973917 -6.0696268 -6.2640691][-9.0212383 -8.594965 -8.3902683 -8.2855406 -8.0504341 -7.2586637 -6.501946 -6.0983315 -6.0807929 -6.2959638 -6.5741382 -6.5918865 -6.5599003 -6.4617605 -6.0218487][-8.1148663 -7.494966 -7.8672271 -8.0694408 -7.5291882 -6.9211082 -6.2556648 -5.9442515 -5.9350405 -5.9001684 -5.9929729 -6.1390891 -6.4122815 -5.8582926 -5.7624545]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 13:24:38.162357: step 17510, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 56h:18m:22s remains)
INFO - root - 2017-12-15 13:24:44.548277: step 17520, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 55h:09m:13s remains)
INFO - root - 2017-12-15 13:24:51.074003: step 17530, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 58h:21m:50s remains)
INFO - root - 2017-12-15 13:24:57.566114: step 17540, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 57h:21m:43s remains)
INFO - root - 2017-12-15 13:25:04.014775: step 17550, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 56h:11m:27s remains)
INFO - root - 2017-12-15 13:25:10.493211: step 17560, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:46m:05s remains)
INFO - root - 2017-12-15 13:25:16.879053: step 17570, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 55h:24m:09s remains)
INFO - root - 2017-12-15 13:25:23.316603: step 17580, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 54h:32m:30s remains)
INFO - root - 2017-12-15 13:25:29.748374: step 17590, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 55h:18m:42s remains)
INFO - root - 2017-12-15 13:25:36.124591: step 17600, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 55h:46m:10s remains)
2017-12-15 13:25:36.639250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0276093 -3.5992794 -4.6487055 -4.59381 -3.7747114 -3.4831676 -3.3032231 -2.1525612 -1.6423035 -3.1019669 -3.9586759 -5.1447582 -6.5200658 -7.0891118 -7.7573547][-2.9752026 -3.5408492 -4.6520371 -5.9512305 -6.4557877 -4.7741165 -3.2492437 -2.8726993 -2.4723167 -3.1056757 -4.2771058 -5.8964972 -7.2678618 -8.3827724 -8.7928972][-2.7521071 -3.9726973 -5.4436483 -5.8620167 -5.9691973 -5.1613836 -3.9488242 -3.6043539 -3.076055 -3.8569956 -4.5328679 -5.9202194 -7.2200828 -8.3422775 -8.9813795][-3.3452997 -3.3949876 -4.2490067 -4.2843437 -4.03668 -3.6368036 -2.8249364 -2.4385324 -2.0459695 -3.5388045 -4.4529204 -5.4573097 -6.6802106 -7.4802341 -7.758862][-3.6671138 -2.8205595 -2.6234674 -1.7195663 -1.0780115 -0.75069237 -0.27414322 -0.46819973 -0.584414 -1.8050437 -2.8102522 -4.4344835 -6.1393428 -7.0151567 -7.2606268][-3.2910771 -1.5872126 -0.51420069 0.38205338 0.671093 1.6011772 1.9665232 0.88685036 0.48166847 -0.82162 -1.7709599 -3.3211956 -4.8720074 -5.9151559 -6.4993095][-3.2753544 -1.811677 -0.47273588 0.87100983 1.8068018 2.9911051 3.9154997 2.8615131 1.5252953 -0.585269 -1.3114023 -2.6125641 -4.2041407 -5.3945084 -6.3341732][-2.9278121 -2.0728579 -0.99919653 0.53824329 1.9644718 3.3110991 4.3876534 4.08696 2.9747629 0.11654997 -1.6369658 -3.1576076 -4.4831247 -5.7075696 -6.2152452][-3.6828709 -2.6416397 -1.8692966 -0.54754972 0.41557026 1.4515896 2.4530678 2.9544897 3.2394476 1.0029602 -1.2726974 -3.7725916 -5.31007 -6.0688338 -6.3803988][-3.9606752 -3.3601875 -3.2381396 -2.4329219 -1.4799495 -0.45597744 0.42013645 1.0765944 1.5920811 -0.097165585 -1.8808432 -4.2107468 -5.8242526 -7.0327873 -7.410481][-6.3141217 -5.6587296 -5.4067698 -4.8220425 -4.1037593 -3.3263679 -2.7293954 -2.4351335 -2.4182334 -3.6307325 -4.8072329 -6.2029772 -7.1520996 -7.9495292 -8.2941856][-7.6972 -6.7826686 -6.5905046 -6.1301866 -5.7023044 -5.0648928 -4.5258493 -4.6572771 -4.9098196 -6.11851 -6.6979208 -7.0223069 -7.3795862 -8.5222473 -8.9222469][-9.8376713 -9.1307373 -8.7143888 -8.185998 -7.7509379 -7.0220537 -6.4661217 -6.7677455 -6.7489324 -7.3063178 -7.65809 -7.7502065 -7.7009239 -7.980927 -7.960125][-9.8063984 -9.3268642 -8.8609934 -8.56522 -8.0932169 -7.3780417 -6.3751512 -6.2111979 -6.6092563 -7.337286 -7.3730712 -7.3627639 -7.0822883 -6.9607687 -6.4900041][-8.6652813 -8.108182 -8.5741091 -8.7534637 -8.6604843 -8.0361071 -6.9906764 -6.7409186 -6.6385541 -7.0242419 -7.3541522 -6.7535467 -6.0780859 -6.1340685 -5.8432674]]...]
INFO - root - 2017-12-15 13:25:43.109924: step 17610, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 55h:41m:25s remains)
INFO - root - 2017-12-15 13:25:49.544568: step 17620, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 54h:34m:01s remains)
INFO - root - 2017-12-15 13:25:55.996391: step 17630, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 57h:11m:24s remains)
INFO - root - 2017-12-15 13:26:02.469584: step 17640, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 54h:53m:17s remains)
INFO - root - 2017-12-15 13:26:08.885497: step 17650, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:19m:07s remains)
INFO - root - 2017-12-15 13:26:15.243048: step 17660, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 54h:12m:25s remains)
INFO - root - 2017-12-15 13:26:21.657154: step 17670, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 54h:56m:48s remains)
INFO - root - 2017-12-15 13:26:28.094919: step 17680, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:23m:40s remains)
INFO - root - 2017-12-15 13:26:34.504804: step 17690, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 55h:50m:12s remains)
INFO - root - 2017-12-15 13:26:40.882324: step 17700, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.617 sec/batch; 53h:59m:15s remains)
2017-12-15 13:26:41.409000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2542377 -5.6772418 -4.7951059 -4.3598385 -4.3093777 -3.229877 -1.8438368 -1.0860839 -1.0310874 -2.4837461 -3.3670473 -3.9067194 -5.1067343 -5.564723 -6.5540953][-5.175889 -4.8494267 -4.8588953 -3.9493892 -2.247005 -1.4210067 -1.2610631 -0.70636034 -0.018933773 -1.6569672 -3.0190988 -3.6500249 -4.7832656 -4.950994 -5.7147112][-6.433042 -5.2115688 -3.9355793 -3.2064734 -2.2378192 -1.6111994 -0.69632006 -0.3206563 -0.17705393 -2.0924864 -2.9268994 -3.8054218 -4.6841393 -4.7043071 -5.46877][-7.4050555 -6.0528879 -4.8469849 -3.1677256 -0.83545971 0.39797735 0.96817732 1.1430116 0.90880442 -1.3419209 -2.7516875 -4.1266713 -5.624176 -5.6453772 -5.4319944][-8.8923378 -6.5629945 -3.6884866 -2.0255437 -0.51746893 0.97990751 2.4364038 2.5133176 1.9580789 -0.882761 -2.6019297 -3.3821769 -5.367425 -6.2116213 -6.8104496][-8.4722586 -5.9762812 -4.1515722 -1.7826114 1.0853896 2.3924527 2.9053721 2.8459563 2.3507791 -0.26017046 -1.8395028 -3.2722349 -5.3275328 -6.1715059 -7.268693][-7.3707242 -5.1373715 -2.4494143 -0.051834106 1.2267823 2.2635522 3.6153979 3.9400725 3.6242604 0.68227243 -1.3908229 -2.6663504 -4.2231374 -5.0466194 -6.0892439][-7.4201255 -4.8325567 -2.4034228 -0.12182188 1.8094525 2.6067557 2.4906011 2.8078942 2.884439 0.59951258 -1.0193052 -2.9550457 -4.4118867 -5.3371162 -6.0830584][-6.9425797 -6.0970387 -3.9507005 -1.3325987 0.2975235 0.36867285 0.50483274 1.3713431 1.8951144 -0.23394871 -2.3076396 -3.7001598 -4.7099996 -5.4905186 -6.2233791][-7.6866179 -6.320878 -4.8598833 -3.3839359 -1.0351696 0.20437813 0.44681406 0.21725273 0.46943617 -1.0769124 -2.5294652 -4.170167 -5.9514689 -6.06198 -6.378562][-7.9283953 -7.4497018 -7.5564647 -6.177681 -3.7554095 -1.9163585 -1.4014716 -1.3476562 -1.199748 -2.4655819 -3.4844494 -4.6367292 -5.7296438 -6.0474348 -6.5019741][-10.70584 -9.4323263 -8.004364 -7.4063506 -6.5291529 -4.8056 -3.2828603 -2.7323017 -3.0418005 -3.6277957 -4.1020088 -4.4904304 -5.757586 -6.7179604 -7.3672223][-11.769333 -10.589064 -9.7165718 -8.6285334 -7.09254 -6.0098257 -4.542285 -3.4329171 -2.8065906 -3.1609011 -4.3281403 -4.6210842 -4.5687027 -4.7017756 -5.2822361][-11.683712 -11.710009 -10.636144 -9.4090576 -8.5479689 -7.157331 -5.5795794 -4.4714365 -3.8067648 -3.6685848 -3.7475288 -4.2324324 -4.6615906 -5.1562748 -5.4489703][-10.502642 -10.538589 -10.194768 -9.7634325 -8.7378912 -7.8810682 -7.0170879 -6.3481903 -5.2344508 -4.4674997 -4.5792265 -5.4305081 -5.6044211 -5.8779192 -6.1495018]]...]
INFO - root - 2017-12-15 13:26:47.911620: step 17710, loss = 0.27, batch loss = 0.16 (11.5 examples/sec; 0.694 sec/batch; 60h:42m:10s remains)
INFO - root - 2017-12-15 13:26:54.338685: step 17720, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 56h:42m:35s remains)
INFO - root - 2017-12-15 13:27:00.804091: step 17730, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 56h:47m:18s remains)
INFO - root - 2017-12-15 13:27:07.198765: step 17740, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 55h:36m:17s remains)
INFO - root - 2017-12-15 13:27:13.639579: step 17750, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:21m:28s remains)
INFO - root - 2017-12-15 13:27:20.081230: step 17760, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 55h:39m:42s remains)
INFO - root - 2017-12-15 13:27:26.505880: step 17770, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 56h:37m:04s remains)
INFO - root - 2017-12-15 13:27:33.018764: step 17780, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 57h:52m:36s remains)
INFO - root - 2017-12-15 13:27:39.419194: step 17790, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.685 sec/batch; 59h:50m:39s remains)
INFO - root - 2017-12-15 13:27:45.830568: step 17800, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 55h:14m:39s remains)
2017-12-15 13:27:46.283700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.49181 -5.9649682 -7.0830421 -7.6400228 -8.0032387 -8.14212 -7.53353 -6.2151594 -4.5636754 -4.6108093 -4.6043549 -5.6811028 -6.8553138 -6.4215751 -6.4911222][-4.0149527 -4.5393076 -5.7027445 -6.8204589 -7.7052011 -7.1377721 -6.6515665 -6.031157 -5.2332311 -5.2692337 -5.0702219 -6.6930122 -7.6958 -6.8968754 -6.4736624][-2.3182812 -3.6081362 -4.4449477 -5.0773911 -5.3285036 -5.4533749 -5.6548471 -5.3404112 -5.3018341 -5.8278494 -6.5555563 -7.9702344 -8.6402836 -8.632102 -8.0603256][-3.2799416 -3.2769775 -3.5011415 -4.9311562 -5.4319177 -4.9378872 -4.1502485 -4.7502346 -4.8815203 -5.1130204 -5.6607194 -7.1198969 -8.4333858 -8.6544 -8.8283653][-2.4632711 -3.2542148 -3.9447892 -3.8624337 -2.9856834 -2.5611415 -2.3879256 -2.3665395 -3.5796838 -4.7984285 -5.1277661 -6.7687531 -7.8045874 -8.1900482 -8.4386425][-4.0113306 -3.9793873 -4.0446777 -3.0367265 -1.2492647 -0.34112167 0.81829548 0.29483271 -1.5569191 -3.3895245 -4.9488449 -6.8437929 -7.4601378 -7.5819454 -7.2093334][-4.1506233 -4.0797114 -3.8704982 -2.4812074 0.35701752 1.9851274 3.4670315 3.1700239 2.1307077 -0.73462534 -3.1934557 -5.8329411 -7.3471146 -6.7691255 -6.4185019][-3.3437767 -3.0983973 -3.1277881 -1.2002034 1.9214106 4.2246628 6.0632 5.6984673 4.8569441 1.4887266 -1.8082266 -4.7587385 -6.5560884 -6.4437232 -6.4753075][-4.5781245 -3.6890602 -2.6532211 -1.1662016 0.1006031 1.8885708 4.4924479 4.5534182 3.5247755 0.55381775 -2.077754 -4.9265118 -6.8174911 -6.4028463 -5.9767714][-6.4799128 -5.5153923 -5.0605536 -3.4458447 -2.2185507 -0.41421938 1.5912886 1.7860575 1.721714 -0.97495985 -3.4837108 -6.2649751 -7.8138018 -7.1937408 -7.3561435][-7.9732504 -7.6732688 -7.2215462 -5.8066478 -4.458518 -2.958077 -1.9013801 -1.3071241 -1.0349851 -3.2399726 -4.3781576 -7.583426 -9.1763248 -8.6463089 -8.1068754][-8.688714 -7.9260597 -8.0028448 -7.4239473 -6.6189651 -6.0694227 -4.8606186 -4.8632855 -4.9111967 -6.2564249 -5.7463665 -7.5889454 -8.3615255 -7.7540359 -8.591567][-9.1778049 -8.4833546 -8.619976 -7.6900697 -7.27345 -6.9593797 -6.8687844 -6.8646231 -6.2347879 -6.9833989 -6.7063932 -8.0219851 -8.9630861 -8.9218025 -9.408762][-8.9337063 -8.6490307 -7.2000337 -6.2342043 -6.6375179 -6.3891392 -6.908567 -7.8363304 -7.989707 -8.5353251 -7.8972378 -8.1773348 -8.6730833 -7.6235971 -7.9193535][-9.2262287 -9.7471857 -9.59956 -8.7645845 -7.7296247 -7.3099236 -7.5572004 -7.83786 -8.4719753 -8.8907442 -8.542037 -8.1876526 -6.9373045 -6.8730631 -6.8812218]]...]
INFO - root - 2017-12-15 13:27:52.707459: step 17810, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 57h:41m:05s remains)
INFO - root - 2017-12-15 13:27:59.254895: step 17820, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.664 sec/batch; 58h:01m:25s remains)
INFO - root - 2017-12-15 13:28:05.676096: step 17830, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 56h:52m:17s remains)
INFO - root - 2017-12-15 13:28:12.113535: step 17840, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 57h:38m:59s remains)
INFO - root - 2017-12-15 13:28:18.572971: step 17850, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 56h:08m:13s remains)
INFO - root - 2017-12-15 13:28:24.948617: step 17860, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 55h:09m:14s remains)
INFO - root - 2017-12-15 13:28:31.300444: step 17870, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 56h:43m:40s remains)
INFO - root - 2017-12-15 13:28:37.688647: step 17880, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 56h:00m:50s remains)
INFO - root - 2017-12-15 13:28:44.081026: step 17890, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 56h:12m:33s remains)
INFO - root - 2017-12-15 13:28:50.496617: step 17900, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 55h:26m:36s remains)
2017-12-15 13:28:51.030592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85511017 -1.2518678 -1.8496308 -2.4060144 -2.9551277 -3.4566541 -3.5466485 -3.1885428 -2.6980867 -3.6091042 -4.3980532 -4.9544487 -5.6775246 -6.4900751 -6.9430346][-1.4719462 -1.871624 -2.4281445 -2.9475303 -3.4437881 -3.5622296 -3.6798568 -3.5795293 -3.4264922 -4.2384119 -4.8348494 -5.5824356 -6.5692282 -7.3717523 -7.8319855][-1.7890368 -1.8088036 -1.9236588 -2.2423263 -2.4612188 -2.5106091 -2.5701914 -2.4515309 -2.4177456 -3.4469237 -4.0899653 -4.7478213 -5.5344572 -6.3688836 -6.9612894][-2.0055609 -1.9903288 -1.9577937 -1.7778802 -1.6515241 -1.4304619 -1.2267575 -1.1458063 -1.2374105 -2.4252944 -3.3869305 -4.4317656 -5.5622134 -6.2080221 -6.4598336][-2.7231846 -2.5505915 -2.3077703 -1.9722366 -1.5156431 -0.83336592 -0.33489037 -0.17413855 -0.080513477 -1.11625 -2.1901236 -3.3816786 -4.5222673 -5.5611129 -6.0748734][-3.6101499 -3.1657968 -2.6112275 -2.063848 -1.4143448 -0.44073343 0.2377553 0.4475522 0.57036829 -0.60050297 -1.6415806 -2.5221591 -3.3757362 -4.5513382 -5.4761639][-3.7016845 -3.0977745 -2.5209846 -1.6539402 -0.72554874 -0.036475182 0.61355734 1.208344 1.5032086 0.34634829 -0.52979374 -1.4822779 -2.540329 -3.8812997 -5.0440855][-3.2885838 -2.8639216 -2.2435603 -1.3685622 -0.44690514 0.390985 1.0798125 1.5911584 2.0637202 1.0732388 0.064101696 -0.92291307 -2.1056023 -3.4305105 -4.2970266][-3.2303767 -2.4614649 -2.1350994 -1.5210013 -0.65592861 0.16200733 0.74994707 1.0913968 1.5123754 0.78490686 0.010811806 -1.3513522 -2.6939697 -4.0558176 -4.8337331][-3.3900557 -2.7545161 -2.2045674 -1.4712224 -0.84542751 -0.17934275 0.31791925 0.58790827 0.97965 -0.15894699 -1.1174359 -2.2989283 -3.7019694 -4.7229085 -5.4427295][-4.5585227 -3.9555507 -3.3429122 -2.715374 -2.1214461 -1.6300287 -1.3351102 -1.1714315 -0.9570899 -2.2888818 -3.1439767 -3.9287968 -4.8027754 -5.6416063 -5.9993196][-5.5163689 -5.0028 -4.488121 -3.8881633 -3.3658471 -2.9494596 -2.8251386 -2.9548044 -2.8338466 -3.7976708 -4.6970143 -5.3273535 -5.9060764 -6.159379 -6.234025][-6.2350178 -5.8203073 -5.4844656 -4.9125347 -4.381424 -4.1235514 -4.101655 -4.0597754 -4.0769076 -4.9395523 -5.5276203 -5.8396978 -6.23889 -6.2127495 -5.959559][-6.535327 -5.8626547 -5.4376411 -5.055336 -4.52334 -4.4084377 -4.4251943 -4.5214715 -4.6640258 -5.4171505 -5.7260909 -5.7624531 -5.897356 -5.843163 -5.4908552][-7.3420734 -7.10004 -6.5958881 -5.9569855 -5.6017904 -5.4928384 -5.5739489 -5.5863862 -5.6239643 -5.8286858 -6.1160469 -6.2568846 -6.3444653 -6.1903124 -5.943089]]...]
INFO - root - 2017-12-15 13:28:57.426335: step 17910, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 55h:12m:27s remains)
INFO - root - 2017-12-15 13:29:03.885655: step 17920, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 56h:28m:20s remains)
INFO - root - 2017-12-15 13:29:10.299667: step 17930, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 56h:02m:19s remains)
INFO - root - 2017-12-15 13:29:16.691400: step 17940, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 55h:20m:31s remains)
INFO - root - 2017-12-15 13:29:23.093886: step 17950, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 54h:24m:01s remains)
INFO - root - 2017-12-15 13:29:29.417577: step 17960, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 54h:58m:26s remains)
INFO - root - 2017-12-15 13:29:35.898190: step 17970, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:43m:40s remains)
INFO - root - 2017-12-15 13:29:42.338398: step 17980, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.643 sec/batch; 56h:08m:06s remains)
INFO - root - 2017-12-15 13:29:48.829594: step 17990, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 55h:23m:19s remains)
INFO - root - 2017-12-15 13:29:55.270175: step 18000, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 55h:12m:33s remains)
2017-12-15 13:29:55.751674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3857813 -6.0626187 -7.4953418 -8.5254745 -8.8933086 -8.378788 -7.3000436 -6.277112 -4.4827147 -4.4313164 -4.7131939 -4.709404 -5.2165723 -6.4533238 -6.6498866][-4.593359 -6.13684 -7.6148953 -8.8544207 -9.6605482 -9.7146091 -9.0004025 -8.1034317 -6.4654942 -6.5366707 -6.7283969 -6.8475094 -7.780149 -8.9254389 -8.7387295][-4.8173342 -5.99655 -7.6478744 -8.816741 -9.2818995 -9.2294378 -8.5591335 -8.0891457 -7.1489744 -7.49068 -8.0564356 -8.7815113 -9.4431953 -10.307761 -10.194918][-7.0748744 -7.0908589 -7.3030419 -7.9349728 -8.1558628 -7.2573147 -6.5829 -6.0548725 -5.1957917 -5.9209852 -7.11669 -8.351573 -9.7852249 -10.696843 -10.413839][-7.3366747 -7.11744 -6.92772 -6.5785971 -6.2763152 -4.6986561 -3.4914083 -2.6342654 -1.1325135 -2.394042 -4.3023267 -6.8455496 -8.8587112 -10.455476 -10.971043][-8.3393745 -7.1884866 -6.141295 -4.5455418 -2.9649725 -0.59821272 1.1849823 1.8573904 2.2123442 0.31620979 -2.2243762 -5.0288997 -7.5529237 -9.5555782 -10.329314][-6.9239693 -5.58486 -4.2408295 -1.768115 0.54156017 3.291378 5.7335749 6.3091097 6.278266 3.3584576 0.22515535 -2.8988805 -5.5285091 -7.5332165 -8.6179857][-6.2036958 -5.002234 -3.5336185 -0.40778017 3.0140982 5.6959543 7.6295309 7.832058 7.883688 4.6580067 1.1331663 -2.0203605 -4.6967583 -6.8193207 -7.753437][-6.7369165 -5.9792547 -5.0702238 -2.3508954 0.14948797 2.6824741 5.2629719 5.8264112 6.3574142 3.0372458 -0.48721361 -2.8687367 -4.9665093 -6.6947069 -7.5141044][-8.8726044 -8.1647129 -7.2668066 -4.8717279 -2.5588388 -0.72560072 1.4286757 1.6944056 1.3850698 -1.9746232 -4.9524717 -6.8439741 -8.1349392 -8.85088 -8.7502251][-12.145992 -12.158939 -11.235893 -9.7968874 -8.0109215 -5.7439995 -3.2394629 -2.4761214 -1.9155216 -4.1043139 -6.3509727 -7.8591127 -9.6192293 -10.53357 -10.573623][-12.915752 -13.046043 -12.537392 -11.733725 -10.350453 -8.3838091 -6.5717249 -5.8420715 -4.6298389 -5.799736 -6.72174 -7.4174662 -9.2355709 -9.9554825 -10.018532][-12.47164 -12.798948 -12.445259 -11.912713 -11.107947 -9.656374 -8.2357264 -7.45096 -6.9522166 -8.1364737 -8.9298878 -8.7698221 -9.1441994 -9.1851664 -8.5614185][-10.09872 -10.34926 -10.202624 -9.95343 -9.2807817 -8.2674465 -7.1294351 -7.1976266 -7.565362 -8.1716728 -8.3389759 -8.2941618 -8.4506693 -8.0342007 -7.5232725][-9.1744509 -9.2517471 -9.4448719 -9.4925318 -9.4021969 -8.5812407 -7.8745317 -7.9744515 -7.9977431 -8.5176849 -8.791996 -8.7212591 -8.5324545 -8.0244312 -7.556159]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 13:30:02.316620: step 18010, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 58h:09m:11s remains)
INFO - root - 2017-12-15 13:30:08.771600: step 18020, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:17m:38s remains)
INFO - root - 2017-12-15 13:30:15.252424: step 18030, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 57h:11m:35s remains)
INFO - root - 2017-12-15 13:30:21.751386: step 18040, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 55h:41m:44s remains)
INFO - root - 2017-12-15 13:30:28.194562: step 18050, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 57h:39m:15s remains)
INFO - root - 2017-12-15 13:30:34.543469: step 18060, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 55h:36m:05s remains)
INFO - root - 2017-12-15 13:30:40.962401: step 18070, loss = 0.35, batch loss = 0.24 (12.3 examples/sec; 0.652 sec/batch; 56h:56m:29s remains)
INFO - root - 2017-12-15 13:30:47.456243: step 18080, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 56h:02m:11s remains)
INFO - root - 2017-12-15 13:30:53.831175: step 18090, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:15m:25s remains)
INFO - root - 2017-12-15 13:31:00.317363: step 18100, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:42m:30s remains)
2017-12-15 13:31:00.849421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2313151 -1.2425251 -1.7445412 -1.5755143 -1.5953612 -1.1570973 -1.1698933 -1.0753407 -1.193975 -2.6685963 -3.6536751 -5.4245729 -7.0592108 -6.9275751 -8.7765951][-3.2228642 -2.791966 -2.5011005 -3.0210233 -3.2185221 -3.2641144 -3.1316724 -2.8732438 -2.980669 -4.0055056 -5.1641555 -6.5237856 -8.2270164 -7.8567824 -9.3655186][-3.8193865 -3.8088136 -3.6031299 -2.9308419 -2.6646333 -2.7762442 -2.8059206 -2.4818063 -2.2585301 -3.6181431 -4.4780436 -6.2734165 -7.5747271 -7.3032618 -8.9883337][-4.0421591 -3.9808793 -3.2634158 -2.4854484 -1.8260956 -1.0625572 -0.922277 -0.76169491 -0.87519312 -2.3117924 -3.148047 -4.788312 -6.2231445 -6.0253425 -7.1793938][-5.7947674 -4.4118004 -3.2653584 -2.3517451 -1.28828 -0.47070885 0.13941717 0.10768795 0.032232761 -2.338521 -3.4500036 -4.7869954 -6.29055 -6.1541138 -6.88474][-5.60192 -4.6657991 -3.7806528 -2.0572448 -0.77723026 0.53497744 1.7899528 1.8917241 1.435431 -0.57873678 -1.9484391 -3.9595015 -5.2131739 -4.9786224 -6.0928745][-5.8017492 -4.8340712 -3.6077561 -1.8904042 -0.22386503 1.2199368 2.9431596 2.9912257 2.7717633 0.639132 -0.98501635 -3.151638 -5.3645053 -5.22515 -5.5479488][-4.5626583 -3.3013654 -2.3528481 -0.660161 1.2464528 2.2819867 3.7684808 4.26421 4.63423 2.0128865 0.22345114 -2.5376806 -4.8059397 -4.9229813 -6.0172205][-3.5810232 -2.2107086 -1.277854 -0.0081911087 1.0728211 2.1168838 3.393312 3.9398646 4.4964442 2.3439202 0.39728594 -2.257534 -4.4272528 -4.66459 -5.9104047][-3.4650831 -2.5297441 -1.7825909 -0.573998 0.44955397 1.2588458 2.2224298 2.4380336 2.3092923 0.70902491 -0.90119028 -3.2659545 -5.0718126 -5.2769251 -6.3568058][-4.5087481 -4.3348351 -3.6074386 -2.4711637 -1.5066624 -0.95741749 -0.30648327 -0.20535135 0.10695171 -1.3821745 -2.7319999 -4.4036503 -5.95897 -5.8422785 -6.6309557][-5.9403334 -5.105104 -4.71457 -4.2273827 -3.5593505 -3.2929392 -2.9309931 -2.6518846 -2.4002781 -3.5058122 -4.0212688 -5.7741909 -6.6797857 -6.6804681 -7.6872687][-7.4494634 -7.3668194 -6.9828348 -6.2801332 -5.7774434 -5.3031044 -4.7713633 -4.7882504 -4.8439417 -5.4665537 -6.0031862 -6.4048862 -7.1255159 -6.9157033 -7.1127477][-7.7803373 -7.7350054 -7.4759917 -6.5889692 -5.7423263 -5.6560831 -5.127265 -5.0413723 -5.2191148 -5.8818889 -6.1453009 -6.59466 -6.6901817 -6.373394 -6.8357553][-8.1058 -7.880096 -7.8219924 -7.6112132 -7.2428842 -6.8460488 -6.103148 -6.06298 -6.1063185 -6.4155016 -6.7957268 -6.9076915 -7.0097051 -6.8625388 -6.55927]]...]
INFO - root - 2017-12-15 13:31:07.279078: step 18110, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 56h:41m:38s remains)
INFO - root - 2017-12-15 13:31:13.799887: step 18120, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 55h:24m:05s remains)
INFO - root - 2017-12-15 13:31:20.210549: step 18130, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 56h:23m:17s remains)
INFO - root - 2017-12-15 13:31:26.650728: step 18140, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 56h:52m:51s remains)
INFO - root - 2017-12-15 13:31:33.027709: step 18150, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 54h:25m:03s remains)
INFO - root - 2017-12-15 13:31:39.428139: step 18160, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 56h:45m:46s remains)
INFO - root - 2017-12-15 13:31:45.847174: step 18170, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 56h:07m:41s remains)
INFO - root - 2017-12-15 13:31:52.224619: step 18180, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 55h:53m:26s remains)
INFO - root - 2017-12-15 13:31:58.623925: step 18190, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 54h:30m:11s remains)
INFO - root - 2017-12-15 13:32:04.986439: step 18200, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 54h:29m:58s remains)
2017-12-15 13:32:05.545246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2690954 -4.9495234 -3.5999475 -3.071907 -2.9619217 -2.8870511 -2.730648 -2.6096292 -2.1226902 -3.3658137 -3.6556935 -4.6049414 -5.2763262 -6.1985846 -6.8141775][-5.1426535 -5.0441203 -4.8192182 -4.2839336 -3.2920442 -2.8025131 -2.7314868 -2.7089829 -2.2643166 -3.4939132 -3.5972757 -4.6198521 -5.6253319 -6.7368097 -7.0341954][-5.3152361 -4.7708626 -4.3648553 -3.9898133 -3.424089 -2.8538361 -2.173121 -1.7917862 -1.7783351 -3.4493032 -3.9618454 -5.0670071 -5.9135752 -6.69904 -6.8763156][-4.9305449 -4.1478353 -3.3379183 -2.6170959 -1.9204926 -1.5457277 -1.1721292 -0.90235758 -0.13246775 -1.331111 -2.13408 -3.7397976 -5.2181787 -6.3045487 -6.4333239][-3.8719018 -3.0716929 -2.840507 -2.5513344 -1.8368587 -0.55918217 0.80122852 1.0314312 0.90462303 -0.65872765 -1.3909602 -2.7817216 -4.6406717 -6.0639563 -6.67065][-4.6619835 -3.6420746 -3.140419 -1.9621696 -0.81331825 0.24859762 1.2600603 1.7452116 2.1591702 0.36270809 -1.1091995 -3.0141859 -4.7331128 -5.7580233 -6.3884783][-4.9196634 -4.0951042 -3.0253482 -1.3272052 0.14474297 1.4164925 2.29455 2.3816509 2.5211506 0.91394711 -0.56842422 -2.7140722 -4.3637753 -5.6030731 -6.2193065][-3.9794219 -3.1044464 -2.5463343 -1.2952099 0.038243771 1.6414022 2.7974072 3.2424479 3.6301394 1.6456108 -0.25876045 -2.5629411 -4.4574995 -5.5469856 -5.9146729][-4.5342774 -3.6076331 -2.6649036 -1.4276838 -0.19502258 1.0069647 1.5142946 1.7435017 1.9443903 0.63221264 -0.44311333 -2.7366781 -4.9642305 -6.3218708 -6.9308562][-4.474474 -3.8925619 -3.2022839 -1.9735284 -0.93883848 0.049609661 0.51807213 0.43326855 0.071837425 -1.2962642 -2.0627942 -3.7541258 -5.401967 -6.4317875 -6.89332][-5.1342163 -4.6746712 -3.9016349 -3.0705109 -2.5307522 -1.7091365 -0.99663687 -0.88030672 -0.9826088 -2.08384 -2.9379578 -4.4307323 -5.1209354 -5.7700858 -6.15501][-6.7033253 -6.2231 -5.8954716 -5.2190838 -4.33962 -3.4415388 -3.0634532 -3.0061789 -3.0145698 -3.4508119 -3.9718888 -4.8860703 -5.2042351 -5.8522344 -6.2743196][-6.1822824 -5.7702384 -5.70948 -5.3864131 -4.9944921 -4.391326 -4.14178 -4.0658088 -3.9820368 -4.7264566 -5.1327868 -5.9051847 -6.1794882 -6.2921896 -6.1718473][-7.2666039 -6.4160671 -5.9191117 -5.555665 -5.0379934 -4.6490483 -4.6597385 -4.704257 -4.4520464 -4.7824697 -4.7616673 -5.4446359 -5.7726111 -5.9586487 -5.9595613][-9.0573692 -8.1982145 -7.2809587 -6.6367164 -5.9815836 -5.5329065 -5.5699387 -5.6475344 -5.8545866 -5.9586673 -5.6690903 -5.7419939 -5.634335 -5.7657137 -5.9760184]]...]
INFO - root - 2017-12-15 13:32:12.001371: step 18210, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 58h:46m:02s remains)
INFO - root - 2017-12-15 13:32:18.374907: step 18220, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 54h:19m:11s remains)
INFO - root - 2017-12-15 13:32:24.810578: step 18230, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 55h:55m:12s remains)
INFO - root - 2017-12-15 13:32:31.208159: step 18240, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 55h:07m:16s remains)
INFO - root - 2017-12-15 13:32:37.615814: step 18250, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.686 sec/batch; 59h:54m:47s remains)
INFO - root - 2017-12-15 13:32:44.089809: step 18260, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 55h:38m:58s remains)
INFO - root - 2017-12-15 13:32:50.579197: step 18270, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.640 sec/batch; 55h:50m:04s remains)
INFO - root - 2017-12-15 13:32:57.005243: step 18280, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.634 sec/batch; 55h:22m:01s remains)
INFO - root - 2017-12-15 13:33:03.350641: step 18290, loss = 0.35, batch loss = 0.23 (12.8 examples/sec; 0.623 sec/batch; 54h:25m:02s remains)
INFO - root - 2017-12-15 13:33:09.781552: step 18300, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 56h:12m:51s remains)
2017-12-15 13:33:10.337957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8494418 -3.9350581 -4.1827431 -4.4087539 -4.2951827 -4.2987909 -4.0868816 -3.8253932 -3.6276698 -4.0968361 -4.09505 -4.7451968 -5.3427382 -5.7837434 -6.7989535][-3.5089693 -3.668005 -3.9740567 -4.3081293 -4.47877 -4.6515779 -4.7363138 -4.86432 -4.7003307 -5.5882249 -5.2200394 -6.1975441 -7.0139065 -7.266387 -8.2595148][-3.7711842 -3.4670873 -2.9214821 -2.7695522 -2.8240404 -2.8759313 -3.1556215 -3.8231914 -4.2539072 -4.9744043 -4.92383 -5.6189356 -6.2454166 -6.6911039 -7.4823351][-2.3972583 -1.5376086 -1.0928245 -1.0670471 -1.3677268 -1.5123458 -1.4205675 -2.0097461 -2.3464723 -3.6158881 -3.858145 -4.5524549 -5.7088108 -5.8024788 -6.5189953][-1.7614536 -1.0541959 -0.17827797 0.043035984 -0.22633171 -0.11239862 0.16729593 -0.45267534 -0.93877697 -2.1369967 -2.4442153 -3.0886536 -4.0674133 -4.5289574 -5.9172945][-1.4733315 -0.43773794 0.0667367 0.31808186 0.37046909 0.50289154 0.91819859 0.53935623 0.22556782 -0.81079197 -0.79579449 -1.2907767 -2.0970955 -2.8973441 -4.3569145][-1.3001771 -0.30766392 0.56643581 0.97267818 1.2239485 1.3205538 1.6235571 1.3854637 1.2230511 0.098433018 -0.11417818 -0.68264389 -1.4900761 -2.2147684 -3.7902672][-0.67434931 -0.039482117 0.81079006 1.614872 2.0492058 2.0581665 2.2923098 2.186985 2.0628481 0.66036987 0.37720203 -0.40240431 -1.3440561 -2.0936918 -3.7249317][-0.84126616 -0.07369709 0.42558098 0.82373905 1.4134169 1.4258242 1.2721672 1.5051231 1.8638287 0.645874 0.15012455 -0.86691666 -1.9823847 -2.7771559 -4.3990626][-2.2326813 -1.8152776 -1.0591946 -0.42392397 0.056685925 0.496377 0.62244797 0.802907 1.2510481 -0.0087666512 -0.60097408 -1.6834974 -2.6375904 -3.4365349 -5.0328369][-3.9132149 -3.3855748 -2.7398953 -2.1588073 -1.3856621 -1.1370592 -0.93173027 -0.83068419 -0.86643982 -2.3276939 -2.722558 -3.7074161 -4.6804953 -5.2034979 -6.3046083][-4.769598 -4.4703674 -4.4343467 -4.4314737 -3.8749981 -3.5049706 -3.2135377 -3.4779029 -3.3781128 -4.5411835 -4.96576 -5.3918533 -5.8932219 -6.098628 -6.6239109][-6.4210148 -6.4750423 -6.3373332 -6.2670832 -6.1149235 -5.7955432 -5.5210161 -5.123435 -4.9728374 -5.31098 -5.2072825 -5.5633621 -6.0295291 -5.6109867 -5.7177081][-6.9100885 -7.2088227 -7.2361808 -7.0911794 -6.7840462 -6.779397 -6.4206023 -6.4707656 -6.45696 -6.884223 -7.0512538 -6.6258578 -6.4904256 -5.5965948 -4.7712488][-6.7532196 -6.6659756 -7.0675535 -7.0608125 -7.4225869 -7.6650109 -7.8414092 -8.0654573 -8.050725 -7.8144064 -7.6785212 -7.372499 -7.1602793 -6.4456997 -5.6008382]]...]
INFO - root - 2017-12-15 13:33:16.764513: step 18310, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 56h:44m:51s remains)
INFO - root - 2017-12-15 13:33:23.100221: step 18320, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 54h:16m:12s remains)
INFO - root - 2017-12-15 13:33:29.481412: step 18330, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 56h:06m:57s remains)
INFO - root - 2017-12-15 13:33:35.996896: step 18340, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 58h:58m:31s remains)
INFO - root - 2017-12-15 13:33:42.456912: step 18350, loss = 0.24, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 57h:51m:27s remains)
INFO - root - 2017-12-15 13:33:48.796366: step 18360, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 54h:54m:54s remains)
INFO - root - 2017-12-15 13:33:55.119742: step 18370, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 56h:04m:43s remains)
INFO - root - 2017-12-15 13:34:01.559112: step 18380, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 56h:10m:08s remains)
INFO - root - 2017-12-15 13:34:08.099176: step 18390, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 55h:59m:06s remains)
INFO - root - 2017-12-15 13:34:14.486127: step 18400, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 58h:25m:18s remains)
2017-12-15 13:34:14.997930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3706875 -5.0260334 -6.0224552 -5.9840479 -6.0870748 -6.1116047 -6.3376579 -6.0347252 -5.1635666 -5.7336683 -5.3129339 -7.4355907 -8.1851234 -9.8521013 -10.715027][-4.565114 -5.8425083 -6.8957791 -7.3109879 -8.0229912 -7.5750465 -5.9008565 -4.7296181 -4.1817551 -6.4731178 -7.5175352 -9.9597292 -10.490642 -11.348517 -11.160658][-3.99014 -4.6575589 -5.9260225 -6.5026393 -6.2247815 -6.1598797 -5.5368414 -4.6803875 -3.1591053 -3.9862509 -5.0260162 -8.5283165 -10.534973 -11.356058 -11.28977][-4.9554687 -5.2474804 -5.2204041 -5.0680494 -4.6576548 -3.7378075 -2.4924579 -2.1489949 -1.3555222 -2.4577508 -3.394742 -6.6572328 -8.6035309 -10.547464 -10.514643][-5.200325 -4.9379773 -4.937314 -4.6372623 -3.43361 -1.2154069 0.69154119 1.4185214 1.4894862 -0.72974825 -2.1484742 -5.6234417 -8.0598011 -9.4917459 -9.8007145][-5.2378778 -4.6314363 -3.9458773 -2.6922297 -1.8238888 0.13168716 2.21948 3.7578368 4.0409379 1.6512475 -0.56417751 -4.5546808 -6.5727196 -8.0243759 -8.4503632][-5.94908 -5.0252533 -3.907439 -1.722733 0.011927128 1.2388101 2.9424529 4.5002542 4.8865581 2.7014985 0.86581182 -3.3767242 -6.3083706 -8.1957235 -8.161684][-5.3637137 -4.7096882 -3.9662118 -1.2979665 0.77355909 2.9518762 4.8832603 5.1057124 4.5895286 2.0329585 0.085772038 -3.8872087 -6.1398525 -8.13936 -8.2686777][-5.9279256 -5.17864 -4.1956449 -2.2188826 -0.47612953 1.9784427 3.9747453 4.9376283 5.005486 1.3266377 -0.54876757 -4.0810432 -6.0655417 -8.1735592 -8.4894991][-6.0321512 -5.9704995 -6.0115471 -4.6170764 -3.1824207 -0.94527721 0.69847822 1.5325427 1.8367162 -0.68245077 -2.5049472 -5.7704325 -7.046937 -8.4948406 -8.5296049][-7.136497 -7.0942936 -7.410316 -6.6860156 -5.3449068 -3.2144094 -1.8857861 -1.5465665 -1.2179003 -3.5976391 -4.9704385 -7.3666763 -8.5899115 -9.5514631 -9.3517075][-8.184145 -8.3340588 -8.1719494 -7.9295974 -7.6719975 -6.534843 -5.3717093 -4.8036728 -4.5346012 -5.4221892 -6.0153103 -7.7107635 -8.4198837 -9.4788074 -9.95072][-9.04056 -8.9758072 -9.0611362 -9.120903 -8.7015953 -8.0517807 -8.165122 -7.9278212 -7.3273954 -7.100955 -7.3321605 -8.1652136 -8.356638 -8.9173012 -8.9249487][-8.9072247 -9.3368263 -8.9539881 -8.3294716 -7.63329 -7.1786594 -6.7118769 -7.3715196 -7.5596318 -7.4151769 -7.4532971 -7.5752878 -7.2117658 -7.2551212 -7.0031638][-7.9146004 -8.9185524 -9.3694305 -9.1980238 -8.9490194 -7.9463282 -7.4463363 -7.2787867 -7.3372593 -7.88148 -8.1267233 -7.8552217 -7.3733253 -7.2475576 -6.7632275]]...]
INFO - root - 2017-12-15 13:34:21.462532: step 18410, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 54h:15m:21s remains)
INFO - root - 2017-12-15 13:34:27.855059: step 18420, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 57h:00m:45s remains)
INFO - root - 2017-12-15 13:34:34.232000: step 18430, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 55h:37m:30s remains)
INFO - root - 2017-12-15 13:34:40.670673: step 18440, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 54h:34m:15s remains)
INFO - root - 2017-12-15 13:34:47.079548: step 18450, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 56h:54m:58s remains)
INFO - root - 2017-12-15 13:34:53.518402: step 18460, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 54h:31m:11s remains)
INFO - root - 2017-12-15 13:34:59.874785: step 18470, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 57h:34m:39s remains)
INFO - root - 2017-12-15 13:35:06.267537: step 18480, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 55h:34m:23s remains)
INFO - root - 2017-12-15 13:35:12.685884: step 18490, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 54h:57m:36s remains)
INFO - root - 2017-12-15 13:35:19.129537: step 18500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 56h:01m:36s remains)
2017-12-15 13:35:19.626387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9563351 -2.5026107 -3.1671576 -4.3607874 -5.6095228 -6.66607 -7.9725776 -9.3493519 -10.58647 -12.308352 -14.261153 -14.412436 -13.460829 -12.190825 -11.350607][-0.48186255 -1.8627243 -3.5476785 -4.4165726 -5.4591331 -6.6558523 -7.7664657 -8.4269037 -8.7364435 -9.6247854 -10.910864 -12.176884 -12.91639 -12.607184 -12.75413][-1.4055591 -1.8835697 -3.0544024 -3.6223083 -4.4661922 -5.1264935 -5.7447958 -6.5484886 -7.0963039 -7.5550117 -8.49066 -9.3588228 -10.261805 -11.148642 -12.726894][-4.2415628 -4.1915684 -4.5878644 -4.0116205 -3.2186394 -2.770896 -2.3390422 -3.1006298 -3.5746746 -4.6074734 -6.4164023 -7.9667811 -8.70702 -9.4855757 -11.105499][-7.522893 -6.8733177 -5.9600616 -5.2535248 -4.6727877 -2.76411 -0.8154788 -0.47719336 -0.60992813 -2.4277654 -5.2822056 -7.9763069 -9.3484488 -10.239904 -11.651266][-8.485631 -8.3010387 -7.5095344 -5.8738103 -3.6780291 -1.38342 0.94779634 1.5192342 1.4976211 -0.63996887 -4.1989031 -7.7311664 -9.7957125 -11.214266 -11.748869][-9.2411423 -8.3540564 -6.6295261 -3.8727491 -0.90123081 1.896276 4.6754365 5.3074708 5.5262723 2.9713035 -1.4746633 -5.9126225 -8.880125 -10.596206 -11.868679][-8.7227392 -8.3494225 -6.8058314 -3.7568145 -0.06526804 2.6385045 4.9543118 5.9810786 6.456181 4.3232656 0.80719995 -3.4994426 -6.4976358 -8.4271526 -9.5300179][-9.25616 -8.7100639 -7.5556049 -5.0258665 -1.8825622 0.415483 2.7831769 3.58209 3.885426 2.5271525 -0.84546232 -4.0473042 -5.9732556 -7.6296387 -9.1242085][-9.5259428 -9.6049528 -8.7831049 -6.9147992 -4.5187826 -1.392622 1.2631621 1.9114642 2.4127975 0.37001944 -3.1136703 -6.1402965 -8.1365366 -8.780344 -9.3237762][-11.223247 -11.374447 -11.251193 -10.075053 -8.5628719 -6.3079462 -3.7491262 -1.8824296 -0.32194567 -1.0918956 -3.8866377 -6.5498295 -7.9806046 -9.03071 -9.98741][-11.434387 -11.706561 -11.494738 -10.573467 -9.6776171 -8.7362032 -7.7406583 -6.6159172 -5.11187 -4.5419931 -5.2909746 -6.2490587 -6.9880247 -7.5602708 -8.5094376][-11.70866 -11.912011 -12.188274 -11.36605 -10.584297 -9.6762009 -8.7744961 -8.6392241 -8.6083593 -8.7956057 -9.036417 -8.6702213 -8.2248926 -8.0874681 -7.5908613][-10.176075 -10.096328 -10.068649 -9.3434038 -8.9988012 -8.3008442 -7.6389236 -7.5309591 -7.4582663 -7.659205 -8.2926388 -8.52076 -8.6559982 -8.53734 -8.4266624][-10.979136 -10.186926 -9.4965191 -8.9746246 -8.46839 -8.1360054 -7.6537676 -7.7975883 -8.1330566 -8.349781 -8.2786741 -7.7194986 -7.2621455 -6.8891554 -6.5098028]]...]
INFO - root - 2017-12-15 13:35:25.911519: step 18510, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 54h:59m:56s remains)
INFO - root - 2017-12-15 13:35:32.238929: step 18520, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 55h:38m:59s remains)
INFO - root - 2017-12-15 13:35:38.658013: step 18530, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 56h:40m:47s remains)
INFO - root - 2017-12-15 13:35:45.027180: step 18540, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 54h:53m:30s remains)
INFO - root - 2017-12-15 13:35:51.429283: step 18550, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 55h:55m:16s remains)
INFO - root - 2017-12-15 13:35:57.929221: step 18560, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 56h:16m:24s remains)
INFO - root - 2017-12-15 13:36:04.344757: step 18570, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 57h:42m:57s remains)
INFO - root - 2017-12-15 13:36:10.723161: step 18580, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 54h:59m:34s remains)
INFO - root - 2017-12-15 13:36:17.134931: step 18590, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 56h:31m:30s remains)
INFO - root - 2017-12-15 13:36:23.560269: step 18600, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 56h:46m:24s remains)
2017-12-15 13:36:24.072864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5197368 -3.5235662 -3.2412329 -3.1986132 -2.9980583 -2.4760289 -2.6303229 -2.9521842 -3.1411204 -4.4190469 -5.1908808 -6.28158 -6.655427 -7.3809834 -7.2473097][-3.2245173 -3.2413893 -2.7861867 -2.6305661 -1.9613371 -1.6319957 -1.5437708 -1.9566188 -3.24994 -4.9361739 -5.8271313 -6.9122086 -7.3860717 -8.2714787 -8.2290726][-3.5860739 -3.5203681 -3.203691 -2.8731332 -2.4749155 -1.6102848 -1.0186353 -1.313735 -2.0218844 -3.8213408 -5.46439 -6.7104492 -7.0613518 -8.29128 -7.8069868][-4.7860703 -4.1333351 -3.4452848 -2.8023529 -2.0564055 -0.85627174 -0.21452713 0.015894413 -0.801188 -2.8377848 -4.1518731 -5.8514438 -6.5638838 -7.8430734 -7.4547768][-4.9118629 -4.1384182 -3.1587029 -2.1263881 -1.0601401 0.071362019 1.4224086 2.0887237 1.6727252 -0.45256472 -2.2575016 -4.3440838 -5.3913054 -7.3564425 -7.3287168][-4.4300375 -4.277976 -3.5324869 -2.2129545 -0.88941336 0.63491869 1.6475472 2.1771045 2.1547961 0.19408464 -0.49972486 -1.8989778 -2.8748631 -5.1565256 -6.31172][-4.0941076 -2.8743086 -1.8023405 -0.51038265 1.0774817 2.1549506 3.2606378 3.5462604 2.8529477 0.8198638 -0.14368677 -1.3092918 -1.5144668 -3.1902223 -4.0680275][-3.4395132 -2.5180755 -1.1498928 1.1544099 3.2269816 4.3039594 5.0539737 4.8924737 4.4363933 2.2280011 0.89322519 -0.49172115 -1.337399 -2.9305563 -3.461338][-4.666595 -3.6212268 -2.5871625 -0.67755508 1.258368 3.285398 4.5234256 4.5005469 4.2324567 2.0181413 0.68862772 -0.60592079 -1.1591682 -3.0947661 -4.0629754][-7.6300836 -6.3388009 -4.9587841 -2.916935 -1.4742904 0.23343134 1.7039733 2.4198308 2.7740741 0.53236628 -0.83131504 -1.8736792 -2.6560688 -3.7904973 -4.0213513][-9.9697371 -9.1032085 -7.9039907 -6.1658397 -4.9418507 -3.242281 -1.9551497 -1.2211704 -0.69317055 -2.2962661 -3.4293694 -4.5294085 -5.1410666 -5.9834366 -6.564003][-11.047082 -10.425043 -9.5593548 -8.325633 -7.1778879 -5.5871878 -4.5731554 -4.113431 -3.6496277 -4.7727032 -5.2544413 -5.9102726 -6.4121127 -7.209712 -7.7980161][-11.397161 -10.898258 -10.572226 -9.5276661 -8.6984262 -7.5464253 -6.47625 -6.0843148 -5.80764 -6.9030366 -7.2077665 -7.3536167 -6.925313 -6.9517484 -6.7265687][-10.957323 -10.654095 -10.598507 -9.9412374 -9.5173092 -8.298028 -7.1487064 -7.2662129 -7.1731215 -7.5534048 -7.5729895 -7.5173206 -7.0602274 -6.8459139 -6.4397988][-10.357048 -10.33893 -10.395039 -9.8418932 -9.3894176 -8.4296494 -7.4274611 -6.8925142 -6.4945188 -6.6330037 -6.7868261 -7.1339889 -6.932776 -6.4901123 -5.7044115]]...]
INFO - root - 2017-12-15 13:36:30.472064: step 18610, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 56h:41m:44s remains)
INFO - root - 2017-12-15 13:36:36.872372: step 18620, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 55h:46m:04s remains)
INFO - root - 2017-12-15 13:36:43.389893: step 18630, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 56h:04m:53s remains)
INFO - root - 2017-12-15 13:36:49.793905: step 18640, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 53h:48m:39s remains)
INFO - root - 2017-12-15 13:36:56.239933: step 18650, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.633 sec/batch; 55h:11m:43s remains)
INFO - root - 2017-12-15 13:37:02.658791: step 18660, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 57h:59m:48s remains)
INFO - root - 2017-12-15 13:37:09.040004: step 18670, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 55h:01m:13s remains)
INFO - root - 2017-12-15 13:37:15.413120: step 18680, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.630 sec/batch; 54h:54m:38s remains)
INFO - root - 2017-12-15 13:37:21.741105: step 18690, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:10m:35s remains)
INFO - root - 2017-12-15 13:37:28.172142: step 18700, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 56h:46m:53s remains)
2017-12-15 13:37:28.684540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8599153 -2.9165077 -3.6270127 -4.2588482 -4.1717062 -3.66508 -3.5322037 -3.5124936 -3.4325423 -4.1467648 -4.8677635 -6.0838704 -6.8431325 -7.8555174 -8.40278][-2.5333862 -1.7918959 -2.716064 -3.852581 -4.6409717 -4.0286913 -3.6526713 -3.6729708 -3.4981637 -4.0166988 -4.659121 -5.963223 -6.5455661 -7.6833682 -8.0361872][-2.0147972 -1.6709914 -2.0083222 -2.634985 -3.8674872 -3.9899139 -3.7701621 -3.4077444 -3.3258619 -4.0143857 -4.9853716 -5.9928756 -6.4983635 -7.3102627 -7.9166474][-1.1299539 -0.22604704 -0.90608025 -2.0171614 -2.4830837 -2.0374408 -1.7812748 -1.6480641 -1.6726127 -3.0990787 -4.4240522 -5.9229307 -6.328125 -6.5384383 -6.6147742][-1.8169117 -0.63423777 -0.65011263 -1.0744486 -1.8760443 -1.1879764 0.049757481 0.24106216 -0.14203358 -1.8786917 -3.5724711 -6.0114908 -6.750021 -7.3827529 -7.2566075][-3.5566988 -2.3476715 -1.6138363 -1.6248255 -1.8579769 -0.63947868 1.3722329 2.3864779 2.2604585 -0.171 -2.5437737 -5.2303581 -6.7450786 -7.7642765 -7.7982745][-2.9551654 -1.8670874 -1.2782192 -0.79298019 -0.27513552 1.2189975 3.1568294 4.0435653 4.4065356 2.1580358 -0.74200821 -4.3239889 -5.8088188 -7.0376625 -7.0733504][-3.1237679 -1.8615484 -0.94211674 0.011209488 0.26021671 1.8701491 4.1742644 5.0517 5.0196214 2.9979692 0.097962379 -3.391222 -5.2489843 -6.4967451 -6.8641396][-4.4228582 -3.3764548 -2.1513481 -1.2711902 -0.79143286 -0.070869446 1.6849618 3.571281 4.1595931 2.1937814 -0.88862419 -3.9915974 -5.620719 -6.8765111 -6.8594451][-4.9924703 -3.8035967 -2.94847 -1.9713674 -1.5229669 -0.79783297 0.16113234 1.0821042 2.3075318 0.62697172 -2.0173774 -4.5586052 -6.5604572 -7.7116485 -8.0035067][-6.3822393 -6.1284971 -5.0925608 -3.9344897 -2.9530063 -2.363306 -2.0070519 -0.88578367 -0.17965889 -1.7734795 -3.3754792 -5.931242 -6.8604741 -7.7895947 -8.5560226][-7.8632159 -7.677876 -7.2065139 -6.2427258 -4.9426613 -4.2423706 -3.7731073 -3.4860005 -3.01548 -3.4376783 -4.6145353 -5.9142413 -6.9196777 -8.0075226 -8.4419241][-9.5065107 -9.0034809 -9.00001 -7.9274063 -6.9318357 -5.6312151 -5.2293382 -4.9204407 -4.8524251 -5.9393353 -6.3327894 -7.015029 -7.1466837 -7.7549787 -8.1074381][-9.7032948 -10.196955 -9.5845289 -8.8844576 -8.1414165 -6.7329087 -6.0440369 -6.2622609 -6.6158986 -6.803762 -7.4065652 -7.5540562 -7.0414333 -6.92325 -6.819911][-9.5848913 -9.6646843 -9.5618849 -8.6937065 -7.9201412 -7.0818624 -6.6613173 -6.7456532 -7.0693569 -7.6411505 -7.878449 -7.4139876 -6.7926855 -6.2045817 -5.7121258]]...]
INFO - root - 2017-12-15 13:37:35.206883: step 18710, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 56h:11m:34s remains)
INFO - root - 2017-12-15 13:37:41.676110: step 18720, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 56h:32m:53s remains)
INFO - root - 2017-12-15 13:37:48.057665: step 18730, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.619 sec/batch; 53h:57m:58s remains)
INFO - root - 2017-12-15 13:37:54.417592: step 18740, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 55h:20m:25s remains)
INFO - root - 2017-12-15 13:38:00.787909: step 18750, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:54m:01s remains)
INFO - root - 2017-12-15 13:38:07.201431: step 18760, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 54h:50m:06s remains)
INFO - root - 2017-12-15 13:38:13.562767: step 18770, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 55h:03m:54s remains)
INFO - root - 2017-12-15 13:38:19.888975: step 18780, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 54h:10m:57s remains)
INFO - root - 2017-12-15 13:38:26.251633: step 18790, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 57h:42m:07s remains)
INFO - root - 2017-12-15 13:38:32.627316: step 18800, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 54h:07m:06s remains)
2017-12-15 13:38:33.112104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2537837 -2.9580355 -2.466836 -2.5422735 -3.0666208 -3.4689007 -4.0076838 -4.2525778 -3.9065905 -6.0648847 -6.6817765 -7.3220172 -8.25358 -9.3332663 -9.1123133][-2.6771474 -2.6535497 -2.579906 -2.6704307 -2.9366274 -3.0943093 -3.2366352 -3.4517236 -3.3902755 -5.8378544 -6.6525979 -7.431931 -8.4833832 -9.7784443 -9.711585][-3.4609284 -3.3004866 -3.2125816 -3.1605701 -3.1522889 -2.735909 -2.3538017 -2.3133683 -1.9498887 -4.0794268 -5.0763655 -5.9392204 -6.94747 -8.2738581 -8.4591513][-3.4900584 -3.5218306 -3.540864 -3.183579 -2.715086 -1.9953961 -1.6188126 -1.7698483 -1.5241084 -3.6527791 -4.460722 -5.5651474 -6.8537512 -7.6946921 -7.6997018][-4.7945232 -3.6588392 -2.4064217 -1.7894368 -0.90830183 -0.12788916 0.097715855 -0.050269127 -0.11348248 -2.3107872 -3.23938 -4.6777878 -6.1218147 -7.10649 -7.0885367][-4.2528763 -3.7467959 -2.3637967 -0.89642048 0.4655695 1.6712685 2.2038898 1.7819643 1.1390934 -1.3552685 -2.3499498 -3.6826849 -5.032866 -6.2682753 -6.4019966][-3.6642461 -3.0079508 -1.3756046 0.3466177 1.7161188 2.7052164 2.9644814 2.5231142 1.8775091 -1.0103707 -2.3403082 -3.7802377 -5.09835 -6.3724155 -6.7978516][-2.506937 -1.4932418 0.19790983 1.87679 2.7715635 3.2702694 3.268795 2.8273592 2.0996189 -0.79290104 -2.2469416 -3.8903637 -5.5962944 -7.0499187 -7.1619139][-1.55545 -0.7921567 0.77783871 2.1493692 2.7220545 2.6363859 2.2344742 1.7003336 1.0386248 -1.5540166 -2.7301426 -4.3035035 -6.0297408 -7.5343704 -7.885951][-1.7716684 -1.1310978 0.21664 1.3482256 1.689292 1.569685 1.1628199 0.65570354 0.19778776 -2.3663292 -3.4189696 -4.7555704 -6.2958131 -7.7361917 -7.7213516][-3.2509036 -2.7523494 -1.8072157 -0.94827747 -0.60292864 -0.5411005 -0.82673788 -1.0475955 -1.1373925 -3.383615 -4.6094456 -5.4933252 -6.8005333 -7.9956708 -8.1243258][-4.1906013 -3.7421753 -3.1454964 -2.3567677 -2.1940041 -2.2370119 -2.550457 -2.6470089 -2.5086136 -4.2147641 -5.5531774 -5.9418297 -6.8215342 -7.55681 -7.4920731][-5.1545119 -5.0164824 -5.1111197 -4.4198647 -3.8465664 -3.5509248 -3.4579277 -3.7529047 -3.7362151 -4.7506056 -5.6452379 -6.0388513 -6.6281633 -6.917912 -6.5758533][-5.5562224 -5.970633 -5.9124703 -5.2631712 -4.5902157 -4.1729913 -4.0182705 -4.0941706 -3.9562221 -4.0688047 -4.6276569 -5.203382 -5.6711149 -6.1255732 -6.2813854][-6.5726309 -6.3243346 -5.9141836 -5.6600037 -5.560029 -5.2616849 -4.9384623 -5.2385826 -5.4690208 -5.0019732 -5.0265932 -5.288723 -5.4782057 -5.561357 -5.6907954]]...]
INFO - root - 2017-12-15 13:38:39.594790: step 18810, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 56h:27m:41s remains)
INFO - root - 2017-12-15 13:38:45.984571: step 18820, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 54h:48m:44s remains)
INFO - root - 2017-12-15 13:38:52.314665: step 18830, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 54h:36m:11s remains)
INFO - root - 2017-12-15 13:38:58.688278: step 18840, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 54h:16m:56s remains)
INFO - root - 2017-12-15 13:39:05.096127: step 18850, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 56h:26m:30s remains)
INFO - root - 2017-12-15 13:39:11.482158: step 18860, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.632 sec/batch; 55h:02m:24s remains)
INFO - root - 2017-12-15 13:39:17.859202: step 18870, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 54h:45m:53s remains)
INFO - root - 2017-12-15 13:39:24.223394: step 18880, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 55h:34m:35s remains)
INFO - root - 2017-12-15 13:39:30.616560: step 18890, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 54h:50m:10s remains)
INFO - root - 2017-12-15 13:39:36.986867: step 18900, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 58h:51m:32s remains)
2017-12-15 13:39:37.490579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6343422 -5.9935408 -7.4425473 -8.2645159 -8.8997383 -9.2217226 -8.9121246 -8.0428848 -6.4291763 -6.3665829 -5.302516 -7.043047 -7.8939209 -7.7974796 -7.7929192][-4.6511822 -5.7098217 -7.3168344 -8.8739395 -10.146501 -10.463393 -10.531029 -9.45194 -7.7871785 -8.0547523 -7.23791 -8.7528934 -9.8232288 -8.9482355 -8.3852463][-4.4255972 -5.2520761 -6.7114544 -7.8547873 -8.8748159 -9.505147 -9.63933 -9.0191441 -7.9060445 -8.2224855 -7.6202717 -9.734931 -10.518906 -10.351471 -10.388084][-6.0074978 -5.7532105 -6.3737073 -7.0030041 -7.4071126 -6.9058518 -6.2991967 -5.8414226 -5.3719006 -6.9375668 -7.2452207 -10.071714 -11.204058 -10.56782 -9.7154417][-6.3858061 -6.1167727 -6.4914246 -5.5271311 -4.8219795 -3.2388735 -2.0429258 -1.8395038 -1.7024069 -3.9301569 -4.8708472 -8.3528271 -10.179358 -10.173439 -9.9341192][-8.379344 -7.4195495 -6.6386871 -4.6131506 -3.0375333 -0.5677247 1.4477448 2.261004 2.6957192 -0.43512964 -2.6922574 -6.718596 -9.166297 -9.2198963 -8.949439][-8.3507509 -7.6911697 -6.3553648 -3.0957837 -0.22873116 2.3253951 4.6884456 5.60469 5.9117217 2.4967408 0.10591221 -4.0724363 -6.8160715 -7.2485743 -7.4140949][-7.9328151 -6.8446493 -5.1070175 -2.0658522 0.94407034 3.8713527 6.1593032 6.7990108 7.0109963 3.1730027 0.75016642 -3.5827117 -6.0499392 -6.3244505 -6.61486][-8.5115223 -8.1730165 -7.2758374 -4.2924547 -1.345582 1.5319104 3.4940095 4.2873406 4.500051 1.1146626 -1.0376334 -5.060751 -6.9391351 -6.7812171 -6.2838717][-10.058826 -9.625391 -8.3827019 -6.2610435 -4.3702216 -2.2067442 -0.78693533 0.40897703 0.60612917 -2.4672813 -4.1041374 -7.24954 -8.7338934 -8.3532324 -7.6427274][-11.905054 -11.763979 -11.117342 -8.9209557 -7.0378919 -5.3494873 -3.8548372 -3.2508287 -3.153286 -5.3981209 -6.37614 -8.4108334 -9.3840542 -9.4002657 -8.9840908][-11.511238 -11.4813 -11.03076 -10.173037 -9.0271215 -7.4545679 -6.4352994 -5.8097944 -5.0613108 -6.6002121 -7.4752536 -8.926506 -9.4872417 -9.2433167 -9.0318146][-11.16628 -11.285307 -11.192217 -9.9422522 -8.7127314 -8.01138 -7.4027925 -6.8633556 -6.6168151 -7.3493137 -7.3643775 -8.3588324 -8.9330235 -8.4093542 -8.31104][-9.171217 -9.5837393 -9.51716 -8.4763279 -7.6005821 -6.4324951 -5.8093452 -6.2967081 -6.4099607 -7.6964555 -8.0792122 -8.4425106 -8.75178 -8.5154018 -8.1214495][-8.8363333 -9.0622406 -9.1383858 -8.3168316 -7.732439 -6.8409753 -6.3031826 -6.5397539 -6.8570137 -7.6140981 -7.6823311 -7.9992814 -7.6364222 -7.615633 -7.3978314]]...]
INFO - root - 2017-12-15 13:39:43.821934: step 18910, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 55h:43m:54s remains)
INFO - root - 2017-12-15 13:39:50.185685: step 18920, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 55h:28m:33s remains)
INFO - root - 2017-12-15 13:39:56.523782: step 18930, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 56h:26m:58s remains)
INFO - root - 2017-12-15 13:40:02.963165: step 18940, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 57h:06m:32s remains)
INFO - root - 2017-12-15 13:40:09.356124: step 18950, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 56h:18m:52s remains)
INFO - root - 2017-12-15 13:40:15.792439: step 18960, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.666 sec/batch; 57h:58m:14s remains)
INFO - root - 2017-12-15 13:40:22.126166: step 18970, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 54h:11m:39s remains)
INFO - root - 2017-12-15 13:40:28.535665: step 18980, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 55h:50m:02s remains)
INFO - root - 2017-12-15 13:40:34.865416: step 18990, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 55h:48m:54s remains)
INFO - root - 2017-12-15 13:40:41.301214: step 19000, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 53h:48m:31s remains)
2017-12-15 13:40:41.810584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.213583 -1.7531567 -2.12223 -2.5751424 -3.2188711 -3.8813958 -4.4470463 -4.7526636 -4.9002657 -6.1214762 -6.6328712 -7.1339312 -7.5985579 -8.0421953 -8.4983368][-3.6509757 -3.4622478 -3.6591549 -3.8081007 -4.1742764 -4.5098944 -4.751327 -5.1433229 -5.3677068 -6.7925663 -7.2678819 -7.7793097 -8.4783783 -8.9396791 -9.4506311][-3.5583506 -3.168179 -3.1268563 -3.0236502 -3.1920266 -3.5238881 -3.6704564 -4.0111904 -4.1348548 -5.5543308 -6.0106945 -6.6569118 -7.1858606 -7.7170076 -8.3977985][-3.5036287 -2.9858193 -2.9170661 -2.7344141 -2.568614 -2.2529202 -1.8718562 -2.0447664 -2.1945896 -3.800705 -4.5382352 -5.2543316 -6.3022208 -7.1007023 -7.68168][-3.6182084 -3.0644031 -2.607357 -2.1741881 -1.8265448 -1.2977309 -0.89391947 -0.77210093 -0.65568686 -2.20648 -2.8654814 -3.641943 -4.6706171 -5.8262596 -6.9837952][-3.8820252 -2.9816818 -2.2852011 -1.7817307 -1.272831 -0.64702606 -0.044721127 0.068348885 0.26432657 -1.2278996 -1.9710422 -2.5973835 -3.6555376 -4.9111128 -6.1642227][-3.418901 -2.9118643 -2.170332 -1.1122446 -0.33168602 0.0721302 0.50394249 0.77293491 1.1241512 -0.44502115 -1.1839519 -1.8735008 -2.9890742 -4.2466688 -5.5264044][-2.9802384 -2.5661955 -1.7214608 -0.61219311 0.30026102 0.64595604 0.88269711 1.1640072 1.4923344 0.051164627 -0.74090385 -1.4878445 -2.6897545 -3.9857659 -4.8993587][-2.5149951 -2.0770116 -1.472949 -0.36116457 0.32268715 0.42262459 0.37424755 0.620841 1.0086117 -0.36229849 -1.1842651 -2.1100149 -3.5316072 -4.7932138 -5.6345615][-3.15071 -2.4723282 -1.7311697 -0.79046249 0.12519646 0.13627911 -0.15487099 -0.014963627 0.30195522 -1.2910142 -2.3746552 -3.2505126 -4.3977141 -5.1103196 -5.8642077][-4.693181 -3.8542221 -3.2810879 -2.6037617 -1.9318595 -1.6655231 -1.75211 -1.8653212 -1.7353745 -3.3652821 -4.4283261 -5.0826035 -5.707819 -6.3096962 -6.7731256][-4.9457126 -4.2077813 -3.8700647 -3.7742019 -3.4215965 -3.11263 -3.3733783 -3.4167557 -3.3591642 -4.5717764 -5.5187912 -5.9322863 -6.4221416 -7.0840735 -7.5498261][-5.4798212 -5.18573 -4.8587713 -4.649991 -4.4251671 -4.2450523 -4.5478868 -4.6375365 -4.6791391 -5.6203852 -6.3631287 -6.4624944 -6.5475049 -6.8645406 -7.0282788][-5.9638853 -5.3676882 -4.9843812 -4.9099703 -4.6261177 -4.5349245 -4.781918 -5.077405 -5.2606769 -5.9911752 -6.3296957 -6.345521 -6.315052 -6.3963442 -6.4092655][-6.8990149 -6.7004943 -6.5602546 -6.2907228 -6.0704384 -5.8990927 -5.9207439 -6.0966778 -6.2259583 -6.4748216 -6.6187544 -6.8027716 -6.9515924 -6.7884197 -6.4248967]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 13:40:48.214016: step 19010, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 55h:43m:10s remains)
INFO - root - 2017-12-15 13:40:54.644748: step 19020, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 55h:13m:53s remains)
INFO - root - 2017-12-15 13:41:01.084191: step 19030, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 56h:01m:33s remains)
INFO - root - 2017-12-15 13:41:07.574081: step 19040, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 57h:40m:16s remains)
INFO - root - 2017-12-15 13:41:13.966987: step 19050, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 56h:43m:11s remains)
INFO - root - 2017-12-15 13:41:20.328604: step 19060, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 56h:38m:56s remains)
INFO - root - 2017-12-15 13:41:26.613358: step 19070, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 54h:26m:17s remains)
INFO - root - 2017-12-15 13:41:33.027546: step 19080, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 54h:55m:07s remains)
INFO - root - 2017-12-15 13:41:39.429882: step 19090, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 57h:24m:00s remains)
INFO - root - 2017-12-15 13:41:45.798572: step 19100, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 56h:47m:10s remains)
2017-12-15 13:41:46.318594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1284933 -4.4177094 -4.7859139 -4.9297371 -4.67494 -4.277832 -3.2970719 -1.7983046 -0.2701602 -0.64408493 -1.3244748 -2.1209464 -3.0833187 -3.597609 -4.6522355][-4.5193596 -4.3917093 -4.5113153 -4.702805 -4.7616158 -4.7924881 -4.3278847 -3.4421916 -2.2496963 -2.6299834 -3.3306327 -3.9526179 -4.3639755 -4.143898 -4.8024378][-5.8713703 -5.5341883 -5.1013126 -4.6927719 -4.38659 -4.3991904 -4.071733 -3.6947749 -3.1689897 -4.2586241 -5.0734711 -5.5992427 -6.0482435 -5.9771409 -6.2665591][-5.811533 -4.8583994 -4.0934219 -3.1944571 -2.4653215 -2.02596 -1.6685734 -1.6293602 -1.4821124 -2.966742 -4.2746196 -5.4943743 -6.6786923 -6.9861789 -7.3668652][-6.6021709 -4.7287579 -2.9614568 -1.5799694 -0.59320593 0.32990742 0.78010464 0.73130322 0.85325146 -0.85683537 -2.5598879 -4.6838179 -6.21902 -6.9609742 -7.58792][-5.9562125 -4.5032578 -2.8022518 -0.92522955 0.58621025 2.2082891 3.0881739 2.7289 2.6848516 0.6255188 -1.5404434 -3.6145191 -5.4578285 -6.4100914 -7.1856155][-5.5885763 -3.6561227 -1.3320684 0.9344759 2.6266193 4.2124519 5.2408266 5.3928432 5.712348 3.25813 0.5621357 -2.323669 -4.5299191 -5.5267892 -6.4756465][-4.3603764 -2.3441815 -0.524076 1.8514433 3.704875 5.1797428 6.2010612 6.4995546 6.8301878 4.4808693 1.935153 -0.95561981 -3.1579709 -4.2539191 -5.2941418][-5.0337772 -3.2213616 -1.7046709 0.50034046 2.2411985 3.7996874 4.704752 4.8352938 5.3027763 3.1448946 0.97206211 -1.4064727 -3.3909669 -4.1219311 -5.1213923][-6.0139456 -4.759408 -3.5490589 -1.7640438 0.086787224 1.5383873 1.9700403 2.5388031 2.9231539 0.48548603 -1.4604754 -3.1064434 -4.6032171 -4.878777 -5.5933185][-7.7060981 -6.9494562 -5.9012108 -4.1910305 -2.7518349 -1.8089771 -1.0580244 -0.53327227 -0.66426563 -2.3689079 -3.699544 -4.8589764 -5.9313769 -6.2847056 -6.5140581][-9.3059444 -8.5127726 -7.5823474 -6.4398994 -5.3613825 -4.6568966 -3.9490583 -3.6499796 -3.196744 -4.2214861 -5.8671536 -6.2132587 -6.3328848 -6.416028 -6.4261661][-9.5097036 -9.481555 -9.0682468 -8.09333 -7.2030196 -6.686286 -6.0985 -6.1088166 -6.001833 -6.3975034 -7.1195254 -6.9328361 -6.9777522 -6.6008339 -6.0528603][-9.5072823 -9.4533854 -8.6681824 -7.956974 -7.4477692 -7.0280123 -6.5750151 -6.7484097 -7.1195087 -7.4996848 -7.8616066 -7.3250918 -6.9769478 -6.711215 -6.5501552][-9.3266954 -8.9960632 -8.4186678 -7.9970326 -7.6081266 -7.2573371 -6.9514017 -7.0640483 -7.2544479 -7.5846138 -7.8914289 -8.1473188 -8.1469316 -7.644742 -7.075819]]...]
INFO - root - 2017-12-15 13:41:52.602065: step 19110, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 55h:06m:25s remains)
INFO - root - 2017-12-15 13:41:59.039212: step 19120, loss = 0.32, batch loss = 0.21 (12.9 examples/sec; 0.621 sec/batch; 54h:05m:37s remains)
INFO - root - 2017-12-15 13:42:05.543452: step 19130, loss = 0.33, batch loss = 0.21 (13.1 examples/sec; 0.613 sec/batch; 53h:19m:25s remains)
INFO - root - 2017-12-15 13:42:11.918814: step 19140, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 55h:39m:05s remains)
INFO - root - 2017-12-15 13:42:18.359054: step 19150, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 56h:33m:27s remains)
INFO - root - 2017-12-15 13:42:24.745137: step 19160, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 54h:26m:47s remains)
INFO - root - 2017-12-15 13:42:31.188210: step 19170, loss = 0.22, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 55h:28m:39s remains)
INFO - root - 2017-12-15 13:42:37.578058: step 19180, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 54h:59m:31s remains)
INFO - root - 2017-12-15 13:42:43.885555: step 19190, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 54h:59m:34s remains)
INFO - root - 2017-12-15 13:42:50.249833: step 19200, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 55h:02m:52s remains)
2017-12-15 13:42:50.795568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.461381 -4.8983955 -4.4328008 -3.8357713 -3.5925264 -3.0495253 -2.2750435 -1.3841815 -0.81259775 -1.878118 -2.132628 -3.7338719 -5.1687145 -6.1010504 -6.8864088][-3.7622166 -4.0271416 -4.2080269 -4.2482271 -4.3309188 -3.3256192 -2.7020512 -1.9584951 -1.1401763 -2.4114161 -3.0462508 -4.3889852 -5.50865 -6.3160768 -7.1239886][-2.9808674 -2.8298783 -3.1478896 -3.1872339 -3.427619 -3.1379571 -2.4200988 -1.7536073 -1.4927835 -2.8770876 -3.5258422 -4.7801046 -5.8426166 -6.6608782 -7.3599749][-3.1977797 -2.8820643 -2.8360009 -2.7783756 -2.7492061 -2.2485495 -1.7618942 -1.4538269 -1.1410828 -2.3609533 -3.074461 -4.7235279 -5.6666274 -6.5087309 -7.2723541][-2.7552314 -2.6912637 -2.2206416 -1.7400937 -1.7748799 -1.0379539 -0.28547812 -0.15203285 0.14889193 -1.2761483 -2.0744381 -3.7699246 -5.1265678 -5.9898596 -7.0191197][-2.450325 -1.8093371 -1.497735 -0.3838191 0.77954865 1.7807989 2.5513554 2.2538128 1.9390144 0.28960085 -0.80665541 -2.70149 -4.177742 -5.4834247 -6.7887483][-3.593071 -2.5495305 -1.3101478 0.0046234131 1.1586399 2.2906694 3.6587439 3.8421717 3.7315044 1.992094 0.70172596 -1.5043836 -3.3123808 -4.4349451 -5.6729107][-3.2489996 -2.4814091 -1.4293814 0.60520554 2.5954103 3.8863773 4.7256956 4.9304771 5.4753714 3.7520294 2.3736115 0.033269882 -2.0309687 -3.3794947 -4.8083344][-3.1161075 -2.6112361 -1.7800312 0.071115017 1.717207 3.2658739 4.632987 5.0153227 5.2489967 3.6883097 2.7690258 0.75382423 -1.1502757 -2.6604939 -4.0356035][-4.6639881 -3.242116 -1.7419972 -0.16827202 1.0815201 1.9691229 2.6459684 3.0011854 3.5554161 2.2317982 1.6654291 0.16701841 -1.4493475 -2.2758574 -3.6992161][-5.1089287 -4.3148465 -3.3645573 -1.4271622 -0.55530643 0.34161472 0.87992382 1.0778522 1.6956978 0.50323677 -0.23070383 -1.4622035 -2.790164 -3.1658311 -3.8883822][-5.502583 -4.6137981 -3.945941 -2.7836318 -2.2730498 -1.8493891 -1.5644941 -1.3926315 -1.1315184 -1.7135739 -2.3279862 -3.3132415 -4.291049 -4.627758 -4.950284][-7.0584941 -5.9087353 -5.3021321 -4.57071 -4.0787697 -3.9048548 -3.4650126 -3.1693754 -2.757246 -3.5475707 -4.1406412 -4.4997931 -5.0832691 -5.2390041 -5.4068928][-7.9014611 -7.2429204 -6.9430971 -6.6988769 -6.8814392 -6.8229585 -6.4915786 -6.1451068 -5.66743 -5.5545988 -5.2465506 -5.3144884 -5.4802933 -5.4143734 -5.4775486][-8.5666943 -8.5242491 -9.0259886 -9.0211143 -8.9503431 -8.8633652 -8.6800814 -8.5505447 -7.8286538 -7.0179849 -6.6212282 -6.0332985 -5.65533 -5.6681013 -5.5425997]]...]
INFO - root - 2017-12-15 13:42:57.189057: step 19210, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 55h:33m:39s remains)
INFO - root - 2017-12-15 13:43:03.579631: step 19220, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 55h:12m:43s remains)
INFO - root - 2017-12-15 13:43:09.979360: step 19230, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:37m:06s remains)
INFO - root - 2017-12-15 13:43:16.307085: step 19240, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 55h:29m:29s remains)
INFO - root - 2017-12-15 13:43:22.635688: step 19250, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 55h:50m:57s remains)
INFO - root - 2017-12-15 13:43:28.996325: step 19260, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 55h:56m:07s remains)
INFO - root - 2017-12-15 13:43:35.361463: step 19270, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 57h:10m:31s remains)
INFO - root - 2017-12-15 13:43:41.754125: step 19280, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 55h:30m:25s remains)
INFO - root - 2017-12-15 13:43:48.291110: step 19290, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.664 sec/batch; 57h:47m:32s remains)
INFO - root - 2017-12-15 13:43:54.719205: step 19300, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 55h:12m:39s remains)
2017-12-15 13:43:55.237839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3423233 -4.1229582 -4.4578896 -4.9537973 -5.1997232 -4.9207859 -4.2561154 -3.623704 -2.8573165 -4.0048714 -5.2455935 -6.9151006 -8.4883833 -9.72637 -10.228021][-5.2259631 -5.2661204 -5.7029438 -6.6753922 -7.1106782 -6.926856 -6.1307445 -5.3747239 -4.9137511 -5.4171324 -6.4756231 -7.8302174 -8.8544064 -10.089678 -10.763995][-5.7457414 -4.7609873 -4.6079731 -4.7919741 -5.103303 -5.1134539 -4.6762123 -4.1624031 -3.7154377 -4.9427729 -5.9202046 -6.9742842 -8.258811 -9.23364 -9.5718822][-5.3287354 -4.1939306 -3.3011708 -3.0207162 -3.2806854 -2.9356761 -2.2594666 -2.2841053 -2.1891055 -3.2871909 -4.2560644 -5.1051464 -6.2607026 -7.0405931 -7.3102374][-4.637579 -3.0547428 -1.9138455 -1.805078 -1.4691057 -0.61278629 0.025870323 0.15979958 0.23215485 -0.99750805 -2.0320144 -3.025259 -4.3183765 -5.2667055 -5.6163211][-3.9155855 -2.740654 -1.9765587 -1.1575632 -0.45944548 0.68711042 1.7269149 1.7422099 1.6466517 0.26603889 -0.95242405 -2.1427417 -3.5098867 -4.7619991 -5.4461918][-4.4891768 -3.5302515 -2.3615627 -1.0964785 0.42847395 1.8351808 2.8441377 3.1066604 3.0258269 1.403646 -0.17310238 -1.9534254 -3.5982752 -4.697298 -5.2438412][-4.7447395 -3.773402 -2.6828218 -0.88542986 1.0454288 2.7886968 4.1081643 3.8669133 3.497314 1.7300305 -0.12223339 -1.9240508 -3.4213319 -4.5695152 -5.3200765][-5.2303686 -4.5309796 -3.2766995 -1.7986155 0.19539547 1.8752122 2.8720307 3.0095229 2.616302 0.76387262 -0.75643349 -2.4742479 -3.7920094 -4.8576193 -5.0742044][-5.86992 -5.2256131 -4.5078869 -3.2483153 -1.741415 -0.54716015 0.602962 1.0124917 1.0711884 -1.0244021 -2.8401623 -3.9674554 -5.0145254 -5.9378934 -6.1377225][-7.0512404 -6.8055725 -5.8333836 -5.0204153 -3.9354069 -2.7806005 -1.9134097 -1.6109047 -1.4412804 -3.0777407 -4.71575 -5.62694 -6.3535609 -6.821063 -6.9862761][-8.0443554 -6.9691176 -6.077796 -5.0246534 -3.8173926 -2.644218 -1.999804 -2.1838541 -2.0817389 -3.7946911 -5.5272665 -6.2321582 -7.3289 -7.8528724 -8.0980892][-7.5832253 -6.5134406 -5.7999663 -4.9188128 -4.1110592 -3.1280355 -2.7258191 -2.9056034 -2.970397 -4.3268385 -5.8179979 -6.5768294 -7.3584461 -7.9457269 -8.3786755][-5.3063622 -4.1728864 -3.6936479 -3.1452656 -2.8096132 -1.8635263 -1.4066219 -1.4236174 -1.8457766 -3.1195011 -4.2007732 -5.563127 -6.853447 -7.4654679 -7.5962477][-5.1251945 -4.0226364 -3.3290324 -2.8276682 -2.3798165 -1.6765914 -0.95852947 -1.1419249 -1.8423958 -2.7528791 -4.0314569 -5.1889858 -5.9280481 -6.5983391 -7.113636]]...]
INFO - root - 2017-12-15 13:44:01.648181: step 19310, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 54h:57m:37s remains)
INFO - root - 2017-12-15 13:44:08.083044: step 19320, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 54h:35m:53s remains)
INFO - root - 2017-12-15 13:44:14.407907: step 19330, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 54h:31m:52s remains)
INFO - root - 2017-12-15 13:44:20.781233: step 19340, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 55h:34m:11s remains)
INFO - root - 2017-12-15 13:44:27.208418: step 19350, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 54h:34m:07s remains)
INFO - root - 2017-12-15 13:44:33.537226: step 19360, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 56h:45m:19s remains)
INFO - root - 2017-12-15 13:44:39.967227: step 19370, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 54h:58m:08s remains)
INFO - root - 2017-12-15 13:44:46.378784: step 19380, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:17m:49s remains)
INFO - root - 2017-12-15 13:44:52.771787: step 19390, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.630 sec/batch; 54h:45m:12s remains)
INFO - root - 2017-12-15 13:44:59.123583: step 19400, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 55h:12m:59s remains)
2017-12-15 13:44:59.630558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6001916 -3.7656693 -3.9969511 -4.1873531 -4.1931105 -4.1635885 -4.0237455 -4.0693455 -3.9020939 -4.7431083 -4.8290191 -5.2698774 -5.9417281 -6.0245352 -7.3728828][-3.3938909 -3.6465383 -3.6888804 -4.0979862 -4.1678638 -4.1237288 -4.2829542 -4.4025745 -4.3095345 -5.2160606 -5.410418 -6.0532365 -7.0509224 -7.170258 -8.2475481][-2.7643471 -2.7152596 -2.4678059 -2.3286362 -2.3035679 -2.4954681 -2.6328354 -2.9822083 -3.3485322 -4.3012896 -4.7185364 -5.4739776 -6.0861912 -6.1344361 -7.2709188][-2.1652932 -1.4773917 -0.84149504 -0.61288214 -0.68131447 -0.75868988 -0.72055387 -1.125493 -1.5764809 -2.8780804 -3.559145 -4.3148322 -5.1405354 -5.4180312 -6.4693813][-1.6973939 -1.2224007 -0.47840929 -0.069892883 -0.29750395 -0.33122635 -0.04947567 -0.38233852 -0.80007744 -1.9749222 -2.4606752 -3.2921538 -4.1142616 -4.2264886 -5.8405337][-1.6903639 -0.79835653 -0.39726973 0.048981667 0.095675945 0.11548996 0.40729332 -0.012022495 -0.32516527 -1.524344 -1.6924548 -2.32893 -3.0529814 -3.2006249 -4.91743][-1.3226066 -0.626102 0.074590206 0.6058979 0.73027992 0.80071545 1.0667162 0.75720787 0.6354084 -0.59584808 -1.0980411 -1.8589897 -2.4842191 -2.7113237 -4.287612][-0.90994024 -0.18364429 0.41526794 1.1724615 1.6452332 1.5646334 1.6277838 1.4709091 1.4433327 0.016813278 -0.41390133 -1.4293885 -2.2398281 -2.2269173 -3.8303514][-1.2056808 -0.59613514 -0.0646224 0.36487293 0.90699959 1.0287018 1.0299768 1.1205292 1.3693972 0.022370815 -0.75440073 -1.8501735 -2.6995358 -2.90421 -4.3888893][-2.4691763 -2.0249405 -1.5126281 -0.7449007 -0.50908613 -0.16112614 0.018975258 0.2745986 0.73410606 -0.3931241 -1.0684643 -2.3481545 -3.3021688 -3.4807081 -4.9874358][-3.9788172 -3.318459 -2.6515565 -2.2477469 -1.702601 -1.6569905 -1.4533734 -1.2462149 -1.1967735 -2.4347034 -3.0244074 -3.7711992 -4.487546 -4.3389816 -5.1885252][-5.0930729 -4.6908474 -4.705873 -4.6757922 -4.2897649 -3.9703426 -3.7251263 -3.8412457 -3.5588374 -4.3670664 -4.5549769 -5.0262947 -5.2769508 -5.1446791 -5.606667][-6.5795188 -6.3550549 -6.15456 -6.1616445 -5.929338 -5.6714315 -5.4153605 -5.0660648 -4.67018 -5.0172205 -4.9537258 -5.036375 -5.4154305 -5.2871885 -5.5866203][-7.5649328 -7.5494847 -7.5390763 -6.92549 -6.399086 -6.2891192 -5.7723536 -5.8190851 -6.0908785 -6.6230254 -6.8790617 -6.330431 -5.9994631 -5.4126973 -4.9102883][-7.45664 -7.1219015 -7.2277794 -7.262146 -7.3893394 -7.2327728 -7.10119 -7.1809134 -7.3566747 -7.2857437 -7.2723384 -6.9301391 -6.7559094 -6.0785031 -5.3306189]]...]
INFO - root - 2017-12-15 13:45:06.058830: step 19410, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:34m:40s remains)
INFO - root - 2017-12-15 13:45:12.390752: step 19420, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:02m:24s remains)
INFO - root - 2017-12-15 13:45:18.792764: step 19430, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 54h:49m:50s remains)
INFO - root - 2017-12-15 13:45:25.110447: step 19440, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 54h:48m:17s remains)
INFO - root - 2017-12-15 13:45:31.422493: step 19450, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 54h:54m:00s remains)
INFO - root - 2017-12-15 13:45:37.712852: step 19460, loss = 0.25, batch loss = 0.13 (13.0 examples/sec; 0.616 sec/batch; 53h:35m:06s remains)
INFO - root - 2017-12-15 13:45:44.168853: step 19470, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 55h:56m:09s remains)
INFO - root - 2017-12-15 13:45:50.584824: step 19480, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 55h:27m:11s remains)
INFO - root - 2017-12-15 13:45:56.926728: step 19490, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 54h:48m:12s remains)
INFO - root - 2017-12-15 13:46:03.367940: step 19500, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 55h:04m:01s remains)
2017-12-15 13:46:03.855696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3709273 -4.7057533 -5.1845055 -5.1025276 -4.2474241 -3.3968573 -3.0541978 -2.7728386 -2.1576209 -3.4059286 -3.8570046 -4.5262756 -6.2699022 -8.3260174 -8.894866][-5.3616247 -5.9279823 -6.6437511 -6.6703019 -6.3930173 -5.154048 -3.5273709 -2.1552901 -2.0604463 -4.497014 -5.9978104 -7.07186 -8.4306526 -10.057681 -10.108993][-4.994812 -5.3140125 -5.82721 -5.2324543 -4.3188272 -3.8128004 -3.2778535 -2.3068504 -1.5682654 -3.447113 -5.3642864 -6.8164816 -8.7504673 -10.497413 -10.430573][-5.0930805 -5.4664664 -5.6096935 -4.9758515 -3.5676847 -1.6926236 -0.52547836 0.14172506 0.25320625 -1.9413934 -3.6249352 -5.7629519 -8.0234318 -10.15598 -10.304577][-4.6867905 -4.7089214 -4.4290228 -3.9241967 -2.5232067 -0.71964741 1.1215019 2.1321993 2.1162291 -0.79012537 -2.7807469 -4.671926 -7.0396142 -8.8700256 -9.47598][-5.4633207 -4.5067654 -3.4912663 -2.0600367 -0.51577759 1.1097193 2.7583656 3.3574572 3.3084812 0.53244972 -1.6878605 -3.681241 -5.758801 -7.2570028 -7.5814123][-5.4186153 -4.7917523 -3.2994194 -1.3584023 0.412817 1.7211542 3.0757074 3.7221889 3.5880451 1.1320896 -0.67666149 -3.0774007 -5.8475647 -7.652339 -7.952939][-4.6614304 -3.8903365 -2.9406719 -0.55593824 1.5628786 3.3800287 5.1990738 5.2182188 4.3785038 1.350174 -0.82835007 -2.9261932 -5.9144807 -8.0030384 -8.05345][-4.710207 -3.9795325 -2.6573591 -0.80931711 0.95380783 2.6496973 4.2104273 4.8063831 4.1687937 0.62746906 -1.4189858 -3.350184 -6.1454215 -8.2616377 -8.70712][-4.7429171 -4.55245 -4.3636169 -2.5665264 -0.67365122 0.7783947 1.9125605 2.0662289 2.0344849 -0.83777046 -3.0692987 -4.9431696 -7.1724281 -8.8887939 -8.5808992][-6.1790504 -5.992569 -5.7483535 -4.9229774 -3.3220038 -1.3669715 -0.45231104 -0.29478788 -0.24626541 -2.5614452 -4.16665 -5.91837 -8.1130562 -9.4911795 -8.8552675][-7.3337846 -7.2315516 -7.3373122 -6.8290715 -5.9174981 -4.5556374 -3.605063 -3.317955 -3.2667947 -4.4094419 -5.2954931 -6.6020432 -7.9245238 -9.4832726 -9.2162552][-8.6094275 -8.5538521 -8.24618 -8.3629351 -7.9449339 -6.9725552 -6.4565611 -6.37877 -5.9766216 -6.9232364 -7.4066768 -8.0632162 -8.4681005 -9.0312777 -8.65083][-9.8106384 -9.4802608 -9.03697 -8.8100252 -7.8234243 -7.261456 -7.2303791 -7.2213073 -7.2550139 -7.5423889 -7.7308874 -7.8541756 -7.5822835 -7.954319 -7.4867439][-9.3388195 -9.3661966 -9.3954439 -9.5760145 -9.32506 -8.2665854 -7.6285954 -7.87607 -7.7722378 -7.7726283 -8.2522287 -8.0938749 -7.8546596 -7.5199676 -7.0540075]]...]
INFO - root - 2017-12-15 13:46:10.262258: step 19510, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 55h:22m:21s remains)
INFO - root - 2017-12-15 13:46:16.656774: step 19520, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 55h:50m:05s remains)
INFO - root - 2017-12-15 13:46:23.012164: step 19530, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 54h:14m:01s remains)
INFO - root - 2017-12-15 13:46:29.524881: step 19540, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 57h:11m:31s remains)
INFO - root - 2017-12-15 13:46:35.934731: step 19550, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 56h:36m:52s remains)
INFO - root - 2017-12-15 13:46:42.307265: step 19560, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 54h:54m:33s remains)
INFO - root - 2017-12-15 13:46:48.700219: step 19570, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 54h:01m:57s remains)
INFO - root - 2017-12-15 13:46:55.106102: step 19580, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 56h:13m:06s remains)
INFO - root - 2017-12-15 13:47:01.478685: step 19590, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 54h:09m:56s remains)
INFO - root - 2017-12-15 13:47:07.809795: step 19600, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 54h:44m:09s remains)
2017-12-15 13:47:08.344736: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6583352 1.1186328 0.29506922 0.47680426 0.68807364 0.50850153 0.50819159 0.69795084 0.10521936 -1.6610003 -3.7696934 -6.4250803 -8.2918215 -9.0996647 -10.476821][2.8401046 2.0452733 1.1591363 1.050406 0.94663858 1.30932 1.2124848 1.19945 0.72071028 -0.99554539 -2.6697063 -5.0792828 -7.6938415 -8.4888 -9.4315643][3.5073152 3.5575461 2.7123485 2.7542691 2.7517219 2.7261739 2.2905955 2.1496892 2.1888547 -0.064077377 -2.9014821 -5.2403545 -6.5295043 -7.1695156 -8.2802143][2.1176343 2.629746 2.624351 2.797008 3.2408748 3.2769551 3.3200145 3.0252385 2.362885 0.1206665 -2.4733491 -5.4588761 -7.2787318 -7.754179 -7.9056911][0.44141054 1.4126306 1.6781564 1.6389747 1.9470763 1.9875875 1.9669738 1.5531135 1.142652 -0.73084021 -2.9582953 -5.74312 -7.1305432 -7.9892116 -8.5849552][-1.3823853 -0.64424133 0.40224695 0.51537371 0.66643381 0.71794271 0.85055494 0.78309393 0.50923586 -1.1245494 -3.08771 -5.6324754 -7.1951618 -7.7911854 -8.0471773][-3.4451861 -2.2449713 -1.1168284 -0.51848221 -0.0090851784 0.483768 0.44700098 0.094089508 -0.064674377 -1.6892023 -3.9624412 -6.8048205 -7.7919335 -8.3138666 -8.8764734][-3.8527606 -3.303565 -3.1371183 -1.9629059 -1.064692 -0.36853552 0.25692558 0.17336321 -0.18280602 -1.6832085 -3.6473098 -6.5371838 -8.13113 -8.6288776 -8.5378246][-4.4414916 -3.5149755 -3.0187449 -2.2630386 -1.3642268 -0.42789268 0.18858862 0.213027 0.34391165 -1.1986556 -3.1922202 -5.8502808 -7.3826756 -8.0663815 -8.7094707][-5.1697783 -4.561491 -3.76094 -3.0436878 -2.2342234 -1.5167642 -0.64712048 -0.30230188 -0.15837049 -1.753346 -3.5770774 -5.1189752 -6.6752777 -7.4276352 -8.3917818][-5.3667479 -5.2381363 -4.4587789 -3.5007768 -2.8989253 -2.3287816 -1.7221384 -1.443377 -1.3527708 -2.8540707 -4.72729 -5.4382315 -6.4461007 -7.3443527 -7.6731186][-5.4494343 -5.301877 -4.8897543 -4.1899185 -3.9358969 -3.3712416 -3.2057495 -3.1495008 -3.1849475 -3.850328 -5.2267842 -5.7320361 -6.2764044 -7.0896993 -7.0627613][-5.5566053 -4.8132877 -4.5124073 -4.2632742 -4.2211857 -4.1147146 -4.1587849 -4.0844455 -3.9486878 -4.0281105 -4.7888823 -5.0937653 -5.3489113 -5.8805628 -6.0074][-4.6782246 -4.7520905 -5.1542778 -4.86956 -4.5214629 -4.4066286 -4.6252165 -4.7365036 -4.4280977 -4.3715239 -4.7904282 -4.8886452 -4.802835 -5.0033975 -5.027133][-5.8174186 -5.9401693 -6.5757809 -6.7827835 -6.4602861 -6.0015707 -5.6582651 -5.5157642 -5.6010571 -5.8126469 -5.7198176 -5.6895008 -5.4007835 -5.1215658 -5.0525789]]...]
INFO - root - 2017-12-15 13:47:14.697244: step 19610, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 55h:36m:19s remains)
INFO - root - 2017-12-15 13:47:21.057040: step 19620, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 53h:48m:09s remains)
INFO - root - 2017-12-15 13:47:27.479655: step 19630, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 54h:05m:34s remains)
INFO - root - 2017-12-15 13:47:33.901300: step 19640, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 54h:56m:17s remains)
INFO - root - 2017-12-15 13:47:40.331558: step 19650, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 55h:13m:05s remains)
INFO - root - 2017-12-15 13:47:46.644339: step 19660, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 54h:15m:16s remains)
INFO - root - 2017-12-15 13:47:52.992172: step 19670, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.628 sec/batch; 54h:35m:47s remains)
INFO - root - 2017-12-15 13:47:59.421158: step 19680, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 56h:16m:27s remains)
INFO - root - 2017-12-15 13:48:05.790911: step 19690, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 53h:58m:08s remains)
INFO - root - 2017-12-15 13:48:12.197986: step 19700, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 54h:51m:31s remains)
2017-12-15 13:48:12.635573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3565874 -3.0919909 -4.0959473 -4.2024174 -3.8754666 -3.6702647 -3.4467134 -3.0912967 -2.5952449 -3.0291004 -3.2353873 -4.9503946 -5.7843103 -6.1856713 -7.2439251][-2.5664692 -3.2523875 -4.2253885 -4.8396635 -4.7435703 -3.9441457 -3.3153787 -3.1070828 -3.2693796 -3.5373054 -3.9297659 -5.5264444 -6.2695165 -6.8002367 -7.5176353][-3.6282473 -3.8748965 -4.4805207 -4.4279852 -4.5990257 -4.4847054 -3.982543 -3.6553898 -2.9421282 -3.7206671 -4.4820976 -6.1812706 -6.8439775 -7.5999689 -8.5342188][-4.4881392 -4.6034303 -4.8871937 -4.5479836 -4.0556154 -3.5547066 -2.808032 -2.5033212 -2.4371481 -2.9495983 -3.5018911 -5.5773592 -6.7179775 -7.476541 -8.13837][-5.5926151 -5.4101286 -5.4188509 -4.8667526 -4.152997 -2.9917517 -1.4411273 -0.92776346 -0.62141132 -1.6117668 -2.7798352 -5.0366058 -6.26473 -7.3192749 -8.2482281][-7.1591468 -6.6575108 -6.0221319 -4.6860342 -3.490447 -1.9281526 -0.12246513 0.90183592 1.2494626 -0.51172829 -2.525713 -5.3318453 -6.5600471 -7.2758393 -7.9727483][-6.9239383 -6.5744009 -6.1798792 -4.4528494 -2.2621794 -0.37453175 1.8695273 2.9582906 3.4432406 1.5526757 -0.87819624 -4.3127646 -5.9470205 -6.5821338 -7.2959604][-6.5882726 -6.0655928 -5.4190946 -3.8996916 -1.8476276 0.92520666 3.5358119 4.0928235 4.0909743 1.9866986 -0.044487476 -3.1926861 -4.9857197 -5.9069495 -6.3566623][-6.7670059 -6.2328329 -5.8166351 -4.5663052 -2.6767478 0.17513704 2.6038375 3.5169311 3.74291 1.3495679 -0.85204744 -3.8795087 -5.3561974 -6.13058 -6.90335][-6.5574193 -6.2669539 -5.9602184 -4.7857323 -3.398376 -1.541697 0.583097 1.3761268 1.68257 -0.45223188 -2.6836057 -5.3477993 -6.3886065 -6.6655517 -7.4942584][-8.1364584 -7.6205163 -7.2800579 -6.1458263 -4.8719864 -3.5669918 -2.3529344 -1.8467426 -1.5008068 -3.1640553 -4.2478728 -6.4701567 -7.4657955 -7.3608918 -7.5732412][-7.6105056 -7.3648071 -7.3146605 -6.3023953 -5.6154466 -4.6379843 -3.5683537 -3.815088 -3.9259734 -4.4737177 -4.9161596 -6.2169881 -6.749681 -7.1618261 -7.716208][-8.7509069 -7.9890051 -7.4289651 -6.5094538 -5.8458433 -5.5857716 -5.1466227 -5.20774 -5.1758752 -5.9674478 -6.312376 -6.4846869 -6.61866 -6.6544375 -7.2930746][-8.49479 -7.7588062 -7.1473575 -6.4111567 -5.8864369 -5.6694307 -5.164114 -5.5355692 -5.7182975 -6.019711 -6.0953255 -6.0838766 -6.0758276 -6.256803 -6.5805278][-8.5262814 -8.25278 -8.1211061 -7.2013788 -6.2858267 -6.180192 -6.4917092 -6.5491219 -6.4777837 -6.5096154 -6.3843975 -6.1905317 -5.9362254 -5.7180705 -5.4402685]]...]
INFO - root - 2017-12-15 13:48:18.987672: step 19710, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 55h:00m:27s remains)
INFO - root - 2017-12-15 13:48:25.405647: step 19720, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.620 sec/batch; 53h:52m:13s remains)
INFO - root - 2017-12-15 13:48:31.829307: step 19730, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 54h:11m:59s remains)
INFO - root - 2017-12-15 13:48:38.260805: step 19740, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 54h:39m:10s remains)
INFO - root - 2017-12-15 13:48:44.675847: step 19750, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 55h:32m:08s remains)
INFO - root - 2017-12-15 13:48:51.034326: step 19760, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 54h:31m:34s remains)
INFO - root - 2017-12-15 13:48:57.552160: step 19770, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 54h:17m:28s remains)
INFO - root - 2017-12-15 13:49:03.945909: step 19780, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 55h:59m:02s remains)
INFO - root - 2017-12-15 13:49:10.327653: step 19790, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 57h:11m:26s remains)
INFO - root - 2017-12-15 13:49:16.733333: step 19800, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 56h:48m:31s remains)
2017-12-15 13:49:17.238171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0306044 -7.1724963 -7.7584667 -7.557631 -7.3240714 -6.982789 -5.9295311 -4.6611509 -3.5369983 -5.1467743 -5.9297252 -5.7901278 -6.1593871 -7.2720613 -7.51294][-6.3823357 -7.0943813 -7.5895729 -8.0075989 -8.1347876 -7.93732 -7.2117376 -5.9970741 -4.6826353 -6.2471485 -6.9164391 -7.0543084 -7.7458563 -8.5833721 -8.8093672][-5.4327993 -5.9801941 -6.2766061 -6.5874939 -6.8475266 -7.0074077 -6.6764145 -5.644001 -5.0828362 -6.75316 -7.1282911 -7.4141164 -7.8807321 -8.8934135 -9.1571741][-6.1473627 -5.9838963 -6.0322647 -5.8200188 -5.8797894 -5.2534027 -4.3203554 -3.7786684 -3.3910465 -5.4970217 -6.6726069 -7.3665428 -8.0061216 -8.7667665 -8.9644775][-7.5142822 -6.9931993 -6.1429911 -5.1479149 -4.6823006 -3.2304974 -2.2175479 -1.8532023 -1.6893492 -4.2832918 -5.6928606 -6.4342356 -7.1598496 -8.0338688 -8.6218252][-9.2751265 -8.24009 -7.0237579 -4.8098726 -2.6607194 -0.64083624 0.91926718 1.0795388 1.0123534 -2.077548 -4.2846365 -5.3301115 -5.9936714 -7.1403155 -8.19099][-9.2932854 -7.8182435 -5.5781193 -2.6570854 0.0022215843 2.7479157 4.4376426 4.9054637 4.9815793 0.8111825 -2.2555118 -3.6657386 -4.7977872 -5.8567619 -6.3608851][-8.246562 -7.126853 -5.7350106 -2.7032671 1.0435481 3.3738351 5.0344338 6.0052638 5.8047757 1.521071 -1.7166305 -3.5920315 -4.6124058 -5.590899 -6.0369205][-8.7284517 -8.3405313 -7.2181582 -4.2654238 -1.2590361 1.1238294 3.0334687 3.6839242 3.8186936 0.44169283 -2.7542048 -4.2842922 -5.1359386 -6.0036974 -5.7938547][-10.27346 -9.4303026 -8.1739559 -5.5446587 -2.3893361 -0.2889905 1.2954974 1.921412 1.5510116 -2.1062346 -4.4827957 -5.5809689 -6.6913033 -7.5844846 -7.3262672][-11.305758 -10.787831 -10.043117 -8.0927677 -6.3503017 -4.0720968 -2.0096397 -2.0208421 -2.5361433 -5.2819958 -6.8953218 -7.4482241 -8.0698414 -9.12488 -8.9979219][-11.766582 -11.002815 -10.151586 -9.3426561 -8.5308838 -7.2416883 -6.4173822 -5.7734423 -5.0476093 -7.4403071 -8.7034941 -8.7931213 -9.1706772 -9.3429546 -8.9409065][-11.879812 -11.246342 -10.724711 -9.7574434 -8.8657341 -7.9436316 -7.266211 -7.1689649 -7.2125063 -9.0634623 -9.7659626 -9.7135429 -9.8306255 -9.6097727 -9.40283][-11.018402 -10.404108 -9.9416513 -9.2782822 -8.3978024 -7.472806 -7.130105 -7.2001023 -7.3235531 -8.6293716 -9.0766563 -9.3981733 -9.8948536 -9.6870661 -9.2300644][-10.457045 -9.7118769 -9.4699221 -8.8887014 -8.2958126 -7.7341471 -7.1827726 -7.5550532 -7.9115391 -8.1225424 -8.2492666 -8.2912111 -8.1592169 -8.1295595 -7.8237119]]...]
INFO - root - 2017-12-15 13:49:23.690231: step 19810, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 57h:40m:08s remains)
INFO - root - 2017-12-15 13:49:30.126255: step 19820, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 57h:56m:51s remains)
INFO - root - 2017-12-15 13:49:36.490509: step 19830, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 55h:28m:30s remains)
INFO - root - 2017-12-15 13:49:42.930970: step 19840, loss = 0.38, batch loss = 0.26 (12.7 examples/sec; 0.630 sec/batch; 54h:44m:32s remains)
INFO - root - 2017-12-15 13:49:49.251414: step 19850, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 54h:33m:01s remains)
INFO - root - 2017-12-15 13:49:55.686825: step 19860, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 55h:23m:09s remains)
INFO - root - 2017-12-15 13:50:02.053066: step 19870, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 55h:03m:08s remains)
INFO - root - 2017-12-15 13:50:08.500221: step 19880, loss = 0.26, batch loss = 0.15 (11.6 examples/sec; 0.690 sec/batch; 59h:55m:54s remains)
INFO - root - 2017-12-15 13:50:14.926948: step 19890, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 55h:39m:44s remains)
INFO - root - 2017-12-15 13:50:21.376459: step 19900, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 56h:56m:00s remains)
2017-12-15 13:50:21.909132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9278293 -6.3205409 -6.4184775 -6.1941452 -6.0108662 -4.7758551 -3.9661996 -3.058146 -1.4915628 -1.5905352 -1.0454326 -4.2312503 -6.5086355 -7.9047351 -9.3776951][-4.3075123 -5.5815492 -7.0119286 -7.5846729 -7.8236041 -7.1694951 -6.6693735 -5.8511558 -4.105051 -3.5505919 -2.5575686 -5.3388748 -6.9385018 -8.1814413 -8.762639][-4.4296684 -4.7878571 -6.2068539 -7.1504154 -8.2626858 -8.8791151 -8.8050756 -8.0441751 -6.7511821 -6.3363357 -5.0266895 -7.1468143 -8.4423094 -9.3693771 -9.8109837][-4.2138677 -4.3697815 -4.9943085 -5.4216638 -6.1340485 -6.5572271 -7.2128611 -7.4093275 -6.4335613 -6.0943623 -4.9366474 -6.9842982 -8.6191854 -9.5278025 -9.9571018][-4.9342608 -4.4603071 -4.2514095 -3.9157455 -4.1413736 -3.7278028 -3.52876 -3.9471347 -3.4750462 -4.431469 -4.3735223 -7.5211792 -9.0087318 -10.282401 -10.284876][-4.6342063 -4.4218268 -4.1567478 -3.0844102 -2.3516016 -0.92016888 0.261209 0.2606678 0.337461 -1.0156913 -1.7780194 -6.0044036 -8.2159414 -9.28151 -9.7325344][-4.4964523 -3.7406743 -2.9744282 -1.2231107 0.17501354 1.2532363 2.6640162 3.6955781 4.6295781 2.8606658 1.4048104 -3.3992834 -6.8884978 -8.671052 -9.2445259][-3.7146196 -2.6696138 -2.1144 -0.054182529 1.6190333 3.4200644 4.7033544 5.1661229 6.35866 4.8111253 3.4003806 -1.8479424 -5.2933578 -7.6362038 -9.124815][-3.7596004 -3.3556366 -3.3383493 -1.7513165 -0.15764713 2.0275331 3.6638007 4.1766639 4.267117 2.0487428 0.59336424 -3.79903 -5.9290838 -7.7871909 -8.77698][-4.9755583 -4.6687589 -4.8654203 -3.717361 -3.0747862 -1.7840414 0.25894451 1.3009944 1.3973126 -1.1923409 -2.6093488 -6.3528385 -8.3394518 -8.54158 -8.4109755][-7.5311604 -7.0187941 -6.9283981 -6.0933228 -5.6325874 -4.6745234 -3.4383192 -2.7445226 -1.8633137 -4.055234 -5.1115384 -7.98762 -9.44421 -9.5418463 -9.40513][-7.4593277 -7.3809295 -7.7199116 -7.3316817 -7.2917819 -6.7961097 -6.2000179 -5.6356392 -5.0266047 -5.8859587 -6.706821 -8.475606 -9.3874588 -9.2885561 -8.6245432][-7.9074492 -7.5907183 -8.3403959 -8.0193176 -7.7354503 -7.7889771 -7.5529518 -7.6553726 -7.9980145 -8.22237 -7.9387984 -8.6956 -8.3839617 -7.7049465 -7.3815064][-8.8577957 -8.20246 -7.9843426 -7.9061327 -7.6851673 -7.7787619 -7.8547258 -7.9012208 -8.13688 -8.622592 -8.2245369 -8.2939444 -7.84134 -7.1008496 -6.8964872][-8.3502226 -8.9980974 -9.3096638 -9.1780949 -9.2409058 -8.5194473 -8.171402 -8.2558584 -8.3472977 -8.6931915 -8.8732977 -8.6585293 -7.8906093 -7.3704906 -6.8973532]]...]
INFO - root - 2017-12-15 13:50:28.274169: step 19910, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 54h:49m:58s remains)
INFO - root - 2017-12-15 13:50:34.811922: step 19920, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 58h:04m:50s remains)
INFO - root - 2017-12-15 13:50:41.250122: step 19930, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 55h:47m:38s remains)
INFO - root - 2017-12-15 13:50:47.735406: step 19940, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 57h:05m:45s remains)
INFO - root - 2017-12-15 13:50:54.098511: step 19950, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 54h:26m:33s remains)
INFO - root - 2017-12-15 13:51:00.471684: step 19960, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 57h:35m:30s remains)
INFO - root - 2017-12-15 13:51:06.820421: step 19970, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:21m:58s remains)
INFO - root - 2017-12-15 13:51:13.239847: step 19980, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:55m:00s remains)
INFO - root - 2017-12-15 13:51:19.532869: step 19990, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 54h:53m:27s remains)
INFO - root - 2017-12-15 13:51:25.991451: step 20000, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:52m:59s remains)
2017-12-15 13:51:26.529292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2442274 -4.6293631 -4.5238247 -4.2471371 -4.1993179 -4.6201181 -4.8155909 -4.4315305 -4.1134319 -3.9900172 -5.0177622 -4.3243656 -5.3809223 -5.9886627 -6.8648219][-3.9856024 -4.2340393 -3.8930783 -3.974818 -3.6285625 -3.3091674 -3.2679977 -3.173244 -2.8381147 -3.1502061 -5.1498117 -4.5121465 -5.3216581 -5.4630704 -5.4182677][-2.1281943 -1.7391372 -1.0366368 -1.1039762 -1.988142 -2.5419369 -2.284801 -1.2524867 -0.67969036 -1.4180946 -3.0495768 -2.4150352 -3.7669084 -4.39554 -4.8847542][-0.63641644 0.5909276 1.3222995 1.3188386 1.1961637 0.86348391 0.11169481 -0.2215023 0.32625628 0.52170134 -0.70765924 -1.0752864 -2.7854042 -3.0833497 -3.1641431][-0.62424707 0.13822317 1.0658984 2.2002416 2.3103242 2.1187329 2.4912581 2.4280343 2.2803426 1.2788997 -0.1970892 -0.043438911 -1.7056727 -2.4840674 -3.3028698][0.43504381 1.9113593 2.0652289 1.9303508 2.616179 3.224401 3.7594333 3.6033988 3.2148681 1.9867864 -0.14197683 -0.82295227 -2.5429754 -2.8734808 -3.6055465][-0.20765781 1.2785821 2.708487 2.9547906 3.010612 3.5292373 3.6408095 3.7189946 3.6326594 2.0399728 -0.71840572 -1.5248456 -2.9650593 -3.8734424 -4.4713192][0.15815783 0.47630167 1.6990361 3.3472905 3.9000916 3.7805486 3.6520925 3.4196296 3.1405845 1.9101605 -0.48371744 -1.51895 -3.3585629 -4.159461 -4.5009241][-0.2348032 0.50238276 0.803782 1.8199859 3.2335696 4.1780381 4.0646434 3.3768582 2.890636 1.4635577 -0.82396317 -1.3488212 -2.6163797 -3.6717668 -4.8882527][-2.7503977 -1.2321858 -0.5209465 0.57724524 1.495585 2.0572267 2.3998332 1.7975335 1.2397914 -0.0669446 -2.3067942 -2.9081988 -3.8611472 -3.7704992 -4.1867385][-4.3811607 -3.6098495 -2.4602203 -2.1623139 -1.9107242 -1.4535413 -1.141665 -1.4141917 -1.6399632 -2.4146686 -4.2215881 -4.7049713 -4.8085394 -5.1246729 -5.692245][-5.2242041 -5.9140186 -5.8551579 -5.12155 -4.3691778 -4.5995579 -4.7824945 -4.6002693 -4.3380547 -4.8396835 -6.0164394 -5.5830994 -5.9252253 -6.2430549 -5.9418688][-7.1488814 -6.8197742 -6.2462649 -6.2945671 -5.9337049 -5.5902433 -4.9947233 -4.8404441 -5.037838 -5.5117178 -5.94767 -5.689662 -5.3900709 -4.85238 -5.1393614][-7.5341859 -7.0193877 -7.0897021 -6.4235268 -6.1399732 -5.718935 -5.25247 -5.660121 -5.1120243 -4.9578238 -5.2525816 -4.9039211 -5.0663023 -4.9335833 -4.4309855][-7.1722636 -6.6633453 -6.2904806 -5.8386 -5.505785 -5.5933876 -5.2416334 -5.4850492 -5.5559239 -5.661232 -5.4870214 -4.8298883 -4.6556878 -4.9115438 -4.8464308]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 13:51:33.743360: step 20010, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 53h:56m:14s remains)
INFO - root - 2017-12-15 13:51:40.130411: step 20020, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 54h:57m:41s remains)
INFO - root - 2017-12-15 13:51:46.562140: step 20030, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 57h:13m:29s remains)
INFO - root - 2017-12-15 13:51:52.968484: step 20040, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 55h:11m:56s remains)
INFO - root - 2017-12-15 13:51:59.372147: step 20050, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 58h:03m:37s remains)
INFO - root - 2017-12-15 13:52:05.896255: step 20060, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 56h:32m:30s remains)
INFO - root - 2017-12-15 13:52:12.289380: step 20070, loss = 0.35, batch loss = 0.24 (12.6 examples/sec; 0.635 sec/batch; 55h:05m:08s remains)
INFO - root - 2017-12-15 13:52:18.659340: step 20080, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 54h:51m:55s remains)
INFO - root - 2017-12-15 13:52:25.070535: step 20090, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 57h:40m:01s remains)
INFO - root - 2017-12-15 13:52:31.476239: step 20100, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.633 sec/batch; 54h:57m:56s remains)
2017-12-15 13:52:32.000722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.97892809 -1.5248752 -2.3131604 -2.8280182 -3.155652 -3.0481553 -2.9149156 -2.6663198 -2.3257046 -2.7510676 -3.4950356 -4.2647624 -5.3876071 -6.0864086 -7.1532459][-1.5810032 -1.8435001 -2.3601689 -2.9132376 -3.6356835 -4.0435772 -3.5306444 -2.6837969 -1.8331304 -2.7704191 -4.1082983 -4.7201781 -5.9235411 -6.7640128 -7.4650917][-2.8340416 -2.8286672 -2.8901238 -3.1624746 -3.3952618 -3.5721302 -3.4058733 -2.9773817 -2.4454846 -3.3823686 -4.543931 -5.5634036 -6.4410214 -7.4812493 -8.5437841][-3.9990497 -3.862711 -3.4984059 -3.4148803 -2.9437919 -2.6835217 -2.0357552 -1.7213702 -1.2550511 -2.5337882 -3.9823065 -5.1517167 -6.3969541 -7.4044785 -8.5229759][-4.4875765 -3.7083974 -2.9999762 -2.3963614 -1.7125483 -0.55447769 0.41009569 0.91045046 1.0253568 -0.7366128 -2.9228091 -4.4748774 -5.9899421 -7.0053105 -8.0604057][-5.1661196 -4.4771094 -3.8278906 -2.6398048 -1.0210705 0.73726606 2.0924392 2.6154199 2.583488 0.597877 -2.0557642 -3.9783509 -5.8436508 -7.0971584 -7.9826097][-4.7633452 -4.2684 -3.9617796 -2.6639447 -0.52661657 1.1760764 2.8785224 3.6513696 3.9494624 2.1462426 -0.38607597 -2.6705532 -5.1630635 -6.7787232 -7.8438892][-4.7729163 -3.9658136 -3.0326648 -1.6575027 0.70251226 2.8611236 4.6359792 4.9271483 4.9911704 3.0217853 0.67682695 -1.5828247 -3.8148937 -5.6896372 -7.1495543][-4.5031481 -4.1013002 -3.5713382 -2.3827562 -0.78400517 1.2800651 3.2489295 4.0870347 4.64395 2.3481069 -0.087535381 -2.0259352 -4.1079597 -5.6908393 -6.9290848][-4.9396858 -4.3285069 -3.7363844 -3.0492387 -2.187108 -1.0959573 0.26337576 1.2719445 2.1541419 0.4994216 -1.4299693 -2.9855113 -4.8972816 -6.106966 -6.9651155][-5.854454 -5.4409947 -4.9642525 -4.3128786 -3.5821819 -2.2997465 -1.1730318 -1.1103921 -0.7179594 -1.9724169 -3.1135378 -4.8142066 -6.2816052 -7.36406 -7.6968246][-7.1912475 -6.7555909 -6.3092623 -5.8652549 -5.5542011 -4.742816 -4.0877862 -4.0841031 -3.8751738 -4.6124964 -4.8905087 -5.7912765 -6.45982 -7.3578463 -7.7354264][-7.6294808 -7.26801 -6.79039 -6.3774014 -6.0244007 -5.6213207 -5.2121377 -5.2300091 -5.3168731 -5.8902965 -5.7081528 -5.93231 -6.23701 -6.7702289 -7.0317912][-7.4782476 -7.3736858 -6.8751345 -6.3478718 -5.9816294 -5.5263243 -5.3110151 -5.5390334 -5.5325813 -5.8658633 -6.0157132 -6.2325044 -6.4975967 -6.5120506 -6.6858873][-7.435051 -7.517139 -7.598093 -7.4359655 -7.1970086 -6.6657772 -6.1707067 -6.44566 -6.7915983 -6.8705554 -6.6520004 -6.5136509 -6.4567313 -6.2399607 -6.0978785]]...]
INFO - root - 2017-12-15 13:52:38.426683: step 20110, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 55h:03m:43s remains)
INFO - root - 2017-12-15 13:52:44.865303: step 20120, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 55h:31m:24s remains)
INFO - root - 2017-12-15 13:52:51.297592: step 20130, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 55h:40m:35s remains)
INFO - root - 2017-12-15 13:52:57.670282: step 20140, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 54h:20m:27s remains)
INFO - root - 2017-12-15 13:53:04.225127: step 20150, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 57h:31m:28s remains)
INFO - root - 2017-12-15 13:53:10.656833: step 20160, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 56h:18m:58s remains)
INFO - root - 2017-12-15 13:53:17.020460: step 20170, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 54h:46m:34s remains)
INFO - root - 2017-12-15 13:53:23.389050: step 20180, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 54h:32m:40s remains)
INFO - root - 2017-12-15 13:53:29.862373: step 20190, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 56h:13m:22s remains)
INFO - root - 2017-12-15 13:53:36.268883: step 20200, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 55h:09m:46s remains)
2017-12-15 13:53:36.744277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2687974 -3.549161 -2.8177562 -2.0045462 -1.7914176 -1.4536476 -1.0671449 -1.2273197 -1.4789715 -1.65383 -2.9763656 -4.2560024 -5.8430715 -7.7810354 -8.477128][-3.980046 -3.9955962 -3.4648314 -3.2168474 -3.2611098 -2.703505 -2.4983139 -2.2702522 -2.1970248 -2.5752659 -3.6734071 -4.7235203 -6.2328372 -7.909452 -8.3017626][-3.1475072 -2.7641172 -2.257091 -1.9410987 -1.8491645 -1.8748012 -1.9622107 -1.9450092 -2.1158433 -2.4714055 -3.2638569 -4.1752386 -5.7294588 -7.4617357 -7.9067025][-1.270112 -0.90893316 -0.639699 -0.77298212 -1.090559 -1.0687876 -0.93980837 -1.1767869 -1.4683557 -2.3072524 -3.3267655 -4.1740627 -5.5902128 -6.8829517 -7.1875997][-1.1470938 -0.55673552 0.1978755 0.10554266 -0.16048145 -0.20399141 -0.118186 -0.28981113 -0.39892244 -1.3770461 -2.4809842 -3.2427344 -4.3231945 -5.5184841 -6.3983989][-0.43382645 0.1662612 0.57190084 0.55245352 0.55341578 0.81796789 1.0614514 0.92422533 1.0109525 0.20170975 -0.77534485 -1.7073917 -3.1587191 -4.6673622 -5.8502021][-0.68837929 0.13775635 0.62975836 0.79076815 0.88119555 1.2302232 1.660974 1.7038684 1.9198661 1.0433488 -0.0051722527 -1.0210657 -2.49474 -4.1930704 -5.41016][-1.2889075 -0.5228405 0.15389681 0.54453421 0.86785364 1.3029571 1.7552114 2.1066022 2.5003972 1.938251 0.98030043 -0.23051786 -1.3499575 -2.8458533 -4.00185][-2.6364641 -1.8488469 -0.88885689 -0.21091604 0.24588346 0.92550611 1.6677699 1.9223409 2.3965812 1.8298812 0.75506163 -0.15338755 -1.301888 -2.8256292 -4.2979536][-4.3984742 -3.0086412 -2.0209346 -1.626853 -0.92352581 -0.39910364 0.28957272 1.0060592 1.7821918 0.91466856 -0.19288397 -1.5934095 -2.8867011 -3.7467256 -4.7191381][-6.8483391 -5.8739724 -4.9228177 -4.1982765 -3.1337261 -2.6042504 -1.9481969 -1.6004534 -1.5694604 -2.2702103 -3.1634974 -3.8726358 -4.55577 -5.2972965 -5.91315][-8.7832708 -7.89583 -6.9888334 -6.3871088 -5.8853045 -5.1950645 -4.4327631 -4.2011528 -3.817112 -4.1698012 -4.76884 -5.3453531 -5.8096375 -6.4472561 -7.1288753][-8.59324 -8.7232533 -8.4504242 -7.6396971 -7.0862217 -6.6106405 -5.9760852 -6.1115665 -5.9615626 -6.0257521 -6.4282742 -6.5314417 -6.5993495 -6.661232 -6.7377014][-7.7236481 -7.6102738 -7.8282037 -7.63855 -7.327075 -6.8559656 -6.4316492 -6.4420156 -6.515995 -6.9156332 -7.2365518 -7.2885785 -7.2703218 -6.9841657 -6.5349727][-7.8711557 -8.2091532 -8.3118286 -8.1189785 -7.4596233 -6.9475679 -6.7635622 -6.8801503 -6.9422345 -7.2568493 -7.2784419 -7.360199 -7.4912829 -7.3587117 -6.9851193]]...]
INFO - root - 2017-12-15 13:53:43.173283: step 20210, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 56h:14m:05s remains)
INFO - root - 2017-12-15 13:53:49.524615: step 20220, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 54h:11m:31s remains)
INFO - root - 2017-12-15 13:53:55.862471: step 20230, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 55h:15m:06s remains)
INFO - root - 2017-12-15 13:54:02.264413: step 20240, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.669 sec/batch; 58h:04m:09s remains)
INFO - root - 2017-12-15 13:54:08.641272: step 20250, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 56h:08m:54s remains)
INFO - root - 2017-12-15 13:54:15.027036: step 20260, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 55h:29m:45s remains)
INFO - root - 2017-12-15 13:54:21.485525: step 20270, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 56h:16m:26s remains)
INFO - root - 2017-12-15 13:54:27.921378: step 20280, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 55h:53m:17s remains)
INFO - root - 2017-12-15 13:54:34.337375: step 20290, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 55h:20m:22s remains)
INFO - root - 2017-12-15 13:54:40.756562: step 20300, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 55h:44m:24s remains)
2017-12-15 13:54:41.268814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6284685 -3.956892 -3.7633276 -3.4222403 -3.2534227 -3.3124428 -3.1949563 -3.2518044 -2.8559165 -3.4220805 -3.6408195 -5.2471085 -5.9319582 -6.2532063 -7.4561229][-4.8969793 -4.6037025 -4.1513777 -3.7302237 -3.4625478 -3.5497184 -3.453402 -3.3641858 -2.9541302 -3.5950308 -3.7451494 -5.3257275 -6.2823124 -6.7348018 -7.9295468][-3.8406222 -4.1424551 -4.2883697 -4.2321472 -3.8834083 -3.6258678 -3.1583958 -3.1110687 -2.754056 -3.4592524 -3.9274745 -5.6727285 -6.3083296 -6.4784594 -7.5734963][-4.9678974 -4.9863744 -4.6537962 -4.0609846 -3.5938597 -2.9030576 -2.0293694 -1.7623358 -1.4717293 -2.3287053 -2.923666 -4.9458294 -5.893609 -6.3368182 -7.6384916][-6.3408685 -6.5436511 -5.9297724 -4.5628 -3.0813041 -1.7660766 -0.27061224 0.28332281 0.88287115 -0.32985973 -1.5737023 -4.0075502 -5.4669151 -6.0855618 -7.55003][-7.2601452 -7.0847526 -6.509491 -4.6290493 -2.4931598 -0.576293 1.4901624 1.9453588 2.1934018 0.97893095 -0.18364382 -2.6971235 -4.0969238 -5.047925 -6.8776245][-6.0373721 -5.5405569 -4.7068005 -2.9956636 -1.2344046 0.52218103 2.5528274 3.337976 3.6913762 2.2629066 0.8604598 -1.7298293 -3.1689672 -4.0154476 -5.9337964][-4.9488878 -3.9312146 -2.7465405 -0.78713703 0.99146128 2.2488093 3.6701665 4.4638505 4.9165416 3.3116508 1.4643512 -1.3119545 -2.7724147 -3.5784664 -5.2586718][-3.9737349 -3.522501 -2.897181 -1.3077154 0.37411547 1.8457217 3.2978015 3.6951022 3.9831986 2.7105193 1.3923688 -1.3452706 -2.8570089 -3.5441866 -4.9842281][-3.6597323 -2.8080845 -2.3846507 -1.3864269 -0.44764996 0.51004362 1.515594 1.9359984 2.14858 1.0582252 0.53114939 -1.6120567 -2.7846966 -3.5385103 -5.174736][-3.8838892 -3.0502286 -2.5346937 -1.3795776 -0.53956795 0.079219341 0.68516111 0.56567907 0.71364641 -0.65533161 -1.3425531 -2.9538569 -3.8418779 -3.8101475 -4.790761][-4.0856123 -3.3346124 -2.3499618 -1.2910666 -0.50683784 -0.14694452 0.037041187 -0.40784979 -0.71230507 -1.6005063 -1.6990685 -3.17802 -3.8789904 -4.1115103 -5.2497749][-4.9056616 -4.3603477 -3.7801461 -2.9016609 -2.2542462 -1.7449312 -1.5075026 -1.9402642 -2.0813832 -3.1699452 -3.3205338 -4.3704615 -4.694315 -4.9276495 -5.8083196][-4.4005623 -4.0941277 -4.1967268 -3.9884181 -4.0482574 -3.8493388 -3.7404194 -3.8427436 -3.8141091 -4.4121017 -4.2829351 -4.9730091 -5.3184638 -5.3331041 -5.9389725][-6.3515887 -5.2371545 -4.3400869 -4.2353907 -4.6975751 -4.8235559 -5.3369269 -5.6493354 -5.5447192 -5.3475089 -5.366785 -5.4764562 -5.7140222 -6.1448092 -6.6617193]]...]
INFO - root - 2017-12-15 13:54:47.609098: step 20310, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 55h:49m:02s remains)
INFO - root - 2017-12-15 13:54:53.940689: step 20320, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 55h:19m:34s remains)
INFO - root - 2017-12-15 13:55:00.393407: step 20330, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 56h:05m:09s remains)
INFO - root - 2017-12-15 13:55:06.664116: step 20340, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.613 sec/batch; 53h:11m:12s remains)
INFO - root - 2017-12-15 13:55:13.076707: step 20350, loss = 0.35, batch loss = 0.24 (12.2 examples/sec; 0.657 sec/batch; 56h:57m:29s remains)
INFO - root - 2017-12-15 13:55:19.508005: step 20360, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:57m:17s remains)
INFO - root - 2017-12-15 13:55:25.900341: step 20370, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 53h:59m:06s remains)
INFO - root - 2017-12-15 13:55:32.312673: step 20380, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 54h:20m:46s remains)
INFO - root - 2017-12-15 13:55:38.689362: step 20390, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 55h:15m:45s remains)
INFO - root - 2017-12-15 13:55:45.068248: step 20400, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 57h:50m:30s remains)
2017-12-15 13:55:45.576172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.826921 -5.9625611 -4.4539452 -3.1085172 -2.7814841 -2.3980856 -2.1120386 -2.370481 -2.1216898 -3.934525 -4.5505276 -5.142941 -5.3482809 -6.2293749 -7.7009449][-4.3500071 -4.3440704 -3.8241124 -3.1684241 -2.8964458 -2.5286078 -2.8865032 -3.0382385 -3.2756839 -4.8874989 -5.0071993 -6.1816945 -6.307303 -6.6211262 -7.8141823][-3.87619 -2.5730906 -1.1042209 -0.46987057 -0.23524952 -0.43784904 -0.96303511 -1.6673579 -2.1705842 -3.8105214 -3.9390805 -4.63171 -4.8961105 -5.2929626 -6.3627214][-2.4714098 -1.7412353 -0.36837053 0.9231925 1.2869563 1.4735608 0.98263788 -0.086266994 -0.89538431 -3.1694584 -3.1853237 -3.5062008 -3.6753521 -3.8810136 -5.3018475][-1.0674682 0.21521378 1.5167994 1.842855 1.5900855 1.9110618 1.9918132 1.0465713 0.76597548 -1.3421645 -1.4437308 -1.3912959 -2.1104045 -2.668066 -4.403697][-1.4069815 -0.1512208 1.1779971 2.0498824 2.1864877 2.2854342 2.7281728 2.5237269 2.0929103 -0.064437389 -0.142735 -0.12529612 -0.51185083 -0.92296982 -2.3726206][-2.5218158 -1.6021156 0.52488756 1.5160298 1.5891604 2.0009446 2.515944 2.5745378 2.7936416 0.85118151 0.53491354 0.42929697 -0.36241531 -1.3319459 -3.0790772][-4.1765995 -2.5540981 -0.52471924 0.70362425 1.5287995 1.8123136 2.1115537 2.5522237 3.0491014 1.0635304 0.88555765 0.51733065 -0.55333853 -1.2223172 -3.0789533][-4.8187504 -3.4690676 -1.9816957 -0.35095263 0.9819169 1.337254 1.8311124 2.0730691 2.5060363 0.99890661 0.75810194 0.38878202 -1.2531037 -2.2743716 -3.6272869][-6.1578608 -4.5169454 -2.8970466 -1.5602808 -0.32364273 0.25646448 1.0541053 1.1610532 1.3745341 -0.33320141 -0.76996136 -1.1360536 -2.5075588 -3.5285025 -4.7582836][-7.2431622 -5.7824469 -4.5208445 -2.9298482 -2.2321186 -1.5674229 -0.44128656 -0.35077286 -0.12020016 -1.8913383 -2.8537579 -3.3064909 -4.8476319 -5.0116773 -5.7508178][-7.681982 -5.7795653 -3.9824786 -3.2701807 -2.7264109 -2.2414184 -1.580307 -1.8092856 -2.0125885 -3.1059623 -3.7533896 -3.7922871 -4.9306469 -5.1688795 -5.9296079][-8.2977591 -6.6102381 -4.3535604 -3.2001424 -2.8423529 -2.0880308 -1.7328968 -2.3149314 -2.749054 -4.050807 -4.885294 -4.8274221 -5.361773 -5.0871992 -5.4219394][-8.2426615 -6.6719952 -4.9788451 -4.1473341 -2.9879613 -2.3212252 -2.2434039 -2.5964155 -3.1735797 -3.985435 -4.4740047 -4.8820729 -5.4256535 -5.2046614 -5.5869961][-8.9616394 -7.4599671 -6.0983243 -5.2148752 -4.2412734 -3.4167309 -2.9753819 -3.5777278 -4.037209 -4.423069 -4.7237129 -5.1180143 -5.3218393 -5.3169765 -5.2991648]]...]
INFO - root - 2017-12-15 13:55:52.005231: step 20410, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 56h:14m:18s remains)
INFO - root - 2017-12-15 13:55:58.436944: step 20420, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 54h:23m:28s remains)
INFO - root - 2017-12-15 13:56:04.786473: step 20430, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:25m:24s remains)
INFO - root - 2017-12-15 13:56:11.216113: step 20440, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 54h:27m:37s remains)
INFO - root - 2017-12-15 13:56:17.639568: step 20450, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 56h:21m:42s remains)
INFO - root - 2017-12-15 13:56:23.974986: step 20460, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 56h:14m:01s remains)
INFO - root - 2017-12-15 13:56:30.361169: step 20470, loss = 0.29, batch loss = 0.18 (11.7 examples/sec; 0.687 sec/batch; 59h:30m:17s remains)
INFO - root - 2017-12-15 13:56:36.689154: step 20480, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 55h:00m:50s remains)
INFO - root - 2017-12-15 13:56:43.050889: step 20490, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 55h:56m:38s remains)
INFO - root - 2017-12-15 13:56:49.451774: step 20500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:23m:38s remains)
2017-12-15 13:56:49.952361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.772222 -2.1836891 -1.3844347 -0.89010477 -0.90285206 -0.9468894 -1.5210562 -1.6431518 -1.5339966 -2.1282349 -2.6025972 -3.236619 -4.1982822 -5.614234 -5.9187927][-3.398849 -2.88105 -2.3982477 -2.1597075 -1.8455715 -1.3823695 -1.2099152 -1.3366795 -1.7714906 -2.7757664 -3.5971322 -3.7146192 -4.1839228 -5.3992538 -5.76302][-3.5105109 -3.6338511 -3.5306449 -3.6946993 -3.7677655 -3.4666438 -2.8553772 -2.3818879 -1.7652698 -2.229701 -3.52603 -4.4613876 -5.3401623 -6.2584839 -6.1228676][-3.5532556 -3.3193746 -3.2997913 -3.500814 -3.3798113 -2.6491361 -1.8925166 -1.6399465 -1.44941 -1.722106 -2.5051126 -3.6831994 -4.8857527 -6.3431244 -6.5703382][-4.1845016 -3.3798547 -2.6517525 -2.2360649 -1.912909 -1.2734456 -0.5128665 -0.11970615 0.036310673 -0.978106 -2.1489048 -3.3278823 -4.6760254 -6.1940761 -6.9991436][-3.5658116 -2.952352 -2.6422906 -2.0240979 -1.2757964 -0.26909018 0.46308088 0.46258402 0.431211 -0.38119364 -1.5812101 -2.7421799 -3.916743 -5.613471 -6.2430768][-2.2271137 -1.8143592 -1.485477 -1.0183144 -0.49367714 0.17972231 0.66066408 1.1013951 1.2721076 0.77085447 -0.27035475 -1.1208677 -2.3169761 -4.3528652 -5.0907221][-1.3953934 -0.80161572 -0.22205162 0.13320494 0.31352758 1.1804347 1.832376 2.1311479 2.0482726 1.5200667 0.44069052 -0.51455021 -1.5640864 -3.6908576 -4.4760427][-1.5011516 -1.1142445 -0.2997427 0.40673208 1.0576701 1.4286675 1.5850205 1.7755876 1.7451768 1.3175826 0.21061897 -0.91453791 -2.0167208 -3.9981377 -4.568222][-1.386301 -0.64711046 -0.44360781 -0.1143012 0.3317132 0.959188 1.6845336 1.699142 1.7795043 1.2459302 0.45872641 -0.48483706 -1.9692416 -3.6403794 -3.9939756][-2.3325973 -1.6657047 -1.0203228 -0.6443882 -0.39998198 -0.046071529 0.27954197 0.43024969 0.65353155 0.18610048 0.22696733 -0.008790493 -1.4007883 -2.538239 -2.816565][-3.888684 -2.9627471 -2.0941029 -1.2342629 -0.90013695 -0.61265469 -0.657187 -0.64093065 -0.62199116 -1.0698752 -0.84294844 -0.56485748 -1.0701466 -2.3484836 -2.7552996][-4.8763256 -4.1471248 -3.7124195 -3.2283897 -2.5818372 -1.8526874 -1.5539155 -1.8695354 -1.8953056 -1.9647408 -2.3459492 -2.4708405 -3.1547966 -3.5617151 -3.6331782][-4.4039545 -4.0733023 -3.9289408 -4.3181634 -4.4237442 -3.7597768 -3.2428722 -3.275476 -3.3917761 -3.6169305 -3.786212 -4.4691219 -4.8037009 -5.2489638 -5.3289046][-4.82947 -3.8763242 -3.1922894 -3.5267329 -4.22834 -4.4093494 -4.7330337 -4.6816955 -4.6531081 -4.7282019 -4.8797913 -5.2666168 -5.5657692 -5.6838613 -5.9512982]]...]
INFO - root - 2017-12-15 13:56:56.363326: step 20510, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 56h:44m:19s remains)
INFO - root - 2017-12-15 13:57:02.742858: step 20520, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 55h:02m:04s remains)
INFO - root - 2017-12-15 13:57:09.157738: step 20530, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 57h:45m:34s remains)
INFO - root - 2017-12-15 13:57:15.701809: step 20540, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 56h:45m:42s remains)
INFO - root - 2017-12-15 13:57:22.011539: step 20550, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 54h:44m:35s remains)
INFO - root - 2017-12-15 13:57:28.382925: step 20560, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 54h:33m:51s remains)
INFO - root - 2017-12-15 13:57:34.696385: step 20570, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 55h:22m:04s remains)
INFO - root - 2017-12-15 13:57:41.113487: step 20580, loss = 0.31, batch loss = 0.19 (13.1 examples/sec; 0.613 sec/batch; 53h:04m:40s remains)
INFO - root - 2017-12-15 13:57:47.486930: step 20590, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 55h:14m:21s remains)
INFO - root - 2017-12-15 13:57:53.902863: step 20600, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 55h:25m:30s remains)
2017-12-15 13:57:54.539490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5478454 -3.2704973 -2.7904143 -1.9715147 -1.1126595 -0.58727884 0.033271313 0.60201693 1.1681724 -0.65721893 -1.680397 -3.9777982 -5.236567 -6.7371821 -7.2774415][-3.3979592 -3.3549967 -2.9996533 -2.3405619 -1.7040844 -0.50663042 0.30694866 0.30189276 0.46860552 -1.0267744 -1.5290661 -4.0392275 -6.0626755 -6.8860617 -6.4672089][-4.2398748 -4.552506 -4.4722462 -3.2763519 -2.1886458 -1.2765794 -0.75616646 -0.61751223 -0.35724449 -1.9799213 -2.5422945 -4.5478621 -5.4736071 -6.7310042 -7.0049105][-5.3319359 -5.0266809 -4.8105435 -3.6165786 -2.1736236 -0.71841526 -0.1030345 -0.053439617 0.21961784 -1.6232066 -2.2381539 -4.26015 -4.9432983 -6.1098504 -6.7375717][-4.811193 -4.4261465 -3.6463213 -2.1308765 -0.80623245 0.17746639 0.62852335 0.38507509 -0.11668015 -1.8832521 -2.4368467 -4.5699549 -5.503901 -6.3584356 -6.2328768][-5.2076969 -4.5057049 -3.7384784 -1.544764 0.13987875 1.2717719 1.2073703 0.91108656 0.27597618 -2.17269 -2.8769827 -4.6425862 -5.1285133 -6.1633396 -6.3734045][-3.6818089 -3.44423 -3.2261906 -1.8436289 -0.33635473 1.3177142 1.4986987 1.4101281 1.2130065 -0.95904636 -2.4783053 -5.0975246 -5.6128769 -6.5404739 -6.336926][-3.0731316 -2.1058989 -1.6919994 -1.0303831 -0.17092133 1.7322783 3.1184402 3.467999 2.7374959 -0.44160986 -1.835011 -4.6424227 -5.6597819 -6.62982 -6.2107129][-2.6714664 -2.3773413 -1.2022367 -0.19842434 1.3981357 2.4864888 2.4604735 2.7991157 2.8708386 0.08420372 -1.677011 -4.5037308 -5.673522 -6.7687712 -6.4594769][-3.0935707 -2.5773454 -1.9907799 -0.88787746 0.7285533 2.1222997 3.0349288 2.5848804 2.2429948 -0.22950792 -1.5330029 -4.804379 -6.3766813 -6.9014597 -6.7232604][-3.4982867 -4.1510534 -4.1187687 -3.616353 -2.4084411 -0.21763372 1.546145 1.2462087 0.33930445 -2.6715364 -4.2043209 -5.9643168 -6.7838321 -7.3338041 -6.4195509][-4.9711618 -5.7352376 -5.9092054 -4.4518194 -2.8446989 -0.74270487 0.80610228 0.3555522 -0.46485567 -3.0960169 -4.704381 -6.4713416 -6.7450619 -7.3066554 -6.5626569][-6.9975634 -6.9268255 -6.6934648 -5.46924 -4.7251534 -3.2645755 -2.0880289 -1.8306313 -2.01401 -3.5891523 -4.7341409 -6.2132835 -6.6287746 -7.0385265 -6.2498717][-7.3208523 -7.6867929 -7.5367527 -6.2884331 -5.2375412 -4.4226871 -4.1686115 -4.1373415 -4.3950081 -4.8012743 -5.1273432 -5.7785807 -5.6459274 -6.149766 -6.6067019][-7.8387766 -8.9566717 -9.0266752 -8.2255039 -6.8350682 -5.8317614 -5.6985426 -6.1093979 -6.971447 -6.927855 -6.6836352 -6.728919 -6.6733651 -6.556613 -6.6347141]]...]
INFO - root - 2017-12-15 13:58:00.990378: step 20610, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 57h:53m:51s remains)
INFO - root - 2017-12-15 13:58:07.382094: step 20620, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 55h:04m:28s remains)
INFO - root - 2017-12-15 13:58:13.783759: step 20630, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:12m:07s remains)
INFO - root - 2017-12-15 13:58:20.204598: step 20640, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 56h:07m:24s remains)
INFO - root - 2017-12-15 13:58:26.602318: step 20650, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:59m:31s remains)
INFO - root - 2017-12-15 13:58:32.979652: step 20660, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 55h:35m:17s remains)
INFO - root - 2017-12-15 13:58:39.427079: step 20670, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 56h:52m:03s remains)
INFO - root - 2017-12-15 13:58:45.923924: step 20680, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 55h:57m:50s remains)
INFO - root - 2017-12-15 13:58:52.288974: step 20690, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 55h:17m:13s remains)
INFO - root - 2017-12-15 13:58:58.714445: step 20700, loss = 0.25, batch loss = 0.14 (13.1 examples/sec; 0.613 sec/batch; 53h:03m:28s remains)
2017-12-15 13:58:59.285302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514834 -3.823602 -5.0698929 -5.7287617 -6.0278926 -5.6959867 -5.1934814 -4.5164471 -3.6604767 -3.4003372 -4.8507733 -6.1507077 -7.048914 -7.1092973 -7.6142845][-4.8650036 -4.8973885 -5.3400526 -5.9062214 -6.894783 -6.8795552 -6.22806 -5.4687386 -4.6262989 -4.0495148 -5.2907314 -7.2074528 -8.6591854 -8.5841827 -9.3473511][-2.4908495 -2.9697309 -3.5346823 -4.0799103 -4.5132065 -4.8067694 -5.0298223 -4.66961 -4.2332411 -3.8547041 -5.3545713 -7.00324 -8.1564169 -8.2142687 -8.5756168][-2.4617448 -2.3740029 -2.6506338 -2.8596568 -3.0177193 -2.8389974 -2.7099195 -2.4051948 -2.0660648 -2.1817865 -3.9842865 -5.8881845 -7.3913512 -7.3155704 -8.247406][-1.9706888 -2.3056526 -2.3095074 -2.1918335 -1.4404283 -1.122077 -0.74542141 -0.28087521 -0.34661961 -0.71724892 -2.6878448 -4.9019985 -6.2133365 -5.8297167 -6.8832488][-2.7898169 -2.9610839 -2.5266705 -1.8082805 -0.59192705 0.75484228 1.7733502 2.4531674 2.7075095 1.9500728 -0.57966757 -3.2342172 -4.909574 -4.8277836 -5.3321862][-4.0348454 -3.9306407 -3.4423823 -1.7817044 0.19717836 2.1390691 3.4007354 3.8066497 3.5514016 2.5940166 -0.14431858 -3.242382 -5.249383 -5.4062119 -6.0967264][-4.5697012 -4.1773052 -3.3386526 -1.856205 0.50502825 2.5983739 4.318068 4.6895185 4.26022 2.8056941 -0.25486565 -3.4837317 -5.5731087 -6.1359043 -6.8707786][-5.5588522 -4.5570316 -3.0701098 -1.4959869 0.2437849 2.0715032 3.6091809 3.8945518 3.8417697 2.7470088 -0.55701208 -3.8595924 -6.0582323 -5.9531245 -6.7128072][-6.30691 -5.9442692 -4.4242878 -2.9336562 -1.1662087 0.80953932 1.7430415 1.8418794 1.573421 0.54367018 -2.083148 -4.847528 -6.574399 -6.7713633 -7.2807508][-6.7738638 -6.3571615 -5.0696487 -3.4552102 -2.1457577 -0.84200811 -0.23068094 -0.18080521 -0.21508217 -1.0374408 -3.6568618 -6.6252494 -7.86895 -7.5482945 -8.0388174][-6.6522217 -6.0307989 -5.0377975 -3.9769361 -3.2509727 -2.5716486 -2.3706264 -2.2521076 -2.2787714 -2.8516564 -4.146389 -6.3887596 -7.4017925 -7.7125611 -8.3356886][-7.3049326 -6.294116 -5.3974257 -4.5719757 -4.159831 -4.180934 -4.0220394 -4.2877908 -4.3656712 -4.8498096 -6.0091324 -7.3932576 -8.0285311 -7.8441238 -8.3932638][-7.3585162 -6.4116116 -5.6951828 -4.7972708 -4.7213469 -4.9131823 -5.2175341 -5.6413784 -5.9706917 -6.4390383 -6.64126 -7.4375186 -7.5623512 -7.3000665 -7.5114384][-7.4608564 -6.7860241 -6.0667887 -5.4359937 -5.3651752 -5.7842445 -6.3662295 -6.9200878 -7.4589758 -7.782198 -7.8809295 -7.556416 -7.0737677 -6.7614527 -6.8587074]]...]
INFO - root - 2017-12-15 13:59:05.826639: step 20710, loss = 0.24, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 58h:32m:45s remains)
INFO - root - 2017-12-15 13:59:12.344170: step 20720, loss = 0.37, batch loss = 0.26 (11.9 examples/sec; 0.673 sec/batch; 58h:16m:41s remains)
INFO - root - 2017-12-15 13:59:18.758723: step 20730, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 55h:23m:21s remains)
INFO - root - 2017-12-15 13:59:25.209070: step 20740, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 56h:40m:54s remains)
INFO - root - 2017-12-15 13:59:31.517181: step 20750, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:12m:05s remains)
INFO - root - 2017-12-15 13:59:38.001608: step 20760, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 56h:20m:04s remains)
INFO - root - 2017-12-15 13:59:44.402569: step 20770, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:31m:25s remains)
INFO - root - 2017-12-15 13:59:50.788495: step 20780, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.632 sec/batch; 54h:45m:58s remains)
INFO - root - 2017-12-15 13:59:57.237149: step 20790, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 54h:53m:31s remains)
INFO - root - 2017-12-15 14:00:03.569885: step 20800, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 54h:05m:42s remains)
2017-12-15 14:00:04.105460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5494547 -3.3103156 -4.05021 -4.2111864 -3.686203 -3.3348756 -3.4585 -3.0811572 -2.7540555 -3.4529252 -3.6617718 -5.2689991 -6.1997395 -6.1960216 -7.1716838][-2.8564916 -3.5980597 -4.4359837 -4.8218136 -4.6815457 -4.1561794 -3.4708304 -3.4374328 -3.5450816 -4.0711794 -4.2684984 -5.882484 -6.6635628 -6.9689875 -7.6002212][-3.5562887 -3.7773042 -4.3491087 -4.4960423 -4.6682558 -4.4880695 -4.0927291 -3.8520591 -3.2691011 -4.2168474 -4.8284683 -6.5215244 -7.1560597 -7.5383906 -8.3505831][-4.3550034 -4.5810137 -4.8051109 -4.4920349 -4.0933995 -3.8525143 -3.0416145 -2.5529895 -2.4893813 -3.375453 -3.9307961 -6.0047131 -7.0524144 -7.379971 -7.9765182][-5.492197 -5.2510281 -5.2911472 -4.7809606 -3.9234419 -3.0006833 -1.5894055 -1.1248169 -0.90042591 -2.0878973 -3.258821 -5.4768534 -6.6071281 -7.2092991 -8.0634451][-6.9511857 -6.4202833 -5.8369532 -4.4264064 -3.1132455 -1.5758367 -0.0047292709 0.72326326 0.93141413 -1.0477347 -2.8834739 -5.7469897 -6.8879309 -7.1957073 -7.6482363][-6.803103 -6.1937494 -5.6930461 -3.966063 -1.7742834 0.088126659 2.1944833 2.8551736 3.0289664 1.0948701 -1.0782323 -4.3293867 -6.0133252 -6.2352514 -6.9007196][-6.2928123 -5.8063455 -5.0950832 -3.134192 -1.117722 1.5544868 4.0027795 4.2927136 3.8354974 1.4436631 -0.28976679 -3.2277808 -4.861774 -5.2402334 -5.77757][-6.4859376 -5.8056455 -5.3835354 -3.9276488 -1.9413533 0.63783216 2.9929461 3.6911693 3.64813 0.94715071 -0.9657464 -3.8085761 -5.2205296 -5.4447002 -6.2384577][-6.38084 -5.9383183 -5.6436453 -4.3399172 -2.835887 -1.1083999 0.81953287 1.3214498 1.5128245 -0.73926687 -2.8271995 -5.3440833 -6.3158393 -6.1459394 -6.6184368][-7.8078928 -7.3194914 -6.8683052 -5.6887007 -4.5863514 -3.2378688 -2.0905161 -1.5764036 -1.2362671 -3.1313272 -3.9469326 -6.0244861 -7.0299377 -6.6063156 -6.8673334][-7.6565552 -7.3854985 -7.0864367 -6.159956 -5.5546122 -4.7548265 -3.79762 -3.7796183 -3.7323647 -4.30656 -4.534091 -5.8200984 -6.4931679 -6.7398891 -7.0250349][-8.7821884 -8.0303707 -7.3538647 -6.6783838 -6.1242919 -5.7773385 -5.4405527 -5.4061108 -5.0383825 -6.1546707 -6.438745 -6.5250597 -6.5630774 -6.5299454 -6.8346691][-8.8346739 -7.9917531 -7.2578425 -6.3262091 -5.6710882 -5.6188684 -5.1638403 -5.3393126 -5.3951445 -5.5755196 -5.9508939 -6.0807838 -6.004868 -6.3024225 -6.5308752][-8.7429609 -8.6584358 -8.1582708 -7.2118173 -6.581912 -6.373785 -6.5832925 -6.711926 -6.549489 -6.5913258 -6.6190333 -6.4575787 -6.2573867 -6.0686269 -5.7844453]]...]
INFO - root - 2017-12-15 14:00:10.524959: step 20810, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 55h:42m:46s remains)
INFO - root - 2017-12-15 14:00:16.849559: step 20820, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 55h:06m:42s remains)
INFO - root - 2017-12-15 14:00:23.161815: step 20830, loss = 0.35, batch loss = 0.24 (12.8 examples/sec; 0.623 sec/batch; 53h:55m:19s remains)
INFO - root - 2017-12-15 14:00:29.460378: step 20840, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 53h:27m:04s remains)
INFO - root - 2017-12-15 14:00:35.883116: step 20850, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 55h:37m:40s remains)
INFO - root - 2017-12-15 14:00:42.339184: step 20860, loss = 0.29, batch loss = 0.18 (11.5 examples/sec; 0.694 sec/batch; 60h:04m:12s remains)
INFO - root - 2017-12-15 14:00:48.680669: step 20870, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 56h:16m:40s remains)
INFO - root - 2017-12-15 14:00:55.082397: step 20880, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.648 sec/batch; 56h:07m:23s remains)
INFO - root - 2017-12-15 14:01:01.457142: step 20890, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:56m:26s remains)
INFO - root - 2017-12-15 14:01:07.866947: step 20900, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 54h:48m:42s remains)
2017-12-15 14:01:08.444708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6945958 -6.7727852 -7.2674618 -7.113112 -7.1981425 -6.9667892 -5.9753132 -4.7300568 -3.7640727 -5.3169441 -6.0687046 -5.8689537 -6.0351486 -7.0293226 -7.7551856][-6.19489 -6.8349972 -7.2895079 -7.746594 -8.1319666 -7.9774761 -7.2321291 -6.2213416 -4.9309325 -6.3869448 -7.0943046 -6.9979725 -7.4965692 -8.3035288 -8.9093933][-5.2708354 -5.8479466 -6.1265025 -6.5759335 -7.1472883 -7.3597627 -7.1696572 -6.4315252 -5.7523732 -7.2832937 -7.5745192 -7.5770669 -7.7288961 -8.6171494 -9.2814808][-6.2139382 -5.9925923 -5.9870443 -5.9112964 -6.1167636 -5.6237354 -4.7928677 -4.2686653 -3.8374987 -5.9080215 -6.8466473 -7.293438 -7.8687091 -8.5446682 -8.9978781][-7.3461256 -6.7257552 -6.0481539 -5.082303 -4.707674 -3.4696937 -2.7080612 -2.2325144 -1.7619581 -4.28822 -5.7398124 -6.5576148 -7.1963091 -8.0219679 -8.7233162][-8.8838234 -7.8441687 -6.7908492 -4.7459621 -2.7364445 -1.0949812 0.40587807 0.64668083 0.78830624 -2.0232782 -4.0148611 -5.2005634 -6.0056219 -7.125514 -8.4907646][-8.9270325 -7.5621638 -5.5481949 -2.8252654 -0.28748322 2.2557945 4.1946373 4.5744867 4.48514 0.67641354 -2.1914806 -3.6935072 -4.8058777 -5.9667959 -6.8217254][-8.1317825 -6.9481916 -5.4748974 -2.4239349 1.0170908 3.0685825 5.0624781 6.0217323 5.9945889 1.7174311 -1.7598886 -3.7024267 -4.7175283 -5.7171025 -6.5234318][-8.73707 -8.1725378 -6.9991865 -4.1510568 -1.3405566 1.109458 3.0895033 3.5191956 3.7418661 0.36751842 -2.6454825 -4.2390156 -5.0795336 -5.8246441 -6.0891662][-9.9247723 -9.1635971 -7.875124 -5.3948364 -2.3069634 -0.33642817 1.0634089 1.6080055 1.3456841 -2.025403 -4.1847134 -5.4437814 -6.754806 -7.208899 -7.0596724][-11.102274 -10.529671 -9.9187393 -7.9563031 -6.2894273 -4.1895509 -2.2132215 -2.3491707 -2.8939281 -5.5589967 -7.0440125 -7.4978142 -8.13166 -8.6773357 -8.8638229][-11.567226 -10.799854 -9.913887 -9.1986313 -8.4855356 -7.1857376 -6.2348666 -5.5555038 -5.0034237 -7.4758873 -8.6955938 -8.8282108 -9.0605564 -8.9134426 -8.7669706][-11.916698 -11.393078 -10.680811 -9.7473021 -8.74848 -7.9757457 -7.2105365 -7.037221 -7.263978 -8.8552351 -9.6635027 -9.5743341 -9.7079124 -9.25357 -8.6645489][-10.497795 -9.967041 -9.5413218 -8.8073549 -7.9879861 -7.2334456 -6.6198153 -6.6679583 -6.8958707 -8.0757151 -8.8016415 -9.0020781 -9.5032969 -9.2823448 -8.7268944][-10.092583 -9.4325743 -8.8656054 -8.4596834 -8.0910387 -7.5683103 -6.9891677 -7.1680713 -7.4483981 -7.6193576 -7.7333169 -7.6633916 -7.7512689 -7.7904038 -7.4405437]]...]
INFO - root - 2017-12-15 14:01:14.957974: step 20910, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 55h:08m:05s remains)
INFO - root - 2017-12-15 14:01:21.294408: step 20920, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 57h:14m:19s remains)
INFO - root - 2017-12-15 14:01:27.765474: step 20930, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 56h:18m:31s remains)
INFO - root - 2017-12-15 14:01:34.133476: step 20940, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 53h:49m:44s remains)
INFO - root - 2017-12-15 14:01:40.522565: step 20950, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:59m:11s remains)
INFO - root - 2017-12-15 14:01:46.853412: step 20960, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 56h:33m:06s remains)
INFO - root - 2017-12-15 14:01:53.292477: step 20970, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 57h:23m:32s remains)
INFO - root - 2017-12-15 14:01:59.737990: step 20980, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:55m:04s remains)
INFO - root - 2017-12-15 14:02:06.119871: step 20990, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 54h:33m:39s remains)
INFO - root - 2017-12-15 14:02:12.538234: step 21000, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 56h:53m:42s remains)
2017-12-15 14:02:13.074685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3437257 -3.827208 -4.6524186 -5.3974662 -5.7928562 -5.4514637 -4.8754559 -4.9735355 -4.7277689 -4.58086 -5.4623661 -6.7958131 -7.5428276 -8.5835838 -9.7582588][-3.6320767 -4.4777393 -5.3789978 -6.7872658 -7.4351192 -7.5524035 -7.9706631 -7.6354613 -6.8488593 -7.1160932 -7.7726731 -8.1647215 -8.8580084 -9.6624784 -9.661562][-3.3916259 -4.2184439 -4.8798785 -5.8962455 -6.6768079 -7.5880008 -8.2158318 -8.2331095 -8.0594 -8.1553354 -8.1599646 -8.4440594 -8.949913 -9.2329264 -9.3441458][-3.3353009 -3.8578107 -4.484189 -4.9835396 -5.2461853 -5.5803442 -5.7879949 -6.1619439 -5.9940763 -5.9231434 -6.1370697 -6.7090111 -7.3675613 -8.1906509 -9.4569511][-3.227551 -3.4194188 -3.2319016 -3.4673166 -3.6133194 -2.9161954 -2.3324256 -2.6210184 -2.4182763 -3.021173 -4.2018256 -5.3547392 -6.3507419 -7.4949236 -8.5223894][-2.6811376 -2.9740343 -3.1235943 -2.4915938 -1.3519359 -0.53392935 -0.0026097298 0.45290947 1.0410786 -0.069033623 -1.2710767 -2.9031758 -5.1704693 -6.3386421 -7.1941586][-3.116291 -3.3287277 -2.6904845 -1.4972954 -0.015432358 1.0812769 1.9085598 2.6138973 3.2640095 2.5888996 1.1894293 -1.3854003 -3.7276752 -5.4544253 -7.0619311][-2.3322625 -2.4569502 -2.4562697 -1.6034422 0.38511848 2.1707659 3.3250046 3.7295303 4.1250324 3.1918564 1.2434893 -1.1298809 -3.1741824 -5.326045 -6.9502378][-2.9047861 -2.5547662 -3.1133981 -1.8956089 -0.23659563 1.2280207 2.6642218 3.4084826 3.9707041 2.1500225 -0.69714451 -2.9283452 -4.8772106 -6.0830054 -6.7334223][-3.8104866 -3.7941949 -3.7725244 -2.883903 -1.9820395 -1.1738219 -0.13691092 1.0985699 1.7834339 0.20245457 -1.9497571 -4.2063713 -6.4689345 -7.3392515 -7.4326172][-6.388144 -6.1233845 -5.95919 -5.2387295 -4.317977 -3.6307859 -2.6203651 -2.2294345 -1.9165277 -2.3369222 -3.8662016 -5.3610349 -6.7659225 -7.6175237 -8.03173][-6.6164026 -5.9655933 -6.3839035 -6.0334182 -5.7612591 -5.3199081 -4.8052626 -4.8486633 -4.2716837 -4.7142177 -5.848495 -6.29623 -6.7893605 -7.2073269 -7.3056316][-7.7268929 -7.9024611 -7.4913483 -6.8868213 -6.7221222 -6.6460485 -6.3293934 -6.36659 -6.6233382 -6.8893375 -6.9466095 -6.8744631 -6.7108555 -6.1853724 -5.7691426][-8.1571779 -7.7352347 -7.4987564 -7.2394495 -7.1419544 -6.8292646 -6.5809984 -6.9023952 -6.983716 -7.06971 -7.3463697 -7.4374833 -7.0660763 -6.2621818 -5.7132597][-8.0583858 -8.3229895 -8.8384476 -8.6427851 -8.7938385 -8.4474573 -8.4098415 -8.5692253 -8.355793 -8.3733692 -8.4432487 -8.0213861 -7.3591609 -6.9754992 -6.7698245]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:02:19.533737: step 21010, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 54h:40m:23s remains)
INFO - root - 2017-12-15 14:02:25.881013: step 21020, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.617 sec/batch; 53h:24m:01s remains)
INFO - root - 2017-12-15 14:02:32.268190: step 21030, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 55h:24m:01s remains)
INFO - root - 2017-12-15 14:02:38.603586: step 21040, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 56h:06m:07s remains)
INFO - root - 2017-12-15 14:02:44.993592: step 21050, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 56h:28m:51s remains)
INFO - root - 2017-12-15 14:02:51.224111: step 21060, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 54h:27m:25s remains)
INFO - root - 2017-12-15 14:02:57.652735: step 21070, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 56h:27m:21s remains)
INFO - root - 2017-12-15 14:03:04.115091: step 21080, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.675 sec/batch; 58h:24m:58s remains)
INFO - root - 2017-12-15 14:03:10.532802: step 21090, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 54h:41m:31s remains)
INFO - root - 2017-12-15 14:03:16.910768: step 21100, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:01m:12s remains)
2017-12-15 14:03:17.425348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5159187 -6.4064283 -6.5616369 -6.2542562 -5.7093148 -5.2563238 -4.4172373 -3.9324231 -3.593091 -4.8369017 -5.6423569 -6.3662357 -7.3362546 -8.02339 -8.7105093][-6.0617957 -5.9546809 -6.5052242 -6.9450421 -6.9811492 -6.2169309 -5.2839441 -4.4790335 -3.3297119 -3.617877 -4.5560207 -5.0418282 -5.8726549 -6.4767132 -7.1150851][-2.7295804 -3.5901279 -4.342587 -4.9831305 -5.1664872 -4.9663954 -4.6449642 -3.4576397 -2.548737 -3.0672555 -3.0065536 -3.1161208 -3.879823 -4.3362465 -5.2107592][-1.845664 -1.7851558 -2.5013266 -2.7773962 -3.0572333 -3.4055662 -2.9158869 -2.2694659 -1.4832559 -1.8947949 -1.7774053 -2.2609715 -3.207273 -3.6280499 -4.1309261][-1.9087448 -1.3996253 -1.3691597 -1.1948085 -1.4497194 -1.6495409 -1.186223 -1.1790128 -0.82388687 -1.4028826 -1.3931208 -1.6124587 -2.6579161 -3.2372303 -3.8580651][-1.5659022 -1.0878253 -0.3972249 0.441782 1.4917707 1.7300386 1.8543444 1.3270187 1.1007423 0.028077126 -0.86473417 -1.7847705 -2.8221211 -3.6157451 -4.7545071][-1.1620965 -1.2152815 -0.48395681 0.97479153 1.9440727 2.9979305 3.679882 3.1401834 2.7701101 0.76652718 -0.83629131 -2.0598116 -3.1786623 -3.6134782 -4.325758][-0.37945032 -0.19136095 0.06661129 1.5748196 2.6454735 3.5818768 3.5100603 3.1578426 2.725193 0.95475674 -0.29879141 -1.5284295 -2.6415977 -3.2637258 -4.2087946][-0.16951847 -0.059082985 0.32943439 1.1454344 2.0559998 2.5357552 2.6888266 2.4577827 2.1219692 0.35138988 -0.84780836 -1.8636003 -3.2182088 -3.9056489 -4.5762348][-1.3940125 -0.85897064 -0.49278116 -0.35097647 0.14226913 0.64539623 0.762517 0.97401905 1.0181494 -0.61280823 -1.4707589 -2.727005 -4.1493464 -5.1262293 -6.0449419][-4.0417004 -3.5083013 -3.0710468 -2.8919415 -2.7703686 -2.6380215 -2.5173998 -2.34944 -2.4833078 -3.5499148 -4.3231277 -5.0352545 -5.8932285 -6.8754797 -7.3413224][-5.934062 -5.5550442 -5.1006384 -4.850296 -4.7825212 -5.0214987 -4.9979072 -4.6688671 -4.3777456 -5.1961346 -5.8280106 -6.4064379 -6.8055844 -7.3619366 -7.6492739][-7.4109612 -6.8437338 -6.3119955 -6.262846 -5.9064193 -6.0516357 -6.4404263 -6.4680791 -6.0947165 -6.118206 -6.7060885 -7.00259 -6.87754 -7.1491594 -7.1580811][-7.1823883 -6.9356136 -6.7756724 -6.571022 -6.3564229 -6.4237571 -6.4737806 -6.4400058 -6.6142411 -6.4980683 -6.4738569 -6.6249552 -6.6441383 -7.1880245 -6.7805967][-7.57112 -7.0105538 -7.15835 -7.3451242 -7.0538125 -6.86121 -6.9824076 -7.2382679 -7.30784 -6.710588 -6.4580078 -6.4390826 -5.7739196 -5.8347921 -6.0299549]]...]
INFO - root - 2017-12-15 14:03:23.778570: step 21110, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 56h:11m:42s remains)
INFO - root - 2017-12-15 14:03:30.123408: step 21120, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 54h:59m:21s remains)
INFO - root - 2017-12-15 14:03:36.514748: step 21130, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.618 sec/batch; 53h:24m:37s remains)
INFO - root - 2017-12-15 14:03:42.943057: step 21140, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 57h:33m:13s remains)
INFO - root - 2017-12-15 14:03:49.289363: step 21150, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 55h:07m:07s remains)
INFO - root - 2017-12-15 14:03:55.581523: step 21160, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 55h:15m:13s remains)
INFO - root - 2017-12-15 14:04:01.963040: step 21170, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 56h:03m:53s remains)
INFO - root - 2017-12-15 14:04:08.456716: step 21180, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.681 sec/batch; 58h:55m:15s remains)
INFO - root - 2017-12-15 14:04:14.773565: step 21190, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 55h:44m:57s remains)
INFO - root - 2017-12-15 14:04:21.222476: step 21200, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 56h:58m:37s remains)
2017-12-15 14:04:21.750266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6250305 -3.3620148 -3.6876912 -4.1181879 -4.423811 -4.4513063 -4.76585 -4.7252793 -4.6128 -5.1977558 -6.4808578 -7.9451327 -8.4581013 -9.5801268 -10.211548][-3.697516 -3.8828781 -3.855345 -4.0317144 -4.8068218 -5.21749 -5.1821518 -5.0635834 -4.7020292 -4.5089374 -5.3436375 -6.9726229 -8.1254063 -9.20725 -9.4006376][-4.52366 -3.8137314 -3.8236473 -4.3528466 -4.3725181 -4.268208 -3.9382305 -3.5245614 -3.4945273 -3.682241 -4.5489674 -5.87584 -6.7335176 -7.7731075 -8.3179283][-5.4907179 -4.9826632 -4.5160341 -3.6258464 -3.4599905 -2.7903271 -1.8410449 -1.617023 -1.119771 -1.5159483 -3.010366 -4.6764607 -5.8040619 -6.6248531 -7.1224556][-6.3145208 -5.3444152 -4.6324687 -3.6566215 -2.7440848 -1.7715526 -0.1875267 -0.032247066 0.58113766 0.52348423 -1.263588 -3.1757283 -4.6186385 -5.7435713 -6.4198675][-5.5259752 -5.0985212 -4.0877867 -2.5416365 -1.0903769 0.29734516 1.3481617 1.5196857 1.8621006 1.0738449 -0.79577017 -2.6188135 -4.1782131 -5.1994977 -5.6689787][-5.2777748 -4.2976732 -2.9969931 -1.6563029 -0.23421383 1.5643196 2.8722515 2.9464149 2.8846674 1.7316141 -0.37659264 -2.7231951 -4.1911106 -5.5090075 -5.9533644][-4.1735125 -3.2356339 -2.1972566 -0.72451067 0.62001419 2.0186224 3.0971279 3.3324995 3.3265648 1.8367815 -0.3918829 -2.5886855 -4.2935438 -5.6815834 -6.2231007][-3.5820761 -2.2769065 -0.84318924 0.24643755 1.1702814 2.5533342 3.3335485 3.3738756 3.3125734 1.9075546 -0.72434282 -3.4984741 -5.0852547 -6.1632309 -6.6256685][-2.5297942 -1.297811 -0.014667511 0.93966675 1.7252064 2.5896959 2.8106632 2.7946854 2.6255856 1.1228161 -1.3351903 -3.5412903 -4.7893224 -5.9366374 -6.8560724][-2.8061552 -2.34554 -1.6157351 -0.42510128 0.38057137 1.1058016 1.4588118 1.2424641 0.83795643 -0.33115721 -2.3330469 -4.0594306 -5.562645 -6.3241739 -6.7245188][-3.4108958 -2.5822449 -1.6815391 -0.850986 -0.2153182 0.201334 0.62360191 0.51688576 -0.066086769 -1.4613266 -3.2808681 -4.4077997 -5.400732 -6.1989827 -6.8732605][-4.0305281 -3.6207108 -2.9868398 -2.0054131 -1.3383613 -0.8481245 -0.73435545 -1.0976019 -1.7135792 -2.4779854 -4.09049 -5.0977364 -5.6793423 -5.8748169 -6.1064091][-5.14371 -4.7288704 -3.9861844 -3.2778645 -2.972486 -2.529057 -2.0943103 -2.3025613 -2.6897612 -3.1527314 -4.1941423 -4.5460472 -5.0094213 -5.4474034 -5.6001778][-6.1158257 -5.6001253 -5.3785663 -4.5376158 -3.7676692 -3.4076777 -3.2234826 -3.5689139 -3.8584874 -3.9553673 -4.3572283 -4.6863222 -5.0991311 -5.3026028 -5.3157825]]...]
INFO - root - 2017-12-15 14:04:28.241677: step 21210, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 55h:52m:05s remains)
INFO - root - 2017-12-15 14:04:34.611176: step 21220, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 53h:37m:07s remains)
INFO - root - 2017-12-15 14:04:40.969229: step 21230, loss = 0.30, batch loss = 0.18 (13.0 examples/sec; 0.617 sec/batch; 53h:22m:36s remains)
INFO - root - 2017-12-15 14:04:47.298992: step 21240, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 53h:44m:36s remains)
INFO - root - 2017-12-15 14:04:53.680486: step 21250, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:47m:28s remains)
INFO - root - 2017-12-15 14:04:59.999535: step 21260, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 54h:06m:32s remains)
INFO - root - 2017-12-15 14:05:06.409413: step 21270, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 56h:52m:04s remains)
INFO - root - 2017-12-15 14:05:12.788979: step 21280, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 56h:23m:51s remains)
INFO - root - 2017-12-15 14:05:19.158938: step 21290, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 55h:00m:06s remains)
INFO - root - 2017-12-15 14:05:25.491494: step 21300, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 56h:28m:19s remains)
2017-12-15 14:05:26.023451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1380038 -2.7465067 -2.7927728 -3.1958337 -3.2434888 -3.1791754 -3.1290584 -3.0085721 -2.8117676 -2.8993282 -3.7237194 -5.1342964 -6.3142776 -7.5504112 -8.5026932][-3.2596698 -3.3070545 -3.6394696 -4.2022667 -4.6397014 -4.7125683 -4.4811277 -3.9950855 -3.2659502 -3.3138885 -3.8067696 -5.2101974 -6.0534649 -7.271028 -8.11322][-2.326262 -2.5426903 -3.0316834 -3.6597223 -4.0227218 -4.0459805 -3.8990383 -3.7366977 -3.5281453 -3.5718827 -3.9301975 -4.7286587 -5.06089 -6.1509161 -6.8350649][-2.0191231 -2.2993126 -2.7769308 -3.1328902 -3.2950935 -2.9603095 -2.6909313 -2.5535626 -2.5052528 -2.7412519 -3.1982627 -4.3450146 -4.8483663 -5.6819625 -6.0297184][-2.0709863 -1.9223771 -2.039413 -2.2521429 -2.2072706 -1.4951758 -0.90617466 -0.65250874 -0.59096575 -1.4575009 -2.1285172 -3.3624005 -4.0473051 -4.7517347 -5.3414812][-2.9598756 -2.8322558 -2.6347995 -1.9157996 -1.0300479 0.3131752 1.5230989 1.9767485 2.1056852 1.163763 0.37234402 -1.2388034 -2.5589633 -3.9920063 -5.2575803][-4.1928811 -3.5727143 -2.6364636 -1.5951743 -0.2172246 1.1605358 2.342411 2.7466345 3.1024933 2.3257666 1.5875874 -0.3809948 -2.1114049 -3.4737816 -4.7078485][-4.3751802 -3.218451 -2.1963511 -1.0262322 0.6063118 2.0657845 3.3737183 3.8133917 4.0271454 3.3485126 2.5913849 0.45064354 -1.3977308 -2.8390994 -3.9716825][-4.6454306 -3.5019231 -2.5237823 -1.2910562 -0.032171726 1.4215717 2.870388 3.670743 4.1983414 3.4131918 2.2037668 0.0077662468 -1.7311091 -2.9661708 -3.7103207][-5.3603239 -4.279583 -3.2859325 -2.0262022 -0.91254282 0.11358881 1.2167749 1.7493296 2.2526627 1.4357901 0.44239807 -1.3902259 -2.8858905 -3.9209752 -4.5942411][-6.4793139 -5.7861958 -5.366641 -4.1789818 -3.1013732 -2.0551472 -1.174603 -0.84452343 -0.41388845 -1.3417902 -2.14966 -3.4581981 -4.4492283 -5.2843595 -5.8047147][-8.3299608 -7.89828 -7.3734989 -6.8146305 -5.931242 -4.9166603 -3.93878 -3.1699262 -2.5258222 -3.464694 -4.2952719 -5.2307377 -5.9789076 -6.5264554 -7.1893067][-8.6157188 -8.5766249 -8.308095 -7.789722 -7.0684452 -6.349453 -5.6293879 -5.2346544 -4.8357058 -5.2216949 -5.5215092 -5.9415464 -6.438633 -6.810514 -6.8463593][-7.7802629 -7.7742915 -7.9045758 -7.7197161 -7.1229534 -6.353857 -5.9212265 -5.9300141 -6.0845957 -6.6682253 -7.0352745 -6.968576 -6.6871147 -6.4853473 -6.4359994][-8.1709166 -8.0366488 -7.83062 -7.6610432 -7.3789291 -6.99337 -6.6830435 -6.9478941 -7.3193965 -7.6065669 -7.7903266 -7.6816096 -7.3691092 -7.0146646 -6.57057]]...]
INFO - root - 2017-12-15 14:05:32.349730: step 21310, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 54h:55m:20s remains)
INFO - root - 2017-12-15 14:05:38.778516: step 21320, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 55h:28m:07s remains)
INFO - root - 2017-12-15 14:05:45.132605: step 21330, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 53h:13m:55s remains)
INFO - root - 2017-12-15 14:05:51.537896: step 21340, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 56h:56m:16s remains)
INFO - root - 2017-12-15 14:05:57.924339: step 21350, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 54h:11m:31s remains)
INFO - root - 2017-12-15 14:06:04.280291: step 21360, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 55h:18m:12s remains)
INFO - root - 2017-12-15 14:06:10.682596: step 21370, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 54h:58m:12s remains)
INFO - root - 2017-12-15 14:06:17.073396: step 21380, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 54h:25m:50s remains)
INFO - root - 2017-12-15 14:06:23.410557: step 21390, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 53h:27m:35s remains)
INFO - root - 2017-12-15 14:06:29.836739: step 21400, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 54h:19m:17s remains)
2017-12-15 14:06:30.366259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3983469 -3.6484776 -2.966866 -1.706512 -0.88806057 0.14348507 0.94677258 1.290267 1.5164852 0.1040988 -0.96745729 -3.4184513 -5.1572509 -6.94102 -8.7991848][-4.1016016 -3.4048028 -2.9163761 -2.1041861 -1.2153435 -0.084341049 0.82223225 0.96008587 1.0362101 -0.57458591 -1.5802569 -3.7186782 -5.0339513 -6.5597482 -8.195118][-4.6492615 -4.0703969 -3.3536291 -2.1576314 -1.2050557 -0.29549313 0.47115326 0.74770546 1.0025005 -0.4623313 -1.6027517 -3.8404019 -5.0408287 -6.1076083 -7.5142822][-5.023325 -4.2584553 -3.2364826 -2.1616096 -1.2392616 -0.17015314 0.78352833 0.83593082 0.74828911 -0.8516922 -1.6638188 -3.9099007 -5.4338112 -6.3109775 -7.2729373][-4.9699793 -3.9137611 -2.8366961 -1.734827 -0.7813344 0.13429594 0.96974087 1.1282454 1.1802082 -0.60810852 -1.7101669 -3.8242157 -5.1468778 -6.1666079 -7.3773527][-5.2620554 -4.163929 -3.0331368 -1.4448147 -0.22821474 0.68519211 1.5167809 1.7343378 1.9808626 0.077685833 -1.1079979 -3.4533606 -4.9610033 -5.9344854 -7.1870179][-5.2025638 -4.2279367 -3.0795765 -1.3694577 0.079042912 1.3663969 2.3914137 2.6065969 2.7119904 0.64447784 -0.55282307 -3.3216438 -5.41698 -6.5701675 -7.7071552][-4.3231344 -3.0849628 -1.8066974 -0.4151001 0.69035435 1.81816 2.7745285 3.1838903 3.3656149 1.1265888 -0.28023148 -2.9875021 -4.9794168 -6.5300984 -7.8018312][-3.2902832 -2.4252725 -1.3667803 0.18288946 1.3013773 2.3684254 3.2266645 3.5768805 3.6309614 1.3460045 -0.088210583 -2.870935 -4.798913 -6.1645632 -7.35356][-2.9954605 -2.2484374 -1.1859422 0.13570738 1.2458878 2.4139137 2.9570465 2.873497 2.7063637 0.282681 -1.2650046 -3.9936762 -5.8301253 -6.8156013 -7.6238761][-3.2180934 -2.2405291 -0.97558355 0.37398815 1.4219208 2.3282633 2.4932938 2.055048 1.5396118 -1.0958972 -2.634285 -5.1422615 -6.9542 -7.7192268 -8.0721149][-3.2261992 -1.8296409 -0.18154001 1.0584717 1.9838724 2.6763515 2.5337896 1.7722111 0.86851311 -1.6575351 -3.4181776 -5.7813911 -7.2020183 -7.9081006 -8.3576317][-3.1016679 -1.700747 -0.033714294 1.4044876 2.5217218 2.8631 2.429287 1.2048502 0.021861553 -2.16009 -3.8563573 -5.8923335 -7.4363222 -7.8562508 -8.1060028][-3.4137702 -2.3039737 -0.92807293 0.33937454 1.2411556 1.7786512 1.3786888 0.12769318 -0.9390111 -2.7318048 -4.0794621 -5.5253515 -6.6968069 -7.3189192 -7.6411529][-4.8794436 -3.5292125 -1.9544568 -1.0265532 -0.22945023 -0.14742947 -0.92352533 -1.8972611 -2.9546609 -4.3629103 -5.620687 -6.7587695 -7.5492897 -7.6798649 -7.6238604]]...]
INFO - root - 2017-12-15 14:06:36.764378: step 21410, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 53h:31m:31s remains)
INFO - root - 2017-12-15 14:06:43.208186: step 21420, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 53h:49m:36s remains)
INFO - root - 2017-12-15 14:06:49.538769: step 21430, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 54h:05m:04s remains)
INFO - root - 2017-12-15 14:06:55.878362: step 21440, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:45m:46s remains)
INFO - root - 2017-12-15 14:07:02.176335: step 21450, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 54h:58m:35s remains)
INFO - root - 2017-12-15 14:07:08.536802: step 21460, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 55h:44m:31s remains)
INFO - root - 2017-12-15 14:07:15.020219: step 21470, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 54h:41m:32s remains)
INFO - root - 2017-12-15 14:07:21.380442: step 21480, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 56h:15m:34s remains)
INFO - root - 2017-12-15 14:07:27.766938: step 21490, loss = 0.23, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 54h:11m:31s remains)
INFO - root - 2017-12-15 14:07:34.141684: step 21500, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 56h:47m:48s remains)
2017-12-15 14:07:34.689445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7920132 -5.4167571 -5.7359428 -5.821651 -5.9017711 -5.4193234 -4.5378733 -4.0508289 -3.0577674 -2.7087345 -2.3871355 -3.6146708 -3.6753502 -5.54753 -7.4468174][-4.9702873 -5.3218174 -5.4509478 -5.6609135 -6.0020046 -5.86298 -5.6419764 -5.1743917 -4.1701069 -4.0326357 -3.8191071 -5.14544 -5.4481339 -7.1288095 -8.2855473][-6.0355949 -5.4069042 -4.6744332 -5.0957766 -5.376399 -5.4073305 -5.2482276 -4.4042492 -3.8458331 -4.2080507 -3.9624243 -5.396132 -5.4175444 -6.4193239 -7.41786][-6.6098642 -5.9554682 -5.3260121 -4.6167603 -3.7394032 -3.1138282 -2.8041162 -2.1758585 -1.3119926 -2.0897446 -2.675837 -4.6215649 -4.8820457 -6.1248555 -6.9673491][-6.8448663 -5.7846737 -4.8087716 -3.3826871 -2.3689685 -1.1291161 -0.12936926 0.087197304 0.613801 -0.17914724 -1.1586466 -3.4935389 -4.1148758 -6.0767069 -7.2415824][-6.5566211 -5.8636618 -4.9735527 -2.4268813 -0.41437674 1.1611729 2.4918337 2.6509075 2.6681395 1.4713392 0.21514511 -2.2048798 -2.8270631 -4.7466507 -6.0958967][-6.7490273 -5.3731079 -3.9997792 -2.0065565 -0.62050629 1.8363008 3.8262901 4.3608456 4.2463427 2.7577314 1.1990147 -1.4380031 -2.3878927 -4.0613327 -5.50922][-6.0763941 -5.5245972 -4.4895687 -1.3425126 0.67490768 2.7194548 3.93882 4.3956871 4.7088528 3.3787203 1.4487143 -1.2990909 -2.6284256 -4.1853166 -5.4064379][-4.9656324 -4.3137364 -4.3427711 -2.6785235 -0.96796322 1.8115835 3.4496593 3.4280863 3.2890215 2.4425402 0.98129368 -1.9186988 -3.0975184 -4.6603289 -5.3058367][-4.8379107 -4.2594633 -3.9213219 -2.628921 -2.2525306 -0.85065413 1.052309 1.8700027 1.8558912 0.018102646 -1.273921 -2.8997178 -3.2735968 -5.2397938 -5.6284628][-4.775465 -5.105711 -5.2495265 -4.2782497 -3.5032225 -2.7830458 -2.3357267 -2.2776141 -2.0087266 -2.3446689 -3.079535 -4.0901976 -4.273572 -5.0537252 -5.7230711][-5.6622648 -5.878653 -6.0188775 -5.8284378 -5.7810535 -4.8184805 -4.1293192 -4.2864962 -4.8451719 -4.8267317 -5.3790822 -5.6261463 -5.2396712 -5.4592609 -5.3240538][-6.2709408 -6.6782994 -6.9275684 -6.9688282 -7.4028125 -7.0895047 -6.6226897 -5.7820559 -5.2257795 -5.0988317 -5.9472132 -6.1707778 -5.5655508 -5.6874409 -5.2980251][-6.17578 -6.015666 -6.2467585 -6.3169212 -6.7110634 -6.9709082 -7.0445237 -6.9699688 -6.6611137 -6.2222395 -5.6735735 -5.6507974 -5.2424555 -5.3792086 -5.4359932][-5.5805902 -5.197855 -5.5474105 -5.7328377 -6.0931592 -6.711019 -6.8975687 -7.295814 -7.462842 -7.0380282 -6.7059312 -6.2526035 -5.8555164 -5.6526923 -5.6124954]]...]
INFO - root - 2017-12-15 14:07:41.122763: step 21510, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 56h:02m:41s remains)
INFO - root - 2017-12-15 14:07:47.575212: step 21520, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 56h:53m:55s remains)
INFO - root - 2017-12-15 14:07:53.971044: step 21530, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 53h:37m:56s remains)
INFO - root - 2017-12-15 14:08:00.387579: step 21540, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 54h:44m:28s remains)
INFO - root - 2017-12-15 14:08:06.832105: step 21550, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 54h:04m:34s remains)
INFO - root - 2017-12-15 14:08:13.155441: step 21560, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 54h:33m:29s remains)
INFO - root - 2017-12-15 14:08:19.473360: step 21570, loss = 0.37, batch loss = 0.25 (13.0 examples/sec; 0.616 sec/batch; 53h:14m:13s remains)
INFO - root - 2017-12-15 14:08:25.878549: step 21580, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 56h:21m:53s remains)
INFO - root - 2017-12-15 14:08:32.258595: step 21590, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 54h:03m:13s remains)
INFO - root - 2017-12-15 14:08:38.607446: step 21600, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 55h:35m:29s remains)
2017-12-15 14:08:39.141400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.684804 -5.92723 -5.8092775 -5.5360985 -4.969305 -4.26886 -2.8254533 -2.0443592 -1.6183357 -2.1044755 -3.0128274 -4.196919 -6.1035089 -7.1857367 -8.1261845][-4.9753547 -5.1499739 -5.2349315 -5.2058191 -4.966032 -4.20743 -3.0693564 -2.4996533 -1.2541513 -1.7556162 -2.3863916 -3.7189946 -5.1365194 -6.7708673 -7.888845][-4.9778118 -4.4936461 -4.5481176 -4.9012418 -4.7575893 -4.1281357 -3.1311083 -2.4887342 -1.6156502 -1.9432693 -2.6185408 -3.5395656 -4.5707903 -5.3747005 -6.8567066][-5.4631124 -4.8604393 -3.9664478 -3.720525 -3.3522277 -2.8507938 -1.7826886 -1.1142507 -0.536818 -1.1041365 -2.6109519 -3.1117172 -4.4197206 -4.8954163 -6.0646658][-6.1346164 -4.7272596 -3.794241 -3.1520934 -2.2998838 -1.165978 0.0016889572 0.3268404 0.68518066 -0.34270906 -1.7342601 -2.8907185 -4.0741582 -4.9588375 -6.0742788][-5.193244 -4.0493188 -3.4765048 -2.4853172 -1.4379883 -0.14087772 1.0825281 1.5837164 1.5944719 0.14118814 -1.4722586 -2.8602371 -3.9199393 -4.6825919 -5.95362][-3.0760059 -2.2180543 -1.6355515 -0.71277952 0.52473736 1.8073664 2.8067007 2.8818426 2.5366888 1.0618334 -0.62276649 -2.4659061 -4.2450981 -5.1367168 -5.7485971][-1.4663029 -0.67226267 0.040398121 0.89088345 2.2865973 3.5024729 4.45343 4.7430992 4.2953568 2.3232546 0.16874743 -1.5667634 -3.7146266 -5.0457587 -5.9693065][-1.4346442 -0.4185276 0.35450935 1.3975058 2.9320803 3.9741497 4.8411264 4.8332548 4.5407028 2.5548038 -0.077245712 -2.1565714 -4.0897379 -5.1299314 -5.896821][-2.7361727 -1.485404 -0.31904268 0.5984621 2.0459013 3.3977261 4.7373371 4.6154451 4.1932478 1.9566593 -0.30877829 -2.328392 -4.24129 -5.364779 -6.2109203][-4.4953527 -3.4305148 -2.0885987 -0.99443865 0.040717125 1.5985403 3.2961493 3.6650934 3.2984314 0.969347 -1.2192512 -2.9709463 -4.5848904 -5.8454814 -7.0816994][-4.6142387 -3.668395 -2.8832941 -1.8345995 -1.059248 0.55758095 1.7423248 2.2745485 2.1758795 0.54799366 -1.6926146 -3.5668397 -5.1954288 -6.8258753 -7.7238283][-5.1741319 -4.0969892 -3.5827985 -3.0827498 -2.3087578 -0.86494637 -0.15232658 0.014415264 -0.18917656 -1.2224646 -2.8584542 -4.0602312 -5.5769496 -7.1359367 -7.8200274][-4.4202681 -4.5241032 -4.1456342 -3.5070772 -2.8839369 -2.1000857 -1.3407359 -2.0036054 -2.0023108 -2.2442651 -3.0593972 -4.0481644 -5.3527126 -6.52383 -7.2141457][-4.6824207 -4.7559686 -5.1680832 -4.3679981 -3.5409317 -2.9093571 -3.0278459 -3.3861694 -3.3341565 -3.781754 -4.2176127 -4.8798828 -5.7361379 -6.3857861 -6.8853812]]...]
INFO - root - 2017-12-15 14:08:45.523062: step 21610, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 55h:41m:00s remains)
INFO - root - 2017-12-15 14:08:51.929491: step 21620, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 53h:58m:10s remains)
INFO - root - 2017-12-15 14:08:58.328984: step 21630, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 55h:58m:45s remains)
INFO - root - 2017-12-15 14:09:04.753856: step 21640, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 54h:30m:46s remains)
INFO - root - 2017-12-15 14:09:11.123974: step 21650, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 57h:59m:04s remains)
INFO - root - 2017-12-15 14:09:17.488817: step 21660, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 53h:34m:32s remains)
INFO - root - 2017-12-15 14:09:24.011941: step 21670, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.653 sec/batch; 56h:24m:43s remains)
INFO - root - 2017-12-15 14:09:30.431763: step 21680, loss = 0.27, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 58h:36m:51s remains)
INFO - root - 2017-12-15 14:09:36.804262: step 21690, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 56h:15m:16s remains)
INFO - root - 2017-12-15 14:09:43.144368: step 21700, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:42m:23s remains)
2017-12-15 14:09:43.648953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2647758 -5.6833715 -5.8136439 -5.300509 -3.9583721 -3.1454635 -2.5616078 -1.9207039 -1.0983405 -1.5820665 -2.5442786 -5.1846819 -6.5880303 -6.9553447 -8.2686958][-5.0773115 -5.6683903 -6.0088692 -6.2102346 -5.5302811 -3.8507891 -2.4764428 -2.2614636 -2.4154158 -2.9335852 -3.2530155 -5.114686 -6.7482104 -7.4513187 -8.4412689][-5.044281 -5.6843424 -6.0875349 -6.2467365 -5.7451096 -4.8257713 -3.5493531 -2.3167176 -1.7080584 -3.1211815 -4.7495408 -6.345026 -6.6751 -7.1752734 -8.5546427][-5.1625595 -5.7377768 -5.7693386 -5.5744858 -4.8017244 -3.6779513 -2.2499866 -1.4805007 -1.0740304 -2.1445613 -3.3682032 -6.0367913 -6.9799733 -7.0820394 -7.7669935][-5.4079027 -5.8242469 -6.0294127 -5.4730511 -4.03337 -2.4114933 -0.94633484 -0.48872757 -0.85387039 -2.2844357 -3.5317178 -5.7647896 -6.7271256 -7.1508613 -7.9867349][-7.3671312 -7.1124053 -6.4305544 -5.2997255 -3.541913 -1.4045949 0.74223995 1.2374992 0.67077065 -1.7459383 -4.0863185 -6.345952 -7.3487077 -7.7545238 -8.7493906][-7.8719134 -7.2147474 -6.4794965 -5.0316925 -2.7504478 -0.01651144 2.277194 3.3084946 3.1708498 0.60044575 -2.4838071 -6.0986996 -7.7528806 -8.0873995 -9.1372137][-7.1470742 -6.9485259 -6.289072 -4.4167786 -2.1821918 0.61171818 3.2843275 4.3099566 4.44374 2.0752687 -0.92809582 -4.6234188 -7.2438726 -8.1730766 -8.7310181][-6.3331008 -6.3127174 -6.1871519 -4.8664322 -2.4494863 0.34887409 2.3531427 3.3503122 4.0006952 1.9550409 -0.93944883 -4.634069 -6.941083 -8.0510349 -9.3573532][-5.8630996 -5.7520452 -5.7480087 -5.0308471 -3.6448069 -0.51341486 2.0468149 2.4715347 2.2784128 -0.43814754 -2.3163729 -5.6478062 -7.6088247 -8.1578627 -9.243413][-6.035543 -5.8541689 -5.606164 -4.4999075 -3.611146 -1.7993846 -0.13364029 0.32091713 -0.40759325 -3.8045988 -5.932445 -7.7571211 -8.4863424 -8.4560375 -8.5032043][-5.4344177 -4.5575123 -3.5155926 -2.3702836 -1.2953868 -0.91498137 -0.2945385 -0.26789427 -0.81667566 -3.5139198 -5.4899969 -7.3683019 -8.0765772 -8.213953 -8.6191378][-6.3016176 -4.704566 -3.0543966 -0.82427311 0.23171329 0.39391994 0.39916706 -0.29752111 -0.83777809 -3.270124 -4.702673 -6.5561495 -7.6681008 -7.8245721 -8.12536][-7.7970915 -6.753912 -5.5694542 -2.8078876 -0.9059968 -0.086183071 -0.25349188 -1.8027344 -2.8499432 -4.2483015 -4.8123426 -5.9432483 -6.3016715 -6.9237289 -7.2826262][-8.1228456 -7.8641586 -6.7365155 -5.0998716 -3.42877 -2.0281744 -2.0298209 -2.6769185 -3.6566024 -4.9587617 -5.6271381 -6.1743875 -6.2790813 -6.3583126 -6.58812]]...]
INFO - root - 2017-12-15 14:09:49.950734: step 21710, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 54h:00m:08s remains)
INFO - root - 2017-12-15 14:09:56.305254: step 21720, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 53h:58m:55s remains)
INFO - root - 2017-12-15 14:10:02.683350: step 21730, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 55h:16m:34s remains)
INFO - root - 2017-12-15 14:10:09.148216: step 21740, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 55h:13m:11s remains)
INFO - root - 2017-12-15 14:10:15.500919: step 21750, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:48m:41s remains)
INFO - root - 2017-12-15 14:10:21.907615: step 21760, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 57h:41m:00s remains)
INFO - root - 2017-12-15 14:10:28.261843: step 21770, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 56h:57m:41s remains)
INFO - root - 2017-12-15 14:10:34.612547: step 21780, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.639 sec/batch; 55h:07m:11s remains)
INFO - root - 2017-12-15 14:10:41.021146: step 21790, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 53h:35m:59s remains)
INFO - root - 2017-12-15 14:10:47.398974: step 21800, loss = 0.38, batch loss = 0.26 (12.7 examples/sec; 0.629 sec/batch; 54h:15m:04s remains)
2017-12-15 14:10:47.883170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4618549 -4.1737356 -4.8152008 -5.3172936 -6.0061765 -5.9711442 -5.1782074 -4.1521606 -3.2606688 -4.0799146 -4.8109307 -5.5828972 -6.5964727 -7.4105496 -7.5566506][-3.2232342 -4.1948757 -5.3592229 -6.7865219 -7.2996716 -6.3710227 -5.7250791 -5.1047373 -4.2785988 -5.2381954 -6.1632066 -6.7696104 -7.5987635 -8.4469929 -8.8910217][-2.034586 -2.6188741 -3.6822767 -4.73193 -5.2241049 -5.1376929 -4.4554596 -3.6473513 -3.1227484 -4.359283 -5.1122031 -5.7516947 -6.7094297 -7.4681363 -7.5817761][-2.4537854 -2.5777197 -2.6914182 -2.9401064 -3.0449882 -2.5358925 -1.7310324 -1.1720638 -0.7246666 -2.1527543 -3.3323565 -4.3911247 -5.7991619 -6.6370468 -6.8992047][-3.0273457 -2.4398856 -1.6868529 -1.1043301 -0.67163944 -0.070391178 0.14836884 0.24245024 0.33153152 -1.4165096 -2.7866387 -3.9123223 -5.0053854 -5.7765093 -6.2308273][-3.7990193 -3.0120449 -1.8306704 -0.50514936 0.94053268 2.0549021 2.8627014 2.4854279 1.9365301 -0.23054647 -1.9365897 -2.9892902 -4.119173 -4.8491535 -5.0361729][-4.5241165 -3.2196655 -1.8745294 -0.35040617 1.4458914 2.7766132 3.290143 3.0276918 2.7122879 0.097212315 -1.9519072 -3.3561902 -4.8244982 -5.5014782 -5.90857][-5.2296219 -3.8172061 -2.2716308 -0.3006773 1.7612705 2.8800154 3.1199417 2.4761171 1.6877403 -0.95257092 -3.1622996 -4.7278867 -5.9029555 -6.7538667 -6.9156809][-5.195013 -4.2631321 -3.004271 -1.2264118 0.585804 1.9322577 2.2466993 1.8186722 1.4376488 -1.4531932 -3.7092433 -5.1787357 -6.5286365 -7.1511092 -7.313899][-5.9162064 -5.070056 -3.9571807 -2.26575 -0.501956 0.35511494 0.95779991 0.83106232 0.1792345 -2.4994178 -4.3695555 -5.8825779 -7.0104132 -7.6870413 -7.8871031][-6.8472829 -6.1964474 -5.3635464 -3.9827905 -2.922483 -1.8379688 -1.1328201 -1.5838432 -1.7602339 -3.9578421 -5.7565961 -7.1139622 -8.003314 -8.4040947 -8.3326054][-7.1524296 -6.4233174 -5.4583273 -4.8130531 -4.1836996 -3.5154152 -3.268784 -3.4396644 -3.5557871 -5.0765009 -5.9816713 -6.8426547 -7.5950136 -8.1490068 -8.2186565][-7.0701962 -6.2808466 -5.516336 -4.9453764 -4.6827412 -4.4762745 -4.3275013 -4.8227806 -5.078371 -5.995297 -6.616076 -7.1053772 -7.5552411 -7.5518618 -7.2942867][-6.8506451 -6.0102353 -5.1987543 -4.8459177 -4.6789217 -4.7029028 -5.0596857 -5.4297132 -5.5063343 -6.2850189 -6.2238111 -6.1978788 -6.4642153 -6.5650496 -6.3855586][-7.6169214 -6.7728262 -6.1290536 -5.7933269 -5.5113249 -5.566968 -6.0306182 -6.5492239 -6.88573 -6.726675 -6.6448846 -6.4678097 -6.18474 -5.989038 -5.8319025]]...]
INFO - root - 2017-12-15 14:10:54.263823: step 21810, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 55h:52m:18s remains)
INFO - root - 2017-12-15 14:11:00.603903: step 21820, loss = 0.25, batch loss = 0.13 (13.0 examples/sec; 0.616 sec/batch; 53h:07m:05s remains)
INFO - root - 2017-12-15 14:11:07.077807: step 21830, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 56h:20m:02s remains)
INFO - root - 2017-12-15 14:11:13.430303: step 21840, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 54h:30m:33s remains)
INFO - root - 2017-12-15 14:11:19.774266: step 21850, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 56h:16m:02s remains)
INFO - root - 2017-12-15 14:11:26.123038: step 21860, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 55h:21m:46s remains)
INFO - root - 2017-12-15 14:11:32.446450: step 21870, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.614 sec/batch; 52h:59m:40s remains)
INFO - root - 2017-12-15 14:11:38.819355: step 21880, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 55h:12m:06s remains)
INFO - root - 2017-12-15 14:11:45.181514: step 21890, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 54h:10m:48s remains)
INFO - root - 2017-12-15 14:11:51.566982: step 21900, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 55h:07m:47s remains)
2017-12-15 14:11:52.140013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1663775 -2.3670888 -2.7400174 -3.3835726 -3.6383314 -3.984174 -4.0626655 -3.9644156 -4.0308743 -5.4545383 -5.542057 -6.9066668 -7.7657466 -8.3278389 -9.324275][-2.5304389 -3.0152059 -3.4721556 -3.9090517 -3.9731524 -3.7375679 -3.6218686 -3.7634575 -3.6524048 -5.1687207 -5.8884993 -7.0685081 -8.1618156 -8.9813528 -9.5928011][-3.8212035 -3.867496 -4.0405488 -4.1284637 -3.9125416 -3.3460975 -2.8319073 -2.7894411 -2.5217414 -3.7840078 -4.1176815 -5.4478483 -6.5455165 -7.0700669 -7.942883][-5.1637278 -4.5898705 -4.3220797 -3.6915414 -2.8172979 -1.9236455 -1.2178998 -1.0603642 -0.81439066 -2.2246251 -2.5587468 -4.0297527 -5.1115179 -5.6953754 -6.6440744][-5.4138622 -4.7504492 -3.7945426 -2.6850886 -1.9285131 -0.49072981 0.88458443 1.4085894 1.4756269 -0.5724473 -1.2846899 -2.7391567 -4.0437908 -5.0686822 -5.9015374][-5.1405983 -4.5098853 -3.7834432 -2.1382909 -0.80643654 0.86484337 2.3520718 3.101161 3.4860563 1.2921753 -0.28108072 -2.3908763 -3.6907232 -4.6481919 -5.3219175][-5.5089641 -4.5662289 -3.5824733 -1.6376944 0.32276058 2.1801605 3.5719051 4.0440655 4.4472065 2.1549006 0.5118084 -2.1555748 -4.3552852 -5.527298 -5.8615742][-5.6583848 -4.437129 -2.9890375 -1.4543948 0.3544836 2.7372847 4.0850754 4.6897163 4.9089823 2.4612293 1.0966825 -1.6464038 -3.7879503 -5.7738638 -6.4227781][-4.313056 -3.9778306 -3.3209062 -1.5015569 -0.22202349 1.1673117 2.5157547 3.6151476 4.0877552 1.5205126 0.17577028 -2.1085587 -3.7874777 -5.4282627 -6.5084181][-4.7618828 -4.2028294 -4.0420413 -2.839334 -1.8257804 -0.4917841 0.43033504 1.1455431 1.7225199 -0.4372282 -1.6936932 -4.1162786 -5.3807974 -6.2366505 -6.8540053][-6.9824405 -6.6582756 -6.36913 -5.5183892 -4.4291363 -3.2200994 -2.0158277 -1.3215318 -0.9660821 -2.9340978 -3.9974458 -5.6384993 -6.825315 -7.730545 -8.0767117][-7.691195 -7.6222529 -7.2881346 -6.8492508 -6.4223375 -5.4678688 -4.89717 -4.18626 -3.6327229 -4.8081951 -5.3063579 -6.3218637 -6.7611256 -7.3243866 -7.9048743][-7.4123917 -7.5697889 -7.790791 -7.3491693 -7.0849533 -6.7510123 -6.4690037 -6.1710987 -5.9438682 -6.671658 -6.6987963 -6.9776549 -7.2495055 -7.0704265 -6.9495139][-7.4615388 -7.5969486 -7.5288939 -7.4631629 -7.2290044 -6.6471057 -6.7118816 -6.8568249 -6.8305359 -7.3929467 -7.3373423 -7.049077 -7.0520406 -7.2463756 -7.0753636][-8.50473 -8.3660679 -8.0382013 -7.8283372 -7.872004 -7.794662 -8.0329781 -7.9767175 -8.0474415 -8.0223713 -7.8328819 -7.6643543 -7.2677083 -7.0051389 -6.892036]]...]
INFO - root - 2017-12-15 14:11:58.699667: step 21910, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 53h:58m:44s remains)
INFO - root - 2017-12-15 14:12:05.095294: step 21920, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 55h:20m:11s remains)
INFO - root - 2017-12-15 14:12:11.576788: step 21930, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.681 sec/batch; 58h:43m:30s remains)
INFO - root - 2017-12-15 14:12:17.872488: step 21940, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 53h:34m:00s remains)
INFO - root - 2017-12-15 14:12:24.205217: step 21950, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 55h:23m:50s remains)
INFO - root - 2017-12-15 14:12:30.547014: step 21960, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:54m:34s remains)
INFO - root - 2017-12-15 14:12:36.861690: step 21970, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 55h:33m:42s remains)
INFO - root - 2017-12-15 14:12:43.215890: step 21980, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 54h:17m:43s remains)
INFO - root - 2017-12-15 14:12:49.562080: step 21990, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 54h:41m:39s remains)
INFO - root - 2017-12-15 14:12:55.958393: step 22000, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 57h:18m:56s remains)
2017-12-15 14:12:56.480292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9306965 -2.6849027 -2.659657 -2.3018327 -2.0147972 -1.9662304 -1.3860483 -0.90602446 -0.63796568 -2.3389378 -3.5164661 -4.9977236 -5.5626526 -6.6483445 -6.9618912][-3.3328466 -3.6434226 -3.860955 -3.6811056 -3.164793 -2.6076903 -2.1276016 -1.8720212 -1.7057843 -3.042264 -3.9682579 -5.419529 -6.1999655 -7.3776526 -7.7510791][-3.2025547 -3.153091 -3.3137565 -3.2845473 -2.7323308 -2.0173845 -1.3807483 -1.2070374 -1.2698631 -2.6894922 -3.6114421 -4.7490811 -5.2309022 -6.35742 -6.8307805][-3.5267191 -3.4454193 -3.6124749 -3.4400187 -2.9176717 -2.1831284 -1.3367352 -0.94840813 -0.67042351 -1.9795666 -2.9135671 -4.2496176 -5.0735426 -6.1682038 -6.4814596][-3.559998 -3.1999969 -2.9518414 -2.7915349 -2.2787623 -1.5390439 -0.68533373 -0.29239035 0.064971924 -1.2731543 -2.2071505 -3.5707436 -4.488358 -5.6751356 -6.1822405][-3.6872473 -3.2139978 -2.8016973 -2.1168923 -1.3132167 -0.44943142 0.10546732 0.31172371 0.53851891 -0.87726784 -1.869215 -3.348877 -4.3435016 -5.6034384 -6.2073836][-3.9714258 -3.2270823 -2.5025196 -1.5238624 -0.65484667 0.35652351 0.84616375 1.0380077 1.2531281 -0.27222919 -1.396625 -3.057127 -4.0869851 -5.3720412 -5.9228687][-4.1508126 -3.1604624 -2.1516538 -0.95767593 0.17598867 1.4279757 1.9807997 2.1705456 2.2005386 0.47668934 -0.85852575 -2.756628 -3.9379416 -5.3304515 -5.7606721][-4.2130966 -3.1953893 -2.1074991 -0.832572 0.31238461 1.3139009 1.824213 1.9905024 2.0030403 0.1882515 -1.1443043 -2.9730058 -4.2046347 -5.7509975 -6.2379165][-4.1614947 -3.4834971 -2.3837218 -1.1745753 0.029928684 0.97249031 1.295229 1.3367834 1.1668501 -0.73063183 -1.8756142 -3.6399312 -4.7253065 -6.0560107 -6.6209459][-5.4963045 -4.7943945 -3.9161372 -2.9980588 -1.9540019 -1.0690293 -0.600749 -0.45602226 -0.57010841 -2.4753165 -3.7317431 -5.1102343 -5.8303142 -6.7740273 -6.8322349][-6.6098647 -6.0986595 -5.1702929 -4.2906609 -3.3928366 -2.6369271 -2.3210068 -2.318543 -2.5094213 -3.6277223 -4.4121213 -5.656076 -6.1553292 -6.9887934 -6.9675422][-6.8447905 -6.46717 -5.9377441 -5.3380837 -4.8311377 -4.2469492 -3.909163 -3.9584322 -4.0994987 -4.7118931 -5.0621443 -5.6563411 -6.1024952 -6.6648331 -6.3586802][-6.8992162 -6.6453857 -6.4921136 -6.08925 -5.8767543 -5.2685933 -5.0671997 -5.0020809 -4.9652305 -5.4126835 -5.6151123 -5.59766 -5.6370459 -6.0553131 -5.9693813][-7.9228759 -7.6636062 -7.1257424 -6.9267011 -6.9566746 -6.5847187 -6.5110021 -6.6212845 -6.767612 -6.6130047 -6.5151763 -6.464922 -6.3163733 -6.1608334 -6.0159583]]...]
INFO - root - 2017-12-15 14:13:02.880505: step 22010, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 54h:23m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:13:09.326804: step 22020, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 55h:14m:21s remains)
INFO - root - 2017-12-15 14:13:15.697248: step 22030, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 54h:12m:36s remains)
INFO - root - 2017-12-15 14:13:22.070511: step 22040, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 55h:54m:24s remains)
INFO - root - 2017-12-15 14:13:28.410259: step 22050, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 54h:54m:41s remains)
INFO - root - 2017-12-15 14:13:34.802066: step 22060, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 54h:07m:46s remains)
INFO - root - 2017-12-15 14:13:41.252541: step 22070, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.657 sec/batch; 56h:39m:49s remains)
INFO - root - 2017-12-15 14:13:47.721640: step 22080, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 54h:06m:54s remains)
INFO - root - 2017-12-15 14:13:54.159881: step 22090, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 55h:33m:35s remains)
INFO - root - 2017-12-15 14:14:00.487241: step 22100, loss = 0.37, batch loss = 0.25 (12.6 examples/sec; 0.633 sec/batch; 54h:35m:10s remains)
2017-12-15 14:14:01.047707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3001838 -3.5449128 -4.6793756 -5.4006376 -5.5747557 -5.4520359 -5.0133991 -4.60428 -4.1003084 -4.7814236 -5.0682869 -6.6926117 -7.48846 -8.8411255 -9.8820372][-3.2382293 -3.8243549 -5.0779395 -6.1122885 -7.030282 -7.045598 -7.1011353 -7.0067377 -6.171289 -6.6713 -6.7916188 -7.7062235 -8.3282194 -9.2468891 -9.8653126][-3.5617361 -3.6867008 -4.8526936 -5.6293573 -6.398098 -7.1853166 -7.48146 -7.280982 -7.0499644 -7.667171 -7.66294 -8.3003664 -8.799386 -9.3873024 -9.5619211][-4.2332125 -4.1099005 -4.8613105 -5.1647172 -5.450346 -5.5708485 -5.4956131 -5.7796659 -5.7479362 -6.3890781 -6.2813263 -7.0605569 -7.527317 -8.5973568 -9.5680361][-4.3805571 -3.9745295 -4.1158333 -4.034286 -3.7999985 -3.0159855 -2.4584036 -2.5976043 -2.5062356 -3.6899405 -4.2951932 -5.7469072 -6.82614 -8.1321278 -9.49055][-3.7909555 -3.6396375 -3.74886 -3.0584183 -2.1338358 -0.99418831 0.27026796 0.93994141 1.5122986 -0.21876335 -1.521502 -3.714071 -5.5863004 -7.1243987 -8.61709][-3.7678273 -3.5780449 -3.2724123 -1.8712878 -0.37275743 0.7126646 1.5419722 2.588871 3.770505 2.4244633 0.73803329 -2.2866173 -4.4269433 -6.3556795 -8.2633848][-2.6314583 -2.9897118 -2.8957744 -1.3066444 0.72713947 2.2805595 3.6124945 4.2198524 4.7335434 3.1865759 1.4111061 -1.373085 -3.4310527 -5.5579758 -7.8215876][-3.0854926 -2.8669114 -3.5141425 -2.1533003 -0.75579977 0.68544292 2.3087244 3.2525873 3.6641645 1.7313576 -0.38899088 -3.070518 -4.8835077 -6.2837834 -7.4771957][-4.2554626 -4.5063982 -4.5164623 -4.0085249 -2.8769512 -1.8626299 -0.61159515 0.46723557 1.2277212 -0.85091114 -2.754003 -5.0583982 -6.587678 -7.3605046 -8.0449724][-5.9175057 -6.6232748 -6.5582986 -5.968792 -5.1969137 -4.1787038 -2.9983897 -2.4657288 -2.2982597 -3.60989 -5.0504284 -6.2205029 -7.4279532 -8.1814928 -8.4377632][-6.62941 -6.450458 -7.0687079 -6.9773893 -6.8888936 -6.1266155 -5.3215408 -5.0370655 -4.6383104 -5.7066488 -6.6184311 -7.3309321 -7.7107916 -7.9228921 -7.6384697][-7.7663965 -7.9167848 -7.52401 -7.4246869 -7.3204722 -7.3007851 -7.0616026 -7.1734858 -7.2484088 -7.60794 -8.1656828 -8.11019 -7.7620249 -7.3210659 -6.8664756][-8.517705 -8.3222551 -8.1746025 -7.9369674 -7.7814388 -7.5111494 -7.7241812 -7.9037371 -7.8583174 -8.2944555 -8.4990454 -8.0895834 -7.6386676 -7.0582647 -6.5372725][-8.9194345 -9.1388512 -9.3821249 -9.2259293 -9.1944551 -9.005064 -9.1127768 -9.416604 -9.0590916 -8.9441547 -9.007803 -8.3448353 -8.0167227 -7.6618633 -7.1578569]]...]
INFO - root - 2017-12-15 14:14:07.480694: step 22110, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 54h:13m:19s remains)
INFO - root - 2017-12-15 14:14:13.906745: step 22120, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 54h:46m:45s remains)
INFO - root - 2017-12-15 14:14:20.258832: step 22130, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 53h:57m:03s remains)
INFO - root - 2017-12-15 14:14:26.635290: step 22140, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 53h:59m:40s remains)
INFO - root - 2017-12-15 14:14:33.080004: step 22150, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 55h:10m:27s remains)
INFO - root - 2017-12-15 14:14:39.536775: step 22160, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 56h:45m:31s remains)
INFO - root - 2017-12-15 14:14:45.983325: step 22170, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 54h:34m:47s remains)
INFO - root - 2017-12-15 14:14:52.311581: step 22180, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 55h:13m:06s remains)
INFO - root - 2017-12-15 14:14:58.820332: step 22190, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 55h:25m:37s remains)
INFO - root - 2017-12-15 14:15:05.198992: step 22200, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 55h:47m:12s remains)
2017-12-15 14:15:05.794842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1151786 -0.96412563 -1.143775 -1.6146502 -2.0206065 -2.4239268 -2.7458391 -2.8454227 -3.1581173 -4.7560158 -5.2912025 -5.7750568 -6.4280024 -7.5234976 -8.3069515][-1.6811166 -1.4952741 -1.367065 -1.23283 -1.40379 -1.866169 -2.5668254 -3.0739217 -3.2083349 -4.6551208 -5.3742819 -5.9640141 -6.6314607 -7.9333687 -8.8036051][-2.3846827 -2.1077218 -1.7447824 -1.4232635 -1.2939157 -1.2202458 -1.1481271 -1.6116576 -2.0682187 -3.2640824 -3.856111 -4.9758072 -6.089201 -6.9145732 -7.5129991][-2.8023281 -1.9630961 -1.6577549 -1.3989611 -0.94105673 -0.80468559 -0.60086632 -0.67371464 -0.87094021 -1.9176531 -2.4228334 -3.3983541 -4.4964466 -5.5407147 -6.5298476][-3.6206403 -2.34625 -1.3302031 -0.91367865 -0.70780849 -0.23382235 0.34401989 0.37577915 0.30721092 -0.75085354 -1.1358562 -2.2537246 -3.7170131 -4.8503308 -5.9971][-3.5920091 -2.5243759 -1.6539435 -0.94983864 -0.29744768 0.41781807 1.1048994 1.235445 1.4029455 0.24768734 -0.30563164 -1.3800917 -2.908391 -4.1706409 -5.2551975][-3.5974383 -2.681736 -1.7270966 -0.69663763 0.033032894 0.66307163 1.5588703 1.5522261 1.5940361 0.30522871 -0.33867455 -1.5646286 -2.9477825 -3.910872 -4.94147][-2.7747049 -2.2548943 -1.3784757 -0.24636936 0.5422039 0.90184879 1.4598074 1.6774874 2.032917 0.35011864 -0.55673885 -1.7976737 -2.9822841 -4.29436 -5.3236046][-2.9885597 -2.147521 -1.5062141 -0.53640461 0.22719002 0.73111248 1.4188843 1.2277613 1.1890497 -0.29737234 -1.160089 -2.5165887 -3.8171861 -5.0508304 -5.9523287][-3.5763888 -2.8252969 -2.1652389 -1.4425955 -0.85649347 0.020689011 0.90517521 0.38645172 0.044482231 -1.8138461 -2.6189551 -3.7696216 -5.2061644 -6.421535 -7.2919807][-4.8514585 -4.5045094 -4.2138443 -3.5781603 -2.9237318 -2.1447616 -1.3064804 -1.2738361 -1.3787956 -3.3025808 -4.1883345 -5.311801 -6.5114574 -7.3306293 -7.8954363][-6.1997766 -5.5387039 -5.2667141 -5.1204553 -4.9419594 -4.0929909 -3.0529394 -2.7299676 -2.5513296 -4.0231857 -5.031671 -5.7142344 -6.7502441 -7.583693 -8.0392427][-7.6989965 -7.1809645 -6.8636866 -6.4290581 -5.8435163 -5.3747234 -4.822381 -4.8292251 -4.9084368 -5.4860077 -5.6854105 -5.889204 -6.3550358 -6.9888592 -7.3409019][-6.86818 -6.7190161 -6.5609288 -6.2625952 -6.0318527 -5.2738175 -4.7268629 -4.967041 -4.9402413 -5.1679688 -5.3737631 -5.7828922 -6.1936603 -6.2664466 -6.1178036][-8.186182 -7.7849975 -7.2736807 -7.1944547 -7.1686888 -6.9136 -6.6222348 -6.4329519 -6.3402534 -6.3640161 -6.3757229 -6.2242141 -6.1865778 -6.6360173 -6.7913527]]...]
INFO - root - 2017-12-15 14:15:12.262879: step 22210, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 55h:57m:51s remains)
INFO - root - 2017-12-15 14:15:18.710688: step 22220, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 54h:50m:28s remains)
INFO - root - 2017-12-15 14:15:25.135562: step 22230, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 56h:11m:03s remains)
INFO - root - 2017-12-15 14:15:31.481329: step 22240, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 55h:25m:31s remains)
INFO - root - 2017-12-15 14:15:37.890126: step 22250, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 56h:06m:19s remains)
INFO - root - 2017-12-15 14:15:44.213007: step 22260, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 54h:32m:47s remains)
INFO - root - 2017-12-15 14:15:50.630348: step 22270, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 53h:59m:43s remains)
INFO - root - 2017-12-15 14:15:57.011228: step 22280, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 53h:53m:12s remains)
INFO - root - 2017-12-15 14:16:03.453636: step 22290, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 54h:06m:43s remains)
INFO - root - 2017-12-15 14:16:09.826531: step 22300, loss = 0.30, batch loss = 0.18 (13.1 examples/sec; 0.612 sec/batch; 52h:44m:24s remains)
2017-12-15 14:16:10.349579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.270762 -5.1278982 -4.801816 -4.2157841 -3.2603545 -2.7694893 -3.0254269 -2.9116817 -3.2249794 -3.5596585 -5.8009157 -6.0649157 -6.5542936 -7.5741515 -7.6787248][-5.1996317 -4.2855983 -3.6452079 -3.3810925 -2.8855381 -2.7090926 -2.6088347 -2.7190752 -2.8339162 -3.2065444 -5.5416789 -5.9723759 -6.3949938 -7.4000463 -7.5329][-3.6881471 -3.3551674 -2.5229211 -1.9584913 -1.0860252 -1.140337 -1.4130096 -1.7242808 -1.9094744 -2.1052322 -4.0272207 -4.7462149 -5.8547344 -6.6054287 -6.320426][-2.3954577 -1.947629 -1.8964305 -1.072134 -0.4960742 -0.21065807 0.17075014 0.045236111 -0.46208477 -1.1579037 -3.3632159 -3.7800498 -5.1977477 -6.4216518 -6.33195][-1.9518151 -1.1342869 -0.42933989 0.14678001 0.39216805 0.98926067 1.1347151 0.81603622 0.44284534 -0.30479002 -2.5970607 -3.1536059 -4.1317539 -5.3003554 -5.5504637][-1.7537141 -1.277503 -0.6604991 0.00037240982 0.72102833 1.1664534 1.379591 1.2748766 0.92413235 0.038411617 -1.6374526 -2.1666422 -3.5431547 -4.9661193 -5.2660689][-2.602447 -1.4150853 -0.58824921 0.10260725 0.78520775 1.1735163 1.8778944 1.7727728 1.5184422 1.0575848 -1.2168059 -1.9842014 -3.5439849 -4.9762974 -5.5842142][-3.0464602 -1.9673195 -0.70778608 0.065350056 0.71990395 1.3797665 2.2385883 2.1237621 1.8040915 1.4000597 -0.991158 -2.2808576 -3.8749235 -5.4852076 -5.9501138][-3.6951938 -2.6761622 -1.7850313 -0.97954655 -0.32696676 0.39659309 1.0656071 1.3762474 1.8357611 0.76605606 -1.4482646 -2.595839 -4.143918 -5.2720809 -5.6380863][-4.6989222 -3.850121 -3.0654035 -2.7636719 -2.2108054 -1.3900337 -1.1620212 -0.44705248 -0.032753944 -0.90420628 -2.8250184 -3.8448284 -5.0639839 -6.1133003 -6.1403327][-6.2926922 -5.8223519 -5.3116627 -4.654933 -4.1857882 -3.9413111 -3.5605731 -2.5978851 -2.4738955 -3.1412039 -4.3529015 -5.1995354 -5.9377689 -7.052238 -7.048872][-6.4348497 -6.1703568 -5.3942232 -5.0012856 -4.6542654 -4.32537 -3.8045752 -3.6723962 -3.5160575 -3.2751231 -4.3596039 -4.6608286 -5.5695438 -7.1405168 -7.7619824][-6.5950913 -6.5868678 -6.13153 -5.646678 -5.1086464 -4.2668419 -3.7020676 -3.7067153 -3.8372905 -3.7102785 -4.9745951 -4.5867162 -4.5900059 -6.24121 -6.6046309][-5.9962654 -5.916584 -5.4144087 -5.3397627 -4.3663721 -3.3179779 -2.8342357 -2.6817346 -2.4817486 -2.8586788 -3.7093241 -3.7320116 -4.3827562 -5.00282 -5.0621872][-5.9579325 -6.2243652 -5.4909945 -5.4848652 -5.129467 -4.1805019 -3.4860978 -3.0965447 -3.0629377 -3.2259908 -3.6096234 -3.8883924 -4.6624088 -4.9758825 -4.9978943]]...]
INFO - root - 2017-12-15 14:16:16.710052: step 22310, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 55h:12m:00s remains)
INFO - root - 2017-12-15 14:16:23.051007: step 22320, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.618 sec/batch; 53h:13m:58s remains)
INFO - root - 2017-12-15 14:16:29.419045: step 22330, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 54h:58m:35s remains)
INFO - root - 2017-12-15 14:16:35.662614: step 22340, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 53h:37m:15s remains)
INFO - root - 2017-12-15 14:16:42.013631: step 22350, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 55h:00m:57s remains)
INFO - root - 2017-12-15 14:16:48.276916: step 22360, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 53h:38m:56s remains)
INFO - root - 2017-12-15 14:16:54.623912: step 22370, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 54h:08m:06s remains)
INFO - root - 2017-12-15 14:17:01.003680: step 22380, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:12m:26s remains)
INFO - root - 2017-12-15 14:17:07.372868: step 22390, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 53h:55m:30s remains)
INFO - root - 2017-12-15 14:17:13.879481: step 22400, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 55h:49m:04s remains)
2017-12-15 14:17:14.464540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6091123 -4.9857779 -4.9154863 -5.4771776 -5.4935827 -6.360507 -7.0186229 -7.8141918 -7.3622603 -6.0453348 -5.8138647 -5.4509325 -5.4331932 -5.4805098 -5.7446265][-5.5952306 -5.2515416 -5.1646357 -5.0165553 -5.3020759 -6.4353371 -7.5070992 -8.41877 -8.1015511 -7.0335608 -6.9048533 -6.838695 -6.7665892 -5.9359908 -5.8607697][-6.8591952 -6.4805322 -5.9549274 -4.8480415 -4.49376 -5.04801 -5.9991655 -7.0752678 -7.1642 -6.7832875 -7.10423 -7.5413246 -7.760406 -7.383678 -7.1703181][-7.4750009 -6.8109927 -5.3972893 -3.5347395 -2.5603385 -2.3381844 -2.6814952 -3.6399608 -3.4634862 -3.404665 -4.556344 -6.056026 -7.15422 -7.6012397 -7.770534][-7.0738397 -5.5185347 -3.733707 -1.6779413 -0.64461231 0.21515274 0.29053164 -0.0899086 0.006002903 0.35143471 -0.7364831 -2.9574981 -4.7878418 -6.2116842 -6.8179412][-6.4227877 -4.9514246 -2.6056929 -0.3601141 0.97352505 1.7447891 1.6524448 1.6659546 1.9595613 2.1894016 0.43294048 -2.085259 -3.9378953 -5.3413172 -5.9660482][-5.5517097 -4.1005116 -1.9474277 0.06376791 1.8924294 2.5592966 2.7656012 2.8850422 2.8479509 2.4909191 0.21625805 -2.1093707 -3.7381735 -5.1655369 -5.8234315][-4.8473969 -3.2623525 -1.6967263 0.46281815 1.8596716 2.4384642 2.890646 2.9451008 3.1166277 2.824585 0.73051453 -1.3620005 -3.2138658 -4.6740913 -5.6939688][-4.7469978 -3.6111674 -2.3745594 -1.0570869 -0.34509134 0.54207039 0.9085207 1.2265501 1.2587881 0.71613216 -1.095612 -2.828723 -4.5886793 -5.8075361 -6.1918287][-4.8987265 -4.5488276 -3.8318136 -2.7109609 -1.5601912 -1.2001443 -1.1449428 -1.0109344 -1.3259001 -1.7717676 -3.7034125 -4.8198881 -6.2555737 -6.922648 -7.5094295][-6.4789057 -6.3008265 -5.9002619 -4.9361634 -3.8420131 -3.3543849 -3.0535312 -3.3255486 -3.9677837 -4.2135234 -5.7822542 -6.183969 -7.2912822 -7.442318 -7.3907681][-8.2057791 -8.2300062 -8.1622963 -7.3424373 -6.593327 -5.8573809 -5.3436079 -5.6774592 -5.89275 -6.173388 -7.5522604 -7.99921 -8.5561342 -8.2835331 -7.7985945][-8.7696552 -9.5138 -9.6289558 -8.9028692 -8.0031891 -6.963778 -6.4352889 -6.5171323 -6.5760441 -6.5067368 -7.3942609 -7.4686728 -7.8096704 -7.6738563 -7.4153037][-8.515872 -9.1566925 -9.0470943 -8.6420841 -7.8852234 -6.7666368 -6.0178471 -5.7156553 -5.82687 -5.5974579 -6.0330358 -5.8981729 -6.072484 -6.210072 -5.96801][-8.2691536 -8.4883356 -8.5144491 -8.2879372 -7.5929861 -6.93513 -6.3720675 -6.2031603 -5.5026422 -5.35841 -5.4289589 -5.2256575 -5.3418584 -5.5075974 -5.4325309]]...]
INFO - root - 2017-12-15 14:17:20.869230: step 22410, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 55h:54m:40s remains)
INFO - root - 2017-12-15 14:17:27.251293: step 22420, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 54h:12m:27s remains)
INFO - root - 2017-12-15 14:17:33.583756: step 22430, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 55h:03m:54s remains)
INFO - root - 2017-12-15 14:17:39.968123: step 22440, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 55h:39m:17s remains)
INFO - root - 2017-12-15 14:17:46.354657: step 22450, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 54h:47m:54s remains)
INFO - root - 2017-12-15 14:17:52.682648: step 22460, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:00m:13s remains)
INFO - root - 2017-12-15 14:17:59.035570: step 22470, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 54h:05m:27s remains)
INFO - root - 2017-12-15 14:18:05.375349: step 22480, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 54h:41m:12s remains)
INFO - root - 2017-12-15 14:18:11.759061: step 22490, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 54h:58m:31s remains)
INFO - root - 2017-12-15 14:18:18.092382: step 22500, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 54h:18m:32s remains)
2017-12-15 14:18:18.603486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.754612 -6.8743258 -7.1880493 -7.7035494 -7.8081436 -8.059638 -7.7327228 -6.8000298 -5.9349532 -5.3764048 -5.6365757 -6.086236 -6.1934161 -7.4988508 -8.3274117][-5.6302047 -6.1568661 -6.8161387 -7.4828496 -8.1941242 -8.4157 -8.4064512 -7.6924663 -6.7936387 -6.30961 -6.5294075 -7.1403542 -7.0110173 -8.0129652 -8.6675415][-4.9156742 -5.2292266 -5.3719931 -6.23038 -7.3607354 -7.3321767 -7.1288819 -6.4856477 -5.4991651 -5.3045669 -5.8424134 -6.4266376 -6.506855 -7.4699683 -7.3921881][-4.2564774 -4.3349924 -4.8313904 -5.0257349 -5.5330057 -5.4152079 -5.1448231 -4.4836655 -3.7264037 -3.8965063 -4.8030033 -6.0201139 -6.2791786 -7.14614 -6.9797168][-5.2335691 -4.5702105 -4.0063214 -3.8722672 -3.9786391 -3.2374668 -2.21456 -1.4413075 -0.75867271 -1.7815762 -3.4996152 -5.0725222 -5.6948438 -7.1320958 -7.5036793][-6.1552792 -5.6413145 -4.4372563 -3.0614538 -2.1130185 -0.32219934 1.27913 1.9415493 2.152833 0.50582123 -2.0668182 -4.3047657 -5.3583865 -7.0653973 -7.6341209][-6.7836952 -5.71893 -4.66661 -2.7446456 -0.39826107 1.989151 3.5937824 4.1892796 4.188343 1.7516003 -1.4902353 -4.1311522 -5.8808832 -7.8337688 -8.1585665][-6.2296052 -5.3904047 -4.1708493 -1.899807 0.54596329 3.0345726 4.8956928 5.6548519 4.9876642 2.5244322 -0.82628775 -4.1232386 -5.7244177 -7.9082041 -8.2765732][-6.1210957 -5.520031 -4.1405973 -2.2769494 -0.24830484 2.1925926 4.2535095 4.5593586 4.0653038 1.9960556 -1.4961462 -4.3864851 -5.8579879 -8.0228844 -8.2976637][-6.6301346 -5.9596081 -5.1486435 -3.5427532 -1.7196407 0.11983633 1.2583494 1.7557011 1.8429213 -0.48802328 -3.1329889 -5.86355 -7.3496075 -8.7895737 -8.8527718][-7.4374013 -7.3339887 -6.6933832 -5.621758 -5.0265388 -3.5377398 -2.2502666 -1.9131632 -1.7637911 -3.1244316 -4.9658031 -6.9534931 -7.6315522 -8.9805479 -9.17971][-8.4678383 -8.1045761 -7.4734817 -7.0547614 -6.7025232 -6.0318575 -5.7070961 -5.0631437 -4.6608844 -5.564548 -6.2657266 -7.4401155 -7.8242092 -8.4455538 -8.7430735][-8.36149 -8.7550383 -8.6678686 -8.1673222 -7.6395426 -6.9420037 -6.6025486 -6.6639118 -6.7870774 -7.2653875 -7.6342087 -8.1409492 -7.8111558 -7.9149208 -7.9057083][-7.630095 -7.7532563 -7.5862141 -7.3267722 -7.1501584 -6.7175975 -6.3515105 -6.304605 -6.3661189 -6.5610251 -6.67683 -7.1656446 -7.0025611 -6.9240236 -6.9209609][-8.001565 -7.8075032 -8.0350885 -7.226769 -6.6551766 -6.5260868 -6.3331361 -6.4833493 -6.7010541 -6.8335338 -6.8468065 -6.777555 -6.8686957 -6.3852859 -6.0377522]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 14:18:25.958157: step 22510, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 54h:59m:30s remains)
INFO - root - 2017-12-15 14:18:32.390487: step 22520, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 55h:04m:33s remains)
INFO - root - 2017-12-15 14:18:38.801697: step 22530, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 56h:00m:22s remains)
INFO - root - 2017-12-15 14:18:45.207386: step 22540, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 55h:58m:27s remains)
INFO - root - 2017-12-15 14:18:51.675090: step 22550, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 55h:01m:49s remains)
INFO - root - 2017-12-15 14:18:58.163729: step 22560, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:50m:13s remains)
INFO - root - 2017-12-15 14:19:04.556224: step 22570, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 55h:01m:56s remains)
INFO - root - 2017-12-15 14:19:10.882711: step 22580, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 55h:21m:11s remains)
INFO - root - 2017-12-15 14:19:17.350244: step 22590, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 56h:19m:27s remains)
INFO - root - 2017-12-15 14:19:23.775725: step 22600, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 54h:43m:21s remains)
2017-12-15 14:19:24.321279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4603229 -2.9642072 -2.4335008 -2.4742227 -2.3795857 -2.3297887 -2.4134164 -2.2372794 -1.6523657 -2.8052692 -3.9117546 -4.8663549 -6.0143166 -7.3028255 -8.4170618][-3.0676122 -2.7161241 -2.3540254 -2.4654913 -2.3977809 -2.308032 -2.515203 -2.4409161 -1.9320416 -2.7121453 -3.574131 -4.6523247 -5.5920286 -6.4089255 -7.2331843][-2.573699 -2.1398506 -1.7309608 -1.485846 -1.1566095 -1.2350616 -1.7181015 -1.5299344 -1.1104646 -1.9077954 -2.7129273 -3.5765791 -4.3143706 -5.0736151 -5.58683][-2.4540262 -1.8518524 -1.3955169 -1.3599901 -0.85579395 -0.54578209 -0.33709574 -0.28956604 0.14161825 -0.8960886 -2.1417003 -3.1529179 -3.9373915 -4.5396204 -4.7660809][-2.6945081 -1.9067702 -1.3901896 -1.0785275 -0.79316664 -0.29102421 0.26142979 0.55432034 0.87952518 -0.16213751 -1.4289656 -2.6862431 -3.6087995 -4.5225124 -4.8746395][-3.0660863 -2.2168384 -1.7155924 -1.2032804 -0.72940016 -0.061208725 0.77544785 1.1580629 1.3024797 0.028821468 -1.4607587 -2.9290562 -4.0144281 -5.0405354 -5.7625976][-3.2004752 -2.2662654 -1.668973 -1.0768209 -0.58066416 0.26762533 1.2677727 1.6262617 1.6276102 0.37404919 -0.91001654 -2.5540838 -4.0764275 -5.3994551 -6.3716388][-2.6318364 -1.8123593 -1.0244942 -0.57921362 -0.31949091 0.535532 1.7730064 2.3176775 2.3253393 0.76853848 -0.34548616 -1.5821648 -3.0482459 -5.0371161 -6.1681232][-2.0547285 -1.3047237 -0.59938669 0.013474941 0.199471 0.72127628 1.4699717 1.8654642 1.9304485 0.95833683 0.14549017 -1.3628874 -3.0026698 -4.5197749 -5.5613976][-1.4579487 -0.98117685 -0.43910456 0.27242041 0.29462719 0.40254307 0.76245213 1.0310907 1.1345129 0.024589062 -0.6771698 -1.4218111 -2.8273373 -4.3871942 -5.1232767][-2.4811983 -2.1901102 -1.4122982 -0.4561286 -0.13547897 -0.25460291 -0.48822355 -0.57635355 -0.54230976 -1.2302227 -1.806953 -2.7011318 -3.41824 -4.2311049 -4.4602127][-4.1146517 -3.2799716 -2.5931258 -1.6254396 -1.0118599 -0.97335052 -1.3093123 -1.6574073 -1.9640288 -2.7481332 -3.0662537 -3.3049102 -3.9281788 -4.6194553 -4.4713383][-5.3944263 -4.5860748 -4.1550846 -3.3972592 -2.7876482 -2.5132866 -2.5654922 -2.9717841 -3.2856145 -3.8217518 -4.1585808 -4.4619656 -4.6933327 -4.7467661 -4.7878213][-6.6758447 -5.8382235 -5.4426756 -5.229188 -4.8058434 -4.2003613 -4.0358877 -4.3874779 -4.5314569 -4.7400103 -4.6442609 -4.9984736 -4.9762177 -5.2269764 -5.0252824][-7.3458443 -6.3917794 -5.8328991 -6.0320406 -6.3014522 -5.8637714 -5.4849563 -5.7464161 -5.8398991 -5.6937165 -5.5086265 -5.6353416 -5.6420364 -5.7172289 -5.8379669]]...]
INFO - root - 2017-12-15 14:19:30.771400: step 22610, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 55h:01m:07s remains)
INFO - root - 2017-12-15 14:19:37.216948: step 22620, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:24m:09s remains)
INFO - root - 2017-12-15 14:19:43.680770: step 22630, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 57h:11m:31s remains)
INFO - root - 2017-12-15 14:19:50.096484: step 22640, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 55h:34m:59s remains)
INFO - root - 2017-12-15 14:19:56.513989: step 22650, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 54h:17m:02s remains)
INFO - root - 2017-12-15 14:20:02.960483: step 22660, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:50m:05s remains)
INFO - root - 2017-12-15 14:20:09.401754: step 22670, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 55h:09m:53s remains)
INFO - root - 2017-12-15 14:20:15.838856: step 22680, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 55h:32m:50s remains)
INFO - root - 2017-12-15 14:20:22.394118: step 22690, loss = 0.27, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 58h:18m:08s remains)
INFO - root - 2017-12-15 14:20:28.909899: step 22700, loss = 0.40, batch loss = 0.29 (12.5 examples/sec; 0.640 sec/batch; 55h:05m:45s remains)
2017-12-15 14:20:29.436521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3041649 -5.2478566 -5.2815886 -4.5689821 -3.0253654 -1.2609978 0.63863373 1.8351822 2.6527119 1.4798374 0.20302677 -2.1107888 -3.4711533 -4.8131638 -6.2376118][-4.6442661 -5.0840111 -4.9655018 -4.3053761 -3.17021 -1.9523244 -0.52947807 1.0037613 2.554347 2.1321907 1.3304567 -1.5225496 -3.47264 -4.5896387 -6.0496073][-4.04745 -3.526649 -3.2768412 -2.8744411 -1.5974092 -0.2938261 1.644556 2.8409052 3.3996258 2.5437164 1.3188334 -0.78604364 -2.3629279 -3.4635053 -4.6011214][-4.0766258 -3.9490201 -3.0468006 -1.5857158 0.27349043 1.0024099 1.6723433 2.75356 3.5552731 3.1456337 1.8805399 -0.75354195 -2.2537088 -3.4064803 -4.5320911][-3.9570434 -2.9307656 -1.6743736 -0.98990822 0.22162771 1.4809599 2.4213781 2.9797955 3.0560875 1.5067902 0.33512688 -2.2939863 -3.783628 -4.4485579 -5.4147105][-3.7418582 -2.7636571 -1.8125653 0.093461037 1.9286947 2.4792023 3.181447 3.2492905 3.0223064 1.4460678 0.022611141 -2.3257232 -3.5523291 -4.2870426 -5.1448469][-3.7815247 -2.2602839 -0.73490477 0.58122158 1.8269205 2.8272276 3.8650227 3.6159763 2.902997 0.55780792 -1.1140733 -4.0503855 -5.3739986 -5.9352255 -6.50772][-4.0179472 -2.7617102 -1.5460615 0.43106556 2.3217745 3.0774097 3.4783163 3.2291536 2.7466202 0.45669937 -1.4918575 -4.553133 -5.8956976 -6.822907 -7.57597][-3.1975489 -2.0020404 -0.83904076 0.38393497 2.1118994 3.4850693 4.3152733 3.7198858 2.7305603 0.1893611 -1.9211864 -5.1742296 -6.6024189 -7.2639847 -7.9895124][-4.3036337 -2.8542614 -1.7820907 -0.20400572 1.4932537 2.4866905 3.3278856 3.1188002 2.6872921 0.39371777 -0.97004461 -3.7076223 -5.0444403 -5.9258623 -6.7773347][-5.2813053 -4.4940033 -3.313971 -1.9197884 -0.32313061 1.2527275 2.2906904 2.5041513 2.5258331 0.89561176 -0.065941811 -2.2911081 -3.3700647 -4.2773933 -5.05959][-6.7403622 -5.433671 -4.1166897 -2.5614667 -1.3621445 0.075478554 1.2742004 1.7419977 2.2608213 1.1781101 0.44143963 -1.5732284 -2.792366 -3.2865644 -4.0211782][-7.7081871 -6.5463171 -5.6038475 -3.572165 -1.9641953 -0.3076911 1.2402182 1.6330252 2.2503738 1.6289053 1.1023054 -0.54521179 -1.8590646 -2.812664 -3.6413355][-7.0467968 -5.97316 -5.3274832 -4.4133453 -3.4516935 -1.3195291 0.71978474 2.0813179 2.9621391 2.2307949 1.3688335 0.10400915 -0.80752325 -1.9008389 -3.0524325][-7.8756676 -7.3946862 -6.6933303 -6.0774817 -4.9489174 -3.4080882 -1.8320756 -0.80978489 -0.46604776 -0.22715902 -0.72419691 -1.6976018 -3.0241976 -3.909173 -4.3626003]]...]
INFO - root - 2017-12-15 14:20:35.852111: step 22710, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 54h:06m:19s remains)
INFO - root - 2017-12-15 14:20:42.260679: step 22720, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 54h:53m:09s remains)
INFO - root - 2017-12-15 14:20:48.710518: step 22730, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.663 sec/batch; 57h:02m:09s remains)
INFO - root - 2017-12-15 14:20:55.198685: step 22740, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 55h:00m:09s remains)
INFO - root - 2017-12-15 14:21:01.593777: step 22750, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 54h:07m:40s remains)
INFO - root - 2017-12-15 14:21:07.987576: step 22760, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 53h:35m:03s remains)
INFO - root - 2017-12-15 14:21:14.562390: step 22770, loss = 0.32, batch loss = 0.21 (12.0 examples/sec; 0.664 sec/batch; 57h:07m:55s remains)
INFO - root - 2017-12-15 14:21:21.029904: step 22780, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 57h:38m:48s remains)
INFO - root - 2017-12-15 14:21:27.478104: step 22790, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 55h:22m:58s remains)
INFO - root - 2017-12-15 14:21:33.967367: step 22800, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 55h:27m:20s remains)
2017-12-15 14:21:34.461705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6293273 -6.1243091 -7.8842573 -9.2942886 -9.5207472 -7.3439188 -6.2259488 -4.8557177 -4.3699508 -5.186924 -4.8288488 -6.063365 -6.3173838 -7.5518389 -7.6775575][-6.1268477 -7.1096144 -7.3019743 -8.4696522 -9.2943726 -8.2862959 -7.8702083 -6.439785 -5.4713812 -6.1168418 -5.8047428 -6.8474755 -6.8619862 -6.7351513 -7.66482][-5.3439808 -6.9956841 -7.7342644 -7.3275185 -6.5052767 -6.200861 -5.9198618 -5.5208588 -5.5638628 -6.0964618 -6.1984215 -7.1409292 -7.0921879 -6.9084735 -6.9886422][-3.5312853 -5.1388731 -6.5185995 -6.77075 -5.5718174 -3.5557971 -2.735723 -2.4112597 -3.5840106 -4.8944049 -4.7127762 -6.3401136 -5.3533936 -5.9277973 -6.2892666][-2.0035973 -3.1695471 -4.2607861 -4.06748 -3.3124347 -1.8474512 -0.83203793 -0.90683985 -1.2364197 -3.1768351 -3.547215 -5.29221 -4.7786283 -4.9100857 -5.3283682][-2.271378 -3.1280251 -3.4487772 -2.6455469 -0.6630044 0.89276314 2.0808897 1.7364855 0.89441872 -0.42323112 -0.88483047 -3.1288838 -2.6182227 -3.3031301 -3.6907382][-2.5935369 -2.90104 -2.8755274 -2.3157587 -0.680254 1.7982082 2.5085497 2.8466053 2.4008818 0.9204483 0.15477228 -2.0183902 -2.3149877 -4.3880653 -4.5252366][-3.490922 -3.7179179 -3.2211609 -2.2064619 -0.6991353 1.4815769 2.5953999 2.4845181 2.3073606 1.0306082 -0.046864986 -1.8693414 -2.9526396 -4.8573232 -5.4782934][-5.076961 -4.7698421 -4.0947952 -2.9966722 -1.4097829 -0.0076789856 1.6886225 2.0567961 2.349494 1.221323 -0.70246649 -3.3024373 -4.5003433 -5.8307085 -6.0118008][-5.1393738 -4.6104622 -4.27652 -3.302124 -1.8893418 -1.0328007 -0.51587296 -0.15637112 -0.09446764 -0.77861643 -2.7186065 -5.2490253 -6.4659972 -6.8495045 -6.8170872][-6.3215742 -6.0932751 -4.775218 -3.8887627 -2.839838 -1.5638089 -1.2602501 -1.9178162 -1.2863011 -3.2062626 -4.5216913 -6.2531538 -7.9974766 -7.4427147 -7.4020648][-6.1842461 -6.6842747 -6.8633242 -6.0615668 -5.1602721 -3.6798334 -2.5596132 -3.3747616 -2.9425149 -4.3656569 -6.2115879 -6.4574175 -6.9136229 -6.9690738 -7.505125][-6.654892 -5.5628819 -5.9336419 -6.0532837 -6.6166506 -6.208272 -5.6205521 -5.9751344 -5.5624752 -6.7375822 -7.3250446 -7.1706491 -6.7429137 -7.0161667 -6.2508025][-5.4199157 -5.0942364 -5.2789145 -5.2824335 -4.6956587 -5.0443573 -5.8942876 -6.054862 -6.8650866 -7.6225524 -7.7370763 -7.2757277 -6.9280682 -7.3261547 -6.8090963][-6.0902061 -6.6190805 -6.2757468 -6.4956703 -6.6485853 -6.323576 -6.340703 -7.5547152 -7.741581 -7.8611093 -7.557313 -7.8276534 -6.9902544 -5.9150019 -5.9639635]]...]
INFO - root - 2017-12-15 14:21:40.925783: step 22810, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 55h:26m:59s remains)
INFO - root - 2017-12-15 14:21:47.342109: step 22820, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 54h:56m:31s remains)
INFO - root - 2017-12-15 14:21:53.846966: step 22830, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 56h:19m:59s remains)
INFO - root - 2017-12-15 14:22:00.255918: step 22840, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 53h:37m:05s remains)
INFO - root - 2017-12-15 14:22:06.709192: step 22850, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 54h:17m:46s remains)
INFO - root - 2017-12-15 14:22:13.250647: step 22860, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 55h:56m:17s remains)
INFO - root - 2017-12-15 14:22:19.716573: step 22870, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:55m:08s remains)
INFO - root - 2017-12-15 14:22:26.134468: step 22880, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 54h:30m:56s remains)
INFO - root - 2017-12-15 14:22:32.513438: step 22890, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 54h:09m:32s remains)
INFO - root - 2017-12-15 14:22:38.982347: step 22900, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 54h:41m:27s remains)
2017-12-15 14:22:39.448457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3888464 -4.6369085 -5.3358421 -5.6376314 -5.9529085 -6.1022406 -6.13482 -5.6418772 -4.969779 -5.464385 -5.772687 -6.7227607 -7.7502351 -8.5717354 -8.7230186][-3.6173682 -4.1728535 -5.2563629 -6.0846872 -6.6599321 -6.7248325 -6.2684603 -5.50593 -4.8686914 -5.951304 -6.2452197 -7.3714476 -8.5916147 -9.4465132 -9.677207][-2.1935802 -2.9257765 -3.6736054 -4.6354027 -4.9498587 -4.8653784 -4.634335 -3.9252551 -3.4412317 -4.4565921 -5.0822544 -6.328475 -7.6080842 -8.3343229 -8.1450729][-1.9687147 -2.4523292 -3.1185012 -3.4143457 -3.3454051 -3.0991826 -2.5574465 -2.0839558 -1.5767069 -2.8948088 -3.8029294 -5.3809 -7.0681157 -7.9342351 -8.1464][-2.2551298 -2.5106444 -2.2501092 -2.1232419 -1.4481421 -0.67849064 -0.17610979 0.15606546 0.24441433 -1.4577465 -2.40029 -3.8929121 -5.7989016 -6.8721085 -7.2800131][-3.3962345 -3.1686239 -2.7382798 -1.7788143 -0.13753223 1.4531746 2.719101 3.166482 3.1200657 0.925148 -0.80379963 -2.7390451 -4.4758005 -5.3630085 -5.591011][-4.5839529 -3.9792175 -3.1155496 -1.1791716 0.82665539 2.3981285 3.4728508 3.7462749 3.5351534 1.15131 -0.95543337 -3.3245883 -5.2732635 -5.9702148 -6.0066566][-5.414876 -4.6579285 -3.3913536 -1.0822878 1.5339375 3.2886305 3.9381266 3.9383259 3.3565874 0.69112492 -1.3194704 -3.883332 -5.7741108 -6.7659559 -6.9991713][-5.5964785 -4.682723 -3.1982107 -1.0737996 1.1127596 2.8154554 3.4903994 3.445714 2.93322 -0.041976452 -2.2446728 -4.5647888 -6.4571471 -7.1854315 -7.2492871][-6.7017393 -5.578886 -3.991586 -1.9698691 0.085454464 1.4082451 1.7727118 1.6852045 1.074194 -2.0258846 -3.9721065 -6.0455427 -7.4994831 -7.9459505 -7.8085432][-7.348474 -6.3649192 -4.8327751 -2.6101847 -1.1829047 -0.35977173 -0.32840633 -0.86447477 -1.641716 -4.3294277 -5.8285508 -7.6376028 -8.3994656 -8.6446486 -8.3428593][-7.0866518 -5.9160051 -4.6520839 -3.2968631 -2.5258369 -2.0833406 -2.5077987 -2.837944 -3.0561085 -5.1161966 -6.015521 -7.2177444 -8.0025244 -8.565506 -8.2044153][-6.8884668 -5.6044931 -4.5881424 -3.8510187 -3.4475861 -3.5786963 -3.8977275 -4.5806818 -5.1325283 -6.303535 -6.780642 -7.4724774 -7.9064384 -7.9721465 -7.689538][-6.4869361 -5.1816635 -4.363348 -3.9744043 -4.2140532 -4.4044938 -5.0356436 -5.7137623 -5.9984331 -6.9385428 -6.9982591 -7.0068378 -7.097239 -6.9517384 -6.7203856][-6.6695457 -5.6319704 -4.8982611 -4.7621708 -5.1437707 -5.6637259 -6.5591044 -7.2624731 -7.8138776 -7.7854509 -7.3959227 -7.0849638 -6.6184325 -6.2767649 -6.1337652]]...]
INFO - root - 2017-12-15 14:22:45.854929: step 22910, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 53h:54m:51s remains)
INFO - root - 2017-12-15 14:22:52.275136: step 22920, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 56h:26m:27s remains)
INFO - root - 2017-12-15 14:22:58.719761: step 22930, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 53h:16m:59s remains)
INFO - root - 2017-12-15 14:23:05.112148: step 22940, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 54h:05m:51s remains)
INFO - root - 2017-12-15 14:23:11.489569: step 22950, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 55h:00m:18s remains)
INFO - root - 2017-12-15 14:23:17.906478: step 22960, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 54h:05m:36s remains)
INFO - root - 2017-12-15 14:23:24.397484: step 22970, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:23m:12s remains)
INFO - root - 2017-12-15 14:23:30.862566: step 22980, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 58h:39m:18s remains)
INFO - root - 2017-12-15 14:23:37.331897: step 22990, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 57h:05m:15s remains)
INFO - root - 2017-12-15 14:23:43.769282: step 23000, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:05m:51s remains)
2017-12-15 14:23:44.313287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2660418 -3.2023387 -2.7479882 -2.6009045 -3.0607057 -3.2951908 -3.0242763 -3.3172355 -3.4740391 -5.2795777 -5.6593847 -7.4634314 -8.2388115 -8.6646681 -8.6290665][-2.9888153 -2.9834061 -3.025075 -2.6110868 -2.3412137 -2.7355871 -3.238162 -3.4156795 -3.1257343 -5.0576162 -5.7626152 -6.9851308 -7.2610989 -8.0621042 -8.6332273][-4.498034 -4.0126281 -3.8548994 -3.6695142 -3.6117687 -3.3912244 -3.0412283 -3.3822851 -3.6886563 -5.50006 -5.9661336 -7.63263 -8.2161274 -8.3836775 -8.1648207][-6.0585446 -5.246769 -4.8179541 -3.68471 -2.919724 -2.8092918 -2.7279086 -2.9015322 -2.8564687 -4.5554867 -5.1140366 -6.5254211 -6.9183321 -7.5537882 -7.7425251][-5.860919 -4.7032828 -3.9694514 -3.0627327 -2.7784953 -1.9155011 -1.3341527 -1.4596343 -1.2275147 -2.6847544 -2.9996819 -4.5235672 -5.1648846 -5.9773655 -6.4552851][-3.9014962 -3.6160183 -3.9154155 -2.6338816 -1.678349 -1.2010703 -0.73746157 -0.52636194 -0.45225334 -2.0859408 -2.2508249 -3.3479648 -3.3503876 -4.2682743 -5.1976056][-3.1489992 -2.006721 -1.5854383 -1.2121267 -0.87862396 -0.16523933 0.27025652 0.43099022 0.64054394 -1.0493951 -1.2741623 -2.4834013 -2.6468258 -3.367743 -3.9692519][-2.1483417 -1.4288592 -0.93329573 0.23972225 0.68649292 1.2527084 1.2675829 1.2987547 1.846118 -0.0802412 -0.811244 -2.0235052 -2.4332595 -3.5080729 -4.0006146][-2.0234656 -0.79434013 0.15267658 0.80196381 1.0025902 1.3727379 1.3770523 1.2440653 1.1475601 -0.17805433 -0.56260777 -2.2444892 -3.105021 -4.1277528 -4.6920958][-1.8811059 -1.2083077 -0.769001 -0.23418999 0.629076 1.1647062 0.79955006 0.27069426 0.20049477 -1.4070449 -2.2465134 -3.5078611 -3.7509253 -4.5540438 -5.1976814][-4.4265785 -3.6069236 -3.0058546 -2.3116641 -2.3522782 -1.6998544 -1.3174734 -1.4816875 -1.9197869 -3.4171023 -4.2081432 -5.4338608 -5.8046074 -6.158473 -6.140337][-6.9939113 -5.820879 -4.8977036 -3.720299 -3.2986083 -3.5472374 -3.4143105 -3.6132646 -3.7555223 -4.7354283 -5.8001151 -6.5093684 -6.4371853 -6.6498137 -6.7663541][-6.9048557 -6.7187462 -6.5208611 -5.2927542 -4.4202914 -4.2793961 -4.2447934 -4.4388514 -4.5957146 -5.3140955 -5.5155225 -6.1756744 -6.6226435 -6.4294944 -5.8879566][-5.9558368 -5.7296896 -6.2727275 -6.0685983 -5.6077614 -4.9966292 -4.4645615 -4.7962627 -5.0776992 -5.6225176 -5.8235989 -6.2185354 -6.1706719 -6.122756 -6.0267768][-7.6734505 -6.2207575 -5.1558304 -5.6185818 -6.0517983 -6.1893654 -5.9200668 -6.012023 -5.8450828 -6.3744717 -6.4917746 -6.3758621 -6.1626477 -5.8557391 -5.8039079]]...]
INFO - root - 2017-12-15 14:23:50.793915: step 23010, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 55h:17m:37s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:23:57.263656: step 23020, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 55h:00m:25s remains)
INFO - root - 2017-12-15 14:24:03.646953: step 23030, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 55h:55m:18s remains)
INFO - root - 2017-12-15 14:24:10.093236: step 23040, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 53h:39m:51s remains)
INFO - root - 2017-12-15 14:24:16.598992: step 23050, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 55h:51m:32s remains)
INFO - root - 2017-12-15 14:24:23.012762: step 23060, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 56h:11m:11s remains)
INFO - root - 2017-12-15 14:24:29.414679: step 23070, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.653 sec/batch; 56h:06m:10s remains)
INFO - root - 2017-12-15 14:24:35.846910: step 23080, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 54h:51m:23s remains)
INFO - root - 2017-12-15 14:24:42.193950: step 23090, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 54h:03m:34s remains)
INFO - root - 2017-12-15 14:24:48.573412: step 23100, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 53h:53m:32s remains)
2017-12-15 14:24:49.035692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1209145 -3.3221846 -3.5959396 -3.9109845 -4.0886488 -4.0348535 -3.7302849 -3.1244688 -2.4846864 -3.7076983 -3.8615773 -4.78819 -5.7644863 -6.9581866 -7.7075977][-3.7726207 -4.007442 -4.2097464 -4.68159 -4.8756733 -4.6798296 -4.3152494 -3.7109718 -2.7774863 -4.202898 -4.2727709 -5.51626 -6.6064234 -7.4786372 -8.1238222][-4.0832872 -3.9616218 -3.9400604 -4.0813303 -3.8756733 -3.6301026 -3.2142019 -2.8865652 -2.2262487 -3.5291414 -3.830936 -4.8117647 -5.838387 -6.9703574 -7.7536073][-3.9713521 -3.632103 -3.5198584 -3.1430321 -2.6375213 -2.2542133 -1.9583621 -1.8939705 -1.6474538 -3.1943808 -3.5705538 -4.7661209 -5.5005608 -6.2815533 -6.7820091][-4.0813551 -3.3145247 -2.6309309 -2.1219597 -1.5720682 -0.93275833 -0.58359575 -0.68872833 -0.50062704 -2.2049975 -2.6576343 -3.8220916 -4.7620664 -5.4061785 -6.1029196][-3.9577696 -3.1946073 -2.7063575 -1.6682448 -0.62351036 0.23732424 0.68103218 0.74964809 0.89793015 -0.87260437 -1.5288739 -2.7772627 -3.8767335 -4.6919928 -5.2440996][-3.8526669 -3.0165172 -2.47792 -1.2804804 -0.30055141 0.70790005 1.3422489 1.5096512 1.6220675 -0.11125469 -0.89365339 -2.3236117 -3.5903163 -4.7461963 -5.40172][-4.0324612 -3.1511459 -2.24654 -1.0298038 -0.034965992 0.92229462 1.4793482 2.0277128 2.4560738 0.60464 -0.26177311 -1.7714386 -3.1658115 -4.5686412 -5.3328838][-3.9756014 -3.3321209 -2.6469541 -1.3548446 -0.43825197 0.4513216 0.88680553 1.3769274 1.686676 0.0120368 -0.65241337 -1.9634748 -3.2680683 -4.5737953 -5.4462256][-4.3176303 -3.6025047 -2.8948202 -1.8322639 -0.90844393 0.1054678 0.60317993 1.0774021 1.328249 -0.57901669 -1.4058623 -2.5425353 -3.46069 -4.4435081 -5.1356306][-5.4473572 -5.0280952 -4.4534388 -3.5852833 -2.4533992 -1.4394288 -0.8538413 -0.42976856 -0.26936674 -2.0066681 -2.7861776 -3.833524 -4.50266 -5.2981949 -5.4530077][-6.4472065 -6.0095229 -5.344224 -4.7295761 -3.8726118 -2.9235106 -2.2538934 -2.0884795 -1.9771285 -3.2850943 -3.8465405 -4.5673075 -5.1162825 -5.5417728 -5.7672377][-7.033309 -6.8356481 -6.2179956 -5.4733973 -4.5381336 -3.9674656 -3.5618219 -3.5541897 -3.7430809 -4.6565 -4.8565826 -5.0236359 -5.2466779 -5.3402424 -5.0532289][-6.9817557 -6.8256578 -6.4886136 -5.8480248 -5.232111 -4.5984654 -4.2142286 -4.3485336 -4.5322304 -5.0605936 -5.0508628 -4.8618574 -4.8670621 -4.8664136 -4.7231655][-7.9630518 -7.8542118 -7.3813577 -6.9986377 -6.6324396 -6.0811667 -5.9661403 -6.1012506 -6.2172623 -6.239656 -6.1696186 -6.0060763 -5.8663583 -5.6435728 -5.4156713]]...]
INFO - root - 2017-12-15 14:24:55.477631: step 23110, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 56h:17m:56s remains)
INFO - root - 2017-12-15 14:25:01.894865: step 23120, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 55h:40m:34s remains)
INFO - root - 2017-12-15 14:25:08.342389: step 23130, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 53h:43m:10s remains)
INFO - root - 2017-12-15 14:25:14.715365: step 23140, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 54h:34m:31s remains)
INFO - root - 2017-12-15 14:25:21.085103: step 23150, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 53h:45m:30s remains)
INFO - root - 2017-12-15 14:25:27.479031: step 23160, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 55h:38m:08s remains)
INFO - root - 2017-12-15 14:25:33.859436: step 23170, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 54h:48m:29s remains)
INFO - root - 2017-12-15 14:25:40.219671: step 23180, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 55h:09m:32s remains)
INFO - root - 2017-12-15 14:25:46.631641: step 23190, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 54h:18m:28s remains)
INFO - root - 2017-12-15 14:25:52.976701: step 23200, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 53h:26m:04s remains)
2017-12-15 14:25:53.490561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8053288 -1.7947383 -1.8552623 -1.6724014 -1.4040842 -1.2250018 -1.0142059 -0.78261805 -0.52455711 -2.1573706 -3.3751249 -4.99092 -6.1446714 -7.3359237 -8.0345955][-1.887414 -1.9554935 -2.2949367 -2.3851972 -2.4009523 -2.0874972 -1.6901746 -1.5145764 -1.2386317 -2.6333752 -3.7097139 -5.4562235 -6.7017913 -7.85812 -8.41966][-2.854661 -2.7762771 -2.9366603 -2.8811741 -2.6199288 -2.1456404 -1.6209683 -1.3517852 -1.0784559 -2.6118674 -3.77077 -5.3652105 -6.5581036 -8.0511522 -8.7130661][-3.8969805 -3.7093673 -3.5862842 -3.0444989 -2.2910566 -1.5095296 -0.74513006 -0.53495312 -0.37919998 -2.0732727 -3.3623662 -5.1079268 -6.4183121 -7.6907148 -8.2388535][-4.7728367 -4.3138342 -3.554594 -2.7753859 -1.8243356 -0.75694704 -0.061337471 0.14401913 0.29363966 -1.5141878 -2.9131556 -4.79228 -6.2223988 -7.6240335 -8.2529917][-5.1751842 -4.3791814 -3.5784736 -2.3153486 -1.151504 0.001039505 0.79167747 1.0315142 1.1480675 -0.85753441 -2.4034133 -4.4426146 -6.0341454 -7.480607 -8.2856092][-4.9439564 -4.27982 -3.1358418 -1.6971526 -0.3389802 0.90571022 1.5972862 1.8709641 2.0408783 -0.076059341 -1.9007101 -4.0566087 -5.7090726 -7.2670097 -8.0432768][-4.2975111 -3.1411328 -2.1085725 -0.7257309 0.64981556 1.6523018 2.2537279 2.5435209 2.6275129 0.52892494 -1.3151464 -3.6121664 -5.3955607 -7.0416465 -7.7711864][-3.6005378 -2.5523086 -1.6580396 -0.48251247 0.67114544 1.1853294 1.6223364 2.1037025 2.3057919 0.34492779 -1.3687792 -3.4812694 -5.2203274 -6.92916 -7.6452079][-3.2991972 -2.6102371 -1.7742724 -0.65325975 0.11646128 0.56773376 0.92401218 1.3532982 1.5284386 -0.49674797 -2.0618215 -4.0325718 -5.5703621 -6.9955621 -7.7817712][-5.0392394 -4.3095503 -3.4259911 -2.462883 -1.8249607 -1.3857708 -1.0974822 -0.79483318 -0.68028879 -2.4459486 -3.9087348 -5.2705297 -6.4130616 -7.5597482 -7.8204317][-5.9421577 -5.5087986 -4.6436963 -3.7313223 -3.1633806 -2.7817788 -2.7207775 -2.531322 -2.3836532 -3.6336617 -4.6699448 -5.67422 -6.3696413 -7.3370795 -7.4857936][-7.1704426 -6.7964921 -6.1134233 -5.3766422 -4.9120035 -4.4998045 -4.5208578 -4.5226336 -4.4973993 -5.1496468 -5.8391457 -6.2302442 -6.5212936 -6.9829278 -6.6457562][-7.4867821 -7.0735226 -6.5586061 -5.9940023 -5.3881664 -4.9721985 -4.9875422 -5.0620894 -5.0834546 -5.6228185 -5.9090447 -6.0693617 -6.1338425 -6.5522838 -6.17238][-8.1136446 -7.900888 -7.304636 -7.0657058 -6.7510118 -6.6161747 -6.8126192 -7.0334888 -7.1248631 -7.0584612 -6.9977579 -6.8491521 -6.632463 -6.5098624 -6.3073893]]...]
INFO - root - 2017-12-15 14:25:59.979764: step 23210, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 56h:00m:15s remains)
INFO - root - 2017-12-15 14:26:06.400780: step 23220, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 55h:58m:47s remains)
INFO - root - 2017-12-15 14:26:12.772726: step 23230, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 54h:12m:29s remains)
INFO - root - 2017-12-15 14:26:19.228719: step 23240, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 53h:40m:31s remains)
INFO - root - 2017-12-15 14:26:25.630166: step 23250, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.653 sec/batch; 56h:03m:55s remains)
INFO - root - 2017-12-15 14:26:32.054599: step 23260, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 54h:29m:38s remains)
INFO - root - 2017-12-15 14:26:38.503170: step 23270, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 55h:08m:10s remains)
INFO - root - 2017-12-15 14:26:44.964010: step 23280, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 53h:20m:10s remains)
INFO - root - 2017-12-15 14:26:51.350530: step 23290, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 56h:46m:10s remains)
INFO - root - 2017-12-15 14:26:57.786192: step 23300, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 56h:05m:46s remains)
2017-12-15 14:26:58.304205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8187351 -4.3551188 -4.8590236 -5.066288 -5.4268379 -5.4672174 -4.30361 -2.7640362 -2.1426706 -3.637291 -5.4064322 -5.1799154 -5.3970394 -7.0161309 -7.0923977][-5.5357323 -5.6856508 -5.4933939 -5.6341119 -5.5440311 -5.1864753 -5.1311064 -4.141222 -2.7733054 -3.2650685 -4.3582773 -5.2375441 -6.5353041 -6.8981957 -6.9680753][-4.8247571 -4.9454908 -5.2915382 -5.2778606 -4.7952614 -4.0356731 -2.8781333 -2.2286925 -1.7383585 -2.734962 -3.2524209 -4.349206 -5.8467836 -7.0068951 -6.5706406][-4.9945254 -4.4753914 -4.0394144 -3.2575612 -3.1386337 -2.6649504 -1.3525038 -0.41701221 -0.19178772 -2.1312122 -3.6994815 -3.3275743 -4.6582279 -6.311811 -6.4079227][-5.6909685 -4.2929254 -3.119267 -2.5891109 -1.8920808 -0.99915028 -0.36546993 0.0931344 -0.18109798 -1.8637557 -3.687336 -4.2175789 -4.9932294 -5.8929334 -5.7292538][-3.4687338 -3.0032544 -2.2499547 -1.3484745 -0.53000689 0.14625072 0.87379837 1.5271521 0.9774704 -1.7101541 -4.2139463 -5.109673 -6.56618 -7.4160752 -6.2250881][-2.0079732 -1.3806071 -1.2592235 -0.64215517 0.30265284 1.2519169 1.8459969 2.0620365 2.3424406 -0.5653162 -3.3785524 -4.8324695 -6.5222378 -8.0610514 -7.6498227][-1.6018915 -1.1062403 -0.819263 0.0847249 0.69611263 1.334156 1.5991707 2.1970596 2.3641653 0.1014452 -2.528923 -4.7276278 -6.1939297 -7.5455551 -7.0868526][-2.2242246 -2.0257173 -1.4526815 -0.68204975 0.54470539 1.2059011 1.6627264 1.9082489 1.8192568 -0.22576427 -2.6801386 -3.9314854 -5.8549509 -7.4890485 -6.837729][-3.3257437 -2.7159681 -1.5779538 -1.5657067 -1.2430568 -0.12448549 1.0277605 1.3059969 1.3990974 -0.58317757 -3.0875201 -4.3964224 -5.3707333 -6.5798082 -6.7474403][-4.5141616 -5.1870222 -4.89518 -3.7659957 -3.3432403 -2.6038318 -1.859498 -1.2463231 -1.3365798 -2.622613 -4.2973738 -5.2633371 -6.052443 -6.8203592 -6.3883424][-6.7847195 -6.4541278 -6.2242489 -5.9042764 -4.8441687 -4.6125326 -4.4190321 -3.506711 -3.0218139 -4.3069992 -5.4885902 -5.3512888 -5.8128543 -6.447793 -6.2892103][-7.4781661 -7.8460112 -8.01505 -7.5931373 -6.9427624 -6.229507 -5.231 -4.977469 -5.0261211 -5.2986507 -6.1647358 -5.6957049 -5.1694918 -5.5205956 -5.7680368][-7.3347149 -7.3157 -7.4724846 -7.0414572 -6.91203 -6.6017342 -6.5825639 -6.08761 -5.5660682 -5.0830727 -5.5953588 -5.7796626 -6.0788565 -5.4249353 -5.1453152][-7.8347416 -7.4809265 -7.3168397 -6.9962134 -6.701417 -6.6804876 -6.6209717 -6.5875278 -7.02451 -6.8987513 -6.1709089 -5.7359557 -5.6364317 -5.8986936 -5.8572288]]...]
INFO - root - 2017-12-15 14:27:04.716420: step 23310, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 54h:59m:54s remains)
INFO - root - 2017-12-15 14:27:11.127167: step 23320, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 56h:07m:58s remains)
INFO - root - 2017-12-15 14:27:17.522898: step 23330, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 53h:40m:47s remains)
INFO - root - 2017-12-15 14:27:23.973068: step 23340, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 54h:10m:51s remains)
INFO - root - 2017-12-15 14:27:30.375067: step 23350, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 56h:18m:14s remains)
INFO - root - 2017-12-15 14:27:36.836922: step 23360, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 54h:46m:56s remains)
INFO - root - 2017-12-15 14:27:43.243392: step 23370, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 54h:12m:38s remains)
INFO - root - 2017-12-15 14:27:49.596033: step 23380, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 54h:59m:51s remains)
INFO - root - 2017-12-15 14:27:56.033303: step 23390, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 56h:15m:08s remains)
INFO - root - 2017-12-15 14:28:02.403316: step 23400, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 53h:54m:05s remains)
2017-12-15 14:28:02.896548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9424667 -4.3014503 -3.7477715 -3.2948546 -3.8764961 -3.7249887 -4.0557661 -3.8895588 -3.4665613 -3.8934479 -3.9452295 -5.8023882 -6.5484557 -8.0226212 -9.8102875][-3.7018018 -3.2710819 -3.1333547 -3.0658846 -3.8317771 -4.4199219 -5.0075722 -5.3191328 -4.4792271 -4.7204914 -4.6477556 -6.59188 -6.7856369 -8.1361885 -8.9621782][-2.8865252 -1.8053355 -1.4728217 -1.8355117 -2.615797 -3.1414924 -3.5620251 -3.7669437 -3.7229581 -4.2122164 -4.50721 -6.4243116 -6.3143978 -7.2027483 -7.900022][-2.259634 -1.6554651 -1.05895 -0.68163157 -0.669508 -1.3275533 -1.9675355 -1.8792148 -2.235002 -3.4093466 -4.1790667 -6.6622 -6.41433 -7.041657 -7.2320666][-1.9763856 -1.0443778 -0.11846972 0.060930252 0.032937527 0.50298405 0.13588858 -0.22690916 -0.60895395 -2.0537696 -2.809751 -4.9654083 -5.0713139 -6.2210479 -6.8427324][-1.6242399 -1.203939 -0.62213135 0.28165054 0.66247177 1.4218292 1.9796333 1.8531237 1.865243 0.28118515 -0.65786648 -2.8198409 -2.9380908 -4.3056307 -5.0321054][-2.9600525 -1.843956 -0.69719887 0.63437176 1.6363583 2.4517946 3.07549 3.0847778 2.8943233 1.1277981 -0.31932497 -2.6709051 -2.8414483 -4.18168 -5.3069324][-2.9016867 -2.0998445 -1.5368333 0.54144287 2.1085882 3.1268682 3.6183729 3.1397629 2.7729855 1.240118 -0.25889349 -2.9783449 -3.0960569 -4.1840553 -4.765172][-3.7951066 -2.2711458 -1.8896747 -0.91099882 0.26685476 1.7467613 2.1243038 1.8164577 1.5761681 0.30060577 -0.29159975 -2.7783885 -3.3539038 -4.091001 -4.6514926][-4.5620484 -4.391408 -3.8419569 -2.3840508 -1.4919062 -0.25287104 0.44528103 0.54255676 0.91510677 -0.902987 -1.7251248 -3.5663743 -3.9017007 -4.433351 -4.3973169][-5.6866894 -5.6431179 -5.1299629 -3.9076307 -3.0431318 -1.8683991 -1.2039886 -0.97944975 -0.81206131 -2.4148593 -3.9039459 -5.2787371 -5.643198 -6.3313065 -6.6118855][-5.9929295 -6.1309462 -5.7565804 -4.6582041 -3.7214439 -2.4437432 -1.831327 -2.204834 -2.6608787 -3.4265819 -4.32825 -6.3862395 -7.0313416 -7.355937 -7.3367338][-5.7021265 -6.2120409 -6.1312604 -5.2992358 -4.5861359 -3.3272295 -2.5271993 -2.4433904 -2.7781825 -3.8886135 -4.856123 -6.1845212 -6.53726 -6.8772764 -7.1830993][-5.6973748 -5.4382019 -4.9517021 -4.28277 -3.7126303 -2.9505811 -2.4509211 -2.3286686 -2.7218757 -3.6908376 -4.5769892 -5.3620014 -5.3740215 -5.6917248 -5.4296031][-5.7588954 -5.6556673 -5.2456121 -4.6957417 -4.1193972 -3.4048934 -3.2866073 -3.3241248 -3.5630417 -4.0982332 -4.7749271 -5.4773674 -6.2939157 -6.2414579 -5.7637606]]...]
INFO - root - 2017-12-15 14:28:09.256448: step 23410, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 55h:43m:16s remains)
INFO - root - 2017-12-15 14:28:15.661499: step 23420, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 54h:55m:41s remains)
INFO - root - 2017-12-15 14:28:22.032811: step 23430, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 55h:52m:03s remains)
INFO - root - 2017-12-15 14:28:28.431364: step 23440, loss = 0.40, batch loss = 0.28 (12.6 examples/sec; 0.637 sec/batch; 54h:41m:19s remains)
INFO - root - 2017-12-15 14:28:34.814863: step 23450, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 54h:03m:57s remains)
INFO - root - 2017-12-15 14:28:41.276607: step 23460, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 56h:31m:56s remains)
INFO - root - 2017-12-15 14:28:47.677027: step 23470, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 53h:55m:40s remains)
INFO - root - 2017-12-15 14:28:54.105365: step 23480, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 55h:56m:58s remains)
INFO - root - 2017-12-15 14:29:00.545620: step 23490, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 55h:36m:59s remains)
INFO - root - 2017-12-15 14:29:06.921330: step 23500, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 53h:23m:39s remains)
2017-12-15 14:29:07.461709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2166877 -1.897912 -2.00532 -1.9779816 -2.0141187 -2.1446042 -2.3041415 -2.3251925 -2.5082207 -3.8310025 -4.8578649 -5.8292637 -6.5047884 -7.5755982 -8.2423458][-2.371202 -2.2300916 -2.6391044 -2.7922573 -2.8862505 -3.0733705 -3.3967586 -3.6158328 -3.7565775 -4.4723072 -5.1601973 -5.977129 -6.5889134 -7.471374 -7.9762273][-2.5289464 -2.1333823 -2.2023048 -2.078433 -1.8884935 -1.9216366 -2.2396269 -2.7060094 -2.9042397 -3.5119071 -4.0757227 -5.0936975 -5.8837934 -6.6213746 -7.1654968][-1.2540135 -0.53105736 -0.34460449 -0.73876619 -0.94754076 -0.83452654 -0.84347582 -1.3317595 -1.8950958 -2.9661875 -3.7145789 -4.68188 -5.3259616 -5.7155318 -6.0812173][-0.82626724 -0.037298203 0.45690918 0.21156406 -0.15164042 -0.076503277 -0.066140652 -0.56211138 -0.95814419 -1.6997638 -2.06042 -2.7152772 -3.2855005 -4.1877422 -4.9077826][-1.04702 -0.3586278 0.15729666 0.47067547 0.56268883 1.2084846 1.76931 1.5489721 1.3596907 0.30407333 -0.24065495 -1.2271581 -2.4016681 -3.8346984 -4.9187517][-1.7705517 -0.79554796 -0.0739131 0.50523281 0.77697945 1.2773085 1.7601852 1.7291756 1.7714834 0.62925339 -0.14648628 -1.2331042 -2.4743867 -3.8577747 -4.8167496][-2.1124511 -1.2210631 -0.63115025 -0.012462139 0.59923077 1.1169567 1.7007971 2.0447311 2.371954 1.3820076 0.66785812 -0.50320387 -1.962502 -3.3666358 -4.3918047][-3.1190748 -1.9922824 -1.227241 -0.71036434 -0.29634619 0.36973 1.0614681 1.7216482 2.3487043 1.1968117 0.39559269 -0.80406952 -2.356647 -3.8323424 -4.874012][-4.1762528 -3.503088 -2.6889958 -1.9218063 -1.4267626 -0.77933979 -0.29927063 0.19591713 0.72317982 -0.15037298 -0.87027407 -2.0436459 -3.5774441 -5.2117662 -6.0542088][-5.3522978 -5.1095009 -4.6602688 -3.7583375 -3.1398678 -2.6141853 -2.06606 -1.8812947 -1.7754598 -2.7608061 -3.6459084 -4.3703785 -5.2204828 -6.2436895 -6.9130249][-7.3625703 -6.9447737 -6.6502566 -5.9591 -5.3791165 -4.9339066 -4.4678316 -4.1540551 -3.8416348 -4.5719767 -5.088666 -5.5736952 -6.1559353 -6.952961 -7.503459][-7.6881557 -7.4826689 -7.3418732 -6.885777 -6.3275509 -5.9106722 -5.6438093 -5.6012692 -5.576582 -5.6911359 -5.76908 -6.0631247 -6.4553909 -6.9263625 -7.3311858][-7.3023481 -7.21001 -7.1580524 -7.0466962 -6.8728476 -6.3317423 -5.764184 -5.5832548 -5.4591751 -5.9101267 -6.215116 -6.3641744 -6.4588671 -6.5645075 -6.7093549][-8.079565 -7.9109917 -7.8069286 -7.7134719 -7.5511484 -7.0049829 -6.4448442 -6.4143224 -6.34232 -6.17988 -6.1305995 -6.0666013 -6.1340137 -6.1338663 -6.2145076]]...]
INFO - root - 2017-12-15 14:29:13.838261: step 23510, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 55h:04m:36s remains)
INFO - root - 2017-12-15 14:29:20.224314: step 23520, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 56h:38m:41s remains)
INFO - root - 2017-12-15 14:29:26.552021: step 23530, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 55h:00m:59s remains)
INFO - root - 2017-12-15 14:29:32.871539: step 23540, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 55h:26m:30s remains)
INFO - root - 2017-12-15 14:29:39.327833: step 23550, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 55h:10m:27s remains)
INFO - root - 2017-12-15 14:29:45.735738: step 23560, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 54h:32m:28s remains)
INFO - root - 2017-12-15 14:29:52.061609: step 23570, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 55h:09m:35s remains)
INFO - root - 2017-12-15 14:29:58.415053: step 23580, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 55h:33m:02s remains)
INFO - root - 2017-12-15 14:30:04.862137: step 23590, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 55h:55m:53s remains)
INFO - root - 2017-12-15 14:30:11.253738: step 23600, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 54h:08m:31s remains)
2017-12-15 14:30:11.795764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.14932 -5.0807838 -5.6872826 -5.7977266 -5.7860289 -5.14054 -4.3996897 -3.8840153 -3.3234849 -4.9922762 -6.01396 -7.5833883 -8.7669945 -9.9992056 -10.456418][-3.9066319 -3.9483814 -4.3880606 -4.4817972 -4.568862 -4.0368958 -3.5645142 -3.3510056 -3.4277768 -5.1588669 -6.4277377 -8.3618212 -10.129766 -11.12713 -11.254272][-2.2404809 -2.431119 -2.4091992 -2.7506289 -3.1883211 -2.1737857 -1.5869622 -1.7079964 -1.675704 -3.677165 -5.1192746 -6.9627595 -8.9901428 -10.658545 -10.536994][-2.079165 -1.7767987 -1.7795029 -1.984046 -1.6872673 -1.3058872 -0.22838783 0.33528423 0.34389496 -1.2858934 -2.6501307 -4.989131 -7.3766923 -9.1207685 -9.59052][-2.2931046 -1.4995222 -0.88372087 -0.49750757 -0.51472664 0.31380463 1.5032768 2.0855837 2.32765 0.61141872 -0.92924786 -3.2024841 -5.3674021 -7.6656275 -8.7987986][-2.6046095 -1.7022157 -1.0220585 -0.55943632 0.39574814 1.6206474 2.6550493 3.3674393 3.5678968 1.5313072 0.03675127 -2.3024812 -4.4495564 -6.5853148 -7.4041333][-3.17595 -2.6265888 -1.6625352 -0.72996664 0.7509861 2.4153996 3.4890928 4.2650061 4.7459803 2.2366037 0.54039192 -1.8853693 -4.1750851 -6.3696613 -7.4172654][-3.4722729 -2.8610396 -1.878499 -0.39018011 1.2575054 2.9557438 4.2747459 4.8427353 4.9008818 2.3520575 0.41617393 -1.9179864 -4.4641638 -6.3232088 -6.9240375][-4.244432 -3.3626766 -2.4940119 -0.74321747 0.57011223 2.3751478 3.4037504 3.5075598 3.5982647 1.230792 -0.49882746 -2.8990521 -5.0759993 -6.6328173 -6.6525927][-5.6110029 -4.9561443 -3.7759762 -2.593359 -1.4622164 -0.043435574 0.9154644 1.1472034 1.1640167 -1.2014294 -2.860023 -4.3238773 -6.0012522 -6.9950228 -7.268496][-6.619174 -5.8624854 -5.4083877 -4.32985 -3.4137259 -2.6855588 -1.9777908 -1.9195762 -2.3561978 -4.3109293 -5.5339694 -6.1865797 -7.2407293 -7.8109436 -7.8279953][-6.4029837 -5.8448968 -5.6942048 -5.1115108 -4.9207149 -4.7489505 -4.3431778 -4.5836172 -4.6262093 -5.854908 -6.8973217 -6.9856291 -7.5814934 -7.8943253 -7.3990226][-6.5671043 -6.8057442 -6.2943711 -6.4022026 -5.8655677 -5.6428957 -5.6877437 -5.4803004 -5.6358767 -5.9789553 -6.3511596 -6.2894745 -6.8230314 -6.4755778 -5.5546951][-6.9561877 -6.7250271 -6.5529447 -6.3586802 -6.2348566 -6.060194 -5.7009187 -5.4442124 -5.5065641 -5.5778389 -6.0276728 -5.5740442 -5.4309306 -5.5256853 -5.1675997][-6.3276944 -6.3734388 -6.4573 -6.3779345 -6.0691643 -5.9772329 -5.952167 -5.5831747 -5.4663138 -5.365098 -5.3319569 -5.3925281 -5.4962511 -5.4203396 -5.0924377]]...]
INFO - root - 2017-12-15 14:30:18.156225: step 23610, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 56h:53m:34s remains)
INFO - root - 2017-12-15 14:30:24.604388: step 23620, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 56h:06m:04s remains)
INFO - root - 2017-12-15 14:30:30.933828: step 23630, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 54h:23m:57s remains)
INFO - root - 2017-12-15 14:30:37.311667: step 23640, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 54h:58m:04s remains)
INFO - root - 2017-12-15 14:30:43.679655: step 23650, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 57h:24m:01s remains)
INFO - root - 2017-12-15 14:30:50.080844: step 23660, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 54h:01m:34s remains)
INFO - root - 2017-12-15 14:30:56.520619: step 23670, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 53h:55m:56s remains)
INFO - root - 2017-12-15 14:31:02.935786: step 23680, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 54h:24m:43s remains)
INFO - root - 2017-12-15 14:31:09.370328: step 23690, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:00m:45s remains)
INFO - root - 2017-12-15 14:31:15.759727: step 23700, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 53h:39m:55s remains)
2017-12-15 14:31:16.334033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.0912485 -7.9042816 -7.4695988 -6.9930506 -6.6512976 -6.4279528 -5.8840227 -5.1757722 -4.2545424 -4.9349871 -5.3154211 -7.0202041 -7.1761603 -8.12934 -8.8781719][-7.9398947 -7.8682408 -7.5600066 -7.3159947 -6.9590869 -6.5909977 -6.1104975 -5.6270223 -5.0464535 -5.1596556 -5.0017285 -6.6222534 -7.2031484 -8.08379 -8.5855188][-7.0305424 -6.5651441 -6.334774 -5.9395881 -5.5269232 -5.4756775 -4.7973175 -3.9683237 -3.3680267 -4.2331295 -4.7968445 -6.8626103 -6.9572468 -7.6777892 -8.3985195][-6.4641409 -5.4603791 -5.4823909 -4.6119356 -3.3478599 -2.5306554 -1.8424282 -1.7813859 -1.4433637 -2.1247606 -2.6428967 -5.4016519 -6.1933031 -7.1759639 -8.0886841][-5.8052087 -4.1458645 -3.3822236 -2.39142 -1.5445004 -0.49124289 0.6868248 0.8067379 0.68189621 -0.79369116 -1.3927197 -3.91045 -4.52013 -5.7963672 -6.7800155][-4.4565639 -3.3265967 -2.588582 -0.9807148 0.1478281 1.2338371 2.1315718 2.1299953 2.1457882 0.7033453 0.16517591 -2.5010371 -3.2234735 -4.5745721 -5.7233372][-4.1458416 -2.8637867 -1.8726277 -0.4299612 0.43086624 1.9001598 3.0934 3.0167589 3.0512667 1.5296869 0.8384037 -1.4587793 -2.1821918 -3.8707876 -4.7159443][-3.9994311 -2.8239532 -1.6108875 0.035149097 1.0631571 2.4081326 3.1720352 3.3341656 3.4282789 1.8360481 1.1247835 -1.0843878 -2.090878 -3.6962764 -4.5953684][-3.4893885 -2.7896075 -1.988513 -0.96826172 -0.28856611 1.114357 1.7471018 2.1955481 2.4879856 1.5400772 1.2543583 -0.9220829 -1.7526245 -3.4430313 -4.3287086][-3.6776509 -3.2380166 -2.5444031 -1.9037323 -1.697464 -0.64541674 -0.20253515 0.31814289 0.657177 -0.071855545 -0.20719814 -1.6939092 -1.957315 -3.4909725 -4.4783649][-3.8112695 -3.6567645 -3.2037644 -2.6351147 -2.4783974 -1.9007344 -1.9049168 -1.6373825 -1.6919188 -2.1201682 -2.1989336 -3.1858521 -3.5815859 -4.535697 -4.7928429][-4.0136185 -3.3922477 -2.8950968 -2.3466916 -2.5015211 -2.4147987 -2.4126573 -2.628499 -3.0882187 -3.7375863 -4.3115058 -4.7723355 -4.7167745 -5.2440329 -5.254292][-3.8995974 -3.9046044 -3.3255792 -3.1028309 -3.3159828 -3.5631151 -3.9653513 -4.5708032 -4.8755312 -5.4060383 -5.9677739 -6.0896072 -5.9512029 -5.8906507 -5.77367][-4.4531975 -4.2382751 -3.3960342 -2.7953153 -3.0376859 -3.3016067 -3.4884148 -4.1496077 -4.5959005 -5.2951689 -5.7533665 -5.835855 -5.6264343 -5.5756731 -5.6697702][-5.5182204 -4.418345 -3.5285 -3.3668342 -3.704128 -4.1846061 -4.8651004 -5.2727036 -5.3312244 -5.3581095 -5.3990135 -5.715539 -5.6624689 -5.6129446 -5.4614038]]...]
INFO - root - 2017-12-15 14:31:22.684833: step 23710, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 53h:24m:22s remains)
INFO - root - 2017-12-15 14:31:29.126545: step 23720, loss = 0.28, batch loss = 0.16 (11.6 examples/sec; 0.692 sec/batch; 59h:20m:31s remains)
INFO - root - 2017-12-15 14:31:35.563328: step 23730, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 54h:13m:39s remains)
INFO - root - 2017-12-15 14:31:42.006397: step 23740, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 54h:16m:05s remains)
INFO - root - 2017-12-15 14:31:48.378391: step 23750, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:45m:42s remains)
INFO - root - 2017-12-15 14:31:54.741369: step 23760, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 53h:47m:51s remains)
INFO - root - 2017-12-15 14:32:01.155897: step 23770, loss = 0.29, batch loss = 0.17 (11.7 examples/sec; 0.686 sec/batch; 58h:49m:54s remains)
INFO - root - 2017-12-15 14:32:07.674217: step 23780, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 55h:33m:11s remains)
INFO - root - 2017-12-15 14:32:13.927462: step 23790, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:37m:31s remains)
INFO - root - 2017-12-15 14:32:20.414793: step 23800, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 56h:35m:57s remains)
2017-12-15 14:32:20.923633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9064288 -5.4215422 -5.9094453 -5.4764934 -5.5201588 -4.8064885 -4.2807713 -3.7976141 -3.0004396 -3.4255371 -3.1423469 -5.0771074 -5.5564318 -6.6018448 -7.3505521][-5.1249466 -5.6643066 -6.452105 -6.0440426 -5.7394967 -5.1817741 -4.6191645 -3.792047 -3.1049733 -3.4222121 -3.155158 -5.1237106 -5.4167604 -6.2281523 -7.0858908][-4.220624 -4.576045 -4.7866917 -4.3910103 -4.4131026 -3.9766715 -3.4915113 -2.5984502 -1.6368032 -1.9153175 -1.8703308 -3.9206669 -4.5192385 -5.3354311 -5.9441652][-3.9977245 -3.9781878 -3.7465649 -2.6313248 -1.6968207 -1.1063018 -0.75787306 -0.67127132 -0.32838202 -0.95322847 -1.0409842 -3.3090577 -4.3700981 -5.0085554 -5.3957481][-2.6693249 -2.1339655 -2.0469117 -1.4608793 -0.90756369 -0.45282555 0.044023514 0.34208298 0.66293049 -0.47954273 -0.89427423 -3.255476 -4.1867809 -5.2739773 -6.1357889][-1.9247231 -0.80128 -0.46131516 0.35697746 1.2827711 1.8988934 2.2875481 2.2716942 2.4087315 1.2255287 0.58795071 -1.7770162 -2.2766004 -3.3210225 -4.4692459][-2.4251413 -0.6878562 0.21300697 1.0873165 1.5191202 1.9214058 2.237812 2.1249981 2.2813768 1.0231047 0.22429466 -2.5941863 -3.3707342 -4.2992077 -5.2775068][-3.4421554 -2.14679 -1.6932583 -0.3346591 0.7280159 1.5837469 2.0544882 1.446105 0.94852257 -0.34151745 -1.2225294 -3.6569929 -4.1800785 -5.4333878 -6.1912985][-3.9649322 -2.6114016 -2.2836328 -1.3805194 -0.58365726 0.2658391 0.78333187 0.67926121 0.3386488 -0.9838376 -1.9797268 -4.7412052 -5.0333529 -5.9209466 -6.4388757][-5.945013 -4.545629 -3.7650847 -2.0823574 -1.0126386 -0.36332035 0.095115185 -0.39134645 -0.9123702 -2.6758199 -3.2177072 -5.309844 -5.3114905 -6.1471944 -6.4260149][-7.0930753 -6.4537668 -6.212266 -4.7193594 -3.5486541 -2.5296278 -1.8344941 -2.3438182 -3.0724845 -4.8288326 -5.2865181 -6.7566862 -6.5590887 -6.880496 -6.7256584][-7.996696 -7.7550716 -8.0806837 -7.3593421 -6.7190542 -5.8975158 -4.7823362 -4.510088 -4.6299071 -5.8439174 -6.1927934 -6.932487 -6.2181039 -6.0750976 -5.6670227][-9.2332306 -9.3231449 -9.5934458 -8.6359758 -7.7048492 -7.2728829 -6.6483426 -6.5410709 -6.2079329 -6.3679032 -5.6625576 -6.5280914 -5.3484454 -4.6073704 -3.7668173][-8.6846876 -8.771739 -9.2266254 -9.0178576 -8.85631 -8.4720278 -7.7240896 -8.087369 -8.33746 -8.2832375 -7.132061 -6.2734056 -4.8188643 -4.1103735 -3.5786362][-8.0338278 -8.3589516 -9.3590031 -9.7529411 -9.8711824 -10.100585 -9.9405384 -9.5930853 -9.6337509 -8.9996119 -8.540679 -8.0502491 -7.1926122 -6.305562 -5.3750157]]...]
INFO - root - 2017-12-15 14:32:27.315893: step 23810, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:22m:33s remains)
INFO - root - 2017-12-15 14:32:33.749949: step 23820, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 54h:41m:33s remains)
INFO - root - 2017-12-15 14:32:40.123375: step 23830, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 54h:21m:13s remains)
INFO - root - 2017-12-15 14:32:46.593969: step 23840, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 56h:24m:02s remains)
INFO - root - 2017-12-15 14:32:53.045342: step 23850, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 55h:13m:37s remains)
INFO - root - 2017-12-15 14:32:59.489928: step 23860, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.657 sec/batch; 56h:19m:01s remains)
INFO - root - 2017-12-15 14:33:05.926375: step 23870, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 55h:36m:56s remains)
INFO - root - 2017-12-15 14:33:12.348531: step 23880, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 54h:10m:11s remains)
INFO - root - 2017-12-15 14:33:18.912710: step 23890, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 54h:27m:24s remains)
INFO - root - 2017-12-15 14:33:25.277530: step 23900, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 54h:06m:13s remains)
2017-12-15 14:33:25.783576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6595478 -6.2457304 -6.2429605 -5.3742981 -3.5568838 -1.8703341 -1.2732625 -0.90366125 -0.3196125 -2.0326619 -2.6255965 -3.8244963 -5.2149553 -5.9392405 -6.6740966][-6.0362768 -6.162375 -6.749536 -6.6240888 -5.3911839 -3.6712666 -1.7600327 -1.2396851 -1.2542295 -3.0901093 -4.1004696 -5.3368654 -6.4145083 -6.7442489 -7.1752996][-5.36757 -5.6926141 -5.7799048 -5.6106052 -5.1622229 -4.1223788 -2.8902121 -2.1701846 -1.7915716 -3.6885724 -4.8636045 -6.1650009 -7.4799323 -7.7183261 -7.9578514][-4.7124472 -5.6871257 -5.9878769 -5.3072543 -4.2195125 -3.0681252 -2.4660954 -2.0901942 -1.697031 -3.3696833 -4.5972018 -6.4156361 -7.5608974 -7.5853515 -8.481658][-5.912694 -6.5549054 -6.5701652 -5.54165 -3.7135978 -2.0471854 -1.1568403 -0.92264652 -1.1578174 -3.3369074 -4.3905015 -6.6682205 -8.129261 -8.5372086 -8.9507294][-8.38098 -8.1991358 -7.2274704 -5.2718935 -2.7798057 -0.92648268 0.88716125 1.1176281 1.3368979 -1.1294055 -3.0324717 -5.6230154 -7.7891126 -8.8972769 -9.2570868][-7.9452929 -7.1620731 -6.1987128 -4.5273104 -2.1872816 0.34786892 2.65137 3.1969452 3.2193232 1.1381397 -0.54777288 -3.6604381 -6.343317 -7.9760261 -8.7449131][-6.6897 -6.696732 -5.3988204 -3.0459695 -0.922328 1.4819813 3.2255554 4.8325119 5.7443953 3.3394413 1.1611843 -2.1539073 -5.1283789 -6.9796095 -7.8561087][-6.4340849 -5.191124 -4.8616476 -3.6315546 -1.6871595 0.81498718 2.9361057 3.4025011 4.1472321 2.946825 2.1964684 -0.76031446 -4.2655587 -5.9775157 -7.0173187][-7.18855 -5.5047555 -4.9874716 -4.1594334 -3.6695457 -1.7592974 0.95621777 2.7372398 2.8527374 0.29994631 -0.52507496 -1.9189324 -3.7550478 -5.4042344 -5.928494][-6.0578642 -6.0871191 -5.5404539 -4.70138 -4.1266079 -3.2324972 -2.3315954 -0.11364555 1.123498 -1.1203508 -3.1736174 -4.7654333 -5.418601 -5.9665165 -5.9538217][-5.3392539 -5.3853111 -5.1532 -4.4393425 -3.2535033 -2.0740118 -0.65930223 0.095734596 0.047133923 -1.7225976 -3.320878 -4.8269787 -6.0381737 -6.4031038 -6.0827703][-6.6316137 -5.4207811 -4.7187738 -3.8869121 -2.9658561 -2.0515418 -0.24390888 0.41689014 0.6203804 -2.442246 -3.403492 -4.0183964 -4.4115748 -5.3184862 -6.2646317][-7.1025314 -6.5206337 -5.8713856 -4.0875211 -3.6572771 -3.0410147 -1.8372717 -1.4035444 -0.8412323 -2.5310106 -3.2461023 -4.08571 -3.8726265 -4.2185478 -4.5046692][-6.899147 -6.1876554 -6.2916956 -5.3258944 -3.9634264 -3.0195513 -3.2161217 -3.4962187 -3.1624608 -3.8471754 -3.8683529 -3.8812237 -3.8791006 -4.5836105 -5.0901895]]...]
INFO - root - 2017-12-15 14:33:32.211075: step 23910, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 54h:56m:18s remains)
INFO - root - 2017-12-15 14:33:38.674369: step 23920, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 54h:23m:31s remains)
INFO - root - 2017-12-15 14:33:45.088986: step 23930, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 55h:17m:17s remains)
INFO - root - 2017-12-15 14:33:51.515766: step 23940, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 56h:35m:34s remains)
INFO - root - 2017-12-15 14:33:57.982359: step 23950, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 55h:20m:06s remains)
INFO - root - 2017-12-15 14:34:04.324705: step 23960, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 55h:13m:10s remains)
INFO - root - 2017-12-15 14:34:10.743549: step 23970, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 56h:32m:18s remains)
INFO - root - 2017-12-15 14:34:17.103230: step 23980, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 55h:13m:19s remains)
INFO - root - 2017-12-15 14:34:23.473458: step 23990, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 56h:16m:15s remains)
INFO - root - 2017-12-15 14:34:29.909196: step 24000, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 53h:27m:07s remains)
2017-12-15 14:34:30.456801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7881947 -5.5565987 -4.7034726 -4.8098364 -5.0150037 -4.0331068 -2.809463 -3.2028036 -3.5680809 -5.0701671 -4.828228 -4.9149117 -5.0517416 -6.0296254 -6.3908477][-6.5396914 -6.2068758 -5.9343467 -4.9291949 -4.0256519 -4.0986362 -3.649581 -2.7051148 -2.3077021 -4.3614478 -5.2849474 -5.7502861 -6.171483 -6.5820055 -5.9343834][-7.5764546 -6.9503794 -6.1005783 -5.3955688 -4.3259463 -3.5744715 -3.0695467 -2.9696822 -2.8735113 -4.1201458 -5.0324745 -5.6460657 -6.5518332 -6.9565158 -6.9630957][-6.9683294 -6.3119764 -5.8278227 -4.5358934 -3.2766523 -2.2838759 -2.1599698 -1.7409368 -1.6646476 -3.4102302 -4.3865914 -4.7653508 -5.776711 -6.9244537 -6.4275427][-6.9091587 -4.8935776 -2.9046178 -2.494185 -2.5811834 -1.39709 -0.80369043 -0.80981064 -1.5698357 -3.1751833 -3.7850277 -4.3461771 -4.8391342 -6.5111022 -6.6800494][-3.9791338 -3.3217421 -3.3301673 -1.802587 -0.11430359 0.27445984 0.074232578 0.29868555 0.23344612 -1.9395647 -3.4359102 -3.5462031 -3.8691986 -5.31129 -5.552702][-3.0423341 -1.1127834 0.9350338 1.0922413 0.5931921 0.94846058 1.6211739 1.1321306 0.40973091 -1.3366551 -2.2411709 -2.883492 -3.7239068 -4.7522068 -4.6472988][-1.3057108 -1.0981398 -1.0897479 0.38819885 2.1841688 2.1142473 2.1704645 1.8110266 1.29037 -1.1365075 -2.8409004 -3.3798776 -3.4846988 -4.2942839 -4.9213572][-1.3222203 -0.30561113 0.14542007 0.86658573 0.63196945 1.0304003 1.3533602 1.5623512 1.1831656 -0.94829178 -2.0501652 -3.4330955 -4.5441771 -4.9579058 -4.5662308][-1.8530955 -1.5864344 -1.8239942 -0.93176365 -0.018384933 0.348135 0.15634298 -0.15817833 -0.67220879 -1.9140944 -3.7749817 -4.2428865 -4.3862634 -5.645843 -5.5275822][-4.8585205 -4.3724575 -3.7011504 -3.3067245 -3.2047033 -2.7049394 -2.7380943 -3.0240941 -3.2406955 -4.2949476 -5.341218 -5.6484952 -6.25246 -6.56976 -6.4571943][-5.1765356 -5.432034 -5.429853 -5.1876335 -4.1919565 -3.294488 -3.2048535 -3.9620693 -4.5226407 -5.2317371 -6.1501312 -6.2254148 -6.91658 -7.1380243 -7.5349307][-3.7840569 -4.7009931 -4.9771066 -5.1277838 -5.17241 -4.8590217 -4.23279 -4.3101892 -4.6775508 -5.5453024 -6.7657008 -7.1623659 -7.4063234 -7.402071 -7.7008839][-5.2484951 -4.7672925 -4.8736582 -4.8688269 -4.0892267 -3.6303663 -3.910815 -3.6193252 -3.6562386 -3.8740361 -4.9632006 -5.7593174 -6.4716377 -6.8588009 -7.293808][-4.7136464 -4.8446336 -4.2035542 -4.4042568 -4.2051997 -4.2330151 -3.7972796 -4.1437883 -4.3050051 -4.4823532 -5.0130382 -5.6996679 -6.5968943 -7.0441031 -6.8851295]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:34:36.933008: step 24010, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 55h:47m:51s remains)
INFO - root - 2017-12-15 14:34:43.391109: step 24020, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 54h:00m:02s remains)
INFO - root - 2017-12-15 14:34:49.819346: step 24030, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 53h:58m:15s remains)
INFO - root - 2017-12-15 14:34:56.275870: step 24040, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 55h:17m:48s remains)
INFO - root - 2017-12-15 14:35:02.854395: step 24050, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 55h:59m:38s remains)
INFO - root - 2017-12-15 14:35:09.304602: step 24060, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 57h:03m:28s remains)
INFO - root - 2017-12-15 14:35:15.739056: step 24070, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 54h:06m:36s remains)
INFO - root - 2017-12-15 14:35:22.117524: step 24080, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 53h:36m:49s remains)
INFO - root - 2017-12-15 14:35:28.531642: step 24090, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 55h:23m:04s remains)
INFO - root - 2017-12-15 14:35:34.932227: step 24100, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 54h:01m:47s remains)
2017-12-15 14:35:35.475092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3441696 -4.0058784 -3.6563487 -4.3484335 -4.8512311 -4.9000816 -3.3015189 -2.3849187 -2.5055008 -4.0103741 -4.6068106 -4.6308956 -4.8003345 -5.6011362 -6.1176133][-4.1385489 -3.9350181 -4.3443794 -4.3036966 -3.3634391 -3.480413 -3.9872558 -3.5504708 -2.9209471 -3.4620333 -3.8848081 -4.6366453 -5.2347116 -5.7162857 -6.181716][-5.3874259 -5.1069822 -3.6583157 -2.5929618 -2.6605392 -2.397213 -1.9587998 -2.3760557 -2.561574 -3.7167735 -4.3368688 -5.1958342 -5.2223182 -5.456974 -5.5048342][-4.329 -4.5671535 -4.3225603 -2.5939932 -1.2259402 -1.0494323 -1.2210088 -1.2956405 -1.4319539 -2.9897728 -3.6341314 -4.7231293 -5.5642004 -6.3105545 -5.9709754][-5.5677218 -4.5469346 -3.3091788 -1.3555126 -0.08996582 0.28382158 0.16555691 0.13060093 -0.054638386 -2.0331078 -3.0234756 -4.0563717 -4.8714366 -5.7830276 -5.97268][-3.9772193 -3.1234107 -1.5709434 -0.4993329 0.75710297 2.4552431 2.9866238 2.2522984 0.97144413 -0.89230633 -1.7078848 -3.2443547 -4.3401308 -5.2025795 -5.3250322][-2.4986658 -1.3128409 0.18063211 1.640749 2.5308256 3.6239462 4.0566225 3.3905792 2.6232204 -0.078006744 -1.9112306 -3.0935192 -3.776094 -4.8788161 -5.52048][-0.96583748 -0.21075106 0.70476151 2.3695679 3.698143 3.8841181 3.3962612 2.8999176 2.4152231 0.05364275 -1.4411278 -3.1448898 -4.1714811 -4.8422608 -5.09939][-1.4150867 -0.48533344 -0.31248617 1.6061687 3.0522356 3.3625364 2.8393784 2.6430073 2.7754211 0.60181522 -1.4751549 -3.09512 -4.0567832 -4.8602409 -4.9812346][-2.2680445 -2.1918368 -1.5608115 0.091191769 0.998868 1.8635836 2.073988 2.0725031 1.299552 -0.57083178 -1.5218539 -2.9408302 -4.2504454 -5.0670943 -4.9043159][-4.045476 -3.0523262 -2.8490982 -2.5790577 -2.4807439 -1.72508 -0.81704426 -0.37348127 -0.48365211 -2.4929576 -4.5625381 -4.4669018 -4.7484989 -6.0272079 -6.5352759][-7.0306387 -5.9683928 -4.696516 -4.0506897 -3.872889 -4.0645008 -4.0166588 -3.3975215 -3.0574846 -4.4253912 -5.5644121 -5.5732012 -6.3114548 -7.0691195 -7.0937977][-7.8557787 -7.8519454 -7.3638053 -6.7281661 -6.2867546 -5.7515931 -5.6127729 -5.722414 -5.3628693 -5.686408 -6.54745 -7.061923 -7.5920706 -7.2582345 -6.6994429][-7.8470182 -8.1276274 -8.3473911 -7.6792789 -7.0783558 -7.0781407 -7.31915 -6.5413489 -5.9220996 -6.3714066 -6.373035 -6.1719322 -6.7058034 -7.3226585 -6.995388][-7.8103218 -7.1922684 -6.6585665 -7.6304007 -8.2122154 -7.2888932 -6.4127455 -6.9342852 -7.6895518 -6.7134027 -5.7105489 -5.9764318 -6.5529404 -6.8134112 -6.8637156]]...]
INFO - root - 2017-12-15 14:35:41.851564: step 24110, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 54h:22m:43s remains)
INFO - root - 2017-12-15 14:35:48.138247: step 24120, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 53h:59m:49s remains)
INFO - root - 2017-12-15 14:35:54.587876: step 24130, loss = 0.33, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 55h:48m:14s remains)
INFO - root - 2017-12-15 14:36:00.976432: step 24140, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.644 sec/batch; 55h:10m:29s remains)
INFO - root - 2017-12-15 14:36:07.347043: step 24150, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 55h:55m:09s remains)
INFO - root - 2017-12-15 14:36:13.808318: step 24160, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 55h:22m:45s remains)
INFO - root - 2017-12-15 14:36:20.292995: step 24170, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 57h:28m:01s remains)
INFO - root - 2017-12-15 14:36:26.682334: step 24180, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:06m:46s remains)
INFO - root - 2017-12-15 14:36:33.109647: step 24190, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 53h:41m:43s remains)
INFO - root - 2017-12-15 14:36:39.502794: step 24200, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 54h:47m:57s remains)
2017-12-15 14:36:40.036403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.694746 -2.3820219 -2.2010717 -2.1641288 -2.2322464 -2.2879572 -1.8560309 -1.629663 -1.3640332 -3.042294 -4.1981869 -5.4617558 -6.6868629 -7.6538491 -8.115016][-2.0962257 -2.1372824 -2.4398627 -2.6898623 -2.6697407 -2.5551782 -2.1172819 -1.6431875 -1.4097352 -3.0122342 -4.1080246 -5.4033718 -6.73001 -7.8760738 -8.26824][-2.6365814 -2.3238573 -2.4216924 -2.6208797 -2.5658612 -2.4167571 -1.9996252 -1.6971631 -1.434248 -2.9082675 -3.7597311 -4.7640018 -5.7670908 -6.5875716 -7.1440706][-2.6853342 -2.3872304 -2.3989148 -2.1495929 -1.8974171 -1.5973516 -1.1600723 -1.0425935 -0.77019119 -2.2665095 -3.1165237 -4.1528816 -5.2759562 -6.17833 -6.7184033][-3.1892285 -2.3730264 -1.6818094 -1.3848643 -1.0668893 -0.385952 0.056274891 0.13225126 0.29295492 -1.3932667 -2.403739 -3.5833421 -4.7708349 -5.7625628 -6.1908512][-3.5746055 -2.8438015 -2.2156172 -1.2945075 -0.39019823 0.54232883 0.97851753 0.94876289 0.93144321 -0.89794922 -2.0119681 -3.2355285 -4.3416486 -5.46305 -6.0046291][-4.3661556 -3.5242443 -2.3564811 -1.1623998 0.099940777 1.460741 1.9430656 1.8869467 1.8472853 -0.40830946 -1.952508 -3.4845324 -4.731966 -5.7297792 -6.1101923][-4.5622187 -3.3115401 -2.1342549 -0.99329138 0.50736618 1.7759027 2.4120331 2.6185369 2.6061945 0.35934544 -1.3215747 -3.2945304 -4.8611975 -6.1196136 -6.3460007][-4.5192394 -3.3447466 -2.1908827 -0.79066992 0.609663 1.5883589 2.0116024 2.1829596 2.1670933 -0.0854187 -1.7637978 -3.4346514 -4.8934097 -6.2169256 -6.69486][-4.7791052 -3.948451 -2.8730321 -1.4771962 -0.0026707649 1.0050268 1.4532385 1.6566219 1.5155363 -0.73347235 -2.38838 -4.2984486 -5.5422711 -6.5916638 -7.0654683][-5.9493723 -5.2803936 -4.415576 -3.2094536 -2.0861492 -1.2387395 -0.67237186 -0.37143183 -0.3041811 -2.3468652 -4.065114 -5.4510822 -6.3836975 -7.2614202 -7.3771009][-6.2352095 -5.6826053 -4.923049 -4.0668344 -3.235672 -2.7572832 -2.4477553 -2.2112646 -2.1372228 -3.4355469 -4.5520859 -5.5252986 -6.1683264 -6.8754811 -6.929153][-6.6131277 -6.0789137 -5.6428609 -5.0703983 -4.3715792 -4.119318 -3.8611615 -3.8122149 -3.9404442 -4.6664562 -5.2937727 -5.5722294 -5.932889 -6.108068 -5.9790192][-6.6405158 -6.446918 -6.3662777 -5.6875229 -5.394371 -5.2641592 -5.11707 -4.8697815 -4.6239319 -5.1168661 -5.2820482 -5.3598647 -5.5502377 -5.6870461 -5.7190189][-7.4675732 -7.0914435 -6.5037503 -6.1671848 -6.3340273 -6.4689918 -6.6208086 -6.7334037 -6.8759074 -6.5303478 -6.2024612 -6.1466889 -5.9948158 -5.9138432 -5.7658796]]...]
INFO - root - 2017-12-15 14:36:46.493099: step 24210, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.672 sec/batch; 57h:33m:52s remains)
INFO - root - 2017-12-15 14:36:53.006172: step 24220, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 55h:59m:34s remains)
INFO - root - 2017-12-15 14:36:59.450250: step 24230, loss = 0.22, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 53h:56m:23s remains)
INFO - root - 2017-12-15 14:37:05.818198: step 24240, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 55h:40m:56s remains)
INFO - root - 2017-12-15 14:37:12.227773: step 24250, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 54h:28m:44s remains)
INFO - root - 2017-12-15 14:37:18.629683: step 24260, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 57h:09m:30s remains)
INFO - root - 2017-12-15 14:37:25.018278: step 24270, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 54h:48m:48s remains)
INFO - root - 2017-12-15 14:37:31.451190: step 24280, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 53h:01m:44s remains)
INFO - root - 2017-12-15 14:37:37.909459: step 24290, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 54h:28m:22s remains)
INFO - root - 2017-12-15 14:37:44.352191: step 24300, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 57h:15m:19s remains)
2017-12-15 14:37:44.947651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1865749 -4.672328 -4.6727972 -4.5455675 -4.2262297 -3.3508492 -2.9645534 -2.3628545 -2.4337869 -3.1501861 -4.2363224 -4.7818356 -6.2199969 -7.7189822 -8.4512863][-3.9709532 -4.1282005 -4.1435633 -4.5306783 -4.4864826 -3.6079845 -2.6539764 -1.6087732 -1.621273 -3.0798912 -5.3242149 -5.9829664 -7.0996561 -8.12734 -8.7192526][-4.090879 -4.2586608 -4.6647072 -4.4148645 -3.9948123 -3.2035809 -2.3238769 -1.7418623 -1.6045537 -2.5160565 -4.0943217 -5.5866327 -7.8023758 -9.3398571 -10.166235][-3.4883122 -3.795959 -4.0328608 -4.1246552 -3.6540375 -2.2143116 -0.85780144 -0.33755541 -0.10049105 -1.0797181 -3.37884 -4.8408809 -7.1330614 -8.9150372 -9.9910383][-3.128912 -3.2293172 -3.135828 -3.3295937 -2.3276196 -1.0805244 -0.016753197 0.62730217 0.82104778 -0.14708662 -2.0985684 -3.639421 -6.2939453 -7.8688316 -9.358182][-3.3390665 -3.0911217 -2.8476386 -2.3279443 -1.3133326 -0.15856504 1.2429953 1.4800968 1.4698038 0.4327631 -2.0094023 -3.3299551 -5.4530306 -6.6365891 -7.5967059][-3.9224777 -3.3318691 -2.5033808 -1.4350076 0.18950129 1.4931812 2.5524683 2.6746683 2.6821308 1.2646914 -1.0686092 -2.458427 -4.6082568 -5.7550225 -6.3502421][-2.9979868 -2.8068066 -2.2074246 -0.84921551 0.83098125 2.4237528 3.6367903 3.7284603 3.2151365 1.9021578 -0.63338375 -2.0681534 -4.2842855 -5.5807595 -5.88257][-3.8100431 -3.1299553 -2.0102558 -1.1434855 0.48516369 1.6592436 2.9993572 3.5862808 3.2881279 1.5313902 -0.93236446 -2.1059737 -4.1406469 -5.6396351 -6.2131619][-4.2667112 -3.7831314 -3.1659327 -2.3528461 -1.2500825 0.051382065 1.5951748 2.2466059 2.5702047 1.0505943 -1.3973646 -2.7908068 -4.4120264 -5.6499977 -6.0314288][-5.2233219 -5.2352781 -4.7727289 -4.2735147 -3.4719586 -2.3288064 -1.0020704 -0.27402496 -0.19157839 -1.0297661 -2.6144881 -3.9710739 -5.1765475 -6.3959727 -6.3027916][-5.6606402 -5.3211851 -5.4629865 -5.1350689 -4.4909105 -3.5516891 -2.3082786 -1.9961286 -1.6714911 -2.1784406 -3.5791039 -4.8727479 -5.7315464 -6.958004 -7.3393521][-6.5419245 -6.2385845 -5.9395604 -5.5660963 -5.347218 -4.5358944 -3.6778288 -3.6661596 -3.4412751 -3.8228457 -4.1265149 -4.7847805 -5.7049494 -6.6800289 -7.13404][-6.3438444 -6.9231272 -6.7708311 -6.15455 -5.5794826 -4.7163086 -4.2045231 -4.3560514 -4.4310675 -4.4763231 -5.1975937 -5.6101866 -5.65458 -5.8370376 -6.0600548][-6.3669095 -6.2035179 -6.3849993 -6.298305 -5.4186606 -4.8595347 -4.6287403 -4.59332 -4.5033588 -4.7383471 -5.2031975 -5.429143 -5.6426048 -5.4896946 -5.6537137]]...]
INFO - root - 2017-12-15 14:37:51.462831: step 24310, loss = 0.39, batch loss = 0.28 (12.1 examples/sec; 0.662 sec/batch; 56h:40m:02s remains)
INFO - root - 2017-12-15 14:37:57.837333: step 24320, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 54h:30m:54s remains)
INFO - root - 2017-12-15 14:38:04.248468: step 24330, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 55h:14m:04s remains)
INFO - root - 2017-12-15 14:38:10.677468: step 24340, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 55h:52m:50s remains)
INFO - root - 2017-12-15 14:38:17.091677: step 24350, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 53h:54m:04s remains)
INFO - root - 2017-12-15 14:38:23.565760: step 24360, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 54h:56m:57s remains)
INFO - root - 2017-12-15 14:38:29.953480: step 24370, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 57h:02m:57s remains)
INFO - root - 2017-12-15 14:38:36.301635: step 24380, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 55h:00m:11s remains)
INFO - root - 2017-12-15 14:38:42.765027: step 24390, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 53h:32m:24s remains)
INFO - root - 2017-12-15 14:38:49.179411: step 24400, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 53h:30m:51s remains)
2017-12-15 14:38:49.686198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7384443 -4.4298267 -5.0033455 -5.6955104 -6.31899 -5.6586905 -4.9563074 -4.1647558 -3.5527134 -5.5932326 -6.6239882 -9.0111694 -10.744172 -11.556382 -11.865837][-5.207128 -6.9389071 -8.1209717 -8.4645386 -8.52785 -7.7071457 -6.2254949 -4.7364874 -3.5504766 -5.2616372 -6.9037142 -9.1651859 -10.658216 -11.582376 -11.889796][-4.4483385 -5.8097687 -7.4016228 -7.9869623 -7.9435153 -6.9067912 -5.137373 -3.6213031 -2.2913771 -3.7901661 -5.0541372 -7.6222215 -9.1275473 -10.567703 -10.98806][-4.6781473 -5.3368139 -6.0605607 -6.3578119 -6.2325463 -5.1453195 -3.0484071 -1.3790059 -0.35891676 -1.9660563 -3.3566427 -6.1267786 -8.1432772 -9.2548437 -10.007481][-4.4929 -4.2036448 -4.2172422 -4.6039619 -4.4114938 -3.1067858 -1.7394218 0.086282253 1.2147074 -0.35466862 -1.9684548 -4.7612543 -6.6292567 -8.4083462 -9.1966839][-3.2003784 -3.4287109 -3.8362312 -2.9714961 -2.0274172 -0.5355854 1.0529976 2.1282778 2.4715471 0.80944252 -1.328084 -4.5962334 -7.0566015 -8.1599874 -9.0251808][-2.8246045 -2.3265586 -2.4065819 -1.8221292 -0.91238356 0.75785828 2.2182474 3.0501127 3.3950167 1.2878399 -1.2878623 -5.1118665 -7.7773647 -9.5321512 -10.202925][-3.1737609 -2.883532 -2.3989558 -1.1693368 -0.17366791 1.1063843 2.3985405 2.6706228 2.9215269 0.83929157 -1.4313025 -5.110611 -7.518199 -9.1791248 -10.190304][-2.3931684 -2.0517006 -1.5047011 -0.76955128 -0.092274666 0.83157158 1.7870321 1.9587193 1.6663618 -0.64997387 -2.7030249 -5.5377035 -7.1632919 -8.679327 -9.3587036][-2.9184442 -2.8357811 -2.5718107 -1.8209581 -1.1990557 -0.15234661 0.51403618 0.23045826 0.099596024 -2.5902338 -4.5348964 -7.356389 -8.8326139 -9.1535463 -9.3050814][-4.5154781 -4.7086763 -4.6581306 -3.7420318 -3.2269239 -2.3533225 -1.4603086 -1.5080247 -1.9174161 -4.46325 -5.836174 -7.6591206 -8.3640461 -8.7698345 -8.7760334][-5.7329936 -5.843061 -6.3483067 -5.9020443 -5.4736452 -4.651082 -4.1054344 -3.9631927 -3.8161471 -5.6685648 -6.4046497 -7.575748 -7.8713703 -7.7138591 -7.697278][-5.9333506 -6.6400952 -7.1998305 -7.0261469 -6.6680145 -6.1221175 -5.5243425 -5.6812797 -6.0344496 -6.9788861 -6.9399595 -7.3369484 -7.6976933 -7.456275 -6.9008121][-6.6256795 -6.9410915 -7.0662141 -6.6597753 -6.2265763 -5.9653759 -5.9617758 -5.9243159 -5.7540579 -6.5581532 -6.7811623 -6.9844809 -6.8257117 -6.3982134 -6.4107056][-7.3762312 -7.7279868 -7.9382319 -8.136404 -8.1304827 -7.6396866 -7.3742161 -7.7432895 -8.0071487 -7.7980332 -7.4162893 -7.0767651 -6.72349 -6.48136 -6.476161]]...]
INFO - root - 2017-12-15 14:38:56.146825: step 24410, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 55h:25m:30s remains)
INFO - root - 2017-12-15 14:39:02.518532: step 24420, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.627 sec/batch; 53h:38m:22s remains)
INFO - root - 2017-12-15 14:39:08.926474: step 24430, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 54h:59m:50s remains)
INFO - root - 2017-12-15 14:39:15.376921: step 24440, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 56h:12m:55s remains)
INFO - root - 2017-12-15 14:39:21.851779: step 24450, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 54h:31m:45s remains)
INFO - root - 2017-12-15 14:39:28.236940: step 24460, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 54h:03m:59s remains)
INFO - root - 2017-12-15 14:39:34.653683: step 24470, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 53h:51m:17s remains)
INFO - root - 2017-12-15 14:39:41.114211: step 24480, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 56h:30m:44s remains)
INFO - root - 2017-12-15 14:39:47.485073: step 24490, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 53h:28m:46s remains)
INFO - root - 2017-12-15 14:39:53.870860: step 24500, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 55h:28m:42s remains)
2017-12-15 14:39:54.402227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4137068 -4.7685962 -4.402 -4.6454759 -4.78395 -5.0469923 -5.5388975 -5.2666264 -4.9006538 -4.5652533 -5.8484221 -6.32179 -6.226182 -7.1316156 -7.9684882][-3.9309444 -3.2811904 -3.1164474 -3.5457745 -4.281971 -4.5806022 -5.1753445 -5.5378971 -5.6020956 -5.3488569 -6.2240467 -6.4952173 -6.7488875 -6.6853466 -7.1279087][-3.1575933 -2.4840298 -2.1221452 -1.7615471 -2.0496554 -2.9463978 -3.6293197 -3.95011 -3.8159497 -4.3725061 -5.81661 -6.6518164 -7.0301013 -6.8725786 -7.1164465][-3.0950294 -1.9688931 -1.2542357 -1.3620996 -1.2656903 -1.1000314 -1.5306568 -1.9538174 -2.3354325 -2.6573067 -4.4805603 -5.2685194 -6.2549257 -6.6005764 -6.9213886][-2.8688383 -1.8260975 -0.98512506 -0.557796 -0.4442029 -0.12937498 0.19059181 -0.37532616 -0.97492838 -1.4907112 -3.4933252 -4.1069365 -5.2857685 -5.7770443 -6.5425072][-2.755322 -2.1267462 -1.4507966 -0.69693851 -0.20445395 -0.014034271 0.25648355 -0.00411129 -0.13430166 -0.75820684 -2.7500739 -3.0414238 -3.9218721 -4.5048294 -5.3583441][-2.9464312 -2.11663 -1.267519 -0.51667643 0.16661739 0.49750519 0.57761955 0.33729458 0.41321278 -0.37567711 -2.3605237 -2.8355408 -3.9926097 -4.9133248 -5.3987007][-3.2056241 -2.5645828 -1.8174939 -0.82540035 0.3511467 0.82204056 1.1764669 0.95850563 0.83604813 0.026798248 -1.8945527 -2.9238243 -4.12059 -4.9477897 -5.3525229][-4.3891559 -3.2773762 -2.5865564 -1.6769528 -0.4034853 0.18927765 0.43780422 0.72209358 0.71020794 -0.17097807 -1.898169 -3.1950469 -4.4706359 -5.0191431 -5.2713833][-5.1692877 -4.0596771 -3.2210398 -2.5369749 -1.743576 -1.0600519 -0.81878042 -0.70507193 -0.59074593 -1.3934178 -3.0483751 -3.9317324 -4.9176445 -5.5852909 -5.7954779][-6.7693329 -6.4908357 -5.6291742 -4.8715544 -4.0825148 -3.4825783 -2.744288 -2.5141125 -2.2772923 -2.5899844 -3.9870102 -4.7392154 -5.919487 -6.5032711 -6.7816582][-6.0332713 -6.5451107 -6.4583182 -5.6337972 -4.7267218 -4.0301104 -3.3283987 -3.1273246 -2.8487062 -3.1393456 -4.280241 -4.7152376 -6.0584559 -7.1995678 -7.6595278][-6.3641214 -6.7723126 -6.3500261 -6.1387057 -5.36534 -4.6843443 -4.0095344 -3.8601036 -3.7024143 -3.8176949 -5.1085634 -5.05219 -5.8538871 -6.9355321 -7.5458417][-5.0151758 -5.7667403 -5.1149592 -4.7839222 -4.1983457 -3.3772655 -2.8819914 -2.7272429 -2.8409081 -2.8502245 -3.8788006 -4.9327555 -5.5914207 -6.0623817 -6.3118467][-5.6128788 -5.9198279 -5.3329916 -5.3614063 -4.7373543 -3.6919446 -3.5078015 -3.4120231 -3.7749279 -4.0242696 -4.3485918 -5.0076861 -5.5129194 -5.9473643 -6.1448188]]...]
INFO - root - 2017-12-15 14:40:00.816999: step 24510, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 54h:57m:06s remains)
INFO - root - 2017-12-15 14:40:07.169694: step 24520, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 54h:50m:11s remains)
INFO - root - 2017-12-15 14:40:13.617685: step 24530, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 54h:53m:23s remains)
INFO - root - 2017-12-15 14:40:20.065724: step 24540, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 55h:31m:53s remains)
INFO - root - 2017-12-15 14:40:26.493230: step 24550, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 56h:19m:09s remains)
INFO - root - 2017-12-15 14:40:32.879144: step 24560, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:37m:10s remains)
INFO - root - 2017-12-15 14:40:39.265211: step 24570, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:26m:46s remains)
INFO - root - 2017-12-15 14:40:45.619470: step 24580, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 55h:41m:08s remains)
INFO - root - 2017-12-15 14:40:52.071378: step 24590, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 54h:19m:21s remains)
INFO - root - 2017-12-15 14:40:58.505528: step 24600, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 54h:03m:34s remains)
2017-12-15 14:40:59.000675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.614059 -6.9564714 -7.1533136 -6.6177421 -6.2783518 -5.4019289 -4.4468961 -3.8059652 -3.1748738 -4.0850239 -5.2382793 -6.4825025 -7.2297997 -8.2475863 -8.722415][-5.2448359 -5.6474495 -5.8768439 -6.6383662 -7.0277863 -6.4092493 -5.511095 -4.3303757 -3.0274472 -3.5612698 -4.6052661 -6.4408231 -7.4897995 -8.6457081 -9.1605082][-3.7488365 -4.6035271 -5.6396923 -6.3320503 -6.6457558 -6.3306003 -5.6452341 -4.2306123 -3.0725574 -3.2719617 -4.2739429 -5.9597573 -6.8206286 -8.427021 -9.3708448][-4.10494 -3.9530561 -4.2056012 -4.7769451 -4.8462725 -4.3928576 -3.3393273 -2.7742381 -2.0416327 -2.3492255 -3.6591611 -4.9409227 -5.7730312 -7.0322685 -8.0477533][-4.6722145 -4.1138043 -3.8100739 -3.4413276 -3.1857967 -2.4747505 -1.3006887 -0.629138 0.095388889 -0.69096565 -2.2767072 -3.6078143 -4.7193432 -6.4437604 -7.5580516][-4.5501766 -3.4012527 -2.5542703 -1.5837908 -0.26318264 0.74123573 1.4768505 1.5981121 1.7279091 0.028913021 -1.7520747 -3.2552547 -4.5078039 -5.7432194 -6.8693528][-2.5985556 -1.3818016 -0.36535263 0.66695976 1.5539274 2.3718805 2.9384918 2.8173008 2.7435284 1.1049385 -1.0326109 -2.9457693 -4.3855052 -5.8154197 -6.9575377][-0.70209408 -0.040518284 0.54314613 1.7949152 2.4805031 2.7687006 3.2453604 3.2554646 3.1708097 1.2189779 -1.3243909 -3.5052581 -5.0600147 -6.4148669 -6.8925924][-0.061184406 0.45069981 0.86994648 1.3379564 1.4288578 1.6311026 1.8040419 1.9481611 1.9784718 0.14723778 -2.03299 -4.3701048 -5.73754 -6.7970457 -7.1302595][-0.14595795 -0.29691505 -0.24529457 -0.10555172 -0.094969749 -0.017970562 -0.16871262 0.18226194 0.064886093 -1.7510829 -3.2190604 -4.8650522 -5.8862643 -7.0380583 -7.6697125][-1.6178389 -1.6416492 -1.2572947 -1.4128075 -1.5855722 -1.6469836 -1.620719 -1.7788839 -1.9390569 -3.3800983 -4.4244041 -5.6485696 -6.2352505 -7.0849333 -7.6835561][-3.8434594 -3.1579971 -2.7428813 -3.0075607 -2.8632588 -2.8200431 -2.9215555 -3.2381592 -3.4409356 -4.717773 -5.489985 -6.1606874 -6.2944555 -7.247767 -7.7476249][-5.9261365 -5.2582464 -4.81551 -4.7269354 -4.1630883 -3.8474679 -3.7860973 -4.0147104 -4.2907715 -4.8683548 -5.5418949 -6.1452575 -6.4599438 -7.2220559 -7.7808628][-6.6276131 -6.50743 -6.6057348 -5.67582 -5.0716534 -4.5888348 -4.035934 -3.8990557 -3.6327629 -4.1578331 -4.8355675 -5.2542429 -5.875689 -6.4629283 -6.7837372][-7.6978807 -7.4679184 -7.767355 -7.0985641 -6.8646359 -6.5065832 -6.2512093 -5.6606421 -4.9290481 -4.8570948 -4.78856 -4.8461914 -5.1704354 -5.7058048 -6.157886]]...]
INFO - root - 2017-12-15 14:41:05.416467: step 24610, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 54h:08m:08s remains)
INFO - root - 2017-12-15 14:41:11.844617: step 24620, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 53h:30m:32s remains)
INFO - root - 2017-12-15 14:41:18.173802: step 24630, loss = 0.23, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 53h:37m:33s remains)
INFO - root - 2017-12-15 14:41:24.575394: step 24640, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 54h:14m:55s remains)
INFO - root - 2017-12-15 14:41:30.988826: step 24650, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 54h:28m:09s remains)
INFO - root - 2017-12-15 14:41:37.473473: step 24660, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 54h:50m:36s remains)
INFO - root - 2017-12-15 14:41:43.850320: step 24670, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 54h:18m:21s remains)
INFO - root - 2017-12-15 14:41:50.283686: step 24680, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 53h:33m:29s remains)
INFO - root - 2017-12-15 14:41:56.602440: step 24690, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 54h:15m:28s remains)
INFO - root - 2017-12-15 14:42:02.967710: step 24700, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 53h:57m:52s remains)
2017-12-15 14:42:03.485187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0423064 -2.5717211 -3.552156 -4.2768135 -5.0187483 -5.4612808 -5.8009596 -5.7787161 -5.5896482 -6.374423 -7.2421 -8.6420918 -9.3063221 -10.13005 -10.854128][-2.6456985 -3.1246524 -3.9835441 -5.0124292 -5.9798141 -6.3869457 -6.4511166 -6.0775919 -5.6929708 -6.3795509 -7.5637555 -9.3502674 -10.290415 -11.211143 -11.931715][-3.0394711 -3.7817466 -4.3299952 -4.519743 -4.6359234 -4.7151828 -4.8343396 -4.9979668 -5.0313959 -5.7373152 -6.5043173 -8.1762133 -9.0440245 -9.7132616 -10.633673][-3.1613851 -3.4207602 -3.7472997 -3.8257377 -3.7023618 -3.349793 -2.8296504 -2.5075717 -2.1686807 -2.9562235 -4.1156769 -6.1358018 -7.3281326 -8.4393435 -9.0893593][-4.9293489 -4.587533 -4.0185738 -3.1590586 -2.290884 -0.89686823 0.015174389 -0.46253395 -1.0364757 -2.258317 -3.4461594 -5.1950483 -5.9401817 -6.7264924 -7.6982212][-5.8811736 -5.3201485 -4.47488 -2.904243 -1.6117539 -0.41885042 0.45537758 0.23588181 -0.19774008 -1.59547 -2.5788116 -4.2842207 -5.0605335 -5.5446224 -6.0377722][-6.4714036 -5.7228751 -4.6632576 -2.6999164 -0.74850368 0.93953323 1.9282255 1.4295444 0.695467 -0.8535223 -2.0168777 -3.944145 -4.74669 -5.344986 -5.7666841][-7.0424738 -6.1417642 -4.8830953 -2.9892206 -1.1988029 0.49825382 1.8108215 2.071085 1.8841715 0.028635979 -1.571125 -3.9734659 -5.0358582 -5.9252396 -6.4632821][-6.8544722 -6.4050336 -5.7295818 -4.1300354 -2.232368 -0.46946335 0.73796558 1.2817879 1.6188126 -0.32029676 -2.4698749 -4.9613056 -6.059864 -6.7994418 -7.0268497][-6.5329962 -5.975606 -5.5797243 -4.6999607 -3.4325223 -2.0169802 -0.79012871 -0.070691109 0.47138023 -1.108994 -2.7496405 -5.5994616 -6.9742908 -7.7333531 -8.3499212][-6.8352075 -6.6330395 -6.2701521 -5.8825583 -5.0194788 -3.7273793 -2.5531039 -2.0479178 -1.6768317 -3.0949359 -4.8435259 -6.5202427 -7.1963205 -8.2008238 -9.1439686][-6.2576833 -5.9798493 -5.4975815 -5.0750737 -4.7273479 -4.3076129 -3.9426749 -3.5640817 -3.070467 -3.3863473 -4.4937582 -6.6969972 -7.7709312 -8.068387 -8.4653692][-6.2312918 -6.0639882 -5.5775414 -5.056231 -4.8884363 -4.7348156 -4.5705862 -4.6380997 -4.8438139 -5.4271469 -6.0836763 -6.6453152 -7.1209054 -7.4026194 -7.6235728][-6.4802203 -6.1146555 -5.7703981 -5.21612 -4.8034363 -4.5733013 -4.4709024 -4.7846031 -5.0007048 -5.5189142 -6.2425976 -6.8434067 -6.9332333 -6.5971193 -6.73877][-7.4144249 -7.4619837 -7.1661029 -6.3195953 -5.6960359 -5.5798683 -5.5806808 -5.7731328 -5.9916654 -6.406168 -6.7777624 -6.832931 -6.6700706 -6.3420973 -6.0979004]]...]
INFO - root - 2017-12-15 14:42:09.994007: step 24710, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 55h:50m:57s remains)
INFO - root - 2017-12-15 14:42:16.367257: step 24720, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 54h:32m:29s remains)
INFO - root - 2017-12-15 14:42:22.882045: step 24730, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 54h:50m:47s remains)
INFO - root - 2017-12-15 14:42:29.259945: step 24740, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 53h:21m:11s remains)
INFO - root - 2017-12-15 14:42:35.667744: step 24750, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 54h:02m:33s remains)
INFO - root - 2017-12-15 14:42:42.038811: step 24760, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 53h:27m:19s remains)
INFO - root - 2017-12-15 14:42:48.406675: step 24770, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 53h:37m:37s remains)
INFO - root - 2017-12-15 14:42:54.826763: step 24780, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 54h:00m:19s remains)
INFO - root - 2017-12-15 14:43:01.243697: step 24790, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:36m:14s remains)
INFO - root - 2017-12-15 14:43:07.692081: step 24800, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 56h:32m:16s remains)
2017-12-15 14:43:08.225500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0185266 -6.7489185 -6.3096929 -6.0380917 -5.81604 -5.4708471 -5.0682211 -4.3933859 -3.9808309 -4.0020609 -4.7506304 -6.1377196 -7.071805 -7.6267977 -8.3581009][-7.2121167 -7.1886959 -7.1137276 -7.2913995 -7.0324149 -6.4301324 -5.9881392 -5.3248572 -4.6642771 -4.1247506 -4.7686329 -5.8035741 -7.0147 -7.6638002 -8.2411776][-6.57815 -6.8116479 -6.3950071 -6.1714621 -5.8776989 -5.5465913 -5.2232571 -4.6482229 -4.0839071 -3.8472567 -4.9685221 -5.9059591 -6.9079823 -7.5356097 -7.8441906][-5.73371 -5.4535465 -5.2207708 -4.772296 -4.0047836 -3.3696694 -2.7904305 -2.8151526 -2.5368724 -2.3483028 -3.9953692 -5.2103386 -6.5873179 -7.4630466 -7.8033762][-5.1162677 -4.315136 -3.3933578 -2.9221678 -2.2364755 -1.5456085 -0.91361523 -0.82622004 -0.82636404 -1.2068157 -3.0426865 -4.2998629 -5.462532 -6.6020703 -7.0762472][-4.3212538 -3.2963977 -2.4053383 -1.4475851 -0.40351057 0.22747946 0.92996025 0.80670738 0.71043682 0.27380371 -1.5001407 -2.8396025 -3.9979918 -5.1229696 -6.0789537][-4.1244183 -3.0716906 -2.0746922 -1.0131345 0.15201664 1.178092 2.0284948 2.04179 2.1024199 1.3824883 -0.085574627 -1.6127644 -2.9861455 -4.5393629 -5.2336435][-4.2297764 -2.9524269 -1.7213078 -0.37475014 0.88209057 1.6481361 2.4473648 2.4392061 2.5815125 1.7470646 0.46336365 -0.93514347 -2.6666827 -4.0998983 -5.1409364][-3.6153483 -2.7446856 -1.9922528 -1.1101241 -0.18955469 0.81158352 1.6291695 1.8819742 2.2051239 1.7798033 1.0223818 -0.62752151 -2.0648565 -3.5894547 -4.5816717][-3.7613745 -3.0701232 -2.5868216 -1.9344354 -1.4252224 -0.66174316 0.25715065 0.38558578 0.92590523 0.51579952 -0.12367249 -1.2779326 -2.2950058 -3.5230541 -4.5673742][-3.589911 -3.3460178 -3.0273924 -2.4908862 -2.4193492 -1.9093604 -1.5492907 -1.2399001 -0.93030643 -1.371819 -1.5930166 -3.0160909 -3.9200392 -4.2941694 -4.8665228][-4.0590038 -3.3736525 -2.6614747 -2.3861952 -2.3400869 -2.3599114 -2.6774979 -2.9630203 -3.1235785 -3.6589866 -3.934248 -4.8149862 -4.8887143 -5.1078205 -5.487339][-4.2332344 -3.7521529 -3.4447923 -3.0020447 -3.162487 -3.3674293 -4.0176487 -4.3100471 -4.3810196 -5.175868 -5.2544641 -5.8018265 -5.8864126 -5.5627127 -5.4598446][-4.2009015 -3.6476231 -2.7826252 -2.6101608 -2.8737636 -3.1145239 -3.4625263 -4.1858916 -4.7137737 -5.4324737 -5.7811146 -5.8978915 -5.6303096 -5.6437273 -5.6214142][-5.1104584 -4.2538471 -3.8557417 -3.3210955 -3.6536088 -4.3326464 -4.8977981 -5.2675743 -5.3858204 -5.8121152 -6.1776624 -6.3010182 -6.3668618 -6.1865683 -5.7814007]]...]
INFO - root - 2017-12-15 14:43:14.693067: step 24810, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 57h:33m:52s remains)
INFO - root - 2017-12-15 14:43:21.094311: step 24820, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 54h:52m:01s remains)
INFO - root - 2017-12-15 14:43:27.522109: step 24830, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 54h:15m:04s remains)
INFO - root - 2017-12-15 14:43:33.842282: step 24840, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 54h:03m:58s remains)
INFO - root - 2017-12-15 14:43:40.263647: step 24850, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 54h:38m:20s remains)
INFO - root - 2017-12-15 14:43:46.633194: step 24860, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 54h:23m:51s remains)
INFO - root - 2017-12-15 14:43:52.939433: step 24870, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 54h:50m:30s remains)
INFO - root - 2017-12-15 14:43:59.488680: step 24880, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 53h:34m:43s remains)
INFO - root - 2017-12-15 14:44:05.917729: step 24890, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 54h:38m:08s remains)
INFO - root - 2017-12-15 14:44:12.332041: step 24900, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 55h:27m:06s remains)
2017-12-15 14:44:12.894176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.922328 -5.77774 -5.1225352 -4.273365 -3.3017654 -3.1156125 -3.0989523 -3.0404768 -2.8826594 -3.3632498 -4.0019026 -4.3647208 -4.9123688 -5.5544176 -6.0543008][-4.1772609 -3.9300239 -3.5070457 -3.2013183 -3.1443191 -3.3441324 -3.4671798 -3.4451022 -3.1366963 -3.6903539 -4.11644 -4.8992467 -5.7102208 -5.7706518 -5.5895786][-3.2436194 -2.5454154 -1.7807765 -1.3907528 -1.0004101 -1.4027724 -2.1284161 -2.3762288 -2.3405504 -2.806334 -3.44525 -3.9607167 -4.7076039 -5.4733815 -5.7653604][-2.8331661 -1.8193641 -1.2624326 -0.59645605 -0.27030134 -0.55243826 -0.36419725 -0.78981256 -0.87275267 -1.7173605 -2.5912843 -3.4574647 -4.298934 -4.6838465 -4.9232216][-2.0026059 -0.50727463 0.79638767 1.1102953 0.903347 1.0263309 1.0324497 0.64368248 0.59686852 -0.016497135 -1.1230912 -3.0393238 -4.1036654 -4.7507181 -5.1053467][-0.82125187 0.34089756 1.4866886 2.2660971 2.5004234 2.5679092 2.6960659 2.3882208 2.2514658 1.5418625 -0.13515949 -2.1284676 -3.2829313 -4.0436993 -4.4324007][-1.726315 -0.43881226 0.91295338 1.7805471 2.0910225 2.2553759 2.2218485 2.1237106 2.1562452 1.2083693 -0.5309453 -2.3939071 -3.5039606 -4.2335167 -4.4259777][-3.2730293 -2.1505704 -0.77623129 0.28938198 0.95851517 1.7757196 2.2204485 2.1649294 2.1486454 0.87003708 -0.75475883 -2.2998853 -3.7954319 -4.5225768 -5.1280413][-3.0656934 -2.5896611 -1.9276786 -0.64017773 0.27861929 1.2313356 1.460743 1.6959677 1.8783836 0.12404013 -1.698626 -3.0311437 -4.2961006 -5.0882082 -5.59064][-4.4067116 -3.9501271 -3.2097979 -2.4031463 -1.3271642 -0.21882486 0.12351465 0.37033176 0.45176029 -0.93833542 -2.1107306 -2.8372216 -4.1182766 -5.0365467 -5.4164705][-5.7875991 -5.0484114 -4.6438293 -3.595551 -2.5490127 -1.8677239 -1.1993203 -0.89319324 -0.96064234 -2.0799413 -3.1187177 -2.9278908 -3.40774 -4.4416084 -4.877121][-6.3426032 -5.806478 -5.3054304 -4.4967875 -3.5962057 -2.7669797 -2.2564764 -2.0249591 -1.909832 -2.5511112 -3.0942359 -2.9004912 -3.2811022 -3.8770909 -3.9735537][-6.9538426 -6.1795192 -5.6955729 -4.8770537 -3.9617205 -3.2466726 -2.9013038 -3.2337713 -3.5847082 -3.8840611 -4.4446392 -4.0299354 -3.9394815 -4.14861 -3.9830453][-7.4491982 -6.7399144 -5.9629736 -5.2078714 -4.3777027 -3.7748842 -3.4993219 -3.7139628 -4.1864271 -4.793191 -5.3274889 -4.8313484 -4.4628439 -4.417017 -4.5286722][-7.788806 -7.2023835 -6.7144842 -6.2854052 -5.86242 -5.5243573 -5.2277317 -5.2929535 -5.3795662 -5.4566545 -6.0355272 -6.3495884 -6.2040224 -5.7330451 -5.6597042]]...]
INFO - root - 2017-12-15 14:44:19.230204: step 24910, loss = 0.38, batch loss = 0.26 (13.1 examples/sec; 0.611 sec/batch; 52h:12m:58s remains)
INFO - root - 2017-12-15 14:44:25.617137: step 24920, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 54h:41m:04s remains)
INFO - root - 2017-12-15 14:44:32.107522: step 24930, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 54h:50m:15s remains)
INFO - root - 2017-12-15 14:44:38.493944: step 24940, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 54h:50m:55s remains)
INFO - root - 2017-12-15 14:44:44.801384: step 24950, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 54h:34m:50s remains)
INFO - root - 2017-12-15 14:44:51.195001: step 24960, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 54h:40m:49s remains)
INFO - root - 2017-12-15 14:44:57.603258: step 24970, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 55h:00m:27s remains)
INFO - root - 2017-12-15 14:45:04.030685: step 24980, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 53h:42m:28s remains)
INFO - root - 2017-12-15 14:45:10.409220: step 24990, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 55h:00m:06s remains)
INFO - root - 2017-12-15 14:45:16.877633: step 25000, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 57h:26m:53s remains)
2017-12-15 14:45:17.397194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.244998 -6.4502425 -7.2632217 -8.55296 -8.49715 -7.1945033 -5.6521621 -4.4833703 -4.3505926 -5.6058817 -5.3411007 -5.679399 -5.3744445 -7.5716786 -7.7295156][-6.4657593 -5.9406614 -4.8175554 -6.1115403 -5.9930973 -6.5796061 -7.4212136 -6.5439439 -6.3893352 -7.6605349 -5.4935522 -7.2554135 -6.6163378 -6.2329292 -7.1864409][-5.1617169 -5.4972572 -5.6416855 -4.2115655 -2.4326711 -3.4923372 -4.4050369 -5.8604941 -6.4963374 -7.6605468 -6.7895494 -7.1054516 -6.775466 -6.91372 -6.9097314][-2.5940928 -3.3771143 -3.5727487 -3.4678731 -2.807951 -2.4119487 -1.7760105 -2.9659734 -4.1054 -5.309381 -4.9637518 -5.992918 -5.8936143 -5.819953 -5.8696332][-2.6686115 -1.8188787 -1.8615303 -1.260263 -1.6116114 -0.75696373 -0.73206472 -1.3929253 -1.9351015 -3.8143208 -3.0520263 -3.9313567 -3.5946469 -3.4521914 -4.3462973][-2.7884641 -1.9044251 -1.8167291 -0.88155174 0.50194454 0.4620924 0.095627785 -0.22769403 -0.57383204 -2.0485616 -1.6597843 -2.9318061 -2.292829 -3.0639048 -2.9613671][-2.5914092 -3.1392503 -2.6298809 -1.7251816 0.012691975 0.10708523 -0.10009623 -0.41845417 -0.51581621 -1.4272184 -0.5810318 -1.743423 -2.2041154 -3.84637 -3.9061475][-2.32866 -2.4241629 -1.7478447 -2.0870247 -0.88977242 -0.29601908 0.052444458 -0.20130444 -0.32534456 -1.3196254 -0.9059453 -2.2346349 -2.21109 -4.0838337 -5.1243753][-2.8797402 -3.4380312 -2.4888086 -2.6726327 -1.816761 -0.37889433 -0.2571764 -0.56011677 0.35751724 -1.1849647 -2.029706 -3.1912794 -3.7067959 -4.3675375 -5.0771132][-4.3775396 -2.4532557 -2.4426904 -1.2831817 -1.5964828 -1.6589751 -1.1642408 -0.86939621 -0.580317 -1.9641562 -2.6905146 -4.4534941 -4.84406 -5.6655807 -5.943512][-7.196723 -5.8368526 -3.8748364 -2.4459991 -1.499918 -1.1272917 -1.6716113 -2.9418826 -2.9553833 -4.5288448 -4.130826 -4.623064 -5.9220362 -5.682827 -5.9216857][-5.9371085 -6.4227014 -7.2466092 -6.6043334 -5.3372383 -4.563982 -3.8574381 -4.4371705 -3.8636944 -4.9224377 -6.1321034 -6.4014235 -6.5301495 -5.49682 -5.6484432][-6.6062775 -5.8191905 -6.0670509 -7.20294 -7.5050941 -7.5680146 -6.4389405 -6.5105567 -6.3818259 -6.8065853 -6.6293941 -6.4112792 -6.5908866 -6.3195825 -5.9839859][-6.720048 -5.4806061 -5.30186 -5.30478 -5.7100639 -6.6573944 -6.7948246 -6.7603507 -7.1719036 -6.8874297 -6.610321 -5.6844416 -5.7433004 -6.1939907 -5.9897451][-4.9866538 -5.6153712 -6.6328659 -6.7919674 -6.2341571 -6.9636879 -7.0337439 -7.1547222 -6.7623296 -6.7511215 -6.4873996 -6.1850905 -6.2832708 -5.8756828 -5.69471]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 14:45:24.666228: step 25010, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 54h:40m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:45:31.020664: step 25020, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 55h:09m:01s remains)
INFO - root - 2017-12-15 14:45:37.450393: step 25030, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 54h:40m:32s remains)
INFO - root - 2017-12-15 14:45:43.759915: step 25040, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 54h:49m:56s remains)
INFO - root - 2017-12-15 14:45:50.110670: step 25050, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 54h:09m:53s remains)
INFO - root - 2017-12-15 14:45:56.614293: step 25060, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 54h:06m:23s remains)
INFO - root - 2017-12-15 14:46:03.054805: step 25070, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 54h:27m:13s remains)
INFO - root - 2017-12-15 14:46:09.554327: step 25080, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:46m:08s remains)
INFO - root - 2017-12-15 14:46:16.033702: step 25090, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 55h:08m:25s remains)
INFO - root - 2017-12-15 14:46:22.367656: step 25100, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 55h:33m:52s remains)
2017-12-15 14:46:22.874552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5135975 -3.1762071 -3.2610927 -3.8704064 -5.0042181 -5.3389716 -5.3233504 -4.9198585 -4.5484076 -4.5792565 -5.6833363 -7.3017426 -7.8135567 -7.8503423 -8.33004][-3.2815194 -3.2764769 -4.0245829 -4.794939 -5.3064995 -5.342392 -5.26348 -5.2032185 -4.8003707 -4.581543 -5.7890315 -7.1687717 -7.6457653 -7.5624261 -7.882772][-2.9469361 -3.1780047 -3.829169 -3.9414942 -3.8729947 -3.6815147 -3.2645049 -3.4681377 -3.5003037 -3.4224777 -4.7656078 -6.7898068 -7.4979067 -7.631978 -8.0523767][-3.921612 -3.4433136 -3.8140736 -4.0349488 -3.6176915 -2.9925141 -2.5297956 -2.3963408 -2.3428164 -2.7123985 -3.8484533 -6.2367067 -7.3673325 -7.514122 -7.9942484][-3.7600737 -2.8126287 -2.5233741 -2.6677289 -2.6301694 -1.6433263 -0.84074211 -0.66739988 -0.95695877 -1.711987 -3.2163749 -5.9759626 -7.1387615 -7.5315065 -7.823246][-5.85372 -5.0030756 -4.1346641 -3.3195491 -2.4174204 -0.748549 1.1808186 1.5946531 1.1354866 -0.24672842 -2.4385967 -5.3234406 -6.9485707 -7.6511216 -7.7172403][-5.3134489 -5.1214528 -4.3259554 -2.66825 -1.4149923 0.23975086 2.5384312 3.3887167 3.4894972 2.1921539 -0.29974461 -3.8034551 -5.7780657 -6.4264493 -6.7259254][-4.4882689 -4.5810871 -3.6730771 -1.406713 0.90809917 2.5635681 3.8744335 4.5743141 5.252635 4.014245 1.6955147 -2.2761583 -4.6829891 -5.1880455 -5.4442444][-4.8769789 -3.5189133 -2.7321396 -1.4278035 0.39219856 2.2116575 3.5073261 4.0208664 4.2868061 3.1600695 0.86900043 -2.7494717 -4.9663486 -5.7987366 -5.7245803][-6.5061817 -5.7720613 -4.9055495 -3.3772154 -2.038703 -0.24787617 1.1202412 1.1073322 0.85313034 -0.39246178 -2.2342892 -4.6977468 -6.4973903 -6.9282136 -6.7978516][-7.4392452 -7.65695 -6.8083425 -5.4124374 -4.1748848 -2.6980047 -1.4394631 -1.0134101 -0.59307718 -2.0466361 -3.880147 -6.2919803 -7.5491381 -7.4267368 -7.221909][-8.7249575 -8.5611734 -8.4653931 -7.4643917 -6.4152851 -5.6280327 -4.7144947 -4.4630518 -4.2258706 -4.8005552 -5.756731 -7.1036248 -7.2047453 -7.2934947 -7.654829][-8.019702 -7.590167 -7.7091832 -7.2312326 -6.8706446 -6.4441681 -5.8008366 -5.5334139 -5.7337074 -5.9043913 -6.2428379 -6.6953139 -6.9396925 -6.9515123 -6.871727][-8.1320448 -8.2915039 -7.740612 -6.6667786 -6.139636 -5.6506224 -5.9298582 -6.5405769 -6.50619 -6.5783892 -6.6007442 -7.1430902 -6.9885244 -6.6303349 -6.2830772][-9.8747215 -9.5755568 -8.9778194 -8.1513433 -7.6216149 -7.3706951 -6.8569136 -7.1682663 -7.8129692 -7.9659343 -7.7603378 -7.5015564 -6.6961613 -6.2001295 -5.6138725]]...]
INFO - root - 2017-12-15 14:46:29.266683: step 25110, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 54h:04m:23s remains)
INFO - root - 2017-12-15 14:46:35.628151: step 25120, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 55h:11m:26s remains)
INFO - root - 2017-12-15 14:46:42.041968: step 25130, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 54h:05m:33s remains)
INFO - root - 2017-12-15 14:46:48.479066: step 25140, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 53h:48m:07s remains)
INFO - root - 2017-12-15 14:46:54.874291: step 25150, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:33m:10s remains)
INFO - root - 2017-12-15 14:47:01.296272: step 25160, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 54h:14m:25s remains)
INFO - root - 2017-12-15 14:47:07.693747: step 25170, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 56h:12m:02s remains)
INFO - root - 2017-12-15 14:47:14.179892: step 25180, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 53h:20m:13s remains)
INFO - root - 2017-12-15 14:47:20.516114: step 25190, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 53h:34m:36s remains)
INFO - root - 2017-12-15 14:47:26.993535: step 25200, loss = 0.23, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 55h:37m:02s remains)
2017-12-15 14:47:27.478162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2695847 -6.5188637 -6.3495297 -6.3856783 -7.1107225 -7.0962825 -6.091567 -4.4542408 -3.2714705 -2.4650002 -2.7107534 -4.6387596 -5.4533677 -6.0241303 -6.45644][-5.500392 -5.5299578 -5.6968293 -6.09981 -6.8442631 -7.3175139 -6.9139233 -5.5186329 -3.7572939 -2.5602498 -3.3285313 -5.458847 -6.5273438 -6.3381319 -6.490592][-4.5815897 -4.9210939 -5.5795465 -5.71558 -6.5511 -6.9824872 -6.8034368 -5.9156637 -4.6941586 -3.6607952 -3.8771763 -6.654695 -7.6493897 -7.3347454 -6.9617357][-5.1264458 -4.9623489 -4.9840155 -4.854383 -5.2638397 -5.2944374 -4.8689289 -4.01442 -3.1330385 -3.1743007 -4.5446424 -7.28497 -8.32258 -8.0241442 -7.9371834][-5.0895762 -4.1679249 -3.0929227 -2.542263 -2.4925394 -1.7620859 -0.6907239 0.071662426 0.57752419 -0.12351656 -2.3535614 -6.0230026 -7.9589539 -8.1748753 -7.9777789][-4.2930832 -2.820395 -1.9783306 -0.89008045 0.016546249 1.177124 2.6037321 3.3884039 3.8857851 2.8336639 0.71495819 -3.3277144 -5.7625065 -6.8354626 -7.6365933][-4.3069572 -2.7808347 -0.91993284 0.37925339 1.3679972 2.6782856 3.6533298 4.1322336 4.3555288 3.2513332 1.2722664 -2.093667 -4.6213121 -5.5644541 -6.4871316][-3.4953494 -2.3796415 -1.3568192 0.1168642 1.815979 2.7588005 3.8055372 4.1521349 4.0450058 3.2547665 1.2915382 -2.246624 -4.1680641 -5.0490012 -6.1309204][-4.0854049 -3.1390662 -2.2159786 -0.27527714 1.0527925 1.8761635 2.41607 2.6184196 3.0839233 2.0113583 0.002679348 -2.9030447 -4.631135 -5.0762548 -5.6530991][-5.2736983 -4.5706263 -4.0790429 -2.6392365 -1.2337661 -0.12710619 0.27009964 0.28687429 0.16004419 -0.71511269 -2.1215577 -4.4876766 -6.1154828 -6.1237144 -6.3618274][-7.0280337 -6.8299365 -6.481297 -5.6968117 -4.96459 -3.4880724 -2.1524754 -2.3057165 -2.879849 -3.7556429 -4.7718353 -6.4156876 -6.9677029 -7.4094262 -7.9890614][-8.2647457 -8.44149 -8.005641 -7.3950992 -6.9612422 -6.5872078 -6.1228061 -5.5915642 -5.1333017 -6.1015759 -6.6552372 -7.3795872 -7.7437544 -7.8768735 -8.18494][-9.4094944 -9.7904282 -9.4137115 -8.7262764 -7.5686059 -6.9173284 -7.1012697 -7.3671904 -7.576561 -7.7084746 -7.7572007 -7.55163 -7.0151157 -6.8131957 -6.9896979][-8.2975149 -8.6026087 -9.0014944 -8.3221655 -7.5529122 -7.1201243 -6.3313708 -6.6534395 -6.824532 -7.0711074 -7.61858 -7.5685229 -7.3105731 -6.7839193 -6.4212732][-7.0569696 -7.3003993 -7.6430879 -7.5906987 -7.5950847 -7.5193505 -7.1814303 -7.1894455 -6.7219524 -7.2550664 -7.62578 -7.3684354 -7.6273193 -7.2154613 -6.82659]]...]
INFO - root - 2017-12-15 14:47:33.916468: step 25210, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 52h:39m:18s remains)
INFO - root - 2017-12-15 14:47:40.384292: step 25220, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 53h:59m:28s remains)
INFO - root - 2017-12-15 14:47:46.734413: step 25230, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 55h:28m:27s remains)
INFO - root - 2017-12-15 14:47:53.093479: step 25240, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 53h:02m:33s remains)
INFO - root - 2017-12-15 14:47:59.475525: step 25250, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 52h:57m:04s remains)
INFO - root - 2017-12-15 14:48:05.881065: step 25260, loss = 0.33, batch loss = 0.22 (11.8 examples/sec; 0.681 sec/batch; 58h:05m:39s remains)
INFO - root - 2017-12-15 14:48:12.321826: step 25270, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 55h:19m:55s remains)
INFO - root - 2017-12-15 14:48:18.771835: step 25280, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 54h:36m:19s remains)
INFO - root - 2017-12-15 14:48:25.056393: step 25290, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 53h:00m:22s remains)
INFO - root - 2017-12-15 14:48:31.380304: step 25300, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 56h:03m:55s remains)
2017-12-15 14:48:31.867513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7721591 -3.699141 -3.6286101 -3.7944016 -3.6853676 -3.477685 -3.1580772 -2.9190998 -2.5315242 -3.8514607 -4.6799841 -5.2987247 -5.9868789 -6.4279938 -7.0174794][-3.5256591 -3.5132761 -3.6496558 -3.8753204 -3.7337842 -3.2161818 -3.0774951 -2.981307 -2.7675242 -4.0692706 -4.7710838 -5.6286564 -6.2040377 -6.83967 -7.3249617][-2.5239453 -2.6414094 -2.8388019 -3.0574217 -2.95396 -2.6867619 -2.3114872 -2.0715795 -1.8317037 -3.0371375 -3.6143193 -4.3429351 -5.4245219 -5.9970136 -6.6500673][-2.4250836 -2.54354 -2.6123848 -2.3692594 -1.9390597 -1.4397311 -0.94212675 -0.76752186 -0.71061039 -2.147491 -3.0305662 -4.0611267 -5.1505284 -5.9172449 -6.52547][-2.6886463 -2.2657485 -1.8440723 -1.3176045 -0.56259966 -0.089987278 0.39319134 0.49663258 0.50891495 -1.2085271 -2.444242 -3.5885434 -4.8095522 -5.71081 -6.4946337][-3.71255 -3.0901093 -2.1412225 -1.2453866 -0.28897524 0.57568836 1.1759357 1.1785793 1.0625925 -0.81786728 -2.1659513 -3.5994725 -5.0090055 -5.8470154 -6.533771][-4.0787134 -3.4995451 -2.6132708 -1.1669736 0.30810308 1.3262272 1.9450951 1.9022007 1.7904844 -0.36588764 -2.1020956 -3.6860704 -5.1916566 -6.2526674 -6.9893594][-4.0015554 -3.4053216 -2.1782207 -0.66864157 0.69507313 1.6683207 2.1516714 2.099494 1.9062433 -0.22735834 -2.0102072 -3.7471795 -5.2805891 -6.3683996 -7.0925026][-4.6536417 -3.5622721 -2.5993524 -1.1454787 0.18708229 0.83814049 1.2427616 1.4611206 1.5385485 -0.63624525 -2.3731952 -4.068696 -5.7548246 -6.9126029 -7.519155][-4.9547319 -4.2558622 -3.3011527 -1.9735637 -0.7315526 -0.30825233 0.055901051 0.47161961 0.58776665 -1.6254144 -3.1260138 -4.8006325 -6.1752124 -7.3197284 -8.1038723][-6.0989695 -5.2645597 -4.5014334 -3.4658604 -2.540298 -2.2275753 -2.0222669 -1.8510313 -1.6691055 -3.5752344 -5.0360074 -6.2081642 -7.1862922 -8.028717 -8.3438787][-6.2579889 -5.5323305 -4.7542858 -4.1789474 -3.6309729 -3.5359011 -3.5390902 -3.4116349 -3.3283353 -4.5214229 -5.5790658 -6.5474982 -7.1668596 -7.8532248 -8.0338411][-6.2799659 -5.7298822 -5.1147108 -4.6514416 -4.3906474 -4.6768155 -4.7299585 -4.7954664 -4.7975588 -5.5719709 -6.3890128 -6.7138329 -7.260479 -7.67668 -7.4014778][-5.9281054 -5.50688 -5.1425762 -4.6301222 -4.4177556 -4.6778579 -4.8360014 -4.9493856 -4.9970379 -5.6764078 -5.9496145 -6.0320406 -6.1156235 -6.5149112 -6.4187455][-7.469718 -7.2071881 -6.7187247 -6.1805334 -5.9986148 -6.12687 -6.3711586 -6.6045561 -6.7368088 -6.6393566 -6.6793885 -6.6732459 -6.472455 -6.3029037 -6.1945281]]...]
INFO - root - 2017-12-15 14:48:38.351676: step 25310, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 55h:37m:53s remains)
INFO - root - 2017-12-15 14:48:44.710841: step 25320, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 55h:33m:14s remains)
INFO - root - 2017-12-15 14:48:51.057663: step 25330, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 54h:32m:27s remains)
INFO - root - 2017-12-15 14:48:57.426385: step 25340, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 53h:26m:14s remains)
INFO - root - 2017-12-15 14:49:03.813262: step 25350, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 56h:47m:47s remains)
INFO - root - 2017-12-15 14:49:10.173572: step 25360, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 53h:20m:33s remains)
INFO - root - 2017-12-15 14:49:16.435081: step 25370, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 55h:27m:08s remains)
INFO - root - 2017-12-15 14:49:22.823459: step 25380, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 54h:32m:06s remains)
INFO - root - 2017-12-15 14:49:29.213555: step 25390, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 55h:41m:07s remains)
INFO - root - 2017-12-15 14:49:35.556783: step 25400, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:59m:57s remains)
2017-12-15 14:49:36.061680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.34912872 -0.3774662 -0.38002729 -0.63040781 -0.92792654 -1.2672167 -1.453063 -1.4976649 -1.5903139 -2.7317567 -3.3535891 -5.2124043 -5.9098349 -7.3151169 -7.5746694][-1.5211735 -1.3887048 -1.7659998 -2.0176129 -2.3545074 -2.3190155 -2.2507234 -2.4227238 -2.451787 -3.7776909 -4.5942802 -6.3965635 -7.0362453 -8.0525923 -8.0153255][-3.2770591 -2.9917216 -3.1637578 -3.1019073 -3.0926356 -2.912818 -2.8351274 -3.0534911 -3.245038 -4.6468515 -5.3086548 -7.1341152 -7.7464914 -8.6808586 -8.56309][-4.8960767 -4.6151876 -4.1665668 -3.5629835 -3.1729331 -2.7791562 -2.467947 -2.5656114 -2.7176952 -4.1303568 -4.9636307 -7.0790772 -7.8697805 -8.7877607 -8.3440952][-6.1907692 -5.200069 -4.2401528 -3.413137 -2.529428 -1.7569132 -1.4146328 -1.4780755 -1.6530333 -2.9155221 -3.6298356 -5.69341 -6.4722943 -7.5200071 -7.3887119][-6.3851705 -5.5169139 -4.8537636 -3.337481 -1.9314265 -1.0231228 -0.59712791 -0.47567225 -0.5131278 -1.7028656 -2.3893638 -4.2240705 -4.9248753 -6.1297045 -6.5469437][-6.0661387 -4.8451672 -3.6997027 -2.064539 -0.70307779 0.090828419 0.23857927 0.577261 0.73431396 -0.44909477 -1.1104736 -3.016984 -3.9492993 -5.0404763 -5.439579][-5.0699863 -3.7610581 -2.385376 -0.55512571 0.99815178 1.6245852 1.6192551 1.5481281 1.3859243 0.15219641 -0.48972464 -2.3621531 -3.0942388 -3.9255967 -4.1745405][-3.7405555 -2.3330417 -1.0060616 0.51728821 1.2578897 1.6166668 2.1513853 1.8412981 1.496418 -0.0034079552 -0.42452526 -1.9649477 -2.428709 -3.2056832 -3.5271902][-3.9675212 -2.2768641 -0.935709 -0.1099391 0.50097656 1.1491442 1.3394833 1.6490278 1.5735722 -0.60625267 -1.3581181 -2.6503243 -3.0249052 -3.7499483 -3.8433912][-5.5057631 -4.1091089 -3.1528215 -2.0199442 -1.5643692 -1.1315942 -0.61502647 -0.16542292 -0.12019014 -1.6469336 -2.7660294 -4.4983397 -4.6066608 -4.8495288 -4.4118733][-7.2441316 -6.4713745 -5.2294779 -4.1600327 -3.6445637 -3.0936446 -2.4859419 -2.2966013 -2.2130876 -2.9023709 -3.5530334 -5.0496755 -5.4724784 -5.7637558 -5.25742][-8.4276333 -7.6479626 -7.0297275 -6.3172917 -5.4027119 -4.5656166 -4.02228 -3.8451378 -3.6137552 -4.1509523 -4.4202871 -5.1100521 -5.6899967 -5.814631 -5.1868439][-8.763485 -8.2309 -7.6696448 -6.8126612 -5.8625908 -5.0536423 -4.3149729 -4.2852373 -4.2809386 -4.5826797 -4.8189335 -5.161418 -5.1013584 -5.1459594 -4.7484756][-9.07285 -9.2491922 -9.1549454 -8.134901 -6.9328351 -5.6936646 -5.2673397 -5.2349386 -5.4048939 -5.6457586 -5.7265711 -5.8734426 -5.6253357 -5.3332014 -5.1087866]]...]
INFO - root - 2017-12-15 14:49:42.557576: step 25410, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.631 sec/batch; 53h:51m:59s remains)
INFO - root - 2017-12-15 14:49:48.975100: step 25420, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 53h:57m:36s remains)
INFO - root - 2017-12-15 14:49:55.344582: step 25430, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 52h:51m:49s remains)
INFO - root - 2017-12-15 14:50:01.639493: step 25440, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 52h:37m:29s remains)
INFO - root - 2017-12-15 14:50:07.956936: step 25450, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 53h:14m:29s remains)
INFO - root - 2017-12-15 14:50:14.278629: step 25460, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 54h:04m:07s remains)
INFO - root - 2017-12-15 14:50:20.633429: step 25470, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 57h:18m:01s remains)
INFO - root - 2017-12-15 14:50:27.031813: step 25480, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:38m:54s remains)
INFO - root - 2017-12-15 14:50:33.461510: step 25490, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 54h:30m:01s remains)
INFO - root - 2017-12-15 14:50:39.882564: step 25500, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 55h:00m:24s remains)
2017-12-15 14:50:40.392274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5736918 -4.3713675 -4.244668 -4.1253443 -4.5044723 -5.0061388 -5.3441954 -5.6878386 -6.0355878 -7.6373825 -9.136632 -9.7121506 -10.491356 -10.428023 -9.8061218][-3.9024935 -3.7465811 -3.9784672 -4.1602 -4.6703734 -5.2147894 -5.7110896 -6.0527692 -6.150826 -7.4573932 -8.4316254 -8.503293 -9.6401606 -9.9530392 -9.8981237][-2.5354824 -2.9141951 -3.0929885 -3.3035393 -3.7276969 -4.075695 -4.5067215 -4.8353715 -5.0301971 -6.2202387 -7.21018 -7.6569834 -8.404439 -8.7153263 -8.7460918][-3.7306621 -3.2338185 -3.1028972 -3.0723705 -3.1610956 -3.0117087 -2.397676 -2.1395869 -1.8092813 -3.1471505 -4.8392296 -5.2822504 -6.7574983 -7.703691 -7.7403831][-4.3095722 -3.7902496 -3.0559559 -2.6141729 -1.6678648 -0.91734505 -0.5384326 0.10280848 0.90888405 -0.62425661 -2.4464707 -3.8306563 -5.7413192 -6.4856944 -7.0002346][-4.1351752 -3.2738042 -2.3592997 -1.1283083 -0.075273514 0.7657671 1.6659756 2.0829258 2.2580605 0.75406742 -1.4162078 -2.8077559 -5.0794654 -6.5145082 -6.624826][-3.8017037 -3.1243458 -1.559495 -0.42047644 1.2282686 2.4078674 3.04395 3.2975168 3.4271202 1.5425014 -0.78986311 -2.5042634 -4.9888048 -6.2355671 -6.9791608][-3.2761641 -2.8027315 -1.5017443 -0.10620546 1.4130383 2.599658 3.3915367 3.4286127 3.5622158 1.5687323 -0.9337678 -2.6835489 -4.9916048 -6.50712 -7.2485423][-2.3032417 -1.5275507 -0.68629503 0.13355684 0.92282772 1.6164331 2.1307163 2.2518368 1.9813766 -0.059724331 -1.9566174 -3.5013342 -5.4937043 -6.5820036 -7.3478227][-1.9006453 -1.4072051 -0.63596964 0.15297174 0.5075655 0.73382187 0.85757732 0.70597553 0.49914932 -1.4473114 -3.2746954 -4.446353 -5.7147446 -6.8266273 -7.126853][-4.0319314 -3.5585008 -2.5685039 -1.958056 -1.1634364 -0.8376298 -0.91843271 -1.1124754 -1.1845198 -2.9240994 -4.9327478 -5.9401569 -6.6857796 -6.9319997 -6.6876969][-5.4308729 -4.6820493 -4.0573626 -3.6560683 -3.0015068 -2.7823086 -2.7190938 -2.912838 -2.8924022 -4.382473 -5.6576271 -6.2551408 -6.7540903 -7.0196981 -6.7055683][-6.1734848 -5.4018903 -4.7183352 -4.2579546 -3.9555616 -3.9420462 -3.95229 -4.164957 -4.3730035 -5.36127 -6.3588891 -6.5503817 -6.7791448 -6.5686722 -5.7692738][-6.2009692 -5.8505116 -5.3959389 -4.8930039 -4.6672549 -4.4650903 -4.43266 -4.7699165 -4.9945083 -5.7500024 -6.2699494 -6.2641721 -6.2309942 -5.9689622 -5.5817947][-7.2502632 -6.8974791 -6.1992812 -5.942143 -5.9819093 -5.7365112 -5.6041927 -5.9101624 -6.0781493 -6.3043571 -6.4881339 -6.3520222 -6.2486076 -5.9934311 -5.3674026]]...]
INFO - root - 2017-12-15 14:50:46.854622: step 25510, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 55h:09m:34s remains)
INFO - root - 2017-12-15 14:50:53.386408: step 25520, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 55h:27m:43s remains)
INFO - root - 2017-12-15 14:50:59.841797: step 25530, loss = 0.23, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 53h:52m:30s remains)
INFO - root - 2017-12-15 14:51:06.219780: step 25540, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:19m:51s remains)
INFO - root - 2017-12-15 14:51:12.566311: step 25550, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 54h:47m:42s remains)
INFO - root - 2017-12-15 14:51:18.882985: step 25560, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 53h:25m:31s remains)
INFO - root - 2017-12-15 14:51:25.287228: step 25570, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 54h:55m:52s remains)
INFO - root - 2017-12-15 14:51:31.636734: step 25580, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 53h:46m:02s remains)
INFO - root - 2017-12-15 14:51:37.964555: step 25590, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 52h:20m:51s remains)
INFO - root - 2017-12-15 14:51:44.362369: step 25600, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.629 sec/batch; 53h:35m:09s remains)
2017-12-15 14:51:44.885982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8382735 -2.0430069 -2.8310518 -3.3503132 -4.0909519 -4.6424947 -4.0173635 -3.6366243 -2.6481738 -2.578949 -2.5970368 -3.0442958 -2.762341 -3.8761189 -4.7979903][-3.5498 -3.1066718 -3.0684171 -3.2783055 -3.79498 -4.18777 -3.9083357 -3.4657459 -2.2472944 -2.4322972 -2.6179819 -4.0481119 -4.5752096 -4.725091 -4.838871][-4.2154803 -3.25497 -2.61208 -2.6975789 -2.4537601 -2.5015926 -2.4785962 -2.0195827 -1.3159709 -1.2752566 -1.8520265 -3.7048564 -4.170435 -4.7169018 -5.1077852][-4.1187091 -2.7433114 -2.1138644 -1.4443641 -1.1403866 -0.94287395 -0.1037364 0.03125 0.24427557 -0.49448586 -2.0781102 -3.7162545 -4.1071596 -4.5098677 -4.9758778][-5.048089 -3.0388594 -1.4363313 -0.77231932 -0.063977242 0.56790733 0.98223877 1.4350939 1.631958 0.34938049 -0.83899784 -3.1570396 -4.4052658 -5.2324781 -5.7896214][-4.5890069 -4.0482464 -2.7710857 -1.1004219 0.27575827 1.5364647 2.3085051 2.5745926 2.4282742 1.0713682 -0.19448471 -2.5248947 -3.3618979 -4.457099 -5.1129456][-5.1473713 -3.7979262 -2.3507771 -0.51807261 0.80872345 1.8688507 3.00214 3.3481617 3.3564482 2.1285267 0.19862843 -2.5575938 -3.7766962 -4.7531643 -5.4151182][-3.6833863 -3.2726917 -2.2564626 -0.23447037 1.4998035 2.8425016 3.7230816 3.7588911 3.6889887 2.0696831 0.17203236 -2.3931866 -4.0987859 -5.0059385 -5.5198846][-3.2710514 -2.5103741 -1.6032448 -0.94058561 0.078048229 1.5476904 1.9710283 2.3358784 2.6654119 1.142087 -0.7952075 -3.1879759 -4.8152871 -5.8190622 -6.4418683][-5.2987289 -4.2905149 -3.166172 -1.9898715 -0.78862858 0.028998375 0.076497078 0.57749557 0.79674244 -0.743989 -2.0821247 -3.8025198 -5.0379024 -5.4224095 -5.944067][-6.6201086 -6.7109714 -6.7668839 -5.4693928 -4.2829366 -2.8715177 -1.7163887 -1.6686053 -2.1091886 -3.1560607 -4.0790029 -5.0849552 -5.6684279 -5.6590343 -5.7127857][-8.2275734 -8.0579185 -7.9934554 -7.6256905 -6.83401 -5.5296326 -4.6248155 -4.5452995 -4.495224 -4.7482462 -5.6873579 -6.1255112 -6.2662182 -6.1481156 -5.9148574][-9.0843458 -9.12622 -9.3740578 -8.8687706 -8.205615 -7.41942 -6.8143778 -6.2782912 -5.7212272 -5.5800447 -6.0653863 -6.3573356 -6.5060949 -6.2091727 -5.9428377][-9.266221 -8.99604 -8.6029654 -8.2691422 -8.0004969 -7.5448833 -6.9289322 -6.5389004 -6.2018833 -5.82586 -5.9066916 -5.9916921 -5.88745 -5.7804623 -5.5214491][-9.246212 -9.1709871 -9.1015682 -8.6873474 -8.2838764 -8.2914314 -8.1069212 -7.9351854 -7.4879594 -6.979362 -6.7371955 -6.777379 -6.593008 -6.2901454 -5.8556118]]...]
INFO - root - 2017-12-15 14:51:51.401993: step 25610, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 53h:57m:39s remains)
INFO - root - 2017-12-15 14:51:57.834429: step 25620, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 54h:07m:34s remains)
INFO - root - 2017-12-15 14:52:04.143335: step 25630, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 52h:37m:29s remains)
INFO - root - 2017-12-15 14:52:10.483718: step 25640, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 53h:54m:44s remains)
INFO - root - 2017-12-15 14:52:16.860968: step 25650, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 54h:51m:05s remains)
INFO - root - 2017-12-15 14:52:23.352820: step 25660, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 55h:35m:14s remains)
INFO - root - 2017-12-15 14:52:29.795685: step 25670, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 55h:19m:56s remains)
INFO - root - 2017-12-15 14:52:36.183766: step 25680, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 53h:15m:23s remains)
INFO - root - 2017-12-15 14:52:42.676149: step 25690, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 55h:19m:24s remains)
INFO - root - 2017-12-15 14:52:49.148095: step 25700, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 56h:02m:48s remains)
2017-12-15 14:52:49.641957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7402344 -4.6068683 -4.1853204 -3.7026196 -2.9628477 -2.2392931 -1.3298583 -0.66851234 0.058751106 -0.74082422 -1.1682529 -2.194242 -2.9628921 -4.7750854 -6.0523224][-3.6392736 -3.8268063 -3.8186707 -3.6843653 -3.2929416 -2.9193254 -2.2722068 -1.2914629 -0.074127674 -0.76390314 -1.4305363 -2.5388093 -3.282527 -4.7504058 -5.682333][-2.9217358 -2.5404806 -2.2752357 -2.2712502 -2.2517695 -1.8120079 -1.5717068 -0.93106747 -0.18396091 -0.73979187 -1.1028438 -2.089458 -2.6403408 -3.9110563 -4.9456811][-3.1287808 -2.4384737 -2.0924578 -2.1446753 -1.5511231 -0.97546959 -0.28588343 0.43130684 0.79174995 -0.097191334 -0.63560343 -1.9108105 -2.9575238 -4.4142056 -5.8220444][-3.1293077 -1.8198686 -1.1765594 -1.1905723 -1.1533542 -0.85743952 -0.14872837 0.46854973 1.0012007 0.012452602 -0.77229881 -1.8677435 -2.3784695 -3.7585933 -4.9364758][-3.0057549 -2.0944366 -1.6577458 -0.53685713 0.24413681 0.5084753 1.0363388 1.4498072 1.8339586 0.61767673 -0.19987011 -1.3908014 -2.172255 -3.7062769 -4.9772291][-3.3047738 -2.2529845 -1.3269482 -0.65212965 -0.2631073 0.19611359 0.76270866 1.1504202 1.5863285 0.43037128 -0.322402 -1.4875183 -2.3215284 -4.0415173 -5.350009][-3.558557 -2.6130357 -2.0693016 -1.3080072 -0.59172726 0.013951302 0.64595985 0.848341 1.1550894 0.19512129 -0.44830322 -1.8030548 -2.5658674 -4.0704269 -5.1604319][-3.0076032 -2.2995949 -1.8762245 -1.0027919 -0.38131714 0.053565979 0.69680023 0.86631393 1.2041864 0.37012577 -0.1125536 -1.6616716 -2.7391915 -4.061718 -4.7587376][-3.3857112 -2.5513391 -2.0273213 -0.99881506 -0.1663208 0.070352554 0.4186039 0.5260582 0.71270466 -0.42533684 -1.0636702 -2.0625253 -2.62003 -3.4362812 -4.0257583][-4.4338517 -3.7582843 -3.2593975 -2.5470734 -2.0667596 -1.5342999 -0.89452219 -0.83648539 -0.868155 -1.7495828 -2.0354071 -2.476963 -2.8594494 -3.1367688 -3.2163353][-6.5743961 -5.7912154 -5.1948748 -4.7052827 -4.3119721 -3.6271482 -2.8907609 -2.8148417 -2.8530507 -3.0969539 -2.9796739 -2.8945823 -3.0025496 -3.1480579 -3.3234086][-6.8304949 -6.4485521 -6.169415 -5.9627142 -5.7546005 -5.2215075 -4.4641662 -4.1423073 -3.9305389 -4.0124154 -3.8842027 -3.3845906 -3.173008 -3.2080069 -3.3442779][-7.1661282 -7.2569842 -6.9132905 -6.9394703 -6.9640112 -6.5790071 -6.0505648 -5.8648233 -5.4483767 -5.1863346 -4.827795 -4.2014647 -3.5308409 -3.2653337 -3.279583][-8.1624985 -8.0882759 -7.8991666 -8.170289 -8.5762959 -8.4697733 -8.22949 -7.8338528 -7.2153463 -6.5771279 -5.9617558 -5.4088163 -4.8976851 -4.2133594 -3.7011216]]...]
INFO - root - 2017-12-15 14:52:56.185206: step 25710, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 57h:22m:43s remains)
INFO - root - 2017-12-15 14:53:02.604275: step 25720, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 55h:20m:15s remains)
INFO - root - 2017-12-15 14:53:09.011164: step 25730, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 54h:34m:15s remains)
INFO - root - 2017-12-15 14:53:15.491746: step 25740, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 53h:09m:46s remains)
INFO - root - 2017-12-15 14:53:21.931377: step 25750, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 54h:38m:56s remains)
INFO - root - 2017-12-15 14:53:28.356838: step 25760, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 54h:46m:19s remains)
INFO - root - 2017-12-15 14:53:34.778232: step 25770, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 54h:19m:37s remains)
INFO - root - 2017-12-15 14:53:41.288485: step 25780, loss = 0.25, batch loss = 0.13 (11.0 examples/sec; 0.729 sec/batch; 62h:06m:56s remains)
INFO - root - 2017-12-15 14:53:47.659095: step 25790, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 53h:15m:53s remains)
INFO - root - 2017-12-15 14:53:54.019747: step 25800, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 55h:16m:56s remains)
2017-12-15 14:53:54.521285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8733673 -5.4523916 -5.7282319 -6.1897454 -6.691855 -6.5348091 -6.4494772 -6.3010488 -6.1361079 -6.3825684 -6.3532705 -6.8939781 -6.8914752 -7.2283597 -7.3092651][-2.6295223 -2.8098202 -3.93679 -5.3557329 -6.463943 -6.8872547 -7.23016 -7.4114571 -7.2836885 -7.8912678 -8.2542543 -9.0401535 -8.8804588 -7.9423957 -6.9334192][-1.3353224 -1.8705406 -2.4629893 -3.0734239 -3.9230375 -4.7561283 -5.6687717 -6.4101462 -6.9863305 -7.7478132 -7.9656 -8.5707407 -8.3925714 -8.1456738 -7.8794932][-0.60322332 -1.1124949 -1.9281874 -2.5394478 -3.1702957 -2.9577675 -2.8373928 -3.5779891 -4.5481443 -5.6348276 -5.9518571 -7.0439863 -7.251153 -7.1291208 -7.2750497][-1.9576964 -1.5132341 -1.2782478 -1.9330144 -2.3071113 -2.2296596 -2.1514473 -1.877028 -1.4036245 -2.166256 -2.8261123 -4.3391619 -5.4329262 -5.6283126 -6.0928135][-3.7337782 -2.9782872 -3.0689764 -2.68601 -1.794075 -0.83519173 -0.047198296 0.53408813 1.1644115 0.31163216 -0.43498087 -1.92906 -3.6868143 -4.6876588 -5.4354477][-5.414155 -4.7965069 -3.8221936 -2.4557872 -1.065906 -0.23048735 0.55457115 1.3767099 1.8332977 1.0235023 0.45266342 -1.2151365 -2.8845582 -3.8345962 -4.5221276][-5.0851192 -4.8320932 -4.464654 -3.1740346 -1.9716759 0.082905769 1.8533707 2.3161478 2.5023251 1.2709055 0.36070919 -1.5447569 -3.3510127 -3.8700106 -4.4571671][-5.7712879 -5.0378456 -4.5526738 -3.5758119 -2.2782931 -0.50225115 0.97122288 2.0824642 2.730279 1.3563862 -0.42239714 -2.6947355 -4.2627573 -5.2366076 -5.4963045][-5.9774141 -5.5268307 -5.0341253 -4.1329918 -3.2116642 -1.8056931 -0.53644896 0.37635422 1.161027 -0.81443071 -2.4276171 -4.3022451 -6.5187368 -6.8800392 -6.8698511][-6.2600846 -5.9725161 -5.6730757 -4.9339757 -4.25099 -3.2785611 -2.3519316 -1.5191736 -1.14995 -2.4617295 -3.3505979 -5.2126684 -6.908287 -7.5700154 -7.7656693][-7.1638703 -7.0665541 -6.7758241 -5.7837338 -5.1389227 -4.5468254 -4.3338761 -4.2428074 -3.5865874 -4.1157231 -4.9924726 -5.8787985 -6.8195724 -6.8218355 -7.0867243][-7.223485 -7.4725208 -7.1922207 -6.4645367 -5.7991123 -5.4758263 -5.3856859 -5.7302895 -6.0638576 -6.7103677 -7.1039424 -7.1869321 -7.4960189 -7.6464262 -7.3519077][-6.734345 -6.736588 -6.8740807 -6.1471205 -5.8271761 -5.36843 -5.3784175 -5.8795519 -5.9633679 -6.7703037 -7.4202871 -8.0162058 -8.465024 -7.994422 -7.2837934][-7.6649256 -7.3674846 -7.0412531 -6.8250165 -6.15253 -5.7478876 -5.5852509 -5.8628321 -6.5397 -7.362905 -7.7569194 -7.850904 -7.9428887 -7.4493413 -7.1555629]]...]
INFO - root - 2017-12-15 14:54:00.927287: step 25810, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 53h:15m:27s remains)
INFO - root - 2017-12-15 14:54:07.380726: step 25820, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 54h:56m:45s remains)
INFO - root - 2017-12-15 14:54:13.733019: step 25830, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 52h:58m:36s remains)
INFO - root - 2017-12-15 14:54:20.151101: step 25840, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 53h:31m:00s remains)
INFO - root - 2017-12-15 14:54:26.643436: step 25850, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 53h:43m:26s remains)
INFO - root - 2017-12-15 14:54:33.139154: step 25860, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 55h:58m:39s remains)
INFO - root - 2017-12-15 14:54:39.593649: step 25870, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 54h:03m:56s remains)
INFO - root - 2017-12-15 14:54:46.086734: step 25880, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 56h:21m:09s remains)
INFO - root - 2017-12-15 14:54:52.457862: step 25890, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 55h:47m:39s remains)
INFO - root - 2017-12-15 14:54:58.844658: step 25900, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 53h:34m:45s remains)
2017-12-15 14:54:59.351665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0669374 -6.108624 -6.0141931 -6.2020979 -6.0967765 -5.5113516 -4.8712168 -4.2502851 -4.1147065 -3.9765577 -4.0465603 -5.7212 -6.5800037 -6.3653922 -7.0288343][-5.8592815 -6.222024 -7.0814795 -7.007525 -7.0157318 -6.8409691 -6.5216446 -6.1251154 -6.1024566 -5.9093008 -5.8938608 -7.0355086 -8.1531763 -7.2957392 -7.5726519][-5.503653 -5.9352741 -6.7899642 -6.8438525 -6.7089276 -6.6842036 -6.6853333 -6.591588 -6.9324489 -7.03313 -6.5016823 -7.4354968 -8.205925 -7.722209 -7.8995204][-4.7842445 -5.2487011 -6.02995 -5.7165771 -5.2910838 -5.2114539 -4.9758115 -4.8611655 -5.1377535 -5.51038 -5.3463531 -6.6304388 -7.6575665 -7.2069244 -7.6229811][-5.5105147 -4.8638258 -4.4750996 -4.055078 -3.5032883 -3.1246605 -3.0732403 -3.0077147 -2.7378702 -2.9079609 -2.7962508 -4.3631697 -5.7682605 -5.6024728 -6.5224218][-4.4923744 -3.6582751 -2.9209695 -2.0139332 -1.5226688 -0.818274 -0.59480858 -0.8353405 -0.57670164 -1.0384188 -0.87984657 -2.7046165 -4.2265739 -4.3283172 -5.101438][-3.8873119 -3.0126414 -2.2511954 -1.1664438 -0.16473913 0.4240942 0.89408875 0.86204052 1.3586636 0.48490238 0.1530323 -2.3376279 -3.9528449 -4.3484545 -5.0801029][-2.5887184 -2.8370061 -2.1458368 -1.3429012 -0.36516571 0.18211651 0.76361561 1.2916832 2.1998434 1.5582724 0.75466537 -1.6789522 -3.4812794 -3.9315863 -4.8600359][-3.8811724 -3.9708726 -3.410368 -2.092854 -1.3416729 -0.9000411 -0.018275738 0.84863758 1.4211426 0.85031986 0.47244549 -2.0793982 -3.7879629 -3.9600685 -4.6614962][-5.2467737 -4.6770377 -3.9496431 -3.0204411 -2.3261728 -1.7413836 -1.2857385 -0.4739871 0.57184315 -0.5694685 -1.393363 -2.7507486 -4.1200576 -4.4841537 -5.2303238][-6.9021654 -6.1604095 -5.1578817 -4.5500693 -3.9724593 -3.5272384 -2.8476443 -2.4544916 -2.2865977 -2.9608498 -3.4692297 -4.154686 -5.3333292 -5.5084791 -6.0107193][-7.6547265 -6.58309 -6.098629 -5.5244451 -4.9062834 -4.8551121 -4.5692635 -4.0473733 -3.5395031 -4.3104568 -5.0875063 -5.500855 -6.1607451 -5.9096003 -5.5894017][-8.8859043 -8.0440865 -7.75452 -6.9966221 -6.1334963 -5.4949517 -5.107933 -5.1187096 -4.7272043 -5.0012226 -5.7860303 -5.5592709 -5.5404787 -5.406425 -5.2747583][-7.8424854 -7.4267063 -7.1168356 -6.7073789 -6.0817828 -5.4871225 -4.9321413 -4.7806482 -4.7771511 -4.7369781 -5.2488613 -4.984921 -4.8913 -4.3647413 -4.2158308][-6.6361132 -6.2076979 -6.0993347 -6.0842047 -5.9504986 -5.4525709 -4.9495449 -4.7840548 -5.0409184 -5.078392 -5.4244366 -5.031477 -5.1240649 -5.1486416 -4.8867826]]...]
INFO - root - 2017-12-15 14:55:05.682590: step 25910, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 53h:40m:39s remains)
INFO - root - 2017-12-15 14:55:12.110676: step 25920, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 54h:14m:07s remains)
INFO - root - 2017-12-15 14:55:18.474710: step 25930, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 56h:47m:52s remains)
INFO - root - 2017-12-15 14:55:24.988822: step 25940, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 53h:09m:10s remains)
INFO - root - 2017-12-15 14:55:31.375251: step 25950, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 55h:47m:44s remains)
INFO - root - 2017-12-15 14:55:37.719738: step 25960, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 53h:46m:39s remains)
INFO - root - 2017-12-15 14:55:44.078642: step 25970, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 54h:59m:41s remains)
INFO - root - 2017-12-15 14:55:50.630408: step 25980, loss = 0.33, batch loss = 0.21 (11.9 examples/sec; 0.672 sec/batch; 57h:10m:58s remains)
INFO - root - 2017-12-15 14:55:57.093053: step 25990, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 55h:49m:25s remains)
INFO - root - 2017-12-15 14:56:03.521824: step 26000, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 55h:13m:31s remains)
2017-12-15 14:56:04.093160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6004863 -5.165658 -4.9275417 -4.290803 -4.096827 -4.0285835 -3.5970225 -3.7242606 -3.493494 -4.0212417 -3.7360885 -4.5642138 -5.0604649 -6.348762 -7.4530277][-4.6656971 -4.2829585 -4.6779242 -4.8023214 -4.6917372 -4.7118769 -4.3525367 -4.0947862 -3.2208719 -3.8603754 -4.2739415 -5.3933506 -6.037179 -7.1977234 -7.8159127][-3.1091943 -2.30903 -2.3669033 -2.9673285 -3.6394186 -3.9239171 -3.6636324 -3.467289 -2.6849341 -3.2697 -3.309732 -4.5109348 -5.541255 -7.2500682 -8.5801182][-1.3581052 -0.89422655 -0.93129349 -1.1860709 -1.8907475 -2.1487451 -1.8548875 -2.0111623 -1.4393544 -2.1627059 -2.7802062 -4.3202934 -5.3096075 -6.5851307 -7.8352242][-0.26330662 0.59849453 0.22655296 0.26595306 -0.16578293 -0.39350891 -0.34790564 -0.41426754 -0.085342407 -1.0853281 -1.7617764 -3.6551247 -5.0450811 -6.9535794 -8.1024151][0.71209335 1.2442465 1.6733732 1.6529713 0.99251652 1.3352251 2.0361366 1.9775209 1.9044323 0.17803335 -1.2054725 -3.3077745 -4.3339748 -6.221612 -7.669024][-0.44123983 -0.24294615 0.056098938 1.2892275 1.9327946 2.3864212 3.1538782 3.5377436 3.8033409 1.5732508 -0.47294331 -3.4333644 -5.1695147 -6.9205041 -7.9684639][-0.69952631 -0.192595 -0.18969059 0.74240589 1.7766571 2.7059097 3.1516771 3.4700985 3.8767385 1.9285288 0.365201 -2.7750287 -4.6077862 -6.5926089 -7.7399473][-3.0659995 -2.0431805 -1.3986511 -0.48529434 0.31155205 1.6027327 2.4019117 2.5170622 2.4642868 0.38770771 -1.0248985 -3.2812667 -4.5801029 -6.4902024 -7.6900587][-4.1671391 -4.018609 -3.569541 -2.9531212 -2.1906281 -1.0983238 -0.30978203 0.2489028 0.70755005 -1.0178161 -2.420032 -4.8849354 -6.1247988 -6.9823618 -7.7187567][-7.7060647 -7.4266796 -6.6609607 -5.4631605 -4.5384617 -3.43613 -2.8244591 -2.7567582 -2.4347177 -3.528089 -4.243073 -5.6053495 -6.199132 -7.348968 -8.0415831][-8.0033817 -7.58769 -7.0483871 -6.6304173 -6.0632935 -5.1238852 -4.6621475 -4.3119812 -3.8400323 -4.956727 -5.455976 -6.4197059 -6.5228934 -6.6445312 -7.2321754][-8.7220516 -8.6825657 -8.60493 -7.9739761 -7.5442286 -7.0171833 -6.5395989 -6.6165667 -6.430203 -6.5922189 -6.67054 -7.6342616 -8.3007574 -7.8881087 -7.3901248][-8.1858492 -7.5993552 -7.2819495 -6.8415546 -6.6102362 -6.2605896 -5.9211297 -5.8759561 -5.6651306 -6.2620282 -6.4775925 -6.389523 -6.3520613 -6.6289768 -7.3392262][-9.7935286 -9.386591 -8.95154 -8.0695782 -7.4187455 -7.2476416 -6.8748531 -7.2861876 -7.4806957 -7.429266 -7.27942 -7.156363 -6.9837923 -6.4316359 -6.30252]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 14:56:10.637249: step 26010, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 54h:54m:25s remains)
INFO - root - 2017-12-15 14:56:17.002692: step 26020, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 52h:39m:19s remains)
INFO - root - 2017-12-15 14:56:23.350782: step 26030, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 54h:29m:23s remains)
INFO - root - 2017-12-15 14:56:29.738252: step 26040, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 55h:00m:57s remains)
INFO - root - 2017-12-15 14:56:36.194053: step 26050, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 54h:58m:50s remains)
INFO - root - 2017-12-15 14:56:42.617178: step 26060, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 54h:01m:15s remains)
INFO - root - 2017-12-15 14:56:49.064986: step 26070, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 54h:45m:19s remains)
INFO - root - 2017-12-15 14:56:55.605522: step 26080, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 56h:07m:13s remains)
INFO - root - 2017-12-15 14:57:02.053743: step 26090, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 55h:39m:50s remains)
INFO - root - 2017-12-15 14:57:08.537578: step 26100, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 55h:14m:03s remains)
2017-12-15 14:57:09.106214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8850489 -8.7648878 -10.93548 -12.666771 -13.578382 -12.783287 -12.229937 -9.281271 -7.5731387 -7.5443058 -9.08745 -10.15545 -8.451004 -9.5761509 -8.0433321][-6.4726372 -8.4471979 -11.128609 -13.210808 -14.695179 -14.642349 -13.044132 -9.945302 -8.4244528 -8.8204031 -9.4573469 -10.562428 -11.037573 -12.474216 -9.8304424][-6.3707175 -7.8284383 -10.911045 -12.610599 -14.232284 -12.565458 -11.609627 -9.9866219 -7.7747917 -7.7181025 -8.594717 -10.283353 -10.244442 -12.48016 -11.3652][-6.9972177 -8.22872 -9.7864647 -10.793455 -12.038342 -11.276217 -9.8294353 -8.04872 -7.0783358 -8.747118 -8.0445566 -9.8829079 -9.7854013 -12.299683 -11.215673][-6.6301365 -7.3986406 -8.31001 -8.6682854 -8.23583 -5.8789563 -5.060967 -3.9958525 -3.2135763 -6.9371777 -9.4439869 -11.525873 -10.49921 -12.988097 -11.520322][-6.5575919 -7.3519983 -7.7433925 -7.0577211 -5.5532818 -2.4885964 0.81296825 2.4511166 2.7700672 -2.0784593 -5.7778015 -10.149345 -10.972764 -13.405277 -11.666423][-6.2935748 -6.3019004 -6.3888183 -4.24039 -1.8526921 1.3500071 3.8298512 5.6378374 7.2680578 2.2875776 -0.93732595 -7.2605791 -9.4554319 -13.121048 -11.670053][-6.0719995 -5.2516718 -5.2007856 -2.47045 -0.213871 3.7040443 5.7725353 6.119195 5.532939 1.3950968 -0.89428806 -5.7767396 -7.6925068 -12.666903 -11.87236][-8.2973013 -7.266037 -5.8385472 -3.748817 -1.3645749 1.4284363 3.3442373 3.9437494 3.2850485 -1.6747079 -4.4041529 -7.7245479 -7.9487863 -11.580919 -10.876361][-9.662179 -9.348382 -8.416996 -6.3015289 -4.5696058 -2.0717058 -0.22658777 0.65860653 0.39913654 -4.2393074 -6.7362218 -10.892562 -11.686526 -13.021486 -10.286762][-9.4877539 -9.72872 -9.5305567 -8.9027042 -7.4430518 -4.6681128 -3.4622049 -3.575129 -3.6528726 -6.4110084 -7.7524171 -11.267263 -12.546054 -13.757716 -10.969727][-9.33924 -9.4102659 -9.53397 -9.3170843 -8.2889662 -6.5368204 -5.267849 -5.2768922 -5.8576488 -8.4063377 -8.6091471 -9.324832 -10.077358 -11.799597 -10.478559][-10.443704 -9.8957672 -8.9596386 -8.1633005 -7.4551663 -6.5274444 -5.7329412 -5.9224582 -6.6891251 -8.4199724 -8.0953064 -8.4753246 -7.9195 -8.4069262 -7.899087][-10.530955 -10.343177 -8.783267 -8.200511 -6.9384174 -6.4011774 -5.63933 -6.0313978 -6.6093707 -7.6338997 -7.2969027 -7.6104555 -6.8952684 -6.0901723 -4.74471][-7.7745752 -8.8879519 -8.9392872 -8.9520454 -7.8429713 -6.5212135 -5.403481 -5.5161624 -6.1812983 -6.7509122 -7.0060911 -6.8736658 -6.4509273 -5.9042578 -5.0601687]]...]
INFO - root - 2017-12-15 14:57:15.464627: step 26110, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 54h:56m:52s remains)
INFO - root - 2017-12-15 14:57:21.825966: step 26120, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 54h:56m:11s remains)
INFO - root - 2017-12-15 14:57:28.185592: step 26130, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 53h:53m:48s remains)
INFO - root - 2017-12-15 14:57:34.541919: step 26140, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 53h:15m:01s remains)
INFO - root - 2017-12-15 14:57:40.952342: step 26150, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 53h:39m:47s remains)
INFO - root - 2017-12-15 14:57:47.424058: step 26160, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:24m:35s remains)
INFO - root - 2017-12-15 14:57:53.892876: step 26170, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 56h:26m:52s remains)
INFO - root - 2017-12-15 14:58:00.340787: step 26180, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 57h:17m:12s remains)
INFO - root - 2017-12-15 14:58:06.755302: step 26190, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 54h:36m:00s remains)
INFO - root - 2017-12-15 14:58:13.183316: step 26200, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 53h:42m:33s remains)
2017-12-15 14:58:13.675919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.3903313 -9.7452288 -9.8019924 -8.8099327 -6.9206133 -4.9374337 -2.8770957 -1.3782482 -0.439178 -0.85948038 -1.6098318 -3.0767431 -4.0774961 -5.7835913 -7.1622138][-8.7531967 -9.1853285 -9.8294888 -10.914966 -10.727739 -8.3521109 -6.2217789 -3.8549902 -1.516489 -1.5047851 -1.9753661 -3.1415009 -4.2402124 -5.4291363 -6.3821921][-6.0590668 -7.1681881 -8.3178453 -9.5319633 -10.263035 -10.069332 -8.5220528 -6.0355091 -4.0479097 -3.6388879 -2.9663405 -3.3564062 -3.8091416 -4.3701315 -5.6540747][-4.7139111 -5.5824385 -6.0055828 -6.9112806 -7.7292643 -7.9036121 -7.3813763 -6.1688147 -4.6843929 -4.0003052 -3.6499996 -3.9639263 -4.1415529 -4.8853245 -5.1605535][-3.6390738 -3.6851454 -4.1790113 -4.2042389 -3.986166 -3.5706658 -3.2069716 -3.6790657 -3.8621454 -4.5336862 -4.9632254 -5.1361179 -5.0903177 -5.7340655 -6.2971325][-4.1586809 -3.6193409 -3.4560428 -2.7421598 -1.6552086 -0.26994133 1.2221594 1.2349777 0.35458469 -2.5947804 -4.7956219 -6.0995111 -6.86038 -7.5615125 -7.8674335][-5.0125303 -4.2179136 -2.7697597 -1.082077 0.56304264 2.0598545 3.2886839 3.935029 3.8356361 0.78275681 -2.7469406 -5.519515 -7.1893721 -8.2515936 -8.6295233][-4.4909954 -4.4049816 -3.7512336 -1.7219796 0.6472559 2.89563 4.5784836 4.7518349 4.399085 1.8542395 -1.6097651 -4.8010139 -7.2289562 -8.5821705 -8.9682865][-3.7341373 -3.9130349 -3.6792998 -2.3023829 -0.68483782 0.98596859 2.3438835 2.7510185 3.0288906 0.96741676 -1.2144713 -4.0593214 -6.5719662 -8.20171 -9.0103474][-4.1641579 -3.1625276 -2.1089492 -1.8074846 -1.3698311 -0.27411509 0.52713966 0.64550304 0.24191141 -1.9183884 -3.3800607 -5.4070406 -6.7429705 -8.1760473 -9.220438][-4.0831976 -3.6253853 -2.6652451 -1.8458829 -1.3976264 -1.3195424 -1.1995759 -1.3243575 -1.6097302 -3.8515322 -6.3206158 -7.7298274 -8.3408031 -8.985054 -8.9556923][-4.7794609 -4.921875 -4.2940679 -3.0697312 -2.1456895 -1.8711648 -1.9483485 -2.6075692 -3.3359909 -4.426394 -5.6310339 -7.2866955 -8.28564 -8.88538 -9.1020508][-5.7753153 -5.9254408 -5.4852467 -4.7529383 -3.620378 -2.7983847 -2.2579851 -2.7979374 -3.5788136 -4.925992 -5.6323848 -6.078042 -7.2798328 -9.0156727 -9.8016224][-6.5834508 -7.2534041 -7.5263739 -7.8767796 -7.3884983 -5.9166479 -4.0051684 -3.619545 -4.2791266 -5.6424589 -6.9943304 -7.1368971 -7.5021152 -7.6893649 -7.7982564][-6.9161806 -8.3885889 -8.7358 -9.2051878 -9.2922583 -8.80584 -7.8774281 -7.1934009 -6.4931436 -6.6776805 -7.4624352 -7.9480739 -8.1635275 -7.9057693 -7.5161409]]...]
INFO - root - 2017-12-15 14:58:20.095208: step 26210, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.632 sec/batch; 53h:48m:38s remains)
INFO - root - 2017-12-15 14:58:26.447016: step 26220, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 53h:35m:42s remains)
INFO - root - 2017-12-15 14:58:32.844480: step 26230, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 56h:34m:47s remains)
INFO - root - 2017-12-15 14:58:39.234267: step 26240, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 54h:43m:33s remains)
INFO - root - 2017-12-15 14:58:45.625078: step 26250, loss = 0.24, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 56h:16m:31s remains)
INFO - root - 2017-12-15 14:58:52.063921: step 26260, loss = 0.34, batch loss = 0.23 (12.6 examples/sec; 0.634 sec/batch; 53h:57m:46s remains)
INFO - root - 2017-12-15 14:58:58.524165: step 26270, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 56h:53m:27s remains)
INFO - root - 2017-12-15 14:59:04.887318: step 26280, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 52h:28m:41s remains)
INFO - root - 2017-12-15 14:59:11.233003: step 26290, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 54h:04m:29s remains)
INFO - root - 2017-12-15 14:59:17.833518: step 26300, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 54h:13m:57s remains)
2017-12-15 14:59:18.325786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8434248 -7.2091222 -7.1055007 -6.8214331 -6.107594 -4.8167677 -3.2797236 -1.9352117 -0.34459925 -0.83538151 -1.6849403 -3.2476187 -4.7405586 -5.7218633 -6.8613443][-5.4356956 -5.9174194 -6.5060968 -6.6309195 -6.4052854 -5.71019 -4.834836 -3.4654493 -1.3734431 -1.4564514 -1.5273123 -2.8501954 -4.0023351 -4.7632256 -6.4184208][-5.3509583 -5.8918839 -6.1540427 -6.2889295 -6.590683 -6.4925709 -5.7381897 -4.720027 -3.4595003 -3.7057285 -3.3498268 -4.0246763 -4.7108293 -5.0653534 -6.1508551][-5.1576614 -5.2265162 -4.9651194 -4.4092197 -4.5020523 -4.1588173 -3.9506092 -3.6492758 -3.0861726 -3.7267849 -4.6118879 -5.5508614 -5.59927 -5.4759278 -6.2942238][-5.03662 -4.6301622 -3.9589472 -2.7783136 -2.3227949 -1.993053 -0.90967941 -0.50095987 -0.65173483 -2.1002674 -3.2708826 -4.9812546 -6.3775544 -6.8326969 -7.2942119][-4.0565815 -2.7234287 -1.9639277 -0.60820484 0.34912872 1.0956678 1.9037895 1.7362194 1.7575388 0.081267834 -1.6090245 -3.6027741 -5.1357975 -6.1110168 -7.5865064][-3.4511571 -2.1047096 -0.72207737 0.65984535 1.8733606 3.4166126 4.5195007 4.1370964 3.5223198 1.1643839 -0.84409142 -3.2060008 -4.8837638 -5.8589482 -6.9386806][-3.0533924 -2.5345411 -1.7734337 -0.2639637 1.0405979 2.61244 3.5919485 3.6323023 3.2572222 0.6656847 -1.7163196 -4.2789531 -5.7011609 -6.273469 -7.079659][-3.6604476 -3.0854883 -2.5844793 -1.1103716 -0.27669811 1.3172293 2.0646906 2.0116596 2.042882 -0.33121252 -2.9306369 -5.1573772 -6.1622591 -6.4476738 -7.0384483][-4.9109011 -4.4357452 -3.929498 -2.6927462 -1.8078966 -0.40175915 0.56850338 0.47293186 0.50604343 -1.3991494 -2.9304705 -4.5950856 -5.9091358 -6.2580976 -7.1353188][-7.7486286 -6.974081 -6.396512 -5.3428955 -4.8079462 -3.5784101 -2.3815317 -2.4964113 -2.3598914 -3.6796393 -4.1499166 -5.2324266 -6.1787643 -6.5537338 -7.8965974][-9.1109209 -8.4675674 -7.8865242 -7.0776248 -6.2597222 -5.5587473 -4.7153635 -4.2580795 -4.0238485 -4.8890018 -4.8236217 -5.2794948 -5.8709126 -6.2511978 -7.2716851][-9.2309027 -9.2807751 -8.9892368 -8.0085344 -7.0919566 -6.3074303 -5.3674564 -5.1197634 -5.278616 -5.5871811 -5.2879343 -5.284831 -5.4986482 -5.5611978 -5.9780741][-8.438488 -8.631216 -8.804574 -8.154973 -7.5745792 -7.2559633 -6.5269375 -6.2648888 -6.045414 -6.1340942 -6.2726512 -6.2792211 -6.1368837 -5.8143234 -5.3065872][-6.2305079 -6.282155 -6.7996116 -6.7770886 -7.0090418 -6.8712697 -6.8342385 -7.0394807 -6.6757574 -6.4436641 -6.2492256 -6.1354375 -6.4740896 -6.3258629 -5.8747368]]...]
INFO - root - 2017-12-15 14:59:24.759236: step 26310, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 54h:05m:52s remains)
INFO - root - 2017-12-15 14:59:31.132908: step 26320, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 54h:31m:19s remains)
INFO - root - 2017-12-15 14:59:37.457074: step 26330, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 54h:09m:38s remains)
INFO - root - 2017-12-15 14:59:43.873355: step 26340, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 53h:55m:43s remains)
INFO - root - 2017-12-15 14:59:50.260717: step 26350, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 54h:17m:28s remains)
INFO - root - 2017-12-15 14:59:56.665684: step 26360, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 53h:55m:18s remains)
INFO - root - 2017-12-15 15:00:03.179443: step 26370, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 56h:39m:03s remains)
INFO - root - 2017-12-15 15:00:09.596982: step 26380, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 54h:01m:38s remains)
INFO - root - 2017-12-15 15:00:15.994677: step 26390, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 53h:24m:22s remains)
INFO - root - 2017-12-15 15:00:22.346288: step 26400, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 53h:48m:57s remains)
2017-12-15 15:00:22.890116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2816405 -3.1630402 -3.1474357 -3.1789813 -3.0868282 -2.9376764 -2.6727862 -2.3345342 -2.0819893 -3.2270193 -4.2439327 -5.2002683 -6.332634 -6.9142227 -7.3998451][-2.802218 -2.90444 -3.1775875 -3.2523994 -3.0715928 -2.7967978 -2.3893242 -2.3573904 -2.2400064 -3.433548 -4.5758715 -5.8162994 -7.2049484 -7.8369937 -8.2735825][-2.292572 -2.4842482 -2.8632889 -2.9547935 -2.6028214 -2.1617413 -1.7540607 -1.507586 -1.2273369 -2.2984757 -3.1090574 -4.14684 -5.6597371 -6.3407207 -7.02075][-2.543385 -2.5306163 -2.58571 -2.3154378 -1.8114238 -1.2011199 -0.54432154 -0.46137381 -0.40881348 -1.6161628 -2.5679903 -3.6360836 -5.2768183 -6.1653075 -6.750494][-2.818964 -2.3557177 -1.8439932 -1.3704276 -0.59850359 0.19401026 0.7320013 0.714324 0.69462776 -0.8768115 -2.0022721 -3.0697651 -4.7414751 -5.757956 -6.5382919][-3.6810775 -2.9268827 -2.0453234 -1.1878505 -0.30341816 0.61959171 1.3182802 1.2694178 1.2273016 -0.53267574 -1.9156523 -3.0583625 -4.7923589 -5.7757254 -6.4197297][-4.55616 -3.8111126 -2.6835794 -1.3170056 -0.0036740303 1.0884695 1.6977787 1.6406193 1.4690752 -0.57594919 -2.1353703 -3.3714194 -5.1050353 -6.0768342 -6.7476621][-4.2693033 -3.437355 -2.223609 -0.78123713 0.448761 1.3251028 1.7968292 1.6407862 1.3397617 -0.77036619 -2.5347004 -3.9251728 -5.5482235 -6.5411968 -7.1579566][-4.7157249 -3.636281 -2.6245708 -1.3523064 -0.12063265 0.53699207 0.91577244 1.1064234 1.1030712 -1.0544591 -2.8206682 -4.24084 -6.0605741 -7.0654411 -7.5948963][-4.4146643 -3.7181647 -2.8398457 -1.5892792 -0.658988 -0.32751417 0.024832249 0.30646706 0.32016754 -2.0073409 -3.6025152 -5.09518 -6.6587782 -7.7125206 -8.4217615][-5.7036657 -4.8568678 -4.1433535 -3.187336 -2.4842024 -2.214828 -1.9957781 -1.7877235 -1.655376 -3.673274 -5.2388182 -6.3266835 -7.551723 -8.3034067 -8.5424337][-5.5442915 -5.0569525 -4.3945789 -3.7294717 -3.6377854 -3.6515303 -3.5709639 -3.4195781 -3.2696171 -4.6177363 -5.7519431 -6.5165009 -7.2767639 -8.0476551 -8.2536392][-6.2337809 -5.7235436 -5.2539158 -4.8013773 -4.6551456 -4.7899456 -4.8536863 -4.9138708 -4.823864 -5.6478157 -6.4298182 -6.720284 -7.1898093 -7.5220618 -7.3705354][-5.7763786 -5.5245728 -5.3124828 -4.8114018 -4.7638626 -4.8852129 -4.9871187 -5.0570011 -5.0320539 -5.746151 -5.979126 -6.0920553 -6.1883039 -6.6266179 -6.538619][-7.711237 -7.5217171 -7.0199757 -6.5918741 -6.3851047 -6.4553719 -6.6439929 -6.8230371 -6.8949804 -6.7616396 -6.7123404 -6.6383719 -6.3587627 -6.2424154 -6.1727867]]...]
INFO - root - 2017-12-15 15:00:29.176865: step 26410, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 54h:07m:19s remains)
INFO - root - 2017-12-15 15:00:35.550169: step 26420, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 55h:47m:41s remains)
INFO - root - 2017-12-15 15:00:41.891277: step 26430, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 53h:55m:34s remains)
INFO - root - 2017-12-15 15:00:48.278020: step 26440, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 54h:01m:58s remains)
INFO - root - 2017-12-15 15:00:54.684456: step 26450, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 53h:39m:52s remains)
INFO - root - 2017-12-15 15:01:01.130119: step 26460, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 53h:22m:24s remains)
INFO - root - 2017-12-15 15:01:07.519804: step 26470, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 55h:26m:07s remains)
INFO - root - 2017-12-15 15:01:13.836195: step 26480, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 53h:34m:03s remains)
INFO - root - 2017-12-15 15:01:20.201627: step 26490, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:06m:33s remains)
INFO - root - 2017-12-15 15:01:26.801003: step 26500, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 55h:00m:40s remains)
2017-12-15 15:01:27.340391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6691742 -5.42953 -4.49712 -3.782788 -3.1241279 -2.9055252 -2.6412811 -2.8106942 -3.0453205 -3.5122967 -4.5597119 -5.6689644 -6.0232887 -5.7822084 -6.8029609][-4.9617114 -4.5950975 -4.0263181 -3.398952 -3.0176668 -3.1970468 -3.239253 -3.3066812 -3.1159892 -3.2935352 -4.3426771 -4.799942 -5.2650013 -5.3868389 -6.96674][-5.8892031 -5.4510818 -4.5066257 -3.3047285 -2.7581115 -2.6249256 -2.6187677 -2.8299332 -3.28123 -3.9283795 -5.179153 -5.4176254 -5.9518251 -5.6060486 -6.9406409][-4.4476967 -3.8042076 -3.3157129 -2.8132172 -2.6029043 -2.0040917 -1.46281 -1.3185887 -1.6268797 -2.4167452 -4.0680285 -4.7833853 -5.3532753 -4.7268562 -6.3009081][-4.0281572 -3.4691815 -2.731668 -1.9901967 -1.206439 -1.2960114 -0.86890411 -0.64525509 -0.52276993 -1.1174331 -2.330296 -3.1354308 -4.3858724 -4.9601822 -7.0588522][-2.1508083 -1.5899177 -1.5146179 -1.0625801 -0.91092968 -0.31656122 -0.1144805 -0.23222923 -0.061497688 -0.66026545 -2.0851278 -2.9661765 -3.7892487 -4.318965 -6.3209147][-0.72700214 -0.77759838 -0.18759441 0.80964184 1.3864202 1.143609 0.70095253 0.3353653 -0.25143147 -1.1472011 -2.767168 -3.6471715 -4.0653172 -4.2192597 -5.6026049][0.69058514 1.2661047 1.5045929 1.5740604 2.0175638 2.3176851 2.5055752 1.9732981 1.3427601 0.2820015 -1.9754033 -3.1608934 -4.0866446 -4.093926 -5.6509719][-0.69905472 0.27699804 1.1490126 2.0749407 2.3857937 2.1648111 2.2140179 1.906558 1.7208223 0.51572132 -1.2918806 -2.3821068 -3.3511157 -4.0016952 -6.2014847][-1.5072384 -0.958189 -0.2839303 0.48844624 0.80298328 1.5792475 1.7969093 1.3260851 0.87702847 -0.25975275 -2.1729393 -3.1916132 -4.0670853 -3.8470607 -5.5767384][-2.7276268 -1.9704919 -1.4110956 -0.51207685 -0.35985947 -0.27588749 -0.28925419 -0.25015402 -0.56850195 -1.6910167 -2.9455595 -3.7215478 -4.3613877 -4.6044197 -5.8134737][-3.5908256 -2.7238622 -2.3602471 -2.2159457 -1.5671206 -0.91597462 -0.85042286 -1.0742764 -1.3449197 -1.81253 -3.6643529 -4.8010578 -5.7897863 -6.4314618 -7.2786112][-4.4918833 -4.4864755 -4.3412304 -3.8863287 -3.140923 -2.8290043 -2.1937618 -1.991941 -2.7466826 -3.3773193 -5.0873213 -5.7258182 -6.3514576 -6.7015667 -7.5867276][-5.3605509 -4.884481 -4.7648373 -4.5730419 -4.03677 -3.7249093 -3.2603369 -3.856359 -4.4149694 -4.5793648 -5.8370123 -6.4064975 -6.8023677 -6.7406669 -7.0207896][-5.6269269 -5.1244845 -5.03183 -5.2134295 -5.2752252 -5.4427018 -5.2202272 -5.0294437 -5.0497341 -5.5039043 -5.9839811 -6.3250766 -6.7136474 -7.2488971 -7.563344]]...]
INFO - root - 2017-12-15 15:01:33.683214: step 26510, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 53h:53m:20s remains)
INFO - root - 2017-12-15 15:01:40.080601: step 26520, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 53h:13m:36s remains)
INFO - root - 2017-12-15 15:01:46.438182: step 26530, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 54h:46m:31s remains)
INFO - root - 2017-12-15 15:01:52.962953: step 26540, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 56h:48m:03s remains)
INFO - root - 2017-12-15 15:01:59.378236: step 26550, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:45m:16s remains)
INFO - root - 2017-12-15 15:02:05.824114: step 26560, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 52h:57m:37s remains)
INFO - root - 2017-12-15 15:02:12.210846: step 26570, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 53h:49m:09s remains)
INFO - root - 2017-12-15 15:02:18.606232: step 26580, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 55h:34m:33s remains)
INFO - root - 2017-12-15 15:02:24.998607: step 26590, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 53h:00m:11s remains)
INFO - root - 2017-12-15 15:02:31.481054: step 26600, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 54h:06m:19s remains)
2017-12-15 15:02:31.998196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0184388 -3.9648438 -4.5596104 -5.4468484 -6.560195 -6.137094 -5.8162279 -4.9002824 -4.1907139 -4.7871013 -6.01896 -8.8257256 -10.85791 -12.27389 -12.550989][-3.9330778 -6.5080271 -7.7119951 -8.5407619 -9.169857 -8.8270864 -7.4355783 -5.4644289 -3.7061799 -4.3686647 -6.4962516 -9.1935091 -10.774996 -12.282407 -12.483885][-3.5082803 -5.1379342 -7.0091548 -8.2780418 -8.73911 -8.0022631 -6.4235554 -4.5653639 -2.9386368 -2.9188013 -4.7676859 -7.8671184 -9.9486313 -11.418411 -11.497774][-4.0000372 -5.3274565 -6.1752858 -6.8990488 -7.3139043 -6.6319966 -4.6223469 -2.4721122 -0.97758436 -1.3850222 -3.0898528 -6.1664591 -8.544014 -10.13519 -10.608889][-4.199996 -4.305625 -4.4497657 -5.1290865 -4.9754438 -4.1313524 -3.0031424 -0.97255659 0.921772 0.73017693 -1.595253 -4.8761778 -7.37169 -9.5143318 -9.7575731][-3.0150042 -3.6977592 -4.0826063 -3.7322836 -2.813889 -1.1305189 0.45842838 1.8044548 2.3117218 1.3686104 -1.2656984 -4.7856693 -7.3661122 -8.7821064 -9.3187466][-3.0293908 -2.905942 -3.0475535 -2.3095155 -1.4618926 0.084386349 1.7456169 3.1090717 4.0334988 2.6358252 -0.88466787 -5.1256752 -7.9045639 -9.8349943 -10.287002][-3.2857985 -3.36237 -3.3086953 -2.5145426 -1.4060144 0.32562065 2.1508455 2.9001083 3.3482885 2.2738409 -0.68523216 -4.781764 -7.4934673 -9.43805 -10.194321][-2.6653342 -2.5189834 -2.1578798 -1.6223059 -0.8180356 0.31328392 1.7046986 2.3261108 2.1787643 0.75144291 -2.0364285 -5.2487788 -7.2493916 -8.9846487 -9.5636959][-2.6205344 -3.0056424 -2.8711257 -2.306879 -1.758069 -0.48204947 0.6361227 0.78477383 0.6490078 -1.1461053 -3.9392378 -7.3344231 -9.03591 -9.4981995 -9.2457294][-4.27234 -4.6646118 -4.6018391 -3.8354847 -3.5224066 -3.0107617 -1.8290811 -1.3056426 -1.4290562 -3.2216339 -5.5037031 -7.6560545 -8.55378 -8.9645872 -9.1441126][-5.6394315 -5.9071321 -6.2759 -5.9056244 -5.4943466 -4.7218266 -4.1726446 -3.8648877 -3.6889386 -4.7957773 -6.3505697 -7.8195691 -8.0412235 -7.8790669 -7.8535728][-5.7508593 -6.3755522 -6.8919334 -6.7502789 -6.3325348 -5.9051213 -5.1811647 -5.4955268 -5.7527261 -6.0693107 -6.606678 -7.2104115 -7.6316123 -7.4141192 -6.96774][-5.9807186 -6.2410011 -6.338623 -6.181982 -6.0967908 -6.0021019 -5.8302712 -5.8200145 -5.767365 -5.887311 -6.241869 -6.4540691 -6.2144384 -6.1395087 -6.3558712][-6.7624087 -6.9243817 -7.2142029 -7.6608491 -8.0462465 -7.7755361 -7.4599543 -7.562613 -7.7936192 -7.2303071 -6.7235332 -6.5836859 -6.2702589 -6.3635859 -6.6411614]]...]
INFO - root - 2017-12-15 15:02:38.464971: step 26610, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 53h:56m:40s remains)
INFO - root - 2017-12-15 15:02:44.901758: step 26620, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 53h:54m:20s remains)
INFO - root - 2017-12-15 15:02:51.312994: step 26630, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 54h:55m:46s remains)
INFO - root - 2017-12-15 15:02:57.731562: step 26640, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 56h:34m:20s remains)
INFO - root - 2017-12-15 15:03:04.107461: step 26650, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 54h:30m:52s remains)
INFO - root - 2017-12-15 15:03:10.515366: step 26660, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 53h:37m:01s remains)
INFO - root - 2017-12-15 15:03:16.891363: step 26670, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 53h:39m:32s remains)
INFO - root - 2017-12-15 15:03:23.319663: step 26680, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 55h:36m:25s remains)
INFO - root - 2017-12-15 15:03:29.685603: step 26690, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 54h:11m:40s remains)
INFO - root - 2017-12-15 15:03:36.024890: step 26700, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 53h:57m:00s remains)
2017-12-15 15:03:36.595445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.031682 -6.9146156 -7.8499074 -8.3233643 -8.52729 -8.1428509 -7.9574876 -6.9384 -6.124506 -5.9372559 -5.714407 -6.6235943 -7.3714647 -7.848444 -8.3517885][-6.3552389 -6.6436281 -7.5883551 -8.6591835 -9.5642433 -9.6190739 -9.5067663 -8.5953732 -7.1709919 -6.6840754 -6.4764013 -7.5800414 -8.6221285 -9.0886536 -9.9169779][-4.3796148 -4.5692792 -5.2323895 -6.2807784 -7.0761714 -7.1451921 -7.4593987 -6.5552835 -5.5225167 -5.3374825 -5.4010763 -7.2383261 -8.2272177 -8.0958281 -8.4275923][-3.3005219 -3.5642762 -4.2812924 -4.8907657 -5.4703751 -5.5823746 -5.0644855 -4.2227449 -3.0524979 -3.1155987 -3.6329503 -5.4882846 -6.6196275 -6.8843246 -7.8429327][-2.8953037 -2.4195242 -1.6586704 -2.1743121 -2.2849631 -1.6898074 -0.9205637 -0.497118 0.2248702 -0.38166571 -1.0327826 -3.5621066 -5.15409 -5.5110741 -6.3970509][-2.7851968 -2.5843892 -2.4498339 -1.7811613 -1.094161 -0.27191448 1.0702591 1.5924301 2.2707567 1.1271286 -0.083060265 -1.9268632 -3.9025943 -5.0342507 -6.4652772][-4.0353031 -3.6576109 -2.6113925 -0.88706064 0.62424564 1.9434872 3.2586241 3.1936331 3.2690268 2.1187706 0.5729351 -2.3420157 -4.7467566 -5.9412842 -6.5978761][-5.2415161 -4.8201933 -3.9037964 -1.7196345 0.28048658 2.1907358 3.4830112 3.5830383 3.655798 2.1352005 -0.0048465729 -2.9079432 -5.1175203 -6.0894794 -7.102942][-7.0377436 -6.3689995 -5.3363352 -3.1976242 -0.99704123 1.0781927 2.2673378 2.3330793 2.0938864 0.86217403 -1.0138717 -4.1224289 -6.2530947 -6.6246791 -6.6643963][-7.7577329 -7.0235915 -6.4470577 -4.9636049 -3.4513216 -1.5172825 -0.22414303 0.18173695 0.25770903 -1.0403342 -2.52882 -4.633399 -5.92399 -6.8751569 -7.2422905][-9.024826 -8.6151285 -8.53578 -7.2561474 -5.8883233 -4.7060909 -3.4356933 -3.0346251 -2.8816476 -3.4688907 -4.4146032 -6.1025877 -7.1230044 -7.6864476 -7.5060396][-9.0639009 -8.7452888 -8.4492083 -7.7008529 -6.7223382 -5.5629807 -4.1439867 -3.7679455 -3.1877432 -3.8641698 -4.8055124 -5.6146336 -6.3808622 -6.8713446 -7.5219164][-7.1785417 -7.1494589 -7.4460697 -6.96789 -6.0720925 -5.4559422 -4.6378675 -4.8809223 -4.6632586 -4.7731009 -5.3298254 -6.033987 -6.517745 -6.3227587 -6.3039417][-4.1647043 -4.1702108 -4.602561 -4.5085044 -4.0921297 -3.7196264 -2.9448895 -3.2124462 -3.2923017 -3.6664619 -4.6896358 -4.5928354 -5.1817226 -5.1321726 -5.7971659][-4.6034918 -4.4146843 -4.4069395 -4.3761606 -4.2067795 -4.0472326 -3.9739926 -4.0426769 -4.1755934 -4.3644361 -4.8465481 -4.8111649 -5.3632336 -5.721272 -5.6884365]]...]
INFO - root - 2017-12-15 15:03:42.969943: step 26710, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 54h:21m:31s remains)
INFO - root - 2017-12-15 15:03:49.319519: step 26720, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 53h:05m:27s remains)
INFO - root - 2017-12-15 15:03:55.794693: step 26730, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 54h:14m:55s remains)
INFO - root - 2017-12-15 15:04:02.181404: step 26740, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 54h:40m:34s remains)
INFO - root - 2017-12-15 15:04:08.582132: step 26750, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 54h:24m:05s remains)
INFO - root - 2017-12-15 15:04:14.939629: step 26760, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 54h:33m:40s remains)
INFO - root - 2017-12-15 15:04:21.281371: step 26770, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:27m:25s remains)
INFO - root - 2017-12-15 15:04:27.786953: step 26780, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 54h:13m:46s remains)
INFO - root - 2017-12-15 15:04:34.160592: step 26790, loss = 0.36, batch loss = 0.24 (12.7 examples/sec; 0.629 sec/batch; 53h:25m:25s remains)
INFO - root - 2017-12-15 15:04:40.564489: step 26800, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 54h:36m:03s remains)
2017-12-15 15:04:41.080288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3514376 -8.3666821 -8.5235014 -8.8019142 -8.9178772 -8.394001 -7.8853984 -6.6690884 -5.76589 -5.5030193 -4.8829794 -5.138072 -5.4127884 -6.6280184 -7.3103285][-7.1874642 -6.9941359 -6.869688 -7.1092491 -7.0635653 -7.1646066 -6.5845289 -6.3499122 -5.3389206 -4.8146362 -4.2502403 -4.7008586 -4.9352393 -5.8743286 -6.1795979][-5.3684621 -5.2287245 -4.8819695 -4.8296971 -4.5818505 -4.2378798 -3.7799954 -3.730653 -3.5677471 -3.9998238 -3.8631213 -4.1066046 -4.599678 -5.3700533 -5.5808663][-5.5028019 -4.6192522 -3.4791284 -2.9564223 -2.1472521 -1.5665369 -1.1122627 -0.99117279 -0.80792427 -1.5710869 -2.4647393 -3.8462532 -4.6670866 -5.296854 -5.6766243][-4.1534624 -3.3035088 -2.13204 -0.883512 0.11392546 0.7225647 1.3225203 1.6059761 1.4905949 0.39412785 -0.6701355 -2.7535176 -4.4254913 -5.8212652 -6.2989025][-3.3364663 -2.6528006 -1.4853501 0.0057482719 1.0803785 1.8453503 2.4031343 2.5375824 2.6817722 1.5045757 0.13961935 -1.6839218 -3.287991 -5.2134409 -5.9686265][-3.0439 -1.8586397 -0.46159363 0.78589916 1.4479008 2.2610579 3.2241621 3.4485168 3.630868 2.4115753 0.96258354 -1.6836119 -3.9232607 -5.7879815 -6.3556509][-2.9632759 -2.1610727 -0.99994993 0.78174877 1.8714075 2.6445246 3.1414995 3.4156666 3.6205797 2.6355677 1.1031513 -1.5860429 -3.342082 -5.2751646 -6.1422911][-3.9369359 -2.5112481 -0.752676 0.37922096 0.66214085 1.4438496 2.1249094 2.2942476 2.5320034 1.5733194 0.042376041 -1.9272346 -3.2849765 -5.5370264 -6.2174292][-5.0234003 -3.8119693 -2.8097997 -1.9693556 -1.4658914 -0.61352015 -0.016260147 0.8579216 1.474618 -0.12631512 -1.418786 -3.7890894 -4.9415665 -5.8025007 -6.3684425][-5.5512538 -4.9408464 -4.1193 -3.3402729 -2.7615 -2.0145674 -1.1689296 -0.788126 -0.9206996 -1.8600464 -2.669354 -4.2558346 -4.7665815 -5.7593932 -6.3925714][-7.5613928 -6.9900479 -5.9293451 -5.1829786 -4.4699464 -3.9246416 -3.276752 -3.2148595 -3.1069422 -3.808409 -4.3589163 -5.0507908 -5.4666424 -5.8019085 -5.9242234][-7.5914588 -7.5061646 -7.3351274 -6.7906919 -6.1719303 -5.4926453 -4.8720536 -4.6378489 -4.8120947 -4.9315023 -5.4168096 -5.764411 -5.8026524 -5.6261387 -5.2841468][-8.0362711 -7.8621817 -7.5904541 -7.2214451 -6.7088728 -6.0068588 -5.5609069 -5.6927938 -5.5940971 -5.5011015 -5.6140051 -5.0267887 -4.4179249 -4.270422 -4.219574][-8.7798262 -8.4765549 -7.8176928 -7.2769403 -6.7309966 -6.1713 -6.3705592 -6.6040487 -7.2371731 -6.7559423 -6.02225 -5.342998 -4.877367 -4.6700149 -4.2841244]]...]
INFO - root - 2017-12-15 15:04:47.542753: step 26810, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 55h:17m:52s remains)
INFO - root - 2017-12-15 15:04:53.917234: step 26820, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 53h:40m:25s remains)
INFO - root - 2017-12-15 15:05:00.334428: step 26830, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 55h:05m:08s remains)
INFO - root - 2017-12-15 15:05:06.790654: step 26840, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.675 sec/batch; 57h:17m:26s remains)
INFO - root - 2017-12-15 15:05:13.253494: step 26850, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 56h:30m:17s remains)
INFO - root - 2017-12-15 15:05:19.713647: step 26860, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 56h:04m:24s remains)
INFO - root - 2017-12-15 15:05:26.135347: step 26870, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 55h:39m:03s remains)
INFO - root - 2017-12-15 15:05:32.519717: step 26880, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 53h:34m:24s remains)
INFO - root - 2017-12-15 15:05:38.830497: step 26890, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 54h:02m:14s remains)
INFO - root - 2017-12-15 15:05:45.240873: step 26900, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 53h:41m:51s remains)
2017-12-15 15:05:45.702171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0319977 -4.9163752 -5.106781 -4.6205359 -4.2126741 -3.1318259 -2.5990539 -2.7447295 -2.588079 -3.2280917 -3.4691572 -4.2454853 -4.1699276 -4.764101 -6.5598497][-3.9173086 -3.9838147 -4.040041 -4.0643172 -4.3842316 -4.7212563 -4.5469751 -4.3136044 -4.1079907 -4.4220195 -3.8876245 -4.3673029 -4.7895565 -4.7375574 -5.6211596][-3.5788565 -3.2172303 -2.9808955 -2.3412337 -2.5099454 -3.1751962 -3.9675586 -4.5416117 -4.447464 -4.565114 -4.4660683 -5.2572484 -4.9826565 -5.0598335 -6.3574653][-4.36909 -3.8921509 -2.7971678 -1.7719841 -1.1287994 -0.96483278 -1.2309818 -2.3477607 -3.1553822 -3.8818355 -3.8464994 -4.5919285 -5.1147585 -5.5185113 -6.3239708][-4.0111923 -3.6744857 -2.7711244 -1.6414981 -0.63361025 0.091888905 0.37475491 -0.22857428 -0.77205133 -1.9013853 -2.6252184 -3.8504186 -4.3243494 -4.9636188 -6.4102859][-4.2116718 -3.7753227 -2.8965101 -1.6382208 -0.68637896 0.16749716 0.77620411 0.77071571 0.4997406 0.0031814575 -0.72401953 -2.6341219 -3.6920655 -4.7201648 -5.8429184][-3.2562752 -2.9360332 -1.9751649 -0.692399 0.30919361 0.52982426 0.94863987 0.66787815 0.79292488 -0.059373856 -0.7164011 -1.7097168 -3.0956054 -4.4661789 -5.782239][-1.5143771 -0.74713707 -0.34840012 0.88785267 1.2348003 1.1744184 1.1022043 0.38122368 -0.0746541 -0.38713455 -0.35369396 -2.0460505 -3.3917103 -4.3743477 -6.0036554][-1.880846 -0.49811602 0.93005562 1.6064053 1.8106451 1.5837202 1.4916143 0.51977348 0.1423974 -0.65270805 -1.0968418 -2.6150851 -4.1359978 -5.476903 -6.6192641][-3.0051298 -1.7642012 -0.62095308 0.75960922 1.1182785 1.1350813 1.0884304 0.23078346 0.0036306381 -0.47653818 -1.6606054 -3.2951713 -4.1165228 -5.6667142 -7.4487739][-4.1523533 -3.5452023 -2.8449678 -1.7019076 -1.319675 -1.1082492 -0.78935528 -1.0907755 -1.1903601 -1.6577673 -2.3432498 -3.6463823 -4.6443086 -5.5863166 -7.094965][-6.4171152 -6.3413038 -5.663228 -4.2972946 -3.6055179 -3.3961992 -2.99856 -2.5423555 -2.0116119 -2.6389613 -3.3483982 -3.9324336 -4.5651979 -5.7405748 -7.0185738][-8.8564205 -8.8864012 -8.4279442 -7.2118812 -5.9661369 -5.1567459 -4.600975 -4.0793333 -3.8695385 -3.9747164 -3.933955 -4.0290108 -4.4801693 -5.7195487 -6.730216][-8.2771864 -9.1221218 -8.9848328 -8.0354834 -7.1479454 -6.6599083 -5.9662151 -5.6137986 -5.3884687 -5.5741644 -4.9962807 -4.6867557 -4.8018723 -5.5212126 -6.3176808][-8.0288458 -8.3157377 -8.7831154 -8.5964184 -7.74737 -7.2960882 -7.1186447 -7.2966 -7.2447853 -7.035327 -6.6340194 -5.8613882 -5.8146615 -5.7669315 -5.8034515]]...]
INFO - root - 2017-12-15 15:05:52.137622: step 26910, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 54h:04m:15s remains)
INFO - root - 2017-12-15 15:05:58.457798: step 26920, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 52h:58m:50s remains)
INFO - root - 2017-12-15 15:06:04.779131: step 26930, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 54h:26m:25s remains)
INFO - root - 2017-12-15 15:06:11.151156: step 26940, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 54h:07m:11s remains)
INFO - root - 2017-12-15 15:06:17.528990: step 26950, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 52h:57m:54s remains)
INFO - root - 2017-12-15 15:06:23.839810: step 26960, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 53h:15m:09s remains)
INFO - root - 2017-12-15 15:06:30.280370: step 26970, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 54h:29m:45s remains)
INFO - root - 2017-12-15 15:06:36.621721: step 26980, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 53h:24m:31s remains)
INFO - root - 2017-12-15 15:06:43.007109: step 26990, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 54h:35m:45s remains)
INFO - root - 2017-12-15 15:06:49.406355: step 27000, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.639 sec/batch; 54h:13m:35s remains)
2017-12-15 15:06:49.923331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.001544 -7.10225 -7.2698231 -7.2384 -6.3278227 -4.9797754 -3.9948173 -3.2257967 -2.9458442 -3.9228845 -4.5361004 -4.9287415 -6.5206318 -6.7035408 -7.1800289][-5.8202963 -5.5988178 -5.715055 -5.78536 -5.5858564 -5.0585747 -4.7577639 -4.6866655 -4.4854665 -5.0764356 -5.3809757 -5.8582315 -7.0261364 -7.27098 -7.6116118][-3.8534262 -4.1792603 -4.462141 -4.7271481 -4.813767 -4.843976 -4.6808991 -5.1290269 -4.8240504 -5.5306106 -6.5037746 -7.26803 -8.1560211 -7.83109 -8.1126318][-3.5875573 -3.2351484 -3.3248348 -3.2852287 -3.1287208 -3.1030927 -2.9851646 -3.2903261 -3.6725893 -5.155921 -6.208447 -7.1001563 -8.9264669 -8.8152885 -9.1646194][-3.5476446 -2.8145156 -1.8624754 -1.6102271 -1.8976355 -1.3600831 -1.1387897 -1.4643974 -2.2845078 -3.915313 -5.2836323 -6.422123 -8.0987358 -8.552597 -9.1047392][-3.3137355 -2.7602334 -2.0480409 -1.1926384 -0.26104593 -0.17538548 0.17942286 0.31851292 0.14947987 -1.3764892 -3.5474558 -5.1920729 -7.1530766 -7.9096441 -8.4315863][-3.3677354 -2.4406524 -1.1953382 -0.17012501 1.1781816 1.5116949 2.1919155 2.4813528 2.3942547 0.51040554 -1.7028065 -3.4374728 -5.8090348 -6.810286 -7.741045][-2.2422013 -1.1676922 -0.6743474 0.25777721 1.2711163 2.3794918 3.1990032 2.6765909 2.3602247 0.85065174 -1.1834941 -3.0184383 -5.5207734 -6.5731955 -7.4127135][-3.045949 -2.9263783 -2.4736738 -1.0754304 0.22703028 0.69492149 1.1826649 1.7269325 1.7255993 -0.050298691 -1.4237266 -2.7501965 -4.7116556 -5.7438364 -6.9513636][-3.0224562 -3.6647668 -3.2057509 -2.2778435 -1.6055403 -1.1900764 -0.47702742 -0.54822254 -0.18267202 -1.2396231 -2.8146143 -4.0429754 -5.4653034 -6.105762 -6.3284311][-4.796998 -4.7065735 -4.3536987 -4.473135 -3.6972063 -2.962575 -2.5243702 -2.9374523 -3.0614758 -3.9186046 -5.2441344 -5.8907957 -6.6964235 -6.87728 -7.2261162][-5.1217976 -5.0864272 -4.8794742 -4.6978855 -3.8757851 -4.0861044 -4.198822 -4.7603636 -4.8209343 -5.620573 -5.5003633 -6.02153 -7.0069656 -7.676744 -7.7933545][-6.3575935 -5.9865546 -5.586772 -5.6745071 -5.3470941 -4.7454796 -4.3153286 -4.8616734 -5.3123293 -6.569519 -6.5343103 -6.52598 -6.4058061 -6.4951091 -6.9894085][-6.5516453 -6.1218119 -5.7590389 -5.608901 -5.16801 -5.1871719 -5.3550453 -5.2412515 -5.1235018 -5.5745687 -6.2463541 -6.4921045 -6.4556541 -6.3002095 -5.8250351][-6.3602028 -6.5173616 -5.8502679 -5.6510267 -5.4156866 -5.6467457 -5.6882591 -5.6076436 -5.5814347 -5.234509 -5.287303 -5.4923878 -5.5076981 -5.5915751 -5.4589171]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 15:06:56.437006: step 27010, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 53h:59m:42s remains)
INFO - root - 2017-12-15 15:07:02.839379: step 27020, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 54h:26m:38s remains)
INFO - root - 2017-12-15 15:07:09.194430: step 27030, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 54h:19m:20s remains)
INFO - root - 2017-12-15 15:07:15.578833: step 27040, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 52h:42m:42s remains)
INFO - root - 2017-12-15 15:07:21.921002: step 27050, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.615 sec/batch; 52h:09m:45s remains)
INFO - root - 2017-12-15 15:07:28.344394: step 27060, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.637 sec/batch; 54h:05m:17s remains)
INFO - root - 2017-12-15 15:07:34.678768: step 27070, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:44m:46s remains)
INFO - root - 2017-12-15 15:07:41.062774: step 27080, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 54h:04m:41s remains)
INFO - root - 2017-12-15 15:07:47.434343: step 27090, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 54h:38m:04s remains)
INFO - root - 2017-12-15 15:07:53.839923: step 27100, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 54h:03m:10s remains)
2017-12-15 15:07:54.398402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6398621 -5.1191382 -6.0746346 -6.6153393 -6.8017354 -6.1110768 -4.9073091 -4.0428152 -3.4836235 -4.3352718 -5.2036333 -5.928864 -6.5816259 -7.1713829 -7.4082265][-4.2349796 -4.4952245 -5.2716804 -5.9985995 -6.144875 -5.785656 -4.7777162 -4.0643005 -3.6357846 -4.546443 -5.5427608 -6.1992464 -6.9959345 -7.1069164 -7.3010426][-3.0824714 -3.2405128 -4.0865335 -5.2977862 -5.4451189 -4.6185789 -3.8725574 -3.5592246 -3.4462852 -4.2940345 -5.1536784 -6.1847925 -7.119628 -7.6746793 -7.56043][-3.2979603 -3.5216312 -4.2866445 -4.5815592 -4.2232943 -3.0568681 -1.9956555 -1.5896573 -1.8469906 -3.4360666 -4.8025246 -6.2904091 -7.2972593 -7.7715058 -7.7652993][-4.1281309 -4.3337893 -4.6591043 -4.1953907 -3.6840796 -1.9029922 -0.23206902 0.39814854 0.23262739 -1.2708898 -3.0716014 -4.8951025 -6.3226967 -7.2591028 -7.5541024][-5.2400036 -4.9594 -4.6383858 -3.5348244 -1.9664598 -0.41961432 0.86870861 1.625639 1.7692137 -0.067828655 -1.7410212 -3.357985 -5.0945435 -6.5531883 -7.4268827][-4.4606586 -4.2239461 -3.3416152 -2.108963 -0.82405281 0.70161724 2.4334812 2.9789581 2.8086004 0.87143707 -0.74488354 -2.5867038 -3.9117885 -4.961937 -5.7621565][-4.4353008 -3.457881 -2.9608908 -1.3606229 0.51936913 1.6015091 2.3737335 2.927557 3.1405544 1.1833725 -0.6355648 -2.0179853 -3.4929366 -4.4812 -4.7123957][-5.0337896 -4.0363655 -3.2928691 -1.7268505 -0.62932253 0.30884981 1.3560171 1.3390799 1.1003265 -0.27427149 -1.7779932 -2.9879112 -4.0000792 -4.76318 -5.2206578][-6.2275782 -4.9460835 -4.2883396 -3.1389947 -1.7210379 -1.0712342 -0.88534355 -1.1468954 -0.95708227 -2.93849 -3.929338 -4.2389917 -5.34612 -5.5295706 -5.3218746][-8.0926123 -7.3159881 -6.66693 -5.2458444 -3.9324684 -2.9959574 -2.0550036 -2.8202362 -3.5468416 -4.5860491 -5.4279723 -5.6926565 -6.1806388 -5.9242778 -6.1908884][-9.8039589 -8.1931372 -6.8619919 -5.9333363 -5.032742 -4.0398235 -3.8971798 -4.3031025 -4.273284 -5.5355759 -6.2296886 -6.2752595 -6.8683128 -6.5133991 -6.7674317][-9.7869244 -8.7311745 -7.8658967 -6.8592176 -5.6753397 -4.8912811 -4.493454 -5.0570173 -5.317832 -5.9546676 -6.40495 -6.3261447 -6.3548489 -5.6392236 -5.6028957][-8.7423277 -7.9960732 -7.1288896 -6.4193463 -5.9725466 -5.3837404 -4.9746771 -5.1679063 -5.2633767 -5.896368 -5.9302006 -6.0012817 -6.0192637 -5.2211208 -4.8828087][-7.2367611 -6.9968357 -6.7348862 -6.3860188 -6.1096869 -5.7767992 -5.2243571 -5.4660406 -5.8383951 -5.77075 -5.5368834 -5.3690243 -5.2585182 -5.1172743 -4.8795514]]...]
INFO - root - 2017-12-15 15:08:00.806313: step 27110, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 54h:20m:09s remains)
INFO - root - 2017-12-15 15:08:07.275069: step 27120, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 55h:14m:50s remains)
INFO - root - 2017-12-15 15:08:13.626657: step 27130, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 52h:49m:18s remains)
INFO - root - 2017-12-15 15:08:20.047798: step 27140, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 53h:55m:49s remains)
INFO - root - 2017-12-15 15:08:26.435341: step 27150, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 54h:49m:24s remains)
INFO - root - 2017-12-15 15:08:32.771246: step 27160, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 53h:33m:15s remains)
INFO - root - 2017-12-15 15:08:39.172872: step 27170, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 54h:43m:52s remains)
INFO - root - 2017-12-15 15:08:45.589532: step 27180, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 56h:01m:46s remains)
INFO - root - 2017-12-15 15:08:51.892455: step 27190, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 53h:25m:39s remains)
INFO - root - 2017-12-15 15:08:58.244340: step 27200, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 55h:32m:33s remains)
2017-12-15 15:08:58.746260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0978532 -8.4035072 -11.236635 -12.928019 -14.199729 -12.730057 -10.882515 -9.0929651 -8.68506 -8.95164 -9.3061018 -10.11964 -7.8836951 -8.332159 -8.7375078][-6.1324639 -7.7963133 -10.820013 -13.213545 -15.819541 -15.504128 -14.080151 -10.975934 -8.80529 -8.2379274 -8.8315315 -10.702406 -9.571146 -10.31988 -9.7985058][-6.3054624 -7.8083997 -10.353987 -11.799932 -13.14633 -12.639166 -12.250088 -11.526554 -9.6936 -9.103199 -9.0273476 -10.600587 -9.3231516 -10.070843 -10.710766][-6.7769127 -8.1890574 -10.229792 -10.212088 -9.9769773 -8.6348677 -7.0756712 -6.9005308 -7.4206119 -8.6629963 -9.2670307 -10.635453 -9.2753487 -10.231453 -10.257071][-6.481739 -6.7363791 -8.1137447 -7.5848284 -6.2081094 -3.9652028 -2.2327747 -1.5860996 -2.040257 -4.9819894 -7.8860722 -11.057918 -8.82494 -9.8863792 -10.673764][-5.8514252 -6.2546735 -7.038085 -5.2997041 -3.2542439 -0.71764708 0.94869423 1.8495235 2.167388 -1.0333757 -4.1058645 -8.0498238 -7.5718427 -9.1018152 -9.5669][-4.874413 -4.9396796 -4.9757938 -3.30127 -0.89587641 1.96352 2.7814445 3.5546455 3.8975954 2.3361654 -0.09552145 -5.3330307 -5.2540121 -8.3023243 -9.3878975][-6.0082455 -5.1992736 -3.9054058 -1.4269528 0.58743095 3.1395674 4.1248493 4.2583027 3.5412674 1.3145695 -0.025293827 -3.6382484 -3.2676058 -7.0972953 -9.3752069][-6.8307219 -6.3391056 -5.1275649 -2.6438942 -0.41213989 1.9682121 2.3078537 2.8263683 2.5769463 0.22602987 -2.1367531 -6.3842673 -5.3371959 -6.8903036 -8.2763252][-8.0025082 -7.215941 -5.8103275 -4.0232897 -2.9212132 -1.4208484 -0.44054985 1.1590633 1.4193144 -1.2721086 -3.4177785 -8.2153416 -7.79902 -9.3819485 -9.5421438][-8.3402319 -8.1358252 -6.9189577 -5.38245 -3.9377403 -2.3838544 -1.5651083 -1.2384539 -1.2875485 -3.3920431 -5.0028028 -8.6024818 -8.9887466 -10.849874 -11.56641][-8.0424681 -8.3963022 -7.6870365 -6.9779615 -5.96966 -4.0812597 -2.5224314 -2.3710451 -2.8605676 -5.4729447 -6.6220036 -7.8480773 -7.2552872 -8.5340261 -9.5939026][-10.0135 -9.7949171 -8.5108852 -7.848917 -7.4924827 -5.5954776 -4.00072 -3.4970517 -3.4067397 -5.542881 -6.0897365 -6.2520642 -5.3259273 -5.9451833 -6.7068415][-10.128713 -10.585102 -9.1950159 -8.561367 -8.0093765 -6.7853661 -5.5344486 -4.7710118 -4.1705675 -4.8592672 -4.8902693 -4.3392825 -3.4247699 -3.5440078 -3.892462][-7.5091934 -8.8320827 -8.0263548 -7.7204671 -6.7867541 -5.5148144 -5.0465517 -5.246438 -5.3779545 -5.340364 -4.6059246 -4.0397635 -3.4402647 -2.9883122 -3.1002412]]...]
INFO - root - 2017-12-15 15:09:05.136625: step 27210, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 53h:33m:50s remains)
INFO - root - 2017-12-15 15:09:11.559349: step 27220, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 54h:35m:49s remains)
INFO - root - 2017-12-15 15:09:18.055229: step 27230, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 55h:24m:32s remains)
INFO - root - 2017-12-15 15:09:24.430182: step 27240, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 56h:22m:45s remains)
INFO - root - 2017-12-15 15:09:30.877412: step 27250, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 54h:19m:06s remains)
INFO - root - 2017-12-15 15:09:37.217799: step 27260, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 52h:49m:29s remains)
INFO - root - 2017-12-15 15:09:43.609175: step 27270, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 54h:14m:01s remains)
INFO - root - 2017-12-15 15:09:50.053328: step 27280, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 53h:25m:01s remains)
INFO - root - 2017-12-15 15:09:56.620554: step 27290, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 56h:41m:05s remains)
INFO - root - 2017-12-15 15:10:03.007866: step 27300, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 54h:54m:42s remains)
2017-12-15 15:10:03.527274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2737312 -3.9966085 -3.861295 -3.9317632 -4.0732088 -3.9877279 -3.6342654 -2.9906845 -2.3211703 -2.9544744 -3.1426535 -4.044095 -4.4810715 -6.1686726 -7.6416335][-3.605938 -3.3159561 -3.2748361 -3.7454293 -4.05665 -3.8881793 -3.4472303 -3.2959237 -3.0030684 -3.1355515 -2.7307048 -3.7689307 -5.2796822 -6.6626558 -6.7932816][-3.8332136 -3.3981295 -3.4490757 -3.8997457 -4.2017789 -4.0670471 -3.4215221 -2.9110675 -2.5387187 -3.5234842 -3.6986074 -4.5320253 -5.3642197 -6.879447 -7.5843148][-4.730123 -3.7679796 -3.4985328 -3.1090717 -3.0626202 -2.5268126 -1.874095 -1.6285319 -1.5819426 -3.0793858 -3.7485077 -4.6456885 -5.2622242 -6.4427824 -7.0135765][-5.2552729 -4.0352392 -3.0469189 -2.3589053 -2.0319905 -1.5542645 -1.0971017 -0.96064377 -0.51323223 -1.7001319 -2.7555957 -4.0929804 -5.2030468 -6.3442025 -7.0032868][-4.6794848 -3.753628 -3.3451648 -2.1742167 -1.2322526 -0.21116781 0.64324 0.32987595 0.26245451 -0.96326113 -1.6313348 -2.9892793 -4.2320013 -5.3609381 -6.1485806][-3.9218228 -2.739099 -2.0661063 -0.91374588 0.069665432 1.5233021 2.3549795 2.3707237 2.467083 0.37604332 -0.79144716 -2.4991074 -3.5065002 -4.8779793 -5.94864][-3.5014668 -2.6810184 -1.4125195 -0.023026466 1.161087 2.1817589 2.8874454 3.2091818 3.1358271 1.876668 0.98548603 -1.3411727 -2.7968035 -4.3769526 -5.5266361][-4.1311393 -3.5237379 -2.3372827 -0.58784962 0.19847631 0.84419441 1.6836987 2.3570423 2.70049 1.8644009 0.70364666 -1.3773847 -2.9027653 -4.4001093 -5.5327988][-4.6587477 -3.8504527 -3.3186207 -1.7794304 0.031954765 0.81829262 1.1098232 1.2894993 1.3592491 0.15626955 -0.71573925 -2.2003107 -4.0009985 -5.1147814 -5.4394035][-6.5942574 -5.1947403 -5.1003141 -4.0190806 -2.9308319 -1.8599472 -1.3462892 -1.1280251 -1.2563405 -2.5155954 -3.7656772 -4.6855597 -5.60425 -6.5620832 -6.8779955][-8.9754257 -7.2656884 -6.18352 -5.3854575 -5.3068914 -4.73746 -4.13585 -3.843313 -3.4779921 -3.9181471 -5.0914917 -5.6253033 -6.2439947 -7.2127 -7.3167758][-8.4500084 -8.2810688 -8.0176306 -6.624712 -5.7631569 -5.2720656 -4.865324 -4.8305817 -4.7581563 -5.0473838 -5.82266 -6.1848488 -6.8179297 -6.8659916 -6.7576809][-8.9425621 -8.45142 -8.1761808 -7.493845 -6.7380304 -5.9800911 -5.3785467 -5.4364843 -5.7237811 -5.9827571 -5.8766885 -5.9640651 -6.1064458 -6.2330794 -5.8903103][-9.1738768 -8.4480352 -8.2550392 -8.1082058 -7.5670681 -6.7286878 -6.2619963 -6.1680627 -6.4717555 -6.6559954 -6.7436366 -6.2214026 -6.0538921 -6.1684513 -5.7543783]]...]
INFO - root - 2017-12-15 15:10:09.929639: step 27310, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.615 sec/batch; 52h:09m:26s remains)
INFO - root - 2017-12-15 15:10:16.283563: step 27320, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 53h:47m:44s remains)
INFO - root - 2017-12-15 15:10:22.706554: step 27330, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.615 sec/batch; 52h:06m:42s remains)
INFO - root - 2017-12-15 15:10:29.148567: step 27340, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 55h:39m:59s remains)
INFO - root - 2017-12-15 15:10:35.549890: step 27350, loss = 0.38, batch loss = 0.27 (12.5 examples/sec; 0.642 sec/batch; 54h:27m:29s remains)
INFO - root - 2017-12-15 15:10:41.999935: step 27360, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 52h:15m:07s remains)
INFO - root - 2017-12-15 15:10:48.564436: step 27370, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.658 sec/batch; 55h:43m:43s remains)
INFO - root - 2017-12-15 15:10:54.898501: step 27380, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 53h:00m:06s remains)
INFO - root - 2017-12-15 15:11:01.417240: step 27390, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 55h:23m:48s remains)
INFO - root - 2017-12-15 15:11:07.885373: step 27400, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 54h:25m:24s remains)
2017-12-15 15:11:08.424843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5414786 -3.6919854 -4.1228948 -4.4202547 -4.1882691 -3.5657763 -2.68966 -2.2247734 -1.7823849 -2.644248 -3.0632606 -4.1473618 -5.3311062 -5.811552 -6.5991712][-3.3548808 -4.0557566 -4.8966408 -4.7646456 -4.2728429 -3.76393 -2.7212043 -2.0093784 -1.606873 -2.6769905 -3.0878935 -4.0913725 -5.3001356 -6.0862083 -6.9645162][-3.1666379 -3.5984321 -3.9863293 -4.0359144 -3.4267282 -2.7737508 -2.2096729 -2.0683436 -1.8678594 -2.5102587 -2.9674854 -4.41761 -5.5600939 -6.1065278 -6.7585945][-2.54429 -2.9149895 -3.628912 -3.2300544 -2.2838154 -1.6315603 -1.0564804 -1.0834289 -1.1147075 -2.5441132 -2.9974728 -4.2503519 -5.852479 -6.644495 -7.0116034][-2.8050065 -2.1399512 -1.9278378 -1.4251456 -0.93602037 -0.39728308 -0.20485544 -0.30523729 -0.28876209 -1.7342339 -2.5755911 -4.1322875 -5.6007729 -6.4802217 -7.5039587][-3.7898743 -2.7751231 -1.9218616 -1.0310864 -0.024809837 0.78743839 1.5983057 0.99027634 0.18443012 -1.2247977 -2.1390014 -4.2666082 -6.038599 -6.4833212 -7.101923][-4.0938025 -2.68954 -1.1809626 0.39632893 1.2116432 2.0002041 2.6009398 2.0338688 1.4665127 -0.56683016 -1.7362928 -4.0713258 -5.9683132 -6.5395 -6.962409][-3.4991922 -2.3954492 -1.3741288 0.88552475 2.7498732 3.5509033 4.0355406 3.6115837 3.1884642 0.74842453 -0.90628195 -3.4450984 -5.2612181 -5.5745764 -6.0584183][-2.8988929 -2.5728898 -1.5897441 0.53340912 1.4359741 2.2141409 2.6811161 2.8897982 3.1116695 0.798625 -0.91158533 -3.3755116 -5.0846319 -5.4504871 -5.7688932][-3.6885123 -3.2658944 -2.7412133 -1.1029406 0.2907896 0.68255329 0.9281559 0.819335 0.7827425 -1.3297577 -2.6193123 -4.7176528 -5.9965873 -6.1223536 -5.982646][-4.4922552 -4.390192 -4.0381465 -3.2219086 -2.4834862 -1.7592402 -1.049624 -0.83182478 -0.72481585 -3.0064464 -4.3488913 -5.9665489 -6.7397552 -6.9226384 -6.8063865][-6.3937368 -5.66035 -5.3686008 -4.9296589 -4.4683042 -4.128541 -3.9819424 -3.4073672 -2.8713579 -4.2637682 -4.8390689 -5.5336895 -6.202219 -6.7675614 -7.380126][-6.9333744 -6.578537 -6.477623 -6.1162214 -5.9652715 -5.4002991 -5.2310987 -4.9423447 -4.5530391 -5.1128845 -5.4241166 -5.86719 -6.3829722 -6.5492625 -7.0240421][-7.9613161 -7.6580014 -7.4044604 -7.2547879 -7.0068455 -6.6746149 -6.4865022 -6.2027793 -6.17641 -6.4731369 -6.4913349 -6.3982983 -6.07895 -6.2083244 -6.3586092][-9.4988575 -9.146801 -8.9455042 -8.7519112 -8.4291792 -8.0326424 -7.4820404 -7.6071868 -7.8107138 -7.71769 -7.4011192 -7.2012243 -6.7917662 -6.7188835 -6.7218161]]...]
INFO - root - 2017-12-15 15:11:14.875771: step 27410, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:07m:22s remains)
INFO - root - 2017-12-15 15:11:21.235805: step 27420, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 53h:57m:34s remains)
INFO - root - 2017-12-15 15:11:27.680411: step 27430, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 54h:13m:04s remains)
INFO - root - 2017-12-15 15:11:34.048754: step 27440, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 52h:49m:57s remains)
INFO - root - 2017-12-15 15:11:40.480551: step 27450, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:57m:39s remains)
INFO - root - 2017-12-15 15:11:46.807559: step 27460, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 53h:05m:03s remains)
INFO - root - 2017-12-15 15:11:53.209608: step 27470, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 54h:52m:24s remains)
INFO - root - 2017-12-15 15:11:59.547827: step 27480, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 52h:22m:30s remains)
INFO - root - 2017-12-15 15:12:05.967694: step 27490, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 54h:10m:08s remains)
INFO - root - 2017-12-15 15:12:12.305941: step 27500, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 53h:33m:46s remains)
2017-12-15 15:12:12.808073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.021348 -3.0001764 -2.9458823 -2.7136884 -2.4173164 -2.1917276 -1.9557276 -1.984087 -2.2237663 -4.1453304 -4.6948791 -6.3506107 -7.334352 -7.887403 -8.2274628][-2.894063 -3.1489387 -3.1659794 -3.3683915 -3.2823653 -3.0878091 -3.1567311 -3.2104549 -3.2490644 -4.9728394 -5.1580124 -6.7814884 -7.4347243 -7.8141885 -8.05012][-3.8420889 -4.0060558 -3.8101177 -3.6155376 -3.3051872 -2.6070857 -2.1667314 -2.5842175 -2.9083319 -5.1341405 -5.5405359 -6.834147 -7.4927735 -7.8295984 -8.2374554][-4.6165462 -4.0317788 -3.5719309 -2.9773078 -2.4244728 -1.83566 -1.3920503 -1.603045 -1.8363709 -3.9799356 -4.7145281 -6.3577971 -7.261786 -7.8226056 -8.4728012][-5.267951 -4.251018 -3.3425584 -2.5047617 -1.6915708 -0.94476175 -0.42406321 -0.64154577 -0.84786463 -3.2045903 -4.1479979 -5.7739611 -6.5589242 -7.1616364 -7.8880348][-5.2578745 -4.4255776 -3.5742054 -2.3648181 -1.2030969 -0.20906591 0.39177608 0.37953472 0.390316 -1.8916225 -2.7630291 -4.4829006 -5.647511 -6.3585033 -7.3102107][-4.292202 -3.8533828 -3.1170144 -1.8620791 -0.81872749 0.34166145 1.2107553 1.5266094 1.6365633 -0.55261564 -1.3813758 -3.1209402 -4.3572793 -5.6300573 -6.607707][-3.5982919 -2.769979 -1.8598289 -0.76662683 0.116642 0.9466877 1.3484344 1.8476934 2.2518835 0.18178749 -0.75892067 -2.5899849 -3.827533 -5.063818 -5.9792976][-2.6814647 -2.1851134 -1.5727882 -0.46423483 0.11459637 0.72636223 1.1516418 1.4636621 1.6420403 -0.31661034 -1.0196781 -2.7873006 -4.083817 -5.1180782 -5.9618282][-2.6489539 -2.07896 -1.3880467 -0.64166594 -0.011444569 0.53021812 0.69471169 0.889451 0.89575672 -1.2975664 -2.1318045 -3.545011 -4.7092943 -5.7178698 -6.3922181][-3.8010108 -3.3537169 -2.7113314 -1.6425338 -0.87321472 -0.53725863 -0.55108786 -0.44474983 -0.56089306 -2.4636612 -3.1668277 -4.4091535 -5.3318138 -6.0703893 -6.5882821][-4.9886093 -4.3992558 -3.4632559 -2.4451661 -1.442584 -0.8902154 -0.81608295 -0.90895414 -1.1102366 -2.569768 -3.1357207 -4.1837368 -4.9425745 -5.5229416 -6.1352673][-6.0312853 -5.3918924 -4.4623995 -3.2867293 -2.2470841 -1.4962726 -1.310627 -1.3044872 -1.6647511 -2.8774042 -3.204658 -4.1710052 -4.6489234 -4.9474497 -5.2254477][-6.4029326 -5.9037852 -5.1180696 -3.9661369 -3.2102566 -2.3809209 -2.4295588 -2.3775616 -2.4601817 -3.0538564 -3.1394787 -3.7784913 -4.0864372 -4.5024 -4.7444105][-6.3190727 -5.90948 -5.3263292 -4.8106718 -4.57528 -3.94932 -4.1016254 -4.0576286 -4.0501356 -4.1042461 -3.8069682 -4.2065 -4.4361849 -4.7620335 -4.9795332]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 15:12:20.415012: step 27510, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 55h:09m:32s remains)
INFO - root - 2017-12-15 15:12:26.862666: step 27520, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 54h:34m:52s remains)
INFO - root - 2017-12-15 15:12:33.310175: step 27530, loss = 0.33, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 55h:04m:04s remains)
INFO - root - 2017-12-15 15:12:39.704874: step 27540, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 55h:40m:23s remains)
INFO - root - 2017-12-15 15:12:46.106506: step 27550, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 54h:37m:00s remains)
INFO - root - 2017-12-15 15:12:52.412269: step 27560, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 53h:34m:52s remains)
INFO - root - 2017-12-15 15:12:58.773666: step 27570, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 55h:26m:59s remains)
INFO - root - 2017-12-15 15:13:05.165456: step 27580, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 53h:17m:58s remains)
INFO - root - 2017-12-15 15:13:11.563806: step 27590, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.646 sec/batch; 54h:40m:47s remains)
INFO - root - 2017-12-15 15:13:17.991010: step 27600, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 53h:26m:44s remains)
2017-12-15 15:13:18.526437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1083426 -5.3910732 -7.2224569 -8.7737122 -9.5331631 -10.2295 -10.405686 -9.5073042 -7.8065529 -6.9438405 -6.9577594 -7.0774646 -7.6284285 -7.9361515 -8.4733372][-3.8655875 -5.7154789 -7.50711 -9.0438919 -10.357142 -10.880916 -10.510952 -10.08658 -9.1501141 -8.4544353 -8.8951559 -9.2198868 -9.4777985 -9.8129425 -10.193344][-3.9829977 -5.1343279 -7.2938714 -8.3459873 -9.1087933 -9.2680445 -9.3103514 -9.2487926 -8.5953112 -8.0850573 -9.2533579 -10.35874 -11.166789 -11.363925 -11.557468][-5.8586416 -5.8857169 -6.6492195 -7.0856133 -7.6486459 -7.336184 -6.07291 -5.4919181 -5.0747328 -5.1539636 -6.655107 -8.3924761 -10.458935 -11.856349 -12.274399][-5.9871564 -5.8340464 -5.394979 -4.9360113 -4.3748407 -3.4451714 -2.1973939 -1.4713874 -0.52887964 -0.94669819 -3.35041 -5.7578211 -8.5099106 -10.251326 -11.430249][-5.8735514 -5.3599548 -3.9931364 -2.8313346 -1.829906 -0.16518068 2.1658211 2.826643 3.1614084 1.5650969 -1.2684698 -3.8608816 -6.7564473 -8.7683477 -10.217605][-5.3525209 -4.864007 -3.5959544 -1.6898761 0.65817547 2.39254 4.1778173 4.7716484 5.5152559 3.2895164 -0.086584568 -2.8922 -5.7390833 -7.5674605 -8.6866121][-5.5460954 -5.1720285 -3.7907841 -1.4101238 1.3540602 2.9971571 4.7635 5.4461355 5.6242495 3.2280035 0.21506071 -2.6246834 -5.7118211 -7.4284096 -8.0764847][-6.534441 -6.0810642 -5.193821 -3.4310465 -0.8974824 1.0339117 2.7326517 3.6004019 4.0192375 1.6551714 -1.5599842 -4.088253 -6.2391806 -7.3464451 -8.16841][-7.4562287 -7.1476345 -7.0341234 -5.8172264 -4.1407185 -2.5577497 -0.84187603 -0.16007233 0.012834072 -2.138 -4.8957334 -6.6566691 -7.9543 -8.62973 -9.3828249][-9.0359859 -9.3045149 -9.3438759 -9.24372 -8.0646486 -6.4830294 -5.012476 -4.5929174 -4.092432 -5.2424912 -7.2207689 -8.5353489 -9.418107 -9.9346294 -10.636041][-9.3156252 -9.5385542 -9.8329926 -9.5039349 -9.2198343 -8.7833128 -7.3282032 -6.7311058 -6.3527055 -6.8260226 -7.5430784 -7.9225416 -8.6767721 -9.6683006 -10.198463][-9.4170666 -10.172329 -10.409122 -9.9677153 -9.3574219 -8.940176 -8.5955791 -8.5772343 -7.7652135 -7.5909019 -8.2887592 -8.0737514 -7.8819323 -8.0121651 -8.686][-7.5632544 -7.9495668 -8.3458223 -7.8788004 -7.4450932 -6.86687 -6.7894115 -6.8230581 -7.0933375 -7.0079188 -7.1050162 -6.9538383 -6.9673967 -7.1772661 -7.4184504][-7.0915141 -7.6115026 -8.3052168 -8.3255825 -8.3424139 -7.7774949 -7.5568256 -7.5843139 -7.3718171 -7.453228 -7.7124605 -7.851006 -7.5869527 -7.9426012 -8.0217371]]...]
INFO - root - 2017-12-15 15:13:24.867609: step 27610, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 52h:39m:53s remains)
INFO - root - 2017-12-15 15:13:31.182203: step 27620, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 53h:04m:00s remains)
INFO - root - 2017-12-15 15:13:37.657874: step 27630, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 53h:44m:19s remains)
INFO - root - 2017-12-15 15:13:44.096210: step 27640, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.664 sec/batch; 56h:13m:57s remains)
INFO - root - 2017-12-15 15:13:50.470342: step 27650, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 53h:17m:14s remains)
INFO - root - 2017-12-15 15:13:56.861636: step 27660, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 55h:08m:21s remains)
INFO - root - 2017-12-15 15:14:03.284698: step 27670, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 55h:03m:48s remains)
INFO - root - 2017-12-15 15:14:09.714261: step 27680, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 55h:39m:51s remains)
INFO - root - 2017-12-15 15:14:16.187853: step 27690, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 54h:12m:17s remains)
INFO - root - 2017-12-15 15:14:22.573402: step 27700, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 52h:54m:04s remains)
2017-12-15 15:14:23.096540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1514707 -3.2062593 -3.2002754 -3.0163007 -2.8369575 -2.6591425 -2.2582159 -2.142786 -2.0705948 -3.5477695 -4.7832923 -5.9275126 -6.9370427 -7.7139292 -7.7887383][-4.0288053 -3.9733353 -4.3566117 -4.2321482 -3.8566666 -3.6419683 -3.373857 -3.0263381 -2.6360364 -3.9690127 -5.1207466 -6.98221 -8.4982758 -8.9784422 -9.1642151][-4.5963931 -4.4642758 -4.48886 -4.2284074 -4.3105083 -3.7012455 -2.8590117 -2.6805654 -2.3736005 -3.2769203 -4.1783938 -5.8725252 -7.1299214 -8.0007305 -8.2193775][-5.2282772 -5.494607 -5.2611113 -4.473753 -3.6665483 -2.8301535 -2.2976599 -2.2210808 -2.111752 -3.0605292 -3.80808 -5.1606588 -6.3032746 -6.7156215 -6.6381063][-6.2889481 -6.097682 -5.554256 -4.5357552 -3.2964735 -2.1042738 -0.78630733 -0.57745886 -0.42481709 -1.4650331 -2.7953768 -4.2616663 -5.514925 -6.522027 -6.8596458][-7.072916 -6.70854 -6.1412668 -4.7370625 -2.9873729 -1.2966003 0.14562225 0.6992445 1.1413527 0.14935446 -1.1033778 -3.1141386 -4.7359405 -5.8452063 -6.4168973][-7.8604417 -7.174521 -6.1483159 -4.46898 -2.4431081 -0.55375385 1.1478701 1.8026867 2.3377867 1.3221416 -0.0014123917 -2.1448321 -3.8614743 -5.1913137 -5.866559][-7.2460394 -6.4735985 -5.298954 -3.4968991 -1.521029 0.41669369 1.9823046 2.7861595 3.2625885 2.0917854 0.77092266 -1.3119888 -3.1515555 -4.7118111 -5.6274452][-6.8541417 -6.1669683 -5.5165854 -3.9601767 -2.3177872 -0.64887428 0.85532761 1.5093403 1.9736519 1.1733665 0.013019085 -1.9707026 -3.5944042 -5.1663761 -6.2043848][-7.06673 -6.4476652 -5.60437 -4.4848948 -3.1347041 -1.8426161 -0.80080652 -0.035020351 0.48300934 -0.71266317 -1.7988153 -3.5325446 -4.8941646 -5.8054075 -6.2193556][-7.5411019 -7.2801876 -7.0239844 -6.2026072 -5.2065468 -4.0017138 -2.9657955 -2.7263894 -2.2666941 -2.6141243 -3.5644112 -4.9764071 -6.2375016 -6.5664997 -6.6381807][-8.0310287 -7.7988224 -7.4927487 -6.9934416 -6.4768677 -5.6581125 -4.8744812 -4.4988365 -3.7721403 -4.3893757 -4.9410486 -5.7005916 -6.3242073 -6.7109818 -6.727354][-8.7241335 -8.3160276 -7.9770956 -7.3435364 -6.6607475 -6.4967823 -6.4445391 -6.16468 -5.9056377 -6.2546434 -6.5766125 -6.948103 -7.2331238 -7.1093507 -6.85035][-8.6873083 -8.4331808 -7.8972621 -7.2627387 -6.5892754 -6.4433217 -6.3994155 -6.5393391 -6.6994152 -7.3237438 -7.5257926 -7.6831245 -7.7061691 -7.4233317 -7.0490141][-9.424222 -8.9550486 -8.4630632 -7.9416251 -7.4377565 -7.1855187 -6.8810959 -6.837275 -6.86943 -6.9483929 -7.1799927 -7.4637389 -7.6722074 -7.4149261 -7.1531773]]...]
INFO - root - 2017-12-15 15:14:29.601711: step 27710, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 54h:21m:51s remains)
INFO - root - 2017-12-15 15:14:36.026813: step 27720, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 53h:54m:08s remains)
INFO - root - 2017-12-15 15:14:42.406539: step 27730, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.624 sec/batch; 52h:51m:34s remains)
INFO - root - 2017-12-15 15:14:48.741114: step 27740, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.617 sec/batch; 52h:15m:49s remains)
INFO - root - 2017-12-15 15:14:55.105237: step 27750, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.616 sec/batch; 52h:10m:51s remains)
INFO - root - 2017-12-15 15:15:01.565016: step 27760, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 52h:31m:45s remains)
INFO - root - 2017-12-15 15:15:08.036167: step 27770, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 52h:13m:43s remains)
INFO - root - 2017-12-15 15:15:14.377363: step 27780, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 54h:07m:34s remains)
INFO - root - 2017-12-15 15:15:20.720956: step 27790, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 52h:50m:46s remains)
INFO - root - 2017-12-15 15:15:27.056337: step 27800, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 54h:10m:58s remains)
2017-12-15 15:15:27.582676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7241468 -4.5534234 -3.7796962 -2.5903959 -1.8930254 -1.0328913 -0.40854883 -0.23639679 0.24410248 -0.67905664 -2.2129226 -3.7722754 -5.5700827 -6.83893 -8.3400688][-5.0270815 -3.7352564 -2.9739332 -2.3299589 -2.0289779 -1.9478593 -1.8414569 -1.5991845 -1.0788136 -1.7277508 -2.8326893 -4.3168993 -6.0204363 -6.6943922 -7.7296028][-3.8941464 -2.5839777 -1.3838973 -0.803041 -0.90601683 -0.98426294 -0.84932375 -0.97803211 -0.93015575 -1.5416718 -2.9327788 -4.4397335 -5.3833456 -5.9926128 -6.77286][-2.0076742 -1.0282531 0.011980534 0.44120502 0.54795647 0.52421188 0.3355093 0.51703453 0.48730946 -0.47202826 -2.463418 -4.5003061 -5.3948178 -5.594295 -6.46913][-1.1529341 -0.52289534 -0.1560936 0.12425756 0.40697861 0.71526909 1.080657 1.2598886 1.5692005 0.97736931 -0.731874 -2.7035351 -3.8304541 -4.5019031 -5.3909388][-0.2916503 0.20284605 0.30459213 0.14964485 0.27480459 0.7400341 1.3972692 1.9995108 2.1740227 1.5471964 0.18422604 -1.8291249 -3.192327 -3.920476 -4.7643328][-1.8444881 -1.2437987 -0.815732 -0.25584555 0.3083744 0.91809464 1.7853956 2.5855436 3.1862183 2.0636597 0.375432 -1.6830096 -3.9196632 -4.853755 -5.7070484][-2.7813358 -2.7621288 -2.0969248 -0.93172073 0.32004929 1.3164673 2.450201 2.9587088 3.4232597 2.2483444 0.57597637 -1.4349079 -3.7710898 -4.8413458 -5.7973208][-3.33328 -2.4837914 -2.0873976 -1.2428889 -0.10751724 1.3431654 2.6540365 3.0675812 3.2675457 2.091898 0.54225826 -1.5128446 -3.5439682 -4.3968458 -5.2607989][-4.4944773 -3.681293 -2.845942 -1.7558069 -0.98635197 0.30826807 1.8023071 2.4567118 2.7659922 1.3289289 -0.4553504 -2.2059989 -3.9989824 -4.6970496 -5.2173338][-5.5237484 -4.5864105 -3.7708862 -2.5041385 -1.4425516 -0.29905081 0.96411037 1.6814232 1.8849945 0.73152447 -0.8280015 -2.346776 -4.2491555 -4.9497786 -5.4822536][-6.2976375 -4.9323545 -3.8178821 -2.444294 -1.4823794 -0.43727732 0.68788433 1.2203331 1.5151863 0.57690525 -0.65761423 -2.0015631 -3.8237851 -4.8405991 -5.377326][-6.3991146 -5.1674767 -3.9750333 -2.7273545 -1.8610902 -0.98846674 0.11039448 0.51436329 0.97433472 0.022344112 -1.0829911 -2.0703478 -3.3697371 -4.2616816 -4.7676239][-6.667973 -5.7349472 -4.1801691 -2.8212428 -2.0769463 -1.3430624 -0.46335316 -0.11704588 0.57005024 0.11124516 -0.93705654 -1.3706865 -2.3154821 -3.2271886 -3.9239976][-8.2576389 -6.8528309 -4.9753633 -3.6559019 -2.6087341 -1.8264318 -1.166204 -0.88174772 -0.64034081 -0.94104433 -1.4882646 -2.0785246 -3.0663166 -3.6903062 -4.1934729]]...]
INFO - root - 2017-12-15 15:15:34.051873: step 27810, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 54h:22m:14s remains)
INFO - root - 2017-12-15 15:15:40.527481: step 27820, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 52h:44m:11s remains)
INFO - root - 2017-12-15 15:15:46.961668: step 27830, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 54h:16m:32s remains)
INFO - root - 2017-12-15 15:15:53.287671: step 27840, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 53h:12m:21s remains)
INFO - root - 2017-12-15 15:15:59.718546: step 27850, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 52h:17m:52s remains)
INFO - root - 2017-12-15 15:16:06.087151: step 27860, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 52h:22m:32s remains)
INFO - root - 2017-12-15 15:16:12.616912: step 27870, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 55h:43m:47s remains)
INFO - root - 2017-12-15 15:16:19.001458: step 27880, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 53h:15m:54s remains)
INFO - root - 2017-12-15 15:16:25.467211: step 27890, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 56h:53m:02s remains)
INFO - root - 2017-12-15 15:16:31.873308: step 27900, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 55h:09m:34s remains)
2017-12-15 15:16:32.379473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.807642 -7.9112425 -8.4961433 -8.3961992 -8.4041119 -7.8514953 -6.9881563 -6.4365473 -5.7366037 -5.4595475 -5.2589183 -6.0857835 -6.5188112 -7.1243734 -7.6830239][-6.9819388 -6.593667 -6.7504053 -7.0309067 -7.0659318 -6.9845204 -6.2578554 -5.9039135 -5.0931611 -4.9477081 -4.4263496 -4.5842738 -5.5396366 -6.217207 -6.6200833][-5.6903839 -5.6199169 -5.2353783 -5.0932283 -4.5984278 -4.36426 -3.588799 -3.5894833 -2.8807111 -2.7276044 -2.8099279 -3.2520537 -4.0245714 -4.7556248 -5.7258296][-5.4476714 -4.8328829 -4.0453539 -2.8323116 -2.2215848 -2.014668 -1.5397501 -1.1513491 -0.84984159 -0.98285818 -1.7863512 -3.2316632 -3.9422934 -4.5647678 -5.0680232][-3.9005151 -3.0914698 -1.7988009 -0.83753157 -0.30100298 0.34353256 0.98658085 0.87601662 0.97813034 0.0519166 -1.043376 -2.9737825 -3.9140635 -4.8744297 -5.5794115][-3.4109564 -2.285089 -0.97445488 -0.020636082 0.43498707 1.0358353 1.4780483 1.4529066 1.3170357 0.35417271 -0.78846073 -2.6071115 -3.9998643 -5.2862062 -5.9276381][-2.664721 -1.6301517 -0.28752804 0.37050056 0.6005106 1.2929001 1.8396521 1.8793135 1.9785204 0.94874096 -0.96210241 -2.9988165 -4.5807376 -5.9452562 -6.511919][-2.3495693 -1.3875055 -0.45202541 0.780035 1.3463192 2.0885525 2.6496105 2.7595272 2.8132906 1.8703766 -0.11223698 -2.567915 -3.9667578 -5.3976059 -5.946166][-3.3318696 -2.0085907 -0.73438072 0.25648022 0.79445267 1.231472 1.4377241 1.9365854 2.4107027 1.4897575 -0.13802624 -2.5786443 -4.1348634 -5.4251175 -5.729651][-4.0600657 -3.0267367 -2.1403856 -1.59371 -0.89547396 -0.28190279 0.15329742 0.37943745 0.25698376 -0.9278388 -2.0540962 -4.0256677 -5.17097 -6.2162662 -6.7374549][-4.9056244 -4.1646852 -3.6075506 -2.644558 -2.0398827 -1.6773791 -1.1721425 -1.3537459 -1.7595711 -2.831337 -3.6322169 -4.7459812 -5.393178 -6.1913662 -6.4616327][-6.3878183 -5.9612579 -5.3116503 -4.4621325 -3.9920976 -3.7009752 -3.3436518 -3.1683364 -3.1410809 -4.2418237 -5.1255436 -5.649334 -5.9623113 -6.2656169 -6.13556][-7.1657009 -6.8621163 -6.6283927 -5.9340649 -5.256813 -4.8312278 -4.4944334 -4.6247396 -4.6942673 -5.1695995 -5.9921441 -6.0672045 -5.6259332 -5.678627 -5.5397396][-7.5241456 -7.193306 -7.007618 -6.3533983 -5.5997229 -5.003068 -4.7881784 -4.9158454 -4.7334328 -5.0267963 -5.0417 -5.0030484 -4.4106445 -4.3105841 -3.7880161][-7.9372053 -7.7219224 -7.1051388 -6.6753817 -6.1050787 -5.3544278 -5.194273 -5.209096 -4.7841673 -4.597682 -4.3253231 -4.2260008 -3.9855552 -4.10143 -4.0922031]]...]
INFO - root - 2017-12-15 15:16:38.732152: step 27910, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 53h:03m:26s remains)
INFO - root - 2017-12-15 15:16:45.100044: step 27920, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 53h:25m:06s remains)
INFO - root - 2017-12-15 15:16:51.435493: step 27930, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 53h:44m:11s remains)
INFO - root - 2017-12-15 15:16:57.834740: step 27940, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 53h:10m:07s remains)
INFO - root - 2017-12-15 15:17:04.300020: step 27950, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 53h:13m:51s remains)
INFO - root - 2017-12-15 15:17:10.661993: step 27960, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 54h:16m:47s remains)
INFO - root - 2017-12-15 15:17:16.942394: step 27970, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 53h:57m:25s remains)
INFO - root - 2017-12-15 15:17:23.393594: step 27980, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 55h:17m:53s remains)
INFO - root - 2017-12-15 15:17:29.773821: step 27990, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 53h:57m:36s remains)
INFO - root - 2017-12-15 15:17:36.071126: step 28000, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 52h:55m:58s remains)
2017-12-15 15:17:36.570261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8149233 -5.5016527 -5.3672991 -4.9971542 -4.8514462 -4.2983036 -3.8798571 -3.7531934 -3.32581 -3.6682854 -3.2980614 -4.0583925 -3.7781196 -4.5228863 -6.369657][-4.7993293 -5.1336107 -5.1893253 -5.3669062 -5.5398111 -5.0950518 -5.0059614 -5.12699 -5.1912189 -4.8348408 -4.0609179 -4.9140983 -4.4409933 -4.7264471 -6.118885][-4.1132755 -4.3597307 -4.180562 -3.6689668 -3.8185956 -4.1083269 -4.4246111 -4.8624115 -4.619503 -4.5463934 -4.2566748 -4.82916 -4.2347507 -5.2089667 -6.5997868][-4.7959485 -4.401948 -3.8619268 -2.6066356 -2.3561215 -1.9289598 -2.0760937 -2.87746 -3.5048232 -3.8424137 -3.2763414 -4.0998397 -4.417387 -5.0041618 -5.9159603][-4.0943642 -4.1816921 -3.7530694 -2.7912245 -1.1724901 -0.19100666 0.25002241 -0.49309778 -0.8516922 -1.6715913 -2.4727545 -3.5954876 -3.5254364 -4.6024323 -6.1508913][-4.1145945 -4.0265217 -4.033371 -2.7529006 -1.6293454 -0.3181057 0.6948719 0.85572624 0.70404434 -0.011289597 -0.42141104 -1.931838 -2.7710009 -4.2816744 -5.842556][-3.9789889 -3.5230088 -2.9039607 -1.9799504 -1.1799626 0.0910306 1.0133419 0.92261219 0.879405 0.59385586 0.20079327 -1.4609451 -2.4946337 -4.3760853 -5.5031071][-2.2245069 -2.190382 -1.8852196 -0.76757193 -0.0965271 0.41646385 0.96181583 0.30447388 0.14503956 0.0083146095 0.14541721 -1.3284841 -2.2392716 -3.7615869 -5.7431073][-2.0101123 -1.5703783 -0.16966677 0.69510269 0.74044323 0.87607384 1.0793781 0.32809162 -0.21558189 -0.50057507 -0.53776455 -2.2101831 -3.4980936 -4.9271669 -6.2842569][-3.4155989 -2.8156676 -1.8327146 -0.25824404 0.64710236 0.91919422 0.81285095 0.042137623 -0.26287842 -0.49068165 -1.2332268 -2.7679591 -3.6897354 -5.5386209 -7.6587563][-4.8863621 -4.6586065 -4.4012966 -2.7478819 -1.8227301 -1.2172556 -0.98835325 -1.2892928 -1.6110563 -1.4035468 -1.870461 -2.6678634 -3.1967492 -5.4933319 -7.3607488][-6.6362414 -6.3630366 -6.4974365 -5.3585224 -4.5196609 -4.1333408 -3.3113947 -2.7177472 -2.1128774 -2.3308396 -2.7446055 -3.3321018 -3.5628672 -5.2264786 -6.8888569][-8.4509163 -8.65929 -8.50637 -7.7841353 -6.9359908 -5.9858274 -5.3144245 -4.97649 -4.3644924 -3.8981984 -3.4871526 -3.6742806 -4.1455278 -5.6452589 -6.6191511][-8.37258 -8.8183 -8.568265 -7.7179041 -7.4354711 -7.0374937 -6.0817146 -5.8571157 -5.6457453 -5.4132481 -4.955442 -4.5482445 -4.7973604 -5.4043016 -6.1337233][-8.0270109 -8.402895 -9.1051521 -8.7131548 -7.8535938 -7.5253854 -7.5992727 -7.5528154 -7.1141806 -6.6110334 -6.0983148 -5.6412945 -5.6272964 -5.505497 -5.7067914]]...]
INFO - root - 2017-12-15 15:17:43.018416: step 28010, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 55h:22m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 15:17:49.364779: step 28020, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.610 sec/batch; 51h:35m:07s remains)
INFO - root - 2017-12-15 15:17:55.786749: step 28030, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 55h:42m:41s remains)
INFO - root - 2017-12-15 15:18:02.165015: step 28040, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 53h:14m:22s remains)
INFO - root - 2017-12-15 15:18:08.501915: step 28050, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 55h:27m:24s remains)
INFO - root - 2017-12-15 15:18:14.803944: step 28060, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 53h:19m:42s remains)
INFO - root - 2017-12-15 15:18:21.190031: step 28070, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.643 sec/batch; 54h:20m:08s remains)
INFO - root - 2017-12-15 15:18:27.567980: step 28080, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 53h:36m:17s remains)
INFO - root - 2017-12-15 15:18:33.913115: step 28090, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 52h:58m:48s remains)
INFO - root - 2017-12-15 15:18:40.286424: step 28100, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 54h:02m:19s remains)
2017-12-15 15:18:40.774214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8808055 -6.8848848 -6.8281136 -6.8115358 -6.5647106 -5.9130507 -5.1772547 -4.4401903 -4.0841365 -4.8854685 -5.1459827 -5.8996482 -6.16134 -7.2122393 -7.9520097][-7.55668 -7.848505 -7.7875571 -7.544198 -7.3891411 -7.1887422 -6.7668061 -6.0158796 -5.2335792 -5.5926094 -5.4905729 -6.1195154 -6.3753457 -7.1508279 -7.5102825][-7.9115787 -7.8219881 -7.6590204 -7.5100169 -7.3472934 -7.0089073 -6.72359 -6.3210239 -5.6424112 -6.1772852 -6.0072112 -6.2542892 -6.3328919 -7.1028571 -7.1233635][-7.5316105 -7.3434753 -7.03217 -6.2812662 -5.6621361 -5.0941272 -4.6237903 -4.5524879 -4.5250535 -5.4342651 -5.6918325 -6.5453477 -6.6270475 -7.2434511 -7.601048][-7.1302528 -6.2071404 -5.2682362 -4.5480285 -3.883821 -2.989728 -2.1404166 -1.9844832 -2.2780108 -3.6851583 -4.407268 -5.8238049 -6.5609932 -7.8665552 -8.2645111][-5.8334556 -4.94442 -4.3599057 -3.0480866 -1.8369727 -0.51488924 0.39855671 0.29004335 0.31510544 -1.1555262 -2.3027635 -4.1100807 -5.3797727 -7.0850682 -7.7317052][-4.7735353 -3.5876918 -2.2005429 -0.8920927 -0.020413876 1.2884245 2.2843933 2.1409626 1.9807625 0.26832151 -1.2529359 -3.1527705 -4.1561637 -6.2030373 -7.2960939][-4.3863497 -3.1716604 -1.560534 0.23521996 1.4030628 2.5805387 3.4388056 3.6373396 3.6906481 2.1587753 0.565917 -1.9337564 -3.3361254 -5.3864565 -6.5156031][-4.6350451 -3.3990154 -2.0319529 -0.3809433 0.77026844 1.92873 2.8350382 3.0311098 2.9625635 1.6700478 0.40525341 -1.9027338 -3.0213108 -4.8355126 -6.1812487][-4.6223354 -3.969147 -2.7236381 -1.2395821 -0.03704071 1.0002842 1.5106201 1.5231438 1.3083067 -0.23023081 -1.0860162 -2.7034121 -3.4428563 -4.5202975 -5.5392189][-4.9803619 -4.2256393 -3.3245177 -2.1084547 -0.96971178 -0.030939102 -0.033801079 -0.31273651 -0.594192 -1.7407746 -2.0705848 -3.0933876 -4.1051073 -4.4413571 -4.6584749][-5.9168539 -5.0381804 -4.02878 -3.1740713 -2.3433037 -1.4952793 -1.2075906 -1.4165506 -1.5270443 -2.6634679 -3.0492501 -3.1312733 -3.4131904 -4.2151833 -4.6760197][-6.4998136 -6.0960512 -5.2284875 -4.409934 -3.5919447 -2.8620148 -2.50138 -2.5571642 -2.9206309 -3.5069785 -3.5849953 -3.6416941 -3.900347 -4.1850138 -4.1608186][-6.3217325 -6.0429349 -5.5781069 -5.0767241 -4.5627193 -3.8928623 -3.498414 -3.4630046 -3.7624581 -4.2270241 -4.4997215 -4.354949 -3.9138243 -4.1962872 -4.232553][-6.6423397 -6.2125945 -5.9751906 -5.8801308 -5.566823 -5.178484 -4.8569574 -4.7247672 -5.0952735 -5.4396391 -5.5171165 -5.5147929 -5.6759214 -5.7683582 -5.7453089]]...]
INFO - root - 2017-12-15 15:18:47.154053: step 28110, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 55h:02m:11s remains)
INFO - root - 2017-12-15 15:18:53.544634: step 28120, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 54h:58m:36s remains)
INFO - root - 2017-12-15 15:18:59.926141: step 28130, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 54h:12m:06s remains)
INFO - root - 2017-12-15 15:19:06.237521: step 28140, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.637 sec/batch; 53h:53m:39s remains)
INFO - root - 2017-12-15 15:19:12.666422: step 28150, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 54h:13m:02s remains)
INFO - root - 2017-12-15 15:19:18.970759: step 28160, loss = 0.36, batch loss = 0.24 (12.6 examples/sec; 0.635 sec/batch; 53h:40m:01s remains)
INFO - root - 2017-12-15 15:19:25.420797: step 28170, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 53h:52m:24s remains)
INFO - root - 2017-12-15 15:19:31.769728: step 28180, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 55h:21m:04s remains)
INFO - root - 2017-12-15 15:19:38.122768: step 28190, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 54h:44m:17s remains)
INFO - root - 2017-12-15 15:19:44.470980: step 28200, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 54h:06m:00s remains)
2017-12-15 15:19:45.012889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9920034 -2.4323244 -2.2463107 -1.659461 -1.4716544 -1.816741 -2.3186831 -2.6947436 -2.951828 -4.8593197 -5.4636593 -6.72974 -7.5626464 -8.3485088 -9.3407745][-3.9729013 -3.4711695 -3.1335297 -2.6446362 -2.4635692 -2.5096612 -2.873878 -3.4578509 -3.5659814 -4.9985514 -5.4206514 -6.6556396 -7.2971416 -7.8608813 -8.75355][-5.5660686 -4.7736406 -4.5102777 -3.6493883 -3.0681891 -2.9317183 -2.8722672 -3.2498727 -3.2828259 -4.7892203 -5.4979153 -6.941637 -7.7332449 -8.3219795 -9.1155767][-6.7021551 -5.5502505 -4.7028036 -3.7286553 -2.8314986 -2.040134 -1.6330876 -2.0738049 -2.2544551 -3.8000932 -4.4757576 -6.0846076 -7.0288072 -7.5031629 -8.1437654][-7.79844 -6.3066578 -4.6438594 -3.4280763 -2.2140107 -1.3010507 -0.48628473 -0.78385544 -1.1368289 -3.0033741 -3.6736012 -5.5207262 -6.8308196 -7.3766694 -8.0183115][-7.9649067 -6.9864011 -5.3791132 -3.48423 -1.8962598 -0.48261595 0.78915119 0.2846837 -0.25238132 -2.2681179 -2.7510886 -4.3076825 -5.7228346 -6.5861645 -7.2131934][-8.0265217 -6.31151 -4.1006408 -2.3771257 -0.4514823 0.85597706 1.890666 2.1058531 1.6874619 -1.1049566 -1.7452021 -2.7896647 -3.9615278 -4.4717808 -4.8004103][-6.9092288 -5.5927591 -3.6088486 -1.153069 0.95121384 1.9361181 2.9042397 2.8778019 2.6300268 0.25984478 -0.47359943 -1.5947556 -2.6273847 -3.3991842 -3.965831][-8.18695 -6.9900708 -5.2955532 -2.9105725 -0.63822079 0.5957098 1.6997337 2.0745564 2.0576162 -0.12326956 -0.72533655 -1.6745076 -2.759047 -3.5463037 -3.782727][-10.946182 -9.8054867 -8.2785854 -6.0969815 -3.7842388 -2.2890415 -0.98876095 -0.40837955 -0.20170021 -2.0087571 -2.4592419 -2.8659563 -3.5786266 -3.813024 -3.764775][-12.868654 -11.853109 -11.033808 -9.0600805 -6.9121332 -5.1234226 -3.4033742 -3.2430477 -3.3228631 -4.5369577 -5.0522485 -5.2905197 -5.5016289 -5.5939655 -5.3563919][-13.183087 -12.29777 -11.8617 -10.155766 -8.5720015 -7.2214088 -6.047123 -6.0246415 -6.2362576 -7.0063143 -7.1500521 -6.9787946 -6.8276849 -6.7750316 -6.4906211][-13.557697 -12.644613 -12.315972 -11.634522 -10.675151 -9.5251122 -8.26445 -7.9465127 -7.7405972 -8.4080687 -8.38999 -7.5782022 -6.7510891 -6.2287979 -5.4640255][-11.669708 -11.054903 -10.631363 -9.9812222 -9.4902134 -8.7998161 -7.916471 -7.8196354 -7.8175793 -7.9036045 -7.6755939 -6.9767103 -6.2454 -5.741231 -4.8214364][-10.621604 -10.292665 -9.936799 -9.3372068 -8.5589247 -7.9867735 -7.5143695 -7.3743443 -7.4531751 -7.2051797 -6.7221189 -6.2499146 -5.76558 -5.4277773 -4.7917371]]...]
INFO - root - 2017-12-15 15:19:51.398328: step 28210, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 53h:34m:21s remains)
INFO - root - 2017-12-15 15:19:57.856812: step 28220, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 54h:45m:29s remains)
INFO - root - 2017-12-15 15:20:04.304537: step 28230, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 54h:37m:27s remains)
INFO - root - 2017-12-15 15:20:10.800805: step 28240, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:11m:55s remains)
INFO - root - 2017-12-15 15:20:17.145047: step 28250, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 52h:41m:42s remains)
INFO - root - 2017-12-15 15:20:23.498207: step 28260, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 54h:19m:20s remains)
INFO - root - 2017-12-15 15:20:29.932168: step 28270, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 55h:40m:19s remains)
INFO - root - 2017-12-15 15:20:36.296712: step 28280, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 53h:33m:33s remains)
INFO - root - 2017-12-15 15:20:42.678267: step 28290, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:30m:25s remains)
INFO - root - 2017-12-15 15:20:49.099767: step 28300, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 56h:37m:07s remains)
2017-12-15 15:20:49.650756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0286903 -6.0805674 -6.6840005 -6.2531667 -4.7592597 -4.12043 -3.2181892 -2.7376132 -2.8718877 -3.2823477 -4.5414476 -6.5460272 -7.5791974 -7.307261 -6.9801364][-3.3625078 -4.4455843 -6.5729265 -7.0800138 -6.3814735 -5.6424742 -4.7237253 -4.001668 -3.9700139 -4.3066158 -5.4158821 -6.9302144 -7.9998188 -8.4748917 -8.3216124][-3.4234576 -4.22052 -5.8874884 -6.1952295 -5.8603163 -5.770164 -4.7849197 -4.6641831 -4.0210085 -4.399683 -5.2610469 -6.5064955 -7.3618283 -7.85557 -8.3589983][-4.1000185 -4.1034346 -5.0519981 -4.5555153 -3.5632873 -3.1659679 -2.110877 -2.4171219 -2.680007 -3.5370545 -4.2365141 -5.6696815 -6.9514818 -7.4069257 -8.0063133][-4.8538847 -3.7521935 -3.6833181 -2.8620868 -1.655663 -0.46315241 0.5080719 -0.11837006 -0.74556494 -1.8201566 -2.8108673 -4.9238958 -6.5245552 -7.10716 -7.4516025][-4.9281006 -3.5652609 -2.7382598 -1.6104236 -0.4081521 0.93450737 2.0911245 1.2428961 0.26921225 -0.88521385 -1.9489717 -4.1256709 -5.7380438 -6.38274 -6.8178873][-4.857419 -3.6560397 -2.5814109 -1.0560226 0.55831146 1.9419098 2.9145279 2.38737 1.4216871 -0.41494226 -2.0747666 -4.1923022 -5.5669622 -6.3189936 -6.79549][-5.0829062 -3.8573492 -2.4063272 -0.6370759 0.97833538 2.5241575 3.2573843 2.8993177 2.2676325 -0.080600739 -2.5558257 -5.0086212 -6.2830229 -6.7454319 -7.2487674][-5.415597 -4.771606 -4.0207033 -2.0543394 -0.7077713 0.53246593 1.5955057 1.9595871 1.8990765 -0.31387758 -2.8999982 -5.7491269 -7.2300057 -7.3653555 -7.628293][-5.9777045 -5.8623519 -5.576592 -4.2193785 -2.8984332 -1.8816962 -1.01016 -0.23755407 0.20220232 -1.7944899 -3.513495 -5.8912244 -7.6005449 -8.3115339 -8.5935593][-8.1514988 -7.9581218 -7.771121 -6.794805 -5.6273689 -4.7739735 -4.0978842 -3.6641779 -3.438282 -4.9324169 -6.2054596 -7.4831686 -8.2682743 -8.8092184 -9.2306509][-8.9003143 -8.66666 -8.4998178 -7.7904887 -7.1194468 -6.4671173 -5.8232141 -5.5563517 -5.3205118 -6.6719313 -7.1403246 -7.7488127 -8.0440416 -8.4328394 -8.51285][-10.273655 -9.7732582 -9.596633 -8.8374348 -7.8697376 -7.3876328 -7.0004339 -7.0929694 -6.9790416 -7.7569704 -8.1607561 -7.854486 -7.4923844 -7.6249595 -7.3717842][-9.73259 -9.3911343 -9.1604958 -8.5589352 -7.7991185 -6.878902 -6.0437436 -6.5766249 -7.2400417 -7.7129383 -7.5124025 -7.2892866 -7.0294261 -6.9251266 -6.6428547][-9.3355713 -9.20421 -9.2106285 -9.0456476 -8.768651 -8.4561138 -7.4188385 -7.1009154 -7.3653183 -7.9114985 -7.5964847 -6.9080725 -6.5836654 -7.002358 -7.2824879]]...]
INFO - root - 2017-12-15 15:20:56.199880: step 28310, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 54h:22m:15s remains)
INFO - root - 2017-12-15 15:21:02.640563: step 28320, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 53h:04m:17s remains)
INFO - root - 2017-12-15 15:21:09.065963: step 28330, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 54h:26m:31s remains)
INFO - root - 2017-12-15 15:21:15.379141: step 28340, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 53h:16m:38s remains)
INFO - root - 2017-12-15 15:21:21.812472: step 28350, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 53h:01m:09s remains)
INFO - root - 2017-12-15 15:21:28.182084: step 28360, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 54h:56m:31s remains)
INFO - root - 2017-12-15 15:21:34.516377: step 28370, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 52h:04m:25s remains)
INFO - root - 2017-12-15 15:21:40.919110: step 28380, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 55h:47m:20s remains)
INFO - root - 2017-12-15 15:21:47.224109: step 28390, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 53h:12m:33s remains)
INFO - root - 2017-12-15 15:21:53.608025: step 28400, loss = 0.28, batch loss = 0.16 (13.1 examples/sec; 0.610 sec/batch; 51h:31m:44s remains)
2017-12-15 15:21:54.113690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.280745 -6.8677692 -6.451457 -6.0069904 -5.0520148 -3.7412038 -2.5595517 -1.8643851 -1.0436139 -2.241962 -3.2687621 -4.286377 -5.4455047 -6.2926512 -6.9551086][-6.1618094 -5.7715621 -5.3533382 -4.8209867 -4.04741 -3.1040373 -1.8366046 -1.2514548 -0.57262468 -1.8724427 -2.9413905 -3.8117852 -4.6007285 -5.7684226 -6.7667003][-6.2540488 -5.0118823 -4.3869305 -3.8397222 -2.7632694 -2.0572352 -1.1741114 -0.63830853 -0.34430933 -2.0291805 -2.9294481 -3.6407189 -4.64954 -5.7365403 -6.3467226][-7.8414741 -6.3548646 -5.2151556 -3.3551798 -1.3874803 -0.045534134 0.97701645 0.90302467 0.88131237 -1.0005565 -2.609551 -4.0580034 -5.3425045 -6.1492314 -6.3723664][-10.155075 -7.3770194 -4.952157 -3.2914371 -1.1268711 0.4761591 1.4185524 1.6986217 1.5043421 -0.85771656 -2.3970857 -4.2643108 -5.6629477 -6.7995806 -7.3113184][-10.78615 -8.369483 -5.6276064 -2.4137959 -0.24045277 1.1459179 2.6316242 2.5403156 2.2445593 -0.29338169 -2.0277195 -3.7007554 -5.3031349 -7.1456914 -7.8849678][-10.134417 -6.9897275 -3.617835 -1.1631989 0.72653008 2.1636086 3.3445349 3.6803379 3.3027592 0.61914635 -1.2361078 -3.1816387 -4.2646089 -6.1203837 -7.1580744][-8.6512508 -6.34501 -4.3295336 -1.0874853 1.5687847 2.7369795 3.3879328 3.2310247 3.2832575 1.1660872 -0.63694525 -2.7780681 -4.671761 -6.3831887 -7.2296991][-7.7890477 -5.7601948 -3.5908022 -1.4482861 0.40572262 1.3805094 1.840888 2.156661 2.400794 0.34369564 -1.1653447 -3.1943345 -4.4940052 -5.8872056 -6.879344][-8.6302662 -6.7091465 -5.0876632 -2.5345421 -0.22459221 0.772727 1.3117704 1.0114079 1.0649738 -0.78556728 -2.2319422 -3.8455186 -4.9045992 -5.7156572 -6.0930204][-9.576231 -8.8197155 -6.9692388 -4.922473 -3.0697327 -1.9515166 -1.6964202 -1.2569833 -1.1333652 -2.6643019 -4.14145 -5.3516464 -5.3267784 -6.1573095 -6.4720936][-11.268057 -9.9303389 -8.722126 -7.4315529 -5.2363272 -3.5158086 -2.4320421 -2.557672 -2.772265 -3.1795077 -3.8982158 -4.6567287 -5.1510468 -6.1143165 -6.7897487][-11.796045 -11.692235 -10.803862 -8.8365211 -7.0472183 -4.5261283 -2.5522046 -2.0377445 -1.8319921 -2.7608771 -3.5473495 -4.0205393 -4.4236984 -4.7347779 -5.2689943][-12.395184 -12.060437 -11.045427 -9.60714 -7.8922491 -5.8119555 -3.9812694 -3.2490654 -2.8370457 -2.496757 -2.6738982 -3.250782 -3.7287447 -4.6618385 -5.5318894][-11.952562 -12.248206 -11.536833 -10.398376 -9.2822762 -7.4932494 -5.64983 -4.7396665 -3.8660283 -3.2484822 -3.2769179 -3.650238 -3.9744227 -4.6040268 -5.2410603]]...]
INFO - root - 2017-12-15 15:22:00.506669: step 28410, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:51m:21s remains)
INFO - root - 2017-12-15 15:22:06.890301: step 28420, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 54h:17m:35s remains)
INFO - root - 2017-12-15 15:22:13.183301: step 28430, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 53h:15m:48s remains)
INFO - root - 2017-12-15 15:22:19.544850: step 28440, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.614 sec/batch; 51h:49m:10s remains)
INFO - root - 2017-12-15 15:22:25.919220: step 28450, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 53h:27m:51s remains)
INFO - root - 2017-12-15 15:22:32.361075: step 28460, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 56h:43m:06s remains)
INFO - root - 2017-12-15 15:22:38.728420: step 28470, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 53h:32m:34s remains)
INFO - root - 2017-12-15 15:22:45.078052: step 28480, loss = 0.23, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 53h:48m:24s remains)
INFO - root - 2017-12-15 15:22:51.419531: step 28490, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.629 sec/batch; 53h:05m:49s remains)
INFO - root - 2017-12-15 15:22:57.912053: step 28500, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 54h:55m:18s remains)
2017-12-15 15:22:58.412135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4178863 -4.9022 -6.2016039 -7.2412291 -7.5549388 -7.3620319 -6.6602292 -5.5297341 -4.70953 -4.6401954 -4.9665031 -6.2531404 -6.8821349 -7.0582385 -6.9634976][-3.5945845 -4.603199 -5.9890203 -6.8911715 -6.9463139 -6.2587938 -5.6230841 -5.1348753 -4.727396 -5.2389841 -5.9429307 -7.2243705 -7.653296 -7.5787024 -7.0374169][-3.3695731 -4.0770197 -5.0616112 -5.6832743 -5.5465708 -5.0318909 -4.7435279 -4.6437688 -4.5271177 -5.31454 -6.4581633 -7.8654685 -8.2534285 -8.4830656 -8.0458632][-3.0759959 -3.5082173 -3.7041256 -4.2613955 -3.9862456 -3.8295989 -3.4041648 -3.3833752 -3.6172228 -4.1284552 -5.1715174 -6.7866 -7.9757457 -8.4604836 -8.3362045][-3.6093855 -3.8299744 -3.7613726 -2.8713307 -2.1343741 -1.4291358 -1.3887587 -1.9527769 -2.7899671 -4.1118994 -5.2323174 -6.6840649 -7.3041348 -7.905581 -8.08899][-5.0988989 -4.9219213 -4.2515688 -2.641748 -1.2441769 0.059830189 0.67775059 -0.38701439 -1.9207664 -3.7138317 -5.5078282 -7.0233989 -7.4150348 -7.267643 -6.9355931][-4.5944843 -4.7902446 -4.0856457 -2.1776471 0.71410179 2.426405 3.2482634 2.6536207 1.1302099 -1.6421075 -4.2724595 -6.425139 -6.778636 -6.8973603 -6.537735][-4.0167637 -3.682579 -3.2072158 -0.91068649 1.4445715 4.1307945 5.1325331 4.2065067 2.6952982 -0.088510513 -2.3934669 -5.2614517 -6.0260248 -6.132988 -6.0604577][-4.9429212 -3.8069019 -2.9465942 -1.6942534 0.3969593 2.3351336 3.5471573 3.0977125 1.9246178 -0.6593132 -3.033072 -5.2655196 -6.0720434 -6.4206686 -6.3891478][-6.1508908 -5.7324858 -4.5645618 -2.8066726 -1.2692952 0.12820816 1.0011606 1.2573252 0.87073135 -1.8551064 -4.7758012 -7.0389352 -7.1832166 -7.0058312 -6.9589043][-7.3691406 -7.0495276 -6.4920473 -5.3164558 -3.8972745 -2.2788849 -1.9265027 -1.5019832 -1.4570661 -3.1663084 -5.240468 -7.313508 -7.8919249 -7.9233694 -7.0900741][-7.4782991 -7.5098424 -7.2067132 -6.5274959 -5.9429479 -5.0608678 -4.1902795 -4.290235 -4.4032679 -4.8683481 -5.6261382 -7.1666484 -7.1145921 -7.9312034 -7.6293325][-8.4856539 -7.3857269 -6.6636381 -6.3961196 -5.9466987 -5.9533844 -6.1390367 -5.9874997 -5.6690345 -6.2093945 -6.454524 -7.3501439 -7.963326 -8.26755 -8.1483536][-8.1193447 -8.068203 -6.9000549 -5.8754864 -6.1954293 -6.2220774 -6.7985272 -7.3451309 -7.8238344 -7.7929239 -7.357338 -7.2183275 -7.3060818 -7.7404547 -7.7181683][-9.2344036 -9.2316341 -8.87913 -8.2912 -7.2548957 -7.0662761 -7.0976887 -7.5915675 -7.8882661 -7.9856539 -7.9312506 -7.654315 -6.9898663 -6.679801 -6.9568357]]...]
INFO - root - 2017-12-15 15:23:04.782462: step 28510, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 54h:30m:22s remains)
INFO - root - 2017-12-15 15:23:11.242929: step 28520, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 54h:20m:39s remains)
INFO - root - 2017-12-15 15:23:17.569738: step 28530, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 52h:24m:57s remains)
INFO - root - 2017-12-15 15:23:23.926375: step 28540, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 54h:00m:03s remains)
INFO - root - 2017-12-15 15:23:30.280679: step 28550, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:53m:11s remains)
INFO - root - 2017-12-15 15:23:36.633396: step 28560, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 55h:15m:56s remains)
INFO - root - 2017-12-15 15:23:43.088278: step 28570, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 53h:15m:15s remains)
INFO - root - 2017-12-15 15:23:49.482406: step 28580, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 52h:58m:51s remains)
INFO - root - 2017-12-15 15:23:55.813366: step 28590, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 53h:57m:22s remains)
INFO - root - 2017-12-15 15:24:02.153131: step 28600, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:26m:54s remains)
2017-12-15 15:24:02.672192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2637444 -1.8230505 -2.3810587 -2.8720264 -3.3875613 -3.7167909 -3.8125982 -3.5628309 -3.2540097 -4.2956109 -4.7171826 -5.6933508 -6.0664539 -6.59077 -6.6869388][-1.8802862 -2.3179736 -2.8715644 -3.42596 -3.9605014 -4.1216812 -4.24941 -4.4511366 -4.5604057 -5.6066551 -5.8467321 -6.7428913 -7.1193829 -7.6353574 -7.6973767][-2.2121563 -2.3122034 -2.4917169 -2.6753135 -2.8656855 -2.9939237 -3.0783138 -3.2883725 -3.6757283 -4.828033 -4.9593887 -5.9192343 -6.2409096 -6.7951207 -7.0228019][-2.1861181 -2.1730738 -2.0688467 -1.9203362 -2.0139427 -1.9566255 -1.9903445 -2.1642041 -2.3964725 -3.879513 -4.4539542 -5.51914 -6.0252814 -6.6748853 -6.7727776][-2.5684733 -2.4702225 -2.3146896 -1.8215489 -1.4058719 -0.87119436 -0.39853907 -0.58553791 -0.78411818 -2.3217306 -3.2606554 -4.7090645 -5.4125633 -6.2982655 -6.5705814][-3.4978671 -3.0234032 -2.581255 -1.7459626 -0.96035242 -0.18549156 0.37008 0.52428722 0.5498209 -0.94369888 -1.8053651 -3.2487202 -3.8742797 -4.948668 -5.5464296][-3.5050263 -3.0234513 -2.555346 -1.5268908 -0.59168005 0.31891346 1.0743923 1.4805794 1.7099648 0.27418137 -0.54374409 -2.1155419 -2.8447852 -4.0419521 -4.9712362][-3.165648 -2.7342949 -2.2779012 -1.2141495 -0.12574434 0.86424923 1.6079397 2.1383038 2.6222191 1.2911472 0.40809345 -1.1415048 -1.9834685 -3.4244375 -4.24073][-3.3619142 -2.4945655 -2.0753484 -1.2911577 -0.47257996 0.5343256 1.2226143 1.6764584 2.0611238 0.9380703 0.31198978 -1.2206321 -2.103579 -3.54214 -4.2980614][-3.6063886 -2.8718534 -2.3162775 -1.5175958 -0.97502136 -0.26770782 0.26346874 0.76041889 1.172121 -0.10868692 -0.77291489 -2.2821717 -3.1157098 -4.1703458 -4.81144][-4.7746315 -4.3282638 -3.7892766 -3.0051751 -2.2981591 -1.8535047 -1.6467867 -1.2878828 -0.92586327 -2.1686678 -2.9698195 -4.073174 -4.7543879 -5.5549326 -5.8647122][-5.2616329 -4.9580326 -4.65342 -4.0325618 -3.5712228 -3.0918536 -2.9454923 -3.0558858 -3.0737047 -3.8007395 -4.372396 -5.4622297 -5.8692117 -6.2652082 -6.24798][-6.0032544 -5.797049 -5.6476488 -5.2089491 -4.7931995 -4.50131 -4.5117822 -4.3710346 -4.2971916 -5.0530419 -5.4978294 -6.2757912 -6.550303 -6.5967436 -6.0583792][-6.1038809 -5.8100424 -5.5531263 -5.146369 -4.6804295 -4.5961123 -4.60456 -4.7158089 -4.8794703 -5.5368037 -5.836906 -5.9810209 -5.9836969 -5.9813313 -5.62039][-7.1911812 -6.7963343 -6.3307295 -5.903192 -5.6253614 -5.6602535 -5.732923 -5.8146219 -5.9166203 -6.1981897 -6.4870648 -6.6225996 -6.6648946 -6.3138227 -5.94663]]...]
INFO - root - 2017-12-15 15:24:09.005697: step 28610, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 52h:28m:24s remains)
INFO - root - 2017-12-15 15:24:15.384579: step 28620, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 53h:23m:35s remains)
INFO - root - 2017-12-15 15:24:21.739780: step 28630, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 54h:21m:02s remains)
INFO - root - 2017-12-15 15:24:28.043905: step 28640, loss = 0.24, batch loss = 0.12 (13.0 examples/sec; 0.615 sec/batch; 51h:54m:59s remains)
INFO - root - 2017-12-15 15:24:34.408450: step 28650, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 53h:36m:36s remains)
INFO - root - 2017-12-15 15:24:40.843227: step 28660, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 53h:26m:35s remains)
INFO - root - 2017-12-15 15:24:47.213655: step 28670, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 52h:58m:36s remains)
INFO - root - 2017-12-15 15:24:53.609531: step 28680, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 55h:23m:47s remains)
INFO - root - 2017-12-15 15:25:00.037461: step 28690, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:52m:41s remains)
INFO - root - 2017-12-15 15:25:06.355162: step 28700, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 53h:20m:49s remains)
2017-12-15 15:25:06.849153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1790638 -3.8868873 -4.0100284 -4.2547283 -4.3944616 -4.3717184 -4.3971004 -4.4704866 -4.0198145 -4.0311346 -4.6864166 -4.5023279 -4.8529115 -5.2613935 -5.6740775][-4.2097826 -3.8650684 -3.58534 -3.8077719 -4.1569357 -4.1867442 -4.3864841 -4.3786707 -4.073895 -4.4104357 -5.1655369 -4.9123478 -4.925313 -5.2493443 -5.3820548][-4.9818344 -4.5225019 -4.2790051 -4.09074 -4.2305484 -4.1811819 -4.1548762 -4.3718557 -4.3081212 -4.6418905 -5.9987316 -6.1816745 -6.0945988 -6.2984352 -6.3357906][-5.2942791 -4.8511739 -4.5369196 -3.8333223 -3.5486188 -3.2117772 -2.8593597 -2.8203654 -2.7933435 -3.4047794 -4.8559332 -5.3983583 -6.0799565 -6.4904094 -6.6519213][-6.2938485 -5.3241472 -4.351388 -3.5246472 -2.76649 -2.2556391 -1.8182125 -1.5577874 -1.2691679 -1.8349795 -3.2205863 -3.8753383 -5.1865153 -6.4900064 -6.8686118][-6.4173269 -5.3728943 -4.1757298 -2.9354777 -1.56774 -0.51590586 0.049553871 0.09905386 0.3070879 -0.3293395 -1.796236 -2.3524399 -3.4721646 -4.8127108 -5.7373319][-5.8783569 -4.7334471 -3.3298731 -1.9838614 -0.6236248 0.58946323 1.33039 1.480094 1.5658321 0.9464283 -0.62093878 -1.3854737 -2.3827138 -3.2772427 -4.4728165][-4.750608 -3.7762988 -2.407342 -1.242136 -0.13136101 0.65923119 1.4038687 1.7031546 1.9348297 1.2820797 -0.56169462 -1.3861241 -2.5512052 -3.4917865 -4.4290557][-4.7277327 -3.4530997 -2.2048264 -1.2502098 -0.379663 0.23426104 0.593771 0.74150944 1.1758518 0.47170639 -1.3708487 -2.179121 -3.458786 -4.6047993 -5.4539833][-5.4497614 -4.5776992 -3.3924022 -1.9311624 -0.70723248 -0.34640932 -0.47067785 -0.40162277 -0.19547367 -0.85494518 -2.1824775 -2.605278 -3.6410689 -4.6453671 -5.7935753][-6.5725713 -6.3287683 -5.7545962 -4.4519653 -3.10466 -2.3766551 -1.9548211 -2.0301385 -2.2703867 -2.8296061 -4.0484514 -4.2134638 -4.355628 -4.9852457 -6.216433][-7.7415338 -7.3468366 -6.7489285 -6.0687141 -5.1147079 -4.2238646 -3.5362425 -3.4965181 -3.1500092 -3.4620275 -4.6530147 -4.7887225 -4.9903 -5.2103071 -5.5777326][-8.6930637 -8.3453627 -7.7970548 -6.9896622 -6.3063121 -5.5240006 -4.6480107 -4.4424853 -4.2673512 -4.3399324 -4.9699197 -4.823966 -4.8375912 -4.9932566 -5.0924315][-8.4877157 -8.4365664 -7.9983473 -7.2711229 -6.463254 -5.6913571 -5.0654058 -4.8157339 -4.6585665 -4.7336521 -5.1018949 -4.9576883 -4.6595936 -4.292572 -4.414103][-8.4354782 -8.0658035 -7.7409458 -7.4037251 -6.8206234 -6.2946172 -5.8410277 -5.8231421 -5.7011471 -5.6502562 -5.6973219 -5.5430093 -5.0011454 -4.6785336 -4.3953056]]...]
INFO - root - 2017-12-15 15:25:13.319887: step 28710, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 52h:56m:29s remains)
INFO - root - 2017-12-15 15:25:19.660778: step 28720, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 53h:10m:35s remains)
INFO - root - 2017-12-15 15:25:26.030152: step 28730, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 54h:35m:57s remains)
INFO - root - 2017-12-15 15:25:32.554374: step 28740, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 55h:27m:09s remains)
INFO - root - 2017-12-15 15:25:39.099353: step 28750, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 54h:03m:58s remains)
INFO - root - 2017-12-15 15:25:45.565278: step 28760, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 55h:11m:24s remains)
INFO - root - 2017-12-15 15:25:51.996362: step 28770, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 54h:17m:43s remains)
INFO - root - 2017-12-15 15:25:58.459862: step 28780, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 55h:20m:15s remains)
INFO - root - 2017-12-15 15:26:05.009231: step 28790, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 54h:44m:22s remains)
INFO - root - 2017-12-15 15:26:11.480783: step 28800, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:23m:53s remains)
2017-12-15 15:26:11.968573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.66838 -5.2388926 -4.4710941 -3.7889457 -3.221127 -2.8404646 -2.366888 -2.0444942 -1.7770529 -3.3071017 -3.8227355 -5.221118 -6.18339 -6.6902657 -7.3770695][-4.4071541 -4.6120319 -4.2502947 -3.9065089 -3.4532628 -2.9227824 -2.4904647 -2.2796726 -1.9850845 -3.3292489 -3.8375924 -5.630969 -6.6539474 -7.2353783 -8.1261606][-4.43247 -4.0834808 -3.8510516 -3.4452529 -2.8646755 -2.4055758 -2.0956273 -2.1524138 -2.1924491 -3.79024 -4.3011618 -5.5348296 -6.3726315 -6.94652 -7.6463804][-4.2826242 -4.1946025 -3.6137452 -2.8856916 -2.1907072 -1.7604356 -1.2678504 -0.93480873 -1.0122495 -2.718617 -3.6322217 -5.1280212 -6.190248 -6.7992482 -7.5926924][-5.1311846 -5.0925236 -4.2413721 -3.1673055 -1.7727098 -0.46326923 0.56901836 0.82965755 0.84222317 -0.946053 -2.0853658 -3.9381082 -5.2375135 -6.1074133 -7.2707195][-6.3316722 -5.834095 -4.9102039 -3.2589746 -1.0988407 0.867466 2.5224123 2.909214 2.9439535 1.0209703 -0.40712595 -2.3752275 -3.9347641 -5.0092621 -6.3247962][-5.9141316 -5.1254811 -4.0124116 -2.14217 0.15434933 1.9461155 3.6977282 4.234355 4.3581123 2.5573273 1.2818508 -0.8290019 -2.7544441 -4.0853624 -5.6356525][-3.94432 -3.3531475 -2.5065298 -0.88313484 1.3358278 3.3995781 5.0571012 5.3381357 5.3947334 3.483285 2.0257959 -0.073027134 -2.0303416 -3.4025197 -4.6717577][-4.2726097 -3.5550613 -2.8220558 -1.5073142 0.27949476 2.0663061 4.000042 4.6216364 4.8448553 2.9340477 1.5750694 -0.44399405 -2.2207608 -3.3166356 -4.4775066][-3.6796188 -3.4648623 -3.3124018 -2.0847216 -0.56420708 0.61212349 1.8120031 2.1534271 2.4560776 0.88439369 0.05607748 -1.2131238 -2.5596218 -3.6629453 -4.5013952][-2.5517774 -2.516037 -2.6292305 -2.2354589 -1.3999119 -0.38154459 0.59715557 0.58220482 0.6615696 -0.85991096 -1.4921336 -2.7362447 -3.8700492 -4.213613 -4.835175][-2.4936581 -2.3355947 -2.5022612 -2.1945953 -1.8484349 -1.0710793 -0.61245441 -0.75095606 -0.65587091 -1.8066554 -2.4733834 -3.7450874 -4.6622562 -5.423872 -5.8287277][-3.1365914 -2.6412363 -2.8661952 -2.5506668 -2.4554291 -2.3063745 -1.8806143 -2.2742052 -2.3819952 -3.3694639 -3.6536961 -4.2952223 -5.2284355 -5.6135321 -6.3393097][-3.2816658 -3.135139 -3.1751623 -3.3006825 -3.6859078 -3.6838293 -3.9396667 -4.3280296 -3.9479315 -4.3959951 -4.6921473 -5.4792137 -5.7623506 -6.0124383 -6.8264689][-4.4422979 -3.9821026 -4.2742677 -4.8122511 -5.1422815 -5.3935585 -5.9047256 -6.2873673 -6.088737 -5.8047762 -5.7757463 -5.7982531 -6.05324 -6.2668676 -6.4994287]]...]
INFO - root - 2017-12-15 15:26:18.408663: step 28810, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 53h:35m:39s remains)
INFO - root - 2017-12-15 15:26:24.853259: step 28820, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 55h:07m:03s remains)
INFO - root - 2017-12-15 15:26:31.255007: step 28830, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 55h:22m:31s remains)
INFO - root - 2017-12-15 15:26:37.627237: step 28840, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 53h:37m:00s remains)
INFO - root - 2017-12-15 15:26:43.988653: step 28850, loss = 0.35, batch loss = 0.24 (12.6 examples/sec; 0.632 sec/batch; 53h:20m:42s remains)
INFO - root - 2017-12-15 15:26:50.421730: step 28860, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 54h:27m:06s remains)
INFO - root - 2017-12-15 15:26:56.860268: step 28870, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 54h:20m:48s remains)
INFO - root - 2017-12-15 15:27:03.252129: step 28880, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 54h:19m:25s remains)
INFO - root - 2017-12-15 15:27:09.654184: step 28890, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 53h:06m:32s remains)
INFO - root - 2017-12-15 15:27:16.030810: step 28900, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 53h:55m:21s remains)
2017-12-15 15:27:16.570741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3166552 -3.5026464 -4.0912943 -4.6073647 -4.8341312 -4.7116303 -4.4724035 -4.1405077 -3.7655015 -3.9026384 -4.029273 -4.8850284 -5.8139963 -7.0546317 -8.2480316][-3.8335843 -4.0319929 -4.6184134 -5.0081306 -5.4064484 -5.5923138 -5.1392717 -4.9657288 -4.5314417 -4.7370071 -5.4254866 -6.4404397 -7.8245077 -8.6266232 -9.6888361][-3.8471611 -3.9297352 -4.1200624 -4.4285474 -4.5644426 -4.6664457 -4.9083638 -4.8766046 -3.9079845 -3.5205007 -3.7946997 -5.2827625 -7.14607 -8.3528957 -9.066308][-3.0252323 -3.1488295 -3.0413399 -3.2515411 -3.1452971 -2.8664227 -2.7633867 -2.6182518 -2.8698611 -3.0240731 -3.5631595 -4.8486395 -6.5825748 -7.9670277 -8.7004871][-2.2499294 -1.8727541 -1.4857602 -1.6862488 -1.575789 -1.2572703 -0.71211195 -0.38683319 -0.2014327 -0.60337067 -1.9027953 -3.8997648 -5.910924 -7.1285357 -8.0879049][-1.7864256 -1.5225797 -1.2282634 -0.5307579 0.13728189 0.84562492 1.6193285 1.9120579 1.9697399 1.0987339 -0.31355476 -2.4798055 -4.432498 -5.6794968 -6.5267472][-2.2438116 -1.569088 -0.91172171 -0.0037908554 1.0941219 1.9146862 2.6631966 3.2086535 3.5807943 2.8683443 0.92275047 -1.6165671 -4.0449805 -5.4150534 -6.3043375][-1.7128038 -1.387362 -0.90951252 -0.28090525 0.737627 2.204628 3.3026838 3.4858208 3.5870171 2.6502705 0.99374294 -1.4066963 -3.810462 -5.4656234 -6.5628161][-1.5975475 -1.5722575 -1.3253932 -0.41050911 0.66682243 1.7885246 2.6759892 2.9867907 2.8342924 1.9152336 0.39538574 -1.9156027 -4.079124 -5.56306 -6.5812845][-2.5228767 -2.3462062 -2.1918073 -1.4613214 -0.86687565 0.36194897 1.2341213 1.6666012 1.7790289 0.47588539 -1.2186971 -3.1394324 -4.9661112 -6.3133254 -7.1896496][-4.8318548 -4.6238108 -4.36098 -3.6657066 -3.0011926 -1.9577589 -1.3655419 -0.95778465 -0.77454853 -1.3475409 -2.4153614 -4.308445 -5.5844984 -6.6574736 -7.4610219][-4.6309462 -4.3213353 -3.9604063 -3.5616717 -3.2214513 -2.7591238 -2.3799305 -2.158546 -2.1013355 -2.966949 -3.8392785 -5.1512146 -5.9808397 -6.66053 -7.0138326][-4.972662 -4.9918032 -5.0092268 -4.9294224 -4.5987663 -4.237277 -3.9621499 -3.9840834 -3.9952109 -4.5095987 -4.8043313 -5.7417922 -6.2027683 -6.5101709 -6.9655666][-4.3077707 -4.0459824 -3.9476602 -4.0170379 -4.1163697 -4.0080843 -3.8344991 -3.8469396 -3.9393632 -4.4904022 -4.7054739 -5.2635756 -5.6938343 -6.0249805 -6.4564061][-5.6849613 -5.5630217 -5.6235886 -5.4875727 -5.5796022 -5.5268536 -5.4188108 -5.2251215 -5.1101174 -5.245687 -5.2660036 -5.334712 -5.4035716 -5.5070133 -5.9028196]]...]
INFO - root - 2017-12-15 15:27:23.134848: step 28910, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 56h:30m:10s remains)
INFO - root - 2017-12-15 15:27:29.639361: step 28920, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 55h:27m:51s remains)
INFO - root - 2017-12-15 15:27:36.087678: step 28930, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 53h:29m:41s remains)
INFO - root - 2017-12-15 15:27:42.511831: step 28940, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 52h:53m:36s remains)
INFO - root - 2017-12-15 15:27:48.950757: step 28950, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 53h:31m:22s remains)
INFO - root - 2017-12-15 15:27:55.468831: step 28960, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 54h:24m:46s remains)
INFO - root - 2017-12-15 15:28:01.863163: step 28970, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 52h:55m:48s remains)
INFO - root - 2017-12-15 15:28:08.307860: step 28980, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 55h:09m:05s remains)
INFO - root - 2017-12-15 15:28:14.729632: step 28990, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 54h:06m:27s remains)
INFO - root - 2017-12-15 15:28:21.105029: step 29000, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 53h:37m:03s remains)
2017-12-15 15:28:21.612499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1306944 -5.4896579 -5.1466522 -4.7995262 -5.1186352 -4.8915224 -4.531631 -3.7699583 -3.003479 -3.2744493 -3.7622745 -5.3096471 -6.4824615 -7.9659524 -8.8343544][-5.8725019 -5.6768923 -5.1897221 -4.8766928 -4.8728828 -5.0056925 -5.1968079 -4.1963806 -3.5034971 -3.4654121 -3.962286 -5.4226274 -6.3213792 -7.3818455 -7.4002342][-5.0595317 -5.1097126 -4.6239214 -4.8971157 -4.89381 -4.658884 -4.7340984 -4.2898817 -3.8156452 -3.660346 -4.0488682 -5.3296528 -6.2469535 -7.0256615 -7.3611517][-4.1130958 -3.8699141 -4.1021433 -3.8447618 -3.7571201 -3.9111977 -3.4067988 -3.0284805 -2.4823012 -2.614749 -3.6018505 -5.0989552 -6.2269983 -6.7308583 -6.838922][-4.4184837 -3.4563684 -3.2749038 -2.7787189 -2.4954066 -2.0502028 -1.341157 -1.5517159 -1.0116611 -1.5885448 -2.5282931 -4.1523209 -6.0327935 -7.0873318 -7.3081069][-2.094451 -1.8436542 -1.9215574 -1.2130165 -0.63885069 -0.12549973 0.852005 1.0299644 1.1995296 0.25772429 -1.1652155 -3.1199069 -4.6691132 -5.9890537 -6.524106][-2.141047 -1.0820928 -0.41031075 -0.16955566 0.68672943 1.8206615 2.4426823 2.2928057 2.1753254 0.98240185 -0.7704978 -2.9939041 -4.34103 -5.6586018 -6.2390885][-0.54058838 -0.7584796 0.017182827 0.34461212 0.66516113 1.5413704 2.0145226 2.1611891 2.2200813 0.5653944 -1.2966695 -3.3895144 -5.0122347 -6.1428509 -6.6279054][-0.42104578 -0.39935446 0.3648138 0.54787731 0.83730412 1.3444481 1.459939 1.3616066 1.1092863 0.01328516 -1.3524675 -3.6321597 -5.3329458 -6.3960638 -6.6957664][-1.8883438 -1.563139 -1.0640078 -0.68165731 -0.0841732 0.17867184 -0.13049507 -0.19284248 -0.38666248 -1.2268696 -2.2978253 -3.6856556 -5.0071754 -6.4723215 -7.007637][-3.8998685 -3.7515044 -3.7098815 -3.1809397 -2.6066265 -2.4499679 -2.1548157 -2.2783966 -2.7447581 -2.9793572 -3.4922695 -4.5189762 -5.5559244 -6.53141 -6.7269821][-6.5342808 -6.1141095 -5.8311987 -5.0485868 -4.6371603 -4.7047334 -4.1152792 -3.9800017 -3.9090602 -4.3244944 -5.2296972 -5.5609474 -6.263041 -7.0782046 -6.8213181][-7.9610767 -8.070632 -8.2329245 -7.21534 -6.3729072 -5.6641674 -5.1605225 -5.19693 -5.0499649 -4.9909253 -5.6261969 -5.4697371 -5.9694672 -6.722568 -6.6519623][-7.4472904 -7.5863476 -7.6518965 -7.080174 -6.8750997 -6.1327081 -5.9513388 -5.8318977 -5.3527837 -5.1864195 -5.2611065 -5.3965893 -5.5025291 -5.9003377 -6.300271][-7.809135 -7.4393315 -7.0026722 -6.8117743 -6.9710217 -6.2360415 -6.3825016 -6.3767581 -6.3085184 -6.2545233 -5.7431321 -5.7666612 -5.6640043 -6.0232968 -6.2214131]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 15:28:28.084404: step 29010, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 52h:13m:13s remains)
INFO - root - 2017-12-15 15:28:34.465780: step 29020, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 53h:14m:29s remains)
INFO - root - 2017-12-15 15:28:40.990960: step 29030, loss = 0.43, batch loss = 0.32 (12.4 examples/sec; 0.647 sec/batch; 54h:33m:18s remains)
INFO - root - 2017-12-15 15:28:47.395194: step 29040, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:49m:31s remains)
INFO - root - 2017-12-15 15:28:53.771668: step 29050, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 53h:09m:04s remains)
INFO - root - 2017-12-15 15:29:00.197717: step 29060, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:41m:16s remains)
INFO - root - 2017-12-15 15:29:06.662114: step 29070, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 55h:31m:00s remains)
INFO - root - 2017-12-15 15:29:13.040562: step 29080, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 53h:08m:22s remains)
INFO - root - 2017-12-15 15:29:19.392761: step 29090, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:02m:24s remains)
INFO - root - 2017-12-15 15:29:25.782066: step 29100, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 53h:06m:23s remains)
2017-12-15 15:29:26.305083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1206808 -5.3501682 -5.7813277 -5.4426842 -5.064889 -4.0271273 -3.3436823 -2.6209159 -2.341331 -3.703732 -3.8266869 -5.9555416 -7.0402403 -8.5867662 -9.099885][-4.7727127 -6.0455818 -7.2511749 -7.1248703 -6.259469 -4.512949 -3.2278633 -2.1535273 -2.2730675 -4.6660657 -5.9755211 -8.4719152 -9.7442646 -11.179318 -10.80079][-4.4699192 -5.14172 -5.9259586 -5.5049305 -4.8346615 -3.7224414 -2.7348256 -1.6578202 -0.76140547 -2.8810463 -4.7026067 -7.7361135 -9.4019451 -10.926521 -11.050686][-4.45933 -4.8732424 -5.2638297 -4.59622 -3.4706793 -1.6837821 -0.55736446 0.50148964 0.99358559 -0.86793089 -2.2805643 -6.1312022 -8.0796261 -9.8569126 -10.597445][-4.8923497 -4.1915264 -3.7593584 -3.1378741 -1.8184285 0.35513592 2.1283293 2.8377867 2.9595518 0.49637318 -1.1939144 -4.578301 -6.6344023 -8.4641113 -9.5813274][-4.8883467 -3.7601681 -2.7832179 -1.2701983 0.24949312 2.7703524 4.4797335 4.6838722 4.2895536 1.2086859 -0.6891036 -3.8717439 -5.6683159 -7.0986018 -7.8282814][-4.63081 -3.6155839 -2.3497009 -0.12091732 1.8873005 3.320056 5.0301704 5.2434053 4.62269 2.026062 -0.018726349 -3.5604582 -5.7040114 -7.3599033 -7.9985957][-4.24761 -3.416441 -2.1838298 1.0690908 3.0892181 4.8970118 6.5598764 6.1445389 5.20177 2.1080494 0.21676493 -3.4706345 -5.9511118 -7.6658149 -8.0493917][-4.7402387 -3.956193 -2.823781 -0.22966671 2.2528782 3.932312 5.37899 5.7592974 5.1690903 1.6069727 -0.72665977 -4.1010637 -6.3699293 -8.1754084 -8.3938808][-5.4141936 -5.2325606 -4.9841266 -2.8068466 -0.72171164 1.2883148 2.7523251 2.6688509 2.7675486 -0.53983736 -2.8210616 -5.424778 -7.2847452 -8.5655756 -8.4960365][-6.6432109 -6.7533059 -6.7760983 -5.7885633 -4.3923125 -2.2069082 -0.73349333 -0.31721973 -0.29700947 -3.3351455 -4.6232748 -7.1088047 -8.4266777 -9.4910851 -9.306653][-7.7521682 -7.8340511 -8.094882 -7.47022 -6.780302 -5.4120159 -4.0869546 -3.8325851 -3.7670014 -5.1973815 -5.8605833 -7.7583451 -8.8272247 -9.7834225 -9.7986546][-9.4667006 -9.1000671 -8.7137375 -8.6129284 -8.1796236 -7.4942384 -7.2506227 -6.8580647 -6.2035322 -6.7914176 -7.1400161 -8.23713 -8.3550835 -9.08091 -9.3714552][-9.9405842 -9.5628042 -9.2968073 -8.8344965 -7.8969746 -7.4995165 -6.8844004 -7.3893318 -7.7414207 -7.3347931 -6.9445519 -7.2817025 -7.2021232 -7.5556173 -6.9856539][-9.1403837 -9.3169279 -9.7937431 -9.7906065 -9.391695 -8.163414 -7.5919175 -7.9628644 -7.7614737 -8.0413837 -8.1514406 -7.7867179 -7.400537 -6.9451556 -6.2439222]]...]
INFO - root - 2017-12-15 15:29:32.751883: step 29110, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.671 sec/batch; 56h:33m:12s remains)
INFO - root - 2017-12-15 15:29:39.208346: step 29120, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 53h:11m:54s remains)
INFO - root - 2017-12-15 15:29:45.574042: step 29130, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 52h:45m:29s remains)
INFO - root - 2017-12-15 15:29:51.898251: step 29140, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 54h:21m:32s remains)
INFO - root - 2017-12-15 15:29:58.319636: step 29150, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 54h:22m:23s remains)
INFO - root - 2017-12-15 15:30:04.697206: step 29160, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 55h:00m:40s remains)
INFO - root - 2017-12-15 15:30:11.177204: step 29170, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 54h:36m:03s remains)
INFO - root - 2017-12-15 15:30:17.594339: step 29180, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 54h:57m:44s remains)
INFO - root - 2017-12-15 15:30:23.923749: step 29190, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 54h:58m:56s remains)
INFO - root - 2017-12-15 15:30:30.367193: step 29200, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:19m:19s remains)
2017-12-15 15:30:30.854538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1613622 -4.2164812 -3.9056089 -4.1052351 -4.3708658 -3.5266571 -2.8830147 -2.5333319 -2.44243 -3.604816 -4.4125333 -5.2221451 -6.6081915 -6.4033947 -7.1338177][-4.8778238 -4.6184149 -4.4813104 -3.8423007 -3.2265596 -3.1873789 -2.8384504 -2.7141681 -2.4658556 -3.1225615 -4.0002947 -5.1218276 -6.4428635 -6.4587808 -7.3167763][-5.2714624 -4.6852083 -4.5704432 -4.3048925 -3.7908053 -3.1156836 -2.2751527 -2.2977395 -2.0947881 -2.9903588 -3.8294435 -4.9024515 -6.398757 -6.5006003 -7.2948332][-4.463913 -4.5516996 -4.9305234 -4.0814896 -3.2071023 -2.3624396 -1.3677726 -1.2681985 -1.2364111 -2.4341555 -3.5171347 -4.8669186 -6.4151621 -6.5665603 -7.6160083][-4.7853222 -3.6957505 -2.3960261 -1.7604761 -1.2073789 -0.74876595 -0.27552319 0.13654327 0.44384193 -0.95981073 -2.3987589 -4.2093821 -5.9669995 -6.4056311 -7.5968542][-5.0759249 -4.2071161 -3.4261041 -1.7321439 0.13184166 1.4394913 2.2777004 2.3206606 2.0455484 0.44917965 -1.0982018 -3.0165019 -5.1269436 -5.6018095 -6.8473496][-3.8477111 -3.5333862 -2.0020685 -0.74503231 0.79159641 2.0098457 3.2848053 3.6416836 3.1722794 1.2624912 -0.61115503 -2.1452308 -4.2226739 -4.6882133 -6.4049931][-3.6204257 -2.4220967 -1.299747 0.11280966 1.9580879 3.269269 4.1878338 4.4494047 4.0355053 2.0481615 -0.11367989 -2.5594258 -4.5342779 -4.5257683 -5.5692668][-3.3392448 -2.3257937 -0.95339251 0.64066029 2.1669827 3.0837126 3.5656185 3.7036495 3.4776535 1.7128668 -0.29287338 -2.3661485 -4.7010474 -5.2636366 -6.3644762][-2.173255 -1.7493105 -1.2446561 -0.014574051 0.66567326 1.6119032 2.1102352 2.3095465 2.0884705 0.60718727 -1.2746491 -3.3189759 -5.2972479 -5.5914693 -6.5375614][-3.7379475 -3.64194 -2.2913146 -1.5194802 -0.70404577 -0.42631626 -0.21646976 -0.43390512 -0.24200916 -1.4920397 -3.0134325 -4.379735 -5.669085 -5.6482506 -6.5830069][-4.4814167 -4.5952835 -4.3231587 -3.8421123 -2.8407655 -2.5746131 -2.0490174 -2.3897638 -2.4792824 -3.7726285 -4.7787123 -5.5709038 -6.8437619 -6.8642039 -7.0164671][-3.0735936 -4.3783584 -5.3159838 -5.1218605 -4.5946383 -3.7639968 -3.1764989 -3.8280108 -4.1812882 -4.9801431 -5.9453006 -6.6979446 -7.4191127 -6.7865868 -7.1649604][-4.0901918 -3.9297395 -4.2074509 -4.4461975 -4.6745625 -4.8395281 -5.198823 -5.0905418 -5.0625019 -5.6849418 -6.2816496 -7.02107 -7.2468796 -7.5135479 -7.7699814][-5.3592272 -4.8968887 -4.855463 -5.300107 -5.7658563 -5.9518223 -5.8206735 -6.6368341 -7.4088745 -7.3281517 -7.1714511 -7.3973579 -7.3395267 -7.1595554 -7.0026574]]...]
INFO - root - 2017-12-15 15:30:37.271379: step 29210, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 53h:04m:08s remains)
INFO - root - 2017-12-15 15:30:43.691879: step 29220, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 53h:25m:46s remains)
INFO - root - 2017-12-15 15:30:50.021165: step 29230, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 54h:28m:52s remains)
INFO - root - 2017-12-15 15:30:56.457498: step 29240, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 54h:22m:34s remains)
INFO - root - 2017-12-15 15:31:02.840576: step 29250, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 54h:45m:59s remains)
INFO - root - 2017-12-15 15:31:09.264274: step 29260, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 52h:58m:54s remains)
INFO - root - 2017-12-15 15:31:15.689799: step 29270, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 54h:27m:08s remains)
INFO - root - 2017-12-15 15:31:22.060388: step 29280, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 54h:45m:10s remains)
INFO - root - 2017-12-15 15:31:28.479849: step 29290, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 52h:10m:31s remains)
INFO - root - 2017-12-15 15:31:34.802400: step 29300, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 54h:15m:14s remains)
2017-12-15 15:31:35.284800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8963909 -6.1691127 -6.2407026 -6.4235506 -6.5308027 -6.04568 -5.0537605 -4.162611 -3.1965756 -4.3892984 -5.2581906 -5.393858 -6.2024765 -7.3944964 -7.7693563][-5.8704233 -5.9539371 -6.0533466 -6.0272441 -6.5497022 -6.8396893 -6.3214989 -5.2034106 -4.0493469 -5.1920214 -5.8618288 -6.101182 -7.3482642 -8.7088194 -9.118495][-5.5908585 -5.4316807 -5.6218724 -5.7050123 -5.9587584 -5.9985051 -5.2457523 -4.7330227 -4.1172457 -5.5545931 -6.4815531 -7.0345631 -7.9172411 -9.1506119 -9.5322647][-6.1034985 -5.66063 -5.7401776 -5.2100196 -4.8765526 -4.5224447 -3.8376415 -3.3176475 -2.5222306 -4.0801992 -5.4775229 -6.1724696 -7.4855952 -8.6541328 -9.0733624][-6.7766652 -5.2832503 -3.9304311 -3.3341484 -3.027472 -1.9658809 -0.76147795 -0.093449116 0.60113239 -1.5965137 -3.5713615 -4.625526 -6.4106708 -8.1254034 -8.8035641][-7.3187709 -5.9950953 -4.5240126 -2.7434564 -1.0252023 1.0160418 2.7844934 3.1179638 3.6399059 1.1236696 -1.5197878 -3.2890658 -5.1274652 -7.1325712 -8.1674318][-7.1429243 -5.2012615 -2.7662392 -0.7464366 1.2851028 3.7158985 5.8453417 6.4041052 6.8308649 3.6892805 0.74095249 -1.3865633 -3.7267618 -5.5356951 -6.6818323][-6.7133865 -5.3840714 -3.233377 0.25909805 3.6082039 5.552928 7.1843538 8.4433794 8.8299055 5.25093 1.6641121 -0.99164057 -3.3358798 -4.9134955 -5.9805646][-6.7671013 -6.2066536 -4.7681475 -1.9811954 1.0818844 3.4755478 4.8991089 5.8822613 6.3226242 3.5943241 0.45805645 -1.9572105 -4.0016651 -5.6583848 -6.5052876][-8.2893934 -7.5021319 -6.2998528 -3.9708085 -1.2538972 0.8471508 2.2699013 2.5031242 2.0218182 -0.65987873 -2.8143134 -4.6605425 -6.2954311 -7.048131 -7.1895914][-10.550644 -10.001884 -8.8622055 -6.8657179 -5.2210431 -3.1633048 -1.2114935 -1.0689521 -1.507493 -4.2792435 -6.2448425 -6.8793726 -7.950561 -8.7941694 -8.6538286][-11.755156 -11.010448 -10.173895 -9.1335907 -7.9418955 -6.4287033 -5.2913704 -4.3401165 -3.9642637 -6.316061 -7.9456396 -7.6994257 -8.09744 -8.709507 -8.5784769][-10.572189 -10.639769 -10.547258 -9.4794283 -8.6366234 -7.679841 -6.8862896 -6.6298866 -6.4561872 -7.4620404 -8.2122688 -8.2126579 -8.66187 -8.6639061 -8.0304346][-9.7336235 -9.6566362 -9.2387123 -8.8628368 -8.4155827 -7.5473447 -7.3648205 -7.4163618 -7.4293218 -8.3361368 -8.6498451 -8.2636395 -8.2724962 -8.0612946 -7.4816542][-8.7131262 -8.6242027 -8.3945217 -8.045289 -7.9980879 -7.4287291 -6.996079 -7.5854855 -7.9202228 -7.7253318 -7.8114047 -7.9656949 -7.69482 -7.3033237 -6.8001456]]...]
INFO - root - 2017-12-15 15:31:41.683108: step 29310, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 55h:14m:47s remains)
INFO - root - 2017-12-15 15:31:48.120642: step 29320, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 55h:06m:46s remains)
INFO - root - 2017-12-15 15:31:54.501658: step 29330, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 54h:01m:38s remains)
INFO - root - 2017-12-15 15:32:00.887611: step 29340, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 53h:26m:52s remains)
INFO - root - 2017-12-15 15:32:07.392531: step 29350, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 55h:04m:34s remains)
INFO - root - 2017-12-15 15:32:13.912917: step 29360, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 54h:22m:18s remains)
2017-12-15 15:32:17.701396: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 13491850 get requests, put_count=13491857 evicted_count=9000 eviction_rate=0.000667069 and unsatisfied allocation rate=0.000667366
INFO - root - 2017-12-15 15:32:20.201653: step 29370, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 52h:43m:15s remains)
INFO - root - 2017-12-15 15:32:26.593019: step 29380, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 52h:58m:55s remains)
INFO - root - 2017-12-15 15:32:33.071181: step 29390, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 55h:05m:46s remains)
INFO - root - 2017-12-15 15:32:39.577235: step 29400, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 55h:12m:37s remains)
2017-12-15 15:32:40.089944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9396367 -3.1991658 -3.7100244 -3.6468315 -3.5886369 -3.5889688 -3.7547514 -3.7137876 -3.4063849 -4.7795782 -5.2541122 -6.1358833 -6.2387276 -6.7597528 -7.5737743][-2.6508074 -2.6207352 -3.107882 -3.4455557 -3.6071897 -3.6915669 -3.8859968 -3.8349118 -3.4881873 -4.8198032 -5.0673566 -6.0017405 -6.2489629 -6.4266157 -7.1938963][-2.1994934 -2.084806 -2.5462737 -2.5224719 -2.5812869 -2.8656611 -3.3299589 -3.2975516 -3.2525272 -4.8345585 -5.1924582 -5.8676209 -5.8030939 -5.9214392 -6.6955242][-2.4426737 -1.8525772 -1.8381386 -1.4536247 -1.4997821 -1.3436289 -1.5381045 -1.9275246 -2.2305527 -4.0396495 -4.7484479 -5.8970318 -5.8086181 -5.80336 -6.5925059][-2.9079804 -1.8122978 -1.0492029 -0.34204245 0.020807266 0.44396496 0.4039793 -0.099313259 -0.55517244 -2.9221463 -3.7787597 -5.1259441 -5.3260231 -5.5535617 -6.30655][-3.7191544 -2.5648904 -1.7921171 -0.57141018 0.14649439 1.3910055 1.9915714 1.3599234 0.88998318 -1.7801328 -3.0840869 -4.875452 -5.283699 -5.4708977 -6.2547421][-3.2227931 -2.4106317 -1.5637069 -0.078529358 0.96496582 2.1962118 3.0475349 2.8612261 2.4111481 -0.23169231 -1.9207416 -4.1047564 -4.7752552 -5.2063885 -5.9275656][-2.8687873 -1.9265285 -0.99091721 0.18761349 1.2874117 2.6386585 3.6724491 3.6959133 3.3911076 0.47855759 -1.4500585 -3.4048815 -3.9655149 -4.6829567 -5.6721168][-3.0695724 -2.3260422 -1.5765724 -0.50588942 0.30292511 1.2671242 2.14293 2.8212948 2.9386873 -0.092761517 -1.913455 -3.5535851 -4.1806545 -4.5862522 -5.818][-4.4498491 -3.61588 -2.7648921 -1.6325922 -0.7427516 -0.010826111 0.68991947 1.3304243 1.2310476 -1.6461606 -3.0804467 -4.2266254 -4.811337 -5.6018648 -6.7581115][-6.004055 -5.41487 -4.6903286 -3.516129 -2.7330956 -2.3080173 -1.8783827 -1.2037539 -0.92705965 -3.4182677 -4.6655703 -5.2238054 -5.7693663 -6.5722075 -7.8347783][-7.3725286 -6.8173232 -6.3492403 -5.5097418 -4.9039421 -4.3779707 -4.2167034 -4.0090332 -3.6956284 -4.9373388 -5.3620362 -5.6060696 -6.0698929 -7.2038708 -7.9377127][-8.3236475 -7.8013682 -7.3823566 -6.8221321 -6.3089733 -5.8716097 -5.7039261 -5.5971041 -5.2996254 -6.16057 -6.4026837 -6.3088388 -6.2263718 -6.6657076 -7.2459283][-8.3538923 -7.7889118 -7.4937077 -7.0925674 -7.0049853 -6.534667 -6.1847229 -6.2752662 -6.4009562 -6.3462477 -6.2500982 -6.2444663 -6.164063 -6.39618 -6.3402753][-8.6135054 -8.1208258 -7.6631675 -7.1198487 -7.0548434 -7.0900216 -6.9659472 -7.0072904 -6.77296 -6.9096041 -6.8352876 -6.4351211 -6.1192489 -5.9919949 -5.8373346]]...]
INFO - root - 2017-12-15 15:32:46.561364: step 29410, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 54h:25m:22s remains)
INFO - root - 2017-12-15 15:32:53.050340: step 29420, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.663 sec/batch; 55h:50m:00s remains)
INFO - root - 2017-12-15 15:32:59.435705: step 29430, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 53h:55m:09s remains)
INFO - root - 2017-12-15 15:33:05.926770: step 29440, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 54h:41m:38s remains)
INFO - root - 2017-12-15 15:33:12.308471: step 29450, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 51h:52m:09s remains)
INFO - root - 2017-12-15 15:33:18.686882: step 29460, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 53h:33m:05s remains)
INFO - root - 2017-12-15 15:33:25.035070: step 29470, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 51h:52m:04s remains)
INFO - root - 2017-12-15 15:33:31.380478: step 29480, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:39m:15s remains)
INFO - root - 2017-12-15 15:33:37.751158: step 29490, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 53h:40m:47s remains)
INFO - root - 2017-12-15 15:33:44.197105: step 29500, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 53h:04m:20s remains)
2017-12-15 15:33:44.745274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1670218 -3.288589 -3.0554485 -3.9037907 -4.5934367 -5.3413 -5.6908541 -5.7681565 -5.3244925 -7.161025 -7.48543 -8.257165 -9.1013718 -9.7763577 -8.8876791][-2.59874 -3.2840586 -3.8593369 -4.2016129 -4.601151 -5.1040912 -5.1486568 -5.2032075 -5.3681979 -7.2902517 -7.2540212 -8.1513033 -9.2079 -10.051736 -9.3759918][-3.3829093 -4.1635709 -4.4562283 -4.5242319 -4.8540277 -4.7337074 -4.4074345 -4.2831945 -3.9229891 -5.8229742 -6.0629287 -7.0108037 -7.8584743 -8.7171555 -8.2006931][-3.0158963 -3.9192498 -4.1691337 -4.1430888 -3.8170934 -3.2019734 -2.4120455 -2.2346659 -1.9930763 -4.0772071 -4.6945419 -6.0651207 -7.2900882 -7.8669505 -7.3723259][-4.6956153 -3.827121 -2.518579 -2.1300335 -1.2360411 -0.37045002 0.19130468 0.17355585 0.27577639 -2.0194616 -2.8233457 -4.5570316 -6.0870976 -7.1513095 -6.6643839][-4.0048294 -3.7344732 -2.7169485 -1.0649405 0.51524067 1.750288 2.3968019 2.1745014 1.612443 -0.9440136 -1.6884441 -3.180747 -4.8307791 -5.8829927 -5.6430116][-3.4413714 -3.0923028 -1.63025 0.18132162 1.8956461 3.01124 3.481143 3.3055286 2.7499466 -0.33741331 -1.5928307 -3.1861138 -4.7222538 -5.955204 -6.1194124][-2.1731257 -1.1865749 -0.11262369 1.9107046 3.152751 3.8530197 3.9314585 3.6818275 3.093647 0.055689812 -1.4032869 -3.27884 -5.1697111 -6.6554866 -6.3703661][-1.6751113 -1.1024237 0.47986889 1.9557667 2.6623259 2.7813759 2.5920286 2.3056345 1.7288628 -0.90910578 -1.8708329 -3.5133729 -5.5139418 -7.1958981 -7.2231779][-1.4634824 -1.3421922 -0.081602573 1.143631 1.5491381 1.5646553 1.423562 0.92550468 0.39885807 -2.1433854 -2.9594483 -4.360455 -5.9536695 -7.4504247 -7.1689091][-3.025291 -2.7247181 -2.0068164 -0.97443819 -0.51176739 -0.41059256 -0.6426425 -0.93229771 -0.88593006 -3.0747156 -4.2942829 -5.4614677 -6.7589788 -7.9846539 -7.8433514][-3.8497305 -3.5520868 -2.9367723 -2.4955497 -2.2370095 -2.1349006 -2.3842139 -2.4770694 -2.3888855 -4.0019407 -5.2566633 -5.7326126 -6.7050643 -7.5469384 -7.3706431][-5.1138973 -5.0295739 -5.0013194 -4.3250084 -4.0066853 -3.6880474 -3.4329891 -3.604466 -3.5882063 -4.688426 -5.4935341 -6.0912595 -6.6375189 -7.029541 -6.6092949][-4.7387667 -5.367939 -5.2341394 -4.8732295 -4.3097548 -3.9176025 -3.8339443 -3.8798935 -3.7800353 -3.9369433 -4.1841578 -5.0726428 -5.7430267 -6.4033704 -6.3921185][-5.8680573 -5.4437037 -5.0582361 -5.2725887 -5.3541045 -5.0535212 -4.6408529 -4.7752171 -5.1284418 -4.856636 -4.9229522 -5.1225471 -5.3430371 -5.729619 -6.1018467]]...]
INFO - root - 2017-12-15 15:33:51.141245: step 29510, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 54h:29m:50s remains)
INFO - root - 2017-12-15 15:33:57.523157: step 29520, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 53h:09m:50s remains)
INFO - root - 2017-12-15 15:34:03.891229: step 29530, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 52h:28m:50s remains)
INFO - root - 2017-12-15 15:34:10.304636: step 29540, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 52h:58m:29s remains)
INFO - root - 2017-12-15 15:34:16.728086: step 29550, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 55h:19m:13s remains)
INFO - root - 2017-12-15 15:34:23.080520: step 29560, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 54h:01m:31s remains)
INFO - root - 2017-12-15 15:34:29.478839: step 29570, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 53h:47m:00s remains)
INFO - root - 2017-12-15 15:34:35.832002: step 29580, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 53h:46m:28s remains)
INFO - root - 2017-12-15 15:34:42.263888: step 29590, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 53h:22m:41s remains)
INFO - root - 2017-12-15 15:34:48.684851: step 29600, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 55h:55m:10s remains)
2017-12-15 15:34:49.198189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7381177 -5.1087742 -6.2132006 -6.5391593 -6.8666449 -6.7672591 -5.8803234 -4.9213743 -3.831955 -4.2409029 -4.001895 -5.3318071 -6.4442592 -6.2434568 -6.611187][-3.3173175 -3.9308014 -5.1756182 -6.2407355 -6.5894451 -6.1007056 -5.3277855 -4.742413 -3.8796842 -4.5871572 -4.6648207 -6.2403288 -7.269702 -6.6213861 -6.4839025][-2.0716834 -2.8442411 -3.4164386 -4.15375 -4.3139191 -4.0966353 -3.9631171 -3.8808191 -4.01875 -4.9436073 -5.2166095 -6.9034128 -7.9814 -8.1648312 -7.8994889][-2.6136594 -2.8670282 -3.297339 -4.0620294 -3.7761586 -3.2537446 -2.9120646 -2.9373565 -3.1180091 -4.1730857 -4.4991088 -6.3306236 -7.6890154 -7.8108644 -8.1239729][-2.0772486 -2.4300394 -2.982224 -2.9110184 -2.16954 -0.9585886 -0.48133183 -0.62803459 -1.7393122 -3.4689646 -3.9830844 -5.77233 -7.0352726 -7.2600126 -7.7248483][-3.1904869 -3.0438428 -3.0864859 -1.9428678 -0.10681534 0.830081 2.0113354 1.8392859 0.583478 -1.9788828 -3.4918914 -5.5052743 -6.6183062 -6.9036775 -6.7740197][-3.3214316 -3.1276736 -2.9274917 -0.96163654 1.3134813 2.6354551 4.1716814 4.0315113 2.9201279 0.10912609 -1.7349648 -4.6906242 -6.1241121 -5.9536867 -6.1556916][-2.6726542 -2.2239203 -1.9027491 0.097907543 2.6834841 4.8568668 6.6096687 6.1761904 5.3853226 1.6900644 -0.73250008 -3.7232196 -5.6208677 -5.6325512 -5.8092051][-3.6047688 -2.380631 -1.8439593 -0.10770655 1.3028822 2.9816961 4.9456692 5.0349674 4.73557 1.2612858 -0.98326063 -3.9201758 -5.8805709 -5.8151755 -5.978508][-5.7331085 -5.1305771 -4.2502751 -2.8098145 -1.311996 0.31983852 1.6133175 1.6576004 1.7076616 -1.132988 -2.8186622 -5.473321 -7.5738349 -6.9424696 -6.8805242][-7.0320034 -6.8810968 -6.1706257 -4.6166596 -3.8600147 -2.2017431 -1.3314714 -1.1788344 -0.977128 -3.1837392 -4.2178955 -6.9209127 -8.2651377 -8.12337 -7.7784219][-8.2740726 -7.3699884 -7.1457453 -6.4369211 -5.7738 -5.286705 -4.3997602 -4.29007 -4.1199503 -5.7813735 -5.6675181 -7.4354062 -7.7262506 -7.47201 -8.0012293][-8.8829393 -8.2974224 -8.1652727 -7.2389765 -6.79879 -6.4802155 -6.4624557 -6.46257 -5.9670525 -6.7919655 -6.3778038 -8.1235361 -8.4784918 -8.6230068 -8.7717485][-8.941431 -8.5505514 -7.1923175 -6.5667849 -6.3451114 -6.2177186 -6.9125967 -7.5771523 -7.8384309 -8.4313469 -7.742712 -8.1961794 -8.4058056 -7.7764325 -7.8269463][-9.1734715 -9.4613361 -9.3975658 -8.5059786 -7.8427963 -7.4109955 -7.5796828 -7.9236865 -8.4554825 -8.7683411 -8.5901594 -8.2591753 -7.3107576 -7.0263948 -6.8488932]]...]
INFO - root - 2017-12-15 15:34:55.547958: step 29610, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 54h:00m:10s remains)
INFO - root - 2017-12-15 15:35:02.010469: step 29620, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 54h:23m:11s remains)
INFO - root - 2017-12-15 15:35:08.484109: step 29630, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 55h:43m:57s remains)
INFO - root - 2017-12-15 15:35:14.812701: step 29640, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 53h:56m:10s remains)
INFO - root - 2017-12-15 15:35:21.184964: step 29650, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 52h:15m:43s remains)
INFO - root - 2017-12-15 15:35:27.547852: step 29660, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 53h:01m:15s remains)
INFO - root - 2017-12-15 15:35:33.977677: step 29670, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 54h:04m:44s remains)
INFO - root - 2017-12-15 15:35:40.352580: step 29680, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 53h:10m:26s remains)
INFO - root - 2017-12-15 15:35:46.684306: step 29690, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 53h:50m:27s remains)
INFO - root - 2017-12-15 15:35:53.090587: step 29700, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 54h:36m:37s remains)
2017-12-15 15:35:53.621717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.744977 -4.4598255 -4.2024546 -4.0378714 -4.0084443 -3.9009545 -3.553925 -3.1064444 -2.6377168 -4.4372144 -4.4777536 -4.8570671 -5.6631527 -6.4361229 -7.224236][-4.0923376 -4.1707158 -4.2106104 -4.4307337 -4.5838881 -4.5394268 -4.2436066 -3.6502719 -3.2431788 -4.9994583 -4.8942776 -5.2705183 -6.1702089 -6.9939694 -7.5693169][-2.0766339 -2.1876464 -2.5326338 -3.0737219 -3.3674784 -3.5452042 -3.472672 -3.18222 -3.0393291 -4.9464788 -4.8850513 -5.3172154 -6.124073 -7.0398397 -7.666925][-1.0628448 -0.95222092 -1.2712131 -1.5090871 -1.5852985 -1.4368329 -1.2753487 -1.4967546 -1.6456003 -3.994818 -4.3979316 -5.1295915 -6.3122058 -7.3360214 -7.9337778][-1.2270761 -0.72675562 -0.53086805 -0.511024 -0.264122 0.41409683 0.546999 0.24861336 -0.18616962 -3.0857296 -4.0407257 -5.2426825 -6.6021795 -7.7135243 -8.2707243][-2.1404395 -1.3528323 -0.75331926 -0.36146116 0.22884655 1.1580048 1.8063688 1.7053242 1.1363335 -1.9829979 -3.3260899 -4.939456 -6.7300181 -7.8763056 -8.4304562][-3.1457834 -2.0914569 -1.3706188 -0.40953112 0.82275677 1.9329777 2.8092451 2.7678814 2.438343 -0.6881609 -2.3248916 -4.2139482 -6.2428751 -7.5702891 -8.2279234][-4.0650392 -2.9173536 -1.819087 -0.28127527 0.88450909 2.1413651 2.9830246 2.9751205 2.6468115 -0.33760643 -1.8527975 -3.9815211 -6.1782475 -7.5232325 -8.0563364][-5.211791 -4.0011687 -2.6580224 -0.97629452 0.031549931 0.93293476 1.4689999 1.7618637 1.9595718 -0.800035 -2.1139669 -3.8879404 -5.8216524 -7.2379823 -7.78629][-5.3979611 -4.66006 -3.6764097 -2.2184806 -1.109159 -0.52913761 -0.09480238 0.20363998 0.51871586 -2.0593438 -3.119575 -4.4508309 -6.0443311 -7.2747588 -7.5151062][-6.88187 -6.2125797 -5.4335423 -4.3409519 -3.219985 -2.5997491 -2.1458144 -1.8359694 -1.5211406 -4.063087 -5.0664406 -5.8510714 -6.9534149 -7.8599191 -7.9072323][-7.5741091 -7.0122232 -6.3700185 -5.5439672 -4.6571441 -3.9890392 -3.6361117 -3.5539117 -3.5498095 -5.5368729 -6.1234608 -6.4319034 -7.1570187 -7.8312116 -8.1145048][-7.4322381 -6.9890132 -6.6144366 -6.1295047 -5.4237566 -5.0754614 -4.785429 -4.6788387 -4.6367216 -6.2398415 -6.7024393 -6.6248369 -6.8368969 -7.2090178 -7.478858][-7.5222263 -7.4262533 -7.3452568 -7.0318351 -6.4428539 -5.8570747 -5.623353 -5.5366783 -5.2740006 -5.9125376 -6.3660812 -6.443315 -6.3136511 -6.3281236 -6.4413862][-7.7274952 -7.6379991 -7.4986653 -7.184648 -6.8886538 -6.5577641 -6.2629285 -6.4046707 -6.2549934 -6.1195784 -5.9779925 -6.0909719 -6.1136436 -5.8618116 -5.5831051]]...]
INFO - root - 2017-12-15 15:36:00.098623: step 29710, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 54h:33m:31s remains)
INFO - root - 2017-12-15 15:36:06.470323: step 29720, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:45m:24s remains)
INFO - root - 2017-12-15 15:36:12.804089: step 29730, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 52h:57m:05s remains)
INFO - root - 2017-12-15 15:36:19.227025: step 29740, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 52h:57m:48s remains)
INFO - root - 2017-12-15 15:36:25.634710: step 29750, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 54h:01m:18s remains)
INFO - root - 2017-12-15 15:36:31.983309: step 29760, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 52h:33m:06s remains)
INFO - root - 2017-12-15 15:36:38.400195: step 29770, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.636 sec/batch; 53h:27m:52s remains)
INFO - root - 2017-12-15 15:36:44.820085: step 29780, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 54h:32m:26s remains)
INFO - root - 2017-12-15 15:36:51.192136: step 29790, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 53h:02m:13s remains)
INFO - root - 2017-12-15 15:36:57.701617: step 29800, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 55h:20m:19s remains)
2017-12-15 15:36:58.227234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5369158 -4.9741135 -5.5371313 -5.8551383 -5.7481995 -5.39954 -4.8849707 -4.32464 -3.7526345 -3.6926615 -4.1028929 -4.9645567 -5.9507651 -6.8742318 -7.4136839][-3.6243129 -4.4164486 -5.2254028 -5.847723 -6.3206468 -6.4147105 -5.7986431 -5.2044597 -3.9826269 -3.4066219 -3.4585214 -3.8559418 -4.8591585 -6.3446689 -7.0236917][-4.6081009 -4.955471 -5.5678787 -6.1743298 -6.1957579 -6.3306961 -6.4593019 -5.8269014 -4.6882915 -4.5072451 -4.1474686 -4.0892944 -4.979043 -6.0543737 -6.578618][-5.133131 -5.0727396 -5.134532 -4.7588987 -4.7431965 -5.0227089 -4.8599958 -4.7173223 -4.0574131 -4.1982737 -3.9935191 -4.3153744 -4.8715372 -6.1226354 -6.6503711][-4.0799837 -3.6231618 -3.3112321 -2.4096775 -2.1781893 -2.0091395 -1.6471062 -1.8727093 -1.6128654 -1.9448643 -2.7320352 -3.5267053 -4.42737 -5.5074348 -5.9224238][-2.6035948 -2.3437934 -1.1901531 -0.51318932 0.061754227 0.82633495 1.64997 1.3665628 1.2756748 0.55628014 -0.90121651 -2.3847928 -3.8544195 -4.9350052 -5.3748279][-2.282146 -1.5239158 -0.33929777 0.73358345 1.7669115 2.7971087 3.4541321 3.3630953 3.3121328 2.2596703 0.34272766 -1.3636055 -2.955605 -4.1603475 -5.0257812][-1.8247843 -1.4035931 -0.7273612 0.8382597 1.8657122 2.8898525 3.3180485 3.5361872 3.5951014 2.6480589 0.77348137 -0.89125252 -2.7425318 -4.0222692 -5.1272426][-2.2827692 -1.8229032 -1.6772733 -0.72698116 0.52436066 1.4416962 1.7892065 2.0468445 2.2664661 1.2030106 -0.50154305 -1.6833482 -3.6559973 -5.0350895 -5.6932459][-3.7380428 -2.9414349 -2.8781161 -2.3712969 -1.3431706 -0.64432716 -0.21508074 -0.0075592995 0.15036583 -1.0134311 -2.5186849 -3.2843952 -4.6582332 -5.876996 -6.5458841][-6.3871508 -5.943615 -5.4867 -4.5313649 -3.3210964 -2.8059416 -2.4793487 -2.512073 -2.4244585 -3.1761036 -4.4400063 -4.811317 -5.5554895 -6.4123621 -6.8564043][-8.0978413 -7.973208 -7.9043832 -7.03801 -5.7428846 -4.8824577 -4.2587309 -4.4473572 -4.5065928 -5.111618 -5.6358929 -5.97367 -6.5914383 -7.1111145 -7.2759252][-9.81816 -10.153393 -9.8858776 -9.2337494 -8.3317051 -7.1555896 -6.0850892 -6.3097563 -6.1596994 -6.3547592 -6.9087582 -6.6278629 -6.799139 -6.8614821 -6.8504906][-10.481394 -10.836696 -10.293979 -9.7213078 -9.2122478 -8.3014526 -7.352345 -7.1672177 -6.918983 -6.9766083 -7.0653315 -6.9082031 -6.612761 -6.0586796 -5.8849735][-10.34284 -9.9796085 -9.364933 -9.4012108 -9.0454912 -8.4239521 -7.829277 -7.6552896 -7.7421865 -7.4976983 -7.15547 -6.7252736 -6.1712165 -6.1661797 -5.8990993]]...]
INFO - root - 2017-12-15 15:37:04.597779: step 29810, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 53h:49m:50s remains)
INFO - root - 2017-12-15 15:37:11.077181: step 29820, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 55h:30m:40s remains)
INFO - root - 2017-12-15 15:37:17.442162: step 29830, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 53h:31m:14s remains)
INFO - root - 2017-12-15 15:37:23.955414: step 29840, loss = 0.28, batch loss = 0.16 (11.7 examples/sec; 0.684 sec/batch; 57h:32m:39s remains)
INFO - root - 2017-12-15 15:37:30.459167: step 29850, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 55h:22m:39s remains)
INFO - root - 2017-12-15 15:37:36.894412: step 29860, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 55h:23m:12s remains)
INFO - root - 2017-12-15 15:37:43.245638: step 29870, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.618 sec/batch; 51h:58m:27s remains)
INFO - root - 2017-12-15 15:37:49.619100: step 29880, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 54h:57m:49s remains)
INFO - root - 2017-12-15 15:37:56.095354: step 29890, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 54h:03m:39s remains)
INFO - root - 2017-12-15 15:38:02.400125: step 29900, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 52h:46m:32s remains)
2017-12-15 15:38:02.853603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1131134 -2.1122403 -1.8630829 -2.0150099 -2.1455054 -1.9556689 -1.3642411 -0.97901344 -0.60508537 -2.1598673 -3.2547112 -5.8107295 -6.6558509 -7.8058305 -8.5614862][-3.4737215 -3.6959786 -3.8652418 -4.1094217 -4.1561055 -3.7883334 -3.1331172 -2.6405706 -2.3969183 -3.5281506 -4.4811096 -7.0059361 -7.8957419 -8.6464882 -8.9430265][-4.417881 -4.4976492 -4.9227424 -4.8527784 -4.4756336 -3.8083048 -3.0037169 -2.2650547 -1.8419552 -3.1213584 -4.2862425 -6.6691685 -7.612638 -8.3947134 -8.4255228][-5.2335119 -5.0929484 -4.9004126 -4.1545134 -3.4157362 -2.2900429 -1.1948261 -0.7623 -0.30031252 -1.6645379 -2.8587365 -5.7689362 -6.5868936 -7.8968606 -8.1697063][-6.3089056 -5.8031769 -5.0365028 -3.6117325 -2.6191092 -1.3632212 -0.42451334 -0.11038303 0.19402266 -1.3846159 -2.5239801 -5.6624031 -6.5727892 -7.1112795 -7.0725117][-5.8346786 -5.5717797 -5.0136023 -3.0019579 -1.4352279 -0.093760014 0.87786674 1.3335342 1.3820982 -0.15869617 -1.3703837 -4.3977108 -5.6726446 -6.4226003 -6.3911481][-6.2056131 -5.2953939 -4.2854519 -2.5276065 -0.983521 0.71364975 1.8365164 2.3283348 2.3025713 0.56071663 -0.94311714 -4.2274 -5.3340693 -6.0776315 -5.9577732][-5.0700493 -4.05217 -2.9084215 -1.045804 0.3099432 1.8884163 2.6627808 3.0495796 3.158164 1.1030569 -0.45287275 -3.7605968 -5.0611382 -5.9302568 -6.112359][-3.8773048 -3.3382206 -2.4480653 -1.051126 -0.026336193 1.3258753 2.0480518 2.4434814 2.1298494 0.37069035 -1.015789 -3.8688624 -4.9935656 -5.7526712 -5.8635445][-3.7256229 -3.3919253 -2.896502 -1.8629336 -1.1412082 -0.16203499 0.46079731 0.89023781 0.60981274 -0.904758 -2.4161787 -4.9843864 -5.7981248 -6.3898458 -6.5077238][-4.5654092 -4.4299994 -4.30479 -3.6320338 -3.217247 -2.3980994 -1.8317246 -1.5123868 -1.6172404 -3.1406302 -4.3497791 -6.3807111 -6.8233352 -6.9922585 -6.7754188][-5.5549974 -5.4304676 -5.2994843 -4.8127174 -4.3257775 -3.879504 -3.4953399 -3.2967796 -3.2976475 -4.01996 -5.2850609 -6.7573643 -7.0184932 -6.9856453 -6.6874409][-7.1856828 -7.2137041 -7.1062613 -6.5438261 -5.9684391 -5.2652044 -4.7471919 -4.6457591 -4.8514071 -5.2292595 -5.9070506 -6.6470966 -6.57351 -6.3315058 -5.7248325][-6.7907476 -7.06532 -7.1344752 -6.4830008 -5.7693605 -5.4446373 -5.54076 -5.4389668 -5.3281851 -5.6780186 -6.12942 -6.35823 -6.3779116 -6.276947 -5.9785118][-7.7550507 -7.7470942 -7.4643192 -7.3750334 -7.0123549 -6.5979609 -6.6602607 -6.8682442 -6.9769869 -7.0395126 -7.1070571 -7.3131242 -7.1232047 -6.92453 -6.7885942]]...]
INFO - root - 2017-12-15 15:38:09.183782: step 29910, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 52h:58m:19s remains)
INFO - root - 2017-12-15 15:38:15.625322: step 29920, loss = 0.40, batch loss = 0.29 (12.5 examples/sec; 0.639 sec/batch; 53h:43m:21s remains)
INFO - root - 2017-12-15 15:38:22.077790: step 29930, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 54h:24m:19s remains)
INFO - root - 2017-12-15 15:38:28.511167: step 29940, loss = 0.23, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 52h:41m:01s remains)
INFO - root - 2017-12-15 15:38:34.929999: step 29950, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 55h:28m:52s remains)
INFO - root - 2017-12-15 15:38:41.339528: step 29960, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 53h:52m:00s remains)
INFO - root - 2017-12-15 15:38:47.702629: step 29970, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 53h:19m:30s remains)
INFO - root - 2017-12-15 15:38:54.083881: step 29980, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 53h:48m:24s remains)
INFO - root - 2017-12-15 15:39:00.464500: step 29990, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 53h:06m:41s remains)
INFO - root - 2017-12-15 15:39:06.912529: step 30000, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 54h:11m:22s remains)
2017-12-15 15:39:07.431531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7144089 -4.822216 -5.0624075 -5.2811823 -5.2647562 -4.7339582 -4.3132057 -4.0557218 -3.5682216 -3.8919518 -4.89925 -6.7772264 -8.172965 -9.06485 -9.6413116][-4.6609964 -4.7990084 -5.310678 -5.9224358 -6.5185418 -6.6274309 -6.2862072 -5.0426245 -3.9783604 -4.59041 -6.0139465 -7.7770391 -8.6226492 -8.7688007 -8.687602][-5.33558 -4.9296823 -4.616868 -4.7749209 -4.5951118 -4.1491785 -3.6432347 -3.2544503 -2.5496969 -2.7879219 -3.9666085 -6.0906129 -7.4151063 -7.7197814 -7.80174][-5.1711893 -4.8243217 -4.4934969 -3.695087 -2.6222758 -1.7516952 -0.69614267 -0.088202953 -0.033816338 -1.1483312 -2.8257842 -4.9557686 -6.7615476 -7.0320497 -7.0879688][-4.9183517 -4.1384625 -3.1986198 -2.2568378 -1.5841393 -0.34448814 0.77893066 1.3330345 1.363802 0.178864 -1.7494602 -4.0190821 -5.6583805 -6.5176134 -6.8989925][-4.6158619 -4.2886853 -3.5023928 -2.005743 -0.33472872 1.3766937 2.9558363 3.5530405 3.4923935 1.6534739 -0.81633663 -3.5281563 -5.1682334 -5.6771631 -5.8027139][-5.061552 -4.0971155 -2.5986094 -0.8245163 1.246912 3.099411 4.3841953 4.7660017 4.2716055 1.9143152 -0.89655638 -4.0093465 -5.6936932 -5.9547191 -6.0675931][-4.8313293 -4.3997211 -3.0408936 -0.95197296 0.8737011 3.0451612 4.0734863 4.1630211 3.9484224 1.7880716 -1.0420294 -4.190114 -5.70242 -6.2388787 -6.1561131][-5.0277843 -4.399754 -3.6678357 -2.101788 -0.48712492 1.289156 2.4142361 3.0904217 3.1905231 1.0385714 -1.2669754 -3.9234931 -5.3402648 -5.8340445 -6.1115971][-5.5813694 -5.1523151 -4.250371 -3.1841092 -1.6620808 0.048651695 0.87755203 1.3285618 1.0554256 -0.5612793 -2.3077598 -4.2154207 -5.2772493 -5.8108 -5.945291][-6.5510845 -6.2707548 -5.693707 -4.5901952 -3.4415307 -2.3093276 -1.7827988 -1.6037059 -1.790153 -2.8325343 -3.9851289 -5.3028173 -5.6835036 -5.7836046 -5.8553991][-6.8901916 -6.7801447 -6.3296227 -5.9089327 -5.2653723 -4.5286417 -3.8445024 -3.5527854 -3.5371971 -4.3021297 -5.1970706 -5.5300183 -5.5835204 -5.4646034 -5.4652519][-7.281898 -7.4418654 -7.7324452 -7.2369885 -6.5938225 -6.0361834 -5.4185271 -5.4902186 -5.5980253 -6.0782747 -6.6540079 -6.6478524 -6.6458244 -6.34544 -5.9702387][-6.9045095 -6.7781787 -6.6088276 -6.2136931 -5.9413438 -5.4285207 -5.2527189 -5.6733456 -5.7996306 -6.0049467 -6.0837965 -6.05725 -5.9074059 -5.7202306 -5.9115777][-6.7864089 -6.8727131 -6.6763492 -6.7400541 -6.9344859 -6.7088442 -6.3876877 -6.2798738 -6.218524 -6.5020132 -6.5246282 -6.5424652 -6.7040124 -6.7795868 -6.5439076]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 15:39:14.754306: step 30010, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.631 sec/batch; 53h:03m:33s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 15:39:21.190190: step 30020, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 54h:29m:06s remains)
INFO - root - 2017-12-15 15:39:27.526688: step 30030, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 53h:25m:00s remains)
INFO - root - 2017-12-15 15:39:33.861676: step 30040, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.629 sec/batch; 52h:51m:32s remains)
INFO - root - 2017-12-15 15:39:40.208642: step 30050, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 52h:46m:47s remains)
INFO - root - 2017-12-15 15:39:46.619018: step 30060, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.618 sec/batch; 51h:53m:16s remains)
INFO - root - 2017-12-15 15:39:53.067440: step 30070, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 53h:48m:53s remains)
INFO - root - 2017-12-15 15:39:59.428899: step 30080, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 53h:54m:21s remains)
INFO - root - 2017-12-15 15:40:05.834278: step 30090, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 53h:42m:27s remains)
INFO - root - 2017-12-15 15:40:12.304258: step 30100, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 57h:03m:11s remains)
2017-12-15 15:40:12.799852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7033424 -5.8514528 -6.3227024 -6.5898471 -6.459753 -6.41312 -5.6704521 -4.8229961 -3.4406576 -3.0333877 -3.9829834 -3.534883 -4.0681963 -4.8363867 -5.8136592][-5.5046892 -5.9502339 -6.5561447 -6.6071615 -6.6616716 -6.5692577 -6.1813431 -5.510674 -4.2072978 -3.7406778 -4.95854 -4.9293494 -5.3745217 -5.4359274 -5.7006526][-6.2569389 -6.3255353 -6.1988344 -6.1752019 -5.8739352 -5.6663914 -5.446651 -5.0633864 -4.419817 -4.1211433 -5.175127 -5.2172709 -6.2171688 -6.4623547 -6.478888][-6.3641496 -6.1064639 -5.4706488 -4.7007084 -4.0394783 -3.5310621 -2.8011885 -2.5222392 -1.9925323 -2.350317 -4.09674 -4.4283152 -5.7570219 -6.6958556 -7.3484445][-6.8307257 -5.7296195 -4.4887652 -3.7414277 -2.9170914 -1.8436222 -0.70571184 -0.20210457 0.32925892 -0.32613754 -2.3994985 -3.5056844 -5.6979313 -6.8164968 -7.3972588][-5.7236876 -4.7657738 -3.744879 -2.282371 -0.763289 0.57860374 2.1071377 2.6272688 2.9120369 1.9489155 -0.64264154 -2.1931825 -4.5979452 -5.842236 -6.7564235][-5.2009449 -4.5611391 -2.6217852 -1.1938448 0.25146103 2.343544 4.0355558 4.5150509 4.6374254 3.2408991 0.33996582 -1.4034667 -3.4408588 -4.8270769 -5.766643][-4.6740694 -3.6828218 -2.5960984 -0.68791819 1.1840477 2.2982769 3.5729723 4.3071518 4.4671574 3.0963316 0.29244423 -1.8444071 -3.8793116 -4.7933431 -5.3791251][-4.3003864 -3.7231803 -2.7349091 -1.2668419 0.29059982 1.4668903 2.7973394 3.3717556 3.7223511 2.186389 -1.0618167 -2.5525322 -4.3641396 -5.2119384 -5.4729013][-5.4695215 -4.7520609 -3.5613003 -2.5496144 -1.0345583 0.2315383 0.59191418 1.1362324 1.2265787 0.05818224 -1.9342103 -2.7277837 -4.3463192 -5.3013105 -5.6439586][-7.299479 -6.6460447 -6.2178288 -5.2432017 -3.860419 -2.8079972 -2.0630913 -2.1281767 -2.6507049 -3.1934481 -4.3142743 -4.5984368 -5.2256937 -6.0808296 -6.6536388][-7.8446579 -7.4619908 -6.7879133 -6.273036 -5.39615 -4.6001363 -3.4949689 -3.3506379 -3.3496103 -3.9018335 -4.9566503 -4.7233477 -4.9548488 -5.7790728 -6.2112637][-7.9967685 -8.09745 -7.8582768 -7.301775 -6.6132479 -5.9021616 -5.098372 -4.9965482 -4.8829441 -4.8345709 -5.5736828 -5.1941638 -5.3139844 -5.4947147 -5.4606352][-7.3922982 -7.4615073 -6.8435822 -6.4810691 -5.9989724 -5.3326945 -4.9237318 -4.7408381 -4.7524767 -4.979475 -5.3780322 -5.2142782 -4.9716778 -5.1670961 -5.3987513][-7.1894932 -6.8586044 -6.5826731 -6.3851166 -5.7906561 -5.6233139 -5.397944 -5.5446949 -5.53331 -5.6827569 -5.9743042 -6.0583143 -5.8752537 -5.8497038 -5.7715092]]...]
INFO - root - 2017-12-15 15:40:19.241381: step 30110, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 53h:53m:37s remains)
INFO - root - 2017-12-15 15:40:25.643142: step 30120, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:29m:36s remains)
INFO - root - 2017-12-15 15:40:32.040664: step 30130, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 53h:20m:49s remains)
INFO - root - 2017-12-15 15:40:38.449665: step 30140, loss = 0.38, batch loss = 0.27 (12.0 examples/sec; 0.667 sec/batch; 56h:00m:13s remains)
INFO - root - 2017-12-15 15:40:44.773645: step 30150, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 53h:04m:19s remains)
INFO - root - 2017-12-15 15:40:51.074129: step 30160, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 53h:34m:05s remains)
INFO - root - 2017-12-15 15:40:57.497982: step 30170, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 53h:08m:51s remains)
INFO - root - 2017-12-15 15:41:03.903548: step 30180, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 52h:30m:10s remains)
INFO - root - 2017-12-15 15:41:10.327411: step 30190, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 53h:56m:08s remains)
INFO - root - 2017-12-15 15:41:16.739215: step 30200, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 53h:40m:54s remains)
2017-12-15 15:41:17.223543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5671835 -3.7750771 -4.2257948 -4.3165855 -4.3533659 -4.1435289 -4.2226381 -4.4531183 -4.0933051 -4.3083229 -5.8294811 -4.6116352 -6.1284232 -5.4078307 -3.7979693][-1.0652695 -2.0914326 -2.2764316 -2.5624852 -2.9925017 -2.3772087 -2.66116 -2.850738 -2.86841 -3.6357408 -4.285327 -3.5078573 -5.5188284 -5.0333996 -4.9250154][-0.68647718 -1.8529792 -2.3051114 -2.7252331 -3.0254097 -2.5965118 -2.7285113 -2.7342076 -3.2067761 -4.21854 -5.1072121 -5.7612257 -8.3238058 -8.19209 -8.0161295][-2.5388131 -2.431797 -2.3217521 -2.4841681 -2.3340526 -1.8577256 -1.7639627 -1.8268151 -2.3960814 -3.7736762 -4.8524351 -5.4488215 -7.3383431 -7.8615823 -8.6765165][-3.1588821 -3.0857658 -2.7199287 -2.1393538 -1.3461165 -0.665792 -0.28641844 -0.073338509 -0.26895809 -0.76143456 -1.5855722 -2.3502879 -4.9204187 -6.5808983 -7.0498152][-4.4976254 -3.0060744 -1.9906816 -2.2278457 -1.3770065 -0.22839308 0.83313847 1.4972067 1.6362658 1.7941399 1.3595734 0.57683182 -1.1627107 -3.4164572 -5.3164425][-3.0422206 -2.0717769 -1.3165927 -0.71720123 0.78000546 2.3466711 3.5377007 2.8955317 2.5510597 2.8747597 1.8300114 1.0837078 -0.71954632 -2.5334749 -3.8643458][-2.0391316 -2.0963345 -0.77569437 0.6461668 2.8592749 3.9186831 4.830265 5.2996311 5.6848764 4.3967867 2.5869417 1.807271 0.17480659 -0.99241781 -2.8081141][-2.4256816 -2.731606 -1.8372188 0.11165237 2.300252 2.6597748 3.8983297 5.4587011 6.4071617 5.3957825 2.997282 1.3456163 -2.0253739 -3.5503354 -4.73182][-4.1234274 -4.2035503 -3.2849588 -2.0566092 -0.12225103 0.85122204 2.4953213 3.0829964 3.448926 2.4675283 1.2756939 0.056203365 -3.3352127 -5.0889969 -5.7316213][-6.7301168 -6.5452518 -6.3926954 -5.6405916 -4.0528979 -2.4824347 -0.74160433 -0.56299877 0.10165071 -0.87948608 -2.5239682 -2.6063719 -5.1612568 -6.8161364 -7.907083][-7.3577127 -7.2907143 -7.0306497 -7.5611663 -6.6864204 -5.2008162 -4.296771 -3.3386831 -2.1155953 -3.9697223 -4.735671 -4.3784184 -5.506041 -6.6998606 -7.8085852][-8.822937 -8.3503513 -8.2733469 -8.2896967 -8.1982384 -8.0432863 -7.4614034 -6.7897239 -6.1625919 -6.6194715 -6.349225 -5.6317539 -6.3781452 -6.4548836 -6.6731815][-8.05878 -7.9471273 -7.8709044 -7.7519679 -7.9837074 -7.5141311 -7.34426 -7.6318517 -7.1349468 -7.3027415 -7.1596742 -6.9391441 -6.4446473 -5.6115389 -4.5059462][-8.4135838 -8.1095314 -8.1194506 -7.6758451 -6.9982777 -7.0788212 -6.9520454 -7.4825249 -7.3800807 -7.5969629 -7.4609542 -6.6327572 -6.3882017 -6.1756659 -5.7814775]]...]
INFO - root - 2017-12-15 15:41:23.653248: step 30210, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 52h:08m:52s remains)
INFO - root - 2017-12-15 15:41:29.972630: step 30220, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 52h:35m:59s remains)
INFO - root - 2017-12-15 15:41:36.297652: step 30230, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 52h:38m:25s remains)
INFO - root - 2017-12-15 15:41:42.681105: step 30240, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.630 sec/batch; 52h:54m:49s remains)
INFO - root - 2017-12-15 15:41:49.105445: step 30250, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 54h:00m:11s remains)
INFO - root - 2017-12-15 15:41:55.445592: step 30260, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.617 sec/batch; 51h:46m:27s remains)
INFO - root - 2017-12-15 15:42:01.835356: step 30270, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 53h:22m:56s remains)
INFO - root - 2017-12-15 15:42:08.305210: step 30280, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 53h:40m:49s remains)
INFO - root - 2017-12-15 15:42:14.792908: step 30290, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 54h:49m:27s remains)
INFO - root - 2017-12-15 15:42:21.178011: step 30300, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 52h:58m:40s remains)
2017-12-15 15:42:21.624748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2235522 -2.6123662 -3.1257157 -4.1661873 -5.3538985 -6.3660331 -7.6597314 -8.9208565 -10.206853 -11.818871 -13.795276 -13.877171 -13.12401 -11.730267 -10.747168][-0.78637981 -2.0888381 -3.6349082 -4.3549862 -5.2166643 -6.2975426 -7.2620249 -8.0417728 -8.4713278 -9.4143257 -10.835941 -11.974951 -12.692567 -12.204905 -12.153052][-1.7940326 -2.1877341 -3.1899867 -3.6620097 -4.4556441 -4.9958487 -5.5211496 -6.22933 -6.7634482 -7.2931328 -8.39552 -9.2192726 -10.025085 -10.846441 -12.271665][-4.2278357 -4.3243694 -4.6635036 -4.1276522 -3.2596288 -2.8423319 -2.3678322 -3.1479211 -3.6443529 -4.561882 -6.3116255 -7.6652951 -8.4960861 -9.1738253 -10.597554][-7.4096179 -6.6309223 -5.7653894 -5.1420755 -4.5997066 -2.8578272 -0.99405193 -0.62174034 -0.728209 -2.4337764 -5.2172956 -7.6665587 -8.734211 -9.7379 -11.01582][-8.1691494 -8.0466986 -7.1662011 -5.5544548 -3.2936416 -1.140089 0.96814728 1.453928 1.4890051 -0.41508389 -3.7551837 -7.04547 -9.2417583 -10.565517 -10.987738][-8.8991156 -8.0127764 -6.2353268 -3.5937309 -0.72226095 2.0131092 4.6670866 5.2405329 5.4518776 3.1287127 -1.1680946 -5.3856087 -8.1689806 -9.847599 -11.071946][-8.3283119 -7.9463573 -6.4262133 -3.4745698 0.0651412 2.6807938 4.8445778 5.8803968 6.3961411 4.3674927 1.1075754 -3.0098538 -5.906827 -7.7921982 -8.7472162][-8.7759676 -8.1763544 -7.1298943 -4.7239685 -1.7062631 0.54895592 2.8482828 3.7039709 3.9542894 2.6604624 -0.63598967 -3.6058736 -5.4767003 -7.0407648 -8.41534][-9.1892853 -9.1683969 -8.337945 -6.5336118 -4.4021306 -1.3572211 1.1331396 1.8373194 2.3461866 0.41424847 -2.6813865 -5.6396341 -7.5452747 -8.233511 -8.6853237][-10.571334 -10.784841 -10.786387 -9.6619043 -8.1654348 -6.0415969 -3.6830416 -1.8294172 -0.44319677 -1.0940256 -3.6504908 -6.1950645 -7.472959 -8.4861221 -9.29783][-10.814721 -11.075004 -10.831391 -10.04046 -9.2352209 -8.3521843 -7.4770937 -6.3963718 -4.9484138 -4.3429317 -5.1117883 -6.1780047 -6.8243012 -7.3041158 -7.9285083][-11.270413 -11.431929 -11.65297 -10.911298 -10.155462 -9.3804321 -8.5049639 -8.3589506 -8.4020252 -8.5832653 -8.7802248 -8.4148836 -8.0539913 -7.6661868 -7.0224504][-9.9406948 -9.8360023 -9.8083038 -9.0786247 -8.6912041 -8.0611706 -7.6855178 -7.5279212 -7.4143548 -7.5841341 -8.1739788 -8.4434376 -8.4946012 -8.2433214 -7.9572663][-10.633166 -10.006189 -9.0878162 -8.5751524 -8.2365837 -7.9060159 -7.69064 -7.7882524 -7.897378 -8.1246185 -8.1585016 -7.4437046 -7.1185575 -6.822093 -6.4566822]]...]
INFO - root - 2017-12-15 15:42:27.984006: step 30310, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 53h:03m:17s remains)
INFO - root - 2017-12-15 15:42:34.336764: step 30320, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 52h:28m:12s remains)
INFO - root - 2017-12-15 15:42:40.715512: step 30330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 53h:16m:53s remains)
INFO - root - 2017-12-15 15:42:47.077914: step 30340, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 53h:00m:59s remains)
INFO - root - 2017-12-15 15:42:53.512565: step 30350, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 52h:58m:57s remains)
INFO - root - 2017-12-15 15:42:59.919126: step 30360, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 54h:05m:43s remains)
INFO - root - 2017-12-15 15:43:06.277990: step 30370, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:33m:42s remains)
INFO - root - 2017-12-15 15:43:12.633119: step 30380, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 52h:27m:06s remains)
INFO - root - 2017-12-15 15:43:19.072693: step 30390, loss = 0.23, batch loss = 0.11 (12.7 examples/sec; 0.628 sec/batch; 52h:40m:00s remains)
INFO - root - 2017-12-15 15:43:25.410575: step 30400, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 55h:35m:33s remains)
2017-12-15 15:43:25.881505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5775676 -3.4905548 -4.2609158 -4.3754182 -3.8098629 -3.4610243 -3.6419749 -3.1769795 -2.9936085 -3.5831466 -3.5272875 -5.1822577 -5.9067259 -6.2540665 -7.2522049][-2.9152241 -3.6473322 -4.5160627 -4.8791618 -4.7139215 -4.12533 -3.4456434 -3.4766126 -3.5468068 -4.0315309 -4.2771063 -5.8584585 -6.3020725 -6.9243016 -7.6056294][-3.7587457 -3.9504282 -4.5088859 -4.7246637 -4.7769375 -4.4498329 -4.0710115 -3.7428715 -3.1955419 -4.1791983 -4.7099628 -6.3983278 -6.9419351 -7.6004629 -8.4364319][-4.598042 -4.8527284 -5.0287752 -4.711092 -4.2635851 -3.794667 -2.9040155 -2.4859147 -2.562377 -3.3319407 -3.755394 -5.8389711 -6.8504591 -7.5423169 -8.0791283][-5.8216915 -5.5730042 -5.592217 -5.0102954 -4.2337332 -2.9867263 -1.386549 -0.94121742 -0.72968245 -2.0178971 -3.0278673 -5.2920818 -6.4245815 -7.4152665 -8.2509327][-7.3232341 -6.7835121 -6.16865 -4.7180204 -3.436132 -1.7741013 0.10271311 1.0271492 1.1431656 -0.97069931 -2.8291311 -5.5705695 -6.5992637 -7.2683854 -7.9098454][-7.07198 -6.5290403 -6.1170387 -4.257266 -1.906415 0.081211567 2.3541222 3.1645889 3.3352509 1.1045055 -1.1745372 -4.3554287 -5.8598123 -6.44347 -7.143333][-6.7023034 -6.0985394 -5.3762112 -3.5269318 -1.3695974 1.4736557 3.9932261 4.3285856 4.0399809 1.5520477 -0.29064655 -3.315269 -4.8615522 -5.6467738 -6.1203][-6.7584076 -6.2125273 -5.7373314 -4.2548213 -2.2493067 0.55641556 2.9078941 3.5422363 3.4955149 0.84586048 -1.1682048 -4.0151081 -5.2919807 -5.9796534 -6.7109094][-6.5882149 -6.2671337 -5.9642491 -4.6713362 -3.1482048 -1.2677393 0.71838951 1.2396317 1.3877096 -0.976583 -3.0164571 -5.5315289 -6.3627687 -6.5405035 -7.2450719][-7.9103165 -7.5206923 -7.35174 -6.1482353 -5.062151 -3.5772343 -2.3648238 -1.9319248 -1.6784282 -3.5828996 -4.345603 -6.4341259 -7.2763672 -7.1277142 -7.4339147][-7.9273276 -7.6843867 -7.5315504 -6.6456003 -6.1087379 -5.2116656 -4.087254 -4.0477281 -4.0579882 -4.766758 -4.9527183 -6.2419891 -6.708827 -7.0220356 -7.4856877][-8.9833193 -8.2335005 -7.6972809 -6.8634634 -6.3562646 -6.042717 -5.4864168 -5.4150457 -5.2069025 -6.1874394 -6.3632646 -6.59519 -6.5609159 -6.6061149 -7.1572433][-9.0180731 -8.188221 -7.5120974 -6.7606754 -6.1927662 -5.9131107 -5.3756351 -5.5233955 -5.6181464 -5.7482147 -6.004509 -6.2272444 -6.1663771 -6.349761 -6.55878][-8.7055035 -8.5236979 -8.2669783 -7.3767228 -6.578928 -6.4831147 -6.3514209 -6.4130836 -6.2630062 -6.2588348 -6.385993 -6.2061343 -5.9947844 -5.7159972 -5.3939366]]...]
INFO - root - 2017-12-15 15:43:32.188299: step 30410, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:31m:50s remains)
INFO - root - 2017-12-15 15:43:38.635694: step 30420, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 54h:01m:33s remains)
INFO - root - 2017-12-15 15:43:45.004298: step 30430, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 51h:55m:27s remains)
INFO - root - 2017-12-15 15:43:51.378416: step 30440, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 54h:37m:47s remains)
INFO - root - 2017-12-15 15:43:57.828459: step 30450, loss = 0.41, batch loss = 0.29 (12.4 examples/sec; 0.644 sec/batch; 54h:01m:07s remains)
INFO - root - 2017-12-15 15:44:04.256039: step 30460, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 54h:53m:18s remains)
INFO - root - 2017-12-15 15:44:10.718145: step 30470, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 54h:47m:44s remains)
INFO - root - 2017-12-15 15:44:17.035615: step 30480, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 52h:21m:42s remains)
INFO - root - 2017-12-15 15:44:23.429423: step 30490, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 54h:27m:09s remains)
INFO - root - 2017-12-15 15:44:29.766767: step 30500, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 54h:51m:39s remains)
2017-12-15 15:44:30.306493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4652538 -2.7776012 -3.1533833 -3.5317106 -3.5511532 -3.5787458 -3.4953213 -3.8143888 -4.1783457 -5.9632096 -6.4104953 -7.3687038 -7.244051 -8.02773 -7.6900711][-2.511724 -2.7466512 -3.2904463 -3.7459326 -3.8766441 -3.9524283 -3.9954615 -4.1360836 -4.1653528 -5.8487396 -6.2170081 -7.3045449 -7.3293748 -8.2790308 -7.6650457][-3.3437176 -3.1399708 -3.457458 -3.784224 -3.5611339 -3.5562658 -3.7147675 -3.9315162 -4.1268373 -6.1927075 -6.4847078 -7.3265615 -7.336823 -8.4636822 -8.02091][-2.9235187 -2.745492 -2.739975 -2.3612285 -1.7354717 -1.4318237 -1.7102423 -2.2558737 -2.5460672 -4.6854234 -5.5287671 -6.5707779 -6.8215036 -7.010778 -6.1972685][-2.9472556 -2.2968831 -1.9660239 -1.3777919 -0.51070786 0.19700909 0.41314888 -0.01924181 -0.39868832 -3.0679183 -4.3059678 -5.948689 -6.429101 -6.7187238 -5.9152756][-5.1765966 -3.7402096 -2.8712511 -1.6649694 -0.60298729 1.4270878 2.4047155 2.0957823 1.17587 -1.8166609 -3.3985343 -5.6683273 -6.4379978 -6.8526392 -6.097311][-4.51867 -3.4021125 -2.1668191 -0.027798653 1.2097368 2.8144445 4.0488138 4.2081795 3.2109461 -0.37742949 -2.7630358 -5.0009432 -5.4202585 -6.2293763 -5.3393364][-3.3541937 -1.8473048 -0.32916498 1.4519129 2.5806112 4.8504267 5.4274311 5.2688007 4.4454079 1.3601551 -0.7205205 -3.2598429 -4.6643553 -5.4285545 -4.634347][-4.1005855 -2.1158748 -1.0757074 0.51924896 1.5463724 2.5871792 3.0200653 3.4136887 3.2716942 0.040263653 -1.8784804 -3.8346143 -4.635457 -6.009944 -5.4240446][-4.52457 -3.4298525 -2.3478885 -0.53068924 0.64459133 1.0191031 1.0661926 1.6621161 1.5539064 -1.743217 -3.2937961 -4.7409029 -5.429935 -6.4630189 -6.1458712][-6.3520794 -5.5619955 -4.637114 -3.1366992 -2.4083037 -2.1042004 -1.9995384 -1.4727802 -1.0629854 -3.7497735 -5.3858519 -6.6203308 -6.7596416 -7.7001095 -7.3987932][-7.6907964 -6.7941251 -6.0856409 -5.4565563 -4.8808994 -4.1642227 -4.1986461 -3.6477976 -3.1373358 -4.9082222 -5.7092094 -6.5860562 -6.422411 -7.7676306 -7.80105][-8.6262016 -8.4597349 -8.073494 -7.311532 -6.5753536 -6.0419383 -6.1640549 -5.7954316 -5.6973977 -6.5959444 -7.2391524 -7.3479171 -6.6886654 -6.8150368 -6.7353106][-9.3726349 -9.1612062 -8.4975853 -8.2586031 -7.4689426 -6.7093735 -6.4389286 -6.4917989 -6.7550449 -7.6120148 -7.4542856 -7.5291 -7.04178 -7.3318138 -6.8468623][-10.326803 -9.9201641 -9.2679691 -8.5488443 -8.4550486 -8.1998539 -7.9304409 -7.7945895 -7.7073765 -7.9203649 -7.6675043 -7.3191433 -6.740222 -6.6491466 -6.4320207]]...]
INFO - root - 2017-12-15 15:44:36.748440: step 30510, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 52h:07m:45s remains)
INFO - root - 2017-12-15 15:44:43.136741: step 30520, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 54h:10m:24s remains)
INFO - root - 2017-12-15 15:44:49.513376: step 30530, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 52h:26m:11s remains)
INFO - root - 2017-12-15 15:44:55.972958: step 30540, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 52h:47m:32s remains)
INFO - root - 2017-12-15 15:45:02.367219: step 30550, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 53h:58m:18s remains)
INFO - root - 2017-12-15 15:45:08.684389: step 30560, loss = 0.24, batch loss = 0.12 (12.9 examples/sec; 0.620 sec/batch; 51h:57m:48s remains)
INFO - root - 2017-12-15 15:45:15.014172: step 30570, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 53h:35m:18s remains)
INFO - root - 2017-12-15 15:45:21.459202: step 30580, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:29m:52s remains)
INFO - root - 2017-12-15 15:45:27.865180: step 30590, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.620 sec/batch; 52h:01m:17s remains)
INFO - root - 2017-12-15 15:45:34.213683: step 30600, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 53h:07m:02s remains)
2017-12-15 15:45:34.724214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8435576 -3.9129837 -3.7897496 -4.2401381 -5.0655813 -5.6007171 -5.9493346 -5.9819951 -5.7247076 -7.0824819 -7.725081 -8.0712261 -8.7217236 -9.3888531 -9.07794][-2.973104 -3.6817918 -4.1664686 -4.5543861 -5.2500076 -5.7114067 -6.0207834 -6.0113344 -5.9310474 -7.3558855 -7.5691476 -8.0780134 -9.2608528 -9.8545752 -9.5142965][-2.8137798 -3.6794729 -4.0759516 -4.0162539 -4.1944351 -4.115438 -4.0523529 -3.9935029 -3.9953864 -5.5732169 -6.2491059 -6.8694878 -7.7332125 -8.579812 -8.6723585][-2.6159534 -2.9742436 -3.0604229 -2.76045 -2.9522119 -2.7474513 -2.1408792 -1.9818335 -1.9311414 -3.62177 -4.5596132 -5.8256569 -7.2811084 -8.0487022 -7.6808057][-4.0702491 -3.4054341 -1.8646994 -1.425282 -1.0088367 -0.25223732 0.092213154 0.10979366 0.20942163 -1.7138786 -2.8932905 -4.2287412 -5.9375858 -7.2260294 -7.1990538][-3.2465153 -3.1035724 -2.1436472 -0.96334171 0.3945055 1.5348997 2.3330231 2.4361906 2.1398935 0.035378933 -1.196486 -2.6881475 -4.6460104 -5.7305493 -5.7113681][-3.6198802 -3.5107393 -2.1011233 -0.4026351 1.2656536 2.4206171 3.2869892 3.4208536 3.2261419 0.69699955 -1.0221853 -2.6495037 -4.7008181 -5.8996387 -6.4074726][-2.761538 -2.1329775 -0.99306345 0.89365959 2.277916 3.1358147 3.6529188 3.7659311 3.541997 1.1198902 -0.69842386 -2.3701854 -4.6420164 -6.2074986 -6.6426659][-2.372745 -1.9292274 -0.37086296 1.3471622 2.2550039 2.654603 2.8172865 2.8117037 2.4614954 0.22084093 -1.4587379 -2.8466635 -4.9556322 -6.5762091 -7.1395478][-1.8756518 -1.653656 -0.85582495 0.50713634 1.2561531 1.6534138 1.8087807 1.5008173 0.96351051 -1.2452335 -2.5655975 -3.7490816 -5.4549685 -6.781435 -7.293664][-4.1658926 -3.7604091 -2.6436114 -1.3691325 -0.56039906 -0.19106436 -0.013494968 -0.24870205 -0.29030371 -2.172483 -3.831147 -4.7563992 -6.2809114 -7.3434024 -7.8001184][-4.5105438 -3.9949527 -3.5195351 -2.7723827 -2.2210498 -1.9445539 -1.8129144 -1.8859568 -1.8725262 -3.0764656 -4.2995272 -4.9827719 -6.2166224 -7.3532634 -7.6982322][-5.4821835 -5.5807028 -5.4673095 -4.9677477 -4.5889649 -4.086894 -3.6131978 -3.8114526 -3.7723057 -4.4623232 -5.5213585 -5.9037294 -6.7231741 -7.1535034 -7.0500803][-5.0147252 -5.382112 -5.775517 -5.385941 -4.7166724 -4.1953831 -4.0445719 -4.0557446 -3.6336398 -4.1700621 -4.7215614 -5.0829868 -5.9335041 -6.5968323 -6.9698606][-6.1639276 -5.7363534 -5.469883 -5.4253569 -5.600934 -5.4243383 -4.98209 -4.9835234 -4.9423118 -4.9244709 -5.3044844 -5.5048647 -5.8216767 -6.4478841 -6.9073763]]...]
INFO - root - 2017-12-15 15:45:41.148307: step 30610, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 53h:28m:24s remains)
INFO - root - 2017-12-15 15:45:47.520769: step 30620, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 54h:07m:31s remains)
INFO - root - 2017-12-15 15:45:53.882525: step 30630, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 55h:00m:53s remains)
INFO - root - 2017-12-15 15:46:00.243573: step 30640, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 52h:33m:06s remains)
INFO - root - 2017-12-15 15:46:06.641263: step 30650, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 53h:53m:31s remains)
INFO - root - 2017-12-15 15:46:13.096832: step 30660, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 52h:25m:08s remains)
INFO - root - 2017-12-15 15:46:19.458312: step 30670, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:36m:13s remains)
INFO - root - 2017-12-15 15:46:25.816643: step 30680, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 53h:11m:24s remains)
INFO - root - 2017-12-15 15:46:32.170507: step 30690, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 52h:07m:06s remains)
INFO - root - 2017-12-15 15:46:38.605292: step 30700, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 56h:21m:20s remains)
2017-12-15 15:46:39.211049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8374314 -4.8483524 -4.9630933 -5.27886 -6.1696696 -7.1474805 -7.2627816 -7.1017094 -6.6335135 -7.4160962 -8.93578 -8.1537361 -9.32459 -8.1448326 -8.5911465][-3.5019321 -3.8870287 -4.1599827 -5.1577654 -5.8405337 -6.1345043 -6.7065554 -6.99457 -6.7821488 -7.3045883 -8.2019663 -8.10132 -9.4028149 -8.6856251 -9.1616936][-3.5457668 -3.5306525 -3.975563 -4.4019318 -4.875627 -5.4378171 -5.8899736 -5.663372 -5.6943378 -6.432188 -7.1367564 -6.7215986 -7.9300961 -8.0520468 -9.14346][-2.4844599 -2.6042185 -3.1460247 -3.99382 -4.5900655 -4.7395172 -4.5119104 -4.041234 -3.5509148 -3.8250847 -5.0702252 -5.3092051 -7.0133953 -6.8565693 -7.9704895][-3.1331339 -2.8861103 -2.6844807 -2.2857018 -2.4512343 -2.4188018 -2.5534277 -2.1230164 -1.2451286 -1.5150986 -2.2600274 -2.43932 -5.252038 -6.394331 -7.9595785][-2.6854057 -2.4726591 -2.2528577 -1.9787431 -1.1340322 -1.0259991 -0.67601824 0.046546459 0.46270847 -0.12249279 -0.89018631 -0.83815765 -3.4493484 -4.5816231 -6.3236418][-1.9162021 -1.7210479 -1.405498 -0.53987217 -0.015712261 0.37746716 1.1646109 1.4388552 2.0516615 0.88467884 -0.52690506 -1.1279798 -3.689364 -4.7361355 -6.3767128][-1.4072919 -0.80103588 -0.60133934 -0.32558918 0.3972826 0.91749668 1.7305984 2.4051828 2.7762575 1.5183 -0.35753155 -1.1627455 -3.778357 -4.6990385 -5.9844975][-1.4038396 -0.087097645 -0.090651035 -0.20482492 0.25084019 0.93928432 1.7768764 1.7162418 2.0677404 1.0111313 -1.0155263 -1.4223094 -4.0940866 -5.0202503 -5.972321][-2.1864886 -0.82177877 -0.27917957 -0.20586586 0.048701286 0.20500851 0.81860447 0.71141338 0.59110069 -1.1170478 -2.9778419 -3.2411819 -4.8544569 -5.0828447 -6.0532446][-3.1209102 -2.6666975 -2.5919991 -2.6521921 -2.3959484 -2.2964659 -2.0608 -2.0708776 -1.9971962 -3.3889685 -5.1398954 -4.9353132 -6.2678576 -6.0779653 -6.7845311][-3.4134121 -3.5611978 -3.0781941 -3.6052361 -4.0752149 -4.0178328 -3.7296412 -3.7752869 -3.7127924 -4.8810134 -6.1454248 -5.5822949 -6.507154 -6.0379782 -6.0764389][-4.245451 -4.6906624 -4.7150154 -4.3862395 -4.9118757 -5.1214857 -5.10905 -5.3808694 -5.5086856 -5.7425623 -6.585269 -6.0510364 -5.9403696 -4.9379025 -5.1396365][-4.6927595 -5.436368 -5.699091 -5.2423525 -4.5992727 -4.6062164 -4.9416089 -5.2917576 -5.3575926 -4.8809872 -5.4461632 -5.2831297 -5.2064705 -4.8777065 -4.1600709][-5.392849 -5.48316 -5.358253 -5.7306023 -5.4988484 -5.0006785 -5.2782674 -5.6276374 -5.8548665 -5.8334036 -5.5873489 -5.266005 -5.3958282 -5.6092873 -5.2331867]]...]
INFO - root - 2017-12-15 15:46:45.653564: step 30710, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 53h:15m:07s remains)
INFO - root - 2017-12-15 15:46:52.048210: step 30720, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 53h:03m:15s remains)
INFO - root - 2017-12-15 15:46:58.376038: step 30730, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 53h:34m:25s remains)
INFO - root - 2017-12-15 15:47:04.741631: step 30740, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 54h:09m:22s remains)
INFO - root - 2017-12-15 15:47:11.087502: step 30750, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 51h:38m:34s remains)
INFO - root - 2017-12-15 15:47:17.447435: step 30760, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 53h:02m:18s remains)
INFO - root - 2017-12-15 15:47:23.830818: step 30770, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 54h:13m:42s remains)
INFO - root - 2017-12-15 15:47:30.215123: step 30780, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 53h:25m:26s remains)
INFO - root - 2017-12-15 15:47:36.633966: step 30790, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 55h:19m:19s remains)
INFO - root - 2017-12-15 15:47:43.010990: step 30800, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 53h:13m:52s remains)
2017-12-15 15:47:43.514419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.48982 -1.9320297 -2.4056211 -2.6762576 -2.7666101 -2.7226405 -2.5835781 -2.4910059 -2.1894503 -2.5751271 -3.6417227 -4.1852393 -5.2697682 -6.1868596 -6.7706356][-2.312727 -2.3973389 -2.5577402 -3.0889897 -3.4434209 -3.5247974 -3.1952047 -2.7096195 -2.0847564 -2.8162236 -4.3024454 -4.6223841 -5.8393726 -6.8311744 -7.04118][-3.4395285 -3.3135352 -3.0893908 -3.207366 -3.2238464 -3.3313127 -3.3833747 -3.1661611 -2.9053712 -3.672049 -4.8984995 -5.3714952 -6.2934985 -7.453279 -8.0571146][-4.2837324 -3.9678638 -3.4906902 -3.3168802 -2.8610778 -2.5815477 -2.215333 -1.9681988 -1.6139755 -2.5070205 -4.0402589 -4.8383989 -6.1505079 -7.4022903 -7.9929867][-4.3184109 -3.440311 -2.6244893 -2.2663341 -1.7483869 -0.78771639 -0.098727226 0.24080515 0.3982954 -0.8986516 -3.0285745 -4.3391428 -5.9172421 -7.13739 -7.7730045][-5.2326684 -4.3918524 -3.483242 -2.4356194 -0.98915005 0.5859251 1.6698122 1.9507246 1.7831392 0.16514492 -2.227818 -3.8454502 -5.827383 -7.3425779 -7.7650781][-4.7462626 -4.2306776 -3.5896392 -2.298501 -0.31973696 1.2665358 2.4449158 3.01437 3.1458845 1.7290344 -0.63234425 -2.3953748 -4.9384923 -6.906425 -7.6633635][-4.3345222 -3.6440663 -2.6019783 -1.3725457 0.81613827 2.8749228 4.27563 4.2217627 4.038518 2.6683378 0.4328146 -1.3948545 -3.6655774 -5.9126096 -7.0545206][-4.3207684 -3.9564741 -3.3742723 -2.3537331 -0.78720379 1.2277365 3.1042061 3.5962887 3.8233643 2.0541573 -0.36940289 -1.9861827 -4.0855236 -5.9089231 -6.7623181][-4.8284006 -4.5551338 -3.9218922 -3.3126216 -2.0994248 -0.87054682 0.44863129 1.4319553 2.119319 0.79159451 -1.1904998 -2.4794297 -4.6407566 -6.2023182 -6.6009851][-5.7726126 -5.6686544 -5.2086797 -4.5951309 -3.6283994 -2.2291222 -1.0044317 -0.8176322 -0.525033 -1.4178457 -2.8148279 -4.2170734 -5.9281149 -7.1600137 -7.4150362][-6.6590424 -6.6963778 -6.418189 -5.9953961 -5.4642391 -4.4899368 -3.6331267 -3.4729729 -3.0610757 -3.6964662 -4.3648391 -5.0429935 -6.0676007 -6.9715161 -7.4844713][-7.0630569 -7.0270896 -6.6677575 -6.5026455 -6.0720949 -5.592802 -5.2794628 -5.0469427 -4.86143 -5.3161454 -5.4612956 -5.4805994 -6.190053 -6.7281432 -6.9691696][-7.0778327 -6.9127021 -6.5204587 -6.210218 -5.8046637 -5.3092308 -5.2414455 -5.5632052 -5.5706587 -5.7078781 -5.852107 -5.8362885 -6.4707222 -6.4557228 -6.4366][-7.4291072 -7.724051 -8.0191813 -7.9158773 -7.7559285 -7.1004686 -6.46476 -6.5583553 -6.7106576 -6.9568162 -6.9417634 -6.69528 -6.5913754 -6.4329138 -6.2557983]]...]
INFO - root - 2017-12-15 15:47:49.901465: step 30810, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 55h:18m:08s remains)
INFO - root - 2017-12-15 15:47:56.448524: step 30820, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 54h:47m:07s remains)
INFO - root - 2017-12-15 15:48:02.780222: step 30830, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 52h:26m:41s remains)
INFO - root - 2017-12-15 15:48:09.181794: step 30840, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 54h:40m:05s remains)
INFO - root - 2017-12-15 15:48:15.598960: step 30850, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 54h:44m:11s remains)
INFO - root - 2017-12-15 15:48:21.894941: step 30860, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 52h:41m:28s remains)
INFO - root - 2017-12-15 15:48:28.361806: step 30870, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 55h:37m:34s remains)
INFO - root - 2017-12-15 15:48:34.717388: step 30880, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 52h:42m:28s remains)
INFO - root - 2017-12-15 15:48:41.136518: step 30890, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 54h:26m:09s remains)
INFO - root - 2017-12-15 15:48:47.491918: step 30900, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 53h:25m:42s remains)
2017-12-15 15:48:47.978758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2815652 -2.8580146 -2.9397607 -3.2310395 -3.2855225 -3.2878594 -3.2800031 -3.0339608 -2.7180715 -2.8374681 -3.6729565 -5.0903912 -6.2435765 -7.4478664 -8.3048725][-3.1241231 -3.3750987 -3.8197505 -4.4153481 -4.8851857 -4.89158 -4.578002 -3.9897199 -3.1611619 -3.2180252 -3.7021575 -5.1140122 -5.8981609 -7.0063872 -7.8280954][-2.1352344 -2.51796 -2.9907031 -3.6080933 -3.9519775 -3.9657295 -3.8072903 -3.6294093 -3.4003692 -3.4487629 -3.7992325 -4.6100674 -4.9057226 -5.9517717 -6.6684771][-1.8859773 -2.2692957 -2.7389874 -3.0814939 -3.2301874 -2.8831892 -2.6022744 -2.4447536 -2.3792896 -2.6181393 -3.0780826 -4.232049 -4.5958138 -5.2848139 -5.59608][-2.1944113 -1.9967432 -2.0813584 -2.1898475 -2.1025047 -1.3853946 -0.82733679 -0.55909729 -0.4873805 -1.3647814 -2.0404911 -3.286252 -3.9852481 -4.7007933 -5.2995973][-3.0065622 -2.8471541 -2.4971795 -1.7345529 -0.82337618 0.52208328 1.6076593 2.0656338 2.1909895 1.2410269 0.44906235 -1.1662149 -2.5045228 -3.954874 -5.2232733][-4.1234837 -3.4495692 -2.3782487 -1.3549733 0.050848961 1.3085213 2.4173584 2.8097744 3.1487141 2.3544703 1.610239 -0.36310339 -2.0949149 -3.4555497 -4.6839447][-4.3036919 -3.0531101 -2.008491 -0.77061129 0.80642319 2.2817059 3.445117 3.8630075 4.0647736 3.3656607 2.6035662 0.47387028 -1.3723626 -2.8049645 -3.941654][-4.6443558 -3.4478035 -2.4719849 -1.1193371 0.26348209 1.7269917 2.9279737 3.7190056 4.2411652 3.452878 2.2472191 0.056577682 -1.6834102 -2.9190454 -3.6746736][-5.150732 -4.0532422 -2.9616737 -1.6954598 -0.70631123 0.29057455 1.2406044 1.7665052 2.2674398 1.4563551 0.46707535 -1.3538675 -2.8370242 -3.8599513 -4.5306411][-6.52574 -5.8261509 -5.3953848 -4.1963296 -3.1032648 -2.0491805 -1.1556683 -0.81844521 -0.379323 -1.3047547 -2.1171007 -3.4170761 -4.4018869 -5.2384744 -5.750227][-8.3757572 -7.8922939 -7.3539414 -6.781528 -5.8912377 -4.8690634 -3.8793247 -3.1054325 -2.4524951 -3.3950958 -4.2273612 -5.1715136 -5.9269695 -6.4735665 -7.1313252][-8.7776146 -8.5798349 -8.2200108 -7.6805859 -6.9600277 -6.2785358 -5.5574169 -5.1602874 -4.7578039 -5.149374 -5.4555073 -5.8741608 -6.3697705 -6.7452688 -6.7876768][-7.7846832 -7.7171569 -7.8358054 -7.5607738 -6.9646187 -6.2782531 -5.8314657 -5.8301249 -5.9890404 -6.5831203 -6.9610314 -6.9036407 -6.6264887 -6.4169507 -6.3644524][-8.1681013 -7.8589592 -7.6875281 -7.4934449 -7.3088207 -6.9101028 -6.5978675 -6.8625784 -7.2332616 -7.51987 -7.7088952 -7.6012478 -7.2941356 -6.9400735 -6.4981771]]...]
INFO - root - 2017-12-15 15:48:54.497233: step 30910, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 51h:45m:38s remains)
INFO - root - 2017-12-15 15:49:00.807820: step 30920, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.610 sec/batch; 51h:08m:04s remains)
INFO - root - 2017-12-15 15:49:07.202009: step 30930, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 54h:01m:02s remains)
INFO - root - 2017-12-15 15:49:13.596423: step 30940, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 53h:37m:04s remains)
INFO - root - 2017-12-15 15:49:19.924309: step 30950, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 53h:46m:27s remains)
INFO - root - 2017-12-15 15:49:26.342292: step 30960, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 54h:34m:47s remains)
INFO - root - 2017-12-15 15:49:32.780320: step 30970, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 53h:27m:53s remains)
INFO - root - 2017-12-15 15:49:39.146948: step 30980, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.615 sec/batch; 51h:29m:45s remains)
INFO - root - 2017-12-15 15:49:45.475313: step 30990, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 52h:38m:16s remains)
INFO - root - 2017-12-15 15:49:51.875508: step 31000, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 54h:12m:07s remains)
2017-12-15 15:49:52.392711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4142752 -2.2058601 -2.0092349 -2.2251339 -2.5330615 -3.0454669 -3.4297657 -3.5521865 -3.6255631 -5.2410288 -5.4974747 -6.7993274 -7.7296185 -8.2702265 -8.2905283][-1.9826326 -2.1103878 -2.2838068 -2.6310582 -2.8421311 -3.170167 -3.155766 -3.4299016 -3.4232039 -5.0176229 -5.4849405 -6.99957 -7.905807 -8.6704435 -8.7821016][-2.5199924 -2.5053334 -2.6588283 -3.0286613 -3.0376582 -2.8514128 -2.7149925 -2.6468244 -2.7595849 -4.235424 -4.5446725 -5.84599 -6.9171371 -7.7237539 -8.0913944][-3.333909 -2.8973546 -2.9243464 -2.9914474 -2.717948 -2.4664707 -2.1964602 -2.345541 -2.3831248 -3.6836023 -3.8093467 -4.9874878 -5.7381186 -6.5431623 -6.8221784][-3.6979651 -3.4774256 -3.2991223 -2.7519999 -2.3735437 -1.7260332 -1.4389844 -1.547317 -1.4599719 -3.0415897 -3.4333196 -4.6173663 -5.0447168 -5.8257174 -5.9507313][-4.1248837 -3.5810618 -2.9690294 -2.1058879 -1.1556983 -0.34330893 0.18562031 0.032573223 -0.2634697 -1.825047 -2.3813124 -3.7415702 -4.4894171 -5.3129268 -5.3749542][-4.0754051 -3.5449638 -3.171176 -2.0182528 -0.85985231 0.17902565 0.73404312 0.50911236 0.56552696 -0.96201468 -1.4577951 -2.9031463 -4.0172739 -4.9820871 -5.3172207][-3.5684366 -2.5383115 -1.8370376 -0.9950819 -0.2302475 0.53799438 1.1354914 0.99806976 1.1112013 -0.238729 -0.58850288 -2.2480898 -3.2768393 -4.2584257 -4.8095393][-2.9525361 -2.4431686 -1.817296 -0.857996 0.0068426132 0.90499306 1.2175131 1.328124 1.3501282 -0.3205018 -0.70364428 -2.3727665 -3.5634079 -4.5315356 -4.9579334][-2.5351725 -2.5082054 -2.1958261 -1.0915937 -0.31563187 0.23930883 0.49511909 0.58342648 0.76990891 -0.74795055 -1.3062797 -3.0620537 -4.4333749 -5.4451246 -5.800386][-3.5112929 -3.0592399 -2.7218165 -2.3502598 -1.7643661 -0.80822706 -0.63073397 -0.63848209 -0.58929634 -1.9150391 -2.8110752 -4.4314966 -5.7231131 -6.6089749 -6.7424107][-3.4798126 -4.2188768 -4.3801184 -3.7128096 -3.0918608 -2.5140038 -2.2513194 -1.9786925 -1.8375497 -3.0159721 -3.7857549 -5.1086149 -6.410996 -7.3031483 -7.490519][-4.634922 -5.5085897 -5.8300033 -5.6305218 -4.9651656 -4.0343409 -3.96154 -3.9499834 -3.8229527 -4.5688181 -5.0974274 -6.1957669 -6.9462528 -7.6403952 -7.814002][-5.7266464 -5.8085847 -5.8316097 -5.8270693 -5.3837509 -4.8560567 -4.5631638 -4.4034829 -4.4022465 -5.0656848 -5.439929 -6.1007047 -6.6277966 -7.2662425 -7.6021256][-6.7810287 -7.2068205 -7.2814307 -6.6827645 -6.2117224 -5.898623 -5.7923703 -5.5889683 -5.5856681 -5.8314905 -6.3059015 -6.9478579 -7.1266756 -7.46591 -7.4769382]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 15:49:58.877696: step 31010, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 55h:04m:49s remains)
INFO - root - 2017-12-15 15:50:05.289854: step 31020, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 53h:40m:53s remains)
INFO - root - 2017-12-15 15:50:11.650074: step 31030, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.644 sec/batch; 53h:57m:01s remains)
INFO - root - 2017-12-15 15:50:17.992805: step 31040, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 52h:33m:54s remains)
INFO - root - 2017-12-15 15:50:24.318227: step 31050, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 54h:59m:18s remains)
INFO - root - 2017-12-15 15:50:30.707915: step 31060, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.669 sec/batch; 56h:03m:27s remains)
INFO - root - 2017-12-15 15:50:37.042409: step 31070, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 52h:18m:29s remains)
INFO - root - 2017-12-15 15:50:43.407463: step 31080, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.633 sec/batch; 52h:58m:16s remains)
INFO - root - 2017-12-15 15:50:49.820244: step 31090, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 54h:33m:21s remains)
INFO - root - 2017-12-15 15:50:56.236854: step 31100, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 53h:12m:36s remains)
2017-12-15 15:50:56.759802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.276968 -4.7176571 -4.3183203 -3.8500586 -4.0990191 -3.9035792 -3.8763788 -3.997915 -3.4175682 -3.2606182 -3.8706889 -4.9574928 -5.7552967 -6.8905449 -7.5335069][-4.4652576 -3.9925475 -3.9187295 -3.7354391 -4.0305562 -3.9846437 -3.779228 -3.6920991 -3.7386198 -3.7713392 -4.1303449 -5.0496187 -6.0533953 -6.698555 -7.466413][-3.1189394 -3.0926118 -2.9089775 -2.4912591 -2.5633731 -2.6087909 -2.7234707 -2.8303056 -2.2369962 -2.4163556 -3.4302521 -4.7493868 -5.6301708 -6.098217 -6.5033665][-1.9724107 -1.9629784 -1.8767519 -1.231421 -0.84850454 -0.70472622 -0.45116854 -0.25624561 -0.036194324 -0.2044363 -1.2768474 -2.9756727 -4.4598265 -5.5967054 -6.0942812][-1.5147882 -0.76876593 -0.35677195 -0.2556653 0.13776493 0.78792 1.1652403 1.2569017 1.3828287 0.895607 -0.18360424 -2.1588621 -3.7170527 -4.8540649 -5.6558638][-1.1142354 -0.3874712 0.069393635 0.6873312 1.0582876 1.4115744 2.0802717 2.2116776 2.1790676 1.2873726 -0.12572145 -2.1835213 -3.6407084 -4.75145 -5.5863371][-1.6706471 -0.45389271 0.44206619 0.78331375 1.5561056 2.1411772 2.2937632 2.4018993 2.5617609 1.6389627 0.19001102 -2.1840906 -4.0821028 -5.0656309 -5.5359473][-1.4994864 -0.54747295 0.41459846 1.1118631 1.9213514 2.5774069 3.1981039 3.216218 3.181777 1.9389982 -0.10096455 -2.4451404 -3.9825187 -5.2518687 -6.2053013][-1.5369925 -0.65959692 -0.32847834 0.21742582 1.0100794 2.2168684 2.8165531 2.9249601 2.8949203 1.8307285 0.11745691 -2.5685558 -4.5101805 -5.3949051 -5.8588829][-2.1716323 -1.3955469 -1.043088 -0.70841122 -0.6047225 0.087087154 0.90413857 1.6248646 2.0227222 0.64856339 -1.0256968 -2.8866439 -4.307147 -5.9962707 -6.984715][-3.7363522 -3.7223072 -3.4355392 -2.7499943 -2.5354376 -2.0921092 -1.4684258 -1.2590761 -0.82018948 -1.0401378 -1.974905 -4.0909328 -5.499969 -5.8030796 -6.3000178][-5.3735247 -4.6036344 -4.1656704 -3.8687959 -3.6309347 -2.77426 -2.304656 -1.7635312 -1.2820411 -2.302124 -2.8912663 -3.7427802 -4.860003 -6.4282794 -7.3617859][-6.7403488 -5.7292881 -4.7758088 -3.919414 -3.8390479 -3.7969058 -3.2433686 -2.9514103 -2.9246349 -3.1223969 -3.2566347 -4.1505561 -4.6534395 -5.3020296 -5.9214344][-6.4297533 -5.6713133 -4.6856704 -3.839494 -2.8940921 -2.243371 -1.622489 -1.7969933 -1.9320793 -2.4841228 -2.7516656 -3.1539578 -4.0100026 -5.0747719 -5.5325556][-6.5283623 -5.5232 -4.7577181 -3.9609506 -3.3087802 -3.1419835 -2.6607227 -2.3355799 -2.4925966 -2.5741591 -2.793149 -3.0724459 -3.88068 -4.6792507 -5.4973078]]...]
INFO - root - 2017-12-15 15:51:03.160795: step 31110, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 53h:27m:43s remains)
INFO - root - 2017-12-15 15:51:09.492071: step 31120, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 52h:41m:44s remains)
INFO - root - 2017-12-15 15:51:15.868764: step 31130, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 54h:21m:24s remains)
INFO - root - 2017-12-15 15:51:22.238538: step 31140, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 53h:59m:31s remains)
INFO - root - 2017-12-15 15:51:28.677040: step 31150, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 53h:13m:14s remains)
INFO - root - 2017-12-15 15:51:34.971642: step 31160, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 52h:03m:36s remains)
INFO - root - 2017-12-15 15:51:41.335088: step 31170, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 53h:35m:50s remains)
INFO - root - 2017-12-15 15:51:47.707786: step 31180, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 53h:42m:46s remains)
INFO - root - 2017-12-15 15:51:54.099578: step 31190, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 54h:55m:54s remains)
INFO - root - 2017-12-15 15:52:00.583451: step 31200, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 53h:46m:49s remains)
2017-12-15 15:52:01.104171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9836054 -5.2861047 -5.2439795 -5.5352106 -5.6905904 -5.5705748 -5.5371437 -4.6945944 -3.6074629 -2.8023643 -2.902667 -3.9298079 -4.9080963 -6.1799192 -7.076685][-3.9785883 -4.6927924 -5.1371069 -5.6405487 -5.8912048 -6.0733948 -6.0246644 -5.4847679 -4.3781633 -3.0768585 -2.658555 -3.482492 -4.2463083 -5.4101019 -6.5134988][-3.672966 -3.7385898 -4.01992 -4.5774393 -5.2593651 -5.4708838 -5.613369 -5.4079037 -4.6435919 -3.9432085 -3.6670427 -4.0740891 -4.6157303 -5.6118579 -6.3748894][-4.4848533 -4.3562531 -3.844095 -3.268671 -3.5440187 -3.846525 -3.976855 -3.9657822 -3.529716 -3.0675473 -3.0315099 -3.7442291 -4.67138 -5.71835 -6.6983628][-3.6843276 -2.9147377 -2.4598804 -2.2482119 -1.8577399 -1.870584 -1.6345372 -1.3168693 -0.78715897 -0.40648651 -0.91188812 -2.19599 -3.380003 -4.7923384 -6.0049734][-2.3122668 -1.7491941 -1.7219844 -1.2077789 -0.72678995 -0.22220087 0.64537334 1.0592165 1.3214369 1.4061489 0.18324184 -1.6420245 -3.2137341 -4.8555202 -5.8988442][-2.5285211 -1.7831893 -0.94404125 -0.21755552 0.47972584 1.2046337 1.9058275 2.2333393 2.6999416 2.632081 1.0179625 -1.2205615 -2.9239702 -4.6258979 -6.0515385][-2.471952 -1.6793213 -1.0414119 0.19087505 1.6021051 2.1893797 2.6149511 2.9559116 3.4063396 2.9794874 1.50842 -0.68608475 -2.6736388 -4.4334354 -5.6460834][-2.1891513 -1.4144621 -1.1301236 -0.45832014 0.64125919 1.6231289 2.237813 2.6215286 2.8485498 2.480113 1.0938997 -1.0377197 -2.9098673 -4.7123308 -5.5333738][-2.8523164 -2.2015443 -1.4605665 -0.84415245 -0.033595562 0.58542633 0.8797884 1.3544655 1.4096775 0.81095791 -0.74831915 -2.5900779 -3.8836143 -5.2386465 -6.2275791][-4.8805761 -4.4082928 -3.9051952 -2.9287009 -1.6857653 -0.96846485 -0.68942308 -0.76415825 -0.94777441 -1.6537771 -2.3824468 -3.7493804 -4.7339234 -5.9249706 -6.5549445][-7.0604305 -6.749598 -6.4203939 -5.5449252 -4.6712484 -3.707541 -3.3094044 -3.416738 -3.5672402 -4.0879326 -4.5984693 -5.5164371 -5.9963045 -6.6914597 -7.0199323][-8.21058 -8.4789009 -8.48986 -7.8953223 -6.975049 -6.0646024 -5.2259493 -4.9953461 -5.1629171 -5.250689 -5.8947244 -6.37119 -6.5251031 -6.7191505 -6.5899353][-8.3266077 -8.0416889 -7.858285 -7.6246347 -7.2945433 -6.64057 -6.0913148 -5.8831525 -5.7508321 -5.9585385 -6.0167203 -6.01262 -6.0494404 -6.5464215 -6.6422906][-9.3314428 -9.1065187 -8.6093817 -8.26374 -7.9512615 -7.09489 -6.596879 -6.7492933 -7.0703225 -7.1856003 -6.9478493 -6.9302149 -6.6181021 -6.6364279 -6.5720863]]...]
INFO - root - 2017-12-15 15:52:07.459636: step 31210, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 53h:51m:24s remains)
INFO - root - 2017-12-15 15:52:13.803582: step 31220, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 53h:23m:31s remains)
INFO - root - 2017-12-15 15:52:20.194377: step 31230, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 52h:31m:00s remains)
INFO - root - 2017-12-15 15:52:26.574931: step 31240, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 52h:54m:34s remains)
INFO - root - 2017-12-15 15:52:32.991247: step 31250, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 52h:14m:42s remains)
INFO - root - 2017-12-15 15:52:39.353032: step 31260, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 52h:48m:39s remains)
INFO - root - 2017-12-15 15:52:45.673045: step 31270, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 52h:26m:15s remains)
INFO - root - 2017-12-15 15:52:52.144689: step 31280, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 54h:40m:27s remains)
INFO - root - 2017-12-15 15:52:58.621038: step 31290, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 55h:16m:35s remains)
INFO - root - 2017-12-15 15:53:05.104093: step 31300, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 53h:10m:46s remains)
2017-12-15 15:53:05.663245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5654244 -2.6444492 -3.1386514 -3.4325452 -3.6197963 -3.1221271 -2.9197459 -3.2286949 -3.736624 -3.8282213 -5.6870384 -7.700676 -7.6809597 -8.7621727 -8.5296354][-2.8837051 -3.5547018 -4.4756775 -5.0376844 -5.2446146 -4.8518763 -4.3976526 -4.2186136 -4.4547648 -4.6361647 -6.3342934 -8.3100557 -8.462471 -9.2763529 -10.168791][-3.4384551 -4.3241229 -4.6284275 -5.0683918 -5.90292 -5.5688095 -4.7617073 -4.7621183 -4.3625922 -4.5917139 -5.8902221 -7.7667222 -7.7976265 -9.048748 -9.4206619][-4.2393837 -5.0983982 -5.4722528 -5.5907726 -6.0899739 -5.4282618 -4.9868851 -4.3859262 -4.2033472 -4.312809 -5.5717349 -7.7871475 -7.426229 -8.5925789 -8.7842169][-4.9165716 -6.100174 -6.2554111 -6.0938396 -6.0667262 -5.0173221 -3.8985207 -3.4694004 -3.5155468 -3.9088652 -5.3476238 -7.2312841 -6.9735374 -8.1620312 -8.4414482][-5.8685784 -7.0924845 -7.1986041 -6.569222 -5.5156536 -4.0196109 -2.5797715 -2.1943035 -2.0218759 -2.4747705 -4.4904051 -6.8910871 -6.7314181 -7.8245544 -8.2950745][-7.0038176 -7.6171751 -7.34405 -6.1897864 -5.1169653 -3.3696876 -1.3259788 -0.53471851 -0.29826355 -0.74288273 -2.7219324 -5.3411732 -5.5902414 -6.6125546 -7.0219936][-5.4951048 -6.5025806 -6.8404417 -5.5437531 -4.1013203 -2.1231828 -0.10374069 0.64856148 1.137743 0.42549324 -1.8112931 -4.5645113 -4.1998553 -4.85303 -4.7330818][-5.521656 -6.1209168 -5.8748765 -4.6679735 -3.511374 -1.6301241 0.39555836 1.5468388 1.7291412 1.1370573 -1.1954198 -4.0768337 -4.427124 -5.4564524 -5.1827602][-6.0542855 -5.9220572 -5.3276963 -4.5191584 -3.8735695 -2.7284422 -1.333015 -0.23128557 0.39478779 -0.70257139 -3.283215 -5.761261 -5.8763328 -6.7711563 -6.762013][-7.6603174 -7.5699606 -7.1903319 -5.5144796 -4.8516631 -4.1422715 -3.2169223 -2.8639541 -2.4841437 -2.8342166 -4.0868406 -6.3486366 -6.6559181 -6.8184071 -6.9939752][-6.0628357 -6.605938 -6.6423664 -5.7071795 -5.2670937 -4.4347639 -4.1378374 -4.0335321 -3.4339933 -3.8691826 -4.0990133 -5.9716034 -6.0749731 -6.4574494 -6.9177537][-7.0837326 -6.4686327 -6.40941 -5.9892135 -5.7436914 -5.172554 -4.8491297 -4.8191285 -4.8891997 -4.9633636 -4.9226875 -6.0175209 -5.5520611 -5.6080976 -5.9156137][-7.8658342 -7.0115409 -5.9865484 -5.3114872 -4.9408069 -4.5427728 -4.5083818 -4.5780077 -4.5100312 -4.8055182 -4.6752672 -5.122303 -4.9992051 -5.2265654 -5.58579][-9.8829594 -10.269741 -9.4977684 -8.0166674 -7.0812926 -6.4577141 -6.2382517 -6.2340403 -6.2474079 -6.3009481 -6.1904864 -6.2428026 -6.0556297 -6.0387673 -5.7767086]]...]
INFO - root - 2017-12-15 15:53:12.161339: step 31310, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 54h:15m:54s remains)
INFO - root - 2017-12-15 15:53:18.690379: step 31320, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 55h:12m:05s remains)
INFO - root - 2017-12-15 15:53:25.128154: step 31330, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 53h:21m:32s remains)
INFO - root - 2017-12-15 15:53:31.465098: step 31340, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 53h:03m:19s remains)
INFO - root - 2017-12-15 15:53:37.900230: step 31350, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 53h:32m:42s remains)
INFO - root - 2017-12-15 15:53:44.168966: step 31360, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 52h:21m:03s remains)
INFO - root - 2017-12-15 15:53:50.560548: step 31370, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 52h:05m:13s remains)
INFO - root - 2017-12-15 15:53:56.972667: step 31380, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 54h:30m:56s remains)
INFO - root - 2017-12-15 15:54:03.330758: step 31390, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 54h:21m:42s remains)
INFO - root - 2017-12-15 15:54:09.728870: step 31400, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 53h:34m:31s remains)
2017-12-15 15:54:10.237558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6510568 -2.5018811 -3.323091 -3.6299214 -3.590312 -3.5081062 -3.1335087 -2.5874081 -1.9768906 -2.6965189 -3.0563478 -5.1849971 -5.8303895 -6.131691 -7.0040894][-2.4739542 -2.81283 -3.4425788 -4.4914517 -5.020092 -4.4959893 -3.3297772 -2.7996869 -2.5498819 -3.6338429 -4.1937141 -6.2656212 -6.9164286 -7.4940033 -8.3017483][-3.2963138 -3.604434 -4.1794991 -4.5764322 -4.7001472 -4.5554914 -3.7071669 -3.2328959 -2.6360383 -3.8774824 -4.9461107 -7.0577416 -7.5176344 -8.1597271 -9.1707907][-4.1443176 -4.2049503 -4.454215 -4.4276433 -4.2124357 -3.3582659 -2.0504179 -1.56248 -1.4418035 -2.8428664 -3.5845208 -6.0261507 -7.1541305 -8.0035353 -8.5942278][-5.3132091 -5.3060274 -5.5551271 -5.0732064 -4.3259382 -2.8745236 -1.0009069 0.10121107 0.52424622 -1.346086 -3.026175 -5.836463 -6.8080215 -7.8251548 -8.9061642][-6.8676987 -7.0561323 -6.7269244 -5.1726809 -3.6601787 -1.545155 1.0121593 2.1815872 2.7253513 0.69028282 -1.722496 -5.3035278 -7.0138392 -8.0066738 -8.906395][-6.9588652 -6.9281683 -6.8963275 -4.9181719 -2.4115596 0.56959915 3.5306692 5.1912842 5.9502611 3.75638 1.0482159 -3.3854795 -5.7032318 -7.2793632 -8.1225605][-6.5484834 -6.4995813 -5.9031425 -4.1171341 -2.0374637 1.4197159 4.7545309 6.3928652 7.3675289 5.13684 2.4002924 -1.8603787 -4.6496592 -6.295661 -7.376646][-6.8361344 -6.195148 -5.8098106 -4.2630386 -2.3262429 0.61912537 3.0493393 4.851428 6.2207127 4.0769644 1.6140842 -2.2821679 -4.3166881 -5.9754953 -7.4567685][-6.5779934 -5.9263797 -5.4177885 -4.0263481 -2.8029165 -0.97191811 0.81923866 2.0433836 2.8270931 0.62853622 -0.88782406 -3.9883223 -5.2651706 -5.9712663 -7.1623745][-6.9440866 -6.5954943 -6.1670613 -5.0934443 -4.1098108 -2.9883137 -2.2910867 -1.6341124 -0.88439178 -2.3232422 -3.7601993 -6.2964559 -6.5928297 -6.7658634 -6.9473267][-7.0320277 -6.9328308 -6.3997407 -5.471211 -4.8834515 -3.9351926 -2.9971008 -2.9438252 -2.9108205 -3.6996636 -4.4724903 -6.5460339 -6.5260181 -6.8984642 -7.6066837][-7.9526868 -7.4770045 -6.9885545 -6.0237961 -5.2715693 -4.715858 -4.4905014 -4.3134704 -4.0618591 -5.0874829 -5.65347 -6.5784297 -6.4311752 -6.9887733 -7.1948843][-7.9398074 -8.074255 -7.4950528 -6.3634319 -5.7919455 -5.0269327 -4.3598804 -4.6533208 -5.0689011 -5.5564594 -5.5356369 -6.1087074 -5.9312139 -6.171762 -5.9547005][-8.2621546 -8.423584 -8.1622458 -7.2027593 -6.538527 -5.9491463 -5.88625 -5.7577162 -5.6459284 -5.9129796 -6.1352072 -5.8689213 -5.3653469 -5.1224422 -4.8902254]]...]
INFO - root - 2017-12-15 15:54:16.648240: step 31410, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 53h:13m:38s remains)
INFO - root - 2017-12-15 15:54:23.223646: step 31420, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 53h:43m:33s remains)
INFO - root - 2017-12-15 15:54:29.658274: step 31430, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 53h:10m:15s remains)
INFO - root - 2017-12-15 15:54:36.082832: step 31440, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 54h:19m:14s remains)
INFO - root - 2017-12-15 15:54:42.492623: step 31450, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 54h:03m:05s remains)
INFO - root - 2017-12-15 15:54:48.941247: step 31460, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 53h:20m:46s remains)
INFO - root - 2017-12-15 15:54:55.430869: step 31470, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 52h:40m:19s remains)
INFO - root - 2017-12-15 15:55:01.963315: step 31480, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 55h:46m:21s remains)
INFO - root - 2017-12-15 15:55:08.458664: step 31490, loss = 0.24, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 54h:15m:00s remains)
INFO - root - 2017-12-15 15:55:14.913222: step 31500, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 55h:25m:28s remains)
2017-12-15 15:55:15.403210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7357802 -4.831233 -4.8690996 -4.4404154 -3.7239091 -3.4366179 -2.6629157 -2.0986528 -1.7573357 -2.10462 -2.9668622 -5.4008865 -6.5486 -7.653697 -8.1663971][-5.6655073 -5.6933317 -5.9435329 -6.0096955 -5.2041874 -4.348186 -3.4902968 -3.0271897 -2.617156 -3.4017081 -4.6913204 -7.5644236 -8.6992579 -9.4913445 -10.125767][-4.9927359 -4.8508358 -5.1298633 -5.0831547 -4.44236 -3.7740474 -2.9770293 -2.8054695 -2.97091 -3.6882787 -4.6863508 -7.5304828 -8.5292425 -9.3410854 -9.7035656][-4.9163446 -5.1673536 -4.8496609 -4.0554619 -3.3811903 -2.4195743 -1.2133293 -1.2800884 -1.9607391 -3.2137313 -4.7830815 -7.3596716 -8.4133654 -9.4382725 -9.9573307][-6.4025531 -6.3996234 -5.9664326 -4.963932 -3.5731974 -2.0782523 -0.86860323 -0.61833572 -1.0404472 -2.3846507 -3.9065623 -6.8155084 -8.0501776 -8.8524275 -9.1051636][-6.9502029 -6.7465472 -6.37765 -5.3052225 -3.9324765 -2.2947617 -0.85081339 -0.47876978 -0.60184145 -2.1849742 -3.8971264 -6.515111 -7.5552282 -8.1017971 -8.5492258][-6.692441 -6.2386804 -5.5233116 -4.2163491 -2.6086574 -0.68724489 0.71472931 1.1603146 1.0835505 -0.47468376 -2.3454995 -5.4046636 -6.7306466 -7.5454531 -8.0169258][-6.2060475 -5.6830111 -4.6452975 -3.0859294 -1.368917 0.5356493 2.051527 2.1199732 1.8778305 0.41169739 -1.6285439 -5.07969 -6.8184128 -7.5987926 -7.9732003][-4.7928085 -4.0737381 -3.4860196 -2.1065207 -0.81000566 0.85997963 2.2080612 2.5685959 2.8101864 1.4677792 -0.26304197 -3.9183209 -5.7690291 -6.9044461 -7.2870793][-3.7918313 -2.9550991 -2.6493502 -1.6278853 -0.56529 0.41217422 1.4055767 1.5919743 1.5589581 -0.37597609 -1.7656121 -4.8242574 -5.9419966 -6.4839449 -6.6637669][-3.150568 -2.8709979 -2.7968707 -2.08216 -1.5077739 -0.6148386 0.087004185 0.12247849 0.0051879883 -1.6415362 -3.1283498 -6.129065 -6.9470968 -6.8541141 -6.5016894][-2.9966869 -2.8706846 -2.806829 -2.1433821 -1.9323802 -1.5286522 -0.98921394 -1.2754364 -1.8276396 -2.8372064 -4.19189 -6.8007474 -7.5845447 -7.4055247 -6.7919717][-4.2128286 -4.2199364 -4.15037 -3.7755547 -3.7523057 -3.3046894 -3.0162382 -3.30341 -3.3356152 -4.1450071 -4.764616 -5.9761562 -6.1809874 -6.4907656 -6.1218705][-4.848896 -4.8742013 -4.332993 -3.4650974 -3.0123982 -2.4603758 -2.361237 -2.7861376 -3.3403535 -4.3191433 -4.8177166 -5.5134687 -5.4195023 -5.4788866 -5.4383698][-5.435914 -5.613678 -5.0801334 -4.629858 -4.5709386 -4.1438408 -4.1615319 -4.056716 -4.0976162 -4.7126288 -4.74631 -4.9621477 -5.1431971 -5.439805 -5.482338]]...]
INFO - root - 2017-12-15 15:55:21.867564: step 31510, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 52h:57m:54s remains)
INFO - root - 2017-12-15 15:55:28.321326: step 31520, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:20m:55s remains)
INFO - root - 2017-12-15 15:55:34.703567: step 31530, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 53h:37m:29s remains)
INFO - root - 2017-12-15 15:55:41.195561: step 31540, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 52h:43m:56s remains)
INFO - root - 2017-12-15 15:55:47.626003: step 31550, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 54h:33m:19s remains)
INFO - root - 2017-12-15 15:55:54.009639: step 31560, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 53h:26m:25s remains)
INFO - root - 2017-12-15 15:56:00.412659: step 31570, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 52h:26m:41s remains)
INFO - root - 2017-12-15 15:56:06.826351: step 31580, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 56h:02m:39s remains)
INFO - root - 2017-12-15 15:56:13.289524: step 31590, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.655 sec/batch; 54h:47m:17s remains)
INFO - root - 2017-12-15 15:56:19.614413: step 31600, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 53h:12m:48s remains)
2017-12-15 15:56:20.178884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5968404 -6.66901 -7.1272645 -6.8868003 -6.8245773 -6.3777819 -5.3177328 -4.2775421 -3.2174268 -5.0144043 -5.8399668 -5.69481 -6.0825472 -7.0885606 -7.6076589][-6.0499897 -6.3938012 -6.7257881 -7.3237238 -7.5955067 -7.2026339 -6.55575 -5.4947643 -4.2630043 -5.95183 -6.6037145 -6.7215576 -7.4479322 -8.2053976 -8.8039885][-5.0410089 -5.5498333 -6.0365915 -6.2835007 -6.5697546 -6.6815748 -6.2087693 -5.4454808 -4.6870847 -6.503119 -6.9631243 -6.9643517 -7.476872 -8.6030388 -9.1555977][-5.7477961 -5.3835325 -5.358027 -5.3450174 -5.5405169 -4.8949471 -4.15849 -3.6733322 -3.1138768 -5.4176235 -6.4743204 -6.9899054 -7.4727387 -8.3127317 -8.9421778][-7.2530375 -6.4175129 -5.3326035 -4.5066833 -4.1057129 -3.1731348 -2.3028498 -1.8115892 -1.6279755 -4.2242284 -5.51099 -6.0802417 -6.6929073 -7.7475958 -8.3791943][-8.6665792 -7.470861 -6.3335619 -4.1896753 -2.0941768 -0.66477489 0.33108521 0.70545006 0.6919508 -2.277905 -4.3837614 -5.191082 -5.7831273 -7.1562824 -8.4854183][-8.8509407 -7.1548386 -5.0227485 -2.2323246 0.15898514 2.1604443 3.7388439 4.0923271 3.9380465 0.064545155 -2.5839982 -3.8766015 -5.0057831 -6.2215252 -7.2079263][-7.917716 -6.768301 -5.5364761 -2.5909119 0.86853313 2.9262476 4.3405676 5.0777397 5.1237221 0.906929 -2.1582499 -3.8410981 -4.6188164 -5.942265 -6.9193487][-8.6411238 -7.9823155 -6.7553625 -3.9450541 -1.1792989 0.99944878 2.6639481 3.351387 3.5486326 -0.22700834 -3.0979662 -4.3791609 -5.2427721 -6.13804 -6.5409203][-9.9333935 -8.9347792 -7.7114511 -4.944766 -1.9228215 0.042015553 1.5168085 1.8148832 1.3631172 -2.4907889 -4.5260868 -5.7373242 -6.7898231 -7.5179725 -7.4798021][-10.820443 -10.284958 -9.4902916 -7.4538617 -5.9406004 -3.8519208 -1.8661914 -1.9260626 -2.333169 -5.3833685 -6.6725936 -7.48232 -8.06811 -8.8406754 -8.8992147][-11.24556 -10.487414 -9.5888329 -8.9059258 -8.1538925 -6.7937918 -6.0950594 -5.5756583 -5.0755877 -7.5561104 -8.5240965 -8.7628956 -9.0096292 -9.3110981 -8.942729][-11.805326 -11.049435 -10.259377 -9.514864 -8.6413412 -7.8109241 -7.151897 -6.94714 -7.1613388 -9.091403 -9.7518845 -9.69385 -9.8352814 -9.4394436 -8.9373722][-10.825478 -10.27982 -10.028824 -9.3561163 -8.4321909 -7.4518919 -6.8850594 -6.9667912 -7.2836404 -8.3346357 -8.4621792 -8.8897638 -9.3556242 -9.0146894 -8.07854][-10.157852 -9.5447817 -9.2900524 -8.8088436 -8.4668255 -7.7167859 -7.0459156 -7.0685439 -7.176837 -7.5314069 -7.645462 -7.5729465 -7.2500386 -6.9825573 -6.5265703]]...]
INFO - root - 2017-12-15 15:56:26.639958: step 31610, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 52h:38m:27s remains)
INFO - root - 2017-12-15 15:56:33.083011: step 31620, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 52h:48m:18s remains)
INFO - root - 2017-12-15 15:56:39.491834: step 31630, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 55h:19m:14s remains)
INFO - root - 2017-12-15 15:56:45.886982: step 31640, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 54h:13m:30s remains)
INFO - root - 2017-12-15 15:56:52.317059: step 31650, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 52h:48m:25s remains)
INFO - root - 2017-12-15 15:56:58.845947: step 31660, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 54h:32m:14s remains)
INFO - root - 2017-12-15 15:57:05.299620: step 31670, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 53h:22m:38s remains)
INFO - root - 2017-12-15 15:57:11.698746: step 31680, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 54h:28m:54s remains)
INFO - root - 2017-12-15 15:57:18.165308: step 31690, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 55h:18m:36s remains)
INFO - root - 2017-12-15 15:57:24.588920: step 31700, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 53h:52m:34s remains)
2017-12-15 15:57:25.123566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1539578 -6.23347 -6.3620524 -5.9747858 -5.2876787 -3.9821842 -2.5332475 -0.8886838 0.4137125 0.19213867 -1.0300894 -3.9653137 -6.4507823 -8.6862307 -9.6542816][-5.07471 -5.4045887 -6.0766091 -6.2360449 -6.1042223 -4.918982 -3.0233994 -0.86461926 0.53525543 0.31477928 -1.1480556 -3.6827426 -5.8612285 -8.4661055 -9.8726807][-5.1906786 -5.2253771 -5.3505378 -4.9301362 -4.290834 -3.4501834 -2.2873878 -0.99459887 0.363616 -0.10868597 -1.2877917 -3.8196783 -5.5902071 -7.1187768 -8.4051][-5.0328455 -4.5238562 -4.3672533 -3.7919633 -3.3240666 -2.3174462 -1.0163684 -0.2506237 0.5583601 0.04715538 -0.92765665 -3.5495648 -5.2471991 -6.7502823 -7.6654344][-5.012352 -4.2827654 -3.4525537 -2.7092986 -1.9142942 -0.65964413 0.57911873 1.2800741 1.6581602 0.78146839 -0.75124025 -3.2641835 -5.0055618 -6.6519976 -7.5609827][-4.8265524 -3.8440175 -2.5083985 -1.539001 -0.37193394 0.86907578 2.549078 3.0038977 3.2514954 2.1598759 0.29239511 -2.6946669 -4.9022884 -6.4044857 -7.2034917][-4.4570909 -3.1185226 -1.8247113 -0.78372145 0.51402283 1.5944033 3.0439711 3.8690281 4.4408283 2.9263372 0.98281765 -2.3291173 -4.6782775 -6.4291644 -7.3716207][-3.9228075 -2.4913898 -0.91397381 0.12875223 1.3659019 1.9877138 2.6631317 3.6037025 4.2576189 3.2811108 1.254755 -1.9916873 -4.553124 -6.564343 -7.6442046][-3.1857157 -1.9599895 -0.53942013 0.61445332 1.8903446 2.3204813 2.7760811 3.1713877 3.4862633 2.3542337 0.76840591 -1.8910017 -4.3729658 -6.6414618 -8.0943995][-2.5882726 -1.6591973 -0.2347188 0.69382572 1.7212629 2.2139874 2.8198624 2.9993649 3.13054 1.2952728 -0.64763737 -3.1364126 -5.1012297 -6.7089214 -7.79105][-2.2529664 -1.6023188 -0.72339153 0.30784893 1.3653879 1.5732594 1.9319677 2.0407047 2.1076441 0.22501755 -2.1459365 -4.1916122 -5.50747 -6.9583554 -7.9326472][-2.9350686 -1.8991046 -0.86866283 0.010060787 0.65453053 0.66779804 0.72678185 0.3063302 -0.24884176 -1.0843778 -2.4611573 -4.2479 -5.8858705 -7.2205262 -7.9356055][-4.3091965 -3.1951895 -2.0575762 -1.0722833 -0.091890335 -0.10292578 -0.48176479 -1.00175 -1.4639401 -2.4390068 -3.5698166 -4.6947389 -5.9332156 -6.8817611 -7.7570386][-6.1893573 -5.5795412 -4.1933956 -3.1016788 -1.8955469 -1.2899055 -1.4141822 -1.6062617 -2.1300473 -3.1236086 -4.4389544 -5.5634556 -6.4937248 -6.8658419 -6.8450642][-7.2200465 -6.729228 -5.8459234 -5.4154992 -4.6457705 -3.6221905 -3.1988692 -3.1951017 -3.9586775 -4.6338959 -5.7141514 -6.49 -6.726614 -6.7235851 -6.6727967]]...]
INFO - root - 2017-12-15 15:57:31.547351: step 31710, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 54h:29m:09s remains)
INFO - root - 2017-12-15 15:57:37.876440: step 31720, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 52h:18m:07s remains)
INFO - root - 2017-12-15 15:57:44.326334: step 31730, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 52h:32m:44s remains)
INFO - root - 2017-12-15 15:57:50.690253: step 31740, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 52h:26m:50s remains)
INFO - root - 2017-12-15 15:57:57.122753: step 31750, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:51m:08s remains)
INFO - root - 2017-12-15 15:58:03.479114: step 31760, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 54h:48m:18s remains)
INFO - root - 2017-12-15 15:58:09.909842: step 31770, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 54h:14m:25s remains)
INFO - root - 2017-12-15 15:58:16.354636: step 31780, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 54h:48m:16s remains)
INFO - root - 2017-12-15 15:58:22.808816: step 31790, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 55h:09m:33s remains)
INFO - root - 2017-12-15 15:58:29.301707: step 31800, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 54h:25m:59s remains)
2017-12-15 15:58:29.762220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.40659 -3.9653614 -4.4418297 -4.6026664 -4.8505278 -4.9256349 -3.7237198 -2.5394158 -1.8586922 -3.55471 -5.1648006 -4.751503 -5.3115149 -6.7924995 -6.985673][-5.0164332 -5.5814028 -5.4302816 -5.1549664 -5.0720978 -4.8361444 -4.7587376 -3.9427745 -2.7201142 -3.574151 -4.4942951 -4.9648914 -5.97983 -6.4855804 -7.0001903][-4.4033442 -4.471035 -4.7889681 -4.8603907 -4.583992 -3.8329616 -2.6713128 -2.0913935 -1.7745619 -2.6935205 -3.445056 -4.3887615 -5.9116111 -6.5732493 -6.4270668][-4.7114277 -4.0812092 -3.6790504 -2.9962869 -2.8787174 -2.5019112 -1.2242174 -0.36228323 -0.11564064 -2.0416083 -3.447814 -2.898603 -4.1805553 -5.6809282 -5.9955406][-5.3817968 -4.1154909 -2.7595744 -2.220365 -1.5372148 -0.72426653 -0.11334085 0.1526556 -0.20829773 -1.8171697 -3.4124722 -3.7451937 -4.5142879 -5.2070885 -5.3074059][-3.2738428 -2.8414178 -2.2792039 -1.4723248 -0.54957485 0.19444942 1.0102739 1.578619 1.0652275 -1.4791827 -3.5620298 -4.3565283 -6.1765981 -6.8635778 -5.9817348][-2.1424451 -1.5671163 -1.2937226 -0.33609533 0.48174191 1.2888546 1.8694506 2.10672 2.4207392 -0.34947681 -2.8128223 -4.0451756 -5.92301 -7.4704747 -7.3416452][-1.7425823 -1.0754027 -0.87270117 0.084037781 0.78173733 1.3991098 1.8029985 2.3805504 2.489707 0.24731255 -2.214262 -4.0067635 -5.6056957 -6.8990021 -7.035099][-2.1534095 -1.9303551 -1.4535708 -0.63968706 0.49742603 1.1954737 1.7432785 2.0790281 1.8985662 -0.14337969 -2.3547721 -3.4147515 -5.3798056 -7.0829787 -6.9306564][-2.9383159 -2.2759786 -1.2901173 -1.3721395 -1.0148692 0.03303194 1.1046772 1.6239319 1.7400932 -0.30763054 -2.6540585 -3.9782722 -5.1715117 -6.3233471 -6.7565546][-3.9016123 -4.6638403 -4.4721985 -3.4119005 -3.0703406 -2.2750549 -1.4972253 -0.87138176 -0.95113373 -2.162004 -3.6922812 -4.5701876 -5.6206551 -6.6909733 -6.365922][-6.3186378 -5.8666034 -5.7305956 -5.5325375 -4.5643883 -4.3117895 -4.188159 -3.2434549 -2.7400618 -3.9540508 -5.0853357 -4.8900046 -5.4912734 -6.2812471 -6.2215014][-7.1081424 -7.5132327 -7.2889223 -6.9640379 -6.4290652 -5.7825284 -4.8672104 -4.65979 -4.8512239 -4.9773493 -5.7601662 -5.5055046 -5.0827141 -5.4970131 -5.6135163][-6.9957471 -7.0445271 -7.3839293 -6.9357538 -6.704699 -6.5261126 -6.5173478 -6.1298995 -5.4664545 -4.9476814 -5.3825178 -5.7549844 -6.14839 -5.4743223 -5.0236406][-7.6061144 -7.2301426 -7.1983161 -6.8877516 -6.4301915 -6.3525519 -6.6871843 -6.5724382 -6.993494 -6.9988065 -6.2972422 -5.8046913 -5.5574985 -5.9524221 -5.856802]]...]
INFO - root - 2017-12-15 15:58:36.205479: step 31810, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 54h:13m:34s remains)
INFO - root - 2017-12-15 15:58:42.569773: step 31820, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 54h:57m:26s remains)
INFO - root - 2017-12-15 15:58:48.959342: step 31830, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 54h:07m:21s remains)
INFO - root - 2017-12-15 15:58:55.437866: step 31840, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 54h:03m:07s remains)
INFO - root - 2017-12-15 15:59:01.820231: step 31850, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 51h:43m:33s remains)
INFO - root - 2017-12-15 15:59:08.245922: step 31860, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 53h:58m:20s remains)
INFO - root - 2017-12-15 15:59:14.631066: step 31870, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 53h:37m:52s remains)
INFO - root - 2017-12-15 15:59:21.009275: step 31880, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 54h:03m:59s remains)
INFO - root - 2017-12-15 15:59:27.431387: step 31890, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 53h:42m:13s remains)
INFO - root - 2017-12-15 15:59:33.810445: step 31900, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:12m:36s remains)
2017-12-15 15:59:34.335615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5742188 -5.4263506 -5.1222038 -4.9516525 -4.5508795 -4.4283047 -4.6721687 -4.4137721 -4.4844303 -5.5320988 -7.1970387 -7.5374823 -9.1157694 -9.786972 -10.023769][-5.9649515 -5.8481951 -5.7607059 -5.8461337 -5.6359205 -5.4278831 -5.2868118 -5.1717949 -5.31569 -5.8459811 -7.0596161 -7.5921583 -9.03311 -9.5420294 -9.8971233][-6.0393076 -5.436348 -5.21669 -5.3269572 -5.3869085 -5.203486 -4.5119567 -4.2750626 -4.2205138 -4.9756637 -6.2232623 -6.7515373 -7.9956393 -8.3890562 -8.64104][-4.5628858 -4.4119959 -4.8117852 -4.4267607 -3.7726951 -3.4721951 -2.761786 -3.036447 -2.8727131 -3.7568972 -5.1359339 -5.83383 -7.2704372 -7.8764057 -8.1899414][-4.3280964 -3.59228 -3.4841552 -3.1767092 -2.837245 -2.2908273 -1.4335895 -1.2414069 -1.1491699 -2.1212211 -3.7928038 -4.6467586 -6.1434669 -6.6943307 -7.0100923][-3.6806068 -3.4481025 -2.6977811 -2.1972737 -1.4234834 -0.59536791 0.005589962 -0.0892725 -0.38214493 -1.4565616 -2.9291701 -3.9635081 -5.7006578 -6.2743392 -6.8256631][-4.2925577 -3.7795327 -2.6783366 -1.9292765 -0.88234949 0.13294649 1.0724115 0.93289471 0.66149807 -0.72795582 -2.8523674 -3.9772327 -5.4301004 -6.1540127 -6.7333755][-4.9597616 -4.489964 -3.2903771 -1.9083481 -0.76146746 0.27822065 0.91446018 0.77001667 0.4094305 -1.0525107 -3.051753 -4.1184435 -5.7725925 -6.5613017 -6.9879069][-5.4816742 -4.8598566 -4.1232471 -3.1394615 -1.9186745 -0.93139505 -0.35073519 -0.28501415 -0.2638936 -1.3536325 -3.1750331 -4.1211472 -5.8787565 -6.7457042 -6.9768176][-5.5693974 -5.0739384 -4.6415262 -3.9682686 -3.6217318 -3.0236864 -2.6500554 -2.3915081 -2.2733722 -3.3581376 -4.8260164 -5.1880703 -6.3948507 -6.9655023 -7.4573016][-6.656374 -6.3192554 -6.074656 -6.0731158 -5.8581843 -5.0821781 -4.8396153 -4.8829947 -4.7599497 -5.3131633 -6.3114285 -6.3644848 -6.9768271 -7.3621621 -7.7207036][-7.4152117 -7.0661464 -6.8254619 -6.5388465 -6.2681146 -6.3084078 -6.119103 -6.1466212 -6.061841 -6.4789982 -7.2548242 -7.0266342 -6.9367042 -6.8914618 -7.0484667][-7.5474305 -7.303659 -6.8106785 -6.6973333 -6.3657603 -6.2396073 -6.2197733 -6.5498881 -6.5320244 -6.7397661 -7.1197257 -6.8531237 -6.881402 -6.7865753 -6.4628077][-6.3952713 -6.3038964 -5.9506149 -5.457294 -5.2256069 -5.3649931 -5.4226732 -5.7093687 -5.7335196 -5.6200609 -5.9308434 -6.0188088 -6.2735744 -6.5128551 -6.5413556][-6.1327276 -5.8728857 -5.5486903 -5.7401466 -5.7917309 -5.775888 -5.8111329 -6.0168066 -5.9998031 -5.9210515 -5.9500442 -6.0544567 -6.1982574 -6.6010604 -6.723021]]...]
INFO - root - 2017-12-15 15:59:40.739984: step 31910, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 53h:51m:27s remains)
INFO - root - 2017-12-15 15:59:47.084497: step 31920, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 52h:03m:48s remains)
INFO - root - 2017-12-15 15:59:53.525900: step 31930, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 54h:19m:38s remains)
INFO - root - 2017-12-15 15:59:59.906192: step 31940, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 53h:16m:53s remains)
INFO - root - 2017-12-15 16:00:06.412375: step 31950, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 54h:18m:11s remains)
INFO - root - 2017-12-15 16:00:12.900036: step 31960, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 53h:17m:39s remains)
INFO - root - 2017-12-15 16:00:19.343046: step 31970, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 54h:23m:10s remains)
INFO - root - 2017-12-15 16:00:25.741068: step 31980, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 52h:29m:34s remains)
INFO - root - 2017-12-15 16:00:32.198059: step 31990, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.618 sec/batch; 51h:33m:48s remains)
INFO - root - 2017-12-15 16:00:38.631030: step 32000, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 55h:02m:31s remains)
2017-12-15 16:00:39.136281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1508508 -3.6502643 -4.0462294 -4.5803761 -5.0803041 -5.2325058 -5.7201719 -6.1572676 -6.6135578 -6.8932509 -7.8248625 -8.3588448 -8.7094889 -8.9721413 -8.7474737][-3.2981081 -3.2945881 -3.4472365 -3.9903622 -4.3944879 -4.9274306 -5.3380384 -5.8657317 -6.4678326 -6.6970825 -7.309195 -7.6773 -8.0945463 -8.4621458 -8.3294926][-4.2736435 -3.8605034 -3.6472182 -3.5291834 -3.5026283 -3.4622645 -3.866271 -4.6888037 -4.8480825 -5.1166759 -5.9696522 -6.3830957 -6.7808924 -6.652205 -6.6994328][-5.4922876 -5.0237393 -4.094449 -3.1738596 -2.6737742 -2.1704111 -1.9901204 -2.3584824 -2.43919 -3.1065989 -3.803961 -4.146471 -5.0724349 -5.2173209 -5.25352][-5.216186 -4.313364 -3.5869431 -2.6925898 -2.0691524 -1.2314873 -0.6446991 -0.506464 -0.20017719 -1.0069704 -1.8456078 -2.3785658 -3.6463261 -4.3550539 -5.2337112][-4.75263 -4.0393133 -2.904727 -2.2017069 -1.240715 0.01267004 1.2788839 1.7533817 2.1649914 1.3148441 0.13480568 -0.65142679 -2.521327 -3.6526284 -4.5831113][-4.1324644 -3.806566 -2.6900949 -1.4643803 -0.39310551 1.1385698 2.6362677 3.418786 4.1365366 3.270052 1.9254284 0.40500641 -2.0992212 -3.7185659 -4.6909828][-3.7146347 -2.8209267 -1.5746446 -0.3595953 1.3240099 2.7716331 3.6937704 4.5781183 5.1213665 3.8707886 1.9025116 0.16750669 -2.2473822 -4.081049 -5.0665979][-3.4451375 -2.8070097 -1.518239 -0.088119984 1.54463 2.7097082 3.8670654 4.4716759 4.4814453 3.0434723 1.0509949 -1.276372 -3.4149265 -4.79139 -5.1874633][-3.53364 -3.3115263 -2.4663372 -1.3920445 0.14865828 1.2559023 1.4750776 2.0355682 2.3364258 0.88851261 -0.98214865 -2.5421257 -4.4347019 -5.5443735 -6.0122395][-5.6819944 -5.1102886 -4.3761969 -4.1011038 -3.259078 -2.6223431 -2.1675177 -1.5440636 -1.1072888 -2.133285 -4.065053 -4.9972267 -5.6339612 -6.6086917 -6.9603181][-6.7000542 -6.1706233 -6.0081086 -5.6780119 -5.1589022 -4.9019814 -4.5430584 -4.265871 -3.9285846 -4.482275 -5.8818212 -6.0779934 -6.7947965 -7.3127704 -7.2359524][-6.7872572 -6.6861949 -7.0901065 -7.1657495 -6.8846307 -6.4131045 -5.8618574 -5.8184156 -5.8626523 -6.3778057 -7.1486 -6.9608164 -6.9555955 -7.3128304 -7.1394382][-6.2290597 -6.1700749 -5.8670392 -6.1646609 -6.6169977 -6.6195784 -6.0284014 -6.0133958 -5.8926148 -6.1171732 -6.998951 -7.1480031 -7.16696 -6.8905435 -6.4538131][-6.8001437 -6.6002331 -6.7456274 -6.6062117 -6.1219482 -6.0843511 -6.0431705 -6.0397763 -6.3020444 -6.4618878 -6.6206961 -6.81241 -6.7564268 -6.8761392 -6.6361132]]...]
INFO - root - 2017-12-15 16:00:45.665507: step 32010, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 55h:40m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:00:52.119069: step 32020, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 54h:47m:22s remains)
INFO - root - 2017-12-15 16:00:58.522967: step 32030, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 53h:39m:35s remains)
INFO - root - 2017-12-15 16:01:04.923772: step 32040, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 54h:05m:16s remains)
INFO - root - 2017-12-15 16:01:11.330328: step 32050, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 52h:17m:57s remains)
INFO - root - 2017-12-15 16:01:17.742976: step 32060, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 53h:45m:31s remains)
INFO - root - 2017-12-15 16:01:24.099456: step 32070, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:54m:41s remains)
INFO - root - 2017-12-15 16:01:30.515112: step 32080, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 53h:42m:55s remains)
INFO - root - 2017-12-15 16:01:36.900379: step 32090, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:20m:23s remains)
INFO - root - 2017-12-15 16:01:43.301337: step 32100, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 53h:46m:30s remains)
2017-12-15 16:01:43.844977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.97834 -5.551589 -6.0899954 -6.6248074 -6.982645 -8.0762043 -8.3341475 -7.54455 -7.110055 -6.7230258 -6.4258795 -7.2426181 -7.218936 -7.4941564 -7.7334733][-5.4592161 -5.5364056 -5.7804842 -6.2805958 -7.7463765 -8.3101635 -8.2206287 -8.1133146 -7.5372925 -6.5894141 -6.8268523 -7.3383961 -7.8409266 -8.0884924 -8.6014566][-3.615231 -3.6420417 -4.4183569 -5.2663107 -5.95058 -6.5957937 -6.927083 -6.4582934 -5.8086867 -5.2448325 -5.6374054 -6.2963963 -6.5044422 -6.8043585 -7.3398738][-3.7709014 -3.472414 -3.5312915 -3.3218927 -3.9176044 -4.0197339 -3.4502606 -3.1456952 -2.7048717 -2.4530263 -3.1314578 -4.6528778 -5.0306368 -6.0163841 -6.3735609][-2.570581 -1.8898416 -1.5023475 -1.8999691 -2.3077841 -1.4377341 -0.80940342 -0.27023983 0.57951832 0.16101265 -1.2843084 -3.0828953 -3.8274443 -4.7754431 -5.4579735][-2.7395062 -2.1168485 -1.714643 -1.0325489 -0.32131433 0.71687222 1.9153547 2.7754631 3.6541595 2.6256609 0.70371056 -1.7445736 -3.5674238 -5.1312084 -6.0139709][-3.1348243 -1.9494324 -0.99608088 -0.11590147 1.3245535 2.8003864 3.7655067 4.40162 4.767909 3.2363682 0.56239796 -2.6924491 -4.6405087 -6.4179354 -6.9070573][-3.747236 -2.5141778 -1.6675553 -0.22530603 1.6763277 2.9409208 3.9988527 5.0489979 5.1719913 3.3380814 0.64746189 -2.847867 -5.0990829 -6.6444297 -7.0559478][-4.3494282 -3.3416057 -2.5769334 -1.1949697 0.34554768 1.7288389 3.0550365 3.5659685 3.8122234 2.235177 -0.46268845 -3.4663396 -5.6092615 -6.9502664 -7.3627625][-7.110496 -6.3323507 -5.5367069 -4.2180204 -2.6665959 -1.175581 0.19050789 0.73333168 0.75106907 -0.83022261 -3.0188107 -5.4398756 -7.0795245 -8.0307722 -8.28837][-8.5639057 -8.2564545 -7.7296896 -6.5805559 -5.6146307 -4.1955314 -2.5258913 -1.8551464 -1.5356631 -2.709971 -4.9177094 -6.573472 -7.9022341 -8.6726284 -8.7312078][-9.4980125 -9.3333435 -8.6100883 -7.7629175 -6.9428768 -6.0117636 -5.1504221 -4.5206709 -3.7881129 -4.62665 -5.6070404 -6.6262383 -7.7506819 -8.2340059 -8.3020229][-10.37977 -10.340959 -9.63136 -8.5436993 -7.8341284 -7.1080203 -6.8841543 -6.9113207 -6.7992063 -7.24313 -7.4780354 -7.5861535 -7.4838805 -7.3998709 -7.3967872][-8.9490128 -9.2313738 -9.0268326 -8.2800884 -7.3547587 -6.6677456 -5.9108682 -6.0479293 -6.5138431 -6.8313842 -7.2856851 -7.49246 -7.5385251 -7.1119075 -6.7947211][-8.8574572 -8.8350124 -8.6623459 -8.2677908 -7.9501095 -7.8353419 -7.721467 -7.7761774 -7.6818619 -7.8767424 -8.04431 -7.6996245 -7.3946176 -7.05332 -6.6731181]]...]
INFO - root - 2017-12-15 16:01:50.222151: step 32110, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 54h:32m:34s remains)
INFO - root - 2017-12-15 16:01:56.629932: step 32120, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 54h:11m:08s remains)
INFO - root - 2017-12-15 16:02:02.966261: step 32130, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 53h:03m:26s remains)
INFO - root - 2017-12-15 16:02:09.494451: step 32140, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 54h:44m:31s remains)
INFO - root - 2017-12-15 16:02:15.847617: step 32150, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:12m:48s remains)
INFO - root - 2017-12-15 16:02:22.322099: step 32160, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.632 sec/batch; 52h:46m:02s remains)
INFO - root - 2017-12-15 16:02:28.658207: step 32170, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 53h:14m:54s remains)
INFO - root - 2017-12-15 16:02:35.022310: step 32180, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 53h:53m:42s remains)
INFO - root - 2017-12-15 16:02:41.402906: step 32190, loss = 0.38, batch loss = 0.27 (12.6 examples/sec; 0.636 sec/batch; 53h:05m:09s remains)
INFO - root - 2017-12-15 16:02:47.834674: step 32200, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.644 sec/batch; 53h:45m:31s remains)
2017-12-15 16:02:48.358974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.96857 -4.4616466 -3.0774579 -2.1887918 -2.0081053 -1.6554513 -1.7264171 -1.371192 -0.88494444 -2.2059669 -3.3108163 -6.1935482 -7.7112803 -8.5834866 -9.6777945][-4.1765289 -3.7581792 -3.7061756 -3.6671071 -4.1173086 -4.3187585 -3.769069 -3.1526308 -2.6648221 -4.3533583 -5.2580328 -7.3858662 -8.1351891 -7.9034677 -8.9585714][-3.2509789 -2.3073068 -1.9338117 -2.4458661 -3.4830451 -4.4630175 -4.6439495 -4.2993741 -3.3204412 -4.3109345 -5.2784748 -7.0953646 -7.5522981 -6.8675566 -7.7289624][-1.9831486 -1.4493351 -1.1534438 -1.6053977 -1.863822 -2.2617946 -2.6030884 -2.8675547 -2.3732362 -3.075151 -4.0564661 -5.9804635 -6.1312523 -6.2422132 -6.9098034][-1.5687909 -0.88353586 -0.22344923 -0.52177382 -1.0971222 -0.92314863 -0.47706747 -0.095632553 0.19874334 -0.99724007 -2.0081143 -4.3202939 -5.1119232 -5.1604481 -6.4966893][-1.1076088 -0.55287552 -0.31671286 0.43583012 1.1354609 1.809761 2.6184149 3.0261707 3.4462538 1.9071789 0.71611691 -1.2491751 -2.1466818 -2.6355886 -4.6721916][-1.9778132 -0.8483963 0.53173256 1.7725811 2.233983 3.1403589 3.8474245 4.3406954 4.6389694 2.9291306 1.4933634 -0.60814285 -1.6351819 -1.9777832 -3.7401218][-1.533824 -0.64980268 1.2026148 2.8442593 3.5363865 4.2306709 4.803215 4.9299078 4.8955641 3.1000204 1.468502 -0.44348764 -1.2303958 -1.7498465 -3.5471144][-1.5085039 -0.36667395 0.8500967 2.3909349 3.195569 4.1840439 4.4577703 4.4334917 4.0399542 2.1118431 1.2870941 -0.068922043 -0.7072649 -0.95024586 -2.6932821][-2.9001756 -1.1115456 0.14919615 1.4847078 1.9879684 2.8970318 2.8760014 2.7557945 2.4623804 0.38709354 -1.0858669 -1.9973593 -1.7448869 -0.93684196 -2.1839356][-4.0321817 -3.0624075 -1.6211295 -0.42667055 -0.21918344 0.44341087 0.54615974 -0.046554565 -0.75973988 -2.3978405 -3.3981152 -4.2088485 -4.2844839 -3.0980167 -3.203033][-4.7422018 -4.0287127 -3.5427208 -2.7227316 -2.1366863 -1.8612704 -1.4059315 -1.2841339 -1.7446728 -3.8281476 -5.0097933 -5.5453529 -5.2863512 -5.0061955 -4.6110678][-5.6528149 -5.8223052 -5.7173648 -4.850503 -4.17795 -3.3936219 -2.994319 -2.7187715 -2.5652928 -3.4569426 -4.1437964 -5.2079229 -5.6405373 -5.437273 -4.869978][-6.1905222 -6.0974298 -6.460433 -5.9134917 -5.7720852 -5.5031538 -4.8240814 -4.6018639 -4.5707555 -5.1196842 -5.1346154 -5.0807438 -5.2235022 -5.2593155 -5.0159259][-7.3133054 -7.6147137 -7.9197111 -7.8956761 -7.7729692 -7.4395647 -6.7784061 -6.495286 -6.4308691 -6.2258978 -6.2477703 -6.2994318 -6.1924477 -5.7368231 -4.8802238]]...]
INFO - root - 2017-12-15 16:02:54.716135: step 32210, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 54h:48m:20s remains)
INFO - root - 2017-12-15 16:03:01.017138: step 32220, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 51h:51m:06s remains)
INFO - root - 2017-12-15 16:03:07.408958: step 32230, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 52h:55m:29s remains)
INFO - root - 2017-12-15 16:03:13.812322: step 32240, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 54h:31m:53s remains)
INFO - root - 2017-12-15 16:03:20.180223: step 32250, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 52h:17m:29s remains)
INFO - root - 2017-12-15 16:03:26.580374: step 32260, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 53h:56m:24s remains)
INFO - root - 2017-12-15 16:03:32.912082: step 32270, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 52h:24m:11s remains)
INFO - root - 2017-12-15 16:03:39.335628: step 32280, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 53h:37m:48s remains)
INFO - root - 2017-12-15 16:03:45.833891: step 32290, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 52h:50m:52s remains)
INFO - root - 2017-12-15 16:03:52.298420: step 32300, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 52h:59m:37s remains)
2017-12-15 16:03:52.794817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5311718 -4.6412926 -5.2679205 -5.5913658 -5.8417807 -5.5838542 -4.9749041 -3.8363554 -2.2470679 -2.7275577 -2.7016182 -3.1078887 -3.0255866 -4.6014338 -6.2908077][-4.2693868 -4.7608528 -5.6041551 -5.8628645 -6.0130014 -5.9520683 -5.8285975 -5.1806736 -3.7080252 -3.9959614 -4.030262 -4.3697681 -4.1166487 -4.8149333 -5.7691121][-5.2425737 -4.7599692 -5.0671177 -5.3520451 -5.8867455 -5.9484425 -5.8827257 -5.6797028 -4.85019 -5.4928451 -5.5038662 -5.53749 -5.0597992 -5.6109619 -5.9511509][-5.929081 -5.1714029 -4.6231241 -3.957509 -3.8091543 -3.3785067 -3.3244023 -3.6320782 -3.55056 -4.7585964 -5.0701075 -5.5496902 -5.5444584 -6.6257052 -7.295208][-6.2369218 -4.7835112 -3.794857 -2.7963634 -2.1586866 -1.2611818 -0.58690882 -0.68037653 -0.45046234 -2.0566297 -3.4613752 -4.5809493 -4.9028215 -6.2596855 -6.9252968][-6.8483295 -5.18567 -3.6192236 -1.6360192 -0.22232389 0.79879284 1.4387732 1.8131914 2.5932198 0.81818104 -0.73117828 -2.4728847 -3.7765255 -5.5468969 -6.9426422][-6.1575365 -4.2525692 -2.344389 -0.34990788 0.79016685 2.1339283 3.226265 3.8123884 4.4385366 2.6568956 0.98283672 -1.227406 -2.772346 -4.7236795 -6.1844196][-5.4590144 -3.8741689 -2.1528339 0.3557682 2.256566 2.9563122 3.8269606 4.6426916 5.2358103 3.6589727 1.919241 -0.39727688 -1.8494768 -3.8822656 -5.5489159][-4.4473357 -3.4183078 -2.1407089 -0.34933996 0.94706154 2.2386532 3.1630878 3.5433798 4.0231304 2.0070457 -0.036447525 -1.8744693 -3.0525346 -4.8850436 -5.7176456][-4.91993 -3.6177454 -2.7523808 -1.4318042 -0.27402592 0.37393188 1.0807648 2.0776892 2.6765232 0.31451035 -1.6121278 -2.9396763 -4.0890656 -5.7328444 -6.6235662][-6.7681732 -5.5056887 -4.762773 -3.2839828 -2.0739317 -1.6041508 -1.3472509 -1.0168643 -0.90844584 -2.194025 -3.4268856 -4.4957886 -5.4644461 -6.7848682 -7.4823837][-7.9015512 -6.7471838 -5.6263142 -4.3259397 -3.5095468 -2.8898153 -2.6159062 -2.588119 -2.4666505 -3.4973869 -4.4460812 -4.6617403 -4.9605637 -6.1859446 -6.6641073][-8.5512075 -7.625205 -6.9664745 -6.2780471 -5.4051456 -4.698257 -4.465477 -4.4625549 -4.4466553 -4.9642639 -5.4241714 -4.9997954 -4.7739148 -5.0124512 -5.178421][-8.4851217 -7.2567291 -5.7961292 -5.2513456 -5.1148229 -4.4021173 -4.1069317 -4.2055511 -4.26665 -4.4082956 -4.5593705 -4.2904038 -4.0146813 -4.3006396 -4.6194763][-8.2870045 -7.8628263 -6.9882183 -5.8784184 -5.2275677 -5.171133 -5.0767765 -5.0674062 -5.3627377 -5.6377039 -5.8122516 -5.2571669 -4.9261045 -4.4690075 -4.0791264]]...]
INFO - root - 2017-12-15 16:03:59.214760: step 32310, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 54h:08m:34s remains)
INFO - root - 2017-12-15 16:04:05.586592: step 32320, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 53h:42m:57s remains)
INFO - root - 2017-12-15 16:04:12.028058: step 32330, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 54h:03m:18s remains)
INFO - root - 2017-12-15 16:04:18.399309: step 32340, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 52h:33m:40s remains)
INFO - root - 2017-12-15 16:04:24.797910: step 32350, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.645 sec/batch; 53h:45m:41s remains)
INFO - root - 2017-12-15 16:04:31.374394: step 32360, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.652 sec/batch; 54h:22m:49s remains)
INFO - root - 2017-12-15 16:04:37.799934: step 32370, loss = 0.41, batch loss = 0.30 (12.5 examples/sec; 0.640 sec/batch; 53h:23m:17s remains)
INFO - root - 2017-12-15 16:04:44.295101: step 32380, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 54h:00m:05s remains)
INFO - root - 2017-12-15 16:04:50.699047: step 32390, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 51h:55m:37s remains)
INFO - root - 2017-12-15 16:04:57.184774: step 32400, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 52h:58m:31s remains)
2017-12-15 16:04:57.677895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2574668 -2.0882354 -2.088788 -2.2217264 -2.0306325 -1.3799734 -1.1639647 -1.1491156 -0.98346424 -2.3233976 -3.8094273 -4.7990284 -5.376483 -5.3224492 -6.3431106][-2.8202167 -3.3746004 -3.706177 -3.0665283 -2.4516044 -2.1587834 -1.6445231 -1.2734256 -1.0203066 -2.5489864 -3.9591827 -5.4223394 -6.4115586 -6.3291531 -7.1796107][-3.7223098 -4.32364 -4.5652018 -4.133008 -3.299315 -2.3488884 -1.7057285 -1.5837564 -1.2291017 -2.3489246 -3.8828843 -5.5381479 -6.6318655 -6.5963621 -7.6311731][-4.9665513 -5.3396177 -5.217854 -3.932621 -2.7211933 -1.6431708 -0.85226631 -0.60783386 -0.42103958 -1.8025193 -3.5519748 -5.2605982 -6.5604453 -6.4865522 -7.442976][-6.41867 -5.9058275 -4.8330288 -3.3494978 -1.5575647 0.065070629 1.0221863 1.0475674 0.94114208 -0.96514654 -3.0847721 -4.9308939 -6.4873018 -6.72751 -7.80514][-7.7210031 -7.3105583 -6.2502337 -3.7287033 -0.8209734 0.72106075 1.9884624 2.31715 2.0517073 -0.10339355 -2.4265976 -4.6265879 -6.4508233 -6.8326421 -7.9845872][-8.9727983 -7.9264603 -5.6114893 -2.8326902 0.18300915 2.1026382 3.1443863 3.0093489 2.8798895 0.71466541 -1.7499943 -3.8161583 -5.6549606 -6.290041 -7.6278687][-8.0265121 -7.02614 -5.7275281 -2.711503 1.0120678 2.9792795 4.3468418 3.875186 3.2685118 1.0776539 -1.2649598 -3.7154126 -5.771976 -5.9345174 -7.1823187][-7.9562664 -6.5647755 -4.8216724 -3.0940232 -0.57214212 1.5450497 3.243784 3.3081894 3.2135582 0.70720577 -1.6820359 -3.6328192 -5.4863806 -6.1305661 -7.2799034][-8.031559 -7.6997628 -6.9000254 -4.8907919 -2.3416004 -0.79136229 0.89512253 1.2784309 1.2112808 -0.23334646 -2.3266673 -4.4419508 -5.8758392 -5.7521906 -6.3530235][-8.5557756 -8.2762938 -7.9995985 -6.9627767 -5.1634893 -3.5477176 -2.0048637 -1.4291363 -1.0546684 -2.6006961 -4.6786051 -5.4698143 -6.2880797 -6.0685225 -6.3279324][-9.0787792 -8.1482162 -7.5403385 -6.9449039 -5.8264508 -4.565958 -3.37632 -2.9647474 -2.6648445 -3.534337 -4.3799577 -5.1794791 -6.1248827 -5.6783562 -6.0696545][-9.3726377 -9.2248878 -8.9396553 -7.7971106 -6.8716764 -6.0277243 -4.6392651 -4.0387115 -3.5894842 -4.5933876 -5.4890718 -5.4388361 -5.6690521 -5.2067575 -5.5394554][-9.2136469 -8.9604406 -8.4352617 -7.8290248 -6.8310452 -5.8749714 -4.8883629 -4.5123539 -4.05772 -3.9298022 -4.08748 -4.7639427 -5.1987858 -4.9669495 -5.4287539][-10.280703 -9.56043 -9.1153955 -8.5248127 -7.7161918 -7.1923184 -6.2178082 -5.96633 -5.5381627 -5.1458263 -5.1351676 -5.1953115 -5.4934959 -5.6033478 -5.4887362]]...]
INFO - root - 2017-12-15 16:05:04.231729: step 32410, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 55h:29m:43s remains)
INFO - root - 2017-12-15 16:05:10.717429: step 32420, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 54h:06m:06s remains)
INFO - root - 2017-12-15 16:05:17.163371: step 32430, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 54h:59m:06s remains)
INFO - root - 2017-12-15 16:05:23.523680: step 32440, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.664 sec/batch; 55h:18m:48s remains)
INFO - root - 2017-12-15 16:05:30.019871: step 32450, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 54h:07m:50s remains)
INFO - root - 2017-12-15 16:05:36.415883: step 32460, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.620 sec/batch; 51h:41m:30s remains)
INFO - root - 2017-12-15 16:05:42.782621: step 32470, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 52h:23m:20s remains)
INFO - root - 2017-12-15 16:05:49.230671: step 32480, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 53h:32m:11s remains)
INFO - root - 2017-12-15 16:05:55.653484: step 32490, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 52h:16m:42s remains)
INFO - root - 2017-12-15 16:06:02.106295: step 32500, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 55h:57m:26s remains)
2017-12-15 16:06:02.588815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9783359 -3.4284964 -4.02893 -3.9633622 -3.4150829 -3.2749972 -3.1467762 -2.5276251 -1.7432928 -2.2176113 -3.3608975 -5.3951683 -5.87821 -6.0021763 -6.6806951][-3.4431376 -3.5046163 -4.3423839 -5.2721424 -5.0739803 -3.9712837 -2.8713856 -2.3743486 -2.6653953 -3.5826826 -4.2604275 -6.0734224 -6.5783038 -6.9094286 -7.7049046][-4.1100626 -3.8558118 -4.3590088 -4.618783 -4.5474453 -4.2797871 -3.5783582 -2.8621659 -2.2936521 -3.7262843 -5.5304761 -7.3354363 -7.2262726 -7.6542764 -8.6098862][-4.6443529 -4.7793264 -4.9129496 -4.50622 -3.568316 -2.7630434 -1.9024949 -1.8170624 -1.9164486 -3.1809716 -4.6789579 -6.8209596 -7.3694983 -7.76468 -8.1619148][-5.24097 -5.3519936 -5.38755 -4.7880459 -3.7314863 -2.1045809 -0.70073891 -0.34020615 -0.82713175 -2.5562339 -4.4891763 -6.7756691 -7.4040923 -7.852356 -8.4784784][-7.0864062 -6.8715267 -6.1449203 -4.5828347 -3.0059252 -1.0254884 1.0678587 1.6640539 1.2183666 -1.2010121 -4.0041285 -6.803328 -7.8067794 -8.3178835 -8.7623053][-7.2250662 -7.1800847 -6.8857188 -4.9057503 -1.9485955 0.89311314 3.5000505 4.405345 4.4579287 1.9562092 -1.7429566 -5.5016413 -6.88003 -7.5607719 -8.156311][-6.6242681 -6.5508008 -5.7103772 -3.979434 -1.7550898 1.7101145 4.8736162 5.8539095 5.850337 3.2470837 -0.025625706 -3.7061508 -5.6508245 -6.7515726 -7.40907][-7.0579405 -6.3509293 -5.8476276 -4.2324543 -2.2957392 0.49062443 3.0330582 4.3338566 4.7054234 2.0667944 -1.0975986 -4.285099 -5.7169328 -6.6486335 -7.408649][-7.8172612 -7.21001 -6.2192512 -4.7709322 -3.4547882 -1.5420604 0.25436735 1.1579876 1.5064917 -0.78837442 -2.8972 -5.5571289 -6.2396941 -6.7572026 -7.8475337][-8.5623569 -8.1071663 -7.4436135 -6.2506309 -5.035892 -3.7038908 -2.8880692 -2.1640449 -1.8110342 -3.3204808 -5.1077051 -7.3864288 -7.50518 -7.5097976 -7.7054148][-8.346097 -8.1730309 -7.5843072 -6.5099883 -5.8443336 -4.9419289 -3.9798164 -3.9027188 -3.916538 -4.7690239 -5.5921831 -7.2684169 -7.2857122 -7.7710748 -7.9405379][-9.0685043 -8.6349268 -7.8302994 -6.9399137 -6.3598909 -5.8455186 -5.4207125 -5.2654128 -5.2550888 -6.1333132 -6.3395119 -7.1210465 -7.0457826 -7.8312397 -8.2340031][-8.2720613 -8.1423025 -7.5228138 -6.525723 -5.9844217 -5.3883734 -5.0171046 -5.2995996 -5.9385409 -6.5324254 -6.5387115 -6.7193351 -6.3878121 -6.7183414 -6.6683521][-8.5402565 -8.4447489 -8.24172 -7.1906433 -6.4708095 -6.1833673 -6.0504189 -6.3646169 -6.5093608 -6.7382374 -6.7915373 -6.5265965 -5.9517713 -5.606389 -5.3153419]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 16:06:10.191935: step 32510, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 52h:30m:17s remains)
INFO - root - 2017-12-15 16:06:16.590451: step 32520, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 54h:52m:24s remains)
INFO - root - 2017-12-15 16:06:23.039774: step 32530, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 54h:05m:00s remains)
INFO - root - 2017-12-15 16:06:29.493682: step 32540, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 53h:16m:16s remains)
INFO - root - 2017-12-15 16:06:35.790541: step 32550, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 52h:50m:59s remains)
INFO - root - 2017-12-15 16:06:42.197451: step 32560, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.629 sec/batch; 52h:23m:09s remains)
INFO - root - 2017-12-15 16:06:48.597520: step 32570, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 52h:40m:38s remains)
INFO - root - 2017-12-15 16:06:55.006961: step 32580, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 52h:42m:31s remains)
INFO - root - 2017-12-15 16:07:01.453261: step 32590, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 53h:37m:59s remains)
INFO - root - 2017-12-15 16:07:07.884627: step 32600, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 54h:47m:40s remains)
2017-12-15 16:07:08.435945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2184973 -8.019742 -8.06765 -7.46912 -6.9230275 -5.7018242 -3.3916397 -1.637598 -0.6484108 -1.2170763 -1.5608573 -2.8695221 -3.9459076 -5.13428 -6.5163012][-6.3300509 -6.9197702 -7.4062076 -7.6476183 -7.3189397 -6.302206 -5.2857547 -3.9780791 -1.8512316 -1.8408456 -1.8943768 -3.1146445 -3.6594577 -4.1726408 -5.7233105][-6.0796585 -6.433466 -7.1524644 -7.36095 -7.596663 -7.2306614 -5.9718351 -4.7369041 -4.0516934 -4.3760881 -3.5471644 -4.1792631 -4.5723305 -4.9704571 -5.8348069][-6.0748682 -6.1302948 -5.5792065 -5.3840356 -5.7811518 -5.3336658 -4.7204142 -4.3555055 -3.5058122 -3.980916 -4.88161 -5.998857 -5.6910424 -5.7149439 -6.6402149][-6.1040983 -5.38196 -4.4050293 -3.6917593 -3.4791746 -2.9722176 -1.8169827 -1.408536 -1.5085263 -3.3328824 -4.3668203 -5.6920652 -6.7000666 -7.2193093 -7.6600409][-5.0601082 -3.725503 -3.0505176 -1.6213465 -0.50989962 0.36228943 1.173439 1.1465874 1.0901747 -0.98410606 -2.7098222 -4.7492838 -5.683053 -6.3193908 -7.838295][-4.9078221 -3.854847 -2.0851431 -0.31541395 0.74309731 2.2268467 3.3849592 3.5621614 3.5308857 1.1966591 -1.190496 -3.7043216 -4.5805368 -5.7278309 -6.9593892][-3.9447308 -3.068326 -2.9400187 -1.3119588 0.5987978 2.1114807 3.2230539 3.5544024 2.9647417 0.48452473 -1.3665128 -3.604939 -4.9015737 -6.0734386 -6.8986859][-4.5633078 -3.669908 -2.9653769 -1.5945373 -0.99619246 0.4812603 1.5081797 1.5553064 1.9512768 -0.13197327 -2.7501669 -4.7591252 -5.4118218 -5.844264 -6.6917353][-5.3149824 -4.5978336 -4.08097 -3.0746222 -2.183908 -0.77849483 -0.1273632 -0.10728168 -0.1449523 -1.9420071 -3.2226143 -4.3930612 -5.5665941 -6.0026722 -6.497057][-7.7027073 -7.0250726 -6.6093893 -5.8073621 -5.3852067 -4.0797043 -3.1117997 -3.1412783 -3.3169971 -4.604785 -4.8943348 -5.5092096 -6.0905714 -6.532773 -7.5613384][-8.928154 -8.5145445 -7.859674 -6.998682 -6.4598551 -5.74833 -5.1149187 -4.9716463 -4.6234255 -5.5711517 -5.2960119 -5.3950229 -5.8882241 -6.3192086 -6.9851575][-9.065527 -9.11745 -8.8683271 -8.2110329 -7.5718679 -6.6565237 -5.9382777 -5.5354204 -5.5548563 -5.9187474 -5.4349942 -5.2718673 -5.2694631 -5.2897196 -5.8841929][-8.4198523 -8.5262709 -8.5009174 -8.0526047 -7.6689796 -7.3762417 -6.4566545 -6.1435108 -6.356184 -6.1740193 -6.04459 -6.0349169 -6.0526514 -6.1493535 -5.9211683][-6.6872807 -6.7059 -7.1588483 -7.0366945 -7.3678341 -7.3479028 -6.8454442 -7.23174 -6.9517961 -6.7678895 -6.8789558 -6.432673 -6.8571053 -6.9105449 -6.6357112]]...]
INFO - root - 2017-12-15 16:07:14.866508: step 32610, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.627 sec/batch; 52h:16m:16s remains)
INFO - root - 2017-12-15 16:07:21.277453: step 32620, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 55h:04m:48s remains)
INFO - root - 2017-12-15 16:07:27.686289: step 32630, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 52h:27m:30s remains)
INFO - root - 2017-12-15 16:07:34.133220: step 32640, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 53h:20m:34s remains)
INFO - root - 2017-12-15 16:07:40.622951: step 32650, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:09m:39s remains)
INFO - root - 2017-12-15 16:07:47.045650: step 32660, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 53h:28m:48s remains)
INFO - root - 2017-12-15 16:07:53.383844: step 32670, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 52h:59m:39s remains)
INFO - root - 2017-12-15 16:07:59.825515: step 32680, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.643 sec/batch; 53h:30m:54s remains)
INFO - root - 2017-12-15 16:08:06.253109: step 32690, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 54h:49m:31s remains)
INFO - root - 2017-12-15 16:08:12.758631: step 32700, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 54h:19m:53s remains)
2017-12-15 16:08:13.257614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4522409 -4.073782 -3.6572213 -3.3529959 -2.8808594 -2.3602929 -1.5046391 -1.2815437 -1.6861749 -3.0771551 -4.1004667 -4.83432 -6.1924582 -6.8916349 -7.4574146][-4.414516 -4.0899863 -3.65033 -3.4567127 -3.310823 -2.7589955 -2.3834834 -1.9165945 -2.1654015 -3.9645147 -4.7972345 -5.1400185 -6.4373007 -7.4726582 -7.8469152][-4.4231558 -4.0813169 -3.794194 -3.2034745 -2.9331002 -2.5934715 -1.8082442 -1.4335866 -1.4899406 -3.0696497 -3.9965661 -4.4061155 -5.4918613 -6.3886647 -7.1507459][-4.6316414 -4.3321896 -4.0184531 -3.4318056 -2.736125 -2.2124023 -1.5172334 -1.1887188 -0.88088083 -2.3316441 -3.3457766 -3.9649239 -5.0941024 -5.6895781 -6.643774][-4.5979552 -3.9560215 -3.2301979 -2.6849871 -2.1705556 -1.4362702 -0.53323126 -0.34142447 -0.22829962 -1.7898798 -2.7608342 -3.3893614 -4.5669756 -5.34194 -6.2070322][-3.7295954 -3.2602119 -2.689456 -1.9606771 -1.0271287 -0.59947348 -0.060196877 0.17275 0.25134325 -1.3408098 -2.3709092 -3.1441326 -4.4092093 -5.2661209 -6.2448416][-3.3805933 -2.8826103 -2.0710058 -1.406754 -0.73051167 -0.1594224 0.63340378 0.865468 0.97673512 -0.63024664 -1.7845025 -2.65167 -3.9632969 -4.8477678 -5.7694411][-2.5498223 -1.9024668 -1.1138201 -0.41778946 0.55363083 0.96209717 1.3309898 1.445631 1.5958748 -0.1514535 -1.5185924 -2.5419579 -4.0144787 -4.7071381 -5.5500097][-2.100606 -1.4713516 -0.64189768 0.12675095 0.75941277 0.98110008 1.2774515 1.4147015 1.5097713 -0.25525761 -1.5663738 -2.6026688 -4.2365723 -5.074079 -5.9190722][-2.2655039 -1.467689 -0.74482918 -0.11968946 0.39592171 0.46843529 0.52552795 0.493474 0.49676514 -1.4275217 -2.6071711 -3.6704025 -4.9122787 -5.4792738 -6.1589627][-3.1503787 -2.3772411 -1.9890714 -1.3926873 -1.0357022 -1.1916714 -1.2428737 -1.4497952 -1.4569411 -3.2058954 -4.4899378 -5.156662 -5.9362335 -6.2713494 -6.6837225][-3.8013525 -3.2165284 -3.0377674 -2.5080609 -1.8698602 -2.1885324 -2.456923 -2.7967558 -3.0279813 -4.2430458 -5.1293755 -5.5979071 -6.2829819 -6.4527607 -6.6138182][-4.9055891 -4.7306633 -4.88068 -4.4952741 -3.9322691 -3.7382119 -4.143301 -4.4708033 -4.46196 -5.0401506 -5.8385158 -5.8761292 -6.0335093 -5.8188639 -5.88006][-5.2134953 -5.1661682 -5.6485329 -5.4301167 -4.942625 -4.5000358 -4.7139769 -4.9028211 -4.9918985 -5.4949846 -5.706018 -5.6910982 -5.5357223 -5.3758917 -5.2804823][-6.2362552 -5.6358314 -5.7706566 -5.91986 -5.9526987 -5.8136311 -6.2197685 -6.4329691 -6.4085884 -5.9808068 -5.7962236 -5.7657146 -5.4538965 -5.272275 -5.08508]]...]
INFO - root - 2017-12-15 16:08:19.739233: step 32710, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 54h:55m:29s remains)
INFO - root - 2017-12-15 16:08:26.156776: step 32720, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 53h:32m:33s remains)
INFO - root - 2017-12-15 16:08:32.545346: step 32730, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 54h:55m:06s remains)
INFO - root - 2017-12-15 16:08:38.901568: step 32740, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 53h:52m:30s remains)
INFO - root - 2017-12-15 16:08:45.298687: step 32750, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 54h:03m:31s remains)
INFO - root - 2017-12-15 16:08:51.704856: step 32760, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 52h:09m:05s remains)
INFO - root - 2017-12-15 16:08:58.117955: step 32770, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 52h:54m:50s remains)
INFO - root - 2017-12-15 16:09:04.577914: step 32780, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 54h:20m:29s remains)
INFO - root - 2017-12-15 16:09:11.066324: step 32790, loss = 0.26, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 56h:20m:00s remains)
INFO - root - 2017-12-15 16:09:17.529780: step 32800, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:03m:59s remains)
2017-12-15 16:09:18.039252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2454834 -6.5470505 -6.7955766 -6.6187739 -5.6439314 -4.5405488 -3.0698566 -1.4503551 0.087590694 -0.11774683 -0.9835515 -4.1214571 -6.1544733 -8.5790119 -9.83048][-5.1955085 -5.9515691 -6.5839911 -6.8283615 -6.6253958 -5.2647333 -3.5487156 -1.3489881 0.068292141 -0.23817968 -1.4071865 -3.7556586 -5.9595289 -8.2977953 -9.5201607][-4.7615314 -5.4579144 -5.8088655 -5.3742061 -4.6169019 -3.9308884 -2.7862229 -1.3965206 0.15391922 -0.52985859 -1.4593883 -3.7227802 -5.3036008 -7.068738 -8.4275064][-4.9213514 -4.8189883 -4.588881 -3.9455583 -3.1714015 -2.1730928 -1.2310624 -0.62318039 0.2168293 -0.4371686 -1.2121844 -3.760078 -5.23538 -6.6138015 -7.2450337][-5.0532169 -4.421876 -3.5676789 -2.6402431 -1.8936648 -0.61124039 0.53926849 1.1535797 1.4399691 0.45914364 -0.558249 -3.209537 -4.9559269 -6.5549684 -7.4655809][-4.821898 -3.7767222 -2.4116192 -1.3611951 -0.052600384 1.2477951 2.5360765 2.897212 3.2839918 2.0459032 0.5153141 -2.4121857 -4.4254756 -6.184226 -7.2652993][-4.11736 -2.7148266 -1.4775867 -0.38130426 0.903759 1.9545479 3.234869 3.8689013 4.2835197 2.8635778 1.3965578 -2.0057197 -4.1709991 -6.1849542 -7.2184725][-3.4826121 -1.9845409 -0.50451565 0.43834114 1.6052885 2.1980429 2.7639437 3.5641479 4.2699375 3.0955286 1.3384371 -1.903584 -4.2448597 -6.2640519 -7.2223749][-2.7945008 -1.4203806 -0.198174 0.94773865 2.2425165 2.3927765 2.6932907 3.0096111 3.3170624 2.0517731 0.77984142 -2.0213895 -4.2823305 -6.3821735 -7.8532081][-2.5265651 -1.4759431 0.044586182 0.98881817 1.9563408 2.3952541 2.9017887 2.7877474 2.982646 0.99785328 -0.73175144 -3.2135887 -4.8633556 -6.451375 -7.6297889][-2.1534605 -1.4119177 -0.44912577 0.553215 1.55931 1.581089 1.9359541 1.9844427 2.0126486 0.12687111 -1.8001013 -4.1703911 -5.2657251 -6.6557021 -7.6164227][-2.8039403 -1.8706994 -0.84412289 0.040750504 0.77919769 0.70900726 0.57706928 0.3894186 -0.11169577 -0.96090031 -2.3080678 -4.2589111 -5.5531573 -7.1189823 -7.8280087][-4.3812304 -3.5063405 -2.3306088 -1.445353 -0.38428688 -0.25685072 -0.66552114 -1.0611639 -1.4558601 -2.3025398 -3.0568914 -4.2191973 -5.4600163 -6.5213909 -7.3171134][-5.9433641 -5.5207567 -4.2761545 -3.4118514 -2.2926898 -1.7206388 -1.4499946 -1.4512644 -2.0452929 -2.9768062 -4.1041346 -5.1683645 -5.7847414 -6.4099512 -6.3759971][-6.8224292 -6.440073 -5.7328277 -5.4895921 -4.5355511 -3.7911782 -3.41325 -2.9144711 -3.5392222 -3.9564986 -4.804307 -6.0742035 -6.5526257 -6.5205564 -6.39521]]...]
INFO - root - 2017-12-15 16:09:24.503922: step 32810, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 52h:29m:52s remains)
INFO - root - 2017-12-15 16:09:30.929370: step 32820, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 52h:39m:50s remains)
INFO - root - 2017-12-15 16:09:37.362543: step 32830, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 53h:23m:13s remains)
INFO - root - 2017-12-15 16:09:43.816813: step 32840, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 52h:40m:02s remains)
INFO - root - 2017-12-15 16:09:50.141080: step 32850, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 52h:36m:38s remains)
INFO - root - 2017-12-15 16:09:56.529957: step 32860, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 51h:31m:46s remains)
INFO - root - 2017-12-15 16:10:02.973216: step 32870, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 52h:51m:40s remains)
INFO - root - 2017-12-15 16:10:09.329432: step 32880, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 53h:12m:00s remains)
INFO - root - 2017-12-15 16:10:15.788771: step 32890, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 53h:31m:35s remains)
INFO - root - 2017-12-15 16:10:22.180162: step 32900, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 53h:30m:16s remains)
2017-12-15 16:10:22.740372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064456463 -0.34488821 -0.83813572 -2.0147209 -2.8521738 -2.9163184 -3.3634734 -4.3067436 -4.5211687 -5.3635616 -6.099009 -7.3508773 -7.5721955 -8.1326218 -8.36527][-0.43998003 -1.4302077 -2.7000213 -3.2349892 -3.3781905 -3.6276398 -4.1079421 -4.5243893 -4.4783769 -6.1876593 -7.3809066 -8.1674175 -8.0810957 -8.9694672 -8.9201574][-2.2012873 -2.120986 -2.3192334 -2.1829171 -2.2679143 -2.2998719 -2.1342669 -2.4915829 -2.8525381 -4.4752741 -6.0025005 -7.7457685 -8.3845129 -9.120594 -8.7600746][-3.0293956 -3.6346183 -3.2589121 -2.0655665 -0.9999404 -0.17846823 0.4019146 0.24833822 -0.41255093 -2.2377043 -3.82602 -5.9273224 -6.8913894 -8.3768063 -8.5196943][-4.5069804 -4.3974733 -3.2955923 -2.18754 -1.2102518 0.29430866 1.2433729 1.6018019 1.6152983 -0.38446808 -2.2988682 -4.8604712 -6.0465784 -7.2669764 -7.5313044][-5.2499504 -4.4833155 -4.0722132 -2.5771642 -1.0534329 1.0350323 2.512682 2.6562166 2.5590067 0.52089787 -1.2565398 -3.8190727 -5.2545843 -6.4158344 -6.64767][-5.363162 -4.3534503 -3.3800211 -1.7014527 0.3965416 2.5360575 3.6701097 4.0095043 4.42437 2.193203 -0.1702733 -3.1310091 -4.429359 -6.3833609 -6.950901][-4.9231644 -4.096581 -3.0514355 -0.94106722 1.3166304 3.4366751 4.44697 4.6394777 4.5633287 2.57685 0.61860466 -2.7676926 -4.2811322 -5.8857803 -6.8621664][-5.1451111 -4.4180374 -3.4557371 -1.4111018 0.37240028 1.7941065 3.0155344 3.6995401 4.1740694 1.954855 -1.0324554 -3.6563916 -4.8619113 -6.749773 -7.3306465][-6.5430331 -5.9507961 -5.2120132 -3.6455922 -2.0621572 -0.70422411 0.54025459 1.1920815 1.8817844 0.047114849 -2.2038202 -5.060997 -6.5206823 -7.9043555 -8.0440235][-7.2722492 -7.0271711 -6.6961555 -5.5032616 -4.6050954 -3.03128 -1.3539705 -0.76346493 -0.1810298 -2.1033263 -4.1047449 -5.3397703 -6.2213545 -7.5648427 -8.3546476][-7.7814283 -7.5087767 -7.2743711 -6.5832357 -5.8435297 -4.9186497 -4.1903892 -3.4427695 -2.659709 -3.4251065 -4.2076945 -5.6750851 -6.5425892 -7.2543364 -7.5813546][-8.3690434 -8.4199142 -8.0450525 -7.5603423 -7.3370104 -6.3195882 -5.3210449 -5.2068815 -5.0142035 -5.66797 -5.98553 -6.31342 -6.529592 -7.1441913 -7.4741096][-9.0530033 -9.070653 -8.1689224 -7.5016885 -7.19382 -6.57032 -6.2597027 -6.0265913 -5.7645059 -6.3480034 -6.6135869 -6.6656537 -6.4563026 -6.4139252 -6.6773572][-10.162573 -9.4040565 -8.6552925 -8.1304455 -7.4597626 -6.9466758 -6.6717772 -7.2611308 -7.6608953 -7.5052395 -7.3829207 -7.1790218 -6.9172535 -6.5840425 -6.2303276]]...]
INFO - root - 2017-12-15 16:10:29.120416: step 32910, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 54h:34m:49s remains)
INFO - root - 2017-12-15 16:10:35.566713: step 32920, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 54h:26m:30s remains)
INFO - root - 2017-12-15 16:10:41.917528: step 32930, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 53h:25m:47s remains)
INFO - root - 2017-12-15 16:10:48.406526: step 32940, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 55h:14m:13s remains)
INFO - root - 2017-12-15 16:10:54.852058: step 32950, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 54h:04m:20s remains)
INFO - root - 2017-12-15 16:11:01.278465: step 32960, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 53h:54m:27s remains)
INFO - root - 2017-12-15 16:11:07.708097: step 32970, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 52h:51m:54s remains)
INFO - root - 2017-12-15 16:11:14.137421: step 32980, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:38m:45s remains)
INFO - root - 2017-12-15 16:11:20.555796: step 32990, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.632 sec/batch; 52h:36m:59s remains)
INFO - root - 2017-12-15 16:11:27.027917: step 33000, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 54h:32m:22s remains)
2017-12-15 16:11:27.524986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2051392 -6.4721866 -6.1782179 -5.42283 -5.0163689 -4.9388666 -4.2040577 -3.4166908 -2.9409862 -3.8768291 -4.4402437 -6.2892962 -7.3428941 -8.2502537 -9.2727833][-7.194613 -6.8103995 -6.422462 -5.9087214 -5.1496868 -4.9560585 -4.4196568 -3.8754027 -3.4898324 -4.16346 -4.6975422 -6.7897291 -7.7815952 -8.3004808 -9.2249966][-6.6488566 -6.049367 -5.4110842 -4.7396507 -4.1378593 -3.5628104 -2.9302635 -2.8823123 -2.7354007 -3.4466162 -4.2654271 -6.466959 -7.1241851 -7.9201474 -8.6103888][-5.6858196 -4.3437815 -3.8348057 -3.0269442 -2.2851124 -1.7036567 -0.89029551 -0.87612963 -0.91325092 -2.3279042 -3.1091728 -5.1345944 -6.4049234 -7.2792959 -8.0364161][-4.6410275 -3.4405656 -2.7148218 -1.5525351 -1.0115633 -0.11036873 0.99891186 0.791564 0.67368317 -0.83359861 -1.8483505 -4.3308868 -5.613452 -6.4057651 -7.307982][-3.8015885 -2.61022 -1.982408 -0.79093266 0.37465954 1.5749874 2.5970402 2.2053843 2.0890198 0.18287802 -1.141964 -3.6845479 -4.8559318 -5.6418986 -6.6390648][-3.4924426 -2.176044 -1.1360598 0.28292751 1.5583715 2.6528807 3.3696032 3.1195736 3.1678009 0.93358326 -0.59764433 -3.6402507 -5.0978522 -5.6435022 -6.3607512][-3.4608035 -2.4202852 -1.5401864 0.24355507 1.5590305 2.7187996 3.5010777 3.4103537 3.0042782 0.70235729 -0.838737 -4.0009222 -5.330616 -6.1125274 -6.8521833][-3.5295539 -2.9740009 -2.074141 -0.71658087 0.28981638 1.5695343 2.468811 2.6012468 2.3883781 0.25057316 -1.1869187 -4.1671047 -5.5562 -6.4041338 -7.124227][-4.2060547 -3.6755476 -3.0736961 -2.0269389 -1.0237598 -0.28682137 0.10284567 0.35916328 0.32636166 -1.5134401 -2.8926973 -5.1845069 -6.1855192 -7.0387611 -7.9397631][-4.7436428 -4.42436 -4.3107433 -3.5013151 -2.973537 -2.5497136 -2.3318086 -2.2898126 -2.4916883 -3.8282721 -5.0474977 -6.4524369 -7.0709925 -7.840764 -8.3162937][-5.5347877 -5.2541504 -5.354394 -4.71418 -3.9706547 -3.7203722 -3.5093136 -3.8115723 -4.1696644 -5.6180754 -6.54398 -7.3539128 -7.9249258 -8.3207531 -8.3126221][-6.2167482 -5.9051929 -5.8429842 -5.3456583 -4.9514222 -4.5509567 -4.3553476 -4.5841312 -4.9164982 -6.0631528 -6.882081 -7.6134415 -8.0361252 -8.2881937 -8.3466415][-5.1463909 -5.305923 -4.9368916 -4.2743721 -4.1912608 -4.1635256 -3.9351959 -4.2370892 -4.7982268 -5.5208192 -6.4002137 -6.8425522 -7.0557361 -7.4531293 -7.8038273][-5.9047894 -6.0360947 -5.78563 -5.5748038 -5.7126083 -5.2809796 -4.90009 -5.2676554 -5.210361 -5.7654276 -6.108995 -6.4592581 -6.8020492 -6.97859 -6.9355564]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:11:33.990039: step 33010, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 55h:05m:31s remains)
INFO - root - 2017-12-15 16:11:40.342985: step 33020, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:52m:07s remains)
INFO - root - 2017-12-15 16:11:46.738018: step 33030, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 53h:41m:56s remains)
INFO - root - 2017-12-15 16:11:53.174174: step 33040, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 52h:51m:54s remains)
INFO - root - 2017-12-15 16:11:59.559078: step 33050, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 52h:51m:41s remains)
INFO - root - 2017-12-15 16:12:05.883132: step 33060, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 52h:06m:54s remains)
INFO - root - 2017-12-15 16:12:12.270814: step 33070, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 54h:14m:12s remains)
INFO - root - 2017-12-15 16:12:18.647700: step 33080, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 53h:10m:41s remains)
INFO - root - 2017-12-15 16:12:25.101634: step 33090, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 54h:35m:03s remains)
INFO - root - 2017-12-15 16:12:31.523782: step 33100, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 52h:35m:37s remains)
2017-12-15 16:12:32.035550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7233114 -4.4529333 -5.2007551 -5.6234241 -5.7626185 -5.4849091 -5.1274366 -4.4721212 -3.6429191 -2.8485703 -2.676887 -4.1939707 -5.3685789 -6.480895 -7.2316666][-3.2241778 -3.5697365 -4.3345475 -5.2335234 -6.0250964 -6.1525 -5.8857837 -5.1826291 -3.9861274 -3.2475929 -2.9757423 -3.9682653 -4.7202997 -6.0388331 -6.8974767][-4.4252968 -4.5032454 -4.9285412 -5.1802149 -5.3363161 -5.6424818 -5.5357966 -5.4796362 -4.8585367 -4.3560019 -3.8847735 -4.7800131 -5.6836872 -6.4687405 -6.814363][-4.6145897 -4.5983019 -4.6563287 -4.0318794 -3.6114478 -3.4740334 -3.5147552 -3.8394358 -3.6168571 -3.3111882 -3.8117104 -4.9962454 -5.829792 -6.6780076 -7.5694494][-4.3467503 -4.0660958 -3.2597046 -2.39503 -1.9490528 -1.3538771 -0.8506732 -0.89838123 -0.76988029 -0.75929689 -1.803997 -3.6215892 -4.8106909 -5.94585 -6.8110981][-3.87887 -3.1996298 -2.290781 -1.2678938 -0.28998995 0.4386158 0.7569313 0.89229393 1.1147528 1.0516949 -0.05991888 -2.1203437 -3.5282264 -4.6421518 -5.2116194][-3.1098709 -2.4678264 -1.1433291 -0.15378904 0.79410648 1.2356949 1.4823198 1.6666861 1.794631 1.5671835 0.29425955 -1.4473028 -2.6849675 -4.0360327 -4.6899929][-2.4576869 -1.7718272 -0.59021759 0.057204247 0.6642704 1.2235889 1.5811386 1.7177248 1.802021 1.7294273 0.66618824 -1.3010197 -2.9329424 -3.9175072 -4.8740759][-2.138814 -1.647532 -1.0936551 -0.54586315 -0.2772646 0.10452032 0.35110664 0.86248779 1.1880054 0.86683083 -0.47367764 -2.2167692 -3.9424546 -5.0432734 -5.8551531][-2.506505 -2.0770254 -1.6988153 -1.3272052 -1.2856336 -1.1929226 -1.1963224 -0.73927593 -0.51224422 -0.80389071 -2.1735458 -3.4058475 -4.9212661 -6.055881 -7.0140996][-5.1020203 -4.5526648 -4.090632 -3.4478621 -3.2658143 -2.9169297 -2.6397772 -2.4597859 -2.6155906 -3.0747414 -4.0607262 -4.8452034 -5.911706 -6.6063175 -7.3544164][-7.0010986 -6.9629292 -7.2869434 -6.4851522 -5.6716452 -5.1951437 -4.9300156 -4.8424664 -4.6211262 -4.9659786 -5.8007011 -6.3866048 -6.8001413 -6.7302332 -6.9271955][-8.1829834 -8.3313494 -8.5193014 -8.0215616 -7.3973155 -6.6565256 -6.1745677 -6.1591706 -6.02416 -5.9144993 -6.4055638 -6.5326562 -6.7323256 -6.5384574 -6.25019][-9.11911 -9.0497684 -8.736599 -7.9875555 -7.7036757 -6.9642234 -6.2641573 -6.1423459 -6.1034551 -6.2389779 -6.4002519 -6.1425304 -6.0360847 -5.579226 -5.3602953][-8.1880846 -8.30798 -8.2066736 -7.79755 -7.3331738 -6.7707558 -6.3342161 -6.4353762 -6.3435564 -6.299695 -5.9262185 -5.9792252 -5.809123 -5.647665 -5.402256]]...]
INFO - root - 2017-12-15 16:12:38.338324: step 33110, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 51h:35m:23s remains)
INFO - root - 2017-12-15 16:12:44.805933: step 33120, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 52h:10m:28s remains)
INFO - root - 2017-12-15 16:12:51.182997: step 33130, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 52h:31m:13s remains)
INFO - root - 2017-12-15 16:12:57.542021: step 33140, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 54h:38m:10s remains)
INFO - root - 2017-12-15 16:13:03.969355: step 33150, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 53h:20m:10s remains)
INFO - root - 2017-12-15 16:13:10.339903: step 33160, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 54h:56m:31s remains)
INFO - root - 2017-12-15 16:13:16.870772: step 33170, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 54h:05m:45s remains)
INFO - root - 2017-12-15 16:13:23.361954: step 33180, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 53h:56m:36s remains)
INFO - root - 2017-12-15 16:13:29.721353: step 33190, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 53h:59m:51s remains)
INFO - root - 2017-12-15 16:13:36.113135: step 33200, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 53h:41m:50s remains)
2017-12-15 16:13:36.583038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6417484 -5.92319 -5.7573414 -5.804008 -5.853334 -5.4814281 -4.7941031 -4.7370176 -4.5755196 -5.2599449 -6.494504 -7.1864281 -6.6736808 -7.9683113 -7.8713613][-5.1742458 -5.4953089 -5.9919281 -6.5669613 -6.26221 -6.1497631 -5.5945759 -4.5691271 -3.6742191 -4.8339148 -6.2417288 -7.3048735 -7.8929715 -9.3826818 -8.8701134][-4.8799963 -5.2962766 -5.2058415 -5.8411474 -6.5003929 -6.3980522 -5.4680204 -4.753952 -4.0044594 -4.4214954 -5.3788681 -6.3385887 -6.9229288 -8.06671 -8.2290134][-4.0566978 -4.5512013 -4.8877773 -5.2971969 -5.1604996 -4.4624844 -3.9536419 -3.201395 -2.5271897 -3.1481056 -4.5469761 -5.8453045 -5.86203 -7.206727 -7.6264353][-5.1937122 -5.3038468 -4.3854036 -4.0499773 -3.6354132 -2.5500145 -1.4593477 -0.58598614 0.12534809 -1.2659502 -2.8523674 -4.3521452 -5.0940089 -6.8387418 -7.2392979][-5.5386744 -4.9768372 -3.9832587 -2.6126404 -1.1190205 0.81923485 2.0272217 2.5642395 2.7491264 0.644166 -2.0245452 -4.0559354 -4.426775 -5.9323454 -6.6526256][-5.2961769 -4.3359933 -3.4143724 -1.5086513 0.99979305 3.1608562 4.5609531 5.0745029 5.051199 1.827034 -1.3669753 -3.5884161 -4.8938365 -6.5032024 -6.2211175][-4.4581394 -3.7428319 -2.0116591 -0.02272892 1.9782705 4.2805767 5.5960264 6.0661612 5.3784237 2.2551556 -0.68400478 -3.522923 -4.8571434 -6.5250463 -6.8049803][-4.6386809 -3.4435577 -2.8979936 -1.0027452 1.2009306 3.0679016 4.2191305 4.5570517 4.1189308 1.444046 -1.9648023 -4.3337517 -4.9485488 -6.8571153 -6.6159053][-5.60013 -5.002243 -4.0446086 -2.4751935 -1.0130858 0.59803009 1.3722372 1.7273655 1.7368317 -0.830987 -3.3640928 -5.668766 -6.768786 -7.920197 -7.4673762][-6.9761105 -6.403616 -6.1714692 -5.2835245 -4.4316559 -3.4082665 -2.5657601 -2.3567333 -2.3691869 -4.0410652 -6.0263638 -7.4364805 -7.5687714 -8.8658609 -8.8151][-7.4555435 -7.4617667 -6.9599004 -6.5592432 -6.3138466 -5.7336888 -5.6634741 -5.2716761 -4.9509783 -6.424758 -7.13543 -7.8905363 -8.0319748 -8.392869 -8.1665077][-8.7033415 -8.4206772 -7.8641028 -7.1643806 -6.8634677 -6.6103354 -6.2655735 -6.4781809 -6.9163313 -7.6006227 -8.17181 -8.1260748 -7.8721733 -7.7406526 -7.6883597][-7.4791656 -7.8466043 -7.5540104 -6.8201742 -6.5831089 -6.2685614 -6.1676073 -6.2945757 -6.2755442 -7.1415687 -7.2490668 -7.5638523 -7.0991793 -6.7630434 -6.8197579][-8.9061546 -8.2764034 -8.0308552 -6.9976482 -6.7581177 -6.7501378 -6.6085324 -6.5277877 -6.9940038 -7.1556911 -7.0248303 -6.7914715 -6.4553523 -5.9470272 -5.5436311]]...]
INFO - root - 2017-12-15 16:13:42.978404: step 33210, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:04m:59s remains)
INFO - root - 2017-12-15 16:13:49.423173: step 33220, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 53h:10m:41s remains)
INFO - root - 2017-12-15 16:13:55.858473: step 33230, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:05m:10s remains)
INFO - root - 2017-12-15 16:14:02.225788: step 33240, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 52h:25m:35s remains)
INFO - root - 2017-12-15 16:14:08.590001: step 33250, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:54m:41s remains)
INFO - root - 2017-12-15 16:14:15.080060: step 33260, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 52h:49m:12s remains)
INFO - root - 2017-12-15 16:14:21.520573: step 33270, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 54h:16m:47s remains)
INFO - root - 2017-12-15 16:14:27.898410: step 33280, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 51h:34m:31s remains)
INFO - root - 2017-12-15 16:14:34.345200: step 33290, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 55h:40m:54s remains)
INFO - root - 2017-12-15 16:14:40.722353: step 33300, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 52h:41m:04s remains)
2017-12-15 16:14:41.192295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6287346 -3.4195457 -3.6936543 -3.3756742 -2.9699936 -2.4449859 -1.9143701 -1.3442869 -0.8986268 -1.5298162 -2.5949502 -4.2435837 -5.8545556 -7.0089645 -8.4234495][-2.6641965 -2.6550918 -3.1040816 -3.1918144 -3.1065698 -2.7115917 -2.0301809 -1.4609318 -0.91883993 -1.4612031 -1.875433 -2.9701252 -4.4594297 -5.5846243 -6.9860272][-3.0749159 -2.662004 -2.4974198 -2.2955427 -2.1165776 -1.7001987 -0.99885559 -0.57773161 -0.15961409 -0.55930662 -0.94991493 -2.1132212 -3.0104175 -4.0166483 -5.3736668][-2.8248858 -2.2262926 -2.0955844 -1.9373631 -1.852623 -1.2978277 -0.64354086 -0.35865211 -0.038533211 -0.67091465 -1.3577847 -2.474503 -3.4615769 -3.9744213 -4.7582626][-3.2498608 -2.5360742 -2.0632024 -1.6103506 -0.98005247 -0.19739342 0.48112106 0.59039116 0.61941528 -0.29944229 -1.123085 -2.4148951 -3.4279046 -4.0558128 -4.9031277][-3.5722694 -3.021709 -2.5705585 -1.5553617 -0.50444984 0.5156889 1.6066408 1.926959 1.749073 0.44772243 -0.91352844 -2.7847724 -4.1879129 -4.8503022 -5.441257][-3.9496791 -3.3741388 -2.5970855 -1.7019053 -0.86146593 0.1103282 1.3326731 1.6693916 1.7582378 0.58344269 -0.84867048 -2.8463063 -4.3940778 -5.4328504 -6.4362388][-3.4421711 -3.2405624 -2.7147889 -1.7405081 -0.70761442 0.2323041 1.1733875 1.5685854 1.7954969 0.62466049 -0.74358034 -2.9256988 -4.5851278 -5.63223 -6.4333596][-2.8417053 -2.6155758 -2.4446006 -1.508297 -0.50643158 0.39459991 1.2275133 1.4453888 1.5387239 0.46751213 -0.81277704 -2.86696 -4.3915663 -5.3064327 -6.1165538][-2.5715919 -2.2711072 -1.8861699 -1.1944842 -0.36326456 0.20153618 0.79851246 1.1088581 1.1783915 -0.08673954 -1.4125037 -3.22222 -4.7693796 -5.5588627 -6.3192759][-2.8213181 -2.8451495 -2.5954385 -1.7957616 -0.85715103 -0.31364918 0.17590857 0.4464674 0.12403154 -1.0485277 -2.0882673 -3.6752734 -4.9542818 -5.3355737 -5.65506][-4.2879086 -3.7795329 -3.0837069 -2.7292638 -1.8887444 -1.0725007 -0.47897863 -0.45600653 -0.64750195 -1.2923594 -2.0105977 -3.4502063 -4.764338 -4.7554398 -5.0133905][-5.806241 -5.1831617 -4.3519816 -3.6449218 -2.85213 -1.9230461 -1.4561343 -1.4108105 -1.4449096 -2.1749024 -2.8207498 -3.7899504 -4.5794649 -4.661375 -4.8372593][-6.3669519 -5.9251833 -5.2265434 -4.9162526 -4.3154035 -3.5724373 -3.2752166 -2.9625645 -2.6335392 -3.0673065 -3.49395 -4.333746 -4.8133593 -4.7903719 -5.1180978][-7.5194583 -7.1838388 -6.8316216 -6.4152675 -6.2499995 -5.7201262 -5.3802247 -5.3762012 -5.4048009 -5.270875 -5.1199536 -5.2390685 -5.5893908 -5.8668623 -6.02545]]...]
INFO - root - 2017-12-15 16:14:47.545533: step 33310, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.632 sec/batch; 52h:33m:40s remains)
INFO - root - 2017-12-15 16:14:53.972062: step 33320, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 51h:25m:00s remains)
INFO - root - 2017-12-15 16:15:00.393779: step 33330, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 53h:24m:54s remains)
INFO - root - 2017-12-15 16:15:06.759006: step 33340, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 51h:21m:35s remains)
INFO - root - 2017-12-15 16:15:13.155299: step 33350, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 53h:02m:06s remains)
INFO - root - 2017-12-15 16:15:19.547499: step 33360, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 53h:02m:50s remains)
INFO - root - 2017-12-15 16:15:25.969795: step 33370, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 53h:04m:46s remains)
INFO - root - 2017-12-15 16:15:32.469669: step 33380, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 53h:04m:11s remains)
INFO - root - 2017-12-15 16:15:38.964881: step 33390, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 53h:10m:33s remains)
INFO - root - 2017-12-15 16:15:45.311019: step 33400, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 54h:50m:30s remains)
2017-12-15 16:15:45.874140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3432555 -4.8294516 -5.2444386 -5.9502325 -5.94131 -6.0535617 -6.2145925 -5.2739677 -4.3610725 -3.9077764 -4.2935247 -4.649087 -5.4577513 -6.91148 -7.4361024][-4.0838013 -4.0044575 -4.3940754 -5.1915283 -5.9712548 -7.0895905 -7.1387138 -6.6258845 -5.8591104 -4.5141182 -4.1471319 -4.1568041 -4.4351912 -5.4730768 -6.1072993][-5.0170979 -4.9446235 -4.9183989 -5.387641 -6.1598315 -6.89998 -7.2429581 -7.4002275 -6.557291 -5.8620558 -5.65026 -5.59251 -5.1080837 -5.8828006 -6.2901039][-5.8461647 -5.5577097 -5.256187 -4.9793396 -5.0869341 -5.2304626 -5.3925791 -5.5841784 -5.4072628 -5.2416534 -5.8069191 -6.1403589 -6.1154613 -6.935442 -6.7606325][-5.5306721 -4.7953348 -4.3410034 -3.7957938 -3.472703 -3.2206426 -2.8142323 -2.6416206 -2.7198973 -2.7059288 -3.7607148 -5.0702181 -5.7769938 -7.1326523 -7.1295042][-4.5233197 -3.7721038 -2.9330106 -1.9197397 -1.3638124 -0.6793294 0.14914846 0.56223774 0.69782734 0.44921398 -1.0616279 -3.2160916 -4.2001581 -5.8921208 -6.0481825][-3.9847779 -3.3724794 -1.9177384 -0.90320873 0.076051712 1.2498846 2.026845 2.4418993 2.660511 2.2291594 0.044868469 -1.8588357 -2.77253 -4.7009821 -5.12144][-3.7343237 -2.7859941 -1.9068894 -0.74360847 0.548789 1.7051792 2.3708048 2.8230286 3.0615349 2.3787651 0.21038771 -1.5172882 -2.5961223 -4.1941924 -4.5219374][-3.1894708 -2.5446014 -1.9186883 -1.125278 -0.40982151 0.68472195 1.6314859 1.9326801 2.0355234 1.2605543 -0.77233267 -2.1862001 -3.4729629 -5.11545 -5.4528627][-3.9114962 -2.8863068 -2.0997229 -1.5634785 -0.91196585 -0.38771439 -0.29032135 0.31348896 0.34337711 -0.44183397 -2.1967716 -3.125515 -4.2856879 -5.8510237 -6.30025][-5.7305632 -4.6864882 -4.240449 -3.1925654 -2.3713465 -2.3331027 -1.950067 -1.7721124 -2.2148681 -2.7520947 -4.3657789 -4.6980228 -5.3918648 -6.7175179 -6.9404697][-7.2081571 -7.0868759 -6.3305726 -5.077167 -4.2907624 -3.7342696 -3.0840364 -3.305584 -3.6523833 -3.847003 -5.3673735 -5.8636003 -6.34891 -7.2697196 -7.2806978][-8.7396755 -9.1424093 -8.8231936 -8.0777512 -7.4383593 -6.5103207 -5.6278009 -5.5563993 -5.5402241 -5.358078 -6.0691257 -5.9683876 -6.1594934 -6.6158757 -6.5763011][-8.64278 -9.08502 -8.8621187 -8.4075384 -7.9049611 -7.1261206 -6.4207025 -5.9481764 -5.8605108 -5.1399264 -5.4439139 -5.25377 -5.07178 -5.5394382 -5.5145731][-8.5017309 -8.5575438 -8.3365583 -8.3408489 -7.9822245 -7.4669876 -6.8557024 -6.5104074 -6.1908178 -5.7918854 -5.5796423 -5.07833 -4.457119 -4.6578584 -4.6237688]]...]
INFO - root - 2017-12-15 16:15:52.238216: step 33410, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 52h:21m:47s remains)
INFO - root - 2017-12-15 16:15:58.732218: step 33420, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 54h:20m:32s remains)
INFO - root - 2017-12-15 16:16:05.111670: step 33430, loss = 0.34, batch loss = 0.22 (12.0 examples/sec; 0.665 sec/batch; 55h:16m:26s remains)
INFO - root - 2017-12-15 16:16:11.528300: step 33440, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 51h:46m:12s remains)
INFO - root - 2017-12-15 16:16:17.960443: step 33450, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 54h:40m:38s remains)
INFO - root - 2017-12-15 16:16:24.325662: step 33460, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 53h:50m:30s remains)
INFO - root - 2017-12-15 16:16:30.796469: step 33470, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 53h:17m:38s remains)
INFO - root - 2017-12-15 16:16:37.123333: step 33480, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 52h:01m:51s remains)
INFO - root - 2017-12-15 16:16:43.519216: step 33490, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 53h:20m:04s remains)
INFO - root - 2017-12-15 16:16:49.985182: step 33500, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 54h:14m:15s remains)
2017-12-15 16:16:50.486243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6952214 -6.9816446 -7.0361238 -6.2448964 -4.7506304 -3.668025 -2.6699953 -2.027288 -1.4757295 -1.5784698 -3.2422347 -4.7456751 -5.4775467 -6.2720766 -7.3300118][-6.6835423 -6.89301 -7.5192432 -6.937295 -5.9630251 -4.4994254 -2.9854636 -2.4967189 -2.2928562 -2.7682705 -4.2194109 -5.8071795 -6.7532353 -7.1398282 -7.7517805][-6.5518789 -7.0056472 -7.3008857 -6.7786722 -6.4489961 -5.4864311 -3.7133207 -3.0163722 -2.6458702 -3.2525029 -4.9674339 -6.3907375 -7.410171 -8.2446527 -8.8980913][-6.0889163 -6.6865711 -7.1042619 -6.4885807 -5.5074797 -4.4409266 -3.2296462 -2.8005519 -2.5028214 -3.0011206 -4.9532709 -7.02442 -7.68172 -8.0151 -8.9961748][-7.1615829 -7.6428719 -7.422524 -6.6044712 -5.0277414 -3.6625509 -2.6015859 -2.2408271 -2.0438066 -2.583889 -4.00431 -6.1972466 -7.6486249 -8.7216015 -9.5278788][-8.5262947 -8.8138752 -8.0795679 -6.5845227 -4.8000174 -2.9208155 -0.98954296 -0.6877079 -0.5997448 -1.5840335 -3.6465507 -5.2848506 -6.9093709 -8.5993614 -9.695797][-8.7900419 -8.1270733 -6.92359 -5.68655 -3.970927 -1.6406684 0.3704443 0.75877666 1.186017 0.42880058 -1.6561446 -4.3074136 -6.1497335 -7.7899489 -9.3583336][-7.9506264 -8.0231466 -7.0221848 -4.2762918 -1.9298644 -0.2624836 1.1657457 2.1694403 3.4792042 2.4654522 0.04865551 -2.6149373 -5.0216732 -7.0780149 -8.5642471][-7.35039 -6.7815313 -6.4570804 -5.1271863 -3.3954864 -0.53415394 1.3106537 1.3502197 2.3293 2.4778004 1.0397024 -1.3185964 -3.4744229 -5.6963243 -7.8239851][-7.7188282 -6.8970776 -6.4899564 -5.70428 -4.8046646 -3.0391822 -1.2794452 0.36955547 1.1200466 0.16550589 -1.0408082 -2.5038886 -3.7977698 -5.4289193 -6.6636057][-6.973763 -7.2539382 -6.8138962 -5.8934836 -5.03507 -3.9804163 -3.0954156 -2.4773006 -1.2286744 -1.728054 -4.0601521 -5.3531461 -5.9114394 -6.0743685 -6.5361271][-6.0501418 -6.1300383 -5.547205 -4.9673429 -4.0624428 -2.6690798 -1.3205142 -1.1435585 -1.1773095 -2.688827 -4.6256285 -6.0117073 -7.0006623 -7.2499328 -7.2572656][-6.7524223 -5.647645 -4.8393993 -3.9890065 -2.9274673 -2.6710515 -1.7375832 -0.806232 -0.37844133 -2.7312775 -4.4773722 -5.2353058 -5.4223237 -6.225163 -6.8591309][-7.2562022 -6.4694815 -5.4601889 -4.3150034 -3.5474677 -2.5230165 -1.3856912 -1.9010177 -1.5833097 -2.0618858 -3.0909619 -4.57255 -5.1302648 -5.2901578 -5.4827566][-6.9911594 -6.5694375 -5.86438 -4.88585 -4.0781264 -3.3161068 -2.5750027 -2.4401398 -2.5658574 -3.4832516 -4.077486 -4.3645239 -4.4801965 -5.6598268 -6.2631216]]...]
INFO - root - 2017-12-15 16:16:56.950465: step 33510, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 52h:58m:20s remains)
INFO - root - 2017-12-15 16:17:03.414561: step 33520, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 54h:07m:30s remains)
INFO - root - 2017-12-15 16:17:09.851455: step 33530, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 52h:49m:11s remains)
INFO - root - 2017-12-15 16:17:16.223479: step 33540, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 53h:54m:22s remains)
INFO - root - 2017-12-15 16:17:22.617379: step 33550, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 52h:04m:33s remains)
INFO - root - 2017-12-15 16:17:29.148059: step 33560, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 54h:56m:56s remains)
INFO - root - 2017-12-15 16:17:35.603500: step 33570, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 53h:20m:26s remains)
INFO - root - 2017-12-15 16:17:42.046902: step 33580, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 53h:15m:00s remains)
INFO - root - 2017-12-15 16:17:48.426803: step 33590, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 53h:25m:10s remains)
INFO - root - 2017-12-15 16:17:54.874581: step 33600, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 53h:04m:40s remains)
2017-12-15 16:17:55.391164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2059555 -3.1792741 -3.61241 -3.3926415 -3.4000936 -2.7816329 -2.7533259 -2.9498639 -2.6753292 -4.5145493 -5.2395053 -6.3697281 -6.8291545 -7.4981446 -9.0831442][-3.7447517 -3.5363331 -4.3021555 -4.0259428 -3.8295383 -3.6305451 -3.3907728 -2.8837576 -3.1617708 -5.1905322 -5.9746456 -7.4016495 -8.0573235 -7.9424205 -8.2429047][-3.3466187 -3.290885 -3.2838106 -2.3534756 -2.1527705 -1.8479099 -1.7830606 -1.8029237 -1.775116 -3.875402 -4.873878 -6.1526504 -6.4412336 -6.5339088 -7.3076386][-1.9918818 -1.758296 -1.6634798 -0.66366291 0.059923172 0.46270657 0.50418854 0.64903259 0.15676403 -2.7628398 -4.4097729 -5.4130869 -5.9968534 -5.6527596 -5.8510156][-3.563858 -1.8023286 -0.55605364 0.65413 1.3253155 1.7586126 2.0453415 2.0727377 2.1915483 -0.60776854 -2.2827215 -4.0360603 -4.93859 -5.0909653 -5.8757548][-3.0447898 -1.8638759 -0.9518528 0.61407471 1.8692961 2.7517376 3.3678389 3.0232134 2.8639746 0.32189751 -1.5657411 -3.532999 -4.9326348 -5.2236962 -5.6075048][-3.8243747 -2.4465203 -1.1921091 -0.026531696 1.7491207 3.1769209 4.1405964 3.9013348 3.4712706 0.50444889 -1.4473777 -3.5444674 -5.1128645 -5.7191992 -6.7182631][-3.2053604 -1.8777609 -0.46229172 1.2229052 2.5464115 3.2044706 3.9555578 3.6202784 3.0952168 0.31685638 -1.5372148 -4.0876226 -5.6200285 -6.483336 -7.6235418][-3.8317878 -2.427537 -1.0837445 0.83834839 2.1185303 2.8271008 3.2924995 3.447526 3.6214142 0.77345276 -1.5864539 -3.7910497 -5.626812 -6.8720646 -7.9912419][-3.1837597 -2.5829148 -1.761795 0.10255957 1.444005 1.8497372 2.1353149 2.1972017 2.1681633 -0.31126213 -1.9415312 -4.2246304 -6.1850572 -7.3985381 -8.63424][-5.0681586 -4.5079813 -3.6345735 -2.533792 -1.3834734 -0.57507563 -0.20955086 -0.034787178 0.32006741 -2.2218204 -4.4604073 -5.3648729 -6.3551884 -7.4340658 -8.5893011][-6.1498117 -5.0231209 -4.4922161 -3.5667171 -2.5671425 -2.1747413 -1.8572555 -1.8115535 -1.692255 -2.6870646 -3.5687451 -5.171731 -6.3546705 -7.2762938 -8.5027056][-7.772285 -7.0405636 -6.161468 -5.6075163 -5.2438612 -4.4271417 -3.8355598 -4.0817137 -4.0324893 -4.637651 -5.4148154 -5.5526552 -6.5295768 -6.9258504 -7.6525135][-7.6765947 -7.588295 -7.1325712 -6.7233748 -6.3332367 -5.9839745 -5.4048138 -5.0210028 -4.9078236 -5.4002719 -5.615428 -6.0932641 -6.3080049 -6.5095615 -6.9477344][-9.4609709 -9.0775919 -8.6620636 -8.5846624 -8.2986393 -7.7507892 -7.0549612 -7.0073142 -6.8021855 -6.35651 -6.7044353 -6.8002343 -6.8730197 -6.6320186 -6.3696861]]...]
INFO - root - 2017-12-15 16:18:01.834200: step 33610, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 53h:02m:09s remains)
INFO - root - 2017-12-15 16:18:08.174104: step 33620, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 51h:39m:40s remains)
INFO - root - 2017-12-15 16:18:14.607963: step 33630, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:52m:09s remains)
INFO - root - 2017-12-15 16:18:21.042112: step 33640, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 53h:55m:30s remains)
INFO - root - 2017-12-15 16:18:27.513185: step 33650, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 53h:57m:45s remains)
INFO - root - 2017-12-15 16:18:33.923924: step 33660, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 54h:07m:12s remains)
INFO - root - 2017-12-15 16:18:40.351457: step 33670, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.618 sec/batch; 51h:18m:48s remains)
INFO - root - 2017-12-15 16:18:46.721792: step 33680, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 52h:32m:59s remains)
INFO - root - 2017-12-15 16:18:53.106597: step 33690, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 52h:16m:56s remains)
INFO - root - 2017-12-15 16:18:59.616564: step 33700, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 54h:54m:08s remains)
2017-12-15 16:19:00.138771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3948669 -5.9843073 -6.8432183 -6.313241 -5.2378664 -5.4723673 -5.7978234 -5.1590004 -4.0743895 -4.8159604 -4.5965991 -6.10175 -5.8889432 -5.5869765 -5.8297873][-5.695775 -5.6741753 -5.8441978 -5.2602525 -4.4510489 -4.474627 -4.5201082 -5.1194439 -4.8200421 -4.9685555 -4.412447 -7.21809 -7.4857426 -6.5549808 -6.3972549][-5.455121 -5.8598385 -6.1984215 -4.752841 -3.3078299 -3.4175878 -3.4673729 -3.7766826 -4.101016 -5.1450815 -4.9722948 -7.4001851 -7.389739 -7.3978629 -7.2472415][-5.8460875 -5.436512 -5.3793912 -3.9904146 -2.6671219 -2.0352149 -1.4726205 -1.950408 -2.3376722 -3.5837188 -4.1669207 -7.2074976 -7.8360763 -7.5850668 -7.341383][-4.3889637 -3.4161644 -2.8124089 -0.98875856 -0.42631483 -0.17344522 0.28310585 -0.46753407 -1.3230095 -2.8758712 -2.7637606 -6.040782 -7.5359521 -7.79652 -7.6551914][-4.5893493 -2.77766 -1.092814 -0.018234253 -0.1140871 0.50461292 0.35477543 -0.40050983 -0.88081551 -2.4850368 -2.9584785 -5.8807039 -6.7621989 -7.5640793 -8.13548][-4.2500896 -2.6582441 -1.0997415 0.32436466 1.0533094 1.3478727 1.0910053 -0.30958366 -1.5636578 -2.7080626 -2.6433783 -6.5772796 -7.8860097 -7.6923604 -7.453289][-4.3405495 -2.7335825 -1.4718542 0.10402536 1.3368549 2.0914698 1.765132 0.017747402 -1.1418748 -2.5943661 -3.3755856 -6.9394174 -7.3902206 -7.180831 -6.8194194][-6.1780477 -4.4200644 -3.4156327 -1.5484576 -0.0096139908 1.0650215 0.7408123 -0.01830864 -0.055568218 -1.9858232 -3.0296707 -6.8764744 -8.2202168 -7.479856 -6.7728834][-7.6787415 -7.1744609 -6.588789 -4.3363743 -2.543076 -1.6141434 -1.1961179 -0.67057371 -0.8044343 -2.966608 -3.6655774 -6.8280072 -7.6958628 -7.6945496 -7.2482123][-9.3855085 -9.1884289 -9.0982027 -7.2562909 -6.1248107 -5.5014076 -4.7641106 -3.7221944 -3.4119639 -4.8582888 -5.7449365 -7.6296544 -7.3810639 -7.2970567 -7.5439897][-9.7991858 -9.7148151 -9.4396486 -8.3718805 -7.6697855 -7.2850733 -6.7083917 -5.9982738 -5.5642586 -6.2042656 -6.3359351 -7.2620449 -7.0747447 -7.4972649 -7.2778192][-11.065357 -10.606944 -10.026967 -9.2684507 -8.3186541 -7.8499179 -7.6077628 -7.1778297 -7.1691265 -7.9903913 -7.7380524 -7.2237754 -6.0374036 -5.6717825 -5.9192228][-10.793592 -10.317093 -9.9734163 -9.26605 -8.5907993 -7.6422987 -6.5754433 -6.794529 -7.38064 -7.7210808 -7.2230663 -7.0457582 -6.0180149 -5.6308913 -5.0553083][-8.7457047 -8.8346462 -9.0679312 -9.1056767 -9.2376337 -8.1040459 -7.4635816 -7.4148245 -7.1955338 -7.3677573 -7.3258724 -6.8873429 -6.4540868 -6.2888126 -6.0864081]]...]
INFO - root - 2017-12-15 16:19:06.488394: step 33710, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 52h:14m:55s remains)
INFO - root - 2017-12-15 16:19:12.895324: step 33720, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.639 sec/batch; 52h:59m:55s remains)
INFO - root - 2017-12-15 16:19:19.318862: step 33730, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 52h:12m:45s remains)
INFO - root - 2017-12-15 16:19:25.685508: step 33740, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 53h:54m:08s remains)
INFO - root - 2017-12-15 16:19:31.985534: step 33750, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 51h:50m:55s remains)
INFO - root - 2017-12-15 16:19:38.370740: step 33760, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:31m:03s remains)
INFO - root - 2017-12-15 16:19:44.741952: step 33770, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 53h:56m:42s remains)
INFO - root - 2017-12-15 16:19:51.064139: step 33780, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.643 sec/batch; 53h:18m:49s remains)
INFO - root - 2017-12-15 16:19:57.402541: step 33790, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.670 sec/batch; 55h:37m:11s remains)
INFO - root - 2017-12-15 16:20:03.873469: step 33800, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 54h:05m:08s remains)
2017-12-15 16:20:04.382252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8114271 -2.57311 -2.6316319 -2.3407445 -2.3337684 -2.1375184 -1.7999649 -1.3812704 -1.5305476 -3.203423 -4.1527309 -5.478837 -6.1301994 -6.800797 -7.1976275][-3.1479297 -3.3408165 -3.6608081 -3.4815407 -3.1565051 -2.8003702 -2.5471649 -2.0943007 -2.1256986 -3.6219468 -4.4514618 -5.8592024 -6.27184 -7.2811885 -7.5981455][-2.781425 -2.7811375 -2.8702693 -2.8432045 -2.4683652 -1.8047147 -1.4591384 -1.3602791 -1.2967391 -2.7902713 -3.6948743 -4.8919325 -5.2227 -6.0960407 -6.7932167][-3.0802655 -3.0397139 -3.1758919 -2.8899913 -2.440167 -1.904448 -1.1923165 -0.8816328 -0.70716953 -2.2004414 -3.0668521 -4.3055816 -4.9531794 -5.8708572 -6.3475084][-3.2234755 -2.8566308 -2.7378912 -2.6107783 -2.1312966 -1.4834132 -0.83358955 -0.32616234 0.058315754 -1.522521 -2.2668643 -3.5883927 -4.4363775 -5.492939 -6.0800033][-3.678844 -3.1067042 -2.8045039 -2.0402169 -1.1939139 -0.48691416 0.040364742 0.33727837 0.63484859 -0.9301424 -1.7818809 -3.1875477 -4.0777121 -5.2726049 -5.9756594][-3.8584967 -3.2416482 -2.6594949 -1.6885605 -0.71412992 0.19176674 0.7301693 1.02495 1.267355 -0.42541885 -1.4529295 -3.0633254 -4.0643969 -5.0842381 -5.6872115][-4.0581241 -2.9808321 -2.1340346 -0.94176435 0.085383892 1.1070871 1.6834993 1.9338455 1.9834719 0.031806469 -1.120491 -2.9577365 -4.1380405 -5.36893 -5.7226186][-3.8200107 -2.7727051 -1.7975974 -0.54185438 0.49606705 1.2235088 1.6246462 1.7768812 1.7172117 -0.29670572 -1.4797692 -3.1230164 -4.3526421 -5.689929 -6.2946329][-3.5581093 -2.6434679 -1.5295463 -0.54107618 0.40939236 1.0435953 1.2931509 1.2707052 1.0895042 -0.97433472 -2.0412812 -3.8389308 -5.0007911 -5.9976668 -6.6229353][-5.0624733 -4.19806 -3.390368 -2.3757491 -1.5661306 -0.83734655 -0.50842762 -0.53494072 -0.798635 -2.9121885 -4.0216866 -5.3910065 -6.1511688 -6.8035979 -6.8752408][-6.1601491 -5.4704514 -4.4493365 -3.5637298 -2.8494034 -2.3165154 -2.1533079 -2.3003426 -2.5689912 -4.0265255 -4.7889452 -5.8046846 -6.3045826 -6.9116783 -7.0322571][-6.7165656 -6.3277078 -5.7719717 -5.2325211 -4.6524172 -4.0961733 -3.9183686 -4.042593 -4.2636356 -5.0125093 -5.2936087 -5.7290211 -6.143146 -6.3545322 -6.0854349][-6.6063943 -6.3808775 -6.2790489 -5.8006763 -5.5835314 -5.0209389 -4.851222 -4.763206 -4.6840544 -5.3973866 -5.5259666 -5.5172586 -5.6275864 -5.7513332 -5.6302347][-7.6899624 -7.3921804 -6.6608562 -6.5337 -6.5956421 -6.2842684 -6.3077974 -6.492558 -6.721025 -6.5084915 -6.240263 -6.2154803 -6.0677991 -5.9578323 -5.8231492]]...]
INFO - root - 2017-12-15 16:20:10.800746: step 33810, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 55h:52m:15s remains)
INFO - root - 2017-12-15 16:20:17.136898: step 33820, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 51h:48m:11s remains)
INFO - root - 2017-12-15 16:20:23.582063: step 33830, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 53h:38m:59s remains)
INFO - root - 2017-12-15 16:20:29.960199: step 33840, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 52h:57m:40s remains)
INFO - root - 2017-12-15 16:20:36.333474: step 33850, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 51h:36m:54s remains)
INFO - root - 2017-12-15 16:20:42.763709: step 33860, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 52h:07m:24s remains)
INFO - root - 2017-12-15 16:20:49.184226: step 33870, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 55h:01m:33s remains)
INFO - root - 2017-12-15 16:20:55.578527: step 33880, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 51h:35m:44s remains)
INFO - root - 2017-12-15 16:21:01.994908: step 33890, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 55h:06m:02s remains)
INFO - root - 2017-12-15 16:21:08.463094: step 33900, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 53h:01m:37s remains)
2017-12-15 16:21:09.081532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3924065 -6.8631244 -7.1232429 -6.9637222 -5.9690313 -4.9412832 -3.633358 -1.937808 -0.4567275 -0.40579414 -1.3284264 -4.05735 -6.2149148 -8.5682631 -9.870244][-5.4522824 -6.2181549 -6.8107162 -7.3115468 -7.1303344 -5.8255644 -4.0684681 -1.8224163 -0.34915924 -0.56437874 -1.5642972 -3.6857419 -5.8162475 -8.40799 -9.7783241][-5.0086021 -5.7626176 -6.0065088 -5.7005563 -5.0908966 -4.53325 -3.5258975 -2.0678926 -0.57553434 -0.98130274 -1.7711015 -3.7842093 -5.1660547 -7.0625744 -8.4459562][-5.0973482 -5.0588617 -4.8138781 -4.2724981 -3.6658716 -2.7023621 -1.6706719 -1.1674371 -0.59206963 -0.8624177 -1.4943895 -3.9041162 -5.3099418 -6.7236648 -7.374474][-5.2583351 -4.6337185 -3.7480981 -2.9848194 -2.2583761 -0.90310812 0.29869318 0.80422115 1.07796 0.15708542 -0.94057846 -3.4467978 -5.1391449 -6.7564554 -7.7513876][-4.9542065 -3.8440146 -2.6063838 -1.6127052 -0.26318264 1.0375443 2.3189478 2.8108816 3.164094 1.9013023 0.26460743 -2.5941629 -4.5766177 -6.4081182 -7.5419569][-4.2050428 -2.8887463 -1.6627498 -0.64642668 0.58785915 1.7077103 3.0397491 3.7727489 4.2092218 2.7888393 1.0862112 -2.1568546 -4.4273467 -6.4460421 -7.542902][-3.6267633 -2.1757593 -0.75175285 0.16243315 1.3467588 1.9755011 2.5854216 3.4447002 4.1993494 3.0268555 1.1400137 -2.0121374 -4.4076519 -6.5593204 -7.5681782][-2.9161844 -1.6450691 -0.46799326 0.68206978 1.9857721 2.2000942 2.541441 2.8532228 3.1650362 1.917264 0.50969982 -2.1001606 -4.2958527 -6.5851016 -8.1591415][-2.5826716 -1.5900154 -0.15573311 0.75590038 1.7178202 2.2222614 2.8202429 2.7758665 2.8156338 0.89656258 -1.0242763 -3.3859844 -4.9671907 -6.5432577 -7.6423922][-2.1941319 -1.5216351 -0.57950878 0.36673355 1.3617449 1.4134865 1.7471218 1.8677721 1.8933496 0.036050797 -2.0685363 -4.3513007 -5.4928226 -6.8244352 -7.7370729][-2.8067832 -1.8881941 -0.84812975 0.0098285675 0.7562933 0.62930965 0.43519592 0.13125324 -0.31347036 -1.1620665 -2.3082952 -4.2154884 -5.7933106 -7.285212 -7.9329033][-4.2921858 -3.5172987 -2.3242459 -1.3893142 -0.33067083 -0.27388287 -0.72635365 -1.2696629 -1.7103686 -2.4642282 -3.1982765 -4.2487087 -5.4997082 -6.6481519 -7.4535694][-6.0749421 -5.5794296 -4.226397 -3.2787962 -2.1532249 -1.5793915 -1.4496937 -1.4666953 -2.1354985 -3.0529923 -4.300499 -5.3187666 -5.79315 -6.3884354 -6.4236374][-6.9039087 -6.4810281 -5.6976185 -5.4259543 -4.5021639 -3.6035657 -3.1539984 -2.8424983 -3.5815883 -4.0830421 -4.9503098 -6.1227531 -6.6194139 -6.5687437 -6.4057608]]...]
INFO - root - 2017-12-15 16:21:15.477508: step 33910, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 54h:28m:42s remains)
INFO - root - 2017-12-15 16:21:21.887182: step 33920, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 52h:27m:47s remains)
INFO - root - 2017-12-15 16:21:28.297696: step 33930, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 52h:08m:11s remains)
INFO - root - 2017-12-15 16:21:34.652409: step 33940, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 52h:34m:53s remains)
INFO - root - 2017-12-15 16:21:41.076433: step 33950, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 53h:04m:44s remains)
INFO - root - 2017-12-15 16:21:47.441856: step 33960, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 52h:11m:27s remains)
INFO - root - 2017-12-15 16:21:53.852970: step 33970, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 52h:02m:10s remains)
INFO - root - 2017-12-15 16:22:00.253728: step 33980, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 54h:20m:44s remains)
INFO - root - 2017-12-15 16:22:06.695652: step 33990, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 51h:44m:04s remains)
INFO - root - 2017-12-15 16:22:13.195462: step 34000, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 53h:16m:54s remains)
2017-12-15 16:22:13.708761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8423986 -2.5197172 -3.257596 -3.8732212 -3.5916476 -3.4825969 -3.6941197 -4.08687 -4.6643982 -5.6822124 -6.4572015 -7.4242539 -8.1888 -9.2777481 -10.034021][-3.2203984 -3.2175283 -3.4279556 -4.0427332 -4.2693644 -4.5246844 -4.5483217 -4.59177 -4.7569952 -5.7672482 -6.1579905 -7.3575468 -8.4470806 -9.1789656 -9.7190714][-2.64846 -2.8810873 -3.509871 -4.1280222 -4.1534472 -4.0095024 -3.7784224 -4.42413 -4.8032851 -5.4008231 -5.6761141 -7.0054436 -8.3479261 -9.045536 -8.8336153][-1.9903984 -3.0063949 -3.3320584 -3.2227211 -3.2656698 -3.1938357 -3.0537949 -3.1398153 -3.6424112 -4.5636797 -5.4765973 -6.5109806 -7.3363585 -8.0653343 -8.5835943][-3.698997 -2.9242845 -3.3298435 -3.398983 -2.4114423 -1.8260393 -1.7870078 -1.9074845 -2.1819673 -2.8981738 -3.5216103 -4.9712811 -6.0710492 -6.70829 -7.2934823][-4.1515856 -3.7549942 -3.4248295 -3.1103945 -2.3388844 -1.4162936 -0.31905174 0.050281525 -0.37659264 -1.5294657 -2.0350084 -3.2896805 -4.5883331 -5.674612 -6.0913315][-4.5671864 -4.397234 -3.7552886 -2.8072629 -1.3848476 -0.6094408 0.35171413 1.0204535 1.0972948 -0.39498806 -1.3197761 -2.2959146 -3.3916469 -4.8316298 -5.7441368][-4.1871257 -3.8547783 -3.3952069 -2.3415222 -1.138236 0.20702553 1.3182316 1.2177515 0.97569656 0.64617729 -0.35608673 -2.4210606 -3.3891287 -4.6835318 -5.7859139][-4.478385 -3.5677981 -2.7341652 -1.968267 -0.92922831 0.44867229 1.1879253 1.5163393 1.2340126 0.2245903 -0.51946592 -2.2629013 -3.9717939 -5.4248381 -5.9409661][-4.5377173 -4.1831179 -3.4017162 -2.0978389 -0.83810616 -0.17430401 0.35074329 0.788476 0.984684 -0.38812733 -1.7178421 -3.3088341 -4.1476107 -5.5375643 -6.619565][-4.7657475 -4.6070557 -4.1768513 -3.2123914 -2.0580888 -0.90447664 -0.28309965 -0.36137009 -0.1917634 -1.2747111 -2.411581 -3.9316196 -4.9222317 -5.8639817 -6.9773636][-5.4233732 -4.4392672 -4.1995525 -3.8690169 -3.202003 -2.2125831 -1.4266057 -1.5133977 -1.7441864 -2.136713 -3.0803661 -4.5764575 -5.366045 -6.32874 -7.0584249][-6.0272408 -5.7246847 -4.8808231 -4.2171135 -4.1785574 -3.9754341 -3.3991218 -3.0495219 -2.91504 -3.4642892 -4.3472452 -4.8693533 -5.5726566 -6.0254049 -6.4556675][-7.0580425 -6.9563246 -6.57267 -5.8708405 -5.4877825 -5.3433542 -4.9579563 -4.8283234 -4.6287861 -4.5005379 -4.8461304 -5.0860233 -5.3627615 -5.3465452 -5.5761232][-8.1750526 -7.3230357 -7.2678351 -7.3556042 -6.656878 -5.7581625 -5.7349091 -6.0999184 -5.7730961 -5.4001284 -5.3611045 -5.2105532 -4.8092556 -5.0570507 -5.3847313]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:22:20.165714: step 34010, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 54h:02m:42s remains)
INFO - root - 2017-12-15 16:22:26.571991: step 34020, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 53h:02m:48s remains)
INFO - root - 2017-12-15 16:22:32.875081: step 34030, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 55h:27m:08s remains)
INFO - root - 2017-12-15 16:22:39.190464: step 34040, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 52h:02m:45s remains)
INFO - root - 2017-12-15 16:22:45.640759: step 34050, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.645 sec/batch; 53h:26m:44s remains)
INFO - root - 2017-12-15 16:22:52.019796: step 34060, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 53h:04m:03s remains)
INFO - root - 2017-12-15 16:22:58.485086: step 34070, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 52h:47m:00s remains)
INFO - root - 2017-12-15 16:23:04.914259: step 34080, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 54h:45m:58s remains)
INFO - root - 2017-12-15 16:23:11.369168: step 34090, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 53h:14m:38s remains)
INFO - root - 2017-12-15 16:23:17.774491: step 34100, loss = 0.37, batch loss = 0.26 (12.6 examples/sec; 0.636 sec/batch; 52h:41m:22s remains)
2017-12-15 16:23:18.278804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1835284 -3.4162264 -3.4026446 -3.0205412 -3.4116631 -3.5815392 -3.5771132 -3.695029 -3.0911164 -3.5449653 -4.2310033 -5.6115026 -7.0374107 -8.60809 -10.360712][-3.5642605 -2.9257231 -2.7327957 -2.975533 -3.8274007 -4.3968782 -4.8836837 -4.859674 -3.8174689 -4.0748758 -4.8137875 -5.9499149 -6.8124156 -7.8808441 -8.8085756][-2.6565652 -2.2063503 -1.7965636 -1.4922562 -2.2107363 -2.7186971 -3.1078067 -3.404366 -3.2690487 -3.7785037 -4.5377398 -5.7985229 -6.33322 -6.9363127 -7.6985569][-1.7738156 -1.4448233 -1.1332507 -0.48556566 -0.46092272 -0.84378481 -1.4494796 -1.50524 -1.771831 -3.1597285 -4.3969393 -5.9646654 -6.4482718 -7.02641 -7.4167509][-1.7612705 -0.70849276 -0.022398949 0.28704977 0.64142513 0.90628719 0.71874142 0.24181986 -0.15380859 -1.7241325 -3.0746436 -4.4145012 -4.9721823 -5.8707948 -6.5941267][-1.8935823 -1.577621 -0.49750996 0.38780594 0.91576481 1.7073231 2.5223722 2.6142845 2.366271 0.59361172 -0.8263464 -2.0594411 -2.5133643 -3.7998798 -4.9661903][-2.3987775 -1.7810245 -0.4793787 1.0158634 2.1624699 3.0362864 3.7144117 3.6172123 3.3613043 1.3765402 -0.17933512 -1.7567205 -2.6015625 -4.1477871 -5.2917433][-2.5531645 -2.1964841 -1.3641052 0.76824856 2.2638426 3.2763004 3.9177046 3.4732103 3.0938206 1.3118296 -0.32635689 -2.0894356 -2.7213736 -4.0674944 -4.9161043][-3.725992 -2.3983378 -2.3322921 -0.85877705 0.78758907 1.8785934 2.3302059 2.1805849 2.17972 0.76703644 -0.56821728 -2.3011112 -2.7333159 -3.7120337 -4.8632832][-5.0366216 -4.5528612 -3.8140919 -2.5505009 -1.3197088 0.19202662 0.65291309 0.69200134 1.4078293 -0.30827045 -1.724771 -3.1156611 -3.5771627 -3.8624542 -4.40043][-5.8223929 -5.4715595 -4.7909203 -3.6154289 -2.3764677 -1.1394687 -0.57850981 -0.57252121 -0.47702169 -1.9632978 -3.0214787 -4.4860067 -5.260952 -5.80479 -5.9894147][-6.5082932 -6.2289524 -5.4933834 -4.5285211 -3.6354065 -2.2214255 -1.4383454 -1.7541647 -2.3035073 -3.8549693 -4.68748 -5.8709726 -6.7080069 -7.1232996 -7.2634387][-6.1679411 -6.2604461 -5.9188886 -5.2019262 -4.50955 -3.42835 -2.4909372 -2.42484 -2.6173062 -3.9235477 -5.443326 -6.0078654 -5.7461929 -6.44709 -6.8340321][-5.3194818 -5.0531483 -4.5563431 -3.9164095 -3.3760114 -2.8954692 -2.481287 -2.4987569 -2.5786319 -2.9195147 -4.1854806 -5.0823169 -5.0000358 -5.196733 -5.4175835][-5.4483709 -5.6099205 -5.3338418 -4.8756762 -4.3423595 -3.6420264 -3.4440379 -3.5766444 -3.8369389 -4.0933104 -4.6779652 -5.0941124 -5.8411756 -5.8652811 -5.685215]]...]
INFO - root - 2017-12-15 16:23:24.700019: step 34110, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 53h:56m:28s remains)
INFO - root - 2017-12-15 16:23:31.186337: step 34120, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 53h:47m:39s remains)
INFO - root - 2017-12-15 16:23:37.613399: step 34130, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 53h:52m:14s remains)
INFO - root - 2017-12-15 16:23:44.017522: step 34140, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 52h:20m:41s remains)
INFO - root - 2017-12-15 16:23:50.370718: step 34150, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 52h:00m:43s remains)
INFO - root - 2017-12-15 16:23:56.772126: step 34160, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:47m:38s remains)
INFO - root - 2017-12-15 16:24:03.179453: step 34170, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 53h:35m:34s remains)
INFO - root - 2017-12-15 16:24:09.562025: step 34180, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 53h:31m:15s remains)
INFO - root - 2017-12-15 16:24:15.883269: step 34190, loss = 0.37, batch loss = 0.26 (12.7 examples/sec; 0.629 sec/batch; 52h:09m:42s remains)
INFO - root - 2017-12-15 16:24:22.275294: step 34200, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:55m:37s remains)
2017-12-15 16:24:22.830397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.096045 -2.6435013 -3.2966709 -3.0685129 -2.8562088 -2.8981886 -3.2375684 -3.4974484 -3.4381552 -4.175056 -4.5431552 -5.1889648 -5.8709946 -7.0571113 -8.0477095][-2.4503021 -2.4766345 -2.92619 -3.344532 -3.9212172 -4.0256557 -4.0658727 -4.2156615 -4.2459917 -4.6888332 -4.6333055 -5.2521329 -5.7722435 -6.7043824 -7.2660418][-1.4189548 -1.0032353 -1.3886633 -1.5366306 -1.8784561 -2.3010254 -2.9710617 -3.0957584 -3.1316438 -3.9415917 -3.9578295 -4.591733 -5.0736504 -6.2698865 -6.7667427][-0.27409172 0.17747688 -0.17991495 -0.46197367 -0.92148209 -0.98461819 -1.107007 -1.6555409 -2.1778498 -2.9954109 -3.0350461 -3.7752278 -4.2951636 -5.1769648 -5.8747616][-0.53141165 0.13346195 0.18572855 0.016211987 -0.22886372 0.2835598 0.79203224 0.33778477 -0.12898731 -1.073576 -1.3676023 -2.1591163 -3.0847354 -4.3472271 -5.1127996][-1.4286561 -0.6666317 -0.23854589 0.11176872 0.38475704 1.3522377 2.3206024 2.2505188 2.0697489 0.69441319 0.066487312 -1.0291052 -2.3305783 -3.8668444 -4.9789867][-2.0373478 -1.2923961 -0.97127438 -0.46362495 0.22873831 1.2058258 2.3097267 2.6495056 2.9988365 1.5451555 0.49009132 -0.76219225 -2.0967598 -3.6285272 -4.7709103][-2.8512974 -1.9583035 -1.3830957 -0.69783115 0.20485067 1.397603 2.6548328 3.2702274 3.9481773 2.471755 1.3799267 -0.095883846 -1.6527672 -3.3928885 -4.6060238][-3.6716194 -2.7380137 -2.0453558 -1.4552946 -0.83244038 0.69555569 2.051301 2.8296413 3.62457 2.1546869 1.0026579 -0.53934669 -2.0550694 -3.9690356 -5.1314015][-4.341671 -3.7672954 -3.2329087 -2.3095455 -1.5356288 -0.58134842 0.14715528 0.78017521 1.4666948 0.37824726 -0.57839108 -1.9251385 -3.2223544 -4.7784214 -5.9169922][-5.7220411 -5.2807226 -4.9315825 -4.054101 -3.2220197 -2.2662921 -1.6123633 -1.4181652 -1.201498 -2.2789884 -3.0266209 -3.7709153 -4.5700836 -6.0746074 -7.2535563][-7.3372321 -7.07764 -6.8399935 -6.3184137 -5.7110014 -4.7894864 -4.2094822 -4.1040516 -4.0015197 -4.8014994 -5.2706757 -5.4706554 -5.9966855 -6.9399295 -7.6674705][-7.8002052 -7.4650574 -7.4111671 -6.9885902 -6.480834 -5.8017845 -5.3166142 -5.2650471 -5.236732 -5.4073792 -5.7009344 -5.9574637 -6.2943907 -6.8331242 -7.355639][-7.511117 -7.3398967 -7.4308867 -7.31024 -7.1008129 -6.3435159 -5.7560863 -5.6335192 -5.6277452 -6.0310745 -6.2315817 -6.04143 -6.0006361 -6.3796511 -7.0238557][-7.5004473 -7.2173595 -7.1846581 -7.1435494 -7.0054255 -6.2506194 -5.7189417 -5.6317511 -5.5496922 -5.6389718 -5.7317047 -5.83794 -5.9137363 -5.9284325 -5.8948412]]...]
INFO - root - 2017-12-15 16:24:29.305412: step 34210, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 54h:41m:04s remains)
INFO - root - 2017-12-15 16:24:35.666023: step 34220, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 53h:38m:31s remains)
INFO - root - 2017-12-15 16:24:42.060987: step 34230, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 52h:31m:31s remains)
INFO - root - 2017-12-15 16:24:48.482303: step 34240, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 53h:54m:38s remains)
INFO - root - 2017-12-15 16:24:54.903637: step 34250, loss = 0.33, batch loss = 0.22 (12.2 examples/sec; 0.657 sec/batch; 54h:25m:41s remains)
INFO - root - 2017-12-15 16:25:01.401709: step 34260, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 52h:32m:16s remains)
INFO - root - 2017-12-15 16:25:07.751345: step 34270, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 53h:26m:35s remains)
INFO - root - 2017-12-15 16:25:14.106701: step 34280, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.623 sec/batch; 51h:35m:46s remains)
INFO - root - 2017-12-15 16:25:20.440147: step 34290, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 52h:40m:54s remains)
INFO - root - 2017-12-15 16:25:26.842426: step 34300, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 53h:19m:00s remains)
2017-12-15 16:25:27.342661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3547692 -2.5800028 -2.5205135 -2.8695831 -3.6111307 -3.4747987 -3.1316218 -2.5714321 -1.5754852 -2.8999166 -3.3033361 -4.3182373 -5.6515508 -7.6861725 -8.7774086][-3.6339521 -3.679769 -3.519805 -4.148273 -4.5206938 -4.1306047 -3.8549995 -3.2520313 -2.9574776 -4.4514494 -4.2217979 -5.4502859 -7.592597 -9.8369331 -10.487279][-2.1267619 -2.1805873 -2.4828329 -3.5137796 -4.039938 -4.0288563 -3.4477038 -3.4344859 -3.0086341 -4.7369652 -5.2924542 -5.7369432 -6.8715806 -8.9854765 -10.165978][-1.9573693 -1.8494883 -1.9393988 -1.986742 -1.7561259 -1.6854701 -1.301465 -0.92523909 -0.745986 -2.8566818 -3.525887 -4.1142693 -5.8521881 -7.9332485 -8.8657751][-2.5035086 -2.1194062 -1.3356829 -1.5340314 -1.4972162 -0.48116875 0.75731945 1.1539135 1.1822395 -1.3904219 -2.2623868 -3.8281109 -5.4785252 -7.3029008 -7.9097872][-2.9898725 -2.4079151 -1.9677644 -1.3186111 -0.36656189 0.71600246 1.849247 2.3160295 2.5448484 -0.58737659 -2.7155704 -4.20918 -6.1009197 -7.5978422 -7.9549303][-3.68684 -2.9932094 -1.6746807 -0.47517347 0.41808033 1.5768976 3.0533724 3.688364 3.6566029 0.51319027 -1.5486913 -3.5983543 -5.99688 -7.8039165 -8.3628693][-3.9249313 -2.9809432 -2.5450411 -0.80068636 1.0934839 2.7970505 3.4915152 3.8930368 3.9208002 0.72824383 -1.2974586 -3.361073 -5.3749647 -7.4129691 -8.1752558][-4.2556825 -3.6354303 -2.9207783 -1.4212966 -0.14620638 1.0810966 1.9679031 2.6092176 2.7603407 -0.31547356 -2.0079165 -3.4153514 -5.0985556 -7.2598267 -8.3417244][-4.9246292 -4.1924772 -3.3914747 -1.9051299 -0.65353441 0.23436499 0.81551266 1.1276999 1.3231678 -1.4566116 -2.7542825 -4.2326937 -5.4798183 -7.3026357 -8.0691738][-5.3769007 -5.0797472 -4.85917 -3.8241041 -2.891746 -1.8519773 -0.88558674 -0.44715071 -0.32136202 -3.0725808 -4.2535892 -5.29268 -5.974647 -7.5437679 -7.9371028][-6.2586851 -5.6315541 -5.0172157 -4.4619465 -3.7952037 -2.9504786 -2.7465639 -2.3731709 -1.9690862 -4.0288434 -4.6384859 -5.4435172 -6.1051388 -7.3005738 -7.55578][-7.2886477 -6.5447645 -5.6266394 -5.1064987 -4.4356327 -4.2851925 -3.9313965 -3.8738236 -3.9599562 -5.4810114 -5.7812991 -5.7532263 -6.2237906 -7.0569525 -7.0762782][-6.6092391 -5.7599549 -5.4244742 -5.0992188 -4.8278961 -4.83221 -4.835494 -4.9614477 -4.9702148 -6.0117645 -5.7730503 -5.6243825 -5.5959721 -5.8758168 -5.877491][-8.2243814 -7.689733 -7.1201921 -6.7770538 -6.6921711 -6.4013238 -6.6289783 -6.6984773 -7.0412512 -7.5507832 -7.409265 -7.143435 -6.4895449 -6.0540562 -5.6530952]]...]
INFO - root - 2017-12-15 16:25:33.781416: step 34310, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 53h:47m:42s remains)
INFO - root - 2017-12-15 16:25:40.241300: step 34320, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.658 sec/batch; 54h:32m:29s remains)
INFO - root - 2017-12-15 16:25:46.636636: step 34330, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 52h:09m:52s remains)
INFO - root - 2017-12-15 16:25:53.031580: step 34340, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 52h:11m:37s remains)
INFO - root - 2017-12-15 16:25:59.460888: step 34350, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 54h:37m:16s remains)
INFO - root - 2017-12-15 16:26:05.791604: step 34360, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 52h:08m:42s remains)
INFO - root - 2017-12-15 16:26:12.151720: step 34370, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 53h:05m:00s remains)
INFO - root - 2017-12-15 16:26:18.569203: step 34380, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 53h:07m:14s remains)
INFO - root - 2017-12-15 16:26:25.037958: step 34390, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.656 sec/batch; 54h:19m:39s remains)
INFO - root - 2017-12-15 16:26:31.358619: step 34400, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:39m:32s remains)
2017-12-15 16:26:31.841241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5045438 -2.1265111 -1.6915479 -1.5911946 -1.2820888 -0.97273445 -0.86808586 -0.84584951 -0.72225571 -1.7999368 -2.7530227 -4.0721884 -5.4763079 -6.410202 -7.3391142][-2.7018485 -2.4465914 -2.0574064 -2.0905128 -1.7862692 -1.5832734 -1.3523231 -0.96102047 -0.9750104 -2.0680056 -3.2647667 -4.7620668 -6.1063519 -6.6904244 -7.5166645][-2.1207814 -1.8302684 -1.245049 -0.99016619 -0.4545579 -0.318336 -0.24191809 -0.099813461 -0.24114895 -1.4017315 -3.0307746 -4.5272813 -5.5545092 -6.0461035 -6.6865368][-1.313664 -0.8256259 -0.319829 0.097596645 0.85563087 1.2397509 1.4252386 1.6540842 1.6161575 0.33566189 -1.3425236 -3.2788396 -4.6610589 -5.289382 -6.0879121][-1.9498906 -1.1698699 -0.45253134 -0.022910118 0.627759 1.5093708 2.4058428 2.7524834 2.7131376 1.126688 -0.53552532 -2.522409 -4.2360992 -5.1536407 -6.1123123][-1.9194031 -1.4637938 -1.0067792 -0.0546484 0.94412136 1.8117142 3.0360889 3.5160351 3.6470432 2.0357552 0.21387243 -1.7468748 -3.4392376 -4.4217343 -5.4955359][-2.2989836 -1.5887489 -0.9997468 -0.15128422 1.2126245 2.3293371 3.6584387 4.1830139 4.4404564 2.7875404 0.987731 -1.2232337 -3.1196485 -4.1128855 -5.3011589][-2.3170419 -1.8308272 -1.2066083 -0.10608292 1.2448082 2.4098921 3.7816877 4.3277817 4.5400572 2.9231472 1.2131844 -0.91081524 -2.7101521 -3.7848198 -5.0923443][-1.9432316 -1.3429546 -0.92220736 -0.20500183 0.7575531 1.912302 3.2629862 3.6900187 3.8812904 2.2724 0.78071976 -1.0384088 -2.5801296 -3.6194921 -4.8609118][-2.4410458 -1.96911 -1.2161036 -0.30288219 0.5496397 1.1483727 1.9933758 2.2095022 2.2545757 0.94052124 -0.25432491 -1.8804326 -3.1466942 -3.7631736 -4.6549826][-2.8476353 -2.4801431 -2.2028675 -1.3832641 -0.56890011 0.16620874 0.79790306 0.8580265 0.71795082 -0.51840687 -1.4552417 -2.8037677 -3.8745344 -4.3084731 -4.8229609][-4.7474828 -3.8470585 -3.2465158 -2.555912 -1.939321 -1.312993 -0.90708017 -1.0268188 -1.3253689 -2.4149127 -3.105289 -3.9172175 -4.53257 -4.8693933 -5.3307753][-5.8713222 -5.401269 -4.6781011 -3.6773667 -2.8164883 -2.2452383 -2.1072464 -2.5395584 -2.8997798 -3.770891 -4.4152908 -5.0104551 -5.4854822 -5.551012 -5.9220457][-5.582015 -5.0948915 -4.3751211 -3.571496 -2.9353352 -2.717411 -2.7947793 -3.1447258 -3.3321123 -3.8161259 -4.1989017 -4.6193504 -4.9789238 -5.2380829 -5.65069][-6.1899576 -5.3799734 -4.870677 -4.4404054 -4.2186785 -3.9424288 -3.8860664 -4.0500679 -4.0965223 -4.0602913 -4.2507496 -4.4019561 -4.5808668 -4.7667952 -5.0569324]]...]
INFO - root - 2017-12-15 16:26:38.287798: step 34410, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 54h:24m:37s remains)
INFO - root - 2017-12-15 16:26:44.747766: step 34420, loss = 0.24, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 55h:40m:45s remains)
INFO - root - 2017-12-15 16:26:51.217137: step 34430, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 52h:49m:44s remains)
INFO - root - 2017-12-15 16:26:57.574377: step 34440, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 52h:58m:52s remains)
INFO - root - 2017-12-15 16:27:04.042766: step 34450, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 52h:43m:49s remains)
INFO - root - 2017-12-15 16:27:10.519001: step 34460, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 53h:40m:51s remains)
INFO - root - 2017-12-15 16:27:16.930216: step 34470, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:40m:44s remains)
INFO - root - 2017-12-15 16:27:23.322202: step 34480, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 53h:01m:49s remains)
INFO - root - 2017-12-15 16:27:29.697384: step 34490, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 52h:47m:53s remains)
INFO - root - 2017-12-15 16:27:36.170144: step 34500, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 52h:04m:27s remains)
2017-12-15 16:27:36.754718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.112196 -1.889257 -1.7266297 -2.2982664 -3.0548706 -2.8076563 -2.2437315 -2.1603923 -2.3439426 -2.7005529 -4.5140514 -5.2818418 -6.6449594 -7.4860439 -8.3078613][-2.5840292 -2.5448885 -2.2977738 -2.1644278 -2.7120194 -2.6644249 -2.3788719 -2.2681494 -2.5733743 -2.8914056 -4.5538807 -5.0647764 -6.4570007 -7.2553477 -8.2445974][-1.3900638 -1.5424862 -1.8483105 -1.9507313 -2.2693682 -1.5979881 -1.033875 -0.9841404 -1.4563813 -1.9586816 -3.4959044 -4.1031923 -5.4506979 -5.9102 -7.14468][-1.9779058 -1.5300093 -1.2680235 -1.0511475 -1.0665417 -0.64313269 -0.25231743 -0.36047697 -0.8997488 -1.6213026 -3.1682224 -3.6853466 -4.8091159 -5.6213045 -6.5485435][-2.3602719 -1.837503 -1.4968748 -1.1778679 -1.0218163 -0.26473618 0.5866518 0.65011692 0.6332016 0.063332081 -1.5949931 -2.3931375 -3.7502525 -4.9094968 -6.0799046][-1.7488728 -1.5907207 -1.2224936 -0.55924129 0.144979 0.70602322 1.5502682 1.5947285 1.3883076 0.92351627 -0.64520311 -1.0292716 -2.2118382 -3.3078437 -4.7395105][-3.3709474 -2.7512393 -1.6728587 -0.79071665 0.01441431 1.0380421 2.3217621 2.4986839 2.3368177 1.7098246 -0.23573494 -0.85631514 -2.2211094 -3.1832118 -4.6549172][-3.0761719 -2.7689776 -2.2291918 -1.447021 0.00053596497 1.1277685 2.1800613 2.5290127 2.7850533 2.4026871 0.41278172 -0.11427784 -1.7045212 -3.0879221 -4.4862146][-3.4330516 -2.2107196 -1.0821638 -0.4561882 0.18191767 0.96329212 1.8699598 2.0329762 2.3386822 2.2449484 0.30558729 -0.34853983 -1.9765177 -3.3280478 -4.7598658][-3.3156571 -3.0165672 -2.4088392 -1.4456954 -0.42887688 0.6803894 1.5568914 1.4583883 1.2838678 0.96098709 -0.71799278 -1.0569148 -2.4415798 -3.3719769 -4.4846749][-4.5904212 -3.9435244 -3.5410252 -3.1916037 -2.8223715 -2.1467705 -1.2590652 -1.0591116 -1.1062598 -1.6349354 -2.966156 -2.8192339 -3.5028915 -3.962697 -4.8683472][-6.3533525 -5.8743176 -5.4136534 -5.1779952 -5.0783005 -4.7467637 -4.2059784 -4.1472745 -3.9543462 -3.7775233 -4.7341394 -4.7433887 -5.1760249 -5.0475807 -5.4228973][-6.790741 -6.7093229 -6.4524035 -6.7636323 -6.8643522 -6.7601347 -6.4124017 -6.5514164 -6.2164407 -5.5652728 -5.6568727 -4.8006411 -4.6165619 -5.0368118 -5.3192091][-7.1736517 -7.3271685 -6.8868332 -7.271203 -7.4662948 -7.4589438 -7.0100951 -7.0572224 -7.1259723 -6.8249397 -6.868125 -6.4689822 -5.9184875 -5.38407 -5.0870228][-8.4851875 -8.7239456 -8.091774 -8.1682215 -8.1850634 -8.2083549 -7.9469242 -8.050848 -7.7518458 -7.7794819 -7.6335597 -7.7276688 -7.2257023 -6.85156 -6.2173057]]...]
INFO - root - 2017-12-15 16:27:43.140627: step 34510, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 53h:59m:49s remains)
INFO - root - 2017-12-15 16:27:49.569785: step 34520, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 53h:32m:04s remains)
INFO - root - 2017-12-15 16:27:55.955693: step 34530, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 51h:15m:42s remains)
INFO - root - 2017-12-15 16:28:02.357302: step 34540, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 54h:46m:24s remains)
INFO - root - 2017-12-15 16:28:08.827799: step 34550, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 53h:53m:33s remains)
INFO - root - 2017-12-15 16:28:15.199286: step 34560, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 54h:15m:21s remains)
INFO - root - 2017-12-15 16:28:21.573887: step 34570, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 52h:55m:50s remains)
INFO - root - 2017-12-15 16:28:27.976534: step 34580, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 52h:08m:53s remains)
INFO - root - 2017-12-15 16:28:34.340563: step 34590, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:52m:51s remains)
INFO - root - 2017-12-15 16:28:40.713730: step 34600, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 52h:59m:13s remains)
2017-12-15 16:28:41.212361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3495059 -6.255971 -6.1856918 -5.1523876 -3.3866329 -2.0919352 -1.8163009 -1.1160841 -0.59057379 -1.9734464 -2.921926 -3.9528227 -5.4392605 -6.190176 -6.7850533][-6.1099906 -5.9853516 -6.8248677 -6.2909307 -5.1307468 -3.17036 -1.5384021 -1.4845662 -1.6395869 -2.9732518 -3.6549239 -5.0039558 -6.4828033 -6.72052 -7.0425549][-5.8164115 -5.6485848 -5.9622173 -5.8524208 -5.5178909 -4.1832671 -2.3531623 -1.6988301 -1.4185457 -3.2910562 -4.2566376 -5.4719172 -7.059772 -7.6691642 -8.0085106][-5.3518777 -5.70245 -5.7876892 -5.3025875 -4.2247629 -3.3859568 -2.0948577 -1.4548182 -1.1075692 -2.9605799 -4.0895166 -5.7710562 -6.9532251 -7.4469342 -8.2734032][-6.2246675 -6.3041821 -6.1275339 -5.3040919 -3.3434768 -2.1107764 -1.1343961 -0.98676538 -0.9333868 -2.6588202 -3.2491622 -5.3521404 -6.8767138 -7.8504038 -8.4215708][-7.6742311 -7.290525 -6.6777477 -5.1030064 -2.7867517 -0.87628317 0.78508472 0.89251328 0.990222 -1.391089 -2.8924932 -4.290473 -6.6243052 -8.28859 -8.76324][-7.6463056 -6.5016775 -5.7697611 -3.9588547 -1.9015059 0.20239258 2.0032816 2.6691694 3.0937004 0.82074928 -0.89630461 -3.3770938 -5.5331459 -7.4283247 -8.708107][-6.7866354 -6.4873452 -5.433867 -2.4144797 0.098394871 2.1640072 3.5898 4.4397173 5.2902384 3.0473804 0.97944641 -1.8556628 -4.3787928 -6.2882638 -7.4977369][-5.7728195 -5.0371342 -5.0922108 -3.4670773 -1.0714688 1.8193235 3.3549423 3.6690273 4.3996792 3.5370426 2.5252857 0.12868929 -2.797009 -4.9077096 -6.6419992][-6.1091471 -5.3581123 -4.6756968 -3.7444029 -2.8045502 -1.0338254 1.2355928 2.9888144 2.6547594 0.6989336 0.72793865 -0.615293 -2.9654932 -4.6355076 -5.8135595][-5.9399214 -5.606823 -5.0909853 -4.3966331 -3.5283647 -2.4886026 -0.9692359 0.24389076 0.75206947 -1.4823432 -2.7957473 -3.4105062 -4.6589565 -5.1104937 -5.4891157][-4.9294763 -4.7855835 -4.477313 -4.0446963 -2.9933076 -1.5519309 -0.11374855 0.38873768 0.12603521 -2.6758313 -3.7689428 -5.4261761 -6.351438 -6.5176544 -5.7288814][-5.3575368 -4.4051991 -3.8837788 -3.4226942 -2.8352294 -2.2853441 -0.59515476 0.52769661 0.40910244 -2.7489347 -4.064846 -5.0000439 -5.4782314 -6.6928921 -6.7330213][-6.6623573 -5.3354249 -4.6364508 -3.3528247 -2.6549563 -2.5183468 -1.7275496 -1.4953642 -0.91813278 -2.1574512 -3.67417 -4.9683762 -5.5167618 -5.7709413 -5.7833][-6.7585793 -6.0095611 -5.8175883 -4.5080976 -3.5865097 -2.972753 -2.8140879 -2.7049837 -2.7323165 -4.0775938 -4.307889 -4.5315619 -4.8809042 -6.0862646 -6.7063608]]...]
INFO - root - 2017-12-15 16:28:47.591765: step 34610, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 52h:46m:17s remains)
INFO - root - 2017-12-15 16:28:54.032481: step 34620, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.644 sec/batch; 53h:15m:25s remains)
INFO - root - 2017-12-15 16:29:00.401640: step 34630, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 52h:21m:47s remains)
INFO - root - 2017-12-15 16:29:06.917177: step 34640, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.649 sec/batch; 53h:39m:49s remains)
INFO - root - 2017-12-15 16:29:13.288740: step 34650, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:27m:06s remains)
INFO - root - 2017-12-15 16:29:19.584622: step 34660, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 51h:28m:31s remains)
INFO - root - 2017-12-15 16:29:26.100959: step 34670, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 54h:24m:23s remains)
INFO - root - 2017-12-15 16:29:32.472702: step 34680, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 52h:46m:13s remains)
INFO - root - 2017-12-15 16:29:38.841045: step 34690, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 53h:07m:31s remains)
INFO - root - 2017-12-15 16:29:45.183169: step 34700, loss = 0.29, batch loss = 0.18 (13.1 examples/sec; 0.610 sec/batch; 50h:27m:52s remains)
2017-12-15 16:29:45.677848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1143613 -5.7284155 -4.7193108 -4.5685277 -4.375278 -3.4719443 -2.3709455 -3.0486465 -3.1911416 -4.9557219 -4.6558814 -5.1507988 -5.1832552 -5.5877314 -6.2225785][-6.5669909 -6.283308 -5.6684623 -4.2981977 -3.6373482 -3.5549526 -2.9135838 -2.1339464 -2.0213361 -4.5902109 -5.27512 -5.9878106 -6.2846584 -5.8856912 -5.7928562][-7.669558 -6.854075 -5.9661059 -4.651238 -3.3364258 -2.810081 -2.6071339 -2.4615912 -2.5162263 -4.5304227 -5.3913021 -6.3183222 -6.6065407 -6.6289988 -6.3415051][-7.0735383 -5.9667583 -5.354537 -4.1581249 -2.6947317 -1.9800215 -1.5916405 -1.200892 -1.3190632 -3.7636573 -4.6614876 -5.29677 -5.8098278 -6.53094 -6.5292616][-7.04963 -5.0779514 -2.9654999 -1.9470301 -1.8860931 -1.1897659 -0.72091722 -0.66859674 -1.3851032 -3.4671926 -4.2410884 -4.83183 -5.2046909 -5.9999208 -6.5442686][-3.2801089 -2.8445559 -2.9075723 -1.2323952 0.29410315 0.63421631 0.56612778 0.43379021 0.27564287 -2.1703691 -3.5554862 -3.8238437 -4.1845083 -4.8992 -5.5858278][-2.7592444 -0.84856033 1.268878 1.783083 0.890234 1.4056358 1.8499546 1.0335646 0.39154148 -1.7101994 -2.4334865 -3.2435188 -3.876514 -4.2863626 -4.539309][-0.83274126 -1.342011 -1.1360984 0.858778 2.787673 2.9249821 2.8962059 1.7338896 1.0485563 -1.5005136 -3.1861668 -3.843869 -3.821182 -4.0617886 -4.832356][-1.0753975 -0.51264524 0.34998703 1.1399212 1.382021 2.0448914 2.3058453 1.8665485 1.3536682 -1.4785943 -2.5544572 -3.7899992 -5.0079951 -4.9289589 -4.8962812][-1.8691602 -1.7057261 -1.8450809 -0.80613852 0.54903412 1.0275049 0.73869038 0.26157379 -0.22216892 -2.27429 -4.0748091 -4.5949516 -4.8964629 -5.6376634 -5.8898306][-5.4791527 -4.6294336 -3.943311 -3.3496776 -2.8958936 -2.3451791 -2.2941179 -2.5008154 -3.0652657 -4.3751168 -5.4455462 -6.0130272 -6.6734247 -6.6455326 -6.7232757][-6.1702118 -5.9102135 -5.4533887 -4.8138866 -3.8223536 -2.8617921 -2.8289514 -3.68545 -4.5923729 -5.4201975 -6.4328756 -6.6458406 -7.2237 -7.4263539 -7.569551][-4.859849 -5.3405752 -5.4284992 -5.3367739 -5.1067963 -4.8627844 -4.1662884 -4.2899909 -4.9634805 -5.803617 -7.2958212 -7.5017319 -7.621614 -7.6895633 -7.6559467][-5.110292 -4.6927748 -4.8561859 -4.7655196 -3.9757965 -3.2863178 -3.2239738 -3.2112918 -3.4991164 -3.7296598 -5.18522 -6.0456591 -6.9059196 -7.2059345 -7.331676][-4.9825487 -4.8398876 -4.3046179 -4.115799 -4.2670717 -4.0156169 -3.4296174 -3.7956772 -4.1315227 -4.5771885 -5.3439326 -5.8768053 -7.007966 -7.3077931 -6.9516282]]...]
INFO - root - 2017-12-15 16:29:52.033931: step 34710, loss = 0.42, batch loss = 0.31 (12.4 examples/sec; 0.647 sec/batch; 53h:31m:52s remains)
INFO - root - 2017-12-15 16:29:58.465496: step 34720, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 53h:54m:48s remains)
INFO - root - 2017-12-15 16:30:04.874689: step 34730, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 52h:47m:29s remains)
INFO - root - 2017-12-15 16:30:11.227310: step 34740, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 53h:22m:09s remains)
INFO - root - 2017-12-15 16:30:17.577278: step 34750, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 52h:00m:04s remains)
INFO - root - 2017-12-15 16:30:24.054997: step 34760, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 53h:32m:45s remains)
INFO - root - 2017-12-15 16:30:30.471560: step 34770, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 52h:58m:04s remains)
INFO - root - 2017-12-15 16:30:36.886169: step 34780, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 54h:59m:06s remains)
INFO - root - 2017-12-15 16:30:43.280154: step 34790, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 53h:12m:03s remains)
INFO - root - 2017-12-15 16:30:49.696809: step 34800, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 51h:41m:21s remains)
2017-12-15 16:30:50.225595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6805496 -4.514327 -4.5223546 -5.1509571 -5.6061177 -5.53822 -4.2742586 -3.6609836 -3.0768738 -4.0991917 -4.9653668 -4.9934711 -5.4218445 -6.0922375 -6.139945][-4.6566067 -4.410964 -4.64813 -4.7631235 -4.2580967 -4.2377858 -4.2791595 -3.9472535 -3.1958175 -4.0711737 -4.93121 -5.4321613 -5.6713877 -5.7281232 -6.2478743][-5.1044569 -5.1636462 -4.2621384 -3.8002329 -3.7145424 -3.2836843 -2.6276717 -2.4581327 -2.7160163 -3.7323394 -4.2822313 -5.23394 -5.7890425 -5.98998 -6.2809181][-4.9854107 -4.3523192 -4.2710495 -3.1476583 -2.1839151 -1.3764391 -1.0762391 -0.78839159 -0.752398 -2.267962 -3.2690206 -4.5367002 -5.299943 -5.8062558 -6.3623939][-4.2655115 -3.8790562 -2.7628779 -1.4758515 -0.25867081 0.31002045 0.74965763 1.3154726 1.1230917 -0.93371296 -2.062058 -3.3421516 -4.4066935 -5.3203154 -6.187757][-3.2622547 -2.5153861 -1.3291345 0.23241901 1.091116 1.6322899 2.5330992 2.4883413 2.026515 0.11724043 -1.0823269 -2.6804852 -4.0006609 -4.8883572 -5.788271][-2.3146658 -1.7753644 -0.51650286 0.96300983 1.9220657 2.868947 3.4758883 2.9781141 2.644536 0.60504341 -1.1148214 -2.5493026 -3.6977282 -4.5375986 -5.5392175][-1.5981388 -0.83028507 -0.20563459 1.4457693 2.8761578 3.4900255 3.6894331 3.1791925 2.7752352 0.445652 -0.94269657 -2.5957079 -3.8326197 -4.4295816 -4.9814653][-1.4734931 -1.1443124 -0.62923527 0.44031715 2.291007 3.3599958 3.169961 2.9762487 2.9526138 0.65713024 -1.0893345 -2.8518605 -3.7122753 -4.4144726 -5.0588694][-2.7029705 -1.9262557 -1.7359395 -1.0050244 0.1463871 1.3202667 1.7371511 2.1680737 1.7988644 -0.33592176 -1.5233312 -2.9434791 -4.1566515 -4.7165995 -5.3437653][-4.6020122 -3.6609821 -3.0952926 -2.7164445 -2.4501805 -1.9682832 -1.0750651 -0.32304382 -0.16970778 -1.5493064 -3.6332564 -4.8536577 -4.6726789 -5.3021727 -6.3327551][-7.2272592 -6.2862811 -5.3250484 -4.7818642 -4.6184025 -4.624156 -4.4818821 -4.179359 -3.3566432 -3.9602835 -5.1399069 -5.8297024 -6.4561276 -6.3784261 -6.7538595][-8.2030993 -7.8524547 -7.6670971 -6.99893 -6.5410776 -6.5382824 -6.187573 -6.2916336 -6.2962484 -6.0092916 -6.4393005 -6.6954694 -7.2501307 -7.0349231 -6.9952917][-7.7032394 -7.782578 -8.0727911 -7.8446803 -7.269156 -7.1048994 -7.1380992 -6.844286 -6.4836721 -6.6692104 -6.8593383 -6.3927608 -6.6215162 -6.7808437 -6.9571819][-7.7242007 -7.5640669 -6.6712384 -7.2257004 -7.9904513 -7.6608267 -6.797956 -6.7888961 -7.3905063 -7.0833483 -6.43511 -6.7673006 -6.7456493 -6.9464121 -7.0311532]]...]
INFO - root - 2017-12-15 16:30:56.593469: step 34810, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 52h:19m:32s remains)
INFO - root - 2017-12-15 16:31:03.034653: step 34820, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 54h:14m:11s remains)
INFO - root - 2017-12-15 16:31:09.464309: step 34830, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 53h:44m:36s remains)
INFO - root - 2017-12-15 16:31:15.988910: step 34840, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 53h:26m:28s remains)
INFO - root - 2017-12-15 16:31:22.357486: step 34850, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 53h:26m:25s remains)
INFO - root - 2017-12-15 16:31:28.799203: step 34860, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 53h:14m:10s remains)
INFO - root - 2017-12-15 16:31:35.271000: step 34870, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 54h:22m:16s remains)
INFO - root - 2017-12-15 16:31:41.655011: step 34880, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.620 sec/batch; 51h:17m:22s remains)
INFO - root - 2017-12-15 16:31:48.001089: step 34890, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:36m:00s remains)
INFO - root - 2017-12-15 16:31:54.382115: step 34900, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 52h:47m:11s remains)
2017-12-15 16:31:54.889421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4435449 -3.2834272 -3.210144 -3.1998711 -3.000226 -2.7924962 -2.4484148 -1.7477255 -1.1994286 -2.4565673 -3.7283292 -5.1754932 -6.10501 -7.3266473 -8.3372107][-3.5682902 -3.7408824 -4.0004873 -4.176445 -4.03807 -3.5475159 -3.035502 -2.4153957 -1.736105 -2.9295712 -4.0644789 -5.4922323 -6.6606517 -7.6877832 -8.61954][-2.5761113 -2.6296406 -3.04176 -3.3144207 -3.258709 -2.999712 -2.4521055 -1.8091316 -1.3949451 -2.8789129 -4.2901306 -5.6244845 -6.6003032 -7.9212675 -8.892067][-2.3047442 -2.1700497 -2.1232896 -1.9242268 -1.7417541 -1.2876854 -0.82545519 -0.68977451 -0.66275883 -2.6118708 -4.2952285 -6.0119553 -7.1069188 -7.927855 -8.5858278][-2.7879462 -2.3821759 -1.7564011 -1.2329588 -0.736619 -0.10441732 0.3317728 0.21593571 -0.034419537 -2.1582608 -3.9832056 -5.765358 -6.9165425 -7.898705 -8.5623016][-2.8679805 -2.369185 -1.8371835 -0.96002626 -0.13408566 0.89797211 1.4875889 1.2647066 0.93568993 -1.5592794 -3.6955309 -5.4704895 -6.6525707 -7.7223186 -8.5660934][-3.0739536 -2.4722586 -1.5529947 -0.30128622 0.92585659 2.1701136 2.6741457 2.5255108 2.0776844 -0.69077444 -3.2372732 -5.3629436 -6.4630146 -7.6197505 -8.6131039][-3.0483503 -2.3091388 -1.4373269 -0.074821472 1.4442177 2.7332315 3.0975246 3.0359268 2.6394367 -0.091062546 -2.6275811 -5.0340014 -6.4283547 -7.5582252 -8.2925081][-3.3574767 -2.48417 -1.7522373 -0.65332413 0.79938221 1.8717098 2.214591 2.2310343 2.0990953 -0.378747 -2.8384523 -4.9119883 -6.2232122 -7.5300245 -8.3489695][-3.7118797 -3.0714736 -2.2529435 -1.1910119 0.092955589 1.0960903 1.3351707 1.435627 1.4265261 -1.14432 -3.1612473 -5.0554047 -6.1921682 -7.4284482 -8.3757515][-5.2040262 -4.627018 -3.8888769 -2.9186964 -1.8534756 -0.77198935 -0.39920092 -0.6943202 -0.93186235 -3.0454783 -4.817585 -6.0534945 -6.6712928 -7.6381459 -8.1951017][-6.2399583 -5.8529959 -5.0139408 -4.2962637 -3.4943438 -2.5735769 -2.4602332 -2.7758503 -2.9681168 -4.5601091 -5.6343651 -6.3930287 -6.7780967 -7.5097642 -7.8640213][-6.994061 -6.4759083 -5.8322263 -5.1425457 -4.5108237 -3.9903686 -3.7404275 -3.9967525 -4.4508257 -5.4761286 -6.275383 -6.5458403 -6.6389961 -6.9207797 -6.9798155][-7.0443745 -6.6467676 -5.9572992 -5.38381 -5.049448 -4.69084 -4.7670355 -5.0055504 -5.1196432 -5.7646914 -6.0384116 -6.2104712 -6.2502713 -6.443294 -6.290432][-8.0756245 -7.6691675 -7.1665039 -6.8683109 -6.7252707 -6.4580545 -6.5953417 -6.9123025 -7.1601391 -6.9371386 -6.5385666 -6.2371087 -6.1084771 -6.1096325 -5.9396386]]...]
INFO - root - 2017-12-15 16:32:01.308262: step 34910, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 52h:14m:31s remains)
INFO - root - 2017-12-15 16:32:07.694278: step 34920, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 52h:16m:10s remains)
INFO - root - 2017-12-15 16:32:14.102546: step 34930, loss = 0.34, batch loss = 0.23 (12.6 examples/sec; 0.634 sec/batch; 52h:22m:49s remains)
INFO - root - 2017-12-15 16:32:20.544172: step 34940, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 53h:01m:13s remains)
INFO - root - 2017-12-15 16:32:27.056719: step 34950, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 55h:07m:45s remains)
INFO - root - 2017-12-15 16:32:33.430061: step 34960, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 52h:35m:57s remains)
INFO - root - 2017-12-15 16:32:39.957417: step 34970, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.672 sec/batch; 55h:32m:05s remains)
INFO - root - 2017-12-15 16:32:46.350400: step 34980, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 51h:56m:10s remains)
INFO - root - 2017-12-15 16:32:52.682736: step 34990, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 54h:09m:25s remains)
INFO - root - 2017-12-15 16:32:59.161432: step 35000, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 54h:05m:52s remains)
2017-12-15 16:32:59.725437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1162462 -4.5354404 -4.8580847 -4.7248149 -4.2456055 -3.6954539 -3.2126737 -3.1157231 -3.2234306 -5.1699729 -6.4521155 -7.234098 -7.59389 -7.7248816 -7.6903143][-3.1895881 -3.8167229 -4.5279026 -4.5707951 -4.3108778 -3.2183595 -2.5370069 -2.5193095 -3.136539 -5.7098293 -6.9846091 -8.1900063 -9.0838652 -9.7587872 -9.3448944][-3.166769 -3.5417514 -3.9909308 -3.7981882 -3.3360863 -2.7434649 -1.8750863 -1.3956385 -1.4542565 -4.2368345 -5.9868064 -7.4046826 -8.8418217 -10.088214 -10.277985][-3.2758203 -3.4382939 -3.6810813 -3.257266 -2.1810713 -1.1912389 -0.43359375 -0.31469917 -0.31016111 -2.5093703 -3.7041504 -5.7425575 -7.4403639 -8.964716 -9.4749184][-3.0078979 -2.5821095 -2.2554326 -1.6365252 -0.42735147 0.69120884 1.1061907 0.70890713 0.53070831 -1.6645975 -3.0225821 -4.3190322 -5.8260274 -7.3117332 -8.2427711][-3.5444069 -2.1430616 -1.0553656 -0.012153625 0.8071804 1.7001829 2.4392395 2.3389482 1.875843 -0.61984253 -2.0495076 -3.5011134 -4.6191053 -5.5281734 -6.4595137][-2.8984256 -1.5876613 -0.52106667 0.84482384 2.3938198 3.3621607 3.5327902 3.4103336 3.2960243 0.65458107 -1.1948395 -3.0167484 -4.4064708 -5.3713512 -5.8953743][-2.4255438 -1.2909904 -0.42218924 1.3375301 2.927947 4.1708603 4.8635464 4.2069778 3.8022594 1.5247116 -0.33704281 -2.4971318 -4.042366 -5.2010975 -5.7658968][-3.0039062 -1.9743757 -1.1257181 0.45851898 2.131712 3.390564 4.0853367 4.1725445 3.6115217 0.85357666 -0.67025518 -2.2622771 -3.878371 -5.0730228 -5.5813775][-4.2275867 -3.3164816 -2.8013592 -1.4690804 -0.067648888 1.0754013 2.4166517 3.0677691 2.978611 0.14299154 -1.7522039 -3.1659656 -4.3005929 -5.3099256 -5.6490541][-5.1388988 -4.5598793 -3.8415477 -2.558877 -1.6029034 -0.891695 0.27053738 0.84560585 0.946867 -1.1220279 -2.4277029 -4.0073767 -5.0484586 -5.8618383 -5.9075704][-5.616972 -5.1197443 -4.6285534 -3.6919477 -2.9588504 -2.15234 -1.3853679 -1.0295806 -1.0763946 -2.5140076 -3.5215683 -4.684885 -5.3432817 -6.0891924 -6.2724781][-6.2994633 -5.6872516 -5.1165967 -4.5826483 -4.1449609 -3.5035734 -2.9037266 -2.6703172 -2.5362072 -3.5706925 -4.0449438 -4.519845 -5.1470265 -5.6860533 -6.1277995][-7.0503206 -6.2300053 -5.3973637 -4.6234579 -4.08768 -3.7926161 -3.8703563 -3.9838722 -4.047616 -4.1821938 -4.4218378 -4.549541 -4.43793 -4.6788521 -5.0126543][-6.9758205 -6.88377 -6.3552728 -5.3979645 -4.5995326 -4.2932329 -4.325408 -4.5337715 -4.9190531 -4.7776937 -4.9843721 -4.8884239 -4.6645079 -4.6310463 -4.7174835]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:33:07.024776: step 35010, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 54h:08m:45s remains)
INFO - root - 2017-12-15 16:33:13.414079: step 35020, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.627 sec/batch; 51h:50m:32s remains)
INFO - root - 2017-12-15 16:33:19.892706: step 35030, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 53h:51m:07s remains)
INFO - root - 2017-12-15 16:33:26.271830: step 35040, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 52h:24m:17s remains)
INFO - root - 2017-12-15 16:33:32.653024: step 35050, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 52h:16m:16s remains)
INFO - root - 2017-12-15 16:33:39.084936: step 35060, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:23m:34s remains)
INFO - root - 2017-12-15 16:33:45.420870: step 35070, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:21m:51s remains)
INFO - root - 2017-12-15 16:33:51.739043: step 35080, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 50h:59m:54s remains)
INFO - root - 2017-12-15 16:33:58.083310: step 35090, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 51h:51m:12s remains)
INFO - root - 2017-12-15 16:34:04.421456: step 35100, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 51h:17m:49s remains)
2017-12-15 16:34:05.004369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3981895 -3.1732111 -3.6175876 -3.8927994 -4.1508255 -4.2572336 -3.846 -3.853888 -4.252254 -6.2023525 -7.395874 -7.9163 -8.495863 -9.1450691 -9.1584568][-3.8225186 -3.6490912 -3.6022558 -3.5415826 -3.6290832 -3.5160809 -3.4070868 -3.378283 -3.7207632 -6.0943518 -7.72637 -8.3693781 -9.2043295 -9.1686058 -9.2208052][-3.7319179 -3.1800919 -2.7857385 -2.1670241 -1.8625093 -1.3996902 -0.95942259 -1.3279366 -2.1993046 -4.8229809 -6.3005352 -7.1188869 -7.9707003 -8.865942 -8.9673767][-3.9160943 -3.0804353 -2.1341062 -1.4793162 -0.87862015 -0.49490547 0.18419075 0.47042751 0.011658192 -2.7523379 -4.1506224 -5.4955053 -6.7733388 -7.4469404 -8.027607][-3.9186292 -2.6460156 -1.40766 -0.24181414 0.47903061 1.3297215 2.0250511 1.82096 1.4254656 -1.0040851 -2.7770448 -3.8771839 -5.2901368 -6.4569855 -7.0158153][-3.1496134 -2.4274197 -1.6769824 -0.57267475 0.84335232 1.8469639 2.3431091 2.3261471 2.0116453 -0.58974075 -1.6696777 -3.0148835 -5.1083112 -6.4371343 -7.4598732][-3.0239964 -1.8153887 -0.69874 -0.064361572 1.0287199 1.9421635 2.3038397 2.2670403 2.3099737 -0.022514343 -1.1994219 -2.5202851 -4.3573446 -5.8534989 -6.8284159][-2.4915524 -1.8985715 -0.81170988 -0.053759575 0.8097477 1.2093515 1.8415527 2.486927 2.7438374 0.23310041 -0.88464451 -2.5396919 -4.6853352 -6.1188593 -7.0401793][-3.312335 -2.6359348 -1.8509188 -0.732903 0.33507252 1.0815248 1.2014055 1.3752079 1.7624874 -0.55268431 -1.8302832 -3.2685261 -5.2215843 -6.5774355 -7.5217061][-2.7457323 -2.396297 -1.9242969 -0.885993 -0.44413328 -0.1311388 0.17281485 0.21561909 0.40742493 -2.0422668 -2.9030027 -4.1243649 -5.8019662 -7.0823393 -7.874866][-4.3853455 -3.5538974 -2.8329835 -2.3176241 -2.142827 -1.9763937 -1.8999982 -2.1341734 -1.9519181 -3.97054 -4.9054914 -5.3381195 -6.1318893 -7.2157969 -7.7267432][-6.1388311 -5.690083 -4.6728992 -3.9155874 -3.5626884 -3.1747169 -3.2567072 -3.3702888 -3.2191973 -4.9382963 -5.6825962 -5.5108871 -6.2204285 -6.9314861 -7.01474][-6.3678684 -5.8591666 -5.2042923 -4.5453892 -4.065094 -3.8420465 -4.0450068 -4.2723627 -4.3519344 -5.5039721 -6.4023151 -6.2160907 -6.2533956 -6.2664118 -6.2248254][-5.9158545 -5.1842628 -4.8087921 -4.3735371 -3.9969025 -4.1387987 -4.5932183 -4.6618319 -4.2566051 -4.9676542 -5.439867 -5.6763439 -6.0577841 -6.2022681 -5.9402504][-7.0188513 -6.5779924 -5.970943 -5.6173434 -5.5131726 -5.1635618 -5.39596 -5.5422735 -5.714891 -5.5632429 -5.4563622 -5.4003992 -5.441412 -5.6710162 -5.9128337]]...]
INFO - root - 2017-12-15 16:34:11.322998: step 35110, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.643 sec/batch; 53h:04m:49s remains)
INFO - root - 2017-12-15 16:34:17.761845: step 35120, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 53h:00m:39s remains)
INFO - root - 2017-12-15 16:34:24.170565: step 35130, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 52h:40m:43s remains)
INFO - root - 2017-12-15 16:34:30.538582: step 35140, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 52h:56m:53s remains)
INFO - root - 2017-12-15 16:34:36.933400: step 35150, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 52h:29m:40s remains)
INFO - root - 2017-12-15 16:34:43.359149: step 35160, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 52h:04m:06s remains)
INFO - root - 2017-12-15 16:34:49.799133: step 35170, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 52h:32m:43s remains)
INFO - root - 2017-12-15 16:34:56.219610: step 35180, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 53h:42m:45s remains)
INFO - root - 2017-12-15 16:35:02.654500: step 35190, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 53h:34m:20s remains)
INFO - root - 2017-12-15 16:35:09.073523: step 35200, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.675 sec/batch; 55h:44m:51s remains)
2017-12-15 16:35:09.588223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5244203 -6.6717157 -5.6236753 -5.1228995 -4.2476754 -3.5509529 -2.7377915 -2.2135987 -1.8690176 -2.5217071 -5.0714445 -5.93836 -6.8939424 -8.2554331 -8.0176907][-4.6999693 -4.9297552 -4.9381185 -4.1349888 -3.7690542 -3.4281573 -2.6789179 -2.045908 -2.0370851 -2.6859608 -4.894495 -5.7812777 -6.95835 -7.6523838 -7.6981187][-3.4514422 -4.0383635 -4.0483451 -2.9621472 -2.3781271 -2.10858 -1.6695223 -1.4792161 -1.6491027 -1.4854493 -3.5922461 -4.2174892 -5.71713 -6.7416577 -6.40597][-2.3047762 -2.3487334 -2.4986382 -1.9309754 -0.8678546 -0.77821112 -0.046886444 0.28305769 0.0084524155 -0.64551449 -2.84826 -3.4477129 -4.769897 -5.9170275 -6.11817][-2.1197357 -1.5205417 -0.82467508 -0.62607718 -0.065100193 0.62919712 1.0574703 1.0623112 0.84671974 0.011062622 -1.9297867 -2.4970284 -3.9746692 -5.1521463 -5.2676][-2.5267634 -1.6098585 -0.87909842 -0.45116949 0.29585838 1.0346613 1.4098473 1.2745914 1.0417261 0.28785229 -1.5764346 -1.8893356 -3.0476141 -4.4295211 -4.7914872][-3.4797649 -2.4532743 -1.1362219 -0.11792183 0.87412167 1.2488976 1.5873699 1.7487125 1.3812666 0.63311386 -1.1797538 -1.5371542 -2.7531581 -4.5749865 -5.0029888][-4.2501211 -2.8955059 -1.6307821 -0.34437943 0.46152687 0.96191692 1.54111 1.7676392 1.5029926 1.0563974 -0.79953575 -1.5585208 -3.0994339 -4.6665692 -4.958847][-4.7241306 -3.7757893 -2.3852406 -1.2002954 -0.015621185 0.38599396 0.98055172 1.0427866 1.513958 0.8628273 -1.3288279 -2.1533866 -3.5097237 -4.6646643 -5.0720425][-4.6106315 -4.1037846 -2.9093723 -1.9532881 -1.3205957 -0.88730288 -0.77610493 -0.56565142 -0.16720724 -0.74950552 -2.5465655 -3.5744019 -4.4938335 -5.2363014 -5.0651569][-6.3443642 -5.68223 -5.4064856 -4.3499041 -3.28443 -2.9901786 -2.9724107 -2.5980425 -2.5559278 -3.1279874 -4.0644007 -4.65291 -5.079999 -5.9716606 -6.0322347][-6.5239825 -5.9272251 -5.0209155 -4.3386641 -3.4345274 -2.9879913 -2.7577267 -2.7861733 -3.1992354 -3.6415744 -4.3341269 -4.5657816 -5.1762209 -6.179378 -6.8001642][-6.9876733 -6.539021 -5.9020252 -5.0873818 -3.8755322 -2.6752853 -2.3800058 -2.5281472 -2.9123678 -3.4799328 -4.6536541 -4.2521214 -4.04245 -5.2873821 -6.1962528][-7.1960449 -6.4261866 -5.7525768 -5.3233109 -3.963635 -2.7977772 -2.0129404 -1.8176217 -1.8656774 -2.0129004 -3.0571871 -3.1683192 -3.695379 -4.4829216 -4.6564617][-6.8158417 -6.6630092 -5.8648629 -5.2135963 -4.971796 -3.660068 -2.8893466 -2.9466224 -2.4044862 -2.1158366 -2.1166639 -2.8581843 -3.8503785 -4.6805825 -5.1044149]]...]
INFO - root - 2017-12-15 16:35:16.053848: step 35210, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 51h:53m:56s remains)
INFO - root - 2017-12-15 16:35:22.456779: step 35220, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 53h:18m:20s remains)
INFO - root - 2017-12-15 16:35:28.858606: step 35230, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 53h:03m:53s remains)
INFO - root - 2017-12-15 16:35:35.215207: step 35240, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 52h:02m:21s remains)
INFO - root - 2017-12-15 16:35:41.655042: step 35250, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:15m:44s remains)
INFO - root - 2017-12-15 16:35:47.967080: step 35260, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 51h:10m:51s remains)
INFO - root - 2017-12-15 16:35:54.335884: step 35270, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.619 sec/batch; 51h:07m:16s remains)
INFO - root - 2017-12-15 16:36:00.764921: step 35280, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 52h:55m:05s remains)
INFO - root - 2017-12-15 16:36:07.148713: step 35290, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 53h:38m:35s remains)
INFO - root - 2017-12-15 16:36:13.679359: step 35300, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 53h:28m:58s remains)
2017-12-15 16:36:14.200186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8377247 -1.9048915 -2.1548753 -2.5716906 -2.6800976 -2.5988765 -2.5907588 -2.4530778 -2.3587809 -3.2299833 -3.6639366 -4.1670332 -4.8004866 -5.5926361 -6.8202744][-2.5880427 -2.9308882 -3.3800011 -3.6658483 -3.6549511 -3.5489564 -3.1490974 -2.5933847 -2.3206635 -3.402658 -4.146441 -5.0082936 -6.0645728 -6.7790504 -7.619503][-2.8379226 -2.5260625 -2.4229269 -2.5332365 -2.5852556 -2.5669384 -2.235786 -2.0034833 -1.8771639 -2.5917411 -3.0582423 -3.9068394 -4.8922911 -5.8238206 -7.0320563][-1.4190321 -1.2082324 -0.88578796 -0.81667376 -0.75254917 -0.48420191 -0.10369158 0.028335094 0.0065808296 -1.0594049 -2.0263152 -3.2411275 -4.3968697 -5.2023482 -6.3572731][-1.176403 -0.53466797 0.021648407 0.033298969 -0.095231056 0.322196 0.62994194 0.69080734 0.81504345 -0.27371645 -1.1018934 -2.1569171 -3.548789 -4.5663929 -6.0604897][-1.3618927 -0.78077412 -0.34539604 0.035224915 0.29778671 0.80054855 1.2297153 1.2763596 1.2791052 0.085540295 -0.86353493 -1.9544659 -3.2555652 -4.2285557 -5.535357][-2.1443253 -1.3091855 -0.70790434 -0.069462776 0.71118641 1.0081453 1.3832998 1.6460638 1.9279327 0.66649532 -0.39634562 -1.4984102 -3.0055876 -4.1457047 -5.5758324][-2.3686042 -1.3770585 -0.8492713 -0.11317873 0.85455322 1.656126 2.2178802 2.3270073 2.4809036 1.1396589 0.17653942 -1.1616817 -2.7943239 -4.0145411 -5.5219545][-2.1719542 -1.5925145 -0.80103111 -0.066492081 0.58463573 1.3967438 2.0599365 2.3948584 2.5703 1.1324949 -0.073946 -1.3891277 -2.7998576 -4.1202869 -5.7088404][-2.6042342 -2.1962242 -1.5653477 -0.90857792 -0.38369226 0.26321793 1.0324144 1.2303228 1.4173222 0.39332867 -0.72977448 -1.9750867 -3.4967465 -4.5707655 -5.87338][-3.6979892 -3.1694736 -2.7263913 -2.1656709 -1.5118008 -0.63930273 0.2056551 0.18742228 0.19892979 -1.0439181 -1.8338532 -2.8323703 -3.9272079 -5.0698681 -6.06785][-5.3019338 -4.653193 -4.1724539 -3.7940679 -3.266675 -2.7086182 -2.1752276 -1.9150434 -1.7209516 -2.5782757 -3.0500331 -3.9694204 -4.8638916 -5.2025709 -5.8931255][-6.1127586 -5.6783304 -5.255599 -4.9750032 -4.5306535 -4.1191072 -3.7458253 -3.7552495 -3.8103173 -4.3990555 -4.5680504 -5.1843596 -5.6155686 -6.0796103 -6.3680191][-6.7648463 -6.22175 -5.8939738 -5.56821 -5.2447848 -4.9752021 -4.8487334 -5.0209246 -5.0345244 -5.637928 -5.79148 -6.09651 -6.3775368 -6.330729 -6.6371422][-7.6321149 -7.440815 -7.1845512 -7.0043478 -6.7679243 -6.5242205 -6.1393671 -6.2042971 -6.408535 -6.6024017 -6.6173973 -6.8467813 -6.8933344 -6.7359962 -6.6166415]]...]
INFO - root - 2017-12-15 16:36:20.585922: step 35310, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 54h:45m:04s remains)
INFO - root - 2017-12-15 16:36:26.937158: step 35320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 52h:13m:20s remains)
INFO - root - 2017-12-15 16:36:33.359415: step 35330, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 51h:49m:47s remains)
INFO - root - 2017-12-15 16:36:39.796913: step 35340, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 51h:55m:35s remains)
INFO - root - 2017-12-15 16:36:46.136834: step 35350, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 54h:00m:47s remains)
INFO - root - 2017-12-15 16:36:52.605597: step 35360, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 53h:34m:08s remains)
INFO - root - 2017-12-15 16:36:58.960291: step 35370, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 52h:02m:18s remains)
INFO - root - 2017-12-15 16:37:05.320338: step 35380, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.638 sec/batch; 52h:38m:23s remains)
INFO - root - 2017-12-15 16:37:11.646586: step 35390, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 52h:36m:56s remains)
INFO - root - 2017-12-15 16:37:17.998577: step 35400, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:14m:53s remains)
2017-12-15 16:37:18.552860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8268976 -5.8327875 -5.7685351 -5.7601962 -5.2499657 -5.1463952 -4.7662153 -4.432127 -3.9513865 -5.5885992 -6.5282378 -7.8842106 -8.5365419 -9.6337776 -10.42627][-6.0867233 -5.8808432 -5.7859035 -5.3652682 -4.8797588 -4.8160925 -4.304544 -4.0886173 -3.6114011 -4.961309 -5.9550543 -7.3890157 -8.44637 -9.3399715 -10.108456][-6.540041 -6.0517578 -5.2175093 -4.7729368 -4.2927866 -3.5101819 -2.5083618 -2.2877698 -2.0918293 -3.4723759 -4.3475027 -5.8431959 -7.1187949 -8.1510792 -9.1711044][-6.8424149 -6.2886143 -5.2105007 -3.6087804 -2.3168879 -1.5217838 -0.86280251 -0.57189178 -0.26895618 -1.8100348 -2.9677124 -4.7728338 -6.224402 -7.2410426 -8.1344128][-5.9109793 -4.8895273 -3.7358418 -2.4240484 -0.92172432 0.5325346 1.3350267 1.5552139 1.609271 -0.37369537 -2.0812984 -3.7138777 -4.9782429 -6.6722331 -7.8277121][-5.7640085 -5.0218134 -3.4913955 -1.2796006 0.31578827 1.6228151 2.9827881 3.1731806 2.9183378 0.757638 -1.00983 -3.4844522 -5.1793818 -6.6311026 -7.2496424][-4.9824333 -4.2944536 -2.9218216 -0.70235348 1.1304016 2.1139841 2.8655157 3.7034502 3.9357395 1.6994448 -0.49788761 -3.1548223 -4.9530463 -6.7447748 -7.4602652][-3.9474518 -3.8395567 -2.9782414 -1.6520343 0.049453259 1.6517363 2.8014822 3.1186886 3.4284744 1.7475929 -0.35752249 -2.8811092 -4.6272769 -6.5753393 -7.4039106][-4.008419 -3.9547863 -3.371366 -2.4104052 -1.9409628 -0.6427331 0.69297123 1.2350111 1.6264782 0.29357243 -0.89467955 -3.349051 -5.0358295 -6.7181764 -7.4706531][-6.460701 -6.5079951 -6.219058 -4.6949921 -3.7528002 -2.9786263 -1.8432426 -1.0372028 -0.18776178 -1.8531775 -2.8417759 -4.5109429 -5.8245592 -7.2721467 -8.06387][-7.1296806 -7.94865 -7.4456134 -6.6165352 -5.8904686 -4.88447 -4.1679478 -4.0332336 -3.4937668 -4.4327059 -4.6357527 -5.7459693 -6.7794709 -7.7436271 -8.3298264][-7.8252282 -8.35289 -8.2487783 -7.3400211 -6.4724469 -5.420207 -4.7448077 -5.41387 -5.9410892 -7.0603809 -7.3861618 -7.9742393 -8.5374947 -8.8992567 -8.8343725][-8.97727 -8.5494814 -7.5440283 -6.6727581 -5.969615 -5.7113094 -5.7955246 -6.4226894 -7.0140705 -8.1946 -8.6177015 -8.5807037 -8.7257462 -8.8175077 -8.6681767][-7.7140493 -7.2835383 -6.6063476 -5.6206527 -4.7431288 -4.3243113 -4.1288915 -5.3895731 -6.309763 -7.3668327 -7.9206891 -8.1030951 -7.9352527 -7.7221603 -7.5755477][-7.9342232 -7.5559564 -6.9742155 -6.3521013 -5.7967882 -5.1524124 -5.0755053 -5.365108 -6.2042742 -7.3453608 -7.8208308 -8.0618858 -7.7755475 -7.6319737 -7.5617747]]...]
INFO - root - 2017-12-15 16:37:25.022072: step 35410, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 51h:33m:59s remains)
INFO - root - 2017-12-15 16:37:31.472424: step 35420, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 52h:49m:10s remains)
INFO - root - 2017-12-15 16:37:37.986560: step 35430, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 52h:24m:15s remains)
INFO - root - 2017-12-15 16:37:44.284174: step 35440, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 51h:49m:37s remains)
INFO - root - 2017-12-15 16:37:50.670392: step 35450, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 51h:54m:26s remains)
INFO - root - 2017-12-15 16:37:57.048937: step 35460, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 51h:51m:47s remains)
INFO - root - 2017-12-15 16:38:03.547821: step 35470, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 52h:59m:08s remains)
INFO - root - 2017-12-15 16:38:09.851253: step 35480, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 51h:30m:10s remains)
INFO - root - 2017-12-15 16:38:16.220935: step 35490, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 54h:29m:39s remains)
INFO - root - 2017-12-15 16:38:22.537016: step 35500, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 52h:27m:29s remains)
2017-12-15 16:38:23.046971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6420355 -5.9669414 -5.5447865 -5.6581135 -6.6201086 -7.5173388 -8.3059931 -8.5474854 -8.4767685 -8.8854218 -9.032712 -10.020468 -9.6788807 -9.5825205 -10.323431][-5.6639972 -6.2119226 -6.3415322 -6.64079 -6.9981422 -7.4649749 -8.167449 -8.8490028 -8.7987766 -8.9916267 -9.0461559 -10.378197 -10.51762 -10.28818 -10.731381][-5.897047 -5.0792103 -5.27091 -5.3158226 -5.1573019 -5.4437122 -5.7522697 -5.9844766 -5.9600544 -6.7045307 -6.8707552 -7.8899307 -8.340929 -8.10235 -8.5769749][-4.47118 -4.1482868 -3.790561 -2.9741178 -2.65774 -2.8903093 -2.8478823 -2.9822774 -3.3057027 -4.2222695 -4.9233356 -6.6175728 -7.0651274 -7.1992865 -7.9163079][-4.8686233 -3.5306363 -2.0302377 -0.95825148 -0.12232256 0.46930218 0.99775028 0.60024357 0.3664341 -0.94288874 -2.2079396 -4.8186789 -5.911479 -5.9648361 -6.5901623][-2.9147754 -2.7184978 -2.535296 -0.78974295 0.73043442 2.0755262 3.5544157 3.7433081 4.0074835 2.2824316 0.1448164 -2.6830621 -3.9861829 -4.2588477 -5.3619785][-3.4149365 -2.7790871 -1.8612528 -0.11534834 1.6757946 3.4151392 5.0176067 5.1720438 5.4713449 3.9822731 1.7957411 -1.353127 -3.5172248 -4.2935491 -5.597641][-3.3635802 -2.9491954 -2.1724191 -0.50403214 1.5407314 3.6766129 5.3788157 5.8499613 6.1508112 4.3534641 2.5589819 -0.83280182 -3.4060578 -4.2826262 -5.5304117][-3.452703 -3.0203609 -2.845335 -1.3933058 -0.0658555 1.521945 3.2835321 4.3242474 4.5051479 2.7374563 1.161974 -1.7818327 -3.7100031 -4.5420957 -5.9739466][-3.6698852 -3.8471479 -3.6618366 -2.453362 -1.6955371 -0.35434198 0.84356403 1.405983 1.7013922 0.53875351 -1.1860304 -3.7478154 -5.3704152 -5.8928566 -6.504179][-5.3431149 -5.3742514 -5.2149444 -4.5241485 -3.8965411 -2.4943066 -1.4503818 -1.1926599 -0.78047895 -1.5656824 -2.605576 -4.3871217 -5.74125 -6.2017841 -7.1545033][-6.6367149 -6.7927332 -6.8313551 -6.1963434 -5.5902224 -5.0267506 -4.4001369 -3.843241 -3.3528466 -3.6179876 -4.1156616 -5.7033 -6.1677289 -5.9487443 -6.0125928][-6.7194233 -6.7637205 -6.8794861 -6.3320546 -6.0434837 -5.4263196 -4.771368 -4.8751087 -5.1486645 -5.1138144 -5.0403347 -5.5119276 -5.6939092 -5.7911763 -5.6347384][-6.2228832 -6.5711422 -6.5159311 -5.825182 -5.8289285 -5.22931 -5.074091 -5.1344123 -5.1068444 -5.290946 -5.1939797 -5.4447021 -5.3543267 -5.2001 -5.0706534][-6.0906987 -6.6313272 -6.8210678 -6.5448 -6.616117 -6.2671857 -5.7610087 -5.9272466 -6.0677652 -6.3610792 -5.9162531 -5.7064304 -5.8264647 -5.6543012 -5.5625591]]...]
INFO - root - 2017-12-15 16:38:29.653406: step 35510, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 54h:18m:57s remains)
INFO - root - 2017-12-15 16:38:35.998698: step 35520, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 51h:31m:01s remains)
INFO - root - 2017-12-15 16:38:42.458602: step 35530, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 54h:03m:42s remains)
INFO - root - 2017-12-15 16:38:48.811438: step 35540, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 51h:34m:53s remains)
INFO - root - 2017-12-15 16:38:55.176263: step 35550, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 52h:33m:36s remains)
INFO - root - 2017-12-15 16:39:01.469378: step 35560, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 52h:40m:00s remains)
INFO - root - 2017-12-15 16:39:07.873692: step 35570, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 52h:32m:59s remains)
INFO - root - 2017-12-15 16:39:14.264728: step 35580, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 51h:32m:45s remains)
INFO - root - 2017-12-15 16:39:20.708967: step 35590, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 53h:20m:50s remains)
INFO - root - 2017-12-15 16:39:27.153889: step 35600, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 52h:44m:39s remains)
2017-12-15 16:39:27.615984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5174809 -6.0242643 -5.7486138 -5.1985817 -4.372117 -3.960794 -3.7528858 -4.3509016 -4.2978907 -4.9996567 -6.2917948 -6.5966287 -7.6284657 -7.1545143 -7.5427847][-4.3050303 -4.9388633 -5.0714951 -5.1271114 -4.8496881 -4.3727808 -3.5526495 -3.6411476 -3.2087026 -3.908586 -4.4425526 -4.8058753 -6.3230996 -6.8366365 -7.57882][-3.4206429 -3.92199 -3.6540718 -3.6205921 -3.2671514 -2.357245 -1.5617361 -1.5136924 -1.3437042 -2.065794 -3.0562973 -3.2181845 -3.7177455 -3.944778 -5.4154587][-3.359694 -3.5008254 -2.9269376 -2.1725473 -1.5358691 -0.23515654 0.87351513 0.81305981 0.82612991 -0.21503782 -1.2658815 -1.5326357 -2.5890675 -2.5840087 -3.2557611][-2.986928 -2.3316088 -1.0207076 -0.10534382 0.93524742 1.7849255 2.5940151 2.8121471 2.8310442 1.6749115 0.13899279 -1.1235495 -2.1468925 -2.1326513 -2.7257085][-2.0567217 -1.5782437 -0.058168411 1.1732779 2.4333687 3.7384825 5.0170717 5.4872494 5.5701303 4.077569 1.7569313 0.033892632 -1.7898092 -2.4474874 -2.9044027][-2.314137 -1.8654013 -0.88510132 0.85817623 2.4648085 4.0676565 5.3461714 5.7235403 5.5141363 3.6650534 1.3072147 -0.61608696 -2.7806091 -3.10596 -4.1444082][-2.5662794 -1.5747313 -0.30236435 1.1947832 2.4705267 2.8287334 3.7325926 4.0744238 4.2729816 2.7283783 0.63541889 -1.2099757 -3.0738587 -3.9321945 -5.3169603][-3.4104824 -2.8304396 -1.8251433 -0.59823036 0.85971069 2.0429726 2.6987858 3.2566023 3.5442438 1.5451593 -0.62523317 -1.8942566 -4.0921335 -4.9526224 -6.3261776][-3.4139075 -2.966805 -2.3953233 -1.4297981 -0.38389635 0.9782753 2.0759363 2.1001024 1.9610081 -0.005692482 -2.2951584 -3.824275 -5.5073571 -5.8011885 -6.9982314][-5.3429966 -4.915256 -4.2562876 -2.8406749 -1.5269208 -0.4092083 0.42773628 0.009663105 -0.10624409 -1.5807338 -3.2522101 -3.4141259 -4.2148061 -4.8235869 -6.4260616][-5.7497563 -4.910759 -4.7191257 -4.291389 -3.2569423 -2.13593 -1.2377625 -0.92208815 -0.79496241 -1.9146051 -2.8870673 -3.9660335 -4.9526119 -4.6005893 -5.3189688][-7.2918692 -6.4374676 -5.5554967 -4.7214894 -3.8550704 -2.7180676 -1.5822682 -1.6573076 -1.7190905 -2.5024953 -3.4450154 -3.6028137 -3.6617522 -3.4729738 -3.974498][-7.5645046 -6.3140755 -5.5530539 -5.0323124 -4.4150991 -3.7751806 -2.962728 -2.8043971 -2.8532667 -3.5906563 -3.9755349 -4.4542871 -4.8164454 -4.2988377 -4.0905933][-8.1370764 -7.5396762 -6.82423 -6.1483364 -5.8990622 -5.7390876 -5.3056679 -5.3952355 -5.4381142 -5.3295684 -5.3494978 -5.4969044 -5.3550463 -5.1919985 -4.854826]]...]
INFO - root - 2017-12-15 16:39:34.037496: step 35610, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 51h:21m:58s remains)
INFO - root - 2017-12-15 16:39:40.427448: step 35620, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 52h:57m:34s remains)
INFO - root - 2017-12-15 16:39:46.903442: step 35630, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 54h:19m:00s remains)
INFO - root - 2017-12-15 16:39:53.210486: step 35640, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 52h:29m:08s remains)
INFO - root - 2017-12-15 16:39:59.679364: step 35650, loss = 0.24, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 55h:31m:50s remains)
INFO - root - 2017-12-15 16:40:06.032338: step 35660, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 53h:38m:43s remains)
INFO - root - 2017-12-15 16:40:12.382764: step 35670, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 52h:53m:00s remains)
INFO - root - 2017-12-15 16:40:18.738492: step 35680, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 53h:03m:06s remains)
INFO - root - 2017-12-15 16:40:25.213715: step 35690, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 52h:49m:46s remains)
INFO - root - 2017-12-15 16:40:31.569148: step 35700, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 51h:47m:29s remains)
2017-12-15 16:40:32.114792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2218041 -7.5953565 -8.076066 -7.272954 -5.7390594 -3.2637496 -1.8262601 -1.8100705 -1.4987531 -2.2726817 -3.1822777 -4.4502563 -5.1998148 -6.1745834 -6.1366825][-7.5376148 -7.6376061 -7.7930369 -7.3412123 -6.5743408 -5.0467548 -3.1040382 -2.1353936 -2.0944438 -3.6381626 -4.8917246 -5.9840274 -6.5564871 -7.1459632 -7.2356882][-6.3217964 -6.7639241 -6.9591155 -6.7415137 -5.824934 -4.3651409 -3.1968446 -3.0908537 -3.0818944 -4.5512342 -5.7004662 -6.7799826 -7.2102432 -8.3541183 -8.5259295][-6.230073 -6.48298 -6.228282 -5.378335 -4.3043237 -3.032506 -2.388958 -2.4516048 -2.5066762 -4.1455126 -5.7230806 -6.7531214 -7.4738712 -8.175622 -8.6270866][-7.1360259 -7.1536651 -6.4622703 -5.3252172 -3.3460431 -1.008152 0.12608099 -0.47331619 -1.6158752 -3.2874866 -4.5365629 -5.9179335 -7.1599092 -8.6477585 -9.0115][-8.3070278 -8.0113144 -6.6187658 -3.7145264 -1.0556746 0.66573524 1.6986284 2.1357555 1.8187609 -0.38428211 -2.606473 -4.237689 -5.2472706 -7.4301333 -8.6855259][-7.846365 -7.3731437 -5.6355085 -3.147202 -0.030572891 2.7336454 4.1560497 3.9690781 3.7258568 2.4011984 0.82732582 -1.673542 -3.671464 -5.7959809 -7.2075558][-7.1544037 -7.125711 -5.6713486 -2.4339581 0.55321407 3.0189505 4.1746807 4.8335886 5.5203409 4.3486481 2.9236078 0.77537155 -1.5693173 -4.3640556 -5.8881779][-6.6915393 -5.7105994 -5.4318666 -3.6497669 -0.91534948 1.8255653 3.3276129 3.656064 4.1800795 3.548562 2.8318882 1.1049652 -0.89977169 -3.23883 -4.4796419][-8.2043924 -7.1288686 -5.5626979 -3.6681175 -2.3114667 -0.94053984 0.15470028 1.8937626 2.9482861 1.5672512 0.24785566 -0.68361092 -1.3764796 -3.2388349 -3.8531232][-6.251369 -6.740767 -6.7143917 -5.1165738 -3.1151571 -1.8039365 -1.568924 -0.81711674 0.82616997 0.084740162 -1.8963146 -3.4625535 -4.1779385 -4.9526129 -4.7020845][-5.7931643 -5.3084526 -4.9856806 -4.1029863 -3.1016932 -1.792582 -0.99928617 -0.66258526 -0.21186113 -1.642087 -2.8304896 -4.0097046 -5.0757732 -6.3051682 -6.4301491][-6.5762844 -5.7994909 -4.922019 -3.8297794 -3.1618614 -2.189826 -0.98283482 0.075484753 0.36567593 -1.47299 -3.1871114 -3.8023384 -3.9092309 -5.621213 -6.825542][-6.2874036 -5.85571 -5.1439466 -4.5032454 -4.0738115 -3.01818 -2.241128 -1.7519364 -1.3328252 -1.7759609 -3.582624 -4.3542504 -4.2822256 -4.6638527 -4.8205147][-6.0864558 -5.7301598 -5.9105892 -4.87064 -3.805932 -3.8815117 -3.7709351 -3.1719975 -3.0117474 -3.627666 -3.7733617 -4.1385098 -4.5873151 -5.3630819 -5.7356625]]...]
INFO - root - 2017-12-15 16:40:38.482458: step 35710, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 53h:57m:10s remains)
INFO - root - 2017-12-15 16:40:44.902989: step 35720, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 53h:54m:29s remains)
INFO - root - 2017-12-15 16:40:51.268612: step 35730, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 52h:20m:19s remains)
INFO - root - 2017-12-15 16:40:57.701448: step 35740, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 52h:38m:16s remains)
INFO - root - 2017-12-15 16:41:04.055491: step 35750, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:39m:12s remains)
INFO - root - 2017-12-15 16:41:10.428536: step 35760, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 52h:48m:24s remains)
INFO - root - 2017-12-15 16:41:16.779293: step 35770, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 52h:43m:09s remains)
INFO - root - 2017-12-15 16:41:23.195933: step 35780, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 52h:29m:35s remains)
INFO - root - 2017-12-15 16:41:29.544327: step 35790, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 54h:17m:06s remains)
INFO - root - 2017-12-15 16:41:35.869875: step 35800, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 51h:41m:53s remains)
2017-12-15 16:41:36.363529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2242212 -9.5387783 -9.4580221 -8.5221863 -6.7177958 -4.7551651 -2.7213964 -1.3284063 -0.55552626 -0.93800735 -1.6382098 -3.0962334 -4.0667291 -5.7408104 -7.0005221][-8.4547825 -8.9025812 -9.5693436 -10.513557 -10.234188 -8.18601 -6.0321226 -3.7477067 -1.4911237 -1.4840326 -2.0232635 -3.1879649 -4.0892491 -5.2518377 -6.2983704][-5.9713182 -7.1437969 -8.2090626 -9.3636084 -10.08803 -9.8562145 -8.4254055 -6.10967 -4.0155916 -3.5532756 -2.9397674 -3.2900743 -3.7433057 -4.3952117 -5.6957645][-4.6410341 -5.4653149 -6.0012703 -6.8921008 -7.59955 -7.7114439 -7.3041568 -6.1570182 -4.6970863 -4.0765266 -3.7259662 -3.9882603 -3.9514425 -4.687603 -5.0522451][-3.9760482 -3.8715906 -4.3264647 -4.3346043 -4.0311589 -3.6200871 -3.2897077 -3.7088256 -3.8368046 -4.4965925 -4.8212719 -5.0836716 -5.0226555 -5.5874434 -6.0737224][-4.4079747 -4.0284052 -3.7883475 -2.8625159 -1.6881571 -0.43575191 0.92802429 1.0066433 0.13111925 -2.7087488 -4.831912 -6.1036539 -6.6895161 -7.2886906 -7.6896367][-5.3260193 -4.4996214 -3.0218077 -1.4045186 0.34110546 1.8297529 3.0111618 3.5759134 3.4562712 0.50325394 -2.8882766 -5.6654663 -7.1896124 -8.0871258 -8.3354435][-4.8300543 -4.6558743 -3.991405 -2.0367861 0.45466995 2.6316509 4.15302 4.2891941 3.9161091 1.3972282 -1.8232265 -4.9174585 -7.1522141 -8.5119047 -8.8255033][-3.8699751 -4.1202574 -3.9945776 -2.570056 -0.89531708 0.67968273 2.0286341 2.4289436 2.6174374 0.60085106 -1.5694451 -4.2962422 -6.5658493 -8.1118612 -8.7710094][-4.2396646 -3.2950196 -2.2865129 -1.9774971 -1.5732341 -0.55117321 0.2024889 0.28429127 -0.01936388 -1.9445233 -3.3151293 -5.2810907 -6.6784234 -8.013669 -8.9859161][-4.3042717 -3.8189263 -2.8918409 -2.0796471 -1.7340884 -1.6802435 -1.5811133 -1.7005906 -1.8884573 -3.8894331 -6.0474892 -7.3672519 -7.959908 -8.6257143 -8.7102461][-4.9559093 -5.0369616 -4.4922218 -3.3225827 -2.2801614 -1.8265624 -1.8433261 -2.46096 -3.3194022 -4.2480631 -5.27331 -7.067709 -7.9920039 -8.5425262 -8.7369671][-5.8244419 -6.0256228 -5.4129066 -4.6329331 -3.6611786 -2.4618273 -1.8044162 -2.5130467 -3.3553824 -4.670785 -5.4591789 -5.9916735 -7.0825782 -8.6577339 -9.4968052][-6.5131731 -7.2185793 -7.5656123 -7.7793989 -7.134819 -5.7838516 -3.8960016 -3.4935012 -4.3840818 -5.5042338 -6.7071018 -7.0736623 -7.4224954 -7.5038424 -7.6308937][-6.77584 -8.17432 -8.5395842 -9.2105789 -9.2229071 -8.5120134 -7.5721207 -6.7191219 -6.2740488 -6.3411875 -7.032248 -7.8501539 -8.0995722 -7.91026 -7.4766989]]...]
INFO - root - 2017-12-15 16:41:42.690908: step 35810, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.613 sec/batch; 50h:30m:59s remains)
INFO - root - 2017-12-15 16:41:48.955465: step 35820, loss = 0.29, batch loss = 0.18 (13.1 examples/sec; 0.609 sec/batch; 50h:10m:24s remains)
INFO - root - 2017-12-15 16:41:55.331120: step 35830, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:40m:00s remains)
INFO - root - 2017-12-15 16:42:01.663097: step 35840, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 51h:37m:29s remains)
INFO - root - 2017-12-15 16:42:08.000721: step 35850, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.614 sec/batch; 50h:36m:46s remains)
INFO - root - 2017-12-15 16:42:14.351209: step 35860, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 51h:53m:32s remains)
INFO - root - 2017-12-15 16:42:20.750255: step 35870, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 54h:16m:17s remains)
INFO - root - 2017-12-15 16:42:27.123822: step 35880, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 51h:48m:01s remains)
INFO - root - 2017-12-15 16:42:33.497950: step 35890, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 51h:45m:06s remains)
INFO - root - 2017-12-15 16:42:39.857548: step 35900, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 53h:33m:32s remains)
2017-12-15 16:42:40.340281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7469997 -3.1952 -3.79008 -4.1682105 -4.0376205 -3.4356484 -2.6346288 -2.2874746 -1.7242179 -2.4406824 -3.0939875 -3.8446519 -5.2189183 -5.8826189 -6.610465][-3.4474521 -3.26958 -3.4885297 -3.8007524 -4.070097 -3.9961622 -3.3102355 -2.440671 -1.7753377 -3.0017967 -3.9688385 -4.54467 -5.6056895 -6.5347605 -7.4220943][-3.6245213 -3.2567387 -3.1192336 -2.8847513 -2.6435313 -2.6523347 -2.5712051 -2.2142081 -1.7026386 -2.9444413 -4.1152825 -5.0537848 -6.1253462 -7.1894169 -8.1435843][-3.7241302 -3.1246123 -2.8281436 -2.2370539 -1.6010594 -1.1017966 -0.72597647 -0.74179077 -0.81635141 -2.4704175 -3.7036109 -5.00535 -6.4926848 -7.3204961 -8.1499166][-5.0103464 -3.8470676 -3.0153165 -2.1890812 -0.98223495 0.10028076 1.0231714 1.0149307 0.73308277 -1.4941034 -3.4922304 -5.2311134 -6.9265776 -7.5991397 -8.3475418][-6.4265203 -5.4792571 -4.733243 -3.2446494 -1.3081989 0.18521452 1.64464 1.7701883 1.5897274 -0.58435345 -2.9651904 -5.0858889 -7.1813507 -8.3005428 -8.8860512][-6.54045 -5.6770239 -4.5960636 -3.3169122 -1.2028728 0.82211685 2.5691261 2.975769 3.0249472 1.0580101 -0.99482489 -3.4562359 -5.72917 -7.185 -8.2378864][-6.0841732 -5.3033895 -4.323204 -2.6978216 -0.54628563 1.674778 3.9397402 4.4554739 4.4636459 2.0491676 -0.37930727 -2.6166697 -4.7879024 -6.2015748 -7.3639917][-6.3682027 -5.4659605 -4.3754559 -3.1805167 -1.6093001 0.14759779 2.2570543 3.5119019 4.2887945 1.4991989 -1.3295574 -3.3566251 -5.4009662 -6.5961728 -7.7907391][-7.2644925 -6.4680743 -5.5374727 -4.0393133 -2.5361581 -1.7501631 -0.45593691 0.258492 0.78648281 -1.2170105 -3.4625602 -5.3368659 -6.9765677 -7.4138522 -7.9874072][-8.1264277 -7.66708 -6.9375906 -5.85182 -4.5584097 -3.2042246 -1.9640532 -2.08961 -1.8168964 -3.1529999 -4.5073185 -5.8451257 -7.240859 -7.7429028 -7.9256315][-8.2418566 -7.5524697 -6.9841156 -6.2709627 -5.5383568 -4.7047873 -3.8546786 -3.8467917 -3.9189005 -4.6868649 -5.0711927 -5.7672076 -6.8202524 -7.6697836 -8.2131834][-8.5055208 -7.8117409 -6.9222155 -6.2818756 -5.71801 -5.2666879 -4.9244924 -5.0851226 -5.19305 -6.2244358 -6.6053753 -6.4745784 -6.7005405 -7.3498793 -7.7392349][-7.8627768 -7.883389 -7.5079923 -6.586041 -5.6682978 -5.1167717 -4.6847138 -5.0600672 -5.4582434 -6.0375738 -6.0590792 -6.2606258 -6.4724021 -6.4844637 -6.4176655][-7.7058091 -8.2136126 -8.41933 -7.7611322 -6.9869165 -6.3321753 -5.7751956 -6.1292024 -6.5179462 -6.7729435 -6.776896 -6.6737719 -6.4638739 -6.1887193 -5.8741837]]...]
INFO - root - 2017-12-15 16:42:46.707550: step 35910, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 51h:49m:17s remains)
INFO - root - 2017-12-15 16:42:53.066431: step 35920, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 53h:07m:54s remains)
INFO - root - 2017-12-15 16:42:59.455234: step 35930, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 50h:52m:11s remains)
INFO - root - 2017-12-15 16:43:05.838159: step 35940, loss = 0.35, batch loss = 0.23 (12.1 examples/sec; 0.660 sec/batch; 54h:24m:35s remains)
INFO - root - 2017-12-15 16:43:12.344285: step 35950, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 53h:09m:34s remains)
INFO - root - 2017-12-15 16:43:18.840630: step 35960, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 51h:21m:15s remains)
INFO - root - 2017-12-15 16:43:25.286521: step 35970, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 52h:39m:19s remains)
INFO - root - 2017-12-15 16:43:31.761973: step 35980, loss = 0.36, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 54h:04m:21s remains)
INFO - root - 2017-12-15 16:43:38.241356: step 35990, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 53h:08m:38s remains)
INFO - root - 2017-12-15 16:43:44.692689: step 36000, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.643 sec/batch; 52h:55m:08s remains)
2017-12-15 16:43:45.235981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4178257 -3.0357056 -2.9462838 -2.7900362 -2.6790595 -2.29172 -1.897089 -1.7622137 -1.8120117 -3.8192916 -4.3318787 -5.7756572 -6.377346 -7.2715855 -8.09125][-3.5038338 -3.2068806 -3.0682559 -3.2734318 -3.4698052 -3.1987548 -3.0556803 -2.9767375 -2.8205252 -4.3903189 -4.5737629 -5.7237062 -6.5214629 -7.5658641 -8.3433619][-3.4705338 -2.9791627 -2.8607979 -2.8518906 -2.7333574 -2.4151511 -2.2998729 -2.4680943 -2.4659934 -4.3958321 -4.7802258 -5.9374084 -6.7558546 -7.7633004 -8.58965][-3.7139213 -3.1769052 -2.9682269 -2.71345 -2.5109153 -2.0331564 -1.6026435 -1.8946609 -2.1130733 -4.1548491 -4.7274666 -5.9238372 -6.7156124 -7.6286516 -8.4809952][-4.561286 -3.5522265 -3.0944967 -2.6718202 -2.2788453 -1.6347404 -1.2989702 -1.6532598 -1.7417626 -3.7273505 -4.3504763 -5.5360775 -6.3338223 -7.1257648 -8.1291733][-5.4461117 -4.3253841 -3.4303846 -2.5271602 -1.6438498 -0.78717709 -0.50726748 -0.81057978 -0.82335329 -2.8040848 -3.4279084 -4.5499883 -5.5231352 -6.5680614 -7.4460344][-4.890183 -4.20061 -3.4202161 -2.1373096 -1.0896845 0.077782154 0.70799923 0.61557579 0.42349148 -1.6857004 -2.3768296 -3.5542674 -4.47892 -5.6730628 -6.6720552][-4.2831316 -3.2520933 -2.2787061 -1.1506181 -0.22924089 0.67906952 1.1374254 1.3179646 1.4699059 -0.702723 -1.633739 -2.8916879 -3.8968246 -5.0500555 -6.0849371][-3.8664224 -3.1034698 -2.1590047 -1.1930375 -0.47872782 0.099328995 0.34415436 0.70685863 1.0904875 -0.83496618 -1.635725 -3.0606112 -4.1425943 -5.2661209 -6.1261339][-3.1522489 -2.676825 -2.0057588 -1.2294464 -0.66831923 -0.19979191 0.059542179 0.39615631 0.5670948 -1.3164477 -2.0235076 -3.3753395 -4.5228853 -5.6101656 -6.4548054][-4.3562346 -3.4655032 -2.5781035 -1.7848535 -1.1739149 -0.88810635 -0.98542166 -0.85695076 -0.64418268 -2.4863029 -3.370996 -4.3521605 -5.3010368 -6.1939082 -6.7115278][-4.5804272 -3.7838774 -2.9610553 -2.2139211 -1.7343249 -1.329905 -1.3547978 -1.5330734 -1.4248376 -2.6912522 -3.3358917 -4.4151993 -5.1917095 -5.9078245 -6.5700722][-5.7147636 -4.9133692 -4.1590023 -3.3872166 -2.9785705 -2.5060086 -2.4793668 -2.4677763 -2.4205918 -3.426435 -3.7185435 -4.5386324 -4.8798761 -5.4104891 -5.9024034][-6.3192558 -5.5677977 -4.83869 -3.9473789 -3.2459402 -2.6591406 -2.8391881 -2.9095922 -2.8779168 -3.4462714 -3.4972854 -3.8753896 -4.03533 -4.5213356 -4.9656687][-6.5188794 -6.1983147 -5.5050516 -4.9886923 -4.6572337 -3.9785058 -4.2165184 -4.427887 -4.5515137 -4.3838139 -4.0988932 -4.2427416 -4.4217815 -4.6494493 -4.8456221]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:43:51.645387: step 36010, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 53h:45m:34s remains)
INFO - root - 2017-12-15 16:43:58.047600: step 36020, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 53h:07m:28s remains)
INFO - root - 2017-12-15 16:44:04.513195: step 36030, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 52h:59m:33s remains)
INFO - root - 2017-12-15 16:44:11.114705: step 36040, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 53h:43m:39s remains)
INFO - root - 2017-12-15 16:44:17.541640: step 36050, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 53h:18m:21s remains)
INFO - root - 2017-12-15 16:44:24.089167: step 36060, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 54h:39m:15s remains)
INFO - root - 2017-12-15 16:44:30.585540: step 36070, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 53h:28m:53s remains)
INFO - root - 2017-12-15 16:44:37.087005: step 36080, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 53h:14m:18s remains)
INFO - root - 2017-12-15 16:44:43.451672: step 36090, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 52h:33m:01s remains)
INFO - root - 2017-12-15 16:44:49.986112: step 36100, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 55h:01m:29s remains)
2017-12-15 16:44:50.501839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7971511 -2.1190147 -2.626821 -2.8501444 -3.0702491 -3.7088344 -3.9675655 -3.6180849 -3.0689344 -4.7043209 -4.998539 -5.9382157 -6.9890761 -7.2383032 -8.0341873][-2.3124843 -2.2817674 -2.7607031 -3.1841321 -3.5885496 -4.0519576 -4.3244085 -4.4906883 -4.0554433 -5.8156252 -6.1847372 -7.1686754 -8.4509525 -8.6169748 -9.2050581][-3.1812677 -2.717535 -2.8229942 -2.8103352 -2.8702831 -3.5533938 -3.6436329 -3.7473319 -3.6414523 -5.7641664 -6.4174275 -7.6600461 -9.00935 -9.4295864 -10.013251][-3.7650018 -3.2073698 -3.0639091 -2.628449 -2.2639236 -2.3170691 -1.8483481 -1.7822108 -1.6337938 -3.7826436 -4.6587124 -6.2490034 -7.9853754 -8.6517286 -9.4585667][-4.5726414 -3.788811 -3.4384112 -2.7806358 -2.1472912 -1.3970375 -0.28781128 0.28774595 0.86714172 -1.0868192 -2.1530485 -4.5621943 -6.7640071 -7.9417238 -9.114336][-6.1502986 -4.9767218 -4.6318254 -3.3457322 -1.9266028 -0.71455479 0.97814369 1.8141518 2.6392393 0.54396057 -0.69863081 -3.1189184 -5.6147618 -6.9805837 -8.694191][-6.2855253 -5.1453276 -4.1512089 -2.4122772 -0.67771578 1.0262327 2.9087162 4.1326733 5.6102705 3.2052517 1.5467806 -1.0690312 -3.7864318 -5.2201948 -6.9017658][-6.2047019 -4.5908957 -3.6306252 -1.8378067 0.34833241 2.2642965 4.3118324 5.3834839 6.6397066 4.369669 3.1433735 0.33352852 -2.4879813 -3.8709838 -5.5329237][-6.3516917 -5.0285 -4.066483 -2.3345184 -0.46476936 1.2147999 3.1407633 3.7615633 4.3434105 1.765871 0.5921011 -1.8697176 -3.6100931 -4.4550419 -5.8720965][-7.3269858 -5.8982134 -4.8612881 -3.2489452 -1.6775622 -0.67688942 0.60618591 1.5457573 1.9785595 -1.4131212 -2.8175244 -4.669364 -5.9294467 -6.2322478 -7.157618][-9.1001415 -7.863029 -6.9542871 -5.3711548 -4.0770493 -2.8562617 -1.7059851 -1.4093628 -1.1798105 -3.7023349 -4.7967515 -6.6336894 -7.9376678 -8.0692663 -8.2951508][-9.9622412 -9.1017742 -8.2561941 -7.1303244 -5.8573403 -4.8428059 -4.0998135 -3.8986363 -3.5823274 -5.3494067 -5.76213 -7.2507463 -8.2615833 -8.3150167 -8.4706917][-9.6240044 -9.09278 -8.9193659 -7.6772652 -6.5856352 -5.84266 -5.0375986 -4.5709314 -4.6061163 -6.1509118 -6.2785954 -6.9937677 -7.7186904 -7.6564012 -7.79426][-9.9334488 -9.24419 -9.0008869 -8.2125187 -7.2316818 -6.099072 -5.270812 -5.2694507 -5.233603 -6.0758944 -6.2151933 -6.4782982 -6.9132948 -6.6516986 -6.6360645][-10.286269 -9.6179295 -9.4580669 -8.615303 -7.4320135 -6.5174565 -5.8196268 -5.520009 -5.6833668 -5.8025441 -5.8446279 -6.0309854 -5.9020185 -5.6353531 -5.53837]]...]
INFO - root - 2017-12-15 16:44:56.967430: step 36110, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 53h:17m:07s remains)
INFO - root - 2017-12-15 16:45:03.371316: step 36120, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 53h:43m:23s remains)
INFO - root - 2017-12-15 16:45:09.849093: step 36130, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 54h:20m:30s remains)
INFO - root - 2017-12-15 16:45:16.285347: step 36140, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 52h:20m:13s remains)
INFO - root - 2017-12-15 16:45:22.744738: step 36150, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 52h:15m:52s remains)
INFO - root - 2017-12-15 16:45:29.177326: step 36160, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.668 sec/batch; 54h:58m:37s remains)
INFO - root - 2017-12-15 16:45:35.617991: step 36170, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 53h:50m:39s remains)
INFO - root - 2017-12-15 16:45:42.047427: step 36180, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 53h:09m:07s remains)
INFO - root - 2017-12-15 16:45:48.570626: step 36190, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 54h:02m:49s remains)
INFO - root - 2017-12-15 16:45:55.038597: step 36200, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 53h:18m:21s remains)
2017-12-15 16:45:55.563815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3429251 -3.6699109 -2.9113746 -2.8344035 -2.9051991 -2.5357137 -2.2942452 -2.38806 -2.2966642 -3.1433105 -3.6609278 -4.4920044 -5.4249897 -6.2424245 -6.5398526][-3.6647 -3.6749959 -3.4097614 -3.5077353 -3.5030613 -3.2781415 -3.0187206 -3.0825458 -2.7621241 -3.6164055 -4.16241 -5.0611348 -5.8818116 -6.3914533 -6.2598066][-3.5211148 -3.1208878 -2.8959579 -3.1372566 -3.1784053 -2.9297938 -3.0669832 -3.1094255 -2.4698458 -3.5178814 -3.6241078 -4.1051693 -4.9927731 -5.6981091 -6.2369571][-3.3713546 -3.1399431 -3.2269225 -2.7207217 -2.0039892 -1.8547254 -1.4745073 -1.219389 -0.95917845 -2.2848182 -3.1198182 -4.3826356 -5.1979389 -5.7156324 -6.144773][-4.7011323 -3.6764965 -2.4235792 -1.773757 -1.4640598 -0.48658991 0.52704811 0.24416924 -0.070235252 -1.6266394 -2.3587656 -3.4447374 -4.7216172 -5.8456917 -6.468009][-4.5233288 -3.8624642 -3.1012287 -1.3831072 0.52500534 1.3574743 1.728508 2.0910559 2.1057568 -0.017482281 -1.8223057 -3.5873747 -4.9536848 -6.2880168 -6.9679608][-4.9455862 -3.9319909 -2.46953 -1.3533449 -0.13097715 1.7313251 3.124115 3.2844687 2.8323441 1.1991873 -0.19569826 -2.1930203 -4.3258591 -5.9021659 -6.5445514][-5.1624231 -3.9810252 -2.7176404 -1.1977596 0.79753494 2.372179 3.0235558 3.6345835 3.7247238 1.7372961 -0.20118427 -1.9736347 -3.4545403 -5.3152132 -6.4798312][-4.6319141 -3.7510614 -2.3169198 -0.96837568 -0.021703243 1.2883949 2.5627785 2.9546127 3.0060511 1.5029898 0.15923738 -1.5403214 -3.4614 -4.9394016 -5.5246549][-4.5570941 -3.8147447 -2.9048719 -2.0534725 -0.67160892 0.56208515 0.92900085 1.0588779 1.479722 0.14881849 -0.82982779 -1.9372334 -3.3529325 -4.6632795 -5.6747437][-3.9583893 -3.6327896 -3.5315118 -3.1029639 -2.2477713 -1.4912205 -0.81518316 -0.51772547 -0.48594332 -1.7746053 -2.6185927 -3.0234823 -3.9475477 -4.5645618 -4.8380704][-4.678894 -3.7277071 -2.79954 -2.7301083 -2.5526638 -2.41124 -2.1520777 -2.0094461 -1.9801984 -2.3383894 -2.6512218 -3.2189903 -4.0119529 -4.5510283 -5.0062551][-4.1602745 -3.5324583 -3.1921206 -3.1243386 -2.9508696 -2.9657626 -2.9501209 -2.9612732 -2.9595947 -3.0441923 -3.4235344 -3.257915 -3.0220947 -3.8149908 -4.3704329][-4.0251942 -3.8840148 -3.7476122 -3.9954145 -4.4844923 -4.7879152 -5.1889653 -4.8614221 -4.2687941 -3.8531675 -3.6159792 -3.8367875 -4.3200731 -4.3634844 -4.3539143][-4.6458645 -4.1957855 -3.7823896 -4.443038 -5.2611217 -5.9115915 -6.6249003 -7.0539746 -7.1753273 -5.8955374 -4.8294444 -4.8298721 -4.7595978 -4.9964981 -5.5020895]]...]
INFO - root - 2017-12-15 16:46:02.030260: step 36210, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 54h:11m:23s remains)
INFO - root - 2017-12-15 16:46:08.399488: step 36220, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 54h:20m:21s remains)
INFO - root - 2017-12-15 16:46:14.784668: step 36230, loss = 0.28, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 55h:51m:11s remains)
INFO - root - 2017-12-15 16:46:21.232514: step 36240, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 53h:18m:58s remains)
INFO - root - 2017-12-15 16:46:27.662811: step 36250, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 51h:06m:26s remains)
INFO - root - 2017-12-15 16:46:34.054868: step 36260, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 52h:38m:18s remains)
INFO - root - 2017-12-15 16:46:40.523227: step 36270, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 54h:03m:11s remains)
INFO - root - 2017-12-15 16:46:46.960945: step 36280, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 52h:45m:58s remains)
INFO - root - 2017-12-15 16:46:53.406286: step 36290, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 53h:05m:49s remains)
INFO - root - 2017-12-15 16:46:59.837522: step 36300, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 52h:12m:16s remains)
2017-12-15 16:47:00.339733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0138927 -6.631855 -6.2554317 -6.0547705 -6.0724864 -6.0472565 -5.3204632 -4.2306719 -2.9259992 -2.5811415 -3.3271046 -4.778966 -6.4991994 -7.5406537 -8.6891489][-7.3691669 -6.922843 -6.6378055 -6.8347368 -7.2779593 -7.5653372 -6.9567637 -5.8674116 -4.49006 -3.362493 -3.5260892 -3.9673131 -5.9396768 -6.831388 -8.1450634][-6.1095319 -5.28199 -5.2153606 -5.6619773 -6.1823134 -6.76259 -6.6912141 -5.6695409 -4.3641768 -3.4921412 -3.64469 -3.4481859 -4.4980917 -5.0300064 -6.010767][-4.4075055 -3.4622684 -2.9950013 -2.9546456 -3.5838513 -3.9139094 -3.7257233 -3.4931774 -2.7367558 -1.885612 -2.3251457 -3.1291232 -3.9431241 -4.4152145 -5.1137853][-3.2900524 -2.3687315 -1.5866356 -1.0657411 -1.248764 -0.78506041 -0.45895004 -0.37993622 0.092668056 -0.01584959 -1.0609856 -2.215641 -3.7479658 -4.4274025 -4.8446379][-2.5003381 -1.4394112 -1.1995554 -0.46912956 0.384161 1.6675348 2.8406038 3.0127649 3.27254 2.2563705 0.173594 -1.7344031 -3.4517741 -5.1661005 -5.5774312][-3.935076 -3.1200643 -1.976172 -0.73496485 0.77151775 2.4647493 4.2699623 4.589757 5.1576576 3.8229923 1.3842926 -1.1550155 -3.6337714 -5.12585 -6.0157471][-4.5167971 -4.2210097 -2.9515281 -1.4584785 0.81598186 2.5265331 4.5098772 5.2185812 5.6721964 4.1322203 1.524622 -1.217865 -3.6360745 -5.1897807 -6.511579][-5.4361773 -4.7111626 -3.9784355 -2.8125038 -0.62420368 1.3875742 3.2551184 4.1007881 4.6448097 3.5477018 1.0403137 -1.7659616 -4.1105556 -5.7891488 -6.5197172][-6.1042137 -5.5598869 -4.5158467 -4.1113906 -2.7024746 -0.71717358 0.99910831 1.7130108 2.1080513 0.97911835 -1.4457564 -3.1393576 -5.1444159 -6.3752623 -7.2685332][-7.3723111 -7.3807716 -6.8500514 -6.5227966 -5.5965939 -4.727922 -3.2865291 -2.3769965 -1.9690018 -2.5053058 -4.1448069 -5.0144072 -6.1802835 -6.9764371 -7.7465272][-8.1801519 -7.6877089 -7.73356 -7.5410242 -6.9549117 -6.5148144 -5.8774872 -5.4196005 -4.6781316 -5.1887665 -6.3297877 -6.3321972 -6.8702588 -7.4948192 -8.017951][-8.6546488 -8.6347694 -8.3612194 -8.3211155 -7.6560869 -7.0608449 -6.7124577 -6.6098924 -6.4306488 -6.3821077 -7.2726064 -7.2319312 -7.1103845 -6.7299066 -7.1307521][-6.7027149 -7.106956 -6.8463292 -6.7481804 -6.4129686 -5.9711337 -5.6624479 -5.8668294 -6.0040965 -6.0180869 -6.5689268 -6.8240662 -6.949894 -6.8865471 -6.4456019][-5.8023605 -5.5083818 -5.3292818 -5.5171123 -5.50721 -5.3700747 -5.26637 -5.2288609 -5.1869926 -5.193099 -5.6733637 -6.1390991 -6.5429344 -7.1052589 -7.1230941]]...]
INFO - root - 2017-12-15 16:47:06.723998: step 36310, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 51h:05m:38s remains)
INFO - root - 2017-12-15 16:47:13.165931: step 36320, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 52h:39m:22s remains)
INFO - root - 2017-12-15 16:47:19.541293: step 36330, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.634 sec/batch; 52h:09m:47s remains)
INFO - root - 2017-12-15 16:47:25.994781: step 36340, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 52h:01m:43s remains)
INFO - root - 2017-12-15 16:47:32.485097: step 36350, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 52h:21m:24s remains)
INFO - root - 2017-12-15 16:47:38.942271: step 36360, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:35m:33s remains)
INFO - root - 2017-12-15 16:47:45.419384: step 36370, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 53h:43m:27s remains)
INFO - root - 2017-12-15 16:47:51.901869: step 36380, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 51h:25m:36s remains)
INFO - root - 2017-12-15 16:47:58.325730: step 36390, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 52h:52m:36s remains)
INFO - root - 2017-12-15 16:48:04.778549: step 36400, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 51h:29m:18s remains)
2017-12-15 16:48:05.291544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2454252 -3.0222507 -3.5824766 -4.2662811 -4.661787 -4.7666092 -4.9080238 -5.2835922 -5.0896339 -5.8176966 -6.9044437 -6.814177 -7.1584578 -7.850585 -8.2847347][-3.1972375 -3.3643041 -3.8533928 -4.850852 -5.413311 -5.69734 -6.2648597 -6.5030622 -6.3238025 -6.4275975 -7.3666825 -7.0951691 -8.1307182 -8.5869217 -9.1233282][-5.0843873 -4.8670292 -4.3563976 -5.1072702 -5.7017255 -5.8670583 -6.0617571 -6.6011558 -6.7326775 -6.8774376 -7.6102738 -7.1653423 -7.582128 -8.3960609 -9.4500513][-5.2937956 -4.9281464 -4.6126757 -4.2780209 -3.719523 -4.0209188 -4.3997869 -4.7539062 -4.4575906 -4.8870664 -6.0496311 -5.8812571 -6.6820941 -7.4815979 -8.2648268][-5.2134171 -4.6463356 -4.0319719 -3.5082402 -2.9607687 -2.5864229 -2.128449 -1.9524803 -1.3457127 -1.7424173 -2.8483596 -2.9662318 -4.6986294 -5.7612209 -7.4607849][-3.6054111 -2.8509974 -2.0791407 -1.6178937 -0.86599779 -0.18746185 0.44414997 0.94839668 1.7626572 1.4898605 0.36469173 -0.56525135 -2.6854992 -4.3465519 -5.6830397][-2.2741876 -1.4565244 -0.55711985 -0.072770119 0.83496 1.8623209 3.0151834 3.714426 4.267025 3.708663 1.8527145 0.45617962 -1.5159688 -3.3354816 -4.8195362][-1.3630214 -1.2859268 -0.44315243 0.42940712 1.556654 2.66004 4.2462206 5.1691875 5.6335449 4.6998329 2.0923395 0.49757385 -2.0823798 -3.3237138 -4.5959377][-1.0434942 -0.28008461 0.29785204 1.593605 2.6169853 3.3237591 4.3712339 5.1333818 5.9147243 4.6974096 1.9581642 -0.0029678345 -2.48484 -4.11164 -5.4401503][-1.3644547 -0.76028252 -0.24170208 0.82270241 1.2069778 1.8046656 2.2943068 2.8394623 3.4050713 2.3814268 0.52016544 -1.2325087 -3.4230223 -4.8343897 -6.1917][-4.029758 -2.9529777 -1.8180223 -1.6739316 -1.8347764 -1.4870324 -1.1100125 -0.73569632 -0.62306833 -0.89544916 -1.8819242 -2.9772606 -4.4790916 -6.3401995 -7.35113][-6.1299448 -4.8511891 -3.9159291 -3.900321 -3.9225571 -3.747045 -3.6226964 -3.358623 -3.2414804 -3.4265428 -4.3144388 -4.5806203 -5.0456409 -6.4462109 -7.101757][-6.4030809 -5.7403493 -4.7663178 -4.623982 -4.81946 -4.9158111 -4.6509686 -4.818778 -5.0155406 -5.1667938 -5.3765254 -5.1345015 -5.2220707 -5.761591 -5.8059936][-5.1341581 -5.0368547 -4.7471371 -4.1762075 -4.0211725 -4.2470851 -4.3522172 -4.7442493 -4.6706638 -4.6040354 -4.7700825 -4.4347715 -4.4205885 -4.6080818 -4.6605606][-3.8647695 -3.7169516 -4.3332734 -3.8547473 -4.0603094 -3.9320626 -3.9051321 -4.132452 -4.4312329 -4.4685497 -4.1725588 -4.5089879 -4.8673468 -4.7772512 -4.7447104]]...]
INFO - root - 2017-12-15 16:48:11.798301: step 36410, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 52h:31m:06s remains)
INFO - root - 2017-12-15 16:48:18.292878: step 36420, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 52h:36m:11s remains)
INFO - root - 2017-12-15 16:48:24.745628: step 36430, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 53h:46m:33s remains)
INFO - root - 2017-12-15 16:48:31.286806: step 36440, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.671 sec/batch; 55h:10m:39s remains)
INFO - root - 2017-12-15 16:48:37.693650: step 36450, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 53h:32m:11s remains)
INFO - root - 2017-12-15 16:48:44.194128: step 36460, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.629 sec/batch; 51h:45m:48s remains)
INFO - root - 2017-12-15 16:48:50.574340: step 36470, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:06m:36s remains)
INFO - root - 2017-12-15 16:48:57.014407: step 36480, loss = 0.34, batch loss = 0.22 (12.2 examples/sec; 0.658 sec/batch; 54h:04m:26s remains)
INFO - root - 2017-12-15 16:49:03.419937: step 36490, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:23m:49s remains)
INFO - root - 2017-12-15 16:49:09.907748: step 36500, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 54h:01m:27s remains)
2017-12-15 16:49:10.406916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3117485 -3.354784 -3.5087986 -3.6133857 -4.0629864 -4.171 -4.2266669 -4.0046754 -4.077045 -4.9757328 -4.5621548 -5.6026735 -6.2140303 -6.9292831 -7.5776577][-2.8918014 -3.11904 -3.6709714 -4.6774421 -5.2675791 -5.29401 -5.1636467 -5.3236356 -5.3531914 -6.1437607 -5.9506578 -6.9560905 -7.177907 -7.9691586 -8.0539579][-2.3692017 -2.360579 -2.6913152 -3.2423038 -3.8404317 -4.0412455 -4.2172112 -4.3116713 -4.5933814 -5.94154 -6.0490084 -7.0110788 -6.7192273 -7.3587837 -7.5198741][-0.11425781 -0.72083712 -1.6023612 -2.2502952 -2.8481531 -2.7275257 -2.5893564 -2.8801117 -3.2812862 -4.7114468 -4.9088688 -6.5431828 -7.1573768 -7.8480525 -8.1807661][-0.49627352 0.026723385 0.18000078 -0.7205081 -1.2141004 -1.3292704 -1.3212833 -1.0802946 -1.0748296 -2.4466534 -2.7894783 -4.765151 -5.5175066 -6.7895074 -7.4686265][-2.3562665 -1.5372314 -0.97643757 -0.77881813 -0.46908379 -0.0844717 0.46555328 0.95643044 1.3782396 0.11832571 -0.61725187 -2.3373632 -3.5318518 -5.34225 -6.2501855][-3.3449435 -2.8391013 -2.0865021 -0.7662096 0.51259136 1.0548353 1.7039194 2.1199036 2.5422297 1.3530521 0.92894077 -0.58860064 -1.8928432 -3.439959 -4.5999088][-3.5948873 -3.1837096 -2.8247533 -1.3264031 -0.081070423 1.6943197 3.3955793 3.5873432 3.8064947 2.3614855 1.8305368 0.033341885 -1.1521788 -2.3876972 -3.3293209][-4.4912105 -3.7058399 -3.0870633 -1.6745439 -0.40277195 1.5288115 3.0851669 3.615428 4.2371044 2.0179644 0.46304798 -1.2988787 -2.4845505 -3.7517705 -4.4704037][-4.9014888 -4.2040291 -3.6899548 -2.5572343 -1.457056 -0.16531658 0.94253254 1.7310143 2.411067 0.12958145 -1.3211579 -3.3293562 -5.0128951 -5.7042904 -6.2076588][-5.5603619 -5.1882167 -4.5212975 -3.7267456 -2.9386601 -1.8070855 -0.99322367 -0.60110807 -0.15990067 -2.1688576 -2.7945662 -4.5124397 -5.4700704 -5.9831533 -6.8897986][-6.4585595 -5.9258876 -5.3264685 -4.564971 -3.9239271 -3.286797 -3.0482039 -2.8576951 -2.5737953 -3.6800284 -4.2920938 -5.869277 -6.1577911 -5.7620597 -6.5424881][-6.71014 -6.3350029 -5.8442278 -4.9497581 -4.5208006 -4.2886152 -4.126668 -4.5611119 -4.6917429 -5.3568845 -5.7585111 -6.5722427 -6.7569151 -6.8728805 -6.9637246][-6.6563129 -6.1780224 -6.2878342 -5.6282907 -5.1758432 -4.8198614 -4.6505961 -4.9214835 -5.1261959 -5.8718133 -6.2164507 -6.6453233 -6.774497 -6.4148607 -6.442255][-7.6474733 -7.0421948 -7.0233378 -6.5829167 -6.1410131 -5.8708253 -5.8123894 -6.2017589 -6.5589666 -6.9950175 -7.3010635 -7.3289146 -7.2886748 -6.0565815 -5.6454811]]...]
INFO - root - 2017-12-15 16:49:16.854905: step 36510, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 53h:36m:59s remains)
INFO - root - 2017-12-15 16:49:23.339741: step 36520, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 52h:29m:37s remains)
INFO - root - 2017-12-15 16:49:29.762058: step 36530, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 54h:24m:49s remains)
INFO - root - 2017-12-15 16:49:36.191314: step 36540, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 52h:54m:15s remains)
INFO - root - 2017-12-15 16:49:42.584774: step 36550, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 52h:40m:07s remains)
INFO - root - 2017-12-15 16:49:49.007508: step 36560, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 54h:13m:34s remains)
INFO - root - 2017-12-15 16:49:55.404453: step 36570, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 51h:45m:48s remains)
INFO - root - 2017-12-15 16:50:01.946666: step 36580, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 54h:51m:16s remains)
INFO - root - 2017-12-15 16:50:08.342801: step 36590, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 53h:19m:07s remains)
INFO - root - 2017-12-15 16:50:14.718261: step 36600, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 52h:51m:42s remains)
2017-12-15 16:50:15.233834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6146564 -5.1442 -4.6446028 -4.4016209 -4.7397714 -4.9865141 -5.3658581 -4.6287608 -3.8282661 -5.2732964 -6.3356524 -7.7454906 -8.65635 -9.5045462 -10.571654][-6.3044758 -6.6657553 -6.5614614 -6.054184 -5.793756 -5.9537854 -5.98754 -5.9407873 -5.2053137 -6.1622219 -6.4223385 -8.3724766 -9.2843809 -9.5127478 -10.480698][-6.1851716 -5.6003175 -5.1773825 -5.35501 -5.5947576 -5.1359959 -4.7676148 -4.6041274 -4.1217461 -5.5925589 -6.392971 -7.817112 -8.10058 -8.2550812 -8.699873][-5.1473584 -4.9291077 -4.5758963 -4.0017576 -3.2876973 -2.8828979 -2.516284 -2.6405025 -2.5077872 -3.6001062 -4.1993971 -5.6219015 -6.6943264 -7.6826124 -8.5876646][-4.4221687 -3.9535859 -3.0905695 -2.4281092 -2.006536 -1.4063025 -0.25500393 0.054380894 0.58877468 -1.466167 -2.7806602 -4.3118191 -5.0334039 -5.4004993 -6.2620773][-4.1270714 -3.355341 -2.6249676 -1.0954442 0.25178623 1.3323069 2.6284637 2.5904799 2.8103838 0.57746029 -1.2397346 -3.031426 -4.1448722 -4.7294111 -5.5266061][-4.871933 -4.2495093 -3.4190907 -2.0540428 -0.30273438 1.7023449 3.3300734 3.1610088 3.4633255 1.3488417 -0.39650679 -2.3378229 -3.931278 -5.0445681 -6.3299842][-4.4877024 -4.0029926 -2.8244028 -1.1602268 0.73790455 2.2630424 3.741272 4.320365 4.5315542 2.00356 0.14104033 -2.0434918 -3.673327 -4.8572164 -6.12037][-4.1991906 -3.6024704 -3.1814766 -1.3991404 0.051082134 1.2890205 2.8622465 3.9049244 4.4627628 2.0165653 0.25461721 -1.8027968 -3.4522066 -4.7186432 -6.3370614][-4.5433292 -4.0243788 -2.8353496 -1.5580273 -0.49866772 0.45901012 1.6704082 1.8214884 1.713685 -0.021041393 -1.5815811 -3.3897238 -4.8539305 -5.3435869 -6.7861338][-4.965354 -4.4913645 -4.01478 -3.0573907 -2.2735782 -0.644032 0.24850225 0.10395432 0.1044035 -1.4695206 -2.5619659 -3.6294703 -4.7370567 -5.3788829 -6.5182924][-6.2614279 -4.9056163 -3.7212591 -3.295495 -3.6058788 -2.8149009 -2.7210827 -2.3603048 -1.6534338 -3.0212393 -3.7218719 -4.5328455 -5.4535222 -6.0674744 -5.9790807][-5.1104279 -4.89999 -4.5517645 -4.5440068 -4.6010437 -3.9844866 -3.5786562 -3.6173391 -3.9226513 -5.0958033 -5.0230656 -5.0772734 -6.2092233 -7.0896573 -7.1794276][-5.9375038 -5.0878325 -4.6559448 -4.2662344 -4.2463064 -3.9505033 -4.2031527 -4.3526154 -4.541512 -5.3862052 -5.4291153 -6.2556095 -6.86964 -6.195909 -5.6832609][-7.7966852 -6.9001026 -6.1898928 -5.5494862 -5.0203223 -4.737123 -4.8696814 -5.3646526 -5.8267803 -5.5729327 -5.7173648 -6.1874552 -6.6454897 -6.6740212 -6.5193233]]...]
INFO - root - 2017-12-15 16:50:21.580210: step 36610, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 51h:48m:34s remains)
INFO - root - 2017-12-15 16:50:28.006091: step 36620, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 52h:25m:24s remains)
INFO - root - 2017-12-15 16:50:34.439972: step 36630, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 52h:44m:20s remains)
INFO - root - 2017-12-15 16:50:40.927769: step 36640, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 52h:59m:53s remains)
INFO - root - 2017-12-15 16:50:47.336855: step 36650, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 52h:06m:32s remains)
INFO - root - 2017-12-15 16:50:53.743436: step 36660, loss = 0.30, batch loss = 0.19 (11.8 examples/sec; 0.675 sec/batch; 55h:29m:36s remains)
INFO - root - 2017-12-15 16:51:00.230829: step 36670, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 52h:21m:31s remains)
INFO - root - 2017-12-15 16:51:06.567239: step 36680, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 52h:25m:31s remains)
INFO - root - 2017-12-15 16:51:13.127418: step 36690, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 54h:53m:55s remains)
INFO - root - 2017-12-15 16:51:19.570103: step 36700, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 53h:19m:09s remains)
2017-12-15 16:51:20.133924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5129585 -6.5674734 -6.37967 -6.0868583 -5.7999029 -5.3969245 -4.8370705 -4.3040085 -3.8280122 -3.7352631 -3.4684768 -4.7895489 -5.7648573 -6.560617 -7.6374888][-5.3692365 -5.5041623 -5.6714044 -6.3953533 -6.6891432 -6.4431219 -6.3139238 -5.2907734 -3.8551641 -3.5535927 -3.7252872 -5.2825384 -6.8222585 -7.9087524 -8.7475958][-3.4430265 -3.476068 -4.1149006 -4.6336584 -5.5290437 -5.7476716 -5.295682 -4.5869045 -3.2214346 -2.8937507 -3.0070968 -4.9245052 -6.8437071 -8.1582031 -9.2686348][-1.1945114 -1.2572713 -2.307241 -2.9374571 -3.6722989 -4.0039959 -3.605329 -3.0361457 -1.6830769 -1.5669169 -2.1446033 -4.5587444 -6.4797268 -7.9103861 -8.9701414][-0.871428 0.0097384453 -0.15083408 -0.6339941 -1.4974236 -1.8970904 -1.3426847 -0.886919 -0.2098279 -0.31413555 -1.2798586 -4.1058421 -6.3785381 -7.8991647 -9.1845055][-0.29448605 0.23444748 0.26089287 0.24065161 0.028004169 0.42405891 1.0954494 1.5068798 1.828413 0.6113081 -1.2028408 -4.0389619 -6.0144939 -7.4995036 -8.5443487][-2.5794663 -1.8473125 -1.2633972 -0.19355106 0.90008545 1.866888 2.9673319 3.3731842 3.5630856 1.4846573 -0.93896866 -4.2690811 -6.8235073 -7.9106874 -8.6597643][-3.619782 -3.0943398 -2.4303012 -0.95964241 0.30550909 1.7853603 3.2011232 4.0296087 4.2022104 2.0076418 -0.26426077 -3.9912384 -6.3975391 -7.35473 -8.0756445][-5.3002534 -4.4209394 -4.1831441 -2.95716 -1.5122619 0.068822384 1.186264 1.623806 1.8716459 0.16312504 -1.684473 -4.6417665 -6.8590846 -7.65842 -8.0062857][-6.5691128 -6.3472676 -5.6872706 -4.8312321 -3.7467921 -2.1756659 -0.989871 -0.71353483 -0.63037491 -2.5280242 -3.9885855 -6.4221821 -7.8347988 -8.11804 -8.5688677][-9.6374149 -9.7013369 -8.8130474 -7.6666131 -6.7817645 -5.2372875 -4.2941332 -4.1096196 -3.7806709 -4.957962 -5.8959751 -7.659534 -8.4969044 -8.7279224 -8.8224707][-9.8785772 -9.5821438 -9.29653 -8.7376347 -8.0646639 -6.8988323 -6.4107533 -5.9335184 -5.3249569 -6.18577 -6.4740863 -7.8313227 -8.4051342 -7.9955053 -7.8119726][-9.8855371 -10.109962 -9.7110443 -9.1449509 -8.5862045 -7.8586283 -7.1660423 -7.2053237 -7.2624612 -7.8599739 -8.1746025 -8.8960094 -8.8555365 -8.2616825 -7.6223884][-8.9424458 -8.8029671 -8.4248323 -7.7719722 -7.1539969 -6.4473553 -6.3622479 -6.2712193 -6.2224274 -6.9835916 -7.2468596 -7.6191678 -7.9090934 -7.5669618 -7.1708946][-10.057206 -9.8997841 -9.5702143 -8.6351032 -8.2298517 -7.7345991 -7.1726227 -7.1881695 -7.2829351 -7.0404243 -7.1681857 -7.2364774 -7.3667026 -7.1181355 -6.7522311]]...]
INFO - root - 2017-12-15 16:51:26.609960: step 36710, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 51h:37m:19s remains)
INFO - root - 2017-12-15 16:51:32.932664: step 36720, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 51h:45m:15s remains)
INFO - root - 2017-12-15 16:51:39.366870: step 36730, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 51h:59m:21s remains)
INFO - root - 2017-12-15 16:51:45.783216: step 36740, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 51h:31m:36s remains)
INFO - root - 2017-12-15 16:51:52.243442: step 36750, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 51h:40m:09s remains)
INFO - root - 2017-12-15 16:51:58.597208: step 36760, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 51h:33m:29s remains)
INFO - root - 2017-12-15 16:52:05.003621: step 36770, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 52h:37m:48s remains)
INFO - root - 2017-12-15 16:52:11.427243: step 36780, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 51h:54m:38s remains)
INFO - root - 2017-12-15 16:52:17.795090: step 36790, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 52h:02m:12s remains)
INFO - root - 2017-12-15 16:52:24.185044: step 36800, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 53h:26m:59s remains)
2017-12-15 16:52:24.699457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.488337 -4.6464839 -4.7399693 -5.0635929 -4.9586725 -5.2696991 -4.3231087 -2.9507289 -3.1406121 -3.1632481 -4.6828132 -4.3261518 -6.0109358 -4.6373563 -4.850668][-4.8436904 -3.9474611 -2.6766925 -2.6928253 -3.6939313 -3.9039345 -4.6167212 -4.5074453 -3.0463238 -1.7902732 -3.475265 -3.9581494 -7.6270823 -4.8077431 -4.1080322][-2.4136944 -2.8571353 -2.4494133 -1.4981894 -2.3539481 -2.1147275 -1.6861525 -0.72164774 -1.6533852 -2.0830536 -3.3635526 -1.9312325 -5.8999958 -5.0851297 -5.3150148][-0.18040609 -0.35124683 -2.3884845 -1.8506074 -0.78030777 -0.89317179 -1.6705937 -1.7383456 -0.022691727 0.16514635 -2.2189727 -2.9293036 -5.3562469 -4.1229839 -4.0005312][-5.7700763 -2.0451794 0.56801987 0.098003864 -0.86588764 -1.0580449 -0.47294903 -0.99501467 -0.50950384 -1.042696 -2.7156215 -3.2811794 -5.7375388 -5.6155367 -4.9632215][-2.4995775 -2.2609258 -2.2459998 -0.099971294 2.8745441 3.5205641 2.7116098 1.3924198 1.4910469 0.79750347 -1.3249669 -2.8507352 -5.4490891 -4.5872397 -4.4734063][-2.4937806 -0.43019485 0.96357918 0.81300735 0.63830471 2.0823917 4.0029268 4.2341709 3.939435 1.8143244 -0.96537304 -2.1124787 -5.5214009 -4.9517469 -4.4893975][-0.60600328 -0.60927677 -0.05721283 2.3694239 3.0147123 2.5180635 2.316493 2.8734426 3.6677008 2.2673063 -1.2202845 -2.4907804 -5.9329662 -5.3494663 -5.5051355][-1.4457374 -0.4527936 -0.23463774 1.1546431 2.6116753 3.5837851 4.0945063 2.9647942 2.6025677 1.8347769 -0.20910835 -0.99833727 -4.6756196 -4.27638 -4.90296][-4.178853 -3.6650243 -2.1036973 -0.42952728 0.27787733 0.22011852 0.90011406 1.2157574 2.2721167 1.1785679 -1.052403 -1.266223 -3.8987236 -3.3122859 -3.8525569][-6.2012229 -5.4163795 -4.9776068 -4.8176765 -3.665647 -3.2535343 -2.51787 -2.6714082 -2.3393593 -1.8905292 -3.4933672 -3.3127241 -5.7610278 -5.3449411 -4.9080539][-7.8621597 -6.8763862 -6.4339962 -6.2362504 -5.80849 -6.1322074 -5.7071071 -4.5061116 -4.3415532 -3.5683303 -5.0033255 -4.9506884 -6.1638985 -4.9798408 -5.5853844][-6.2132506 -7.8568635 -8.3590069 -8.4874754 -7.1507139 -6.2855005 -5.6533747 -5.8757629 -5.4175997 -4.7038317 -6.0939198 -5.9654522 -5.3971815 -5.43361 -5.6362858][-6.2631669 -5.69855 -5.4634285 -6.9012904 -6.9700861 -6.6312819 -5.9459114 -5.5507374 -5.729291 -6.3186436 -6.0552163 -5.2147436 -5.5618086 -5.6782532 -5.9731956][-4.9549026 -5.4882774 -5.7342005 -5.8258696 -5.3618164 -6.2553134 -6.2980909 -6.3373284 -6.1117811 -5.9039207 -5.589263 -6.6414342 -7.4602251 -6.5016265 -5.9268041]]...]
INFO - root - 2017-12-15 16:52:31.176483: step 36810, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 53h:45m:37s remains)
INFO - root - 2017-12-15 16:52:37.669154: step 36820, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 52h:58m:28s remains)
INFO - root - 2017-12-15 16:52:44.098779: step 36830, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 53h:03m:22s remains)
INFO - root - 2017-12-15 16:52:50.487021: step 36840, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 52h:38m:19s remains)
INFO - root - 2017-12-15 16:52:56.966041: step 36850, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:17m:15s remains)
INFO - root - 2017-12-15 16:53:03.422824: step 36860, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 53h:20m:34s remains)
INFO - root - 2017-12-15 16:53:09.928439: step 36870, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 52h:15m:53s remains)
INFO - root - 2017-12-15 16:53:16.397325: step 36880, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 55h:17m:29s remains)
INFO - root - 2017-12-15 16:53:22.931388: step 36890, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 53h:46m:12s remains)
INFO - root - 2017-12-15 16:53:29.361317: step 36900, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 52h:00m:33s remains)
2017-12-15 16:53:29.884595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2169824 -4.8277888 -5.5467186 -5.9864488 -5.76762 -5.5226984 -5.034852 -4.503747 -4.2438555 -5.14099 -5.3658514 -5.7814245 -5.9629507 -6.2720189 -6.4617958][-3.7473276 -4.6616774 -5.7333021 -6.2168965 -6.2622032 -5.5818706 -5.11915 -5.2112494 -5.0750937 -6.2182074 -6.7943792 -7.160203 -7.0969396 -7.4177561 -7.7459269][-4.3471484 -4.5521407 -5.4538107 -6.1967325 -6.1327662 -5.5741944 -5.1818848 -5.0931811 -5.1676388 -6.4715524 -7.1005263 -7.6583471 -7.4066329 -7.6511407 -7.809258][-4.3741169 -4.6607771 -5.1110344 -5.4377103 -5.54882 -4.6784563 -3.8812754 -3.6684093 -3.7125711 -4.9319525 -5.9320345 -7.0361376 -7.547586 -7.7992492 -7.7137003][-4.3285055 -4.0940237 -3.4262013 -2.5965877 -1.9364214 -1.6152253 -1.8693757 -2.5136967 -3.0525303 -4.6365805 -5.8083296 -6.7813926 -7.1121173 -7.3662972 -7.7497463][-4.5668893 -4.2295008 -3.6534448 -1.3289752 0.50408936 1.3657684 1.632906 0.22170115 -1.372283 -3.7843907 -5.4464874 -6.3172407 -6.6017213 -6.6504965 -6.8034554][-5.2084427 -4.158154 -2.9616437 -0.713583 1.8761978 4.1196957 4.8510923 3.8600454 1.7430191 -2.0272813 -4.6250095 -6.1064477 -6.5299749 -6.4069166 -6.3191323][-5.2142363 -3.8693483 -1.9152803 0.70809841 3.3115463 5.1347923 5.3787022 4.6879196 3.6209173 -0.076357365 -2.964962 -5.1747165 -6.2695255 -6.5483942 -6.4840746][-5.9692545 -4.6770287 -3.0885129 -0.40704107 1.3570442 2.9467726 4.0559368 3.603056 2.8700638 0.11658907 -2.5302415 -4.4093032 -5.3945856 -6.3698335 -6.819046][-6.5074763 -5.8440132 -4.5938911 -2.6416817 -1.062427 0.64199924 1.5240488 1.6431398 1.3644037 -1.3377528 -3.4537663 -5.3756065 -6.1261039 -6.8347058 -7.5506096][-6.8293867 -6.2522411 -6.0247674 -4.9376135 -3.7143302 -3.0995331 -2.7272916 -2.5862627 -2.6136551 -4.6648016 -6.1408443 -7.2754288 -7.9029517 -8.1222124 -8.1977835][-7.5301113 -6.8202748 -6.1712294 -6.0219703 -6.0980682 -5.5409966 -5.2912636 -5.4634986 -5.33808 -6.0333204 -6.6810503 -7.8014073 -8.3145752 -8.8799677 -8.9568148][-8.0945072 -7.2325358 -6.6052742 -6.3120093 -6.6488953 -6.66913 -6.4560676 -6.7831926 -6.8542743 -7.1307292 -6.9429455 -7.5567837 -8.0341015 -8.5623732 -8.9759836][-7.4245315 -6.6349688 -5.70358 -5.4966822 -5.749589 -5.8331127 -6.0819869 -6.7198348 -6.76515 -6.6439686 -6.1447811 -5.865653 -6.2051311 -6.8399515 -7.1358681][-6.775003 -6.8071051 -6.2364564 -5.5889978 -5.3178253 -5.5119162 -6.2138352 -6.78857 -7.325974 -7.1320381 -6.513526 -5.8713245 -5.36485 -5.6173782 -6.28523]]...]
INFO - root - 2017-12-15 16:53:36.306273: step 36910, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 52h:09m:35s remains)
INFO - root - 2017-12-15 16:53:42.761900: step 36920, loss = 0.30, batch loss = 0.18 (11.3 examples/sec; 0.706 sec/batch; 57h:59m:27s remains)
INFO - root - 2017-12-15 16:53:49.192502: step 36930, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 52h:54m:10s remains)
INFO - root - 2017-12-15 16:53:55.607808: step 36940, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 52h:16m:57s remains)
INFO - root - 2017-12-15 16:54:02.010150: step 36950, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:12m:22s remains)
INFO - root - 2017-12-15 16:54:08.433923: step 36960, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 52h:17m:29s remains)
INFO - root - 2017-12-15 16:54:14.836126: step 36970, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 51h:00m:16s remains)
INFO - root - 2017-12-15 16:54:21.195814: step 36980, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 51h:33m:41s remains)
INFO - root - 2017-12-15 16:54:27.511099: step 36990, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 50h:36m:02s remains)
INFO - root - 2017-12-15 16:54:33.890187: step 37000, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 53h:48m:49s remains)
2017-12-15 16:54:34.395944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3140955 -7.4826741 -7.7702 -7.7043033 -7.4154897 -6.9255486 -5.9688811 -4.566206 -3.0105705 -2.3337626 -3.2020392 -3.8055215 -4.212842 -5.384902 -6.1381769][-6.1097684 -6.3157606 -6.5428524 -7.0938015 -7.3662076 -7.5529041 -7.1207142 -5.8172593 -4.0067329 -3.2989306 -4.0155225 -4.8496284 -4.90303 -5.4595609 -5.8355556][-5.4775143 -6.0310793 -6.7099304 -7.287221 -7.6091638 -8.0331888 -7.9954481 -7.3038154 -5.9967456 -4.9449606 -5.24767 -6.1011744 -6.702476 -6.8790836 -6.73752][-6.516469 -6.6393385 -5.9357314 -5.9556928 -6.7470121 -6.7484951 -6.3215666 -5.6291623 -4.5637674 -4.8351994 -6.5402279 -7.2938809 -7.5327458 -8.3231068 -8.4333744][-6.52796 -5.361764 -4.1831131 -3.6964521 -3.5450749 -2.7201886 -1.7831192 -1.4002967 -1.0771208 -2.0639024 -4.1109591 -6.2219458 -7.2941413 -8.1949987 -8.4332066][-4.7857833 -3.8964524 -2.3625889 -0.85546684 0.044192314 1.0316257 1.6833038 1.9516735 2.7161398 1.7244177 -0.85391521 -3.4517941 -4.9866443 -6.366859 -7.6013556][-4.7419567 -2.9977231 -1.0278711 0.49790382 2.1602497 3.2839642 3.5281572 3.8794203 4.3755016 2.796258 0.12209797 -1.5612183 -2.8672996 -4.7699628 -6.1133285][-3.6425023 -2.9879022 -2.0149999 0.099133968 2.1872349 3.4267387 4.3631754 4.3966713 3.7878666 2.7060642 0.73217487 -1.8617697 -3.0700665 -4.3096657 -5.6556759][-4.7105522 -3.559803 -2.1058888 -0.16026831 1.2720814 1.6724892 2.1891985 2.9304628 3.512435 1.8403463 -0.8962636 -2.2028866 -3.0239215 -4.6595588 -5.7553811][-5.382206 -4.97153 -4.2212677 -2.5564885 -0.98641491 0.31028175 0.84570408 0.84255314 0.6918354 -0.34492779 -2.3388767 -3.5614676 -4.7449265 -6.0037889 -6.3611336][-8.0586653 -7.6600552 -6.9761291 -6.4441218 -5.4939137 -3.6671314 -2.5452223 -2.5458775 -2.9730744 -3.6701741 -4.9460907 -6.0921745 -6.8420973 -8.0281429 -8.541666][-8.8078709 -9.1350384 -8.8255081 -7.9701 -7.3083105 -7.2056189 -6.7484608 -5.4308496 -4.7807174 -5.8793111 -6.5766573 -6.5761147 -7.2746587 -8.3342237 -8.8375492][-9.9238691 -10.242056 -9.5690918 -8.6978779 -7.9202948 -7.3603711 -7.1914759 -7.8817325 -7.6507373 -7.2216287 -7.3844566 -7.4237909 -6.9974442 -6.7237225 -6.8823366][-8.4611578 -9.0021553 -9.0971432 -8.6818924 -8.001524 -7.2658882 -6.7081776 -6.6859784 -7.1575713 -7.3527107 -7.494391 -7.3163695 -7.1224532 -6.9475131 -6.6385527][-6.4267864 -6.6532516 -6.9523387 -7.4660244 -7.5750117 -7.3877769 -7.3309112 -7.3674583 -6.9084215 -7.0315323 -7.5836873 -7.6467304 -7.5912075 -7.724112 -7.296464]]...]
INFO - root - 2017-12-15 16:54:40.877830: step 37010, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 51h:10m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 16:54:47.348253: step 37020, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.676 sec/batch; 55h:29m:53s remains)
INFO - root - 2017-12-15 16:54:53.776620: step 37030, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 52h:53m:42s remains)
INFO - root - 2017-12-15 16:55:00.246531: step 37040, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 53h:54m:04s remains)
INFO - root - 2017-12-15 16:55:06.837224: step 37050, loss = 0.36, batch loss = 0.25 (11.8 examples/sec; 0.677 sec/batch; 55h:31m:55s remains)
INFO - root - 2017-12-15 16:55:13.401563: step 37060, loss = 0.33, batch loss = 0.22 (12.0 examples/sec; 0.667 sec/batch; 54h:45m:59s remains)
INFO - root - 2017-12-15 16:55:19.886970: step 37070, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 51h:57m:30s remains)
INFO - root - 2017-12-15 16:55:26.347702: step 37080, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 55h:20m:53s remains)
INFO - root - 2017-12-15 16:55:32.845803: step 37090, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 52h:52m:01s remains)
INFO - root - 2017-12-15 16:55:39.257908: step 37100, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 52h:23m:02s remains)
2017-12-15 16:55:39.760415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1193175 -2.8537645 -2.8506613 -2.9079165 -2.8857918 -2.8476858 -2.6124945 -2.1969404 -1.7732773 -2.6563468 -3.493176 -4.5395384 -5.2843685 -6.0920935 -6.7171612][-2.7105575 -2.7429395 -2.9305577 -3.0144119 -2.9284825 -2.8473759 -2.6130781 -2.3427987 -1.9191442 -2.7267189 -3.5128751 -4.7088623 -5.8685031 -6.7489791 -7.3415842][-2.2549891 -2.3123136 -2.4992743 -2.5201855 -2.2888222 -2.0052166 -1.6837139 -1.4979773 -1.2851024 -2.3051085 -3.17658 -4.4300718 -5.4745684 -6.2898011 -6.9633093][-2.3052778 -2.1701407 -2.2109532 -1.9180508 -1.4927826 -1.062326 -0.59316015 -0.45915222 -0.34249449 -1.5085421 -2.4817333 -3.8267519 -5.0143943 -6.0669994 -6.7663355][-2.9432573 -2.3524132 -1.6524582 -0.981123 -0.29339314 0.33751202 0.72920609 0.78626823 0.80251026 -0.61145878 -1.8198233 -3.2351756 -4.4625778 -5.6432619 -6.4783974][-3.8422465 -2.9048214 -1.9247303 -0.8528533 0.10367775 1.0741062 1.726882 1.6808739 1.6149445 -0.074252605 -1.561379 -3.1182342 -4.5278568 -5.6551542 -6.3881469][-4.3870859 -3.621707 -2.4118085 -0.91366768 0.48964119 1.6071844 2.1375618 2.1282625 1.9843645 0.047973633 -1.6011672 -3.2683926 -4.7121053 -5.8491235 -6.6067467][-3.9426754 -3.0275035 -1.9819474 -0.57999372 0.76614 1.7492657 2.1750946 2.0563793 1.8072977 -0.18814182 -1.9941854 -3.8200619 -5.2183123 -6.3328443 -6.9651723][-4.2235727 -3.3875246 -2.3871551 -1.1009274 0.17797089 1.0941381 1.41887 1.5714312 1.5267534 -0.52099991 -2.2898817 -4.0819988 -5.5759335 -6.8071389 -7.4046226][-4.1601067 -3.3601232 -2.6125798 -1.3515496 -0.3074131 0.13581753 0.48577785 0.68016911 0.64316368 -1.5244269 -3.0770769 -4.8947783 -6.1580496 -7.3870921 -8.2421741][-5.2757215 -4.6320963 -3.8132339 -2.9385028 -2.2425294 -1.866498 -1.6417432 -1.4933691 -1.4013748 -3.2981124 -4.768239 -6.2262096 -7.1217127 -8.0618982 -8.4741755][-5.6208324 -4.9401741 -4.3713255 -3.8263428 -3.3972254 -3.2879868 -3.249548 -3.1569424 -3.1026163 -4.3755293 -5.4547615 -6.6025524 -7.2808094 -8.0396948 -8.2986422][-6.6240516 -6.0964713 -5.6645746 -5.3013296 -4.9154892 -4.8830681 -4.9199982 -4.9364834 -4.8875351 -5.6499872 -6.4314203 -6.8808308 -7.3122339 -7.7656956 -7.6129217][-6.1798744 -5.72916 -5.5811944 -5.1171751 -4.781538 -4.9379086 -5.034945 -5.076313 -5.0371022 -5.6623359 -5.830821 -5.9135256 -6.095026 -6.4802046 -6.6074495][-7.5266356 -7.2683 -7.0015435 -6.6342845 -6.335537 -6.4348831 -6.614677 -6.7806473 -6.8421259 -6.7374654 -6.696702 -6.5494051 -6.2321844 -6.0748711 -5.9465475]]...]
INFO - root - 2017-12-15 16:55:46.098455: step 37110, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 51h:44m:13s remains)
INFO - root - 2017-12-15 16:55:52.465472: step 37120, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 51h:51m:38s remains)
INFO - root - 2017-12-15 16:55:58.866365: step 37130, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:14m:42s remains)
INFO - root - 2017-12-15 16:56:05.293861: step 37140, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 53h:18m:37s remains)
INFO - root - 2017-12-15 16:56:11.682497: step 37150, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 51h:46m:53s remains)
INFO - root - 2017-12-15 16:56:18.075099: step 37160, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 52h:31m:22s remains)
INFO - root - 2017-12-15 16:56:24.363247: step 37170, loss = 0.23, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 51h:26m:46s remains)
INFO - root - 2017-12-15 16:56:30.734712: step 37180, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 51h:56m:56s remains)
INFO - root - 2017-12-15 16:56:37.108487: step 37190, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 52h:46m:53s remains)
INFO - root - 2017-12-15 16:56:43.610443: step 37200, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 54h:44m:43s remains)
2017-12-15 16:56:44.096682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7381248 -1.9283051 -2.275619 -2.7316175 -2.9142447 -3.3492956 -3.6725574 -3.4660139 -2.9954047 -3.9620812 -4.8694744 -5.8621154 -6.51649 -6.7435341 -7.6503429][-2.4419851 -2.5052247 -2.9022422 -3.1666079 -3.5068798 -3.8880255 -4.2690344 -4.280117 -3.9057803 -5.1798744 -5.9302883 -6.9908834 -7.8057547 -8.0535927 -8.640027][-3.6986017 -3.2298455 -3.1990218 -3.1492324 -3.3156905 -3.7873559 -3.8603714 -4.0884395 -3.9753973 -5.482605 -6.5583649 -7.8528428 -8.7655649 -9.0757408 -9.6350508][-4.3742104 -3.89929 -3.5460777 -2.9670067 -2.4840665 -2.351047 -2.0016661 -1.9584966 -1.7130294 -3.2048078 -4.6986432 -6.2198811 -7.4581304 -8.1235752 -8.989809][-4.4577351 -3.9603331 -3.5245318 -2.8409953 -2.1053042 -1.1769118 -0.30437374 0.20340919 0.69093895 -0.484416 -1.8413377 -4.3692303 -6.0772948 -7.2356505 -8.3574848][-5.728219 -4.9915996 -4.5423341 -3.3225775 -2.0555382 -0.73442507 0.79485607 1.6056032 2.3099546 0.82752037 -0.56088495 -2.9411726 -5.0154004 -6.2675543 -8.0497847][-5.9225311 -5.0641694 -4.0938764 -2.6074505 -0.98934221 0.42553234 1.8745432 3.1341028 4.5399694 3.2120714 1.680728 -0.79513693 -3.2176871 -4.8364487 -6.6738305][-5.4570384 -4.3323288 -3.4262257 -1.7411394 0.16301489 1.9662914 3.52845 4.3766537 5.4796066 4.2704363 3.0922165 0.6881361 -1.5512567 -3.0135489 -4.9283919][-5.63503 -4.3772187 -3.4455447 -1.9433923 -0.45764923 1.0325613 2.7852087 3.4307194 3.8624706 1.9803925 0.99250317 -0.86476851 -2.1850457 -3.3379874 -5.1518793][-6.2129035 -5.1193953 -4.114491 -2.9330692 -1.5146294 -0.83893728 0.15562582 1.1745243 1.5643301 -1.0946908 -2.7643147 -4.3429451 -5.1527457 -5.5532985 -6.50634][-8.2712584 -7.0020394 -6.2838907 -4.8371129 -3.9046485 -2.7443738 -2.0246525 -1.7939053 -1.5937986 -3.2435026 -4.6603251 -6.2789106 -7.4832458 -7.9431262 -8.0770626][-9.0915947 -8.1994038 -7.2692041 -6.0738077 -4.8837767 -4.1496968 -3.5254316 -3.3684649 -3.1460109 -4.4348125 -5.2426825 -6.4883461 -7.5014873 -7.776619 -8.276103][-8.7981625 -8.3081865 -7.7289944 -6.7662764 -5.6602592 -4.935245 -4.255929 -3.9898047 -4.0210047 -5.0790625 -5.3724213 -6.2683554 -6.838 -7.0644112 -7.4091625][-8.8235416 -8.2113962 -7.7174506 -6.7052536 -5.82412 -4.7766991 -4.2980561 -4.2755013 -4.2848988 -4.8548126 -5.2370992 -5.5663891 -5.996191 -6.0346766 -6.2511177][-9.6920071 -9.4575691 -9.1556683 -8.0507107 -7.0571418 -6.0112925 -5.3737154 -5.2268267 -5.4051409 -5.6625557 -5.91157 -6.1223569 -6.0769081 -5.8826103 -5.8819566]]...]
INFO - root - 2017-12-15 16:56:50.420999: step 37210, loss = 0.33, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 51h:31m:32s remains)
INFO - root - 2017-12-15 16:56:56.863005: step 37220, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 52h:32m:17s remains)
INFO - root - 2017-12-15 16:57:03.197709: step 37230, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 52h:43m:29s remains)
INFO - root - 2017-12-15 16:57:09.646252: step 37240, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 54h:40m:29s remains)
INFO - root - 2017-12-15 16:57:16.028113: step 37250, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 52h:33m:27s remains)
INFO - root - 2017-12-15 16:57:22.474954: step 37260, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.613 sec/batch; 50h:15m:58s remains)
INFO - root - 2017-12-15 16:57:28.873705: step 37270, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 52h:17m:53s remains)
INFO - root - 2017-12-15 16:57:35.301503: step 37280, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 53h:39m:41s remains)
INFO - root - 2017-12-15 16:57:41.633766: step 37290, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:59m:54s remains)
INFO - root - 2017-12-15 16:57:47.996576: step 37300, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 52h:52m:19s remains)
2017-12-15 16:57:48.508758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9105701 -3.2685819 -3.1671352 -3.4273052 -3.3955107 -3.0601006 -2.6268797 -2.4100819 -2.163415 -2.8815408 -3.2193856 -4.2012925 -5.3626914 -6.468225 -7.3410892][-4.2909365 -4.5385356 -4.9474897 -4.9330397 -4.8129969 -4.9227548 -4.4990296 -3.6888576 -3.142663 -3.8141382 -4.1319838 -5.218605 -6.7104759 -7.3033738 -7.7664833][-4.718008 -4.321804 -4.2576017 -4.4993668 -4.6249318 -4.3905697 -3.5440674 -2.9465151 -2.4247217 -2.8980713 -3.2854967 -4.460485 -5.9106526 -6.9263549 -7.9383965][-4.9956083 -5.1323023 -4.949439 -3.9564912 -2.8365421 -2.0965939 -1.3956504 -1.0815625 -0.91187 -1.9491982 -2.7157173 -4.2684584 -5.7088037 -6.4814167 -7.2280879][-6.1957245 -5.2141361 -4.3764315 -3.5235662 -2.3760839 -1.045589 0.30549717 0.8157711 0.86985111 -0.63882875 -1.6329885 -3.4445634 -5.3104925 -6.5708857 -7.3930469][-6.4719305 -6.042048 -5.4630775 -3.8338051 -2.0509181 -0.20742273 1.491621 2.0654631 2.2120581 0.60948372 -0.58857489 -2.6138153 -4.8318481 -6.215096 -7.370575][-6.6783895 -5.8744678 -4.7799997 -3.0023885 -0.83113146 1.0884972 2.6805477 3.386344 3.487299 1.4900751 0.093419552 -2.1397586 -4.422945 -5.923152 -7.1192141][-6.206872 -5.2104425 -4.0610847 -1.9586396 0.27221346 2.3699846 3.7884293 4.2846804 4.2367029 1.9717321 0.46852875 -2.0641155 -4.4919348 -6.0048842 -6.9265308][-6.1086168 -5.318089 -4.4095659 -2.7587714 -1.1104112 0.53419971 1.9513369 2.354022 2.3807278 0.53467846 -0.77790642 -2.7570162 -4.8555036 -6.116004 -7.0969968][-6.0499959 -5.6839409 -4.9477997 -3.4690948 -1.7340498 -0.48265839 0.17849445 0.4077282 0.53040695 -1.0958505 -2.3282619 -4.1367373 -5.7758656 -6.6549511 -7.0386529][-6.7554502 -6.6347132 -6.305851 -5.2246227 -4.1848598 -2.7561054 -1.7821155 -1.8913145 -1.8976278 -3.2110233 -4.0854168 -5.3956337 -6.5748816 -7.2401557 -7.2471304][-8.41571 -7.9750986 -7.6213455 -6.9400229 -6.0828896 -5.0550318 -4.4158278 -4.1990862 -3.9455307 -4.7949176 -5.3766279 -6.105238 -6.8306727 -6.9901457 -6.6656494][-8.0169086 -8.1930008 -8.0224323 -7.1551981 -6.4395132 -5.89399 -5.6790466 -5.7399478 -5.7442822 -6.3710604 -6.7073555 -6.9236369 -7.2260747 -7.215035 -7.0532131][-7.8313422 -7.5344529 -7.4283752 -7.087822 -6.7612824 -6.3243756 -6.1235232 -6.3618851 -6.5693808 -7.1627059 -7.4272203 -7.5199838 -7.6032348 -7.5287991 -6.9941254][-8.4357109 -8.0622959 -7.4285913 -6.9832573 -6.6861148 -6.5947342 -6.5218053 -6.8271675 -7.1548624 -7.2122536 -7.3051367 -7.4903445 -7.5078759 -7.4277291 -6.9268122]]...]
INFO - root - 2017-12-15 16:57:54.949411: step 37310, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 53h:27m:37s remains)
INFO - root - 2017-12-15 16:58:01.342098: step 37320, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 51h:05m:30s remains)
INFO - root - 2017-12-15 16:58:07.669140: step 37330, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 52h:42m:20s remains)
INFO - root - 2017-12-15 16:58:14.115692: step 37340, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 51h:37m:12s remains)
INFO - root - 2017-12-15 16:58:20.488811: step 37350, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 52h:07m:06s remains)
INFO - root - 2017-12-15 16:58:26.873673: step 37360, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 53h:03m:17s remains)
INFO - root - 2017-12-15 16:58:33.243423: step 37370, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 51h:44m:12s remains)
INFO - root - 2017-12-15 16:58:39.709604: step 37380, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 55h:07m:18s remains)
INFO - root - 2017-12-15 16:58:46.069891: step 37390, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 51h:14m:32s remains)
INFO - root - 2017-12-15 16:58:52.458542: step 37400, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 51h:47m:59s remains)
2017-12-15 16:58:53.047797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1944366 -6.6885958 -6.8186159 -6.0887356 -5.7754269 -4.9721594 -4.1721268 -3.1755686 -2.0736661 -2.0505929 -3.5768766 -5.2160521 -7.0452752 -8.7647924 -9.2751045][-6.3765635 -7.2098708 -7.513123 -7.4940844 -7.10467 -6.4014287 -5.6908441 -4.34165 -3.0767765 -2.99191 -4.5669956 -6.5925016 -8.7647591 -10.251621 -9.8138466][-4.6845865 -5.5541544 -6.3181648 -6.1750894 -5.7260742 -5.0867314 -4.1396465 -3.1344328 -2.3887234 -2.6455126 -4.1383896 -6.0475669 -7.6242447 -8.4857988 -8.422348][-3.9194949 -3.9848688 -4.1058307 -3.5571203 -3.3651338 -2.5263214 -1.5483408 -1.0861621 -0.94009018 -1.7089739 -3.2869835 -5.1070347 -6.7033215 -7.23632 -7.2900543][-3.7766488 -2.5421467 -1.6678514 -1.2690969 -0.90151358 0.14464903 1.0578661 1.2150545 0.94048882 -0.13644505 -1.9623275 -4.2131157 -6.0209312 -6.8732285 -6.8845377][-2.9463186 -1.8750806 -0.78688669 -0.29130173 0.49670982 1.3767242 2.4462385 2.5022411 1.8172607 0.41678238 -1.7204876 -3.7652357 -5.5117664 -6.3454275 -6.0860119][-4.026309 -2.5027027 -1.0210018 0.081269264 1.1929121 2.4256105 3.4164352 3.4996996 3.000041 1.1688318 -1.6036878 -3.8650496 -5.7206869 -6.5587368 -6.3370829][-4.8657255 -3.6569219 -2.2431879 -0.82923222 0.68504524 1.8312845 2.9225368 3.2708626 3.0531931 1.1993027 -1.4149065 -3.920084 -5.6445317 -6.2658944 -6.0569434][-5.5916557 -4.4669738 -3.5563793 -2.2443652 -1.082871 0.19146347 1.1983967 1.8610439 1.8113232 0.37481785 -1.8803015 -4.2181382 -5.8349752 -6.35756 -5.8931475][-6.0967259 -5.6269827 -4.712121 -3.8537104 -2.7602487 -1.8721328 -0.79825687 0.10954952 0.18855858 -1.2041054 -3.1945782 -4.90676 -6.4030113 -6.7946887 -6.4947414][-7.5055485 -6.9207234 -6.5372071 -5.5251832 -4.7668896 -3.9608123 -2.9669552 -2.4883184 -2.3758044 -3.2667208 -4.4395809 -5.5223851 -6.2210126 -6.6127024 -6.5585823][-7.295815 -7.0446982 -6.4418564 -5.8739233 -5.4249134 -5.5600471 -5.2075996 -4.766468 -4.4037724 -4.7594051 -5.5716438 -5.6504869 -6.0878382 -6.3735313 -6.2350283][-7.6918931 -7.2261744 -7.0713243 -6.4607325 -5.9793882 -5.9885931 -6.0019288 -6.0747795 -5.9822006 -6.0734406 -6.4249763 -6.3242035 -6.519825 -6.1327419 -5.9379258][-6.6341276 -6.4019051 -6.419992 -6.1790009 -5.83098 -5.5547562 -5.4369564 -5.6930289 -5.7654681 -5.9603729 -6.1178069 -5.9742322 -5.980135 -6.058465 -6.0776668][-5.8511419 -5.9332433 -6.0209589 -6.1718969 -6.476562 -6.2345133 -5.9949522 -6.0898275 -5.86527 -5.97073 -5.9987926 -6.035183 -6.2876329 -6.5617 -6.4389563]]...]
INFO - root - 2017-12-15 16:58:59.512325: step 37410, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:58m:11s remains)
INFO - root - 2017-12-15 16:59:05.938768: step 37420, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 52h:33m:41s remains)
INFO - root - 2017-12-15 16:59:12.342413: step 37430, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 53h:00m:46s remains)
INFO - root - 2017-12-15 16:59:18.731842: step 37440, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 51h:39m:12s remains)
INFO - root - 2017-12-15 16:59:25.099271: step 37450, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 53h:03m:10s remains)
INFO - root - 2017-12-15 16:59:31.507423: step 37460, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 52h:34m:19s remains)
INFO - root - 2017-12-15 16:59:37.881388: step 37470, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 52h:16m:41s remains)
INFO - root - 2017-12-15 16:59:44.273189: step 37480, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 52h:23m:10s remains)
INFO - root - 2017-12-15 16:59:50.605559: step 37490, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 51h:31m:26s remains)
INFO - root - 2017-12-15 16:59:57.090479: step 37500, loss = 0.29, batch loss = 0.17 (11.8 examples/sec; 0.677 sec/batch; 55h:28m:30s remains)
2017-12-15 16:59:57.633914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.741919 -3.4173384 -3.5086746 -3.6700544 -3.2409091 -3.0260897 -2.5581503 -1.9862037 -1.4475274 -2.8242359 -3.7766459 -4.8081379 -5.8442225 -7.5273185 -8.2528486][-3.1772456 -3.037251 -3.1622229 -3.3198185 -3.5549107 -3.1592293 -2.6023684 -2.0800681 -1.564991 -2.647058 -3.3437753 -4.7264557 -5.8657169 -7.18152 -8.3101645][-2.5595975 -2.2062211 -2.3863654 -2.4822707 -2.6222291 -2.5346699 -2.1431131 -1.4416223 -0.78483582 -1.797111 -2.3714628 -3.671032 -5.0018253 -6.6269727 -7.4953132][-1.9142199 -1.452004 -1.5730419 -1.8970752 -1.9226141 -1.3202968 -1.0207996 -0.66603947 -0.5501523 -1.4044785 -2.1204424 -3.7931249 -4.9387655 -6.3345289 -7.4126587][-2.1339011 -1.3091717 -0.89453983 -0.77685976 -1.0149245 -0.56550407 0.19399261 0.37139702 0.47201538 -0.80307913 -1.7401142 -3.3900828 -4.7285175 -6.112083 -6.9962249][-2.4843473 -1.430593 -1.1221766 -0.97880745 -0.77802896 0.078440666 0.85956 0.91477585 0.99615383 -0.31799698 -1.2623806 -2.7967143 -3.8757896 -5.2406497 -6.2298265][-3.3622189 -2.3261366 -1.54351 -0.79558659 -0.23512125 0.68338203 1.4568806 1.5128641 1.4512548 -0.15584087 -1.3929815 -2.9888759 -4.2137804 -5.5369029 -6.4010811][-3.3178105 -2.368185 -2.0482216 -1.2423649 -0.46458435 0.52562618 1.5506811 1.4912453 1.5945005 0.099508762 -1.3087859 -2.7736435 -4.1050496 -5.4153419 -6.18524][-3.5021582 -2.3278761 -1.7270799 -0.82359648 -0.14505148 0.59713745 1.1208296 1.0637293 0.99819851 -0.45674515 -1.7861114 -3.1587 -4.2990723 -5.4788294 -6.051362][-3.7653708 -2.7222142 -1.6697373 -0.94287968 -0.25632572 0.31883621 0.99260235 0.70107174 0.39754772 -0.99811029 -2.3332891 -3.1405659 -3.9762125 -4.7956161 -5.428277][-5.0862031 -3.6408525 -2.7985878 -2.1802335 -1.6099958 -1.1407185 -0.63361788 -1.2334628 -1.3240747 -2.3088045 -2.9419198 -3.7550073 -4.1151266 -4.3237662 -4.6840205][-6.55122 -5.2866387 -4.27254 -3.6753001 -3.4511485 -2.8024 -2.2784119 -2.473022 -2.689743 -3.6487565 -3.6754436 -4.075634 -4.2499704 -4.4260364 -4.461637][-6.5690622 -5.9158773 -5.212575 -4.705811 -4.3537817 -3.6631165 -2.8676028 -3.2189922 -3.6603718 -4.2793717 -4.2721825 -4.3397942 -4.4173 -4.431962 -4.7462335][-7.0560508 -6.482017 -6.044188 -5.8811355 -5.3410797 -4.7797956 -4.2677417 -4.335124 -4.1520209 -4.4800682 -4.1930666 -4.4628582 -4.575808 -4.5384789 -4.5313735][-8.1824121 -7.2243996 -6.4929433 -6.46581 -6.5132947 -6.2442355 -5.8530817 -5.8318658 -5.7918859 -5.6876822 -5.4533024 -5.4312048 -5.5696039 -5.4887676 -5.3714695]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 17:00:05.184866: step 37510, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 51h:50m:42s remains)
INFO - root - 2017-12-15 17:00:11.610547: step 37520, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 52h:28m:51s remains)
INFO - root - 2017-12-15 17:00:18.029128: step 37530, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 53h:08m:35s remains)
INFO - root - 2017-12-15 17:00:24.395822: step 37540, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 51h:35m:30s remains)
INFO - root - 2017-12-15 17:00:30.658431: step 37550, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 50h:27m:21s remains)
INFO - root - 2017-12-15 17:00:37.071303: step 37560, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 54h:07m:10s remains)
INFO - root - 2017-12-15 17:00:43.513142: step 37570, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 51h:36m:12s remains)
INFO - root - 2017-12-15 17:00:50.013163: step 37580, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 52h:40m:54s remains)
INFO - root - 2017-12-15 17:00:56.448853: step 37590, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 53h:26m:05s remains)
INFO - root - 2017-12-15 17:01:02.779771: step 37600, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 52h:31m:25s remains)
2017-12-15 17:01:03.330737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1692734 -3.1849651 -3.1072412 -3.3907099 -3.4562039 -3.5700264 -3.7730379 -3.2570024 -2.8548408 -3.2801695 -3.90265 -4.9309349 -5.9179897 -5.9605107 -7.0117507][-1.8961582 -2.3758197 -3.2249618 -4.0069432 -4.3893337 -4.6381054 -4.3574123 -4.2002621 -4.109149 -4.434432 -4.8436451 -5.47225 -6.1408215 -6.2185478 -6.8438096][-1.5810733 -2.1301808 -2.6145005 -2.9512429 -3.1598692 -3.4260302 -3.7108912 -3.9412887 -4.2085495 -5.0181651 -5.228899 -5.9634171 -6.2572641 -6.1211128 -6.7792826][0.075851917 -0.41364241 -1.2748857 -1.9201088 -2.6021581 -2.9245052 -3.0682354 -3.3546705 -3.6311259 -4.4849081 -4.996871 -5.9149547 -6.8108091 -6.518405 -7.0309024][0.79906178 0.48892021 -0.14913559 -0.66755819 -0.99782705 -1.2086291 -1.5294652 -1.7119112 -1.7336493 -2.3549862 -2.6003513 -3.7922497 -5.1446981 -5.734849 -6.6453519][-0.69453096 -0.65878868 -0.68324375 -0.49851274 -0.27038193 0.12934971 0.48401737 0.62409115 0.85305691 0.11605692 -0.27503204 -1.6104879 -3.2057734 -4.0239391 -5.203383][-1.5830054 -1.1642137 -0.78406334 -0.27438211 0.4059124 1.0375462 1.933794 2.2525053 2.6018314 1.7319937 1.1946602 0.091817379 -1.5604606 -2.6955285 -4.1360254][-2.0156775 -1.8199406 -1.2454543 -0.46173334 0.63786316 1.6880379 2.8310194 3.4943285 4.0746984 3.2389584 2.705121 1.2448797 -0.61037064 -1.5105619 -2.8057399][-2.7842984 -2.2075095 -1.9643908 -1.0521069 0.066737175 1.4863224 3.10948 3.6429052 4.1000185 2.9390173 1.9898472 0.50051594 -1.4555478 -2.1315308 -3.3598361][-3.3464408 -2.7095618 -2.2958665 -1.4496856 -0.68674326 0.42016888 1.4750319 2.2299967 2.9350882 1.5174799 0.086269855 -2.0660014 -3.8579321 -4.0921631 -4.5661678][-4.664257 -4.1332483 -3.8018103 -2.9752331 -2.4029112 -1.6441789 -0.65859461 -0.12020874 0.51705551 -0.63003397 -1.8681407 -3.3670526 -4.939559 -5.1611323 -5.9459081][-5.8688717 -5.6723986 -5.2254548 -4.52149 -3.842253 -3.1174026 -2.7883563 -2.380919 -1.7241611 -2.7794509 -3.6239963 -4.6142163 -5.6297331 -5.8138781 -6.744688][-6.5647058 -6.2133718 -5.8228116 -5.1955 -4.5545759 -4.2124114 -3.8132997 -3.8043272 -3.8288374 -4.4978714 -5.0708094 -5.8375359 -6.7896295 -6.667284 -6.8260093][-7.4185 -7.1397109 -6.597178 -6.1133237 -5.5321331 -5.2952595 -4.9885492 -5.1331234 -4.9740839 -5.757412 -6.1545877 -6.68892 -6.8476291 -6.885704 -7.1639156][-7.7851396 -7.7014651 -7.6816115 -7.2769303 -6.7617617 -6.455976 -6.3551049 -6.511189 -6.7416573 -6.7704477 -6.9010425 -7.0738206 -7.4648037 -7.192914 -6.6330719]]...]
INFO - root - 2017-12-15 17:01:09.747549: step 37610, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 52h:21m:40s remains)
INFO - root - 2017-12-15 17:01:16.124694: step 37620, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 53h:04m:41s remains)
INFO - root - 2017-12-15 17:01:22.561363: step 37630, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 52h:02m:07s remains)
INFO - root - 2017-12-15 17:01:29.033140: step 37640, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.649 sec/batch; 53h:08m:11s remains)
INFO - root - 2017-12-15 17:01:35.376749: step 37650, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.612 sec/batch; 50h:06m:24s remains)
INFO - root - 2017-12-15 17:01:41.755520: step 37660, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 51h:31m:40s remains)
INFO - root - 2017-12-15 17:01:48.077787: step 37670, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 51h:30m:43s remains)
INFO - root - 2017-12-15 17:01:54.463860: step 37680, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 53h:03m:50s remains)
INFO - root - 2017-12-15 17:02:00.838437: step 37690, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 51h:31m:52s remains)
INFO - root - 2017-12-15 17:02:07.316628: step 37700, loss = 0.32, batch loss = 0.21 (11.9 examples/sec; 0.674 sec/batch; 55h:09m:10s remains)
2017-12-15 17:02:07.860077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8560436 -3.9445918 -4.588624 -4.6988993 -4.8218403 -4.6787806 -4.5493603 -4.5354366 -4.6075964 -4.7094603 -5.7721114 -7.3252692 -7.129365 -8.0453186 -8.4249973][-4.6776123 -4.4591651 -4.6490192 -5.0700731 -5.2914219 -5.1819687 -5.1197515 -5.1047144 -5.1061974 -5.5945177 -6.769331 -8.05025 -7.6078234 -8.7608461 -9.5064716][-4.6153035 -4.723053 -4.5562792 -4.5777254 -4.5183382 -4.018537 -3.5093856 -3.4678154 -3.6042495 -3.9211793 -5.0528231 -6.3886619 -6.2963495 -7.3880711 -8.2265892][-4.0799341 -4.3692241 -4.4264088 -4.3494062 -3.8910809 -2.8960509 -1.9396319 -1.7533426 -1.7059097 -2.0368934 -3.1725459 -5.0577488 -5.4066186 -6.3503475 -7.3123136][-4.526248 -4.380908 -3.6694374 -3.4248023 -3.0425158 -1.9668131 -0.756886 -0.5524869 -0.46227217 -0.97978687 -2.2595372 -4.1567183 -4.8971338 -6.2322955 -7.4229112][-3.964025 -4.1278725 -3.6825061 -2.4231563 -1.4417267 -0.29116726 0.62894058 1.0993214 1.3456278 0.48926067 -0.98088312 -2.9496307 -3.6842103 -5.2781215 -6.8974924][-5.2665091 -4.426362 -3.3330979 -1.9519167 -0.6177969 0.97632122 2.1396608 2.2952042 2.2281141 1.7024698 0.096982479 -2.4446135 -3.4616747 -4.8353915 -6.4181852][-5.1004424 -4.9170179 -3.797894 -1.7924523 -0.15928364 1.3669176 2.7373142 2.7034893 2.707902 1.73211 -0.065779209 -2.1311464 -3.1276913 -4.87362 -6.4970756][-4.28111 -4.32728 -3.9284356 -2.5733008 -1.369143 0.1336832 1.6150837 2.1837606 2.6043921 1.560585 -0.27920723 -2.6340151 -3.6549964 -4.8672352 -6.2328134][-5.5176439 -4.8351364 -4.1866121 -3.10049 -2.2143979 -0.8213768 0.58262444 0.69265175 1.0349073 0.32361412 -1.036922 -3.0509319 -3.7286782 -5.1727509 -6.6379762][-6.769321 -6.254941 -5.47602 -4.4639759 -3.6972554 -2.8476248 -1.8873553 -1.9371471 -1.8744378 -2.5067148 -3.2938004 -5.0406942 -5.1620722 -5.5516119 -6.9362111][-7.13867 -6.7251821 -6.6420746 -6.2417126 -5.7524667 -5.1178908 -4.4520364 -4.195528 -4.0112472 -4.396718 -4.8619714 -6.5445271 -7.0484872 -6.6669588 -6.94762][-8.2921848 -7.6987648 -7.3278089 -6.9675918 -6.5610833 -5.9910893 -5.5502548 -5.7413449 -5.7684755 -6.0470738 -6.1249042 -6.650785 -6.7011685 -7.0011773 -7.3183069][-7.6033978 -7.8393612 -7.6498189 -6.789959 -6.29781 -5.9287081 -5.7822666 -6.2333813 -6.4917316 -6.6401744 -6.7174997 -7.1302509 -7.0819082 -6.3045626 -6.1453686][-6.9613733 -7.056911 -7.1959257 -6.9381967 -6.6258 -6.2662721 -6.3065577 -6.3755703 -6.3608885 -6.5811548 -6.6822681 -6.6270771 -6.62638 -6.6921859 -6.7125225]]...]
INFO - root - 2017-12-15 17:02:14.193255: step 37710, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 51h:00m:41s remains)
INFO - root - 2017-12-15 17:02:20.616612: step 37720, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 52h:30m:41s remains)
INFO - root - 2017-12-15 17:02:27.003188: step 37730, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:56m:15s remains)
INFO - root - 2017-12-15 17:02:33.375464: step 37740, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.620 sec/batch; 50h:44m:24s remains)
INFO - root - 2017-12-15 17:02:39.821324: step 37750, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.620 sec/batch; 50h:46m:19s remains)
INFO - root - 2017-12-15 17:02:46.154433: step 37760, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 52h:13m:12s remains)
INFO - root - 2017-12-15 17:02:52.485617: step 37770, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 51h:38m:33s remains)
INFO - root - 2017-12-15 17:02:58.911365: step 37780, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 52h:27m:57s remains)
INFO - root - 2017-12-15 17:03:05.312320: step 37790, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 53h:19m:45s remains)
INFO - root - 2017-12-15 17:03:11.721502: step 37800, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 52h:48m:22s remains)
2017-12-15 17:03:12.285182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.844573 -5.8674755 -6.9099307 -7.2678852 -7.3745041 -6.8734322 -6.5032606 -5.7298656 -5.0365748 -4.685256 -6.7147064 -7.3108997 -7.3461618 -8.6234045 -8.52237][-4.237154 -5.2742367 -5.3733072 -6.1016288 -6.4781985 -6.1227942 -5.7328548 -5.3730078 -5.2306886 -4.5748768 -6.3812637 -6.8548932 -6.8578458 -7.9459658 -7.5538659][-2.8689299 -3.4044237 -3.9742539 -4.2604733 -3.9119005 -4.30343 -4.5701141 -4.4115567 -4.2094774 -3.6928437 -5.8015785 -5.8526816 -5.6036234 -6.6061974 -6.3868842][-2.1959085 -2.6146674 -3.1374502 -2.9255233 -3.1878204 -2.7800026 -2.2946362 -2.9580245 -3.2881112 -3.02063 -4.7754889 -4.8133574 -4.6909366 -5.4939537 -5.567801][-1.1802945 -1.6206737 -1.8461685 -1.9958768 -1.8392615 -1.2065768 -1.0759344 -1.2654233 -1.1856661 -1.6066875 -3.4102283 -3.6417356 -3.3930349 -4.2165947 -4.8156815][-0.90775967 -1.1641159 -1.0206542 -1.0574541 -0.7752409 -0.25265026 0.21547461 0.34652519 0.32763767 -0.0055956841 -1.7993684 -2.468369 -2.5207248 -3.6204033 -4.06147][-1.4135952 -1.1993337 -1.5588846 -1.1846991 -0.70265913 0.38067913 0.78410816 0.75262451 0.87343025 0.549551 -1.1035614 -1.7479544 -2.4775877 -3.4397712 -4.1712351][-1.3249631 -1.683877 -1.7714148 -1.1873431 -0.67086124 0.43911457 1.0616961 0.92412663 0.90092087 0.76578712 -0.79181337 -1.838377 -2.6628089 -3.8220978 -4.6765814][-1.7646279 -2.1695247 -1.6662912 -1.4254751 -0.66856623 0.54863453 1.3419657 1.3683863 1.3675108 1.0719147 -0.7704978 -1.9792695 -2.9896722 -4.198617 -4.5991468][-4.420639 -3.8118815 -3.1139936 -2.3193169 -1.0845499 0.030357361 0.67328835 1.1314087 1.4368238 0.76439095 -1.1603761 -2.4460502 -3.0913906 -4.3206325 -4.7581186][-8.247962 -7.2323017 -6.2296605 -4.8618555 -3.6412897 -2.3873916 -1.3351884 -0.91542578 -0.7231679 -1.0018935 -2.4669533 -3.2785211 -3.8580317 -4.8716149 -5.3316851][-9.0252733 -8.6606226 -7.5613222 -6.4574509 -5.770895 -4.5190039 -3.3516035 -3.3781528 -2.8554902 -2.7407508 -3.8471065 -4.4953651 -4.6002254 -5.265965 -5.4802237][-8.8327618 -9.1440878 -8.3541441 -7.2988653 -6.9208908 -5.9191127 -4.836565 -4.4370832 -3.8948357 -3.9265649 -4.6468191 -4.9836278 -5.0631714 -5.0839605 -4.7454453][-7.7672491 -8.352747 -7.8433127 -7.3453989 -7.203619 -6.607305 -6.0239396 -5.7668591 -5.4065261 -5.1372466 -5.6201677 -5.4810362 -5.0054717 -5.0451961 -5.1206026][-8.1647282 -8.2747822 -7.9773836 -8.0238523 -8.0662107 -7.686235 -7.3718066 -7.0625067 -6.9536119 -6.8225293 -6.5678334 -6.5074248 -6.4643993 -6.3074007 -5.7618484]]...]
INFO - root - 2017-12-15 17:03:18.706546: step 37810, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 52h:48m:49s remains)
INFO - root - 2017-12-15 17:03:25.038818: step 37820, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 50h:50m:06s remains)
INFO - root - 2017-12-15 17:03:31.402400: step 37830, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 51h:56m:56s remains)
INFO - root - 2017-12-15 17:03:37.769680: step 37840, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 52h:32m:49s remains)
INFO - root - 2017-12-15 17:03:44.211195: step 37850, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.637 sec/batch; 52h:10m:26s remains)
INFO - root - 2017-12-15 17:03:50.689702: step 37860, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 51h:35m:18s remains)
INFO - root - 2017-12-15 17:03:57.096517: step 37870, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 51h:52m:28s remains)
INFO - root - 2017-12-15 17:04:03.500403: step 37880, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 52h:10m:01s remains)
INFO - root - 2017-12-15 17:04:09.822903: step 37890, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.617 sec/batch; 50h:31m:28s remains)
INFO - root - 2017-12-15 17:04:16.107118: step 37900, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 50h:49m:06s remains)
2017-12-15 17:04:16.662724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0790186 -3.1562748 -4.1922555 -4.3368611 -3.9049649 -4.1600704 -3.6988354 -3.1504455 -3.1350331 -3.010294 -5.9740839 -4.4930668 -5.7215 -4.978519 -4.1420755][-3.6460724 -2.8074841 -1.8097873 -2.2593617 -2.5900321 -3.0101252 -3.4709659 -3.4122648 -1.8821354 -1.4559765 -3.8837018 -4.0283961 -6.3850603 -5.1328192 -3.5021629][-1.5427966 -2.5883722 -2.2540584 -1.4912577 -1.5343809 -1.7879295 -0.95225716 -1.9090362 -1.748034 -1.7839799 -3.9169145 -2.9768395 -5.6784267 -5.7506323 -4.9133887][-1.0601325 -0.12751627 -1.0761371 -1.275032 -0.72359276 -0.71025562 -0.58661652 -0.5515213 -0.011993408 -0.46286726 -3.2923174 -3.871526 -5.3502655 -4.790204 -4.2612076][-3.3290992 -2.2707124 -0.12772083 0.42240906 0.43748665 -0.011229515 0.051629066 -0.27545786 0.25541162 -0.032073498 -2.6660781 -3.1253667 -5.423418 -5.1771097 -4.6223164][-1.1046162 -1.4156551 -1.5601187 -0.38672352 1.3123388 2.1441345 2.3183975 1.4418793 1.153595 0.60764217 -1.875463 -2.9654989 -4.943532 -4.936121 -4.7249341][-2.3074932 -1.1952372 0.063905239 -0.0198555 0.76338005 1.7843914 3.2508392 3.3153896 2.9082041 1.4499779 -1.9022226 -2.6737218 -5.7061839 -4.8999996 -4.3284206][-0.8859725 -1.4200749 -0.8099246 0.30257702 1.1743355 1.7244253 2.5038939 2.5638857 2.557972 2.0096188 -1.0778079 -2.0192285 -4.9323459 -5.2068067 -4.9162169][-1.351665 -0.81355953 -0.38198948 0.75904369 1.6683702 1.9371939 2.8512869 2.6591358 2.7311029 1.9382067 -1.0443444 -1.0435381 -3.9241724 -3.8711209 -4.5096984][-2.4867392 -2.5061278 -1.6774035 -0.93110323 -0.21435261 0.087881565 0.17167711 0.38834286 1.3130636 1.1616802 -1.6440787 -2.0716114 -3.8721976 -3.8737736 -3.7784908][-5.2491837 -5.1535378 -4.3971958 -4.3319249 -3.6373892 -2.9118471 -2.4035525 -2.4887276 -2.4782147 -2.0989361 -3.7372398 -3.7832108 -4.9238663 -4.4448957 -4.7571058][-6.3105063 -6.3044195 -5.8781705 -5.5678463 -5.2113395 -5.2861948 -5.2180123 -4.6085763 -4.1985927 -3.8627968 -4.8450742 -4.9136028 -5.7848182 -5.3877096 -5.7749648][-6.058147 -6.7851534 -6.6794 -7.0712142 -6.217783 -6.011086 -5.8736849 -6.3551722 -6.0377984 -5.2906022 -6.0161338 -5.7714486 -5.3343763 -5.7999263 -6.2131681][-5.8949966 -6.0379782 -5.833961 -6.4327917 -6.2526894 -6.5586905 -5.829978 -5.6282024 -5.5595741 -5.8805809 -6.3432236 -6.216382 -5.8651009 -5.514297 -6.1454086][-6.1741362 -5.9496202 -6.135426 -6.4110565 -5.9763031 -6.3642845 -6.799685 -6.8181648 -6.4945526 -6.4542561 -6.5300322 -6.0515895 -6.647716 -6.428256 -5.8638897]]...]
INFO - root - 2017-12-15 17:04:23.009197: step 37910, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 51h:08m:29s remains)
INFO - root - 2017-12-15 17:04:29.316301: step 37920, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 52h:55m:12s remains)
INFO - root - 2017-12-15 17:04:35.691815: step 37930, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 53h:11m:07s remains)
INFO - root - 2017-12-15 17:04:42.098216: step 37940, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 53h:35m:35s remains)
INFO - root - 2017-12-15 17:04:48.509064: step 37950, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 52h:05m:58s remains)
INFO - root - 2017-12-15 17:04:54.772820: step 37960, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.620 sec/batch; 50h:45m:28s remains)
INFO - root - 2017-12-15 17:05:01.207917: step 37970, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 52h:39m:12s remains)
INFO - root - 2017-12-15 17:05:07.612756: step 37980, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 52h:19m:27s remains)
INFO - root - 2017-12-15 17:05:13.957109: step 37990, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 53h:10m:03s remains)
INFO - root - 2017-12-15 17:05:20.350885: step 38000, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 52h:30m:46s remains)
2017-12-15 17:05:20.848801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6242914 -4.8338118 -5.2054629 -5.4721041 -5.0893764 -4.6571846 -4.2678165 -4.0352707 -3.4666758 -4.2905388 -4.741601 -5.8508453 -7.0172949 -7.4169564 -8.1822376][-4.6897264 -4.7742033 -5.1588478 -5.2601323 -5.4748383 -5.412694 -5.2024021 -4.8759956 -4.2814641 -5.0901647 -5.4554181 -6.4949532 -7.6478505 -7.81091 -8.0171843][-4.650629 -4.7974062 -4.5205827 -3.8322768 -4.1057577 -4.160728 -3.9426744 -4.2550488 -4.2356062 -4.9968319 -5.2817354 -6.2916846 -7.1428952 -7.2593975 -7.7007594][-5.0272884 -4.5949421 -4.1449823 -3.3220325 -2.5219455 -2.4913735 -2.4740915 -2.3023295 -2.7954359 -4.1719851 -4.9471197 -5.8141794 -6.6681166 -6.9934173 -7.4859972][-4.8088226 -4.0758371 -3.1745882 -2.3604159 -1.4641619 -0.97138357 -0.76938629 -0.66218758 -0.68553591 -2.240665 -3.3469305 -4.4116726 -5.3928394 -6.1043525 -7.1658545][-3.7532792 -3.4050803 -2.8701091 -1.4105625 -0.057300568 0.66474152 1.0398016 0.43783283 0.21483135 -0.97312593 -1.7666984 -2.9256711 -4.0867033 -4.7361989 -5.9268551][-2.9332929 -2.2157588 -1.8062115 -0.70326281 0.70491695 2.1950769 2.6386986 1.9477997 1.6546631 0.28575993 -0.75630045 -2.0721021 -3.3698783 -4.3514395 -5.41642][-3.3431125 -2.2037702 -1.1836581 -0.1054163 1.128087 2.1426134 3.1026144 2.9552975 3.185833 1.8001738 0.72691059 -1.0163503 -2.7609854 -3.7675648 -4.6443939][-3.4647493 -2.6793308 -1.7890205 -0.22318268 1.3783522 1.9781637 1.9573774 2.4027119 2.4736071 1.1790915 0.40094376 -1.5670176 -3.0182147 -3.4815841 -4.5879812][-4.4008164 -4.269042 -3.4144545 -1.6658387 -0.62432003 0.012478352 0.86738396 1.051137 1.2113571 -0.05284977 -1.1547861 -2.7962861 -4.0725307 -4.5706396 -5.0786147][-6.6507139 -5.9834619 -4.83259 -3.3356671 -2.2014356 -1.2978749 -0.65146589 -0.71694517 -0.49165726 -1.5388269 -2.6523004 -3.6702342 -4.5819016 -5.4194 -5.7297029][-7.7530322 -7.3442183 -6.3209953 -5.4127164 -4.5329704 -3.3212481 -3.1564698 -2.8660011 -2.5935287 -3.6098204 -4.4288106 -5.0647135 -5.7641029 -6.0655723 -6.240983][-8.1006145 -7.59105 -7.6803455 -6.2252035 -5.2506838 -4.6219659 -4.1813335 -4.0665073 -3.9751608 -4.0450296 -4.7782211 -5.5488639 -5.61908 -5.8698654 -6.1781335][-7.994339 -7.6993947 -8.0924635 -6.9935942 -5.9392567 -5.754941 -5.5270934 -5.40174 -5.5362587 -5.6909909 -5.8317261 -5.9994931 -6.3601527 -6.5671692 -6.4662042][-9.0382385 -8.8377228 -7.9996848 -8.1112747 -8.0767832 -7.1503463 -6.953845 -7.1129179 -7.40099 -7.3294425 -7.3770423 -7.1055965 -7.2022419 -7.2673731 -6.8680859]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:05:27.273119: step 38010, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 51h:52m:50s remains)
INFO - root - 2017-12-15 17:05:33.661735: step 38020, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 51h:57m:23s remains)
INFO - root - 2017-12-15 17:05:39.998939: step 38030, loss = 0.25, batch loss = 0.14 (13.1 examples/sec; 0.610 sec/batch; 49h:55m:08s remains)
INFO - root - 2017-12-15 17:05:46.435691: step 38040, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 52h:03m:34s remains)
INFO - root - 2017-12-15 17:05:52.796111: step 38050, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.615 sec/batch; 50h:16m:42s remains)
INFO - root - 2017-12-15 17:05:59.227130: step 38060, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 51h:27m:53s remains)
INFO - root - 2017-12-15 17:06:05.575294: step 38070, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 51h:41m:19s remains)
INFO - root - 2017-12-15 17:06:11.901790: step 38080, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 51h:00m:48s remains)
INFO - root - 2017-12-15 17:06:18.276038: step 38090, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 52h:14m:11s remains)
INFO - root - 2017-12-15 17:06:24.651296: step 38100, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 52h:10m:02s remains)
2017-12-15 17:06:25.167842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8914618 -3.5267258 -4.1005526 -4.3845563 -4.4062877 -4.2633142 -3.8791139 -3.4970241 -3.1230564 -3.8600962 -4.743742 -4.9727793 -5.8058081 -6.5023117 -7.510221][-3.7771788 -3.9568975 -4.2170806 -4.3059931 -4.5226545 -4.5489478 -4.1622868 -3.7856357 -3.3460546 -4.3481512 -5.5259843 -5.7345314 -6.3956885 -6.9802542 -7.8528295][-4.116991 -4.0951443 -4.1722918 -3.905334 -3.60571 -3.6696258 -3.5448537 -3.5136456 -3.4575367 -4.4645667 -5.6002893 -6.0698357 -6.7151909 -7.4161444 -8.4032669][-4.8119307 -4.5180588 -4.0901585 -3.5150614 -2.9332247 -2.6770821 -2.2790565 -2.1993461 -2.1318192 -3.3727508 -4.8713665 -5.512341 -6.2625809 -6.9749007 -8.0760756][-5.9491463 -5.0193548 -3.9360893 -3.1626825 -2.1817088 -1.2141337 -0.30903912 -0.34663773 -0.33605003 -1.7658978 -3.6590409 -4.9010067 -6.076561 -6.8591676 -8.034606][-6.7617445 -6.08961 -5.2348146 -3.8538086 -2.2025013 -0.55190325 0.99862194 1.2261286 1.2064409 -0.70086336 -2.9864998 -4.2264566 -5.8715639 -6.95354 -8.1858931][-6.5059471 -5.7780328 -4.5986986 -3.4214306 -1.5803528 0.27529 1.7535191 2.224515 2.5520086 0.85617924 -1.5014296 -3.2257442 -5.08366 -6.1716819 -7.2032485][-6.0073247 -5.3600864 -4.2260613 -2.5723271 -0.40939236 1.5344639 3.0502996 3.3039274 3.5085049 1.6469898 -0.60594893 -2.3590012 -4.0780869 -5.140748 -6.2618933][-5.87635 -5.1662483 -4.4280472 -3.2163873 -1.5919814 0.0064144135 1.6016321 2.5659761 3.1295481 0.80957127 -1.7795219 -3.0894308 -4.4805245 -5.5228777 -6.9577389][-6.4222379 -5.7930923 -4.9564934 -3.6925311 -2.2171774 -1.5913992 -0.76141596 -0.11568737 0.29705954 -1.3593311 -3.2408242 -4.4118576 -5.6852221 -6.4206324 -7.5184174][-7.2330351 -6.825068 -6.302742 -5.4606915 -4.4374695 -3.23312 -2.1899939 -2.4366903 -2.4648848 -3.5458803 -4.7090611 -5.4376822 -6.3830872 -7.3076897 -8.0561256][-7.8665543 -7.3050065 -6.6511259 -6.3830161 -6.15262 -5.4414577 -4.5679178 -4.5964203 -4.7272549 -5.4173975 -5.8926907 -5.8517895 -6.3722057 -7.3969493 -8.0628376][-8.020895 -7.6976852 -7.0296116 -6.4725385 -6.0473323 -5.690083 -5.4097457 -5.63066 -5.7770872 -6.6258774 -6.9560423 -6.3873816 -6.4550405 -7.2233748 -7.5437927][-7.3913407 -7.3640323 -6.8486557 -6.2939534 -5.8879662 -5.2874002 -4.9688015 -5.33957 -5.5873585 -6.1078367 -6.1616964 -5.8959236 -5.9273272 -6.2462292 -6.4690027][-7.6056767 -7.7395315 -7.7241769 -7.4088655 -6.968658 -6.6229219 -6.3656816 -6.5267453 -6.8972368 -7.0334549 -6.9052296 -6.8065772 -6.6601934 -6.5475292 -6.5373578]]...]
INFO - root - 2017-12-15 17:06:31.639968: step 38110, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 53h:12m:40s remains)
INFO - root - 2017-12-15 17:06:38.090483: step 38120, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 53h:04m:32s remains)
INFO - root - 2017-12-15 17:06:44.581835: step 38130, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 54h:15m:22s remains)
INFO - root - 2017-12-15 17:06:51.074124: step 38140, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 53h:42m:02s remains)
INFO - root - 2017-12-15 17:06:57.467644: step 38150, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 52h:03m:44s remains)
INFO - root - 2017-12-15 17:07:03.852407: step 38160, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 51h:45m:51s remains)
INFO - root - 2017-12-15 17:07:10.315154: step 38170, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 53h:22m:43s remains)
INFO - root - 2017-12-15 17:07:16.660821: step 38180, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 53h:33m:42s remains)
INFO - root - 2017-12-15 17:07:23.027513: step 38190, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 52h:20m:29s remains)
INFO - root - 2017-12-15 17:07:29.403549: step 38200, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 51h:35m:36s remains)
2017-12-15 17:07:29.909654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0577312 -2.1905861 -2.4836054 -2.9212284 -3.3948374 -3.8671594 -4.2249508 -4.2793574 -4.312067 -5.6608372 -6.7151165 -7.5613689 -7.9619384 -8.3174486 -8.5613012][-2.0471444 -2.2853689 -2.9606171 -3.8090868 -4.4613752 -4.7137127 -5.021625 -5.4636397 -5.8168097 -6.9988928 -7.7837644 -8.6094332 -9.1642895 -9.7571354 -9.9220867][-1.4684968 -1.5288577 -1.7796192 -2.3885851 -3.0248194 -3.3725386 -3.6667237 -4.1556773 -4.5542011 -5.9951429 -6.9842091 -7.8696489 -8.4279127 -8.8598356 -9.0682983][-0.79502153 -1.0632501 -1.3944473 -1.5670919 -1.7439551 -1.859839 -1.9256248 -2.3232441 -2.6874185 -4.1737156 -5.28884 -6.327476 -7.0164948 -7.9259915 -8.2827864][-1.1892748 -0.97135067 -0.84561396 -0.73205471 -0.48984957 -0.23562384 -0.042434216 -0.10626316 -0.12519503 -1.3431706 -2.6350508 -3.9674945 -5.0883722 -6.2610974 -7.0558405][-1.9751945 -1.5268192 -1.1441631 -0.79051685 -0.41832304 0.24138117 0.88524151 1.1349297 1.4993906 0.29429817 -1.0553269 -2.4721427 -3.6683817 -4.8988142 -5.7822804][-2.8809257 -2.1910653 -1.4322076 -0.55804968 0.38295078 0.87308216 1.4926939 2.1472921 2.8194647 1.6333485 0.37765217 -1.0129461 -2.3237567 -3.776335 -4.9171667][-3.2240262 -2.5517812 -1.7418609 -0.63305283 0.51333904 1.4297266 2.0563898 2.517849 3.2594481 2.4170675 1.2891073 -0.17657614 -1.4841442 -2.8468685 -3.9739244][-3.2896028 -2.3835454 -1.763454 -0.83529282 0.24853706 1.1912918 1.9460564 2.1383352 2.3914719 1.2322407 0.0313797 -1.2497354 -2.2092519 -3.4279566 -4.3985853][-3.7973056 -3.091466 -2.5001473 -1.417027 -0.4802475 0.1303978 0.81820583 1.3734846 1.6197634 -0.19043827 -1.5117888 -2.8381529 -3.8118761 -4.752799 -5.425415][-5.2045646 -4.4235497 -3.6771979 -2.800962 -2.1060147 -1.4432373 -0.81138706 -0.61020613 -0.46444511 -1.990294 -3.3640018 -4.617219 -5.2923965 -6.0659013 -6.7081418][-5.2436972 -5.0477743 -4.5099249 -3.9377446 -3.3887696 -2.5530257 -2.2742548 -2.44551 -2.5045314 -3.6545029 -4.724586 -5.8512931 -6.449954 -6.9057446 -6.9281812][-5.9988031 -5.5243511 -5.1609836 -4.7609863 -4.3976 -3.8344519 -3.8394165 -3.9564028 -4.1282415 -5.1567764 -6.0003767 -6.5536294 -6.9423561 -7.2124524 -7.021719][-6.170763 -5.3963609 -5.00058 -4.6434717 -4.427516 -4.0187063 -4.0617886 -4.358892 -4.637394 -5.4863539 -5.8544426 -6.0896978 -6.139317 -6.5933652 -6.4333143][-7.0292826 -6.6962729 -6.3633671 -5.9720736 -5.5103407 -5.2419157 -5.4414892 -5.6131105 -5.8454232 -6.1689997 -6.485292 -6.7315645 -6.9110837 -6.5421367 -6.2542863]]...]
INFO - root - 2017-12-15 17:07:36.222696: step 38210, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 51h:42m:07s remains)
INFO - root - 2017-12-15 17:07:42.603103: step 38220, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 52h:58m:38s remains)
INFO - root - 2017-12-15 17:07:49.004450: step 38230, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:48m:04s remains)
INFO - root - 2017-12-15 17:07:55.357338: step 38240, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 51h:01m:35s remains)
INFO - root - 2017-12-15 17:08:01.640462: step 38250, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 51h:07m:55s remains)
INFO - root - 2017-12-15 17:08:07.988176: step 38260, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 51h:04m:45s remains)
INFO - root - 2017-12-15 17:08:14.340980: step 38270, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.614 sec/batch; 50h:10m:46s remains)
INFO - root - 2017-12-15 17:08:20.654903: step 38280, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 51h:59m:24s remains)
INFO - root - 2017-12-15 17:08:27.048007: step 38290, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 52h:10m:51s remains)
INFO - root - 2017-12-15 17:08:33.416647: step 38300, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 51h:27m:33s remains)
2017-12-15 17:08:33.953707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0843434 -5.3821287 -4.7392988 -4.0115986 -3.4858007 -2.7223377 -3.041544 -3.0872006 -3.5341191 -5.6471529 -5.440526 -6.0444188 -6.0710258 -7.46422 -6.6235018][-5.5632381 -4.6018453 -4.2344046 -3.9528763 -3.572372 -2.6516309 -2.9512172 -3.0477052 -3.5691457 -5.1029634 -4.5447397 -5.3513136 -5.7753291 -7.2760849 -6.6619873][-5.6735229 -4.369832 -3.5728221 -2.9346704 -2.8400683 -2.6920919 -3.010788 -3.1912127 -3.3268375 -4.9993935 -4.3504434 -5.2891264 -5.3343463 -6.6048222 -5.9209414][-4.7302895 -4.3384957 -3.8152261 -3.07542 -2.8140507 -2.5028105 -2.2151985 -2.3294225 -2.6309581 -4.2141218 -3.264504 -4.2061996 -4.9698544 -6.0342922 -5.0659094][-3.9563971 -3.339304 -2.77596 -2.5994821 -2.1751208 -2.0691562 -2.1425285 -2.3322792 -2.15309 -3.6660562 -2.7553787 -3.6548052 -3.6783867 -4.7415352 -4.6358848][-2.5834208 -2.5389962 -2.0562606 -1.6822662 -1.1780581 -0.97856236 -0.76649 -0.87270212 -0.92672253 -3.0288563 -2.0679255 -2.9439621 -3.3703494 -5.0586071 -4.7354908][-1.0283566 -1.4815135 -1.1300535 -0.89645672 -0.20371771 0.34511471 0.130692 -0.31698418 -0.60131073 -2.6687636 -2.0503592 -3.2218375 -3.6989844 -5.0442667 -4.7544003][-0.270648 0.1361742 0.33807468 0.10192871 0.48622704 0.87275314 0.98558331 0.79777145 0.36515522 -1.6737361 -1.5305882 -2.8952208 -3.9171953 -5.9340067 -5.4028339][0.10512304 0.28151369 0.78465366 0.570734 0.57869434 0.87264061 1.0248747 0.7288065 0.49620819 -1.5704675 -1.4557314 -3.0863662 -4.0386467 -5.8530388 -5.6393118][-0.75228024 -0.31730223 -0.25726223 0.095875263 0.468935 0.2814641 0.47593307 0.50446415 0.3324976 -1.9455791 -1.8228416 -3.4506063 -4.6856937 -6.0713987 -5.8094387][-2.4509315 -1.1419816 -0.96545506 -0.87030506 -0.5782609 0.10882235 0.16249752 -0.13514376 -0.29944324 -2.2407093 -2.1903219 -4.0011911 -5.0268841 -6.0960541 -6.0917907][-4.2528172 -3.5556064 -2.6444707 -2.0808368 -1.576776 -1.0021777 -0.75722265 -0.36780453 -0.90394306 -3.0550904 -2.8583045 -4.0554333 -5.2222967 -6.5829978 -7.0756207][-5.1712828 -4.4969225 -4.0720949 -3.7155278 -3.0955095 -2.2024302 -1.845737 -1.5290146 -1.6007133 -3.1356707 -3.3920507 -4.8343487 -5.8226333 -7.1811504 -7.3358469][-5.532198 -4.7793026 -4.7671213 -4.2168283 -3.9352934 -3.3253379 -2.76299 -2.4335761 -2.645946 -3.6835313 -3.7991173 -5.1042371 -6.1683888 -7.2344117 -7.2287107][-6.60229 -5.9673443 -5.4088869 -5.0684171 -5.0081854 -4.4116817 -4.2212648 -4.3952188 -4.2376919 -4.4528427 -4.6678181 -5.5366769 -6.43331 -6.9946928 -7.2509985]]...]
INFO - root - 2017-12-15 17:08:40.401750: step 38310, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 50h:47m:16s remains)
INFO - root - 2017-12-15 17:08:46.740362: step 38320, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 51h:02m:55s remains)
INFO - root - 2017-12-15 17:08:53.135942: step 38330, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 53h:32m:02s remains)
INFO - root - 2017-12-15 17:08:59.501381: step 38340, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 51h:11m:53s remains)
INFO - root - 2017-12-15 17:09:05.934938: step 38350, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 53h:02m:23s remains)
INFO - root - 2017-12-15 17:09:12.308732: step 38360, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 53h:09m:11s remains)
INFO - root - 2017-12-15 17:09:18.712177: step 38370, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 52h:34m:46s remains)
INFO - root - 2017-12-15 17:09:25.084886: step 38380, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 51h:54m:27s remains)
INFO - root - 2017-12-15 17:09:31.479562: step 38390, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 50h:49m:54s remains)
INFO - root - 2017-12-15 17:09:37.855537: step 38400, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 52h:51m:21s remains)
2017-12-15 17:09:38.391886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3380651 -3.0404043 -3.0961804 -3.5472417 -3.5695252 -3.3405762 -2.7358775 -1.9902024 -1.3615222 -1.6049418 -2.3760233 -4.6799359 -5.5505037 -6.5323415 -7.5661426][-4.93437 -4.1855121 -4.3570175 -4.8995986 -4.7432446 -4.5621643 -4.1093283 -3.2510409 -2.7216563 -3.0483027 -3.6639595 -5.6338763 -6.4419241 -7.3348632 -8.2328081][-5.4520845 -5.0218487 -4.7001181 -4.6671972 -4.6677685 -4.216197 -3.5282826 -3.0076785 -2.3321753 -2.6919198 -3.3594303 -5.2755461 -6.1372857 -7.0044374 -7.8764687][-6.0759921 -5.7905068 -5.3243494 -4.415204 -3.244163 -2.3372321 -1.2769265 -0.95884848 -0.86626768 -1.7252626 -2.8305349 -4.9566865 -5.7777495 -6.6072907 -7.2527509][-6.7518826 -6.1474304 -5.66296 -4.6452188 -3.2110314 -1.8310328 -0.46489048 -0.16996336 0.048863411 -1.121911 -2.2690082 -4.6391277 -5.7683287 -6.6241703 -7.5591936][-7.8991551 -6.9315429 -5.8799376 -3.8815577 -2.0467072 -0.2481842 1.4607801 1.7683201 1.8527422 0.41094017 -1.0628991 -3.9109068 -5.3429327 -6.4106083 -7.59298][-8.0613537 -7.038496 -5.6386566 -3.1613674 -0.70614243 1.3060675 2.7911949 3.159194 3.33922 1.6471968 -0.0955534 -3.0016084 -4.7359557 -6.2490668 -7.6563191][-7.7888746 -6.6143322 -5.0369864 -2.428339 -0.065838814 2.2723532 3.7010126 3.8753052 3.692256 1.9099464 0.6442337 -2.6766219 -4.5919752 -6.1105571 -7.7658][-7.1853662 -6.4819393 -5.4048967 -3.19067 -1.3344569 0.66883469 2.0589218 2.1369696 2.0045595 0.41908264 -0.7842145 -3.3832417 -5.011075 -6.1995039 -7.69186][-7.07633 -6.8040848 -5.7710547 -4.2571249 -2.6850324 -0.90718651 -0.017414093 0.18719625 0.50435257 -1.1832175 -2.4397764 -4.7294407 -5.9184008 -6.6991854 -7.8117023][-7.6580176 -7.7283359 -7.1930904 -5.9508419 -4.8734236 -3.4413323 -2.5697718 -2.3127375 -2.0735269 -3.153831 -3.7808321 -5.6520872 -6.5152006 -7.0551906 -7.5671496][-8.3431931 -8.1205044 -7.6508336 -6.6502614 -5.9108076 -4.8435097 -4.2155609 -4.1458359 -3.9254625 -4.6693859 -4.9870024 -6.1349335 -6.5424652 -6.7652121 -6.9430132][-8.8113 -8.9265795 -8.64391 -7.7022505 -6.8284788 -5.9385424 -5.6858182 -5.7570438 -5.787374 -6.2801342 -6.6420088 -7.1995835 -7.3386316 -7.3135738 -7.4006147][-7.546124 -7.4493761 -7.1696153 -6.7870045 -6.3389411 -5.6230054 -5.4911146 -5.8836317 -6.0699291 -6.6772738 -6.9666867 -7.1826181 -7.153194 -7.35493 -7.3954363][-9.5285769 -9.051568 -8.24058 -7.2928133 -6.9825768 -6.867424 -6.79354 -7.1179395 -7.500165 -7.7573571 -7.9933267 -7.7580266 -7.4827461 -7.5337453 -7.2532716]]...]
INFO - root - 2017-12-15 17:09:44.943987: step 38410, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 54h:13m:28s remains)
INFO - root - 2017-12-15 17:09:51.326214: step 38420, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 52h:16m:22s remains)
INFO - root - 2017-12-15 17:09:57.826358: step 38430, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 54h:10m:01s remains)
INFO - root - 2017-12-15 17:10:04.257455: step 38440, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 52h:23m:05s remains)
INFO - root - 2017-12-15 17:10:10.732188: step 38450, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 54h:32m:37s remains)
INFO - root - 2017-12-15 17:10:17.065036: step 38460, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 52h:11m:57s remains)
INFO - root - 2017-12-15 17:10:23.452956: step 38470, loss = 0.32, batch loss = 0.21 (11.9 examples/sec; 0.672 sec/batch; 54h:50m:42s remains)
INFO - root - 2017-12-15 17:10:29.795577: step 38480, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 51h:18m:54s remains)
INFO - root - 2017-12-15 17:10:36.174809: step 38490, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:45m:19s remains)
INFO - root - 2017-12-15 17:10:42.673423: step 38500, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 51h:55m:50s remains)
2017-12-15 17:10:43.139775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4013109 -5.8789568 -5.5495224 -4.8343678 -4.2580361 -3.4983764 -2.5857477 -1.9254804 -1.1938472 -2.2607937 -3.3483419 -4.3807497 -4.5103288 -5.3814044 -6.5718021][-4.9090376 -4.4846506 -3.8350585 -3.3589215 -2.6238298 -2.230907 -2.2751884 -2.5819082 -1.8453569 -2.1822639 -2.5120068 -3.9943831 -4.4494648 -5.2093029 -6.1915293][-3.5630026 -2.6997871 -1.940351 -2.0042 -1.5706873 -1.328208 -1.1341553 -1.2003126 -0.98986149 -1.285573 -1.5211639 -3.7243156 -4.6293745 -4.8892107 -5.2075882][-1.866334 -1.4234004 -1.2054324 -0.78613138 -0.30765295 -0.37642097 -0.30636215 -0.25844526 0.22489977 0.048578262 -1.0621238 -3.5747743 -4.3665543 -4.6389608 -5.6478338][-1.9460311 -1.0401893 -0.71181345 -0.40746069 -0.17867088 -0.14777422 0.25185156 0.43970871 0.34521961 -0.23730564 -0.85515928 -2.62216 -3.4415007 -4.3633547 -5.9143624][-1.1912527 -0.923223 -0.99329424 -0.7509656 -0.23998213 -0.10101652 0.2715888 0.47816277 0.57409096 0.091366768 -0.50371885 -2.2837563 -3.3532796 -4.3909674 -5.5292273][-2.0710068 -1.6933699 -1.009336 -1.0956054 -0.80884361 -0.43486404 0.047573566 0.48716927 0.92726994 0.31713581 -0.78955269 -2.5347581 -3.9393704 -4.8860912 -5.6923938][-2.4130912 -2.408226 -2.1670299 -1.7093458 -0.98385048 -0.65161228 -0.077426434 0.46509552 1.0459347 0.34031487 -0.73718262 -2.518126 -4.241992 -4.96539 -5.8604274][-3.7095501 -3.3760405 -2.5882053 -1.8597012 -1.3420401 -0.64762592 0.31223679 0.59549141 0.9676342 0.35140038 -0.95893717 -2.732255 -4.2206764 -4.8603773 -5.7213221][-4.642859 -4.1119738 -3.5443978 -2.7244878 -1.9603543 -1.2793779 -0.22473335 0.15980291 0.41248226 -0.55779028 -2.1152892 -3.3449998 -4.2040834 -4.4685526 -5.0532694][-5.9984264 -4.7373996 -4.5504351 -4.0423708 -3.1406021 -2.4008698 -1.0999174 -0.68874454 -0.857502 -1.6742177 -2.7763309 -3.2297764 -3.6628332 -3.814724 -4.5356712][-7.5821829 -6.6297841 -5.9593382 -4.8960867 -3.9417262 -3.4505057 -2.4144959 -1.736691 -1.7102704 -1.6019874 -2.37367 -2.913353 -3.9038658 -4.2622747 -4.5838108][-8.5343342 -8.2115269 -6.8587155 -5.4786091 -4.1883688 -3.5230575 -2.9041791 -2.5825577 -2.2680302 -1.2388639 -2.0369062 -2.533349 -3.2423172 -3.6078329 -4.0353112][-8.5385866 -8.2704058 -7.3292947 -6.2916374 -4.7242427 -4.0570965 -3.1964998 -2.7745314 -2.577414 -2.2209167 -2.8480248 -2.6272693 -2.3525887 -2.6950769 -3.6066856][-8.0352354 -7.8212752 -7.4330611 -6.77888 -5.7510962 -5.2386913 -4.3184605 -4.008769 -3.9450138 -3.4692783 -3.1841617 -3.2231913 -3.0761495 -3.2491527 -3.629571]]...]
INFO - root - 2017-12-15 17:10:49.460516: step 38510, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 52h:28m:44s remains)
INFO - root - 2017-12-15 17:10:55.790518: step 38520, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 51h:11m:20s remains)
INFO - root - 2017-12-15 17:11:02.226400: step 38530, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 54h:57m:05s remains)
INFO - root - 2017-12-15 17:11:08.545704: step 38540, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 51h:02m:46s remains)
INFO - root - 2017-12-15 17:11:15.032931: step 38550, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 54h:31m:21s remains)
INFO - root - 2017-12-15 17:11:21.407145: step 38560, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 53h:31m:09s remains)
INFO - root - 2017-12-15 17:11:27.863505: step 38570, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 50h:32m:32s remains)
INFO - root - 2017-12-15 17:11:34.266992: step 38580, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 51h:38m:50s remains)
INFO - root - 2017-12-15 17:11:40.687712: step 38590, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 53h:13m:03s remains)
INFO - root - 2017-12-15 17:11:47.179660: step 38600, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.664 sec/batch; 54h:10m:14s remains)
2017-12-15 17:11:47.690343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.163063 -5.7056608 -6.0069776 -6.3355985 -6.4304366 -6.36883 -6.3013716 -5.9332876 -4.842865 -3.6153316 -4.1683865 -4.6216006 -4.5932446 -4.3913975 -5.52867][-5.3035769 -5.3250513 -5.7609243 -5.9230957 -6.4106336 -6.9602675 -7.1462097 -6.8558226 -6.10863 -4.994586 -5.5897932 -6.497756 -6.5500536 -5.4203644 -5.8444738][-5.7836156 -6.1052284 -6.5362849 -6.3360467 -6.3208542 -6.9215388 -7.2684793 -7.4189215 -7.1124783 -6.2281504 -6.9961991 -7.6289067 -7.7630243 -6.8545003 -6.86413][-6.39683 -5.9282055 -5.7929611 -5.58248 -5.3618402 -5.2069254 -4.618434 -4.6757293 -4.3994431 -4.1646633 -5.4248042 -6.6215792 -7.2643008 -6.7814164 -7.3726792][-7.1728334 -6.5241818 -5.6267667 -4.5121622 -3.8841412 -3.5151153 -2.9179139 -2.73244 -2.1320615 -1.6720023 -3.037765 -4.8771687 -6.3899164 -6.6401367 -7.5600057][-6.4474058 -5.4918838 -4.6984339 -3.5260015 -2.2857594 -1.51509 -0.61734056 -0.35886812 0.17373323 0.373003 -1.3369417 -3.1398234 -4.591785 -5.3354859 -6.6743593][-5.9496727 -5.0586324 -3.6109896 -1.6792965 -0.54859447 0.22297525 1.1974325 1.7008076 2.3048429 2.1248417 -0.18163347 -2.53547 -4.2125006 -4.57567 -5.6735983][-4.6982336 -4.1241484 -3.6959391 -2.1432023 -0.63334179 0.58881569 1.9507265 2.4505091 2.9762459 2.6962147 0.48423481 -1.9852686 -3.8378251 -3.972856 -4.9522839][-5.5620251 -4.6124878 -3.1978655 -1.662076 -0.58437967 0.13953733 1.1748133 1.7787027 2.2013254 1.4206038 -0.89715719 -3.2616525 -4.4594622 -4.3827429 -5.0192208][-5.4761066 -4.9433951 -4.1834478 -2.7309718 -1.2824244 -0.11833477 0.64586735 0.85628796 1.0277472 0.46899128 -1.4054337 -3.2680125 -4.5113688 -4.8841572 -5.7691784][-6.8608217 -6.5032492 -5.96299 -5.0696859 -4.1288476 -3.2662907 -2.1370859 -1.8902178 -2.129993 -2.6358972 -3.8622315 -4.8608837 -5.019331 -5.5259714 -6.4590883][-7.9919009 -8.0554008 -7.410984 -6.3984594 -5.3574228 -4.7652144 -3.9281578 -3.437305 -3.0949044 -3.1984639 -4.2602367 -4.9895806 -5.1312132 -5.2187858 -5.5704532][-8.8751583 -8.8820534 -8.8716364 -7.9764137 -6.994729 -6.1986265 -5.0908127 -4.858881 -4.6074629 -4.327302 -4.89208 -4.7952976 -4.6554394 -4.9585476 -5.4035378][-7.5294061 -7.8467746 -7.8414636 -7.2520332 -6.9642715 -6.2446275 -5.2066779 -4.8949108 -4.6595135 -4.476603 -4.6491604 -4.4814878 -4.1210623 -3.66441 -4.0094948][-6.7637324 -6.9319777 -6.7560782 -6.4899707 -6.2582679 -5.9238563 -5.604002 -5.4510846 -5.2682824 -5.2230043 -5.4948969 -5.513742 -5.3067675 -5.104455 -4.7380228]]...]
INFO - root - 2017-12-15 17:11:54.053638: step 38610, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 50h:36m:56s remains)
INFO - root - 2017-12-15 17:12:00.422460: step 38620, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 51h:36m:17s remains)
INFO - root - 2017-12-15 17:12:06.842518: step 38630, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 51h:29m:16s remains)
INFO - root - 2017-12-15 17:12:13.228664: step 38640, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 52h:51m:02s remains)
INFO - root - 2017-12-15 17:12:19.693883: step 38650, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 52h:25m:16s remains)
INFO - root - 2017-12-15 17:12:26.013813: step 38660, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.626 sec/batch; 51h:06m:23s remains)
INFO - root - 2017-12-15 17:12:32.377287: step 38670, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 51h:34m:15s remains)
INFO - root - 2017-12-15 17:12:38.806803: step 38680, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 53h:23m:53s remains)
INFO - root - 2017-12-15 17:12:45.103353: step 38690, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 53h:38m:48s remains)
INFO - root - 2017-12-15 17:12:51.538033: step 38700, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 54h:03m:24s remains)
2017-12-15 17:12:52.053360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4340959 -2.6265488 -3.5323958 -4.1745939 -5.0585365 -4.9533472 -4.430645 -3.2803168 -2.3552876 -1.8497295 -3.4079289 -4.3365545 -5.3873892 -5.9170656 -6.4366713][-2.0477185 -2.1808667 -2.2587385 -2.747087 -3.2201605 -4.403163 -4.9804678 -5.3929338 -4.6299372 -4.5006342 -4.9703808 -4.42404 -4.6971865 -4.7853956 -5.9574933][-3.893714 -3.0506005 -2.7798333 -3.227159 -3.1323104 -3.4492531 -4.0788116 -5.0707235 -5.7359858 -6.1124358 -6.6103921 -6.134994 -5.6147828 -4.9672308 -5.4787364][-4.6948981 -5.1576166 -4.2255812 -3.2942295 -2.2794785 -2.5628037 -2.5485392 -2.2348142 -2.4391785 -3.5822644 -6.1542826 -7.3245273 -7.2071075 -6.9627018 -7.0380459][-5.2721272 -4.3915567 -4.19103 -2.4924212 -1.5402851 -1.04983 -0.43987274 -0.47405434 -0.41686344 -0.89655828 -3.905494 -5.76013 -8.2048893 -8.8107719 -8.7114344][-1.8799362 -1.7679291 -2.089282 -0.49591684 0.8956728 0.87198257 1.5261049 1.4875698 1.4906178 0.87188244 -1.4841356 -2.9851527 -5.26646 -7.1991372 -8.4413452][-0.94621181 -1.0626893 0.062922955 1.2070713 1.8640575 3.3080063 4.1317844 3.9764929 2.4927998 0.93800926 -1.8329639 -3.6686668 -4.7890863 -5.2780113 -7.1048961][0.42864895 -0.059743404 0.21028757 0.025731087 0.85690689 1.8650932 2.9895859 3.1182079 2.9835749 1.6810503 -1.7073884 -3.3846445 -4.8935518 -4.9701047 -5.2171173][-1.7055068 0.71372414 1.0759897 0.81806946 0.36795712 0.53216648 1.2073603 1.950058 3.1653433 2.2568998 -0.52468777 -1.8564229 -4.9862165 -6.4955583 -6.8447776][-3.6567473 -1.8281569 -0.97840738 -0.66975975 -0.12951136 -0.12234116 -0.62438965 -0.43891716 0.075514793 0.1268959 -1.9581161 -3.0126529 -4.7774792 -5.8599377 -7.4080663][-6.5069447 -6.0917816 -5.0235682 -4.5453048 -3.7691011 -3.014956 -3.2678752 -2.9910684 -3.0496221 -2.6349034 -3.7498178 -4.4759808 -5.5773039 -7.0830379 -8.5252161][-7.8276792 -7.8210611 -7.3720994 -6.9584 -6.1397915 -5.7364836 -4.8876829 -4.279727 -4.007442 -4.054594 -5.3809562 -5.5571556 -5.9047966 -7.0837846 -7.15647][-8.883688 -9.028595 -8.9352074 -8.1892395 -7.4669151 -7.30678 -6.4587345 -7.1009016 -6.4802923 -6.0997658 -5.9923105 -5.2391276 -5.11038 -6.446291 -6.6300035][-5.519248 -6.3086157 -7.0846057 -6.93332 -6.8781738 -6.8104558 -6.5768204 -6.5345306 -6.3770332 -5.9081488 -6.1294355 -5.2934923 -5.3021116 -5.0348105 -5.2752352][-4.2869806 -4.4872675 -5.72201 -6.5867753 -7.106051 -6.3011746 -5.865159 -6.3608227 -7.42418 -7.6648211 -7.6989884 -7.400672 -7.0402203 -6.6729646 -6.38084]]...]
INFO - root - 2017-12-15 17:12:58.488112: step 38710, loss = 0.23, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 52h:31m:00s remains)
INFO - root - 2017-12-15 17:13:04.811639: step 38720, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 52h:12m:42s remains)
INFO - root - 2017-12-15 17:13:11.291224: step 38730, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 52h:18m:08s remains)
INFO - root - 2017-12-15 17:13:17.696248: step 38740, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 53h:02m:01s remains)
INFO - root - 2017-12-15 17:13:24.117666: step 38750, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:54m:39s remains)
INFO - root - 2017-12-15 17:13:30.476787: step 38760, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 52h:35m:25s remains)
INFO - root - 2017-12-15 17:13:36.894642: step 38770, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 53h:06m:23s remains)
INFO - root - 2017-12-15 17:13:43.401320: step 38780, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 54h:09m:48s remains)
INFO - root - 2017-12-15 17:13:49.801483: step 38790, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 51h:27m:15s remains)
INFO - root - 2017-12-15 17:13:56.188770: step 38800, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 52h:46m:42s remains)
2017-12-15 17:13:56.728997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7123494 -2.33529 -3.2352676 -4.4727497 -5.8573551 -6.7487741 -7.7853503 -9.1473427 -10.164131 -11.8496 -13.398984 -13.77967 -12.988701 -11.619843 -10.569208][-0.81682396 -1.9878945 -3.5133123 -4.5841179 -5.5877676 -6.4177394 -7.4594378 -8.3540068 -8.78701 -10.026999 -10.833189 -12.023726 -13.068866 -12.819096 -12.764528][-1.5662327 -2.1113968 -3.2925167 -3.674562 -4.14602 -4.8497949 -5.365591 -6.2851381 -6.7378526 -7.6022487 -8.2291727 -9.2490788 -10.386316 -11.357752 -13.389194][-4.201859 -4.1429672 -4.4476066 -4.0352688 -3.2656121 -2.7943377 -1.9742723 -2.4667373 -3.1121612 -4.4826803 -5.7657523 -7.5300927 -8.6663284 -9.7409153 -11.703657][-7.3423371 -6.6270518 -5.7432804 -5.0590439 -4.4554844 -2.5002403 -0.31826258 -0.062312603 0.051972866 -1.7425389 -4.0810909 -7.1224642 -8.9757738 -10.372625 -12.132683][-8.9533215 -8.3859186 -7.5034285 -5.6622643 -3.5362287 -0.98365021 1.4681063 2.2118893 2.7623558 0.26189709 -2.8376822 -7.0012856 -9.99008 -11.57695 -12.424078][-9.7143126 -8.49678 -6.5667315 -3.6218038 -0.24537373 2.8963423 5.714489 6.2302589 6.5816994 3.9417076 0.24609756 -4.7958469 -8.6358118 -10.793069 -12.275172][-8.56705 -8.1701241 -6.7819595 -3.2611766 0.96933937 4.0889721 6.6300688 7.6330414 7.9091358 5.366312 2.2951956 -2.4855456 -6.0443516 -8.4777689 -10.111345][-9.1921921 -8.5997467 -7.5160189 -4.2678781 -0.8593955 1.839859 4.172616 4.7291164 5.1474094 3.1915112 0.44439602 -3.1224113 -5.5689306 -7.1772761 -8.8397026][-9.858366 -9.8841829 -9.0051184 -6.4777555 -3.818188 -0.79674673 1.9836063 2.5031443 2.9712505 0.47254848 -2.6013737 -5.7892718 -7.744009 -8.35929 -8.8687744][-11.539993 -11.755247 -11.570749 -9.8625727 -8.0953941 -5.5519314 -2.8223028 -1.3807302 0.30217552 -0.98483086 -3.4150133 -5.995276 -7.3573675 -8.3227377 -9.5192451][-11.261333 -11.582117 -11.735083 -10.873619 -9.7265406 -8.1833906 -6.7591 -5.2120085 -3.69517 -4.0679426 -4.3520756 -5.3871593 -6.301075 -7.1612234 -8.2077942][-11.21855 -11.54303 -11.743629 -11.077408 -10.48534 -9.485589 -8.2510176 -7.7493553 -7.6841083 -8.06949 -8.4092312 -8.0532284 -7.5008492 -7.5994773 -7.2449694][-9.7269611 -9.588213 -9.5130177 -9.0229187 -8.8628616 -8.1169491 -7.3558373 -7.1670184 -7.1239982 -7.85355 -8.1055145 -8.3007994 -8.59827 -8.6310949 -8.3110771][-10.451158 -9.75972 -9.3129988 -8.699048 -8.431551 -8.0242834 -7.4787431 -7.7219076 -8.0286522 -8.3445778 -8.2618761 -7.8121905 -7.2827072 -6.9890924 -6.5928369]]...]
INFO - root - 2017-12-15 17:14:03.110501: step 38810, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 51h:42m:11s remains)
INFO - root - 2017-12-15 17:14:09.482390: step 38820, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 54h:38m:50s remains)
INFO - root - 2017-12-15 17:14:15.938515: step 38830, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 52h:28m:37s remains)
INFO - root - 2017-12-15 17:14:22.304720: step 38840, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 51h:58m:47s remains)
INFO - root - 2017-12-15 17:14:28.752323: step 38850, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 52h:06m:57s remains)
INFO - root - 2017-12-15 17:14:35.226329: step 38860, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 51h:52m:30s remains)
INFO - root - 2017-12-15 17:14:41.607591: step 38870, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 52h:14m:40s remains)
INFO - root - 2017-12-15 17:14:48.033752: step 38880, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 51h:57m:25s remains)
INFO - root - 2017-12-15 17:14:54.422085: step 38890, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 52h:58m:06s remains)
INFO - root - 2017-12-15 17:15:00.785540: step 38900, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 51h:47m:21s remains)
2017-12-15 17:15:01.299275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7922902 -3.2632651 -3.5989079 -4.3031473 -4.9457073 -5.2153015 -5.07703 -4.6849566 -4.3778419 -3.9528759 -4.2169971 -4.6192541 -5.9619308 -6.2467294 -6.6228104][-2.938252 -3.3741841 -3.755652 -4.6254439 -4.3640127 -4.3855295 -4.7861934 -5.2449493 -5.6236911 -5.4074926 -5.7134504 -6.0269575 -6.6439672 -6.692472 -6.3537407][-3.4971395 -3.8057804 -4.2748423 -4.7357254 -4.3730574 -3.985714 -3.8100326 -4.1672421 -4.9035091 -5.4735694 -6.3433857 -6.7787585 -8.1883841 -7.7037821 -6.9856739][-4.7373028 -4.4011269 -4.3301048 -4.489336 -4.0638657 -3.5591874 -2.7650304 -2.7668781 -2.9478173 -3.7844632 -5.094738 -5.9434953 -7.8884182 -7.5944867 -7.6214209][-4.2747536 -3.9570854 -3.7480197 -3.9903014 -3.0999551 -1.7473583 -0.75412083 -0.80399132 -1.0975003 -2.3105526 -3.7882392 -5.1692638 -7.3872895 -7.208663 -6.8188987][-4.9486332 -4.6271062 -4.1875563 -2.9687996 -1.8751707 -1.0837989 0.23873758 0.81986046 0.80038071 -0.26257467 -1.4401522 -3.0315366 -5.1203947 -5.6847162 -5.9770937][-4.75867 -4.1510258 -3.1018848 -1.8535976 -0.012756824 1.4054461 2.7141886 2.7172518 2.2753057 0.88842106 -0.17519665 -1.6013889 -3.1464777 -3.6119156 -3.8892682][-3.9146934 -2.6189904 -1.5451632 0.0097103119 2.2263288 3.6758213 4.9297476 5.1021986 4.1512842 2.5613604 1.1299181 -0.68283844 -2.2299705 -2.6460652 -2.6378593][-5.2511764 -3.9942608 -2.4127121 -0.700788 1.2425032 2.7802124 4.0813885 4.6361465 4.8768997 3.3500185 1.4206085 -0.2526288 -1.6748447 -2.5701237 -3.1932802][-6.9945388 -5.9597697 -4.515059 -3.0854015 -1.3365345 0.23881435 1.7505951 2.5919867 2.9281788 1.4595146 0.000521183 -1.2869649 -2.5120406 -3.242661 -3.2679715][-8.9147243 -8.3521423 -7.3309078 -5.810503 -4.1643567 -2.2926455 -1.0007052 -0.55335 -0.16699314 -1.0300531 -2.4813771 -3.6702628 -4.7793016 -5.0305986 -5.0319672][-9.2061815 -9.0421753 -8.7736111 -8.1784935 -6.6625881 -5.1030712 -4.01397 -3.7743452 -3.3033 -3.5794687 -3.9567554 -4.8922243 -5.7743239 -6.30477 -6.8647828][-10.03236 -9.7745361 -9.3976622 -9.0041294 -8.3290844 -7.05765 -5.6852107 -5.3769727 -5.5448174 -6.1339025 -6.0277767 -6.0538015 -6.0220618 -6.1281371 -6.4141612][-9.1037722 -9.4258223 -9.1596975 -8.5207386 -7.7058334 -6.8179131 -6.2988071 -6.0023217 -5.8317471 -5.9534225 -6.1632895 -6.5704231 -6.7168512 -6.0984993 -5.3672819][-8.9050817 -9.2924871 -9.9954195 -9.5842762 -8.2376795 -6.9860153 -6.0884957 -6.0051665 -6.4922562 -6.7960391 -6.9612889 -7.027688 -6.7303567 -6.063118 -5.4451194]]...]
INFO - root - 2017-12-15 17:15:07.810601: step 38910, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 53h:15m:49s remains)
INFO - root - 2017-12-15 17:15:14.178682: step 38920, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 51h:57m:59s remains)
INFO - root - 2017-12-15 17:15:20.648306: step 38930, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 53h:40m:38s remains)
INFO - root - 2017-12-15 17:15:27.091378: step 38940, loss = 0.29, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 54h:40m:09s remains)
INFO - root - 2017-12-15 17:15:33.495380: step 38950, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 51h:26m:45s remains)
INFO - root - 2017-12-15 17:15:39.932864: step 38960, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 52h:00m:48s remains)
INFO - root - 2017-12-15 17:15:46.371368: step 38970, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 52h:55m:16s remains)
INFO - root - 2017-12-15 17:15:52.700551: step 38980, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 51h:59m:54s remains)
INFO - root - 2017-12-15 17:15:59.060392: step 38990, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 51h:53m:41s remains)
INFO - root - 2017-12-15 17:16:05.519485: step 39000, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 52h:58m:51s remains)
2017-12-15 17:16:06.072157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3694367 -7.7065377 -9.7203283 -10.735128 -11.925541 -11.84993 -11.507607 -9.8378782 -8.8907232 -7.7462058 -8.0485992 -11.376074 -8.93015 -9.15593 -8.5152655][-6.2677298 -7.7651458 -9.9301605 -11.220757 -13.142582 -12.778572 -11.962845 -9.4606047 -8.277771 -8.1979752 -8.7549887 -12.387067 -11.290505 -11.457375 -10.718653][-5.8809962 -7.2283463 -9.1554222 -10.179077 -11.697125 -11.103668 -10.049735 -8.5993519 -7.2099848 -6.8214059 -6.9312243 -11.175638 -11.191597 -12.291224 -12.286247][-5.6058388 -6.6395607 -8.1855564 -8.3150511 -9.2709961 -8.8331022 -7.7497921 -7.0147109 -6.34533 -7.2165093 -6.7107921 -10.915172 -10.077211 -11.445331 -12.37204][-5.3389864 -6.1374483 -7.2009239 -6.5960312 -6.26449 -4.4365039 -3.7380967 -3.5595779 -3.100533 -5.5090818 -7.3307304 -12.352301 -10.75351 -11.780818 -12.054268][-6.2727351 -7.193428 -7.0877924 -5.4582314 -4.5420532 -2.1130829 0.71543884 2.2352161 2.8857241 -1.2473817 -4.1472044 -10.627211 -11.359962 -12.699091 -11.932361][-5.9610496 -6.4546223 -6.5891051 -4.1797457 -2.6958313 0.62750149 3.17447 4.5386229 5.5855951 3.0686493 -0.18150568 -8.2186289 -10.468101 -12.810913 -12.582995][-6.6037264 -6.0158873 -5.7858448 -3.3575349 -1.634799 1.7335758 4.8649778 4.9111118 4.4071579 2.9066725 0.63305855 -5.9066606 -8.4883938 -12.266633 -13.744251][-8.2184114 -7.10245 -6.3140497 -3.9060366 -2.4204912 0.34843254 2.5020256 3.3734312 2.8972273 -0.36799335 -2.8906584 -7.9862876 -8.4039211 -11.163495 -12.691573][-9.9191036 -9.38508 -8.4441652 -6.343812 -4.6880388 -2.9219337 -1.1673484 0.281456 0.61669827 -2.5779643 -5.6546888 -11.010075 -11.012292 -11.742293 -11.487933][-10.58463 -10.62899 -10.583313 -8.5046616 -6.8133698 -5.2557611 -3.6177721 -3.6468873 -3.1997485 -4.7220335 -5.8789487 -10.902448 -12.232153 -12.445944 -11.776847][-9.0015335 -9.2967854 -10.224896 -9.1978216 -8.342474 -6.6001043 -5.443264 -5.3680639 -5.6946545 -7.1784654 -7.0268755 -8.9041786 -10.555992 -11.37949 -11.112073][-9.3668308 -8.7281437 -8.3948116 -8.0490446 -7.6159949 -7.1448393 -6.2566748 -5.9843974 -6.7602158 -8.0115833 -8.0773811 -8.6501436 -8.2482738 -8.161315 -8.3605824][-8.2845659 -8.4834242 -7.6671491 -6.5960932 -6.2856784 -5.7171574 -5.6552734 -6.0159173 -6.4555497 -7.2184615 -7.6183472 -7.9174528 -7.6677947 -6.8188262 -6.2053223][-7.2982159 -8.2126389 -8.48069 -8.1772137 -6.3921623 -5.9273844 -6.0158319 -6.1745276 -6.831531 -7.7761326 -8.0945559 -8.2366905 -7.9523635 -7.4300442 -6.8666115]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:16:12.480177: step 39010, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 51h:38m:32s remains)
INFO - root - 2017-12-15 17:16:18.927323: step 39020, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 51h:40m:20s remains)
INFO - root - 2017-12-15 17:16:25.382313: step 39030, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 52h:52m:54s remains)
INFO - root - 2017-12-15 17:16:31.752962: step 39040, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 54h:08m:03s remains)
INFO - root - 2017-12-15 17:16:38.157163: step 39050, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 52h:04m:39s remains)
INFO - root - 2017-12-15 17:16:44.476773: step 39060, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 51h:39m:20s remains)
INFO - root - 2017-12-15 17:16:50.910186: step 39070, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 53h:22m:32s remains)
INFO - root - 2017-12-15 17:16:57.387828: step 39080, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 51h:08m:58s remains)
INFO - root - 2017-12-15 17:17:03.758694: step 39090, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 52h:44m:29s remains)
INFO - root - 2017-12-15 17:17:10.190936: step 39100, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.630 sec/batch; 51h:22m:12s remains)
2017-12-15 17:17:10.719162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8598242 -5.2511649 -5.406106 -5.1121273 -4.8491526 -4.3074074 -3.8070385 -3.4765306 -3.7159359 -6.4130888 -6.0407271 -6.4522657 -7.0297613 -7.270432 -7.5178823][-5.0441279 -5.5792508 -5.8265958 -5.1239104 -4.4858189 -4.1841888 -3.870733 -3.2903576 -3.1162477 -5.9295177 -5.8480349 -6.3854766 -6.9768157 -7.3573904 -7.9157515][-5.2036572 -5.1815243 -5.3974476 -5.1924124 -4.3890018 -3.9695883 -3.8176908 -3.7463679 -3.6729198 -5.9791551 -5.4750333 -6.0734053 -6.8138704 -7.1259789 -7.76084][-6.6248641 -6.1180396 -5.9827123 -4.8726053 -3.3793111 -2.8314915 -2.5251899 -2.4639621 -2.4582686 -5.1648636 -4.7331691 -5.361712 -6.3208361 -6.6337862 -7.0394034][-7.2744884 -6.1871142 -5.6426539 -4.7865143 -3.042376 -1.1099076 0.18775702 0.1422267 -0.31034613 -3.0686836 -2.8835249 -3.8985136 -4.8425751 -5.3684764 -6.3782692][-8.5544214 -7.6103883 -7.2426429 -5.086956 -2.661221 -1.0790858 0.54388618 1.5900497 1.9681187 -0.83653212 -0.83590746 -1.8207049 -3.1401496 -3.9723618 -5.2611847][-9.3731422 -7.9192762 -6.4260092 -3.86744 -1.8058076 -0.099784851 1.7486219 2.7408419 3.0462971 0.24199772 0.24154425 -0.7542696 -1.7557821 -2.2207403 -3.8471737][-9.40279 -8.2383747 -6.8639607 -3.2621083 -0.4376483 0.89516544 2.516058 4.0354729 4.5771561 1.4240484 1.0002899 0.0415287 -1.0128417 -1.7487121 -3.2281113][-8.5433483 -8.2693939 -7.5115347 -4.6130142 -2.7156191 -0.66867924 1.8395863 3.0170593 3.6279154 1.225955 1.3535595 0.063769817 -1.2842522 -1.8250704 -3.0486445][-9.0966015 -8.71599 -7.9263754 -5.6836004 -3.8750815 -2.1019568 -0.38070774 1.2732983 2.4428864 -0.0010967255 -0.09733057 -1.1046576 -2.2501912 -2.9603186 -3.7761729][-9.4081955 -9.0029249 -9.084074 -7.7588782 -6.5566149 -4.7581224 -2.8678007 -2.1385298 -1.5604663 -3.669909 -3.6385336 -4.2405338 -4.8643074 -5.2534847 -5.6978474][-10.498758 -9.7209921 -9.3472729 -9.05087 -8.6070976 -6.9328609 -5.5089569 -4.548461 -3.7509441 -5.3994269 -5.3679409 -5.7460318 -6.2811251 -6.3762407 -6.4246025][-9.7878265 -9.7092781 -10.069611 -9.7843437 -8.6228132 -7.7134166 -6.9637837 -6.1442318 -5.3786564 -6.2222452 -6.0362744 -6.079392 -6.4639988 -6.3287234 -6.2334576][-9.0096931 -8.7296162 -9.3699694 -9.7759047 -9.4283829 -8.5559206 -8.01289 -7.7862959 -7.2437077 -7.2029862 -7.1501474 -6.9035006 -6.7753744 -6.6228952 -6.4799075][-9.37438 -8.3663845 -8.4218311 -9.1121244 -9.19808 -8.9570389 -8.5364866 -8.463933 -8.1978054 -7.9510722 -7.7489738 -7.5919523 -7.7414284 -7.7138333 -7.3677263]]...]
INFO - root - 2017-12-15 17:17:17.137712: step 39110, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 53h:57m:53s remains)
INFO - root - 2017-12-15 17:17:23.554698: step 39120, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 53h:13m:33s remains)
INFO - root - 2017-12-15 17:17:30.000181: step 39130, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.659 sec/batch; 53h:42m:26s remains)
INFO - root - 2017-12-15 17:17:36.478481: step 39140, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 52h:49m:32s remains)
INFO - root - 2017-12-15 17:17:42.877467: step 39150, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 51h:34m:14s remains)
INFO - root - 2017-12-15 17:17:49.274255: step 39160, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 51h:09m:49s remains)
INFO - root - 2017-12-15 17:17:55.749621: step 39170, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.668 sec/batch; 54h:23m:41s remains)
INFO - root - 2017-12-15 17:18:02.215365: step 39180, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 52h:19m:55s remains)
INFO - root - 2017-12-15 17:18:08.574844: step 39190, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:51m:25s remains)
INFO - root - 2017-12-15 17:18:14.923670: step 39200, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 51h:29m:26s remains)
2017-12-15 17:18:15.410607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.795104 -4.6387577 -4.4006724 -4.2395792 -3.5859461 -2.4054708 -1.5302753 -1.2546091 -0.74131632 -2.2230148 -2.9728346 -3.8220577 -4.2550755 -5.8070006 -6.2576675][-3.9376676 -4.1113834 -4.1343188 -3.6828513 -3.0468264 -2.4878855 -1.7743382 -0.93352032 -0.35082006 -2.4042559 -2.8871355 -3.5583506 -4.4603596 -5.9886065 -5.9777036][-3.872036 -3.8476138 -4.0345664 -4.0498943 -3.7388608 -2.9816017 -2.1976833 -1.6109076 -1.2008867 -2.8903127 -3.6914518 -4.6032791 -5.2431326 -6.2892227 -6.1586833][-2.9180532 -2.7058992 -2.799962 -2.6834116 -2.4710555 -2.2178845 -1.8093162 -1.2809539 -0.76026058 -2.6677032 -3.6734538 -4.441483 -5.2690887 -6.6470528 -6.7252564][-3.7224743 -2.7065072 -2.274991 -2.027669 -1.4245758 -0.87403393 -0.521111 -0.39167166 -0.034840107 -2.1067119 -3.2356339 -4.5502405 -5.5494471 -6.5169148 -6.605659][-3.6939075 -2.9240851 -2.1260862 -1.0444698 -0.33048248 0.19418955 0.11287403 0.27274132 0.430274 -1.5395222 -2.5819569 -3.7714396 -4.752 -6.1060009 -6.4879909][-3.549962 -2.3626895 -1.232532 0.22521973 1.7373114 2.5080862 2.6085739 2.0665092 1.4600229 -0.84118032 -2.0522861 -3.1945119 -4.2710223 -5.8121147 -5.9815655][-2.5643673 -1.5360136 -0.7366209 1.3225145 2.9326515 3.6550369 3.8702517 3.8646498 3.4332886 0.49486446 -1.2481914 -2.7574015 -4.1503735 -5.6775012 -5.87934][-1.428267 -0.61899805 -0.12738132 1.3025208 2.1195383 2.8529425 3.2919331 3.5308018 3.0579872 0.22788763 -1.3602781 -2.8092413 -4.0451317 -5.44506 -5.7779231][-1.7001486 -1.1866045 -0.80131292 0.16910648 0.95385265 1.4647827 1.9113646 2.0836344 1.8557529 -0.63293266 -2.0924315 -3.5343809 -4.6102872 -5.8503666 -5.8932953][-4.019444 -3.5961127 -3.08082 -1.9959421 -1.6286883 -1.0076737 -0.61415195 -0.65615416 -0.97666311 -3.1270537 -4.10159 -5.1001406 -5.9769516 -6.9583898 -6.49414][-6.0699635 -4.9669924 -4.4515848 -4.401372 -4.1408272 -3.6355109 -3.4997053 -3.2779608 -3.1925573 -4.9365277 -5.508966 -6.1570845 -6.4951792 -7.3280611 -7.2301588][-7.4520473 -7.2010064 -6.7217689 -6.4298496 -6.0330729 -5.5454016 -5.365417 -5.1865425 -5.0618849 -6.2036748 -6.627212 -7.2100549 -7.447763 -7.6289907 -7.1088295][-8.1643257 -8.2287178 -8.0085506 -7.8735471 -7.2988787 -6.7385492 -6.4883828 -6.2324271 -6.1606312 -6.6720619 -6.6097555 -7.3352485 -7.6595531 -7.5908146 -7.193047][-8.998024 -8.6948662 -8.5790319 -8.4539433 -8.0888977 -7.7847075 -7.5676785 -7.5247474 -7.3928647 -7.0263219 -6.6953969 -6.7279778 -6.5450287 -6.6019673 -6.5704107]]...]
INFO - root - 2017-12-15 17:18:21.834935: step 39210, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 52h:46m:29s remains)
INFO - root - 2017-12-15 17:18:28.232783: step 39220, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 51h:46m:06s remains)
INFO - root - 2017-12-15 17:18:34.578631: step 39230, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 52h:38m:04s remains)
INFO - root - 2017-12-15 17:18:41.022361: step 39240, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 52h:02m:56s remains)
INFO - root - 2017-12-15 17:18:47.464737: step 39250, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 53h:07m:44s remains)
INFO - root - 2017-12-15 17:18:53.822684: step 39260, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 53h:29m:20s remains)
INFO - root - 2017-12-15 17:19:00.173136: step 39270, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 50h:33m:26s remains)
INFO - root - 2017-12-15 17:19:06.543910: step 39280, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 52h:15m:07s remains)
INFO - root - 2017-12-15 17:19:12.958046: step 39290, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 53h:17m:58s remains)
INFO - root - 2017-12-15 17:19:19.345156: step 39300, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 50h:51m:10s remains)
2017-12-15 17:19:19.889619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6896834 -2.9613566 -3.0247383 -3.121242 -3.5111485 -4.1568451 -4.4642968 -4.4246478 -4.4703989 -5.5259285 -6.6560259 -7.34629 -7.3599806 -7.8173323 -7.8767328][-2.6577325 -2.9166746 -3.292181 -3.8446226 -4.2609768 -4.5792446 -4.9179497 -5.4452171 -5.5824757 -6.5656734 -7.415904 -8.2040443 -8.4385548 -8.946269 -9.0319948][-1.9226837 -1.9614091 -1.9935093 -2.3675795 -2.7299924 -3.1311526 -3.5098543 -4.0948153 -4.4721107 -5.6300764 -6.8110037 -7.5976954 -8.0821657 -8.7894354 -9.0554457][-1.4249177 -1.5539513 -1.3878512 -1.2775507 -1.5797834 -1.753284 -1.8704977 -2.28963 -2.6032481 -4.078362 -5.4468851 -6.3592157 -6.895824 -7.9254303 -8.54701][-1.4708252 -1.3139977 -1.0901785 -0.95494843 -0.711081 -0.54111624 -0.46140528 -0.73027658 -0.84433794 -2.1016793 -3.4596972 -4.708312 -5.4653473 -6.5799208 -7.300386][-2.5390139 -1.8967943 -1.3046579 -0.95392895 -0.4978056 0.081159115 0.59029675 0.69772339 0.93320751 -0.29680824 -1.6588087 -2.9410138 -3.9416559 -5.2409763 -6.0988636][-3.2488933 -2.5465264 -1.6778154 -0.80646706 0.047500134 0.659626 1.1324406 1.7002602 2.3306122 1.160408 -0.28379869 -1.3826971 -2.2367206 -3.8508048 -5.0974789][-3.5713086 -2.8180318 -2.0189247 -0.89291382 0.17854309 1.0984888 1.5682707 2.12049 2.9832335 2.105751 0.86102104 -0.45996571 -1.4057469 -2.8137307 -3.9336627][-3.6269531 -2.8550272 -2.2800493 -1.2675076 -0.35714054 0.7010107 1.4414253 1.8335714 2.278656 1.3265142 0.092906475 -1.1482501 -1.9354396 -3.2552195 -4.2891245][-4.1201658 -3.5109425 -2.9109697 -2.0455265 -1.1460595 -0.3394022 0.39430714 1.1173038 1.6003761 -0.090517521 -1.5279403 -2.8571577 -3.700664 -4.7756224 -5.4522772][-5.4572191 -4.7847204 -4.1171551 -3.4207311 -2.7060761 -2.058918 -1.2959008 -0.78368044 -0.572258 -1.991724 -3.4247665 -4.5005569 -4.9437323 -5.8740606 -6.489099][-5.5043993 -5.2809486 -4.6391077 -3.9548204 -3.3865986 -2.6518869 -2.1166797 -2.084341 -1.9064097 -2.9047985 -4.1055098 -5.2677374 -5.6750727 -6.3355865 -6.4852262][-6.2072382 -5.6852493 -5.0998411 -4.4982548 -4.0056119 -3.3728485 -3.0875568 -2.9208326 -2.86624 -3.7452774 -4.7055902 -5.3533068 -5.9182439 -6.458602 -6.173471][-6.0310364 -5.5433774 -4.73139 -4.0899324 -3.6762853 -3.1374383 -2.9963732 -3.0721483 -3.2630138 -3.9633839 -4.419096 -4.9005613 -5.4082079 -6.1008115 -5.981442][-6.9410887 -6.5496902 -5.8360138 -5.2474594 -4.6871061 -3.9517758 -3.7407904 -3.8896399 -4.127635 -4.3541451 -4.6270533 -5.17148 -5.791647 -5.8123035 -5.6928449]]...]
INFO - root - 2017-12-15 17:19:26.388389: step 39310, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 51h:26m:03s remains)
INFO - root - 2017-12-15 17:19:32.775778: step 39320, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 52h:20m:03s remains)
INFO - root - 2017-12-15 17:19:39.223495: step 39330, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 54h:22m:43s remains)
INFO - root - 2017-12-15 17:19:45.671584: step 39340, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 52h:40m:04s remains)
INFO - root - 2017-12-15 17:19:51.935698: step 39350, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 50h:38m:17s remains)
INFO - root - 2017-12-15 17:19:58.330650: step 39360, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 52h:15m:14s remains)
INFO - root - 2017-12-15 17:20:04.676467: step 39370, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 52h:11m:41s remains)
INFO - root - 2017-12-15 17:20:11.170399: step 39380, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:41m:45s remains)
INFO - root - 2017-12-15 17:20:17.517471: step 39390, loss = 0.23, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 52h:28m:13s remains)
INFO - root - 2017-12-15 17:20:24.075067: step 39400, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 53h:23m:37s remains)
2017-12-15 17:20:24.622216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8624163 -5.6546855 -6.1052084 -6.8702517 -7.6218014 -7.6045418 -7.1476789 -6.3350649 -5.0550432 -5.8769507 -5.6808505 -4.7858725 -5.1935086 -7.0440702 -7.9614964][-4.907217 -5.4184566 -6.4505682 -7.3867674 -8.1099739 -8.4286852 -8.7272854 -8.7012 -8.4338255 -9.5385494 -8.9985046 -8.0467463 -8.4443846 -9.4870987 -9.4723263][-5.2166319 -5.5457473 -6.7059431 -7.05896 -8.2017555 -8.5087929 -8.396965 -8.8444395 -8.8146172 -10.477334 -11.052946 -11.055061 -11.147167 -11.862211 -11.203795][-6.1277819 -5.95521 -6.0303869 -5.8972282 -6.417295 -6.1001453 -5.6775169 -5.8078451 -5.5512371 -8.0002546 -9.0620413 -10.119383 -11.56376 -12.994312 -12.644114][-6.2654123 -5.9485993 -5.439343 -5.1702356 -5.2482529 -3.9717145 -2.8104172 -2.392108 -1.6729937 -4.1602306 -5.7139778 -7.2335062 -9.1711311 -11.385264 -12.149714][-7.1353426 -6.4378204 -6.0124793 -4.8836279 -3.4578528 -1.6425214 -0.36614227 0.66793442 1.6559248 -1.1249022 -3.2061749 -4.8586884 -6.7952557 -9.2834826 -10.681993][-5.608325 -4.3786621 -3.0008273 -1.0073214 0.95072556 2.983181 5.1236744 5.9639969 5.8374491 2.297245 0.086195946 -1.7149005 -4.1254516 -6.7655282 -8.3200178][-4.1087418 -2.4894843 -0.79279947 1.3684912 3.8249846 6.1049595 7.9087009 8.1707211 7.8633308 4.0648756 1.4748898 -0.5162673 -3.107182 -5.9908285 -7.5670052][-4.6499691 -3.0925226 -1.8975945 0.079308987 2.2384853 3.7763662 5.5961285 6.0732994 5.5214376 1.1928291 -1.7988243 -2.86585 -4.3744531 -6.54732 -7.6726861][-7.0211234 -5.9469686 -5.078506 -3.0863824 -1.3118954 0.16381931 2.4947748 2.3240719 1.4873095 -2.7283049 -5.1489325 -6.449811 -7.9270883 -8.6380091 -8.5907707][-11.156165 -10.291306 -9.4038153 -7.5789785 -6.555057 -4.242485 -1.849442 -2.4703631 -2.0889688 -5.3054743 -6.8539138 -7.6306849 -8.7393713 -9.5203581 -9.5730362][-11.963831 -11.330564 -11.085962 -10.601339 -9.7108822 -7.5493212 -6.033031 -5.7302136 -5.262002 -7.7598691 -8.0995808 -8.0471926 -8.8389 -9.0593805 -9.0400152][-11.205879 -11.254958 -11.304595 -11.157225 -10.946548 -9.8013315 -8.90999 -8.4628782 -8.1743526 -9.7112818 -9.7998114 -9.5520687 -9.582655 -9.1242542 -8.894701][-10.245109 -10.461049 -10.505652 -10.353168 -10.146769 -9.4304667 -8.5621967 -8.4908438 -8.2889118 -9.2137165 -8.72794 -8.7942486 -9.1162386 -8.4270544 -7.9307046][-9.7945328 -9.9119539 -10.34297 -10.434224 -10.264254 -9.689662 -8.9847364 -8.9390306 -9.0809078 -9.0827169 -8.6006689 -8.2029152 -7.8287673 -7.7660375 -7.5011077]]...]
INFO - root - 2017-12-15 17:20:30.967402: step 39410, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 50h:50m:51s remains)
INFO - root - 2017-12-15 17:20:37.332669: step 39420, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 51h:14m:38s remains)
INFO - root - 2017-12-15 17:20:43.701864: step 39430, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 51h:00m:25s remains)
INFO - root - 2017-12-15 17:20:50.136827: step 39440, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 52h:49m:36s remains)
INFO - root - 2017-12-15 17:20:56.568145: step 39450, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.678 sec/batch; 55h:10m:49s remains)
INFO - root - 2017-12-15 17:21:02.978900: step 39460, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:53m:55s remains)
INFO - root - 2017-12-15 17:21:09.384392: step 39470, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 52h:31m:03s remains)
INFO - root - 2017-12-15 17:21:15.722657: step 39480, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 52h:41m:46s remains)
INFO - root - 2017-12-15 17:21:22.169965: step 39490, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 51h:54m:34s remains)
INFO - root - 2017-12-15 17:21:28.520894: step 39500, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 51h:12m:32s remains)
2017-12-15 17:21:29.069364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2459598 -4.1696172 -4.5572023 -4.4314718 -4.0975304 -3.8699834 -3.2727666 -2.4992123 -1.6508846 -2.8836708 -3.7508063 -5.4882522 -5.5992408 -5.5341845 -6.2787628][-4.2529125 -4.5109348 -4.6998348 -5.1390204 -5.2753272 -4.8310175 -3.8254459 -2.5157871 -1.5689831 -2.7810946 -4.1952791 -6.3983078 -7.0075207 -6.93324 -7.9838328][-3.4877753 -4.2308054 -4.7036572 -4.6800227 -4.524044 -4.4944429 -4.1454082 -3.4761581 -2.5292921 -2.9343734 -3.8426652 -5.8036571 -6.0312214 -6.3444118 -6.8377018][-3.3269525 -3.5993152 -3.5816932 -3.4021544 -3.2660437 -3.0384402 -2.7936239 -2.5177379 -2.0977306 -3.1208062 -4.0867519 -5.71121 -6.384738 -6.6532936 -7.2790661][-4.2823181 -4.4718976 -4.0231605 -3.1150408 -2.4250617 -2.3039932 -1.6290555 -1.6905417 -1.6711268 -2.7248168 -3.5056238 -5.5442352 -6.2097807 -6.7286634 -7.8772669][-5.3634872 -4.9062638 -3.9775724 -2.4892988 -1.4053826 0.061295986 0.60553074 0.2944808 0.41326809 -1.3103094 -2.5052786 -4.7674747 -5.640974 -6.3450289 -7.6415396][-5.937418 -5.8458838 -4.697979 -2.5145507 -0.58658791 1.1677055 1.9953003 1.8921499 2.0096569 0.19619417 -0.98591805 -3.6768003 -5.0063591 -5.9337254 -7.36613][-5.4223061 -5.2102251 -4.3392696 -2.4413877 -0.53420591 1.6087332 3.2036085 3.311923 3.4942417 1.4708157 -0.036637306 -3.0176897 -4.399806 -5.796072 -7.57427][-4.6334357 -4.7958632 -4.1723652 -3.1281915 -1.8162384 0.17015743 1.4038 2.1295204 2.7156763 0.85288811 -0.45047998 -3.2263546 -4.4519577 -5.5120344 -7.20586][-4.0279083 -3.9683254 -3.8128231 -3.5459375 -2.5899715 -1.5767198 -0.84181309 -0.36991215 0.22841215 -1.3367243 -2.1234541 -4.2461696 -4.8901844 -5.9576116 -7.2751837][-5.2072625 -4.7795563 -4.3262081 -3.8885252 -3.3001623 -2.6952624 -2.2165098 -2.2372937 -2.2296734 -3.4543133 -4.2103848 -5.8193064 -5.8620386 -6.2372403 -7.5070686][-5.0713053 -4.7529469 -4.2618408 -3.8213787 -3.3910871 -3.2126427 -3.09066 -3.2878094 -3.3463402 -4.3307743 -4.9022055 -6.4117222 -6.1252871 -6.433044 -6.929554][-6.6132727 -6.1079144 -5.3399925 -4.7622156 -4.5427427 -4.3491936 -4.4612613 -4.5037231 -4.6834574 -5.6078939 -6.2293167 -6.731976 -6.5443492 -6.6639571 -7.1883731][-6.445025 -6.2490788 -5.998795 -5.2318916 -4.6036544 -3.9793608 -3.607604 -3.8292525 -4.1494207 -5.0822077 -5.5597873 -5.8712726 -5.767982 -5.935101 -5.9083667][-7.0940909 -7.059762 -7.0157008 -6.5335908 -6.122448 -5.4964924 -5.0648675 -4.9108028 -4.8026476 -5.1384134 -5.7550554 -5.8793855 -5.896862 -5.7282705 -5.7135887]]...]
INFO - root - 2017-12-15 17:21:35.587538: step 39510, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 53h:25m:57s remains)
INFO - root - 2017-12-15 17:21:42.010068: step 39520, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 51h:14m:22s remains)
INFO - root - 2017-12-15 17:21:48.335503: step 39530, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.639 sec/batch; 51h:58m:48s remains)
INFO - root - 2017-12-15 17:21:54.789758: step 39540, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 50h:47m:53s remains)
INFO - root - 2017-12-15 17:22:01.086968: step 39550, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:55m:42s remains)
INFO - root - 2017-12-15 17:22:07.505946: step 39560, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 52h:38m:00s remains)
INFO - root - 2017-12-15 17:22:13.890129: step 39570, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 52h:28m:09s remains)
INFO - root - 2017-12-15 17:22:20.265071: step 39580, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 51h:59m:23s remains)
INFO - root - 2017-12-15 17:22:26.701930: step 39590, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 51h:59m:28s remains)
INFO - root - 2017-12-15 17:22:33.093402: step 39600, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 50h:33m:41s remains)
2017-12-15 17:22:33.599370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0920048 -3.4505749 -3.6248269 -3.7588775 -4.0041571 -3.9760103 -3.3731837 -2.7803435 -1.9400196 -2.7905011 -3.4750495 -3.5513096 -3.001431 -5.4074845 -7.4068446][-4.5087676 -4.2928343 -3.9497771 -4.3522034 -4.5946336 -3.9928987 -3.704318 -2.8820362 -1.812284 -2.1325779 -2.5782032 -3.9437931 -4.74975 -5.4293532 -6.1788397][-5.5239458 -4.7066345 -4.2317467 -4.446588 -4.00591 -3.3444219 -3.2974377 -2.6507421 -1.9711885 -2.6347227 -2.8992786 -3.349267 -3.6643786 -5.0960178 -6.0811329][-5.2384262 -4.9089537 -4.2383018 -4.0662422 -3.6215429 -2.3886175 -1.5627394 -1.4811783 -1.2506671 -2.1931143 -2.9620223 -4.0628648 -4.2506552 -4.8491468 -5.9960423][-5.0033255 -4.359817 -3.6135397 -3.3281021 -2.8685083 -1.8395982 -0.93848085 -0.39034271 -0.28228283 -1.7686954 -2.7632589 -4.219501 -4.7294493 -5.648612 -6.9486609][-3.9871249 -4.1404591 -3.9673877 -2.7238564 -1.4755635 -0.06846571 1.0314407 1.2875776 0.92941952 -1.1061325 -2.8380489 -4.5159006 -5.1718826 -6.098155 -6.7464809][-4.5331736 -3.816361 -2.7035441 -1.8604426 -1.0617108 0.92186832 2.1602516 2.2697096 1.990243 -0.059902668 -2.2209058 -4.3918805 -5.267128 -6.1517906 -6.9077969][-3.4361377 -3.0519614 -2.1610565 -0.5414362 0.89515877 1.9995422 2.7028913 3.0794058 3.0265017 0.58470535 -1.6618328 -3.7860327 -5.2307291 -6.3202429 -7.175621][-2.8378963 -2.1217284 -1.4888396 -0.4884758 0.94985962 2.7157621 3.3563557 3.4166384 3.3258934 0.83503723 -1.4981122 -3.4712405 -4.7655144 -6.1143708 -7.2745576][-3.5969181 -2.6690583 -1.6601572 -0.42803812 0.40817451 1.0971184 1.6832008 2.1720343 2.3072786 -0.32262516 -1.9735436 -3.6893144 -4.9353604 -6.1295424 -7.3326607][-5.1038275 -4.4329319 -3.7931495 -2.9280272 -2.1456876 -1.3359957 -1.1570864 -1.3387256 -1.4349532 -3.249671 -4.2714009 -5.1439934 -5.7591367 -6.6447721 -7.7465658][-6.0663595 -5.8298054 -5.3087015 -4.6150079 -3.7650011 -2.926877 -2.9116545 -2.9511533 -2.9969211 -4.2061071 -5.1598697 -5.96146 -6.3594742 -6.5414753 -6.9905605][-6.6651483 -6.3589935 -5.6688147 -5.1938648 -4.610199 -4.1152167 -4.1511393 -4.3932486 -4.4623952 -5.028842 -5.85088 -6.1129465 -5.9821796 -6.1167922 -6.7224717][-5.2618442 -5.085165 -5.0071554 -4.9553518 -4.67979 -4.757925 -4.5694494 -4.5348086 -4.8633437 -5.3996019 -6.0119662 -6.12288 -5.8932981 -5.9786386 -6.7378535][-4.87976 -4.5634265 -4.8133349 -4.97132 -5.1381993 -5.19728 -5.2190933 -5.4539852 -5.5950794 -5.981575 -6.4012113 -6.6121421 -6.939487 -6.8909564 -6.5808949]]...]
INFO - root - 2017-12-15 17:22:39.974732: step 39610, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 51h:52m:27s remains)
INFO - root - 2017-12-15 17:22:46.300214: step 39620, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 50h:42m:08s remains)
INFO - root - 2017-12-15 17:22:52.610102: step 39630, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:38m:36s remains)
INFO - root - 2017-12-15 17:22:59.013551: step 39640, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:41m:54s remains)
INFO - root - 2017-12-15 17:23:05.331732: step 39650, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 50h:20m:46s remains)
INFO - root - 2017-12-15 17:23:11.702000: step 39660, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 51h:04m:15s remains)
INFO - root - 2017-12-15 17:23:18.003529: step 39670, loss = 0.25, batch loss = 0.13 (13.0 examples/sec; 0.616 sec/batch; 50h:04m:13s remains)
INFO - root - 2017-12-15 17:23:24.446202: step 39680, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 54h:42m:28s remains)
INFO - root - 2017-12-15 17:23:30.774066: step 39690, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 51h:56m:21s remains)
INFO - root - 2017-12-15 17:23:37.105777: step 39700, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 51h:00m:36s remains)
2017-12-15 17:23:37.586881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0295248 -5.3156996 -5.7054186 -5.9573073 -6.2036309 -6.3966866 -6.334197 -5.5748711 -4.1134367 -4.1388025 -4.9053 -4.6972623 -4.4953065 -5.4469318 -6.3452892][-5.3544788 -5.4856234 -5.8283563 -6.0816178 -6.328805 -6.4444032 -6.7892566 -6.4147844 -5.2489977 -5.5345287 -6.1440396 -6.0118618 -6.0934429 -6.4546127 -6.6880817][-6.7765646 -6.6939797 -6.5311623 -6.3268657 -6.2485237 -6.1212354 -6.5169 -6.3056488 -5.7173223 -6.5012336 -7.3307266 -7.1936712 -7.1529918 -7.5293331 -7.502409][-7.9068623 -7.3680291 -6.7031665 -5.4601245 -4.5485268 -3.8361027 -3.7275918 -3.6007571 -3.3936086 -4.6226449 -6.1272488 -6.7049093 -7.2042704 -7.9335661 -8.2423449][-8.3204069 -7.5297685 -6.3754888 -4.7847729 -3.4173756 -1.9529924 -1.3644848 -1.1009755 -0.68096066 -2.0829315 -3.7932694 -4.7983823 -6.0435123 -7.7097235 -8.6957664][-8.2719917 -6.9663868 -5.2808971 -3.144424 -1.3181286 0.39186478 1.5507679 2.0294838 2.4116611 1.0359678 -0.59812021 -2.016624 -3.8506036 -6.0126314 -7.5209894][-7.2038274 -6.0699034 -4.16592 -1.7947955 0.38765335 2.2356596 3.5987091 4.4401655 5.0812435 3.628583 1.4930601 -0.30103493 -2.0299478 -4.49842 -6.0232344][-5.9272261 -4.8518715 -3.2940679 -1.268961 0.70547104 2.1397438 3.211277 4.53961 5.54953 4.0874748 1.8140659 -0.14659262 -1.8928547 -4.0407553 -5.1250567][-6.0610266 -5.0289288 -3.72846 -1.6734838 0.20208836 1.1791611 2.1045618 3.39993 4.4357452 2.8113136 0.47563839 -1.5904226 -3.370677 -5.0963383 -5.6511111][-7.0764608 -5.8547349 -4.3404546 -2.5480928 -0.95401335 -0.29527044 0.19411945 1.1849365 1.7658863 0.11420536 -1.8577666 -3.3428059 -4.647933 -6.1922493 -6.8184743][-8.8056774 -8.140564 -7.1002264 -5.4070292 -3.9079111 -3.1950822 -2.6147404 -2.2932448 -2.2537041 -3.4332666 -5.0836935 -5.9929409 -6.5980921 -7.4191136 -7.7577009][-9.1535273 -8.7905664 -8.1058464 -6.8521976 -5.5570116 -4.5579929 -3.74269 -3.6143856 -3.5915575 -4.9035358 -6.3835497 -6.57798 -6.7987614 -7.5102687 -7.3554025][-9.7947826 -9.5641747 -9.1565514 -8.34223 -7.2456708 -6.1738205 -5.5411434 -5.4587808 -5.4239845 -6.147182 -7.1416345 -6.7824025 -6.4592304 -6.5994234 -6.2317286][-8.99096 -8.594676 -7.8869548 -7.2877135 -6.6737738 -5.850419 -5.2980289 -5.3102846 -5.4284377 -5.8861341 -6.3940048 -6.340332 -5.9873075 -5.8525538 -5.5967693][-7.9557056 -7.547637 -6.7826104 -6.3883972 -5.9421735 -5.5923591 -5.4011517 -5.5584254 -5.7000432 -5.9176068 -6.2980533 -6.3772321 -6.2015982 -6.1547847 -5.7488446]]...]
INFO - root - 2017-12-15 17:23:43.974579: step 39710, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 51h:27m:59s remains)
INFO - root - 2017-12-15 17:23:50.359760: step 39720, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 51h:37m:35s remains)
INFO - root - 2017-12-15 17:23:56.840262: step 39730, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 50h:47m:00s remains)
INFO - root - 2017-12-15 17:24:03.173793: step 39740, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 52h:22m:31s remains)
INFO - root - 2017-12-15 17:24:09.473904: step 39750, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:43m:45s remains)
INFO - root - 2017-12-15 17:24:15.818348: step 39760, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 51h:18m:10s remains)
INFO - root - 2017-12-15 17:24:22.363813: step 39770, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 50h:25m:27s remains)
INFO - root - 2017-12-15 17:24:28.786600: step 39780, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 53h:08m:26s remains)
INFO - root - 2017-12-15 17:24:35.179480: step 39790, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 52h:00m:08s remains)
INFO - root - 2017-12-15 17:24:41.499487: step 39800, loss = 0.31, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 50h:34m:44s remains)
2017-12-15 17:24:41.997302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3798428 -4.1637893 -5.1045666 -5.19271 -5.2830715 -5.3245215 -5.4903708 -4.6359491 -3.9661171 -3.8412731 -4.6847339 -5.8772907 -7.4731283 -9.1046181 -9.5063477][-3.6801982 -4.3574853 -5.2988253 -5.8367319 -6.9077892 -6.1873569 -5.3668814 -4.2621145 -3.8244 -5.0664682 -6.576746 -8.1502743 -9.5265713 -10.360964 -10.519373][-3.7744241 -4.3189435 -5.0822058 -5.1060162 -4.7764606 -4.8880224 -4.5670114 -4.1458683 -3.1366658 -3.35004 -5.2526169 -7.3872204 -9.5385628 -10.497632 -10.34131][-4.2318945 -4.6318917 -4.87223 -4.69176 -4.3059931 -3.168992 -1.9908876 -1.1581044 -0.51314449 -1.4883418 -3.422 -6.1579146 -8.6761284 -10.39625 -10.516796][-4.3617959 -4.7239666 -4.4716935 -3.9724264 -2.7208962 -0.77683973 0.82849979 1.6650152 2.138236 0.40639305 -1.593883 -4.3378038 -7.2436762 -8.505703 -8.8961563][-5.0542011 -4.3533406 -3.763315 -2.7810574 -1.6257601 -0.011331081 2.0529337 3.7187729 4.2116175 2.6351423 -0.2278738 -3.2274413 -6.0383697 -7.2277741 -7.6745143][-5.4316912 -5.0178909 -3.5446496 -2.0026197 0.086334229 1.777154 3.6655073 4.9368181 5.6847029 4.1804323 1.2551289 -2.2113953 -5.9202609 -7.6470323 -7.713376][-4.493474 -3.8141627 -2.8989534 -1.021493 1.1696491 3.54663 5.9491463 6.8532763 6.223567 4.0957632 1.187603 -1.9964838 -5.5562773 -7.5096722 -7.785028][-4.7697091 -4.0382967 -2.9201741 -1.6272216 0.11243629 2.2074585 4.4651918 5.4583092 5.7586737 3.354578 0.38298416 -2.4229236 -5.3733568 -7.3373032 -8.0311069][-4.7277365 -4.5334978 -4.233057 -2.7750216 -1.3433504 0.76275539 2.2220697 2.9614496 3.3732433 1.082098 -1.4199991 -4.4573522 -6.8187947 -8.0258541 -8.2313013][-6.0117493 -5.9549427 -6.0099907 -4.9397745 -3.8340585 -1.8596616 -0.38149691 -0.08051157 0.19835663 -1.5828819 -3.5196953 -5.7251749 -7.7206516 -8.8712835 -8.9949][-6.9549408 -7.0215793 -7.1132703 -6.9439831 -6.3141732 -5.0424719 -4.1610107 -3.508503 -3.0760665 -4.1349888 -4.711688 -6.2950087 -7.6959925 -8.8102732 -9.40583][-7.9223204 -7.9048934 -8.1220942 -7.9990416 -7.5522161 -7.245132 -6.8095422 -6.6618509 -6.088398 -6.4001317 -7.0354214 -7.8217964 -8.2843723 -8.6329765 -8.9029627][-8.6500359 -8.9809132 -8.7543974 -8.36697 -7.6778469 -7.2670565 -6.9512053 -7.2832341 -7.1651983 -7.5326781 -7.5192842 -7.4968553 -7.8176389 -7.7000513 -7.7469029][-7.9646692 -8.1057663 -8.5857611 -8.4750423 -8.15735 -7.8650122 -7.4369688 -7.1191421 -7.1790395 -7.1777587 -7.4601517 -7.4367914 -7.1560211 -7.1994109 -6.9948177]]...]
INFO - root - 2017-12-15 17:24:48.409613: step 39810, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:51m:27s remains)
INFO - root - 2017-12-15 17:24:54.834567: step 39820, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 50h:18m:25s remains)
INFO - root - 2017-12-15 17:25:01.139055: step 39830, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 50h:04m:40s remains)
INFO - root - 2017-12-15 17:25:07.506649: step 39840, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 52h:25m:12s remains)
INFO - root - 2017-12-15 17:25:13.921914: step 39850, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 51h:15m:03s remains)
INFO - root - 2017-12-15 17:25:20.336355: step 39860, loss = 0.27, batch loss = 0.16 (11.7 examples/sec; 0.682 sec/batch; 55h:24m:09s remains)
INFO - root - 2017-12-15 17:25:26.622949: step 39870, loss = 0.23, batch loss = 0.12 (13.1 examples/sec; 0.611 sec/batch; 49h:41m:11s remains)
INFO - root - 2017-12-15 17:25:32.969308: step 39880, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 51h:28m:34s remains)
INFO - root - 2017-12-15 17:25:39.356418: step 39890, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 51h:02m:39s remains)
INFO - root - 2017-12-15 17:25:45.673296: step 39900, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 51h:31m:23s remains)
2017-12-15 17:25:46.189180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7689891 -2.1070147 -2.8837333 -4.0759411 -5.3721533 -6.4310484 -7.7392211 -8.9183426 -10.076044 -11.862731 -13.216623 -13.479101 -12.740373 -11.324097 -10.323713][-0.80944109 -2.1363888 -3.7014172 -4.5825396 -5.4047966 -6.34534 -7.2754245 -7.9393687 -8.2459373 -9.5760517 -10.51243 -11.719069 -12.673208 -12.21641 -11.938322][-1.5430107 -2.1510773 -3.0299091 -3.6598096 -4.3988962 -4.9814491 -5.3846078 -5.9944868 -6.3635259 -7.1511426 -7.7796807 -9.0457306 -10.108514 -10.956012 -12.370409][-3.9059126 -4.1516638 -4.2925444 -3.8347936 -3.2025495 -2.8706856 -2.2987471 -2.9647188 -3.4357276 -4.5878811 -6.0067334 -7.5321321 -8.7325468 -9.4728394 -10.99877][-7.1366067 -6.410727 -5.5048418 -4.9012008 -4.3369117 -2.6178436 -0.68874407 -0.56109667 -0.73695087 -2.6257806 -5.1030025 -7.6025376 -8.8639984 -9.8936405 -11.127042][-8.3623581 -8.0103626 -7.0242729 -5.4584265 -3.2818403 -1.1329684 1.0918674 1.568511 1.5813389 -0.74051332 -3.8703012 -7.0413752 -9.4171219 -10.54456 -11.028976][-9.1365337 -7.9235744 -6.0009995 -3.3787775 -0.50961018 2.2217875 4.7744493 5.2158451 5.4236736 2.7441025 -1.238893 -5.4600058 -8.3881292 -9.8136253 -10.907272][-8.3063688 -7.4641094 -5.9139376 -2.9286594 0.60608864 3.14248 5.2170496 6.1174822 6.503396 4.0860853 1.0945368 -3.078547 -6.0143132 -7.7466831 -8.5729513][-8.7031345 -7.9779363 -6.925374 -4.1838083 -1.1762271 1.0487843 3.1408567 3.844408 4.0702925 2.3017626 -0.73102474 -4.0504951 -5.98139 -7.2064471 -8.3776226][-9.2759581 -9.0370684 -8.216404 -6.1402669 -3.922507 -1.2100749 1.243968 1.8621912 2.32232 -0.023049831 -2.8638983 -5.8286929 -7.6496043 -8.3900785 -8.7735052][-10.698256 -10.673731 -10.707104 -9.1915178 -7.6375551 -5.4888268 -3.1851268 -1.6765785 -0.34106636 -1.4652004 -3.7876871 -6.2977576 -7.5297637 -8.4896421 -9.265132][-10.863878 -11.029963 -10.796455 -9.9897261 -9.0807228 -7.8973784 -6.9935036 -5.776319 -4.4392381 -4.65448 -5.0580211 -6.1259036 -6.8103395 -7.4265356 -7.8893347][-11.307303 -11.429642 -11.610197 -10.756873 -10.001743 -9.1814671 -8.2903461 -8.0478754 -8.0652514 -8.4149466 -8.6818113 -8.24962 -7.9293551 -7.531651 -7.0070977][-9.9818449 -9.7935572 -9.7252884 -9.0397015 -8.6908627 -7.9329658 -7.4118419 -7.42041 -7.41933 -8.0182476 -8.285058 -8.4260387 -8.41299 -8.2028084 -7.8849273][-11.180973 -10.247265 -9.49131 -8.9533863 -8.493845 -8.0981073 -7.70587 -7.8670845 -8.2816963 -8.52028 -8.4265366 -7.7091737 -7.1169724 -6.7707987 -6.3781562]]...]
INFO - root - 2017-12-15 17:25:52.617097: step 39910, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 50h:55m:24s remains)
INFO - root - 2017-12-15 17:25:59.049238: step 39920, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 51h:15m:49s remains)
INFO - root - 2017-12-15 17:26:05.525706: step 39930, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 51h:46m:21s remains)
INFO - root - 2017-12-15 17:26:11.949038: step 39940, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 52h:01m:50s remains)
INFO - root - 2017-12-15 17:26:18.422700: step 39950, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:50m:04s remains)
INFO - root - 2017-12-15 17:26:24.826800: step 39960, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 51h:54m:00s remains)
INFO - root - 2017-12-15 17:26:31.175447: step 39970, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 50h:59m:23s remains)
INFO - root - 2017-12-15 17:26:37.578859: step 39980, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 51h:26m:51s remains)
INFO - root - 2017-12-15 17:26:43.999186: step 39990, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 50h:30m:29s remains)
INFO - root - 2017-12-15 17:26:50.555865: step 40000, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 52h:56m:04s remains)
2017-12-15 17:26:51.051406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4911032 -3.1779199 -4.1505804 -4.34307 -3.9752088 -3.4837713 -3.349874 -3.355547 -2.899714 -3.4823527 -3.6510253 -5.3165178 -6.2047043 -6.0941238 -7.1775765][-2.8527098 -3.6399193 -4.5499411 -5.1589336 -5.0656285 -4.4100809 -3.6697221 -3.5855517 -3.7460332 -4.3042827 -4.5018034 -6.0155663 -6.786181 -6.9867554 -7.6560731][-3.6751304 -4.0726919 -4.7894883 -4.694355 -4.992003 -4.8531566 -4.3037071 -3.9500387 -3.4357629 -4.2152166 -4.8694782 -6.6385112 -7.3240705 -7.6218247 -8.4261208][-4.5522852 -4.7359819 -5.1067066 -4.7311268 -4.2796016 -3.9816232 -3.3523293 -2.8783927 -2.5919909 -3.3556561 -3.8054976 -5.9925733 -7.02759 -7.3673115 -8.0313358][-5.6582174 -5.4921074 -5.5576963 -5.0428543 -4.1292295 -3.2889309 -1.7686019 -1.2210817 -0.932652 -2.102056 -3.2756605 -5.6470246 -6.7318783 -7.2401385 -8.1404123][-7.2362957 -6.6286383 -6.0489545 -4.7831039 -3.4359426 -2.1119885 -0.3907609 0.68210411 1.0804873 -1.0817418 -2.997179 -5.9959106 -7.0764589 -7.1963186 -7.8438592][-6.9847536 -6.4798169 -5.9972792 -4.3634586 -2.2493887 -0.26539469 1.9527664 2.8227253 3.1016674 1.2835541 -0.93048096 -4.4643173 -6.308763 -6.5619116 -7.2132082][-6.4490089 -5.9491587 -5.2834969 -3.4911537 -1.5345349 1.240325 3.7221975 4.0879135 3.846446 1.5929527 -0.07428503 -3.1562037 -4.8778448 -5.27476 -5.9410181][-6.4461474 -5.9244928 -5.5260067 -4.1472607 -2.2651172 0.42293167 2.9120874 3.5696344 3.6586676 1.0117416 -0.82987547 -3.7057035 -5.1785603 -5.496419 -6.4690633][-6.4085541 -6.0672913 -5.8793154 -4.6069264 -3.206459 -1.4699707 0.51821423 1.0357161 1.4054861 -0.939209 -2.823329 -5.4232826 -6.4329085 -5.8606415 -6.7590885][-7.6994281 -7.3983326 -7.0284424 -5.8375554 -4.6706781 -3.3362789 -2.1718726 -1.7019911 -1.3080769 -3.1281271 -3.906539 -5.9269466 -6.9947672 -6.3210874 -6.80135][-7.63465 -7.2189951 -6.970809 -6.0129972 -5.3811321 -4.8829546 -3.8907983 -3.7276616 -3.7241542 -4.4263892 -4.46641 -5.8860765 -6.5242987 -6.395463 -6.9103856][-8.6907492 -7.8377376 -7.1035576 -6.4578528 -5.8357849 -5.743082 -5.4284754 -5.2418489 -4.94987 -5.9803066 -6.1355009 -6.3745613 -6.440114 -6.4127846 -6.8120952][-8.3925133 -7.8530788 -6.9392333 -5.9211569 -5.4300594 -5.5206518 -5.1905766 -5.3589087 -5.3766747 -5.7051 -5.8532705 -6.1018095 -6.125834 -6.257782 -6.61487][-8.4694777 -8.5279379 -8.1442175 -7.0545416 -6.2046256 -6.1080379 -6.2949004 -6.5190382 -6.436739 -6.534174 -6.6469197 -6.5307751 -6.3117847 -6.0428977 -5.6884737]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:26:58.349934: step 40010, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 53h:07m:01s remains)
INFO - root - 2017-12-15 17:27:04.672626: step 40020, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 51h:31m:26s remains)
INFO - root - 2017-12-15 17:27:11.205890: step 40030, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 51h:34m:26s remains)
INFO - root - 2017-12-15 17:27:17.596392: step 40040, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 52h:16m:12s remains)
INFO - root - 2017-12-15 17:27:24.022394: step 40050, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 53h:21m:34s remains)
INFO - root - 2017-12-15 17:27:30.484759: step 40060, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 52h:34m:42s remains)
INFO - root - 2017-12-15 17:27:36.884352: step 40070, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 52h:58m:45s remains)
INFO - root - 2017-12-15 17:27:43.315778: step 40080, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 53h:20m:15s remains)
INFO - root - 2017-12-15 17:27:49.754250: step 40090, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 53h:36m:36s remains)
INFO - root - 2017-12-15 17:27:56.215541: step 40100, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 52h:42m:03s remains)
2017-12-15 17:27:56.705067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3406038 -2.7910323 -3.1635437 -3.6305513 -3.5720906 -3.5544777 -3.4530067 -3.808188 -4.1977625 -5.903368 -6.1250048 -7.26099 -7.1198745 -7.8554583 -7.604959][-2.5423565 -2.8042841 -3.3114057 -3.803915 -3.9180913 -4.0274134 -3.975733 -4.167666 -4.2312117 -5.7423949 -5.7902374 -7.0393624 -7.2330217 -8.2011709 -7.3802614][-3.2586622 -3.1840587 -3.5561728 -3.8208888 -3.3913083 -3.4378796 -3.67796 -3.9026272 -4.0945206 -5.9824805 -6.2292209 -7.1330576 -7.2290363 -8.1922894 -7.7232118][-3.0544777 -2.8056359 -2.6193604 -2.1953745 -1.6480432 -1.3143291 -1.5354195 -2.1100445 -2.5116858 -4.6825953 -5.3876228 -6.6384835 -6.8783865 -7.0255618 -6.0395308][-3.0020666 -2.3149376 -1.9826798 -1.3779464 -0.32659912 0.39461517 0.45776272 -0.035733223 -0.38215113 -3.1388826 -4.4007874 -6.0337811 -6.465929 -6.7947078 -5.9321461][-5.1007614 -3.7234571 -3.1417408 -2.0216727 -0.69233084 1.573884 2.5169344 2.2386293 1.2807922 -1.7659192 -3.3181548 -5.6723614 -6.4666181 -6.9071236 -5.9466381][-4.6248441 -3.4253292 -2.3031349 -0.15570498 1.0623198 2.9039402 4.0857277 4.3802338 3.3012714 -0.3674221 -2.5095549 -4.8717651 -5.3720951 -6.2404394 -5.1669927][-3.436748 -1.8946648 -0.60285854 1.1906347 2.3362188 4.72921 5.3115988 5.2830658 4.4086828 1.3174658 -0.59507513 -3.3179512 -4.5693612 -5.2734308 -4.425559][-3.9379091 -1.9676838 -1.3033457 0.5823698 1.5120525 2.6010056 3.059638 3.412281 3.2995596 0.091278553 -1.6637516 -3.7824111 -4.66267 -6.1503277 -5.202981][-4.6960983 -3.4937205 -2.6481023 -0.77171087 0.159266 0.83548832 1.1399813 1.8449717 1.6684008 -1.6853008 -2.9688272 -4.5984206 -5.4523287 -6.4637604 -6.1076012][-6.5196023 -5.6826825 -4.6850019 -3.220912 -2.5112414 -2.0255666 -1.9816093 -1.4848356 -0.982903 -3.7034237 -5.1457872 -6.5218639 -6.8059192 -7.7010694 -7.3036618][-7.7765908 -6.8000584 -6.0541925 -5.4133091 -4.8488727 -3.9387896 -3.9209034 -3.4478741 -3.0878 -4.8440647 -5.3239326 -6.4339113 -6.2560482 -7.6646819 -7.6456776][-8.628705 -8.5730629 -8.2277336 -7.1904812 -6.5973997 -5.9435043 -6.1119475 -5.7294307 -5.6706381 -6.6093054 -7.0920305 -7.3406692 -6.6848779 -6.7520256 -6.6641207][-9.6850748 -9.299859 -8.564599 -8.2786713 -7.61028 -6.5514059 -6.3593478 -6.4851589 -6.8118916 -7.5722284 -7.1149912 -7.4020753 -6.9703717 -7.2919703 -6.7941346][-10.318635 -9.8656969 -9.2218046 -8.5153351 -8.3486586 -8.150938 -7.9580297 -7.8152552 -7.7449656 -8.2082682 -7.7027426 -7.3316088 -6.7605267 -6.6936526 -6.4969096]]...]
INFO - root - 2017-12-15 17:28:03.043036: step 40110, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 52h:36m:23s remains)
INFO - root - 2017-12-15 17:28:09.482362: step 40120, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 52h:37m:36s remains)
INFO - root - 2017-12-15 17:28:15.854860: step 40130, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 50h:03m:01s remains)
INFO - root - 2017-12-15 17:28:22.177550: step 40140, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 50h:28m:17s remains)
INFO - root - 2017-12-15 17:28:28.617276: step 40150, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 51h:44m:31s remains)
INFO - root - 2017-12-15 17:28:34.943749: step 40160, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 52h:38m:08s remains)
INFO - root - 2017-12-15 17:28:41.226993: step 40170, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 53h:01m:00s remains)
INFO - root - 2017-12-15 17:28:47.518369: step 40180, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 51h:15m:47s remains)
INFO - root - 2017-12-15 17:28:53.897454: step 40190, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 51h:03m:45s remains)
INFO - root - 2017-12-15 17:29:00.298538: step 40200, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 51h:20m:26s remains)
2017-12-15 17:29:00.831075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.80928516 -0.87470484 -1.1546626 -1.2839971 -1.477376 -1.7850838 -2.0373688 -2.0454502 -2.0273986 -3.5551229 -4.3960514 -5.5496225 -6.42171 -6.8111663 -7.1641][-2.1089044 -2.0228786 -2.2584105 -2.6316485 -2.8535213 -2.9454427 -3.029819 -3.2170682 -3.2222581 -4.6650448 -5.5959978 -6.6595683 -7.4589925 -7.941205 -8.02645][-3.9405985 -3.5941896 -3.583797 -3.54775 -3.444591 -3.3580508 -3.302465 -3.4778714 -3.6965225 -5.3315196 -6.2833567 -7.520422 -8.4937077 -9.0555477 -9.25334][-4.9583683 -4.6866612 -4.4937553 -3.8779478 -3.299479 -2.8090653 -2.3948612 -2.5492511 -2.7560148 -4.4077644 -5.4973831 -6.9191995 -8.0626421 -8.60668 -8.7710772][-6.0343695 -5.1293731 -4.31238 -3.4567237 -2.6514835 -1.7811337 -1.090703 -1.1194248 -1.3938746 -2.9051785 -3.8525326 -5.2192526 -6.4409003 -6.9824328 -7.3974552][-6.3669004 -5.4219952 -4.600646 -3.24306 -1.9087381 -0.94850588 -0.2448864 -0.29004669 -0.47580576 -1.9348965 -2.9322329 -4.0633106 -5.1352534 -5.6465282 -6.4378695][-6.2367744 -4.5751104 -3.2071404 -1.8298225 -0.53153706 0.49244976 1.1004658 1.1580248 1.077095 -0.6435709 -1.738668 -2.8410439 -3.9290967 -4.2374487 -4.7942619][-5.212923 -3.7017803 -2.4231553 -0.79456806 0.74839211 1.6486139 2.098753 2.0420341 1.9398766 0.44921112 -0.37336302 -1.7362518 -2.7376332 -2.6811156 -3.0957112][-4.6754594 -2.9897609 -1.5221152 -0.026146889 1.017808 1.4016609 1.7225132 1.7906532 1.781045 0.16731167 -0.67840481 -1.6248322 -2.3184056 -2.5623603 -2.854702][-5.4005728 -3.5943308 -2.1362734 -0.85127974 0.0098180771 0.5281477 0.740468 0.94421482 1.0492821 -0.5985918 -1.6548748 -2.5817752 -3.1522608 -3.33575 -3.27346][-7.1468749 -5.77993 -4.5685573 -3.2455754 -2.3977537 -1.9697304 -1.5823555 -1.3768115 -1.182693 -2.2540331 -3.2039676 -4.3984184 -4.9002042 -4.8523316 -4.4748325][-8.8992615 -7.935977 -6.8145957 -5.3887434 -4.5993109 -4.0439444 -3.569622 -3.5084667 -3.3919339 -4.21821 -4.7514067 -5.3059931 -5.7826681 -5.9881239 -5.5885992][-9.3920612 -8.71689 -7.9476914 -6.9215231 -5.8269053 -4.9861822 -4.5536337 -4.4636345 -4.4269838 -5.184516 -5.4675207 -5.5003343 -5.81803 -5.8218675 -5.4915876][-9.5185795 -8.8831739 -8.0509214 -6.905519 -6.19933 -5.3509779 -4.9047451 -4.9222274 -5.0842743 -5.3662143 -5.5847311 -5.5397677 -5.4530187 -5.3732967 -5.1648264][-9.6782341 -9.53119 -9.2547855 -8.2475481 -7.1153822 -6.1856422 -5.734354 -5.7806058 -5.9879227 -6.2380829 -6.3934684 -6.5527878 -6.4462962 -6.3620148 -6.2892675]]...]
INFO - root - 2017-12-15 17:29:07.131760: step 40210, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 52h:00m:49s remains)
INFO - root - 2017-12-15 17:29:13.504219: step 40220, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 53h:17m:15s remains)
INFO - root - 2017-12-15 17:29:19.874226: step 40230, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 52h:32m:05s remains)
INFO - root - 2017-12-15 17:29:26.190572: step 40240, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 52h:56m:23s remains)
INFO - root - 2017-12-15 17:29:32.555801: step 40250, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 49h:59m:13s remains)
INFO - root - 2017-12-15 17:29:38.971114: step 40260, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 52h:16m:58s remains)
INFO - root - 2017-12-15 17:29:45.300979: step 40270, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 51h:44m:39s remains)
INFO - root - 2017-12-15 17:29:51.618735: step 40280, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 51h:47m:16s remains)
INFO - root - 2017-12-15 17:29:57.964802: step 40290, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 51h:22m:00s remains)
INFO - root - 2017-12-15 17:30:04.328941: step 40300, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 50h:53m:43s remains)
2017-12-15 17:30:04.862667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1827259 -8.6368189 -9.6443443 -9.9808979 -9.6701813 -8.5270882 -7.4617791 -6.2735128 -5.0684338 -5.2393665 -5.4092226 -6.8890018 -7.3763247 -7.0844855 -7.3129187][-6.3424459 -8.4694395 -9.9264088 -10.487837 -10.367591 -9.7300472 -8.5745716 -6.8971572 -5.2498045 -6.0826731 -6.7888088 -8.4041958 -8.9748688 -9.1401415 -8.1405783][-6.7884912 -8.1546412 -9.5704012 -10.279169 -10.488368 -9.6495762 -8.3106833 -7.344955 -6.3232102 -6.8399935 -7.1458216 -8.9209881 -10.047952 -10.241979 -9.7864666][-6.9396238 -7.8850164 -8.46239 -8.4341011 -8.0739965 -7.3431358 -6.5618339 -6.1050615 -5.4568348 -6.5634027 -7.5464339 -9.4675207 -10.121876 -10.114262 -10.325985][-6.1585989 -5.675756 -5.6599226 -4.991003 -4.3435354 -3.2990017 -1.9154897 -1.6876373 -1.556901 -3.7707725 -5.7185984 -8.4816389 -9.9650459 -10.259394 -10.217463][-6.3643632 -5.3264475 -4.6567788 -2.9812264 -1.4636745 0.31894398 2.0809956 2.3481913 2.4330673 -0.45779276 -3.0741439 -6.6998124 -8.7350044 -9.01817 -9.1700516][-6.0300579 -5.4647603 -3.6383 -0.81105614 1.4618654 3.2064791 4.7341633 4.8966312 4.6184206 1.3904676 -1.5545368 -4.9581761 -6.9998603 -7.7581735 -8.2485619][-5.9512253 -4.5943608 -2.6096826 0.097770691 2.6351061 4.2091827 5.7397451 5.8314075 5.6070948 2.3771772 -1.0115342 -4.257338 -6.0720215 -6.5105104 -7.2633929][-6.1934834 -5.036788 -4.0661573 -2.0607843 -0.478261 1.635273 3.4755354 3.9196224 4.1321087 0.55446148 -2.5981884 -5.6953726 -7.0896883 -7.5527186 -7.7356524][-8.45194 -7.9180279 -7.1376286 -5.0916424 -3.4149256 -2.3751388 -1.6902137 -0.22473431 0.29944658 -3.021668 -5.8782973 -8.5372076 -9.6605682 -9.2815142 -8.7145481][-10.342547 -10.404106 -9.7719469 -8.0329418 -6.6530886 -5.8344393 -5.1584196 -4.8806953 -5.0947275 -6.8904166 -8.32755 -9.6490908 -10.090218 -10.062069 -9.7416143][-11.404684 -11.14883 -11.08939 -10.167522 -9.3686047 -8.4768333 -7.676837 -7.2985458 -6.6285429 -7.52577 -8.5685 -8.8597641 -9.0631752 -9.07486 -9.1114225][-10.413476 -10.832153 -11.299856 -10.002653 -8.7007675 -7.94749 -7.2569733 -6.9904971 -6.8810239 -7.5183139 -8.2573576 -8.3727083 -8.0790253 -7.886899 -7.750659][-9.1790171 -9.5208368 -9.3990812 -8.7855806 -8.0398817 -7.1996217 -6.4667697 -6.465199 -6.9032936 -7.2100229 -8.1904573 -8.1563568 -8.1769438 -7.8376064 -7.2612219][-7.2926083 -7.3251276 -7.8632126 -7.9096079 -7.8947129 -7.5857053 -6.9479609 -7.429348 -7.318675 -7.521853 -7.7076912 -7.8389144 -8.390789 -8.245163 -7.9088011]]...]
INFO - root - 2017-12-15 17:30:11.143359: step 40310, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.621 sec/batch; 50h:22m:30s remains)
INFO - root - 2017-12-15 17:30:17.632397: step 40320, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 51h:52m:24s remains)
INFO - root - 2017-12-15 17:30:23.999360: step 40330, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:27m:19s remains)
INFO - root - 2017-12-15 17:30:30.357440: step 40340, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 52h:46m:03s remains)
INFO - root - 2017-12-15 17:30:36.809091: step 40350, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 52h:58m:58s remains)
INFO - root - 2017-12-15 17:30:43.193796: step 40360, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 50h:24m:07s remains)
INFO - root - 2017-12-15 17:30:49.529641: step 40370, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.621 sec/batch; 50h:24m:03s remains)
INFO - root - 2017-12-15 17:30:55.904561: step 40380, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 53h:26m:35s remains)
INFO - root - 2017-12-15 17:31:02.356053: step 40390, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 52h:00m:10s remains)
INFO - root - 2017-12-15 17:31:08.719314: step 40400, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 50h:57m:48s remains)
2017-12-15 17:31:09.257029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0659766 -1.4836836 -2.0736723 -2.6075702 -3.1218662 -3.7108066 -3.5074768 -3.1901584 -2.8560271 -3.8857672 -4.6354065 -5.1540804 -5.9080863 -6.9147878 -7.3013773][-1.4899178 -1.8714261 -2.4213047 -2.8907337 -3.3585644 -3.5966716 -3.6087475 -3.3776503 -3.3582897 -4.2652483 -5.0267882 -5.8657846 -6.800622 -7.7502208 -8.1418743][-1.7536917 -1.763484 -1.935111 -2.2246561 -2.447125 -2.5545411 -2.4736419 -2.441112 -2.4418006 -3.5713105 -4.3384256 -5.0915318 -5.7986207 -6.7664328 -7.3066163][-1.9345527 -1.9645758 -1.88621 -1.7917976 -1.6675706 -1.5529637 -1.3204913 -1.3852782 -1.4276638 -2.6216602 -3.6738124 -4.7114649 -5.4842939 -6.325614 -6.5493655][-2.7009368 -2.5351658 -2.2934732 -1.9014988 -1.5384068 -1.0084853 -0.59118032 -0.46477079 -0.38080883 -1.3225088 -2.2301812 -3.3615403 -4.2250371 -5.43742 -5.9210124][-3.4508734 -2.9394741 -2.3263369 -1.8243008 -1.3489871 -0.55007887 0.062356949 0.26992416 0.45839882 -0.5631671 -1.397078 -2.1795759 -2.76477 -4.1165876 -5.0287395][-3.3135095 -2.9590178 -2.4644008 -1.5759959 -0.86317778 -0.26546907 0.30568504 0.88547611 1.2829189 0.36550617 -0.28960133 -1.1138835 -1.9356356 -3.4579244 -4.6355257][-3.0911765 -2.6699581 -2.0948162 -1.3733983 -0.60844517 0.12124252 0.76096821 1.2482119 1.7472792 0.91301823 0.079995632 -0.79473639 -1.7084317 -3.1780972 -4.1640596][-3.0791082 -2.3622794 -2.0406504 -1.4587412 -0.81653023 -0.069979668 0.43915272 0.80668831 1.1913996 0.44883633 -0.20473766 -1.4160347 -2.477159 -3.9857228 -4.7103076][-3.2715592 -2.5842843 -2.0371532 -1.5037236 -0.98049879 -0.41200161 -0.0073065758 0.2664876 0.62686062 -0.49917364 -1.4469872 -2.6495543 -3.7768071 -4.8615661 -5.4780197][-4.6333294 -4.1002049 -3.5796967 -3.0044708 -2.4474983 -1.9802852 -1.7324905 -1.5838575 -1.3783832 -2.6134629 -3.5252609 -4.3804417 -5.0680294 -6.0406961 -6.4356503][-5.2937851 -4.9581108 -4.4195471 -3.8730192 -3.4495721 -3.0819883 -2.9106345 -3.0194526 -2.9879389 -3.9184194 -4.9091959 -5.7065444 -6.1604214 -6.6349516 -6.6898689][-6.1372266 -5.8308921 -5.5022869 -4.9999561 -4.5997133 -4.2985783 -4.2937536 -4.2656841 -4.2046766 -4.9619389 -5.6542053 -6.1619 -6.3827333 -6.6101575 -6.439168][-6.2070885 -5.5882483 -5.2150345 -4.8715482 -4.4402733 -4.3239031 -4.3109007 -4.398571 -4.5477114 -5.2937164 -5.6775312 -5.8279452 -5.8749256 -6.01447 -5.8422837][-7.0305696 -6.8386135 -6.395782 -5.7885032 -5.4216385 -5.3332062 -5.4378057 -5.4909019 -5.5381751 -5.78708 -6.150044 -6.3524051 -6.4630446 -6.3957605 -6.1286516]]...]
INFO - root - 2017-12-15 17:31:15.738333: step 40410, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 53h:52m:41s remains)
INFO - root - 2017-12-15 17:31:22.231565: step 40420, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 51h:17m:50s remains)
INFO - root - 2017-12-15 17:31:28.683377: step 40430, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 53h:19m:33s remains)
INFO - root - 2017-12-15 17:31:35.129101: step 40440, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 51h:47m:03s remains)
INFO - root - 2017-12-15 17:31:41.608888: step 40450, loss = 0.24, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 54h:42m:24s remains)
INFO - root - 2017-12-15 17:31:48.052946: step 40460, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 53h:12m:24s remains)
INFO - root - 2017-12-15 17:31:54.451337: step 40470, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 51h:55m:45s remains)
INFO - root - 2017-12-15 17:32:00.971490: step 40480, loss = 0.26, batch loss = 0.14 (11.8 examples/sec; 0.680 sec/batch; 55h:08m:37s remains)
INFO - root - 2017-12-15 17:32:07.487112: step 40490, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 53h:17m:17s remains)
INFO - root - 2017-12-15 17:32:14.021904: step 40500, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 53h:07m:54s remains)
2017-12-15 17:32:14.546285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3304863 -5.8161483 -6.2601681 -6.9602866 -7.3022 -7.2533007 -6.7867489 -5.7479582 -4.4884105 -3.8257167 -3.62645 -4.1068468 -4.567358 -5.7367306 -6.61772][-4.9502935 -5.2983274 -6.0791235 -6.761344 -7.2892952 -7.242116 -6.8054376 -6.2888317 -5.1876206 -4.9132547 -5.0921459 -5.5875783 -5.7969823 -6.0976958 -6.4468861][-4.520586 -4.6539788 -4.7379541 -5.1253786 -5.2385798 -5.2445822 -5.3948774 -5.3019447 -4.732686 -4.6809936 -5.0463266 -5.4654231 -5.3898144 -5.6010189 -5.634119][-4.4071784 -4.1880608 -3.9191532 -3.6596718 -3.5475507 -3.2914677 -2.9395413 -2.9114461 -2.7588019 -3.2280383 -4.0196681 -4.6192989 -4.8669291 -5.4249725 -5.71002][-4.6606226 -3.5075517 -3.0426502 -2.7755947 -2.4835315 -1.7141671 -1.0870357 -0.91872215 -1.0426021 -2.1183152 -3.394259 -4.7213163 -5.139329 -5.5072289 -5.717082][-4.3367348 -3.812814 -2.9747705 -2.0988002 -1.2133503 0.17001295 1.1781597 1.2360878 1.1956692 0.39442253 -0.5655427 -2.3616376 -3.501245 -4.6084671 -5.4849553][-4.823781 -4.0251889 -2.7832708 -1.575201 -0.48431253 0.91448593 1.9383688 2.119669 2.2479544 1.5588923 0.65455818 -1.3846483 -2.7495565 -4.2347107 -5.3864641][-5.03804 -4.13242 -3.0138745 -1.7813139 -0.30361366 1.3704319 2.4777861 2.743125 2.8530416 2.2035255 1.181756 -0.983366 -2.4654269 -4.1246395 -5.354764][-4.9819822 -4.0972257 -3.0806909 -1.7841563 -0.68398237 0.74818707 2.0252542 2.622839 3.029295 2.3411083 1.178793 -1.0784783 -2.6752758 -4.0048037 -4.851305][-5.6734447 -4.7332287 -3.9552197 -2.9692855 -1.8943477 -0.50676441 0.57572174 1.2581024 1.8282251 0.95127678 -0.11186886 -1.6737866 -3.0984936 -4.481945 -5.4097252][-6.8882236 -6.1051626 -5.7232971 -4.8092337 -4.1686029 -3.203361 -2.21595 -1.8086238 -1.5245509 -2.2097521 -2.9925041 -3.8930547 -4.5626774 -5.1712484 -5.846673][-8.0064936 -7.3563066 -7.0921412 -6.6507111 -6.4124784 -5.901206 -4.8729944 -4.44643 -3.9419875 -4.334075 -4.783289 -5.38311 -6.0034819 -6.7070107 -6.9630885][-8.0734816 -7.5716805 -7.2834406 -7.0805311 -7.1724195 -6.8108878 -6.41006 -6.385222 -6.1688795 -6.1789975 -6.3653336 -6.383523 -6.6059704 -6.7643447 -6.585237][-6.6366529 -6.7638574 -6.9271488 -7.0764041 -6.8914466 -6.8566885 -6.4982285 -6.7053547 -6.9216013 -7.1089225 -7.6508183 -7.7119832 -7.4622712 -6.95202 -6.6119242][-6.9070172 -6.7905278 -6.9806476 -7.25052 -7.4114656 -7.2766323 -6.7160597 -6.7731919 -7.0542574 -7.7951403 -8.5105724 -8.4677019 -8.238389 -7.74637 -7.2772532]]...]
INFO - root - 2017-12-15 17:32:20.930494: step 40510, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 53h:41m:44s remains)
INFO - root - 2017-12-15 17:32:27.423005: step 40520, loss = 0.32, batch loss = 0.21 (12.0 examples/sec; 0.668 sec/batch; 54h:09m:41s remains)
INFO - root - 2017-12-15 17:32:33.829155: step 40530, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 52h:00m:20s remains)
INFO - root - 2017-12-15 17:32:40.261330: step 40540, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 52h:03m:47s remains)
INFO - root - 2017-12-15 17:32:46.757282: step 40550, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.657 sec/batch; 53h:16m:58s remains)
INFO - root - 2017-12-15 17:32:53.161421: step 40560, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 51h:02m:29s remains)
INFO - root - 2017-12-15 17:32:59.551179: step 40570, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 51h:41m:04s remains)
INFO - root - 2017-12-15 17:33:05.942487: step 40580, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 51h:57m:03s remains)
INFO - root - 2017-12-15 17:33:12.409050: step 40590, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 53h:07m:10s remains)
INFO - root - 2017-12-15 17:33:18.800074: step 40600, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 51h:44m:02s remains)
2017-12-15 17:33:19.355305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.103498 -3.4215164 -3.5041676 -3.9421227 -4.1538067 -4.080853 -4.2880573 -4.3233566 -4.7252884 -6.1126003 -7.0891223 -7.7480068 -8.1205378 -6.8265719 -7.2200561][-3.4979682 -3.5945058 -3.713367 -3.9465346 -4.1774478 -4.1693411 -4.4842377 -4.9383459 -5.4329329 -6.8975177 -7.8357329 -8.5106716 -9.0339489 -7.7856579 -8.076869][-3.9324069 -3.4315424 -2.7814622 -2.1315122 -1.9991951 -2.3076739 -2.8457031 -3.3574758 -4.1091275 -5.69522 -6.9779539 -8.0179281 -8.612525 -7.5824828 -8.238862][-3.1466408 -2.5005431 -2.0178127 -1.163847 -0.81215143 -0.890913 -1.0310607 -1.5151572 -2.1121426 -4.1991835 -5.7622547 -7.0894537 -7.8848915 -6.9929681 -7.339231][-4.0659885 -2.4865203 -1.6100183 -0.84779787 -0.25093126 0.049096584 0.37007141 0.017060757 -0.85997248 -2.7563376 -4.3296256 -5.9447918 -7.4271679 -6.7325335 -7.1656871][-4.4860387 -3.1988335 -2.0905528 -0.81541634 0.3551445 0.97379971 1.4717274 0.77743053 0.19797277 -1.8805184 -3.3860736 -4.9092393 -6.250175 -5.8781824 -6.7043338][-3.8202751 -2.9256825 -2.2296414 -0.81649017 0.41834259 1.200593 2.0122156 1.728693 1.3072357 -0.64076138 -2.2638087 -4.1443729 -5.8053217 -5.8108964 -6.9683719][-3.3369637 -2.2950339 -1.2908721 -0.36850309 0.73025608 1.492054 2.3626623 2.3188505 2.1984444 0.51797962 -1.7351851 -4.1032429 -5.8633032 -5.8196774 -6.9758554][-4.0501246 -3.1676278 -2.8192706 -1.4524937 0.23849058 0.76913548 1.670558 2.3307419 2.7515192 0.57333469 -1.480123 -4.13093 -6.3189554 -6.3737125 -7.7751312][-4.1529026 -3.7955396 -2.8435216 -2.2977242 -1.6156259 -0.91501 -0.37049198 -0.099911213 0.38287735 -1.1574941 -3.0841289 -5.2366009 -6.7606792 -7.0299129 -8.5414276][-6.1307378 -4.7670035 -4.1151195 -4.0188284 -3.1675076 -3.0187206 -2.1362743 -1.6692853 -1.2028179 -2.8685818 -4.4542861 -6.2141953 -7.71371 -7.8300753 -8.9930868][-6.0558672 -5.4546385 -4.9405475 -4.6641974 -4.3341742 -4.3930416 -3.9414282 -3.3922286 -3.0491271 -3.950721 -5.0344906 -6.5387187 -7.6145072 -7.7285357 -8.52581][-6.8772192 -6.1549029 -5.4533377 -5.2362633 -4.7447958 -4.87478 -4.7010703 -4.7691145 -4.576189 -5.3762054 -6.2252645 -6.7477069 -7.5989633 -7.5124493 -7.8999743][-7.1722441 -6.0408525 -5.2765961 -4.6586618 -4.2567892 -4.6094189 -4.5645914 -4.7480326 -4.9904528 -5.5633955 -5.8599496 -6.6817632 -6.76944 -6.8743262 -7.2287831][-8.8403826 -7.9047308 -7.1344943 -6.1605277 -5.3521533 -5.558363 -5.812397 -5.8580375 -6.0636816 -6.2691803 -6.6781459 -7.0049806 -7.2343373 -7.1119266 -6.9025841]]...]
INFO - root - 2017-12-15 17:33:25.905197: step 40610, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.678 sec/batch; 54h:57m:47s remains)
INFO - root - 2017-12-15 17:33:32.357811: step 40620, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:44m:02s remains)
INFO - root - 2017-12-15 17:33:38.800382: step 40630, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 53h:07m:10s remains)
INFO - root - 2017-12-15 17:33:45.172463: step 40640, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 51h:37m:37s remains)
INFO - root - 2017-12-15 17:33:51.571846: step 40650, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:24m:03s remains)
INFO - root - 2017-12-15 17:33:57.985428: step 40660, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 52h:07m:24s remains)
INFO - root - 2017-12-15 17:34:04.485356: step 40670, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 53h:24m:17s remains)
INFO - root - 2017-12-15 17:34:10.968280: step 40680, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 51h:12m:53s remains)
INFO - root - 2017-12-15 17:34:17.348555: step 40690, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 50h:49m:19s remains)
INFO - root - 2017-12-15 17:34:23.886743: step 40700, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 54h:10m:53s remains)
2017-12-15 17:34:24.389462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2811818 -7.0302358 -6.5641718 -6.2791491 -5.7173896 -4.2411289 -3.4301319 -2.5288777 -1.7064366 -2.1893501 -2.9683127 -5.1044569 -6.41621 -7.2890711 -7.8000827][-5.7809105 -5.8412275 -5.917767 -5.9955935 -5.7654457 -4.8608332 -4.2634792 -3.8344512 -3.5377669 -3.6560745 -4.3376179 -6.2490873 -7.367784 -8.0151758 -8.1781273][-5.0631924 -5.1403255 -5.3121233 -4.9957128 -4.4979639 -3.8200626 -3.6738219 -3.5450048 -3.2183986 -3.6910746 -4.47985 -6.0372715 -6.8530211 -7.5571647 -7.66587][-3.928477 -3.7264535 -3.9038262 -3.4424138 -2.7797146 -2.1088567 -1.7344923 -1.7032909 -1.6759033 -2.5062146 -3.5638218 -5.1355486 -6.173481 -6.7902484 -7.0709114][-2.500021 -2.1574898 -1.8840275 -1.7166166 -1.03723 -0.088588715 0.2570672 -0.050403118 -0.1116972 -1.0715089 -2.11591 -3.6516256 -4.5778513 -5.4650145 -6.3144016][-1.7694688 -1.4138994 -1.1172247 -0.53451824 0.0080003738 0.5037508 0.92751884 0.84327221 0.60962391 -0.32271433 -1.2712793 -2.5852871 -3.548316 -4.5311985 -5.2759218][-2.5981169 -2.1632824 -1.1227999 -0.28295898 0.55820751 0.91810417 1.2356968 1.1360693 0.92404461 -0.037167072 -0.98568392 -2.3036003 -3.3930144 -4.4199586 -5.2136984][-3.2066708 -2.37399 -1.2314472 -0.21708822 0.524806 1.2574663 1.4280033 1.3256769 1.1143999 0.0422287 -0.93888187 -2.325675 -3.6585288 -4.7216473 -5.6235952][-3.8143685 -3.2163272 -1.9357247 -0.84910822 0.014891148 0.78890896 1.3582706 1.2377396 1.181304 0.15387917 -1.0024729 -2.5802951 -3.9548707 -4.8072696 -5.5351019][-5.467289 -4.3831692 -3.13582 -1.7947383 -1.0147138 -0.30256224 -0.043048382 0.10699415 0.27846813 -0.69593143 -1.6822429 -3.2117558 -4.3365421 -5.0115824 -5.698967][-7.4813333 -6.0868917 -4.6160355 -3.4695439 -2.5541334 -1.8737855 -1.792419 -1.7519622 -1.5508232 -1.9378376 -2.6227131 -3.7201037 -4.7505703 -5.2128506 -5.804502][-7.918232 -6.2780719 -5.0588307 -4.11996 -3.3129506 -2.6412501 -2.5189176 -2.6940246 -2.7076311 -3.0254354 -3.5216236 -3.7274876 -4.3571119 -4.812849 -5.3216496][-8.3619318 -7.3457131 -6.1260867 -5.0247374 -3.8574519 -2.928297 -2.6073303 -2.8581843 -2.831202 -2.9655986 -3.4282384 -3.5363178 -3.6729355 -3.8015723 -3.9902749][-7.626214 -7.0900421 -6.2334604 -5.4187279 -4.5142188 -3.4422426 -2.8777847 -2.7366681 -2.7259684 -2.7639236 -2.9678335 -2.73379 -2.4943027 -2.6278243 -2.8902469][-8.0228367 -7.3998652 -6.7958107 -6.4450006 -5.6292992 -4.5120668 -3.9793916 -3.6766386 -3.2635183 -2.9373937 -2.8964939 -2.990799 -2.9522033 -2.8922157 -3.00499]]...]
INFO - root - 2017-12-15 17:34:30.794338: step 40710, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 51h:50m:11s remains)
INFO - root - 2017-12-15 17:34:37.248872: step 40720, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 50h:13m:17s remains)
INFO - root - 2017-12-15 17:34:43.586564: step 40730, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 52h:13m:19s remains)
INFO - root - 2017-12-15 17:34:49.852498: step 40740, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 51h:11m:12s remains)
INFO - root - 2017-12-15 17:34:56.185570: step 40750, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 52h:56m:17s remains)
INFO - root - 2017-12-15 17:35:02.511466: step 40760, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 51h:34m:07s remains)
INFO - root - 2017-12-15 17:35:08.973638: step 40770, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 51h:31m:40s remains)
INFO - root - 2017-12-15 17:35:15.392742: step 40780, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 50h:47m:33s remains)
INFO - root - 2017-12-15 17:35:21.878024: step 40790, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.661 sec/batch; 53h:34m:26s remains)
INFO - root - 2017-12-15 17:35:28.237066: step 40800, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 51h:11m:39s remains)
2017-12-15 17:35:28.762108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.145916 -4.2281475 -4.280581 -4.4555569 -4.321722 -4.0911942 -3.8064258 -3.7541215 -3.435205 -4.2550349 -4.2184834 -4.894949 -5.4419079 -5.7773857 -6.5592756][-3.83302 -3.9865382 -4.1443977 -4.4144344 -4.3520865 -4.3106337 -4.3897781 -4.2771645 -4.3671284 -5.1379051 -5.1741152 -6.1787705 -7.0370617 -7.4803481 -8.3249931][-3.9280772 -3.7407353 -3.2266827 -3.1391478 -2.7939186 -2.6866055 -2.8661056 -2.9457474 -3.4030681 -4.4443455 -4.7099948 -5.6732612 -6.4400206 -6.7197113 -7.6247025][-2.4256673 -1.7891779 -0.96651363 -0.74405 -0.64221859 -0.46581936 -0.58094788 -1.0331082 -1.4025893 -2.9559951 -3.5136046 -4.2917314 -5.3082857 -5.5773911 -6.3960009][-1.5555282 -0.98897171 -0.15343237 0.22125578 -0.0021972656 0.12746477 0.36073685 0.068938732 -0.26046848 -1.5316229 -2.0142369 -2.8736582 -3.9528329 -4.5033593 -6.04519][-1.103056 -0.21669149 0.094127178 0.56167889 0.68311214 0.83280373 1.1601753 0.79863834 0.47043991 -0.65290308 -0.903327 -1.4924188 -2.3082714 -3.0929456 -4.5409989][-1.1115332 -0.089237213 0.62657928 1.1850185 1.4464207 1.5666828 1.8438454 1.5812063 1.3788538 0.20658827 -0.21545076 -0.90046263 -1.7876973 -2.5197744 -4.0388355][-0.40157509 0.27300406 1.0461788 1.8940716 2.3644772 2.322629 2.5250673 2.3073902 2.1534405 0.85285854 0.38215065 -0.51375866 -1.5747905 -2.3326569 -3.8605416][-0.70274687 0.080648422 0.55582523 1.011157 1.5867004 1.6429586 1.4923277 1.6235228 1.9543562 0.91006947 0.25105286 -0.910326 -2.122117 -3.0497274 -4.5902524][-2.0576854 -1.4711823 -0.77933073 -0.17644119 0.31429863 0.61326694 0.79227924 0.94395542 1.2567959 0.21045732 -0.66439915 -1.7997928 -2.8911896 -3.61168 -5.0726914][-3.2876225 -2.9271173 -2.2951107 -1.8960772 -1.1531606 -0.99267244 -0.78529978 -0.85132217 -0.9549284 -2.2094784 -2.7222714 -3.7587812 -4.8532372 -5.336894 -6.30652][-4.3029203 -4.1127234 -4.0014706 -4.0586767 -3.6880198 -3.3885927 -3.311986 -3.4430132 -3.3113079 -4.4207439 -4.9493876 -5.3945971 -5.8101187 -5.9487276 -6.5583391][-6.2390118 -6.208395 -6.0799093 -5.9657068 -5.8623395 -5.6017833 -5.743782 -5.4135818 -5.1627283 -5.5256987 -5.4074926 -5.5906668 -5.9426727 -5.7201815 -5.9485655][-6.625083 -6.8654842 -6.9012609 -6.7761827 -6.8408837 -6.7052507 -6.5640659 -6.7379107 -6.7201867 -7.0314136 -7.1557541 -6.7258148 -6.5815668 -5.7033615 -4.7985425][-7.0055141 -6.8361135 -7.1995525 -7.2388325 -7.5761948 -7.6105642 -7.6221118 -8.0239754 -8.0539322 -8.0580244 -7.88142 -7.4496627 -7.0530257 -6.4577665 -5.8168821]]...]
INFO - root - 2017-12-15 17:35:35.253079: step 40810, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.643 sec/batch; 52h:05m:13s remains)
INFO - root - 2017-12-15 17:35:41.701641: step 40820, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 51h:36m:39s remains)
INFO - root - 2017-12-15 17:35:48.012866: step 40830, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 50h:28m:25s remains)
INFO - root - 2017-12-15 17:35:54.391001: step 40840, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 52h:01m:46s remains)
INFO - root - 2017-12-15 17:36:00.811247: step 40850, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 53h:15m:36s remains)
INFO - root - 2017-12-15 17:36:07.151167: step 40860, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 52h:35m:23s remains)
INFO - root - 2017-12-15 17:36:13.507899: step 40870, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 51h:26m:51s remains)
INFO - root - 2017-12-15 17:36:19.895756: step 40880, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.640 sec/batch; 51h:50m:06s remains)
INFO - root - 2017-12-15 17:36:26.372755: step 40890, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 52h:08m:17s remains)
INFO - root - 2017-12-15 17:36:32.871139: step 40900, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 51h:20m:50s remains)
2017-12-15 17:36:33.380407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.16256 -6.6878638 -7.4099226 -7.2147255 -5.9981041 -4.2671905 -2.7535586 -2.3420167 -1.829494 -3.5073066 -4.3330574 -5.57232 -7.71519 -8.17826 -8.1360235][-6.6243453 -6.4876471 -6.8575664 -7.193944 -7.2578993 -6.2359877 -4.5686793 -2.8737011 -1.950666 -3.8731012 -5.3431273 -6.3326769 -7.9444695 -8.2714729 -8.3177862][-6.0658007 -6.8710318 -7.3812685 -7.2891836 -6.7433186 -6.5495844 -6.3066139 -4.9398708 -3.49302 -4.206049 -5.3906956 -6.6870708 -8.4999046 -8.5811415 -8.6272583][-6.594595 -6.6735415 -6.3858309 -6.3834929 -6.1418257 -5.6895638 -4.882781 -3.9007039 -2.7948904 -4.3750181 -5.6473217 -6.6767 -8.5658407 -9.0292912 -8.5736713][-6.7391968 -6.3991404 -6.5143542 -6.2565184 -5.0014467 -3.9266899 -2.7297821 -1.4725852 -1.0000906 -3.2533736 -4.9388714 -6.8482442 -8.3365116 -8.7413521 -8.8145571][-6.566277 -6.3124819 -5.7743816 -5.0881348 -3.871599 -1.9626923 0.13234425 1.3062334 1.8132515 -0.73996544 -3.816709 -6.3001132 -8.3317928 -8.9897766 -8.764411][-6.9444027 -6.4127851 -6.0856256 -5.0990896 -2.9150243 -0.43121052 2.061902 3.6189671 4.338625 2.0508041 -0.87329292 -3.8858533 -7.5150032 -9.0675125 -9.1004944][-6.6248841 -6.8433642 -6.1077156 -4.1761546 -1.7158875 0.42357159 2.8426113 4.8096838 5.6870651 3.378643 0.70971966 -2.0336981 -5.8251281 -7.9162769 -8.7832747][-6.5037665 -6.4990578 -6.1010981 -5.3584156 -2.9039025 -0.34188128 2.260294 3.9603233 4.2967863 1.6109781 -1.2814102 -3.4019127 -5.6752415 -6.9164119 -7.5606074][-6.3879809 -6.465126 -6.3828812 -5.1398382 -3.2827411 -1.3735385 0.89308739 2.0401888 2.8081417 -0.35210133 -3.6782389 -5.8613148 -7.8010993 -7.6032367 -7.1259375][-5.9744706 -6.57911 -7.010767 -5.7222834 -4.0946484 -2.4738193 -0.93114758 -0.26894236 0.23495197 -2.6260581 -4.7256465 -6.5777183 -8.2371407 -8.6633806 -8.2838545][-5.3987255 -4.9777689 -4.9071031 -4.8519664 -3.9823029 -2.9047356 -2.1051302 -2.5008707 -2.7956266 -3.8602982 -4.83533 -6.6228256 -8.0500336 -8.77786 -8.9869289][-4.74387 -4.7362356 -5.2019081 -4.0847225 -2.7468948 -2.2874441 -2.0472679 -2.6907835 -3.1582384 -4.7147837 -6.2320266 -7.0768466 -7.8980818 -8.2286282 -8.2861385][-5.0706105 -4.9747667 -5.4708366 -5.2598572 -4.7331905 -3.0330019 -1.8472362 -2.8088427 -3.3508468 -4.7530136 -6.0497704 -6.85626 -7.4839926 -7.177804 -6.9044619][-6.0809741 -5.4974804 -5.6101551 -6.107914 -5.9260316 -5.4328294 -5.5625877 -5.5928221 -5.3620448 -5.0649338 -5.2080755 -6.2210894 -6.8844457 -6.7326107 -6.6361527]]...]
INFO - root - 2017-12-15 17:36:39.884526: step 40910, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 52h:42m:37s remains)
INFO - root - 2017-12-15 17:36:46.294535: step 40920, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:22m:02s remains)
INFO - root - 2017-12-15 17:36:52.669537: step 40930, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 52h:47m:57s remains)
INFO - root - 2017-12-15 17:36:59.082929: step 40940, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 52h:06m:02s remains)
INFO - root - 2017-12-15 17:37:05.443197: step 40950, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:23m:06s remains)
INFO - root - 2017-12-15 17:37:11.814278: step 40960, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 51h:35m:45s remains)
INFO - root - 2017-12-15 17:37:18.280231: step 40970, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 51h:24m:41s remains)
INFO - root - 2017-12-15 17:37:24.728647: step 40980, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 51h:47m:35s remains)
INFO - root - 2017-12-15 17:37:31.267394: step 40990, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 53h:15m:08s remains)
INFO - root - 2017-12-15 17:37:37.722546: step 41000, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:40m:52s remains)
2017-12-15 17:37:38.275013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.820735 -3.9028096 -4.2345581 -3.8166723 -3.42213 -2.5799894 -1.8896317 -1.1491694 -0.62621784 -2.0865164 -3.1691117 -7.00233 -8.9915905 -10.426579 -12.07987][-2.5492682 -3.4411311 -4.6478586 -4.97935 -5.0914545 -4.0785017 -3.0313292 -2.2090874 -1.4196892 -2.8674831 -4.0389929 -7.9507332 -9.7047167 -10.359459 -11.434962][-2.4389286 -2.7745538 -3.3356285 -3.5138264 -3.7655199 -3.4426532 -2.938283 -1.9963493 -1.6290221 -3.2825303 -4.586689 -7.7869115 -9.0366631 -9.4093409 -9.8846741][-2.5483174 -2.4177818 -2.7070298 -2.6590118 -1.9636035 -1.6256151 -1.5624094 -0.90066385 -0.6722331 -2.7696457 -3.9257202 -7.5097523 -8.8811893 -8.82135 -9.1559677][-2.1750236 -1.7995148 -1.9890122 -2.0499697 -1.629375 -0.63000774 0.28593397 1.1343536 1.3443155 -1.1618409 -2.6520491 -6.8063011 -8.4636831 -8.8847914 -9.7806606][-2.9190536 -2.3921251 -1.7729306 -1.1514626 -0.30152035 1.3754416 2.6255274 3.0305815 3.21972 0.6945734 -1.3041229 -5.2127113 -7.0721731 -7.461309 -7.8660221][-3.79641 -2.8614125 -1.8015323 -0.54798031 0.67003155 1.9639292 3.0474138 3.5587759 3.7634735 1.1352282 -0.81011724 -5.107935 -6.8200903 -7.5484705 -8.2751455][-3.3471117 -2.8626003 -2.0482421 -0.50872421 0.95684147 2.0763607 2.8155766 3.5440445 3.6156015 1.3209658 -0.33347607 -4.5007124 -6.11137 -7.0088491 -7.7783213][-3.5637441 -2.5348721 -1.9361658 -1.0434842 0.15484762 1.6553106 2.450223 3.0747528 3.5893803 1.4199228 -0.6022315 -4.5538616 -6.0764766 -6.5986452 -7.59641][-5.1151848 -4.363801 -3.3998904 -2.2905388 -1.2364454 -0.073899269 0.67305756 1.0665789 1.4593372 -0.52831078 -2.0388222 -5.0600843 -6.2860155 -6.8261948 -7.8370867][-6.1882262 -6.261899 -5.5582266 -4.4802155 -3.3399892 -2.5239744 -1.7763963 -1.3251562 -1.4378905 -3.1348615 -4.4447317 -6.6596713 -7.5804906 -8.0733595 -8.3701839][-8.3035583 -7.909977 -6.9064302 -6.0324836 -5.4992008 -5.0210309 -4.7870779 -4.5946927 -4.4186597 -5.1734247 -5.9369755 -7.3269215 -7.6347418 -8.5038452 -8.8935413][-8.90708 -8.6836939 -7.9368677 -7.4673281 -6.9018159 -6.4930863 -6.541481 -6.3980522 -6.016655 -5.9716611 -6.2909427 -7.1057677 -6.7695169 -6.9550328 -7.09933][-7.8683543 -8.1827917 -8.0283308 -7.7423677 -7.3855009 -7.0944891 -6.937336 -6.3897104 -5.8589959 -5.7028656 -5.5540724 -5.7076979 -5.4375343 -5.772634 -5.7876983][-8.3165283 -8.9297562 -8.8994122 -9.097868 -8.8570013 -8.43654 -7.9889336 -7.7916551 -7.32928 -7.1345067 -6.8739991 -6.0495195 -5.6226845 -5.9497581 -5.8155475]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:37:44.796951: step 41010, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.668 sec/batch; 54h:05m:45s remains)
INFO - root - 2017-12-15 17:37:51.351093: step 41020, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 52h:09m:21s remains)
INFO - root - 2017-12-15 17:37:57.789762: step 41030, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 52h:15m:35s remains)
INFO - root - 2017-12-15 17:38:04.255923: step 41040, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 51h:51m:52s remains)
INFO - root - 2017-12-15 17:38:10.676093: step 41050, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 51h:27m:52s remains)
INFO - root - 2017-12-15 17:38:17.061754: step 41060, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:35m:20s remains)
INFO - root - 2017-12-15 17:38:23.461102: step 41070, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 51h:45m:08s remains)
INFO - root - 2017-12-15 17:38:29.898637: step 41080, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 52h:44m:52s remains)
INFO - root - 2017-12-15 17:38:36.312060: step 41090, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 51h:16m:23s remains)
INFO - root - 2017-12-15 17:38:42.720479: step 41100, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 50h:52m:25s remains)
2017-12-15 17:38:43.197495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7726774 -3.5718718 -4.3934 -4.463716 -3.850853 -3.6209326 -3.63874 -3.3394527 -2.9749708 -3.2423272 -3.5301466 -5.3835049 -6.3191719 -6.1817212 -7.2047863][-3.2035141 -3.918164 -4.7264366 -5.0027113 -4.8270807 -4.27546 -3.6900263 -3.5933304 -3.6754632 -3.8958795 -4.2256823 -5.9730692 -6.7387085 -6.9549809 -7.5983148][-3.743624 -4.1285396 -4.7729273 -4.7105494 -4.8297453 -4.5710716 -4.10017 -3.738162 -3.196825 -3.8661196 -4.4809084 -6.4802012 -7.2242703 -7.5046744 -8.3133249][-4.4159174 -4.7117834 -5.0460472 -4.6999979 -4.26861 -3.8805325 -2.9575586 -2.4287429 -2.255631 -2.8625102 -3.5953851 -5.8490887 -6.989841 -7.3546929 -7.934351][-5.4795933 -5.5846987 -5.6978178 -5.1186757 -4.286787 -3.3909101 -1.8824844 -1.1410108 -0.65996408 -1.7055235 -3.0478778 -5.54121 -6.7254329 -7.23329 -8.1554537][-6.9333863 -6.5659137 -6.1028037 -4.9819918 -3.616137 -2.1456218 -0.42938232 0.54366493 1.0075655 -0.57379007 -2.6890831 -5.8435507 -6.8361077 -7.1022763 -7.7441659][-6.8875008 -6.3668242 -6.0663872 -4.56242 -2.4793191 -0.30555964 1.9420652 2.622117 2.9918079 1.4473209 -0.88801241 -4.401773 -6.1427407 -6.44497 -7.0878887][-6.3349228 -6.000567 -5.3592196 -3.7877467 -1.9344258 0.88135624 3.4354582 3.8967628 3.4746962 1.6587458 -0.14143562 -3.3111806 -4.9807339 -5.4027252 -6.09093][-6.3908577 -5.9459915 -5.6313953 -4.27262 -2.3960748 0.045030117 2.6419573 3.2557278 3.1727724 0.98238945 -1.0185294 -3.9645989 -5.3988004 -5.6899033 -6.6038723][-6.497282 -6.1565323 -6.017581 -4.8186841 -3.40312 -1.5846562 0.37989044 0.93835354 1.147007 -0.84749651 -2.9258146 -5.5978765 -6.6634717 -6.1910081 -6.97567][-7.9017715 -7.5970321 -7.2334967 -6.0910897 -5.0064764 -3.7233987 -2.4889235 -1.965692 -1.6810527 -3.0556359 -3.9947512 -6.2100258 -7.164763 -6.5921879 -7.026886][-7.6890531 -7.4217291 -7.1943536 -6.3159409 -5.7416573 -4.8906837 -3.8988614 -3.8498359 -3.7514482 -4.3637791 -4.5102687 -5.9678669 -6.6820607 -6.6119719 -6.9262018][-8.9018745 -8.1570759 -7.488481 -6.6746445 -6.1481152 -5.974102 -5.6044655 -5.4757104 -5.147975 -6.18505 -6.3765039 -6.4968777 -6.5248728 -6.6460862 -6.8727484][-8.9192429 -8.2921677 -7.4387817 -6.3650556 -5.6943016 -5.7314262 -5.3433146 -5.5018272 -5.4244618 -5.7128453 -6.1298285 -6.2420197 -6.2743163 -6.5626168 -6.6783175][-8.9637575 -8.9289141 -8.401125 -7.3425012 -6.5961962 -6.4264393 -6.6414013 -6.7626848 -6.6013308 -6.5584469 -6.6470814 -6.3686361 -6.1824074 -5.9801531 -5.7122188]]...]
INFO - root - 2017-12-15 17:38:49.597462: step 41110, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 51h:16m:13s remains)
INFO - root - 2017-12-15 17:38:56.117920: step 41120, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:17m:24s remains)
INFO - root - 2017-12-15 17:39:02.521728: step 41130, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:23m:47s remains)
INFO - root - 2017-12-15 17:39:08.949689: step 41140, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 52h:16m:43s remains)
INFO - root - 2017-12-15 17:39:15.276735: step 41150, loss = 0.35, batch loss = 0.23 (13.0 examples/sec; 0.616 sec/batch; 49h:50m:48s remains)
INFO - root - 2017-12-15 17:39:21.728981: step 41160, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 51h:24m:10s remains)
INFO - root - 2017-12-15 17:39:28.187303: step 41170, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 53h:13m:19s remains)
INFO - root - 2017-12-15 17:39:34.690227: step 41180, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 53h:17m:54s remains)
INFO - root - 2017-12-15 17:39:41.185057: step 41190, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 51h:24m:16s remains)
INFO - root - 2017-12-15 17:39:47.542522: step 41200, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 50h:42m:20s remains)
2017-12-15 17:39:48.024668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7494 -6.7709265 -7.2953472 -7.0322495 -7.0182428 -6.7695684 -5.6999421 -4.5908785 -3.5758057 -5.1423168 -5.8911462 -5.6770554 -6.1292782 -7.2362027 -7.3229232][-6.3077717 -6.917326 -7.0922928 -7.6037955 -7.8597665 -7.5291791 -6.8705554 -5.9631305 -4.7077188 -6.1877232 -6.753511 -6.7998405 -7.5608482 -8.4055853 -8.8603792][-5.3972883 -5.7518959 -6.0467787 -6.4425035 -6.7476788 -7.003603 -6.6989 -5.8691368 -5.2305036 -6.8413534 -7.178092 -7.2315283 -7.745492 -8.7957325 -9.3546448][-6.3499985 -6.0585346 -6.0147495 -5.8184209 -5.9378624 -5.3629847 -4.4088669 -3.9132988 -3.4576602 -5.514729 -6.5684309 -7.1456122 -7.8894172 -8.688652 -9.1137218][-7.5622821 -6.9494963 -6.0403476 -5.0989289 -4.5735407 -3.226757 -2.3032751 -1.9791694 -1.7836914 -4.2990217 -5.6293154 -6.3266697 -7.139647 -8.0295258 -8.6394043][-9.0929384 -8.0310907 -6.9389815 -4.7858849 -2.6702609 -0.90672207 0.58443737 0.74683094 0.735837 -2.1943302 -4.387136 -5.492259 -6.1885095 -7.3387356 -8.4804106][-9.4055424 -7.95912 -5.7922235 -2.8867764 -0.28674698 2.3388805 4.14641 4.4206715 4.3817759 0.35555744 -2.6377583 -4.1059227 -5.3168678 -6.340641 -7.0001168][-8.4330025 -7.3125768 -6.0218954 -2.9943829 0.51530361 2.8210583 4.7104979 5.5792885 5.4938431 1.2084894 -2.1938953 -4.1340613 -5.1230073 -6.0733185 -6.7290711][-9.1548452 -8.6110315 -7.4799118 -4.6562157 -1.8471642 0.66790962 2.7211008 3.2130814 3.4656496 0.034539223 -2.9958177 -4.4942622 -5.4680972 -6.2505484 -6.3300858][-10.367483 -9.7502232 -8.4666939 -5.9026041 -2.8223653 -0.65853739 1.1477222 1.7907944 1.4067974 -2.2358079 -4.4559512 -5.6274118 -6.85832 -7.5938387 -7.3753672][-11.58646 -11.03371 -10.362623 -8.5783024 -6.70551 -4.4325171 -2.2214251 -2.202373 -2.69766 -5.3954172 -6.919507 -7.5359921 -8.2101212 -9.1017685 -9.0619087][-11.657038 -10.944443 -10.106608 -9.3540831 -8.5758448 -7.2819657 -6.4441619 -5.7144375 -5.0908709 -7.3940439 -8.6018324 -8.8300877 -9.112443 -9.289259 -8.9184875][-11.843336 -11.279377 -10.728239 -9.7772512 -8.8552818 -7.90282 -7.1851912 -7.0681787 -7.2212892 -8.9859447 -9.7415714 -9.5947094 -9.8066168 -9.5386448 -9.0451345][-10.760715 -10.195837 -9.8847589 -9.2571993 -8.2936211 -7.40484 -6.9447565 -6.8411031 -7.0635986 -8.35767 -8.8405619 -9.14998 -9.6858215 -9.71034 -8.9534216][-10.247308 -9.5004807 -9.2141085 -8.6814032 -8.2452631 -7.676116 -7.0460558 -7.2609072 -7.3668962 -7.7148314 -7.8553748 -7.867907 -7.8104806 -7.883409 -7.5456324]]...]
INFO - root - 2017-12-15 17:39:54.459887: step 41210, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 52h:03m:24s remains)
INFO - root - 2017-12-15 17:40:00.825237: step 41220, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 50h:58m:36s remains)
INFO - root - 2017-12-15 17:40:07.377666: step 41230, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 54h:39m:05s remains)
INFO - root - 2017-12-15 17:40:13.801012: step 41240, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 51h:00m:53s remains)
INFO - root - 2017-12-15 17:40:20.316816: step 41250, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:28m:46s remains)
INFO - root - 2017-12-15 17:40:26.801190: step 41260, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 51h:15m:57s remains)
INFO - root - 2017-12-15 17:40:33.133201: step 41270, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:32m:48s remains)
INFO - root - 2017-12-15 17:40:39.516683: step 41280, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 51h:39m:54s remains)
INFO - root - 2017-12-15 17:40:46.038399: step 41290, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 51h:32m:09s remains)
INFO - root - 2017-12-15 17:40:52.366320: step 41300, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 50h:03m:28s remains)
2017-12-15 17:40:52.874423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7781777 -3.7571387 -3.6427388 -3.2526269 -2.7146912 -2.4200225 -1.8222928 -1.5681105 -1.5843711 -3.4564481 -6.0809383 -7.2354646 -7.8591957 -9.3515053 -9.4609251][-4.7316589 -4.6398306 -4.1911669 -3.9157462 -3.8199553 -3.4538155 -3.1948762 -2.8508945 -2.5829387 -4.3371296 -6.62402 -7.2593446 -8.2687588 -9.8097792 -9.705162][-4.7045059 -3.9882007 -3.7427967 -3.1297178 -2.6514688 -2.7291951 -2.3626671 -1.9998975 -1.7579069 -3.2108679 -5.1431303 -6.2759328 -7.0872569 -8.9103842 -9.3069038][-3.7384598 -3.4692054 -2.7259736 -1.9327145 -1.652122 -1.3203573 -0.78441381 -0.55822849 0.047575474 -1.2298527 -3.0798202 -3.9797387 -5.1577616 -6.5300326 -6.9661551][-3.5780597 -2.5856819 -1.7102599 -1.0253997 -0.2100935 0.8363018 1.6767921 2.031374 2.1249485 0.64927483 -1.3233752 -3.2270927 -5.0248127 -6.7106471 -7.1825104][-3.2899041 -2.2028341 -1.1167383 0.013702869 1.1030617 2.3836384 3.307251 3.9127712 4.5709124 2.8388643 -0.051091671 -1.9268222 -3.7543051 -5.947978 -6.468049][-3.3327403 -2.0719972 -0.59922934 0.75600815 2.1021318 3.599781 4.4615793 5.176774 5.9273911 3.9261703 0.82381725 -1.9076834 -4.2253 -6.1102943 -6.1167283][-3.6162806 -2.0663643 -0.6373868 0.94098377 2.7446384 4.2323971 5.3774939 5.9953432 6.0017319 3.5643454 0.34723282 -1.9174004 -4.2938533 -6.3662157 -6.32823][-4.0484338 -3.1716962 -1.503087 0.40215874 2.3366337 4.2782784 4.9707708 4.9731178 4.5124865 1.7367163 -0.85866785 -2.5131097 -4.4863234 -6.2210407 -6.3539209][-3.849673 -3.1090055 -1.9256778 -0.26894903 1.3297749 2.5104628 2.9900618 3.4494905 2.4747047 -0.073910236 -1.9127889 -3.6470909 -4.6150303 -6.1015024 -5.9831223][-4.8178248 -4.2446804 -3.5102882 -2.3445468 -1.0222597 0.094799519 1.1201792 0.85095978 0.072104454 -1.8819823 -4.4105449 -5.8293524 -6.3088341 -6.9574776 -6.4653416][-5.9879918 -5.6684003 -5.0956278 -4.6398554 -3.884661 -2.65059 -2.0671778 -2.495543 -2.6525784 -4.0338931 -5.7259674 -6.3484845 -6.8686976 -7.3486681 -7.02209][-7.0456748 -7.0999775 -6.722744 -5.9506593 -5.50881 -4.5788469 -3.893153 -3.7358198 -3.9300492 -4.8736997 -6.1191869 -6.5013981 -6.9294877 -7.00706 -6.2400422][-7.6102414 -7.3274007 -6.5054665 -6.0173683 -5.4762163 -4.2717028 -3.6374011 -3.7952292 -4.0911226 -4.84056 -5.4991922 -5.8836422 -6.3453403 -6.57238 -6.0563521][-7.9915533 -8.00166 -7.082931 -6.4494238 -5.90272 -5.0100393 -4.6181326 -4.8704166 -5.184761 -5.3824248 -5.66071 -5.6625795 -5.7981472 -5.9306421 -5.9379964]]...]
INFO - root - 2017-12-15 17:40:59.421097: step 41310, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 52h:13m:33s remains)
INFO - root - 2017-12-15 17:41:05.896257: step 41320, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 52h:43m:46s remains)
INFO - root - 2017-12-15 17:41:12.364990: step 41330, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 52h:58m:41s remains)
INFO - root - 2017-12-15 17:41:18.808507: step 41340, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 52h:20m:26s remains)
INFO - root - 2017-12-15 17:41:25.264255: step 41350, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 51h:42m:22s remains)
INFO - root - 2017-12-15 17:41:31.731123: step 41360, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 52h:37m:44s remains)
INFO - root - 2017-12-15 17:41:38.183712: step 41370, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 53h:15m:29s remains)
INFO - root - 2017-12-15 17:41:44.700573: step 41380, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 53h:48m:33s remains)
INFO - root - 2017-12-15 17:41:51.046719: step 41390, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.632 sec/batch; 51h:08m:22s remains)
INFO - root - 2017-12-15 17:41:57.508648: step 41400, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 50h:24m:41s remains)
2017-12-15 17:41:58.070750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.56343 -9.1862717 -10.315996 -11.124259 -11.204615 -10.250506 -9.5668736 -8.5448866 -6.7455764 -5.7814889 -5.5780964 -5.8682117 -5.6278658 -6.082181 -6.4609203][-7.4197559 -9.2034636 -10.472034 -11.246518 -11.628551 -11.059736 -10.380571 -9.08408 -7.460155 -6.88382 -6.9351883 -7.0642796 -7.0632133 -6.9777613 -6.8979826][-7.7376609 -8.9822292 -10.064586 -10.906454 -11.269588 -10.537799 -9.4919415 -8.9256182 -8.0387936 -7.7508855 -7.76989 -8.3318186 -8.4359579 -8.0692 -8.2364731][-7.7231545 -8.1392488 -8.2077885 -7.9638062 -7.8900523 -7.2240405 -6.5747323 -6.2302904 -5.4560204 -6.1416082 -7.221242 -8.3260193 -8.3505335 -8.9668884 -9.1005707][-7.3916287 -6.4483061 -5.8781466 -5.3494282 -4.9805803 -3.3030858 -1.8927441 -1.5782018 -1.6443453 -3.6121626 -5.5526962 -7.3878555 -8.5333557 -9.143301 -9.3480072][-6.2572842 -5.143548 -4.2161236 -2.1691122 -0.85950565 0.45253181 2.0128393 2.6826 2.9118919 0.29185724 -3.1280742 -6.1402774 -7.494535 -8.4833794 -9.0700541][-5.8535957 -5.0827837 -3.1424618 -0.80763674 0.7465353 2.3719225 3.7014647 4.3406029 4.8508472 1.9818916 -1.2747116 -4.3177552 -6.0650311 -7.3395185 -7.9635782][-5.4702463 -4.698185 -3.4528246 -0.8937192 1.3775024 2.5891504 3.8002825 4.2236109 4.4018564 2.3226662 -0.6352849 -3.7467182 -5.5798993 -6.5500889 -7.2638249][-5.1977329 -4.53553 -3.7864327 -2.2882166 -1.434577 0.34221554 1.8938351 2.3976965 3.1530113 0.57040405 -2.3184314 -4.7933297 -6.2880096 -7.1896143 -7.4819908][-6.8394456 -6.2092338 -5.6335068 -4.1734586 -2.9173131 -2.2615724 -2.084053 -1.0026064 -0.48061991 -3.0939317 -5.4432116 -7.2083898 -8.1995831 -8.3203421 -8.1304274][-8.7456837 -8.6641331 -8.4941568 -7.52411 -6.2710476 -5.3728313 -4.7701359 -4.4385834 -4.7521391 -6.0411525 -7.600235 -8.5846748 -8.89551 -9.2398739 -9.142581][-10.048487 -9.8358936 -9.2390776 -8.5169888 -7.7946062 -7.2678204 -7.0047174 -6.5083776 -6.0636306 -6.9770942 -8.17877 -8.2789154 -8.2771978 -8.289362 -8.3076544][-9.2204456 -9.217144 -9.8979187 -9.197298 -8.3241453 -7.6441641 -7.2057109 -7.4721589 -7.1654077 -7.4538159 -8.1348257 -7.6658258 -7.2430263 -7.1435037 -7.1615095][-8.8608809 -8.3155117 -7.8220663 -7.4441361 -7.6977816 -7.1056762 -6.4835358 -6.4892383 -6.78497 -6.9903383 -7.3486347 -7.1585522 -7.3149219 -6.9896479 -6.1958714][-7.0673819 -7.2206321 -7.5165515 -6.9788136 -6.8919806 -6.6842051 -6.9019222 -7.3932428 -7.705327 -7.5893888 -7.2137055 -7.3158078 -7.49796 -7.2790842 -6.8501081]]...]
INFO - root - 2017-12-15 17:42:04.506213: step 41410, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:56m:05s remains)
INFO - root - 2017-12-15 17:42:10.919102: step 41420, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 51h:34m:56s remains)
INFO - root - 2017-12-15 17:42:17.390848: step 41430, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 50h:44m:48s remains)
INFO - root - 2017-12-15 17:42:23.970026: step 41440, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 53h:05m:51s remains)
INFO - root - 2017-12-15 17:42:30.444418: step 41450, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 51h:52m:42s remains)
INFO - root - 2017-12-15 17:42:36.852334: step 41460, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.669 sec/batch; 54h:06m:44s remains)
INFO - root - 2017-12-15 17:42:43.321900: step 41470, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 53h:01m:47s remains)
INFO - root - 2017-12-15 17:42:49.796404: step 41480, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 50h:37m:00s remains)
INFO - root - 2017-12-15 17:42:56.232980: step 41490, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 53h:03m:00s remains)
INFO - root - 2017-12-15 17:43:02.683604: step 41500, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:13m:29s remains)
2017-12-15 17:43:03.237576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3074102 -3.179091 -3.2885647 -2.9264598 -1.9419708 -1.1538491 -1.0745964 -1.1541071 -0.85876656 -0.68869019 -1.9470077 -4.064455 -4.9068904 -5.7850046 -6.6930571][-2.4810038 -3.1073904 -3.8310277 -3.6293731 -3.0125089 -1.7927642 -0.79995346 -0.7892189 -0.83405447 -1.1759772 -2.4513006 -4.3044581 -5.5833316 -6.5799813 -7.2358093][-3.8575518 -3.6839423 -3.8943462 -3.8604586 -3.336504 -2.7060061 -1.9670901 -1.7340889 -1.6110888 -2.0247416 -3.4281573 -5.0154772 -5.7413054 -7.1840463 -7.984323][-4.4607182 -4.3800511 -3.9116297 -3.246006 -2.4722733 -1.8123651 -1.1720557 -1.0791407 -1.1176505 -1.5734401 -2.8044224 -4.783473 -5.5105228 -6.5532084 -7.3326163][-4.172874 -3.755908 -2.8040614 -1.8337336 -1.2010937 -0.13912773 0.27164841 -0.052042007 -0.69500065 -1.4999347 -2.9327469 -4.8102865 -5.3686504 -6.2703762 -7.0723233][-4.7657461 -4.17624 -3.163826 -1.7538829 -0.22299385 0.7409935 1.2610407 0.79976654 -0.0045762062 -1.4875202 -3.508204 -5.4513044 -6.2528772 -6.7994118 -7.3501735][-4.4742613 -4.0039387 -3.2957196 -1.4010344 0.66762161 2.1309443 2.9914646 2.573966 1.4712305 -0.64890528 -3.1255088 -5.3413925 -5.8520164 -6.625196 -7.1232424][-4.0907869 -3.0957079 -1.9242725 -0.24688053 1.495472 3.1714315 3.8386555 3.4247942 2.397891 0.47031975 -1.9760995 -4.9566317 -5.92903 -6.7164316 -6.9882803][-4.2333651 -3.3151436 -2.3535676 -1.2559762 0.47589779 2.0716696 2.7163153 2.5909147 2.0469837 0.1081953 -2.2927108 -4.8212442 -5.7101116 -6.5879574 -7.2507176][-5.0961022 -4.8955793 -4.1807566 -3.1789374 -1.8850169 -0.42715168 0.53400135 1.0137711 1.0078392 -0.92205858 -3.3487735 -5.4974513 -6.3378277 -7.1138449 -7.7876697][-5.9753518 -5.8706684 -5.5398388 -4.6601949 -4.1090608 -2.9225383 -2.0589294 -1.787014 -1.773859 -3.0904465 -4.7421227 -6.7735825 -6.9202652 -7.6350656 -7.6286654][-7.310113 -7.0966935 -6.7258396 -6.1825647 -5.8630943 -5.1390986 -4.370791 -4.0599751 -3.7664459 -4.04913 -5.0560842 -6.6597443 -6.6132951 -7.3196421 -7.662981][-7.9162927 -7.5485058 -7.0243778 -6.5656233 -6.2986 -5.8408785 -5.3083858 -5.2379608 -4.9324989 -5.1542549 -5.3283567 -6.0757537 -6.182611 -6.9603348 -7.4240389][-7.7623992 -7.5491982 -7.1430988 -6.5299397 -6.4503179 -6.2378278 -5.9865294 -6.1259718 -6.1363297 -6.4117303 -6.327354 -6.312542 -6.5350084 -6.6908727 -6.5988259][-8.2140265 -8.151804 -8.2815008 -7.8884926 -7.81963 -7.4474959 -7.2282486 -7.3483577 -7.283534 -6.8955569 -6.66385 -6.8591576 -6.3346415 -6.6398849 -7.1843419]]...]
INFO - root - 2017-12-15 17:43:09.712666: step 41510, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 51h:53m:18s remains)
INFO - root - 2017-12-15 17:43:16.077619: step 41520, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 50h:50m:50s remains)
INFO - root - 2017-12-15 17:43:22.509702: step 41530, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 52h:11m:56s remains)
INFO - root - 2017-12-15 17:43:28.958041: step 41540, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 52h:13m:38s remains)
INFO - root - 2017-12-15 17:43:35.291126: step 41550, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 50h:22m:51s remains)
INFO - root - 2017-12-15 17:43:41.727517: step 41560, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 53h:14m:43s remains)
INFO - root - 2017-12-15 17:43:48.156623: step 41570, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 52h:31m:40s remains)
INFO - root - 2017-12-15 17:43:54.520209: step 41580, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.615 sec/batch; 49h:40m:49s remains)
INFO - root - 2017-12-15 17:44:00.931981: step 41590, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 49h:53m:20s remains)
INFO - root - 2017-12-15 17:44:07.284281: step 41600, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 51h:13m:59s remains)
2017-12-15 17:44:07.819431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0375137 -3.5875425 -3.5795255 -3.7816329 -4.33335 -4.6297054 -4.3107967 -4.00463 -3.5957966 -3.3284922 -5.0841455 -5.9405508 -6.3501635 -7.5315261 -8.4919538][-3.0747213 -3.1618443 -3.5324111 -3.9378071 -4.5627079 -4.7662392 -5.0902605 -4.6581869 -3.9829798 -3.4318876 -4.5127478 -5.3616123 -5.6032791 -6.7693005 -7.6216598][-2.1450067 -2.5196729 -2.9076138 -3.4594569 -3.8193672 -3.8389912 -4.0210819 -3.9311247 -3.5419211 -3.5099158 -4.4862728 -4.9053154 -4.8768158 -5.6674757 -6.3845377][-2.1811924 -1.8310151 -2.3454089 -2.7501359 -2.7936006 -2.489295 -2.1651134 -2.5867119 -2.4197755 -2.5964632 -3.9394052 -4.25209 -4.7347813 -5.566205 -5.9074187][-2.7595754 -2.5641651 -1.9596629 -1.7761211 -1.6464896 -1.013556 -0.23136044 -0.14851332 -0.055119514 -0.7120018 -2.6259356 -3.2352843 -4.030941 -4.9190712 -5.5952363][-3.41848 -3.2648573 -2.4747705 -1.8532052 -0.78033209 0.52013493 1.4892006 1.7165365 1.8450317 1.2466412 -0.60907221 -1.6460857 -2.8484178 -4.2005653 -5.3072476][-4.0935736 -3.4887013 -2.0852566 -1.1427956 0.054341793 1.3361282 2.2267256 2.4849892 2.7419939 2.2736387 0.41835785 -0.95665026 -2.3502188 -3.61097 -4.6348152][-3.9940627 -3.2972913 -2.2335949 -1.0619144 0.30981922 1.6874914 2.6827555 3.0725679 3.2834244 2.8235798 1.0481577 -0.56196547 -2.0889277 -3.1420498 -4.0539861][-4.639308 -3.4439774 -2.6102304 -1.5243769 -0.23582983 0.99137592 1.8681221 2.6434622 3.1545658 2.6063938 0.42596817 -1.1059222 -2.4755039 -3.3365526 -3.9931738][-5.4753175 -4.4697208 -3.3361912 -2.3007851 -1.191195 -0.26077938 0.44175816 0.94844151 1.3269768 0.75608253 -0.99594545 -2.228169 -3.4371462 -4.3308115 -5.0964231][-7.3440309 -6.5700626 -5.8618345 -5.1704683 -4.1090651 -3.3532257 -2.3964162 -1.8672123 -1.7050457 -2.2157736 -3.7399659 -4.4087186 -4.931437 -5.7269807 -6.5660558][-8.4852076 -8.2417145 -7.6943908 -6.9837146 -6.3938246 -5.9006925 -4.9846168 -4.3758292 -3.7484097 -3.9370036 -5.376997 -5.7326436 -6.0679111 -6.8016195 -7.1450996][-8.489933 -8.5235672 -8.1305895 -7.874455 -7.517065 -6.8370204 -6.2889228 -6.1999016 -5.9038281 -5.6596584 -6.4483452 -6.433847 -6.4458432 -6.7532182 -6.9033208][-7.5655475 -7.9295411 -7.9283314 -7.6507363 -7.3011303 -6.7943521 -6.5106297 -6.5699253 -6.8746791 -6.8919277 -7.4918923 -7.4263148 -7.1628737 -6.7509446 -6.2652774][-7.546442 -7.2137537 -6.9944963 -7.0297089 -6.831811 -6.3611631 -6.251771 -6.5396409 -6.852808 -7.190403 -7.4361014 -7.3157768 -7.1677661 -7.2551255 -6.9209929]]...]
INFO - root - 2017-12-15 17:44:14.153851: step 41610, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:32m:38s remains)
INFO - root - 2017-12-15 17:44:20.537504: step 41620, loss = 0.23, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 51h:23m:58s remains)
INFO - root - 2017-12-15 17:44:26.954324: step 41630, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 51h:01m:11s remains)
INFO - root - 2017-12-15 17:44:33.331101: step 41640, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 53h:05m:00s remains)
INFO - root - 2017-12-15 17:44:39.795496: step 41650, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 52h:46m:53s remains)
INFO - root - 2017-12-15 17:44:46.262042: step 41660, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 52h:04m:17s remains)
INFO - root - 2017-12-15 17:44:52.666498: step 41670, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 51h:58m:02s remains)
INFO - root - 2017-12-15 17:44:59.100460: step 41680, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 52h:52m:40s remains)
INFO - root - 2017-12-15 17:45:05.538666: step 41690, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 51h:57m:54s remains)
INFO - root - 2017-12-15 17:45:11.930795: step 41700, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 51h:01m:44s remains)
2017-12-15 17:45:12.528363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3266921 -1.1112552 -1.3901672 -1.5668621 -2.0888567 -2.7339702 -3.1672368 -3.2578979 -3.2319512 -4.350677 -4.5140247 -6.248384 -7.9262018 -8.3494043 -8.9546223][-1.679213 -1.6222081 -1.9668007 -2.5506692 -3.1148596 -3.3374629 -3.390358 -3.8287287 -3.8628213 -5.1896539 -5.702817 -6.9926896 -7.94998 -8.6601324 -9.924036][-2.0888553 -1.6867399 -1.7547483 -2.000072 -2.5509462 -3.0763164 -3.4158654 -3.7631211 -3.762578 -5.0687265 -5.4198761 -6.755229 -7.8174548 -8.22059 -8.8731222][-2.4917026 -2.0814624 -2.1461754 -1.98136 -1.9764342 -2.0617204 -2.2739921 -2.7058663 -2.9583044 -4.3165646 -4.647923 -6.1247206 -7.1992 -7.4297295 -8.0766726][-2.8115497 -2.2591443 -1.7490077 -1.3729234 -1.0319891 -0.817575 -0.89866877 -1.1876311 -1.421751 -3.1206689 -3.808383 -4.9013596 -5.7048416 -6.2400184 -6.8881497][-3.3923407 -2.712543 -1.8960767 -0.95971632 -0.057365894 0.33645535 0.19895601 0.041025639 -0.19011879 -1.7994456 -2.5714269 -4.203949 -5.31292 -5.500042 -6.1133819][-4.1933732 -3.3093224 -2.4488449 -1.3056855 0.010181427 0.85616207 1.514163 1.7088003 1.5674791 -0.10464334 -1.114821 -2.8912902 -4.26222 -5.0591192 -6.0204282][-4.94419 -3.9936171 -2.5404105 -1.2910314 0.025137901 1.06281 1.9360304 2.6984653 3.139823 1.6893835 0.25802851 -2.3538332 -4.3651 -5.1614437 -6.0864635][-4.5399 -4.0091472 -2.9972548 -1.5045538 0.08852911 1.2346544 1.9554338 2.9283981 3.5725012 2.0920315 0.64359856 -1.8092518 -4.0378323 -5.4059067 -6.33913][-4.810812 -4.330349 -3.421865 -2.039124 -0.729321 0.41436672 1.6885986 2.5127964 2.9797926 1.9498339 1.189373 -1.0086436 -3.2866144 -4.6374712 -5.89539][-5.8600721 -5.4034181 -4.5916924 -3.4053307 -1.9124284 -0.70632792 0.46265316 1.245122 1.9917393 1.141777 0.05622673 -1.6889091 -3.3122258 -4.5234179 -5.735086][-5.3920527 -5.5089178 -5.3946781 -4.32086 -3.0088224 -1.969986 -1.0081639 -0.30908966 0.24849129 -0.5667901 -1.0199833 -2.3359427 -4.0793476 -4.6908145 -5.3630748][-5.6640072 -5.4095621 -4.9585705 -4.4522772 -3.7389998 -2.8931522 -2.1381364 -2.06879 -1.7482529 -1.8543568 -2.0152297 -2.8160577 -3.7094848 -3.9816959 -4.6628532][-5.5162911 -5.6217947 -5.6028953 -5.1203957 -4.5754747 -3.9729526 -3.6635909 -3.4633856 -3.0490956 -3.3437161 -3.3393221 -3.2916613 -3.4429398 -4.0229578 -4.7514453][-6.0678968 -6.0160789 -6.026011 -5.9979048 -5.9733596 -5.6429071 -5.5924158 -5.5901465 -5.6204491 -5.544107 -5.2517033 -5.2389555 -5.0980387 -5.0764403 -5.2263832]]...]
INFO - root - 2017-12-15 17:45:18.851474: step 41710, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 51h:36m:03s remains)
INFO - root - 2017-12-15 17:45:25.282562: step 41720, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 51h:41m:46s remains)
INFO - root - 2017-12-15 17:45:31.652952: step 41730, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 51h:35m:26s remains)
INFO - root - 2017-12-15 17:45:37.992569: step 41740, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 51h:29m:04s remains)
INFO - root - 2017-12-15 17:45:44.412632: step 41750, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 52h:14m:18s remains)
INFO - root - 2017-12-15 17:45:50.786561: step 41760, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 52h:10m:50s remains)
INFO - root - 2017-12-15 17:45:57.192396: step 41770, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 50h:57m:25s remains)
INFO - root - 2017-12-15 17:46:03.624793: step 41780, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 52h:24m:18s remains)
INFO - root - 2017-12-15 17:46:10.168521: step 41790, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 51h:03m:25s remains)
INFO - root - 2017-12-15 17:46:16.644995: step 41800, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 52h:20m:35s remains)
2017-12-15 17:46:17.318860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.054307 -6.1011515 -5.6474676 -5.8213558 -6.2759643 -6.2006397 -6.3034716 -6.4170885 -6.2125239 -6.7442765 -7.1902418 -8.0828 -8.347785 -9.2618837 -9.1676331][-5.1930156 -5.4895 -5.8933215 -6.3591394 -6.1789994 -6.23366 -6.6572442 -6.4453964 -6.1668777 -6.6717663 -6.7613015 -7.7110405 -8.5620575 -9.322648 -9.01064][-5.3885884 -5.08097 -5.0430713 -5.1672721 -5.478889 -5.5992179 -5.2574167 -5.2645411 -5.15292 -5.4248171 -6.3614197 -7.3767629 -8.1219044 -8.84897 -8.6586885][-5.1925097 -4.98312 -4.9991355 -4.6455374 -4.361475 -3.901866 -3.6079445 -3.3849139 -2.7071867 -3.1026855 -3.8439171 -5.5853515 -7.4514608 -8.4962769 -8.3952379][-4.8508515 -4.1960039 -3.6453409 -3.5379071 -3.3825412 -2.5924482 -1.4395137 -1.0106292 -0.51649046 -1.234158 -2.1147141 -3.8666785 -5.1186533 -6.669704 -7.3998313][-4.2885351 -3.480134 -2.96671 -2.2192349 -1.3044519 -0.41863155 0.3907938 0.76483727 1.6785803 0.813715 -0.55358458 -2.9329495 -4.8794942 -6.0286756 -5.8423371][-3.7896943 -3.1779346 -2.3990946 -1.2546425 -0.3696394 0.92729378 1.8778687 2.2380667 2.6756229 1.4185743 -0.37797928 -3.1676769 -5.0495129 -6.517746 -6.6363373][-2.83961 -2.4348302 -1.7575397 -0.8095665 0.2632184 1.5537643 2.7941437 3.2267351 3.3892965 1.9101887 0.038761139 -3.0576186 -5.1887403 -6.6708508 -6.55983][-2.2626829 -1.93294 -1.0298953 -0.17692184 0.55142879 1.7146063 2.5388107 2.9270897 2.9969215 1.5900192 -0.061048985 -2.860826 -4.5918922 -5.8750339 -5.9651294][-1.9694796 -1.6511636 -0.77102232 -0.014229298 0.70593071 1.61864 2.0571032 1.9917688 1.5081949 0.21794271 -1.4351807 -3.6887746 -5.0778036 -6.1286511 -6.13329][-1.6940246 -1.3725057 -0.51360416 -0.21864653 0.21815491 0.940382 1.1606159 1.1409597 1.0027428 -0.17675781 -2.2344193 -4.124012 -5.1428442 -6.3919029 -6.3859138][-2.3639441 -1.5923786 -0.74320316 -0.21838474 0.25396109 0.87637234 1.0900812 0.99619675 0.47919655 -0.73034286 -2.5538993 -4.201756 -5.6136184 -6.6863875 -6.5916753][-3.5531058 -3.2811871 -2.9945459 -2.2527962 -1.4492087 -0.97128916 -0.46596956 -0.17931795 -0.24428511 -1.2478886 -2.74516 -4.124299 -5.4361367 -6.7318468 -6.7330604][-3.658721 -3.580668 -3.7886772 -3.9140215 -3.2643895 -2.6077352 -2.062531 -1.6512074 -1.9064598 -2.4449353 -3.4325595 -4.5290675 -5.8140955 -6.5817986 -6.3154149][-4.4264193 -4.5779552 -4.7325821 -4.9438744 -4.8752675 -4.3871236 -3.8801079 -3.9072404 -4.3608084 -4.7283297 -5.4412279 -6.2553558 -6.3977242 -6.5467539 -6.8037052]]...]
INFO - root - 2017-12-15 17:46:23.846788: step 41810, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 50h:48m:27s remains)
INFO - root - 2017-12-15 17:46:30.366272: step 41820, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 51h:13m:11s remains)
INFO - root - 2017-12-15 17:46:36.693352: step 41830, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 50h:51m:23s remains)
INFO - root - 2017-12-15 17:46:43.111506: step 41840, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 51h:45m:39s remains)
INFO - root - 2017-12-15 17:46:49.621618: step 41850, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 52h:14m:32s remains)
INFO - root - 2017-12-15 17:46:56.016711: step 41860, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 51h:17m:50s remains)
INFO - root - 2017-12-15 17:47:02.338254: step 41870, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 52h:30m:21s remains)
INFO - root - 2017-12-15 17:47:08.805293: step 41880, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 53h:04m:39s remains)
INFO - root - 2017-12-15 17:47:15.232402: step 41890, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 51h:47m:24s remains)
INFO - root - 2017-12-15 17:47:21.638349: step 41900, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 51h:32m:56s remains)
2017-12-15 17:47:22.151475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9674664 -5.1835384 -5.2020931 -5.1325378 -4.9806232 -4.4548707 -3.9613984 -4.1727233 -4.2723246 -5.031806 -5.6250515 -5.6774249 -5.4861612 -6.444767 -6.5238957][-5.1596546 -4.8322964 -4.5668564 -4.2743444 -4.3953252 -4.6559668 -4.54653 -4.23909 -3.7250292 -4.2806191 -4.8218241 -5.2470851 -5.1848841 -5.4380836 -5.2164459][-5.438818 -4.9513617 -4.9206238 -4.7897763 -4.4626436 -4.4109182 -4.427341 -4.4779806 -4.148643 -4.3008642 -4.5095735 -5.0885468 -5.1807642 -5.713686 -5.5723791][-6.15672 -5.2478514 -4.75041 -4.168746 -3.9337196 -3.8128188 -3.3316388 -3.254878 -2.9061713 -3.3418212 -4.0851665 -4.940156 -5.2646618 -6.0780487 -6.4301319][-6.3791618 -5.1358051 -4.1409144 -3.3484383 -2.9915748 -2.5082364 -1.8537178 -1.7695622 -1.3410945 -1.8953943 -2.6943803 -4.1853714 -5.4561386 -6.5927286 -6.8396125][-4.859684 -4.0842786 -3.5958643 -2.5812316 -1.698791 -1.0521049 -0.48791218 -0.30446863 0.044500828 -0.54570246 -1.3969884 -2.9465704 -4.4749441 -6.3903704 -7.359705][-4.0345879 -2.4019017 -1.3534083 -1.2179003 -0.75032806 0.30697584 1.0906115 1.0446758 1.2791805 1.0596886 0.35851765 -1.498498 -3.231657 -5.347312 -6.8624673][-3.1441913 -1.9155149 -0.928463 0.031745911 0.90415192 1.5263643 1.9645233 1.8926382 2.06981 1.5368881 0.73159122 -0.9110837 -2.6746984 -4.9521227 -5.96131][-2.8731489 -1.5972219 -0.72041512 0.23541021 1.3174524 2.1272659 2.2828121 2.0423689 2.0354424 1.1938648 0.28458023 -1.0155687 -2.4719329 -4.5940285 -5.6622629][-3.7126253 -2.4721112 -0.71917009 0.39160633 1.3894777 2.3991079 2.9050245 2.4900475 2.1654568 1.1243582 0.21038628 -0.90471554 -1.975996 -3.5872846 -4.9011307][-4.0094662 -3.7104743 -2.824687 -1.5320511 -0.3044014 0.7531929 1.8223305 2.1053276 2.2303295 1.0928364 0.2745409 -0.1780448 -0.94424582 -2.8533263 -4.5179195][-3.7580655 -4.12233 -4.0958471 -3.3675838 -2.2875595 -0.75202131 0.83817863 1.5365477 1.8088646 1.0731993 0.65957737 -0.0328269 -1.1334028 -2.8901796 -4.50943][-4.037405 -4.645792 -4.4642849 -4.090333 -3.5971184 -2.1654277 -0.58214808 -0.098586559 0.07816124 -0.1849308 -0.28754759 -0.67733669 -1.3348513 -2.8474803 -4.4516439][-4.1238904 -4.7301683 -4.6253214 -4.7544556 -4.558506 -3.8829658 -2.75379 -2.3101172 -1.9828143 -1.9478874 -2.3769116 -2.5582695 -2.3649845 -3.3589902 -4.6728706][-3.8075235 -4.4787731 -5.4480309 -5.8828459 -5.4997616 -5.0065722 -4.4555178 -4.590929 -4.3893328 -4.0584097 -3.9438224 -4.2620106 -4.6113157 -4.9935589 -5.4944487]]...]
INFO - root - 2017-12-15 17:47:28.600043: step 41910, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 52h:19m:15s remains)
INFO - root - 2017-12-15 17:47:34.950241: step 41920, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 52h:58m:30s remains)
INFO - root - 2017-12-15 17:47:41.342934: step 41930, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 51h:07m:54s remains)
INFO - root - 2017-12-15 17:47:47.735885: step 41940, loss = 0.32, batch loss = 0.20 (12.0 examples/sec; 0.669 sec/batch; 53h:57m:27s remains)
INFO - root - 2017-12-15 17:47:54.161471: step 41950, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.683 sec/batch; 55h:09m:26s remains)
INFO - root - 2017-12-15 17:48:00.622051: step 41960, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 52h:07m:25s remains)
INFO - root - 2017-12-15 17:48:06.935430: step 41970, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:30m:22s remains)
INFO - root - 2017-12-15 17:48:13.281619: step 41980, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 53h:04m:14s remains)
INFO - root - 2017-12-15 17:48:19.702852: step 41990, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 51h:54m:12s remains)
INFO - root - 2017-12-15 17:48:26.138670: step 42000, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 52h:19m:01s remains)
2017-12-15 17:48:26.636444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3185091 -6.302619 -5.9147062 -5.3956623 -4.1676683 -2.9595971 -1.6848145 -0.63842058 -0.12993765 -1.9039149 -3.282548 -5.2288809 -6.6149879 -7.191772 -8.26775][-5.6770935 -5.9685659 -6.0897603 -5.72799 -4.3565578 -2.7731276 -1.6656199 -0.772491 -0.66078186 -2.5962782 -3.8112068 -5.8135138 -7.1481438 -7.4661775 -8.6716938][-6.2652378 -6.2534947 -6.2314415 -5.5441103 -4.3840685 -3.0954685 -1.4788828 -0.5125742 -0.5340147 -2.9001431 -4.382298 -6.0641174 -6.8297267 -7.1742392 -8.5613518][-6.0700665 -6.2773094 -6.479959 -5.4564676 -3.9682329 -1.9467506 -0.098324776 0.8572731 1.3572922 -1.0636816 -2.9981046 -5.6750374 -6.71897 -6.7746482 -7.7769074][-6.928195 -6.37571 -5.7823572 -4.32993 -2.5868635 -0.82590342 1.3520517 1.814661 1.5274935 -1.0113196 -2.8097043 -4.8991642 -6.1127243 -6.718328 -7.7590742][-7.5907469 -6.8475261 -5.9532437 -3.9987843 -1.7175164 0.38210392 2.3759346 3.0500441 2.9195309 -0.54606819 -3.1798253 -5.5481548 -6.8143334 -7.0797482 -8.2303534][-8.048048 -6.9624844 -5.3683243 -3.3729024 -1.1913028 1.3384676 3.5696182 3.9849739 3.4626808 0.24538898 -2.2742691 -5.3072548 -6.8412142 -7.4573064 -8.6948929][-7.2050486 -6.4416704 -5.2672749 -3.0023623 -0.22895098 2.2127247 4.3956556 5.3966732 5.3616467 1.3760929 -2.0450134 -5.1551952 -7.0585718 -7.4815764 -8.1546488][-6.769937 -5.8336458 -4.4634638 -2.574832 -0.055077076 2.1207037 3.9912949 4.4554253 4.3937397 0.90484047 -2.242887 -5.3858738 -7.1872082 -7.5026159 -8.4569454][-6.6407743 -5.7755752 -5.009407 -2.7158756 -0.40969419 1.6607704 3.0309677 3.3178272 3.1399059 -0.64718628 -3.2611675 -6.2636137 -7.83595 -7.7716441 -8.3315382][-6.4433784 -5.9306684 -5.0136886 -3.0233459 -1.289721 0.6354332 1.5310774 0.91784859 -0.23378325 -3.4893594 -5.5008922 -7.0035038 -7.5753293 -7.3785038 -8.0900345][-5.8611565 -4.6893287 -3.0675597 -1.0835547 0.42902946 1.4060583 1.8329287 0.90542221 -0.4820013 -3.4426055 -5.0734529 -6.2511406 -7.1032743 -6.7213979 -7.4000936][-7.0786386 -5.1288118 -2.9653082 -0.65636826 0.55284119 1.2605543 1.6642923 0.68091583 -0.13028383 -3.0382414 -4.6859331 -5.5324469 -6.3047862 -5.9458184 -6.7137971][-8.1534576 -6.5435967 -4.4919043 -1.9891086 -0.42542267 -0.384315 -0.80722952 -1.0943251 -1.3492942 -2.5788145 -3.5553107 -4.9064989 -5.3023281 -5.2718821 -5.7769861][-8.557477 -7.3550034 -5.9766922 -4.0064635 -2.493535 -1.7927809 -1.7897129 -2.74925 -3.5530829 -4.0725126 -4.5186973 -4.8961539 -5.0582275 -5.0033517 -5.1778007]]...]
INFO - root - 2017-12-15 17:48:33.077565: step 42010, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 52h:48m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:48:39.495988: step 42020, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 51h:59m:23s remains)
INFO - root - 2017-12-15 17:48:45.856056: step 42030, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 50h:34m:58s remains)
INFO - root - 2017-12-15 17:48:52.193146: step 42040, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 51h:19m:45s remains)
INFO - root - 2017-12-15 17:48:58.667268: step 42050, loss = 0.30, batch loss = 0.18 (11.7 examples/sec; 0.683 sec/batch; 55h:05m:46s remains)
INFO - root - 2017-12-15 17:49:05.070784: step 42060, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 52h:01m:44s remains)
INFO - root - 2017-12-15 17:49:11.404061: step 42070, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 51h:17m:49s remains)
INFO - root - 2017-12-15 17:49:17.748152: step 42080, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:14m:03s remains)
INFO - root - 2017-12-15 17:49:24.089746: step 42090, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 51h:38m:09s remains)
INFO - root - 2017-12-15 17:49:30.537895: step 42100, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 51h:39m:57s remains)
2017-12-15 17:49:31.023855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4223728 -2.4533672 -2.8820634 -2.9355264 -2.7339506 -2.6229787 -2.3099685 -1.7608294 -1.2457981 -2.3191919 -3.2537208 -4.9155989 -5.4066095 -5.9850316 -6.6465015][-2.83384 -2.7318802 -3.274786 -3.8119366 -3.9211817 -3.3378248 -2.5641718 -2.1655359 -1.9734111 -2.9521866 -3.6877155 -5.4167666 -6.1049204 -6.8315554 -7.2807665][-3.4151826 -3.3624992 -3.7548456 -4.0069714 -4.0414891 -3.8192356 -3.1123838 -2.4407978 -2.0033994 -3.4493957 -4.5877471 -5.986732 -6.3252468 -7.0906062 -7.825212][-3.6980591 -3.7437973 -3.9437628 -3.5836577 -3.1626248 -2.5025625 -1.6493878 -1.1576676 -0.91722679 -2.3310966 -3.5712862 -5.4167247 -6.1888275 -7.0044532 -7.3326778][-4.1093531 -4.0578594 -3.9366446 -3.3110976 -2.7133865 -1.337132 -0.018771648 0.39963531 0.28816128 -1.4990015 -3.0967579 -5.3152866 -6.2159624 -7.0464215 -7.683351][-5.140039 -5.0801749 -4.7955837 -3.5508528 -2.0437078 -0.29539967 1.2985792 2.0138197 2.1069441 -0.320776 -2.5744514 -5.1027403 -6.29644 -7.2612653 -7.90926][-5.2125988 -5.0017014 -4.7919755 -3.2751107 -1.2373085 1.1607971 2.9820051 3.8391285 4.29531 2.055686 -0.62956381 -4.121438 -5.7374535 -6.8476048 -7.5138922][-4.7568979 -4.4514084 -4.1374159 -2.5868154 -0.77796364 1.8856516 4.1026239 5.2483292 5.43719 2.9851675 0.56961823 -2.8341193 -4.9646482 -6.3605108 -7.0212069][-5.1393185 -4.5831213 -3.9591889 -3.0580378 -1.5194149 0.73027706 2.6357851 3.9025354 4.6416759 2.4284286 -0.0016803741 -3.0385361 -4.6048822 -6.0456438 -7.075109][-5.3324537 -4.8150015 -4.2321682 -3.3064423 -2.2413244 -0.79585743 0.55803013 1.7950954 2.1949739 0.16283703 -1.5506611 -4.1613846 -5.33554 -6.37681 -7.1547847][-6.5392756 -6.0232511 -5.5656271 -4.7892847 -4.0466003 -3.0502691 -2.1857319 -1.484704 -0.97027731 -2.5403633 -4.1347671 -5.8873396 -6.5325522 -7.2429495 -7.4047885][-7.2527847 -6.8352242 -6.3026409 -5.5125141 -5.0505357 -4.5206866 -3.8091009 -3.7494688 -3.665946 -4.407979 -5.1960621 -6.7244325 -6.8379974 -7.5804605 -7.7570744][-7.9538326 -7.4917312 -6.7235861 -6.0788279 -5.732543 -5.129849 -5.2489543 -5.2449446 -5.1416559 -5.9197407 -6.1231737 -6.997869 -7.1795039 -7.7381315 -7.4537954][-7.697834 -7.3594565 -7.2037358 -6.4610653 -5.7721806 -5.4067144 -5.1976695 -5.5770903 -5.8738976 -6.2294044 -6.3708143 -6.6271992 -6.6340871 -6.9143672 -6.5256581][-8.41227 -8.2058268 -7.828547 -7.41602 -6.8723912 -6.2841611 -6.1674485 -6.5024881 -6.7095447 -6.9444847 -7.0199366 -6.9552851 -6.5665245 -6.3318949 -6.1204228]]...]
INFO - root - 2017-12-15 17:49:37.386020: step 42110, loss = 0.43, batch loss = 0.32 (12.7 examples/sec; 0.629 sec/batch; 50h:45m:30s remains)
INFO - root - 2017-12-15 17:49:43.717693: step 42120, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:31m:59s remains)
INFO - root - 2017-12-15 17:49:50.031416: step 42130, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:31m:48s remains)
INFO - root - 2017-12-15 17:49:56.460098: step 42140, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 52h:00m:24s remains)
INFO - root - 2017-12-15 17:50:02.889037: step 42150, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:15m:09s remains)
INFO - root - 2017-12-15 17:50:09.291046: step 42160, loss = 0.31, batch loss = 0.19 (11.8 examples/sec; 0.676 sec/batch; 54h:32m:20s remains)
INFO - root - 2017-12-15 17:50:15.647021: step 42170, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 51h:13m:06s remains)
INFO - root - 2017-12-15 17:50:21.969232: step 42180, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 51h:43m:22s remains)
INFO - root - 2017-12-15 17:50:28.367150: step 42190, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 52h:24m:02s remains)
INFO - root - 2017-12-15 17:50:34.755016: step 42200, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 52h:02m:02s remains)
2017-12-15 17:50:35.273166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0545845 -3.2523994 -3.2739091 -3.4162817 -3.3821459 -3.1517577 -3.0570035 -2.6451855 -1.832448 -1.604794 -2.5355711 -4.7939191 -5.2199488 -6.2058959 -6.2311449][-3.3902431 -3.6969311 -4.0744534 -4.927659 -5.0662637 -4.488009 -3.8815649 -3.3225226 -2.8371744 -2.8142209 -3.513361 -5.7834225 -5.9928036 -7.1036134 -7.2477884][-3.0622168 -3.3151236 -3.7704947 -3.8583319 -4.3171711 -4.1970568 -3.827831 -3.2185187 -3.0026054 -3.3909764 -4.0944118 -6.2655158 -6.343328 -7.230422 -7.4195294][-3.4271402 -3.5192146 -3.5857177 -3.7237105 -3.5207872 -2.71528 -1.9982853 -1.4295664 -1.0871558 -1.7154818 -3.1772871 -5.5602283 -5.7669125 -6.6640387 -7.2207646][-4.55431 -4.8302097 -4.9719791 -4.5075359 -3.6294637 -2.3391986 -1.2506638 -0.60318041 -0.55912066 -1.5884418 -3.20932 -5.8784008 -6.287077 -6.9673619 -7.1706524][-5.9652491 -5.93659 -5.7443452 -4.790503 -3.6711864 -1.7136064 -0.11444139 0.57044125 0.50860214 -0.86026669 -2.6447225 -5.4895592 -6.1120992 -6.6224585 -6.8960128][-6.7954674 -6.2159886 -5.610466 -4.3517084 -2.6766105 -0.31974602 1.5800962 2.3547649 2.3482685 0.83105755 -1.4366136 -4.9368382 -6.0324211 -6.5962358 -6.8608518][-6.8131509 -6.3666859 -5.3180962 -3.6254249 -2.1889548 0.0096483231 1.9990911 2.6535778 2.1539278 0.81351852 -0.98038292 -4.197751 -5.4403648 -6.5032034 -6.45499][-5.2553482 -4.708107 -4.0318022 -2.349453 -0.82479191 1.0138187 2.2386303 2.9602194 2.9228678 1.2765999 -0.61327028 -3.7806056 -4.6276178 -5.7397695 -6.1793966][-4.2431436 -3.5176582 -3.0841789 -1.8399744 -0.87946177 0.65488148 1.5172234 1.4440432 1.1778536 -0.46711016 -1.8215127 -4.7356339 -5.3092766 -5.898911 -5.6054444][-3.6791945 -3.287931 -2.5147481 -1.7035985 -1.0426278 -0.082209587 0.57649422 0.094560623 -0.5699954 -2.3864632 -4.0322342 -6.3838634 -6.6940989 -6.7448263 -6.2017379][-2.9647717 -2.6350713 -1.9425259 -1.1710277 -0.89848328 -0.5645175 -0.60400009 -1.3652272 -2.3461609 -3.5300164 -4.8001995 -6.7640195 -7.0786238 -7.3036404 -6.7462635][-4.1568003 -3.3346357 -2.6355762 -2.0083642 -1.8367081 -1.578896 -2.0036144 -2.7524433 -3.614109 -5.0244164 -5.83754 -7.0175138 -6.8314958 -7.4696188 -6.9047966][-4.274848 -3.5554032 -3.0151796 -2.2376776 -1.8828611 -1.554975 -1.7351847 -2.4074373 -3.3117785 -4.6481981 -5.166903 -5.8378897 -5.920908 -6.3224263 -6.24568][-5.8887219 -4.7267609 -3.9210744 -3.2898059 -3.194325 -3.0861583 -3.5495429 -3.8983772 -4.2515774 -5.0482903 -5.3259339 -5.7213683 -6.05147 -6.2856293 -6.4323149]]...]
INFO - root - 2017-12-15 17:50:41.746589: step 42210, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.653 sec/batch; 52h:41m:06s remains)
INFO - root - 2017-12-15 17:50:48.074285: step 42220, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 52h:13m:22s remains)
INFO - root - 2017-12-15 17:50:54.493763: step 42230, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 52h:45m:43s remains)
INFO - root - 2017-12-15 17:51:00.958391: step 42240, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 52h:34m:59s remains)
INFO - root - 2017-12-15 17:51:07.386633: step 42250, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 50h:51m:37s remains)
INFO - root - 2017-12-15 17:51:13.782086: step 42260, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 51h:19m:50s remains)
INFO - root - 2017-12-15 17:51:20.086642: step 42270, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 51h:03m:43s remains)
INFO - root - 2017-12-15 17:51:26.483000: step 42280, loss = 0.35, batch loss = 0.23 (12.9 examples/sec; 0.620 sec/batch; 50h:00m:29s remains)
INFO - root - 2017-12-15 17:51:32.863953: step 42290, loss = 0.40, batch loss = 0.29 (11.7 examples/sec; 0.685 sec/batch; 55h:11m:14s remains)
INFO - root - 2017-12-15 17:51:39.146340: step 42300, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 50h:34m:30s remains)
2017-12-15 17:51:39.664610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1212034 -4.4852266 -3.8126113 -3.1218987 -2.8036876 -2.1520677 -1.9087896 -1.9418182 -1.9393983 -3.6075435 -4.8131886 -5.6471682 -6.6755996 -7.5044394 -8.2966156][-4.5655441 -4.2851572 -3.7718925 -3.1454229 -2.724607 -2.2885795 -2.2748365 -2.2561607 -1.9443984 -3.5729785 -4.8553672 -5.5247774 -6.7617331 -7.800034 -8.3250694][-4.22169 -3.8719947 -3.3921428 -3.0733418 -2.7252245 -2.2548194 -1.9622326 -1.9590693 -1.9566398 -3.7954552 -5.0444012 -5.6321478 -6.8130474 -7.5652533 -7.8539147][-3.9674819 -3.597446 -3.2203455 -2.6693883 -2.2420425 -1.9689507 -1.6495562 -1.592905 -1.486412 -3.0371356 -4.1311922 -4.8409491 -6.0445127 -6.9754114 -7.3697371][-4.1264696 -2.968152 -2.0206838 -1.7409744 -1.6390538 -1.3457942 -0.87354851 -0.87644911 -0.7758503 -2.2624679 -3.2603807 -3.90305 -5.2982073 -6.3327894 -6.5049343][-3.3837867 -2.5348477 -2.0342031 -1.5081296 -0.65486 -0.32781363 -0.32058096 -0.42321205 -0.23314524 -1.7458348 -2.8295984 -3.6463366 -4.8548608 -5.7400169 -6.2834368][-2.6854219 -2.3587308 -1.500946 -1.0944538 -0.51502419 0.0028214455 0.28503895 0.24725008 0.23525524 -1.3246765 -2.3293953 -3.1709385 -4.5434527 -5.3987422 -5.7808723][-2.3971767 -1.6245842 -1.1088104 -0.51916075 0.44910145 0.95010853 0.84988403 1.0228472 1.2980022 -0.44871664 -1.7534184 -2.7596469 -4.234746 -5.35012 -5.4945984][-2.2767134 -1.5175934 -0.6510396 0.46688747 0.87350464 1.1449175 1.4463053 1.3491869 1.292717 -0.60211849 -1.9689903 -2.9530902 -4.4733534 -5.5911493 -5.90266][-2.8476114 -1.6361866 -0.5476799 0.25101042 0.73876572 1.2185888 1.1342106 0.89890385 0.83858681 -1.2975674 -2.7411966 -3.8837395 -5.2646589 -6.2159109 -6.4330797][-3.5838342 -2.9778023 -2.4948039 -1.403821 -0.52934122 -0.21152878 -0.255332 -0.39220285 -0.6122551 -2.6422286 -3.9867513 -4.8182955 -6.1936183 -6.9574757 -6.9605055][-4.24076 -3.604073 -2.8131833 -1.778131 -1.0611963 -0.61315489 -0.49742365 -0.88820505 -1.4163818 -3.0805812 -4.2487659 -4.8729239 -6.2235923 -7.022902 -6.9857173][-4.731411 -4.3197193 -4.1121984 -3.3941321 -2.9305878 -2.2748795 -1.9882479 -2.3494091 -2.7383008 -3.9346616 -5.0302248 -5.219162 -5.8469138 -6.2703524 -6.152802][-5.0963392 -4.7115645 -4.5855079 -4.41428 -4.5295172 -4.0217438 -3.7588842 -3.812319 -3.748841 -4.4474955 -4.8811431 -5.0788059 -5.4869757 -5.7465973 -5.3790207][-6.2609506 -6.169 -5.8983822 -5.6609731 -5.6792064 -5.3298855 -5.1250906 -5.130836 -5.251317 -5.2514324 -5.260438 -5.4412146 -5.4481983 -5.3900566 -5.2082787]]...]
INFO - root - 2017-12-15 17:51:46.106088: step 42310, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 51h:06m:14s remains)
INFO - root - 2017-12-15 17:51:52.536349: step 42320, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 51h:49m:02s remains)
INFO - root - 2017-12-15 17:51:58.914798: step 42330, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 52h:02m:55s remains)
INFO - root - 2017-12-15 17:52:05.386408: step 42340, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 53h:10m:40s remains)
INFO - root - 2017-12-15 17:52:11.869595: step 42350, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 50h:27m:34s remains)
INFO - root - 2017-12-15 17:52:18.295369: step 42360, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 54h:25m:31s remains)
INFO - root - 2017-12-15 17:52:24.778352: step 42370, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 50h:54m:25s remains)
INFO - root - 2017-12-15 17:52:31.120014: step 42380, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 49h:56m:33s remains)
INFO - root - 2017-12-15 17:52:37.476087: step 42390, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 51h:23m:06s remains)
INFO - root - 2017-12-15 17:52:43.871349: step 42400, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 51h:41m:16s remains)
2017-12-15 17:52:44.444779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8656714 -4.1048717 -4.7237835 -5.8036461 -6.5613241 -6.7562423 -6.9155774 -6.6611257 -6.53571 -6.1678948 -6.8453979 -8.0626087 -8.3851528 -8.9131842 -8.9007778][-3.5460525 -3.8591819 -4.7534876 -5.7746878 -6.2380323 -6.7206526 -7.05359 -7.1481028 -6.6590366 -5.8377647 -6.7414613 -7.9324837 -8.3951378 -8.9836416 -9.1492338][-2.9923491 -3.0937452 -3.7482271 -4.2363567 -4.7824426 -4.6282635 -5.0376835 -5.4883871 -5.3795786 -4.8234444 -5.5168476 -6.7066369 -7.6160874 -8.8210649 -9.0423794][-3.487299 -2.8345408 -2.8851814 -3.0127859 -2.9620686 -2.8651767 -2.980329 -3.255909 -3.0221291 -2.5402794 -3.6423712 -5.0142531 -6.1142921 -7.3336086 -8.3471346][-4.1364241 -3.1219368 -2.2665634 -1.6427469 -1.2753992 -1.0571365 -0.84727144 -1.0441384 -0.517571 -0.42639923 -1.74434 -3.6519532 -5.4520521 -6.96084 -7.7826619][-3.1739383 -2.8972235 -2.3098569 -1.4171405 -0.510787 0.56479073 0.99107265 0.93509293 1.132556 1.2098074 -0.40547562 -2.8359051 -4.9832792 -6.4525156 -7.2699804][-3.2721677 -3.0535264 -1.7117667 -0.75243807 0.16771317 1.6737614 2.7661676 3.0941906 3.5120182 3.0096064 0.81090736 -2.196454 -4.3612738 -5.9931235 -6.7204347][-2.8740778 -2.4534783 -1.5987782 -0.67875957 0.26031876 1.5812063 2.5414782 3.3175497 4.0663509 3.5328817 1.0574303 -2.10636 -4.3374 -5.7989492 -6.5409484][-3.5732412 -3.2557693 -2.3596749 -1.1524506 -0.11048889 0.9424572 1.5697136 2.2933903 3.0093708 2.7399025 0.24859238 -3.2555008 -5.5690107 -7.060504 -7.4274416][-5.30978 -4.7397556 -4.5153637 -3.9332047 -2.9801936 -1.6161504 -0.64710951 0.05087328 0.23787785 -0.068655014 -2.4671531 -5.1642537 -6.8748035 -7.9083786 -8.2323227][-7.5500774 -7.4479852 -7.0672174 -6.4514961 -5.4317412 -4.7502389 -3.5998368 -2.9499426 -2.8081055 -2.810113 -4.685401 -6.8058305 -7.6226325 -8.5151386 -8.3249674][-8.6018524 -8.4056664 -7.95895 -7.31742 -6.867331 -6.6097956 -5.7741075 -5.3060989 -4.5870781 -4.465992 -6.4908371 -7.4278021 -8.0572176 -8.51679 -8.2335968][-8.8313742 -9.2295418 -8.9526014 -8.3330412 -7.8450837 -7.3292093 -6.7342529 -6.7949982 -6.7636023 -6.3020868 -7.354826 -7.8790884 -8.1394968 -8.0754128 -7.3269863][-8.1019249 -8.2714891 -8.2090712 -7.5536866 -7.1893568 -6.9531817 -6.3948793 -6.5318651 -6.7024255 -6.4877586 -7.4137115 -7.1308932 -6.8942804 -6.7880211 -6.7777338][-8.8479805 -9.3252192 -9.3213 -9.1708813 -8.9301434 -7.8885489 -7.3975077 -7.6298666 -7.910449 -7.9450555 -8.2741451 -8.4147453 -7.7262321 -7.4129677 -6.6995578]]...]
INFO - root - 2017-12-15 17:52:50.826161: step 42410, loss = 0.23, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 50h:33m:03s remains)
INFO - root - 2017-12-15 17:52:57.199677: step 42420, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 52h:35m:25s remains)
INFO - root - 2017-12-15 17:53:03.574292: step 42430, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 49h:53m:39s remains)
INFO - root - 2017-12-15 17:53:09.985042: step 42440, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 52h:14m:23s remains)
INFO - root - 2017-12-15 17:53:16.358797: step 42450, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 50h:54m:57s remains)
INFO - root - 2017-12-15 17:53:22.669254: step 42460, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 51h:40m:16s remains)
INFO - root - 2017-12-15 17:53:29.034782: step 42470, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 51h:44m:10s remains)
INFO - root - 2017-12-15 17:53:35.509604: step 42480, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 53h:06m:42s remains)
INFO - root - 2017-12-15 17:53:41.840538: step 42490, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 49h:26m:16s remains)
INFO - root - 2017-12-15 17:53:48.219372: step 42500, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 51h:04m:19s remains)
2017-12-15 17:53:48.771760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3057718 -6.7910857 -6.9472427 -6.9086189 -6.8595848 -6.8026347 -6.2285032 -5.4700384 -4.3534937 -5.2199221 -5.0816431 -5.5845466 -5.0893168 -5.2696886 -6.1043158][-5.9444427 -7.0009975 -7.2436061 -7.2019329 -7.302774 -7.4370685 -6.9629574 -6.047081 -5.1228256 -5.7019353 -5.3037176 -5.7714648 -5.4439864 -5.2057743 -5.5271597][-5.2801113 -5.6982112 -6.1648231 -6.5002379 -6.8476043 -6.4976788 -6.075388 -5.446105 -5.0689592 -5.8436623 -5.5136194 -5.9727864 -5.7948565 -5.7949529 -5.9455981][-4.6973872 -4.6351013 -5.0649285 -4.8389263 -4.8579235 -4.64888 -4.4154949 -4.0085783 -3.7594838 -4.948595 -5.1140375 -5.9758277 -5.9375072 -5.9085441 -6.2274156][-4.0803862 -3.384141 -3.3440595 -3.1796799 -3.0260291 -2.3271933 -1.7542915 -2.0297303 -1.9469981 -3.3862133 -3.7264147 -5.2050395 -5.6819878 -5.8433552 -6.3752031][-3.0550456 -2.048367 -1.5889206 -0.69853687 -0.074142456 0.61808491 0.9405117 0.29661036 -0.090390682 -2.0377107 -2.634088 -4.2535639 -4.9056206 -5.3421397 -6.2062154][-2.6794782 -1.1190977 -0.16561413 0.88334846 1.7675571 2.6402006 3.1947374 2.4349966 1.7962885 -0.85137033 -2.0446386 -3.6631846 -4.3971815 -5.0561113 -5.8145056][-2.4349871 -1.2919998 0.0140481 1.5550327 2.658042 3.3560114 3.6561327 3.2912102 2.9508762 0.065882206 -1.6880417 -3.5814524 -4.30851 -4.8237257 -5.663][-3.6803026 -2.9908438 -1.9548593 0.050687313 1.2383394 1.5718746 1.9310522 1.9713154 2.1017942 -0.44710398 -2.2486014 -4.0644722 -4.9152622 -5.4518881 -5.8918681][-4.9885931 -4.2749648 -3.4290438 -2.4270978 -1.2756767 -0.56332064 -0.062744141 0.036321163 0.016718864 -1.9765148 -3.0783806 -4.5934944 -5.7543621 -6.3523183 -6.89644][-8.5694609 -7.4859624 -6.4718156 -5.721756 -4.9823904 -3.7793078 -2.9548841 -2.6364446 -2.6787977 -4.3697729 -5.3683476 -6.1116209 -6.95971 -7.6307373 -8.1010447][-9.793766 -9.0533218 -8.3694611 -7.5189362 -6.984365 -6.4595404 -5.764307 -5.1238708 -4.5960369 -5.6485853 -6.2513137 -6.5957637 -6.976326 -7.3398767 -7.6127653][-9.7490616 -9.6547089 -9.562109 -8.6172562 -7.8269105 -7.40965 -7.045805 -6.76219 -6.3677559 -6.5469623 -6.8905878 -6.7981644 -6.7965975 -6.901742 -6.572979][-9.3370218 -8.7515526 -8.6986094 -8.0077076 -7.429019 -7.0040665 -6.7314034 -6.6127529 -6.3364019 -6.3104849 -6.2540913 -6.1922836 -6.34794 -6.2358904 -6.02802][-9.7877884 -8.927947 -8.3603458 -7.8164296 -7.4746957 -7.1107016 -6.5914879 -6.6503463 -6.4404893 -6.0960879 -6.0500555 -5.7871733 -5.3196144 -5.222291 -5.3672247]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 17:53:56.225314: step 42510, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 52h:01m:25s remains)
INFO - root - 2017-12-15 17:54:02.504611: step 42520, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:24m:21s remains)
INFO - root - 2017-12-15 17:54:08.913280: step 42530, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 52h:50m:28s remains)
INFO - root - 2017-12-15 17:54:15.271739: step 42540, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.644 sec/batch; 51h:51m:40s remains)
INFO - root - 2017-12-15 17:54:21.628084: step 42550, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 50h:10m:33s remains)
INFO - root - 2017-12-15 17:54:27.969581: step 42560, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 50h:36m:34s remains)
INFO - root - 2017-12-15 17:54:34.352514: step 42570, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 50h:32m:49s remains)
INFO - root - 2017-12-15 17:54:40.730382: step 42580, loss = 0.28, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 54h:14m:17s remains)
INFO - root - 2017-12-15 17:54:47.126871: step 42590, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 50h:12m:01s remains)
INFO - root - 2017-12-15 17:54:53.451542: step 42600, loss = 0.38, batch loss = 0.27 (12.6 examples/sec; 0.637 sec/batch; 51h:16m:31s remains)
2017-12-15 17:54:54.015721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120222 -3.951468 -4.0985951 -4.9807568 -5.8006215 -5.5334015 -5.0038853 -5.0406718 -5.58536 -6.1078086 -7.6871033 -7.0203266 -6.8709264 -8.0509815 -8.2428761][-4.4539042 -5.3092046 -6.0424662 -5.3223448 -5.1325722 -6.1298385 -6.7001891 -5.8210192 -5.1480904 -5.4896679 -7.3279014 -6.8830929 -7.1117115 -7.8131561 -7.6077657][-6.3681936 -5.1899362 -4.5394206 -5.4918289 -6.067935 -5.458724 -5.0579576 -5.1068916 -4.8774033 -5.113759 -6.9649835 -6.9275355 -6.7929716 -6.8670387 -7.0915656][-5.5389204 -5.6163111 -5.0171041 -3.5944643 -2.9880123 -3.5298767 -3.7177906 -3.3494778 -2.6053543 -2.2731824 -4.6023884 -5.3516121 -5.7250042 -6.4891949 -6.8849874][-6.7454119 -4.8733311 -3.0520048 -3.0508728 -2.7065654 -1.5388484 -0.50552225 -0.6732049 -0.96231937 -0.79072475 -2.7307658 -3.5749116 -4.4961662 -5.8104458 -6.6389151][-5.1169338 -4.8798113 -3.283761 -1.7250075 -0.90154076 -0.37471056 0.46427631 1.255043 1.5259743 1.0324192 -1.6619167 -2.6064072 -3.1974344 -4.5117383 -5.4444137][-4.8236485 -4.138464 -2.2122107 -1.4570117 -1.158741 0.38476753 1.730957 1.8809185 2.0237455 1.7746029 -0.82645607 -2.197104 -3.0439434 -4.2471466 -5.2692595][-3.7383025 -3.3818879 -2.6161013 -1.5631561 -0.57133007 0.34108925 0.99615765 1.7330399 2.4127045 1.7621489 -1.0677752 -2.4216042 -3.2705531 -4.0495472 -4.8101292][-3.4336557 -3.204968 -2.8109336 -2.2462344 -1.6290102 -0.27970362 1.0539703 1.131712 1.1234884 1.345809 -1.0862713 -2.4256563 -3.6719742 -5.0787086 -5.7431221][-4.8225455 -4.2951345 -3.18758 -2.4773984 -2.0855131 -0.99519444 0.1131525 0.98685265 1.1686392 0.63640976 -1.688159 -2.0499892 -2.7340446 -4.0279293 -5.3758411][-4.8864765 -4.7639713 -4.2933249 -3.6330714 -3.0408525 -2.1033869 -1.3909178 -1.1880379 -1.1924009 -1.1299958 -2.5371494 -2.800221 -3.5795178 -4.64711 -5.4837952][-5.9133563 -5.1912355 -4.688982 -4.4721718 -4.1203117 -3.1473446 -2.3566122 -2.0494041 -2.2085242 -2.7640762 -3.9863489 -3.9365728 -4.0706635 -4.7571259 -5.7893953][-6.8114219 -7.035573 -6.6642556 -5.7783384 -5.3559551 -4.6828132 -3.7862439 -3.2168632 -3.3138223 -3.41497 -4.4422355 -4.6121821 -4.9447489 -5.0924892 -5.0623426][-5.7419887 -6.2587523 -6.2183867 -6.3014388 -6.3525724 -5.4047041 -4.8287282 -4.1619339 -3.4666877 -3.1497703 -4.0053186 -4.0883389 -4.00963 -4.5739965 -5.2833557][-6.3174715 -5.0693827 -5.0159225 -5.8273139 -6.24421 -6.1883612 -5.8283777 -5.4716139 -5.0972414 -4.8187094 -4.2670965 -4.13694 -4.6268616 -4.81493 -4.8960309]]...]
INFO - root - 2017-12-15 17:55:00.522908: step 42610, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.682 sec/batch; 54h:56m:02s remains)
INFO - root - 2017-12-15 17:55:07.034262: step 42620, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 52h:32m:25s remains)
INFO - root - 2017-12-15 17:55:13.537301: step 42630, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 51h:56m:27s remains)
INFO - root - 2017-12-15 17:55:19.952557: step 42640, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 52h:39m:38s remains)
INFO - root - 2017-12-15 17:55:26.459469: step 42650, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 52h:23m:13s remains)
INFO - root - 2017-12-15 17:55:32.887217: step 42660, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.630 sec/batch; 50h:45m:25s remains)
INFO - root - 2017-12-15 17:55:39.294113: step 42670, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 51h:04m:50s remains)
INFO - root - 2017-12-15 17:55:45.724867: step 42680, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 52h:34m:55s remains)
INFO - root - 2017-12-15 17:55:52.189084: step 42690, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 51h:36m:04s remains)
INFO - root - 2017-12-15 17:55:58.637182: step 42700, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 52h:55m:00s remains)
2017-12-15 17:55:59.118496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6839275 -4.64023 -4.7222929 -4.7448497 -4.6498146 -4.184556 -3.6574216 -2.8218527 -1.8499503 -2.0690398 -3.1524496 -4.4778023 -5.2494059 -6.1082354 -6.5447845][-4.5399857 -4.59234 -4.8474407 -4.8330116 -4.8313847 -4.8687668 -4.5694771 -4.0361032 -3.2988281 -3.3782454 -3.1425972 -3.7032402 -4.9650264 -5.7201252 -6.4448304][-4.3375206 -4.2080913 -4.3329477 -4.3115468 -3.8685451 -3.7168341 -3.6782756 -3.659627 -3.3795171 -3.5864587 -3.7796009 -4.35262 -4.6175928 -5.2572851 -5.7892489][-4.9548216 -3.9681571 -3.5172148 -2.9656949 -2.3609519 -1.988452 -2.0721111 -1.9707885 -1.8406472 -2.5780964 -3.3373094 -4.0450783 -4.7604871 -5.7287555 -6.0683231][-5.7401814 -4.3362894 -3.2336831 -2.1500635 -1.1871161 -0.46894121 -0.14921188 -0.21071768 -0.13773012 -1.267035 -2.1993976 -3.2274141 -4.5835371 -5.6883926 -6.2779388][-4.5137997 -3.4251456 -2.644505 -1.4385734 -0.23050451 0.74681568 1.3842106 1.2273808 1.0471115 0.089997292 -0.99819183 -2.2933483 -3.8353891 -5.11429 -5.8223214][-3.8755157 -3.0136943 -1.8499627 -0.61861038 0.61827278 1.9820795 2.8300695 2.7299442 2.591733 1.2225285 -0.15675783 -1.7541871 -3.6694188 -5.0138068 -5.8438787][-2.9769526 -2.1652064 -1.4979258 -0.28271389 1.1095676 2.2597923 2.9319344 3.19806 3.3348713 2.086874 0.49015236 -1.5527425 -3.2119136 -4.533185 -5.3998127][-3.5573997 -2.4949932 -1.4372096 -0.10720301 1.136857 1.8060322 2.1198626 2.4436102 2.854969 2.0203161 0.64138126 -1.2375956 -3.0438242 -4.5719519 -5.2847729][-3.7778342 -3.2711053 -2.5760956 -1.6053848 -0.48521185 0.73660278 1.3635244 1.4221191 1.2898083 0.16118383 -0.78111029 -2.0295072 -3.3098888 -4.2776093 -4.8580904][-5.1498742 -4.8934135 -4.6969557 -3.9965942 -3.1480041 -2.2874236 -1.5428958 -1.2059364 -1.171278 -2.2346072 -3.2178802 -4.032548 -4.8633938 -5.5157776 -5.7060714][-7.2512956 -6.8673077 -6.26303 -5.727458 -4.9515686 -4.3233237 -3.8393502 -3.8730016 -3.7524633 -4.2299457 -4.9969854 -5.4323516 -6.1182609 -6.4631763 -6.237566][-8.2393608 -8.0845356 -8.0984173 -7.1870914 -6.4483566 -5.9528818 -5.4374251 -5.4272518 -5.36002 -5.5976534 -6.1910458 -6.0207295 -6.2415113 -6.6501021 -6.2016125][-8.1681032 -8.1774569 -7.5568905 -7.1312504 -6.8765764 -6.2109556 -5.7455511 -5.7717381 -5.9188557 -6.0002341 -6.2501855 -6.0790181 -5.7540641 -5.8343906 -5.6962757][-7.6960812 -7.4701114 -7.2020259 -7.0340033 -6.6108351 -6.219213 -6.1910977 -6.212635 -6.2491007 -6.3267479 -6.3176026 -6.3583264 -6.2591472 -6.03154 -5.934103]]...]
INFO - root - 2017-12-15 17:56:05.632275: step 42710, loss = 0.33, batch loss = 0.22 (12.2 examples/sec; 0.653 sec/batch; 52h:35m:42s remains)
INFO - root - 2017-12-15 17:56:12.071750: step 42720, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 52h:36m:54s remains)
INFO - root - 2017-12-15 17:56:18.501060: step 42730, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 52h:15m:00s remains)
INFO - root - 2017-12-15 17:56:24.898140: step 42740, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 50h:42m:31s remains)
INFO - root - 2017-12-15 17:56:31.317442: step 42750, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 52h:23m:25s remains)
INFO - root - 2017-12-15 17:56:37.744100: step 42760, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.627 sec/batch; 50h:28m:06s remains)
INFO - root - 2017-12-15 17:56:44.130167: step 42770, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 52h:15m:37s remains)
INFO - root - 2017-12-15 17:56:50.601602: step 42780, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.644 sec/batch; 51h:51m:58s remains)
INFO - root - 2017-12-15 17:56:57.059160: step 42790, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 51h:16m:14s remains)
INFO - root - 2017-12-15 17:57:03.449031: step 42800, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 51h:56m:23s remains)
2017-12-15 17:57:03.965844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8327107 -1.5250072 -1.3617764 -0.85781527 -0.64982271 -0.537097 -0.506371 -0.58130455 -0.83208132 -2.320828 -3.3995156 -5.8621416 -6.9522967 -7.8773603 -8.4715567][-1.838778 -1.4742069 -1.2857418 -1.2059093 -1.4580364 -1.4506707 -1.6408553 -1.7318087 -2.0128269 -3.5306811 -4.5281878 -6.3446131 -7.3519721 -8.7079639 -9.1386261][-2.1022696 -1.8537517 -1.3903766 -1.2817535 -1.7229619 -1.5147634 -1.6485958 -1.5639443 -1.5933728 -3.2044191 -4.0019474 -5.9734793 -6.5220804 -7.6034083 -8.6705856][-3.2715678 -2.6730762 -2.0767832 -1.4947529 -1.4933858 -1.0100322 -0.53978014 -0.51253223 -0.52280045 -1.9333029 -2.9361463 -5.2730317 -6.2748189 -7.4323096 -8.0873528][-3.3439775 -2.9242334 -2.2997098 -1.5440321 -1.2564869 -0.33946276 0.3912096 0.96085739 1.2252226 -0.16194725 -0.97064829 -3.256721 -4.58517 -6.364687 -7.4775009][-3.933357 -3.2746844 -2.491344 -1.4731522 -0.58560848 0.47918892 1.2441568 2.1284351 2.4259062 0.80714607 0.040160179 -2.1262197 -3.6160326 -5.2775612 -6.3708839][-3.7664523 -3.3535695 -2.9539571 -1.7334514 -0.7143445 0.76467991 2.079814 2.6884718 3.1265583 1.9171829 0.913002 -1.4311404 -3.0719218 -4.7659922 -6.0693769][-4.4966335 -3.5410657 -2.5429764 -1.4606543 -0.30602455 1.1796379 2.2252493 3.080554 3.6846638 2.0395937 0.93383694 -1.194159 -2.8020773 -4.5211077 -5.6620741][-3.3336439 -2.799171 -1.9901409 -0.65970707 0.37080002 1.48771 2.30861 2.5682373 2.8736334 1.4250603 0.4247942 -1.6350713 -2.9627957 -4.375555 -5.5274763][-3.4153614 -2.6107802 -1.9131141 -0.8236022 -0.040679455 0.7902174 1.4493742 1.4515848 1.4267807 -0.20256042 -1.0543246 -2.8004756 -3.9386754 -5.1540766 -6.0493164][-4.3949189 -3.8101981 -3.3084378 -1.997869 -0.929224 -0.23949289 0.18055916 0.15283251 0.04579401 -1.5991254 -2.5525966 -4.3353 -5.3277035 -6.1284614 -6.5887442][-4.269352 -3.7692828 -3.2249441 -2.5690808 -1.8633642 -0.73541832 -0.32771826 -1.0115566 -1.3009591 -2.8324842 -3.7982159 -5.4027514 -6.1839905 -6.6656318 -7.0807095][-3.9663882 -3.3781786 -3.0130639 -2.3679256 -2.0949216 -1.6299748 -1.1160798 -1.7835717 -2.3253345 -3.9063618 -4.8582935 -6.140501 -6.9005055 -7.162775 -7.2082949][-3.4812479 -3.0569787 -2.9779434 -2.6813936 -2.5365615 -2.4076395 -2.4031982 -2.9450049 -3.1563935 -4.2888408 -5.1113348 -5.8687115 -6.345459 -6.7208195 -7.0177345][-4.6114163 -4.28403 -4.4232054 -4.3437023 -4.3298969 -4.2289577 -4.1657472 -4.6020861 -4.6708326 -5.2828345 -5.7446938 -6.17413 -6.8081627 -7.0104589 -7.2630229]]...]
INFO - root - 2017-12-15 17:57:10.473700: step 42810, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.681 sec/batch; 54h:47m:33s remains)
INFO - root - 2017-12-15 17:57:16.964081: step 42820, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 50h:51m:17s remains)
INFO - root - 2017-12-15 17:57:23.448148: step 42830, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 53h:06m:43s remains)
INFO - root - 2017-12-15 17:57:29.898030: step 42840, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 51h:38m:54s remains)
INFO - root - 2017-12-15 17:57:36.334573: step 42850, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 51h:31m:25s remains)
INFO - root - 2017-12-15 17:57:42.754837: step 42860, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 49h:54m:06s remains)
INFO - root - 2017-12-15 17:57:49.107780: step 42870, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 52h:02m:03s remains)
INFO - root - 2017-12-15 17:57:55.507179: step 42880, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:20m:14s remains)
INFO - root - 2017-12-15 17:58:01.864896: step 42890, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 50h:27m:08s remains)
INFO - root - 2017-12-15 17:58:08.317362: step 42900, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 51h:07m:24s remains)
2017-12-15 17:58:08.806695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.765976 -4.1089473 -3.1034184 -2.7243934 -2.4573097 -1.7391329 -1.1598415 -1.0439534 -1.6365132 -3.8261981 -4.71233 -6.2041011 -7.5549 -8.0539331 -9.1742516][-4.4357886 -4.0779061 -3.7358565 -2.9704542 -2.3655672 -1.6299009 -1.5001845 -1.7402325 -1.8706818 -4.0631332 -5.3130908 -6.633235 -8.0366926 -8.313941 -9.3048906][-4.1890259 -3.6231861 -2.8064647 -1.804595 -1.1371732 -0.31918621 -0.27983665 -0.95832491 -1.3767419 -3.5830607 -4.7205305 -5.7287817 -6.8220549 -7.3940387 -8.469059][-2.9187813 -2.0430665 -1.31284 -0.24188995 0.54060459 0.93890953 0.86017513 0.022862434 -0.56761026 -2.5750203 -3.4802723 -4.3050613 -5.4916449 -6.0617447 -6.5303359][-3.256547 -1.3834319 0.38105488 1.1881723 1.5354071 1.7281179 1.4164171 0.62009621 -0.10751438 -2.1441884 -2.6689386 -2.8317122 -3.6329474 -4.4000564 -5.15256][-3.253243 -1.6659188 -0.10455656 1.1353054 1.8804941 1.9726086 1.5888786 1.0918379 0.65838337 -1.2128282 -1.3126388 -1.3716054 -2.2138247 -2.7136502 -3.7047093][-4.0417957 -2.3082676 -0.77806616 0.44110107 0.64022732 0.886878 1.076438 0.95499516 1.1003199 -0.1032753 -0.001686573 -0.20894337 -1.3519173 -1.6985641 -2.7473488][-4.8444204 -3.4349022 -1.694694 -0.37264538 0.28739119 0.55883121 0.76305008 1.202342 1.9961386 0.97265339 1.3609018 0.87576103 -0.61053228 -1.5570459 -2.9881334][-5.5065823 -3.8901212 -2.2452393 -0.74378014 0.048330307 0.71146965 1.0718536 1.3175697 1.9483852 1.2446127 1.8281956 1.3403463 -0.15440035 -1.4848661 -2.8724766][-7.1530738 -5.6316957 -3.8186104 -2.0872731 -1.016326 -0.012908936 0.68880749 0.95119286 1.4117422 0.32417202 0.57382107 -0.21691608 -1.4595532 -2.2587733 -3.1747069][-7.3756118 -6.2557688 -4.9413595 -3.5265841 -2.4751315 -1.7308412 -0.81326056 -0.17636824 0.500885 -0.65842962 -0.83097839 -1.3154755 -2.6558309 -3.6333218 -4.4436817][-7.6744576 -5.8138056 -4.8773937 -3.5411434 -3.0415206 -2.7060318 -2.2250443 -2.0204024 -1.4814835 -2.3493 -2.4454598 -2.7141423 -3.5970783 -4.1035638 -4.9559784][-7.7654614 -6.4106927 -5.0653372 -3.8863041 -3.181253 -2.6242375 -2.761744 -2.7240958 -2.5143919 -3.5985551 -3.7319982 -3.9901798 -4.3479729 -4.2196941 -4.8431692][-7.0854173 -6.0570116 -5.191329 -4.2426872 -3.5936465 -3.0443935 -3.4803662 -3.5582571 -3.6174684 -4.0012321 -4.2472305 -4.1385922 -4.3469276 -4.198894 -4.452126][-7.68248 -6.4646764 -5.6082354 -5.2185173 -4.4213433 -3.8955498 -4.1661682 -4.7527561 -5.2312355 -5.0632954 -5.0289917 -5.0248318 -4.7966108 -4.902504 -5.0665913]]...]
INFO - root - 2017-12-15 17:58:15.205115: step 42910, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.643 sec/batch; 51h:41m:21s remains)
INFO - root - 2017-12-15 17:58:21.629827: step 42920, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 50h:31m:21s remains)
INFO - root - 2017-12-15 17:58:28.122607: step 42930, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 50h:03m:16s remains)
INFO - root - 2017-12-15 17:58:34.510037: step 42940, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 50h:49m:59s remains)
INFO - root - 2017-12-15 17:58:40.932395: step 42950, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 50h:08m:15s remains)
INFO - root - 2017-12-15 17:58:47.320763: step 42960, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 53h:11m:21s remains)
INFO - root - 2017-12-15 17:58:53.755808: step 42970, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 51h:43m:24s remains)
INFO - root - 2017-12-15 17:59:00.097548: step 42980, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 51h:23m:37s remains)
INFO - root - 2017-12-15 17:59:06.603336: step 42990, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 53h:20m:42s remains)
INFO - root - 2017-12-15 17:59:13.098752: step 43000, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 50h:33m:04s remains)
2017-12-15 17:59:13.596250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1691871 -4.5517516 -4.0745506 -3.8751209 -3.8688908 -3.648129 -3.2155972 -3.2880936 -3.2290888 -4.4244032 -5.3587575 -5.4622765 -5.531477 -6.2597666 -6.5778847][-4.5532951 -4.0236063 -3.3600788 -3.1885796 -3.2522244 -3.5293965 -3.4764347 -3.2987618 -2.9329453 -3.9087789 -4.4995937 -5.0461683 -5.4296031 -5.4861846 -5.5689373][-5.116188 -4.813395 -4.2704239 -3.97548 -3.7690988 -3.4148102 -3.1898184 -3.4633694 -3.6223736 -4.2717085 -4.5148411 -5.033288 -5.5699749 -5.9538393 -5.9880066][-6.3853464 -5.665103 -4.9529152 -4.2414541 -3.7286084 -3.1033192 -2.4440894 -2.5318966 -2.7731023 -3.9575019 -4.9879651 -5.69278 -6.0770044 -6.3873081 -6.6007605][-7.0990219 -6.0378995 -5.2888031 -4.4633932 -3.3161964 -1.995297 -1.0960741 -1.3216667 -1.716711 -2.8281779 -3.9609003 -5.4186954 -6.4309683 -6.8716488 -6.8486819][-6.2884059 -5.5014334 -4.8913651 -3.8794818 -2.4592462 -1.1104236 -0.046917439 -0.089996815 -0.37625885 -1.3510323 -2.6013045 -3.9717724 -5.209012 -6.4596395 -7.1377044][-5.0331621 -3.967418 -2.9421978 -2.3083286 -1.4688058 0.077850819 1.3379211 1.4255695 1.257884 0.41580486 -0.861609 -2.2543564 -3.4552236 -4.8619561 -5.9120817][-3.6005473 -2.7489643 -1.9866772 -0.86465788 0.29845381 1.3486137 2.2711353 2.3960562 2.2842083 1.4622593 -0.0029392242 -1.382309 -2.5522637 -3.9332426 -4.9488287][-2.9975305 -1.7186036 -1.065032 -0.48041296 0.40479469 1.3292437 2.1502571 2.3054838 2.1081285 1.3764524 0.07047081 -1.4132547 -2.5260358 -3.6781788 -4.507874][-3.351891 -1.9217529 -0.67365217 -0.071174622 0.50349712 1.2134581 1.9601068 2.0857821 2.0273485 0.99389267 0.0279603 -0.76376534 -1.9976878 -3.1237998 -3.7987616][-3.2811594 -2.1518936 -1.5201054 -0.8117156 -0.15522814 0.33452129 0.99099445 1.2945862 1.6198177 0.58694649 -0.10761309 -0.59391403 -1.480392 -2.4338965 -2.9262838][-3.3004069 -2.1498985 -1.7282453 -1.6392832 -1.3907466 -0.86509418 -0.16480207 -0.027371407 0.28919935 -0.273242 -0.25604773 -0.731266 -2.112577 -3.0801578 -3.656034][-3.6983325 -2.7139754 -2.1527953 -2.3554802 -2.498085 -2.1066232 -1.6784754 -1.6076345 -1.4244094 -1.6353126 -1.4977803 -1.5260029 -2.4827652 -3.5511026 -3.9861605][-3.7198291 -3.1168671 -2.8363476 -3.1533165 -3.5178752 -3.3837271 -3.0542541 -3.1858134 -3.2258134 -3.1819425 -3.3020196 -3.1533885 -3.5355372 -4.0366631 -4.3928003][-3.3758173 -2.9213991 -3.1344395 -3.9329741 -4.5649362 -4.7978306 -4.8862286 -5.070034 -5.1160183 -5.02304 -4.9334269 -5.0703454 -5.0238409 -4.8867207 -5.2271595]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 17:59:20.067464: step 43010, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 50h:18m:24s remains)
INFO - root - 2017-12-15 17:59:26.450663: step 43020, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 52h:28m:40s remains)
INFO - root - 2017-12-15 17:59:32.803169: step 43030, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 51h:34m:57s remains)
INFO - root - 2017-12-15 17:59:39.254791: step 43040, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 52h:14m:13s remains)
INFO - root - 2017-12-15 17:59:45.690009: step 43050, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 51h:12m:46s remains)
INFO - root - 2017-12-15 17:59:52.116087: step 43060, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 50h:58m:12s remains)
INFO - root - 2017-12-15 17:59:58.483890: step 43070, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 53h:16m:35s remains)
INFO - root - 2017-12-15 18:00:04.905965: step 43080, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 52h:59m:40s remains)
INFO - root - 2017-12-15 18:00:11.372837: step 43090, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 52h:18m:31s remains)
INFO - root - 2017-12-15 18:00:17.795388: step 43100, loss = 0.35, batch loss = 0.23 (12.9 examples/sec; 0.618 sec/batch; 49h:42m:30s remains)
2017-12-15 18:00:18.297024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8631291 -6.8986807 -6.870501 -5.86537 -4.3404937 -3.2159891 -2.2127185 -1.6195302 -1.1221247 -2.0558548 -2.595048 -5.7772217 -7.0005507 -7.1465015 -8.3248539][-6.7546988 -6.55314 -6.5227947 -6.2459774 -5.210443 -3.5505986 -1.7778807 -1.2605877 -1.6190801 -2.6313663 -3.0545554 -5.6332169 -7.31738 -7.9731417 -8.96337][-6.50357 -6.2936306 -6.0010891 -5.2359438 -4.542017 -3.3069472 -1.5818152 -0.82264423 -0.81160688 -2.7466183 -3.8195884 -6.2583332 -6.827786 -7.3397403 -8.9831266][-5.6354613 -6.2340374 -6.0712423 -4.7230434 -3.2401209 -2.3901815 -1.0063405 -0.099420547 0.22538805 -1.515646 -2.5895634 -5.6820488 -6.7644062 -7.2547841 -7.9691863][-6.0030413 -6.2558346 -6.3946657 -5.1480122 -2.9415216 -0.93054867 0.47987366 0.040504456 -0.21205711 -2.0203066 -3.1468334 -5.7385678 -7.0963554 -7.3145232 -8.4477615][-7.181159 -6.7702017 -6.1156449 -4.6649446 -2.8016796 -1.137589 0.73109436 0.97710037 0.36702728 -2.191411 -3.7635036 -6.6269403 -7.5832119 -7.6179142 -8.8884706][-7.7154531 -6.7872329 -5.9057984 -4.6619692 -2.5208125 -0.84633112 0.89231873 1.2889071 1.0475445 -1.5916882 -3.3650427 -7.0223322 -8.0548563 -8.25589 -9.0615158][-6.6487021 -6.5718784 -6.0235877 -4.0090036 -2.2957129 -0.33514261 1.3464117 1.8312912 1.6988487 -0.86705637 -2.7399635 -6.3516693 -8.16099 -8.4737949 -8.9634628][-6.975666 -5.8817768 -5.4006915 -4.3856053 -2.1871271 -0.20459557 1.1694326 1.2552748 0.84867382 -1.2824097 -2.8777752 -6.6396732 -8.2363338 -8.7204781 -9.6562977][-6.4314117 -5.6408606 -5.5074496 -4.5256557 -2.9275641 -1.2055397 0.7758131 0.81017113 0.04485178 -2.8926935 -4.058 -7.3652148 -8.599041 -8.7478781 -9.3783865][-6.6251941 -6.5294418 -5.5748425 -4.4076676 -3.2837753 -1.5455446 -0.61944294 -0.58730173 -1.2236118 -4.571558 -5.8755083 -8.249671 -8.9404926 -8.8097506 -9.0008135][-5.3473248 -4.7389402 -4.0028839 -2.5152802 -0.6241107 0.012069702 0.76671028 0.54541016 -0.34106922 -3.01296 -4.7300172 -7.3669519 -8.1131144 -8.1043053 -8.2768459][-6.8979273 -5.5087934 -3.6321511 -1.3764167 0.012588978 0.5493803 0.78772736 -0.089102268 -0.91467094 -3.8226871 -4.9975448 -6.8153696 -7.586885 -7.4454622 -7.858089][-7.346252 -6.5715942 -4.9024334 -2.2713122 -0.66513777 -0.10505199 -0.64110804 -1.7956538 -2.975193 -4.9796791 -5.6718044 -6.6975141 -6.6578116 -6.2659326 -6.240808][-8.5500307 -7.7946758 -6.6963859 -4.737586 -2.8993554 -2.1019702 -2.1538577 -2.9008508 -4.2431498 -5.5971107 -6.6203914 -7.4253755 -7.2577891 -6.7516403 -6.1664224]]...]
INFO - root - 2017-12-15 18:00:24.762757: step 43110, loss = 0.25, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 54h:01m:42s remains)
INFO - root - 2017-12-15 18:00:31.158257: step 43120, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:51m:15s remains)
INFO - root - 2017-12-15 18:00:37.475165: step 43130, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 50h:54m:34s remains)
INFO - root - 2017-12-15 18:00:43.834222: step 43140, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:38m:19s remains)
INFO - root - 2017-12-15 18:00:50.304923: step 43150, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 51h:29m:15s remains)
INFO - root - 2017-12-15 18:00:56.816567: step 43160, loss = 0.32, batch loss = 0.20 (12.1 examples/sec; 0.663 sec/batch; 53h:17m:05s remains)
INFO - root - 2017-12-15 18:01:03.201803: step 43170, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 50h:22m:56s remains)
INFO - root - 2017-12-15 18:01:09.566426: step 43180, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 52h:47m:25s remains)
INFO - root - 2017-12-15 18:01:15.995854: step 43190, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 52h:50m:56s remains)
INFO - root - 2017-12-15 18:01:22.413958: step 43200, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 51h:55m:29s remains)
2017-12-15 18:01:22.966678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8754015 -2.5178165 -3.0404959 -3.3566527 -3.382122 -3.45433 -3.2105041 -2.6103926 -1.817049 -2.7368379 -3.552506 -5.0982018 -5.6621246 -6.0355511 -6.5510449][-2.2150149 -2.5979347 -3.196671 -4.0048943 -4.4265213 -4.1914997 -3.5320544 -3.0813508 -2.7032938 -3.6397219 -4.248292 -5.734951 -6.5176377 -7.0945473 -7.4834552][-3.1810842 -3.2820106 -3.641305 -4.1261892 -4.4978638 -4.528688 -3.9416773 -3.4014978 -3.0087624 -4.2998257 -5.3680077 -6.7313371 -7.2103648 -7.7209311 -8.31302][-3.8763213 -3.836504 -4.0051737 -3.9113667 -3.7812638 -3.2849884 -2.4324818 -1.9174256 -1.6413383 -2.9419184 -4.1299076 -5.8665576 -6.7919297 -7.5080595 -7.7703166][-4.3061666 -4.3666468 -4.282732 -3.8862691 -3.394999 -2.2877879 -0.84364033 -0.036409855 0.19586802 -1.3866062 -2.926311 -4.9899287 -6.2113256 -7.1340904 -7.8487577][-5.5619512 -5.5984793 -5.48145 -4.2391014 -2.799542 -1.238122 0.63578033 1.6524363 2.1930647 0.1169982 -2.1333966 -4.5773158 -6.1405892 -7.3348951 -7.97548][-5.7373114 -5.644062 -5.4185147 -4.0000248 -1.8250418 0.53618431 2.5144873 3.6872597 4.59811 2.9152431 0.2977891 -3.2638378 -5.3452148 -6.6711764 -7.49234][-5.0979557 -5.0693216 -4.8615332 -3.4440489 -1.5645313 1.2255535 3.7045088 5.1540346 6.0344172 4.1423931 1.7937317 -1.5811095 -4.2984047 -5.9542389 -6.7294426][-5.5470552 -4.8506317 -4.5289574 -3.6369605 -2.1450758 0.20032597 2.4483681 3.8856535 5.0920229 3.3332863 0.98839092 -2.0240002 -4.00049 -5.6435204 -6.7953043][-5.5521564 -5.0770121 -4.5118732 -3.52602 -2.6896191 -1.2152596 0.35659027 1.54457 2.3069124 0.53057384 -1.1662364 -3.8252733 -5.1976061 -6.094049 -6.9608197][-6.3658171 -5.8420959 -5.3289471 -4.6377411 -4.0026312 -3.180582 -2.5315566 -1.9219465 -1.1732879 -2.5187545 -4.0069904 -5.7571864 -6.4869127 -7.0036125 -7.0787473][-6.6539373 -6.4446058 -6.1647406 -5.413631 -4.9603419 -4.5920467 -3.9717114 -3.87351 -3.7562246 -4.2820807 -4.87436 -6.4195523 -6.7824507 -7.2100935 -7.4716611][-7.2968273 -7.0262585 -6.5152354 -5.8607531 -5.4157286 -4.9264493 -5.03086 -5.0673494 -4.8534164 -5.5716047 -5.7948952 -6.6437197 -7.0730276 -7.4138794 -7.3900175][-7.4411316 -7.3457451 -6.9282894 -6.1428261 -5.7508249 -5.4098907 -5.2353425 -5.4802213 -5.77116 -6.0174413 -6.0197706 -6.3017178 -6.4316645 -6.7281995 -6.5427742][-7.95923 -7.9472871 -7.7195091 -7.1700864 -6.603579 -6.154336 -6.2148576 -6.3128343 -6.4255857 -6.5971971 -6.5315208 -6.3090186 -5.9605055 -5.8754325 -5.913012]]...]
INFO - root - 2017-12-15 18:01:29.369465: step 43210, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 51h:00m:26s remains)
INFO - root - 2017-12-15 18:01:35.778269: step 43220, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.666 sec/batch; 53h:32m:01s remains)
INFO - root - 2017-12-15 18:01:42.164616: step 43230, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.614 sec/batch; 49h:19m:13s remains)
INFO - root - 2017-12-15 18:01:48.733886: step 43240, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 52h:12m:16s remains)
INFO - root - 2017-12-15 18:01:55.220331: step 43250, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:12m:29s remains)
INFO - root - 2017-12-15 18:02:01.693415: step 43260, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 53h:48m:45s remains)
INFO - root - 2017-12-15 18:02:08.040682: step 43270, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 50h:05m:48s remains)
INFO - root - 2017-12-15 18:02:14.472904: step 43280, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 51h:47m:06s remains)
INFO - root - 2017-12-15 18:02:20.864942: step 43290, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:35m:23s remains)
INFO - root - 2017-12-15 18:02:27.340185: step 43300, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 51h:30m:35s remains)
2017-12-15 18:02:27.921987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6884127 -4.0321112 -4.2678928 -4.4303532 -3.71605 -3.5204949 -2.9924111 -1.899682 -1.5539632 -1.8438377 -2.7767773 -5.3553677 -6.121798 -7.1929655 -7.6751032][-4.9499779 -5.2885675 -5.5692492 -6.0065627 -5.2528553 -4.4025922 -3.7126117 -2.9549375 -2.4466844 -2.729991 -3.9123125 -6.5519419 -7.4544158 -8.60983 -9.2424469][-4.2111716 -4.2228322 -4.090457 -4.411293 -3.7580342 -3.378634 -2.6793752 -2.3794365 -2.6253643 -3.0163536 -3.7724502 -6.3145843 -7.0961342 -7.9774733 -8.3840675][-4.137393 -4.6914206 -4.6972275 -4.3525958 -3.5338607 -2.596612 -1.4160304 -0.86945486 -1.053865 -1.9167519 -3.3384771 -6.0483718 -6.8820753 -7.5810804 -8.0574083][-5.6316624 -5.8098531 -5.7324209 -5.0059805 -3.599411 -2.1021409 -0.58752823 -0.0015907288 -0.25891685 -1.4365077 -3.0260572 -5.7788472 -6.4765325 -7.0630283 -7.3497696][-6.5133314 -6.404706 -6.0741844 -4.9960575 -3.399332 -1.5359592 0.12258863 0.82102585 0.81593895 -0.71435022 -2.6749973 -5.5740995 -6.3420315 -6.725359 -7.1723366][-6.7501822 -6.3829374 -5.9264154 -4.5985155 -2.8059263 -0.49539614 1.4385386 2.2254972 2.3699741 0.94070721 -1.1658463 -4.6069551 -5.89909 -6.4121866 -6.7698016][-6.4801488 -5.9613461 -5.13054 -3.7272198 -2.1734385 0.18875647 2.2449961 2.7702942 2.504384 1.1938238 -0.91269445 -4.6003618 -6.4941864 -6.9892478 -7.1523409][-4.9047585 -4.138268 -3.645318 -2.3639503 -1.150517 0.54278564 2.4090366 3.1099825 3.379571 2.1824665 0.24983072 -3.4707685 -5.2749715 -6.4544144 -6.8694043][-3.6020327 -2.8098125 -2.53449 -1.4118319 -0.48506832 0.75471783 1.8886452 2.0917749 2.0898933 0.22036839 -1.4962382 -4.5656095 -5.8836632 -6.5097256 -6.6000071][-3.0967102 -3.0125265 -2.8088522 -2.0533123 -1.489819 -0.43336344 0.40592289 0.2656703 -0.034245968 -1.4742889 -3.0459414 -5.8258719 -6.9062147 -6.9707165 -6.6914544][-2.5436349 -2.4466238 -2.2232919 -1.6181097 -1.509335 -1.0816779 -0.8839345 -1.2822652 -1.9748263 -3.0577807 -4.0225306 -6.4791 -7.2505007 -7.2965126 -6.9097195][-3.7351382 -3.6719117 -3.3484187 -3.1597128 -3.2012138 -2.8623633 -2.7298341 -2.9689875 -3.2964463 -3.9754128 -4.8876433 -5.82733 -6.3199053 -6.6072426 -6.2312055][-3.8616691 -4.0652862 -3.6168017 -2.9476848 -2.4842792 -2.2172565 -2.1382518 -2.5091705 -3.110157 -4.0182981 -4.5781832 -5.1389275 -5.6382189 -5.8310404 -5.7479959][-4.6979475 -4.7553816 -4.3948526 -3.9890394 -3.957922 -3.793314 -4.1659546 -3.9757872 -4.3136091 -4.9053106 -4.7805715 -4.8647218 -5.2333097 -5.583066 -5.840219]]...]
INFO - root - 2017-12-15 18:02:34.413568: step 43310, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 53h:44m:28s remains)
INFO - root - 2017-12-15 18:02:40.902830: step 43320, loss = 0.24, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 52h:54m:19s remains)
INFO - root - 2017-12-15 18:02:47.279306: step 43330, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 52h:52m:50s remains)
INFO - root - 2017-12-15 18:02:53.749106: step 43340, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 53h:40m:17s remains)
INFO - root - 2017-12-15 18:03:00.145408: step 43350, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 51h:18m:18s remains)
INFO - root - 2017-12-15 18:03:06.459357: step 43360, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 50h:12m:17s remains)
INFO - root - 2017-12-15 18:03:12.859635: step 43370, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 50h:15m:44s remains)
INFO - root - 2017-12-15 18:03:19.243857: step 43380, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:52m:27s remains)
INFO - root - 2017-12-15 18:03:25.708266: step 43390, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.632 sec/batch; 50h:47m:22s remains)
INFO - root - 2017-12-15 18:03:32.021826: step 43400, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 50h:42m:05s remains)
2017-12-15 18:03:32.518034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1508412 -3.8078544 -3.3125138 -2.7996554 -2.2538943 -1.9076104 -1.6966801 -1.7262959 -1.7858438 -3.5061402 -4.5860176 -5.435771 -6.1537933 -6.8014703 -7.746388][-4.0491638 -3.6783886 -3.2678885 -2.9383035 -2.5024276 -2.3534026 -2.2125416 -2.0768948 -2.2608943 -3.9002125 -4.9040093 -5.6062431 -6.4499865 -7.3687887 -8.1285791][-4.1741438 -3.9284952 -3.7394619 -3.4417377 -3.0695744 -2.4624662 -1.8141932 -1.6706495 -1.7105446 -3.0293846 -3.8922021 -4.3930416 -5.232348 -6.2030873 -7.2466645][-4.19401 -4.1663418 -3.910114 -3.2707248 -2.6722331 -1.9812379 -1.1569724 -0.89104652 -0.79035378 -2.2150836 -3.2399654 -4.0491371 -5.1765075 -5.9445863 -7.052948][-4.493166 -3.8584316 -3.0520244 -2.6725802 -2.1873975 -1.16539 -0.12380362 0.015109539 -0.00483799 -1.5472808 -2.5686221 -3.4812794 -4.7310848 -5.6858182 -6.688169][-4.0746651 -3.4856272 -2.9017138 -2.074832 -1.2175164 -0.48881245 0.41459656 0.67307663 0.707366 -0.82941628 -1.9221754 -2.9764686 -4.4210691 -5.6167908 -6.5702548][-3.6453419 -2.9923005 -2.1259546 -1.3854294 -0.59835529 0.35755634 1.3897295 1.6247959 1.7615538 0.080762863 -1.2487373 -2.4051414 -3.8925815 -5.1404963 -6.1588106][-2.8778954 -1.9810538 -0.99764776 0.030973911 1.1095877 1.688611 2.3706026 2.46245 2.4308252 0.55751038 -1.0208588 -2.3760076 -3.8577802 -5.0615959 -6.098834][-2.3643374 -1.5845299 -0.62757158 0.47606182 1.3748407 1.7014399 1.998661 2.1594582 2.283247 0.335989 -1.3363185 -2.7366486 -4.3127556 -5.4372191 -6.4064][-2.3782859 -1.3019676 -0.51899481 0.29805708 1.1060543 1.329483 1.5993061 1.5186901 1.3268766 -0.64872074 -2.0796251 -3.5207953 -4.9322128 -5.9578381 -6.8449297][-3.5562491 -2.597074 -1.860117 -0.99788857 -0.3283906 -0.36861658 -0.50116014 -0.55731821 -0.5011611 -2.481297 -3.9937041 -4.8496046 -5.7457805 -6.46584 -7.1448736][-4.723052 -4.0957375 -3.5306172 -2.6666102 -1.718111 -1.8650532 -1.8860674 -2.1521778 -2.4790683 -3.6868773 -4.8250713 -5.630528 -6.20684 -6.6275358 -6.9598274][-6.0288396 -5.8381853 -5.7740908 -5.1142969 -4.2814803 -4.0731468 -4.01417 -4.1500568 -4.1502666 -4.7987232 -5.5137396 -5.7425885 -5.9970264 -6.1155591 -6.2490468][-6.352109 -6.4873023 -6.619103 -5.9118524 -5.1888623 -4.9287105 -4.7809496 -4.8012986 -4.6781797 -5.1332006 -5.4050961 -5.4355454 -5.3996887 -5.416851 -5.5386262][-7.3652048 -7.155036 -6.7657251 -6.7182651 -6.3889427 -6.1725407 -6.1923528 -6.4016962 -6.380969 -5.9547148 -5.7768345 -5.7279105 -5.4785061 -5.2632289 -5.06847]]...]
INFO - root - 2017-12-15 18:03:38.905204: step 43410, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.618 sec/batch; 49h:35m:25s remains)
INFO - root - 2017-12-15 18:03:45.422814: step 43420, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:12m:44s remains)
INFO - root - 2017-12-15 18:03:51.793618: step 43430, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 51h:36m:13s remains)
INFO - root - 2017-12-15 18:03:58.193724: step 43440, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 51h:27m:07s remains)
INFO - root - 2017-12-15 18:04:04.713127: step 43450, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 52h:27m:15s remains)
INFO - root - 2017-12-15 18:04:11.030218: step 43460, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 51h:07m:12s remains)
INFO - root - 2017-12-15 18:04:17.387656: step 43470, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 51h:21m:18s remains)
INFO - root - 2017-12-15 18:04:23.820064: step 43480, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:12m:48s remains)
INFO - root - 2017-12-15 18:04:30.220265: step 43490, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 52h:37m:58s remains)
INFO - root - 2017-12-15 18:04:36.620848: step 43500, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 50h:43m:35s remains)
2017-12-15 18:04:37.144397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8652906 -6.2112012 -6.5318022 -6.4115553 -5.6292706 -4.4195986 -2.8930721 -1.3209505 0.22180891 0.088686943 -0.94300747 -4.0468073 -6.0761189 -8.5191841 -9.8273211][-4.969017 -5.5728297 -6.2506394 -6.5051923 -6.4367909 -5.1565838 -3.4838309 -1.2990799 0.15655422 -0.064167976 -1.2275548 -3.5287275 -5.6091175 -8.0834217 -9.6605272][-4.6240754 -5.2226877 -5.3638382 -4.9261026 -4.4816771 -3.7843168 -2.6659989 -1.3827548 0.087395668 -0.38207388 -1.3284984 -3.5394225 -4.9547777 -6.8204036 -8.3520689][-4.6773529 -4.622539 -4.3206024 -3.7303538 -3.1290827 -2.1734314 -1.162241 -0.55306578 0.18330193 -0.34938955 -1.1589489 -3.5488505 -4.8699441 -6.3587623 -7.1529531][-5.04109 -4.3951392 -3.5238094 -2.630949 -1.9091973 -0.64796591 0.54737473 1.1675539 1.481142 0.56784916 -0.57820559 -3.156723 -4.7609673 -6.3524547 -7.3257823][-4.7868762 -3.7290635 -2.4285898 -1.4148669 -0.22958755 1.062314 2.3874445 2.8242683 3.2276087 2.0408278 0.43787766 -2.4183321 -4.27334 -6.0638938 -7.1841183][-4.1139336 -2.7134233 -1.5872846 -0.4927001 0.64037037 1.6956005 2.9947224 3.6474466 4.0969629 2.8163004 1.2329426 -2.0488319 -4.046349 -6.048758 -7.1195307][-3.376461 -1.9887586 -0.59143543 0.33236122 1.3074722 1.9349308 2.5653191 3.3580971 4.0410767 2.9455652 1.1810284 -1.9054947 -4.072485 -6.1088696 -7.156939][-2.5887294 -1.363133 -0.17187691 0.89132595 1.9495468 2.1674347 2.509572 2.8290291 3.1135693 1.9088612 0.58572388 -2.0912957 -4.0764484 -6.187417 -7.6986995][-2.2862439 -1.2766418 0.098104 0.99777126 1.7782688 2.1965914 2.7428274 2.7035875 2.8246155 0.93883324 -0.86817455 -3.2505927 -4.6816683 -6.2891731 -7.4350734][-2.0354915 -1.288631 -0.31650829 0.62112427 1.375206 1.405262 1.7429619 1.8983192 1.9310579 0.2279582 -1.7173939 -4.0413246 -5.1272678 -6.5596366 -7.4917912][-2.8840652 -1.8798699 -0.81694412 -0.091042042 0.4749918 0.39752483 0.31028652 0.12704611 -0.29376221 -1.0286179 -2.2535791 -3.9888043 -5.3829851 -7.0018387 -7.6899333][-4.5541186 -3.6034575 -2.3941131 -1.5321822 -0.54779673 -0.52775335 -0.82834148 -1.2513828 -1.6197548 -2.412981 -3.0983019 -4.1792 -5.3282142 -6.4315963 -7.2723026][-6.3641419 -5.8304853 -4.4947491 -3.6043649 -2.4546342 -1.8622246 -1.5184336 -1.5898542 -2.1759639 -3.0530643 -4.1270227 -5.1223083 -5.654489 -6.2720861 -6.3391118][-7.3153691 -6.7579536 -5.9840426 -5.7164145 -4.8006487 -3.9961722 -3.3911495 -2.8267851 -3.4090428 -3.9195127 -4.8132782 -5.949646 -6.4094296 -6.4717159 -6.3494287]]...]
INFO - root - 2017-12-15 18:04:43.593797: step 43510, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 50h:35m:44s remains)
INFO - root - 2017-12-15 18:04:49.973730: step 43520, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 51h:38m:02s remains)
INFO - root - 2017-12-15 18:04:56.352247: step 43530, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 50h:19m:29s remains)
INFO - root - 2017-12-15 18:05:02.795822: step 43540, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 50h:44m:37s remains)
INFO - root - 2017-12-15 18:05:09.189997: step 43550, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 52h:20m:48s remains)
INFO - root - 2017-12-15 18:05:15.566932: step 43560, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:49m:14s remains)
INFO - root - 2017-12-15 18:05:22.022227: step 43570, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 50h:52m:58s remains)
INFO - root - 2017-12-15 18:05:28.487138: step 43580, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 51h:12m:07s remains)
INFO - root - 2017-12-15 18:05:34.886196: step 43590, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 52h:13m:49s remains)
INFO - root - 2017-12-15 18:05:41.346667: step 43600, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 52h:48m:27s remains)
2017-12-15 18:05:41.931689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4110579 -1.1834512 -1.1017299 -1.4244642 -1.6705575 -2.1890273 -2.4518046 -2.8540401 -3.6384282 -5.2420568 -5.26506 -6.5215893 -7.4265876 -8.4623556 -8.8399239][-1.6360545 -1.5734205 -1.5275278 -1.3421335 -1.3236914 -1.6658983 -2.1574988 -2.9322548 -3.3730459 -4.7181683 -5.26262 -6.3263426 -7.0159259 -8.4216747 -9.1981668][-2.3331289 -2.1783495 -1.8634768 -1.487462 -1.1319795 -1.0500035 -1.1945662 -1.5517859 -1.9769311 -3.3435874 -3.6301804 -4.891057 -5.9980011 -7.1673245 -7.7019124][-3.286386 -2.5616441 -2.5281153 -1.8855381 -1.1040311 -0.79669523 -0.7312274 -0.834147 -0.95419693 -1.9684587 -2.2175832 -3.5019407 -4.3821383 -5.6621666 -6.24098][-4.2326488 -3.1933055 -2.44096 -1.7293725 -1.2224555 -0.39126396 0.13102865 0.17451715 0.33735657 -0.40875959 -0.45576429 -1.8780727 -2.9590702 -4.4782691 -5.2666054][-4.3548832 -3.2408519 -2.6224141 -1.8862352 -1.1003184 -0.093613148 0.65860081 0.94509697 1.2338543 0.46024513 0.44582748 -1.1255379 -2.36384 -3.857193 -4.5986013][-4.273643 -3.4876375 -2.8718467 -1.7270308 -1.1637177 -0.20044184 0.75446033 1.0060863 1.2262449 0.1982646 0.10579109 -1.5414376 -3.0257673 -4.1089044 -4.4279518][-3.6272182 -3.0908985 -2.4584365 -1.3937197 -0.61470556 0.0712266 0.67800426 1.0368328 1.5957222 0.0024356842 -0.66370821 -2.564785 -3.6492882 -4.6146555 -4.904737][-3.6911192 -2.7571435 -2.187614 -1.1962967 -0.46515179 0.086745739 0.82022 0.8473177 1.0020638 -0.59446478 -1.3304443 -3.5173078 -4.8825731 -5.8525386 -5.7731695][-4.4148245 -3.7238617 -2.9603076 -2.089952 -1.5950518 -0.6414566 -0.024399281 -0.24356556 -0.29544926 -2.1812129 -3.0061512 -4.8892527 -6.1190224 -7.2374468 -7.4754696][-5.7735724 -5.2100668 -4.9701538 -4.2950807 -3.5591202 -2.8378782 -2.2592964 -2.2518287 -2.3658767 -4.0310154 -4.7716284 -6.4379973 -7.4505472 -8.478754 -8.2954283][-7.101253 -6.4706097 -6.1290655 -5.7563748 -5.5547714 -4.7672205 -3.9239471 -3.8330965 -3.7116268 -4.9402838 -5.6561031 -6.6242323 -7.5512404 -8.3757553 -8.3540649][-8.0693655 -7.5318961 -7.3419504 -6.9455934 -6.3253617 -5.5525579 -4.9745727 -5.0195751 -5.0632734 -5.653038 -6.0124044 -6.0674624 -6.5618143 -7.2512975 -7.1792855][-7.8228 -7.6566958 -7.645731 -7.1661134 -6.7567449 -5.7427893 -5.0407119 -5.38618 -5.5011768 -5.7668295 -5.9445314 -5.9674397 -6.2096548 -6.2283058 -5.9884739][-8.0789413 -7.7594805 -7.7539935 -7.5270576 -7.2973104 -6.6652455 -6.1306586 -6.2575521 -6.224659 -6.2543955 -6.2197247 -6.2012949 -6.2073603 -6.4182396 -6.220233]]...]
INFO - root - 2017-12-15 18:05:48.357993: step 43610, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.658 sec/batch; 52h:50m:23s remains)
INFO - root - 2017-12-15 18:05:54.784410: step 43620, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 51h:53m:19s remains)
INFO - root - 2017-12-15 18:06:01.251394: step 43630, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:00m:33s remains)
INFO - root - 2017-12-15 18:06:07.588861: step 43640, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 51h:10m:14s remains)
INFO - root - 2017-12-15 18:06:13.975813: step 43650, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 50h:58m:55s remains)
INFO - root - 2017-12-15 18:06:20.485309: step 43660, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 52h:47m:15s remains)
INFO - root - 2017-12-15 18:06:26.905753: step 43670, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 51h:17m:05s remains)
INFO - root - 2017-12-15 18:06:33.288203: step 43680, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 49h:53m:25s remains)
INFO - root - 2017-12-15 18:06:39.657598: step 43690, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.625 sec/batch; 50h:08m:40s remains)
INFO - root - 2017-12-15 18:06:46.104654: step 43700, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 51h:11m:42s remains)
2017-12-15 18:06:46.688916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4920392 -5.0161085 -3.4711409 -3.4540005 -3.6014829 -3.1763692 -2.6491866 -2.5720625 -2.4666605 -4.5139408 -5.8473177 -6.0076504 -7.0060191 -7.7061863 -8.7031269][-5.0567188 -4.7516065 -4.4833775 -3.7644076 -2.909481 -2.6459613 -2.2138214 -1.9116955 -1.856627 -3.8520389 -5.0451865 -5.9280024 -7.1808934 -7.3524122 -8.4439783][-4.2730064 -3.3934364 -2.7467499 -2.6557627 -2.073627 -1.4614749 -0.73933029 -0.43183661 -0.22772837 -2.0708456 -3.8529193 -4.6208868 -5.73173 -6.3664961 -7.3539505][-3.8394413 -2.4878931 -1.5842733 -0.93157911 -0.037731171 0.80182171 1.5496216 1.36269 1.3098373 -0.74623013 -2.3958154 -3.5768595 -4.9971447 -5.101954 -6.0051727][-3.7008667 -1.9962382 -0.58429337 0.36834145 1.2625809 2.0568562 2.84795 2.935503 2.581439 0.17552423 -1.729135 -3.0547523 -4.3861513 -4.8668141 -5.6788025][-3.0343027 -1.7221971 -0.84641123 0.42307854 1.8283606 2.7019958 3.5892029 3.5555525 3.4312801 1.1019258 -0.96408081 -2.3468122 -3.9027169 -4.4046531 -5.2263131][-2.8797131 -1.545887 -0.32814646 1.0768843 2.3047457 3.3822145 4.1982546 3.9278746 3.6368103 1.3763638 -0.38930273 -1.8897877 -3.525825 -4.019948 -4.7727485][-2.1199279 -1.0859365 -0.12975216 1.4175539 2.821044 3.5252419 4.2668495 4.1505232 3.9934816 1.4322357 -0.50852728 -1.8281164 -3.5638561 -4.1970844 -4.9467406][-1.758347 -1.0970654 -0.35830021 0.3929224 1.5027494 2.4762068 3.0883961 3.3727026 3.2756653 0.86794758 -0.70142078 -1.8125653 -3.3821578 -4.266161 -5.0755482][-1.6774883 -1.5356193 -1.2012858 -0.98710537 -0.49653816 0.18060255 0.84555721 1.3939505 1.3133688 -1.0855737 -2.319962 -3.3078399 -4.3631248 -4.600812 -5.1076026][-2.5732226 -2.3803897 -2.4688077 -2.0438585 -1.980021 -1.6440177 -1.5084419 -1.1637907 -1.3108902 -3.3098173 -4.7395363 -5.1146922 -5.300283 -5.6161413 -6.2775431][-3.9305146 -3.4357362 -2.8011708 -2.6367226 -2.5474272 -2.2948084 -2.314167 -2.4190001 -2.9981742 -4.4606609 -5.1082745 -5.9097357 -6.7478724 -6.7241774 -6.8231][-4.5222964 -3.8079929 -3.6849298 -3.38058 -3.1883841 -3.1587081 -3.0399113 -3.3500795 -3.8933814 -5.3136063 -6.4502974 -6.7871537 -7.0502305 -7.1585822 -7.3571][-4.0146971 -4.1894636 -3.9159596 -3.7848663 -3.60258 -3.5317059 -3.873642 -3.8232539 -3.6369424 -4.3309703 -4.9270782 -5.9972262 -7.10202 -6.5901322 -6.4227142][-5.3476586 -4.1979361 -4.0124893 -4.3734388 -4.7319174 -4.5526514 -4.4006176 -4.514823 -4.797719 -4.9008427 -5.0212622 -5.2783847 -5.6773033 -6.5562444 -7.0977378]]...]
INFO - root - 2017-12-15 18:06:53.060416: step 43710, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:13m:28s remains)
INFO - root - 2017-12-15 18:06:59.497852: step 43720, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 50h:37m:41s remains)
INFO - root - 2017-12-15 18:07:06.015052: step 43730, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 53h:28m:51s remains)
INFO - root - 2017-12-15 18:07:12.408563: step 43740, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:45m:58s remains)
INFO - root - 2017-12-15 18:07:18.785653: step 43750, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 52h:51m:26s remains)
INFO - root - 2017-12-15 18:07:25.167914: step 43760, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 50h:26m:50s remains)
INFO - root - 2017-12-15 18:07:31.648337: step 43770, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 53h:05m:12s remains)
INFO - root - 2017-12-15 18:07:38.115240: step 43780, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:44m:41s remains)
INFO - root - 2017-12-15 18:07:44.580920: step 43790, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 52h:41m:07s remains)
INFO - root - 2017-12-15 18:07:50.942555: step 43800, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.631 sec/batch; 50h:37m:20s remains)
2017-12-15 18:07:51.456223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.693815 -4.2371 -4.8971386 -5.2405477 -5.5927162 -5.4038849 -5.1887546 -4.7305222 -3.87457 -3.1299992 -2.9229007 -4.3993855 -5.3591871 -6.5481777 -7.0430288][-3.4028358 -3.3658862 -4.0979018 -5.0656624 -5.8521638 -6.0963655 -6.0309258 -5.4108229 -4.361423 -3.4953294 -3.180582 -4.0276241 -4.8559775 -6.1137733 -6.5057688][-4.1842175 -4.1651039 -4.8251925 -5.0022378 -5.4828281 -5.6722641 -5.3642635 -5.4778147 -4.768127 -4.302443 -3.8291204 -4.9056215 -5.9080215 -6.5012021 -6.3938122][-3.9604051 -4.0111465 -4.2293086 -3.935627 -3.6768503 -3.4582834 -3.2998781 -3.6226125 -3.0731292 -3.0671353 -3.3565278 -4.76334 -5.7540727 -6.7344627 -7.1245728][-3.666513 -3.4148459 -2.6813483 -2.0811625 -1.503449 -1.0051813 -0.58600283 -0.728919 -0.33568239 -0.33321142 -1.1944404 -3.2821317 -4.4950933 -5.5817 -6.20003][-3.3316102 -2.6502194 -1.7693696 -0.85401344 0.2455945 0.76841927 1.0230093 1.1397142 1.3851786 1.1158466 0.35729885 -1.8354635 -3.2858863 -4.2614279 -4.6231213][-2.6581764 -2.0956974 -0.82016468 0.19695807 1.1362581 1.6627865 1.7437181 1.9138317 2.0465431 1.6085215 0.45429611 -1.401741 -2.6597672 -3.9269688 -4.2104449][-2.0144157 -1.3247604 0.0026907921 0.59141541 1.0760641 1.5534382 1.8950596 2.0555649 2.1697083 1.8988981 0.77770996 -1.1699147 -2.8779311 -3.7976563 -4.6998711][-1.5272169 -0.906322 -0.54652405 0.010532856 0.14744806 0.36240864 0.65626526 1.2471972 1.5239801 1.0329895 -0.20802736 -1.8558297 -3.6607695 -4.7534719 -5.6908741][-2.1934657 -1.5100789 -1.3605819 -1.1376367 -1.3409805 -1.1738582 -0.98784876 -0.61984539 -0.33832598 -0.65870953 -1.9595828 -3.0608711 -4.5738735 -5.8530903 -6.7314773][-5.6093316 -4.5365362 -4.2084293 -3.8055689 -3.5310783 -3.1023545 -2.6798711 -2.49338 -2.6393485 -3.0063915 -4.0762663 -4.755887 -5.8862295 -6.5709295 -7.2001724][-7.0714154 -6.854033 -7.58584 -6.7648454 -6.1614962 -5.5498991 -4.9817538 -4.72596 -4.4130869 -4.6565323 -5.47286 -6.0071268 -6.8423929 -6.8310719 -6.73811][-8.1958342 -8.4634552 -8.40231 -8.0558109 -7.6813855 -6.7796626 -6.1972075 -6.1542244 -5.9215384 -5.6732588 -6.0681295 -6.1842732 -6.5760169 -6.3898163 -6.1064186][-8.5196838 -8.6332941 -8.5312185 -8.0309982 -7.4935079 -6.6869664 -6.0635486 -5.904089 -5.7441354 -5.8340378 -6.111865 -6.0932927 -5.9635592 -5.5288267 -5.2462931][-7.7835507 -8.0158405 -7.9713883 -7.7164707 -7.1130433 -6.6050048 -6.2445498 -6.3031845 -6.0729375 -5.8972592 -5.763092 -5.8298674 -5.8192954 -5.7384014 -5.3609676]]...]
INFO - root - 2017-12-15 18:07:57.808074: step 43810, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 50h:12m:24s remains)
INFO - root - 2017-12-15 18:08:04.173555: step 43820, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 51h:11m:00s remains)
INFO - root - 2017-12-15 18:08:10.609458: step 43830, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 50h:58m:26s remains)
INFO - root - 2017-12-15 18:08:16.951687: step 43840, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 50h:11m:32s remains)
INFO - root - 2017-12-15 18:08:23.375131: step 43850, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 51h:23m:25s remains)
INFO - root - 2017-12-15 18:08:29.811452: step 43860, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 52h:16m:19s remains)
INFO - root - 2017-12-15 18:08:36.151774: step 43870, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 50h:56m:44s remains)
INFO - root - 2017-12-15 18:08:42.446121: step 43880, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 49h:25m:57s remains)
INFO - root - 2017-12-15 18:08:48.897290: step 43890, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 53h:27m:30s remains)
INFO - root - 2017-12-15 18:08:55.309715: step 43900, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 52h:57m:51s remains)
2017-12-15 18:08:55.870259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5880122 -3.4559131 -2.9988289 -2.0103045 -1.1385703 -0.08513546 0.75884914 0.60431194 0.85079384 -0.067763329 -1.3590803 -3.3502336 -4.5639868 -6.082098 -7.3305264][-3.4077716 -3.0291748 -2.80926 -2.4531417 -1.6252646 -0.66100645 -0.12699795 0.12605762 0.1608243 -1.3809648 -2.3786001 -3.7446237 -5.3525138 -6.6707807 -7.26094][-3.7846236 -3.3876219 -3.0106583 -2.3436055 -1.8132129 -1.0232782 -0.062953949 -0.23021126 -0.30787277 -1.3912392 -2.444212 -4.16561 -5.3025856 -6.1678262 -6.8279276][-4.5854073 -4.0910311 -3.9929247 -3.1547461 -2.2247472 -1.1239414 -0.6590848 -0.805655 -0.49919033 -1.9156637 -3.0343475 -4.4741592 -5.6759558 -6.349637 -6.6924558][-5.0991068 -4.43881 -3.591588 -2.6850281 -1.8349109 -0.765255 -0.15436172 -0.3483181 -0.39170218 -1.870419 -2.8130398 -4.0715809 -5.2143831 -6.2396841 -6.5605431][-4.9387136 -4.2532787 -3.5103202 -2.0097227 -0.71173811 -0.19211245 0.12525415 0.36114979 0.60427094 -0.83463383 -2.0084743 -3.4536376 -4.49844 -5.4392915 -5.9524803][-5.0943365 -4.2913094 -3.217319 -1.9000154 -1.070663 0.12037611 1.0934324 1.2029524 1.3591356 0.10395527 -0.87735319 -2.6007566 -4.2607841 -5.7759733 -6.60176][-4.7326622 -3.6814747 -2.7386308 -1.6549745 -0.48873186 0.46905041 0.95412064 1.4263039 2.0422239 0.62316036 -0.85869503 -2.3920555 -3.862963 -5.4256544 -6.5537925][-4.0590634 -3.5049353 -2.7977266 -1.2181063 -0.016616821 0.32701111 0.78945827 1.0941944 1.7305775 0.91244125 -0.034534931 -1.934382 -3.9608037 -5.2707939 -6.1528673][-4.378088 -3.5740633 -2.353797 -1.3557448 -0.68378496 0.37857533 1.2548838 1.3801184 1.7873936 0.79358196 -0.023481369 -1.9396729 -3.6607742 -5.50093 -6.7592583][-3.8803759 -3.2577195 -2.6624823 -1.8856707 -0.97788668 -0.63924837 -0.40766573 0.58908463 1.4276323 -0.19827366 -1.2335 -2.5682979 -4.4369564 -6.2466321 -7.1388688][-2.951035 -3.4966197 -3.3153386 -2.4143224 -2.047421 -1.8912878 -1.3155642 -0.73638582 -0.18235683 -0.60399008 -1.5946522 -3.3472223 -4.9585676 -6.0979424 -6.9201136][-4.2817216 -3.8935714 -3.774781 -3.6559944 -3.0211906 -2.3111186 -1.8620377 -1.6542525 -1.0963559 -1.4132371 -2.2680197 -3.5415254 -5.31636 -6.5520158 -7.1986761][-3.9354923 -4.1861391 -4.611743 -4.1807127 -4.169219 -3.7784774 -3.1449256 -2.5327158 -2.1916552 -2.8350263 -3.2045617 -4.0789924 -5.0941658 -6.4949727 -7.3388233][-5.9833584 -6.1988316 -5.85122 -5.7659969 -5.9071989 -6.167469 -6.2616682 -5.4289274 -4.7571993 -4.6866312 -4.8413653 -5.3605804 -6.1365342 -6.748095 -7.1390896]]...]
INFO - root - 2017-12-15 18:09:02.260905: step 43910, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 51h:00m:21s remains)
INFO - root - 2017-12-15 18:09:08.655077: step 43920, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 51h:52m:58s remains)
INFO - root - 2017-12-15 18:09:14.994347: step 43930, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:44m:34s remains)
INFO - root - 2017-12-15 18:09:21.429335: step 43940, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 50h:05m:58s remains)
INFO - root - 2017-12-15 18:09:27.897707: step 43950, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 53h:18m:22s remains)
INFO - root - 2017-12-15 18:09:34.221572: step 43960, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 49h:45m:08s remains)
INFO - root - 2017-12-15 18:09:40.635901: step 43970, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 52h:25m:50s remains)
INFO - root - 2017-12-15 18:09:47.018705: step 43980, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:13s remains)
INFO - root - 2017-12-15 18:09:53.399564: step 43990, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 51h:16m:00s remains)
INFO - root - 2017-12-15 18:09:59.699559: step 44000, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 50h:13m:39s remains)
2017-12-15 18:10:00.225259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0407667 -3.1563368 -4.0107756 -4.1183977 -4.4603896 -4.4986019 -4.2357893 -4.2000141 -4.1950588 -4.5365629 -5.7388449 -8.1016836 -8.6096783 -9.5693712 -9.821249][-3.4445705 -3.632987 -3.8515103 -4.0578051 -3.6032443 -3.7340846 -3.5152917 -3.9558756 -4.5692439 -4.8908567 -5.352972 -7.4085617 -7.7474141 -8.1844788 -8.4270992][-2.9939265 -2.0778251 -2.8439493 -2.851758 -2.6707783 -3.0227752 -2.8410978 -3.3792362 -3.2178783 -3.2696042 -3.5819864 -5.7492294 -6.2905264 -7.1367769 -7.0105543][-3.1603165 -2.5022087 -2.6890073 -2.0493956 -1.8625603 -1.6427927 -0.92607594 -1.6699023 -2.2732244 -2.7074571 -3.0949764 -4.950202 -4.96327 -5.6569042 -5.8814578][-2.7423491 -2.6577168 -2.779027 -1.876986 -1.4532065 -0.76862144 -0.33295155 -0.4666934 -0.54626751 -1.3012528 -1.8082237 -3.4204435 -3.2898135 -4.1751781 -4.3599606][-3.2989507 -2.8178225 -2.7187142 -1.6777525 -1.0135727 -0.22022009 0.63057995 0.67460251 0.38721085 -0.10801554 -0.3749733 -2.1278496 -2.151576 -3.4295831 -4.2258053][-4.4201155 -4.2788134 -3.8158569 -2.2632489 -1.0661626 0.5769949 1.1513014 1.2881765 1.5915422 0.98559 0.51816368 -1.2382569 -1.4301825 -2.3868365 -2.8039556][-4.8271313 -4.7465448 -3.9245088 -2.7008204 -1.6894979 -0.26151037 0.88870525 1.7910242 2.2590532 1.5061207 1.1154604 -0.74032068 -1.282218 -2.3554702 -3.2959719][-5.2336197 -5.0005436 -4.3021221 -3.328301 -2.59581 -1.0871058 0.03083849 0.81917858 1.4962845 1.482975 1.3951979 -0.40947914 -1.5771871 -2.3237772 -3.0350142][-5.1528888 -5.1893539 -4.7712393 -4.1207976 -3.6229429 -2.2355008 -0.99384689 -0.57553387 0.54211712 0.098495483 -0.91497707 -2.5589266 -3.4382582 -3.9283195 -4.4246159][-6.7711482 -5.5045738 -4.2885714 -3.8857508 -3.5418057 -2.6700735 -2.1981997 -1.4262638 -0.95414925 -2.2840919 -3.4381847 -3.7364721 -4.3994608 -5.1409841 -5.5464945][-5.1926455 -4.4001093 -3.7831488 -3.3902454 -3.3146763 -2.9736333 -2.5549307 -2.9966121 -3.3776393 -4.3074617 -5.4713569 -6.376111 -7.1914916 -7.3076668 -6.7595034][-5.2587938 -4.7310128 -3.718981 -3.5723391 -3.3498702 -2.8287458 -2.8955479 -3.4509788 -3.9127507 -4.8003578 -6.5228081 -7.9681253 -8.7754049 -8.4 -7.5946851][-5.6756067 -5.2084627 -4.3179903 -3.9737208 -3.2773881 -2.8389845 -2.7142162 -2.9952168 -3.8339279 -4.9025965 -6.6694107 -7.7748876 -8.2392025 -8.7051716 -8.6685839][-6.1618629 -5.6590457 -4.8604884 -4.561142 -3.940855 -3.5507627 -3.6549625 -3.8279173 -4.3015652 -5.2122936 -5.7531776 -6.4880004 -7.3878694 -8.613 -8.8590946]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 18:10:06.595240: step 44010, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 50h:10m:35s remains)
INFO - root - 2017-12-15 18:10:12.998142: step 44020, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 51h:01m:09s remains)
INFO - root - 2017-12-15 18:10:19.415335: step 44030, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 49h:48m:42s remains)
INFO - root - 2017-12-15 18:10:25.793963: step 44040, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 49h:35m:42s remains)
INFO - root - 2017-12-15 18:10:32.168548: step 44050, loss = 0.37, batch loss = 0.26 (12.1 examples/sec; 0.659 sec/batch; 52h:47m:05s remains)
INFO - root - 2017-12-15 18:10:38.515839: step 44060, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 51h:07m:03s remains)
INFO - root - 2017-12-15 18:10:44.855921: step 44070, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 50h:36m:09s remains)
INFO - root - 2017-12-15 18:10:51.128627: step 44080, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.613 sec/batch; 49h:07m:02s remains)
INFO - root - 2017-12-15 18:10:57.622723: step 44090, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 49h:37m:34s remains)
INFO - root - 2017-12-15 18:11:04.039283: step 44100, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 51h:03m:29s remains)
2017-12-15 18:11:04.629485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5984049 -3.5945907 -3.6575065 -3.8885512 -3.6100788 -3.346168 -3.1691952 -3.2767353 -3.6242375 -4.3870935 -5.0379534 -6.3727031 -7.2210541 -8.3047609 -8.2792187][-3.0620675 -3.2322683 -3.7610726 -3.9572263 -3.574162 -2.9728384 -2.8219953 -3.0434628 -3.2421746 -3.8165264 -4.3232794 -5.8004661 -6.6746283 -7.8853106 -8.1176949][-1.6344581 -1.6187716 -2.1461191 -2.2721815 -1.9974499 -1.4298916 -1.3517675 -1.4028082 -1.7351146 -2.4744658 -3.3433647 -4.7926412 -5.6478357 -7.0556588 -7.5135431][-0.92069578 -0.79314947 -0.95178223 -0.86120319 -0.8559618 -0.45780373 -0.19334555 -0.20911026 -0.57487726 -1.5608497 -2.7973228 -4.2409062 -5.0402031 -6.4796329 -6.8700857][-1.0051951 -0.80177355 -0.97489023 -0.8677001 -0.64838219 0.05258894 0.54897785 0.58708 0.38780785 -0.71038151 -1.843668 -3.2530675 -4.1552491 -5.6295176 -5.8851886][-0.97390127 -0.7960887 -0.80450392 -0.48656893 -0.33237743 0.36168194 1.2034664 1.3078642 1.2156963 0.30760098 -0.57731152 -1.7144098 -2.4862127 -3.953336 -4.4557381][-2.5030589 -1.9294796 -1.6445928 -1.2371111 -0.544055 0.62197781 1.4686146 1.6099405 1.5991707 0.7216711 -0.14708424 -1.3642302 -2.3076563 -3.8184392 -4.6604562][-3.6607976 -2.8990378 -2.4582648 -1.6692448 -0.5805397 0.59843636 1.564558 1.9910221 2.060318 1.3111324 0.383893 -0.80695629 -1.9388924 -3.5205374 -4.5143781][-4.1801386 -3.1048136 -2.4482493 -1.6480684 -0.686944 0.42782021 1.3301 1.7492628 2.1665878 1.5835381 0.58378983 -0.68352413 -2.071866 -3.8488646 -4.6442924][-4.9564967 -3.9505434 -3.0869803 -2.2399836 -1.3532071 -0.34760666 0.54931259 0.96088028 1.4204035 0.82983685 -0.358665 -1.4312377 -2.697134 -4.1039066 -4.8872633][-6.1819406 -5.3689842 -4.6823235 -3.6651592 -2.7679453 -2.0767303 -1.5206904 -1.2165923 -0.95986652 -1.7676501 -2.7773709 -3.1774006 -3.672101 -4.4641471 -4.9568243][-7.4403133 -6.7439885 -6.23629 -5.7032948 -5.0802736 -4.4063129 -3.7727504 -3.745518 -3.6137929 -4.1939311 -4.9138112 -4.9256659 -5.1120863 -5.5043955 -6.0586004][-7.7021232 -7.3578086 -7.1515994 -6.935708 -6.576582 -6.1000166 -5.78184 -5.9060936 -5.9482536 -6.1508908 -6.4119573 -6.0159979 -6.1302686 -6.109972 -5.9108648][-7.5103741 -7.3809924 -7.3731503 -7.5855808 -7.2826662 -7.1140747 -7.002696 -7.2265186 -7.2434988 -7.2922697 -7.2493205 -6.6448212 -6.0540442 -5.8765321 -5.8801546][-8.0695133 -8.2719164 -8.013032 -8.1540623 -8.2244167 -8.2460012 -8.2488327 -8.4356527 -8.414629 -8.2480679 -7.9278483 -7.365973 -6.721777 -6.413765 -6.4448791]]...]
INFO - root - 2017-12-15 18:11:11.025835: step 44110, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:58m:26s remains)
INFO - root - 2017-12-15 18:11:17.376510: step 44120, loss = 0.25, batch loss = 0.14 (13.1 examples/sec; 0.611 sec/batch; 48h:55m:32s remains)
INFO - root - 2017-12-15 18:11:23.732363: step 44130, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 50h:52m:21s remains)
INFO - root - 2017-12-15 18:11:30.077453: step 44140, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 50h:52m:46s remains)
INFO - root - 2017-12-15 18:11:36.389021: step 44150, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 49h:41m:20s remains)
INFO - root - 2017-12-15 18:11:42.766477: step 44160, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 50h:18m:28s remains)
INFO - root - 2017-12-15 18:11:49.126638: step 44170, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 50h:47m:14s remains)
INFO - root - 2017-12-15 18:11:55.478556: step 44180, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:58m:40s remains)
INFO - root - 2017-12-15 18:12:01.879866: step 44190, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:15s remains)
INFO - root - 2017-12-15 18:12:08.280531: step 44200, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.623 sec/batch; 49h:55m:10s remains)
2017-12-15 18:12:08.790495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5184999 -2.3324075 -2.3849826 -2.8220487 -3.0692258 -2.5140705 -1.9533553 -2.4398427 -2.9171319 -4.0598531 -4.271934 -5.1392088 -6.248311 -6.9877024 -6.7786827][-3.339427 -3.4469233 -3.8719711 -3.9112811 -3.5974617 -3.7358978 -3.9496481 -3.6429224 -3.098557 -4.5929618 -5.2745638 -6.2994804 -7.3120513 -7.9028277 -8.1267729][-5.1105785 -4.9905739 -5.2460165 -5.16381 -4.6390419 -4.3132024 -3.9554102 -4.0254564 -4.1387677 -5.4397521 -5.5055084 -6.7246046 -7.8800759 -8.8390265 -8.4946585][-5.684988 -5.4311638 -5.5482988 -4.616703 -3.7563179 -3.0253067 -2.1911554 -2.1590352 -2.3658614 -4.2808952 -4.7416368 -6.2337875 -7.4110203 -7.9997597 -7.5109258][-7.359045 -5.9547057 -4.7341957 -3.7474217 -2.8284912 -1.5165119 -0.24292898 -0.18059635 -0.44319773 -2.4665079 -2.8643823 -4.4778891 -5.6635752 -6.8367109 -6.7556534][-7.8973188 -6.9563785 -6.5036445 -4.31758 -2.1607275 -0.27026606 1.1293526 1.2139511 0.97893238 -1.2070637 -1.9436917 -3.9000313 -5.6727972 -7.0236096 -6.9641171][-8.4401178 -6.6656818 -4.6036234 -2.4624109 -0.57871389 1.7471933 3.2800388 3.4756031 3.1692505 0.36863995 -0.86596489 -2.9714785 -4.6019964 -5.958777 -6.0527225][-7.8348117 -6.1798897 -4.9129381 -1.7255054 1.2793207 3.1788683 4.5015907 5.0185785 4.9967709 2.0844469 0.82382488 -1.6666789 -3.5679612 -4.6670122 -4.4570155][-7.5772619 -6.5323515 -5.013669 -2.564167 -0.69123316 1.048563 2.3467054 3.4796228 4.140708 1.660099 0.48623085 -1.5698409 -3.1716452 -4.8092041 -5.1682034][-8.466815 -7.1691456 -6.2153859 -3.9881408 -1.8203034 -0.46411991 0.44495773 1.2024078 1.7379742 -0.63405848 -1.9630623 -3.7033393 -5.0121341 -5.8723087 -5.7089167][-8.7352648 -8.9014158 -8.6555786 -6.8544283 -5.180768 -3.4922738 -2.2368321 -1.999898 -1.9358144 -3.6997681 -4.3437729 -5.526804 -6.7583294 -7.3254504 -6.8003073][-9.7198486 -8.995615 -8.6628466 -8.0122538 -7.1997294 -5.9449725 -5.1450658 -4.2514873 -3.2925224 -4.9274688 -5.601995 -5.9666905 -6.2857776 -7.1616182 -7.5908852][-9.2451239 -9.1938057 -9.10961 -8.3056765 -7.5300331 -6.6700077 -6.0041766 -5.6115432 -5.1998339 -5.5723615 -5.4057884 -5.6878943 -6.0557914 -6.4853826 -6.2556314][-8.5379848 -8.8737516 -8.9182739 -8.4528713 -7.7111344 -6.6007032 -6.1541977 -6.2297616 -6.3425574 -6.5347509 -6.4975529 -6.3826113 -6.2467813 -6.3765635 -6.372179][-8.76123 -8.2076969 -8.3105831 -8.4128189 -7.96918 -7.2315297 -7.0509472 -7.2764368 -7.3722572 -7.025095 -6.8604751 -7.3475595 -7.4825864 -7.1701155 -6.8339357]]...]
INFO - root - 2017-12-15 18:12:15.197077: step 44210, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 50h:15m:43s remains)
INFO - root - 2017-12-15 18:12:21.568192: step 44220, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 49h:31m:59s remains)
INFO - root - 2017-12-15 18:12:27.973519: step 44230, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 52h:36m:16s remains)
INFO - root - 2017-12-15 18:12:34.414131: step 44240, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 52h:26m:25s remains)
INFO - root - 2017-12-15 18:12:40.818721: step 44250, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 50h:14m:11s remains)
INFO - root - 2017-12-15 18:12:47.144018: step 44260, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:00s remains)
INFO - root - 2017-12-15 18:12:53.617824: step 44270, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 53h:06m:09s remains)
INFO - root - 2017-12-15 18:13:00.021962: step 44280, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 51h:05m:18s remains)
INFO - root - 2017-12-15 18:13:06.344743: step 44290, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 50h:16m:16s remains)
INFO - root - 2017-12-15 18:13:12.723363: step 44300, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 50h:41m:11s remains)
2017-12-15 18:13:13.219259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0556993 -3.5964265 -5.0683975 -5.3082066 -5.4891682 -5.5755968 -5.3244085 -4.5653391 -3.7100313 -4.9193163 -5.422389 -6.4012012 -6.7953997 -7.4622493 -8.2479134][-3.543457 -3.7627027 -4.6110277 -5.4477692 -6.3872881 -6.2879343 -5.6215405 -4.5562868 -3.7690904 -5.0611515 -5.4846392 -6.7848892 -7.5216508 -8.4722252 -9.2100649][-2.5748239 -2.7171531 -3.2438149 -3.4726572 -3.881932 -4.195466 -3.9138212 -3.3421469 -2.9493585 -4.4115815 -5.0579386 -6.2207007 -6.78627 -7.7530346 -8.3559732][-1.2065539 -1.47753 -2.0996451 -2.5676594 -2.3864374 -2.0386033 -1.812459 -1.5295362 -1.1177301 -3.2154264 -4.1027079 -5.4319715 -6.4249358 -7.4820032 -8.7797031][-1.8115306 -1.1976423 -0.86792755 -1.1893497 -0.87181282 -0.31903172 0.216753 0.5865736 0.41575813 -1.4873791 -2.3704433 -3.9028349 -4.6884918 -5.9290285 -7.2759914][-2.3426814 -2.1682949 -1.6052828 -0.78887367 0.10120487 1.2258215 2.4818859 3.0034227 3.2257137 0.99118328 -0.21646357 -2.2413888 -3.5245876 -4.5659881 -5.3983011][-3.6199865 -3.0842686 -2.2473683 -0.89027643 0.85455036 2.6157131 3.8665047 3.7836752 3.7052574 1.2945719 -0.16199589 -2.4120135 -3.9023535 -5.1129103 -6.420033][-3.5652127 -2.9798846 -2.1115212 -0.24442863 1.6496649 3.1323929 4.1678896 4.1993408 3.9217825 1.0997639 -0.43552923 -3.0050945 -4.3986483 -5.5996356 -6.9817448][-4.1347628 -3.2158933 -1.9477286 -0.31635571 1.3677998 2.5907669 3.4042826 3.8260326 4.1446657 1.160902 -1.1425233 -3.2734804 -4.6381087 -5.979764 -7.1155424][-4.7390575 -3.8483233 -2.9463377 -1.2314625 0.54063511 1.5066681 2.0408449 2.4094238 2.6402769 -0.0047607422 -1.7023253 -4.4075203 -5.5218163 -6.3693104 -7.5517211][-5.40948 -4.6791177 -4.0783663 -2.6731257 -1.458499 -0.07089138 0.18610764 0.056902409 0.29850197 -2.32556 -4.0512342 -5.6741242 -6.3699994 -7.4793406 -8.3443794][-5.6367865 -4.6573572 -3.817565 -3.2698946 -2.6027288 -1.8742089 -1.8586445 -1.5332284 -1.5822663 -3.2565398 -4.0734062 -5.9140058 -6.436564 -7.28703 -8.4143486][-5.935957 -4.8166904 -4.3047867 -4.1367121 -4.1562977 -3.9007461 -3.5605688 -4.055728 -4.1260304 -5.3759489 -6.1659908 -6.6980863 -7.2520056 -7.7853861 -8.3355227][-6.1746073 -5.2828956 -4.9569912 -4.4504404 -4.5009575 -4.3408327 -4.5024605 -4.837883 -5.1463609 -6.0404878 -6.1713991 -6.9054327 -6.8739882 -7.1965747 -7.6060829][-6.1209192 -6.1537161 -5.8829565 -5.5182934 -5.5389528 -5.8992929 -6.3049955 -6.9313641 -7.42763 -7.4338431 -7.2777867 -7.2332163 -7.1653996 -6.9917445 -6.9102807]]...]
INFO - root - 2017-12-15 18:13:19.541314: step 44310, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.629 sec/batch; 50h:19m:00s remains)
INFO - root - 2017-12-15 18:13:25.847173: step 44320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:19s remains)
INFO - root - 2017-12-15 18:13:32.277424: step 44330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:44s remains)
INFO - root - 2017-12-15 18:13:38.634918: step 44340, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 50h:58m:49s remains)
INFO - root - 2017-12-15 18:13:45.128073: step 44350, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 51h:40m:08s remains)
INFO - root - 2017-12-15 18:13:51.627899: step 44360, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 50h:52m:38s remains)
INFO - root - 2017-12-15 18:13:58.003214: step 44370, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 50h:47m:26s remains)
INFO - root - 2017-12-15 18:14:04.367608: step 44380, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 51h:02m:50s remains)
INFO - root - 2017-12-15 18:14:10.724292: step 44390, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 52h:14m:59s remains)
INFO - root - 2017-12-15 18:14:17.131207: step 44400, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 51h:38m:39s remains)
2017-12-15 18:14:17.597089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4772563 -2.7179003 -3.086463 -3.7044942 -4.3498573 -4.7234111 -4.7692 -4.7688 -4.7317176 -5.4345522 -5.7474771 -6.8647118 -8.0259876 -8.5366735 -9.345829][-2.84228 -3.1444588 -3.6223502 -4.0281296 -4.2914381 -4.2622728 -4.0228066 -4.1491995 -4.1072569 -4.9339585 -5.8613162 -7.2227116 -8.3777924 -8.9211254 -9.64109][-3.8296075 -4.0522327 -4.2398906 -4.3186736 -4.2242584 -3.9681675 -3.6756196 -3.5056133 -3.1339345 -3.905123 -4.3749275 -5.4595761 -6.4949045 -7.1096535 -7.8525271][-4.9997759 -4.6231103 -4.702848 -4.0908551 -3.2630024 -2.5404234 -1.9034534 -1.7168174 -1.4284091 -2.0776863 -2.6410413 -4.205164 -5.306797 -5.6559429 -6.2802448][-5.2486353 -4.8936157 -4.1603556 -3.2764139 -2.36726 -0.99411631 -0.11617947 0.22190905 0.61311436 -0.36060238 -1.0807061 -2.6476951 -4.247036 -5.0148735 -5.7561712][-5.0455647 -4.6723018 -3.9514663 -2.7841887 -1.3619084 0.275702 1.7859144 2.2901983 2.4728079 1.0967312 -0.26300764 -2.1005645 -3.4270291 -4.2483926 -4.8046503][-5.412756 -4.7636695 -3.8875711 -2.2431769 -0.71156693 1.1595078 2.9030304 3.3972921 3.8007574 2.1060114 0.33336926 -1.985991 -4.0557451 -4.7767887 -5.3836832][-5.1458206 -4.262711 -3.3274794 -1.8688731 -0.16938162 1.9373627 3.151679 3.6734304 3.9678879 2.3104267 0.77157307 -1.9897914 -4.049952 -5.2650757 -5.8653049][-4.308805 -3.8872044 -3.3132024 -1.837009 -0.53277349 0.91922569 1.9478197 2.7318096 3.1474018 1.4019423 -0.12860727 -2.2062511 -4.0632067 -5.50084 -6.4124441][-4.9810076 -4.5415993 -4.2638268 -3.1318927 -2.1691561 -0.91447639 0.03134203 0.76375103 1.1198368 -0.72900772 -1.7780252 -3.9443445 -5.1236129 -5.9461966 -6.866147][-6.6870041 -6.5666504 -6.2909427 -5.3525457 -4.4110622 -3.2589288 -2.3741055 -1.8313565 -1.5117888 -2.9358354 -4.0849504 -5.8870034 -6.8983235 -7.4605608 -7.7817712][-7.2726026 -7.0875959 -6.8183794 -6.5127497 -5.980814 -5.2215195 -4.6663256 -4.1693859 -3.6868124 -4.7768269 -5.547205 -6.45307 -7.0163021 -7.4361033 -7.860394][-7.6227541 -7.7245665 -7.788476 -7.3713765 -7.2019506 -6.852809 -6.607038 -6.4359031 -6.3147483 -6.90874 -7.3697877 -7.7046108 -7.8292375 -7.6205778 -7.5600038][-7.0481124 -7.2956829 -7.3601432 -7.2511749 -7.0289106 -6.5427713 -6.4860315 -6.6631503 -6.7061386 -7.2081466 -7.6790547 -7.6063504 -7.6200619 -7.3647738 -7.0210309][-8.5851946 -8.4322529 -8.3346529 -8.118227 -7.8386717 -7.6551247 -7.9325132 -7.8988833 -8.017909 -8.0915537 -8.1243811 -8.061614 -7.5738611 -7.1006718 -6.8538527]]...]
INFO - root - 2017-12-15 18:14:23.978359: step 44410, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 51h:47m:53s remains)
INFO - root - 2017-12-15 18:14:30.366683: step 44420, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:38m:06s remains)
INFO - root - 2017-12-15 18:14:36.748535: step 44430, loss = 0.28, batch loss = 0.16 (11.8 examples/sec; 0.676 sec/batch; 54h:05m:47s remains)
INFO - root - 2017-12-15 18:14:43.229965: step 44440, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:40m:59s remains)
INFO - root - 2017-12-15 18:14:49.657024: step 44450, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 53h:03m:59s remains)
INFO - root - 2017-12-15 18:14:56.168650: step 44460, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 52h:51m:49s remains)
INFO - root - 2017-12-15 18:15:02.589187: step 44470, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 52h:26m:37s remains)
INFO - root - 2017-12-15 18:15:09.021527: step 44480, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 50h:51m:24s remains)
INFO - root - 2017-12-15 18:15:15.429101: step 44490, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 50h:39m:47s remains)
INFO - root - 2017-12-15 18:15:21.753715: step 44500, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 51h:09m:44s remains)
2017-12-15 18:15:22.258215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8740892 -2.6490116 -2.5665264 -2.5323348 -2.6580071 -2.7992525 -2.6257925 -2.29743 -1.9650383 -3.0374804 -3.7569573 -5.1661053 -5.9593539 -6.8700991 -7.4996662][-2.1952763 -2.2876072 -2.6344666 -2.9387374 -3.1296687 -2.9241614 -2.7190471 -2.4197226 -2.1199222 -3.1721959 -3.7297583 -5.0397396 -6.0399165 -7.1235094 -7.5940223][-2.7397556 -2.6308336 -2.7429738 -2.9075575 -3.0255246 -2.7892609 -2.618804 -2.4260144 -2.1209197 -3.262342 -3.9731035 -5.2024107 -5.89849 -6.7554398 -7.2194977][-2.9728918 -2.7445927 -2.8110323 -2.7000537 -2.483284 -2.1389413 -1.8425174 -1.8211641 -1.6482644 -2.823247 -3.3794494 -4.6672277 -5.523427 -6.3879871 -6.8621974][-3.3085151 -2.6049013 -2.1347442 -1.8756728 -1.5637469 -1.0748868 -0.67170763 -0.595994 -0.40061235 -1.8704147 -2.7063947 -4.0910015 -5.0268545 -5.9405179 -6.3613529][-3.7034729 -3.1598058 -2.6876144 -1.8843007 -0.94479179 -0.17030954 0.21128798 0.24841309 0.29623413 -1.2660594 -2.1585336 -3.6797943 -4.6390028 -5.7227731 -6.1981273][-4.266273 -3.4717827 -2.6897349 -1.5963144 -0.62116051 0.43203259 0.9571209 1.024353 1.0516529 -0.83994675 -2.0902781 -3.8427627 -4.8241272 -5.841187 -6.1401768][-4.4680552 -3.4427395 -2.4937801 -1.3312879 -0.13698339 0.79608154 1.3098278 1.5844383 1.6633797 -0.20497131 -1.5354061 -3.6396279 -4.9147754 -6.0363846 -6.1794128][-4.2982292 -3.5077858 -2.5578465 -1.3737636 -0.22478724 0.45123005 0.9872036 1.319273 1.3944798 -0.53104973 -1.8348145 -3.7214022 -5.0106268 -6.3176136 -6.601759][-4.8009405 -4.0298929 -3.130425 -1.995811 -0.86034107 -0.10773945 0.3138628 0.66234493 0.74555779 -1.2234545 -2.3583465 -4.1778641 -5.4250479 -6.6314297 -7.0397859][-5.9261179 -5.153553 -4.388267 -3.3557949 -2.5278606 -1.9912634 -1.5001826 -1.15692 -1.065248 -2.8624678 -4.1312261 -5.5161295 -6.3086638 -7.2014613 -7.2493072][-6.4137626 -6.02846 -5.352107 -4.4063349 -3.7086644 -3.2731128 -3.0714617 -2.9403691 -2.8758855 -4.012814 -4.718585 -5.7604609 -6.3051691 -6.9329662 -6.9583774][-6.85539 -6.3282475 -5.720366 -5.1148648 -4.6131258 -4.3838391 -4.3167791 -4.3265619 -4.4180293 -5.0373096 -5.3593144 -5.754106 -6.0241718 -6.3391995 -6.1178131][-6.9424572 -6.5472264 -6.1527638 -5.5308418 -5.2736526 -5.0802 -5.0088081 -4.937211 -4.8167005 -5.3377504 -5.6286268 -5.5813341 -5.5992556 -5.8664074 -5.7549973][-7.2944727 -7.1318965 -6.6011066 -6.2936115 -6.3618827 -6.5048008 -6.653522 -6.7992 -6.91604 -6.7754822 -6.5428782 -6.4336953 -6.1994739 -6.0703444 -5.8924084]]...]
INFO - root - 2017-12-15 18:15:28.648308: step 44510, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.669 sec/batch; 53h:29m:17s remains)
INFO - root - 2017-12-15 18:15:34.935033: step 44520, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 50h:10m:45s remains)
INFO - root - 2017-12-15 18:15:41.293456: step 44530, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 49h:38m:19s remains)
INFO - root - 2017-12-15 18:15:47.653953: step 44540, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 51h:34m:01s remains)
INFO - root - 2017-12-15 18:15:54.071184: step 44550, loss = 0.28, batch loss = 0.16 (13.1 examples/sec; 0.612 sec/batch; 48h:58m:05s remains)
INFO - root - 2017-12-15 18:16:00.420844: step 44560, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 51h:19m:03s remains)
INFO - root - 2017-12-15 18:16:06.787960: step 44570, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.643 sec/batch; 51h:23m:16s remains)
INFO - root - 2017-12-15 18:16:13.102457: step 44580, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 50h:36m:22s remains)
INFO - root - 2017-12-15 18:16:19.514093: step 44590, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 50h:32m:27s remains)
INFO - root - 2017-12-15 18:16:25.868853: step 44600, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:37m:27s remains)
2017-12-15 18:16:26.389951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7385039 -5.9521213 -6.4655805 -6.3232455 -5.5642223 -4.2014542 -3.1482072 -1.6171274 -0.21021605 -0.58252048 -1.4622703 -4.2319875 -6.2223358 -8.4251881 -9.7744446][-4.77343 -5.4956584 -6.2730575 -6.5339346 -6.2569928 -5.2887535 -3.6563268 -1.9071355 -0.44780159 -0.60652351 -1.8132081 -3.7411332 -5.9016876 -7.9715242 -9.2847815][-4.5810108 -5.1542797 -5.4437881 -5.1552577 -4.44436 -3.8088906 -2.8551407 -1.6437693 -0.30101633 -0.9377923 -1.7793965 -3.7349219 -5.1807389 -6.94244 -8.2895794][-4.6514492 -4.4174819 -4.1598015 -3.5667586 -2.9257355 -2.1306281 -1.2837276 -0.67489719 0.011541367 -0.77286053 -1.6141071 -3.5021157 -4.9762821 -6.4472241 -7.0852118][-4.820857 -4.1546164 -3.2801027 -2.4308085 -1.7751231 -0.67053223 0.44549179 1.0723019 1.356638 0.39794159 -0.75367975 -3.0732808 -4.8362675 -6.33887 -7.2638869][-4.6213331 -3.5967917 -2.2794614 -1.3051071 -0.10603714 1.1646109 2.4207592 2.8565702 3.2131186 2.0103245 0.36423492 -2.310328 -4.3273721 -6.070497 -7.1914372][-3.9193318 -2.562613 -1.4109445 -0.34560633 0.73593044 1.7525206 2.9886913 3.6040373 4.0116167 2.7112045 1.0851049 -1.9712224 -4.0358515 -5.9883862 -7.0455647][-3.1297684 -1.7414355 -0.36653471 0.47420025 1.3887339 1.9833536 2.5459585 3.3099833 3.9218855 2.7735043 0.9730072 -1.9191451 -4.1732163 -6.1162515 -7.0514059][-2.2137761 -1.11446 -0.044071198 1.0404892 2.0886669 2.19314 2.4222574 2.7343569 2.9921246 1.7332706 0.28546286 -2.169147 -4.2159147 -6.2464957 -7.6381211][-1.856586 -1.1085382 0.19876671 1.0758057 1.8176632 2.1608238 2.6563721 2.5483694 2.6617374 0.782671 -1.0323572 -3.2350497 -4.6589842 -6.2167864 -7.4098291][-1.4363317 -1.0254636 -0.21517849 0.65839767 1.3323784 1.2723894 1.5912218 1.7313404 1.6481657 0.0933218 -1.7894402 -3.9443238 -5.0336442 -6.4306378 -7.3976364][-2.3527694 -1.7009706 -0.91638851 -0.29872465 0.23719931 0.094954491 0.065487862 -0.032141209 -0.41258478 -1.1167765 -2.380579 -3.9907434 -5.4526272 -6.883028 -7.7049317][-3.998647 -3.5554013 -2.5943036 -1.813035 -0.91303158 -0.87281132 -0.97363377 -1.3434343 -1.5810595 -2.3975182 -3.0935359 -4.118721 -5.3848815 -6.4642334 -7.1794767][-5.8933458 -5.7685995 -4.7319512 -3.8728282 -2.9388719 -2.3292418 -1.7615457 -1.7947087 -2.223238 -3.0155511 -4.0600328 -4.9568624 -5.5998721 -6.2920952 -6.3662276][-6.920043 -6.64363 -5.9893332 -5.8985887 -4.9888477 -4.2766237 -3.7170434 -3.0299606 -3.5188317 -3.9555056 -4.6992474 -5.7915392 -6.3841295 -6.4795709 -6.3554783]]...]
INFO - root - 2017-12-15 18:16:32.806570: step 44610, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 51h:14m:11s remains)
INFO - root - 2017-12-15 18:16:39.122451: step 44620, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 51h:38m:34s remains)
INFO - root - 2017-12-15 18:16:45.626020: step 44630, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 51h:35m:23s remains)
INFO - root - 2017-12-15 18:16:52.039815: step 44640, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 50h:53m:59s remains)
INFO - root - 2017-12-15 18:16:58.428044: step 44650, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 51h:00m:24s remains)
INFO - root - 2017-12-15 18:17:04.828448: step 44660, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 49h:59m:26s remains)
INFO - root - 2017-12-15 18:17:11.218732: step 44670, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 51h:32m:12s remains)
INFO - root - 2017-12-15 18:17:17.579756: step 44680, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 50h:10m:37s remains)
INFO - root - 2017-12-15 18:17:24.015948: step 44690, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 51h:14m:01s remains)
INFO - root - 2017-12-15 18:17:30.414269: step 44700, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 50h:44m:57s remains)
2017-12-15 18:17:30.932495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1154165 -3.55308 -3.8727543 -3.9439552 -4.0413303 -4.0920577 -3.3207626 -2.1462803 -1.4121804 -2.7373805 -4.1791506 -4.8486013 -5.6471395 -7.55808 -7.7460246][-4.4973474 -4.9898214 -5.1780348 -4.9401259 -4.7604365 -4.2123175 -4.3210735 -3.9966798 -2.6735764 -3.3106055 -4.3524742 -5.0240831 -6.2347889 -7.8319039 -7.7475481][-4.7160797 -4.2654986 -4.517755 -4.6901593 -4.3581018 -3.4500322 -2.490612 -2.0929022 -1.7506366 -2.655406 -3.51024 -4.7074251 -6.0137696 -7.5523829 -7.4386005][-5.10639 -4.46012 -4.1620903 -3.5510936 -2.989213 -2.3344269 -1.5700679 -0.90107107 -0.42925549 -1.7829242 -3.2643681 -3.6311331 -4.5174522 -6.4459596 -6.082643][-5.7164822 -4.8278751 -3.5493469 -2.9363604 -2.2209334 -1.1873093 -0.40765 0.053144455 -0.321167 -1.9993114 -3.5121541 -4.3789978 -5.3118162 -6.4288287 -6.0897851][-4.0306964 -3.5326796 -2.7540646 -1.9664207 -1.008347 -0.22928762 0.47289276 1.0270109 0.82105064 -1.4647641 -3.6161442 -5.1208735 -6.8083692 -7.8187962 -7.0530539][-2.4755058 -1.9834089 -1.6359401 -0.65988874 -0.0070719719 0.9123621 1.5959597 1.8264914 2.0114946 -0.50737476 -2.7122622 -4.4525132 -6.5160084 -8.61684 -8.1826658][-2.0123439 -1.3387256 -0.96623707 -0.22923517 0.54091358 1.2304192 1.4867783 2.09412 2.2092161 0.25223398 -2.2048554 -4.5214887 -6.1095991 -8.1736784 -7.8426924][-2.0738964 -1.7070098 -1.4662104 -0.78634357 0.31596756 0.91091919 1.6805458 1.9099236 1.7630854 -0.19263554 -2.3569098 -3.8410068 -5.9587717 -8.0389261 -7.6408114][-3.5101571 -2.7339716 -1.8957715 -1.4284391 -0.86300945 0.11464405 0.93212509 1.4443808 1.753726 -0.34670591 -2.5150967 -4.30726 -6.1418643 -7.3731637 -7.22213][-4.3830285 -4.9196854 -4.7037039 -3.6739497 -3.3724318 -2.515286 -1.7510605 -0.84479952 -0.85228062 -2.3395853 -3.9916036 -5.288475 -6.6012774 -7.7538481 -7.3807631][-6.4039583 -6.3501348 -5.9626474 -5.6335983 -4.8867693 -4.4186864 -4.1734114 -3.5611653 -2.8962231 -3.8814058 -5.0394716 -5.3501019 -6.4501252 -7.137002 -6.9630985][-7.7555227 -7.8787751 -7.9955935 -7.5126324 -6.7239885 -6.0749264 -5.1877985 -5.0254264 -5.1229224 -5.4028897 -6.627389 -6.5556602 -6.1041775 -6.2776022 -6.2467518][-7.3104734 -7.5165195 -7.5827537 -7.3837581 -7.0754709 -6.6773419 -6.665 -6.1128564 -5.5017405 -5.3109875 -5.9046645 -5.9962406 -6.2791605 -6.0788994 -5.4441223][-7.8284707 -7.5005493 -7.4450917 -7.2546716 -6.9612875 -6.8724508 -6.7823677 -6.6734247 -7.1290207 -6.6953993 -6.3664632 -6.2073965 -6.0032692 -5.9683557 -5.8358221]]...]
INFO - root - 2017-12-15 18:17:37.328617: step 44710, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 49h:59m:58s remains)
INFO - root - 2017-12-15 18:17:43.653210: step 44720, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 50h:05m:18s remains)
INFO - root - 2017-12-15 18:17:50.006001: step 44730, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 51h:37m:30s remains)
INFO - root - 2017-12-15 18:17:56.476612: step 44740, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 50h:46m:56s remains)
INFO - root - 2017-12-15 18:18:02.900042: step 44750, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 52h:51m:47s remains)
INFO - root - 2017-12-15 18:18:09.262615: step 44760, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 51h:25m:54s remains)
INFO - root - 2017-12-15 18:18:15.600083: step 44770, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 51h:53m:23s remains)
INFO - root - 2017-12-15 18:18:22.015363: step 44780, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 52h:31m:30s remains)
INFO - root - 2017-12-15 18:18:28.378783: step 44790, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 50h:07m:28s remains)
INFO - root - 2017-12-15 18:18:34.832642: step 44800, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 52h:39m:17s remains)
2017-12-15 18:18:35.383045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0317669 -3.4537482 -3.4915972 -3.4445624 -3.9603844 -4.9817171 -5.3448744 -5.2880664 -5.0967693 -4.593894 -6.0295997 -6.4996738 -7.1716714 -8.0332451 -8.917078][-3.0206828 -2.6629119 -2.1861725 -2.5854735 -3.6596189 -4.5799007 -5.1479034 -5.3390203 -5.2246361 -4.4243164 -5.6662855 -6.1825933 -6.8281889 -7.690341 -7.7563496][-1.9554873 -1.5799322 -2.1056943 -2.3451571 -2.7197237 -3.2320094 -3.3540888 -3.5388889 -3.869256 -3.7975278 -5.2132664 -5.7846656 -6.2975087 -7.2737756 -7.6115613][-2.4170723 -1.8379183 -1.9023933 -1.9096203 -2.2172756 -2.2291775 -1.9646454 -2.1069384 -2.3344688 -2.5699568 -4.5420756 -5.2162104 -6.165082 -7.0719242 -7.0210338][-3.2289004 -2.8303123 -2.0560803 -1.4762421 -1.6230769 -1.2042675 -0.9415226 -1.2604365 -1.2894921 -1.7384276 -3.9009795 -3.9672456 -4.8285866 -6.3641062 -6.4729223][-3.8632221 -3.7535746 -2.9669871 -2.2003741 -1.6424766 -1.1916304 -0.80970573 -0.69248915 -0.72733641 -1.1322107 -3.2204933 -3.2846775 -3.7431021 -4.8979559 -5.5983915][-3.2196589 -2.9076958 -2.0929022 -1.3252568 -0.78703022 -0.38959169 -0.097833157 0.055076122 0.0037794113 -0.66800261 -2.7580261 -2.8698082 -3.8420839 -4.625967 -4.8121724][-2.6426229 -2.5317283 -1.7322698 -0.84082317 -0.3616004 0.24076796 0.68965054 0.56830883 0.28701973 -0.50122166 -2.5356731 -3.0558906 -4.1581955 -4.9692678 -5.1214914][-4.6279092 -3.7258384 -3.2600555 -2.4024277 -1.4974198 -0.80846453 -0.42258644 -0.30552673 -0.19031048 -0.68285179 -2.5922947 -3.3285675 -3.9591084 -4.5736523 -5.5461645][-5.46675 -4.9780059 -4.8934488 -4.1357555 -3.360508 -2.6361437 -2.0770578 -1.9147978 -1.5277686 -1.7851753 -3.3988843 -3.7755685 -4.35701 -4.9042187 -5.6420097][-6.8596134 -6.7371473 -6.4018855 -5.8813915 -5.0647211 -4.6546216 -4.05947 -3.9745176 -3.980124 -4.2003193 -5.1465521 -5.4177895 -5.7979474 -6.2843752 -6.5331049][-6.9023776 -7.4689016 -7.1214323 -6.7195582 -6.0655785 -5.2857609 -4.3186445 -4.4776926 -4.8694997 -5.4312177 -6.639185 -6.8303361 -7.5882683 -7.9612617 -7.8742485][-7.0935588 -7.4566193 -7.1092234 -7.0159092 -6.7277007 -5.9271345 -5.127759 -5.0032368 -4.8055162 -5.1488419 -7.0645967 -6.9711542 -6.7902551 -7.0081048 -7.4081907][-5.613636 -5.7425575 -5.5600758 -5.6868696 -5.4580531 -4.7870197 -4.5596247 -4.5990267 -4.4969268 -4.4343719 -5.34756 -5.9060307 -6.2283392 -5.9819803 -5.6741753][-5.6017027 -5.587678 -5.0745668 -5.1975946 -5.1705308 -4.9138927 -5.1834869 -5.3214397 -5.4230742 -5.5085549 -5.5044742 -5.5710773 -5.9765577 -6.2526608 -6.2383156]]...]
INFO - root - 2017-12-15 18:18:41.792243: step 44810, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 50h:10m:06s remains)
INFO - root - 2017-12-15 18:18:48.220933: step 44820, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 51h:26m:00s remains)
INFO - root - 2017-12-15 18:18:54.582361: step 44830, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:52m:35s remains)
INFO - root - 2017-12-15 18:19:01.043759: step 44840, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 53h:08m:56s remains)
INFO - root - 2017-12-15 18:19:07.436212: step 44850, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 52h:11m:21s remains)
INFO - root - 2017-12-15 18:19:13.755798: step 44860, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 49h:38m:19s remains)
INFO - root - 2017-12-15 18:19:20.122061: step 44870, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 50h:30m:26s remains)
INFO - root - 2017-12-15 18:19:26.523059: step 44880, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:15m:12s remains)
INFO - root - 2017-12-15 18:19:32.937697: step 44890, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 51h:52m:54s remains)
INFO - root - 2017-12-15 18:19:39.398912: step 44900, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 51h:23m:57s remains)
2017-12-15 18:19:39.945969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4184618 -1.97646 -2.1687722 -2.1949091 -2.2074804 -2.290997 -2.4772811 -2.5812578 -2.7999105 -4.0579486 -5.1226635 -5.823698 -6.4285107 -7.4193225 -8.3017693][-2.4021096 -2.2051749 -2.454803 -2.6079092 -2.7673769 -3.0279937 -3.3204451 -3.5249081 -3.6608105 -4.3903532 -4.9754081 -5.7133036 -6.2815361 -7.0429015 -7.9380469][-2.6172595 -2.0603151 -1.8971925 -1.8414116 -1.7038093 -1.7064099 -2.0795183 -2.70961 -2.8493409 -3.3770018 -4.0144138 -5.0244265 -5.7680635 -6.4059896 -7.0728884][-1.2013307 -0.65697956 -0.30309963 -0.66000652 -0.83470154 -0.56688023 -0.5790019 -1.1498971 -1.7830353 -2.9588308 -3.63381 -4.3560266 -4.886724 -5.2011752 -5.7705503][-0.8629756 0.0022249222 0.50303078 0.26111937 -0.11090374 -0.0058817863 0.073042393 -0.28870106 -0.77308512 -1.5082636 -1.8192744 -2.4259787 -2.9744005 -4.0239916 -5.0797997][-1.0836892 -0.45673513 0.22984457 0.67173576 0.69929886 1.241128 1.8195915 1.6454096 1.4611588 0.37913704 -0.15913773 -1.0941339 -2.2041445 -3.5764961 -4.9011679][-1.7243104 -0.779089 0.0029482841 0.57549381 0.85046482 1.2212925 1.694602 1.7547836 1.8313465 0.66593456 -0.08931303 -1.1335049 -2.3244395 -3.7518501 -4.8928385][-2.0583253 -1.1105242 -0.5111599 0.12038803 0.6941309 1.1026535 1.6964331 2.0666847 2.3633022 1.3568335 0.63169479 -0.59450674 -1.9212489 -3.3508978 -4.5381184][-2.8408413 -1.9306359 -1.087501 -0.59988737 -0.20754051 0.37581348 1.0903387 1.6991339 2.2542057 1.0961351 0.27302742 -0.88256025 -2.3090849 -3.8384166 -5.0315156][-4.0143061 -3.3862596 -2.5588675 -1.720459 -1.2970395 -0.67459059 -0.25604296 0.14396143 0.67937851 -0.21553993 -0.95199537 -2.1331987 -3.5265751 -5.0462084 -6.128561][-5.2321596 -5.1471915 -4.7131691 -3.7663095 -3.1294551 -2.5304542 -2.0343676 -1.9352536 -1.8202558 -2.822454 -3.6640611 -4.2856169 -5.0418415 -6.0280313 -6.8650188][-7.2711906 -6.7376041 -6.5204968 -6.0760469 -5.4789939 -4.9031725 -4.4128809 -4.0733566 -3.8207076 -4.7068348 -5.2234077 -5.6355667 -6.0451813 -6.6355338 -7.5151143][-7.4302058 -7.3029184 -7.1693263 -7.0152521 -6.4973564 -5.8870764 -5.5977745 -5.5389023 -5.50311 -5.6175194 -5.7108335 -6.045145 -6.3770518 -6.6774006 -7.3033767][-7.228128 -7.1923923 -7.0445752 -6.9310985 -6.9566631 -6.265357 -5.6954012 -5.5262814 -5.4353943 -5.8894987 -6.1901679 -6.3297734 -6.3169093 -6.2513046 -6.6348515][-7.7381611 -7.6679254 -7.5217628 -7.2925129 -7.2465954 -6.8332272 -6.3519607 -6.327836 -6.2393074 -6.0807767 -6.0424337 -5.9742508 -6.0558019 -6.0583363 -6.1306686]]...]
INFO - root - 2017-12-15 18:19:46.406648: step 44910, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 50h:08m:53s remains)
INFO - root - 2017-12-15 18:19:52.841028: step 44920, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 50h:54m:03s remains)
INFO - root - 2017-12-15 18:19:59.244021: step 44930, loss = 0.25, batch loss = 0.14 (11.8 examples/sec; 0.679 sec/batch; 54h:15m:50s remains)
INFO - root - 2017-12-15 18:20:05.638627: step 44940, loss = 0.35, batch loss = 0.23 (12.3 examples/sec; 0.653 sec/batch; 52h:08m:08s remains)
INFO - root - 2017-12-15 18:20:11.996361: step 44950, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 52h:08m:46s remains)
INFO - root - 2017-12-15 18:20:18.412442: step 44960, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 50h:56m:31s remains)
INFO - root - 2017-12-15 18:20:24.936469: step 44970, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 52h:16m:54s remains)
INFO - root - 2017-12-15 18:20:31.290340: step 44980, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 50h:53m:21s remains)
INFO - root - 2017-12-15 18:20:37.703450: step 44990, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 51h:42m:16s remains)
INFO - root - 2017-12-15 18:20:44.019228: step 45000, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 50h:18m:18s remains)
2017-12-15 18:20:44.516269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8945074 -5.3477092 -5.7877612 -6.0202312 -5.6274986 -4.9152822 -4.0530548 -3.2898474 -3.3027363 -4.852807 -6.2745767 -8.6112 -9.4150543 -9.2646694 -10.252535][-5.2250295 -4.8182535 -4.7869072 -4.9394913 -5.4063272 -5.301012 -4.4738808 -3.8257906 -3.6017828 -5.3276987 -6.412055 -9.2772207 -10.958672 -10.775612 -11.398293][-3.8699121 -3.3104382 -3.6676207 -4.1716819 -3.4716482 -3.0237322 -2.8957944 -2.6430588 -2.5926843 -4.3769526 -5.8728652 -8.5878735 -9.8933125 -9.7896872 -10.372481][-3.1104107 -2.8554239 -2.9023986 -2.5275722 -2.3315225 -1.9266 -0.8561759 -0.56773329 -0.61157036 -2.0897088 -3.708544 -6.9610624 -8.6409454 -9.0242939 -9.9963169][-2.8678927 -1.8877296 -1.2593403 -0.92522764 -0.5945096 -0.16208839 0.84315395 1.5210829 1.7142334 -0.12820578 -1.863575 -5.1297617 -7.3269315 -7.9025912 -9.2798138][-2.7406693 -2.63375 -2.4504142 -0.94514322 0.34424114 0.99206638 2.1610241 2.5927811 2.9736376 1.3698368 -0.34171391 -3.7283192 -6.0177965 -6.7278833 -8.1896725][-3.7842038 -2.9798961 -2.1635199 -0.91301107 0.58766365 1.91928 3.2844429 4.0566978 4.5801849 2.7900009 0.80688572 -2.9337945 -5.2714748 -6.150929 -7.8645754][-3.4038486 -2.8570418 -1.9614549 -0.3990202 1.1703959 2.678236 3.9864388 4.5480461 4.59593 2.7966604 0.97353077 -2.6094522 -5.0475883 -5.8067589 -7.1942263][-4.0890083 -3.3861542 -2.5550261 -1.0246992 0.41072941 1.941659 2.9579849 3.2682514 3.4475155 1.1923237 -0.5330224 -3.4012003 -5.2664924 -5.8115387 -7.2406373][-5.3534422 -4.5058 -3.610497 -2.2477851 -0.92712069 0.27628708 1.0955229 1.0888243 1.0805807 -0.8322835 -2.2274714 -4.942153 -6.3192167 -5.942596 -7.0876975][-5.4713478 -5.4858313 -4.8230543 -3.7628462 -3.1978879 -1.9772277 -1.1735721 -1.3604355 -1.5337157 -3.2130237 -4.4727368 -6.518681 -7.3462367 -6.7620625 -7.200707][-6.2225094 -5.6488466 -5.2851353 -4.5507565 -4.1461487 -3.7502007 -3.6052771 -3.4409256 -3.3702478 -4.5065985 -5.2437611 -6.6859813 -7.211297 -6.6985946 -6.924911][-7.103466 -6.641459 -6.3903556 -5.9188366 -5.7425637 -5.3285255 -4.9235468 -5.2391729 -5.1597891 -5.5545831 -5.7330813 -6.5407071 -6.7134662 -6.0260205 -6.1499548][-6.6411128 -6.180687 -5.5926485 -5.2478237 -5.2572365 -5.3458457 -5.3779712 -5.16706 -4.9135256 -5.2059922 -5.3441534 -5.917057 -6.042397 -5.5134516 -5.5702972][-6.3873549 -6.678762 -6.6386175 -6.0551858 -5.8729486 -5.9582429 -5.9978852 -6.1662593 -6.04237 -5.8097191 -5.749454 -5.9501381 -6.1037178 -6.0846272 -6.017313]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 18:20:51.749764: step 45010, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 50h:09m:45s remains)
INFO - root - 2017-12-15 18:20:58.087112: step 45020, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 50h:04m:48s remains)
INFO - root - 2017-12-15 18:21:04.436337: step 45030, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 52h:07m:54s remains)
INFO - root - 2017-12-15 18:21:10.752500: step 45040, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 50h:46m:53s remains)
INFO - root - 2017-12-15 18:21:17.138190: step 45050, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 51h:48m:02s remains)
INFO - root - 2017-12-15 18:21:23.499497: step 45060, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:50m:48s remains)
INFO - root - 2017-12-15 18:21:29.899990: step 45070, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 51h:37m:54s remains)
INFO - root - 2017-12-15 18:21:36.266992: step 45080, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 50h:00m:04s remains)
INFO - root - 2017-12-15 18:21:42.592182: step 45090, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:41m:09s remains)
INFO - root - 2017-12-15 18:21:48.978762: step 45100, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 53h:54m:46s remains)
2017-12-15 18:21:49.486739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7470665 -4.9043207 -4.2969322 -3.9107666 -3.0316243 -2.525465 -2.0276365 -1.3590517 -0.64123726 -2.4902 -3.3887725 -5.1218309 -6.9571919 -8.3660183 -10.041065][-6.3884916 -6.2412481 -5.95134 -6.0885754 -5.7761488 -5.5090489 -4.2900896 -3.8579786 -3.3062124 -4.2202387 -4.6064548 -6.1874132 -7.4317813 -8.4936409 -10.369679][-6.3313532 -5.1487932 -4.808403 -5.1125984 -5.1985765 -4.8891788 -4.3951731 -4.4937859 -3.9497979 -5.2525644 -5.6220589 -6.5968647 -7.5084915 -8.4706221 -9.9126978][-6.7897892 -5.5264974 -4.5224295 -3.6336884 -3.1656094 -2.9633021 -2.321629 -1.9445558 -2.0184016 -4.0211115 -5.1135635 -6.7575526 -7.8875151 -8.3439865 -9.3271122][-5.8197536 -4.2727504 -3.0551963 -1.8361669 -0.73729753 -0.19633341 0.52654171 0.12005663 -0.45223093 -2.275794 -3.4230804 -5.3294086 -6.8591213 -7.365346 -8.4746761][-4.4391747 -3.7490687 -3.2043095 -1.7856417 -0.49337626 0.64669895 1.4120321 1.2892904 1.039896 -0.97408247 -1.9580216 -3.365428 -4.3361397 -5.1076493 -6.3971238][-4.344779 -3.4744992 -2.3054953 -1.0227284 0.35409451 1.4637232 2.1267633 2.248435 2.2550077 0.51513577 -0.34582615 -2.2584262 -3.6590166 -4.2994947 -5.48505][-4.3054447 -2.6176124 -1.2063661 0.5548296 1.813612 2.535656 2.9192123 2.6014156 2.5260973 0.60138035 -0.37385607 -2.0632625 -3.2288485 -4.469389 -5.5176382][-1.6216431 -1.2936797 -0.72575378 0.011724949 0.64831161 1.3689222 1.3785124 1.5599508 1.7964878 0.32480526 -0.40218735 -2.3263869 -3.7664936 -4.2953081 -5.0400262][-0.74449396 -0.5019722 -0.68683863 -1.2162976 -0.77101326 -0.16139555 0.28252649 0.46460915 0.71810246 -0.66483259 -0.9989171 -2.6352911 -3.8013055 -4.7578316 -5.472971][-1.5877318 -1.3405828 -1.1139708 -1.2751107 -0.98773623 -0.61778021 -0.53833866 -0.50322771 -0.80168533 -2.341743 -3.0073733 -4.4075365 -5.5571465 -6.2352419 -6.624835][-2.1672997 -1.8054256 -1.396421 -1.3082528 -1.3074865 -1.4547696 -1.4401021 -1.8565803 -2.4948435 -3.7877746 -4.2841434 -5.673831 -6.791214 -7.1185575 -7.239934][-2.9288297 -2.0333972 -1.9613647 -2.7789254 -3.5811105 -3.7209003 -3.6488185 -3.9666059 -4.3086767 -5.6295977 -6.2062554 -6.9347658 -7.8899469 -7.9299517 -7.5929289][-3.2456255 -2.6778445 -2.2572031 -2.2317548 -2.7202845 -3.3095593 -3.6012897 -4.3043962 -5.0829425 -6.4942679 -6.8546519 -7.3443971 -8.0723553 -8.14791 -7.9967256][-5.1074781 -4.4890404 -4.1481357 -3.8942392 -4.2878075 -4.618166 -4.8228235 -5.4217205 -5.9171152 -6.6281443 -7.0173836 -7.4551935 -8.1024847 -8.1422958 -7.98647]]...]
INFO - root - 2017-12-15 18:21:55.938679: step 45110, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 51h:19m:14s remains)
INFO - root - 2017-12-15 18:22:02.241178: step 45120, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 50h:23m:43s remains)
INFO - root - 2017-12-15 18:22:08.554660: step 45130, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 50h:29m:49s remains)
INFO - root - 2017-12-15 18:22:14.977836: step 45140, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 51h:05m:16s remains)
INFO - root - 2017-12-15 18:22:21.361533: step 45150, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:48m:40s remains)
INFO - root - 2017-12-15 18:22:27.785963: step 45160, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.627 sec/batch; 50h:04m:57s remains)
INFO - root - 2017-12-15 18:22:34.210922: step 45170, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 49h:57m:29s remains)
INFO - root - 2017-12-15 18:22:40.528537: step 45180, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 50h:53m:31s remains)
INFO - root - 2017-12-15 18:22:46.820458: step 45190, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 50h:57m:03s remains)
INFO - root - 2017-12-15 18:22:53.169823: step 45200, loss = 0.40, batch loss = 0.29 (12.6 examples/sec; 0.637 sec/batch; 50h:52m:02s remains)
2017-12-15 18:22:53.723626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5562305 -3.8391025 -3.66534 -4.20969 -4.3874321 -4.5444741 -4.3308182 -4.0633035 -3.9674981 -5.3195066 -5.9548922 -6.5891962 -6.9139843 -7.4754992 -8.1718559][-3.8923364 -3.9618111 -4.5325875 -5.0835695 -5.3811636 -5.320116 -5.3125696 -4.8566351 -4.5012794 -5.4703655 -6.095027 -6.7706947 -7.3639197 -7.1993418 -7.4627934][-2.5132928 -3.1122355 -3.6190348 -4.2038679 -4.9039555 -5.3204584 -4.96973 -4.1990395 -3.8523731 -4.8491421 -5.327322 -6.2066946 -6.674356 -6.7204604 -6.976212][-1.4412723 -1.6666808 -2.6494069 -3.1869149 -3.3713617 -2.9208388 -2.1551423 -1.7010036 -1.4569564 -2.6264806 -3.4554362 -4.7397 -5.4573135 -5.4360719 -6.2523408][-1.5324178 -1.4009333 -1.8064432 -2.0057855 -2.0446954 -1.4175596 -0.59611225 0.038370132 0.60715771 -0.78257704 -1.9973035 -3.8338597 -4.872931 -5.4362307 -6.0661259][-2.1890998 -1.9243751 -1.9315958 -1.3029208 -0.68963671 0.33415127 1.425601 2.0650473 2.560154 0.88813972 -0.86990404 -2.7332549 -3.9085388 -4.5987287 -5.5933404][-3.3422012 -2.5904841 -2.2155981 -1.1600204 0.18336916 1.6332769 2.8042927 3.3141775 3.723196 1.514945 -0.55758142 -2.7896037 -4.40762 -5.1645403 -6.1594834][-4.17883 -3.2789826 -2.5726371 -1.2402401 0.41178894 2.1609993 3.6921616 4.1484871 4.3711357 2.0473166 -0.012016773 -2.3720407 -4.0841141 -4.9193439 -5.9823112][-4.4256654 -3.5735731 -2.949666 -1.4887729 -0.0098896027 1.5649033 3.0612602 3.5043821 3.7017241 1.4545326 -0.27287769 -2.4361444 -4.0280128 -4.9875345 -5.93396][-4.8635297 -4.2167492 -3.3344054 -2.2778258 -1.1807041 -0.098984241 1.0582027 1.4871302 1.6513395 -0.47165298 -2.1313505 -3.7720118 -4.6018038 -5.16356 -6.3869328][-6.33758 -5.5144863 -4.9428759 -4.1328955 -3.5335526 -2.5627499 -1.365253 -1.5335345 -1.62957 -3.2477584 -4.47904 -5.4476247 -6.0114031 -6.159265 -6.7420039][-6.3694615 -5.7847271 -5.8141513 -5.6572108 -5.3682938 -4.9643955 -4.4170618 -4.1843338 -3.9583776 -5.3575077 -6.07684 -6.5798812 -6.8911324 -6.9300942 -7.3034048][-6.7897773 -6.540791 -6.4079542 -6.4526834 -6.57013 -6.2808504 -5.6220269 -5.8805194 -5.9408803 -6.5955644 -7.0363688 -7.0208988 -6.744997 -6.4194775 -6.5408545][-5.5869107 -5.4219079 -5.2805343 -5.1646528 -5.2862225 -5.4197192 -5.2177086 -5.2797484 -5.4940982 -6.179987 -6.1211424 -6.444869 -6.6522675 -6.3217545 -5.8635492][-5.9019547 -5.6065392 -5.4926376 -5.6626844 -5.9239645 -6.1808586 -6.2678833 -6.578074 -6.5978909 -6.7924628 -6.9242153 -7.059411 -6.9595237 -6.81975 -6.83144]]...]
INFO - root - 2017-12-15 18:23:00.176598: step 45210, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 49h:55m:19s remains)
INFO - root - 2017-12-15 18:23:06.498255: step 45220, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 49h:57m:26s remains)
INFO - root - 2017-12-15 18:23:12.927872: step 45230, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 50h:05m:35s remains)
INFO - root - 2017-12-15 18:23:19.356623: step 45240, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 51h:49m:24s remains)
INFO - root - 2017-12-15 18:23:25.850064: step 45250, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 49h:57m:12s remains)
INFO - root - 2017-12-15 18:23:32.265579: step 45260, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 50h:42m:39s remains)
INFO - root - 2017-12-15 18:23:38.664373: step 45270, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 51h:26m:48s remains)
INFO - root - 2017-12-15 18:23:45.148642: step 45280, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 51h:20m:11s remains)
INFO - root - 2017-12-15 18:23:51.621435: step 45290, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 50h:25m:19s remains)
INFO - root - 2017-12-15 18:23:57.960409: step 45300, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 51h:24m:39s remains)
2017-12-15 18:23:58.469016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9280944 -6.6399231 -6.6679664 -6.1219721 -4.5851588 -3.0118704 -2.292418 -1.8653579 -1.4096808 -2.3483152 -3.3280067 -4.4542036 -5.4338465 -6.2473564 -6.67386][-5.6921158 -6.2405758 -7.1074429 -7.0948467 -6.2292109 -4.2796116 -2.3013349 -1.9131546 -1.8458247 -3.416337 -4.732235 -6.289515 -6.8461924 -6.9816117 -7.3091288][-5.2616148 -6.0762272 -6.2187586 -6.0314894 -6.0473347 -5.0764837 -3.6645212 -2.8414607 -2.3596954 -3.9083083 -5.2501249 -6.8222742 -7.79129 -8.0265274 -8.0077868][-4.586504 -5.8543482 -6.530632 -5.9019389 -4.5072203 -3.8342028 -3.1016459 -2.3602171 -2.0006943 -3.308641 -4.7703404 -6.6782808 -7.5566988 -7.6740413 -8.5431576][-6.0223293 -6.5404677 -6.8372889 -5.9726615 -4.1837282 -2.3132091 -1.1520495 -0.91714334 -0.93879128 -2.626976 -4.137897 -6.5431747 -7.8121581 -8.4984388 -8.8226967][-8.1320906 -8.3513079 -7.4730616 -5.5394254 -3.1748204 -1.2129922 0.64319038 1.3180389 1.7322464 -0.023066998 -2.4990721 -5.3498297 -7.0810323 -8.5208464 -8.96573][-7.8375649 -7.2691517 -6.2806339 -4.8433475 -2.4169707 0.43936062 2.66963 3.1029816 3.2815266 2.21838 0.0794878 -3.2250605 -5.7201219 -7.6131849 -8.503088][-6.7553439 -6.9823661 -5.8985982 -3.4241166 -1.1301131 1.2839231 3.1044369 4.8829651 6.0240173 4.25403 1.7392607 -1.8007822 -4.5537882 -6.5136166 -7.4405618][-6.3789062 -5.339077 -5.121089 -3.9742806 -1.9340611 0.8519392 2.8498907 3.3601723 4.30033 3.7771206 2.7176161 -0.523088 -3.6838403 -5.6667442 -6.69268][-7.2918043 -5.8080549 -4.950532 -4.1958275 -3.7620525 -1.6778121 0.91043091 2.7721319 2.9792376 1.0046577 -0.21218204 -1.7305079 -3.4049425 -5.1322007 -5.7637639][-5.7665997 -5.9146886 -5.4912157 -4.4171743 -3.6709352 -3.0643978 -2.218996 -0.17088747 1.344573 -0.43033457 -2.9921865 -4.4748707 -5.24887 -5.8586411 -5.8971214][-4.9620013 -4.9918413 -4.7853966 -4.1089125 -2.9683595 -1.6534262 -0.33398914 0.32747936 0.085862637 -1.4091964 -3.0810156 -4.7659426 -5.8056622 -6.1444979 -5.8243685][-6.2708168 -5.3428521 -4.6225977 -3.666358 -2.6003022 -1.71 -0.21511412 0.49068356 0.45143127 -2.4341922 -3.566751 -3.8853121 -4.2572727 -5.41224 -6.132833][-6.3442812 -6.1270337 -5.3309622 -3.7194114 -3.2504649 -2.707088 -1.8153262 -1.4779367 -1.1805582 -2.4633431 -3.4541221 -4.1330204 -3.7483518 -4.188107 -4.4805775][-6.41399 -5.6060171 -5.7562237 -5.000967 -3.6975367 -2.9624786 -2.944243 -3.5485687 -3.647367 -4.056499 -4.0866795 -3.9419281 -3.8001685 -4.5351067 -5.0461874]]...]
INFO - root - 2017-12-15 18:24:04.868151: step 45310, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 52h:57m:20s remains)
INFO - root - 2017-12-15 18:24:11.293236: step 45320, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.618 sec/batch; 49h:18m:12s remains)
INFO - root - 2017-12-15 18:24:17.638601: step 45330, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 49h:46m:02s remains)
INFO - root - 2017-12-15 18:24:24.011115: step 45340, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 51h:51m:19s remains)
INFO - root - 2017-12-15 18:24:30.364386: step 45350, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 51h:19m:04s remains)
INFO - root - 2017-12-15 18:24:36.830747: step 45360, loss = 0.31, batch loss = 0.19 (11.7 examples/sec; 0.684 sec/batch; 54h:32m:18s remains)
INFO - root - 2017-12-15 18:24:43.202588: step 45370, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 51h:24m:22s remains)
INFO - root - 2017-12-15 18:24:49.581434: step 45380, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 50h:02m:32s remains)
INFO - root - 2017-12-15 18:24:56.015466: step 45390, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 50h:46m:15s remains)
INFO - root - 2017-12-15 18:25:02.413507: step 45400, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 50h:57m:47s remains)
2017-12-15 18:25:02.960622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5313044 -4.82405 -4.9457216 -5.1496525 -5.2177439 -4.88442 -4.397954 -3.4370975 -2.72504 -4.1164217 -5.0841851 -5.5655527 -6.3964252 -7.4188704 -8.5927811][-3.9939842 -4.128583 -4.5903087 -5.4163733 -6.038619 -5.7630286 -4.8969245 -3.639256 -2.8360162 -4.3183718 -5.0960913 -6.1504025 -7.4132929 -8.5128012 -9.273983][-3.0929184 -3.4328408 -3.0860124 -3.5214896 -4.1380625 -3.867877 -3.3610082 -2.9449387 -2.3284912 -3.48883 -4.2376251 -4.7214303 -5.6953773 -6.9124875 -7.9577703][-1.7889581 -2.2094202 -2.2478595 -2.0234065 -1.8565993 -1.3645353 -0.85113859 -0.15117359 0.32444859 -1.8250117 -2.6301374 -3.5625534 -5.0688076 -6.3415103 -7.65897][-2.2411299 -1.3132591 -0.54961538 -0.37543821 0.18486691 0.75540352 1.2678747 1.63908 1.5397129 -0.31799364 -1.3575296 -2.392087 -3.5814056 -4.8067045 -6.2000489][-1.9721589 -1.5880847 -0.87163258 -0.015248775 0.92263412 2.0890636 2.8748264 3.2246923 3.06849 0.64318562 -0.9807272 -2.451879 -3.9961808 -4.8111277 -5.4321527][-2.5978527 -2.3851347 -1.3690705 -0.013187408 1.6803331 3.101902 4.1761827 4.1354437 3.9464035 1.3867006 -0.41830111 -2.16612 -4.0483685 -5.0767612 -6.2630429][-3.1216059 -2.7786541 -1.650322 0.11857033 2.0205698 3.2937555 4.1825361 4.1884546 3.8791656 1.0199232 -0.77985907 -2.8471613 -4.5879116 -5.9070024 -7.1018991][-3.9771316 -3.1219058 -2.1614208 -0.58615017 1.1189346 2.1327486 3.0543299 3.5249662 3.6921387 0.864872 -1.5493016 -3.1086965 -4.7552152 -6.2617111 -7.5947371][-5.3086829 -4.3780246 -3.383256 -1.8213954 0.034924507 0.96917725 1.5164022 1.8063431 2.0172129 -0.6605916 -2.5549793 -4.3436327 -5.557519 -6.4571538 -7.6935124][-6.0291605 -5.2140689 -4.4782724 -3.4049959 -2.3824024 -0.97286606 -0.66312408 -0.70463943 -0.399621 -2.7881045 -4.6231184 -5.6098042 -6.4821539 -7.405314 -8.3494453][-5.8896685 -5.1357946 -4.4373965 -3.9709394 -3.4439745 -2.8436465 -2.5895967 -2.4563465 -2.5591502 -3.9965913 -5.0699244 -6.0560069 -6.6626148 -7.3107252 -8.3854218][-6.7546916 -5.8578587 -4.79632 -4.460463 -4.0130191 -4.0817208 -3.7015517 -4.1768889 -4.484828 -5.6632929 -6.542994 -6.8275828 -7.3221855 -7.6409249 -8.1500111][-6.172749 -5.6802034 -5.2053089 -4.7176671 -4.5889482 -4.80163 -4.9412546 -5.2141218 -5.4863338 -6.5244727 -6.6222553 -7.0422378 -7.0188818 -7.27351 -7.5606337][-6.9882526 -6.5852885 -6.1861153 -6.0726986 -6.1151371 -6.3582659 -6.5743217 -7.0450296 -7.3642898 -7.3701344 -7.3636141 -7.326622 -7.3003082 -7.1659937 -7.06452]]...]
INFO - root - 2017-12-15 18:25:09.299086: step 45410, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 51h:24m:40s remains)
INFO - root - 2017-12-15 18:25:15.659168: step 45420, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:38m:15s remains)
INFO - root - 2017-12-15 18:25:22.034324: step 45430, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 52h:32m:05s remains)
INFO - root - 2017-12-15 18:25:28.330905: step 45440, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 50h:13m:58s remains)
INFO - root - 2017-12-15 18:25:34.736547: step 45450, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 52h:08m:14s remains)
INFO - root - 2017-12-15 18:25:41.103365: step 45460, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:26m:01s remains)
INFO - root - 2017-12-15 18:25:47.482675: step 45470, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 50h:34m:15s remains)
INFO - root - 2017-12-15 18:25:53.932526: step 45480, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 52h:28m:54s remains)
INFO - root - 2017-12-15 18:26:00.350773: step 45490, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 51h:42m:18s remains)
INFO - root - 2017-12-15 18:26:06.780599: step 45500, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:28m:33s remains)
2017-12-15 18:26:07.318582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9265933 -4.4605236 -4.568882 -4.0646558 -3.7275386 -3.7208946 -2.9782071 -2.5572557 -2.5907249 -4.1477108 -4.1090512 -4.6278124 -4.8112936 -5.4814835 -6.1345778][-3.3058739 -3.0912824 -3.5111785 -3.7692306 -2.6828346 -2.2094522 -2.6465359 -2.4079332 -1.805171 -3.1591339 -3.545752 -4.3869019 -4.7719765 -4.8156013 -5.3573828][-4.3681555 -4.1708503 -3.6257939 -3.1186004 -2.5179806 -2.2057166 -1.7510767 -1.5581484 -1.2210183 -2.6893592 -3.2608027 -3.9473639 -4.6281223 -4.9447517 -5.0078793][-3.973536 -3.4966908 -3.5538859 -2.3842225 -0.9979291 -0.39207411 -0.18891954 -0.0057587624 0.21707821 -1.4323182 -2.1735735 -3.2976632 -4.08749 -4.7819691 -5.1839647][-2.7169161 -2.3149781 -1.6181307 0.079174042 1.6675186 1.8529253 2.0287905 2.2522907 1.947032 0.21719265 -0.56818771 -2.129704 -3.807296 -4.4074345 -4.66001][-1.7804928 -0.14791536 1.0204229 1.5534573 2.6010075 3.5339413 4.260355 3.7315712 2.5317183 0.87710571 0.28121662 -1.3731794 -3.1143341 -4.137753 -5.082365][0.0067257881 1.2295151 2.2813702 3.1175861 3.2606668 3.9624596 4.8965921 3.9759178 3.2641573 1.013751 -0.48382425 -1.9118662 -3.2732716 -4.2446194 -4.8843846][0.63834381 1.7066689 2.4687576 4.1482143 4.9680595 4.7271338 4.3548708 3.9014874 3.2210569 0.5945425 -0.46549082 -2.2526689 -3.7326615 -4.0616813 -4.4686561][-0.56658792 0.58730888 1.2235994 2.5605993 4.1505079 4.8922987 4.1227493 3.2594662 3.1352415 0.63874626 -0.82401466 -2.7095447 -3.697336 -4.3233395 -4.9838076][-1.5550194 -0.8257575 -0.49644423 0.65195274 1.8508263 2.0787878 2.1051884 2.3523884 1.9437456 -0.51904678 -1.3067393 -3.0409012 -4.4354525 -5.1227274 -5.4678421][-4.3214531 -3.3690162 -2.8635497 -2.0487843 -1.9027524 -1.42979 -0.77842379 -0.73041964 -0.91978025 -2.8348722 -4.3370118 -5.3007469 -5.3780684 -6.0384989 -6.8417411][-6.4205589 -5.674037 -5.2756925 -4.6990623 -4.3038359 -4.1947241 -4.4184809 -3.94214 -3.4969139 -5.1325588 -5.4317937 -5.4900961 -5.9623952 -6.7419105 -7.35863][-8.0802517 -7.27888 -7.2145219 -6.5472965 -5.9375973 -5.9234986 -5.7555976 -5.8784218 -5.7919316 -6.2124529 -6.20352 -6.58167 -6.8666406 -6.8162689 -6.6660695][-7.684238 -7.9045587 -8.670351 -8.107708 -7.4032521 -7.1687703 -7.0627718 -6.8328838 -6.3728971 -7.2949324 -6.9708114 -6.5769868 -6.7967329 -6.8092113 -7.2983618][-7.1067076 -6.6906209 -6.6556029 -7.2755008 -7.4804249 -7.0773659 -6.3608627 -6.5477095 -7.2292805 -6.8944345 -5.9296846 -6.1777935 -6.5127416 -7.0493717 -7.3374023]]...]
INFO - root - 2017-12-15 18:26:13.811447: step 45510, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 52h:35m:40s remains)
INFO - root - 2017-12-15 18:26:20.307275: step 45520, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 51h:20m:16s remains)
INFO - root - 2017-12-15 18:26:26.744711: step 45530, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 50h:06m:50s remains)
INFO - root - 2017-12-15 18:26:33.163797: step 45540, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 49h:59m:41s remains)
INFO - root - 2017-12-15 18:26:39.472929: step 45550, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 49h:57m:20s remains)
INFO - root - 2017-12-15 18:26:45.884602: step 45560, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.685 sec/batch; 54h:38m:08s remains)
INFO - root - 2017-12-15 18:26:52.197274: step 45570, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 50h:40m:34s remains)
INFO - root - 2017-12-15 18:26:58.546440: step 45580, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:42m:54s remains)
INFO - root - 2017-12-15 18:27:04.917224: step 45590, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 50h:10m:49s remains)
INFO - root - 2017-12-15 18:27:11.317837: step 45600, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 53h:25m:23s remains)
2017-12-15 18:27:11.811416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7692342 -2.4545956 -2.4775333 -2.3409395 -2.0365486 -1.3679695 -0.67473841 0.0094590187 0.59119415 -1.2012057 -2.6063185 -4.4546576 -6.3118439 -7.7033939 -9.0142746][-2.899087 -2.8719406 -3.0225363 -2.8251748 -2.401782 -1.5394545 -0.58105278 0.15542698 0.6380682 -1.3621755 -2.6670504 -4.5187964 -6.3451595 -7.659061 -8.733264][-3.2361059 -2.9944344 -3.0058427 -2.6225667 -2.049798 -1.2152371 -0.2927537 0.31028938 0.675437 -1.5024672 -3.0931172 -4.7725954 -6.4979382 -7.8130045 -8.8473806][-3.9857032 -3.4960771 -3.1295185 -2.3945274 -1.4135127 -0.42933273 0.59882545 1.1262226 1.3921833 -0.98247147 -2.7597742 -4.6068382 -6.3157096 -7.3543453 -8.3081112][-3.9201906 -3.2047219 -2.3676043 -1.6093392 -0.47151852 0.7869072 1.8213978 2.0817814 2.1337843 -0.416934 -2.3326826 -4.261816 -6.0586543 -7.217824 -8.224205][-3.7159369 -2.9503212 -2.3286786 -1.3157148 -0.10221481 1.3331652 2.3755655 2.6144505 2.4324512 -0.37680292 -2.3632927 -4.2857552 -6.1494884 -7.1622477 -8.0842762][-3.4425869 -2.6890335 -1.8490982 -0.64277315 0.67765141 2.1399269 3.0524082 3.1278667 2.8507433 -0.11762381 -2.3546786 -4.5014844 -6.2954903 -7.556169 -8.6172056][-3.0202022 -2.2646394 -1.3925343 -0.21966219 1.1940241 2.4804134 3.1879501 3.2564888 2.9990835 0.037767887 -2.1193643 -4.5106997 -6.4659986 -7.5672765 -8.4144449][-2.8599253 -2.2909102 -1.5521474 -0.27888536 0.97872353 2.1771126 2.5871964 2.7663784 2.645443 -0.13471317 -2.2884622 -4.5155811 -6.4448524 -7.6436167 -8.4514732][-3.0155578 -2.5120058 -2.000349 -0.8043642 0.30554771 1.4367027 1.5968075 1.6851063 1.6828451 -1.2553868 -2.9968033 -5.0537615 -6.8580174 -7.7442446 -8.6211147][-4.4573236 -4.008544 -3.1982293 -2.2584677 -1.2937479 -0.18299103 -0.049474239 -0.29652023 -0.60259581 -3.3368754 -4.8739843 -6.1829453 -7.3186641 -8.0154343 -8.5323849][-5.5321016 -5.066864 -4.49987 -3.5165586 -2.6163564 -1.7447248 -1.764328 -2.0638266 -2.4123688 -4.4988031 -5.4588966 -6.4798222 -7.2478838 -7.6783566 -8.165555][-6.5592527 -6.203794 -5.8397751 -5.0974722 -4.3483462 -3.6232591 -3.5985284 -3.9894419 -4.3583536 -5.7346849 -6.3260336 -6.5849357 -6.909214 -6.927743 -7.0761247][-6.7344918 -6.2370486 -5.8319149 -5.3543844 -5.0634303 -4.7468758 -4.8277526 -5.0567045 -5.1855049 -5.7496514 -5.9006186 -5.9909768 -6.3404737 -6.3399267 -6.5759549][-8.2457314 -7.8947077 -7.2891712 -6.9520593 -6.6507807 -6.3560796 -6.5283694 -6.8997173 -7.1385412 -7.0241241 -6.6317358 -6.4149113 -6.30964 -6.3313308 -6.242959]]...]
INFO - root - 2017-12-15 18:27:18.165291: step 45610, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:44m:08s remains)
INFO - root - 2017-12-15 18:27:24.491538: step 45620, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 50h:08m:38s remains)
INFO - root - 2017-12-15 18:27:30.906401: step 45630, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 51h:14m:36s remains)
INFO - root - 2017-12-15 18:27:37.288276: step 45640, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:38m:01s remains)
INFO - root - 2017-12-15 18:27:43.752591: step 45650, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 50h:19m:47s remains)
INFO - root - 2017-12-15 18:27:50.182334: step 45660, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 50h:46m:14s remains)
INFO - root - 2017-12-15 18:27:56.652480: step 45670, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 51h:41m:27s remains)
INFO - root - 2017-12-15 18:28:02.985992: step 45680, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 50h:02m:30s remains)
INFO - root - 2017-12-15 18:28:09.404192: step 45690, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 51h:31m:02s remains)
INFO - root - 2017-12-15 18:28:15.869858: step 45700, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:24m:17s remains)
2017-12-15 18:28:16.441792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5411973 -2.705811 -2.9980812 -3.1559343 -3.3595667 -3.6253262 -3.7130289 -3.6593475 -3.2945127 -4.4010115 -5.176527 -6.0064797 -6.5581484 -6.9257984 -7.6230655][-2.6893773 -2.5799947 -2.7869887 -3.081625 -3.4142742 -3.767817 -4.0504165 -4.0829992 -3.8031681 -5.2128506 -5.9852324 -6.9510589 -7.5872827 -8.025115 -8.5350447][-3.3376737 -2.8723774 -2.8622255 -2.8651686 -3.0039444 -3.2799044 -3.2916036 -3.5682774 -3.5484867 -5.0792484 -6.1070809 -7.2852678 -8.1632042 -8.5511608 -9.1867714][-3.7308054 -3.2470365 -2.8177404 -2.2776523 -1.9465189 -1.9017968 -1.8338776 -1.8721337 -1.8039756 -3.3122382 -4.5759048 -5.8195548 -6.7081995 -7.3820095 -8.1668386][-3.7027476 -2.8537803 -2.1006708 -1.4821415 -0.9399724 -0.43346167 -0.043651104 0.0017471313 0.28242731 -1.0896044 -2.3386316 -4.6708922 -6.2043104 -7.1967821 -8.1531706][-4.1334686 -3.1130052 -2.550982 -1.3708973 -0.36576748 0.40276623 1.2154408 1.6082592 2.0249271 0.19702339 -1.4847479 -3.6888256 -5.609694 -6.5728235 -8.013195][-4.3482614 -3.2460737 -2.2590923 -0.53162861 1.1091347 1.9220695 2.9114971 3.7409029 4.6239386 2.7028198 0.799942 -1.6329618 -4.1046572 -5.5824032 -7.0403824][-4.5014505 -3.4175863 -2.4859405 -0.71185923 1.3788328 2.9995499 4.4469004 5.0141125 5.6834917 3.9704151 2.4358702 -0.28625917 -2.6172285 -4.16035 -5.7495022][-5.6834106 -4.3650179 -3.5178189 -2.0051107 -0.33107328 1.5028229 3.2864056 3.9245167 4.0334587 1.9349031 0.50325108 -1.6487675 -3.2388077 -4.4258032 -6.028934][-7.5696931 -6.3713989 -5.0841475 -3.6770148 -2.1904602 -1.2507071 0.1876483 1.4150248 1.8906097 -0.88873529 -2.6775322 -4.3992805 -5.5287213 -6.17817 -6.8502607][-9.3006706 -8.7080269 -7.461935 -6.2639618 -4.8393469 -3.5479608 -2.3009462 -1.8834591 -1.3931513 -3.0949435 -4.5742331 -6.1382432 -7.4589925 -7.9614892 -8.2939014][-10.383245 -9.9651918 -9.2272882 -7.8872204 -6.6786413 -5.2731075 -4.3202982 -3.8689892 -3.5822186 -4.7412047 -5.5420504 -6.770021 -7.5131669 -7.6924748 -7.8977456][-10.036432 -9.7631207 -9.1208916 -7.9626079 -7.0245318 -6.0105619 -5.1071968 -4.7887082 -4.804904 -5.7897754 -5.8116355 -6.433342 -6.7053065 -6.6537266 -6.90603][-10.239405 -9.6163263 -8.8179951 -7.6962538 -6.660183 -5.7698526 -5.1238174 -5.28294 -5.4449415 -5.9869289 -6.0528135 -5.918088 -6.0745268 -5.7929382 -6.1622624][-10.557145 -10.312147 -9.8031321 -8.8338852 -7.7031145 -6.6872725 -6.0040116 -5.8798728 -6.074451 -6.2653546 -6.3497987 -6.3720531 -5.956954 -5.6139488 -5.5749521]]...]
INFO - root - 2017-12-15 18:28:22.856288: step 45710, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.631 sec/batch; 50h:16m:55s remains)
INFO - root - 2017-12-15 18:28:29.259016: step 45720, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 51h:04m:03s remains)
INFO - root - 2017-12-15 18:28:35.671411: step 45730, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 50h:15m:21s remains)
INFO - root - 2017-12-15 18:28:42.008693: step 45740, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 50h:17m:52s remains)
INFO - root - 2017-12-15 18:28:48.293789: step 45750, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 50h:02m:50s remains)
INFO - root - 2017-12-15 18:28:54.676745: step 45760, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 51h:01m:54s remains)
INFO - root - 2017-12-15 18:29:01.014165: step 45770, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 49h:59m:59s remains)
INFO - root - 2017-12-15 18:29:07.377533: step 45780, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 51h:43m:25s remains)
INFO - root - 2017-12-15 18:29:13.804454: step 45790, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 49h:54m:34s remains)
INFO - root - 2017-12-15 18:29:20.162633: step 45800, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 51h:27m:10s remains)
2017-12-15 18:29:20.627288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3487525 -4.3960752 -4.1635933 -4.6207943 -5.109488 -5.4433126 -5.451333 -5.4444761 -5.1212826 -6.0633526 -6.9735203 -8.139184 -9.0204353 -9.6227474 -10.313497][-3.4896812 -3.7949755 -3.9880543 -4.8460093 -5.5079041 -5.7565389 -6.1162124 -6.0059242 -5.76994 -6.8331985 -7.9214883 -9.0432615 -9.8065472 -10.603938 -11.363677][-2.3520441 -2.8611717 -2.8301277 -3.5678377 -3.6678028 -3.7548337 -3.7107091 -3.8613844 -4.1532707 -5.5676961 -7.0483379 -8.3459406 -9.08699 -9.6612835 -10.202318][-3.5553837 -3.1640306 -2.5651398 -2.3533311 -2.2057033 -1.7432675 -0.99303055 -1.0031095 -1.1453447 -2.8811202 -4.713625 -6.4961319 -7.7540927 -8.3348656 -9.254282][-4.7309432 -3.2924728 -1.7121024 -1.1042461 -0.34707022 0.64349365 1.6685991 1.893384 1.7201071 -0.39338303 -2.4756131 -4.6314039 -5.8094897 -6.9667773 -8.0534592][-4.597949 -3.6331406 -1.9247155 -0.43721771 0.68376637 1.8912878 3.1894598 3.3573704 3.2473412 0.79584789 -1.3746805 -3.4224205 -5.3438787 -6.2727556 -7.2043009][-5.3490415 -4.2631941 -2.7782149 -0.81995678 1.3433485 2.9085913 4.2984743 4.4007921 4.1830578 1.8763447 -0.26219511 -2.6702914 -4.5169616 -6.0685968 -7.81152][-4.1583533 -3.6377506 -2.1434188 -0.37312269 1.4036713 3.2900171 4.5075445 4.4984245 4.5153742 2.3053637 0.36256886 -1.9124665 -3.8181996 -5.3839011 -7.0626826][-3.4829965 -2.9397945 -2.0979862 -0.87474966 0.75701332 2.2035484 3.1651545 3.4332094 3.5234957 1.5869141 -0.17355156 -2.2571993 -3.8854356 -5.3282638 -7.0269504][-2.9637012 -3.0923791 -2.3725424 -1.660543 -0.63448668 0.906126 1.8938627 1.9536066 2.080636 -0.017230511 -1.7461267 -3.5370979 -4.9983048 -6.0316887 -7.4304328][-3.4814339 -3.4152884 -2.6763749 -2.278585 -1.4032984 -0.58904076 0.21907187 0.16440964 -0.033328533 -1.8946562 -3.533752 -4.7692013 -5.4940419 -6.0863991 -6.9157834][-3.7448759 -3.7509854 -3.5035429 -3.4212422 -3.074585 -2.2918878 -1.9565897 -2.1020427 -2.0883303 -3.4754 -4.6224484 -5.5100112 -5.8816571 -6.1039863 -6.57994][-4.7892752 -4.6110296 -4.0869627 -4.2008181 -3.9268048 -3.4574418 -2.8848653 -3.0147491 -3.3089871 -4.369173 -5.5290918 -6.0356011 -6.1977367 -6.2958727 -6.5806284][-5.5066805 -5.0460806 -4.4267721 -4.2778254 -3.9080434 -3.4797902 -2.9745765 -3.3114705 -3.6986322 -4.7322097 -5.32882 -5.4765415 -5.5410872 -5.5650806 -5.5349336][-6.6968651 -6.5189562 -5.9138041 -5.7796836 -5.4472237 -5.1596594 -4.8132372 -4.7541156 -5.0630841 -5.4510069 -5.6482658 -5.8985267 -5.9944224 -5.5830631 -5.1836095]]...]
INFO - root - 2017-12-15 18:29:26.952154: step 45810, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:37m:50s remains)
INFO - root - 2017-12-15 18:29:33.346789: step 45820, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 51h:03m:50s remains)
INFO - root - 2017-12-15 18:29:39.702817: step 45830, loss = 0.32, batch loss = 0.21 (12.0 examples/sec; 0.668 sec/batch; 53h:10m:28s remains)
INFO - root - 2017-12-15 18:29:46.110940: step 45840, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 51h:00m:48s remains)
INFO - root - 2017-12-15 18:29:52.519758: step 45850, loss = 0.23, batch loss = 0.12 (12.7 examples/sec; 0.630 sec/batch; 50h:08m:49s remains)
INFO - root - 2017-12-15 18:29:58.982179: step 45860, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 50h:37m:22s remains)
INFO - root - 2017-12-15 18:30:05.303506: step 45870, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:23m:50s remains)
INFO - root - 2017-12-15 18:30:11.619057: step 45880, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:06m:05s remains)
INFO - root - 2017-12-15 18:30:17.944252: step 45890, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 52h:59m:36s remains)
INFO - root - 2017-12-15 18:30:24.307199: step 45900, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 50h:12m:09s remains)
2017-12-15 18:30:24.814263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5663023 -4.6680765 -3.3647666 -2.8332257 -2.7031522 -1.3413858 -0.76282787 -2.3134909 -2.751328 -4.5984526 -6.6052046 -7.3296356 -8.0830364 -8.7861338 -8.7764168][-5.2948256 -4.1192207 -3.1603289 -3.1159158 -3.0984764 -2.9431443 -3.0092645 -2.1761007 -1.9074168 -4.5373392 -6.1530156 -6.8572049 -7.8378572 -8.9305887 -8.7816267][-4.5010424 -4.080452 -3.7010274 -2.5363994 -1.5588264 -1.2192702 -0.65210247 -1.0413837 -1.3832746 -2.0739822 -3.6032429 -5.2370853 -6.5625362 -6.955822 -7.409338][-4.2448826 -2.9367146 -1.7265677 -0.97415686 -0.098403931 0.61584473 1.3860025 1.1419506 0.58478642 -1.1902909 -2.8251042 -3.8286574 -5.6636343 -6.6628304 -6.8195157][-3.0524864 -1.395287 -0.6108737 -0.13627481 0.63433933 1.5269775 2.10213 1.9656963 1.5939875 -0.6362772 -2.7179117 -3.6908429 -5.1004095 -5.8750753 -6.5457706][-1.9374475 -1.5323982 -1.0082741 -0.0898428 0.70604324 1.5880651 2.3161869 2.2096577 1.6648998 -0.080668926 -1.7079501 -2.9333196 -4.1228514 -4.9733667 -5.3502336][-2.5826421 -1.5195441 -0.61901426 0.46448231 1.4036655 2.2350597 3.0809994 3.0375385 2.6253109 0.54777145 -1.4130111 -2.6923671 -4.5544577 -5.3809242 -5.9108529][-2.6228251 -1.5803409 -0.48380184 0.83253765 2.0288744 2.8814449 3.782074 3.1921577 2.51023 0.55495167 -1.345686 -3.1917753 -5.0745468 -5.8966618 -6.1722174][-1.8890672 -1.2147536 -0.78232622 -0.04320097 0.59431267 1.6950178 2.4366102 2.0033846 1.9454842 -0.61324024 -2.4085488 -3.2350841 -4.64387 -5.7718844 -6.4791441][-2.43538 -2.1786327 -1.8784199 -1.3186626 -1.051322 -0.374712 0.26206303 0.33028603 0.39159966 -1.6990314 -2.9641094 -3.6763535 -4.5092959 -5.1376953 -5.8209248][-4.4996881 -3.8447731 -3.3315277 -2.7369437 -2.1561604 -1.6465011 -1.1536121 -0.72340441 -0.6117034 -2.1793532 -2.7852979 -3.6195259 -5.1000991 -5.7162561 -6.391016][-5.5265608 -3.5720377 -2.2962909 -1.8091602 -1.3718767 -1.0674958 -0.75982237 -1.0561738 -2.0218453 -3.1548777 -4.0090694 -4.8539038 -5.70748 -6.5058742 -7.1212363][-5.441268 -4.35768 -2.9649229 -1.9663477 -1.2591572 -1.3721185 -1.3043771 -1.9685388 -2.4331479 -3.8696671 -5.4483371 -5.4721274 -5.8607264 -6.4316225 -7.0209351][-5.003839 -4.2295728 -3.8418443 -2.8677468 -2.0696163 -1.8776102 -2.0075603 -2.4277949 -2.3641825 -3.2647305 -3.5486727 -4.0344324 -4.7133083 -5.3763356 -6.3035975][-6.4468088 -4.7613735 -4.12323 -3.1713147 -3.1982899 -2.8132734 -2.8108845 -2.9934373 -3.7612262 -3.9915137 -4.1995344 -4.6953869 -5.2522163 -5.8316021 -6.3131056]]...]
INFO - root - 2017-12-15 18:30:31.302757: step 45910, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 51h:26m:49s remains)
INFO - root - 2017-12-15 18:30:37.753072: step 45920, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 51h:33m:57s remains)
INFO - root - 2017-12-15 18:30:44.092730: step 45930, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:34m:31s remains)
INFO - root - 2017-12-15 18:30:50.623735: step 45940, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 53h:45m:16s remains)
INFO - root - 2017-12-15 18:30:57.035721: step 45950, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 50h:33m:11s remains)
INFO - root - 2017-12-15 18:31:03.335305: step 45960, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 49h:38m:47s remains)
INFO - root - 2017-12-15 18:31:09.720150: step 45970, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 50h:15m:28s remains)
INFO - root - 2017-12-15 18:31:16.037902: step 45980, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 50h:02m:17s remains)
INFO - root - 2017-12-15 18:31:22.601074: step 45990, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 52h:27m:24s remains)
INFO - root - 2017-12-15 18:31:29.050389: step 46000, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 51h:25m:30s remains)
2017-12-15 18:31:29.592113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.078146 -5.9994726 -5.7401676 -5.5714941 -5.9529762 -5.3536663 -4.2361069 -3.1898465 -2.0141392 -2.1546669 -2.7172089 -4.2961464 -5.9671574 -7.5949244 -8.9735842][-4.2396674 -4.9769926 -5.7650137 -6.140758 -6.416132 -6.2913475 -6.1326842 -4.6074371 -2.8515339 -2.843483 -3.2635617 -5.2711792 -6.7984796 -7.8829374 -8.7064238][-3.6937613 -4.208775 -5.12654 -6.2177324 -6.9592423 -6.8236914 -5.9622478 -5.0172567 -4.056242 -3.4792237 -3.578548 -5.3868747 -6.8951139 -8.04991 -9.0357161][-3.3630357 -3.768147 -4.1664715 -4.5642796 -4.8952293 -4.8221841 -4.3950148 -3.5984817 -2.4616346 -2.6523595 -3.3088117 -4.7282171 -6.3154221 -7.4058232 -8.6975632][-3.5123839 -3.5475259 -3.5146918 -3.2609863 -2.8722939 -2.3578453 -1.5798502 -1.0671482 -0.60066175 -1.0341721 -1.7745686 -3.6529336 -5.154851 -6.2599692 -7.6047549][-3.2637625 -2.8356552 -2.4971924 -1.5925264 -0.73020697 -0.25935125 0.54837894 1.1005583 1.5907001 0.68079185 -0.31303835 -1.9701228 -3.9636669 -5.503685 -6.9239779][-2.8082471 -2.0869298 -1.0177255 -0.14507866 1.0637665 1.6397314 2.4206839 2.9502554 3.4617376 2.488512 0.8679018 -1.6321716 -3.6000571 -5.0152025 -6.7157979][-2.0203953 -1.3494892 -0.74016047 0.16623783 1.2251692 1.8131561 2.6558704 3.3259468 3.9293346 2.7003222 0.7691803 -2.1115437 -4.6289644 -5.8787689 -6.5424213][-1.8354073 -1.1904364 -0.75729752 -0.57755852 -0.13423681 0.3552599 1.0553036 1.8727026 2.2755985 1.0743475 -0.57513189 -3.2278919 -5.3974543 -6.3495378 -7.0645032][-1.1493683 -1.2657304 -0.97598886 -1.1140175 -1.117044 -0.9508214 -0.68236065 0.1263938 0.70225525 -0.54590988 -2.1756964 -4.365325 -5.9922934 -6.6521931 -7.1840677][-3.006041 -2.2476864 -1.826736 -2.4775391 -2.6579552 -2.7036581 -2.5396123 -2.3125639 -2.4132423 -3.337564 -4.2806177 -5.911912 -6.8234177 -7.0590053 -7.3759432][-5.5868607 -5.0285711 -4.5218754 -4.0004187 -3.5515671 -3.7032158 -3.9804668 -4.2099295 -4.5967083 -5.5982213 -6.4645853 -7.2527475 -7.4705772 -7.2497787 -7.4684534][-8.130621 -7.191926 -6.9358468 -6.2311559 -5.2309818 -4.9145293 -4.5706949 -4.9326129 -5.3087196 -5.9437842 -6.7500482 -7.0145965 -7.3622737 -7.1841044 -7.1285248][-8.8924837 -8.941082 -8.6688271 -7.9711027 -7.5748715 -6.5964966 -5.8143444 -6.0614085 -5.9926343 -6.39094 -6.6398644 -6.6912212 -6.840631 -6.2592335 -6.1438818][-8.0758648 -8.3020267 -8.3333206 -7.6541967 -7.3323064 -7.086503 -6.8661914 -6.3609238 -6.1304569 -6.1878853 -5.9940839 -5.9089985 -5.9218082 -5.7040267 -5.6598015]]...]
INFO - root - 2017-12-15 18:31:35.960175: step 46010, loss = 0.32, batch loss = 0.21 (13.1 examples/sec; 0.609 sec/batch; 48h:29m:16s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 18:31:42.249928: step 46020, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.616 sec/batch; 48h:59m:21s remains)
INFO - root - 2017-12-15 18:31:48.593614: step 46030, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 50h:49m:20s remains)
INFO - root - 2017-12-15 18:31:54.967537: step 46040, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 50h:57m:10s remains)
INFO - root - 2017-12-15 18:32:01.328403: step 46050, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 51h:02m:39s remains)
INFO - root - 2017-12-15 18:32:07.701123: step 46060, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 49h:18m:46s remains)
INFO - root - 2017-12-15 18:32:14.040338: step 46070, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:58m:01s remains)
INFO - root - 2017-12-15 18:32:20.395451: step 46080, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 49h:37m:32s remains)
INFO - root - 2017-12-15 18:32:26.694827: step 46090, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 49h:15m:27s remains)
INFO - root - 2017-12-15 18:32:33.065973: step 46100, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 51h:02m:48s remains)
2017-12-15 18:32:33.588005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9590149 -5.4011803 -5.1421814 -5.1241026 -5.1178174 -4.633944 -4.6788397 -4.7099915 -4.1818757 -5.8696337 -7.0439672 -7.1847816 -7.6969233 -8.29224 -9.224967][-5.8862638 -5.5355711 -5.3867326 -5.6318922 -6.040103 -5.7060661 -6.0836358 -6.2476616 -6.20177 -7.4040289 -7.6714244 -8.43494 -9.0815916 -9.8499041 -11.028624][-4.3225832 -4.0094719 -4.0213714 -4.3541346 -4.465085 -4.3267937 -4.7871876 -5.2084475 -5.691082 -7.2337575 -7.7534447 -8.1649971 -8.8231964 -9.4631109 -10.332453][-2.7183685 -2.7491045 -2.7604809 -2.4522181 -2.4522586 -1.8997097 -1.7847381 -2.3023281 -2.63838 -4.7897377 -6.3151236 -7.3242793 -8.2370386 -8.7936039 -9.4760866][-2.1933789 -2.0235 -1.6674695 -1.768157 -1.4957762 -0.4069891 0.064863682 0.060244083 0.1027813 -2.4353604 -4.515214 -6.29778 -7.6094584 -8.7273664 -9.4526978][-2.8208442 -2.5806642 -2.1706796 -1.2994652 -0.16450882 1.1033564 2.4099627 2.8299446 3.0749111 0.36278057 -1.9784684 -3.9779449 -5.7942348 -7.1366682 -7.9514008][-4.0591183 -3.3617024 -2.1434436 -0.75024939 0.75456142 2.4653845 3.7930698 4.3745642 4.6452522 2.0136385 -0.2380681 -2.9847221 -5.5296841 -6.8755727 -8.1544514][-4.5911169 -3.6453681 -2.7420135 -0.9722538 0.81261635 2.4674835 4.2484818 4.9369593 5.642828 3.0282984 0.59112072 -2.1655731 -4.7483487 -6.3128076 -7.4453292][-5.8727064 -5.0956669 -3.9408855 -2.2864556 -0.48051739 1.1347094 2.3320732 3.1576281 3.9152727 1.6133575 -0.26598024 -2.3861704 -4.3407412 -5.814919 -7.0944858][-6.4208097 -5.5577044 -4.6936092 -3.1721954 -1.5134912 -0.1902957 1.1829586 1.9255714 2.3779421 0.19385433 -1.6244364 -3.0823359 -4.5360394 -5.6374598 -6.6391516][-7.7436805 -6.7937422 -5.929018 -4.9322853 -3.9654298 -2.746305 -1.5327954 -0.84312868 -0.080913067 -1.9327369 -3.5184898 -4.564569 -5.5088997 -6.1938372 -6.6254196][-8.26058 -7.6349516 -7.0401773 -6.0642524 -4.9334021 -4.1760621 -3.3968477 -2.6241679 -1.9924479 -3.3964963 -4.6697311 -5.4979897 -6.001121 -6.3395977 -6.8155551][-8.2740917 -8.0972013 -7.4565873 -6.6644306 -5.8814535 -5.1981392 -4.4043961 -4.2458005 -3.9672873 -4.7667236 -5.7459688 -5.7590504 -6.0434332 -6.08877 -6.1351485][-7.0972791 -6.4393468 -6.2953115 -5.8051615 -5.116468 -5.0086374 -4.7205029 -4.6281266 -4.4735575 -5.228137 -5.4803562 -5.5423169 -5.2587085 -4.9342184 -5.075829][-7.8459458 -7.7479448 -7.2795768 -7.0934258 -7.1792912 -6.7885208 -6.7993932 -6.8562851 -6.9225106 -7.1066041 -7.3799953 -7.3907428 -6.8513589 -6.1933722 -5.6368723]]...]
INFO - root - 2017-12-15 18:32:39.987689: step 46110, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 50h:30m:12s remains)
INFO - root - 2017-12-15 18:32:46.347286: step 46120, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 53h:23m:55s remains)
INFO - root - 2017-12-15 18:32:52.815930: step 46130, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 51h:57m:56s remains)
INFO - root - 2017-12-15 18:32:59.263475: step 46140, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 51h:42m:23s remains)
INFO - root - 2017-12-15 18:33:05.650003: step 46150, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 50h:09m:11s remains)
INFO - root - 2017-12-15 18:33:12.097606: step 46160, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.627 sec/batch; 49h:54m:35s remains)
INFO - root - 2017-12-15 18:33:18.549449: step 46170, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 51h:34m:25s remains)
INFO - root - 2017-12-15 18:33:24.902294: step 46180, loss = 0.34, batch loss = 0.22 (12.9 examples/sec; 0.621 sec/batch; 49h:21m:45s remains)
INFO - root - 2017-12-15 18:33:31.252820: step 46190, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.638 sec/batch; 50h:46m:07s remains)
INFO - root - 2017-12-15 18:33:37.695972: step 46200, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 50h:39m:32s remains)
2017-12-15 18:33:38.213550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1147413 -5.3547964 -5.574265 -5.7239165 -5.5279756 -4.9130487 -4.3075113 -3.6988606 -3.5137658 -3.5807695 -3.8824732 -5.7349496 -6.5486679 -6.4461327 -7.7962685][-5.0571003 -5.4654393 -6.2309685 -6.4735537 -6.5420814 -6.1790509 -5.8306851 -5.5502214 -5.3706427 -5.3364897 -5.6033325 -7.1320205 -7.8308887 -7.4760323 -7.9575629][-4.8599958 -5.4677324 -6.3136759 -6.5271521 -6.3724866 -6.2839937 -6.184927 -6.2840905 -6.6453829 -6.7545629 -6.4851718 -7.5318928 -8.3037424 -7.7925763 -8.2369833][-4.5865974 -5.1856489 -5.6512318 -5.4081907 -4.9972429 -5.0396833 -4.5689182 -4.6831026 -4.9974508 -5.4299135 -5.3122354 -6.7035666 -7.5616059 -6.9291415 -7.7635264][-5.0003357 -4.6105151 -4.3863707 -3.8740652 -3.2583504 -3.0628448 -3.0381384 -2.9670787 -2.8125052 -2.8103485 -2.9546666 -4.3247294 -5.6251154 -5.5589848 -6.8438454][-4.0139155 -3.3230977 -2.3868446 -1.6739931 -1.1671543 -0.6680479 -0.63464928 -0.75582504 -0.47869825 -0.85896587 -0.86299992 -2.4642062 -4.1365814 -4.4110503 -5.5172105][-3.4575658 -2.6802306 -1.7834082 -0.98159552 0.15027618 0.79724026 1.2332458 1.242837 1.9252901 1.0727634 0.38047504 -2.0878282 -3.9910543 -4.480649 -5.5800371][-2.4679437 -2.5246806 -1.6607461 -0.97026634 -0.0026679039 0.61671638 1.4048786 1.9061813 2.535533 1.8608027 0.89142227 -1.5140076 -3.5369053 -3.8922429 -5.2368965][-3.8774 -3.8022876 -3.1171951 -1.7897806 -0.94344711 -0.33886003 0.53757763 1.2140789 1.9091501 1.3457336 0.51226139 -1.8681436 -3.4084334 -3.7030113 -5.0248427][-4.6907625 -4.121151 -3.4702129 -2.6342306 -1.9757218 -1.3549433 -0.7678442 -0.091433525 0.76724052 -0.24700642 -1.1951756 -2.5098853 -3.849813 -4.1528525 -5.216054][-7.1743841 -6.3931289 -5.2082386 -4.480813 -3.9740894 -3.7316544 -3.0519795 -2.6828547 -2.67624 -3.3071771 -3.5467634 -4.224308 -5.3685675 -5.52897 -5.9571152][-7.5945563 -6.6801457 -6.0399342 -5.3768082 -4.917552 -4.8883443 -4.6601367 -4.1427431 -3.7045915 -4.3489804 -5.0438166 -5.5307064 -6.1743393 -6.0395961 -5.8215036][-8.9576321 -8.1263552 -7.7510595 -7.0965486 -6.5089455 -5.7785668 -5.174305 -5.2759004 -4.8354082 -5.0760345 -5.8318729 -5.6020756 -5.6358562 -5.4948206 -5.489397][-8.0771313 -7.4930558 -7.1704454 -6.4893909 -5.7679586 -5.2647915 -4.6670628 -4.5596471 -4.5268984 -4.7766504 -5.0621209 -4.7712955 -4.77194 -4.5193377 -4.5990381][-6.8045778 -6.2723231 -6.7304726 -6.3263521 -5.7774868 -5.2584085 -4.7321749 -4.6428547 -4.7349405 -5.0623622 -5.193059 -4.950736 -5.2533894 -5.3625059 -5.216054]]...]
INFO - root - 2017-12-15 18:33:44.570977: step 46210, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:18m:12s remains)
INFO - root - 2017-12-15 18:33:50.935523: step 46220, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:33m:23s remains)
INFO - root - 2017-12-15 18:33:57.284708: step 46230, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 49h:13m:15s remains)
INFO - root - 2017-12-15 18:34:03.686617: step 46240, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 50h:54m:27s remains)
INFO - root - 2017-12-15 18:34:10.034955: step 46250, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 52h:22m:28s remains)
INFO - root - 2017-12-15 18:34:16.351491: step 46260, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 50h:20m:20s remains)
INFO - root - 2017-12-15 18:34:22.773216: step 46270, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 50h:31m:36s remains)
INFO - root - 2017-12-15 18:34:29.258152: step 46280, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.652 sec/batch; 51h:50m:06s remains)
INFO - root - 2017-12-15 18:34:35.651702: step 46290, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 49h:11m:06s remains)
INFO - root - 2017-12-15 18:34:42.097834: step 46300, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 52h:00m:23s remains)
2017-12-15 18:34:42.646430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0361366 -4.1667318 -3.6732187 -3.4177098 -3.161756 -3.4533329 -3.1720157 -3.5799627 -3.5089989 -4.44508 -4.7660069 -5.6695261 -6.1109557 -6.6801682 -7.5952439][-2.6466441 -3.7270885 -3.8369265 -3.9827607 -3.8783321 -4.2905321 -4.0567036 -4.1776538 -3.7229874 -4.0144329 -4.0190125 -4.6722727 -5.3827229 -5.5485315 -6.6189861][-2.8142629 -2.487041 -2.0843134 -2.5185752 -2.8016529 -2.8822517 -2.5573688 -2.391623 -2.466404 -2.9578691 -2.5931363 -3.0892515 -3.6209393 -3.8996546 -4.9433489][-1.769269 -2.1461864 -2.0280447 -1.32091 -1.0004473 -0.92066622 -0.62185621 -0.43289995 -0.15008688 -0.97710037 -1.0763936 -2.0007119 -2.4250903 -3.0642142 -3.8258247][-1.0751333 -0.33906603 0.081765652 -0.22502613 -0.5128417 -0.177948 0.56137466 0.907568 1.4479427 0.69842243 0.77981186 -1.0951228 -1.9164801 -2.5527678 -3.0770974][0.010798454 -0.099178791 -0.28096342 0.13374233 0.547801 1.2290325 2.1285505 2.6133652 3.0565968 2.339118 2.1054745 -0.45843029 -1.4570847 -2.8699965 -3.6486092][-1.2506957 0.021169186 0.070973873 0.36046982 0.48849297 1.4391375 2.3468742 2.8620996 3.3889189 2.1829433 1.4698429 -1.1042042 -2.252553 -3.841161 -4.5227451][-1.6024084 -1.1320691 -0.28871822 0.71914577 1.4524231 1.8560324 2.2787275 2.5722628 2.7378178 1.6405687 1.0668726 -1.6487403 -2.870368 -4.6161804 -5.6066337][-1.1822033 -0.88637304 -1.0534978 -0.34575748 -0.064250946 1.0763702 2.1987944 2.2446909 2.3358822 0.88199711 -0.11746264 -2.4919386 -3.8069665 -5.3065786 -6.201][-2.4618049 -1.6670399 -1.4771581 -0.47314072 -0.12221813 0.35441685 0.91442013 1.0963058 1.4407864 0.39968491 -0.65214634 -2.8757777 -4.0416884 -4.9930058 -5.9937024][-3.5486984 -2.88406 -3.0064621 -2.0966377 -1.6631756 -0.67432785 0.38550282 0.4275465 0.62503242 -0.438334 -1.5108542 -2.5983644 -3.3650455 -4.4861813 -5.6277714][-4.2999892 -3.8032751 -3.6971376 -3.1083231 -2.5871124 -2.1130614 -1.4146647 -0.83489847 -0.0017981529 -0.90898514 -1.6002488 -2.6317949 -3.2410727 -3.5370684 -4.3222237][-6.153265 -5.8608503 -5.9295139 -4.8939667 -4.0671959 -3.1137342 -2.40022 -2.3761787 -2.0828094 -2.0379448 -2.3257551 -3.4604459 -4.0833654 -4.148262 -4.3016825][-7.1442537 -6.4563413 -5.8676977 -5.1151438 -5.1356249 -4.2804174 -3.5191698 -3.4121122 -3.2206411 -3.6583037 -3.7168748 -3.8804309 -4.095314 -4.2032404 -4.5580997][-8.8530884 -8.3775043 -8.0171127 -6.9378834 -6.0613728 -5.4923763 -5.2018418 -5.2246027 -4.895051 -4.93594 -4.9003973 -5.1578674 -5.253129 -5.16582 -5.1592622]]...]
INFO - root - 2017-12-15 18:34:49.035903: step 46310, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 50h:31m:12s remains)
INFO - root - 2017-12-15 18:34:55.460780: step 46320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 50h:39m:54s remains)
INFO - root - 2017-12-15 18:35:01.855957: step 46330, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 49h:47m:37s remains)
INFO - root - 2017-12-15 18:35:08.309461: step 46340, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 50h:46m:47s remains)
INFO - root - 2017-12-15 18:35:14.751521: step 46350, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 51h:41m:45s remains)
INFO - root - 2017-12-15 18:35:21.187379: step 46360, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 51h:16m:37s remains)
INFO - root - 2017-12-15 18:35:27.573377: step 46370, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 50h:42m:39s remains)
INFO - root - 2017-12-15 18:35:33.914968: step 46380, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 50h:24m:42s remains)
INFO - root - 2017-12-15 18:35:40.307672: step 46390, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 50h:45m:59s remains)
INFO - root - 2017-12-15 18:35:46.685341: step 46400, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 50h:50m:30s remains)
2017-12-15 18:35:47.221816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2773361 -7.1158738 -8.3975773 -10.272215 -10.641281 -9.428978 -7.3836813 -4.993248 -3.8034604 -3.7017667 -3.6845326 -4.7416763 -5.8501863 -7.3166552 -7.7682438][-6.2002492 -7.3681388 -8.5242968 -10.234538 -11.22931 -11.678948 -10.679461 -8.15087 -6.5869293 -5.767375 -5.2674122 -5.3996582 -5.833786 -6.8759527 -8.035799][-6.4120965 -7.4440069 -8.334549 -9.4355469 -9.2981672 -9.1785183 -8.9094973 -8.03743 -7.7479296 -7.0642867 -5.7817278 -5.9301963 -6.1733289 -6.4878111 -7.3772335][-5.0335808 -6.3028841 -8.356472 -8.1673088 -7.4902539 -6.1378508 -5.6722574 -5.9472404 -5.7458639 -6.2372084 -5.65506 -5.7660375 -5.32523 -6.2115946 -6.2468319][-3.2155566 -3.7600152 -4.8957038 -5.9625711 -6.1498036 -4.4780865 -3.5945363 -2.7503729 -2.3446994 -3.83152 -3.890353 -5.4237318 -5.1494145 -5.8106875 -6.052989][-3.4367089 -3.9065406 -4.24734 -3.6635609 -2.6103144 -0.98815536 -0.21705151 0.035432339 0.33577728 -0.74692488 -0.9824996 -3.1970596 -3.6235361 -5.1631136 -5.352396][-3.8810148 -4.3960714 -4.090354 -2.8520694 -1.6238103 -0.037156105 1.0991402 1.6330948 1.9287481 0.23552752 -0.25535536 -2.2218575 -3.4203076 -5.1544685 -5.3404622][-3.4419794 -4.2479944 -3.8471761 -3.2033658 -1.6988583 0.63971519 1.7214308 1.6562243 1.5430231 0.017811775 -0.2947197 -2.3065429 -3.4889922 -5.3131046 -5.5249877][-5.3836594 -4.9286346 -4.41628 -3.330636 -1.9314685 -0.704329 0.26995134 1.5061579 1.5412807 0.13454676 -1.2131481 -4.0912189 -4.8596277 -6.3298216 -6.6858187][-6.6583123 -6.1197567 -5.9161453 -4.9722824 -3.68541 -2.2469754 -2.2868767 -1.8904676 -1.1792817 -2.2787724 -2.6199665 -4.8586054 -6.5102282 -7.8314648 -7.6946349][-8.4561586 -8.0091171 -7.62568 -6.0470123 -4.6083069 -3.9930186 -3.8300452 -3.1304374 -3.2531123 -4.4023132 -5.1170073 -6.3216314 -7.283916 -7.7286243 -8.1948147][-9.0390177 -9.2466 -9.1172237 -7.9984655 -6.7976642 -5.3357239 -4.6949844 -5.0282159 -5.0975714 -6.2642007 -6.7838016 -7.1454673 -8.491889 -8.8935747 -8.6988411][-8.67721 -8.8387642 -8.5448675 -7.8675003 -7.4040771 -6.5123582 -6.3038654 -6.30794 -6.0836415 -6.6782036 -7.0820289 -6.7842031 -6.9077754 -7.7278748 -7.8125806][-7.5687819 -8.1450233 -8.2782612 -7.1598582 -6.100431 -5.893302 -5.8410416 -5.5827045 -6.0664892 -7.0116363 -7.1615176 -6.9497681 -7.5807314 -7.6640563 -7.1063256][-7.9253039 -8.6682587 -8.711236 -8.4399776 -7.5632243 -6.8819332 -6.9544749 -7.2279572 -7.2960305 -6.635685 -6.4583869 -7.0920591 -7.0998712 -6.7767143 -6.6194921]]...]
INFO - root - 2017-12-15 18:35:53.613973: step 46410, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 51h:18m:53s remains)
INFO - root - 2017-12-15 18:36:00.195156: step 46420, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 52h:37m:33s remains)
INFO - root - 2017-12-15 18:36:06.693230: step 46430, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 51h:25m:36s remains)
INFO - root - 2017-12-15 18:36:13.125875: step 46440, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 51h:45m:00s remains)
INFO - root - 2017-12-15 18:36:19.691418: step 46450, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 50h:16m:23s remains)
INFO - root - 2017-12-15 18:36:26.150969: step 46460, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:17m:58s remains)
INFO - root - 2017-12-15 18:36:32.568430: step 46470, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 50h:13m:39s remains)
INFO - root - 2017-12-15 18:36:39.081846: step 46480, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 51h:18m:52s remains)
INFO - root - 2017-12-15 18:36:45.563276: step 46490, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:56m:59s remains)
INFO - root - 2017-12-15 18:36:51.985790: step 46500, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 50h:52m:06s remains)
2017-12-15 18:36:52.513088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5831981 -5.5664186 -4.6635456 -4.6386814 -4.4593763 -3.6843843 -2.7615008 -2.9670644 -2.9451089 -4.2624559 -4.4324417 -4.9413223 -5.1909237 -5.4767332 -6.5302114][-5.8260183 -5.7507668 -5.0223317 -4.1630468 -3.5660286 -3.6456351 -3.3072195 -2.6385722 -2.4298577 -4.2826958 -5.1507568 -5.6571875 -5.9362583 -5.2622876 -6.0114226][-6.7222209 -6.4493675 -5.4444542 -4.1480303 -2.8499551 -2.6475763 -2.7568274 -2.5947909 -2.8126225 -4.624372 -5.5675507 -6.2962708 -6.592206 -6.1472464 -6.3674703][-6.7403727 -5.3822026 -4.8697186 -3.8298018 -2.4760604 -1.8048553 -1.5312696 -1.0368695 -1.1702194 -3.2740059 -4.6947355 -5.4880276 -6.0103483 -6.2538772 -6.5827417][-6.4778848 -4.8647227 -2.8282676 -1.7949276 -1.4194646 -1.089005 -0.57376528 -0.63479376 -1.2635145 -3.179719 -4.0262108 -4.6828742 -5.3636312 -5.7644863 -6.344461][-3.1819496 -2.6833849 -2.7110066 -1.0214801 0.47644997 0.8115778 0.98440933 0.78434467 0.54071426 -1.7965174 -3.4133329 -3.9097145 -4.2273874 -4.7791824 -5.6577559][-2.0885611 -0.70170116 1.4579268 2.0042543 1.4531126 2.0250931 2.1187897 1.2659578 0.88947773 -1.0155163 -2.1228747 -3.1729703 -3.7583668 -4.5043659 -5.0712681][-0.98430347 -1.0683079 -0.88256025 1.1966963 3.1471872 3.5187855 3.58846 2.4705057 1.4162045 -0.87125683 -2.6263924 -3.6688266 -3.8065333 -3.9778252 -5.0557437][-0.70131445 -0.085339546 0.98381615 1.6656361 2.0321903 2.6001787 2.9039059 2.3818016 1.7704468 -0.79669714 -2.0676503 -3.4502039 -4.7394366 -4.8399258 -5.1345787][-1.4172139 -1.3128819 -1.253283 -0.074841976 1.0802479 1.7617054 1.498518 0.75720215 0.30788279 -1.7352252 -3.5595741 -4.1877255 -4.7594843 -5.4991493 -5.9444194][-4.5700359 -4.0543509 -3.2008991 -2.5238628 -2.0569811 -1.4789777 -1.5710659 -1.5464239 -2.1516829 -3.8712325 -4.9391346 -5.6334963 -6.4645782 -6.3480129 -6.7856555][-5.5324059 -5.2030406 -5.032568 -4.1055622 -3.3425183 -2.1847754 -2.1931524 -2.8050461 -3.6940773 -4.7682123 -6.0148487 -6.4649353 -6.9708977 -7.1696353 -7.6506109][-4.5197172 -4.7328796 -5.1016073 -4.749217 -4.828681 -4.6455073 -3.9970934 -3.8396919 -4.8361616 -5.6565065 -7.1783886 -7.4657249 -7.6240559 -7.6861253 -7.5435472][-5.0505466 -4.293067 -4.686409 -4.727664 -3.9362521 -2.9570966 -3.3245125 -3.2722683 -3.676796 -3.8914223 -5.01437 -5.9119134 -6.8544793 -7.0859613 -7.4327531][-5.1127567 -4.8297281 -4.4045897 -4.1368275 -4.4236879 -4.0087457 -3.3556972 -3.305222 -4.287231 -4.8573303 -5.798914 -6.30971 -7.1722641 -7.5947394 -7.0676937]]...]
INFO - root - 2017-12-15 18:36:58.908094: step 46510, loss = 0.35, batch loss = 0.23 (12.3 examples/sec; 0.649 sec/batch; 51h:35m:20s remains)
INFO - root - 2017-12-15 18:37:05.362554: step 46520, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 50h:31m:33s remains)
INFO - root - 2017-12-15 18:37:11.831683: step 46530, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 52h:28m:59s remains)
INFO - root - 2017-12-15 18:37:18.210489: step 46540, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 50h:20m:28s remains)
INFO - root - 2017-12-15 18:37:24.689871: step 46550, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 51h:21m:35s remains)
INFO - root - 2017-12-15 18:37:31.145835: step 46560, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 51h:24m:06s remains)
INFO - root - 2017-12-15 18:37:37.554357: step 46570, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 50h:48m:30s remains)
INFO - root - 2017-12-15 18:37:43.946360: step 46580, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:18m:07s remains)
INFO - root - 2017-12-15 18:37:50.375836: step 46590, loss = 0.27, batch loss = 0.16 (11.7 examples/sec; 0.682 sec/batch; 54h:11m:56s remains)
INFO - root - 2017-12-15 18:37:56.858302: step 46600, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:25m:09s remains)
2017-12-15 18:37:57.400663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7264318 -8.0351143 -7.7337341 -7.9225125 -8.199543 -7.7772064 -6.1062775 -4.5537686 -3.0175958 -2.8566084 -3.260839 -4.0403738 -4.4858322 -4.976882 -6.1551704][-5.8221188 -6.3219328 -6.606391 -7.2340412 -7.746417 -7.5635862 -7.21939 -5.8118954 -3.5118876 -3.04806 -3.6239223 -5.0498457 -5.1752377 -5.1328096 -6.3562531][-5.5327263 -5.6751709 -6.0121078 -7.1221628 -7.9014487 -8.05562 -7.6421719 -6.4859509 -5.393714 -5.1973071 -5.3720121 -6.7270794 -7.0450115 -7.009706 -7.1408596][-5.6485767 -5.6707964 -5.5336647 -5.4760513 -6.2101464 -6.3165331 -5.8267565 -5.116724 -4.2147045 -5.0762916 -6.4318857 -7.9166517 -7.8492336 -7.9388695 -8.54777][-6.1787329 -5.2019048 -4.206296 -3.7053854 -2.8889642 -2.2747202 -1.5257907 -1.0211239 -0.44888544 -2.0826259 -4.2276669 -6.4026356 -7.3435698 -7.8135643 -8.6302395][-4.9671807 -3.8692036 -3.3453803 -1.9962544 -0.47668505 0.69312572 2.2282429 2.6432314 3.2631407 1.5365982 -0.40269661 -3.3999133 -5.10452 -6.2967715 -7.6466131][-5.0739527 -3.1277695 -1.3553243 -0.0010485649 1.1609392 2.6598158 4.4705172 4.59869 4.6166477 2.7930107 0.83723164 -1.5199199 -3.1326494 -4.7428112 -6.76221][-4.0000339 -2.855463 -1.939178 0.13259459 2.0061712 3.0135288 4.3666735 4.7148743 4.8220463 2.6806402 0.49142933 -1.9529562 -3.0887852 -4.002286 -5.6370468][-4.39639 -3.4184103 -2.6435409 -0.69939756 1.0119352 2.2085161 2.9561262 2.7859068 3.4656181 1.590703 -1.186202 -3.1272774 -3.802397 -4.2053471 -5.390027][-6.0956497 -5.1855135 -4.5579882 -2.8615127 -1.2641425 -0.077278137 0.25640869 -0.024497032 0.2807312 -1.3311739 -2.4012871 -4.5553584 -5.7879744 -6.0774379 -6.6881528][-7.6848221 -7.5026956 -7.0723453 -6.4931965 -5.94054 -4.330657 -3.1102538 -3.0001659 -3.454205 -5.1281624 -5.5847974 -7.0751319 -7.301806 -7.7143636 -8.9272661][-9.7049208 -9.1417627 -8.8450937 -8.1241436 -7.6267347 -7.4628868 -7.0911627 -6.3358312 -5.8669863 -7.1219544 -7.5432272 -8.1380329 -7.4536266 -7.744904 -8.5748749][-10.029872 -10.60258 -10.288074 -9.3561773 -8.5190964 -7.8930464 -7.5007486 -7.8745284 -8.2420921 -8.6929636 -8.646451 -8.77869 -7.669847 -6.7886758 -7.233633][-9.0073128 -9.1684227 -9.6069345 -9.26599 -8.58938 -8.3149729 -7.5967307 -7.4253531 -7.9349484 -8.4133 -8.46363 -8.5251541 -8.1447668 -7.4037156 -6.6419563][-7.2747936 -7.1998067 -7.402719 -7.1499062 -7.4916949 -7.6342058 -7.2881455 -7.5084429 -7.0968876 -7.1021972 -7.58966 -7.7171779 -7.8682475 -7.5379677 -6.9965506]]...]
INFO - root - 2017-12-15 18:38:03.756944: step 46610, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 49h:26m:06s remains)
INFO - root - 2017-12-15 18:38:10.279135: step 46620, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 49h:38m:42s remains)
INFO - root - 2017-12-15 18:38:16.648186: step 46630, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 49h:29m:49s remains)
INFO - root - 2017-12-15 18:38:23.017396: step 46640, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 49h:49m:52s remains)
INFO - root - 2017-12-15 18:38:29.455812: step 46650, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 51h:15m:47s remains)
INFO - root - 2017-12-15 18:38:35.878651: step 46660, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 50h:51m:23s remains)
INFO - root - 2017-12-15 18:38:42.306191: step 46670, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.658 sec/batch; 52h:16m:57s remains)
INFO - root - 2017-12-15 18:38:48.721907: step 46680, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:17m:23s remains)
INFO - root - 2017-12-15 18:38:55.068424: step 46690, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:58m:11s remains)
INFO - root - 2017-12-15 18:39:01.516837: step 46700, loss = 0.37, batch loss = 0.26 (12.1 examples/sec; 0.661 sec/batch; 52h:30m:52s remains)
2017-12-15 18:39:02.047254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9503973 -3.8777504 -3.9600189 -4.087471 -4.3056087 -4.0687609 -4.0017672 -3.4173088 -2.8876724 -3.7496853 -4.5090456 -5.221046 -6.0578113 -6.7784572 -7.6483345][-3.4610133 -3.5971828 -4.2306356 -4.7794867 -4.7360091 -4.3602033 -4.0759563 -3.6396017 -3.3298955 -4.2023773 -5.0163593 -5.9574771 -6.9736552 -7.6768694 -8.42727][-2.3790145 -2.3593016 -2.9339666 -3.1089191 -2.8124676 -2.1880274 -1.8887315 -1.5921259 -1.3560667 -2.3011603 -2.9626594 -4.1058674 -5.3581023 -6.0788507 -6.9707851][-2.7392616 -2.4294348 -2.3217068 -2.0804338 -1.630384 -1.0527382 -0.47225189 -0.30669022 -0.24104881 -1.5665665 -2.3157902 -3.4968615 -5.0193005 -5.7427015 -6.6673923][-2.9892397 -2.2824287 -1.688035 -1.0802727 -0.34668589 0.34003544 0.95828152 1.113616 1.0004921 -0.54674387 -1.6006036 -2.884418 -4.4541273 -5.4872217 -6.49932][-3.8742373 -2.8434448 -1.9266448 -1.1943817 -0.23955441 0.79821014 1.6775265 1.630271 1.5570469 -0.33267736 -1.6247048 -2.9789691 -4.56247 -5.5893736 -6.6317029][-4.7010412 -3.9259391 -2.7851787 -1.4434195 0.0032176971 1.2033262 2.1939411 2.1532459 1.9385624 -0.25298882 -1.777205 -3.2886076 -4.9656954 -5.8922968 -6.9686284][-4.6895328 -3.7291842 -2.5761962 -1.1269689 0.27293491 1.2726307 2.1150723 2.1040144 1.8578262 -0.43904829 -2.198616 -3.90873 -5.4773788 -6.4061236 -7.4145107][-5.0065565 -4.2034092 -3.1457214 -1.7342329 -0.31931448 0.33992481 0.83922195 1.2785263 1.5368729 -0.71692562 -2.508749 -4.16126 -5.8514128 -6.9571614 -7.9308305][-5.4147305 -4.7870312 -3.8752468 -2.4855971 -1.1277561 -0.759233 -0.31125879 0.14591551 0.47912502 -1.9210811 -3.4662657 -5.0204563 -6.5350175 -7.6554441 -8.7825289][-6.5618191 -6.0270081 -5.27369 -4.14941 -3.1133189 -2.7380562 -2.4757853 -2.1496162 -1.7944684 -3.8284893 -5.2746878 -6.4241066 -7.5426779 -8.2819271 -8.9553814][-6.6934085 -6.0513372 -5.6305256 -4.9135666 -4.2281284 -4.0528469 -4.015008 -3.8803864 -3.7196116 -5.0292144 -5.931447 -6.5692525 -7.3192663 -8.171649 -8.7998457][-7.0911431 -6.6773806 -6.1778526 -5.7067213 -5.2203245 -5.1977844 -5.3172083 -5.4105473 -5.288662 -6.0237765 -6.70319 -6.8080678 -7.2948394 -7.7844505 -8.03927][-6.5653973 -6.3750267 -6.2310162 -5.5866628 -5.0650625 -5.2378607 -5.4423895 -5.6526237 -5.7123137 -6.3787966 -6.3485246 -6.2452574 -6.3030353 -6.63591 -6.825407][-7.9494057 -7.7119694 -7.3512859 -6.7852845 -6.2801142 -6.3527565 -6.6363788 -6.8783131 -7.0241318 -6.9904394 -6.98456 -6.8321667 -6.4792562 -6.3186169 -6.1301832]]...]
INFO - root - 2017-12-15 18:39:08.567358: step 46710, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 51h:35m:32s remains)
INFO - root - 2017-12-15 18:39:14.945814: step 46720, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 50h:02m:14s remains)
INFO - root - 2017-12-15 18:39:21.284401: step 46730, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.622 sec/batch; 49h:22m:39s remains)
INFO - root - 2017-12-15 18:39:27.770965: step 46740, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 49h:56m:05s remains)
INFO - root - 2017-12-15 18:39:34.147970: step 46750, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 50h:51m:51s remains)
INFO - root - 2017-12-15 18:39:40.584227: step 46760, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 50h:27m:16s remains)
INFO - root - 2017-12-15 18:39:46.922709: step 46770, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 50h:53m:03s remains)
INFO - root - 2017-12-15 18:39:53.324591: step 46780, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 51h:41m:32s remains)
INFO - root - 2017-12-15 18:39:59.779865: step 46790, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 51h:26m:13s remains)
INFO - root - 2017-12-15 18:40:06.222326: step 46800, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:34m:26s remains)
2017-12-15 18:40:06.738821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3217449 -0.90270519 -0.71832705 -0.57818747 -0.67912006 -0.9131608 -1.1177335 -1.1661015 -1.1665411 -2.7549686 -4.078619 -5.6214657 -6.700284 -6.9091682 -7.5741434][-1.7173285 -1.7832737 -1.8863311 -2.0065546 -2.00905 -2.0534511 -2.0800858 -2.1888442 -2.1258106 -3.8635197 -5.0273809 -6.4812164 -7.6589241 -8.2135429 -8.5666571][-3.5749884 -3.4161634 -3.2708759 -3.0859356 -2.9272313 -2.8447752 -2.7600365 -2.8355126 -2.9718542 -4.7466435 -5.8087525 -7.218945 -8.1695166 -8.8050327 -9.2136707][-4.3158083 -4.1553183 -3.8881757 -3.3528466 -2.8995929 -2.6015406 -2.2855606 -2.3885674 -2.5105009 -4.1943588 -5.3014464 -6.9274483 -7.852706 -8.2482758 -8.7765636][-5.56248 -4.816258 -3.9771225 -3.1734786 -2.447083 -1.8143411 -1.3822813 -1.4976449 -1.6493273 -3.2032456 -4.2190065 -5.8116293 -6.974699 -7.3886681 -8.0148067][-6.621779 -5.8311253 -4.9251823 -3.4548059 -2.0669365 -1.1947618 -0.5057025 -0.57441711 -0.7249341 -2.1636972 -3.1821094 -4.6428308 -5.8241639 -6.4707713 -7.1847258][-6.6044464 -4.9462137 -3.4883027 -2.2679105 -1.1392927 -0.21909761 0.36841393 0.53984261 0.60052586 -0.770206 -1.8401065 -3.1920204 -4.1574736 -5.0283618 -5.8461118][-5.5914822 -4.1884341 -2.8722911 -1.2582397 0.27958393 0.94621563 1.3850241 1.5145988 1.4985914 0.096752644 -0.83619738 -2.2801442 -3.3532176 -3.8859551 -4.4896207][-4.940815 -3.29249 -1.8286161 -0.51839304 0.59484863 1.0271883 1.2809219 1.305294 1.3693295 0.059363365 -0.73207283 -2.0750461 -2.8453012 -3.2287393 -4.0323439][-5.1400328 -3.2628107 -1.702949 -0.52087021 0.62843323 1.128685 1.0646982 1.1326342 1.0469503 -0.66955042 -1.6071067 -2.5372586 -3.042357 -3.3506212 -3.7783811][-5.8490167 -4.3708668 -3.1480923 -1.7454982 -0.95759678 -0.62325621 -0.577785 -0.60291481 -0.62591124 -2.1453929 -3.2723918 -4.1058092 -4.3802443 -4.381424 -4.3775997][-7.7557845 -6.3793836 -5.0889721 -3.7500854 -2.8032799 -2.4110045 -2.195436 -2.3388543 -2.5904999 -3.8718042 -4.6302729 -5.2062769 -5.3554764 -5.1452904 -4.9424582][-8.8734818 -7.8727646 -6.5852752 -5.6059017 -4.5279441 -3.7693615 -3.3743691 -3.4710526 -3.6353016 -4.7073193 -5.1042013 -5.5236197 -5.8019381 -5.2817039 -5.038475][-9.2583113 -8.2505217 -7.066144 -5.7276611 -4.8161764 -4.0010738 -3.6684337 -3.8870945 -4.2176313 -4.8667231 -5.1304588 -5.2240047 -5.3675938 -4.9784288 -4.7080383][-9.1476345 -8.6002941 -7.9492383 -6.8385682 -5.6867647 -5.0191107 -4.5376635 -4.6838441 -4.9630423 -5.33522 -5.62886 -5.7793188 -5.5243473 -5.1431446 -4.8726635]]...]
INFO - root - 2017-12-15 18:40:13.215443: step 46810, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 51h:28m:37s remains)
INFO - root - 2017-12-15 18:40:19.600108: step 46820, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 49h:55m:09s remains)
INFO - root - 2017-12-15 18:40:25.998245: step 46830, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 52h:03m:29s remains)
INFO - root - 2017-12-15 18:40:32.326334: step 46840, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 49h:51m:44s remains)
INFO - root - 2017-12-15 18:40:38.727179: step 46850, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 52h:17m:02s remains)
INFO - root - 2017-12-15 18:40:45.135290: step 46860, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 50h:29m:50s remains)
INFO - root - 2017-12-15 18:40:51.578628: step 46870, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 49h:15m:10s remains)
INFO - root - 2017-12-15 18:40:57.995896: step 46880, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 51h:13m:07s remains)
INFO - root - 2017-12-15 18:41:04.476299: step 46890, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 51h:36m:54s remains)
INFO - root - 2017-12-15 18:41:10.913341: step 46900, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 51h:39m:52s remains)
2017-12-15 18:41:11.429264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0485291 -3.330833 -3.0708742 -3.5936055 -4.7254577 -4.7853003 -4.4083128 -4.92483 -5.35571 -7.3725309 -8.4511423 -8.9489346 -10.29436 -10.545923 -10.964227][-3.0521007 -3.2449107 -3.2724652 -3.7727528 -4.2563705 -4.2474508 -4.2907119 -4.8740268 -5.5812578 -8.2918911 -8.9797411 -9.5342073 -10.62352 -10.622805 -10.758018][-2.7294812 -2.404088 -2.405302 -2.3075051 -2.8405266 -2.3584747 -2.2629123 -3.0668535 -4.0996408 -6.8669496 -8.1082325 -8.9207306 -9.99259 -10.185934 -9.941515][-3.1574016 -1.9842968 -1.2356796 -1.0917854 -1.0277176 -0.91329479 -0.69908524 -1.2384977 -1.9569373 -4.4937677 -5.4817905 -6.6646442 -8.3596039 -8.7941914 -9.02327][-3.6202312 -2.2364025 -1.1187334 -0.64596176 -0.36950493 0.65737438 1.2746792 0.89037228 0.70942116 -1.7071614 -3.3234982 -4.6086612 -6.3560529 -7.4206514 -8.40915][-3.465065 -2.2861571 -1.7784562 -0.73239946 0.1012392 1.1490812 1.9723921 2.2502174 1.7926435 -0.7397728 -1.9579525 -3.3866591 -5.5404034 -6.4633961 -7.2799067][-2.5111909 -2.1014447 -1.0672188 -0.30683374 0.36766815 1.4679384 2.4720488 2.4988489 3.1205111 0.82253265 -0.74301767 -2.2222967 -4.1371684 -5.4666085 -6.6752205][-1.882956 -1.6206288 -0.73769093 -0.12460136 1.0032396 1.7215023 2.5602007 3.3220921 3.9867887 1.6808453 0.23376989 -1.543735 -4.1293092 -5.8712163 -7.0059128][-1.5319967 -1.360435 -0.87491894 -0.01768446 0.81157112 1.7017632 2.20432 2.377058 2.67338 0.21527052 -1.2691321 -2.7211609 -5.05584 -6.40059 -7.3158908][-1.5135312 -1.5474243 -1.3130584 -0.48003387 0.35692978 0.97371292 1.2730322 1.2634821 1.2142239 -1.541368 -2.5098681 -3.5709786 -5.3961964 -6.7999554 -7.9471903][-2.9738488 -2.9133806 -2.4843731 -1.7081308 -1.4454937 -0.9506259 -0.1602664 -0.31389809 -0.38704252 -3.11133 -4.247241 -4.9336433 -6.2738314 -7.4087481 -7.7531376][-4.7880507 -3.9213376 -3.4144573 -2.9404588 -2.7624168 -2.3220668 -2.1907501 -2.0377302 -1.9013796 -4.1572471 -5.0493755 -5.6477084 -6.4651265 -7.0886831 -7.3613129][-6.0699878 -5.7230177 -4.6970892 -3.98916 -3.3801594 -3.2745156 -3.035429 -3.0426583 -3.1444249 -4.6836715 -6.0249767 -5.9806294 -6.6698322 -6.7078953 -6.7724595][-6.314384 -5.6330457 -5.0989947 -4.5384235 -3.9125905 -3.8160462 -3.6680527 -3.6544166 -3.6766305 -4.7461815 -5.362114 -5.8976173 -6.4191217 -6.3267641 -6.5083756][-6.5440712 -5.9834318 -5.2855616 -4.8098974 -4.4987726 -4.7214746 -4.7280722 -4.7445831 -4.8070068 -5.037199 -5.1033072 -5.3683958 -5.4277248 -5.5223155 -5.5248308]]...]
INFO - root - 2017-12-15 18:41:17.842172: step 46910, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 50h:30m:26s remains)
INFO - root - 2017-12-15 18:41:24.181121: step 46920, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 51h:14m:40s remains)
INFO - root - 2017-12-15 18:41:30.683685: step 46930, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 51h:47m:17s remains)
INFO - root - 2017-12-15 18:41:36.968187: step 46940, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 49h:17m:16s remains)
INFO - root - 2017-12-15 18:41:43.356438: step 46950, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 49h:19m:07s remains)
INFO - root - 2017-12-15 18:41:49.789960: step 46960, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 49h:46m:30s remains)
INFO - root - 2017-12-15 18:41:56.190984: step 46970, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 50h:10m:49s remains)
INFO - root - 2017-12-15 18:42:02.586065: step 46980, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 51h:07m:42s remains)
INFO - root - 2017-12-15 18:42:09.004772: step 46990, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 50h:56m:27s remains)
INFO - root - 2017-12-15 18:42:15.414246: step 47000, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 50h:53m:40s remains)
2017-12-15 18:42:15.918731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.644053 -5.915081 -6.5585709 -7.4473977 -8.1376152 -8.6544514 -8.621274 -7.800868 -6.8962922 -6.3239222 -6.7275205 -7.323019 -8.1652164 -8.78809 -9.330574][-6.0514903 -6.2338009 -7.0666184 -7.924922 -8.8146772 -9.7754049 -9.8102083 -9.267252 -8.3384457 -8.0432062 -8.416338 -8.9393606 -9.7570972 -10.289352 -10.696901][-4.8708811 -4.9414206 -5.42294 -6.0788665 -6.9019732 -7.7509432 -7.8802366 -7.337316 -6.8280792 -6.8798337 -7.7602725 -8.8641787 -9.5617266 -10.185259 -10.288851][-4.3221788 -3.9005904 -3.8091521 -4.39152 -4.5545053 -4.6118 -4.5149946 -4.0025005 -3.4018683 -3.273036 -4.7273574 -6.5877895 -7.6516628 -8.5197821 -9.1072645][-4.301918 -3.1902924 -2.3296232 -2.1536551 -1.555654 -0.76534081 -0.26013756 -0.033540726 0.54564762 -0.058454037 -1.9492149 -4.119895 -5.892045 -7.0120659 -8.0017433][-3.9823072 -3.2272577 -2.0833445 -0.8710928 0.38371277 1.8537321 3.0457954 3.2066498 3.3162336 1.8666553 -0.075291157 -2.2212529 -4.0268769 -5.5161886 -6.4717536][-4.8401871 -4.1176109 -2.6779618 -0.78639841 0.96837425 2.704422 3.9779778 4.4358759 4.4563 2.9195328 0.093900681 -2.3175888 -4.2875471 -5.8207436 -6.8212285][-6.4681883 -5.6620035 -4.0982318 -1.6718369 0.57232189 2.2519503 3.9958897 4.5776625 4.2679176 3.1138525 -0.061754704 -2.809577 -4.6577573 -5.905529 -6.4513035][-7.3077364 -6.3381205 -4.9751225 -2.6362109 -0.3294549 1.6069679 3.2250481 3.5176792 3.4731941 2.7421494 -0.13302183 -3.03292 -4.7279429 -5.985229 -6.1372442][-7.5644855 -6.8960271 -5.9120369 -4.3183475 -2.3934383 -0.061954975 1.5445204 1.7560434 1.3406858 0.27340269 -1.7880206 -3.9090323 -5.2132983 -6.2485714 -6.8033066][-7.5834332 -7.2247658 -6.4101286 -5.146049 -3.8243179 -2.5308967 -1.2963781 -0.79429579 -0.84093332 -2.2649403 -3.7428458 -4.9103761 -6.0955863 -6.9973598 -7.5670986][-7.0699911 -6.6927872 -6.2100654 -5.0946617 -4.0477591 -3.3016043 -2.6822505 -2.4165955 -2.2362738 -3.9447322 -4.7981944 -5.3569446 -6.3834305 -6.8339128 -7.0456924][-5.7911234 -5.29988 -4.7462139 -4.3162718 -3.7533331 -3.3775907 -2.9722881 -3.4069605 -3.8432887 -4.4967122 -5.5769005 -5.9250402 -6.4129124 -6.1838036 -6.5549555][-4.8488588 -4.4602633 -4.0980515 -3.6906047 -3.7666461 -3.547255 -3.4129462 -4.2133722 -4.7819462 -4.6560793 -5.4538612 -5.4087567 -5.7425041 -5.4291582 -5.0316944][-5.2239666 -4.4871912 -3.7206225 -3.7752895 -4.163888 -4.4461069 -4.6467791 -4.998353 -5.4654894 -5.3786573 -5.3684006 -5.4703956 -5.7374372 -5.4624128 -4.8694606]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 18:42:22.391687: step 47010, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 50h:01m:58s remains)
INFO - root - 2017-12-15 18:42:28.801191: step 47020, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 51h:15m:13s remains)
INFO - root - 2017-12-15 18:42:35.223462: step 47030, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 51h:04m:30s remains)
INFO - root - 2017-12-15 18:42:41.669812: step 47040, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 51h:48m:46s remains)
INFO - root - 2017-12-15 18:42:48.116848: step 47050, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 52h:23m:24s remains)
INFO - root - 2017-12-15 18:42:54.532079: step 47060, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 50h:11m:40s remains)
INFO - root - 2017-12-15 18:43:00.886409: step 47070, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 49h:59m:47s remains)
INFO - root - 2017-12-15 18:43:07.266152: step 47080, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 49h:20m:48s remains)
INFO - root - 2017-12-15 18:43:13.725693: step 47090, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:21m:01s remains)
INFO - root - 2017-12-15 18:43:20.081450: step 47100, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 49h:52m:33s remains)
2017-12-15 18:43:20.581955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.21682 -5.3311481 -5.266119 -4.8553782 -4.2509975 -3.2556677 -1.6953664 -0.63974094 -0.033869743 -0.82586813 -1.7705927 -2.179379 -3.0398602 -4.7566471 -6.4688711][-4.6902218 -4.9060154 -4.4059258 -4.09551 -3.9391353 -3.6541686 -3.1864305 -2.2639542 -1.1102509 -1.2253904 -2.0815568 -2.7606969 -3.9158018 -4.9293795 -5.6383247][-3.5658965 -3.7797508 -3.4630594 -3.2557192 -2.7807479 -2.3925147 -2.2523742 -1.5670447 -0.59052134 -0.78490734 -1.6923251 -2.3073711 -3.4878221 -4.6775627 -5.37479][-3.7025416 -3.5134277 -3.0698824 -2.938839 -2.4431667 -1.9045906 -1.0447645 -0.054283619 0.54667187 -0.21659613 -1.3048077 -1.9270582 -2.9554262 -4.687808 -5.9001][-4.1995177 -2.8728604 -1.9755039 -2.0103173 -1.5764532 -1.0415339 -0.34185219 0.24767876 0.76105785 -0.060456753 -1.2437568 -1.8285642 -2.6116972 -4.2303762 -5.3718529][-4.1167593 -3.1853409 -2.1639147 -1.2867551 -0.51418686 0.13631392 1.0253296 1.4120855 1.6273413 0.68527889 -0.63691378 -1.4330506 -2.6212091 -4.1595278 -5.37446][-4.3010063 -3.4905806 -2.5831389 -1.7102618 -0.67484474 -0.044949055 0.55882645 1.0581799 1.4687786 0.40996075 -0.9856472 -1.5230522 -2.7906394 -4.5545878 -5.5810823][-4.7395706 -3.7422442 -3.0127254 -2.1238728 -1.2485623 -0.578959 0.22004986 0.64438438 1.1353016 0.41924858 -1.1790819 -1.94905 -3.0666876 -4.4701996 -5.3480682][-4.3034883 -3.2992382 -2.5206513 -1.8988252 -1.2576485 -0.54111671 0.43798733 0.67365837 0.96875477 0.34090614 -1.1796961 -1.9481502 -3.2928867 -4.5514789 -5.2025166][-3.8593459 -3.111659 -2.5409794 -1.5065613 -0.61381578 -0.24662495 0.3486433 0.59027481 0.78724575 -0.31351852 -1.6137967 -1.9564161 -2.8924847 -3.8321404 -4.083673][-5.1873646 -4.6920953 -3.9438345 -3.1545186 -2.493784 -1.9452701 -1.2463641 -1.1348543 -1.0752358 -2.1885381 -2.8869729 -2.9562869 -3.3316398 -3.5928144 -3.4761786][-6.4436688 -5.971673 -5.4862432 -5.1164737 -4.6297045 -3.8920846 -3.1423917 -3.092701 -2.8970203 -3.4887767 -3.3113732 -3.0436845 -3.3806973 -3.7417874 -3.6017475][-6.7490625 -6.5055385 -6.0648761 -6.1985283 -6.0642438 -5.4376283 -4.6161222 -4.6124492 -4.502789 -4.5775762 -3.630661 -2.7879138 -2.8123245 -3.0635438 -2.9950695][-6.9144764 -6.926991 -6.7823977 -7.3190465 -7.4813724 -7.1373482 -6.5219178 -6.2378659 -5.7033019 -5.6012011 -4.9683332 -3.8599396 -3.0097408 -2.8991475 -2.7882333][-7.5356717 -7.6291227 -7.3673768 -8.0251637 -8.7194548 -8.8374748 -8.4363756 -7.8645897 -7.0679488 -6.4435444 -5.8387995 -5.0905018 -4.3175535 -3.7879519 -3.23808]]...]
INFO - root - 2017-12-15 18:43:26.916912: step 47110, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 51h:06m:56s remains)
INFO - root - 2017-12-15 18:43:33.284661: step 47120, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 49h:09m:21s remains)
INFO - root - 2017-12-15 18:43:39.640238: step 47130, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 50h:15m:51s remains)
INFO - root - 2017-12-15 18:43:46.081563: step 47140, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:11m:27s remains)
INFO - root - 2017-12-15 18:43:52.463859: step 47150, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 50h:37m:39s remains)
INFO - root - 2017-12-15 18:43:58.928277: step 47160, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 50h:21m:41s remains)
INFO - root - 2017-12-15 18:44:05.337525: step 47170, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 49h:53m:42s remains)
INFO - root - 2017-12-15 18:44:11.725237: step 47180, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 50h:11m:12s remains)
INFO - root - 2017-12-15 18:44:18.114967: step 47190, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 53h:00m:08s remains)
INFO - root - 2017-12-15 18:44:24.520316: step 47200, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.631 sec/batch; 50h:00m:03s remains)
2017-12-15 18:44:25.020835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6254549 -6.0609159 -6.1901083 -6.1255436 -6.2247109 -5.9825239 -5.8258667 -5.6973515 -5.3455286 -5.5875607 -5.7805567 -6.6086187 -6.7331357 -6.8254404 -6.49802][-3.8291373 -4.1924553 -4.8569636 -5.2188349 -5.802598 -6.1151638 -6.4973035 -6.7273884 -6.851182 -7.2635212 -7.409555 -8.0860634 -8.0000534 -7.3995686 -6.5595951][-3.1045933 -2.9307537 -3.1451244 -3.54396 -4.2579317 -4.7576246 -5.320694 -6.2145996 -6.3160157 -6.5004654 -6.7305965 -7.9155493 -8.2864456 -7.97963 -7.3488407][-1.919035 -2.0671186 -2.9528656 -3.0480223 -3.0390558 -2.8411951 -3.0551672 -3.6351237 -3.9266098 -4.6749034 -5.205719 -6.6413 -7.37688 -7.3820333 -7.0257845][-2.7919307 -2.2972474 -2.3282523 -2.1599936 -2.2586436 -2.1439395 -1.6240597 -1.2194424 -0.95723057 -1.645401 -2.7229319 -4.7900977 -5.9322414 -6.2692881 -6.7624903][-3.1215706 -2.749578 -2.8822007 -2.2756619 -1.4154415 -0.28661537 0.40892982 1.0122204 1.3987417 0.84791183 -0.058695316 -2.4946661 -4.152483 -5.3650284 -6.0701718][-5.1305089 -4.3752327 -3.0793748 -2.0603166 -1.279532 -0.17942953 1.0318289 2.0751028 2.8316011 2.323329 1.2668409 -1.3878641 -3.256 -4.8867884 -5.4452248][-4.9841547 -4.30149 -3.7472055 -2.8972902 -1.7811337 -0.0012378693 1.2841921 2.1983747 3.1657419 2.4342146 1.5203133 -1.3348279 -3.4605932 -4.7656446 -5.2324562][-5.5236759 -4.9075441 -4.4915476 -3.3486075 -2.2255425 -0.76418638 0.46926975 1.8263369 2.7331066 2.0411634 0.69095039 -2.5914774 -4.3476868 -5.7068415 -6.217814][-6.2781343 -5.92915 -5.5326614 -4.4611549 -3.5612402 -2.2557168 -1.2323833 0.012295723 0.83528423 -0.24391651 -1.3820138 -3.9488435 -5.8940349 -7.0965819 -7.4440546][-6.9268012 -6.9113541 -6.832026 -5.8264766 -4.9416423 -3.8987932 -3.0273781 -2.5678644 -2.1951003 -2.6827893 -3.4157643 -5.1197348 -6.3372178 -7.4873476 -8.1123772][-7.5285277 -7.4554462 -7.2418652 -6.6044226 -6.2136021 -5.6349516 -4.8594937 -4.3934278 -3.958221 -4.5759039 -5.2626252 -6.3208694 -7.2381992 -7.5956674 -7.702003][-7.8155313 -7.9881306 -7.9134784 -7.307991 -6.772747 -6.1866226 -5.7948475 -6.0173483 -6.1907096 -6.3902769 -6.3542147 -6.6153255 -7.0494027 -7.0590839 -7.1061106][-7.2484741 -7.5780077 -7.5624309 -6.9336214 -6.2973671 -5.7524586 -5.4371676 -5.5173678 -5.6564713 -6.2153749 -6.731504 -7.3273988 -7.6385522 -7.0412731 -6.7479568][-7.8769069 -7.8299837 -7.4565682 -7.1211677 -6.7146573 -6.0961313 -5.9386258 -6.2434373 -6.532495 -7.1411843 -7.4954886 -7.68022 -7.4056592 -7.0529156 -6.6874285]]...]
INFO - root - 2017-12-15 18:44:31.371805: step 47210, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 52h:08m:50s remains)
INFO - root - 2017-12-15 18:44:37.783657: step 47220, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 49h:33m:17s remains)
INFO - root - 2017-12-15 18:44:44.231641: step 47230, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:09m:45s remains)
INFO - root - 2017-12-15 18:44:50.592365: step 47240, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 50h:11m:22s remains)
INFO - root - 2017-12-15 18:44:56.959225: step 47250, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 48h:46m:12s remains)
INFO - root - 2017-12-15 18:45:03.357198: step 47260, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 52h:02m:55s remains)
INFO - root - 2017-12-15 18:45:09.815354: step 47270, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 50h:07m:09s remains)
INFO - root - 2017-12-15 18:45:16.257987: step 47280, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 50h:28m:04s remains)
INFO - root - 2017-12-15 18:45:22.726462: step 47290, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 50h:52m:24s remains)
INFO - root - 2017-12-15 18:45:29.179477: step 47300, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 50h:24m:22s remains)
2017-12-15 18:45:29.659287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3600121 -4.538044 -5.4003444 -5.3288031 -5.243269 -5.0882673 -4.5138078 -3.6797853 -2.7275438 -2.6346197 -3.9538305 -5.0276804 -4.9497566 -7.0516796 -7.4676762][-4.9959631 -5.5629454 -6.5052338 -6.8678961 -6.8398438 -6.747704 -6.375773 -5.566546 -4.3966651 -3.4636073 -4.4770317 -6.3702893 -7.3559508 -8.0956125 -8.0294428][-5.8329215 -5.6446724 -5.8792486 -6.5006585 -6.8698158 -6.5979834 -6.7231393 -6.1141944 -5.3684416 -4.1122742 -4.4712124 -5.4261222 -6.223156 -8.3532009 -8.3817558][-4.3138313 -4.9340992 -5.4771581 -5.03551 -5.2926207 -5.5801549 -5.4920282 -4.9963646 -4.4529352 -3.857305 -4.5175495 -5.7131376 -5.9464135 -7.1413856 -7.4363036][-4.6914964 -3.8789375 -3.2318153 -3.0509219 -3.1822767 -3.0635977 -3.197525 -3.0356846 -2.4467607 -1.9361491 -3.1755056 -4.5066791 -5.4219809 -6.749733 -6.8780141][-2.6673484 -2.8099442 -2.2215996 -1.2890077 -0.79741383 -0.48777056 -0.39278841 -0.30087996 -0.1984272 0.12072134 -1.1110611 -2.8918304 -4.2038393 -5.8026385 -6.3113441][-2.0680642 -2.0420551 -1.5549259 -0.70266342 0.1759367 0.86169434 1.4213343 1.6312742 1.7006102 1.6823997 -0.26488352 -2.6177335 -4.105279 -5.7655573 -6.2628055][-2.2752323 -1.8083563 -1.4310389 -0.77863407 0.17618132 0.82332993 1.3999987 2.5318584 3.2466393 3.0002985 0.892128 -1.7193661 -3.5986829 -5.7790194 -6.2290964][-3.1941619 -3.1563506 -2.6046782 -1.8972635 -1.1705799 0.089632988 1.0830441 1.5199118 1.9391594 2.3214302 1.012991 -1.5243683 -3.1123009 -5.4879236 -6.7353606][-3.7027273 -3.4916711 -2.994339 -2.1842914 -1.6440501 -0.95121241 -0.16952896 1.1551027 2.2614059 1.4515467 -0.35711718 -2.3616843 -3.4311485 -5.2994518 -6.4809146][-5.5503645 -4.9832916 -4.3480773 -3.3405566 -2.3201718 -1.6148562 -1.1307588 -0.60067225 -0.16209078 0.079231739 -0.63965416 -2.7896481 -4.2559862 -5.6663775 -6.5866823][-5.9283447 -5.7898135 -5.41792 -4.7262039 -3.6003361 -2.629909 -2.4852967 -2.1386251 -1.9363675 -1.624774 -2.1488428 -3.1952734 -4.0349236 -5.3514695 -6.32657][-5.9215226 -5.4722633 -4.6724091 -4.5034337 -3.9318542 -2.9842277 -2.6898108 -2.4752107 -2.4391384 -2.4992385 -2.848722 -3.1103487 -3.6041865 -4.5666871 -5.7797976][-5.1771994 -4.6812487 -3.976907 -3.862148 -3.5859909 -3.1445613 -2.8922558 -2.9804602 -3.2071166 -3.0795641 -3.3417225 -3.3597236 -3.7623892 -4.3533325 -4.7964768][-5.1151733 -5.5851059 -4.9911776 -4.9096928 -4.7636356 -4.3405361 -4.2848897 -4.4115238 -4.2043724 -4.2580719 -4.671339 -4.7648864 -4.584197 -4.9794397 -5.3503046]]...]
INFO - root - 2017-12-15 18:45:35.973304: step 47310, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:20m:37s remains)
INFO - root - 2017-12-15 18:45:42.405642: step 47320, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 51h:32m:52s remains)
INFO - root - 2017-12-15 18:45:48.740180: step 47330, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 50h:27m:21s remains)
INFO - root - 2017-12-15 18:45:55.181273: step 47340, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 51h:25m:35s remains)
INFO - root - 2017-12-15 18:46:01.594304: step 47350, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.664 sec/batch; 52h:33m:31s remains)
INFO - root - 2017-12-15 18:46:07.996354: step 47360, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 50h:45m:58s remains)
INFO - root - 2017-12-15 18:46:14.514621: step 47370, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 51h:56m:06s remains)
INFO - root - 2017-12-15 18:46:20.864454: step 47380, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 51h:09m:26s remains)
INFO - root - 2017-12-15 18:46:27.245596: step 47390, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 49h:56m:30s remains)
INFO - root - 2017-12-15 18:46:33.610019: step 47400, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 50h:33m:18s remains)
2017-12-15 18:46:34.151793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9983315 -3.5349894 -3.8530204 -4.0918112 -4.3653965 -4.7960682 -5.3127279 -5.3769522 -5.4352026 -6.4841661 -8.5869055 -8.4486256 -8.7031937 -8.2490234 -8.8020916][-3.4886427 -3.7958832 -3.9188945 -4.0100451 -3.8165433 -3.7774394 -4.2011633 -4.7021313 -4.1815071 -5.3097439 -7.6903148 -8.10211 -8.97647 -8.4045582 -9.3543577][-2.959033 -2.5752668 -2.4020858 -2.8099127 -2.8617334 -2.51756 -2.48345 -2.5503116 -2.9025106 -4.0602245 -5.9739394 -6.0185833 -7.5347395 -7.662797 -8.677968][-3.6449542 -3.1098886 -2.5384412 -1.767077 -1.6485558 -1.6088467 -1.1297603 -0.62361813 -1.0271888 -2.7681699 -4.972229 -5.0047565 -6.2352028 -6.191577 -7.2481236][-4.1743956 -3.4374785 -2.3778162 -1.370831 -1.3826294 -1.0498743 -0.18862772 -0.032238007 0.073870659 -1.2604022 -2.885922 -3.2514043 -5.00834 -5.4350872 -6.39123][-3.0528021 -1.9576578 -1.3183646 -0.5142417 0.45358467 1.1667442 1.7705145 1.4288073 1.6027431 0.4995575 -0.88424969 -1.2431769 -3.238276 -3.960078 -5.2340417][-2.5314488 -1.4102421 -0.02041769 0.68564892 1.7607746 2.8823957 3.6419592 3.173089 2.9428825 1.1441069 -0.42639256 -1.2645965 -3.0778546 -3.8571622 -4.84534][-1.3935733 -1.0558786 0.097865582 1.5718002 2.8336086 2.9140444 3.6024313 3.8924208 3.8634958 1.9257669 -0.74416065 -1.905447 -3.2160153 -3.824646 -4.922081][-1.7675409 -1.2177882 -0.85743237 1.0370378 2.832058 2.92387 2.5457783 2.9550209 3.6239309 1.571209 -1.3875275 -2.4255471 -4.3728704 -4.850523 -5.61531][-3.5989308 -2.5825253 -1.9234085 -1.6646852 -0.37933016 0.52934837 1.0477552 0.94391823 0.83982754 -0.45999479 -2.8594689 -3.7360058 -5.0591478 -5.150959 -6.3319454][-5.7850361 -5.6801386 -4.3347597 -4.2886152 -3.6621876 -2.7959681 -1.7373819 -1.6845684 -1.9075451 -3.6820564 -6.1141019 -6.0304241 -6.5514297 -6.571877 -7.35317][-6.9531589 -6.1526937 -5.524466 -5.8664966 -5.7222881 -5.7044029 -4.8773336 -4.4464846 -3.9016881 -4.7983284 -7.1644568 -7.2692895 -7.4292636 -7.4580479 -7.9977818][-7.5573392 -7.2995739 -6.5737538 -6.5800524 -6.7278728 -6.7994771 -6.4084358 -6.4872017 -6.4977541 -6.9275265 -7.8932471 -7.4010115 -6.8536444 -6.8829 -7.6057591][-6.8063335 -7.0421915 -6.3146453 -6.0786262 -6.0169592 -5.8900948 -5.9385195 -6.0375104 -5.8985491 -5.7650881 -6.9583454 -6.87807 -6.4552641 -5.7371364 -5.3410378][-7.9375529 -8.0005159 -7.3581305 -7.2712965 -7.2197 -7.0766244 -6.587914 -6.4075432 -6.7142048 -6.8116612 -6.9955835 -6.4003172 -6.2353539 -6.0698853 -5.6240196]]...]
INFO - root - 2017-12-15 18:46:40.487545: step 47410, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 49h:39m:55s remains)
INFO - root - 2017-12-15 18:46:46.867304: step 47420, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 50h:14m:20s remains)
INFO - root - 2017-12-15 18:46:53.198431: step 47430, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 51h:59m:38s remains)
INFO - root - 2017-12-15 18:46:59.493576: step 47440, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 50h:46m:52s remains)
INFO - root - 2017-12-15 18:47:05.929790: step 47450, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 49h:29m:19s remains)
INFO - root - 2017-12-15 18:47:12.333858: step 47460, loss = 0.37, batch loss = 0.25 (12.8 examples/sec; 0.626 sec/batch; 49h:34m:09s remains)
INFO - root - 2017-12-15 18:47:18.732004: step 47470, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 52h:22m:50s remains)
INFO - root - 2017-12-15 18:47:25.110048: step 47480, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 49h:40m:34s remains)
INFO - root - 2017-12-15 18:47:31.574194: step 47490, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.623 sec/batch; 49h:17m:05s remains)
INFO - root - 2017-12-15 18:47:37.979103: step 47500, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 51h:19m:13s remains)
2017-12-15 18:47:38.487194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3655596 -3.9604342 -3.8997884 -4.1341753 -3.9038184 -3.3033338 -2.3073235 -1.2776041 -2.580338 -3.0023918 -5.0320396 -4.107326 -5.5294995 -5.0152516 -4.8913531][-4.642086 -3.9238207 -3.2308602 -2.3921771 -1.8087759 -1.8470721 -3.2243371 -2.5266747 -0.73602724 -0.92928028 -4.1528287 -4.1671648 -6.124114 -5.016952 -5.1791639][-3.729254 -3.4424691 -2.4835167 -1.9601326 -1.8778524 -0.6572156 1.6377239 0.51301861 -1.5618124 -1.6180916 -2.486742 -2.32409 -4.9398761 -4.3619461 -4.5694084][-1.4703813 -0.83291054 -2.8716154 -3.0273752 -0.92205381 -0.853045 -1.3072343 -0.4919343 0.8818531 -0.139575 -3.2540278 -3.8290033 -4.3005147 -4.5003405 -4.6947737][-5.9681225 -2.2206964 0.98295879 0.74694157 0.091187954 0.14534664 0.63045216 -0.85617352 -1.0744967 -1.2114305 -2.7580295 -2.8734245 -5.2150316 -3.9763882 -4.0564461][-2.4971709 -2.9066133 -2.8425541 -0.42232227 3.0531902 3.3275986 2.8934174 1.9238825 1.0703564 0.26904774 -2.603415 -2.4752941 -4.3463116 -4.3604259 -4.4407187][-2.1408534 0.21575499 1.2339468 1.1184072 0.79575253 1.9565544 3.3632689 3.9129038 3.3891478 1.9335985 -1.4563565 -2.1097174 -4.4441743 -3.8952932 -3.8871832][-0.95181656 -0.8130517 0.17081785 2.0379744 3.077343 3.318284 2.6273375 2.2230225 2.902113 2.4464045 -0.99718523 -1.7728715 -4.3590841 -5.047286 -5.2284412][-0.90511608 0.21740818 0.80138969 1.4085979 2.2603951 3.0143118 3.909358 3.3649645 2.9444294 1.9986944 -0.011811256 -0.37203693 -3.8880081 -4.1633596 -4.8183374][-3.2425671 -2.6205 -1.7185783 -0.87770987 0.36047077 1.4777985 1.758132 1.8986397 2.7865057 2.5508747 0.74542332 -0.44980955 -3.2147107 -3.1798563 -4.0394688][-4.8598528 -4.3041849 -3.9711592 -3.6565628 -2.7865977 -2.0277481 -0.65755081 -0.73869324 -0.9826169 -0.21559143 -2.1615043 -2.0021825 -3.8802273 -4.0085883 -4.9060354][-8.1934433 -6.7082405 -5.3476391 -4.9205322 -4.1478257 -4.0164566 -3.5324874 -2.7923965 -3.1498175 -2.1920824 -3.8590834 -4.9720736 -6.343513 -5.8238621 -6.1930361][-6.3744493 -7.4440575 -7.6607304 -6.7697821 -5.511251 -4.9907093 -4.3873415 -4.4751081 -4.28052 -3.0060391 -4.9188213 -4.580965 -5.2665586 -6.084446 -5.9586091][-4.776207 -4.8060169 -4.7883787 -6.27138 -6.7263165 -5.8920989 -4.8541737 -5.0456638 -5.1472616 -5.487608 -5.7187815 -5.9347668 -5.7490187 -5.2441521 -6.0677223][-4.8224411 -4.5334311 -4.4148197 -4.1775379 -4.4138603 -5.7709494 -5.962606 -5.7131772 -5.79749 -5.9388957 -5.9610138 -6.285265 -6.5499973 -6.461411 -6.5766826]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 18:47:46.248386: step 47510, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 52h:04m:15s remains)
INFO - root - 2017-12-15 18:47:52.631076: step 47520, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:20m:09s remains)
INFO - root - 2017-12-15 18:47:59.009176: step 47530, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 49h:29m:31s remains)
INFO - root - 2017-12-15 18:48:05.401194: step 47540, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:55m:27s remains)
INFO - root - 2017-12-15 18:48:11.846576: step 47550, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:44m:32s remains)
INFO - root - 2017-12-15 18:48:18.236754: step 47560, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 50h:32m:28s remains)
INFO - root - 2017-12-15 18:48:24.649334: step 47570, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 49h:52m:23s remains)
INFO - root - 2017-12-15 18:48:31.000963: step 47580, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 50h:27m:24s remains)
INFO - root - 2017-12-15 18:48:37.501967: step 47590, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 52h:22m:00s remains)
INFO - root - 2017-12-15 18:48:43.862991: step 47600, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 49h:37m:26s remains)
2017-12-15 18:48:44.350114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5421867 -3.8221333 -3.9472158 -4.0789804 -4.3678846 -3.7573586 -2.7291951 -2.3101473 -1.5552273 -2.8321691 -2.889884 -3.9597323 -5.5295992 -5.7560062 -6.831636][-3.1415052 -3.4153261 -3.4314137 -4.1449227 -3.9707861 -3.4184885 -2.8611798 -2.0836515 -1.0897598 -2.8161716 -2.5884032 -4.2452407 -5.5955629 -5.0307856 -6.1946507][-2.9462657 -2.8570213 -2.9909816 -3.1179924 -2.903161 -2.6966639 -2.4505973 -2.0766892 -1.553988 -2.8845448 -2.6253262 -4.3003006 -5.4318104 -5.6617517 -6.5046005][-3.9984081 -3.2970867 -3.5017824 -2.9797869 -2.162663 -1.6404648 -1.4169259 -1.2211552 -0.96658468 -2.742662 -2.9008117 -3.9262371 -4.6245775 -5.13631 -6.0950484][-4.1152153 -2.68512 -1.7560692 -1.1091299 -0.91551733 -0.778728 0.13823748 0.62676334 1.0298767 -1.1669354 -1.9892812 -3.6153879 -4.891716 -5.2053375 -6.2503662][-3.2790732 -2.4321494 -1.9735947 -1.0855508 0.094796181 0.86192131 1.4650908 1.3630276 1.5813141 -0.18828869 -0.970129 -2.7107553 -3.980799 -4.5041084 -5.572546][-3.0871921 -2.2887273 -2.2188683 -0.83851337 0.3717289 1.5372658 2.5296011 2.1784563 2.3526545 0.46347713 -0.33695984 -1.7601495 -3.1099868 -4.1416645 -5.5102367][-3.606214 -2.2266479 -1.1309562 0.57909107 1.8441496 2.7533197 4.0077848 3.833869 4.1679449 2.60532 1.4514313 -0.34865761 -2.3534064 -3.8574805 -4.9631581][-3.8919029 -3.0119205 -1.7572522 -0.10240698 1.0147591 1.7094221 2.365015 2.8971186 3.4929953 1.8027935 0.92706966 -1.2453818 -2.9526844 -3.7109118 -4.8119659][-5.1477275 -4.2887115 -2.765923 -0.767344 0.37410164 0.68171787 1.2344666 1.0828838 1.628726 0.43016243 -0.69810724 -2.880743 -4.0152187 -3.9872353 -4.4698362][-6.5294738 -4.8834853 -4.143342 -3.3792553 -2.0055566 -1.0404773 -0.761302 -0.9714489 -0.84707403 -2.3333559 -2.9696918 -4.2412386 -5.1119914 -5.1335888 -5.5644121][-8.4438305 -7.7246103 -6.4835739 -5.4332819 -4.8544006 -4.0073128 -3.4464412 -3.0174475 -2.7125092 -4.015656 -4.3082809 -5.2684336 -5.9481606 -5.5407906 -6.1237988][-9.17926 -7.5628567 -6.6877632 -5.9974532 -4.9124665 -4.2882519 -3.7340403 -3.3914638 -3.50915 -3.9053702 -4.5441437 -5.5856266 -5.9366169 -5.5589776 -5.6735706][-7.9032822 -7.7123213 -7.5281591 -5.7154074 -4.6214318 -4.4733939 -4.3972812 -4.73535 -4.9928246 -5.4649277 -5.5509405 -5.5129967 -6.0495186 -5.947928 -5.9987254][-10.707065 -9.34704 -8.1025543 -7.9385815 -7.9644713 -7.3124542 -6.4551787 -6.9740443 -7.4700208 -7.6492295 -7.4604349 -7.6503854 -7.5386205 -7.2928219 -7.0196762]]...]
INFO - root - 2017-12-15 18:48:50.812818: step 47610, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 51h:02m:40s remains)
INFO - root - 2017-12-15 18:48:57.207452: step 47620, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 50h:17m:01s remains)
INFO - root - 2017-12-15 18:49:03.578652: step 47630, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 51h:29m:22s remains)
INFO - root - 2017-12-15 18:49:10.027448: step 47640, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 50h:21m:14s remains)
INFO - root - 2017-12-15 18:49:16.458293: step 47650, loss = 0.37, batch loss = 0.25 (12.5 examples/sec; 0.641 sec/batch; 50h:41m:16s remains)
INFO - root - 2017-12-15 18:49:22.918012: step 47660, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 51h:08m:06s remains)
INFO - root - 2017-12-15 18:49:29.501473: step 47670, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 51h:28m:18s remains)
INFO - root - 2017-12-15 18:49:35.937681: step 47680, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.671 sec/batch; 53h:06m:46s remains)
INFO - root - 2017-12-15 18:49:42.282519: step 47690, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.618 sec/batch; 48h:51m:52s remains)
INFO - root - 2017-12-15 18:49:48.727921: step 47700, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 52h:23m:45s remains)
2017-12-15 18:49:49.254678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1781549 -3.8485756 -4.475894 -4.4309959 -4.042625 -3.570755 -3.1381631 -2.6024737 -2.0848393 -1.9930253 -2.6700253 -4.81148 -5.8260269 -5.9257731 -6.6883736][-3.9828887 -4.4315553 -4.737 -5.1347322 -5.4218769 -4.5736818 -3.466794 -3.0032 -2.5298777 -2.7453489 -3.5625982 -5.3916531 -6.3325138 -6.4786639 -7.0683875][-3.9568193 -4.2534 -4.2945795 -4.151679 -4.1600137 -3.9102633 -3.4100347 -2.7232137 -2.1357074 -2.6599874 -3.8174288 -5.7127018 -6.5553403 -7.1107078 -7.918716][-4.2732792 -3.9404087 -3.6866927 -3.5371628 -2.9241633 -2.353837 -1.9739747 -1.6917753 -1.5702734 -1.9622402 -3.2066407 -5.6217389 -6.870111 -7.3527241 -7.6463437][-4.3242807 -4.1802168 -3.7328951 -2.6727333 -1.6630292 -0.88044262 -0.50783873 -0.35624504 -0.37222672 -1.5197196 -3.2213221 -5.7132711 -6.6846113 -7.0371571 -7.4829168][-5.9489207 -5.2941103 -4.6004753 -3.09417 -1.5930839 -0.31633854 0.76274681 1.1206303 0.823205 -0.56650972 -2.7016358 -5.7379036 -7.3198438 -7.4241419 -7.7938852][-6.1569281 -5.6598306 -4.8939524 -2.7332449 -0.8723855 0.579916 2.204073 2.5677004 2.5840626 1.3960485 -1.0452332 -4.2813597 -6.1387033 -6.8624024 -7.3394628][-5.4289694 -4.888114 -3.9700935 -2.2214723 -0.34525156 2.1543379 3.8766947 3.7823591 3.586585 2.4880171 0.24063969 -3.3244758 -5.1831937 -6.0453634 -6.685164][-5.2976036 -4.9654965 -4.3628836 -2.7504139 -0.98433781 1.2068968 3.0419292 3.3160887 3.6153574 1.845499 -0.62032843 -3.5796113 -5.1994834 -5.9796028 -6.77366][-6.1165028 -5.8345852 -5.2750845 -3.9601426 -2.6871505 -1.3327851 0.062697887 0.67702484 0.96696281 -0.8955369 -2.6007628 -4.8571215 -5.9524088 -6.0653219 -6.8935251][-7.3748651 -7.5221834 -6.8104944 -5.4794235 -4.195035 -3.0581589 -2.1086187 -1.4992127 -1.2313952 -2.5033979 -3.5640702 -6.0261807 -6.6782732 -6.3190355 -6.6323891][-6.6341863 -6.5731688 -6.5274286 -6.0213814 -5.4743938 -4.3070793 -3.6190805 -3.3891206 -3.2918386 -3.7915328 -4.1342459 -6.0135679 -6.391046 -6.5384817 -7.15781][-8.0929661 -7.3974423 -6.9736791 -6.4617734 -5.9511681 -5.8961859 -5.5525813 -5.1964731 -5.20866 -5.449821 -5.6554813 -6.6603684 -6.7084322 -6.8431282 -7.36036][-8.3025875 -8.0856762 -7.5838952 -6.3477278 -5.6074867 -5.4683285 -5.2321911 -5.6845551 -5.9027548 -6.3944159 -6.3940024 -6.5225339 -6.6179423 -6.7581782 -7.1418591][-8.208724 -9.2918682 -9.6443882 -8.5418835 -7.6288886 -6.9901652 -6.9730768 -7.1587238 -7.3357344 -7.5470076 -7.4070187 -7.1366596 -6.8007374 -6.491281 -6.3658319]]...]
INFO - root - 2017-12-15 18:49:55.618483: step 47710, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 49h:38m:39s remains)
INFO - root - 2017-12-15 18:50:02.068110: step 47720, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:23m:06s remains)
INFO - root - 2017-12-15 18:50:08.434168: step 47730, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 51h:19m:21s remains)
INFO - root - 2017-12-15 18:50:14.847197: step 47740, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.675 sec/batch; 53h:24m:18s remains)
INFO - root - 2017-12-15 18:50:21.243073: step 47750, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.613 sec/batch; 48h:27m:59s remains)
INFO - root - 2017-12-15 18:50:27.722868: step 47760, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 52h:37m:44s remains)
INFO - root - 2017-12-15 18:50:34.145785: step 47770, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 50h:25m:00s remains)
INFO - root - 2017-12-15 18:50:40.597758: step 47780, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 50h:29m:33s remains)
INFO - root - 2017-12-15 18:50:46.997574: step 47790, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 50h:08m:03s remains)
INFO - root - 2017-12-15 18:50:53.312439: step 47800, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 50h:29m:02s remains)
2017-12-15 18:50:53.875276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3528419 -3.3420253 -3.4352403 -3.3220639 -4.0424795 -4.6860962 -5.0161037 -4.8484278 -4.4297552 -6.2817416 -6.6010919 -7.2836618 -7.797112 -8.1717625 -8.50092][-3.1229677 -3.5436711 -3.8328562 -3.7709785 -4.2072573 -4.7591543 -4.8852749 -4.8203487 -4.5987387 -6.5544095 -6.7030988 -7.2543244 -7.6517692 -8.04459 -8.5643253][-2.4944353 -2.4541445 -2.5799279 -2.5881844 -3.0376892 -3.2069359 -3.0443583 -2.7894325 -2.7055387 -5.0313072 -5.6998682 -6.4647808 -6.9335566 -7.4570947 -8.0199175][-3.1209393 -2.6190324 -2.184792 -1.5365877 -1.4473505 -1.52594 -1.2900352 -0.98255157 -0.82367849 -3.243372 -4.069644 -5.0265551 -5.6223631 -6.3119841 -7.5155616][-3.5072379 -2.6472325 -1.9326243 -1.4191742 -0.92393351 -0.32648897 0.31179523 0.50141811 0.47827721 -1.7889843 -2.7338109 -4.01655 -5.1645894 -5.7371368 -6.5050292][-3.4265041 -2.8542814 -2.2453156 -1.5133 -0.77944326 -0.02346468 0.85965061 1.1558266 1.1919012 -1.2005081 -2.1183438 -3.2143555 -4.4019089 -5.257164 -6.3853159][-3.0931067 -2.8041611 -2.0673065 -1.4886518 -0.72461414 0.26136065 1.0082817 1.1359758 1.1516132 -0.92827225 -1.671124 -2.6952639 -3.78903 -4.9812994 -5.7354355][-2.9498072 -2.70111 -2.3417225 -1.5243421 -0.65012217 -0.014525414 0.69863033 0.98177814 0.899663 -1.127789 -1.8277984 -2.6820889 -3.677444 -4.8660889 -5.7271571][-3.4042373 -2.7609639 -1.99754 -1.2399979 -0.57127523 0.076129913 0.42060661 0.41294003 0.29061365 -1.5927811 -2.4542375 -3.586278 -4.8238921 -5.9047627 -6.5052166][-4.3851333 -3.6012135 -2.5161347 -1.4437723 -0.36374903 -0.073633671 0.10676622 0.055152893 -0.27106428 -2.3952308 -3.0652242 -4.2158356 -5.2831984 -6.4270997 -7.1175885][-5.8042064 -4.9398546 -3.7788901 -2.6055694 -1.6182985 -0.95118904 -0.6416316 -0.87898445 -1.1030197 -2.8777514 -3.6942339 -4.8513541 -5.75219 -6.6080003 -7.2552752][-6.4143114 -5.471261 -4.205884 -2.9888535 -1.8249736 -1.1044698 -0.50224209 -0.5902071 -0.90537977 -2.5359139 -3.3953629 -4.923336 -5.8335276 -6.6593585 -7.4119983][-6.5795383 -5.9138641 -4.8277588 -3.6820173 -2.4565392 -1.6459665 -0.8674655 -0.43586493 -0.32946968 -1.5523019 -2.3533516 -3.6060514 -4.8852878 -5.6863041 -6.4211369][-6.5080996 -5.6690016 -4.9416943 -4.1994796 -3.2028847 -2.4971385 -1.6606421 -0.94740391 -0.77313566 -0.94516659 -1.2880363 -2.4254107 -3.5577483 -4.6289034 -5.4982619][-7.273489 -6.65783 -5.8336186 -5.4600916 -4.841363 -3.9727321 -3.298614 -2.6508331 -2.37225 -2.143033 -2.0877862 -2.7049861 -3.5823026 -4.4030762 -5.1061087]]...]
INFO - root - 2017-12-15 18:51:00.234010: step 47810, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 52h:29m:39s remains)
INFO - root - 2017-12-15 18:51:06.606929: step 47820, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 52h:12m:34s remains)
INFO - root - 2017-12-15 18:51:13.059022: step 47830, loss = 0.28, batch loss = 0.16 (11.8 examples/sec; 0.680 sec/batch; 53h:47m:55s remains)
INFO - root - 2017-12-15 18:51:19.441334: step 47840, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 50h:16m:27s remains)
INFO - root - 2017-12-15 18:51:25.787385: step 47850, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:19m:40s remains)
INFO - root - 2017-12-15 18:51:32.287914: step 47860, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 52h:24m:27s remains)
INFO - root - 2017-12-15 18:51:38.703712: step 47870, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 50h:03m:04s remains)
INFO - root - 2017-12-15 18:51:45.103148: step 47880, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 49h:41m:28s remains)
INFO - root - 2017-12-15 18:51:51.509176: step 47890, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 49h:06m:53s remains)
INFO - root - 2017-12-15 18:51:57.864041: step 47900, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 50h:30m:47s remains)
2017-12-15 18:51:58.346415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4890513 -5.4234438 -4.56818 -3.9644156 -3.6592021 -3.1695595 -3.1213536 -3.2882071 -3.2586932 -4.7339535 -5.0444117 -6.0074544 -6.3578563 -7.1477156 -6.5017419][-5.7669258 -4.592701 -4.2810755 -4.0053163 -3.22437 -2.4170942 -2.756506 -2.8073969 -2.7199149 -3.6580558 -4.1008453 -5.0759134 -5.781745 -7.1865463 -6.8052449][-5.6355734 -4.8015971 -4.1435061 -3.25498 -2.922081 -2.8303738 -2.5000906 -2.0231385 -1.9667726 -3.584837 -3.9607584 -4.2824173 -4.6947851 -6.0061626 -5.8195176][-4.8597069 -4.3557367 -3.6855283 -2.8893762 -2.6488113 -1.947825 -1.215416 -1.1497035 -1.4579854 -2.6550946 -3.0268974 -3.9842446 -4.7897367 -6.3640022 -5.0698953][-4.2122183 -3.6275415 -2.8532948 -2.8653283 -1.7660313 -0.92823124 -1.2908154 -1.1876512 -1.0962429 -2.2991209 -2.5362473 -3.7609727 -4.0326653 -5.1599779 -4.83745][-3.2739706 -2.4469776 -2.3907413 -1.4593015 -0.39580631 -0.27155828 0.15871954 0.059903145 -0.35453129 -2.0943656 -2.4321036 -3.309629 -3.5222602 -4.9976683 -5.1313915][-2.183249 -1.7452888 -1.0676537 -0.33508873 0.0061750412 0.946187 0.98751259 0.67953587 0.22094965 -1.7447338 -2.5618134 -3.6489687 -4.3026152 -5.9093237 -5.0751467][-0.50576735 -0.11916494 -0.21247911 0.08719492 0.78110981 1.4945107 1.3639183 1.4257164 0.74993706 -0.80884886 -1.7516465 -3.0249782 -4.4243894 -6.4376903 -5.8516164][-0.17575264 0.28691912 0.76562214 0.79333878 0.63101292 0.94696045 1.1633625 0.89426804 0.53853226 -0.97053194 -2.145072 -3.5460558 -4.1953874 -5.7568512 -5.9824815][-1.3221183 -0.420043 0.24144745 0.48946667 0.58284664 0.48753834 0.570446 0.43591213 0.26055861 -1.7452145 -2.5304837 -3.8555627 -5.1580205 -6.0614004 -5.5990419][-1.5189543 -0.95519066 -0.74297857 -0.67055368 -0.19930077 0.14590597 -0.057181358 -0.35121489 -0.66168165 -2.3413525 -2.9279804 -4.3614345 -5.4578695 -6.3724775 -6.4529738][-3.7898645 -3.3785472 -2.2625194 -1.5989347 -1.922092 -1.5150828 -1.3564191 -1.1751199 -1.5558982 -3.3271065 -3.8271368 -4.7605224 -5.2948685 -6.358964 -6.8964758][-4.8472805 -3.8959043 -3.3542528 -3.5424962 -3.3290906 -2.918716 -2.8715429 -2.3922062 -2.5180926 -3.4133487 -4.3566408 -5.276741 -5.7825279 -7.0107684 -7.0770154][-4.9482288 -4.3494482 -4.415659 -4.0561123 -3.9096355 -3.9748325 -3.7924166 -3.349215 -3.6822906 -4.0153589 -5.0517316 -5.6490145 -5.7524028 -6.71883 -6.3909349][-6.2777905 -5.8525577 -5.0506787 -4.7211962 -4.7276087 -4.6418877 -4.8364959 -4.9537 -4.6924119 -4.6830492 -5.3592367 -5.9968176 -6.3119287 -6.6010275 -7.0012574]]...]
INFO - root - 2017-12-15 18:52:04.765677: step 47910, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 50h:58m:51s remains)
INFO - root - 2017-12-15 18:52:11.258712: step 47920, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 50h:13m:27s remains)
INFO - root - 2017-12-15 18:52:17.697474: step 47930, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 51h:50m:04s remains)
INFO - root - 2017-12-15 18:52:24.042303: step 47940, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 52h:13m:08s remains)
INFO - root - 2017-12-15 18:52:30.483940: step 47950, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.671 sec/batch; 53h:01m:38s remains)
INFO - root - 2017-12-15 18:52:36.867138: step 47960, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 50h:14m:09s remains)
INFO - root - 2017-12-15 18:52:43.218121: step 47970, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 51h:00m:18s remains)
INFO - root - 2017-12-15 18:52:49.574490: step 47980, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 51h:19m:40s remains)
INFO - root - 2017-12-15 18:52:56.040608: step 47990, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:39m:14s remains)
INFO - root - 2017-12-15 18:53:02.452042: step 48000, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 51h:21m:48s remains)
2017-12-15 18:53:03.008055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2026405 -2.0827308 -2.0637293 -1.8558335 -1.7315798 -1.6354823 -1.7005816 -1.4236245 -1.1313233 -2.5594234 -3.1283789 -4.4124746 -5.4058776 -5.752471 -6.0922627][-3.3021431 -3.1093063 -3.3738222 -3.4536715 -3.4806213 -2.9197354 -2.1582804 -2.1813135 -1.6555872 -2.594048 -2.8149624 -3.8117259 -4.7383823 -5.5728045 -6.23566][-3.1395869 -3.2356067 -3.3063769 -3.3436117 -3.5986023 -3.3804727 -2.9059052 -2.6279602 -2.1119742 -2.9683709 -3.3730536 -4.1295719 -4.3438549 -4.9076223 -5.6007867][-3.9287581 -3.5481515 -3.1311145 -2.0678668 -1.6049132 -1.1771765 -1.0220985 -1.5828204 -1.4428091 -2.9290829 -3.8730228 -4.3573132 -4.9312172 -5.1977849 -5.3305893][-5.29419 -4.1551304 -2.5181589 -1.8029609 -1.1521769 -0.33459091 0.079191685 -0.0068221092 -0.13920975 -1.7377214 -2.6040425 -3.8577428 -4.94121 -5.3033895 -5.5789213][-3.8778434 -2.7327647 -1.8373699 -0.763535 0.019201279 0.59079838 0.9984684 1.0510244 1.2371082 -0.14068317 -1.0974221 -2.1968217 -3.5741858 -4.525228 -4.9833326][-2.3921695 -1.4944763 -0.14874172 0.67903423 1.3683395 1.878005 2.1224174 1.8375282 1.4272633 0.10035706 -0.66111517 -1.7778106 -2.7350931 -3.5873218 -4.7664976][-1.1281552 -0.080161095 0.29717112 1.1974201 2.3921528 3.0129576 3.2220955 2.8972301 2.4727449 0.5101614 -0.62903976 -1.4824972 -2.7319794 -3.9242949 -4.7841711][-0.98971415 -0.45980167 0.61206532 1.4312515 1.9092331 2.293354 2.2701731 2.1663513 1.7573948 0.04737711 -1.0859213 -2.3348622 -3.6016154 -4.6829 -5.8658428][-2.1907439 -1.0007558 -0.27218533 0.72343349 1.4965715 1.6468134 1.4029884 1.2457495 1.2873354 -0.28235245 -1.3102326 -2.5213838 -4.1236668 -5.1117086 -5.890511][-2.7638025 -3.199749 -2.4461212 -1.7275448 -0.99506187 -0.19394636 0.14123726 0.27341461 0.22480202 -0.58247137 -1.9936934 -3.4808025 -4.6226282 -6.0396724 -7.1702414][-5.4120417 -4.1072893 -3.805819 -3.5606065 -2.9751377 -2.4778843 -1.9760251 -1.3516464 -0.58101273 -1.7581148 -3.182003 -4.846005 -6.3677545 -7.253099 -7.5025992][-6.250227 -6.1007776 -5.7984161 -5.2433457 -4.2703609 -3.4839926 -2.7693791 -2.149395 -1.8770409 -3.1903567 -4.7572927 -5.7548184 -7.2548161 -8.1659422 -7.9954729][-7.5363193 -7.0706444 -6.6100993 -5.8837109 -5.2502842 -4.4866209 -4.0932198 -3.4964957 -3.1701999 -3.988873 -5.2312517 -6.9699173 -7.8995309 -8.25208 -8.6280832][-8.4867086 -8.0179033 -7.458549 -7.0230327 -6.7545266 -6.3739409 -5.803421 -5.4624043 -5.4132242 -6.1609182 -7.5889478 -7.8217692 -7.9575443 -8.3900127 -8.5931635]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 18:53:09.465618: step 48010, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 51h:42m:23s remains)
INFO - root - 2017-12-15 18:53:15.891138: step 48020, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 50h:49m:55s remains)
INFO - root - 2017-12-15 18:53:22.258658: step 48030, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 52h:35m:25s remains)
INFO - root - 2017-12-15 18:53:28.647316: step 48040, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 49h:53m:52s remains)
INFO - root - 2017-12-15 18:53:35.041264: step 48050, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.614 sec/batch; 48h:31m:18s remains)
INFO - root - 2017-12-15 18:53:41.352812: step 48060, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 48h:54m:32s remains)
INFO - root - 2017-12-15 18:53:47.806471: step 48070, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 50h:35m:27s remains)
INFO - root - 2017-12-15 18:53:54.129344: step 48080, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.618 sec/batch; 48h:51m:42s remains)
INFO - root - 2017-12-15 18:54:00.544251: step 48090, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 51h:04m:24s remains)
INFO - root - 2017-12-15 18:54:07.025634: step 48100, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 50h:49m:31s remains)
2017-12-15 18:54:07.540103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8816204 -3.8559737 -2.3732977 -1.9341612 -1.9400067 -2.4169073 -2.9569697 -3.1419082 -3.4055958 -4.1707859 -5.1547947 -6.5530314 -6.7886524 -6.606854 -6.4247389][-4.5428524 -4.5121546 -3.5806231 -2.7560811 -1.9929008 -1.6557088 -1.9592705 -2.3269076 -2.6919656 -3.7739074 -4.9476681 -6.6422882 -6.9080062 -6.61014 -6.1038609][-2.7319794 -2.7491636 -2.6429071 -2.3613091 -1.7739325 -1.0444756 -1.0952201 -0.98841 -1.1073203 -2.0576959 -3.1875176 -5.2152228 -6.2206793 -6.392745 -6.3574963][-2.4111214 -2.1112309 -1.6748857 -0.98832321 -1.154376 -0.20477104 0.48400211 0.56749725 0.55136871 -0.67586327 -1.9964666 -4.169589 -5.0973463 -5.2716255 -5.3338337][-2.8936768 -2.4207115 -1.7279491 -0.12136745 0.31334972 1.0437822 1.2905293 1.4387388 1.6587067 0.38279247 -1.1052365 -3.5685754 -4.6139669 -5.2175541 -5.231246][-3.0363078 -2.3010778 -1.5799317 -0.14401722 1.0195637 2.1937704 2.7070789 2.8895369 2.9095583 1.4351711 -0.45273829 -3.3191514 -4.4578724 -5.2549353 -5.7433238][-2.9460797 -1.7133813 -0.6296258 0.52524185 1.5101051 2.5494604 3.2779541 3.5788841 3.3368969 1.4880724 -0.56021976 -3.5604181 -5.2725039 -5.9592228 -6.0728631][-2.3895907 -1.3847117 -0.70684147 0.88260746 1.936594 2.5886211 3.1857719 3.4533463 3.2434053 1.1655092 -0.74062967 -3.4452381 -5.1328287 -5.8502874 -6.1667838][-3.0325785 -2.5710607 -1.7071657 -0.5802145 0.44137955 1.2488356 1.6481686 1.6255379 1.415885 0.16332865 -1.3101344 -3.9957783 -5.3989887 -5.8250446 -5.6531339][-3.6883726 -3.5828891 -3.68295 -3.1135554 -2.1557603 -1.1845059 -0.80329323 -0.81756735 -0.610754 -1.6119142 -2.8613944 -4.8723559 -5.7598147 -5.5790186 -5.5782719][-5.5933952 -5.2086134 -4.6761603 -4.0565205 -3.5375457 -2.664968 -2.4352188 -2.4135547 -2.3254685 -3.3221269 -4.3270636 -5.309885 -5.8074126 -6.0361385 -6.0013032][-6.2697034 -5.3653884 -5.0393 -4.8844004 -4.1132717 -3.6282811 -3.5607681 -3.6588593 -3.9624262 -5.2272315 -5.7527714 -6.5569429 -7.4531994 -7.3368783 -6.8400803][-5.6262951 -5.2935772 -4.8375421 -4.3851485 -4.2813358 -3.9073732 -3.9422784 -4.1672697 -4.4070139 -5.1668639 -6.1987314 -7.1631117 -7.211987 -6.7527819 -7.0312324][-5.5304651 -4.6253095 -3.9867396 -3.1639404 -2.0862937 -2.0203495 -2.4940009 -2.631011 -2.8351054 -3.809906 -4.3426819 -5.3674674 -6.4618053 -6.3626943 -6.0205932][-5.7962689 -4.5245695 -3.3276458 -2.8438387 -2.9835048 -3.1411424 -3.3445878 -4.0953941 -4.4389572 -4.585228 -5.0603094 -5.7103782 -6.478672 -6.99319 -7.382761]]...]
INFO - root - 2017-12-15 18:54:14.035146: step 48110, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.623 sec/batch; 49h:13m:02s remains)
INFO - root - 2017-12-15 18:54:20.453779: step 48120, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 51h:22m:10s remains)
INFO - root - 2017-12-15 18:54:26.834065: step 48130, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:38m:33s remains)
INFO - root - 2017-12-15 18:54:33.227369: step 48140, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.620 sec/batch; 48h:57m:00s remains)
INFO - root - 2017-12-15 18:54:39.658903: step 48150, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:37m:08s remains)
INFO - root - 2017-12-15 18:54:46.066559: step 48160, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 48h:59m:56s remains)
INFO - root - 2017-12-15 18:54:52.488056: step 48170, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 50h:18m:47s remains)
INFO - root - 2017-12-15 18:54:58.928147: step 48180, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 49h:38m:13s remains)
INFO - root - 2017-12-15 18:55:05.278659: step 48190, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 50h:25m:17s remains)
INFO - root - 2017-12-15 18:55:11.674504: step 48200, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.615 sec/batch; 48h:32m:08s remains)
2017-12-15 18:55:12.196032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8010521 -1.3130288 -1.6884947 -2.8481789 -2.9899092 -3.3004894 -3.367342 -3.5087137 -3.5391388 -4.3509979 -4.3387623 -5.5442228 -6.2701159 -7.1793265 -8.1123238][-1.485826 -1.2746358 -1.6830463 -3.002624 -3.5754228 -3.9161232 -3.9683013 -3.7212677 -3.6009536 -4.2249246 -4.2236795 -5.3925152 -6.1495104 -6.9483666 -7.6975589][-1.2825193 -0.89663839 -1.2450123 -2.5754042 -3.2699142 -3.5932021 -3.4488106 -3.2607384 -2.9303966 -3.3721027 -3.3228145 -4.4133043 -5.2388172 -5.9448156 -6.6061831][-2.0079169 -1.4957728 -1.4577951 -2.0622144 -2.0654778 -1.7355118 -1.1815672 -1.0702772 -1.0583901 -1.7645502 -2.0702419 -3.4024835 -4.5880122 -5.6305542 -6.4485912][-2.3963919 -1.3277354 -1.0294852 -1.4533415 -1.2671542 -0.6833601 0.0028948784 0.33846855 0.28525877 -0.774034 -1.2563868 -2.8248963 -4.0920773 -5.3637824 -6.4679232][-2.7981892 -1.5914054 -1.3426485 -1.046494 -0.818172 -0.052001476 0.82034111 1.0400953 1.1928759 -0.044681072 -0.84717941 -2.4539657 -3.6859365 -5.0285473 -6.2930675][-2.7030315 -1.6886435 -1.2627077 -0.66495419 -0.17528248 0.572917 1.7105379 1.8227863 1.8545942 0.49336433 -0.51473379 -2.5337043 -4.0516644 -5.4220743 -6.5363865][-2.2841148 -1.1320677 -0.71969795 0.21809483 0.83295441 1.6855774 2.5964136 2.5136518 2.2747402 0.63357353 -0.44933319 -2.3964849 -3.979831 -5.3300915 -6.3689442][-2.2010503 -1.2160378 -0.94147539 -0.018390179 0.64799786 1.2970028 1.9083176 1.9215107 1.8903685 0.27994156 -0.62848473 -2.5040565 -3.9564605 -5.1756434 -5.9816775][-2.6645684 -1.8311219 -1.5109782 -0.95796824 -0.337049 0.43741417 1.0592413 0.99948788 1.026042 -0.43697643 -1.2368517 -2.5004606 -3.5862374 -4.8850179 -5.7498055][-4.1773558 -3.3966026 -3.2101636 -2.8138018 -2.3460798 -1.3474331 -0.57489157 -0.7071681 -0.76833725 -2.0843949 -2.6878471 -3.5806665 -4.2571359 -4.9947577 -5.6907024][-5.34814 -4.2773819 -3.9174891 -3.4913487 -3.1377692 -2.3613191 -1.715116 -1.8730493 -2.077311 -3.3525991 -3.8052545 -4.6320019 -5.0172272 -5.7034969 -6.50173][-6.0673556 -5.1496944 -4.9416242 -4.697896 -4.4809394 -3.8135552 -3.3819804 -3.6677308 -3.926187 -4.6722994 -5.050684 -5.3592939 -5.7522745 -6.133532 -6.7344913][-5.8340516 -4.8332086 -4.5008664 -4.5507984 -4.3366604 -3.6930947 -3.3125839 -3.3605828 -3.3732629 -3.8127172 -4.3090458 -4.9396067 -5.3566842 -5.9556179 -6.58667][-6.3960876 -5.7044239 -5.2142076 -5.1579638 -4.8284931 -4.3179722 -4.2752829 -4.5957813 -4.6025772 -4.7217321 -5.0097346 -5.482646 -6.0256834 -6.3618097 -6.87247]]...]
INFO - root - 2017-12-15 18:55:18.614175: step 48210, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 51h:11m:10s remains)
INFO - root - 2017-12-15 18:55:24.982516: step 48220, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 49h:44m:34s remains)
INFO - root - 2017-12-15 18:55:31.359763: step 48230, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 50h:20m:58s remains)
INFO - root - 2017-12-15 18:55:37.704437: step 48240, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 50h:22m:54s remains)
INFO - root - 2017-12-15 18:55:44.123209: step 48250, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 51h:25m:51s remains)
INFO - root - 2017-12-15 18:55:50.580840: step 48260, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.663 sec/batch; 52h:18m:30s remains)
INFO - root - 2017-12-15 18:55:56.941056: step 48270, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 51h:08m:33s remains)
INFO - root - 2017-12-15 18:56:03.286936: step 48280, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 49h:41m:43s remains)
INFO - root - 2017-12-15 18:56:09.783845: step 48290, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:37m:58s remains)
INFO - root - 2017-12-15 18:56:16.099216: step 48300, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:45m:11s remains)
2017-12-15 18:56:16.585401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2261271 -3.4022369 -3.7455668 -3.5275908 -3.582047 -3.6080122 -3.296443 -3.1209579 -3.0091243 -4.0154066 -4.7374163 -6.6988683 -7.4618425 -7.3995161 -7.3218436][-3.2617431 -3.2997394 -3.8737044 -4.0579481 -4.1318607 -3.9524667 -3.714478 -3.6541715 -3.2455964 -4.1196504 -4.7117991 -6.4133949 -7.0123577 -7.2481194 -7.2246857][-3.2485104 -3.2992325 -4.1156845 -4.3907948 -4.5105953 -4.2630749 -3.8212023 -3.6159368 -3.3657012 -4.1842318 -4.6383104 -6.1873522 -6.5343118 -6.5158482 -6.376771][-3.9538472 -3.7398584 -3.9521875 -3.5061884 -3.3236475 -3.3674803 -3.1986051 -3.2397695 -3.0005674 -3.8775506 -4.41733 -5.9732065 -6.4495468 -6.4627619 -6.3669968][-4.5547009 -3.5500469 -3.1727061 -2.8327742 -2.785635 -2.1845775 -1.1130681 -1.2878904 -1.2621484 -2.5996561 -3.7244773 -5.5396934 -6.3322773 -6.60338 -6.7063184][-4.7624812 -3.7655668 -2.9715762 -1.8164339 -0.72809172 0.038453102 0.72986794 0.91748619 1.1449919 -0.36587954 -1.7270823 -4.303257 -5.8240123 -6.2933292 -6.5113111][-4.7815895 -3.7140203 -2.9190946 -1.2313204 0.20998955 1.1748781 2.2913446 2.3859291 2.4571066 1.1867647 -0.068615913 -2.8179989 -4.3162966 -5.1115861 -5.8984661][-4.9311066 -3.6617537 -2.5610976 -0.93935966 0.70448494 1.863945 2.9274406 2.8936319 3.1741047 1.6442785 0.40966606 -1.9502726 -3.6157146 -4.6085544 -5.5951037][-4.9739628 -3.7262244 -2.7129121 -1.3575702 -0.30739975 0.84267521 2.0220413 2.2974072 2.8718081 1.2016363 -0.41590786 -2.8106675 -4.1685104 -5.2329454 -6.0812778][-5.7635264 -4.8875666 -4.1006713 -3.0279374 -2.1676331 -1.3146515 -0.6159935 0.18498468 1.1072006 -0.60827732 -1.8694301 -4.2942915 -5.6518278 -6.1775107 -6.7582293][-6.7625046 -6.376637 -6.0806947 -5.1624246 -4.5694556 -3.7304785 -2.7834806 -2.637084 -2.2824097 -3.160666 -3.6620073 -5.2042866 -6.1416454 -6.8501992 -7.4009056][-8.0449514 -7.3901834 -6.6439981 -6.1297531 -5.4275508 -4.9155769 -4.530633 -4.4290447 -3.9734893 -4.9210854 -5.3274302 -6.1749191 -6.6640534 -6.731699 -7.0049248][-7.9308572 -7.7510805 -7.3253756 -6.5116534 -5.92945 -5.7318625 -5.4933519 -5.5571108 -5.568325 -6.053421 -6.2604332 -6.5524421 -6.5142965 -6.7582226 -6.658143][-7.9092827 -7.8509679 -7.4896536 -6.8222909 -6.1640892 -5.5779963 -5.2363529 -5.50867 -5.5578642 -6.1013451 -6.373219 -6.4216485 -6.5731373 -6.372571 -6.2605648][-7.6160979 -7.9330168 -7.868824 -7.3494773 -6.89538 -6.4596395 -5.9577475 -6.058373 -6.1034131 -6.3906541 -6.6403861 -6.7766514 -6.8151731 -6.4106832 -5.9645615]]...]
INFO - root - 2017-12-15 18:56:22.955282: step 48310, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 50h:35m:25s remains)
INFO - root - 2017-12-15 18:56:29.443503: step 48320, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 51h:54m:09s remains)
INFO - root - 2017-12-15 18:56:35.920415: step 48330, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.652 sec/batch; 51h:25m:45s remains)
INFO - root - 2017-12-15 18:56:42.265761: step 48340, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 49h:00m:01s remains)
INFO - root - 2017-12-15 18:56:48.678290: step 48350, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 49h:05m:18s remains)
INFO - root - 2017-12-15 18:56:55.007112: step 48360, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 50h:07m:28s remains)
INFO - root - 2017-12-15 18:57:01.342175: step 48370, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 49h:55m:53s remains)
INFO - root - 2017-12-15 18:57:07.732528: step 48380, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.648 sec/batch; 51h:10m:39s remains)
INFO - root - 2017-12-15 18:57:14.093059: step 48390, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 50h:10m:16s remains)
INFO - root - 2017-12-15 18:57:20.494469: step 48400, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:39m:19s remains)
2017-12-15 18:57:21.002213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9568696 -6.1187115 -5.7624931 -5.4202118 -4.88657 -4.1240673 -3.4369903 -2.8745284 -2.0906849 -3.783325 -3.0760355 -4.5231733 -5.0641236 -5.7046747 -7.0144758][-5.060894 -4.5643525 -4.611331 -4.5162988 -4.3967361 -4.3873882 -4.1077175 -4.0150642 -4.2259369 -6.779 -5.6820917 -6.2949872 -6.6134849 -6.8623853 -8.0961552][-4.3811474 -3.3436255 -3.2937737 -3.5597544 -3.9084048 -3.8185074 -3.4699111 -3.5356011 -4.260128 -7.1168256 -6.9558754 -7.9678564 -7.8831024 -8.0364523 -8.9425745][-3.8654156 -3.2035332 -3.1227918 -2.6822248 -2.3726258 -2.5087304 -2.1443577 -2.1924877 -2.8096218 -5.8632131 -6.18363 -8.1488113 -9.0223665 -9.3569765 -10.246556][-3.6126938 -2.6358571 -2.4332108 -1.6276565 -0.957881 -0.24003124 0.11897755 -0.23426199 -0.83802509 -4.2461195 -4.8677235 -6.923492 -8.01154 -9.04144 -10.20731][-3.0236135 -2.2227283 -1.9715166 -1.2125378 -0.47384357 0.40012932 1.7822514 1.575511 0.91958904 -2.0472069 -2.5494208 -4.8896351 -6.3799381 -7.5550842 -9.1634283][-2.5392852 -2.1799335 -1.6543689 -0.03073597 0.96774578 1.9863548 2.9760637 3.3240309 3.6150818 0.40109825 -0.48872614 -2.953156 -5.0107579 -6.4139857 -7.8781533][-1.8249011 -1.6236115 -1.1931314 0.29365635 1.5508041 3.0660343 4.7635794 4.3047342 4.0059919 1.6462898 1.0308037 -1.6668558 -3.6980419 -5.5477972 -7.1817651][-0.82290792 -0.88929176 -1.1601973 -0.39321041 0.26831579 1.5559626 2.6608925 3.0959387 3.6493359 0.94385815 0.6067028 -1.2672768 -2.9071369 -4.9445171 -6.6060343][-2.28858 -2.2906384 -1.6389561 -0.51817417 0.10272884 0.19886732 0.89248848 0.95763779 1.2169809 -0.7980895 -0.89219236 -2.6465549 -3.8248851 -4.726161 -5.9132342][-4.283947 -3.5201006 -3.1165771 -2.8555741 -2.2577443 -1.6239862 -1.1470413 -1.0812721 -0.90550947 -2.8193827 -3.4931173 -4.9694328 -5.9374647 -6.1730571 -6.3962841][-5.7756534 -4.6852312 -4.2391973 -3.7740257 -2.9359498 -2.802907 -2.7813044 -3.0177984 -3.2641768 -4.8157206 -4.9236374 -6.2600241 -6.6775351 -7.1035395 -7.1376443][-6.9601707 -6.0330768 -5.9075222 -5.3049603 -5.1264687 -4.7005348 -3.9910362 -4.0209951 -4.2750978 -5.3527966 -5.7339773 -6.5019484 -6.5426311 -7.1084995 -7.4614124][-8.0098438 -6.8515267 -6.239994 -5.7285028 -5.7720861 -5.3250475 -5.0545874 -5.460639 -5.2894163 -5.4220304 -5.6053276 -6.1685209 -6.0789151 -6.5410657 -6.5152445][-6.5860295 -6.38702 -6.3920164 -5.8389063 -5.7644038 -5.654932 -5.5281577 -5.5795803 -5.7936916 -5.5152187 -5.2310638 -5.5797796 -5.719749 -5.7467289 -5.7035608]]...]
INFO - root - 2017-12-15 18:57:27.511152: step 48410, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:45m:15s remains)
INFO - root - 2017-12-15 18:57:33.881466: step 48420, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 49h:16m:48s remains)
INFO - root - 2017-12-15 18:57:40.253102: step 48430, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 50h:21m:19s remains)
INFO - root - 2017-12-15 18:57:46.654882: step 48440, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 51h:56m:05s remains)
INFO - root - 2017-12-15 18:57:53.082211: step 48450, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 52h:02m:45s remains)
INFO - root - 2017-12-15 18:57:59.416923: step 48460, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 52h:13m:17s remains)
INFO - root - 2017-12-15 18:58:05.884991: step 48470, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 51h:40m:31s remains)
INFO - root - 2017-12-15 18:58:12.352068: step 48480, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 49h:50m:52s remains)
INFO - root - 2017-12-15 18:58:18.738419: step 48490, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:35m:12s remains)
INFO - root - 2017-12-15 18:58:25.130582: step 48500, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 52h:01m:53s remains)
2017-12-15 18:58:25.676863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6532393 -6.7419882 -7.683877 -8.0451889 -8.1832285 -7.45777 -6.6664505 -5.8432727 -5.4906774 -5.7221231 -6.6790676 -7.6963005 -7.8397212 -8.4968634 -8.1729507][-5.188364 -5.837553 -6.087904 -6.8221006 -7.2835693 -6.9690619 -6.741601 -6.0815511 -5.7603345 -5.9735622 -6.9236441 -7.5587525 -7.6255054 -8.3079453 -7.8555126][-3.1641054 -3.7043118 -4.2366557 -4.5184741 -4.3214183 -4.635922 -4.9110842 -4.86012 -4.9112697 -5.0883322 -6.0890718 -6.3669715 -6.2765656 -6.8558211 -6.5196896][-1.8892822 -2.5587635 -3.1622725 -3.3592434 -3.1620035 -2.79629 -2.4139071 -3.0205774 -3.4046769 -3.8851655 -4.8826771 -5.0802484 -4.9948854 -5.3975239 -5.053123][-1.4309111 -1.8496413 -2.0054469 -2.5738907 -2.2450886 -1.4216318 -1.0843816 -1.4425068 -1.5743232 -2.4660468 -3.3966975 -3.7451549 -3.8592381 -4.1793766 -4.0562143][-1.3814745 -1.4663324 -1.8595252 -1.6213765 -1.2506661 -0.57385731 0.15041304 0.236063 0.11263514 -0.9243083 -2.0388241 -2.6442323 -2.9363165 -3.5175724 -3.3218727][-1.5262098 -1.5951996 -2.1992149 -1.9963231 -1.0133476 -0.31944704 0.29259872 0.56906796 0.75706005 -0.0037093163 -0.98534441 -1.7747388 -2.9250736 -3.3464684 -3.2656789][-1.6324353 -1.911562 -2.3633809 -1.9101925 -1.2237382 -0.042621136 0.62749958 0.65803051 0.85625172 0.057590008 -0.77314234 -1.6122313 -2.5101485 -3.3361945 -3.8442376][-2.9771967 -2.5669208 -2.4342308 -2.1296902 -1.0341964 0.20939684 1.1718941 1.1883812 1.2490931 0.42293072 -0.92399883 -1.9918232 -2.7634697 -3.5658293 -3.8124623][-5.467535 -4.924613 -4.1295118 -3.1880226 -1.6215115 -0.13373041 0.81250572 1.1850204 1.4441872 0.31193733 -0.97614813 -1.9883475 -3.2034106 -4.474072 -4.7895746][-9.3825111 -8.159193 -6.98911 -5.5288057 -4.1948957 -2.4613786 -1.0054941 -0.63413715 -0.42871952 -0.81671524 -1.8158407 -2.5869393 -3.2507977 -4.330215 -4.748332][-10.333092 -9.6003075 -8.4343653 -6.9896374 -5.8179388 -4.6177416 -3.6259122 -3.5285854 -2.9847126 -3.2300887 -3.8508472 -4.2997074 -4.4846392 -4.8174539 -4.8275013][-9.3173008 -9.2183857 -8.6064978 -7.8252411 -7.1482539 -6.0761137 -5.0617614 -4.4523811 -4.0052977 -4.253562 -4.7652283 -5.0585313 -5.083849 -5.0723877 -4.6707296][-7.9364614 -8.407361 -8.1354055 -7.7133713 -7.6126719 -7.0503964 -6.4693079 -5.8379269 -5.3916874 -5.3129711 -5.4947672 -5.3728223 -5.0535822 -5.1355295 -5.1260338][-7.7102714 -7.6449509 -7.6852961 -7.8440757 -7.8370128 -7.6901193 -7.6162624 -7.4305449 -7.1460252 -6.896059 -6.6349993 -6.4169245 -6.3155708 -6.0251017 -5.5817776]]...]
INFO - root - 2017-12-15 18:58:32.197564: step 48510, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 51h:00m:23s remains)
INFO - root - 2017-12-15 18:58:38.670268: step 48520, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 51h:00m:18s remains)
INFO - root - 2017-12-15 18:58:45.071757: step 48530, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:09m:26s remains)
INFO - root - 2017-12-15 18:58:51.345327: step 48540, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 50h:19m:12s remains)
INFO - root - 2017-12-15 18:58:57.706171: step 48550, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 49h:36m:19s remains)
INFO - root - 2017-12-15 18:59:04.120193: step 48560, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 51h:33m:53s remains)
INFO - root - 2017-12-15 18:59:10.469689: step 48570, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 51h:36m:55s remains)
INFO - root - 2017-12-15 18:59:16.872645: step 48580, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 51h:35m:51s remains)
INFO - root - 2017-12-15 18:59:23.301749: step 48590, loss = 0.37, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 49h:46m:32s remains)
INFO - root - 2017-12-15 18:59:29.676211: step 48600, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 48h:24m:10s remains)
2017-12-15 18:59:30.315302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3057137 -9.2062521 -11.161394 -12.475681 -13.255803 -11.379932 -10.345868 -7.9964423 -6.5785112 -6.2948265 -7.61277 -9.1393652 -6.6938057 -7.6886568 -6.7989678][-7.4041848 -9.1600533 -11.136831 -12.659839 -14.543639 -13.769642 -12.022815 -8.1487207 -6.5714254 -6.7697124 -7.7976346 -10.019979 -9.5090532 -9.9903917 -8.2030611][-7.2425132 -8.386363 -10.714665 -11.754419 -12.68189 -11.395529 -10.659885 -8.327857 -6.6209 -6.18038 -6.9926844 -10.028627 -8.9991455 -10.460053 -9.7823286][-8.4800043 -9.3382139 -10.672363 -10.443325 -10.974031 -9.3149281 -7.8379869 -6.0113392 -5.8196082 -7.0715137 -7.0614147 -9.9385548 -9.0607443 -10.690484 -10.221367][-7.3372879 -8.1049376 -8.8976154 -7.9063778 -7.3060904 -5.09867 -4.1432223 -2.965044 -2.3769703 -5.9193363 -8.662137 -11.898487 -9.9433537 -11.593397 -10.652936][-7.098546 -7.6273432 -7.6135321 -6.1132579 -4.5581551 -1.9764528 0.56456375 2.4126863 2.5253391 -1.5945139 -4.981802 -10.894413 -10.550946 -11.937036 -10.722805][-6.6452389 -6.6485105 -6.6352596 -3.8973556 -2.0295973 0.95043659 2.8334484 4.70675 5.5994673 2.1511345 -1.0575123 -7.5550122 -8.9511585 -11.809494 -11.269539][-5.9329252 -5.0969214 -4.9320545 -2.3303561 -0.50102663 3.1087923 5.0013142 5.4806614 4.4713516 1.5204239 -0.80284452 -6.3478079 -6.9727955 -11.27398 -11.745153][-7.7219281 -6.5326695 -5.12621 -3.2360549 -1.458004 1.5060577 3.1921244 3.7667427 3.3049126 -0.54735613 -3.8544824 -8.0191936 -7.1739745 -10.202899 -10.478653][-8.7541819 -8.1485119 -7.149405 -5.0422473 -3.5581856 -2.046195 -0.10655355 1.4994249 1.6740808 -2.7643857 -5.7394123 -10.302243 -10.178415 -11.506782 -9.9636154][-8.8660374 -8.87084 -8.34202 -6.8843846 -5.4558644 -3.6704297 -2.3641963 -1.8906336 -1.8425188 -4.5307379 -6.3566451 -10.281275 -10.864039 -12.025068 -10.95476][-7.684516 -8.2958632 -8.0600567 -7.1957607 -7.0072527 -5.2268176 -3.832159 -3.9460385 -4.2456136 -6.2136278 -7.0117178 -8.53327 -8.998621 -9.9440422 -9.6237431][-9.3580341 -8.2317944 -7.87242 -6.6859169 -6.4752989 -5.1409588 -3.9909868 -3.9325366 -4.4013758 -6.3656387 -6.8414416 -6.9508076 -6.4623637 -7.1751761 -7.2355852][-9.0810041 -9.1982412 -8.4120293 -7.1842904 -6.34191 -4.7804451 -3.7791665 -3.232852 -3.6979208 -5.0179281 -5.4659405 -5.9164023 -5.6983714 -5.0397682 -4.1227989][-6.8774633 -7.7943316 -8.2883081 -7.404563 -6.7926717 -5.6070752 -4.6655512 -4.0043421 -4.1682177 -4.5242729 -5.4071093 -5.7176967 -5.7397738 -5.50671 -4.6410284]]...]
INFO - root - 2017-12-15 18:59:36.660383: step 48610, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 50h:09m:30s remains)
INFO - root - 2017-12-15 18:59:43.060710: step 48620, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 51h:46m:23s remains)
INFO - root - 2017-12-15 18:59:49.416436: step 48630, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 50h:49m:17s remains)
INFO - root - 2017-12-15 18:59:55.814331: step 48640, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 50h:32m:58s remains)
INFO - root - 2017-12-15 19:00:02.258910: step 48650, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 49h:38m:53s remains)
INFO - root - 2017-12-15 19:00:08.621139: step 48660, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 50h:17m:00s remains)
INFO - root - 2017-12-15 19:00:15.082430: step 48670, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 49h:32m:54s remains)
INFO - root - 2017-12-15 19:00:21.478433: step 48680, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 49h:46m:02s remains)
INFO - root - 2017-12-15 19:00:27.814668: step 48690, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 49h:42m:47s remains)
INFO - root - 2017-12-15 19:00:34.159499: step 48700, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 50h:33m:04s remains)
2017-12-15 19:00:34.720081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7832685 -4.169848 -4.8969803 -4.5175991 -4.7184134 -4.5427132 -3.1453247 -2.6404924 -2.6413007 -2.3873162 -1.986834 -3.444171 -3.7356489 -5.6105995 -8.046196][-3.4627323 -4.0277052 -4.3223324 -4.4802122 -5.3744836 -5.135 -4.9468441 -4.3851995 -2.7623854 -1.9986525 -1.7575421 -3.6758604 -4.6209536 -5.1102285 -5.7841439][-3.9497709 -4.5381675 -6.014658 -5.8654556 -5.1510649 -5.4234858 -5.2188845 -4.7432728 -4.6574197 -4.2168179 -2.9407139 -3.472249 -4.1598039 -5.3816476 -7.5356383][-4.7822251 -5.0698829 -5.6391759 -5.8596478 -5.5080786 -4.9148817 -3.9727759 -4.0850735 -4.1858206 -3.8765857 -3.960222 -4.9017229 -4.2958016 -4.3767238 -6.0208077][-5.5728946 -5.123136 -4.9830213 -4.6677217 -4.4809637 -3.3736649 -2.7683549 -2.8709002 -2.7239876 -3.3097081 -3.6368828 -5.5493822 -6.5023794 -6.7378387 -7.6178145][-5.3060331 -4.1902204 -3.6752076 -2.7816033 -2.2791309 -1.052875 0.80903912 0.61161232 -0.025679588 -1.0145807 -1.5145597 -3.7830927 -5.3177185 -5.8498135 -7.6115618][-5.6245995 -4.209487 -2.0170197 -0.64543533 0.12476206 0.73733521 1.4719944 1.5557222 1.8008804 1.2329054 0.83402061 -2.0737462 -4.324645 -5.5549488 -7.4941945][-2.0008864 -1.9695439 -1.8821425 -0.1128788 1.5072489 2.9632025 4.1439114 3.4393778 3.2478981 2.5318651 1.6816416 -1.486125 -3.0377846 -4.1173773 -6.3454766][-2.2992859 -1.3712358 -0.88212633 -0.58636761 -0.23538733 1.0516891 3.1223259 3.7152472 4.5145788 3.5918083 1.5665312 -1.6900101 -3.7350006 -5.0043197 -6.531651][-2.6031442 -2.6550364 -2.3824825 -0.72993135 0.7034111 1.25418 1.1549406 1.6193275 2.5737696 1.6946964 0.84751892 -1.3933721 -3.9445968 -4.7644176 -6.1903553][-4.8937984 -4.6915379 -5.0435786 -4.2475929 -3.5318055 -1.9705791 -1.0272236 -1.0419984 -1.2955427 -2.9412665 -3.7428453 -5.151432 -5.8471003 -6.2751131 -7.1509838][-7.4601583 -5.9987612 -5.6895752 -4.7234025 -4.28448 -4.44186 -4.3094406 -4.1098909 -3.3506236 -4.4372082 -5.6850581 -6.6323795 -7.1917949 -7.4056478 -7.9389553][-8.5655737 -8.6359644 -8.3382158 -7.1768217 -6.1069174 -4.7064362 -3.66428 -4.4017591 -5.36477 -6.445539 -6.9056411 -7.0197639 -7.0915751 -7.1812449 -7.5462556][-9.0579834 -8.4163227 -7.8323278 -7.7211504 -7.2156267 -6.3397894 -5.2473841 -4.8993468 -4.266017 -4.5235958 -6.0653033 -6.8544235 -6.7187757 -5.8281007 -5.4591217][-8.62432 -8.1271343 -8.4616117 -7.7944608 -7.4111834 -7.4134941 -7.3090596 -7.3237705 -6.8137636 -6.2436695 -5.5046539 -5.9279966 -6.4875784 -6.419528 -5.8789139]]...]
INFO - root - 2017-12-15 19:00:41.205727: step 48710, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 50h:49m:00s remains)
INFO - root - 2017-12-15 19:00:47.562452: step 48720, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 49h:36m:51s remains)
INFO - root - 2017-12-15 19:00:53.915364: step 48730, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 49h:41m:47s remains)
INFO - root - 2017-12-15 19:01:00.326813: step 48740, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 50h:09m:52s remains)
INFO - root - 2017-12-15 19:01:06.742475: step 48750, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 51h:13m:25s remains)
INFO - root - 2017-12-15 19:01:13.089752: step 48760, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 49h:18m:10s remains)
INFO - root - 2017-12-15 19:01:19.465043: step 48770, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 51h:20m:41s remains)
INFO - root - 2017-12-15 19:01:25.863739: step 48780, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 50h:04m:20s remains)
INFO - root - 2017-12-15 19:01:32.328250: step 48790, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 49h:52m:58s remains)
INFO - root - 2017-12-15 19:01:38.765730: step 48800, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 51h:08m:15s remains)
2017-12-15 19:01:39.223020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7134361 -4.1040726 -4.6750507 -4.8974013 -4.5602622 -3.7763309 -2.9270697 -2.6141934 -2.2479968 -2.5922456 -3.3417635 -4.421607 -5.4890013 -5.6729293 -6.5460835][-4.3808336 -4.4631195 -4.4930134 -4.754735 -5.1919584 -4.7203865 -3.7432349 -3.0111876 -2.5534148 -3.1316185 -4.1405697 -5.0785179 -6.1555872 -6.552135 -7.3967094][-4.5322142 -4.3622952 -4.1583157 -3.9011803 -3.7184448 -3.7699809 -3.5017905 -3.0368471 -2.6263514 -3.4261622 -4.9513497 -6.1176033 -7.003758 -7.4523907 -8.1231318][-4.4968166 -3.9532576 -3.6274338 -3.1285996 -2.5251746 -2.2701855 -1.7855964 -1.6607537 -1.4813027 -2.5670447 -4.3371487 -5.9384184 -7.1231074 -7.4346147 -8.0002642][-5.2510567 -4.4402852 -3.5179853 -2.6387091 -1.6258368 -0.86847973 -0.17322636 -0.16406488 -0.25196075 -1.8395996 -4.0549283 -5.8503103 -7.4120646 -7.7810287 -8.4286833][-6.7785897 -6.0700207 -5.3201809 -3.79109 -2.0897508 -0.54759932 0.90629387 0.82853413 0.59582138 -1.028378 -3.4277616 -5.7394218 -7.6222529 -8.34297 -9.09178][-7.1449933 -6.0999937 -4.9009085 -3.5026994 -1.5571504 0.37817669 2.0544415 2.2710361 2.2860422 0.79020596 -1.6740603 -4.2279758 -6.4005013 -7.3397207 -8.396471][-6.4756823 -5.6575432 -4.3953152 -2.7246628 -0.64991474 1.6912975 3.6099195 3.9543953 4.1720419 2.4159193 -0.28202629 -2.8077998 -4.9819613 -6.2045908 -7.3818507][-6.74638 -5.7486057 -4.6403561 -3.4411526 -2.039741 -0.299088 1.7115774 2.8169813 3.5783644 1.552392 -1.3426833 -3.8059332 -5.6188016 -6.5577645 -7.866302][-7.5777469 -6.9007854 -5.9038582 -4.22244 -2.6593008 -1.7865934 -0.67880487 0.11463881 0.79938507 -0.7555728 -3.1155391 -5.4796114 -7.153758 -7.5371928 -8.213232][-8.2605314 -8.0892868 -7.3933725 -6.1051569 -4.6302214 -3.1675181 -2.3235154 -2.3304687 -2.0589571 -3.1110353 -4.5926027 -6.2172284 -7.4589772 -7.956481 -8.2311573][-8.196332 -7.9972887 -7.5906343 -6.8935256 -6.079339 -5.1374493 -4.3863378 -4.30896 -4.2724385 -4.8758278 -5.5718327 -6.4040666 -7.172646 -7.8359003 -8.3040228][-8.6881628 -8.2904844 -7.7318149 -7.0157018 -6.4211845 -6.0610542 -5.4834256 -5.3955517 -5.3781443 -6.2450919 -6.8086238 -6.9806714 -7.2910886 -7.5864 -7.8386726][-8.1531839 -8.3125839 -7.9105453 -6.9653411 -6.0937939 -5.5747509 -5.1589823 -5.5248289 -5.7174263 -6.2552552 -6.4147077 -6.728128 -7.0771179 -7.1763654 -7.0568857][-8.239 -8.6384 -8.9400949 -8.4488726 -7.6045685 -6.9493694 -6.4883161 -6.7453547 -7.0331345 -7.2118931 -7.2167621 -7.2262268 -7.0701661 -7.0118337 -6.7637291]]...]
INFO - root - 2017-12-15 19:01:45.617364: step 48810, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 51h:29m:32s remains)
INFO - root - 2017-12-15 19:01:52.024742: step 48820, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 49h:31m:48s remains)
INFO - root - 2017-12-15 19:01:58.508365: step 48830, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 50h:58m:26s remains)
INFO - root - 2017-12-15 19:02:04.968252: step 48840, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.664 sec/batch; 52h:18m:00s remains)
INFO - root - 2017-12-15 19:02:11.377683: step 48850, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:08m:42s remains)
INFO - root - 2017-12-15 19:02:17.734685: step 48860, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 48h:37m:23s remains)
INFO - root - 2017-12-15 19:02:24.112875: step 48870, loss = 0.35, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 50h:41m:40s remains)
INFO - root - 2017-12-15 19:02:30.532054: step 48880, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 51h:44m:18s remains)
INFO - root - 2017-12-15 19:02:36.842957: step 48890, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 51h:04m:26s remains)
INFO - root - 2017-12-15 19:02:43.276265: step 48900, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 51h:21m:22s remains)
2017-12-15 19:02:43.812360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9012568 -4.1135349 -4.4815011 -4.1097107 -3.8719459 -3.442205 -2.7441368 -2.6136947 -2.043633 -4.4717531 -5.1862164 -5.8462133 -6.5141273 -7.2819118 -7.8118997][-3.6629028 -4.0154638 -4.5652218 -4.6624126 -4.6756315 -4.6563926 -4.2503638 -3.9675367 -3.4004493 -5.2068052 -5.5719047 -6.3447967 -6.9636221 -7.7632246 -8.2501945][-3.6052575 -3.6851797 -3.9880908 -3.8301568 -3.8418813 -3.8637486 -3.6729274 -3.5991116 -2.9814262 -4.9221096 -5.0622158 -5.824543 -6.4522629 -7.3778963 -8.11798][-3.1563292 -2.9748006 -3.2600751 -3.2416272 -2.8900976 -2.7030458 -2.2166834 -2.2763543 -2.2822948 -4.318862 -4.7520294 -5.7483416 -6.4211864 -7.5053906 -8.2052526][-4.2671618 -2.9322481 -2.5205421 -2.2726045 -1.7659321 -1.4440303 -1.2006049 -1.5765467 -1.5561681 -3.8540239 -4.5944452 -5.4049063 -6.2573466 -7.2103596 -8.0623112][-5.1904573 -3.8574841 -2.7124834 -1.7930689 -0.92624807 -0.09337759 0.090572834 -0.390666 -0.53048468 -2.7125478 -3.4416919 -4.4060369 -5.3926888 -6.5543838 -7.3382359][-4.8153296 -3.5869699 -2.5874462 -1.1537447 0.029323578 1.2140732 1.5378962 1.2238874 0.9499712 -1.476613 -2.2311907 -3.1853065 -4.2185154 -5.45097 -6.5211306][-4.1710763 -2.8721504 -1.7048774 -0.23955488 0.89784241 1.8271952 2.1148424 2.1509924 2.1907997 -0.27032995 -1.336195 -2.3672009 -3.3007159 -4.6416388 -5.7340546][-3.3720446 -2.4874797 -1.6003122 -0.48017979 0.22585297 0.93914223 1.330966 1.6572142 1.8866482 -0.34549475 -1.2568407 -2.4709716 -3.6970294 -4.7601104 -5.7973313][-2.4517078 -1.8494177 -1.1668191 -0.36844254 0.23469353 0.54330826 0.73958969 1.1429405 1.3815022 -0.87058592 -1.7761483 -2.8127003 -3.8277607 -4.9109373 -5.8217683][-3.4366298 -2.3059587 -1.3466234 -0.8982029 -0.38715506 -0.21363401 -0.31214714 -0.37475252 -0.19676542 -2.3499398 -3.4631896 -4.3135738 -5.2350211 -5.7848134 -6.3717046][-3.3031864 -2.6164222 -1.9251504 -1.314671 -0.90567589 -0.96109152 -0.84414721 -1.1143532 -1.0838251 -2.731782 -3.5498419 -4.3998365 -5.3639431 -6.3053203 -6.6696768][-4.5312009 -3.7315474 -2.8632169 -2.2554698 -1.8998966 -1.9426246 -2.0587831 -2.1421514 -1.9806547 -3.4314561 -4.0250864 -4.4651995 -4.9617767 -5.570014 -5.9797115][-5.2545538 -4.2970204 -3.6392632 -2.8551297 -2.357378 -2.3192062 -2.4436245 -2.6488595 -2.6603947 -3.0933681 -3.1711049 -3.8315868 -4.1114893 -4.6473589 -5.1971054][-5.5639877 -5.14937 -4.4614315 -3.9936612 -3.6374645 -3.5190072 -3.7483358 -3.8533144 -4.0096722 -3.7766743 -3.3652096 -3.7104058 -4.0432453 -4.3437338 -4.526453]]...]
INFO - root - 2017-12-15 19:02:50.161110: step 48910, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 51h:11m:49s remains)
INFO - root - 2017-12-15 19:02:56.606869: step 48920, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 49h:55m:40s remains)
INFO - root - 2017-12-15 19:03:03.037223: step 48930, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 52h:22m:37s remains)
INFO - root - 2017-12-15 19:03:09.391669: step 48940, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 49h:12m:27s remains)
INFO - root - 2017-12-15 19:03:15.723472: step 48950, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:11m:33s remains)
INFO - root - 2017-12-15 19:03:22.066810: step 48960, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 49h:04m:24s remains)
INFO - root - 2017-12-15 19:03:28.503655: step 48970, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 50h:22m:34s remains)
INFO - root - 2017-12-15 19:03:34.868930: step 48980, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 52h:03m:29s remains)
INFO - root - 2017-12-15 19:03:41.240178: step 48990, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 49h:38m:53s remains)
INFO - root - 2017-12-15 19:03:47.631577: step 49000, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:03m:37s remains)
2017-12-15 19:03:48.162835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.78304 -6.9192214 -7.9364023 -8.5270061 -8.7823639 -8.570405 -7.3055048 -6.3582149 -5.1856422 -5.0173531 -5.7464 -7.1640706 -7.4459434 -8.1414022 -7.36834][-5.1296329 -6.4653149 -7.9671769 -8.77338 -8.9452133 -8.84765 -8.2453566 -7.1772008 -5.4066668 -6.0032215 -7.4802547 -9.2374239 -9.7363148 -9.6618385 -9.2335052][-5.199379 -6.5979223 -8.2928724 -9.1451111 -9.3833742 -8.20125 -7.2724915 -6.6749544 -6.0075064 -6.1130877 -7.2712398 -9.8696547 -11.198978 -11.522564 -11.143961][-7.032393 -7.8584151 -8.3551683 -8.0936069 -8.1762953 -7.052948 -5.8889141 -4.5269303 -3.9111173 -4.7958288 -6.9311523 -9.58464 -10.901125 -11.707533 -11.71583][-7.3419213 -6.820159 -7.3096967 -6.8314619 -6.2895308 -4.09322 -2.2169776 -1.0728321 -0.73332024 -2.2876143 -4.7936449 -8.014823 -9.6480513 -10.738586 -10.587914][-7.2676554 -6.8283415 -6.6032319 -5.0418167 -3.62363 -1.1992507 0.98259735 2.3694525 2.8445263 0.55365849 -2.176116 -5.9150877 -7.7894068 -9.0141792 -9.4940233][-6.2494111 -5.4100924 -4.2091103 -1.8108916 0.16050291 2.64264 5.062851 5.7806168 5.2949781 2.3412495 -0.78209162 -4.8011122 -6.746954 -8.3095036 -8.1889734][-6.1130733 -5.476038 -3.3762069 -0.37382364 2.3792582 4.8610506 6.5914059 6.497282 5.9702082 2.5343323 -1.6767778 -5.6031141 -7.1064844 -7.9548674 -7.4457011][-6.78788 -6.0711 -5.3648272 -3.3694415 -0.54877949 2.1268368 3.9742689 4.4669971 4.1887922 0.60956383 -3.48449 -7.1193304 -8.7945385 -9.20596 -8.0334167][-9.4974022 -8.9446964 -8.2945509 -6.8813629 -4.6428623 -2.1997972 -0.95070076 -0.42618036 -0.77609873 -3.7916396 -6.5534439 -9.697 -10.853732 -10.376283 -8.846756][-10.813954 -10.898881 -10.729738 -9.7268772 -8.3599224 -6.5853887 -5.2438517 -5.1650209 -5.6434937 -7.2025723 -8.4006042 -10.062984 -10.841639 -11.236824 -10.575245][-11.370146 -11.551367 -10.849522 -10.304022 -9.685895 -8.88292 -8.1439781 -8.0531034 -7.7911267 -8.4636326 -9.2642021 -9.7097654 -9.9316244 -9.8241053 -9.363822][-10.344185 -11.072817 -11.24144 -10.547747 -9.4599571 -9.2713165 -8.62937 -8.4662857 -8.4139538 -8.9147081 -9.737319 -9.1788645 -8.7949209 -8.5824261 -8.0132713][-8.0852165 -8.3109446 -9.0453272 -9.0044394 -8.7097311 -8.1558876 -7.6309509 -7.8280487 -7.6990728 -7.3301959 -8.2559919 -8.2813482 -8.1977978 -7.4260468 -6.6022129][-6.3669643 -6.7920885 -7.2727509 -7.9059916 -8.7448463 -8.2083206 -7.9250574 -7.6249852 -7.510016 -7.617909 -7.0466533 -6.4359417 -6.9475584 -7.1087337 -7.1940103]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:03:54.579472: step 49010, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 51h:40m:04s remains)
INFO - root - 2017-12-15 19:04:00.956728: step 49020, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 49h:23m:30s remains)
INFO - root - 2017-12-15 19:04:07.325465: step 49030, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 51h:02m:24s remains)
INFO - root - 2017-12-15 19:04:13.735673: step 49040, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 51h:30m:23s remains)
INFO - root - 2017-12-15 19:04:20.093937: step 49050, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 50h:17m:19s remains)
INFO - root - 2017-12-15 19:04:26.521986: step 49060, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 50h:38m:44s remains)
INFO - root - 2017-12-15 19:04:32.887061: step 49070, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 49h:30m:53s remains)
INFO - root - 2017-12-15 19:04:39.327133: step 49080, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 51h:39m:38s remains)
INFO - root - 2017-12-15 19:04:45.710234: step 49090, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 50h:07m:24s remains)
INFO - root - 2017-12-15 19:04:52.083635: step 49100, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 50h:45m:26s remains)
2017-12-15 19:04:52.600055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6012659 -1.6416011 -2.3081717 -2.5442772 -2.6923523 -2.3349414 -2.174489 -2.1828189 -2.3189521 -3.8421419 -4.1495314 -6.1467328 -6.6721487 -6.7389374 -7.1238866][-2.2803636 -2.3696551 -2.9757948 -3.546061 -3.7176068 -3.3298407 -3.07649 -2.8590002 -2.7395644 -4.243845 -4.6793127 -6.5358005 -7.0310454 -7.1690817 -7.9143562][-2.5993347 -2.7484102 -3.4228497 -3.7497482 -3.9454327 -3.8384869 -3.3106709 -3.1952519 -3.3001142 -4.3323784 -4.3520012 -6.1610932 -6.5519838 -6.6729174 -7.240694][-3.6092143 -4.0922928 -4.7942057 -4.7422891 -4.3202896 -3.7297521 -2.8677239 -2.5302577 -2.2807403 -3.1506085 -3.2638831 -4.97451 -5.4708891 -5.8863206 -6.4063163][-4.7794185 -5.2150631 -5.6393137 -5.3896818 -4.7917066 -3.7184408 -2.3432493 -1.4575977 -0.69693995 -1.3751254 -1.4413323 -3.2911105 -4.1259623 -4.7884507 -5.6083813][-6.4994879 -6.5818477 -6.6219268 -5.8043904 -4.5984273 -3.1645627 -1.3674812 -0.30678225 0.66956329 -0.041796684 -0.17065 -2.0186462 -2.7225842 -3.2782488 -4.2215791][-7.8714657 -7.3424649 -7.0828156 -5.7735982 -4.2068624 -2.4609361 -0.15866566 0.99612713 2.018528 1.3480988 1.1743956 -0.95883703 -2.0723548 -2.6957335 -3.6807714][-7.7539511 -6.9532866 -6.326829 -4.8477278 -3.2466755 -1.2430506 1.0496435 2.4949694 3.6428909 2.8651714 2.6301403 0.27496386 -0.99997568 -2.0472684 -3.577004][-7.1437445 -6.4730692 -6.0945554 -4.5041885 -3.1600947 -1.3535838 0.501215 1.4097242 2.0856361 1.281085 1.2567081 -0.75430059 -2.009829 -2.7999921 -3.902205][-7.5493917 -6.8212495 -6.2024617 -4.9378028 -3.6610141 -2.2041855 -0.69915915 -0.33047247 0.30577803 -0.90492821 -1.3672795 -2.9537139 -3.8807731 -3.9985485 -4.6692286][-8.3038054 -7.7646494 -7.3745551 -6.0642548 -5.0129852 -3.7690639 -2.9732437 -2.6665401 -1.9920535 -2.9116621 -2.9378018 -4.6459904 -5.8493156 -5.8359313 -6.2422113][-8.48188 -7.6724095 -7.009552 -5.9465442 -5.1681204 -4.222075 -3.9618292 -4.1350594 -4.1151304 -4.9600368 -4.924964 -5.5998774 -6.0448213 -6.0494595 -6.2336087][-9.33555 -8.587369 -7.7075615 -6.6451607 -5.9825988 -5.7658706 -5.6091394 -5.80074 -5.8153558 -6.3188667 -6.4778919 -6.8745441 -6.9388895 -6.6397767 -6.1786728][-9.3646126 -8.6261749 -7.7876468 -6.5354753 -5.5383759 -5.5797496 -5.5539322 -5.9077692 -5.9688373 -6.3702602 -6.2048588 -6.1367178 -6.321744 -5.9418211 -5.6005607][-10.253117 -9.5228643 -8.57137 -7.5995674 -7.1203518 -6.796082 -6.7386003 -6.8715153 -6.9550481 -7.1864452 -7.3319588 -7.0050836 -6.5779452 -6.1058884 -5.7175703]]...]
INFO - root - 2017-12-15 19:04:59.017596: step 49110, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.628 sec/batch; 49h:27m:37s remains)
INFO - root - 2017-12-15 19:05:05.389516: step 49120, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 51h:35m:37s remains)
INFO - root - 2017-12-15 19:05:11.763589: step 49130, loss = 0.23, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 50h:16m:02s remains)
INFO - root - 2017-12-15 19:05:18.162297: step 49140, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 48h:27m:05s remains)
INFO - root - 2017-12-15 19:05:24.574283: step 49150, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 49h:53m:48s remains)
INFO - root - 2017-12-15 19:05:30.984727: step 49160, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 49h:26m:11s remains)
INFO - root - 2017-12-15 19:05:37.441706: step 49170, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 49h:09m:37s remains)
INFO - root - 2017-12-15 19:05:43.865669: step 49180, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 51h:24m:20s remains)
INFO - root - 2017-12-15 19:05:50.293233: step 49190, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 51h:01m:07s remains)
INFO - root - 2017-12-15 19:05:56.721432: step 49200, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 50h:48m:15s remains)
2017-12-15 19:05:57.252067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.01032 -5.4911757 -5.6603804 -6.1893697 -6.6393628 -6.4335642 -6.0118527 -5.9572611 -6.2477341 -6.6959038 -7.3481965 -7.6136723 -7.7419353 -8.144372 -7.9175911][-4.5846424 -4.3199058 -4.0803814 -4.6657786 -4.8977118 -5.4144497 -6.1167016 -6.4538341 -5.805088 -5.4244852 -5.9221692 -6.7220483 -7.2272248 -7.2871513 -7.07528][-3.4602304 -3.3722558 -3.7194538 -3.6542306 -3.5057998 -3.8390663 -4.0586143 -4.2753229 -4.7274237 -5.3202639 -5.2151537 -5.3967338 -5.6283932 -6.1742597 -6.3274684][-2.649322 -2.4297147 -2.2031126 -2.3029532 -2.5118327 -2.3055129 -1.9900599 -2.1600809 -1.9707928 -2.6451964 -3.5821872 -4.658021 -5.3454885 -6.4240708 -6.2481713][-2.6626658 -2.4618301 -2.2210507 -1.8767791 -1.4848676 -0.58103609 0.16290236 0.057454109 0.37943363 -0.35949039 -1.3839478 -2.5978336 -3.6651087 -5.2080803 -5.7713776][-1.8284001 -1.4680119 -1.0457106 -0.31674671 0.075874329 0.81426907 1.7043295 1.9581013 2.1141338 0.81681156 -0.536221 -2.0916376 -3.5798111 -5.3803463 -5.8196764][-2.4960933 -2.0896463 -1.4656248 -0.077291012 0.98873329 1.7510576 2.3960953 2.6480856 2.9365931 2.001461 0.34517479 -1.5637836 -3.3139524 -5.2704291 -5.7967334][-2.1194139 -2.0828424 -1.7556534 -0.5534668 0.60778046 1.6644793 2.5728273 3.1322842 3.345829 2.1948137 0.49903965 -1.2799478 -3.2222514 -5.1512222 -5.3217306][-3.9255958 -3.2160215 -2.5137968 -1.63374 0.072836876 1.4889059 2.2645187 2.6818037 2.9420443 1.7621403 -0.012642384 -1.7971053 -3.6958082 -5.3653235 -5.3727183][-4.6425676 -4.1053224 -3.1511068 -2.4840965 -1.3850374 -0.58518934 -0.079753876 0.68697071 0.84354973 -0.17562485 -1.4903483 -2.8360572 -4.1663361 -5.6411309 -6.3396015][-5.263998 -4.2309752 -3.6692004 -3.1457338 -2.1438417 -1.2724218 -0.872396 -1.0299797 -1.3513103 -2.1284027 -3.1354146 -3.6518545 -4.2577214 -5.1719322 -4.9682655][-6.2788887 -5.5442643 -4.9321194 -4.3074808 -3.5506825 -2.5644922 -2.2897 -2.4004846 -2.3485627 -3.161406 -4.2153058 -4.5050945 -5.0735769 -5.1904354 -4.8288026][-6.9312406 -6.064569 -5.3705425 -5.00358 -4.2806249 -3.9019847 -3.8385992 -4.1240644 -4.4381037 -4.5776854 -5.1453505 -5.3334136 -5.274086 -5.3646479 -4.9315157][-6.547945 -5.8855639 -5.7363276 -5.6555095 -5.1675053 -4.5385532 -4.2030334 -4.5891132 -4.6542206 -5.1163025 -5.7291994 -5.5599813 -5.6920352 -5.6422138 -4.7699022][-7.9824314 -7.1392627 -6.43844 -6.2341571 -5.8681555 -5.4655523 -5.2739191 -5.3381424 -5.5611029 -5.9569459 -6.0072575 -5.8622313 -5.7026792 -5.3311558 -4.865737]]...]
INFO - root - 2017-12-15 19:06:03.642606: step 49210, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 49h:20m:19s remains)
INFO - root - 2017-12-15 19:06:10.054220: step 49220, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.643 sec/batch; 50h:33m:44s remains)
INFO - root - 2017-12-15 19:06:16.387274: step 49230, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 49h:31m:19s remains)
INFO - root - 2017-12-15 19:06:22.799205: step 49240, loss = 0.27, batch loss = 0.15 (11.7 examples/sec; 0.682 sec/batch; 53h:38m:57s remains)
INFO - root - 2017-12-15 19:06:29.259270: step 49250, loss = 0.26, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 52h:58m:08s remains)
INFO - root - 2017-12-15 19:06:35.637141: step 49260, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 49h:34m:42s remains)
INFO - root - 2017-12-15 19:06:42.081245: step 49270, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 49h:04m:56s remains)
INFO - root - 2017-12-15 19:06:48.443497: step 49280, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 49h:32m:39s remains)
INFO - root - 2017-12-15 19:06:54.811918: step 49290, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 49h:43m:46s remains)
INFO - root - 2017-12-15 19:07:01.166872: step 49300, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 49h:25m:20s remains)
2017-12-15 19:07:01.697066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3797245 -6.5260315 -6.2871108 -6.1611819 -6.0430326 -5.6931992 -5.0961819 -4.7808294 -4.2796507 -3.8236852 -4.0739722 -4.6986222 -4.912611 -6.25511 -7.026473][-5.782455 -5.4969463 -5.3503275 -5.617218 -5.8027692 -5.3872204 -5.00811 -4.8172235 -4.3434353 -4.3357735 -4.3002605 -4.6655235 -4.824482 -5.9546213 -7.0392642][-5.1429353 -4.7394466 -4.3539991 -3.9876778 -3.7472389 -3.6727881 -3.8579628 -3.9619546 -3.6091495 -3.2725811 -3.0087638 -3.7481065 -4.1583471 -5.3594656 -6.4656882][-3.6506615 -3.3439217 -2.784142 -2.1494703 -2.2858357 -1.8028283 -1.1358404 -1.323554 -1.2269912 -2.0689402 -2.6615973 -3.5057659 -3.807862 -4.6531248 -5.7043667][-3.1422734 -2.1658516 -1.6300979 -0.93891907 -0.39903498 0.22470951 0.51364613 0.38474178 0.59008884 -0.41439009 -1.2670116 -2.3898644 -3.2789068 -4.6638088 -5.7136765][-1.3896532 -1.3559217 -0.40851402 0.48378849 0.81037235 1.6028757 2.1046515 1.8249702 1.6678362 0.42981815 -0.830482 -2.1633015 -3.2391729 -4.7861633 -5.8857727][-1.1045809 -0.7533741 -0.042588711 0.72472 1.4458981 1.9702044 2.4833565 2.5003939 2.3511782 1.0090113 -0.8041296 -2.5457048 -3.8936646 -5.4622593 -6.1649036][-0.95202637 -0.92955256 -0.44590712 0.651247 1.5591965 2.086607 2.2913857 2.5216141 2.4077921 1.0924368 -0.55075741 -2.6109571 -4.1064558 -5.8068752 -6.8804007][-2.3561673 -1.5553608 -0.44532681 0.057343006 0.51896858 1.4298677 1.9478254 2.0491772 1.986331 0.846962 -0.72548532 -2.4198213 -3.7599418 -5.7984276 -7.13882][-2.3879218 -1.5796499 -0.97782707 -0.5655632 -0.20920801 0.16715622 0.38560677 0.89073372 1.1001406 0.023082256 -1.1936007 -2.9650331 -4.5882225 -5.959219 -7.0410786][-4.3037224 -3.3753605 -2.5141563 -2.1173248 -1.9222093 -1.8030829 -1.5374217 -1.7092152 -1.873919 -2.4215331 -3.3806796 -4.1585765 -5.1104622 -6.0357447 -6.6534986][-5.5205755 -4.8871174 -3.819133 -3.4226775 -3.2126484 -3.3693624 -3.1092267 -3.0237517 -2.9691539 -3.4988837 -4.4496365 -5.094614 -5.6542349 -5.6530437 -5.9521594][-5.9638839 -5.37796 -4.6869969 -4.2465143 -3.6557527 -3.9352722 -4.0719786 -4.2677612 -4.4175253 -3.9101439 -4.4866447 -4.8925953 -5.2735734 -5.3838449 -5.3115511][-6.0290065 -5.3659554 -4.8314075 -4.7219958 -4.6728306 -4.4473228 -4.5299063 -5.0175686 -5.1335168 -5.0210676 -5.4412117 -5.1678252 -4.9665928 -4.6167479 -4.7785683][-7.6182404 -6.9028816 -6.3249741 -6.2153888 -6.2576051 -6.2093678 -6.1570945 -6.081563 -6.4562516 -5.9263721 -5.1826458 -4.8560572 -4.9253769 -4.632575 -4.3314419]]...]
INFO - root - 2017-12-15 19:07:08.115657: step 49310, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 51h:21m:49s remains)
INFO - root - 2017-12-15 19:07:14.567934: step 49320, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 49h:54m:15s remains)
INFO - root - 2017-12-15 19:07:20.918862: step 49330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 50h:03m:33s remains)
INFO - root - 2017-12-15 19:07:27.308722: step 49340, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 51h:50m:18s remains)
INFO - root - 2017-12-15 19:07:33.718463: step 49350, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 51h:00m:21s remains)
INFO - root - 2017-12-15 19:07:40.194901: step 49360, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.641 sec/batch; 50h:24m:51s remains)
INFO - root - 2017-12-15 19:07:46.663709: step 49370, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 50h:49m:08s remains)
INFO - root - 2017-12-15 19:07:53.163952: step 49380, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.674 sec/batch; 53h:01m:06s remains)
INFO - root - 2017-12-15 19:07:59.640929: step 49390, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 52h:50m:59s remains)
INFO - root - 2017-12-15 19:08:06.070012: step 49400, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 50h:24m:09s remains)
2017-12-15 19:08:06.625374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1762877 -3.9108906 -3.5248179 -5.43622 -6.6945992 -7.1192112 -6.47093 -5.0973015 -3.9912822 -3.8603797 -3.0885005 -3.7794907 -6.553772 -6.5538487 -6.7881236][-3.4344482 -2.975172 -3.527544 -4.3676357 -4.4861326 -4.864933 -5.9521418 -6.5400715 -5.4548788 -5.9965343 -5.29429 -5.0427904 -4.8305411 -4.9791641 -4.9875479][-5.6852894 -3.7690432 -3.7489524 -2.5695643 -2.1955705 -3.4548607 -3.4190831 -5.0468988 -5.1832275 -7.5526791 -7.3700919 -6.8253331 -7.5482807 -6.4850554 -4.6432896][-6.5772738 -5.6154661 -5.5771084 -4.2536464 -3.0765271 -2.1726122 -1.6213398 -1.9690137 -2.4396234 -4.5669103 -6.3092866 -7.2675948 -7.9117527 -7.1338634 -6.710556][-8.5259762 -7.1150584 -5.98222 -3.4842763 -2.296711 -1.9268103 -1.2557421 -1.135673 -0.98452425 -3.0362139 -4.1688967 -5.7700882 -7.2002172 -8.4036055 -7.6638222][-4.5157242 -4.122963 -3.4913311 -1.5125675 0.3267765 1.9744072 1.6985426 1.4539776 1.0740204 -0.73256159 -1.3691154 -2.9878001 -4.9236016 -6.3849936 -6.4852591][-3.568131 -3.1158495 -1.8605614 0.41447163 2.4362717 3.2511148 3.3922806 3.4338217 1.7749872 -0.67767763 -1.4239931 -3.2353215 -5.0953846 -6.1298723 -6.5971818][-1.3902636 -0.26599026 0.40757751 1.380105 1.5316887 2.783884 4.6132326 3.7003336 2.5783081 0.16213417 -2.5941796 -3.5367718 -5.8188953 -5.7583284 -5.3102741][-1.9894004 -0.084559441 0.29139662 1.0557528 1.583374 2.9496937 3.5438471 3.4207048 4.4878531 0.083940029 -2.4176092 -3.5363989 -5.7911253 -5.6083441 -5.0147038][-4.41006 -3.1277137 -1.9686098 -0.61310291 -0.474545 0.705595 0.41748047 2.0236969 2.4660673 -0.63963032 -2.0160165 -3.9642072 -5.8297615 -6.1536784 -5.8441324][-7.5622897 -6.9966879 -5.9890623 -5.5006714 -4.1169996 -3.8091593 -3.4351163 -2.8291807 -2.7623329 -3.5162625 -4.6919975 -5.33062 -5.6810293 -6.6989808 -7.4319396][-9.2528934 -9.1294727 -7.7202215 -6.8532796 -6.3810806 -6.0602407 -6.7877197 -6.2308 -5.8625164 -5.1305161 -5.3965282 -4.9514771 -6.1950207 -7.7653537 -7.7939591][-9.996563 -9.5031481 -8.5854969 -7.9980083 -7.3570614 -8.0296469 -7.8210974 -8.0877552 -8.0959606 -7.7655916 -7.3449368 -6.9679241 -6.5277581 -6.5155177 -7.0267491][-9.0472059 -8.9216909 -8.54903 -8.2752428 -7.0296116 -6.2559471 -5.711175 -6.1280651 -7.011703 -7.4021249 -7.2880149 -6.2992735 -6.1990004 -6.4749293 -5.9894514][-7.2633991 -8.5971355 -9.02406 -8.448534 -8.1422825 -7.3007421 -6.448844 -5.8911581 -6.9277964 -7.6779733 -8.3046656 -8.5059023 -8.5398149 -7.5430384 -6.5218782]]...]
INFO - root - 2017-12-15 19:08:13.065301: step 49410, loss = 0.32, batch loss = 0.21 (12.1 examples/sec; 0.661 sec/batch; 51h:57m:51s remains)
INFO - root - 2017-12-15 19:08:19.446841: step 49420, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 48h:28m:50s remains)
INFO - root - 2017-12-15 19:08:25.822250: step 49430, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 50h:31m:01s remains)
INFO - root - 2017-12-15 19:08:32.214012: step 49440, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 49h:00m:53s remains)
INFO - root - 2017-12-15 19:08:38.599361: step 49450, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 50h:50m:13s remains)
INFO - root - 2017-12-15 19:08:45.008135: step 49460, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 51h:30m:43s remains)
INFO - root - 2017-12-15 19:08:51.408771: step 49470, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 50h:52m:41s remains)
INFO - root - 2017-12-15 19:08:57.932599: step 49480, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 50h:29m:59s remains)
INFO - root - 2017-12-15 19:09:04.330620: step 49490, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 49h:08m:20s remains)
INFO - root - 2017-12-15 19:09:10.750946: step 49500, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 49h:27m:12s remains)
2017-12-15 19:09:11.289981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.7116013 -8.54483 -7.5347867 -6.9184518 -5.548789 -4.4905972 -3.2488918 -2.4616151 -1.5226002 -1.5020118 -2.5366058 -3.9820428 -5.6985111 -7.4264178 -8.7008657][-8.5947895 -8.5788364 -8.0514326 -8.0035028 -7.729331 -6.9285054 -5.80317 -4.2093563 -2.8885031 -2.7230167 -2.813169 -4.5037804 -6.2497253 -7.1344328 -8.8410006][-9.2709036 -8.5765648 -7.7912536 -7.0720029 -6.6157875 -5.7938404 -5.0824652 -3.8510787 -2.8849354 -2.6946297 -2.900497 -4.9592247 -6.1667404 -6.9452353 -8.329133][-8.6623583 -8.1989317 -7.5237231 -6.2400379 -4.8256807 -3.6457705 -2.1626954 -1.6938882 -1.2010875 -1.2787352 -2.2410569 -4.495286 -5.9493856 -6.9694004 -7.7853351][-9.3744507 -8.0317221 -6.7783608 -5.6944427 -3.908335 -2.3586202 -0.47561932 0.05062294 0.49453259 -0.21445799 -1.2730327 -3.5809336 -5.4503036 -6.4953384 -7.507462][-7.2601819 -6.5828495 -6.2168684 -4.6007648 -2.9505458 -1.1952481 0.75764561 1.1002693 1.376543 0.57570076 -0.66890812 -3.35358 -4.8812232 -5.5698261 -6.34842][-6.6976781 -5.1270189 -3.8057096 -2.3658223 -1.0936055 0.224967 1.812254 2.3398409 2.4991169 0.95375347 -0.42911434 -3.1902723 -5.0568705 -5.6802988 -6.5223055][-6.176465 -4.9629602 -3.3840303 -0.777575 1.1308165 2.2482462 3.0814152 2.8831444 2.8764524 1.5760012 0.14561749 -3.0392938 -4.8472738 -5.6306248 -6.4801598][-5.2783551 -4.292007 -3.0819364 -1.2799959 0.35485649 1.8788729 2.1211052 2.1066418 2.2700949 0.9591198 -0.20890522 -3.3431826 -5.281044 -6.0071416 -6.6635108][-4.3947392 -3.5264511 -3.3750901 -1.8129997 -0.59415245 0.40390587 1.2888803 1.2301922 0.81900692 -0.60876465 -2.2001648 -4.9110756 -6.4352207 -7.0702786 -7.3394752][-3.3909717 -3.283464 -2.8531232 -1.947803 -1.6916571 -0.68104649 0.24700928 0.078836918 -0.13369799 -1.7630801 -3.1532755 -5.4951305 -7.4527593 -8.1217632 -8.1899872][-4.9518981 -3.8680756 -2.7941484 -1.875689 -1.3176708 -1.084435 -0.65782785 -0.64615917 -0.92186403 -2.3738475 -3.8738031 -6.1241188 -7.3630681 -7.7003174 -8.1659994][-4.423007 -4.261323 -3.7325504 -2.8318205 -2.7016687 -1.7451005 -1.5806751 -2.0546112 -2.8206577 -3.9661386 -4.8519745 -6.3440971 -7.1836681 -7.3883171 -7.2113981][-5.1615505 -4.3414321 -3.7236714 -3.1552625 -2.562799 -2.2212644 -2.5499315 -3.1765232 -3.9284587 -5.1233454 -5.9432926 -6.6232719 -6.6586347 -6.6472316 -6.9066968][-5.488656 -5.0645351 -4.5883493 -3.9464903 -3.3258476 -2.6857891 -2.4325738 -3.3910923 -4.864912 -5.8702364 -6.3251209 -6.9196925 -7.2379012 -7.1138592 -7.0675197]]...]
INFO - root - 2017-12-15 19:09:17.670622: step 49510, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 49h:35m:30s remains)
INFO - root - 2017-12-15 19:09:24.041241: step 49520, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:23m:20s remains)
INFO - root - 2017-12-15 19:09:30.366406: step 49530, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 49h:05m:12s remains)
INFO - root - 2017-12-15 19:09:36.824675: step 49540, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 49h:44m:48s remains)
INFO - root - 2017-12-15 19:09:43.249217: step 49550, loss = 0.32, batch loss = 0.21 (12.0 examples/sec; 0.665 sec/batch; 52h:14m:21s remains)
INFO - root - 2017-12-15 19:09:49.653845: step 49560, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 51h:25m:03s remains)
INFO - root - 2017-12-15 19:09:56.059668: step 49570, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 49h:59m:23s remains)
INFO - root - 2017-12-15 19:10:02.454651: step 49580, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 50h:50m:34s remains)
INFO - root - 2017-12-15 19:10:08.824380: step 49590, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 49h:02m:03s remains)
INFO - root - 2017-12-15 19:10:15.182510: step 49600, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 50h:48m:43s remains)
2017-12-15 19:10:15.712164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5401611 -4.77625 -5.061101 -5.1231084 -5.3096294 -5.3080349 -4.6981268 -4.3444748 -3.708107 -3.9454126 -4.8175926 -5.4020605 -6.0142083 -6.7212977 -7.3624682][-4.7553277 -4.5434713 -4.5467348 -4.8680258 -5.41923 -5.7173572 -5.3073387 -4.7281942 -3.9875362 -4.3195086 -5.191679 -6.0010557 -7.2067666 -7.6522059 -7.9374433][-4.7844391 -4.4877596 -4.6423235 -4.6386633 -4.6804609 -4.6804113 -4.6419926 -4.5516834 -3.9579427 -4.3797464 -5.2765608 -6.2379804 -7.271349 -8.2062006 -9.128334][-5.4651008 -4.5481319 -4.0685968 -3.5873942 -3.4059043 -3.4271803 -3.0943608 -2.7393761 -2.2541041 -3.1153073 -4.6506977 -5.7382364 -6.8630161 -7.6390948 -8.4388218][-5.2060242 -3.9468746 -2.956975 -2.3369241 -1.9286237 -1.2656846 -0.3022995 0.23616505 0.70362759 -0.537539 -2.2664304 -3.8869832 -5.637866 -6.908041 -7.9300046][-5.3487844 -4.300396 -3.223289 -1.7196245 -0.40441895 0.43040943 1.3207827 1.7247219 2.5902481 1.4229097 -0.85692263 -2.7992692 -4.6298194 -5.8900766 -7.1314731][-4.7967958 -3.812849 -2.5381908 -0.84751606 0.92239 2.4085541 3.4674873 3.5010777 3.9170713 2.6796007 0.67750359 -1.4490137 -3.4650021 -4.7747488 -6.1579843][-4.9100828 -3.8132775 -2.2746534 -0.0065875053 2.2544003 3.4290619 4.6791592 5.2519846 5.418519 3.3942585 0.61612225 -1.5282207 -3.4333706 -4.716773 -5.7492771][-5.5396552 -4.8522558 -3.9355378 -2.2516904 -0.18309546 1.5582933 2.9783859 3.7798662 4.5521326 2.8373308 0.014576435 -2.3824949 -4.455369 -5.5548124 -6.2131491][-7.31494 -6.4504843 -5.2363186 -4.0264649 -2.6007676 -1.0960045 -0.20734882 0.30397749 0.70135784 -0.65955639 -2.7871537 -4.8842974 -6.6323128 -7.201478 -7.5865388][-9.3281078 -9.0360126 -8.3996439 -7.1566772 -5.8106608 -4.7332411 -3.6150961 -3.2198906 -3.4395761 -4.673553 -6.2189083 -7.35255 -8.6177616 -9.171258 -9.410635][-10.587023 -9.8737011 -8.8384666 -8.540782 -7.8041573 -6.7959428 -5.8987417 -5.4518414 -5.0528994 -5.9623747 -7.1712508 -7.3499508 -8.0676947 -8.6940823 -9.0945654][-9.6602936 -9.6023865 -9.290844 -8.4682074 -7.8083854 -7.1707234 -6.4511008 -6.2682071 -6.1060324 -6.1907206 -6.7755375 -7.1567111 -7.4715075 -7.3079295 -7.4857206][-7.6562715 -7.8354788 -7.9279823 -7.6293936 -7.1135306 -6.6204042 -6.1018457 -5.9728756 -6.1470513 -6.8038077 -7.525291 -7.50345 -7.4695473 -7.0528708 -6.7880325][-6.9307613 -6.8772211 -7.0554 -6.9020886 -7.0080528 -6.7434239 -6.308928 -6.4496236 -6.6395464 -6.6702795 -6.7430177 -6.8241549 -6.8522415 -6.74152 -6.4377046]]...]
INFO - root - 2017-12-15 19:10:22.171190: step 49610, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 51h:06m:41s remains)
INFO - root - 2017-12-15 19:10:28.598044: step 49620, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 50h:13m:42s remains)
INFO - root - 2017-12-15 19:10:34.955358: step 49630, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 49h:44m:05s remains)
INFO - root - 2017-12-15 19:10:41.338515: step 49640, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 50h:27m:29s remains)
INFO - root - 2017-12-15 19:10:47.841410: step 49650, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 50h:49m:07s remains)
INFO - root - 2017-12-15 19:10:54.238253: step 49660, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 52h:22m:49s remains)
INFO - root - 2017-12-15 19:11:00.657323: step 49670, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 52h:02m:36s remains)
INFO - root - 2017-12-15 19:11:07.024916: step 49680, loss = 0.23, batch loss = 0.12 (11.5 examples/sec; 0.699 sec/batch; 54h:53m:18s remains)
INFO - root - 2017-12-15 19:11:13.483502: step 49690, loss = 0.35, batch loss = 0.23 (11.9 examples/sec; 0.671 sec/batch; 52h:44m:02s remains)
INFO - root - 2017-12-15 19:11:20.052583: step 49700, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 50h:10m:05s remains)
2017-12-15 19:11:20.538949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2487507 -8.7913437 -10.941219 -11.876862 -13.182329 -12.091503 -11.509376 -9.8513317 -8.673562 -8.7582588 -7.8566651 -10.252445 -8.1228018 -7.684917 -7.7933455][-6.4147243 -8.8499432 -11.081314 -12.588324 -14.706472 -14.440536 -13.838173 -10.242875 -8.3116961 -8.5096188 -7.8982496 -11.036381 -10.304682 -10.738943 -9.3475332][-6.7071714 -7.8738251 -9.5717955 -10.852855 -12.65801 -12.415781 -11.935981 -10.484121 -9.0731668 -8.40095 -7.2310729 -10.612291 -10.309334 -11.334969 -11.34167][-6.828228 -7.2802792 -8.7792072 -9.005127 -9.883975 -9.87763 -8.806879 -8.3015251 -7.5234928 -9.3652029 -7.8456054 -10.860752 -9.8560181 -11.153584 -12.043291][-6.1625395 -6.3610845 -7.2175946 -6.7065077 -6.5278554 -4.71476 -3.9331057 -4.053484 -3.3708711 -6.7752352 -7.8695822 -12.061061 -10.585652 -11.442653 -11.973925][-6.5160036 -7.4579253 -7.47312 -5.380374 -4.401001 -2.0530987 0.6531477 2.1390657 2.8688545 -2.3136134 -4.28119 -10.325851 -11.09863 -12.176139 -12.105024][-5.9038215 -6.1043744 -6.4097271 -4.0042548 -2.675189 0.51199245 2.722023 4.5860424 5.6798096 2.1376438 -0.20763254 -7.7487779 -9.8812141 -12.105968 -12.578211][-7.1643157 -6.3578343 -5.6310744 -2.5782208 -1.0952935 2.3256617 5.0449295 5.2310753 4.7241125 2.1373348 0.87975121 -5.5636144 -7.8090591 -11.552087 -12.836841][-8.36181 -7.3406825 -6.6552858 -4.2876453 -2.2144771 1.0440798 2.9913368 3.0874519 2.8007584 -1.2948475 -2.9028172 -7.7469034 -8.106493 -10.668953 -11.960886][-9.8629189 -9.5741987 -8.7063675 -6.7464552 -4.7254863 -2.6345048 -0.8934927 0.78436852 1.1856279 -3.3618422 -5.7993083 -10.953255 -10.857239 -11.40658 -11.045819][-9.99581 -10.888931 -10.492752 -8.6226883 -6.8325992 -4.9878159 -3.2241631 -3.5130868 -3.062376 -5.1547747 -5.7387052 -10.313063 -11.721831 -12.010365 -11.71209][-9.2781935 -9.7798891 -10.556149 -9.2838745 -8.8325024 -6.7787261 -5.1418324 -5.4564309 -5.9795122 -7.5816031 -7.5491347 -8.9273758 -10.462858 -10.831223 -10.636881][-10.520325 -9.8028049 -9.3918142 -8.0899973 -7.8738585 -6.9418836 -5.7393832 -5.6887031 -6.3295693 -7.7062798 -8.2814054 -8.1169987 -7.6333103 -7.7419558 -8.0147324][-9.32352 -9.4214058 -8.8140745 -7.3922062 -6.9716673 -5.6127625 -5.3800583 -6.0432224 -6.2940359 -7.0519753 -7.6049056 -7.5535469 -7.1404924 -6.0450816 -5.3059263][-7.621491 -8.4123917 -8.90826 -8.5785522 -7.3546424 -5.9780207 -5.7645636 -6.0244946 -6.4504414 -7.231575 -7.5417018 -7.72253 -7.5341363 -6.8576813 -6.1001143]]...]
INFO - root - 2017-12-15 19:11:26.905548: step 49710, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:52m:52s remains)
INFO - root - 2017-12-15 19:11:33.215471: step 49720, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 51h:01m:40s remains)
INFO - root - 2017-12-15 19:11:39.680679: step 49730, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 51h:54m:38s remains)
INFO - root - 2017-12-15 19:11:46.083238: step 49740, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 49h:00m:25s remains)
INFO - root - 2017-12-15 19:11:52.506295: step 49750, loss = 0.36, batch loss = 0.24 (11.9 examples/sec; 0.672 sec/batch; 52h:46m:16s remains)
INFO - root - 2017-12-15 19:11:58.934566: step 49760, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 50h:57m:22s remains)
INFO - root - 2017-12-15 19:12:05.377253: step 49770, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 52h:19m:55s remains)
INFO - root - 2017-12-15 19:12:11.797128: step 49780, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 51h:08m:52s remains)
INFO - root - 2017-12-15 19:12:18.164558: step 49790, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 50h:19m:57s remains)
INFO - root - 2017-12-15 19:12:24.585346: step 49800, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 49h:54m:42s remains)
2017-12-15 19:12:25.141496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9557805 -6.2691245 -5.7542067 -5.3999152 -4.9053011 -4.071908 -3.3233252 -3.1201997 -2.8590918 -4.4359684 -5.2715034 -6.2874336 -7.2188163 -8.2450047 -8.8623676][-5.60548 -5.1703262 -4.7401695 -4.4763327 -4.3359251 -4.1072407 -3.4634829 -2.9550791 -2.8515415 -4.7360878 -5.671524 -6.2872047 -7.236742 -8.503933 -8.9100685][-5.5058813 -4.6029139 -4.2941818 -3.97775 -3.2864542 -2.9639978 -2.7614322 -2.5486417 -2.3364129 -4.1527367 -5.3895612 -6.4446859 -7.45011 -8.2169418 -8.8214369][-6.5577583 -5.2671752 -4.3786073 -3.1147656 -2.2946768 -1.422574 -0.80161858 -1.1173253 -1.2695804 -2.9612517 -4.2157221 -5.4530587 -6.94095 -7.8067536 -7.9435444][-7.6694541 -5.374886 -3.4001141 -2.1166964 -1.0800695 -0.22369242 0.31028175 0.30387592 0.17374516 -1.7905703 -3.1792288 -4.4410524 -5.8621216 -6.835134 -7.2850304][-8.0668974 -6.4454355 -4.2162008 -1.4981141 -0.24701977 0.68825531 1.8703165 1.7165642 1.7251339 0.050551891 -1.0218167 -1.8975234 -3.612113 -5.1680565 -6.3012476][-8.45856 -5.831306 -3.0831041 -1.1616426 0.25828123 2.0177727 3.2097359 3.1484489 2.9761143 1.1482 0.24238777 -0.54223442 -2.0915828 -3.6252193 -4.5480642][-7.3325267 -5.7881479 -4.2353973 -0.96088362 1.2928543 2.2781849 2.7666559 2.6934423 2.9167595 1.1714773 0.12407494 -0.61494112 -2.6675997 -4.140233 -4.8564448][-8.26276 -6.4877481 -4.115417 -1.8972321 0.12391996 1.1508942 1.7738218 2.0816422 2.2257957 0.37295914 -0.92899323 -1.8216991 -3.0907006 -4.5327644 -5.2983141][-9.7866545 -8.4899206 -7.0038919 -3.9358077 -1.1399031 0.56864357 1.4469109 1.2361727 1.3523388 -0.68317032 -2.0509453 -2.6249013 -4.1140504 -4.9778547 -4.9566278][-10.843642 -10.30112 -8.87769 -6.2756162 -3.8708034 -2.0339751 -1.4818425 -1.4568133 -1.5785236 -2.84542 -4.1399736 -5.1815834 -5.6719589 -5.9890881 -6.1668696][-11.936783 -11.12128 -9.8692017 -8.0084162 -5.7066536 -3.66951 -2.7726622 -2.7902617 -2.922915 -3.6600809 -4.879467 -5.2183971 -5.6603479 -6.4819336 -6.9449177][-12.664007 -12.381795 -11.456998 -9.7315178 -7.4371996 -4.9553328 -2.8897433 -2.1902919 -2.1391306 -3.1413102 -4.0611258 -4.3331337 -4.8688536 -5.0091143 -5.06061][-12.827058 -12.358633 -11.630586 -10.479386 -8.6125078 -6.296689 -4.371562 -3.4848571 -2.9365134 -2.8396163 -3.2855062 -3.8193018 -4.0727491 -4.4497328 -5.0854111][-12.543329 -12.577776 -12.299171 -11.44633 -10.082199 -8.218524 -6.2684917 -5.3376951 -4.4993353 -4.1741323 -3.9525905 -3.8115234 -4.0727053 -4.5163603 -5.1614151]]...]
INFO - root - 2017-12-15 19:12:31.537482: step 49810, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 48h:48m:49s remains)
INFO - root - 2017-12-15 19:12:37.941250: step 49820, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 49h:30m:19s remains)
INFO - root - 2017-12-15 19:12:44.325186: step 49830, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 50h:52m:23s remains)
INFO - root - 2017-12-15 19:12:50.775516: step 49840, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 50h:58m:39s remains)
INFO - root - 2017-12-15 19:12:57.142406: step 49850, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 48h:24m:45s remains)
INFO - root - 2017-12-15 19:13:03.537391: step 49860, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 50h:06m:40s remains)
INFO - root - 2017-12-15 19:13:09.895697: step 49870, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.642 sec/batch; 50h:22m:57s remains)
INFO - root - 2017-12-15 19:13:16.389038: step 49880, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 50h:45m:45s remains)
INFO - root - 2017-12-15 19:13:22.753769: step 49890, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 50h:14m:58s remains)
INFO - root - 2017-12-15 19:13:29.243208: step 49900, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 50h:57m:58s remains)
2017-12-15 19:13:29.751130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.168993 -4.7200665 -6.1267247 -7.0052452 -7.2742462 -7.2306743 -6.5909328 -5.6690736 -4.5229058 -4.4201174 -5.0948458 -6.2945962 -6.7425361 -7.057785 -6.9157457][-3.2713742 -4.5364003 -5.8089786 -6.61316 -6.6491504 -5.9942846 -5.5264359 -5.1871271 -4.74385 -5.10845 -6.0511227 -7.2363853 -7.3864083 -7.4954629 -7.1164927][-3.1386142 -3.8544767 -4.6920652 -5.1170044 -4.9706497 -4.4514685 -4.079083 -4.1203127 -4.188251 -5.0992413 -6.6695919 -7.9887242 -8.08101 -8.3594 -7.9421039][-2.8138595 -3.0300984 -2.9987812 -3.6094613 -3.3548489 -3.0959105 -2.5893426 -2.6637902 -3.0428224 -3.6557608 -5.1263409 -6.7888608 -7.7814765 -8.3556261 -8.2435684][-3.3897891 -3.4758396 -3.2353797 -2.4114618 -1.3725305 -0.49087191 -0.628654 -1.2697973 -2.0652742 -3.4004679 -5.0051928 -6.5185957 -6.9600959 -7.7422514 -7.8792133][-4.8719282 -4.5781031 -3.7778687 -2.1234441 -0.52122545 1.0663548 1.6688356 0.59784889 -1.1748543 -3.085536 -5.3773584 -6.9618759 -6.9633541 -7.052444 -6.7971034][-4.3472958 -4.4473686 -3.6051159 -1.6148701 1.3416395 3.1387825 4.1511421 3.5956316 2.1257305 -0.84476566 -3.9946802 -6.2247391 -6.6130524 -6.6136851 -6.286159][-3.8985147 -3.5223055 -2.8328924 -0.45179558 2.08646 4.7379179 5.9940624 5.1839113 3.7455015 0.94821835 -2.0393615 -5.059701 -5.7014055 -6.0525212 -6.0232325][-4.2965765 -3.4719553 -2.6151457 -0.97419167 0.88154316 2.966733 4.1369705 3.8676758 2.661005 0.027127266 -2.7375717 -5.0586877 -5.8373661 -6.3864841 -6.2903056][-6.1052094 -5.6838584 -4.7292662 -2.9777832 -1.1710987 0.34451389 1.2331734 1.4634848 1.2124529 -1.5688782 -4.6757793 -6.8791218 -6.767725 -6.8463926 -6.912447][-7.5638995 -7.4140487 -6.4884019 -5.2408695 -3.8945434 -2.3012757 -1.7392607 -1.2041063 -1.1947732 -2.9393282 -5.096611 -6.995616 -7.535284 -7.853785 -7.1358337][-7.442781 -7.1709194 -6.9864283 -5.9257975 -5.5479422 -4.6591039 -3.7436931 -3.8368707 -4.0769615 -4.4829865 -5.3565111 -6.9703827 -6.6197462 -7.5550141 -7.4139977][-8.3200645 -7.4109898 -6.4889216 -6.1611166 -5.5770426 -5.6439438 -5.7848959 -5.52456 -5.1633358 -5.8285103 -6.2663226 -7.2474575 -7.6300421 -8.2416267 -8.1619825][-8.4297686 -8.1725979 -7.0194988 -6.0318642 -6.1646314 -6.1345959 -6.7414579 -7.3651567 -7.8497295 -7.8851624 -7.3608789 -7.2407846 -7.2041855 -7.8452544 -7.8069911][-9.2971706 -9.2310333 -8.8019867 -8.2590075 -7.1216388 -7.0181527 -7.1281557 -7.5809855 -7.9855866 -8.2554531 -8.3287525 -7.9051366 -7.1225381 -6.8321104 -6.787746]]...]
INFO - root - 2017-12-15 19:13:36.088643: step 49910, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 51h:05m:30s remains)
INFO - root - 2017-12-15 19:13:42.555373: step 49920, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 52h:09m:47s remains)
INFO - root - 2017-12-15 19:13:48.931544: step 49930, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.620 sec/batch; 48h:40m:26s remains)
INFO - root - 2017-12-15 19:13:55.301479: step 49940, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.623 sec/batch; 48h:51m:40s remains)
INFO - root - 2017-12-15 19:14:01.705886: step 49950, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.618 sec/batch; 48h:28m:45s remains)
INFO - root - 2017-12-15 19:14:08.087993: step 49960, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 49h:55m:07s remains)
INFO - root - 2017-12-15 19:14:14.479712: step 49970, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 51h:25m:26s remains)
INFO - root - 2017-12-15 19:14:20.800654: step 49980, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 50h:49m:03s remains)
INFO - root - 2017-12-15 19:14:27.271958: step 49990, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:16m:37s remains)
INFO - root - 2017-12-15 19:14:33.758415: step 50000, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 49h:53m:55s remains)
2017-12-15 19:14:34.249570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2168622 -2.543613 -2.7726955 -2.9167523 -3.2640815 -3.7136395 -4.13688 -4.3643379 -4.2253628 -5.4393282 -6.4242554 -7.2728219 -7.5507493 -7.9749951 -8.0580959][-2.1873035 -2.473805 -3.13271 -3.9205563 -4.39207 -4.6721134 -5.141119 -5.6985044 -5.7552114 -6.8686275 -7.6858683 -8.537715 -8.8451309 -9.3133755 -9.635848][-1.3295646 -1.4983845 -1.7397051 -2.3090181 -2.9190555 -3.352118 -3.8287606 -4.4713554 -4.7132244 -5.9693842 -6.8497453 -7.7234178 -8.0992022 -8.5327978 -8.5634813][-0.55063725 -0.71970558 -0.94705725 -1.2258358 -1.5452948 -1.9014668 -2.253624 -2.84313 -2.9882531 -4.2535405 -5.2821584 -6.1792626 -6.77769 -7.5539627 -8.0558233][-1.27175 -1.0945845 -0.91500664 -0.81572056 -0.595726 -0.45922232 -0.3986845 -0.50193644 -0.48834562 -1.7515721 -2.9966679 -4.2376876 -5.0495949 -6.1170545 -6.8698416][-2.3231149 -1.6437807 -0.98369503 -0.68711042 -0.37171364 0.1469593 0.66243649 0.78305817 1.0705118 -0.26138783 -1.6091123 -2.848824 -3.9613056 -5.0490932 -5.8165216][-2.9710999 -2.0793614 -1.1528926 -0.33659935 0.44155979 0.87110233 1.3633671 1.8516083 2.330039 0.96530437 -0.34733582 -1.5994754 -2.6865635 -4.0835648 -5.1587329][-3.142663 -2.3453498 -1.5304794 -0.4177351 0.64126778 1.4703979 1.9856606 2.3331966 2.9303303 1.8889151 0.69571018 -0.71481657 -1.9124789 -3.1838808 -4.2525549][-3.0525889 -2.2283311 -1.6601658 -0.74922657 0.18858862 0.97145748 1.590827 1.8046694 2.0293674 0.7262516 -0.52747393 -1.688448 -2.5920558 -3.7499545 -4.5807381][-3.5798907 -3.0032978 -2.4296284 -1.4648314 -0.57000971 -0.10799503 0.39688683 0.89819431 1.2211018 -0.62864208 -2.037837 -3.3267236 -4.2731271 -5.14686 -5.7061405][-4.9720573 -4.276803 -3.7630947 -2.8596921 -2.1805596 -1.6924634 -1.0886035 -1.0129366 -0.96345663 -2.4914708 -3.8526046 -4.9075165 -5.4132547 -6.0312319 -6.4500332][-5.1475067 -4.86352 -4.4066076 -3.9016564 -3.3789 -2.7254682 -2.4064565 -2.6687841 -2.6423416 -3.8416235 -4.952158 -5.8612347 -6.2861609 -6.5973415 -6.5476189][-5.8206024 -5.3438449 -4.906847 -4.5327463 -4.1016912 -3.8681743 -3.7384191 -3.7482915 -3.8624239 -4.8178248 -5.5662227 -6.0656328 -6.5594988 -6.7937112 -6.4262333][-5.6053581 -5.0423689 -4.6179161 -4.2524948 -3.9323294 -3.6660523 -3.518827 -3.7827296 -3.9668393 -4.7753887 -5.0225048 -5.2678623 -5.5088868 -5.93653 -5.817812][-6.7332816 -6.2378759 -5.8793368 -5.5273104 -5.1783857 -4.9639139 -4.9018803 -5.0938087 -5.2197204 -5.3694105 -5.5345416 -5.8119211 -6.0334411 -5.6422858 -5.4950309]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:14:41.758229: step 50010, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 49h:46m:57s remains)
INFO - root - 2017-12-15 19:14:48.160554: step 50020, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 51h:15m:39s remains)
INFO - root - 2017-12-15 19:14:54.485812: step 50030, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 48h:45m:22s remains)
INFO - root - 2017-12-15 19:15:01.011837: step 50040, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 49h:17m:01s remains)
INFO - root - 2017-12-15 19:15:07.478920: step 50050, loss = 0.25, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 53h:39m:00s remains)
INFO - root - 2017-12-15 19:15:13.897682: step 50060, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 48h:53m:36s remains)
INFO - root - 2017-12-15 19:15:20.229808: step 50070, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 51h:11m:50s remains)
INFO - root - 2017-12-15 19:15:26.623223: step 50080, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 50h:31m:13s remains)
INFO - root - 2017-12-15 19:15:32.973994: step 50090, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 49h:10m:47s remains)
INFO - root - 2017-12-15 19:15:39.413809: step 50100, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:55m:40s remains)
2017-12-15 19:15:39.958293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3038688 -5.4297791 -5.4645114 -5.9106989 -6.0020127 -5.6412172 -5.1938581 -3.8002031 -2.4581037 -2.0064096 -1.9005637 -2.8300076 -3.6543617 -4.4190869 -5.0231056][-4.41352 -5.0711045 -5.448225 -5.6376543 -5.9047318 -5.9520221 -5.6224828 -4.8350377 -3.3331218 -2.7078242 -2.5149364 -3.155211 -3.7223716 -4.6731329 -5.1048393][-3.9219284 -4.2356291 -4.4725852 -4.9321651 -5.5868073 -5.7327223 -5.1981997 -4.4872 -3.4681 -3.4449825 -3.7704399 -4.2387586 -4.8306246 -5.2461185 -5.4147358][-3.4943352 -2.8625855 -2.5339746 -2.3285675 -2.4926043 -2.7079248 -2.3443341 -1.9703207 -1.5079823 -1.9028287 -2.6174412 -4.3324432 -5.8091459 -6.3161817 -6.9088969][-3.7099681 -2.5761876 -2.0047345 -0.70351744 0.12771893 0.64406204 1.0738392 0.9047451 1.2050123 0.20309496 -1.2247958 -3.0983543 -4.3385415 -5.9646964 -7.02352][-2.7885451 -1.9335537 -1.0199227 0.49745083 1.8710556 3.1701937 4.4705791 4.4269886 4.1337233 2.32479 0.50464344 -2.1637583 -3.5747361 -5.1504269 -5.8759065][-2.7777777 -1.6339698 -0.39309692 1.4443951 3.3095055 4.9066772 6.1300898 6.4348507 6.59007 5.05624 2.1147423 -0.59117937 -2.762774 -4.5748529 -5.0356445][-2.7467303 -2.3535151 -1.3248653 0.98667812 2.8411169 4.6246948 5.9843264 6.2622414 6.4424458 5.3071222 2.6240129 0.047254562 -2.3076973 -4.0146265 -5.2491293][-1.7285433 -1.4978242 -0.77783489 0.8272686 1.5582733 3.5114231 4.7617893 4.6200991 4.4015408 3.1088629 0.82066917 -1.4045362 -2.8820558 -4.398242 -5.3688126][-1.9388371 -1.6248913 -0.86948824 0.084824562 0.72691154 1.7615166 2.1412392 2.6505575 2.5911674 1.1556396 -0.8500104 -2.2603474 -3.9951978 -5.0928478 -5.6766491][-4.664567 -3.8198671 -2.7207208 -2.1911888 -1.7592149 -1.2906895 -1.1663542 -1.0298085 -1.3360801 -1.7884259 -3.3259521 -4.2036023 -5.3174415 -6.1303811 -6.38756][-7.3626428 -6.3792448 -5.5565405 -4.8739147 -4.3173766 -3.9668131 -3.9904215 -4.267786 -4.2227068 -4.3326683 -5.0824976 -5.4089212 -5.97711 -6.59951 -6.6523552][-7.6194258 -7.4250841 -6.6023693 -5.988596 -5.7808728 -5.4396157 -5.1182137 -5.409339 -5.5724697 -5.5285993 -5.639195 -5.4005113 -5.8485231 -6.2731271 -6.1203518][-8.6531839 -7.9999547 -7.4154224 -6.7922959 -6.4151497 -5.8899531 -5.510747 -5.6321926 -5.6955872 -5.531908 -5.8573442 -5.5909071 -5.5048771 -5.7602615 -5.8000364][-9.7425289 -8.8069592 -8.29608 -7.8099141 -7.3039637 -6.6418834 -6.6892543 -6.9360542 -6.8482571 -6.6597834 -6.6713972 -6.4255056 -6.1952524 -6.0482063 -5.9393988]]...]
INFO - root - 2017-12-15 19:15:46.364229: step 50110, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 50h:39m:46s remains)
INFO - root - 2017-12-15 19:15:52.725713: step 50120, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 49h:48m:37s remains)
INFO - root - 2017-12-15 19:15:59.095097: step 50130, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 49h:22m:25s remains)
INFO - root - 2017-12-15 19:16:05.467430: step 50140, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 50h:40m:24s remains)
INFO - root - 2017-12-15 19:16:11.843681: step 50150, loss = 0.30, batch loss = 0.18 (13.0 examples/sec; 0.615 sec/batch; 48h:12m:04s remains)
INFO - root - 2017-12-15 19:16:18.193619: step 50160, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 50h:43m:41s remains)
INFO - root - 2017-12-15 19:16:24.610247: step 50170, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 49h:55m:56s remains)
INFO - root - 2017-12-15 19:16:31.076102: step 50180, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 48h:23m:26s remains)
INFO - root - 2017-12-15 19:16:37.550423: step 50190, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 51h:01m:17s remains)
INFO - root - 2017-12-15 19:16:43.977360: step 50200, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 49h:29m:54s remains)
2017-12-15 19:16:44.485912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7801113 -2.7217283 -2.6853671 -3.16038 -3.5319443 -2.9403672 -2.5689764 -1.8573532 -1.2673759 -2.847331 -3.2480631 -4.8644495 -6.8633142 -7.0774 -7.0622382][-2.6832242 -3.0099568 -3.0978985 -3.3073554 -3.6665363 -3.3378954 -2.966464 -2.5367126 -2.08853 -3.4629431 -4.046011 -5.0800467 -6.2834668 -6.9198132 -7.4663148][-2.8668685 -3.0200343 -3.1863995 -3.7162251 -3.4278069 -3.4020982 -3.4736366 -3.1622219 -2.8029318 -4.017004 -4.2917457 -5.2486029 -6.685482 -6.8651943 -7.0559916][-3.2636175 -3.2106156 -3.6873598 -3.5829473 -3.033545 -2.9838743 -2.4008126 -2.4442472 -2.4095826 -3.7896385 -4.2872791 -5.0670624 -6.358295 -6.688271 -7.1626425][-3.8043754 -2.8266473 -2.5037618 -2.4739847 -2.401916 -1.5400662 -0.60549831 -0.63448954 -0.39771461 -1.9015379 -2.6119423 -3.9752765 -6.0444555 -6.4357615 -6.9345989][-3.1404357 -2.6416626 -2.4925027 -1.2611356 0.098784924 0.58896732 1.2202101 1.5250854 1.520874 -0.16130495 -0.98373318 -2.509119 -4.4834051 -5.3154535 -6.4622][-3.9904826 -3.5877719 -2.2946324 -0.21672106 0.70381546 2.1211662 2.9350481 3.2691441 3.6097221 1.9025402 1.263485 -0.58217859 -3.008945 -3.8811741 -5.1079693][-3.2433357 -3.4501643 -2.8106923 -0.73813581 1.3436832 2.8382978 3.7773256 4.5644274 4.8364668 2.9245043 2.0590458 0.70014381 -1.4192924 -2.3691435 -3.6716743][-3.2884445 -2.9601817 -1.8283339 -0.79468155 0.57927132 1.91996 3.1020927 3.9321318 4.36042 2.8241892 2.1200581 0.52777576 -1.4783368 -1.87883 -3.1219192][-4.3092008 -4.0419168 -2.5442476 -0.86552429 0.45330715 1.117486 1.6174307 2.0721006 2.9169731 1.5513258 1.225585 -0.46845484 -1.9005227 -2.3132052 -3.1261616][-5.9295745 -5.0260077 -3.9251852 -2.4511914 -1.3897052 -0.54289722 -0.2718811 0.1201992 0.6107645 -0.99529791 -0.78450441 -1.558413 -2.9488106 -4.0018234 -4.3958068][-6.4442616 -5.2077742 -4.7593288 -3.8525999 -3.2743468 -3.3022275 -3.1625018 -2.9050703 -2.8007522 -3.8219571 -3.8330185 -4.4088869 -4.6178989 -4.5851936 -4.9270806][-7.3382773 -6.630939 -5.9951506 -5.3876591 -5.4655619 -5.5324264 -5.4839067 -5.224885 -5.0818796 -5.2645068 -6.24928 -5.7769952 -5.8908772 -5.7574759 -5.2210207][-7.92731 -7.8451891 -7.7477279 -7.1505132 -7.3670783 -7.5453434 -7.9681487 -7.48422 -7.1730337 -6.8561831 -6.7700229 -6.81045 -6.7921414 -6.5354891 -5.8520322][-7.6641536 -7.2333021 -7.5280476 -8.3632164 -9.2769423 -9.4584675 -9.6041021 -9.2702084 -8.82809 -8.4306993 -8.1090069 -7.2456403 -6.5900192 -6.6667066 -6.316041]]...]
INFO - root - 2017-12-15 19:16:50.848316: step 50210, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 49h:50m:14s remains)
INFO - root - 2017-12-15 19:16:57.299133: step 50220, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 49h:42m:44s remains)
INFO - root - 2017-12-15 19:17:03.635507: step 50230, loss = 0.36, batch loss = 0.25 (12.6 examples/sec; 0.637 sec/batch; 49h:57m:11s remains)
INFO - root - 2017-12-15 19:17:10.039412: step 50240, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 50h:19m:35s remains)
INFO - root - 2017-12-15 19:17:16.439454: step 50250, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 49h:31m:17s remains)
INFO - root - 2017-12-15 19:17:22.845108: step 50260, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 51h:09m:30s remains)
INFO - root - 2017-12-15 19:17:29.219074: step 50270, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 51h:24m:59s remains)
INFO - root - 2017-12-15 19:17:35.658187: step 50280, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 50h:49m:48s remains)
INFO - root - 2017-12-15 19:17:41.966977: step 50290, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 49h:51m:43s remains)
INFO - root - 2017-12-15 19:17:48.280886: step 50300, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.618 sec/batch; 48h:24m:22s remains)
2017-12-15 19:17:48.773383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.695611 -6.0360365 -6.1417408 -6.2026515 -6.116334 -5.7229376 -4.9526625 -4.0246754 -2.9266696 -3.173728 -5.0733247 -6.8759971 -7.690649 -9.0695324 -9.8316221][-6.2452745 -6.8572235 -7.0546522 -7.0740395 -6.9418974 -6.3914118 -6.3128452 -5.7594728 -4.5390797 -4.7160196 -5.9784117 -7.8764191 -8.8688011 -10.253633 -10.47749][-5.7125959 -5.7582655 -5.8690224 -6.0452967 -5.808208 -5.4155979 -4.8934259 -4.2985964 -3.736891 -4.1636629 -5.842216 -7.5768318 -8.6804848 -9.8497477 -10.130493][-4.7234879 -4.5264039 -4.6205645 -4.4805727 -4.1764903 -3.6326785 -2.6376367 -1.7882295 -0.77543163 -1.5741224 -3.9671993 -5.9958072 -7.7093296 -9.2355776 -9.4776316][-4.2888861 -3.5210295 -2.8138819 -2.508316 -1.9116669 -1.215826 -0.820837 -0.32652283 0.73707771 -0.33307266 -2.329495 -4.5736914 -6.1833496 -7.4631386 -7.8548603][-2.9719267 -2.3250127 -1.7764091 -0.87227917 0.0067958832 0.88988304 1.5133429 1.8540363 2.1906443 0.86849594 -1.496932 -3.8718598 -5.2035193 -6.3619022 -6.2936835][-2.7238574 -1.6882606 -0.90732718 0.018483639 1.1955366 2.3791571 2.98973 3.422843 3.5409966 1.9559793 -0.71362305 -3.4212704 -4.8762188 -6.1132722 -6.15794][-2.2885704 -1.6789999 -0.87721968 0.48255062 1.6629076 3.1140718 3.6371298 3.7043571 3.5288725 1.4998493 -1.2805648 -4.0341549 -5.23668 -6.204505 -6.2651639][-2.8258328 -1.9466515 -1.0520225 0.12870979 1.1711187 2.0271235 2.3333492 2.5389528 2.2405663 0.26603413 -2.1690917 -4.5221944 -5.593627 -6.4996819 -6.495132][-2.9806595 -2.386991 -1.5593553 -0.90984631 -0.18642187 0.7455368 0.98218632 1.0727711 0.84242916 -1.1655807 -3.3857012 -5.0993366 -6.1283631 -7.0098243 -7.0160322][-5.4046097 -4.3839717 -3.7661982 -3.2315106 -2.6462607 -2.0238028 -1.5185509 -1.3566093 -1.6169791 -2.8595157 -4.6796665 -6.1976271 -7.02366 -7.7450228 -7.3657708][-6.229723 -5.4558368 -4.5585184 -4.3438721 -4.0192356 -3.5045428 -3.0648646 -2.9358711 -2.8389921 -4.0199451 -5.49191 -6.3232894 -6.9523582 -7.301579 -6.9444671][-6.8358679 -6.5230856 -6.0984068 -5.8740773 -5.4072371 -4.9136143 -4.6586447 -4.4326096 -4.3453107 -4.6959305 -5.5951915 -6.1171618 -6.671629 -6.9441147 -6.4991307][-6.3333092 -6.1014647 -5.6834145 -5.5076847 -5.49098 -5.3694582 -4.9781256 -4.8728294 -4.5251217 -4.5491138 -4.922493 -5.0570297 -5.8315563 -6.1635923 -6.3466921][-6.99816 -6.5999942 -6.0885429 -5.9056072 -5.83343 -5.8539076 -5.7100725 -5.5596223 -5.4478245 -5.2279263 -5.1576385 -5.2351465 -5.3726768 -5.5591731 -5.6293583]]...]
INFO - root - 2017-12-15 19:17:55.247403: step 50310, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 49h:02m:09s remains)
INFO - root - 2017-12-15 19:18:01.590996: step 50320, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 49h:12m:50s remains)
INFO - root - 2017-12-15 19:18:08.097537: step 50330, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 48h:54m:36s remains)
INFO - root - 2017-12-15 19:18:14.490294: step 50340, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 49h:12m:10s remains)
INFO - root - 2017-12-15 19:18:20.870075: step 50350, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 50h:14m:22s remains)
INFO - root - 2017-12-15 19:18:27.208009: step 50360, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 50h:19m:22s remains)
INFO - root - 2017-12-15 19:18:33.552913: step 50370, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 49h:23m:20s remains)
INFO - root - 2017-12-15 19:18:39.963415: step 50380, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 50h:00m:47s remains)
INFO - root - 2017-12-15 19:18:46.412073: step 50390, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 50h:38m:11s remains)
INFO - root - 2017-12-15 19:18:52.803644: step 50400, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 50h:12m:06s remains)
2017-12-15 19:18:53.377091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.841208 -5.3905978 -5.9368086 -6.347075 -6.4575782 -6.5397811 -6.721909 -6.5201559 -5.360672 -4.6261044 -5.3290157 -5.6495543 -5.4772291 -5.1450768 -5.9437561][-5.3933043 -5.87432 -6.3215756 -6.4984541 -6.9749842 -7.4418135 -7.8960638 -7.4453006 -6.6740689 -5.9536324 -6.7799664 -7.206512 -7.3926048 -6.3269072 -6.525279][-5.9312744 -6.3269472 -6.5853529 -6.451561 -6.3565531 -6.6917548 -7.0236707 -7.1222734 -6.6546826 -6.3748112 -7.2207804 -7.7147584 -8.1932907 -7.2666068 -7.3522625][-7.0417252 -7.0109782 -6.5484447 -5.8334789 -5.2085857 -5.0661669 -4.5815344 -4.4509773 -3.95706 -4.2197371 -5.383347 -6.2137108 -7.0768609 -7.1451893 -8.0307159][-7.3525033 -6.4806089 -5.5004268 -4.5406752 -3.7516241 -3.2032509 -2.4972234 -2.0735059 -1.2801218 -1.177825 -2.4862871 -3.8546948 -5.882051 -6.53197 -7.6753726][-6.7829809 -5.4943819 -4.05499 -2.812448 -1.7226973 -0.51943922 0.31471634 0.56806183 1.4099655 1.2885065 -0.45391464 -1.9915442 -4.0540137 -5.1061745 -6.5790777][-6.1892104 -4.9353914 -2.9661989 -1.4446874 -0.17120457 1.0783739 1.9819269 2.6492529 3.3055992 2.4549189 0.068576336 -2.3152585 -4.2101026 -4.4058018 -5.4679785][-4.9977627 -4.1482358 -3.080965 -1.1023622 0.36718369 1.4909697 2.8976221 3.9237804 4.0881891 2.9386253 -0.014893055 -2.5677595 -4.2744875 -4.1667476 -4.7757683][-5.2525454 -4.2400217 -3.1169324 -1.4590111 -0.019609451 0.78577042 1.9078121 2.9056454 3.4395094 1.83881 -1.2618217 -3.353858 -4.5996943 -4.5878634 -5.1636305][-6.4412 -5.5745211 -4.2642622 -2.5257773 -1.0273919 -0.02302599 0.54703045 0.925333 1.0664539 0.0061559677 -2.0154128 -3.2458606 -4.6024532 -5.020071 -6.2990546][-7.5724206 -7.1662197 -6.6873207 -5.9339037 -4.4635434 -3.1441503 -2.2321267 -2.108613 -2.4988456 -3.3507252 -4.5161362 -4.7839422 -5.130075 -5.9168558 -7.1188712][-8.0044632 -7.4255853 -7.1219397 -6.5483928 -5.7681007 -4.9905052 -3.9259875 -3.3083024 -3.0347629 -3.8809273 -5.0385818 -4.6889153 -4.8768644 -5.4250045 -6.014339][-8.6928768 -8.6762114 -8.4462366 -7.9521003 -7.0725489 -6.3351245 -5.5156326 -5.3902845 -4.9849911 -5.0254059 -5.6148276 -5.1955156 -5.2467628 -5.6055708 -5.476306][-7.5194345 -7.6446605 -7.330441 -6.8681045 -6.3769045 -5.7364078 -5.0923376 -4.9014988 -4.7401276 -4.7064962 -4.8452606 -4.1923361 -3.9953182 -4.1249981 -4.704422][-6.3979449 -6.4464197 -6.5150952 -6.4240928 -6.1664443 -6.0373659 -5.9057808 -5.9353437 -5.8475866 -5.6390343 -5.8398046 -5.7359343 -5.35667 -5.4060869 -5.3031092]]...]
INFO - root - 2017-12-15 19:18:59.801309: step 50410, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 49h:17m:12s remains)
INFO - root - 2017-12-15 19:19:06.213786: step 50420, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:52m:28s remains)
INFO - root - 2017-12-15 19:19:12.590962: step 50430, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 50h:08m:00s remains)
INFO - root - 2017-12-15 19:19:18.961034: step 50440, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 50h:58m:05s remains)
INFO - root - 2017-12-15 19:19:25.285272: step 50450, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 49h:20m:20s remains)
INFO - root - 2017-12-15 19:19:31.663534: step 50460, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 50h:05m:47s remains)
INFO - root - 2017-12-15 19:19:38.055691: step 50470, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 48h:14m:01s remains)
INFO - root - 2017-12-15 19:19:44.529164: step 50480, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 50h:44m:55s remains)
INFO - root - 2017-12-15 19:19:50.928550: step 50490, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 52h:11m:56s remains)
INFO - root - 2017-12-15 19:19:57.305037: step 50500, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 49h:24m:31s remains)
2017-12-15 19:19:57.808635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8921347 -5.5741606 -6.3594112 -6.2156858 -5.0359669 -4.0591016 -3.2734551 -2.9550624 -2.6151562 -3.5101748 -4.2388391 -6.5044937 -8.4546785 -9.88302 -10.658538][-5.3003511 -5.7211876 -7.1363826 -7.1845074 -6.3535366 -4.3974648 -3.0971565 -2.0823174 -2.3164682 -4.45553 -5.7058415 -7.8707404 -9.7035131 -11.008846 -11.093941][-5.4681473 -5.7953281 -6.4473476 -6.1333451 -5.1456203 -3.7924178 -2.4057498 -0.94284391 -0.9012723 -2.905828 -4.9604588 -7.4295611 -9.1311064 -10.43961 -10.93919][-4.9673405 -5.271266 -5.6169009 -5.0848856 -3.4939504 -1.7222514 -0.0088310242 0.40297794 0.67147923 -1.0711083 -2.8780303 -5.8432078 -7.9111705 -9.06972 -9.6586666][-5.1560922 -4.5746784 -4.3807735 -3.8663261 -2.1818 0.594224 2.3417902 2.3848677 1.9299488 -0.29343224 -1.9126749 -4.6127958 -7.23209 -8.5159988 -9.1532116][-4.9726224 -4.0814929 -3.0807304 -1.3955035 -0.13448048 2.1012335 4.3539085 4.6020994 3.2073278 0.62643433 -1.4414005 -4.2530222 -5.980999 -6.98343 -7.6437354][-5.0765328 -4.6378989 -2.9240751 -0.56093645 1.4983168 2.9002953 4.1916571 4.6016331 4.008255 1.2505035 -1.5306916 -4.4746118 -6.7812219 -7.8137221 -7.999908][-4.5812683 -3.9970152 -3.0268068 -0.59234238 1.6386337 3.5510654 5.00922 4.8327827 4.0050583 1.0907106 -1.3471093 -4.76892 -7.2138915 -8.2411442 -8.2163382][-4.7908983 -4.507411 -3.5613084 -1.1800203 0.7978878 2.6440477 4.2598467 4.1956062 3.21912 -0.15604973 -2.3426046 -5.349884 -7.5758219 -8.7668962 -8.4800339][-5.1027513 -5.233386 -5.1150231 -3.4794273 -1.2630701 0.42860317 1.474349 1.747839 1.2219992 -1.6901884 -3.7981675 -6.3816366 -8.2540617 -9.2271671 -9.5686512][-6.1815891 -6.1077952 -6.4398584 -5.6910706 -4.2353849 -2.1251512 -1.4364767 -1.6916952 -1.8744488 -3.9962027 -5.7964706 -7.9123135 -9.0145321 -9.74308 -9.7230167][-7.3048296 -7.251575 -7.2934632 -6.7436905 -5.900939 -4.7880917 -3.7145116 -3.9270587 -4.2158823 -5.41903 -6.331789 -7.9072862 -8.6552668 -9.4718313 -9.52696][-9.1630058 -8.5667458 -8.5102129 -8.467905 -8.1120253 -7.3637452 -6.8011312 -6.6867757 -6.49316 -6.8277144 -7.6254282 -8.2001543 -8.4203014 -8.6802588 -8.400075][-9.3155994 -9.0937366 -8.4980726 -8.0708666 -7.7349033 -6.7417197 -5.6741934 -6.6870632 -7.0717783 -6.938086 -7.0866652 -7.0971041 -7.4826221 -7.1451097 -6.8199015][-8.6789331 -8.9993191 -9.0466852 -8.5925245 -8.4256659 -7.7689939 -7.3992333 -7.3632832 -7.6824956 -8.3131466 -8.1459332 -7.5839739 -6.9840608 -6.6759329 -6.3296547]]...]
INFO - root - 2017-12-15 19:20:04.169846: step 50510, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 49h:34m:13s remains)
INFO - root - 2017-12-15 19:20:10.556655: step 50520, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 49h:36m:41s remains)
INFO - root - 2017-12-15 19:20:16.982584: step 50530, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 51h:46m:01s remains)
INFO - root - 2017-12-15 19:20:23.361312: step 50540, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 49h:39m:32s remains)
INFO - root - 2017-12-15 19:20:29.760254: step 50550, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 49h:43m:27s remains)
INFO - root - 2017-12-15 19:20:36.178461: step 50560, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 50h:10m:42s remains)
INFO - root - 2017-12-15 19:20:42.527492: step 50570, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 49h:08m:41s remains)
2017-12-15 19:20:47.632822: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 23404329 get requests, put_count=23404337 evicted_count=19000 eviction_rate=0.000811815 and unsatisfied allocation rate=0.000811944
INFO - root - 2017-12-15 19:20:48.888212: step 50580, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.611 sec/batch; 47h:51m:11s remains)
INFO - root - 2017-12-15 19:20:55.410981: step 50590, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:10m:00s remains)
INFO - root - 2017-12-15 19:21:01.785713: step 50600, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 50h:06m:01s remains)
2017-12-15 19:21:02.295641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6384044 -5.1964526 -4.8153572 -4.7926521 -4.531549 -4.085885 -3.6333294 -3.3418965 -3.1076608 -4.5955949 -4.6583786 -4.4436951 -4.6367273 -5.3722534 -6.0932236][-4.8055305 -4.1947985 -3.8420873 -3.4223614 -3.38967 -3.3656535 -3.2316718 -3.1605511 -3.5089025 -5.5848351 -5.9047637 -6.0765667 -6.6241446 -7.2914848 -7.9088931][-4.0452566 -3.8821981 -3.6797676 -3.3583479 -3.604753 -3.6956608 -3.7690704 -3.6657166 -3.7895844 -5.9051991 -6.2576628 -6.8527861 -7.3697567 -7.991241 -8.3643255][-3.1546717 -2.990243 -3.0923719 -3.0927362 -3.1188827 -2.9132814 -3.0358963 -3.1843905 -3.7966011 -5.6869678 -5.876967 -6.3721137 -7.0406089 -8.0401335 -8.3936377][-2.7835851 -2.6149149 -2.5263944 -2.133781 -1.674046 -0.63343239 -0.12522268 -0.51707411 -1.3054066 -3.6683183 -4.4010859 -5.1587524 -6.0508385 -7.0486655 -7.9486933][-2.8993359 -2.316164 -1.6402206 -0.75516653 0.37281704 1.3994741 2.2217741 2.1708393 1.5503569 -1.2732983 -2.3644576 -3.7057428 -5.0260005 -6.4360981 -7.3896956][-2.0153055 -1.3110003 -1.039084 -0.20866489 1.2876215 2.6083841 3.5124512 3.605031 3.285161 0.69381428 -0.54616451 -2.0821009 -3.9414277 -5.7614846 -7.1054444][-1.7087069 -1.1198344 -0.79544115 0.35855103 1.5666809 2.7303762 3.765172 3.9793329 3.5548706 1.2348919 0.078289986 -1.0683603 -2.8759151 -4.7920132 -6.2272816][-2.1146789 -1.402339 -0.68054962 0.17776012 0.29810858 0.86783791 1.6953955 2.481823 2.8832808 0.92892265 -0.23970318 -1.417614 -2.8697224 -4.4965816 -5.6545396][-2.7268615 -2.4390421 -1.9966745 -1.0754724 -0.48904371 -0.22575283 -0.063950062 0.64863682 1.2072039 0.092514038 -0.81109571 -2.5199423 -4.0172987 -5.0089812 -5.5903435][-3.5481839 -3.4657888 -3.5779939 -3.0781441 -2.4436369 -1.8143048 -1.2766752 -1.3565512 -1.1862741 -2.1809745 -2.9427137 -4.1118789 -5.6097779 -6.6141949 -7.0173426][-4.6789188 -4.0444374 -4.0809546 -4.2671504 -4.1549916 -3.9350157 -3.5256424 -3.6301894 -3.7466917 -4.8825655 -4.8784237 -5.1465321 -6.2230659 -7.5350261 -8.3200788][-6.7435837 -5.9657946 -5.7830563 -5.60602 -5.2150445 -4.8596735 -4.6611419 -4.8653736 -5.2560997 -5.809752 -6.1874905 -6.2969556 -6.553091 -7.1009068 -7.5172839][-7.5594592 -7.037818 -6.7455344 -6.5653958 -6.30138 -5.9455152 -5.5543537 -5.5450597 -5.5264759 -5.724247 -6.0125093 -6.0373812 -6.19101 -6.3447962 -6.2000613][-5.9883332 -5.7482271 -5.865654 -5.7831311 -5.9400005 -5.7692819 -5.6521816 -5.4427967 -5.3837996 -5.2008619 -4.9940834 -5.0391269 -5.1649742 -5.4054785 -5.3925924]]...]
INFO - root - 2017-12-15 19:21:08.826825: step 50610, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 51h:02m:22s remains)
INFO - root - 2017-12-15 19:21:15.189039: step 50620, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 49h:59m:09s remains)
INFO - root - 2017-12-15 19:21:21.536039: step 50630, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 49h:05m:49s remains)
INFO - root - 2017-12-15 19:21:27.902870: step 50640, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 50h:07m:58s remains)
INFO - root - 2017-12-15 19:21:34.287262: step 50650, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 49h:52m:28s remains)
INFO - root - 2017-12-15 19:21:40.691823: step 50660, loss = 0.35, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 49h:30m:29s remains)
INFO - root - 2017-12-15 19:21:47.035163: step 50670, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:10m:31s remains)
INFO - root - 2017-12-15 19:21:53.402111: step 50680, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 49h:53m:37s remains)
INFO - root - 2017-12-15 19:21:59.738427: step 50690, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 48h:48m:57s remains)
INFO - root - 2017-12-15 19:22:06.052845: step 50700, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 49h:05m:44s remains)
2017-12-15 19:22:06.520726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4744239 -3.2006006 -4.1707597 -4.4537735 -4.0541515 -3.6050897 -3.350966 -3.3077273 -2.8226328 -3.5799327 -3.6988475 -5.4029088 -6.0862117 -6.2684288 -7.2228336][-2.8798375 -3.7138779 -4.5625029 -5.2051706 -5.1358461 -4.4411821 -3.8064663 -3.6119776 -3.8117771 -4.3838687 -4.4347486 -6.0242682 -6.604248 -7.0639014 -7.6445627][-3.5762706 -3.9635339 -4.7895284 -4.6651225 -4.9794521 -4.8440027 -4.2590737 -4.0052738 -3.4215465 -4.2836313 -4.8579779 -6.6098371 -7.0752134 -7.6591763 -8.3356228][-4.4790668 -4.6806726 -5.0752454 -4.7912593 -4.432826 -3.9802287 -3.3067002 -2.8728485 -2.5901508 -3.5199823 -3.7946725 -6.0464554 -6.9160976 -7.4821115 -7.9971557][-5.65125 -5.5299339 -5.63628 -5.1015415 -4.2407613 -3.264133 -1.6899176 -1.1814508 -0.91865158 -2.1875825 -3.2832322 -5.6348391 -6.571938 -7.3684778 -8.1190119][-7.23297 -6.7254243 -6.1803975 -4.8451509 -3.6135769 -1.9890299 -0.18542194 0.88328266 1.1805496 -1.1414113 -2.9443645 -5.9511023 -6.9032631 -7.3243132 -7.8551593][-7.0495667 -6.4970989 -6.0794759 -4.4187694 -2.2582107 -0.070828915 2.2097454 3.174531 3.4125948 1.4395084 -0.91458082 -4.474443 -6.0531917 -6.5685964 -7.1509051][-6.4668407 -5.9996371 -5.4039993 -3.6048608 -1.5942168 1.4228373 4.0439243 4.4711704 4.1904182 1.7068691 -0.040621758 -3.2730098 -4.8620768 -5.5237656 -5.90345][-6.6174641 -5.9872417 -5.5953088 -4.2543097 -2.4208274 0.47229576 3.009202 3.8572931 3.9043236 1.1228666 -0.77017021 -3.8012178 -5.0375862 -5.647892 -6.5130949][-6.4273286 -6.2656932 -6.0123491 -4.778182 -3.3711457 -1.4327693 0.64534187 1.2518473 1.4435339 -0.89202166 -2.9205303 -5.6546273 -6.432786 -6.1482496 -6.817934][-7.8524504 -7.52351 -7.1758595 -6.0334706 -4.938859 -3.4324951 -2.2420897 -1.7336493 -1.3379235 -3.3069034 -4.1099625 -6.244381 -7.0749259 -6.6686888 -6.8586273][-7.7466993 -7.4132624 -7.1445374 -6.181046 -5.6400661 -5.0172577 -4.01379 -3.8438392 -3.9238575 -4.6198468 -4.6813841 -6.1224356 -6.4808035 -6.7242618 -7.0381393][-8.7390137 -7.9711313 -7.351079 -6.6302919 -6.0368848 -5.8509226 -5.5035667 -5.2906117 -5.0128202 -6.1133823 -6.2832518 -6.5414333 -6.5640774 -6.4716287 -6.7849541][-8.4657869 -7.9067378 -7.2514825 -6.1147137 -5.54271 -5.5314054 -5.1506433 -5.4197474 -5.5034008 -5.7217541 -5.8407168 -6.0898447 -6.0688605 -6.2067614 -6.5874434][-8.7050362 -8.4665623 -8.0218153 -7.1682386 -6.3222127 -6.12047 -6.3017054 -6.5257215 -6.4106359 -6.4853067 -6.6159115 -6.4468317 -6.2797141 -6.0094452 -5.6416345]]...]
INFO - root - 2017-12-15 19:22:12.961715: step 50710, loss = 0.36, batch loss = 0.25 (12.4 examples/sec; 0.647 sec/batch; 50h:38m:39s remains)
INFO - root - 2017-12-15 19:22:19.314196: step 50720, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 49h:59m:54s remains)
INFO - root - 2017-12-15 19:22:25.778190: step 50730, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 50h:38m:52s remains)
INFO - root - 2017-12-15 19:22:32.238084: step 50740, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 51h:36m:20s remains)
INFO - root - 2017-12-15 19:22:38.674850: step 50750, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 50h:20m:50s remains)
INFO - root - 2017-12-15 19:22:45.077214: step 50760, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 48h:47m:07s remains)
INFO - root - 2017-12-15 19:22:51.423349: step 50770, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 50h:13m:53s remains)
INFO - root - 2017-12-15 19:22:57.776501: step 50780, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 50h:51m:32s remains)
INFO - root - 2017-12-15 19:23:04.106202: step 50790, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 50h:23m:38s remains)
INFO - root - 2017-12-15 19:23:10.484556: step 50800, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 48h:55m:43s remains)
2017-12-15 19:23:10.973564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8813927 -3.7538605 -3.8528786 -3.9952555 -3.908798 -3.6336961 -3.2371817 -2.87288 -2.471909 -3.7690418 -4.5274773 -5.2287021 -5.9977226 -6.3573322 -6.8661385][-3.6503139 -3.7228014 -3.8329122 -3.9498808 -3.6598997 -3.1822362 -3.1828456 -3.0176806 -2.7581105 -4.0972276 -4.6953974 -5.5369692 -6.2418876 -6.8612671 -7.3039732][-2.8249941 -2.8530955 -2.9819369 -3.0678716 -2.8386121 -2.5488434 -2.1739607 -1.9432406 -1.7201853 -2.8869214 -3.4367914 -4.3133397 -5.4302268 -5.9293056 -6.7694745][-2.593142 -2.4056063 -2.2185578 -1.9523153 -1.5018845 -1.0229578 -0.60860443 -0.56925488 -0.55470276 -2.0219445 -2.9071054 -3.9649673 -5.1537776 -5.8734 -6.5238624][-2.8001623 -2.1843963 -1.3801775 -0.66052294 0.049166203 0.48180676 0.87934494 0.87058067 0.78781796 -1.0684938 -2.2446017 -3.4070959 -4.7746129 -5.7001467 -6.5078049][-4.1322565 -3.1326332 -1.7804794 -0.70475292 0.335207 1.1969738 1.7254763 1.5320034 1.263464 -0.65918636 -1.9213142 -3.3743253 -4.9109116 -5.7822475 -6.6016917][-4.2673368 -3.3628983 -2.115921 -0.5300355 0.88495827 1.9356537 2.5521946 2.4342871 2.1560869 -0.13984776 -1.7755771 -3.4067273 -5.0509076 -6.1109033 -6.9469028][-4.0762897 -2.9685516 -1.5989804 0.013926983 1.2994232 2.1197443 2.5932655 2.4541636 2.1445465 -0.098549366 -1.8256149 -3.5436864 -5.2192659 -6.2686925 -7.0641212][-4.33383 -3.2528334 -2.0611434 -0.66371727 0.606267 1.2721071 1.7065382 1.8129177 1.762743 -0.511168 -2.1654105 -3.9022062 -5.6899881 -6.8278775 -7.5285358][-4.7183342 -4.0159845 -3.17765 -1.8694844 -0.67029333 -0.18657589 0.41132927 0.77640533 0.80943489 -1.5595512 -3.0583663 -4.6732759 -6.2307906 -7.3177962 -8.1069145][-5.811141 -5.0951 -4.3975468 -3.342618 -2.6541777 -2.2983189 -1.7316008 -1.5918102 -1.4562788 -3.4524007 -4.8634439 -6.1199226 -7.2424 -7.9727073 -8.1412888][-6.2747211 -5.6856656 -4.9093876 -4.2616792 -3.6516781 -3.5378914 -3.2670441 -3.1585536 -3.0837374 -4.4050131 -5.4263773 -6.3916254 -7.1857715 -7.8518982 -8.00522][-6.4989953 -6.0669723 -5.6370459 -5.2065725 -4.80588 -4.7968197 -4.5329976 -4.575366 -4.5167103 -5.4177504 -6.1953697 -6.631094 -7.2469177 -7.6411085 -7.4092693][-6.5096111 -6.1942835 -5.8027878 -5.4647675 -5.2475882 -5.0819597 -4.8784876 -4.9310522 -4.8892403 -5.6705317 -5.8771248 -5.9950294 -6.2519393 -6.5519114 -6.4150739][-7.7056737 -7.5108113 -6.917222 -6.52631 -6.4248352 -6.4828367 -6.5402007 -6.718647 -6.7636962 -6.613553 -6.6544981 -6.6145148 -6.3725157 -6.2283554 -6.1416864]]...]
INFO - root - 2017-12-15 19:23:17.259869: step 50810, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 49h:44m:59s remains)
INFO - root - 2017-12-15 19:23:23.623961: step 50820, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 50h:52m:40s remains)
INFO - root - 2017-12-15 19:23:29.941917: step 50830, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 48h:58m:02s remains)
INFO - root - 2017-12-15 19:23:36.298763: step 50840, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 50h:21m:53s remains)
INFO - root - 2017-12-15 19:23:42.746244: step 50850, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 51h:02m:28s remains)
INFO - root - 2017-12-15 19:23:49.156980: step 50860, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 49h:21m:53s remains)
INFO - root - 2017-12-15 19:23:55.502922: step 50870, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 50h:01m:17s remains)
INFO - root - 2017-12-15 19:24:01.892612: step 50880, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 50h:35m:20s remains)
INFO - root - 2017-12-15 19:24:08.303218: step 50890, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 51h:41m:43s remains)
INFO - root - 2017-12-15 19:24:14.656345: step 50900, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 49h:59m:48s remains)
2017-12-15 19:24:15.190368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0659332 -3.7127833 -3.7127218 -3.3366604 -3.00776 -2.5571461 -1.97471 -1.7457738 -1.8068466 -4.1471987 -5.706255 -7.6418686 -9.0241184 -9.9021788 -9.5987244][-5.1503406 -4.6757526 -4.315239 -4.0350542 -3.7780504 -3.4345117 -3.0154419 -2.7548933 -2.4008446 -4.6721735 -6.0945582 -7.5373511 -8.6182671 -9.69667 -9.9732409][-5.1397381 -4.4854584 -3.6676793 -3.1141677 -2.537426 -2.3646011 -1.8200378 -1.3350744 -0.90205812 -2.705297 -3.972373 -5.9704332 -7.5892091 -8.757081 -9.5524187][-3.7086368 -3.1738305 -2.4045672 -1.7956815 -1.25566 -0.81548834 -0.1930232 0.44059944 1.1831551 -0.46166325 -1.7417707 -3.7294374 -5.5330029 -6.8058176 -7.3851542][-4.0729856 -2.6675544 -1.2715955 -0.50891447 0.426239 1.6709976 2.6055679 3.3183527 3.8418331 1.8675947 0.22316742 -2.4012651 -4.8581829 -6.3343468 -6.7962022][-3.8241737 -2.3767443 -1.0730243 0.35717106 1.7016459 2.7454405 3.8347244 4.4737921 4.9845209 2.8832979 0.85579586 -1.3322635 -3.5133848 -5.05509 -5.9623079][-3.7885656 -2.4975677 -1.006465 0.78143024 2.632514 3.9919891 5.0044756 5.6187229 6.1120558 3.3620224 1.1552105 -1.4749498 -4.2857871 -5.3087926 -5.6212864][-4.0634928 -2.2825928 -0.82795334 1.0928364 2.7588062 4.2493963 5.384305 5.8211327 5.5935011 2.8137102 0.565444 -2.1055183 -4.658855 -5.6900306 -5.8130836][-4.52104 -3.3501191 -2.0226717 0.19440413 2.0974312 3.6276417 4.6983118 4.7369156 3.9231482 1.0655842 -0.85423708 -3.1631136 -5.2088 -6.0160394 -5.864409][-4.0951309 -3.4382386 -2.5975828 -0.89127445 0.66550922 1.841073 2.8007164 2.72618 2.2627802 -0.1675849 -1.7050848 -3.6651406 -5.0413628 -5.8644409 -5.898366][-5.6311421 -5.1711979 -4.6451254 -3.4471173 -2.2050529 -0.82771063 0.24141026 0.1515317 0.027258873 -2.1984382 -3.8364918 -5.4453373 -6.7015872 -6.9925027 -6.5985942][-6.9911122 -6.4024477 -5.6825752 -4.9657879 -4.3453856 -3.1691823 -2.6315398 -2.6990924 -2.6752424 -4.4160833 -5.1512728 -6.334116 -6.8071356 -6.9236422 -6.6728263][-7.1623034 -7.1433549 -6.9138904 -6.2667437 -5.5462923 -4.7480373 -4.1925244 -4.1303573 -4.1727037 -5.3334794 -5.7306194 -6.5166059 -6.9229646 -6.5184603 -5.933548][-7.4740629 -7.5018697 -7.1499758 -6.73056 -6.1802273 -5.1063662 -4.5061388 -4.5748234 -4.5268703 -5.1792936 -5.605948 -6.0140934 -6.4541016 -6.148087 -5.9992456][-8.7250824 -8.9515171 -8.7426109 -8.2188835 -7.3190179 -6.4787822 -6.4071245 -6.4372478 -6.7045736 -6.5619192 -6.3122244 -6.1742716 -6.0913119 -5.9970231 -5.82225]]...]
INFO - root - 2017-12-15 19:24:21.548837: step 50910, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 51h:09m:52s remains)
INFO - root - 2017-12-15 19:24:27.968546: step 50920, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 49h:21m:11s remains)
INFO - root - 2017-12-15 19:24:34.375387: step 50930, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 50h:43m:59s remains)
INFO - root - 2017-12-15 19:24:40.762258: step 50940, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:52m:24s remains)
INFO - root - 2017-12-15 19:24:47.166340: step 50950, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 50h:13m:43s remains)
INFO - root - 2017-12-15 19:24:53.654509: step 50960, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 50h:12m:56s remains)
INFO - root - 2017-12-15 19:25:00.155007: step 50970, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 50h:41m:53s remains)
INFO - root - 2017-12-15 19:25:06.530479: step 50980, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 49h:20m:50s remains)
INFO - root - 2017-12-15 19:25:12.911562: step 50990, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 50h:26m:42s remains)
INFO - root - 2017-12-15 19:25:19.359946: step 51000, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 49h:28m:42s remains)
2017-12-15 19:25:19.875503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8624439 -3.723249 -4.8535442 -5.633677 -6.467515 -7.5370765 -7.9857764 -7.4188862 -7.0069089 -6.7127476 -6.9471812 -7.9462533 -7.5845156 -8.0675774 -7.3401279][-3.1404419 -3.4442568 -4.1258059 -5.5518436 -6.3442221 -6.66421 -6.8581157 -6.8294311 -6.6914639 -6.5393224 -7.2444882 -8.5751743 -8.4953356 -8.406847 -7.9428816][-2.2071934 -3.0790353 -3.6745896 -3.9569044 -4.0861416 -4.4283066 -4.6876717 -4.5047045 -4.3047132 -5.1124544 -6.4875541 -7.8428679 -8.3054028 -8.7963152 -8.0532007][-2.4423456 -3.1419678 -3.0253015 -2.68848 -2.7962413 -2.5494037 -2.4011931 -2.67375 -2.7927032 -3.4131408 -4.2700539 -6.1788893 -6.7591672 -7.7456622 -7.4354386][-3.4009557 -3.2071338 -3.0499697 -2.5584216 -1.7108722 -0.50148344 0.0084767342 -0.37230778 -1.0169749 -2.1022215 -3.7978954 -5.4901853 -5.650342 -6.3456693 -6.4957552][-5.0182428 -4.6667094 -4.1801033 -3.1497393 -1.8976417 -0.092096806 1.2140779 1.6670532 1.2780256 -0.47681904 -2.8121104 -4.8658371 -5.3855 -6.0153456 -5.76116][-5.5152168 -4.9820328 -4.530941 -3.0755491 -1.1050401 0.874032 2.9276781 3.8417635 3.8333874 2.4990988 0.042932987 -2.6324353 -4.0145454 -5.05027 -4.6432505][-5.3917923 -4.7064524 -3.6483402 -2.019165 -0.071698666 2.1914539 3.9036713 4.9578056 5.2285166 4.1730146 2.3822479 -0.11364126 -1.6136141 -3.269825 -3.0865712][-5.2990236 -4.2253923 -3.4143014 -1.8847852 -0.15035009 1.4162483 2.6634426 3.3505592 3.4625425 2.6722908 1.340333 -0.63933468 -1.5120912 -2.4816937 -2.4169436][-6.7485967 -6.2155423 -4.8154807 -3.4611278 -2.3053765 -1.1265059 0.11121321 0.94948387 1.5900059 0.85865593 -0.89455175 -2.4066296 -2.8450656 -2.7401743 -2.4937468][-7.8507557 -7.35634 -6.2492905 -4.9646235 -4.2360134 -3.132359 -2.268271 -1.5594778 -0.78679752 -1.1308961 -2.154542 -3.4169292 -3.9933639 -4.4613457 -3.555449][-7.3471489 -6.949091 -6.5024328 -5.68052 -4.8698797 -4.3462329 -3.8321178 -3.5510607 -3.5008502 -3.3660393 -3.8520172 -5.1103072 -5.1701589 -5.6153474 -5.358654][-7.1528955 -6.2922168 -5.8318286 -5.077034 -4.9068007 -4.796855 -4.7594543 -4.7679758 -4.4548526 -4.5493841 -4.7664337 -5.0920539 -5.0949173 -5.8061819 -6.2288136][-7.7607341 -7.372015 -6.2521806 -5.4764328 -5.5536394 -5.7507548 -5.7453427 -6.0038567 -5.8712316 -6.0407553 -6.0635395 -5.3811221 -5.2916741 -5.5843697 -5.5937014][-8.5628529 -8.6535053 -8.2772484 -7.2492676 -6.3314247 -6.6907797 -7.0294895 -7.0169439 -7.2411447 -7.0720186 -6.7647 -6.4515247 -5.8510809 -5.4740114 -5.4740124]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:25:26.283279: step 51010, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 49h:25m:30s remains)
INFO - root - 2017-12-15 19:25:32.632544: step 51020, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 49h:11m:49s remains)
INFO - root - 2017-12-15 19:25:38.981862: step 51030, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 51h:36m:36s remains)
INFO - root - 2017-12-15 19:25:45.371522: step 51040, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 50h:33m:23s remains)
INFO - root - 2017-12-15 19:25:51.698007: step 51050, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 49h:44m:09s remains)
INFO - root - 2017-12-15 19:25:58.120576: step 51060, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 50h:29m:59s remains)
INFO - root - 2017-12-15 19:26:04.438920: step 51070, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 49h:00m:31s remains)
INFO - root - 2017-12-15 19:26:10.803695: step 51080, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 49h:01m:02s remains)
INFO - root - 2017-12-15 19:26:17.202993: step 51090, loss = 0.35, batch loss = 0.23 (12.3 examples/sec; 0.650 sec/batch; 50h:47m:18s remains)
INFO - root - 2017-12-15 19:26:23.562009: step 51100, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 50h:34m:30s remains)
2017-12-15 19:26:24.132772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0895481 -4.6567149 -5.4449263 -5.2656374 -4.2701874 -3.3502932 -2.8240595 -2.3559041 -1.5860457 -1.5786209 -2.6565604 -4.702312 -5.4246292 -5.8979182 -6.3264928][-3.5065575 -4.279983 -5.0247579 -5.3804274 -5.1116362 -4.2015333 -3.215591 -2.6229692 -2.2184787 -2.539463 -3.4788299 -4.9922342 -6.04824 -6.8497267 -7.2157488][-3.7206776 -4.0963383 -4.3194828 -4.2945571 -4.1622143 -3.7699409 -3.5827284 -3.26965 -2.9889607 -3.5226316 -4.6661892 -6.115304 -6.2453709 -7.1936398 -7.8687372][-3.8566463 -4.1403956 -4.0687523 -3.681994 -3.2295761 -2.8230085 -2.421422 -2.1739326 -2.0910878 -2.5825224 -3.8256757 -5.6566563 -6.2288542 -7.0691562 -7.389586][-3.7388206 -4.0482974 -3.4508781 -2.4915957 -1.6190891 -0.63481855 -0.31976271 -0.572958 -1.0770998 -1.8343449 -3.3166814 -5.1641588 -6.0172057 -6.8030996 -7.3672462][-5.0327024 -4.9927397 -3.9022412 -2.148469 -0.41489124 0.966589 1.8860521 1.3638458 0.39669514 -1.1520839 -3.1457267 -5.1457281 -6.0291924 -7.0229115 -7.6936111][-5.20365 -4.9160743 -3.99191 -1.6772909 0.48014164 2.0601711 3.4190006 3.1511927 2.4391947 0.88583469 -1.5908418 -4.1310186 -5.5370417 -6.39056 -6.9216819][-3.9152212 -3.8932364 -3.0859847 -0.91078234 1.2476053 3.7934971 5.1024132 4.6585855 3.9715929 2.4407349 -0.058201313 -2.8807092 -4.5799017 -5.9040723 -6.3764329][-4.1073074 -3.5684724 -2.9137125 -2.0246406 -0.17477798 1.9249067 3.1146164 3.4309826 3.4642963 2.031085 -0.064171791 -2.684834 -4.2136192 -5.6346588 -6.4935923][-5.7341547 -5.2311668 -4.1961365 -2.6877322 -1.482327 -0.089049816 1.1004887 1.5154886 1.6348104 0.20786619 -1.5452223 -3.7213559 -4.7775092 -5.6514812 -6.4907875][-5.7842417 -6.0243044 -5.568758 -4.4237108 -3.4344287 -2.2715492 -1.3184066 -0.99987078 -0.86107111 -1.6972585 -3.1785088 -5.3773489 -5.6981797 -6.4268341 -6.6671214][-6.9521174 -6.64719 -6.3033981 -5.7025166 -4.9825187 -4.31107 -3.8420889 -3.6254344 -3.621634 -3.5049582 -4.2164354 -5.5809145 -5.5354023 -6.2381649 -6.713285][-7.4557457 -7.410903 -7.1822009 -6.5507383 -6.270494 -5.6210647 -5.1589828 -5.0992179 -4.79789 -4.9203916 -5.0358086 -5.8966312 -5.7300649 -6.1354871 -6.5185251][-6.9604416 -6.9463511 -6.8904195 -6.2995739 -6.0965919 -5.7313066 -5.3393183 -5.5086017 -5.7505875 -6.0921111 -5.8371205 -5.9458966 -5.9172244 -5.8863635 -5.9091358][-7.6542416 -7.3196039 -7.5287123 -7.1455903 -7.087534 -6.8187547 -6.31924 -6.1133366 -6.1645441 -6.3713808 -6.3373795 -6.3078942 -5.9393797 -5.6689157 -5.6877203]]...]
INFO - root - 2017-12-15 19:26:30.479074: step 51110, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 50h:26m:28s remains)
INFO - root - 2017-12-15 19:26:36.859158: step 51120, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 48h:21m:59s remains)
INFO - root - 2017-12-15 19:26:43.224282: step 51130, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.643 sec/batch; 50h:13m:02s remains)
INFO - root - 2017-12-15 19:26:49.481318: step 51140, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 49h:14m:57s remains)
INFO - root - 2017-12-15 19:26:55.859919: step 51150, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 48h:19m:20s remains)
INFO - root - 2017-12-15 19:27:02.237882: step 51160, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 48h:50m:53s remains)
INFO - root - 2017-12-15 19:27:08.621171: step 51170, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 49h:31m:13s remains)
INFO - root - 2017-12-15 19:27:14.951471: step 51180, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 49h:24m:53s remains)
INFO - root - 2017-12-15 19:27:21.360896: step 51190, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 49h:21m:02s remains)
INFO - root - 2017-12-15 19:27:27.693556: step 51200, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 49h:02m:02s remains)
2017-12-15 19:27:28.164083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9203887 -5.4921703 -4.9647379 -4.8801203 -4.9403682 -5.1263981 -4.7056851 -3.9660668 -2.9543447 -3.1207294 -3.4342198 -3.8653812 -4.6219797 -5.1772976 -5.5909429][-4.8981671 -5.0049591 -4.9408665 -5.0998507 -5.2093954 -4.9978485 -4.85413 -4.3740711 -3.1766315 -3.0690355 -3.3463798 -4.2035818 -4.6321106 -5.3339224 -6.0414662][-3.7158589 -3.9282823 -4.0426702 -4.0917845 -4.4947476 -4.5692053 -4.0613618 -3.5100274 -2.8783183 -2.9673209 -3.8781056 -4.600565 -5.2079029 -5.65546 -6.1653748][-3.2613068 -2.4478006 -1.9522295 -1.6985598 -1.8272166 -2.0271235 -1.417819 -0.99319124 -0.63204527 -1.2090526 -2.3834367 -3.7543035 -5.222559 -6.1649675 -7.1933722][-2.7943835 -1.922883 -1.1263299 -0.13100958 0.86210823 1.3213139 1.9038706 1.7359638 2.2610884 1.0280151 -0.80258417 -2.6086426 -3.74134 -5.3694134 -6.8362403][-1.7875743 -1.3713984 -0.3484745 0.906785 2.1321239 3.4921732 4.9043884 4.9274683 4.7350225 2.8755484 0.60762978 -1.9114404 -3.5456491 -4.75718 -5.7626104][-1.8071723 -0.81203747 0.32810497 1.8592663 3.43262 4.836647 6.242322 6.6719685 6.8018122 5.5455141 2.4610262 -0.59265137 -2.6721239 -4.3719635 -5.0674434][-1.7031302 -0.69715405 0.13412571 1.7563314 3.6852665 5.161767 6.3278227 6.4851971 6.7662067 5.6224546 2.920208 0.25849724 -2.1218848 -3.8082309 -5.0923796][-1.0774503 -0.75741959 0.21135283 1.21488 2.3313646 3.8286228 5.2411089 5.2395248 4.9138803 3.5392685 1.1442471 -1.0640116 -2.9509687 -4.32839 -5.4930024][-1.5004363 -1.0167742 -0.36908388 0.22668266 1.0764093 2.1473808 2.7276611 3.2888174 3.2677679 1.8983507 -0.63769341 -2.6298776 -4.1623149 -5.2889719 -6.2161608][-3.5694666 -2.7751555 -2.3068419 -1.7476554 -1.1517296 -0.78199482 -0.44103384 -0.12980604 -0.33729839 -1.071907 -2.7057443 -4.2043476 -5.1815233 -5.9604321 -6.65816][-6.0073028 -5.2345848 -4.5353832 -4.0590353 -3.2542682 -3.3126206 -3.4355917 -3.5129356 -3.4216661 -3.7063408 -4.6668754 -5.3644094 -5.9159255 -6.6252632 -6.8513265][-7.1292019 -6.6822634 -6.1616044 -5.95063 -5.656239 -5.444746 -5.4085259 -5.4644146 -5.4715204 -5.3062353 -5.5605011 -5.4380207 -5.6670446 -6.1884332 -6.1761723][-8.5136938 -7.9258623 -7.4089022 -6.5195484 -6.1143179 -5.9742627 -5.9061413 -5.9594855 -5.8559694 -5.5585604 -5.94499 -5.6827803 -5.3248634 -5.6245546 -5.7550163][-10.070034 -8.9460011 -8.52964 -8.2288647 -7.4917989 -7.1679296 -7.0700626 -7.1758022 -7.0274754 -7.00693 -6.9613771 -6.4445133 -6.0061984 -5.7670527 -5.6767378]]...]
INFO - root - 2017-12-15 19:27:34.638744: step 51210, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 50h:27m:05s remains)
INFO - root - 2017-12-15 19:27:41.140167: step 51220, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 51h:00m:22s remains)
INFO - root - 2017-12-15 19:27:47.482178: step 51230, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 50h:09m:15s remains)
INFO - root - 2017-12-15 19:27:53.931274: step 51240, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 50h:00m:36s remains)
INFO - root - 2017-12-15 19:28:00.344018: step 51250, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 51h:00m:21s remains)
INFO - root - 2017-12-15 19:28:06.791639: step 51260, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:59m:26s remains)
INFO - root - 2017-12-15 19:28:13.247566: step 51270, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 49h:55m:02s remains)
INFO - root - 2017-12-15 19:28:19.562860: step 51280, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 48h:58m:11s remains)
INFO - root - 2017-12-15 19:28:25.976893: step 51290, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.618 sec/batch; 48h:15m:12s remains)
INFO - root - 2017-12-15 19:28:32.378615: step 51300, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 51h:19m:40s remains)
2017-12-15 19:28:32.927796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.35782337 -0.17160034 -0.54921293 -0.64395857 -0.400167 -0.22707129 -0.061825275 0.046177864 0.084251881 -1.0223913 -2.4427509 -3.9398339 -5.954483 -7.189795 -8.5055265][-1.8768358 -1.8126621 -2.2812772 -2.6664982 -2.9100137 -2.7625322 -2.5371709 -2.1029968 -2.1041427 -3.0097384 -4.1565943 -5.763 -7.8061748 -8.7703743 -9.911335][-2.5080819 -2.6145616 -2.9191208 -3.0784287 -2.9416738 -2.6374502 -2.0549736 -1.717505 -1.6803565 -2.8031912 -3.7140386 -4.9373164 -6.772583 -7.7709708 -8.613905][-3.4783592 -3.3963151 -3.2839656 -2.9203963 -2.3956275 -1.3607693 -0.35693836 0.10258818 0.29663706 -0.89639235 -2.1368847 -4.0266638 -5.7284031 -6.9079924 -8.0329838][-4.8330817 -4.1030874 -3.8923643 -3.1525311 -2.2304096 -1.0252409 0.15480709 0.58451462 0.66379833 -0.79031992 -2.2016687 -3.9141293 -5.7302108 -6.4717736 -7.3336577][-5.2386179 -4.4383039 -3.7532046 -2.6809974 -1.2688012 0.32448769 1.8086805 2.1800966 2.2228336 0.64611816 -0.940794 -2.8181815 -4.2592878 -5.3282719 -6.0240574][-5.387661 -4.8205881 -3.8987188 -2.4729829 -0.95557928 0.9008255 2.5972033 3.0476332 3.1917067 1.5355835 -0.042978764 -2.3071847 -4.2629442 -4.8597603 -5.5095425][-4.3185177 -3.3838701 -2.3736444 -0.79569483 1.0471087 2.5305204 4.2681065 4.96824 5.0269175 2.8487873 0.90370846 -1.57268 -3.9293203 -5.0336108 -5.843029][-2.8713999 -2.1054296 -1.1895304 0.13034248 1.3497076 2.6156397 4.0218821 4.4826555 4.6867123 2.843523 0.88189316 -1.3182983 -3.5525603 -4.5183887 -5.3936071][-2.8615708 -1.8714514 -1.4777555 -0.67232609 0.27878284 1.3739624 2.6028576 3.1620655 3.0190725 1.1988487 -0.25299215 -2.3766427 -4.0676384 -5.0895081 -6.1253128][-3.8410618 -3.5343127 -3.0166717 -2.161077 -1.6535563 -0.61746359 0.45885563 0.92105579 1.0141621 -0.60810566 -2.2703462 -3.6918766 -5.3430691 -5.9774084 -6.5854883][-5.2106552 -4.6519032 -4.3913608 -3.9788752 -3.3848624 -2.8207068 -2.2293682 -1.9329133 -1.6754079 -2.7980351 -3.5128651 -4.8566265 -5.90471 -6.2403874 -7.1016269][-6.4733267 -6.3498106 -6.0683012 -5.6741285 -5.5514212 -4.6793766 -4.1281567 -3.9783545 -3.9067676 -4.5577388 -4.9879727 -5.6405716 -5.939065 -6.1980066 -6.2013988][-6.7546692 -6.4098086 -6.5833187 -6.2076435 -5.6739674 -5.2190557 -4.638073 -4.5930233 -4.4530716 -4.905345 -5.1384764 -5.7481337 -6.2778864 -5.9662251 -6.2944908][-7.2415028 -7.3677158 -7.3839927 -7.2225914 -7.126904 -6.8537145 -6.3249407 -6.4369841 -6.435626 -6.3439279 -6.4603291 -6.5218039 -6.5626397 -6.4654427 -6.391027]]...]
INFO - root - 2017-12-15 19:28:39.371632: step 51310, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 51h:41m:38s remains)
INFO - root - 2017-12-15 19:28:45.760630: step 51320, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 50h:38m:27s remains)
INFO - root - 2017-12-15 19:28:52.227202: step 51330, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 49h:56m:31s remains)
INFO - root - 2017-12-15 19:28:58.595489: step 51340, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 50h:27m:02s remains)
INFO - root - 2017-12-15 19:29:04.935472: step 51350, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 49h:41m:28s remains)
INFO - root - 2017-12-15 19:29:11.334839: step 51360, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 49h:35m:04s remains)
INFO - root - 2017-12-15 19:29:17.778476: step 51370, loss = 0.32, batch loss = 0.21 (11.9 examples/sec; 0.672 sec/batch; 52h:29m:06s remains)
INFO - root - 2017-12-15 19:29:24.157369: step 51380, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 50h:43m:07s remains)
INFO - root - 2017-12-15 19:29:30.672729: step 51390, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 49h:44m:02s remains)
INFO - root - 2017-12-15 19:29:37.016154: step 51400, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 48h:36m:21s remains)
2017-12-15 19:29:37.552572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1155453 -6.1463928 -5.9173217 -5.3628483 -4.5427256 -3.8405 -3.3926754 -2.8778276 -2.3673725 -2.6468387 -4.8998175 -6.5254421 -7.725287 -8.3170776 -8.8526592][-5.7462344 -5.42745 -5.2508726 -4.9593754 -4.5988064 -4.1485071 -3.6910417 -3.3608527 -3.18889 -3.3392558 -5.372179 -6.4922142 -7.8427353 -8.2432089 -8.3340034][-4.9435253 -4.9581242 -4.7614374 -3.9734118 -3.4749579 -3.4515471 -3.1618838 -2.9884863 -2.8854795 -3.229928 -5.0186062 -5.9908385 -7.2880878 -7.699007 -7.6803417][-4.208766 -4.0137444 -3.6728764 -2.811924 -2.3021202 -1.8592691 -1.522439 -1.5759063 -1.5170226 -1.9698582 -3.9016812 -4.8714128 -6.3992987 -7.2029858 -7.7160668][-3.1186771 -2.5959558 -2.2282138 -1.7608109 -1.0721884 -0.5835886 -0.13423395 -0.073257923 -0.05490303 -0.55581236 -2.2742195 -3.4567275 -5.1395292 -6.0070243 -6.81136][-2.8610291 -2.2394581 -1.5902729 -1.1299753 -0.8199811 -0.39731884 0.14129353 0.24159098 0.22031069 -0.11909056 -1.511528 -2.6086402 -4.2113152 -5.2973671 -6.0705352][-3.2402744 -2.5977025 -1.4931178 -0.76373625 -0.21807289 -0.030973434 0.27098083 0.26230192 0.3730793 0.096590996 -1.2714014 -2.3028312 -4.1100597 -5.1490145 -5.894877][-3.2444344 -2.5563116 -1.4588466 -0.62807703 0.062962532 0.40020752 0.76528358 0.556159 0.59873581 0.0062522888 -1.4017344 -2.5824561 -4.3190732 -5.166008 -6.1506152][-4.1830764 -3.3503966 -2.1893163 -1.3050733 -0.62516689 0.03353548 0.50316715 0.47024536 0.57782745 0.0044312477 -1.3104639 -2.8689175 -4.4980984 -5.1579361 -5.8197174][-4.8087749 -4.0404625 -3.1049113 -2.4389844 -1.795157 -1.3101625 -0.75418139 -0.43190432 -0.16404295 -0.81973505 -2.0794768 -3.2291708 -4.27894 -5.1023345 -5.8450551][-6.7165723 -5.726809 -4.6814189 -3.8742983 -3.6538281 -3.4167972 -2.9836364 -2.4144435 -1.8691506 -2.1276445 -3.0375528 -4.1641731 -5.0744677 -5.7344437 -5.9868932][-7.5946956 -6.3154254 -5.2962084 -4.5800948 -3.980418 -3.6645732 -3.3934007 -3.1257129 -3.0227466 -2.9777765 -3.4558487 -3.8903649 -4.6643281 -5.3462462 -5.718564][-7.5227222 -6.6826725 -5.9636993 -5.1982784 -4.3746653 -4.0624552 -3.509501 -3.4417248 -2.9620233 -3.05414 -3.5145793 -3.3849125 -3.791733 -4.1537371 -4.3357911][-7.1878881 -6.7627535 -6.370594 -5.493679 -4.9295597 -4.0219316 -3.5027628 -3.4579277 -3.1548195 -2.9681144 -2.9585533 -2.8679404 -3.2864923 -3.4571962 -3.9475982][-7.6334782 -7.2363234 -6.7543626 -6.5134363 -5.9422483 -5.0992737 -4.5222712 -4.0700331 -3.4139619 -3.4672461 -3.3335481 -3.5334463 -3.3867903 -3.8153493 -4.3932681]]...]
INFO - root - 2017-12-15 19:29:43.968966: step 51410, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 51h:16m:25s remains)
INFO - root - 2017-12-15 19:29:50.322453: step 51420, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 49h:26m:08s remains)
INFO - root - 2017-12-15 19:29:56.810889: step 51430, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 49h:25m:11s remains)
INFO - root - 2017-12-15 19:30:03.187296: step 51440, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 50h:05m:21s remains)
INFO - root - 2017-12-15 19:30:09.619114: step 51450, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:03m:33s remains)
INFO - root - 2017-12-15 19:30:15.993661: step 51460, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 49h:57m:53s remains)
INFO - root - 2017-12-15 19:30:22.365897: step 51470, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 48h:26m:07s remains)
INFO - root - 2017-12-15 19:30:28.699830: step 51480, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 49h:19m:28s remains)
INFO - root - 2017-12-15 19:30:35.041732: step 51490, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 49h:05m:57s remains)
INFO - root - 2017-12-15 19:30:41.365145: step 51500, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 49h:45m:16s remains)
2017-12-15 19:30:41.875150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0751095 -4.171958 -4.5677905 -5.0952229 -5.2390108 -5.1277885 -4.8547564 -4.0414071 -2.9090672 -2.3982987 -3.4236326 -3.5503817 -4.0544114 -4.8954635 -5.4227681][-3.8609998 -4.0727396 -4.4130807 -4.9290705 -5.2864227 -5.2155008 -4.6580105 -4.1469011 -3.4359112 -2.5955634 -3.4718528 -4.0908079 -4.6081085 -5.2708158 -5.4267044][-4.1972446 -4.4770455 -4.86408 -4.9298077 -4.9560084 -4.6514845 -4.21867 -3.9350893 -3.0158243 -2.6636634 -3.8427522 -4.4394512 -5.12823 -5.7306519 -5.983057][-4.3864026 -4.258368 -4.2185802 -3.9303782 -3.7502525 -3.1511869 -2.4130793 -2.102591 -1.5558119 -1.5930572 -3.1222992 -4.0562859 -5.1406097 -6.130549 -6.4400663][-4.77991 -4.050468 -3.4038868 -2.8853712 -2.2232695 -1.2585177 -0.26188087 -0.04041338 0.30740643 0.049483776 -1.6939955 -2.7986708 -4.5584326 -6.05178 -6.6139927][-4.443223 -3.9894252 -3.0769486 -1.9743552 -0.76606369 0.55510807 1.8268156 2.0909653 2.3330193 1.8450851 -0.26954556 -1.5512986 -3.2905841 -4.8653555 -5.9385462][-4.3652148 -3.7785966 -2.4448957 -1.0923276 0.45158672 2.1338863 3.3218794 3.7069445 4.0286741 3.2037354 1.0148726 -0.56843185 -2.291254 -3.9549961 -5.14818][-3.749985 -3.1484504 -2.0689435 -0.71663475 0.99727821 2.4179993 3.572978 4.104845 4.3074579 3.4818449 1.0567942 -0.56831646 -2.235857 -3.6842875 -4.623992][-3.8257754 -3.1941447 -2.3163152 -1.0481663 0.42762089 1.6801891 2.4583645 2.9251375 3.5316706 2.7677355 0.12067509 -1.0972157 -2.5696182 -3.8948872 -4.450932][-4.192276 -3.8913851 -3.3038783 -2.4801154 -1.1210146 0.40596676 0.81043434 1.2683296 1.5835447 0.71825504 -1.1555662 -2.0472746 -3.277277 -4.3632712 -4.8755031][-5.907526 -5.46562 -5.1164885 -4.5809207 -3.4295273 -2.6105719 -1.9219484 -1.4731073 -1.4970007 -2.0444102 -3.3921046 -4.0559568 -4.5608358 -5.3584051 -5.9410124][-7.3230963 -7.0811129 -6.6180224 -6.1152859 -5.3762913 -4.8893833 -4.136178 -4.0654583 -3.8655102 -3.9877002 -5.18276 -5.1394191 -5.4361868 -6.1884856 -6.4001589][-7.579154 -7.6932578 -7.4638591 -7.0394468 -6.5751562 -5.9321423 -5.4782524 -5.5357113 -5.3146977 -5.2048788 -6.0188518 -5.6579981 -5.6821275 -6.2760248 -6.2472329][-7.0648322 -7.2176914 -7.1655602 -6.9990563 -6.6938949 -6.1015434 -5.6881895 -5.608489 -5.5307169 -5.4923944 -5.8783832 -5.7743235 -5.7755551 -5.8080158 -5.6769695][-7.0497036 -6.8469677 -6.6119 -6.6566682 -6.4663134 -6.2280941 -6.0605197 -6.0134306 -5.9842277 -5.9664464 -5.9524908 -5.9935102 -5.9164996 -5.9880896 -6.1160965]]...]
INFO - root - 2017-12-15 19:30:48.282726: step 51510, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 49h:42m:45s remains)
INFO - root - 2017-12-15 19:30:54.699778: step 51520, loss = 0.34, batch loss = 0.22 (12.8 examples/sec; 0.626 sec/batch; 48h:49m:48s remains)
INFO - root - 2017-12-15 19:31:01.110548: step 51530, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 50h:19m:31s remains)
INFO - root - 2017-12-15 19:31:07.457333: step 51540, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 49h:21m:06s remains)
INFO - root - 2017-12-15 19:31:13.854344: step 51550, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 49h:41m:20s remains)
INFO - root - 2017-12-15 19:31:20.176926: step 51560, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 49h:51m:15s remains)
INFO - root - 2017-12-15 19:31:26.546490: step 51570, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 48h:54m:07s remains)
INFO - root - 2017-12-15 19:31:32.878541: step 51580, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 50h:31m:34s remains)
INFO - root - 2017-12-15 19:31:39.259708: step 51590, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 50h:01m:18s remains)
INFO - root - 2017-12-15 19:31:45.624568: step 51600, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 49h:51m:44s remains)
2017-12-15 19:31:46.145330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2134581 -2.1953592 -1.6634288 -0.99331331 -0.4626255 0.12380838 0.79225922 0.8100481 0.69603443 -0.72356939 -2.0111661 -4.1097445 -5.4699984 -6.4672031 -7.3690667][-1.8377314 -1.5775938 -1.3422871 -0.9951911 -0.69449329 0.29288673 1.0938663 1.1301241 0.93223 -0.52538061 -1.7890987 -3.8434443 -5.5793929 -6.5797 -7.3549566][-2.55836 -1.9758611 -1.613626 -1.0289755 -0.59866571 -0.06046772 0.78163242 1.0700655 1.057435 -0.59559631 -2.090168 -3.8574049 -5.0238476 -6.3303757 -7.5605755][-3.3753572 -2.6621513 -2.1821542 -1.3442049 -0.41209412 0.3860569 0.85061455 0.88995647 0.97437096 -0.54498005 -1.9115801 -3.8201377 -5.0568876 -5.9344587 -7.087636][-4.1463737 -3.1542306 -2.264133 -1.3822823 -0.28915358 0.40405273 1.0158911 0.88275337 0.60236454 -1.0130973 -2.4027195 -4.2992644 -5.2524681 -6.081583 -6.9336891][-3.6999516 -2.9212666 -2.1664414 -1.389739 -0.44737434 0.23389196 0.69090748 0.91826439 0.69148922 -1.2352881 -2.5005751 -4.27789 -5.5002208 -6.2891083 -7.1457448][-2.9065604 -2.3431435 -1.6969113 -0.70002222 0.28155661 0.65341091 0.82147217 0.91325951 0.86603069 -0.868526 -2.5938067 -4.6272812 -5.4743338 -6.2115717 -7.0711236][-1.6948571 -1.2833786 -0.47473049 0.59172153 1.4994249 2.2695618 2.3932238 2.0006104 1.4400988 -0.47330332 -2.044292 -4.3730459 -5.4132538 -6.106698 -6.7881594][-1.4299512 -0.50436306 0.33001804 1.4457588 2.5391865 2.7443333 2.970232 2.6536694 1.8634977 -0.1706171 -1.7956147 -4.1188622 -5.416831 -6.1735878 -6.7675147][-2.0089908 -0.77653646 0.4592638 1.6125298 2.4783411 3.1944017 3.5567608 3.0237494 2.5240812 0.11311769 -1.8460207 -4.138587 -5.4541445 -6.404531 -7.1884155][-4.3921504 -3.1043386 -1.1420031 0.72367096 2.0693169 2.9165335 3.1101761 2.2370663 1.2986746 -1.1246085 -3.2261119 -5.0897074 -5.7716718 -6.275527 -6.8394084][-5.6938086 -4.3188863 -2.2396083 0.20534468 1.7141151 2.5640478 2.4687138 1.2276478 0.24523449 -1.8989735 -3.4483213 -5.2201071 -5.9696579 -6.3275051 -6.433712][-5.3359346 -4.8233271 -3.6167479 -1.81598 -0.55546188 0.075850964 0.27604055 -0.22697926 -1.3979998 -2.9843535 -4.5166364 -5.5234165 -6.1891184 -6.4217815 -6.4004645][-6.2500825 -6.0774765 -5.0807848 -3.4366202 -2.0577049 -1.4339247 -1.3555055 -1.8836856 -2.518486 -3.7179878 -4.5825348 -5.3815718 -5.5275044 -5.833199 -6.1104932][-8.4780893 -8.2099972 -7.1843967 -5.5955834 -4.45761 -3.8431056 -3.5374188 -4.0251074 -4.5796127 -4.9887009 -5.4771495 -5.9626431 -6.031683 -5.9685774 -6.1112704]]...]
INFO - root - 2017-12-15 19:31:52.433226: step 51610, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.639 sec/batch; 49h:50m:40s remains)
INFO - root - 2017-12-15 19:31:58.797568: step 51620, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 51h:12m:31s remains)
INFO - root - 2017-12-15 19:32:05.131800: step 51630, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 50h:20m:56s remains)
INFO - root - 2017-12-15 19:32:11.521668: step 51640, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 48h:38m:49s remains)
INFO - root - 2017-12-15 19:32:17.804670: step 51650, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.616 sec/batch; 48h:04m:44s remains)
INFO - root - 2017-12-15 19:32:24.243824: step 51660, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 50h:30m:47s remains)
INFO - root - 2017-12-15 19:32:30.646388: step 51670, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 48h:14m:32s remains)
INFO - root - 2017-12-15 19:32:37.022978: step 51680, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 49h:40m:55s remains)
INFO - root - 2017-12-15 19:32:43.386240: step 51690, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 50h:21m:40s remains)
INFO - root - 2017-12-15 19:32:49.766497: step 51700, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.618 sec/batch; 48h:12m:31s remains)
2017-12-15 19:32:50.297917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.898283 -3.3993173 -4.0215454 -4.0942287 -4.4208522 -3.8823047 -3.1448455 -2.9037256 -2.3643389 -3.4484153 -5.6362286 -7.7954464 -8.8977451 -9.9785118 -10.401551][-2.4077029 -3.3426461 -4.5078545 -5.32992 -5.6915989 -5.2146063 -4.5230937 -3.5469003 -3.2004333 -4.3085556 -5.8082952 -7.4962053 -8.95462 -9.756216 -10.269093][-2.2010083 -2.5476141 -3.776938 -5.003037 -5.4639173 -5.3435259 -4.5230227 -3.9435418 -3.4666672 -4.1077633 -5.6906595 -7.1246896 -8.02874 -9.0030107 -9.7407227][-2.57449 -2.675149 -3.7323384 -3.974021 -3.0620112 -2.6710782 -2.1817961 -1.6489305 -1.1989355 -2.5576553 -4.8405523 -7.0171266 -8.6160965 -9.0411177 -9.2278872][-3.0261078 -2.2554865 -2.368628 -2.1510282 -1.8474121 -0.53067541 1.2832203 1.4795055 1.469224 -0.23232508 -3.169951 -6.04049 -7.6597505 -8.4779873 -9.132616][-3.2141337 -2.6463556 -1.6595697 -0.84742546 -0.022377968 1.8981695 3.5507708 3.7931471 3.9909544 1.4776831 -1.5820966 -4.2465219 -6.9159551 -8.154727 -8.3752422][-3.28053 -2.7689381 -1.5288525 -0.49206257 0.6672802 2.7695179 4.8050518 5.5003586 5.3220291 2.7132998 -0.673563 -4.1566834 -6.3480558 -7.7376776 -8.4546118][-3.2297316 -2.6928153 -1.9207497 -0.32926702 1.3556061 2.9125071 4.1133375 4.9926405 5.2724342 2.9913664 -0.22937107 -3.4515147 -6.0041909 -7.4232731 -7.6752853][-3.5202293 -2.7819362 -2.2992616 -1.0834942 0.40668488 1.6781578 2.9878273 4.2642908 4.5407295 2.4739447 -0.84238434 -4.0706367 -6.1195922 -7.1332073 -7.8532925][-4.658237 -4.095871 -3.108459 -2.3707981 -1.5630155 -0.7993145 -0.10533381 0.58658314 0.92720127 -0.68544817 -2.7883115 -5.0010586 -6.8639259 -7.8224072 -8.2217951][-6.6492906 -5.9715695 -5.107224 -4.2376575 -3.5459023 -3.0039854 -2.5688438 -2.5979872 -2.7823386 -4.1359177 -6.2387238 -7.5730991 -8.2776222 -8.9513483 -8.9682627][-7.9284611 -7.4125161 -6.8151107 -6.2556543 -5.5292988 -5.210063 -5.1169252 -5.1837988 -5.22304 -6.0524306 -7.5901566 -7.8904724 -8.5088491 -9.4998846 -9.1516991][-8.3631668 -8.2629824 -7.8780503 -7.2616181 -6.799252 -6.7688346 -6.3742747 -6.112371 -6.1999416 -6.1991463 -7.1561322 -7.5088048 -7.8715334 -8.2143488 -7.8653293][-7.8802171 -8.3182859 -8.0523081 -7.4443235 -7.1978436 -6.9960032 -6.6546297 -6.1428056 -5.4044704 -5.6353469 -6.1022496 -5.5242653 -5.6093597 -6.1318355 -5.9333339][-7.4691267 -8.1551723 -8.4494257 -8.5560741 -8.5459967 -8.3077621 -8.0835724 -7.84323 -7.4913826 -7.1719246 -6.7346578 -5.9583206 -5.4925275 -5.8624697 -5.7269335]]...]
INFO - root - 2017-12-15 19:32:56.700041: step 51710, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 48h:36m:33s remains)
INFO - root - 2017-12-15 19:33:03.129424: step 51720, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.653 sec/batch; 50h:53m:38s remains)
INFO - root - 2017-12-15 19:33:09.552721: step 51730, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 50h:20m:05s remains)
INFO - root - 2017-12-15 19:33:15.932396: step 51740, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 50h:28m:32s remains)
INFO - root - 2017-12-15 19:33:22.387133: step 51750, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.615 sec/batch; 47h:55m:42s remains)
INFO - root - 2017-12-15 19:33:28.684376: step 51760, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 48h:59m:42s remains)
INFO - root - 2017-12-15 19:33:35.061000: step 51770, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 49h:26m:57s remains)
INFO - root - 2017-12-15 19:33:41.536643: step 51780, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:39m:15s remains)
INFO - root - 2017-12-15 19:33:47.947742: step 51790, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 48h:42m:56s remains)
INFO - root - 2017-12-15 19:33:54.367063: step 51800, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 49h:50m:29s remains)
2017-12-15 19:33:54.873492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6113691 -3.4680004 -4.2402678 -4.3191986 -3.8447325 -3.3330526 -3.5367265 -3.2328086 -2.8345752 -3.2659521 -3.5843596 -5.6366777 -6.1673565 -6.1386547 -7.2950521][-3.0590177 -3.9918587 -5.0291057 -5.3086147 -5.0367594 -4.1732311 -3.6292253 -3.6524029 -3.7211154 -4.2829933 -4.5048094 -6.2390504 -6.7330728 -6.7993593 -7.5852971][-3.6915829 -3.9687564 -4.7697248 -4.9254127 -5.059248 -4.6921082 -4.219635 -3.9185174 -3.3148279 -4.1049528 -4.8073936 -6.7459 -7.1582003 -7.4415326 -8.3072109][-4.5111008 -4.7967415 -5.0398846 -4.5158939 -4.2268848 -4.0443697 -3.3053098 -2.7358632 -2.4273019 -2.9738183 -3.6801562 -6.0266457 -6.8504519 -7.1898427 -7.8206954][-5.4284329 -5.3779712 -5.4325476 -4.8260636 -3.9178922 -2.9588084 -1.5989962 -1.1896405 -0.841671 -1.9590402 -3.1975422 -5.5545368 -6.3806725 -6.9662962 -7.9469094][-7.1320744 -6.6181626 -6.057404 -4.6005669 -3.4224892 -1.9950557 -0.19468498 0.79510689 0.84625244 -1.3726211 -3.3176756 -6.1824756 -6.7799411 -6.925427 -7.6563897][-6.8861971 -6.5564952 -6.1088929 -4.3160329 -2.1069322 -0.0064411163 2.1684313 2.8608713 3.0237923 1.1532412 -1.3526855 -4.862812 -6.0724888 -6.2946129 -6.9751863][-6.3823853 -5.9443 -5.3127027 -3.5386705 -1.3776493 1.4933558 3.9978876 4.0419731 3.6271515 1.5677872 -0.31824589 -3.4895182 -4.7899485 -5.1047029 -5.6918058][-6.4192514 -5.9934492 -5.5894814 -4.2099771 -2.1750398 0.82325554 3.2618465 3.7459602 3.5179405 0.97110176 -0.93653584 -3.9426091 -5.0431523 -5.2969465 -6.2765894][-6.3087749 -6.102612 -5.9726572 -4.8054132 -3.2537961 -1.2736621 0.85067844 1.4201384 1.3554268 -0.76470232 -2.9316864 -5.7414336 -6.3687363 -5.8867984 -6.77972][-7.71011 -7.4503746 -7.0344696 -5.802268 -4.7186441 -3.3438749 -2.0953937 -1.6564364 -1.3380127 -2.932457 -3.9927037 -6.2933445 -7.0037117 -6.3760881 -6.8499923][-7.6910939 -7.3588209 -7.0594749 -6.1121054 -5.5832691 -4.9211082 -3.9498217 -3.8257952 -3.8229308 -4.1672125 -4.4244261 -5.9851494 -6.3375106 -6.2780075 -6.8274584][-8.7034264 -7.9650793 -7.3662052 -6.5110397 -5.9222493 -5.9234896 -5.4363093 -5.3674722 -5.0289841 -5.7281256 -6.0379543 -6.3976374 -6.3946795 -6.3652506 -6.6961122][-8.567873 -8.0631676 -7.2376575 -6.358583 -5.8956332 -6.0970726 -5.4870634 -5.56508 -5.6239939 -5.8812857 -6.0912647 -6.2617264 -6.1771936 -6.3070955 -6.5960455][-8.6371956 -8.8428583 -8.2484875 -7.33979 -6.5642691 -6.6649895 -6.7142529 -6.7221074 -6.6389594 -6.57045 -6.7229419 -6.5431771 -6.2831311 -6.009068 -5.6936164]]...]
INFO - root - 2017-12-15 19:34:01.178600: step 51810, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 49h:53m:50s remains)
INFO - root - 2017-12-15 19:34:07.677161: step 51820, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 49h:57m:01s remains)
INFO - root - 2017-12-15 19:34:14.095360: step 51830, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 50h:31m:31s remains)
INFO - root - 2017-12-15 19:34:20.578764: step 51840, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 49h:26m:55s remains)
INFO - root - 2017-12-15 19:34:27.053056: step 51850, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 51h:06m:09s remains)
INFO - root - 2017-12-15 19:34:33.503990: step 51860, loss = 0.24, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 50h:52m:26s remains)
INFO - root - 2017-12-15 19:34:39.973610: step 51870, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 49h:01m:58s remains)
INFO - root - 2017-12-15 19:34:46.383101: step 51880, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 51h:11m:02s remains)
INFO - root - 2017-12-15 19:34:52.804654: step 51890, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 50h:32m:00s remains)
INFO - root - 2017-12-15 19:34:59.270815: step 51900, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 49h:26m:09s remains)
2017-12-15 19:34:59.793054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6692491 -2.8051 -3.2248936 -3.4347782 -3.3910556 -3.3390322 -3.3843379 -3.4764695 -3.6955769 -5.3372269 -5.370429 -6.4184961 -6.7262287 -7.0484581 -7.7352824][-2.9259591 -2.9202724 -3.4974189 -3.8315058 -4.042767 -3.8473189 -3.8136196 -4.0364437 -4.1424437 -5.6042318 -5.4623747 -6.5887814 -6.9787488 -7.3488173 -7.7142105][-3.4343123 -3.3601818 -3.9531095 -4.0497494 -4.0724592 -3.9099352 -3.899034 -4.0025468 -4.145102 -5.8285236 -5.8291111 -6.8010712 -7.0528841 -7.4366212 -8.0849876][-3.2602577 -2.8938651 -3.0758114 -2.8069992 -2.7328906 -2.3517227 -2.2251678 -2.4812021 -2.7838778 -4.570507 -5.0057287 -6.4083223 -6.7702584 -6.77865 -7.0818138][-3.3074551 -2.5510468 -2.3448992 -1.9488368 -1.7859054 -0.984766 -0.55308151 -0.88688183 -1.0582557 -3.1182733 -3.7032473 -5.5807991 -6.3749704 -6.5723114 -7.00721][-4.8228688 -3.7738447 -3.7163506 -2.9016728 -2.0971599 -0.54773188 0.57293892 0.53152275 0.25577545 -2.2475967 -3.0925651 -5.3214684 -6.2220826 -6.50396 -7.1135921][-4.107338 -3.5635 -3.2743506 -1.8129053 -0.82334089 0.6792593 1.9897318 2.4175777 2.1351891 -0.88602829 -2.2560563 -4.5718246 -5.3937416 -5.6767197 -6.1652136][-2.995862 -2.0213289 -1.4885378 -0.45075369 0.59277153 2.2548323 3.6283035 3.7493858 3.6189232 0.52671051 -0.85397482 -3.4228387 -4.477088 -4.742672 -5.2150679][-3.9303882 -2.7582507 -2.0789657 -0.91156816 -0.35714436 0.72936058 1.9388752 2.3886223 2.6580963 -0.36967611 -1.6763954 -3.7508895 -4.8302746 -5.2528152 -6.1265984][-4.8077612 -3.8403 -3.047677 -1.9168396 -0.93271589 -0.1636343 0.68939304 1.2497377 1.1748152 -1.7152166 -2.7037134 -4.4809709 -5.3551235 -5.8115306 -6.4887781][-6.1781473 -5.4522877 -4.726059 -3.670249 -3.0419836 -2.3678956 -1.849092 -1.3612909 -1.1287694 -3.4178572 -4.2677765 -5.6663156 -6.2874537 -6.638021 -7.3966374][-7.4824877 -6.8591752 -6.0648036 -5.4316158 -4.8690042 -4.2143688 -4.017952 -3.7398219 -3.4576912 -4.7065034 -4.8505611 -5.6018486 -6.1692991 -6.6149259 -7.4411273][-8.2983332 -7.9253225 -7.5344009 -6.6907492 -6.1286411 -5.8037019 -5.6076045 -5.3693943 -5.3112569 -6.0407691 -5.941937 -6.1932082 -5.9595509 -6.0586095 -6.3604836][-8.6756058 -8.2865133 -8.0755138 -7.5966487 -7.1102643 -6.3849559 -5.9850459 -6.050601 -6.0860491 -6.50488 -6.26497 -6.3524203 -6.3818865 -6.2995906 -6.4119349][-9.4872923 -8.981925 -8.6511374 -8.1097612 -7.8227811 -7.5112739 -7.1853862 -7.1016951 -7.031651 -6.9485016 -7.02204 -6.6974869 -6.4126573 -6.1356807 -5.9180069]]...]
INFO - root - 2017-12-15 19:35:06.180947: step 51910, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:00m:46s remains)
INFO - root - 2017-12-15 19:35:12.602644: step 51920, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:38m:19s remains)
INFO - root - 2017-12-15 19:35:19.067774: step 51930, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 49h:54m:55s remains)
INFO - root - 2017-12-15 19:35:25.542262: step 51940, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 50h:43m:54s remains)
INFO - root - 2017-12-15 19:35:32.068250: step 51950, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 49h:12m:43s remains)
INFO - root - 2017-12-15 19:35:38.492644: step 51960, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 49h:16m:52s remains)
INFO - root - 2017-12-15 19:35:44.943054: step 51970, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 50h:21m:42s remains)
INFO - root - 2017-12-15 19:35:51.359976: step 51980, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 49h:36m:12s remains)
INFO - root - 2017-12-15 19:35:57.871124: step 51990, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 49h:44m:11s remains)
INFO - root - 2017-12-15 19:36:04.241409: step 52000, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 50h:06m:46s remains)
2017-12-15 19:36:04.755896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9763124 -3.7103598 -3.6478853 -3.2675848 -2.8816643 -2.4930739 -1.8481951 -1.6317654 -1.707037 -4.4476938 -6.2544456 -8.0619678 -9.4960775 -10.035207 -10.135809][-5.1200933 -4.4310637 -4.0392179 -3.8963408 -3.5760899 -3.2125869 -2.8265867 -2.8272033 -2.6458712 -5.1385474 -6.478549 -7.8683949 -9.3237734 -10.089756 -10.424616][-4.9960928 -4.3460426 -3.5530806 -2.8785977 -2.2448645 -2.1816516 -1.6836433 -1.4142265 -1.1512694 -3.1303339 -4.4943733 -6.4310989 -8.3936958 -9.3185406 -10.056213][-3.6186366 -2.9186053 -2.17098 -1.5491381 -0.99117708 -0.8204689 -0.17220497 0.53228855 1.2395363 -0.67033005 -1.9637823 -3.9150128 -5.9354086 -6.6914244 -7.5711884][-3.8308623 -2.5190063 -1.1947293 -0.31940889 0.64827156 1.6253796 2.6843805 3.0951262 3.3762751 1.2395468 -0.34830332 -2.6583438 -5.2409925 -6.476789 -7.2558575][-3.6677876 -2.24892 -0.99295473 0.502409 1.8171883 2.9376059 4.1375875 4.5653 5.0812435 2.9191713 1.0778255 -1.0738988 -3.2980704 -4.962234 -6.3432665][-3.7605443 -2.2959776 -0.78195572 0.97229385 2.8882313 4.1797104 5.1857281 5.6889935 6.4093294 3.6661539 1.61872 -0.97829819 -3.955323 -5.1548424 -5.9930778][-3.8996835 -2.2310109 -0.76659107 1.1844835 2.8881416 4.2071533 5.4636831 5.9631262 6.0473967 3.1353006 1.0297318 -1.6167579 -4.4474821 -5.5466738 -6.1301732][-4.5863853 -3.1902714 -1.8246694 0.39581585 2.3034563 3.8070364 4.9706221 4.9455395 4.2372236 1.1608877 -0.45939159 -2.811852 -5.0666981 -5.8838577 -6.2877264][-4.2207961 -3.3894639 -2.4782557 -0.59520864 1.0142183 2.1911497 3.1370659 3.0459976 2.6915636 0.11409998 -1.3157353 -3.4014807 -4.9117718 -5.7941236 -6.2419348][-5.5950794 -5.12871 -4.5362778 -3.1462355 -1.8470855 -0.53475666 0.565938 0.52376652 0.39918137 -1.9880767 -3.7159822 -5.4109974 -6.6670732 -6.83894 -7.0738492][-7.0948052 -6.3978891 -5.6766539 -4.8629313 -4.2098389 -3.1389141 -2.5694418 -2.7407813 -2.7261739 -4.4631386 -5.2486305 -6.2732797 -6.9540334 -7.0126204 -6.9376345][-7.44479 -7.2886124 -6.9924927 -6.3754463 -5.6317906 -5.044301 -4.4210243 -4.347858 -4.4477572 -5.5536208 -5.8782425 -6.739316 -7.1176739 -6.7467723 -6.2378531][-7.1057825 -7.0289063 -6.7720132 -6.33593 -5.7487803 -4.816433 -4.008338 -4.1051488 -4.2186003 -4.9762554 -5.4070997 -5.9104228 -6.4214921 -6.3089733 -6.2018185][-8.745121 -8.6019783 -8.4035892 -7.7390947 -7.06458 -6.3097925 -5.8912492 -6.0115209 -6.3002248 -6.2757139 -6.1931834 -6.174552 -6.2020125 -6.2436676 -6.1251979]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:36:11.286212: step 52010, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 50h:26m:21s remains)
INFO - root - 2017-12-15 19:36:17.640614: step 52020, loss = 0.37, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 49h:10m:53s remains)
INFO - root - 2017-12-15 19:36:24.123568: step 52030, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 52h:20m:50s remains)
INFO - root - 2017-12-15 19:36:30.627130: step 52040, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 50h:30m:36s remains)
INFO - root - 2017-12-15 19:36:37.062750: step 52050, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 48h:55m:01s remains)
INFO - root - 2017-12-15 19:36:43.506409: step 52060, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 51h:04m:00s remains)
INFO - root - 2017-12-15 19:36:49.966506: step 52070, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 51h:28m:00s remains)
INFO - root - 2017-12-15 19:36:56.308187: step 52080, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 50h:06m:36s remains)
INFO - root - 2017-12-15 19:37:02.790857: step 52090, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 50h:06m:45s remains)
INFO - root - 2017-12-15 19:37:09.136813: step 52100, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 48h:49m:33s remains)
2017-12-15 19:37:09.647626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5567026 -4.7414427 -5.1334896 -5.316381 -4.8449326 -4.9335556 -4.7783809 -4.2095356 -4.7161875 -4.6708584 -5.3102846 -6.0728769 -6.6182375 -7.1576257 -6.8320069][-2.5889249 -2.3768973 -2.4959455 -3.2988143 -3.9378555 -4.6272821 -4.4430323 -4.6923914 -4.9002938 -4.8364544 -5.3906231 -5.9146533 -6.7020493 -7.1095109 -6.7473874][-1.991724 -2.1162615 -2.5061202 -3.31183 -3.780797 -3.96249 -4.4629345 -4.6238976 -4.2994595 -4.878839 -5.6690259 -6.4189816 -7.4462862 -7.6816149 -6.9999104][-3.4992666 -2.9032269 -2.5447345 -2.6168585 -2.993669 -3.1142006 -3.6800551 -3.8239727 -3.7477705 -4.5656385 -5.2390671 -5.8711791 -6.6570988 -6.4703016 -6.0429335][-4.7528343 -4.5409174 -4.3516335 -3.7102213 -2.9427872 -2.2708077 -1.7470899 -1.7178564 -1.9052515 -2.1967239 -2.5354266 -3.4060936 -4.5568805 -5.0965204 -5.3577852][-1.60816 -1.9489584 -2.3456697 -2.1832862 -2.203269 -1.5526404 -0.82631063 -0.791317 -1.0338035 -1.6334519 -1.6838689 -2.4310741 -3.5908365 -4.13525 -4.24939][-0.34734583 0.46532822 1.0665054 0.085045338 -0.10025978 0.72024536 0.61926079 0.54247284 0.56546021 -0.80134106 -1.8540835 -2.8749661 -3.8525732 -4.4687815 -4.5439253][0.093770981 0.30408621 0.433815 0.40570164 0.7254076 0.99836636 1.2224064 1.6044168 2.0977859 0.82702255 -0.43909645 -2.3377733 -3.8621435 -4.2971144 -4.7527084][0.1957612 0.69328594 0.7489872 1.132206 1.2961493 1.0607672 0.98260593 1.1962271 1.4468079 0.69210815 -0.06261158 -1.9346256 -3.4502997 -4.13441 -4.782022][1.3566656 1.7905121 1.6272335 2.0650206 1.6697664 1.1564093 1.379632 1.3436184 0.82966042 0.22497654 -0.57626534 -2.2036991 -3.5802193 -4.6811118 -4.9389114][-1.3956795 -1.5472364 -1.2500253 -1.5076475 -1.7960525 -1.4963408 -1.208662 -1.1836905 -1.066783 -2.1167107 -3.9439054 -4.6740241 -5.8228722 -6.9848413 -7.0129633][-4.4286938 -3.7288771 -3.6903448 -4.0714474 -4.1604142 -4.5057745 -5.05326 -4.8298669 -4.1600018 -5.0110779 -6.0128226 -6.287715 -7.4191155 -8.293663 -8.6126413][-4.9504571 -4.4233828 -4.559761 -4.8736072 -5.5824785 -5.6024008 -5.4994354 -5.5038071 -5.3713417 -5.9125328 -7.0052056 -7.2310176 -8.1080027 -8.24959 -7.6130877][-6.4522929 -5.8479233 -6.1343503 -5.8604889 -5.9382215 -6.2306042 -5.9811931 -6.2018723 -6.3207655 -6.239789 -6.6940041 -6.4863291 -6.5766425 -6.38165 -6.2401719][-5.179286 -5.293642 -5.0335236 -5.18863 -5.4560885 -5.0032749 -4.92237 -4.7700224 -4.4992132 -4.4723969 -4.9992552 -5.36053 -5.5768037 -5.7666459 -5.6932144]]...]
INFO - root - 2017-12-15 19:37:15.984031: step 52110, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 48h:42m:03s remains)
INFO - root - 2017-12-15 19:37:22.349589: step 52120, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 49h:30m:08s remains)
INFO - root - 2017-12-15 19:37:28.693880: step 52130, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:29m:18s remains)
INFO - root - 2017-12-15 19:37:35.141312: step 52140, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 48h:36m:51s remains)
INFO - root - 2017-12-15 19:37:41.612848: step 52150, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 49h:53m:25s remains)
INFO - root - 2017-12-15 19:37:48.016182: step 52160, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 48h:58m:42s remains)
INFO - root - 2017-12-15 19:37:54.443757: step 52170, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 50h:28m:35s remains)
INFO - root - 2017-12-15 19:38:00.796646: step 52180, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 48h:58m:00s remains)
INFO - root - 2017-12-15 19:38:07.252104: step 52190, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 50h:02m:41s remains)
INFO - root - 2017-12-15 19:38:13.634544: step 52200, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 49h:40m:55s remains)
2017-12-15 19:38:14.137817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9299598 -2.7943606 -2.9041915 -2.6880593 -1.9452319 -0.99547243 -0.68743467 -0.43024158 0.051927567 0.079441071 -1.5621581 -3.7042553 -5.3669224 -6.1865997 -6.9695196][-2.2783494 -2.9885244 -3.653975 -3.3729773 -2.5682998 -1.3479528 -0.50939083 -0.26606321 -0.10516882 -0.47761488 -2.0168738 -3.9680684 -6.2136078 -6.5993528 -7.3964806][-3.8750293 -3.7650614 -3.8554316 -3.8237975 -2.95259 -2.1668496 -1.182116 -0.89526367 -0.98289824 -1.3227696 -2.959537 -4.6805205 -6.74153 -7.6508074 -8.334342][-5.02856 -4.705533 -4.2802649 -3.5539579 -2.2232122 -1.1761932 -0.41053629 -0.27938938 -0.31930733 -1.0113564 -2.801589 -4.7557364 -6.3797188 -7.1229758 -7.8536716][-5.0222654 -4.4392505 -3.6934013 -2.5056906 -1.1439471 0.54398155 1.3331518 0.99717712 0.17225313 -0.96204758 -2.9612079 -4.9552317 -6.4591751 -6.9619861 -7.4890776][-5.5136452 -4.5143471 -3.2273536 -1.6540284 0.638546 2.1363544 3.1619549 2.5904045 1.4377222 -0.6174531 -3.2858448 -5.6981039 -7.3945351 -7.2661858 -7.651113][-4.8738995 -4.0376415 -2.6436648 -0.71936607 1.7748919 3.5552597 4.7241392 4.1352043 2.74648 0.44022942 -2.700223 -5.1620569 -6.9810581 -7.2847033 -7.3822145][-4.492867 -3.2378454 -1.6795778 -0.0079016685 1.9915934 3.8509111 5.0249796 4.5477238 3.310111 0.95292091 -2.22437 -4.9387512 -6.7748594 -7.1039181 -7.0798769][-4.6629086 -3.9285026 -2.5822878 -1.078578 0.74494743 2.2526159 3.1276903 2.9811888 2.301259 0.11369753 -2.6008544 -4.9597597 -6.6855145 -6.963563 -7.307117][-6.1206865 -5.5025034 -4.7933092 -3.4655147 -2.1184578 -0.92176867 0.26730967 0.51794529 0.57866955 -1.3779154 -3.9222496 -5.9933124 -7.6657157 -7.5375962 -7.9999185][-7.1356969 -7.0910616 -6.42159 -5.2648511 -4.7066436 -3.6545415 -2.6828685 -2.7389383 -2.5834775 -3.8732803 -5.1587753 -6.7369995 -7.9555936 -8.0858307 -8.20451][-8.0698681 -7.6358314 -7.4557042 -6.7656593 -5.9676952 -5.2930188 -4.7175207 -4.3787389 -4.0920997 -4.7085571 -5.5554404 -6.5860472 -7.3813109 -7.1650515 -7.7904458][-9.2070837 -8.820158 -8.40017 -7.6305418 -6.8298292 -6.37323 -5.84064 -6.1237559 -5.7606964 -5.9275661 -5.8846483 -6.3860445 -6.5633397 -6.5728192 -7.4063492][-8.8805952 -8.4872618 -7.8690991 -7.1861076 -6.921936 -6.5327206 -6.10398 -6.3359642 -6.3904095 -6.7064767 -6.3004804 -5.901475 -6.3431044 -5.689105 -5.8233647][-8.5378208 -8.4649849 -8.2768631 -8.0469828 -7.5613265 -7.0605483 -6.9302773 -7.1738386 -7.1322231 -6.7724776 -6.4831467 -6.3794332 -5.6403465 -5.6456795 -5.9352894]]...]
INFO - root - 2017-12-15 19:38:20.536986: step 52210, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 50h:22m:22s remains)
INFO - root - 2017-12-15 19:38:26.909514: step 52220, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.632 sec/batch; 49h:14m:34s remains)
INFO - root - 2017-12-15 19:38:33.369089: step 52230, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 48h:58m:55s remains)
INFO - root - 2017-12-15 19:38:39.816193: step 52240, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 51h:32m:34s remains)
INFO - root - 2017-12-15 19:38:46.303267: step 52250, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 51h:27m:07s remains)
INFO - root - 2017-12-15 19:38:52.721084: step 52260, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 50h:25m:38s remains)
INFO - root - 2017-12-15 19:38:59.156306: step 52270, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 49h:57m:03s remains)
INFO - root - 2017-12-15 19:39:05.604045: step 52280, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 50h:13m:07s remains)
INFO - root - 2017-12-15 19:39:12.064275: step 52290, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 50h:40m:07s remains)
INFO - root - 2017-12-15 19:39:18.459334: step 52300, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 48h:47m:49s remains)
2017-12-15 19:39:18.972496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.56032 -4.1980586 -3.8800659 -3.7159767 -3.1753869 -1.9667048 -0.97097254 -0.42717171 -0.0050878525 0.20250416 -1.0725617 -2.6967611 -4.23463 -6.2495613 -7.4208612][-4.789958 -4.4159594 -4.0944204 -3.959172 -3.5620127 -2.8838429 -2.0459595 -1.63906 -1.0167832 -0.82128477 -2.0840249 -3.5646849 -4.6529479 -6.1350913 -6.8799214][-3.289701 -2.9503798 -2.7762966 -2.8320556 -2.7522464 -2.3221436 -1.8922529 -1.7168331 -1.2936287 -1.2784295 -2.1988797 -3.3777757 -4.3877726 -5.8586745 -6.474678][-2.3873644 -1.7937779 -1.3189726 -1.4693317 -1.6822853 -1.6473556 -1.2032294 -0.98401356 -0.84624004 -0.99361229 -1.9455967 -2.8493471 -4.164382 -5.5269408 -6.1673703][-1.6479683 -0.7330184 -0.36548042 -0.38774109 -0.37474108 -0.0050187111 0.26570702 0.45533657 0.42814922 -0.22255802 -1.575911 -2.8891239 -4.3227463 -5.6074429 -6.3532343][-1.2724757 -0.55524397 0.17751265 0.35122013 0.66463757 0.9313488 1.2242823 1.3436909 1.4454937 0.79044628 -0.46794987 -1.5805125 -3.1351719 -4.530529 -5.9861813][-1.7416835 -0.80412531 0.15854883 0.53249073 0.88587189 1.4054203 1.8683939 2.0466232 2.0589724 1.5829353 0.37412453 -1.0722322 -2.8573322 -4.61453 -6.1639481][-2.5129995 -1.537971 -0.56854153 0.1922307 1.1098471 1.8509817 2.3616867 2.5374012 2.5967932 1.9753017 0.5543251 -0.95508337 -2.5648537 -4.0933666 -5.1746688][-3.1442819 -2.002367 -0.92077637 0.044660568 0.99080849 1.8328781 2.2377844 2.4450226 2.5753956 2.1460876 0.98417282 -0.54320288 -2.0478816 -3.4865766 -4.5317583][-4.5628338 -3.5046873 -2.5092325 -1.6699109 -0.69229937 0.22594547 0.88907242 1.4258862 1.8923883 0.93282413 -0.36641979 -1.676528 -3.0228124 -3.9675274 -4.6653881][-6.4783382 -5.6769142 -5.1357589 -4.2890229 -3.3566318 -2.3018966 -1.7372217 -1.4735394 -1.0921855 -1.7168279 -2.6098175 -3.6068335 -4.6940613 -5.4485722 -6.0552006][-8.0869217 -7.5143538 -7.1398168 -6.4599648 -5.8151588 -4.7316895 -3.9609411 -3.5397911 -3.2295518 -3.8718562 -4.9335685 -5.5002937 -6.1949358 -6.7528191 -7.1292019][-8.3966579 -8.4121275 -8.3780422 -7.7758164 -7.1173816 -6.3711338 -5.7972612 -5.558053 -5.3995762 -5.6607761 -6.3564963 -6.7788992 -7.151207 -7.226789 -7.1367564][-7.9871597 -7.920784 -7.8442192 -7.606802 -7.3161321 -6.7199769 -6.2017879 -6.1375542 -6.2031889 -6.6204395 -7.2789159 -7.3115745 -7.0403051 -6.62113 -6.4822855][-8.1994543 -8.3517084 -8.1425037 -8.1398792 -8.1227522 -7.8128676 -7.4815516 -7.7786703 -8.01776 -8.2573166 -8.37187 -8.1140976 -7.6876855 -7.2879539 -6.711319]]...]
INFO - root - 2017-12-15 19:39:25.456516: step 52310, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 50h:39m:22s remains)
INFO - root - 2017-12-15 19:39:31.872376: step 52320, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 50h:01m:37s remains)
INFO - root - 2017-12-15 19:39:38.341794: step 52330, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 49h:59m:46s remains)
INFO - root - 2017-12-15 19:39:44.734648: step 52340, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 48h:37m:01s remains)
INFO - root - 2017-12-15 19:39:51.135760: step 52350, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 51h:24m:01s remains)
INFO - root - 2017-12-15 19:39:57.524665: step 52360, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 50h:53m:48s remains)
INFO - root - 2017-12-15 19:40:03.921544: step 52370, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 49h:18m:45s remains)
INFO - root - 2017-12-15 19:40:10.414348: step 52380, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 49h:25m:53s remains)
INFO - root - 2017-12-15 19:40:16.767971: step 52390, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 50h:47m:59s remains)
INFO - root - 2017-12-15 19:40:23.159616: step 52400, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 50h:44m:52s remains)
2017-12-15 19:40:23.705465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2081108 -6.0625463 -6.6792941 -6.9952707 -7.5230408 -8.0872679 -8.5904627 -8.078476 -7.8461485 -7.8520174 -7.2940841 -6.8449121 -7.4330835 -8.3263044 -8.4002762][-4.992775 -5.341507 -5.47799 -5.9756012 -7.8202391 -7.9449658 -8.2468224 -8.0963993 -7.5890837 -7.87534 -7.3507013 -6.7631826 -7.2555118 -7.9847407 -7.8896961][-3.4905796 -3.0357733 -3.2983613 -4.6016097 -5.1320958 -5.9264078 -6.3492889 -5.9011869 -5.581068 -6.2188454 -5.7106161 -5.7620831 -5.9869108 -6.7293096 -6.7224779][-3.6490908 -2.4871325 -2.7238836 -2.3745008 -2.8960776 -3.2235065 -2.4017568 -2.6824031 -2.2952638 -3.0637388 -3.9041479 -4.1863375 -4.6800976 -5.398674 -5.5859504][-2.6395411 -1.4899597 -1.1396947 -1.3415022 -1.8401017 -0.99170876 -0.27305031 0.30008888 0.98610878 -0.18865299 -1.7217917 -2.4938688 -3.9002666 -4.8089314 -5.344243][-2.8331118 -1.9253302 -1.4697065 -0.68768549 0.50725842 1.5752439 2.6532574 3.3027353 3.7808475 2.2146187 0.18137693 -1.1958818 -3.3554339 -5.2456188 -5.9751978][-2.9827576 -1.6202164 -0.70966434 0.41739368 1.8551788 3.1117268 3.9726667 4.3068619 4.6475992 2.4525614 0.31835175 -1.8089585 -3.960314 -6.0618792 -6.5198603][-3.6994488 -2.5671816 -1.7580194 0.37784672 2.2704191 3.2120953 4.1609812 4.6240292 4.7694788 2.3593206 0.093603134 -2.0748405 -4.3632975 -6.3320851 -6.9295311][-4.4631062 -3.6394978 -2.6648865 -1.2490754 0.26149511 2.2395563 3.4447031 3.7163687 4.0225973 1.6417036 -0.83317995 -2.8018441 -5.0590916 -6.8491583 -7.291348][-7.2387247 -6.5446229 -5.6199207 -4.5539865 -2.8750577 -1.1178565 -0.12675953 0.8406105 0.90674496 -1.3230777 -3.0170975 -4.5014753 -6.2829261 -7.7148786 -8.02784][-8.6420155 -8.3883667 -8.0776138 -7.2039566 -6.1980281 -4.8863173 -3.3737941 -2.3842421 -2.1583748 -3.676537 -5.5254006 -6.3686972 -7.3150249 -8.4423037 -8.6441259][-9.9971075 -9.6179676 -9.3216515 -8.6691723 -8.0754642 -7.2534308 -6.1092553 -5.6368895 -4.8288651 -5.6917224 -6.7838016 -7.05001 -8.1354656 -8.6778736 -8.6885347][-10.812806 -10.315207 -10.040995 -9.3313484 -8.6909389 -8.1878157 -7.8013878 -7.6361456 -7.4395185 -7.4823079 -7.8783164 -7.5738554 -7.435976 -7.4883485 -7.3941774][-9.6622305 -9.3694077 -9.0719213 -8.499712 -7.6431341 -7.2270408 -6.5397439 -6.7225256 -7.230022 -7.279346 -7.9745288 -7.5924058 -7.2907557 -6.8835454 -6.3229022][-9.0179367 -8.7413769 -8.5261154 -8.2749681 -8.2394791 -7.7879734 -7.3122768 -7.3908558 -7.3061743 -7.6258769 -8.0008335 -7.7541771 -7.2573872 -6.7233753 -6.3090963]]...]
INFO - root - 2017-12-15 19:40:30.084537: step 52410, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 48h:56m:01s remains)
INFO - root - 2017-12-15 19:40:36.536811: step 52420, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 50h:00m:31s remains)
INFO - root - 2017-12-15 19:40:42.900560: step 52430, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 50h:10m:38s remains)
INFO - root - 2017-12-15 19:40:49.395019: step 52440, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 50h:27m:49s remains)
INFO - root - 2017-12-15 19:40:55.745951: step 52450, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 48h:27m:04s remains)
INFO - root - 2017-12-15 19:41:02.084125: step 52460, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 49h:17m:55s remains)
INFO - root - 2017-12-15 19:41:08.611892: step 52470, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 50h:08m:17s remains)
INFO - root - 2017-12-15 19:41:14.994969: step 52480, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 49h:31m:56s remains)
INFO - root - 2017-12-15 19:41:21.422145: step 52490, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 49h:42m:51s remains)
INFO - root - 2017-12-15 19:41:27.858737: step 52500, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:21m:35s remains)
2017-12-15 19:41:28.345729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6486483 -4.5585041 -4.4601345 -4.4332581 -4.4982548 -4.2001038 -4.0260572 -3.3724947 -2.7832108 -3.3620386 -4.2241173 -5.5891504 -6.2479024 -7.6414967 -8.4497662][-4.9529233 -4.8744965 -5.0517273 -4.9339066 -5.0328131 -4.8787928 -4.6541734 -4.0268593 -3.3797207 -3.6604519 -4.180109 -5.5359688 -6.2422624 -7.4737244 -8.2496243][-5.406003 -5.0726976 -5.148437 -4.7227077 -4.20889 -3.8581197 -3.5045838 -2.7137322 -2.01124 -2.5317206 -3.4733562 -4.8767867 -5.2600937 -6.4237185 -7.3674326][-5.68005 -5.1962051 -4.8888378 -4.097508 -3.145915 -2.3889871 -1.550992 -0.95974016 -0.43853426 -1.0812988 -2.3406396 -3.8755813 -4.5275416 -5.7533813 -6.5604162][-5.2914648 -4.4220228 -3.7945688 -2.9595671 -1.9799304 -0.74200964 0.31864643 0.83503914 1.2729092 0.50668049 -0.85979652 -2.4364028 -3.3460822 -4.6615095 -5.3123016][-3.9581304 -3.3927526 -2.9377789 -1.6946378 -0.73662472 0.38175678 1.6032047 1.8315067 1.8164425 0.89837646 -0.18968105 -1.9193377 -2.7792606 -4.1213808 -5.031579][-3.9051821 -2.6988397 -1.5901785 -0.34439135 0.62896633 1.4793453 2.3280106 2.2663412 1.903204 0.68490124 -0.57281733 -1.9203329 -2.7947607 -4.3603277 -5.0158033][-2.1264386 -1.3092217 -0.54600668 0.76402855 1.8584356 2.3028889 2.5398827 2.2983484 1.8981972 0.4647646 -1.0000463 -2.0688796 -3.0854869 -4.605176 -5.3794289][-1.5147057 -0.69175482 0.094136238 1.0044584 1.5395088 1.7753487 1.7458954 1.4861164 1.0836811 0.16297293 -0.73348427 -1.7995272 -2.6127753 -4.0524397 -4.9481516][-1.651485 -1.0320625 -0.43175364 0.15260172 0.3997488 0.64236164 0.89436626 0.4124403 -0.036631584 -0.97127438 -1.9893837 -2.3871679 -2.9157352 -4.2070827 -4.7358923][-3.637032 -3.3349943 -2.6369381 -2.2389607 -2.0353904 -1.7978482 -1.8195429 -1.9082608 -2.0713944 -3.0978928 -3.95955 -4.12881 -4.7093382 -4.995616 -5.23629][-4.9174309 -4.7444682 -4.8506417 -4.4356647 -4.0276117 -3.7978427 -3.9079115 -3.9071703 -3.9330797 -4.3887095 -5.0228243 -5.2661324 -5.1804686 -5.6298428 -5.8904309][-5.8658476 -5.9025879 -5.921916 -6.0140047 -6.2458153 -5.9441776 -5.32549 -5.1366005 -4.7195807 -4.7956314 -5.2151318 -4.9316721 -5.2513742 -5.434618 -5.2381907][-4.8768959 -5.252594 -5.5430555 -5.828196 -5.736516 -5.5667534 -5.2864647 -5.0012007 -4.9071288 -4.9679418 -5.061234 -4.7301831 -4.7150965 -4.7858434 -5.0289044][-5.5313883 -5.6879225 -5.7638931 -5.7997041 -5.8543305 -5.7836342 -5.5995588 -5.7808442 -5.7259645 -5.792747 -6.1828084 -5.9701266 -5.5561523 -5.293828 -4.8310132]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 19:41:35.743960: step 52510, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 51h:21m:17s remains)
INFO - root - 2017-12-15 19:41:42.071205: step 52520, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 49h:09m:47s remains)
INFO - root - 2017-12-15 19:41:48.509603: step 52530, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 50h:21m:58s remains)
INFO - root - 2017-12-15 19:41:55.090120: step 52540, loss = 0.28, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 52h:06m:35s remains)
INFO - root - 2017-12-15 19:42:01.555828: step 52550, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 50h:23m:28s remains)
INFO - root - 2017-12-15 19:42:07.964927: step 52560, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:20m:44s remains)
INFO - root - 2017-12-15 19:42:14.392481: step 52570, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 51h:00m:33s remains)
INFO - root - 2017-12-15 19:42:20.769376: step 52580, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 48h:24m:53s remains)
INFO - root - 2017-12-15 19:42:27.068596: step 52590, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 50h:05m:46s remains)
INFO - root - 2017-12-15 19:42:33.462986: step 52600, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 49h:15m:21s remains)
2017-12-15 19:42:33.975591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7261286 -5.1098304 -4.6337805 -4.0694532 -3.7008712 -3.4559474 -3.0506134 -3.0893407 -3.008306 -3.8589275 -5.2703977 -5.4635491 -5.6625175 -6.3075943 -6.6197667][-4.7605152 -4.2809267 -3.8593552 -3.9151757 -3.6724892 -3.301374 -3.1910543 -3.3591886 -3.0658388 -3.6603684 -4.4839373 -4.9437723 -5.5499945 -5.5402865 -5.6115179][-4.9499245 -4.2185631 -3.7397134 -3.8851335 -3.5138149 -3.4405127 -3.4701719 -3.823952 -3.7228239 -4.1044025 -4.6911774 -4.804565 -5.5569258 -5.6262026 -5.7032108][-6.2806425 -5.1627483 -4.0341005 -3.53052 -2.986969 -2.7177086 -2.4588614 -3.0333085 -3.2564688 -3.8559804 -4.6473942 -5.1612597 -5.653954 -5.9195261 -6.0448604][-6.8241372 -5.4404616 -4.8690052 -3.6477437 -2.525682 -1.7589979 -1.5000277 -1.7959762 -1.9165263 -2.9658608 -3.8964474 -4.6013408 -5.7473326 -6.4035068 -6.7219181][-5.89544 -4.4244461 -3.8513603 -3.086834 -1.9159002 -0.77424 -0.035387516 -0.10126257 -0.11002874 -1.21346 -2.3352919 -3.0968308 -4.6050663 -5.846293 -6.8170338][-4.060338 -2.8305106 -1.8376822 -1.0048466 -0.65744305 0.428545 1.2786341 1.5204802 1.5172358 0.56159019 -0.42083979 -1.4603243 -3.2242556 -4.8369217 -6.1237774][-2.9726906 -1.8903036 -1.0290313 -0.14995337 0.76409817 1.51122 2.2069206 2.513938 2.6266088 1.4809685 0.20370674 -0.76405764 -2.1118221 -3.8564167 -5.1664791][-2.5727305 -1.4068828 -0.72482347 -0.17101097 0.84840775 1.9087448 2.7160978 2.8310385 2.6023054 1.7071066 0.38312054 -0.75871277 -2.0334988 -3.2732496 -4.6252308][-3.1657305 -2.2557316 -0.83999395 -0.21679163 0.65852451 1.7842255 2.9881458 2.9017906 2.4770718 1.37675 0.068336487 -0.80389071 -1.7567401 -2.4788809 -3.5431218][-3.4177265 -2.9119296 -2.5438395 -1.7631087 -0.51327229 0.54376316 1.8638639 2.0209818 1.9448509 0.95423126 -0.18589735 -0.861114 -1.4432859 -2.2312427 -3.2271156][-3.0962043 -2.9221969 -2.9583621 -2.7470646 -1.9287958 -0.88134 0.56322 1.0181885 1.3857899 0.73633289 0.074178696 -0.71705151 -1.5756483 -2.2324824 -3.4696732][-3.6430297 -3.6524277 -3.5939493 -3.2599239 -3.094214 -2.4937778 -1.4235926 -0.99825287 -0.71483278 -0.65684652 -0.87137985 -1.4908495 -2.2090988 -3.0709143 -3.8358462][-2.818758 -3.6822052 -3.7718804 -3.7674243 -3.8668458 -3.397377 -2.9047475 -2.8896837 -2.6307821 -2.4447494 -2.5663319 -2.5585694 -2.8101139 -3.5130153 -3.951813][-3.200037 -3.4566946 -3.8465102 -4.7337179 -5.0276022 -4.8138022 -4.3505087 -4.3138475 -4.4272509 -4.423224 -4.1932383 -4.2482481 -4.4661055 -4.7345057 -5.1598873]]...]
INFO - root - 2017-12-15 19:42:40.480479: step 52610, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 50h:02m:56s remains)
INFO - root - 2017-12-15 19:42:46.932572: step 52620, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.675 sec/batch; 52h:28m:00s remains)
INFO - root - 2017-12-15 19:42:53.423768: step 52630, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 51h:27m:16s remains)
INFO - root - 2017-12-15 19:42:59.844731: step 52640, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 50h:15m:56s remains)
INFO - root - 2017-12-15 19:43:06.332482: step 52650, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:44m:47s remains)
INFO - root - 2017-12-15 19:43:12.858801: step 52660, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 49h:52m:49s remains)
INFO - root - 2017-12-15 19:43:19.266083: step 52670, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 48h:53m:59s remains)
INFO - root - 2017-12-15 19:43:25.679986: step 52680, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 50h:06m:56s remains)
INFO - root - 2017-12-15 19:43:32.031984: step 52690, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:58m:45s remains)
INFO - root - 2017-12-15 19:43:38.538534: step 52700, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 50h:05m:43s remains)
2017-12-15 19:43:39.086498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9431829 -4.8464308 -5.657917 -6.4089451 -6.4430075 -6.5018659 -6.8132277 -6.9831004 -6.5286589 -7.4552813 -6.1102743 -7.3784251 -8.3887177 -7.7439542 -5.694993][-3.2330723 -3.3898897 -3.5723944 -4.4270096 -4.5157928 -4.8568187 -5.727694 -6.1026182 -6.3599339 -7.602756 -6.2023807 -7.2616444 -7.5777168 -6.2829943 -4.7184362][-0.72528076 -1.5846877 -2.4910741 -3.4753571 -3.1186442 -2.721909 -3.7432823 -4.41004 -5.293251 -6.7982659 -5.4770169 -6.6746635 -7.6628857 -7.4155054 -6.2757459][-3.1451311 -2.9846549 -3.2834926 -3.6939244 -3.4649811 -3.3601465 -3.4441037 -3.3891687 -4.2093258 -5.8812923 -5.2003136 -6.9887118 -8.1588326 -8.116704 -7.9241486][-3.9915917 -3.8045039 -4.0621386 -3.1115127 -1.9797363 -1.1884937 -1.2894783 -1.2445712 -1.5419822 -2.9948778 -2.6692028 -4.4939337 -6.5064182 -7.4153953 -7.4716859][-5.8043318 -4.8329587 -5.0728512 -3.699388 -2.1983485 -1.5381312 -0.040661335 0.82751274 0.829648 0.2362752 0.44480419 -1.3015347 -3.1565518 -5.0010705 -6.0302787][-5.5459104 -4.8958158 -4.5379052 -2.9949493 -1.3470325 0.35980415 2.1832457 2.298152 2.4132595 1.8127851 2.2651968 1.1474695 -1.5382409 -3.0548778 -3.8134897][-4.6383419 -4.0882874 -3.3056259 -1.6735306 0.62595749 2.8663273 4.5388079 4.7300911 4.9655342 2.7918253 2.4955378 1.8123531 0.20168304 -1.5465202 -2.7405877][-4.9026413 -4.6511965 -4.032589 -2.1946406 -0.26582146 1.9895153 3.507925 5.1057158 6.0525618 3.8211594 2.9924698 0.45710659 -2.08537 -3.0016971 -4.0317287][-6.6984229 -6.3410711 -5.5106821 -4.007412 -2.3267927 -0.59381819 1.1527901 2.921689 3.8253136 2.0589657 1.8066711 -0.39814138 -3.1871991 -4.6924095 -5.6580896][-8.3954277 -7.9525414 -7.4550409 -5.8611512 -4.45786 -2.6094794 -1.1258969 -0.17432356 1.1603251 0.39292526 0.4686327 -1.6988125 -4.3731389 -5.47313 -6.9062638][-8.7231331 -8.2275648 -8.2502956 -7.7322025 -6.7660561 -5.334857 -4.3851142 -3.381597 -2.0854678 -2.7073097 -1.8521466 -2.50458 -4.3888097 -5.33381 -6.5491204][-8.8594627 -8.3191452 -9.0500927 -8.936244 -7.6977134 -7.1314397 -6.5035629 -5.5461164 -4.9011908 -4.7740574 -4.3502951 -4.5824184 -4.8313227 -5.3597031 -6.4109011][-9.0026035 -8.7700357 -8.823966 -8.5664587 -8.3304777 -7.7547832 -7.4193244 -7.2797956 -6.9348526 -6.914969 -6.2833776 -6.3146806 -6.6653552 -5.7846556 -5.238853][-9.3266621 -9.3622093 -9.5248442 -9.0881615 -8.1190662 -8.2309742 -8.2634964 -8.1377907 -8.0590353 -7.9239955 -7.5384603 -7.5534582 -6.961555 -6.6417737 -6.09643]]...]
INFO - root - 2017-12-15 19:43:45.530221: step 52710, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 49h:04m:38s remains)
INFO - root - 2017-12-15 19:43:51.914112: step 52720, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 48h:30m:11s remains)
INFO - root - 2017-12-15 19:43:58.398483: step 52730, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:30m:43s remains)
INFO - root - 2017-12-15 19:44:04.870013: step 52740, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 48h:57m:53s remains)
INFO - root - 2017-12-15 19:44:11.258258: step 52750, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 48h:26m:47s remains)
INFO - root - 2017-12-15 19:44:17.685591: step 52760, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:00m:24s remains)
INFO - root - 2017-12-15 19:44:24.172322: step 52770, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 50h:34m:11s remains)
INFO - root - 2017-12-15 19:44:30.581989: step 52780, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 48h:29m:55s remains)
INFO - root - 2017-12-15 19:44:36.939026: step 52790, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 49h:53m:12s remains)
INFO - root - 2017-12-15 19:44:43.406522: step 52800, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:24m:10s remains)
2017-12-15 19:44:43.902155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5389872 -3.9672093 -4.7275238 -4.9027376 -4.8216534 -3.9708612 -2.7114716 -2.0211768 -1.7998366 -2.6116633 -4.993371 -7.2178259 -8.9315786 -10.14552 -10.602592][-2.5726485 -3.2235937 -4.4126663 -5.580864 -5.8229594 -5.17588 -4.2724667 -3.2777619 -2.550343 -2.9644341 -5.1551218 -7.2485952 -9.1947279 -10.085655 -10.030888][-1.9314928 -2.590807 -3.6140223 -4.3712969 -4.6633105 -4.6545582 -3.9337821 -3.355628 -2.9087267 -3.2940397 -5.4142632 -6.902348 -8.4374561 -9.0516386 -9.1803665][-2.2665243 -2.2250371 -2.8861847 -3.4593282 -3.0149779 -2.428421 -1.745513 -1.1799226 -0.587049 -1.562799 -4.1152592 -6.5143576 -8.6263247 -8.768486 -8.5727482][-2.2378855 -1.8731132 -2.0056114 -1.9234586 -1.6627579 -0.54007435 1.1582966 1.6623077 1.6204348 0.31982803 -2.8158712 -5.6750431 -7.714426 -8.2418365 -8.6465473][-2.2101531 -2.3073711 -1.8470945 -1.172081 -0.31803226 1.4777288 3.1800938 3.6044054 3.8473587 1.9141207 -1.4704003 -4.1814294 -6.8756251 -7.7524529 -7.6685019][-2.8549852 -2.4512162 -1.5904665 -0.74207592 0.56158066 2.5000563 4.2053041 5.1552086 5.1011562 3.2047853 -0.44183493 -3.9622061 -6.40565 -7.544558 -7.9237614][-2.6036191 -1.9903784 -1.6428523 -0.31865215 1.210474 2.8075943 3.9620743 5.0050297 5.3698196 3.5959177 0.0066223145 -3.237123 -6.1293521 -7.198657 -7.2727766][-2.8525772 -1.9718742 -1.463459 -1.0116091 0.2100668 1.7753677 3.1631594 4.3827925 4.8354168 3.2363768 -0.29807806 -3.5929384 -6.0857458 -6.9480081 -7.3883152][-4.3088551 -3.8973191 -3.02562 -2.1540303 -1.19488 -0.3096261 0.54167366 1.1857824 1.4899902 0.26267576 -2.3689189 -4.7409596 -6.8455076 -7.592062 -7.8415956][-6.42517 -5.7904439 -5.0347757 -4.009789 -3.2369647 -2.6778941 -2.0873528 -1.964273 -2.2682643 -3.3963223 -5.9733758 -7.6313944 -8.3889809 -8.7305164 -8.5156221][-8.169096 -7.5019412 -6.5532422 -5.9857173 -5.5283804 -5.2143173 -5.0395193 -5.0316968 -5.0311327 -5.7099476 -7.4327936 -8.1120958 -8.7209339 -9.4571218 -9.4056635][-8.3773861 -8.4283295 -7.6407647 -6.8753381 -6.72026 -6.7899 -6.6276054 -6.4751272 -6.0783052 -5.9728727 -6.6499658 -7.1610236 -8.0861149 -8.5519485 -8.3416729][-8.0653276 -8.435463 -8.1932516 -7.5573764 -7.0680671 -6.902328 -6.8318543 -6.5542884 -5.9357991 -5.9740915 -6.3628902 -5.9120164 -5.805068 -6.0930023 -6.09536][-7.7222018 -8.0305862 -8.151865 -8.4177608 -8.5063705 -8.400322 -8.3124552 -7.7579827 -7.4429135 -7.205616 -6.8533149 -6.3378878 -5.7846055 -5.9215894 -6.2849212]]...]
INFO - root - 2017-12-15 19:44:50.308318: step 52810, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 47h:42m:15s remains)
INFO - root - 2017-12-15 19:44:56.717322: step 52820, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 48h:52m:36s remains)
INFO - root - 2017-12-15 19:45:03.187935: step 52830, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 50h:17m:51s remains)
INFO - root - 2017-12-15 19:45:09.635824: step 52840, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 48h:43m:17s remains)
INFO - root - 2017-12-15 19:45:16.055734: step 52850, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 50h:46m:50s remains)
INFO - root - 2017-12-15 19:45:22.397765: step 52860, loss = 0.23, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 49h:16m:00s remains)
INFO - root - 2017-12-15 19:45:28.818154: step 52870, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 50h:11m:04s remains)
INFO - root - 2017-12-15 19:45:35.135265: step 52880, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 49h:59m:48s remains)
INFO - root - 2017-12-15 19:45:41.588825: step 52890, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 50h:47m:13s remains)
INFO - root - 2017-12-15 19:45:48.080389: step 52900, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 51h:33m:13s remains)
2017-12-15 19:45:48.594491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1910233 -2.6728115 -2.4534774 -2.6978607 -2.7378058 -2.4457827 -1.9325519 -1.9354601 -2.0536532 -3.9395278 -4.9013472 -5.9525352 -6.8085804 -7.1552362 -8.3957357][-3.2194958 -2.8467989 -2.4494333 -2.2622976 -2.2083454 -2.2796974 -1.8804054 -1.5429893 -1.4907579 -3.5280614 -4.3250065 -5.4220939 -6.2792826 -6.341269 -7.0967646][-4.7019758 -4.2618594 -3.5771751 -2.9091444 -2.380837 -2.2168794 -1.8681903 -1.5161061 -1.6914907 -3.8345788 -4.5105739 -5.5244336 -6.4163027 -6.7650561 -7.3718572][-5.3287582 -4.5443869 -4.1094522 -2.8696752 -1.7078142 -1.3041148 -0.91453409 -0.52201509 -0.62252045 -2.9191632 -4.1195478 -5.4393549 -6.2651443 -6.670691 -7.3130069][-5.9215012 -3.9476924 -2.4803395 -1.6475058 -1.2462816 -0.13138962 0.64626503 0.72386265 0.58409119 -1.5924969 -2.9315553 -4.5180664 -5.7018428 -6.3600826 -6.99466][-5.6832061 -3.9426029 -3.0614295 -1.2538943 0.40349483 1.1922083 2.0020971 1.9786243 1.3529053 -0.84674168 -1.9178348 -3.5905943 -5.0497208 -5.8197823 -6.8643012][-5.2068176 -3.5063047 -1.8227544 -0.033018589 1.1142807 2.3613024 3.4435797 3.0903664 2.5389624 0.014256001 -1.1739421 -2.5037069 -3.8959811 -4.9526615 -6.1576524][-3.8238993 -2.1606994 -1.0233493 0.54969406 2.5525589 3.4586716 3.9248533 3.9125776 3.6455994 0.902648 -0.34116411 -1.7514429 -3.1679697 -4.1800122 -5.15948][-3.7474146 -2.1126189 -0.067291737 1.2677488 1.9176559 2.3850422 2.9624977 2.8488121 2.6732483 0.58848381 -0.48615885 -1.8596287 -3.2939286 -4.414175 -5.3057084][-4.9945068 -3.6201587 -2.3151169 -0.39450312 1.5413857 1.9431629 1.4860382 1.421279 1.2600918 -0.72847843 -1.6493731 -2.8127856 -3.9907515 -4.4136848 -5.0902634][-5.8679113 -5.4829745 -4.8669367 -2.8529272 -1.3451695 -0.61597347 -0.43173218 -0.51030445 -0.46907187 -2.3148994 -3.455215 -4.0692568 -4.5568972 -5.1240807 -5.7212429][-8.8664255 -7.2963204 -6.2521982 -5.4012609 -3.8956013 -3.0260105 -2.7545877 -2.73098 -2.9037189 -4.1081982 -4.7932181 -5.1554203 -5.4956493 -5.5932837 -5.9904747][-9.9569731 -9.0700264 -7.9582825 -6.6721239 -5.6923642 -5.0862684 -4.1841598 -3.9384747 -4.0864935 -5.1823831 -5.7186985 -5.5303574 -5.5674925 -5.4657879 -5.7150679][-9.5028467 -9.38363 -8.3725605 -7.3653655 -6.2826872 -5.8109565 -5.5134039 -5.2789316 -5.0579681 -5.3417354 -5.4543238 -5.4689708 -5.7333727 -5.78098 -5.7703137][-8.532897 -8.1600542 -7.734942 -7.5866642 -7.0510445 -6.6810493 -6.3420019 -6.403698 -6.2970409 -6.166297 -6.4690866 -6.1623163 -5.8934278 -6.2319098 -6.16379]]...]
INFO - root - 2017-12-15 19:45:54.961843: step 52910, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 49h:27m:37s remains)
INFO - root - 2017-12-15 19:46:01.321393: step 52920, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 48h:48m:51s remains)
INFO - root - 2017-12-15 19:46:07.790797: step 52930, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 50h:10m:04s remains)
INFO - root - 2017-12-15 19:46:14.235257: step 52940, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 49h:20m:08s remains)
INFO - root - 2017-12-15 19:46:20.593240: step 52950, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 49h:12m:40s remains)
INFO - root - 2017-12-15 19:46:27.004716: step 52960, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 50h:44m:58s remains)
INFO - root - 2017-12-15 19:46:33.419764: step 52970, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 50h:26m:52s remains)
INFO - root - 2017-12-15 19:46:39.859588: step 52980, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 50h:11m:34s remains)
INFO - root - 2017-12-15 19:46:46.191309: step 52990, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 48h:59m:57s remains)
INFO - root - 2017-12-15 19:46:52.583978: step 53000, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 52h:12m:56s remains)
2017-12-15 19:46:53.185578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9103556 -5.4686232 -4.5727797 -3.9836538 -3.6001911 -3.1773458 -2.9101844 -2.7606344 -2.2728744 -3.573463 -4.8564568 -5.6716852 -6.9532356 -7.1540279 -7.8672934][-5.1520162 -4.8666668 -4.2935586 -3.6946931 -3.1178603 -2.9276376 -3.0226998 -3.1860518 -2.9660029 -4.5492 -5.4844389 -5.6966124 -6.7838264 -7.1125307 -7.8125949][-4.9307046 -4.5411158 -3.9445641 -3.4322119 -3.0397725 -2.6549044 -2.338253 -2.4429817 -2.4414024 -4.1355872 -5.3390579 -5.7297897 -6.9516177 -7.0633841 -7.6394572][-4.0111265 -3.5591044 -3.2517419 -2.7638264 -2.3129721 -2.1830144 -1.8757162 -1.8747907 -1.7173429 -3.3745399 -4.5199385 -5.0311947 -6.3851871 -6.5232162 -7.1240005][-3.7933116 -2.6173868 -1.5819659 -1.4538488 -1.5483193 -1.3178778 -0.73060465 -0.74978971 -0.83805943 -2.4784608 -3.3690557 -3.9044082 -5.4581175 -5.8684921 -6.4315691][-3.0500216 -2.2586555 -1.6708574 -1.0007777 -0.14848423 0.082407475 0.028996468 -0.042543888 0.12071419 -1.5690117 -2.7562146 -3.328373 -4.7226324 -5.1452708 -6.0640168][-2.5302186 -2.1382108 -1.1724248 -0.5260582 0.066100121 0.65305328 1.2145081 1.1479769 1.0172977 -0.77150965 -2.0024 -2.5835633 -4.0757322 -4.6319532 -5.5214987][-2.0579586 -1.2158709 -0.76710415 -0.074873447 1.0311651 1.6568499 1.7462702 1.9696474 2.3248959 0.35526657 -1.111227 -1.9859209 -3.8127029 -4.5845823 -5.4698467][-2.1106787 -1.5166278 -0.30213213 0.83337593 1.2301455 1.5247898 1.8789129 1.9542379 2.0831528 0.079998016 -1.3551083 -2.2505636 -4.0244575 -4.8155284 -5.8736172][-2.8235474 -1.9573865 -1.2095342 -0.14927578 0.68283081 1.2582264 1.1739006 1.0328856 1.1521769 -1.0513549 -2.4122453 -3.5268731 -5.14095 -5.6605167 -6.4236851][-3.5666485 -2.9826503 -2.6286201 -1.4891615 -0.76444149 -0.43673134 -0.39454937 -0.57335711 -0.71635437 -2.5785851 -3.9038138 -4.7774963 -6.1415281 -6.6576538 -7.1361203][-4.5780616 -3.8414588 -3.0929017 -2.2973242 -1.4629207 -0.95833254 -0.94049311 -1.443131 -1.8543196 -3.5301285 -4.6675882 -5.3623886 -6.7228575 -7.1866145 -7.311954][-4.4172053 -4.3350506 -4.7537832 -4.1307364 -3.2475772 -2.6954422 -2.3113046 -2.7549171 -3.2127361 -4.2901616 -5.3696709 -5.6600742 -6.3043203 -6.467464 -6.4576445][-4.8786039 -4.366951 -4.5350909 -4.6244082 -4.9315119 -4.4019375 -3.9453051 -4.2706008 -4.3713946 -4.9050841 -5.1333675 -5.49609 -5.9366407 -6.1201024 -5.8990955][-5.6855617 -5.4508171 -5.1682997 -5.5384097 -5.8704023 -5.43044 -5.3324952 -5.4438863 -5.6556 -5.4498205 -5.273509 -5.6583796 -5.4776888 -5.4316206 -5.2376728]]...]
INFO - root - 2017-12-15 19:46:59.678307: step 53010, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 49h:33m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:47:06.136063: step 53020, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 49h:48m:00s remains)
INFO - root - 2017-12-15 19:47:12.494270: step 53030, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 48h:44m:04s remains)
INFO - root - 2017-12-15 19:47:18.982371: step 53040, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:43m:11s remains)
INFO - root - 2017-12-15 19:47:25.388578: step 53050, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 51h:11m:40s remains)
INFO - root - 2017-12-15 19:47:31.847174: step 53060, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 49h:47m:23s remains)
INFO - root - 2017-12-15 19:47:38.137803: step 53070, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 49h:39m:17s remains)
INFO - root - 2017-12-15 19:47:44.526768: step 53080, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 49h:46m:57s remains)
INFO - root - 2017-12-15 19:47:50.923055: step 53090, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 50h:05m:18s remains)
INFO - root - 2017-12-15 19:47:57.289307: step 53100, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 49h:34m:15s remains)
2017-12-15 19:47:57.767658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8346062 -5.9241257 -6.1474147 -5.9109612 -5.1884422 -4.2022009 -2.4729724 -0.81613064 0.64151478 0.607172 -0.67934513 -3.7679751 -6.14815 -8.4316587 -9.481988][-5.0506868 -5.4289193 -6.1000528 -6.319108 -5.9925318 -4.7607231 -2.9004321 -0.83139324 0.55911064 0.53994274 -0.88674784 -3.5895534 -5.5503473 -8.1603985 -9.5986471][-4.7013941 -5.104538 -5.2384539 -4.8888764 -4.3267756 -3.3612971 -2.1322246 -0.83180141 0.51357555 0.22831678 -0.95708942 -3.6153188 -5.2042637 -6.9339924 -8.2978115][-4.5449281 -4.3189573 -4.1868191 -3.7947934 -3.2873006 -2.3529606 -1.1205411 -0.21974993 0.72925472 0.39952278 -0.75271177 -3.3786054 -4.8291903 -6.4619818 -7.2694254][-4.8107 -4.1894975 -3.523807 -2.75779 -1.9596443 -0.68716812 0.53655815 1.2668762 1.6684942 0.86996746 -0.56201458 -3.2194624 -4.8622065 -6.3213768 -7.2234144][-4.6580544 -3.7872338 -2.6805592 -1.6963878 -0.51680565 0.77930737 2.3211498 2.8610811 3.2028627 2.0656252 0.39691734 -2.6366053 -4.6370535 -6.2020941 -7.1819477][-4.2408295 -3.0102382 -2.0145969 -0.92696142 0.35577297 1.4384937 2.8269739 3.6212978 4.1773624 2.8852406 1.1312456 -2.279573 -4.3444519 -6.20973 -7.1732697][-3.7037086 -2.3796387 -1.1190205 -0.13301134 1.0364285 1.7252846 2.4789639 3.3755035 4.069953 3.095048 1.2774 -1.9731288 -4.3051934 -6.258729 -7.2625875][-3.1115232 -1.8284855 -0.61496687 0.47898388 1.6745634 1.9970913 2.4905596 2.9300203 3.2812357 2.1681376 0.80177116 -2.0177422 -4.2866735 -6.4103842 -7.7535849][-2.6568708 -1.6947618 -0.30963945 0.58831596 1.5042629 1.9317179 2.5481873 2.7565517 2.9449987 1.195322 -0.64306641 -3.1914868 -4.9279985 -6.5518513 -7.5885587][-2.5226016 -1.8429904 -0.85148811 0.12890911 1.1249228 1.3021774 1.6869059 1.8522139 1.9930372 0.22867584 -1.8777957 -4.1616354 -5.3425417 -6.7206712 -7.5973291][-3.3878899 -2.3118153 -1.1185746 -0.23375082 0.40671253 0.41831017 0.39959621 0.12631369 -0.33272362 -1.2063165 -2.3993435 -4.3079081 -5.7239223 -6.9901414 -7.6966538][-4.6723452 -3.6074543 -2.1492729 -1.2012453 -0.36448097 -0.36781073 -0.89763832 -1.2275009 -1.5294337 -2.427866 -3.2620282 -4.6744518 -5.8094673 -6.7201061 -7.497385][-6.2898355 -5.8543749 -4.1097965 -3.2855449 -2.1180177 -1.5014071 -1.476346 -1.5807624 -2.2160525 -3.022038 -4.2023549 -5.3376236 -6.0433564 -6.6261587 -6.63404][-7.5582457 -6.8900027 -5.9282231 -5.4136114 -4.4959583 -3.5868196 -3.1511469 -3.0229959 -3.6973598 -4.3915019 -5.4097 -6.293644 -6.467545 -6.56354 -6.5773349]]...]
INFO - root - 2017-12-15 19:48:04.138626: step 53110, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 49h:22m:49s remains)
INFO - root - 2017-12-15 19:48:10.642093: step 53120, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 49h:56m:24s remains)
INFO - root - 2017-12-15 19:48:17.055027: step 53130, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:18m:55s remains)
INFO - root - 2017-12-15 19:48:23.597953: step 53140, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 51h:12m:16s remains)
INFO - root - 2017-12-15 19:48:29.964815: step 53150, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 47h:46m:41s remains)
INFO - root - 2017-12-15 19:48:36.355088: step 53160, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.617 sec/batch; 47h:52m:26s remains)
INFO - root - 2017-12-15 19:48:42.756521: step 53170, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 48h:59m:41s remains)
INFO - root - 2017-12-15 19:48:49.134301: step 53180, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 50h:34m:21s remains)
INFO - root - 2017-12-15 19:48:55.547188: step 53190, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:51m:52s remains)
INFO - root - 2017-12-15 19:49:01.954523: step 53200, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 49h:16m:34s remains)
2017-12-15 19:49:02.485478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6570001 -1.4375176 -1.5282183 -1.8492479 -2.1487451 -2.6561532 -2.7673106 -2.9910617 -3.5792141 -5.2275848 -5.3012776 -5.8019929 -6.5117512 -7.4386344 -8.1455727][-1.9300499 -1.5917382 -1.6272469 -1.6368847 -1.8735185 -2.1929903 -2.729876 -3.1990409 -3.3412304 -4.7842178 -5.1073327 -5.68118 -6.4606528 -7.6917911 -8.4722223][-2.7659998 -2.2879 -2.0750313 -1.7294745 -1.4625664 -1.4419832 -1.355072 -1.5670013 -2.0209913 -3.6560941 -3.8602805 -4.7897654 -5.8841858 -6.67261 -7.2209396][-3.6294913 -2.6131063 -2.38347 -1.9241586 -1.2805285 -1.0246425 -0.72567749 -0.69025469 -0.85309458 -2.1929622 -2.3592391 -3.3016429 -4.4878073 -5.4160175 -6.3558216][-4.0124955 -3.0113406 -2.2041221 -1.5414357 -1.1145854 -0.56189203 0.086028576 0.18018436 0.3908577 -0.82247972 -0.79109955 -1.8333535 -3.4926629 -4.6148791 -5.76087][-4.0609531 -2.8580832 -2.1823382 -1.5942636 -0.79357862 -0.00042629242 0.71515656 0.61533356 0.88913918 -0.26276255 -0.30712128 -1.24507 -2.784276 -4.178936 -5.1156282][-3.9970036 -3.1843495 -2.3145323 -1.1536894 -0.58641291 0.21501589 1.1443701 0.95208549 0.86883926 -0.75497055 -0.715436 -1.5303421 -3.0163536 -3.9945035 -4.9487944][-3.3441219 -2.5015264 -1.9364915 -1.1067939 -0.32164717 0.33600807 1.0432806 1.3547764 1.767231 -0.373446 -1.0086346 -2.2509422 -3.4639864 -4.6001539 -5.532506][-3.5192046 -2.6322589 -2.053503 -1.2518296 -0.55734444 0.03180933 0.810462 0.82318878 0.96603775 -0.73663473 -1.1331263 -2.72513 -4.0604696 -5.2421818 -6.0298719][-4.0462914 -3.3662128 -2.698544 -2.1571918 -1.6424766 -0.88550234 -0.19580364 -0.304626 -0.39527655 -2.1040468 -2.714417 -3.8571625 -5.2822695 -6.46025 -7.2156134][-5.4547305 -4.8407 -4.576571 -4.0938077 -3.4210024 -2.9077754 -2.1421766 -2.1132236 -2.2634406 -3.7458577 -4.2661738 -5.3422337 -6.6370511 -7.7040486 -7.8918748][-7.0251975 -6.2028556 -5.8379941 -5.6040611 -5.3848782 -4.6259069 -3.6363482 -3.6204886 -3.4522491 -4.6699543 -5.3264332 -5.6067009 -6.5890484 -7.6137013 -7.9460897][-7.666707 -7.1985979 -7.0549693 -6.5434179 -5.8177614 -5.4196105 -5.0277052 -5.0045462 -5.0705633 -5.45236 -5.5875568 -5.6802769 -6.1614628 -6.7185526 -6.9251008][-7.044004 -6.8451834 -6.9007864 -6.7779331 -6.6367593 -5.7595625 -5.0714288 -5.2660379 -5.3538437 -5.8780584 -5.9071908 -5.9478774 -6.3654261 -6.2986617 -6.2951622][-8.0527363 -7.3952913 -6.7910938 -6.7275772 -6.7146 -6.639842 -6.2956877 -6.2208977 -6.15005 -5.9804955 -6.0170488 -6.1032457 -6.13801 -6.4023113 -6.4680533]]...]
INFO - root - 2017-12-15 19:49:09.028713: step 53210, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 51h:34m:16s remains)
INFO - root - 2017-12-15 19:49:15.412523: step 53220, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 50h:49m:10s remains)
INFO - root - 2017-12-15 19:49:21.833376: step 53230, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 50h:45m:09s remains)
INFO - root - 2017-12-15 19:49:28.320625: step 53240, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 49h:55m:06s remains)
INFO - root - 2017-12-15 19:49:34.764516: step 53250, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.633 sec/batch; 49h:05m:06s remains)
INFO - root - 2017-12-15 19:49:41.162168: step 53260, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 48h:59m:35s remains)
INFO - root - 2017-12-15 19:49:47.524135: step 53270, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 49h:00m:43s remains)
INFO - root - 2017-12-15 19:49:53.897731: step 53280, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 48h:59m:25s remains)
INFO - root - 2017-12-15 19:50:00.266804: step 53290, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 48h:36m:11s remains)
INFO - root - 2017-12-15 19:50:06.629848: step 53300, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 49h:27m:47s remains)
2017-12-15 19:50:07.171061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.751617 -4.514071 -4.3506556 -5.1086922 -5.5193887 -5.3156986 -5.2567892 -4.9249096 -4.7720709 -5.6928425 -6.4817705 -6.7119226 -6.7982674 -8.0640259 -8.8224878][-5.0498152 -5.805603 -6.2012944 -5.598712 -5.5343156 -5.8063684 -6.3421164 -5.72593 -5.094449 -6.2452173 -6.5161653 -6.6623178 -6.6676779 -7.2904892 -7.5778937][-6.7018619 -5.5961065 -5.122344 -5.5110579 -5.2829108 -4.523406 -4.1268759 -4.1127748 -3.578578 -4.5554247 -5.1308355 -5.6573362 -5.53407 -5.9604864 -6.6393995][-6.2100968 -5.3437319 -4.644146 -3.11692 -2.1718721 -2.1077247 -1.6239958 -1.3207912 -0.67619371 -1.7852044 -3.1381092 -4.0579948 -4.6308956 -5.6697893 -6.6115904][-6.7771664 -4.408618 -3.2749681 -2.3800941 -0.97429276 -0.12866116 1.0857515 1.1866045 1.4372072 0.40561295 -1.0549979 -2.5296855 -3.402647 -4.5828795 -5.8841062][-5.1727762 -4.3285761 -3.1936765 -1.3334336 0.26335907 1.301671 2.6054134 3.2191792 3.4894114 2.0475273 0.59900951 -1.3736997 -2.5054874 -3.7813566 -4.9566083][-5.0054159 -3.5871902 -2.0547228 -0.93852139 0.10668182 1.7889175 3.5829306 3.8738041 3.984374 2.7735405 1.4429054 -0.64682961 -1.8264203 -3.5541406 -5.0478239][-4.1419821 -3.4210248 -2.6298757 -1.2619004 0.018237591 1.0901699 2.329587 3.3743219 4.119441 2.5961208 0.975441 -1.1800303 -2.3508711 -3.6716857 -4.8705153][-4.013011 -3.3072791 -2.8950849 -2.045506 -0.92490005 0.55960083 2.1971951 2.3680363 2.7398643 1.9845076 0.83925533 -1.3971782 -2.6942015 -4.3596592 -5.5399733][-5.064497 -4.2732897 -3.2744231 -2.4653292 -1.799355 -0.69943333 0.98100281 1.8481579 1.9381247 0.68443775 -0.4982028 -1.6905007 -2.3501797 -3.8658204 -5.4291644][-4.907033 -4.514533 -3.8214529 -3.2515664 -2.7875519 -2.0018249 -0.87123156 -0.39780092 -0.43471766 -1.050415 -2.3095508 -3.5613594 -4.3851504 -5.0237675 -5.8517704][-6.704195 -6.1424947 -5.3383303 -4.8602023 -4.0391092 -3.0164375 -1.9378424 -1.5131445 -1.6626248 -2.0569487 -3.3306417 -4.0210924 -4.5490856 -5.4443264 -5.9599352][-6.8971 -7.6609697 -7.0007849 -6.07968 -5.8587251 -4.3730631 -3.308394 -3.0242839 -2.8849082 -3.0063896 -3.6428356 -4.023406 -4.5360942 -5.0180912 -5.4147377][-5.7220726 -5.7641134 -5.5835085 -5.8259687 -6.1991844 -5.3697596 -4.5828209 -3.9018712 -3.3228664 -3.4437981 -4.1160097 -4.1355457 -4.0571489 -4.5327215 -4.8412085][-6.03467 -5.3329659 -5.1739411 -5.5666862 -6.0232444 -5.5418205 -5.1721058 -4.7730665 -4.5661974 -4.4372635 -4.2523489 -4.690568 -5.1349282 -4.97318 -4.7059813]]...]
INFO - root - 2017-12-15 19:50:13.624371: step 53310, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 48h:40m:54s remains)
INFO - root - 2017-12-15 19:50:20.024301: step 53320, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 48h:55m:56s remains)
INFO - root - 2017-12-15 19:50:26.314521: step 53330, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 49h:24m:24s remains)
INFO - root - 2017-12-15 19:50:32.727481: step 53340, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.640 sec/batch; 49h:36m:44s remains)
INFO - root - 2017-12-15 19:50:39.138618: step 53350, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 48h:03m:48s remains)
INFO - root - 2017-12-15 19:50:45.490006: step 53360, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 48h:00m:29s remains)
INFO - root - 2017-12-15 19:50:51.839109: step 53370, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 50h:11m:18s remains)
INFO - root - 2017-12-15 19:50:58.199797: step 53380, loss = 0.30, batch loss = 0.18 (13.0 examples/sec; 0.614 sec/batch; 47h:35m:11s remains)
INFO - root - 2017-12-15 19:51:04.584059: step 53390, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 50h:01m:35s remains)
INFO - root - 2017-12-15 19:51:10.919968: step 53400, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 49h:58m:13s remains)
2017-12-15 19:51:11.470672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.77193 -5.9846334 -6.1918087 -6.6567574 -6.5755177 -6.1086388 -5.631052 -4.5647821 -3.7693834 -4.2914143 -5.2282162 -6.1375575 -6.3348179 -6.6576204 -7.3534102][-5.3628745 -6.1284685 -6.5438418 -7.0391855 -6.8360863 -6.5019507 -6.2303877 -5.9616284 -5.58429 -5.8211989 -6.4372663 -6.7672634 -7.1700697 -7.07637 -7.3647275][-6.6941915 -6.3307338 -5.9198194 -5.4622631 -5.1681261 -5.1745391 -4.9710922 -4.9753113 -4.7199135 -5.7036991 -6.8984365 -7.6062908 -7.8617678 -7.75379 -7.4282236][-6.1558061 -5.3793745 -4.7414179 -3.3973923 -2.3024292 -1.526619 -1.1511307 -1.7985134 -2.1015673 -3.5520029 -5.41568 -6.5747004 -7.3527522 -7.5338607 -7.596252][-6.330514 -4.9342413 -3.4288645 -1.8250771 -0.4461751 0.49532509 1.3384876 0.85749722 0.52151489 -1.2547994 -3.334106 -4.8181229 -6.2144918 -6.94116 -7.3947811][-4.9500732 -3.7541938 -2.4092674 -0.5135088 1.2062664 2.6840658 3.401432 2.7021093 2.1111259 0.19477463 -1.5154266 -3.0738554 -4.3531561 -5.286777 -6.0417824][-4.8368082 -3.2879543 -1.6772628 0.43252659 2.3221769 4.05021 4.9880447 4.7946033 4.3052549 2.0073118 -0.18943405 -1.8554707 -3.5035968 -4.5358634 -5.29699][-4.411653 -3.3879261 -2.0191813 0.37389851 2.2435827 3.7399426 4.725872 4.8264 4.86331 3.0523586 0.65934849 -1.5406275 -3.1962533 -3.9894159 -4.6740489][-4.5920687 -3.3449116 -2.0299067 -0.562984 0.56011772 2.097146 3.1293116 3.4309177 3.486392 1.5697794 -0.54118061 -2.1316953 -3.5174441 -4.37341 -5.0692425][-6.1461725 -4.6214304 -3.5324521 -2.1185765 -0.72284746 0.3120079 0.71743107 1.1327486 1.261241 -0.53748989 -2.150795 -3.0592937 -3.6949039 -4.1152773 -4.5557876][-8.4281769 -7.3411818 -6.3378305 -4.9394197 -3.7856138 -2.6940866 -2.0205526 -1.9578347 -2.2224064 -3.4387021 -4.5752239 -5.2945905 -5.6195426 -5.7855973 -5.782197][-9.27111 -8.78279 -7.4290719 -6.1204038 -5.4877405 -4.9338918 -4.6061883 -4.7507882 -4.4607129 -4.9140968 -5.6161566 -5.8639197 -6.02601 -6.0865192 -5.638617][-9.13389 -9.023881 -8.5360346 -7.5662627 -6.6595368 -6.0289216 -5.8296041 -5.8825188 -5.741899 -5.9285007 -6.5161386 -6.1535516 -5.8470373 -5.8254671 -5.212286][-8.5643606 -8.1243887 -7.4316926 -6.968257 -6.4475322 -5.8590293 -5.6348624 -5.7168226 -5.9662056 -5.8475637 -6.0044079 -5.6636686 -5.3275709 -4.7493572 -4.5014715][-7.7503772 -7.2749224 -6.7890129 -6.6848345 -6.4000978 -6.0602932 -6.0108795 -6.0709753 -6.1980162 -6.4131174 -6.6149197 -6.3615584 -6.0532565 -5.5149727 -5.0168958]]...]
INFO - root - 2017-12-15 19:51:17.886445: step 53410, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 50h:09m:33s remains)
INFO - root - 2017-12-15 19:51:24.268338: step 53420, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.624 sec/batch; 48h:20m:30s remains)
INFO - root - 2017-12-15 19:51:30.703889: step 53430, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 48h:40m:13s remains)
INFO - root - 2017-12-15 19:51:37.116845: step 53440, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 50h:35m:02s remains)
INFO - root - 2017-12-15 19:51:43.439839: step 53450, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 49h:21m:57s remains)
INFO - root - 2017-12-15 19:51:49.838580: step 53460, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 49h:25m:17s remains)
INFO - root - 2017-12-15 19:51:56.173249: step 53470, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 48h:39m:59s remains)
INFO - root - 2017-12-15 19:52:02.543625: step 53480, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.612 sec/batch; 47h:26m:54s remains)
INFO - root - 2017-12-15 19:52:08.949576: step 53490, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 48h:58m:53s remains)
INFO - root - 2017-12-15 19:52:15.328688: step 53500, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:51m:04s remains)
2017-12-15 19:52:15.862447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3655195 -1.9963484 -1.7840157 -1.3881879 -1.0668702 -0.75093746 -0.36129713 -0.3121891 -0.27829456 -1.5767298 -2.0683866 -3.778734 -4.5157156 -5.9325738 -6.9961872][-2.91403 -2.4858427 -2.1807928 -1.8828468 -1.6864777 -1.4157186 -1.301877 -1.4097314 -1.2174382 -2.7012548 -3.1700516 -4.9631281 -5.7451282 -6.6776881 -7.3376164][-4.1009707 -3.3020163 -3.0873952 -2.5943613 -2.2397199 -1.6709404 -1.3212128 -1.7134013 -1.7965722 -3.0574808 -3.4729743 -5.1402245 -5.6486397 -6.6011209 -7.1074281][-4.847559 -4.1439781 -3.9763682 -2.9655447 -2.400547 -1.8973856 -1.518044 -1.6700935 -1.714922 -3.176301 -3.6438189 -5.2616439 -5.9594259 -6.7647233 -7.1979766][-5.0262461 -4.2094879 -3.4913316 -2.4505095 -2.0441828 -1.0958037 -0.673759 -1.0126543 -1.0614071 -2.6654639 -3.1407089 -4.6894159 -5.4545393 -6.247468 -6.8600659][-4.7701664 -4.0040188 -3.3431144 -1.7770109 -0.67928696 -0.085783482 -0.1629324 0.0025734901 0.20003414 -1.2581954 -1.8549871 -3.572835 -4.4621797 -5.7243309 -6.4227824][-4.7614422 -4.0016432 -3.3453655 -1.945446 -1.0902524 0.17137432 1.1234598 1.0273628 0.89186954 -0.2132616 -0.45391369 -2.3970037 -3.6891055 -4.9463305 -6.0923343][-4.8392611 -3.8206172 -2.93821 -2.0852022 -1.273509 -0.0022478104 0.50378227 0.91249943 1.4287901 0.15596294 -0.49706078 -2.1537275 -3.0707197 -4.5201173 -5.6516371][-4.0274086 -3.914279 -3.3494816 -1.7214684 -0.416996 0.24922752 0.33032322 0.50104237 0.47305965 -0.36891603 -0.56365538 -2.4405265 -3.5356145 -4.7316332 -5.3272243][-3.8558643 -3.2482715 -2.6806636 -1.8415046 -0.8802247 0.30987644 0.89377689 0.68504429 0.48378849 -0.59312296 -0.96380186 -2.9126587 -3.8278289 -5.3658595 -6.5997658][-3.2868447 -3.3322206 -3.2334113 -2.2826471 -1.5242548 -1.2522988 -1.4318037 -0.8810997 -0.41236496 -1.6519775 -2.3096833 -3.5047131 -4.3981872 -5.7557125 -6.4135981][-3.3890386 -3.7702794 -4.0027108 -3.8242071 -3.4407687 -2.9205956 -2.7357717 -2.7996035 -2.7146502 -2.6062527 -2.5223026 -4.1081409 -5.1484728 -6.0814781 -6.77047][-3.8405182 -4.1274028 -4.6526113 -4.8618059 -4.813076 -4.9416122 -4.8553343 -4.3116279 -3.8362868 -3.8978875 -3.7630897 -4.5690126 -5.55651 -6.4189167 -7.10101][-4.3429356 -4.6119537 -5.0294828 -5.0957718 -5.5171614 -5.7847195 -5.7685685 -5.3230619 -4.6902943 -4.4239845 -4.5277872 -4.9514465 -5.1875706 -6.0909772 -7.0445323][-7.0858655 -6.9022942 -6.8879647 -7.0424194 -7.1094327 -7.3726087 -7.7663231 -7.600142 -6.9828148 -6.2295966 -5.811327 -5.7515664 -6.177536 -6.8824387 -7.2493453]]...]
INFO - root - 2017-12-15 19:52:22.284388: step 53510, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 49h:19m:07s remains)
INFO - root - 2017-12-15 19:52:28.649238: step 53520, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 48h:16m:37s remains)
INFO - root - 2017-12-15 19:52:35.031037: step 53530, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 49h:34m:04s remains)
INFO - root - 2017-12-15 19:52:41.388119: step 53540, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 50h:27m:22s remains)
INFO - root - 2017-12-15 19:52:47.742359: step 53550, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 49h:56m:55s remains)
INFO - root - 2017-12-15 19:52:54.074189: step 53560, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 48h:51m:14s remains)
INFO - root - 2017-12-15 19:53:00.418213: step 53570, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 48h:57m:41s remains)
INFO - root - 2017-12-15 19:53:06.951749: step 53580, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 49h:39m:41s remains)
INFO - root - 2017-12-15 19:53:13.342700: step 53590, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 48h:53m:19s remains)
INFO - root - 2017-12-15 19:53:19.713565: step 53600, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 50h:08m:14s remains)
2017-12-15 19:53:20.270140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4864054 -1.509326 0.02561903 -0.11593866 -0.12785959 -0.3659277 -1.4221997 -2.8041806 -3.8649013 -6.5579853 -7.9954391 -9.6805334 -10.613809 -10.568395 -10.582249][-3.042666 -1.8705635 -0.88095808 -0.52153587 -0.72453308 -1.3444471 -1.7337513 -3.1048536 -4.3644724 -6.2981992 -6.7999673 -8.4661646 -9.4981279 -9.5920219 -9.7799406][-2.6874852 -1.5709558 -0.46908951 -0.205307 0.075381279 -0.28120708 -0.69342375 -1.3288546 -2.2211413 -4.745945 -5.4067469 -6.5757771 -7.0789857 -7.6119313 -8.3456306][-3.3514247 -2.0442433 -0.61122322 0.37542629 0.61363983 1.3816452 1.4809141 0.3849926 -0.524683 -2.8187604 -3.9425554 -5.16192 -6.050087 -6.8094482 -7.4365549][-2.7150927 -1.9570451 -0.6929245 0.12529802 0.61658859 1.465766 2.5204563 2.0313082 1.1082373 -0.94577646 -1.6178679 -2.6993065 -3.746877 -4.6593018 -5.8676682][-2.9703507 -2.3455625 -1.0918026 -0.074718952 0.80326271 2.01441 3.4034386 3.8829269 3.7752314 1.1707344 -0.50346804 -1.5187092 -1.8131652 -2.8525987 -4.1810765][-3.5882578 -2.9126453 -2.0509658 -0.30490494 1.0552464 2.2637844 3.5740643 3.7156239 3.6823397 1.7053356 0.15472984 -0.92603493 -1.7561054 -2.5389833 -3.4477777][-4.1065483 -2.7923675 -1.6381612 -0.60292387 0.57003021 1.9086246 3.0965891 3.3318119 3.5081768 1.6298962 0.1487608 -0.67869806 -1.4143896 -2.0574994 -3.437079][-2.9788289 -2.3766966 -1.8223062 -1.2247295 -0.42027521 0.51187611 1.5515461 2.0398216 2.9072313 1.4379349 0.4648428 -0.77810812 -1.6120601 -2.5858216 -3.8577907][-2.8042159 -1.824584 -1.2074466 -1.4075818 -1.1183681 -0.31987762 0.523633 0.84722805 1.6262856 0.27666998 -0.49095869 -1.4270554 -2.2872472 -3.1517758 -4.5955405][-3.5968604 -2.6050711 -1.7863116 -1.6514616 -0.77272844 -0.46225691 -0.25102043 -0.23548794 0.43561172 -1.093298 -2.7445197 -2.6970792 -2.5609622 -3.7973866 -4.8422489][-3.0358796 -2.0531211 -1.1288252 -1.3710485 -1.3005147 -0.68074417 -0.60937548 -1.75987 -1.8818445 -2.2908139 -3.1264811 -4.8429356 -5.8563137 -5.7662907 -6.4757562][-3.3616824 -2.1546855 -1.3130007 -1.4479132 -1.6166973 -1.1174393 -1.3875198 -1.5373888 -1.1957617 -3.1433439 -4.0954447 -4.413383 -5.7438312 -7.0254703 -8.09177][-4.0328016 -2.9997 -2.1172509 -1.5822206 -1.1266804 -1.093657 -1.8944392 -1.8705683 -1.4628358 -2.5445371 -3.4488592 -4.2240868 -5.5424004 -6.4730549 -7.4406652][-6.6714797 -5.0728521 -3.4846425 -2.8790665 -2.6630554 -2.2971339 -2.9863582 -3.25837 -3.3595533 -3.3297863 -3.5222983 -4.4580865 -5.30834 -6.2524447 -6.9999394]]...]
INFO - root - 2017-12-15 19:53:26.710068: step 53610, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 49h:46m:02s remains)
INFO - root - 2017-12-15 19:53:33.179513: step 53620, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 50h:53m:16s remains)
INFO - root - 2017-12-15 19:53:39.586449: step 53630, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 49h:17m:56s remains)
INFO - root - 2017-12-15 19:53:45.913721: step 53640, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 48h:40m:58s remains)
INFO - root - 2017-12-15 19:53:52.343723: step 53650, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 50h:28m:49s remains)
INFO - root - 2017-12-15 19:53:58.776021: step 53660, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:52m:35s remains)
INFO - root - 2017-12-15 19:54:05.226122: step 53670, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 49h:25m:06s remains)
INFO - root - 2017-12-15 19:54:11.633511: step 53680, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 48h:59m:58s remains)
INFO - root - 2017-12-15 19:54:18.013185: step 53690, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 50h:54m:04s remains)
INFO - root - 2017-12-15 19:54:24.411696: step 53700, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 48h:52m:23s remains)
2017-12-15 19:54:24.921142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1861811 -3.2137208 -3.13371 -2.6714921 -2.3704276 -2.4118428 -2.989419 -2.9689217 -2.6059675 -2.4177284 -2.888289 -3.4419875 -5.4518051 -6.2585936 -6.852428][-3.0930605 -3.3099446 -3.0229988 -2.98636 -2.9489441 -2.8310752 -3.839843 -4.1963158 -3.7748754 -3.7467623 -3.7263906 -4.0493751 -6.3370886 -7.1933002 -7.2258315][-2.964519 -3.1045756 -3.0167913 -2.9560566 -2.75774 -2.327014 -2.2797179 -3.0729847 -3.5816011 -3.9831314 -4.1828828 -4.2099237 -6.096386 -7.4009056 -8.0579147][-3.8551831 -2.9050384 -2.2817569 -2.3118329 -1.9041152 -1.6879954 -1.3072605 -1.393043 -1.9610763 -2.521235 -3.4700308 -3.9472818 -5.8751879 -7.1395245 -7.903017][-3.5315185 -3.2028985 -2.24606 -1.5763803 -0.60685825 -0.082230091 0.19225216 -0.24353838 -0.875577 -1.6787248 -2.6733632 -3.3427753 -5.5919704 -6.743928 -7.5984831][-4.1889219 -3.5841446 -2.9230065 -1.4738407 -0.11349297 0.083496571 0.35972214 0.14322472 -0.23432589 -0.8277359 -1.8411913 -2.4971719 -4.663816 -5.9740214 -6.7170534][-4.562727 -3.9054573 -2.8814011 -1.4612193 0.53346729 1.3194818 1.7197123 1.228302 0.81312656 0.15601587 -0.88503551 -1.2984385 -3.2398729 -4.2197323 -4.6486483][-3.8462157 -3.3897405 -3.1507487 -1.8127761 0.21791172 1.7804117 3.0310907 3.1406422 2.8414097 1.5760965 0.18387794 -0.74937582 -2.906909 -3.5156879 -3.9254267][-4.4122696 -4.0459194 -3.7167425 -2.3861775 -0.72522736 0.80186176 1.9191494 2.264039 2.3768311 1.5068216 0.433156 -0.59446 -2.861445 -3.7156124 -4.1507134][-6.3980331 -5.2410474 -4.2851934 -3.5881343 -2.0856133 -0.65697289 0.66231632 1.2899189 1.451417 0.49139118 -0.58464861 -1.3146768 -2.9524927 -3.7752538 -4.4489689][-7.1913066 -6.923708 -6.1474261 -5.332613 -3.9460478 -2.4381709 -1.2030292 -0.44097567 -0.020846367 -0.7375102 -1.7926788 -2.4292655 -4.0550766 -4.6347113 -5.1696386][-7.2666473 -6.9509134 -6.8193464 -6.3675518 -5.00159 -3.6592922 -2.7456326 -2.5897541 -2.5404873 -3.1104732 -3.3377385 -4.3523121 -5.3296442 -5.4413242 -5.9261222][-9.071311 -7.9203253 -7.0793085 -6.8365164 -6.6055937 -5.5348644 -4.1773314 -4.169239 -4.0368872 -4.8464651 -4.7838554 -5.0802402 -5.3176956 -5.4236708 -5.5661097][-8.5506268 -8.0191622 -7.6692758 -7.1064978 -6.062305 -5.257103 -4.6194258 -4.8710546 -4.9122782 -5.2083206 -4.8605123 -5.129632 -5.4905906 -5.5387664 -5.397202][-8.6455984 -8.3497429 -8.275054 -7.9513745 -6.8623672 -6.0092974 -5.3396645 -5.1081772 -5.2475872 -5.7070484 -5.867197 -5.83459 -5.3423491 -5.204217 -5.2070413]]...]
INFO - root - 2017-12-15 19:54:31.346966: step 53710, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 49h:44m:28s remains)
INFO - root - 2017-12-15 19:54:37.680667: step 53720, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 50h:30m:44s remains)
INFO - root - 2017-12-15 19:54:44.146848: step 53730, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 49h:36m:40s remains)
INFO - root - 2017-12-15 19:54:50.507780: step 53740, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 50h:09m:35s remains)
INFO - root - 2017-12-15 19:54:56.822666: step 53750, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 49h:29m:57s remains)
INFO - root - 2017-12-15 19:55:03.262366: step 53760, loss = 0.33, batch loss = 0.22 (11.9 examples/sec; 0.670 sec/batch; 51h:52m:22s remains)
INFO - root - 2017-12-15 19:55:09.739383: step 53770, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 50h:32m:32s remains)
INFO - root - 2017-12-15 19:55:16.251319: step 53780, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 49h:28m:47s remains)
INFO - root - 2017-12-15 19:55:22.702083: step 53790, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 49h:56m:33s remains)
INFO - root - 2017-12-15 19:55:29.239753: step 53800, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 49h:17m:49s remains)
2017-12-15 19:55:29.760857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7819762 -5.6710796 -5.568078 -5.352951 -4.5186076 -3.3305521 -2.09163 -1.3185267 -1.0525837 -2.3328071 -3.7411273 -5.5244164 -7.0589657 -7.6766334 -8.4073887][-5.4192591 -5.7045927 -5.740798 -5.6247015 -4.7486019 -3.3907385 -2.24619 -1.4536247 -1.1397023 -2.8173957 -4.40661 -6.116087 -7.67806 -7.88395 -8.83713][-5.9309044 -6.5197191 -6.609087 -6.2443914 -5.1910849 -3.9294724 -2.1126671 -1.2396288 -1.2396746 -3.1553268 -4.843462 -6.5171127 -7.4333067 -7.6834121 -8.760293][-6.1536913 -6.7578254 -7.0778108 -6.3039579 -4.7387056 -2.7970724 -1.0337892 0.074363232 0.65341568 -1.3669605 -3.6320624 -6.1137214 -7.3554988 -7.3328118 -7.9310961][-7.1219993 -7.35132 -6.77197 -5.7505312 -3.9884789 -1.5822392 0.76511574 1.7017994 1.4208231 -0.82542467 -2.8289161 -4.9238195 -6.4846878 -7.0997028 -8.0107737][-8.8015213 -8.5195389 -7.6511364 -5.7454472 -2.6555457 0.23900652 2.5695238 3.399847 3.2612352 0.17993259 -2.9279218 -5.3809094 -6.896965 -7.3206568 -8.3841228][-9.2078657 -8.4277706 -6.9035759 -5.036727 -2.3894873 1.2542715 4.2027626 5.1315718 4.3615732 0.87004566 -2.0536394 -4.9104171 -6.8762903 -7.6693645 -8.8168316][-8.2020464 -7.4786739 -6.4567184 -4.261898 -1.0928183 2.1752472 4.7667179 5.9164743 6.1514645 2.4651556 -1.4938436 -4.657908 -7.0298381 -7.6848083 -8.3172321][-7.3731041 -6.2091126 -5.1916237 -3.36696 -0.95196104 1.9023066 4.3037004 5.1887712 5.0821924 1.5409641 -1.8063793 -4.8322372 -6.9834461 -7.6713572 -8.8784533][-7.1503553 -6.4127245 -5.7546816 -3.5632291 -1.2804685 1.1921473 2.9108982 3.1915646 2.6462431 -0.87941217 -3.6394916 -6.5456519 -8.4969025 -8.2811785 -8.5297241][-6.6359673 -6.2813597 -5.6004076 -3.9096875 -2.0890732 -0.12553596 1.1653843 0.77951336 -0.55330372 -3.7626483 -6.1786995 -7.5235782 -8.3256779 -8.1293 -8.5797834][-5.9522357 -5.0363445 -3.6019802 -1.7411895 -0.36135912 0.47267056 1.1108932 -0.14420319 -1.4893785 -4.2890649 -5.8626423 -6.8058457 -7.9148955 -7.3676472 -7.6661673][-7.1751723 -5.3909397 -3.3329549 -1.0559053 0.13505793 0.63549137 0.80251312 -0.2621851 -1.2561569 -4.2887659 -6.0736346 -6.2357383 -6.7183838 -6.5231438 -6.89325][-7.8964128 -6.5000167 -4.5935936 -2.2912083 -0.90699196 -0.7270031 -1.3493824 -1.7900419 -2.0405936 -3.2040567 -4.0608664 -5.4011011 -5.755774 -5.8220773 -6.1540031][-8.2157478 -6.7512207 -5.7805705 -3.8271959 -2.5916824 -2.6189914 -2.4978037 -3.3962579 -4.3123627 -4.5953045 -4.784543 -5.1249948 -5.2269297 -4.9528565 -5.1048408]]...]
INFO - root - 2017-12-15 19:55:36.281301: step 53810, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 49h:34m:37s remains)
INFO - root - 2017-12-15 19:55:42.680888: step 53820, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 50h:22m:25s remains)
INFO - root - 2017-12-15 19:55:49.150184: step 53830, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 49h:09m:57s remains)
INFO - root - 2017-12-15 19:55:55.596394: step 53840, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 50h:36m:22s remains)
INFO - root - 2017-12-15 19:56:01.980710: step 53850, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 48h:29m:24s remains)
INFO - root - 2017-12-15 19:56:08.476458: step 53860, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 51h:06m:34s remains)
INFO - root - 2017-12-15 19:56:14.827755: step 53870, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 49h:36m:48s remains)
INFO - root - 2017-12-15 19:56:21.178206: step 53880, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 49h:40m:06s remains)
INFO - root - 2017-12-15 19:56:27.628656: step 53890, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.637 sec/batch; 49h:20m:08s remains)
INFO - root - 2017-12-15 19:56:34.090148: step 53900, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 50h:51m:20s remains)
2017-12-15 19:56:34.646087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6751404 -4.432909 -4.6166515 -4.8356247 -5.07474 -5.3623714 -5.3837209 -4.9610763 -4.227993 -5.0168734 -5.6091146 -6.1192279 -7.2276082 -8.26701 -9.0283213][-3.4058585 -3.7738593 -4.5867748 -5.5872717 -6.1308565 -6.1865234 -5.9598093 -5.1613755 -4.2738881 -5.5168934 -6.0639529 -6.7191124 -7.9912944 -9.1740627 -10.290624][-2.3154597 -2.4201403 -2.692368 -3.6111264 -3.9658616 -3.9462028 -3.8073232 -3.3305702 -2.9919415 -4.1918945 -5.1800671 -6.094183 -7.1166706 -8.0651379 -8.789608][-2.1222715 -2.4315577 -2.5409646 -2.6360459 -2.709549 -2.1860948 -1.6953278 -1.3246284 -0.81161785 -2.2817907 -3.4978056 -4.6598206 -6.4205456 -7.4828253 -8.3141756][-2.217123 -2.1698575 -1.6350784 -1.517951 -0.68414068 0.20806313 0.88233757 1.2140408 1.1430817 -0.70931149 -2.0713506 -3.379046 -5.132246 -6.4866872 -7.4684057][-3.3441906 -2.8662906 -1.8467226 -0.78870487 0.53387928 2.0072632 3.4328566 3.9398079 3.842206 1.5009298 -0.47283936 -2.1215248 -3.9320395 -5.18227 -6.0061703][-3.9515848 -3.5380731 -2.4869733 -0.61441088 1.3949642 2.8767042 4.0123806 4.4383221 4.4465132 1.8327026 -0.61482334 -2.681118 -4.7788172 -5.7630959 -6.5402532][-4.7476277 -3.9039063 -2.592515 -0.4310956 1.8558121 3.3350868 4.2207184 4.326292 4.006485 1.3557453 -0.979249 -3.2049785 -5.1549749 -6.4744987 -7.4225378][-4.8952131 -3.9447715 -2.74509 -0.80034447 1.3447094 2.872551 3.698966 3.807271 3.5608406 0.65251064 -1.8751011 -3.9508677 -5.8343692 -6.9582176 -7.6705694][-5.7271166 -4.7063427 -3.50492 -1.8213944 -0.020602703 1.0843239 1.794075 1.9020071 1.6014423 -1.092484 -3.34375 -5.3634062 -6.7853236 -7.5671062 -8.30699][-6.6967421 -5.6270871 -4.420804 -2.7368031 -1.4940977 -0.72455883 -0.36513281 -0.6666441 -0.806478 -3.1243682 -5.0983467 -6.7213788 -7.7840323 -8.4350281 -8.9172878][-6.8743367 -5.8882408 -4.87215 -3.7865796 -3.1000624 -2.2193837 -2.3382797 -2.558424 -2.71462 -4.446897 -5.4907427 -6.6404705 -7.4521847 -8.1454372 -8.614049][-6.8040466 -5.7253513 -4.7870636 -4.1183367 -3.7668359 -3.7112789 -3.69477 -4.3040075 -4.6562018 -5.7380786 -6.5600467 -7.0757766 -7.7012753 -7.8078151 -7.8639297][-6.3740153 -5.280911 -4.5864515 -4.2149172 -4.2966909 -4.3306551 -4.6906252 -5.1667786 -5.4674549 -6.4206824 -6.3919787 -6.4388618 -6.6873775 -6.73265 -6.866168][-6.6848373 -5.7419629 -4.99669 -4.7757874 -5.0298128 -5.3538547 -5.9700317 -6.6543303 -7.222446 -7.2554083 -7.055728 -6.8924475 -6.5516553 -6.3259158 -6.1539016]]...]
INFO - root - 2017-12-15 19:56:41.054386: step 53910, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 48h:08m:11s remains)
INFO - root - 2017-12-15 19:56:47.400916: step 53920, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 48h:34m:16s remains)
INFO - root - 2017-12-15 19:56:53.757278: step 53930, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:11m:19s remains)
INFO - root - 2017-12-15 19:57:00.228038: step 53940, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 49h:25m:30s remains)
INFO - root - 2017-12-15 19:57:06.677254: step 53950, loss = 0.27, batch loss = 0.15 (11.6 examples/sec; 0.691 sec/batch; 53h:28m:03s remains)
INFO - root - 2017-12-15 19:57:13.250766: step 53960, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 50h:57m:08s remains)
INFO - root - 2017-12-15 19:57:19.745705: step 53970, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:07m:26s remains)
INFO - root - 2017-12-15 19:57:26.192708: step 53980, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 50h:39m:29s remains)
INFO - root - 2017-12-15 19:57:32.660061: step 53990, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 51h:20m:58s remains)
INFO - root - 2017-12-15 19:57:39.091283: step 54000, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 50h:00m:23s remains)
2017-12-15 19:57:39.604714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3929825 -4.5980806 -4.9066033 -5.0216837 -4.3989029 -3.2374477 -2.3640351 -1.4366241 -0.98540211 -1.6797295 -2.4203148 -5.0680361 -5.604598 -7.0886784 -7.5724297][-4.1594739 -4.6616015 -5.1920953 -5.7574148 -5.4517059 -4.2803426 -3.386168 -2.6723452 -2.1868281 -2.6666026 -3.6514902 -6.1084833 -6.263526 -7.8856635 -8.2753487][-3.5491786 -3.7056823 -3.9967208 -4.28244 -4.4805069 -3.6832843 -2.4044132 -1.8249135 -1.9964581 -1.9631715 -2.8854618 -5.3664351 -5.835794 -7.2159128 -7.4730043][-3.9691768 -4.2905645 -4.2785692 -4.2246389 -3.923342 -2.9431648 -2.2427912 -1.1958055 -1.1766863 -1.9047332 -2.5559669 -4.7536736 -4.7218437 -6.2908797 -6.8537655][-4.804832 -5.0017781 -5.3581333 -4.8551626 -3.7759669 -2.1614656 -0.894958 0.19592381 0.50842285 -0.39030313 -1.9661846 -4.4522753 -4.4393883 -5.8064909 -6.0612016][-5.666235 -5.5886321 -5.5243826 -4.8302164 -3.5411172 -1.1254363 0.61740589 1.6350832 1.5359154 0.290236 -1.3855247 -4.3382998 -4.8615046 -5.7895546 -6.1485014][-6.5998955 -5.9458346 -5.3576756 -4.62341 -3.1244335 0.0054392815 2.5626345 3.914505 3.8198786 2.2270641 -0.24266386 -3.8575094 -4.8644056 -5.97092 -6.2210445][-6.4127183 -5.9649267 -5.0728717 -3.7426817 -2.4684157 0.082075596 2.7488155 3.809865 3.41471 1.7664995 -0.17521667 -3.4698935 -5.1428843 -6.525176 -6.58959][-5.4723186 -4.7258248 -3.9595654 -2.6841474 -1.1218004 0.90579987 2.833643 3.8177662 3.8118486 2.6008959 0.77575684 -2.7312837 -4.1071062 -5.8393459 -6.6599178][-4.2839475 -3.7447324 -3.2940378 -2.3429227 -1.0550046 0.68760872 1.8848667 2.4617767 2.4825583 1.0611668 -0.6930809 -3.8255732 -4.9786358 -6.4721007 -6.8597288][-4.7126169 -4.1451745 -3.2409754 -2.3109093 -1.6505928 -0.38208485 0.28659964 0.497303 0.25699091 -1.1800799 -2.4837542 -5.0994415 -5.8750448 -6.8010893 -6.8059492][-3.7397003 -3.539176 -2.6774626 -1.6383858 -1.1401296 -0.48173237 -0.509418 -0.85432005 -1.7100539 -2.78126 -3.6404181 -5.4804316 -6.3387604 -7.7905693 -7.4031773][-4.6362352 -3.5481415 -2.5539269 -1.8757081 -1.530283 -0.86747503 -1.244648 -1.6528044 -2.6162333 -4.1438723 -4.6856666 -5.6080055 -5.5147429 -7.0329885 -7.2062917][-3.9177237 -3.6957934 -2.3623466 -1.339417 -0.96255827 -0.71852636 -0.94992971 -1.4595137 -2.6292286 -4.2736864 -4.4353075 -5.0883389 -5.1434431 -6.3175917 -6.7665453][-4.7225475 -4.1902075 -3.1240535 -2.8117909 -2.1882334 -2.6088858 -3.2684298 -3.6446438 -4.111732 -4.9137292 -5.3305645 -5.3130045 -5.7274723 -6.1655631 -6.4508257]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 19:57:46.235684: step 54010, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 50h:32m:57s remains)
INFO - root - 2017-12-15 19:57:52.744589: step 54020, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 50h:58m:55s remains)
INFO - root - 2017-12-15 19:57:59.227022: step 54030, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 49h:30m:57s remains)
INFO - root - 2017-12-15 19:58:05.636680: step 54040, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 48h:54m:30s remains)
INFO - root - 2017-12-15 19:58:12.091038: step 54050, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 49h:17m:32s remains)
INFO - root - 2017-12-15 19:58:18.490149: step 54060, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 50h:26m:36s remains)
INFO - root - 2017-12-15 19:58:24.945129: step 54070, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 49h:52m:16s remains)
INFO - root - 2017-12-15 19:58:31.362382: step 54080, loss = 0.38, batch loss = 0.26 (12.5 examples/sec; 0.639 sec/batch; 49h:27m:05s remains)
INFO - root - 2017-12-15 19:58:37.827460: step 54090, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 51h:08m:47s remains)
INFO - root - 2017-12-15 19:58:44.196305: step 54100, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.638 sec/batch; 49h:19m:55s remains)
2017-12-15 19:58:44.683659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8776708 -2.3635888 -2.1091905 -1.9253554 -1.753861 -1.742744 -2.0544081 -2.2121553 -2.2577715 -4.0031338 -4.9189825 -5.9746361 -6.3700829 -7.011899 -7.31419][-2.7350659 -2.4362311 -2.3301234 -2.1730509 -1.981976 -1.8354564 -1.8612733 -1.923883 -2.0374665 -3.7854166 -4.8871317 -5.975193 -6.4043794 -7.0880947 -7.5735164][-3.2432508 -3.031075 -2.98414 -2.6098433 -2.2207303 -1.6043153 -1.1207399 -1.1773024 -1.0873046 -2.5332232 -3.440619 -4.499774 -5.0190382 -5.8360634 -6.6470909][-3.5292687 -3.3103371 -3.0880089 -2.4942112 -1.8523984 -1.0151715 -0.39125395 -0.32172394 -0.2379899 -1.754632 -2.6613007 -3.7655284 -4.5959773 -5.585866 -6.1302748][-4.133173 -3.6319852 -2.7977061 -2.003109 -1.2150459 -0.34350348 0.26707935 0.4501667 0.5082159 -1.1219649 -2.0410767 -3.1619568 -4.1635165 -5.3344803 -6.0889606][-4.122612 -3.4140706 -2.627749 -1.5093651 -0.4644928 0.38804626 0.96666145 1.1694336 1.2401419 -0.4806962 -1.6644793 -3.0889325 -4.1664753 -5.4381857 -6.2306538][-3.8367374 -2.9108782 -1.8005538 -0.78095293 0.13130522 0.94021988 1.3865719 1.5985231 1.7600794 -0.082001209 -1.4891124 -3.1024323 -4.1678681 -5.3420639 -6.0584135][-3.4640875 -2.1968684 -0.93278694 0.31693935 1.2608175 1.9653559 2.1302519 2.0375977 1.9546928 -0.1289835 -1.7931871 -3.5209413 -4.6118803 -5.7085333 -5.9389257][-3.0195637 -1.744175 -0.42245007 0.59727287 1.3343172 1.7813988 1.6282454 1.7010832 1.7058821 -0.45225334 -2.0421348 -3.9931211 -5.1262684 -6.213623 -6.4385791][-2.3533998 -1.0377293 -0.154459 0.54952431 1.167037 1.3625708 1.539217 1.3403702 0.89962006 -1.3858738 -3.0106883 -4.759614 -5.7342029 -6.5167503 -6.6872663][-3.2035079 -2.2710261 -1.4861259 -0.72599125 -0.31750011 -0.3481245 -0.42058372 -0.61353493 -0.96683884 -3.1379824 -4.54702 -5.7746649 -6.3023682 -6.7596917 -6.7699866][-4.5893526 -3.8865035 -3.0731068 -2.2761168 -1.8996072 -1.9869599 -2.0807757 -2.2655349 -2.5976105 -3.9819055 -5.1198225 -6.032733 -6.2489724 -6.4648814 -6.4235158][-5.6638832 -5.3452888 -4.9942203 -4.54852 -4.2390113 -3.9118702 -3.9109578 -3.9959233 -3.9859335 -4.7260108 -5.4707441 -5.810791 -5.932961 -5.9039612 -5.532012][-5.5636292 -5.4519014 -5.2141628 -4.817668 -4.651196 -4.3313713 -4.3923035 -4.3827562 -4.356153 -4.9127712 -4.9967251 -5.0276856 -5.0736833 -5.140985 -5.2278624][-7.0386295 -6.5190539 -5.83805 -5.6550436 -5.55528 -5.6875286 -5.854022 -5.8828835 -5.9417286 -5.7517252 -5.6069546 -5.4562769 -5.3936996 -5.4760542 -5.6034536]]...]
INFO - root - 2017-12-15 19:58:51.095540: step 54110, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 50h:34m:37s remains)
INFO - root - 2017-12-15 19:58:57.543046: step 54120, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 50h:29m:34s remains)
INFO - root - 2017-12-15 19:59:03.955974: step 54130, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.670 sec/batch; 51h:46m:59s remains)
INFO - root - 2017-12-15 19:59:10.351628: step 54140, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 49h:34m:50s remains)
INFO - root - 2017-12-15 19:59:16.743170: step 54150, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 48h:46m:06s remains)
INFO - root - 2017-12-15 19:59:23.230094: step 54160, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 49h:19m:30s remains)
INFO - root - 2017-12-15 19:59:29.594522: step 54170, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 49h:55m:22s remains)
INFO - root - 2017-12-15 19:59:36.034581: step 54180, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 50h:55m:06s remains)
INFO - root - 2017-12-15 19:59:42.338660: step 54190, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 49h:19m:56s remains)
INFO - root - 2017-12-15 19:59:48.711292: step 54200, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 48h:25m:38s remains)
2017-12-15 19:59:49.179869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4972968 -4.7999582 -5.2488904 -5.6930408 -6.1517749 -5.6485586 -4.2709246 -2.9791245 -1.8917842 -1.9149776 -2.5109873 -4.4976883 -4.7266474 -5.9043469 -7.5261855][-4.2959995 -4.7792435 -5.4815612 -6.2281036 -6.6419792 -6.2462187 -5.8032603 -4.701654 -3.3205743 -3.6424255 -4.3978128 -6.349566 -6.8747482 -7.7857533 -8.3347683][-3.5735912 -3.9978423 -4.1779871 -4.3584337 -5.0023317 -5.3641033 -5.3185167 -4.8582706 -4.474885 -5.0735788 -5.1273031 -6.3748713 -6.8166695 -7.7182055 -8.6228628][-2.92137 -2.9321852 -2.886549 -2.6401024 -2.6254683 -3.2487216 -3.6735306 -3.4902244 -2.9029288 -3.920433 -5.0965953 -6.3150525 -6.8021894 -7.5740662 -8.5115623][-2.6516404 -1.8189902 -1.1665206 -0.70046949 -0.7599473 -0.80903149 -0.75645781 -0.65065193 0.064923763 -0.88952732 -2.4876308 -4.6505671 -5.8600521 -7.2778454 -8.2717218][-1.9996443 -1.0683212 0.22671175 0.89451122 1.1984043 1.4321909 1.4859676 1.3162842 1.9415007 0.86702251 -0.49812508 -3.3795929 -4.9739313 -6.2897568 -7.2788472][-1.4293985 -0.50974274 0.56513882 1.252305 1.6814461 2.266922 2.9271812 3.0708904 3.3593616 1.6414309 -0.076914787 -2.9977317 -4.5312033 -5.7703609 -6.7683291][-1.9600215 -1.4579129 -0.85419035 0.96164513 1.7800694 2.7552891 3.5110121 3.8420725 4.0595388 2.012394 -0.3221488 -3.3502307 -4.41783 -5.4974594 -6.2485194][-2.4892373 -1.5399547 -0.55765295 0.60878468 1.5574303 2.5431881 2.9257822 3.4500589 3.9307556 1.8799496 -0.30690527 -3.2068338 -4.3282719 -5.3151937 -6.1021633][-2.856482 -2.5209994 -1.9945564 -1.3020535 -0.55511522 0.14040327 0.7866478 1.4776201 1.9026909 -0.12348032 -1.8661599 -4.3286047 -5.2209539 -5.9649248 -6.4661226][-6.3235488 -5.8667984 -5.6278915 -4.667161 -3.6671548 -2.7911482 -2.0307369 -1.6679006 -1.8461328 -3.5807481 -4.9955506 -6.826828 -7.34662 -7.5029883 -7.5749254][-8.466053 -8.09814 -7.4285631 -6.779685 -6.3600512 -5.3629794 -4.651566 -4.4786444 -4.3312044 -5.3195543 -6.7019739 -7.0862522 -6.9488831 -7.3448043 -7.7906246][-9.35526 -9.4128637 -9.2336912 -8.5895071 -7.9244609 -7.169673 -6.4329123 -6.238224 -6.2128544 -6.7133732 -7.4992723 -7.6089344 -7.287817 -7.2962828 -7.0856538][-8.4381895 -8.3984127 -8.1362162 -7.7255759 -7.531477 -6.7853994 -6.23229 -6.2035708 -6.1664596 -6.5756807 -7.1666541 -7.1682014 -6.66675 -6.5043268 -6.4271379][-8.2079983 -7.8158693 -7.7169404 -7.230444 -7.2435346 -6.9066596 -6.9995031 -7.1370368 -7.3450108 -7.477067 -7.4477463 -6.964272 -6.217802 -5.8065367 -5.5402379]]...]
INFO - root - 2017-12-15 19:59:55.613190: step 54210, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 49h:38m:01s remains)
INFO - root - 2017-12-15 20:00:02.015328: step 54220, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 51h:09m:26s remains)
INFO - root - 2017-12-15 20:00:08.388134: step 54230, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.639 sec/batch; 49h:22m:20s remains)
INFO - root - 2017-12-15 20:00:14.746395: step 54240, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 48h:11m:32s remains)
INFO - root - 2017-12-15 20:00:21.150471: step 54250, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 49h:44m:46s remains)
INFO - root - 2017-12-15 20:00:27.528809: step 54260, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 49h:20m:33s remains)
INFO - root - 2017-12-15 20:00:33.977037: step 54270, loss = 0.27, batch loss = 0.16 (11.6 examples/sec; 0.689 sec/batch; 53h:14m:44s remains)
INFO - root - 2017-12-15 20:00:40.368900: step 54280, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 48h:56m:28s remains)
INFO - root - 2017-12-15 20:00:46.721512: step 54290, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 47h:52m:53s remains)
INFO - root - 2017-12-15 20:00:53.139310: step 54300, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 48h:25m:07s remains)
2017-12-15 20:00:53.675580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.98999405 -1.2946892 -1.4954572 -1.882329 -2.4729714 -3.2278123 -4.1781654 -4.5586128 -4.8820505 -5.3872986 -5.9260774 -6.892571 -7.5209661 -7.343576 -7.998538][-1.7913289 -1.5636225 -2.0015116 -2.6053209 -3.3184209 -3.445941 -3.6089635 -4.4659634 -5.1016183 -5.7000976 -6.643455 -7.6817503 -8.2897015 -7.8742018 -8.2189779][-2.1399765 -2.2476134 -2.3028746 -1.9557805 -1.6587615 -2.1369295 -2.798296 -3.412498 -3.5674958 -4.7409868 -5.909193 -6.9583688 -7.9045558 -7.8803425 -8.6150618][-2.4735947 -2.5536404 -2.5645099 -2.3159881 -1.4879827 -1.2744169 -0.88424063 -1.0973573 -1.471879 -2.8230815 -4.2921195 -5.7296267 -6.8866668 -7.0703511 -8.000701][-2.5263076 -2.5384398 -2.0695662 -1.6346045 -0.87587404 -0.43558168 -0.13258982 -0.089490891 -0.093158245 -1.402247 -2.7655196 -4.3201265 -5.523119 -5.856802 -7.2577863][-3.4264774 -2.5741706 -1.7803864 -1.2893515 -0.57429695 -0.11760187 0.81162453 0.68447781 0.93579674 -0.13654184 -1.8029871 -3.6647677 -4.9287863 -5.090745 -6.2576222][-3.2583933 -2.4620576 -1.5719366 -0.49701643 0.93846035 2.0455399 2.9687214 2.3882685 2.3008051 1.0975523 -0.88145018 -3.1055417 -4.5633 -4.6801538 -5.9210215][-2.6279879 -1.9820175 -1.195292 0.24341583 2.147028 3.2683859 4.4537888 4.2562637 3.8046169 2.0738802 -0.018732071 -2.7968316 -4.3019934 -4.6635752 -6.2510214][-2.9987502 -2.1395102 -1.5865769 -0.4021697 0.93137646 1.917366 3.324296 3.9884367 4.5728788 2.7801266 -0.067433834 -2.7511759 -4.9784775 -5.0749626 -6.2368579][-3.9069805 -3.6551743 -3.222106 -2.2113223 -1.0879989 -0.25069094 0.77874947 1.4667044 2.2129135 0.79167652 -0.83628035 -3.1193881 -5.1411114 -6.4528275 -7.8197274][-5.7449946 -5.2752724 -4.8614216 -4.1042538 -3.3445883 -2.6918359 -1.6940794 -1.0323296 -0.52128458 -1.9285874 -3.5241251 -4.96758 -5.9113503 -6.4318018 -8.0451431][-6.4100962 -6.2450905 -5.9119778 -5.5461206 -5.18231 -4.7677479 -4.0755696 -3.5141973 -2.8770509 -3.4339919 -4.2231293 -5.1126142 -5.973444 -7.1031661 -8.1649752][-7.7243261 -7.3100209 -7.3148794 -6.8559375 -6.6451993 -6.5858507 -6.0704365 -5.8350625 -5.4414263 -5.6451035 -5.8718538 -6.35801 -6.62506 -7.1139565 -7.8636622][-8.1343069 -8.1278429 -7.752162 -7.2473435 -6.9288797 -6.5880527 -6.5194421 -6.7750554 -6.70327 -6.7874613 -6.7139797 -6.6927409 -6.6060691 -6.7191515 -6.9648728][-9.16403 -8.6985245 -8.5293 -8.1783628 -7.6197319 -7.3692904 -6.6711922 -7.0498743 -7.5942087 -7.5649529 -7.3682823 -7.0431709 -6.8948326 -6.8678188 -6.8413668]]...]
INFO - root - 2017-12-15 20:01:00.007872: step 54310, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 48h:23m:15s remains)
INFO - root - 2017-12-15 20:01:06.326152: step 54320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 49h:05m:24s remains)
INFO - root - 2017-12-15 20:01:12.741753: step 54330, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 48h:37m:22s remains)
INFO - root - 2017-12-15 20:01:19.142718: step 54340, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 50h:48m:38s remains)
INFO - root - 2017-12-15 20:01:25.659975: step 54350, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.655 sec/batch; 50h:38m:10s remains)
INFO - root - 2017-12-15 20:01:32.043315: step 54360, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 49h:46m:14s remains)
INFO - root - 2017-12-15 20:01:38.418223: step 54370, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 49h:37m:54s remains)
INFO - root - 2017-12-15 20:01:44.815034: step 54380, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 50h:13m:41s remains)
INFO - root - 2017-12-15 20:01:51.188805: step 54390, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 49h:01m:27s remains)
INFO - root - 2017-12-15 20:01:57.656728: step 54400, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 48h:50m:10s remains)
2017-12-15 20:01:58.176317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1385365 -6.5824485 -5.5330067 -4.5563812 -3.6490641 -3.3648067 -3.5729294 -3.4263635 -2.7829242 -2.837708 -3.625927 -5.495563 -6.0510397 -6.824811 -8.105999][-6.6306157 -6.4677248 -5.7499204 -4.9867058 -4.7320852 -4.2401924 -3.9275975 -3.8663456 -3.8252282 -3.8057404 -4.3745542 -6.2304316 -6.8080053 -7.4921188 -8.063055][-6.3962517 -6.1611919 -5.1424456 -4.2329788 -3.5699921 -3.2723417 -3.0664387 -2.8792 -2.3874855 -2.6744385 -3.9127212 -5.9511452 -6.3588405 -6.8837533 -7.2238932][-5.7198858 -5.1613564 -4.5277853 -3.7144232 -2.6196942 -1.3630548 -0.38272238 -0.40320349 -0.45381165 -0.98279428 -2.6240606 -5.07267 -5.4677334 -5.9499245 -6.4215851][-5.3504395 -4.3221617 -2.8446436 -1.6324778 -0.52600479 0.32705688 0.89534092 0.75174522 0.48487949 -0.38890886 -1.879385 -4.2637129 -4.9221377 -5.5276031 -5.7468905][-4.3683009 -3.2397552 -1.8994594 -0.51520348 0.73842335 1.5969486 2.298048 1.8329325 1.2638178 0.37047672 -0.74542475 -2.7201581 -3.2248311 -4.1810427 -4.7874012][-5.1560922 -3.2522674 -1.1150579 0.615325 1.7521324 2.4001274 2.7706909 2.657403 2.5443964 1.1830139 0.072897911 -1.9006424 -2.6151915 -3.719022 -4.5300636][-4.7608991 -2.5943322 -0.44954872 1.3611155 2.835454 3.3455648 3.3939753 3.2062826 3.0141125 1.8854275 0.81896591 -1.7269368 -2.9302468 -4.109189 -4.8772469][-3.6632328 -2.1997004 -0.53236294 0.96768284 1.8769312 2.5933876 2.9579954 2.7060566 2.3513651 1.5962811 1.0975399 -1.5167055 -2.956532 -4.1122279 -5.0079031][-3.2603784 -2.4827938 -1.7415175 -0.57696152 0.23432398 0.99735451 1.3282824 1.4585171 1.4832649 0.44898987 -0.19854212 -2.0721698 -2.7716026 -4.2781606 -5.4812098][-3.6325669 -3.0666542 -2.2641025 -1.6203513 -1.2287197 -0.55170536 -0.3504405 0.17516994 0.7792778 -0.60984659 -1.8441105 -3.3183074 -3.9244084 -4.9924908 -6.0674295][-4.0514655 -3.4853277 -2.7955647 -1.8425193 -1.1477542 -0.58597803 -0.592051 -0.76956749 -0.91869688 -2.0048556 -2.61204 -3.8428056 -5.4330597 -6.5704374 -6.5918941][-4.8285513 -4.1295023 -3.4511943 -2.6888533 -2.1908994 -1.2589793 -0.97354174 -1.1920829 -1.7073421 -2.8317685 -3.7838471 -4.401598 -4.81369 -6.3943205 -7.4564815][-5.174058 -4.2946215 -3.2709179 -2.2121711 -1.9747152 -1.38837 -1.2534742 -1.5606313 -1.7323976 -2.3081541 -3.1643262 -4.4700623 -5.4196234 -5.9902115 -6.3510513][-6.3851767 -5.0042496 -4.2568574 -3.3074412 -2.5796151 -2.1425934 -2.1689825 -2.3990583 -2.6529779 -3.2120085 -3.619514 -4.1838303 -5.2814655 -6.0211053 -6.2152796]]...]
INFO - root - 2017-12-15 20:02:04.656519: step 54410, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 49h:15m:42s remains)
INFO - root - 2017-12-15 20:02:11.110854: step 54420, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 51h:55m:40s remains)
INFO - root - 2017-12-15 20:02:17.646381: step 54430, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 49h:22m:35s remains)
INFO - root - 2017-12-15 20:02:24.046959: step 54440, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 51h:09m:52s remains)
INFO - root - 2017-12-15 20:02:30.440673: step 54450, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 48h:56m:28s remains)
INFO - root - 2017-12-15 20:02:36.938815: step 54460, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:39m:03s remains)
INFO - root - 2017-12-15 20:02:43.360657: step 54470, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 50h:08m:59s remains)
INFO - root - 2017-12-15 20:02:49.915761: step 54480, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 47h:57m:04s remains)
INFO - root - 2017-12-15 20:02:56.318499: step 54490, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 50h:35m:35s remains)
INFO - root - 2017-12-15 20:03:02.648523: step 54500, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 50h:28m:17s remains)
2017-12-15 20:03:03.197822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4643259 -5.8313994 -5.7739449 -6.13311 -6.291141 -6.5483007 -6.8254294 -6.8152776 -6.5378375 -6.065783 -6.7703032 -7.3449306 -6.9245672 -6.7777109 -6.9624419][-5.3182468 -5.5609145 -5.616601 -5.4451122 -5.4006014 -5.67437 -6.429121 -7.0357661 -7.5770802 -7.5703468 -8.6301632 -9.2697849 -8.8151293 -8.1084671 -7.7937317][-6.0799208 -5.9499578 -5.7846665 -5.2396669 -4.9920707 -4.8594217 -4.7336864 -5.2535419 -5.9492817 -6.9339018 -8.66553 -9.6530285 -9.668931 -9.2325907 -8.6819668][-7.0103884 -6.5135446 -5.8774862 -4.3051767 -3.2631273 -2.816184 -2.6599159 -3.0535927 -3.2196093 -4.2314272 -6.0256486 -7.8672 -8.6903858 -8.6469784 -8.8133516][-7.7554274 -6.6475554 -5.6246891 -4.0487947 -2.6167812 -1.2469063 -0.26583385 -0.18113804 -0.20073414 -1.3830466 -3.1405125 -4.9111595 -5.816021 -6.7397804 -7.6458287][-7.2885923 -6.6470504 -5.860465 -3.7031705 -1.8647704 -0.29865551 1.2460985 1.7995443 2.186944 1.4076986 0.074523449 -1.7630496 -2.9714541 -4.1314974 -5.5293245][-6.691093 -5.6297874 -4.2611365 -2.196908 -0.31663656 1.2566452 2.6587553 2.8325949 3.3161163 2.8436985 1.5580626 -0.29593325 -1.4696512 -2.8209295 -4.1675997][-5.6731234 -4.5798006 -2.9130063 -0.96132374 0.91759682 2.3031826 3.5355072 3.6687574 3.7249727 2.4686146 1.1511564 -0.39436007 -1.342669 -2.760551 -4.3509054][-6.1072822 -5.2936907 -3.7243502 -1.6578975 0.21068811 1.5706396 2.618269 2.9904976 3.5337296 2.5219994 0.98827267 -0.99579668 -1.8832135 -2.7400966 -4.2283421][-6.8648481 -6.4505577 -5.8206086 -4.1834173 -1.9243946 -0.41391611 0.34521675 0.73849487 1.3050098 0.80256653 0.35054779 -1.59549 -2.912457 -3.9843082 -5.0986481][-7.0221319 -7.1963954 -7.083427 -6.4961576 -5.5320969 -4.2335711 -2.8976603 -2.3883114 -2.3623242 -2.7020302 -3.1432157 -4.1132965 -4.47398 -5.0016651 -6.152153][-7.2857671 -7.7539139 -8.1174126 -8.0825377 -7.8217807 -7.4522581 -6.6781034 -5.9078569 -5.0681391 -4.6507335 -4.8112516 -5.419219 -5.8201127 -5.9319143 -5.924582][-7.622541 -7.7654018 -8.0521774 -8.3423185 -8.2830305 -8.0468636 -7.6455259 -7.4660115 -7.0715752 -6.6615944 -6.7298512 -6.5109806 -6.1174479 -5.9226613 -5.8373532][-7.3095903 -7.5310469 -7.457068 -7.5655847 -7.5587535 -7.2817559 -6.9105587 -6.70854 -7.0045161 -7.3922973 -8.0955467 -8.1189537 -7.3848491 -6.4766512 -5.7054839][-7.4662395 -7.3627396 -7.0162516 -6.7078075 -6.6129513 -6.4463959 -6.1973763 -6.0593853 -6.2047358 -6.3409967 -6.7058334 -7.2480063 -7.3457417 -7.5297356 -6.8606987]]...]
INFO - root - 2017-12-15 20:03:09.714651: step 54510, loss = 0.41, batch loss = 0.30 (12.6 examples/sec; 0.637 sec/batch; 49h:12m:30s remains)
INFO - root - 2017-12-15 20:03:16.111740: step 54520, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:46m:35s remains)
INFO - root - 2017-12-15 20:03:22.446760: step 54530, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 48h:41m:08s remains)
INFO - root - 2017-12-15 20:03:28.932094: step 54540, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 49h:23m:23s remains)
INFO - root - 2017-12-15 20:03:35.386094: step 54550, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:47m:30s remains)
INFO - root - 2017-12-15 20:03:41.818689: step 54560, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 48h:50m:08s remains)
INFO - root - 2017-12-15 20:03:48.202210: step 54570, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 49h:22m:24s remains)
INFO - root - 2017-12-15 20:03:54.612682: step 54580, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 50h:09m:51s remains)
INFO - root - 2017-12-15 20:04:01.022737: step 54590, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 49h:33m:13s remains)
INFO - root - 2017-12-15 20:04:07.475145: step 54600, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 50h:14m:49s remains)
2017-12-15 20:04:08.071009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4449182 -6.7206869 -6.8297644 -6.539566 -6.3202877 -6.1336489 -5.7252545 -4.8288631 -3.8575146 -3.5330791 -4.3369741 -5.0855 -5.9757938 -7.1298084 -7.5275431][-5.6482048 -6.1124072 -6.4630127 -6.7329679 -7.3777924 -7.5387249 -6.8592629 -6.2873 -5.2019591 -5.4525466 -5.420598 -5.929996 -7.032815 -7.8646646 -8.4470968][-4.3859091 -4.8766012 -5.6163116 -6.542985 -7.4495139 -7.0712304 -6.8985071 -5.9563951 -4.5461597 -4.1006784 -4.6208634 -5.4462214 -6.2867107 -7.2852983 -7.5224781][-3.7258394 -3.7879009 -4.9464903 -5.3307533 -5.7349124 -5.3211021 -4.8217449 -4.1421146 -3.2862763 -3.564219 -4.0096436 -4.8372259 -5.9012671 -6.8188577 -6.88126][-4.3393326 -3.8224151 -3.6918726 -3.9534833 -4.3436418 -3.080965 -1.9258714 -0.8563242 -0.15852642 -1.5373497 -2.6426768 -3.8964484 -5.5247097 -6.8486824 -7.0146031][-5.2983894 -4.422822 -3.5737538 -2.5990148 -1.1842928 0.60355759 2.2347393 2.854331 2.8940372 1.186183 -1.1705723 -3.2728472 -5.3981762 -6.8705645 -7.46623][-6.1802235 -4.8772163 -3.5761271 -1.7630873 0.80037785 3.1938057 5.0852165 5.7916889 5.4363337 2.4278536 -0.37262392 -3.2219887 -5.9941926 -7.469914 -7.6481328][-5.5618172 -4.5569363 -3.0172105 -1.0097513 1.8239756 4.3563232 6.3419256 6.9215231 6.5615997 3.9545698 0.43267727 -2.7440886 -5.6580219 -7.4651446 -7.5332508][-5.7655048 -4.7285223 -3.5013227 -1.5029559 1.1141729 3.2964678 4.9900208 5.3687658 5.1991682 2.8206205 -0.11633015 -2.8538232 -5.6671686 -7.3827066 -7.3610392][-6.5450077 -5.7819228 -4.93426 -3.2470007 -1.1968966 0.31180191 1.5032492 2.0089169 1.5528574 -0.92100191 -3.20077 -5.1786194 -7.31214 -8.3569727 -8.2729445][-8.13201 -7.9912648 -7.1473494 -6.0907097 -5.13459 -3.9762573 -2.623023 -2.6948953 -2.5461712 -4.0669231 -5.9818554 -6.9459739 -8.4956369 -9.3017082 -9.0664253][-9.1372919 -8.4601212 -8.07599 -7.7058239 -7.2514896 -6.5806642 -6.0720544 -5.6817055 -5.1124039 -6.8654127 -7.4005775 -8.0802546 -9.3596392 -9.1413155 -8.9376926][-9.9724789 -9.8625422 -9.3041143 -8.315752 -7.7042961 -7.250617 -6.9867325 -7.2459903 -7.3974156 -7.6712532 -8.2579927 -7.9173732 -8.0744247 -7.8133492 -7.7065449][-7.9755912 -8.0564232 -7.8301959 -7.9109612 -7.1446009 -6.4511781 -6.0013943 -6.0425572 -6.036922 -6.6882896 -6.8326354 -6.7763853 -7.0355096 -6.6230316 -6.3494864][-8.4789286 -8.2393141 -7.9172521 -7.4491577 -6.911952 -6.789691 -6.7178626 -6.5965528 -6.549336 -6.6644359 -6.7589064 -6.4012589 -5.9752541 -5.5826092 -5.4540987]]...]
INFO - root - 2017-12-15 20:04:14.408492: step 54610, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 51h:07m:08s remains)
INFO - root - 2017-12-15 20:04:20.809502: step 54620, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:49m:19s remains)
INFO - root - 2017-12-15 20:04:27.199905: step 54630, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 49h:36m:32s remains)
INFO - root - 2017-12-15 20:04:33.728302: step 54640, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 48h:58m:45s remains)
INFO - root - 2017-12-15 20:04:40.143239: step 54650, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.664 sec/batch; 51h:16m:53s remains)
INFO - root - 2017-12-15 20:04:46.529598: step 54660, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 50h:21m:18s remains)
INFO - root - 2017-12-15 20:04:52.952934: step 54670, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 50h:46m:19s remains)
INFO - root - 2017-12-15 20:04:59.388718: step 54680, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 49h:12m:09s remains)
INFO - root - 2017-12-15 20:05:05.794303: step 54690, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 51h:21m:32s remains)
INFO - root - 2017-12-15 20:05:12.234736: step 54700, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 49h:09m:12s remains)
2017-12-15 20:05:12.769122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9047477 -3.881027 -3.9337823 -4.2333817 -4.3315144 -4.2448111 -4.2675095 -4.1180973 -3.6353846 -3.9245954 -4.0531921 -4.7070103 -5.0051365 -5.8242636 -6.7460928][-3.6507215 -3.8417828 -3.9948907 -3.9985552 -4.0245323 -4.1525264 -4.3011189 -4.1696234 -3.8628442 -4.1320972 -4.5030532 -5.3380527 -5.5595551 -6.0929675 -6.6761456][-3.1643362 -3.0348639 -3.1490068 -3.2704835 -3.3388476 -3.4771423 -3.4613423 -3.3146024 -3.1099119 -3.6670151 -4.04014 -4.8878784 -5.0178423 -5.6069326 -6.2356272][-3.2042327 -2.7238398 -2.2348323 -1.8358703 -1.6871028 -1.735939 -1.656743 -1.5236201 -1.4153428 -2.0268898 -2.6100435 -3.7773755 -4.2806182 -4.875267 -5.6845851][-2.77029 -2.069962 -1.4509072 -0.79506159 -0.15381908 0.32288074 0.59301949 0.56573772 0.59014225 -0.33394241 -1.2886572 -2.6274261 -3.5341783 -4.6237879 -5.4992461][-2.12225 -1.3089972 -0.78461981 0.037295341 0.615571 1.5314207 2.3620262 2.3571873 2.1374817 1.1810904 0.0971117 -1.6178732 -2.8067656 -4.0527563 -5.1921511][-2.3088732 -1.510746 -0.832149 0.3195219 1.2180595 1.9710312 2.6537228 2.6792507 2.5509806 1.6298656 0.57841492 -1.2027578 -2.447391 -3.9360454 -5.0979481][-2.3872747 -1.5568914 -0.752213 0.39915562 1.1929035 2.0040359 2.6362543 2.6775322 2.6990271 1.66955 0.45716667 -1.3842459 -2.4812226 -3.923439 -5.1531649][-2.7849197 -2.0242534 -1.1639919 0.11143589 0.944767 1.6065283 2.1172132 1.9336891 1.7661982 0.79712868 -0.30642748 -2.2005033 -3.2978897 -4.6177778 -5.7159891][-2.7681642 -2.45266 -1.9500256 -0.8624177 0.024388313 0.656332 1.08395 0.943038 0.75227737 -0.3388319 -1.1638689 -3.0365205 -3.9076436 -4.9561558 -5.968502][-3.185534 -2.872046 -2.4836354 -1.6618624 -1.0450215 -0.50050211 -0.020155907 -0.36499834 -0.72370243 -1.5396767 -2.1442227 -3.6768327 -4.4861593 -5.3107615 -6.0145316][-4.3591433 -4.1884127 -3.810822 -3.0408406 -2.4431324 -1.823463 -1.3606329 -1.5455308 -1.9348607 -2.8325391 -3.4428349 -4.337842 -4.6445255 -5.2270317 -5.7738047][-4.8663092 -5.1249928 -4.9895811 -4.2667131 -3.6816869 -3.0230079 -2.5730038 -3.0862427 -3.7092631 -4.3569703 -4.68784 -5.1101694 -5.0769372 -5.3465757 -5.7215686][-4.7703629 -5.0333815 -5.2756929 -4.8947029 -4.3842797 -4.0540838 -3.691381 -3.8080947 -4.1574678 -4.9783573 -5.4968758 -5.7873068 -5.6471581 -5.590395 -5.3622313][-5.6650124 -5.4318953 -5.4576068 -5.7352934 -5.8852215 -5.4706149 -5.2327175 -5.5336456 -5.8911438 -6.0742779 -6.2968044 -6.7604313 -7.0273108 -6.9577117 -6.5690289]]...]
INFO - root - 2017-12-15 20:05:19.132278: step 54710, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 49h:39m:54s remains)
INFO - root - 2017-12-15 20:05:25.554618: step 54720, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 48h:42m:59s remains)
INFO - root - 2017-12-15 20:05:31.894032: step 54730, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 48h:19m:52s remains)
INFO - root - 2017-12-15 20:05:38.232825: step 54740, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.615 sec/batch; 47h:28m:43s remains)
INFO - root - 2017-12-15 20:05:44.732945: step 54750, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 48h:44m:58s remains)
INFO - root - 2017-12-15 20:05:51.198578: step 54760, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 49h:15m:31s remains)
INFO - root - 2017-12-15 20:05:57.633840: step 54770, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.683 sec/batch; 52h:39m:48s remains)
INFO - root - 2017-12-15 20:06:04.086804: step 54780, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 50h:34m:38s remains)
INFO - root - 2017-12-15 20:06:10.511490: step 54790, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 49h:18m:08s remains)
INFO - root - 2017-12-15 20:06:16.960632: step 54800, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 51h:09m:32s remains)
2017-12-15 20:06:17.515190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1602077 -3.2840652 -3.506259 -4.0116224 -4.243763 -4.0873618 -4.314539 -3.758919 -3.0789022 -2.0007162 -1.6998115 -3.1049113 -3.9399743 -5.460546 -6.1486597][-3.1781855 -3.3297248 -3.445838 -4.1490211 -4.651341 -4.871026 -5.2872505 -4.8811951 -4.215663 -3.3383241 -3.0522451 -3.8554556 -4.0873823 -4.9319386 -5.4051409][-4.2981167 -4.6146989 -4.7285566 -4.5805349 -4.5483389 -4.4565687 -4.722 -5.2084427 -5.2563934 -4.7749166 -4.5182629 -5.2232313 -5.4298382 -5.8379331 -5.7917728][-5.9588394 -5.7991886 -5.47845 -4.335824 -3.5439992 -3.1267538 -3.0216022 -3.5027723 -3.7042217 -4.1231022 -4.3839073 -5.4760823 -5.9747176 -6.3901825 -6.6418424][-5.569993 -4.8042955 -4.2471027 -3.217824 -2.2785878 -1.5361447 -1.2470331 -1.3548484 -1.5072951 -1.8864851 -2.300693 -3.9077418 -4.8932076 -6.0858092 -6.7089963][-4.4722042 -3.7635269 -3.0211015 -1.7861357 -0.84736919 0.066298962 0.761364 1.0045872 1.021553 0.73001575 -0.051036835 -1.8052783 -2.9513073 -4.6580687 -5.8492289][-3.6005015 -2.6959205 -1.632915 -0.051013947 0.97349548 2.0578995 2.7580013 2.8325558 2.9340677 3.1508942 2.2452517 0.27187204 -1.1390848 -3.4002743 -4.9564214][-3.7777572 -2.4636912 -1.1796269 0.60970592 2.2392731 3.139658 3.6591024 4.0072613 4.392458 4.2906761 3.2220182 1.029706 -0.49617958 -2.8884816 -4.4339356][-3.5177131 -2.516768 -1.4658437 -0.22420883 1.0472946 2.3796186 2.9981079 3.2679348 3.6951008 3.5359335 2.7709465 0.26824522 -1.4645352 -3.4012318 -4.780972][-3.8605683 -3.2717547 -2.3453646 -1.3258 -0.33678198 0.63801479 1.1984119 1.7803001 2.2483826 1.8639364 1.2303848 -0.62916708 -1.7712398 -3.751591 -5.3774114][-4.3234935 -4.2318182 -4.06534 -3.1928797 -2.1671185 -1.5605421 -1.0834966 -0.55425024 -0.31248617 -0.70402336 -0.94092083 -2.3904991 -3.2560925 -4.6261358 -5.7674046][-6.2531185 -6.4993396 -6.4089355 -5.6786914 -4.6674066 -3.7423618 -3.0273571 -2.7395892 -2.634798 -2.9520888 -3.493454 -4.7126732 -5.5429583 -6.3601766 -7.0055356][-7.6758852 -8.2769356 -8.7486811 -7.9296341 -6.9616327 -5.8228383 -4.8619413 -4.4209709 -4.2563181 -4.2849879 -4.9241014 -5.3693829 -5.5165882 -6.0282993 -6.1213636][-7.946322 -8.56789 -9.0141144 -8.6125841 -8.2832718 -7.2556138 -6.5160236 -6.0688758 -5.990716 -5.7126617 -5.6463089 -5.5278311 -5.4019814 -5.7923212 -5.8634176][-9.4042635 -9.3619518 -9.0655909 -8.82987 -8.6354494 -8.17325 -7.8497357 -7.6827602 -7.6578283 -7.1124582 -6.870708 -6.2935171 -5.9048595 -5.841351 -5.8683958]]...]
INFO - root - 2017-12-15 20:06:24.073736: step 54810, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 49h:50m:58s remains)
INFO - root - 2017-12-15 20:06:30.436762: step 54820, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 49h:55m:22s remains)
INFO - root - 2017-12-15 20:06:36.861623: step 54830, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.633 sec/batch; 48h:49m:48s remains)
INFO - root - 2017-12-15 20:06:43.466907: step 54840, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.647 sec/batch; 49h:55m:56s remains)
INFO - root - 2017-12-15 20:06:49.903771: step 54850, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 48h:35m:55s remains)
INFO - root - 2017-12-15 20:06:56.293317: step 54860, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 50h:25m:04s remains)
INFO - root - 2017-12-15 20:07:02.773539: step 54870, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 49h:51m:19s remains)
INFO - root - 2017-12-15 20:07:09.258076: step 54880, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 49h:24m:42s remains)
INFO - root - 2017-12-15 20:07:15.597903: step 54890, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.624 sec/batch; 48h:06m:25s remains)
INFO - root - 2017-12-15 20:07:22.014308: step 54900, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 48h:41m:04s remains)
2017-12-15 20:07:22.490075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5208859 -3.7372646 -3.9116974 -4.1176157 -4.2611685 -3.5793357 -3.2220321 -3.4110146 -3.4593649 -4.5927157 -5.3639116 -5.6138606 -6.6869435 -7.0062828 -6.8049197][-4.5517178 -4.8018193 -5.1699815 -5.044425 -5.0175047 -5.0654564 -4.9363184 -4.3972969 -4.0149317 -5.5624824 -6.4206495 -6.7592111 -7.7475991 -7.9421129 -8.0190468][-5.677711 -5.8509531 -6.4070444 -6.4166484 -6.0452719 -5.7185464 -5.2905512 -5.1629839 -5.1671581 -6.2455263 -6.6157212 -7.29442 -8.6801777 -8.9676714 -8.8741255][-6.7440486 -6.7562819 -6.5247679 -5.395689 -4.755044 -4.1074 -3.4644403 -3.3262715 -3.3552303 -4.8765817 -5.660718 -6.4714413 -7.846283 -7.8118162 -7.5937424][-8.1944284 -7.1899524 -6.074913 -4.941226 -3.72279 -1.7250128 -0.532022 -0.563077 -0.52142286 -2.2393494 -3.1405821 -4.2758093 -5.8200779 -6.3120623 -6.7544851][-9.0730886 -8.6534033 -8.09144 -5.4875345 -2.6095095 -0.39706039 1.2936144 1.4221144 1.0968409 -0.91999245 -2.2504921 -3.6336946 -5.4892893 -6.3622184 -6.969842][-9.0841732 -7.5622969 -5.2773385 -3.4284658 -0.88238096 1.6230793 3.3215113 3.9430923 3.8171539 1.1764431 -0.39362383 -2.2027469 -4.1390018 -5.0907254 -5.5525179][-8.008954 -6.8303514 -5.3188872 -1.7827177 1.7579584 3.2913666 4.9370832 5.5875721 5.2819405 2.471427 0.73092937 -0.93878937 -2.5652041 -3.8188758 -4.1916146][-8.0600748 -7.5708661 -6.170002 -3.7937069 -1.3299208 0.59792519 2.576745 3.4430208 3.8900099 1.607584 -0.34014559 -2.1369376 -3.9148893 -4.7479296 -4.846694][-9.0749493 -8.4828987 -7.5065265 -5.1690722 -2.6440487 -1.6081786 -0.60531759 -0.21458006 0.050810814 -1.8076377 -3.2292061 -4.0681524 -5.8130302 -5.9068289 -5.1857586][-9.4174232 -9.632947 -9.3814325 -8.18676 -6.7259631 -4.657733 -3.099205 -3.7799096 -4.0612011 -5.3708944 -6.206821 -6.1843753 -7.2958865 -6.9224949 -6.29021][-10.501678 -9.9743366 -9.4253283 -9.2095947 -8.6814442 -7.2542772 -5.9980865 -5.0634956 -4.3166451 -6.7068963 -7.7996454 -7.0421968 -7.4937582 -7.2182932 -7.5924139][-10.725733 -10.609648 -9.9919271 -9.3188448 -8.4088688 -7.7342582 -7.4356828 -6.8627839 -6.1741886 -6.7448921 -7.0271578 -7.2759285 -7.6921673 -6.84635 -6.3866348][-8.5425491 -8.9933987 -8.8483057 -8.3904276 -7.6478491 -7.0186667 -6.734395 -6.7462578 -6.8693695 -7.3817797 -7.3532352 -7.0081091 -6.8726845 -6.5009651 -6.0084686][-7.5665956 -7.0458527 -7.3093677 -7.9150252 -7.5397005 -6.9744759 -6.53109 -6.7470813 -6.7735529 -6.7957063 -7.0321097 -7.3372674 -7.4089026 -6.996129 -6.8637037]]...]
INFO - root - 2017-12-15 20:07:28.905576: step 54910, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 49h:37m:44s remains)
INFO - root - 2017-12-15 20:07:35.269777: step 54920, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 48h:56m:57s remains)
INFO - root - 2017-12-15 20:07:41.624114: step 54930, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 47h:50m:49s remains)
INFO - root - 2017-12-15 20:07:48.119293: step 54940, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 48h:45m:03s remains)
INFO - root - 2017-12-15 20:07:54.514759: step 54950, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 49h:33m:37s remains)
INFO - root - 2017-12-15 20:08:00.979881: step 54960, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 51h:04m:39s remains)
INFO - root - 2017-12-15 20:08:07.532625: step 54970, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 49h:12m:25s remains)
INFO - root - 2017-12-15 20:08:13.959296: step 54980, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 48h:10m:35s remains)
INFO - root - 2017-12-15 20:08:20.459788: step 54990, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 48h:50m:21s remains)
INFO - root - 2017-12-15 20:08:26.903514: step 55000, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 48h:50m:31s remains)
2017-12-15 20:08:27.430634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9526241 -4.0113268 -4.9731274 -5.1391926 -5.167099 -4.9429579 -4.8130846 -4.2363281 -3.3608751 -4.696908 -5.38617 -5.8164582 -6.6668119 -6.5567389 -7.5295796][-4.0874052 -4.9721804 -5.2245531 -5.5778465 -6.078145 -5.8964324 -5.1278048 -4.3256836 -3.6090956 -4.794507 -5.04717 -6.1041851 -7.2393146 -7.3108029 -8.4564085][-3.1966991 -3.6768632 -3.4097371 -3.5893955 -3.7101753 -3.6262546 -3.5134659 -3.2998886 -2.9711876 -4.4480953 -4.8165879 -5.4812765 -6.3686457 -6.6796646 -7.7374454][-2.395956 -3.1636724 -3.3423381 -2.881916 -2.8471951 -2.2785664 -1.5929356 -1.2947292 -1.3841734 -3.3144207 -3.9784653 -4.7916951 -6.2629085 -6.4048357 -7.8503027][-2.9778762 -2.0118465 -1.1770067 -1.3147683 -0.754179 -0.51325274 -0.30961227 0.097285271 0.067212105 -1.9896436 -2.6499538 -3.6321111 -4.8120766 -5.2388024 -6.7499409][-2.7495198 -2.193295 -1.3426843 -0.23623419 0.77191544 1.7611904 2.5296926 2.5333967 2.3997564 0.48219681 -0.59078217 -2.1429443 -3.6128578 -3.9668946 -5.0347219][-3.4010596 -2.6593871 -1.2388759 0.32136059 1.7185402 3.2490568 3.5202961 3.064435 2.724225 0.33846664 -0.62214756 -2.1852121 -3.8025951 -4.4056773 -5.9130878][-3.2660632 -2.4787917 -1.3100858 0.71756554 2.3180561 3.33712 4.0831537 3.7309036 2.9075422 0.25279093 -1.0534744 -2.9986815 -4.6184621 -5.0620303 -6.7305903][-3.3680038 -2.6639152 -1.4670849 0.17876577 2.0576515 2.4825048 2.8993349 3.1620188 3.2275677 0.2638588 -1.6546035 -3.2902708 -4.8403282 -5.3435235 -6.8237119][-4.4501648 -3.6165695 -2.4762406 -0.4905057 0.946887 1.5538063 1.5287237 1.63698 1.6198883 -1.0743971 -2.4857063 -4.4278255 -5.5914907 -5.606657 -7.0242796][-4.8034925 -4.1150775 -3.5611095 -2.213068 -1.2342372 -0.41161394 -0.23314905 -0.18941975 0.14105463 -2.5434103 -4.2395239 -5.4245138 -6.3775735 -7.0912323 -7.9758344][-5.3869343 -4.15305 -3.4826679 -3.0657196 -2.6568604 -2.3043671 -2.394454 -1.8575282 -1.8714447 -3.7482538 -4.5272613 -5.8274374 -6.4005132 -6.9539433 -8.0975494][-5.6598382 -4.5039444 -4.2816167 -4.0633831 -4.1196957 -3.9873126 -3.4462423 -3.85454 -3.9624782 -5.3408031 -6.1262631 -6.3690104 -6.85106 -7.2574911 -8.0330038][-5.7861366 -5.2162638 -4.9044843 -5.1883206 -5.2647533 -5.1054859 -5.2025194 -5.052763 -5.1268358 -6.1435785 -6.0762315 -6.5755939 -6.7378778 -6.9291725 -7.7301164][-6.0321503 -5.78528 -5.9076014 -5.7174869 -6.2244053 -6.5780854 -6.6221633 -7.0995092 -7.4250174 -7.1232986 -6.9786429 -7.0661983 -6.9709392 -7.0333638 -7.0681205]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 20:08:35.112017: step 55010, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 51h:35m:28s remains)
INFO - root - 2017-12-15 20:08:41.543976: step 55020, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 50h:25m:46s remains)
INFO - root - 2017-12-15 20:08:48.051204: step 55030, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 49h:23m:22s remains)
INFO - root - 2017-12-15 20:08:54.632555: step 55040, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 50h:09m:20s remains)
INFO - root - 2017-12-15 20:09:01.049564: step 55050, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 50h:17m:25s remains)
INFO - root - 2017-12-15 20:09:07.440791: step 55060, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 48h:56m:51s remains)
INFO - root - 2017-12-15 20:09:13.963607: step 55070, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 50h:45m:47s remains)
INFO - root - 2017-12-15 20:09:20.407821: step 55080, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 49h:36m:45s remains)
INFO - root - 2017-12-15 20:09:26.825116: step 55090, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 49h:07m:37s remains)
INFO - root - 2017-12-15 20:09:33.265853: step 55100, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.644 sec/batch; 49h:38m:51s remains)
2017-12-15 20:09:33.807949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1054325 -4.6488953 -5.7926126 -6.3415627 -6.3572073 -6.4580956 -6.192215 -5.3388181 -3.9558384 -3.7238321 -4.6018038 -5.8284936 -6.5393591 -6.8148308 -6.543797][-2.9796934 -3.7843134 -5.0342216 -5.9848218 -6.6499615 -6.2576437 -5.7370443 -5.1653562 -4.5190258 -4.3163877 -5.1699123 -6.5994482 -6.9719005 -7.3324437 -6.891397][-3.370327 -3.7733676 -4.1979456 -4.4846 -4.5723763 -4.375824 -4.0318222 -3.7542012 -3.7513735 -4.0069771 -5.32515 -6.5075946 -7.2035708 -7.7045336 -7.2149019][-3.8804128 -3.917603 -4.0289373 -4.0925837 -3.8669946 -3.4694757 -3.1062384 -3.0672069 -2.6831331 -2.6973367 -3.7684393 -5.5576611 -6.5418324 -7.37784 -7.8026686][-3.8853066 -4.2281075 -4.2389936 -3.4657426 -2.524663 -1.0559144 -0.54875278 -0.93107653 -2.0545888 -2.6388488 -3.3051915 -4.9395037 -5.8575912 -6.7804422 -7.1289349][-5.1546497 -4.7947855 -4.125752 -2.91858 -1.4000754 0.1494751 1.4516048 1.2037888 0.11939478 -1.1602097 -3.2364163 -4.8194036 -5.560771 -6.435441 -6.46618][-4.8834295 -4.7173953 -3.8081851 -2.2228198 0.230093 2.0957747 3.2649975 3.2960577 2.537322 0.7567358 -2.10156 -4.5311856 -5.4304295 -5.7973981 -5.5393295][-4.2184997 -3.8259439 -3.0775275 -1.5081863 1.2373075 3.7746544 4.7988253 4.5360622 3.6754045 1.7645798 -1.3997011 -4.00033 -5.108387 -5.6588774 -5.4084406][-4.7434368 -3.6573644 -2.5117421 -1.3526192 0.27123594 2.075511 3.6818838 4.1665564 3.3397064 1.2856092 -1.5595965 -4.0927744 -5.3372459 -5.8084555 -5.7504525][-6.0353918 -5.7698836 -4.8404827 -3.3638821 -2.5485506 -0.98015165 0.65047359 1.2916126 1.4350834 -0.56108713 -3.8814771 -6.1733861 -7.0418835 -7.0739889 -7.2143912][-7.6588554 -7.3240747 -6.6103725 -5.4386215 -4.3196077 -3.3721457 -2.6333017 -1.9240584 -1.2777214 -2.1504631 -4.1646042 -6.6249933 -7.40091 -7.6952329 -7.3340921][-7.7742615 -7.4523787 -7.2301044 -6.3712735 -6.2289734 -5.5415688 -4.5687361 -4.1086407 -4.221158 -4.5187025 -4.971127 -6.3113813 -6.8354883 -7.3271418 -7.3147445][-8.1981039 -7.5090604 -7.3451524 -6.6890621 -6.4876361 -6.4392238 -6.4275866 -6.1717391 -5.7532725 -6.0683279 -6.2352915 -7.063242 -7.3952146 -7.8465605 -7.5607209][-9.3289814 -8.6211329 -7.2874217 -6.7071538 -6.3662424 -6.2392168 -6.6380248 -7.427599 -7.6188841 -7.9055319 -7.2785006 -7.0744023 -7.0820684 -6.7682934 -6.6291518][-9.6393957 -9.2474651 -8.5471315 -8.0450239 -6.9221454 -6.7332163 -6.8823662 -7.1442776 -7.6910582 -7.9428191 -7.997345 -7.5841656 -6.8073368 -6.3832574 -5.9107089]]...]
INFO - root - 2017-12-15 20:09:40.243937: step 55110, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 49h:00m:05s remains)
INFO - root - 2017-12-15 20:09:46.704569: step 55120, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 50h:56m:36s remains)
INFO - root - 2017-12-15 20:09:53.088665: step 55130, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 49h:19m:56s remains)
INFO - root - 2017-12-15 20:09:59.454722: step 55140, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 48h:11m:12s remains)
INFO - root - 2017-12-15 20:10:05.857315: step 55150, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 49h:00m:06s remains)
INFO - root - 2017-12-15 20:10:12.257396: step 55160, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 48h:30m:44s remains)
INFO - root - 2017-12-15 20:10:18.625250: step 55170, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 51h:34m:15s remains)
INFO - root - 2017-12-15 20:10:25.062346: step 55180, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 49h:10m:47s remains)
INFO - root - 2017-12-15 20:10:31.479563: step 55190, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 48h:25m:27s remains)
INFO - root - 2017-12-15 20:10:37.863966: step 55200, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 48h:29m:40s remains)
2017-12-15 20:10:38.347944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0832343 -3.3879876 -3.6830463 -3.9379139 -4.179647 -4.2147713 -3.7844009 -3.1015615 -2.2988124 -3.4625597 -3.8433514 -5.2470713 -6.3412638 -7.4864922 -8.1914854][-3.9013462 -4.2191968 -4.4290104 -4.8824072 -5.0822306 -4.8984704 -4.3931942 -3.6151004 -2.642591 -3.8056695 -4.1122537 -5.606791 -6.8631434 -7.7123075 -8.3647366][-4.2947745 -4.2772503 -4.2051787 -4.3156853 -4.3237743 -3.9841378 -3.4013176 -2.904747 -2.0197711 -3.0176406 -3.6367412 -5.0351996 -6.1187029 -7.3979917 -8.1032629][-4.0744085 -3.8823946 -3.7636011 -3.3692112 -2.8911662 -2.4779654 -2.1022649 -1.9513812 -1.5189786 -2.7333856 -3.166831 -4.5124869 -5.2946181 -6.0957832 -6.8220549][-4.0324059 -3.3156991 -2.6397052 -2.214458 -1.7519407 -1.0921497 -0.6944952 -0.76468182 -0.48709011 -1.8809423 -2.2887459 -3.5838308 -4.5001583 -5.2430787 -6.054739][-3.816628 -3.1343107 -2.686245 -1.5916905 -0.57744694 0.23257065 0.67645073 0.69226265 0.833971 -0.70978212 -1.4056072 -2.6318765 -3.5848074 -4.4915981 -5.1694183][-3.8269942 -2.8727942 -2.1373582 -1.0060792 -0.18470478 0.762661 1.3550367 1.4250402 1.47087 -0.039230824 -0.80655479 -2.2672296 -3.275166 -4.365397 -5.1179056][-4.1629796 -3.018538 -1.8602109 -0.66879463 0.36054707 1.1974716 1.6764774 2.081665 2.3653374 0.64819717 -0.32750845 -1.7965088 -2.8774786 -4.1752577 -5.0101509][-3.8024108 -3.2548923 -2.4628954 -1.1298761 -0.15116453 0.8644762 1.3027182 1.6682081 1.8752079 0.23099804 -0.56026459 -1.911684 -2.9689088 -4.2382345 -5.1364479][-4.4301529 -3.6448436 -2.9990878 -1.9169927 -0.86927748 0.221663 0.83618832 1.2446423 1.4633818 -0.34154987 -1.3078065 -2.4464478 -3.215189 -4.1579227 -4.8674469][-5.8594527 -5.3311634 -4.6935215 -3.7545416 -2.5835295 -1.5298491 -0.78355551 -0.30064869 -0.051006794 -1.5977402 -2.4516807 -3.5636392 -4.2124214 -4.9955435 -5.1236134][-6.6171794 -6.3502688 -5.7416649 -5.0657544 -4.1768126 -3.111258 -2.1673307 -1.8370137 -1.7597251 -2.9894953 -3.5420871 -4.3060479 -4.9059734 -5.3063326 -5.5517426][-7.2576528 -7.0742526 -6.7664571 -6.083467 -5.2291522 -4.4017229 -3.6556916 -3.3893204 -3.4691505 -4.2038317 -4.5569811 -4.8038979 -5.0077696 -5.203145 -4.91137][-7.4148726 -7.3092442 -7.0871654 -6.5602055 -5.9183784 -5.1389656 -4.4705186 -4.4915104 -4.4551058 -4.7979288 -4.7833338 -4.6132441 -4.5973096 -4.7142282 -4.6954403][-8.009409 -8.101347 -7.7373676 -7.464221 -7.0403538 -6.3730125 -6.0631256 -6.0687776 -6.1217604 -6.0396137 -6.0350451 -5.8916531 -5.7710733 -5.54738 -5.2988076]]...]
INFO - root - 2017-12-15 20:10:44.844271: step 55210, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 50h:07m:45s remains)
INFO - root - 2017-12-15 20:10:51.268856: step 55220, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 50h:01m:09s remains)
INFO - root - 2017-12-15 20:10:57.751488: step 55230, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 49h:41m:57s remains)
INFO - root - 2017-12-15 20:11:04.209117: step 55240, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 48h:30m:52s remains)
INFO - root - 2017-12-15 20:11:10.655702: step 55250, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.624 sec/batch; 48h:03m:13s remains)
INFO - root - 2017-12-15 20:11:16.997504: step 55260, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:35m:12s remains)
INFO - root - 2017-12-15 20:11:23.405302: step 55270, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 50h:44m:10s remains)
INFO - root - 2017-12-15 20:11:29.796998: step 55280, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 48h:45m:26s remains)
INFO - root - 2017-12-15 20:11:36.122691: step 55290, loss = 0.26, batch loss = 0.14 (13.1 examples/sec; 0.612 sec/batch; 47h:08m:59s remains)
INFO - root - 2017-12-15 20:11:42.500984: step 55300, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 49h:03m:51s remains)
2017-12-15 20:11:43.032430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5775518 -4.8092837 -5.1813931 -4.8453217 -4.8996849 -4.1114774 -3.6835518 -2.6142998 -2.4315662 -3.8783889 -3.9088335 -4.582109 -5.6217213 -7.1960735 -8.3975992][-3.9529614 -4.1559157 -4.3264 -4.7271056 -4.7618222 -4.0535 -3.6297722 -2.6013227 -2.0966573 -3.6268139 -4.5769138 -5.9446268 -7.3108773 -8.4716349 -9.2602968][-4.5263548 -4.6034527 -4.9321136 -4.5177488 -4.1528778 -3.4547157 -2.888175 -2.2930679 -2.2230558 -3.6780477 -4.1795254 -6.0921259 -7.9679742 -9.7119818 -10.831049][-4.1797132 -4.483429 -4.7969232 -4.8845506 -3.9535625 -2.5810761 -1.2827067 -0.57800531 -0.35754728 -1.7570524 -3.1676769 -5.221312 -7.557755 -9.4502134 -10.587874][-3.0945039 -3.0815701 -3.6865802 -3.4450016 -2.5109115 -1.2479815 -0.19480371 0.20883799 0.44776726 -1.0886559 -1.8522911 -3.7304487 -6.2524724 -8.1848993 -9.5078487][-3.8030491 -3.4291072 -2.9979267 -2.3303952 -1.4148765 0.14663363 1.0470848 1.2726727 1.3739052 -0.02301836 -1.4573107 -3.4657631 -5.3865442 -6.585299 -7.8651681][-4.2843041 -3.4215164 -2.7366991 -1.6231589 0.049838543 1.5221739 2.8570595 2.8955536 2.9287796 1.0375166 -0.18211985 -2.0244546 -4.0346708 -5.5014439 -6.2922459][-3.6013694 -2.920413 -2.1545339 -0.56972218 0.80145168 2.4813614 3.9362335 3.9943676 3.8847198 2.0616455 0.71323013 -1.4442744 -3.5040979 -5.0845618 -5.7351756][-4.2134075 -3.3465362 -2.6161923 -0.96551275 0.42008209 1.6798916 3.1483393 3.6643496 3.7175112 1.6723957 0.2455616 -1.384829 -3.2954211 -4.9000654 -5.7152042][-4.538578 -3.9951644 -3.7534468 -2.7593107 -1.4843736 -0.14200354 1.6194839 2.2137175 2.6732845 1.0196362 -0.12270021 -2.0020661 -3.7583513 -4.9806776 -5.793746][-5.7894444 -5.6026378 -5.4983273 -4.6404772 -4.042922 -2.850615 -1.0585046 -0.342371 0.21387577 -1.1206369 -1.9185381 -3.4481297 -4.7102418 -6.0780487 -6.3293881][-5.823144 -5.4830704 -5.782918 -5.5486956 -5.0679684 -4.2334166 -2.5985065 -2.0597029 -1.7130799 -2.4382334 -3.1574111 -4.1256876 -5.6619592 -6.9393249 -7.3897882][-6.9154778 -6.5456591 -6.6019535 -6.346837 -6.0606966 -5.39732 -4.2025242 -3.9696436 -3.6233659 -4.2740664 -4.4628572 -4.851706 -5.6306438 -6.3626952 -6.9651747][-6.4325418 -7.0470052 -6.9769411 -6.5821586 -6.0574188 -5.2311139 -4.3710713 -4.3991919 -4.3104811 -4.5168009 -4.6840096 -5.3684025 -5.6927056 -5.9642019 -6.0944948][-6.6441779 -6.5373106 -6.6296287 -6.8877249 -6.1607561 -5.4414721 -5.0568628 -4.8614612 -4.6611547 -4.5352764 -5.0278358 -5.2599545 -5.4059668 -5.358222 -5.4420743]]...]
INFO - root - 2017-12-15 20:11:49.488089: step 55310, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:34m:35s remains)
INFO - root - 2017-12-15 20:11:55.864754: step 55320, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 48h:18m:06s remains)
INFO - root - 2017-12-15 20:12:02.297632: step 55330, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 48h:50m:10s remains)
INFO - root - 2017-12-15 20:12:08.747583: step 55340, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 50h:57m:22s remains)
INFO - root - 2017-12-15 20:12:15.184258: step 55350, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 50h:03m:14s remains)
INFO - root - 2017-12-15 20:12:21.550060: step 55360, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 48h:10m:06s remains)
INFO - root - 2017-12-15 20:12:27.994142: step 55370, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 51h:05m:14s remains)
INFO - root - 2017-12-15 20:12:34.472176: step 55380, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 50h:03m:08s remains)
INFO - root - 2017-12-15 20:12:40.825349: step 55390, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 50h:20m:49s remains)
INFO - root - 2017-12-15 20:12:47.282972: step 55400, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 49h:11m:55s remains)
2017-12-15 20:12:47.784878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9188375 -1.3528318 -1.9069357 -2.9738202 -3.2461538 -3.1633711 -3.1668172 -3.5706539 -4.2438984 -5.5401568 -6.6215124 -6.9693508 -7.7349944 -8.25125 -8.4718819][-3.2999892 -3.1518846 -2.9746747 -2.674861 -3.1660233 -3.6798263 -4.438354 -5.2044063 -5.2549219 -5.2267447 -5.8004627 -6.8334413 -7.8214073 -8.48284 -8.3650227][-2.5882382 -3.3170996 -3.7534497 -3.3704014 -3.2531085 -3.2113 -3.3274708 -3.6213236 -4.0714078 -4.5014191 -5.3209734 -5.7748756 -6.4369473 -6.9644275 -7.4983697][-2.5573211 -3.1199546 -2.6312275 -1.8206525 -2.1738749 -2.6775742 -3.2826304 -3.3814373 -3.2807236 -3.4073658 -4.8427238 -5.5570822 -5.9387736 -5.9749265 -6.6914797][-4.394948 -3.4262705 -1.6884952 -1.3428955 -1.1270394 -0.88775492 -0.95738506 -1.4492955 -1.7159967 -2.3513274 -3.73892 -4.364161 -4.7878938 -5.5400128 -5.9705176][-3.6486459 -3.2028074 -2.382864 -1.7532792 -1.0297298 -0.20846128 0.089548111 0.12534523 -0.083698273 -0.944994 -2.1722608 -2.7039123 -3.6581717 -4.5296946 -4.5203][-4.193397 -3.3315215 -2.1945868 -1.238462 -0.26084471 0.53162193 1.0253391 1.111269 1.1816025 -0.040516376 -1.6877446 -2.3101044 -3.2056026 -4.4412708 -4.9583168][-3.0493078 -2.5938621 -2.1115975 -0.95694065 0.87856007 2.2168055 2.6516266 2.3295832 1.6769505 0.97551441 -0.81273842 -2.3001232 -3.3962145 -4.5755782 -5.2976818][-2.4544668 -1.6159415 -0.92000389 0.37185287 1.2837286 2.0621319 2.0715218 1.6822567 1.6188135 0.64334679 -1.1983142 -2.5334344 -3.9788384 -5.3602462 -5.9891024][-2.9738212 -2.519505 -1.5859151 -0.4912796 0.48884869 1.0061083 1.2786808 1.1892786 0.94074535 -0.12584162 -1.7050691 -3.1139002 -4.446002 -5.4862309 -6.19028][-3.8542066 -3.1789985 -2.6953559 -2.0962873 -1.4379315 -0.99013615 -0.40171003 -0.01151228 0.17304707 -0.54740715 -2.324378 -3.396174 -4.2207465 -5.2390566 -5.9918628][-4.58776 -3.9085567 -2.9343328 -2.5928874 -2.1390343 -1.4913225 -1.2844529 -1.4578738 -1.5630717 -1.9860849 -3.0496397 -3.6372638 -4.585887 -5.7871819 -6.3933859][-4.7599258 -4.0588093 -3.8396928 -3.2514005 -3.0909896 -2.8361483 -2.6591921 -2.6187391 -2.5149813 -3.0377951 -4.0897951 -4.34324 -5.10636 -5.9782405 -6.6998954][-5.7096033 -5.8370676 -5.1725206 -4.7193928 -4.307725 -4.2219715 -4.1010618 -4.0777378 -4.0692596 -4.2953024 -4.5186706 -4.5755863 -5.0225024 -5.9896193 -6.5645323][-7.2717214 -6.7009954 -6.1046677 -6.2793951 -6.3145065 -6.0685472 -5.8804092 -5.8983393 -5.4800425 -5.1579666 -5.1064978 -5.1271439 -5.2547951 -5.58044 -5.9072247]]...]
INFO - root - 2017-12-15 20:12:54.166450: step 55410, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 50h:14m:34s remains)
INFO - root - 2017-12-15 20:13:00.502734: step 55420, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 48h:21m:05s remains)
INFO - root - 2017-12-15 20:13:06.905905: step 55430, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 48h:48m:01s remains)
INFO - root - 2017-12-15 20:13:13.285119: step 55440, loss = 0.35, batch loss = 0.23 (12.4 examples/sec; 0.646 sec/batch; 49h:44m:18s remains)
INFO - root - 2017-12-15 20:13:19.636139: step 55450, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 48h:56m:01s remains)
INFO - root - 2017-12-15 20:13:26.057268: step 55460, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 51h:30m:19s remains)
INFO - root - 2017-12-15 20:13:32.481870: step 55470, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 49h:26m:54s remains)
INFO - root - 2017-12-15 20:13:38.873750: step 55480, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 48h:24m:29s remains)
INFO - root - 2017-12-15 20:13:45.259614: step 55490, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 48h:54m:59s remains)
INFO - root - 2017-12-15 20:13:51.636299: step 55500, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 48h:58m:13s remains)
2017-12-15 20:13:52.128937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4434118 -6.2419844 -6.1503496 -5.110816 -3.975816 -2.9697261 -1.9425197 -1.8130655 -1.7827849 -2.676127 -3.7363906 -6.0351253 -6.7000322 -6.8610377 -6.8826323][-5.2585187 -5.6705828 -5.5212545 -4.725502 -4.1834192 -3.2405362 -2.7405047 -2.7401109 -2.8151746 -3.6572905 -4.8274779 -7.2807817 -7.7526808 -8.1673021 -8.2855759][-4.5487118 -4.2749844 -3.6704693 -3.1297083 -2.1522489 -1.4948192 -1.5011415 -1.5684128 -1.9005828 -2.9824357 -4.1648064 -6.4374886 -6.7762365 -7.0728207 -6.7602239][-2.7139235 -2.7855544 -2.6455235 -1.9036374 -0.65266609 0.25247669 0.65918446 0.56281948 0.044516563 -1.502398 -2.7220769 -5.0240936 -5.6560068 -6.2448897 -6.2778225][-1.7093506 -0.96828985 -0.36988831 0.27621746 0.73935032 1.7638512 2.2021217 1.797348 1.2132931 -0.45410824 -1.6931305 -3.6805367 -4.4013286 -5.3214264 -5.7625966][-2.1602578 -1.2236133 -0.24754 0.83383179 1.8116693 2.6505079 2.7908602 2.52147 2.1101418 0.57804012 -0.16696072 -1.8945479 -2.9902158 -4.015955 -4.5770183][-3.3606143 -2.2326555 -0.55820322 1.0350599 2.30865 3.044878 3.0980568 3.1311808 2.9245987 1.1011019 0.40986824 -1.3399668 -2.5573459 -3.9454169 -4.63925][-3.6288872 -2.535243 -1.0728111 0.84425545 2.43431 3.0836439 3.4169111 3.4919014 3.3220835 1.9428253 1.0602312 -1.2574844 -2.8414459 -4.4000964 -5.3879309][-3.7646737 -2.9411707 -1.6396251 -0.038781643 1.2859077 2.1905479 2.6930914 2.4144897 2.4260988 1.5517941 0.9954834 -1.3738866 -2.7814345 -4.0876513 -4.9511509][-4.92702 -3.8702729 -2.9387441 -1.6947665 -0.84076834 0.0031261444 0.64339447 0.9220171 1.1756363 -0.13099241 -0.79439592 -2.7047305 -3.734688 -4.6482964 -5.12061][-6.0593724 -5.2127218 -4.2925053 -3.0481839 -2.2553306 -1.7108951 -1.2755222 -0.85720205 -0.38571644 -1.7651892 -2.6432819 -3.6897545 -4.2452145 -5.3365064 -5.8194342][-6.2418671 -5.5060892 -4.4781761 -3.560554 -2.9962769 -2.1057062 -1.9439445 -1.9156642 -1.4727802 -2.25101 -2.7724442 -3.7687812 -4.46694 -5.068222 -5.8471127][-6.8379593 -6.1443167 -5.1635895 -4.0509071 -3.154871 -2.1300964 -1.7284093 -1.9495258 -1.8030233 -2.3028193 -2.4282961 -3.0525928 -3.5180659 -4.5237541 -5.7222791][-6.6600204 -6.1274757 -5.4046926 -4.50209 -3.3492265 -2.0978761 -1.4330058 -1.5116549 -1.4583697 -2.0005255 -1.7798696 -2.1787782 -2.7641788 -3.632256 -4.4471788][-8.0142841 -6.9465623 -5.5882044 -4.7725034 -4.0867186 -3.0259051 -2.2272363 -1.852118 -1.7412686 -1.9454279 -2.0116453 -2.6025476 -3.047545 -3.8329506 -4.5824862]]...]
INFO - root - 2017-12-15 20:13:58.567990: step 55510, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 49h:25m:54s remains)
INFO - root - 2017-12-15 20:14:04.949774: step 55520, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 48h:43m:39s remains)
INFO - root - 2017-12-15 20:14:11.485026: step 55530, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 50h:21m:03s remains)
INFO - root - 2017-12-15 20:14:17.888993: step 55540, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 49h:17m:06s remains)
INFO - root - 2017-12-15 20:14:24.245549: step 55550, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 49h:20m:52s remains)
INFO - root - 2017-12-15 20:14:30.628661: step 55560, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 48h:21m:15s remains)
INFO - root - 2017-12-15 20:14:37.098934: step 55570, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 49h:04m:11s remains)
INFO - root - 2017-12-15 20:14:43.484796: step 55580, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 48h:43m:38s remains)
INFO - root - 2017-12-15 20:14:49.892788: step 55590, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 51h:57m:31s remains)
INFO - root - 2017-12-15 20:14:56.333263: step 55600, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 48h:39m:52s remains)
2017-12-15 20:14:56.868584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1475763 -2.7682519 -3.4808302 -3.3220167 -3.0939775 -3.1636977 -3.458302 -3.5761833 -3.4415145 -4.0990515 -4.4294519 -5.1355572 -5.7661724 -6.9767466 -7.8352723][-2.7153263 -2.8183713 -3.2817578 -3.6874075 -4.157136 -4.19048 -4.2210617 -4.3148727 -4.3315821 -4.710104 -4.4818816 -5.0826712 -5.6711044 -6.6574278 -7.1056824][-1.403892 -1.0133958 -1.4304256 -1.6471701 -1.9951668 -2.360116 -3.0176539 -3.1049123 -3.1896405 -3.9859362 -3.9493454 -4.5883665 -5.1285963 -6.3851013 -6.7483411][-0.3339715 0.15402937 -0.22977018 -0.51133108 -0.99859858 -1.0162139 -1.1412764 -1.6785722 -2.1710877 -3.0228291 -3.024087 -3.8048263 -4.3355255 -5.3440237 -5.8108382][-0.60956478 0.092949867 0.093477726 -0.050153255 -0.33199644 0.24773169 0.80726624 0.3795557 -0.059541702 -1.0224843 -1.2787876 -2.1688137 -2.8925543 -4.2632895 -5.0250421][-1.5143232 -0.717751 -0.33738661 0.083191395 0.33449745 1.3750238 2.345089 2.3048649 2.1520967 0.74163818 0.12902927 -1.0348024 -2.232233 -3.928761 -5.0158691][-2.0857534 -1.3655 -1.0873022 -0.5209794 0.16869164 1.2224226 2.4222622 2.8232889 3.1694212 1.6349878 0.57855797 -0.74765253 -2.0997782 -3.7810543 -4.8397388][-2.9097576 -1.9960313 -1.4676104 -0.78053904 0.14805841 1.4341707 2.731411 3.4350739 4.1650858 2.6251888 1.5236635 -0.064291954 -1.5275278 -3.3416953 -4.6417694][-3.8247063 -2.8877521 -2.1316185 -1.5055995 -0.85596943 0.71117115 2.0781107 2.8658056 3.6473742 2.2026949 1.1203413 -0.45898438 -1.9597173 -4.0086603 -5.1824169][-4.4854255 -3.943974 -3.394877 -2.4260411 -1.5898175 -0.60807228 0.14640236 0.78885841 1.5066624 0.38859272 -0.5660305 -1.9265313 -3.2038846 -4.8370767 -5.9838548][-5.8706861 -5.4820633 -5.1220222 -4.2182436 -3.3708057 -2.3404503 -1.6839638 -1.4868941 -1.2709241 -2.3665724 -3.0750303 -3.8430953 -4.6407895 -6.2618132 -7.3338213][-7.5683246 -7.2942996 -7.0336471 -6.4753551 -5.8545012 -4.8443842 -4.2484379 -4.1388187 -4.0422649 -4.8668833 -5.3401923 -5.5409675 -6.0565929 -7.019455 -7.7955418][-7.8485227 -7.5442333 -7.5324955 -7.0917149 -6.5484657 -5.7846036 -5.2816591 -5.2161217 -5.191968 -5.3832865 -5.681673 -5.9365354 -6.2838979 -6.8188882 -7.3455935][-7.48399 -7.322969 -7.4616218 -7.3700552 -7.14891 -6.3188248 -5.7100005 -5.5642843 -5.5658503 -5.9842811 -6.1851768 -5.9776931 -5.9228029 -6.305171 -6.9518442][-7.4709353 -7.20214 -7.2259569 -7.1695905 -6.9523158 -6.1598411 -5.6335154 -5.4997969 -5.3460879 -5.5043621 -5.6202412 -5.7734742 -5.7835145 -5.7937884 -5.7610435]]...]
INFO - root - 2017-12-15 20:15:03.271890: step 55610, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 49h:08m:40s remains)
INFO - root - 2017-12-15 20:15:09.793280: step 55620, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 51h:12m:39s remains)
INFO - root - 2017-12-15 20:15:16.271154: step 55630, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 50h:40m:03s remains)
INFO - root - 2017-12-15 20:15:22.709100: step 55640, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 48h:22m:55s remains)
INFO - root - 2017-12-15 20:15:29.173212: step 55650, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 49h:10m:51s remains)
INFO - root - 2017-12-15 20:15:35.615773: step 55660, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 48h:25m:38s remains)
INFO - root - 2017-12-15 20:15:41.977955: step 55670, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 48h:36m:10s remains)
INFO - root - 2017-12-15 20:15:48.400698: step 55680, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 48h:31m:36s remains)
INFO - root - 2017-12-15 20:15:54.846691: step 55690, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 50h:28m:27s remains)
INFO - root - 2017-12-15 20:16:01.224182: step 55700, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 48h:43m:36s remains)
2017-12-15 20:16:01.775479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9970503 -4.778789 -3.7070577 -2.4934816 -1.516643 -0.026213169 1.0045528 1.3942337 1.3390036 -0.52990961 -1.0882087 -2.8040528 -4.5119472 -6.0788283 -6.9101539][-5.8923616 -4.9026093 -3.8795533 -2.8735352 -2.2248726 -1.2442279 -1.0692163 -0.6989603 0.000957489 -1.2003155 -1.4132047 -3.3836212 -4.8018932 -5.9294286 -6.7879434][-5.3820248 -5.2844543 -5.1453977 -4.5709343 -3.9889786 -3.0779657 -2.2266021 -1.7592244 -1.5250401 -3.242579 -3.6366858 -4.9145803 -5.5181713 -6.1973991 -6.8119745][-4.9501705 -4.4977522 -4.3919134 -4.0274334 -3.7793438 -3.5921044 -3.4417982 -3.2573943 -2.63379 -4.2777386 -3.8355634 -5.3394613 -6.1413946 -6.9563246 -7.6560936][-4.2972517 -3.4020863 -2.8527284 -2.6775241 -3.0027332 -2.6305532 -2.2337098 -2.5227532 -2.439786 -4.1144047 -3.7277799 -5.1264114 -6.1251812 -6.9324789 -7.2728052][-4.3075848 -3.1920481 -2.8435636 -1.9289279 -1.0421996 -0.10237074 -0.005692482 -0.47674608 -0.76838779 -2.888751 -2.7925134 -4.6245317 -6.0191035 -6.90282 -7.4940066][-3.0886831 -2.3361416 -1.4409528 -0.3158164 0.33216 0.8910656 1.2588453 1.5931473 1.6001196 -0.651875 -0.90787792 -2.7740812 -4.3613548 -5.4234858 -5.9878778][-2.4512959 -1.0538287 0.42882729 1.2538176 1.8757305 2.2155409 2.0213728 2.2398586 2.5780249 0.48017216 0.17482519 -1.4574962 -3.3032794 -4.3981247 -5.20496][-0.87876654 -0.13767338 0.071168423 1.0772104 1.6069136 1.796999 2.1814852 2.7227125 2.7335072 0.79715633 0.45605564 -1.2310615 -2.7906313 -3.5616984 -4.3545837][-1.1934581 0.097141743 0.51469135 0.7762413 1.37714 1.5861149 1.7704573 1.9560852 2.3664894 0.56380749 0.10527563 -0.8821826 -2.2640228 -3.0690036 -3.7738366][-2.1550164 -1.5041122 -1.9275775 -1.3462429 -0.47629166 -0.18421745 0.38832474 1.0411358 1.6858158 -0.10072041 -0.71912956 -1.7682748 -2.7788768 -3.22058 -3.5603371][-6.3063807 -5.9583354 -5.3671484 -4.3772364 -2.6543856 -1.3939714 -0.07848978 0.53582191 0.88826084 -0.25515223 -0.64133406 -2.2040477 -3.1160827 -3.6214342 -4.405839][-7.8406219 -7.7155051 -7.7973328 -6.5367708 -4.9299374 -3.1003623 -1.5480857 -0.442585 0.31008148 -0.0013160706 -0.2799325 -1.1024728 -2.2112236 -2.7704697 -3.3205919][-10.069019 -9.361412 -8.7029915 -8.0258274 -6.3107433 -4.4033217 -3.3236084 -2.535748 -1.6738877 -1.4008837 -1.6749759 -2.077991 -2.9637742 -3.2407703 -3.6925137][-9.6642733 -9.8219213 -9.5002937 -8.735323 -7.4273834 -5.952023 -4.8189878 -4.1440506 -3.7475841 -3.1587915 -2.9796014 -3.3434458 -3.6463046 -4.4491196 -4.6741862]]...]
INFO - root - 2017-12-15 20:16:08.281498: step 55710, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 49h:39m:47s remains)
INFO - root - 2017-12-15 20:16:14.654981: step 55720, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 48h:59m:16s remains)
INFO - root - 2017-12-15 20:16:21.005446: step 55730, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 47h:52m:31s remains)
INFO - root - 2017-12-15 20:16:27.415812: step 55740, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 48h:46m:46s remains)
INFO - root - 2017-12-15 20:16:33.826617: step 55750, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 49h:19m:47s remains)
INFO - root - 2017-12-15 20:16:40.195333: step 55760, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 49h:21m:06s remains)
INFO - root - 2017-12-15 20:16:46.547267: step 55770, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 48h:02m:10s remains)
INFO - root - 2017-12-15 20:16:52.944519: step 55780, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:28m:15s remains)
INFO - root - 2017-12-15 20:16:59.378219: step 55790, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 48h:52m:19s remains)
INFO - root - 2017-12-15 20:17:05.774051: step 55800, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 50h:33m:10s remains)
2017-12-15 20:17:06.266544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3503642 -2.8577237 -2.3923774 -1.8428378 -1.4414802 -1.0058146 -0.95670223 -1.1336632 -1.2701898 -2.3696637 -3.5322495 -4.4110565 -5.6277943 -6.859879 -7.9377003][-2.8244267 -2.5227585 -2.0941515 -1.7335052 -1.5143642 -1.1447978 -0.83325195 -0.91593933 -1.1352811 -2.5267286 -3.8382225 -5.0676 -6.3191233 -7.2378678 -7.8428626][-2.2314658 -1.7348471 -1.0992565 -0.60970545 -0.23746157 -0.016788483 0.031532764 -0.22549582 -0.48577309 -1.819562 -3.1635957 -4.6630116 -5.8066988 -6.7980862 -7.4646258][-1.3951755 -0.82704592 -0.42333698 0.0091719627 0.35921574 1.1852684 1.5579882 1.2741375 1.1452723 -0.15197086 -1.7957668 -3.4200306 -5.0278168 -5.9343624 -6.5977683][-1.6829424 -1.0057445 -0.22024202 0.17842293 0.57416916 1.3602943 2.1740808 2.4232569 2.5307531 1.1388588 -0.63544607 -2.4587946 -4.0958157 -5.5514464 -6.4715834][-1.8813224 -1.2121015 -0.78805733 0.0737257 1.0949345 2.0613928 2.8111734 3.3838053 3.7305889 2.4874287 0.60792542 -1.3339014 -3.1858044 -4.628953 -5.7195344][-2.0282006 -1.3564453 -0.52577734 0.10205889 1.0087414 2.0769348 3.321949 3.9543095 4.4383154 2.9592104 1.0215712 -0.76643562 -2.474875 -4.0581141 -5.5378809][-2.3630285 -1.8230033 -0.877635 0.14767122 1.2886372 2.0820684 3.0640526 3.9611206 4.5892839 3.0868082 1.1016455 -0.65493059 -2.2306948 -3.7805245 -5.1546555][-1.818274 -1.4968448 -0.97670841 -0.0024409294 0.95541668 1.9765224 3.0296173 3.4812984 3.8016748 2.3826208 0.44492817 -1.0083132 -2.3690662 -3.6381044 -4.9304519][-2.2310205 -1.4529338 -0.89461231 0.050913811 0.71824265 1.335866 2.0579109 2.3152847 2.3560076 1.0603466 -0.52672243 -1.8054023 -3.1260762 -4.1885457 -5.1360531][-2.6431994 -2.135612 -1.7191954 -0.93083239 -0.33203554 0.24215984 0.74643993 0.69413185 0.526536 -0.84794521 -2.2073393 -3.0568047 -3.9843886 -4.5827923 -5.3223915][-4.1106815 -3.4980226 -2.973597 -2.51751 -1.8198824 -1.3969221 -0.99733639 -0.98581648 -1.203886 -2.2976608 -3.5727019 -4.1740379 -4.611547 -4.8914385 -5.5700259][-5.4375544 -4.8654 -4.1955442 -3.3748455 -2.6862798 -2.1892071 -2.027781 -2.4264755 -2.7595916 -3.4796047 -4.2102642 -4.6318521 -5.2012792 -5.2995911 -5.6298919][-5.8120279 -5.1074524 -4.6655788 -4.1181564 -3.6091557 -2.9626679 -2.7054548 -2.9643836 -3.2752147 -3.6752682 -4.1912823 -4.363205 -4.5900278 -4.7830954 -5.0275164][-6.5787444 -5.7307358 -4.9611964 -4.5437589 -4.2434306 -3.9357312 -3.7195103 -4.02641 -4.4368992 -4.2547388 -3.9955759 -4.0758071 -4.3494759 -4.5544219 -4.6230583]]...]
INFO - root - 2017-12-15 20:17:12.655541: step 55810, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 49h:04m:40s remains)
INFO - root - 2017-12-15 20:17:19.153774: step 55820, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 50h:17m:00s remains)
INFO - root - 2017-12-15 20:17:25.586875: step 55830, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 49h:22m:39s remains)
INFO - root - 2017-12-15 20:17:32.049242: step 55840, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 49h:00m:50s remains)
INFO - root - 2017-12-15 20:17:38.445654: step 55850, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 48h:56m:30s remains)
INFO - root - 2017-12-15 20:17:44.815613: step 55860, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 48h:53m:10s remains)
INFO - root - 2017-12-15 20:17:51.192019: step 55870, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 48h:37m:49s remains)
INFO - root - 2017-12-15 20:17:57.555301: step 55880, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.636 sec/batch; 48h:51m:59s remains)
INFO - root - 2017-12-15 20:18:03.882995: step 55890, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 48h:27m:50s remains)
INFO - root - 2017-12-15 20:18:10.326068: step 55900, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 48h:17m:14s remains)
2017-12-15 20:18:10.830502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3063684 -2.1408086 -2.0222745 -2.4028788 -2.5249615 -2.2330232 -1.8085089 -1.7448807 -1.8356667 -2.040462 -2.6867638 -4.056675 -5.0127087 -6.4627867 -7.5430045][-2.501215 -2.1594267 -2.255146 -2.9010868 -3.02252 -2.4912558 -2.1213799 -2.183084 -2.2468624 -2.6594028 -3.2520223 -4.6601095 -5.5101337 -6.9589014 -7.8081069][-1.8707566 -1.592123 -1.6216512 -1.8858948 -1.8221121 -1.4862447 -1.1840749 -1.3233066 -1.4471121 -1.9003439 -2.5843329 -4.1614285 -4.9793038 -6.45989 -7.2908449][-1.8519344 -1.3927937 -1.2328186 -1.3809099 -1.2217851 -0.91918039 -0.61543036 -0.76628351 -0.835629 -1.2904797 -1.974153 -3.2409167 -3.9908104 -5.4144616 -6.3469439][-2.1408505 -1.2329597 -0.906795 -0.95342588 -0.87188148 -0.33752251 0.312006 0.28207541 0.21092415 -0.38537598 -1.156702 -2.4853191 -3.7825625 -5.4908867 -6.6591234][-1.5306005 -0.79870796 -0.48840427 -0.3167491 0.021192551 0.70727634 1.2988081 1.1266422 1.0357027 0.3614769 -0.31307745 -1.4387174 -2.6791959 -4.5573659 -5.8553338][-2.4621158 -1.3929024 -0.73189354 -0.0934577 0.40052795 0.96813107 1.7084255 1.6248894 1.4486628 0.72590733 0.017137051 -1.249959 -2.5860395 -4.5030847 -5.7890649][-2.0932503 -1.4512663 -0.91091013 -0.30178881 0.36805916 1.3493347 2.1177139 2.0826788 2.0113096 1.1846266 0.36920452 -0.95748615 -2.3800807 -4.2281141 -5.3903303][-1.8022208 -0.89213467 -0.5052166 0.085154533 0.47011471 1.1198158 1.8010302 1.9430561 1.9649763 1.1440678 0.28099442 -1.2091384 -2.6977243 -4.3683138 -5.417913][-1.8767195 -1.0107627 -0.4698205 -0.026653767 0.25139856 0.94474888 1.6476727 1.5296144 1.4563122 0.57135868 -0.37904787 -1.8901825 -2.93582 -4.2799516 -5.3690143][-2.9292169 -2.2899833 -1.6753826 -1.439415 -1.2346702 -0.69033384 -0.21342564 -0.30449295 -0.40267277 -1.5266261 -2.0622635 -2.9475546 -3.6128955 -4.3844275 -5.1628809][-4.9180069 -4.3588853 -3.9003775 -3.7220047 -3.9478307 -3.578958 -2.99184 -3.2000399 -3.1935754 -3.5272956 -3.555027 -3.9160445 -4.3883896 -5.0686903 -5.4903479][-6.62516 -6.3225679 -6.2669158 -6.4524617 -6.5655775 -6.2180872 -5.4480147 -5.0349426 -4.5559883 -4.6872344 -4.7290092 -4.8609223 -4.8311315 -4.9980259 -4.9090614][-7.2776852 -7.2207556 -7.2641478 -7.0685096 -7.1870947 -6.9206467 -6.3131227 -6.1730976 -6.1146793 -6.1730161 -5.771328 -5.3417873 -5.0735536 -4.9232168 -4.8050423][-8.6382046 -8.2147493 -8.0325623 -8.1717186 -8.2899141 -8.0683956 -7.8255844 -7.8740778 -7.7640929 -7.455008 -7.1639223 -6.6607695 -6.2642527 -5.8715997 -5.6833487]]...]
INFO - root - 2017-12-15 20:18:17.257893: step 55910, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 49h:30m:10s remains)
INFO - root - 2017-12-15 20:18:23.644813: step 55920, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 49h:52m:05s remains)
INFO - root - 2017-12-15 20:18:30.058923: step 55930, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 50h:42m:59s remains)
INFO - root - 2017-12-15 20:18:36.551620: step 55940, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:29m:01s remains)
INFO - root - 2017-12-15 20:18:42.905448: step 55950, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 49h:06m:50s remains)
INFO - root - 2017-12-15 20:18:49.325912: step 55960, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 49h:40m:58s remains)
INFO - root - 2017-12-15 20:18:55.727890: step 55970, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 48h:19m:45s remains)
INFO - root - 2017-12-15 20:19:02.150010: step 55980, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.614 sec/batch; 47h:09m:56s remains)
INFO - root - 2017-12-15 20:19:08.503215: step 55990, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 48h:58m:32s remains)
INFO - root - 2017-12-15 20:19:14.871095: step 56000, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 49h:13m:44s remains)
2017-12-15 20:19:15.366683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9074783 -6.152833 -6.56957 -5.7284875 -4.8068857 -3.6504602 -2.4462481 -1.8860869 -1.5244551 -1.7700777 -2.5583644 -4.5500736 -5.1767149 -6.2671518 -7.6529946][-5.6314039 -5.7604637 -6.9599838 -6.6768012 -5.5136876 -4.3444381 -2.8644609 -2.2324238 -2.3697286 -3.0924807 -3.6541333 -5.4141779 -6.1264162 -7.1161842 -7.7885327][-5.2958641 -5.499855 -6.3040628 -6.3426423 -6.1025152 -5.308301 -3.5680828 -2.8492732 -2.9108057 -3.5395966 -4.567771 -6.2215548 -6.8591924 -7.9992566 -8.6943407][-5.5934639 -5.6101623 -6.0083213 -6.0717998 -5.5406828 -4.6675014 -3.5822887 -3.0349889 -2.9252768 -3.8042116 -4.653429 -6.5565548 -7.0865932 -7.9608097 -9.3649015][-6.2906661 -6.7338028 -6.5290112 -6.0516582 -5.146997 -3.749229 -2.6625795 -2.473527 -2.4053226 -3.0844951 -3.7384186 -5.6952496 -6.44221 -7.9204831 -9.1893806][-7.3564496 -7.2736478 -7.1088338 -5.8402791 -4.1597652 -2.2492032 -0.819726 -0.82332516 -1.1151161 -2.306746 -3.1409955 -5.0416918 -5.70541 -7.6132679 -9.0315933][-7.3027511 -6.9122243 -5.8673639 -4.8524532 -3.4597211 -1.3974919 0.78114223 1.505599 0.95240974 0.2775588 -0.82077694 -3.3555813 -4.4123898 -6.5044422 -8.2240753][-6.8160567 -6.5558057 -5.8224707 -3.4371915 -1.3066425 0.40333271 1.7237568 2.5568743 3.7009974 2.676899 0.92039871 -1.3179941 -2.9731183 -5.3542328 -7.2039566][-5.7873006 -5.4249864 -5.5694633 -4.5935946 -2.6742539 -0.10528946 1.4065027 1.8316689 2.6198568 2.7345667 2.4078112 -0.28115988 -1.4197664 -3.7882307 -6.2972589][-6.2546654 -5.6266212 -5.3647962 -4.3957062 -4.0654182 -2.4278522 -0.87798119 0.31138515 1.0327282 0.21987295 0.11518908 -1.1279812 -1.8803754 -3.9794703 -5.5492163][-5.8551893 -5.9380112 -5.7277179 -4.9472837 -4.3588963 -3.495976 -2.6751413 -1.9633017 -1.2244592 -1.4090557 -2.9355359 -4.1310291 -3.9566665 -4.9730754 -5.6483064][-5.4075842 -5.3319669 -5.0164537 -4.4195395 -3.7249141 -2.6235108 -1.4551992 -1.4392605 -1.5472469 -2.488122 -3.5147882 -5.1452351 -5.9877558 -6.86821 -6.9113808][-6.5152111 -5.558629 -4.7922363 -4.2305503 -3.8909116 -2.9816008 -1.9532552 -1.581656 -1.2005744 -2.5241261 -3.7500577 -4.97723 -5.1119971 -6.281951 -7.2080007][-7.2075124 -6.2100282 -5.0013542 -4.0277624 -3.6220479 -3.1939054 -2.3991833 -2.2125311 -1.7494717 -1.9589977 -2.484796 -4.2549953 -4.7013712 -5.2256069 -5.9387922][-7.4307208 -6.7264318 -5.8067093 -5.1028214 -4.2871656 -3.8356049 -3.3549013 -3.3086228 -3.5533094 -3.5761151 -3.91945 -3.8207886 -4.2450829 -5.131084 -5.5526867]]...]
INFO - root - 2017-12-15 20:19:21.779228: step 56010, loss = 0.29, batch loss = 0.18 (13.1 examples/sec; 0.611 sec/batch; 46h:53m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 20:19:28.212399: step 56020, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 48h:47m:21s remains)
INFO - root - 2017-12-15 20:19:34.638199: step 56030, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 49h:37m:51s remains)
INFO - root - 2017-12-15 20:19:41.034320: step 56040, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 48h:06m:30s remains)
INFO - root - 2017-12-15 20:19:47.462106: step 56050, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 48h:19m:47s remains)
INFO - root - 2017-12-15 20:19:53.895028: step 56060, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 50h:58m:52s remains)
INFO - root - 2017-12-15 20:20:00.427363: step 56070, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 51h:42m:56s remains)
INFO - root - 2017-12-15 20:20:06.987297: step 56080, loss = 0.25, batch loss = 0.13 (11.8 examples/sec; 0.678 sec/batch; 52h:03m:59s remains)
INFO - root - 2017-12-15 20:20:13.378725: step 56090, loss = 0.23, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 49h:30m:15s remains)
INFO - root - 2017-12-15 20:20:19.873303: step 56100, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 48h:38m:21s remains)
2017-12-15 20:20:20.419334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0056167 -5.6777611 -4.708303 -4.7831378 -4.9636869 -4.0765467 -3.0075421 -3.3692908 -3.5861092 -5.1033645 -4.9071407 -5.3650875 -5.5539427 -6.0118208 -6.5732732][-6.4480839 -6.3028741 -5.9130936 -4.9627733 -3.9994819 -4.1352854 -3.7321548 -3.2024102 -2.7577205 -4.7691708 -5.62349 -5.897 -6.2179894 -6.4092565 -6.1594582][-7.5251656 -6.925931 -5.9658895 -4.9824886 -3.8592358 -3.3933582 -3.0920467 -3.2644162 -3.1292214 -4.7055006 -5.5979729 -6.1235914 -6.6818814 -6.7277427 -6.8559484][-7.3587742 -6.1850767 -5.5930538 -4.5104189 -3.2089343 -2.0684624 -1.8850703 -1.6874504 -1.6457119 -3.5701561 -4.675231 -5.3391895 -6.0757346 -6.7086024 -6.3605032][-7.3072515 -5.2341704 -3.1751742 -2.3558593 -2.1961989 -1.5921817 -0.93790436 -0.75484848 -1.5847468 -3.3840475 -4.0955629 -4.524929 -4.9681892 -6.1768546 -6.5797119][-4.2752023 -3.6739593 -3.8209114 -2.3529305 -0.45752716 0.40093136 0.3992672 0.41744041 0.20403719 -2.114922 -3.644804 -3.6643739 -3.896508 -5.1409483 -5.4989996][-3.3712626 -1.6896553 0.44952583 1.1619024 0.56138515 0.95089531 1.4322863 0.87836742 0.40830708 -1.4257469 -2.3006458 -3.0281005 -3.7479982 -4.4781837 -4.6825447][-1.7093925 -1.4624953 -1.2441735 0.082378387 1.857954 2.2186251 2.3647652 1.6605844 1.0029249 -1.500639 -2.9569783 -3.4290476 -3.5580583 -4.3107452 -4.9159184][-1.4617839 -0.634789 0.26497078 1.1029377 0.82686424 1.0633163 1.400815 1.3441944 0.96456623 -1.2656837 -2.4389687 -3.5246515 -4.4699116 -4.7637529 -4.8297405][-1.8087907 -1.7622542 -1.895865 -0.86761379 0.31937027 0.67061234 0.51216412 -0.043519497 -0.57391596 -2.1177263 -3.8655446 -4.4084253 -4.6939249 -5.7649555 -5.8538404][-5.0446453 -4.3093748 -3.3588805 -2.9149337 -2.5988622 -2.2208757 -2.332912 -2.6450858 -3.0439296 -4.2855482 -5.2373934 -5.6157122 -6.3744659 -6.4562531 -6.5415659][-5.2327313 -5.2886009 -5.2775397 -4.7492332 -3.7979257 -2.8527646 -2.5803852 -3.35676 -4.0374374 -4.8996878 -5.9456658 -6.1750212 -6.9369183 -7.2117667 -7.4891424][-4.8296 -4.8236084 -4.8832626 -4.7484522 -4.6888494 -4.5982866 -4.0157557 -3.9582584 -4.503139 -5.454423 -6.69066 -7.15057 -7.5114188 -7.407918 -7.5443664][-5.64866 -5.26336 -5.0810285 -5.05395 -4.0778465 -3.3645787 -3.45574 -3.1968069 -3.3193221 -3.6405859 -4.8502979 -5.6700048 -6.5125136 -6.9616709 -7.1819372][-5.7295909 -5.2704711 -4.8050795 -4.4370689 -4.3576727 -3.850229 -3.4887366 -3.6991904 -3.8927674 -4.2924957 -5.0525885 -5.6938686 -6.6301842 -7.1379995 -6.9532967]]...]
INFO - root - 2017-12-15 20:20:26.776514: step 56110, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 47h:57m:33s remains)
INFO - root - 2017-12-15 20:20:33.226677: step 56120, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 49h:26m:41s remains)
INFO - root - 2017-12-15 20:20:39.598962: step 56130, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 48h:37m:03s remains)
INFO - root - 2017-12-15 20:20:46.040471: step 56140, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 48h:57m:01s remains)
INFO - root - 2017-12-15 20:20:52.416153: step 56150, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 48h:48m:42s remains)
INFO - root - 2017-12-15 20:20:58.871742: step 56160, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 49h:13m:03s remains)
INFO - root - 2017-12-15 20:21:05.272122: step 56170, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 48h:33m:35s remains)
INFO - root - 2017-12-15 20:21:11.592336: step 56180, loss = 0.30, batch loss = 0.18 (13.0 examples/sec; 0.618 sec/batch; 47h:24m:10s remains)
INFO - root - 2017-12-15 20:21:17.987551: step 56190, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:22m:12s remains)
INFO - root - 2017-12-15 20:21:24.540035: step 56200, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 50h:24m:44s remains)
2017-12-15 20:21:25.056866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7550564 -4.6931167 -4.0835657 -4.3341489 -4.3577428 -2.830174 -2.2282596 -2.2999434 -2.6356893 -3.5660586 -4.1268916 -3.5647159 -3.5869608 -4.7869244 -6.98265][-4.47524 -4.7424655 -4.729836 -3.2710214 -3.500216 -4.2034082 -3.6494665 -2.9449763 -2.3004937 -3.1201367 -4.2388821 -4.8635712 -4.88507 -5.4885178 -7.7782574][-5.860806 -4.0968304 -3.3251085 -2.76891 -2.2752786 -1.959631 -2.143168 -2.4282002 -2.5189209 -3.8039789 -4.3484278 -4.6565194 -4.827673 -5.2889175 -7.6734638][-4.4074545 -3.7345893 -2.4432487 -1.5481672 -2.0719447 -1.9477568 -0.52222347 -0.8851738 -1.6610899 -2.667161 -4.1008396 -5.1707935 -5.3617945 -5.2140851 -7.0156407][-6.3583837 -5.0716658 -2.4239883 -0.7644186 -0.7770896 -0.62981081 -0.061242104 -0.26422548 -0.4074192 -1.959126 -3.4160304 -4.2225943 -4.869256 -5.5505829 -7.6244063][-3.9838843 -3.1118546 -2.4974246 -0.51066303 0.80670261 1.528101 2.4367924 1.3450203 0.42166233 -0.9883585 -2.377275 -3.5577168 -4.251164 -4.9034815 -6.9360857][-4.6318488 -2.1940193 0.70229053 2.1138754 2.671773 3.5081596 3.7267866 3.0881319 2.5479212 0.027282238 -2.11242 -2.8343935 -3.6629295 -4.7887115 -7.4521751][-2.816298 -1.3327479 0.20719147 2.3149643 3.8556652 3.4924564 3.3636351 2.4685516 1.6287661 -0.018334389 -1.7759624 -3.3427472 -3.7608418 -4.2442236 -7.1895676][-2.182713 -1.8503318 -0.84426165 1.3581219 2.3426228 2.0043745 2.142437 2.2143002 2.3501415 -0.012540817 -2.1937084 -3.223412 -3.8358295 -4.8826447 -7.3378735][-4.2046757 -3.4443932 -2.0573006 -0.79949903 0.76114559 1.2333918 1.7025681 0.79275322 0.37208843 -0.57660151 -1.696157 -2.9855084 -4.063345 -4.717844 -7.2771254][-5.3429079 -5.056231 -4.6317506 -3.9863698 -3.3642521 -2.05049 -0.72874069 -0.90543938 -1.2812362 -3.173912 -4.45611 -4.3902984 -4.8926764 -6.2316823 -7.9263787][-8.0692835 -6.8403931 -5.475174 -5.3713903 -5.1822996 -4.8946085 -3.8657033 -3.3768377 -3.3698835 -4.1388664 -4.8318615 -5.5842948 -6.6319933 -6.7297697 -7.9295521][-8.4674015 -8.1094227 -8.3665161 -7.0841331 -6.6323671 -6.5047355 -5.7925668 -5.6990089 -5.3906245 -5.9115839 -6.5596485 -7.1827469 -7.2202353 -7.2131405 -7.7074871][-8.9851589 -9.2677 -8.2469244 -7.9739852 -7.9046617 -7.6260724 -6.6529846 -6.5004044 -6.2562876 -6.087863 -6.5717688 -6.9155917 -7.1399374 -6.5145664 -6.7122684][-8.303751 -7.7690454 -8.4667139 -8.2364359 -7.61967 -7.1779027 -7.0987859 -7.2605577 -6.7685909 -6.54932 -6.8567691 -6.9302688 -6.5121822 -6.6394968 -6.4052858]]...]
INFO - root - 2017-12-15 20:21:31.468585: step 56210, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 49h:43m:20s remains)
INFO - root - 2017-12-15 20:21:37.807437: step 56220, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 50h:11m:50s remains)
INFO - root - 2017-12-15 20:21:44.203467: step 56230, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 47h:38m:24s remains)
INFO - root - 2017-12-15 20:21:50.661528: step 56240, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.659 sec/batch; 50h:35m:56s remains)
INFO - root - 2017-12-15 20:21:57.071960: step 56250, loss = 0.37, batch loss = 0.26 (12.4 examples/sec; 0.646 sec/batch; 49h:35m:00s remains)
INFO - root - 2017-12-15 20:22:03.436190: step 56260, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 48h:44m:34s remains)
INFO - root - 2017-12-15 20:22:09.895155: step 56270, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 48h:35m:25s remains)
INFO - root - 2017-12-15 20:22:16.329641: step 56280, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 47h:39m:15s remains)
INFO - root - 2017-12-15 20:22:22.786095: step 56290, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 49h:03m:36s remains)
INFO - root - 2017-12-15 20:22:29.183495: step 56300, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.639 sec/batch; 48h:59m:16s remains)
2017-12-15 20:22:29.697939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4925575 -2.6596589 -2.6918721 -2.9948192 -3.2617192 -3.2865682 -2.8784404 -2.2967677 -1.9711051 -3.2952394 -3.7156758 -4.6834054 -5.7703791 -6.8778934 -7.824326][-3.3178449 -3.5740275 -3.5970306 -4.1555042 -4.3490176 -4.1590757 -3.733521 -3.1035576 -2.5960712 -4.0499468 -4.3277063 -5.5730419 -6.7198229 -7.5168643 -8.2736588][-3.5277057 -3.4711471 -3.2377653 -3.4541454 -3.387239 -3.2330127 -2.9419789 -2.5241241 -2.1359348 -3.4003916 -3.8087363 -4.768292 -5.7629795 -6.987844 -7.9408336][-3.6674738 -3.4623156 -3.1265917 -2.8186717 -2.3827624 -2.1655216 -2.007998 -1.9224997 -1.7007704 -3.2320342 -3.7791402 -4.8672533 -5.6286449 -6.4011478 -7.2683554][-3.8033445 -3.1561894 -2.5598974 -2.0585136 -1.6487684 -1.0948124 -0.7663188 -0.89533758 -0.72116375 -2.1911392 -2.6511898 -3.7533121 -4.7965231 -5.3260889 -6.2494884][-3.9133716 -3.185564 -2.6035781 -1.5628319 -0.58181047 0.11279774 0.52522755 0.56947994 0.689085 -0.95507 -1.6419725 -2.6978846 -3.7722611 -4.6194644 -5.472085][-3.8542323 -3.0757027 -2.4088335 -1.1913462 -0.18397379 0.71940041 1.3000221 1.3167439 1.4008703 -0.24820089 -1.0834823 -2.4026847 -3.6332493 -4.5704751 -5.3876934][-4.1562719 -3.1748066 -2.2162323 -0.96427727 0.20591497 1.1515903 1.8137026 2.2415771 2.5498705 0.73199749 -0.24839544 -1.7436781 -3.1738629 -4.3920064 -5.28187][-3.8623354 -3.3428473 -2.6461258 -1.3670545 -0.38107681 0.684186 1.1999712 1.693079 2.086729 0.48894596 -0.29834652 -1.7166533 -3.0262275 -4.3087025 -5.3916254][-4.4032164 -3.671968 -3.0725017 -2.0729189 -0.993937 0.131917 0.92828465 1.4101181 1.7070141 -0.032034874 -0.97360849 -2.2070012 -3.2529917 -4.2224369 -5.2130418][-5.9666204 -5.4537992 -4.7734632 -3.8094547 -2.6072578 -1.5834093 -0.77656507 -0.27484417 0.045493603 -1.5020719 -2.4823408 -3.6016216 -4.4364033 -5.2086487 -5.6062651][-6.6954908 -6.257905 -5.7300925 -5.0094471 -4.0516391 -3.0259776 -2.2974858 -2.0087438 -1.811974 -2.9030614 -3.5201778 -4.3597593 -5.1343741 -5.7512007 -6.2823896][-7.03585 -6.8740511 -6.5012569 -5.7603755 -5.0295362 -4.2804136 -3.8643806 -3.7198348 -3.7365146 -4.3650875 -4.6788616 -4.9427629 -5.2837257 -5.3842716 -5.5909882][-7.0921512 -6.9420557 -6.7274103 -6.1541605 -5.6128807 -4.8482351 -4.5174236 -4.6335888 -4.5962849 -5.0447249 -5.0789003 -4.9693556 -5.0480084 -4.9879355 -5.2169271][-7.966464 -7.7411261 -7.2153811 -7.0752668 -6.6879196 -6.3094912 -6.239233 -6.3069677 -6.313406 -6.2699127 -6.2081666 -6.1341472 -6.1686392 -5.8820648 -5.6018195]]...]
INFO - root - 2017-12-15 20:22:36.084614: step 56310, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 47h:38m:11s remains)
INFO - root - 2017-12-15 20:22:42.461683: step 56320, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.624 sec/batch; 47h:51m:58s remains)
INFO - root - 2017-12-15 20:22:48.787538: step 56330, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 48h:12m:14s remains)
INFO - root - 2017-12-15 20:22:55.274500: step 56340, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 51h:03m:32s remains)
INFO - root - 2017-12-15 20:23:01.662828: step 56350, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 48h:20m:53s remains)
INFO - root - 2017-12-15 20:23:08.052816: step 56360, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 48h:06m:03s remains)
INFO - root - 2017-12-15 20:23:14.442952: step 56370, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 48h:10m:15s remains)
INFO - root - 2017-12-15 20:23:20.865243: step 56380, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.613 sec/batch; 47h:02m:25s remains)
INFO - root - 2017-12-15 20:23:27.363438: step 56390, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 48h:09m:30s remains)
INFO - root - 2017-12-15 20:23:33.663402: step 56400, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.653 sec/batch; 50h:06m:56s remains)
2017-12-15 20:23:34.155412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5163946 -5.2230029 -5.3851948 -5.0626764 -4.6709871 -4.740983 -4.5684013 -4.4208 -4.3686981 -5.9105997 -6.7547541 -7.0645685 -7.2904339 -7.7670012 -7.417357][-4.0425353 -4.9644895 -5.4523048 -5.2498407 -4.8934531 -4.1863508 -4.1042218 -3.9981387 -4.234046 -6.1240973 -7.519383 -8.3954067 -8.77952 -9.3051376 -8.9938431][-3.9762921 -4.2614012 -4.710567 -4.8279724 -4.5230141 -3.8220265 -3.2306376 -2.5386105 -2.6048765 -4.7019286 -6.3154197 -7.4370189 -8.5965786 -9.4360695 -9.3587914][-4.0578613 -4.1416774 -4.14535 -3.5095186 -3.040123 -2.5419097 -2.1855121 -1.9429636 -1.6496558 -3.3731523 -4.3179035 -5.7974224 -7.156116 -8.5803928 -8.8626051][-4.1285152 -3.9989884 -3.0785918 -2.4341311 -1.1370196 -0.34597063 -0.12914467 -0.604444 -0.64663744 -2.1484461 -3.2148438 -4.1719818 -5.3372593 -6.945436 -7.423367][-3.9805527 -2.8912587 -2.4262443 -1.4510221 -0.28181744 0.99116707 1.6122532 1.3008547 0.85915947 -0.93552971 -1.9872918 -3.3472624 -4.4632454 -5.2711411 -5.6698074][-4.3680496 -2.8793621 -1.4493718 -0.33870697 0.89390945 2.2763109 2.8871784 2.8537655 2.7590895 0.75402069 -0.7532835 -2.3483243 -3.8487623 -5.16605 -5.4310865][-4.2431707 -3.202116 -2.1956625 -0.27485132 1.5594425 2.9285069 4.0126753 3.8000364 3.3462439 1.4431038 0.19449806 -1.5417962 -3.0561943 -4.0549622 -4.2816792][-4.5303698 -3.9575303 -3.1624207 -1.3706408 0.5500412 2.4912558 3.6103783 3.6390381 3.3620806 1.1361542 -0.33864403 -1.8912091 -3.1130948 -4.3741035 -4.5003757][-5.4942083 -4.6795721 -4.4171963 -3.1388011 -1.5182395 0.40880394 1.7184954 2.5655127 2.7467375 0.56057072 -0.979959 -2.5490584 -3.9402735 -4.7534075 -4.8962564][-5.5476961 -5.5826168 -5.2427025 -3.97336 -2.6904864 -1.5793848 -0.29874516 0.33749676 0.61743069 -0.89333296 -2.0782375 -3.4845886 -4.4748383 -5.4958067 -5.5258446][-6.185524 -5.7732773 -5.6419287 -4.91136 -4.0402317 -2.9682012 -1.7729874 -1.4244308 -1.2899566 -2.6044993 -3.3652697 -4.4830971 -5.04828 -6.092896 -6.4167571][-6.8708205 -6.3403635 -6.0179386 -5.6823063 -5.3191085 -4.5907531 -3.7318046 -3.4623952 -3.2635317 -4.0403204 -4.5662842 -4.6912589 -5.0777698 -5.8662305 -6.3102593][-7.2846823 -6.7131376 -6.1450243 -5.3652387 -5.0439081 -4.956151 -4.6636257 -4.6041827 -4.624773 -4.5594378 -4.6539721 -4.5221233 -4.74043 -5.2048163 -5.3952594][-7.1193066 -7.0316629 -6.6990643 -6.0661826 -5.5730147 -4.756156 -4.6436911 -5.302083 -5.3359632 -5.1688242 -4.7910151 -4.799181 -4.7343893 -4.7117615 -4.92545]]...]
INFO - root - 2017-12-15 20:23:40.518649: step 56410, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:24m:00s remains)
INFO - root - 2017-12-15 20:23:46.973399: step 56420, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 50h:01m:47s remains)
INFO - root - 2017-12-15 20:23:53.372012: step 56430, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 48h:52m:14s remains)
INFO - root - 2017-12-15 20:23:59.726999: step 56440, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 48h:04m:25s remains)
INFO - root - 2017-12-15 20:24:06.084200: step 56450, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 48h:41m:09s remains)
INFO - root - 2017-12-15 20:24:12.505523: step 56460, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:28m:41s remains)
INFO - root - 2017-12-15 20:24:18.795904: step 56470, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.620 sec/batch; 47h:30m:40s remains)
INFO - root - 2017-12-15 20:24:25.130711: step 56480, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 48h:13m:44s remains)
INFO - root - 2017-12-15 20:24:31.546665: step 56490, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 48h:43m:25s remains)
INFO - root - 2017-12-15 20:24:37.874117: step 56500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 48h:57m:42s remains)
2017-12-15 20:24:38.383620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4797473 -4.132967 -4.4622564 -4.5085707 -4.9936275 -5.2377272 -4.4282942 -3.074841 -2.2788534 -3.5197988 -4.7295704 -5.2505188 -5.8126707 -7.6952124 -7.4122519][-4.7991858 -5.4934978 -5.7426844 -5.8134151 -5.5225258 -5.194006 -5.3193283 -5.0110521 -3.593792 -4.0165119 -4.7693119 -5.2799735 -6.536253 -8.0816545 -7.3118081][-4.3529358 -4.0710373 -4.6845846 -5.2732038 -5.3133268 -4.711607 -3.7526567 -3.1328626 -2.6858368 -3.2615018 -3.7242856 -4.6130171 -5.911375 -7.5260758 -7.510529][-4.4891319 -3.8843265 -3.6671329 -3.5109239 -3.3969493 -2.9455867 -2.135273 -1.2328348 -0.59233761 -1.7834997 -3.2048044 -3.4993486 -4.4371786 -6.6228447 -6.28727][-5.0540457 -3.9201887 -2.8516493 -2.6407056 -2.1702762 -1.2629509 -0.54224348 0.021539688 -0.067324638 -1.6826973 -3.0493798 -4.044035 -5.0446739 -6.238739 -5.9274459][-3.3839445 -2.9012165 -2.3387761 -1.5876951 -0.80310822 0.18116617 0.77453327 1.5998468 1.5710068 -0.7002368 -3.188591 -4.693058 -6.305079 -7.466126 -6.5182066][-2.1306868 -1.4701133 -1.1815124 -0.43459558 0.1719842 1.284483 2.098835 2.5483665 2.8598099 0.44574547 -2.2985978 -3.998564 -5.9649744 -8.1541748 -7.6043849][-2.0538983 -1.4783864 -0.67353678 0.15853024 1.0481415 1.819849 2.12747 2.9380388 3.1362047 1.0607576 -1.4070768 -3.9238598 -5.5527091 -7.8053946 -7.3208623][-2.8348188 -1.869936 -1.3242793 -0.58664417 0.49933434 1.2930965 1.9799719 2.3597488 2.0922909 0.11294842 -2.119586 -3.369998 -5.1437683 -7.4729247 -6.9960957][-3.3614902 -2.7835174 -2.0547237 -1.4848204 -0.8128624 0.35253334 0.93921947 1.3239136 1.6632538 -0.34973526 -2.399797 -4.1594062 -5.7702875 -6.9425735 -6.6143141][-4.0162544 -4.3486834 -4.062571 -3.866127 -3.4786897 -2.6137967 -1.9629073 -1.1251874 -1.0717897 -2.5678468 -4.2051373 -5.4328136 -6.6425972 -7.7864137 -7.1598277][-6.1288762 -5.7645659 -5.2917347 -5.6379862 -5.2912121 -4.8984442 -4.46205 -3.9843042 -3.5531592 -4.3692532 -5.5796986 -5.7640734 -6.752193 -7.2736988 -6.7730827][-7.4245405 -7.5655613 -7.63912 -7.4789243 -7.2156343 -6.5370774 -5.8208952 -5.5593672 -5.7690773 -6.0791259 -6.9568405 -6.8017664 -6.2802978 -6.251904 -5.8634953][-7.2987828 -7.2455659 -7.1203437 -7.0674558 -7.0488634 -6.9766841 -6.7992811 -6.3168807 -5.72685 -5.2585306 -6.1413274 -6.1852465 -6.6071568 -6.3060827 -5.8023376][-8.545929 -7.8827682 -7.5984168 -7.4502172 -7.1340942 -6.6403608 -6.6239872 -6.7044873 -7.1096644 -6.8138924 -6.2644758 -6.174078 -6.5173416 -6.3315611 -6.40451]]...]
INFO - root - 2017-12-15 20:24:44.759415: step 56510, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 48h:44m:45s remains)
INFO - root - 2017-12-15 20:24:51.093841: step 56520, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 48h:23m:58s remains)
INFO - root - 2017-12-15 20:24:57.473249: step 56530, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 48h:44m:12s remains)
INFO - root - 2017-12-15 20:25:03.788532: step 56540, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 47h:51m:04s remains)
INFO - root - 2017-12-15 20:25:10.207600: step 56550, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 50h:22m:04s remains)
INFO - root - 2017-12-15 20:25:16.553236: step 56560, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 50h:01m:37s remains)
INFO - root - 2017-12-15 20:25:22.978228: step 56570, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 48h:53m:26s remains)
INFO - root - 2017-12-15 20:25:29.353974: step 56580, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 49h:35m:50s remains)
INFO - root - 2017-12-15 20:25:35.710298: step 56590, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 48h:45m:03s remains)
INFO - root - 2017-12-15 20:25:42.110199: step 56600, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 48h:59m:28s remains)
2017-12-15 20:25:42.659271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2888813 -3.3392978 -2.9408798 -1.9756522 -1.9085321 -1.0844197 -0.75923204 -1.4143667 -1.4700708 -2.5866904 -4.0657253 -5.1955948 -6.9337997 -7.8980246 -8.6913242][-2.3373694 -2.8052478 -3.3966179 -3.4506125 -2.8060174 -2.1217947 -1.3915877 -1.0369959 -0.69301367 -1.7650471 -3.5163088 -5.4244294 -6.8337464 -7.2255783 -7.7007914][-2.389257 -2.6062098 -2.5985489 -2.2757611 -2.245357 -1.4185019 -0.43007278 -0.46554041 -0.54765606 -1.4275513 -3.320322 -5.2102747 -6.5916867 -7.0176206 -7.1777406][-2.5080533 -2.3023319 -2.294064 -1.7941356 -1.0476041 -0.12719011 0.72442436 1.1013994 0.96698856 -0.66118431 -2.6634951 -4.5427036 -5.9608364 -6.4957495 -6.8283973][-3.7146392 -3.3413801 -2.3198242 -1.5693827 -0.694036 0.29309845 1.1222944 1.3559332 1.5686083 0.91309452 -0.94066668 -2.919642 -4.596982 -5.3525114 -6.0203123][-4.1071491 -3.0811243 -2.5166025 -1.1822333 0.44392586 1.7715397 2.7698793 3.1881075 3.1348791 2.1845922 0.18133974 -1.9762201 -3.935446 -4.9541769 -5.8418036][-3.8246703 -3.3426261 -1.751121 -0.15733051 1.4739723 2.5172729 3.3181248 3.8856478 4.0267439 2.3763256 0.1583662 -2.2559509 -4.3560591 -5.3582096 -5.9334717][-2.871387 -2.0966568 -1.8951573 -0.36773729 1.1510439 2.8876076 4.12228 3.8785963 3.9019146 2.633812 0.22849512 -2.2341051 -4.72157 -5.8521013 -6.3465309][-3.442039 -2.9195848 -1.5498123 -0.018576622 1.4835739 2.8651714 3.9437199 4.121686 3.7715931 1.7675343 -0.67498636 -2.6244564 -4.5500951 -5.3618774 -5.9196367][-4.2731466 -3.2441831 -2.8704462 -1.3571358 0.73083782 2.0564737 2.8419971 3.1455364 2.9492178 1.1850958 -1.1026831 -2.686482 -4.2107449 -5.136014 -5.8145218][-5.2112923 -4.7820415 -3.4361033 -2.1990166 -1.3989382 -0.25953674 1.2268686 1.373209 1.6757078 0.89175797 -0.75043917 -2.412879 -4.2963696 -4.5078421 -4.904336][-6.5063972 -5.4553 -4.1587849 -3.35362 -1.6472478 -0.40449 0.45551586 0.57411289 0.73624992 -0.13729954 -1.083231 -1.9722281 -3.6235013 -4.5088558 -4.7586179][-5.9957209 -5.1784868 -4.0369906 -3.1402245 -1.7005997 -0.28281784 1.0723028 1.0009747 0.5117178 -0.72496176 -2.0783582 -2.4787951 -2.9217834 -3.4500041 -4.545826][-6.4935112 -5.6093988 -4.5850635 -3.6868749 -2.60807 -1.5226336 -1.165772 -0.86823893 -0.29133797 -0.91160107 -1.3752332 -1.9028029 -3.0105062 -3.359376 -3.7659073][-7.3641367 -6.4792051 -5.1589494 -5.0597229 -4.7097111 -4.2227221 -3.4840431 -2.8014183 -2.2162843 -1.7054601 -1.7412696 -2.1004295 -2.9473524 -3.6223903 -4.8026109]]...]
INFO - root - 2017-12-15 20:25:49.197231: step 56610, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 50h:26m:55s remains)
INFO - root - 2017-12-15 20:25:55.576591: step 56620, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 48h:56m:33s remains)
INFO - root - 2017-12-15 20:26:01.948415: step 56630, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 49h:29m:08s remains)
INFO - root - 2017-12-15 20:26:08.385618: step 56640, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 50h:55m:39s remains)
INFO - root - 2017-12-15 20:26:14.770236: step 56650, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 49h:16m:22s remains)
INFO - root - 2017-12-15 20:26:21.221079: step 56660, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 50h:16m:14s remains)
INFO - root - 2017-12-15 20:26:27.666259: step 56670, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 48h:07m:27s remains)
INFO - root - 2017-12-15 20:26:34.019126: step 56680, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 49h:22m:30s remains)
INFO - root - 2017-12-15 20:26:40.558686: step 56690, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 49h:46m:39s remains)
INFO - root - 2017-12-15 20:26:46.890573: step 56700, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 49h:49m:17s remains)
2017-12-15 20:26:47.452160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0397692 -6.2430439 -6.5166397 -6.5253997 -6.3093944 -5.6924577 -4.7984023 -3.2788963 -1.8750606 -1.9970255 -2.7012215 -5.0064731 -6.6264086 -8.0251312 -8.4143572][-5.5878654 -6.6083651 -7.6363964 -7.9271865 -8.0230522 -7.5382009 -6.9280658 -5.2521534 -3.6447515 -3.100256 -3.2622676 -5.9894047 -7.8236032 -8.6474657 -8.5348][-4.64354 -5.5553102 -6.7724442 -7.8039474 -7.94833 -7.6198473 -6.9436288 -5.8567767 -4.7561274 -3.9249537 -3.9871383 -6.1347342 -7.218039 -8.1467638 -8.6472807][-3.9233098 -4.4941149 -5.140903 -5.3076324 -5.4013033 -5.6847534 -5.1778221 -4.3238678 -3.0923219 -3.0282187 -3.5777922 -5.358037 -6.8235574 -7.9001517 -8.3612337][-3.638164 -2.9293551 -2.8757725 -2.6587815 -2.9276733 -2.7301912 -2.1270294 -1.6273665 -0.64937258 -1.0039501 -1.7059903 -4.3039312 -6.1819882 -7.2737761 -7.6713371][-3.1962843 -2.8017206 -1.9742098 -1.0443215 -0.5988903 -0.12791395 0.41138935 0.90212631 1.5545683 0.73171806 -0.23153305 -3.1619034 -5.28414 -6.478477 -6.7822213][-2.980794 -1.8968678 -1.0315251 -0.037141323 0.86283684 2.0860939 2.6065464 2.8808746 3.1486225 1.8142376 0.13718271 -3.2265706 -5.0773363 -6.0951567 -6.4134359][-1.9587245 -1.5883164 -0.86712885 0.099397659 1.1078882 2.232091 2.924962 3.5299034 3.7947359 2.2411804 0.34361172 -3.4471025 -5.2195921 -6.1335373 -6.2050314][-2.5718875 -1.8969831 -1.4272957 -0.47853374 0.32943535 0.93904591 1.2790689 2.0119352 2.3103724 0.83742332 -1.2144423 -4.7233262 -6.2468476 -6.8790884 -6.4366727][-3.7451696 -3.1515875 -2.4067793 -1.8710008 -1.224041 -0.41888332 0.11158514 0.6611805 0.6815033 -0.60321188 -2.1963773 -4.7781096 -6.3297038 -7.2411766 -6.9547043][-5.4211721 -4.5220766 -3.6140738 -3.2579188 -3.097867 -2.4754825 -2.1742654 -1.9950409 -2.234169 -3.0836291 -4.0809259 -5.6181917 -6.4652615 -7.111949 -7.0407186][-6.732605 -5.3549204 -4.8247347 -4.2328196 -3.6966846 -3.84221 -3.9227045 -3.7905338 -3.9307036 -4.6266117 -5.2992678 -5.9772477 -6.1626496 -6.4031186 -6.23726][-7.7729869 -7.1467476 -6.7248764 -5.8724494 -5.3391657 -4.932663 -5.1511173 -5.4988217 -5.6625123 -5.7082682 -6.0781593 -6.2147942 -6.0664434 -5.8608551 -5.44979][-7.49677 -7.3030882 -6.8820157 -6.0007935 -5.7346368 -5.4386063 -5.4734983 -5.3611965 -5.2935896 -5.2271376 -5.2750993 -5.2942324 -5.1515436 -5.0668626 -5.2115097][-7.2099333 -6.9835734 -6.370122 -6.0579047 -6.243875 -6.2299819 -6.482564 -6.5237269 -6.4512863 -6.2286825 -6.0390439 -5.7411251 -5.400454 -5.2527337 -5.1127949]]...]
INFO - root - 2017-12-15 20:26:53.978895: step 56710, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 50h:30m:31s remains)
INFO - root - 2017-12-15 20:27:00.427331: step 56720, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 49h:03m:11s remains)
INFO - root - 2017-12-15 20:27:06.827986: step 56730, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:17m:25s remains)
INFO - root - 2017-12-15 20:27:13.271905: step 56740, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 49h:08m:50s remains)
INFO - root - 2017-12-15 20:27:19.646200: step 56750, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 49h:10m:26s remains)
INFO - root - 2017-12-15 20:27:26.095587: step 56760, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 47h:02m:42s remains)
INFO - root - 2017-12-15 20:27:32.494160: step 56770, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 49h:02m:47s remains)
INFO - root - 2017-12-15 20:27:38.876555: step 56780, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 50h:24m:41s remains)
INFO - root - 2017-12-15 20:27:45.288304: step 56790, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 49h:39m:35s remains)
INFO - root - 2017-12-15 20:27:51.600771: step 56800, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 48h:38m:00s remains)
2017-12-15 20:27:52.095035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6210728 -2.903861 -2.8548827 -3.2712383 -3.6418052 -3.9218857 -3.9853926 -3.88415 -3.8239975 -4.1053095 -4.0343409 -5.3092737 -5.5767975 -6.4643307 -7.5701766][-2.0147381 -3.0355105 -3.535965 -3.5757713 -3.8392909 -3.9691958 -4.1001215 -3.7946684 -3.3937297 -3.9470274 -3.7400517 -5.1797471 -5.9262953 -6.5774322 -7.6235685][-1.9034452 -2.162714 -2.8229589 -3.5223546 -4.1138744 -4.2088547 -3.7778692 -3.461503 -3.2054563 -3.787421 -3.9261045 -5.09608 -5.4468575 -6.1871238 -6.8506956][-2.4790068 -2.7378488 -2.8431368 -2.9838371 -3.0827904 -2.8868871 -2.8523879 -2.6959929 -2.2137918 -2.6494269 -2.5715089 -3.9983337 -4.8304644 -5.9718642 -6.8320904][-3.6635332 -2.8885946 -2.4598732 -2.5081034 -2.747756 -1.970665 -1.0774698 -0.56603241 -0.025584221 -0.81793833 -1.3918533 -3.2624393 -4.2915936 -5.6372938 -6.8172803][-4.3078108 -3.867816 -3.5769162 -2.0380011 -0.85217381 0.073215008 0.98561382 1.6480303 1.8817024 0.54329586 -0.089328289 -2.5256906 -4.1483183 -5.63238 -6.9078288][-4.6430092 -3.8138878 -3.1076379 -1.8164353 -0.19950104 1.1080713 2.3075838 3.0497684 3.5382462 2.2884235 1.0901775 -1.5989356 -3.1926274 -5.0011721 -6.5276723][-4.6537094 -3.0569711 -1.8355913 -0.12311125 1.3471375 2.2999372 3.2063923 3.6579494 4.269125 3.0607843 2.097702 -0.73966408 -2.574533 -4.5553532 -6.1519833][-3.777591 -2.7872496 -1.7582765 -0.54390049 0.69487286 2.2849455 3.2449894 3.371438 3.5325327 1.7147341 0.6863451 -1.8315821 -3.236568 -5.09042 -6.5695753][-3.9228017 -3.2315154 -2.6940556 -1.6596065 -0.93219423 0.11686802 0.79616165 1.0331726 1.2907553 -0.69364786 -1.7674541 -4.1386833 -5.3064556 -6.3638611 -7.5192113][-5.929584 -5.2531066 -4.4151978 -3.5298986 -2.8040004 -2.1730747 -1.8147569 -2.0320849 -2.138217 -3.6111174 -4.1408916 -5.9409742 -6.5423889 -7.351161 -8.2016163][-7.0552263 -6.6600628 -6.1284432 -5.4930735 -4.7298517 -4.1570306 -3.8798873 -3.8902791 -3.8073246 -5.20527 -5.4696603 -6.8003683 -7.210813 -7.6050706 -8.36835][-8.1154375 -7.8254776 -7.6194677 -7.263824 -6.7925768 -6.2294474 -5.7590466 -5.7655096 -5.9433155 -6.8203259 -6.9842849 -7.706573 -7.6206794 -7.4918318 -7.86959][-7.6884189 -7.5293579 -7.5367746 -6.8853951 -6.393549 -6.0371342 -5.4911718 -5.4592633 -5.465467 -6.0384054 -6.4923372 -6.8739562 -7.0503612 -6.9081683 -7.2465625][-8.2339249 -8.3917236 -8.0611134 -7.4657931 -7.0151887 -6.5354795 -6.33957 -6.5706458 -6.5662231 -6.6716566 -6.5371661 -6.6705565 -6.560482 -6.5406189 -6.5964394]]...]
INFO - root - 2017-12-15 20:27:58.571922: step 56810, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 49h:24m:39s remains)
INFO - root - 2017-12-15 20:28:04.926917: step 56820, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 48h:08m:11s remains)
INFO - root - 2017-12-15 20:28:11.327797: step 56830, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 48h:11m:05s remains)
INFO - root - 2017-12-15 20:28:17.777609: step 56840, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 47h:53m:11s remains)
INFO - root - 2017-12-15 20:28:24.160860: step 56850, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 49h:28m:32s remains)
INFO - root - 2017-12-15 20:28:30.526176: step 56860, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 49h:11m:09s remains)
INFO - root - 2017-12-15 20:28:36.941233: step 56870, loss = 0.27, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 52h:00m:04s remains)
INFO - root - 2017-12-15 20:28:43.319149: step 56880, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 50h:46m:58s remains)
INFO - root - 2017-12-15 20:28:49.631419: step 56890, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 48h:23m:55s remains)
INFO - root - 2017-12-15 20:28:55.999015: step 56900, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 47h:43m:35s remains)
2017-12-15 20:28:56.509008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2797346 -4.9092169 -5.1518431 -5.5332403 -5.5651278 -5.2179346 -4.7100172 -4.6766896 -4.73545 -4.8900776 -5.0627232 -5.9536862 -7.116221 -8.6610527 -9.3957253][-2.4765244 -2.9001279 -4.0464416 -5.193356 -5.6675692 -6.3918428 -6.9222093 -6.946939 -6.6718984 -6.7683463 -6.5708389 -7.0903454 -7.9004593 -8.8822079 -9.1924868][-2.3884482 -2.4297314 -3.1524324 -3.6966605 -4.3213997 -5.4319811 -6.2787027 -6.7942557 -6.8475089 -7.05566 -6.7799878 -7.0485277 -7.5164442 -8.3976974 -8.72827][-2.9178858 -2.2398424 -2.6122122 -3.27597 -3.7978034 -4.1488657 -4.3097563 -4.4770708 -4.3578849 -4.7078404 -4.546381 -5.6839042 -6.848187 -7.8479257 -8.6230536][-3.0141997 -2.2682285 -2.347816 -2.4961319 -2.9807987 -2.9020138 -2.3256369 -1.8354044 -1.0445037 -1.199995 -1.3533311 -3.0632935 -4.6595259 -6.3145647 -7.5793486][-3.1027794 -2.4835591 -2.341682 -1.9965954 -1.6832986 -0.92970991 -0.075129509 0.26863766 0.63786411 0.37728119 0.2802577 -1.6916084 -3.331151 -5.012043 -6.1501932][-3.4149904 -2.644702 -2.1777606 -1.4065967 -0.38945103 0.60021687 1.5835485 1.5484409 1.9321032 1.026845 0.37394524 -1.4616795 -3.1298165 -4.9579029 -6.2084856][-3.4019208 -2.6501284 -2.4479141 -1.7136278 -0.56711435 0.39712334 1.7377958 2.305438 2.8488712 1.7585421 0.835454 -1.2802835 -3.0960336 -5.1208591 -6.4148746][-3.7428427 -3.1105733 -3.15666 -2.2499676 -0.92166567 0.50911045 1.9423037 2.248189 2.8545427 2.0997334 1.1727877 -1.4625516 -3.3606682 -5.0634966 -6.3524175][-3.8812671 -3.3266788 -3.0349054 -2.1260605 -1.0164714 0.2932353 1.1924305 1.2435284 1.458786 1.0358152 0.3009758 -2.0985427 -3.7005041 -5.4700017 -6.6825957][-4.6182532 -4.1531634 -3.6371937 -3.2463183 -2.5463672 -1.8667102 -1.3967557 -1.1406736 -1.1719227 -1.3975663 -1.8115048 -3.0855112 -4.4018159 -6.2301235 -7.6346827][-3.986392 -3.6082759 -3.2824221 -3.1487265 -2.8502784 -2.3346982 -2.1425076 -2.2919159 -2.1303692 -2.5525055 -3.5738626 -4.0725503 -5.06513 -6.6638951 -7.8903613][-5.1784081 -5.1248503 -4.4098806 -3.1573248 -2.3178873 -1.9312029 -2.0296779 -2.5360236 -2.6333065 -3.3703895 -4.8832951 -5.8015232 -6.5487981 -7.2404165 -7.7047257][-4.6447191 -5.6353364 -5.5422363 -4.4173784 -3.636498 -2.7541099 -2.5005913 -2.7576432 -2.8110161 -3.5040832 -4.257699 -5.4730768 -6.4849563 -6.6815643 -6.797051][-4.236083 -4.8022966 -5.1713953 -5.1200109 -5.1335649 -4.28058 -3.9982126 -3.8933728 -4.1229553 -4.3498459 -4.6321716 -5.5492182 -6.0908766 -6.5221663 -6.8550825]]...]
INFO - root - 2017-12-15 20:29:02.922327: step 56910, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 49h:28m:19s remains)
INFO - root - 2017-12-15 20:29:09.222529: step 56920, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 49h:02m:11s remains)
INFO - root - 2017-12-15 20:29:15.569021: step 56930, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:12m:19s remains)
INFO - root - 2017-12-15 20:29:21.986149: step 56940, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 48h:46m:00s remains)
INFO - root - 2017-12-15 20:29:28.402145: step 56950, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 48h:47m:10s remains)
INFO - root - 2017-12-15 20:29:34.762111: step 56960, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 49h:06m:34s remains)
INFO - root - 2017-12-15 20:29:41.120513: step 56970, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 49h:42m:36s remains)
INFO - root - 2017-12-15 20:29:47.489343: step 56980, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 50h:02m:19s remains)
INFO - root - 2017-12-15 20:29:53.909857: step 56990, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 50h:33m:58s remains)
INFO - root - 2017-12-15 20:30:00.256705: step 57000, loss = 0.36, batch loss = 0.25 (12.1 examples/sec; 0.662 sec/batch; 50h:38m:34s remains)
2017-12-15 20:30:00.829394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5803814 -3.3608775 -3.4377608 -3.2737541 -3.4742398 -3.8315957 -3.9992573 -4.3472252 -4.6102171 -5.1378422 -6.2429762 -6.9345436 -7.8377409 -8.6793842 -9.0526829][-3.1441617 -3.0121026 -3.2215877 -3.1166563 -3.1605148 -3.2786913 -3.8319981 -3.8924756 -3.846087 -4.5189667 -5.9093666 -6.9022112 -7.9323 -8.6770325 -9.0375986][-2.7026095 -2.732131 -2.928792 -2.8019066 -2.7029452 -2.4486613 -2.6556287 -2.7249322 -2.8366814 -3.5051184 -5.2134337 -6.1575551 -7.08521 -7.6140494 -7.8020191][-4.2056451 -3.7283359 -3.1676111 -2.8420024 -2.5597053 -1.9337816 -1.183116 -1.1466765 -1.303174 -2.3799677 -3.7276773 -4.8560629 -5.6008873 -6.4683881 -7.0955472][-4.5471725 -3.5056157 -2.5273132 -2.0362687 -1.7761493 -0.83174562 0.17180967 0.14224768 -0.17033291 -1.2834826 -2.2118955 -2.7442026 -3.9428506 -5.2900524 -6.394145][-3.797812 -2.9209394 -1.9461131 -1.0474892 -0.46317625 0.47941017 1.3594723 1.5311031 1.5477104 0.42473793 -0.51640034 -1.4422112 -2.7826424 -4.2571282 -5.490346][-3.3578286 -2.744987 -1.5979285 -0.42354155 0.95699596 1.9246759 2.524271 2.6606321 2.6664906 1.6506357 0.79641342 -0.57093048 -2.4635711 -4.2432532 -5.5524521][-2.6791286 -2.2681766 -1.6452742 0.15793562 1.9586926 3.1040831 3.8498745 4.1250134 4.2114906 2.9212418 1.7996864 0.084093094 -1.8054881 -3.7608829 -5.2005076][-3.215003 -2.4745288 -1.721961 -0.41891766 1.4572277 2.5869312 2.9036636 3.5541821 4.2745876 3.0960369 1.3068352 -0.5473671 -2.2593298 -3.8801298 -4.6537724][-3.9568315 -3.3980145 -3.1850538 -2.0634317 -0.8905673 0.70332718 1.3427982 1.76614 2.3881931 0.93944836 -0.68097544 -2.0298896 -3.2922702 -4.518352 -5.46194][-6.0924387 -5.7308612 -5.0807953 -4.6289968 -3.733 -2.7555604 -1.9455166 -1.6183944 -1.5755067 -2.7828364 -4.2178211 -4.78811 -5.4306841 -6.14658 -6.7379446][-7.3485713 -6.7826338 -6.5418353 -6.5198388 -6.5053024 -5.7083874 -4.8511162 -4.6245375 -4.0889368 -4.751966 -5.8453989 -6.2128344 -6.3633246 -6.8823824 -7.6941934][-8.1318874 -8.24647 -7.2240343 -7.3220439 -7.3030715 -6.9082813 -6.329103 -6.158042 -5.7594204 -5.8922462 -6.4967442 -6.3520417 -5.932024 -6.2953663 -6.6735191][-7.4086514 -8.1365337 -7.5510607 -7.1461587 -6.8667445 -6.1546493 -5.9186993 -5.9621034 -5.747303 -5.97896 -6.4810987 -5.9861164 -5.8998928 -5.701911 -5.3826475][-8.1080351 -8.08906 -7.5630693 -7.0170541 -6.637671 -6.0922461 -5.9595337 -6.0797873 -6.2229428 -6.3058019 -6.2481294 -6.04819 -6.0539551 -5.9968562 -5.8891768]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 20:30:07.335554: step 57010, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 49h:33m:17s remains)
INFO - root - 2017-12-15 20:30:13.760576: step 57020, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 49h:48m:29s remains)
INFO - root - 2017-12-15 20:30:20.135995: step 57030, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 49h:06m:58s remains)
INFO - root - 2017-12-15 20:30:26.546117: step 57040, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:15m:47s remains)
INFO - root - 2017-12-15 20:30:33.078914: step 57050, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 49h:47m:33s remains)
INFO - root - 2017-12-15 20:30:39.608794: step 57060, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 50h:40m:13s remains)
INFO - root - 2017-12-15 20:30:46.017769: step 57070, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 48h:24m:53s remains)
INFO - root - 2017-12-15 20:30:52.395940: step 57080, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 49h:24m:20s remains)
INFO - root - 2017-12-15 20:30:58.752845: step 57090, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 49h:49m:43s remains)
INFO - root - 2017-12-15 20:31:05.106924: step 57100, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 47h:14m:17s remains)
2017-12-15 20:31:05.655045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1849785 -6.2864647 -6.9938364 -7.1337652 -7.4085541 -6.1176586 -4.42218 -3.6654329 -2.4601769 -4.9111004 -4.9887514 -6.1151915 -6.8066015 -7.8903365 -8.6452084][-6.4749489 -7.4939661 -6.925375 -7.3129516 -6.861845 -6.3509336 -6.593833 -5.9939728 -5.2497282 -6.4832187 -5.9802294 -6.4497991 -7.4625988 -7.0659218 -8.489481][-5.8359485 -6.8005404 -6.9673262 -6.2309914 -4.4932547 -4.265379 -4.5293841 -5.643538 -6.1352725 -6.3889909 -5.8285847 -7.145824 -7.2839723 -6.9535871 -7.1599531][-3.0914111 -3.6559587 -4.3022485 -4.3264151 -3.9479268 -3.0891519 -2.2762771 -3.3554111 -4.1830664 -5.4956717 -6.0037451 -6.1869683 -5.9704318 -6.5764422 -6.5327759][-2.9677 -3.01505 -2.7279334 -2.0324626 -1.2148695 -0.96606207 -0.7583971 -1.281961 -2.5650158 -4.5991526 -4.4551077 -5.1586742 -5.52297 -4.8785839 -4.662981][-2.2031064 -2.2418866 -1.6209822 -0.76966476 1.1571007 1.6109715 1.0952301 0.42158031 -0.23491859 -1.7836542 -2.2135978 -3.3203015 -2.8352294 -3.1386514 -3.1215615][-1.4365177 -2.1817617 -2.8677382 -1.7360377 0.81005764 1.9672918 1.6335478 1.160306 0.49458694 -1.0523839 -1.0819349 -1.888351 -2.7034216 -4.3063717 -4.4837623][-2.5925775 -2.7204776 -1.9940596 -1.8617811 -0.67460728 1.309516 1.7861376 1.2610226 1.6040974 0.04985857 -0.6006465 -2.1600294 -2.9290991 -4.5028329 -6.1082788][-3.5182805 -2.911881 -2.9426951 -2.4945245 -1.582911 0.38118267 1.7124882 1.6886358 1.9794779 0.60154533 -1.0848484 -3.2611094 -3.8151538 -4.5986543 -5.1480694][-4.8338637 -3.6447597 -2.4977269 -1.6159434 -1.0652051 -0.80075359 -0.38087082 -0.059849739 1.0895872 -0.81100082 -2.0826893 -4.0192676 -5.2396669 -5.7999649 -6.23365][-8.0514069 -6.6094379 -5.2702317 -3.9140608 -2.07015 -1.5728874 -1.5141101 -2.0175247 -2.3603992 -3.5986347 -4.4121819 -5.195189 -6.19435 -6.1119375 -5.9358435][-6.6830187 -6.5667892 -7.4920912 -6.7781596 -5.8232274 -4.9170132 -3.9838886 -4.4106088 -3.4379296 -4.373848 -5.579226 -6.1589651 -7.2394834 -7.220583 -6.7135363][-8.3558512 -7.0315523 -6.4300923 -7.2597356 -7.8483725 -6.9157481 -6.231163 -6.0017147 -6.1105776 -6.2134314 -6.0032005 -6.0100441 -6.2887316 -6.06226 -5.8956127][-7.0187125 -6.4450183 -6.0456471 -5.7382011 -5.3487825 -6.2129593 -6.4445982 -6.4419155 -6.48302 -6.7082257 -6.7378154 -5.6732349 -5.3089008 -5.6933274 -5.5645156][-4.6953115 -5.3740072 -6.7869039 -7.4590764 -7.1414037 -7.2280684 -7.5153236 -7.9101005 -7.6568289 -7.2400427 -6.839704 -6.7920732 -6.9474783 -6.0044456 -5.5504608]]...]
INFO - root - 2017-12-15 20:31:11.999762: step 57110, loss = 0.32, batch loss = 0.20 (12.9 examples/sec; 0.619 sec/batch; 47h:20m:54s remains)
INFO - root - 2017-12-15 20:31:18.409785: step 57120, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 50h:10m:53s remains)
INFO - root - 2017-12-15 20:31:24.839855: step 57130, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 48h:47m:14s remains)
INFO - root - 2017-12-15 20:31:31.278711: step 57140, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 50h:27m:35s remains)
INFO - root - 2017-12-15 20:31:37.749321: step 57150, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 49h:09m:50s remains)
INFO - root - 2017-12-15 20:31:44.235906: step 57160, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 49h:34m:26s remains)
INFO - root - 2017-12-15 20:31:50.621457: step 57170, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 48h:14m:40s remains)
INFO - root - 2017-12-15 20:31:56.949218: step 57180, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 49h:31m:33s remains)
INFO - root - 2017-12-15 20:32:03.263110: step 57190, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 48h:33m:14s remains)
INFO - root - 2017-12-15 20:32:09.631638: step 57200, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 48h:24m:36s remains)
2017-12-15 20:32:10.116313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9261239 -3.123013 -3.5694003 -3.4940467 -3.9764638 -3.538465 -3.0555134 -3.0835867 -2.737041 -3.185533 -3.7019956 -4.7362585 -6.3386278 -6.7499571 -7.495635][-4.2964344 -4.2475171 -4.269794 -4.0465641 -3.09649 -2.6412392 -2.6198759 -2.7189932 -2.5883455 -3.0728097 -3.5710292 -4.397994 -5.86434 -6.5671673 -7.5605292][-5.0322189 -4.4989438 -4.1661024 -4.2043042 -3.711127 -3.0070333 -2.0475011 -1.8422952 -1.7329426 -2.2718415 -3.0611138 -4.3352747 -5.8937578 -6.3285332 -7.3370266][-3.7790627 -3.878278 -4.9108477 -4.232595 -3.5440512 -2.4430037 -1.1229339 -1.1214666 -0.86049891 -1.7579775 -2.798666 -4.1676846 -5.8083081 -6.6511374 -7.1851044][-3.8575361 -2.5093522 -1.5180683 -1.6073184 -1.551703 -1.0409923 -0.70489216 -0.1750226 0.37902641 -0.56077194 -1.469018 -3.2595949 -5.4668818 -6.5953722 -7.6386676][-4.4208918 -3.80219 -3.3853869 -2.0135474 -0.43094397 0.8654995 1.8206387 2.1139841 2.0198336 0.95197105 -0.17759991 -2.1282382 -4.5994387 -5.9151297 -7.2636328][-3.2897053 -3.5091968 -2.2394824 -1.4629784 0.019553661 1.3650246 2.6891813 3.2249346 3.2596846 1.9397497 0.55846405 -1.0137548 -3.5836267 -5.016118 -6.5506434][-2.9057341 -2.415669 -1.7852283 -0.33805323 1.3441572 2.63196 4.0117512 4.6355228 4.2103157 2.719986 1.2283115 -1.2516112 -3.6266246 -4.669116 -5.6611919][-2.0489178 -1.6746182 -0.83018732 0.45821381 1.8399448 2.5652409 3.2401066 3.7496996 3.9462595 2.5908451 0.728878 -1.4776492 -3.8122323 -5.3054762 -6.662962][-1.7602177 -0.94162273 -0.93731022 0.053986073 0.28756571 1.2174015 1.7884665 2.1173277 2.205122 1.0392141 -0.29289246 -2.4408712 -4.900033 -6.0946107 -6.6019077][-3.5733104 -3.8345878 -2.1882138 -1.496624 -0.65228319 -0.28998327 -0.21551895 -0.4593358 -0.39924383 -1.6645675 -2.6266737 -3.689589 -5.290041 -6.0053129 -7.0822635][-3.9305396 -4.8774571 -4.84447 -4.4607983 -3.528892 -2.9061418 -2.3014932 -2.5305505 -2.4799876 -3.5029588 -4.5572405 -5.516077 -6.8546987 -7.2788582 -7.2762237][-3.42558 -4.2964478 -5.6790242 -5.6625013 -5.404603 -4.5668011 -3.8613033 -4.1257763 -4.4118557 -4.8989477 -5.589386 -5.9851685 -7.3470178 -7.0723557 -7.1888666][-4.575892 -4.2231069 -4.3828917 -4.8801537 -5.3087697 -5.6140881 -6.1573763 -5.8197627 -5.3485947 -5.8435326 -6.2316365 -6.7518964 -7.0104966 -7.4168906 -7.9310942][-5.9596548 -5.9427118 -5.6952806 -6.5266442 -6.6891518 -6.5438347 -6.4007692 -6.9043322 -7.7812967 -7.4631467 -7.3099041 -7.5237813 -7.4454165 -7.2389517 -6.9627161]]...]
INFO - root - 2017-12-15 20:32:16.475767: step 57210, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 48h:33m:55s remains)
INFO - root - 2017-12-15 20:32:22.834146: step 57220, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 48h:12m:15s remains)
INFO - root - 2017-12-15 20:32:29.286965: step 57230, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 50h:48m:34s remains)
INFO - root - 2017-12-15 20:32:35.653054: step 57240, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 47h:54m:57s remains)
INFO - root - 2017-12-15 20:32:42.036451: step 57250, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 50h:26m:34s remains)
INFO - root - 2017-12-15 20:32:48.389501: step 57260, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 50h:31m:48s remains)
INFO - root - 2017-12-15 20:32:54.896655: step 57270, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 49h:46m:18s remains)
INFO - root - 2017-12-15 20:33:01.284610: step 57280, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 48h:56m:49s remains)
INFO - root - 2017-12-15 20:33:07.609687: step 57290, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 47h:34m:50s remains)
INFO - root - 2017-12-15 20:33:13.956635: step 57300, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 49h:18m:03s remains)
2017-12-15 20:33:14.521014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8004665 -6.7583318 -6.583077 -6.5663748 -6.6040087 -6.2269378 -5.0134172 -3.4733243 -2.1544814 -1.968049 -3.3125935 -3.6274843 -4.8628259 -5.8140793 -6.8363976][-5.8443608 -5.9907703 -6.0249953 -6.5220356 -6.7990379 -6.6347027 -6.007463 -4.4380517 -2.4468856 -1.7195592 -3.2573581 -4.3677807 -5.2483044 -5.8365321 -6.5749516][-4.7518206 -5.0085154 -5.6748104 -6.5540371 -7.3286257 -7.3640714 -7.0539317 -5.8807306 -4.6072149 -3.5868955 -4.5639544 -5.5888929 -6.6542974 -6.5369682 -6.9167252][-5.4631281 -5.2645159 -4.7915716 -5.1548924 -5.968174 -6.13778 -5.6138282 -4.62917 -3.7956815 -3.9273577 -5.5921831 -6.4831886 -7.1517563 -7.6823292 -7.9081869][-5.1822066 -4.5497594 -3.9159033 -3.3083043 -2.881947 -2.3020926 -1.6736107 -1.177331 -0.48504353 -1.5238504 -4.0326619 -6.0923047 -7.2617683 -7.7000308 -8.2176771][-4.0896969 -3.4084296 -2.6979041 -1.7255898 -0.4698782 0.57306957 1.9449911 2.7253065 3.2710581 2.1561174 -0.40346956 -2.9425211 -5.2823009 -6.6418223 -7.9212441][-4.6368222 -2.6999259 -0.80275965 0.28245115 1.5185423 2.6739302 4.1980991 4.0730228 4.336338 3.4085779 1.1341 -0.84900761 -3.2735395 -4.94993 -6.649127][-3.154892 -2.2417254 -0.93697882 0.52061653 2.1778994 3.5109568 4.8255863 4.9535894 4.79838 3.4060383 0.9084425 -0.6516943 -2.6505265 -4.1993475 -6.0436244][-3.8299258 -2.8958163 -1.9005041 -0.15483332 1.6318588 2.2834816 2.8980894 3.5316591 3.9885502 2.4484453 -0.69330072 -2.2570539 -3.281538 -4.0881243 -5.2066336][-4.7798643 -4.2720551 -3.555512 -2.2333026 -0.499393 0.56261826 0.6392746 0.22389746 0.6533947 -0.0022206306 -2.0405526 -3.6883945 -4.8679876 -5.3356829 -6.0057383][-7.0117235 -6.4791179 -6.2002692 -5.8696947 -4.8748465 -3.1318731 -1.9807396 -2.2700272 -3.1352339 -4.1964149 -4.9931579 -6.3509283 -6.9666414 -7.6561742 -8.3992252][-8.5768642 -8.3047514 -7.471612 -6.9550271 -6.7198262 -6.6676459 -5.9698706 -5.3115268 -4.9070721 -6.0438795 -6.9981179 -7.3023391 -7.5555944 -7.871407 -8.8264694][-9.9244709 -10.095097 -9.8485327 -9.045083 -7.8872113 -7.3134918 -7.1174169 -7.7461028 -7.7205892 -8.1069069 -8.6327286 -7.8713832 -7.0406871 -6.7503939 -7.6418948][-8.6882162 -9.4621859 -9.5111675 -9.008132 -8.4452372 -7.8110247 -7.0624228 -7.0105124 -7.4122148 -7.6475849 -8.0727844 -7.9682069 -7.7560649 -6.716763 -6.5948811][-7.5438643 -7.6737885 -7.9324679 -8.2626667 -8.4235792 -8.5578547 -8.2933893 -7.8667846 -7.2910576 -7.3955159 -7.9810648 -7.903017 -7.8117347 -7.5218639 -6.5623584]]...]
INFO - root - 2017-12-15 20:33:20.873299: step 57310, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 49h:13m:48s remains)
INFO - root - 2017-12-15 20:33:27.252040: step 57320, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 48h:15m:08s remains)
INFO - root - 2017-12-15 20:33:33.632656: step 57330, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 47h:42m:49s remains)
INFO - root - 2017-12-15 20:33:40.052169: step 57340, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 49h:57m:53s remains)
INFO - root - 2017-12-15 20:33:46.561928: step 57350, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 50h:29m:38s remains)
INFO - root - 2017-12-15 20:33:52.915659: step 57360, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 49h:38m:46s remains)
INFO - root - 2017-12-15 20:33:59.339644: step 57370, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 48h:51m:30s remains)
INFO - root - 2017-12-15 20:34:05.640207: step 57380, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 48h:30m:34s remains)
INFO - root - 2017-12-15 20:34:11.965985: step 57390, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 47h:52m:07s remains)
INFO - root - 2017-12-15 20:34:18.237546: step 57400, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 47h:32m:59s remains)
2017-12-15 20:34:18.741109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7306774 -3.6706052 -3.8347955 -3.8740933 -3.7055104 -3.6231923 -3.3953061 -3.052319 -2.7117996 -3.8931513 -4.5899343 -5.2518692 -6.16736 -6.3539476 -6.9679337][-3.3603935 -3.4898806 -3.6597538 -3.8264666 -3.6233692 -3.3616786 -3.1400828 -3.0411539 -2.8929348 -3.9284356 -4.59412 -5.4051113 -6.4949551 -6.7202158 -7.21585][-2.8142118 -3.0127139 -3.1403203 -3.3209543 -3.1404266 -2.8724713 -2.5212507 -2.3027015 -2.0718603 -3.3065972 -3.9819868 -4.5638652 -5.8363595 -6.3981681 -7.1627717][-2.4773145 -2.6664267 -2.7282367 -2.5972452 -2.167706 -1.6720762 -1.1386609 -0.9904747 -0.95428896 -2.3951778 -3.3581433 -4.2104769 -5.5985394 -6.2660036 -6.9733849][-2.8944416 -2.4128904 -1.9466548 -1.5507417 -0.84850979 -0.37825203 0.1019392 0.19934368 0.18374634 -1.5546141 -2.8101177 -3.7431393 -5.1871691 -6.0249 -6.958066][-4.0796366 -3.3408995 -2.2830625 -1.5557175 -0.72952414 0.16641617 0.92165947 0.92616272 0.7677803 -1.1376214 -2.5450492 -3.7658613 -5.3602839 -6.17511 -7.0192914][-4.1253471 -3.6818333 -2.815177 -1.4985418 -0.22784853 0.78013992 1.539216 1.5952406 1.4798079 -0.66224384 -2.4699044 -3.8487117 -5.5378666 -6.56777 -7.3752666][-4.25412 -3.5276055 -2.3743095 -0.97131395 0.21653223 0.9894352 1.6621952 1.7415266 1.5868044 -0.52471638 -2.3639879 -3.8861353 -5.6586046 -6.6723356 -7.4650769][-4.6722879 -3.8196607 -2.8698964 -1.5242734 -0.2812624 0.19063711 0.68251705 1.0314054 1.1675634 -0.96990633 -2.7586355 -4.1540823 -6.0404263 -7.0860939 -7.822217][-5.1480169 -4.3680992 -3.4660182 -2.3154688 -1.1817021 -0.93886471 -0.54906988 -0.034619331 0.24091387 -1.9772191 -3.5542684 -4.9076509 -6.5661359 -7.6079288 -8.5014744][-6.2711744 -5.6610556 -4.6906052 -3.6708856 -3.0432229 -2.9112587 -2.6994452 -2.4638968 -2.1628184 -4.0118341 -5.4713082 -6.3792372 -7.643105 -8.3521547 -8.7706375][-6.299614 -5.8032427 -5.2261124 -4.4337978 -3.9080651 -3.9906971 -3.9838405 -3.9124789 -3.8463869 -5.0529351 -6.09838 -6.7518883 -7.4426823 -8.0927582 -8.4864559][-6.5106573 -6.1329107 -5.5608954 -5.0672255 -4.724576 -4.9488888 -5.0396652 -5.1698418 -5.1651869 -5.9107885 -6.7094097 -6.87493 -7.4040227 -7.7695427 -7.7254758][-6.1758041 -5.9328871 -5.6571989 -5.1601682 -4.8172112 -5.0059328 -5.1863127 -5.30752 -5.3937945 -6.0842505 -6.3199906 -6.275311 -6.4206891 -6.7903023 -6.81479][-7.3980646 -7.3212986 -6.9152107 -6.4116497 -6.1085205 -6.2663555 -6.5023947 -6.7145386 -6.8400974 -6.7721663 -6.8338251 -6.8405466 -6.6518846 -6.4678617 -6.3000875]]...]
INFO - root - 2017-12-15 20:34:25.120064: step 57410, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 48h:15m:25s remains)
INFO - root - 2017-12-15 20:34:31.558487: step 57420, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 50h:26m:54s remains)
INFO - root - 2017-12-15 20:34:37.944083: step 57430, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 48h:50m:29s remains)
INFO - root - 2017-12-15 20:34:44.335803: step 57440, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 47h:32m:11s remains)
INFO - root - 2017-12-15 20:34:50.771229: step 57450, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 47h:18m:34s remains)
INFO - root - 2017-12-15 20:34:57.130595: step 57460, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 47h:48m:07s remains)
INFO - root - 2017-12-15 20:35:03.592452: step 57470, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:53m:29s remains)
INFO - root - 2017-12-15 20:35:09.974434: step 57480, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 47h:06m:49s remains)
INFO - root - 2017-12-15 20:35:16.372655: step 57490, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 49h:27m:45s remains)
INFO - root - 2017-12-15 20:35:22.768692: step 57500, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 48h:17m:52s remains)
2017-12-15 20:35:23.266785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4044638 -1.7994232 -2.2322936 -2.5423698 -2.8476257 -3.3108068 -3.3668504 -3.0167713 -2.2813311 -3.6756577 -4.1734457 -5.2208958 -5.9060173 -6.1063008 -7.2994084][-2.0070658 -2.1020985 -2.5286403 -2.8307648 -3.3615274 -3.8676856 -3.8792107 -3.7817941 -3.263659 -4.7777648 -5.308485 -6.4202456 -7.1267905 -7.1572986 -8.1974211][-3.1013889 -2.7435656 -2.6564832 -2.8188457 -3.218616 -3.6547327 -3.5315175 -3.6477642 -3.4594922 -5.0366483 -5.7265453 -7.14625 -7.7368555 -7.7667494 -8.6969643][-3.8548925 -3.2929015 -2.9770284 -2.5148096 -2.2103963 -2.1311779 -1.8272166 -1.740665 -1.4861851 -3.1860647 -4.1162138 -5.5698586 -6.5229082 -6.9330859 -8.2052021][-4.0875092 -3.2833037 -2.7387238 -2.115839 -1.5345325 -0.72653675 0.0064024925 0.41531086 0.84898281 -0.89453506 -2.1650138 -4.3719549 -5.5997391 -6.4650941 -7.9615464][-5.3177686 -4.1730604 -3.4031034 -2.0466785 -0.86143255 0.2400589 1.5342617 2.1928968 2.5831528 0.34274864 -1.4042406 -3.8777952 -5.2228489 -6.0138617 -7.73211][-5.3180256 -4.22946 -3.3436027 -1.3337064 0.54744148 1.6028595 2.8743343 3.9366789 4.868906 2.5871229 0.70657921 -2.0681014 -4.0546751 -5.1248684 -6.9607453][-4.7774796 -3.8139255 -2.8024669 -0.65085649 1.4827709 3.0597792 4.657094 5.1178112 5.8305454 3.8837652 2.2628384 -0.48426247 -2.4698572 -3.7446065 -5.5593572][-5.2480106 -4.3082638 -3.3051715 -1.7456818 -0.12588072 1.8923903 3.7727375 3.9937744 4.1848068 1.9672184 0.68793678 -1.4722605 -2.9430614 -3.9913845 -5.7578831][-6.3807673 -5.1944065 -4.1336985 -2.8591609 -1.614048 -0.24416208 0.89981461 1.6670017 1.9299698 -1.2833533 -2.6009998 -4.2071848 -5.2037 -5.3712296 -6.5254526][-8.3088026 -7.2732043 -6.1430154 -4.9703722 -3.906996 -2.875267 -1.8749619 -1.2426805 -1.0781317 -3.7101591 -4.8743229 -6.5652647 -7.4386163 -7.4137769 -7.9873848][-9.3304987 -8.7430353 -7.8578696 -6.8377218 -5.6975431 -4.7395239 -3.9638758 -3.7340419 -3.4678364 -4.9727979 -5.5810242 -7.3085375 -7.9676175 -7.5676565 -8.09326][-9.3068161 -8.8015108 -8.3569145 -7.3671942 -6.5908709 -5.55674 -4.7844028 -4.7236814 -4.7791796 -5.5182676 -5.6657095 -6.6782379 -6.9700317 -6.7352304 -7.4948359][-9.1058683 -8.4041691 -7.9577928 -7.0890179 -6.0459232 -5.2091808 -4.4406185 -4.570148 -4.772851 -5.3359165 -5.5201745 -5.7025204 -5.8416128 -5.7273464 -6.2720337][-9.7115507 -9.31072 -9.1237049 -8.1545715 -7.1681118 -6.4390965 -5.6797056 -5.4485803 -5.6665554 -5.9228077 -6.0505414 -5.9610777 -5.7348804 -5.5277424 -5.4711714]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 20:35:30.422728: step 57510, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 47h:42m:07s remains)
INFO - root - 2017-12-15 20:35:36.786115: step 57520, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 48h:13m:33s remains)
INFO - root - 2017-12-15 20:35:43.129239: step 57530, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 48h:56m:26s remains)
INFO - root - 2017-12-15 20:35:49.479529: step 57540, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 48h:27m:14s remains)
INFO - root - 2017-12-15 20:35:55.950816: step 57550, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 47h:51m:11s remains)
INFO - root - 2017-12-15 20:36:02.394469: step 57560, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 48h:26m:43s remains)
INFO - root - 2017-12-15 20:36:09.114125: step 57570, loss = 0.27, batch loss = 0.16 (10.7 examples/sec; 0.751 sec/batch; 57h:20m:55s remains)
INFO - root - 2017-12-15 20:36:15.538567: step 57580, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 48h:16m:29s remains)
INFO - root - 2017-12-15 20:36:21.964419: step 57590, loss = 0.32, batch loss = 0.21 (12.0 examples/sec; 0.669 sec/batch; 51h:04m:00s remains)
INFO - root - 2017-12-15 20:36:28.328245: step 57600, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 48h:22m:43s remains)
2017-12-15 20:36:28.863324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3082986 -3.2920146 -2.9499178 -2.6616945 -1.8543634 -1.9935584 -2.4128237 -2.5696359 -1.627254 -2.9808407 -4.0471821 -5.2787066 -5.928699 -6.81501 -7.2366605][-2.783915 -2.4325147 -2.0910077 -2.284915 -2.5797968 -2.9683847 -2.4798427 -2.1723475 -2.426743 -4.0476847 -4.4430037 -5.6598091 -6.4502435 -6.9073195 -7.4464178][-2.6085029 -2.762917 -2.9704833 -2.6108532 -1.6087847 -0.84996271 -0.90496063 -1.4085116 -1.5471015 -3.064291 -4.2317696 -5.0953159 -5.4347992 -6.2414289 -6.6708641][-3.8882411 -2.9935966 -1.7069035 -1.2187114 -1.1789598 -1.2979975 -0.71476555 -0.14572525 -0.111485 -1.787034 -2.887291 -4.1108284 -4.8727956 -5.5240355 -5.7155781][-3.4049616 -2.2629843 -0.97956419 -0.17481804 -0.27252626 -0.19983196 -0.0020418167 -0.15478897 -0.095724106 -1.204452 -1.7962527 -3.2406216 -4.216928 -5.0937834 -5.518589][-2.8034029 -1.6567326 -0.3045826 0.79207325 1.7573833 1.7614126 1.3980293 1.2755203 1.1933098 -0.3794961 -1.2788525 -2.379703 -2.925456 -3.9893472 -4.4514675][-2.3071871 -0.85990334 0.58902836 1.4871187 1.7825937 2.3991795 2.6169424 2.1818161 1.7040462 0.27206802 -0.4374032 -2.1396971 -3.128818 -3.8593614 -4.2808676][-1.5699949 0.027751923 1.1997261 2.8775406 3.5916767 3.4726572 2.9946432 2.8000622 2.5874739 0.64668751 -0.47561026 -1.6871872 -2.2408805 -3.1320176 -3.7727358][0.39585876 1.6228123 2.3970032 2.6610518 3.0034952 2.9954939 2.5668039 2.0025206 1.6606302 0.8718605 0.51776505 -1.2761126 -2.7426834 -3.4173927 -3.753552][-0.27263546 0.45000744 0.71202469 1.5157957 1.7615757 1.6684723 1.4134169 1.2333412 1.0500708 -0.34652615 -1.4158702 -2.0565419 -2.3043475 -3.2420144 -3.6347585][-1.1738992 -1.1325693 -1.3050046 -1.0828276 -0.70946455 -0.73779392 -0.81880903 -0.59614229 -0.48570871 -1.0921512 -1.6837783 -3.0856042 -4.4282193 -5.263113 -5.4761505][-2.9350963 -2.9211993 -2.9755206 -3.1344342 -3.1844602 -2.6699262 -2.3096166 -2.2363477 -2.2524381 -3.4836369 -4.5881495 -5.0811443 -4.9118547 -5.6040506 -6.1165137][-3.4366846 -4.3197794 -5.1677513 -5.0824614 -5.0428829 -4.9773035 -4.5795217 -4.1264591 -4.3991175 -5.3178787 -5.7065654 -6.5063076 -7.073143 -7.3419242 -6.983057][-5.8194804 -5.875535 -5.8247242 -5.9243774 -6.0276532 -5.5139217 -5.244668 -5.4903088 -5.642911 -5.9113474 -6.768003 -7.4065356 -7.9603205 -7.9912996 -8.1314][-4.7507954 -6.0890079 -6.8458967 -7.0643182 -7.11281 -7.0989685 -6.967051 -6.7335033 -6.5972619 -6.8649693 -7.0434408 -7.6414924 -7.8178177 -7.8063231 -7.9273543]]...]
INFO - root - 2017-12-15 20:36:35.384935: step 57610, loss = 0.40, batch loss = 0.28 (12.1 examples/sec; 0.662 sec/batch; 50h:30m:49s remains)
INFO - root - 2017-12-15 20:36:41.807849: step 57620, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 48h:02m:58s remains)
INFO - root - 2017-12-15 20:36:48.236657: step 57630, loss = 0.37, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 49h:02m:42s remains)
INFO - root - 2017-12-15 20:36:54.747794: step 57640, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 49h:09m:44s remains)
INFO - root - 2017-12-15 20:37:01.293034: step 57650, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 50h:33m:35s remains)
INFO - root - 2017-12-15 20:37:07.724439: step 57660, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 48h:17m:01s remains)
INFO - root - 2017-12-15 20:37:14.104370: step 57670, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 47h:44m:10s remains)
INFO - root - 2017-12-15 20:37:20.569073: step 57680, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 49h:08m:38s remains)
INFO - root - 2017-12-15 20:37:27.096606: step 57690, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 49h:54m:37s remains)
INFO - root - 2017-12-15 20:37:33.491945: step 57700, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 48h:26m:29s remains)
2017-12-15 20:37:33.976259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9988179 -4.7065887 -4.2922621 -4.1744123 -4.1103649 -4.1002216 -4.2523108 -3.9798486 -4.0509529 -4.7031841 -6.1647186 -7.4088078 -8.6862392 -9.43847 -9.8981667][-4.0147104 -4.511488 -4.5977497 -4.9971251 -5.0615196 -4.9857693 -4.9652996 -4.6019154 -4.0852208 -4.2521148 -5.3515749 -6.5216079 -8.0196342 -8.8178711 -9.5113764][-2.8189521 -2.8861265 -3.5820675 -4.4857883 -4.6925192 -4.7175045 -4.8891873 -4.5553393 -3.9440444 -3.4552898 -4.2871304 -4.6627064 -5.9801331 -6.6013384 -7.2766008][-1.6218944 -1.2683291 -1.7978468 -2.2867441 -2.9535546 -3.043364 -2.8985949 -2.8369827 -2.661253 -2.3524728 -3.2136245 -3.8557243 -4.9742203 -5.370059 -5.8662086][-1.5420384 -1.3878503 -1.2601647 -1.0787849 -1.0847888 -0.73447609 -0.20054197 -0.18301821 -0.044826984 -0.22985554 -1.4623404 -2.9602041 -4.3532491 -4.9991736 -5.3817844][-1.7765303 -1.9448562 -1.828033 -1.5033855 -0.597209 0.6157608 1.6606636 1.8620577 2.0122786 1.411952 -0.25121593 -2.0858827 -4.2080717 -5.4074216 -6.3359013][-2.7999725 -2.4852724 -1.5627875 -0.46690416 0.73599911 2.1057262 3.31151 3.4820633 3.2751875 2.1082563 -0.0944953 -2.1628838 -4.6186247 -5.9241714 -6.8355312][-3.1099062 -2.9445 -2.0456123 -0.7324872 0.86350918 2.3027573 3.7782164 4.1750803 4.1445541 2.7752352 0.20449638 -2.1056719 -4.4085932 -5.6429939 -6.4481106][-3.3022981 -3.0258613 -2.1628075 -1.0363564 0.59457016 1.9995852 2.8719759 2.9647217 2.9941788 2.1161022 -0.0796237 -2.4499726 -4.7200756 -5.686862 -6.0174708][-3.585865 -3.4219708 -2.7744012 -1.8712444 -0.42581606 0.6817646 1.3604994 1.1973152 0.77332497 -0.42951202 -2.6648135 -4.3015251 -5.8379192 -6.6391249 -7.160326][-6.0525017 -5.1659827 -4.5246887 -4.3080645 -3.3726583 -2.9055467 -2.2415891 -2.2711706 -2.6515012 -3.7299833 -5.1623812 -6.2551947 -7.3813434 -7.8391337 -7.8035064][-6.1805539 -5.7650084 -5.7163534 -5.4636154 -5.0193911 -5.0925746 -4.8299026 -4.7758803 -4.7443676 -5.7200313 -6.9865618 -7.2999363 -7.7238731 -8.0816326 -8.4029226][-6.8541131 -6.5744638 -6.8706331 -6.8449492 -6.3934855 -6.1293907 -5.958046 -6.228569 -6.3744869 -6.6627836 -7.7924323 -7.8828373 -7.7423425 -7.0029387 -6.8388858][-6.1582365 -6.2912169 -6.26363 -6.2200265 -6.25039 -5.8003902 -4.9085264 -5.1585264 -5.5839682 -5.754055 -6.3739104 -6.7355871 -7.3047805 -7.0108461 -6.3800154][-5.1386614 -5.739872 -6.2890663 -6.3531542 -6.0693817 -5.7171669 -5.2431488 -5.3444805 -5.2253809 -5.47139 -5.8445826 -5.8626566 -5.9268675 -6.3673506 -6.4412651]]...]
INFO - root - 2017-12-15 20:37:40.503942: step 57710, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 49h:54m:39s remains)
INFO - root - 2017-12-15 20:37:46.865381: step 57720, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 50h:14m:25s remains)
INFO - root - 2017-12-15 20:37:53.321631: step 57730, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.668 sec/batch; 50h:57m:13s remains)
INFO - root - 2017-12-15 20:37:59.850153: step 57740, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 48h:32m:37s remains)
INFO - root - 2017-12-15 20:38:06.253471: step 57750, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 48h:41m:54s remains)
INFO - root - 2017-12-15 20:38:12.764844: step 57760, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 50h:09m:07s remains)
INFO - root - 2017-12-15 20:38:19.150547: step 57770, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 49h:05m:31s remains)
INFO - root - 2017-12-15 20:38:25.632872: step 57780, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 50h:26m:34s remains)
INFO - root - 2017-12-15 20:38:32.100310: step 57790, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 50h:02m:30s remains)
INFO - root - 2017-12-15 20:38:38.593393: step 57800, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 50h:49m:18s remains)
2017-12-15 20:38:39.129594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.158473 -2.686738 -2.8558326 -2.8136621 -2.9362144 -2.9507995 -3.0080013 -2.9252529 -3.1784043 -4.3507166 -4.8591466 -5.7456613 -6.274848 -6.440043 -7.4994187][-2.261416 -2.7586336 -3.19282 -3.4025126 -3.5236907 -3.4314818 -3.2031074 -2.902669 -2.8871317 -4.3052139 -5.0985713 -6.5488577 -7.5844049 -7.85745 -8.5821953][-2.4608283 -2.4024849 -2.1633863 -2.1235261 -1.9822354 -1.946125 -1.9912343 -1.8961544 -2.0344563 -3.2048359 -3.6112366 -4.8567495 -5.957449 -6.4143162 -7.4365048][-1.1980753 -0.87244749 -0.65222645 -0.40460825 -0.27537775 -0.17483282 -0.14065838 -0.065899372 -0.10844994 -1.4594121 -2.3035593 -3.8512857 -5.0050707 -5.3390651 -6.3102098][-1.0368357 -0.39368343 0.10193253 0.20330667 0.20761061 0.44419956 0.60943317 0.68868446 0.92209721 -0.30420685 -1.1181059 -2.6961308 -4.0754833 -4.7485666 -5.9362717][-1.4120517 -0.73187113 -0.057923794 0.34497452 0.57284641 0.80383968 1.2293825 1.4925947 1.7083082 0.20193577 -0.88143873 -2.3953204 -3.7161431 -4.0413008 -4.9866138][-1.9357266 -1.127574 -0.4797616 0.36444569 1.0194063 1.1796017 1.5576963 1.9152946 2.177247 0.59960079 -0.56975079 -2.2073846 -3.6218524 -4.0561905 -5.1002035][-2.2223577 -1.2826128 -0.78825188 0.17761946 1.1181173 1.8542147 2.3612461 2.5386915 2.769908 1.0924273 -0.094894409 -1.7196441 -3.265111 -3.9136906 -5.0188651][-2.5831327 -1.7219286 -0.89606333 0.066037178 0.88328648 1.6927719 2.3783379 2.7518644 2.9752398 1.0710783 -0.38468742 -1.9742332 -3.4353671 -4.1125278 -5.2081766][-2.8625717 -2.3067989 -1.5749207 -0.6517911 0.087387085 0.81437588 1.594986 1.92523 2.0867043 0.29480791 -0.853899 -2.5185061 -4.1420374 -4.7016687 -5.8966742][-4.2003212 -3.5737319 -3.0332947 -2.208766 -1.434288 -0.60349417 0.17701626 0.26300621 0.31703568 -1.4053335 -2.5053468 -3.5900655 -4.7525196 -5.2369843 -6.0640106][-5.3916745 -4.7483187 -4.3368921 -3.7571933 -3.0501547 -2.458734 -1.8981247 -1.604938 -1.3673463 -2.6878991 -3.2904649 -4.3129244 -5.3733091 -5.6062818 -6.2670584][-6.1451459 -5.7925634 -5.4871931 -4.9901867 -4.3636746 -3.9650371 -3.6362677 -3.5467024 -3.584537 -4.157196 -4.6050854 -5.181859 -5.8211288 -6.1621056 -6.5154557][-6.4132948 -5.9591627 -5.6559715 -5.2951097 -4.7445717 -4.4233789 -4.3713794 -4.5191879 -4.537066 -5.0771933 -5.214231 -5.6132407 -5.90082 -6.0617552 -6.3966503][-7.6419873 -7.3179526 -7.1913786 -6.7391725 -6.4665875 -6.205658 -5.7937636 -5.9213567 -6.1165323 -6.1317878 -6.2615047 -6.39398 -6.4368849 -6.5310564 -6.6167593]]...]
INFO - root - 2017-12-15 20:38:45.530157: step 57810, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 49h:15m:53s remains)
INFO - root - 2017-12-15 20:38:52.008314: step 57820, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 48h:24m:23s remains)
INFO - root - 2017-12-15 20:38:58.444760: step 57830, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 49h:02m:40s remains)
INFO - root - 2017-12-15 20:39:05.016441: step 57840, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 50h:26m:48s remains)
INFO - root - 2017-12-15 20:39:11.422101: step 57850, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 48h:12m:56s remains)
INFO - root - 2017-12-15 20:39:17.790321: step 57860, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 47h:44m:08s remains)
INFO - root - 2017-12-15 20:39:24.152341: step 57870, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 49h:04m:29s remains)
INFO - root - 2017-12-15 20:39:30.550166: step 57880, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 48h:48m:04s remains)
INFO - root - 2017-12-15 20:39:36.867631: step 57890, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.619 sec/batch; 47h:10m:47s remains)
INFO - root - 2017-12-15 20:39:43.242944: step 57900, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 50h:15m:25s remains)
2017-12-15 20:39:43.825890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.858191 -4.6634369 -4.0754681 -3.9902637 -3.9087114 -3.1981134 -2.5019689 -1.8078661 -1.6349893 -3.8787258 -4.8494263 -6.4262867 -7.6039491 -8.5283947 -8.7451115][-4.0597477 -4.3021178 -4.4175243 -4.5202794 -4.0826511 -3.4451065 -2.378695 -2.0416946 -1.5160828 -3.1221266 -3.8131504 -5.5594187 -7.3331823 -8.8477478 -9.4744186][-3.2951884 -3.4582696 -3.1890049 -2.7330332 -2.6664519 -2.4338818 -1.8901591 -1.5861473 -0.78503895 -2.0594559 -2.6492038 -4.5209837 -5.736237 -7.1326323 -7.9306026][-2.7928309 -2.6114249 -2.1711493 -1.6950402 -1.2478204 -0.55321646 -0.35537338 -0.39521837 0.051305771 -1.197453 -1.8277025 -3.5657973 -4.9853091 -6.303813 -7.2334447][-3.0300193 -1.7520971 -0.68954468 -0.50786495 -0.2299118 0.30758715 0.75123787 1.449646 2.0377626 0.63548279 -0.30444956 -2.1082492 -4.0678596 -5.3272419 -5.9452934][-1.5538278 -1.0657911 -0.34136772 0.32821465 1.1007433 2.0726738 2.9711914 3.3187027 3.7463388 2.1500931 0.88872051 -1.2091765 -2.9831824 -4.56196 -5.2397447][-1.5189924 -0.81586504 0.039505005 0.74868488 1.6542673 2.6933584 3.8877649 4.3616085 4.6430683 2.7414494 1.2865782 -1.2108984 -3.5251427 -5.0120153 -5.8595076][-1.9560328 -1.1039443 -0.43194675 0.7973938 1.7262096 2.7379274 3.8009138 4.5205832 4.9435167 2.8508081 1.3073292 -0.93464518 -3.0419474 -5.0680647 -6.2734518][-1.5492134 -0.59715796 0.52935123 1.2420254 1.8203688 2.3763981 3.2114172 3.899497 4.0800676 2.1486483 0.93844604 -1.3566403 -3.3437147 -4.9914947 -5.9534588][-1.8045759 -0.35477114 0.46391773 0.94826889 1.6616049 2.3219233 3.212986 3.4639254 3.579792 1.1840076 -0.47038269 -2.6246858 -4.1928225 -5.4486837 -5.963912][-2.7501163 -1.592082 -0.29351664 0.29519606 0.72714806 1.4101124 2.0244446 2.0588369 1.9723234 -0.31488609 -1.3912029 -3.1952004 -4.707408 -5.8784132 -6.0267334][-2.9474297 -1.9804578 -0.99485493 0.26820707 1.0957413 1.6959581 1.6597548 1.3852711 1.1090965 -0.92004395 -2.4022841 -3.921207 -4.991786 -5.8133059 -6.3756027][-3.1362267 -2.5720959 -1.0856013 -0.1928525 0.72270489 1.1206331 1.6535816 1.36098 0.96449375 -1.0954418 -2.8606491 -4.2096167 -5.083653 -5.9101496 -6.5269012][-3.5648098 -2.7084494 -2.1417904 -1.5211239 -0.80655336 -0.61301661 -0.036801815 -0.29123735 -0.29452991 -1.2382693 -2.1536007 -3.8215387 -4.7839651 -5.412374 -6.0585818][-4.9652905 -4.0971236 -3.5008621 -3.1953816 -3.189621 -2.8833389 -2.8431954 -3.191258 -3.7735176 -4.0677595 -3.8722179 -4.5456557 -5.3229923 -5.709116 -6.0264339]]...]
INFO - root - 2017-12-15 20:39:50.210314: step 57910, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 49h:18m:36s remains)
INFO - root - 2017-12-15 20:39:56.644798: step 57920, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 50h:06m:05s remains)
INFO - root - 2017-12-15 20:40:03.097974: step 57930, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 48h:11m:44s remains)
INFO - root - 2017-12-15 20:40:09.494618: step 57940, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 48h:21m:12s remains)
INFO - root - 2017-12-15 20:40:15.880376: step 57950, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 50h:34m:30s remains)
INFO - root - 2017-12-15 20:40:22.342291: step 57960, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 47h:36m:16s remains)
INFO - root - 2017-12-15 20:40:28.753149: step 57970, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 48h:05m:26s remains)
INFO - root - 2017-12-15 20:40:35.246747: step 57980, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 48h:23m:51s remains)
INFO - root - 2017-12-15 20:40:41.722616: step 57990, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 49h:53m:15s remains)
INFO - root - 2017-12-15 20:40:48.173652: step 58000, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 47h:20m:59s remains)
2017-12-15 20:40:48.746128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4851933 -5.9944057 -6.1496153 -6.337914 -6.8119287 -7.0406675 -7.1450448 -7.2541347 -6.923059 -7.4142785 -8.3803225 -9.0261116 -9.3551674 -8.9331875 -9.4861879][-5.181838 -5.4191422 -6.0659804 -6.3546729 -6.6218891 -7.0281787 -7.323163 -7.4701438 -7.3712864 -7.379992 -8.0189257 -8.7857733 -9.0839176 -8.7729855 -9.6884947][-5.2383113 -5.0619087 -5.0481348 -4.9389248 -5.1260357 -5.6162934 -5.5766654 -5.7907972 -6.1200151 -6.8042417 -7.6476316 -7.9220963 -8.4094353 -8.0497389 -8.8077278][-5.8469257 -5.2568455 -4.9597483 -4.3360472 -3.765146 -3.5974512 -3.2126017 -3.4118528 -3.6948383 -4.5770674 -5.4836092 -6.1035447 -6.7150803 -6.6873231 -7.7538557][-5.302887 -4.6422243 -4.3515263 -3.5436559 -2.5677333 -1.9128466 -1.3247557 -1.2798471 -1.1686692 -2.2021761 -3.2568574 -4.0778933 -5.0782108 -5.382432 -6.4992042][-4.640151 -4.0088282 -3.2530451 -1.9596291 -0.80790186 -0.056337833 1.0135336 1.0228586 1.0198765 -0.44657946 -2.0228434 -3.0011106 -4.2162642 -4.4114327 -5.4770021][-3.9651337 -3.6079392 -2.9180565 -1.7781515 -0.22569704 0.86891365 1.9574251 1.9754 2.0733509 0.42883396 -1.2533326 -2.6979866 -4.0575566 -4.3929262 -5.4560437][-4.0624723 -3.7897668 -3.0462236 -1.6639295 -0.2707653 0.7637167 1.7718143 1.7918606 1.8051395 0.12607241 -1.4807458 -3.0039248 -4.3203859 -4.5220966 -5.6784668][-4.9189053 -4.3588362 -3.7094393 -2.8439403 -1.8672929 -0.540575 0.94312477 1.3316145 1.4384365 -0.29788685 -2.3967824 -3.8277421 -4.89958 -5.1419373 -6.03487][-6.1227856 -6.0436144 -5.3951492 -4.8473234 -3.8831153 -2.9960408 -2.3043985 -1.5542197 -0.83596468 -1.9334397 -3.2856169 -4.5647478 -5.9160662 -5.9245796 -6.612958][-7.3388953 -7.1732492 -6.7900686 -6.4972405 -5.981883 -5.6523294 -4.9310045 -4.5660405 -4.3024116 -4.7341323 -5.7410555 -5.9710631 -6.37739 -6.4586706 -7.0960612][-8.352561 -8.1802111 -7.8362813 -7.5608697 -7.1649971 -7.0099874 -6.4401917 -6.3959661 -6.203968 -6.5461516 -7.4263654 -7.4499784 -7.5745568 -7.2129021 -6.8191729][-8.7597923 -8.5451574 -8.4169893 -7.9311967 -7.2812071 -7.0365682 -6.6163831 -6.6554947 -6.5885172 -6.5590849 -7.1337762 -7.2473788 -7.4660387 -7.1932521 -6.7496643][-7.9402752 -8.0233088 -7.7842326 -7.3718224 -6.6910825 -6.4422226 -6.0602274 -6.0894508 -6.0565233 -6.0026159 -6.4103246 -6.3391404 -6.1855159 -6.1931305 -5.8696265][-7.2609863 -7.3168559 -7.2630563 -7.1599708 -6.6303921 -6.2585158 -5.8095121 -5.878222 -6.1920528 -6.2108412 -6.198525 -6.2326865 -6.199769 -6.1370878 -5.8096528]]...]
INFO - root - 2017-12-15 20:40:55.276843: step 58010, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 49h:26m:55s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 20:41:01.652428: step 58020, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 48h:49m:05s remains)
INFO - root - 2017-12-15 20:41:08.110008: step 58030, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 50h:15m:49s remains)
INFO - root - 2017-12-15 20:41:14.517595: step 58040, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 48h:41m:56s remains)
INFO - root - 2017-12-15 20:41:20.880552: step 58050, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 48h:25m:58s remains)
INFO - root - 2017-12-15 20:41:27.301171: step 58060, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.622 sec/batch; 47h:23m:08s remains)
INFO - root - 2017-12-15 20:41:33.838981: step 58070, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 46h:55m:51s remains)
INFO - root - 2017-12-15 20:41:40.321744: step 58080, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 49h:58m:07s remains)
INFO - root - 2017-12-15 20:41:46.823139: step 58090, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 49h:36m:10s remains)
INFO - root - 2017-12-15 20:41:53.148069: step 58100, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.623 sec/batch; 47h:27m:01s remains)
2017-12-15 20:41:53.633672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3652058 -6.4158969 -7.3155909 -7.6432562 -7.8509903 -7.1356597 -6.3780575 -5.5242605 -5.0486469 -5.3796425 -6.1508446 -7.1646709 -7.8023567 -8.0853424 -8.3243361][-5.46614 -5.6897717 -5.9056091 -6.6983838 -6.8552837 -6.6184354 -6.6114454 -6.1437917 -5.63175 -5.7561111 -6.448751 -6.9208 -7.5585575 -7.9970579 -8.2268162][-3.1868162 -4.031496 -4.19501 -4.3067408 -4.2905893 -4.4714613 -4.6920881 -4.8383312 -4.9446373 -5.261034 -5.8213234 -6.0873184 -6.4159064 -6.5028753 -6.9565754][-1.7675595 -2.5639386 -3.1936774 -3.1998672 -2.84233 -2.6966987 -2.6513171 -3.0209603 -3.1433573 -3.7302577 -4.6045227 -4.8747597 -5.1799517 -5.2844 -5.4318709][-1.5400295 -1.3224206 -1.5194464 -1.965611 -2.0052547 -1.2568831 -0.8869977 -1.1485677 -1.3887053 -2.2498274 -2.9391379 -3.3880754 -3.7619555 -3.908874 -4.2280989][-1.6005049 -1.5749865 -1.9527168 -1.3242431 -0.71885824 -0.18906021 0.28571415 0.3784771 0.32938671 -0.67414522 -1.686245 -2.3263631 -2.8301692 -3.1754475 -3.5002952][-1.2724352 -1.8880382 -2.3040233 -1.9918261 -0.91148567 -0.10172749 0.47817135 0.77521992 1.0458765 0.33673954 -0.40568209 -1.2188387 -2.5923066 -2.8979235 -3.5150776][-1.5687828 -1.7441435 -2.0779276 -2.1688728 -1.18786 -0.11629963 0.54101086 0.64460468 0.93271637 0.25828123 -0.41413546 -1.2309098 -2.2983623 -2.876997 -4.1188459][-2.4864178 -2.3548827 -2.0904393 -1.6382403 -0.81980371 0.30198574 1.049264 1.1831808 1.313015 0.36236858 -0.76903486 -1.902935 -3.1072717 -3.5011659 -4.2000656][-4.9466181 -4.225193 -3.8494267 -2.6164432 -1.3578887 0.12894583 1.0537825 1.2845678 1.5426531 0.42118168 -0.6720233 -1.676578 -3.307476 -4.3750186 -5.32465][-8.5368662 -7.6418066 -6.528429 -4.8845472 -3.6310444 -2.1915946 -0.7599206 -0.46734953 -0.18348694 -0.80748796 -1.7167091 -2.8398995 -3.6786923 -4.2144861 -4.927021][-10.281299 -9.6664734 -8.4979906 -6.8921533 -5.5744686 -4.4886737 -3.3002381 -3.2989521 -3.0001612 -3.5408673 -4.11327 -4.5513148 -4.9873362 -5.2234955 -5.511076][-9.5842266 -9.47859 -8.5718117 -7.804162 -7.1824012 -6.0420337 -4.884079 -4.4512215 -4.0262461 -4.37 -4.8198857 -5.1618891 -5.5089588 -5.5210905 -5.2858343][-8.6705 -9.1439047 -8.9275656 -8.0897465 -7.6639829 -7.2210674 -6.7289095 -6.3275495 -5.8125849 -5.7201519 -5.9542155 -5.8112011 -5.8784137 -5.5089841 -5.3219051][-8.13606 -8.0820656 -7.8863616 -7.8874054 -7.9001307 -7.6226816 -7.4717655 -7.6535726 -7.3756819 -7.0653734 -6.8152981 -6.6421437 -6.4701233 -6.2025766 -5.729167]]...]
INFO - root - 2017-12-15 20:42:00.044343: step 58110, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.623 sec/batch; 47h:28m:48s remains)
INFO - root - 2017-12-15 20:42:06.447952: step 58120, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 48h:42m:28s remains)
INFO - root - 2017-12-15 20:42:12.963858: step 58130, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 49h:39m:35s remains)
INFO - root - 2017-12-15 20:42:19.463715: step 58140, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 51h:07m:17s remains)
INFO - root - 2017-12-15 20:42:25.995245: step 58150, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 49h:19m:27s remains)
INFO - root - 2017-12-15 20:42:32.479354: step 58160, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 47h:58m:09s remains)
INFO - root - 2017-12-15 20:42:39.010204: step 58170, loss = 0.34, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 48h:34m:50s remains)
INFO - root - 2017-12-15 20:42:45.486120: step 58180, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 48h:51m:43s remains)
INFO - root - 2017-12-15 20:42:51.931167: step 58190, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 49h:01m:20s remains)
INFO - root - 2017-12-15 20:42:58.264516: step 58200, loss = 0.36, batch loss = 0.24 (12.5 examples/sec; 0.638 sec/batch; 48h:36m:13s remains)
2017-12-15 20:42:58.775477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9745266 -4.747467 -4.8743591 -5.0056181 -4.4683385 -3.4925771 -2.6616025 -2.6986146 -3.4690223 -3.9041519 -4.5772524 -3.5808692 -5.9763875 -5.3226914 -4.7964478][-5.4261541 -4.606997 -3.8210235 -3.6885734 -3.3028116 -3.5908432 -5.0365191 -4.3873281 -2.0050426 -1.7701817 -3.7934625 -4.5787716 -7.2748876 -5.5151091 -5.1734524][-3.2465973 -3.7990246 -2.9813781 -1.9459343 -1.7536259 -0.92701244 0.45260334 -0.694366 -2.6431313 -3.4422579 -3.0662928 -1.3934779 -4.7737546 -4.8453536 -5.0662727][-0.77309752 -0.99097252 -3.064723 -3.3085933 -0.92443037 -0.57401943 -0.80042934 -1.1339021 0.380682 -0.77324867 -3.1124439 -3.5584726 -4.6703539 -4.0076461 -3.638073][-5.581552 -2.3743148 0.69935513 -0.41145134 -1.2607355 -1.4083223 -0.55028486 -1.4708853 -1.4645505 -2.1837645 -2.7493515 -2.71775 -5.1886368 -4.8245249 -4.94489][-2.879086 -2.9794354 -3.644382 -1.3689198 2.2207747 3.0724325 2.6564951 1.3965521 0.67934895 -0.69492865 -2.1994424 -2.7184215 -4.8678379 -4.4145985 -4.6002073][-2.3014793 -1.5624042 0.38102436 1.1001253 0.9007864 2.06291 3.8569565 4.0290842 3.7123299 1.2986555 -1.3970685 -2.0347176 -4.7649012 -4.3239293 -3.8794606][-0.65443325 -1.1896319 -0.55943775 1.8539391 3.4398718 3.5767841 2.5133543 2.5435925 3.6014156 2.0436525 -0.90470314 -1.8498988 -4.8260574 -4.8515806 -5.1500411][-1.1842117 -0.70309448 -0.11692572 0.91323185 2.8135929 4.17513 3.6517935 2.5401611 2.6200638 1.7947159 0.098289967 -0.61410236 -3.9985478 -4.14535 -4.7349172][-3.7878613 -3.2706189 -2.0712023 -1.0088115 -0.05503273 0.68582916 1.8353262 2.2267551 2.6516657 1.1410751 -0.65165424 -1.02774 -3.3477249 -3.5383739 -4.37165][-5.1048365 -4.4998713 -3.9463408 -4.3223743 -3.1923771 -2.17555 -1.637599 -2.0043907 -1.160924 -0.16358376 -2.3608675 -2.3597841 -4.1351337 -4.4622941 -4.9451313][-7.5494957 -6.8191795 -6.03987 -5.6667061 -5.5372009 -5.7152715 -4.7123661 -3.9014864 -3.9289429 -3.2686286 -4.2859159 -4.894495 -6.8215966 -6.4262094 -6.6630459][-5.8094168 -7.3616371 -8.0236969 -7.439796 -6.4652681 -5.3056211 -4.8284616 -4.7795525 -4.18309 -3.3825011 -4.82037 -4.6520443 -5.1382604 -5.5259776 -5.993535][-6.8096476 -6.3143311 -5.7403774 -6.8110948 -7.3628731 -6.5578642 -5.3856587 -5.4898758 -5.5645728 -6.0213165 -5.7517729 -5.0296841 -5.8127122 -5.4802351 -6.13097][-4.6347761 -5.720448 -5.6041265 -5.6010284 -5.2064495 -6.3334103 -6.4053636 -5.6651735 -5.4273739 -6.0834565 -6.1726856 -6.7767024 -7.0790973 -6.6014109 -6.2496333]]...]
INFO - root - 2017-12-15 20:43:05.236865: step 58210, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 49h:43m:36s remains)
INFO - root - 2017-12-15 20:43:11.656034: step 58220, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 49h:11m:59s remains)
INFO - root - 2017-12-15 20:43:18.109785: step 58230, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 49h:25m:27s remains)
INFO - root - 2017-12-15 20:43:24.431721: step 58240, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 48h:35m:48s remains)
INFO - root - 2017-12-15 20:43:30.903953: step 58250, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 49h:53m:53s remains)
INFO - root - 2017-12-15 20:43:37.307136: step 58260, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 49h:13m:29s remains)
INFO - root - 2017-12-15 20:43:43.689518: step 58270, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.627 sec/batch; 47h:47m:54s remains)
INFO - root - 2017-12-15 20:43:50.047342: step 58280, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 47h:42m:14s remains)
INFO - root - 2017-12-15 20:43:56.454539: step 58290, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 49h:56m:03s remains)
INFO - root - 2017-12-15 20:44:02.842800: step 58300, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 50h:37m:04s remains)
2017-12-15 20:44:03.425224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4334073 -8.2855473 -10.029049 -10.776808 -10.247261 -9.084178 -7.7908134 -6.1160994 -4.9024262 -5.7665577 -6.3266826 -7.4983263 -7.9700351 -7.7408743 -7.4850235][-5.7283564 -7.7036543 -9.7960854 -11.281475 -11.014935 -9.4795818 -8.1272135 -6.5443974 -5.9017386 -7.3916321 -7.9940548 -9.1398029 -9.5957108 -9.430007 -8.9489355][-4.3510208 -5.9285879 -7.9398541 -9.4265165 -10.05173 -9.1274662 -7.8945785 -6.6611824 -5.9546461 -7.2133741 -8.19111 -10.023965 -10.508483 -10.087875 -9.5088587][-3.8326595 -5.3158188 -7.0428829 -8.4294214 -8.8339758 -7.5881252 -6.2495193 -5.1831465 -4.5217886 -6.1732903 -7.2483611 -9.4008007 -10.422397 -10.290911 -10.080482][-3.5226455 -5.1693163 -6.2241068 -6.9078317 -6.46086 -4.67289 -3.1282763 -2.6514726 -2.4968071 -4.6683769 -6.2127724 -8.7416983 -10.186482 -10.260149 -10.250028][-5.5215054 -6.1625118 -6.6355844 -5.7884665 -3.8007829 -1.4810376 0.70689869 1.5142183 0.93045425 -2.2339392 -5.2819262 -8.1045208 -9.6949759 -9.8520584 -10.039342][-7.0395269 -6.9373984 -6.4865422 -4.8693848 -1.8999681 1.7453918 4.7975245 5.7882414 5.0685072 1.1990242 -2.8578544 -6.633203 -8.8923254 -9.4454327 -9.6053085][-6.5742855 -6.7405577 -6.1190815 -3.5466337 -0.13665247 3.2771263 6.5883255 7.8733215 7.3418646 3.487525 -0.61440229 -5.5532703 -8.2239513 -8.9460011 -9.103426][-7.2022719 -6.7328424 -6.0988197 -4.3012714 -1.8625913 1.9103203 5.4715939 6.3941479 6.5545187 2.8406334 -1.131474 -5.1287813 -7.9505954 -8.7218723 -9.2285385][-7.8636885 -7.2206206 -6.6871457 -5.4751549 -3.6257238 -0.843904 1.851181 3.1810055 3.3244848 0.15362358 -2.7966909 -6.58847 -8.8881588 -9.4679852 -9.6287327][-8.1178188 -7.679709 -7.290102 -6.9018579 -5.9852872 -3.9189098 -1.7658834 -0.83476305 -0.62950516 -3.3111567 -5.5286579 -8.4606905 -10.1555 -10.177988 -10.005536][-7.7168989 -7.4706659 -7.545661 -7.1000624 -6.6844621 -5.8100934 -4.8854122 -4.1991034 -4.3673744 -5.8833642 -6.65416 -8.9539995 -10.075462 -10.170465 -10.007088][-8.3064394 -7.83433 -7.6279993 -7.3274031 -6.9510913 -6.9035645 -7.2498851 -6.8548918 -6.2254395 -6.9731655 -7.5516562 -8.953578 -9.6613579 -9.7866278 -9.7370558][-7.91112 -7.7465615 -7.1970673 -6.72238 -6.4161758 -6.24506 -6.553834 -7.28087 -7.7929029 -7.5969772 -6.9903 -7.8079023 -7.9971561 -7.9838276 -8.2527962][-7.6750255 -7.5963607 -7.3122883 -6.6126461 -5.9593287 -5.9936204 -6.60091 -6.8487821 -7.4223466 -7.83908 -7.539144 -7.1011033 -6.3863668 -6.2715282 -6.3947577]]...]
INFO - root - 2017-12-15 20:44:09.946310: step 58310, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 49h:16m:51s remains)
INFO - root - 2017-12-15 20:44:16.371092: step 58320, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 48h:17m:45s remains)
INFO - root - 2017-12-15 20:44:22.886611: step 58330, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.643 sec/batch; 48h:56m:07s remains)
INFO - root - 2017-12-15 20:44:29.300830: step 58340, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 48h:52m:49s remains)
INFO - root - 2017-12-15 20:44:35.706882: step 58350, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:43m:15s remains)
INFO - root - 2017-12-15 20:44:42.095786: step 58360, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 49h:07m:46s remains)
INFO - root - 2017-12-15 20:44:48.485072: step 58370, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 47h:41m:25s remains)
INFO - root - 2017-12-15 20:44:54.888207: step 58380, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 47h:59m:23s remains)
INFO - root - 2017-12-15 20:45:01.251535: step 58390, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 48h:37m:35s remains)
INFO - root - 2017-12-15 20:45:07.716548: step 58400, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 48h:50m:13s remains)
2017-12-15 20:45:08.321485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9998541 -8.385478 -8.2970266 -8.2059736 -7.5992036 -6.4294491 -4.6180367 -3.0142355 -1.3146906 -1.0722866 -1.5683699 -2.4421339 -3.391119 -4.7295418 -5.9522161][-6.578517 -7.0484633 -7.2584434 -7.3895092 -7.1624017 -6.5305481 -5.629941 -4.0145741 -2.18818 -2.125586 -2.2000365 -2.9522715 -3.6558261 -4.3636656 -5.5783739][-6.3178248 -6.5945759 -6.4534 -6.5409446 -6.5323505 -6.3617454 -5.6770477 -4.8842134 -3.7290545 -3.5662394 -3.8306339 -4.8788729 -5.2919416 -5.5258684 -6.1762824][-6.1270061 -5.8116622 -5.41335 -4.894866 -4.9003592 -4.5497093 -3.9380369 -3.5299621 -2.8977013 -3.7109492 -4.7691679 -5.8405218 -6.3174481 -6.7812986 -7.1856713][-6.624217 -5.2688384 -4.2057309 -3.341372 -2.7194991 -2.2597942 -1.5444179 -1.1540465 -0.66874266 -1.8509741 -3.624506 -5.337347 -6.4996734 -7.3421373 -7.8674569][-5.4933996 -4.1419983 -2.8649492 -1.1901197 0.26549149 1.0272303 1.4729738 1.3156729 1.6447458 0.25389624 -1.4504843 -3.3957658 -5.2072697 -6.2695608 -7.08245][-5.1904116 -3.3381929 -1.5372162 0.1226449 1.8862352 3.0953226 3.9936352 3.6507597 3.5250092 1.7926817 0.027894497 -1.9454932 -3.3475862 -5.1409407 -6.6425705][-4.6022797 -3.1393228 -1.6584911 0.27272654 1.9675655 2.8180962 3.818018 4.0846481 4.0006828 1.764575 -0.4166894 -2.3971519 -3.617053 -4.6850133 -5.6505756][-4.9803786 -3.6501789 -2.4368906 -0.66653728 0.72253895 1.5630875 2.3470087 2.6091356 2.8157825 0.84030628 -1.4758105 -3.2392907 -4.3519273 -5.2855577 -5.8696456][-6.457273 -5.4431019 -4.4342108 -2.8009505 -1.679256 -0.48445511 0.35436153 0.58427238 0.53226662 -1.1170411 -2.1341262 -3.5652528 -4.918704 -5.74812 -6.2045431][-8.20125 -7.8727016 -7.0049119 -5.9550428 -5.2036867 -3.7660162 -2.6245089 -2.5253973 -2.6274209 -3.707021 -4.0509634 -4.8742771 -5.7229805 -6.538815 -7.4691181][-9.0975857 -8.8489647 -8.4743881 -7.5610824 -6.6707039 -5.9887733 -5.8026409 -5.227354 -4.7976861 -5.5538392 -5.2925386 -5.3416214 -5.616478 -6.3241458 -6.9430652][-9.2449284 -9.8105459 -9.609314 -8.5198774 -7.8676009 -7.2491503 -6.6716161 -6.9023056 -7.1279225 -7.1450944 -6.5592909 -6.0696931 -6.0765419 -5.8640051 -6.0925331][-8.7449112 -9.0544109 -9.11298 -8.7386541 -8.3729906 -7.7583137 -7.2654343 -7.5516391 -7.6772337 -8.1131115 -7.7932582 -7.4197297 -6.9694381 -6.477283 -6.3681641][-6.6536369 -6.8263669 -7.2369938 -7.03128 -7.3918476 -7.3121924 -7.1713552 -7.4055295 -7.4471464 -7.5392995 -7.8865705 -7.9185228 -7.6889071 -7.35148 -7.1115985]]...]
INFO - root - 2017-12-15 20:45:14.748306: step 58410, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 48h:47m:41s remains)
INFO - root - 2017-12-15 20:45:21.200047: step 58420, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 48h:29m:35s remains)
INFO - root - 2017-12-15 20:45:27.690656: step 58430, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.656 sec/batch; 49h:55m:03s remains)
INFO - root - 2017-12-15 20:45:34.185486: step 58440, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 48h:16m:29s remains)
INFO - root - 2017-12-15 20:45:40.615393: step 58450, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.629 sec/batch; 47h:52m:19s remains)
INFO - root - 2017-12-15 20:45:47.026483: step 58460, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 48h:33m:36s remains)
INFO - root - 2017-12-15 20:45:53.523522: step 58470, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 48h:33m:23s remains)
INFO - root - 2017-12-15 20:45:59.897361: step 58480, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 49h:27m:50s remains)
INFO - root - 2017-12-15 20:46:06.308892: step 58490, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 49h:54m:36s remains)
INFO - root - 2017-12-15 20:46:12.777092: step 58500, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 48h:54m:40s remains)
2017-12-15 20:46:13.277478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4258304 -4.4514036 -4.4453278 -4.3824615 -4.2140703 -3.8834684 -3.3715634 -3.0007496 -2.4984617 -3.6470051 -4.5064611 -5.5987306 -6.1565156 -7.0804467 -7.5021887][-3.94283 -4.050478 -4.6660271 -5.0910692 -5.1593423 -4.6447325 -4.184391 -4.0019274 -3.3560386 -4.1629362 -4.7701645 -5.770206 -6.4364042 -7.4244895 -7.8521152][-3.783958 -3.6530828 -3.7501817 -4.1084371 -3.9896553 -3.4858704 -3.0964766 -2.8962946 -2.600657 -3.7526417 -4.4641519 -5.4716215 -6.3982477 -7.5232582 -8.0553894][-3.3267479 -3.058043 -3.0875564 -3.2412333 -3.1264062 -2.4778666 -1.9771075 -2.1471515 -2.1643438 -3.4280314 -4.2935619 -5.3589869 -5.9940958 -7.1971955 -7.9382811][-4.2702541 -3.3097548 -2.6212811 -2.2533422 -1.9274249 -1.2844334 -0.94669628 -1.1832705 -1.3624616 -2.8571219 -3.8537574 -4.9000721 -5.6013141 -6.74092 -7.4766946][-4.8432631 -3.8384547 -2.6317677 -1.7606812 -1.0721178 -0.14334917 0.27731419 -0.088418961 -0.21329641 -1.7808523 -2.9199815 -4.0210767 -4.7949052 -6.0844874 -6.9339123][-4.2584572 -3.6499267 -2.5978732 -1.2815566 -0.24477911 0.76940823 1.1554413 1.0476942 0.89844513 -0.92723894 -2.1328154 -3.2185555 -3.7165468 -5.0673766 -6.1679659][-3.6882396 -2.7835798 -1.7304244 -0.53060055 0.46839046 1.1741133 1.3703451 1.4609776 1.6336441 -0.01281023 -1.4010077 -2.6009111 -3.1290884 -4.4392786 -5.4339247][-3.5501847 -2.8212686 -1.835412 -0.87070227 -0.054047585 0.39152527 0.49569225 0.87281895 1.324605 -0.30470467 -1.7287393 -2.9585471 -3.6182938 -4.7496548 -5.4210138][-3.1304736 -2.5444636 -1.9378839 -0.78602839 -0.0075764656 0.066394806 0.25945997 0.455904 0.78246784 -0.97211695 -2.284802 -3.4220185 -4.3307047 -5.50596 -6.0715904][-4.642725 -3.6401691 -2.5033798 -1.4549561 -0.72209311 -0.70180178 -0.92836761 -0.81241941 -0.51380825 -2.25101 -3.5670724 -4.449811 -5.2983017 -6.4696169 -6.8262296][-4.5780358 -3.7937315 -2.9187059 -1.9550858 -1.3063669 -1.0985422 -0.99510765 -1.0748262 -1.1235828 -2.4107003 -3.8204274 -4.9898376 -5.6643081 -6.805728 -7.2240286][-5.6619186 -5.0542068 -4.224659 -3.0620995 -2.0189714 -1.4668779 -1.3047948 -1.1672363 -1.4783039 -2.7040219 -3.7278461 -4.52157 -5.0408182 -5.930603 -6.2520661][-6.0747867 -5.342618 -4.5340977 -3.4208379 -2.3277292 -1.5037646 -1.4752374 -1.6295018 -1.9168487 -2.4799275 -2.7146616 -3.4127979 -4.11909 -5.0252743 -5.4077][-6.7599287 -6.2375059 -5.492672 -4.7680531 -3.8669612 -3.1399646 -3.3244739 -3.4435463 -3.5515656 -3.5256104 -3.4754906 -3.8253379 -4.3503895 -4.7771015 -4.9428797]]...]
INFO - root - 2017-12-15 20:46:19.682913: step 58510, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 47h:47m:43s remains)
INFO - root - 2017-12-15 20:46:26.177031: step 58520, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 49h:54m:33s remains)
INFO - root - 2017-12-15 20:46:32.671613: step 58530, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 48h:55m:09s remains)
INFO - root - 2017-12-15 20:46:39.107728: step 58540, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 48h:22m:48s remains)
INFO - root - 2017-12-15 20:46:45.438477: step 58550, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 47h:30m:23s remains)
INFO - root - 2017-12-15 20:46:51.919865: step 58560, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 48h:54m:50s remains)
INFO - root - 2017-12-15 20:46:58.291828: step 58570, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 48h:16m:25s remains)
INFO - root - 2017-12-15 20:47:04.758613: step 58580, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.671 sec/batch; 51h:02m:22s remains)
INFO - root - 2017-12-15 20:47:11.244415: step 58590, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 48h:57m:13s remains)
INFO - root - 2017-12-15 20:47:17.644917: step 58600, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 49h:53m:22s remains)
2017-12-15 20:47:18.216494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2617183 -5.3266349 -4.1661873 -3.2093906 -2.5886221 -2.0716906 -1.9020286 -2.4623151 -2.819118 -4.3987246 -4.4553738 -5.2088261 -6.1908369 -6.2667537 -6.7980251][-5.3786097 -4.1581926 -3.5491867 -2.7327585 -2.4108768 -2.2911968 -2.3969793 -2.350028 -2.4203682 -3.7255402 -4.1347246 -4.9976358 -6.1507368 -6.5067363 -6.852221][-6.1638188 -4.7181044 -3.4539151 -2.6165671 -2.2753749 -2.0891023 -1.8670807 -2.14533 -2.5347981 -3.8848672 -4.1847897 -5.1089573 -6.0090985 -6.4055123 -6.6714139][-5.4272251 -3.856535 -3.1127043 -2.3077512 -2.0734444 -1.4647107 -0.97714186 -0.9490509 -1.2532477 -2.8531876 -3.4894137 -3.821876 -5.0034819 -5.1116562 -5.9748006][-3.4319587 -2.89578 -2.3833418 -1.4292812 -0.62173367 -0.36253595 -0.16688395 -0.40069246 -0.076285839 -1.009696 -1.0872188 -2.5471768 -4.1141157 -4.8278837 -5.6586361][-2.0383253 -1.3382702 -0.52328968 -0.1121521 0.23239565 0.45666504 0.4099884 0.69484329 0.90856075 -0.30994987 -0.90156221 -2.108829 -3.570663 -4.6507835 -5.7046356][-0.46772671 0.16308308 1.157177 1.9134197 2.6167316 2.469058 2.0302048 1.3812428 0.84937954 -0.63610935 -1.2310958 -2.2866063 -3.8096597 -4.4758959 -5.1868052][1.2134819 1.2377243 1.6747456 2.2069788 3.1341496 3.5029163 3.690794 2.9876938 2.4947004 0.29384613 -1.0996141 -2.6460357 -4.3444033 -5.144433 -5.8148956][-0.62823963 0.866025 1.8455896 2.5177431 2.9972925 3.096818 3.0180626 2.9758339 2.4423466 0.42489243 -0.54589605 -2.7272792 -4.687582 -5.6722775 -6.7298203][-1.8158545 -1.0164008 -0.049957752 1.1049891 1.5696907 2.3872147 2.5715971 1.9505939 1.512064 -0.61194611 -1.4330125 -2.8458 -4.0269537 -5.1084504 -6.0941653][-3.2723045 -2.4348011 -1.3828669 -0.27850962 0.2484436 0.67778492 0.76659012 0.99052334 0.52460384 -1.6281719 -2.4439368 -3.403295 -4.2601938 -4.3714685 -5.6645861][-4.378962 -3.4060984 -2.979629 -2.4030437 -1.2667742 -0.65311432 -0.3223691 -0.41333008 -0.42129612 -1.5748138 -2.7165341 -4.3058443 -5.5323882 -5.5972443 -6.5513449][-5.7239985 -5.0931416 -4.2726612 -3.3330956 -2.8047843 -2.069479 -1.4702888 -1.3953667 -1.5697055 -2.9108706 -3.4229865 -4.5842223 -5.6932378 -6.2997031 -7.4252167][-5.9083247 -5.9480915 -5.5625858 -5.0119548 -3.9904094 -3.1912961 -2.7075014 -3.0909042 -2.8908877 -3.4708991 -4.7427416 -5.6670704 -6.2352571 -5.7044749 -6.9495692][-6.4161358 -5.990449 -5.9400425 -5.9448733 -5.8412061 -5.4444041 -5.0489445 -4.38365 -4.2119718 -4.7277355 -5.6668148 -6.1003337 -6.83704 -7.3925281 -7.4481893]]...]
INFO - root - 2017-12-15 20:47:24.733251: step 58610, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 49h:22m:56s remains)
INFO - root - 2017-12-15 20:47:31.162680: step 58620, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 48h:28m:51s remains)
INFO - root - 2017-12-15 20:47:37.494268: step 58630, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 48h:46m:21s remains)
INFO - root - 2017-12-15 20:47:43.898998: step 58640, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 47h:04m:02s remains)
INFO - root - 2017-12-15 20:47:50.282257: step 58650, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 48h:10m:02s remains)
INFO - root - 2017-12-15 20:47:56.717547: step 58660, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 50h:09m:07s remains)
INFO - root - 2017-12-15 20:48:03.100306: step 58670, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 50h:21m:36s remains)
INFO - root - 2017-12-15 20:48:09.618803: step 58680, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 49h:47m:38s remains)
INFO - root - 2017-12-15 20:48:16.002995: step 58690, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 47h:48m:31s remains)
INFO - root - 2017-12-15 20:48:22.444788: step 58700, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:44m:38s remains)
2017-12-15 20:48:22.994640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.869174 -2.1194367 -1.8707023 -1.9460888 -2.05619 -2.2005248 -2.3866444 -2.8424835 -3.2869139 -4.9701986 -4.9030318 -5.2998638 -6.0540771 -6.4009285 -7.0084429][-3.2322626 -3.1615772 -3.1467395 -2.9501896 -3.5664754 -4.0826244 -4.2525072 -4.5926714 -4.8503461 -6.2509832 -5.5322533 -5.7776494 -6.4842939 -7.2809706 -7.7303686][-3.7973549 -3.2731647 -3.0563092 -3.5452089 -3.8406856 -3.6521044 -3.7027545 -4.0974426 -4.2558632 -5.6728525 -4.92095 -5.2284794 -5.8642874 -6.4047694 -6.7367311][-4.0374808 -3.3316102 -3.3246164 -2.5468659 -2.3171716 -2.225256 -1.8528123 -1.7853274 -1.9069052 -3.9512691 -4.1058664 -4.3753119 -4.8605051 -5.4734669 -5.9570937][-4.7699351 -3.6245203 -2.2645779 -1.9596853 -2.2381339 -1.2630448 -0.44953012 -0.13278389 -0.03565979 -2.0393667 -2.2648339 -2.8893847 -3.9161456 -4.63824 -5.2289925][-4.2411232 -3.1795588 -2.7442245 -1.3952508 0.036980152 0.62907219 1.0345211 1.2020712 0.99520111 -1.1695986 -1.7502589 -2.5531297 -3.7214327 -4.794239 -5.8914137][-4.8361464 -3.5483961 -2.1159382 -0.79354382 -0.0044236183 1.7450171 3.1527319 2.9561596 2.5716324 0.051243305 -0.73124027 -1.7437048 -3.4891973 -4.834228 -5.8901038][-4.9232874 -3.2406096 -2.1188641 -0.38163376 1.9496708 3.2860212 4.0368996 4.1547604 3.7704077 1.2079992 0.53735256 -0.60889721 -2.4733143 -4.0324745 -5.1006126][-4.0826163 -3.1806788 -1.7445207 0.083664417 1.5683746 2.8528748 3.7252159 3.7779064 3.8254681 1.7871304 1.1201773 -0.49686432 -2.3731389 -3.7950816 -5.025373][-4.6822863 -3.4428253 -2.224041 -1.0955749 0.38564777 1.6837959 2.5067434 2.6936769 2.793602 0.94169426 0.57723522 -0.57890987 -2.0610647 -3.8342481 -5.02915][-4.4081392 -4.0738606 -3.2295556 -2.2220049 -1.2909441 -0.54492331 0.33469391 0.75439167 1.0855188 -0.67443657 -1.0057683 -1.7755494 -2.9946923 -3.7959335 -4.5295825][-5.233427 -4.3889122 -3.5967922 -3.3832932 -2.9843221 -2.3935685 -1.6061182 -1.5769339 -1.4873195 -2.3459983 -2.0049314 -2.4501386 -3.5131807 -4.2812114 -5.0739202][-4.6192493 -4.5587025 -4.4726648 -4.299602 -4.06333 -4.15698 -3.7262959 -3.6973131 -3.2554474 -3.5821366 -3.0783525 -3.4220605 -4.4461956 -5.0568342 -6.0247645][-4.2067442 -4.6328554 -4.9931097 -5.3364487 -5.7214642 -5.6186705 -4.8607225 -4.709506 -4.5497561 -4.55546 -3.9615762 -4.2520609 -5.1104689 -5.3667479 -6.1430774][-5.4633284 -5.4412603 -4.6172228 -5.5172834 -6.16358 -6.6716843 -6.0707259 -5.9201803 -6.1408358 -5.3478069 -5.1872873 -5.664731 -5.9205055 -6.0901814 -6.4379139]]...]
INFO - root - 2017-12-15 20:48:29.418700: step 58710, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 49h:19m:04s remains)
INFO - root - 2017-12-15 20:48:35.800244: step 58720, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 48h:33m:19s remains)
INFO - root - 2017-12-15 20:48:42.326986: step 58730, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 48h:11m:55s remains)
INFO - root - 2017-12-15 20:48:48.719543: step 58740, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 49h:01m:18s remains)
INFO - root - 2017-12-15 20:48:55.138404: step 58750, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 49h:04m:05s remains)
INFO - root - 2017-12-15 20:49:01.531152: step 58760, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 48h:28m:38s remains)
INFO - root - 2017-12-15 20:49:07.944262: step 58770, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 50h:35m:56s remains)
INFO - root - 2017-12-15 20:49:14.400846: step 58780, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:40m:57s remains)
INFO - root - 2017-12-15 20:49:20.846090: step 58790, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 49h:09m:59s remains)
INFO - root - 2017-12-15 20:49:27.310339: step 58800, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 48h:54m:52s remains)
2017-12-15 20:49:27.842416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.949882 -4.9668546 -6.1743803 -6.9751892 -7.5840383 -7.9098883 -7.1421075 -6.3521366 -5.3932076 -5.6347175 -6.0172253 -6.8339744 -7.4212284 -7.3991013 -7.6549759][-3.6008959 -5.1808352 -6.7666092 -7.5015092 -7.9941845 -8.3942413 -8.3687038 -7.9469953 -6.7440963 -6.7904296 -7.2623572 -7.73725 -7.8466864 -8.2852421 -9.4700279][-4.1203957 -4.6401448 -5.1908393 -6.1384344 -6.9995112 -7.5004177 -7.4268 -7.0700169 -6.7846642 -7.1664734 -7.6371202 -8.10104 -7.9444013 -8.3035564 -8.8522358][-3.8147547 -4.1988811 -4.9013419 -4.9946442 -5.1121187 -5.3994908 -5.0606565 -4.4421616 -3.7439604 -4.07975 -5.294333 -6.5157409 -6.633131 -7.3104362 -8.0458927][-4.6118927 -4.314992 -3.9530435 -3.4961882 -3.2179031 -2.7889323 -1.9023986 -1.6742373 -1.224577 -1.909771 -3.1045184 -4.2900996 -4.7353821 -5.4862666 -6.4888744][-4.8031836 -4.3079605 -3.3105431 -2.1087155 -1.1445704 -0.21329403 0.95819187 1.1433945 1.481226 0.25784397 -0.62638283 -1.8346825 -2.6271443 -3.6073956 -4.9591713][-3.9667645 -3.672266 -2.7818804 -1.1847296 0.41076565 1.6531334 2.9306479 3.3850889 3.5240126 1.5956774 0.37320518 -0.65601826 -1.821981 -2.9072289 -4.1601658][-2.6039824 -1.8887763 -1.3096929 -0.27616978 0.75533676 2.1081829 3.679081 4.1455574 4.1448975 2.3145523 0.95995522 -0.78124619 -2.2812977 -3.2591691 -4.2660828][-3.5312915 -2.3369331 -0.88122559 -0.23932743 0.081979752 1.1752472 2.0458765 2.7269468 3.3381157 1.7111883 0.26851559 -1.1410899 -2.2953291 -3.4384146 -4.6109958][-4.6901407 -3.679636 -2.9057994 -1.7487159 -0.74410582 -0.28776741 -0.033290386 0.52581978 0.90846062 -0.73166847 -1.9971867 -3.14845 -4.1636314 -4.9474726 -6.2088571][-6.396915 -5.4295273 -4.8582926 -4.0430555 -3.1568642 -2.0929198 -1.7618246 -2.0265346 -2.4192929 -3.6386147 -4.3400049 -4.722723 -5.3530188 -6.2456808 -7.0873389][-8.4844112 -7.416398 -6.9666638 -6.3291278 -5.4341393 -4.3796968 -3.9523408 -3.7702489 -3.7441416 -5.3890476 -6.3281837 -6.0689163 -5.976965 -6.1032977 -6.1348014][-8.8042078 -8.8470011 -8.909523 -7.9444561 -7.0469575 -6.4455271 -5.9112949 -5.5084448 -5.0439649 -5.9322252 -6.7403893 -6.7727876 -6.5346365 -6.3610172 -6.0624394][-9.0529881 -8.2095556 -7.4406366 -7.210597 -6.8363442 -6.210639 -5.7001009 -5.6988516 -5.7240038 -6.0620685 -6.1755152 -5.9075437 -5.6693621 -5.3483672 -5.0303173][-7.8985214 -8.2847223 -8.6602812 -7.8673849 -7.2665043 -7.0902729 -6.6129637 -6.4061766 -6.406426 -6.5622249 -6.2823324 -5.9697151 -5.7608576 -5.1401796 -4.3315296]]...]
INFO - root - 2017-12-15 20:49:34.358732: step 58810, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.666 sec/batch; 50h:38m:02s remains)
INFO - root - 2017-12-15 20:49:40.792243: step 58820, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 48h:23m:04s remains)
INFO - root - 2017-12-15 20:49:47.230909: step 58830, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 48h:40m:59s remains)
INFO - root - 2017-12-15 20:49:53.654582: step 58840, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 47h:12m:35s remains)
INFO - root - 2017-12-15 20:50:00.120835: step 58850, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 48h:50m:45s remains)
INFO - root - 2017-12-15 20:50:06.512892: step 58860, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 47h:45m:34s remains)
INFO - root - 2017-12-15 20:50:12.939082: step 58870, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 48h:43m:15s remains)
INFO - root - 2017-12-15 20:50:19.370063: step 58880, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 50h:06m:48s remains)
INFO - root - 2017-12-15 20:50:25.867805: step 58890, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 49h:24m:01s remains)
INFO - root - 2017-12-15 20:50:32.287911: step 58900, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 48h:53m:42s remains)
2017-12-15 20:50:32.760117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2202754 -2.7090912 -3.1322236 -3.0747089 -2.8987284 -2.8028035 -2.7773976 -2.5343585 -2.3048458 -3.5128102 -4.1374922 -4.8406367 -5.3904095 -5.9086175 -7.0649438][-2.8434443 -3.141994 -3.5142827 -3.5818934 -3.5102148 -3.3089442 -2.963614 -2.4596748 -2.1954589 -3.6890488 -4.4656 -5.4546614 -6.4483895 -7.1637964 -8.0668764][-2.6651964 -2.5533633 -2.5365047 -2.4088984 -2.34192 -2.2600498 -1.9579568 -1.767561 -1.7119126 -2.7717781 -3.24013 -4.1469679 -5.117383 -5.9452267 -7.1587262][-1.3246579 -0.917994 -0.7159853 -0.52439833 -0.33215237 -0.033075809 0.21466637 0.33751965 0.47065639 -0.98089266 -1.8876309 -3.2468433 -4.4489098 -5.1094923 -6.1943388][-1.2892313 -0.62384748 0.074411392 0.19342279 0.25544214 0.7158823 1.0821571 1.1423311 1.2104874 -0.20627308 -1.130868 -2.34132 -3.6566668 -4.4360466 -5.7485461][-1.4775629 -0.749763 -0.10639048 0.42502975 0.73132896 1.2447662 1.7555094 1.7877178 1.7074976 0.043457985 -1.0458331 -2.2549982 -3.3873677 -4.1104221 -5.1260066][-1.9833779 -1.1047215 -0.5093956 0.34503555 1.1375322 1.6019173 2.0661402 2.2467508 2.2642155 0.56321239 -0.60095119 -1.952961 -3.2776361 -4.0457664 -5.1911745][-2.2898741 -1.3098564 -0.74030304 0.32019997 1.5106688 2.2848577 2.7827511 2.8879089 2.926693 1.0500307 -0.17312813 -1.6345987 -3.0110478 -4.014523 -5.1703196][-2.5624275 -1.6821699 -0.79966211 0.23351669 1.1251907 2.0525398 2.6406279 2.8858528 3.0601234 1.0869246 -0.34834146 -1.7845635 -3.1279254 -4.1881618 -5.4303608][-2.665781 -2.1528435 -1.4349141 -0.49474859 0.22946692 1.0361633 1.7433939 1.8682928 1.9081478 0.18265963 -0.94722843 -2.3198934 -3.7143219 -4.694849 -5.9350524][-3.9573734 -3.400249 -2.849649 -1.9692068 -1.2258086 -0.31926441 0.48482132 0.46494198 0.44823551 -1.3112793 -2.3900828 -3.3312898 -4.3865719 -5.2575722 -6.1880021][-5.3614674 -4.6650476 -4.206995 -3.664434 -3.0793495 -2.485568 -2.0265908 -1.8603992 -1.7094016 -2.9814148 -3.43647 -4.1827488 -5.0407705 -5.4632778 -6.168766][-6.3643737 -5.8727331 -5.4808121 -5.098197 -4.5347905 -4.182478 -3.872144 -3.8654668 -3.9515069 -4.5744529 -4.90696 -5.3929729 -5.8466582 -6.1063414 -6.3480687][-6.7398534 -6.1435676 -5.6505489 -5.305295 -4.9508066 -4.6674442 -4.6453848 -4.8306046 -4.8672113 -5.4368429 -5.6027422 -5.8718805 -6.1038504 -6.1067348 -6.3181548][-7.7430992 -7.239819 -6.9503808 -6.639359 -6.4687648 -6.2769976 -6.0036535 -6.2027922 -6.3224611 -6.455843 -6.5000858 -6.5495396 -6.5499449 -6.5366244 -6.566462]]...]
INFO - root - 2017-12-15 20:50:39.096818: step 58910, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 47h:42m:46s remains)
INFO - root - 2017-12-15 20:50:45.513643: step 58920, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 48h:05m:44s remains)
INFO - root - 2017-12-15 20:50:51.858878: step 58930, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 48h:21m:04s remains)
INFO - root - 2017-12-15 20:50:58.273705: step 58940, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 49h:33m:19s remains)
INFO - root - 2017-12-15 20:51:04.785164: step 58950, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 49h:19m:39s remains)
INFO - root - 2017-12-15 20:51:11.164266: step 58960, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 48h:20m:28s remains)
INFO - root - 2017-12-15 20:51:17.484812: step 58970, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.629 sec/batch; 47h:48m:29s remains)
INFO - root - 2017-12-15 20:51:23.962797: step 58980, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 49h:09m:59s remains)
INFO - root - 2017-12-15 20:51:30.433921: step 58990, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 51h:03m:12s remains)
INFO - root - 2017-12-15 20:51:36.820588: step 59000, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 48h:26m:35s remains)
2017-12-15 20:51:37.302928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0186553 -3.1844049 -3.5955606 -3.5906887 -3.9034865 -4.2016597 -4.2944641 -4.2333679 -4.2025719 -4.3533983 -5.9397388 -6.3117356 -6.9940119 -8.4859285 -9.0123739][-3.4399376 -3.4983945 -3.8850262 -4.4816151 -5.1927238 -5.4057975 -5.1126585 -5.0556717 -5.4111376 -5.5451446 -6.796648 -7.233943 -8.0341282 -9.3818007 -10.064659][-2.5055232 -2.7520409 -3.1227236 -3.4958196 -4.0054283 -4.3369088 -4.2793264 -4.3231649 -4.351089 -4.6773968 -6.088017 -6.7777381 -7.8455572 -9.1795835 -9.3950024][-1.4210362 -1.7438445 -2.0414081 -2.2284513 -2.6932974 -2.763247 -2.5166678 -2.6315684 -2.7611909 -3.2550054 -4.6742859 -5.5507979 -6.5043225 -7.892179 -8.46249][-1.471704 -1.1922865 -0.88622761 -1.3465543 -1.8101444 -1.4540329 -0.81636667 -0.62580442 -0.76014328 -1.5985875 -3.2086525 -4.2542496 -5.4749975 -7.0112104 -7.8294945][-2.7282324 -2.0706081 -1.2564206 -0.91961288 -0.70062017 0.11491489 0.97824955 1.4722557 1.4937983 0.58163166 -1.3382177 -2.7744384 -4.0758581 -5.9189615 -6.97318][-3.2226272 -2.2689624 -1.4575534 -0.95039749 0.17377758 1.375206 2.3301306 2.5944853 2.3411217 1.1892042 -0.94780588 -2.6342335 -4.0228987 -5.5774813 -6.6296229][-2.8577147 -1.9520936 -1.3022017 -0.75948238 0.29495525 1.8067112 2.8788176 3.2718935 2.9995232 1.5390968 -1.0109391 -2.8594513 -4.2945023 -6.2440548 -7.1943269][-2.6205106 -2.2765546 -1.9996533 -0.91787529 0.25486755 1.3182163 2.2771378 2.489625 2.384222 1.2085752 -1.5945096 -3.5576568 -4.8744574 -6.5287547 -7.2870903][-3.8306088 -3.505621 -3.3937588 -2.4700174 -1.3053751 -0.29226494 0.40843773 0.72321987 0.60849571 -1.0456109 -3.0577307 -4.5998764 -5.9435744 -7.3413854 -8.04111][-5.3403959 -5.2240844 -4.594368 -3.8635638 -3.1394773 -2.3820748 -1.614738 -1.8215494 -1.9397316 -2.777885 -4.6767707 -5.5962286 -6.0925655 -7.3092456 -8.1542768][-5.4458075 -5.3780127 -4.93459 -4.3480177 -4.0099659 -3.5799022 -3.0857925 -3.2873974 -3.2906747 -4.6275024 -5.8916254 -6.44125 -6.9550195 -7.3162065 -7.5277562][-6.2950211 -6.3168473 -5.8813915 -5.5361414 -5.1807528 -4.6507549 -4.0479832 -4.459723 -4.93241 -5.4268122 -6.4034214 -6.5812426 -6.8752375 -7.0820923 -6.9318895][-5.7691112 -5.7167368 -5.0847974 -4.8138952 -4.715723 -4.1795363 -3.5820341 -3.6135182 -3.7170665 -4.6028857 -5.3693314 -5.6544838 -5.7962265 -6.2604957 -6.0185404][-6.8067837 -7.0328851 -6.55493 -5.815959 -5.5912247 -5.5329847 -4.97713 -4.8116064 -4.7351818 -4.8720427 -5.2262487 -5.101223 -5.36946 -5.3664865 -5.0615973]]...]
INFO - root - 2017-12-15 20:51:43.684541: step 59010, loss = 0.27, batch loss = 0.15 (13.1 examples/sec; 0.610 sec/batch; 46h:19m:00s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 20:51:50.043456: step 59020, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 47h:43m:36s remains)
INFO - root - 2017-12-15 20:51:56.415734: step 59030, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:00m:15s remains)
INFO - root - 2017-12-15 20:52:02.871581: step 59040, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 48h:59m:25s remains)
INFO - root - 2017-12-15 20:52:09.337404: step 59050, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 49h:17m:48s remains)
INFO - root - 2017-12-15 20:52:15.694775: step 59060, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 49h:25m:35s remains)
INFO - root - 2017-12-15 20:52:22.122345: step 59070, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 49h:22m:07s remains)
INFO - root - 2017-12-15 20:52:28.563180: step 59080, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.671 sec/batch; 50h:56m:28s remains)
INFO - root - 2017-12-15 20:52:35.128153: step 59090, loss = 0.27, batch loss = 0.16 (11.6 examples/sec; 0.687 sec/batch; 52h:10m:05s remains)
INFO - root - 2017-12-15 20:52:41.537044: step 59100, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 47h:56m:50s remains)
2017-12-15 20:52:42.064085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3828449 -3.7831705 -4.7203026 -5.1877561 -5.2855148 -4.5432663 -3.6525259 -2.7877131 -2.0552955 -3.5126681 -4.4279361 -6.0844216 -7.4314079 -9.0259819 -9.7696552][-3.9111879 -4.6074238 -4.6529741 -5.2912264 -5.9649215 -6.0250621 -5.6091065 -4.7482877 -3.8481352 -4.42027 -5.1716895 -6.9123363 -7.9796352 -8.92974 -9.7538157][-4.3064551 -4.5727167 -4.6920376 -4.7520189 -4.4784508 -4.1962948 -4.107563 -3.8434205 -3.278336 -3.6952403 -4.6897507 -6.6506472 -7.5287876 -8.7143917 -9.8355293][-2.8871689 -3.1066856 -3.2265739 -3.3683739 -3.3051629 -2.8079462 -1.9448814 -1.4519558 -1.06987 -1.7407265 -3.0884604 -5.4821463 -6.5132928 -7.8428979 -8.7549429][-2.5118685 -2.4296193 -2.2291355 -1.7783909 -1.574141 -1.1276627 -0.41520596 -0.067631721 0.35135841 -0.84329987 -2.0984783 -4.2000952 -5.1839981 -6.6044712 -7.56956][-2.0713291 -1.8444872 -1.3272552 -0.50541306 -0.20445251 0.63570023 1.6384783 2.0345726 2.5918446 1.3377237 0.30154276 -1.9930716 -3.4491849 -4.8465104 -5.9380155][-2.3182673 -1.5685053 -1.0603547 0.27477598 0.92661476 2.1604328 3.173316 3.4157686 3.8160391 2.2204876 0.90765667 -1.5585642 -3.1003423 -4.7304163 -5.8804545][-3.2158761 -2.2562761 -1.4415302 0.17541504 1.3116922 2.439929 3.4561357 4.0707893 4.4455776 2.58181 0.73020458 -1.4758568 -2.9533048 -4.7739592 -5.5842748][-3.9063902 -2.8331485 -2.0151973 -0.567822 0.59535694 1.7369843 2.6455784 2.9265537 3.2176743 1.6452208 0.085154533 -1.8623028 -3.0916457 -4.6658778 -5.3516526][-4.7506118 -3.9843478 -3.5510821 -2.4615135 -0.97552061 0.014101505 0.80403996 1.2628908 1.3404703 -0.7113018 -2.5329785 -3.8760405 -4.6056919 -5.5820951 -5.8804822][-6.1513791 -5.28953 -4.382123 -3.4367647 -2.3115029 -1.402009 -0.86590242 -1.1218295 -1.1972914 -2.5493636 -4.0859194 -5.0231113 -5.5557551 -6.362257 -6.6548495][-7.4590111 -6.6550293 -5.9397116 -5.0440521 -4.1527677 -3.39883 -2.7802587 -2.8046632 -3.0895338 -3.8418276 -4.9049711 -5.6314421 -5.9395485 -6.178452 -6.218832][-6.9634142 -6.6524286 -6.6270738 -6.4636955 -5.9669685 -5.3741207 -4.7257195 -4.1441221 -3.7612202 -4.3510618 -5.7028456 -6.0392909 -5.8238397 -5.9230943 -5.7921314][-6.8204665 -6.4610534 -6.5991354 -6.5526266 -6.02889 -5.4386635 -4.98401 -4.7909417 -4.5909967 -4.7240152 -5.3463454 -5.3722672 -5.1620855 -5.3812351 -5.7812309][-7.3429813 -6.98971 -6.7257848 -7.0469022 -7.251152 -6.9359756 -6.43189 -6.1937227 -5.9514503 -5.6680574 -5.3595095 -5.3565331 -5.5327358 -5.6026893 -5.583427]]...]
INFO - root - 2017-12-15 20:52:48.522195: step 59110, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 48h:19m:17s remains)
INFO - root - 2017-12-15 20:52:54.981536: step 59120, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 50h:38m:22s remains)
INFO - root - 2017-12-15 20:53:01.393652: step 59130, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 48h:14m:15s remains)
INFO - root - 2017-12-15 20:53:07.760020: step 59140, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 48h:49m:07s remains)
INFO - root - 2017-12-15 20:53:14.081122: step 59150, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 47h:25m:14s remains)
INFO - root - 2017-12-15 20:53:20.500260: step 59160, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 49h:48m:07s remains)
INFO - root - 2017-12-15 20:53:26.924832: step 59170, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 48h:50m:17s remains)
INFO - root - 2017-12-15 20:53:33.282290: step 59180, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 48h:06m:31s remains)
INFO - root - 2017-12-15 20:53:39.595646: step 59190, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.624 sec/batch; 47h:22m:05s remains)
INFO - root - 2017-12-15 20:53:45.916615: step 59200, loss = 0.33, batch loss = 0.22 (12.9 examples/sec; 0.619 sec/batch; 47h:00m:52s remains)
2017-12-15 20:53:46.425069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4238358 -4.6012774 -5.571044 -6.049921 -6.1994348 -5.7050505 -5.4306784 -4.7679458 -3.9869323 -5.15483 -5.8971281 -6.9059615 -7.1183548 -8.1538849 -8.3739614][-2.0627565 -3.360446 -4.6551681 -5.5175943 -5.9657764 -5.4137039 -4.9703918 -4.2202606 -3.1894374 -3.9300294 -3.9351349 -4.9570122 -5.3781767 -6.7321243 -7.4485865][-2.1600704 -2.76155 -3.6150088 -4.757782 -5.5729809 -5.0085449 -4.4328861 -4.1069584 -3.5643582 -4.4228754 -4.7042894 -5.3948088 -5.6631808 -7.1810508 -7.7020345][-3.1715927 -3.0820265 -3.7024755 -4.1906114 -4.4565973 -3.5661807 -2.7787647 -2.7807159 -2.6981177 -4.0389032 -4.8890285 -5.6040907 -5.9886003 -7.2343869 -7.5847125][-3.0115438 -2.7121377 -2.8424659 -3.13033 -3.2349696 -2.5008802 -1.120163 -0.43967485 -0.0977993 -2.131422 -3.5167599 -4.6890345 -5.6032047 -7.3584633 -7.805079][-4.1584821 -3.3554182 -2.6455426 -1.7015171 -0.96815729 -0.184443 0.71124935 0.8855772 1.4880304 -0.0085477829 -1.1406488 -2.5973377 -3.9636075 -6.0593643 -7.0985632][-2.7461414 -1.9622402 -1.719285 -0.47013664 0.85448551 1.8608704 3.3203526 4.0105381 4.40705 2.1455011 0.86952877 -0.36849356 -1.5937204 -3.8873496 -5.1186328][-1.4151325 -0.93414211 -0.98994923 0.46319771 2.1340418 3.158576 4.5683241 5.4659958 6.0986366 4.4274693 3.460535 1.4029493 -0.49722147 -2.5729551 -3.9219754][-1.9361258 -1.2547903 -1.1937585 -0.076654911 0.8496809 1.686573 2.9286833 4.1376286 5.1134224 3.7969017 2.7017746 1.5125284 0.41247559 -1.9490147 -3.5734091][-4.6851478 -3.89014 -3.0800805 -2.0141649 -1.4730544 -0.63811827 0.65789413 1.5273123 2.2379875 1.0308466 0.26449966 -0.35344315 -1.0049815 -2.5794382 -3.3863177][-8.2653456 -6.8795815 -5.9218526 -4.9028096 -3.9872608 -2.9826951 -1.9788427 -1.954546 -1.7364812 -3.002048 -3.5395761 -4.1966033 -4.6702204 -5.6509566 -5.9540858][-11.180691 -9.3575306 -7.7789149 -6.7402887 -5.8478031 -5.0423942 -4.4069405 -4.1562567 -4.1868582 -6.0272174 -6.9110255 -6.8731842 -6.9295239 -7.5953217 -7.6120038][-11.833574 -10.281192 -8.836812 -7.8440652 -7.1130223 -6.6157389 -6.19971 -6.2231874 -6.6869321 -8.1085482 -8.6291685 -8.3348675 -7.8621264 -7.5958033 -6.9965682][-9.86955 -9.1530981 -8.3970766 -7.8967252 -7.6334081 -7.4326229 -7.2556925 -7.6837816 -7.9784079 -8.6859751 -8.7976437 -8.4445095 -8.2098017 -7.7997561 -6.9911308][-9.9633093 -9.0643673 -8.919673 -8.59906 -8.4221878 -8.1094732 -8.2828407 -8.4973879 -9.050683 -9.35815 -8.8608093 -8.0740213 -7.3581605 -6.9345474 -6.2001729]]...]
INFO - root - 2017-12-15 20:53:52.932546: step 59210, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 49h:25m:04s remains)
INFO - root - 2017-12-15 20:53:59.360393: step 59220, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 47h:26m:09s remains)
INFO - root - 2017-12-15 20:54:05.788190: step 59230, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 48h:46m:03s remains)
INFO - root - 2017-12-15 20:54:12.226813: step 59240, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 47h:52m:16s remains)
INFO - root - 2017-12-15 20:54:18.727337: step 59250, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 50h:36m:07s remains)
INFO - root - 2017-12-15 20:54:25.179463: step 59260, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 48h:18m:42s remains)
INFO - root - 2017-12-15 20:54:31.550036: step 59270, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:33m:53s remains)
INFO - root - 2017-12-15 20:54:37.964221: step 59280, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 48h:07m:39s remains)
INFO - root - 2017-12-15 20:54:44.388351: step 59290, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 49h:33m:35s remains)
INFO - root - 2017-12-15 20:54:50.850771: step 59300, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 48h:51m:54s remains)
2017-12-15 20:54:51.439876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6096673 -7.8667874 -10.200584 -11.411903 -11.822197 -10.106047 -8.202179 -6.7780976 -6.9249878 -8.0992956 -8.4131966 -10.093171 -7.9081688 -8.32875 -7.5949054][-5.7893181 -6.6854897 -9.6097546 -12.0504 -14.069997 -13.464916 -12.014834 -8.31674 -5.6847658 -6.6537552 -7.5331879 -10.501543 -9.258153 -9.8722916 -9.699173][-6.4229345 -7.4302244 -9.5742359 -10.675985 -11.580321 -11.267966 -11.010965 -10.625166 -8.8134956 -7.6554866 -6.5239739 -9.501379 -8.26133 -9.3009138 -9.5325041][-5.9068208 -6.5020161 -8.3689079 -8.8271694 -8.7571859 -7.5407419 -6.5858874 -7.1815586 -7.6334043 -9.5747051 -8.0253878 -10.008749 -8.1390591 -8.7741251 -9.44559][-5.5523543 -5.8607335 -7.3041749 -6.4381218 -4.8815203 -3.2984691 -2.2560477 -1.971519 -2.0582995 -6.92998 -8.8728409 -11.915819 -8.7669058 -9.5258446 -10.010593][-5.8158107 -6.0150523 -6.645401 -5.6486287 -4.0926809 -1.5199909 0.72998524 1.3736906 2.1901026 -2.1648464 -4.9553843 -10.313721 -9.9320831 -10.287687 -9.3957186][-5.8607922 -5.7326136 -5.5889049 -4.1618271 -2.075582 0.90626717 2.3055191 3.3629742 4.3800039 1.8583708 0.23165226 -5.9419093 -7.956882 -10.530234 -10.688242][-6.5885005 -5.9034963 -5.629427 -3.242105 -0.17930174 2.3854685 4.8644695 5.0546656 4.3339071 2.5389433 0.675601 -3.5490203 -4.6490374 -8.8351326 -11.440313][-7.0483246 -6.3112445 -5.9685974 -4.019105 -1.4788833 0.8597393 2.6902542 3.9564037 3.9946947 0.1727972 -2.0681434 -6.6481447 -6.8292704 -8.6031427 -9.3380156][-8.2795382 -6.8575015 -5.4540424 -4.3142476 -2.6159716 -1.1700521 0.44394302 2.1535759 1.993639 -1.6001892 -3.574173 -8.722909 -9.8483829 -10.973926 -10.646402][-8.1495161 -7.8890572 -6.4747343 -3.8454337 -2.4088216 -1.2081895 0.7383194 0.93676662 0.86586285 -2.4142313 -4.795403 -8.7138491 -9.92327 -11.360807 -11.920274][-6.828999 -6.6872973 -6.4432316 -4.7147021 -3.1899419 -0.75044775 -0.0055084229 -0.56744719 -0.7341671 -3.5081744 -4.8046393 -6.9210687 -8.1721907 -9.8084326 -10.262391][-7.8854632 -6.4535108 -5.4818268 -3.8292439 -3.3973351 -1.9390059 -0.23583364 -0.0097675323 -1.7055864 -4.0207548 -5.0362558 -5.4584618 -5.4165792 -6.56751 -7.5500479][-8.28366 -8.0709362 -6.4595337 -4.2223425 -4.3360443 -3.7970269 -1.9127483 -1.600698 -1.8775282 -3.4778686 -4.6671486 -5.20348 -4.5260324 -4.1873636 -4.9143162][-5.9236455 -7.1737747 -6.8414693 -6.520926 -4.7906561 -4.2736621 -4.4085708 -4.4913173 -4.4605303 -4.1535873 -4.2761726 -4.8109417 -5.1924143 -4.8340368 -4.3278809]]...]
INFO - root - 2017-12-15 20:54:57.973705: step 59310, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 48h:51m:10s remains)
INFO - root - 2017-12-15 20:55:04.371046: step 59320, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 49h:58m:14s remains)
INFO - root - 2017-12-15 20:55:10.817342: step 59330, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 50h:00m:09s remains)
INFO - root - 2017-12-15 20:55:17.256060: step 59340, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:38m:57s remains)
INFO - root - 2017-12-15 20:55:23.560698: step 59350, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 47h:43m:37s remains)
INFO - root - 2017-12-15 20:55:30.093168: step 59360, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 50h:30m:44s remains)
INFO - root - 2017-12-15 20:55:36.557964: step 59370, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 49h:20m:56s remains)
INFO - root - 2017-12-15 20:55:43.012577: step 59380, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 50h:16m:43s remains)
INFO - root - 2017-12-15 20:55:49.486623: step 59390, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.680 sec/batch; 51h:34m:40s remains)
INFO - root - 2017-12-15 20:55:55.966170: step 59400, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 49h:40m:31s remains)
2017-12-15 20:55:56.507442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7851152 -4.7893734 -5.1485796 -5.2141666 -5.1816449 -4.7448683 -4.1932535 -3.7565424 -3.2007561 -4.1947784 -4.8760557 -5.5268006 -6.3528214 -7.3007593 -8.1707335][-4.3771787 -4.7965746 -4.85475 -5.1676593 -5.449966 -5.1353731 -4.7468266 -4.3374605 -3.8699386 -5.128089 -5.62644 -6.363553 -7.205936 -7.838275 -8.1090765][-4.5889149 -4.3293567 -4.5657959 -4.0296674 -3.9013991 -3.8317022 -3.5706539 -3.421432 -3.4137974 -4.7353783 -5.1465693 -6.1361437 -6.8204923 -7.8641205 -8.1313925][-4.369009 -3.9361289 -3.5965481 -3.2044735 -2.8055577 -2.6495409 -2.0799093 -2.2132754 -2.3917713 -3.7607291 -4.6587181 -5.3541989 -5.9346747 -7.16318 -8.1294031][-3.5129452 -2.6498995 -2.6282587 -2.189188 -1.7050285 -1.2998142 -0.95683432 -0.92311382 -0.91096306 -2.5233006 -3.6773047 -4.4514074 -5.2682257 -6.4009008 -7.4755306][-3.7380996 -2.3511171 -1.6362534 -1.0374866 -0.62146759 0.089722633 0.73522472 0.56949425 0.23537064 -1.1712608 -2.0685086 -3.2516737 -4.4084477 -5.2884164 -6.233285][-3.6590953 -2.2823305 -1.5522771 -0.19867134 0.826601 1.6500654 2.1098547 2.0093546 1.9056511 0.4034605 -0.773129 -2.5127158 -3.5696549 -4.9145756 -5.6636181][-2.1424656 -1.5245872 -1.207037 0.44522095 1.2511234 2.1108036 3.0184278 3.2416868 3.0170794 1.5019007 0.48597431 -1.498239 -2.7116857 -4.0362606 -5.1312876][-2.2524004 -1.7770352 -0.85053968 0.18496513 0.95277119 1.6809015 2.2417202 2.8297749 3.1696711 0.88381481 -0.2631259 -1.6627011 -2.6788487 -3.735604 -4.7886143][-3.894418 -3.7690206 -2.6399989 -1.2835922 0.27605104 0.87022305 1.094532 1.4524221 1.7889404 -0.34840012 -1.4775391 -2.3163967 -3.5961547 -4.7892118 -5.1123533][-5.5426493 -5.1129665 -4.9518671 -3.61554 -2.5373955 -1.2388487 -0.608521 -0.46980667 -0.4287324 -1.5655341 -2.9978824 -3.8325744 -4.6381712 -5.8901381 -6.42492][-7.1542473 -6.7416024 -6.2284565 -5.1897945 -4.9506607 -4.1174068 -3.3643656 -3.0937366 -2.7018542 -3.5614452 -4.8235388 -5.0649891 -5.6443129 -6.5297556 -6.8859892][-8.6723347 -8.1685352 -7.3672361 -6.4201384 -5.6561184 -5.4060936 -4.9541407 -5.2459164 -5.4020929 -5.34717 -5.949543 -6.08735 -6.1845369 -6.622921 -6.7583871][-9.9727955 -8.641861 -7.6588721 -7.0133648 -6.1978431 -5.6490097 -5.5149384 -5.2602768 -5.5836372 -5.9725332 -6.5293632 -6.4292073 -6.7028322 -6.809144 -6.4673724][-9.6736784 -9.6498308 -9.9417324 -9.186801 -8.1401939 -7.5915823 -7.28112 -6.9097514 -6.7876387 -6.6800976 -6.6958365 -6.672781 -6.8326888 -6.8197575 -6.6635766]]...]
INFO - root - 2017-12-15 20:56:02.934944: step 59410, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 49h:17m:41s remains)
INFO - root - 2017-12-15 20:56:09.436108: step 59420, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 49h:01m:00s remains)
INFO - root - 2017-12-15 20:56:15.806420: step 59430, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 48h:37m:33s remains)
INFO - root - 2017-12-15 20:56:22.242587: step 59440, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 48h:33m:51s remains)
INFO - root - 2017-12-15 20:56:28.626590: step 59450, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 47h:40m:50s remains)
INFO - root - 2017-12-15 20:56:34.886875: step 59460, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 48h:03m:24s remains)
INFO - root - 2017-12-15 20:56:41.173318: step 59470, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 47h:07m:30s remains)
INFO - root - 2017-12-15 20:56:47.575846: step 59480, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 48h:14m:02s remains)
INFO - root - 2017-12-15 20:56:53.957578: step 59490, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 49h:46m:21s remains)
INFO - root - 2017-12-15 20:57:00.325294: step 59500, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 48h:14m:53s remains)
2017-12-15 20:57:00.840511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.874507 -4.0672045 -4.1644678 -4.5170436 -4.5348892 -3.92806 -3.1374812 -2.2355509 -1.5649705 -2.1030273 -2.4471388 -3.3443398 -3.8929205 -5.6896276 -5.8729291][-3.1122584 -3.2345719 -3.7115102 -3.7662961 -3.9808469 -4.08951 -3.9275475 -2.9243174 -1.5655279 -1.6835613 -2.2147827 -3.5557313 -3.9155133 -5.1118088 -6.2770472][-3.4559612 -3.0281281 -2.7793727 -3.0114326 -3.6762061 -3.7480488 -3.3919272 -3.1358476 -2.8758602 -3.3446836 -3.2762103 -4.0454092 -4.5237751 -5.4829087 -6.3562574][-2.9676976 -3.0892563 -2.9324198 -2.4483681 -2.264967 -2.6877241 -2.6647873 -2.1792474 -1.897459 -3.1814561 -4.2610269 -4.9819913 -4.9075937 -6.4854383 -7.1090217][-3.6724792 -2.6637478 -1.8268185 -1.372848 -1.4124255 -1.1693916 -0.99996281 -0.86095762 -0.64958858 -1.8190479 -2.8786879 -4.4755764 -5.4995756 -6.4103074 -6.6728673][-3.1743612 -2.2594337 -1.4930348 -0.64784479 0.17724037 1.2061424 1.4621134 0.925251 0.6108284 -0.68660927 -2.1524734 -3.7811036 -4.8091021 -6.043149 -6.6491423][-2.5139222 -1.473464 0.22363091 1.2582941 1.9226608 2.6659451 3.4632254 3.1589355 2.6626644 0.22132826 -1.7733397 -3.4144082 -4.198904 -5.566946 -6.3599381][-0.43227863 -0.40643644 -0.48358297 1.5194273 3.3680992 3.9155769 3.7143393 3.9601221 3.8758154 1.6697121 -0.72679663 -3.4043479 -4.80972 -5.8485746 -5.8599482][-0.95202351 -0.10605097 0.10160017 0.76113224 1.55828 2.282321 2.60948 3.2588673 3.3544722 1.3950033 -1.073813 -3.3965421 -4.5633941 -5.8773284 -6.33337][-2.7585807 -2.0319586 -0.84745026 -0.040318489 0.65959835 1.2380943 1.592454 1.3203001 0.79540062 -0.87206316 -2.20335 -4.1172633 -5.2111235 -6.0211639 -6.4375525][-3.8104191 -3.8380613 -3.6463218 -2.7990918 -2.4646716 -1.5956955 -0.9385848 -0.90332222 -1.1968083 -3.4407372 -4.9083233 -5.9289446 -6.4370079 -7.4461555 -7.1417274][-7.2136812 -5.9454203 -5.2581072 -5.0581679 -4.75257 -4.3888607 -4.3686266 -3.8752952 -3.7866039 -5.1108284 -6.2120161 -6.8469625 -7.1821685 -7.9127793 -7.7342992][-7.8400841 -8.1711073 -7.4090118 -6.6509356 -6.5042524 -6.1761732 -5.4509196 -5.6949224 -6.034637 -6.6554956 -7.03337 -7.1776094 -7.239471 -7.6936169 -7.780901][-8.2523174 -8.3475475 -8.3431664 -7.810492 -7.1804304 -7.0960693 -7.3782935 -6.9788146 -6.5294576 -6.8816504 -6.8143258 -7.4075947 -7.4323707 -6.987082 -6.4982648][-8.0735331 -7.4588413 -7.0360832 -7.3680139 -7.9082575 -7.3092728 -7.0318742 -7.2659421 -7.5695491 -7.3194723 -7.0625057 -6.8748736 -6.3517046 -6.3512034 -6.1089683]]...]
INFO - root - 2017-12-15 20:57:07.210748: step 59510, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.626 sec/batch; 47h:29m:57s remains)
INFO - root - 2017-12-15 20:57:13.600426: step 59520, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 48h:46m:52s remains)
INFO - root - 2017-12-15 20:57:19.985402: step 59530, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 47h:03m:12s remains)
INFO - root - 2017-12-15 20:57:26.384307: step 59540, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 47h:42m:34s remains)
INFO - root - 2017-12-15 20:57:32.806677: step 59550, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 47h:53m:43s remains)
INFO - root - 2017-12-15 20:57:39.180388: step 59560, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 49h:59m:27s remains)
INFO - root - 2017-12-15 20:57:45.532659: step 59570, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:31m:47s remains)
INFO - root - 2017-12-15 20:57:51.901702: step 59580, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:31m:29s remains)
INFO - root - 2017-12-15 20:57:58.274045: step 59590, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 47h:54m:57s remains)
INFO - root - 2017-12-15 20:58:04.645386: step 59600, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 47h:31m:30s remains)
2017-12-15 20:58:05.203994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0524874 -5.8930836 -5.7067804 -5.2764616 -4.942193 -4.6396065 -4.5279303 -3.7988684 -3.3208156 -4.3399162 -4.9411316 -5.946156 -6.45128 -7.0290022 -7.576962][-4.9103975 -5.2689452 -5.4569178 -5.6507306 -5.6515493 -5.0487347 -4.791337 -4.5067377 -4.273355 -5.087491 -5.4161086 -6.3148236 -6.5316682 -6.8020573 -7.3176703][-3.8731344 -4.3184752 -4.6050143 -4.7091227 -4.6934013 -4.4748039 -4.1601896 -3.9161327 -3.6824255 -5.0053539 -5.770649 -6.3344641 -6.42157 -6.6488967 -6.9358516][-2.1312666 -2.1482949 -1.9838357 -2.1732464 -2.3103056 -2.3691897 -2.1623135 -2.2397022 -2.3734484 -3.8261039 -4.8599653 -6.0238481 -6.5169592 -6.7281518 -7.1052542][-1.1224842 -1.0333705 -0.71737289 -0.60479641 -0.49483252 -0.27992487 -0.065087795 -0.32254887 -0.56440592 -2.6437893 -4.1552639 -5.6478853 -6.3099236 -6.8029079 -7.3813429][-1.7677684 -1.2264218 -0.59335613 0.0064764023 0.49723244 1.055294 1.5752039 1.3273296 1.1698093 -0.9892087 -2.4805284 -3.9839418 -5.220829 -6.0196643 -6.9228992][-2.5927305 -1.9005046 -0.82647038 0.57773209 1.5503178 2.3237066 2.96552 2.7106619 2.4002066 0.020517826 -1.6905313 -3.5013447 -5.0238752 -6.1584606 -7.2099466][-2.9378934 -1.9136086 -0.59297562 1.1809711 2.3663979 2.9661493 3.3427477 3.1617994 2.8174639 0.28964281 -1.6433821 -3.5582175 -5.0804729 -6.3230724 -7.3791265][-3.5594897 -2.5661922 -1.3017507 0.36337852 1.3773565 1.9319715 2.2741718 2.2354679 2.1340971 -0.084635735 -1.825222 -3.7093096 -5.2213707 -6.489007 -7.391212][-4.5847855 -3.6613283 -2.5577435 -1.2217879 -0.37447643 0.21189737 0.58746243 0.65258121 0.55552006 -1.6924677 -2.8288918 -4.234643 -5.3962135 -6.5193157 -7.4833136][-5.3202763 -4.7364187 -3.8076425 -2.5882049 -1.7693419 -1.3074021 -1.0268035 -1.1201773 -1.2173033 -3.4331884 -4.5772943 -5.6627369 -6.5111313 -7.16425 -7.727819][-5.0459476 -4.601202 -3.9493759 -3.3033419 -2.7098107 -2.307044 -2.2363367 -2.3892746 -2.5034227 -4.0706234 -4.9302244 -6.1726894 -6.8834658 -7.4262218 -7.751812][-4.8858871 -4.34 -3.8761177 -3.6667128 -3.4103084 -3.2250934 -3.3541574 -3.6006317 -3.7142754 -4.6761961 -5.2250748 -6.3179674 -6.8941779 -7.2450776 -7.258286][-4.0511217 -3.6466432 -3.147099 -2.9242859 -2.6811137 -2.7077603 -3.2370348 -3.7322476 -3.8732471 -4.6915922 -5.2147093 -5.7713947 -6.0899954 -6.3994665 -6.2955923][-3.8152862 -3.3755107 -2.9378138 -2.759954 -2.4998803 -2.8200803 -3.6383772 -4.2728052 -4.627408 -4.8174028 -5.0490818 -5.5561018 -5.7061396 -5.7133541 -5.5370388]]...]
INFO - root - 2017-12-15 20:58:11.636382: step 59610, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 49h:32m:33s remains)
INFO - root - 2017-12-15 20:58:18.014534: step 59620, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 47h:52m:31s remains)
INFO - root - 2017-12-15 20:58:24.439344: step 59630, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 50h:27m:19s remains)
INFO - root - 2017-12-15 20:58:30.744471: step 59640, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 48h:20m:34s remains)
INFO - root - 2017-12-15 20:58:37.032620: step 59650, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 48h:53m:31s remains)
INFO - root - 2017-12-15 20:58:43.460339: step 59660, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 49h:52m:04s remains)
INFO - root - 2017-12-15 20:58:49.893494: step 59670, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 48h:00m:45s remains)
INFO - root - 2017-12-15 20:58:56.297419: step 59680, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 47h:33m:19s remains)
INFO - root - 2017-12-15 20:59:02.698577: step 59690, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 49h:44m:05s remains)
INFO - root - 2017-12-15 20:59:09.074781: step 59700, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 48h:33m:10s remains)
2017-12-15 20:59:09.650769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.689116 -5.97549 -7.0076685 -7.9796247 -8.0687923 -7.254889 -6.0522146 -4.8277931 -3.6096749 -3.9581592 -4.3242841 -4.7485561 -5.3951654 -6.0507793 -6.9650216][-4.8502254 -5.5538349 -6.7395267 -7.8417473 -8.29213 -7.7640271 -6.997499 -5.7296052 -4.3798528 -5.333437 -6.0438471 -6.7943745 -7.7938619 -8.1919327 -8.7986584][-4.3782768 -4.9899187 -6.5667076 -7.0936227 -7.27544 -6.92289 -6.5467458 -6.2014694 -5.4702425 -5.994905 -6.8667812 -7.9560022 -8.5430746 -9.0999069 -9.7711353][-5.8569937 -5.5984163 -6.0396504 -6.2006922 -5.9128118 -5.1754045 -4.2237325 -3.4882998 -3.0047154 -3.8898289 -5.1142292 -6.8018432 -8.2978029 -8.9441509 -9.6420374][-6.6799231 -6.0475636 -5.88673 -5.4032111 -4.5462232 -2.9307489 -1.281589 -0.40554285 0.23991108 -1.6647382 -3.754493 -5.8102603 -7.6881819 -8.8069391 -9.7941723][-7.3505988 -6.6237893 -5.7559195 -3.9403193 -1.8432708 0.14458084 2.5898619 3.3662338 3.3419762 0.9335537 -1.5916386 -4.3547721 -6.6794176 -8.0424747 -9.6245689][-5.9519367 -5.1392136 -3.9891672 -1.6842942 0.92276859 3.3812838 6.036768 6.5555277 6.0805721 3.3698368 0.118011 -2.8295755 -5.4971142 -7.0284662 -8.3987322][-6.0204124 -4.9766521 -3.7563899 -0.91937113 2.4879503 4.7326269 6.9600153 7.3131752 7.3953838 4.0961266 0.65795422 -1.8067126 -4.8040066 -6.5229993 -7.5808196][-6.5037303 -5.7175732 -5.1293211 -2.9139457 -0.03377676 2.2646627 4.49255 5.007556 5.2999249 2.1095972 -0.93062687 -3.0160866 -4.9336243 -6.0401349 -7.5425858][-8.6198149 -7.8951764 -6.9845314 -4.8788939 -2.8463058 -0.96420336 0.86772633 0.90107727 1.2001104 -1.8076177 -4.2516165 -6.0327177 -7.2785444 -7.2160869 -7.7095952][-11.078222 -10.968065 -10.524248 -9.3706141 -7.9134817 -6.0758715 -4.31888 -4.5230513 -3.9581606 -6.3446174 -7.4248357 -8.1775627 -9.1278505 -9.0291052 -9.2872086][-12.656906 -12.185518 -11.527018 -10.8331 -9.8993359 -8.5052013 -7.1218877 -6.6738749 -5.8940053 -7.5599046 -7.9857726 -8.3671007 -8.8980293 -9.004241 -9.829565][-12.166117 -12.221047 -11.840899 -10.897266 -10.127165 -9.2226124 -8.0298414 -7.8923903 -7.855525 -9.0350084 -9.2910519 -9.070035 -8.7735443 -8.4050312 -8.4483051][-10.08255 -9.9891739 -10.136768 -9.5543413 -8.8421688 -8.1419592 -7.038949 -7.1199284 -7.1797662 -7.9990191 -8.1277447 -8.0073776 -7.9137888 -6.9615216 -6.9308004][-8.5284061 -8.8456888 -9.0632143 -9.175498 -9.1579866 -8.36579 -7.6180267 -7.3935385 -7.0707645 -7.2145004 -7.3685756 -7.4845114 -7.3984923 -7.1340642 -6.67477]]...]
INFO - root - 2017-12-15 20:59:15.998245: step 59710, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 48h:05m:18s remains)
INFO - root - 2017-12-15 20:59:22.345550: step 59720, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 49h:09m:04s remains)
INFO - root - 2017-12-15 20:59:28.746065: step 59730, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 49h:59m:05s remains)
INFO - root - 2017-12-15 20:59:35.158340: step 59740, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 48h:57m:11s remains)
INFO - root - 2017-12-15 20:59:41.532700: step 59750, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.630 sec/batch; 47h:43m:54s remains)
INFO - root - 2017-12-15 20:59:47.916612: step 59760, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 48h:59m:21s remains)
INFO - root - 2017-12-15 20:59:54.281921: step 59770, loss = 0.36, batch loss = 0.25 (12.3 examples/sec; 0.650 sec/batch; 49h:13m:09s remains)
INFO - root - 2017-12-15 21:00:00.679659: step 59780, loss = 0.36, batch loss = 0.24 (12.2 examples/sec; 0.654 sec/batch; 49h:34m:35s remains)
INFO - root - 2017-12-15 21:00:07.154759: step 59790, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 48h:11m:32s remains)
INFO - root - 2017-12-15 21:00:13.714785: step 59800, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 49h:04m:11s remains)
2017-12-15 21:00:14.227467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9165878 -3.4416566 -3.3372989 -2.9570613 -2.4495664 -2.2058396 -2.3198709 -2.5984855 -2.7365794 -3.8237584 -3.8328722 -5.4082227 -5.5114889 -5.8858719 -7.0870004][-3.9735255 -3.6794381 -4.1024861 -4.3386564 -4.3958778 -3.8443627 -3.4621043 -3.327991 -3.2917819 -4.8067579 -5.3154383 -6.6402264 -6.8465848 -7.2455206 -8.2306843][-4.4973803 -4.08871 -3.8023293 -3.4388781 -3.0768328 -2.5369849 -2.1772294 -1.6731052 -1.3639617 -2.7873497 -3.2554946 -5.0708547 -5.5687761 -6.1379848 -6.9095516][-4.1070266 -3.6283755 -3.3018618 -3.024878 -2.6869373 -2.2186494 -1.8941641 -1.4583015 -1.1169252 -2.4227977 -3.1507187 -4.5439892 -4.9493852 -5.125814 -5.80563][-5.1609612 -3.7402565 -2.6102061 -1.955296 -1.6511812 -1.110249 -0.6484952 -0.88340616 -1.1315069 -2.4694734 -2.9052863 -4.57165 -5.1636 -5.3694649 -6.156455][-5.2786741 -3.696063 -2.325819 -1.2052722 -0.87466478 0.1736331 0.72481728 0.34193993 0.00099229813 -1.2772818 -1.9668264 -3.5793715 -4.101265 -4.5595717 -5.1451669][-5.4036865 -4.1123018 -2.8381619 -1.1436348 0.27144909 1.494772 2.055933 1.8824482 1.4567747 -0.052707672 -0.92814064 -3.2506824 -4.6414771 -4.8808918 -5.6698275][-4.4267526 -3.3917689 -2.5451865 -0.74873447 0.75078297 2.3084993 3.4891768 3.4702845 3.1220684 1.0773401 -0.26479578 -2.8491511 -4.2245188 -4.8743572 -5.5363822][-3.0539331 -2.4675126 -1.883636 -0.77115822 0.21412468 1.1845331 2.0537043 2.6740561 3.1446457 1.3073349 -0.31993008 -3.2314396 -4.7361279 -4.8767757 -5.6708965][-2.325129 -2.0958562 -1.41358 -1.2325869 -0.64068317 -0.018085957 0.019376278 0.42690945 0.80900192 -0.580955 -1.7587585 -4.2201157 -5.1242638 -5.7650414 -6.3667126][-3.6512122 -3.2176614 -2.5248547 -1.8435669 -1.4077306 -1.195241 -1.3054247 -1.4938984 -1.4924092 -2.7923751 -3.8779991 -5.8765182 -6.3174119 -6.2419109 -6.8045111][-5.2111411 -5.1822066 -4.649754 -4.1207848 -3.3444524 -2.9741945 -2.9354248 -3.0958595 -3.2509847 -4.0128593 -4.6087503 -6.3571033 -6.6486807 -6.8929009 -7.0324135][-6.7384143 -6.1857667 -5.87031 -4.9477243 -4.4881477 -4.2819228 -4.13268 -4.2882156 -4.7317066 -5.5969424 -6.1908441 -6.9142094 -7.1016355 -7.0648937 -7.2109547][-7.3982868 -7.0970087 -6.8335247 -5.8294473 -4.8358707 -4.2430286 -3.9638634 -4.4670696 -4.9711771 -5.8620796 -6.4215984 -6.7131686 -6.6949387 -6.8392878 -6.6911902][-7.5650263 -7.9432092 -8.32062 -7.8277082 -7.0596647 -6.431376 -5.6423759 -5.6864052 -5.8924017 -6.3732023 -7.0898008 -7.1031404 -7.0450983 -6.5226574 -6.2296047]]...]
INFO - root - 2017-12-15 21:00:20.722428: step 59810, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 48h:38m:40s remains)
INFO - root - 2017-12-15 21:00:27.152919: step 59820, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 49h:53m:43s remains)
INFO - root - 2017-12-15 21:00:33.492562: step 59830, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 47h:59m:04s remains)
INFO - root - 2017-12-15 21:00:39.867589: step 59840, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 47h:47m:45s remains)
INFO - root - 2017-12-15 21:00:46.268184: step 59850, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 49h:56m:55s remains)
INFO - root - 2017-12-15 21:00:52.591838: step 59860, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 47h:09m:58s remains)
INFO - root - 2017-12-15 21:00:59.025626: step 59870, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 49h:08m:04s remains)
INFO - root - 2017-12-15 21:01:05.497058: step 59880, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 47h:25m:19s remains)
INFO - root - 2017-12-15 21:01:11.939935: step 59890, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 49h:51m:57s remains)
INFO - root - 2017-12-15 21:01:18.393246: step 59900, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 48h:50m:21s remains)
2017-12-15 21:01:18.903479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0168114 -5.7337842 -6.0270343 -5.94551 -6.1885171 -6.1506376 -5.5295868 -4.89258 -4.9987674 -5.7207031 -6.2954021 -7.7555342 -9.3524513 -11.047068 -12.143869][-5.4269657 -5.9291134 -6.56694 -6.0505953 -5.9009972 -6.1152267 -6.0668054 -5.0069513 -4.0964308 -4.9640718 -6.1046953 -7.9002047 -9.78234 -11.172574 -12.12886][-5.7111607 -5.2092361 -5.6745596 -5.56369 -5.0968189 -4.8811407 -4.4679184 -3.6681347 -2.9936805 -3.6357388 -4.0951786 -6.2041726 -8.5512495 -10.320025 -11.679038][-5.7301292 -4.7142386 -4.7474523 -4.2174582 -3.6466246 -3.4304852 -2.64922 -1.6172218 -0.67628241 -1.6806951 -2.9176044 -5.2492514 -6.8211036 -7.9041429 -9.0268373][-4.9803314 -3.4294586 -2.2457013 -2.2208886 -2.1291757 -1.1799989 0.049298286 0.66251469 1.1175442 0.18193007 -0.71530056 -2.8030486 -4.896842 -6.4094119 -7.2143726][-3.3961658 -2.5364752 -1.3215728 -0.35415649 0.37537193 1.123374 1.6554737 1.8655033 1.9928055 0.52339745 -0.70758009 -2.5271015 -3.800293 -4.700974 -5.4207926][-1.9756269 -1.3954973 -0.78787994 0.28342772 1.3777695 2.3214798 2.7908678 2.7305269 2.5942345 0.78444767 -0.84259653 -3.03017 -4.388772 -5.1210618 -5.0180588][-0.48977232 -0.21038008 0.1263032 1.3307629 2.5959387 3.379796 3.5883312 3.3286562 2.9555893 0.86212254 -1.0375648 -3.3365178 -4.7469339 -5.2142935 -5.0433331][-0.078142166 1.1686792 1.8654804 2.0990849 2.438509 3.0452986 3.1160231 2.9585266 2.5940933 0.64675331 -1.0749531 -3.2578845 -4.9596338 -5.6483097 -5.4058943][-0.33766317 0.4458189 0.78026295 0.74089336 1.0187578 1.7126427 2.1554565 1.6738291 0.84721375 -0.99655914 -2.3012705 -3.6446333 -4.9418054 -5.3840947 -5.2828388][-2.3167315 -2.0215011 -1.6199431 -1.6519051 -1.7386689 -1.3054867 -0.9428525 -1.1819372 -1.6327772 -3.0342813 -4.0410233 -4.818912 -5.4587936 -5.9043474 -5.9255295][-4.5781422 -4.2885456 -3.9541304 -3.7664831 -3.380847 -3.3728418 -3.330162 -3.1758103 -3.3440557 -4.1461439 -4.974432 -5.4052796 -5.7306218 -5.75706 -5.6322603][-5.1101966 -4.976759 -5.2214737 -5.7316012 -5.7110863 -5.7359891 -5.390902 -4.9637623 -4.9528303 -5.25539 -5.8746996 -6.0075903 -5.6944723 -5.3215637 -5.191689][-4.8319778 -4.9005909 -5.4419069 -5.7448349 -5.457613 -5.4166431 -5.3388262 -5.5203533 -5.6389341 -5.2555971 -4.98657 -4.7891111 -4.5815434 -4.5734529 -4.7955518][-5.067719 -4.461071 -4.6419444 -5.6414866 -5.9769564 -6.1943965 -6.2045717 -6.2370138 -6.2087011 -5.9597549 -5.7408948 -5.5620103 -5.1627707 -4.9652934 -5.0060835]]...]
INFO - root - 2017-12-15 21:01:25.374262: step 59910, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 48h:46m:14s remains)
INFO - root - 2017-12-15 21:01:31.846365: step 59920, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.674 sec/batch; 51h:00m:03s remains)
INFO - root - 2017-12-15 21:01:38.286171: step 59930, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 49h:52m:31s remains)
INFO - root - 2017-12-15 21:01:44.584222: step 59940, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 48h:42m:41s remains)
INFO - root - 2017-12-15 21:01:50.879217: step 59950, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 47h:51m:22s remains)
INFO - root - 2017-12-15 21:01:57.245328: step 59960, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 47h:17m:35s remains)
INFO - root - 2017-12-15 21:02:03.634540: step 59970, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 48h:39m:45s remains)
INFO - root - 2017-12-15 21:02:10.153326: step 59980, loss = 0.31, batch loss = 0.19 (11.8 examples/sec; 0.675 sec/batch; 51h:07m:23s remains)
INFO - root - 2017-12-15 21:02:16.660301: step 59990, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 48h:00m:13s remains)
INFO - root - 2017-12-15 21:02:23.033095: step 60000, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 47h:36m:30s remains)
2017-12-15 21:02:23.544089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9814997 -7.0825992 -7.4282455 -7.0908065 -7.1711035 -6.9906926 -6.6181116 -5.4110909 -4.4611373 -4.1979494 -3.9704385 -4.7424088 -5.2255177 -5.9870768 -6.6263304][-7.5524387 -7.0018783 -7.8840661 -8.0374584 -8.0611506 -7.6037626 -6.9686556 -6.0398989 -5.4581022 -4.7447557 -4.4401426 -4.8250475 -5.5451455 -6.6923313 -7.6236873][-6.5339727 -6.3724451 -6.6165318 -6.4042959 -5.9496231 -5.294899 -5.1719489 -4.9807549 -4.0614 -3.5459394 -3.0284834 -4.1301584 -5.3080778 -6.4024434 -7.0954041][-5.8385 -5.439692 -4.8737936 -4.5444064 -3.9759383 -3.3640342 -2.3915796 -2.2693162 -1.704742 -2.2886934 -2.5243745 -3.9123621 -4.7193317 -5.3653765 -6.0654869][-3.9257908 -3.861352 -2.9313583 -2.1618195 -1.5250535 -0.5795846 0.33136368 0.1177969 0.40454865 -0.83596039 -1.4261594 -3.2247396 -4.494997 -5.3871679 -6.2097073][-3.1457343 -2.4681845 -1.3246598 -0.42942333 0.36058998 1.0166197 1.6811514 1.4530258 1.3865147 -0.023488998 -1.089107 -3.0525684 -4.5673456 -5.7572956 -6.5833511][-2.6905918 -1.9311581 -0.5009284 0.042082787 0.66826725 1.6708708 2.2712183 2.0298252 1.8666048 0.34903145 -1.1509647 -3.377389 -5.3670683 -6.7906985 -7.3827395][-2.0318146 -1.7956386 -1.1685772 0.05393362 0.95140743 1.5779209 2.1779518 2.3727942 2.3168325 0.81441879 -1.0623484 -3.5621848 -5.727809 -7.070941 -7.475656][-3.5254993 -2.6966925 -1.468492 -0.57276249 0.22623539 0.95728779 1.2715921 1.6814241 1.822238 0.59683323 -0.95786095 -3.6093564 -5.5248146 -6.8349509 -7.324019][-4.1699624 -3.1146965 -2.218565 -1.6828284 -1.2953644 -0.603611 -0.16931534 0.36058903 0.55169296 -0.69844007 -2.2868867 -4.42665 -6.074522 -7.3623319 -8.0497084][-5.7110729 -5.0698471 -4.1901822 -3.7247641 -3.2320981 -2.8887572 -2.5078564 -2.5265083 -2.6449542 -3.3459353 -4.0944805 -5.50136 -6.3145156 -6.8445234 -7.042017][-5.6631851 -4.9057865 -4.3128633 -4.0041304 -3.5048151 -3.1175113 -3.0536132 -2.9440751 -2.6661916 -3.6930103 -4.5308228 -4.7017078 -5.2914963 -5.861681 -6.173954][-6.7130308 -6.2122808 -5.237401 -4.7419195 -4.193347 -3.9005582 -3.9202187 -4.1246395 -4.3051987 -4.3468065 -4.8765497 -5.1425738 -5.4172163 -5.1267462 -5.1432467][-6.3317065 -6.0558009 -5.4047909 -5.0219989 -4.419127 -4.451519 -4.6814308 -4.9361887 -5.22967 -5.3092546 -5.7119384 -4.8781013 -4.6051521 -4.2803068 -4.2863646][-7.7328939 -7.1776838 -6.5531454 -6.76081 -6.2641511 -5.8213387 -5.9558263 -6.3885555 -6.6419997 -5.7485404 -4.963695 -4.4557266 -4.17321 -3.6968491 -3.6267962]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:02:31.172457: step 60010, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 47h:59m:27s remains)
INFO - root - 2017-12-15 21:02:37.687878: step 60020, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 50h:29m:33s remains)
INFO - root - 2017-12-15 21:02:44.188050: step 60030, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 48h:53m:04s remains)
INFO - root - 2017-12-15 21:02:50.698355: step 60040, loss = 0.36, batch loss = 0.24 (12.5 examples/sec; 0.642 sec/batch; 48h:37m:20s remains)
INFO - root - 2017-12-15 21:02:57.309302: step 60050, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 50h:25m:04s remains)
INFO - root - 2017-12-15 21:03:03.778521: step 60060, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 49h:26m:51s remains)
INFO - root - 2017-12-15 21:03:10.271495: step 60070, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 47h:33m:04s remains)
INFO - root - 2017-12-15 21:03:16.860593: step 60080, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.668 sec/batch; 50h:31m:19s remains)
INFO - root - 2017-12-15 21:03:23.398621: step 60090, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 48h:46m:42s remains)
INFO - root - 2017-12-15 21:03:29.923060: step 60100, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 48h:58m:07s remains)
2017-12-15 21:03:30.483481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21073 -4.4884357 -5.1645279 -4.5196905 -3.4310575 -2.2685781 -1.6116056 -1.6750126 -1.7527032 -3.013834 -3.3240747 -5.1352935 -7.04418 -9.3311625 -9.85301][-4.7502146 -5.3283873 -5.8920603 -6.4310741 -5.8484144 -4.3398685 -2.0034885 -1.1115847 -1.4521847 -4.0140371 -5.37667 -7.2834816 -9.1494713 -10.956207 -10.912821][-5.0202971 -5.4471693 -5.7328711 -5.0545444 -3.8672094 -3.0720243 -1.9231219 -0.87589455 -0.50933075 -2.7408853 -4.5158157 -6.5190182 -8.609396 -10.36535 -10.647856][-4.6736717 -5.2842 -5.726675 -4.7264433 -3.1140232 -0.94752026 0.43404102 0.6031456 0.60011482 -1.4298658 -3.0087423 -5.8723049 -8.1275005 -10.246591 -10.462938][-4.3395271 -3.8239582 -3.9542587 -3.8435423 -2.1517205 0.48979855 2.3368454 2.9069719 2.488883 -0.40956736 -2.1533494 -4.3071866 -6.7865534 -8.8036766 -9.2096252][-4.2133303 -3.5799837 -2.554461 -1.3347473 -0.043859482 2.000246 3.9397049 4.404314 3.6283894 0.82017803 -1.1995387 -3.4325318 -5.3005095 -6.7945633 -7.3355637][-4.6068449 -4.0448713 -2.6239824 -0.48531818 1.3367643 2.8194561 3.9642467 4.2282658 3.95471 1.290782 -0.93360329 -3.7406843 -6.0295887 -7.6784863 -7.8065114][-3.7178745 -3.1891994 -2.2453966 0.16257143 2.3632555 4.3124228 5.8409595 5.498477 4.3921652 1.2891188 -0.78321218 -3.7694998 -6.42557 -8.28892 -8.0324144][-4.29354 -3.6505108 -2.3704634 -0.34849119 1.4829102 3.2509995 4.9853773 5.2830982 4.3294516 0.71334267 -1.469872 -4.1250134 -6.540761 -8.6542044 -8.7620192][-4.0022249 -4.1465273 -4.1138473 -2.267745 -0.3079977 1.2684078 2.5148478 2.6938334 2.2770815 -1.0898185 -3.3594422 -5.9525056 -7.5760674 -9.17258 -8.8754826][-5.3479691 -5.2711091 -5.2161694 -4.3334217 -2.7158613 -0.66190767 0.014103889 -0.04226923 -0.36003351 -3.0651474 -4.9061489 -7.3110533 -8.804987 -9.8484125 -9.0310192][-6.8091307 -6.5310616 -6.5381656 -6.0976877 -5.3669496 -3.8933384 -2.9478331 -2.9003272 -3.1446152 -4.3664136 -5.5198421 -7.200274 -8.3833408 -9.94275 -9.6351357][-8.2990675 -8.0435228 -7.9602013 -7.7140818 -7.1387 -6.1601653 -6.035594 -6.2069254 -5.9439187 -6.8094916 -7.4906778 -8.3110046 -8.3297091 -8.86111 -8.3517828][-9.3989305 -9.1414843 -8.7898827 -8.4349165 -7.6257963 -6.5518589 -6.1530552 -6.8207326 -7.4198623 -7.7896709 -7.9526443 -8.0560122 -7.8102727 -7.7709074 -7.0383005][-8.6556816 -8.7626781 -9.0880022 -9.382823 -9.52067 -8.2151947 -7.6722131 -7.8335361 -7.8055363 -7.9547133 -8.1651144 -8.0915546 -7.5794716 -7.3379145 -6.8418059]]...]
INFO - root - 2017-12-15 21:03:36.971967: step 60110, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 49h:26m:05s remains)
INFO - root - 2017-12-15 21:03:43.485598: step 60120, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 48h:20m:40s remains)
INFO - root - 2017-12-15 21:03:49.901404: step 60130, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 46h:42m:12s remains)
INFO - root - 2017-12-15 21:03:56.430486: step 60140, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 48h:52m:13s remains)
INFO - root - 2017-12-15 21:04:02.887186: step 60150, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 49h:00m:26s remains)
INFO - root - 2017-12-15 21:04:09.289874: step 60160, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.654 sec/batch; 49h:27m:47s remains)
INFO - root - 2017-12-15 21:04:15.754487: step 60170, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 49h:50m:30s remains)
INFO - root - 2017-12-15 21:04:22.167575: step 60180, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 49h:41m:56s remains)
INFO - root - 2017-12-15 21:04:28.579980: step 60190, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 47h:45m:40s remains)
INFO - root - 2017-12-15 21:04:35.059416: step 60200, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 48h:44m:30s remains)
2017-12-15 21:04:35.624556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3558636 -5.6462069 -5.6833658 -5.771862 -5.5029516 -4.351141 -3.3069224 -2.7162628 -2.1420851 -3.049336 -2.9436994 -3.7810836 -4.7829556 -5.152173 -5.4828835][-1.2957034 -2.9658995 -3.9684463 -5.3060389 -6.039341 -5.2354193 -4.833899 -4.15265 -3.2469435 -4.0334511 -4.028779 -4.9337196 -5.6261144 -5.90341 -6.1394863][-0.12674809 -1.6833448 -3.1569147 -4.2648878 -5.8328066 -6.2467246 -6.360486 -5.6000919 -4.8696837 -5.5283976 -4.6506033 -5.4119182 -6.5459948 -7.1111159 -7.2211571][-0.85124016 -1.4197059 -3.2442703 -4.1575451 -4.6687984 -4.87953 -4.9401264 -4.6697297 -4.4609809 -5.2430396 -4.7736759 -5.4308176 -6.3160839 -7.2576146 -7.7604995][-1.7200928 -2.0255594 -3.128448 -3.1569037 -2.9233661 -1.9503493 -0.72841215 -0.48634148 -0.897614 -2.6201181 -3.1073346 -4.5342207 -5.8058109 -6.589694 -7.7492919][-1.5771456 -0.66509819 -1.3866925 -1.7296133 -0.56599569 0.76487064 1.8061361 2.2978964 2.5708752 0.66063118 -0.35658979 -2.6431279 -5.28792 -6.519969 -7.5439453][-0.62043858 -0.042594433 0.20375681 0.61950016 1.2333708 2.6077414 3.9528923 4.4581909 4.4335365 2.4846735 1.3074255 -0.95509481 -3.4400711 -5.6696138 -7.3686175][-0.34801292 0.38899708 1.146019 1.9706783 2.6161299 3.6995974 5.1046333 5.7698488 5.5753021 2.8115959 1.6451206 -0.44455147 -2.8683071 -4.6364069 -6.18327][-0.73357296 -0.18242931 0.31903076 1.6828527 2.2774057 3.0157709 4.0413704 4.90784 5.1480503 2.4779396 1.4477196 -0.91028881 -2.855422 -4.0524406 -5.6071949][-1.9800391 -1.2384052 -0.92413139 -0.04360199 0.69751072 1.440136 2.3496208 2.5763149 2.7846632 0.95908928 0.47947788 -2.0200849 -4.2323313 -5.0910778 -6.0642624][-3.6894875 -3.0442238 -2.8666825 -2.3187203 -2.2373567 -1.9439402 -1.3308887 -1.1939421 -1.4502912 -3.2479453 -3.1960206 -4.6878729 -6.6468577 -7.0677419 -7.3154845][-5.6416907 -4.9272957 -4.7485147 -4.7151184 -4.6586728 -4.5757732 -4.6456475 -5.0199361 -4.8833442 -6.342062 -6.3983917 -6.7108397 -7.889442 -8.2961674 -8.7445993][-8.2067 -7.4700189 -7.5165896 -7.2162743 -6.5549664 -6.5971093 -6.2840586 -6.6739135 -7.2703319 -8.1607609 -8.2752609 -8.6434336 -8.9551964 -8.5529995 -8.3777733][-8.4031982 -8.3540554 -8.6645985 -8.5478973 -7.8867621 -7.4294357 -6.9320707 -6.8498726 -6.9853892 -7.4988236 -7.7491322 -7.8418145 -8.3475027 -7.7673616 -6.9741645][-6.2451291 -6.5043788 -7.1747713 -7.4598808 -7.2356324 -6.9203806 -6.6844473 -6.3675537 -6.3084345 -6.0074658 -5.88827 -5.7818007 -5.77879 -6.0646925 -5.9936104]]...]
INFO - root - 2017-12-15 21:04:42.020140: step 60210, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:28m:23s remains)
INFO - root - 2017-12-15 21:04:48.420244: step 60220, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 48h:30m:44s remains)
INFO - root - 2017-12-15 21:04:54.907347: step 60230, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 48h:06m:42s remains)
INFO - root - 2017-12-15 21:05:01.317821: step 60240, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 49h:20m:07s remains)
INFO - root - 2017-12-15 21:05:07.798503: step 60250, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 48h:59m:20s remains)
INFO - root - 2017-12-15 21:05:14.232962: step 60260, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 49h:12m:37s remains)
INFO - root - 2017-12-15 21:05:20.690315: step 60270, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:59m:39s remains)
INFO - root - 2017-12-15 21:05:27.084097: step 60280, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 47h:01m:10s remains)
INFO - root - 2017-12-15 21:05:33.532104: step 60290, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 50h:11m:08s remains)
INFO - root - 2017-12-15 21:05:39.979017: step 60300, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 48h:41m:10s remains)
2017-12-15 21:05:40.543293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7638903 -6.2685738 -6.5183716 -5.8611827 -4.9973459 -3.8971035 -3.2597785 -3.3008342 -3.4010777 -4.5728703 -5.66963 -5.9891434 -6.7030468 -7.4561892 -7.5467892][-6.3561654 -6.6395907 -5.9351163 -5.8386045 -5.8923993 -4.9693842 -4.3815832 -4.1566148 -3.8216212 -4.8713207 -5.8162441 -6.3999166 -7.106575 -7.5250835 -7.5086918][-5.9885955 -6.4014854 -6.3671651 -6.2643857 -5.8571835 -5.1159792 -4.5807862 -4.6100483 -4.4252543 -5.2057037 -6.1517086 -6.5751009 -7.1145267 -7.643477 -7.9041224][-6.5571022 -6.5094638 -5.9674435 -5.5798097 -5.118063 -4.2221723 -3.7788236 -4.08745 -4.0244951 -4.7708635 -5.4243155 -5.942266 -6.4093843 -7.054534 -7.2231059][-7.0499306 -6.8590183 -5.879209 -4.2905321 -2.9760327 -1.873704 -1.5599675 -1.7902169 -2.0136356 -3.0773888 -4.4668536 -5.0678263 -5.7565064 -6.8790021 -6.863049][-6.3845596 -6.5115428 -4.9912872 -2.9325657 -1.029798 -0.020605564 0.36353493 -0.21950245 -0.80160522 -2.0889959 -3.572536 -4.5679321 -5.4638824 -6.6466317 -6.9248781][-5.8660574 -5.1441841 -3.4185939 -1.8105164 -0.22895575 1.0202179 1.854517 1.1142378 0.37572289 -1.2162957 -2.7650819 -3.3644366 -4.1704092 -5.4542503 -5.298378][-3.6918776 -3.0565352 -3.0967803 -1.4142509 0.95282555 1.4444838 1.5252495 1.9133911 1.8288088 0.3578577 -1.267364 -2.5009232 -3.5075006 -4.301281 -4.3764238][-3.5046897 -2.901063 -2.2712426 -1.1612549 -1.0168381 0.54795074 1.9566727 1.44948 0.83760262 -0.283041 -1.4603848 -1.7633386 -2.7693105 -4.4365306 -4.6922193][-4.9190264 -3.8095624 -3.2181897 -2.6065559 -1.756413 -1.3448553 -0.86530018 -0.069385529 0.1345005 -1.5342083 -2.7271528 -3.2297721 -4.0480461 -4.2906275 -4.7332559][-6.269053 -5.5825186 -5.4146843 -4.2549043 -3.4588003 -2.7936778 -2.1445599 -2.578043 -2.7222872 -3.4328475 -4.1838489 -4.1517811 -4.62669 -4.941308 -5.415863][-7.7630186 -6.368928 -5.0382147 -5.099504 -4.7005205 -4.1630173 -3.6935713 -3.5619454 -3.6265206 -4.512598 -5.3087988 -5.2447906 -5.741981 -5.7212057 -6.2659254][-9.0520086 -8.0177746 -7.5048852 -6.8759775 -6.3297119 -5.8467984 -5.5989718 -5.3528452 -5.2318735 -5.7918839 -5.8058014 -5.7896566 -6.0906649 -5.6536546 -5.5130363][-9.0189323 -7.8678808 -7.6954293 -7.2779918 -6.8347983 -6.3651056 -5.9986796 -6.1172638 -6.3060822 -6.4052286 -6.0386715 -6.1845131 -6.2893677 -5.56732 -5.5570865][-7.4131927 -6.9686007 -7.09984 -6.6909904 -6.64111 -6.6034827 -6.6647949 -6.6446018 -6.4007654 -6.7159538 -6.7706709 -6.2417316 -5.9497957 -6.1172972 -5.5934906]]...]
INFO - root - 2017-12-15 21:05:46.942553: step 60310, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 47h:53m:33s remains)
INFO - root - 2017-12-15 21:05:53.445132: step 60320, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 48h:18m:59s remains)
INFO - root - 2017-12-15 21:05:59.884207: step 60330, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 50h:17m:17s remains)
INFO - root - 2017-12-15 21:06:06.323489: step 60340, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 49h:55m:37s remains)
INFO - root - 2017-12-15 21:06:12.794206: step 60350, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 48h:14m:13s remains)
INFO - root - 2017-12-15 21:06:19.279740: step 60360, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 50h:56m:19s remains)
INFO - root - 2017-12-15 21:06:25.742345: step 60370, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 49h:02m:13s remains)
INFO - root - 2017-12-15 21:06:32.197049: step 60380, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:25m:25s remains)
INFO - root - 2017-12-15 21:06:38.710614: step 60390, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 49h:11m:05s remains)
INFO - root - 2017-12-15 21:06:45.189990: step 60400, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 46h:48m:54s remains)
2017-12-15 21:06:45.742212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1736326 -4.8307643 -5.7297454 -5.1959553 -4.2895594 -4.1272969 -3.2501101 -2.5372119 -2.8891411 -4.1246262 -5.64139 -7.0048475 -7.9796524 -7.7218146 -7.7121916][-3.2649121 -4.4946413 -6.0259395 -6.8960485 -6.3781872 -4.8265753 -3.4075561 -3.0684481 -3.399291 -4.5799742 -6.0879049 -7.3528929 -8.7742729 -9.173358 -8.9725151][-3.6662097 -4.6850166 -5.9348803 -5.7297711 -5.596487 -4.9751225 -3.9037166 -3.4996881 -3.1342897 -4.4755316 -6.122242 -7.3955836 -8.3430271 -8.8820515 -9.1534891][-3.6804914 -3.9145491 -4.7825174 -3.9078586 -3.0740275 -2.791779 -2.2631798 -2.4172544 -2.2172375 -3.8090329 -5.1870012 -6.5851212 -7.8743753 -8.0814123 -8.1489677][-3.6810026 -2.4009767 -2.3281484 -1.7674193 -0.74745035 -0.1816678 0.1390996 -0.40988779 -0.75727272 -2.0712862 -3.7254181 -5.5846014 -7.2127628 -7.4499435 -7.5765281][-3.3478341 -1.7409854 -0.5775218 0.14109278 0.47989559 1.57827 2.0946846 0.92411232 0.11816835 -1.1843681 -2.542747 -4.3310347 -5.9142451 -6.2988906 -6.6784706][-3.5154219 -1.9772081 -0.84121466 0.57877254 1.7890654 2.9660616 3.7798042 2.4238062 1.011795 -0.64027262 -1.9082227 -3.7760279 -5.22906 -5.7076197 -6.3003225][-4.0579419 -2.6814342 -1.6868563 0.036902905 1.6402645 2.8909492 4.1849155 3.3685236 2.1191044 -0.390913 -2.4165301 -4.1986842 -5.5040812 -5.9670916 -6.4391336][-4.7643547 -3.9711533 -3.3289413 -1.7010913 -0.28954458 0.69011593 1.8349209 2.2051239 2.2535267 -0.0947752 -2.6443071 -5.1334124 -6.3475485 -6.405057 -6.6780548][-5.6731176 -5.0806537 -4.905057 -3.6498389 -2.3695369 -1.3933916 -0.3512702 0.255682 0.77146053 -1.1677384 -3.1543698 -5.6518974 -7.1909618 -7.4578171 -7.4458447][-7.7196336 -7.1949215 -6.9210114 -6.0534787 -5.1466193 -4.2899733 -3.2500811 -3.2166834 -3.3154945 -4.6759682 -5.9058375 -7.1737728 -7.9278708 -8.3470259 -8.5750494][-8.6307545 -8.0015974 -7.7315021 -7.116271 -6.603363 -5.9026685 -5.0969276 -5.2112913 -5.4722919 -6.7805638 -7.194787 -7.5559578 -7.90215 -8.5069542 -8.712286][-9.9796457 -9.2474937 -9.0067577 -8.4906464 -7.8376317 -7.2259984 -6.6685076 -6.88306 -6.767169 -7.4138222 -7.8277369 -7.6563683 -7.3849325 -7.5966778 -7.3967557][-9.8024569 -9.2259474 -8.6359539 -8.1811161 -7.7559438 -6.9082909 -5.9620256 -6.225594 -6.5576544 -7.0741749 -7.1566796 -7.0675721 -6.6261215 -6.3850632 -5.989923][-8.8259487 -8.77999 -9.0642366 -9.0353842 -8.6627321 -8.0954657 -7.2113743 -6.8489571 -7.0306273 -7.4427018 -7.4040494 -6.671504 -6.0936842 -5.9591928 -5.4829493]]...]
INFO - root - 2017-12-15 21:06:52.221723: step 60410, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 49h:21m:08s remains)
INFO - root - 2017-12-15 21:06:58.661521: step 60420, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 48h:13m:09s remains)
INFO - root - 2017-12-15 21:07:05.186331: step 60430, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 48h:03m:20s remains)
INFO - root - 2017-12-15 21:07:11.657878: step 60440, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 48h:54m:13s remains)
INFO - root - 2017-12-15 21:07:18.092647: step 60450, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 48h:26m:59s remains)
INFO - root - 2017-12-15 21:07:24.495306: step 60460, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 47h:11m:43s remains)
INFO - root - 2017-12-15 21:07:30.931761: step 60470, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 48h:11m:43s remains)
INFO - root - 2017-12-15 21:07:37.422834: step 60480, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 50h:14m:17s remains)
INFO - root - 2017-12-15 21:07:43.893439: step 60490, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 47h:20m:56s remains)
INFO - root - 2017-12-15 21:07:50.301761: step 60500, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 48h:24m:43s remains)
2017-12-15 21:07:50.825402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0770354 -6.9073033 -6.5295768 -6.1046124 -5.1259708 -3.8654225 -2.6462622 -1.88941 -0.6970253 -2.2568135 -2.9356093 -3.9924345 -5.1649685 -5.7738686 -7.2703815][-6.1021166 -5.8173304 -5.7873664 -5.0651331 -3.8924482 -2.8180676 -1.6956739 -0.74116468 0.13121986 -0.96484375 -2.1173573 -3.3082151 -4.3062658 -4.7954235 -6.4089823][-6.3178859 -5.5331554 -4.8178339 -3.8030794 -2.7398953 -1.8779511 -0.89796591 -0.44514132 0.32329369 -1.3044677 -2.3252721 -3.2289228 -4.7350645 -4.5692959 -6.1736422][-8.0528584 -6.5448513 -5.4870582 -3.6338854 -1.7343116 -0.5721736 0.69562149 1.0296869 0.85800266 -1.2101784 -2.096384 -3.8005717 -5.4369612 -5.6303349 -6.262073][-9.5783758 -7.0479379 -4.7750511 -3.1165104 -0.97142887 0.82524014 1.7763062 1.8940401 1.8512373 -0.58503675 -1.9722042 -3.5307474 -5.5199895 -6.1423879 -7.2579837][-9.8303108 -7.2283587 -5.0413685 -2.6361437 -0.25812864 1.2674189 2.6900406 2.7654171 2.1808596 -0.1867795 -1.6191139 -3.3906641 -5.5976191 -6.6570911 -7.9349294][-9.121562 -6.4888744 -3.4583535 -1.1333632 0.80317688 2.0928411 3.2648134 3.514142 3.3747463 0.85114574 -0.86923361 -2.5944047 -4.4419532 -5.5466766 -7.0481977][-8.484726 -5.9470992 -3.4960351 -0.80273342 1.3018827 2.294857 3.5436974 3.5931473 3.4346371 1.1754684 -0.56240034 -2.486589 -4.064168 -5.4690571 -7.053093][-8.0027561 -6.4125071 -4.0839615 -1.4902463 0.24370527 0.901577 1.0046473 1.7962646 2.4256649 0.35844898 -1.1471486 -2.9554472 -4.8873281 -5.8280954 -7.1331944][-8.0904951 -6.4370952 -5.1983271 -2.8290968 -0.603858 0.30864382 1.0613546 0.88894081 1.0343056 -0.65846729 -2.1267238 -3.5777855 -5.3103623 -6.0117307 -7.0289845][-8.8296061 -8.3419933 -7.2247624 -5.2172036 -3.2251749 -1.6940756 -1.3234344 -1.2301259 -0.88404274 -2.4843521 -3.2620425 -4.5500755 -5.4825029 -6.0675354 -6.7004437][-10.875244 -9.7815075 -8.6004725 -7.5753889 -5.6492591 -4.1306849 -2.7302513 -2.4404716 -2.8855958 -3.41471 -4.2901726 -4.8935862 -5.8513508 -6.9595466 -7.67886][-11.499294 -10.719708 -9.98637 -8.7195034 -7.2132092 -5.4271088 -3.5822749 -2.8416505 -2.3868995 -3.0839629 -4.0333338 -4.3248711 -5.0639877 -5.6195965 -6.5045447][-12.085391 -12.056437 -11.442202 -10.205551 -8.6633978 -6.847928 -4.8222494 -3.6321797 -3.1468558 -3.198823 -3.5228934 -4.0074739 -4.4902539 -5.2930889 -5.8999104][-11.293017 -11.79114 -11.555153 -10.653288 -9.49099 -8.2765789 -6.7950573 -5.7905025 -4.7191515 -4.170001 -4.2951365 -4.9548416 -5.5045033 -6.119462 -6.3432832]]...]
INFO - root - 2017-12-15 21:07:57.293234: step 60510, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 47h:27m:37s remains)
INFO - root - 2017-12-15 21:08:03.811308: step 60520, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 48h:42m:32s remains)
INFO - root - 2017-12-15 21:08:10.272233: step 60530, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 47h:24m:52s remains)
INFO - root - 2017-12-15 21:08:16.773782: step 60540, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 48h:59m:46s remains)
INFO - root - 2017-12-15 21:08:23.186791: step 60550, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 49h:39m:35s remains)
INFO - root - 2017-12-15 21:08:29.662733: step 60560, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 47h:26m:02s remains)
INFO - root - 2017-12-15 21:08:36.061901: step 60570, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 49h:36m:13s remains)
INFO - root - 2017-12-15 21:08:42.605597: step 60580, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 50h:13m:45s remains)
INFO - root - 2017-12-15 21:08:49.148947: step 60590, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 49h:51m:13s remains)
INFO - root - 2017-12-15 21:08:55.482106: step 60600, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 47h:52m:00s remains)
2017-12-15 21:08:55.977437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0814605 -4.3830194 -5.4773321 -5.9070435 -6.2552395 -6.7700109 -6.6884422 -5.9513426 -5.246767 -4.7515273 -5.305232 -6.5858097 -6.8277111 -6.6594915 -6.4270868][-3.2691021 -4.1150293 -5.2742186 -5.8729367 -6.42157 -5.8092861 -5.367054 -5.1081924 -4.7481194 -4.8577929 -5.7146139 -7.299058 -7.3415971 -6.9716759 -6.65199][-3.1201072 -3.5892916 -4.538805 -4.9325914 -4.8106279 -4.6942134 -4.2596841 -4.0926948 -3.8893321 -4.27512 -5.9075527 -7.4800196 -7.6000943 -7.8471565 -7.4592314][-2.9531202 -3.3116217 -3.3097944 -3.3553886 -3.1456213 -3.0384336 -2.8312869 -2.8162618 -2.5105529 -2.6209354 -4.1063528 -6.1788154 -7.1030531 -7.3049512 -7.2918863][-3.5604286 -3.533452 -3.4706893 -2.6640444 -1.5778193 -0.64468813 -0.57640457 -1.138268 -1.8370309 -2.6081333 -3.8035157 -5.668808 -6.0524974 -6.6467676 -6.9834957][-5.018013 -4.7102089 -4.0241418 -2.637536 -1.2477517 0.66925335 1.3590755 0.91596222 -0.090863228 -1.7065544 -4.3001394 -6.2812352 -6.5212765 -6.6205716 -6.6802673][-4.816071 -4.6581569 -3.9630215 -2.3124971 0.17973375 2.2055635 3.3456726 3.1641178 2.396884 0.40011883 -2.827549 -5.6777029 -6.3649364 -6.0446577 -6.0015979][-4.0624042 -3.8414574 -3.2055802 -1.4521832 0.85699558 3.3757286 4.7501755 4.2289524 3.2879543 1.2706652 -1.6009569 -4.6370192 -5.4537878 -5.9277029 -5.8309541][-4.7776651 -3.8295808 -2.9186516 -1.7183595 -0.065264225 2.2554951 3.753747 3.7639456 2.6789541 0.58254051 -2.2240291 -5.1086082 -5.7706714 -6.0996356 -6.360055][-6.1681833 -5.9466176 -5.2325654 -4.0405884 -2.4535675 -0.67729759 0.8432703 1.498168 1.5938406 -0.64731932 -4.2109542 -6.8986907 -7.1908059 -7.1726279 -7.1617689][-7.5795288 -7.06888 -6.5074782 -5.6807632 -4.7590265 -3.4317489 -2.2707171 -1.4405489 -1.0506368 -2.0275159 -4.1072311 -6.6310072 -7.2953959 -7.3658943 -7.1545835][-6.9525528 -7.0715971 -6.817524 -6.2681408 -6.1485548 -5.4154711 -4.5540791 -4.1299934 -3.9501741 -4.1042347 -4.9288282 -6.6165929 -6.6183276 -7.3148909 -7.0972815][-6.9440289 -6.5072145 -6.4843779 -6.386529 -6.3195095 -6.5021219 -6.4341426 -6.0770593 -5.5831833 -5.8432455 -6.261117 -7.1464782 -7.3720245 -7.841383 -7.9633532][-7.4718809 -6.8017111 -6.2047749 -5.9397354 -6.3521361 -6.7568007 -7.0549874 -7.4369144 -7.6482382 -7.6204505 -7.1976171 -7.1051164 -6.9954429 -7.0866365 -7.2220373][-8.1666851 -7.7381682 -7.4541755 -6.9440565 -6.8138614 -7.0266924 -7.19509 -7.4732256 -7.793632 -7.8417673 -7.8037128 -7.4076309 -6.5688481 -6.0913234 -5.7300563]]...]
INFO - root - 2017-12-15 21:09:02.346651: step 60610, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 48h:06m:24s remains)
INFO - root - 2017-12-15 21:09:08.724887: step 60620, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 48h:14m:14s remains)
INFO - root - 2017-12-15 21:09:15.163496: step 60630, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 50h:12m:13s remains)
INFO - root - 2017-12-15 21:09:21.587205: step 60640, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 48h:15m:37s remains)
INFO - root - 2017-12-15 21:09:28.023921: step 60650, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 48h:54m:59s remains)
INFO - root - 2017-12-15 21:09:34.407309: step 60660, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 48h:21m:52s remains)
INFO - root - 2017-12-15 21:09:40.773536: step 60670, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 47h:53m:33s remains)
INFO - root - 2017-12-15 21:09:47.235436: step 60680, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 49h:29m:49s remains)
INFO - root - 2017-12-15 21:09:53.688313: step 60690, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 50h:12m:15s remains)
INFO - root - 2017-12-15 21:10:00.089749: step 60700, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.623 sec/batch; 47h:00m:10s remains)
2017-12-15 21:10:00.597428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9993634 -3.2510734 -3.6474638 -3.8905835 -4.127718 -4.3666096 -4.5219431 -4.7148404 -4.718749 -6.13512 -7.3983927 -8.2670832 -8.7032824 -9.0433626 -9.1676073][-2.8963275 -3.3122811 -4.1820116 -4.8547287 -5.2584686 -5.2776136 -5.3216181 -5.8044314 -5.9481363 -7.0530376 -8.1454906 -9.2603521 -10.083458 -10.388743 -10.484801][-1.9600344 -2.3622627 -2.8405881 -3.3817821 -3.5248818 -3.665617 -3.7483816 -4.1099067 -4.3841457 -5.8516874 -7.2103195 -8.3355961 -8.9851942 -9.4890709 -9.8051462][-1.4335651 -1.7494531 -2.1955867 -2.2971339 -2.3497248 -2.3205323 -2.2509212 -2.5728106 -2.6823125 -4.2005053 -5.5876012 -6.521471 -7.43519 -8.1337471 -8.67907][-1.8101373 -1.6215434 -1.3845015 -1.1817813 -0.71644115 -0.37817383 -0.045417786 -0.10784674 -0.18645334 -1.4982853 -2.7970471 -4.0666046 -5.2781639 -6.4024653 -7.2351651][-2.980298 -2.3247814 -1.5956397 -1.2220011 -0.59027052 0.14289856 0.87955 1.0930014 1.3503742 -0.054647923 -1.4020782 -2.6153836 -3.964608 -5.1742706 -6.0526981][-3.5281692 -2.7365117 -1.7665358 -0.85320807 0.1902914 0.73230743 1.3134079 1.8240728 2.3902884 1.0662203 -0.332664 -1.4651384 -2.6786175 -4.1383963 -5.4286695][-3.8106089 -3.1022768 -2.1769257 -0.9479208 0.25712776 1.1096668 1.6918306 2.1661072 2.9946194 1.9880724 0.76596546 -0.4677186 -1.7619452 -3.1633554 -4.3769627][-3.5565886 -2.8408661 -2.2707968 -1.1808829 -0.17631578 0.63609886 1.3665171 1.8509073 2.3415346 1.2360573 -0.0011425018 -1.1053081 -2.2099385 -3.4890275 -4.7368259][-3.9682882 -3.4618673 -2.8720322 -1.853519 -0.89293337 -0.32568455 0.39169788 1.1657238 1.7785835 0.10668325 -1.237587 -2.386672 -3.4159923 -4.5739031 -5.5286722][-5.330308 -4.7774115 -4.2598963 -3.3793468 -2.7109213 -2.0762315 -1.2367654 -0.79471493 -0.42135859 -1.7408638 -3.068491 -3.9520571 -4.62821 -5.6725225 -6.5793333][-5.7571259 -5.510251 -4.9966564 -4.4724512 -3.9241548 -3.199584 -2.6794295 -2.5938559 -2.3692269 -3.5509939 -4.5982094 -5.4661155 -6.0352492 -6.4576006 -6.61032][-6.3440409 -5.928525 -5.581975 -5.1808596 -4.6814995 -4.2578182 -3.9414456 -3.8506513 -3.8859479 -4.82261 -5.6061358 -5.9319715 -6.4687138 -6.896131 -6.7315197][-6.2086 -5.7411532 -5.29727 -4.9602232 -4.755826 -4.4013038 -4.1868634 -4.3656559 -4.5540419 -5.4566936 -5.8590593 -5.9807048 -6.042099 -6.4918108 -6.3149137][-7.2886548 -7.0744758 -6.71493 -6.3218985 -6.1043682 -5.8743982 -5.75588 -5.8450422 -5.9724064 -6.2555861 -6.6160154 -6.96972 -7.091382 -6.6594391 -6.3856015]]...]
INFO - root - 2017-12-15 21:10:07.010943: step 60710, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 49h:47m:19s remains)
INFO - root - 2017-12-15 21:10:13.466742: step 60720, loss = 0.23, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 48h:56m:48s remains)
INFO - root - 2017-12-15 21:10:19.883092: step 60730, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:55m:54s remains)
INFO - root - 2017-12-15 21:10:26.260469: step 60740, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.678 sec/batch; 51h:12m:16s remains)
INFO - root - 2017-12-15 21:10:32.663314: step 60750, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 47h:00m:38s remains)
INFO - root - 2017-12-15 21:10:39.170630: step 60760, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 50h:31m:06s remains)
INFO - root - 2017-12-15 21:10:45.601935: step 60770, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 48h:39m:25s remains)
INFO - root - 2017-12-15 21:10:52.003630: step 60780, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 47h:27m:45s remains)
INFO - root - 2017-12-15 21:10:58.419144: step 60790, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 49h:33m:38s remains)
INFO - root - 2017-12-15 21:11:04.830868: step 60800, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 48h:00m:08s remains)
2017-12-15 21:11:05.359979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1931939 -5.9593496 -5.8960662 -5.8111134 -5.6852531 -5.5050206 -5.3535151 -4.8439188 -3.9205549 -4.0059786 -4.6616187 -5.1444712 -5.5830154 -6.1622205 -6.5239954][-5.7305818 -5.6413326 -5.544704 -5.8706508 -6.0879078 -5.9706345 -5.5756388 -5.2071409 -4.3084755 -4.0245528 -4.1704783 -4.8148227 -5.5235462 -5.9602823 -6.7003412][-4.8335848 -4.9393406 -4.7614603 -5.0937834 -5.8171082 -5.86908 -5.3958325 -5.06939 -4.697752 -4.4102688 -4.4502316 -4.8614712 -5.1747956 -5.7812657 -6.4126353][-4.4858775 -3.9622011 -3.9034576 -3.4686346 -3.197165 -3.6749783 -3.762965 -3.1836972 -2.351923 -3.0236387 -4.3875 -5.1199722 -5.1838436 -6.2800469 -7.1212196][-4.1740427 -3.5650663 -3.4216681 -3.1321292 -3.0200038 -2.4037004 -1.2011843 -1.1120234 -0.57456875 -1.4331446 -2.9412088 -4.6057296 -5.5759678 -6.6292062 -7.3971038][-3.563807 -3.1553535 -2.4494896 -1.5552783 -1.2318192 -0.61206627 0.15475035 0.44869328 1.1143093 0.22463608 -1.3553944 -3.5414839 -4.6824322 -6.2373061 -6.9859152][-3.8583226 -3.3111992 -2.2600451 -0.91466141 0.22818661 1.3821106 2.2736588 2.2426395 2.2760849 1.6775894 -0.11242247 -1.8326416 -2.9971261 -4.8822346 -6.1250911][-3.1995745 -3.00308 -2.0987244 -0.996994 -0.12797737 1.2639332 2.8369646 2.7968864 2.7336054 1.9552774 -0.55349874 -2.1691384 -3.2021809 -4.3666725 -5.4741745][-3.5310774 -3.0168257 -1.9149556 -1.1830783 -0.32408237 0.64756584 1.3911066 1.8475904 2.2969294 1.0225735 -1.4909816 -3.0891523 -4.2381945 -5.4870749 -5.7154007][-3.446197 -3.3625464 -2.9344139 -1.9461579 -0.57623625 -0.08376503 -0.08854866 0.44688511 0.58207321 -0.21043396 -2.1319551 -3.4744287 -4.6204891 -5.7057676 -6.3267932][-4.3605256 -4.3153634 -4.1422615 -3.6121306 -2.7524757 -2.469625 -2.0942655 -2.3123498 -2.5845079 -2.7246809 -3.9002531 -4.8118796 -5.4397879 -5.8296032 -6.4608269][-5.7590551 -5.0096521 -4.6109486 -4.4795637 -3.9718776 -3.9385347 -3.6665044 -3.8852251 -4.1193066 -4.67525 -5.4516096 -5.4076805 -6.0160789 -6.4799309 -6.9113917][-6.8140688 -6.6646109 -5.8596468 -5.3917942 -5.316247 -5.4601593 -5.4807606 -5.4690638 -5.3896308 -5.5852613 -6.4081287 -5.7123528 -5.8769388 -6.0993223 -6.2484894][-7.2028017 -7.155427 -7.4883966 -6.7645092 -5.9464111 -5.9355483 -6.1239605 -6.1324363 -6.094996 -6.0565748 -6.7031832 -6.2611485 -6.1928306 -5.9605131 -5.9256859][-8.7021227 -7.9402075 -7.8572903 -8.0184116 -7.9661059 -7.4928832 -7.2763419 -7.3624387 -7.27749 -6.9061208 -6.7873535 -6.5907736 -6.4914308 -6.1062269 -5.9461722]]...]
INFO - root - 2017-12-15 21:11:11.788236: step 60810, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 48h:13m:48s remains)
INFO - root - 2017-12-15 21:11:18.207681: step 60820, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 47h:10m:04s remains)
INFO - root - 2017-12-15 21:11:24.622706: step 60830, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 47h:24m:20s remains)
INFO - root - 2017-12-15 21:11:31.070003: step 60840, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 49h:28m:46s remains)
INFO - root - 2017-12-15 21:11:37.558630: step 60850, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.672 sec/batch; 50h:41m:13s remains)
INFO - root - 2017-12-15 21:11:43.966197: step 60860, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 47h:21m:51s remains)
INFO - root - 2017-12-15 21:11:50.449046: step 60870, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 48h:51m:52s remains)
INFO - root - 2017-12-15 21:11:56.908843: step 60880, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 48h:50m:11s remains)
INFO - root - 2017-12-15 21:12:03.333088: step 60890, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 49h:14m:08s remains)
INFO - root - 2017-12-15 21:12:09.821514: step 60900, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.674 sec/batch; 50h:50m:32s remains)
2017-12-15 21:12:10.343184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82368565 -0.89514446 -0.98543167 -1.0356927 -1.0004354 -0.95761013 -1.0861583 -1.0999665 -1.3490024 -3.3023067 -4.4405165 -5.6011715 -6.9827123 -7.1327276 -7.8201284][-2.3085356 -2.5067043 -2.9717174 -3.1518836 -3.2828422 -3.1239171 -2.9717069 -2.8219337 -3.0091767 -4.6591635 -5.5097256 -6.410676 -7.9430823 -7.8884172 -8.3760157][-3.5388865 -3.1873755 -2.7269254 -2.4786334 -1.8626261 -1.5163383 -1.2156453 -1.224566 -1.4990282 -2.9919519 -3.9781909 -5.1227379 -6.2605796 -6.580121 -7.3610158][-4.1607389 -3.8516541 -3.1960926 -2.5125294 -1.5502491 -1.010282 -0.55875158 -0.51598406 -0.67837667 -2.366086 -3.1901069 -3.9177885 -4.8783579 -5.0720119 -5.472682][-3.9980857 -3.0780935 -2.45582 -2.0941973 -1.4421053 -0.65945721 0.12609863 0.16578722 -0.20443916 -1.9639559 -2.7751136 -3.629662 -4.8824968 -4.7119374 -5.3565645][-4.1572075 -3.2387161 -2.7392497 -1.5805783 -0.128407 1.0808134 1.997221 1.9226742 1.8634825 -0.13313484 -1.3503747 -2.4552264 -3.5231309 -3.6445289 -4.1930361][-3.965652 -3.0110664 -2.1812716 -1.0189404 0.60238171 1.9054794 2.9651203 2.9245243 2.4966297 0.59209061 -0.66591072 -1.9976258 -3.4065847 -3.4212141 -3.752877][-2.5408778 -1.4215064 -0.59936237 0.63946629 2.23905 3.1777554 4.0278416 3.844264 3.4603004 1.1265583 -0.43756771 -1.9309669 -3.4463377 -3.8328781 -4.2604084][-1.6508117 -0.51083279 -0.050807476 0.83199215 1.6970158 2.4709482 2.9135103 2.9418182 3.0901651 1.1805573 -0.36907911 -2.0721917 -3.8503697 -4.0389314 -4.3582253][-2.0208354 -1.2051868 -0.83352137 -0.038022518 0.69896221 1.4803247 2.1345034 2.1481924 1.9974918 0.22800875 -1.0744891 -2.6202769 -4.1212244 -4.6473842 -5.2983904][-3.165966 -2.8208871 -2.4451423 -1.7928672 -1.2536435 -0.4107852 0.1674962 0.18951321 0.12076998 -1.6879764 -2.9225535 -3.6390553 -4.8577442 -5.2057447 -5.69543][-5.4963541 -4.547823 -4.0947552 -3.2842388 -3.0308847 -2.779139 -2.5357184 -2.3469825 -2.2221131 -3.3081369 -3.9100196 -4.7302895 -5.642518 -5.818717 -6.1178269][-7.3359485 -6.7698884 -6.3770218 -5.7429132 -4.9912195 -4.3560748 -4.2222366 -4.169405 -4.0060997 -4.3061829 -4.4774961 -4.9589882 -5.6163039 -5.856123 -5.7620888][-7.5361986 -7.1031685 -6.6206651 -5.7400794 -5.3027687 -4.8060265 -4.7284241 -4.5647354 -4.6052666 -5.0572639 -5.1009941 -5.3036819 -5.683073 -5.3417711 -5.47806][-8.3415461 -8.2278 -7.597836 -7.3613558 -7.1462197 -6.5894194 -6.0879755 -6.2331619 -6.2063241 -6.1826696 -6.3444343 -6.4030633 -6.2977018 -5.9477839 -5.6001453]]...]
INFO - root - 2017-12-15 21:12:16.664054: step 60910, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 46h:34m:41s remains)
INFO - root - 2017-12-15 21:12:23.013411: step 60920, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 48h:59m:45s remains)
INFO - root - 2017-12-15 21:12:29.454137: step 60930, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 48h:34m:35s remains)
INFO - root - 2017-12-15 21:12:35.840519: step 60940, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:17m:45s remains)
INFO - root - 2017-12-15 21:12:42.367409: step 60950, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 49h:22m:11s remains)
INFO - root - 2017-12-15 21:12:48.842012: step 60960, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 48h:49m:27s remains)
INFO - root - 2017-12-15 21:12:55.282832: step 60970, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 49h:12m:04s remains)
INFO - root - 2017-12-15 21:13:01.631774: step 60980, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 47h:22m:45s remains)
INFO - root - 2017-12-15 21:13:08.001787: step 60990, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 47h:22m:54s remains)
INFO - root - 2017-12-15 21:13:14.360514: step 61000, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 48h:39m:27s remains)
2017-12-15 21:13:14.897719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0509272 -5.658361 -4.3915014 -3.9718204 -3.7744915 -3.5905948 -3.4159503 -3.6354795 -3.27987 -4.452044 -4.4403305 -4.3186703 -4.8698711 -6.0212336 -6.8016152][-6.3299131 -5.637279 -5.0514607 -4.7240677 -4.1559668 -3.9545302 -3.7759879 -3.3414178 -2.8742051 -4.0032792 -4.3556395 -4.5079718 -5.276526 -6.2275124 -6.918489][-6.1322179 -5.6913519 -5.3869648 -4.9829559 -4.4788785 -3.5734921 -3.21938 -2.9622803 -2.6520939 -4.2172165 -4.4378738 -4.8900805 -5.5872393 -6.3077817 -6.447845][-5.5877008 -4.7158966 -4.1452579 -3.7277639 -3.2129326 -2.6925311 -2.0295286 -1.6475897 -0.91176081 -2.4236822 -3.2598338 -3.9283648 -5.0396347 -6.1197033 -6.235589][-4.8021088 -4.0284138 -3.3923454 -2.9299293 -2.2041054 -1.0730634 0.15535879 0.30991364 0.70604134 -0.97202158 -1.8018751 -2.5959969 -4.3420024 -6.0672803 -6.7878618][-5.5600352 -4.4538641 -3.6417255 -2.6310625 -1.4880161 -0.10590315 0.992959 1.2849503 1.3973618 -0.65032387 -1.693573 -2.6673141 -4.1237416 -5.496973 -6.39163][-5.1520538 -4.2437043 -3.1257281 -1.5337472 -0.34026623 0.7870369 1.7591381 2.1089172 2.1312771 -0.2229352 -1.4999042 -2.913126 -4.2564335 -5.4308095 -5.9774733][-4.5911345 -3.3778505 -2.4055977 -0.97649574 0.50721359 1.8790894 2.4945288 2.9015465 3.2281847 1.0733681 -0.82959795 -2.6048026 -4.09351 -5.1612463 -5.7597203][-4.9515033 -4.24548 -3.450891 -1.8959084 -0.42647886 1.0143681 1.3197975 1.6094112 1.7647333 0.12710476 -1.1168046 -2.9490809 -4.7599182 -5.8439279 -6.464788][-5.2907948 -4.405417 -3.7150111 -2.387249 -1.2598548 -0.17094326 -0.13166523 -0.3263073 -0.63358021 -2.3419337 -2.8743458 -3.8128788 -5.3476505 -6.5961094 -6.9306555][-5.2295408 -4.6998892 -3.9414496 -2.7904816 -2.0082107 -1.3328547 -0.96756744 -0.90247393 -1.173811 -2.6201377 -3.1753578 -3.8217902 -4.6702042 -5.5444546 -6.2718925][-6.9356389 -6.475071 -5.8945613 -5.1763086 -4.1663971 -3.4998965 -3.2203641 -3.1553159 -3.2793865 -3.9513321 -4.4632764 -4.8530936 -5.08733 -5.5643892 -6.0543647][-7.4547553 -6.9740572 -6.4984303 -5.8730106 -5.3053346 -4.8633652 -4.6537709 -4.5906148 -4.4125433 -5.2456741 -5.5445242 -6.0446897 -6.3609333 -6.4746485 -6.0843759][-6.9543233 -6.1441851 -5.5709405 -5.4579039 -5.1098957 -4.9540181 -4.927927 -4.9451332 -4.7641268 -5.4123864 -5.510139 -5.8966942 -6.0065675 -6.3124895 -6.1059628][-8.9126892 -7.956768 -6.9600515 -6.2734861 -6.0064259 -5.7766404 -5.9889417 -5.7957163 -5.778028 -5.8200016 -5.3847342 -5.3672333 -5.5548058 -5.7921772 -5.9359426]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:13:21.333049: step 61010, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 48h:00m:15s remains)
INFO - root - 2017-12-15 21:13:27.716244: step 61020, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 47h:43m:36s remains)
INFO - root - 2017-12-15 21:13:34.125358: step 61030, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.654 sec/batch; 49h:19m:15s remains)
INFO - root - 2017-12-15 21:13:40.552764: step 61040, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 48h:01m:11s remains)
INFO - root - 2017-12-15 21:13:47.021380: step 61050, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 47h:50m:34s remains)
INFO - root - 2017-12-15 21:13:53.457599: step 61060, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 47h:37m:10s remains)
INFO - root - 2017-12-15 21:13:59.954988: step 61070, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 47h:48m:54s remains)
INFO - root - 2017-12-15 21:14:06.400402: step 61080, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 47h:52m:44s remains)
INFO - root - 2017-12-15 21:14:12.929753: step 61090, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 49h:59m:21s remains)
INFO - root - 2017-12-15 21:14:19.321954: step 61100, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 47h:53m:18s remains)
2017-12-15 21:14:19.913558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8287783 -4.7769651 -4.5964031 -4.9818487 -4.8047228 -4.5793562 -4.5777407 -3.9821181 -3.2501826 -3.4903388 -3.99547 -4.73032 -5.0757122 -5.8242903 -6.2409129][-4.7598033 -4.8024735 -5.0384088 -4.9655166 -4.79105 -4.6796789 -4.5568228 -4.1077242 -3.514009 -3.5734715 -4.1740723 -5.1205597 -5.5790334 -5.9192648 -5.801034][-5.65842 -5.1764565 -5.08461 -4.6774879 -4.3921208 -4.1931381 -3.7131836 -3.5138321 -3.1788659 -3.4823203 -4.2706079 -5.2507772 -5.8523874 -6.1937566 -5.8689327][-6.0130029 -5.3566728 -5.0781431 -3.9971457 -3.2806334 -2.7874393 -2.1787381 -1.8372908 -1.6330957 -2.4034023 -3.5597897 -4.90968 -5.5524983 -6.5153675 -6.8183589][-6.2085705 -5.0725584 -4.0932026 -2.8969951 -2.0124431 -1.1359501 -0.21270704 -0.14925003 -0.2267437 -0.95613718 -2.1176982 -3.5704021 -4.9790373 -6.3235641 -6.8363][-5.8585958 -4.8617358 -3.9404769 -2.2343044 -0.73072004 0.58612251 1.5418491 1.4847326 1.5021009 0.57423878 -0.794703 -2.3794279 -3.8754687 -5.3849707 -6.3079543][-5.5518417 -4.2939281 -2.8269033 -1.1859627 -0.18132019 1.4408855 2.5242863 2.5360651 2.8564749 1.9421024 0.37319183 -1.6982856 -3.3488941 -4.7370081 -5.5178413][-4.8866882 -3.9292989 -2.4647927 -0.76615524 0.65369797 2.0905581 2.5971661 2.9447756 3.4081602 2.5046692 1.0301876 -1.0599685 -2.7411547 -4.1610126 -5.1596179][-4.7048535 -3.7044492 -2.6247754 -1.2240906 0.23073387 1.3766012 1.9123135 2.3217983 2.707077 1.9478159 0.59512615 -1.331255 -2.700645 -4.1945848 -5.1638842][-5.5662031 -5.0352125 -3.9482636 -2.8062897 -1.2637215 -0.12082195 -0.032922745 0.45370865 0.61672783 -0.16089439 -1.2691493 -2.4308543 -3.0771661 -4.1461697 -5.1990871][-6.935143 -6.3053989 -5.8125305 -4.8383846 -3.6950557 -2.9589276 -2.5417304 -2.0710187 -2.1621728 -2.4107838 -3.1953321 -3.7295756 -4.3099742 -5.2289906 -5.6656961][-7.8077927 -7.7055988 -7.1818743 -6.3820505 -5.4463058 -4.9625177 -4.3074074 -4.4461346 -4.2620354 -4.2602634 -4.7892542 -4.7669563 -5.0477614 -5.9452763 -5.862287][-7.94083 -7.7849231 -7.8692746 -7.1984067 -6.273838 -5.8669572 -5.3870215 -5.3997669 -5.2246065 -5.1549006 -5.7986689 -5.5074415 -5.8259912 -6.2309742 -5.6551676][-7.6296134 -7.3587923 -6.8672976 -6.2657566 -6.0987682 -5.6366367 -5.2401381 -5.3486595 -5.285337 -5.2685561 -5.6406908 -5.52718 -5.5477524 -5.9576521 -5.9886155][-7.6470971 -7.5220671 -6.740737 -6.4608278 -6.1244726 -5.8081617 -5.8548875 -5.7666082 -5.8493776 -6.1158061 -6.2188525 -6.4107842 -6.1845465 -6.1142039 -6.3847284]]...]
INFO - root - 2017-12-15 21:14:26.338018: step 61110, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 47h:43m:59s remains)
INFO - root - 2017-12-15 21:14:32.741139: step 61120, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 47h:41m:17s remains)
INFO - root - 2017-12-15 21:14:39.193134: step 61130, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 47h:40m:49s remains)
INFO - root - 2017-12-15 21:14:45.672706: step 61140, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 49h:26m:33s remains)
INFO - root - 2017-12-15 21:14:52.018618: step 61150, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 47h:21m:03s remains)
INFO - root - 2017-12-15 21:14:58.468967: step 61160, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 50h:04m:13s remains)
INFO - root - 2017-12-15 21:15:04.965585: step 61170, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 48h:43m:30s remains)
INFO - root - 2017-12-15 21:15:11.424584: step 61180, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 48h:00m:02s remains)
INFO - root - 2017-12-15 21:15:17.834728: step 61190, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 48h:07m:59s remains)
INFO - root - 2017-12-15 21:15:24.234751: step 61200, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 50h:26m:58s remains)
2017-12-15 21:15:24.759265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0000129 -3.2377787 -3.341877 -4.35957 -5.354691 -5.3871183 -5.6196561 -5.702672 -5.7729545 -4.565896 -5.9922 -4.8525524 -6.1702495 -5.6909409 -6.5015287][-2.8547711 -3.2049289 -3.51583 -4.1049213 -3.9596736 -3.1278572 -3.5420685 -4.0661588 -4.6057186 -4.1264725 -7.3114414 -6.2453613 -7.1562862 -6.3352504 -6.5902719][-4.1333666 -3.2054629 -2.7365713 -2.8963537 -3.1189346 -3.2247424 -2.5069833 -2.6573787 -2.234375 -2.6042633 -6.6130424 -5.9553633 -7.856082 -7.9572554 -7.949388][-3.3938541 -3.9207094 -4.3022285 -3.0932174 -2.2303829 -1.5507627 -1.6990995 -1.7550936 -1.4235516 -1.5730276 -3.88368 -4.8270178 -7.6994815 -7.6926112 -7.6463876][-5.9816504 -4.8096209 -4.1196818 -3.0924597 -1.620616 -1.2279873 -2.0046425 -1.2622762 -0.74572611 -0.87950325 -2.9901814 -3.7113121 -5.5481396 -7.0293794 -8.5766068][-3.7829664 -3.3374405 -3.1076345 -1.4984102 0.6062336 2.2925186 3.6333237 2.6864262 0.83152485 0.50206375 -2.1865926 -2.6010675 -5.0728507 -5.2170572 -5.1409287][-3.4586205 -2.6488905 -1.2027822 0.1097393 1.3391199 3.1587477 5.2344828 4.9237318 4.59412 2.3339109 -1.7283397 -1.9966254 -3.4411793 -5.259944 -5.6539507][-3.1975851 -2.7149725 -1.5074716 -0.3537674 0.60983181 2.2818985 4.1027 3.9526443 3.483882 2.3987789 -0.56526041 -2.7298145 -5.3256464 -6.2820034 -6.3240633][-4.5556965 -3.8696263 -2.0729852 -1.0987072 0.095586777 1.2016382 1.5872812 1.8666086 3.0422707 1.8024321 -1.8153782 -2.1797662 -4.650404 -6.4202051 -7.2251067][-6.3836379 -6.3715196 -4.9209538 -4.1974049 -2.5150809 -0.52558851 -0.29405642 -0.4160881 -0.32938766 -0.52213478 -1.4944148 -3.7257955 -6.2384043 -6.7519608 -6.78376][-7.6356654 -7.7195797 -7.6505828 -6.6735458 -5.2793846 -5.4048147 -4.330966 -2.6477199 -2.1399679 -2.5680189 -4.1752467 -5.1143975 -5.7424469 -7.5676618 -8.1491966][-7.9145584 -8.2175283 -7.9366908 -8.0836773 -6.6389675 -5.9098554 -4.8830128 -5.5585394 -5.8437996 -5.0778027 -5.4659791 -5.39073 -6.7369528 -7.424314 -7.4461565][-7.8810205 -7.7918234 -7.7351913 -7.7779231 -7.1832285 -5.7429056 -4.6505241 -4.4404612 -4.9704542 -5.7472224 -6.8511229 -6.7199888 -6.628861 -6.6799822 -6.6956692][-6.2587576 -7.431088 -8.5297194 -8.160388 -8.4563913 -7.3990412 -6.6207685 -5.464591 -4.4112616 -4.223423 -5.2371855 -5.9688115 -7.0850334 -7.67861 -7.7863793][-7.1946349 -6.8778934 -7.6238003 -8.3225813 -7.8736763 -7.4403267 -7.2724457 -6.8520126 -5.9461632 -5.9262166 -5.5987883 -6.5016618 -7.7609296 -7.8396411 -7.4766474]]...]
INFO - root - 2017-12-15 21:15:31.232245: step 61210, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 48h:07m:02s remains)
INFO - root - 2017-12-15 21:15:37.627287: step 61220, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 48h:42m:05s remains)
INFO - root - 2017-12-15 21:15:44.009333: step 61230, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:12m:53s remains)
INFO - root - 2017-12-15 21:15:50.402433: step 61240, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 49h:20m:01s remains)
INFO - root - 2017-12-15 21:15:56.811382: step 61250, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 47h:20m:26s remains)
INFO - root - 2017-12-15 21:16:03.224613: step 61260, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 47h:40m:41s remains)
INFO - root - 2017-12-15 21:16:09.634334: step 61270, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 49h:03m:43s remains)
INFO - root - 2017-12-15 21:16:15.998049: step 61280, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 47h:35m:47s remains)
INFO - root - 2017-12-15 21:16:22.382019: step 61290, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 48h:37m:43s remains)
INFO - root - 2017-12-15 21:16:28.717057: step 61300, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 47h:43m:17s remains)
2017-12-15 21:16:29.237206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5436163 -3.3510575 -3.3546796 -3.1111927 -2.7279806 -2.4141421 -1.656517 -0.77552605 -0.38510418 -1.6861787 -3.8143392 -8.2085867 -10.744842 -12.065166 -13.31423][-1.4813933 -2.2710876 -3.8461239 -4.2844992 -4.2239294 -3.3261333 -2.1738143 -1.6218238 -1.7680154 -3.1824856 -4.42268 -8.082468 -10.163597 -10.907768 -11.953479][-0.90396833 -1.4839392 -2.270412 -2.7258911 -3.3781104 -2.567709 -1.6319094 -0.91180372 -0.81708288 -2.4970818 -4.4719772 -8.1832047 -9.2378635 -9.7255478 -10.725737][-1.1244311 -1.1369472 -1.5812001 -1.6331902 -1.6816993 -1.4687462 -0.806365 -0.32337952 -0.47234249 -2.0104346 -3.7157714 -7.8989363 -9.3430281 -9.668251 -10.027409][-1.9112029 -0.97647047 -1.330287 -1.5207214 -1.1406927 -0.51073503 0.9414444 1.7523155 1.6031685 -0.56232786 -2.496841 -6.7007132 -8.6006069 -9.0775251 -10.167561][-2.6614003 -1.6927567 -1.5447354 -1.0578279 -0.4350729 0.44398403 1.7756329 2.5922661 2.7070713 0.96496964 -1.070457 -5.3363075 -6.8431635 -7.7442665 -8.4407673][-3.7750585 -2.1413221 -1.1814365 -0.55125 -0.04532671 0.70276737 1.5757809 2.225852 2.567174 1.0554419 -0.80196953 -5.1378469 -6.8829637 -7.4124107 -8.3840027][-4.0292807 -2.9467845 -2.1638088 -0.81906319 0.1373167 0.43047047 1.0278215 1.6289463 1.963769 0.73085213 -0.55044079 -4.5213671 -6.1142726 -7.0513377 -8.093257][-4.0487795 -3.1858029 -2.6036839 -1.467833 -0.45572567 0.097999096 0.70316505 1.2616711 1.498023 0.14385176 -1.3752074 -5.1326475 -5.9948411 -6.62282 -7.806036][-4.951601 -4.1946435 -3.7133322 -2.8018446 -2.1253519 -1.3494558 -0.39274073 0.035870075 0.274611 -1.0382342 -2.3654323 -5.1497459 -6.0073223 -6.3883572 -7.3028359][-6.2084503 -6.0325079 -5.7325788 -4.7883749 -4.0290403 -3.6275177 -2.8789577 -2.4446697 -2.3530574 -3.4467053 -4.2659507 -6.1187811 -6.3504004 -6.40444 -6.6556087][-7.5932097 -7.3666868 -6.9831018 -6.2538996 -5.8899555 -5.3911428 -4.70395 -4.4068928 -4.113287 -4.6269922 -5.0325708 -6.2640014 -6.2109709 -6.2998228 -6.5600066][-8.088479 -8.2346668 -8.0172319 -7.6005907 -7.0673666 -6.703074 -6.15464 -5.8033352 -5.4800692 -5.520679 -5.6696129 -5.8643851 -5.5350456 -5.8262882 -5.8228593][-7.5267935 -7.4284167 -7.3542719 -7.2673473 -7.1794376 -7.0468531 -7.1361341 -6.6856642 -5.8524542 -5.5694017 -5.4040565 -5.428443 -5.1731963 -5.1860809 -5.3959956][-7.9133611 -7.8400922 -8.2504692 -8.1839886 -8.4902124 -8.4516239 -7.5990653 -7.0999923 -6.861155 -6.4467864 -5.8701591 -5.468092 -5.1732016 -5.1742325 -4.8601027]]...]
INFO - root - 2017-12-15 21:16:35.666890: step 61310, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 48h:22m:19s remains)
INFO - root - 2017-12-15 21:16:42.130074: step 61320, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 48h:34m:09s remains)
INFO - root - 2017-12-15 21:16:48.514242: step 61330, loss = 0.30, batch loss = 0.18 (13.1 examples/sec; 0.611 sec/batch; 46h:03m:20s remains)
INFO - root - 2017-12-15 21:16:54.908645: step 61340, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 49h:02m:51s remains)
INFO - root - 2017-12-15 21:17:01.318061: step 61350, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 47h:45m:19s remains)
INFO - root - 2017-12-15 21:17:07.800890: step 61360, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.643 sec/batch; 48h:23m:36s remains)
INFO - root - 2017-12-15 21:17:14.214868: step 61370, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.623 sec/batch; 46h:55m:08s remains)
INFO - root - 2017-12-15 21:17:20.642320: step 61380, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 49h:17m:20s remains)
INFO - root - 2017-12-15 21:17:27.027746: step 61390, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:14m:01s remains)
INFO - root - 2017-12-15 21:17:33.533867: step 61400, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 49h:59m:12s remains)
2017-12-15 21:17:34.036041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0067921 -1.6042967 -1.2666407 -0.91813993 -0.86763859 -1.0075703 -1.5428901 -1.8593559 -1.9422917 -3.5283623 -4.3398228 -5.5064135 -6.5957251 -8.4096861 -9.5236864][-2.3347282 -1.7686481 -1.3664207 -1.2603755 -1.1999874 -1.2827926 -1.8457541 -2.550787 -2.7407269 -3.8545556 -4.2946672 -5.4265046 -6.3380418 -7.7129927 -8.4372644][-3.1708202 -2.7766142 -2.6646857 -2.3115063 -2.2636309 -1.9845958 -1.9975243 -2.5067363 -2.840538 -3.9750478 -4.5842752 -5.5324383 -6.4108562 -8.1665115 -9.10639][-4.8721695 -3.6353421 -2.7818785 -2.2502451 -1.9282656 -1.2555976 -1.0229626 -1.5324168 -1.758255 -2.9918771 -3.8489296 -4.6962852 -5.7225947 -7.3908324 -7.8237576][-5.9094133 -4.585619 -3.2873607 -1.9382443 -1.1302009 -0.12728262 0.81846619 0.62329006 0.046089172 -1.6568618 -2.7612786 -4.3181982 -5.6941271 -7.345902 -7.4926734][-6.2769217 -5.41346 -4.2219591 -2.3034687 -0.86898375 0.13109016 0.99544811 1.5326748 1.3572474 -0.5646739 -1.5764813 -2.8363919 -4.29959 -6.5669127 -6.965652][-5.654984 -4.1602783 -2.6637235 -0.82312679 1.040307 2.1771097 3.3002691 3.2820415 2.6918087 0.58636665 -0.46554661 -1.5385346 -2.6186142 -4.2756319 -4.7795849][-4.0467081 -3.3259192 -1.8711257 0.57323647 2.6146345 3.8219328 4.6316423 4.8832703 4.6859875 2.5138741 1.6103258 -0.067682266 -1.6801291 -3.5825405 -4.2313385][-5.0733619 -4.5247087 -3.0486574 -0.66785336 1.5335522 2.862874 3.9082203 4.3141584 4.0015459 2.2295055 1.5855598 0.30502176 -0.98813677 -3.062005 -3.9202683][-7.0587487 -6.3373718 -5.400723 -3.8074853 -1.8934669 0.17316723 1.986659 2.738905 2.8693781 0.6936636 -0.42999315 -1.2642555 -1.8906112 -3.2161102 -3.6657271][-9.3338852 -8.7185841 -7.9436173 -6.4019117 -5.2776985 -3.3958406 -1.4705954 -0.84730434 -0.5302062 -1.8038688 -2.611536 -3.8444464 -4.6994376 -5.5301185 -5.6625471][-10.965357 -10.207496 -9.3021355 -7.9856219 -6.9010625 -5.5530262 -4.4148712 -4.0885015 -3.6729088 -4.67237 -5.3984675 -5.8514791 -6.37776 -6.9268365 -7.2179313][-11.944181 -11.025587 -10.543719 -9.6315365 -8.8666239 -7.7233019 -6.481225 -6.4320307 -6.3453417 -7.1295791 -7.2631636 -7.1351409 -6.9314184 -6.8549023 -6.4028273][-10.715372 -10.605399 -10.613434 -9.6633759 -9.1103725 -8.226099 -7.3510079 -7.7578239 -7.7339234 -7.9302135 -7.7937 -7.5938687 -7.2787137 -6.8947835 -6.3738232][-9.9001684 -9.7271566 -9.910696 -9.6610823 -8.9315662 -7.9303689 -7.2734323 -7.4682488 -7.6401706 -7.6357784 -7.7282495 -7.5573549 -7.0149322 -6.5039139 -5.6824522]]...]
INFO - root - 2017-12-15 21:17:40.468161: step 61410, loss = 0.37, batch loss = 0.25 (12.6 examples/sec; 0.635 sec/batch; 47h:46m:47s remains)
INFO - root - 2017-12-15 21:17:46.871411: step 61420, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 47h:31m:11s remains)
INFO - root - 2017-12-15 21:17:53.283220: step 61430, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 48h:37m:16s remains)
INFO - root - 2017-12-15 21:17:59.712991: step 61440, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 47h:58m:22s remains)
INFO - root - 2017-12-15 21:18:06.179176: step 61450, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 49h:48m:22s remains)
INFO - root - 2017-12-15 21:18:12.600628: step 61460, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 48h:19m:50s remains)
INFO - root - 2017-12-15 21:18:19.054134: step 61470, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 47h:49m:28s remains)
INFO - root - 2017-12-15 21:18:25.523928: step 61480, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 47h:16m:41s remains)
INFO - root - 2017-12-15 21:18:31.919111: step 61490, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 49h:01m:47s remains)
INFO - root - 2017-12-15 21:18:38.326928: step 61500, loss = 0.26, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 50h:02m:05s remains)
2017-12-15 21:18:38.976964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9204354 -2.1641893 -2.9031119 -3.3632941 -3.3180418 -3.0055051 -3.000917 -3.1504941 -2.9858007 -4.0923672 -5.2700744 -5.9623604 -7.0454874 -7.7614508 -7.9556656][-2.5120568 -1.9283953 -2.5311322 -3.7114217 -4.2336283 -3.7451963 -3.5793204 -3.530098 -3.4357576 -4.0317755 -5.4402437 -6.0189695 -7.0453205 -8.2728415 -8.0727053][-1.8040924 -2.0212855 -2.3118925 -2.9323173 -3.6365786 -3.57897 -3.3276067 -3.1990919 -3.2005744 -3.9612291 -5.6136656 -6.2984385 -6.8566208 -7.8070908 -8.3139515][-1.0602889 -0.31020498 -0.69470882 -1.5944071 -1.9046578 -1.4512687 -1.224287 -1.0709548 -1.3631668 -2.8375187 -4.9556408 -6.0528603 -6.3381519 -6.6097646 -6.2640691][-1.6331553 -0.89792156 -0.3767786 -0.8216238 -1.2993212 -0.54797077 0.15631199 0.1319561 -0.21044588 -2.0794005 -4.4647055 -5.8755083 -6.6395335 -7.2230992 -6.9115429][-3.4953198 -2.4875135 -1.7824044 -1.6148343 -1.3695221 -0.38204336 1.2597551 1.8809986 1.3386497 -0.96159458 -3.8652956 -5.7415428 -6.7904086 -7.4949174 -7.3116994][-3.4564152 -2.4528809 -1.2711406 -0.55591488 0.42904282 1.7303505 2.9799509 3.3894424 3.5002155 1.1165123 -2.668139 -4.8388376 -5.8787742 -6.8179488 -6.7002831][-3.1502442 -1.6125636 -0.76877737 -0.23201561 0.67781734 2.4691753 4.1176262 4.6784506 4.4830265 2.0774174 -1.4745402 -4.0559907 -5.352767 -6.4536042 -6.3229241][-4.4974418 -3.2028847 -2.0689874 -1.1065516 -0.59063768 0.068007469 1.7383413 3.2074966 3.539134 1.5931416 -2.106976 -4.545722 -5.8597 -6.9563141 -6.3926377][-4.960484 -3.8505864 -3.0156856 -1.984704 -1.3192015 -0.86249876 -0.074670315 0.8469286 1.476759 -0.52454567 -3.4456425 -4.9603858 -6.3776259 -7.500689 -7.21095][-6.305378 -5.9712629 -4.7752862 -3.3918986 -2.7913837 -2.419631 -1.7845283 -1.2041092 -0.2909646 -2.0183272 -4.5664234 -5.6773291 -6.544579 -7.4506865 -7.551301][-7.1678395 -6.953218 -6.0774059 -5.1564989 -4.2497072 -3.6695113 -3.144712 -2.7634568 -2.1184406 -3.1576004 -4.2158914 -5.0013676 -6.3742428 -7.4018264 -7.3967991][-8.9061327 -8.5899839 -8.0561867 -6.8242078 -5.9088464 -5.0061312 -4.5621362 -4.2818685 -4.0300446 -5.2803726 -5.8902812 -6.0213747 -6.7276192 -7.1209927 -7.098][-9.7099037 -9.8268642 -8.9097071 -8.2102108 -7.1811037 -5.9798145 -5.5166631 -5.8541441 -5.9071112 -6.8315754 -6.8771629 -6.5042877 -6.5366812 -6.6954947 -6.8058605][-9.846343 -9.6686211 -9.1385 -8.4020853 -7.7706041 -6.9528213 -6.6802931 -6.5012417 -6.7535391 -7.1291122 -7.2862792 -7.167161 -6.5396781 -5.9792418 -5.5883412]]...]
INFO - root - 2017-12-15 21:18:45.416412: step 61510, loss = 0.30, batch loss = 0.19 (13.1 examples/sec; 0.612 sec/batch; 46h:03m:37s remains)
INFO - root - 2017-12-15 21:18:51.796977: step 61520, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 48h:25m:49s remains)
INFO - root - 2017-12-15 21:18:58.194684: step 61530, loss = 0.23, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 49h:39m:50s remains)
INFO - root - 2017-12-15 21:19:04.611687: step 61540, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.640 sec/batch; 48h:11m:11s remains)
INFO - root - 2017-12-15 21:19:11.024383: step 61550, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 48h:33m:29s remains)
INFO - root - 2017-12-15 21:19:17.357877: step 61560, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 47h:35m:16s remains)
INFO - root - 2017-12-15 21:19:23.802362: step 61570, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 48h:06m:42s remains)
INFO - root - 2017-12-15 21:19:30.213875: step 61580, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 47h:26m:53s remains)
INFO - root - 2017-12-15 21:19:36.716791: step 61590, loss = 0.36, batch loss = 0.25 (12.4 examples/sec; 0.647 sec/batch; 48h:40m:57s remains)
INFO - root - 2017-12-15 21:19:43.148066: step 61600, loss = 0.24, batch loss = 0.12 (12.8 examples/sec; 0.623 sec/batch; 46h:54m:48s remains)
2017-12-15 21:19:43.699012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4119205 -4.3447905 -4.0978174 -4.4738245 -5.3127789 -5.6823678 -6.0009465 -6.0231023 -5.6258841 -6.983429 -7.768723 -8.0342312 -8.7412558 -9.3806839 -9.0387621][-3.5160165 -4.2248526 -4.7471752 -5.0669794 -5.7186594 -6.0521793 -6.1880674 -6.0251575 -5.7879848 -7.0986395 -7.5499668 -8.1629925 -9.2123833 -9.7589884 -9.4435587][-3.4013395 -4.0966663 -4.3974347 -4.2634258 -4.44551 -4.3822403 -4.2329373 -4.1034184 -3.9426548 -5.5739269 -6.3153186 -6.8854856 -7.9443922 -8.7201958 -8.6757545][-3.0705242 -3.2849455 -3.3662372 -3.1351728 -3.0897441 -2.8167992 -2.3399324 -2.1252589 -1.9391017 -3.7764781 -4.8978643 -6.0594482 -7.4036288 -8.0939264 -7.8687172][-4.0016694 -3.2911553 -2.1474257 -1.6565924 -1.1784286 -0.57770395 -0.066705704 0.1677227 0.26727438 -1.7914681 -3.1526279 -4.4300685 -6.1069245 -7.3447404 -7.2654209][-3.3332233 -3.0890565 -1.9127588 -0.78173208 0.48907471 1.6335268 2.4504795 2.4965439 2.2201748 -0.019113064 -1.5863738 -3.0151811 -4.9430943 -6.0111265 -5.9650488][-3.7596934 -3.4212418 -1.7846704 -0.047587872 1.5553856 2.6726456 3.480978 3.56746 3.2723846 0.56415367 -1.3763418 -2.992456 -5.1285019 -6.2641077 -6.5584135][-3.04219 -2.2844353 -0.82637882 1.1710777 2.4415503 3.2933588 3.7667227 3.8626232 3.5180578 0.94238377 -1.0676522 -2.65518 -4.9422278 -6.4409275 -6.867259][-2.0327315 -1.5030365 -0.192101 1.4829664 2.3283806 2.7051573 2.8567867 2.9263954 2.5287085 0.18699312 -1.7642274 -3.1279945 -5.2615604 -6.7518468 -7.2836456][-1.8145838 -1.6605701 -0.80940628 0.56540775 1.5002651 1.8445663 1.863575 1.5594368 1.0602512 -1.1631355 -2.6896534 -3.9111121 -5.6741009 -6.9649944 -7.4210629][-4.17691 -3.6032629 -2.5658164 -1.2800665 -0.48379755 -0.1183095 0.089031219 -0.14341688 -0.30785418 -2.2974062 -3.9572783 -4.8802128 -6.3491645 -7.3938808 -7.8892136][-4.4311261 -4.0361071 -3.5002165 -2.8213072 -2.3263798 -2.0692458 -1.9487414 -2.0133696 -1.9711413 -3.0871329 -4.3798275 -4.930089 -6.2369375 -7.1906729 -7.5763478][-5.39785 -5.5582361 -5.5753951 -5.1310625 -4.6693335 -4.1952991 -3.8432457 -3.9944067 -3.9321685 -4.4520464 -5.5346856 -5.8053045 -6.7232566 -7.035955 -6.9994793][-5.2515774 -5.5379181 -5.83366 -5.5358319 -5.0307212 -4.6117473 -4.4425611 -4.3422894 -3.9703622 -4.1286087 -4.7316885 -5.1753349 -5.7572575 -6.4108624 -6.8297729][-5.8542337 -5.5832629 -5.4420319 -5.729557 -5.9447722 -5.80432 -5.4524879 -5.2664261 -5.2230282 -4.8861742 -5.0245781 -5.4033146 -5.6562681 -6.1786823 -6.7108111]]...]
INFO - root - 2017-12-15 21:19:50.123000: step 61610, loss = 0.34, batch loss = 0.23 (12.9 examples/sec; 0.621 sec/batch; 46h:42m:09s remains)
INFO - root - 2017-12-15 21:19:56.504873: step 61620, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 49h:28m:14s remains)
INFO - root - 2017-12-15 21:20:02.833711: step 61630, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:07m:52s remains)
INFO - root - 2017-12-15 21:20:09.274902: step 61640, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 47h:25m:11s remains)
INFO - root - 2017-12-15 21:20:15.707308: step 61650, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 48h:15m:08s remains)
INFO - root - 2017-12-15 21:20:22.079303: step 61660, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 48h:49m:11s remains)
INFO - root - 2017-12-15 21:20:28.568479: step 61670, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 47h:18m:08s remains)
INFO - root - 2017-12-15 21:20:35.075997: step 61680, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 48h:01m:12s remains)
INFO - root - 2017-12-15 21:20:41.546344: step 61690, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 49h:42m:06s remains)
INFO - root - 2017-12-15 21:20:48.013223: step 61700, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 49h:20m:44s remains)
2017-12-15 21:20:48.521877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7973187 -5.2958193 -6.5844626 -7.1983523 -7.3094945 -7.1118879 -6.7438688 -5.8595524 -4.5820022 -4.8273354 -4.95906 -6.1826797 -6.4750338 -6.9316163 -6.78231][-3.7372866 -5.0908427 -6.2336416 -6.77598 -6.8994856 -6.1648445 -5.6025915 -5.045702 -4.8770151 -5.5257745 -5.7194386 -6.9706864 -7.2751451 -7.5042729 -7.1905866][-3.436017 -4.3009148 -5.1900816 -5.3423247 -5.0854683 -4.5642128 -4.1193781 -3.9524362 -3.9796469 -5.1661682 -6.3351245 -7.7226582 -7.8223333 -8.2788687 -7.9749441][-3.2336349 -3.455092 -3.5273929 -3.946521 -3.7412727 -3.1693702 -2.5804276 -2.6435881 -2.7576842 -3.7291124 -4.8147326 -6.7937927 -7.7380948 -8.1248932 -8.1949081][-3.712456 -3.8728449 -3.6540556 -2.7136016 -1.6906352 -0.7016716 -0.41666937 -0.83443737 -1.7791719 -3.4867821 -4.4994316 -6.3916721 -6.9934568 -7.5848713 -7.9132681][-5.1156454 -4.7236624 -3.84533 -2.2441974 -0.88816309 0.84197807 1.6840229 0.877944 -0.61512566 -3.0608759 -5.1237211 -6.9480705 -7.2108588 -7.2062778 -7.1917725][-4.7510042 -4.231039 -3.6368952 -1.7467117 1.1499052 3.1918468 4.1267729 3.5920115 2.3274193 -0.82259369 -3.7103229 -6.43114 -6.9048686 -6.8751707 -6.67089][-3.7361267 -3.54183 -2.8876519 -0.62488604 1.9256458 4.6316423 5.8293552 5.1764288 3.8039722 0.52747154 -2.0437303 -5.1982393 -5.951766 -6.3995562 -6.5067844][-4.2436004 -3.5247989 -2.7096272 -1.0606418 0.89540672 2.542758 3.8998995 3.6968994 2.7063274 -0.29784346 -2.8427873 -5.52454 -6.2435141 -6.628293 -6.7422094][-6.2055988 -5.5719652 -4.539753 -2.6878624 -1.2037663 0.241045 1.0113411 1.1025381 1.1005144 -2.0649471 -4.9932575 -7.1770654 -7.1734843 -7.2732162 -7.289638][-7.8657002 -7.4099636 -6.6973453 -5.3329763 -3.8756373 -2.4008136 -1.9727888 -1.5306177 -1.4789677 -3.63836 -5.3398104 -7.2366719 -7.6909022 -7.8750577 -7.3945217][-7.3854218 -7.56511 -7.4061027 -6.7601333 -6.1089983 -5.1566677 -4.0495915 -4.1306353 -4.20294 -5.2454863 -5.7843084 -7.1582308 -6.816885 -7.5200195 -7.3551726][-7.9213924 -6.9887033 -6.7045341 -6.1785965 -6.1728048 -6.0893965 -5.931026 -5.7750783 -5.4210124 -6.3071375 -6.570189 -7.2389412 -7.6265984 -8.0747671 -8.0365019][-8.0804634 -7.769999 -6.6237268 -5.9171171 -6.1760387 -6.4202776 -6.8657417 -7.31168 -7.7321897 -7.9366207 -7.3510652 -7.2061625 -7.0412006 -7.5102725 -7.6357937][-8.9778242 -9.0313845 -8.7736378 -8.0387182 -7.3590527 -7.4030771 -7.341742 -7.5952992 -7.9492507 -8.1226215 -8.0614948 -7.5109076 -6.8166142 -6.3723316 -6.1032143]]...]
INFO - root - 2017-12-15 21:20:54.911088: step 61710, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 47h:49m:33s remains)
INFO - root - 2017-12-15 21:21:01.421557: step 61720, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 46h:59m:26s remains)
INFO - root - 2017-12-15 21:21:07.778377: step 61730, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 47h:56m:25s remains)
INFO - root - 2017-12-15 21:21:14.188609: step 61740, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 47h:51m:22s remains)
INFO - root - 2017-12-15 21:21:20.655961: step 61750, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 48h:16m:39s remains)
INFO - root - 2017-12-15 21:21:27.014441: step 61760, loss = 0.24, batch loss = 0.12 (12.8 examples/sec; 0.624 sec/batch; 46h:53m:44s remains)
INFO - root - 2017-12-15 21:21:33.422744: step 61770, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 47h:28m:24s remains)
INFO - root - 2017-12-15 21:21:39.845966: step 61780, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 47h:48m:40s remains)
INFO - root - 2017-12-15 21:21:46.186534: step 61790, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 46h:32m:56s remains)
INFO - root - 2017-12-15 21:21:52.519973: step 61800, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 49h:11m:11s remains)
2017-12-15 21:21:53.039772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0127 -4.9990063 -5.0382547 -5.1674457 -5.2097073 -4.836134 -4.1638088 -3.5199003 -2.9656034 -4.0210133 -4.721262 -5.87338 -6.4375639 -7.1097345 -8.12365][-4.6521783 -4.9039421 -5.1948061 -5.6705675 -5.7627134 -4.93087 -4.477787 -4.210762 -3.734741 -4.4532456 -4.7207532 -5.805119 -6.4651875 -7.0637431 -8.005394][-3.2399764 -3.7432623 -4.1866379 -4.8178577 -5.3654356 -4.8176622 -4.236124 -3.7777979 -3.3746853 -4.4735203 -4.7583346 -5.71659 -6.2796354 -6.9390335 -7.9849362][-2.2231436 -2.3145895 -2.566752 -3.1196747 -3.4500551 -3.0366745 -2.5176053 -2.4572258 -2.2080283 -3.511827 -4.2775126 -5.5561209 -6.1665473 -6.8384194 -7.7668996][-2.1430607 -2.0280404 -1.7775097 -1.7546458 -1.6413631 -1.1450605 -0.90051651 -1.2866426 -1.5266833 -3.2851486 -4.2360153 -5.5504985 -6.1282287 -6.7607207 -7.7243433][-3.2254434 -2.2726531 -1.4930682 -0.98363447 -0.52305746 0.36682224 0.89805412 0.55337715 0.25858498 -1.864603 -3.0444775 -4.6680393 -5.823853 -6.7455053 -7.7609887][-3.9600399 -3.0186639 -1.9911494 -0.55311918 0.48817062 1.5223732 2.2471628 1.9471931 1.4790497 -0.95306253 -2.5231833 -4.4494481 -5.7583451 -7.0272417 -8.0148592][-4.316318 -3.1734614 -1.7553792 -0.11538267 1.2213221 2.2720881 2.7633762 2.5583181 2.1061611 -0.54948664 -2.4177108 -4.4309845 -5.6938977 -6.8813004 -7.9783597][-4.9996009 -3.8546548 -2.5357385 -0.90586805 0.29943085 1.3507509 1.9265604 1.9158764 1.7094154 -0.6487155 -2.341166 -4.4203072 -5.9102597 -6.9811091 -7.8592505][-5.7258377 -4.6452122 -3.3796277 -2.02915 -1.1075721 -0.21348381 0.41471291 0.43665981 0.3820467 -1.8421869 -2.987042 -4.5746889 -5.883153 -7.0216122 -7.9989624][-6.6571436 -5.8923025 -4.923171 -3.7489107 -2.7757831 -2.0215015 -1.4717212 -1.4360418 -1.4710941 -3.4796486 -4.4086218 -5.7076526 -6.6966748 -7.4550323 -8.0379906][-7.1143808 -6.59087 -5.887157 -5.2536922 -4.5101523 -3.6093078 -3.0352192 -2.9936085 -2.9479141 -4.437861 -5.2364841 -6.3332148 -6.9703856 -7.4786434 -7.7674079][-7.5717311 -7.0827427 -6.6082735 -6.2812824 -5.8758936 -5.3308134 -4.7844677 -4.6424155 -4.4996634 -5.2151947 -5.5965514 -6.3975472 -6.7858872 -7.0617023 -7.12029][-7.575573 -7.3014731 -7.0272679 -6.7894945 -6.2970018 -5.8179669 -5.5936756 -5.5522661 -5.2041211 -5.5489292 -5.5979424 -5.8564696 -6.091136 -6.2852592 -6.2795544][-7.932549 -7.7952452 -7.7376986 -7.5330858 -6.9918551 -6.7160463 -6.7875338 -6.9613643 -7.0298762 -6.8549032 -6.5971518 -6.4396434 -6.2263589 -6.0447893 -5.8237181]]...]
INFO - root - 2017-12-15 21:21:59.486393: step 61810, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 47h:39m:18s remains)
INFO - root - 2017-12-15 21:22:05.904972: step 61820, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 47h:47m:17s remains)
INFO - root - 2017-12-15 21:22:12.278059: step 61830, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 47h:15m:50s remains)
INFO - root - 2017-12-15 21:22:18.626458: step 61840, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 49h:17m:20s remains)
INFO - root - 2017-12-15 21:22:25.055202: step 61850, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 48h:15m:15s remains)
INFO - root - 2017-12-15 21:22:31.431443: step 61860, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 49h:12m:40s remains)
INFO - root - 2017-12-15 21:22:37.824381: step 61870, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 48h:10m:34s remains)
INFO - root - 2017-12-15 21:22:44.335826: step 61880, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 47h:01m:34s remains)
INFO - root - 2017-12-15 21:22:50.775017: step 61890, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 49h:24m:13s remains)
INFO - root - 2017-12-15 21:22:57.310039: step 61900, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 49h:21m:52s remains)
2017-12-15 21:22:57.875427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4890146 -5.5192866 -5.2230129 -5.0927544 -4.9711733 -4.98425 -5.2980404 -5.0891256 -4.3450685 -4.3942647 -4.2302451 -5.4663429 -6.0542459 -6.3594069 -6.0654478][-3.789037 -3.89906 -4.1673479 -4.7343645 -5.1760836 -5.4740105 -5.7489843 -5.4212255 -4.5344524 -3.6128464 -3.4754043 -4.3511596 -5.0516195 -5.6187272 -5.3368235][-3.9562535 -4.287591 -4.6541586 -5.1090422 -5.684607 -6.270597 -6.3613214 -5.9534135 -5.021843 -4.3195925 -4.0390406 -4.5337949 -5.2619767 -5.2477932 -5.5386753][-4.3825445 -3.7057862 -4.0670805 -4.0307016 -4.0953341 -4.6301327 -4.4928703 -4.3562636 -3.7843678 -3.7887986 -4.0172606 -4.907546 -5.9318962 -6.0412283 -6.4711976][-4.1546278 -3.6019354 -3.3431978 -2.5242715 -2.5498085 -2.3478322 -1.4062185 -0.94134569 -0.51387405 -1.012362 -1.8629589 -3.7727363 -5.586585 -6.2363114 -7.219255][-2.9945464 -2.9836483 -2.8663387 -1.4425173 -0.70836306 -0.48757458 0.39308834 0.79052448 1.5406437 1.2484207 0.1009326 -2.2853241 -4.1355958 -5.4332342 -6.566916][-3.3651719 -3.1262889 -2.1160717 -0.6803174 0.46809292 1.9858809 3.1922789 2.9891443 3.2000208 2.9914608 1.2584209 -1.1753488 -3.0998511 -4.3078346 -5.3085046][-3.5062733 -3.2086134 -1.9382343 -0.460279 0.56674767 2.0157576 3.1913776 3.3277416 3.7139626 3.2995243 1.4595289 -0.84802675 -2.8220105 -3.9837687 -4.8183594][-2.9248443 -2.5929227 -2.032836 -0.99807787 0.038475513 1.5634012 2.3224163 2.5781536 2.4906311 1.5656519 0.017663479 -1.7560372 -3.4518123 -4.7453356 -5.291667][-2.761332 -2.3081932 -2.211164 -1.9975176 -0.90518093 0.17985964 0.83001041 1.7123184 1.7540636 0.76067066 -0.8899641 -2.7181091 -4.3569894 -5.3990297 -6.0318732][-4.0360785 -3.6536241 -3.3965783 -3.0020185 -2.6982017 -2.3911853 -1.9421525 -1.5988798 -1.4284372 -1.5301418 -2.7621875 -4.2517824 -5.4453554 -6.1505442 -7.0179234][-6.6665263 -5.9199648 -5.5203 -5.1101408 -4.6411228 -4.5531344 -4.0600839 -4.3375635 -4.5418186 -4.8109989 -5.0986166 -5.172338 -5.8883362 -6.6247911 -7.145164][-7.6223845 -7.8495212 -7.6869106 -7.132411 -6.639421 -6.3892193 -6.026228 -6.2296658 -6.5786252 -6.4207654 -6.3854923 -6.1008897 -6.0169382 -6.21023 -6.2090769][-7.9622111 -8.1632614 -8.8668118 -8.41503 -7.4317293 -6.9353437 -6.6915751 -6.582705 -6.466711 -6.6125116 -6.6737318 -6.6806545 -6.6507134 -5.9538584 -5.89213][-8.6045084 -8.2918873 -8.421319 -8.3581638 -8.1759014 -7.7155108 -7.1970558 -7.1049829 -7.0454063 -6.7549133 -6.8554516 -7.0847192 -6.929316 -6.4271855 -6.1089334]]...]
INFO - root - 2017-12-15 21:23:04.277795: step 61910, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 46h:49m:38s remains)
INFO - root - 2017-12-15 21:23:10.695999: step 61920, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 48h:46m:56s remains)
INFO - root - 2017-12-15 21:23:17.095281: step 61930, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 47h:24m:16s remains)
INFO - root - 2017-12-15 21:23:23.517704: step 61940, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 49h:37m:51s remains)
INFO - root - 2017-12-15 21:23:29.955496: step 61950, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 48h:40m:35s remains)
INFO - root - 2017-12-15 21:23:36.351970: step 61960, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 48h:15m:28s remains)
INFO - root - 2017-12-15 21:23:42.818100: step 61970, loss = 0.32, batch loss = 0.21 (12.1 examples/sec; 0.659 sec/batch; 49h:31m:33s remains)
INFO - root - 2017-12-15 21:23:49.215002: step 61980, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 47h:07m:18s remains)
INFO - root - 2017-12-15 21:23:55.579575: step 61990, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 46h:54m:02s remains)
INFO - root - 2017-12-15 21:24:02.028292: step 62000, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 48h:41m:50s remains)
2017-12-15 21:24:02.582850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0874085 -3.0267768 -3.0342889 -3.3894229 -3.761945 -3.4443974 -3.1467524 -2.5162597 -1.8588147 -3.2810841 -3.6546741 -5.0094771 -6.3065462 -7.0737772 -6.7898335][-2.8788271 -3.233892 -3.4904666 -3.5929494 -3.7929053 -3.6188536 -3.6459627 -3.3961573 -2.8409562 -4.0924067 -4.2763329 -5.4369011 -6.0265861 -6.8126802 -7.2355862][-3.500011 -3.3661571 -3.5296855 -3.7327945 -3.9992678 -3.707962 -3.6060615 -3.6703429 -3.4021788 -4.3365221 -4.48049 -5.5022511 -6.148406 -6.8505254 -6.9370337][-3.7264776 -3.4266868 -3.5258322 -3.7695043 -3.5888944 -3.1049042 -2.6493287 -2.6568036 -2.7080369 -3.941391 -4.4787588 -5.4285421 -5.9234457 -6.811512 -6.8560133][-3.9736915 -3.1805258 -2.8036075 -2.3680429 -2.2928681 -1.562604 -0.77618885 -0.60037518 -0.49903202 -1.744132 -2.5321217 -4.0481954 -5.3005762 -6.3104391 -6.4865761][-2.743371 -2.3285804 -2.2959003 -1.2982101 -0.24364376 0.52141953 0.825099 1.2177925 1.5117044 0.06656456 -0.87859392 -2.5452747 -3.7064059 -5.189436 -5.9571066][-3.0733194 -3.0326433 -2.6092229 -0.92471313 0.39137936 1.3686752 2.3077784 2.6651087 2.9177723 1.4681168 0.68515968 -0.93709373 -2.2426805 -4.021244 -4.7280703][-3.4058948 -3.3661423 -3.26094 -1.6361785 0.056732655 1.6141977 2.7577944 3.4415169 4.0523844 2.3098526 1.2854643 -0.23484135 -0.98534441 -2.5617695 -3.3720379][-3.643579 -3.1634984 -2.0693517 -1.2728381 -0.22203112 0.82375145 1.6685944 2.6506596 3.1418858 2.0223007 1.2293005 -0.60543537 -1.7550769 -2.8078051 -3.0461292][-4.0452442 -3.9183521 -2.7015862 -0.7181015 0.53841305 0.7057724 0.83296776 1.2825737 1.9384737 0.56232929 0.48818398 -1.1399903 -2.509871 -3.3270125 -3.8934722][-5.1230526 -4.98699 -4.1714697 -3.0319252 -1.9738617 -1.0201268 -0.61683369 -0.25740957 0.040663242 -1.4885807 -1.4910927 -2.2980914 -3.2835255 -4.1797628 -5.1159587][-6.0434685 -5.3819122 -4.9742117 -4.2891626 -4.0253019 -4.0803013 -3.9212761 -3.5718803 -3.2297297 -4.2400875 -4.2250395 -4.9429579 -5.2066927 -4.9620848 -5.1567659][-7.1040425 -6.85094 -6.5312405 -6.3731346 -6.2949443 -6.3603396 -6.4728289 -6.0361857 -5.92416 -6.1704912 -6.292798 -6.3538918 -6.3262329 -5.9439678 -5.3822713][-7.3012924 -7.3362823 -7.6754022 -7.6238775 -8.1094 -8.3090763 -8.4580812 -7.968329 -7.6307416 -7.3881235 -6.9622812 -7.0732889 -6.996491 -6.8086052 -5.9653935][-7.4713149 -6.7863917 -6.7536588 -7.8578429 -8.9979763 -9.1995344 -9.36249 -9.2217731 -8.8934383 -8.40632 -8.0993891 -7.6482525 -7.1750917 -7.0354381 -6.7785287]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:24:09.088675: step 62010, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 49h:15m:43s remains)
INFO - root - 2017-12-15 21:24:15.547172: step 62020, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 48h:22m:36s remains)
INFO - root - 2017-12-15 21:24:22.043342: step 62030, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 47h:00m:15s remains)
INFO - root - 2017-12-15 21:24:28.465074: step 62040, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.617 sec/batch; 46h:22m:59s remains)
INFO - root - 2017-12-15 21:24:34.869329: step 62050, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 47h:21m:29s remains)
INFO - root - 2017-12-15 21:24:41.307426: step 62060, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 49h:25m:21s remains)
INFO - root - 2017-12-15 21:24:47.745368: step 62070, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 48h:59m:13s remains)
INFO - root - 2017-12-15 21:24:54.066367: step 62080, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 46h:49m:12s remains)
INFO - root - 2017-12-15 21:25:00.511036: step 62090, loss = 0.34, batch loss = 0.23 (12.6 examples/sec; 0.634 sec/batch; 47h:38m:26s remains)
INFO - root - 2017-12-15 21:25:06.917732: step 62100, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 47h:22m:22s remains)
2017-12-15 21:25:07.459705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3362188 -4.0553513 -3.9735327 -4.1830292 -4.5909071 -4.6246333 -4.2968283 -3.9547477 -3.4035821 -4.6471453 -5.2101641 -6.3713965 -7.6388907 -8.4590416 -8.809433][-5.4347773 -5.0880995 -5.5197344 -5.6458025 -5.5176687 -5.5163488 -5.3257694 -4.7146664 -4.0193243 -5.2765932 -5.8163385 -6.9856377 -8.31737 -8.8628159 -8.7688875][-5.9510455 -5.7589107 -5.7497253 -5.549726 -5.186028 -4.4674559 -3.7008722 -3.2895083 -2.9848437 -4.064805 -4.5438557 -5.9766893 -7.7718968 -8.6507359 -8.7655611][-4.7602005 -4.322062 -4.3706231 -4.2861505 -3.7858429 -3.0919304 -2.2738032 -1.6321325 -1.1537247 -2.6148887 -3.4548736 -5.0677319 -6.8463058 -7.8622313 -8.1765347][-4.8683481 -4.2604322 -3.5106974 -2.9311624 -2.1005058 -1.3861098 -0.35600471 -0.13191509 -0.063443184 -1.8244205 -3.0150213 -4.9616623 -6.7128448 -7.687542 -7.8989625][-4.9757414 -4.3779688 -3.6934321 -2.4140821 -0.69543934 0.67323208 1.8413582 2.1226244 2.025444 -0.33645821 -2.1072378 -4.0931215 -6.0632071 -7.1191015 -7.2189965][-5.7649856 -4.580987 -3.3309278 -1.5923734 0.37663174 1.8745327 3.0961342 3.3999853 3.4354877 0.93675137 -0.98167419 -3.4893045 -5.4051285 -6.3723392 -6.7539835][-5.4200153 -4.3725438 -3.5723476 -1.8618913 -0.078364372 1.6456985 2.890583 3.2897005 3.436902 0.91182613 -1.0451279 -3.5969081 -5.5321531 -6.5087585 -6.5183849][-5.3194447 -4.3546391 -3.6197739 -2.4200215 -1.2565985 0.35795689 1.4153376 1.88132 2.0102215 -0.3050456 -1.7699523 -3.96979 -5.9081426 -6.7743869 -6.9169865][-6.0179663 -5.5663586 -5.0826054 -3.6874018 -2.1815958 -0.58460331 0.38701439 0.590024 0.30154705 -1.8741512 -3.12498 -5.0196824 -6.5180058 -7.51202 -7.4466734][-6.8390107 -6.3625526 -6.0407753 -5.1027346 -3.8067837 -2.2360144 -1.3193173 -1.0883322 -1.2891231 -3.1078439 -4.2440519 -5.8492064 -6.875248 -7.4544983 -7.0504069][-7.1385784 -6.923677 -6.6173439 -6.415761 -5.6685266 -4.6234941 -3.8861222 -3.4434247 -3.1822329 -4.3348255 -5.2417889 -6.2261791 -6.6409645 -7.05528 -6.803524][-7.1730242 -7.1497903 -7.0314693 -6.7459178 -6.4511104 -5.7954082 -5.0246611 -4.9093714 -4.8181348 -5.5594721 -6.0278645 -6.9494228 -7.7117038 -7.7236347 -7.1712766][-7.7565346 -7.7089977 -7.3577466 -6.8595195 -6.24839 -5.7679543 -5.7771335 -5.8978996 -5.8633032 -6.4486141 -6.7314763 -7.2632465 -7.4102054 -7.4903193 -7.4908586][-8.2138586 -7.9557009 -7.7055235 -7.6376233 -7.4781675 -7.0672174 -6.843895 -6.7774887 -6.8740807 -6.8588581 -6.6945353 -6.7761707 -6.9815063 -7.0698709 -7.0814362]]...]
INFO - root - 2017-12-15 21:25:13.979056: step 62110, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 49h:19m:16s remains)
INFO - root - 2017-12-15 21:25:20.421098: step 62120, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 48h:33m:10s remains)
INFO - root - 2017-12-15 21:25:26.922872: step 62130, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:37m:42s remains)
INFO - root - 2017-12-15 21:25:33.321711: step 62140, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 46h:49m:04s remains)
INFO - root - 2017-12-15 21:25:39.723462: step 62150, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 49h:42m:38s remains)
INFO - root - 2017-12-15 21:25:46.146207: step 62160, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 49h:16m:55s remains)
INFO - root - 2017-12-15 21:25:52.583775: step 62170, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 47h:36m:32s remains)
INFO - root - 2017-12-15 21:25:58.960556: step 62180, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 47h:50m:38s remains)
INFO - root - 2017-12-15 21:26:05.380235: step 62190, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 47h:34m:10s remains)
INFO - root - 2017-12-15 21:26:11.771773: step 62200, loss = 0.33, batch loss = 0.22 (12.1 examples/sec; 0.664 sec/batch; 49h:49m:33s remains)
2017-12-15 21:26:12.268912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8347454 -4.7359982 -4.722753 -4.7365184 -4.6253586 -4.4799795 -4.4883862 -4.6376553 -4.5537186 -5.5427566 -5.5118303 -5.5125031 -5.962265 -6.184576 -6.9495139][-4.0763645 -4.0731583 -4.2074547 -4.0571346 -3.8320742 -3.9748228 -4.030467 -4.2908983 -4.4664774 -5.702518 -5.950366 -6.4088535 -7.0622077 -7.1958542 -7.7308607][-3.9716015 -3.409626 -2.9129562 -2.6023664 -2.1810417 -2.2547302 -2.2542572 -2.4499464 -3.0345526 -4.5821028 -4.9560146 -5.42309 -6.3358588 -6.6051626 -7.1346774][-2.7570558 -1.8590322 -1.063118 -0.85796261 -0.66645575 -0.70572519 -0.63158083 -1.0564828 -1.5539269 -3.0445051 -3.4746785 -4.1821642 -5.3267 -5.8235016 -6.5096855][-1.9343476 -0.86848116 0.036013603 0.19446754 0.069973946 0.44811821 0.85097408 0.414546 -0.16227007 -1.5541725 -1.9457245 -2.7014484 -3.7892592 -4.5552292 -5.6550703][-1.5960836 -0.59447193 0.16083193 0.58772182 0.79497814 1.0616894 1.4051723 0.93928623 0.51049709 -0.86597252 -1.1003532 -1.6249461 -2.6158018 -3.5673423 -4.7248363][-1.5136995 -0.50788641 0.31400204 0.85953617 1.3445892 1.481986 1.7150068 1.576335 1.458457 0.0020370483 -0.47302961 -1.1084633 -2.2880282 -3.1650357 -4.2297239][-1.6369834 -0.64399576 0.36717033 1.3259621 1.8915405 1.8880501 2.0996532 2.0352011 2.0198193 0.63423538 0.033638 -0.95573521 -2.3707814 -3.1892447 -4.1042848][-1.9966192 -1.2841344 -0.42938662 0.26144314 0.71266365 0.77284527 0.93952656 1.1275654 1.414341 0.023875237 -0.85214949 -1.9365096 -3.2386556 -3.8230746 -4.6256189][-3.4210448 -2.8887234 -2.3188429 -1.4990182 -1.0704842 -0.75914621 -0.4691987 -0.17765379 0.2233367 -1.028625 -1.9303102 -2.609045 -3.5654187 -4.2987266 -5.3254967][-5.0248351 -4.1651793 -3.7864635 -3.451066 -2.8642864 -2.5796394 -2.3180618 -2.1692996 -1.9441085 -2.9953198 -3.5897636 -4.1512408 -4.9657474 -5.4080596 -5.6638207][-6.1546831 -5.9763288 -6.0213041 -5.9292111 -5.5599594 -5.1731243 -4.9835682 -5.1311245 -5.0302148 -5.4902372 -5.615128 -5.6350889 -5.8546972 -6.1361046 -6.1837626][-6.5914879 -6.7187128 -6.9296408 -6.8865757 -6.6268196 -6.7023144 -6.6901097 -6.4376507 -5.994956 -6.0439539 -5.7722569 -5.7508106 -5.5607862 -5.4757214 -5.3364477][-7.2672744 -7.3616157 -7.45423 -7.5181332 -7.5691476 -7.5700016 -7.323822 -7.0976524 -6.972774 -7.2449112 -7.0951147 -6.5818481 -6.0777016 -5.4708796 -4.4915528][-7.1062222 -6.8366957 -6.87705 -6.80629 -6.8526397 -7.2087975 -7.3562303 -7.406321 -7.6781797 -7.6775942 -7.5426621 -7.3453174 -7.0124664 -6.3862739 -5.5255003]]...]
INFO - root - 2017-12-15 21:26:18.629755: step 62210, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 46h:54m:34s remains)
INFO - root - 2017-12-15 21:26:25.079004: step 62220, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 47h:58m:26s remains)
INFO - root - 2017-12-15 21:26:31.486324: step 62230, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 49h:38m:54s remains)
INFO - root - 2017-12-15 21:26:37.939687: step 62240, loss = 0.28, batch loss = 0.16 (11.9 examples/sec; 0.671 sec/batch; 50h:23m:57s remains)
INFO - root - 2017-12-15 21:26:44.436099: step 62250, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 46h:37m:29s remains)
INFO - root - 2017-12-15 21:26:50.959710: step 62260, loss = 0.35, batch loss = 0.23 (12.2 examples/sec; 0.657 sec/batch; 49h:17m:16s remains)
INFO - root - 2017-12-15 21:26:57.449571: step 62270, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 48h:46m:21s remains)
INFO - root - 2017-12-15 21:27:03.889193: step 62280, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 48h:39m:38s remains)
INFO - root - 2017-12-15 21:27:10.416685: step 62290, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 49h:01m:44s remains)
INFO - root - 2017-12-15 21:27:16.864025: step 62300, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:35m:57s remains)
2017-12-15 21:27:17.425855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3050728 -3.9286563 -4.6375618 -5.08924 -4.9923229 -5.1893263 -5.7516623 -6.0354209 -5.7175045 -6.1358538 -6.0627847 -6.8634615 -7.3683538 -7.3393717 -5.6460586][-3.3502421 -3.105257 -3.2792034 -3.8634496 -3.7994888 -3.8818266 -4.6025753 -5.1819057 -5.5228567 -6.2630253 -6.3478675 -6.7200346 -6.8199105 -7.0117545 -5.2201118][-1.2855248 -2.2133994 -2.8563771 -3.3525658 -2.6110182 -2.4035296 -3.0970821 -3.67694 -4.3682346 -5.6272411 -5.6296992 -6.4782753 -6.7928429 -7.7585053 -6.7457075][-3.6276722 -2.9957871 -2.9695258 -3.5636106 -3.1905751 -3.1178045 -2.7568326 -2.6118927 -3.2353573 -4.4095707 -4.8292894 -6.3818955 -7.1211376 -8.4891968 -7.8906622][-3.8894589 -4.0696383 -3.6904364 -2.9353886 -1.7014513 -0.71643448 -0.61360407 -0.58361816 -1.0110016 -1.9370255 -2.0530062 -3.7182534 -5.3715067 -7.3125319 -7.3326163][-4.8987904 -4.9056592 -4.5734367 -3.0998774 -2.0864325 -1.2969532 0.20820475 1.3843012 1.1903543 0.54219151 0.42636395 -1.1659236 -2.619957 -4.9317465 -5.9582796][-4.7721004 -4.6114407 -4.2293715 -2.5248728 -1.2264972 0.52315331 1.7563763 1.9480906 1.9579258 1.8619547 1.5949059 0.44148064 -1.6347737 -3.4878521 -3.7743456][-4.28271 -3.7080054 -3.0079613 -1.325202 0.26898432 2.5100718 4.5686808 4.535881 4.5689 2.8247261 1.8924484 1.4930754 -0.30538464 -2.3765702 -3.0376825][-4.7965136 -4.3872032 -3.6032715 -2.3537302 -0.36221838 2.1456671 3.3054552 4.9520826 6.0328579 3.6917915 2.3844109 0.32169437 -1.7442527 -3.5269012 -3.6003928][-5.6980715 -5.5527334 -5.2447195 -3.869241 -2.0584211 -0.66346884 0.67192364 2.5403881 3.3025942 2.2992496 1.1245871 -0.91062117 -2.9678431 -4.8835669 -5.4326067][-7.4614472 -7.2833047 -6.8294315 -5.6626949 -4.2704268 -2.7855868 -1.9436417 -0.94149828 0.34117508 0.16920185 0.040147781 -2.1876621 -4.1792736 -5.6909704 -6.4509573][-7.9158297 -7.8347836 -8.0691271 -7.6536541 -6.8651457 -5.3456869 -4.7324581 -3.9837425 -3.0426378 -3.1831532 -2.8467269 -3.660892 -4.2379937 -5.7531958 -6.6960158][-8.2106485 -7.83219 -8.3266 -8.508482 -7.6455417 -7.0683403 -6.8216119 -6.0364804 -5.3247313 -5.0515146 -5.0376339 -5.26582 -4.8636971 -5.5989876 -6.4178443][-8.7030535 -8.1213417 -7.6466117 -7.6429691 -7.8168879 -7.220448 -7.1416755 -7.180068 -6.7728705 -6.8287015 -6.5802431 -6.6578722 -6.4189939 -5.6451845 -5.3358741][-8.8015766 -8.972024 -9.1175165 -8.5574675 -7.5557704 -7.1728597 -7.5962539 -7.7188835 -7.5673275 -7.5393262 -7.3684711 -7.4267831 -6.8915329 -6.9071507 -6.47767]]...]
INFO - root - 2017-12-15 21:27:23.852090: step 62310, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 48h:11m:34s remains)
INFO - root - 2017-12-15 21:27:30.334681: step 62320, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 48h:01m:29s remains)
INFO - root - 2017-12-15 21:27:36.675836: step 62330, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 46h:13m:18s remains)
INFO - root - 2017-12-15 21:27:43.064564: step 62340, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 46h:30m:58s remains)
INFO - root - 2017-12-15 21:27:49.453347: step 62350, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 46h:17m:51s remains)
INFO - root - 2017-12-15 21:27:55.898035: step 62360, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 47h:22m:57s remains)
INFO - root - 2017-12-15 21:28:02.367563: step 62370, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 47h:05m:07s remains)
INFO - root - 2017-12-15 21:28:08.756641: step 62380, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 49h:18m:34s remains)
INFO - root - 2017-12-15 21:28:15.174513: step 62390, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 48h:07m:44s remains)
INFO - root - 2017-12-15 21:28:21.518182: step 62400, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:54m:09s remains)
2017-12-15 21:28:22.060073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6035237 -4.6686573 -5.5643392 -5.4303818 -5.2951088 -4.8426061 -4.0236139 -3.5850582 -3.4681139 -4.360199 -6.0837193 -9.6895218 -10.580942 -10.716535 -11.610099][-2.5346518 -4.1408715 -5.5375614 -6.4818773 -7.2100892 -6.5730805 -5.4713717 -4.7966146 -4.1422062 -4.4779868 -5.9497185 -9.13578 -10.623746 -11.252578 -11.76073][-2.7444463 -3.2599883 -4.4743223 -5.6824403 -6.5154257 -6.3567257 -5.6723433 -4.9443827 -4.2049642 -4.5158033 -5.829865 -8.4178715 -9.52653 -10.117458 -11.022896][-3.3520641 -3.2109122 -3.8988783 -4.1570482 -4.0285568 -3.2310452 -2.5771418 -2.2165928 -1.6687751 -2.8351345 -4.7096424 -7.9915881 -9.3115559 -9.4951286 -10.18684][-4.1959419 -3.4977536 -3.2811427 -2.5823808 -2.1900086 -0.951385 1.0770817 1.9003487 1.3606262 -0.4958868 -2.8324142 -6.7928433 -8.7154083 -9.1910706 -10.126925][-4.287466 -3.9883389 -3.4947195 -2.1407371 -0.64597178 1.8221359 3.756978 4.2404175 4.3400364 2.0341721 -1.2437654 -5.2381067 -7.5496321 -8.7111073 -9.1332541][-4.0362825 -3.515595 -2.9500518 -1.8062873 0.20349407 2.8301382 5.0279388 6.1671658 6.2760162 3.8874035 0.19501019 -4.883934 -7.1939459 -8.0353613 -8.8837051][-4.19409 -3.6172867 -2.5646257 -1.222578 0.42498589 2.9116659 4.8309727 5.7955561 6.1368608 3.9102697 0.80381393 -3.8198998 -6.519248 -7.8504887 -8.60488][-4.3591232 -3.5257392 -3.0133734 -2.0316753 -0.77868938 1.4438047 3.048111 4.111249 4.4007072 2.1592751 -0.65618467 -4.8099051 -6.5427256 -7.5690756 -8.6454535][-5.1103945 -4.1046162 -3.2907324 -2.8023748 -2.0525193 -1.0638652 -0.58433867 0.43670464 1.2881346 -0.63166952 -2.9611082 -6.313096 -8.0309944 -8.3286352 -8.8191481][-6.9868531 -6.5490313 -5.9145083 -4.7461185 -3.8698814 -3.2514582 -3.0187941 -3.0447049 -3.2396832 -4.5653753 -6.09211 -8.1525173 -8.9506989 -9.0180874 -9.43869][-8.2239189 -7.7639666 -7.4332514 -6.959919 -6.4946003 -5.8832574 -5.3869209 -5.4540534 -5.6999912 -6.2651348 -7.4652328 -8.5057678 -9.0037146 -9.3868647 -9.3804417][-9.0293779 -8.4916515 -7.7416224 -7.2538533 -7.0877309 -6.9701114 -6.9885716 -6.7325392 -6.3753543 -6.2515779 -6.9532084 -7.5789781 -7.5398755 -7.9668689 -8.0349131][-8.4666157 -8.8249788 -8.6212072 -7.9100275 -7.2896328 -7.0367184 -7.1185179 -6.882864 -6.0964689 -5.912158 -5.8974514 -5.2941628 -5.6687765 -6.38841 -6.5696297][-7.6357546 -8.2923021 -9.0059338 -9.2404966 -8.8034191 -8.3266773 -7.88964 -7.8048639 -7.824358 -7.0773411 -6.1601448 -5.62258 -5.5530243 -5.9108443 -5.9897723]]...]
INFO - root - 2017-12-15 21:28:28.526499: step 62410, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 47h:14m:50s remains)
INFO - root - 2017-12-15 21:28:34.859978: step 62420, loss = 0.24, batch loss = 0.13 (13.0 examples/sec; 0.615 sec/batch; 46h:06m:30s remains)
INFO - root - 2017-12-15 21:28:41.290684: step 62430, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 47h:38m:14s remains)
INFO - root - 2017-12-15 21:28:47.693905: step 62440, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 47h:11m:45s remains)
INFO - root - 2017-12-15 21:28:54.151302: step 62450, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 49h:06m:23s remains)
INFO - root - 2017-12-15 21:29:00.505538: step 62460, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 48h:18m:18s remains)
INFO - root - 2017-12-15 21:29:06.901639: step 62470, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 49h:01m:45s remains)
INFO - root - 2017-12-15 21:29:13.280023: step 62480, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:55m:37s remains)
INFO - root - 2017-12-15 21:29:19.718722: step 62490, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 48h:39m:20s remains)
INFO - root - 2017-12-15 21:29:26.185813: step 62500, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 48h:48m:17s remains)
2017-12-15 21:29:26.729014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1914825 -3.9273286 -4.8116741 -4.728549 -4.168736 -3.8074193 -4.0899444 -3.8262815 -3.4961467 -3.2764349 -4.0007396 -5.8416076 -6.7157078 -6.5417681 -7.5254879][-3.5860567 -4.4534044 -5.293664 -5.3820739 -5.2375588 -4.3957443 -4.0732107 -4.2099323 -4.2500467 -4.2269983 -4.9320712 -6.4266558 -7.256855 -7.19612 -7.8446727][-3.813237 -4.2229223 -4.7637024 -4.8995533 -4.8577547 -4.6134868 -4.3036671 -3.9945147 -3.5235929 -3.790164 -4.9235497 -6.6919785 -7.4609985 -7.6532884 -8.4105663][-4.4218788 -4.8777142 -5.0781536 -4.5989189 -4.139082 -4.0190816 -3.4094825 -2.9098086 -2.4906611 -2.5120606 -3.8468046 -5.9742279 -7.1607771 -7.4221997 -7.9235458][-5.3801808 -5.6973038 -5.6647978 -5.17935 -4.1864882 -3.3346238 -2.2129388 -1.6804733 -1.2460241 -1.8109903 -3.5569406 -5.8293691 -6.9741049 -7.3750567 -8.1518126][-7.1784444 -6.8105516 -6.1234589 -5.0470781 -3.5388064 -2.3126631 -0.61198044 0.28225422 0.41120911 -1.0246415 -3.4844728 -6.2793579 -7.2408333 -7.2080317 -7.7327585][-6.7993956 -6.6051779 -6.158721 -4.6208463 -2.4345226 -0.026532173 2.0957804 2.8253002 2.8942528 1.5530758 -1.3007364 -4.7060428 -6.3970823 -6.4726486 -7.0830555][-6.2867908 -5.9214883 -5.2185059 -3.8122344 -1.8006611 1.1173391 3.5742989 3.8039379 3.2315025 1.9098606 -0.24049616 -3.1880107 -4.7468748 -5.073288 -5.784359][-6.09122 -5.7433486 -5.2585411 -3.9182246 -2.0826688 0.58543396 3.078105 3.4384851 3.0518827 1.2587805 -1.24717 -3.9000244 -5.3265586 -5.29521 -6.1236367][-6.128582 -6.1460934 -5.7041788 -4.6506643 -3.2112346 -1.1988549 0.54090977 0.94818115 0.86203289 -0.69996786 -2.9764619 -5.5659981 -6.5546422 -5.8873167 -6.671669][-7.1027007 -7.0228381 -6.6159563 -5.6387167 -4.5373573 -3.3084278 -2.245646 -1.8790507 -1.5851049 -2.4767866 -3.716913 -5.8942413 -7.0669165 -6.39733 -6.6509452][-7.3495417 -7.2379251 -6.8758035 -6.0630684 -5.3102417 -4.6870127 -3.9924088 -3.8139262 -3.7361355 -4.0899668 -4.3060408 -5.80633 -6.4366217 -6.2177014 -6.5977015][-8.7724142 -8.2501259 -7.7803645 -6.919395 -6.1335316 -6.1449676 -5.9658718 -5.8365512 -5.6047573 -6.3234386 -6.4376822 -6.5668535 -6.5606818 -6.6021295 -6.8761148][-8.4078293 -8.0584116 -7.4276752 -6.652348 -5.9542885 -5.9424667 -5.6570482 -5.7204762 -5.6753554 -6.1866913 -6.3830242 -6.5591092 -6.5012836 -6.7269325 -6.9122868][-9.236886 -9.1223354 -8.3210506 -7.5654521 -6.9207931 -7.040761 -7.3260202 -7.393424 -7.061069 -6.9665046 -7.0763774 -6.659193 -6.3583169 -6.0315061 -5.7684183]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 21:29:34.077206: step 62510, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 48h:51m:35s remains)
INFO - root - 2017-12-15 21:29:40.429854: step 62520, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 48h:33m:11s remains)
INFO - root - 2017-12-15 21:29:46.830475: step 62530, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 47h:20m:06s remains)
INFO - root - 2017-12-15 21:29:53.195008: step 62540, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 48h:02m:36s remains)
INFO - root - 2017-12-15 21:29:59.563778: step 62550, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 48h:43m:47s remains)
INFO - root - 2017-12-15 21:30:05.921498: step 62560, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 47h:56m:35s remains)
INFO - root - 2017-12-15 21:30:12.467613: step 62570, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 48h:15m:56s remains)
INFO - root - 2017-12-15 21:30:18.875384: step 62580, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 46h:42m:39s remains)
INFO - root - 2017-12-15 21:30:25.319880: step 62590, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:00m:27s remains)
INFO - root - 2017-12-15 21:30:31.772926: step 62600, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 48h:07m:41s remains)
2017-12-15 21:30:32.318373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4863091 -6.6985478 -7.5953579 -7.9352064 -8.2360735 -8.0753326 -7.1187949 -5.8144159 -4.677917 -6.0181112 -6.1786785 -6.1945691 -6.3116341 -7.5944047 -8.0088673][-5.835155 -6.9560847 -7.8159609 -8.8683252 -9.42577 -9.3083925 -8.7566175 -7.5958352 -6.2722993 -7.4918246 -7.5273719 -7.4280019 -7.9849358 -9.0418348 -9.0731392][-5.0292749 -5.8285689 -6.884922 -7.7128773 -8.2944651 -8.6023016 -8.6310177 -7.8193145 -6.9018168 -8.4471464 -7.989995 -7.86954 -7.8687177 -9.1450539 -9.6557045][-6.6509037 -6.4318562 -6.638463 -6.7582588 -6.9862709 -6.5693011 -5.6438475 -5.0865612 -4.6649275 -6.4860926 -6.7692018 -7.3419995 -8.0059042 -9.12013 -9.12813][-7.44674 -7.1303635 -6.82137 -5.8019104 -5.4666395 -4.2431316 -3.0994964 -2.1855078 -1.675199 -4.2428951 -5.2818232 -6.216342 -7.11872 -8.4624138 -9.1612225][-9.0011034 -8.1445665 -7.2088184 -5.2656827 -3.2121377 -1.4306064 0.25165367 1.0765772 1.2185659 -1.5401788 -3.0275311 -4.3449535 -5.6478782 -7.2973523 -8.1470089][-9.2608852 -8.18798 -6.4954166 -3.5544219 -0.85437965 2.1052589 4.1686869 4.7737646 4.9100523 1.1647549 -1.2249627 -3.0876832 -4.6388683 -6.1692495 -6.7147117][-8.524765 -7.5422816 -6.1833272 -3.1585355 0.24639034 3.0360823 5.284339 6.5937948 6.5409851 2.0633583 -0.99966049 -3.4660368 -4.8451862 -6.0668545 -6.4459624][-9.0586405 -8.5846748 -7.7581177 -4.9766178 -2.2437325 0.37303638 2.7526245 3.8141365 4.134038 0.55247974 -1.8373938 -4.1143646 -5.3842068 -6.2614446 -6.0168324][-10.464857 -9.8740053 -8.7519264 -6.2079387 -3.5140939 -1.3124714 0.3923912 1.1516848 1.1052608 -2.2896128 -4.2151995 -5.9081583 -6.8836117 -7.6972818 -7.3964934][-11.901014 -11.520473 -10.978565 -9.27072 -7.41263 -5.0894251 -3.272665 -3.1824541 -3.5286818 -5.972477 -7.1735373 -8.1370764 -8.3309021 -9.3498735 -9.260788][-12.160946 -11.348466 -10.633945 -9.8716869 -9.1765375 -7.6552572 -6.6610241 -5.9276962 -5.5982027 -7.8149829 -8.6212482 -8.9789324 -8.9633083 -9.3405609 -9.0741787][-12.411837 -11.941181 -11.399557 -10.402861 -9.1837358 -8.08228 -7.5697155 -7.4188728 -7.5181804 -9.1046362 -9.7763071 -9.8252544 -9.584197 -9.3672771 -8.7065611][-10.039309 -9.8387108 -9.6277657 -8.9551945 -8.2063913 -7.0958185 -6.4455833 -6.4443431 -6.75432 -8.0336752 -8.6644154 -9.0655241 -9.49382 -9.3926945 -9.1713057][-9.2389069 -8.83828 -8.5804644 -8.2687979 -7.9431038 -7.3876877 -7.0347328 -7.1700916 -7.4265842 -7.4161067 -7.6118493 -7.8104262 -7.9845781 -8.2153082 -8.1010628]]...]
INFO - root - 2017-12-15 21:30:38.718136: step 62610, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 47h:16m:42s remains)
INFO - root - 2017-12-15 21:30:45.168031: step 62620, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 47h:43m:58s remains)
INFO - root - 2017-12-15 21:30:51.585744: step 62630, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 47h:31m:11s remains)
INFO - root - 2017-12-15 21:30:57.979441: step 62640, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 47h:02m:32s remains)
INFO - root - 2017-12-15 21:31:04.282713: step 62650, loss = 0.26, batch loss = 0.14 (13.1 examples/sec; 0.612 sec/batch; 45h:54m:09s remains)
INFO - root - 2017-12-15 21:31:10.597461: step 62660, loss = 0.32, batch loss = 0.20 (12.9 examples/sec; 0.622 sec/batch; 46h:37m:39s remains)
INFO - root - 2017-12-15 21:31:17.077561: step 62670, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 49h:23m:30s remains)
INFO - root - 2017-12-15 21:31:23.450887: step 62680, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 48h:24m:18s remains)
INFO - root - 2017-12-15 21:31:29.923306: step 62690, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 48h:26m:56s remains)
INFO - root - 2017-12-15 21:31:36.284976: step 62700, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 46h:58m:16s remains)
2017-12-15 21:31:36.790017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7580695 -3.449883 -3.8854687 -3.5739164 -3.2766805 -3.4074955 -4.0585852 -4.0467596 -3.8053071 -5.1879444 -5.3844118 -6.6422219 -7.4653492 -8.6025658 -9.5524826][-4.5444202 -4.3614845 -4.2553539 -4.3531008 -4.7178493 -4.809597 -5.1265292 -5.6467423 -5.9636149 -7.180521 -7.1047029 -7.9989915 -8.750412 -9.8237419 -10.587294][-4.5213203 -4.1510234 -3.7553689 -3.6643596 -3.5823879 -3.4617219 -3.9234903 -4.532022 -4.8074532 -6.1488976 -6.4751935 -7.5849867 -8.5374451 -9.67234 -10.157866][-3.20468 -2.7492085 -2.657166 -2.6227555 -2.6712351 -2.8034387 -2.9232092 -3.2755718 -3.3991194 -4.7112021 -4.921102 -5.9553871 -7.0230041 -8.2391205 -8.8115978][-2.6165204 -2.564837 -2.5959735 -2.5899215 -2.4140582 -2.1100903 -1.7191634 -1.7344265 -1.722331 -2.7139635 -3.1200252 -4.5753031 -5.8042312 -6.9892859 -7.8086004][-2.9660306 -2.303308 -1.8078761 -2.0367794 -2.0875349 -1.3959227 -0.45991373 0.045297146 0.415802 -0.22048426 -0.56428242 -1.9187088 -3.3699832 -4.6387434 -5.3048873][-2.8629708 -2.5285897 -1.9650087 -1.5188842 -1.102447 -0.66449928 -0.067540169 0.96516609 1.6596155 0.73965836 0.14909792 -1.352767 -2.7845411 -4.2882605 -4.77906][-2.5897698 -2.4549346 -2.5937548 -2.03091 -1.0148163 -0.32301569 0.34650421 0.88244247 1.4991655 0.90096378 0.66757679 -0.8105669 -2.0999722 -3.2654314 -3.95581][-2.927618 -2.2342658 -1.859448 -1.447711 -0.9514327 0.43416023 1.3113947 1.1639175 1.1047258 0.43800831 0.14697409 -1.3102617 -2.5311761 -3.5738692 -4.1076946][-2.9736428 -2.7723074 -2.3558736 -1.5165319 -0.88176918 0.37627792 1.2957535 1.1498594 1.0002594 -0.5769577 -1.0011992 -2.5610261 -3.6869464 -4.9019337 -5.1678171][-4.8186741 -4.0115681 -3.2293844 -2.6025267 -2.0285206 -0.90651178 -0.49029779 -0.44026136 -0.44592142 -2.1537127 -2.9467988 -4.1557846 -4.8395033 -5.6366625 -5.98335][-5.3249197 -4.747046 -4.5817003 -3.7109497 -3.261867 -2.1744237 -1.4902167 -1.9577198 -2.3928876 -3.5741348 -3.9910851 -5.0473518 -5.3348379 -5.6793842 -5.8569641][-5.700726 -5.2235155 -4.8204417 -4.2392368 -3.8227656 -3.1710486 -2.9175439 -3.3680878 -3.6513963 -4.5372119 -4.6466236 -5.4313221 -5.661315 -5.8443041 -5.4811144][-5.8096361 -5.0277748 -4.4531956 -4.1098108 -3.8393161 -3.2401233 -3.0565772 -3.3095784 -3.7634978 -4.7733107 -5.1092987 -5.1435742 -5.1080866 -5.4816155 -5.4832811][-6.9454045 -6.7094269 -6.6435251 -6.2011251 -5.73005 -5.1761041 -4.8607092 -5.0530949 -5.2193203 -5.3228951 -5.6903753 -5.5938148 -5.6275549 -5.3983784 -5.1896219]]...]
INFO - root - 2017-12-15 21:31:43.255529: step 62710, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 48h:53m:51s remains)
INFO - root - 2017-12-15 21:31:49.647587: step 62720, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 47h:38m:00s remains)
INFO - root - 2017-12-15 21:31:56.103123: step 62730, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 47h:50m:55s remains)
INFO - root - 2017-12-15 21:32:02.568181: step 62740, loss = 0.35, batch loss = 0.23 (12.9 examples/sec; 0.619 sec/batch; 46h:22m:30s remains)
INFO - root - 2017-12-15 21:32:09.044346: step 62750, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:59m:30s remains)
INFO - root - 2017-12-15 21:32:15.499130: step 62760, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 49h:01m:09s remains)
INFO - root - 2017-12-15 21:32:21.937586: step 62770, loss = 0.29, batch loss = 0.17 (11.9 examples/sec; 0.675 sec/batch; 50h:34m:16s remains)
INFO - root - 2017-12-15 21:32:28.331374: step 62780, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 48h:04m:57s remains)
INFO - root - 2017-12-15 21:32:34.806308: step 62790, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 47h:10m:37s remains)
INFO - root - 2017-12-15 21:32:41.206016: step 62800, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 48h:07m:06s remains)
2017-12-15 21:32:41.698669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0113368 -1.2949009 -1.6202517 -2.2008514 -2.8732204 -3.4898462 -3.8408227 -4.3342967 -4.6478062 -5.2890835 -5.6935053 -6.6526923 -7.1771231 -7.7803054 -8.4968271][-1.3970909 -1.5932722 -2.2242723 -2.7692413 -3.0160332 -3.293335 -3.6216311 -3.9646113 -4.4585037 -5.5274305 -6.3280573 -7.5131593 -7.7795568 -8.1333542 -8.6817551][-2.5349207 -2.3341718 -2.3043866 -2.2304883 -2.1630869 -2.0828943 -2.0816231 -2.4815769 -2.9295897 -3.9049962 -4.8982754 -6.6682978 -7.7154822 -8.24505 -8.9840479][-3.4503174 -3.541285 -3.18753 -2.3274188 -1.682322 -1.1617622 -0.8434062 -0.86734772 -0.98807669 -2.3890176 -3.4919171 -5.4025745 -6.4482937 -7.0412207 -8.3139429][-4.1211228 -3.4816828 -2.8108535 -2.3723378 -1.8589787 -0.883667 -0.13109732 0.051087856 0.23304129 -0.9465127 -2.0469785 -4.3068409 -5.8522177 -6.3534493 -7.2725248][-4.0852385 -3.460196 -3.1422267 -1.9170732 -0.74844408 0.19771099 1.1565619 1.2342024 1.1315794 -0.18950891 -1.3888006 -3.7326193 -5.2668333 -5.8803482 -6.7909594][-3.3692842 -2.805788 -2.0829744 -0.71773052 0.57091522 1.5189953 2.4789743 2.6510324 2.8502026 1.2423296 -0.64670181 -3.3412633 -4.7914114 -5.7668285 -6.9972239][-3.4742441 -2.7722998 -1.9232588 -0.10104227 1.6137285 2.5258818 3.4439697 3.4300461 3.4126511 1.8146963 0.22029114 -2.8881645 -4.6831903 -5.6475964 -7.025342][-3.9479589 -3.3151231 -2.4355016 -0.75386715 0.64303684 1.5429163 2.5271568 3.0606785 3.4850302 1.4188604 -0.91757107 -3.2134418 -4.56678 -6.1147957 -7.5569849][-5.2787752 -4.6378274 -4.1853733 -3.092052 -1.776576 -0.71453094 0.53975487 1.1003666 1.6408253 -0.12002707 -1.9409332 -4.5785942 -6.1473107 -6.6574006 -7.7811809][-6.7599187 -6.5473356 -6.239562 -5.2889891 -4.4916587 -3.2933927 -1.9424663 -1.7502484 -1.483829 -3.1378183 -4.5260134 -5.6023617 -6.5035105 -7.2915053 -8.3872156][-7.6343675 -7.4497895 -7.4807334 -6.9059763 -6.3507223 -5.6821957 -4.797051 -4.2891669 -3.6837721 -4.473423 -4.8687634 -6.8128633 -7.767024 -7.4586005 -8.2821569][-8.349391 -8.24141 -8.1045647 -7.6413078 -7.4644113 -6.9831095 -6.0380487 -6.0155053 -5.8855233 -6.061974 -6.1374393 -6.4258242 -6.9476428 -7.3926816 -7.907433][-8.3462143 -8.1093636 -7.5989571 -7.2181711 -7.1076355 -6.8250184 -6.1257153 -6.1593714 -6.1689196 -6.4257522 -6.6103668 -6.9365153 -6.808146 -6.2492542 -6.6704116][-9.2916136 -8.8691254 -8.5198421 -8.1439734 -7.8934712 -7.5790429 -6.8746305 -7.056766 -7.3326907 -7.4621897 -7.4323692 -7.1731119 -6.8299489 -6.3907108 -6.0809488]]...]
INFO - root - 2017-12-15 21:32:48.118461: step 62810, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 48h:45m:06s remains)
INFO - root - 2017-12-15 21:32:54.539582: step 62820, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 48h:35m:48s remains)
INFO - root - 2017-12-15 21:33:00.994555: step 62830, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 50h:03m:39s remains)
INFO - root - 2017-12-15 21:33:07.404920: step 62840, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 47h:30m:03s remains)
INFO - root - 2017-12-15 21:33:13.909580: step 62850, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 47h:21m:05s remains)
INFO - root - 2017-12-15 21:33:20.252018: step 62860, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 48h:00m:23s remains)
INFO - root - 2017-12-15 21:33:26.646501: step 62870, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 48h:39m:16s remains)
INFO - root - 2017-12-15 21:33:32.978047: step 62880, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 48h:24m:43s remains)
INFO - root - 2017-12-15 21:33:39.383920: step 62890, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 48h:17m:06s remains)
INFO - root - 2017-12-15 21:33:45.787475: step 62900, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 48h:47m:46s remains)
2017-12-15 21:33:46.331463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8660927 -4.7936096 -4.6170359 -4.2637053 -3.8263373 -3.515131 -3.1009445 -2.5253983 -2.0669484 -3.9152021 -4.9320545 -5.7093439 -6.35123 -6.5424819 -7.4647374][-4.6715488 -4.8769655 -4.9427643 -4.7603445 -4.383502 -3.94331 -3.2283158 -2.6895647 -2.2171025 -4.1362 -4.8288984 -5.6389689 -6.6597695 -6.9734688 -7.9443421][-2.9640298 -3.3040314 -3.529716 -3.714844 -3.5939956 -3.6260543 -3.0247326 -2.5350132 -2.2042422 -4.0331688 -4.88739 -5.4803753 -6.3385758 -6.6937003 -7.7635984][-1.8001132 -1.9531236 -2.0678105 -1.9917822 -1.6864519 -1.4789767 -1.2076917 -1.3344202 -1.4066901 -3.489635 -4.5387993 -5.51784 -6.57198 -6.7692308 -7.595633][-1.5860467 -1.1220212 -0.53522921 -0.48443937 -0.26996851 0.256073 0.57006741 0.20507336 -0.32631636 -3.0297499 -4.2778349 -5.5658755 -6.7763934 -7.02826 -7.7043962][-2.3901076 -1.4621892 -0.39843893 0.25114679 0.83664036 1.6172895 2.3979864 2.0825891 1.4787436 -1.5111785 -3.1344085 -4.7578812 -6.3750944 -7.0220342 -8.05627][-2.6561656 -1.6868057 -0.48154783 0.79456139 1.9050827 2.852994 3.7151318 3.3697138 2.895853 -0.18705893 -2.2367673 -4.101099 -6.0006089 -6.9192896 -8.0708942][-3.2396922 -2.0814013 -0.59673262 1.2281322 2.5029755 3.3045702 4.1064529 3.8164396 3.3710327 0.33306217 -1.6775818 -3.8032241 -5.908174 -6.8142171 -7.9304504][-4.1734781 -2.9535823 -1.4943905 0.29350281 1.3209171 2.1331549 2.7641573 2.7662373 2.6597233 -0.11014557 -1.9130712 -3.7849102 -5.7982988 -6.8958759 -7.9001346][-5.056344 -3.9173238 -2.7832918 -1.0070891 -0.12059927 0.39817524 1.0481844 1.2317648 1.2118979 -1.4731493 -2.7981043 -4.2016373 -5.8434091 -6.7244873 -7.6472864][-6.4015636 -5.6264629 -4.4717407 -3.1345048 -2.1959438 -1.6662812 -1.2029686 -1.0076003 -0.867568 -3.5457902 -4.9098186 -5.7223988 -6.9344525 -7.4468508 -8.0797272][-7.1662745 -6.5113444 -5.7290926 -4.7430735 -3.9235549 -3.350317 -2.9110007 -2.7727509 -2.8062353 -4.7009416 -5.6115227 -6.3518748 -7.2386746 -7.6755619 -8.1607189][-7.5867209 -6.924212 -6.3094015 -5.8605251 -5.4546242 -4.8410807 -4.5417328 -4.388752 -4.2546053 -5.8375163 -6.3628736 -6.3896923 -6.8272896 -7.1593504 -7.5178075][-7.6171184 -7.3424029 -7.1559324 -6.8519244 -6.38179 -5.8840289 -5.5692348 -5.5016818 -5.2383366 -5.8383865 -6.2403936 -6.4717364 -6.4550867 -6.4877863 -6.6207542][-7.7823563 -7.6099644 -7.3918724 -7.3901558 -7.0955682 -6.7698059 -6.9487486 -6.9756861 -6.7271132 -6.5291786 -6.3339815 -6.3478417 -6.2689414 -6.048357 -5.883532]]...]
INFO - root - 2017-12-15 21:33:52.734411: step 62910, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 48h:46m:56s remains)
INFO - root - 2017-12-15 21:33:59.131277: step 62920, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:31m:22s remains)
INFO - root - 2017-12-15 21:34:05.414682: step 62930, loss = 0.32, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 46h:58m:40s remains)
INFO - root - 2017-12-15 21:34:11.794092: step 62940, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 47h:46m:23s remains)
INFO - root - 2017-12-15 21:34:18.165540: step 62950, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 46h:23m:20s remains)
INFO - root - 2017-12-15 21:34:24.666248: step 62960, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 48h:42m:45s remains)
INFO - root - 2017-12-15 21:34:31.051201: step 62970, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:31m:24s remains)
INFO - root - 2017-12-15 21:34:37.383848: step 62980, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 45h:58m:52s remains)
INFO - root - 2017-12-15 21:34:43.941512: step 62990, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:53m:56s remains)
INFO - root - 2017-12-15 21:34:50.421340: step 63000, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 48h:08m:29s remains)
2017-12-15 21:34:50.989286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1971655 -3.47149 -2.9814425 -2.5024848 -2.0042725 -1.661314 -1.5344896 -1.6463308 -1.655663 -0.85055256 -2.4321132 -5.0019684 -5.8338046 -7.6294022 -8.4441051][-3.9590859 -4.035285 -3.5862389 -3.190042 -3.1503458 -2.9396124 -2.661377 -2.5430498 -2.3625236 -1.6020436 -3.1417584 -5.794219 -6.2546539 -7.6866527 -8.0652275][-3.10386 -2.8702574 -2.4411106 -2.3439217 -2.1961222 -2.1165142 -2.385489 -2.5146766 -2.401114 -1.6980329 -2.9343867 -5.1610708 -5.6393671 -6.9703045 -7.2006269][-1.9278798 -1.4824438 -1.1676784 -1.1187558 -1.2699022 -1.4406381 -1.4082556 -1.5826316 -1.8381157 -1.643189 -3.1156502 -5.047451 -5.4669361 -6.52821 -6.6441879][-1.5145164 -0.66765404 -0.14905643 -0.12216091 -0.52277136 -0.56756544 -0.52373743 -0.84467936 -0.78615141 -0.70476627 -2.2105174 -3.6942263 -4.3079548 -5.3203821 -5.6961403][-1.043489 -0.40332508 0.098860741 0.10536814 -0.030924797 0.39670277 0.776103 0.7311945 0.95812225 0.93195438 -0.57377148 -2.0667539 -3.1800246 -4.427783 -5.5507936][-1.9506135 -0.89635515 0.098786354 0.30949879 0.36779308 0.61229229 0.94018555 1.2095051 1.613719 1.7673216 0.36162949 -1.3263702 -2.7723832 -4.1134877 -5.177886][-2.4088593 -1.439712 -0.65122557 -0.1832037 0.214077 0.7061758 1.1741381 1.4385519 1.8561544 2.1142416 1.072876 -0.58727741 -1.7045989 -2.805491 -3.7885811][-3.5394502 -2.3473496 -1.5639753 -0.80526257 -0.27683163 0.29987049 0.94319916 1.3650541 1.7857695 1.7971144 0.82366943 -0.80342293 -1.6855173 -2.8559737 -3.9842603][-4.8279839 -3.571682 -2.6997337 -2.3179021 -1.6421051 -1.01577 -0.27001572 0.37681389 1.0785713 0.91907692 -0.086466789 -1.7391109 -3.0144658 -3.8690777 -4.7672224][-7.4200759 -6.5446944 -5.6535397 -4.8348312 -3.6895566 -3.025619 -2.439909 -2.0455446 -1.6748586 -1.8598576 -2.5922351 -3.8202682 -4.6512575 -5.3881464 -5.9115572][-8.9911833 -8.5217295 -7.9005985 -7.0347667 -6.1923904 -5.5784 -4.6810479 -4.3265305 -4.1249437 -4.1003866 -4.8514438 -5.5399976 -6.0447974 -6.5555019 -7.1545243][-8.57261 -8.8342876 -8.530735 -7.7396946 -7.1870646 -6.4364119 -5.8523459 -5.7152667 -5.3447442 -5.0471325 -5.5872774 -6.0824833 -6.4264226 -6.7300296 -6.908287][-7.6698775 -8.2696114 -8.4431057 -7.9970751 -7.2804041 -6.6256886 -6.1725888 -6.1762128 -6.138711 -6.1908989 -6.7993522 -7.1220956 -7.0104723 -6.6239462 -6.4921989][-7.7686391 -8.0802155 -8.4444714 -8.216423 -7.9520454 -7.4455333 -7.09072 -6.99645 -6.8950038 -7.2388668 -7.3445916 -7.3818049 -7.2837291 -7.2046928 -6.7764997]]...]
INFO - root - 2017-12-15 21:34:57.384456: step 63010, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 47h:48m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:35:03.915971: step 63020, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 48h:24m:34s remains)
INFO - root - 2017-12-15 21:35:10.337750: step 63030, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 49h:19m:29s remains)
INFO - root - 2017-12-15 21:35:16.748784: step 63040, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 46h:56m:45s remains)
INFO - root - 2017-12-15 21:35:23.086660: step 63050, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 47h:58m:21s remains)
INFO - root - 2017-12-15 21:35:29.479823: step 63060, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 49h:02m:15s remains)
INFO - root - 2017-12-15 21:35:35.825132: step 63070, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 47h:18m:25s remains)
INFO - root - 2017-12-15 21:35:42.281667: step 63080, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 48h:55m:13s remains)
INFO - root - 2017-12-15 21:35:48.672732: step 63090, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 46h:25m:02s remains)
INFO - root - 2017-12-15 21:35:55.042787: step 63100, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 47h:31m:00s remains)
2017-12-15 21:35:55.561885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6689286 -1.8468456 -1.6554413 -1.5349145 -2.1441855 -2.3781819 -2.4997673 -2.839952 -3.0049391 -4.8480062 -5.337184 -6.6897774 -7.37655 -8.3162928 -8.3382483][-1.4036317 -1.7052307 -2.0865216 -2.0881929 -2.2284617 -2.3881798 -2.6926422 -3.3568087 -3.4899158 -5.2830458 -5.4633904 -6.9950213 -7.3997841 -8.0057955 -7.7734818][-2.5457048 -2.3075809 -2.0649209 -1.555593 -1.610127 -1.5152755 -1.746295 -2.5749946 -3.396523 -5.190135 -5.3415747 -6.7719736 -6.9697566 -7.4087377 -7.0031109][-3.2213311 -3.0303936 -2.6512032 -1.7796698 -1.3893275 -0.98761082 -0.98870134 -1.3628592 -2.0017 -4.2347555 -4.728837 -6.1605473 -6.77555 -7.011735 -6.5952435][-3.5192208 -3.4527493 -3.5078964 -2.92413 -1.8971553 -1.1433926 -0.74888945 -0.93155575 -0.91677809 -2.8267817 -3.2450809 -4.7582827 -5.4685802 -6.190649 -6.2026291][-4.4506702 -4.0838 -3.7818182 -3.0674605 -2.5459018 -1.3061104 -0.58599234 -0.29640532 -0.266531 -1.7865605 -1.6911917 -3.2994709 -4.0135126 -5.2488604 -5.6487255][-4.3806992 -4.0178413 -3.7273581 -2.6866302 -1.8814101 -0.76861143 -0.12157106 0.32580662 0.51612186 -1.1894412 -1.2332058 -2.7451906 -3.7910953 -5.0154204 -5.5676203][-3.1525712 -2.9003077 -2.7010899 -1.9237618 -1.1599441 -0.030130386 0.37319469 0.59987831 0.89272881 -0.74472857 -0.96418 -2.68533 -3.3600812 -4.8788857 -5.9057522][-2.4450879 -2.1011095 -2.3680463 -2.1023731 -1.2732792 -0.124053 0.7201004 1.0985327 1.0505648 -0.40397835 -0.43855953 -2.2295175 -3.3139935 -4.6328077 -5.4317784][-1.7508292 -1.8149619 -2.109045 -1.7737041 -1.4550066 -0.64208794 -0.1487031 0.40005875 0.76517868 -0.59665394 -0.62602997 -2.2144871 -3.318779 -4.7601852 -5.5535111][-1.4198132 -1.5766754 -2.0510368 -1.8299942 -1.4309764 -0.779634 -0.39355564 -0.368711 -0.57581091 -1.7403626 -1.9224143 -3.0484738 -3.9281411 -4.8414412 -5.6370788][-1.464591 -1.3910007 -1.7006092 -1.6031041 -1.6491327 -1.4321995 -1.3850827 -1.5092683 -1.7205853 -2.8230085 -3.2826657 -4.2166338 -5.0011597 -5.6887417 -6.0662651][-1.7037249 -2.2007294 -2.3292232 -2.1943054 -2.417913 -2.3524766 -2.4772983 -2.9957824 -3.3136911 -4.2927523 -4.920907 -5.5272036 -6.3186069 -6.7368584 -6.5854836][-2.6540222 -2.3078184 -2.9298196 -3.1151876 -3.1589451 -2.9330206 -3.057035 -3.4682589 -3.914206 -5.1246681 -5.7372727 -6.0039759 -6.635478 -7.0793471 -7.2551136][-4.6023884 -4.3079405 -4.2513542 -4.0879383 -4.8081741 -4.8972616 -4.9704409 -5.1753836 -5.3061962 -5.9276714 -6.7649765 -7.3188753 -7.5681925 -7.419838 -7.4191632]]...]
INFO - root - 2017-12-15 21:36:02.030141: step 63110, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 47h:50m:38s remains)
INFO - root - 2017-12-15 21:36:08.412973: step 63120, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 47h:31m:03s remains)
INFO - root - 2017-12-15 21:36:14.767935: step 63130, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 47h:04m:45s remains)
INFO - root - 2017-12-15 21:36:21.124323: step 63140, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 47h:23m:57s remains)
INFO - root - 2017-12-15 21:36:27.532357: step 63150, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 47h:59m:19s remains)
INFO - root - 2017-12-15 21:36:33.920557: step 63160, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 48h:17m:16s remains)
INFO - root - 2017-12-15 21:36:40.274869: step 63170, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 47h:34m:31s remains)
INFO - root - 2017-12-15 21:36:46.654837: step 63180, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 47h:16m:07s remains)
INFO - root - 2017-12-15 21:36:52.949679: step 63190, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.625 sec/batch; 46h:44m:09s remains)
INFO - root - 2017-12-15 21:36:59.368764: step 63200, loss = 0.40, batch loss = 0.29 (12.8 examples/sec; 0.627 sec/batch; 46h:53m:23s remains)
2017-12-15 21:36:59.894715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0054617 -4.4804263 -4.6936808 -4.7200928 -4.7360544 -4.4267683 -4.1768484 -4.1519055 -3.7855792 -3.8364003 -4.2785511 -6.5607419 -7.6046529 -8.2502565 -9.0209007][-5.055963 -5.1043425 -5.2953248 -5.3853207 -5.7161422 -5.6879845 -5.4587264 -5.0265169 -4.1076069 -4.347765 -4.470953 -6.6028967 -7.8072863 -9.1458454 -10.408424][-4.2827168 -4.1440248 -3.9368358 -4.1116719 -4.5886307 -4.51979 -4.6386824 -4.5270591 -4.3614407 -4.2063904 -3.93311 -5.8826914 -6.6737819 -7.6103454 -8.6636915][-3.2604179 -2.4523377 -2.0625634 -2.0449152 -2.1835175 -2.1415787 -1.9874878 -2.3944492 -2.3126302 -2.3566327 -2.7143502 -5.1111712 -6.3074408 -7.0959821 -7.6071067][-2.5564632 -1.4504199 -0.67917585 -0.41464043 -0.41015005 0.00021505356 0.59190273 0.24997759 -0.057383537 -0.54244614 -1.2508698 -3.8558979 -5.0693059 -5.8091578 -6.2051563][-2.7670331 -2.2762761 -1.5765147 -0.54300022 0.12888622 0.69164371 1.5656633 1.595171 1.7380304 1.232233 0.33199596 -2.5013375 -3.85904 -4.9040179 -5.8399186][-3.8524148 -3.2385702 -2.5471387 -1.5344434 -0.6076951 0.89528084 2.2980356 2.47569 2.7867947 2.0742168 0.93700695 -2.4853315 -4.3098373 -5.3964319 -6.0994325][-4.4827414 -3.8741956 -3.2634387 -1.899508 -0.746892 0.75574112 2.1001921 2.6970024 3.1810226 2.506691 1.4092493 -1.977057 -3.8054419 -5.2501073 -6.2713556][-5.2890396 -4.3700051 -3.6752176 -2.0899649 -0.93694735 0.13257027 1.4270325 2.2292509 2.7219172 2.0924292 1.1195145 -2.1135478 -3.9863074 -5.1668167 -5.875104][-5.7967796 -4.7902203 -4.2697506 -3.5725675 -2.8197913 -1.6720662 -0.25031757 0.37290287 1.0603981 0.36255741 -0.52188396 -3.2295666 -4.519763 -5.6034565 -6.3318062][-5.8090353 -5.337534 -5.2301216 -4.5016356 -3.7684078 -2.7997069 -1.7640352 -1.3310733 -0.73825312 -0.89440823 -1.2336869 -4.0007095 -5.7932434 -6.2760143 -6.7622051][-4.6750021 -4.6625962 -4.7024994 -3.7986274 -3.0775537 -2.1806111 -1.5532813 -1.3162966 -1.134655 -2.1473742 -2.6848283 -4.2336621 -5.300951 -6.199553 -7.191112][-4.4672546 -4.5429621 -4.4950771 -3.6517959 -3.2680225 -2.7370996 -2.6299257 -2.5823631 -2.5376792 -3.5297198 -3.5512877 -4.8956461 -5.8718858 -6.2877178 -6.7290449][-4.3693867 -4.2314329 -4.0045853 -3.3623514 -3.0019979 -2.2759223 -2.1672077 -2.3026671 -2.7022266 -3.4878001 -4.0137234 -4.9958706 -5.4676 -5.7995176 -6.267035][-5.6150246 -6.0261345 -5.8008413 -4.7619634 -3.8870459 -2.9486275 -2.7299986 -3.1196189 -3.4786844 -4.3703671 -5.1565084 -5.9514246 -6.667367 -6.9758196 -7.1560745]]...]
INFO - root - 2017-12-15 21:37:06.357691: step 63210, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 47h:41m:33s remains)
INFO - root - 2017-12-15 21:37:12.818532: step 63220, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 48h:40m:07s remains)
INFO - root - 2017-12-15 21:37:19.256822: step 63230, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 47h:21m:37s remains)
INFO - root - 2017-12-15 21:37:25.692823: step 63240, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 48h:50m:30s remains)
INFO - root - 2017-12-15 21:37:32.115796: step 63250, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 47h:03m:51s remains)
INFO - root - 2017-12-15 21:37:38.481969: step 63260, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 47h:20m:10s remains)
INFO - root - 2017-12-15 21:37:44.891102: step 63270, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 47h:29m:35s remains)
INFO - root - 2017-12-15 21:37:51.206374: step 63280, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 46h:44m:11s remains)
INFO - root - 2017-12-15 21:37:57.568949: step 63290, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 47h:25m:21s remains)
INFO - root - 2017-12-15 21:38:04.029325: step 63300, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 47h:09m:50s remains)
2017-12-15 21:38:04.560832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4718742 -4.7150164 -5.0712676 -5.537632 -5.9546518 -6.0502176 -5.5749159 -4.1409869 -3.0721521 -2.5141082 -2.3735447 -3.972384 -5.0972857 -6.1802979 -6.9746895][-3.7781079 -4.4673004 -5.0157652 -5.6346283 -6.2216606 -6.536448 -6.1250944 -5.0904045 -3.7481675 -2.5300593 -1.8669462 -3.1869483 -3.8530102 -4.9542179 -6.3548293][-4.2974977 -4.5871096 -5.0340424 -5.8772073 -6.41565 -6.6263905 -6.3512244 -5.7639661 -4.8953285 -4.3510218 -3.7730153 -4.737628 -5.1732635 -5.7992377 -6.8127303][-5.5812416 -5.4255695 -5.2075176 -5.0935478 -5.0464396 -5.0855851 -4.6850929 -4.259553 -3.6426973 -3.3448453 -3.3133936 -4.9719315 -5.8727612 -6.6184573 -7.4003725][-5.1126795 -4.1693392 -3.2990174 -2.6867461 -2.1198826 -1.4788365 -0.91829729 -0.67559147 -0.33398247 -0.37649393 -0.88536215 -2.8705158 -4.1607132 -5.3178997 -6.6134973][-3.7540526 -2.8757215 -2.1258397 -1.2540913 -0.25323915 0.68093109 1.4689026 1.856946 2.1037378 1.7844191 0.57516289 -1.7568269 -3.3641286 -4.9279509 -6.188406][-2.8321609 -1.8360362 -0.88961124 0.15567493 1.3415327 2.2136688 2.9871683 3.4130173 3.5571737 2.8190279 1.2582626 -1.3508701 -3.0170527 -5.0669374 -6.5285788][-2.4926219 -1.4808779 -0.5583005 0.94383526 2.4509497 3.4178095 4.2568331 4.8274717 5.1913719 4.36755 2.8058414 -0.081242085 -2.1102657 -4.0566092 -5.229908][-1.8935661 -0.86165571 0.20615339 1.2175159 1.9878502 2.6729774 3.1967869 3.698781 4.0293894 3.2738333 2.1102047 -0.53337955 -2.534708 -4.2981567 -5.4667292][-2.805522 -1.7631607 -0.8539958 0.30701733 0.86576366 1.0873604 1.3350153 1.6614857 1.7044249 0.63115025 -0.4236455 -2.396893 -3.8303037 -5.0707383 -6.2138753][-4.8080873 -4.5592871 -3.6333485 -2.3769045 -1.4548135 -1.0119977 -0.85071945 -0.96755934 -1.3625493 -2.5045791 -3.0358438 -4.5446248 -5.8274822 -6.3058152 -6.8736873][-7.6835575 -7.2166605 -6.7823668 -5.7272124 -4.8738966 -4.091238 -3.6840992 -3.8536615 -4.2500844 -5.3290782 -5.714818 -6.2878022 -7.0203047 -7.0896926 -7.6917987][-10.198492 -10.478965 -10.023845 -8.9966 -7.6996245 -6.7093248 -6.1573372 -6.0434914 -6.4055343 -6.8606935 -7.2506151 -7.4033928 -7.6545258 -7.5030589 -7.4112906][-10.06965 -9.8805618 -9.7020922 -8.9488668 -8.0894814 -6.9103851 -6.2693772 -6.1831169 -6.4666491 -6.8163481 -6.9566941 -6.7948661 -6.732439 -6.6160274 -6.4588723][-9.7944317 -8.8501816 -8.1111355 -7.7686367 -7.3254609 -6.5465117 -6.1801424 -6.501286 -7.0675306 -7.3146839 -7.0588207 -6.598001 -6.174571 -5.85208 -5.6625013]]...]
INFO - root - 2017-12-15 21:38:10.973844: step 63310, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.640 sec/batch; 47h:51m:06s remains)
INFO - root - 2017-12-15 21:38:17.426230: step 63320, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 47h:06m:20s remains)
INFO - root - 2017-12-15 21:38:23.875166: step 63330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 47h:25m:53s remains)
INFO - root - 2017-12-15 21:38:30.217757: step 63340, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 46h:57m:53s remains)
INFO - root - 2017-12-15 21:38:36.715449: step 63350, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 48h:43m:44s remains)
INFO - root - 2017-12-15 21:38:43.152988: step 63360, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 48h:47m:57s remains)
INFO - root - 2017-12-15 21:38:49.456142: step 63370, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 46h:46m:10s remains)
INFO - root - 2017-12-15 21:38:55.859894: step 63380, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 46h:25m:15s remains)
INFO - root - 2017-12-15 21:39:02.280860: step 63390, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 48h:27m:30s remains)
INFO - root - 2017-12-15 21:39:08.716242: step 63400, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 49h:22m:05s remains)
2017-12-15 21:39:09.309487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6709142 -6.7770615 -6.6480718 -6.105083 -5.4543123 -4.0013857 -2.6277928 -1.8512344 -1.3880038 -2.5238271 -4.1243277 -5.9999714 -7.3758774 -7.9787588 -8.6622467][-6.6636353 -6.6946259 -6.6788111 -5.9577708 -5.2650948 -4.0053806 -2.8062625 -2.0493331 -1.5230846 -2.9225941 -4.3715529 -6.4933896 -7.8587842 -8.341485 -9.2439251][-6.9462986 -7.3817725 -7.4773855 -6.6945281 -5.6885834 -4.050004 -2.4130321 -1.5625243 -1.2859979 -2.8962803 -4.6041508 -6.8346457 -7.7269855 -7.95706 -8.7173347][-6.6032434 -7.1136489 -7.3535261 -6.5426321 -5.0557289 -3.0079174 -1.2450318 -0.11801481 0.43292236 -1.1533251 -3.2507987 -6.1130729 -7.5463271 -7.5833669 -8.0876484][-7.12139 -7.1640797 -6.4814897 -5.4022388 -4.1123352 -2.1802607 0.073560715 1.0668392 1.1051769 -0.95425653 -3.1549816 -5.8027425 -6.9988952 -7.2623377 -8.0626144][-8.2129726 -8.09574 -7.1607785 -5.3914323 -3.0944238 -0.48307753 1.3498611 2.2983704 2.5729246 -0.075502396 -3.0834656 -6.3015556 -7.817688 -8.1658382 -8.6799135][-8.9318256 -8.2096138 -6.4964333 -4.7173033 -2.7310429 0.36092186 2.8597574 3.4846554 2.9405556 0.3376236 -2.5604553 -5.7498159 -7.6215882 -8.5512543 -9.3390026][-7.9253135 -7.5985937 -6.8307996 -4.605267 -1.6570339 1.1308317 3.7264223 4.9375391 5.0736637 1.9146147 -1.7889667 -5.4031143 -7.6224613 -8.303071 -8.8585644][-7.2827988 -6.5387478 -5.802587 -3.995923 -1.7908845 1.0237436 3.495306 4.3367424 4.3191118 1.3053427 -1.8091736 -5.3908644 -7.791132 -8.2257681 -8.9935179][-7.0026255 -6.7835503 -6.4690366 -4.3670988 -1.9178662 0.53719139 2.3743277 3.3365793 3.2066536 -0.34049988 -3.6427941 -6.8931847 -8.523325 -8.4392395 -8.427269][-6.6910186 -6.5754285 -5.8980789 -4.0560479 -1.9201126 -0.062987328 1.0602398 0.63341331 -0.21791887 -3.1483226 -6.0742636 -7.8439431 -8.3454361 -7.7771549 -7.8221359][-5.9710941 -4.8462739 -3.3480525 -1.3284688 0.052697659 1.286562 1.6499672 0.12676573 -0.90108109 -3.6605453 -5.747508 -6.8566208 -7.7148185 -7.268343 -7.4743876][-6.7756329 -4.9007964 -2.9637275 -0.68059683 0.778368 1.6418982 1.7301121 0.41240215 -1.0172696 -3.5818558 -5.2242956 -6.0049219 -6.3535748 -6.5041313 -6.7469583][-7.8359485 -6.2420197 -4.177525 -1.9339056 -0.33303213 -0.18498659 -0.74217272 -1.2657781 -1.9180722 -2.7923584 -3.6258831 -4.8451128 -5.1141567 -5.539784 -6.22447][-8.5381823 -7.4671192 -6.2396736 -3.9893055 -2.5704474 -1.8734393 -1.5884972 -2.7307549 -3.7080343 -4.4546776 -4.9852681 -5.3738632 -5.6806355 -5.6798973 -5.6450949]]...]
INFO - root - 2017-12-15 21:39:15.676558: step 63410, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:25m:09s remains)
INFO - root - 2017-12-15 21:39:22.027549: step 63420, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 46h:51m:53s remains)
INFO - root - 2017-12-15 21:39:28.443579: step 63430, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:24m:59s remains)
INFO - root - 2017-12-15 21:39:34.890903: step 63440, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 47h:34m:32s remains)
INFO - root - 2017-12-15 21:39:41.334827: step 63450, loss = 0.35, batch loss = 0.24 (11.6 examples/sec; 0.692 sec/batch; 51h:42m:37s remains)
INFO - root - 2017-12-15 21:39:47.728711: step 63460, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 47h:17m:18s remains)
INFO - root - 2017-12-15 21:39:54.089561: step 63470, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 47h:53m:42s remains)
INFO - root - 2017-12-15 21:40:00.485791: step 63480, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 47h:28m:48s remains)
INFO - root - 2017-12-15 21:40:06.845248: step 63490, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 47h:42m:17s remains)
INFO - root - 2017-12-15 21:40:13.171330: step 63500, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:24m:24s remains)
2017-12-15 21:40:13.718657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8959737 -2.6635256 -3.9568269 -4.731348 -4.7091422 -3.9712284 -2.8997216 -1.7752628 -0.71651411 -1.8893056 -2.3356528 -5.8242044 -7.30189 -8.2200756 -9.33533][-0.86385059 -1.9296331 -3.3212948 -4.5791011 -5.422595 -5.3701611 -4.0786767 -2.479847 -0.82045269 -1.4652252 -2.3066087 -6.0461922 -8.0079746 -8.9345913 -9.7389536][-1.7451096 -2.1031179 -3.0932627 -3.9460742 -4.6044507 -4.8441877 -4.4963779 -3.739933 -2.6336193 -2.5546222 -2.9580674 -6.0064397 -7.53283 -8.7328386 -9.7238][-2.061358 -2.440371 -3.3282547 -3.1609406 -2.9546614 -2.430861 -1.524035 -1.1939626 -0.88265753 -1.9387298 -3.1768064 -7.1214056 -8.4878864 -9.0054207 -9.5544138][-2.584703 -2.2665291 -1.6748009 -1.5198698 -1.4190292 0.55913162 2.8929777 3.476697 3.2097425 0.51467514 -1.7369962 -6.2290769 -7.9754343 -8.43659 -9.3067684][-2.6531267 -3.2814589 -2.7117295 -1.5523915 0.23213291 2.8726473 4.7669735 5.3204546 5.2929811 2.6600227 -0.14044619 -5.149044 -7.5441418 -8.7993994 -9.3775063][-2.8491788 -3.077229 -2.3835859 -0.97275019 0.86947727 3.6038399 5.7006264 7.0976524 7.7527905 5.0743885 1.5104752 -4.4705391 -6.9014692 -8.0523376 -8.5768986][-2.1882987 -1.5616446 -1.0879893 0.17843914 2.0098448 4.3730679 6.1295147 7.2591381 7.5913763 5.0455675 2.1052895 -3.3756509 -5.7278552 -7.0972681 -7.849021][-2.8476105 -1.8604121 -1.1263247 -0.68111706 0.28838873 2.5393524 4.0273161 5.43978 5.742507 3.0886841 0.30630875 -4.6813421 -6.2864814 -7.2319326 -8.310667][-3.7695048 -3.2339277 -2.7535281 -2.089488 -1.3106503 -0.622252 -0.39602709 0.56120682 1.4352493 -0.52332687 -2.4332066 -6.7260418 -8.0278635 -8.1744785 -8.5884542][-6.11009 -4.948771 -4.5653448 -3.7931006 -3.1470518 -2.823617 -2.6372843 -2.8690977 -3.2378058 -4.764636 -6.2937684 -8.7714252 -8.9169149 -8.9574966 -9.0860491][-7.619422 -6.7349796 -6.2224932 -5.7191744 -5.6027861 -5.3678837 -5.1165762 -5.2797956 -5.8042088 -6.4979286 -7.48237 -8.77923 -8.7396812 -8.9681978 -8.9936352][-7.8669786 -7.7104778 -7.6120372 -6.6653004 -6.2882748 -6.177557 -6.0972409 -6.0659447 -5.8588934 -6.1667862 -6.7782912 -7.5872917 -7.2331147 -7.2709846 -7.3238878][-7.9871831 -7.7458515 -7.6955309 -7.0758533 -6.72938 -6.5391932 -6.2924461 -6.0603085 -5.5899067 -6.2850637 -6.7419591 -6.4209671 -5.610157 -5.7394657 -5.5658493][-7.51469 -7.5676527 -7.9427953 -8.2677135 -8.3007565 -7.5768023 -6.7767596 -6.8704615 -7.2373853 -7.0278888 -6.6270313 -6.0727482 -5.861084 -5.6870089 -5.3025961]]...]
INFO - root - 2017-12-15 21:40:20.079065: step 63510, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.632 sec/batch; 47h:15m:31s remains)
INFO - root - 2017-12-15 21:40:26.510819: step 63520, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 47h:10m:21s remains)
INFO - root - 2017-12-15 21:40:32.926451: step 63530, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 50h:02m:55s remains)
INFO - root - 2017-12-15 21:40:39.418772: step 63540, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 46h:51m:13s remains)
INFO - root - 2017-12-15 21:40:45.817090: step 63550, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 49h:36m:12s remains)
INFO - root - 2017-12-15 21:40:52.389350: step 63560, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.677 sec/batch; 50h:34m:40s remains)
INFO - root - 2017-12-15 21:40:58.761309: step 63570, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 47h:30m:26s remains)
INFO - root - 2017-12-15 21:41:05.183718: step 63580, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 49h:25m:43s remains)
INFO - root - 2017-12-15 21:41:11.690938: step 63590, loss = 0.36, batch loss = 0.24 (12.2 examples/sec; 0.654 sec/batch; 48h:50m:49s remains)
INFO - root - 2017-12-15 21:41:18.057167: step 63600, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 47h:09m:37s remains)
2017-12-15 21:41:18.713654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6018615 -3.4431505 -4.4165187 -4.52521 -4.0079174 -3.3832293 -3.5064244 -3.3367538 -2.9607406 -3.702352 -3.6262636 -5.3026896 -6.0821533 -6.4797134 -7.5166869][-2.8872991 -3.7279286 -4.751441 -4.9600458 -4.8343 -4.2547607 -3.6994243 -3.610383 -3.6703682 -4.412632 -4.4902954 -6.0718746 -6.5967479 -7.2173934 -7.8832536][-3.7149093 -3.9593823 -4.6857233 -4.715292 -4.8442807 -4.5140038 -4.0829024 -3.9980884 -3.3987446 -4.3952513 -4.8353977 -6.5874729 -7.1648836 -7.8915524 -8.6101265][-4.5516214 -4.7446737 -4.929492 -4.6106014 -4.2506218 -3.8915429 -3.2102618 -2.7656903 -2.7358494 -3.3779073 -3.7214253 -5.9166546 -6.852416 -7.6827397 -8.2443008][-5.7467604 -5.49073 -5.49304 -4.8963008 -4.175395 -3.0632081 -1.7036991 -1.2695699 -0.8985858 -2.2033405 -3.2016349 -5.462944 -6.5153708 -7.4969778 -8.2657595][-7.3876143 -6.7857876 -6.1657648 -4.7513247 -3.6144929 -1.980001 -0.17555141 0.86519623 1.0455399 -1.2162623 -2.8858933 -5.7013016 -6.7600889 -7.3940296 -7.9376764][-7.1896567 -6.5677013 -6.1362696 -4.3495488 -2.1907077 -0.095038891 2.1481714 3.0381317 3.2502356 1.2207594 -0.88405561 -4.1945925 -5.7984757 -6.4639292 -7.1779351][-6.7702494 -6.2304983 -5.4682927 -3.5220661 -1.4698763 1.4063406 3.8888569 4.2428761 4.0863485 1.7486134 0.1144228 -2.939455 -4.3990812 -5.2703819 -5.8278413][-6.790257 -6.2207041 -5.76274 -4.3386989 -2.3935533 0.5024128 2.8292866 3.5376883 3.5876656 1.0560904 -0.70885992 -3.5961814 -4.8973823 -5.5274377 -6.3164635][-6.763525 -6.5354471 -6.1644096 -4.87339 -3.4732122 -1.4303975 0.62440586 1.0998363 1.303113 -0.94341707 -2.8310428 -5.3846426 -6.223218 -6.1332388 -6.7434235][-7.9747157 -7.6390676 -7.2753634 -5.9931841 -4.9998903 -3.4406595 -2.2228379 -1.7872105 -1.3708305 -3.2664881 -3.9302735 -6.028429 -6.9981251 -6.6910844 -6.9092555][-7.9680119 -7.5926671 -7.2845006 -6.3220406 -5.798563 -4.9475889 -3.960417 -3.9826119 -3.8969111 -4.567358 -4.656518 -5.9457417 -6.3702512 -6.4902987 -6.9397078][-8.7156353 -7.9891973 -7.4553185 -6.6932087 -6.0983367 -5.8173127 -5.3907003 -5.3683891 -4.979403 -6.0218735 -6.120759 -6.2249212 -6.287117 -6.2179489 -6.7617154][-8.6310244 -8.0721388 -7.3516445 -6.3766561 -5.8398 -5.7901363 -5.2853189 -5.3900824 -5.4486504 -5.7439289 -5.9591718 -6.0001307 -5.9583364 -6.1582608 -6.384366][-8.7771206 -8.6225767 -8.2256374 -7.3116703 -6.4023285 -6.1973934 -6.3248129 -6.5389118 -6.3469396 -6.3961091 -6.3647413 -6.2589922 -5.9722266 -5.6793709 -5.4087677]]...]
INFO - root - 2017-12-15 21:41:25.197871: step 63610, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 47h:32m:37s remains)
INFO - root - 2017-12-15 21:41:31.619076: step 63620, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 48h:17m:17s remains)
INFO - root - 2017-12-15 21:41:37.946425: step 63630, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 46h:57m:52s remains)
INFO - root - 2017-12-15 21:41:44.377472: step 63640, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 49h:44m:09s remains)
INFO - root - 2017-12-15 21:41:50.759742: step 63650, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 48h:02m:30s remains)
INFO - root - 2017-12-15 21:41:57.183274: step 63660, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 46h:43m:54s remains)
INFO - root - 2017-12-15 21:42:03.607897: step 63670, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.659 sec/batch; 49h:12m:12s remains)
INFO - root - 2017-12-15 21:42:10.097112: step 63680, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 46h:44m:01s remains)
INFO - root - 2017-12-15 21:42:16.564732: step 63690, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 49h:20m:15s remains)
INFO - root - 2017-12-15 21:42:23.030226: step 63700, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 47h:59m:40s remains)
2017-12-15 21:42:23.537728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5574169 -3.0892801 -2.7395697 -2.4320216 -2.349412 -2.4343824 -2.5352263 -2.6647563 -2.6237764 -4.3316565 -5.0222015 -5.8535047 -6.511549 -7.2087407 -7.3464117][-3.0492411 -2.7310495 -2.6393633 -2.5663729 -2.7025037 -2.5450425 -2.5358691 -2.5073318 -2.4793849 -4.3729048 -5.1522522 -5.8103976 -6.480504 -7.4134331 -7.8241467][-3.64282 -3.2779531 -3.279285 -3.1315818 -2.847322 -2.2436032 -1.7497802 -1.7667422 -1.5882535 -3.1194992 -3.7493346 -4.4936552 -5.2012768 -6.1093087 -6.7787356][-4.0100446 -3.5593653 -3.4253683 -3.0350037 -2.3539171 -1.4730701 -0.85838747 -0.86425591 -0.761456 -2.3807631 -3.0446825 -3.7750528 -4.7812281 -5.6967435 -6.3470364][-4.4335723 -3.7936485 -3.0749121 -2.5999875 -1.8754835 -0.89058113 -0.11496782 0.021870136 0.0054583549 -1.7079177 -2.31083 -3.0995936 -4.4287705 -5.5128336 -6.2111645][-4.5024371 -3.8299117 -3.1469512 -2.1428933 -1.0617352 -0.21524191 0.44391727 0.61748981 0.65660954 -1.168777 -1.9472284 -2.8643947 -4.2726917 -5.612937 -6.3700104][-3.8560586 -3.0645351 -2.2336807 -1.2967358 -0.40964556 0.34371185 0.98962212 1.2305174 1.3457899 -0.67769432 -1.7629099 -2.8295755 -4.14223 -5.4085855 -6.199369][-3.5811949 -2.4560547 -1.2388868 -0.058437347 0.97192478 1.4656887 1.5991182 1.5316458 1.5373039 -0.59985065 -1.9528308 -3.2733679 -4.4917088 -5.576 -5.9766884][-3.286345 -2.2782569 -1.125658 0.16254807 0.98531723 1.2598314 1.1795912 1.2962027 1.3925476 -0.82658863 -2.1782055 -3.7243056 -5.0629129 -6.1581421 -6.4816022][-2.4214044 -1.4069209 -0.68131733 0.08299017 0.64142704 0.883173 0.98574257 0.79195404 0.48493862 -1.7505331 -3.0417752 -4.5052729 -5.6208825 -6.5129237 -6.8496847][-3.5375538 -2.7050114 -1.8424211 -0.96372747 -0.6842823 -0.7887888 -0.90513563 -1.1958718 -1.4861054 -3.6491222 -4.8371019 -5.6053295 -6.1899033 -6.7682643 -6.963264][-5.0415707 -4.4034281 -3.4708238 -2.5041609 -2.0909338 -2.2393179 -2.2931514 -2.471838 -2.816062 -4.3059263 -5.4146748 -5.909421 -6.1452932 -6.437026 -6.4728508][-5.6350894 -5.6591034 -5.2612381 -4.6847239 -4.339304 -3.9806168 -3.9901114 -4.0097833 -3.9525464 -4.7655988 -5.6184621 -5.8740258 -5.8189669 -5.6773968 -5.4128742][-5.5118876 -5.5785408 -5.448226 -5.0064244 -4.8565965 -4.5667343 -4.5818024 -4.4525042 -4.3351717 -4.9524364 -5.1802511 -5.2056675 -5.0891161 -5.1537719 -5.1547136][-7.1384196 -6.5399704 -5.8640881 -5.6325879 -5.5472207 -5.5831137 -5.7621717 -5.683023 -5.7208614 -5.4077406 -5.3644085 -5.3880024 -5.224102 -5.2776732 -5.38898]]...]
INFO - root - 2017-12-15 21:42:30.041865: step 63710, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 48h:17m:53s remains)
INFO - root - 2017-12-15 21:42:36.493290: step 63720, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 48h:54m:11s remains)
INFO - root - 2017-12-15 21:42:42.923150: step 63730, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 48h:24m:37s remains)
INFO - root - 2017-12-15 21:42:49.401007: step 63740, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 46h:34m:24s remains)
INFO - root - 2017-12-15 21:42:55.812615: step 63750, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 47h:01m:41s remains)
INFO - root - 2017-12-15 21:43:02.260385: step 63760, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:53m:32s remains)
INFO - root - 2017-12-15 21:43:08.712638: step 63770, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 46h:57m:29s remains)
INFO - root - 2017-12-15 21:43:15.265112: step 63780, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 47h:50m:30s remains)
INFO - root - 2017-12-15 21:43:21.677425: step 63790, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 49h:05m:13s remains)
INFO - root - 2017-12-15 21:43:28.330821: step 63800, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.660 sec/batch; 49h:16m:23s remains)
2017-12-15 21:43:28.855914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9532156 -5.5874023 -6.3253303 -6.9691682 -7.3933692 -7.3674421 -7.2961607 -7.1530643 -6.75705 -6.3848023 -6.7628088 -8.13732 -8.1281042 -8.1657219 -8.6505289][-4.2007871 -5.1684275 -6.738802 -7.9421663 -8.5100079 -8.3575554 -7.8632135 -7.5565681 -6.8535023 -6.4399452 -6.9572725 -8.4430208 -8.4887619 -8.7600317 -9.3105183][-3.4356055 -4.02656 -4.5724421 -5.1707611 -5.9782352 -6.5237074 -7.0282354 -6.7091703 -5.8973131 -5.4960432 -6.0086632 -7.4829512 -8.3753357 -9.1714087 -9.74778][-4.5471377 -3.5710149 -3.656004 -3.715914 -3.9736793 -4.1887894 -4.5572653 -4.8189011 -4.2828236 -3.9503784 -4.5026231 -6.1630077 -7.1358962 -8.39287 -9.6992559][-4.6852894 -3.3921356 -2.7013574 -2.29074 -2.3599472 -1.9178281 -1.6940827 -1.6719637 -1.1711602 -1.8519912 -3.2779169 -5.5003872 -6.843421 -8.0228825 -8.987649][-4.2330937 -3.6275487 -2.7208562 -1.7698154 -1.5263991 -0.72605371 0.17803717 0.97636986 1.8394384 0.84687424 -1.1882315 -4.1623783 -6.0813403 -7.0343065 -8.0189753][-3.6178956 -3.1014891 -2.5679064 -1.4088049 0.059293747 1.2039213 1.8293858 2.3627586 3.2344885 2.7968235 0.9770565 -2.61511 -4.6092348 -5.9441819 -7.1784105][-3.4453778 -2.4888582 -1.5998015 -0.49079561 0.58998585 1.8857937 3.1856689 3.3849325 3.0820036 2.120944 0.10700226 -2.9661865 -4.2088494 -5.4857197 -6.7002048][-4.0121336 -3.4092832 -2.3095798 -1.1288877 -0.43165874 0.79084682 2.1109428 3.008831 3.2491045 1.3352041 -1.7250438 -4.8423519 -5.835844 -6.2631989 -6.7149167][-5.5908642 -5.5018969 -5.3980269 -4.01155 -2.7397099 -1.6148152 -0.85000324 -0.077201366 0.75688744 -0.76188231 -3.4280734 -6.2352509 -7.5659986 -7.7655888 -8.0273981][-8.4831867 -8.2388906 -7.9973888 -6.82476 -5.6341262 -4.369072 -3.0547667 -2.4715996 -2.2124367 -3.2334509 -4.9388704 -6.7677517 -7.6728458 -8.17659 -8.652544][-8.9926815 -8.9666672 -8.5396671 -7.8036165 -7.2478986 -6.3082867 -5.2071085 -4.7033167 -4.3417134 -5.1654186 -6.4682288 -7.6871343 -8.0804987 -8.3039322 -8.3193026][-10.471873 -10.115565 -9.52116 -8.9434805 -8.3981009 -7.6066041 -6.7785692 -6.7631907 -6.8827944 -7.6588149 -8.2957106 -8.322073 -7.9941316 -7.9638534 -7.8814864][-9.9221649 -9.5901613 -9.434885 -8.8314791 -8.00789 -7.3351622 -6.5386014 -6.7054029 -7.0960522 -7.3684144 -7.6335506 -7.1482325 -6.6667128 -6.5299153 -6.6453547][-9.5452175 -9.9464455 -10.361974 -10.068143 -9.41234 -8.6414223 -8.2179232 -8.0509272 -7.9931822 -8.22861 -8.4058266 -8.235096 -7.9105277 -7.4150386 -6.65133]]...]
INFO - root - 2017-12-15 21:43:35.352178: step 63810, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 49h:29m:06s remains)
INFO - root - 2017-12-15 21:43:41.803803: step 63820, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 47h:09m:03s remains)
INFO - root - 2017-12-15 21:43:48.231769: step 63830, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:24m:57s remains)
INFO - root - 2017-12-15 21:43:54.658638: step 63840, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 47h:25m:47s remains)
INFO - root - 2017-12-15 21:44:01.060370: step 63850, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 49h:20m:00s remains)
INFO - root - 2017-12-15 21:44:07.483286: step 63860, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 48h:22m:44s remains)
INFO - root - 2017-12-15 21:44:13.902789: step 63870, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 48h:32m:23s remains)
INFO - root - 2017-12-15 21:44:20.294805: step 63880, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 46h:12m:01s remains)
INFO - root - 2017-12-15 21:44:26.726430: step 63890, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 48h:46m:20s remains)
INFO - root - 2017-12-15 21:44:33.134642: step 63900, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 47h:57m:25s remains)
2017-12-15 21:44:33.635646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.585752 -4.3659816 -4.5535517 -5.0035176 -5.1284895 -4.6855917 -4.4375362 -4.1371684 -4.1684594 -4.7244878 -5.0283289 -5.5606985 -5.9998693 -6.3045273 -6.1644154][-4.0305586 -3.9595485 -4.3083334 -4.8281975 -5.1084843 -5.4521151 -5.4369693 -5.1354523 -4.857728 -5.059154 -5.194313 -5.3345261 -6.1955366 -6.3408952 -5.4745569][-2.1207023 -1.8434267 -2.3688512 -3.4308085 -4.19829 -4.5696073 -4.5678968 -4.5089817 -4.4239116 -4.7038059 -4.5856972 -3.5782614 -3.575254 -4.8885436 -5.4992032][-0.31234455 -0.046555042 -0.0025339127 -1.0690622 -2.0553446 -2.5689268 -2.6120548 -2.3462467 -1.8131003 -2.4759912 -3.0061989 -2.9432154 -3.2391648 -3.3566074 -3.2617412][0.66926765 1.4444122 2.210638 1.5809793 0.99926853 0.88068104 0.19052172 -0.048606396 0.025675774 -0.43008471 -0.87338257 -0.83500195 -2.112864 -2.9659324 -3.493618][1.016058 1.3524199 1.5953121 2.2126751 3.04255 3.1082373 3.0027771 2.4260149 1.8163786 1.0406551 -0.0042176247 -0.69124031 -1.9424238 -2.9249091 -3.1147823][-0.0060901642 0.92598915 1.7584076 2.2132473 2.6905451 3.570015 4.154191 3.7339783 3.1492176 1.2454395 -0.21765518 -1.0740643 -2.6160202 -3.706244 -4.0958848][-0.77021027 -0.32815123 -0.2460928 0.9879694 2.2068958 3.0058298 3.7788239 3.7454119 3.6711636 1.4254923 -0.97309065 -2.5249333 -3.9689615 -5.1823168 -5.6426907][-2.8386612 -1.975729 -1.3840928 0.26769924 1.4991112 2.3347654 2.9807177 3.0853958 3.0158024 1.253273 -0.74767447 -2.4432936 -4.0954895 -5.28472 -5.838172][-4.2323437 -3.9968367 -3.2908401 -1.8305235 -0.34100103 0.77741528 1.4612122 1.6699877 1.5545578 -0.40995598 -2.3058753 -3.5377283 -5.0500565 -6.1448665 -7.1042547][-6.0245972 -5.6494665 -5.1863289 -4.0704641 -3.0195236 -2.2995172 -1.9352646 -2.02527 -2.0123911 -3.1364775 -4.9640608 -5.7729731 -6.4480319 -6.9093122 -7.7868743][-7.1526461 -5.6574607 -4.8606095 -4.8542933 -4.331636 -3.9375098 -4.02555 -4.0706782 -3.9282794 -4.6167974 -5.6104746 -6.69828 -7.5620508 -7.794415 -8.5612221][-8.425724 -8.4760551 -7.7903547 -7.1619692 -6.846611 -6.4502921 -6.19682 -6.3931179 -6.5746417 -6.8683314 -7.6417289 -7.6231103 -7.8488245 -7.5697837 -7.7286935][-7.1696153 -7.361599 -7.544591 -7.0867686 -6.506752 -6.0492821 -5.9017959 -5.8777027 -5.9442654 -6.3695612 -6.9307942 -6.98879 -7.1487293 -6.4355211 -6.1428723][-7.3858018 -6.7810416 -7.0214915 -6.7772307 -6.8056164 -6.4891214 -6.3007755 -6.4274139 -6.1904182 -5.9556084 -5.9657321 -5.9552603 -5.7596674 -5.6621943 -5.3026409]]...]
INFO - root - 2017-12-15 21:44:40.038544: step 63910, loss = 0.36, batch loss = 0.25 (12.6 examples/sec; 0.634 sec/batch; 47h:18m:28s remains)
INFO - root - 2017-12-15 21:44:46.489653: step 63920, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 49h:55m:15s remains)
INFO - root - 2017-12-15 21:44:52.961646: step 63930, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 49h:23m:59s remains)
INFO - root - 2017-12-15 21:44:59.349742: step 63940, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:51m:58s remains)
INFO - root - 2017-12-15 21:45:05.700673: step 63950, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 47h:42m:39s remains)
INFO - root - 2017-12-15 21:45:12.145372: step 63960, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 49h:02m:36s remains)
INFO - root - 2017-12-15 21:45:18.578197: step 63970, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 49h:30m:32s remains)
INFO - root - 2017-12-15 21:45:25.018494: step 63980, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 48h:38m:10s remains)
INFO - root - 2017-12-15 21:45:31.407457: step 63990, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 47h:32m:39s remains)
INFO - root - 2017-12-15 21:45:37.850034: step 64000, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 47h:46m:32s remains)
2017-12-15 21:45:38.373950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6374884 -4.308527 -3.9947174 -3.4738703 -2.9144077 -2.3617249 -1.5865407 -0.69230795 -0.088721752 -1.3296385 -2.366323 -4.0827885 -5.3134613 -6.2907133 -7.3028889][-4.05083 -3.8647022 -3.7683058 -3.3598342 -3.1722932 -2.5278006 -1.7954559 -1.0080085 -0.030748367 -0.733644 -1.4437633 -3.1639071 -4.7688985 -5.847733 -6.5073104][-4.6235709 -4.0674219 -3.8758337 -3.7547851 -3.4767041 -2.9854131 -2.3541307 -1.786963 -1.137073 -2.055584 -2.648725 -3.8437843 -4.7676926 -5.2022262 -6.1928053][-5.0098171 -4.2122364 -3.7289791 -2.9499102 -2.3738937 -2.0515723 -1.5934949 -1.0412364 -0.32415295 -1.480649 -2.5885334 -4.1416454 -5.1494727 -5.6923 -6.1465969][-5.4097328 -4.4870787 -3.373476 -2.2917295 -1.5185184 -0.81909561 -0.13664198 0.12924242 0.54810715 -0.70849609 -1.871676 -3.685173 -5.1287913 -5.6082792 -6.0106964][-5.0499625 -3.7222035 -3.162405 -1.8949738 -0.38604593 0.46822548 1.1679955 1.5641766 1.9036913 0.50417423 -0.99364424 -2.9507108 -4.2713728 -4.7020559 -5.0708][-4.6581817 -3.4136457 -1.6559181 -0.30275011 1.0823507 1.8054562 2.2792587 2.6458263 2.7728081 1.2204094 -0.066950321 -2.0039558 -3.4462538 -4.0038366 -4.7521281][-3.2019081 -2.2616472 -1.0339203 0.77347088 2.3979549 3.4096813 3.9597692 4.023324 4.0476637 2.4316454 0.80822468 -1.3356199 -3.081306 -3.8899989 -4.6126804][-3.4811621 -2.2794189 -0.40664816 0.78284454 1.7384405 2.7830658 3.0951996 3.2804995 3.357337 1.9493513 0.76247978 -1.0637507 -2.863585 -3.6658802 -4.7406135][-3.4185123 -2.9570727 -1.8025699 -0.44519186 0.63068771 1.2594738 1.1274834 1.3140383 1.2759609 -0.19099951 -1.3275232 -2.553299 -3.6306291 -4.1057463 -4.5666428][-4.7275949 -3.9241464 -2.615334 -1.5812449 -1.3376136 -0.70155048 -0.35103655 -0.49876833 -0.8537879 -1.9876423 -2.7745891 -4.2006693 -5.2818069 -5.2390585 -5.3318949][-6.9600644 -5.8159037 -5.038991 -4.0506468 -3.4137645 -2.7173462 -2.514523 -2.3416929 -2.2666659 -3.0968127 -4.4654331 -5.3953013 -5.8075423 -5.8268318 -5.78666][-7.7406368 -6.9909396 -6.4311152 -5.38052 -5.0852089 -4.6122456 -4.2079144 -3.8796639 -3.9214482 -4.4173579 -5.0939131 -5.2477922 -5.8640203 -5.5943666 -5.1977158][-7.4049439 -6.8851542 -6.3724241 -5.7964487 -5.3370514 -4.7553234 -4.5795727 -4.5383458 -4.6278372 -4.7272682 -4.9676962 -5.1388273 -5.0592728 -4.9522562 -4.9609151][-8.1961346 -7.9976268 -7.7405071 -6.9233279 -6.4927063 -6.2487574 -5.924336 -5.8833117 -6.0047817 -6.1222644 -6.3480887 -6.1201115 -5.6005282 -5.3476048 -4.9150324]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:45:44.993460: step 64010, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.649 sec/batch; 48h:22m:34s remains)
INFO - root - 2017-12-15 21:45:51.372804: step 64020, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 46h:25m:20s remains)
INFO - root - 2017-12-15 21:45:57.785789: step 64030, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.625 sec/batch; 46h:36m:29s remains)
INFO - root - 2017-12-15 21:46:04.116954: step 64040, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 47h:39m:31s remains)
INFO - root - 2017-12-15 21:46:10.538541: step 64050, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 48h:15m:21s remains)
INFO - root - 2017-12-15 21:46:16.921462: step 64060, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 46h:40m:53s remains)
INFO - root - 2017-12-15 21:46:23.350791: step 64070, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 49h:14m:51s remains)
INFO - root - 2017-12-15 21:46:29.739575: step 64080, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 46h:22m:49s remains)
INFO - root - 2017-12-15 21:46:36.137476: step 64090, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 46h:08m:51s remains)
INFO - root - 2017-12-15 21:46:42.544610: step 64100, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.637 sec/batch; 47h:28m:10s remains)
2017-12-15 21:46:43.058509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3945594 -5.5200481 -5.7420888 -5.791955 -5.70852 -5.6256542 -5.4279709 -5.240324 -4.7973437 -5.6155243 -5.45732 -6.2154117 -5.847405 -5.9487524 -6.0958233][-4.4000473 -4.8240318 -5.3226786 -5.5118504 -5.6067324 -5.8056021 -6.135304 -5.8031497 -5.4279842 -6.14473 -5.8288894 -6.4538369 -6.0506639 -6.2621512 -6.5043173][-4.7319784 -4.624187 -4.4247007 -4.3121891 -4.3465614 -4.4327545 -4.7045832 -4.8533835 -4.6010656 -5.4854259 -5.5498343 -6.6267023 -6.60186 -6.6649418 -6.6375566][-3.7965913 -3.3921032 -3.0195146 -2.6672969 -2.2819762 -2.1062407 -1.9575534 -2.0249314 -2.1371322 -3.2001233 -3.6040506 -5.1933279 -5.820663 -6.3473291 -6.4524713][-2.7620792 -2.3176436 -1.513834 -0.65990496 -0.0157938 0.51298904 0.75614452 0.76394081 0.53742886 -1.0167322 -1.820519 -4.0005469 -5.231061 -6.1517243 -6.7203517][-2.0792871 -1.4408607 -0.80867958 -0.020641804 0.73281 1.3961983 1.7834387 1.8902102 1.869257 0.059060097 -1.1483054 -3.4541011 -4.8052931 -5.7530055 -6.337173][-2.2909369 -1.573298 -1.0787082 0.28690577 1.262208 1.852829 2.3471098 2.3683023 2.2317905 0.43902302 -0.71847343 -3.1010246 -4.4448509 -5.3964062 -5.8839293][-2.2169757 -1.2627811 -0.36320925 0.68887043 1.5465384 2.1798563 2.5148821 2.4869022 2.2272749 0.39024925 -0.60809278 -2.9158769 -4.1681185 -5.3273287 -5.8377748][-2.5259829 -1.9783564 -1.2595067 -0.18875217 0.45306778 1.1323061 1.4367399 1.4300127 1.166173 -0.68199253 -1.8424034 -4.2208271 -5.2873926 -6.0945382 -6.3999891][-3.0854602 -2.6142 -2.1797862 -1.2974248 -0.77689362 -0.47818661 -0.23869753 -0.18738079 -0.46338081 -2.1818166 -3.1863666 -5.0421691 -5.7943153 -6.3613567 -6.5124397][-4.8712759 -3.8353024 -2.95889 -2.3295159 -1.9745779 -1.6565948 -1.5275388 -1.6319947 -1.7843509 -3.2034025 -4.0971584 -5.348155 -5.7887926 -6.2466497 -6.2115965][-5.9613304 -5.699522 -4.9255562 -4.2547979 -3.7920413 -3.5868187 -3.7325518 -3.907006 -3.9575856 -4.7416754 -5.2424107 -6.1272426 -6.304872 -6.3998737 -6.3636684][-7.5673018 -7.0660462 -6.7870331 -6.2285814 -5.751771 -5.3141847 -5.1434264 -5.2980108 -5.3887072 -5.8637028 -6.1590004 -6.5672145 -6.5893445 -6.1875181 -5.71953][-8.180582 -7.1578188 -6.3832479 -6.0222387 -5.8326283 -5.5396223 -5.6425447 -5.8722849 -5.9067698 -6.125711 -6.0284705 -6.0130463 -5.9532595 -6.0109057 -5.980937][-8.6551714 -8.2866583 -7.7277865 -7.2005348 -7.036396 -7.0872874 -7.2330875 -7.4319415 -7.5569267 -7.5165606 -7.3702493 -7.1553636 -7.1341376 -7.0071683 -6.9991632]]...]
INFO - root - 2017-12-15 21:46:49.575087: step 64110, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 49h:17m:16s remains)
INFO - root - 2017-12-15 21:46:55.957048: step 64120, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 47h:28m:30s remains)
INFO - root - 2017-12-15 21:47:02.327897: step 64130, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 48h:43m:44s remains)
INFO - root - 2017-12-15 21:47:08.894866: step 64140, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 48h:48m:51s remains)
INFO - root - 2017-12-15 21:47:15.330267: step 64150, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 49h:08m:12s remains)
INFO - root - 2017-12-15 21:47:21.709319: step 64160, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 46h:53m:41s remains)
INFO - root - 2017-12-15 21:47:28.149041: step 64170, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 47h:26m:34s remains)
INFO - root - 2017-12-15 21:47:34.498732: step 64180, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 46h:40m:30s remains)
INFO - root - 2017-12-15 21:47:40.905668: step 64190, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 46h:05m:14s remains)
INFO - root - 2017-12-15 21:47:47.309967: step 64200, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 46h:42m:20s remains)
2017-12-15 21:47:47.852755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5484314 -1.6198416 -1.8024125 -1.5083666 -1.2250643 -1.0476527 -0.9818759 -1.0579481 -0.81403875 -1.9722424 -1.9690714 -3.8065541 -4.8394737 -6.6346083 -7.6622014][-2.0317488 -2.2534776 -2.4764571 -2.3768396 -2.1703954 -1.6949091 -1.4905066 -1.3457313 -1.2320333 -2.7573395 -3.2120152 -4.8458881 -5.6583261 -6.7649288 -7.3898907][-2.4819717 -2.6486797 -2.4384851 -2.1543856 -1.7990761 -1.19842 -1.292912 -1.7509336 -1.7555704 -2.6869059 -2.9808497 -4.5857549 -5.0902386 -6.3182983 -7.2616415][-2.6342683 -2.6575475 -2.6203198 -2.1323018 -1.3199325 -0.65259457 -0.054335117 -0.018820286 -0.26085472 -1.7093382 -2.2567739 -3.8474891 -4.7233171 -5.9651656 -6.5096722][-3.2121954 -2.853951 -2.4627266 -1.6269441 -0.61343622 0.57304287 1.2651024 1.4250412 1.4265556 0.1278491 -0.75648546 -2.7637959 -3.9719238 -5.1520495 -6.0224118][-3.3862257 -2.5878673 -2.0228357 -1.1808734 -0.39354658 0.59302807 1.4329233 2.2204933 2.6547947 1.4838295 0.63620853 -1.4258614 -2.6730032 -4.4731121 -5.380868][-3.1922088 -2.6947918 -2.2695169 -1.0599203 -0.41002369 0.75675964 1.7496939 2.6056356 3.0174885 2.0071411 1.2309265 -0.80880785 -1.8352323 -3.4134765 -4.4210863][-2.7013931 -2.0940614 -1.6349387 -0.70704556 0.18333769 1.3597317 2.291214 3.2458916 3.7877226 2.7116146 1.7896118 -0.17311335 -1.1855822 -2.2691302 -3.0995011][-1.9192295 -1.7627325 -1.6536045 -0.90393448 -0.37165117 0.41230965 1.2704716 2.2248716 2.8157825 2.06645 1.6820679 -0.24003601 -1.3629375 -2.3673477 -3.1503558][-1.8801651 -2.2221136 -2.2538 -2.13239 -1.6134834 -0.38768291 0.33280659 1.1738491 1.9652472 0.84626389 0.314456 -1.066813 -1.6267099 -2.853879 -3.5018191][-2.7315297 -3.4902687 -4.0149221 -3.4331651 -2.7792921 -1.9134755 -1.0354733 -0.19140053 0.30974388 -0.93409777 -1.2876048 -2.5216784 -3.3590221 -3.7528214 -4.1567917][-3.1758623 -3.6979864 -3.945817 -3.8300459 -3.2511535 -2.224278 -1.7287688 -1.8415108 -1.9426484 -2.653492 -3.0336223 -4.6672835 -5.3377991 -5.34344 -5.5552492][-3.9577665 -4.8024464 -5.3324852 -5.3962879 -4.8833914 -4.167398 -3.8624907 -3.7097123 -3.808434 -4.8694715 -5.3800135 -6.1314492 -6.686554 -7.0504818 -7.0673022][-3.8324537 -4.4341035 -5.1738372 -5.4200768 -5.2226086 -4.4454136 -4.1911688 -4.4203429 -4.7320795 -5.8481665 -6.2491045 -6.7852511 -6.9607086 -6.839581 -7.0085249][-5.5027647 -5.7832823 -6.2653613 -6.1628785 -6.09815 -5.8246922 -5.7622132 -5.9406404 -5.9357615 -6.3861 -6.8156028 -7.3731294 -7.452198 -6.8878922 -6.5433111]]...]
INFO - root - 2017-12-15 21:47:54.273591: step 64210, loss = 0.36, batch loss = 0.24 (12.5 examples/sec; 0.642 sec/batch; 47h:50m:56s remains)
INFO - root - 2017-12-15 21:48:00.668706: step 64220, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 46h:25m:00s remains)
INFO - root - 2017-12-15 21:48:07.144098: step 64230, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.664 sec/batch; 49h:26m:39s remains)
INFO - root - 2017-12-15 21:48:13.576259: step 64240, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 47h:52m:36s remains)
INFO - root - 2017-12-15 21:48:19.972875: step 64250, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 47h:27m:48s remains)
INFO - root - 2017-12-15 21:48:26.388913: step 64260, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 47h:57m:17s remains)
INFO - root - 2017-12-15 21:48:32.901025: step 64270, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 48h:25m:06s remains)
INFO - root - 2017-12-15 21:48:39.357223: step 64280, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 48h:55m:19s remains)
INFO - root - 2017-12-15 21:48:45.828102: step 64290, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 49h:25m:37s remains)
INFO - root - 2017-12-15 21:48:52.242989: step 64300, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 47h:12m:26s remains)
2017-12-15 21:48:52.810144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9280243 -6.7923751 -6.8619432 -7.8727918 -8.4000378 -8.0035591 -7.4513068 -6.1635394 -4.2369642 -4.2390537 -3.1712003 -3.8552954 -5.1743975 -5.4228497 -6.9240165][-6.4125843 -7.0249391 -7.4121766 -8.05941 -8.4449835 -8.3761454 -7.7855043 -6.8212037 -5.5444069 -6.048914 -5.437314 -5.6885853 -6.895781 -6.6519146 -7.0846481][-5.2384853 -6.0588651 -7.2882862 -7.4623485 -7.8293319 -7.4307189 -5.9725714 -5.4836283 -4.7693677 -5.5148044 -6.0173259 -6.1515088 -6.8390036 -6.9466939 -7.5062604][-4.8430176 -4.9835329 -6.0736132 -6.4811935 -5.954226 -4.9486628 -3.9769986 -3.1312871 -1.8307762 -3.181087 -4.0230932 -4.8561974 -5.959815 -6.309269 -6.7958493][-4.4681573 -4.5060673 -4.1922927 -3.8408325 -3.8723238 -2.6594162 -0.71264362 -0.088335514 0.55716515 -0.82474804 -1.7773995 -3.3351297 -4.7100172 -5.6125584 -6.5502486][-4.2423182 -3.6995513 -2.8985 -1.6712537 -0.25999975 0.95186424 1.9629583 2.6417046 3.3711853 1.2338972 -0.15053701 -2.0563898 -3.7219832 -4.4458904 -5.4830289][-4.3687286 -3.4132152 -1.8212981 -0.37502718 1.0212841 2.5132942 3.9636431 4.3555832 4.7029209 2.6620703 1.4571724 -0.84198475 -2.8800197 -3.6944406 -4.9811487][-3.8224542 -2.9610143 -1.0962515 0.74334908 2.4349232 3.3828373 3.9162846 4.44485 4.7332029 2.1870012 0.75142 -1.2542963 -2.871449 -3.9267721 -4.917779][-3.9894228 -2.7941446 -1.5812263 0.11484528 1.6601534 2.2774725 2.8424282 2.7614965 2.2693853 0.066752434 -0.50795794 -2.3004417 -3.9814887 -4.5960932 -5.37349][-3.4186373 -2.995698 -2.3872719 -1.261157 -0.16987228 0.39535522 0.79215908 0.3717804 -0.49176121 -3.0072618 -3.4817619 -4.1803393 -5.1801395 -4.9734993 -5.2211242][-3.7036681 -3.2828698 -3.1620722 -2.7859893 -2.0932169 -1.5703397 -1.3958402 -1.5932431 -2.20568 -4.72458 -5.3016577 -5.3064141 -6.0307112 -5.3985023 -5.3223505][-4.8204718 -4.2765636 -3.8559766 -3.7630718 -3.8060293 -3.8722403 -4.1049209 -4.7701936 -5.015502 -6.3829 -6.9563136 -6.6157126 -6.4878497 -5.1767182 -5.3701239][-5.0230885 -5.311367 -5.2669272 -5.4135714 -5.6032829 -5.5956364 -6.0262046 -6.8058691 -7.2185836 -7.835597 -7.8239212 -7.2433023 -7.1653066 -5.5243225 -4.9815483][-6.6318884 -6.5919886 -6.0999928 -6.3461018 -6.5057297 -6.6868267 -7.4680557 -8.07227 -8.6065416 -8.5958071 -8.260685 -7.271235 -6.5948734 -5.5093784 -4.7876749][-7.6128221 -7.1583257 -6.3183308 -6.3509903 -6.9239922 -7.3278213 -8.0414591 -8.8045988 -9.0524273 -8.8676119 -8.5206175 -7.7394557 -6.8637652 -5.9561749 -5.1004019]]...]
INFO - root - 2017-12-15 21:48:59.210692: step 64310, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 46h:47m:38s remains)
INFO - root - 2017-12-15 21:49:05.694982: step 64320, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 48h:22m:37s remains)
INFO - root - 2017-12-15 21:49:12.117261: step 64330, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.625 sec/batch; 46h:32m:34s remains)
INFO - root - 2017-12-15 21:49:18.520468: step 64340, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 48h:43m:49s remains)
INFO - root - 2017-12-15 21:49:24.998013: step 64350, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 47h:03m:39s remains)
INFO - root - 2017-12-15 21:49:31.401783: step 64360, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 46h:43m:25s remains)
INFO - root - 2017-12-15 21:49:37.809478: step 64370, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 47h:04m:52s remains)
INFO - root - 2017-12-15 21:49:44.244168: step 64380, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 48h:16m:56s remains)
INFO - root - 2017-12-15 21:49:50.675344: step 64390, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 48h:34m:36s remains)
INFO - root - 2017-12-15 21:49:57.062191: step 64400, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 47h:20m:43s remains)
2017-12-15 21:49:57.594376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6171417 -4.127779 -3.4301915 -3.0721259 -3.0304189 -2.895833 -2.6323586 -1.9871378 -1.4971824 -1.8840322 -3.0736475 -3.9483178 -5.0856276 -6.5971684 -6.9903345][-4.27415 -3.9188585 -3.3387127 -2.8988504 -2.9388614 -2.812294 -2.5710473 -2.2280712 -1.6535535 -2.0416985 -3.2689991 -4.0447664 -4.7359304 -6.0811796 -6.5590649][-4.6372061 -4.1692414 -3.6634536 -3.4569931 -3.4171739 -2.7160411 -2.5561562 -2.019825 -2.0371814 -2.4658113 -3.7538676 -4.6306505 -5.0490961 -5.6700687 -5.8967867][-4.761086 -3.9359899 -3.6046553 -3.1375933 -2.834712 -2.2872419 -1.7063618 -1.2097373 -0.614943 -1.3620038 -3.0179458 -3.9889228 -4.9443264 -6.0144973 -6.1209164][-3.9429903 -2.80274 -2.2025642 -2.0557823 -1.4977355 -0.93162775 -0.24031448 0.1551013 0.70087242 0.16684008 -1.3602686 -2.4084492 -3.9392579 -5.3753519 -6.094017][-2.5643573 -1.9121847 -1.1479101 -0.51923323 -0.017882824 0.46127224 0.94765568 1.0485077 1.0528326 0.16195345 -1.5756741 -2.2865496 -3.2658882 -4.6696625 -5.5854874][-1.3948536 -0.90230656 -0.040136337 0.5365324 0.91767883 1.4762878 1.9168568 2.0701227 1.7621946 0.18241215 -1.6854529 -2.5050821 -3.4463506 -4.6633658 -5.2396603][-0.37308598 0.10267258 0.7762804 1.1005888 1.532629 2.2395382 2.8265076 2.8542452 2.7171831 1.3389015 -0.9903841 -2.5431457 -3.8494306 -4.7782526 -5.2355771][-0.51801252 0.1379838 0.98438168 1.3982296 2.0353603 2.4632082 3.0843544 3.0569782 2.8221655 1.4602375 -0.76510239 -2.0738125 -3.6140165 -5.1087751 -5.3901682][-1.3090186 -0.71269989 -0.11875343 0.49462032 1.2926149 2.247263 2.9655561 2.792881 2.2257376 0.70291996 -1.0894628 -2.649497 -3.6542368 -4.6710253 -5.1789026][-2.7092304 -2.3674159 -1.9871297 -1.3172712 -0.64827919 0.36014366 0.97754955 1.0905724 0.69472408 -0.88992643 -2.6437821 -3.5654283 -4.5836072 -5.4615893 -6.1052384][-4.1696606 -3.3485012 -2.1352315 -1.4365864 -0.75347042 0.10039425 -0.043934345 -0.29295683 -0.68092918 -1.8686152 -3.6307492 -4.8029952 -6.02769 -6.7024164 -6.7957411][-4.1998634 -4.297555 -3.5871015 -2.5103545 -1.7193623 -1.0444608 -1.031805 -1.6075974 -2.2995925 -3.1915994 -4.881856 -5.9985051 -7.0235634 -7.3867126 -7.2680082][-3.8900979 -4.2260332 -4.2953167 -4.2714157 -3.8073716 -3.3292599 -3.1970897 -3.2884021 -3.6332989 -4.11471 -5.3931675 -5.9634609 -6.4639626 -6.8979278 -6.9365997][-3.8037391 -3.6457329 -3.6783938 -4.421711 -4.9568343 -4.7265787 -4.679317 -4.9720516 -5.29303 -5.5608797 -5.9919891 -6.3127174 -6.3927155 -6.6337404 -6.9210358]]...]
INFO - root - 2017-12-15 21:50:04.024853: step 64410, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 47h:25m:48s remains)
INFO - root - 2017-12-15 21:50:10.477900: step 64420, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.637 sec/batch; 47h:28m:09s remains)
INFO - root - 2017-12-15 21:50:16.977013: step 64430, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 47h:07m:41s remains)
INFO - root - 2017-12-15 21:50:23.359244: step 64440, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 49h:35m:42s remains)
INFO - root - 2017-12-15 21:50:29.760025: step 64450, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 47h:23m:31s remains)
INFO - root - 2017-12-15 21:50:36.174961: step 64460, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 49h:05m:20s remains)
INFO - root - 2017-12-15 21:50:42.603349: step 64470, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 47h:14m:54s remains)
INFO - root - 2017-12-15 21:50:49.000851: step 64480, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 46h:11m:58s remains)
INFO - root - 2017-12-15 21:50:55.436574: step 64490, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 46h:27m:08s remains)
INFO - root - 2017-12-15 21:51:01.881803: step 64500, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 47h:01m:21s remains)
2017-12-15 21:51:02.350754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4166555 -4.7322421 -4.4238439 -4.28648 -4.0996132 -4.160202 -2.7238259 -2.1989431 -1.405355 -2.9664531 -3.0420175 -4.6114044 -5.759182 -6.5965996 -7.642952][-4.9386387 -4.9129105 -4.6998396 -4.7359953 -4.3835182 -4.0149975 -3.6246772 -3.445931 -2.4114718 -3.2877455 -3.13309 -4.7605844 -5.6075177 -6.5937634 -7.8096719][-4.81117 -4.9559631 -5.1549482 -5.7072353 -5.7461271 -5.3097763 -4.4836731 -3.6083627 -3.3899217 -4.4128056 -4.0501 -5.0289273 -5.9524183 -7.2345424 -8.2439651][-4.834033 -5.1076026 -5.3544312 -5.6146855 -5.7989244 -5.383893 -4.4083509 -3.8447838 -3.2281384 -4.2937927 -3.9171767 -5.0859604 -5.9253435 -6.7044625 -7.685554][-5.1048851 -4.8722429 -4.8949633 -4.6836195 -4.45014 -4.0203295 -2.6833668 -2.0291796 -1.7565084 -3.151825 -3.3238816 -5.0721855 -5.9198356 -6.8198395 -7.550065][-5.5485826 -5.1841373 -5.3193512 -4.4336271 -3.9869576 -3.2120419 -1.7123852 -0.77743244 0.30204153 -1.2972102 -1.8734102 -3.7525938 -5.1094446 -6.4445381 -7.6256189][-5.834116 -5.3678169 -5.6279664 -4.3671894 -3.0741639 -1.5620174 -0.35277176 0.44490814 1.370141 -0.0039081573 -0.72267866 -3.095345 -4.6597819 -6.21561 -7.8108954][-5.2058649 -5.204546 -5.2337141 -4.3325882 -2.8790135 -0.31541824 1.3906708 2.6826792 2.9664364 0.99495411 0.55348396 -1.871604 -4.1615367 -5.7145414 -6.7449265][-4.53217 -4.9304233 -4.3573675 -3.4575744 -2.4983778 -0.88547707 0.99927711 2.4166193 3.263648 2.0512476 0.95575809 -1.6271758 -3.8352237 -5.5077462 -6.8714175][-4.971025 -5.5061121 -5.3462458 -4.4848518 -3.0546808 -0.99385834 0.2180872 1.1635561 2.3072691 0.66622353 0.39649677 -2.0064006 -3.886627 -5.5257406 -6.8469973][-5.1162057 -5.5565643 -5.8198929 -5.2251663 -4.5182748 -3.2028341 -1.7399569 -1.0681977 -0.65323877 -2.5247827 -2.7433252 -4.7422848 -5.52003 -6.4185395 -7.0932956][-5.3687057 -5.1760397 -5.0021286 -4.2944326 -3.7267494 -3.1969018 -2.9152374 -2.9639239 -2.9709587 -4.58969 -4.1714516 -5.7988825 -6.4734793 -7.142293 -7.59471][-6.3647966 -5.576663 -5.6494379 -4.9804125 -4.7155895 -3.8735723 -3.3386068 -3.6857219 -3.8452156 -5.1427355 -5.2899971 -5.9879723 -6.6022167 -7.1666875 -7.084691][-7.6270585 -6.8319716 -6.4476833 -5.5071011 -5.5165415 -4.8125992 -4.4274015 -4.1012783 -3.6776171 -3.9733937 -4.3158264 -5.1334095 -5.9968615 -6.550879 -7.2225065][-7.287416 -7.1892967 -6.8650336 -6.9028211 -6.3849792 -5.8096743 -5.7777767 -5.3900671 -5.2256451 -5.0393953 -4.9524331 -5.4310608 -6.0098124 -6.2994142 -6.9027205]]...]
INFO - root - 2017-12-15 21:51:08.768783: step 64510, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 48h:20m:46s remains)
INFO - root - 2017-12-15 21:51:15.104815: step 64520, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 46h:33m:52s remains)
INFO - root - 2017-12-15 21:51:21.565673: step 64530, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 47h:59m:26s remains)
INFO - root - 2017-12-15 21:51:27.937675: step 64540, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 47h:27m:42s remains)
INFO - root - 2017-12-15 21:51:34.421552: step 64550, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 49h:11m:44s remains)
INFO - root - 2017-12-15 21:51:40.890204: step 64560, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 46h:45m:28s remains)
INFO - root - 2017-12-15 21:51:47.289732: step 64570, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 48h:39m:55s remains)
INFO - root - 2017-12-15 21:51:53.654751: step 64580, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 48h:29m:27s remains)
INFO - root - 2017-12-15 21:52:00.215832: step 64590, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 48h:10m:06s remains)
INFO - root - 2017-12-15 21:52:06.729697: step 64600, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 47h:13m:44s remains)
2017-12-15 21:52:07.256679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5783172 -3.7674747 -4.1913929 -4.7250953 -5.187932 -5.5300264 -5.6776538 -5.368082 -4.9015512 -6.0373154 -6.3281822 -7.0922832 -7.6449041 -8.5833826 -8.8455772][-1.9312449 -2.95224 -4.0295267 -4.975708 -5.6770978 -5.8520632 -5.8058915 -5.3714876 -4.698617 -6.1195936 -6.5998459 -7.445755 -8.1391764 -9.1851406 -9.945178][-0.58776045 -1.4190192 -2.4675045 -3.6677718 -4.2729959 -4.34453 -4.1640291 -3.7456172 -3.1827335 -4.4280643 -4.8304319 -5.77194 -6.505877 -7.6692281 -8.183445][-0.13257885 -0.54243422 -1.4778767 -2.3683348 -2.7265344 -2.5473523 -1.9358087 -1.3802381 -1.0172567 -2.4629202 -3.0190029 -3.8835318 -4.912199 -6.2635813 -6.9692917][0.026424885 -0.2826848 -0.79183674 -1.0714688 -1.1834478 -0.9344244 -0.31898022 0.3323946 0.82439709 -0.92242575 -1.6758933 -2.8529425 -4.0346045 -5.2549639 -5.9576564][-1.6249804 -1.2300682 -1.2248383 -1.0941148 -0.43209076 0.66696644 1.4547863 1.8514709 2.2643442 0.49232292 -0.512238 -1.8279328 -2.8726559 -4.1260595 -4.6968932][-3.2127361 -2.6173773 -2.1772537 -1.2089634 -0.13124609 1.0405407 1.8578291 2.2251005 2.6024075 0.57309914 -0.46739817 -2.0605226 -3.663826 -5.0010805 -5.6668129][-4.2159252 -3.6178584 -2.9405417 -1.5060973 0.040341854 1.2065048 1.8196831 2.0677862 2.3313684 0.15179157 -1.1381817 -2.7503505 -4.15372 -5.8397202 -6.7522669][-4.7807083 -4.177063 -3.4226637 -2.2476063 -0.52717304 0.93548584 1.7159472 2.0437059 2.2784309 -0.33102798 -1.8938155 -3.5724306 -4.9363708 -6.3539419 -7.1492724][-4.9985628 -4.4511223 -3.8141041 -2.5565109 -1.1579747 -0.17192554 0.57129478 1.3606453 1.6197023 -1.0238318 -2.5688205 -4.599772 -5.9552865 -7.3277569 -7.8729587][-5.6598988 -4.9913425 -4.2869821 -3.1581726 -1.9553099 -1.0758748 -0.62620974 -0.48571682 -0.35998821 -2.5020123 -3.8476977 -5.4887915 -6.8086658 -8.1759491 -8.4744253][-5.5219288 -5.1038675 -4.511651 -3.6403213 -2.9328866 -2.2855597 -1.9409852 -1.8125105 -1.790194 -3.5821838 -4.3688631 -5.4167233 -6.2566185 -7.7626872 -8.1848707][-6.256248 -5.4491348 -4.5600767 -3.9061935 -3.3537621 -3.4041691 -3.7509463 -4.1730905 -4.5533161 -5.622488 -6.1051269 -6.7522721 -7.0605512 -7.4468613 -7.2157164][-5.9713578 -5.3799524 -4.508018 -3.6797123 -3.35397 -3.3105736 -3.5700583 -4.0941458 -4.6054859 -5.7067204 -6.0210228 -6.1519537 -6.199512 -6.6473007 -6.4833665][-7.0823274 -6.719213 -5.9942007 -5.1913838 -4.8701515 -4.9529037 -5.3907418 -5.8211164 -6.2961664 -6.6604252 -6.8772078 -6.6494179 -6.3222 -6.14592 -6.1093521]]...]
INFO - root - 2017-12-15 21:52:13.705780: step 64610, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 48h:49m:38s remains)
INFO - root - 2017-12-15 21:52:20.059675: step 64620, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 47h:27m:24s remains)
INFO - root - 2017-12-15 21:52:26.559358: step 64630, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 48h:44m:26s remains)
INFO - root - 2017-12-15 21:52:32.948328: step 64640, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 46h:31m:40s remains)
INFO - root - 2017-12-15 21:52:39.335176: step 64650, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 46h:40m:08s remains)
INFO - root - 2017-12-15 21:52:45.873178: step 64660, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 48h:29m:42s remains)
INFO - root - 2017-12-15 21:52:52.306727: step 64670, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 47h:16m:44s remains)
INFO - root - 2017-12-15 21:52:58.734258: step 64680, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 48h:32m:16s remains)
INFO - root - 2017-12-15 21:53:05.162122: step 64690, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 49h:34m:33s remains)
INFO - root - 2017-12-15 21:53:11.649110: step 64700, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 47h:39m:40s remains)
2017-12-15 21:53:12.183517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2432103 -2.0704961 -2.1317778 -2.6659908 -2.9665828 -3.2602081 -3.2488213 -3.3543019 -3.3601856 -4.6917448 -5.1265049 -6.4698882 -7.3496714 -8.2281084 -9.17981][-2.9648504 -2.85954 -3.0963678 -3.4314427 -3.5626602 -3.3966737 -3.2682104 -3.3773389 -3.2728066 -4.8064103 -5.5968466 -7.2512946 -8.4814949 -9.4192772 -10.20044][-4.3256664 -4.039268 -4.0783777 -4.1205373 -3.8408356 -3.2759991 -2.6581883 -2.519176 -2.2443056 -3.3895869 -3.9309771 -5.3072796 -6.7637115 -7.7563977 -8.9021072][-5.7889609 -4.9914689 -4.5866361 -3.8869338 -2.9872861 -1.9476218 -0.98707962 -0.86468458 -0.75805473 -2.2375259 -2.6908145 -4.1603642 -5.2866626 -6.3765569 -7.5137124][-6.2805562 -5.0402107 -3.9222028 -2.8520012 -2.0596523 -0.5100069 0.79031754 1.2497988 1.3039865 -0.74080276 -1.4495726 -3.160995 -4.4328527 -5.392787 -6.5300655][-5.3658991 -4.8104219 -4.1530786 -2.5021853 -1.0873418 0.59544754 2.0994253 2.7789946 3.1414309 0.93155193 -0.71521044 -2.8267078 -4.1485972 -5.08105 -5.8598619][-5.7488008 -4.809411 -3.7944453 -1.9708085 -0.097628593 1.881382 3.3541527 3.9464426 4.3571243 2.105751 0.43672276 -2.4983358 -4.6687174 -5.7523489 -6.3750505][-5.7226019 -4.6013951 -3.2952323 -1.434226 0.4379797 2.5959435 3.8758354 4.3229647 4.4505396 2.036931 0.50109386 -2.4339695 -4.3656626 -6.0847888 -6.7373352][-4.5230608 -4.0459595 -3.2581196 -1.6628141 -0.32666779 1.3474579 2.524745 3.3654919 3.5976171 1.0914841 -0.19379473 -2.759624 -4.3929048 -5.8211241 -6.75844][-4.6821194 -4.1744223 -3.9931748 -2.799921 -1.82054 -0.44714928 0.50616455 1.0883579 1.4095964 -0.83238649 -2.0251317 -4.443 -5.6204081 -6.4991474 -7.0344095][-6.8285174 -6.5534496 -6.2572145 -5.3162937 -4.3416524 -2.9368215 -1.8271017 -1.2217302 -0.9326005 -3.0674191 -4.3325672 -6.2881684 -7.2550693 -8.0604239 -8.2940989][-7.4655328 -7.2300606 -7.0603676 -6.490849 -5.9672785 -5.1174545 -4.6143713 -3.8878789 -3.4530244 -4.67475 -5.3284073 -6.6905704 -6.998457 -7.5837846 -7.8887806][-7.2167468 -7.3875842 -7.4500213 -7.1216269 -6.9116111 -6.458199 -6.2149792 -5.944366 -5.757163 -6.4761477 -6.5847516 -7.1651907 -7.3197064 -7.1854887 -7.0877085][-7.3360076 -7.3850436 -7.4415717 -7.0956926 -6.8881779 -6.4033189 -6.48716 -6.5650358 -6.5368633 -7.0817156 -7.14054 -7.2230186 -7.0430021 -7.2113152 -7.1233253][-8.5898914 -8.5244818 -8.2483444 -7.9752078 -7.8707356 -7.7063766 -8.01784 -7.9756255 -8.0333252 -7.9913468 -7.779 -7.67572 -7.3495688 -7.0678678 -6.7671185]]...]
INFO - root - 2017-12-15 21:53:18.572091: step 64710, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:42m:34s remains)
INFO - root - 2017-12-15 21:53:25.036661: step 64720, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 48h:35m:13s remains)
INFO - root - 2017-12-15 21:53:31.395149: step 64730, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 49h:04m:34s remains)
INFO - root - 2017-12-15 21:53:37.819586: step 64740, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 47h:35m:16s remains)
INFO - root - 2017-12-15 21:53:44.327265: step 64750, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 46h:50m:23s remains)
INFO - root - 2017-12-15 21:53:50.847107: step 64760, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.673 sec/batch; 50h:04m:46s remains)
INFO - root - 2017-12-15 21:53:57.258211: step 64770, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 48h:20m:51s remains)
INFO - root - 2017-12-15 21:54:03.749458: step 64780, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:15m:10s remains)
INFO - root - 2017-12-15 21:54:10.252293: step 64790, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 48h:11m:07s remains)
INFO - root - 2017-12-15 21:54:16.742310: step 64800, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 46h:54m:11s remains)
2017-12-15 21:54:17.227467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7371392 -5.2805872 -4.5160809 -3.8255084 -3.4830322 -3.1213398 -2.8151808 -2.7526321 -2.4007235 -3.7209759 -4.7448111 -5.5802627 -6.8801336 -7.3316545 -8.1618214][-4.7788382 -4.3791528 -3.9234438 -3.4317985 -3.0890617 -2.899096 -2.7857566 -2.8360176 -3.0520744 -4.3999028 -5.3952656 -5.9404354 -6.8740182 -7.2739878 -7.9189262][-3.8911674 -3.4625282 -3.1342263 -2.9132752 -2.720459 -2.4910955 -2.3187957 -2.3688421 -2.3617096 -3.8846836 -5.0396132 -5.7249393 -6.7171926 -7.031651 -7.7370176][-3.4289293 -3.0843868 -2.8090558 -2.3111024 -1.9018054 -1.8417273 -1.7159081 -1.8182101 -1.7644415 -3.234076 -4.2738037 -5.1028833 -6.2392211 -6.7128148 -7.4160919][-3.2926111 -2.2538719 -1.2253809 -1.046257 -1.0681143 -0.917212 -0.62660122 -0.79025507 -0.88447857 -2.5046601 -3.5998263 -4.2531095 -5.437892 -6.1170506 -6.7538738][-2.7667646 -2.2513738 -1.6462069 -0.91733408 -0.2158885 0.1359477 0.33720207 0.17168379 0.17283916 -1.5344901 -2.7951689 -3.6521726 -4.9097037 -5.50734 -6.3711867][-2.5614557 -2.0135322 -1.2403741 -0.66275263 -0.11551762 0.56508064 1.2291059 1.2416611 1.2153606 -0.46644735 -1.8008924 -2.8004236 -4.0781765 -4.847641 -5.8251848][-2.0675049 -1.318121 -0.85689831 -0.16125679 0.76084614 1.2971964 1.6532068 1.9111996 2.3632069 0.68385315 -0.73707294 -1.8982625 -3.5570278 -4.5943966 -5.7128792][-2.2479033 -1.6851983 -0.7737565 0.33623886 1.0244799 1.4572868 1.7452726 1.8462582 2.0994978 0.3828125 -1.0437627 -2.2177482 -3.7291548 -4.7407 -5.8618531][-2.7783618 -1.7645974 -1.1665688 -0.29595327 0.75388622 1.2713547 1.3368664 1.3307352 1.3166571 -0.73833513 -2.024662 -3.4587154 -4.9572277 -5.6948462 -6.4800386][-3.7486067 -3.0126681 -2.2773 -1.2412291 -0.50841331 -0.068224907 0.043018818 -0.11436033 -0.26121855 -2.1710343 -3.6612306 -4.8462944 -6.1758537 -6.8619895 -7.3729439][-4.3801985 -3.6194177 -2.9471984 -2.009367 -1.0546169 -0.39280081 -0.41279364 -0.705533 -1.0757561 -2.75488 -4.1204171 -5.2929497 -6.5986261 -7.2497725 -7.7295365][-5.0153837 -4.5221453 -4.2774248 -3.5410986 -2.8758035 -2.1317978 -1.9696116 -2.3154616 -2.78363 -3.9567823 -5.1116266 -5.8005724 -6.5131283 -6.889256 -7.1997509][-4.9988527 -4.5375409 -4.3966832 -4.0171194 -3.7538202 -3.3805685 -3.3555059 -3.6750221 -3.8962142 -4.579092 -4.9012508 -5.3394241 -5.9436851 -6.3233242 -6.5225267][-5.6715732 -5.6739616 -5.3116612 -5.2162848 -5.4803333 -4.9763651 -4.9943519 -5.022017 -5.4948797 -5.5540638 -5.3948388 -5.6542377 -5.7183104 -5.7580781 -5.6347771]]...]
INFO - root - 2017-12-15 21:54:23.679887: step 64810, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 49h:56m:51s remains)
INFO - root - 2017-12-15 21:54:30.105283: step 64820, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 47h:07m:54s remains)
INFO - root - 2017-12-15 21:54:36.536318: step 64830, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 46h:01m:31s remains)
INFO - root - 2017-12-15 21:54:42.918475: step 64840, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 46h:28m:15s remains)
INFO - root - 2017-12-15 21:54:49.370221: step 64850, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 47h:56m:05s remains)
INFO - root - 2017-12-15 21:54:55.796660: step 64860, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 46h:58m:11s remains)
INFO - root - 2017-12-15 21:55:02.153370: step 64870, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 48h:45m:56s remains)
INFO - root - 2017-12-15 21:55:08.619065: step 64880, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 48h:57m:25s remains)
INFO - root - 2017-12-15 21:55:15.075117: step 64890, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 48h:23m:01s remains)
INFO - root - 2017-12-15 21:55:21.456425: step 64900, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 46h:52m:57s remains)
2017-12-15 21:55:21.939642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5435066 -5.0948191 -5.285944 -6.0407457 -6.2628059 -6.4095068 -6.5615263 -6.2824955 -6.2495208 -6.6082568 -7.3595395 -9.1195889 -9.9437771 -10.71043 -11.436211][-3.8044639 -5.1540036 -6.8527513 -7.4096923 -8.048893 -8.0080175 -8.1887684 -7.8713508 -7.1325469 -6.9164886 -7.5498185 -8.8765621 -9.512989 -10.435554 -11.200219][-3.531137 -4.1071768 -4.6813345 -5.8066316 -6.9728122 -7.5132713 -7.8649492 -6.8845878 -5.9766293 -5.889576 -6.2402687 -7.5424714 -8.6979074 -9.5694809 -10.666546][-4.9827557 -4.3847733 -3.8648493 -3.8104653 -4.3115358 -4.6801662 -4.8062868 -4.7787333 -4.3810191 -4.0686693 -4.7594147 -6.3378372 -7.3182826 -8.7126942 -9.9509792][-4.5040045 -3.5791512 -3.01192 -2.9434681 -2.7357883 -2.0382156 -1.8082538 -1.6441789 -1.3880868 -2.2903914 -3.802146 -5.9089565 -7.2642097 -8.36241 -9.2187424][-3.9060771 -3.9090595 -3.2313986 -2.3725839 -1.5811062 -0.88621092 0.32683945 0.725441 0.78359127 -0.52730751 -2.4104757 -4.8558464 -6.5442934 -7.6228924 -8.6266508][-3.2192836 -3.4326024 -3.2491817 -2.1590605 -0.501245 0.58695793 1.7248516 2.1960783 2.7366571 1.676796 -0.67706537 -3.729455 -5.7260251 -7.118762 -8.418745][-4.3554363 -4.0242944 -2.8751378 -1.5680876 -0.20666027 1.1837587 2.5933971 2.7993965 2.9413414 1.9478664 -0.2958622 -3.1512547 -4.795001 -6.4375129 -7.9057364][-4.2728848 -4.4211802 -3.663487 -2.2527204 -1.265626 0.29141378 2.0758572 2.5958643 2.9433155 1.0353212 -1.5502992 -4.2444296 -5.8071389 -6.7058725 -7.7544284][-5.6180859 -5.721036 -5.7027464 -4.8448963 -3.7219186 -2.3997493 -1.1302271 -0.33324528 0.5510149 -1.1957059 -3.1263232 -5.3293285 -6.8774381 -7.5297971 -8.179388][-8.614336 -8.7936335 -8.3038568 -7.2984262 -6.1167 -4.8553362 -3.273284 -2.8099298 -2.4907384 -3.4102225 -4.6733952 -6.474781 -7.5413761 -7.9388981 -8.4928408][-9.0123806 -9.1172962 -9.1291771 -8.3125668 -7.7573481 -6.64268 -5.1555481 -4.6772609 -4.0487485 -4.67505 -5.5287209 -6.666368 -7.0918827 -7.37209 -7.5871916][-9.4933681 -9.7501717 -9.7870569 -9.13624 -8.3260727 -7.4189596 -6.3701162 -6.1334834 -5.7615623 -5.9223986 -6.2042975 -6.520226 -7.0568585 -7.2181096 -7.1675153][-9.5454159 -9.6131792 -9.3854389 -8.966671 -8.4592915 -7.690414 -6.2356157 -6.1269569 -6.2681613 -6.46807 -6.9037342 -6.6568222 -6.5370016 -6.8394232 -6.803031][-9.8484983 -10.508448 -10.787617 -10.356771 -10.018243 -9.3031874 -8.1990328 -7.7164903 -7.4672203 -7.65212 -8.0327063 -8.3205414 -7.9372287 -7.5624161 -6.8976049]]...]
INFO - root - 2017-12-15 21:55:28.363631: step 64910, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 48h:13m:01s remains)
INFO - root - 2017-12-15 21:55:34.728558: step 64920, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:23m:41s remains)
INFO - root - 2017-12-15 21:55:41.144595: step 64930, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 46h:45m:18s remains)
INFO - root - 2017-12-15 21:55:47.569832: step 64940, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 46h:22m:37s remains)
INFO - root - 2017-12-15 21:55:53.958384: step 64950, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 46h:37m:26s remains)
INFO - root - 2017-12-15 21:56:00.402517: step 64960, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 48h:26m:51s remains)
INFO - root - 2017-12-15 21:56:06.969685: step 64970, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.675 sec/batch; 50h:10m:51s remains)
INFO - root - 2017-12-15 21:56:13.397589: step 64980, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 48h:42m:30s remains)
INFO - root - 2017-12-15 21:56:19.758618: step 64990, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 46h:58m:35s remains)
INFO - root - 2017-12-15 21:56:26.126305: step 65000, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 48h:14m:41s remains)
2017-12-15 21:56:26.645090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3887653 -1.4101601 -1.2884336 -1.4018788 -1.5745387 -2.0469308 -2.6872935 -3.4278083 -3.81868 -5.1699085 -5.880096 -6.7644663 -7.1809464 -7.8821306 -8.3937817][-1.782548 -1.9319863 -2.5168 -2.5233464 -2.9623094 -3.2490926 -3.4786916 -3.5576444 -3.747715 -5.7564197 -7.0272312 -8.1139565 -8.6396389 -9.1119537 -9.1255131][-3.5526619 -2.8816304 -2.505621 -2.2183657 -2.1989813 -1.8663411 -1.7955332 -2.398737 -2.5621023 -4.432848 -5.4048491 -7.0085974 -8.076602 -8.7243443 -9.0111609][-4.0555196 -3.7162237 -3.2990227 -2.4923668 -1.6087389 -0.77012968 -0.16163015 -0.3102479 -0.59995317 -2.5351229 -3.908447 -5.5710344 -6.8250918 -7.6422448 -8.2034788][-4.89338 -3.8105762 -2.8731399 -2.0565124 -1.5466857 -0.6566515 0.13705969 0.52688026 0.77386665 -1.1833634 -2.6145382 -4.4723654 -5.8287768 -6.9347277 -7.623075][-5.0918179 -3.8593254 -3.1786804 -1.8691039 -0.97419119 0.30633068 1.5164213 1.4531956 1.4823332 -0.56949282 -2.0949326 -4.27756 -5.7281904 -6.70142 -7.2323475][-4.8750238 -3.8419273 -2.6791034 -0.82281685 0.79196835 1.9450312 2.878767 3.551218 4.0601463 1.6392593 -0.38651848 -3.1596432 -5.0013022 -6.5966859 -7.4445992][-4.6222534 -3.4711504 -1.9949999 -0.10234213 1.5300016 3.2386475 3.988925 4.3021784 4.2547817 2.1026068 0.70820141 -2.0260105 -3.6410303 -5.75983 -7.1483212][-4.4620647 -3.5470285 -2.7744155 -0.84835196 0.49456406 1.8609619 3.1358223 3.661294 4.2469635 1.6389637 -0.98363495 -3.038136 -4.3848476 -6.11123 -7.1476703][-5.7629557 -4.8686171 -3.8806162 -2.5892873 -1.3897986 -0.23169041 0.9575882 1.6289692 2.2454004 0.33530045 -1.6553693 -4.2145543 -5.763876 -7.06555 -7.6833916][-6.5760651 -6.2654552 -5.7158875 -4.4712105 -3.4195652 -1.9225459 -0.660182 -0.18835688 -0.15422153 -2.2096696 -3.6468334 -4.8038263 -5.5196352 -6.8322377 -7.6542268][-7.325892 -6.7040734 -6.2925496 -5.6953731 -5.1705303 -4.2606306 -3.376884 -2.6475616 -1.954247 -3.068007 -4.1404619 -5.0395317 -5.9861732 -6.4790864 -7.0271368][-8.303791 -7.8149476 -7.4352517 -6.967618 -6.6272168 -6.1107588 -5.4831295 -5.2764711 -5.0717058 -5.8154988 -5.9535251 -6.22066 -6.5677905 -7.0761075 -7.3797317][-8.2970181 -7.7224488 -7.1321878 -6.7679305 -6.4113388 -6.4297085 -6.4899082 -6.5090055 -6.3930082 -6.8629632 -7.0597425 -7.1724763 -7.0333843 -6.6910729 -6.7185574][-9.7295389 -9.1090012 -8.7346983 -8.3372145 -7.6428547 -7.3428273 -7.01548 -7.4473257 -7.94638 -8.0071163 -7.8180809 -7.5399413 -7.1450491 -6.7312379 -6.5827303]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 21:56:34.245886: step 65010, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 46h:45m:45s remains)
INFO - root - 2017-12-15 21:56:40.685563: step 65020, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 47h:21m:44s remains)
INFO - root - 2017-12-15 21:56:47.204481: step 65030, loss = 0.29, batch loss = 0.17 (11.4 examples/sec; 0.700 sec/batch; 51h:58m:43s remains)
INFO - root - 2017-12-15 21:56:53.626031: step 65040, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 48h:05m:10s remains)
INFO - root - 2017-12-15 21:57:00.055002: step 65050, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:10m:31s remains)
INFO - root - 2017-12-15 21:57:06.444660: step 65060, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 48h:40m:42s remains)
INFO - root - 2017-12-15 21:57:12.853937: step 65070, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 47h:01m:47s remains)
INFO - root - 2017-12-15 21:57:19.271991: step 65080, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 48h:01m:03s remains)
INFO - root - 2017-12-15 21:57:25.686837: step 65090, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 46h:52m:12s remains)
INFO - root - 2017-12-15 21:57:32.075452: step 65100, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 46h:38m:42s remains)
2017-12-15 21:57:32.651295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6901197 -3.819809 -3.9357431 -4.0036173 -3.9051034 -4.14592 -4.3954887 -4.6352806 -4.5903387 -4.8590231 -6.4948621 -7.8531351 -8.0739851 -8.546771 -8.9949636][-3.2090654 -3.9574177 -4.98442 -5.455555 -5.8014836 -5.9124928 -6.0756955 -6.18988 -5.7749019 -5.7603855 -6.9860225 -8.2384272 -8.7046595 -9.1019173 -9.3903332][-4.1107512 -4.5805674 -4.7520895 -5.2553606 -5.6338358 -5.8839622 -6.3620324 -6.7901316 -6.2475114 -6.0428495 -7.4467354 -8.4781857 -8.292779 -8.9864016 -9.4700985][-4.1098146 -4.5381889 -4.6562634 -4.505487 -4.5383725 -4.8241014 -5.0449972 -5.4665041 -5.417264 -4.8329935 -6.19947 -7.5446692 -7.4130845 -7.9052286 -8.5905418][-4.247714 -4.4360962 -4.0781612 -4.0964737 -3.7398236 -3.3308587 -3.1772428 -3.2762518 -3.2632489 -2.8222322 -4.646637 -6.0452762 -6.44213 -7.2536798 -7.795897][-3.9879858 -4.0042562 -3.6785455 -3.0013752 -2.2763124 -1.579546 -0.5886178 -0.30558014 -0.27667284 -0.30991173 -2.4284806 -3.6228967 -4.1956081 -5.2052312 -5.9308567][-3.9889424 -4.0428429 -3.3508778 -1.9099965 -0.62811327 0.36259937 1.1814241 1.7322721 2.3273325 2.0052347 -0.31609297 -1.3162079 -1.7460265 -2.902987 -4.4376106][-4.1775923 -3.7621441 -2.9453321 -1.3154154 0.36569977 1.7593546 2.9658604 3.3885231 3.4804764 3.1751852 1.6377258 0.19903708 -0.62265253 -2.0684161 -3.4929504][-4.2128611 -3.9925809 -2.9540348 -1.6097412 -0.090702057 1.4867277 2.8224449 3.0967636 2.865448 2.2088518 0.90731716 0.0096154213 -0.40502405 -1.978714 -3.5638108][-4.5554161 -4.5286894 -3.7893739 -2.3842731 -1.2089586 0.011122704 1.143384 1.5324497 1.7593231 1.0334969 -0.56207228 -1.861043 -1.9155192 -2.511703 -3.352181][-6.3134475 -5.7639513 -4.9327569 -4.0220432 -2.8808398 -1.7143435 -0.5547533 -0.40437269 -0.728539 -1.1084323 -1.7119927 -3.0280075 -4.0164242 -4.6371164 -5.0227795][-6.900569 -6.7164545 -6.0834618 -5.4408941 -4.9446349 -4.0868015 -3.1003537 -2.7745643 -2.786509 -3.677227 -4.5097156 -5.04603 -5.0306416 -5.7865996 -6.4791808][-7.1836958 -6.7383094 -6.6013088 -6.7627439 -6.7943 -6.3177385 -5.5878053 -5.212594 -4.7452588 -4.9038486 -5.5088253 -6.0188069 -6.0494118 -6.1367469 -6.2372894][-7.2020578 -7.079721 -7.0718102 -6.898654 -6.9067369 -6.8270116 -6.4077411 -6.23946 -6.2351303 -6.32238 -6.2156458 -5.80535 -5.2899017 -5.700799 -6.02915][-7.1941996 -7.6403508 -7.9661031 -8.0302563 -7.6427283 -7.5258245 -7.4354305 -7.251472 -7.0990167 -7.3246584 -7.3382697 -6.8575497 -6.2617812 -6.0572844 -6.2147961]]...]
INFO - root - 2017-12-15 21:57:39.119423: step 65110, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 47h:15m:00s remains)
INFO - root - 2017-12-15 21:57:45.530868: step 65120, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.614 sec/batch; 45h:36m:25s remains)
INFO - root - 2017-12-15 21:57:51.909102: step 65130, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 46h:20m:06s remains)
INFO - root - 2017-12-15 21:57:58.270842: step 65140, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 45h:59m:27s remains)
INFO - root - 2017-12-15 21:58:04.655899: step 65150, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 47h:42m:07s remains)
INFO - root - 2017-12-15 21:58:11.186563: step 65160, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:26m:06s remains)
INFO - root - 2017-12-15 21:58:17.590266: step 65170, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 46h:41m:59s remains)
INFO - root - 2017-12-15 21:58:23.999194: step 65180, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 48h:25m:33s remains)
INFO - root - 2017-12-15 21:58:30.429137: step 65190, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 48h:29m:07s remains)
INFO - root - 2017-12-15 21:58:36.827200: step 65200, loss = 0.23, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 47h:04m:39s remains)
2017-12-15 21:58:37.381096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4184647 -4.8600678 -5.22089 -5.981493 -6.406909 -7.0070262 -7.2860031 -6.9987087 -6.3713546 -5.6708479 -5.1819873 -5.4542403 -5.9369478 -6.0866008 -6.2650642][-4.76619 -4.5041418 -4.8187695 -5.1455364 -5.7549887 -7.0961542 -7.5554185 -7.5629983 -6.6843867 -6.2150631 -5.4194293 -5.4895525 -5.7545366 -6.004384 -6.0783272][-5.4118958 -4.9444017 -4.7627831 -4.4776773 -4.7222729 -5.5757837 -6.3864336 -6.6905761 -6.3349252 -6.3505859 -5.7885327 -6.2956409 -6.7388968 -7.1496596 -7.1320128][-6.2908688 -5.2566271 -4.2928143 -3.0563788 -2.571207 -2.6030397 -2.937243 -3.3518057 -3.2468767 -3.5659127 -3.6309609 -4.7158489 -5.9967647 -6.6698737 -7.0521817][-6.6699319 -4.8423734 -3.1430168 -1.2172346 -0.49661398 0.034430981 0.2548151 -0.023486614 0.25898457 0.008831501 -0.80384064 -2.8153572 -4.7474051 -6.0559635 -6.5694275][-5.3801336 -3.90723 -1.9586601 -0.34949493 0.74163818 1.5257864 1.9673719 1.9227304 2.3314762 1.690444 0.10287809 -2.2608175 -4.4846611 -5.9084864 -6.2904825][-3.9999516 -2.5825968 -0.7533102 0.86626053 2.2026339 3.1236629 3.4544535 3.374959 3.2932453 2.23213 0.25552082 -2.1412196 -4.1298552 -5.5811405 -6.1691995][-3.5134411 -1.7003818 -0.42934752 1.2478333 2.3254671 2.8872328 3.5102997 3.6028509 3.8679838 2.8072195 0.97023773 -1.6529412 -3.6262236 -5.2996855 -5.8707938][-3.8269761 -2.2864294 -1.0826063 0.32011986 0.80241203 1.3741226 1.6117592 1.7786627 2.1499166 0.8555088 -0.7071619 -2.920536 -5.0451856 -6.4527869 -6.6090407][-3.7592959 -3.2932482 -2.3971472 -1.4668641 -0.95217371 -0.53002405 -0.19257879 -0.12095356 -0.26430273 -1.486897 -2.9684777 -4.3863525 -6.1881342 -7.1709223 -7.6811013][-6.0445313 -5.3550081 -4.7657204 -3.9532192 -3.1942677 -2.7080665 -2.1985149 -2.5621438 -3.0988455 -3.8317168 -4.9212074 -5.6558208 -6.8308578 -7.2420006 -6.8267174][-8.5330229 -7.8600559 -7.4526148 -6.637785 -5.9313979 -5.219965 -4.9850321 -5.4842663 -5.4520493 -6.1220188 -6.7693696 -7.1548281 -8.24225 -8.1822958 -7.0728226][-10.096704 -10.072567 -9.6888256 -8.6121588 -7.6124563 -6.558526 -5.9638357 -6.0590315 -6.190711 -6.3716068 -7.117137 -7.1497126 -7.3647804 -7.2065082 -6.6847334][-9.287385 -9.4583578 -8.8648767 -7.8787332 -6.9505215 -5.7849574 -5.2369709 -4.8107347 -5.1088305 -5.10273 -5.4548235 -5.6503458 -6.0637255 -6.053134 -5.5968304][-9.6443348 -9.0170393 -8.869381 -8.3479137 -7.3864942 -6.26444 -5.7524734 -5.4247894 -5.2833033 -5.1324482 -5.275209 -5.2869396 -5.4564867 -5.9945893 -6.0217228]]...]
INFO - root - 2017-12-15 21:58:43.770155: step 65210, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 46h:38m:23s remains)
INFO - root - 2017-12-15 21:58:50.252643: step 65220, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 46h:35m:58s remains)
INFO - root - 2017-12-15 21:58:56.631508: step 65230, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 46h:15m:39s remains)
INFO - root - 2017-12-15 21:59:02.981398: step 65240, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 47h:09m:53s remains)
INFO - root - 2017-12-15 21:59:09.382730: step 65250, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 48h:20m:17s remains)
INFO - root - 2017-12-15 21:59:15.761119: step 65260, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 45h:54m:09s remains)
INFO - root - 2017-12-15 21:59:22.042145: step 65270, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:21m:03s remains)
INFO - root - 2017-12-15 21:59:28.403150: step 65280, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 47h:20m:24s remains)
INFO - root - 2017-12-15 21:59:34.784738: step 65290, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 46h:28m:00s remains)
INFO - root - 2017-12-15 21:59:41.207885: step 65300, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.675 sec/batch; 50h:07m:14s remains)
2017-12-15 21:59:41.720703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.380754 -6.7689576 -6.8257008 -6.664546 -6.5945392 -6.1164093 -5.1590939 -4.5221553 -4.5741296 -5.0012512 -6.4741483 -7.0068307 -7.39373 -8.48344 -8.2364855][-5.6166296 -6.6073151 -7.4816556 -7.9729562 -7.7475891 -7.40716 -6.2868576 -4.9142904 -4.0042429 -4.2547588 -5.9251652 -7.4304881 -8.2266636 -8.8951254 -9.3966389][-5.130383 -6.1743941 -6.5293889 -7.4270892 -8.3585 -8.3049908 -7.0447755 -5.28384 -4.2250128 -3.9315042 -4.8373742 -5.6077566 -6.4603815 -7.9254251 -8.1101561][-4.0480208 -4.944119 -5.4730406 -5.9027228 -6.2974529 -5.8269296 -5.5016007 -4.6172543 -3.4155931 -2.8088098 -4.0124035 -5.0289621 -5.3085928 -6.5500131 -7.73342][-5.6422739 -5.196435 -4.3735256 -4.3732376 -4.2373266 -2.9840646 -1.9787431 -1.2823691 -0.800529 -1.4341998 -2.8584714 -4.0959239 -4.6776404 -6.3495026 -7.2675314][-5.8736944 -5.7260761 -4.1707821 -2.9541569 -1.6462979 0.41697598 1.6984015 1.989089 1.9433661 0.20972872 -2.2535863 -4.4320574 -5.0538158 -6.1592512 -7.0228462][-5.2863679 -4.2316914 -3.4454765 -1.741035 0.62047577 2.6261969 4.19281 4.418952 4.114028 1.4332333 -1.9637256 -4.6384363 -5.6925235 -6.7523403 -6.9305449][-4.4203963 -3.8963623 -2.1813803 -0.29537678 1.3720236 3.7791014 5.1383247 5.4740124 4.9429502 2.1893291 -1.1112218 -4.159132 -5.5749369 -6.9584846 -7.0697756][-4.5192337 -3.5507956 -3.1518173 -1.6527958 0.40833187 2.2052488 3.4303627 3.9230642 3.9123659 1.7550535 -2.0391331 -4.4947071 -5.3992786 -6.7569809 -6.9842587][-5.7294378 -4.8450241 -3.6506147 -2.6987596 -2.00461 -0.3326354 0.57962036 1.1474018 1.588541 -0.575531 -2.8677678 -5.6396456 -6.9769645 -7.8042903 -7.61196][-6.87171 -6.801198 -6.342021 -5.3824034 -4.853 -3.9949467 -3.0464144 -2.7518649 -2.5110116 -3.9025483 -5.7299609 -7.3455715 -7.7453456 -9.0205345 -9.1365538][-7.6011596 -7.5617371 -7.6548247 -7.1871858 -6.8462343 -6.3522925 -6.1291876 -5.8213811 -5.4900694 -6.2367239 -6.8017282 -7.9562907 -8.257412 -8.375308 -8.2474756][-7.9774871 -8.7193012 -8.396327 -7.5859947 -6.9784031 -6.9665937 -6.4640427 -6.8391385 -7.283246 -7.7491522 -8.3008146 -8.3941469 -8.0508261 -8.0763769 -8.0067282][-7.5742893 -7.9890976 -7.9415188 -7.1477122 -6.4557462 -5.9709277 -5.8293447 -5.9341946 -6.1479497 -6.684041 -7.0704713 -7.4876685 -7.311235 -7.1886897 -7.1318269][-8.534009 -8.17474 -7.9570451 -7.4339275 -6.9919367 -6.4572825 -6.143302 -5.8654995 -6.3559237 -6.7123423 -6.8943934 -6.914598 -6.8092537 -6.3408289 -5.9620209]]...]
INFO - root - 2017-12-15 21:59:48.083431: step 65310, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 46h:38m:31s remains)
INFO - root - 2017-12-15 21:59:54.468512: step 65320, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 45h:52m:38s remains)
INFO - root - 2017-12-15 22:00:00.946450: step 65330, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 46h:41m:31s remains)
INFO - root - 2017-12-15 22:00:07.356254: step 65340, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 48h:02m:13s remains)
INFO - root - 2017-12-15 22:00:13.769145: step 65350, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 48h:24m:42s remains)
INFO - root - 2017-12-15 22:00:20.212061: step 65360, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 46h:54m:22s remains)
INFO - root - 2017-12-15 22:00:26.670626: step 65370, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 49h:09m:10s remains)
INFO - root - 2017-12-15 22:00:33.137811: step 65380, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:02m:18s remains)
INFO - root - 2017-12-15 22:00:39.529844: step 65390, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 46h:58m:07s remains)
INFO - root - 2017-12-15 22:00:45.907491: step 65400, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 47h:53m:47s remains)
2017-12-15 22:00:46.480823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1763525 -1.8892164 -2.1008592 -1.858665 -1.9039726 -1.9042501 -2.0388856 -1.9356117 -1.8628864 -3.3946652 -3.9849181 -5.0134745 -6.3842731 -7.5853977 -8.43973][-2.0923729 -1.9392371 -1.9837956 -2.0458188 -1.9705019 -1.7299881 -1.9942636 -1.9235001 -1.8049316 -3.03445 -3.4783463 -4.5778246 -5.8328266 -7.024662 -7.9628916][-2.2054238 -1.705337 -1.3407478 -1.1250739 -1.0167046 -0.86270952 -1.1764255 -1.062294 -0.83121681 -1.9920588 -2.4403138 -3.4263992 -4.4998097 -5.6233091 -6.5483751][-2.1693301 -1.2390456 -0.52019119 -0.50496483 -0.32498837 0.04948473 0.16727829 0.34148026 0.563941 -0.78086424 -1.557025 -2.6127925 -3.648603 -4.5758572 -5.4333735][-2.6058173 -1.369657 -0.45366859 -0.13766003 0.45299816 1.0904608 1.732789 1.8037176 1.6553221 0.14193869 -0.83502245 -2.0484881 -3.1923103 -4.1773796 -5.0314927][-2.8217812 -1.7798252 -0.8424716 -0.17840052 0.6779871 1.4686403 2.2443647 2.2657709 2.1741829 0.5758276 -0.50144625 -1.8062372 -3.2238808 -4.275919 -5.1685953][-2.9460769 -2.1489296 -1.3599515 -0.25079536 1.0026197 1.7337027 2.6122379 2.6571703 2.6544733 0.84560776 -0.34868336 -1.865448 -3.4734106 -4.7483 -5.6513987][-2.696053 -1.8255515 -1.0106478 0.10790539 1.3535738 2.2342978 3.1328745 3.1998291 3.1958942 1.1339149 -0.13244438 -1.8507643 -3.636898 -4.9138956 -5.8519945][-2.7271161 -1.8352742 -1.075901 -0.14198112 0.63950157 1.7299376 2.7012043 2.7787676 2.8732224 0.98468494 0.090771675 -1.665946 -3.6713271 -5.1173944 -6.0945888][-2.7895508 -1.9819226 -1.158637 -0.37189436 0.031713009 1.0733213 1.8558531 1.6788607 1.7263298 -0.0642457 -0.99572229 -2.3026543 -3.6687226 -5.5018883 -6.7389531][-3.7549267 -2.9653416 -2.1687727 -1.1212087 -0.70535278 0.10665178 0.7272253 0.75579643 0.60199928 -1.0481944 -1.8991098 -3.057632 -4.1111908 -5.6662922 -6.9387832][-5.1176023 -4.1034422 -3.3744264 -1.9959722 -1.1210713 -0.5633688 -0.34908772 -0.42572117 -0.64188671 -1.9669924 -2.9760222 -4.15777 -4.9066558 -5.8802843 -6.671195][-5.467145 -4.3587823 -3.5628624 -2.5056672 -1.5457387 -1.0894666 -1.0344391 -1.1587725 -1.4870667 -2.7628169 -3.7153902 -4.7653813 -5.4494591 -6.2323322 -6.7374277][-6.1864662 -4.9001141 -3.9946351 -3.4706025 -2.706799 -1.8056083 -1.5951252 -1.6031656 -1.7495193 -2.5844011 -3.4644485 -4.7040873 -5.6918306 -6.4182849 -6.9433274][-7.3252811 -5.4701943 -4.0697689 -4.1989007 -3.914938 -2.7859707 -2.4130945 -2.6048341 -2.8074732 -3.4558573 -3.9578846 -4.4706049 -4.9907341 -5.9304357 -6.6923766]]...]
INFO - root - 2017-12-15 22:00:52.999489: step 65410, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 48h:48m:35s remains)
INFO - root - 2017-12-15 22:00:59.393964: step 65420, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 46h:53m:34s remains)
INFO - root - 2017-12-15 22:01:05.713411: step 65430, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 47h:16m:41s remains)
INFO - root - 2017-12-15 22:01:12.086338: step 65440, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 47h:45m:19s remains)
INFO - root - 2017-12-15 22:01:18.463892: step 65450, loss = 0.39, batch loss = 0.27 (12.8 examples/sec; 0.626 sec/batch; 46h:24m:13s remains)
INFO - root - 2017-12-15 22:01:24.830809: step 65460, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 47h:38m:22s remains)
INFO - root - 2017-12-15 22:01:31.204829: step 65470, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.623 sec/batch; 46h:13m:15s remains)
INFO - root - 2017-12-15 22:01:37.564099: step 65480, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 47h:08m:49s remains)
INFO - root - 2017-12-15 22:01:43.960662: step 65490, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 46h:26m:38s remains)
INFO - root - 2017-12-15 22:01:50.374568: step 65500, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 48h:39m:57s remains)
2017-12-15 22:01:50.899865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7745543 -3.7268715 -4.2804937 -4.3955064 -4.0737362 -3.533545 -3.2795134 -3.2627678 -3.3071241 -4.1541524 -4.527667 -5.9321074 -6.7292466 -7.9670634 -8.1870651][-3.2098017 -3.3238897 -4.2401733 -4.4659729 -4.1141129 -3.1881671 -2.8920712 -3.1184053 -3.2681084 -4.0822258 -4.3212681 -5.8415995 -6.7451792 -7.876565 -8.12177][-1.9378705 -1.8993683 -2.4922824 -2.7417674 -2.5001612 -1.7586117 -1.4284673 -1.5249171 -1.7627726 -2.6739111 -3.2901955 -4.8399658 -5.7412734 -7.0862732 -7.6077776][-0.99761581 -0.91688061 -1.17524 -1.101727 -1.0936055 -0.78819513 -0.31014585 -0.35749769 -0.68601561 -1.9313278 -2.903161 -4.3612428 -5.1368346 -6.5298796 -6.9754896][-1.0441337 -0.82554293 -0.85385513 -0.65729475 -0.45602274 0.13022566 0.56025887 0.56688404 0.34262657 -1.1046515 -2.0035419 -3.434123 -4.3247204 -5.7015839 -6.0152359][-1.0860376 -0.73395681 -0.59912205 -0.29364395 -0.18627262 0.50934792 1.3226089 1.3831625 1.2673626 0.094883919 -0.55761719 -1.7160869 -2.4986553 -3.8710794 -4.4479942][-2.5876036 -1.8977509 -1.4983025 -1.0102782 -0.30817652 0.77368355 1.6035585 1.6767693 1.6077776 0.49188709 -0.16514778 -1.4260483 -2.3956089 -3.8217449 -4.7012115][-3.6231494 -2.6178985 -2.2314229 -1.3870273 -0.3671298 0.74914551 1.6968374 2.040884 2.0689621 1.0845804 0.415308 -0.81317568 -1.974546 -3.5885682 -4.6025333][-3.7313547 -2.8082852 -2.2116594 -1.4639249 -0.52430725 0.54047871 1.3940229 1.7756014 2.1799097 1.3441811 0.57771778 -0.69620991 -2.0844393 -3.943362 -4.751585][-4.8630886 -3.6591763 -2.8584003 -2.001296 -1.1724539 -0.26476669 0.60487461 1.0134192 1.4635496 0.63514328 -0.30884314 -1.4282837 -2.7985606 -4.1518 -4.9388533][-6.1110563 -5.0905623 -4.4317646 -3.2728667 -2.5853524 -1.9793758 -1.4593525 -1.1113672 -0.75145245 -1.8247237 -2.596323 -3.0850511 -3.6793971 -4.2934461 -4.8260946][-7.2078142 -6.4271464 -5.9035697 -5.3714752 -4.8851385 -4.101408 -3.4790764 -3.3422489 -3.3068566 -4.24235 -4.7730322 -4.9090781 -5.0828896 -5.403017 -5.9577904][-7.4220581 -7.0357003 -6.8024168 -6.5860152 -6.3564434 -5.8943796 -5.6362505 -5.613512 -5.6348572 -6.1303883 -6.2606759 -5.9351711 -5.9396048 -6.0921674 -5.90062][-7.2985797 -7.017087 -6.95817 -7.1900682 -7.07146 -6.9192371 -6.9427 -7.2106085 -7.1763334 -7.3201504 -7.0818248 -6.3778973 -5.842916 -5.7795811 -5.6710329][-7.970355 -8.0881 -7.7354903 -7.8909726 -8.2388105 -8.2423573 -8.2948322 -8.5055513 -8.4781332 -8.2769394 -7.9075847 -7.3625221 -6.6882281 -6.2271743 -6.0527105]]...]
INFO - root - 2017-12-15 22:01:57.236328: step 65510, loss = 0.24, batch loss = 0.12 (13.1 examples/sec; 0.612 sec/batch; 45h:23m:27s remains)
INFO - root - 2017-12-15 22:02:03.639336: step 65520, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 47h:58m:30s remains)
INFO - root - 2017-12-15 22:02:10.017624: step 65530, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 47h:31m:05s remains)
INFO - root - 2017-12-15 22:02:16.370814: step 65540, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 47h:43m:23s remains)
INFO - root - 2017-12-15 22:02:22.780952: step 65550, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 47h:00m:27s remains)
INFO - root - 2017-12-15 22:02:29.160071: step 65560, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 47h:36m:48s remains)
INFO - root - 2017-12-15 22:02:35.505498: step 65570, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:26m:08s remains)
INFO - root - 2017-12-15 22:02:41.946604: step 65580, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 46h:59m:59s remains)
INFO - root - 2017-12-15 22:02:48.375049: step 65590, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 48h:06m:10s remains)
INFO - root - 2017-12-15 22:02:54.811087: step 65600, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 46h:15m:42s remains)
2017-12-15 22:02:55.321918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.73785 -6.9746585 -6.5803375 -6.0381622 -5.221561 -4.0026789 -2.6911483 -1.6835313 -1.0403051 -2.1324768 -3.8959746 -5.5396414 -7.2736835 -7.7437921 -8.6779118][-8.6312609 -8.7151318 -8.0618229 -7.145215 -5.8683219 -4.5046511 -3.280385 -2.4039102 -1.9335437 -3.2278857 -4.73149 -6.1690121 -7.807188 -8.1609745 -9.1120892][-8.334631 -8.6311588 -8.5296278 -7.8805947 -6.6208248 -4.9988074 -3.3923087 -2.7871194 -2.5277529 -3.8026075 -5.4228678 -6.9895325 -8.2174988 -8.0938616 -8.8461008][-8.3263521 -8.2752466 -7.8679476 -6.6065264 -4.8894567 -3.454874 -2.0318661 -1.2727938 -0.98618841 -2.8508863 -4.7469769 -6.6790361 -8.1070395 -7.900754 -8.4905939][-8.1027966 -7.8873105 -6.9429889 -5.4971089 -3.7631845 -1.9139481 -0.039847374 0.30433941 0.33283424 -1.7188263 -4.1504006 -6.3397846 -7.9236422 -7.971951 -8.62186][-8.0102577 -7.267427 -6.4106326 -4.5442324 -2.0318642 -0.16510057 1.7325459 2.3503389 2.3712311 -0.1872654 -2.8712645 -5.6509337 -7.791678 -8.231945 -9.0330906][-8.1148167 -7.2712903 -5.8648648 -3.9869809 -1.6998677 0.9048481 3.7864056 4.276474 3.957428 1.3243618 -1.8705854 -4.8666573 -7.32315 -8.18099 -9.0696993][-6.8847566 -6.6186709 -5.8969059 -3.5354748 -0.1007638 2.7606125 5.4517326 5.9232073 5.6925764 2.7296314 -0.66745806 -3.9084487 -6.8148837 -7.4590292 -8.3871441][-6.5760603 -5.9805651 -5.3387237 -3.6922002 -1.1856217 1.4953508 4.3660831 5.4992981 5.8439779 2.6170855 -1.2256308 -4.3195782 -6.8516965 -7.4213133 -8.252162][-6.5544267 -6.5169945 -6.0924063 -4.4157696 -2.0872364 0.3795433 2.6531219 3.50564 3.6860609 1.3460093 -1.4335661 -4.7381754 -7.0314679 -7.1367826 -7.7829995][-4.8614445 -5.2949524 -5.52054 -4.6210718 -2.9681501 -1.3288045 0.54898739 0.87842655 0.40952492 -1.8759665 -4.7492685 -6.398838 -7.3063178 -6.840929 -7.2067881][-4.828486 -4.1446333 -3.5064363 -2.5184202 -1.7036395 -1.1309109 -0.52238464 -1.5791607 -2.2834353 -4.175765 -5.7689862 -6.6091576 -7.8771172 -7.3617969 -7.5954366][-5.9154263 -5.104589 -4.37869 -3.1810937 -1.9890418 -1.6472058 -1.0260539 -1.9286785 -2.7159553 -4.4788771 -6.3736639 -7.3284688 -7.9989886 -7.5939136 -7.85939][-6.4789696 -5.6413441 -4.7868176 -3.853451 -3.1352224 -2.967423 -2.8180361 -2.7861843 -2.5397835 -3.281033 -4.3493004 -5.9715405 -6.8978271 -7.0676956 -7.5376139][-6.850893 -5.9750795 -6.0777006 -5.5830064 -5.0392065 -4.79976 -4.0565739 -4.570797 -5.270225 -5.4958649 -6.001214 -6.8743882 -7.3041725 -7.1129766 -6.7328076]]...]
INFO - root - 2017-12-15 22:03:01.693983: step 65610, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 49h:28m:42s remains)
INFO - root - 2017-12-15 22:03:08.089978: step 65620, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 47h:17m:59s remains)
INFO - root - 2017-12-15 22:03:14.501511: step 65630, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 45h:45m:48s remains)
INFO - root - 2017-12-15 22:03:20.899717: step 65640, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 46h:17m:04s remains)
INFO - root - 2017-12-15 22:03:27.369273: step 65650, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 49h:31m:40s remains)
INFO - root - 2017-12-15 22:03:33.751793: step 65660, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 47h:27m:41s remains)
INFO - root - 2017-12-15 22:03:40.248738: step 65670, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 48h:11m:18s remains)
INFO - root - 2017-12-15 22:03:46.595457: step 65680, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 47h:19m:53s remains)
INFO - root - 2017-12-15 22:03:53.041585: step 65690, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 46h:37m:02s remains)
INFO - root - 2017-12-15 22:03:59.472486: step 65700, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 47h:16m:37s remains)
2017-12-15 22:03:59.983293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5158319 -2.1716709 -1.7562099 -1.3921261 -1.8058519 -2.3037181 -2.6417665 -2.9294972 -2.9580779 -4.06548 -5.062748 -6.3724546 -6.6373162 -7.0335231 -6.9116855][-2.1291661 -2.0594378 -1.8251038 -1.5262961 -1.4655485 -1.7109294 -2.347116 -2.8044939 -2.6855397 -3.9277973 -4.9086671 -6.1508493 -6.6271038 -6.9830256 -6.6461267][-2.1902847 -1.7164888 -1.3577399 -1.084332 -1.2123055 -1.4515147 -1.9466786 -2.1844316 -2.2441545 -3.593071 -4.515614 -5.9123263 -6.4609261 -7.0068989 -7.0951486][-2.7351923 -2.1508346 -1.7201619 -1.2135644 -0.94222546 -0.92277 -0.88767147 -0.82333088 -0.75975847 -2.2628183 -3.6657195 -5.2747445 -6.0473275 -6.9894333 -6.862401][-3.919894 -3.1449952 -2.1368208 -1.5339274 -1.1650538 -0.43434286 0.37169266 0.74558926 0.87008476 -0.85377359 -2.4731994 -4.4147406 -5.4692574 -6.3527122 -6.4585805][-5.9641585 -5.2020273 -3.9625034 -2.5490108 -1.100698 0.32093239 1.2376194 1.8060036 2.1829348 0.61310768 -1.2077804 -3.6424928 -4.9411507 -6.0426731 -6.3031778][-5.9254627 -5.2347279 -4.1984391 -2.8843112 -1.1859722 0.89624119 2.2510195 2.9768238 3.2435312 1.4504213 -0.37990808 -2.94946 -4.3709497 -5.4143295 -5.6861725][-6.5024171 -5.3052197 -4.0731416 -2.6063051 -0.65545511 1.1668358 2.7941256 3.8728886 4.1603422 2.181015 0.13163805 -2.253294 -3.6497021 -5.0154657 -5.2458725][-7.1857877 -6.3089519 -4.7523832 -2.6367531 -0.91365385 0.9535675 2.190341 2.9098778 3.1135454 1.2372494 -0.41761112 -2.8550858 -4.1960874 -5.3100863 -5.7694621][-7.8986921 -7.0166926 -5.8769588 -3.8380544 -1.7641621 -0.16310358 1.0560932 1.6873274 1.5608864 -0.55200386 -1.8931494 -3.7399828 -4.9502316 -5.8291512 -5.9690156][-8.3033552 -7.5843358 -6.6532779 -4.9697714 -3.5929337 -2.5273385 -1.4615879 -1.039073 -0.9522438 -3.0633783 -4.587513 -5.9763036 -6.6283917 -7.0793295 -7.1715755][-9.6768923 -9.1296005 -7.6354895 -6.041995 -5.1578007 -4.21877 -3.5582566 -3.5504074 -3.5868359 -5.1217108 -6.0675039 -6.8308883 -6.9701362 -7.2737808 -7.1209488][-9.6753235 -9.2634783 -8.51573 -7.6165104 -6.2959538 -5.2121921 -4.8048105 -5.0183668 -5.0351028 -6.0790977 -6.5482593 -6.3182931 -6.1879878 -6.2313137 -5.7968769][-8.8454323 -8.22672 -8.0293036 -7.54969 -7.028059 -6.49046 -6.0025344 -6.2104859 -6.2698946 -6.6565275 -6.4258194 -5.9908943 -5.7112336 -5.5573077 -5.3586364][-9.2074041 -8.4222031 -7.6092014 -7.0508003 -7.2073407 -7.0306377 -7.1913 -7.5455432 -7.8036122 -7.6955705 -7.2336125 -6.5346222 -5.9936528 -5.1818752 -4.6892729]]...]
INFO - root - 2017-12-15 22:04:06.374195: step 65710, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 47h:24m:54s remains)
INFO - root - 2017-12-15 22:04:12.816515: step 65720, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 48h:32m:28s remains)
INFO - root - 2017-12-15 22:04:19.222531: step 65730, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 47h:04m:14s remains)
INFO - root - 2017-12-15 22:04:25.536671: step 65740, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 46h:09m:50s remains)
INFO - root - 2017-12-15 22:04:31.962827: step 65750, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 46h:38m:44s remains)
INFO - root - 2017-12-15 22:04:38.336951: step 65760, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 46h:56m:17s remains)
INFO - root - 2017-12-15 22:04:44.714750: step 65770, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 45h:54m:44s remains)
INFO - root - 2017-12-15 22:04:51.068889: step 65780, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 48h:18m:33s remains)
INFO - root - 2017-12-15 22:04:57.497572: step 65790, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 46h:49m:46s remains)
INFO - root - 2017-12-15 22:05:03.794379: step 65800, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 47h:07m:42s remains)
2017-12-15 22:05:04.278133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1672187 -2.3747072 -2.810956 -3.490334 -4.0507593 -4.5263138 -4.6019812 -4.6340079 -4.6954169 -5.6637278 -5.9535127 -7.4097872 -7.7550316 -8.5902777 -9.2103834][-2.6497793 -2.7840552 -3.3068509 -3.8287466 -4.1672649 -4.1611853 -4.0241156 -3.8821788 -3.9889052 -5.193902 -5.9120846 -7.5834126 -8.2850294 -9.1480722 -9.638504][-3.7768006 -3.7971039 -4.0911808 -4.2847004 -4.3209305 -4.0593119 -3.6279731 -3.5763721 -3.3444757 -4.3815489 -4.7864065 -6.0574112 -6.4068317 -7.0206394 -7.710351][-4.979847 -4.5805483 -4.560679 -4.0018578 -3.2480526 -2.4685106 -1.6990328 -1.6260858 -1.3573594 -2.3037939 -2.5905304 -4.3314638 -5.1379409 -5.5589371 -6.4085622][-5.4206371 -4.7648087 -4.3261542 -3.4589205 -2.5989695 -1.1272287 0.25007868 0.60975933 0.99088955 -0.31772661 -0.86741877 -2.7367792 -3.9025285 -4.8799033 -5.790359][-5.152482 -4.6399403 -4.0106974 -2.6452832 -1.3871007 0.32935047 2.0240088 2.5891571 2.8465948 1.3342543 0.32926464 -1.9334331 -2.9038157 -4.1327763 -4.9796042][-5.7169533 -4.9631014 -3.9930804 -2.0860782 -0.39234829 1.4713354 3.2073927 3.62992 4.0288334 2.1606512 0.66845417 -2.0241709 -3.7355621 -4.8882561 -5.5920215][-5.5217991 -4.4582825 -3.4519258 -1.6190877 0.16212988 2.324687 3.7995605 4.379859 4.801136 2.9239626 1.6187086 -1.5796318 -3.3992414 -5.1128187 -6.1323967][-4.3978987 -4.0081549 -3.3229561 -1.7355728 -0.53025961 1.2195501 2.4663858 3.3700991 3.7971735 2.0318022 0.84282684 -1.8156061 -3.3425708 -5.0071983 -6.3897314][-4.7781477 -4.3487854 -4.0211358 -2.7196183 -1.8458438 -0.42867184 0.47445679 1.2012358 1.6492138 -0.1784811 -1.20087 -3.7098351 -4.7515464 -5.7835717 -6.84527][-6.6518326 -6.4987183 -6.3248672 -5.1494112 -4.1024923 -2.7320752 -1.7580466 -1.1556954 -0.82087994 -2.483417 -3.6374826 -5.7711296 -6.5868168 -7.3458729 -7.839664][-7.3519034 -7.19482 -6.957346 -6.5208955 -5.9957509 -4.9563355 -4.3832951 -3.6003461 -3.0263066 -4.1915221 -4.8714857 -6.2880697 -6.6231761 -7.2902536 -7.8313184][-7.453896 -7.6823959 -7.6429486 -7.3265185 -7.0859771 -6.6433887 -6.4004893 -6.0561175 -5.8213167 -6.3809791 -6.564281 -7.1928148 -7.4269681 -7.2898936 -7.2322569][-7.3095226 -7.5182347 -7.6326122 -7.34292 -7.115304 -6.5130181 -6.4927444 -6.5583525 -6.5080423 -7.0796242 -7.3543425 -7.5072188 -7.3041897 -7.2531195 -6.9440312][-8.8112154 -8.800518 -8.4179049 -8.2381477 -8.0377178 -7.8889275 -8.0917034 -8.0358953 -8.2195892 -8.24188 -8.0430746 -7.8874 -7.403511 -7.0389605 -6.8914909]]...]
INFO - root - 2017-12-15 22:05:10.692535: step 65810, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 46h:42m:49s remains)
INFO - root - 2017-12-15 22:05:17.055056: step 65820, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 46h:51m:33s remains)
INFO - root - 2017-12-15 22:05:23.491355: step 65830, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 46h:52m:03s remains)
INFO - root - 2017-12-15 22:05:29.888016: step 65840, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 48h:18m:09s remains)
INFO - root - 2017-12-15 22:05:36.285722: step 65850, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 46h:57m:00s remains)
INFO - root - 2017-12-15 22:05:42.653078: step 65860, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 47h:42m:30s remains)
INFO - root - 2017-12-15 22:05:49.093954: step 65870, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 47h:38m:49s remains)
INFO - root - 2017-12-15 22:05:55.514390: step 65880, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 46h:49m:47s remains)
INFO - root - 2017-12-15 22:06:01.935748: step 65890, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.660 sec/batch; 48h:54m:32s remains)
INFO - root - 2017-12-15 22:06:08.342371: step 65900, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:31m:57s remains)
2017-12-15 22:06:08.852648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2692685 -3.0911603 -3.129447 -3.0002627 -2.6885562 -2.4584656 -2.0519109 -1.560185 -1.0031629 -2.0493789 -3.4126854 -5.10103 -6.5234642 -7.4758844 -8.4140377][-3.227664 -3.3410702 -3.654254 -3.7842588 -3.67176 -2.9855719 -2.4044571 -1.9258208 -1.2514834 -2.4391179 -3.7869287 -5.3661208 -6.8351679 -7.5457873 -8.4055853][-2.3425846 -2.3181658 -2.7168818 -2.8938742 -2.6111183 -2.1795368 -1.5243182 -1.0653548 -0.81027603 -2.3691435 -4.0638375 -5.6433954 -7.0566463 -7.8623304 -8.6266308][-2.4138212 -2.2294531 -2.1659617 -1.8422303 -1.3924475 -0.79518652 -0.26480532 -0.15366507 -0.18196964 -2.1511197 -3.9729033 -5.8314919 -7.3692904 -7.8357816 -8.4322643][-2.8273373 -2.4482722 -1.7293873 -1.1493545 -0.32752037 0.51885986 1.0219154 0.92068958 0.57506847 -1.5252247 -3.5158687 -5.4747028 -7.0924997 -7.8068266 -8.3234463][-2.9393005 -2.4121904 -1.7919717 -0.69337749 0.34983635 1.4925852 2.1349058 1.8796902 1.4735794 -0.98155594 -3.267962 -5.2310629 -6.91459 -7.6145849 -8.3745174][-3.1574612 -2.5518603 -1.5269403 -0.077188969 1.2634058 2.5307178 3.0400248 2.8937674 2.3877 -0.33194685 -2.9083171 -5.122262 -6.773427 -7.6008091 -8.4776249][-3.3816037 -2.5807548 -1.6464658 -0.05732584 1.5696354 2.7516146 3.1670332 3.0884495 2.658145 0.034747124 -2.4988213 -4.8983879 -6.6601982 -7.4546371 -8.1341238][-3.7366221 -2.9240651 -2.2072744 -0.87678576 0.65241146 1.7368259 2.1633472 2.1485243 1.9289541 -0.41513348 -2.8216825 -4.8323374 -6.5010405 -7.271934 -8.0056915][-4.1938553 -3.5288634 -2.9086332 -1.6351061 -0.24871349 0.7541151 1.1786165 1.2647476 1.2145338 -1.2259679 -3.1344504 -4.98493 -6.5725183 -7.2435822 -8.0598669][-5.7828603 -5.2813768 -4.3626156 -3.2887568 -2.197072 -1.1610751 -0.68480062 -0.99498272 -1.1948552 -3.1256514 -4.7814264 -5.8910756 -6.9294596 -7.5321746 -7.9132042][-6.8780408 -6.4143515 -5.3957043 -4.4652796 -3.7874024 -2.9178834 -2.6979198 -3.0139413 -3.1986299 -4.6152253 -5.6293097 -6.3371792 -6.8065619 -7.1529684 -7.403131][-7.3635497 -6.9612784 -6.1100535 -5.113759 -4.5191445 -4.1172733 -3.9501827 -4.22542 -4.6841688 -5.5164461 -6.2594442 -6.356585 -6.4169278 -6.4394493 -6.395267][-7.5363255 -7.0713072 -6.2795162 -5.4566331 -5.1547904 -4.8420548 -4.8988419 -5.200429 -5.3666878 -5.907938 -5.9300957 -6.0187697 -6.0459056 -5.9659848 -6.0064583][-8.4382486 -7.9248033 -7.1957579 -6.7517872 -6.5709982 -6.4393163 -6.6289473 -6.9971724 -7.1454253 -6.8544712 -6.4471669 -6.0825996 -5.9254122 -5.9486856 -5.890275]]...]
INFO - root - 2017-12-15 22:06:15.241220: step 65910, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 46h:12m:00s remains)
INFO - root - 2017-12-15 22:06:21.631316: step 65920, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 47h:05m:31s remains)
INFO - root - 2017-12-15 22:06:28.090828: step 65930, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.664 sec/batch; 49h:10m:52s remains)
INFO - root - 2017-12-15 22:06:34.483479: step 65940, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 45h:56m:14s remains)
INFO - root - 2017-12-15 22:06:40.800921: step 65950, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 46h:44m:16s remains)
INFO - root - 2017-12-15 22:06:47.209822: step 65960, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 46h:33m:23s remains)
INFO - root - 2017-12-15 22:06:53.548516: step 65970, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 46h:00m:23s remains)
INFO - root - 2017-12-15 22:06:59.904344: step 65980, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 47h:50m:48s remains)
INFO - root - 2017-12-15 22:07:06.359795: step 65990, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 47h:55m:08s remains)
INFO - root - 2017-12-15 22:07:12.760601: step 66000, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:22m:38s remains)
2017-12-15 22:07:13.328915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.849771 -1.6037235 -1.6365876 -1.4571142 -1.2330437 -1.0808468 -0.88420105 -0.65518332 -0.39981937 -2.069736 -3.3297892 -4.7971067 -6.0884585 -7.113606 -7.8014841][-1.8618946 -1.7561979 -2.0576472 -2.1493049 -2.1970682 -1.9228683 -1.558918 -1.397737 -1.1318712 -2.5555444 -3.669477 -5.2811475 -6.6615319 -7.68454 -8.1264582][-2.6843576 -2.5626383 -2.69205 -2.611454 -2.3586698 -1.9196081 -1.443018 -1.2180648 -0.97890806 -2.5453973 -3.7321451 -5.1942544 -6.517828 -7.7860131 -8.4282808][-3.6248484 -3.3816729 -3.2142677 -2.7325954 -2.0225682 -1.3180027 -0.63473415 -0.47389555 -0.35131836 -2.07055 -3.3636084 -4.9606805 -6.4067373 -7.6277857 -8.1381979][-4.4766359 -3.9125948 -3.1819258 -2.4883165 -1.6159897 -0.637836 -0.0099391937 0.14646196 0.25635529 -1.5800505 -2.991468 -4.7283659 -6.2953429 -7.649106 -8.22158][-4.863512 -4.0204821 -3.1957626 -2.1010909 -1.0410209 0.0097413063 0.74177647 0.92759037 0.96256447 -1.0393114 -2.5553269 -4.4413948 -6.1499662 -7.5544648 -8.2868681][-4.6348443 -3.9445503 -2.865963 -1.5103731 -0.27003336 0.83708668 1.4698963 1.712101 1.7878637 -0.35613298 -2.123775 -4.1049042 -5.846468 -7.3843455 -8.0743933][-3.9926944 -2.8983893 -1.8574123 -0.616992 0.61235619 1.5170097 2.0817604 2.3374596 2.3467226 0.23556137 -1.556138 -3.7031212 -5.5765438 -7.1912384 -7.8163486][-3.4057999 -2.3830466 -1.4722595 -0.32892704 0.71949196 1.1629076 1.5888472 1.987051 2.096138 0.12630033 -1.5748982 -3.5507441 -5.3852968 -7.0664258 -7.6901741][-2.9960017 -2.1811705 -1.3467674 -0.29818726 0.43456745 0.68660259 1.0231199 1.3792534 1.4780455 -0.56582737 -2.132175 -3.9722345 -5.618031 -7.0450516 -7.767045][-4.6246595 -3.97746 -3.0125771 -2.0945444 -1.4805732 -1.1431994 -0.86540079 -0.62857485 -0.55528736 -2.3449745 -3.8365834 -5.0992489 -6.3446536 -7.4991627 -7.7269135][-5.4708614 -5.001369 -4.1831036 -3.282443 -2.710217 -2.4260249 -2.3688974 -2.2241993 -2.1199121 -3.3833914 -4.4409776 -5.3508992 -6.2165852 -7.2078843 -7.3565049][-6.7519655 -6.2651215 -5.6116333 -4.8326683 -4.4275188 -4.1073618 -4.1450148 -4.1688166 -4.1730809 -4.8488774 -5.5821872 -5.9908485 -6.3153334 -6.7985377 -6.4784865][-7.1714239 -6.7168922 -6.2181072 -5.4750538 -4.981842 -4.6569014 -4.6804237 -4.7819867 -4.807457 -5.3537526 -5.6736827 -5.8712177 -5.9757032 -6.4139719 -6.0697126][-8.01323 -7.769289 -7.1804075 -6.912941 -6.5672736 -6.3678074 -6.5610294 -6.772522 -6.8824992 -6.8493 -6.8045921 -6.6766229 -6.4832191 -6.4260921 -6.2407608]]...]
INFO - root - 2017-12-15 22:07:19.750090: step 66010, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 48h:46m:30s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 22:07:26.158068: step 66020, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 46h:26m:58s remains)
INFO - root - 2017-12-15 22:07:32.477237: step 66030, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 47h:27m:40s remains)
INFO - root - 2017-12-15 22:07:38.881275: step 66040, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 46h:36m:35s remains)
INFO - root - 2017-12-15 22:07:45.345587: step 66050, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:18m:00s remains)
INFO - root - 2017-12-15 22:07:51.748251: step 66060, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 48h:21m:48s remains)
INFO - root - 2017-12-15 22:07:58.053264: step 66070, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 46h:24m:43s remains)
INFO - root - 2017-12-15 22:08:04.525487: step 66080, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 47h:43m:20s remains)
INFO - root - 2017-12-15 22:08:10.889671: step 66090, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 46h:55m:13s remains)
INFO - root - 2017-12-15 22:08:17.214857: step 66100, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 46h:05m:22s remains)
2017-12-15 22:08:17.740836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18162 -3.6443534 -4.0277166 -4.200315 -4.2550187 -4.337194 -3.4209394 -2.1350975 -1.3826866 -2.6788125 -4.301322 -4.64227 -5.3221912 -7.5086055 -7.5068693][-4.9887333 -5.3593245 -5.3337917 -5.2014637 -5.0112877 -4.3957062 -4.4617267 -3.9380803 -2.5467429 -2.8555741 -4.0588126 -4.8249378 -6.1954985 -7.538929 -7.52752][-4.88297 -4.7092681 -4.8868494 -4.8682938 -4.3858128 -3.635118 -2.6409082 -2.11688 -1.8168349 -2.6701427 -3.354053 -4.2470484 -5.9796324 -7.5165272 -7.4072275][-5.1926327 -4.5075483 -4.2530766 -3.6077104 -3.1774282 -2.6376042 -1.532661 -0.91683865 -0.47887802 -1.8560433 -3.2509208 -3.2721505 -4.3892765 -6.6207933 -6.4962015][-5.988812 -4.8021851 -3.3571711 -2.8530135 -2.2704535 -1.244554 -0.35575294 0.11405849 -0.40196085 -2.0138574 -3.5052395 -3.9677334 -4.99537 -6.3956394 -6.0916033][-3.8912754 -3.4795766 -2.9041834 -1.9957342 -1.1675773 -0.36921453 0.38380146 1.0873861 0.7411871 -1.5036883 -3.595798 -4.6928072 -6.6356974 -8.0318031 -6.7871342][-2.6777568 -1.9536071 -1.7745738 -0.89262295 -0.11619902 0.67566109 1.3124638 1.6221495 1.9996824 -0.51393032 -2.7868047 -4.1913576 -6.3409252 -8.5958185 -8.0763664][-2.0931964 -1.4857264 -1.1596346 -0.22516298 0.46999836 1.0577259 1.2561426 1.9554462 2.11263 0.15022373 -2.2952027 -4.3569975 -5.9659748 -8.0297327 -7.6439242][-2.3325047 -2.0376492 -1.6442585 -0.95890427 0.26666927 0.84167385 1.362998 1.7086906 1.6092567 -0.34440136 -2.432271 -3.7025011 -5.7766633 -8.0974169 -7.5423365][-3.3674002 -2.7671328 -1.6106162 -1.4695048 -1.1179852 -0.097188473 0.79669666 1.310976 1.4927998 -0.56997728 -2.7260323 -4.177886 -5.5347481 -7.3351488 -7.2897816][-4.2804818 -4.9189472 -4.7930765 -3.6485252 -3.3704772 -2.5365772 -1.6825633 -0.94200659 -0.99234867 -2.2802706 -3.9326661 -4.90225 -6.030921 -7.4977217 -7.089632][-6.1420956 -6.2478895 -5.9659734 -5.6972766 -4.7120876 -4.39961 -4.2784548 -3.4555354 -2.896452 -4.0157948 -5.1656857 -5.1491146 -5.8697958 -7.0483818 -6.6885514][-7.3310928 -7.6462097 -7.7263865 -7.3313727 -6.4948263 -5.8224497 -5.0466595 -4.8044596 -4.8854942 -5.1760206 -6.0605497 -5.7524781 -5.490397 -6.039124 -6.1666474][-7.2952948 -7.5319533 -7.8126297 -7.1823387 -6.88855 -6.675993 -6.6986184 -6.3382721 -5.7101774 -5.2218685 -5.5901628 -5.6250529 -5.9886527 -5.8302088 -5.3727942][-7.6686363 -7.4601822 -7.5208311 -7.3524418 -6.9315019 -6.8394 -7.0020194 -6.8308468 -7.10007 -7.0125146 -6.5735226 -6.043973 -5.8767767 -6.0249157 -5.8216052]]...]
INFO - root - 2017-12-15 22:08:24.156548: step 66110, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 48h:07m:06s remains)
INFO - root - 2017-12-15 22:08:30.580211: step 66120, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 47h:04m:17s remains)
INFO - root - 2017-12-15 22:08:36.938838: step 66130, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 47h:01m:23s remains)
INFO - root - 2017-12-15 22:08:43.403030: step 66140, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 48h:18m:41s remains)
INFO - root - 2017-12-15 22:08:49.737128: step 66150, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 46h:11m:06s remains)
INFO - root - 2017-12-15 22:08:56.162508: step 66160, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 47h:44m:33s remains)
INFO - root - 2017-12-15 22:09:02.580178: step 66170, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 48h:13m:47s remains)
INFO - root - 2017-12-15 22:09:09.012198: step 66180, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 46h:01m:16s remains)
INFO - root - 2017-12-15 22:09:15.357927: step 66190, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 48h:09m:45s remains)
INFO - root - 2017-12-15 22:09:21.753887: step 66200, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 47h:14m:33s remains)
2017-12-15 22:09:22.342522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7438374 -4.5110679 -3.910733 -3.886055 -4.2976151 -4.5238972 -5.0696287 -4.9130068 -4.5894241 -4.8908997 -5.9816103 -6.1245613 -6.11215 -7.5959682 -8.3584518][-3.4355059 -2.59137 -2.308497 -2.6696234 -3.4759893 -3.7925801 -4.4668055 -5.0101347 -5.0628109 -5.3740911 -6.0265288 -6.1927509 -6.4644151 -7.0943356 -7.3619194][-2.0896368 -1.5179276 -1.3284011 -1.1431046 -1.2614198 -2.0279093 -2.9688759 -3.348103 -3.2099633 -4.2285748 -5.3488526 -5.9342976 -6.2745519 -6.6523905 -6.7459884][-1.6709833 -1.0324044 -0.72513533 -0.50829029 -0.14823246 -0.22114038 -0.74485159 -1.2862935 -1.6665616 -2.8655262 -4.3015294 -4.9264441 -5.8153682 -6.71004 -6.7466283][-1.8511415 -0.688539 0.10655737 0.46252823 0.58101845 1.0228901 1.3943624 0.73367596 0.22763968 -1.2920642 -2.9665265 -3.38861 -4.43782 -5.5163212 -5.9358625][-1.735568 -0.84890509 -0.12592268 0.83919811 1.5186186 1.6743803 1.7964954 1.5835714 1.5214558 0.084341049 -1.5137901 -2.059648 -2.9844093 -4.0580978 -4.7118464][-2.1846547 -1.1307349 -0.23234224 0.93203449 1.9994078 2.2969561 2.2962723 2.0599842 2.1052637 0.34846306 -1.1666913 -1.8705611 -3.0706062 -4.4010477 -4.9459224][-2.6599379 -1.6027713 -0.71773148 0.46937943 1.799962 2.3440151 2.6370392 2.3024521 2.0128279 0.52456379 -0.81602716 -1.9991479 -3.3036141 -4.6857758 -5.0933628][-3.6002555 -2.4072852 -1.7055917 -0.65601826 0.41061115 1.1968002 1.556159 1.6143408 1.6407661 0.1080327 -1.2891006 -2.4496999 -3.558167 -4.5053816 -4.7140813][-5.1121964 -4.1119747 -3.3579493 -2.3618178 -1.3072147 -0.52276945 -0.064596653 0.088574409 0.37113571 -1.0793242 -2.6772289 -3.5451927 -4.5275965 -5.4695148 -5.4436693][-6.3928776 -6.0635977 -5.1571422 -4.1082764 -3.1480613 -2.2689323 -1.5976396 -1.5769691 -1.4130921 -2.32233 -3.615799 -4.5556927 -5.5835047 -6.4577236 -6.5551491][-6.2506151 -6.418663 -5.9260039 -5.2751608 -4.2180934 -3.0347919 -2.127522 -2.1608829 -2.3385363 -3.2896905 -4.1264963 -4.8115039 -5.9901466 -7.1346931 -7.3905907][-6.2120395 -6.2220068 -5.5219707 -5.2613935 -4.3040257 -3.466949 -2.5137687 -2.3858809 -2.289485 -2.9277997 -4.3983808 -4.69508 -5.2420697 -6.2477365 -6.833262][-5.6059546 -5.6011391 -4.7658052 -4.210434 -3.3437343 -2.4964724 -1.6940265 -1.8149581 -1.9370427 -2.25027 -3.320117 -4.3004751 -5.0263948 -5.651341 -5.8797879][-5.4749174 -5.3213091 -4.818779 -4.8107557 -4.0500889 -3.0635681 -2.5812912 -2.8521643 -3.1897373 -3.2021279 -3.5827394 -4.4976368 -5.4370193 -5.4672871 -5.2579069]]...]
INFO - root - 2017-12-15 22:09:28.727955: step 66210, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 48h:04m:30s remains)
INFO - root - 2017-12-15 22:09:35.156280: step 66220, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 46h:16m:23s remains)
INFO - root - 2017-12-15 22:09:41.537251: step 66230, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.630 sec/batch; 46h:35m:47s remains)
INFO - root - 2017-12-15 22:09:47.909782: step 66240, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 47h:22m:25s remains)
INFO - root - 2017-12-15 22:09:54.257805: step 66250, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 46h:11m:47s remains)
INFO - root - 2017-12-15 22:10:00.594162: step 66260, loss = 0.35, batch loss = 0.24 (12.8 examples/sec; 0.624 sec/batch; 46h:08m:19s remains)
INFO - root - 2017-12-15 22:10:07.032552: step 66270, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 46h:50m:52s remains)
INFO - root - 2017-12-15 22:10:13.541877: step 66280, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 47h:33m:40s remains)
INFO - root - 2017-12-15 22:10:19.972596: step 66290, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 46h:35m:45s remains)
INFO - root - 2017-12-15 22:10:26.375692: step 66300, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 46h:13m:57s remains)
2017-12-15 22:10:26.873525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.36654 -1.4026475 -2.0349784 -2.455637 -2.6310024 -2.9061475 -3.0165458 -2.877121 -2.7586708 -3.4641209 -4.0632019 -5.0120606 -5.7689924 -6.1324997 -7.0090785][-1.6948233 -1.7136631 -2.2174473 -2.9411926 -3.1685591 -3.1947374 -3.5601082 -3.4241247 -3.0101142 -3.6932247 -4.1511135 -5.037858 -6.080214 -6.4304032 -7.2729092][-1.3467946 -1.3853111 -1.603251 -1.7838922 -1.9840555 -2.1420088 -2.0377684 -2.1228905 -1.9280958 -2.526731 -3.0950732 -3.9556465 -4.8163748 -5.2788153 -6.1883678][-1.0647392 -0.70780182 -0.75925684 -0.82145977 -0.9378047 -0.94231892 -0.72536993 -0.54431486 -0.2932353 -1.2158623 -1.9317765 -2.969882 -4.0209141 -4.4947157 -5.3047757][-1.406333 -0.96853161 -0.63839912 -0.43201351 -0.13762617 0.2530961 0.63011265 0.65223122 0.55747795 -0.4105444 -1.4002528 -2.522428 -3.6478767 -4.3165808 -5.30171][-1.8796329 -1.4232812 -1.1180091 -0.55842924 0.14551163 1.0579853 1.805624 2.0967588 2.1533098 0.91969109 -0.30288839 -1.7702932 -3.2492304 -4.2320786 -5.5202589][-2.4744334 -1.8399024 -1.2912302 -0.52814245 0.48877239 1.1957798 1.8862228 2.1803522 2.3857908 1.225172 -0.089944363 -1.7461963 -3.5017982 -4.4454041 -5.7548294][-2.8091006 -2.2670646 -1.40446 -0.45432234 0.86348152 1.7266045 2.4068565 2.5575466 2.5142021 1.1953354 -0.1202054 -1.8657694 -3.593606 -4.5465164 -5.6209183][-2.63826 -2.1433868 -1.4468012 -0.66590405 0.20527649 1.2452059 1.9144449 2.1087399 2.1688662 0.73929405 -0.543962 -2.1575232 -3.7172248 -4.6157737 -5.8241773][-3.0886364 -2.5907836 -2.0143728 -1.2384691 -0.5220623 0.14939642 0.78570271 0.88236237 0.79352188 -0.58749056 -1.6295586 -2.989697 -4.4014869 -5.2824726 -6.3723707][-3.6718302 -3.2630057 -2.7930927 -2.2102928 -1.4847445 -0.96358061 -0.36959696 -0.36086226 -0.45140982 -1.8417511 -3.0183258 -4.2854395 -5.1530008 -5.7348619 -6.5349541][-4.8218794 -4.3365059 -3.9426568 -3.5792947 -3.2009068 -2.7366347 -2.4531932 -2.4765468 -2.4785109 -3.3015213 -3.8412406 -4.8809929 -5.5406661 -5.8303056 -6.2817278][-5.9982457 -5.4932451 -5.0910063 -4.6805196 -4.3203173 -4.0938053 -3.8095741 -3.923418 -4.1078167 -4.6788177 -4.9570627 -5.6647363 -6.212018 -6.0522814 -6.12971][-6.6769629 -6.3650742 -5.8220396 -5.4547777 -5.2028084 -5.0343943 -5.0238562 -5.1824131 -5.2511511 -5.7617211 -5.9207897 -6.095789 -6.3603172 -6.3497992 -6.4410219][-7.5042081 -7.342834 -7.0588255 -6.6931076 -6.3697672 -6.1123834 -5.9517865 -6.1034007 -6.3113527 -6.4958262 -6.5875006 -6.6074605 -6.590724 -6.489027 -6.3269691]]...]
INFO - root - 2017-12-15 22:10:33.285766: step 66310, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.637 sec/batch; 47h:08m:15s remains)
INFO - root - 2017-12-15 22:10:39.800147: step 66320, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 47h:16m:18s remains)
INFO - root - 2017-12-15 22:10:46.239474: step 66330, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 47h:10m:11s remains)
INFO - root - 2017-12-15 22:10:52.601411: step 66340, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 46h:02m:03s remains)
INFO - root - 2017-12-15 22:10:59.021763: step 66350, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 47h:43m:06s remains)
INFO - root - 2017-12-15 22:11:05.326764: step 66360, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 46h:58m:03s remains)
INFO - root - 2017-12-15 22:11:11.782871: step 66370, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.667 sec/batch; 49h:16m:17s remains)
INFO - root - 2017-12-15 22:11:18.211329: step 66380, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 47h:53m:42s remains)
INFO - root - 2017-12-15 22:11:24.674455: step 66390, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 48h:54m:39s remains)
INFO - root - 2017-12-15 22:11:31.043385: step 66400, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 47h:43m:09s remains)
2017-12-15 22:11:31.597647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3624191 -5.9644504 -5.88911 -6.2519283 -6.1013551 -5.8787255 -5.3118753 -4.8480744 -3.8658369 -5.5017447 -5.4869103 -5.8606992 -6.6735845 -8.196702 -9.9595528][-5.0461526 -5.2844658 -6.0735674 -6.743649 -6.2794805 -5.8585939 -4.859354 -4.118124 -2.8188214 -4.3306293 -4.7854209 -6.0309334 -7.2477236 -8.281333 -9.212574][-5.1772604 -4.8505535 -4.9239192 -5.0338893 -4.9403505 -4.4289284 -3.7311723 -3.1637664 -2.3916883 -4.0050507 -4.0706043 -4.5526552 -6.4037781 -7.7410955 -8.30806][-4.7599874 -4.210721 -4.3826752 -3.9295812 -3.4912667 -2.6485176 -1.8691173 -1.2397184 -0.85334969 -2.57441 -2.89012 -3.8945429 -6.0661278 -7.2082453 -7.9978361][-3.3594294 -2.6858687 -2.0534806 -1.803628 -1.637949 -0.68544912 0.74830723 1.6603708 2.2115402 0.05769825 -1.0815487 -2.5010295 -4.4874816 -6.3231616 -7.1315579][-4.65975 -3.499382 -2.4659843 -1.0996671 0.26500273 1.4284115 2.1870251 2.5230684 2.5774069 0.29941607 -0.71340227 -2.4343395 -4.49785 -5.8858442 -6.6553569][-6.25769 -4.6805534 -3.4483414 -1.5543938 0.051477909 1.8781824 3.288949 4.0507727 4.3422451 1.4605656 -0.12597704 -2.1677489 -4.42173 -6.1310444 -6.9305091][-5.9621038 -4.8835092 -3.7882385 -2.2117114 -0.35904217 1.3206015 2.7777767 3.9385033 4.8450432 2.0651045 0.42940712 -1.6524005 -4.1445351 -6.0977674 -6.8517642][-6.5694933 -5.3821669 -5.2983675 -3.8791494 -2.2359576 -0.39967299 1.4988546 2.2446098 2.9087029 0.58605576 -0.48950672 -2.6908984 -4.8440709 -6.2749896 -6.9649692][-8.0603628 -7.1454883 -6.8149667 -5.6685114 -4.1240931 -2.1635303 -0.34367609 0.39931965 1.0781221 -1.1585574 -2.3774552 -3.8606284 -5.4772282 -6.6756573 -7.1871037][-7.8142662 -8.0574694 -8.0355663 -7.7330947 -6.6010957 -5.2891073 -3.3425674 -2.6259208 -1.9599876 -3.1222277 -3.9816468 -5.1930075 -6.6392651 -7.7298374 -8.102253][-6.9465494 -6.704638 -6.0639458 -6.7070737 -7.1335378 -6.8054709 -5.8898563 -5.6053767 -4.9844542 -6.3033152 -6.371891 -6.647254 -7.460917 -7.9248724 -7.86806][-7.4141855 -7.499661 -7.5800705 -7.5066714 -6.87591 -6.9110579 -6.3543367 -6.6161017 -6.9088755 -7.4570384 -7.6256671 -7.3979077 -7.5259576 -7.2061744 -6.5640106][-7.093245 -6.8345814 -6.093585 -6.4615393 -6.352087 -6.3110247 -5.7009273 -5.8010874 -5.6244745 -6.5092912 -6.9929051 -6.7246537 -6.6523538 -6.3341422 -6.2680016][-6.977344 -6.5122719 -6.6485233 -6.630024 -6.2602654 -6.3680258 -6.3240604 -6.4453144 -6.3611593 -6.1166034 -6.147717 -6.6060119 -6.4105988 -6.0787487 -6.3270512]]...]
INFO - root - 2017-12-15 22:11:38.044114: step 66410, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 49h:09m:12s remains)
INFO - root - 2017-12-15 22:11:44.461470: step 66420, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:56m:46s remains)
INFO - root - 2017-12-15 22:11:50.856832: step 66430, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 48h:21m:46s remains)
INFO - root - 2017-12-15 22:11:57.300087: step 66440, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 45h:40m:02s remains)
INFO - root - 2017-12-15 22:12:03.762144: step 66450, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 47h:23m:45s remains)
INFO - root - 2017-12-15 22:12:10.217887: step 66460, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.672 sec/batch; 49h:39m:17s remains)
INFO - root - 2017-12-15 22:12:16.657236: step 66470, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 46h:53m:47s remains)
INFO - root - 2017-12-15 22:12:23.001959: step 66480, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 46h:19m:10s remains)
INFO - root - 2017-12-15 22:12:29.336560: step 66490, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 47h:07m:22s remains)
INFO - root - 2017-12-15 22:12:35.698973: step 66500, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 45h:59m:35s remains)
2017-12-15 22:12:36.247830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3898611 -4.3430033 -3.4436698 -3.1936674 -3.231668 -3.1462479 -2.7572527 -1.9656248 -1.3347669 -1.6968894 -2.2085295 -4.5315056 -5.5737147 -7.359272 -8.3968019][-4.7207737 -4.8285074 -4.7450342 -4.4306955 -4.2859044 -4.0518866 -3.7851596 -3.3923016 -2.6600652 -2.5651512 -3.3769159 -5.5469117 -6.1623983 -7.5300527 -8.40212][-4.8575196 -3.762707 -3.2488046 -3.4745917 -3.4802132 -3.2551303 -2.9288507 -2.8488469 -2.4548769 -2.7052517 -3.3845158 -5.2917218 -5.7794294 -7.2427425 -8.060977][-4.6006088 -3.9946849 -3.2974286 -2.2387052 -1.6809444 -1.2576637 -0.89901972 -0.93071175 -0.76198292 -1.2824373 -2.7049198 -5.3824158 -6.0092087 -7.15492 -7.4977918][-3.9511459 -2.7913866 -1.9287195 -0.82825279 0.065126419 0.871398 1.2311869 0.73920631 0.0010237694 -0.82313251 -2.2804842 -4.6311274 -5.2258487 -6.7195153 -7.0263281][-3.0716767 -2.5818357 -1.9981618 -0.81328154 0.43108845 1.4011183 1.9520302 1.7009344 1.3354816 0.40751076 -1.1393156 -3.6210179 -3.9526587 -5.2512684 -5.6497107][-3.8664317 -2.8753033 -1.448493 -0.084996223 1.248498 2.168335 2.6026144 2.4492712 2.0974855 1.2226028 0.042527676 -2.4846387 -3.1483135 -4.6848769 -5.3780303][-2.8358083 -1.9477501 -0.91723919 0.78890705 2.1602488 2.7190084 3.2366104 3.1084881 2.7696943 1.5869617 -0.021799564 -2.6008 -2.99441 -4.5258894 -5.1504087][-1.3005185 -0.51059151 0.032747746 0.51895142 1.0967197 1.6985207 2.0292578 2.2358036 2.2966032 1.56357 0.60891819 -2.0518985 -3.0538049 -4.6722894 -5.27048][-1.3207645 -0.8843174 -0.62731314 -0.65472174 -0.39790535 0.22192478 0.71257114 0.948452 1.1272764 0.29825783 -0.547904 -2.4160147 -2.9438877 -4.6340332 -5.6368532][-1.4802318 -1.2923884 -1.2781744 -1.3562775 -1.1089697 -1.0259647 -0.92276764 -0.64046955 -0.74235344 -1.7254868 -2.2703114 -3.9626405 -4.7008305 -5.671483 -6.2647438][-2.2599273 -1.6966481 -1.4912963 -1.7739711 -1.8371024 -1.9636598 -2.0803404 -2.1332598 -2.4794726 -3.5841913 -4.2298031 -5.6222253 -6.07349 -6.5943236 -6.8690343][-3.1319246 -2.312325 -2.1043997 -2.7143044 -3.3543048 -3.4957442 -3.5067019 -3.562501 -3.9919102 -5.1616082 -5.8058267 -6.7346468 -7.2958121 -7.5990438 -7.6432233][-3.9553559 -3.5409651 -3.1979132 -3.0435 -2.9804969 -3.2298846 -3.3630986 -3.6776972 -4.1835208 -5.19436 -5.865644 -6.8322906 -7.3508329 -7.6151381 -7.69605][-5.4933519 -4.5732231 -4.1210527 -3.8912554 -3.9940016 -4.0343294 -4.322505 -4.7165751 -5.1202078 -5.6552782 -6.0754781 -6.7383389 -7.4321756 -7.7231746 -7.6127658]]...]
INFO - root - 2017-12-15 22:12:42.754030: step 66510, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 47h:33m:10s remains)
INFO - root - 2017-12-15 22:12:49.164967: step 66520, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 47h:59m:59s remains)
INFO - root - 2017-12-15 22:12:55.580331: step 66530, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 45h:30m:26s remains)
INFO - root - 2017-12-15 22:13:01.946478: step 66540, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 46h:45m:58s remains)
INFO - root - 2017-12-15 22:13:08.380183: step 66550, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 46h:45m:24s remains)
INFO - root - 2017-12-15 22:13:14.771679: step 66560, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 47h:31m:51s remains)
INFO - root - 2017-12-15 22:13:21.144966: step 66570, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:10m:27s remains)
INFO - root - 2017-12-15 22:13:27.494892: step 66580, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 46h:26m:06s remains)
INFO - root - 2017-12-15 22:13:33.822936: step 66590, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:15m:43s remains)
INFO - root - 2017-12-15 22:13:40.237447: step 66600, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 47h:12m:49s remains)
2017-12-15 22:13:40.761834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0741634 -6.0676332 -5.4842396 -4.8949289 -4.4862537 -3.7750268 -2.9305496 -2.2767177 -1.3110147 -1.535502 -2.3525381 -3.7133267 -5.0436211 -6.0378728 -6.6847067][-4.9376378 -5.2354865 -6.0903206 -6.059473 -5.4420295 -4.6295843 -3.8179355 -2.5480676 -1.1693988 -1.4764943 -2.2605448 -3.34695 -4.78739 -5.7594805 -6.515089][-4.6249094 -4.7912836 -4.701036 -4.558918 -4.2542868 -3.7274203 -3.0521359 -1.9280515 -0.78537369 -0.67987108 -1.3788676 -2.4881601 -3.5900741 -4.385788 -5.437696][-4.4368649 -4.3768454 -4.1335678 -3.4768729 -2.6183767 -1.9052615 -1.2146816 -0.74456835 -0.25494909 -0.58477736 -1.239099 -2.49605 -3.6278982 -4.0517473 -4.5301452][-4.1258817 -3.849375 -2.8406982 -1.8713417 -1.0761776 -0.435915 0.1146965 0.54194927 0.65251637 -0.12122726 -1.1857958 -2.2450762 -3.2573471 -4.0559778 -4.5968504][-2.6084752 -2.1732583 -2.0155025 -1.1085224 0.0137043 0.60027027 1.1303329 1.2524309 1.1125927 0.25959778 -0.77087116 -1.9916363 -3.6340075 -4.2745571 -4.6748424][-3.2821603 -2.2520885 -1.2810502 -0.51413584 -0.23324776 0.025856018 0.3197813 0.36491489 0.40015507 -0.52064514 -1.6556463 -2.9650311 -4.4304361 -5.4385157 -6.2401137][-3.7069511 -2.829546 -1.9476442 -0.90811396 -0.033168793 0.24012232 0.38659382 0.36848736 0.20218372 -0.59114838 -2.2110186 -3.7652802 -5.2234654 -6.1657181 -6.920311][-3.6598897 -2.4927306 -1.6163116 -1.1599002 -0.88265276 -0.10045099 0.40127182 0.24375963 0.13332224 -0.38689423 -1.7751365 -3.3678598 -5.1318312 -6.2349296 -7.0781503][-3.95609 -2.8818798 -2.0541973 -1.4123898 -0.84831333 -0.65121937 -0.39397621 -0.089691162 0.029212952 -0.77285147 -2.0694203 -3.2935271 -4.5766706 -5.051034 -5.6667881][-5.5160341 -4.76439 -4.1001358 -3.0977869 -2.3252912 -2.2040997 -2.2367196 -2.3124981 -2.3554125 -2.73708 -3.0228877 -3.3780451 -4.1174431 -4.653769 -4.7619133][-6.7428861 -6.4239392 -5.8935442 -5.2968721 -4.4222212 -3.8051941 -3.3882394 -3.2473192 -2.7629519 -2.9823184 -3.1225858 -3.0458779 -3.3779187 -3.3514338 -3.3891697][-7.3753858 -7.1162949 -7.1455116 -7.0047188 -6.4024324 -5.8217821 -5.15468 -4.3983316 -3.7001033 -3.2096434 -2.9145546 -2.554224 -2.646358 -2.4110684 -2.3198624][-8.3215218 -8.1966238 -8.1060781 -8.2020864 -8.1249866 -7.7697406 -7.0123038 -6.3191485 -5.242136 -4.1732 -3.0348592 -2.1569242 -1.572536 -1.5893531 -1.8156838][-8.8285475 -8.8477058 -8.7231369 -8.9946756 -9.38073 -9.1020947 -8.1695385 -7.5674868 -6.8001566 -5.4615297 -3.8992155 -2.8511424 -1.931725 -1.6919312 -1.8854551]]...]
INFO - root - 2017-12-15 22:13:47.035990: step 66610, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 46h:52m:35s remains)
INFO - root - 2017-12-15 22:13:53.394509: step 66620, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 46h:43m:44s remains)
INFO - root - 2017-12-15 22:13:59.715465: step 66630, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 47h:45m:09s remains)
INFO - root - 2017-12-15 22:14:06.099995: step 66640, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 46h:32m:54s remains)
INFO - root - 2017-12-15 22:14:12.510054: step 66650, loss = 0.36, batch loss = 0.24 (12.6 examples/sec; 0.633 sec/batch; 46h:45m:00s remains)
INFO - root - 2017-12-15 22:14:18.897680: step 66660, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.650 sec/batch; 47h:59m:37s remains)
INFO - root - 2017-12-15 22:14:25.303377: step 66670, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 47h:35m:27s remains)
INFO - root - 2017-12-15 22:14:31.739406: step 66680, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 46h:51m:30s remains)
INFO - root - 2017-12-15 22:14:38.142923: step 66690, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 47h:43m:10s remains)
INFO - root - 2017-12-15 22:14:44.505757: step 66700, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 48h:13m:36s remains)
2017-12-15 22:14:45.075915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3115091 -5.9209676 -6.6484756 -7.6038122 -8.4080982 -6.0583477 -4.6943607 -3.797879 -2.7723603 -5.3251076 -4.3696041 -5.0494633 -4.7705545 -7.1323977 -7.5535007][-6.3155503 -6.6627507 -5.8953943 -6.1017952 -6.2624989 -6.6281509 -6.7699437 -6.1558924 -5.8130665 -6.2225046 -5.0745244 -7.369349 -6.8858037 -6.3338375 -7.2476749][-5.3676891 -6.543685 -6.5220423 -5.7848253 -4.0232458 -4.1576176 -4.4120703 -5.6521058 -6.2794876 -6.324214 -6.3520408 -6.6967216 -6.9174318 -7.7724972 -8.0564775][-2.8807802 -3.7716939 -4.2882843 -4.4952049 -4.31406 -3.7223542 -2.7326512 -3.4257112 -4.0346947 -5.1244893 -4.8883314 -6.0400677 -6.1417761 -6.3388829 -6.0404053][-2.8470526 -2.8084149 -2.5319715 -1.9566841 -2.0285373 -1.5921779 -1.6582108 -1.8355021 -2.4935455 -3.7856286 -3.6627083 -4.8331761 -4.1543784 -4.2329626 -5.4043331][-3.3799191 -2.8108368 -2.4977016 -1.1440277 0.81882 1.2064991 0.63651848 -0.23832512 -0.6792841 -1.8986015 -1.8138037 -3.42384 -2.34694 -2.8366365 -2.7970939][-3.3142619 -3.2183838 -2.8316097 -1.3695984 0.48289585 1.141427 1.2303314 0.69554424 -0.12200403 -1.2424493 -0.82790995 -2.4859934 -2.493835 -4.2557821 -4.31736][-2.5923362 -2.9405975 -2.0388441 -1.5443435 -0.43883944 0.87453461 1.2223415 0.65899181 0.506094 -0.62251616 -0.5636425 -2.6571345 -2.3894043 -4.3488908 -5.1375442][-3.8057251 -4.0555744 -2.9840398 -2.1595616 -0.93196487 0.76709652 0.99744606 -0.04757452 1.0248213 -0.65596581 -1.8093839 -3.3696551 -3.6813512 -4.6113691 -5.0511789][-4.5026107 -3.0197754 -3.0457931 -1.9945493 -1.295126 -0.87576342 -0.99384403 -0.58936977 -0.22226858 -1.7752194 -2.5821013 -4.7272396 -4.6805172 -5.8456078 -6.0418224][-7.7784276 -5.8270607 -4.4516239 -2.90349 -1.5136619 -1.4750085 -1.6222048 -2.9483275 -2.4627061 -4.1285748 -3.7062397 -4.8194952 -5.7053671 -5.87267 -6.0667276][-6.8855433 -6.9820509 -7.3139319 -6.5937552 -5.5935526 -4.7429829 -4.2262774 -4.5346622 -3.6356006 -4.7659349 -6.0195336 -6.579814 -6.5528131 -5.9268985 -6.1560016][-7.7177887 -7.04333 -6.30159 -7.0460815 -7.5272841 -7.2780519 -6.6190252 -6.7131677 -6.3341904 -6.3198152 -6.2974863 -6.7772188 -6.7227726 -6.711503 -6.0636187][-7.0227604 -5.7622633 -5.8978472 -5.71438 -5.5474772 -6.2767835 -6.7954545 -6.7094407 -7.2632771 -6.934186 -6.9223561 -6.173202 -5.79849 -5.9276834 -5.4662933][-5.1581507 -5.94639 -6.6355262 -6.9474878 -6.3887358 -6.9790497 -7.3646841 -7.5926595 -6.9077082 -7.0107045 -6.5546741 -6.687808 -6.7679768 -6.1030059 -5.673028]]...]
INFO - root - 2017-12-15 22:14:51.592964: step 66710, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 48h:17m:38s remains)
INFO - root - 2017-12-15 22:14:58.018527: step 66720, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.623 sec/batch; 45h:57m:32s remains)
INFO - root - 2017-12-15 22:15:04.430343: step 66730, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 48h:57m:10s remains)
INFO - root - 2017-12-15 22:15:10.823989: step 66740, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 47h:23m:13s remains)
INFO - root - 2017-12-15 22:15:17.190759: step 66750, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 46h:19m:59s remains)
INFO - root - 2017-12-15 22:15:23.580156: step 66760, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 47h:53m:19s remains)
INFO - root - 2017-12-15 22:15:29.953411: step 66770, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 47h:42m:04s remains)
INFO - root - 2017-12-15 22:15:36.393783: step 66780, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 47h:31m:04s remains)
INFO - root - 2017-12-15 22:15:42.790093: step 66790, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 48h:03m:44s remains)
INFO - root - 2017-12-15 22:15:49.203920: step 66800, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.641 sec/batch; 47h:20m:42s remains)
2017-12-15 22:15:49.709281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9146175 -5.51958 -4.7201185 -4.6767454 -4.6707115 -3.958257 -2.7055945 -2.9519119 -3.093226 -4.8902054 -4.4521995 -5.0375051 -4.83731 -5.8052073 -5.9002228][-6.4140797 -6.1432877 -5.6792431 -4.6599116 -3.9278018 -3.959832 -3.4592934 -2.6745076 -2.2604465 -4.2989507 -5.024909 -5.7713513 -5.8426876 -6.2659903 -5.8651814][-7.3664722 -6.7769856 -5.9089117 -4.9579592 -3.8779175 -3.2018938 -2.8262887 -2.769876 -2.6684418 -4.3865347 -4.9261632 -5.7659645 -6.3261585 -6.8652263 -6.5525274][-7.2588615 -5.9147854 -5.58708 -4.2877569 -2.8746076 -1.7533717 -1.6962714 -1.1951823 -1.0597744 -3.2416968 -4.1857486 -4.6761479 -5.4469128 -6.7123518 -6.4975348][-7.1055813 -4.8887234 -2.9618649 -2.0958066 -1.9481034 -0.80292463 -0.18674135 -0.16899109 -0.83375978 -2.7526083 -3.3064365 -4.1340613 -4.549222 -6.0438609 -6.1521707][-4.1591673 -3.2146125 -3.213623 -1.5042481 0.3990593 1.0926771 0.93227482 1.0387669 0.99836063 -1.4966288 -2.9870448 -3.4371319 -3.413403 -5.0048275 -5.0779238][-3.0693145 -1.1786909 1.2046661 1.6903687 1.0653868 1.8911591 2.3302202 1.6899605 1.2584171 -0.897861 -1.6869383 -2.8419123 -3.2802382 -4.5455904 -4.5375876][-1.5044408 -1.1935253 -1.0355496 0.78764153 2.8791361 2.8426981 3.22604 2.5405035 1.8685493 -0.69966221 -2.1127367 -3.3552947 -3.342514 -4.3530221 -4.6964359][-1.3325815 -0.48356915 0.85846329 1.5609446 1.2479324 1.7600498 2.2180681 2.2208357 1.8434114 -0.74191189 -1.5953584 -3.1840372 -4.3345938 -5.0171585 -4.724824][-1.763247 -1.5552664 -1.7335882 -0.60963058 0.47102642 0.99639225 0.84475422 0.34162807 -0.0058588982 -1.5547266 -3.4517345 -4.30188 -4.5238743 -6.0260553 -5.9090252][-4.3232093 -3.984575 -3.43228 -2.7414393 -2.4569726 -1.8375206 -2.0951681 -2.1160192 -2.6592774 -4.0617619 -5.0062218 -5.6067867 -6.422987 -6.6885657 -6.8122497][-4.9255276 -5.024334 -5.1109056 -4.4986138 -3.787421 -2.5545025 -2.490355 -3.3705058 -4.2081876 -4.6900997 -6.1767941 -6.3045797 -7.0171041 -7.2592497 -7.6816349][-3.9809163 -4.6629939 -4.9015064 -4.7764268 -4.9162264 -4.6361055 -3.9868016 -3.6950884 -4.7704296 -5.155056 -6.7826953 -7.0784435 -7.6646767 -7.504487 -7.6864486][-4.8691206 -4.76949 -4.39441 -4.598609 -3.9904428 -2.9086533 -3.4847503 -3.3067822 -3.6078353 -3.9267068 -4.9386282 -5.7365088 -6.7006664 -6.8464165 -7.4280505][-4.9062028 -4.8149672 -4.3232307 -4.2038679 -4.2912807 -4.331645 -3.4843898 -3.5480685 -4.2082767 -4.5286131 -5.3155756 -5.8311629 -6.7862434 -7.2536373 -6.8426681]]...]
INFO - root - 2017-12-15 22:15:56.133704: step 66810, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 47h:12m:46s remains)
INFO - root - 2017-12-15 22:16:02.500521: step 66820, loss = 0.33, batch loss = 0.21 (12.8 examples/sec; 0.625 sec/batch; 46h:07m:41s remains)
INFO - root - 2017-12-15 22:16:08.852257: step 66830, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 47h:12m:04s remains)
INFO - root - 2017-12-15 22:16:15.220620: step 66840, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 47h:07m:44s remains)
INFO - root - 2017-12-15 22:16:21.716726: step 66850, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 46h:59m:42s remains)
INFO - root - 2017-12-15 22:16:28.018377: step 66860, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:46m:54s remains)
INFO - root - 2017-12-15 22:16:34.449250: step 66870, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:04m:38s remains)
INFO - root - 2017-12-15 22:16:40.954988: step 66880, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 47h:54m:57s remains)
INFO - root - 2017-12-15 22:16:47.325231: step 66890, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 46h:10m:59s remains)
INFO - root - 2017-12-15 22:16:53.729799: step 66900, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 49h:05m:02s remains)
2017-12-15 22:16:54.229927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0693064 -4.1418571 -3.8384395 -4.64991 -5.2462196 -5.3882747 -4.5661063 -4.2344875 -3.8642139 -4.4352636 -4.9012108 -4.9703293 -5.6305346 -6.043118 -6.1855249][-3.7809191 -3.0723333 -4.0259166 -4.4024506 -4.1461754 -4.0909758 -3.8622038 -4.105464 -3.8619068 -4.7363486 -5.1658583 -5.3445406 -6.0796704 -6.1237392 -6.4975514][-4.9741383 -4.3151064 -3.5258865 -2.7838016 -3.1945014 -3.2484856 -2.8022871 -2.4689012 -2.7243819 -3.8016996 -4.6944294 -5.5263405 -6.5685773 -6.5320244 -6.8599243][-5.0434341 -4.1928139 -4.2817206 -2.9998097 -1.5980372 -0.77738762 -1.2568731 -0.76196671 -0.67854929 -2.1719208 -3.3768783 -4.9160328 -6.1926422 -6.6848607 -7.1690993][-4.5006142 -4.2465343 -2.7967577 -1.6563339 -0.43002081 0.67212582 0.94098186 1.462038 1.0527258 -0.63246346 -1.5581436 -3.445015 -5.062675 -6.4142938 -7.4421177][-3.297627 -2.5385084 -1.3429747 -0.011879921 0.9531498 1.8221693 2.87109 2.9873257 2.1334572 0.45790386 -0.54396868 -2.5882812 -4.498786 -5.9244633 -7.026412][-2.022542 -1.349587 -0.42759752 1.0428324 2.2069216 2.9874153 3.5954733 3.3070154 3.2045832 1.1536264 -0.53634548 -2.1659184 -3.9118497 -5.3286743 -6.5563965][-1.1793995 -0.70341587 0.13409662 1.694581 3.2945833 3.9285936 3.8620567 3.27493 2.971921 0.88359261 -0.24216843 -2.1633415 -3.8209543 -4.69693 -5.71177][-1.2831817 -0.88742781 -0.34189034 0.92239571 2.6138315 3.5389194 3.5233545 3.3818073 3.2299442 0.85873318 -0.6983614 -2.3074484 -3.4372077 -4.2217474 -5.5727978][-2.7267041 -1.9788036 -1.5464363 -0.65484095 0.41518784 1.6466713 2.0833035 2.5636396 2.3051338 0.3203001 -0.96900368 -2.5955782 -4.1025486 -4.3142424 -5.1696787][-4.792038 -3.6044235 -3.1566162 -2.6966786 -2.1658173 -1.6293273 -0.82089043 -0.11691046 0.21698093 -0.90146065 -3.0089269 -4.1722784 -4.6458206 -5.3831034 -6.0697007][-7.0518894 -6.3026152 -5.5597658 -4.8365669 -4.3816943 -4.3558607 -4.1381121 -3.901437 -3.2121735 -3.7969389 -4.5430369 -5.2135105 -6.2073059 -6.3823147 -6.954083][-8.5016346 -7.9349213 -7.1707735 -6.9127393 -6.8583221 -6.5252624 -5.8018465 -5.999393 -6.0587878 -5.818285 -6.316556 -6.4750271 -7.0767646 -6.948925 -7.1427984][-8.0136967 -8.4331722 -9.028451 -8.25675 -7.0969977 -6.9765644 -7.527668 -6.9383364 -6.3517189 -6.5797181 -6.92171 -6.4301629 -6.6019244 -6.6912527 -6.8189831][-8.4510355 -8.24755 -7.6177235 -7.8990893 -8.5048895 -7.8761988 -6.9826217 -6.784852 -7.24228 -6.8841329 -6.341928 -6.6943207 -6.6802225 -6.7914243 -6.6856103]]...]
INFO - root - 2017-12-15 22:17:00.675095: step 66910, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 47h:19m:18s remains)
INFO - root - 2017-12-15 22:17:07.126411: step 66920, loss = 0.29, batch loss = 0.18 (11.5 examples/sec; 0.694 sec/batch; 51h:10m:36s remains)
INFO - root - 2017-12-15 22:17:13.509824: step 66930, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:48m:37s remains)
INFO - root - 2017-12-15 22:17:19.947934: step 66940, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 46h:06m:56s remains)
INFO - root - 2017-12-15 22:17:26.293246: step 66950, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 46h:06m:51s remains)
INFO - root - 2017-12-15 22:17:32.731610: step 66960, loss = 0.35, batch loss = 0.24 (12.9 examples/sec; 0.618 sec/batch; 45h:37m:13s remains)
INFO - root - 2017-12-15 22:17:39.070916: step 66970, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 47h:46m:36s remains)
INFO - root - 2017-12-15 22:17:45.513791: step 66980, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 47h:20m:52s remains)
INFO - root - 2017-12-15 22:17:51.965163: step 66990, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 47h:55m:31s remains)
INFO - root - 2017-12-15 22:17:58.428361: step 67000, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 48h:01m:50s remains)
2017-12-15 22:17:58.967717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1781807 -4.1105309 -3.9520011 -3.5495811 -3.1750889 -2.6529307 -1.9407315 -1.3432364 -0.89275026 -2.4131141 -3.5718622 -5.12572 -6.3231168 -7.6151247 -8.4259253][-3.8414388 -4.2799768 -4.5236645 -4.5299048 -4.2398634 -3.5107889 -2.765748 -2.1685495 -1.651535 -2.9031706 -4.0756626 -5.5785437 -7.1523643 -7.9736357 -8.762989][-2.3487206 -2.5015607 -2.8601961 -3.0193167 -2.8248258 -2.3843994 -1.9140663 -1.4830556 -1.0431328 -2.6223264 -4.0869508 -5.5746679 -6.6767321 -7.880908 -8.8544874][-1.6447663 -1.8054953 -1.9808817 -1.9378009 -1.7945681 -1.428813 -1.0888925 -0.99480772 -0.86583281 -2.6392717 -4.1420984 -5.8326168 -7.04294 -7.7669611 -8.4456778][-2.1444721 -1.7453194 -1.3407116 -1.0064082 -0.55572176 0.10856485 0.64560795 0.54525089 0.34309769 -1.8595586 -3.6212058 -5.4125633 -6.7873187 -7.7432461 -8.422019][-2.4602666 -1.9053183 -1.4684815 -0.64101267 0.31812477 1.4280739 2.0996923 1.947053 1.5791731 -1.1410332 -3.3511672 -5.2101564 -6.5783625 -7.5365467 -8.343133][-3.0586548 -2.4054656 -1.4062762 -0.041481972 1.3776512 2.762392 3.4670753 3.4827442 3.0555992 -0.019553661 -2.7000008 -4.9957991 -6.3785787 -7.4303365 -8.3352289][-3.3066554 -2.5191092 -1.6354189 -0.035581112 1.7702341 3.2663527 3.9238949 4.1666393 3.8507271 0.90527916 -1.7395644 -4.387722 -6.1395593 -7.2558675 -7.960125][-3.8219707 -3.0031672 -2.131947 -0.71472692 0.88842869 2.1727257 2.6648722 2.8674536 2.8620176 0.24444962 -2.1816211 -4.3336744 -5.9569306 -7.1803079 -7.9767513][-4.2854128 -3.6477504 -2.7977052 -1.4945197 -0.090703011 1.012639 1.302145 1.5536985 1.6034479 -1.0885839 -3.0038614 -4.85944 -6.0829172 -7.1564913 -8.0523129][-5.7110443 -5.2218075 -4.4344587 -3.2573571 -2.0995193 -1.0148926 -0.61531544 -0.78480959 -0.97277975 -3.1563234 -4.8519249 -6.0480661 -6.8204732 -7.5120211 -8.0006437][-6.758009 -6.2819357 -5.2431149 -4.4176159 -3.5653162 -2.6231141 -2.6386104 -2.9032478 -3.1061935 -4.8219447 -5.8692417 -6.637732 -7.1251636 -7.5799713 -7.8483806][-7.1254196 -6.4945965 -5.74954 -4.8863087 -4.2756577 -3.995199 -3.9155092 -4.1298952 -4.5433464 -5.7201414 -6.6924629 -6.8828783 -7.19594 -7.2122984 -7.1192369][-7.2638879 -6.9280806 -6.3133593 -5.5926561 -4.9804826 -4.6216745 -4.7397385 -5.0446682 -5.1283607 -5.9668961 -6.3482461 -6.579165 -6.6105003 -6.7156525 -6.4756517][-8.0452881 -7.6114669 -7.3664141 -6.9181137 -6.5059557 -6.3272519 -6.5982742 -6.9890742 -7.2411928 -7.1014948 -6.753778 -6.3975945 -6.2254419 -6.1196623 -5.8800664]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 22:18:05.493595: step 67010, loss = 0.24, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 48h:16m:15s remains)
INFO - root - 2017-12-15 22:18:11.899258: step 67020, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 47h:37m:30s remains)
INFO - root - 2017-12-15 22:18:18.316065: step 67030, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 46h:53m:38s remains)
INFO - root - 2017-12-15 22:18:24.691270: step 67040, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:01m:24s remains)
INFO - root - 2017-12-15 22:18:31.120052: step 67050, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 46h:05m:20s remains)
INFO - root - 2017-12-15 22:18:37.544755: step 67060, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 46h:46m:35s remains)
INFO - root - 2017-12-15 22:18:43.998605: step 67070, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 47h:01m:53s remains)
INFO - root - 2017-12-15 22:18:50.321336: step 67080, loss = 0.28, batch loss = 0.16 (13.1 examples/sec; 0.611 sec/batch; 45h:01m:46s remains)
INFO - root - 2017-12-15 22:18:56.747949: step 67090, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 47h:01m:30s remains)
INFO - root - 2017-12-15 22:19:03.163922: step 67100, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 46h:46m:37s remains)
2017-12-15 22:19:03.718843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9535408 -2.8790011 -3.1349325 -3.3601837 -3.5431972 -3.7959871 -3.907784 -3.8750517 -3.4962559 -4.4001875 -5.326086 -6.22734 -6.7617645 -6.9519424 -7.7628603][-2.7733564 -2.763905 -2.9934716 -3.1785159 -3.4526649 -3.8799942 -4.1931949 -4.2101283 -3.9509132 -5.1380582 -6.0606031 -7.1325502 -7.7391291 -7.9592285 -8.5486612][-3.4766865 -2.9715858 -2.8849559 -2.8547831 -3.0553007 -3.3390269 -3.339467 -3.661839 -3.6258569 -4.949317 -6.1067691 -7.3748097 -8.2261324 -8.36868 -9.09787][-3.7660098 -3.2795935 -2.8837805 -2.261631 -1.9739542 -1.990046 -1.9111104 -2.0139775 -1.8621626 -3.1325712 -4.5573368 -5.8462267 -6.8204637 -7.2667394 -8.201436][-3.6654677 -2.8975248 -2.1123471 -1.4472365 -0.93872166 -0.52237463 -0.15728283 -0.13821268 0.12257814 -0.98825121 -2.4781251 -4.8257909 -6.3752265 -7.115716 -8.1162195][-3.79458 -2.9653339 -2.4823427 -1.3226013 -0.36650658 0.37097931 1.1804886 1.5057726 1.9123278 0.29354811 -1.6484728 -3.9141614 -5.8091393 -6.5233455 -7.9871483][-3.9917912 -2.9816456 -2.0588756 -0.52527523 1.0171938 1.8460169 2.7502823 3.5185232 4.3711519 2.7542992 0.73235416 -1.8111944 -4.368433 -5.5269585 -7.1031761][-4.0764179 -3.1439123 -2.2336388 -0.64704752 1.3339329 2.9722261 4.3212557 4.7991438 5.4163122 3.9347229 2.2717762 -0.323493 -2.7094855 -4.1436467 -5.7942715][-5.2833967 -4.0445509 -3.1837802 -1.7990146 -0.28963757 1.5019245 3.3361359 3.8409262 3.9576378 1.9762764 0.51528549 -1.565999 -3.2722716 -4.34845 -6.0922723][-7.2717009 -6.0663533 -4.8352919 -3.5476294 -2.2352643 -1.2422428 0.20991898 1.439353 1.9459343 -0.68572187 -2.6820459 -4.4897089 -5.6389828 -6.08179 -6.9278784][-8.9430857 -8.3564243 -7.096961 -6.0349693 -4.7033296 -3.5650129 -2.3667622 -1.7720127 -1.2743387 -2.6531582 -4.213171 -5.9664049 -7.3809848 -7.763186 -8.2898722][-10.056507 -9.5915909 -8.9584169 -7.5917954 -6.4180889 -5.2512903 -4.3283768 -3.7609713 -3.3925929 -4.2174578 -5.1560497 -6.4250126 -7.3714333 -7.5052509 -7.8193665][-9.6837463 -9.5093975 -8.9033871 -7.7797489 -6.8591232 -5.8948641 -4.9906187 -4.770184 -4.7708745 -5.357883 -5.4828672 -6.1194077 -6.4562035 -6.4862323 -6.897141][-9.8948669 -9.3978672 -8.6266394 -7.5458632 -6.5534639 -5.7791595 -5.2055035 -5.3020449 -5.3869419 -5.6483564 -5.858037 -5.7349787 -5.8245292 -5.6640892 -6.1394854][-10.145068 -10.051685 -9.6270638 -8.7073469 -7.6912909 -6.8018851 -6.0235586 -5.8760028 -5.9668255 -6.1053805 -6.2072086 -6.2330379 -6.0171661 -5.6782045 -5.6282778]]...]
INFO - root - 2017-12-15 22:19:10.294106: step 67110, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 47h:38m:07s remains)
INFO - root - 2017-12-15 22:19:16.660834: step 67120, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 47h:12m:52s remains)
INFO - root - 2017-12-15 22:19:23.068539: step 67130, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 49h:39m:37s remains)
INFO - root - 2017-12-15 22:19:29.380015: step 67140, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 46h:31m:06s remains)
INFO - root - 2017-12-15 22:19:35.767685: step 67150, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 47h:25m:33s remains)
INFO - root - 2017-12-15 22:19:42.204138: step 67160, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 45h:47m:36s remains)
INFO - root - 2017-12-15 22:19:48.564534: step 67170, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 46h:58m:20s remains)
INFO - root - 2017-12-15 22:19:54.943664: step 67180, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 46h:44m:43s remains)
INFO - root - 2017-12-15 22:20:01.338485: step 67190, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 48h:19m:50s remains)
INFO - root - 2017-12-15 22:20:07.714590: step 67200, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 45h:26m:56s remains)
2017-12-15 22:20:08.315093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3010073 -3.0123796 -2.737957 -2.2248473 -1.8626151 -1.8797746 -2.1957774 -2.2500253 -2.1321483 -2.814642 -3.5213885 -4.735918 -5.4768476 -6.3337903 -6.356854][-2.6517749 -2.6293492 -2.6423869 -2.4548888 -2.5302062 -2.2073827 -1.9638762 -1.569324 -1.367641 -3.0227666 -4.0715761 -4.8616905 -5.4102993 -6.578567 -6.5910006][-1.8534031 -2.1805682 -2.5467772 -2.3572836 -2.2860203 -1.8051524 -1.1651073 -0.92327023 -0.67428017 -1.3677106 -2.1048493 -3.3706055 -3.9770739 -5.2498174 -5.7717209][-2.3976669 -1.9881082 -1.4585233 -1.24715 -1.3090925 -1.0435643 -0.93127966 -0.30138493 0.36393261 -1.0726571 -2.3067741 -3.2690701 -3.9313266 -5.4062185 -5.4674087][-3.012466 -1.9966722 -1.1655378 -0.24436522 0.38433552 0.85362148 0.82583809 0.53748226 0.23168468 -0.79516315 -1.4011054 -2.4311309 -3.3293767 -4.9867 -5.52823][-2.1070428 -1.377574 -0.54242277 -0.072614193 0.48520374 1.4319954 1.8582067 2.0026369 2.0780993 0.45860291 -0.76548386 -1.933166 -2.7858667 -4.4263592 -4.7354116][-1.950758 -1.0054579 -0.41019344 0.57256603 1.3399582 1.8420382 2.4033136 2.225523 1.7428684 0.60344028 -0.16718531 -1.4478779 -2.3494291 -3.9517331 -4.6377][-2.9248185 -1.4992156 0.049154282 1.316721 2.5041809 3.1393194 2.8993063 2.677947 2.5045881 0.79044724 -0.20857525 -0.96112585 -1.5925536 -3.3017812 -3.7938845][-1.3992839 -0.41026068 0.98185825 2.1155519 2.4898252 2.8430185 3.0256071 2.5331364 1.8936558 0.88446236 0.40397072 -0.83948469 -1.5074139 -2.902215 -3.8661907][-2.5286856 -0.30727577 1.4714022 2.5084677 3.2212019 2.570035 1.7025566 1.217905 1.2372847 0.080071449 -0.38024521 -0.75668049 -1.2396502 -2.8532057 -3.2861338][-2.5839405 -1.2531991 -0.10558176 0.45662022 0.071331024 -0.13651943 -0.28834438 -0.41616726 -0.1388073 -0.8151989 -1.1585279 -2.0527911 -2.4744473 -3.4296827 -4.4607739][-2.9908032 -1.8700409 -1.1017342 -1.2628007 -1.7162266 -1.7504511 -1.8163114 -1.8178749 -1.7359686 -2.5806103 -3.1608458 -3.3864665 -3.6716771 -4.681776 -5.7247138][-3.0088983 -2.6490078 -2.5093732 -2.6111808 -3.4138856 -3.9395716 -4.2388334 -3.73849 -3.1013927 -3.3856902 -3.7563131 -4.6164894 -6.2159233 -6.6822877 -6.784502][-4.9606843 -4.9245882 -4.1677542 -4.6117487 -4.8856726 -4.9257727 -5.1185074 -4.8868876 -4.6305914 -4.6008687 -4.937603 -5.6932712 -6.4488149 -7.4664965 -7.9374313][-4.3564391 -5.06157 -5.8581491 -6.1032114 -6.3735328 -6.5501223 -6.6846986 -6.7718511 -6.3468909 -5.9438949 -6.3341427 -6.8673286 -7.56313 -7.5556917 -7.677228]]...]
INFO - root - 2017-12-15 22:20:14.722182: step 67210, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 47h:43m:34s remains)
INFO - root - 2017-12-15 22:20:21.035115: step 67220, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 46h:45m:48s remains)
INFO - root - 2017-12-15 22:20:27.358745: step 67230, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 45h:35m:48s remains)
INFO - root - 2017-12-15 22:20:33.814621: step 67240, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 46h:38m:59s remains)
INFO - root - 2017-12-15 22:20:40.215887: step 67250, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 46h:33m:29s remains)
INFO - root - 2017-12-15 22:20:46.682959: step 67260, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:05m:36s remains)
INFO - root - 2017-12-15 22:20:53.063096: step 67270, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 47h:32m:48s remains)
INFO - root - 2017-12-15 22:20:59.412571: step 67280, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 47h:10m:30s remains)
INFO - root - 2017-12-15 22:21:05.766133: step 67290, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 46h:31m:09s remains)
INFO - root - 2017-12-15 22:21:12.184008: step 67300, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 47h:10m:19s remains)
2017-12-15 22:21:12.740343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5382628 -2.0322227 -1.8214021 -2.0541897 -2.3683434 -2.3483944 -2.8479786 -2.6722908 -2.4941816 -3.4965043 -4.4183397 -6.3450737 -7.9509683 -8.8239889 -9.1909771][-1.899076 -1.4311991 -1.2082686 -1.7285948 -2.8498087 -3.2840557 -3.8401411 -3.7286532 -3.4655733 -4.3276024 -5.29022 -6.80802 -7.9242997 -8.5783663 -9.2805738][-1.9801817 -1.7975216 -1.5655308 -1.4971123 -2.655972 -2.6469312 -2.9751062 -3.2504969 -3.1335158 -4.2109995 -5.1520424 -6.624608 -7.5863791 -8.310708 -8.4594221][-1.6065588 -1.1495261 -1.6708121 -1.495213 -1.6242061 -1.3724089 -1.0814424 -1.10467 -1.7327032 -3.2613564 -4.2504883 -5.8218937 -6.7607675 -7.201818 -7.4755044][-2.3637156 -1.7472072 -1.5818653 -1.515748 -1.8264575 -1.0052528 -0.25552607 -0.17459774 -0.29980803 -1.9208837 -3.2143102 -5.1161757 -6.2092052 -6.8246679 -6.7501779][-2.8566475 -2.4957595 -2.3585715 -1.803329 -1.0542884 -0.08888483 1.0982723 1.4808531 1.3890142 0.024632931 -1.3458543 -3.7890792 -5.4163513 -6.0503397 -6.2488985][-3.7746665 -3.2532477 -2.3489089 -1.3829169 -0.1112566 1.113122 1.9157963 2.1422548 2.0355892 0.21327066 -0.98934174 -3.3835535 -5.1374164 -6.1477361 -6.5613761][-2.950037 -2.7681479 -2.2645192 -1.2749925 0.06448698 1.4510813 2.4123898 2.6121063 2.4625759 0.94740295 -0.49113464 -3.0919137 -4.8856173 -6.1447353 -6.5132709][-2.5648141 -2.3664441 -1.7010694 -0.51458883 0.32573032 1.6210375 2.3302336 2.3717394 2.1940851 0.42449284 -0.57941103 -2.8451757 -4.79953 -5.7964787 -6.1358185][-3.8643789 -3.0062475 -2.6840305 -1.255868 0.16763401 1.3840523 2.0178394 1.8322163 1.4803181 -0.47457504 -1.9668083 -3.7192416 -5.1609268 -6.1489415 -6.6383181][-4.314127 -4.1427784 -3.5228915 -2.2243147 -1.5541606 -0.16069126 0.76127434 0.76997375 0.50048065 -1.2118888 -2.4482994 -4.405221 -5.7800264 -6.22161 -6.7041254][-5.9455814 -5.2899575 -4.6832161 -3.8863785 -2.5351043 -1.5133204 -0.82557917 -0.38017464 -0.15420961 -1.6121564 -2.8894787 -4.1624303 -5.4784908 -6.2940559 -6.6825404][-5.8035855 -4.905118 -4.1011758 -2.8297267 -1.6115527 -0.73616505 0.02374649 -0.45071173 -0.7568059 -1.6131582 -2.7797875 -3.8942258 -4.7347393 -5.40059 -6.3854623][-5.22867 -4.21469 -3.1308212 -1.7809496 -0.53831911 0.31712246 0.3560667 -0.031207085 -0.2681284 -1.4927568 -2.3056202 -2.9590497 -3.9607418 -4.688189 -5.4228315][-4.9361897 -4.0687113 -3.5254898 -2.6486793 -2.1299148 -1.3296423 -0.80551434 -0.65388584 -0.91998053 -1.5348449 -2.2401953 -3.2900457 -4.0989156 -4.920732 -5.8115859]]...]
INFO - root - 2017-12-15 22:21:19.163739: step 67310, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 47h:39m:41s remains)
INFO - root - 2017-12-15 22:21:25.659630: step 67320, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 46h:32m:37s remains)
INFO - root - 2017-12-15 22:21:32.027420: step 67330, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 46h:38m:40s remains)
INFO - root - 2017-12-15 22:21:38.429707: step 67340, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 47h:20m:34s remains)
INFO - root - 2017-12-15 22:21:44.878378: step 67350, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 45h:38m:41s remains)
INFO - root - 2017-12-15 22:21:51.369986: step 67360, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 46h:39m:39s remains)
INFO - root - 2017-12-15 22:21:57.791312: step 67370, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 46h:13m:30s remains)
INFO - root - 2017-12-15 22:22:04.218412: step 67380, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 49h:08m:31s remains)
INFO - root - 2017-12-15 22:22:10.598365: step 67390, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 46h:21m:51s remains)
INFO - root - 2017-12-15 22:22:16.980957: step 67400, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 46h:52m:55s remains)
2017-12-15 22:22:17.526964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2462606 -5.3855138 -5.353693 -5.9864531 -6.6448889 -6.65628 -6.9713984 -7.0454607 -7.105597 -7.264565 -7.561255 -7.9915676 -7.0067768 -6.7434077 -6.60838][-5.6612711 -5.8528881 -5.9435024 -5.3436384 -5.2652178 -5.7337322 -6.2688522 -6.7819991 -7.4398947 -7.6204691 -8.7913589 -10.069082 -8.9201546 -8.015295 -7.2485094][-7.0218239 -6.2954116 -5.4175534 -4.7915015 -4.5653076 -4.6010561 -4.622725 -5.0928369 -5.6134953 -6.779717 -8.7013206 -10.040299 -9.397191 -8.8812275 -8.2460871][-7.6311145 -6.9841876 -6.2668419 -4.2786222 -3.0751123 -2.1700821 -1.9652605 -2.5030837 -2.7539334 -4.1848669 -5.9468136 -7.8288064 -8.104331 -8.2234735 -8.2347555][-8.2464991 -6.6970716 -5.363904 -3.500659 -2.2784786 -0.33234835 0.78921318 0.78082752 0.57556152 -0.75558853 -2.3771348 -4.3526583 -5.1584444 -6.3878374 -7.1520185][-7.4570904 -6.2081456 -4.9736466 -2.7782397 -1.0407233 0.902606 1.9234095 2.5563021 2.979147 1.8874636 0.88239765 -1.2900896 -2.7249508 -3.9651504 -5.065917][-6.3276958 -5.2276058 -3.8357217 -1.9010229 -0.2926631 1.3334274 2.5978622 3.0189543 3.0098743 1.7262774 0.85453224 -0.97271585 -1.8138065 -3.553349 -4.7165933][-5.5126781 -4.2200661 -2.9045696 -0.24631119 1.4772558 1.6980457 2.1654701 3.0096197 3.3118286 2.0042152 0.22942114 -2.2246032 -2.5925975 -3.5826774 -4.323751][-6.3290963 -5.675776 -4.3098164 -1.7125039 0.20851088 1.9332628 2.4983549 1.9889822 2.4414539 1.8255777 0.24833822 -2.1446042 -2.8069472 -4.0113878 -4.5900283][-6.6423922 -6.2362552 -5.8090544 -4.5730925 -2.8137364 -0.58766603 0.38724327 0.79602242 0.86508083 -0.49796057 -1.1370749 -2.4957724 -3.0424008 -3.463181 -4.3488045][-7.2277455 -7.5234628 -7.9526854 -7.6050391 -7.0957313 -5.8896685 -4.5178246 -3.241703 -2.5874734 -2.8750615 -3.4594893 -4.6849146 -4.7650261 -5.5945988 -5.9628658][-8.0055656 -8.17239 -8.2215 -8.0810585 -7.8487349 -7.6519923 -7.5067539 -6.9422264 -6.1417637 -5.8853397 -5.7412324 -5.8932385 -5.9573956 -6.2694893 -6.2328005][-8.0121565 -8.452693 -9.1426067 -9.4045324 -9.2630844 -8.6951551 -8.1676226 -8.1769333 -8.4244022 -7.9193773 -8.16559 -7.9280868 -6.8740973 -5.9558659 -5.5794048][-7.4056807 -7.335114 -6.9318686 -6.8208537 -6.8946104 -6.7219963 -6.7894392 -6.589592 -6.9903865 -7.18357 -7.7577105 -7.8239989 -7.6047244 -7.0312781 -6.3865685][-7.4771008 -7.3452024 -7.0061522 -6.7028937 -6.6842613 -6.3147821 -5.636714 -5.5807266 -6.1559591 -6.4785385 -6.9002419 -7.1804748 -7.0594006 -7.1784363 -6.7375]]...]
INFO - root - 2017-12-15 22:22:23.897524: step 67410, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 47h:06m:54s remains)
INFO - root - 2017-12-15 22:22:30.255561: step 67420, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 46h:40m:09s remains)
INFO - root - 2017-12-15 22:22:36.681767: step 67430, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 46h:07m:27s remains)
INFO - root - 2017-12-15 22:22:43.098790: step 67440, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 47h:02m:10s remains)
INFO - root - 2017-12-15 22:22:49.418766: step 67450, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 46h:32m:32s remains)
INFO - root - 2017-12-15 22:22:55.776102: step 67460, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 47h:49m:12s remains)
INFO - root - 2017-12-15 22:23:02.171656: step 67470, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 46h:29m:17s remains)
INFO - root - 2017-12-15 22:23:08.553364: step 67480, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 47h:55m:58s remains)
INFO - root - 2017-12-15 22:23:14.870592: step 67490, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 47h:41m:07s remains)
INFO - root - 2017-12-15 22:23:21.193137: step 67500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 47h:03m:41s remains)
2017-12-15 22:23:21.702615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5805054 -4.5125027 -5.0893631 -5.9120512 -6.005722 -5.6884003 -4.9717407 -4.2182512 -3.6516643 -4.2913718 -5.738142 -6.930944 -8.0683365 -8.6530275 -8.6359453][-4.078896 -4.3464451 -5.1185789 -5.9235783 -6.3841834 -5.9597054 -5.3946953 -4.7473493 -4.0404434 -4.4198189 -5.5047369 -6.5419292 -7.9111285 -8.2022448 -8.9571419][-4.7208467 -4.5877419 -4.8109303 -5.3886352 -5.396419 -4.9144297 -3.9524095 -3.2442079 -2.81781 -3.1128039 -3.9792404 -5.3049212 -6.40278 -7.3056426 -7.6841459][-3.8241684 -3.97784 -4.2313867 -3.9369369 -3.8134747 -3.3310008 -1.8682599 -1.2444606 -0.79324389 -1.7227859 -3.2953634 -4.6343579 -6.2382836 -6.8522158 -7.3728628][-3.764874 -3.3978872 -3.1490016 -2.8570285 -2.3516111 -1.4464216 -0.27537966 0.34535885 0.69174862 -0.42673349 -2.0500011 -4.0745335 -6.0497141 -7.0051107 -7.7414064][-3.3571596 -3.3392053 -2.9701133 -2.3423791 -1.419291 -0.16139936 0.71631527 1.2358551 1.9676771 0.71874428 -1.1120701 -2.9593139 -4.8114862 -6.1698236 -7.1273446][-5.1062608 -4.28742 -2.9555216 -2.3175168 -0.58724737 1.0718746 2.2457943 2.9717703 3.1457338 1.7702618 -0.38244486 -2.6300216 -4.9002333 -6.1802034 -7.3843408][-5.2054176 -4.3639841 -3.7799864 -2.295814 -0.48326731 0.95536041 2.2861519 2.8327827 3.2699604 1.8874426 -0.46527243 -3.0542636 -5.4922562 -6.8889003 -7.9341855][-4.7967362 -3.9780319 -3.1810503 -1.8323445 -0.62037373 0.40884972 1.2290401 1.6028872 1.7799406 0.67022896 -1.0889215 -3.5099821 -5.5911112 -6.8410091 -7.8147693][-5.1563005 -4.6288905 -3.8687341 -2.8794737 -1.933629 -0.60872126 0.010070324 0.14963675 0.26027966 -0.9432292 -2.6903424 -4.7105908 -6.2903666 -7.0961714 -7.9757752][-6.7988682 -6.4975958 -5.8751321 -5.481431 -4.4259715 -3.1752734 -2.6211162 -2.4558282 -2.2805843 -3.029635 -4.4414005 -6.1240087 -7.296339 -7.9628344 -8.6767349][-6.8441076 -6.4626484 -5.766408 -4.9251657 -4.0714455 -3.2140627 -3.2003188 -3.424418 -3.4707875 -4.3502164 -5.3308854 -6.3441496 -7.1373806 -7.443778 -7.8899937][-7.8075447 -7.2456031 -6.2940578 -5.5348148 -4.9743128 -4.4306335 -4.4551749 -4.7849197 -4.9343648 -5.5490837 -6.1061006 -6.4426985 -7.2410917 -7.236105 -7.1313152][-6.8819 -6.4601731 -5.9907341 -5.3456154 -4.7136431 -4.30969 -4.5695071 -4.6437044 -4.5469975 -4.9823389 -5.1440964 -5.9747229 -6.7326427 -6.761662 -6.7319775][-7.5913887 -7.1960607 -6.5085421 -5.7902756 -5.5380297 -5.3208942 -5.4745588 -5.5964093 -5.6553574 -5.5892897 -5.8516569 -6.3946366 -6.5989461 -6.6190929 -6.5620265]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 22:23:29.596943: step 67510, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 47h:11m:13s remains)
INFO - root - 2017-12-15 22:23:36.015588: step 67520, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 47h:23m:52s remains)
INFO - root - 2017-12-15 22:23:42.383009: step 67530, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 48h:00m:06s remains)
INFO - root - 2017-12-15 22:23:48.763259: step 67540, loss = 0.34, batch loss = 0.23 (12.3 examples/sec; 0.652 sec/batch; 47h:59m:20s remains)
INFO - root - 2017-12-15 22:23:55.145963: step 67550, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:12m:50s remains)
INFO - root - 2017-12-15 22:24:01.502802: step 67560, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 46h:31m:40s remains)
INFO - root - 2017-12-15 22:24:07.913697: step 67570, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 47h:48m:04s remains)
INFO - root - 2017-12-15 22:24:14.358211: step 67580, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 47h:26m:45s remains)
INFO - root - 2017-12-15 22:24:20.722198: step 67590, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 48h:14m:29s remains)
INFO - root - 2017-12-15 22:24:27.170251: step 67600, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 46h:11m:09s remains)
2017-12-15 22:24:27.742027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7528133 -3.7160249 -3.9816406 -4.1185918 -4.4163942 -3.9468513 -3.2295551 -2.4975519 -1.7636561 -3.5369725 -5.1949744 -6.4986215 -7.5125742 -8.5203629 -8.4988022][-3.8728213 -3.0845222 -3.0435724 -3.1553936 -3.8021193 -3.7869415 -3.7666354 -3.4663606 -2.9266748 -3.810034 -4.7257719 -6.5832715 -8.3933287 -8.7766781 -8.24892][-3.9689977 -3.3894243 -3.0983272 -3.0588937 -2.5961075 -2.2100625 -2.0286846 -2.5682335 -2.6773615 -3.5942645 -4.4043007 -5.6398273 -6.4888968 -7.3287826 -8.0553684][-3.3535166 -2.2821531 -1.8884854 -1.3694081 -1.0691943 -0.6569128 -0.4261775 -0.18456697 0.388031 -0.92746353 -2.6917448 -4.0789967 -5.3566542 -6.7555323 -6.92349][-4.8947659 -3.1441283 -1.7978535 -0.92925787 -0.18290424 0.79439735 1.6080208 1.8181534 2.112957 0.56042767 -0.82618761 -2.940372 -4.6611748 -6.1181431 -6.2720566][-4.2545609 -3.2925887 -2.3977723 -1.0640974 0.38377094 1.7659702 3.0905933 3.5097837 3.8095303 1.9756012 0.31183147 -1.8296833 -3.6127715 -4.6770992 -4.6996613][-3.8429816 -2.5542898 -1.0667844 0.16039848 1.6412821 3.2256575 4.4023542 4.8030815 4.9236012 2.7693005 0.99424553 -0.95464087 -2.8078895 -3.9492018 -4.5776911][-2.3404989 -1.3356133 -0.3311944 0.6501236 2.0363026 3.4305277 4.5164242 5.0114546 5.1305103 2.7058077 0.51452446 -1.4155126 -3.7783129 -4.8236523 -5.1215963][-3.2099748 -1.7815571 -0.66679955 0.057852745 0.84942722 2.1178293 3.16965 3.8343935 3.7533865 1.6091108 -0.23079109 -2.03013 -4.2488217 -5.2069893 -5.557301][-3.9259579 -2.7800751 -1.5647197 -0.34352636 0.52401352 0.80938625 0.9940424 1.4058113 1.36238 -0.88261318 -2.5019603 -3.7843168 -5.3246455 -5.7849474 -5.4638996][-5.19755 -4.7215509 -3.8899035 -2.9786429 -2.1612592 -1.3885632 -1.0128984 -1.600317 -2.0262098 -3.6611724 -4.6429462 -5.0865602 -6.1603236 -6.4051352 -6.2894173][-7.8856874 -7.3429656 -6.8748751 -5.8289518 -4.8446746 -3.9068894 -3.7077906 -3.9307737 -4.3247795 -5.6454287 -6.8334856 -6.7930765 -6.9754429 -6.9081492 -6.7788577][-8.2851677 -8.5574818 -8.3834391 -7.795651 -6.7915382 -5.9478645 -5.712121 -5.3605766 -5.50288 -6.222991 -6.7323842 -6.7624736 -7.1648631 -6.6841311 -5.80491][-7.7213063 -7.4740329 -7.404355 -6.9699268 -6.05532 -5.4353085 -5.4807854 -5.538909 -5.6392593 -5.9423265 -5.84916 -5.8392425 -5.7788582 -5.888361 -5.6353703][-7.1019616 -7.3777423 -7.6190386 -7.4128532 -7.3290925 -6.9961228 -6.424336 -6.4266291 -6.5670853 -6.6833739 -6.5136786 -6.4754796 -6.2543249 -5.8373332 -5.4871716]]...]
INFO - root - 2017-12-15 22:24:34.264991: step 67610, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 48h:37m:07s remains)
INFO - root - 2017-12-15 22:24:40.658396: step 67620, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 45h:42m:30s remains)
INFO - root - 2017-12-15 22:24:46.991734: step 67630, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 45h:44m:15s remains)
INFO - root - 2017-12-15 22:24:53.435082: step 67640, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 48h:35m:19s remains)
INFO - root - 2017-12-15 22:24:59.877665: step 67650, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 46h:21m:45s remains)
INFO - root - 2017-12-15 22:25:06.190356: step 67660, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 46h:19m:35s remains)
INFO - root - 2017-12-15 22:25:12.529499: step 67670, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 47h:03m:49s remains)
INFO - root - 2017-12-15 22:25:18.937920: step 67680, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 46h:45m:49s remains)
INFO - root - 2017-12-15 22:25:25.250538: step 67690, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.618 sec/batch; 45h:27m:26s remains)
INFO - root - 2017-12-15 22:25:31.740967: step 67700, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 46h:55m:53s remains)
2017-12-15 22:25:32.240219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1715031 -3.9874849 -3.758563 -4.4094753 -4.7442341 -4.9247627 -4.6641006 -4.3771439 -3.9161589 -4.8877716 -4.7583923 -5.5011992 -5.835845 -6.6281967 -7.35724][-3.9333587 -4.211916 -5.0817127 -5.5228024 -6.2479539 -5.9928904 -5.21332 -4.7758045 -4.0502911 -4.4565248 -4.41875 -5.0609994 -5.7281165 -6.558517 -7.5046716][-3.9022624 -3.7867744 -3.7826514 -4.341795 -5.0139952 -5.0931835 -4.3844223 -3.3251123 -2.5607023 -3.2145352 -3.0963697 -4.0909915 -4.6974897 -5.7567949 -6.5645905][-2.7595754 -2.3965101 -2.4555225 -2.6952724 -3.1703868 -2.6673603 -1.9069462 -1.4628572 -0.67534208 -1.4791203 -2.0516319 -3.101388 -4.0104733 -4.8857985 -5.7499776][-3.6038685 -2.64334 -2.1612277 -2.0351243 -2.0501847 -1.265749 -0.53936005 0.22416687 0.70271778 -0.55715656 -1.3454356 -2.9146004 -4.0201311 -5.0663033 -5.9704895][-3.4146938 -2.6017938 -2.1389809 -1.1174774 -0.20376873 0.5757513 1.5498667 1.9697084 2.2885685 0.46722984 -0.9155097 -2.7323837 -3.7001853 -4.8286309 -5.8634167][-4.4326611 -3.3225532 -2.409698 -0.79478025 0.5459938 1.8944836 2.9052591 3.1820097 3.4422064 1.1990423 -0.51885939 -2.4697676 -3.9969764 -5.2956219 -6.3354573][-4.4772882 -3.2679334 -2.3284082 -0.73586655 0.8836422 2.3216143 3.6268396 3.8830976 3.8795614 1.6275139 -0.03247118 -2.1713209 -3.7836823 -5.1627507 -6.3319645][-4.3428035 -3.4631281 -2.91743 -1.3132062 0.058880806 1.4622145 2.6903753 3.0781717 3.1615782 0.94220829 -0.56596184 -2.4201355 -3.8108013 -5.0621233 -5.9851031][-5.210887 -4.3607707 -3.691133 -2.4769316 -1.3831954 -0.43027449 0.76394367 0.90751457 0.7663269 -1.0470195 -2.170836 -3.4853129 -4.4584141 -5.4180193 -6.6403332][-6.6798267 -5.9621153 -5.5273514 -4.7466588 -4.2723579 -3.266871 -2.0352135 -2.2253008 -2.268548 -3.8182847 -4.6978931 -5.4306498 -5.9600639 -6.3950348 -7.1023712][-7.4407997 -6.9800467 -6.7252569 -6.3621254 -6.0928421 -5.6345568 -5.0767231 -4.8238511 -4.6436353 -5.8363471 -6.4217315 -6.9124007 -7.2684879 -7.1709461 -7.2411866][-7.9148021 -7.6798487 -7.60015 -7.313952 -7.1409717 -6.8288465 -6.1838741 -6.2703371 -6.3685131 -6.9408274 -7.1274829 -6.95683 -6.8029847 -6.5762196 -6.6759109][-6.37488 -6.10744 -6.071559 -6.0043635 -6.0957856 -5.9194951 -5.5929585 -5.5618129 -5.5904083 -6.1958103 -6.079031 -6.2993088 -6.3639317 -6.2778473 -5.8636131][-6.9306612 -6.5912547 -6.458725 -6.428246 -6.6488371 -6.8146048 -6.5726957 -6.6932306 -6.7740421 -6.7925315 -6.6262131 -6.49995 -6.4856653 -6.4395485 -6.5521917]]...]
INFO - root - 2017-12-15 22:25:38.761058: step 67710, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 47h:17m:47s remains)
INFO - root - 2017-12-15 22:25:45.135474: step 67720, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 46h:10m:19s remains)
INFO - root - 2017-12-15 22:25:51.507101: step 67730, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 46h:40m:46s remains)
INFO - root - 2017-12-15 22:25:57.996700: step 67740, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 46h:55m:31s remains)
INFO - root - 2017-12-15 22:26:04.458912: step 67750, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 49h:24m:47s remains)
INFO - root - 2017-12-15 22:26:10.846464: step 67760, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 46h:24m:19s remains)
INFO - root - 2017-12-15 22:26:17.193113: step 67770, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 46h:21m:44s remains)
INFO - root - 2017-12-15 22:26:23.474625: step 67780, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.626 sec/batch; 45h:59m:55s remains)
INFO - root - 2017-12-15 22:26:29.836930: step 67790, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 46h:17m:05s remains)
INFO - root - 2017-12-15 22:26:36.225605: step 67800, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 47h:28m:23s remains)
2017-12-15 22:26:36.745815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0864544 -6.3183722 -7.3686371 -7.9041276 -7.9083953 -6.9702358 -4.8346996 -4.1978831 -3.1709719 -5.513938 -5.484777 -6.2406912 -6.4235849 -7.8455358 -9.238018][-5.9982734 -6.8614607 -5.6968975 -6.4224081 -6.4568009 -6.3481808 -7.0211096 -6.3499308 -5.4873204 -7.0802317 -6.8376613 -7.2156415 -7.4373074 -6.8868265 -7.5075278][-5.3279562 -6.1985397 -6.1109676 -5.4721828 -3.0697803 -3.0618792 -4.0360665 -5.5108733 -6.1020627 -6.6238136 -6.0537457 -7.3442278 -7.7832046 -7.2181306 -6.7440133][-2.3468471 -2.7649589 -3.6635208 -3.961931 -3.3815379 -2.1215205 -1.1075907 -2.6656294 -3.9066164 -5.3006725 -5.8439903 -6.0096149 -5.4544826 -6.017704 -6.9363136][-2.0758152 -2.3608127 -1.9640827 -1.3932257 -1.2183986 -0.80396605 0.19094944 -0.327044 -1.7857342 -4.0536404 -3.9442399 -4.3805847 -4.5434055 -4.2839146 -3.9758673][-2.3301725 -2.0067058 -1.1444502 -0.34429359 1.3012571 1.6810608 1.3539019 0.75613594 0.11695051 -1.546505 -1.9024873 -2.8797722 -2.1892138 -2.4655743 -3.1472931][-1.6470346 -2.3124218 -2.7564721 -1.8497286 0.846303 1.8462181 1.5258961 1.3004837 0.73431492 -0.91399479 -0.90058994 -1.479928 -1.9065619 -3.8590057 -4.0291743][-2.6876135 -2.6147938 -1.5452676 -1.6092005 -0.66491318 1.1810322 1.5319023 1.0428791 1.5845871 -0.073340893 -0.63234425 -1.8603673 -2.235661 -3.7823322 -5.67099][-3.3416328 -2.63378 -2.317337 -2.3449593 -1.7968659 0.28728628 1.5014868 1.338665 1.7361145 0.40517521 -1.2363234 -3.3280439 -3.6687803 -4.4249105 -4.9407616][-5.1714978 -3.7482731 -2.0908895 -1.3430419 -1.0015254 -0.98896217 -0.38647366 0.0005235672 0.96074104 -0.67981195 -1.8085341 -3.668561 -5.0891662 -5.9179697 -6.0373034][-7.3787708 -6.5869665 -5.4120131 -3.6744061 -1.761548 -1.3829784 -1.470046 -1.6942158 -1.9810176 -4.1147041 -4.6352129 -5.0374441 -6.0821819 -6.1658115 -6.1012182][-6.6609416 -6.3805265 -7.6763978 -6.8679171 -5.7338715 -4.9369292 -3.9758036 -4.73966 -3.6518936 -4.7496619 -5.97841 -6.3740177 -6.9866905 -6.8409686 -6.4948468][-8.3018618 -6.8041973 -6.3286791 -7.8584218 -8.0671949 -6.8414388 -6.1296539 -6.1858335 -6.3756051 -6.9808116 -6.7934852 -6.2582674 -6.3591137 -5.948318 -5.6361561][-7.278285 -6.0651426 -5.7296333 -5.8084574 -5.1464381 -6.2916489 -6.3758059 -6.5581913 -6.5283775 -6.9998341 -6.8982034 -5.6422167 -5.1142874 -5.8243322 -5.979537][-4.8086777 -5.5422134 -6.7066774 -7.1516495 -6.8424449 -6.9456987 -6.9782686 -7.4511509 -7.324894 -6.9904327 -6.7382994 -6.5314364 -6.8064737 -6.0565367 -5.4538136]]...]
INFO - root - 2017-12-15 22:26:43.193626: step 67810, loss = 0.37, batch loss = 0.25 (12.5 examples/sec; 0.639 sec/batch; 46h:56m:53s remains)
INFO - root - 2017-12-15 22:26:49.629317: step 67820, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.614 sec/batch; 45h:09m:31s remains)
INFO - root - 2017-12-15 22:26:56.087831: step 67830, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 45h:40m:34s remains)
INFO - root - 2017-12-15 22:27:02.456753: step 67840, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 47h:48m:45s remains)
INFO - root - 2017-12-15 22:27:08.867291: step 67850, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 47h:51m:23s remains)
INFO - root - 2017-12-15 22:27:15.349394: step 67860, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.671 sec/batch; 49h:21m:14s remains)
INFO - root - 2017-12-15 22:27:21.793604: step 67870, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 45h:31m:53s remains)
INFO - root - 2017-12-15 22:27:28.192262: step 67880, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 48h:31m:16s remains)
INFO - root - 2017-12-15 22:27:34.561420: step 67890, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 46h:25m:27s remains)
INFO - root - 2017-12-15 22:27:41.038527: step 67900, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 46h:23m:32s remains)
2017-12-15 22:27:41.590516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2498999 -1.7052741 -1.4353251 -1.3401866 -1.5548873 -1.9248719 -2.2734051 -2.6195126 -2.8508515 -3.9510508 -5.0504417 -6.2652617 -7.1645184 -7.3184938 -7.462523][-1.6470857 -1.6811361 -1.5031486 -1.1511893 -1.0708976 -1.5459704 -2.1650443 -2.6474476 -2.7581525 -3.9745514 -4.9367533 -5.9727526 -6.8454194 -6.9415932 -7.0182314][-1.8772535 -1.2408223 -1.0701532 -0.96487045 -1.0215135 -1.3361239 -1.9379311 -2.3278484 -2.4494295 -3.5755157 -4.347744 -5.3967166 -6.420145 -6.7527046 -6.979063][-2.4888878 -2.0259385 -1.714179 -1.0517292 -0.88993168 -1.1088924 -1.27033 -1.3042402 -1.2235117 -2.5859103 -3.6710863 -5.0058336 -6.0817118 -6.5429621 -6.7879972][-3.5194807 -2.835206 -2.0619335 -1.6998272 -1.4767632 -0.77141666 -0.11918926 0.10502863 0.33704567 -1.1107383 -2.4927177 -4.191288 -5.5359526 -5.9127207 -6.1216888][-5.6426287 -4.7698879 -3.7930562 -2.6021376 -1.3789864 -0.28872252 0.56470394 1.1685066 1.6605654 0.28081512 -1.0845966 -3.1576004 -4.8887615 -5.6091161 -6.0061226][-5.3471174 -5.1204538 -4.1065683 -2.9255271 -1.5176368 0.35225677 1.6075668 2.2016106 2.5626516 1.1194839 -0.36931038 -2.5008512 -4.3541889 -5.0618119 -5.4875917][-6.0528345 -4.7288504 -3.6875224 -2.7596006 -1.1492138 0.66565323 2.1932859 3.3761539 3.7466574 1.9511833 0.24416542 -1.7579193 -3.5413461 -4.516099 -5.1645803][-6.5733695 -5.8067436 -4.3715687 -2.4970136 -0.973207 0.786952 1.9594021 2.727107 3.0455627 1.3437357 -0.056066036 -2.0491667 -3.9806988 -4.8721018 -5.4353356][-7.5099363 -6.5539551 -5.361269 -3.7816429 -1.7335906 -0.060152054 1.1339817 1.8713837 1.8460093 -0.10155058 -1.5263429 -3.443295 -4.8171496 -5.5109572 -6.063036][-7.9425039 -7.3275495 -6.6421294 -5.0265779 -3.4053254 -2.2658591 -1.1878529 -0.6249404 -0.3562727 -2.3380566 -3.9102628 -5.3845062 -6.4183259 -7.0235586 -7.4676228][-9.3261843 -8.5092764 -7.10749 -5.7043614 -4.6482811 -3.7320807 -2.8733439 -2.7179923 -2.6641936 -4.1425076 -5.1664977 -6.1554022 -6.6937509 -7.2309179 -7.5196347][-9.4337292 -8.9066677 -8.2305107 -7.1066952 -5.7696047 -4.8228254 -4.2697659 -4.389101 -4.4253531 -5.493803 -6.0730033 -5.931529 -5.9972892 -6.0133371 -5.9529238][-8.7225733 -8.2971411 -7.8252697 -7.0851836 -6.5206757 -5.8636503 -5.1979008 -5.4786658 -5.558847 -6.2408738 -6.1492734 -5.9161572 -5.8220644 -5.7425642 -5.7433367][-9.0428085 -8.1716824 -7.3714819 -7.0060005 -6.9338236 -6.4986 -6.4687748 -6.7745419 -7.1579881 -6.9992085 -6.6383286 -6.3526773 -5.8895025 -5.1700597 -4.7433987]]...]
INFO - root - 2017-12-15 22:27:48.024939: step 67910, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 46h:57m:17s remains)
INFO - root - 2017-12-15 22:27:54.376714: step 67920, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 47h:31m:40s remains)
INFO - root - 2017-12-15 22:28:00.749171: step 67930, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 47h:14m:14s remains)
INFO - root - 2017-12-15 22:28:07.185631: step 67940, loss = 0.33, batch loss = 0.22 (11.7 examples/sec; 0.681 sec/batch; 50h:04m:24s remains)
INFO - root - 2017-12-15 22:28:13.597157: step 67950, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 45h:56m:32s remains)
INFO - root - 2017-12-15 22:28:19.933460: step 67960, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 45h:41m:28s remains)
INFO - root - 2017-12-15 22:28:26.275235: step 67970, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 46h:05m:14s remains)
INFO - root - 2017-12-15 22:28:32.730928: step 67980, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 47h:20m:35s remains)
INFO - root - 2017-12-15 22:28:39.039895: step 67990, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 45h:51m:11s remains)
INFO - root - 2017-12-15 22:28:45.469625: step 68000, loss = 0.33, batch loss = 0.21 (12.8 examples/sec; 0.623 sec/batch; 45h:46m:35s remains)
2017-12-15 22:28:45.977534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0443683 -6.0713067 -5.8865891 -6.2640076 -6.1182966 -5.4853425 -4.6496229 -3.6161418 -2.4472342 -1.9530568 -2.9917502 -3.8278558 -4.1103735 -5.1528482 -6.269918][-5.1456876 -5.3355432 -5.31592 -6.051506 -6.2286081 -6.2892036 -5.9454603 -4.6168427 -3.080997 -2.3131375 -3.1417394 -4.0062432 -4.8109684 -5.5820742 -6.0076981][-5.0546341 -5.2979045 -5.5660629 -6.4437041 -7.352591 -7.6043487 -7.3603554 -6.5337782 -5.3066263 -4.4418631 -4.823123 -5.4112043 -6.1044664 -6.8597269 -7.0832596][-6.2211022 -6.1281543 -5.4146547 -5.3355103 -5.7313051 -5.8431091 -5.7701955 -5.035491 -3.8784494 -3.8915811 -5.3755121 -6.4598026 -6.8625507 -7.7023573 -8.1322708][-4.8315396 -4.279335 -3.9731705 -3.0370374 -2.2506266 -1.961772 -1.8778133 -1.2746873 -0.50681639 -1.3100085 -3.2920146 -5.367178 -6.6473527 -7.7156048 -8.0921917][-3.7484608 -2.8700519 -1.8940606 -0.47661877 0.44276714 1.1103182 1.8701754 2.461031 3.0024767 2.0934677 3.4332275e-05 -2.4198461 -4.5653543 -6.3437734 -7.4453259][-3.9272504 -2.0457168 -0.11448336 1.0720615 2.3841658 3.7279549 4.4577026 4.0651207 3.9435225 3.0696163 0.87165451 -1.0983748 -3.0013351 -4.9606657 -6.275938][-2.4955964 -1.6944199 -0.60811329 1.1568804 2.8161201 4.0933819 5.2315331 5.1536655 4.8069019 3.1276503 0.7099781 -1.0325489 -2.5452237 -4.1334505 -5.3405085][-3.4967375 -2.3667469 -1.0605035 0.81717873 2.2470369 2.5349646 2.6210365 3.4734793 4.4511404 2.7038012 -0.38450861 -1.8924327 -2.8511066 -3.9875839 -4.7358427][-4.4168882 -3.739954 -3.0092597 -1.5786476 -0.25972176 0.83638382 1.2108097 0.93283367 0.81214523 0.047712326 -1.6097941 -3.41038 -4.8565521 -5.4351435 -5.9312344][-7.0842056 -7.1004009 -6.4910278 -5.9896436 -5.3654919 -3.7226944 -2.6069617 -2.7177148 -2.966692 -3.7904682 -4.7906408 -6.1478529 -7.3955445 -8.085062 -8.5576324][-8.6532278 -8.44756 -7.9070225 -7.3495135 -6.7700377 -6.7530355 -6.5208712 -5.7020326 -5.05103 -6.0618062 -6.73774 -7.0172839 -7.4837379 -8.0373669 -8.7300835][-10.256755 -10.396349 -9.9161282 -8.879859 -7.7941093 -7.2970791 -7.0757642 -7.5791359 -7.88383 -7.9586058 -8.0954018 -8.0122442 -7.3180165 -6.635015 -6.807054][-7.7670813 -8.5336857 -9.1201715 -8.6568079 -7.8668752 -7.1123724 -6.4259472 -6.4198847 -6.6864491 -6.9932523 -7.4137187 -7.52091 -7.6176891 -7.0757704 -6.5388594][-6.7239056 -6.6949892 -6.8028321 -7.1414943 -7.5236158 -7.6203146 -7.4983883 -7.4795089 -6.97428 -6.939518 -7.183672 -7.5290623 -7.7729697 -7.8113036 -7.5054665]]...]
INFO - root - 2017-12-15 22:28:52.380490: step 68010, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:55m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 22:28:58.684447: step 68020, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 46h:16m:49s remains)
INFO - root - 2017-12-15 22:29:05.035819: step 68030, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 46h:40m:36s remains)
INFO - root - 2017-12-15 22:29:11.532445: step 68040, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 47h:21m:39s remains)
INFO - root - 2017-12-15 22:29:17.901180: step 68050, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 46h:10m:54s remains)
INFO - root - 2017-12-15 22:29:24.410527: step 68060, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:55m:05s remains)
INFO - root - 2017-12-15 22:29:30.729602: step 68070, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 46h:14m:43s remains)
INFO - root - 2017-12-15 22:29:37.150767: step 68080, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 48h:20m:26s remains)
INFO - root - 2017-12-15 22:29:43.609361: step 68090, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 47h:44m:50s remains)
INFO - root - 2017-12-15 22:29:50.016387: step 68100, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.623 sec/batch; 45h:45m:20s remains)
2017-12-15 22:29:50.558053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7544165 -3.9133072 -3.955529 -4.2377434 -4.1386213 -3.887116 -3.6369028 -3.1959829 -2.5482597 -3.6703877 -4.6764297 -5.426836 -6.3750548 -6.8283539 -7.4419622][-3.4430351 -3.7779775 -4.3367023 -4.69794 -4.5875111 -4.024478 -3.8681362 -3.4727135 -3.0209265 -4.0842972 -5.0037909 -6.2282548 -7.322032 -7.8359523 -8.36501][-2.5504551 -2.6784887 -2.9859028 -3.3201737 -3.210732 -2.7182884 -2.5066113 -2.3009777 -2.0009432 -2.9392858 -3.4751706 -4.2888432 -5.6019282 -6.2487636 -7.0498629][-2.2158718 -2.2459202 -2.35752 -2.4026542 -2.0537581 -1.6099977 -1.1144676 -0.95122051 -0.79961205 -2.0312095 -2.9002571 -3.7639048 -5.2892075 -5.9537578 -6.6987157][-2.7783356 -2.2537546 -1.7484155 -1.3646445 -0.65766478 -0.037387371 0.46803284 0.62385368 0.68281364 -0.8890667 -1.9914403 -3.0847907 -4.657239 -5.5986576 -6.5538163][-3.9275479 -3.10471 -2.0827579 -1.2320867 -0.43285608 0.44972038 1.2622519 1.2687397 1.1456394 -0.58756256 -1.8339529 -3.024775 -4.689105 -5.6177568 -6.4702482][-4.6851144 -3.8136137 -2.7259946 -1.3649211 0.077271938 1.1056128 1.9373446 1.9746809 1.7539196 -0.31132936 -1.8680987 -3.1741247 -4.9156742 -5.85713 -6.74765][-4.3708057 -3.537519 -2.2754197 -0.74371958 0.45175457 1.29953 2.0331125 2.0244637 1.7605696 -0.34247923 -2.10919 -3.6537452 -5.3952165 -6.2755318 -7.0587254][-4.7556081 -3.8489356 -2.8555107 -1.4400263 -0.097047806 0.59489346 1.0322399 1.3299112 1.4129982 -0.72030258 -2.445673 -3.8559043 -5.6730356 -6.7541051 -7.4699588][-4.930934 -4.2484417 -3.3693204 -2.0574245 -0.78251648 -0.34104967 0.0010147095 0.30698586 0.49658775 -1.7469287 -3.3969493 -4.8282118 -6.3483429 -7.3609695 -8.3238239][-5.9825788 -5.2791109 -4.3818541 -3.5103369 -2.6350799 -2.2363057 -2.0277786 -1.8369842 -1.571116 -3.5141311 -5.061512 -6.1609297 -7.362781 -8.058672 -8.5721645][-5.6653948 -5.0764856 -4.6125555 -3.905704 -3.681088 -3.5796623 -3.556128 -3.4429569 -3.3303404 -4.5497108 -5.6176767 -6.4666438 -7.2104063 -7.9168596 -8.43362][-6.2631578 -5.8099761 -5.4071608 -5.041923 -4.7257433 -4.6680007 -4.7598772 -4.8448386 -4.7951255 -5.55357 -6.4176092 -6.7314587 -7.2841339 -7.6441708 -7.780652][-5.5798354 -5.3342867 -5.3134875 -4.8681211 -4.6074986 -4.7786641 -4.9005527 -4.9746585 -5.0117512 -5.7294364 -5.9523077 -6.0782804 -6.1858959 -6.5065503 -6.7881989][-7.1410818 -6.8794928 -6.5089951 -6.2896357 -6.1079025 -6.2002997 -6.4146357 -6.5997691 -6.7104082 -6.5708647 -6.5480585 -6.552722 -6.3680162 -6.250608 -6.1420894]]...]
INFO - root - 2017-12-15 22:29:56.927236: step 68110, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 46h:32m:40s remains)
INFO - root - 2017-12-15 22:30:03.252582: step 68120, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 46h:12m:16s remains)
INFO - root - 2017-12-15 22:30:09.695948: step 68130, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 48h:32m:12s remains)
INFO - root - 2017-12-15 22:30:15.996518: step 68140, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:39m:39s remains)
INFO - root - 2017-12-15 22:30:22.549954: step 68150, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 47h:11m:09s remains)
INFO - root - 2017-12-15 22:30:28.902657: step 68160, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 46h:53m:06s remains)
INFO - root - 2017-12-15 22:30:35.276947: step 68170, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 47h:36m:56s remains)
INFO - root - 2017-12-15 22:30:41.637358: step 68180, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 46h:34m:10s remains)
INFO - root - 2017-12-15 22:30:48.038532: step 68190, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.635 sec/batch; 46h:35m:58s remains)
INFO - root - 2017-12-15 22:30:54.447103: step 68200, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 46h:21m:33s remains)
2017-12-15 22:30:54.996686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5796142 -1.8448014 -2.3694549 -2.6031504 -2.8322511 -3.1341887 -3.0777183 -2.989531 -2.6914043 -3.883563 -4.2615232 -5.8866153 -6.4106116 -7.7255497 -7.581121][-2.4594941 -2.5137205 -2.8856287 -3.2374191 -3.589138 -3.6685905 -3.7532809 -3.804836 -3.4660087 -4.6739745 -5.2240939 -6.9096107 -7.5341825 -8.7629 -8.3508816][-3.5639591 -3.4387226 -3.65477 -3.644105 -3.6984215 -3.7670889 -3.7307813 -3.6274366 -3.5729179 -5.1256118 -5.6773973 -7.45856 -8.3308077 -9.7278051 -9.4115152][-4.3835945 -4.27615 -4.0573511 -3.4693398 -3.1393762 -2.8358321 -2.449204 -2.3775263 -1.8924694 -3.1905556 -4.0146451 -6.0981617 -7.0021911 -8.567214 -8.6489677][-4.9153347 -4.4852066 -3.9975967 -3.4846716 -2.9772997 -1.9449325 -0.98314333 -0.62339258 0.042771816 -1.0745034 -1.7145309 -4.275178 -5.6902189 -7.55637 -7.8316655][-5.9622908 -5.484499 -4.8564587 -3.80158 -2.4621587 -1.0143418 0.017131805 0.93151188 1.8057165 0.58654976 -0.27352905 -3.1366882 -4.88618 -7.0288138 -7.5356188][-5.8160868 -4.8977194 -4.0013638 -2.1486988 -0.47360134 0.652503 1.5081768 2.8011208 4.2008085 2.9495125 1.7362328 -1.415658 -3.4249587 -5.7799554 -6.4446836][-5.0290394 -4.0212641 -3.0008078 -0.9684968 0.929945 2.4455805 3.4048824 4.1654816 5.2126493 4.2617397 3.2780876 0.20788002 -1.8490167 -4.4081879 -5.1608791][-5.1280737 -4.20389 -3.1188159 -1.5204425 -0.0040478706 1.8154287 3.2427578 3.6768179 3.8800611 2.3423557 1.5761166 -0.85105181 -2.4976168 -4.8054209 -5.6614089][-5.8741245 -4.6811161 -3.5886388 -2.2083082 -1.1350369 -0.053431034 0.80539417 1.7784214 1.8919096 -1.0342999 -2.0330839 -4.1773076 -5.3364782 -6.9843907 -7.0619454][-7.3122611 -6.3096275 -5.7306876 -4.321105 -3.3475847 -2.284596 -1.5837889 -0.79809284 -0.66088772 -2.7778778 -4.373702 -7.0706878 -7.6890888 -8.6759958 -7.7506876][-8.4888859 -7.8865 -7.0171156 -6.0173421 -5.3194165 -4.1690583 -3.5127635 -3.2693896 -3.2852912 -4.151556 -5.1147857 -7.2659554 -8.0305243 -8.6833105 -7.9145727][-8.8715754 -8.5045977 -8.0247622 -7.1512003 -6.4166183 -5.4910688 -4.759088 -4.4594965 -4.47337 -5.1647162 -5.5214596 -6.6737118 -7.5667176 -8.1941872 -7.4514055][-8.8211708 -8.6497059 -8.3591118 -7.5232315 -6.5265675 -5.7556171 -5.0945091 -4.942904 -5.0532045 -5.559618 -6.0400286 -6.5581007 -6.5910311 -7.0578418 -6.449738][-9.4364872 -9.5494928 -9.4709587 -8.49316 -7.4804444 -6.5918522 -6.2376251 -5.957757 -6.1314707 -6.3142185 -6.6636643 -6.7722368 -6.5934572 -6.3139877 -6.0208735]]...]
INFO - root - 2017-12-15 22:31:01.401997: step 68210, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 47h:40m:28s remains)
INFO - root - 2017-12-15 22:31:07.768410: step 68220, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 45h:43m:08s remains)
INFO - root - 2017-12-15 22:31:14.097848: step 68230, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.620 sec/batch; 45h:30m:15s remains)
INFO - root - 2017-12-15 22:31:20.415758: step 68240, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 46h:15m:03s remains)
INFO - root - 2017-12-15 22:31:26.926662: step 68250, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 46h:24m:13s remains)
INFO - root - 2017-12-15 22:31:33.284130: step 68260, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 47h:02m:39s remains)
INFO - root - 2017-12-15 22:31:39.760089: step 68270, loss = 0.30, batch loss = 0.19 (11.9 examples/sec; 0.674 sec/batch; 49h:26m:39s remains)
INFO - root - 2017-12-15 22:31:46.232179: step 68280, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:42m:31s remains)
INFO - root - 2017-12-15 22:31:52.645008: step 68290, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 47h:23m:19s remains)
INFO - root - 2017-12-15 22:31:58.986311: step 68300, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 47h:41m:38s remains)
2017-12-15 22:31:59.518399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0915942 -2.9374456 -2.9679394 -3.2496924 -3.7116475 -3.6659613 -3.3179789 -3.5947189 -3.865242 -5.7931147 -7.1768026 -7.610877 -8.3215046 -8.854619 -9.04056][-3.6654234 -3.439115 -3.622725 -3.5368204 -3.5314379 -3.5152774 -3.3536367 -3.3987002 -3.8432083 -6.2375851 -7.6997528 -8.43886 -9.2756662 -9.1011152 -9.2325821][-3.7715962 -3.2569218 -2.8118386 -2.1536551 -1.8528261 -1.426199 -0.863904 -1.1518154 -2.0550108 -4.7487316 -6.3277111 -7.2682304 -8.064024 -8.7840843 -9.1083651][-4.3416805 -3.3555703 -2.4070172 -1.622272 -0.86728573 -0.5746541 0.15352535 0.39901829 -0.12708616 -2.8440652 -4.2890191 -5.583621 -6.8912039 -7.4301171 -8.0991268][-4.1410394 -2.9125361 -1.706872 -0.43505669 0.47586155 1.307456 1.8681183 1.6589146 1.3857517 -1.2179656 -3.0344005 -4.057085 -5.4356036 -6.4863729 -7.0992837][-3.4464803 -2.6252928 -1.6975284 -0.64527512 0.516984 1.6012297 2.3779106 2.4037914 1.9841833 -0.74420404 -1.8999214 -3.2286954 -5.3306003 -6.5955329 -7.787394][-3.3850932 -2.1276026 -0.77917194 -0.27597427 0.80281448 1.707427 2.1832218 2.2488785 2.3986492 -0.19393158 -1.5962296 -2.881659 -4.8403478 -6.1516438 -7.1723471][-2.8355112 -2.0525312 -0.97631216 -0.0022158623 0.79271126 1.2852201 1.7376165 2.4106436 2.660038 0.032779694 -1.1959229 -2.8762097 -5.1473107 -6.3741212 -7.3186502][-3.0237889 -2.5040116 -1.7094569 -0.54473829 0.37788868 0.95909405 1.4173031 1.45961 1.6724949 -0.79919195 -2.0670605 -3.4700527 -5.5749454 -6.8270226 -7.8147287][-2.4987717 -2.2340655 -1.6900349 -0.86384773 -0.42358303 0.031521797 0.096573353 0.13135481 0.28137541 -2.2283182 -3.2133436 -4.4877791 -6.1389236 -7.2896013 -8.1881075][-4.2846746 -3.480392 -2.6657286 -2.1625233 -1.9293299 -1.6893587 -1.712646 -1.9488239 -1.8158622 -3.955775 -5.15757 -5.507236 -6.3879719 -7.5333319 -8.1452579][-5.95555 -5.471137 -4.87397 -3.9565334 -3.4143343 -3.15521 -3.2261534 -3.2504463 -3.1317377 -5.0826578 -5.938416 -5.6900673 -6.3204346 -7.04253 -7.3912659][-6.5943851 -6.2266359 -5.3503308 -4.5853958 -4.3673167 -4.1881309 -4.20538 -4.4157648 -4.4038916 -5.5087466 -6.4980717 -6.3951712 -6.41881 -6.4612389 -6.4771738][-6.7616487 -6.1108608 -5.5169535 -4.6745381 -4.2024193 -4.1410456 -4.6136332 -4.6815815 -4.3082991 -4.992075 -5.4234176 -5.7370977 -6.0441246 -6.1905627 -6.3372297][-7.7841892 -7.2314386 -6.6609478 -6.2905116 -5.7470517 -5.5398455 -5.7199922 -5.8256345 -6.0070157 -5.8579297 -5.6990023 -5.6971111 -5.6065111 -5.8519497 -5.964159]]...]
INFO - root - 2017-12-15 22:32:05.955047: step 68310, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 47h:20m:42s remains)
INFO - root - 2017-12-15 22:32:12.417647: step 68320, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 48h:06m:33s remains)
INFO - root - 2017-12-15 22:32:18.876601: step 68330, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 46h:51m:24s remains)
INFO - root - 2017-12-15 22:32:25.281857: step 68340, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 46h:55m:59s remains)
INFO - root - 2017-12-15 22:32:31.726695: step 68350, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 45h:23m:11s remains)
INFO - root - 2017-12-15 22:32:38.024653: step 68360, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 46h:14m:39s remains)
INFO - root - 2017-12-15 22:32:44.404428: step 68370, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 46h:41m:41s remains)
INFO - root - 2017-12-15 22:32:50.751075: step 68380, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 45h:31m:13s remains)
INFO - root - 2017-12-15 22:32:57.064626: step 68390, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 45h:40m:07s remains)
INFO - root - 2017-12-15 22:33:03.450755: step 68400, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 47h:39m:10s remains)
2017-12-15 22:33:03.974986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5685072 -8.8445921 -8.8586988 -8.2656555 -7.9124708 -7.2615747 -6.5003471 -5.5829349 -4.2911472 -3.5750642 -2.7193203 -3.1391382 -3.3176632 -3.6213708 -5.0546694][-8.4956522 -8.4260731 -8.4269276 -8.4468269 -8.7716465 -8.29569 -7.7959886 -6.8988357 -5.2990952 -5.359273 -4.7722874 -4.5844145 -5.1750174 -5.3329268 -5.7643843][-6.512816 -6.483602 -6.6594348 -6.5718 -6.2429414 -6.4233694 -6.4082022 -5.7612481 -4.9470153 -5.3372893 -4.8009543 -5.4670029 -5.5849476 -5.5991125 -6.1858926][-4.5610428 -4.4995465 -4.6580296 -4.403821 -4.4661655 -4.136322 -3.6482806 -2.9720669 -2.1349683 -2.7746973 -2.8728418 -3.6027036 -4.4022236 -5.0196095 -5.439805][-3.346436 -3.085381 -2.8798079 -2.6568446 -2.4723988 -1.7941885 -0.91327047 -0.32387638 0.2687993 -0.60349131 -1.473505 -3.217936 -4.7839403 -5.402029 -5.8087049][-3.3831534 -2.7112775 -2.3833756 -1.7930956 -0.79399443 -0.0011725426 0.97561646 1.670558 2.0261068 0.74590397 -0.49485636 -2.4965663 -4.1346054 -5.1622009 -6.0202188][-3.6105847 -2.7518115 -1.6655927 -0.56957912 0.41278172 1.4944172 2.6439972 2.9923306 3.5046062 1.7754698 -0.18261814 -2.5936513 -4.8111148 -5.55764 -6.2858887][-4.018961 -3.0177002 -2.0557775 -0.37915897 1.0671301 2.4939356 3.8659945 4.0962143 4.1506462 2.0184622 -0.075000763 -2.591496 -4.6037607 -5.6858511 -6.8228245][-5.3927402 -4.6279869 -3.7575927 -2.1255283 -0.46690226 1.045948 2.0508652 2.5913725 3.1571026 1.2438078 -0.68981028 -3.02987 -4.9219093 -5.8712311 -6.4821467][-7.1906614 -6.5125227 -5.645915 -4.0944176 -2.7632375 -1.4439011 -0.13715506 0.22457409 0.39951229 -1.5439348 -2.9618311 -4.390203 -5.6033192 -6.2152119 -6.8452544][-9.0578365 -8.5122967 -8.1627178 -7.0865641 -6.2547655 -5.1727705 -3.8872521 -3.832058 -3.5343113 -4.5477772 -5.0887232 -5.7920976 -6.5861235 -6.7931852 -7.1498652][-10.598586 -9.8387642 -9.1576138 -8.4249115 -7.815855 -6.6589708 -5.4523258 -5.2345734 -4.8881459 -6.3683286 -6.9489846 -6.7259808 -6.9412842 -7.2511868 -7.6307993][-9.3664322 -9.0828743 -8.8294935 -8.2896128 -7.6240315 -7.1688371 -6.4966264 -6.4354496 -6.5248079 -7.2460194 -7.6897764 -7.7232809 -7.9620624 -7.6028252 -7.5365028][-5.831687 -5.6253281 -5.6173563 -5.6791463 -5.3730755 -5.2011557 -4.9060287 -5.2039289 -5.6507158 -6.4694767 -6.8821049 -6.5596886 -6.8573618 -7.3014379 -6.991838][-3.9845293 -4.3650131 -4.6693673 -4.6599932 -4.8509769 -4.7049494 -4.5410981 -4.706831 -5.1050949 -5.5937262 -6.0175991 -6.4014268 -6.8070149 -6.9279962 -6.7659578]]...]
INFO - root - 2017-12-15 22:33:10.419957: step 68410, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 46h:25m:15s remains)
INFO - root - 2017-12-15 22:33:16.761441: step 68420, loss = 0.33, batch loss = 0.22 (13.0 examples/sec; 0.614 sec/batch; 45h:02m:30s remains)
INFO - root - 2017-12-15 22:33:23.176515: step 68430, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 47h:07m:06s remains)
INFO - root - 2017-12-15 22:33:29.597316: step 68440, loss = 0.36, batch loss = 0.25 (12.7 examples/sec; 0.631 sec/batch; 46h:15m:55s remains)
INFO - root - 2017-12-15 22:33:35.999126: step 68450, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 46h:37m:44s remains)
INFO - root - 2017-12-15 22:33:42.423464: step 68460, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 48h:14m:05s remains)
INFO - root - 2017-12-15 22:33:48.791800: step 68470, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 46h:27m:24s remains)
INFO - root - 2017-12-15 22:33:55.151519: step 68480, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 47h:09m:48s remains)
INFO - root - 2017-12-15 22:34:01.547735: step 68490, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:40m:05s remains)
INFO - root - 2017-12-15 22:34:08.051236: step 68500, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 48h:00m:01s remains)
2017-12-15 22:34:08.570385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0610614 -5.7673745 -6.010983 -5.9572725 -5.95758 -5.5406618 -5.2544684 -4.5475421 -4.0800257 -3.7225504 -3.9923742 -4.4263048 -4.89496 -5.9767437 -6.8965979][-6.1917567 -5.7130427 -5.9277444 -6.4043407 -6.8682661 -6.1554117 -5.2947817 -4.9190083 -4.6666222 -4.1890635 -4.3930545 -4.7505846 -5.270041 -6.472806 -7.4470706][-5.2494164 -5.0227871 -4.9296393 -4.6344452 -4.4622822 -4.1605225 -4.1574965 -4.2297211 -3.5954018 -3.3235612 -3.4128652 -4.0602369 -4.7738695 -5.979238 -6.9923038][-4.40045 -4.2860947 -3.5164423 -3.1137991 -2.9920592 -2.4180007 -1.4468684 -1.6147361 -1.4279761 -2.2076054 -2.9900742 -3.8445988 -4.2717228 -5.0629821 -6.0758152][-3.2582717 -2.7635422 -2.2773266 -1.5452123 -0.67956972 -0.23889542 0.53407955 0.34092522 0.47367764 -0.56618071 -1.5002284 -2.8813324 -3.8224807 -5.0396013 -6.0197949][-1.8995895 -1.9829702 -0.818542 0.011450291 0.55586624 1.3946323 2.0398741 1.7987652 1.6390467 0.50979233 -0.80780172 -2.3360929 -3.4914947 -4.899868 -6.142807][-1.3275886 -1.1446042 -0.0048279762 0.58082581 1.2081728 1.9048395 2.6048937 2.4847727 2.3777943 1.1614361 -0.83582973 -2.6488419 -4.3003597 -5.7439194 -6.5997891][-1.4450908 -1.2153711 -0.65952015 0.62638378 1.5418329 2.1959524 2.5423803 2.6878233 2.6219082 1.4732647 -0.47035646 -2.7123961 -4.4839864 -6.0453773 -7.1212249][-2.429987 -1.9395771 -0.81517315 -0.18234825 0.35514069 1.6260061 2.1410322 2.3025045 2.2731838 1.1656361 -0.75551462 -2.5612764 -4.0864458 -5.9998083 -7.288765][-2.7869897 -1.9964967 -1.239665 -0.91012955 -0.42532682 0.24931049 0.52418613 1.1082745 1.4029226 0.541708 -0.97775126 -3.1366496 -4.7982745 -5.9633813 -7.2292848][-4.8749256 -3.8227639 -3.0074258 -2.4339967 -2.1223292 -1.9321661 -1.5598416 -1.7321882 -1.8490114 -2.3347802 -3.1262274 -4.1897793 -5.1371346 -6.0875258 -6.6760645][-5.3894711 -4.6704273 -3.9654272 -3.5207572 -3.4093847 -3.0062966 -2.8624821 -2.7243152 -2.6053739 -3.0021319 -4.0586205 -4.7300415 -5.253828 -5.4661961 -5.9963093][-6.2453136 -5.7835045 -4.9155493 -4.6715794 -4.12049 -4.1125822 -4.0805254 -4.2266264 -4.1762905 -3.7718492 -4.418766 -4.7432804 -5.1225553 -5.1847305 -5.184391][-6.2831459 -5.573205 -5.1275692 -4.8063116 -4.4040112 -4.4539709 -4.7222586 -4.923357 -5.0475893 -4.8502617 -5.3122525 -4.7695246 -4.3537745 -4.2691145 -4.4475865][-7.8927712 -7.2700653 -6.6219773 -6.666225 -6.3659143 -6.07497 -6.1499653 -6.2382345 -6.532362 -5.73832 -5.0409737 -4.5552979 -4.3591032 -4.0239239 -3.6787038]]...]
INFO - root - 2017-12-15 22:34:14.967005: step 68510, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 45h:37m:13s remains)
INFO - root - 2017-12-15 22:34:21.404459: step 68520, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 48h:29m:11s remains)
INFO - root - 2017-12-15 22:34:27.834310: step 68530, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.661 sec/batch; 48h:27m:05s remains)
INFO - root - 2017-12-15 22:34:34.213654: step 68540, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:39m:43s remains)
INFO - root - 2017-12-15 22:34:40.599691: step 68550, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 48h:05m:11s remains)
INFO - root - 2017-12-15 22:34:47.058163: step 68560, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 48h:11m:14s remains)
INFO - root - 2017-12-15 22:34:53.542566: step 68570, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 46h:48m:29s remains)
INFO - root - 2017-12-15 22:34:59.865475: step 68580, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 45h:41m:18s remains)
INFO - root - 2017-12-15 22:35:06.368139: step 68590, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 46h:56m:30s remains)
INFO - root - 2017-12-15 22:35:12.735510: step 68600, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:36m:41s remains)
2017-12-15 22:35:13.243092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2217789 -2.0776482 -2.4227657 -3.0740323 -3.874439 -4.4994507 -4.986239 -5.4592323 -5.57852 -6.9609346 -7.5756707 -7.582356 -8.76151 -9.467248 -9.8406773][-2.5449119 -2.5870419 -3.5579133 -4.7192593 -5.2457142 -5.5141492 -6.16324 -6.0793753 -5.5443649 -7.0239878 -8.0602989 -8.2567873 -9.8176222 -10.461123 -10.500496][-2.6231313 -3.1354246 -4.025641 -5.0044231 -4.8390694 -4.6464357 -4.4234414 -4.2753367 -4.19703 -5.8965735 -6.75778 -6.8105984 -8.67201 -9.8433161 -9.9049339][-2.9137383 -3.2451115 -3.9005835 -4.1228361 -3.6279292 -3.0044699 -2.5705428 -2.319994 -2.1144662 -3.6058664 -4.7736912 -5.3135204 -7.2298551 -8.4229736 -8.7398968][-4.2348957 -4.0930891 -3.8411808 -3.3582091 -2.5381088 -1.2976408 -0.2380805 -0.13914251 -0.57984257 -2.8833489 -4.4809189 -4.7593756 -6.7333336 -7.7492623 -7.59944][-5.3396091 -4.7972288 -4.1533527 -3.4078393 -1.803977 -0.55988836 0.57431412 0.68389606 0.29697418 -2.1750622 -3.7828789 -4.57018 -6.5907345 -7.4914374 -7.196301][-6.1763544 -5.1198969 -3.9996459 -2.9198809 -0.95225382 0.73638344 1.8249474 1.863883 1.4721947 -1.2561398 -3.0179949 -3.7786884 -5.8782454 -7.078721 -6.8089547][-5.861011 -5.0656548 -4.0167642 -2.5561814 -0.7555356 0.79416561 1.7435322 2.0788431 1.9987926 -0.62971592 -2.2874551 -3.4126115 -5.8508205 -7.23927 -7.309134][-6.1216259 -5.1673613 -4.1876655 -2.9836822 -1.2688699 -0.056936264 0.70716286 0.995327 1.1718102 -1.0711641 -2.8384686 -3.8591604 -6.0300708 -7.4386764 -7.7745423][-6.6545105 -5.9853549 -5.1706529 -3.8991704 -2.5100813 -1.4320564 -0.59950733 -0.030853748 0.51478481 -1.9612412 -3.4271431 -4.4489965 -6.6504154 -7.8903422 -8.1585321][-7.193841 -6.1373382 -5.6862984 -4.905045 -3.9903755 -3.0228438 -2.2468481 -2.0037851 -1.5403795 -3.4626484 -4.9421587 -5.69818 -6.7559118 -7.9862881 -8.4382706][-6.7262425 -6.1859913 -5.416297 -4.715692 -4.4075894 -3.9159226 -3.7964971 -3.4261103 -3.0393496 -4.95977 -5.5171137 -5.8847094 -7.2270675 -8.0411463 -8.0580235][-7.2836823 -6.4067707 -5.8744555 -5.3436985 -4.761342 -4.3746052 -4.2995777 -4.4074535 -4.6974635 -5.9825859 -6.5611053 -6.2687531 -6.8186717 -7.1976414 -7.3109455][-7.0313969 -6.5885477 -6.0411453 -5.3170929 -4.9899983 -4.74931 -4.7097521 -4.8696947 -5.0041375 -6.0807018 -6.3256569 -6.3128734 -6.7850943 -6.8350658 -6.35481][-7.2583609 -6.6161556 -6.1149197 -5.7380705 -5.60781 -5.6233077 -5.9376764 -6.0478315 -6.3465204 -6.4561348 -6.2255058 -6.1276627 -6.1318259 -5.7824206 -5.5514593]]...]
INFO - root - 2017-12-15 22:35:19.606297: step 68610, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 46h:38m:43s remains)
INFO - root - 2017-12-15 22:35:25.983730: step 68620, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 46h:00m:28s remains)
INFO - root - 2017-12-15 22:35:32.382065: step 68630, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 47h:03m:53s remains)
INFO - root - 2017-12-15 22:35:38.713489: step 68640, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 45h:27m:37s remains)
INFO - root - 2017-12-15 22:35:45.129461: step 68650, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 47h:45m:08s remains)
INFO - root - 2017-12-15 22:35:51.561073: step 68660, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 46h:26m:11s remains)
INFO - root - 2017-12-15 22:35:57.958119: step 68670, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 46h:28m:35s remains)
INFO - root - 2017-12-15 22:36:04.353159: step 68680, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:55m:31s remains)
INFO - root - 2017-12-15 22:36:10.843815: step 68690, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 48h:10m:12s remains)
INFO - root - 2017-12-15 22:36:17.223988: step 68700, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 47h:51m:25s remains)
2017-12-15 22:36:17.700941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9237185 -6.7245669 -6.6360579 -6.8279138 -6.5216532 -6.0465832 -5.0432177 -4.2931623 -3.1374002 -3.5999331 -3.7988026 -5.2273736 -5.8946548 -6.4862452 -6.4457717][-8.0527182 -8.0929623 -8.358552 -8.396925 -8.8780661 -8.66608 -7.8306832 -6.6194654 -4.9165592 -5.0214028 -4.8579893 -5.8232012 -6.5767221 -7.6246109 -7.7955728][-6.5065174 -7.0274177 -7.5642686 -7.8378119 -7.7060623 -7.4012518 -7.0616016 -6.5787592 -5.2020769 -5.3290725 -4.9652047 -5.8232675 -6.3410211 -6.9557505 -7.0757122][-6.071177 -5.900475 -5.8770413 -5.7470622 -5.3463936 -4.6302795 -3.9343941 -3.6356525 -3.0933104 -4.4034262 -4.7277188 -5.6994762 -6.3297434 -6.8187065 -6.7803645][-7.0391769 -6.5819049 -6.2470145 -5.6389494 -4.4484711 -3.1574216 -1.9479804 -1.5056295 -1.3163395 -3.0278873 -4.1801577 -6.0786133 -7.0630279 -7.8212919 -7.3921175][-8.1557407 -7.32992 -6.5303254 -5.4329357 -4.0575457 -2.4993377 -0.99894047 -0.23832321 0.15138769 -1.8256111 -3.1406884 -5.2559471 -6.6087418 -7.5273886 -7.2257833][-6.900732 -6.3412814 -5.4223566 -3.7110896 -1.8884444 0.063049793 1.3231726 1.6676636 1.6753483 -0.78187656 -2.531322 -4.6681337 -6.2210541 -7.3334737 -6.8902388][-6.0357833 -4.8254952 -3.7201734 -1.9308395 -0.15670204 1.3481607 2.1535263 2.9244204 2.7462769 0.14979792 -1.7305498 -4.1158361 -5.4292407 -6.3213067 -6.2684751][-6.6704478 -5.5097265 -3.9875114 -2.0164433 -0.37226677 0.73267078 1.2024956 1.1973495 0.77596951 -0.90969849 -2.0548921 -4.09624 -5.299305 -6.3121634 -5.90392][-5.8744411 -5.5573864 -5.0175238 -3.1930642 -1.7443209 -0.58352995 0.17585421 -0.12034845 -0.87750912 -2.6956267 -3.4939504 -5.3258619 -6.0769334 -6.9270639 -6.6358328][-5.6508775 -5.6174974 -5.0842781 -3.9000151 -2.8274865 -1.7369404 -1.1003489 -1.2741036 -1.5476098 -2.9913349 -4.1594114 -5.47818 -6.349431 -6.6490555 -5.8631458][-6.8192344 -6.3495111 -5.3093524 -4.5980392 -3.8215594 -2.8286743 -2.628109 -2.8114429 -2.9722042 -3.9891472 -4.1093354 -4.7689247 -5.1413727 -5.7206149 -5.5010519][-8.1795387 -7.9326935 -7.5307794 -6.2654686 -5.3705225 -4.7558756 -4.4315491 -4.5841427 -4.9079738 -5.7020817 -5.6605072 -6.0162234 -5.8034935 -5.9543676 -5.2957058][-7.4400721 -7.2969551 -7.1002269 -6.6433921 -6.3663807 -5.8185678 -5.6719403 -5.4868197 -5.3217278 -5.9189043 -5.95551 -6.0936637 -6.2578888 -6.2648163 -6.2179708][-7.7658243 -7.5025048 -6.9887443 -6.5225606 -6.4299722 -6.18971 -6.4191127 -6.3986044 -6.2054887 -5.6912851 -5.3707476 -5.3229647 -5.3946047 -5.7127671 -5.9177237]]...]
INFO - root - 2017-12-15 22:36:24.135717: step 68710, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 45h:49m:06s remains)
INFO - root - 2017-12-15 22:36:30.505601: step 68720, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 46h:44m:19s remains)
INFO - root - 2017-12-15 22:36:36.936163: step 68730, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 46h:29m:26s remains)
INFO - root - 2017-12-15 22:36:43.345305: step 68740, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 47h:10m:06s remains)
INFO - root - 2017-12-15 22:36:49.688836: step 68750, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 45h:49m:05s remains)
INFO - root - 2017-12-15 22:36:56.015892: step 68760, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 46h:06m:47s remains)
INFO - root - 2017-12-15 22:37:02.353941: step 68770, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 46h:35m:59s remains)
INFO - root - 2017-12-15 22:37:08.814355: step 68780, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 48h:15m:50s remains)
INFO - root - 2017-12-15 22:37:15.166253: step 68790, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.652 sec/batch; 47h:46m:56s remains)
INFO - root - 2017-12-15 22:37:21.520422: step 68800, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 46h:06m:34s remains)
2017-12-15 22:37:22.037218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7726545 -4.2761879 -4.7283521 -5.2613268 -5.6672115 -5.6001043 -5.7689457 -5.7676597 -6.0665565 -6.5724354 -7.5516949 -7.5227923 -8.5547848 -8.7848 -8.91576][-4.2186594 -4.6151628 -4.5760374 -4.7479649 -4.6176124 -4.1826396 -4.4749322 -4.7057734 -4.7281761 -5.2016358 -6.6717567 -7.3528728 -8.4282894 -8.6165466 -8.60424][-4.0526853 -3.7537813 -3.9288032 -4.1889124 -4.222991 -3.7138665 -3.4839725 -3.0276432 -2.7706318 -3.7395847 -4.907536 -5.4308681 -7.0284724 -7.723196 -8.317543][-5.0094066 -4.676136 -4.1800342 -3.3277478 -3.0211983 -2.7371588 -2.1954207 -1.7787228 -1.5986676 -2.5731502 -3.7252836 -3.8335912 -5.5820045 -6.1219463 -6.2752452][-5.295496 -4.1995664 -3.1599422 -2.7680254 -2.3811445 -1.5721779 -0.65112495 -0.41194725 -0.10452604 -1.0317087 -2.0369987 -2.1950135 -3.8622499 -5.0946517 -5.4163284][-4.6433783 -3.7640982 -2.4251814 -1.6600819 -0.7500391 0.3231163 0.92259026 1.0909491 1.6528215 0.756485 -0.54892826 -0.75717783 -2.4924111 -3.8385332 -4.5985775][-3.690954 -2.8992271 -1.7178922 -0.39293098 0.60997677 1.5296049 2.1298704 2.3923626 2.7266607 1.6861362 0.50977707 -0.49154568 -2.5186172 -3.7705498 -4.5266867][-2.9817858 -2.3500733 -1.8045754 -0.50939846 1.2431927 2.1345949 2.7154188 2.9187536 3.0306034 1.8433285 0.36057663 -0.85541344 -2.6071639 -3.7786114 -4.4523163][-3.4030881 -2.422296 -1.4631901 -0.51780891 1.0531969 1.702589 1.8185463 2.1785536 2.3817787 0.97001839 -0.8928442 -2.0613461 -3.6649241 -4.6415796 -4.803864][-3.8588495 -2.8906064 -2.334764 -2.1888132 -1.3052683 -0.71165991 -0.11665916 0.20283604 0.25299454 -1.1023593 -3.0628819 -3.7347875 -4.939085 -5.5192137 -5.7425823][-5.9615145 -4.9671726 -4.25689 -4.4799109 -4.1447372 -3.7423236 -3.3994403 -3.6020789 -3.6184683 -4.3510504 -5.7051468 -5.8984036 -6.2320876 -7.3231568 -7.4240556][-6.6199517 -6.0346775 -5.7573638 -5.8310361 -5.9361982 -5.838191 -5.7652988 -5.8250403 -5.6570845 -6.3883662 -7.4353938 -7.0964293 -7.1117697 -7.7283192 -7.474555][-7.3528361 -6.9794278 -6.8525758 -6.71481 -6.5447264 -6.6169419 -6.9136314 -6.9604535 -6.9109726 -7.0848746 -7.6382394 -7.1032944 -6.8345852 -7.1492724 -7.11488][-6.6877985 -6.6498661 -6.525928 -6.3139524 -6.1722927 -6.13557 -6.4372005 -6.4775596 -6.5120177 -6.5378242 -6.6301327 -6.2778268 -6.1582584 -6.4289203 -6.0358267][-7.5885463 -7.5665722 -7.1652064 -7.126184 -6.775918 -6.5970435 -6.3521171 -6.3341923 -6.2345605 -6.2108951 -6.230835 -6.1136236 -6.0875335 -6.0462456 -5.8792157]]...]
INFO - root - 2017-12-15 22:37:28.468594: step 68810, loss = 0.35, batch loss = 0.24 (12.1 examples/sec; 0.661 sec/batch; 48h:24m:06s remains)
INFO - root - 2017-12-15 22:37:34.786056: step 68820, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.613 sec/batch; 44h:54m:26s remains)
INFO - root - 2017-12-15 22:37:41.200762: step 68830, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:36m:47s remains)
INFO - root - 2017-12-15 22:37:47.506003: step 68840, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 47h:07m:30s remains)
INFO - root - 2017-12-15 22:37:53.828018: step 68850, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 46h:34m:09s remains)
INFO - root - 2017-12-15 22:38:00.141550: step 68860, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 46h:34m:34s remains)
INFO - root - 2017-12-15 22:38:06.408268: step 68870, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 45h:34m:37s remains)
INFO - root - 2017-12-15 22:38:12.821612: step 68880, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:34m:23s remains)
INFO - root - 2017-12-15 22:38:19.296891: step 68890, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 47h:32m:56s remains)
INFO - root - 2017-12-15 22:38:25.705996: step 68900, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 46h:45m:38s remains)
2017-12-15 22:38:26.294404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2149744 -7.5171247 -7.5331454 -8.3061619 -9.4385586 -8.5661564 -7.2128558 -5.63516 -5.7489977 -5.1070361 -5.5711288 -6.8012795 -5.6818328 -6.685977 -6.9356861][-6.072978 -5.846107 -6.2169781 -8.1286983 -8.806263 -8.3226156 -8.51236 -6.7563906 -6.7438135 -6.2443705 -6.25287 -6.6260214 -6.5000134 -7.121891 -6.3452797][-5.7293954 -5.3067174 -5.9778519 -6.4340534 -6.9153514 -6.7170062 -5.9736991 -5.2747564 -6.2043366 -5.5756297 -6.0271344 -7.591526 -6.2254691 -6.11019 -7.0807114][-3.9435008 -4.8088322 -5.4503965 -6.167572 -5.5218868 -5.2770958 -4.6392117 -3.6141472 -3.7206655 -4.4178052 -4.6496515 -5.397768 -5.6608963 -6.0441332 -5.31544][-2.1118126 -2.4055986 -3.2925305 -4.2337031 -4.6011715 -3.1616349 -2.5989861 -2.9228377 -2.2329082 -2.9841075 -3.4939113 -4.669445 -4.6125469 -4.8864903 -5.1014566][-2.3466158 -1.9978018 -2.6497293 -2.9008131 -2.1551137 -1.3739595 -0.55515814 0.025771618 -0.22387075 -0.91398668 -0.861042 -3.7595139 -3.4216561 -3.8511822 -4.39661][-3.4192529 -3.5515428 -3.0171194 -2.6517134 -2.3651628 -1.2400651 -0.078402996 0.57109451 1.1659641 -0.54829359 -1.0133429 -2.439086 -3.2325268 -5.1131525 -4.2262211][-2.9644327 -3.6196342 -3.9855387 -3.4570045 -2.8665667 -1.0326581 -0.89321566 -0.56903315 0.12115622 -0.72324848 -1.0526471 -3.3288779 -2.8606339 -4.9576244 -5.5973358][-3.7417574 -3.6520171 -3.8645248 -4.0762434 -3.1666803 -1.994246 -0.81495905 -0.11022043 0.20177221 -0.84133625 -1.3258076 -3.4382558 -3.9465878 -5.5384436 -5.5594664][-4.2933974 -4.9549208 -3.2419109 -3.7813549 -3.5919862 -3.0636148 -2.240819 -1.5447822 -1.2687402 -1.8159294 -2.9142127 -5.4599333 -5.161541 -5.912447 -6.765583][-6.6924219 -5.5128312 -4.517951 -4.8928747 -4.2294979 -3.6085839 -2.5886316 -1.8906875 -2.8752561 -3.7438364 -3.5421033 -5.7078114 -6.7634993 -7.1114559 -6.86343][-6.7154336 -6.65368 -6.9639392 -6.1228967 -5.4485493 -4.9121847 -3.8726122 -3.9301023 -3.1421084 -4.1286831 -4.6090155 -5.0069895 -5.9849477 -6.9491391 -6.8393312][-5.8466454 -6.1093173 -6.462431 -6.4790907 -6.6160665 -5.9608011 -5.5779324 -5.9932694 -6.1240339 -6.1620216 -5.4163704 -5.4743433 -5.942657 -6.1259804 -6.86145][-5.0704155 -4.9612474 -5.3900003 -5.4343386 -5.3048887 -5.4225359 -5.6282887 -5.7971187 -5.5490913 -5.9429526 -5.3069153 -5.7762547 -5.8040161 -6.64199 -5.9445038][-7.1712232 -6.492075 -6.3772964 -6.6759419 -6.4510622 -6.1501727 -6.3337493 -6.5482054 -6.9735479 -6.8228836 -6.0994825 -6.3961411 -6.5846028 -6.108098 -6.0170813]]...]
INFO - root - 2017-12-15 22:38:32.664202: step 68910, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 45h:55m:27s remains)
INFO - root - 2017-12-15 22:38:39.051169: step 68920, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 46h:24m:23s remains)
INFO - root - 2017-12-15 22:38:45.498423: step 68930, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 48h:53m:56s remains)
INFO - root - 2017-12-15 22:38:51.943677: step 68940, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.678 sec/batch; 49h:37m:51s remains)
INFO - root - 2017-12-15 22:38:58.340953: step 68950, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 46h:26m:45s remains)
INFO - root - 2017-12-15 22:39:04.675043: step 68960, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 45h:59m:12s remains)
INFO - root - 2017-12-15 22:39:11.078652: step 68970, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.644 sec/batch; 47h:07m:25s remains)
INFO - root - 2017-12-15 22:39:17.570812: step 68980, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 46h:06m:04s remains)
INFO - root - 2017-12-15 22:39:23.982745: step 68990, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 47h:14m:44s remains)
INFO - root - 2017-12-15 22:39:30.322004: step 69000, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 45h:42m:56s remains)
2017-12-15 22:39:30.878190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7967963 -7.6499853 -8.3129911 -8.4195881 -7.3337584 -6.167367 -5.3901682 -4.5393219 -3.8737152 -4.03557 -4.9023032 -6.3732605 -7.2184458 -6.8660016 -7.61607][-6.3108764 -7.1200361 -8.0144768 -8.5049667 -8.0682526 -6.4050326 -5.6753507 -4.873558 -4.5847082 -5.4270344 -6.0846853 -7.5704408 -8.3484335 -7.9224248 -8.4927826][-5.0451841 -5.9243541 -6.1881776 -6.3738346 -6.3117347 -5.85025 -5.2842445 -4.6856904 -4.4064808 -5.46521 -6.62439 -8.0087385 -8.471282 -8.1507378 -9.1619053][-4.207489 -5.1018476 -5.334846 -5.2760897 -4.8157406 -3.7509258 -3.4184284 -3.1895742 -2.8597822 -3.7565634 -5.2953997 -7.5826769 -8.707427 -8.5790339 -9.042551][-4.5374231 -5.4949551 -5.4951658 -4.6002393 -3.0902948 -0.86156225 0.073204041 -0.2029891 -0.65151644 -2.63159 -4.5648632 -6.969593 -8.7376833 -8.682312 -9.4818516][-5.55556 -5.7326326 -5.2206135 -3.2382774 -1.3088217 0.93912888 2.5341272 2.8138027 1.9913511 -0.89776945 -3.7191818 -6.710465 -8.3266239 -8.6653328 -9.6915054][-6.0004668 -5.903059 -4.9094644 -2.4331884 -0.04183197 2.6990566 4.7659588 5.3723164 4.746316 1.6443663 -1.757669 -5.21424 -7.4653997 -8.0539351 -9.0967216][-5.9528255 -5.855401 -5.0512991 -2.9100447 -0.013334751 2.7680283 5.2741833 5.8679075 5.6444483 2.9935408 0.022379875 -3.7945166 -6.3657703 -7.1785908 -8.3820934][-6.8787351 -6.4735384 -5.8938408 -3.9537342 -1.3479638 1.1802073 3.5453529 3.651022 3.9141684 1.899703 -1.0989294 -3.9609668 -6.20308 -6.897646 -8.4469376][-7.4464068 -6.8855381 -6.2490792 -4.9383888 -3.1627374 -1.0924253 0.46453381 1.0706301 1.0606689 -1.0637283 -2.7855802 -5.4796462 -6.954958 -7.0360136 -8.5450706][-8.62327 -8.2442608 -7.6446881 -6.5283785 -5.5440474 -4.2378416 -2.9308252 -2.1983142 -1.9351988 -3.9590845 -5.8468609 -7.9220843 -8.8190918 -8.36472 -8.4805851][-8.9322777 -8.519805 -7.9314594 -6.8419585 -6.2999725 -5.8090105 -5.2891369 -5.3451385 -5.2033057 -5.8862319 -6.3163428 -7.8994408 -8.91044 -8.4312477 -8.9025][-8.4738884 -8.3653059 -7.8909416 -7.1150455 -6.4399052 -6.5894189 -6.4784594 -6.3823671 -6.495101 -7.1185746 -7.6870255 -8.1693783 -8.9137554 -8.4681807 -9.0675344][-7.61469 -7.7498932 -7.5833745 -6.9621925 -6.3881845 -6.4901705 -6.5426059 -6.9399028 -7.2718563 -7.8908625 -7.7210331 -7.667213 -7.9938107 -7.6441555 -7.7679873][-7.486834 -7.64405 -8.1455345 -7.6786652 -7.2792459 -7.0775166 -6.8833961 -6.9985356 -7.5433111 -8.1475382 -8.5174179 -8.4743013 -8.05461 -7.3806772 -6.9824796]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 22:39:37.292151: step 69010, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 45h:56m:24s remains)
INFO - root - 2017-12-15 22:39:43.700226: step 69020, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 46h:41m:31s remains)
INFO - root - 2017-12-15 22:39:50.129707: step 69030, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 47h:22m:49s remains)
INFO - root - 2017-12-15 22:39:56.465620: step 69040, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 47h:35m:53s remains)
INFO - root - 2017-12-15 22:40:02.828150: step 69050, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 45h:32m:39s remains)
INFO - root - 2017-12-15 22:40:09.263416: step 69060, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 47h:07m:29s remains)
INFO - root - 2017-12-15 22:40:15.688437: step 69070, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 46h:49m:01s remains)
INFO - root - 2017-12-15 22:40:22.009598: step 69080, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 45h:09m:56s remains)
INFO - root - 2017-12-15 22:40:28.439338: step 69090, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 47h:09m:55s remains)
INFO - root - 2017-12-15 22:40:34.794475: step 69100, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 46h:01m:36s remains)
2017-12-15 22:40:35.388609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5117593 -3.719348 -4.1514349 -4.33193 -4.1326513 -3.7795844 -3.229249 -2.9123077 -2.5071611 -3.20617 -3.9013598 -4.652566 -5.6212378 -5.8813391 -6.8559942][-3.844341 -4.020853 -4.4676352 -4.6258121 -4.6561069 -4.6105804 -4.0798855 -3.4547782 -2.78092 -3.5934105 -4.186841 -5.0355968 -6.1699 -6.4689808 -7.5627222][-4.3623753 -4.1874685 -4.1927977 -4.2108784 -4.0929432 -4.2130995 -4.0122814 -3.8189845 -3.3834782 -4.3185949 -5.1565428 -5.9492035 -6.8616533 -7.1603308 -8.07419][-4.6201868 -3.9003327 -3.588223 -3.0873709 -2.7194209 -2.8273845 -2.4721122 -2.4256635 -2.2499394 -3.4056311 -4.4248219 -5.5562592 -6.6798162 -7.0351872 -8.065][-5.0806026 -4.1663132 -3.2695613 -2.5643373 -1.945065 -1.3306327 -0.58547878 -0.65548515 -0.692575 -2.3793778 -4.0364046 -5.5421925 -7.0391974 -7.2603569 -8.160923][-6.2375927 -5.191597 -4.4207134 -3.0230851 -1.5970039 -0.70554304 0.55903816 0.52815533 0.45436382 -1.1844158 -3.0248528 -4.9499846 -6.8818636 -7.4687304 -8.6944647][-6.4396296 -5.61452 -4.3305216 -2.3423014 -0.44003677 0.99288464 2.6324854 2.7868032 3.0060177 0.99274921 -1.1085806 -3.0649672 -5.3993716 -6.3475971 -7.6465392][-6.0154824 -5.1997385 -4.0519767 -1.96383 0.2428031 2.2143784 4.0210657 4.4657907 4.490201 2.1763268 0.347682 -1.7505779 -3.8585644 -5.0658154 -6.8243594][-6.127501 -5.4533176 -4.5108309 -3.0979505 -1.4381366 0.34077263 2.2486706 2.8315182 3.6349735 1.6262941 -0.48992682 -2.5083175 -4.5312357 -5.42917 -6.98159][-7.2257438 -6.5037622 -5.6968508 -4.1793385 -2.6740942 -1.4541788 -0.086310387 0.50170326 1.1818409 -0.6902585 -2.4833965 -4.4212761 -6.0026054 -6.3908634 -7.3395863][-8.4284515 -8.0950546 -7.5963869 -6.6880522 -5.355875 -3.8788755 -2.5077958 -2.2249718 -2.03188 -3.4175291 -4.5740662 -5.8872747 -6.9895077 -7.4754634 -7.9791594][-8.4254522 -7.9483132 -7.7296944 -7.2675843 -6.4804149 -5.6521988 -4.8724556 -4.5083895 -4.1550493 -4.9170532 -5.5206294 -6.566452 -7.3982739 -7.8985224 -8.2463951][-8.8609238 -8.4848881 -8.2186031 -7.6584277 -7.1202879 -6.630065 -5.8885217 -5.8167148 -5.8223066 -6.2053008 -6.5331964 -7.0466437 -7.3221865 -7.5096774 -7.8297319][-8.2272415 -8.4118462 -8.123312 -7.4252162 -6.9449334 -6.5112553 -5.8839512 -5.856637 -5.9141617 -6.2826576 -6.3825436 -6.7217259 -7.1356668 -7.2224445 -7.31884][-8.2608871 -8.292655 -8.5296593 -8.4767323 -7.9013114 -7.4107141 -6.9411163 -7.1163206 -7.3279538 -7.1627941 -7.0685573 -7.1995511 -7.02341 -6.88614 -6.6729097]]...]
INFO - root - 2017-12-15 22:40:41.746179: step 69110, loss = 0.24, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 47h:38m:42s remains)
INFO - root - 2017-12-15 22:40:48.089076: step 69120, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:28m:04s remains)
INFO - root - 2017-12-15 22:40:54.479525: step 69130, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 46h:47m:20s remains)
INFO - root - 2017-12-15 22:41:00.829164: step 69140, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 46h:08m:45s remains)
2017-12-15 22:41:06.584451: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 32083117 get requests, put_count=32083124 evicted_count=29000 eviction_rate=0.000903902 and unsatisfied allocation rate=0.000904027
INFO - root - 2017-12-15 22:41:07.248437: step 69150, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 48h:23m:55s remains)
INFO - root - 2017-12-15 22:41:13.660902: step 69160, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 48h:50m:02s remains)
INFO - root - 2017-12-15 22:41:20.158644: step 69170, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 46h:19m:59s remains)
INFO - root - 2017-12-15 22:41:26.502669: step 69180, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 46h:31m:36s remains)
INFO - root - 2017-12-15 22:41:32.914822: step 69190, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 46h:51m:04s remains)
INFO - root - 2017-12-15 22:41:39.286496: step 69200, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 46h:03m:38s remains)
2017-12-15 22:41:39.828596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.758666 -3.6349244 -3.965498 -3.8047466 -3.8571959 -3.5833979 -3.157074 -3.0303049 -2.3850417 -4.1374321 -4.3480997 -5.7295866 -6.2792749 -7.0442123 -7.8790159][-3.6272779 -3.7473471 -4.3637657 -4.9111776 -5.3354788 -4.8948278 -4.4874063 -4.2346573 -3.5360236 -4.9561939 -4.8366423 -6.0825915 -6.494647 -7.2563157 -8.1180363][-3.6451197 -3.4301696 -3.7838159 -3.9025435 -4.1515551 -3.5884285 -3.5952225 -3.6767144 -3.3511648 -4.9308825 -5.0201635 -6.1627617 -6.554183 -7.6260324 -8.3552341][-3.378119 -2.9557528 -3.1297827 -3.2556062 -3.574204 -2.9916639 -2.5094571 -2.6024275 -2.6828742 -4.4277372 -4.8231349 -5.9717593 -6.4257612 -7.5514178 -8.4098473][-4.1917162 -3.2712522 -2.8691449 -2.6285529 -2.4950089 -1.8376603 -1.2850704 -1.4530087 -1.7709923 -3.6228638 -4.2963448 -5.5671296 -6.1511803 -7.1166062 -8.01799][-5.1225681 -4.0967369 -3.1728086 -2.3728428 -1.6358485 -0.65043926 -0.054994106 -0.40680933 -0.6719656 -2.6465607 -3.3427939 -4.4999886 -5.3043895 -6.460722 -7.4253073][-4.4738512 -3.844116 -3.1741915 -1.7708921 -0.70676041 0.44118023 1.2022495 1.2367334 1.0162573 -1.2511239 -2.1871986 -3.2967358 -3.9509437 -5.3329854 -6.4727368][-4.0366788 -3.2009044 -2.3830829 -0.98506451 0.049066544 1.2036009 1.7779322 2.0158119 2.1538515 0.14791536 -0.87438869 -2.3024063 -3.0376453 -4.2676883 -5.4112749][-3.7711456 -3.0865269 -2.2374892 -1.1306758 -0.26085997 0.5257864 0.84908485 1.3430738 1.7182007 -0.12749147 -1.009706 -2.2984886 -3.2424359 -4.4592628 -5.3881359][-2.9746094 -2.7514482 -1.9692798 -1.0986013 -0.24190283 0.18317652 0.26886845 0.68419838 0.88049221 -0.90943384 -1.6506972 -2.8572321 -3.9511662 -4.9645529 -5.9140968][-4.205379 -3.4580183 -2.5051332 -1.4394226 -0.50681925 -0.51150942 -0.88586187 -0.86258173 -0.64919329 -2.3581061 -3.1926742 -4.0768967 -4.9223928 -5.9537573 -6.6808777][-4.316144 -3.7137134 -2.8288875 -2.1747217 -1.4944916 -1.126389 -1.3223348 -1.704257 -1.6312261 -3.0186725 -3.6290498 -4.7004161 -5.5734949 -6.3812995 -7.1212378][-5.1790867 -4.5201988 -3.7538748 -3.0257578 -2.2387853 -1.9095664 -1.8775873 -1.96737 -2.011466 -3.3181071 -4.1161537 -5.2147713 -5.5518274 -6.0509505 -6.5754647][-5.8951149 -4.9485707 -4.1867733 -3.2248573 -2.2981753 -1.8827567 -2.1834278 -2.4162121 -2.4245167 -3.0268621 -3.4107752 -3.9958591 -4.4147291 -5.0253253 -5.6405215][-6.4146218 -5.5787115 -4.893755 -4.4472675 -3.9323549 -3.53824 -3.6859469 -3.8108397 -3.898443 -3.6101379 -3.5647683 -3.9491158 -4.3309073 -4.767633 -5.1438046]]...]
INFO - root - 2017-12-15 22:41:46.212118: step 69210, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 47h:24m:37s remains)
INFO - root - 2017-12-15 22:41:52.622079: step 69220, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 45h:39m:25s remains)
INFO - root - 2017-12-15 22:41:59.017016: step 69230, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 48h:16m:55s remains)
INFO - root - 2017-12-15 22:42:05.378597: step 69240, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.623 sec/batch; 45h:31m:30s remains)
INFO - root - 2017-12-15 22:42:11.785724: step 69250, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.637 sec/batch; 46h:36m:30s remains)
INFO - root - 2017-12-15 22:42:18.223476: step 69260, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 47h:59m:51s remains)
INFO - root - 2017-12-15 22:42:24.652982: step 69270, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 47h:25m:12s remains)
INFO - root - 2017-12-15 22:42:30.978644: step 69280, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 46h:38m:00s remains)
INFO - root - 2017-12-15 22:42:37.287204: step 69290, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 46h:08m:17s remains)
INFO - root - 2017-12-15 22:42:43.648124: step 69300, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 45h:52m:08s remains)
2017-12-15 22:42:44.132522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4206629 -4.9696722 -4.0950713 -3.4748545 -2.5000458 -2.2061348 -1.8755975 -1.9796896 -2.6423693 -3.7722261 -5.3389239 -4.7152214 -6.3795424 -6.0182667 -5.4546032][-5.3345461 -4.5546031 -3.1684589 -2.8783174 -2.3013186 -2.2680869 -2.2428923 -2.4129481 -2.4068809 -3.3763962 -4.9409118 -4.5776463 -5.880475 -5.8325925 -6.2618852][-5.8419962 -5.5245581 -4.1407528 -3.0455451 -1.9671578 -1.6956997 -1.8464513 -2.0451775 -2.2703352 -3.4193106 -5.0589862 -4.5635161 -6.4345083 -6.1217065 -6.4777126][-3.6162548 -3.6662531 -3.2072558 -2.7953634 -1.9689198 -0.93384075 -0.80328894 -0.58587265 -0.51376057 -1.3592324 -3.5417857 -3.5829015 -5.3363495 -5.0151596 -5.7669072][-3.8458169 -1.7803025 -1.313086 -0.66821289 0.011156559 0.17679787 0.051909447 0.015946865 0.029653549 -0.098352432 -1.24582 -1.4128952 -4.0265884 -4.7382126 -5.6403103][-0.66937065 -0.57613182 -0.21770668 0.56523418 1.0706587 1.3035126 0.95454407 0.48296833 0.62605572 0.063686371 -1.7808113 -1.7187676 -3.5239105 -4.1316586 -5.1892538][0.43360806 1.0303059 1.8155041 2.3527937 3.3420048 3.3508835 2.6539755 2.1135111 1.1561031 -0.20746565 -2.2390852 -2.11445 -4.4537458 -4.2849951 -4.5219531][2.1724854 2.7238283 2.9008608 2.8453083 3.0584326 3.6369982 4.1672411 3.7012005 3.0487509 1.6700401 -1.0417314 -1.694634 -4.3654785 -4.7956333 -5.2106371][-0.46431494 1.3228321 2.8769712 3.0653486 3.0571156 2.9082174 2.9431849 3.3438025 3.2847471 1.8544588 -0.11774969 -0.58724403 -3.4605761 -4.5184231 -5.5404091][-1.1126189 -0.4252286 -0.069591522 0.75197029 1.2235146 2.0112066 2.4308958 1.980484 1.8181829 0.7908802 -0.85556412 -1.5163603 -3.0270615 -3.7529678 -4.5044661][-2.2467723 -1.6936264 -0.86092377 -0.38875103 -0.18747139 0.083546638 0.16241121 0.76085377 0.75603867 -0.5408082 -2.1790233 -2.1360388 -3.6544952 -4.2120857 -4.7904043][-3.4088244 -2.6919718 -2.2283397 -2.3089824 -1.6465354 -0.69516182 -0.35333347 -0.84311867 -0.93518734 -0.74863243 -2.7192039 -3.3990636 -5.2675619 -5.6637378 -6.6772957][-4.5128789 -4.8252368 -4.391408 -3.6108804 -2.975533 -2.5487947 -1.884366 -1.5607204 -1.6361752 -2.7173882 -4.981144 -4.6208735 -5.7451234 -6.0432429 -7.4230223][-4.5178857 -4.7675624 -4.8468843 -5.1848316 -4.7239509 -3.7821519 -2.9667082 -3.494916 -3.6926587 -3.7191217 -4.9001675 -5.8373346 -6.587184 -6.0181885 -6.66965][-5.3275003 -4.9038782 -4.9303684 -5.6502151 -5.7962518 -5.6763325 -5.4235845 -5.1605673 -4.5733862 -4.8363194 -5.9078155 -6.3685994 -6.7822785 -6.990396 -7.6943183]]...]
INFO - root - 2017-12-15 22:42:50.531919: step 69310, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 45h:31m:15s remains)
INFO - root - 2017-12-15 22:42:56.851609: step 69320, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:21m:07s remains)
INFO - root - 2017-12-15 22:43:03.198649: step 69330, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.615 sec/batch; 44h:57m:24s remains)
INFO - root - 2017-12-15 22:43:09.515306: step 69340, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:25m:07s remains)
INFO - root - 2017-12-15 22:43:15.846818: step 69350, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 46h:58m:47s remains)
INFO - root - 2017-12-15 22:43:22.352291: step 69360, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 46h:30m:17s remains)
INFO - root - 2017-12-15 22:43:28.716127: step 69370, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 45h:39m:03s remains)
INFO - root - 2017-12-15 22:43:35.076071: step 69380, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:18m:59s remains)
INFO - root - 2017-12-15 22:43:41.476552: step 69390, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 45h:09m:39s remains)
INFO - root - 2017-12-15 22:43:47.874988: step 69400, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:46m:37s remains)
2017-12-15 22:43:48.358747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6027365 -5.0012207 -4.8474045 -4.5185289 -4.7895966 -4.868423 -4.8537731 -5.019105 -4.6824837 -6.3403707 -6.401792 -6.2209196 -7.0139885 -7.5689707 -7.8545208][-4.9745569 -4.7809439 -5.2871237 -5.0272446 -4.7553525 -4.8550119 -5.127038 -5.1165977 -4.9464035 -6.7881017 -6.9263177 -7.561388 -7.9210334 -7.307 -7.8523641][-4.2971888 -3.4174614 -3.7658203 -3.6897445 -3.2712669 -3.159606 -3.1934805 -3.7544487 -3.6129642 -5.0141973 -5.1233349 -5.8067203 -6.76554 -6.5059805 -7.0856056][-4.605763 -3.4650822 -3.4344697 -2.5220447 -1.3228974 -1.0581436 -1.3695192 -1.9444327 -1.9879718 -4.00555 -4.3619485 -4.9352093 -5.9345007 -5.6802692 -5.7988415][-5.156817 -2.8595734 -1.5286717 -1.277854 -0.47487593 0.1135087 0.36335659 0.0038552284 -0.12699795 -1.7548985 -2.4131398 -3.16958 -4.4518766 -4.1162934 -4.6686187][-2.8344359 -1.4285684 -1.0723777 0.28670502 1.4167957 1.8725128 2.4814425 2.37813 2.4344044 0.60143089 0.025680542 -0.62065506 -2.145031 -2.3724504 -3.1458292][-4.2809715 -2.3749766 -0.62771463 -0.10046101 0.65319633 2.0978746 3.1671829 2.9191341 3.0399351 1.7088709 1.3227825 0.39027405 -1.5016503 -1.9578438 -3.5464935][-4.4745383 -2.7848425 -2.1356235 -0.65759277 1.33745 2.4366236 3.2831631 3.5863314 3.7749634 2.3769989 2.023509 1.6455126 -0.090100765 -1.2005696 -3.0638466][-4.4139633 -3.76616 -2.6237197 -1.1608706 -0.084266186 1.2707491 2.262558 2.7075167 3.223361 2.4840603 2.1318398 1.0392542 -1.3122635 -2.2766008 -3.7429233][-5.6358843 -4.5500917 -4.2197218 -3.3537655 -2.340704 -1.0272894 0.065262794 0.81383896 1.8607731 0.879838 0.2976222 -0.52010441 -2.1083369 -2.8772917 -4.0698366][-6.9554257 -6.6930876 -6.3374825 -5.5227757 -4.7438822 -3.5980048 -2.5851727 -1.7147322 -0.73139906 -1.6919951 -2.2111945 -2.9367013 -4.2764153 -4.9378009 -5.5385656][-8.5362854 -7.804965 -7.7682643 -7.5128322 -6.7229371 -5.6212807 -4.7663383 -4.1531339 -3.7265239 -4.6362715 -4.9628034 -5.0578136 -5.6503305 -5.8615055 -5.8646483][-8.7234163 -8.57037 -8.5679274 -8.1123142 -7.64313 -6.8926659 -6.9025011 -6.5965748 -5.9014525 -6.4757848 -7.2215362 -7.0452933 -6.9263229 -6.7512712 -6.8758812][-9.4653826 -9.2431087 -8.6983528 -8.4436836 -7.87207 -7.3868842 -7.3290439 -7.4406376 -7.5920258 -8.3340006 -8.3480635 -7.5447536 -7.724824 -8.0924711 -7.9138212][-9.6303749 -9.3392963 -9.3214025 -9.0885849 -8.2091246 -7.6156158 -7.3434639 -7.7604113 -8.2369471 -8.415554 -9.0644016 -9.5322828 -9.2028828 -8.5215788 -8.0283442]]...]
INFO - root - 2017-12-15 22:43:54.694563: step 69410, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 46h:47m:03s remains)
INFO - root - 2017-12-15 22:44:01.020794: step 69420, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 45h:49m:25s remains)
INFO - root - 2017-12-15 22:44:07.420776: step 69430, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 45h:53m:40s remains)
INFO - root - 2017-12-15 22:44:13.792987: step 69440, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.623 sec/batch; 45h:29m:16s remains)
INFO - root - 2017-12-15 22:44:20.120227: step 69450, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 45h:14m:20s remains)
INFO - root - 2017-12-15 22:44:26.483651: step 69460, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 46h:09m:42s remains)
INFO - root - 2017-12-15 22:44:32.881078: step 69470, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 46h:37m:23s remains)
INFO - root - 2017-12-15 22:44:39.400705: step 69480, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 46h:10m:57s remains)
INFO - root - 2017-12-15 22:44:45.762963: step 69490, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 47h:07m:01s remains)
INFO - root - 2017-12-15 22:44:52.171208: step 69500, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.658 sec/batch; 48h:06m:19s remains)
2017-12-15 22:44:52.715445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5997896 -3.119 -1.887558 -1.2394462 -0.33741236 -0.094352245 -0.093206406 -0.25534439 -0.49245787 -1.6256757 -3.4532733 -4.7759304 -5.70976 -6.3081183 -6.8171082][-3.2188692 -2.2986522 -1.5741429 -0.76392174 -0.31524897 -0.28059673 0.21547413 0.44995022 0.22767258 -1.0151114 -2.6449976 -3.836237 -5.2383204 -6.3200555 -6.5376773][-2.5090871 -1.891705 -1.0433064 -0.37318039 0.66482162 1.0238571 0.70970535 0.7328043 1.1763077 0.23396683 -1.5080361 -2.4764109 -3.2102981 -4.3163042 -5.337749][-2.4227514 -1.4182987 -0.46762085 -0.2622261 0.48544979 1.7818403 2.0315914 1.7861633 1.2852545 0.41260242 -1.0637255 -2.5728321 -3.6543484 -4.1017256 -4.6706743][-1.5047302 -0.63548326 0.13191032 0.67174911 1.1400805 1.4838781 2.0593529 2.4920149 2.0633097 0.5428896 -1.0658617 -2.4984736 -3.6400352 -4.7223063 -5.6001258][-1.0053396 -0.97679424 -0.51460457 0.702322 1.7252083 2.0733624 1.8006124 1.3289604 1.1414509 0.36253166 -1.0161548 -2.3202715 -3.0953479 -4.0298967 -4.5183525][-1.4549441 -0.62731266 0.23663378 0.65987968 1.4302435 2.2729473 2.1335287 1.7081165 1.5390263 0.85105038 -0.91331244 -2.4721456 -3.3796797 -4.2303824 -4.6773367][-2.2684875 -1.3920174 -0.21774387 0.68780327 1.2031517 1.8781614 2.5673552 1.9878511 1.1566439 0.051581383 -1.3249297 -2.2345667 -3.3982244 -4.43764 -5.130507][-2.8323436 -2.0839052 -0.92866564 -0.054750919 0.33452892 0.84377384 1.2933388 1.57166 1.4443474 0.072471619 -1.3627372 -2.722271 -3.9802341 -5.0651426 -5.9125614][-3.144136 -2.6961436 -2.4563341 -1.67238 -1.3136392 -0.89652538 -0.64874029 -0.33673143 -0.024849415 -0.83719969 -2.1943088 -3.2706184 -4.2361 -4.5604286 -5.4943643][-5.3934965 -4.65013 -4.4330606 -4.1411319 -3.803355 -3.1897149 -2.7645411 -2.2048116 -2.047739 -2.9199986 -4.43467 -5.3882146 -6.17804 -6.2478561 -6.5518537][-6.3412242 -5.2349844 -4.2860951 -4.0871925 -4.1903691 -3.9912405 -3.8015928 -3.6312814 -3.579257 -3.8259521 -4.4248409 -4.897686 -5.7030807 -6.521585 -6.98883][-7.3994913 -6.6357369 -5.5967693 -4.7339964 -4.5270286 -4.6134043 -4.1796236 -4.11679 -4.3377833 -4.700274 -5.6026483 -5.8322382 -6.1653948 -6.4116678 -6.7055087][-6.1141815 -6.0944047 -5.7219429 -4.9671574 -3.6185236 -3.3307528 -3.6433473 -3.5118828 -3.3360538 -3.6895375 -4.3675652 -4.0781279 -4.4671249 -5.3897324 -6.1920223][-6.3548794 -5.9620123 -5.3116951 -5.339983 -5.3484192 -4.5195308 -4.1499958 -4.9088902 -5.3247976 -5.1918864 -5.6993423 -5.9271822 -6.20304 -6.4300628 -6.4427443]]...]
INFO - root - 2017-12-15 22:44:59.157928: step 69510, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 47h:00m:05s remains)
INFO - root - 2017-12-15 22:45:05.611426: step 69520, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 47h:44m:35s remains)
INFO - root - 2017-12-15 22:45:12.011115: step 69530, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 45h:45m:46s remains)
INFO - root - 2017-12-15 22:45:18.370506: step 69540, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 47h:01m:58s remains)
INFO - root - 2017-12-15 22:45:24.780495: step 69550, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 47h:49m:43s remains)
INFO - root - 2017-12-15 22:45:31.157697: step 69560, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 46h:48m:18s remains)
INFO - root - 2017-12-15 22:45:37.514544: step 69570, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 46h:36m:13s remains)
INFO - root - 2017-12-15 22:45:43.919189: step 69580, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 46h:24m:17s remains)
INFO - root - 2017-12-15 22:45:50.403199: step 69590, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 48h:11m:43s remains)
INFO - root - 2017-12-15 22:45:56.847145: step 69600, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 46h:14m:50s remains)
2017-12-15 22:45:57.353146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0320282 -3.7657578 -3.7974975 -4.0684581 -4.0156665 -3.4940004 -2.6004343 -1.8709679 -1.3193326 -2.1741276 -3.5525713 -4.4111233 -5.8649158 -7.89156 -8.8872681][-3.8634567 -3.5385404 -3.5227785 -3.951905 -4.2743216 -3.7874846 -3.3795848 -2.7353487 -2.207088 -2.22828 -3.631526 -4.4108009 -5.9225578 -7.9373178 -8.83291][-3.2001271 -3.260365 -3.1767707 -3.6368418 -3.4553766 -2.8692665 -2.6405597 -2.0361338 -1.5165105 -2.014544 -3.3421082 -3.902029 -5.1803126 -7.1128979 -8.297164][-3.7044561 -3.1155567 -2.674221 -2.4972782 -2.3602118 -1.8090105 -1.0044236 -0.65645504 -0.65559769 -1.5596466 -2.6977787 -3.2844267 -4.6764784 -6.3639536 -7.7416182][-3.742939 -2.5367041 -1.9477849 -1.6587977 -1.3782415 -0.42467451 0.31972122 0.62075996 0.620739 -0.51667643 -1.7677431 -2.8801541 -4.2675056 -5.9764838 -7.1413751][-3.9305937 -2.8366003 -2.0983667 -1.3585553 -0.81424522 0.076776981 0.90721035 0.95304012 0.8903532 -0.20986128 -1.8759813 -2.6938133 -3.9698672 -5.5863261 -6.4687724][-4.1918974 -3.0833807 -1.9056463 -1.2724066 -0.34088945 0.75086594 1.5378056 1.3060751 0.94783592 -0.24454021 -1.9758949 -3.0059052 -4.322608 -5.6933813 -6.455482][-4.2687697 -3.0386996 -2.0671983 -1.2876592 -0.18503284 0.79788589 1.5987358 1.3086872 0.98147678 -0.09308672 -1.945611 -2.6343083 -4.0886054 -5.6056795 -6.3012114][-4.0861959 -2.9270463 -1.890285 -1.1624451 -0.33079958 0.38866615 0.92486763 0.8830328 0.68659496 -0.4089179 -2.3233857 -2.8304634 -3.948204 -5.275383 -5.9679036][-4.3883128 -3.3415303 -2.3333507 -1.7990408 -1.2075658 -0.39666986 0.204175 0.1580739 -0.041454792 -1.012907 -2.656198 -2.9757023 -3.6580739 -4.8400879 -5.3535357][-5.4687557 -4.641006 -3.7572181 -3.2021966 -2.515254 -1.7743964 -1.2287493 -1.2687817 -1.3914151 -2.0599546 -2.9501286 -3.3675203 -3.9242764 -4.3813314 -4.4908466][-6.3302708 -5.6759276 -4.9545927 -4.7904673 -4.245904 -3.4369736 -2.795002 -3.0424247 -3.3245654 -3.9312267 -4.4133844 -4.2891364 -4.6300015 -4.8741021 -4.8949924][-6.96596 -6.5083895 -5.7457895 -5.5322647 -5.3861823 -4.7544284 -3.9329386 -3.9984922 -4.0860543 -4.7393188 -5.1900873 -4.7374926 -5.027586 -5.2555203 -5.3844323][-7.0126357 -6.4988289 -6.1562004 -6.3226318 -6.4102769 -5.8612413 -5.0726795 -4.73933 -4.6213741 -5.0341692 -5.0659757 -4.9748211 -5.1233387 -5.3263979 -5.2562222][-8.3413115 -7.6712608 -6.5611868 -6.9038887 -6.9087138 -6.5493956 -6.2100511 -6.1133585 -6.1020122 -5.8187366 -5.8167725 -5.9446955 -5.9526668 -5.9206381 -5.8376923]]...]
INFO - root - 2017-12-15 22:46:03.760082: step 69610, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.675 sec/batch; 49h:17m:14s remains)
INFO - root - 2017-12-15 22:46:10.234689: step 69620, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 46h:16m:20s remains)
INFO - root - 2017-12-15 22:46:16.602779: step 69630, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:23m:34s remains)
INFO - root - 2017-12-15 22:46:22.971182: step 69640, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 46h:05m:45s remains)
INFO - root - 2017-12-15 22:46:29.306701: step 69650, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:37m:26s remains)
INFO - root - 2017-12-15 22:46:35.726918: step 69660, loss = 0.28, batch loss = 0.17 (11.6 examples/sec; 0.688 sec/batch; 50h:15m:06s remains)
INFO - root - 2017-12-15 22:46:42.232634: step 69670, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 47h:20m:10s remains)
INFO - root - 2017-12-15 22:46:48.605251: step 69680, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 46h:31m:55s remains)
INFO - root - 2017-12-15 22:46:55.005752: step 69690, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 47h:12m:08s remains)
INFO - root - 2017-12-15 22:47:01.414181: step 69700, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 47h:08m:11s remains)
2017-12-15 22:47:02.094380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0424318 -4.7289276 -4.8814173 -4.3254519 -3.8743789 -2.9848137 -2.8636374 -2.846086 -2.630825 -3.7975552 -3.7117548 -5.0289431 -4.9971647 -5.9743042 -7.42301][-3.8844707 -4.0468287 -4.135603 -4.1593609 -4.6474609 -4.7495856 -4.8242579 -4.0182776 -3.8860033 -4.5226088 -3.7370298 -4.6284566 -4.8214769 -5.2802691 -6.7966967][-2.9520802 -3.4838605 -3.9312749 -3.7758279 -4.2356243 -4.6799498 -5.2588186 -5.0615559 -4.8311954 -5.3581743 -4.8250494 -4.880909 -4.7433271 -5.100976 -6.674418][-3.986434 -4.3762083 -3.64816 -3.3626766 -3.4792647 -3.5231695 -3.42421 -3.6995616 -4.2155738 -4.8901982 -4.3224869 -5.0225677 -5.49959 -5.5189962 -6.4161572][-4.1725168 -4.5844355 -4.0210013 -3.2186542 -2.3471532 -1.714467 -1.3264012 -1.605207 -1.8634439 -2.9926805 -3.2726641 -4.3515892 -4.828681 -5.4460306 -6.9960613][-4.9078922 -4.9054003 -3.9926124 -2.6863751 -1.7892461 -0.57501173 0.53471565 0.63155651 0.38732815 -0.70469236 -1.0731854 -2.7624092 -3.6200428 -4.4565945 -5.731739][-3.4378719 -4.0978746 -3.5512156 -1.9098563 -0.52739239 0.48963737 1.5467825 1.53228 1.8593292 1.1508751 0.34939194 -1.1152744 -2.3424091 -3.539341 -5.3937445][-3.0214229 -2.7418437 -1.7396379 -0.31140518 0.60151196 1.1878958 1.7177868 1.5572939 1.7129269 1.4013615 1.1181574 -1.1625409 -2.5616803 -3.3980079 -5.151288][-2.9386873 -2.3016133 -0.61226034 0.27369881 1.0678902 1.3804293 1.5692511 1.1318302 1.2030869 0.69536209 0.1463294 -1.6424708 -3.0460553 -4.6294241 -6.10218][-2.8974304 -2.0395336 -1.7528496 -0.99964523 0.023031235 0.58621311 0.88645458 0.09106636 0.57915974 0.25332737 -0.45350885 -2.4385333 -3.5872498 -5.1012077 -7.0574946][-4.8758307 -3.9948311 -3.5375333 -3.1181092 -2.4057102 -1.71977 -1.4098134 -1.3022108 -1.0903225 -1.5521078 -1.9424076 -3.4075642 -4.0973568 -5.5667524 -7.2462082][-7.1739063 -7.0294471 -6.32745 -5.2002449 -4.5620012 -4.1727142 -4.2152157 -3.621479 -2.6303191 -3.1155825 -3.7838671 -4.4935913 -4.491993 -5.8068647 -7.1787987][-9.3248482 -8.9982605 -8.9380875 -7.8746133 -7.4235926 -7.062768 -6.4538455 -5.7277966 -5.6380539 -5.6651611 -5.1292973 -4.937799 -5.0183234 -6.257576 -7.1241112][-8.5747433 -9.5486641 -9.5368071 -8.7994089 -8.3354845 -8.03129 -7.7411118 -7.3597765 -6.8640809 -6.5238008 -5.9210815 -5.4507542 -5.4972463 -6.0870686 -6.5373359][-7.9549069 -8.0209627 -8.9794559 -8.8607674 -8.4313269 -8.2149992 -7.9900317 -8.5488138 -8.4646435 -7.7621951 -6.9251885 -6.2336526 -6.058846 -6.0122256 -6.4248948]]...]
INFO - root - 2017-12-15 22:47:08.689201: step 69710, loss = 0.34, batch loss = 0.23 (12.0 examples/sec; 0.669 sec/batch; 48h:51m:42s remains)
INFO - root - 2017-12-15 22:47:15.097530: step 69720, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 47h:57m:21s remains)
INFO - root - 2017-12-15 22:47:21.566077: step 69730, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-15 22:47:27.938686: step 69740, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 46h:59m:16s remains)
INFO - root - 2017-12-15 22:47:34.369953: step 69750, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 46h:17m:27s remains)
INFO - root - 2017-12-15 22:47:40.829145: step 69760, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 47h:45m:09s remains)
INFO - root - 2017-12-15 22:47:47.297338: step 69770, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 47h:15m:11s remains)
INFO - root - 2017-12-15 22:47:53.742138: step 69780, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 47h:52m:57s remains)
INFO - root - 2017-12-15 22:48:00.155624: step 69790, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 46h:16m:57s remains)
INFO - root - 2017-12-15 22:48:06.616768: step 69800, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 46h:07m:09s remains)
2017-12-15 22:48:07.216776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4865475 -7.4127393 -6.0827937 -6.0199685 -5.7218585 -4.9549422 -4.0430317 -3.4005342 -2.6075625 -3.2628856 -3.7754874 -5.41102 -6.3366561 -7.7076406 -8.2674484][-7.7247076 -7.4289045 -7.800436 -7.2973647 -6.6229267 -5.832458 -5.3055553 -4.106317 -3.323163 -4.4838018 -5.0188646 -6.3415012 -7.1356783 -8.4301662 -9.4574022][-7.698946 -7.2377443 -6.8384914 -5.9642811 -5.3561463 -4.758976 -4.0718975 -3.4775324 -2.7985563 -4.04261 -4.9161153 -5.9713292 -6.7129421 -7.8301444 -8.3557072][-7.3844314 -7.0685043 -6.1492009 -4.420692 -3.4568172 -2.9187269 -2.1596241 -1.8065906 -1.5883064 -3.1168714 -4.0233359 -5.50207 -6.1869764 -7.1359739 -7.7884812][-6.4418182 -4.74683 -3.2577438 -1.9661169 -1.4593825 -0.49858284 0.48012066 0.25297689 -0.13122606 -2.0077548 -3.0051336 -4.6945705 -5.6642685 -6.9260163 -7.2188039][-4.2333078 -3.1384072 -2.0971823 -0.64931011 0.54149342 1.9314661 2.7482643 2.2648792 2.0056391 -0.24933577 -1.6627293 -3.4177284 -4.342062 -5.6182442 -5.9977255][-2.7621174 -1.4898362 -0.78625107 0.55215359 1.9159641 3.180316 3.7689724 3.3849754 3.2792168 1.3515224 -0.070764542 -2.208251 -3.8878644 -5.23631 -5.9497275][-2.6285839 -1.6284871 -0.58072138 0.905632 2.0836143 3.5031538 4.2060928 3.7785969 3.6670265 1.5080929 -0.13813639 -2.3389916 -3.7517219 -5.2241526 -5.9995213][-2.4962649 -1.9646239 -1.4480886 0.045679092 1.3335638 2.4080954 3.0031643 2.9308805 2.6992693 0.61413765 -0.77591085 -2.7582617 -3.9729156 -5.6241183 -6.6709037][-2.413084 -2.1427078 -1.4837489 -0.606236 -0.12934732 0.91161823 1.6497202 1.8280926 1.7434139 -0.20559263 -1.340507 -2.9069314 -4.397872 -5.7851992 -6.6760445][-3.1066313 -2.2920575 -1.93395 -1.3567119 -0.60908031 0.11544228 0.30781126 0.066212177 -0.0012750626 -1.2311707 -2.0077472 -3.3353271 -4.3839865 -5.5750141 -6.1910605][-3.3402719 -3.1842003 -3.5628896 -3.6306896 -3.0100498 -1.9210353 -1.6182775 -1.6195645 -1.3788486 -2.6731377 -3.530879 -3.9197025 -4.3818569 -5.5485916 -6.0425611][-3.8838158 -4.1680536 -4.4451728 -4.8337955 -4.5435257 -3.8343539 -3.8099174 -3.9373288 -3.5884404 -3.8939483 -3.7389257 -3.8091843 -4.2769694 -5.4522686 -6.2466192][-2.9479012 -3.884629 -4.1553011 -3.9909487 -4.089242 -4.6200523 -4.5722761 -4.3098125 -3.9588823 -4.3835993 -4.5896568 -4.7084517 -4.874053 -5.2060313 -5.5342522][-5.4675283 -5.3043394 -5.1774945 -5.5543623 -6.0637088 -6.1580954 -6.2961369 -6.2497926 -6.0809269 -5.7655234 -5.6041093 -5.3213649 -5.1836586 -5.4288373 -5.7343698]]...]
INFO - root - 2017-12-15 22:48:13.644295: step 69810, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 47h:33m:20s remains)
INFO - root - 2017-12-15 22:48:20.032104: step 69820, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 47h:54m:53s remains)
INFO - root - 2017-12-15 22:48:26.520873: step 69830, loss = 0.39, batch loss = 0.28 (12.7 examples/sec; 0.630 sec/batch; 45h:56m:01s remains)
INFO - root - 2017-12-15 22:48:32.826283: step 69840, loss = 0.24, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 45h:43m:13s remains)
INFO - root - 2017-12-15 22:48:39.226453: step 69850, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 46h:06m:27s remains)
INFO - root - 2017-12-15 22:48:45.636403: step 69860, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 45h:46m:52s remains)
INFO - root - 2017-12-15 22:48:52.148234: step 69870, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 47h:42m:16s remains)
INFO - root - 2017-12-15 22:48:58.570810: step 69880, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 47h:11m:17s remains)
INFO - root - 2017-12-15 22:49:04.935124: step 69890, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 46h:17m:31s remains)
INFO - root - 2017-12-15 22:49:11.268621: step 69900, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 45h:17m:37s remains)
2017-12-15 22:49:11.749835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5973878 -5.7908006 -5.8305378 -5.678638 -5.781198 -6.0713568 -6.4339542 -6.2706976 -5.8863678 -5.7902985 -7.6598916 -7.7337627 -8.2205687 -9.2030048 -8.6425047][-4.7994051 -5.033134 -4.6749978 -4.9580669 -5.1423931 -5.268786 -5.7867289 -5.7980728 -5.60019 -5.334446 -6.8400664 -6.3168964 -6.9442463 -8.0471344 -7.6261053][-3.6128139 -3.4730392 -3.5989776 -3.5643296 -3.2770662 -4.0984516 -4.5004568 -4.2804213 -4.1974039 -3.9077485 -5.8622046 -5.2831044 -5.7682686 -6.7850394 -6.4528956][-3.1225457 -2.4558983 -2.383543 -2.1434975 -2.4674392 -2.8863153 -2.6146374 -2.7337832 -2.5427275 -2.5379033 -4.3960371 -3.8994431 -4.90746 -6.211885 -5.9344587][-2.8195691 -2.3967013 -1.8913341 -1.4869242 -1.2076097 -0.87009811 -0.80795908 -0.70238924 -0.15156746 -0.66439533 -2.6610413 -2.08362 -3.2364688 -4.8675852 -4.8878679][-2.1068721 -1.7879786 -1.1421456 -0.24544382 0.14795208 0.65747833 1.0726681 1.2839937 1.4758081 0.86206532 -1.2694578 -0.94678831 -2.3337297 -3.8102584 -3.905102][-1.9872861 -1.5925379 -0.87726974 -0.2824049 0.5837841 1.4363241 2.0171776 2.257453 2.2799606 1.3762512 -0.74431276 -0.79594469 -2.5866756 -4.4119272 -4.2863574][-1.84094 -1.689527 -1.13697 -0.41922283 0.41241455 1.4929228 2.2440252 2.5266056 2.6583195 1.6620407 -0.74163246 -1.0596113 -2.7806582 -5.1216178 -5.1076307][-2.3108673 -2.1268506 -1.2223458 -0.66735649 0.17738676 1.0995159 1.6957264 2.0385523 2.1608524 1.2941008 -1.0797677 -1.9161696 -3.3945913 -5.4100971 -5.3672314][-3.9859185 -3.5187521 -2.7041874 -1.8294315 -0.60335827 0.20087433 0.82958794 1.1723652 1.2434502 0.10740376 -1.8522716 -2.4871063 -3.8905971 -5.5846233 -6.1346712][-7.080966 -6.068059 -5.4317222 -4.3073416 -3.2600751 -2.4247441 -1.4312372 -0.972435 -0.88874578 -1.655623 -3.4971738 -4.1961246 -5.2134 -6.2919374 -6.6036139][-8.3734941 -7.7024665 -6.9511456 -6.1742139 -5.4802084 -4.4539728 -3.4761219 -3.325068 -3.1329541 -3.4299412 -4.472229 -4.7404966 -5.4512739 -6.5502629 -6.6633825][-8.2370129 -8.6142273 -7.921464 -7.3234978 -6.7995415 -5.8772426 -4.7714491 -4.3535376 -4.0737281 -4.2752919 -5.0079374 -4.822958 -4.84894 -5.48571 -5.7630949][-7.1338167 -7.8417096 -7.5762119 -7.4793344 -7.1875753 -6.3539476 -5.3229284 -4.9820833 -4.8553267 -4.6666946 -5.2306638 -5.0574989 -4.904799 -5.1351614 -5.2385][-7.5602403 -7.69957 -7.3975406 -7.4717264 -7.461792 -7.009367 -6.5086608 -6.2887411 -6.3949785 -6.2651806 -6.2859893 -6.2459459 -6.2651811 -6.2922192 -5.9778237]]...]
INFO - root - 2017-12-15 22:49:18.124856: step 69910, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 46h:29m:38s remains)
INFO - root - 2017-12-15 22:49:24.510673: step 69920, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 46h:53m:15s remains)
INFO - root - 2017-12-15 22:49:30.869103: step 69930, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 45h:38m:38s remains)
INFO - root - 2017-12-15 22:49:37.253120: step 69940, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 48h:16m:41s remains)
INFO - root - 2017-12-15 22:49:43.782795: step 69950, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 45h:11m:37s remains)
INFO - root - 2017-12-15 22:49:50.178792: step 69960, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 46h:42m:44s remains)
INFO - root - 2017-12-15 22:49:56.593924: step 69970, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 46h:27m:39s remains)
INFO - root - 2017-12-15 22:50:02.908252: step 69980, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 46h:48m:57s remains)
INFO - root - 2017-12-15 22:50:09.380415: step 69990, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 46h:26m:37s remains)
INFO - root - 2017-12-15 22:50:15.755981: step 70000, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 47h:42m:17s remains)
2017-12-15 22:50:16.275378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8128595 -2.3042479 -1.6058955 -1.8323936 -1.8855076 -3.0688205 -1.1856413 -1.2426186 -1.095521 0.99196243 -3.4879022 -5.2266874 -5.7232227 -3.9613314 -3.5676427][-0.21869993 -0.93266392 -0.42559958 0.82833767 1.2813826 -1.3778858 -1.4163074 -0.52732229 0.7330265 1.6895876 -2.6699915 -3.9231484 -5.4283185 -4.1527185 -3.6531448][-1.4721847 -1.7540984 -0.88635921 -0.87313461 1.9013395 1.98281 2.4381227 1.2490444 0.69949627 2.1755152 -1.0527349 -2.8363533 -4.7958727 -4.1821127 -4.04552][0.33012867 -0.72731638 -0.99875259 0.49901676 1.3331242 0.94360447 2.8373585 2.2043409 3.9005013 4.1695967 -2.0066414 -3.5330958 -4.1823845 -4.1616068 -4.3333616][-1.4217396 -0.52649355 -0.010851383 -0.45115662 1.3913994 2.1623831 2.1357241 1.8192215 2.9981289 5.5705605 2.7426748 -1.0439472 -4.4181662 -3.978147 -4.4970665][1.2706747 -0.18179226 -1.0581551 0.27126694 1.410697 2.0363779 3.5609369 3.1801081 2.3811951 3.2375441 -0.12999916 0.096385956 -2.0349207 -3.6059923 -4.5440364][-0.571538 -0.35429621 2.4484272 2.7285347 2.6152458 3.2564297 5.1905804 6.0948954 6.283453 5.6773672 0.76811504 -2.0377712 -3.8126848 -2.6337128 -4.0878358][-1.4106569 -0.22835159 0.32244396 2.1688385 4.722249 4.8966341 4.49222 5.5665064 7.7074661 7.0111361 1.1044598 -0.9714818 -3.3994589 -3.9752333 -4.1634016][-1.2687693 -1.2998662 0.77477741 1.7781944 3.870841 4.2290325 5.62131 6.2183952 6.1227436 6.4604197 1.2174177 -1.1472249 -3.1238351 -2.6021147 -3.5802011][-2.7211776 -2.7231727 -0.49289846 1.297473 3.1443815 2.1164751 3.1325378 4.7503538 4.4069414 4.3999357 0.56601906 -0.13245964 -2.0849662 -2.0200019 -2.5783319][-2.4745374 -4.4480543 -3.5113187 -1.9814544 -0.44404125 1.1143837 0.4864502 0.25230694 -0.22375584 0.48019981 -2.0324755 -2.6912818 -3.5972619 -3.4129848 -4.1718311][-5.4741087 -5.500721 -4.7955728 -4.06719 -3.0481648 -3.6924512 -2.3086877 -1.3793001 -1.4372225 -1.1004314 -1.8331366 -2.8200474 -5.1628876 -4.4510703 -4.6804705][-6.631701 -7.1126628 -6.5115008 -5.6200871 -3.2192574 -3.9433777 -4.4547386 -3.2877369 -2.2545447 -1.0612793 -2.0665984 -1.8650193 -3.6266837 -4.9090786 -5.8681021][-5.0186934 -6.0110817 -5.8014779 -5.1061649 -4.6244369 -4.3395133 -2.9819727 -2.744401 -3.2267723 -2.0582657 -2.4631767 -2.4832993 -2.8924141 -3.761519 -5.5369306][-3.7489014 -4.2480688 -3.3608546 -3.8608477 -3.2988749 -3.3664265 -2.4155207 -2.529213 -2.1206617 -1.8925252 -2.2571096 -1.9202332 -2.6197834 -3.5300875 -3.8716836]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 22:50:23.703800: step 70010, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 46h:30m:42s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 22:50:30.103673: step 70020, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 45h:58m:43s remains)
INFO - root - 2017-12-15 22:50:36.525085: step 70030, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 45h:41m:44s remains)
INFO - root - 2017-12-15 22:50:42.904173: step 70040, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 45h:57m:57s remains)
INFO - root - 2017-12-15 22:50:49.333971: step 70050, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 46h:14m:10s remains)
INFO - root - 2017-12-15 22:50:55.723617: step 70060, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 45h:52m:02s remains)
INFO - root - 2017-12-15 22:51:02.024134: step 70070, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 45h:50m:27s remains)
INFO - root - 2017-12-15 22:51:08.401635: step 70080, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 46h:45m:35s remains)
INFO - root - 2017-12-15 22:51:14.818791: step 70090, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 49h:07m:52s remains)
INFO - root - 2017-12-15 22:51:21.220337: step 70100, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 45h:45m:53s remains)
2017-12-15 22:51:21.758253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4595494 -3.377255 -3.7205648 -3.8144596 -3.9409389 -3.9952011 -3.4111619 -2.6053586 -1.7118263 -2.5426221 -3.5705085 -4.7697821 -4.2234726 -6.1993222 -7.4404135][-3.9397345 -4.2393775 -3.8931448 -3.688942 -3.8462489 -3.7684388 -3.7315226 -3.3994236 -2.4669313 -2.6268072 -3.0818162 -4.3832054 -4.8647881 -6.673945 -6.9607177][-4.3005896 -4.7402143 -4.8904638 -4.4385157 -4.0514369 -3.6873541 -3.196949 -2.8778543 -2.6948338 -3.5133653 -3.9779086 -4.804615 -4.5393453 -5.813849 -6.8909945][-4.5865965 -4.4035559 -4.28 -3.4922843 -3.0971088 -2.4960151 -1.8662548 -1.911623 -1.857697 -2.8096242 -3.8127003 -5.3673687 -4.845953 -5.7468052 -6.6753883][-5.002903 -4.3360462 -3.3887529 -1.9754686 -1.6405821 -1.0519285 -0.41657209 -0.35688972 -0.088149071 -1.2945857 -2.4803982 -4.1382961 -4.7698569 -6.3506227 -6.9895687][-4.9597254 -4.2297344 -3.1957507 -1.2423806 -0.57129049 0.5885191 1.4003582 1.0207863 0.85585022 -0.009721756 -0.74950171 -2.6708407 -3.6003871 -4.8514767 -6.4248614][-3.6484551 -2.7485867 -1.5283809 0.076507092 1.0142879 2.0348749 2.7757168 2.2762661 2.1349897 0.72902393 -0.21511507 -2.0426798 -2.9235163 -4.6315579 -6.2144976][-1.3203163 -0.72448587 -0.65434742 0.82897472 1.8720512 2.6510572 3.6957855 3.8439207 3.4485207 1.8382883 0.13123655 -2.1395721 -2.6088128 -4.3624067 -5.8942657][-2.6627278 -1.0081053 0.179595 0.895031 0.75676918 1.3767242 2.3620758 3.0461674 3.5470619 2.5736923 0.15637589 -2.497046 -3.5828567 -5.2429466 -6.2233524][-3.6511946 -3.086112 -2.2645245 -0.36067772 0.79285812 1.1512194 0.89663792 1.3289051 1.6766167 0.9206686 -0.31920385 -2.5747957 -4.0408478 -5.5783463 -6.3333244][-4.8514137 -4.6140504 -4.4511685 -3.3895602 -2.0322261 -1.306035 -0.67405939 -1.0344758 -1.3957381 -2.4953437 -3.5591469 -4.85905 -5.0693884 -5.6639524 -6.769784][-7.9900947 -6.8050013 -5.7136106 -5.3771057 -5.198277 -4.5221949 -3.66858 -3.254395 -2.8384719 -4.2576194 -5.9302588 -6.2202826 -6.4764805 -6.6853142 -6.990973][-8.3217535 -8.36198 -6.9121532 -6.0486388 -5.9560313 -5.0460334 -4.5535803 -4.3177013 -4.2078691 -4.8229761 -6.119369 -6.3558483 -6.408123 -6.1574221 -6.2424006][-7.9449024 -7.7321448 -7.6311336 -7.0023146 -6.3251166 -5.2672682 -5.0059023 -5.0400858 -4.9176779 -4.8191442 -5.6731992 -5.7851033 -5.7167506 -5.7492504 -5.5980225][-7.3527408 -6.9103985 -7.3957734 -7.269403 -6.922864 -6.6239061 -6.2759094 -6.1721597 -6.1873565 -6.6161923 -6.3123717 -5.7929029 -5.8321533 -5.7258306 -5.7216692]]...]
INFO - root - 2017-12-15 22:51:28.201713: step 70110, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 47h:24m:56s remains)
INFO - root - 2017-12-15 22:51:34.573255: step 70120, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 47h:46m:35s remains)
INFO - root - 2017-12-15 22:51:40.946437: step 70130, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:38m:23s remains)
INFO - root - 2017-12-15 22:51:47.384659: step 70140, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 46h:27m:37s remains)
INFO - root - 2017-12-15 22:51:53.706121: step 70150, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 47h:10m:57s remains)
INFO - root - 2017-12-15 22:52:00.134459: step 70160, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 46h:48m:22s remains)
INFO - root - 2017-12-15 22:52:06.509994: step 70170, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 47h:09m:21s remains)
INFO - root - 2017-12-15 22:52:12.956924: step 70180, loss = 0.32, batch loss = 0.20 (12.1 examples/sec; 0.659 sec/batch; 48h:01m:57s remains)
INFO - root - 2017-12-15 22:52:19.412093: step 70190, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 48h:27m:59s remains)
INFO - root - 2017-12-15 22:52:25.932613: step 70200, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:16m:05s remains)
2017-12-15 22:52:26.449566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1527677 -4.4052076 -4.5506678 -4.5768957 -4.4594078 -3.5627732 -2.4019122 -1.4940858 -0.6363349 -1.3870487 -1.6300097 -3.1411471 -4.4015207 -4.9986753 -6.0375452][-3.0779057 -3.7793441 -4.2384529 -4.18589 -3.9465582 -3.2768655 -2.3481398 -1.127739 -0.17907953 -0.81215525 -1.2334118 -2.7887373 -3.9001873 -4.9072652 -5.9500694][-3.000278 -3.1061668 -3.6921084 -3.8186245 -3.3680096 -2.7021809 -2.0562177 -1.7713184 -0.98067331 -1.2965727 -1.6373944 -3.36167 -4.4208527 -5.5946264 -6.4579439][-2.6610146 -2.7680373 -3.275166 -2.6980996 -2.1297197 -1.7511368 -1.0947266 -0.71790171 -0.51579666 -1.4916434 -1.9488378 -3.5479269 -4.8589921 -6.0027814 -6.6724982][-2.667254 -2.45995 -2.0874391 -1.4163523 -0.99913645 -0.49562931 -0.042961597 0.15331268 0.22091722 -0.84867191 -1.7606931 -3.680748 -4.7725039 -5.9571347 -7.1613317][-3.3473263 -2.1949363 -1.6835775 -0.65074873 0.15104628 0.94782352 1.63865 1.1984072 0.57759285 -0.62653828 -1.4287605 -3.719964 -5.1563768 -5.8864079 -6.5992222][-2.7077227 -1.7257724 -0.53052807 0.87009525 1.9874783 3.0975676 3.5767298 2.9409037 2.2117977 0.1940465 -1.0892577 -3.3996658 -5.0471277 -6.1301794 -6.5391064][-2.1592937 -1.1098661 -0.46766281 1.2515945 2.7127886 3.7700796 4.6456594 4.64841 4.05824 1.5779228 -0.10869122 -3.1344085 -4.800765 -5.6712055 -6.2706852][-1.953825 -1.5017548 -0.76993036 0.727252 1.693449 2.3736858 2.8095188 3.2463474 3.5501022 1.3883371 -0.35136223 -3.2683334 -4.6352878 -5.6692257 -6.2286234][-2.8398623 -2.4075341 -2.360796 -0.775033 0.45769215 0.96358681 1.2973413 1.5246124 1.5879097 -0.42240143 -1.9357657 -4.6893625 -5.5504508 -6.2509761 -6.5421391][-4.7261114 -4.210423 -3.7182837 -2.9158683 -2.4018254 -1.604362 -1.1533318 -0.67364931 -0.34643793 -2.524797 -3.8528252 -5.7423944 -6.4711161 -6.9653726 -7.2365251][-6.3800287 -5.6689773 -5.4857993 -4.7592707 -4.1473203 -3.8814638 -3.5477524 -3.2521367 -3.0878758 -4.0025091 -4.4817991 -5.8401384 -6.3202667 -7.0915937 -7.7745705][-7.173677 -7.0111489 -6.8198156 -6.1014571 -5.8216171 -5.1700644 -4.9560595 -4.8573456 -4.6310635 -5.27722 -5.4944806 -6.1988859 -6.600369 -6.6912317 -7.0942812][-7.9891872 -7.7481246 -7.6181178 -7.1041584 -6.7745619 -6.7380743 -6.4107409 -5.9698772 -6.0996137 -6.4336619 -6.2170882 -6.314908 -6.1573615 -6.7679133 -7.0135083][-9.1370316 -8.3418036 -8.2072477 -8.2789345 -8.0072994 -7.760519 -7.3302994 -7.5224543 -7.3357415 -7.1878147 -6.7916245 -6.7231765 -6.6052842 -6.518383 -6.5270967]]...]
INFO - root - 2017-12-15 22:52:32.896440: step 70210, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 45h:42m:03s remains)
INFO - root - 2017-12-15 22:52:39.221487: step 70220, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 45h:31m:43s remains)
INFO - root - 2017-12-15 22:52:45.599922: step 70230, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 48h:13m:01s remains)
INFO - root - 2017-12-15 22:52:51.998411: step 70240, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 45h:38m:14s remains)
INFO - root - 2017-12-15 22:52:58.430074: step 70250, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 45h:44m:03s remains)
INFO - root - 2017-12-15 22:53:04.925589: step 70260, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 46h:55m:18s remains)
INFO - root - 2017-12-15 22:53:11.280835: step 70270, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 45h:04m:24s remains)
INFO - root - 2017-12-15 22:53:17.575119: step 70280, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 45h:19m:26s remains)
INFO - root - 2017-12-15 22:53:23.986075: step 70290, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 47h:32m:12s remains)
INFO - root - 2017-12-15 22:53:30.343390: step 70300, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 46h:05m:30s remains)
2017-12-15 22:53:30.895412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5060782 -3.1232023 -3.4487181 -3.8310535 -4.1113129 -4.3538713 -4.6213369 -4.8369493 -4.8306856 -6.0514469 -6.720736 -7.8046279 -8.1221457 -8.3756075 -8.9929028][-2.943346 -3.3045249 -3.600843 -4.2981634 -4.8195591 -4.9727097 -5.2315073 -5.614913 -5.4899812 -6.6534138 -7.3111787 -8.7562275 -9.4393568 -9.7214146 -10.213219][-1.8330894 -2.2352571 -2.5242891 -2.9825048 -3.2527404 -3.4046602 -3.5625305 -4.0422125 -4.3770247 -5.9315796 -6.5804777 -7.7799287 -8.3570471 -8.5877371 -9.1723824][-1.6601863 -1.9047108 -1.9606709 -2.0656557 -2.1743731 -2.223732 -2.2644844 -2.5476027 -2.3871202 -3.7687242 -4.6454105 -5.9865465 -6.8615141 -7.6229534 -8.4367342][-1.8815107 -1.6450877 -1.6497092 -1.4781294 -1.125237 -0.78315306 -0.39612293 -0.34786129 -0.22107649 -1.601203 -2.6273055 -4.3315005 -5.4821181 -6.530622 -7.5604653][-3.077466 -2.5785089 -2.1746554 -1.6361814 -0.99849653 -0.28560162 0.59335327 0.91897392 1.2757721 -0.1196208 -1.1535115 -2.917891 -4.3129611 -5.3978014 -6.3526611][-3.753469 -3.0849471 -2.4151063 -1.3973827 -0.40320206 0.14201784 0.95373631 1.6159973 2.2382345 0.79544258 -0.26473141 -1.822329 -3.092783 -4.3898726 -5.6524925][-4.20168 -3.46146 -2.6647019 -1.4722695 -0.21073532 0.62412453 1.3613815 2.0036774 2.7967644 1.5652142 0.61232948 -1.1579833 -2.5186825 -3.7890024 -5.1059752][-4.0831909 -3.2763739 -2.6321492 -1.6890144 -0.76246166 0.13437223 1.0487089 1.4519739 1.9421339 0.57425594 -0.42956829 -1.9896383 -3.1264458 -4.2235441 -5.4410706][-4.4622231 -3.837563 -3.1463885 -2.1706209 -1.3114724 -0.61322784 0.065171242 0.57658863 1.0008049 -0.80611897 -1.6933231 -3.2161531 -4.3286047 -5.1559191 -6.1258097][-5.6431293 -5.1768475 -4.5980921 -3.6566534 -2.9248066 -2.1659064 -1.5467811 -1.3035955 -1.0972686 -2.7244697 -3.6482468 -4.8442516 -5.626544 -6.2745957 -6.9700289][-6.1157227 -5.8636794 -5.3725395 -4.6992178 -4.1862869 -3.3216953 -2.7745724 -2.872704 -3.0712576 -4.457077 -5.124999 -6.0308695 -6.2827134 -6.5647945 -7.014751][-6.6124811 -6.1186028 -5.9045291 -5.3514709 -4.8952351 -4.4704666 -4.247859 -4.2640533 -4.4088154 -5.49547 -5.9735041 -6.6067691 -6.9963956 -6.9052095 -6.8119521][-6.2125645 -5.8173141 -5.5114546 -5.1468239 -4.90942 -4.5585728 -4.3876143 -4.5523443 -4.74391 -5.7061586 -6.1115532 -6.1477237 -6.2067461 -6.4002457 -6.4115448][-7.2820621 -7.0232325 -6.7609243 -6.4024076 -6.05436 -5.7774906 -5.7566776 -5.88789 -6.0015855 -6.3394585 -6.6488075 -6.8082409 -6.8684 -6.4936194 -6.1695356]]...]
INFO - root - 2017-12-15 22:53:37.268623: step 70310, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 47h:40m:45s remains)
INFO - root - 2017-12-15 22:53:43.633274: step 70320, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:56m:17s remains)
INFO - root - 2017-12-15 22:53:50.030649: step 70330, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:19m:14s remains)
INFO - root - 2017-12-15 22:53:56.431852: step 70340, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 47h:18m:29s remains)
INFO - root - 2017-12-15 22:54:02.825697: step 70350, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 47h:22m:21s remains)
INFO - root - 2017-12-15 22:54:09.223246: step 70360, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 48h:01m:43s remains)
INFO - root - 2017-12-15 22:54:15.652058: step 70370, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.632 sec/batch; 46h:03m:10s remains)
INFO - root - 2017-12-15 22:54:21.998894: step 70380, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 47h:01m:36s remains)
INFO - root - 2017-12-15 22:54:28.551527: step 70390, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 46h:36m:43s remains)
INFO - root - 2017-12-15 22:54:34.941588: step 70400, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 47h:08m:44s remains)
2017-12-15 22:54:35.466163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9462862 -7.0371909 -8.5819082 -9.1022339 -9.2037592 -8.8419647 -7.8587112 -6.9201484 -5.9907818 -5.7860408 -6.0124373 -7.5804977 -7.5520487 -7.4584618 -6.8046346][-5.4954882 -6.7149591 -8.11591 -9.2784147 -9.8316164 -9.6653671 -8.9373465 -7.7806759 -6.3194242 -6.9380774 -8.0326633 -9.6447611 -9.8018827 -9.78407 -9.105814][-5.4196429 -6.9193764 -8.7497425 -9.49259 -9.56218 -8.616354 -7.8834229 -7.5465584 -6.8255658 -6.9730897 -7.8180127 -10.494008 -11.443688 -11.479561 -11.04558][-7.0660591 -7.7612076 -8.3642588 -8.2969379 -8.29811 -7.502378 -6.3883448 -5.3162818 -4.5552316 -5.6762791 -7.3478565 -9.7798491 -10.915029 -11.393482 -11.62438][-7.2490897 -6.9863744 -7.3930316 -6.8089485 -6.2754745 -4.0768738 -2.227457 -1.2993827 -0.95031738 -2.3667746 -4.6301227 -7.7812757 -9.4611778 -10.279154 -10.248932][-6.8876219 -6.3719077 -6.1902785 -4.545404 -3.3071675 -1.0421014 1.2510157 2.7436266 3.2446156 0.80748749 -1.9526472 -5.608254 -7.5521245 -8.5952749 -9.1748362][-5.7995095 -4.8868694 -3.7208745 -1.2729149 0.65485954 3.0194616 5.2797012 5.9681034 5.4498291 2.5745888 -0.52026367 -4.5144835 -6.5899549 -7.7970624 -7.7925677][-5.6130095 -4.8310881 -2.9379435 0.08520937 2.8210478 5.0778885 6.8039541 6.7080727 6.0933981 2.4624548 -1.575664 -5.3906851 -6.7883897 -7.3972306 -7.1054864][-6.4164286 -5.6566 -5.0157037 -2.8075995 -0.0471673 2.3813667 4.3343487 4.6438961 4.3048048 0.58061028 -3.2600117 -6.8689318 -8.5478067 -8.6497927 -7.6973686][-8.96647 -8.4817438 -7.8442693 -6.3086705 -4.3874989 -2.0406199 -0.74895382 -0.56368446 -0.88497734 -3.8219595 -6.3800135 -9.5328293 -10.707323 -9.9896088 -8.625701][-10.309837 -10.355643 -10.114541 -9.233984 -8.0660677 -6.2754831 -4.805481 -5.0471888 -5.6962285 -7.4368639 -8.4950562 -9.8878269 -10.606174 -10.824265 -10.437014][-10.982717 -10.982334 -10.554852 -10.203887 -9.60611 -8.4606256 -7.7629638 -7.8389144 -7.8001733 -8.8881521 -9.3927307 -9.4832754 -9.7017956 -9.4084435 -9.2251034][-10.306664 -11.065275 -11.175526 -10.377834 -9.1807632 -8.9506683 -8.4320526 -8.2299137 -8.4528179 -9.2794218 -9.6454773 -8.901413 -8.4297867 -8.170084 -7.7752132][-8.0991 -8.1816683 -8.9724007 -8.6801224 -8.4530821 -8.0046587 -7.4630051 -7.7629557 -7.7387562 -7.5996618 -8.480195 -8.342474 -8.0254126 -7.0497689 -6.3130465][-6.3351722 -6.6362209 -7.10204 -7.4052081 -8.2287493 -7.9494467 -7.5516648 -7.3689227 -7.3057847 -7.608902 -7.2068605 -6.7592497 -6.9540286 -7.1470547 -7.1894193]]...]
INFO - root - 2017-12-15 22:54:41.845945: step 70410, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 46h:59m:26s remains)
INFO - root - 2017-12-15 22:54:48.192424: step 70420, loss = 0.26, batch loss = 0.14 (13.1 examples/sec; 0.610 sec/batch; 44h:26m:28s remains)
INFO - root - 2017-12-15 22:54:54.590109: step 70430, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 46h:51m:16s remains)
INFO - root - 2017-12-15 22:55:01.018562: step 70440, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 46h:48m:25s remains)
INFO - root - 2017-12-15 22:55:07.388037: step 70450, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 45h:06m:28s remains)
INFO - root - 2017-12-15 22:55:13.751321: step 70460, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 45h:26m:02s remains)
INFO - root - 2017-12-15 22:55:20.168397: step 70470, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 47h:14m:30s remains)
INFO - root - 2017-12-15 22:55:26.622440: step 70480, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 48h:35m:33s remains)
INFO - root - 2017-12-15 22:55:32.982606: step 70490, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.616 sec/batch; 44h:50m:57s remains)
INFO - root - 2017-12-15 22:55:39.350022: step 70500, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:56m:53s remains)
2017-12-15 22:55:39.853832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8820996 -2.4759245 -2.7379451 -2.7493563 -2.4188886 -2.086421 -1.4579129 -0.86534166 -0.83379507 -1.8757071 -3.0773191 -5.3377924 -6.4134388 -7.1644239 -7.6756887][-3.7153211 -3.2739015 -3.9877682 -4.2089043 -4.1037159 -3.6690245 -2.9340549 -2.3007598 -2.0446124 -3.0871034 -4.0039697 -6.2469845 -7.2636442 -7.9858651 -8.667407][-4.6316876 -3.9761255 -4.0578008 -4.378603 -4.3874555 -4.0536375 -3.0985885 -2.6131778 -2.2300224 -2.895082 -3.5841374 -5.5719442 -6.5412812 -7.0352917 -7.7949176][-5.295135 -5.1672049 -5.0792141 -4.396481 -3.6938264 -2.8540363 -1.8103962 -1.6212559 -1.404541 -2.4743137 -3.2079773 -5.0463657 -5.97849 -6.3194847 -6.5135579][-5.9888716 -5.6885109 -5.6924515 -4.9293814 -3.8218071 -2.6140985 -1.2554979 -0.9035244 -0.70560265 -1.9030175 -2.4129448 -4.5485926 -5.6380253 -6.0437512 -6.6848092][-7.0288959 -6.4081545 -5.8223038 -4.4344969 -3.0080786 -1.2666483 0.56687069 0.99550533 1.2154408 -0.21888542 -1.1875367 -3.6738057 -4.9132118 -5.7362132 -6.4497948][-7.8849936 -6.8855405 -5.9162512 -3.9477131 -1.8685546 0.090419769 1.876605 2.2198219 2.4976692 0.86732006 -0.21173811 -2.6933026 -4.4303303 -5.3951454 -6.3601255][-7.6356039 -6.45967 -5.3352337 -3.082171 -0.96763468 1.2472048 2.898591 3.2583055 3.3012018 1.7820969 0.8299818 -2.1543136 -4.001615 -5.4012165 -6.770288][-7.2717104 -6.4227228 -5.7497487 -3.7848434 -1.9964485 -0.012910366 1.4765835 1.660511 1.5765371 0.478364 -0.10695696 -2.4226623 -4.042819 -5.33657 -6.70043][-6.7485137 -6.5257006 -5.7603846 -4.4492369 -3.0883355 -1.3899198 -0.22464085 0.0047631264 0.26914263 -1.1777821 -1.9735713 -4.034863 -5.0305042 -6.0326219 -7.0458603][-7.74646 -7.6715159 -7.4736395 -6.2600265 -5.0456839 -3.7184751 -2.7617731 -2.6559958 -2.3614511 -3.26787 -3.5930009 -5.19315 -6.2683506 -6.7164412 -7.1319003][-7.962718 -7.6134844 -7.5448208 -6.737318 -6.0796471 -5.0980053 -4.299417 -4.2380457 -3.9854102 -4.59751 -4.5571718 -5.4702339 -6.1083922 -6.3141918 -6.8625307][-8.669281 -8.7455606 -8.5043373 -7.7089815 -6.8738804 -6.1990948 -5.8016853 -5.9114747 -5.8137627 -6.2704697 -6.4414935 -6.8638978 -7.0958719 -6.8741822 -7.1117396][-7.7631545 -7.6970239 -7.3862147 -6.8968396 -6.2206116 -5.8041277 -5.5276585 -5.9742622 -6.1981888 -6.7104506 -6.8764777 -7.0750027 -7.3793116 -7.4620681 -7.51753][-9.62562 -8.8461914 -8.0302629 -7.4163141 -6.8911591 -6.895144 -6.7827163 -7.0371552 -7.4827352 -7.66025 -7.7883744 -7.8187909 -7.5692353 -7.4816914 -7.2459707]]...]
INFO - root - 2017-12-15 22:55:46.208650: step 70510, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 45h:32m:04s remains)
INFO - root - 2017-12-15 22:55:52.663011: step 70520, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 47h:29m:54s remains)
INFO - root - 2017-12-15 22:55:59.058076: step 70530, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 45h:58m:07s remains)
INFO - root - 2017-12-15 22:56:05.403644: step 70540, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 45h:15m:17s remains)
INFO - root - 2017-12-15 22:56:11.867698: step 70550, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 48h:06m:40s remains)
INFO - root - 2017-12-15 22:56:18.279155: step 70560, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 48h:11m:42s remains)
INFO - root - 2017-12-15 22:56:24.631024: step 70570, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 46h:28m:55s remains)
INFO - root - 2017-12-15 22:56:31.062646: step 70580, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.666 sec/batch; 48h:26m:32s remains)
INFO - root - 2017-12-15 22:56:37.513919: step 70590, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 45h:24m:40s remains)
INFO - root - 2017-12-15 22:56:43.840793: step 70600, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 46h:25m:00s remains)
2017-12-15 22:56:44.401495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5917907 -3.4537988 -2.5576816 -1.5563097 -0.87790823 -0.50589752 -0.29502583 -0.084560394 -0.069646358 -1.4159327 -3.3702822 -4.485939 -5.2713323 -6.013072 -6.7913651][-3.3419333 -1.9860091 -1.2292337 -1.12463 -0.98544121 -0.56356907 0.078377247 0.37236309 0.39198494 -0.78798246 -2.4145055 -3.7988369 -5.2907953 -6.2633848 -7.1187687][-2.8687792 -2.3371458 -1.1249723 -0.14911222 0.3961544 0.3868742 0.30841637 0.57753468 1.0512199 0.30307341 -1.1115947 -2.2246103 -3.35214 -4.1898613 -5.6127033][-2.135705 -1.485671 -0.63856316 -0.17825127 0.76849461 1.2236519 1.4347668 1.2280874 0.94553185 0.42115211 -0.93600559 -2.2606668 -3.2961392 -3.6849728 -4.4839611][-1.4213743 -0.52982569 0.18119955 0.39684486 0.65590191 1.3421869 1.9750204 1.9376364 1.8467884 0.92510033 -0.77588415 -2.5102167 -3.79655 -4.1786094 -5.0803318][-1.4851122 -1.0520945 -0.44548512 0.4425087 1.3163786 1.2826633 1.0787573 1.1952496 1.2294054 0.4766531 -0.86486149 -2.0932236 -3.1185312 -3.6528873 -4.3752294][-1.4822822 -1.0065737 -0.251956 0.4039917 1.1873198 1.4736948 1.5209684 1.1918449 0.95377827 0.46631145 -1.1958804 -2.4819102 -3.6689887 -3.8757777 -4.5428143][-1.69946 -1.2610011 -0.7439971 0.30633497 0.89601707 1.0814428 1.5364676 1.5201693 1.0211344 0.091991425 -1.0173473 -2.2784176 -3.7848232 -4.2628503 -5.0202084][-2.8726134 -2.0524464 -0.99174738 -0.54101753 0.022035122 0.63076496 1.0421829 0.87823772 0.74538803 0.025159836 -1.2536192 -2.1936674 -3.3960309 -4.2432175 -5.4840546][-3.2548866 -3.0002403 -2.5926661 -2.0809379 -1.704392 -1.2400527 -0.50540543 -0.20179367 -0.027013302 -1.0172148 -2.193337 -2.7714677 -3.8939438 -4.2235074 -5.2696509][-4.8628078 -4.3109388 -3.6276731 -3.5797663 -3.4049096 -2.75975 -2.4947147 -2.2524209 -1.8811007 -2.9637842 -3.6629014 -4.0453644 -4.639864 -4.394722 -5.4306583][-6.7324204 -5.1800394 -4.6276493 -4.1199532 -3.6560607 -3.4505658 -3.3548179 -3.8230348 -4.2574682 -4.6242104 -4.4665256 -4.78152 -5.1543283 -5.2124681 -6.0436716][-7.1980987 -6.8719716 -5.8031993 -4.8924141 -4.575099 -4.2061229 -4.0530043 -3.8008413 -3.8566234 -4.8180246 -5.2948794 -5.1295552 -5.1774497 -5.22254 -6.3415475][-6.1205964 -5.8024769 -5.7667379 -5.0863237 -4.1650887 -4.0219984 -3.936465 -4.3671012 -4.6433382 -4.5323954 -4.3997211 -4.5559235 -4.8846803 -4.7236786 -5.6782923][-7.1224351 -6.26415 -5.2216372 -4.8926315 -4.8495026 -4.9316807 -4.7041125 -4.834197 -5.0052819 -5.4095926 -5.8872623 -5.8539877 -5.8831282 -6.107368 -6.085495]]...]
INFO - root - 2017-12-15 22:56:50.910770: step 70610, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 46h:45m:26s remains)
INFO - root - 2017-12-15 22:56:57.297764: step 70620, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 47h:41m:32s remains)
INFO - root - 2017-12-15 22:57:03.804185: step 70630, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 47h:47m:28s remains)
INFO - root - 2017-12-15 22:57:10.237584: step 70640, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:16m:04s remains)
INFO - root - 2017-12-15 22:57:16.651622: step 70650, loss = 0.34, batch loss = 0.22 (12.8 examples/sec; 0.627 sec/batch; 45h:36m:15s remains)
INFO - root - 2017-12-15 22:57:23.062521: step 70660, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 45h:47m:08s remains)
INFO - root - 2017-12-15 22:57:29.367182: step 70670, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 45h:23m:37s remains)
INFO - root - 2017-12-15 22:57:35.748264: step 70680, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 45h:38m:30s remains)
INFO - root - 2017-12-15 22:57:42.152154: step 70690, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 47h:24m:02s remains)
INFO - root - 2017-12-15 22:57:48.529724: step 70700, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 47h:59m:32s remains)
2017-12-15 22:57:49.057537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8635235 -5.1503592 -5.48099 -5.8391867 -6.1877489 -6.320797 -6.29 -5.7467904 -4.4838085 -4.0629721 -4.7932053 -4.8669977 -4.7591424 -5.4504008 -6.1555634][-5.234684 -5.3020182 -5.6904984 -5.9231858 -6.2884169 -6.4932089 -6.657021 -6.3498688 -5.337697 -5.2395597 -5.7870412 -5.831531 -5.9495544 -6.5398459 -6.40741][-6.2650037 -6.3115654 -6.3361406 -6.0656404 -6.085269 -6.2240787 -6.4281888 -6.3391938 -5.8229666 -6.1720333 -6.9128933 -6.9687858 -6.9983144 -7.2848978 -7.1987219][-7.3356123 -6.8213906 -6.0674586 -4.9661627 -4.2822814 -3.7187285 -3.6676645 -3.5951247 -3.4889045 -4.2826319 -5.6645021 -6.4307537 -6.8991146 -7.5399771 -7.8329816][-7.6562715 -6.823875 -5.62591 -4.1929045 -3.0501842 -1.9524961 -1.7245259 -1.4641924 -0.99269915 -1.9935703 -3.6778812 -4.8054295 -6.08214 -7.7619147 -8.5009117][-7.7589817 -6.4437995 -4.9261317 -3.100883 -1.5512319 -0.11578894 0.66473389 1.0436134 1.3511457 0.52133942 -0.90864038 -2.5112882 -4.3911028 -6.3690171 -7.5976949][-6.8717322 -5.7342744 -3.9512432 -1.8907948 0.11838198 1.7679739 2.8178892 3.356308 3.9307957 3.092926 1.0524406 -0.94233704 -2.6119637 -4.9822683 -6.2238889][-5.4971151 -4.246335 -2.7631884 -1.2655048 0.69826317 2.2008114 2.9743557 4.1074486 5.0130968 4.0941343 1.9006205 -0.32328367 -2.0700264 -4.2250204 -5.1006985][-5.2871761 -4.3423395 -3.2664022 -1.4960666 0.16767025 0.86116314 1.7420874 3.0392046 3.9625483 2.910533 0.51926517 -1.7512031 -3.4051347 -5.0966768 -5.5763664][-6.3348494 -5.3807068 -3.8959167 -2.3471293 -0.99670076 -0.43768024 0.04468441 1.0725794 1.6188898 0.3802557 -1.6364188 -3.3234048 -4.5862303 -6.0610237 -6.4513063][-8.59648 -7.8168664 -6.7556753 -5.1620622 -4.0229526 -3.5861526 -2.8717384 -2.2581487 -2.2168212 -3.10566 -4.7360868 -5.6893764 -6.5609274 -7.4619317 -7.6635532][-8.764637 -8.4165363 -7.6014972 -6.2811794 -5.3316555 -4.5136328 -3.7308784 -3.7128711 -3.6135049 -4.5119295 -5.9460273 -6.2616167 -6.7941685 -7.5988946 -7.2902236][-9.4220419 -9.2619734 -8.7939434 -7.9016962 -6.8904748 -5.9599781 -5.4115319 -5.2685804 -5.3847027 -5.9617534 -6.893312 -6.5244274 -6.4842005 -6.7195024 -6.234376][-8.32617 -8.3007679 -7.6435003 -7.0463881 -6.2772875 -5.5466728 -5.1249437 -5.2529726 -5.4267874 -5.8751636 -6.50592 -6.4778032 -6.3277173 -6.2929 -5.8727236][-7.7729945 -7.4587655 -6.8907876 -6.4687891 -5.9310203 -5.7678652 -5.6073503 -5.762454 -5.9921231 -6.1839495 -6.6956968 -6.9560132 -6.8272152 -6.6277509 -6.253541]]...]
INFO - root - 2017-12-15 22:57:55.501180: step 70710, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 47h:30m:22s remains)
INFO - root - 2017-12-15 22:58:01.839477: step 70720, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 46h:48m:53s remains)
INFO - root - 2017-12-15 22:58:08.274481: step 70730, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 46h:41m:20s remains)
INFO - root - 2017-12-15 22:58:14.749432: step 70740, loss = 0.28, batch loss = 0.17 (11.7 examples/sec; 0.684 sec/batch; 49h:45m:35s remains)
INFO - root - 2017-12-15 22:58:21.158533: step 70750, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 46h:38m:10s remains)
INFO - root - 2017-12-15 22:58:27.506304: step 70760, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 45h:11m:40s remains)
INFO - root - 2017-12-15 22:58:33.838895: step 70770, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.630 sec/batch; 45h:47m:29s remains)
INFO - root - 2017-12-15 22:58:40.282130: step 70780, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 47h:36m:03s remains)
INFO - root - 2017-12-15 22:58:46.658061: step 70790, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 47h:38m:30s remains)
INFO - root - 2017-12-15 22:58:53.022014: step 70800, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 46h:43m:42s remains)
2017-12-15 22:58:53.544073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0396042 -2.0342131 -2.0135603 -3.0225458 -4.184351 -5.4133949 -4.9349604 -4.1558013 -3.4698682 -3.1948977 -4.2580557 -5.3048024 -5.27423 -5.5564895 -5.4847651][-2.5798759 -2.5539484 -2.6444783 -2.7318115 -3.011373 -3.5476413 -3.6830378 -4.1004086 -4.249301 -4.7444196 -6.33962 -6.6321878 -6.2062578 -5.7672954 -5.576683][-5.3677368 -3.899292 -3.0764494 -3.0739298 -3.4852762 -3.2134881 -2.3026452 -3.0333905 -3.556746 -5.1216717 -7.073411 -7.6615334 -7.4687772 -7.0739837 -6.5298877][-6.0777407 -5.734046 -5.7162924 -3.9794064 -3.0023289 -1.8776073 -1.5012012 -1.5113387 -1.2548018 -2.470243 -4.6813483 -7.4498549 -7.5956759 -8.2251682 -7.5451589][-6.6933589 -6.3223815 -4.3942127 -3.5412517 -3.441659 -2.0793538 -1.2774091 -0.38010263 -0.019638062 -0.74023914 -2.8153453 -4.8809395 -6.6183295 -8.229166 -8.7689161][-3.8790605 -3.3194556 -2.7596459 -1.0404158 0.60614777 1.1773176 0.76308823 0.325428 0.90130806 0.36558056 -1.0631285 -2.8533344 -5.397296 -5.6259127 -6.6549225][-2.2448726 -2.570056 -1.9090896 -0.17521381 0.2957654 2.3427277 2.9391413 2.8827181 1.7440109 0.25089359 -1.4305801 -3.9901555 -4.1579237 -5.6513109 -6.9240646][-2.0909734 -1.8193278 -0.97919512 -0.46708775 0.15988064 1.4022932 1.8680639 1.7850399 1.3117628 0.213058 -2.1599722 -3.8597064 -5.2608175 -5.936655 -5.6478934][-2.9741135 -2.1207118 -1.3024659 -0.4735136 0.24808407 1.1967545 1.8946829 1.7806168 2.1765242 0.56341743 -2.6229157 -3.6277952 -4.9952869 -5.3914175 -6.3704958][-5.3770618 -4.4996939 -2.9181695 -2.9442711 -1.8412585 -0.6388669 -0.68791294 0.075824738 0.45330715 0.10708809 -1.9088888 -4.2645316 -5.7071772 -6.2821627 -6.7926974][-7.3465538 -6.5447369 -6.6962924 -5.9468231 -4.825006 -4.6032009 -4.1179609 -2.5072489 -2.8497977 -2.1141024 -3.9923747 -5.1218214 -5.25511 -6.8837333 -7.663909][-7.6060514 -7.8244991 -7.3363848 -7.2563024 -5.9891891 -6.0595932 -5.8947773 -5.9675679 -4.9351044 -3.8026183 -4.2724028 -5.1475587 -5.1720862 -6.0302892 -6.9286647][-7.2636318 -7.9162178 -8.0887 -7.7640638 -7.3891187 -7.0018497 -6.2584305 -6.49044 -7.0243707 -6.6494946 -6.4973192 -6.0231347 -5.3093677 -5.3846316 -5.5269594][-6.7756371 -7.3758769 -7.6411815 -7.2649727 -6.4126992 -5.5574102 -5.4170542 -5.435564 -5.9003134 -5.4294262 -6.5628786 -6.5062485 -6.943862 -6.5462623 -5.8085709][-6.5173578 -6.3476286 -6.5252438 -7.2658882 -7.1911364 -6.6416483 -6.3400173 -5.9231873 -5.8518486 -7.0485344 -7.42536 -8.299984 -8.1306305 -7.600606 -6.9789381]]...]
INFO - root - 2017-12-15 22:58:59.859993: step 70810, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 45h:46m:44s remains)
INFO - root - 2017-12-15 22:59:06.290868: step 70820, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 46h:19m:07s remains)
INFO - root - 2017-12-15 22:59:12.763938: step 70830, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 47h:41m:47s remains)
INFO - root - 2017-12-15 22:59:19.216819: step 70840, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 45h:51m:00s remains)
INFO - root - 2017-12-15 22:59:25.697726: step 70850, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 47h:53m:04s remains)
INFO - root - 2017-12-15 22:59:32.129464: step 70860, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 46h:09m:53s remains)
INFO - root - 2017-12-15 22:59:38.479785: step 70870, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 45h:48m:49s remains)
INFO - root - 2017-12-15 22:59:44.903388: step 70880, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 47h:27m:21s remains)
INFO - root - 2017-12-15 22:59:51.202762: step 70890, loss = 0.32, batch loss = 0.20 (12.1 examples/sec; 0.660 sec/batch; 47h:58m:41s remains)
INFO - root - 2017-12-15 22:59:57.563948: step 70900, loss = 0.33, batch loss = 0.22 (13.0 examples/sec; 0.617 sec/batch; 44h:48m:32s remains)
2017-12-15 22:59:58.058152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4461317 -7.1096344 -6.9350891 -7.2538314 -7.5130553 -6.8639517 -5.6767187 -4.6283674 -3.8082604 -4.4030342 -4.847609 -6.0210452 -6.7983909 -7.9745488 -8.4792137][-7.8947744 -8.0311766 -7.8936133 -7.6080685 -7.465836 -7.0431318 -6.3078127 -5.3046188 -4.1761427 -4.3520613 -4.4871078 -5.73279 -6.2617035 -6.8368254 -6.9762783][-7.5059175 -7.6614051 -8.0888844 -8.4168177 -8.0311584 -6.768198 -5.5498395 -4.61518 -3.6763215 -4.1166754 -4.3697243 -5.2906313 -5.6343832 -6.0837264 -6.0332537][-7.438724 -7.199203 -7.3076634 -6.8159456 -5.9982209 -5.0451689 -3.7887707 -2.8758135 -2.0631876 -3.2902637 -4.3455391 -5.7556658 -6.1459727 -6.2786303 -6.1344833][-6.9248967 -6.458344 -6.4167848 -5.6467209 -4.4388924 -2.6520009 -1.5386825 -1.1119409 -0.87302494 -2.3892784 -3.4582863 -5.3798304 -6.4141183 -6.8143735 -6.7032685][-6.2027273 -5.8590717 -5.4602928 -4.0203524 -2.4803309 -0.53874254 0.80305004 1.0284119 0.62313461 -1.2543964 -2.5865202 -4.6372304 -5.53074 -6.2064934 -6.5436878][-6.41002 -5.4329605 -4.2702832 -2.6409192 -1.1838422 0.66549683 2.1100502 2.1947374 1.7068167 -0.63416338 -2.4105196 -4.3638649 -5.4178243 -6.4052258 -6.637167][-5.6515427 -4.8339405 -3.7531254 -2.0007792 -0.31241417 1.1491385 2.2166748 2.3315868 1.9912176 -0.36350584 -2.1664453 -4.342196 -5.5353336 -6.4061255 -6.4575925][-5.5250196 -4.8237152 -3.8196971 -2.480165 -1.021996 0.49409676 1.6749706 1.6560631 1.4027185 -0.36619949 -2.0507102 -4.4209538 -5.3785787 -6.365591 -6.7644582][-5.6974635 -5.0264015 -3.9000077 -2.6679058 -1.4093018 -0.38119602 0.060175896 0.0083608627 0.21574926 -1.2762942 -2.6614413 -4.23574 -4.9617286 -6.1480036 -6.8195992][-5.1891851 -4.8248043 -4.0528927 -3.4612508 -2.5619969 -1.6942182 -1.0298595 -0.76745987 -0.5811739 -2.2079816 -3.1235075 -4.1841555 -5.1882982 -5.9258375 -6.6816225][-5.2004137 -4.8412271 -4.3361235 -4.0316095 -3.8307939 -2.9059787 -2.1817822 -1.7560596 -1.5377059 -2.5196104 -3.0502567 -4.2021918 -5.3955297 -6.0271034 -6.7700272][-6.2658205 -5.7506766 -5.0156097 -4.4871411 -3.9236062 -3.5929275 -3.3851991 -3.0376935 -2.5855479 -2.8480587 -3.554369 -4.117136 -4.9665456 -6.0243378 -6.5717969][-6.1732445 -6.0405264 -5.5849195 -5.27363 -5.1960211 -4.9909563 -4.6790271 -4.1584167 -3.7055392 -3.6976461 -3.6519632 -4.1775408 -5.2006574 -5.5409174 -6.0248322][-5.7844138 -5.845355 -5.7215185 -5.9452915 -6.1850367 -6.2041936 -6.2181063 -5.7257996 -5.2141018 -5.0040846 -4.9357653 -5.0944157 -5.2905908 -5.5014987 -5.806602]]...]
INFO - root - 2017-12-15 23:00:04.432674: step 70910, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 46h:28m:30s remains)
INFO - root - 2017-12-15 23:00:10.868793: step 70920, loss = 0.24, batch loss = 0.12 (12.9 examples/sec; 0.619 sec/batch; 44h:59m:45s remains)
INFO - root - 2017-12-15 23:00:17.201038: step 70930, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 47h:50m:53s remains)
INFO - root - 2017-12-15 23:00:23.614923: step 70940, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:05m:44s remains)
INFO - root - 2017-12-15 23:00:29.981760: step 70950, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 47h:11m:26s remains)
INFO - root - 2017-12-15 23:00:36.429810: step 70960, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 46h:10m:47s remains)
INFO - root - 2017-12-15 23:00:42.923962: step 70970, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 48h:27m:08s remains)
INFO - root - 2017-12-15 23:00:49.402777: step 70980, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 45h:54m:39s remains)
INFO - root - 2017-12-15 23:00:55.813562: step 70990, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 46h:58m:13s remains)
INFO - root - 2017-12-15 23:01:02.241634: step 71000, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 45h:00m:03s remains)
2017-12-15 23:01:02.784819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5339661 -5.9180136 -5.7889085 -5.7835865 -5.9606705 -5.8772449 -4.7592459 -4.1758204 -3.39955 -4.0674725 -5.222168 -5.0592456 -5.8918018 -6.5932536 -7.8006144][-5.2877235 -5.0510712 -4.9138136 -5.2995877 -5.9233708 -6.3159995 -5.5697489 -4.835391 -3.8291965 -4.6360655 -5.696156 -5.6756253 -7.20161 -7.1500235 -8.36857][-4.6958756 -4.68183 -4.825799 -4.882699 -5.2760129 -5.6312432 -5.2194147 -4.5569887 -3.8760278 -4.9826574 -6.2421684 -6.4074097 -7.5454545 -8.2759628 -9.3189087][-5.6896133 -4.7397575 -4.6572313 -4.4334464 -4.3184566 -4.2178793 -3.5989103 -3.3411465 -2.6753488 -4.0552945 -5.805212 -5.9497819 -7.2123418 -7.5344582 -9.1135988][-6.0558004 -4.6448584 -3.1881356 -2.4924855 -2.1160307 -1.5346847 -0.54866219 -0.041752815 0.58847618 -1.1095495 -3.1600828 -3.9671555 -5.80193 -6.6368752 -8.3118277][-5.561533 -4.2656717 -3.2363124 -1.4768553 0.24398756 1.2330904 2.5298758 2.7444143 3.6346655 1.7957783 -1.1727586 -2.3424497 -4.3610735 -5.3167963 -7.4858084][-5.1910658 -4.1365509 -2.1745319 -0.37289238 1.6872263 3.2933216 4.9541922 5.1112709 5.4729633 3.2649307 0.66006756 -0.84106636 -3.1697812 -4.1440887 -6.2938948][-5.154532 -4.2069955 -2.533309 0.30529261 3.2506437 4.1682453 5.861599 6.7127457 7.0209665 4.0898876 0.747282 -0.98323917 -3.1512661 -4.0217113 -5.7732277][-5.645978 -5.2793784 -4.1311207 -1.8653126 1.0284147 2.778985 4.032526 4.4251575 5.0575542 2.8710756 -0.44720411 -2.1746798 -4.2002711 -4.784246 -6.1450958][-7.6784453 -6.8636875 -5.4912529 -4.3634548 -1.9845352 0.050686836 1.1320086 0.97767067 0.52859116 -1.3473563 -3.5109229 -4.7072811 -6.5162067 -6.6603169 -7.34052][-9.9355659 -9.4798927 -8.5184813 -7.3226404 -5.8135381 -4.7478933 -2.92949 -2.4104495 -3.1588907 -5.4873066 -7.3910255 -7.6701984 -9.1880836 -9.1120586 -9.381609][-11.552573 -10.616928 -9.4350967 -8.8420305 -8.0673933 -7.1243372 -6.1026564 -5.7355804 -4.9608555 -6.8311014 -8.5810938 -8.2281313 -8.8040514 -8.5764532 -9.1267567][-10.769911 -10.511997 -10.369081 -9.0930014 -8.420763 -8.0208025 -7.2081366 -7.1818357 -6.9223242 -7.3979664 -8.35465 -8.0026255 -8.6547365 -7.6717582 -7.85479][-8.608942 -8.5542421 -8.3129005 -7.9548755 -7.4969316 -6.9026775 -6.7848268 -7.0310607 -6.858119 -7.5538859 -8.33697 -8.1533518 -8.1550217 -6.9536405 -6.481472][-7.1721926 -6.9150553 -6.9727454 -6.6120658 -6.8844457 -6.93052 -6.33947 -7.0961123 -7.4379711 -7.2555304 -7.4915891 -7.1912966 -7.1638288 -6.9119267 -6.3189864]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:01:09.269299: step 71010, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:12m:55s remains)
INFO - root - 2017-12-15 23:01:15.763070: step 71020, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 47h:24m:12s remains)
INFO - root - 2017-12-15 23:01:22.135217: step 71030, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 45h:50m:49s remains)
INFO - root - 2017-12-15 23:01:28.496927: step 71040, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 46h:03m:44s remains)
INFO - root - 2017-12-15 23:01:34.929953: step 71050, loss = 0.36, batch loss = 0.24 (12.2 examples/sec; 0.655 sec/batch; 47h:32m:06s remains)
INFO - root - 2017-12-15 23:01:41.371299: step 71060, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 46h:26m:54s remains)
INFO - root - 2017-12-15 23:01:47.768738: step 71070, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 48h:44m:51s remains)
INFO - root - 2017-12-15 23:01:54.174870: step 71080, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 46h:11m:25s remains)
INFO - root - 2017-12-15 23:02:00.580405: step 71090, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:03m:05s remains)
INFO - root - 2017-12-15 23:02:06.970531: step 71100, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 46h:34m:19s remains)
2017-12-15 23:02:07.551509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5010214 -5.7128153 -6.4546647 -6.2528658 -4.8082581 -3.3639913 -2.609457 -2.147274 -2.0454893 -3.4077039 -4.7111273 -6.2389431 -7.1684427 -7.215127 -7.6964726][-3.8448656 -4.6161556 -6.3151813 -6.6599135 -5.85 -5.3227816 -4.0838604 -3.4138136 -3.3371086 -4.7367859 -5.8060513 -7.053215 -7.7013426 -7.9370193 -8.2068682][-4.1449137 -4.85098 -5.4329834 -5.1404991 -4.7532725 -4.6202326 -3.986089 -4.0889788 -3.6419349 -4.7187738 -5.72896 -6.7442417 -7.52135 -7.4877305 -7.9881697][-4.109045 -3.540493 -4.0685797 -3.0731602 -2.0043383 -1.867938 -1.3911676 -1.7510967 -2.2027044 -3.6872721 -4.7009125 -5.9715266 -7.0919876 -7.1478925 -7.5357618][-4.5431962 -2.697722 -2.1899118 -1.5177169 -0.25067377 0.44188881 0.52105141 0.011027336 -0.54404306 -2.4679914 -3.8818233 -5.0342455 -6.2748942 -6.8617377 -7.3334436][-4.2967129 -2.8422837 -1.7399688 -0.32373333 0.48796368 1.1525059 1.6236391 0.8322382 0.043186188 -1.4857907 -2.72962 -4.430428 -5.6896896 -6.1915236 -6.7805538][-5.115592 -3.6341033 -2.2646718 -0.48924255 1.0545321 1.6714897 2.3434896 1.5966816 0.69874763 -1.3019323 -2.7454305 -4.3977585 -5.7908506 -6.2701979 -6.8615112][-5.432168 -4.4191589 -2.9867501 -0.70310545 0.59343624 1.5744896 2.4050779 1.9532604 1.0843172 -1.5623379 -3.5734024 -5.3875961 -6.4342146 -6.5816975 -6.916409][-5.6547761 -5.1091089 -4.4896269 -2.5438566 -1.5650291 -0.74326134 0.51122761 1.0266304 0.99808979 -1.949708 -4.1395559 -6.113873 -7.0475936 -6.8437338 -7.1419086][-6.7413464 -6.211791 -5.838748 -4.7167511 -3.4749928 -2.4151769 -1.6166883 -1.1616855 -0.77214432 -3.1021094 -4.7357454 -6.449111 -7.8819318 -7.9077907 -8.2321978][-8.5562019 -8.1996775 -8.0024433 -7.1268492 -6.0158892 -5.0797863 -4.3308635 -4.2530704 -4.2860122 -6.1707449 -6.9966884 -7.7436652 -8.339798 -8.6791849 -8.8913612][-9.3915806 -8.7442741 -8.4450006 -8.0595779 -7.4041452 -6.9114094 -6.4331059 -6.352561 -6.2862034 -7.584054 -7.982111 -8.0843077 -7.8379736 -7.9216223 -8.5601978][-10.534461 -9.9638958 -9.4965477 -8.9891148 -8.2068892 -7.8832808 -7.4665651 -7.4911776 -7.2532625 -7.8011088 -8.230629 -7.7953882 -7.3010678 -7.2189074 -7.2336097][-10.280317 -9.6709623 -8.9867973 -8.38633 -7.872963 -7.6758494 -7.0618806 -7.1472707 -7.2416162 -7.3625937 -7.4024777 -7.2084522 -6.686419 -6.2617788 -6.1240416][-8.9600658 -9.2389469 -9.4407034 -9.4109268 -9.2245426 -8.7520409 -7.7039833 -7.3738127 -7.1604714 -7.4560938 -7.2511883 -7.0946603 -6.4263 -6.1903205 -6.4338255]]...]
INFO - root - 2017-12-15 23:02:13.946808: step 71110, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 46h:04m:48s remains)
INFO - root - 2017-12-15 23:02:20.301413: step 71120, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 46h:36m:28s remains)
INFO - root - 2017-12-15 23:02:26.756959: step 71130, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 46h:56m:49s remains)
INFO - root - 2017-12-15 23:02:33.239285: step 71140, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 47h:16m:57s remains)
INFO - root - 2017-12-15 23:02:39.713544: step 71150, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 47h:18m:52s remains)
INFO - root - 2017-12-15 23:02:46.150154: step 71160, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 47h:02m:17s remains)
INFO - root - 2017-12-15 23:02:52.535242: step 71170, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 45h:36m:49s remains)
INFO - root - 2017-12-15 23:02:58.937051: step 71180, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 47h:45m:16s remains)
INFO - root - 2017-12-15 23:03:05.314869: step 71190, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 45h:38m:47s remains)
INFO - root - 2017-12-15 23:03:11.718846: step 71200, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:08m:28s remains)
2017-12-15 23:03:12.294737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.564302 -3.50805 -3.3557482 -3.1287622 -3.3741684 -3.4007568 -2.7691622 -2.2881413 -1.530211 -2.6921968 -3.8143106 -3.9293685 -4.6696663 -6.5388508 -8.2360773][-3.2852082 -3.3093204 -2.8310804 -2.7596326 -3.3024406 -3.5836358 -3.186512 -2.6096044 -1.9913368 -3.0659533 -3.2450266 -3.8335819 -5.4912891 -6.515543 -6.9872003][-3.8928273 -3.0371675 -3.0742197 -3.045701 -3.1481953 -3.1987462 -2.8685751 -2.5630374 -2.2699008 -3.5664477 -4.1084442 -4.5266027 -5.137 -6.3914571 -7.5878296][-4.6485672 -3.5341873 -2.5639033 -2.2807894 -2.175683 -1.6151805 -1.1018233 -1.0521598 -1.3095188 -3.0584145 -4.1208572 -4.4747849 -5.0935454 -6.2868023 -7.0950942][-5.0924635 -3.6479821 -2.5852275 -2.2022452 -1.698698 -1.3559284 -0.54925919 -0.31232834 -0.22036505 -1.6415157 -3.204011 -4.2287912 -5.2829523 -6.3531036 -7.2939057][-4.7036891 -3.8095279 -2.6312962 -1.5658092 -0.561038 0.39154148 0.95955944 0.70647812 0.67656136 -0.63875961 -1.881454 -3.099576 -4.5171194 -5.3681078 -6.3860273][-4.1414347 -2.8035617 -1.8095655 -0.46468163 1.2071266 2.0879936 2.8705111 2.7357512 2.5722246 0.64378071 -0.84423828 -2.2543855 -3.726892 -5.1195011 -6.1331725][-3.0605116 -2.3004804 -1.4444218 0.065103054 1.4620171 2.2828941 3.4509115 3.4771976 3.4905148 2.2319174 0.5342083 -1.527163 -2.9188037 -4.4114237 -5.8675995][-3.8118489 -3.103776 -1.7546134 -0.34656429 0.6422205 1.2625542 2.1641169 2.7095346 3.2545595 2.014432 -0.0029335022 -1.6550488 -3.1328573 -4.6613913 -5.7032413][-4.2515845 -3.3061886 -2.750598 -1.2716784 0.308465 0.83240128 1.5147057 1.6593075 1.8834858 0.37740993 -1.1023164 -2.3954415 -4.0770149 -5.3184481 -5.6236615][-6.1955442 -5.3667278 -4.632741 -3.3428693 -2.3206477 -1.2629213 -0.72640514 -0.807662 -0.683331 -1.9164529 -3.2171984 -4.1148081 -5.2815914 -6.4585066 -6.8567123][-8.3277636 -6.3988762 -5.5971556 -4.9372158 -4.5893993 -3.824317 -3.3132524 -3.015914 -2.665875 -3.5223885 -4.448473 -4.8112512 -5.6405716 -6.8953052 -7.122138][-8.9604216 -8.3029184 -7.511198 -6.1629882 -5.5362835 -4.9768219 -4.4502048 -4.1668849 -4.4180293 -4.81269 -5.3250742 -5.7888832 -6.3220034 -6.6347251 -6.6464758][-9.84873 -8.97331 -7.9955373 -7.2963033 -6.5250273 -5.5616741 -5.116312 -5.4948053 -5.618125 -5.6941385 -5.8877029 -6.0195103 -6.362783 -6.6368809 -6.2760863][-9.3749371 -8.8945856 -8.9104757 -8.1420813 -7.4787707 -6.7758102 -6.4873004 -6.407752 -6.3822145 -6.7761831 -6.8494053 -6.6790414 -6.670877 -6.5232296 -6.2571263]]...]
INFO - root - 2017-12-15 23:03:18.782526: step 71210, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 46h:59m:36s remains)
INFO - root - 2017-12-15 23:03:25.131874: step 71220, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 46h:12m:58s remains)
INFO - root - 2017-12-15 23:03:31.534015: step 71230, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 46h:16m:24s remains)
INFO - root - 2017-12-15 23:03:37.946144: step 71240, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:21m:15s remains)
INFO - root - 2017-12-15 23:03:44.437167: step 71250, loss = 0.34, batch loss = 0.23 (12.4 examples/sec; 0.645 sec/batch; 46h:47m:23s remains)
INFO - root - 2017-12-15 23:03:50.920280: step 71260, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:49m:00s remains)
INFO - root - 2017-12-15 23:03:57.230148: step 71270, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 45h:58m:56s remains)
INFO - root - 2017-12-15 23:04:03.552241: step 71280, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 46h:26m:28s remains)
INFO - root - 2017-12-15 23:04:09.950698: step 71290, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 46h:23m:08s remains)
INFO - root - 2017-12-15 23:04:16.339428: step 71300, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 46h:49m:55s remains)
2017-12-15 23:04:16.877094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.80871 -4.1060114 -5.1008453 -5.575388 -5.8499756 -6.4654064 -6.4350381 -5.5710168 -4.3796883 -3.6769152 -4.450387 -5.789197 -6.63198 -6.4740977 -6.6675167][-3.2616587 -3.613904 -4.7785168 -5.7271094 -6.2288866 -5.846674 -5.3176632 -4.8500328 -4.233192 -3.9650431 -4.9365211 -6.413672 -7.340158 -7.1303396 -7.0886869][-3.0381784 -3.2305584 -4.0140533 -4.2123384 -4.3325753 -4.2139258 -3.998369 -3.4632168 -3.2634978 -3.6893864 -4.9263754 -6.5065861 -7.3505197 -7.6968141 -7.8722949][-3.3536553 -3.5266356 -3.6958168 -3.6776104 -3.2725625 -2.8272672 -2.4934816 -2.3849044 -1.9833093 -2.0127902 -3.1898623 -5.2760139 -6.5163727 -6.9926977 -7.4721675][-3.537869 -3.8325884 -3.8683341 -3.1108627 -2.0835567 -0.81191778 -0.14884186 -0.33018351 -1.5105357 -2.1401916 -3.2835956 -5.0559082 -6.021481 -6.5054483 -7.0228424][-4.8373718 -4.8915138 -4.4215374 -3.1474395 -1.4981203 0.38338375 1.5336266 1.6692696 0.61957455 -0.96108389 -3.6537361 -5.5738497 -6.227757 -6.2204685 -6.4097958][-5.0192285 -4.8120308 -4.0016155 -2.5163069 -0.098017216 1.9646692 3.1221933 3.2060251 2.7292137 1.0858736 -2.3253527 -5.1277475 -6.176425 -5.8738976 -5.7810225][-4.4658675 -4.1821089 -3.3887615 -1.9021573 0.62223148 3.0043507 4.7476206 4.3441162 3.4179277 1.5161715 -1.620213 -4.4872208 -5.965291 -6.0742049 -5.8724389][-5.3829212 -4.4097376 -3.2470293 -2.0932713 -0.40975761 1.3503628 3.4223223 3.7620831 2.9406853 0.78971577 -2.167079 -4.9177752 -6.1910076 -6.1772575 -6.5147285][-6.6940379 -6.54923 -5.9604163 -4.5130615 -2.9070067 -1.3282499 0.2855711 0.9078989 1.2146473 -0.63256121 -3.8517315 -6.423965 -7.6864476 -7.3671455 -7.7091026][-8.1064034 -7.6110916 -7.2686539 -6.2642841 -5.0873451 -3.9743507 -2.6062498 -1.9725223 -1.7284489 -2.4375081 -4.1411324 -6.4854517 -7.6544762 -7.4889379 -7.6102571][-7.9154849 -7.85275 -7.4746461 -7.0435009 -6.4282937 -5.6110458 -4.4766684 -3.899796 -3.9330249 -4.5207 -5.2544489 -6.5216732 -7.1190872 -7.3449941 -7.4723034][-8.7012663 -7.9962006 -7.7622452 -7.1971745 -7.275269 -7.0565 -6.8553357 -6.4098835 -5.813221 -6.321187 -6.7258286 -7.7081041 -7.9022164 -7.8728523 -7.550478][-9.2679977 -8.7003212 -7.830328 -6.9065738 -6.7020292 -6.4067545 -6.7590542 -7.3229661 -7.3678145 -7.55872 -7.0275106 -7.0822959 -7.0182805 -6.7757921 -6.6001191][-9.9749165 -9.6902876 -9.1610985 -8.4806662 -7.6733813 -7.2249804 -7.111093 -7.3132582 -7.7481265 -7.9446583 -7.7825789 -7.2591267 -6.4965796 -5.98364 -5.7570977]]...]
INFO - root - 2017-12-15 23:04:23.282048: step 71310, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 47h:43m:56s remains)
INFO - root - 2017-12-15 23:04:29.652637: step 71320, loss = 0.32, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 48h:03m:08s remains)
INFO - root - 2017-12-15 23:04:36.073666: step 71330, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 46h:30m:52s remains)
INFO - root - 2017-12-15 23:04:42.408416: step 71340, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.632 sec/batch; 45h:52m:57s remains)
INFO - root - 2017-12-15 23:04:48.767640: step 71350, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 44h:47m:30s remains)
INFO - root - 2017-12-15 23:04:55.148889: step 71360, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 46h:42m:16s remains)
INFO - root - 2017-12-15 23:05:01.531776: step 71370, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 45h:37m:48s remains)
INFO - root - 2017-12-15 23:05:07.889157: step 71380, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 46h:32m:10s remains)
INFO - root - 2017-12-15 23:05:14.279690: step 71390, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.617 sec/batch; 44h:44m:18s remains)
INFO - root - 2017-12-15 23:05:20.736155: step 71400, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 46h:42m:47s remains)
2017-12-15 23:05:21.251848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4125791 -1.8159838 -2.3914685 -2.8044081 -3.3575139 -3.6553798 -3.7583251 -3.4437943 -3.1440153 -3.9867964 -4.736495 -5.3215876 -5.8014393 -6.602459 -7.0298767][-2.0216475 -2.3386359 -2.8391337 -3.2805314 -3.8130221 -3.8138702 -3.9062896 -3.896646 -3.8011177 -4.6152077 -5.0883164 -5.7961907 -6.5563946 -7.3749771 -7.7520289][-2.2961688 -2.193985 -2.3471775 -2.5260172 -2.7148643 -2.8005981 -2.9049978 -2.79991 -2.7182178 -3.6416717 -4.2237325 -4.8342867 -5.4513025 -6.2323394 -6.8274117][-2.1664062 -2.0420651 -1.9758115 -1.8538985 -1.959126 -1.7440038 -1.5957422 -1.5762372 -1.6299796 -2.7714152 -3.7282965 -4.6285343 -5.3957939 -6.1039619 -6.3344426][-2.8011084 -2.4217377 -2.2094955 -1.8726602 -1.4671502 -0.87708855 -0.41758966 -0.27329969 -0.30812311 -1.4285707 -2.4510307 -3.5114794 -4.314187 -5.29252 -5.7826262][-3.5616074 -3.0238924 -2.2483602 -1.7514892 -1.2136183 -0.34706259 0.23667479 0.39044476 0.47855663 -0.6901269 -1.7352638 -2.5255218 -3.1901922 -4.4117355 -5.2480927][-3.4083176 -2.9473472 -2.3281355 -1.3894277 -0.52521133 0.20529938 0.75590992 1.2572575 1.4920387 0.35950851 -0.56062269 -1.4516296 -2.3956838 -3.8845677 -4.9939733][-3.1739979 -2.6995568 -1.9795694 -1.1050839 -0.1494298 0.757988 1.3243904 1.7287359 2.1580954 1.1138887 0.028723717 -0.95870781 -2.0361023 -3.5484982 -4.5235219][-3.062993 -2.3852549 -1.9353809 -1.2199297 -0.40233278 0.49628258 1.0592508 1.3964033 1.8272982 0.96091175 -0.023436546 -1.3578553 -2.6214447 -4.205164 -5.0260973][-3.3936543 -2.7101254 -2.0696659 -1.2793136 -0.53689241 0.1507206 0.52030373 0.85439968 1.3192978 0.084121704 -1.0622373 -2.3308759 -3.6224017 -4.8241806 -5.6093569][-4.398736 -3.6628981 -3.1096444 -2.5249176 -1.9454098 -1.4536786 -1.1301513 -0.94457293 -0.66597795 -2.0212731 -3.120914 -3.951103 -4.7898846 -5.8181114 -6.2096572][-5.1134248 -4.5900936 -4.0572004 -3.6839018 -3.3092341 -3.0082273 -2.8375263 -2.8160048 -2.5804405 -3.6237025 -4.5977988 -5.2374034 -5.8413553 -6.26502 -6.4050555][-5.7300725 -5.4816036 -5.1216831 -4.6747122 -4.3121605 -4.08909 -4.0620384 -3.9652243 -3.8833356 -4.6439867 -5.4084635 -5.7141562 -5.99757 -6.1511221 -5.9229975][-6.2769351 -5.5133886 -5.1037445 -4.8950872 -4.5514145 -4.4217691 -4.455409 -4.5418177 -4.7019196 -5.4413881 -5.77179 -5.718966 -5.7143078 -5.813714 -5.6002316][-6.9691834 -6.653945 -6.1748991 -5.7039995 -5.4995527 -5.3740053 -5.4449892 -5.5048723 -5.5810194 -5.8008447 -6.0561996 -6.1983409 -6.29137 -6.1134691 -6.0060673]]...]
INFO - root - 2017-12-15 23:05:27.576823: step 71410, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 45h:54m:27s remains)
INFO - root - 2017-12-15 23:05:33.931069: step 71420, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 46h:45m:20s remains)
INFO - root - 2017-12-15 23:05:40.310979: step 71430, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.613 sec/batch; 44h:28m:52s remains)
INFO - root - 2017-12-15 23:05:46.686393: step 71440, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 47h:08m:56s remains)
INFO - root - 2017-12-15 23:05:53.215340: step 71450, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 47h:52m:03s remains)
INFO - root - 2017-12-15 23:05:59.616126: step 71460, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:25m:28s remains)
INFO - root - 2017-12-15 23:06:05.976099: step 71470, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 45h:50m:50s remains)
INFO - root - 2017-12-15 23:06:12.463426: step 71480, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 47h:14m:46s remains)
INFO - root - 2017-12-15 23:06:18.937922: step 71490, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.615 sec/batch; 44h:35m:07s remains)
INFO - root - 2017-12-15 23:06:25.366022: step 71500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 46h:15m:57s remains)
2017-12-15 23:06:25.862166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8484926 -5.1699553 -4.3748183 -4.4938831 -4.4422574 -4.1185522 -3.6669397 -2.8263593 -2.06041 -3.1782527 -4.04844 -4.186964 -4.3321552 -4.7787228 -5.3530188][-4.3287115 -4.51245 -4.4034042 -4.5955262 -4.8574729 -4.703598 -4.3768678 -3.5029426 -2.4262471 -3.1336746 -3.6533871 -3.9842844 -4.7367277 -5.1655116 -5.3869781][-4.253377 -3.7403598 -3.8259687 -3.8451545 -3.8646309 -3.6347046 -3.3482704 -3.3238449 -2.6815753 -3.8421147 -4.6030283 -4.7757363 -5.0651875 -5.123805 -5.6214409][-3.53541 -2.5435324 -2.153162 -1.7528796 -1.6713681 -1.6433082 -1.2885232 -0.88832474 0.16208076 -1.8923078 -3.1566782 -4.0175915 -4.7357206 -5.888598 -6.711359][-1.8339329 -0.88861084 -0.57330942 -0.014268398 0.40583515 1.1521101 1.9452868 2.162714 2.56248 0.35650253 -1.2416019 -2.8693109 -4.5342383 -5.7539539 -6.691216][-1.8532791 -0.76587963 0.17074919 1.7583542 2.8158865 3.4694643 4.4787397 4.7195911 4.7277708 2.4131689 -0.12110615 -2.1723237 -3.8748627 -5.0051265 -6.214438][-2.2167726 -0.3966341 1.5791807 3.6376266 5.1293488 6.1961813 7.23063 6.8820486 6.8180742 4.602704 1.7282963 -0.049930573 -1.9358621 -3.6479726 -5.0511732][-0.35546827 0.83825111 2.1836052 4.4686432 6.2164965 7.6956825 8.7210379 8.6101713 8.5027761 5.6022243 2.0133877 -0.19166183 -2.1414933 -3.4379182 -4.6515408][-0.299294 0.82531929 1.7656908 3.110816 4.784833 5.7890272 6.5698996 6.9403067 6.921896 4.715189 1.4480429 -0.71477175 -2.9233623 -4.3509378 -5.0792789][-1.3774204 -0.636601 0.22762251 1.5902653 3.1354895 3.7798204 4.1249523 4.0431108 3.8661947 2.3170137 0.42295837 -1.6105976 -3.7183559 -4.5271797 -5.3883066][-4.3057051 -3.4412265 -2.0007625 -1.0475202 -0.054315567 0.83827877 1.4445114 0.97114849 0.79024029 -0.52979946 -2.0480781 -3.35001 -4.2728128 -4.8753166 -5.7505512][-6.2667012 -5.7152548 -5.0384045 -4.0581341 -2.9314947 -2.5471306 -2.5135937 -2.7804255 -2.7995305 -3.3200498 -4.0731306 -4.5411892 -5.010354 -5.3524146 -6.1016364][-7.1285477 -6.7969117 -6.203918 -5.7669544 -5.2309127 -4.5540476 -4.3101 -4.6455622 -4.6341619 -4.9057751 -5.5489492 -5.6204052 -5.6629944 -5.2191658 -5.590064][-8.6332588 -7.6732736 -7.5499682 -7.0735054 -6.4795585 -6.0303855 -5.6849985 -5.6591835 -5.3685913 -5.6342716 -5.9368219 -5.90425 -6.1496868 -5.7259965 -5.7760949][-9.9789009 -8.5568209 -7.7810049 -7.8425264 -7.5280962 -7.3215909 -7.6116853 -7.8999867 -7.526732 -7.07903 -6.7872877 -6.4416571 -5.9816556 -5.5704327 -5.5777321]]...]
INFO - root - 2017-12-15 23:06:32.206605: step 71510, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 45h:51m:25s remains)
INFO - root - 2017-12-15 23:06:38.611207: step 71520, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 45h:38m:13s remains)
INFO - root - 2017-12-15 23:06:44.939296: step 71530, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 45h:19m:35s remains)
INFO - root - 2017-12-15 23:06:51.265410: step 71540, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 44h:45m:36s remains)
INFO - root - 2017-12-15 23:06:57.653129: step 71550, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:05m:22s remains)
INFO - root - 2017-12-15 23:07:04.041719: step 71560, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 45h:46m:28s remains)
INFO - root - 2017-12-15 23:07:10.446074: step 71570, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 45h:37m:23s remains)
INFO - root - 2017-12-15 23:07:16.853137: step 71580, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 45h:29m:44s remains)
INFO - root - 2017-12-15 23:07:23.178413: step 71590, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 45h:39m:34s remains)
INFO - root - 2017-12-15 23:07:29.590782: step 71600, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 44h:59m:09s remains)
2017-12-15 23:07:30.133644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9374156 -3.0275717 -2.8234129 -3.1001925 -3.2548404 -2.744369 -2.1014256 -1.993958 -1.8624353 -2.317668 -2.4728494 -3.3261867 -4.4285975 -6.0654449 -6.8911109][-4.4062147 -4.130228 -4.6489797 -4.7220297 -4.5131903 -4.2818327 -3.8270366 -3.0053225 -2.0598845 -2.7030611 -3.5382109 -4.9361315 -6.1091957 -7.1328044 -7.2636747][-4.8488512 -4.1205587 -4.0762386 -4.2665424 -4.339457 -4.1899424 -3.5553088 -2.9482131 -2.2895479 -2.6523781 -2.9948287 -4.0642776 -5.2610207 -6.930481 -7.7587256][-5.160675 -4.8641882 -4.7763872 -3.9504497 -2.9821544 -2.1957002 -1.4481163 -1.0872974 -0.65391874 -1.4699211 -2.2884636 -3.9788706 -5.3426456 -6.5831327 -6.9929519][-6.3252058 -5.1928754 -4.1999583 -3.4693389 -2.5038419 -1.0640311 0.20359707 0.68468857 0.79256916 -0.6884923 -1.8414345 -3.6407266 -5.2599616 -7.0047851 -7.5973234][-6.5538545 -6.0525656 -5.3373423 -3.6626043 -2.0354872 -0.097224712 1.517993 1.9800577 2.2873468 0.67309189 -0.78564453 -3.1092782 -5.2074251 -6.8887715 -7.6102428][-6.7725267 -5.7112236 -4.6560054 -3.1003308 -1.1569738 0.91951847 2.4194403 3.1047363 3.2845516 1.2080717 -0.40539742 -2.7293067 -4.7270966 -6.7632084 -7.6132441][-6.195025 -5.209518 -4.0312376 -1.9623766 -0.025037766 2.0057087 3.4131384 3.9808235 4.0133352 1.7333174 -0.208601 -2.9541183 -5.0959415 -6.8542442 -7.3597188][-6.41407 -5.4843769 -4.4929848 -2.9097548 -1.2808275 0.38721466 1.5853529 2.0937605 2.2381563 0.29688072 -1.356473 -3.5781398 -5.4968586 -7.1142521 -7.6761041][-6.3615851 -6.0546703 -5.3592339 -4.0376539 -2.2573385 -0.85874176 -0.19632626 0.11418009 0.14553642 -1.3592486 -2.8284812 -4.932559 -6.2734537 -7.4381132 -7.5217667][-7.3537617 -7.1964126 -6.6715903 -5.557725 -4.5383749 -3.0457287 -1.9435763 -1.9405751 -1.9269814 -3.2726536 -4.44477 -5.9556036 -7.07374 -7.9978571 -7.8446383][-8.6671934 -8.400631 -8.1140051 -7.3601966 -6.29207 -5.17299 -4.6195707 -4.3143396 -4.0706711 -5.0606523 -5.8179717 -6.6719713 -7.3315392 -7.7699423 -7.4208288][-8.5670261 -8.474062 -8.304265 -7.739943 -7.1033015 -6.2900376 -5.8621006 -5.8635254 -5.8934803 -6.56588 -7.0395036 -7.294229 -7.5407453 -7.6545763 -7.4247665][-8.4800816 -8.1593256 -7.9006958 -7.4273353 -6.9405193 -6.43091 -6.1902466 -6.2751508 -6.5025229 -7.1058092 -7.5237312 -7.6246548 -7.8570127 -7.6664743 -7.2342305][-8.6818533 -8.3417768 -7.9902997 -7.5083232 -7.0429888 -6.632978 -6.5790806 -6.9779072 -7.3150511 -7.4012194 -7.5200243 -7.70961 -7.7130322 -7.5702229 -7.2646227]]...]
INFO - root - 2017-12-15 23:07:36.469419: step 71610, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 46h:50m:25s remains)
INFO - root - 2017-12-15 23:07:42.815802: step 71620, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 46h:00m:39s remains)
INFO - root - 2017-12-15 23:07:49.193258: step 71630, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.635 sec/batch; 46h:02m:52s remains)
INFO - root - 2017-12-15 23:07:55.548259: step 71640, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 46h:18m:29s remains)
INFO - root - 2017-12-15 23:08:01.902083: step 71650, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 45h:39m:34s remains)
INFO - root - 2017-12-15 23:08:08.295858: step 71660, loss = 0.26, batch loss = 0.14 (11.8 examples/sec; 0.678 sec/batch; 49h:07m:53s remains)
INFO - root - 2017-12-15 23:08:14.676664: step 71670, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 45h:15m:45s remains)
INFO - root - 2017-12-15 23:08:21.095539: step 71680, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 46h:58m:08s remains)
INFO - root - 2017-12-15 23:08:27.476673: step 71690, loss = 0.32, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 46h:39m:09s remains)
INFO - root - 2017-12-15 23:08:33.855519: step 71700, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 47h:05m:03s remains)
2017-12-15 23:08:34.383405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8283792 -4.7369161 -4.6800418 -5.39799 -5.5978823 -5.8073368 -5.9128585 -4.8956175 -4.0530682 -4.1929955 -4.9730577 -6.0706635 -6.8056135 -7.6052246 -8.4146147][-5.0457931 -4.9659262 -5.6196489 -6.3168683 -6.7162375 -6.451952 -6.221015 -5.5156851 -4.8470192 -4.4621568 -4.9506645 -6.0192528 -6.9278178 -7.8668728 -8.3661709][-4.7637043 -4.8281455 -4.7198 -4.9723086 -5.1209121 -4.9782476 -4.5941238 -3.7837033 -3.1637411 -3.0585465 -3.6965215 -5.0101223 -5.8317394 -6.6539154 -7.3728361][-4.50121 -3.9210334 -3.9590304 -4.1406031 -3.8712144 -3.5417976 -2.7419205 -1.8195372 -1.3425212 -1.4270792 -2.3708892 -3.72569 -4.6302752 -5.9150324 -6.7374177][-4.9855328 -3.5512228 -3.0418115 -2.5926971 -1.8440518 -1.0090585 -0.065759659 0.471982 0.94781685 0.53750324 -0.96638155 -2.5352712 -3.6154585 -5.0004425 -5.920228][-3.9301105 -3.0628009 -2.3948369 -1.1501589 -0.43130064 0.31597233 1.2135181 1.4842396 1.6129894 0.74199963 -0.82364321 -2.1819358 -3.3109288 -4.7271934 -5.5092783][-3.2140293 -2.2890239 -1.0846548 -0.18731117 0.68658924 1.569499 2.2017536 2.09369 2.0916376 1.0491753 -0.6759696 -2.14362 -3.2270594 -4.7963953 -5.8576007][-1.6097646 -0.6968689 0.11587429 1.0685816 1.8758001 2.503046 2.9119473 2.7124271 2.5536184 1.3465147 -0.6370182 -2.4747729 -3.6825547 -5.0826392 -5.67716][-0.5645709 0.32333755 0.97945786 1.6482058 2.2573662 2.6602879 2.5258799 2.3906422 2.0678329 0.63935852 -0.99256706 -2.8004785 -4.0983529 -5.0196829 -5.5650349][-1.4852538 -0.87564039 -0.19140005 0.33417225 0.71127892 1.3153877 1.2034836 0.83743572 0.52120781 -0.81101513 -2.5844526 -3.8881638 -4.4377413 -5.4736471 -5.9996929][-3.9325438 -3.3576994 -2.5439901 -1.9095302 -1.4353509 -1.2383156 -1.5551052 -1.6391916 -1.8769298 -2.8430915 -4.0804319 -4.9558291 -5.7116413 -6.2353668 -6.747952][-5.4025011 -5.0430689 -4.469779 -3.9640367 -3.6555605 -3.5670958 -3.7168822 -3.8722212 -3.8935688 -4.3022695 -5.0378513 -5.5870333 -5.8626008 -6.6337838 -7.2686119][-6.347333 -6.1879406 -5.9994497 -5.8315187 -5.444623 -5.1684723 -5.0618439 -5.0789013 -4.9640422 -4.9771433 -5.7637053 -5.9621534 -6.1214323 -6.2946692 -6.2966146][-5.6759748 -5.7276988 -5.5563922 -5.4684238 -5.4017076 -4.9497147 -4.77894 -4.8237371 -4.9075928 -5.0340433 -5.491641 -5.563282 -5.4772615 -5.4395094 -5.6699333][-5.3193207 -5.104259 -4.8363228 -5.21636 -5.3781052 -5.1093512 -5.1948843 -5.3743219 -5.5473709 -5.5975313 -5.7355738 -5.7365623 -5.5529237 -5.5538487 -5.4034338]]...]
INFO - root - 2017-12-15 23:08:40.706899: step 71710, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 45h:35m:21s remains)
INFO - root - 2017-12-15 23:08:47.052609: step 71720, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 46h:58m:24s remains)
INFO - root - 2017-12-15 23:08:53.435776: step 71730, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:19m:41s remains)
INFO - root - 2017-12-15 23:08:59.750872: step 71740, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 44h:57m:06s remains)
INFO - root - 2017-12-15 23:09:06.132523: step 71750, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 46h:16m:44s remains)
INFO - root - 2017-12-15 23:09:12.478156: step 71760, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 46h:42m:32s remains)
INFO - root - 2017-12-15 23:09:18.937467: step 71770, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 46h:35m:39s remains)
INFO - root - 2017-12-15 23:09:25.332689: step 71780, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 45h:52m:31s remains)
INFO - root - 2017-12-15 23:09:31.722860: step 71790, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 46h:37m:30s remains)
INFO - root - 2017-12-15 23:09:38.057028: step 71800, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 45h:44m:40s remains)
2017-12-15 23:09:38.598779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3266764 -4.3749104 -4.5001793 -5.1723671 -5.454042 -5.8343925 -6.3471155 -6.3006735 -5.4847307 -5.344367 -5.1474681 -5.7356887 -5.4467583 -6.1244087 -6.2355442][-4.0385709 -4.4120731 -5.2169056 -5.5220432 -5.7514825 -6.1703272 -6.5416193 -6.6587672 -6.1078014 -6.112915 -6.019311 -6.7225332 -6.7231369 -7.0708447 -6.7591028][-5.7168088 -5.33383 -5.8128681 -5.6683512 -5.6901321 -6.0997992 -6.2138429 -6.2836218 -5.637661 -5.88564 -6.095964 -6.9734 -7.3420224 -8.1393118 -7.8813505][-6.2564039 -5.8818636 -6.0487022 -5.221045 -4.535224 -4.1008353 -3.738301 -3.7621353 -3.3744063 -3.9294055 -4.0574913 -5.3391991 -6.048646 -7.5751691 -8.267436][-6.6002216 -5.4126835 -4.5008612 -3.5867872 -2.8498936 -1.9356833 -0.95172071 -0.67904758 -0.1822648 -0.70193052 -1.1793046 -3.0726609 -4.4484968 -6.5759706 -7.5056067][-6.0776711 -5.6667533 -4.6233368 -2.8902063 -1.4509053 0.28320503 1.424264 1.9692392 2.6573133 1.4844913 0.26326323 -1.7793813 -3.274827 -5.6343594 -6.75254][-5.5591207 -4.6630616 -3.4783392 -1.6878858 -0.48033953 1.5714989 2.7956028 3.9377747 4.8796206 3.6003389 1.9457045 -0.90886116 -2.7107038 -4.6261349 -5.7396312][-5.2353024 -4.5762281 -3.1442966 -0.72502613 0.94965172 2.5149097 3.1405773 4.3248148 5.355031 4.4043388 2.847909 0.07571888 -1.9172058 -4.1742082 -5.2228565][-5.0523643 -4.3815522 -3.3502202 -1.6358767 0.060470104 1.7275162 2.749773 3.6614723 3.9925852 2.9073706 1.8005915 -0.55652 -2.3206029 -4.8195448 -5.9338965][-5.9663572 -5.6146536 -4.4167218 -2.9568567 -1.2691989 -0.034518242 0.49379635 1.5021324 1.6005316 0.32735729 -0.87092495 -2.4766316 -3.5068731 -5.5353403 -6.3467808][-7.0204697 -6.5562553 -6.4360971 -5.6478086 -4.259285 -3.0800066 -2.3170624 -1.6255655 -1.92624 -2.3364191 -3.2436838 -4.6598587 -5.4649515 -7.0112329 -7.1923451][-8.2596788 -8.03038 -7.7798777 -6.8332534 -5.8982439 -5.4858112 -4.7391691 -4.2218575 -3.9730575 -4.0945215 -5.116962 -5.6404781 -6.058857 -7.148849 -6.8627453][-7.6384497 -7.7920632 -8.2134218 -7.637958 -6.7047739 -6.0412626 -5.4571886 -5.3949175 -5.1661968 -5.1100183 -5.696826 -5.9331031 -6.1175184 -6.4834847 -6.0242844][-7.40693 -6.9917307 -6.776907 -6.4694133 -6.2305107 -5.7979112 -5.3595047 -5.3041668 -5.264761 -5.3946896 -5.5554047 -5.5150766 -5.4441667 -5.9095211 -5.846734][-7.5484748 -7.3986564 -6.626915 -6.1989932 -5.8299713 -5.3993311 -5.5195842 -5.6215315 -5.8141294 -5.9889174 -6.2762337 -6.4651346 -6.4763927 -6.5072904 -6.578023]]...]
INFO - root - 2017-12-15 23:09:44.972734: step 71810, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 45h:51m:54s remains)
INFO - root - 2017-12-15 23:09:51.383852: step 71820, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 45h:30m:26s remains)
INFO - root - 2017-12-15 23:09:57.775261: step 71830, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 44h:51m:23s remains)
INFO - root - 2017-12-15 23:10:04.145649: step 71840, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 46h:46m:13s remains)
INFO - root - 2017-12-15 23:10:10.669952: step 71850, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:17m:02s remains)
INFO - root - 2017-12-15 23:10:17.098180: step 71860, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 45h:09m:59s remains)
INFO - root - 2017-12-15 23:10:23.544753: step 71870, loss = 0.25, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 48h:18m:57s remains)
INFO - root - 2017-12-15 23:10:29.911274: step 71880, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 45h:18m:30s remains)
INFO - root - 2017-12-15 23:10:36.320381: step 71890, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.621 sec/batch; 44h:55m:31s remains)
INFO - root - 2017-12-15 23:10:42.730421: step 71900, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 46h:02m:59s remains)
2017-12-15 23:10:43.321990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8313086 -4.8286481 -5.037817 -5.1496038 -4.5871296 -3.8994825 -3.7409766 -3.3810344 -3.0036278 -3.2439485 -3.7525663 -4.835557 -6.6983423 -8.4024086 -9.13885][-4.6296744 -5.2672968 -6.1447682 -6.0239863 -6.2518573 -5.0955095 -3.8246884 -2.5256257 -2.3869638 -4.172061 -5.9931688 -6.6039982 -8.0901365 -9.2101555 -9.8440943][-4.421339 -5.3671722 -5.6326051 -5.0984383 -4.4416313 -4.3195248 -3.4862509 -2.7978477 -1.6427832 -2.3707566 -4.7208862 -6.7766018 -8.57869 -9.3263445 -9.697855][-4.9180508 -5.1800275 -5.6819496 -4.9467535 -3.8032119 -2.095747 -0.89091969 -0.32736921 -0.35494232 -1.1852522 -3.3370967 -5.6135268 -7.9890127 -9.5409088 -9.7743025][-4.6785083 -5.035778 -4.4624376 -4.2019911 -2.7814355 -0.43773031 1.2992525 2.0898743 2.140337 0.15079737 -1.8729544 -4.0643258 -7.0779486 -7.9255915 -8.711729][-5.1540446 -4.7322435 -3.9023092 -2.7174926 -1.2037225 0.67391109 2.7457628 3.8354797 3.4534302 1.6779985 -1.3456898 -3.6692233 -6.0348616 -6.9890256 -7.5117345][-5.36751 -4.922749 -3.4956203 -1.8539782 0.32223797 1.9949007 3.4839954 4.2639332 4.2198486 2.4954348 -0.41182137 -3.1143508 -6.377645 -7.5335431 -7.658834][-4.4290133 -3.9244571 -3.2758441 -1.2928591 1.1491785 3.335043 5.3696461 5.5692081 4.4483738 2.3880291 -0.34242392 -2.8043256 -5.9162445 -7.5604157 -7.7570539][-4.5318031 -4.0855088 -3.072475 -1.5174999 0.56442928 2.2995281 4.0771923 4.6787539 4.4827681 1.8601952 -0.88627338 -2.9580388 -5.9763842 -7.7606425 -8.3330307][-4.2921357 -4.2747989 -4.2351942 -2.8421159 -0.98283148 0.57179165 2.0566416 2.3151627 2.1278954 -0.02806139 -2.7772498 -4.7242174 -7.040616 -8.2038231 -8.6769867][-5.7710247 -5.9122438 -5.8137302 -4.8614073 -3.453454 -1.6309237 -0.59118271 -0.78868723 -0.44384241 -2.2163272 -4.3561106 -5.9644279 -8.1993256 -9.0710554 -8.9648781][-6.99274 -6.8128347 -6.9531841 -6.6808252 -5.7479796 -4.5722704 -3.6883302 -3.3447881 -3.367794 -4.4384527 -5.121151 -6.2966151 -7.8470078 -8.819416 -9.4521675][-8.2638979 -8.3173714 -8.2097168 -7.7614479 -7.2961879 -6.7913713 -6.4809332 -6.4881992 -5.8768806 -6.492753 -7.3955212 -7.5409322 -8.225214 -8.5997887 -8.6575279][-9.1199532 -9.0966272 -8.6040478 -8.3352633 -7.4480834 -6.8209658 -6.6153259 -7.3004012 -7.2619882 -7.5260963 -7.4353671 -7.3800049 -7.6015296 -7.34954 -7.4318428][-8.6880713 -8.6931286 -8.7463741 -8.6403141 -8.4121265 -7.7538586 -7.48513 -7.2314339 -7.1657081 -7.2878456 -7.70289 -7.6153979 -7.3170171 -7.1760488 -6.8415608]]...]
INFO - root - 2017-12-15 23:10:49.805680: step 71910, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 46h:22m:08s remains)
INFO - root - 2017-12-15 23:10:56.208076: step 71920, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 46h:11m:59s remains)
INFO - root - 2017-12-15 23:11:02.610232: step 71930, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 46h:01m:50s remains)
INFO - root - 2017-12-15 23:11:09.145392: step 71940, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.660 sec/batch; 47h:45m:25s remains)
INFO - root - 2017-12-15 23:11:15.580828: step 71950, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 47h:01m:19s remains)
INFO - root - 2017-12-15 23:11:22.028802: step 71960, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:40m:24s remains)
INFO - root - 2017-12-15 23:11:28.412641: step 71970, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.648 sec/batch; 46h:54m:40s remains)
INFO - root - 2017-12-15 23:11:34.804425: step 71980, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 46h:29m:29s remains)
INFO - root - 2017-12-15 23:11:41.293176: step 71990, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 46h:03m:34s remains)
INFO - root - 2017-12-15 23:11:47.667926: step 72000, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 45h:53m:59s remains)
2017-12-15 23:11:48.257078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1511841 -5.1375751 -4.5631523 -3.9300425 -3.2582736 -2.8751278 -2.3997097 -2.1687937 -2.2245369 -4.0077286 -4.392705 -4.5216389 -4.9806895 -5.6554594 -6.0006566][-4.5684962 -4.9869633 -4.7779522 -3.7347903 -2.98696 -2.7322369 -2.2797132 -2.2783093 -2.3021531 -4.0100455 -4.9953146 -5.9324389 -6.7719636 -7.1694751 -7.0620279][-3.9906216 -4.8717213 -4.640945 -3.9977014 -3.4357948 -3.1539974 -2.799273 -2.7522655 -2.9387031 -4.5899296 -5.1215868 -6.2672286 -7.6129723 -8.1963453 -8.0113459][-4.6275454 -4.5704942 -4.0755892 -3.7765756 -3.2111239 -2.5744033 -2.170629 -2.3277955 -2.6711721 -4.6526966 -5.1193972 -6.0676932 -7.2142048 -8.0107641 -8.5230074][-3.7566314 -3.3538733 -2.86509 -2.0294023 -1.1371436 0.077633858 0.63338089 0.024895668 -1.1399465 -3.9161832 -5.0521727 -5.945014 -6.7536416 -7.2630787 -8.03542][-2.0228477 -1.8284101 -1.6220889 -0.8613658 0.30554342 1.6455908 2.6251326 2.1118584 1.232111 -1.8145308 -3.8947563 -5.6760693 -6.8108621 -7.3142085 -7.6480231][-0.22320509 -0.21225691 -0.26500845 0.96008778 2.4503775 3.5736742 4.4922123 4.0035696 3.0523596 0.17215919 -1.5203948 -3.7551982 -5.7756648 -6.9616432 -7.6978312][0.73836327 1.1627235 0.99685478 1.738636 3.4021635 4.699048 5.7127 5.5770922 4.8677673 1.7946167 -0.291574 -2.3632188 -4.4115057 -5.7914228 -6.9455595][-0.48428822 0.45191479 0.98800278 1.7397022 2.2195044 2.7730618 3.7910204 4.407548 4.303462 1.7472382 -0.21885538 -2.2540212 -3.8707337 -4.9662018 -5.9720106][-2.7329078 -1.8573895 -1.2649932 -0.37228346 0.62449551 1.7189131 2.279253 2.4977083 2.703558 0.48718643 -1.0533442 -2.7592959 -4.0588155 -5.1409779 -6.0625567][-4.561182 -3.7804346 -3.340951 -2.6518054 -2.1111603 -1.3336549 -0.93125486 -0.53762531 0.082693577 -1.7345185 -3.1490164 -4.5741835 -5.6960688 -6.3813286 -6.6073918][-6.709281 -6.1978221 -5.7862477 -4.896348 -4.0941744 -3.8381169 -3.6720777 -3.600955 -3.394999 -4.4690237 -4.9564848 -5.4388523 -6.2697587 -7.3323402 -7.7200861][-8.0583191 -7.8248029 -7.2016068 -6.2679882 -5.6625891 -5.3255758 -4.7278929 -4.642663 -4.5877562 -5.3008976 -5.4105515 -5.70589 -6.3041716 -6.7160287 -7.3415241][-8.5662413 -8.5019016 -8.0464487 -7.6886678 -7.2054811 -6.6774826 -6.4654531 -6.3745923 -6.1951971 -6.2243161 -6.0823836 -5.8177214 -5.9702864 -6.1924567 -6.2413511][-6.741396 -7.2402725 -7.5198178 -7.6407666 -7.1614432 -6.7220674 -6.5190783 -6.3017807 -6.0420852 -6.001184 -6.1907644 -6.0568132 -5.6086254 -5.583909 -5.615283]]...]
INFO - root - 2017-12-15 23:11:54.812427: step 72010, loss = 0.37, batch loss = 0.25 (12.7 examples/sec; 0.629 sec/batch; 45h:32m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:12:01.117661: step 72020, loss = 0.27, batch loss = 0.16 (13.1 examples/sec; 0.609 sec/batch; 44h:03m:06s remains)
INFO - root - 2017-12-15 23:12:07.402497: step 72030, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 46h:00m:01s remains)
INFO - root - 2017-12-15 23:12:13.760098: step 72040, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 46h:56m:44s remains)
INFO - root - 2017-12-15 23:12:20.083021: step 72050, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 45h:00m:38s remains)
INFO - root - 2017-12-15 23:12:26.433691: step 72060, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 44h:59m:23s remains)
INFO - root - 2017-12-15 23:12:32.852271: step 72070, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 46h:18m:25s remains)
INFO - root - 2017-12-15 23:12:39.227420: step 72080, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 45h:37m:40s remains)
INFO - root - 2017-12-15 23:12:45.641785: step 72090, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 47h:02m:14s remains)
INFO - root - 2017-12-15 23:12:52.062679: step 72100, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 45h:20m:01s remains)
2017-12-15 23:12:52.530638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8822923 -6.126545 -6.1843925 -6.4280744 -6.4950047 -6.0037136 -5.06256 -4.2396278 -3.2387981 -4.2532892 -5.2823896 -5.6197329 -6.4129019 -7.4147043 -7.5908155][-5.9266644 -6.1467371 -6.1779757 -5.9185934 -6.3979712 -6.6852341 -6.122797 -5.0248256 -3.8977613 -4.9563346 -5.8583841 -6.3086519 -7.4042397 -8.6315813 -9.160429][-5.7055607 -5.3831263 -5.5404406 -5.469306 -5.7238793 -5.6596851 -4.8533134 -4.4078426 -3.7879272 -5.0871 -6.2878456 -7.002923 -7.8690462 -8.8191633 -9.2548056][-5.9917717 -5.6831989 -5.58385 -4.971354 -4.7056103 -4.2252808 -3.5759711 -3.0055966 -2.0463352 -3.6809473 -5.2695274 -6.1624975 -7.5526338 -8.5554218 -9.0024023][-6.7744813 -5.2255516 -4.0491304 -3.7154903 -3.3497391 -2.0654216 -0.91341496 -0.29352713 0.48624325 -1.5101933 -3.57718 -4.7865305 -6.5309258 -8.0821371 -8.783143][-7.47811 -6.0641794 -4.6745129 -3.0790968 -1.3497095 0.81429482 2.5037365 2.7220974 3.3673935 0.88038826 -1.8324947 -3.6892252 -5.3853645 -7.0297952 -8.0149727][-7.323277 -5.3833923 -2.9431772 -1.136692 0.9354248 3.1176491 5.2165728 6.1912146 6.5605259 3.4341402 0.42617798 -1.8561573 -4.0249672 -5.5819011 -6.6230359][-6.8233933 -5.6552043 -3.5486155 -0.2393856 2.9943094 4.7697115 6.4753971 7.791853 8.2216244 5.0352135 1.3567381 -1.3654227 -3.6133766 -5.0486708 -6.0147152][-6.9075904 -6.4604235 -5.0066533 -2.3719606 0.62875748 2.9345274 4.4314156 5.3280087 5.7247343 3.32755 0.17109013 -2.3686423 -4.37271 -5.9165592 -6.67175][-8.4485626 -7.6966276 -6.5201993 -4.0470347 -1.4312792 0.53165531 2.0174122 2.3026762 1.832695 -0.80466986 -3.1228309 -4.7414894 -6.4066906 -7.0837288 -7.1590204][-10.410736 -9.9308777 -8.71658 -6.7352138 -5.0726004 -2.9940228 -1.0901465 -1.0395179 -1.4574509 -4.0829816 -6.1499434 -6.5005336 -7.8572197 -8.5538054 -8.3938465][-11.403214 -10.822321 -10.014226 -8.7988224 -7.6068673 -6.1559496 -5.061162 -4.2020993 -3.9407179 -6.2231588 -7.8392978 -7.6259408 -8.2415819 -8.5419617 -8.3350592][-10.234314 -10.545304 -10.314425 -9.3775539 -8.5357285 -7.5394015 -6.9099803 -6.6698337 -6.3778648 -7.4279327 -8.3699732 -8.3823633 -9.0285845 -8.7734375 -8.1067286][-9.5977392 -9.5615177 -9.1580858 -8.7676449 -8.2378559 -7.4785976 -7.3577738 -7.4298635 -7.4926219 -8.3396988 -8.503746 -8.1104336 -8.2020178 -8.2441225 -7.6758957][-9.071 -8.9640694 -8.6504011 -8.1925325 -8.0772209 -7.333765 -6.8716774 -7.6550074 -7.9302135 -7.7516651 -7.8322406 -7.9464221 -7.5617404 -7.0863762 -6.6295547]]...]
INFO - root - 2017-12-15 23:12:59.010838: step 72110, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 47h:56m:12s remains)
INFO - root - 2017-12-15 23:13:05.409316: step 72120, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 46h:07m:03s remains)
INFO - root - 2017-12-15 23:13:11.745225: step 72130, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 47h:27m:37s remains)
INFO - root - 2017-12-15 23:13:18.188448: step 72140, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 46h:32m:41s remains)
INFO - root - 2017-12-15 23:13:24.591354: step 72150, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 46h:49m:14s remains)
INFO - root - 2017-12-15 23:13:30.965508: step 72160, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 46h:08m:28s remains)
INFO - root - 2017-12-15 23:13:37.326553: step 72170, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 47h:14m:05s remains)
INFO - root - 2017-12-15 23:13:43.787587: step 72180, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:39m:33s remains)
INFO - root - 2017-12-15 23:13:50.239230: step 72190, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 46h:04m:18s remains)
INFO - root - 2017-12-15 23:13:56.530753: step 72200, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.618 sec/batch; 44h:41m:25s remains)
2017-12-15 23:13:57.046202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.52454 -3.9988632 -4.1639805 -4.155653 -3.9782906 -3.7720642 -3.3466144 -2.7746081 -1.8231649 -3.5831633 -3.9088109 -5.401823 -6.5653763 -6.8543978 -7.8334417][-3.9739923 -4.4399352 -4.925065 -4.955646 -4.8676333 -4.4932871 -3.9470098 -3.2729707 -2.4116426 -4.0496721 -4.3274326 -5.8309393 -7.0629153 -7.5159116 -8.4254713][-4.1488991 -4.3397818 -4.8360181 -5.0494452 -4.9673243 -4.5787649 -4.1922464 -3.716774 -3.1195049 -4.853755 -5.2169 -6.8126335 -7.587379 -8.089077 -8.9479885][-4.3220081 -3.9543927 -3.8678327 -3.7604618 -3.383245 -2.5509 -1.8204427 -1.704124 -1.5661054 -3.7845049 -4.3740239 -6.2640743 -7.2612195 -7.7076468 -8.671031][-4.4037333 -3.4683008 -3.1367617 -2.6421161 -1.8050027 -0.66066504 0.3037262 0.40742397 0.13523006 -2.4055352 -3.4425664 -5.5050569 -6.7647591 -7.3445396 -8.2361679][-6.1179113 -4.8821182 -4.2980533 -2.9875536 -1.5539575 0.21839094 1.5341225 1.520915 1.0223131 -2.0552306 -3.3425789 -5.4342337 -6.5802784 -7.1483693 -7.9873581][-6.4491644 -5.2940559 -4.6457944 -2.6118455 -0.55536842 1.4561844 3.0424814 3.2871046 2.9316177 -0.34664536 -2.1527424 -4.46099 -5.7187495 -6.2525911 -7.1404166][-5.8570848 -4.5318422 -3.6437263 -1.8310413 0.24789143 2.33671 4.2480459 4.6081963 4.273736 1.0125055 -0.87625742 -3.4392776 -5.0423546 -5.7631426 -6.6519036][-5.9849653 -4.8288193 -4.180233 -2.6306033 -1.4786224 0.52039146 2.3032846 2.8425856 2.8670073 -0.17422771 -1.4268956 -3.5958128 -4.8163614 -5.432703 -6.3782253][-6.2994928 -5.4215565 -4.7692327 -3.6586547 -2.5562701 -1.3251972 -0.014780045 0.37229919 0.27666759 -2.4813743 -3.1708965 -4.6302691 -5.4522333 -5.8174534 -6.5352783][-7.6809835 -6.9253345 -6.468811 -5.730628 -5.1153269 -3.9852939 -2.8548827 -2.4666357 -2.4323387 -4.6934218 -4.9497404 -5.9292068 -6.4722753 -6.5326157 -7.1311831][-7.6984816 -7.0606632 -6.5212851 -6.1656032 -6.0625734 -5.4619246 -4.8738823 -4.6364307 -4.6375494 -6.0748615 -5.6521664 -6.1554737 -6.4530835 -6.4246368 -6.9519238][-8.84699 -8.0651464 -7.7830458 -7.1453419 -6.6408963 -6.3091087 -5.9592133 -5.8717985 -5.8901134 -6.8718944 -6.7035618 -6.7605605 -6.6884623 -6.4754963 -6.5857286][-9.0896206 -8.3327618 -7.9352765 -7.4751334 -7.0252671 -6.6803188 -6.0209217 -6.1216445 -6.0794754 -6.5566416 -6.5020504 -6.4777007 -6.4523745 -6.5489826 -6.6167889][-9.3089027 -8.5046644 -8.1871233 -7.7012672 -7.2882113 -7.1181412 -6.8390064 -6.99465 -7.1206107 -7.0940566 -6.9261112 -6.7180676 -6.5359836 -6.5489092 -6.4857817]]...]
INFO - root - 2017-12-15 23:14:03.491316: step 72210, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 45h:51m:17s remains)
INFO - root - 2017-12-15 23:14:10.014093: step 72220, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.639 sec/batch; 46h:11m:02s remains)
INFO - root - 2017-12-15 23:14:16.512267: step 72230, loss = 0.35, batch loss = 0.24 (12.6 examples/sec; 0.634 sec/batch; 45h:50m:55s remains)
INFO - root - 2017-12-15 23:14:22.883028: step 72240, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 46h:33m:29s remains)
INFO - root - 2017-12-15 23:14:29.398025: step 72250, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 48h:03m:45s remains)
INFO - root - 2017-12-15 23:14:35.856673: step 72260, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 45h:58m:01s remains)
INFO - root - 2017-12-15 23:14:42.364273: step 72270, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 47h:15m:36s remains)
INFO - root - 2017-12-15 23:14:48.854113: step 72280, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 47h:09m:25s remains)
INFO - root - 2017-12-15 23:14:55.412106: step 72290, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 47h:29m:13s remains)
INFO - root - 2017-12-15 23:15:01.818610: step 72300, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 45h:45m:17s remains)
2017-12-15 23:15:02.354808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2512274 -3.9795151 -3.7479794 -3.3316736 -3.0163717 -2.7438393 -2.1708083 -1.930253 -1.9957647 -4.7724466 -6.8665066 -8.5148468 -9.5916309 -10.321918 -10.475594][-5.1276913 -4.6735106 -4.2833338 -4.0030041 -3.7093282 -3.388834 -3.0346661 -2.8952312 -2.7506862 -5.3510218 -7.0289311 -8.3468361 -9.44314 -10.529686 -10.729243][-5.1120634 -4.39983 -3.5890241 -3.0033464 -2.413806 -2.2564707 -1.7510233 -1.5041628 -1.3041172 -3.3779178 -4.9817882 -6.7538791 -8.2332458 -9.3806343 -10.063219][-3.8379509 -3.1642332 -2.2807527 -1.5845537 -1.0293493 -0.72681904 -0.098527908 0.44570255 1.1268234 -0.76628017 -2.3558755 -3.9798532 -5.5985055 -6.9117751 -7.8787928][-4.0231867 -2.7510204 -1.3527832 -0.40166473 0.73228741 1.7968407 2.6801825 3.0405855 3.3852644 1.2775087 -0.44748545 -2.5990705 -4.8789163 -6.37531 -7.2262192][-4.0696306 -2.585454 -1.1611328 0.31609821 1.7612371 2.9879036 4.1661396 4.6712322 5.1428356 2.9534512 0.83660126 -1.1985717 -3.2237406 -5.0014734 -6.3226557][-3.9680748 -2.637279 -1.2135301 0.68077469 2.6004858 4.0262203 5.2239742 5.7962475 6.4107828 3.6179094 1.2234268 -1.2968864 -3.9798663 -5.1879845 -5.8619046][-4.2896748 -2.5686865 -1.0878978 0.78693962 2.6287956 4.1478386 5.4190044 6.0103264 6.1156569 3.0518293 0.53081226 -2.0551376 -4.5586853 -5.7522821 -6.1015892][-4.9365888 -3.5866866 -2.1150918 0.15501547 2.0748844 3.6173019 4.7849731 4.8214836 4.0628843 0.97976875 -0.95915794 -3.1578231 -5.2249541 -6.1808209 -6.2620168][-4.6630697 -3.85362 -2.8538356 -0.89476013 0.6990242 1.8428411 2.8064928 2.6751022 2.2930956 -0.34747267 -2.0056767 -3.9424314 -5.2849355 -6.0787821 -6.2640276][-5.8493032 -5.3920851 -4.8244123 -3.4446383 -2.2460084 -0.85677338 0.23846006 0.13125992 -0.022306919 -2.5131273 -4.1590681 -5.7790036 -6.7890077 -7.0416784 -6.8510461][-7.3952188 -6.8621874 -6.0250187 -5.2154288 -4.5651493 -3.3904405 -2.8150425 -3.0546875 -3.152247 -4.8912206 -5.7131763 -6.6202979 -7.1406522 -7.1490579 -6.9980154][-7.7152472 -7.6325488 -7.3548098 -6.7076559 -5.9819727 -5.2350912 -4.5473204 -4.5082278 -4.6862116 -5.693872 -6.0005026 -6.6801214 -6.9510608 -6.6979952 -6.1811934][-7.430604 -7.2571955 -6.9102845 -6.4478498 -5.9448748 -4.8928003 -4.2219691 -4.2408009 -4.1612606 -4.96916 -5.3555984 -5.7679439 -6.067327 -6.0567112 -6.016324][-8.6153316 -8.5431776 -8.0031557 -7.3644471 -6.6637273 -5.9602118 -5.8228736 -5.777081 -5.9649816 -5.9908891 -5.8102241 -5.7797422 -5.86106 -5.9150944 -5.8582582]]...]
INFO - root - 2017-12-15 23:15:08.740413: step 72310, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 45h:28m:39s remains)
INFO - root - 2017-12-15 23:15:15.174038: step 72320, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 46h:07m:24s remains)
INFO - root - 2017-12-15 23:15:21.649890: step 72330, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 47h:28m:16s remains)
INFO - root - 2017-12-15 23:15:28.101688: step 72340, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 46h:21m:45s remains)
INFO - root - 2017-12-15 23:15:34.573848: step 72350, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 46h:45m:22s remains)
INFO - root - 2017-12-15 23:15:41.015421: step 72360, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 46h:40m:43s remains)
INFO - root - 2017-12-15 23:15:47.439756: step 72370, loss = 0.35, batch loss = 0.23 (12.3 examples/sec; 0.652 sec/batch; 47h:08m:06s remains)
INFO - root - 2017-12-15 23:15:53.806630: step 72380, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 46h:28m:59s remains)
INFO - root - 2017-12-15 23:16:00.253678: step 72390, loss = 0.32, batch loss = 0.21 (11.8 examples/sec; 0.676 sec/batch; 48h:51m:41s remains)
INFO - root - 2017-12-15 23:16:06.751506: step 72400, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 46h:02m:32s remains)
2017-12-15 23:16:07.280319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4026041 -9.9770565 -11.337468 -11.959591 -11.752357 -11.090105 -9.7511129 -8.0121851 -5.8803778 -5.9771271 -5.4430513 -5.5213418 -5.7413955 -5.8084545 -6.1709085][-7.9484582 -9.8764648 -11.818579 -12.90785 -13.006936 -12.193873 -10.911782 -9.23188 -7.1075735 -7.3496137 -7.0090909 -7.4736128 -7.4592829 -7.23826 -6.825511][-8.3823557 -9.8034315 -11.413195 -12.411281 -12.227086 -11.42322 -10.075152 -9.0151224 -7.3044953 -7.8769097 -7.6100307 -8.3239384 -8.84833 -8.7558765 -8.395896][-8.9762545 -9.1890535 -10.250988 -10.176723 -9.1680031 -8.09557 -6.8921185 -5.9665918 -4.815506 -6.3249741 -7.212009 -8.5840406 -9.1604156 -9.4723 -9.09495][-8.9722652 -8.2240782 -8.1113205 -6.9969211 -5.3481703 -3.2587519 -1.5263429 -1.2738738 -1.0772133 -3.4955897 -5.5156326 -7.4441924 -8.8361006 -9.4369822 -9.1066647][-9.1135616 -8.1305752 -7.3338675 -4.8503246 -2.0255475 0.66123295 3.05587 3.6083612 3.4340792 -0.56117249 -3.3795009 -5.6659765 -7.6191163 -8.723484 -8.8313017][-8.0767231 -7.1683106 -5.5692563 -2.2289462 0.45698929 2.962822 5.0337315 5.79692 5.9937296 1.9644508 -1.4675937 -4.4650793 -6.3439403 -6.9801989 -7.2714596][-7.2017369 -6.3822937 -4.9500208 -1.8931904 0.68963623 3.064291 4.9232941 5.0634661 5.0902653 1.8942175 -0.63487291 -3.5232487 -5.30278 -6.2695532 -6.398313][-7.5747705 -6.9443011 -6.3834906 -4.2995658 -2.3628531 0.081529617 2.1308384 2.9470186 3.0095348 -0.65491009 -2.8894882 -4.6086483 -5.4633236 -6.1962419 -6.33869][-9.8125238 -8.9548368 -8.1653852 -6.1204462 -4.4853144 -3.2803512 -2.2741318 -0.9752841 -0.65019989 -4.14172 -5.9894352 -7.7959604 -8.4795351 -7.673533 -6.8054872][-11.524025 -11.184614 -10.823323 -9.1254807 -7.7135253 -6.520401 -5.3780756 -5.4166646 -5.6570125 -7.5686622 -8.1315022 -9.109869 -9.3729267 -9.0524139 -8.2812614][-11.869749 -11.908556 -11.741823 -10.924452 -9.9910936 -8.5910358 -7.4038906 -7.1885667 -6.8471594 -8.139101 -8.5865393 -8.5537233 -8.5539465 -8.4387207 -8.0039139][-11.184816 -10.900807 -10.639235 -9.7373915 -8.7218943 -8.0370216 -7.2169924 -7.0279374 -6.872066 -7.5787225 -7.8870745 -7.7281413 -7.7466369 -7.7379069 -7.3503232][-9.9689693 -9.5072222 -8.8692665 -8.0843048 -7.2675939 -6.4484272 -5.9897518 -6.1761074 -6.2858357 -6.6972013 -7.1503544 -7.0747013 -7.182302 -7.099494 -7.0098715][-7.522594 -7.7269349 -8.1498117 -7.3782058 -7.0917578 -6.6127315 -5.9709029 -6.0252762 -6.6389914 -7.0103288 -7.1880417 -6.9590878 -6.9417996 -7.308619 -7.4653659]]...]
INFO - root - 2017-12-15 23:16:13.772607: step 72410, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.649 sec/batch; 46h:52m:56s remains)
INFO - root - 2017-12-15 23:16:20.231426: step 72420, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:55m:12s remains)
INFO - root - 2017-12-15 23:16:26.714988: step 72430, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.651 sec/batch; 47h:00m:20s remains)
INFO - root - 2017-12-15 23:16:33.121633: step 72440, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 46h:26m:32s remains)
INFO - root - 2017-12-15 23:16:39.642277: step 72450, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 46h:40m:31s remains)
INFO - root - 2017-12-15 23:16:46.122534: step 72460, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 47h:26m:25s remains)
INFO - root - 2017-12-15 23:16:52.525536: step 72470, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 46h:56m:27s remains)
INFO - root - 2017-12-15 23:16:59.039735: step 72480, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 46h:01m:15s remains)
INFO - root - 2017-12-15 23:17:05.602683: step 72490, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 46h:26m:38s remains)
INFO - root - 2017-12-15 23:17:12.025835: step 72500, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 46h:06m:43s remains)
2017-12-15 23:17:12.589919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.22459 -8.06312 -8.3642225 -9.2924929 -9.2449942 -8.994894 -8.4437342 -6.9547286 -5.8303986 -6.3903174 -6.4507041 -6.7828207 -6.0170679 -6.6306553 -6.9826608][-7.2799592 -8.1927176 -9.5817995 -10.12703 -10.097462 -9.8210735 -9.4828329 -8.5378866 -7.0020485 -6.8467984 -6.4711447 -6.9571824 -7.3005638 -7.5163822 -7.0628881][-7.235774 -7.4884958 -7.6290374 -8.8005495 -9.2978506 -8.5890417 -7.760613 -7.2337165 -6.6078143 -7.5857391 -7.6976953 -8.2420053 -8.03005 -7.5815964 -8.078927][-7.0683389 -6.2000308 -7.032577 -6.489 -5.5155439 -5.306025 -4.47339 -3.7378895 -3.324008 -4.6850376 -5.7900929 -7.0661745 -7.7953534 -8.31317 -8.401062][-5.8657737 -5.17517 -4.8867855 -3.7000146 -3.088243 -1.4421911 -0.00080442429 -0.45913363 -0.38111019 -1.8671789 -3.2491469 -5.3016634 -6.3655705 -7.65349 -8.506567][-4.4054289 -3.3741789 -2.60538 -0.75462008 0.55737591 1.7180986 3.3505344 4.0043154 4.2967434 1.7071085 -0.42595005 -2.8064322 -4.078896 -5.5260391 -6.3717632][-4.0331411 -2.7752953 -1.4341326 0.69371223 2.12049 4.0307045 5.6097975 5.5321007 5.3563576 2.9394054 0.53400421 -2.0279431 -3.225441 -4.40787 -5.3093047][-3.4842391 -3.0797606 -2.0765691 0.12670231 2.3008585 3.9216747 4.8234396 5.2695093 5.2584448 2.2794352 -0.16934204 -2.2596664 -3.4242973 -4.4059229 -5.1806431][-3.80204 -3.4786544 -3.1176915 -1.6302538 -0.0065965652 1.7824917 3.073822 2.8381357 2.4696388 0.36133575 -0.99028492 -3.13413 -4.8513269 -5.6913877 -5.9750981][-4.7749166 -4.8150368 -4.4046335 -3.3808107 -1.775063 -0.48628235 -0.068167686 -0.21997356 -0.51200581 -1.9759188 -3.2309542 -4.5983381 -5.5011892 -6.7708759 -7.302423][-6.6473107 -6.3989549 -5.820406 -5.7435083 -4.6998329 -3.9532413 -3.1077352 -3.2427058 -4.0626407 -4.7874746 -5.74462 -5.9521732 -6.5042191 -6.8446736 -6.6425557][-8.9983625 -8.3018808 -7.893774 -7.2385521 -6.8582273 -6.1909089 -5.869503 -6.4475985 -6.5157566 -7.0189281 -7.7067313 -7.1677856 -6.7206964 -6.68109 -6.5746069][-9.233305 -9.4599762 -9.2752972 -8.5675783 -8.6001377 -7.6839261 -7.2110629 -7.49126 -8.1139612 -8.5819407 -8.8494139 -8.1389141 -7.3337212 -6.9534516 -6.2911162][-9.4896278 -9.2546034 -8.7180691 -8.2857285 -7.7994294 -7.3656058 -7.6888876 -7.4106736 -7.1461158 -7.65676 -8.26753 -7.9562879 -7.481256 -7.0932126 -6.9429584][-8.8567572 -7.9141879 -7.4173393 -7.3964739 -7.1549525 -6.7292857 -6.8919754 -7.1942821 -7.8333368 -7.869307 -7.6175623 -7.3556309 -6.8265405 -6.7801518 -6.5517006]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 23:17:20.141474: step 72510, loss = 0.23, batch loss = 0.12 (12.7 examples/sec; 0.630 sec/batch; 45h:28m:12s remains)
INFO - root - 2017-12-15 23:17:26.586199: step 72520, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 45h:49m:59s remains)
INFO - root - 2017-12-15 23:17:32.909667: step 72530, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 44h:53m:30s remains)
INFO - root - 2017-12-15 23:17:39.252337: step 72540, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 45h:47m:14s remains)
INFO - root - 2017-12-15 23:17:45.707370: step 72550, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 45h:36m:06s remains)
INFO - root - 2017-12-15 23:17:52.228821: step 72560, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 46h:58m:51s remains)
INFO - root - 2017-12-15 23:17:58.639937: step 72570, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 46h:49m:01s remains)
INFO - root - 2017-12-15 23:18:05.174684: step 72580, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 47h:05m:10s remains)
INFO - root - 2017-12-15 23:18:11.682755: step 72590, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 46h:09m:32s remains)
INFO - root - 2017-12-15 23:18:18.084809: step 72600, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 47h:38m:25s remains)
2017-12-15 23:18:18.590128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.870986 -5.9843364 -6.0287585 -5.9981289 -5.6197453 -4.6497793 -3.6832075 -2.8032765 -2.2539058 -2.1127734 -2.8923879 -4.1583495 -5.2754393 -6.224072 -7.1565828][-5.0403714 -4.9568405 -4.898037 -4.6945782 -4.7728195 -4.2104597 -3.1160302 -2.2834387 -1.853446 -2.2442207 -3.7094526 -4.823379 -5.4577827 -6.0886106 -6.4895039][-4.1313062 -4.0051947 -3.748461 -3.4804344 -3.2437434 -2.8534012 -2.5963993 -2.390399 -2.0595694 -2.2716036 -3.2131257 -4.2498941 -5.0260868 -5.3036661 -5.6649027][-3.6549897 -3.0534258 -2.7993517 -2.6038737 -2.4253035 -1.8137164 -1.2782121 -1.2892933 -1.4946337 -1.8858585 -2.6598358 -3.5655508 -4.3646188 -5.0364714 -5.6358485][-3.7953773 -2.649672 -1.481535 -1.0733857 -0.90785694 -0.37342453 0.012835503 0.030278683 -0.1236887 -0.81932783 -2.1883359 -3.648632 -4.5181675 -5.20133 -5.6416345][-3.279985 -2.3727789 -1.2986274 -0.79798985 -0.027229309 0.95381927 1.2113724 1.1431351 1.0549612 0.42489338 -0.78168249 -2.4135594 -3.5273504 -4.6035471 -5.4063044][-3.9896398 -2.8234863 -1.5388947 -0.6045723 0.26937008 1.0684853 1.3763857 1.5987196 1.9365835 1.2512617 -0.18670082 -2.0935583 -3.2631755 -4.4434772 -5.4674339][-4.0542607 -2.9969745 -2.0248685 -0.93010378 0.38757133 1.6003313 1.8250332 1.8861246 2.132884 1.5947552 0.13885975 -1.9295764 -3.1402512 -4.2603827 -5.0714455][-4.3603306 -3.2545085 -2.2757158 -0.947587 0.19087219 1.013566 1.3190041 1.7809544 2.2080727 1.3845158 -0.12192869 -1.8379807 -3.0860157 -3.9657915 -4.6200576][-5.2405119 -4.12347 -3.3304143 -2.4469576 -1.4237909 -0.58185816 -0.25934458 0.27553129 0.87067318 0.076589584 -1.4721856 -2.6905627 -3.3845396 -4.1336079 -4.7655144][-7.086617 -6.3515372 -5.7758374 -4.9072161 -3.9846873 -3.2276368 -2.8029842 -2.5801907 -2.5050793 -3.1455379 -4.0755253 -4.6986928 -5.1309195 -5.3780317 -5.681427][-8.0909729 -7.8892303 -7.6941791 -6.9877625 -6.4332323 -6.1119018 -5.5498343 -5.0549593 -4.7195063 -5.1356144 -5.8203955 -6.14856 -6.3647466 -6.4383855 -6.3644614][-8.1449986 -7.9776149 -7.8534608 -7.9506927 -8.048624 -7.6556797 -7.2428451 -7.0844059 -6.7850518 -6.62035 -6.9033442 -6.9935479 -6.8629436 -6.883388 -6.5906811][-7.6291666 -7.9093909 -7.8901815 -7.9232879 -7.8860431 -7.6881518 -7.3295112 -7.3653188 -7.6082435 -7.7458744 -8.1028614 -8.1185846 -7.5165858 -6.9546776 -6.4969554][-7.4224753 -7.201931 -7.5048714 -7.8583302 -7.7222481 -7.166646 -6.83157 -7.0930233 -7.4324231 -7.87272 -8.1964779 -8.0478163 -7.7065506 -7.6925044 -7.1993871]]...]
INFO - root - 2017-12-15 23:18:24.908672: step 72610, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:51m:09s remains)
INFO - root - 2017-12-15 23:18:31.315568: step 72620, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:34m:58s remains)
INFO - root - 2017-12-15 23:18:37.756607: step 72630, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 46h:40m:22s remains)
INFO - root - 2017-12-15 23:18:44.196421: step 72640, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:35m:00s remains)
INFO - root - 2017-12-15 23:18:50.610452: step 72650, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 46h:18m:57s remains)
INFO - root - 2017-12-15 23:18:57.023045: step 72660, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 45h:37m:04s remains)
INFO - root - 2017-12-15 23:19:03.451610: step 72670, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 46h:23m:15s remains)
INFO - root - 2017-12-15 23:19:09.781292: step 72680, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 45h:39m:45s remains)
INFO - root - 2017-12-15 23:19:16.256386: step 72690, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 47h:09m:07s remains)
INFO - root - 2017-12-15 23:19:22.726088: step 72700, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 47h:41m:06s remains)
2017-12-15 23:19:23.337942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.66225195 -0.69874 -1.2535496 -1.1696954 -0.97486925 -0.97711372 -1.0033889 -0.92537928 -1.0025964 -2.4955 -3.5905881 -5.0319748 -6.633729 -7.701169 -8.6265059][-2.5068111 -2.4190459 -2.9003377 -3.1462059 -3.4139128 -3.5943322 -3.6396656 -3.490252 -3.4024129 -4.6801777 -5.6207032 -6.8770103 -8.4281969 -9.6060429 -10.660943][-3.2671056 -2.9384179 -3.1846747 -3.2880387 -3.2268019 -3.3132119 -3.3521681 -3.2981796 -3.3373461 -4.2734962 -5.0030541 -6.0546665 -7.4556613 -8.5141029 -9.2636242][-4.7080035 -4.2506714 -3.9357262 -3.4019775 -2.745997 -2.284914 -1.6495118 -1.2324862 -1.0575891 -2.2373614 -3.0704083 -4.2770309 -5.6419764 -6.9537334 -8.0178595][-5.4291534 -4.9431248 -4.5539112 -3.6884904 -2.5605187 -1.3835888 -0.33956432 -0.178195 -0.1979537 -1.5953689 -2.5697589 -3.900439 -5.2399454 -6.0117435 -6.6915922][-5.7969031 -4.9407597 -4.5035238 -3.5045624 -2.071291 -0.47895098 0.98471832 1.3097353 1.3531847 -0.19332933 -1.3027382 -2.5042071 -4.0093012 -4.8758154 -5.4250507][-5.8719549 -5.1673422 -4.3891068 -2.9283214 -1.1136746 0.63496017 2.2324038 2.8065977 2.9071922 1.2240791 -0.11991119 -1.8790808 -3.7954917 -4.6016779 -5.0379267][-4.1098347 -3.2199526 -2.4088778 -0.8712244 0.84759521 2.4582243 4.0792007 4.434617 4.3356771 2.3914165 0.86492443 -1.2269735 -3.3816352 -4.2309017 -4.7901049][-2.924397 -2.3323464 -1.5043044 -0.21143007 1.0210352 2.4434481 3.5943689 3.7975426 3.8141003 1.8772297 0.38187218 -1.4316816 -3.5821114 -4.5638089 -5.0488358][-2.8304391 -2.1802773 -1.8010759 -0.74677324 0.30622816 1.5605278 2.4907436 2.6975403 2.5449619 0.90033245 -0.40984631 -2.2318783 -3.9617352 -5.0553045 -5.7236686][-4.4628782 -4.0529609 -3.6196251 -2.9509521 -2.2592831 -0.96514034 0.11767769 0.28035498 0.18443394 -1.3252854 -2.5587296 -4.1490498 -5.6396623 -6.1622419 -6.4170971][-5.8505964 -5.8744917 -5.8901911 -5.1589231 -4.2769089 -3.3835974 -2.6952758 -2.5366907 -2.5080028 -3.4240098 -4.0629082 -5.3022037 -6.0773153 -6.595181 -6.8794665][-7.4735575 -7.4176846 -7.2227507 -7.0667515 -6.5777092 -5.4437885 -4.993423 -4.8028364 -4.70025 -5.1389408 -5.5140114 -6.2781444 -6.7665114 -6.8966579 -6.5302548][-7.143856 -6.9316216 -6.9413147 -6.8044724 -6.4600611 -5.798912 -5.4392319 -5.48287 -5.6422019 -5.6755152 -5.6424789 -6.0320673 -6.4386296 -6.6439428 -6.6326365][-7.4270372 -7.5187216 -7.7290006 -7.8487654 -7.7457018 -7.0358171 -6.4209127 -6.5333347 -6.5884447 -6.5626073 -6.813015 -6.8836374 -6.7498765 -6.5709586 -6.4753733]]...]
INFO - root - 2017-12-15 23:19:29.775734: step 72710, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 46h:56m:43s remains)
INFO - root - 2017-12-15 23:19:36.190015: step 72720, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 45h:12m:14s remains)
INFO - root - 2017-12-15 23:19:42.588531: step 72730, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 46h:27m:37s remains)
INFO - root - 2017-12-15 23:19:48.941924: step 72740, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 46h:44m:34s remains)
INFO - root - 2017-12-15 23:19:55.467388: step 72750, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 45h:33m:46s remains)
INFO - root - 2017-12-15 23:20:01.970635: step 72760, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 47h:23m:53s remains)
INFO - root - 2017-12-15 23:20:08.454827: step 72770, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 45h:48m:02s remains)
INFO - root - 2017-12-15 23:20:14.850537: step 72780, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 45h:24m:53s remains)
INFO - root - 2017-12-15 23:20:21.305072: step 72790, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 47h:13m:00s remains)
INFO - root - 2017-12-15 23:20:27.626160: step 72800, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 46h:14m:19s remains)
2017-12-15 23:20:28.195069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9967265 -5.1236444 -5.093895 -5.3452854 -5.1363182 -5.1132669 -4.6859484 -4.2116375 -3.3256516 -3.7206008 -4.897953 -5.8881617 -6.9308357 -8.0216589 -9.0090828][-7.1914072 -7.3463545 -7.6505218 -7.9149771 -7.9677272 -7.7243075 -7.0310955 -6.382905 -5.5198708 -5.303371 -6.1769385 -7.4666195 -8.1746941 -9.3851471 -9.9417286][-8.1496239 -7.7459598 -7.7678576 -7.7151122 -7.5427518 -6.8578949 -5.9574804 -5.1359644 -4.1199684 -4.1434965 -5.418541 -6.6521893 -7.5605912 -8.2761993 -8.7725506][-6.9188728 -6.949914 -6.8490338 -6.4235139 -6.0340118 -4.9960556 -3.5739064 -2.8332996 -2.1796279 -2.4727821 -4.0001726 -5.4333172 -6.6150122 -7.6223617 -7.9050026][-7.3322725 -6.721962 -5.45103 -4.5384307 -3.4347343 -2.32405 -1.1943741 -0.5827322 -0.022096634 -0.62037611 -2.3106947 -3.9807594 -5.4397316 -6.4851847 -6.9686093][-5.2319574 -4.8234868 -4.2004395 -3.1296954 -1.5402293 -0.022882938 1.3689756 1.9061012 2.1917982 1.2574472 -0.5837245 -2.0852656 -3.34312 -4.5995369 -5.1764345][-4.6147585 -3.5673914 -2.3026896 -0.76471186 0.71603489 1.8476238 2.6944485 2.8855581 2.7764397 1.516346 -0.52788639 -2.1351504 -3.5091572 -4.6411562 -5.3026934][-2.653121 -1.7299681 -0.60590696 0.85389614 2.2165146 2.7243156 2.8441744 2.6159096 2.1765594 0.82491684 -1.0856891 -2.5334921 -3.77235 -4.7995286 -5.3395724][-1.885201 -0.86881828 0.532712 1.5712624 2.3367996 2.71836 2.5189657 2.2150326 1.807497 0.45685196 -1.2895174 -2.4988446 -3.5132456 -4.0748754 -4.4861689][-1.9224076 -0.95725584 -0.013800621 0.87143707 1.5575171 1.8844767 1.4402294 0.83857918 0.25677347 -1.1048603 -2.7810788 -3.6628509 -4.1387911 -4.6987495 -4.9799471][-4.2626371 -3.3477807 -2.9145117 -2.0388536 -1.4459915 -1.3620625 -1.7272596 -2.1612439 -2.5216579 -3.5266757 -4.73135 -4.9936771 -5.2429485 -5.5341058 -5.6921787][-5.8639278 -4.9423189 -4.3991466 -3.957958 -3.6442075 -3.4198694 -3.501667 -3.7745793 -3.8168306 -4.4400415 -5.6315002 -5.6669273 -5.762001 -5.9874787 -6.1100416][-7.5242672 -7.1492004 -6.624486 -6.5138278 -6.4372773 -5.9567995 -5.6190767 -5.3036447 -4.9568319 -5.1274061 -5.7019882 -5.452754 -5.5093393 -5.2060986 -5.1625671][-5.8633223 -5.9323487 -6.0811138 -6.0573545 -5.803164 -5.3802795 -5.0098982 -4.6641455 -4.531601 -4.6356153 -4.8328333 -4.8104515 -4.510746 -4.4752254 -4.7537646][-5.0889225 -5.1169119 -5.3953085 -5.8397946 -5.9834847 -5.8911853 -5.6692767 -5.4698324 -5.381072 -5.17402 -5.168396 -5.1807919 -5.1159115 -4.9404163 -4.543189]]...]
INFO - root - 2017-12-15 23:20:34.578111: step 72810, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 45h:35m:37s remains)
INFO - root - 2017-12-15 23:20:41.014452: step 72820, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 45h:39m:34s remains)
INFO - root - 2017-12-15 23:20:47.420020: step 72830, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 45h:05m:13s remains)
INFO - root - 2017-12-15 23:20:53.781783: step 72840, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:43m:24s remains)
INFO - root - 2017-12-15 23:21:00.140231: step 72850, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:47m:54s remains)
INFO - root - 2017-12-15 23:21:06.578827: step 72860, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 45h:30m:23s remains)
INFO - root - 2017-12-15 23:21:13.061618: step 72870, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 46h:33m:06s remains)
INFO - root - 2017-12-15 23:21:19.477931: step 72880, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 46h:21m:19s remains)
INFO - root - 2017-12-15 23:21:25.824904: step 72890, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:08m:21s remains)
INFO - root - 2017-12-15 23:21:32.257429: step 72900, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 45h:44m:28s remains)
2017-12-15 23:21:32.880327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.870115 -4.0005479 -4.2485633 -4.3235393 -4.2837439 -4.15504 -3.8154621 -3.7705638 -3.6299338 -4.6627588 -4.6348867 -5.1275539 -5.8755379 -6.0401878 -6.9259696][-3.6907165 -3.8579013 -3.947238 -3.9965529 -4.1169796 -3.9878953 -3.8443296 -4.1121712 -4.1544895 -5.0339966 -5.2070146 -6.0420389 -6.7047734 -7.0866022 -7.8805771][-3.3488498 -3.0244846 -2.7447948 -2.3484793 -2.1385975 -2.1913905 -2.2542391 -2.6369247 -3.0414338 -4.1333852 -4.2107072 -5.16625 -5.70668 -6.1163678 -7.01814][-2.2026043 -1.3616543 -0.7311368 -0.53954458 -0.47618437 -0.35105133 -0.3134551 -0.814796 -1.1541429 -2.6121974 -3.2713714 -4.1194344 -4.7713609 -5.0973015 -5.8444557][-1.6054001 -1.1006699 -0.38805151 0.12131548 0.024645805 0.14394855 0.25012636 -0.021261692 -0.32288456 -1.4593816 -1.8172898 -2.7810478 -3.3163781 -3.8786364 -5.3184791][-1.1776719 -0.27189207 -0.014807224 0.4102459 0.66678524 0.79550076 0.93026638 0.65612793 0.35744667 -0.84927511 -1.1526308 -1.8265634 -2.4800296 -3.0233302 -4.2646656][-1.2013144 -0.21047926 0.551384 1.2031002 1.4209404 1.548934 1.7197647 1.4269228 1.1410055 -0.13341618 -0.55848026 -1.3074636 -2.0281463 -2.6412015 -3.982024][-0.49538422 0.26385546 0.88659668 1.824234 2.3042154 2.1720839 2.1610126 1.9638987 1.8417435 0.53369713 -0.00982523 -1.0569348 -1.9501128 -2.5031347 -3.7200615][-0.86777353 -0.06648016 0.38354588 0.87188339 1.4291887 1.5628614 1.436799 1.4554033 1.720027 0.41924381 -0.322145 -1.3665342 -2.406764 -3.1002703 -4.4375353][-1.922843 -1.4110546 -0.90706158 -0.221035 0.24905872 0.45765591 0.651741 0.8309145 1.194231 0.13055801 -0.73334694 -2.2386293 -3.4109039 -3.8227963 -4.9591923][-3.2610774 -2.7700272 -2.2473097 -1.7653604 -1.3058839 -1.2314343 -1.105207 -0.97510052 -0.902195 -2.0918036 -2.682714 -3.7515039 -4.7671394 -4.9890518 -5.7714968][-4.1834116 -4.0774355 -4.0174584 -4.0105119 -3.7190053 -3.3390656 -3.3702331 -3.2985015 -3.2077675 -4.1543016 -4.8005109 -5.174202 -5.55831 -5.5352688 -5.9444742][-6.0966382 -5.8637137 -5.6508942 -5.6137514 -5.5737848 -5.4024224 -5.3884172 -5.182672 -4.9213262 -5.39429 -5.2587118 -5.5683422 -5.9946642 -5.8065548 -5.7713084][-6.7618322 -6.8998251 -6.89998 -6.4574432 -6.2880106 -6.3734818 -6.4124308 -6.6570768 -6.6744881 -7.0899887 -7.0194912 -6.5005641 -6.1914854 -5.3837075 -4.662178][-7.3594594 -7.0778036 -7.2317047 -7.411417 -7.6762476 -7.6057491 -7.5094409 -7.8103576 -7.9958739 -7.9892316 -7.7731576 -7.3226404 -6.7127814 -6.0000296 -5.1283226]]...]
INFO - root - 2017-12-15 23:21:39.373737: step 72910, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 47h:39m:20s remains)
INFO - root - 2017-12-15 23:21:45.800748: step 72920, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 45h:57m:46s remains)
INFO - root - 2017-12-15 23:21:52.309486: step 72930, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 46h:00m:44s remains)
INFO - root - 2017-12-15 23:21:58.746505: step 72940, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 46h:51m:37s remains)
INFO - root - 2017-12-15 23:22:05.089621: step 72950, loss = 0.39, batch loss = 0.27 (12.6 examples/sec; 0.633 sec/batch; 45h:36m:58s remains)
INFO - root - 2017-12-15 23:22:11.524831: step 72960, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 45h:10m:24s remains)
INFO - root - 2017-12-15 23:22:17.972265: step 72970, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:48m:25s remains)
INFO - root - 2017-12-15 23:22:24.505036: step 72980, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 46h:30m:59s remains)
INFO - root - 2017-12-15 23:22:30.969472: step 72990, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 46h:33m:52s remains)
INFO - root - 2017-12-15 23:22:37.404146: step 73000, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 47h:27m:43s remains)
2017-12-15 23:22:38.020807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7466941 -4.0289135 -5.0492039 -5.5933094 -5.8305345 -6.1226768 -6.8040485 -6.9033651 -6.4155526 -6.917244 -7.0688877 -7.3555756 -8.25607 -7.7079988 -5.9058738][-2.548903 -2.821516 -3.3170047 -3.8116095 -4.1511884 -4.3866005 -5.21628 -5.9714093 -6.521471 -7.3236556 -6.9328489 -6.70856 -6.8719187 -6.7003574 -5.9292727][-0.77994347 -1.8179679 -2.4298396 -2.9425216 -2.6448717 -2.5812764 -3.399888 -4.0011005 -4.9436455 -6.1887355 -6.3753633 -6.8820825 -7.4859123 -8.14682 -8.288537][-3.0524697 -2.6280165 -3.0609074 -3.2238674 -3.1150494 -3.0313249 -2.8348556 -3.0180869 -3.8377862 -5.5759773 -6.2397528 -7.5180078 -8.5029793 -8.5696373 -8.4757][-3.4424582 -3.4417949 -3.4285469 -2.7285829 -1.6566248 -1.2414417 -1.2062421 -1.0588007 -1.6593719 -3.1741133 -3.541718 -4.9199467 -7.144031 -8.4022884 -8.5421352][-5.177598 -4.7208824 -4.5936203 -3.4733887 -2.2216382 -1.5576973 -0.34021902 0.686841 0.67724323 0.058278561 -0.2486043 -1.5488653 -3.311193 -5.5232368 -6.5418749][-5.1239262 -4.7943192 -4.1438537 -2.7325888 -1.2319069 0.35432053 1.9003105 1.9367085 1.8232412 1.4249191 1.5884228 0.28920412 -2.0808468 -4.0321665 -4.9577384][-4.9529634 -4.4532866 -3.5900087 -1.9612823 0.43567181 3.0005636 4.5672178 4.66712 4.6511497 2.9213934 2.2762804 2.1452913 0.4031477 -1.8906841 -3.3410039][-5.126791 -4.64625 -4.07335 -2.7193832 -0.688426 1.8660641 3.707449 5.4333925 6.3813181 4.3757439 2.7971668 0.4524622 -1.6918192 -3.5608797 -4.7096205][-6.4541063 -6.070138 -5.3838339 -3.9743304 -2.3133154 -0.56529951 1.0396709 2.8641615 3.9757872 2.8118086 1.7397261 -0.50779009 -3.5765519 -5.1419125 -5.8608279][-7.6717105 -7.6641941 -7.222743 -6.0172129 -4.3473835 -2.7984223 -1.5732083 -0.84218788 0.63006496 0.61598015 0.33059978 -1.3907237 -3.7768362 -6.0540366 -7.4290309][-7.8486218 -7.5387788 -7.8896666 -7.8241024 -7.110127 -5.4350891 -4.1597586 -3.4673896 -2.442421 -2.3779254 -2.3313026 -2.9770417 -4.1997724 -5.5202017 -6.9766769][-8.294487 -7.8842444 -8.228281 -8.4161129 -7.7433643 -7.1008606 -6.7052064 -6.044919 -5.2423792 -5.0423965 -4.6041045 -4.56022 -4.9318075 -5.5702925 -6.7607651][-8.4241123 -8.0825338 -7.9171939 -7.720284 -8.0383291 -7.5866036 -7.4388666 -7.2492161 -6.9264059 -6.6676726 -6.3468738 -6.5661397 -6.8278828 -6.2304382 -5.8601322][-8.938015 -9.1917658 -9.2900581 -8.6599178 -7.8524518 -7.4513631 -7.4653382 -7.8706255 -8.1089525 -8.1232481 -7.8934722 -7.5091748 -6.9955144 -7.2993469 -6.9606037]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:22:44.546216: step 73010, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 46h:03m:13s remains)
INFO - root - 2017-12-15 23:22:51.060137: step 73020, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 46h:07m:48s remains)
INFO - root - 2017-12-15 23:22:57.547150: step 73030, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 47h:34m:27s remains)
INFO - root - 2017-12-15 23:23:04.007833: step 73040, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 45h:41m:52s remains)
INFO - root - 2017-12-15 23:23:10.491144: step 73050, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 46h:11m:34s remains)
INFO - root - 2017-12-15 23:23:16.941290: step 73060, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 45h:20m:50s remains)
INFO - root - 2017-12-15 23:23:23.401310: step 73070, loss = 0.25, batch loss = 0.13 (11.9 examples/sec; 0.675 sec/batch; 48h:38m:13s remains)
INFO - root - 2017-12-15 23:23:29.797650: step 73080, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 46h:53m:56s remains)
INFO - root - 2017-12-15 23:23:36.257220: step 73090, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 45h:57m:25s remains)
INFO - root - 2017-12-15 23:23:42.608504: step 73100, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 45h:44m:50s remains)
2017-12-15 23:23:43.137659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2082052 -3.1273918 -4.3926373 -4.6868572 -4.6349411 -3.4561543 -2.1407442 -1.2238474 -0.6606946 -1.1069245 -2.5356331 -5.121644 -6.9593306 -8.2127209 -8.7928228][-1.5922666 -2.5046206 -3.571919 -5.0908756 -5.7092247 -5.3309727 -4.0936604 -2.3460555 -0.91978264 -1.070375 -2.674901 -5.120965 -7.3372774 -8.6169109 -8.6615047][-1.5963449 -2.1825371 -3.488893 -4.6025696 -5.2331061 -5.3338337 -4.4625421 -3.4112558 -2.6673055 -2.1357198 -3.190166 -5.4539385 -6.9289389 -8.3490286 -9.1115875][-2.5354843 -2.8060746 -3.4914584 -3.6869922 -3.1176724 -2.6859064 -2.4167914 -2.2126441 -1.6447506 -2.2190919 -4.1848974 -6.2538314 -7.9918947 -8.692359 -8.6399994][-2.5781221 -2.1399198 -2.6803751 -2.4763088 -2.3079305 -0.85072565 0.84313774 1.4488955 1.4255152 -0.17073011 -2.9281793 -6.1080546 -8.0071621 -8.7189264 -8.9593391][-2.3971004 -2.448173 -2.0927773 -1.4122396 -0.64033175 1.3770561 3.3158903 3.7856579 3.6879854 1.9935322 -1.1430216 -4.5049429 -7.3252211 -8.583539 -8.5938492][-2.3939357 -2.1456876 -1.3251896 -0.10773516 0.71850109 2.5604544 4.9611874 5.7160769 5.6544924 3.839529 0.40124798 -3.6728268 -6.3163371 -7.6240325 -8.1351891][-2.5625033 -1.603826 -0.95632887 0.5288763 2.1111355 3.643321 4.8553648 5.9299498 6.1937828 4.1004934 0.56970787 -3.121006 -5.5705032 -6.9509711 -7.1234455][-2.5765066 -1.7200131 -1.3260741 -0.56108284 0.731019 2.1942577 3.3908272 4.5518141 4.836525 2.9284878 -0.10921717 -3.6157498 -5.7083826 -6.6728277 -7.2405][-3.3454318 -3.1316366 -2.3246922 -1.6473236 -1.3129354 -0.64180374 -0.063349247 0.82207012 1.3532295 0.23524904 -1.8096409 -4.6259413 -6.6198092 -7.5049505 -7.5370913][-5.706213 -5.2740846 -4.2022047 -3.3808479 -2.8912354 -2.7087493 -2.6059337 -2.7713037 -3.0921135 -3.9809844 -5.7159958 -7.418129 -8.05426 -8.3356171 -8.0656452][-7.4251494 -6.5409489 -5.5639324 -5.0783496 -4.5860863 -4.5613379 -4.9793787 -5.2377453 -5.3729248 -5.9036374 -7.2868061 -7.7579565 -8.03944 -8.7198114 -8.6730261][-7.6386557 -7.5964746 -7.154532 -6.5256 -6.1823311 -6.4244137 -6.4361858 -6.6191854 -6.4052706 -6.3228316 -6.976315 -6.9271679 -7.1044559 -7.7683363 -7.628943][-6.9995532 -7.213151 -7.2301192 -7.0561171 -7.0755095 -6.8336372 -6.6758461 -6.4823666 -5.9170885 -5.89382 -6.2562151 -5.7605133 -5.5940838 -5.8949857 -5.9418907][-6.991714 -7.2444873 -7.3025541 -7.4771357 -7.5013943 -7.5958977 -7.9533453 -7.6508813 -7.3600712 -6.8609447 -6.2221184 -5.9351864 -5.5581884 -5.4944377 -5.9092374]]...]
INFO - root - 2017-12-15 23:23:49.519635: step 73110, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 46h:12m:48s remains)
INFO - root - 2017-12-15 23:23:55.886423: step 73120, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 46h:33m:04s remains)
INFO - root - 2017-12-15 23:24:02.413525: step 73130, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:46m:18s remains)
INFO - root - 2017-12-15 23:24:08.841086: step 73140, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.668 sec/batch; 48h:05m:24s remains)
INFO - root - 2017-12-15 23:24:15.253908: step 73150, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 45h:54m:42s remains)
INFO - root - 2017-12-15 23:24:21.633603: step 73160, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.628 sec/batch; 45h:16m:15s remains)
INFO - root - 2017-12-15 23:24:28.113118: step 73170, loss = 0.27, batch loss = 0.16 (11.7 examples/sec; 0.684 sec/batch; 49h:14m:48s remains)
INFO - root - 2017-12-15 23:24:34.549062: step 73180, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 46h:14m:43s remains)
INFO - root - 2017-12-15 23:24:40.916292: step 73190, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:26m:00s remains)
INFO - root - 2017-12-15 23:24:47.337649: step 73200, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 46h:01m:21s remains)
2017-12-15 23:24:47.875665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7472377 -3.4319592 -4.2434912 -4.6149988 -4.312006 -4.1019278 -4.0129886 -3.8888266 -3.4480224 -3.2577691 -4.0088482 -5.6676755 -6.6054025 -6.4257512 -7.4055362][-3.1604509 -3.9438069 -4.8892822 -5.4623795 -5.5762739 -4.8705387 -4.203218 -4.249052 -4.3655839 -4.2027588 -5.0319118 -6.4274535 -7.3374128 -7.31402 -7.8849893][-3.6260672 -4.1986575 -4.7129803 -4.7930994 -5.0625815 -4.9091148 -4.5786419 -4.1214867 -3.655601 -3.7342134 -4.9825277 -6.6636806 -7.52351 -7.8110847 -8.5231781][-4.2902226 -4.7440176 -5.0847239 -4.7270193 -4.2301393 -4.040555 -3.539216 -3.0463376 -2.6191397 -2.5860567 -3.7851267 -5.9433789 -7.1844563 -7.5231147 -8.0537615][-5.3660817 -5.6417685 -5.7431717 -5.3539448 -4.4197807 -3.6806426 -2.4223361 -1.751956 -1.2163553 -1.6324883 -3.3726473 -5.7516556 -7.0446205 -7.5127254 -8.2423115][-7.1988549 -6.828105 -6.1992893 -5.2972608 -3.8731177 -2.6807041 -0.98216534 0.090007782 0.51711655 -0.66123438 -3.0351391 -6.0559082 -7.4178424 -7.439075 -7.877532][-6.9936395 -6.7450447 -6.4156857 -5.0852261 -3.074172 -0.79399872 1.6631269 2.7560406 3.1141911 2.0852633 -0.75817871 -4.3540978 -6.5015664 -6.8441648 -7.3578229][-6.3675318 -6.0640845 -5.3662696 -4.151473 -2.2556105 0.43626118 3.0117712 3.7705812 3.5874624 2.2775364 0.14878941 -2.9346247 -4.8069477 -5.2968912 -5.9339437][-6.3118248 -5.9759607 -5.5828242 -4.3490877 -2.4672842 0.070514679 2.7803688 3.5065365 3.5244169 1.727706 -0.87100363 -3.7132587 -5.3454752 -5.4264565 -6.1691647][-6.3273396 -6.2472482 -5.9833751 -4.933176 -3.5120821 -1.5973687 0.35436058 0.97771168 1.2330151 -0.43345118 -2.6600823 -5.319212 -6.5310063 -5.8875408 -6.6366129][-7.2495828 -7.3739066 -6.9792495 -5.8288479 -4.7699032 -3.5119958 -2.2147789 -1.7076788 -1.3694906 -2.3581538 -3.743542 -5.8047037 -7.1547756 -6.466023 -6.7169795][-7.3046446 -7.2708173 -6.9554987 -6.0505457 -5.4708576 -4.7384996 -3.9042122 -3.6279578 -3.4348936 -3.9943483 -4.2805214 -5.819098 -6.4875083 -6.3496089 -6.6996546][-8.5696707 -8.1793728 -7.4391322 -6.73536 -6.0893507 -6.1534438 -5.6623793 -5.6274939 -5.2769442 -6.0084743 -6.2881012 -6.60747 -6.6562719 -6.6507173 -6.9245563][-8.3068886 -8.2224112 -7.359961 -6.3438826 -5.66799 -5.7717066 -5.3887615 -5.45201 -5.41947 -5.8827176 -6.0082173 -6.3388762 -6.34063 -6.6181889 -6.8775325][-8.9189291 -9.1556 -8.8901339 -7.5736456 -6.8284273 -6.7420621 -6.8058391 -6.9816775 -6.869061 -6.9619565 -7.01767 -6.600647 -6.2906408 -6.030726 -5.7720733]]...]
INFO - root - 2017-12-15 23:24:54.266316: step 73210, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 45h:51m:07s remains)
INFO - root - 2017-12-15 23:25:00.755665: step 73220, loss = 0.23, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 46h:26m:05s remains)
INFO - root - 2017-12-15 23:25:07.188717: step 73230, loss = 0.29, batch loss = 0.18 (11.9 examples/sec; 0.670 sec/batch; 48h:14m:00s remains)
INFO - root - 2017-12-15 23:25:13.646969: step 73240, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 47h:10m:40s remains)
INFO - root - 2017-12-15 23:25:20.141779: step 73250, loss = 0.26, batch loss = 0.15 (11.6 examples/sec; 0.687 sec/batch; 49h:30m:23s remains)
INFO - root - 2017-12-15 23:25:26.623105: step 73260, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 45h:30m:44s remains)
INFO - root - 2017-12-15 23:25:33.011297: step 73270, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 46h:19m:14s remains)
INFO - root - 2017-12-15 23:25:39.405614: step 73280, loss = 0.32, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 44h:47m:10s remains)
INFO - root - 2017-12-15 23:25:45.796093: step 73290, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 45h:21m:00s remains)
INFO - root - 2017-12-15 23:25:52.217606: step 73300, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 45h:57m:07s remains)
2017-12-15 23:25:52.757492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1749063 -3.404861 -4.2257485 -4.4063139 -4.3144555 -3.9859633 -3.9382122 -3.8849189 -3.7537842 -5.9409184 -6.4371395 -7.1326957 -6.9798346 -8.0317965 -7.3730884][-2.5530643 -2.502172 -3.2106252 -3.8484967 -4.2225671 -4.2949743 -4.2615967 -4.0394683 -3.5321569 -5.5762587 -5.8577838 -6.2877588 -6.3519464 -7.5873303 -7.2764115][-1.1997843 -1.2301269 -2.0595918 -2.5487642 -2.7185907 -3.0859342 -3.7917287 -3.735456 -3.4377871 -5.6456203 -5.949738 -6.107008 -5.534359 -6.4448605 -6.3000989][-1.3618269 -0.55749893 -0.42456865 -0.61445332 -1.154253 -0.852365 -1.3963876 -1.6775141 -2.0587597 -5.0429678 -6.0870371 -6.44022 -5.8753233 -6.313468 -5.4492626][-2.16747 -0.8241291 -0.29139996 0.12287426 0.47294617 1.1452866 0.357831 0.012578487 -0.64367867 -4.2010403 -5.4433913 -6.3169188 -5.9686317 -6.7439108 -5.5076451][-4.499341 -2.6439433 -1.4630356 -0.49797344 0.081059456 1.6996355 2.2799664 1.8365154 0.76427078 -3.324995 -4.8900824 -6.5966892 -6.6318884 -7.07897 -5.8360386][-4.4292912 -2.5804696 -1.0791769 0.63856792 1.9569349 3.714654 4.2306261 4.2048531 3.0142555 -1.5204492 -4.108964 -6.2262611 -6.25778 -7.2323813 -5.8271422][-3.7521551 -1.953866 -0.787282 0.88690567 1.8234406 3.779171 4.7179604 5.2707014 4.3908434 -0.025444984 -2.7346239 -5.1478367 -5.538959 -6.9992723 -5.8070993][-4.0210834 -2.3191123 -1.4293947 0.11320877 0.86696625 1.7930202 2.4649506 3.4628334 3.2905836 -0.93001938 -3.1660514 -5.0051408 -5.6505251 -7.0705619 -5.7758808][-4.5805225 -3.2987537 -2.1779404 -0.43929195 0.067739487 0.695878 1.4378328 2.2872066 1.9696903 -2.1100459 -4.2014909 -5.2515507 -5.7351422 -7.1867843 -6.633008][-6.0587411 -5.3194456 -4.5098004 -2.9258847 -2.1806741 -1.6471658 -1.3066058 -0.19146729 0.52333736 -3.2666469 -5.5385861 -6.367949 -7.2722096 -8.5770836 -7.8712769][-7.7716694 -6.9839168 -6.0211992 -4.915905 -4.3959455 -4.0024228 -3.6546025 -3.0076981 -2.5532589 -4.902791 -5.8329115 -6.3007631 -6.8912964 -8.5733871 -8.5018883][-9.2307758 -9.0402794 -8.2738352 -7.1841869 -6.1625876 -5.3952789 -5.4902773 -5.2885489 -4.8828249 -6.9475222 -7.6407943 -7.5056696 -7.4694581 -7.9530554 -7.875515][-9.9122858 -9.56183 -8.8131161 -8.5532818 -7.8651567 -6.7872014 -6.522326 -6.5825863 -6.7686939 -7.525641 -7.3765154 -7.3133965 -6.82507 -7.22631 -7.0351696][-9.8166637 -9.201849 -8.6027775 -8.1224642 -8.0811214 -7.8521452 -7.8984189 -7.6069808 -7.0326915 -7.0269771 -6.9549332 -6.7207494 -6.24395 -5.9875283 -5.6833434]]...]
INFO - root - 2017-12-15 23:25:59.182655: step 73310, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 45h:41m:39s remains)
INFO - root - 2017-12-15 23:26:05.558365: step 73320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 45h:53m:19s remains)
INFO - root - 2017-12-15 23:26:12.002948: step 73330, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 45h:44m:24s remains)
INFO - root - 2017-12-15 23:26:18.448505: step 73340, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 47h:01m:23s remains)
INFO - root - 2017-12-15 23:26:24.976944: step 73350, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 46h:11m:25s remains)
INFO - root - 2017-12-15 23:26:31.329505: step 73360, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 45h:12m:30s remains)
INFO - root - 2017-12-15 23:26:37.728139: step 73370, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:39m:44s remains)
INFO - root - 2017-12-15 23:26:44.084007: step 73380, loss = 0.24, batch loss = 0.12 (13.0 examples/sec; 0.618 sec/batch; 44h:27m:47s remains)
INFO - root - 2017-12-15 23:26:50.538798: step 73390, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 46h:09m:46s remains)
INFO - root - 2017-12-15 23:26:57.034362: step 73400, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 46h:55m:35s remains)
2017-12-15 23:26:57.616951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0994406 -4.0561485 -3.9612358 -3.5878673 -3.2305288 -2.8585525 -2.08314 -1.8472342 -1.8800616 -3.8508565 -5.7771716 -6.8998022 -7.7007351 -9.687191 -9.618906][-4.9912262 -5.0071507 -4.5833035 -4.6798363 -4.3854656 -3.665359 -3.2054238 -2.5583029 -2.2459264 -4.571909 -6.1215725 -7.1932988 -8.4793634 -9.95896 -9.7142782][-5.0447598 -4.2387085 -3.9105952 -3.3898959 -2.6777072 -2.4680848 -2.1611757 -1.7275157 -1.6477323 -3.3808274 -4.7009077 -6.1962266 -7.2173953 -8.778841 -9.1902533][-3.4464331 -3.1430655 -2.5010614 -2.0112472 -1.6619687 -1.0887794 -0.69068956 -0.45338249 0.24464846 -1.2686405 -2.6825128 -3.8099878 -5.4159822 -7.0328288 -7.051734][-3.4463134 -2.2682524 -1.5360417 -1.0772481 -0.25245905 0.8510437 1.7694731 2.3465385 2.3177156 0.45344353 -1.026988 -2.8971581 -5.0302057 -6.6331024 -7.0248404][-3.110002 -2.2437911 -1.2155204 -0.010581017 1.2040596 2.4577398 3.3696909 4.0585537 4.4957008 2.335681 -0.05907917 -1.9497204 -3.9340537 -5.9929538 -6.4446592][-3.1880012 -1.9772573 -0.85542583 0.4577713 1.9687691 3.4428606 4.3197861 5.097044 5.636879 2.9695578 0.41862297 -2.1290479 -4.5221167 -6.0431361 -6.1340041][-3.576952 -2.110661 -0.84990263 0.8339138 2.5928602 4.1274643 5.3081264 5.8357611 5.4894247 2.6561728 0.012832165 -2.4761114 -4.76251 -6.2419071 -6.2497692][-3.8087437 -2.8340721 -1.4258523 0.44948006 2.2216539 4.0449228 4.9354544 4.869976 4.0970478 1.2485008 -0.84819794 -2.7636151 -4.8851876 -6.1799059 -6.298985][-3.5060844 -2.6962972 -1.6875849 -0.11860895 1.4205589 2.3309994 2.8292007 3.095437 2.0766239 -0.76937532 -2.1703057 -4.0582652 -4.9623761 -6.2417994 -5.8909988][-4.2773857 -3.6898198 -3.2658944 -2.1522765 -1.0966368 -0.050951004 0.75751114 0.28639317 -0.32818365 -2.5294852 -4.382174 -5.8285036 -6.1546788 -6.6490932 -6.2192025][-5.7094646 -5.3873596 -5.0483685 -4.4384203 -3.931689 -2.7930703 -2.308867 -2.6524744 -2.8144569 -4.4640479 -5.7213917 -6.5026851 -6.6735635 -7.0319581 -6.6560688][-6.9761071 -6.7838645 -6.56649 -6.0125284 -5.8065162 -4.9019041 -4.3249331 -4.2500887 -4.3579988 -5.514163 -6.3179412 -6.7033858 -6.8245258 -6.6829753 -5.8045697][-8.06849 -7.7433157 -7.2101536 -6.7382746 -6.17391 -5.1877518 -4.6530924 -4.6378736 -4.7970772 -5.4833221 -5.7278943 -6.0797744 -6.1841555 -6.037837 -5.4180851][-8.3695126 -8.5576744 -7.8438063 -7.111918 -6.4517241 -5.5942774 -5.1837692 -5.3242922 -5.5908642 -5.5809426 -5.5693398 -5.396018 -5.4495096 -5.5477653 -5.3331785]]...]
INFO - root - 2017-12-15 23:27:04.001262: step 73410, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 45h:39m:02s remains)
INFO - root - 2017-12-15 23:27:10.431245: step 73420, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 46h:23m:02s remains)
INFO - root - 2017-12-15 23:27:16.865695: step 73430, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 45h:05m:56s remains)
INFO - root - 2017-12-15 23:27:23.251559: step 73440, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.640 sec/batch; 46h:04m:05s remains)
INFO - root - 2017-12-15 23:27:29.646968: step 73450, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 47h:42m:48s remains)
INFO - root - 2017-12-15 23:27:36.109450: step 73460, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 46h:45m:09s remains)
INFO - root - 2017-12-15 23:27:42.484105: step 73470, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 46h:41m:48s remains)
INFO - root - 2017-12-15 23:27:48.853769: step 73480, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 45h:55m:09s remains)
INFO - root - 2017-12-15 23:27:55.324982: step 73490, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:41m:32s remains)
INFO - root - 2017-12-15 23:28:01.730475: step 73500, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 45h:36m:06s remains)
2017-12-15 23:28:02.268598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8985434 -5.6455479 -4.84017 -5.1937103 -5.6915579 -5.7423997 -6.2505441 -5.4543052 -4.8499947 -5.7830391 -6.9660649 -7.1757331 -7.3039927 -7.9605174 -9.4541149][-4.6359015 -5.4740787 -4.6887121 -4.4944792 -4.9073286 -5.1423035 -4.8015361 -4.357625 -3.8130665 -4.394146 -4.8102741 -6.02261 -6.9991946 -7.6447554 -8.87629][-2.9082627 -3.6555371 -4.3824453 -3.9124708 -3.3392854 -3.0298634 -3.2372503 -2.7217889 -1.6002402 -2.4793777 -3.7227886 -4.004137 -5.2358747 -6.4029117 -6.569664][-2.3614311 -1.9495215 -1.2025914 -1.595396 -1.6470723 -1.0438757 -0.67497015 0.34283543 0.86290455 -0.0059757233 -0.94403028 -1.8039589 -2.8922558 -4.3611593 -5.7697639][1.1088324 0.52951527 -0.53255177 0.42745876 1.315505 1.4161263 1.9007425 2.7794933 2.7068138 1.8649483 0.92166138 -0.93934345 -2.8640671 -3.5129981 -4.2739925][0.25409698 1.7759581 1.6651011 1.1673212 1.52981 2.7049313 3.350894 3.264535 3.8856783 2.8513384 1.4996929 0.07878685 -1.3725638 -2.1569114 -3.3255363][0.53218079 1.7462568 3.187458 2.4995661 1.9223146 3.58255 5.0778475 4.8406448 4.5906963 3.2752647 1.5356884 -0.095312119 -1.1555896 -2.1400676 -3.9645395][2.5843544 2.844471 3.4110374 4.1137257 4.1353617 3.8748331 4.5142593 5.281456 5.3699503 3.1601734 1.5533295 0.2927227 -0.7888875 -1.9682436 -3.6049132][3.846036 3.81814 3.2917137 3.7694044 4.8255043 4.80634 4.9289169 5.6191721 5.8148556 3.548337 1.2547731 -0.39761686 -1.4523497 -2.5686474 -4.0458784][2.9444389 2.9108648 2.9553852 3.4829197 4.0216494 4.3015289 4.8267059 4.9000645 4.3562088 2.6669397 1.0428591 -0.95924044 -2.7661834 -3.4601736 -4.0442305][-0.87096119 0.040041447 0.81545925 0.74100971 0.78068924 0.65138817 1.0708084 1.3640776 1.3854408 -0.67043638 -2.2650661 -3.3748808 -4.4770489 -4.9033828 -5.2641335][-4.8723149 -3.2712164 -2.185421 -2.7682958 -3.1062441 -2.5403452 -2.3989406 -2.4655604 -2.4299569 -3.644464 -4.31536 -4.3216753 -5.5251989 -7.0445809 -7.4478812][-6.2639718 -5.919899 -5.567812 -5.655405 -5.238987 -5.0313206 -4.6940985 -4.6706095 -4.575036 -5.4838996 -6.4984212 -6.1832318 -6.4670477 -7.0359631 -6.7924066][-4.4657226 -5.7519207 -6.6708984 -5.9784279 -5.352356 -5.4710693 -5.3555136 -5.1792679 -5.4213562 -5.6114731 -5.6384745 -5.3170061 -5.19049 -5.3490467 -5.04183][-4.7920585 -5.6688476 -5.8893743 -5.9826975 -6.3039317 -5.9520826 -5.392695 -5.4002666 -5.6956778 -5.3995876 -4.9356656 -4.6332092 -4.7331738 -4.9343605 -5.0295606]]...]
INFO - root - 2017-12-15 23:28:08.721436: step 73510, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:39m:52s remains)
INFO - root - 2017-12-15 23:28:15.152221: step 73520, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 47h:07m:07s remains)
INFO - root - 2017-12-15 23:28:21.608213: step 73530, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 46h:27m:14s remains)
INFO - root - 2017-12-15 23:28:28.036193: step 73540, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 46h:30m:53s remains)
INFO - root - 2017-12-15 23:28:34.482889: step 73550, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 44h:48m:15s remains)
INFO - root - 2017-12-15 23:28:40.856926: step 73560, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 45h:12m:10s remains)
INFO - root - 2017-12-15 23:28:47.194752: step 73570, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 45h:18m:37s remains)
INFO - root - 2017-12-15 23:28:53.539463: step 73580, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 44h:58m:09s remains)
INFO - root - 2017-12-15 23:28:59.940069: step 73590, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 45h:18m:15s remains)
INFO - root - 2017-12-15 23:29:06.431344: step 73600, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 47h:25m:42s remains)
2017-12-15 23:29:06.963950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8070416 -3.0202775 -3.3705444 -3.7217281 -3.6006689 -3.1209311 -2.6029921 -1.9042921 -1.243216 -2.1914005 -3.2914371 -4.8710403 -6.1861939 -6.7894521 -7.2588716][-3.6125593 -3.7759218 -4.2053471 -4.4572859 -4.609272 -4.1239681 -3.3746247 -2.6176605 -1.9709725 -3.1369414 -4.3803735 -5.8226571 -7.1414962 -8.0036888 -8.63548][-4.7129526 -4.395587 -4.4733515 -4.2776451 -3.9955268 -3.3923407 -2.9437375 -2.7621102 -2.3961024 -3.6781816 -5.1108394 -6.767735 -7.847168 -8.5957394 -9.0878334][-5.5913415 -5.1394639 -4.7972269 -3.8587964 -2.7931042 -1.6665673 -0.80201912 -0.64324856 -0.75352955 -2.5606103 -4.3475003 -6.5117016 -7.9551859 -8.767889 -9.1479244][-6.4679341 -5.6555195 -4.7430048 -3.384913 -1.8647995 -0.032179356 1.2854061 1.5148621 1.1978388 -1.2101212 -3.4155574 -6.0418286 -7.9415255 -8.8675528 -9.3624249][-7.6383591 -6.9205065 -5.7483082 -3.3473473 -0.91639137 1.1406984 2.910533 3.1041689 2.8041945 0.20397472 -2.4286757 -5.426053 -7.6837826 -8.9892168 -9.699501][-7.660253 -6.7166595 -5.3876171 -2.9683809 0.084274769 2.7252531 4.9386091 5.3042068 5.0399952 2.0875502 -0.98247671 -4.2592578 -6.6221142 -8.1197691 -8.9209414][-7.25941 -6.4379311 -5.0205917 -2.4932294 0.26064777 3.0039215 5.1536064 5.615531 5.5911751 2.7059431 -0.23154688 -3.5239587 -5.9627943 -7.4407754 -8.0969114][-7.4515028 -6.9975114 -6.0557241 -4.1428781 -1.9080009 0.52036476 2.5910721 3.2460442 3.4669313 0.95349979 -1.6028924 -4.4285088 -6.3459105 -7.38762 -7.9830117][-8.4364882 -7.7538581 -6.9470134 -5.4477205 -3.6638618 -2.0711589 -0.44666243 0.027567387 0.44051743 -1.7654786 -3.9375412 -6.1623888 -7.7426372 -8.0685768 -8.1899624][-9.5394125 -9.2755413 -8.7795143 -7.5266562 -6.2627292 -4.5507603 -3.1938906 -3.1405187 -2.9763231 -4.79165 -6.2905583 -7.7583323 -8.7696 -8.65812 -8.5558052][-9.8642 -9.671627 -9.3535986 -8.486062 -7.5961628 -6.5778527 -5.6185551 -5.1286612 -4.9484324 -6.0995317 -6.9909487 -7.971292 -8.5468588 -8.1755085 -7.995327][-9.2848854 -9.3111982 -9.1431351 -8.395957 -7.5247474 -6.6000867 -5.6902142 -5.8221788 -5.8709922 -6.5401664 -7.0190797 -7.4388452 -7.7173419 -7.2385464 -6.9793315][-8.374896 -8.228117 -8.0558338 -7.3786216 -6.7905512 -6.3016129 -5.7086458 -5.6209393 -5.6394081 -6.0909271 -6.2895403 -6.4401717 -6.5677271 -6.2769189 -6.1202049][-8.2989178 -8.123415 -8.01589 -7.6464553 -7.272954 -6.6732497 -6.2706585 -6.4431844 -6.6164703 -6.4213443 -6.1210818 -5.990077 -5.7073641 -5.4412661 -5.2865672]]...]
INFO - root - 2017-12-15 23:29:13.319206: step 73610, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 45h:45m:14s remains)
INFO - root - 2017-12-15 23:29:19.744836: step 73620, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 45h:34m:49s remains)
INFO - root - 2017-12-15 23:29:26.140469: step 73630, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:22m:39s remains)
INFO - root - 2017-12-15 23:29:32.500771: step 73640, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 45h:26m:46s remains)
INFO - root - 2017-12-15 23:29:38.892411: step 73650, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 46h:08m:25s remains)
INFO - root - 2017-12-15 23:29:45.401532: step 73660, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 47h:25m:18s remains)
INFO - root - 2017-12-15 23:29:51.770039: step 73670, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:45m:29s remains)
INFO - root - 2017-12-15 23:29:58.205015: step 73680, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 46h:19m:23s remains)
INFO - root - 2017-12-15 23:30:04.556900: step 73690, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 45h:22m:43s remains)
INFO - root - 2017-12-15 23:30:11.064976: step 73700, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 46h:17m:44s remains)
2017-12-15 23:30:11.708807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2016711 -4.3873205 -4.3800011 -5.0310354 -5.4980116 -5.8712873 -5.7890964 -5.6030178 -5.1516476 -5.1848083 -6.5310297 -6.0527248 -6.9105496 -7.2548289 -7.4116378][-4.664227 -4.4654365 -4.8980742 -4.8410487 -4.8475718 -5.1285696 -5.2547545 -5.3131428 -5.3442183 -5.3449664 -6.968915 -6.9679174 -7.6764054 -8.099061 -8.1852808][-5.5554647 -5.4202633 -4.8227048 -4.4054341 -3.9235928 -3.902879 -3.5387244 -3.602571 -4.0952072 -4.8958206 -6.8266492 -6.8667049 -7.7173848 -8.0072947 -8.1453619][-5.0815926 -5.0736094 -4.6204638 -3.8759556 -2.7004561 -2.1055479 -1.8896542 -1.4355526 -1.725698 -2.4307504 -5.2151794 -5.6833897 -6.8367424 -7.401443 -8.0030718][-5.4820485 -4.6043339 -3.7625287 -2.671031 -1.8404984 -0.73397636 0.31628418 0.60837841 0.24116898 -0.65050268 -3.5652571 -4.15572 -5.1985707 -6.1232061 -7.0594907][-4.9629784 -4.25593 -2.7410374 -1.3813748 -0.28670311 0.66967964 1.8151398 1.7903309 1.4666653 0.71154594 -1.8847704 -2.5835867 -4.0305424 -4.8189678 -6.1694322][-4.1359396 -3.4772515 -2.1649327 -1.0933952 0.3638773 1.4206324 2.4878798 2.4885597 2.0527792 0.76322174 -1.8291745 -1.9063988 -3.3289418 -4.3755093 -5.7547474][-3.5364761 -3.3462224 -2.2846241 -1.1157312 0.3496418 1.2348299 2.0287476 2.0803633 1.9875221 0.9866724 -1.7108617 -2.1753435 -3.8054008 -4.4658403 -5.3505793][-4.69363 -3.7865624 -2.5404615 -1.6399102 -0.684453 -0.042598248 0.55533409 0.81918907 1.0557861 0.50713539 -1.9586983 -2.4120264 -4.1589279 -5.0367355 -5.3776217][-5.5299435 -5.0055056 -3.8418555 -2.8586864 -1.9750185 -1.5066857 -1.458992 -1.3974423 -1.1604629 -1.9209909 -3.6881294 -3.769706 -4.6105657 -5.5087194 -6.068562][-6.938098 -6.1439762 -5.8175192 -4.7363729 -3.9070523 -3.4929581 -3.2759776 -3.7619987 -3.9322937 -4.6029859 -6.0552182 -5.5948839 -5.6674662 -6.1976552 -7.0056148][-8.5393 -7.6086507 -6.5851417 -6.4163456 -5.8812971 -5.3973293 -4.8459516 -5.0561929 -5.0432072 -5.5705805 -6.8680782 -6.657177 -6.5874052 -6.6141543 -6.6699977][-8.8831272 -9.1613169 -8.742012 -7.6462555 -6.6224113 -6.120821 -6.1487908 -6.1881466 -5.8468866 -5.8446951 -6.9494634 -6.1643786 -5.7316017 -5.7127695 -6.1098104][-8.4198561 -7.9476666 -7.7833695 -7.8861718 -7.317636 -6.5679278 -6.4275064 -6.2577119 -6.0934181 -5.7429667 -6.2792645 -6.0271244 -5.5168123 -5.2981949 -5.0454288][-7.5491505 -7.4869094 -6.9676294 -7.1378779 -7.1689458 -6.8026876 -6.4046507 -6.4659619 -6.1386361 -6.205574 -6.2608957 -5.9983015 -5.5090828 -5.5461717 -5.0024414]]...]
INFO - root - 2017-12-15 23:30:18.116296: step 73710, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 44h:58m:54s remains)
INFO - root - 2017-12-15 23:30:24.552344: step 73720, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 46h:47m:24s remains)
INFO - root - 2017-12-15 23:30:31.026872: step 73730, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 45h:31m:36s remains)
INFO - root - 2017-12-15 23:30:37.381391: step 73740, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 45h:14m:27s remains)
INFO - root - 2017-12-15 23:30:43.768337: step 73750, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 46h:31m:42s remains)
INFO - root - 2017-12-15 23:30:50.137255: step 73760, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 44h:53m:32s remains)
INFO - root - 2017-12-15 23:30:56.643662: step 73770, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 46h:15m:39s remains)
INFO - root - 2017-12-15 23:31:03.050242: step 73780, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:34m:34s remains)
INFO - root - 2017-12-15 23:31:09.456627: step 73790, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 46h:11m:24s remains)
INFO - root - 2017-12-15 23:31:15.849267: step 73800, loss = 0.31, batch loss = 0.20 (13.0 examples/sec; 0.617 sec/batch; 44h:19m:02s remains)
2017-12-15 23:31:16.436703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6577082 -2.5889473 -2.9523153 -3.1828051 -3.5906444 -3.5390964 -3.7613215 -3.6392479 -3.4623947 -5.0202131 -5.3441448 -6.7716951 -7.5722375 -8.4216557 -8.3306961][-2.4715643 -2.4989572 -3.0564246 -3.7195251 -4.1761341 -4.1082239 -3.7673345 -3.6287303 -3.1203637 -4.5659943 -4.9730535 -6.532568 -7.3338923 -8.3351955 -8.456953][-2.5346913 -2.6942511 -3.1817384 -3.5607367 -3.9834542 -3.7976491 -3.7521114 -3.438076 -3.306509 -4.6124516 -4.6712623 -5.935194 -6.7597075 -7.5599885 -7.7305765][-3.3083534 -3.250689 -3.4252043 -3.6924906 -3.5932221 -3.1308107 -2.7508922 -2.7699971 -2.8219061 -4.1627579 -4.1319814 -5.2946777 -5.9231005 -6.7186842 -6.6869221][-3.9889324 -3.7294493 -3.8119135 -3.315937 -2.9751825 -2.232409 -1.8832111 -1.8804307 -1.710959 -3.3862944 -3.7595918 -5.0022187 -5.4230251 -6.1330214 -6.1131258][-4.4920492 -3.9012403 -3.5546227 -2.5078564 -1.6311669 -0.69224977 -0.051757812 -0.13492489 -0.4260478 -2.1779785 -2.751616 -4.10806 -4.9877257 -5.7258763 -5.6218729][-3.98432 -3.7140386 -3.4358974 -2.4645734 -1.3044157 -0.015935421 0.60730076 0.47986317 0.34347534 -1.2767158 -1.8693986 -3.3443146 -4.4136162 -5.4124813 -5.7385035][-3.721405 -2.6474171 -2.1865606 -1.3347478 -0.53585386 0.3759985 1.0004368 0.79846764 0.73534775 -0.76807642 -1.3021874 -2.8711414 -3.8206646 -4.773149 -5.2567239][-2.9551725 -2.5373154 -1.9640164 -1.0988846 -0.37484646 0.63793755 1.0621843 1.0458336 0.97281933 -0.80947018 -1.3779864 -3.0538397 -4.2795076 -5.1107311 -5.3607159][-2.4685626 -2.4567618 -2.4531217 -1.3849359 -0.53752232 0.089088917 0.30602741 0.36467266 0.51879787 -1.1396494 -1.869617 -3.6200356 -4.9064488 -5.8610134 -6.0492048][-3.8683994 -3.3693089 -3.1756511 -2.7888937 -2.078527 -1.014554 -0.64343834 -0.68174839 -0.63996458 -2.0805264 -2.9882302 -4.679728 -5.9733782 -6.6746879 -6.71192][-4.2274995 -4.8694181 -4.8899608 -4.0905638 -3.4145055 -2.6109133 -2.141293 -1.8463736 -1.7511015 -2.9111795 -3.7272847 -5.1956015 -6.43808 -7.2992711 -7.3963027][-5.2068257 -5.9431744 -6.241601 -5.8937712 -5.1708803 -4.061214 -3.7653074 -3.7054179 -3.5688176 -4.4435954 -5.0537157 -6.2197886 -7.0892186 -7.7661676 -7.7995458][-5.9940572 -6.1513772 -6.4131031 -6.1506414 -5.495635 -4.7636614 -4.344377 -4.1716604 -4.0994835 -4.9462719 -5.392868 -6.1046524 -6.8510256 -7.5072732 -7.5586166][-6.9540033 -7.4966421 -7.5976624 -7.1801839 -6.5823298 -5.9120674 -5.5718136 -5.4047375 -5.3584719 -5.7822323 -6.244266 -7.0802922 -7.5151482 -7.7763777 -7.6836066]]...]
INFO - root - 2017-12-15 23:31:22.843430: step 73810, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 46h:06m:04s remains)
INFO - root - 2017-12-15 23:31:29.229035: step 73820, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 47h:04m:03s remains)
INFO - root - 2017-12-15 23:31:35.609977: step 73830, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 47h:03m:50s remains)
INFO - root - 2017-12-15 23:31:42.106306: step 73840, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 46h:16m:44s remains)
INFO - root - 2017-12-15 23:31:48.567979: step 73850, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 47h:14m:43s remains)
INFO - root - 2017-12-15 23:31:54.904020: step 73860, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 45h:34m:04s remains)
INFO - root - 2017-12-15 23:32:01.374219: step 73870, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 47h:32m:31s remains)
INFO - root - 2017-12-15 23:32:07.861719: step 73880, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.678 sec/batch; 48h:41m:26s remains)
INFO - root - 2017-12-15 23:32:14.399503: step 73890, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 46h:18m:10s remains)
INFO - root - 2017-12-15 23:32:20.790395: step 73900, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 44h:58m:14s remains)
2017-12-15 23:32:21.335394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.503119 -6.18834 -6.5930662 -6.63136 -6.4291534 -6.3204927 -6.1785154 -4.975997 -4.0571032 -3.6327763 -3.6502767 -5.1990619 -6.099432 -7.0069027 -7.8043337][-5.0642357 -6.0807343 -6.364337 -6.8652759 -7.1105852 -6.3841357 -6.1406755 -5.3708582 -4.8190355 -5.3262358 -5.5219707 -7.3259873 -8.4759579 -8.7942266 -8.672617][-4.9767742 -5.1434274 -4.7205348 -4.4935169 -4.1851072 -4.0596709 -4.0069075 -3.7672842 -3.9474497 -4.2662778 -4.3140316 -5.8348265 -6.9707832 -7.67423 -7.80909][-3.5241494 -3.076828 -2.7853379 -3.0749774 -2.9019356 -2.6042013 -2.543932 -1.9634666 -1.4677696 -1.9099865 -2.3809 -4.080102 -5.8360949 -6.7212873 -7.2511539][-2.9895225 -2.4617624 -1.8475947 -1.5619688 -1.3969827 -0.62958384 0.070730686 0.39957619 0.47286129 -0.080770969 -0.72342491 -2.7906756 -4.634551 -5.5170755 -6.4479461][-2.38721 -1.2283926 -0.57731676 -0.25416374 0.32709217 0.59420204 1.0306292 1.3475733 1.5625792 1.0612078 0.63812828 -1.194407 -2.883182 -4.2525554 -5.70669][-2.2300949 -0.731575 0.3782053 0.87466145 1.0862122 1.3086758 1.8505993 2.2129793 2.7735481 1.8730307 1.0125189 -1.1307888 -3.3523846 -4.6529884 -5.733016][-1.9014726 -1.1805534 -0.49642372 0.60987949 0.97878075 1.4989853 2.0905037 2.3994808 2.929 1.6765862 0.79323387 -1.5500283 -3.4293184 -4.4325809 -5.9023986][-3.7386851 -2.9428124 -1.9601803 -0.52534389 0.36942482 1.1834526 1.4581184 1.7058382 2.1425819 0.91038418 0.12695742 -2.1711526 -3.7835732 -4.8631382 -5.7128444][-4.9366302 -4.1595111 -3.1108189 -2.0331326 -1.4802079 -0.49024057 0.053100109 0.33199692 0.3697834 -1.0841331 -1.7711306 -3.8030331 -5.2123508 -5.9378281 -6.6078396][-6.9904757 -6.0107388 -5.1624908 -4.1936684 -3.4564414 -2.9704533 -2.7326007 -2.6415343 -2.6941767 -3.9691293 -4.2373056 -5.7954507 -7.0269995 -7.4155221 -7.740828][-7.2554603 -6.7347627 -6.1845865 -5.409863 -4.73625 -4.4070821 -4.381207 -4.6434231 -4.5493975 -5.7796693 -6.315722 -6.8318343 -7.245995 -7.7836533 -8.164114][-7.569293 -7.3580179 -7.3202438 -7.008357 -6.6474743 -6.6216412 -6.5391984 -6.3231163 -5.9012308 -6.4184933 -6.4742651 -6.9207635 -7.1758442 -6.9826422 -7.1022129][-5.4860106 -5.6493044 -6.0202427 -6.2264934 -6.1916952 -6.0595856 -5.941752 -5.8263893 -5.8329992 -5.9606838 -5.8316889 -5.7946382 -5.6969094 -5.94174 -6.2535825][-6.178287 -6.4065475 -6.6994214 -6.8052893 -6.7887988 -6.477973 -6.2321367 -6.0824919 -5.86163 -5.5034962 -5.279171 -5.252574 -5.1240053 -5.3564854 -5.4306731]]...]
INFO - root - 2017-12-15 23:32:27.727712: step 73910, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 46h:28m:41s remains)
INFO - root - 2017-12-15 23:32:34.092284: step 73920, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 46h:59m:06s remains)
INFO - root - 2017-12-15 23:32:40.458305: step 73930, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 46h:00m:12s remains)
INFO - root - 2017-12-15 23:32:46.849007: step 73940, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 45h:14m:29s remains)
INFO - root - 2017-12-15 23:32:53.272406: step 73950, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 45h:54m:16s remains)
INFO - root - 2017-12-15 23:32:59.748212: step 73960, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 46h:20m:59s remains)
INFO - root - 2017-12-15 23:33:06.141738: step 73970, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 47h:30m:17s remains)
INFO - root - 2017-12-15 23:33:12.542865: step 73980, loss = 0.35, batch loss = 0.24 (12.7 examples/sec; 0.631 sec/batch; 45h:17m:57s remains)
INFO - root - 2017-12-15 23:33:18.952557: step 73990, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 46h:30m:33s remains)
INFO - root - 2017-12-15 23:33:25.431469: step 74000, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 46h:34m:56s remains)
2017-12-15 23:33:25.927456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6758013 -5.2848268 -5.3044748 -4.8165488 -4.6786156 -4.3031764 -3.6224451 -3.7561834 -3.8131444 -4.8686557 -5.3056178 -5.6399283 -5.9931903 -6.3239632 -6.3215685][-5.1908503 -4.8287821 -4.3814716 -4.2342944 -4.43166 -4.2020378 -4.0984011 -3.8483975 -3.4547186 -4.02329 -4.2492542 -4.7108531 -5.2392855 -5.4634533 -5.2433462][-4.98883 -4.8527989 -4.9734936 -4.7073374 -4.3970203 -4.2914662 -3.8227324 -3.4859729 -3.2465572 -3.6334047 -3.9346468 -4.2985878 -4.9187536 -5.2285929 -5.4180984][-5.9012737 -5.2984266 -4.5044508 -4.1922779 -4.0461349 -3.5348935 -2.7232323 -2.2536092 -1.5036082 -2.2146482 -3.2780333 -4.3964443 -5.2941923 -5.5880089 -5.6345139][-6.2372227 -5.2363749 -4.0706825 -3.6937993 -3.1040945 -2.3779473 -1.4799919 -0.84567785 -0.077992916 -1.1405449 -2.2346058 -3.9451895 -5.58051 -6.3127785 -6.5831785][-5.2575016 -4.002378 -3.3058906 -2.5387187 -1.4403315 -0.56108093 0.30076075 0.85987377 1.4769173 0.19789839 -1.2128572 -3.1065602 -4.9957471 -6.3034821 -6.9534407][-4.3105431 -2.8158445 -1.3190594 -0.61394596 0.1568737 0.98768044 1.7769451 2.3082352 2.7782927 1.631587 0.25524426 -1.8255587 -3.9995158 -6.0004711 -7.0503745][-2.9680729 -1.7427411 -0.38647842 0.67879772 1.5844326 2.0492086 2.5503531 2.9508629 3.4757462 2.1225815 0.58158112 -1.6586394 -3.7848535 -5.5418472 -6.4647951][-2.5521135 -0.92815781 0.72767258 1.5114994 2.4630966 2.8330812 3.1787338 3.4462509 3.5256615 2.2385216 0.82898426 -1.3920956 -3.5014572 -5.1885338 -5.9852996][-3.5086956 -1.78515 0.38097095 1.4800253 2.4905596 3.0803041 3.8227215 3.6890659 3.7386742 2.3765116 1.2255793 -0.51815224 -2.7898726 -4.7384758 -5.7986097][-4.01217 -3.0302634 -2.1366758 -0.71841669 0.34987926 1.6260891 2.8324528 2.9744129 3.2964811 1.9835863 0.857605 -0.74340248 -2.4021454 -4.3320103 -5.9676833][-4.24912 -3.601624 -3.5016842 -2.2620888 -0.91107464 0.51109886 1.5933161 2.1838627 2.7302465 1.7682915 0.93681526 -0.54189253 -2.4311895 -4.5498271 -5.7544737][-4.1157084 -4.2330503 -4.3073325 -3.3843017 -2.5863271 -1.0160666 0.14548588 0.39445877 0.53423691 0.24270535 0.00381279 -1.0963449 -2.5476427 -4.350873 -5.6044083][-4.05784 -4.5427074 -4.5647464 -3.9437606 -3.7529004 -2.6623216 -1.6511493 -1.2012658 -0.9656415 -1.0852165 -1.4414444 -1.7826104 -2.6445379 -4.2355905 -5.7264643][-4.0845675 -4.4438543 -5.0365829 -4.5919533 -4.5490685 -4.132268 -3.877867 -3.5100293 -2.9887547 -2.843997 -2.9770908 -3.6480551 -4.4753652 -5.263433 -6.1442938]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:33:32.326179: step 74010, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 45h:44m:43s remains)
INFO - root - 2017-12-15 23:33:38.753303: step 74020, loss = 0.35, batch loss = 0.24 (12.1 examples/sec; 0.664 sec/batch; 47h:39m:51s remains)
INFO - root - 2017-12-15 23:33:45.214083: step 74030, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 45h:05m:09s remains)
INFO - root - 2017-12-15 23:33:51.586409: step 74040, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 44h:14m:50s remains)
INFO - root - 2017-12-15 23:33:58.022444: step 74050, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 44h:43m:15s remains)
INFO - root - 2017-12-15 23:34:04.413422: step 74060, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 45h:59m:45s remains)
INFO - root - 2017-12-15 23:34:10.817871: step 74070, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 45h:05m:09s remains)
INFO - root - 2017-12-15 23:34:17.218987: step 74080, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 45h:50m:06s remains)
INFO - root - 2017-12-15 23:34:23.679609: step 74090, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 45h:27m:04s remains)
INFO - root - 2017-12-15 23:34:30.144275: step 74100, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 46h:57m:40s remains)
2017-12-15 23:34:30.749868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2157393 -4.2672834 -5.0277672 -5.8384047 -5.6069274 -5.6737509 -6.2162094 -6.6076832 -6.3762527 -6.5386133 -7.4216561 -6.7026658 -6.6101255 -6.5807219 -4.0569048][-2.5629125 -2.8366055 -3.0562911 -3.82271 -3.7013106 -3.9028893 -4.6567659 -5.161973 -5.4504032 -6.0272079 -6.9460649 -5.9201975 -5.7328897 -6.1751418 -4.6768436][-1.3928308 -2.5110669 -2.8936591 -3.446743 -3.026454 -2.6129894 -3.3580432 -4.0286889 -4.7622709 -5.864336 -6.6276951 -6.6952062 -7.6736031 -8.8636484 -7.4461136][-3.903079 -3.3192081 -2.9253063 -3.3374815 -3.4398451 -3.2173 -2.9325485 -3.0863776 -3.8663108 -5.0989685 -6.2828436 -6.9854045 -7.5919251 -8.8309917 -8.8462315][-4.2721376 -4.5831814 -3.9149916 -3.2262669 -2.2319822 -0.98100615 -0.91510773 -1.0047698 -1.4566169 -2.5276384 -3.2960505 -4.7036276 -6.3111954 -8.2885094 -8.2595034][-5.34883 -4.7042985 -3.9794974 -3.0323133 -2.2172375 -1.4332647 -0.34939241 0.98144627 0.8615284 0.65662575 -0.33203363 -1.2101517 -2.3036652 -4.940136 -5.7972603][-4.6809149 -4.1652431 -3.1395469 -1.6856661 -0.47348928 0.61056137 1.4619093 1.2059917 1.1839113 1.8115311 1.0372572 0.14747715 -1.7142482 -3.836143 -3.9562261][-3.7549069 -3.1257911 -2.4697509 -0.81261635 0.743001 2.9304523 4.3847628 3.9910011 4.1594353 2.3670015 1.4566975 1.4395885 0.1263051 -1.8687563 -3.2571535][-4.730772 -4.0481405 -3.1743021 -1.7238612 0.44009018 2.5349369 3.5367365 4.7712574 5.7192383 3.65203 1.6288271 -0.42925215 -2.0384617 -3.7993765 -4.4706764][-5.7500668 -5.5131817 -4.7401838 -3.5202322 -1.7532716 -0.052345753 1.2830744 3.3194447 3.8354778 2.754776 0.72189236 -1.1821113 -2.9244871 -5.1927547 -5.7600036][-8.14201 -7.8149261 -7.0342531 -5.7217503 -4.5572729 -3.0838995 -1.7997732 -0.6256156 0.9190712 0.761652 -0.257164 -2.0563064 -3.9064045 -6.0625834 -6.8445644][-7.8630047 -8.2398481 -8.1337185 -7.613492 -6.9302025 -5.1750073 -4.4159536 -3.7108266 -2.529448 -2.6653719 -2.6900196 -3.3684168 -3.7340536 -5.4481745 -6.7344017][-8.5436268 -7.9585729 -8.7678547 -8.6308022 -8.1336708 -7.3241577 -6.9773397 -6.3716407 -5.7172265 -5.4803243 -5.6139069 -4.9020863 -4.3187933 -5.1702814 -5.992588][-8.690341 -8.2862968 -7.7901683 -7.2514071 -7.9416509 -7.4188414 -7.5239172 -7.1980834 -6.61631 -6.750361 -6.3079453 -6.2074003 -5.9375658 -5.4727268 -4.8797827][-9.0032072 -8.9424191 -8.8646631 -8.26223 -7.5440493 -7.0746679 -7.097333 -7.8051014 -7.8234429 -7.6922321 -7.3058014 -6.80911 -6.583992 -6.460556 -6.002739]]...]
INFO - root - 2017-12-15 23:34:37.358181: step 74110, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 45h:34m:11s remains)
INFO - root - 2017-12-15 23:34:43.775999: step 74120, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 44h:48m:44s remains)
INFO - root - 2017-12-15 23:34:50.104002: step 74130, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.624 sec/batch; 44h:45m:00s remains)
INFO - root - 2017-12-15 23:34:56.495577: step 74140, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 46h:42m:38s remains)
INFO - root - 2017-12-15 23:35:02.873531: step 74150, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 44h:33m:04s remains)
INFO - root - 2017-12-15 23:35:09.258964: step 74160, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 45h:59m:31s remains)
INFO - root - 2017-12-15 23:35:15.734435: step 74170, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 45h:45m:11s remains)
INFO - root - 2017-12-15 23:35:22.080900: step 74180, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 46h:05m:05s remains)
INFO - root - 2017-12-15 23:35:28.490668: step 74190, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 47h:41m:49s remains)
INFO - root - 2017-12-15 23:35:34.821846: step 74200, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.620 sec/batch; 44h:30m:16s remains)
2017-12-15 23:35:35.398216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8762455 -2.3808889 -2.8550191 -2.9639549 -2.8002667 -2.6014943 -2.2595868 -1.6458683 -1.0013337 -2.1933036 -3.2494659 -5.0470533 -5.9080157 -6.3210683 -7.117486][-2.2783623 -2.5241113 -3.2114887 -3.7753978 -3.9642828 -3.405467 -2.5186806 -2.0210633 -1.6731472 -2.7371349 -3.6835504 -5.5539083 -6.5728207 -7.2716885 -7.7315812][-2.8825331 -3.1069779 -3.5549684 -3.9663494 -4.0879 -3.837496 -2.9940176 -2.3415194 -1.9940896 -3.3828349 -4.5388041 -6.1502929 -6.7894106 -7.411478 -8.0290976][-3.2192159 -3.4618931 -3.7886453 -3.7132688 -3.6315513 -3.0111523 -1.8876929 -1.3090177 -0.89364529 -2.3360538 -3.7062027 -5.6370125 -6.6808128 -7.3597488 -7.6072292][-3.3987989 -3.576756 -3.7392659 -3.4688225 -2.9820046 -1.8154941 -0.41229916 0.36090851 0.56516171 -1.192956 -2.8778214 -5.1494236 -6.45612 -7.2711282 -7.8690853][-4.0483975 -4.2035418 -4.3131332 -3.5708241 -2.3440428 -0.781867 0.70493889 1.692193 2.058939 -0.1462431 -2.3819914 -4.8886976 -6.5212073 -7.5985336 -8.1917887][-4.1129026 -3.8822656 -3.8930216 -2.9896855 -1.3371549 0.76172447 2.3687449 3.3348436 4.1830063 2.44631 0.00248909 -3.5222855 -5.6846895 -6.9759688 -7.6574802][-3.5905528 -3.507153 -3.2220087 -2.1937037 -0.87168455 1.4153748 3.4701929 4.7098446 5.4773912 3.6948929 1.4152851 -1.8885431 -4.4219418 -6.2291861 -7.0514154][-4.0099039 -3.4866428 -2.999424 -2.3301568 -1.1486335 0.7296505 2.3861322 3.6791592 4.7415009 3.1464634 1.0407896 -2.0256495 -4.0570006 -5.7207508 -6.894701][-4.2239289 -3.9578829 -3.520771 -2.7754455 -2.0961552 -0.90505552 0.25516176 1.309577 1.9838009 0.30411768 -1.2491541 -3.8047292 -5.1470876 -6.2568984 -7.0109925][-5.7413425 -5.3019476 -4.8667431 -4.3416119 -3.8890352 -3.1270518 -2.8150969 -2.3291364 -1.7873502 -2.8751173 -4.1359496 -5.799993 -6.4302921 -6.9904733 -6.878633][-6.596941 -6.2520418 -5.8099957 -5.2583466 -4.99197 -4.7011333 -4.5905876 -4.6480274 -4.4806314 -4.8910661 -5.2289076 -6.4602489 -6.7877054 -7.2756882 -7.5343351][-7.3763604 -7.0745592 -6.3762226 -5.7083931 -5.3560257 -5.0011568 -5.4008946 -5.5500207 -5.4829907 -6.1694417 -6.1378117 -6.8613358 -7.3792624 -7.6749654 -7.5675721][-7.7017035 -7.4445639 -6.6096988 -5.9312077 -5.4719419 -5.162364 -5.2901897 -5.5587234 -5.8826556 -6.0966296 -6.1888456 -6.4502363 -6.4321966 -6.8709264 -6.7149377][-8.3483009 -8.1673546 -7.4903855 -6.9650869 -6.3728886 -5.9531097 -6.1132522 -6.3499894 -6.401803 -6.5136943 -6.5370846 -6.4114666 -6.21945 -5.9279118 -6.0732126]]...]
INFO - root - 2017-12-15 23:35:41.806135: step 74210, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 45h:17m:29s remains)
INFO - root - 2017-12-15 23:35:48.271921: step 74220, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 46h:23m:00s remains)
INFO - root - 2017-12-15 23:35:54.720509: step 74230, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 45h:12m:00s remains)
INFO - root - 2017-12-15 23:36:01.202154: step 74240, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 44h:57m:32s remains)
INFO - root - 2017-12-15 23:36:07.592288: step 74250, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 46h:15m:35s remains)
INFO - root - 2017-12-15 23:36:14.076841: step 74260, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 46h:23m:33s remains)
INFO - root - 2017-12-15 23:36:20.445804: step 74270, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 45h:31m:32s remains)
INFO - root - 2017-12-15 23:36:26.865718: step 74280, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 47h:09m:56s remains)
INFO - root - 2017-12-15 23:36:33.251729: step 74290, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 46h:18m:51s remains)
INFO - root - 2017-12-15 23:36:39.656809: step 74300, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 44h:30m:25s remains)
2017-12-15 23:36:40.170409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3559179 -2.5263758 -2.976099 -3.5028124 -4.0219679 -4.3452253 -4.5843363 -4.64398 -4.64186 -5.612824 -6.0912018 -7.0176616 -7.9771805 -8.6156273 -9.6159611][-3.2061052 -3.156919 -3.6732273 -4.1161604 -4.3223743 -4.1552382 -4.037477 -4.1093025 -3.8641417 -5.2134228 -6.1462483 -6.7767854 -8.0466833 -8.9569492 -9.8275528][-4.1979375 -4.1080503 -4.196846 -4.1835904 -4.1083069 -3.6564875 -3.2115889 -3.2886934 -3.13137 -4.2894773 -4.8889046 -5.5708113 -6.6337185 -7.181459 -7.8978925][-5.4791241 -4.8887491 -4.7124996 -3.9603229 -3.0651326 -2.2297273 -1.456635 -1.3893819 -1.23424 -2.28021 -2.9325342 -4.1338339 -5.2970448 -5.7892303 -6.592423][-5.7697015 -5.1168976 -4.1257677 -3.0734692 -2.1410527 -0.666173 0.4794054 0.73117161 1.0185804 -0.46229887 -1.371232 -2.3927617 -3.9408393 -4.9800034 -5.7818031][-5.6008873 -4.6330843 -3.8585074 -2.634891 -1.024159 0.60302162 2.0124683 2.4796209 2.7015219 1.0997295 -0.31339025 -1.8810496 -3.2035041 -4.0065908 -4.9313631][-5.7167845 -4.9076385 -3.8276966 -2.1438441 -0.31032419 1.5779686 3.1577749 3.6438837 4.0470648 1.9468746 0.10483456 -1.6911616 -3.8619859 -4.5857964 -5.1747918][-5.6948419 -4.6682177 -3.4044008 -2.1476836 -0.093284607 2.2020588 3.5781507 4.0217276 4.3015461 2.4357128 0.7724638 -1.6158342 -3.7969508 -4.9800959 -5.9231997][-4.6367273 -4.2814827 -3.6807137 -1.9848132 -0.51626015 0.87658787 2.1730394 3.009943 3.4205465 1.3852978 -0.21824121 -1.911242 -3.7933307 -5.00305 -6.1928964][-5.481679 -4.8525929 -4.62876 -3.5119257 -2.3064461 -1.0145097 -0.10057926 0.60175705 1.1225691 -0.74547768 -2.0993762 -3.9033473 -5.2970104 -5.9800882 -6.770494][-7.28507 -7.0573115 -6.7665358 -5.914073 -4.7431164 -3.52666 -2.4239001 -1.9599099 -1.6533279 -3.1275897 -4.5333853 -5.8444862 -7.09507 -7.7025075 -7.8471403][-7.8520851 -7.7753129 -7.5151381 -7.0493708 -6.3755198 -5.42404 -4.8856506 -4.4522629 -3.9393919 -5.2665939 -5.893281 -6.4173989 -7.2099838 -7.6489263 -8.0591078][-7.7351718 -7.9148717 -8.1149864 -7.6370268 -7.3418508 -6.9109955 -6.6738234 -6.394424 -6.1816158 -7.0571384 -7.2348814 -7.2987533 -7.6303825 -7.4131832 -7.211987][-7.4425278 -7.5345736 -7.5623016 -7.5061903 -7.2262392 -6.5892735 -6.5019083 -6.6737275 -6.7608562 -7.3959 -7.653008 -7.259305 -7.3065953 -7.2973289 -6.9814062][-8.49641 -8.287199 -8.3022795 -7.9544916 -7.779007 -7.6448922 -7.8447094 -7.7710919 -7.9149671 -7.9723206 -7.8897748 -7.66217 -7.4422116 -7.1235714 -6.8968158]]...]
INFO - root - 2017-12-15 23:36:46.720269: step 74310, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.648 sec/batch; 46h:30m:14s remains)
INFO - root - 2017-12-15 23:36:53.112695: step 74320, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:14m:59s remains)
INFO - root - 2017-12-15 23:36:59.528166: step 74330, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 48h:01m:45s remains)
INFO - root - 2017-12-15 23:37:05.989965: step 74340, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 46h:35m:29s remains)
INFO - root - 2017-12-15 23:37:12.427971: step 74350, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 44h:33m:35s remains)
INFO - root - 2017-12-15 23:37:18.757811: step 74360, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 47h:27m:56s remains)
INFO - root - 2017-12-15 23:37:25.220413: step 74370, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 46h:51m:54s remains)
INFO - root - 2017-12-15 23:37:31.595237: step 74380, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 47h:08m:55s remains)
INFO - root - 2017-12-15 23:37:37.901633: step 74390, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.632 sec/batch; 45h:20m:47s remains)
INFO - root - 2017-12-15 23:37:44.240526: step 74400, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 45h:39m:52s remains)
2017-12-15 23:37:44.809471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3340316 -4.7108259 -3.9563365 -3.3709273 -2.9585314 -2.2865624 -1.7829151 -2.1036348 -1.8716149 -3.1848445 -4.5426617 -5.6730022 -6.8526688 -7.4183125 -8.4958906][-4.398725 -4.2023773 -3.7017481 -3.16574 -2.5313673 -2.0815144 -2.1523418 -2.275619 -2.0522242 -3.6970732 -4.9098806 -5.5027323 -6.5823135 -7.3851089 -8.3152685][-3.88606 -3.4965057 -3.0384865 -2.807116 -2.3995404 -1.9212828 -1.532743 -1.5334773 -1.520936 -3.3871961 -4.6124878 -5.3541946 -6.6655149 -7.1781745 -7.8367419][-3.2214446 -2.9090791 -2.5951433 -1.9977536 -1.5562243 -1.4722991 -1.1976295 -1.2053013 -1.195384 -2.9145126 -4.0244408 -4.7927761 -6.0273476 -6.7205744 -7.4799867][-3.1426306 -2.117589 -1.0850692 -1.007834 -1.1720772 -0.824121 -0.3676219 -0.52259064 -0.57656622 -2.3283548 -3.3524756 -4.0128412 -5.4033794 -6.0712991 -6.6302276][-2.7381544 -1.9118962 -1.2527194 -0.74691105 -0.21010733 -0.12900448 -0.066227913 -0.13782501 -0.076677322 -1.8387394 -2.9089074 -3.7427988 -5.060008 -5.6030674 -6.4139233][-2.5223994 -1.8388524 -0.91057634 -0.59822178 -0.39657879 0.057373047 0.53495121 0.49121094 0.44353294 -1.3122282 -2.3989763 -3.2862563 -4.6991253 -5.2111988 -5.8551946][-2.2589507 -1.3753252 -0.87805462 -0.30507374 0.47779846 0.8181572 0.89981174 1.1588411 1.443265 -0.36201048 -1.6722922 -2.763689 -4.3709922 -5.1414361 -5.7002287][-2.0592294 -1.5334935 -0.57193518 0.37188053 0.74310112 0.95521736 1.4051752 1.4454327 1.5325289 -0.41950607 -1.8272281 -2.9480329 -4.5910921 -5.3882618 -6.0873165][-3.0393243 -1.9690371 -0.96916628 -0.024896145 0.70886421 1.2380018 1.3167076 1.2087965 1.3192511 -0.98411846 -2.414196 -3.8315239 -5.3051405 -5.99305 -6.5712042][-3.6921 -3.0087175 -2.3597302 -1.2635379 -0.43244123 0.12676096 0.16513872 0.05044508 -0.080854893 -2.2056293 -3.6225572 -4.5860882 -5.8352041 -6.7415528 -7.1179752][-4.5782633 -3.438798 -2.7071347 -1.8010349 -0.81239748 -0.12533474 -0.057415009 -0.41397953 -0.91795588 -2.6025457 -3.9172704 -4.8423615 -6.0673032 -6.8629789 -6.9959569][-4.84515 -4.2851758 -3.9287057 -3.0287075 -2.1359372 -1.5164618 -1.2660403 -1.7681251 -2.2685828 -3.5803447 -4.7822342 -5.0120125 -5.6373234 -6.0944929 -6.1780691][-5.2920713 -4.6161633 -4.4479742 -4.0172644 -3.8396146 -3.1868258 -2.9405804 -3.327796 -3.4732842 -4.1608596 -4.5164843 -4.9483004 -5.4334164 -5.7872629 -5.6737432][-5.887023 -5.584836 -5.0692639 -5.0788069 -5.0864573 -4.6647615 -4.6865358 -4.7760696 -5.0070314 -5.0674868 -5.0435338 -5.3006158 -5.2435341 -5.2403874 -5.0654526]]...]
INFO - root - 2017-12-15 23:37:51.138521: step 74410, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.628 sec/batch; 45h:00m:13s remains)
INFO - root - 2017-12-15 23:37:57.580023: step 74420, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 45h:42m:06s remains)
INFO - root - 2017-12-15 23:38:03.971865: step 74430, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 45h:34m:11s remains)
INFO - root - 2017-12-15 23:38:10.335538: step 74440, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:30m:54s remains)
INFO - root - 2017-12-15 23:38:16.654649: step 74450, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 45h:03m:09s remains)
INFO - root - 2017-12-15 23:38:23.124442: step 74460, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 47h:03m:52s remains)
INFO - root - 2017-12-15 23:38:29.594245: step 74470, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 46h:18m:31s remains)
INFO - root - 2017-12-15 23:38:36.033462: step 74480, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 45h:44m:50s remains)
INFO - root - 2017-12-15 23:38:42.469287: step 74490, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:14m:33s remains)
INFO - root - 2017-12-15 23:38:48.893197: step 74500, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 46h:34m:21s remains)
2017-12-15 23:38:49.399870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8018961 -2.5769224 -2.5089941 -2.1058025 -1.7121496 -1.6299081 -1.6146169 -1.5670195 -2.0032878 -4.234736 -5.6136684 -7.4659333 -9.1605825 -10.099152 -9.9724655][-4.3201056 -3.5335274 -3.0407095 -2.88488 -2.9597869 -2.712101 -2.5063996 -2.9628491 -3.2781367 -5.2233343 -6.6524019 -8.2393923 -9.6069221 -10.799105 -11.269144][-3.7923687 -3.0386286 -2.2932868 -2.0610695 -2.3489852 -2.6925116 -2.6630845 -2.6608014 -2.9935751 -4.9889731 -5.9571753 -7.1624842 -8.4169693 -9.4428844 -9.9175854][-2.6307216 -1.6603131 -0.88611507 -0.68786526 -0.59058666 -1.0853386 -1.4381676 -1.5045614 -1.4005547 -3.4154539 -4.6848598 -5.826097 -7.0877419 -8.2124567 -8.8125248][-1.9163713 -1.1154032 -0.20131302 0.20138073 0.30532026 0.19139481 0.027192116 -0.40075636 -0.69714212 -2.2598162 -3.3899693 -4.8465 -6.2214217 -7.0498614 -7.4660993][-1.4740038 -0.074483395 0.73094273 0.80403996 0.78283596 0.63414955 0.63698387 0.53305054 0.47739887 -1.4361539 -2.5296063 -3.7472832 -4.9431305 -5.8861132 -6.3405623][-2.0223951 -0.77312469 0.37304974 0.84531116 1.2719297 1.1744308 1.0947857 1.3657789 1.7423229 -0.11522913 -1.5343785 -3.2346363 -4.6949234 -5.6298814 -6.1748042][-3.0816369 -1.9926991 -1.1051335 0.028360367 1.0813465 1.4349222 1.8045559 1.6436586 1.6899166 0.18087149 -1.1003046 -2.8795047 -4.2431831 -5.3378921 -5.8503194][-4.0023432 -2.3710184 -1.1771836 -0.0088396072 0.76812649 1.7672825 2.5050287 2.0496655 1.7265892 0.11047173 -0.98840666 -2.85011 -4.21873 -5.203681 -5.5083265][-4.2589741 -3.27836 -2.2788014 -0.9438982 0.22181845 1.3871889 2.0198145 2.1011324 1.9350309 -0.3752985 -1.297081 -3.0917115 -4.38133 -5.846005 -6.2667675][-5.4048023 -4.7937751 -4.1510849 -2.7274265 -1.5555954 -0.26325846 0.5303297 0.88590336 0.7860775 -1.5038924 -2.6788664 -4.2661314 -5.3433657 -6.199029 -6.5379753][-6.58575 -6.0658889 -5.4741206 -4.4107022 -3.3826518 -2.0828466 -1.0754561 -0.9061861 -1.0818963 -2.3851204 -2.8211875 -4.4592285 -5.3183088 -6.0296631 -6.3559341][-6.8992991 -6.6012549 -5.9277077 -4.9223542 -3.9440527 -3.0200195 -2.4309292 -1.9966502 -1.8425856 -3.0407295 -3.6026917 -4.69048 -5.0198784 -5.5093327 -5.4354277][-6.770721 -6.4818578 -6.2316809 -5.219686 -4.3601437 -3.4714103 -2.8553028 -2.9466767 -3.08947 -3.5199437 -3.6324553 -3.870738 -4.3788857 -4.9350061 -5.2454777][-7.8885789 -7.7037973 -7.505682 -6.6795292 -6.0399818 -5.185853 -4.6163549 -4.6521034 -4.7844429 -4.7044773 -4.7428317 -4.7240486 -4.7189112 -4.4772396 -4.3587222]]...]
INFO - root - 2017-12-15 23:38:55.774352: step 74510, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 47h:55m:06s remains)
INFO - root - 2017-12-15 23:39:02.145017: step 74520, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 46h:12m:53s remains)
INFO - root - 2017-12-15 23:39:08.498321: step 74530, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 44h:52m:03s remains)
INFO - root - 2017-12-15 23:39:14.878665: step 74540, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 45h:08m:46s remains)
INFO - root - 2017-12-15 23:39:21.322243: step 74550, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 47h:49m:04s remains)
INFO - root - 2017-12-15 23:39:27.618514: step 74560, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 45h:08m:41s remains)
INFO - root - 2017-12-15 23:39:33.929235: step 74570, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:10m:46s remains)
INFO - root - 2017-12-15 23:39:40.327830: step 74580, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 45h:47m:44s remains)
INFO - root - 2017-12-15 23:39:46.673014: step 74590, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 45h:26m:24s remains)
INFO - root - 2017-12-15 23:39:52.989814: step 74600, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:29m:04s remains)
2017-12-15 23:39:53.532531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1525984 -6.3417444 -6.2477326 -6.1320434 -5.9727588 -5.3058739 -4.5024061 -4.5007305 -4.5298376 -5.19592 -5.6958385 -6.3448043 -6.6368203 -7.1964369 -7.9929237][-6.2648206 -6.853291 -6.7214079 -6.6936221 -6.1267319 -5.6103029 -5.5991573 -5.2182808 -4.6329737 -5.4888248 -5.7816629 -6.1149316 -6.5612311 -6.9094882 -7.3177223][-4.4899788 -4.3801546 -4.7896862 -4.7478266 -4.33573 -4.3552437 -4.4607267 -4.1617775 -4.0686121 -4.9466414 -4.8394403 -5.1106005 -5.1545362 -5.12296 -4.9774919][-2.5771275 -2.5501866 -3.15026 -2.7481613 -2.640141 -2.6051898 -2.6656375 -2.4657278 -2.3676081 -3.4676762 -3.7633986 -3.9032698 -4.13666 -4.2529049 -4.219049][-2.460639 -1.8604527 -1.9125957 -1.8282919 -2.0302491 -1.8323164 -1.5294871 -1.4686251 -1.5260782 -2.5639648 -3.0111609 -3.5247326 -3.8775654 -4.2252741 -4.2203989][-2.6114984 -2.3355341 -1.9648795 -1.0868917 -0.21946859 0.62914848 1.1891603 0.78656673 0.023637295 -1.5013738 -2.5399776 -3.2642331 -3.8247385 -4.6770477 -4.9004812][-3.24762 -2.9109368 -2.0926609 -0.81748915 -0.038253784 1.0777311 1.7377281 1.384943 0.99062443 -0.93393564 -2.1269002 -3.3583193 -3.9842453 -4.4791551 -4.5223522][-3.5262237 -2.7539601 -1.9900646 -0.90332413 0.019483566 1.2125807 1.7324133 1.3847723 0.9955101 -0.93902349 -2.1794481 -3.3133879 -3.9687514 -4.7397432 -4.7806983][-3.9787681 -3.4269447 -2.5249004 -1.7111816 -0.78332138 0.097296238 0.56281376 0.73605824 0.658391 -1.1402311 -2.6484256 -3.7927749 -4.7337809 -5.5609179 -5.5449066][-5.31555 -4.3697267 -3.5882397 -2.5429621 -1.473465 -0.95527554 -0.81745815 -0.44801664 -0.55572271 -2.0323377 -3.1969981 -4.3282661 -5.3176861 -6.5158429 -6.8037076][-6.7969494 -6.3239808 -5.9907541 -5.1518507 -4.3668623 -3.6935658 -3.3776507 -3.3760643 -3.3543429 -4.15267 -5.0930672 -5.5173292 -6.3208494 -7.4344764 -7.505857][-8.0472927 -7.4019794 -7.0856581 -6.9393516 -6.655323 -6.1280909 -5.6372409 -5.1157942 -4.6695585 -5.1103182 -6.1490541 -6.2132921 -6.7689061 -7.3758116 -7.6805534][-8.4827871 -8.1618834 -7.7282648 -7.4794006 -7.2238688 -7.0297346 -6.7294197 -6.7144966 -6.5504971 -6.4681745 -7.0124578 -6.4858193 -6.6080246 -6.9312654 -7.0374947][-8.1969767 -7.6475191 -7.4909449 -7.4557328 -7.0862608 -6.8453865 -6.6543427 -6.9228344 -6.8788848 -6.8025155 -6.8503027 -6.8194571 -7.0217452 -7.2289124 -6.7536368][-8.2437906 -7.7586112 -7.6144676 -7.4089308 -7.3483739 -7.2281442 -6.792717 -7.1199808 -6.9682045 -6.7236962 -6.9978447 -6.9967175 -6.5803041 -6.3942642 -6.3742695]]...]
INFO - root - 2017-12-15 23:40:00.043359: step 74610, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 47h:46m:10s remains)
INFO - root - 2017-12-15 23:40:06.430839: step 74620, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 44h:36m:53s remains)
INFO - root - 2017-12-15 23:40:12.772377: step 74630, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.618 sec/batch; 44h:15m:12s remains)
INFO - root - 2017-12-15 23:40:19.171992: step 74640, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 47h:03m:30s remains)
INFO - root - 2017-12-15 23:40:25.620733: step 74650, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 44h:19m:15s remains)
INFO - root - 2017-12-15 23:40:31.972914: step 74660, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 44h:38m:03s remains)
INFO - root - 2017-12-15 23:40:38.420742: step 74670, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 46h:23m:26s remains)
INFO - root - 2017-12-15 23:40:44.770867: step 74680, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:12m:53s remains)
INFO - root - 2017-12-15 23:40:51.134358: step 74690, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 44h:48m:33s remains)
INFO - root - 2017-12-15 23:40:57.442284: step 74700, loss = 0.35, batch loss = 0.23 (12.8 examples/sec; 0.626 sec/batch; 44h:50m:18s remains)
2017-12-15 23:40:57.990397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8871245 -4.0218081 -3.3854628 -3.6935074 -4.27717 -4.2264361 -4.0836287 -4.0418148 -4.2604237 -5.2016115 -6.7194381 -7.5354161 -8.341011 -9.3244867 -9.7915316][-4.4833565 -4.2737169 -4.1330633 -3.7009687 -3.8772008 -4.3984132 -4.7325335 -4.8631315 -4.8884296 -4.8912921 -5.8447714 -6.7334647 -7.9023638 -8.7526827 -8.8933134][-5.3566437 -4.2765169 -3.547462 -4.1633215 -4.3059392 -3.9164298 -3.7669575 -3.713002 -3.7792027 -4.1496782 -4.8659506 -5.1678643 -6.3158216 -7.2974734 -7.5622158][-5.6769595 -5.0559316 -4.2231917 -4.0177269 -3.5846934 -2.8958802 -2.3408608 -2.0631328 -1.3448119 -2.0887623 -3.417573 -4.3934751 -5.4628487 -6.1067829 -6.6050453][-6.25936 -5.1288061 -4.3566251 -3.5499792 -2.9418731 -1.8197618 -0.30702782 -0.29970789 0.091197491 -0.28733683 -1.5214844 -2.4915376 -4.1968961 -5.1856346 -5.8454976][-5.1813083 -4.4593935 -3.7692883 -2.8375421 -1.7120709 -0.1053648 1.3156185 1.7002745 1.8111191 0.88504219 -0.55550194 -1.7511287 -3.4332576 -4.5545149 -5.2813168][-4.59599 -3.8606169 -2.7560511 -1.9349513 -0.8813982 0.88444042 2.5732412 3.1109915 3.2137613 2.2335711 0.15776873 -1.3732605 -2.9289203 -4.7795515 -5.6953964][-3.3493719 -3.3590055 -2.6698308 -1.473464 -0.22789049 1.0633583 2.5631676 3.232111 3.4423313 2.4304523 0.35370827 -1.6424479 -3.5998521 -4.710855 -5.4617047][-3.2143936 -2.7681456 -2.0390654 -0.90874958 0.063169 1.32938 2.3914623 2.838378 2.7819557 1.8865833 -0.13767385 -2.3589315 -4.2168207 -5.7369232 -6.2733979][-3.4982996 -2.7865233 -1.9272509 -0.72116375 0.48104954 1.5916634 2.21447 2.3572731 2.2023 1.3479509 -0.70947552 -2.1613426 -3.7284458 -5.4011488 -6.4471788][-3.6190519 -3.354022 -2.68962 -1.7888117 -0.72047567 0.16525984 0.86956024 1.1065702 0.75121212 0.15332317 -1.5048914 -2.6351867 -4.32014 -5.6069832 -6.3234196][-4.7724361 -4.00345 -3.3838058 -2.2198319 -1.3596435 -0.47632694 -0.084178448 0.16213703 0.059748173 -0.75995588 -2.5977774 -3.5976257 -4.8790312 -5.702539 -6.3389354][-6.1954479 -5.1879292 -4.6942682 -3.8470244 -2.7886877 -1.7098212 -1.3577528 -1.3170643 -1.6018577 -1.9402781 -3.1320624 -4.0419645 -5.2793918 -5.8090105 -5.7370439][-6.1706038 -5.9840565 -5.4243307 -4.7633696 -4.065032 -3.3130913 -2.5675383 -2.4780431 -2.6202979 -2.9241319 -3.6541481 -4.1061149 -4.99279 -5.2966104 -5.3760338][-6.1600475 -5.8380432 -5.8739362 -5.7325382 -5.0130348 -4.1857281 -3.60812 -3.5381212 -3.7861485 -4.0599117 -4.0947862 -4.4704037 -5.1032629 -5.1215415 -5.1144824]]...]
INFO - root - 2017-12-15 23:41:04.374768: step 74710, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 46h:01m:39s remains)
INFO - root - 2017-12-15 23:41:10.849942: step 74720, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 45h:49m:02s remains)
INFO - root - 2017-12-15 23:41:17.257734: step 74730, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 45h:53m:24s remains)
INFO - root - 2017-12-15 23:41:23.629398: step 74740, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 47h:08m:12s remains)
INFO - root - 2017-12-15 23:41:30.076713: step 74750, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 45h:38m:42s remains)
INFO - root - 2017-12-15 23:41:36.539083: step 74760, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:56m:37s remains)
INFO - root - 2017-12-15 23:41:42.896634: step 74770, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 45h:17m:00s remains)
INFO - root - 2017-12-15 23:41:49.253524: step 74780, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 45h:31m:53s remains)
INFO - root - 2017-12-15 23:41:55.596268: step 74790, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 45h:00m:38s remains)
INFO - root - 2017-12-15 23:42:02.063099: step 74800, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 46h:09m:09s remains)
2017-12-15 23:42:02.625313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1512756 -4.4412861 -4.3980551 -4.4935708 -4.2537537 -3.8629622 -3.0962577 -2.4227629 -1.6795645 -1.8115802 -2.5596929 -4.1798859 -4.9343424 -5.3264685 -6.6185513][-4.001142 -4.5493922 -4.9061594 -5.1260252 -4.8158436 -4.5051131 -4.1994753 -3.5320897 -2.6825204 -2.6715226 -2.7920203 -3.8279145 -4.8515444 -5.759305 -7.3694453][-4.7188864 -4.7851 -4.6588678 -4.7235112 -4.6861653 -4.44476 -4.1167841 -3.8781202 -3.3257365 -3.4360585 -3.8183575 -4.4746528 -4.8730335 -5.3647938 -6.5992041][-4.9677134 -4.2477665 -3.68783 -3.1738005 -2.7776227 -2.6595392 -2.5215721 -2.8159518 -2.3055243 -2.6631517 -3.3160644 -4.1289835 -4.4876494 -4.8232093 -5.9085274][-5.0428495 -4.0135603 -2.8019023 -1.9455743 -1.3196874 -0.94959831 -0.34678078 -0.28670406 -0.40623331 -1.1977496 -2.235589 -3.6392784 -4.4773965 -5.2885275 -6.33571][-4.2577019 -3.7104375 -3.0139041 -1.840498 -1.0609102 -0.035278797 1.1682768 1.2267628 1.0373631 0.26573086 -0.98551512 -2.5421391 -3.5503016 -4.5412006 -5.8660927][-4.1665359 -3.3136363 -2.3998318 -0.90894938 -0.0086917877 1.0778198 2.1114292 2.2861986 2.7016773 1.8511477 0.36632729 -1.4889112 -3.1522532 -4.567337 -5.9028821][-3.5340705 -2.5181217 -1.5379424 0.28515244 1.4958076 2.4190245 2.9084892 2.9634 3.487999 2.952095 1.8830833 -0.17212915 -2.1662664 -3.7033827 -5.3476286][-3.2062345 -2.7128448 -1.9185438 -0.52494144 0.56442261 1.6900892 2.3678427 2.6839561 2.9624605 2.3629332 1.4661512 -0.52118254 -2.1614184 -3.5150881 -5.054265][-3.2654891 -3.3771229 -2.9680519 -1.8281312 -0.66506243 0.75523853 1.7330275 1.9292345 1.941865 0.92012787 -0.47182417 -2.2343359 -2.8272491 -3.4280405 -4.7173328][-4.4652677 -4.2898331 -3.8924954 -3.1739101 -2.3924246 -1.3606339 -0.33307028 0.14226913 0.38114452 -0.60331106 -1.7730098 -3.3139024 -4.0365772 -4.7181296 -5.4018555][-6.0906796 -5.4925871 -5.0592337 -4.3920135 -3.5382748 -2.7916818 -2.1969962 -2.2466917 -2.0891824 -2.3409362 -3.089354 -4.2714806 -4.6285114 -5.3610106 -5.9826241][-7.2703414 -7.1936474 -6.8737373 -5.885829 -5.11977 -4.5756388 -4.1070547 -3.9548087 -3.6366367 -3.9219019 -4.4428124 -5.0405712 -4.8776321 -5.0898418 -5.659647][-7.4675198 -7.6326466 -7.3150239 -6.4865136 -6.2920942 -5.8526287 -5.3587952 -5.299799 -5.2942638 -5.474432 -5.4498076 -5.3756018 -5.0105391 -5.033185 -5.4969077][-8.0922089 -8.2090063 -7.4253469 -7.1621785 -7.1542044 -6.89144 -6.5718923 -6.5271168 -6.5114889 -6.6061525 -6.5933361 -6.671864 -6.7916427 -6.480649 -6.503211]]...]
INFO - root - 2017-12-15 23:42:09.031131: step 74810, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 45h:30m:57s remains)
INFO - root - 2017-12-15 23:42:15.398170: step 74820, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 44h:31m:58s remains)
INFO - root - 2017-12-15 23:42:21.764844: step 74830, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 45h:27m:18s remains)
INFO - root - 2017-12-15 23:42:28.134525: step 74840, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 45h:22m:57s remains)
INFO - root - 2017-12-15 23:42:34.527582: step 74850, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.613 sec/batch; 43h:51m:06s remains)
INFO - root - 2017-12-15 23:42:40.959914: step 74860, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 46h:56m:41s remains)
INFO - root - 2017-12-15 23:42:47.369463: step 74870, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:24m:48s remains)
INFO - root - 2017-12-15 23:42:53.763521: step 74880, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 45h:01m:05s remains)
INFO - root - 2017-12-15 23:43:00.183986: step 74890, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 45h:43m:29s remains)
INFO - root - 2017-12-15 23:43:06.545649: step 74900, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.626 sec/batch; 44h:48m:17s remains)
2017-12-15 23:43:07.100481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9900055 -1.8023419 -1.9113879 -2.1677189 -2.170805 -2.1605921 -2.124548 -2.2930155 -2.4077468 -3.8285801 -3.6292195 -5.1402683 -5.9698467 -7.0476561 -7.3081832][-2.402976 -2.6562023 -2.4398036 -2.54804 -2.7156572 -2.8077164 -2.7419553 -2.4816279 -2.1783071 -4.0259151 -4.0106187 -5.2310128 -5.4142451 -6.2516289 -6.7853732][-2.9243436 -2.6526694 -2.4195147 -2.5941067 -2.3868914 -2.2755513 -2.5758514 -2.7157969 -2.6033807 -3.8637536 -3.6066189 -4.9605618 -5.3670044 -6.2405348 -6.1843252][-3.9799249 -3.3652325 -2.6842084 -2.1163273 -1.8288379 -1.7177234 -1.6970291 -2.0073991 -2.023705 -3.648746 -3.6030283 -4.883544 -5.2969427 -5.9455967 -5.9993386][-4.5136604 -3.7576866 -3.1345286 -2.5753098 -2.0262976 -1.4753523 -1.2646112 -1.2386794 -1.2090487 -2.9811144 -2.9297428 -4.3547859 -4.988667 -6.0335779 -6.17549][-5.2042422 -4.6042285 -3.871676 -2.8315668 -1.9909229 -1.0515575 -0.62075853 -0.44204044 -0.34810066 -2.1123466 -2.2258058 -3.83458 -4.8033776 -6.2278786 -6.7226157][-4.7722297 -4.3136387 -3.7695079 -2.7209177 -1.6009064 -0.3784833 0.15056038 0.3206358 0.29694748 -1.4922409 -1.8325586 -3.7620845 -5.0084 -6.5313406 -7.3354635][-3.9464395 -3.66009 -3.0208812 -2.1127362 -1.3566933 -0.278656 0.36582184 0.6378336 0.6846323 -1.2058978 -1.4003429 -3.2878795 -4.9337544 -6.8906312 -7.8256316][-3.3787971 -3.3808322 -3.1543078 -2.2203794 -1.1195397 0.031876087 0.673996 1.0459175 1.3646107 -0.61339426 -0.97748137 -2.9856591 -4.3103132 -6.3207021 -7.5841403][-3.5457177 -3.3350458 -3.1735458 -2.5215015 -1.6733513 -0.53081703 0.10833168 0.5989151 0.94704628 -0.54421186 -0.62396383 -2.7323217 -4.1412659 -5.7999983 -6.9252591][-3.5406046 -3.894321 -4.0438309 -3.3987117 -2.6317067 -1.744082 -0.90067577 -0.78857422 -0.49097729 -1.5525546 -1.5222931 -2.9998527 -4.25833 -5.834332 -6.5333476][-3.3842883 -3.6184139 -4.1161289 -3.9413476 -3.3830128 -2.3810616 -1.87221 -1.8427734 -1.6730995 -2.6296358 -2.819767 -3.8673286 -4.6890125 -5.6641479 -6.4606457][-4.0036058 -4.53819 -4.7965345 -4.656723 -4.42274 -3.5653934 -2.8977351 -3.1308298 -3.1239491 -3.924171 -4.1458187 -5.0608397 -5.8162642 -6.1786995 -6.0639529][-4.7559338 -5.0047059 -5.7831321 -5.6280823 -4.9443393 -4.3528805 -3.9792588 -4.1409669 -4.191246 -4.9378843 -4.8948054 -5.2718191 -5.8509779 -6.2854786 -6.2763309][-5.8953204 -6.3363543 -6.50385 -6.5783386 -6.6219773 -5.9820824 -5.5869145 -5.7066936 -5.6869936 -5.8927488 -6.0258026 -6.4285326 -6.4384909 -6.4944463 -6.6945858]]...]
INFO - root - 2017-12-15 23:43:13.501558: step 74910, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 45h:18m:59s remains)
INFO - root - 2017-12-15 23:43:19.896464: step 74920, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 46h:49m:04s remains)
INFO - root - 2017-12-15 23:43:26.359806: step 74930, loss = 0.39, batch loss = 0.28 (12.4 examples/sec; 0.646 sec/batch; 46h:12m:51s remains)
INFO - root - 2017-12-15 23:43:32.764181: step 74940, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 45h:59m:30s remains)
INFO - root - 2017-12-15 23:43:39.264256: step 74950, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 46h:05m:43s remains)
INFO - root - 2017-12-15 23:43:45.592669: step 74960, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 45h:07m:33s remains)
INFO - root - 2017-12-15 23:43:51.961858: step 74970, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:23m:36s remains)
INFO - root - 2017-12-15 23:43:58.316226: step 74980, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 45h:17m:49s remains)
INFO - root - 2017-12-15 23:44:04.826301: step 74990, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 46h:02m:19s remains)
INFO - root - 2017-12-15 23:44:11.162224: step 75000, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 46h:33m:44s remains)
2017-12-15 23:44:11.675492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8875151 -7.653471 -5.9466085 -3.6749067 -2.5595627 -2.291873 -2.2297974 -2.1003327 -1.9776683 -3.8806283 -4.9578018 -5.8333397 -5.7277246 -6.8167849 -8.0228186][-6.0596862 -5.7961683 -4.499404 -3.1447425 -2.6057091 -2.0939817 -2.3887277 -2.7527008 -2.9995384 -4.6329241 -5.8382506 -7.3750772 -7.0592723 -7.0842981 -7.968637][-5.06909 -3.6066351 -1.655128 -0.50431061 -0.17996073 -0.10449076 -0.49257708 -1.0205741 -1.7224493 -3.6857142 -4.3250122 -5.3318682 -5.4890866 -5.86677 -6.949584][-2.9283972 -2.3982992 -0.90638971 1.042304 1.9217129 2.2178984 2.0394936 0.88741684 0.097625256 -2.5013008 -3.7730381 -4.2919779 -3.7809229 -4.1021109 -5.62066][-1.8698788 -0.2471571 1.4147825 2.2263117 2.312396 2.6594315 2.9539852 2.0259542 1.4133043 -0.48075151 -1.3974404 -2.3612556 -2.8277349 -3.2272029 -5.1454325][-2.2835994 -0.91005754 0.77925491 2.1557322 2.6990671 2.7924614 3.1281033 3.0665112 2.7860498 0.8543787 0.18779802 -0.22239065 -1.0077958 -1.6562376 -3.0213342][-2.8493047 -2.3195987 -0.40875959 1.2307062 1.9675674 2.5500479 3.1455345 3.2079086 3.2019262 1.631896 1.0208797 0.51679516 -0.7872715 -1.7332773 -3.4629035][-4.6917391 -3.2271056 -1.257112 0.50460148 1.5639305 2.1798182 2.813447 3.1416187 3.4129105 1.6480055 1.0032492 0.42817497 -0.89734077 -1.5150347 -3.3158908][-5.2716947 -4.515965 -2.3851061 -0.49248934 0.88796043 1.6240034 2.3748999 2.6587563 2.8649254 1.4233408 0.830143 0.087795258 -1.4148064 -2.6367288 -4.2298632][-6.5307279 -5.0698662 -3.1549234 -1.7485991 -0.38361168 0.32002544 1.1095991 1.41471 1.5877361 0.093633652 -0.4581933 -1.779983 -3.1448374 -3.77734 -5.2686357][-8.1195679 -6.6547904 -4.7704458 -3.0964255 -2.2293591 -1.3599615 -0.51306915 -0.46571779 0.05714035 -1.5069909 -2.8823223 -3.6364222 -4.6818166 -4.9800572 -5.8753643][-8.0947714 -6.2077808 -4.724967 -3.5320177 -2.379827 -2.0138984 -1.4080734 -1.6699562 -1.9568114 -2.7186794 -3.3093066 -4.1920109 -5.2790356 -5.1574783 -5.8668065][-7.9891777 -6.7470164 -4.6187124 -3.3343096 -2.8252139 -1.9536672 -1.263988 -1.7965183 -2.4441619 -3.6031251 -4.3389397 -4.8623533 -5.0305443 -5.0010338 -5.4666343][-7.6480789 -6.5162969 -5.0979919 -3.91393 -2.9173026 -2.3429475 -2.0532136 -2.6246929 -2.909575 -3.1212478 -3.5115294 -4.3106813 -5.1077232 -4.7827663 -5.1893377][-9.0869589 -7.6258135 -5.88258 -5.1367512 -4.1921091 -3.1873369 -2.5134745 -3.4158878 -4.1507874 -4.4670882 -4.5553303 -4.6207404 -4.8382549 -5.1416044 -5.0413237]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 23:44:18.892447: step 75010, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 45h:16m:58s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:44:25.274462: step 75020, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 45h:07m:03s remains)
INFO - root - 2017-12-15 23:44:31.654007: step 75030, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 45h:29m:25s remains)
INFO - root - 2017-12-15 23:44:38.124758: step 75040, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 47h:15m:11s remains)
INFO - root - 2017-12-15 23:44:44.500448: step 75050, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 44h:18m:41s remains)
INFO - root - 2017-12-15 23:44:50.945103: step 75060, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 46h:18m:56s remains)
INFO - root - 2017-12-15 23:44:57.401298: step 75070, loss = 0.35, batch loss = 0.23 (11.7 examples/sec; 0.683 sec/batch; 48h:50m:41s remains)
INFO - root - 2017-12-15 23:45:03.833664: step 75080, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 46h:18m:20s remains)
INFO - root - 2017-12-15 23:45:10.230518: step 75090, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 44h:46m:00s remains)
INFO - root - 2017-12-15 23:45:16.641655: step 75100, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 46h:13m:31s remains)
2017-12-15 23:45:17.245925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0869493 -4.32413 -4.313199 -4.1598463 -3.4851933 -2.8587399 -2.60854 -2.328126 -2.3350296 -2.390285 -3.3708811 -4.2757759 -5.4324827 -6.8778129 -7.5073953][-3.8647523 -4.000493 -3.972383 -4.021245 -4.1160355 -4.00498 -3.6313748 -3.7687125 -3.9050353 -3.4782777 -3.8034341 -4.5269842 -5.2484212 -6.7314148 -6.9754944][-2.5216236 -2.6449451 -2.8940969 -3.406651 -3.7510631 -3.6448727 -3.6115956 -3.6305227 -3.6358409 -3.4824042 -3.6932964 -4.5191946 -5.3908749 -5.9853477 -6.1424642][-1.5216141 -1.8504872 -1.93157 -2.294755 -2.3956084 -2.3776441 -2.2858143 -2.3328547 -2.4483194 -2.6701097 -2.950253 -3.9131944 -5.0965471 -6.0314355 -6.3614454][-1.8764319 -1.6847401 -1.1113105 -1.012589 -0.94714737 -0.3853488 0.058314323 0.11093092 0.16868305 -0.25965023 -0.83819962 -2.2074933 -3.87831 -5.443346 -6.1829081][-2.3449001 -2.2503109 -1.7026262 -1.2747302 -0.61762667 0.35600948 1.1544409 1.4831381 1.7384663 1.3512754 0.66485023 -0.84515715 -2.1869369 -4.0559697 -5.3916345][-3.0650439 -2.4943738 -1.9829011 -1.0652165 -0.31661463 0.90283108 1.9375391 2.268549 2.6280251 1.9473505 0.92122746 -0.66434526 -2.4538937 -4.4453611 -5.6279221][-3.6464434 -2.7535362 -1.7989688 -0.82739639 0.19170904 1.0740757 2.0335035 3.0478277 3.7620497 2.7763557 1.6137104 -0.403378 -2.2704296 -4.4348354 -5.8239703][-3.9773951 -3.2750707 -2.3108311 -1.1867867 -0.19786644 0.99694633 2.1519794 2.8306456 3.3716497 3.1072731 1.5672216 -0.50208855 -2.5604353 -4.8683281 -5.6697383][-4.6882005 -4.0853968 -3.4738564 -2.6338944 -1.6606026 -0.49052286 0.45157051 0.79573154 1.3121166 0.65819645 -0.42313433 -2.2277827 -3.7911577 -5.0391564 -6.0256152][-6.2199712 -5.927835 -5.7239323 -4.7709208 -4.096343 -3.1285868 -2.2892509 -1.90242 -1.5943322 -1.6786904 -1.9444208 -3.2822719 -4.6177588 -5.4942613 -5.9677162][-5.9706364 -5.7602186 -5.5150127 -5.1531029 -4.63944 -3.6066942 -2.8658781 -2.90235 -2.8699131 -3.6466246 -4.1387405 -3.9202886 -5.1944227 -5.5027785 -5.9366837][-6.3760304 -6.29312 -6.5077076 -6.4818425 -5.8698888 -5.00246 -4.3646407 -4.5277996 -4.684782 -4.8018456 -5.2799435 -5.8744454 -6.5627751 -6.4966469 -6.5289822][-6.000351 -6.0025263 -5.9796944 -6.1600275 -6.122056 -5.357698 -4.9353828 -5.4661851 -5.8384666 -5.572886 -5.9754429 -6.1960015 -6.6845779 -6.859551 -6.7784042][-7.4784713 -6.750453 -6.1430693 -6.1502681 -6.0172234 -5.5843363 -5.3078871 -5.2946057 -5.4634876 -5.6206474 -5.9151592 -6.4519725 -6.8945541 -6.7832956 -6.8684525]]...]
INFO - root - 2017-12-15 23:45:23.632319: step 75110, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:24m:36s remains)
INFO - root - 2017-12-15 23:45:30.013642: step 75120, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 45h:48m:16s remains)
INFO - root - 2017-12-15 23:45:36.400568: step 75130, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 44h:39m:57s remains)
INFO - root - 2017-12-15 23:45:42.840876: step 75140, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 46h:06m:37s remains)
INFO - root - 2017-12-15 23:45:49.278165: step 75150, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 46h:45m:22s remains)
INFO - root - 2017-12-15 23:45:55.701566: step 75160, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 44h:57m:37s remains)
INFO - root - 2017-12-15 23:46:02.064820: step 75170, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 45h:32m:16s remains)
INFO - root - 2017-12-15 23:46:08.495829: step 75180, loss = 0.35, batch loss = 0.24 (12.8 examples/sec; 0.625 sec/batch; 44h:38m:38s remains)
INFO - root - 2017-12-15 23:46:14.871300: step 75190, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 45h:33m:30s remains)
INFO - root - 2017-12-15 23:46:21.343453: step 75200, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 47h:04m:36s remains)
2017-12-15 23:46:21.857272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9051123 -6.7028651 -6.5306606 -5.6815414 -4.7377515 -3.5785151 -2.4845877 -1.9246125 -1.3406386 -2.2920909 -3.9747846 -6.18608 -7.3466191 -7.619493 -8.8909731][-8.25423 -8.2772732 -7.595746 -6.7513537 -5.6588316 -4.2493372 -2.9288945 -2.2774138 -2.0459619 -3.1890378 -4.6907463 -6.68302 -7.8812475 -7.7530818 -8.6512566][-8.0393772 -8.3094816 -8.0928965 -7.3711085 -6.0431604 -4.7146535 -3.3029675 -2.48425 -2.286634 -3.3130264 -5.1385889 -7.4754391 -8.0910587 -7.7971048 -8.6902933][-7.7760015 -7.7083755 -7.0884929 -5.87187 -4.60508 -3.4329515 -2.3282013 -1.6565585 -1.330821 -2.7289038 -4.6101694 -7.1327639 -7.9431925 -7.6132622 -8.2584915][-7.7038078 -7.461473 -6.6234946 -5.0464592 -3.2849083 -2.0461707 -0.95866108 -0.68841887 -0.84563112 -2.4100528 -4.5251322 -7.0160527 -7.8248496 -7.8588796 -8.6610231][-7.8011231 -6.866231 -5.6988621 -3.8855436 -2.2485185 -0.56117678 1.0934391 1.2818832 0.91268539 -1.3106718 -3.7577822 -6.7559862 -7.7414837 -7.9105134 -8.7420444][-7.581624 -6.8693633 -5.6052017 -3.5623155 -1.3864269 0.78642178 2.741271 2.9704189 2.6770172 0.22376108 -2.900497 -6.1981668 -7.822753 -8.1610842 -8.9525881][-7.0258465 -6.3106356 -5.36549 -2.9142909 -0.11314392 2.427227 4.8826094 5.1053991 4.787303 1.807745 -1.4614463 -5.1751957 -7.1870637 -7.628129 -8.6498194][-6.5172567 -5.9765353 -5.3961949 -3.5700607 -1.1748481 1.3539362 3.7214956 4.3576918 4.4177942 1.8295908 -1.3329482 -4.8089514 -6.6396451 -7.1941276 -8.2999105][-6.4251447 -6.2780175 -5.7961836 -3.9795191 -2.0539818 0.071241856 2.278882 2.855835 2.9705095 0.40507317 -2.3805089 -5.7369847 -7.1513839 -6.9622946 -8.007658][-5.3671904 -5.353735 -5.1740284 -3.7852876 -2.3424244 -1.0088267 0.89618683 0.70512772 0.17448044 -2.0933471 -4.7754493 -6.7901015 -7.6730618 -7.2607336 -7.5281482][-4.79084 -4.1578751 -3.5272861 -2.5359159 -1.7246127 -1.0261064 -0.062142372 -0.86546564 -1.2883172 -3.2741289 -4.81857 -6.5841928 -7.8415265 -7.3380337 -8.0126829][-6.2402573 -5.403553 -4.2565103 -3.1354032 -2.2880359 -2.1025877 -1.4567599 -1.927536 -2.615725 -4.3347206 -5.869565 -6.9275885 -7.8872805 -7.6360397 -8.2761793][-6.8113203 -5.7738132 -5.1174684 -3.9039414 -3.1808052 -2.59123 -1.9342284 -2.2829976 -2.5666308 -4.1155744 -5.133357 -6.6371236 -7.5061059 -7.4027853 -7.8518672][-7.5227795 -6.650207 -6.1261349 -5.2355661 -4.9733434 -5.0280356 -4.2545681 -4.7709322 -5.1392517 -5.7086954 -6.5884871 -7.1413164 -7.4528537 -7.4638295 -7.5123434]]...]
INFO - root - 2017-12-15 23:46:28.374784: step 75210, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.624 sec/batch; 44h:36m:38s remains)
INFO - root - 2017-12-15 23:46:34.698917: step 75220, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 46h:32m:38s remains)
INFO - root - 2017-12-15 23:46:41.163267: step 75230, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 47h:31m:07s remains)
INFO - root - 2017-12-15 23:46:47.470504: step 75240, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 45h:00m:02s remains)
INFO - root - 2017-12-15 23:46:53.798269: step 75250, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 44h:26m:42s remains)
INFO - root - 2017-12-15 23:47:00.135204: step 75260, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 45h:41m:05s remains)
INFO - root - 2017-12-15 23:47:06.481545: step 75270, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 45h:16m:00s remains)
INFO - root - 2017-12-15 23:47:12.989515: step 75280, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 47h:47m:11s remains)
INFO - root - 2017-12-15 23:47:19.444125: step 75290, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.658 sec/batch; 47h:02m:51s remains)
INFO - root - 2017-12-15 23:47:25.838975: step 75300, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 44h:53m:38s remains)
2017-12-15 23:47:26.446551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3692565 -5.9363403 -6.1242089 -6.22265 -6.5915575 -6.4310827 -6.0944376 -5.3116455 -4.5337381 -5.1505842 -5.6587076 -6.916429 -8.1022167 -8.7521248 -9.1795549][-4.5218339 -5.5012512 -6.3016543 -7.2423568 -7.6596413 -6.962 -6.1447411 -5.1389866 -4.4193916 -5.8994775 -6.4033766 -7.5645251 -9.0394888 -9.4740219 -10.080811][-2.9380078 -3.691082 -4.1117725 -5.1145992 -5.639864 -5.3848896 -4.9446249 -3.9550436 -3.1895595 -4.1971645 -4.9240055 -6.4779768 -7.6548352 -7.8999715 -8.2774315][-1.9773202 -3.0363016 -3.0571828 -3.2573009 -3.4217901 -2.7756076 -2.0336518 -1.7511601 -1.2455964 -2.3254538 -3.3340864 -5.280169 -7.1781864 -7.9701262 -8.233551][-2.2863231 -2.1425109 -1.5364275 -1.6144423 -1.0479717 -0.054337025 0.23874617 0.48290348 0.2871747 -1.3762846 -2.0622139 -3.9378774 -5.4782066 -6.6223035 -7.722002][-3.39212 -2.8398094 -1.6551676 -0.50295305 0.72922611 1.9564352 2.8292637 2.6709604 2.2751579 -0.13840008 -1.664125 -3.3744659 -4.8297706 -5.3559232 -6.1665297][-4.1066675 -3.4566808 -2.2132106 -0.11627913 1.9009275 3.4021444 3.9844685 3.7710009 3.3352795 0.72378445 -1.4309855 -3.9292145 -5.6265726 -6.0361142 -6.81155][-5.3463221 -4.1661592 -2.5464668 -0.062974453 2.3037777 3.7097158 3.8357906 3.1533909 2.4507065 -0.26815271 -2.254519 -4.783843 -6.3654747 -7.1143956 -7.717546][-5.6216831 -4.5282059 -2.9157314 -0.9337821 1.1897964 2.7268429 2.9766769 2.6988945 2.3746414 -0.64290714 -2.9209995 -5.1799908 -6.8533306 -7.5042939 -8.0639343][-6.9770069 -5.9490104 -4.463006 -2.3570018 -0.47462273 0.77469063 1.1278067 0.96762848 0.51879597 -2.33641 -4.279829 -6.3070536 -7.5329318 -8.0421515 -8.7405348][-7.8244557 -7.047812 -5.6755877 -3.7348452 -2.3994703 -1.5713234 -1.6614208 -2.0751247 -2.2007532 -4.6561403 -6.1173182 -7.7505927 -8.5741949 -8.9743948 -9.0994034][-7.5438194 -6.7281466 -5.6284013 -4.548233 -3.9350073 -3.5188532 -3.7919712 -3.9395733 -4.1101971 -5.8763647 -6.4792709 -7.5027962 -7.9903817 -8.4729595 -8.98723][-7.660244 -6.6964636 -5.698319 -4.7482853 -4.237298 -4.34814 -4.5809412 -5.1633253 -5.5562263 -6.5534449 -7.1186266 -7.7162051 -8.1199436 -8.053587 -8.082777][-7.5043936 -6.3412271 -5.4559307 -5.1196265 -5.0557847 -5.3780003 -5.9125738 -6.537581 -6.8000717 -7.5053706 -7.2159195 -7.1095681 -7.1207476 -6.9753656 -7.10153][-7.4497447 -6.7520127 -5.827569 -5.7698078 -5.9610033 -6.5033426 -7.1203623 -7.6436582 -8.023735 -7.9008942 -7.469018 -7.2145081 -6.7870483 -6.6266975 -6.4473095]]...]
INFO - root - 2017-12-15 23:47:32.826176: step 75310, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:21m:24s remains)
INFO - root - 2017-12-15 23:47:39.229382: step 75320, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:20m:05s remains)
INFO - root - 2017-12-15 23:47:45.558121: step 75330, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.635 sec/batch; 45h:20m:45s remains)
INFO - root - 2017-12-15 23:47:51.833737: step 75340, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.627 sec/batch; 44h:47m:50s remains)
INFO - root - 2017-12-15 23:47:58.178175: step 75350, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.642 sec/batch; 45h:52m:43s remains)
INFO - root - 2017-12-15 23:48:04.710141: step 75360, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 46h:01m:28s remains)
INFO - root - 2017-12-15 23:48:11.133866: step 75370, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 47h:30m:48s remains)
INFO - root - 2017-12-15 23:48:17.501910: step 75380, loss = 0.29, batch loss = 0.18 (13.0 examples/sec; 0.616 sec/batch; 44h:01m:13s remains)
INFO - root - 2017-12-15 23:48:23.821700: step 75390, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.620 sec/batch; 44h:15m:07s remains)
INFO - root - 2017-12-15 23:48:30.208772: step 75400, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 46h:37m:29s remains)
2017-12-15 23:48:30.689704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1081395 -1.9870644 -2.1083121 -2.7073946 -3.2913723 -3.5897436 -3.5980458 -3.5930305 -3.4706497 -4.8143306 -5.090271 -6.5993342 -7.2664919 -8.1595068 -9.1313963][-2.7747498 -2.8324814 -3.1293349 -3.5170069 -3.6823421 -3.5887804 -3.5124187 -3.6500368 -3.5107803 -5.06425 -5.7235565 -7.4464884 -8.567955 -9.4322424 -10.185875][-3.9600005 -3.8781221 -4.0815439 -4.1922073 -3.9672105 -3.4798436 -2.9716573 -2.9043036 -2.5373116 -3.6453557 -4.0040808 -5.3041706 -6.6356144 -7.6644173 -8.8079643][-5.5008364 -4.8345404 -4.4797182 -3.8783624 -3.0398631 -2.0805535 -1.2143793 -1.101408 -0.99190569 -2.4228058 -2.7047009 -4.1530781 -5.0923281 -6.2122679 -7.3311315][-5.9511633 -4.9907513 -3.8539717 -2.8167157 -2.0445194 -0.54890347 0.70838928 1.2600412 1.3448248 -0.73357964 -1.3395352 -3.0779929 -4.1448269 -5.1279364 -6.1817722][-5.1577234 -4.7272167 -4.0551968 -2.3153534 -0.89331818 0.77042484 2.2710772 3.1268702 3.4371786 1.1951246 -0.44945145 -2.7150879 -3.8155448 -4.6799946 -5.5484715][-5.67937 -4.7523971 -3.7713704 -1.7152114 0.26177883 2.2167187 3.6406126 4.2716818 4.7502413 2.4509783 0.7146225 -2.3554335 -4.4063349 -5.3588409 -6.0260496][-5.7550426 -4.5752969 -3.2387614 -1.3764768 0.61265182 2.8859558 4.1771622 4.701396 4.8528004 2.4628839 0.88253593 -2.1546965 -4.0088854 -5.7510424 -6.4489565][-4.3301373 -4.04723 -3.2346435 -1.6034021 -0.28859377 1.417058 2.622366 3.5599728 3.7857418 1.2998314 0.078552723 -2.5480418 -4.0135574 -5.4811459 -6.4207668][-4.4607167 -4.030045 -3.932575 -2.7289834 -1.6904159 -0.28383303 0.62986088 1.2202349 1.5655489 -0.68056154 -1.9004903 -4.4670773 -5.4551167 -6.2523842 -6.7432113][-6.7334833 -6.4888673 -6.2883706 -5.2986469 -4.3177056 -2.89359 -1.7742577 -1.170311 -0.91450167 -3.0096359 -4.1599517 -6.1663985 -7.125948 -7.927434 -7.9796524][-7.5110512 -7.3231769 -7.1305251 -6.6529741 -6.1318855 -5.2322383 -4.6939478 -3.9370551 -3.5228791 -4.7257895 -5.2439775 -6.7024479 -6.9114685 -7.5175295 -7.8596883][-7.349081 -7.4994297 -7.5730076 -7.2085271 -7.04392 -6.5911055 -6.3134975 -6.0066032 -5.8246922 -6.5213337 -6.59866 -7.3302073 -7.3422136 -7.1303444 -6.9969397][-7.5865335 -7.5017285 -7.4467497 -7.1204729 -6.9459267 -6.4519296 -6.488389 -6.5648479 -6.5820713 -7.1483369 -7.2423105 -7.3439083 -7.1346154 -7.2780876 -7.0737762][-8.7858944 -8.5768385 -8.212636 -7.9421949 -7.8681731 -7.7710805 -8.01081 -7.91967 -8.0052576 -7.9894996 -7.8149176 -7.6749239 -7.425684 -7.0652456 -6.9053597]]...]
INFO - root - 2017-12-15 23:48:37.100482: step 75410, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 45h:40m:21s remains)
INFO - root - 2017-12-15 23:48:43.478316: step 75420, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 45h:15m:21s remains)
INFO - root - 2017-12-15 23:48:49.820098: step 75430, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 46h:36m:42s remains)
INFO - root - 2017-12-15 23:48:56.193212: step 75440, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 44h:46m:51s remains)
INFO - root - 2017-12-15 23:49:02.609528: step 75450, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 47h:51m:37s remains)
INFO - root - 2017-12-15 23:49:08.988986: step 75460, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 45h:23m:35s remains)
INFO - root - 2017-12-15 23:49:15.359250: step 75470, loss = 0.34, batch loss = 0.23 (13.0 examples/sec; 0.617 sec/batch; 44h:01m:09s remains)
INFO - root - 2017-12-15 23:49:21.739031: step 75480, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.619 sec/batch; 44h:10m:51s remains)
INFO - root - 2017-12-15 23:49:28.096011: step 75490, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 46h:11m:18s remains)
INFO - root - 2017-12-15 23:49:34.511295: step 75500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 45h:47m:04s remains)
2017-12-15 23:49:35.032626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0040865 -3.2793055 -2.7860594 -2.4344468 -1.9318905 -1.5871568 -1.154675 -0.80267572 -0.98057604 -2.7019644 -3.9193587 -5.026453 -6.3248634 -7.1092167 -8.0086527][-3.596858 -3.2359328 -3.0863748 -2.7648005 -2.44524 -2.012794 -1.6310244 -1.3442578 -1.4760203 -3.1609073 -4.3915758 -5.4171362 -6.622355 -7.4657097 -8.2781258][-3.8820243 -3.6004257 -3.6784601 -3.3355665 -2.7458386 -2.1464891 -1.4325109 -1.1424327 -0.98920155 -2.436729 -3.4109521 -4.2279563 -5.3862143 -6.2099967 -7.3413787][-3.8755236 -3.7098265 -3.6384916 -3.1293621 -2.4321561 -1.6586747 -0.87145615 -0.4451189 -0.2861104 -1.7216983 -2.6102877 -3.5227098 -4.8305492 -5.6989489 -6.8265123][-3.9935079 -3.5534964 -3.0446339 -2.5001502 -1.9085546 -1.0404897 -0.055894375 0.19712782 0.42567444 -1.1318011 -2.1465974 -3.1732287 -4.522419 -5.2695503 -6.3290596][-3.9691615 -3.2879443 -2.8354969 -1.7553682 -0.74232912 -0.081843853 0.67762184 0.8535223 0.97794914 -0.61658 -1.7464404 -2.8430991 -4.2848444 -5.2414761 -6.343164][-3.8685167 -3.3738389 -2.5134716 -1.342998 -0.30020094 0.68694496 1.6377687 1.7451792 1.8018818 -0.0062389374 -1.4435859 -2.6924305 -4.1298885 -5.066761 -6.0395741][-3.7893271 -2.7498956 -1.635757 -0.42833042 0.85988522 1.6378136 2.2721167 2.4064054 2.4135437 0.51972008 -1.0704336 -2.7295928 -4.3522882 -5.312851 -6.2633114][-3.5496731 -2.5359364 -1.4429927 -0.068295 1.0324564 1.6256475 2.0032721 2.0651712 2.1464643 0.20091534 -1.4370642 -2.9853654 -4.8144789 -5.7659531 -6.7250266][-3.4708657 -2.2066445 -0.99368143 -0.023769379 0.95811272 1.3354111 1.4822216 1.4149809 1.316844 -0.72163582 -2.1390734 -3.8487215 -5.3735046 -6.1824675 -7.1424961][-4.702713 -3.6148229 -2.7816505 -1.6364226 -0.88674927 -0.533905 -0.49541903 -0.62709045 -0.67365408 -2.5400252 -4.0378885 -5.2791667 -6.2715931 -6.86399 -7.4352674][-5.5333557 -4.4595313 -3.7824473 -2.8943143 -2.0785518 -1.8872929 -2.0794559 -2.2394805 -2.4025083 -3.6367311 -4.6500411 -5.6885567 -6.4843073 -6.8338356 -7.2964735][-6.4585552 -5.7447958 -5.3754048 -4.8096862 -4.1372356 -3.8547568 -3.8879254 -4.1836309 -4.2108068 -4.9123263 -5.4400063 -5.7614274 -6.2362113 -6.2339087 -6.4837503][-6.7291579 -6.2365303 -6.1207156 -5.60647 -5.1494637 -4.7485991 -4.7848487 -4.8477116 -4.9402514 -5.3611817 -5.5568733 -5.6339498 -5.813314 -5.7003994 -5.8986521][-7.8068089 -7.2231293 -6.6989212 -6.6093359 -6.3640962 -6.1296206 -6.32987 -6.555449 -6.6961374 -6.3617077 -6.1270766 -6.1071553 -5.9045196 -5.699676 -5.5164208]]...]
INFO - root - 2017-12-15 23:49:41.449194: step 75510, loss = 0.28, batch loss = 0.16 (11.5 examples/sec; 0.695 sec/batch; 49h:38m:24s remains)
INFO - root - 2017-12-15 23:49:47.947824: step 75520, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 45h:20m:00s remains)
INFO - root - 2017-12-15 23:49:54.363494: step 75530, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 44h:42m:50s remains)
INFO - root - 2017-12-15 23:50:00.748790: step 75540, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 45h:21m:14s remains)
INFO - root - 2017-12-15 23:50:07.243172: step 75550, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 46h:54m:07s remains)
INFO - root - 2017-12-15 23:50:13.680784: step 75560, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 44h:19m:24s remains)
INFO - root - 2017-12-15 23:50:20.117017: step 75570, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 46h:36m:02s remains)
INFO - root - 2017-12-15 23:50:26.553145: step 75580, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 45h:20m:03s remains)
INFO - root - 2017-12-15 23:50:32.948789: step 75590, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 47h:00m:50s remains)
INFO - root - 2017-12-15 23:50:39.326636: step 75600, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 46h:00m:31s remains)
2017-12-15 23:50:39.861953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9138021 -6.0898519 -6.1930566 -6.1061335 -5.4280248 -4.4306355 -2.9222159 -1.1666536 0.28019238 0.035292149 -1.0285745 -4.0345254 -6.0978236 -8.3222628 -9.3006353][-4.8465967 -5.3705463 -6.0007606 -6.2608376 -6.2676954 -5.0639749 -3.5142951 -1.3750749 0.19255781 0.12578773 -1.1892948 -3.7355766 -5.6593466 -8.0791759 -9.53792][-4.6482935 -5.055315 -5.1677561 -4.9074926 -4.4617162 -3.6324382 -2.5273724 -1.3498769 -0.043606281 -0.29204988 -1.1014686 -3.6180482 -5.371438 -7.0924945 -8.3451614][-4.471529 -4.14627 -4.0356121 -3.6711774 -3.3011847 -2.3904305 -1.2983603 -0.45304585 0.29464865 -0.12535906 -0.924242 -3.506 -4.9509382 -6.5364933 -7.2792826][-4.6791868 -3.9911625 -3.4099565 -2.6821795 -2.0700994 -0.81758595 0.29965162 1.0148211 1.361187 0.57313919 -0.68622446 -3.2869334 -5.0427713 -6.4252615 -7.0979996][-4.4267263 -3.7525592 -2.6656041 -1.6784024 -0.53778839 0.63523388 2.188921 2.7087049 2.9567261 1.827795 0.23037815 -2.7614169 -4.7946215 -6.2227974 -7.0838718][-4.1209717 -2.9471555 -2.0285234 -0.96762228 0.21714783 1.246419 2.5579834 3.381835 3.9330263 2.6208305 0.94683838 -2.3676729 -4.4190369 -6.1427975 -6.9408913][-3.6698427 -2.3035283 -1.1402292 -0.20948696 0.82940197 1.5124178 2.2857971 3.1534338 3.7874508 2.8458023 1.0940361 -2.0660977 -4.3630896 -6.193294 -7.0484471][-3.2316513 -1.9209199 -0.65108204 0.39243412 1.4692593 1.794198 2.3274002 2.7244949 3.03055 1.936799 0.63967705 -2.1140537 -4.3472404 -6.308877 -7.4581413][-2.5996509 -1.69561 -0.38745785 0.46475697 1.2857389 1.6697874 2.298933 2.5429411 2.6927509 1.0068588 -0.77401924 -3.2828498 -4.9450049 -6.4394407 -7.3386083][-2.4528389 -1.7688193 -0.9166007 -0.015115738 0.89742851 1.0619402 1.4848137 1.6510105 1.7807226 0.06270504 -1.9205065 -4.2210608 -5.3944645 -6.7075987 -7.4619164][-3.192 -2.1329098 -1.2207565 -0.46933985 0.088527679 0.1142745 0.20365381 -0.071242809 -0.48764133 -1.3174734 -2.4228973 -4.2580767 -5.6456022 -6.8518667 -7.6992812][-4.4638557 -3.4450312 -2.2365518 -1.3446269 -0.66244316 -0.65389156 -1.1425643 -1.4394274 -1.6902351 -2.5947127 -3.4098191 -4.776782 -5.8535357 -6.6777182 -7.521461][-6.4088416 -5.7751169 -4.3734055 -3.4783592 -2.3046169 -1.6394167 -1.5460949 -1.6824875 -2.3630714 -3.1816249 -4.3208132 -5.3700037 -6.2230487 -6.692904 -6.5984416][-7.6057396 -6.9962339 -6.1866794 -5.5972538 -4.6942196 -3.814301 -3.2276826 -3.0943389 -3.7019243 -4.4222589 -5.5416937 -6.3972206 -6.5111923 -6.5777922 -6.7116742]]...]
INFO - root - 2017-12-15 23:50:46.370770: step 75610, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 46h:01m:47s remains)
INFO - root - 2017-12-15 23:50:52.750194: step 75620, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 45h:42m:23s remains)
INFO - root - 2017-12-15 23:50:59.133080: step 75630, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:14m:00s remains)
INFO - root - 2017-12-15 23:51:05.502911: step 75640, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 45h:33m:29s remains)
INFO - root - 2017-12-15 23:51:11.986331: step 75650, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 47h:03m:05s remains)
INFO - root - 2017-12-15 23:51:18.388408: step 75660, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 46h:33m:29s remains)
INFO - root - 2017-12-15 23:51:24.962467: step 75670, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.648 sec/batch; 46h:11m:49s remains)
INFO - root - 2017-12-15 23:51:31.416834: step 75680, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 45h:25m:15s remains)
INFO - root - 2017-12-15 23:51:37.822541: step 75690, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:13m:46s remains)
INFO - root - 2017-12-15 23:51:44.269620: step 75700, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 45h:10m:40s remains)
2017-12-15 23:51:44.832635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1440148 -5.35651 -5.0806341 -5.6850538 -5.6255264 -4.9367805 -5.1097097 -5.3728409 -5.1701956 -5.48988 -5.3961992 -5.2495794 -5.5413256 -5.8101053 -4.711545][-3.7578149 -4.5693893 -5.5141745 -6.1601768 -6.4348145 -5.9848595 -5.5469122 -6.0090771 -5.9013338 -6.4137187 -6.4766808 -7.0633984 -7.5016975 -7.4055905 -6.1963391][-3.6501131 -3.8722813 -4.6970243 -4.8478436 -4.79175 -4.9020104 -5.2632313 -5.5214643 -5.1862488 -5.6208038 -5.8387184 -7.0823035 -7.7920227 -8.2492619 -8.518013][-1.9829931 -3.2170029 -3.8932688 -3.3616304 -3.3114209 -3.8974035 -4.0962472 -4.072 -4.3223953 -5.0538931 -5.107163 -6.0928 -7.324666 -7.788126 -7.7542834][-2.9588165 -2.7581153 -2.1701407 -1.9871082 -1.6290984 -1.5545197 -1.8916726 -1.9675994 -1.8778539 -2.568686 -3.3984575 -4.36975 -4.7736425 -5.6260738 -6.4426489][-3.34404 -3.5004549 -3.8074834 -3.2670555 -2.1764317 -0.76936436 0.086623669 0.58390141 0.88913822 0.51651096 -0.50308371 -2.4654245 -3.9317367 -4.7699823 -4.9220014][-4.0018282 -4.0862041 -3.6687832 -2.6992111 -1.4753809 -0.26776791 0.82535458 1.8414907 2.9102983 2.6897898 1.141573 -1.640419 -3.8051357 -5.5646458 -5.8760653][-3.58077 -3.5562496 -3.4978952 -2.2666807 -0.62159348 0.95017242 1.9938641 2.7570305 3.5825243 3.670249 2.4571323 -0.479187 -2.8946805 -5.3215437 -6.3190556][-3.427115 -3.0411658 -2.7509727 -1.4621124 -0.017415524 1.3245945 2.2917242 2.7944307 3.4570351 3.0388994 1.7071648 -0.75997448 -2.6465988 -4.7786174 -5.7524171][-3.7269692 -3.4317679 -2.8795919 -1.618917 0.0064206123 1.438695 2.1941147 2.5203238 2.6194172 1.8221674 0.33125973 -2.0574923 -4.0946751 -5.8720617 -6.2913618][-3.2319484 -2.7072759 -2.3270931 -1.5229559 -0.41539335 0.79088116 1.6324244 1.7979059 1.9426937 0.73811722 -1.0108376 -2.7835107 -4.1893778 -5.7332311 -6.3819895][-3.1048746 -2.4717798 -1.540658 -0.94284678 -0.050931454 0.4117403 0.44252682 0.59617329 0.74309731 -0.18273783 -1.2090712 -2.5603943 -4.8103724 -5.8721433 -5.9097838][-3.4818735 -3.0434241 -2.1786895 -2.0584745 -1.3186355 -0.70232344 -1.0294151 -1.6922998 -1.984108 -2.775 -3.6632833 -3.5617423 -4.50554 -5.5493608 -6.1263747][-3.2896147 -2.8989992 -2.0211444 -1.6694613 -1.8172164 -1.9747787 -2.0905566 -2.637886 -3.1663723 -3.6878281 -4.0134473 -4.3943772 -5.0328865 -5.2061853 -5.4973888][-3.2876148 -2.8474474 -2.6125693 -2.9329553 -3.6358685 -3.9602609 -4.0499611 -4.8909111 -5.9262881 -5.8455968 -5.5081129 -5.2893066 -5.4011583 -5.3603611 -5.76128]]...]
INFO - root - 2017-12-15 23:51:51.167144: step 75710, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 45h:08m:12s remains)
INFO - root - 2017-12-15 23:51:57.608604: step 75720, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 46h:36m:27s remains)
INFO - root - 2017-12-15 23:52:03.980839: step 75730, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 45h:32m:43s remains)
INFO - root - 2017-12-15 23:52:10.418813: step 75740, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 45h:16m:46s remains)
INFO - root - 2017-12-15 23:52:16.864761: step 75750, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 45h:28m:52s remains)
INFO - root - 2017-12-15 23:52:23.340565: step 75760, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 45h:52m:49s remains)
INFO - root - 2017-12-15 23:52:29.886005: step 75770, loss = 0.32, batch loss = 0.20 (11.9 examples/sec; 0.672 sec/batch; 47h:53m:50s remains)
INFO - root - 2017-12-15 23:52:36.388740: step 75780, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 45h:55m:42s remains)
INFO - root - 2017-12-15 23:52:42.790039: step 75790, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 45h:32m:21s remains)
INFO - root - 2017-12-15 23:52:49.179363: step 75800, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 45h:51m:00s remains)
2017-12-15 23:52:49.723649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3858557 -4.7850738 -5.3715777 -5.4896822 -5.6515236 -5.1595812 -4.5445614 -3.8812289 -2.9817286 -4.1369829 -4.2689624 -6.3702054 -6.8657088 -6.6949949 -7.3330283][-3.6746974 -4.3759375 -5.3407249 -5.6134205 -5.8896732 -5.5760174 -5.7723002 -4.9882336 -3.8795264 -4.6553688 -4.545588 -5.8854594 -5.9281893 -6.4963059 -7.0964971][-3.18955 -3.7503359 -4.1603594 -4.75723 -5.7706981 -5.6880894 -5.2992811 -4.8312531 -4.8944631 -5.6685638 -5.5361695 -7.0065823 -6.8981318 -6.2789149 -6.5166769][-3.723295 -3.70723 -3.8150904 -3.6133099 -3.3736863 -3.3582525 -3.3295851 -2.8059511 -2.4290781 -3.5319996 -4.8349934 -6.4793119 -7.0761304 -7.1267195 -7.3385091][-5.0280981 -3.4759364 -2.4861836 -2.3177967 -1.9506054 -1.2387938 -0.24596453 -0.55855131 -0.40107679 -1.546536 -2.4863768 -4.1844912 -5.3382683 -6.086668 -7.3681455][-3.7416883 -3.2438874 -2.3536777 -0.33389473 0.56876278 0.693347 1.6579151 2.1789703 2.1241751 0.44378757 -0.66267252 -2.3467808 -3.4664803 -3.8580639 -5.2325644][-2.899827 -2.0771623 -1.4553347 0.0070061684 1.1468391 2.0714931 2.8648157 2.6503859 2.6068239 0.97809696 -0.25123167 -1.8789277 -2.5591583 -2.9662933 -4.7771912][-2.6507473 -2.2142029 -1.5314326 -0.086026192 0.59099293 1.3587122 2.1515188 2.1878119 2.26713 0.61082172 -0.56715822 -2.3915439 -3.3519111 -3.3595791 -4.4639711][-2.5701199 -2.1058645 -1.4811749 -0.97311306 -0.56372547 0.18859768 0.74360657 0.77871227 0.85170174 -0.6588006 -1.3999696 -2.7404976 -3.5732832 -3.97247 -5.1334305][-3.1420608 -2.240695 -1.6986551 -1.8113885 -1.8295369 -1.532527 -1.478107 -1.8156695 -1.8946905 -2.9913325 -3.4674568 -4.3392696 -4.5554972 -4.7214031 -5.6499653][-4.68459 -3.8782387 -2.979754 -2.9340816 -3.4426131 -3.2754951 -3.5097904 -3.8327525 -4.4634356 -5.2755823 -5.2854252 -5.7996588 -5.678195 -5.7217312 -6.5863938][-6.95339 -5.8370452 -5.3683395 -4.7801523 -4.4665694 -4.9442654 -5.0330009 -5.03556 -5.3061695 -6.4564753 -6.8363433 -6.9756083 -6.0003219 -6.4159741 -6.8391595][-8.0733376 -8.1854982 -7.8043976 -7.0006204 -6.51157 -6.2188473 -6.0445318 -6.0495887 -6.0425396 -5.9833746 -6.6691465 -7.0911918 -6.2879596 -6.116611 -5.9397106][-7.2509084 -7.2221713 -7.572875 -7.1566291 -7.2727895 -6.6715469 -6.3345785 -6.2338433 -6.3493304 -5.680829 -5.6635532 -5.459506 -5.31478 -5.4571514 -5.4841385][-6.5217023 -6.1825147 -5.899508 -6.2118149 -6.8653169 -6.5264597 -6.4911337 -6.3725009 -6.3576241 -6.0891914 -6.2709355 -5.7493849 -5.3504109 -4.9387913 -4.8620224]]...]
INFO - root - 2017-12-15 23:52:56.153842: step 75810, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:16m:33s remains)
INFO - root - 2017-12-15 23:53:02.579112: step 75820, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 46h:12m:41s remains)
INFO - root - 2017-12-15 23:53:08.979135: step 75830, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 45h:05m:34s remains)
INFO - root - 2017-12-15 23:53:15.308927: step 75840, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 45h:36m:07s remains)
INFO - root - 2017-12-15 23:53:21.695925: step 75850, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 44h:41m:07s remains)
INFO - root - 2017-12-15 23:53:28.096278: step 75860, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 45h:40m:34s remains)
INFO - root - 2017-12-15 23:53:34.559496: step 75870, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 45h:53m:12s remains)
INFO - root - 2017-12-15 23:53:41.022684: step 75880, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 46h:04m:37s remains)
INFO - root - 2017-12-15 23:53:47.412390: step 75890, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 44h:46m:43s remains)
INFO - root - 2017-12-15 23:53:53.816780: step 75900, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 46h:02m:59s remains)
2017-12-15 23:53:54.314327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5148983 -5.0622396 -5.1254063 -5.9403968 -6.22576 -6.9893384 -7.5614572 -7.3854136 -6.7456708 -6.1730604 -5.4998188 -5.4792223 -5.4507484 -5.3916559 -5.49891][-5.3681822 -5.1790962 -5.3287868 -5.4598012 -5.9868441 -7.2210875 -7.9319091 -8.2144308 -7.6568794 -7.1423788 -6.5708747 -6.5655842 -6.6438403 -6.0495472 -5.8739386][-6.1676593 -5.6806431 -5.3708024 -4.7796535 -4.6397514 -5.5306563 -6.2491536 -6.9469223 -6.8969517 -7.0711713 -6.8115788 -7.3508959 -7.809813 -7.49935 -6.9941974][-6.9665275 -5.8477411 -4.5531588 -3.1772375 -2.477663 -2.5349002 -2.7354102 -3.5384016 -3.3427944 -4.0959444 -4.661581 -5.99649 -7.1589937 -7.55112 -7.5874443][-6.5889444 -4.9861374 -3.0733643 -1.1317306 -0.29222059 0.53314495 0.68678856 0.0939765 0.15534687 -0.35553598 -1.2324529 -3.5737863 -5.4845934 -6.6843963 -6.8233843][-6.0177364 -4.3978195 -2.2231193 -0.33317471 1.0346375 1.9864531 2.2488356 2.1465025 2.490921 1.7140751 0.15525103 -2.6604548 -4.6341648 -5.9050207 -6.2660642][-5.1416073 -3.5974112 -1.60146 0.084445477 1.9974995 3.061677 3.4861822 3.6325426 3.5352783 2.3539133 0.34817314 -2.2780318 -4.0881786 -5.6509366 -6.1443682][-4.5280867 -2.9880848 -1.5595713 0.63784313 1.9938602 2.7863607 3.5293655 3.6088705 3.737793 2.6878605 0.92127323 -1.5032797 -3.3983192 -5.0010138 -5.8693857][-5.0396137 -3.768594 -2.4100614 -0.78573513 0.088098049 0.80556011 1.2804346 1.3898315 1.5249119 0.28109264 -0.96926165 -3.0183792 -4.8669538 -6.1187668 -6.5252843][-5.3361864 -4.9136181 -4.266273 -2.873642 -1.6664343 -1.0534005 -0.73921585 -0.56459808 -1.0231233 -2.2319689 -3.5642147 -4.8716946 -6.4732666 -7.238739 -7.8436794][-6.8828053 -6.5436988 -6.0508909 -4.8795424 -3.8829663 -3.4554625 -2.9780998 -3.1495857 -3.7487969 -4.3742433 -5.5077491 -6.237936 -7.4871678 -7.7204003 -7.44214][-8.8135729 -8.5316858 -8.2706184 -7.2882791 -6.3592339 -5.6612482 -5.1659403 -5.4526043 -5.642415 -6.3020353 -7.299696 -7.8891187 -8.5401783 -8.4504461 -7.8607559][-9.317132 -9.7630177 -9.8919048 -8.9504471 -7.8425679 -6.7018967 -6.1869287 -6.1880255 -6.3272195 -6.5779781 -7.3268361 -7.3290291 -7.7992492 -7.7666297 -7.5266905][-9.133894 -9.6754436 -9.5271645 -8.91117 -8.0922785 -6.7388897 -6.071321 -5.6462936 -5.82718 -5.5737405 -6.1180472 -5.9537649 -6.3285165 -6.4197192 -6.0907516][-9.4278135 -9.1215944 -9.0196047 -8.6432419 -8.0446529 -7.1052732 -6.5167179 -6.2349658 -5.6867862 -5.5037374 -5.6705475 -5.451067 -5.6596622 -5.7964773 -5.8416557]]...]
INFO - root - 2017-12-15 23:54:00.795515: step 75910, loss = 0.33, batch loss = 0.21 (12.1 examples/sec; 0.661 sec/batch; 47h:06m:07s remains)
INFO - root - 2017-12-15 23:54:07.167853: step 75920, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 44h:40m:42s remains)
INFO - root - 2017-12-15 23:54:13.561732: step 75930, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 44h:48m:34s remains)
INFO - root - 2017-12-15 23:54:19.923382: step 75940, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 44h:49m:43s remains)
INFO - root - 2017-12-15 23:54:26.346578: step 75950, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 46h:39m:59s remains)
INFO - root - 2017-12-15 23:54:32.812842: step 75960, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 46h:41m:25s remains)
INFO - root - 2017-12-15 23:54:39.245331: step 75970, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 44h:59m:46s remains)
INFO - root - 2017-12-15 23:54:45.659042: step 75980, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.640 sec/batch; 45h:36m:10s remains)
INFO - root - 2017-12-15 23:54:51.996139: step 75990, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 46h:03m:53s remains)
INFO - root - 2017-12-15 23:54:58.436189: step 76000, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 45h:15m:15s remains)
2017-12-15 23:54:58.922538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9444025 -3.5510159 -3.1918173 -2.0829902 -1.1101866 -0.019969463 0.91211796 0.81221867 0.98346043 -0.3955574 -1.4596772 -3.4114614 -4.689692 -5.968174 -7.1241465][-3.7377207 -3.1216707 -2.4925537 -1.7999749 -0.88864422 0.10551167 1.1413107 1.2870464 1.3175325 -0.50274706 -1.5290542 -3.2186074 -4.5721908 -5.9026518 -6.3650475][-3.6950223 -3.247159 -2.8638239 -2.1469407 -1.236958 -0.25658178 0.6421423 0.756526 0.8441515 -0.52363062 -1.3390837 -3.0309219 -4.2397814 -5.1204829 -5.8386774][-3.9211137 -3.5635014 -3.3386559 -2.6938643 -1.6015439 -0.37489557 0.57977867 0.73358917 1.0127172 -0.57858419 -1.7119484 -3.638998 -4.73125 -5.3878422 -5.71042][-4.0435314 -3.4827724 -2.8991656 -2.1679816 -1.3512049 -0.39532804 0.50637341 0.91505623 1.1154547 -0.66909075 -1.7838163 -3.4918127 -4.8874245 -5.9044347 -6.3724113][-4.3055916 -3.8330052 -3.3738222 -2.0851908 -0.75834179 -0.0034918785 0.71238422 1.1118841 1.2625885 -0.39246893 -1.3442969 -3.1033273 -4.370316 -5.4991221 -6.2541027][-4.4557934 -3.7021461 -3.3032608 -2.3495049 -1.3572288 -0.14850426 1.1155186 1.4282103 1.6797409 0.16173887 -0.96567583 -3.0112233 -4.5039 -5.5636082 -6.2605233][-4.32858 -3.1976357 -2.3208485 -1.4764891 -0.30961704 0.83179379 1.5972452 2.156868 2.6368742 0.884779 -0.28340578 -2.3638139 -3.9389005 -5.4687481 -6.6669888][-3.3993139 -2.8965635 -2.3212743 -0.92460442 0.27802467 1.0760241 1.9892683 2.4707918 2.6901436 0.898963 -0.19212389 -2.4775429 -4.1257486 -5.3893967 -6.3843555][-3.8000908 -2.9583249 -2.1892557 -1.3353047 -0.37172651 1.008564 2.1659527 2.3485031 2.5667353 0.88879585 0.010957241 -2.2582541 -4.0079012 -5.694706 -6.7911782][-3.9582973 -3.6005216 -2.9959049 -1.7864389 -0.610744 0.50244045 1.4913044 2.2295647 2.3263273 0.25334358 -0.7409668 -3.0045972 -4.8245177 -6.5853596 -7.4499431][-3.4508643 -3.1732082 -2.586411 -1.4078212 -0.10237741 0.91967106 1.9330158 2.1125193 1.9129286 0.31385994 -1.0784626 -3.5628219 -5.3429179 -6.561852 -7.4649768][-3.7035689 -3.1786437 -2.37742 -1.4988341 -0.58556414 0.28854036 1.045269 1.2517595 1.099103 -0.2417841 -1.3836474 -3.5398793 -5.2115169 -6.6506615 -7.4265413][-3.8325586 -3.4702749 -3.0130458 -2.3197656 -1.4017348 -0.72823811 -0.054985046 -0.0059776306 -0.30698538 -1.2230582 -2.1147242 -3.4901123 -4.8897357 -6.3356075 -7.1523919][-4.8042321 -4.5927544 -4.0375714 -3.6806641 -3.1166496 -2.7907696 -2.5410819 -2.2080626 -2.3950105 -3.1693463 -3.7704034 -4.7373314 -5.8605247 -6.5044923 -6.8365779]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 23:55:05.318011: step 76010, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 45h:22m:47s remains)
INFO - root - 2017-12-15 23:55:11.721654: step 76020, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:44m:37s remains)
INFO - root - 2017-12-15 23:55:18.055506: step 76030, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:10m:38s remains)
INFO - root - 2017-12-15 23:55:24.546816: step 76040, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 47h:33m:57s remains)
INFO - root - 2017-12-15 23:55:31.000518: step 76050, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.634 sec/batch; 45h:10m:26s remains)
INFO - root - 2017-12-15 23:55:37.342338: step 76060, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 44h:58m:57s remains)
INFO - root - 2017-12-15 23:55:43.798315: step 76070, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 46h:04m:00s remains)
INFO - root - 2017-12-15 23:55:50.209596: step 76080, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 45h:49m:37s remains)
INFO - root - 2017-12-15 23:55:56.652814: step 76090, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 46h:20m:58s remains)
INFO - root - 2017-12-15 23:56:03.029580: step 76100, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 44h:39m:29s remains)
2017-12-15 23:56:03.579841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.782124 -2.5927749 -1.6106534 -1.461513 -1.6864462 -2.0755038 -2.4418883 -2.0117435 -1.4663968 -2.5197959 -3.3973126 -5.1199884 -6.5650125 -7.4736571 -8.6724873][-2.3392138 -2.0422235 -1.7555566 -1.4616685 -2.1291738 -2.6015673 -3.2705975 -3.1091814 -2.2947626 -2.8278975 -3.2967863 -4.7220426 -6.4119821 -7.7875051 -8.8648043][-0.96030331 0.11682558 0.23624563 -0.055455208 -1.1527476 -1.965169 -2.2715898 -2.2359667 -1.6124473 -2.8882093 -3.1547079 -3.7797828 -5.0461712 -6.0261965 -6.7724242][-0.45354176 0.1938324 0.42441273 0.30534267 -0.25777149 -0.93420935 -1.0491743 -0.81222725 -0.47926474 -1.8241873 -2.3206844 -3.085269 -4.6981544 -5.4724126 -6.1863594][-0.73404074 0.36101627 0.67691994 0.37532139 -0.0032782555 0.058871269 0.48437786 1.0392237 1.5918369 -0.28756809 -1.0018907 -2.2670212 -3.9346673 -4.4707112 -5.5243073][-0.8248086 -0.63079071 -0.77154112 -0.31871319 0.65628338 0.99735546 1.6490307 2.0394669 2.70049 1.0156994 0.15993357 -0.32293129 -1.7528367 -2.4295793 -3.6241674][-2.1239333 -1.2316437 -0.85660744 -0.50495863 -0.037842751 1.0907726 2.4055462 2.9208117 3.4221058 1.3736563 0.30032587 -0.48643303 -1.994431 -2.8087811 -4.3372059][-4.1163282 -3.1716456 -2.2985468 -0.95177221 0.2571125 1.4169741 2.3162165 2.8624992 3.455802 1.443079 0.51300335 -0.5620594 -2.3113923 -2.6208181 -4.1202621][-4.8998432 -4.2826943 -3.2379227 -1.9445019 -0.62588453 0.86103439 1.9884996 2.6268873 2.8846178 0.80551529 -0.0097985268 -0.94887781 -2.7193546 -3.1579485 -4.0501833][-5.5222292 -5.1721034 -4.7516904 -3.2656288 -1.6499014 -0.21409225 0.92631912 1.2168531 1.3185616 -0.32195902 -1.1717467 -2.1993141 -3.797085 -3.8462644 -4.3584461][-6.3093791 -5.8599939 -5.8079844 -4.3821573 -2.7253017 -1.1752567 0.097970009 -0.063086987 -0.031666756 -2.073792 -3.1007614 -3.7535055 -5.4604912 -5.3104544 -5.297473][-7.4674625 -6.8901744 -6.380806 -5.1019759 -3.7390234 -2.5346384 -1.4976473 -1.6315012 -2.2142482 -3.7583008 -4.8186064 -5.0187788 -5.7795038 -5.7929516 -5.7911005][-7.3472323 -6.6651216 -5.8602157 -5.1895871 -4.6002684 -3.6227999 -2.9722781 -3.3600245 -4.016531 -5.2055545 -5.7589407 -5.6940823 -6.2913284 -5.9436831 -5.0420113][-7.6226707 -6.67594 -5.3586917 -4.56937 -3.572619 -3.0475726 -2.4664698 -3.1648006 -4.74667 -5.8607769 -6.2419162 -6.0939884 -6.5844159 -6.0800881 -5.781343][-7.59432 -6.8579383 -5.7842078 -4.9799433 -4.4082527 -3.7809145 -3.5915937 -4.1818504 -5.2773466 -6.0188231 -6.5195007 -6.5756855 -6.3278794 -6.242064 -5.8739986]]...]
INFO - root - 2017-12-15 23:56:10.068556: step 76110, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 46h:25m:59s remains)
INFO - root - 2017-12-15 23:56:16.428597: step 76120, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 46h:56m:18s remains)
INFO - root - 2017-12-15 23:56:22.852568: step 76130, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:42m:52s remains)
INFO - root - 2017-12-15 23:56:29.306640: step 76140, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 45h:59m:59s remains)
INFO - root - 2017-12-15 23:56:35.659958: step 76150, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.621 sec/batch; 44h:14m:37s remains)
INFO - root - 2017-12-15 23:56:42.097150: step 76160, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 47h:10m:26s remains)
INFO - root - 2017-12-15 23:56:48.518401: step 76170, loss = 0.23, batch loss = 0.12 (12.7 examples/sec; 0.628 sec/batch; 44h:44m:01s remains)
INFO - root - 2017-12-15 23:56:54.885463: step 76180, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.616 sec/batch; 43h:52m:59s remains)
INFO - root - 2017-12-15 23:57:01.377960: step 76190, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 45h:43m:03s remains)
INFO - root - 2017-12-15 23:57:07.714366: step 76200, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 45h:31m:45s remains)
2017-12-15 23:57:08.263127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6995325 -3.6410632 -3.9171617 -4.0942507 -3.7120912 -3.2401924 -3.0752969 -3.0444217 -3.2441902 -4.1658535 -4.8621092 -6.0242734 -6.8871069 -8.1587992 -8.0177708][-3.170578 -3.3382173 -3.9020991 -4.0340328 -3.5784883 -3.0386066 -2.7126756 -2.8693895 -3.0701838 -3.6235838 -4.1955986 -5.5044031 -6.3827314 -7.6667843 -7.9255252][-1.7173796 -1.7520986 -2.1517181 -2.3065157 -2.0399337 -1.5421381 -1.2400947 -1.2608457 -1.600831 -2.2889671 -3.1557498 -4.5749059 -5.4757347 -6.9135723 -7.3041868][-1.0445752 -1.0194149 -1.0460548 -0.934721 -0.95321894 -0.550807 -0.13616276 -0.16049194 -0.54294014 -1.4378448 -2.602922 -4.1456156 -4.8927231 -6.2521996 -6.5525794][-1.0095134 -0.78314018 -0.91891241 -0.79300976 -0.57695866 0.091163635 0.54880524 0.58464527 0.38096619 -0.59371471 -1.6065779 -3.1141844 -4.0204186 -5.4737787 -5.7429414][-0.94415092 -0.76078606 -0.73253107 -0.44762325 -0.31307316 0.35898495 1.1691265 1.2546482 1.1505842 0.25427866 -0.63748837 -1.7914581 -2.5720758 -4.0496469 -4.55327][-2.4440618 -1.88799 -1.5818543 -1.1953979 -0.53238344 0.59387684 1.3990774 1.5133829 1.4941044 0.63511372 -0.23859739 -1.4647069 -2.4000907 -3.9287553 -4.7593765][-3.5918598 -2.839889 -2.4040451 -1.6500096 -0.59720945 0.55221367 1.4828978 1.8560371 1.9071436 1.1846828 0.26417685 -0.93440104 -2.0435 -3.6384649 -4.6208811][-4.0990195 -3.0373297 -2.3915505 -1.6295304 -0.70568752 0.38196659 1.2585011 1.6481819 2.0466394 1.4728851 0.45668316 -0.81214142 -2.1729207 -3.9370933 -4.7213764][-4.8202991 -3.8366933 -2.9927115 -2.184763 -1.3247905 -0.34621859 0.52671623 0.91850853 1.3494816 0.75404167 -0.44681644 -1.5268226 -2.7862682 -4.1862979 -4.9587994][-6.0450907 -5.2356529 -4.551075 -3.5674081 -2.7101073 -2.0502491 -1.5194783 -1.2347217 -0.9972868 -1.8136687 -2.8429627 -3.2508297 -3.7515528 -4.5423808 -5.0314426][-7.3667297 -6.6583614 -6.1525397 -5.5979452 -5.067255 -4.3770776 -3.7630506 -3.6885757 -3.6699581 -4.2524204 -4.9999857 -4.9240866 -5.1219721 -5.5372248 -6.0811877][-7.6356306 -7.2260513 -7.0359049 -6.8608532 -6.5022359 -5.9811254 -5.6587534 -5.7932448 -5.873075 -6.0951061 -6.4039912 -5.9275365 -6.058085 -6.1099072 -5.9028521][-7.3978639 -7.1911626 -7.1826425 -7.4451284 -7.1978869 -6.9086266 -6.7802649 -7.0624676 -7.1830797 -7.0894895 -7.0711126 -6.5131674 -6.0579333 -5.8645482 -5.8779621][-7.9604578 -8.1585026 -7.8967462 -7.9695463 -8.0026112 -8.0606766 -8.04111 -8.1847744 -8.133585 -8.0454454 -7.7432828 -7.2139182 -6.5854111 -6.3692636 -6.4166207]]...]
INFO - root - 2017-12-15 23:57:14.686933: step 76210, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 46h:37m:11s remains)
INFO - root - 2017-12-15 23:57:21.103127: step 76220, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 44h:17m:44s remains)
INFO - root - 2017-12-15 23:57:27.519159: step 76230, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 45h:36m:30s remains)
INFO - root - 2017-12-15 23:57:34.007350: step 76240, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 46h:47m:24s remains)
INFO - root - 2017-12-15 23:57:40.422022: step 76250, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 45h:41m:21s remains)
INFO - root - 2017-12-15 23:57:46.816882: step 76260, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 47h:38m:47s remains)
INFO - root - 2017-12-15 23:57:53.219284: step 76270, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:09m:37s remains)
INFO - root - 2017-12-15 23:57:59.648343: step 76280, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 46h:00m:15s remains)
INFO - root - 2017-12-15 23:58:06.097896: step 76290, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 45h:02m:44s remains)
INFO - root - 2017-12-15 23:58:12.574265: step 76300, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.623 sec/batch; 44h:20m:22s remains)
2017-12-15 23:58:13.079135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7007074 -2.6708798 -2.8084121 -2.8386955 -2.7084336 -2.1464949 -1.63521 -1.3793221 -1.3687859 -2.1472263 -3.1681728 -4.483613 -5.1033115 -5.4437313 -6.5119443][-3.5904737 -3.7994533 -4.2876449 -4.0639353 -3.9130206 -3.4100919 -2.7752762 -2.2023292 -1.9432101 -2.7467852 -3.930433 -5.4580894 -6.0687032 -6.6882172 -8.06497][-4.28496 -4.583477 -5.0116243 -4.8519764 -4.2559962 -3.39951 -2.9366279 -3.1447539 -3.065557 -3.6036658 -4.7270079 -6.06126 -6.5983067 -7.0888033 -8.3978214][-5.2018118 -5.5142837 -5.6278653 -4.6129985 -3.4340463 -2.0526505 -1.1266785 -1.2604771 -1.7409563 -2.8838544 -4.3231173 -6.0745773 -7.2005877 -7.2985625 -8.3344145][-6.9749355 -6.4273133 -5.5714498 -4.2144976 -2.5244327 -0.59747267 0.81091022 0.69689178 -0.070890427 -1.9228778 -4.0498281 -5.9707246 -7.1100745 -7.3524208 -8.4910526][-8.8651781 -8.31838 -7.6424241 -5.477066 -2.6611171 -0.30649519 1.9042482 2.0047379 1.067009 -1.2099872 -3.5775027 -5.7041159 -7.05305 -7.3815975 -8.93526][-10.102932 -8.6475449 -6.7631049 -4.6975183 -2.0574851 0.72175694 3.3177652 3.6843882 3.0419626 0.51326275 -2.4943104 -5.2241011 -6.8916063 -7.3240232 -8.640955][-8.7921991 -8.0269375 -6.8553395 -4.0216112 -0.48353577 2.2346821 4.7641439 4.8480825 3.9161482 1.4570093 -1.1856413 -4.327424 -6.2891521 -6.5917015 -7.6645889][-8.77662 -7.6385927 -6.3187737 -4.4977841 -2.1305108 -0.0934453 2.4701176 3.5661554 3.9447823 1.3919573 -1.5339417 -3.8270397 -5.4039536 -6.2307959 -7.1829672][-8.8230295 -8.58241 -7.5944324 -5.7587295 -3.3718233 -1.7815304 -0.031522274 0.44498158 0.8119545 -0.79902411 -3.0055838 -5.1621761 -6.1523743 -5.9058704 -6.902113][-8.9389458 -8.8884678 -8.7232914 -7.5732369 -6.03975 -4.526402 -2.8211412 -2.2567821 -1.7292714 -3.2440634 -5.262836 -6.5911465 -7.4667273 -7.0152493 -7.455646][-9.3736267 -8.8812819 -8.3651552 -7.5757141 -6.8756795 -6.2170997 -4.98002 -4.286231 -3.65874 -4.5943928 -5.1994381 -5.9894319 -6.5839691 -6.2954855 -7.1740365][-9.3550816 -9.6312885 -9.3548422 -8.4396086 -7.6150255 -7.0104728 -5.9879332 -5.7902431 -5.2519417 -5.7538505 -6.5485225 -6.599896 -6.2589917 -6.0365591 -6.5577512][-8.7617569 -9.1787014 -9.0004787 -8.3967352 -7.4482961 -6.6742244 -5.8044181 -5.5663347 -5.6350393 -5.7869554 -6.1924839 -6.539916 -6.2724347 -6.3828621 -6.6699605][-9.5983658 -9.6001425 -9.4426937 -9.0925159 -8.18714 -7.8312507 -7.0121236 -7.2247858 -7.3911252 -6.9820485 -7.0013275 -7.3351846 -7.395144 -7.3125696 -6.9834194]]...]
INFO - root - 2017-12-15 23:58:19.496233: step 76310, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 45h:29m:05s remains)
INFO - root - 2017-12-15 23:58:25.948525: step 76320, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 45h:33m:56s remains)
INFO - root - 2017-12-15 23:58:32.453361: step 76330, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.652 sec/batch; 46h:23m:25s remains)
INFO - root - 2017-12-15 23:58:38.885812: step 76340, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 45h:40m:46s remains)
INFO - root - 2017-12-15 23:58:45.297250: step 76350, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 45h:26m:54s remains)
INFO - root - 2017-12-15 23:58:51.816970: step 76360, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 46h:03m:07s remains)
INFO - root - 2017-12-15 23:58:58.283825: step 76370, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 46h:34m:53s remains)
INFO - root - 2017-12-15 23:59:04.682615: step 76380, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 45h:38m:05s remains)
INFO - root - 2017-12-15 23:59:11.124863: step 76390, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 44h:55m:40s remains)
INFO - root - 2017-12-15 23:59:17.607543: step 76400, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 45h:16m:20s remains)
2017-12-15 23:59:18.144633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0397973 -3.5671992 -3.8767848 -4.5441933 -4.9730992 -5.1023827 -4.8841677 -3.9026828 -2.8010521 -3.8818839 -5.0710812 -5.9415088 -6.9018326 -7.4183335 -8.967145][-3.770411 -3.6257987 -3.8713007 -4.4142923 -5.2499981 -5.5456343 -5.1893406 -4.4585786 -3.5729294 -4.1584759 -4.4938169 -6.1044197 -7.7290688 -8.3830976 -9.816246][-2.5422158 -2.9694705 -2.9253192 -3.2097321 -3.3643022 -3.40618 -3.1830363 -2.8880968 -2.3827305 -3.3930831 -4.1838694 -5.1719117 -6.26144 -7.2694864 -8.8404465][-1.6947947 -1.7851143 -2.0105996 -2.1628084 -2.495254 -1.8845267 -0.86844015 -0.25277948 0.050012112 -1.3457026 -2.9319644 -4.2750187 -5.8800025 -6.82199 -8.5808887][-2.9405618 -2.0858259 -0.72696829 -0.39388847 0.11722469 0.55754185 1.110652 1.6675758 1.9140263 0.53247166 -0.738853 -2.3111029 -4.1409025 -4.7639608 -6.8466868][-2.7545161 -2.5719385 -1.9860306 -0.48171854 0.88047028 2.4699078 3.5703487 3.8160782 3.9453163 2.1315022 0.355649 -1.9327812 -4.1313643 -4.9738922 -5.9751139][-4.2974997 -3.8106906 -2.3809342 -0.88561821 0.83333683 3.1203938 4.4909334 4.5570431 4.1958008 1.9007807 -0.14386177 -2.518096 -4.6026649 -5.578392 -7.1839662][-4.6513567 -4.1553564 -2.8273244 -0.95499754 1.1224604 2.7245913 4.0121183 4.4118452 4.3662529 1.7109861 -0.67836905 -3.592504 -5.6730366 -6.5511789 -8.1095257][-5.5932665 -4.3758717 -2.6128445 -0.9057312 1.06637 2.5814867 3.1927986 3.4401827 3.6947536 1.4566593 -1.3405108 -4.0463362 -5.7623081 -6.6504297 -8.2274666][-5.9662 -5.3957682 -4.310667 -2.2311134 -0.2222805 1.2874012 1.574666 1.7506714 1.7905931 -0.26165295 -2.4781384 -5.1727934 -6.62824 -7.0926504 -8.4066324][-6.4755917 -5.5876756 -4.6295986 -3.4416661 -2.4472857 -0.9685545 -0.29981613 -0.33758593 -0.049346447 -2.2235694 -4.59768 -6.2461066 -7.2117844 -7.962235 -9.2012615][-6.4618082 -5.3046808 -4.4853477 -3.3654447 -2.6600771 -1.8738604 -2.1434588 -1.9834008 -1.9663429 -3.5136147 -4.7635183 -6.3178382 -7.1706138 -7.5778909 -8.7011986][-6.4308405 -5.4570436 -4.160141 -3.7260721 -3.7040312 -3.3913813 -3.0580382 -3.4820242 -3.7514687 -5.0324669 -6.4035125 -6.6261282 -7.1789966 -7.2001562 -8.0002642][-6.22254 -5.7042351 -5.04792 -4.3857021 -4.2854576 -4.1286073 -4.1857233 -4.367424 -4.6458626 -5.4296885 -5.8745322 -6.5961046 -6.901062 -6.7373161 -7.1426764][-6.4538422 -5.5341988 -5.316328 -5.240232 -5.4440274 -5.7794533 -6.2376714 -6.736517 -7.1056032 -7.0811429 -7.1861916 -6.9584088 -6.7833276 -6.6164403 -6.5568867]]...]
INFO - root - 2017-12-15 23:59:24.664224: step 76410, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 46h:53m:05s remains)
INFO - root - 2017-12-15 23:59:31.104480: step 76420, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 46h:56m:58s remains)
INFO - root - 2017-12-15 23:59:37.529816: step 76430, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 46h:24m:39s remains)
INFO - root - 2017-12-15 23:59:43.963984: step 76440, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:16m:21s remains)
INFO - root - 2017-12-15 23:59:50.316101: step 76450, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 45h:27m:40s remains)
INFO - root - 2017-12-15 23:59:56.769776: step 76460, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 45h:37m:54s remains)
INFO - root - 2017-12-16 00:00:03.166628: step 76470, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:56m:15s remains)
INFO - root - 2017-12-16 00:00:09.607389: step 76480, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:58m:09s remains)
INFO - root - 2017-12-16 00:00:16.122382: step 76490, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 46h:18m:33s remains)
INFO - root - 2017-12-16 00:00:22.466245: step 76500, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 45h:05m:46s remains)
2017-12-16 00:00:23.046036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1771164 -3.4098282 -3.2666111 -3.40034 -3.5856619 -3.2816105 -2.6430178 -2.3174696 -2.32196 -4.5602813 -4.8000646 -5.8786612 -6.5241466 -6.7922544 -7.6846271][-3.2777181 -3.07832 -3.3027306 -3.2443023 -2.8995609 -2.7969093 -2.5555291 -2.2309132 -1.7215939 -3.9700756 -4.4137058 -5.5813208 -6.0958719 -6.0751486 -6.6311603][-4.40333 -4.318265 -4.1426578 -3.8154464 -3.2798023 -2.7573247 -2.3493972 -2.3462105 -2.3037176 -4.5087328 -4.6899853 -5.9514465 -6.65709 -6.7642107 -7.0764856][-5.21281 -4.4815121 -4.4592552 -3.4939022 -1.8398628 -1.2447214 -1.1179051 -0.84675932 -0.6642518 -3.3526678 -4.0158191 -5.6172314 -6.2901545 -6.72377 -7.004981][-5.9639349 -4.2067327 -2.5765138 -1.2236328 -0.70992804 0.62987137 1.6647625 1.354743 0.882452 -1.9073644 -2.7340751 -4.6127028 -5.639575 -6.4430809 -6.8855038][-6.1020484 -4.6184478 -3.8617227 -1.54072 1.1399746 2.6074915 3.3468943 3.013464 2.2203503 -0.67834425 -1.6744452 -3.6110091 -4.8527989 -5.9487081 -6.766212][-5.6912594 -3.7690866 -2.4372573 -0.032169819 1.2387733 2.8760366 4.7822533 4.8288336 4.0530109 0.80992508 -0.53096485 -2.3780417 -3.6488762 -4.9417372 -5.9135561][-4.8918939 -3.1462574 -1.7691259 0.51740837 2.86757 4.4649639 5.183115 5.1696711 5.0901442 2.2772236 0.84825993 -1.114759 -2.5877867 -4.1457911 -5.12827][-4.795785 -3.4259982 -1.6278706 0.74398518 1.7267218 2.3361511 3.3090067 3.6043663 3.8394585 1.5708551 0.2319417 -1.6284056 -2.8097773 -3.947438 -5.1079159][-5.789165 -5.1123533 -3.9274292 -1.5706358 0.524539 1.5533609 1.8368607 1.4532042 1.4284534 -0.57822657 -1.1283011 -2.687079 -3.9088287 -4.1427994 -4.6421833][-5.8625917 -5.6749716 -5.8363628 -4.3973455 -3.0601158 -1.9819617 -0.91759205 -0.55328083 -0.51024485 -2.6618266 -3.6859837 -4.7622948 -5.2481937 -5.5085516 -5.6003489][-9.5295277 -8.5473928 -7.3322277 -6.5363069 -5.7013092 -4.6156321 -3.8768349 -3.701479 -3.6720171 -4.944911 -4.9957252 -5.2431736 -5.6560717 -6.110888 -6.4728527][-10.170293 -9.8765421 -9.6662979 -8.0733356 -6.7056637 -6.0473161 -5.5304289 -5.0445013 -4.6558843 -5.5041637 -5.9760637 -6.1194458 -6.1806607 -6.148819 -6.3447752][-9.2346392 -9.53771 -9.3477612 -8.4081039 -7.5117555 -6.5431557 -5.8426571 -5.6117077 -5.4333334 -5.6023645 -5.5620556 -5.8138971 -6.1564631 -6.2970061 -6.2007642][-8.8359718 -8.1418447 -7.7632961 -8.0643072 -8.3108521 -7.6670942 -6.7568865 -6.3101096 -6.245039 -6.3022385 -6.1889715 -6.2418027 -6.3169518 -6.7463555 -6.8291588]]...]
INFO - root - 2017-12-16 00:00:29.499376: step 76510, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 46h:04m:54s remains)
INFO - root - 2017-12-16 00:00:35.962658: step 76520, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 44h:57m:06s remains)
INFO - root - 2017-12-16 00:00:42.401122: step 76530, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 44h:50m:09s remains)
INFO - root - 2017-12-16 00:00:48.850769: step 76540, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 46h:06m:00s remains)
INFO - root - 2017-12-16 00:00:55.248970: step 76550, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 43h:53m:35s remains)
INFO - root - 2017-12-16 00:01:01.670495: step 76560, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 45h:34m:11s remains)
INFO - root - 2017-12-16 00:01:08.146569: step 76570, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 46h:26m:21s remains)
INFO - root - 2017-12-16 00:01:14.731779: step 76580, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 46h:06m:07s remains)
INFO - root - 2017-12-16 00:01:21.067026: step 76590, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 44h:25m:46s remains)
INFO - root - 2017-12-16 00:01:27.488599: step 76600, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 46h:36m:28s remains)
2017-12-16 00:01:27.967732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1771808 -2.9998927 -2.8551054 -2.6951251 -2.34476 -2.0887442 -1.4932437 -1.1645975 -0.79559278 -2.9167166 -4.1620727 -5.6486387 -6.4573689 -7.3207846 -7.9308281][-3.4837151 -3.6528373 -3.6971426 -3.7562926 -3.2146564 -2.6218395 -2.1222038 -1.8975506 -1.5946622 -3.4436545 -4.4674931 -5.860445 -6.565783 -7.6934304 -8.1850824][-3.0949788 -2.9646258 -2.9881387 -3.1118174 -2.5166125 -1.7451086 -1.1540174 -1.0993328 -0.95841885 -2.9174705 -4.0331683 -5.2094283 -5.8117776 -6.8146706 -7.4002404][-3.1500421 -3.100997 -3.2301908 -3.0931249 -2.474577 -1.8595166 -1.070025 -0.6950264 -0.26606941 -1.9061847 -2.6932845 -3.7873785 -4.669786 -5.528717 -6.1287217][-3.3193679 -2.8967996 -2.6664481 -2.4787111 -1.8306541 -1.0531778 -0.29721165 0.09056282 0.45665264 -1.2071939 -2.0031877 -3.1898651 -4.1540346 -5.19773 -5.8277416][-3.4192872 -2.7028036 -2.344418 -1.5208745 -0.65695572 0.023041248 0.57348633 0.78310966 1.00809 -0.65165234 -1.5049453 -2.7992382 -3.7732902 -4.953989 -5.7206154][-3.7330759 -2.8882108 -2.1341209 -1.0990314 -0.10915852 0.75954628 1.3122368 1.518527 1.6785431 -0.12563181 -1.0928793 -2.5733433 -3.7611 -4.7502308 -5.3491278][-3.979193 -2.8404627 -1.8501415 -0.51988554 0.64074326 1.5938492 2.1164789 2.3251019 2.3944826 0.312994 -0.85445881 -2.5189304 -3.7406876 -4.9285507 -5.3682079][-3.7985325 -2.7393775 -1.7342792 -0.385324 0.73864937 1.5582819 2.00276 2.0639563 2.0105228 -0.093806744 -1.2193751 -2.9213438 -4.1543703 -5.3483624 -5.8917675][-3.7026155 -2.8171825 -1.7135401 -0.534194 0.54606724 1.1739216 1.4357376 1.3506689 1.1535034 -1.0164795 -2.0206003 -3.7121069 -4.8638391 -5.8731132 -6.4026866][-5.118926 -4.3311138 -3.5275798 -2.4164729 -1.4910536 -0.7276473 -0.41593647 -0.49448013 -0.77189064 -2.9405518 -3.9330966 -5.2914839 -5.929883 -6.6165757 -6.6557841][-6.1968813 -5.5014129 -4.5035496 -3.6470127 -2.9013853 -2.2850933 -2.0588746 -2.2392325 -2.5483818 -3.9518194 -4.696795 -5.7737865 -6.270196 -6.72242 -6.7935023][-6.6376386 -6.2165418 -5.6601191 -5.0985684 -4.5990095 -4.0189872 -3.7704113 -3.8977423 -4.13624 -4.9825506 -5.296679 -5.6741209 -6.0209146 -6.2755442 -6.0367246][-6.6360416 -6.3541856 -6.1573715 -5.7576809 -5.6648121 -5.0850596 -4.9719105 -4.842926 -4.8434134 -5.4547443 -5.5099344 -5.4285197 -5.5450125 -5.5769329 -5.528944][-7.5953159 -7.289855 -6.5273304 -6.5093937 -6.6536961 -6.3283806 -6.2885008 -6.4214725 -6.5105491 -6.238101 -5.9705224 -5.9606276 -5.8470755 -5.6818781 -5.5320044]]...]
INFO - root - 2017-12-16 00:01:34.470462: step 76610, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 46h:34m:23s remains)
INFO - root - 2017-12-16 00:01:40.917703: step 76620, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 45h:36m:45s remains)
INFO - root - 2017-12-16 00:01:47.326452: step 76630, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 45h:03m:06s remains)
INFO - root - 2017-12-16 00:01:53.616810: step 76640, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 45h:15m:52s remains)
INFO - root - 2017-12-16 00:02:00.070582: step 76650, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 45h:00m:34s remains)
INFO - root - 2017-12-16 00:02:06.584193: step 76660, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 46h:40m:53s remains)
INFO - root - 2017-12-16 00:02:12.994160: step 76670, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 45h:01m:35s remains)
INFO - root - 2017-12-16 00:02:19.405715: step 76680, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 45h:57m:35s remains)
INFO - root - 2017-12-16 00:02:25.800476: step 76690, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 44h:47m:02s remains)
INFO - root - 2017-12-16 00:02:32.216571: step 76700, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 45h:57m:53s remains)
2017-12-16 00:02:32.729535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3035016 -5.3374481 -5.4290924 -5.6617355 -5.1659946 -4.2759333 -2.5310636 -0.78913116 0.7153759 0.89920044 -0.43136406 -3.6509538 -5.7031574 -8.1554489 -9.275157][-4.7019682 -4.8506966 -5.3078632 -5.5294895 -5.6914911 -4.7487268 -2.9248476 -0.9108963 0.45448208 0.5825243 -0.83129263 -3.5412512 -5.3156333 -8.027915 -9.50625][-4.5987158 -4.6057715 -4.8384523 -4.5401187 -4.0679655 -3.0728846 -2.0589356 -0.91603327 0.4747839 0.49716949 -0.88849783 -3.5720015 -5.3935404 -7.1188354 -8.1874876][-4.1372805 -3.9486089 -3.7978911 -3.4479394 -3.2158732 -2.2089887 -1.0112667 -0.061649323 0.87015343 0.77139759 -0.30420542 -3.307662 -4.7673874 -6.4661031 -7.5917783][-4.5142784 -3.77709 -3.1330595 -2.5038195 -1.8859386 -0.72893047 0.49265766 1.4017391 1.8532505 1.2425251 -0.34163237 -3.0517545 -4.72895 -6.6844192 -7.1572514][-4.3786092 -3.5462132 -2.517067 -1.5922861 -0.66130543 0.49161816 2.191021 2.666976 2.9656582 2.3544493 0.53374767 -2.7175622 -4.6168418 -6.3799968 -7.2395334][-4.222198 -2.9697952 -1.8172007 -0.98574734 0.10751915 1.1618662 2.4656496 3.419776 4.1866179 2.9740362 1.2666454 -2.2249184 -4.4470539 -6.2841849 -7.2443085][-3.9238567 -2.4561396 -1.1656199 -0.076153278 0.95614433 1.5227547 2.3429823 3.2907238 3.9774237 3.4167538 1.6079283 -1.8100128 -4.0997915 -6.3466792 -7.2949843][-3.2248378 -2.0108876 -0.72058344 0.3026228 1.3765097 1.8567648 2.4592915 2.9579859 3.3905401 2.5892296 1.1765471 -1.6222429 -3.8760378 -6.3169494 -7.7429247][-2.5059991 -1.7631235 -0.54078865 0.24649572 1.1594877 1.6958265 2.4040108 2.7523079 3.0584078 1.6467495 -0.12238741 -2.9178905 -4.6337295 -6.3889208 -7.5323849][-2.6296053 -2.0326281 -1.1288848 -0.24298954 0.87490749 1.0799751 1.4764967 1.815546 2.0836725 0.49368382 -1.5655642 -3.8739381 -5.029767 -6.66252 -7.6534214][-3.4243312 -2.48379 -1.3985848 -0.62889433 -0.042682171 0.18255568 0.30778646 0.030164242 -0.36930466 -0.97904634 -2.0323496 -3.9292622 -5.389329 -6.8129997 -7.8541026][-4.7967534 -3.6750727 -2.4415722 -1.6151309 -0.76849747 -0.44238806 -0.82851076 -1.1792002 -1.3656778 -2.3413367 -3.4742856 -4.5474758 -5.7671628 -6.7901621 -7.6050487][-6.2700176 -5.7305846 -4.4837389 -3.5221009 -2.3749127 -1.6765866 -1.7141867 -1.9298038 -2.3242712 -3.3272324 -4.4255185 -5.4561634 -6.3957286 -6.8048449 -6.6415][-7.4673061 -6.8986754 -5.7833996 -5.475584 -4.7646961 -3.8043535 -3.5239205 -3.5482349 -4.2048626 -4.6930552 -5.8537731 -6.5814133 -6.6905165 -6.6120181 -6.7886033]]...]
INFO - root - 2017-12-16 00:02:39.082975: step 76710, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 45h:37m:15s remains)
INFO - root - 2017-12-16 00:02:45.499604: step 76720, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 46h:48m:03s remains)
INFO - root - 2017-12-16 00:02:51.908824: step 76730, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 47h:00m:21s remains)
INFO - root - 2017-12-16 00:02:58.362051: step 76740, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 45h:30m:10s remains)
INFO - root - 2017-12-16 00:03:04.879736: step 76750, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.674 sec/batch; 47h:54m:43s remains)
INFO - root - 2017-12-16 00:03:11.302622: step 76760, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 44h:41m:01s remains)
INFO - root - 2017-12-16 00:03:17.657236: step 76770, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 43h:57m:03s remains)
INFO - root - 2017-12-16 00:03:24.040367: step 76780, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 45h:53m:03s remains)
INFO - root - 2017-12-16 00:03:30.482025: step 76790, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 45h:23m:24s remains)
INFO - root - 2017-12-16 00:03:36.870282: step 76800, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 46h:02m:55s remains)
2017-12-16 00:03:37.348750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1268067 -3.5499811 -4.2134666 -4.44524 -4.6374993 -4.713222 -3.6910429 -2.3051033 -1.6339574 -3.4756994 -5.0734487 -4.800354 -5.2591324 -7.0187922 -7.2669067][-4.7870913 -5.0599337 -5.0928335 -5.0308895 -4.816155 -4.4340777 -4.4771538 -3.8450787 -2.6737657 -3.4714622 -4.5597153 -5.1127062 -6.1663122 -6.8498974 -7.2017732][-4.6200147 -4.4553061 -4.4393864 -4.4935551 -4.2462707 -3.5714765 -2.6251454 -2.0922766 -1.716711 -2.6703744 -3.5481553 -4.6495767 -6.1699338 -6.972599 -6.6875057][-4.8060694 -4.227231 -3.8436587 -3.1986465 -2.989367 -2.3217459 -1.088788 -0.31420994 -0.13653708 -1.9371014 -3.4302645 -3.1886587 -4.5682039 -6.2746906 -6.4258013][-5.4656777 -4.1991515 -2.8808279 -2.3715177 -1.6870937 -0.85656595 -0.07868433 0.16657925 -0.17949867 -1.8541775 -3.6681252 -4.0116315 -4.9198275 -5.8051271 -5.7418494][-3.3382416 -2.9222293 -2.4040132 -1.4962716 -0.48047447 0.23502064 0.98772144 1.4316006 0.87398911 -1.6692357 -3.8275576 -4.6786885 -6.524085 -7.3448491 -6.2555432][-2.1616778 -1.5531912 -1.2801952 -0.33352423 0.52188873 1.3420486 1.8629475 2.0070972 2.1635036 -0.68268776 -3.1351504 -4.3430176 -6.2010117 -7.8897896 -7.6271768][-1.5602388 -0.92594337 -0.70248508 0.17391825 0.817997 1.394104 1.706109 2.3413019 2.41393 0.10280418 -2.4770155 -4.3032093 -5.824111 -7.2567043 -7.2644811][-1.8930326 -1.6967206 -1.2719636 -0.54595613 0.56582737 1.2267046 1.7597504 2.0270233 1.8427534 -0.21062756 -2.4423213 -3.5137892 -5.5569315 -7.4419427 -7.09946][-3.1063209 -2.4617863 -1.3287768 -1.3154478 -0.97106934 0.063351631 1.0318604 1.5406847 1.6543274 -0.39442444 -2.766458 -4.1153469 -5.3395386 -6.6554556 -6.9084535][-4.1215391 -4.8058767 -4.5541849 -3.517138 -3.0060568 -2.2245259 -1.4678416 -0.82725 -0.98780251 -2.2566209 -3.8235984 -4.7475195 -5.8253317 -7.0897927 -6.5425081][-6.4680367 -6.0894356 -5.9801154 -5.7188387 -4.5930719 -4.2936716 -4.1644616 -3.2337551 -2.5792704 -3.819443 -5.1002903 -5.073525 -5.6553535 -6.6240816 -6.4429541][-7.4536371 -7.6596184 -7.700243 -7.2478709 -6.5458131 -5.7291107 -4.9742432 -4.7969189 -5.1285257 -5.4092579 -6.32812 -5.9797773 -5.3818235 -5.7696571 -5.9007597][-7.1886821 -7.396697 -7.546207 -7.1863804 -6.9658036 -6.8425059 -6.7319832 -6.2515035 -5.6747322 -5.39082 -5.9210005 -6.055306 -6.3567343 -5.688972 -5.2617168][-7.7566261 -7.3838177 -7.2166295 -6.98141 -6.88279 -6.8317533 -7.0620694 -7.0468974 -7.2356067 -6.9207377 -6.4166517 -6.0397329 -5.8292603 -6.0393219 -5.9132018]]...]
INFO - root - 2017-12-16 00:03:43.808806: step 76810, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:12m:20s remains)
INFO - root - 2017-12-16 00:03:50.223857: step 76820, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:36m:31s remains)
INFO - root - 2017-12-16 00:03:56.649084: step 76830, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 46h:12m:15s remains)
INFO - root - 2017-12-16 00:04:03.114451: step 76840, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 45h:16m:28s remains)
INFO - root - 2017-12-16 00:04:09.498039: step 76850, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 45h:41m:38s remains)
INFO - root - 2017-12-16 00:04:15.924813: step 76860, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 45h:48m:43s remains)
INFO - root - 2017-12-16 00:04:22.369770: step 76870, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 44h:58m:55s remains)
INFO - root - 2017-12-16 00:04:28.740208: step 76880, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 46h:03m:18s remains)
INFO - root - 2017-12-16 00:04:35.222948: step 76890, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 46h:55m:22s remains)
INFO - root - 2017-12-16 00:04:41.667237: step 76900, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 45h:53m:04s remains)
2017-12-16 00:04:42.250553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5816936 -4.6316462 -5.0140767 -5.7699628 -6.0822048 -6.5694723 -6.76447 -6.5739112 -6.1193118 -6.3589082 -7.3699789 -7.6156783 -8.0462389 -7.7818046 -7.9393392][-4.4120846 -4.9665079 -5.2899933 -5.9350038 -6.5130854 -6.6486177 -6.1135683 -5.7784662 -5.5008078 -6.0016632 -6.498395 -6.2614861 -7.026412 -7.5087962 -7.7588582][-4.3446617 -4.6945076 -4.3014421 -4.73822 -4.69301 -5.0665622 -4.9921741 -4.5180092 -3.7566113 -4.279067 -5.3385534 -5.5293159 -5.9342546 -6.1123576 -7.0130167][-3.5492864 -3.8228583 -4.0018454 -4.0933132 -3.6366882 -2.8605437 -2.0931258 -1.5946002 -0.8979454 -1.6131082 -3.4283233 -4.1906681 -5.0665817 -5.5261817 -6.3383479][-4.4899073 -3.6593466 -3.3144603 -3.2591238 -2.4854875 -0.86987591 0.6175909 1.3646822 1.7596178 1.1051559 -0.75441837 -2.0817237 -3.5807695 -4.3817062 -5.6311369][-4.8723264 -4.6856294 -3.3256779 -1.4411473 0.059641838 1.3331518 2.84939 3.7125263 4.2779322 3.0199518 1.2321587 0.10555124 -1.5497031 -2.9557266 -4.5443168][-5.1037111 -4.0839453 -1.8686328 -0.47939253 1.1587496 3.0946846 4.6012497 4.647543 4.9126263 3.7524033 2.1141415 1.2058287 -0.58673429 -2.0221219 -3.9418895][-4.4434462 -3.4196777 -1.7032547 0.539011 2.4470367 3.6699 4.7338314 5.3300114 5.6330719 4.1460247 2.2355089 1.8915749 0.12031507 -2.1243792 -3.8893769][-4.2296391 -2.4009881 -1.1862164 -0.21249104 1.3941545 2.987855 4.048624 3.7870369 4.168294 3.7931538 2.1650343 1.058382 -0.73261023 -1.6287999 -3.3438945][-4.9600549 -3.9800172 -2.4964004 -1.2549314 -0.23446178 0.72197628 1.5775776 1.7074137 1.7058601 0.24448586 -1.3254256 -1.217237 -2.1402698 -3.0761209 -3.4617705][-6.3935065 -5.29702 -4.7359996 -4.2565193 -2.9600692 -2.4183078 -1.8127875 -2.1452665 -2.266726 -3.2056541 -4.0139976 -4.2699671 -4.4961686 -4.6548266 -5.2604756][-6.9042206 -5.9782591 -5.6891279 -5.7184625 -5.7310448 -5.3176203 -4.9134789 -4.9809456 -4.6575966 -5.2581372 -6.0080366 -5.2699423 -4.9827147 -5.5197563 -6.0457153][-5.7809386 -5.9996166 -5.9925675 -6.518168 -6.1816683 -5.9649973 -5.4844255 -5.6705604 -5.250701 -5.1675625 -6.0612249 -5.3512459 -5.1605353 -4.9475284 -5.0323553][-4.3624115 -4.3926535 -4.1136751 -5.10981 -5.22622 -5.419064 -5.1105461 -5.0937243 -5.200438 -4.849915 -5.0991936 -4.6444869 -4.2801905 -4.1517158 -4.7693834][-4.0853596 -4.0789509 -3.8571587 -4.4957361 -4.7267084 -4.8550282 -4.8247433 -5.0465493 -5.1164122 -5.0522261 -4.96445 -4.7960415 -4.7428374 -4.8288512 -4.8180285]]...]
INFO - root - 2017-12-16 00:04:48.651963: step 76910, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 45h:33m:21s remains)
INFO - root - 2017-12-16 00:04:55.095940: step 76920, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 46h:17m:53s remains)
INFO - root - 2017-12-16 00:05:01.594062: step 76930, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 47h:36m:43s remains)
INFO - root - 2017-12-16 00:05:07.991838: step 76940, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:05m:01s remains)
INFO - root - 2017-12-16 00:05:14.438315: step 76950, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:52m:15s remains)
INFO - root - 2017-12-16 00:05:20.848284: step 76960, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 45h:35m:28s remains)
INFO - root - 2017-12-16 00:05:27.255947: step 76970, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 45h:54m:34s remains)
INFO - root - 2017-12-16 00:05:33.659171: step 76980, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 45h:09m:17s remains)
INFO - root - 2017-12-16 00:05:40.034776: step 76990, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 45h:32m:31s remains)
INFO - root - 2017-12-16 00:05:46.434546: step 77000, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 45h:49m:57s remains)
2017-12-16 00:05:46.982297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1709948 -1.9148693 -1.9529734 -2.0149808 -2.018908 -2.0627003 -2.2696795 -2.4111543 -2.6776271 -3.8849235 -4.9134049 -5.880435 -6.2948833 -7.3472338 -8.4283495][-2.3140302 -2.1540623 -2.3998027 -2.4988618 -2.6411824 -2.8808007 -3.1292129 -3.3840704 -3.4726367 -4.1555858 -4.7479639 -5.7449765 -6.4897332 -7.1917925 -8.1434126][-2.5093594 -1.960237 -1.7057467 -1.5625896 -1.5088701 -1.5400639 -1.8557849 -2.5185585 -2.8179417 -3.3804812 -4.0024133 -5.1422687 -5.6545982 -6.45349 -7.21174][-0.96001959 -0.41758776 -0.080322266 -0.39115906 -0.71947241 -0.52793789 -0.51422119 -1.0230436 -1.6233749 -2.8039293 -3.5020185 -4.508461 -5.0799513 -5.5604463 -6.1140761][-0.6309948 0.18822718 0.67994213 0.46199322 0.02635479 0.19102049 0.18641996 -0.1946125 -0.66648436 -1.3942528 -1.7524781 -2.5142832 -2.8764892 -4.0068746 -5.105217][-0.71119452 -0.16310501 0.39801693 0.79558086 0.82496452 1.4212608 1.9579735 1.7525301 1.5484743 0.42168236 -0.12875843 -1.1779375 -2.1612763 -3.5685663 -4.9397688][-1.3581305 -0.44489384 0.28828287 0.77897167 1.0804739 1.4407635 1.8097391 1.9053965 1.9974833 0.86869812 0.12444067 -1.093204 -2.1155686 -3.5845985 -4.8241072][-1.8399115 -0.89964581 -0.30368519 0.24193859 0.8480854 1.2942762 1.8262978 2.1986656 2.5267544 1.5680227 0.95016956 -0.34162664 -1.6236267 -3.0855885 -4.4129791][-2.5602937 -1.6654515 -0.94316196 -0.46631432 -0.081625938 0.53784084 1.2854652 1.9659815 2.4739609 1.2972784 0.52003288 -0.71917391 -1.9727421 -3.6174326 -4.9512677][-3.8264453 -3.1210256 -2.3957529 -1.5761638 -1.2135391 -0.600883 -0.13016415 0.30401516 0.8708725 0.033450127 -0.69155169 -1.8909364 -3.2659698 -4.900867 -6.1038618][-4.8610697 -4.7613134 -4.3601284 -3.4183769 -2.8544321 -2.3168683 -1.7856865 -1.7078037 -1.5651755 -2.49794 -3.3720059 -4.0227728 -4.6871266 -5.7554984 -6.774169][-7.0590634 -6.5437536 -6.3430114 -5.7077942 -5.1894436 -4.7179031 -4.2288184 -3.8778331 -3.5711594 -4.439321 -4.9457045 -5.3624191 -5.9587874 -6.4337826 -7.304563][-7.4702535 -7.2052484 -7.0082812 -6.61096 -6.1567488 -5.752182 -5.4597135 -5.39837 -5.3732018 -5.553863 -5.6519704 -5.9674482 -6.3053441 -6.6271958 -7.2348723][-7.03187 -6.9496179 -6.8508663 -6.7916365 -6.63783 -6.0800114 -5.4768887 -5.4593592 -5.3384552 -5.7818122 -6.0854053 -6.2205071 -6.3124971 -6.2288971 -6.653553][-7.9612589 -7.8559208 -7.6468172 -7.5149651 -7.4287081 -6.9218936 -6.3608794 -6.4147325 -6.3657317 -6.2090869 -6.1748667 -6.0733175 -6.14941 -6.1521959 -6.1674633]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:05:53.470050: step 77010, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 45h:48m:04s remains)
INFO - root - 2017-12-16 00:05:59.861526: step 77020, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 44h:26m:32s remains)
INFO - root - 2017-12-16 00:06:06.259220: step 77030, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:04m:27s remains)
INFO - root - 2017-12-16 00:06:12.598621: step 77040, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 44h:45m:33s remains)
INFO - root - 2017-12-16 00:06:19.070929: step 77050, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 44h:32m:11s remains)
INFO - root - 2017-12-16 00:06:25.498107: step 77060, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 45h:35m:44s remains)
INFO - root - 2017-12-16 00:06:31.950567: step 77070, loss = 0.30, batch loss = 0.19 (11.8 examples/sec; 0.676 sec/batch; 47h:56m:16s remains)
INFO - root - 2017-12-16 00:06:38.418247: step 77080, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:58m:25s remains)
INFO - root - 2017-12-16 00:06:44.790672: step 77090, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 44h:29m:42s remains)
INFO - root - 2017-12-16 00:06:51.176366: step 77100, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 45h:38m:29s remains)
2017-12-16 00:06:51.771417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6541262 -5.0947886 -5.9964437 -6.8358297 -7.2011857 -6.9282393 -7.0672846 -7.0909462 -7.1843729 -8.4257717 -7.8389521 -9.1430826 -8.5078354 -7.0011911 -5.533989][-1.6289158 -2.7411103 -3.3961949 -4.0052395 -4.0243635 -4.1617932 -4.9293962 -5.2343817 -5.7074013 -6.9231448 -5.9930177 -7.7723746 -7.6618443 -6.3254671 -5.6704435][-0.75545692 -1.1679134 -1.9698858 -2.7459326 -2.8342414 -2.9286551 -3.6997726 -3.7929938 -4.6438179 -6.1207628 -5.5412722 -7.717772 -8.233098 -8.1195307 -8.16238][-1.5577393 -1.7522945 -2.6875229 -2.5071316 -2.1614242 -2.4139676 -2.6594005 -3.0237408 -4.1523275 -6.120841 -5.401403 -7.7644057 -9.1177282 -8.6809254 -8.9338713][-3.2890449 -2.8696785 -2.9107971 -2.2442355 -1.2254524 -1.2293711 -0.81416845 -0.71821976 -1.7575488 -3.3295026 -2.5818267 -4.3979611 -6.5550652 -7.6139803 -8.27376][-5.1280375 -4.3901005 -4.4754095 -3.6881866 -2.353538 -0.72980022 0.95930576 1.438159 1.1376123 0.84767818 1.23662 -1.3618631 -2.8735867 -5.2010927 -6.5695181][-4.6738019 -4.4922972 -3.3938003 -1.6824217 -0.072793484 1.73843 3.521224 3.7174129 3.9844151 3.5084286 3.8289585 1.1950321 -1.3894625 -2.9398475 -4.5529728][-4.42725 -3.9412007 -3.1071482 -0.72339153 2.0260687 4.4061203 6.360034 6.8170824 7.1575365 5.3588982 5.836669 3.821023 1.1983566 -1.2973552 -2.44059][-4.4387083 -4.2320185 -3.5733581 -1.0922265 0.80477715 3.1928778 5.0787134 6.2336044 7.2541647 5.4492655 5.1781273 1.5480347 -1.3752646 -2.8069901 -4.3922319][-6.4209547 -5.8483477 -5.3963642 -3.5898671 -1.3007703 0.86506939 2.551755 3.7076645 3.9875078 1.9012489 2.25669 -1.4459863 -4.7543793 -5.5593653 -6.8699274][-7.9440432 -7.7802482 -7.8601708 -6.4764681 -5.0138941 -2.8520827 -0.696002 0.22509813 1.2267618 -0.1153512 -0.41329384 -2.521956 -5.677537 -6.8181791 -8.3910341][-9.1337671 -8.5238552 -8.9540138 -8.9121876 -7.4616804 -6.137991 -4.9409885 -3.6561551 -1.8493366 -2.2979245 -1.8720126 -3.460238 -5.3607759 -6.6306939 -8.0947189][-8.6469135 -8.2261572 -9.24424 -9.1448326 -8.33501 -7.5597234 -7.061089 -6.3173175 -5.7515116 -5.5947585 -4.5484867 -4.8548741 -5.9435425 -5.6203995 -6.9469509][-8.9246578 -9.0546169 -9.1817417 -8.7165747 -8.6531515 -8.3538628 -7.9821911 -7.7517529 -7.4399958 -7.4786067 -7.1071162 -6.9371881 -6.9199915 -6.3320389 -5.4851208][-8.8265486 -8.4048309 -8.9754791 -8.2364359 -7.8008857 -7.6043482 -7.3454661 -7.7741714 -7.82396 -8.0003567 -7.6321278 -7.3081942 -7.0004964 -6.9724936 -6.5280428]]...]
INFO - root - 2017-12-16 00:06:58.198601: step 77110, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 44h:12m:45s remains)
INFO - root - 2017-12-16 00:07:04.645817: step 77120, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 45h:49m:21s remains)
INFO - root - 2017-12-16 00:07:11.117892: step 77130, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 46h:06m:58s remains)
INFO - root - 2017-12-16 00:07:17.502498: step 77140, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 45h:44m:31s remains)
INFO - root - 2017-12-16 00:07:23.933060: step 77150, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 47h:19m:48s remains)
INFO - root - 2017-12-16 00:07:30.349390: step 77160, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 45h:09m:12s remains)
INFO - root - 2017-12-16 00:07:36.726589: step 77170, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 44h:42m:24s remains)
INFO - root - 2017-12-16 00:07:43.109945: step 77180, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 44h:36m:50s remains)
INFO - root - 2017-12-16 00:07:49.458483: step 77190, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 44h:26m:56s remains)
INFO - root - 2017-12-16 00:07:55.860260: step 77200, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 44h:55m:16s remains)
2017-12-16 00:07:56.454031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8548927 -4.2773495 -3.771441 -2.9008789 -2.135602 -1.8298779 -1.92903 -1.3113089 -0.60266018 -2.4546728 -3.8798647 -5.2916651 -7.5826654 -9.2108746 -10.658045][-5.6223335 -5.1384535 -5.1337709 -5.2985058 -5.191226 -4.8179617 -4.0602741 -3.594842 -3.4757648 -4.4457741 -5.390727 -6.7984309 -8.8191214 -10.151299 -11.691607][-5.7060595 -4.5016594 -3.9391475 -4.0716786 -4.0892162 -3.7784951 -3.5385556 -3.6374912 -3.3756437 -5.09147 -6.5957975 -7.346406 -8.6464376 -9.7795906 -10.908964][-4.5500317 -3.5938573 -2.5278435 -1.7128029 -1.1271982 -1.1805301 -0.97163439 -0.98814487 -1.4218292 -3.4726672 -5.1676474 -6.2819695 -8.1898746 -9.0971689 -9.535594][-4.3104606 -3.2936339 -1.861671 -0.69330215 0.45294285 1.0248432 1.3767519 0.94045544 0.27443886 -1.5733242 -3.3330655 -4.448884 -6.349822 -7.0503378 -7.8636332][-3.7690709 -2.7424803 -1.740243 -0.51299 0.98342705 2.1024561 2.8000612 2.6626043 2.3836555 0.545249 -1.1887841 -2.0791903 -3.8704891 -4.86287 -5.6684265][-3.6371551 -2.6483016 -1.1674805 0.51100636 1.9547482 3.0595894 3.6058083 3.6247711 3.3493004 1.5797834 -0.056053638 -1.0392509 -3.0591993 -3.808285 -4.7326245][-2.7538357 -1.3342733 -0.23407602 1.2308903 2.6925325 3.480835 3.8793917 3.6526403 3.3964586 1.6249208 -0.0074214935 -1.0603685 -2.7920346 -3.8122175 -4.6681566][-0.56429815 -0.0077533722 0.51463032 1.0995731 1.7566519 2.3502684 2.4528198 2.6083784 2.8517885 1.4416084 0.23383474 -0.653666 -2.7850695 -3.7465022 -4.5394344][-0.15953064 0.5351696 0.65535831 0.64943027 0.79456234 1.2148685 1.7161961 1.7952785 1.8825054 0.80808163 -0.22896194 -1.3407588 -2.7522297 -3.9135222 -4.6664405][-0.070389271 0.40942764 0.70521736 0.61707973 0.59148884 0.57751083 0.47768497 0.61044979 0.46491432 -0.99775839 -2.234498 -3.052815 -4.4991159 -5.2378445 -5.4594736][0.0018076897 0.13503647 0.21943617 -0.18619061 -0.61552048 -0.90927839 -0.8330555 -1.2423887 -1.6416388 -2.864933 -3.9022729 -5.1298723 -6.0614419 -6.3206477 -6.2104268][-1.3546944 -1.0894942 -1.1447477 -1.965704 -2.6683784 -2.5779972 -2.6655116 -3.06101 -3.5900021 -5.1058578 -6.2164812 -6.8530703 -7.6895723 -7.7048922 -6.9311385][-3.2882047 -2.6894813 -2.2053008 -2.0978436 -2.183804 -2.9902821 -3.1551571 -3.9101746 -4.8736978 -6.1588259 -6.7233887 -7.569788 -8.0727348 -7.9791403 -7.5494981][-4.987565 -4.0993538 -3.8027656 -3.5072837 -3.871007 -4.3589773 -4.9008236 -5.7201219 -6.3983073 -7.0195022 -7.6224351 -8.0516081 -8.5111818 -8.47388 -8.0063181]]...]
INFO - root - 2017-12-16 00:08:02.865020: step 77210, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 46h:28m:17s remains)
INFO - root - 2017-12-16 00:08:09.355539: step 77220, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 47h:00m:31s remains)
INFO - root - 2017-12-16 00:08:15.785987: step 77230, loss = 0.34, batch loss = 0.23 (12.3 examples/sec; 0.648 sec/batch; 45h:58m:08s remains)
INFO - root - 2017-12-16 00:08:22.198862: step 77240, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 47h:17m:37s remains)
INFO - root - 2017-12-16 00:08:28.678666: step 77250, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 45h:41m:53s remains)
INFO - root - 2017-12-16 00:08:34.979405: step 77260, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 46h:03m:17s remains)
INFO - root - 2017-12-16 00:08:41.327342: step 77270, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 44h:41m:50s remains)
INFO - root - 2017-12-16 00:08:47.812854: step 77280, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 44h:37m:06s remains)
INFO - root - 2017-12-16 00:08:54.210578: step 77290, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 44h:32m:04s remains)
INFO - root - 2017-12-16 00:09:00.616873: step 77300, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 45h:48m:16s remains)
2017-12-16 00:09:01.163165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9644461 -3.0968075 -3.8308377 -4.5062928 -5.7119894 -5.8309751 -6.1805911 -6.489624 -6.8245878 -8.6267157 -9.588748 -11.219205 -11.475597 -11.301572 -10.933632][-2.9199958 -3.7875707 -5.0041261 -5.39419 -5.6614752 -5.3789978 -5.5301895 -5.616147 -5.8655629 -7.9467964 -8.881238 -10.494875 -10.934545 -11.579893 -11.639854][-3.6723766 -3.2363858 -3.48041 -3.7036543 -3.9747267 -3.4793653 -3.3097782 -3.319488 -3.5091729 -4.8639684 -5.5628157 -7.415287 -8.3839836 -9.3703632 -9.9869041][-4.660305 -4.4682722 -4.3803577 -4.0353169 -3.6274576 -2.7886763 -1.8959932 -1.3360515 -0.84427595 -1.9852786 -2.93825 -4.616961 -5.8297305 -6.9147835 -7.5189734][-5.8384171 -4.8089466 -3.192801 -2.2381806 -1.8696947 -0.97939062 0.031919956 0.46674919 1.0443516 0.29543161 0.029750347 -1.1884198 -2.1067939 -3.6089425 -4.9357677][-5.828968 -4.7245078 -3.3920417 -1.313026 0.10641527 1.2467413 1.8652239 1.8865614 2.331706 1.5481749 1.0366182 -0.39075232 -1.4569726 -2.7094755 -3.7452061][-5.0502977 -4.0197277 -2.5538616 -0.32786369 1.3341742 2.2917519 2.887557 3.2691355 3.4601097 1.9837885 1.1266212 -0.4861846 -1.903821 -3.3164248 -4.2322307][-3.6030931 -2.8271995 -1.977994 -0.16242552 1.9541807 3.1041794 3.968461 4.0858221 4.0189819 2.0041838 0.75958538 -1.1944509 -2.6583161 -3.8550653 -4.5989513][-4.0406046 -2.5499754 -1.2053585 0.21164322 1.4997454 2.8183098 3.5271654 3.2831535 3.2645369 1.0713587 -0.58908081 -2.5085731 -3.9703624 -5.084816 -5.6598949][-3.3688798 -2.2246141 -1.6231561 -1.0844007 -0.095896244 0.98604584 1.5903454 1.4618387 1.2178106 -0.892571 -2.7340288 -5.0693321 -6.5447788 -7.1986179 -7.2893109][-5.2391024 -4.5109496 -3.6252804 -3.5265675 -3.1070795 -2.3071928 -1.7568793 -2.0989103 -2.3657918 -3.899735 -5.6581488 -6.8823061 -7.7732577 -8.6613541 -8.9848957][-5.8562937 -5.4034863 -5.2443705 -5.2309928 -5.1338177 -5.029501 -5.1384983 -5.3651156 -5.2633505 -6.1761765 -7.1621904 -7.8863111 -8.5958033 -8.6659641 -8.8056107][-7.2747297 -6.7308388 -6.6122284 -6.6398058 -6.8865337 -6.8808255 -7.0668964 -7.3594093 -7.3925266 -7.5200438 -8.0741034 -8.2099819 -8.24059 -8.0198784 -7.5221152][-6.9137645 -6.7080941 -6.4265289 -6.2822886 -6.4984651 -6.5083303 -6.6756582 -6.8272371 -6.8574052 -7.2843928 -7.6142111 -7.6203284 -7.4544163 -7.2791114 -7.2219467][-7.6740575 -7.2206049 -7.3963284 -7.4741378 -7.5223732 -7.1800508 -6.7970834 -6.5481949 -6.48318 -6.5708961 -6.2992506 -6.5537529 -6.9536428 -6.7328115 -6.5104713]]...]
INFO - root - 2017-12-16 00:09:07.625839: step 77310, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 45h:24m:22s remains)
INFO - root - 2017-12-16 00:09:14.059116: step 77320, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 44h:16m:24s remains)
INFO - root - 2017-12-16 00:09:20.435742: step 77330, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 45h:25m:51s remains)
INFO - root - 2017-12-16 00:09:26.816471: step 77340, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.628 sec/batch; 44h:30m:39s remains)
INFO - root - 2017-12-16 00:09:33.329140: step 77350, loss = 0.24, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 45h:51m:57s remains)
INFO - root - 2017-12-16 00:09:39.663370: step 77360, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 44h:46m:52s remains)
INFO - root - 2017-12-16 00:09:46.032256: step 77370, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.613 sec/batch; 43h:26m:01s remains)
INFO - root - 2017-12-16 00:09:52.427249: step 77380, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 45h:35m:49s remains)
INFO - root - 2017-12-16 00:09:58.732804: step 77390, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 44h:22m:25s remains)
INFO - root - 2017-12-16 00:10:05.129294: step 77400, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.629 sec/batch; 44h:35m:55s remains)
2017-12-16 00:10:05.693199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5472245 -5.5449481 -5.546382 -4.9750948 -3.6868463 -2.8119688 -2.5019441 -2.3564296 -1.6738577 -2.3202538 -3.5761328 -4.1885624 -5.0180945 -5.1615562 -5.7431564][-4.9280405 -5.8384295 -6.530529 -6.4946327 -5.8893504 -4.4982872 -3.4301372 -3.2325377 -2.7222724 -3.4705439 -4.8069439 -5.5719633 -6.5208116 -6.3595181 -6.4874911][-5.2607374 -5.8868055 -6.3486438 -6.5267816 -6.1097021 -5.2682772 -4.0386448 -3.6408772 -3.5059872 -4.2095785 -5.5047045 -6.0634174 -7.1615229 -7.3416224 -7.4368739][-4.7926226 -5.7829876 -6.0319734 -5.7512107 -5.0810957 -4.3200192 -3.6037283 -3.328795 -2.78262 -3.5868459 -5.2322435 -6.0831203 -7.1911826 -7.402997 -8.1445017][-5.6309729 -5.9745865 -6.0555549 -5.19483 -3.7553208 -2.0946321 -1.0346222 -1.0319147 -1.4828525 -2.8568263 -4.2060051 -5.3732891 -7.119946 -7.6429372 -8.2701359][-6.9573603 -6.8374343 -6.1639767 -4.6040292 -2.5474629 -0.69557524 1.2286863 1.7500801 1.2207413 -0.37438965 -2.8954105 -4.3375473 -5.9669075 -6.7667747 -7.8809266][-6.7806993 -6.4191613 -5.3233986 -4.33682 -2.2166796 0.17861319 2.3996849 2.8453159 3.3400097 2.7610617 0.3109436 -2.00991 -4.5656853 -5.7447433 -6.7241178][-6.1735687 -5.8518891 -5.0091238 -3.1158338 -0.4716959 1.6237183 3.5022087 4.3639212 5.5644989 4.2662086 2.1156969 -0.052292824 -2.8162031 -4.55206 -6.0345154][-5.3437018 -4.4652815 -4.3086905 -3.1178393 -0.99340439 1.3335285 3.3877783 3.3633242 3.8220615 3.0696516 1.9563961 -0.135252 -2.31924 -3.6253858 -4.8330946][-5.90312 -5.3382516 -4.2626 -3.6260815 -2.5134721 -1.2937918 0.84980774 2.0090523 2.948678 1.1536694 -0.77773905 -1.1204119 -2.3852053 -3.4205031 -3.9710796][-5.0724235 -5.1983538 -5.1505013 -4.5143747 -3.4333477 -2.6309738 -1.4784145 -0.80074692 1.4550104 0.33537006 -1.6915116 -2.7260962 -4.3407221 -4.5743923 -4.5625658][-5.492065 -5.19658 -4.3743782 -3.5324545 -2.7769504 -1.7367039 -0.885447 -0.48070335 -0.37355423 -1.6912627 -2.8003507 -3.7430379 -5.5089154 -6.1421475 -5.9482656][-6.0816832 -5.3641558 -4.25252 -3.488956 -2.9290123 -2.5207686 -1.4027438 -0.41480207 -0.076966763 -1.7268705 -3.1764894 -3.5817795 -4.1934032 -5.0910959 -6.0687542][-6.1189847 -5.6629496 -4.5269842 -3.4935203 -2.8054638 -2.7774482 -2.6706991 -2.3189244 -1.4791379 -1.6115923 -3.0697713 -3.5281577 -4.290719 -4.3775039 -4.6383595][-6.1258068 -5.1263032 -4.7761583 -4.4874372 -3.7063684 -3.2354045 -2.7488608 -3.2836232 -3.8355892 -4.3205462 -3.9777734 -3.5082669 -4.2933769 -4.8507986 -5.2291565]]...]
INFO - root - 2017-12-16 00:10:12.080111: step 77410, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 44h:56m:35s remains)
INFO - root - 2017-12-16 00:10:18.456330: step 77420, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 44h:02m:17s remains)
INFO - root - 2017-12-16 00:10:24.890627: step 77430, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 45h:26m:20s remains)
INFO - root - 2017-12-16 00:10:31.244253: step 77440, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 44h:11m:25s remains)
INFO - root - 2017-12-16 00:10:37.636179: step 77450, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 46h:09m:45s remains)
INFO - root - 2017-12-16 00:10:44.178834: step 77460, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 46h:07m:08s remains)
INFO - root - 2017-12-16 00:10:50.476849: step 77470, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 45h:23m:03s remains)
INFO - root - 2017-12-16 00:10:56.910639: step 77480, loss = 0.36, batch loss = 0.24 (12.2 examples/sec; 0.656 sec/batch; 46h:27m:55s remains)
INFO - root - 2017-12-16 00:11:03.293185: step 77490, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.637 sec/batch; 45h:07m:56s remains)
INFO - root - 2017-12-16 00:11:09.773713: step 77500, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 44h:10m:17s remains)
2017-12-16 00:11:10.286089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2158804 -4.1072912 -4.4715023 -4.5510921 -4.4622993 -3.7508664 -3.2791753 -3.0719161 -3.3755136 -5.2678127 -6.0138197 -7.7114387 -8.4950523 -8.5753393 -9.7941074][-3.9520457 -4.0821333 -5.0222635 -4.8319817 -4.7275181 -4.2737956 -3.96937 -3.3482938 -3.8236089 -5.5237851 -6.9516253 -8.6998692 -9.5999594 -9.239872 -9.5468445][-3.3981662 -3.7539804 -3.4813728 -3.1685195 -3.0239491 -2.3376923 -2.6019793 -2.6124163 -2.6465292 -4.4127913 -5.8139811 -7.6677809 -7.7258091 -7.6779346 -8.5446463][-1.7242088 -2.5707994 -2.6716151 -1.7903152 -0.81370544 -0.069043636 0.4765892 0.25242949 -0.15125322 -3.0429659 -5.283514 -6.8317657 -6.7329493 -5.99666 -6.607728][-3.5507398 -2.5859923 -1.3560658 -0.77359629 -0.019263744 1.0139475 1.7982092 2.2890711 2.0271854 -0.76668453 -3.0408888 -5.22735 -5.9241643 -5.1967316 -5.8006186][-3.7566524 -2.8548851 -2.185256 -0.80027866 0.52782536 1.8888159 3.1141949 3.2227125 3.0339937 0.4427166 -1.8917704 -4.3856149 -5.3945284 -4.9823761 -5.6013875][-4.2225027 -3.0157452 -1.8432841 -1.0358906 0.79259682 2.3764076 4.195363 4.0884085 3.4379416 0.5724659 -1.454741 -4.1382117 -5.6468534 -5.7221355 -6.6990175][-3.642653 -2.6404161 -1.6820946 -0.21598816 1.3455906 2.610074 3.415801 3.8835287 3.8416996 0.86360264 -1.7351222 -4.5192809 -6.0594282 -6.5127878 -7.6857991][-4.7196922 -3.457149 -2.1918015 -0.22950554 1.5477304 2.3508291 2.6610928 2.8460999 3.0926228 0.76056862 -1.9775438 -4.7953653 -6.316998 -6.7566309 -7.981679][-4.6571188 -4.17432 -3.0790386 -1.2546248 0.36080456 1.0835934 1.4435167 1.6984758 1.8012638 -0.5763483 -2.5680704 -5.1294942 -7.0772386 -7.6720972 -8.7490721][-5.9924164 -5.1582036 -4.3214903 -3.1286798 -2.173306 -1.2191749 -0.8149786 -0.22436094 0.42501926 -2.2457986 -4.9541779 -6.340621 -7.035604 -7.6651559 -9.0080633][-6.8905315 -5.9109936 -5.24981 -4.2376747 -3.3974247 -2.751833 -2.7813869 -2.5465388 -2.1051989 -3.2904034 -4.3985629 -6.2295566 -7.1049523 -7.5649014 -8.5778618][-8.0861864 -7.4691858 -6.8455782 -6.0327063 -5.5537319 -4.7381611 -4.4627581 -4.7366538 -4.7643089 -5.6542635 -6.4093547 -6.2679987 -6.9882812 -7.3257747 -8.0896263][-8.0448875 -7.9539127 -7.2186322 -6.852066 -6.6349325 -6.1497555 -5.7706733 -5.538075 -5.5423961 -6.0391679 -6.5127859 -6.899312 -7.0128427 -6.9119086 -7.0671558][-9.1788559 -8.6884422 -8.1205072 -8.0045452 -7.9717216 -7.8261056 -7.2736993 -7.2520175 -7.1211958 -6.7583451 -7.0689321 -7.0411205 -7.0038686 -6.7796183 -6.5501423]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-77500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-77500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 00:11:17.897823: step 77510, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:29m:49s remains)
INFO - root - 2017-12-16 00:11:24.322542: step 77520, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 44h:46m:38s remains)
INFO - root - 2017-12-16 00:11:30.737345: step 77530, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:55m:36s remains)
INFO - root - 2017-12-16 00:11:37.071945: step 77540, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 44h:50m:54s remains)
INFO - root - 2017-12-16 00:11:43.424947: step 77550, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 44h:36m:12s remains)
INFO - root - 2017-12-16 00:11:49.842500: step 77560, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:27m:53s remains)
INFO - root - 2017-12-16 00:11:56.136872: step 77570, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:52m:21s remains)
INFO - root - 2017-12-16 00:12:02.586186: step 77580, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 45h:12m:39s remains)
INFO - root - 2017-12-16 00:12:08.997693: step 77590, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.620 sec/batch; 43h:52m:32s remains)
INFO - root - 2017-12-16 00:12:15.386086: step 77600, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 44h:01m:22s remains)
2017-12-16 00:12:15.967131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2571621 -3.3752155 -3.0889721 -2.9992685 -2.9321332 -2.4041824 -1.9731879 -2.3601217 -2.5656228 -4.3063431 -4.9495115 -6.7501 -7.9752169 -8.5402594 -9.2052288][-3.9101231 -3.8809609 -3.1407323 -1.9326377 -1.8354568 -1.7480359 -1.6066599 -1.5260544 -1.2187028 -3.4731503 -4.6764221 -5.7476258 -6.1784062 -7.2516694 -8.3393431][-3.9592566 -3.5329108 -3.5211658 -3.0001044 -3.0611186 -2.744349 -2.5017138 -2.4392381 -2.117455 -3.8822458 -4.445456 -6.0461688 -6.8383489 -7.23343 -7.3721356][-5.1460772 -4.6761942 -4.1046295 -2.8736506 -2.6362128 -2.8077617 -3.1182456 -3.1178622 -2.8232784 -4.4795661 -5.0060549 -6.1574488 -6.2610497 -6.8121037 -7.2990479][-5.6489325 -4.6306458 -3.8890326 -3.1098566 -2.9903965 -2.1786027 -1.9510202 -2.2437754 -2.5071239 -4.4257011 -5.15291 -6.3595161 -6.5041046 -6.6349831 -6.384057][-5.237597 -4.6948462 -4.1083293 -2.160984 -0.73341942 -0.35955572 -0.38588667 -0.79382563 -1.5015869 -3.7051158 -4.5431056 -5.7328491 -5.9195895 -6.4017348 -6.7455235][-3.3882303 -2.0197654 -1.1498446 -0.30795765 0.37771797 1.3443899 2.1147003 1.4547243 0.67791557 -1.8231359 -2.8030658 -4.0477896 -4.6660528 -5.4932189 -5.8556156][-1.3274655 -0.17559338 -0.29028368 1.8142414 3.5056839 3.5408535 3.3059521 2.5715399 2.1792984 -0.11019182 -1.4465976 -2.9014592 -3.3399277 -4.3155107 -5.2557139][-1.1233392 0.49275017 1.9062853 2.3212662 2.0790319 2.6796122 3.6192474 3.1068974 2.745347 0.73745251 -0.29585743 -2.0853972 -3.2150092 -4.4164882 -5.430335][-1.5962276 -0.38233137 -0.16371822 1.7405062 2.8089266 2.6526814 2.6572533 2.4227571 2.321681 0.1111269 -1.0261388 -2.1674209 -2.9161386 -3.9750416 -5.02358][-3.6878929 -2.7650185 -1.8963122 -0.85069895 -0.5675149 -0.00098657608 0.30928898 0.30245209 0.32781982 -1.2841592 -2.5795927 -3.7536337 -4.540431 -5.2261071 -5.8466563][-6.3309841 -5.3696184 -4.4700956 -2.8991089 -2.0184832 -1.6478863 -1.5526476 -1.6853824 -1.6155224 -2.8949008 -3.88252 -4.5082235 -5.4296422 -6.2867856 -7.0648279][-8.2639933 -7.3485937 -6.66472 -5.2350068 -3.8658125 -3.5053263 -3.347991 -3.3430095 -3.235395 -3.9907739 -4.7433844 -5.2819443 -5.6126881 -5.9167986 -6.3702335][-9.5623989 -8.6809769 -7.6686244 -6.8813038 -6.1535864 -5.2660313 -4.7214394 -4.7771716 -4.9695439 -5.4346943 -5.4030719 -5.9503956 -6.351131 -6.3070145 -6.1523046][-9.8069906 -8.5008783 -8.3431778 -7.8191524 -6.9925175 -6.7556028 -6.4885311 -6.20894 -5.9472003 -6.5442519 -6.4812522 -6.2310538 -5.9522538 -5.914485 -5.7593389]]...]
INFO - root - 2017-12-16 00:12:22.372300: step 77610, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 45h:40m:03s remains)
INFO - root - 2017-12-16 00:12:28.792664: step 77620, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:02m:28s remains)
INFO - root - 2017-12-16 00:12:35.231176: step 77630, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 46h:23m:33s remains)
INFO - root - 2017-12-16 00:12:41.696311: step 77640, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 45h:45m:58s remains)
INFO - root - 2017-12-16 00:12:48.070416: step 77650, loss = 0.32, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 46h:25m:34s remains)
INFO - root - 2017-12-16 00:12:54.422346: step 77660, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 45h:18m:20s remains)
INFO - root - 2017-12-16 00:13:00.830163: step 77670, loss = 0.50, batch loss = 0.39 (12.3 examples/sec; 0.648 sec/batch; 45h:51m:27s remains)
INFO - root - 2017-12-16 00:13:07.216173: step 77680, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 45h:51m:33s remains)
INFO - root - 2017-12-16 00:13:13.643843: step 77690, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.621 sec/batch; 43h:57m:51s remains)
INFO - root - 2017-12-16 00:13:20.046320: step 77700, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 44h:44m:38s remains)
2017-12-16 00:13:20.587780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5779605 -3.5767827 -3.2514377 -3.0271955 -2.6510744 -2.1138039 -1.4897256 -0.90053225 -0.52409267 -2.0656219 -3.3060441 -4.7079706 -6.1282206 -7.2066584 -8.1090288][-3.20158 -3.5276141 -3.6027737 -3.4351611 -3.0722127 -2.4869337 -1.9275508 -1.3798466 -0.97727346 -2.37959 -3.7548242 -5.1343451 -6.4872527 -7.2938108 -8.050477][-2.6550241 -2.3734775 -2.2575235 -1.9340363 -1.3820329 -0.92526388 -0.44443226 -0.064078808 0.097528458 -1.7317419 -3.5164571 -4.9111485 -6.3535414 -7.6443162 -8.4350262][-2.1204863 -2.0538993 -1.8563213 -1.2480164 -0.63880348 -0.029158115 0.4925127 0.75612259 0.75575352 -1.2845516 -3.0721912 -4.842793 -6.3788395 -7.3050075 -8.0660639][-2.3945427 -1.8621697 -1.0251589 -0.34610319 0.42326546 1.1547842 1.5717983 1.4213734 1.0756617 -1.1050839 -3.1152763 -4.7150354 -6.2306175 -7.240973 -7.8090339][-2.6215057 -1.9358802 -1.1454582 -0.084845066 0.86568642 1.9237881 2.3987799 2.1005659 1.6749353 -0.76278591 -2.970293 -4.8232269 -6.4195085 -7.3131161 -8.0148754][-3.1514916 -2.3473172 -1.2020359 0.1295805 1.3755484 2.5608253 2.9318962 2.7238274 2.2228556 -0.44072437 -2.8955626 -4.9087267 -6.4373555 -7.4343839 -8.2194967][-3.3201795 -2.5342245 -1.5540547 -0.1084547 1.3268471 2.4310102 2.6647387 2.5462313 2.1209297 -0.40271282 -2.7927761 -5.0026855 -6.5386553 -7.4552689 -8.0128574][-3.7846382 -2.7657013 -1.8502207 -0.7383337 0.75420761 1.8161945 2.0238886 1.86232 1.473443 -0.84976816 -3.2256417 -5.0930381 -6.564652 -7.4949913 -7.9956994][-3.9926181 -3.3397002 -2.5615849 -1.376018 -0.08080864 0.93849373 1.2023172 1.2798185 1.0832376 -1.5000153 -3.5436497 -5.3867726 -6.7383919 -7.562664 -8.2081871][-5.5087442 -4.9673691 -4.0494 -2.9224973 -1.8548975 -0.85201979 -0.61221552 -0.97102118 -1.2640214 -3.3236327 -5.0439539 -6.1514907 -6.9678888 -7.8002615 -8.022706][-6.2721848 -5.5678072 -4.5488048 -3.7375162 -2.8902774 -2.2020602 -2.1251082 -2.5655332 -2.9886756 -4.5864668 -5.6478848 -6.339788 -6.6776862 -7.3693786 -7.5642805][-6.345901 -5.8048687 -4.7950258 -3.9112306 -3.1679006 -3.0141797 -3.0712404 -3.439847 -4.106895 -5.1721005 -6.0299339 -6.2205081 -6.2290773 -6.4641056 -6.3772697][-6.3040147 -5.9630814 -5.1659079 -4.484828 -4.1040468 -4.0974369 -4.3076744 -4.5643115 -4.6997428 -5.4239473 -5.5382242 -5.6989684 -5.7128229 -5.9313011 -5.7953758][-7.4412165 -7.020462 -6.3790712 -5.8420553 -5.682704 -5.7880044 -6.1046252 -6.471776 -6.7122078 -6.4580078 -6.067914 -5.8171525 -5.6589074 -5.7227697 -5.7397346]]...]
INFO - root - 2017-12-16 00:13:27.028750: step 77710, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 45h:09m:50s remains)
INFO - root - 2017-12-16 00:13:33.488932: step 77720, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 46h:18m:53s remains)
INFO - root - 2017-12-16 00:13:39.967129: step 77730, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 46h:41m:11s remains)
INFO - root - 2017-12-16 00:13:46.352369: step 77740, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.624 sec/batch; 44h:07m:31s remains)
INFO - root - 2017-12-16 00:13:52.756843: step 77750, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 45h:00m:00s remains)
INFO - root - 2017-12-16 00:13:59.099719: step 77760, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.637 sec/batch; 45h:06m:33s remains)
INFO - root - 2017-12-16 00:14:05.489088: step 77770, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 46h:20m:30s remains)
INFO - root - 2017-12-16 00:14:11.957441: step 77780, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 44h:59m:08s remains)
INFO - root - 2017-12-16 00:14:18.383970: step 77790, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 46h:29m:06s remains)
INFO - root - 2017-12-16 00:14:24.759952: step 77800, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 44h:12m:59s remains)
2017-12-16 00:14:25.292274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0272365 -5.6324863 -5.83959 -5.6631894 -5.2434778 -4.3401632 -3.7270854 -3.3831162 -3.4772949 -5.1403236 -6.2122307 -6.9152193 -7.8790665 -8.3724289 -8.6211176][-4.4752684 -4.7676268 -4.826138 -5.0368404 -4.8951836 -4.1429749 -2.9831834 -2.264667 -2.7208104 -5.0343294 -6.6342096 -7.7651019 -9.128788 -9.6102247 -9.9905262][-4.7117176 -4.9514389 -5.1062436 -4.9070539 -4.0864506 -3.4070635 -2.7972293 -2.2385411 -2.2005606 -3.9564395 -5.6117306 -7.144762 -8.9495687 -10.069237 -10.569571][-4.1264997 -4.1594124 -4.51748 -4.8618512 -3.8861763 -2.5296502 -1.244278 -1.0178084 -1.0637441 -2.6396008 -4.2453518 -6.166008 -8.3529205 -9.6174755 -10.625031][-4.1295352 -3.8763413 -3.7039492 -3.1719451 -2.0634241 -0.82361889 0.42137432 0.79278755 0.45586014 -1.5155201 -3.1939669 -4.9076853 -6.9717646 -8.3355083 -9.5117817][-4.1483755 -3.0932031 -2.5567894 -1.9766045 -0.91091394 0.47899342 1.4684782 1.5897532 1.5036497 -0.36611223 -2.7153945 -4.3203554 -5.8564415 -6.7908077 -7.6541324][-4.0620623 -3.331883 -2.3211002 -0.8451376 0.78799248 2.0402384 2.9224091 3.0312157 3.1261654 1.1581106 -1.0663319 -3.1748424 -5.1200309 -5.9396415 -6.5854769][-3.2525411 -2.2923288 -1.6428885 -0.21958685 1.4716015 2.9440622 4.1773548 4.0621538 3.4740639 1.4425364 -0.60254049 -2.4280615 -4.5196791 -5.605196 -6.0913124][-3.7465789 -2.7716579 -2.2943721 -0.970428 0.3287468 1.5823803 3.0864248 3.5348396 3.1460371 0.84817123 -1.0832233 -2.4080939 -4.2824354 -5.2948518 -6.1036816][-4.3535223 -3.3088369 -2.81914 -2.3013797 -1.5175643 0.011843681 1.627038 2.4054499 2.8215666 0.67594433 -1.4020319 -2.7768497 -4.3021545 -5.324728 -6.1375518][-5.2121816 -4.970026 -4.374228 -3.7334049 -3.0086141 -2.1032715 -0.94981813 -0.32581997 -0.25756121 -1.2723742 -2.4174366 -3.6905084 -4.966177 -5.6215906 -6.0692811][-5.9021134 -5.6314626 -5.6230764 -5.166954 -4.4130459 -3.7064118 -2.604198 -2.0680518 -1.7332253 -2.9305186 -3.799541 -4.4532537 -5.4593678 -6.1373014 -6.6446109][-6.8884125 -6.0065536 -5.3364773 -5.03079 -5.0944729 -4.6834431 -3.9948952 -3.7462602 -3.3176007 -3.5379009 -3.8733804 -4.4829025 -4.8030753 -5.7065544 -6.13629][-7.0092812 -7.2591372 -6.8577905 -6.00319 -5.3321919 -4.9875884 -4.7657928 -4.89859 -5.0861468 -4.7639565 -5.1579123 -5.1457777 -4.7069979 -4.9544353 -4.9799604][-6.1325727 -6.0472612 -6.3964539 -5.9792471 -5.237936 -4.6709127 -4.6056614 -4.8054667 -4.8399153 -4.6252337 -4.7926741 -5.0303516 -4.7967052 -4.7465148 -4.799479]]...]
INFO - root - 2017-12-16 00:14:31.709373: step 77810, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 44h:35m:30s remains)
INFO - root - 2017-12-16 00:14:38.143590: step 77820, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 44h:45m:09s remains)
INFO - root - 2017-12-16 00:14:44.569598: step 77830, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 44h:51m:37s remains)
INFO - root - 2017-12-16 00:14:50.997953: step 77840, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 45h:55m:46s remains)
INFO - root - 2017-12-16 00:14:57.377766: step 77850, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:52m:18s remains)
INFO - root - 2017-12-16 00:15:03.792875: step 77860, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 44h:54m:19s remains)
INFO - root - 2017-12-16 00:15:10.221953: step 77870, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 46h:36m:39s remains)
INFO - root - 2017-12-16 00:15:16.620178: step 77880, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 46h:15m:29s remains)
INFO - root - 2017-12-16 00:15:23.062454: step 77890, loss = 0.33, batch loss = 0.22 (12.1 examples/sec; 0.663 sec/batch; 46h:51m:21s remains)
INFO - root - 2017-12-16 00:15:29.474699: step 77900, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 45h:23m:56s remains)
2017-12-16 00:15:30.068375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5532832 -4.3466816 -4.1598697 -4.0173817 -4.2413836 -4.782218 -5.0301733 -5.3465729 -5.3439026 -6.4455223 -7.2618327 -8.6636314 -9.1994028 -9.4531078 -9.9636307][-4.1633177 -4.1699777 -4.433095 -4.6790924 -5.0048208 -5.3801508 -5.809844 -6.0423422 -6.1171975 -7.2643075 -7.6995521 -8.3212814 -8.7501793 -9.6011562 -10.099937][-3.5937815 -3.3152881 -3.4395967 -3.8653214 -4.3625426 -4.7986817 -4.9170742 -5.266324 -5.4950981 -6.663167 -6.9514275 -7.5514169 -7.8990197 -8.5608177 -8.87785][-4.7979832 -4.253974 -4.0532541 -4.1304483 -4.1165333 -3.922585 -3.5123181 -3.0712848 -2.6024647 -3.9469688 -4.8893404 -5.7355762 -6.5733843 -7.5219016 -8.0596771][-4.7476149 -4.1886864 -4.0432639 -3.4829307 -2.8223634 -2.2850866 -1.5036411 -0.73344135 -0.041163921 -1.2996287 -2.649868 -4.632473 -6.0084023 -7.0285959 -7.7612467][-4.9384818 -3.9689991 -2.964458 -2.0414743 -1.3325391 -0.19617271 0.6683836 1.1552534 1.5911598 0.19652367 -1.3737674 -3.2480655 -4.910223 -6.6018414 -7.5360856][-4.4694767 -3.9110665 -2.8610306 -1.4571586 0.059887409 1.3374605 2.1273155 2.6057549 3.2750111 1.6468515 -0.05867815 -2.2623978 -4.3348956 -6.2164345 -7.6753106][-3.3633595 -3.12579 -2.5284243 -1.0992761 0.30277443 1.7059669 2.8420324 3.4344912 3.8127918 2.0938549 0.61966515 -1.8221073 -3.9675264 -5.9748621 -7.4690824][-2.6546268 -2.1201138 -0.98957682 -0.29880953 0.021984577 1.2589741 2.0305948 2.0421886 2.3459558 0.90397167 -0.44912815 -2.6615939 -4.5861034 -6.4150047 -7.7578793][-2.5918741 -2.0110369 -0.97411728 -0.40535021 -0.0518651 0.32047081 0.43677139 0.64923191 0.8578434 -1.1555548 -2.4288507 -4.2910833 -5.538588 -7.1319637 -8.1693172][-4.3035088 -4.0126729 -3.3169475 -2.2312489 -1.5012403 -1.3927355 -1.3452539 -1.3611865 -1.4832449 -3.4498587 -4.6385064 -6.1703968 -6.7244344 -7.5455837 -8.0045147][-5.9120188 -5.3849344 -4.6580658 -3.6055231 -3.027874 -2.7525115 -2.7087755 -3.0708327 -3.2701249 -4.6425734 -5.6406269 -6.55383 -6.6844988 -7.3390326 -7.5560875][-6.3424807 -5.4140325 -4.5688343 -3.9718168 -3.5059924 -3.5125809 -3.743228 -3.9306366 -4.1764832 -5.233314 -5.9671907 -6.4348121 -6.6407967 -6.6051702 -6.4014988][-6.7958865 -6.1088457 -5.5352712 -4.8931623 -4.3716626 -4.1847897 -4.1021061 -4.3671455 -4.7514172 -5.5581808 -6.0823526 -6.2003407 -6.0394711 -6.2078338 -6.1677747][-7.7098246 -7.0118771 -6.3700361 -5.8223104 -5.5672626 -5.4192924 -5.3231745 -5.4484816 -5.5984554 -5.7627196 -6.0733476 -6.1103592 -6.1939383 -5.8984747 -5.4907503]]...]
INFO - root - 2017-12-16 00:15:36.463494: step 77910, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 46h:15m:49s remains)
INFO - root - 2017-12-16 00:15:42.809813: step 77920, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.633 sec/batch; 44h:47m:32s remains)
INFO - root - 2017-12-16 00:15:49.237558: step 77930, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 45h:25m:17s remains)
INFO - root - 2017-12-16 00:15:55.755351: step 77940, loss = 0.31, batch loss = 0.20 (11.5 examples/sec; 0.695 sec/batch; 49h:07m:29s remains)
INFO - root - 2017-12-16 00:16:02.228179: step 77950, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 46h:25m:39s remains)
INFO - root - 2017-12-16 00:16:08.690521: step 77960, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 45h:07m:27s remains)
INFO - root - 2017-12-16 00:16:15.106487: step 77970, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 47h:02m:47s remains)
INFO - root - 2017-12-16 00:16:21.481816: step 77980, loss = 0.28, batch loss = 0.17 (13.1 examples/sec; 0.613 sec/batch; 43h:18m:21s remains)
INFO - root - 2017-12-16 00:16:27.902802: step 77990, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 47h:01m:59s remains)
INFO - root - 2017-12-16 00:16:34.260229: step 78000, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 44h:13m:41s remains)
2017-12-16 00:16:34.772374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9712477 -4.4812117 -4.9096022 -5.1926193 -5.1796942 -5.2879777 -5.2218676 -4.6117096 -4.3140693 -3.8611808 -4.8892574 -6.4201031 -8.0103607 -8.7469769 -9.8409843][-4.1506009 -3.9726512 -3.8909452 -4.4572029 -5.7226672 -6.6213264 -6.7075939 -6.2358017 -5.5222874 -4.8400774 -5.7425642 -7.1939707 -8.68451 -9.4252605 -10.569833][-4.3682251 -3.9778605 -3.7151322 -3.8000879 -4.3459139 -5.0538177 -5.4638944 -5.3358068 -4.9472408 -4.249012 -4.9172411 -6.4189463 -7.1265969 -7.4362154 -8.5423031][-4.0065408 -3.3030181 -2.8471131 -2.7054367 -2.6589265 -2.924736 -2.9723115 -2.98635 -2.995872 -2.4377313 -3.2380137 -4.882513 -5.8377113 -5.9743714 -6.8059573][-3.6491327 -2.635921 -1.8917069 -1.6441855 -1.4738579 -0.94829988 -0.32343292 -0.19052267 -0.33207226 -0.17412376 -1.3982172 -3.2368078 -4.2507691 -4.7335749 -5.7542667][-3.1767783 -2.6396837 -2.189034 -1.6513767 -1.0541563 -0.19479275 0.94066143 1.2728252 1.3578663 1.2249756 -0.27549267 -2.4048829 -3.6333132 -4.1570711 -5.4412508][-3.4556952 -2.8623614 -2.2339482 -1.4481072 -0.12701273 0.86026287 1.8983603 1.8314133 1.9094477 1.401536 -0.74686813 -3.28475 -4.5592117 -5.0517426 -6.2152662][-3.5736656 -3.0521121 -2.0023546 -0.87327003 0.52072334 1.6110106 2.7517071 2.6340361 2.4471169 1.4731865 -0.75262213 -3.6091127 -5.3984022 -5.8038845 -6.3604989][-4.3604641 -3.5574079 -2.2981114 -0.98847485 0.14262533 0.95179749 1.9211674 1.7538357 1.4314489 0.7495966 -0.96338463 -3.6365457 -5.5109329 -5.791029 -6.3360443][-3.5303826 -3.5321522 -3.0520773 -2.104497 -1.3428359 -0.45765114 0.29681635 -0.12867785 -0.72966671 -1.5054183 -3.1719213 -4.9275827 -6.2645345 -6.5907826 -7.4360986][-4.8008986 -4.426312 -4.0259476 -3.6386557 -3.2565556 -3.1091218 -2.8148818 -2.9773827 -3.3694024 -4.3816652 -6.2267828 -6.8574243 -6.8332877 -7.051528 -7.9919958][-5.9220328 -5.5346088 -5.0918474 -4.2296824 -4.1654549 -4.648015 -5.0294118 -5.2911329 -5.4160318 -6.4353194 -7.7081971 -7.6223388 -7.452311 -7.8970795 -8.23067][-7.0977311 -6.9667778 -6.7170334 -5.58259 -4.4552937 -4.2314177 -4.5803566 -4.9678125 -5.34601 -5.8555679 -7.0134287 -6.6421051 -6.3457146 -6.9750562 -7.4451756][-6.7590547 -6.5000319 -6.206953 -5.2991819 -4.2638922 -3.5117841 -2.8497109 -3.2686691 -3.6981928 -3.9205654 -4.7124615 -5.0949507 -5.0698833 -5.1928563 -5.4932051][-4.6530752 -4.7148347 -5.2354832 -5.0613794 -4.6850996 -4.1113482 -3.7443328 -3.8219507 -3.3374963 -3.5579224 -3.6575017 -3.6584868 -4.2690964 -5.1409636 -5.4570556]]...]
INFO - root - 2017-12-16 00:16:41.205980: step 78010, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 45h:24m:57s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:16:47.671222: step 78020, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 44h:26m:45s remains)
INFO - root - 2017-12-16 00:16:54.084631: step 78030, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 45h:37m:27s remains)
INFO - root - 2017-12-16 00:17:00.505793: step 78040, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:38m:24s remains)
INFO - root - 2017-12-16 00:17:06.873481: step 78050, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 44h:13m:42s remains)
INFO - root - 2017-12-16 00:17:13.293509: step 78060, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 44h:37m:47s remains)
INFO - root - 2017-12-16 00:17:19.693774: step 78070, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 45h:04m:32s remains)
INFO - root - 2017-12-16 00:17:26.104117: step 78080, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:59m:48s remains)
INFO - root - 2017-12-16 00:17:32.480071: step 78090, loss = 0.24, batch loss = 0.12 (12.9 examples/sec; 0.618 sec/batch; 43h:40m:51s remains)
INFO - root - 2017-12-16 00:17:38.929923: step 78100, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 45h:29m:09s remains)
2017-12-16 00:17:39.451500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8927479 -2.8152957 -3.2443485 -3.0667496 -3.0664887 -3.4143023 -3.6392231 -3.5709152 -3.5119314 -3.9973321 -4.662467 -5.4345407 -6.3878155 -7.4009466 -8.2834463][-2.2505059 -2.3841152 -2.9341569 -2.9805894 -3.7034185 -4.1222448 -4.33249 -4.5520077 -4.4405394 -4.3500013 -4.7001553 -5.426116 -5.9925857 -6.7402039 -7.3980503][-1.2000008 -0.6940918 -1.0288663 -1.2408857 -1.5727935 -2.0772443 -2.9705744 -3.3219385 -3.3569069 -3.5758958 -3.866271 -4.7221756 -5.2786484 -5.9240403 -6.4699764][-0.34807205 0.38936043 0.21715307 -0.10149717 -0.41692924 -0.82713461 -1.2111464 -1.6897268 -2.219471 -2.6867428 -3.1151385 -4.0895443 -4.6615334 -5.1808777 -5.9439936][-0.41671467 0.23645353 0.43106747 0.43007469 0.22271013 0.41946125 0.78258133 0.194808 -0.27270889 -0.9203229 -1.60256 -2.4123826 -3.318161 -4.2273026 -5.1423397][-1.2105408 -0.80990505 -0.34736347 0.081100941 0.4442997 1.4549437 2.386034 2.1155233 2.0173473 1.2350407 0.29788589 -0.99560165 -2.3084412 -3.5407929 -4.7300396][-1.9845343 -1.5157619 -1.1713681 -0.67433119 0.098851681 1.1139231 2.3073025 2.7483921 3.1709976 2.1761494 0.99377346 -0.63217449 -2.1478739 -3.4898596 -4.5433617][-2.8965402 -2.3024378 -1.7059374 -1.1158581 -0.32016039 0.93263435 2.4363937 3.2299175 3.9207335 2.9602528 1.7974949 0.058164597 -1.7067213 -3.3016992 -4.5503316][-3.8613529 -3.0182738 -2.279294 -1.8010187 -1.211195 0.16077805 1.6989937 2.7339163 3.627964 2.7193022 1.2359657 -0.52879667 -2.0571046 -3.7964063 -5.0160418][-4.6484962 -4.1598539 -3.3933077 -2.69591 -2.0879521 -0.84813118 0.041611671 0.56324005 1.4205465 0.94389915 -0.29704189 -1.9043174 -3.4835429 -4.7252178 -5.779376][-5.7047496 -5.5295506 -5.2598209 -4.4151659 -3.7536 -2.7594533 -2.0803461 -1.6154046 -1.2940898 -1.8082147 -2.5894718 -3.77137 -4.8402624 -6.03945 -7.0966916][-7.4431772 -7.2810826 -7.0613966 -6.4794388 -5.972465 -5.35864 -4.7909651 -4.5250988 -4.3839121 -4.6214428 -5.1861625 -5.5769176 -6.3074169 -7.1438012 -7.8228011][-7.8245077 -7.6793513 -7.557775 -7.330863 -6.9582019 -6.2375422 -5.7658033 -5.76415 -5.6828675 -5.5789604 -5.7504134 -6.1778841 -6.5606217 -7.0167007 -7.4358335][-7.2072039 -7.2897043 -7.5268316 -7.4758897 -7.2969112 -6.6034756 -5.9506187 -5.8512864 -5.7719426 -5.7032709 -6.2375078 -6.342526 -6.3826685 -6.492919 -6.9521503][-7.0520072 -7.044198 -7.1290374 -7.2300887 -7.11372 -6.3414345 -5.7000413 -5.6330814 -5.5701122 -5.6239328 -5.7936425 -5.7638946 -5.6883936 -6.027832 -6.1517625]]...]
INFO - root - 2017-12-16 00:17:45.923758: step 78110, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 44h:57m:05s remains)
INFO - root - 2017-12-16 00:17:52.294130: step 78120, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 46h:05m:33s remains)
INFO - root - 2017-12-16 00:17:58.715097: step 78130, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 43h:51m:14s remains)
INFO - root - 2017-12-16 00:18:05.127575: step 78140, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 46h:20m:08s remains)
INFO - root - 2017-12-16 00:18:11.561358: step 78150, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 44h:14m:33s remains)
INFO - root - 2017-12-16 00:18:17.917869: step 78160, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 45h:22m:40s remains)
INFO - root - 2017-12-16 00:18:24.229455: step 78170, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 45h:11m:21s remains)
INFO - root - 2017-12-16 00:18:30.672158: step 78180, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 45h:44m:17s remains)
INFO - root - 2017-12-16 00:18:37.097705: step 78190, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 45h:24m:49s remains)
INFO - root - 2017-12-16 00:18:43.480965: step 78200, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.625 sec/batch; 44h:07m:29s remains)
2017-12-16 00:18:44.055284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.486165 -3.9210372 -4.5251865 -4.9873257 -4.8311672 -4.6020727 -4.5789795 -4.6011238 -4.981452 -6.3659487 -7.544488 -9.0562181 -9.3525906 -9.768981 -10.418483][-3.3903136 -3.8446171 -4.6522603 -5.1387968 -5.3485694 -5.042738 -5.1433372 -5.5146008 -5.7096486 -7.1608791 -8.6303482 -10.4051 -10.835946 -11.087797 -11.582328][-3.6967752 -3.9832683 -4.1904178 -3.8963337 -3.6881838 -3.5598574 -3.8772705 -4.561533 -4.8809681 -6.3714857 -7.3778014 -9.2258186 -9.82206 -10.593237 -11.359776][-3.3376474 -3.2291121 -3.0321798 -2.4528894 -2.1504769 -1.5943694 -1.4632578 -1.5040793 -1.9794641 -3.6363645 -5.1169815 -7.208807 -8.397151 -9.111517 -9.9805536][-5.137742 -4.1988087 -3.2461524 -2.0514903 -1.350636 -0.61624575 -0.044839859 -0.66001463 -1.265861 -2.5114326 -3.6182046 -5.4401512 -6.3548069 -7.3258452 -8.3602886][-5.5051055 -4.2530823 -2.9828191 -1.5817919 -0.62084866 0.089283466 0.5451479 0.091752529 -0.074227333 -1.0234365 -2.0512342 -3.649735 -4.3812819 -5.0081921 -6.0033569][-5.70457 -4.2427573 -2.9790416 -1.4883523 -0.25505209 0.73719692 1.2087278 1.0512648 1.0749311 -0.14812374 -1.5636687 -3.2101912 -4.1405115 -4.8004923 -5.5174894][-5.5894938 -4.6006274 -3.390141 -2.0490012 -0.553288 0.59043312 1.5533457 1.5675325 1.494628 0.024513245 -1.4978986 -3.3999023 -4.2525253 -4.8570833 -5.613349][-6.3011723 -5.3111343 -4.2562723 -2.7160707 -1.2675047 -0.057738781 0.790288 0.78596592 1.1526356 -0.50512838 -2.5810251 -4.7078228 -5.57821 -5.9762335 -6.3564277][-6.2138891 -5.8307438 -5.2796173 -4.0816717 -3.0857701 -1.773036 -0.80783367 -0.2695117 0.24749136 -1.436255 -3.0789022 -5.465353 -6.5778284 -7.0688653 -7.6372886][-7.3022451 -7.1385541 -6.8124409 -5.726172 -4.8276572 -3.803905 -2.7645812 -2.3001413 -2.0278955 -3.5194016 -5.1202621 -6.9780188 -7.7166796 -8.0137043 -8.53752][-6.5194488 -6.2362309 -5.9251246 -5.2751102 -4.8725443 -4.5158281 -3.9228206 -3.5122452 -3.0535936 -4.2331376 -5.3132067 -7.133152 -8.001647 -8.2903728 -8.61519][-6.7111273 -6.2569203 -5.7814608 -5.1516771 -4.8846645 -4.8280945 -4.5911551 -4.7429543 -4.8922882 -5.78825 -6.4069729 -7.3665047 -7.9516306 -7.9936829 -8.0003738][-6.2371359 -5.8025894 -5.6603637 -5.1174603 -4.8057756 -4.7548351 -4.8128691 -5.11718 -5.3989258 -6.1473632 -6.6011419 -7.1104856 -7.1443386 -7.1833696 -6.89027][-7.5569654 -7.111989 -6.7340231 -6.0412774 -5.5875192 -5.6343737 -5.9343538 -6.4739027 -6.8107314 -7.1216211 -7.5435357 -7.3941779 -7.153749 -6.8898659 -6.5585237]]...]
INFO - root - 2017-12-16 00:18:50.541251: step 78210, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 45h:24m:31s remains)
INFO - root - 2017-12-16 00:18:56.951377: step 78220, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 44h:35m:45s remains)
INFO - root - 2017-12-16 00:19:03.264858: step 78230, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.617 sec/batch; 43h:36m:05s remains)
INFO - root - 2017-12-16 00:19:09.600529: step 78240, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 44h:46m:47s remains)
INFO - root - 2017-12-16 00:19:15.941322: step 78250, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.629 sec/batch; 44h:23m:46s remains)
INFO - root - 2017-12-16 00:19:22.406688: step 78260, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 44h:05m:02s remains)
INFO - root - 2017-12-16 00:19:28.741349: step 78270, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 46h:04m:22s remains)
INFO - root - 2017-12-16 00:19:35.005771: step 78280, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 44h:03m:00s remains)
INFO - root - 2017-12-16 00:19:41.437868: step 78290, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.637 sec/batch; 45h:00m:48s remains)
INFO - root - 2017-12-16 00:19:47.826858: step 78300, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 45h:13m:00s remains)
2017-12-16 00:19:48.352922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7284346 -4.6365175 -4.674943 -5.2408624 -5.7817774 -6.0954118 -6.5085387 -6.9021239 -7.0206161 -7.8851213 -7.6073642 -8.4847574 -8.5953283 -8.8859711 -8.4153662][-6.8679771 -6.9334836 -6.8534675 -7.0289717 -7.4020114 -7.6336989 -7.82147 -7.9162707 -8.0375814 -9.08804 -9.2505455 -10.187578 -10.29389 -10.560199 -10.104147][-6.2037172 -6.5509515 -7.2122316 -7.300951 -6.9520774 -6.5533142 -6.5801325 -6.6004477 -6.4516859 -7.5164618 -7.5596828 -8.5891933 -8.8809538 -9.4740543 -8.90662][-5.8105135 -5.6492205 -5.8284068 -5.7759428 -5.7008042 -5.5153913 -5.2858534 -5.09144 -4.9483614 -6.0972571 -5.8743782 -6.397068 -6.9854631 -7.7208662 -7.8470774][-6.022716 -5.1746421 -4.6467772 -4.16282 -3.8142204 -3.3209171 -3.2069688 -3.2024193 -3.1847239 -4.5229206 -4.419817 -5.5399818 -6.3608713 -7.328249 -7.195158][-5.0965481 -4.6195545 -4.2561812 -3.2514925 -2.425828 -1.5984497 -1.0020423 -0.7176075 -0.59119177 -2.3527651 -2.6511459 -3.8287179 -4.9010315 -6.0768356 -6.2406311][-4.4388323 -3.9269216 -3.6533241 -2.5105777 -1.3298249 0.065593719 0.89153004 1.0983 1.2910538 -0.66966105 -1.3342195 -2.6813583 -3.729785 -4.9943438 -5.4974465][-4.8276262 -4.0803528 -3.3254204 -1.7550654 -0.36977482 1.0181551 1.634655 2.2031517 2.3171244 0.31376457 -0.33650398 -2.1772532 -3.6413283 -5.0959063 -5.5268][-5.1033654 -4.4099979 -3.7691739 -2.4592853 -1.2749815 0.074346542 0.82068825 1.2118216 1.1054754 -0.89282417 -1.5714006 -2.9924579 -4.1518245 -5.5581379 -5.7014842][-4.8327231 -4.2182169 -3.8036027 -2.8677416 -2.2275033 -1.2101541 -0.50199127 -0.2659235 -0.59471226 -2.7057009 -3.6143341 -4.762651 -5.4098978 -6.2080874 -6.2176266][-6.1619515 -5.9739022 -5.4323263 -4.5179892 -3.764436 -2.9130683 -2.3954782 -2.1270409 -2.0585418 -3.7838531 -4.9049845 -5.9932337 -6.7503672 -7.3586731 -6.9459805][-7.0690026 -7.0652986 -7.1246748 -6.5913224 -6.2037277 -5.30287 -4.5571008 -3.9468684 -3.5958743 -4.9078417 -5.509078 -6.2106738 -7.0527916 -7.4929833 -7.5458274][-7.5395036 -7.6746221 -7.6796865 -7.5129495 -7.1598496 -6.5575886 -6.0040479 -5.7512007 -5.6274071 -6.2418618 -6.6625223 -6.9630976 -7.0319562 -6.8067832 -6.2823439][-6.8149762 -6.7391267 -6.6163363 -6.5901313 -6.43025 -5.9678812 -5.6972327 -5.6094427 -5.6979914 -6.3822408 -6.69966 -6.7900934 -6.6582437 -6.4133878 -6.3237557][-8.9980431 -8.4519482 -7.9364781 -7.4863806 -7.2828417 -6.9477639 -6.803236 -6.8488846 -6.9420991 -7.0344019 -7.3020186 -7.3728333 -7.1894469 -6.7545114 -6.297667]]...]
INFO - root - 2017-12-16 00:19:54.751323: step 78310, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 43h:35m:57s remains)
INFO - root - 2017-12-16 00:20:01.144952: step 78320, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 45h:30m:59s remains)
INFO - root - 2017-12-16 00:20:07.582424: step 78330, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 46h:11m:03s remains)
INFO - root - 2017-12-16 00:20:14.061695: step 78340, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 45h:20m:58s remains)
INFO - root - 2017-12-16 00:20:20.516327: step 78350, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 46h:57m:16s remains)
INFO - root - 2017-12-16 00:20:26.887941: step 78360, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 44h:23m:35s remains)
INFO - root - 2017-12-16 00:20:33.240223: step 78370, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 45h:30m:28s remains)
INFO - root - 2017-12-16 00:20:39.607960: step 78380, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 44h:42m:20s remains)
INFO - root - 2017-12-16 00:20:45.948689: step 78390, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 44h:49m:36s remains)
INFO - root - 2017-12-16 00:20:52.321714: step 78400, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 44h:20m:07s remains)
2017-12-16 00:20:52.850936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.454071 -5.4807048 -5.8423023 -5.9032431 -5.4007297 -5.6216207 -5.5578618 -4.8507857 -4.1058192 -4.6258259 -4.3916607 -5.7155218 -6.9376736 -7.6774273 -8.0917559][-5.4115467 -5.6287689 -6.1329174 -6.4064236 -6.593255 -6.3850679 -6.0990996 -5.3112669 -4.1918836 -4.5223384 -4.4937038 -5.6501727 -6.9993806 -7.2505574 -7.8415346][-4.8122597 -4.9588251 -5.1298122 -5.5351248 -5.3612838 -4.7654762 -4.4082251 -3.5091114 -2.5320663 -3.1064382 -3.2009912 -4.7292385 -6.2552371 -6.3636961 -6.8063674][-3.7787988 -3.660192 -3.7938097 -3.881618 -3.6824069 -2.9432859 -2.3485761 -1.8307266 -1.519289 -2.3780417 -2.6691017 -4.4162788 -5.6458788 -5.860075 -6.233573][-3.3907952 -2.4240026 -1.8942361 -1.7885637 -1.9111104 -1.0830646 0.057287216 0.23282862 0.24764633 -1.1031485 -2.0824833 -4.219995 -5.6263213 -6.3667169 -6.4869184][-2.3587809 -1.1803622 -0.45480013 0.1583848 0.63449764 1.4671316 2.5764561 3.1049652 3.2958755 1.094759 -0.55402422 -2.9931283 -5.0383472 -5.72396 -6.1520109][-1.7316718 -0.899487 -0.017745972 1.1400576 2.2827415 2.9157181 3.7998018 3.8769579 3.7562771 1.3385735 -0.101861 -2.9691873 -5.1012597 -6.0924945 -6.5418372][-0.7691741 -0.59246492 -0.272408 1.0520334 2.0871534 3.2114964 4.2007303 4.1875248 3.8760376 1.3890772 0.019629955 -2.6686425 -4.524971 -6.1892257 -7.0082617][-1.9491873 -1.6236329 -1.3457189 -0.36891937 0.990962 2.1216183 2.7899523 3.1790142 3.428937 1.2008276 -0.61151934 -3.2032418 -5.1754756 -6.2460232 -6.4260798][-3.3235073 -2.7768507 -2.3925042 -1.3065095 -0.55338192 0.10202646 0.68585682 0.83847618 1.2102385 -1.0467105 -2.2668624 -4.2450619 -5.9765692 -6.9703574 -7.4687996][-5.92154 -5.3178568 -4.7265091 -4.0017309 -3.3594112 -2.6825809 -2.2909517 -2.244657 -2.3137054 -3.8373253 -4.6957035 -5.8584723 -6.1398458 -7.2229385 -8.2097912][-7.5672541 -7.3037181 -7.2829413 -6.8634167 -6.3561921 -5.4659281 -4.9645386 -4.7546778 -4.3405786 -5.1492972 -5.4156656 -5.9481111 -6.5915508 -7.12842 -7.6809235][-8.4754791 -7.989675 -7.7179656 -7.4687767 -7.1066723 -6.4794941 -5.8121433 -5.8623991 -5.9251351 -6.3766971 -6.6350927 -6.8016911 -7.0763211 -7.2045341 -7.2729692][-7.8411565 -7.3734403 -6.9142 -6.5381818 -6.241559 -5.7337537 -5.1888857 -5.2687721 -5.5892682 -6.2744145 -6.8327842 -7.0452766 -7.1798058 -6.984838 -6.89026][-8.604866 -8.025074 -7.8963957 -7.7373462 -7.550148 -7.1266909 -6.5413294 -6.5437894 -6.6359921 -6.8292351 -7.3268852 -7.6189647 -7.681365 -7.707181 -7.5034704]]...]
INFO - root - 2017-12-16 00:20:59.266745: step 78410, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 45h:21m:28s remains)
INFO - root - 2017-12-16 00:21:05.636509: step 78420, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 46h:57m:57s remains)
INFO - root - 2017-12-16 00:21:12.017448: step 78430, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 44h:38m:56s remains)
INFO - root - 2017-12-16 00:21:18.355843: step 78440, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 44h:33m:14s remains)
INFO - root - 2017-12-16 00:21:24.662934: step 78450, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 44h:52m:41s remains)
INFO - root - 2017-12-16 00:21:31.023344: step 78460, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 44h:21m:01s remains)
INFO - root - 2017-12-16 00:21:37.446555: step 78470, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 44h:15m:41s remains)
INFO - root - 2017-12-16 00:21:43.786032: step 78480, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 43h:49m:44s remains)
INFO - root - 2017-12-16 00:21:50.100095: step 78490, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 44h:54m:43s remains)
INFO - root - 2017-12-16 00:21:56.473719: step 78500, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 44h:10m:00s remains)
2017-12-16 00:21:57.025682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1886086 -5.3157053 -7.6092582 -8.6462994 -9.0920305 -7.3050413 -6.1544905 -4.6522303 -4.58286 -5.4403405 -5.9273043 -6.085453 -5.1199808 -7.3325253 -7.0843568][-5.9829326 -6.5820522 -6.5002437 -7.57067 -8.960248 -7.970736 -7.8358912 -6.2284379 -5.9863262 -6.9114947 -5.8214197 -7.2511959 -7.1992631 -7.4683619 -6.9709349][-4.7091279 -5.9335995 -6.4867253 -6.6340084 -5.3298459 -5.1084867 -5.0439234 -5.2354746 -5.532299 -5.8666668 -5.4129543 -6.7419257 -6.8289471 -7.1182942 -7.3335395][-2.3943896 -3.9800379 -5.1766338 -5.380373 -4.2112021 -2.8536639 -2.3183393 -2.0827527 -3.1153126 -4.812366 -4.7196226 -5.469676 -4.9008169 -5.9281096 -5.4137087][-2.2761688 -2.4519553 -3.1436152 -2.906631 -2.6080461 -1.1717186 -0.28011322 -0.43335152 -0.89636326 -2.8029757 -2.9212599 -4.7797241 -4.37973 -4.3581314 -4.7718749][-2.5888896 -3.0158963 -3.5737081 -2.2259521 -0.071456909 1.4448061 2.2321815 1.9436302 1.22546 -0.43256283 -0.40812826 -2.814909 -2.5734911 -2.9853897 -3.0117207][-3.1062007 -3.2121797 -2.8795533 -2.100502 -0.53221893 1.8646851 2.549984 2.6872711 2.2151556 0.68338203 0.16212034 -1.5892448 -1.963491 -4.4001665 -4.6759996][-4.2994223 -3.8283739 -2.9796991 -1.8585567 -0.36120224 1.7515783 2.3355665 2.6719503 2.2843657 0.65731716 -0.12028503 -1.8548684 -2.5967379 -4.487833 -4.841279][-5.0944343 -5.0254459 -4.127768 -2.5700974 -0.55373812 0.66499329 2.1124859 1.5422392 2.0888062 0.66006947 -1.0133047 -3.4946308 -4.365428 -5.6994724 -5.4767008][-5.2421379 -4.2566833 -3.9155347 -2.9423151 -1.5717702 -0.66446066 -0.053209305 0.82913589 0.50112152 -0.98357916 -2.5960956 -5.2016621 -5.9589915 -6.7518454 -6.8150048][-7.3489943 -6.417201 -5.0351019 -3.72469 -1.8749237 -0.86315155 -0.48572159 -1.6128621 -1.5910859 -3.256721 -3.9708903 -5.0357113 -7.1926947 -7.7167807 -7.233418][-6.2949677 -6.4916358 -7.0231476 -6.657959 -5.7265897 -4.2744856 -3.1894121 -3.8564668 -3.3006039 -4.3665676 -5.9799786 -6.7316232 -7.6217318 -6.9482951 -7.0806527][-6.7446041 -6.0919609 -6.0319872 -6.2162118 -6.646739 -6.5129185 -5.8544126 -6.7536736 -5.9195623 -7.0454578 -7.2983294 -7.2635031 -7.2464552 -7.1792293 -6.4558024][-6.1758013 -5.4365482 -5.5982051 -5.4664831 -5.22542 -5.0152884 -6.0736647 -6.0612988 -6.7637348 -7.4912591 -7.913321 -6.5896707 -6.4983253 -7.0891905 -6.1218357][-5.2687225 -5.9481 -6.0593905 -6.249445 -6.2969446 -6.2108502 -6.0377574 -7.3899627 -7.2995868 -7.2712326 -6.8587155 -7.0006242 -6.6073456 -5.92467 -5.6830969]]...]
INFO - root - 2017-12-16 00:22:03.354266: step 78510, loss = 0.36, batch loss = 0.25 (12.7 examples/sec; 0.628 sec/batch; 44h:18m:06s remains)
INFO - root - 2017-12-16 00:22:09.813809: step 78520, loss = 0.27, batch loss = 0.15 (11.6 examples/sec; 0.687 sec/batch; 48h:29m:19s remains)
INFO - root - 2017-12-16 00:22:16.211850: step 78530, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 46h:04m:15s remains)
INFO - root - 2017-12-16 00:22:22.590205: step 78540, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 46h:26m:52s remains)
INFO - root - 2017-12-16 00:22:28.984519: step 78550, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 44h:59m:46s remains)
INFO - root - 2017-12-16 00:22:35.284062: step 78560, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 44h:42m:26s remains)
INFO - root - 2017-12-16 00:22:41.643252: step 78570, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 44h:54m:27s remains)
INFO - root - 2017-12-16 00:22:48.019345: step 78580, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.614 sec/batch; 43h:18m:12s remains)
INFO - root - 2017-12-16 00:22:54.366769: step 78590, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:44m:36s remains)
INFO - root - 2017-12-16 00:23:00.722973: step 78600, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 44h:38m:27s remains)
2017-12-16 00:23:01.269848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4727154 -4.3120365 -3.3325253 -2.6045022 -2.070961 -1.5743203 -1.0210791 -0.982285 -1.1570373 -2.3152585 -3.2810187 -4.4726458 -5.9042535 -5.7911487 -6.6013007][-5.3877888 -4.494607 -3.9927895 -3.0326858 -1.7661228 -1.55299 -1.4250617 -1.2612987 -1.09553 -1.7514076 -3.0383697 -4.1963558 -5.4511237 -5.783947 -7.022913][-5.615612 -4.9246912 -4.4168644 -3.7991271 -3.2050776 -2.2480021 -0.91643143 -0.9395051 -1.0017695 -2.0390587 -2.9375496 -4.0009661 -5.5833564 -5.7906857 -6.8240409][-5.4261074 -4.8381863 -4.65103 -3.620965 -2.6275654 -1.7968545 -1.1486864 -0.79556942 -0.45081997 -1.6525583 -2.9311566 -4.4965057 -6.114377 -6.0807409 -7.2127819][-5.5993619 -4.8060417 -3.7217374 -3.0179057 -2.2572002 -0.95420122 -0.13026333 0.064021111 0.24958944 -0.95781612 -2.2131891 -3.8136396 -5.6084456 -5.7720008 -7.0044446][-4.9073839 -4.1488829 -3.5480394 -2.2785659 -0.79842424 -0.017390251 0.692009 0.95761013 1.0875998 -0.27399302 -1.4127636 -3.2010851 -5.4335041 -5.3774204 -6.3302765][-3.8150485 -3.4996471 -2.5728178 -1.3643389 -0.11696482 0.79231548 1.6490364 1.6287794 1.274971 -0.22158003 -1.591289 -3.1879044 -4.9534645 -5.3303638 -6.556551][-3.5478468 -2.5045724 -1.6402311 -0.53769684 0.77160263 1.927886 2.6094446 2.46181 2.1177139 0.44546795 -1.221889 -3.3908734 -5.4114661 -5.639616 -6.76202][-3.5044012 -2.5097265 -1.2383113 0.27487659 1.3768454 2.2289391 2.7346106 2.9696589 2.5973959 0.62475872 -1.3678765 -3.3066297 -5.4400282 -6.0446205 -7.2985845][-2.8613019 -2.2072635 -0.92239428 0.14393473 1.032196 1.4925461 1.555583 1.875989 1.986763 0.15967607 -1.0326223 -3.3858571 -5.3535275 -6.0782127 -7.6285567][-4.2483058 -3.1268573 -2.3749337 -1.298007 -0.37777758 -0.24473429 -0.47155142 -0.87149477 -0.60639524 -1.6049666 -3.1765966 -4.5404787 -5.6638823 -6.3590708 -7.5394449][-5.0902882 -4.3859177 -3.7604923 -3.0355234 -2.1967764 -1.932415 -1.7587218 -2.2420859 -2.4851832 -3.5643387 -4.4649205 -5.0906496 -6.2011404 -6.5502582 -7.4923229][-5.7508707 -5.4135461 -5.0913134 -4.9202929 -4.64184 -3.7815933 -3.2731214 -3.5083632 -3.5483704 -4.2721329 -5.0676508 -5.8897252 -6.5757051 -6.2241344 -6.600894][-6.03167 -5.6116447 -5.1894269 -4.7798052 -4.8234949 -5.090476 -5.1400595 -4.8433952 -4.6614065 -4.8886023 -5.0577464 -5.677516 -5.5794673 -5.8802743 -6.4462771][-7.0878954 -6.8194823 -6.3340211 -6.4775124 -6.3099494 -6.1513076 -6.0299406 -6.57675 -7.2018542 -6.5816965 -6.0576682 -6.13278 -6.1712208 -5.93985 -5.5441976]]...]
INFO - root - 2017-12-16 00:23:07.694259: step 78610, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 45h:41m:56s remains)
INFO - root - 2017-12-16 00:23:14.154030: step 78620, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 44h:23m:38s remains)
INFO - root - 2017-12-16 00:23:20.602538: step 78630, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 45h:45m:27s remains)
INFO - root - 2017-12-16 00:23:27.083895: step 78640, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 44h:27m:09s remains)
INFO - root - 2017-12-16 00:23:33.438449: step 78650, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:54m:10s remains)
INFO - root - 2017-12-16 00:23:39.871539: step 78660, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 45h:54m:24s remains)
INFO - root - 2017-12-16 00:23:46.339526: step 78670, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 45h:59m:58s remains)
INFO - root - 2017-12-16 00:23:52.786600: step 78680, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 47h:01m:00s remains)
INFO - root - 2017-12-16 00:23:59.277367: step 78690, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 45h:05m:29s remains)
INFO - root - 2017-12-16 00:24:05.764263: step 78700, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 45h:47m:06s remains)
2017-12-16 00:24:06.264665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1653156 -3.6752329 -4.1780128 -4.103694 -3.6257315 -3.1445556 -2.8460207 -2.3474221 -1.8444147 -1.8536248 -2.6144032 -4.5974545 -5.4257135 -5.4422975 -6.1341548][-4.2460184 -4.4014726 -4.7611847 -5.0534434 -5.2376318 -4.4085493 -3.4367228 -2.8898349 -2.1823587 -2.5885897 -3.432755 -5.238862 -6.201757 -6.3762197 -6.9593182][-4.0787125 -4.4404993 -4.4193058 -4.1849451 -3.9117553 -3.7372038 -3.4777293 -3.0080614 -2.4036307 -2.9472857 -4.061779 -5.7388515 -6.6567698 -7.1861186 -7.8060074][-4.3040237 -3.8032358 -3.5400195 -3.4646945 -2.8957887 -2.4714265 -2.20322 -1.9919314 -1.8296652 -2.4934883 -3.7197466 -5.74233 -6.7239943 -7.2737193 -7.6774435][-4.3084922 -3.729712 -3.1177344 -2.1076951 -1.2796073 -0.84754515 -0.85301208 -0.75307846 -0.755476 -1.895083 -3.5800195 -5.7781057 -6.8556232 -7.3183694 -7.7289739][-6.1171331 -5.1696119 -4.1123505 -2.236865 -0.84283209 -0.023943424 0.70489025 0.86021805 0.58059406 -0.84509659 -2.9873152 -5.8930836 -7.5006385 -7.7358894 -8.13987][-6.2201157 -5.385169 -4.3878675 -1.962122 -0.13592196 1.240963 2.74928 2.7313786 2.4221478 1.28578 -1.1297703 -4.3153811 -6.1371751 -7.0956988 -7.6861238][-5.4348884 -4.3837423 -3.1914387 -1.2268515 0.76521683 2.9856653 4.3973227 3.9916067 3.8548479 2.603898 0.2367754 -2.972908 -4.7156525 -6.0091105 -6.7898088][-5.0744038 -4.5239019 -3.9561255 -2.3489294 -0.71482515 1.5282917 3.1760321 3.3634548 3.6218128 1.7479544 -0.66697931 -3.2800503 -4.7725296 -5.7871437 -6.5781689][-5.9616141 -5.5677967 -4.9513712 -3.8708744 -2.6067185 -1.2924438 0.099985123 0.80745125 1.1792097 -0.73599815 -2.2825661 -4.6664791 -5.8632946 -6.00067 -6.702673][-7.7377386 -7.2029686 -6.420948 -5.1678524 -4.025712 -2.9212861 -1.9593654 -1.3762288 -1.0188456 -2.3448648 -3.2236104 -5.6730213 -6.43083 -6.0606842 -6.3697634][-6.5968447 -6.6369677 -6.3023634 -5.8786869 -5.1138139 -4.0727949 -3.5366077 -3.328136 -3.2489405 -3.5502443 -3.8903096 -5.7411528 -5.988543 -5.932425 -6.6835818][-7.742619 -7.1805129 -6.7405753 -6.4190936 -5.8794117 -5.7233634 -5.5054111 -5.0490851 -5.0542669 -5.2241063 -5.4561234 -6.3860383 -6.2968969 -6.4308844 -6.8435597][-8.3121138 -7.755394 -7.1701579 -6.1027269 -5.7109914 -5.5252028 -5.2803135 -5.6940541 -5.9912829 -6.5098052 -6.5114708 -6.5590219 -6.6181035 -6.7646837 -7.0191236][-8.4840193 -9.3473644 -9.4350185 -8.35394 -7.5084591 -6.9615259 -7.0047512 -7.1759877 -7.3324847 -7.5602946 -7.4701266 -7.2308187 -6.8935475 -6.6246538 -6.6930857]]...]
INFO - root - 2017-12-16 00:24:12.753496: step 78710, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:45m:08s remains)
INFO - root - 2017-12-16 00:24:19.191614: step 78720, loss = 0.28, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 43h:51m:25s remains)
INFO - root - 2017-12-16 00:24:25.605594: step 78730, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 46h:24m:19s remains)
INFO - root - 2017-12-16 00:24:31.963236: step 78740, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.626 sec/batch; 44h:07m:29s remains)
INFO - root - 2017-12-16 00:24:38.416318: step 78750, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:51m:13s remains)
INFO - root - 2017-12-16 00:24:44.797612: step 78760, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 44h:39m:55s remains)
INFO - root - 2017-12-16 00:24:51.155137: step 78770, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 44h:52m:16s remains)
INFO - root - 2017-12-16 00:24:57.490392: step 78780, loss = 0.33, batch loss = 0.21 (12.8 examples/sec; 0.626 sec/batch; 44h:06m:00s remains)
INFO - root - 2017-12-16 00:25:03.832273: step 78790, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 44h:12m:20s remains)
INFO - root - 2017-12-16 00:25:10.273908: step 78800, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:44m:43s remains)
2017-12-16 00:25:10.828949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4051886 -3.175138 -3.0390429 -2.7676907 -2.6366711 -2.2719712 -1.9145575 -2.2008767 -1.7986083 -3.8490655 -4.6282587 -5.8399763 -6.7409387 -7.1571512 -8.0584488][-3.2434874 -2.9442549 -3.1799779 -3.1428242 -3.1083846 -3.1300497 -3.1225066 -3.5700221 -3.2113147 -4.8186626 -5.3634453 -6.3144026 -7.0919728 -7.679677 -8.4393539][-3.2291346 -2.7431927 -2.8183055 -2.6058865 -2.6220388 -2.5261388 -2.6852045 -2.9810967 -2.8299742 -5.056601 -5.6231565 -6.4388332 -7.3353891 -7.9396143 -8.9599171][-3.2727804 -2.7861719 -2.7402697 -2.5557003 -2.3062987 -1.936502 -1.6286182 -1.9660392 -2.1256776 -4.2109756 -5.0066862 -6.3237629 -7.2158146 -7.8444433 -8.8248472][-4.6460094 -3.4922938 -2.7226195 -2.2583346 -1.9407372 -1.3140721 -1.2081337 -1.5682068 -1.5484748 -3.7250855 -4.6670809 -5.9000645 -6.9409676 -7.6738362 -8.5933743][-5.4153175 -4.3088074 -3.321074 -2.1389918 -1.2031665 -0.41867161 -0.10911226 -0.39964533 -0.643487 -2.705174 -3.5988703 -4.9625597 -6.05603 -6.9106245 -8.0145836][-5.1825094 -4.3730874 -3.1846728 -1.8282266 -0.68908548 0.6605854 1.2243605 1.0130548 0.95390892 -1.2554579 -2.2428689 -3.7390842 -4.9074583 -5.8010588 -6.9217706][-4.5744677 -3.2940192 -2.1787729 -0.95123243 0.30464268 1.3221779 1.8771906 2.1461182 2.2597008 -0.099831104 -1.3362846 -2.9475751 -4.1908379 -4.9965363 -6.0487132][-3.7760427 -2.8702354 -1.8915095 -0.71613359 0.046581268 0.63841152 0.94782352 1.3558598 1.8063936 -0.2190485 -1.4046993 -3.0903983 -4.429122 -5.270175 -6.2342205][-3.3177471 -2.4696279 -1.4996514 -0.663836 -0.13093424 0.1283164 0.4491787 0.73092461 1.0356445 -0.98517466 -1.9578362 -3.3520522 -4.6571603 -5.4336476 -6.2949343][-4.1615739 -3.0423517 -2.0219893 -1.260664 -0.79366207 -0.63661909 -0.64882612 -0.599278 -0.47854471 -2.565146 -3.6890187 -4.6760912 -5.7555242 -6.2235045 -6.6959291][-4.0513682 -3.244801 -2.3148232 -1.383038 -0.935081 -1.0160117 -1.1377316 -1.3702745 -1.3950505 -2.6190996 -3.6676102 -4.7258081 -5.387023 -6.0280876 -6.7213264][-5.4214005 -4.3316975 -3.3657031 -2.4382 -2.0596981 -2.0561132 -2.1807308 -2.3490772 -2.3350172 -3.3157501 -3.931246 -4.5120497 -4.9645061 -5.3841372 -6.0413313][-5.8188133 -4.9213743 -4.2875671 -3.1485753 -2.4290028 -2.4417305 -2.6002088 -2.7768254 -2.7960372 -3.2767196 -3.2083616 -3.5894151 -3.9425795 -4.2431965 -4.8029852][-6.2170076 -5.7837229 -5.0839772 -4.4657488 -3.96344 -3.6553941 -4.1561537 -4.4725504 -4.5368791 -4.3876543 -3.9906988 -4.0029583 -4.1161733 -4.1211824 -4.2170496]]...]
INFO - root - 2017-12-16 00:25:17.191967: step 78810, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 44h:51m:26s remains)
INFO - root - 2017-12-16 00:25:23.547805: step 78820, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 46h:03m:40s remains)
INFO - root - 2017-12-16 00:25:30.001263: step 78830, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 44h:22m:27s remains)
INFO - root - 2017-12-16 00:25:36.330899: step 78840, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 45h:05m:42s remains)
INFO - root - 2017-12-16 00:25:42.736416: step 78850, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 45h:28m:31s remains)
INFO - root - 2017-12-16 00:25:49.078561: step 78860, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:39m:46s remains)
INFO - root - 2017-12-16 00:25:55.493441: step 78870, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 44h:46m:55s remains)
INFO - root - 2017-12-16 00:26:01.903750: step 78880, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 46h:08m:55s remains)
INFO - root - 2017-12-16 00:26:08.348543: step 78890, loss = 0.33, batch loss = 0.21 (12.3 examples/sec; 0.650 sec/batch; 45h:47m:26s remains)
INFO - root - 2017-12-16 00:26:14.749402: step 78900, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 44h:35m:41s remains)
2017-12-16 00:26:15.253254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8400297 -5.5369124 -5.8328986 -5.5814371 -4.9645362 -4.2279916 -3.8504386 -3.4740458 -3.4978614 -5.7122068 -6.7662277 -7.7780213 -7.7260876 -7.8743944 -7.2590938][-3.8167505 -4.7311831 -5.0913677 -5.1287827 -4.9329934 -4.0099497 -3.5421982 -3.142869 -3.8673511 -6.0665154 -7.1719894 -8.7303886 -9.4851866 -9.8943491 -9.0896511][-3.7721448 -4.3457747 -4.7097836 -4.6537924 -3.8929224 -3.2789197 -2.650569 -2.1910958 -2.1739383 -4.8311205 -6.46132 -8.2913227 -9.0015278 -10.155055 -10.022897][-3.458497 -3.8553061 -4.2033491 -4.1400604 -3.2580109 -2.0986605 -1.3643627 -1.1538925 -1.0132985 -3.1036305 -4.3138771 -6.7359605 -7.8809509 -9.4033766 -9.6000729][-3.4972425 -3.0786495 -2.7143388 -2.3340683 -1.200017 -0.23697948 0.42288876 0.18582392 0.074858665 -2.1630864 -3.5761847 -5.3994179 -6.2558832 -7.6787977 -8.2112474][-3.8996851 -2.787848 -1.7636867 -0.69946957 0.0098509789 1.1424704 1.7376032 1.6460934 1.1444702 -1.0703602 -2.3673944 -4.2422872 -4.8237009 -5.9826231 -6.3303461][-3.7226646 -2.3803182 -1.1959338 0.38970757 1.9127626 3.0681343 3.3560934 3.0147943 2.7897053 0.4157505 -1.2202611 -3.4849267 -4.4135456 -5.2858071 -5.5808377][-3.0175204 -1.9636688 -1.0052266 0.79815674 2.3161278 3.8245306 4.6237555 4.0680532 3.3184223 1.0353289 -0.367661 -2.7217784 -3.6592903 -4.8262463 -4.9540706][-3.6733308 -2.5395446 -1.7158699 -0.0670948 1.4790344 3.0813179 3.7259254 3.8243866 3.261198 0.6594677 -0.81222534 -2.5598321 -3.550662 -4.777154 -4.8406277][-4.7100229 -3.62291 -3.0919662 -1.8775573 -0.69701958 0.89977551 2.165925 2.6875181 2.6769047 0.32472134 -1.4950886 -3.3272567 -3.9443998 -5.0239887 -5.115447][-5.4811497 -4.9082437 -4.1424637 -2.8932185 -1.9969501 -0.9881258 -0.25551748 0.30262518 0.49208832 -1.2329602 -2.2421288 -3.8797371 -4.6793432 -5.7606354 -5.6737061][-5.9585509 -5.5081697 -5.0970726 -4.0798779 -3.3074784 -2.4612098 -1.925837 -1.6037426 -1.5606036 -2.737247 -3.5196571 -4.484868 -5.0126314 -5.9747438 -6.378726][-6.767652 -6.1421132 -5.5645781 -4.7085361 -4.3596115 -3.8372712 -3.4196367 -3.2344246 -3.0771866 -3.6993017 -4.2896695 -4.6759844 -4.9702168 -5.4579964 -6.0269766][-7.0004225 -6.5714588 -6.0316153 -4.955277 -4.4110079 -4.0836439 -4.2414956 -4.4415259 -4.6007748 -4.2404985 -4.7412939 -4.5062075 -4.1389976 -4.4735413 -4.9823742][-6.6893744 -6.7268567 -6.4771733 -5.6125641 -5.0796757 -4.7195492 -4.5380192 -4.9238052 -5.1862383 -4.9103589 -5.0075278 -4.8015108 -4.8067408 -4.4895768 -4.5325313]]...]
INFO - root - 2017-12-16 00:26:21.620261: step 78910, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 43h:55m:56s remains)
INFO - root - 2017-12-16 00:26:28.104148: step 78920, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 45h:08m:57s remains)
INFO - root - 2017-12-16 00:26:34.496778: step 78930, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 44h:29m:02s remains)
INFO - root - 2017-12-16 00:26:40.906445: step 78940, loss = 0.34, batch loss = 0.22 (12.7 examples/sec; 0.632 sec/batch; 44h:32m:29s remains)
INFO - root - 2017-12-16 00:26:47.233662: step 78950, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:31m:06s remains)
INFO - root - 2017-12-16 00:26:53.562882: step 78960, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 45h:18m:09s remains)
INFO - root - 2017-12-16 00:26:59.919691: step 78970, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 44h:46m:18s remains)
INFO - root - 2017-12-16 00:27:06.239616: step 78980, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.620 sec/batch; 43h:37m:52s remains)
INFO - root - 2017-12-16 00:27:12.623143: step 78990, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 45h:09m:14s remains)
INFO - root - 2017-12-16 00:27:19.005839: step 79000, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 45h:22m:10s remains)
2017-12-16 00:27:19.516020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0530109 -3.4888678 -3.6162763 -3.4777961 -3.7632959 -4.1766043 -4.5498447 -4.8955107 -5.1701837 -6.2878509 -7.545001 -8.533268 -9.4140549 -9.6145449 -9.5904074][-2.9025521 -2.9013395 -3.1988187 -3.4489384 -4.056365 -4.2456264 -4.288672 -5.2335587 -6.3274469 -7.849297 -8.9810219 -9.9420071 -10.293712 -10.600899 -10.624575][-3.0258088 -2.3545928 -1.8909326 -1.8409152 -2.1509266 -2.2242608 -2.4885921 -3.4499488 -4.58904 -6.5228043 -8.082078 -9.0127258 -9.61927 -9.9545937 -10.06394][-3.3709168 -2.7404442 -1.5438204 -0.47893238 -0.22972775 -0.32924795 -0.4866333 -1.3500085 -2.274159 -4.0724473 -5.5624323 -7.0340643 -8.029788 -8.74946 -9.0526209][-3.6278763 -2.7524557 -1.742003 -0.55523062 0.24956942 1.30649 1.6308842 0.84997463 0.18314028 -1.9101043 -3.8463666 -5.3996792 -6.5053229 -7.5186577 -8.2801847][-4.4620633 -3.2943134 -1.9931211 -0.85477638 -0.049161911 1.3363466 2.4565363 1.9370203 1.1479015 -0.68040419 -2.5332875 -4.3900204 -5.7836962 -6.8869185 -7.6347055][-4.0788088 -3.2462902 -2.0700254 -0.66961336 0.53078651 1.4010916 2.11946 2.4221115 2.4995356 0.80215168 -0.97691584 -2.9892592 -4.549017 -6.0269184 -7.2412987][-2.9800606 -2.5654826 -1.3243513 -0.47567892 0.51351452 1.8378801 2.5324078 2.6812077 3.1398554 2.1244164 0.51566982 -1.7143879 -3.6001234 -5.70764 -7.3277788][-3.0891142 -2.6330919 -1.8348227 -0.92507029 0.096485615 1.2765989 1.9250078 2.1060677 2.5891113 1.288765 -0.40762854 -2.5833278 -4.5270662 -6.3670397 -8.1306639][-3.0811405 -2.7856069 -2.0325575 -1.0566688 0.1043973 0.55731869 0.5894022 1.010601 1.3946686 -0.2024374 -1.8097596 -3.5749807 -4.9565234 -6.6944208 -8.3947954][-4.8287811 -4.2268047 -3.323288 -2.5279546 -1.4202585 -1.1277118 -0.81041288 -0.65467215 -0.30052614 -1.6292548 -2.9998827 -4.4124889 -5.6310453 -7.1014962 -8.3050995][-5.56604 -5.3234692 -4.6670613 -3.7880032 -2.9862266 -2.4548974 -2.4499569 -2.1918526 -1.7618165 -2.5332575 -3.3334813 -4.6287422 -5.3746018 -6.6534977 -7.7931104][-6.7500811 -6.2256756 -5.8503776 -4.7635813 -3.5444846 -3.2559195 -3.2813964 -3.1097302 -2.8787928 -3.7015297 -4.54218 -5.0327578 -5.6752672 -6.5052981 -7.2693911][-6.8128119 -6.3531446 -5.4642439 -4.5182776 -3.7979236 -3.528368 -3.5724592 -3.8318565 -3.9103456 -4.3397546 -4.6426711 -5.0428219 -5.3853931 -6.31169 -6.9714031][-7.8230658 -7.4310389 -6.8662753 -5.9673738 -5.0256109 -4.7756453 -4.6994157 -4.51143 -4.6180587 -4.7330089 -5.1319284 -5.4111185 -5.6375017 -5.7369947 -5.8723316]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:27:26.024702: step 79010, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 45h:28m:38s remains)
INFO - root - 2017-12-16 00:27:32.456423: step 79020, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 45h:55m:19s remains)
INFO - root - 2017-12-16 00:27:38.829233: step 79030, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 44h:44m:24s remains)
INFO - root - 2017-12-16 00:27:45.221333: step 79040, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 45h:42m:16s remains)
INFO - root - 2017-12-16 00:27:51.620344: step 79050, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 44h:37m:35s remains)
INFO - root - 2017-12-16 00:27:58.084792: step 79060, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 44h:31m:01s remains)
INFO - root - 2017-12-16 00:28:04.441940: step 79070, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 44h:04m:01s remains)
INFO - root - 2017-12-16 00:28:10.822179: step 79080, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:37m:50s remains)
INFO - root - 2017-12-16 00:28:17.135988: step 79090, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 44h:39m:16s remains)
INFO - root - 2017-12-16 00:28:23.560040: step 79100, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 45h:06m:28s remains)
2017-12-16 00:28:24.130821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1008315 -3.5958929 -3.8381152 -3.8789043 -3.7304602 -3.6276741 -3.3298535 -3.1046562 -2.8621755 -3.0473619 -4.2221041 -6.2444181 -7.7276454 -8.6624594 -9.6437216][-2.5014944 -2.8798661 -3.1900711 -3.9922917 -4.8577857 -5.3733168 -5.3557086 -4.7922382 -4.332664 -3.8730702 -4.3101521 -5.60961 -6.96954 -7.7739863 -9.0403137][-2.3395996 -2.4517803 -2.7827034 -3.4695277 -3.9236841 -4.7009954 -5.07024 -4.829268 -3.9675112 -2.8868451 -3.153265 -4.1664734 -4.9575644 -5.7937884 -7.1403379][-1.9905553 -1.4521713 -1.7686338 -2.352128 -2.6113777 -3.0605369 -3.2912273 -3.23176 -3.1165428 -2.3506103 -2.4437356 -3.4806056 -4.4810848 -5.0563755 -5.618372][-1.2137265 -1.0631523 -1.1310759 -1.1686201 -1.4114189 -1.193037 -0.56503487 -0.79617167 -0.88412666 -0.43259668 -0.83803463 -2.4716244 -3.6835995 -4.1933517 -4.7881794][-1.382329 -1.0669618 -1.2585468 -1.1578016 -0.727231 -0.067405224 0.887517 1.0657377 1.2515173 1.0694036 0.035404682 -2.0234628 -3.6482224 -4.6731324 -5.633255][-2.445044 -1.7309899 -1.3658419 -0.807529 0.11433887 1.2899208 2.2024555 2.1622372 2.2285709 1.5411968 -0.24976063 -2.7609735 -4.5560322 -5.5363913 -6.3833184][-1.5970674 -1.2767401 -0.67229795 0.11276293 0.92172146 2.0891552 3.2996788 3.3501711 3.1070251 1.9515991 -0.10456944 -2.9878983 -5.0289726 -6.094142 -6.7866936][-2.1561604 -1.7016263 -0.88253117 0.086110592 0.88106918 1.6514854 2.5396481 2.6332026 2.6972008 1.9841394 0.24729919 -2.6985602 -4.6937866 -5.5874729 -6.0990958][-2.2357006 -1.6601343 -1.4424968 -0.73176622 0.10394144 0.66123772 0.91139889 1.0287504 1.0611095 0.14397192 -1.4226494 -3.5988111 -5.04203 -5.8039374 -6.5863571][-5.2608705 -4.2542629 -3.8746548 -3.6651416 -3.2610407 -3.0197892 -2.8021975 -2.7890038 -2.9486427 -3.69817 -4.8058829 -6.2377682 -7.0483274 -7.0554333 -7.4691076][-6.6319461 -5.9276776 -5.6884804 -5.425602 -5.2247958 -5.4931726 -5.3817587 -5.5459991 -5.6860971 -6.3431435 -7.1493683 -7.6462221 -8.0996065 -8.6545258 -8.7130775][-8.1627 -7.5579891 -7.6606636 -7.1933279 -6.4281545 -6.4750223 -6.5297041 -7.0382385 -7.0767403 -7.3394809 -8.276639 -8.3640594 -8.3874378 -8.2685394 -8.2346706][-5.7142029 -5.9496322 -6.3170986 -5.9316716 -5.5321722 -5.3661575 -4.8299165 -5.2080555 -5.7465067 -6.1296577 -6.7716885 -6.91525 -6.940208 -6.9595652 -6.9430861][-4.3730755 -4.6858921 -5.3817091 -5.4509611 -5.4776173 -5.19664 -4.8560081 -4.8768015 -4.5230885 -4.9809165 -5.4864063 -5.6519442 -5.8769646 -6.55146 -6.8247652]]...]
INFO - root - 2017-12-16 00:28:30.648942: step 79110, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 46h:52m:24s remains)
INFO - root - 2017-12-16 00:28:37.035925: step 79120, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 44h:57m:27s remains)
INFO - root - 2017-12-16 00:28:43.378552: step 79130, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 44h:27m:30s remains)
INFO - root - 2017-12-16 00:28:49.747437: step 79140, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 44h:08m:26s remains)
INFO - root - 2017-12-16 00:28:56.141837: step 79150, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:40m:30s remains)
INFO - root - 2017-12-16 00:29:02.517459: step 79160, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 46h:01m:53s remains)
INFO - root - 2017-12-16 00:29:08.961554: step 79170, loss = 0.41, batch loss = 0.30 (12.2 examples/sec; 0.656 sec/batch; 46h:10m:21s remains)
INFO - root - 2017-12-16 00:29:15.268049: step 79180, loss = 0.25, batch loss = 0.13 (12.9 examples/sec; 0.621 sec/batch; 43h:42m:52s remains)
INFO - root - 2017-12-16 00:29:21.682604: step 79190, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.627 sec/batch; 44h:06m:47s remains)
INFO - root - 2017-12-16 00:29:28.037008: step 79200, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.630 sec/batch; 44h:17m:34s remains)
2017-12-16 00:29:28.633258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9540048 -4.111649 -4.3952093 -4.4147553 -4.3988585 -4.0974817 -4.2065678 -3.9208994 -2.7924061 -2.3589058 -3.8430436 -4.3774047 -4.3037663 -4.4604321 -6.1010103][-3.9792607 -4.0373778 -3.9213731 -4.1645303 -4.3703971 -4.3706651 -4.4297171 -4.3679857 -3.4938293 -3.31325 -4.393693 -4.583746 -5.2909522 -4.9035521 -5.4604092][-5.5863495 -5.4516039 -4.9998045 -4.6786513 -4.438097 -4.3914814 -4.4990225 -4.3950396 -4.1339073 -4.240922 -5.517777 -5.8404865 -6.250289 -5.9894624 -6.9286346][-5.7946239 -5.8748312 -5.2339683 -4.2580962 -3.5118995 -2.9131827 -2.6438279 -2.5431409 -2.5300498 -2.8468175 -4.763588 -5.5413823 -6.0345678 -6.3339868 -7.3048959][-6.5531 -5.3493338 -4.352644 -3.494463 -2.4649892 -1.5416808 -1.1882477 -0.95840311 -0.625473 -1.0250535 -2.8465466 -3.81721 -4.9095554 -6.088419 -7.6303964][-5.7643976 -4.7588787 -3.6131034 -2.1945381 -0.89414358 -0.068706989 0.55145836 0.74492741 1.1466274 0.68625736 -1.1378531 -2.087913 -3.3945723 -4.6997247 -6.6068377][-4.6002388 -3.7492471 -2.3610449 -0.98914909 0.43731117 1.6403542 2.458806 2.7419748 3.0697546 2.594779 0.40740967 -1.059833 -2.3233256 -3.5153265 -5.3266611][-3.1365247 -2.1294513 -1.2756915 -0.1427269 1.4612417 1.7966261 2.7153702 3.4928732 4.1592979 3.8432093 1.3555784 -0.41200256 -2.080698 -3.3132997 -4.6828222][-3.1986895 -2.2037268 -1.491663 -0.12892962 1.0564575 1.511157 2.1921062 2.9564409 4.1541634 3.6156301 0.83003426 -0.99585772 -2.4951706 -3.9433692 -5.1554956][-4.8072996 -4.0109882 -2.5905404 -1.6463313 -0.51937962 0.15254021 0.53294849 1.0176945 1.6949558 1.2252064 -1.0707412 -2.4880204 -3.6242404 -4.6193323 -5.9620285][-6.4509463 -6.2366095 -5.6699133 -4.4324722 -3.114573 -2.6072206 -2.125957 -1.924212 -1.7547822 -2.4278898 -3.99252 -4.7039385 -5.48179 -5.8561931 -6.7845445][-7.9061522 -7.700284 -7.29944 -6.5857525 -5.84525 -5.0715685 -4.2637196 -4.0506582 -3.9063442 -4.7408867 -6.1221018 -6.2830181 -6.6407003 -6.8298969 -7.44259][-7.9902644 -8.29398 -8.2902374 -7.7749043 -7.0978026 -6.283936 -5.6770649 -5.5732622 -5.4527454 -5.6616096 -6.7116151 -6.7676597 -6.4873943 -6.2738442 -6.6594243][-7.5887408 -7.728693 -7.3629069 -7.1999769 -6.9490581 -6.4687095 -6.0540156 -5.9774623 -5.8842063 -6.0680275 -6.7656283 -6.5855074 -6.4036074 -5.8970003 -5.8607087][-6.8422318 -6.5441909 -6.3944278 -6.3530822 -5.9279075 -6.0363789 -5.9954367 -6.1092029 -6.3879862 -6.4073229 -6.7437534 -6.8738217 -6.7600064 -6.5179858 -6.3005853]]...]
INFO - root - 2017-12-16 00:29:35.028719: step 79210, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 45h:18m:25s remains)
INFO - root - 2017-12-16 00:29:41.467108: step 79220, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 43h:25m:48s remains)
INFO - root - 2017-12-16 00:29:47.859439: step 79230, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 44h:27m:35s remains)
INFO - root - 2017-12-16 00:29:54.191556: step 79240, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 46h:40m:45s remains)
INFO - root - 2017-12-16 00:30:00.597749: step 79250, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 44h:55m:46s remains)
INFO - root - 2017-12-16 00:30:06.957512: step 79260, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 46h:08m:26s remains)
INFO - root - 2017-12-16 00:30:13.365572: step 79270, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 44h:14m:14s remains)
INFO - root - 2017-12-16 00:30:19.841866: step 79280, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 46h:44m:43s remains)
INFO - root - 2017-12-16 00:30:26.239591: step 79290, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 43h:33m:45s remains)
INFO - root - 2017-12-16 00:30:32.647076: step 79300, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 44h:39m:19s remains)
2017-12-16 00:30:33.163349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6350489 -1.5123701 -1.673243 -2.0366216 -2.1585159 -2.625452 -2.8305292 -3.148715 -3.408289 -4.8310785 -4.9078326 -5.9759874 -6.6283784 -7.441782 -8.1642332][-3.1192884 -2.6286082 -2.4513555 -2.8364692 -3.5298281 -4.2323732 -4.4574356 -4.4110556 -4.1592793 -5.1063366 -5.5128407 -7.1398697 -7.8095694 -7.915791 -8.4852448][-2.2608271 -2.6761913 -3.2538581 -3.4486423 -3.77259 -4.0547943 -3.8728926 -4.05289 -4.2388144 -5.6582308 -5.5997095 -6.4907146 -6.7641373 -7.1916103 -7.4028111][-2.9021592 -2.7405653 -2.5987792 -2.5253921 -2.8076048 -3.2151794 -3.1436014 -2.888958 -2.6473637 -4.3420553 -5.1142092 -6.4462466 -6.6805849 -6.4460564 -6.7721958][-3.2437606 -2.863883 -2.6561646 -2.6068234 -2.2210374 -1.4665127 -1.0116472 -1.2261357 -1.3007426 -3.3477774 -4.4765263 -6.4785671 -6.9291368 -6.67109 -6.5553255][-3.3156018 -3.0768037 -3.1899395 -2.0740757 -1.121448 -0.30730915 0.70306206 0.72742462 0.31549549 -2.3313375 -4.0190406 -6.390995 -7.4497976 -6.9943433 -6.8321304][-3.48835 -3.2518792 -2.7756915 -1.5613632 -0.5863409 0.83159924 1.9850559 2.2749548 2.0658436 -0.78254986 -2.732667 -5.7744346 -7.05891 -7.136168 -6.9024391][-3.6204057 -2.9838839 -2.7364416 -1.142858 0.19707251 1.724617 2.6155367 3.4397745 3.6789656 0.67314816 -1.5598106 -4.7495933 -6.3455977 -6.9307246 -7.0766172][-3.5047226 -3.047791 -2.6491671 -1.6933317 -0.85326815 0.46507835 1.4753628 2.2757692 2.7132835 0.28300238 -1.5747194 -4.3159218 -5.9941645 -6.4765344 -6.9485197][-4.4729815 -3.8896759 -3.2271528 -2.4168515 -1.7101698 -0.76328564 -0.11371088 0.5160017 0.96072865 -1.0664783 -2.2345133 -4.6746445 -5.9475861 -6.7711315 -7.3192658][-5.5061994 -4.7699261 -4.4740486 -3.630024 -2.8859787 -1.8820009 -1.5246658 -1.3979869 -1.2548876 -3.1214933 -4.2906332 -5.693862 -6.5271521 -7.3274856 -7.9150681][-5.8644242 -4.9313259 -4.1952128 -4.1619177 -4.3629766 -4.0779471 -3.7754815 -3.3116207 -2.8916521 -3.7697918 -4.3248572 -6.3669696 -6.822659 -7.1257181 -7.4612112][-7.1240454 -6.3602562 -5.3069353 -4.6283836 -4.5310936 -4.5117683 -4.5633497 -4.8592329 -4.9928975 -5.7320595 -5.8127527 -5.9862247 -6.4028406 -6.8833742 -6.879705][-7.52814 -6.8805881 -6.1169376 -5.2396717 -4.493762 -4.47977 -4.5743341 -4.6503048 -4.8495283 -5.6800528 -5.8961272 -6.4740448 -6.1332569 -6.2341113 -6.227922][-8.036974 -7.5822735 -6.6025405 -5.8633089 -5.4428778 -5.6086764 -5.7778816 -5.9425836 -5.8697224 -5.9135342 -6.0024862 -5.9412017 -5.8366656 -5.641758 -5.634798]]...]
INFO - root - 2017-12-16 00:30:39.562882: step 79310, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 44h:48m:30s remains)
INFO - root - 2017-12-16 00:30:45.982332: step 79320, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 44h:49m:45s remains)
INFO - root - 2017-12-16 00:30:52.321417: step 79330, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:36m:51s remains)
INFO - root - 2017-12-16 00:30:58.695768: step 79340, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 44h:46m:43s remains)
INFO - root - 2017-12-16 00:31:05.100983: step 79350, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:37m:35s remains)
INFO - root - 2017-12-16 00:31:11.483831: step 79360, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:50m:03s remains)
INFO - root - 2017-12-16 00:31:17.864747: step 79370, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.635 sec/batch; 44h:38m:42s remains)
INFO - root - 2017-12-16 00:31:24.196061: step 79380, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 44h:54m:14s remains)
INFO - root - 2017-12-16 00:31:30.562496: step 79390, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 45h:11m:54s remains)
INFO - root - 2017-12-16 00:31:36.858361: step 79400, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 44h:11m:51s remains)
2017-12-16 00:31:37.335416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3432741 -5.8912039 -5.425374 -5.4771552 -5.4552608 -5.1059237 -4.4199405 -3.7503674 -3.2562504 -3.5220265 -4.5607042 -6.4463696 -7.9939957 -9.1851892 -10.375929][-6.0399027 -5.8625784 -5.8417807 -6.0343952 -6.2251735 -6.2898746 -5.9466171 -5.3170862 -4.4339914 -4.3582411 -5.1661024 -7.0507536 -8.4510584 -9.1944637 -10.432937][-5.6663742 -4.9335833 -4.70012 -5.03901 -5.3875637 -5.3386793 -4.8857327 -4.628695 -4.3434162 -4.0291963 -5.1632385 -6.9483509 -8.1114044 -8.4780846 -9.2863913][-4.9580212 -4.403522 -4.175889 -3.8479083 -3.5296354 -3.3930964 -2.8573351 -2.7457628 -2.6097364 -2.9046531 -4.1325278 -6.0929465 -7.2449694 -7.9750891 -8.7178249][-4.771039 -3.6220913 -3.3392873 -2.99581 -2.4626036 -1.6903701 -0.813241 -0.83507824 -0.67612028 -1.4905176 -3.0709405 -5.2489462 -6.8732886 -7.7648277 -8.5692406][-4.5612607 -3.9508555 -3.1720371 -2.2059646 -1.3899298 -0.25613832 1.0427046 1.0324717 1.1794777 -0.059960365 -2.0384121 -4.4124737 -6.2527738 -7.2525411 -8.0875053][-4.6766062 -3.9385929 -3.0477123 -1.875412 -0.66103697 0.79810905 2.1339455 2.2936611 2.330987 0.97397423 -1.1447506 -3.9704361 -5.9565382 -6.8207674 -7.6071935][-4.3446112 -3.865937 -3.1081662 -2.0177369 -0.96680927 0.62822819 2.0125189 2.3282576 2.3867474 0.99023819 -1.1409192 -3.8537006 -5.56728 -6.6318979 -7.6436419][-5.0008707 -4.49603 -3.58011 -2.9049296 -2.0575695 -0.83005857 0.27857637 0.81244087 1.1463289 -0.16251659 -2.3367481 -4.809608 -6.4688764 -7.2990184 -7.8864021][-5.8458123 -5.4707537 -5.0551405 -4.5534344 -3.803546 -2.9134431 -2.47514 -2.1057463 -1.8923464 -3.0411925 -4.7485933 -6.6987743 -8.0406284 -8.415823 -8.8739891][-7.6892338 -7.479259 -6.865242 -6.3632069 -5.7009635 -4.8286648 -4.3724623 -4.385211 -4.6360049 -5.2045107 -6.3764925 -7.6102877 -8.5100937 -8.9768677 -9.3688135][-8.0306749 -7.888473 -7.7104692 -7.1058836 -6.2491779 -5.9310265 -5.4697905 -5.4288759 -5.5655708 -6.1199908 -7.0431767 -7.7856274 -8.2974 -8.5883226 -8.5915689][-8.50473 -8.2483311 -8.15433 -7.5241094 -6.5016208 -5.7713761 -5.3925371 -5.6474891 -5.7477865 -6.08984 -6.9878917 -7.2202897 -7.4336553 -7.8458381 -7.7255311][-7.2074451 -7.1688495 -7.1192307 -6.4990077 -5.8141108 -5.0121927 -4.6553431 -5.000164 -5.4023714 -5.8261118 -6.535594 -6.7203636 -6.8555679 -6.9999404 -6.9366279][-7.68082 -8.0000076 -7.87157 -7.5994668 -7.0774112 -6.1553545 -5.61183 -5.8788934 -6.2453465 -6.7250605 -6.8689003 -6.7376165 -6.6082811 -6.5495672 -6.379528]]...]
INFO - root - 2017-12-16 00:31:43.791085: step 79410, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 44h:27m:09s remains)
INFO - root - 2017-12-16 00:31:50.164378: step 79420, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 44h:36m:07s remains)
INFO - root - 2017-12-16 00:31:56.574062: step 79430, loss = 0.30, batch loss = 0.19 (13.0 examples/sec; 0.617 sec/batch; 43h:22m:16s remains)
INFO - root - 2017-12-16 00:32:02.926906: step 79440, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 44h:59m:31s remains)
INFO - root - 2017-12-16 00:32:09.409580: step 79450, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 45h:59m:32s remains)
INFO - root - 2017-12-16 00:32:15.842089: step 79460, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 43h:58m:52s remains)
INFO - root - 2017-12-16 00:32:22.429327: step 79470, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 47h:06m:20s remains)
INFO - root - 2017-12-16 00:32:28.859233: step 79480, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 45h:14m:43s remains)
INFO - root - 2017-12-16 00:32:35.338822: step 79490, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 45h:04m:18s remains)
INFO - root - 2017-12-16 00:32:41.758801: step 79500, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 46h:16m:10s remains)
2017-12-16 00:32:42.302235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1488061 -7.045083 -7.0102129 -6.6987567 -6.0036969 -5.0302653 -4.311615 -3.8964186 -3.7013485 -4.3858032 -4.3541508 -4.8746705 -5.8536925 -7.3854866 -8.07139][-5.2814131 -5.429635 -5.7923594 -6.0011663 -6.1551123 -5.7647381 -5.04321 -4.70442 -3.9056928 -4.0188556 -3.7457206 -4.5518556 -5.452035 -6.8662786 -7.8316541][-3.2791767 -3.1744862 -4.0059423 -4.5072355 -5.2173166 -5.541944 -5.1998997 -4.4776058 -3.7997115 -4.046957 -3.3984675 -4.17095 -5.1581335 -6.8753805 -7.6668425][-1.7846785 -1.2302051 -1.5648251 -2.3284698 -3.1071024 -3.3248215 -2.9553175 -2.6517968 -2.1184363 -2.8824649 -2.7788248 -3.5064979 -4.5839634 -6.5720959 -7.5262537][-0.93033075 -0.371243 -0.26622057 -0.066464424 -0.37307549 -0.44812107 -0.032319546 0.20773172 0.593503 -0.69188452 -1.1552763 -2.6683865 -4.0964746 -5.8118134 -7.0570083][0.2478857 1.0173836 0.88911438 1.262454 1.4257555 1.6375237 1.7868767 2.0071554 2.3381891 0.54797745 -0.26438951 -2.1787672 -3.6787748 -5.49043 -6.7315216][-0.39859438 0.40890121 0.99242878 1.1288977 1.4430218 2.6262541 3.483633 3.3430519 3.038372 0.99653149 -0.29433584 -2.4275603 -3.9706635 -5.9518681 -6.9818616][-1.2124825 -1.0341587 -0.46278667 0.66020775 1.1122589 1.7609739 2.8173428 3.089448 3.0626564 0.98573589 -0.75622892 -2.7643113 -4.3250928 -6.0507636 -6.7154684][-2.7668128 -2.0760846 -1.8377624 -0.71980667 0.18231487 0.70485878 1.3702822 1.8735895 2.3316755 0.10491514 -1.3972921 -2.9617906 -4.4462748 -6.1821852 -6.7184958][-4.3709192 -3.9618304 -3.6108961 -3.1075625 -2.2319217 -1.2003531 -0.41245508 -0.093928337 0.22808743 -1.1952744 -2.369256 -4.0193605 -5.2787676 -6.2062597 -6.90433][-6.8391171 -6.2918262 -6.0320339 -5.7919569 -5.4231424 -4.8322725 -4.331779 -4.1589212 -4.0687256 -4.9825296 -5.6135473 -6.200613 -6.9411917 -7.4915247 -8.1640511][-8.1603155 -7.5867181 -7.2724557 -7.2105789 -7.1295495 -6.9488893 -6.7611184 -6.6190395 -6.3231869 -6.9357977 -7.2062182 -6.9886036 -7.277564 -7.7322788 -7.9570041][-8.566761 -8.5857363 -8.2718563 -8.412447 -8.4285145 -8.0511055 -7.7942715 -7.7883177 -7.9130015 -7.8523355 -8.0056334 -8.1917639 -7.9239426 -7.5817409 -7.18117][-8.5159445 -8.1827374 -7.9752126 -7.9296985 -7.8510761 -7.4799933 -7.18216 -7.0931988 -7.1644363 -7.2462406 -7.4594407 -7.5763836 -7.4474182 -7.2875624 -6.8477464][-8.2024469 -7.8838696 -7.4477959 -7.1985216 -7.28572 -6.9227948 -6.5699368 -6.9102316 -6.9256678 -7.114007 -7.1052036 -6.8970747 -7.0942173 -7.5570269 -7.56026]]...]
INFO - root - 2017-12-16 00:32:48.754283: step 79510, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 44h:55m:27s remains)
INFO - root - 2017-12-16 00:32:55.185916: step 79520, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 44h:56m:51s remains)
INFO - root - 2017-12-16 00:33:01.722562: step 79530, loss = 0.30, batch loss = 0.19 (11.7 examples/sec; 0.683 sec/batch; 47h:57m:59s remains)
INFO - root - 2017-12-16 00:33:08.277494: step 79540, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 44h:42m:57s remains)
INFO - root - 2017-12-16 00:33:14.724052: step 79550, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 44h:55m:26s remains)
INFO - root - 2017-12-16 00:33:21.146617: step 79560, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 44h:18m:06s remains)
INFO - root - 2017-12-16 00:33:27.589623: step 79570, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 45h:33m:42s remains)
INFO - root - 2017-12-16 00:33:34.034856: step 79580, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 44h:45m:13s remains)
INFO - root - 2017-12-16 00:33:40.411703: step 79590, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:36m:43s remains)
INFO - root - 2017-12-16 00:33:46.793671: step 79600, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 45h:32m:55s remains)
2017-12-16 00:33:47.366755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4471359 -4.3573952 -5.477407 -5.6746845 -4.7811613 -4.0377407 -3.5252714 -2.5512791 -2.8799486 -3.6748295 -6.0538416 -4.4705162 -5.7662134 -5.6466303 -4.9998903][-5.8731451 -4.7608047 -3.4932623 -3.7922511 -3.8755624 -4.1900249 -4.9354539 -4.4076004 -2.4992561 -2.1718702 -4.6114306 -4.74284 -6.8490911 -5.767334 -5.0522604][-3.6230178 -4.0011187 -3.5298572 -2.4388051 -1.9619532 -1.5275917 -0.19072533 -0.29998159 -2.5245047 -3.5210695 -4.0564575 -2.8509684 -5.1325951 -5.69129 -5.0580783][-1.8408165 -1.0273995 -2.31187 -2.6525393 -1.6289091 -0.32050991 -0.1281395 -0.15425682 1.0634604 -0.15925884 -3.4682646 -3.7972107 -5.0278096 -4.7398496 -4.1202316][-4.9032068 -2.1305513 0.25676584 -0.061232567 -0.7369833 -0.88236332 0.04633522 -0.62486792 -0.77546787 -1.3549018 -2.6681252 -2.95474 -5.1147747 -5.3848944 -4.74122][-2.9672279 -2.9403729 -3.0099754 -1.3663945 2.0560341 3.2942667 3.4451332 1.8066788 0.90987873 -0.44545269 -2.461122 -2.8949943 -4.643887 -4.8471355 -5.009995][-2.4578228 -1.1429868 0.018413067 0.31031132 0.86598969 2.5304308 4.343751 4.5729628 3.934413 1.5081205 -1.7978702 -2.1911116 -4.2575159 -4.8255157 -4.0978212][-1.1228638 -1.6144795 -0.98837852 2.0595713 3.2358589 3.3770733 3.3561258 3.2200136 4.1143236 2.8161469 -0.93695688 -1.6504884 -4.1649694 -5.0089636 -5.1407986][-1.19942 -0.52991962 -0.026255608 1.0845747 3.1323156 4.3826227 3.833499 3.360239 3.455184 2.2968998 -0.0060544014 -0.21898746 -3.3319716 -4.3754311 -4.8116994][-3.4517736 -2.7417459 -1.4903817 -0.46138334 0.3523531 1.7151241 2.3786955 2.8227129 3.441452 2.0897436 -0.20784521 -0.53720427 -2.9916739 -3.9539673 -4.6111307][-5.56791 -4.7608948 -3.8618731 -3.7289839 -2.7454348 -2.0468888 -0.9682889 -0.93354177 -0.48871851 -0.14313221 -1.7758064 -1.9791985 -3.6480193 -4.7091675 -5.0823431][-6.8437228 -6.6983681 -6.17172 -5.7471352 -5.2581921 -4.9330449 -4.1503687 -3.6838346 -3.3973565 -2.9746327 -4.1190882 -4.3757038 -6.2150073 -6.2219162 -6.2374964][-6.0793395 -7.4046082 -7.7486043 -8.0342522 -6.8587718 -5.1601419 -4.8040056 -4.7231741 -4.5813155 -3.6609726 -4.9856148 -4.875874 -5.2092495 -5.6980505 -6.3583379][-6.1648121 -5.9301472 -5.9514828 -6.9183779 -7.4451013 -6.8586559 -5.6370392 -5.255878 -5.4489703 -5.9172435 -5.8975692 -5.31157 -6.020031 -6.1070623 -6.084198][-4.767 -5.7083321 -5.4397092 -5.589376 -5.4202018 -6.2071896 -6.719408 -6.1066227 -5.9115496 -6.0815086 -6.3015676 -6.6118741 -7.146801 -6.8964334 -6.5448608]]...]
INFO - root - 2017-12-16 00:33:53.724033: step 79610, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 45h:06m:35s remains)
INFO - root - 2017-12-16 00:34:00.202315: step 79620, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 46h:40m:51s remains)
INFO - root - 2017-12-16 00:34:06.617214: step 79630, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 43h:57m:51s remains)
INFO - root - 2017-12-16 00:34:12.975708: step 79640, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 44h:31m:24s remains)
INFO - root - 2017-12-16 00:34:19.432370: step 79650, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 45h:39m:33s remains)
INFO - root - 2017-12-16 00:34:25.867194: step 79660, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 44h:21m:15s remains)
INFO - root - 2017-12-16 00:34:32.224502: step 79670, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:32m:36s remains)
INFO - root - 2017-12-16 00:34:38.713071: step 79680, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 45h:19m:19s remains)
INFO - root - 2017-12-16 00:34:45.179764: step 79690, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 43h:52m:16s remains)
INFO - root - 2017-12-16 00:34:51.543726: step 79700, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 44h:25m:56s remains)
2017-12-16 00:34:52.091189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6791058 -4.9898748 -5.1268578 -6.0333123 -6.6602058 -7.1317592 -6.3362632 -5.4509754 -4.5572224 -4.8862634 -5.5784731 -5.9354229 -7.2469425 -7.3308959 -7.57174][-4.1129813 -4.2964783 -4.6844697 -5.5693665 -5.9656067 -6.77864 -6.0325375 -5.4049139 -4.557683 -4.6312523 -5.2626171 -6.4144597 -7.3295135 -7.4604816 -8.1644][-3.0610962 -3.2357707 -4.7514029 -5.9885049 -6.1401348 -5.7114224 -5.1006255 -4.7059336 -3.4401002 -3.9238043 -4.5125904 -5.6446295 -7.8252678 -8.52077 -8.2634935][-2.7382674 -2.9168758 -4.7049518 -4.6455889 -4.4629774 -4.3592291 -3.3638964 -2.3321505 -1.8169084 -2.8009214 -3.512383 -4.3762808 -6.4371309 -7.439321 -7.8999004][-3.1562209 -3.5518327 -3.8665938 -4.3999033 -4.1956663 -2.554409 -0.63245106 0.84962273 0.56872368 -1.2636466 -3.1165843 -4.0921726 -6.80847 -7.5019741 -8.0616884][-5.0519943 -4.2170649 -3.7107067 -3.696727 -2.3887467 -1.5511293 -0.049248219 1.9631939 3.2161379 1.5602179 -1.7474637 -3.901943 -7.6079631 -8.1682568 -8.3942966][-4.3397169 -5.164587 -3.9461119 -2.9609737 -0.4255867 1.9780388 3.8100443 3.8693762 4.100523 3.14682 0.35210133 -3.116786 -7.3008027 -7.6567659 -8.16114][-1.5005965 -1.3477345 -2.4328895 -2.2595534 -0.53021812 1.2521219 4.6998224 5.570076 5.4173393 2.731719 0.050558567 -2.5291514 -6.5062742 -7.9168353 -8.9699888][-2.7958446 -1.4128065 -0.91377544 -0.89027834 -0.64311695 -0.023070335 1.7385798 3.1055765 4.0218878 1.155323 -0.76832867 -3.5309653 -7.347827 -7.9094582 -8.151969][-1.0246696 -2.475749 -2.2629848 -1.6111231 -1.3745332 -1.288456 -0.69927931 -0.13481903 0.60976124 -0.682024 -2.7123299 -5.2745047 -8.1172523 -9.505579 -9.04795][-1.5073771 -1.8494549 -2.5307674 -4.0624437 -3.9564455 -3.1739864 -2.3779345 -2.4113326 -2.3464174 -3.9820051 -4.5707226 -5.6732473 -8.42792 -9.6574392 -9.8323727][-1.8655567 -2.8602295 -3.672173 -4.7173824 -5.5073347 -5.2441349 -4.2220826 -3.9359677 -4.0347948 -5.1022387 -5.7189713 -7.4967275 -9.7387848 -9.8126431 -9.9310894][-1.506228 -2.4547462 -3.7331278 -4.2170448 -4.6890717 -5.7840815 -6.4594812 -6.5550294 -6.311697 -6.5140867 -7.2863197 -8.2802057 -10.083989 -10.676559 -10.878045][-1.3218818 -2.730865 -3.6973484 -3.9750245 -5.0758486 -4.9092522 -4.6575689 -5.8477449 -7.1580296 -7.7044587 -8.3588991 -8.5970764 -9.0358391 -10.08643 -10.297944][-4.0325012 -4.1859245 -5.5426679 -5.7713122 -5.9064407 -6.7888694 -6.8479466 -5.6060724 -5.606719 -6.7055464 -8.3359776 -8.6841478 -8.7965584 -8.5820961 -9.097023]]...]
INFO - root - 2017-12-16 00:34:58.458125: step 79710, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 45h:35m:16s remains)
INFO - root - 2017-12-16 00:35:04.851307: step 79720, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 45h:01m:49s remains)
INFO - root - 2017-12-16 00:35:11.201410: step 79730, loss = 0.28, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 46h:49m:38s remains)
INFO - root - 2017-12-16 00:35:17.686805: step 79740, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 44h:52m:19s remains)
INFO - root - 2017-12-16 00:35:24.004213: step 79750, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 43h:50m:01s remains)
INFO - root - 2017-12-16 00:35:30.476798: step 79760, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 45h:58m:21s remains)
INFO - root - 2017-12-16 00:35:36.953686: step 79770, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 44h:24m:40s remains)
INFO - root - 2017-12-16 00:35:43.376332: step 79780, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 46h:11m:58s remains)
INFO - root - 2017-12-16 00:35:49.880880: step 79790, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.674 sec/batch; 47h:18m:51s remains)
INFO - root - 2017-12-16 00:35:56.397333: step 79800, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.654 sec/batch; 45h:54m:53s remains)
2017-12-16 00:35:56.916880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5423455 -3.5715528 -4.038146 -4.4483967 -4.3816981 -3.9595628 -3.1981664 -2.6429563 -2.1377587 -3.1375527 -4.0846825 -5.1439004 -6.2630367 -6.0301003 -6.6490784][-4.3827305 -4.3927345 -4.7512927 -4.742238 -4.7608204 -4.8672523 -4.3438253 -3.438096 -2.7216215 -3.7491679 -4.5556035 -5.7466869 -6.9311619 -6.6933064 -7.446137][-4.4189482 -4.3898706 -4.8555655 -4.8339396 -4.5978069 -4.6077065 -4.49135 -4.3844128 -3.9118829 -4.7553229 -5.5948625 -6.9645371 -7.997961 -7.5965157 -8.0969572][-4.7125158 -4.0400805 -4.0108042 -3.4449825 -3.219677 -3.5066195 -2.9989653 -2.9025078 -2.6706414 -3.9851987 -5.0630713 -6.7524166 -8.1161976 -7.6995335 -8.1167383][-5.5846004 -4.4830694 -3.5896034 -2.7664218 -2.0502028 -1.3343415 -0.5625248 -0.62661648 -0.71791506 -2.3844228 -3.9706109 -6.1578064 -8.0187435 -7.7239146 -8.2896376][-6.4705944 -5.5098662 -4.9415278 -3.2039132 -1.6014953 -0.68059254 0.4889946 0.46019077 0.65029716 -0.99790144 -2.8978066 -5.3994493 -7.6176062 -7.8129258 -8.6432362][-7.0044403 -5.7698565 -4.1556969 -2.2350116 -0.20048523 1.2434988 2.4171305 2.3987083 2.589077 0.57896996 -1.3859806 -3.8936093 -6.3145623 -6.7583408 -7.6822572][-6.0706625 -4.7713976 -3.5939484 -1.2809577 1.1317415 2.6001692 4.2924557 4.5100393 4.4005089 1.9126034 -0.28462267 -2.8539062 -4.8343062 -5.2381992 -6.5058084][-6.0665884 -5.1638546 -4.2545586 -2.6448689 -0.7861824 0.8882761 2.5854931 3.0764694 3.7727194 1.4787245 -0.80492544 -3.3523169 -5.3847542 -5.3742313 -6.4293389][-7.3122625 -6.2579355 -5.3219423 -3.7529604 -1.8668432 -0.82440186 0.29216146 0.5031786 0.67666245 -1.146769 -3.2266154 -5.4870911 -6.8397722 -6.3799839 -6.9322276][-8.3372374 -8.232255 -7.6429763 -6.2755179 -4.637465 -3.248693 -1.9165812 -1.8698201 -1.8396101 -3.5419884 -5.2509708 -6.5855055 -7.3920016 -7.157444 -7.4847803][-8.5497255 -7.6669312 -7.3022985 -6.9112835 -6.1777391 -5.5027614 -4.6540556 -4.0052204 -3.6381497 -4.846149 -5.7641125 -6.6421585 -7.3701634 -7.2260327 -7.7841883][-9.5129776 -9.0911264 -8.6113548 -7.7232251 -7.0362334 -6.5611887 -5.9702153 -5.9200206 -5.686594 -6.0137734 -6.5759106 -7.0800066 -7.2628489 -6.9090915 -7.0541258][-8.3604012 -8.7833548 -8.22523 -7.3586707 -6.6127687 -6.2129769 -5.6808119 -5.6552162 -5.8449764 -6.2978811 -6.5652227 -6.7902746 -7.1034613 -6.6713886 -6.5860534][-7.958065 -7.986661 -8.3520517 -8.4045238 -7.7699046 -7.3584452 -6.9701829 -7.0900249 -7.0982018 -6.9135938 -6.9888506 -7.1019154 -6.9281693 -6.8692284 -6.8079758]]...]
INFO - root - 2017-12-16 00:36:03.426472: step 79810, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 45h:02m:47s remains)
INFO - root - 2017-12-16 00:36:09.826529: step 79820, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 45h:27m:47s remains)
INFO - root - 2017-12-16 00:36:16.152319: step 79830, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 43h:52m:53s remains)
INFO - root - 2017-12-16 00:36:22.520736: step 79840, loss = 0.23, batch loss = 0.12 (12.9 examples/sec; 0.621 sec/batch; 43h:33m:32s remains)
INFO - root - 2017-12-16 00:36:28.906733: step 79850, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.630 sec/batch; 44h:12m:16s remains)
INFO - root - 2017-12-16 00:36:35.392915: step 79860, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 44h:50m:23s remains)
INFO - root - 2017-12-16 00:36:41.788642: step 79870, loss = 0.29, batch loss = 0.18 (11.7 examples/sec; 0.684 sec/batch; 48h:00m:44s remains)
INFO - root - 2017-12-16 00:36:48.224865: step 79880, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 44h:54m:30s remains)
INFO - root - 2017-12-16 00:36:54.684887: step 79890, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 45h:59m:15s remains)
INFO - root - 2017-12-16 00:37:01.094414: step 79900, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 44h:14m:59s remains)
2017-12-16 00:37:01.701954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4770398 -3.3706632 -3.7086608 -3.7572773 -3.6590738 -3.3385663 -3.1519451 -2.7304711 -2.2852578 -3.813355 -4.6257629 -5.4884939 -6.5472779 -7.0374851 -7.5044737][-2.8516865 -3.1778078 -3.864594 -3.8881235 -3.6017046 -3.1338372 -2.9689865 -2.5874705 -2.1860175 -3.733165 -4.5966845 -5.664793 -6.9294486 -7.5433421 -8.1634426][-2.0556397 -2.0803924 -2.5735955 -2.8110914 -2.5083413 -1.9201918 -1.6331501 -1.4571338 -1.1797676 -2.4304142 -2.8703837 -3.8910975 -5.416503 -6.0011797 -6.724472][-1.9950314 -1.9378066 -1.9987149 -1.7999363 -1.2752714 -0.74673128 -0.23659754 -0.13483191 -0.036018372 -1.6035709 -2.3405437 -3.396174 -5.0610752 -5.8332162 -6.4257622][-2.755856 -1.9533901 -1.3211055 -0.84808588 -0.12426615 0.52977276 1.0493078 1.1208181 1.2023258 -0.69683743 -1.6728115 -2.7513075 -4.4736648 -5.4806118 -6.2861066][-3.5246444 -2.5847421 -1.5671835 -0.71626949 0.17743063 1.0296955 1.7247686 1.6675434 1.5846233 -0.4907012 -1.6134996 -2.7367287 -4.5695734 -5.5060205 -6.2104931][-4.159359 -3.1877227 -2.0611668 -0.72558165 0.58991909 1.5268192 2.1363802 2.1464109 2.0099373 -0.42729092 -1.8446741 -3.0279775 -4.8379564 -5.8329434 -6.480628][-3.9587052 -2.9856625 -1.7153993 -0.30209541 0.95977116 1.6442747 2.1718779 2.1392593 1.9827042 -0.46878958 -2.080297 -3.5676417 -5.3528943 -6.2673087 -6.8931346][-4.2570214 -3.518918 -2.4355345 -0.963501 0.27662897 0.83818626 1.2882671 1.5555487 1.6293869 -0.89511919 -2.4597564 -3.8335488 -5.7643566 -6.7743669 -7.3672709][-4.4994183 -3.7368603 -2.9712629 -1.6234598 -0.49142551 -0.12064266 0.26444721 0.58902264 0.67717075 -1.9361835 -3.3471279 -4.7387609 -6.3678465 -7.3936467 -8.0817394][-5.484921 -4.8563781 -4.1291838 -3.2345414 -2.4559941 -2.1275883 -1.8779955 -1.6603999 -1.4708352 -3.8136656 -5.1101551 -6.001298 -7.2992158 -8.0740814 -8.4839144][-5.4819956 -4.9055882 -4.4375277 -3.9042022 -3.471334 -3.412951 -3.3599644 -3.2678547 -3.1357646 -4.703721 -5.5954146 -6.336657 -7.1371655 -7.7950072 -8.17953][-5.8084612 -5.3531952 -5.0355406 -4.708724 -4.44788 -4.4422951 -4.5274754 -4.6056595 -4.5655422 -5.6486044 -6.2514496 -6.4296355 -6.9552212 -7.3359628 -7.2674356][-5.5980039 -5.3369131 -5.1658745 -4.7161913 -4.5829115 -4.7477551 -4.8618197 -4.9240179 -4.9205532 -5.6179328 -5.8863792 -5.8753514 -6.0168905 -6.3586016 -6.4539108][-7.0068159 -6.7735214 -6.3149481 -6.0303559 -5.8896151 -6.04455 -6.2740812 -6.4571967 -6.5113769 -6.3590097 -6.3465366 -6.33286 -6.1562023 -6.038888 -6.0000238]]...]
INFO - root - 2017-12-16 00:37:08.279193: step 79910, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 46h:27m:21s remains)
INFO - root - 2017-12-16 00:37:14.687157: step 79920, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:31m:13s remains)
INFO - root - 2017-12-16 00:37:21.095650: step 79930, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 45h:25m:30s remains)
INFO - root - 2017-12-16 00:37:27.486150: step 79940, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 44h:47m:17s remains)
INFO - root - 2017-12-16 00:37:33.915543: step 79950, loss = 0.24, batch loss = 0.12 (12.7 examples/sec; 0.631 sec/batch; 44h:17m:46s remains)
INFO - root - 2017-12-16 00:37:40.308684: step 79960, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 44h:36m:03s remains)
INFO - root - 2017-12-16 00:37:46.738936: step 79970, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 46h:11m:44s remains)
INFO - root - 2017-12-16 00:37:53.122896: step 79980, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 45h:10m:50s remains)
INFO - root - 2017-12-16 00:37:59.499399: step 79990, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.632 sec/batch; 44h:21m:41s remains)
INFO - root - 2017-12-16 00:38:05.985190: step 80000, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 44h:34m:32s remains)
2017-12-16 00:38:06.542605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2328572 -5.4239912 -4.1343889 -3.4137511 -3.4563618 -2.9066873 -2.7835751 -2.8624916 -2.7709646 -4.5031796 -5.4253716 -5.9284458 -5.814836 -7.40976 -6.744709][-5.84863 -4.7135878 -3.8074574 -3.4405575 -3.0999427 -2.4416103 -2.7527862 -2.8637033 -3.1413217 -4.4048185 -4.7088661 -5.404201 -5.1387115 -6.6750636 -6.442801][-6.1567364 -5.3409905 -4.2726336 -2.9676471 -2.4790835 -2.3220925 -2.6912456 -2.3768153 -2.606576 -4.0974393 -4.7170076 -5.2663422 -4.718708 -6.2025671 -5.9216523][-4.4260015 -4.2109022 -3.877882 -3.3522382 -2.4973831 -1.8616462 -1.5989327 -1.5085711 -1.4889088 -3.15313 -3.7006552 -4.5526958 -4.6794596 -5.7718382 -5.0168209][-3.9482524 -3.3774939 -2.4487734 -1.6926994 -1.7505217 -1.4510264 -1.3045125 -1.1512322 -0.99579859 -2.1422477 -2.833631 -3.3824482 -3.4297237 -4.8181019 -4.9491415][-2.1151929 -1.7315173 -1.5860982 -0.89656925 -0.43492365 -0.20180464 0.046663284 0.0028448105 -0.472569 -2.1357284 -2.7283149 -3.3567686 -3.5324578 -5.4728546 -5.2827425][-0.18511105 -0.35616779 0.13952589 0.30829716 0.86727524 1.1860714 0.83655357 0.50617886 -0.072501659 -2.2484074 -3.4463701 -4.3935542 -4.4839683 -5.5890751 -5.0333986][1.16323 1.934823 1.735899 1.5375891 2.0024433 2.2626476 2.0820522 1.6506128 0.73786831 -1.5644851 -2.8046365 -3.9560723 -4.5491714 -6.4600163 -5.9041042][1.3575792 1.9103727 2.0540657 1.9353743 1.985734 2.0333452 2.1797066 1.7592478 1.490262 -0.60211515 -2.7290163 -4.5390015 -4.6546259 -6.184618 -5.8966918][-0.25802612 0.25558996 1.170433 1.2855587 1.7494974 1.3905888 1.6419439 1.6344461 1.2953596 -1.0671391 -2.3022852 -4.1914172 -5.1539183 -6.645895 -6.1729097][-1.3673048 -0.83026981 -0.059588909 -0.15453529 0.27418566 0.88489914 1.0433483 0.4079113 0.022132874 -1.7127042 -2.6793313 -4.3033752 -5.0053349 -6.5512738 -6.9131231][-3.5587826 -2.6663184 -1.7432528 -1.2441659 -0.88904762 -0.28608561 -0.1352005 0.0240345 -0.42759848 -2.729589 -3.1722846 -4.1251369 -5.2082849 -7.2127147 -8.0032835][-4.6611443 -3.9149189 -3.6352568 -3.3178062 -2.7989244 -2.1106577 -1.5515537 -1.1297035 -1.1124754 -2.230001 -3.5783534 -5.1798334 -6.4765821 -7.8748603 -8.0761738][-5.5480046 -4.6647477 -4.0125113 -3.6064935 -3.25815 -3.0695028 -2.5752344 -2.0222831 -2.2382212 -3.3594007 -4.328886 -5.4172449 -6.5957561 -7.7457871 -7.84172][-6.0429959 -5.8501763 -5.1001024 -4.8016262 -4.6475887 -3.9204171 -3.8953364 -3.8024395 -4.0055413 -4.2448196 -4.79496 -5.7586474 -6.4775691 -7.2280617 -7.8916583]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 00:38:13.956318: step 80010, loss = 0.32, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 43h:38m:22s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:38:20.318506: step 80020, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 44h:04m:14s remains)
INFO - root - 2017-12-16 00:38:26.775737: step 80030, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 45h:22m:27s remains)
INFO - root - 2017-12-16 00:38:33.241557: step 80040, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 45h:06m:04s remains)
INFO - root - 2017-12-16 00:38:39.641843: step 80050, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 45h:51m:31s remains)
INFO - root - 2017-12-16 00:38:46.226593: step 80060, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 47h:25m:37s remains)
INFO - root - 2017-12-16 00:38:52.696838: step 80070, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 45h:29m:43s remains)
INFO - root - 2017-12-16 00:38:59.056539: step 80080, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 44h:39m:59s remains)
INFO - root - 2017-12-16 00:39:05.458343: step 80090, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 44h:53m:52s remains)
INFO - root - 2017-12-16 00:39:11.875525: step 80100, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 43h:35m:34s remains)
2017-12-16 00:39:12.458380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0737066 -4.4570112 -5.0017653 -5.7965603 -6.4989777 -5.8348589 -5.1555624 -4.1692109 -3.5968127 -5.1891766 -6.3341885 -8.8447447 -10.52479 -11.469397 -11.949753][-5.3086581 -7.2559104 -8.2918386 -8.4547243 -8.5721159 -7.6522 -6.3303828 -4.8609338 -3.29633 -4.7695179 -6.5859718 -9.2245331 -10.453816 -11.425858 -11.831806][-4.2453623 -5.6821518 -7.3097892 -8.1345959 -8.055994 -6.8116379 -4.9699364 -3.4028711 -1.9135499 -3.1392236 -4.6811018 -7.7255383 -9.1950054 -10.575319 -10.760361][-4.5410643 -5.2529435 -5.9195538 -6.2915154 -6.2599545 -5.1809139 -3.1280751 -1.354454 -0.22571611 -1.5219574 -3.0515928 -5.9810977 -7.964118 -9.2216015 -9.7536163][-4.6164165 -4.1510978 -4.2144308 -4.6033697 -4.3223028 -3.0643787 -1.6746445 0.0647912 1.2021761 -0.016820908 -1.8234568 -4.6818323 -6.8264947 -8.6100283 -9.440732][-3.366116 -3.6042323 -3.9388428 -3.0705361 -2.0132275 -0.52695704 0.92185116 2.0335913 2.3450642 0.77741146 -1.6374326 -5.0551219 -7.4243193 -8.4621868 -9.1081448][-2.8387504 -2.3659568 -2.4317288 -1.8295193 -0.99490595 0.62842178 2.0457478 2.8529625 3.2769241 1.3187027 -1.6265798 -5.4910769 -8.0806217 -9.783186 -10.256097][-3.0944109 -2.8042121 -2.5185099 -1.314836 -0.28975868 0.90613651 2.0993643 2.4272432 2.7445621 0.97343922 -1.5748968 -5.3732924 -7.7731767 -9.3499365 -10.296189][-2.3709412 -1.9942384 -1.6770506 -0.96072817 -0.30771255 0.5999155 1.5902023 1.7228565 1.533288 -0.58429193 -2.8325334 -5.6968279 -7.4006577 -8.8241043 -9.43602][-2.7813125 -2.6364717 -2.5538225 -1.9823661 -1.4670439 -0.41973829 0.24179029 0.067282677 -0.063949108 -2.4070463 -4.5214367 -7.4277849 -8.8728228 -9.2928982 -9.3212481][-4.2792826 -4.5475559 -4.628006 -3.9249871 -3.5407672 -2.7595954 -1.8134298 -1.684855 -2.0571775 -4.2872162 -5.8612547 -7.6695304 -8.4019165 -8.7787476 -9.1133261][-5.4200869 -5.7804317 -6.2957258 -6.0635366 -5.7078977 -5.0084743 -4.4609461 -4.2069206 -3.9706285 -5.4375539 -6.5096874 -7.711113 -8.1331825 -8.1107 -8.1271868][-5.680881 -6.4848943 -7.2235985 -7.1346321 -6.8078537 -6.3133993 -5.6944566 -5.7600126 -6.0783143 -6.6843 -6.8744259 -7.4665275 -7.9172297 -7.6938639 -7.3442383][-6.4773245 -6.8328266 -7.045742 -6.7784634 -6.3984118 -6.1002188 -6.132184 -6.0607948 -5.903758 -6.3123226 -6.7174611 -7.0565944 -6.8777714 -6.5221734 -6.7792425][-7.2696648 -7.5462742 -7.75931 -8.0623779 -8.2001877 -7.7055659 -7.3826571 -7.7276478 -7.9986324 -7.874249 -7.4056106 -7.059833 -6.7387457 -6.4792385 -6.7002034]]...]
INFO - root - 2017-12-16 00:39:19.050576: step 80110, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:44m:19s remains)
INFO - root - 2017-12-16 00:39:25.442157: step 80120, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 44h:49m:08s remains)
INFO - root - 2017-12-16 00:39:31.791246: step 80130, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 44h:32m:19s remains)
INFO - root - 2017-12-16 00:39:38.306938: step 80140, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 43h:50m:10s remains)
INFO - root - 2017-12-16 00:39:44.659776: step 80150, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 44h:30m:29s remains)
INFO - root - 2017-12-16 00:39:51.058665: step 80160, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.617 sec/batch; 43h:16m:14s remains)
INFO - root - 2017-12-16 00:39:57.527964: step 80170, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 44h:55m:41s remains)
INFO - root - 2017-12-16 00:40:03.936506: step 80180, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 44h:35m:29s remains)
INFO - root - 2017-12-16 00:40:10.352411: step 80190, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 45h:10m:33s remains)
INFO - root - 2017-12-16 00:40:16.864866: step 80200, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 46h:04m:53s remains)
2017-12-16 00:40:17.439222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5448227 -4.8246913 -4.6392336 -4.6202893 -4.7629366 -4.7797956 -4.6552858 -4.3439932 -3.9077036 -3.9562945 -5.2144175 -7.0935993 -8.4786806 -9.4185944 -10.401622][-4.0259352 -4.1290274 -4.2209673 -5.1139908 -5.7880821 -6.20256 -6.3117523 -5.7930741 -5.3597541 -5.3678417 -6.1947303 -8.0084667 -9.5690794 -10.571421 -11.565382][-4.0535126 -3.5632782 -3.8410814 -4.5386143 -4.8367038 -5.440568 -5.81447 -5.4143867 -4.96165 -4.5128431 -4.9856524 -6.2751026 -7.6902061 -8.8300838 -9.7175388][-4.3992453 -3.1623425 -3.1587129 -3.3406863 -3.6647472 -3.7688224 -3.881037 -3.8592482 -3.6014986 -2.8892245 -3.4398379 -4.5354571 -5.895781 -7.0317063 -7.98253][-4.5552177 -3.3205423 -2.6382728 -2.2747679 -2.2002034 -1.8678408 -1.4658895 -1.652987 -1.6432366 -1.4689031 -2.2191768 -3.3895726 -4.4799347 -5.5105877 -6.1046715][-3.3232846 -2.7733111 -2.5374475 -2.1624637 -1.6380048 -0.63891792 0.25729465 0.32922935 0.41704369 0.3047657 -0.62811852 -2.2581711 -3.4759812 -4.5428286 -5.3347836][-3.3670235 -2.8321185 -2.1046538 -1.2032738 -0.24913597 0.85935307 1.5447617 1.4379425 1.257185 0.58019733 -0.98772812 -2.8980107 -4.2754331 -5.2677069 -5.9839954][-3.4926457 -2.9099998 -2.1093369 -0.63712025 0.61236477 1.7432871 2.5626221 2.4392624 1.9659185 0.7790966 -1.1019883 -3.5155206 -5.07767 -6.0953255 -6.491487][-3.8018384 -2.8828297 -1.7567787 -0.75702333 0.37202835 1.5675373 2.2053843 1.9720812 1.6194515 0.84069824 -0.95251846 -3.4666743 -5.0424643 -5.9567075 -6.118433][-3.6304908 -3.4435225 -2.6963959 -2.243166 -1.6880732 -0.51824331 0.43940163 0.20606518 0.062412262 -0.54456806 -2.108511 -3.8373492 -4.9778223 -6.0749764 -6.709322][-4.9835539 -4.9486032 -4.7490311 -4.3407817 -3.8698902 -3.2921343 -2.7048292 -2.6239843 -2.7483034 -3.2533197 -4.2154741 -5.2481031 -6.055479 -6.9578595 -7.4829922][-6.1860189 -5.7634554 -5.4061351 -5.0617752 -4.8132157 -4.6634369 -4.5813055 -4.5407352 -4.4542 -5.4573822 -6.7484727 -7.1278887 -7.8321495 -8.536375 -8.7157173][-7.6816483 -7.4765067 -6.8110557 -6.1122308 -5.58957 -5.2657957 -5.3276644 -5.8960938 -6.1096616 -7.0900216 -8.5520477 -8.4409046 -8.3353729 -7.9497685 -7.72649][-6.3396306 -5.9615393 -5.781116 -5.1659269 -4.7680578 -4.1874943 -3.9998732 -4.5391169 -5.2094612 -5.9778519 -6.806654 -7.0962558 -7.3332543 -6.8580589 -6.3585925][-5.0609326 -4.4343405 -4.690104 -4.7583122 -4.56993 -4.1794748 -4.0266857 -4.0352559 -4.2947955 -4.7078133 -5.0989542 -5.5952005 -6.1119308 -6.47433 -6.1156392]]...]
INFO - root - 2017-12-16 00:40:23.948872: step 80210, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 45h:03m:34s remains)
INFO - root - 2017-12-16 00:40:30.343531: step 80220, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 43h:59m:49s remains)
INFO - root - 2017-12-16 00:40:36.702481: step 80230, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 45h:57m:42s remains)
INFO - root - 2017-12-16 00:40:43.129370: step 80240, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 46h:11m:59s remains)
INFO - root - 2017-12-16 00:40:49.630054: step 80250, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 45h:19m:57s remains)
INFO - root - 2017-12-16 00:40:55.983870: step 80260, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 45h:09m:06s remains)
INFO - root - 2017-12-16 00:41:02.321066: step 80270, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 44h:38m:32s remains)
INFO - root - 2017-12-16 00:41:08.737692: step 80280, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 44h:37m:54s remains)
INFO - root - 2017-12-16 00:41:15.156409: step 80290, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 45h:15m:40s remains)
INFO - root - 2017-12-16 00:41:21.479352: step 80300, loss = 0.27, batch loss = 0.16 (13.1 examples/sec; 0.612 sec/batch; 42h:54m:22s remains)
2017-12-16 00:41:22.004951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7141979 -3.2945209 -2.6651449 -3.0312176 -2.9045782 -2.1226816 -2.0227504 -1.465251 -0.75040817 -1.1881952 -2.2068667 -3.1956911 -4.4884577 -6.3663216 -8.0299091][-2.8536901 -2.3422265 -2.5420098 -3.2576733 -3.5388846 -3.3083763 -3.0643106 -2.4205732 -1.5570283 -1.3995814 -2.1845212 -3.1260877 -4.424159 -6.0243406 -7.6238422][-2.0844636 -1.1120095 -1.767415 -2.5813007 -2.8326445 -2.4247479 -2.6171346 -2.0225916 -0.98144722 -1.2982812 -1.912405 -2.9643488 -3.890873 -5.3635941 -6.855773][-1.8749299 -1.2250009 -1.1063304 -1.5928054 -1.4772034 -1.064548 -0.94157076 -0.66946983 0.16255617 -0.39193058 -1.1410322 -2.2346592 -3.1807103 -5.1027932 -6.2466717][-2.6969209 -1.116147 -0.70972252 -0.76877451 -0.79848051 -0.57531309 -0.023292065 0.4326725 0.87646866 0.079279423 -0.75496864 -1.6770205 -2.696084 -4.5577564 -5.4914203][-3.0448146 -1.7719989 -1.3612661 -0.90261316 -0.62170124 0.21044636 0.91355515 1.0073824 1.361002 0.36984444 -0.89723539 -2.1819835 -3.2393861 -4.7426176 -5.5790491][-4.0078478 -2.9040275 -1.9221134 -1.222857 -0.78034544 0.2340517 1.3975296 1.7772303 1.8199749 0.65634918 -0.64888477 -2.324996 -3.6837435 -5.3426466 -6.2264619][-4.1578374 -3.1375809 -2.2027583 -1.30692 -0.47606564 0.30329895 1.3078527 1.7192106 1.9982901 0.65942383 -0.58587074 -2.0556693 -3.4549112 -5.1885777 -6.0046091][-4.3263283 -3.2071075 -2.4345374 -1.6035686 -0.69512796 0.23802662 1.2956629 1.5335407 1.5249805 0.10987806 -1.3032207 -2.6263447 -3.9146941 -5.4573488 -6.0470853][-4.2824 -3.1570129 -2.3765683 -1.7059894 -0.73908091 0.33446789 1.317915 1.4280519 1.3623781 0.008893013 -1.2873006 -2.8259759 -3.7752252 -4.9458256 -5.4242039][-5.3660975 -4.4095254 -3.4094243 -2.7042446 -1.9484591 -0.92801142 -0.063279152 0.10745478 0.1069808 -0.99802923 -1.9791808 -2.8579946 -3.4638138 -4.1479025 -4.4574113][-6.3448133 -5.1582594 -4.2900052 -3.7221138 -3.3319612 -2.4388576 -1.5039277 -1.467123 -1.7659597 -2.6527777 -3.2665968 -3.4399881 -3.93148 -4.3963394 -4.5469341][-6.5927792 -5.8942084 -5.4438934 -5.1818295 -4.6614408 -3.6458912 -2.6099977 -2.7171063 -2.7690244 -3.4860272 -4.0754919 -4.1495895 -4.3914075 -4.6685867 -5.096446][-6.8568597 -6.7338872 -6.3083229 -6.3205781 -6.140954 -5.429575 -4.4312091 -4.0381904 -3.7691855 -4.0751982 -3.9418955 -4.2372351 -4.4533477 -4.6423335 -4.7196198][-8.079484 -7.7427931 -7.2293243 -7.4515114 -7.4186254 -7.1015644 -6.4943752 -6.10699 -6.1266556 -5.6561632 -5.3566246 -5.3938875 -5.500782 -5.5481853 -5.53965]]...]
INFO - root - 2017-12-16 00:41:28.365918: step 80310, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 44h:37m:46s remains)
INFO - root - 2017-12-16 00:41:34.751482: step 80320, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 43h:59m:33s remains)
INFO - root - 2017-12-16 00:41:41.152952: step 80330, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 45h:56m:12s remains)
INFO - root - 2017-12-16 00:41:47.603413: step 80340, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 46h:27m:37s remains)
INFO - root - 2017-12-16 00:41:54.089086: step 80350, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 45h:12m:58s remains)
INFO - root - 2017-12-16 00:42:00.472201: step 80360, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 45h:20m:57s remains)
INFO - root - 2017-12-16 00:42:06.952917: step 80370, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 44h:50m:40s remains)
INFO - root - 2017-12-16 00:42:13.423811: step 80380, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 45h:15m:10s remains)
INFO - root - 2017-12-16 00:42:19.959492: step 80390, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 44h:43m:04s remains)
INFO - root - 2017-12-16 00:42:26.349335: step 80400, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.644 sec/batch; 45h:04m:43s remains)
2017-12-16 00:42:26.861562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8391829 -1.6555643 -1.3370318 -1.9016604 -2.1124706 -2.4391708 -2.5523582 -2.6182618 -2.0724773 -2.131732 -2.4899516 -3.7682822 -4.9337063 -6.7385359 -7.7728963][-2.1447129 -2.1022806 -2.7959747 -2.953301 -3.5583987 -3.6092391 -3.3589416 -3.0956111 -2.6972017 -2.638536 -2.8765144 -4.2421236 -5.3916855 -6.2526536 -6.8743525][-2.1609521 -2.2844305 -2.8385253 -3.3446279 -3.9052866 -4.0946093 -3.89191 -3.9625947 -3.5983853 -3.7426417 -4.124259 -4.9590216 -5.587316 -6.4925222 -6.900486][-1.2440381 -1.54213 -2.7112422 -3.168077 -3.4007907 -3.226717 -2.8511033 -2.8244939 -2.6542468 -3.0849257 -3.5565224 -4.9905267 -5.9143906 -6.8011818 -7.1818428][-1.9956818 -2.1881924 -2.1105614 -2.2208204 -2.2050204 -1.8701372 -1.3743329 -1.0184126 -0.66973114 -1.2536941 -2.2882481 -4.1769638 -5.4692512 -6.3751454 -7.1619515][-2.7448316 -2.5551348 -2.1751242 -1.404964 -0.51365423 -0.011023045 0.59920692 0.80835819 0.95285606 0.21063709 -0.74001694 -2.6906629 -4.1864858 -5.5394115 -6.6241655][-4.158884 -3.64885 -2.5898027 -1.5435972 -0.27303505 0.79024887 1.7769651 2.1746044 2.4214134 1.5039644 0.24676466 -2.2654576 -4.2071147 -5.5785718 -6.7940989][-4.0465727 -3.2461 -2.2657967 -0.90799713 0.59786034 1.7386713 2.6695871 3.0640697 3.1428518 2.0231524 0.53789806 -1.8511009 -3.7965088 -5.4800434 -6.5415764][-3.3549609 -2.5786972 -1.8908134 -0.56559181 0.75349808 2.0869856 2.9190636 2.7796288 2.5569429 1.3637495 -0.018847466 -2.32226 -4.1468043 -5.5470204 -6.4740715][-3.3025579 -2.4928155 -2.1672773 -1.1262102 -0.12206507 1.0292921 1.8911972 1.8710871 1.3706064 0.044338226 -1.6848702 -4.0318451 -5.6777725 -6.3528404 -6.9245276][-5.7319179 -4.8268147 -4.0552921 -2.8863659 -2.0017614 -1.069252 -0.33614731 -0.50930738 -0.56728506 -2.2527294 -4.0025492 -5.5736861 -6.5715122 -6.8504114 -7.3141055][-5.3505898 -5.2419052 -4.7166958 -3.822602 -2.9817843 -1.9676242 -1.4964304 -1.6022491 -1.9100652 -2.9076414 -4.1424379 -5.8049469 -6.99585 -7.1293669 -7.5848794][-6.1218662 -6.0442142 -5.9211683 -5.2773967 -4.3500443 -3.7014034 -3.1346455 -3.2048106 -3.350708 -4.2282658 -5.1965556 -5.9475489 -6.4902191 -6.5863738 -6.8259749][-6.2860956 -6.1959143 -6.097312 -5.3931589 -4.6418061 -3.9837399 -3.2853427 -3.2842283 -3.5650187 -4.3416662 -4.9697752 -5.4279566 -5.6724825 -6.1533165 -6.3768158][-8.1498623 -7.6175761 -6.7390585 -5.8656244 -5.451941 -5.1712627 -4.7240419 -5.2192812 -5.7918673 -5.8103952 -5.7225628 -5.8593912 -6.1312165 -6.1032729 -6.1609683]]...]
INFO - root - 2017-12-16 00:42:33.322113: step 80410, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 45h:01m:31s remains)
INFO - root - 2017-12-16 00:42:39.763153: step 80420, loss = 0.32, batch loss = 0.21 (12.1 examples/sec; 0.659 sec/batch; 46h:10m:26s remains)
INFO - root - 2017-12-16 00:42:46.257131: step 80430, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 44h:16m:20s remains)
INFO - root - 2017-12-16 00:42:52.652136: step 80440, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.628 sec/batch; 43h:58m:10s remains)
INFO - root - 2017-12-16 00:42:59.071093: step 80450, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 45h:23m:18s remains)
INFO - root - 2017-12-16 00:43:05.433526: step 80460, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:27m:18s remains)
INFO - root - 2017-12-16 00:43:11.836790: step 80470, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 44h:51m:02s remains)
INFO - root - 2017-12-16 00:43:18.203031: step 80480, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.619 sec/batch; 43h:19m:56s remains)
INFO - root - 2017-12-16 00:43:24.697531: step 80490, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 45h:43m:08s remains)
INFO - root - 2017-12-16 00:43:31.263156: step 80500, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 46h:42m:21s remains)
2017-12-16 00:43:31.805896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8486843 -7.0892391 -6.53284 -6.5586028 -5.6512461 -4.6778011 -4.0724783 -2.7665882 -2.443583 -4.3115091 -4.7426515 -5.1649108 -5.9390507 -6.1788626 -7.0016971][-6.3052626 -6.1324844 -5.49903 -5.5844684 -5.690321 -5.3034525 -4.7170343 -4.511395 -4.4886088 -5.9850245 -6.2659988 -6.5615563 -6.5285492 -6.7729397 -7.6140513][-4.219594 -4.4934559 -4.51604 -4.7753448 -5.2200661 -5.3053927 -5.1330614 -5.4252014 -5.5358496 -7.2865348 -7.4518194 -8.2161379 -8.30278 -8.0388489 -8.14577][-4.3134232 -3.7866459 -3.3295956 -3.0867558 -2.9378123 -2.9766827 -3.0969772 -3.0812478 -3.8269663 -6.7046061 -7.6856089 -8.4370346 -8.9189405 -8.9110394 -9.3233118][-4.5826483 -3.3003774 -1.9459386 -1.2600079 -1.1033859 -0.39701939 0.077022552 -0.27014589 -1.4634852 -4.2819643 -5.95723 -7.3118277 -8.1648617 -8.7569151 -9.3099976][-3.7273183 -3.1155262 -2.3847423 -0.76759672 0.83515358 1.0540829 1.779747 1.6763973 1.2461319 -1.4864693 -3.6343298 -5.3937058 -6.8377604 -7.8547983 -8.7063808][-3.3589387 -2.7656283 -1.7788377 0.064691544 1.570919 2.7310696 4.0296707 4.0902958 3.8366604 1.035573 -1.0111275 -2.8127489 -5.0293427 -6.5440626 -7.7800269][-2.1943064 -1.5049152 -0.90902328 0.5351181 1.5083103 3.3765917 4.6851559 4.4904604 4.4862432 2.2834253 0.31302643 -1.6154861 -3.9530652 -5.4883342 -7.0338373][-1.6396894 -2.1358485 -2.0347409 -0.835341 0.27353144 0.9923172 2.1220732 2.7914963 3.180934 0.863884 -0.23027515 -1.402185 -3.1381397 -5.0421314 -6.4320579][-2.3808784 -2.4746532 -2.0376868 -1.7716036 -1.2247901 -0.8994813 -0.23124313 -0.12484407 0.59605503 -0.81027126 -1.9624767 -3.1141286 -4.1751614 -4.8394022 -5.8044419][-3.9040043 -3.5685649 -3.1924529 -3.4312115 -2.9733205 -2.9124632 -2.6041808 -2.4138513 -2.0141473 -3.0747895 -4.08842 -5.0576763 -5.9748416 -6.2362313 -6.6800408][-3.9328074 -3.3885217 -3.6582856 -3.5004745 -3.273447 -3.7148237 -3.9891093 -4.3176346 -4.35074 -5.383585 -5.14034 -5.6781087 -6.1214428 -6.7669191 -7.2450495][-6.4627213 -5.5554829 -5.1967869 -5.2754278 -5.5546246 -5.1448421 -4.8758078 -5.4790769 -5.6652 -6.512218 -6.8233352 -6.8157706 -6.2053838 -6.5476222 -6.9584651][-6.1532912 -5.31862 -4.8472452 -4.6777873 -4.7812557 -4.7751884 -5.0162773 -5.2120571 -4.9660454 -5.1603217 -5.9148927 -6.2793732 -6.1464963 -6.2538176 -6.170248][-6.0203676 -5.7748733 -5.2862368 -5.12504 -5.0143609 -5.1796236 -5.4236717 -5.5775089 -5.3263645 -5.1355495 -5.1897531 -5.2984958 -5.3442574 -5.32853 -5.1986322]]...]
INFO - root - 2017-12-16 00:43:38.314734: step 80510, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 46h:18m:50s remains)
INFO - root - 2017-12-16 00:43:44.761769: step 80520, loss = 0.34, batch loss = 0.23 (12.7 examples/sec; 0.631 sec/batch; 44h:11m:34s remains)
INFO - root - 2017-12-16 00:43:51.158265: step 80530, loss = 0.24, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 46h:45m:10s remains)
INFO - root - 2017-12-16 00:43:57.567007: step 80540, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 44h:59m:30s remains)
INFO - root - 2017-12-16 00:44:04.032950: step 80550, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 44h:50m:46s remains)
INFO - root - 2017-12-16 00:44:10.409511: step 80560, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 45h:34m:18s remains)
INFO - root - 2017-12-16 00:44:16.803341: step 80570, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 45h:02m:27s remains)
INFO - root - 2017-12-16 00:44:23.230538: step 80580, loss = 0.33, batch loss = 0.22 (12.7 examples/sec; 0.629 sec/batch; 44h:01m:03s remains)
INFO - root - 2017-12-16 00:44:29.609866: step 80590, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 44h:06m:18s remains)
INFO - root - 2017-12-16 00:44:36.014453: step 80600, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 44h:06m:54s remains)
2017-12-16 00:44:36.522089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3732672 -5.4839873 -5.5425086 -5.5566568 -5.6510029 -5.8393655 -6.1794524 -6.0102754 -5.6511397 -5.568027 -7.12715 -7.3866348 -7.7894721 -8.6921921 -8.6945124][-4.6864862 -5.0363865 -4.6981649 -4.8805509 -5.0835009 -5.1737738 -5.5603824 -5.5912352 -5.4036016 -5.0059195 -6.2419157 -6.144413 -6.57907 -7.4798222 -7.6202755][-3.5576282 -3.3225994 -3.4778585 -3.4430346 -3.098948 -3.9165957 -4.3833857 -4.2334175 -4.0475049 -3.7529423 -5.3532038 -5.2595158 -5.5618081 -6.2984185 -6.5326385][-3.0313606 -2.514245 -2.3664351 -1.9975381 -2.2665763 -2.6494417 -2.5682311 -2.6872745 -2.5967169 -2.6344652 -4.0930433 -3.9205797 -4.76309 -5.7358079 -6.1965][-2.8281779 -2.3040614 -1.8834295 -1.2598534 -0.91854954 -0.70668268 -0.71519613 -0.65376759 -0.21751547 -0.72125959 -2.5804458 -2.2825174 -3.2031131 -4.5162125 -5.0816016][-1.9191937 -1.7780185 -1.1445384 -0.14225101 0.21872282 0.786808 1.2677212 1.3907471 1.4794579 0.82474327 -1.095789 -1.1539607 -2.4001913 -3.5914426 -4.3849955][-2.3044357 -1.8532901 -1.0000772 -0.29770088 0.66157532 1.6191368 2.2084312 2.4162626 2.4821444 1.551857 -0.40716505 -0.81848764 -2.4801998 -4.2151003 -4.7506862][-2.2001657 -2.0098615 -1.2735896 -0.30900478 0.37110233 1.478406 2.3534269 2.5808477 2.6547995 1.7532787 -0.41262674 -1.1616726 -2.8306284 -4.9539013 -5.6476879][-2.500248 -2.3223615 -1.7349286 -0.99269104 0.04201889 1.0500364 1.6879425 1.9444294 1.9700766 1.1704273 -0.94771433 -2.0453529 -3.6521597 -5.443573 -5.877068][-4.3005257 -3.7549508 -3.104382 -2.1885715 -0.95909548 0.14696741 0.8178215 1.1536093 1.2751255 0.2567687 -1.627315 -2.5438457 -3.9167964 -5.3899212 -6.0887671][-7.1900282 -6.1715546 -5.45427 -4.2517605 -3.2634621 -2.362896 -1.4568334 -1.1381063 -1.0307217 -1.8030996 -3.4767032 -4.194809 -5.2597904 -6.1949592 -6.7542133][-8.282691 -7.6924338 -6.9103713 -5.8943453 -5.2568674 -4.2193308 -3.3625922 -3.308579 -3.0073676 -3.3082128 -4.4771714 -4.8513432 -5.5435228 -6.5741587 -6.9092627][-8.5162334 -8.5990267 -7.9923716 -7.3287034 -6.7169652 -5.8593855 -4.8026981 -4.404335 -4.1521893 -4.3867483 -5.1063428 -4.9690228 -5.0379934 -5.5514622 -5.7775011][-7.3631706 -7.9534016 -7.7517705 -7.3707266 -7.0498791 -6.4073906 -5.2709055 -5.0064168 -4.8143296 -4.7240458 -5.1523314 -5.0109835 -4.8875885 -5.1205988 -5.3955374][-7.7030458 -7.6750531 -7.5819378 -7.5733418 -7.5030923 -7.1726513 -6.5711226 -6.3827496 -6.4506626 -6.2829351 -6.1800857 -6.083437 -6.165431 -6.3029423 -5.9798856]]...]
INFO - root - 2017-12-16 00:44:42.920241: step 80610, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 44h:28m:52s remains)
INFO - root - 2017-12-16 00:44:49.328564: step 80620, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 45h:30m:13s remains)
INFO - root - 2017-12-16 00:44:55.741906: step 80630, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 46h:09m:48s remains)
INFO - root - 2017-12-16 00:45:02.152559: step 80640, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 44h:46m:29s remains)
INFO - root - 2017-12-16 00:45:08.591250: step 80650, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:19m:44s remains)
INFO - root - 2017-12-16 00:45:15.027489: step 80660, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 43h:52m:22s remains)
INFO - root - 2017-12-16 00:45:21.481216: step 80670, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 45h:23m:31s remains)
INFO - root - 2017-12-16 00:45:27.855923: step 80680, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 43h:06m:14s remains)
INFO - root - 2017-12-16 00:45:34.274775: step 80690, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 45h:47m:39s remains)
INFO - root - 2017-12-16 00:45:40.691917: step 80700, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 44h:19m:02s remains)
2017-12-16 00:45:41.276449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4022069 -5.7438312 -6.1880422 -6.2015676 -6.9713106 -7.4426126 -5.712759 -4.6542573 -4.4237218 -5.3862476 -5.2426586 -5.8897915 -5.1400757 -4.5628772 -3.9903486][-6.6252193 -5.6503005 -4.1430511 -4.3859868 -5.02703 -5.1268964 -6.1097121 -6.16925 -4.4837446 -4.5118041 -4.8369951 -5.2896147 -6.8949728 -5.7612667 -4.2425585][-2.6220193 -3.8695929 -3.0924063 -2.2679071 -3.287241 -2.5058312 -2.1729202 -1.3890548 -2.8167963 -3.7089763 -4.2602696 -3.593493 -5.1863241 -4.00467 -4.8392582][-0.99276352 -1.3886361 -3.5313954 -2.756423 -1.5911808 -1.1725259 -2.6843328 -1.8517046 -1.0709457 -1.5920801 -3.8571558 -4.6402206 -5.1508493 -3.8419032 -3.4767952][-7.2224159 -3.0358415 -0.39872456 -0.91043329 -1.2477899 -0.51661587 -0.083518982 -0.7306819 -0.8974452 -2.945992 -3.5303826 -4.4598064 -5.7242122 -5.5927968 -5.1461258][-3.1602998 -3.3565564 -4.0001597 -1.6347232 2.0258665 3.0811911 2.6531811 1.7061357 0.96217155 -1.2012901 -2.3906598 -4.4230909 -5.077136 -4.9829988 -4.203835][-3.2331219 -2.0495739 -0.46953487 0.357831 -0.42306328 1.6498213 3.6600676 3.4581652 2.8055801 0.064022541 -1.3597112 -3.455687 -5.155654 -5.0740423 -4.4207029][-1.109633 -1.5631666 -1.030045 1.1351824 1.5818529 2.305521 2.1343222 2.554759 2.7883625 0.60396671 -0.698123 -2.7632055 -5.1638889 -5.1733932 -4.9502349][-1.7878866 -1.9182839 -1.0353913 0.060645103 1.0939131 2.3842316 2.9938583 2.5092936 1.6093102 0.53412628 0.06537056 -1.7778139 -3.9338286 -4.5137739 -5.1708779][-5.0001373 -3.5626602 -2.168283 -1.586575 -0.17225266 -0.91440725 -0.54337883 0.76438904 2.0450153 0.44347286 -1.0886555 -2.0470691 -3.1547532 -3.4084859 -3.9009843][-5.940177 -5.0812426 -4.8459778 -4.1638651 -2.9114842 -3.0669141 -2.6323876 -3.130631 -2.749 -1.5020204 -2.4419122 -2.642827 -3.4407673 -4.0358071 -4.382515][-8.4829416 -6.7989507 -6.9811182 -6.2029634 -6.1859713 -6.3147078 -6.186553 -5.454052 -4.8953109 -3.6485214 -5.1319594 -5.2651629 -5.7125592 -4.6919785 -4.8678265][-6.3813505 -7.3769069 -8.5721016 -8.1220293 -7.3049269 -6.188776 -6.1329389 -5.8861647 -5.726542 -4.5550795 -6.0797572 -5.6459084 -5.5738592 -5.416872 -5.7737169][-6.5784559 -6.3950715 -5.9703221 -6.9780927 -7.6979136 -7.1156421 -6.689971 -6.1440864 -6.0691576 -6.3885336 -6.529326 -6.2472696 -5.9340315 -5.3295469 -6.0004983][-4.5672655 -5.8408575 -6.0605283 -6.0657473 -5.6802778 -6.4865236 -7.07168 -6.0778146 -6.2738781 -6.25667 -5.8794451 -6.902163 -7.7252512 -6.674674 -6.3263512]]...]
INFO - root - 2017-12-16 00:45:47.752892: step 80710, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 46h:18m:39s remains)
INFO - root - 2017-12-16 00:45:54.139705: step 80720, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 45h:49m:31s remains)
INFO - root - 2017-12-16 00:46:00.549186: step 80730, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 44h:48m:02s remains)
INFO - root - 2017-12-16 00:46:07.051630: step 80740, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 44h:33m:34s remains)
INFO - root - 2017-12-16 00:46:13.547772: step 80750, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 46h:26m:32s remains)
INFO - root - 2017-12-16 00:46:19.979923: step 80760, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 45h:17m:48s remains)
INFO - root - 2017-12-16 00:46:26.449191: step 80770, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 45h:04m:07s remains)
INFO - root - 2017-12-16 00:46:32.883406: step 80780, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 44h:28m:07s remains)
INFO - root - 2017-12-16 00:46:39.360138: step 80790, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.627 sec/batch; 43h:52m:24s remains)
INFO - root - 2017-12-16 00:46:45.848940: step 80800, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:19m:12s remains)
2017-12-16 00:46:46.409530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5951209 -2.4834275 -2.3163204 -2.6115437 -2.5565734 -2.7762284 -2.5421276 -1.8673477 -1.0162287 -1.7135048 -2.8957434 -3.9082806 -3.8189497 -5.4559841 -7.0032883][-2.8913183 -2.8506904 -2.26858 -2.1772289 -2.7733507 -2.9451332 -2.5303164 -2.351161 -1.8584218 -2.3695397 -3.0773611 -3.205873 -4.4756894 -5.5112691 -6.7279587][-3.775249 -3.5385332 -3.174489 -2.6285119 -2.6450791 -2.6695113 -2.3509741 -2.1868486 -1.8117776 -3.0684671 -4.0779905 -4.1516371 -4.5826063 -5.6666975 -6.9836373][-4.9990053 -3.7892642 -3.148252 -2.3232765 -1.9997334 -1.5088968 -1.0267053 -0.83621454 -0.76953983 -2.1441121 -3.3771176 -4.2326975 -4.5494561 -5.8797383 -6.5709186][-5.62204 -4.3971376 -3.078793 -1.8155437 -1.0472441 -0.4454031 0.17803383 0.19773865 0.51963234 -0.80417156 -2.2232666 -3.2489014 -4.6460528 -6.0666814 -6.8884516][-5.4550047 -4.3347573 -3.0532756 -1.1893606 -0.19216204 1.0603294 1.6899424 1.3648415 1.2801151 -0.059381485 -1.307539 -2.535274 -4.1564732 -5.362093 -6.5118136][-4.6128922 -3.0638762 -1.2806163 0.36007118 1.8930521 3.033083 3.5581989 3.39258 3.2054338 1.6545458 0.0076904297 -1.7235947 -3.2618189 -4.6529913 -5.9126196][-3.0106053 -1.6549296 -0.44187593 1.138072 2.3898754 3.3696241 4.0323725 4.1735086 4.1643877 2.5678616 0.46830368 -1.4691086 -2.6238184 -4.282968 -5.5043855][-3.4419465 -2.0985336 -0.15533543 0.73641109 1.7496176 2.6126537 3.2291594 3.7103472 4.3473463 2.8717356 0.24754381 -1.7226248 -3.1681237 -4.6727552 -5.5124388][-4.5505943 -3.5360923 -2.4100537 -0.37614107 1.4490595 2.027626 1.8480663 2.0543633 2.1245012 0.84438038 -0.67031956 -2.2342391 -4.0013914 -5.2978859 -5.8786497][-6.399785 -5.3485794 -4.3995247 -3.3081641 -1.8139777 -0.93840647 -0.49288845 -0.48913193 -0.72112894 -2.0286613 -3.3545604 -4.089468 -4.87711 -5.7713757 -6.5163641][-8.6806078 -7.2085571 -5.9709234 -4.9564772 -4.3803377 -3.578105 -3.3074384 -2.9669285 -2.4210067 -3.5265107 -5.2393 -5.3583388 -5.8549972 -6.4556279 -6.4839063][-8.493536 -8.324008 -7.1005459 -5.7337432 -5.1298018 -4.6324425 -4.4255619 -4.2280684 -4.2045503 -4.5688276 -5.6066914 -5.7167807 -6.0800419 -5.95989 -5.913753][-9.0978918 -8.1362991 -7.8507848 -7.1087532 -6.3343115 -5.5884728 -5.3532333 -5.543673 -5.7511 -5.5799017 -5.9344215 -6.1674728 -6.1882133 -6.0151057 -5.6157217][-7.9189138 -7.3505721 -7.4906988 -7.5873947 -6.9522233 -6.2095418 -5.8794131 -5.9847608 -6.11217 -6.4086003 -6.2908773 -6.1861134 -6.296926 -5.9977088 -5.7863722]]...]
INFO - root - 2017-12-16 00:46:52.817210: step 80810, loss = 0.26, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 47h:21m:05s remains)
INFO - root - 2017-12-16 00:46:59.218271: step 80820, loss = 0.23, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 44h:21m:43s remains)
INFO - root - 2017-12-16 00:47:05.541211: step 80830, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 45h:01m:54s remains)
INFO - root - 2017-12-16 00:47:12.007959: step 80840, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 44h:33m:05s remains)
INFO - root - 2017-12-16 00:47:18.353575: step 80850, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 44h:57m:46s remains)
INFO - root - 2017-12-16 00:47:24.777145: step 80860, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 44h:56m:45s remains)
INFO - root - 2017-12-16 00:47:31.209919: step 80870, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 44h:50m:49s remains)
INFO - root - 2017-12-16 00:47:37.577658: step 80880, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 43h:42m:00s remains)
INFO - root - 2017-12-16 00:47:43.951700: step 80890, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 46h:24m:08s remains)
INFO - root - 2017-12-16 00:47:50.321566: step 80900, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 43h:11m:19s remains)
2017-12-16 00:47:50.849725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7863228 -4.0315738 -4.4787312 -4.6701975 -4.6261654 -4.1823335 -3.4628544 -2.9216633 -2.3516641 -2.5902214 -3.1419506 -4.315793 -5.493288 -5.4692383 -6.6330132][-4.4231443 -4.3992529 -4.6437902 -5.1237955 -5.4124489 -5.1783996 -4.4526777 -3.798131 -3.0374393 -3.5153074 -4.0618486 -5.0544186 -6.0967593 -6.3358173 -7.5255642][-4.1919522 -4.0474815 -4.2149324 -4.4009867 -4.3249388 -4.7716637 -4.7269626 -4.1864681 -3.4969707 -4.3055763 -5.113019 -6.1381855 -6.9453678 -7.2727757 -8.4117126][-4.338685 -3.793777 -3.57687 -3.4255276 -3.0556192 -3.333076 -2.9958839 -2.9337683 -2.929215 -3.8904889 -4.8050594 -6.2988725 -7.3554811 -7.5005927 -8.1351767][-5.0564022 -4.4705887 -3.8183005 -2.786716 -1.9451408 -1.6930194 -1.2049479 -1.3828521 -1.5286288 -2.9000292 -4.4142971 -6.0991573 -7.66286 -7.7735343 -8.4548][-6.7166424 -5.9653583 -5.0193205 -3.3109961 -1.7372518 -0.63020849 0.62587357 0.19943476 0.09208107 -1.2750878 -3.1528668 -5.3710132 -7.4351053 -7.9770727 -8.925889][-6.8819313 -5.8841686 -4.6263161 -2.6381397 -0.63041878 1.1575098 3.0778704 2.9180994 2.7841663 1.2948866 -0.893517 -3.5310349 -5.8940811 -6.9237256 -7.9733124][-6.140451 -5.1857295 -3.9143922 -1.7643962 0.38659477 3.099864 4.9151 5.0136747 5.1832495 2.9150362 0.63958073 -2.1506944 -4.6909542 -5.7118216 -7.0093102][-6.3505526 -5.5142813 -4.6924982 -3.0568857 -1.2212596 1.0018711 2.8831816 3.7662392 4.5905504 2.1919518 -0.45476103 -2.9612856 -5.0342264 -5.8504887 -7.2943625][-7.3456011 -6.6586885 -5.7916889 -4.1792889 -2.6823826 -1.4506502 0.13872194 1.0812006 1.6428194 -0.17880535 -2.1233144 -4.8748732 -6.505239 -6.7401047 -7.5702252][-8.7764187 -8.1666641 -7.4739723 -6.0919218 -4.666564 -3.3705263 -2.2270536 -1.8343067 -1.5465546 -2.6641912 -3.7307744 -6.1268992 -7.1814408 -7.5659766 -7.8755288][-8.1492157 -7.8703032 -7.5007143 -7.0855722 -6.3004251 -5.3551474 -4.59499 -4.1904306 -3.9775627 -4.3719878 -4.9995775 -6.59333 -7.0462413 -7.832571 -8.27621][-9.0473452 -8.4797106 -8.1627016 -7.60089 -7.0546665 -6.7329893 -6.277607 -5.9449854 -5.6178713 -6.2064238 -6.6538067 -7.19849 -7.1812372 -7.5256958 -7.8191581][-8.8413553 -8.8754005 -8.6863451 -7.5627542 -6.9676538 -6.506362 -5.971981 -6.1531467 -6.1072817 -6.7094741 -6.7581787 -7.0017557 -7.2379584 -7.2136893 -7.1615353][-7.9586806 -8.6225462 -9.1749935 -8.6125746 -8.0261793 -7.2678614 -7.0527568 -7.0748844 -7.0697422 -7.0557604 -6.9502678 -7.05995 -6.7582936 -6.6561704 -6.4806666]]...]
INFO - root - 2017-12-16 00:47:57.413275: step 80910, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 45h:59m:41s remains)
INFO - root - 2017-12-16 00:48:03.808341: step 80920, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 43h:59m:44s remains)
INFO - root - 2017-12-16 00:48:10.301201: step 80930, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 44h:30m:57s remains)
INFO - root - 2017-12-16 00:48:16.722501: step 80940, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 44h:01m:54s remains)
INFO - root - 2017-12-16 00:48:23.234027: step 80950, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 45h:22m:26s remains)
INFO - root - 2017-12-16 00:48:29.620616: step 80960, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:23m:43s remains)
INFO - root - 2017-12-16 00:48:36.038012: step 80970, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 44h:32m:56s remains)
INFO - root - 2017-12-16 00:48:42.535947: step 80980, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 44h:46m:19s remains)
INFO - root - 2017-12-16 00:48:48.932740: step 80990, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 44h:48m:02s remains)
INFO - root - 2017-12-16 00:48:55.411974: step 81000, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.614 sec/batch; 42h:54m:22s remains)
2017-12-16 00:48:55.996897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6635342 -2.1967559 -2.5756502 -2.680172 -3.0497928 -3.6216378 -3.9314051 -4.1096087 -4.0805259 -5.1251221 -6.1393986 -7.0226011 -7.3030572 -7.7280159 -7.8865447][-1.6895995 -2.1704669 -2.9551134 -3.4603195 -3.8708694 -4.1168461 -4.4711723 -4.9830008 -4.9981337 -6.1379919 -6.8274965 -7.8421655 -8.3857927 -8.8147926 -9.0718][-1.118288 -1.3419838 -1.5940304 -2.0011158 -2.2860932 -2.6613941 -2.9277568 -3.4968252 -3.957284 -5.0780387 -5.99123 -7.0869155 -7.7714033 -8.3876686 -8.7698088][-0.58284473 -0.94430637 -1.1447988 -1.0136905 -1.2601933 -1.3598223 -1.4339609 -1.8435116 -1.9801798 -3.4504795 -4.5696421 -5.7404652 -6.4862151 -7.4531126 -7.8545775][-0.913888 -0.80551863 -0.70573521 -0.61456442 -0.33234882 -0.16211319 -0.042974472 -0.12025261 -0.11963081 -1.2963557 -2.4784579 -4.0319586 -5.1488504 -6.407867 -7.0631638][-1.7721496 -1.4157419 -1.0250173 -0.74384737 -0.40142155 0.20598269 0.79086018 1.0400343 1.3956261 0.17926121 -1.064352 -2.4921865 -3.7096689 -5.0776296 -5.7628193][-2.7019467 -1.9362836 -1.215589 -0.41503811 0.33796597 0.69333172 1.1586876 1.8172255 2.5015192 1.2871122 0.1064086 -1.2814999 -2.5819435 -4.1207066 -5.1157403][-2.9438605 -2.2930164 -1.6049814 -0.54412413 0.46529007 1.2087946 1.7075281 2.2187881 3.0071058 2.1347456 0.98729229 -0.61848211 -1.9514346 -3.4352365 -4.419723][-2.9654236 -2.1745687 -1.6756921 -0.81599617 0.058586121 0.93575764 1.6455708 1.8724508 2.2241449 1.0781536 -0.10844517 -1.4758248 -2.5474529 -3.9669271 -4.76365][-3.3949952 -2.8466392 -2.2987332 -1.4037657 -0.58063936 0.039537907 0.696002 1.3145227 1.5762882 -0.23661137 -1.4484386 -2.9371552 -4.0179944 -5.155787 -5.671258][-4.8371029 -4.1594806 -3.6520905 -2.8487854 -2.2690392 -1.6211224 -0.84936619 -0.52673912 -0.40854979 -1.9603252 -3.3531318 -4.6370316 -5.3208981 -6.2972646 -6.7714658][-5.3193493 -5.2003031 -4.5886011 -4.0138507 -3.4015923 -2.5706573 -2.2184405 -2.3592491 -2.3193283 -3.5465894 -4.7420583 -5.8654337 -6.2467842 -6.7675147 -6.71189][-6.2468987 -5.7307639 -5.45685 -5.0090737 -4.5923615 -4.1161852 -4.0368986 -4.0809727 -4.2292118 -5.2002058 -5.97428 -6.4499884 -6.8778725 -7.0541067 -6.6585541][-6.1787543 -5.7814865 -5.3884082 -4.9931927 -4.7484369 -4.3668613 -4.3692245 -4.5693111 -4.79597 -5.6507478 -6.0044103 -6.1903067 -6.1898179 -6.659996 -6.2637229][-7.29294 -6.9184022 -6.6210327 -6.2228117 -5.8126507 -5.557425 -5.6253839 -5.8296165 -5.9762878 -6.2953386 -6.6600223 -6.8749003 -6.9507461 -6.48725 -6.1489353]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:49:02.454813: step 81010, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:16m:54s remains)
INFO - root - 2017-12-16 00:49:08.879626: step 81020, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 44h:53m:26s remains)
INFO - root - 2017-12-16 00:49:15.301108: step 81030, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 44h:39m:35s remains)
INFO - root - 2017-12-16 00:49:21.810145: step 81040, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 45h:33m:17s remains)
INFO - root - 2017-12-16 00:49:28.153335: step 81050, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 46h:00m:41s remains)
INFO - root - 2017-12-16 00:49:34.602084: step 81060, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 44h:14m:37s remains)
INFO - root - 2017-12-16 00:49:41.067819: step 81070, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:21m:21s remains)
INFO - root - 2017-12-16 00:49:47.424183: step 81080, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 45h:02m:11s remains)
INFO - root - 2017-12-16 00:49:53.848346: step 81090, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:56m:25s remains)
INFO - root - 2017-12-16 00:50:00.224548: step 81100, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 44h:02m:52s remains)
2017-12-16 00:50:00.759462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.78416 -7.2854862 -7.9624257 -8.4547663 -8.6794949 -8.3559046 -7.3501234 -5.6497397 -3.9716988 -3.45863 -3.7884531 -5.4890871 -6.7224545 -6.9845996 -6.8381124][-5.5466638 -6.9828067 -8.6447163 -9.7440176 -9.7472763 -9.2567415 -8.3654318 -6.5741329 -5.4035616 -5.5374436 -5.6800165 -7.5004315 -8.9516821 -8.1345739 -8.27443][-4.8529649 -6.996686 -9.2644224 -10.225128 -9.7600031 -8.6209831 -7.4240131 -6.1310086 -5.4890575 -5.9498587 -7.1143641 -9.4794674 -10.622236 -9.88461 -10.041511][-6.3144631 -6.5319548 -7.99621 -8.05062 -8.2648487 -7.4135365 -5.1472368 -3.8990302 -3.25913 -4.2636456 -5.7864451 -8.7857437 -10.95819 -10.980926 -10.836056][-6.6924005 -6.6827044 -6.7996655 -5.9218163 -6.2730689 -4.0598631 -2.4586983 -1.4974833 -1.0400739 -1.9882579 -3.9086561 -7.8411021 -10.208294 -10.661339 -11.23912][-7.3616529 -6.8989167 -6.5613356 -4.8139606 -3.5200381 -0.926908 0.66902828 2.0862961 2.0302458 -0.39132023 -2.6895161 -6.2810144 -8.9742279 -9.5644817 -10.755429][-6.5810137 -6.0732465 -5.3022408 -3.1442313 -0.008409977 2.9399147 5.2782211 5.8420038 5.5171137 2.4489269 -0.75383806 -5.2639284 -7.9743714 -8.1791258 -8.56156][-5.9541669 -4.7130289 -3.4933743 -0.5625 2.4974098 5.065589 7.6329775 7.6601677 6.8906689 3.7602262 0.54898739 -4.4259491 -7.3629646 -7.6422729 -7.7145815][-5.9869614 -5.3154516 -4.5301352 -1.4296765 1.3603973 3.6551065 5.6556673 5.5543814 5.1653996 2.4591789 -0.81198025 -5.3585367 -7.9533768 -8.24231 -8.0260143][-7.4242735 -7.2225962 -6.34245 -4.8586373 -2.135087 0.22982883 2.3983974 2.2455711 1.7721062 -1.0169921 -4.085537 -7.3053417 -9.2943363 -9.159893 -8.9472914][-9.5132723 -9.8626518 -9.6855259 -8.6026936 -6.6818471 -4.55008 -2.665875 -2.7896934 -2.2685204 -4.448678 -6.7113986 -8.5832834 -10.52273 -9.6926432 -9.3229513][-10.089787 -9.9526367 -9.9222994 -9.7165813 -8.7873688 -7.7485642 -6.5928349 -5.75936 -4.9121313 -6.290514 -6.684782 -7.5360589 -9.663147 -8.9900808 -9.0981808][-9.3929415 -9.6577225 -9.8210106 -9.4069023 -8.8623276 -8.5232534 -7.9082494 -7.58431 -6.7923379 -6.98118 -7.3397546 -7.4131513 -8.01175 -7.5648713 -8.4210033][-8.664856 -8.4970274 -8.2051468 -7.8654103 -7.3831005 -7.1357565 -6.7010937 -6.8974776 -7.401361 -7.3464732 -6.9561672 -7.3557019 -7.5163708 -6.3644342 -6.4837275][-7.4356885 -7.9198093 -8.4289856 -8.2756824 -7.8902259 -7.534996 -7.53388 -7.1685929 -6.9672012 -7.1980991 -7.51304 -7.222312 -6.9149475 -7.0185556 -6.9007726]]...]
INFO - root - 2017-12-16 00:50:07.215102: step 81110, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 45h:25m:40s remains)
INFO - root - 2017-12-16 00:50:13.578027: step 81120, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 44h:34m:26s remains)
INFO - root - 2017-12-16 00:50:20.020706: step 81130, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 46h:39m:22s remains)
INFO - root - 2017-12-16 00:50:26.403249: step 81140, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:14m:47s remains)
INFO - root - 2017-12-16 00:50:32.916528: step 81150, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 46h:40m:09s remains)
INFO - root - 2017-12-16 00:50:39.258998: step 81160, loss = 0.36, batch loss = 0.25 (12.8 examples/sec; 0.627 sec/batch; 43h:45m:07s remains)
INFO - root - 2017-12-16 00:50:45.582599: step 81170, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 43h:53m:34s remains)
INFO - root - 2017-12-16 00:50:52.010058: step 81180, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 44h:08m:55s remains)
INFO - root - 2017-12-16 00:50:58.417257: step 81190, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 44h:08m:26s remains)
INFO - root - 2017-12-16 00:51:04.820194: step 81200, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 44h:05m:28s remains)
2017-12-16 00:51:05.418556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6539183 -3.2682538 -3.349596 -3.310585 -3.1152668 -2.8022537 -2.4598694 -2.2130837 -2.0753398 -2.6822147 -3.5598607 -4.6917639 -5.9505367 -6.8505545 -7.1368127][-4.5420084 -4.5067372 -4.7365408 -4.5711985 -4.575738 -4.3332934 -3.7944191 -3.2667637 -2.7896509 -3.3929472 -4.1321754 -5.6294088 -7.1034608 -7.7325211 -7.9447465][-4.5825605 -4.3414116 -4.1524382 -4.1541696 -4.2490473 -3.6579318 -2.8082862 -2.4318738 -1.9979796 -2.6229396 -3.5067062 -4.8106251 -6.2726951 -7.2436447 -7.6538353][-5.0559931 -5.2372284 -5.0436273 -4.0661221 -2.8920445 -2.1347022 -1.3198895 -1.2303934 -1.1430349 -2.0166116 -2.8843369 -4.3332477 -5.7112494 -6.4878144 -6.6485996][-6.1689339 -5.4618969 -4.6348886 -3.7084041 -2.7225819 -1.4182773 -0.1129694 0.18013859 0.30269432 -0.95405149 -2.1001749 -3.7266004 -5.4401493 -6.5974512 -7.0585885][-6.3500071 -6.138598 -5.5655737 -4.1454897 -2.4066586 -0.54407597 1.0188971 1.4517231 1.6663246 0.3373909 -0.99049187 -2.7022042 -4.5943594 -5.9175735 -6.8133454][-6.942874 -6.1404076 -4.9803562 -3.4709854 -1.4254756 0.71471786 2.2383862 2.7729778 2.7844696 1.1253157 -0.14376068 -1.9634523 -4.0287085 -5.5132103 -6.5236616][-6.467351 -5.3470335 -4.3475308 -2.5098982 -0.33669567 1.7155132 3.2001514 3.6794891 3.4204741 1.6585169 0.39578247 -1.8246198 -4.1055822 -5.6875691 -6.5370178][-6.1585259 -5.3904624 -4.4384441 -3.0128417 -1.5335388 0.11600685 1.4198151 1.8809834 1.8410072 0.4848938 -0.74084377 -2.4242334 -4.4109278 -5.8205338 -6.5980425][-5.9455376 -5.6879663 -4.9715462 -3.6497569 -2.0586786 -0.68117476 -0.097612858 0.23143005 0.36223125 -0.971189 -2.0108018 -3.6707344 -5.2533069 -6.0161877 -6.2582932][-7.1730218 -6.818975 -6.4789085 -5.4076891 -4.29258 -3.003788 -2.0607538 -2.0890603 -2.1165333 -3.0674024 -3.8515532 -5.11098 -6.112154 -6.8092232 -6.768672][-8.07602 -7.7726278 -7.4940214 -6.8219147 -5.995616 -4.9403343 -4.4528303 -4.1671362 -3.7868364 -4.4403057 -5.0025692 -5.7347827 -6.3699679 -6.4407272 -6.3413219][-7.9311748 -8.1471262 -7.9704504 -7.3035312 -6.5179896 -5.9256883 -5.804697 -5.8975706 -5.8712244 -6.3475084 -6.626791 -6.7230606 -7.0418053 -7.0865951 -6.7566934][-7.9541173 -7.7841773 -7.4444222 -7.1770215 -6.7180429 -6.374301 -6.0799069 -6.3290968 -6.594728 -7.0371532 -7.22939 -7.1673937 -7.2987547 -7.1357541 -6.7184577][-8.9921541 -8.4370165 -7.7912469 -7.3251123 -7.0562091 -6.8465409 -6.8031516 -7.0757451 -7.2901134 -7.3020782 -7.4166431 -7.4610262 -7.584609 -7.2865896 -6.9761906]]...]
INFO - root - 2017-12-16 00:51:11.978455: step 81210, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.684 sec/batch; 47h:43m:21s remains)
INFO - root - 2017-12-16 00:51:18.371057: step 81220, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 44h:10m:49s remains)
INFO - root - 2017-12-16 00:51:24.827416: step 81230, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 44h:25m:39s remains)
INFO - root - 2017-12-16 00:51:31.260191: step 81240, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 43h:38m:06s remains)
INFO - root - 2017-12-16 00:51:37.653727: step 81250, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 46h:10m:20s remains)
INFO - root - 2017-12-16 00:51:44.177579: step 81260, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 45h:25m:08s remains)
INFO - root - 2017-12-16 00:51:50.601523: step 81270, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 43h:22m:20s remains)
INFO - root - 2017-12-16 00:51:57.030099: step 81280, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 44h:00m:04s remains)
INFO - root - 2017-12-16 00:52:03.386035: step 81290, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 45h:06m:03s remains)
INFO - root - 2017-12-16 00:52:09.718038: step 81300, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 44h:29m:30s remains)
2017-12-16 00:52:10.224478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7083683 -5.60138 -6.0477829 -5.9365721 -6.0902872 -5.7679472 -5.4087067 -4.6368284 -4.1200914 -5.2346935 -5.0137534 -5.6465793 -7.486918 -8.298811 -9.6313972][-6.6921935 -6.392982 -6.6133571 -7.02224 -6.6844273 -6.0279288 -5.2775936 -4.5735788 -3.6142683 -5.1705551 -5.7956634 -7.1750011 -8.7414465 -9.3167553 -9.1794138][-5.8836718 -6.3415489 -6.1900587 -5.4480953 -4.975214 -4.6341028 -4.354414 -3.5142245 -3.2952528 -4.2152524 -3.9557481 -4.8943348 -7.0026236 -7.8452177 -9.00831][-5.2460413 -4.1547956 -4.41181 -4.67012 -4.3379354 -3.9594297 -3.1467466 -2.2086792 -1.3433595 -2.4599471 -3.1022205 -4.4199319 -5.8848352 -6.7882128 -7.7825341][-4.8589373 -3.726202 -2.8922987 -2.5382843 -2.2709308 -1.7259588 -0.989367 -0.6715517 -0.14386034 -1.092701 -1.7953172 -3.2542977 -5.4273653 -6.231081 -6.8424039][-3.156764 -2.8030682 -1.6741495 -0.77393913 -0.2815156 -0.49537897 -0.62201834 -0.25246334 0.25560236 -1.1195116 -1.8945947 -2.6814318 -4.1780252 -4.728261 -5.3415556][-2.188354 -1.4439187 -0.97937822 -0.29405832 0.15650415 0.59589195 0.72296238 0.44311333 0.75913811 -0.688889 -1.6133852 -2.9569445 -4.5641084 -4.9270277 -5.0677176][-1.3859773 -0.77877426 -0.68978596 -0.22767162 0.27470922 0.97846889 0.94413757 0.49322891 0.45855045 -1.2136416 -2.2211313 -3.335556 -4.8278542 -5.0880113 -5.0734043][-1.5180445 -1.1086531 0.063332081 0.86466503 1.1545382 1.0440931 0.41806698 0.16512728 0.19084311 -1.6625032 -2.613277 -3.7666488 -5.0229826 -5.2292619 -5.3817887][-1.4377608 -0.66934347 -1.096962 -1.1830673 -0.82742071 -0.47375679 -0.35502815 -0.81513357 -1.1295567 -2.8308797 -3.6362953 -4.3187261 -5.5102711 -5.7164207 -5.6906953][-3.0957289 -2.6227608 -1.9909382 -1.9209161 -2.2958918 -2.3578496 -1.9744024 -2.1119428 -2.6467023 -4.0090628 -4.7944522 -5.3405681 -5.7869153 -5.7439985 -5.9742608][-4.7000308 -4.6294622 -4.3564329 -3.8525953 -3.6333966 -3.7263291 -4.1517754 -4.19691 -4.1054497 -5.0700717 -5.8885059 -6.2142859 -6.3217969 -6.2267733 -6.2945786][-5.2005892 -4.9821959 -5.2101173 -5.9041185 -6.0992765 -5.9418163 -5.649457 -5.7175045 -5.6540775 -5.8658266 -6.1898689 -6.4896717 -6.3798842 -6.1347942 -5.9089093][-5.7233934 -5.4724121 -5.5326419 -5.4989595 -5.4435873 -5.5700359 -5.4180584 -5.5767717 -5.47431 -5.7127209 -5.8707204 -5.8607125 -5.570981 -5.4589167 -5.5426884][-6.0024672 -6.4410267 -6.6959419 -6.8833308 -6.9040365 -7.0063744 -6.6087704 -6.3572321 -6.1002536 -5.97381 -6.0526166 -6.2581968 -6.1522465 -5.9149752 -5.8108392]]...]
INFO - root - 2017-12-16 00:52:16.666162: step 81310, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 46h:07m:24s remains)
INFO - root - 2017-12-16 00:52:23.153819: step 81320, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 46h:38m:04s remains)
INFO - root - 2017-12-16 00:52:29.644849: step 81330, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 44h:36m:32s remains)
INFO - root - 2017-12-16 00:52:36.097237: step 81340, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 44h:02m:09s remains)
INFO - root - 2017-12-16 00:52:42.543064: step 81350, loss = 0.32, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 45h:30m:42s remains)
INFO - root - 2017-12-16 00:52:48.921966: step 81360, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 43h:53m:21s remains)
INFO - root - 2017-12-16 00:52:55.282509: step 81370, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 44h:58m:22s remains)
INFO - root - 2017-12-16 00:53:01.660044: step 81380, loss = 0.36, batch loss = 0.24 (12.6 examples/sec; 0.633 sec/batch; 44h:11m:15s remains)
INFO - root - 2017-12-16 00:53:08.076480: step 81390, loss = 0.27, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 46h:43m:48s remains)
INFO - root - 2017-12-16 00:53:14.606380: step 81400, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.643 sec/batch; 44h:48m:56s remains)
2017-12-16 00:53:15.136955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8668542 -6.5849924 -6.4936886 -6.2942133 -5.9520526 -5.4639454 -5.0327854 -4.5944462 -3.9342566 -3.7082133 -3.9142685 -4.3669558 -4.948431 -6.1063814 -6.9715629][-6.2704539 -5.7526927 -5.6546068 -6.040307 -6.3556471 -5.7018213 -4.9840183 -4.4794068 -4.0739594 -4.127357 -4.3480816 -4.6173587 -5.1770124 -6.2218256 -7.2690015][-5.3340349 -4.9345903 -4.6106682 -4.3230968 -3.9751387 -3.5156484 -3.7288592 -3.8741176 -3.654213 -3.4362726 -3.426837 -3.902915 -4.5532246 -5.6789508 -6.716413][-4.11644 -3.6573253 -2.9620929 -2.445838 -2.5613122 -2.1029563 -1.3733835 -1.570044 -1.4939532 -2.2967491 -2.9145408 -3.5775108 -3.8374457 -4.7345991 -5.8062658][-3.1264386 -2.3779755 -1.6610408 -1.2150078 -0.64291382 -0.07407093 0.501215 0.30799723 0.52088165 -0.59305096 -1.4669552 -2.5764117 -3.5091348 -4.9574852 -5.9106255][-1.6659746 -1.8511562 -0.7157774 0.3049984 0.65984726 1.3827982 2.0348167 1.746747 1.5830393 0.24349976 -1.0356326 -2.3733797 -3.492909 -4.9469275 -6.0096931][-1.3333154 -0.91760254 -0.15610552 0.46700668 1.2840862 1.8308725 2.4084806 2.4245348 2.2350931 0.87256432 -0.97483587 -2.719614 -4.0833931 -5.6140389 -6.3747234][-1.4487391 -1.2505569 -0.60991764 0.38642216 1.3296509 2.0125036 2.2488518 2.4545498 2.35812 0.97515965 -0.72260857 -2.7896552 -4.338335 -5.9672804 -7.1541324][-2.6495066 -2.04569 -1.0689945 -0.37675858 0.15419436 1.2608871 1.811697 1.9190931 1.8646545 0.712945 -0.89786005 -2.6240506 -3.9348087 -5.8895445 -7.1061888][-3.1110072 -2.2181516 -1.5607762 -1.2530713 -0.65235472 -0.042590618 0.21490145 0.74518108 0.97672462 -0.11775923 -1.3252449 -3.0894432 -4.75947 -6.0733728 -7.095993][-4.7714033 -3.8789897 -3.0230303 -2.6138206 -2.2258563 -1.9751782 -1.6714559 -1.8400078 -2.0055304 -2.4911485 -3.4367585 -4.2514687 -5.2176857 -6.1089821 -6.704679][-5.763845 -5.1754117 -4.3118052 -3.864213 -3.612174 -3.5284133 -3.244585 -3.1698551 -3.1060743 -3.5928288 -4.5446396 -5.0982409 -5.6046352 -5.6803751 -5.9828925][-6.3771219 -5.7418108 -4.9691582 -4.4528475 -3.9217324 -4.0325413 -4.1294618 -4.341259 -4.4749174 -3.9317682 -4.512825 -4.8373547 -5.1954517 -5.189764 -5.2642756][-6.3982005 -5.66043 -5.2197952 -4.7970152 -4.6741014 -4.5041752 -4.5565853 -5.0445952 -5.2019396 -5.0508738 -5.480814 -5.0387259 -4.7076216 -4.4260826 -4.5446215][-7.9563408 -7.2586646 -6.7203865 -6.4020872 -6.2251806 -6.1847157 -6.1399584 -6.0788956 -6.5105791 -5.968451 -5.175271 -4.7499075 -4.7308278 -4.3926172 -4.147963]]...]
INFO - root - 2017-12-16 00:53:21.552342: step 81410, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 44h:29m:37s remains)
INFO - root - 2017-12-16 00:53:27.903550: step 81420, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 44h:29m:29s remains)
INFO - root - 2017-12-16 00:53:34.274624: step 81430, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 44h:05m:55s remains)
INFO - root - 2017-12-16 00:53:40.716928: step 81440, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 43h:57m:51s remains)
INFO - root - 2017-12-16 00:53:47.083257: step 81450, loss = 0.36, batch loss = 0.25 (12.6 examples/sec; 0.633 sec/batch; 44h:08m:50s remains)
INFO - root - 2017-12-16 00:53:53.498184: step 81460, loss = 0.28, batch loss = 0.16 (11.7 examples/sec; 0.683 sec/batch; 47h:39m:44s remains)
INFO - root - 2017-12-16 00:53:59.918415: step 81470, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 44h:25m:20s remains)
INFO - root - 2017-12-16 00:54:06.356978: step 81480, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 44h:46m:56s remains)
INFO - root - 2017-12-16 00:54:12.757428: step 81490, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 44h:21m:53s remains)
INFO - root - 2017-12-16 00:54:19.173608: step 81500, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 43h:50m:35s remains)
2017-12-16 00:54:19.667411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2075243 -3.7424536 -3.0513463 -3.4477077 -4.6810408 -6.1190672 -5.8160677 -5.3841352 -4.4183493 -3.5571146 -4.3336377 -4.5358658 -6.6082063 -6.6009951 -6.4949131][-3.6697059 -3.5926805 -3.6041536 -2.4791093 -2.420639 -2.6574221 -3.3140039 -4.5484371 -4.7495155 -5.4181995 -5.7701716 -5.7147326 -6.1562891 -5.8303843 -5.2856207][-6.9498415 -5.1902733 -3.9829829 -3.6049833 -2.978261 -2.1564064 -1.521172 -2.7553492 -3.582211 -5.1539922 -7.4482613 -7.6487188 -8.005826 -7.3622317 -6.5498471][-6.0371614 -6.2818074 -6.8370519 -5.3391094 -3.2300935 -2.2962589 -1.419004 -1.2936454 -0.76001167 -2.2597537 -5.3707104 -6.912055 -8.7565784 -8.1117649 -7.8592305][-7.3116207 -6.5854478 -5.3879366 -4.0492411 -3.3397646 -3.2058702 -2.3333898 -1.8886566 -0.78241491 -1.3610368 -3.4893909 -3.9428539 -6.1268597 -7.7561879 -9.23742][-2.5376019 -2.5965595 -3.1865511 -1.9840794 -0.6616931 -0.23839664 0.60822773 0.33093262 0.43401241 0.43057919 -1.1205206 -1.8321958 -4.5136328 -4.6366653 -5.9624615][-1.0630722 -0.92505074 -0.034574986 0.72973347 0.96756744 2.0932627 3.846014 2.6722183 2.2609158 0.95219421 -1.5879512 -1.8855128 -3.8753588 -4.8794088 -5.8428125][-1.1441774 0.82396412 1.1155224 0.52052116 -0.40209866 0.6243248 2.3069897 2.4218254 3.011034 1.6971989 -2.0176415 -3.2624612 -6.111598 -6.6431994 -6.8129497][-2.3861918 -0.55997372 0.60156155 1.9042292 1.3665447 1.1702433 0.89633846 1.216876 2.8746557 1.6015778 -2.0629754 -2.7067785 -5.2802973 -5.5455709 -5.9458714][-3.9384825 -3.2434297 -2.4090619 -1.9247537 -0.78088379 -0.35142088 -0.93974304 0.018438816 1.0598898 1.1507177 -0.23520279 -1.9637876 -4.5415387 -4.2929463 -5.1061368][-7.1452179 -6.5345764 -5.2310696 -5.2495508 -4.2175617 -3.6985548 -3.5244346 -2.8032274 -3.0419674 -2.3202152 -2.7217441 -3.1180391 -3.8070841 -5.3632975 -6.9009457][-7.1045856 -7.2022758 -7.7306352 -7.0274596 -5.6956954 -5.7869163 -6.4775629 -6.4018459 -6.051765 -4.9209242 -5.6316624 -5.2636724 -5.6565332 -6.1456971 -6.8271031][-6.9406366 -6.3164191 -7.07233 -6.4501176 -5.9406419 -5.6793308 -5.2593369 -6.0765772 -6.5324445 -6.5507903 -6.6322346 -5.8886795 -5.3951764 -5.4127769 -6.1125107][-5.9102664 -6.2997937 -7.38788 -6.1988516 -6.0217357 -5.7308149 -5.5196304 -5.470789 -5.8951964 -5.8802919 -6.7899966 -6.2663708 -6.033205 -6.3482542 -6.0325561][-5.5168657 -4.3636665 -4.6418381 -4.9441576 -5.8623629 -6.3253717 -5.3007979 -5.4717569 -6.0828304 -6.1326962 -6.8238158 -7.9711862 -7.7051053 -7.3680563 -7.3542681]]...]
INFO - root - 2017-12-16 00:54:26.113931: step 81510, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.631 sec/batch; 43h:58m:45s remains)
INFO - root - 2017-12-16 00:54:32.542231: step 81520, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 45h:22m:27s remains)
INFO - root - 2017-12-16 00:54:38.976418: step 81530, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 45h:56m:21s remains)
INFO - root - 2017-12-16 00:54:45.346787: step 81540, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 43h:52m:45s remains)
INFO - root - 2017-12-16 00:54:51.688483: step 81550, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 45h:03m:39s remains)
INFO - root - 2017-12-16 00:54:58.125398: step 81560, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 46h:11m:52s remains)
INFO - root - 2017-12-16 00:55:04.581675: step 81570, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 44h:52m:21s remains)
INFO - root - 2017-12-16 00:55:11.006879: step 81580, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 45h:17m:33s remains)
INFO - root - 2017-12-16 00:55:17.448862: step 81590, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 44h:21m:07s remains)
INFO - root - 2017-12-16 00:55:23.756944: step 81600, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 44h:05m:20s remains)
2017-12-16 00:55:24.256322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7919807 -5.9687815 -5.435792 -4.8817663 -4.8269882 -4.2706842 -3.3810234 -2.6421361 -1.8800511 -2.3732176 -1.8592544 -2.334682 -3.6179762 -4.372438 -5.2706108][-4.2609825 -3.8824971 -3.4168634 -3.137208 -3.1089129 -3.1647596 -3.0056949 -3.0718937 -3.1608906 -4.3146176 -3.9784579 -4.7784534 -5.8865623 -6.02505 -6.9019508][-3.1141582 -3.4893203 -3.4499512 -3.0054212 -3.5355949 -3.9561324 -4.0515361 -3.9638636 -4.1809483 -5.1328888 -5.0849214 -6.0924578 -7.3531432 -7.3776493 -7.5815234][-2.8589129 -2.7525358 -3.3292403 -3.600615 -3.1455145 -2.9096065 -3.4217882 -3.5677443 -3.9842393 -4.9170656 -4.8801117 -5.6383581 -7.0409908 -7.679018 -8.0711746][-2.2545195 -2.3419132 -2.4453621 -2.1212664 -1.3334928 -0.39863348 0.1386447 -0.38831091 -1.329134 -2.7436161 -3.0967736 -4.4213929 -6.1037612 -6.535665 -7.379333][-2.4180431 -1.4257526 -1.3874774 -0.62690687 0.82766342 1.9283047 2.6121836 2.6099997 2.0591898 -0.016507626 -1.0777087 -2.7409592 -5.0675287 -6.0052023 -6.9082832][-2.0810556 -1.5312839 -0.72690916 0.31277943 2.0051727 3.1718941 3.8861866 4.1613731 3.923378 2.143465 1.0538177 -1.2194295 -3.7710478 -5.0987425 -6.6946754][-1.1112595 -1.0449553 -0.31655645 0.847908 2.0188351 3.3243103 4.7382221 5.1447868 4.8584137 2.9185562 1.8523798 -0.15676689 -2.9138684 -4.5058665 -5.8421779][-1.5954018 -1.045032 -0.50732517 0.16018677 0.71960926 1.1878357 2.6409721 3.6794271 4.221303 2.880662 1.4544973 -0.458086 -2.69036 -4.1367016 -5.5667849][-2.1781273 -2.4904127 -2.2173972 -1.3773212 -0.89078712 -0.48459578 0.38567448 1.4838467 1.9929171 1.2095261 0.5489912 -1.83846 -4.2042465 -4.8559775 -5.9578915][-3.4731464 -3.4258084 -3.7579353 -3.6777058 -3.4720106 -2.7161274 -1.6330028 -1.498311 -1.0448661 -2.0825005 -2.7360725 -4.387845 -6.3559704 -6.9442286 -7.3685746][-4.30639 -3.9130013 -4.409297 -4.1842113 -3.9077318 -4.0918922 -4.1214457 -4.1794739 -4.1521206 -4.9697213 -5.0949235 -5.6039696 -7.1518955 -8.1633072 -8.6440229][-6.6123428 -5.9251823 -5.819376 -5.672483 -5.1773958 -5.0491347 -4.9355316 -5.4761343 -5.8010025 -6.1123815 -6.5862665 -7.0197377 -7.5554428 -7.9074888 -8.2617807][-7.5479674 -7.2148175 -6.657774 -6.3030634 -6.0259123 -5.6758065 -5.6078892 -5.88024 -5.7735119 -5.9189563 -6.1964188 -5.9982986 -6.5716529 -6.5424051 -6.4223585][-6.1266093 -5.554976 -5.8945627 -5.9667721 -5.6554008 -5.4322205 -5.6007166 -5.5003633 -5.389492 -5.1254482 -5.4172163 -5.5834074 -5.2170391 -5.6396246 -5.5523934]]...]
INFO - root - 2017-12-16 00:55:30.774364: step 81610, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 46h:15m:47s remains)
INFO - root - 2017-12-16 00:55:37.228515: step 81620, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 45h:01m:57s remains)
INFO - root - 2017-12-16 00:55:43.654550: step 81630, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 44h:19m:04s remains)
INFO - root - 2017-12-16 00:55:50.056004: step 81640, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 43h:32m:16s remains)
INFO - root - 2017-12-16 00:55:56.575940: step 81650, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 45h:11m:57s remains)
INFO - root - 2017-12-16 00:56:02.985681: step 81660, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 44h:56m:23s remains)
INFO - root - 2017-12-16 00:56:09.360838: step 81670, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 44h:16m:27s remains)
INFO - root - 2017-12-16 00:56:15.757780: step 81680, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 43h:58m:16s remains)
INFO - root - 2017-12-16 00:56:22.156143: step 81690, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 45h:20m:44s remains)
INFO - root - 2017-12-16 00:56:28.589930: step 81700, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 44h:26m:53s remains)
2017-12-16 00:56:29.146321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.085753 -4.0753851 -4.439683 -3.6772637 -2.7951503 -3.126976 -3.6983135 -4.3492155 -4.5131598 -4.2791142 -4.3505964 -6.5745025 -6.9064245 -7.6045341 -7.6915355][-3.2132635 -4.4031949 -4.3071566 -3.776082 -2.993803 -2.8419867 -2.3299675 -3.4464369 -4.8077993 -5.061903 -4.9526973 -7.1963687 -6.7648177 -7.6398249 -8.0036173][-4.3839521 -4.1369433 -4.5512342 -3.4458127 -2.3806057 -1.8926239 -1.4127369 -1.849546 -2.7687964 -3.2979746 -4.3880219 -6.8174453 -5.9760895 -7.088028 -7.5026736][-4.0612068 -4.4386444 -3.5574818 -2.8412304 -2.1169677 -0.62852144 0.20569706 -0.34621096 -0.39308786 -1.1366954 -2.2191796 -5.1016273 -5.1808958 -6.3464227 -6.7603736][-4.6692233 -4.6233149 -4.6541376 -3.8927481 -2.4379182 -1.1367636 -0.091864586 0.013753414 -0.48809528 -0.73456049 -1.6251135 -4.3700252 -4.4231825 -5.8170791 -6.410965][-7.2332435 -6.0181456 -4.759058 -3.5983605 -2.787169 -0.654171 0.34272861 -0.29279089 -0.21530437 -0.77142954 -1.3536711 -3.5587831 -3.3051267 -4.8154068 -5.3077183][-6.927114 -5.7005405 -5.2269907 -3.4972196 -1.0856776 0.72294617 1.9264975 1.9707022 1.028306 -0.18789721 -0.87966967 -3.7293525 -3.0201983 -3.5780454 -3.5714402][-6.3076239 -6.2168622 -5.4478955 -3.1802897 -1.7437959 0.85651588 2.2499542 2.8680058 2.5317945 1.1489487 -0.2293129 -3.3754334 -3.1914086 -3.8827696 -3.7694335][-6.9528074 -6.1041403 -5.8867626 -4.1477208 -1.9610839 -0.0515213 1.0782261 1.8941212 1.5986633 0.66337776 -0.575665 -4.2595844 -4.10425 -3.8968198 -3.8988676][-9.3132019 -8.4817543 -7.2874312 -5.2770863 -4.3467321 -2.3659077 -0.63289165 -0.27116394 0.12756634 -0.95228291 -2.6099505 -5.2881718 -5.146492 -5.7788582 -5.2920256][-10.239848 -9.87808 -9.5600729 -8.0171719 -6.5730829 -4.5719819 -3.5784407 -2.5823517 -2.3494525 -3.1064553 -3.47304 -6.0146184 -6.2103038 -6.5828876 -6.2138267][-9.0289965 -9.3589325 -9.354248 -7.9470372 -6.9451551 -5.7482266 -4.3513269 -3.9929326 -4.0472336 -3.98189 -4.336215 -6.1126971 -5.4336634 -6.3659906 -6.1791258][-10.578194 -9.7979908 -9.5520363 -8.6236286 -8.2939539 -6.9433212 -5.9310479 -6.028399 -5.6853824 -6.0221863 -5.9508743 -6.7571244 -6.5311918 -6.6515579 -6.3748159][-10.122551 -10.247938 -9.833168 -8.763238 -8.0379152 -7.0351157 -6.8193641 -7.11547 -7.0271711 -7.1329246 -6.7124138 -6.6333771 -6.2205448 -6.6085758 -6.4132142][-10.620489 -10.273167 -10.578568 -8.8692122 -7.97522 -7.0938964 -6.4402261 -6.765801 -6.7309914 -7.705893 -7.682559 -6.71629 -5.8735924 -5.4254127 -5.1719856]]...]
INFO - root - 2017-12-16 00:56:35.545528: step 81710, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 44h:42m:24s remains)
INFO - root - 2017-12-16 00:56:41.951375: step 81720, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 44h:55m:34s remains)
INFO - root - 2017-12-16 00:56:48.430430: step 81730, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 45h:43m:34s remains)
INFO - root - 2017-12-16 00:56:54.830940: step 81740, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 45h:10m:17s remains)
INFO - root - 2017-12-16 00:57:01.265389: step 81750, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 44h:20m:51s remains)
INFO - root - 2017-12-16 00:57:07.657644: step 81760, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 45h:04m:44s remains)
INFO - root - 2017-12-16 00:57:14.103838: step 81770, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 44h:23m:06s remains)
INFO - root - 2017-12-16 00:57:20.491055: step 81780, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 44h:25m:24s remains)
INFO - root - 2017-12-16 00:57:26.939687: step 81790, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 44h:45m:17s remains)
INFO - root - 2017-12-16 00:57:33.272047: step 81800, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 44h:36m:53s remains)
2017-12-16 00:57:33.807100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4189444 -7.1762161 -9.3699112 -10.50469 -12.768378 -12.11706 -12.060155 -10.029163 -9.0216656 -8.25823 -7.2088766 -10.94244 -7.7051907 -7.7136116 -8.6968746][-5.8782721 -7.4947181 -9.9019852 -11.424852 -13.954632 -13.831861 -13.909602 -10.400829 -9.2446613 -8.4777822 -8.1315813 -11.943055 -10.052579 -10.700531 -10.108882][-5.9466743 -7.2649245 -9.26811 -10.454902 -12.556635 -12.122576 -11.646137 -9.7602816 -8.5964146 -7.3165712 -7.0318465 -11.498384 -10.345333 -11.405345 -11.843632][-5.959054 -6.8137369 -8.3679466 -8.6384935 -10.281134 -10.045149 -9.4057264 -7.7993221 -7.0887938 -7.878437 -6.721931 -10.886903 -9.4378433 -10.824585 -12.227094][-5.7664928 -6.1100292 -7.0636325 -6.8076353 -7.1628408 -5.5444345 -4.6227407 -4.0532217 -3.5949683 -5.8060637 -6.8861923 -12.001143 -9.8564644 -11.011415 -12.04121][-6.1025386 -7.240591 -7.5418963 -5.6677551 -5.2156162 -2.8945432 -0.24556637 1.5962496 2.4156675 -2.1069193 -4.1873159 -11.034954 -10.430844 -11.960146 -11.849117][-5.762733 -6.2146397 -6.6227045 -4.2106829 -2.8331065 0.37117958 2.6037006 4.478652 5.8645306 2.7856913 0.40289021 -8.4330778 -9.2446737 -11.855099 -12.4217][-6.4498916 -5.8987913 -5.2416177 -2.4864616 -1.090282 2.495038 5.2825441 5.8626556 5.56481 2.986167 1.9429913 -5.7421169 -7.06624 -11.198446 -13.180786][-7.1462789 -6.40211 -5.9514194 -3.3873777 -1.9881921 1.2444 3.1977119 3.82164 3.5997028 0.18212795 -1.5319037 -7.6007819 -7.0814075 -10.361285 -12.172993][-8.6168251 -8.73361 -7.9473729 -5.8509502 -4.253068 -1.8300066 -0.23878813 1.7692795 2.1877766 -1.6948414 -4.0497212 -10.188286 -9.5604391 -11.123214 -11.333012][-8.7751913 -9.7248974 -9.8391075 -8.0739422 -6.5497417 -4.4765558 -2.6265645 -2.4116225 -1.8765688 -3.7389524 -4.7494144 -10.478382 -10.872536 -11.942746 -11.889315][-8.197649 -9.3788595 -10.051306 -9.4255285 -8.8726559 -7.0439973 -5.1429977 -5.0259228 -5.330555 -6.9765964 -6.6347 -9.3584423 -10.059673 -11.138424 -11.2341][-9.2523794 -9.3404408 -8.6041327 -7.9941134 -8.1758223 -7.1892209 -5.7971954 -5.3999958 -5.6188121 -7.1266546 -7.5652881 -8.4114027 -7.52947 -8.426013 -8.9373875][-8.3395987 -8.7435961 -8.57523 -7.5192423 -7.187624 -6.1469021 -5.7559481 -5.5956535 -5.52236 -6.5143881 -7.008749 -7.7620749 -7.4457011 -6.4986267 -6.0386238][-7.5102615 -8.4810467 -8.7402515 -8.5710888 -7.7591248 -6.2842221 -5.8233018 -5.766602 -6.0648894 -6.8824372 -7.0806255 -7.3319535 -7.2433381 -6.5284586 -5.9916406]]...]
INFO - root - 2017-12-16 00:57:40.201162: step 81810, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 43h:42m:53s remains)
INFO - root - 2017-12-16 00:57:46.674078: step 81820, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 43h:41m:49s remains)
INFO - root - 2017-12-16 00:57:53.067181: step 81830, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 45h:16m:03s remains)
INFO - root - 2017-12-16 00:57:59.410840: step 81840, loss = 0.28, batch loss = 0.17 (13.0 examples/sec; 0.614 sec/batch; 42h:45m:07s remains)
INFO - root - 2017-12-16 00:58:05.764147: step 81850, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 45h:23m:22s remains)
INFO - root - 2017-12-16 00:58:12.215803: step 81860, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 44h:12m:13s remains)
INFO - root - 2017-12-16 00:58:18.599978: step 81870, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 45h:29m:24s remains)
INFO - root - 2017-12-16 00:58:25.061512: step 81880, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 46h:12m:27s remains)
INFO - root - 2017-12-16 00:58:31.492368: step 81890, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 45h:23m:05s remains)
INFO - root - 2017-12-16 00:58:37.895865: step 81900, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.625 sec/batch; 43h:31m:48s remains)
2017-12-16 00:58:38.447849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7952144 -4.0294056 -4.45015 -4.5874295 -4.4734578 -4.0012832 -3.5328689 -3.4397049 -3.6623907 -5.236876 -6.005784 -6.70329 -7.4596863 -7.7680664 -7.8974695][-3.8595963 -4.4571428 -5.0950851 -4.8543911 -4.2910118 -3.8764191 -3.4647574 -3.2413058 -2.8929582 -4.4163952 -5.57734 -6.5648532 -7.3725004 -7.6768603 -7.6973872][-4.4269686 -4.9952488 -5.4082603 -5.5381956 -5.1797504 -4.204607 -3.4664321 -3.3023858 -3.3268104 -4.7450819 -5.2922292 -6.1288815 -7.1164579 -7.5568142 -7.5612588][-6.4603257 -6.2196546 -6.4452491 -5.7208977 -4.2362413 -3.2932587 -2.7036805 -2.2477021 -2.0557451 -3.6595254 -4.4711781 -5.1459866 -6.0784974 -6.8245969 -7.0740085][-7.0800643 -6.696229 -6.0459175 -5.3265157 -4.322937 -2.3557253 -0.54798222 0.17236423 0.080747128 -1.6765885 -2.6163259 -3.4684982 -4.4965096 -4.8493919 -5.3582511][-7.9996719 -7.6508222 -7.1943808 -5.5319834 -3.3725657 -1.788693 -0.17062187 0.98548126 1.8623686 0.81426907 0.061786652 -0.84681273 -2.0683889 -2.9168358 -3.93529][-8.5887089 -8.4866371 -7.0928211 -5.1033292 -3.1952195 -1.5221667 0.36472702 1.5309267 2.4146795 1.3791981 0.98115253 0.58603573 -0.67885113 -1.3728008 -2.5380859][-8.8803253 -8.6325312 -7.7992244 -5.4352779 -2.2752771 -0.5979104 0.5505476 2.030757 3.4853334 2.2165051 1.0390873 0.32209206 -0.61483955 -1.5106349 -2.6281905][-8.3134031 -8.5689611 -7.9906964 -6.2444482 -4.5425029 -2.6572742 -0.091353893 1.3963432 2.0029345 1.1264191 1.1463432 0.24373627 -1.5532408 -2.5864229 -3.4292989][-8.542613 -8.4650917 -8.3075371 -6.9508605 -5.0024738 -3.3379264 -1.7215333 -0.25838375 1.1113052 0.1538763 -0.54194975 -1.4029822 -2.800344 -3.7894139 -4.5727119][-8.9988346 -8.7285614 -8.9756575 -8.4434042 -7.6393757 -6.2016783 -4.2117896 -3.2276583 -2.7966094 -3.5429063 -4.1145353 -4.703825 -5.9805164 -6.1590958 -6.5889764][-9.9715939 -9.8854008 -9.4976215 -9.02802 -8.785758 -7.9343982 -6.798347 -5.9785185 -4.8849277 -5.2250004 -6.3047767 -6.5699115 -7.3099051 -7.3193569 -7.41431][-9.5135508 -9.9576683 -10.291904 -10.273909 -9.3655262 -8.7820721 -8.2246761 -7.7956653 -7.1238813 -7.0964155 -6.996768 -6.7231493 -7.1871586 -6.8029637 -6.4544592][-8.0503159 -8.2191944 -8.7750769 -9.3079643 -9.34938 -8.9695663 -8.4016047 -8.0885315 -7.5501604 -7.2225308 -7.1513944 -6.9478688 -7.1121221 -6.8898473 -6.577651][-8.6595774 -8.262928 -7.9949532 -8.5221758 -9.1200514 -9.1771936 -9.1191254 -9.0342045 -8.6101465 -8.2057791 -7.9594684 -7.7191014 -7.6196771 -7.5847149 -7.6187234]]...]
INFO - root - 2017-12-16 00:58:44.808961: step 81910, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.643 sec/batch; 44h:43m:40s remains)
INFO - root - 2017-12-16 00:58:51.287277: step 81920, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 45h:15m:06s remains)
INFO - root - 2017-12-16 00:58:57.719058: step 81930, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 45h:47m:13s remains)
INFO - root - 2017-12-16 00:59:04.148662: step 81940, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 45h:52m:22s remains)
INFO - root - 2017-12-16 00:59:10.559695: step 81950, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 46h:26m:34s remains)
INFO - root - 2017-12-16 00:59:17.069389: step 81960, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 44h:44m:14s remains)
INFO - root - 2017-12-16 00:59:23.455658: step 81970, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 45h:12m:57s remains)
INFO - root - 2017-12-16 00:59:29.874499: step 81980, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:11m:23s remains)
INFO - root - 2017-12-16 00:59:36.234420: step 81990, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 43h:30m:33s remains)
INFO - root - 2017-12-16 00:59:42.635682: step 82000, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 44h:28m:42s remains)
2017-12-16 00:59:43.185597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0688553 -4.1754818 -4.3547239 -4.6047163 -4.4076781 -3.843878 -2.9776731 -2.4852948 -2.0691767 -3.470511 -3.7450557 -4.5454016 -5.6743307 -7.0158644 -7.1551571][-3.3319373 -3.4793811 -4.0592289 -4.0242095 -3.9010632 -3.4623556 -2.8776846 -2.2609916 -1.4959311 -2.9960861 -3.6501422 -4.4426241 -5.090621 -6.5491371 -7.0563159][-3.5110025 -3.1808133 -3.2659454 -3.4298701 -3.3698192 -3.0731406 -2.5670514 -2.3392892 -2.0783963 -3.3495049 -3.5828056 -4.2640581 -5.2260218 -6.7343616 -7.0173488][-3.9456027 -3.1320562 -3.2927766 -2.6939359 -2.175036 -2.0113339 -1.6232018 -1.1815834 -0.85621881 -2.6450987 -3.2898593 -3.9071472 -4.717773 -6.2622004 -6.6892872][-4.3489895 -3.2832665 -2.6553721 -1.8275332 -1.0366073 -0.64293909 -0.15215302 0.13321447 0.2441411 -1.4938631 -2.2093506 -3.3542376 -4.4764805 -5.9487243 -6.4858527][-3.90615 -2.7319784 -2.1220179 -1.1177764 -0.19682503 0.83438396 1.5714827 1.3850126 1.3795242 -0.49137783 -1.040246 -2.3975968 -3.961164 -5.4399543 -6.1155272][-3.5028524 -1.9812841 -0.60791779 0.44265556 1.0950556 2.4837742 3.5854826 3.3616676 2.8095179 0.27935886 -0.43026066 -1.5811348 -2.9694018 -4.9002252 -5.8022356][-2.5165949 -1.3679199 -0.20589066 1.3516493 2.3920994 3.2812347 3.8435011 4.2623577 4.2445593 1.1596632 -0.26147842 -1.7217693 -3.1818309 -4.8880038 -5.3415108][-2.2124834 -1.791203 -0.50958872 0.715745 1.6368351 2.1602297 2.260951 2.9658356 3.4813585 0.60134697 -0.73354912 -2.4004092 -3.8584933 -5.3455482 -5.6485934][-2.6502948 -2.4685187 -2.3078856 -0.84743738 0.2914443 0.49374771 0.374012 0.64408875 0.6908741 -1.4949656 -2.1242323 -3.7942955 -5.122736 -6.29481 -6.2505965][-4.7879844 -4.4918051 -4.3170462 -3.5506897 -3.0374165 -2.2493682 -1.8915038 -2.048419 -2.2677588 -4.1920071 -5.215991 -5.9086447 -6.6364856 -7.7546096 -7.46465][-7.0677094 -6.1865082 -5.9742742 -5.6358786 -5.0813246 -4.8365283 -4.8064308 -4.6343861 -4.5120363 -5.9673324 -6.7867403 -6.8479247 -7.16717 -8.068861 -7.7863903][-7.9917235 -7.7288837 -7.6689782 -7.353353 -7.0093193 -6.3561487 -5.9288306 -6.0352769 -6.1475029 -6.8543105 -7.4681792 -7.4260368 -7.6345077 -7.9649768 -7.4533129][-7.9118924 -7.9866714 -8.1512957 -7.9697318 -7.7219248 -7.1599069 -6.8033218 -6.7659016 -6.8300653 -7.2321625 -7.1488957 -7.2130332 -7.4497533 -7.59402 -7.0164371][-7.7875524 -7.3653994 -6.9636383 -7.2995491 -7.6981583 -7.338326 -7.0393867 -7.1644526 -7.4038653 -7.2166471 -6.8908138 -6.6374846 -6.565804 -6.6065726 -6.4784622]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 00:59:49.713514: step 82010, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 43h:49m:06s remains)
INFO - root - 2017-12-16 00:59:56.192685: step 82020, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 44h:26m:36s remains)
INFO - root - 2017-12-16 01:00:02.620008: step 82030, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 44h:04m:53s remains)
INFO - root - 2017-12-16 01:00:09.033735: step 82040, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 44h:59m:39s remains)
INFO - root - 2017-12-16 01:00:15.439529: step 82050, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 43h:23m:32s remains)
INFO - root - 2017-12-16 01:00:21.840710: step 82060, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 44h:15m:06s remains)
INFO - root - 2017-12-16 01:00:28.299189: step 82070, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 45h:32m:59s remains)
INFO - root - 2017-12-16 01:00:34.820355: step 82080, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:15m:16s remains)
INFO - root - 2017-12-16 01:00:41.163702: step 82090, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 43h:14m:32s remains)
INFO - root - 2017-12-16 01:00:47.553172: step 82100, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 43h:14m:59s remains)
2017-12-16 01:00:48.079300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2932243 -1.4887409 -1.9781375 -2.4585886 -3.0871973 -3.4033556 -3.5299277 -3.251667 -2.9777961 -3.8294511 -4.2581692 -5.2219667 -5.6308107 -6.453094 -6.8526726][-1.8980603 -2.12262 -2.5873895 -3.1347766 -3.7908514 -3.8710361 -3.9162538 -3.9407825 -3.914691 -4.8118334 -4.9616547 -5.9881716 -6.5270772 -7.3407545 -7.6085219][-2.2491693 -2.151917 -2.3342156 -2.5625334 -2.7427177 -2.714653 -2.7192726 -2.732214 -2.9442072 -3.8965979 -3.973166 -4.9193311 -5.3306932 -6.2492304 -6.9319563][-2.1814308 -2.0619268 -2.0517049 -1.9029412 -1.9873552 -1.7606339 -1.6598887 -1.6853681 -1.7585979 -3.035491 -3.5612755 -4.6145158 -5.2111216 -6.1652536 -6.4022436][-2.5545149 -2.3499231 -2.265399 -1.8357353 -1.4964933 -0.8828187 -0.37713766 -0.28380585 -0.29843998 -1.5550632 -2.2407804 -3.8270528 -4.6006074 -5.811244 -6.2700019][-3.4275699 -2.8616467 -2.3667159 -1.6647344 -1.0395389 -0.36648512 0.1450882 0.28144264 0.38108826 -0.95932531 -1.6439552 -2.9024949 -3.5272441 -4.8042569 -5.4376554][-3.4249077 -2.9025927 -2.3983626 -1.3326311 -0.61422157 -0.022823811 0.5202198 0.93481445 1.2319508 -0.06141901 -0.74912977 -2.1466818 -2.8664627 -4.2100215 -5.1146307][-3.0266809 -2.5543299 -2.1154079 -1.1458817 -0.24515724 0.45669174 0.96453094 1.3795004 1.8359308 0.65655804 -0.077960968 -1.5465479 -2.3912463 -3.9285946 -4.6857777][-3.3381333 -2.3714657 -1.9109111 -1.2748289 -0.64280939 0.16361523 0.80987072 1.1029434 1.4684734 0.38317871 -0.17600346 -1.6475062 -2.5554862 -4.1691818 -4.93813][-3.5223036 -2.8475313 -2.3301115 -1.5357437 -0.97227573 -0.41910934 0.032681942 0.43867016 0.81268311 -0.53180075 -1.1984744 -2.6629677 -3.5085845 -4.7354059 -5.4013805][-4.8142786 -4.3668294 -3.8623626 -3.1561356 -2.46551 -1.9028673 -1.6961865 -1.4346771 -1.047718 -2.3574629 -3.1754246 -4.308156 -4.9933462 -5.9673014 -6.2403159][-5.2924256 -5.0256763 -4.7597837 -4.1354284 -3.565011 -3.1073709 -2.8857832 -2.9956112 -3.0031357 -3.8027987 -4.3831825 -5.4524541 -5.8674316 -6.3832226 -6.4378939][-6.22783 -5.98906 -5.8165221 -5.2101536 -4.7478323 -4.4518418 -4.4323444 -4.30759 -4.1718388 -4.8212547 -5.2329693 -6.0802307 -6.3848062 -6.5032053 -6.0085163][-6.30621 -5.835638 -5.5120296 -5.1091146 -4.6755161 -4.537128 -4.5725088 -4.6650572 -4.7483816 -5.3984685 -5.5631452 -5.5933166 -5.609499 -5.7682261 -5.5546074][-7.43492 -7.0903816 -6.6168451 -6.0507784 -5.7584286 -5.6782589 -5.7027354 -5.7517662 -5.7723169 -5.9099345 -6.1008062 -6.2550197 -6.3171682 -6.0619159 -5.8506808]]...]
INFO - root - 2017-12-16 01:00:54.426059: step 82110, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 43h:45m:26s remains)
INFO - root - 2017-12-16 01:01:00.825278: step 82120, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 44h:49m:30s remains)
INFO - root - 2017-12-16 01:01:07.230044: step 82130, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 44h:56m:42s remains)
INFO - root - 2017-12-16 01:01:13.709127: step 82140, loss = 0.26, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 42h:56m:13s remains)
INFO - root - 2017-12-16 01:01:20.175091: step 82150, loss = 0.25, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 46h:44m:19s remains)
INFO - root - 2017-12-16 01:01:26.603855: step 82160, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:09m:38s remains)
INFO - root - 2017-12-16 01:01:33.002487: step 82170, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 44h:30m:18s remains)
INFO - root - 2017-12-16 01:01:39.409444: step 82180, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 43h:59m:30s remains)
INFO - root - 2017-12-16 01:01:45.806469: step 82190, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:45m:15s remains)
INFO - root - 2017-12-16 01:01:52.163574: step 82200, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 44h:17m:39s remains)
2017-12-16 01:01:52.731698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2692385 -7.3435779 -9.0541782 -10.357366 -10.944914 -10.973051 -10.008265 -8.6635237 -6.8445711 -6.7283673 -6.2525911 -5.7492428 -5.8703928 -6.9352989 -7.1802955][-5.8951435 -7.8652067 -10.10223 -11.792789 -12.761011 -12.7715 -12.4 -11.12569 -9.1693668 -8.942441 -8.1996346 -7.8664689 -8.2721949 -9.2308569 -9.0137863][-3.6521869 -5.7322416 -8.3953676 -10.165901 -11.721272 -12.181402 -12.049339 -11.157225 -9.7826624 -9.7505236 -9.8915787 -10.183523 -10.202818 -10.921845 -10.760266][-4.3854761 -5.7687135 -7.0798335 -8.2310085 -9.41 -9.4420862 -8.988678 -8.0233212 -6.8888903 -8.1276979 -8.8464031 -9.9283085 -10.969754 -12.169179 -11.887295][-5.0546942 -5.6230183 -5.8969536 -6.3030577 -6.4578156 -5.4444056 -4.6988 -3.7995186 -2.7414384 -4.7380314 -6.2352047 -8.3650064 -10.033436 -11.753947 -12.363515][-7.0795588 -6.6098089 -6.3557835 -5.1446447 -3.6121569 -1.6519747 -0.06714344 1.2774658 2.13097 -0.35551882 -2.743763 -5.4953833 -7.7735591 -10.27915 -11.44568][-6.2298813 -5.6518111 -4.7080603 -2.7436304 -0.36269426 2.2393684 5.0598125 6.3895464 6.209199 2.8380661 0.13559437 -2.6214113 -5.24002 -8.1525269 -9.4348764][-6.3950405 -5.6163144 -3.5980287 -0.644207 2.612813 5.276804 7.4891071 8.2109976 8.173111 4.8586025 1.6935234 -1.5891385 -4.4526606 -7.2944431 -8.1920223][-7.36409 -6.528039 -5.3022461 -2.5646863 0.18149233 2.4691772 5.0277205 5.859931 5.9807138 2.5536013 -0.5496254 -2.6403508 -4.6843252 -7.3019819 -7.9847956][-8.4456587 -7.4642944 -6.7555208 -4.5838881 -2.733953 -0.84891081 1.2351303 1.8606968 2.0346184 -1.4496217 -3.9696028 -5.8853846 -7.3740573 -8.3822746 -8.4746943][-10.594444 -10.350094 -9.8427668 -8.8283825 -7.4902425 -5.4235077 -3.5030489 -3.4289784 -3.1872172 -5.7833447 -7.1413612 -8.02974 -8.8205242 -9.8029165 -9.6408491][-11.305758 -11.261668 -11.19607 -10.874025 -10.306507 -8.7267389 -7.2799582 -7.0726137 -6.3597527 -7.942915 -8.0294781 -8.0649548 -8.50017 -9.2076569 -9.3830748][-10.480294 -11.011742 -11.795786 -11.62005 -11.090105 -10.237198 -8.9998674 -8.1970387 -7.8014278 -9.0345125 -8.9406452 -8.6202936 -8.37331 -8.51494 -8.71806][-9.1275768 -9.4228191 -9.7586708 -9.5062313 -8.748 -8.180316 -7.8544788 -7.5987539 -7.3774986 -8.0409613 -7.8580904 -7.7865992 -8.131628 -7.7960725 -7.5675092][-8.6156721 -9.2054472 -9.7134781 -9.8283806 -9.672389 -8.5461121 -7.9578481 -7.8947864 -7.65806 -7.8383012 -7.5553985 -7.1845794 -7.0433431 -7.462543 -7.6269245]]...]
INFO - root - 2017-12-16 01:01:59.020471: step 82210, loss = 0.36, batch loss = 0.25 (12.7 examples/sec; 0.631 sec/batch; 43h:53m:09s remains)
INFO - root - 2017-12-16 01:02:05.427939: step 82220, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 46h:20m:36s remains)
INFO - root - 2017-12-16 01:02:11.765029: step 82230, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 43h:23m:35s remains)
INFO - root - 2017-12-16 01:02:18.169984: step 82240, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 44h:41m:00s remains)
INFO - root - 2017-12-16 01:02:24.560280: step 82250, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 44h:49m:35s remains)
INFO - root - 2017-12-16 01:02:30.960708: step 82260, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 43h:43m:42s remains)
INFO - root - 2017-12-16 01:02:37.389328: step 82270, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.645 sec/batch; 44h:50m:53s remains)
INFO - root - 2017-12-16 01:02:43.795330: step 82280, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 44h:07m:02s remains)
INFO - root - 2017-12-16 01:02:50.158651: step 82290, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 45h:53m:58s remains)
INFO - root - 2017-12-16 01:02:56.617171: step 82300, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 44h:59m:09s remains)
2017-12-16 01:02:57.157629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7748165 -3.3591838 -3.661757 -3.5869865 -2.9959149 -2.7170343 -2.8539553 -3.0632429 -2.8558965 -4.8324003 -5.8479586 -6.3535385 -6.450963 -6.2625818 -7.0968676][-4.0332222 -4.2194576 -4.6326909 -4.4611473 -4.7135725 -4.6298895 -4.4839377 -3.7200234 -2.9875865 -5.0973492 -5.6556463 -6.6693182 -7.5431309 -7.0655589 -7.9702339][-4.7783742 -4.3564911 -3.8999708 -3.4890223 -3.0887775 -3.1139941 -3.3678293 -2.6595349 -2.148932 -3.5060391 -3.7434072 -5.1005449 -5.3132658 -5.5036516 -6.6102695][-4.0886316 -4.0534821 -3.5648303 -2.6270661 -2.4558954 -2.0937791 -1.6589689 -1.7343988 -1.3091226 -2.8398261 -3.5232863 -4.2819738 -4.8153553 -4.6394277 -5.2614107][-5.4085913 -3.6186509 -1.9543824 -1.4928584 -1.21982 -0.65125751 -0.41170311 -0.65698242 -0.85103846 -2.5756793 -2.9204531 -4.0876169 -4.8506894 -4.5056906 -5.7635546][-4.762785 -3.3245449 -2.3807054 -0.93232727 -0.22337866 0.55826187 1.2290382 0.83912182 0.65886211 -1.0278845 -1.8155026 -2.879755 -3.7646277 -4.0633841 -4.6711411][-4.5678058 -3.0672078 -1.7055116 -0.38777542 0.89414787 1.8824177 2.5036278 2.4660158 2.2067938 -0.0028066635 -0.77767754 -2.5545077 -4.1031408 -4.1216516 -5.6136618][-3.631484 -2.3918672 -1.4826145 0.49248791 1.9873428 2.85069 3.8058996 3.654233 3.2156944 1.222086 -0.039940357 -2.2807608 -3.8153167 -4.7627912 -5.8925877][-2.9827476 -1.833437 -1.0309143 0.16623449 1.5444975 2.4665813 2.763051 2.7244806 3.1739216 0.8377943 -0.52147245 -2.5272346 -4.5483961 -4.6553698 -6.1206436][-2.1048946 -1.8673692 -1.2793918 0.025753021 0.9143219 1.0704098 1.2966785 1.2914839 1.493413 -0.33556747 -1.5596671 -3.5059609 -4.887701 -5.5379887 -6.581214][-3.9460053 -3.3814192 -2.4196124 -1.8675232 -1.187007 -0.29483557 0.010769844 -0.71810055 -0.825315 -2.8595457 -3.8634462 -4.8014221 -6.0539174 -6.3470106 -7.2231536][-5.0170989 -4.4820404 -4.4517641 -3.3517609 -2.7664542 -2.7367206 -2.4253478 -2.0098562 -2.1529512 -3.7270505 -4.62908 -5.6127906 -6.0873561 -6.5427656 -7.4337945][-6.1441226 -5.6344142 -4.8154831 -4.3107157 -4.0064311 -3.9036119 -3.8844926 -4.3554726 -4.5021544 -5.7282529 -6.3992567 -6.3554659 -6.8637567 -6.9368372 -7.530488][-6.9152336 -6.2093973 -6.0703506 -5.499897 -4.4131694 -3.9518447 -3.8173137 -4.2074585 -4.7475123 -5.7987309 -6.12956 -6.4401193 -6.3914647 -6.1344376 -6.6826649][-6.6303911 -7.0120373 -7.2045197 -6.9194465 -6.7467937 -6.0794826 -5.7614384 -5.9166646 -6.0125194 -6.1365743 -6.6493006 -6.7889981 -6.9840894 -6.4090919 -5.990427]]...]
INFO - root - 2017-12-16 01:03:03.521981: step 82310, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 44h:31m:31s remains)
INFO - root - 2017-12-16 01:03:09.919522: step 82320, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.616 sec/batch; 42h:47m:53s remains)
INFO - root - 2017-12-16 01:03:16.289491: step 82330, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 44h:35m:36s remains)
INFO - root - 2017-12-16 01:03:22.689631: step 82340, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 44h:30m:40s remains)
INFO - root - 2017-12-16 01:03:29.065540: step 82350, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.621 sec/batch; 43h:07m:31s remains)
INFO - root - 2017-12-16 01:03:35.454022: step 82360, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 44h:12m:43s remains)
INFO - root - 2017-12-16 01:03:41.902497: step 82370, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 44h:36m:58s remains)
INFO - root - 2017-12-16 01:03:48.355340: step 82380, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 45h:40m:48s remains)
INFO - root - 2017-12-16 01:03:54.826962: step 82390, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 45h:25m:24s remains)
INFO - root - 2017-12-16 01:04:01.200181: step 82400, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 44h:15m:48s remains)
2017-12-16 01:04:01.716804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5089526 -4.4156113 -4.5376024 -4.9422774 -5.0132313 -4.2027855 -3.6948502 -3.56993 -3.0462356 -3.8647661 -4.7239313 -6.0812459 -7.4040446 -7.7721453 -8.1522455][-4.6223412 -4.4023695 -5.0466003 -5.48094 -5.6272192 -5.4061718 -5.0220041 -3.8988 -2.9420872 -3.9445083 -4.9238615 -6.28989 -7.6424985 -7.7560315 -7.8500161][-5.1758251 -4.8298273 -4.5613728 -4.477778 -4.1045232 -3.5903234 -2.9644032 -2.4539814 -2.0628057 -2.93924 -3.7380462 -5.4792695 -7.0128555 -7.1758986 -7.3685994][-4.2298508 -4.2588015 -4.1581993 -3.9086976 -3.1862183 -2.3400521 -1.3755946 -0.76194048 -0.62672853 -2.0542154 -3.2878265 -5.0382748 -6.5553303 -6.6182361 -6.7112641][-4.7547626 -4.0408592 -3.1962237 -2.3609385 -1.5328956 -0.6230402 0.27010679 0.61885452 0.675519 -1.0588117 -2.7697968 -4.4485159 -5.8678575 -6.1237173 -6.5097089][-4.5325518 -4.1074085 -3.5127301 -2.0593204 -0.38818455 0.861042 1.9297686 2.3527069 2.3386612 0.20039845 -1.7163916 -3.6794562 -5.4282484 -5.5811734 -5.5508404][-5.0787306 -4.0886297 -2.8545423 -1.1526122 0.53923988 1.8784113 2.7769852 3.0143232 2.6838369 0.39957714 -1.4882073 -4.038208 -5.5956578 -5.5774488 -5.7460728][-4.6623974 -4.2617378 -3.484427 -1.6897306 -0.1893425 1.4694195 2.2990084 2.31738 2.2316656 0.087976456 -1.7922935 -4.2129431 -5.7194142 -5.8983474 -5.8633175][-4.98403 -4.264502 -3.561625 -2.2130446 -0.91876745 0.3827734 1.1704855 1.4845915 1.4818296 -0.55026722 -2.03028 -4.016551 -5.4607611 -5.639678 -5.8299046][-5.3612804 -4.9879866 -4.4326921 -3.3914094 -2.1838083 -0.82462978 -0.14324903 0.14257431 -0.085683346 -1.7536077 -3.0836153 -4.6220646 -5.5906644 -5.5994215 -5.4511337][-6.0398722 -5.849617 -5.5718746 -4.7304497 -3.7268937 -2.6316328 -2.0122285 -1.8919005 -2.1685419 -3.2239008 -3.9932566 -5.1772604 -5.8271141 -5.7326584 -5.66368][-6.41625 -6.3362846 -6.0621443 -5.7724171 -5.19967 -4.5435886 -4.0311656 -3.8219054 -3.7558036 -4.5614376 -5.2448525 -5.7898121 -5.861866 -5.6751137 -5.6127758][-6.5529432 -6.51241 -6.7372875 -6.4972186 -6.1251435 -5.7670612 -5.2857018 -5.3282776 -5.3360834 -5.7721405 -5.9806051 -6.3989539 -6.6809111 -6.5742507 -6.4268794][-6.8269711 -6.552547 -6.4598975 -6.1063561 -5.5461826 -5.3446426 -5.4587245 -5.7337995 -5.8615532 -6.0869212 -5.99163 -6.0095267 -5.8557968 -5.8114719 -6.096189][-7.5760336 -7.7699375 -7.7180109 -7.3694181 -7.3445044 -7.1012516 -6.7725325 -6.7663803 -6.8426032 -7.0745039 -7.0842414 -7.0598121 -7.1693158 -7.0324616 -6.7520022]]...]
INFO - root - 2017-12-16 01:04:08.164276: step 82410, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 45h:22m:31s remains)
INFO - root - 2017-12-16 01:04:14.537182: step 82420, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 44h:22m:57s remains)
INFO - root - 2017-12-16 01:04:21.107128: step 82430, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 46h:09m:29s remains)
INFO - root - 2017-12-16 01:04:27.521759: step 82440, loss = 0.28, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 42h:49m:03s remains)
INFO - root - 2017-12-16 01:04:33.871734: step 82450, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 44h:19m:16s remains)
INFO - root - 2017-12-16 01:04:40.385179: step 82460, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:12m:11s remains)
INFO - root - 2017-12-16 01:04:46.781488: step 82470, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:44m:02s remains)
INFO - root - 2017-12-16 01:04:53.207536: step 82480, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 46h:30m:39s remains)
INFO - root - 2017-12-16 01:04:59.614655: step 82490, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:16m:23s remains)
INFO - root - 2017-12-16 01:05:06.124116: step 82500, loss = 0.28, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 46h:30m:50s remains)
2017-12-16 01:05:06.798052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6341949 -3.9015052 -4.1166182 -4.1383853 -4.2107267 -4.0306082 -3.6636548 -3.7669079 -3.7277892 -4.6986866 -4.6744194 -5.4720936 -5.8114686 -6.0148792 -7.2576175][-3.7106719 -4.1221476 -4.246613 -4.0371237 -4.11948 -3.95282 -3.8903253 -4.1910791 -3.979768 -5.0685549 -5.1954174 -6.1531482 -6.8340893 -7.1643457 -8.1257238][-3.464735 -2.9626398 -2.8757911 -2.3537955 -2.0981393 -2.3077574 -2.3136954 -2.647799 -3.0022097 -4.0784464 -4.3687887 -5.4918008 -6.0569544 -6.2568974 -7.4604559][-2.2291942 -1.4725614 -0.80086946 -0.40622091 -0.392107 -0.38739729 -0.32941389 -0.8735013 -1.2098026 -2.6604519 -3.3359394 -4.2193217 -5.0729 -5.2264709 -6.4381166][-1.4984207 -1.0167332 -0.2281642 0.29560661 0.14801073 0.12311697 0.33643532 0.061727524 -0.28153706 -1.451077 -1.9814882 -2.9682178 -3.653326 -4.2186055 -5.8221273][-1.2688804 -0.53409243 -0.068749905 0.50306225 0.53342915 0.6023283 0.87157917 0.56187248 0.29153919 -0.977118 -1.2315497 -1.9710379 -2.615675 -3.1727781 -4.743968][-1.1988201 -0.4494648 0.29305172 1.055872 1.2183752 1.2496729 1.54881 1.2715197 1.088974 -0.21755171 -0.69168711 -1.6813846 -2.2922816 -2.8441567 -4.4475513][-0.80058765 0.031082153 0.76109695 1.5778141 2.0519314 1.9717398 2.0639277 1.8731575 1.8585014 0.4073534 -0.13668346 -1.3512554 -2.2087741 -2.6627717 -4.0968857][-1.138165 -0.39349604 0.13286352 0.55459404 1.1468248 1.3128233 1.2951593 1.3224583 1.5797262 0.24308205 -0.51145124 -1.8261747 -2.6718941 -3.200285 -4.64528][-2.3951364 -1.8777552 -1.4660397 -0.69003487 -0.23395681 0.003267765 0.26171017 0.48385525 0.89337444 -0.19599581 -1.0443368 -2.5037937 -3.5060053 -3.7984328 -5.2311144][-3.8410966 -3.3704109 -2.7110281 -2.2566681 -1.6399083 -1.6139836 -1.3605475 -1.2060018 -1.1723495 -2.3771133 -3.061727 -3.9541192 -4.6767941 -4.7124968 -5.6383433][-4.9835839 -4.5284281 -4.6500211 -4.5953846 -4.287673 -3.8499708 -3.6640182 -3.8614323 -3.6632657 -4.5093527 -4.7545075 -5.33111 -5.4842896 -5.5707645 -6.0216441][-6.6310606 -6.4340372 -6.255064 -6.0454264 -6.0933642 -5.9767466 -5.764843 -5.4472852 -4.9749117 -5.3874559 -5.2739038 -5.6341748 -5.6961312 -5.6431971 -5.8932247][-7.3342571 -7.3171082 -7.2185841 -6.945714 -6.5988369 -6.5900278 -6.3924551 -6.2843738 -6.4058013 -6.8536444 -7.0050254 -6.6552081 -6.21032 -5.61674 -4.8644886][-7.4042 -6.984695 -7.0880256 -7.1806479 -7.4469585 -7.4386845 -7.287529 -7.308053 -7.707942 -7.6163149 -7.6267281 -7.1837554 -6.8411942 -6.3536973 -5.5278292]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-82500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-82500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 01:05:14.394833: step 82510, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 45h:04m:34s remains)
INFO - root - 2017-12-16 01:05:20.739338: step 82520, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 44h:28m:46s remains)
INFO - root - 2017-12-16 01:05:27.132304: step 82530, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 43h:53m:09s remains)
INFO - root - 2017-12-16 01:05:33.534088: step 82540, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:44m:36s remains)
INFO - root - 2017-12-16 01:05:39.923872: step 82550, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 44h:39m:08s remains)
INFO - root - 2017-12-16 01:05:46.306572: step 82560, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 43h:11m:59s remains)
INFO - root - 2017-12-16 01:05:52.739721: step 82570, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 44h:04m:36s remains)
INFO - root - 2017-12-16 01:05:59.169936: step 82580, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 45h:27m:52s remains)
INFO - root - 2017-12-16 01:06:05.662220: step 82590, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:42m:11s remains)
INFO - root - 2017-12-16 01:06:12.175103: step 82600, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 43h:47m:14s remains)
2017-12-16 01:06:12.767070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8988216 -4.6527085 -5.3477535 -5.3102574 -5.3122678 -5.2934351 -5.1851892 -4.4891005 -3.6103601 -3.6187596 -4.568965 -5.7770286 -7.6354761 -9.0767736 -9.6256618][-4.151495 -4.725018 -5.5296917 -6.0179977 -6.9009371 -6.1649523 -5.1371088 -3.9398222 -3.5830522 -4.7814856 -6.49035 -7.9531746 -9.4367495 -10.355003 -10.540743][-4.1446095 -4.7152309 -5.3246989 -5.1835494 -4.6646109 -4.7735338 -4.3320189 -3.8206677 -2.7127609 -2.9840899 -5.0101833 -7.2383952 -9.5495977 -10.43019 -10.404388][-4.3284349 -4.7500477 -4.8889952 -4.7990856 -4.2706327 -3.0286322 -1.6250458 -0.85334349 -0.059409618 -1.2847996 -3.18262 -6.1154618 -8.616437 -10.414918 -10.604853][-4.7401609 -4.7769938 -4.3975182 -3.9184225 -2.6413751 -0.72951126 1.1177263 2.0934906 2.3737764 0.731658 -1.4822998 -4.191474 -7.3667035 -8.6047878 -9.0398216][-4.895812 -4.308651 -3.7812991 -2.7945409 -1.6128907 0.091960907 2.2223454 4.0575142 4.4064093 2.7749996 -0.16915607 -3.3573966 -6.0459251 -7.3194394 -7.8486919][-5.6391621 -4.9883809 -3.4690528 -1.9892688 0.10886002 1.8188515 3.7683353 5.0026112 5.7055721 4.0993423 1.0662746 -2.4953814 -6.2186 -7.7720795 -7.8396749][-4.376277 -3.7078788 -2.8634005 -1.0241113 1.1966696 3.589056 5.9513588 6.7080116 6.0838432 3.9267588 0.99945831 -2.2634063 -5.9454641 -7.6962447 -7.9813871][-4.6509962 -3.8457842 -2.80023 -1.541326 0.11573935 2.2175579 4.3825035 5.3119621 5.6042166 3.191411 0.25099087 -2.6043596 -5.6212978 -7.5341043 -8.18258][-4.326911 -4.3028212 -4.150382 -2.755444 -1.3407512 0.76752186 2.2665691 2.93085 3.2382717 0.84627819 -1.60882 -4.6213026 -7.016449 -8.0694418 -8.3762856][-5.9114904 -5.7231522 -5.8480992 -4.9742489 -3.7190177 -1.7644763 -0.24595928 -0.097545624 0.087545872 -1.7197418 -3.6409631 -5.8783951 -7.8980532 -9.0721016 -9.1275349][-6.8122883 -6.8845448 -6.9838142 -6.882894 -6.1666574 -4.9655495 -4.0137358 -3.3300323 -3.0466113 -4.2220807 -4.8226023 -6.3753781 -7.849308 -8.9719448 -9.5170784][-7.9710693 -7.8642073 -8.1112986 -7.9729438 -7.3715267 -7.0017486 -6.5602427 -6.4965677 -6.0798507 -6.307488 -7.0218186 -7.86151 -8.3687572 -8.7879028 -8.9914541][-8.6906338 -8.9854126 -8.747016 -8.4083576 -7.5660625 -7.0803266 -6.6746926 -7.0027189 -6.8723187 -7.4860678 -7.5573192 -7.5622149 -7.878171 -7.781352 -7.8111506][-7.8741369 -7.9550958 -8.5030375 -8.3717594 -8.05256 -7.5942874 -7.1600037 -6.8100014 -6.921453 -6.9915504 -7.3706532 -7.4389081 -7.1552458 -7.1436396 -6.9550915]]...]
INFO - root - 2017-12-16 01:06:19.240456: step 82610, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 43h:52m:05s remains)
INFO - root - 2017-12-16 01:06:25.589438: step 82620, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 43h:19m:51s remains)
INFO - root - 2017-12-16 01:06:32.063202: step 82630, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 44h:04m:30s remains)
INFO - root - 2017-12-16 01:06:38.452546: step 82640, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 44h:32m:58s remains)
INFO - root - 2017-12-16 01:06:44.911449: step 82650, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 44h:34m:34s remains)
INFO - root - 2017-12-16 01:06:51.328988: step 82660, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 43h:58m:07s remains)
INFO - root - 2017-12-16 01:06:57.684981: step 82670, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 43h:38m:09s remains)
INFO - root - 2017-12-16 01:07:04.070954: step 82680, loss = 0.31, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 45h:51m:15s remains)
INFO - root - 2017-12-16 01:07:10.513141: step 82690, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 45h:55m:22s remains)
INFO - root - 2017-12-16 01:07:16.887119: step 82700, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 45h:48m:06s remains)
2017-12-16 01:07:17.418679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9493747 -5.2972989 -5.4446359 -5.0737667 -4.7628469 -3.7877173 -3.3775897 -3.0582576 -2.4583116 -3.7993462 -3.9707236 -4.654274 -5.1655073 -6.2854791 -6.9104066][-4.2612572 -4.129652 -3.7120287 -3.6431336 -4.0350432 -3.7429357 -2.9902363 -3.2753205 -3.5006838 -5.2784195 -5.6561546 -6.5860972 -6.82992 -7.5621138 -8.1842184][-3.0151639 -3.2805924 -4.0264482 -3.8462691 -4.0019646 -4.232326 -3.9529083 -3.8454137 -3.8101366 -5.7497091 -6.182116 -7.4068484 -8.1045647 -8.61896 -8.8925352][-3.4644203 -3.4005065 -3.2620292 -2.8882527 -2.959908 -2.6180344 -2.5613351 -2.6365809 -3.0218039 -4.7500577 -5.5953131 -7.0470681 -7.750443 -8.8250742 -9.2135468][-2.9148145 -2.6846447 -2.4815779 -2.1051259 -1.5004659 -0.38022566 0.71380997 0.27721214 -0.6692729 -2.9206648 -3.666894 -5.2376051 -6.3630986 -7.7449155 -8.6997728][-3.507997 -2.2079763 -1.4053054 -0.27731323 0.76526642 1.7106867 2.2935696 2.6496496 2.5012817 -0.13634539 -2.1051631 -4.2433014 -5.406702 -6.9268932 -8.1701412][-2.807919 -2.0147614 -1.0620084 0.43444824 1.9647951 3.11837 4.1847582 4.4926653 4.368515 2.1579437 0.6529274 -1.8988867 -4.2061162 -6.3276014 -7.7278123][-1.7321978 -1.0790029 -1.0018134 0.057941437 1.5303078 3.3152666 4.4142447 4.509304 4.5569239 2.4566555 0.98529148 -1.2697825 -3.1948376 -5.5719366 -7.0309029][-2.2624235 -1.5363517 -1.1579218 -0.35546827 0.37513065 0.88314152 2.2600031 3.2442684 3.411046 1.6753044 0.48465919 -1.2003379 -2.9004288 -5.1703157 -6.324831][-2.5622082 -2.6752276 -2.2888575 -1.300046 -0.85620022 -0.47713184 0.25660467 0.823349 1.4964094 0.54264545 -0.92947149 -3.0475597 -4.0631762 -5.1788454 -6.2141657][-3.9538424 -4.0026 -3.8448775 -3.4221144 -2.805769 -2.523581 -1.9791498 -2.1230164 -1.9779282 -2.844296 -3.7391484 -5.05956 -6.111793 -7.2371235 -7.368444][-4.4830742 -3.891896 -4.4110088 -4.2681246 -4.0667324 -3.961935 -3.8523822 -3.9516313 -3.9128888 -4.9218636 -5.221159 -5.7264657 -6.5523224 -7.7731371 -8.3244429][-7.0820589 -6.2201595 -5.5327272 -5.3804736 -5.4166746 -5.1001406 -4.9156752 -5.2450838 -5.5400553 -6.1155887 -6.2029638 -6.6570807 -6.6998138 -7.1177835 -7.5850511][-6.7066884 -6.0289392 -6.1374969 -5.9227653 -5.5797715 -5.35454 -5.1065159 -5.2268991 -5.15576 -5.4223957 -5.7524805 -5.795929 -6.2202148 -6.238925 -5.6740637][-6.2274694 -6.0936527 -5.5829144 -5.2477112 -5.5526028 -5.54267 -5.2898588 -5.1439595 -5.0224657 -4.6772084 -4.8791637 -5.1302667 -4.9808669 -5.3871393 -5.3701129]]...]
INFO - root - 2017-12-16 01:07:23.866470: step 82710, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 43h:25m:40s remains)
INFO - root - 2017-12-16 01:07:30.410167: step 82720, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 44h:01m:08s remains)
INFO - root - 2017-12-16 01:07:36.878483: step 82730, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 45h:43m:28s remains)
INFO - root - 2017-12-16 01:07:43.295823: step 82740, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:17m:36s remains)
INFO - root - 2017-12-16 01:07:49.770107: step 82750, loss = 0.36, batch loss = 0.24 (12.4 examples/sec; 0.645 sec/batch; 44h:43m:53s remains)
INFO - root - 2017-12-16 01:07:56.246718: step 82760, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 44h:49m:32s remains)
INFO - root - 2017-12-16 01:08:02.694027: step 82770, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.651 sec/batch; 45h:11m:30s remains)
INFO - root - 2017-12-16 01:08:09.024755: step 82780, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 44h:02m:49s remains)
INFO - root - 2017-12-16 01:08:15.384481: step 82790, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 44h:43m:49s remains)
INFO - root - 2017-12-16 01:08:21.814720: step 82800, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 43h:43m:06s remains)
2017-12-16 01:08:22.431920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8060794 -3.6818466 -3.8658092 -3.478209 -3.03378 -3.2732935 -3.6191764 -3.8226025 -4.1467619 -4.4184904 -4.7697258 -6.3397436 -6.7561417 -7.4931254 -8.2997][-3.7940834 -4.1213531 -3.9155424 -3.7221639 -3.8074713 -3.3662767 -2.7575264 -3.2356257 -4.5751557 -4.9824352 -5.0508895 -6.5327144 -6.72198 -7.5409985 -8.3492947][-4.0087452 -4.0548515 -4.0666666 -3.5282607 -2.8762999 -2.3620362 -1.9270701 -2.29707 -2.5290766 -3.1363235 -4.6200857 -6.1540728 -5.897378 -6.9094081 -7.7759752][-4.6937208 -4.5726919 -4.232161 -3.6859322 -2.8012619 -1.660749 -0.70944691 -0.74484968 -1.1264791 -1.877255 -2.9337649 -4.9011097 -5.4102068 -6.3648958 -7.3627186][-5.5498095 -5.1438036 -4.7985058 -3.9064429 -2.9611959 -1.8150907 -0.50304508 -0.37114334 -0.64428091 -1.4211526 -2.3187985 -4.2728596 -4.8191319 -6.124289 -7.1649766][-7.2011356 -5.94997 -4.7283421 -3.7116654 -2.6015215 -0.66408157 0.41993046 0.048727512 0.043241978 -1.0862179 -1.6937995 -3.3683782 -3.6334143 -4.6627326 -5.8978434][-7.3702869 -6.1206846 -5.17183 -3.3375511 -1.5644817 0.42829227 1.8412085 1.8487082 1.4346771 -0.43138933 -1.4009452 -3.15667 -3.2600708 -3.9304209 -4.6283183][-6.6888485 -6.4223251 -5.1065421 -3.1347928 -1.3691187 0.614851 2.0073442 2.7351303 2.6771145 1.0073061 -0.24379206 -2.5656805 -3.0577812 -3.3959355 -4.227807][-7.5723238 -6.5385075 -6.0111551 -4.2144623 -1.9247618 0.035868168 1.464469 1.8114214 1.8624525 0.6945591 -0.808589 -3.2673388 -3.6275411 -3.7845175 -4.2015548][-9.1800508 -8.3210373 -7.1360765 -5.4426937 -3.9364929 -1.9801736 -0.23564291 0.48218822 0.81286716 -0.58012581 -1.8401599 -4.1183968 -4.8155127 -5.2716146 -5.5940132][-10.386804 -9.9336786 -9.5536518 -7.853292 -6.0518651 -4.5254226 -3.1393251 -2.3103542 -2.0094943 -3.042829 -3.6691709 -5.4169121 -6.1105657 -6.356061 -6.3849306][-9.5847769 -9.3915138 -9.2979259 -8.2010336 -7.2669134 -5.6822996 -3.8113155 -3.4548717 -3.3471794 -3.7869949 -4.3267341 -5.4243779 -5.6507692 -6.2662416 -6.5927215][-10.452528 -9.9607038 -9.6729374 -9.058506 -8.7986517 -7.75517 -6.5002422 -6.3582325 -5.8276691 -6.0011015 -6.4891157 -7.0048981 -6.8754978 -7.0114856 -6.7534728][-9.9943714 -9.4782257 -8.8624449 -7.8268604 -7.199564 -6.7471666 -6.2730942 -6.5144014 -6.3389068 -6.5228262 -6.4597883 -5.8340654 -5.570528 -6.0602422 -6.1860976][-10.647403 -10.500024 -10.266745 -9.262886 -8.2606087 -7.2410736 -6.5352921 -6.9711981 -7.3076453 -7.8356037 -7.5935569 -6.6859856 -6.0835242 -5.61765 -5.1300836]]...]
INFO - root - 2017-12-16 01:08:28.772586: step 82810, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 45h:13m:59s remains)
INFO - root - 2017-12-16 01:08:35.197099: step 82820, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 45h:06m:57s remains)
INFO - root - 2017-12-16 01:08:41.648129: step 82830, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:41m:06s remains)
INFO - root - 2017-12-16 01:08:48.010791: step 82840, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 45h:32m:22s remains)
INFO - root - 2017-12-16 01:08:54.378742: step 82850, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 44h:25m:55s remains)
INFO - root - 2017-12-16 01:09:00.803770: step 82860, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 44h:18m:26s remains)
INFO - root - 2017-12-16 01:09:07.173570: step 82870, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 43h:48m:42s remains)
INFO - root - 2017-12-16 01:09:13.547661: step 82880, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 44h:33m:21s remains)
INFO - root - 2017-12-16 01:09:19.883257: step 82890, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 45h:11m:58s remains)
INFO - root - 2017-12-16 01:09:26.391172: step 82900, loss = 0.37, batch loss = 0.26 (11.9 examples/sec; 0.670 sec/batch; 46h:26m:02s remains)
2017-12-16 01:09:26.928730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1135197 -3.0198717 -3.0092187 -2.8813758 -2.5237222 -2.2374454 -1.808033 -1.2253857 -0.86088943 -2.5443006 -3.7323215 -5.2795916 -6.3004837 -7.3538895 -7.9342055][-3.6481109 -3.8971779 -4.1095943 -3.9296196 -3.7081914 -3.0296121 -2.5970106 -2.1599588 -2.0898728 -3.5248146 -4.3224306 -5.7455211 -6.7878556 -7.7000842 -8.2404146][-3.6089673 -3.5117054 -3.7929871 -3.8721232 -3.5851717 -2.7341123 -2.0996046 -1.7790732 -1.720335 -3.0730166 -4.0274458 -5.4829779 -6.2953568 -7.1086063 -7.6107345][-3.9663341 -3.8887644 -4.0697632 -3.8094277 -3.2894335 -2.6087117 -1.8181992 -1.4330664 -1.1396103 -2.5139141 -3.282701 -4.6015778 -5.5835714 -6.3710742 -6.7319522][-4.0044451 -3.5747585 -3.3954988 -3.4083214 -2.9572129 -2.0210834 -1.1262617 -0.83710718 -0.51897526 -1.872705 -2.5817547 -3.921622 -5.0133028 -5.8361845 -6.4279323][-4.0857277 -3.5572433 -3.1555858 -2.3551493 -1.4537735 -0.6826911 -0.12817097 0.056992531 0.27230072 -1.2328639 -2.1105037 -3.5186443 -4.6322136 -5.6138086 -6.3053303][-4.3623972 -3.6561217 -2.9189754 -1.8344631 -0.81758976 0.22317028 0.82002258 0.96565342 1.0418606 -0.611928 -1.6970763 -3.271884 -4.4295096 -5.3917971 -5.9740973][-4.8360376 -3.8627796 -2.7388058 -1.3139167 0.0039916039 1.2382002 1.8424931 2.0473175 2.0795441 0.20303535 -1.1115074 -2.9458795 -4.3736429 -5.4756308 -6.0150332][-4.68819 -3.7618697 -2.758739 -1.3932619 -0.19101858 0.88511276 1.5381737 1.7871656 1.8921785 0.0099697113 -1.316658 -2.9197106 -4.3731565 -5.654717 -6.246202][-4.7096672 -3.9933262 -2.8073301 -1.5110664 -0.3729167 0.50179768 0.83254051 0.99247932 1.0013456 -0.81378746 -1.9246531 -3.5536513 -4.9342918 -5.917038 -6.6663027][-6.0941353 -5.4449039 -4.6340747 -3.7240236 -2.6891437 -1.677011 -1.1784148 -1.0563474 -1.1411428 -2.9115129 -4.0752311 -5.2770128 -6.2429533 -6.8873916 -7.1180854][-7.100709 -6.5170617 -5.6434512 -4.7384448 -3.9595582 -3.2064872 -2.9157567 -2.8159146 -2.874774 -3.9766698 -4.7223473 -5.8846464 -6.5697284 -7.0688028 -7.1736159][-7.4002986 -7.2144465 -6.7478595 -6.2030191 -5.4342632 -4.8602891 -4.5930238 -4.6907415 -4.8411336 -5.3205109 -5.6471295 -5.98944 -6.2961831 -6.516984 -6.3233547][-7.0232372 -6.8238449 -6.7942343 -6.2746472 -6.1435823 -5.5589418 -5.4166284 -5.2347479 -5.057189 -5.5272841 -5.7606821 -5.7656736 -5.8759737 -5.9721479 -6.0535622][-8.0104694 -7.6865854 -7.1135817 -6.9313455 -7.0547791 -6.7670608 -6.6973333 -6.7789507 -6.9003997 -6.6670275 -6.4235797 -6.4420166 -6.3835812 -6.2054629 -6.0762548]]...]
INFO - root - 2017-12-16 01:09:33.358685: step 82910, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 44h:00m:15s remains)
INFO - root - 2017-12-16 01:09:39.785450: step 82920, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 44h:57m:36s remains)
INFO - root - 2017-12-16 01:09:46.230310: step 82930, loss = 0.27, batch loss = 0.15 (11.6 examples/sec; 0.687 sec/batch; 47h:39m:07s remains)
INFO - root - 2017-12-16 01:09:52.594708: step 82940, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 44h:25m:54s remains)
INFO - root - 2017-12-16 01:09:58.979178: step 82950, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 43h:55m:14s remains)
INFO - root - 2017-12-16 01:10:05.383964: step 82960, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 44h:34m:43s remains)
INFO - root - 2017-12-16 01:10:11.758001: step 82970, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 43h:36m:39s remains)
INFO - root - 2017-12-16 01:10:18.219499: step 82980, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 45h:22m:22s remains)
INFO - root - 2017-12-16 01:10:24.632720: step 82990, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 44h:19m:51s remains)
INFO - root - 2017-12-16 01:10:31.008161: step 83000, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 45h:20m:28s remains)
2017-12-16 01:10:31.533848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6328049 -7.0674238 -6.6846933 -6.3207297 -5.6563034 -4.6987448 -3.8623061 -3.5490613 -3.127933 -4.8591833 -5.6140366 -6.4697771 -7.3731332 -8.4043522 -9.3117847][-7.3915596 -6.9590073 -6.4527731 -6.0173206 -5.479414 -4.7989483 -3.5815458 -3.3454943 -3.2863135 -5.2549429 -6.138896 -6.7959561 -7.5972333 -8.61286 -9.3952131][-6.7339869 -5.67888 -5.66323 -5.1054206 -4.3833818 -3.6634045 -2.7465515 -2.5656376 -2.46695 -4.6799555 -5.9105449 -6.6057596 -7.4468012 -8.4585819 -9.0212212][-6.350142 -5.7413707 -4.9193816 -3.8312433 -2.7734294 -1.6139822 -0.734952 -0.875298 -1.0719147 -3.1148162 -4.35086 -5.5495 -6.747201 -7.3364248 -7.8443956][-6.734293 -5.3332844 -3.9790494 -2.8565459 -1.4643087 0.017976761 0.77634907 0.51246738 0.18095064 -1.9905324 -3.1642814 -4.1624737 -5.4712529 -6.3724756 -6.8993425][-6.4844546 -5.5207138 -3.9297535 -1.6047087 -0.29300117 1.2504778 2.5019073 2.3783684 1.8044682 -0.54515457 -1.8642392 -2.9822392 -4.3860931 -5.4229941 -6.2336431][-5.7607641 -3.5840039 -1.6187057 -0.087193966 1.3077984 3.0737667 4.3749 4.2657146 3.8154373 1.4489832 -0.063433647 -1.273191 -2.6415744 -3.8863225 -4.6189065][-4.2933607 -3.1182528 -1.7689381 0.84030247 2.676753 3.5650053 4.4324341 4.8997602 4.56847 2.3266554 1.2388535 0.084673882 -2.1440682 -3.8768632 -4.4454341][-5.1250868 -2.8656111 -1.2120361 0.29490852 1.5047293 2.4659157 3.416441 3.5245647 3.1695795 0.90727806 -0.10625219 -0.96759367 -2.8772092 -4.962698 -5.9748678][-5.9216728 -4.5193448 -2.6985888 -0.28536272 1.1774778 1.7383623 2.319396 2.5550442 2.5275202 0.078579426 -0.84917116 -1.8114433 -3.5884628 -5.0605555 -5.7715058][-7.4128437 -6.14862 -4.5499172 -2.1204805 -0.93087387 0.037899494 0.51279259 0.72289753 0.7613802 -1.3467875 -2.7542772 -3.614924 -4.5496583 -5.6736135 -6.3071327][-7.7825141 -6.4785709 -4.9692788 -3.6401687 -2.2302136 -1.7021413 -1.5845871 -1.6570244 -1.8885183 -2.9990587 -4.1519794 -5.0573068 -5.8574829 -6.6827078 -7.223918][-7.8383894 -6.9881039 -6.3615894 -5.5627851 -4.4380407 -3.2865958 -2.6083798 -2.6430573 -2.1553583 -3.6046453 -4.9773288 -5.3767223 -5.7282562 -6.2024722 -6.6684189][-8.2762012 -7.720942 -7.07353 -6.4893112 -5.4792943 -4.4239521 -3.6628261 -3.0464578 -2.6128058 -3.195725 -3.6515775 -4.45202 -4.6116514 -5.032012 -5.6900163][-8.4025116 -8.2057247 -8.1628752 -7.9538021 -7.0902719 -5.7713223 -4.8032255 -4.4672089 -3.7819388 -3.3297539 -4.2744894 -4.8966885 -4.7615428 -4.1228065 -4.3608546]]...]
INFO - root - 2017-12-16 01:10:37.902434: step 83010, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:35m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 01:10:44.334410: step 83020, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 44h:24m:36s remains)
INFO - root - 2017-12-16 01:10:50.682900: step 83030, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 43h:19m:31s remains)
INFO - root - 2017-12-16 01:10:57.113239: step 83040, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 43h:58m:25s remains)
INFO - root - 2017-12-16 01:11:03.626319: step 83050, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:13m:27s remains)
INFO - root - 2017-12-16 01:11:10.051014: step 83060, loss = 0.35, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 44h:32m:18s remains)
INFO - root - 2017-12-16 01:11:16.492295: step 83070, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.629 sec/batch; 43h:36m:46s remains)
INFO - root - 2017-12-16 01:11:22.862149: step 83080, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 44h:43m:07s remains)
INFO - root - 2017-12-16 01:11:29.227245: step 83090, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 44h:03m:27s remains)
INFO - root - 2017-12-16 01:11:35.662215: step 83100, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 43h:35m:32s remains)
2017-12-16 01:11:36.212741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0094514 -8.3807392 -9.7469063 -10.523686 -10.772862 -10.789009 -10.071178 -8.5777931 -6.552598 -6.4643931 -4.8566 -5.86429 -6.71727 -6.7347522 -7.0598826][-7.2003846 -8.8602362 -10.74224 -12.38098 -12.918419 -12.858362 -11.942644 -10.278025 -8.1884413 -8.4187679 -7.0260615 -7.9093103 -8.7509766 -8.2382278 -7.3761883][-6.7135506 -8.0817108 -10.374333 -11.770777 -12.120911 -12.201799 -11.34654 -10.287325 -8.6972942 -8.871973 -7.7200637 -8.90132 -9.4192648 -9.3981743 -8.82678][-8.3002443 -8.72451 -10.208002 -10.565863 -10.340587 -9.6651125 -8.5410585 -7.5656147 -6.7379065 -7.9296017 -7.6455593 -9.6320305 -10.619711 -10.275456 -9.4489479][-8.37075 -8.2353239 -9.0845556 -8.2281618 -7.0741749 -5.0728178 -3.2144613 -2.8392901 -2.8560157 -4.9472513 -5.3661537 -7.8491488 -9.652833 -10.211337 -9.5239906][-9.511735 -8.2976952 -7.5286527 -5.7547164 -4.0078964 -0.72116089 1.9035015 2.3859606 2.322505 -1.1756225 -2.8145223 -5.9765329 -8.5207415 -9.0381975 -8.7669888][-8.0572882 -7.6068034 -6.7166543 -3.7913003 -1.225986 1.6704798 4.1907578 4.905426 4.9072733 1.1458254 -0.60513449 -3.9191461 -6.5945458 -7.31246 -7.3630309][-7.13051 -6.0503039 -5.0240946 -2.2890663 0.0920248 2.6100397 4.7256947 5.4996653 5.4610052 1.524766 -0.030311584 -3.2538466 -5.4527178 -6.0916042 -6.2954178][-6.8715715 -6.49559 -5.9990039 -3.7435551 -2.1965842 -0.22705936 1.5082359 2.5017271 2.6277437 -0.66191387 -1.8630748 -4.5436497 -6.063961 -6.474535 -6.1389818][-8.5703964 -8.31581 -7.8500929 -5.9205112 -4.9815245 -3.7814662 -2.5125546 -1.069695 -1.1808162 -4.5914721 -5.4998178 -7.4507775 -8.2373085 -7.9225259 -7.0979548][-10.218177 -10.191976 -10.005373 -8.3328829 -7.4204125 -6.7550225 -6.0889606 -5.6735487 -5.55204 -7.5258918 -7.6109166 -8.9490337 -9.821497 -9.4229708 -8.3570766][-10.926119 -10.740018 -11.06972 -10.347736 -9.436142 -8.1244965 -7.2656107 -7.1151633 -6.9251814 -8.1994743 -8.0510235 -8.1329012 -8.4601841 -8.6560535 -8.2357979][-11.133205 -11.040209 -11.336229 -10.271461 -9.3327179 -8.6545963 -8.1476707 -7.7298856 -7.4129906 -7.9842749 -7.6659522 -7.730073 -8.211689 -8.1324167 -7.6960568][-8.7904377 -8.7986012 -9.0240564 -8.3663511 -7.5461774 -6.3886361 -5.662569 -5.6920066 -5.8119345 -6.6543493 -6.8574514 -6.8990436 -7.2250128 -6.9820528 -6.9546609][-7.9506197 -8.0455351 -8.4146538 -7.9850564 -7.5636039 -6.6227508 -6.0830231 -6.08132 -6.3144484 -6.4215326 -6.7576728 -7.0471263 -7.10478 -7.0483088 -6.9556694]]...]
INFO - root - 2017-12-16 01:11:42.676748: step 83110, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 44h:05m:31s remains)
INFO - root - 2017-12-16 01:11:49.024535: step 83120, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 43h:38m:32s remains)
INFO - root - 2017-12-16 01:11:55.369409: step 83130, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 43h:42m:16s remains)
INFO - root - 2017-12-16 01:12:01.854129: step 83140, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 45h:11m:25s remains)
INFO - root - 2017-12-16 01:12:08.227152: step 83150, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 43h:22m:08s remains)
INFO - root - 2017-12-16 01:12:14.620541: step 83160, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 44h:55m:57s remains)
INFO - root - 2017-12-16 01:12:21.028424: step 83170, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 44h:21m:09s remains)
INFO - root - 2017-12-16 01:12:27.469146: step 83180, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 45h:51m:28s remains)
INFO - root - 2017-12-16 01:12:33.878228: step 83190, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 44h:03m:15s remains)
INFO - root - 2017-12-16 01:12:40.258725: step 83200, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 43h:55m:26s remains)
2017-12-16 01:12:40.746329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0669632 -4.9900718 -4.8034868 -4.3226447 -4.0577087 -4.1032052 -4.1280613 -3.5304027 -3.0351291 -4.0946765 -4.829505 -5.80017 -6.4062457 -7.1348619 -8.0095224][-4.2528644 -4.410491 -4.4189138 -4.6072383 -4.5921087 -4.1196594 -3.9262371 -3.7729244 -3.7657466 -4.8743906 -5.5515785 -6.294868 -6.505075 -6.9345107 -7.4924908][-2.6554403 -2.8212752 -3.0651426 -3.265646 -3.4467831 -3.6853094 -3.573205 -3.1700048 -2.9723463 -4.580935 -5.5036273 -5.9748058 -6.3266711 -6.8641629 -7.3553648][-1.2894802 -1.1396565 -1.0677314 -1.330204 -1.5021992 -1.6198497 -1.5338621 -1.6981258 -1.745573 -3.2791286 -4.3236818 -5.5233321 -6.2503085 -6.8164949 -7.2531486][-1.2360907 -0.99240017 -0.6517272 -0.54624987 -0.327837 -0.14867115 0.11189556 -0.0073633194 -0.35832119 -2.4088879 -3.8381624 -5.1979012 -6.1224141 -6.8668151 -7.3442078][-2.2906356 -1.5551248 -0.72998238 -0.24037313 0.23051882 0.74398136 1.2003994 1.0687141 0.93349552 -1.0735979 -2.4702244 -4.040349 -5.2456584 -6.2715034 -7.1253524][-2.9184442 -2.2458329 -1.2879844 0.018614292 0.9095459 1.54282 2.0763779 2.072156 1.9485397 -0.18060064 -1.7500534 -3.6433139 -5.0492506 -6.3524094 -7.3689408][-3.5496602 -2.5795274 -1.4432802 0.11749315 1.3719378 1.960288 2.2951126 2.285656 2.134943 -0.063669682 -1.7468486 -3.6453481 -5.1494179 -6.5143929 -7.5387568][-4.0483141 -3.2232022 -2.0955172 -0.62239504 0.35208511 0.94030476 1.3568316 1.5243998 1.6076698 -0.37438869 -1.8768559 -3.7728353 -5.2333975 -6.5680189 -7.5479126][-4.5255313 -3.7555664 -2.7345557 -1.6160622 -0.87891483 -0.28763914 0.071187973 0.32397079 0.42127037 -1.6956301 -2.8383489 -4.2146997 -5.2607551 -6.6256437 -7.637269][-4.9509892 -4.3353281 -3.772259 -2.84021 -2.1145453 -1.7238383 -1.534894 -1.3723173 -1.2715154 -3.2821336 -4.382278 -5.4500504 -6.3242488 -7.1819468 -7.5588732][-4.9241648 -4.4776258 -3.9359534 -3.4402518 -3.0683193 -2.80379 -2.7154245 -2.8009982 -2.7358346 -4.2089291 -5.0516315 -6.0432906 -6.7519164 -7.49347 -7.8681674][-5.4101033 -4.7968178 -4.26081 -4.0566583 -3.7637725 -3.6295362 -3.8412468 -4.0581527 -4.07318 -5.0912781 -5.5890093 -6.3171644 -6.7744675 -7.2228274 -7.2949986][-4.693347 -4.2594008 -3.882997 -3.5354657 -3.3222694 -3.314991 -3.5090265 -3.9510922 -3.954247 -4.7489414 -5.33084 -5.6390748 -5.7922449 -6.0892992 -5.8955431][-5.401782 -4.7234745 -4.1425776 -3.8570471 -3.744379 -3.8955781 -4.3459997 -4.8686419 -5.2687373 -5.59422 -5.7498093 -5.9363232 -5.9069977 -5.88885 -5.71828]]...]
INFO - root - 2017-12-16 01:12:47.194574: step 83210, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 43h:18m:32s remains)
INFO - root - 2017-12-16 01:12:53.599883: step 83220, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 45h:39m:53s remains)
INFO - root - 2017-12-16 01:13:00.028619: step 83230, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 45h:55m:10s remains)
INFO - root - 2017-12-16 01:13:06.403357: step 83240, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 45h:11m:37s remains)
INFO - root - 2017-12-16 01:13:12.841541: step 83250, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 44h:24m:21s remains)
INFO - root - 2017-12-16 01:13:19.324390: step 83260, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 43h:31m:42s remains)
INFO - root - 2017-12-16 01:13:25.721309: step 83270, loss = 0.31, batch loss = 0.20 (12.1 examples/sec; 0.660 sec/batch; 45h:41m:37s remains)
INFO - root - 2017-12-16 01:13:32.175073: step 83280, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 44h:24m:22s remains)
INFO - root - 2017-12-16 01:13:38.621225: step 83290, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 44h:28m:58s remains)
INFO - root - 2017-12-16 01:13:44.997810: step 83300, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 43h:08m:56s remains)
2017-12-16 01:13:45.515080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7005863 -4.3252811 -4.3406157 -5.6756582 -6.0220218 -5.3274193 -4.2559357 -3.6341934 -2.8609233 -2.9471798 -4.3610191 -5.6893845 -6.6035228 -6.9880033 -7.2109709][-4.24693 -4.9099483 -5.1975183 -5.1384563 -4.8214488 -5.5916796 -5.4858584 -4.5464287 -3.3981738 -4.0926971 -4.8594708 -4.9063339 -5.4150672 -6.2929 -7.3381734][-5.2518029 -5.6716194 -5.375494 -5.2636876 -5.3220987 -5.1839848 -4.8831673 -4.3364468 -4.0527658 -4.7905188 -5.108614 -5.5924063 -5.5822668 -5.8542333 -6.39968][-3.6311841 -3.8988514 -4.6825314 -3.868299 -3.3442812 -3.4795666 -3.6181493 -3.0368586 -2.1791964 -3.1002927 -4.0196919 -4.8972778 -5.12184 -5.726923 -6.3901978][-4.5860572 -2.7269669 -1.2666855 -0.68297148 -1.4455409 -1.2945447 -0.2803297 -0.12607098 0.148808 -0.666574 -2.0081735 -3.6020207 -4.6617217 -5.2607861 -5.678566][-3.6418266 -2.6103048 -0.95387411 0.50362492 1.8262262 2.4950657 2.5361748 1.7492514 1.6377745 0.731472 -0.83264828 -2.4471684 -3.3910308 -4.4109697 -4.9560146][-2.2150669 -1.2573671 0.35661602 1.7648163 2.9612541 4.1122437 4.6066656 4.4733181 4.204752 2.2464046 -0.44284058 -2.0140271 -2.6383495 -3.6652226 -4.7301579][-1.6799231 -0.25253439 0.53161144 1.7863703 3.0732107 3.9674549 4.3454256 4.8682775 5.180109 3.54391 1.0404015 -1.0689521 -3.0049181 -4.1492357 -4.8897648][-2.8775635 -1.1740994 -0.60619545 0.44164085 1.2572527 1.6992254 2.1720676 2.8965988 3.280673 1.8423958 -0.29948235 -2.0268488 -4.038476 -4.9157867 -5.4843411][-5.0077453 -3.7146056 -2.7108507 -0.98985815 -0.15871191 -0.082132816 0.19494724 0.38381481 0.37240314 -0.92027426 -2.8811994 -3.914711 -5.0061994 -5.5802307 -6.1303239][-6.3517637 -6.1952138 -5.1601267 -3.5736308 -2.6735606 -1.850667 -1.0686102 -1.0949354 -1.481854 -2.6735468 -4.3880939 -5.2502432 -5.894701 -5.8857403 -6.2268829][-9.04385 -8.1522579 -7.3732824 -6.7651873 -6.0794234 -5.2350264 -4.4157448 -4.1180754 -3.8192341 -4.863111 -6.3578005 -6.954052 -7.4813509 -7.3274837 -7.1556344][-10.422356 -10.716141 -9.6564245 -8.3609724 -7.7399549 -7.1528249 -6.3979306 -6.2654939 -5.9049082 -6.5550303 -7.39801 -7.1660557 -7.2522254 -6.8760715 -6.9222908][-11.105691 -10.879013 -9.858345 -9.1902628 -8.1681824 -7.6037025 -7.2982826 -7.13498 -6.6534 -6.6384063 -7.2048664 -6.9698472 -6.6606007 -5.555728 -5.1374445][-10.68295 -10.17594 -8.7226171 -8.495739 -8.44114 -8.0659552 -7.1066718 -6.844666 -6.9837637 -7.0475359 -7.1116438 -6.7641211 -6.339366 -6.2559137 -5.7124615]]...]
INFO - root - 2017-12-16 01:13:52.005317: step 83310, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 43h:41m:18s remains)
INFO - root - 2017-12-16 01:13:58.447115: step 83320, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 45h:03m:24s remains)
INFO - root - 2017-12-16 01:14:04.939997: step 83330, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 45h:05m:56s remains)
INFO - root - 2017-12-16 01:14:11.337689: step 83340, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 46h:19m:43s remains)
INFO - root - 2017-12-16 01:14:17.827992: step 83350, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 45h:53m:08s remains)
INFO - root - 2017-12-16 01:14:24.320270: step 83360, loss = 0.27, batch loss = 0.16 (11.8 examples/sec; 0.675 sec/batch; 46h:44m:05s remains)
INFO - root - 2017-12-16 01:14:30.754968: step 83370, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 44h:01m:48s remains)
INFO - root - 2017-12-16 01:14:37.120306: step 83380, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 43h:15m:00s remains)
INFO - root - 2017-12-16 01:14:43.591195: step 83390, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:35m:38s remains)
INFO - root - 2017-12-16 01:14:50.055755: step 83400, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 43h:47m:22s remains)
2017-12-16 01:14:50.604505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8810415 -2.6755185 -2.4240346 -1.9629502 -1.5796537 -1.4600048 -1.1360998 -1.0107312 -1.1063385 -2.7069874 -3.8215723 -5.5415978 -6.9657259 -7.8618016 -8.9676008][-3.395781 -3.3398061 -3.2930193 -3.0634356 -2.4553523 -1.7626057 -1.5805421 -1.3015761 -1.099823 -2.7231884 -4.14577 -6.1305161 -7.7384186 -8.6916323 -9.7819386][-4.0519705 -3.7697709 -3.344985 -2.6984568 -1.5789237 -0.78158951 -0.09918642 0.068006039 0.0023174286 -1.8013196 -3.2382336 -5.2864237 -6.8507977 -7.7565594 -8.9041767][-4.5086584 -3.8539319 -3.2919197 -2.2894268 -0.95930195 0.28386497 1.1943293 1.5947962 1.6225262 -0.11839771 -1.7194843 -3.7364578 -5.547864 -6.8819809 -8.3885612][-4.9547939 -3.9843233 -2.8387527 -1.3914151 0.083935738 1.2256231 2.2609386 2.6945028 2.744257 1.1185894 -0.42829895 -2.6263771 -4.4003916 -5.816947 -7.4188514][-4.1483278 -3.1030326 -1.7516069 -0.18908882 1.3357439 2.5174313 3.355628 3.5456581 3.5261974 1.8646879 0.2711668 -1.9494414 -3.63384 -4.6039906 -5.9027181][-2.9052029 -1.8511219 -0.55626631 1.0083466 2.4488716 3.5446615 4.3098135 4.3037529 4.170825 2.3185129 0.67242241 -1.5876355 -3.303781 -4.3512392 -5.6266274][-2.3916965 -1.2743258 -0.038873672 1.5449286 2.7491627 3.7453737 4.3761053 4.4495296 4.3279448 2.2452984 0.47681808 -1.9911289 -3.9003646 -4.7863483 -5.5819907][-1.8741388 -1.1399903 -0.22667265 1.1277494 2.2491503 2.8693075 3.2727594 3.329505 3.2124119 1.5545149 0.13751698 -2.2385993 -4.1443024 -4.9410758 -5.7118535][-2.4454117 -1.7189469 -0.87981606 0.17748833 1.2745237 2.0880127 2.431509 2.36053 2.289752 0.54645157 -0.77986526 -2.6269526 -4.0878439 -4.8405285 -5.6614914][-3.3093514 -2.9275637 -2.4434314 -1.5848546 -0.78911304 -0.039202213 0.22018576 0.29554176 0.16204309 -1.1528249 -2.1013203 -3.6851687 -4.9305038 -5.4692688 -5.793715][-4.5228176 -4.1975837 -3.4330206 -2.9738121 -2.5392451 -2.33817 -2.16545 -2.1868606 -2.1603465 -2.6915655 -3.2597275 -4.2842245 -5.1673846 -5.7407932 -6.2976403][-4.9555855 -4.8649235 -4.5594645 -4.2737393 -4.0214424 -3.9183111 -3.8514888 -3.7264531 -3.5486012 -3.8642566 -4.0003319 -4.5667095 -5.2206669 -5.5917139 -6.1752167][-5.1964908 -5.3140016 -5.1397109 -5.2342157 -5.459754 -5.3303676 -5.2648859 -4.93402 -4.5706053 -4.6921754 -4.5287228 -4.7385712 -4.9495068 -5.3816071 -5.9666696][-6.3225412 -6.152957 -5.7690773 -6.1436973 -6.4817305 -6.6503162 -6.8216286 -6.555954 -6.0557861 -5.6885195 -5.6449308 -5.7039409 -5.7352605 -5.8470268 -6.0281606]]...]
INFO - root - 2017-12-16 01:14:56.987292: step 83410, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 43h:20m:41s remains)
INFO - root - 2017-12-16 01:15:03.348245: step 83420, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 44h:36m:32s remains)
INFO - root - 2017-12-16 01:15:09.769821: step 83430, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 43h:58m:50s remains)
INFO - root - 2017-12-16 01:15:16.169760: step 83440, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.648 sec/batch; 44h:50m:01s remains)
INFO - root - 2017-12-16 01:15:22.626979: step 83450, loss = 0.25, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 46h:44m:59s remains)
INFO - root - 2017-12-16 01:15:29.041612: step 83460, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 44h:34m:27s remains)
INFO - root - 2017-12-16 01:15:35.468831: step 83470, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:07m:18s remains)
INFO - root - 2017-12-16 01:15:42.001915: step 83480, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 46h:15m:29s remains)
INFO - root - 2017-12-16 01:15:48.459572: step 83490, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 43h:35m:35s remains)
INFO - root - 2017-12-16 01:15:54.908947: step 83500, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 44h:28m:03s remains)
2017-12-16 01:15:55.478173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.678081 -6.1766162 -5.4756842 -5.2149544 -5.0603094 -4.5107708 -4.2708588 -3.7355802 -3.2293711 -3.5030041 -4.6448288 -6.0098524 -6.5078964 -7.8075624 -8.1536531][-5.5839667 -5.2256231 -4.863121 -4.895637 -4.9640064 -4.5517826 -4.3895388 -3.9277914 -3.6713095 -4.043396 -5.0932226 -6.492816 -7.2934 -8.4275293 -8.0096054][-4.4908381 -4.3960009 -4.2459068 -4.0674338 -3.7876701 -3.5791626 -3.565907 -3.296968 -3.2529492 -3.79195 -4.8669028 -5.893527 -6.687964 -7.6955538 -7.4511771][-3.6065931 -3.3354449 -3.2001247 -2.79138 -2.3801599 -2.0880098 -1.8118329 -1.5840626 -1.6897879 -2.2551241 -3.1868625 -4.4302464 -5.7176733 -6.8383341 -7.2798014][-3.7980058 -3.1035166 -2.2984619 -1.6534052 -0.8762517 -0.49528503 -0.27993822 -0.28888988 -0.34866238 -0.91416645 -1.9559278 -3.3054061 -4.3922205 -5.9073453 -6.860486][-3.178009 -3.0062742 -2.2297306 -1.255867 -0.45315504 0.15853453 0.64684963 0.45087337 0.40195751 0.035523415 -1.0114293 -2.3419647 -3.5165648 -5.1310225 -6.0460429][-2.8373084 -2.667829 -1.8060164 -0.92415667 -0.37114811 0.1563096 0.52748013 0.49563313 0.57301807 0.13963795 -0.94385052 -2.1673388 -3.4456015 -5.0746822 -6.2478943][-3.3511 -2.8697581 -1.7604032 -0.81904316 -0.21017838 0.4217577 0.950263 0.83354568 0.93404007 0.29522085 -0.78938961 -2.1326413 -3.5483351 -5.1770968 -6.3643427][-3.9549093 -3.3753428 -2.296082 -1.3989649 -0.97705555 -0.10425329 0.47859955 0.686841 0.9791708 0.2568469 -0.70707941 -2.3199577 -3.6060753 -5.1061945 -6.1466665][-4.1876316 -3.3857408 -2.1978645 -1.6049061 -1.3665867 -0.77416039 -0.50605011 0.013198853 0.2936964 -0.22265196 -1.2696962 -3.0459208 -4.1096764 -5.295496 -6.011673][-6.3496857 -5.3512869 -4.2283316 -3.4813404 -3.0874314 -2.6689525 -2.2957582 -1.963387 -1.9040771 -2.0963225 -3.0123692 -4.2738595 -5.0826912 -5.9945707 -6.42536][-7.0505161 -5.93631 -5.3544106 -4.6062846 -4.1415491 -3.7775948 -3.37422 -3.2847381 -3.2515898 -3.1710854 -3.9941549 -4.5955014 -4.9655519 -5.5101309 -6.0903993][-7.6329269 -6.55758 -5.7593231 -5.1139994 -4.5499973 -3.9651837 -3.75016 -3.5851288 -3.3591495 -3.3088603 -3.9805055 -3.8980606 -3.9508998 -4.3480997 -4.3517904][-7.6159067 -6.8722095 -6.1763592 -5.6480036 -4.915822 -4.0307274 -3.3348193 -3.176919 -2.9557619 -2.3965673 -3.1155586 -3.0175595 -2.8823996 -3.2896013 -3.5251231][-7.9361205 -7.3391061 -6.7346153 -5.8239737 -5.06678 -4.1931753 -3.7021856 -3.3722844 -2.9505811 -2.6079807 -2.4577694 -2.34899 -2.6021404 -3.305829 -3.6058521]]...]
INFO - root - 2017-12-16 01:16:01.872183: step 83510, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 44h:41m:32s remains)
INFO - root - 2017-12-16 01:16:08.358927: step 83520, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 43h:16m:31s remains)
INFO - root - 2017-12-16 01:16:14.765069: step 83530, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 44h:19m:12s remains)
INFO - root - 2017-12-16 01:16:21.123266: step 83540, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 43h:43m:29s remains)
INFO - root - 2017-12-16 01:16:27.507009: step 83550, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 45h:30m:45s remains)
INFO - root - 2017-12-16 01:16:33.896578: step 83560, loss = 0.34, batch loss = 0.23 (12.8 examples/sec; 0.626 sec/batch; 43h:17m:23s remains)
INFO - root - 2017-12-16 01:16:40.391087: step 83570, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 46h:45m:29s remains)
INFO - root - 2017-12-16 01:16:46.812174: step 83580, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 44h:56m:36s remains)
INFO - root - 2017-12-16 01:16:53.154198: step 83590, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 44h:36m:15s remains)
INFO - root - 2017-12-16 01:16:59.545404: step 83600, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:52m:31s remains)
2017-12-16 01:17:00.098839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0061407 -3.7763476 -3.8829687 -3.9635751 -4.1350861 -4.2068577 -4.12901 -4.1256123 -4.0704069 -4.5650511 -4.7851944 -5.0330114 -5.664741 -6.2554679 -7.0641007][-3.5647936 -3.6292686 -3.4967127 -3.5922403 -3.8875024 -4.018755 -4.0761089 -4.1914287 -4.3091841 -5.4810104 -5.7818379 -6.1290665 -7.1637759 -7.4537358 -8.0225782][-3.7198627 -2.9835205 -2.334547 -2.0892072 -2.106545 -2.2975793 -2.4229965 -2.7752461 -3.526917 -4.6231785 -4.826551 -5.2109933 -5.8985496 -6.5402813 -7.2197585][-2.5972414 -1.6281962 -0.8440361 -0.5836072 -0.99183321 -1.0709286 -1.0089364 -1.5156641 -1.9736385 -3.2320046 -3.7121246 -3.9357042 -5.04352 -5.6914883 -6.3881359][-1.9725742 -1.04743 -0.23567247 -0.049842358 -0.43393421 -0.23481226 0.28671265 -0.045239925 -0.68844604 -1.9705067 -2.3544965 -2.7376161 -3.7069 -4.5008731 -5.715651][-1.7040291 -0.61058378 0.17118454 0.37628365 0.17832613 0.37315369 0.92122746 0.55918884 0.22059631 -0.9823494 -1.2010603 -1.2922702 -2.1797051 -3.1786113 -4.5231161][-1.4324741 -0.55533266 0.37010765 0.81678867 0.929966 1.0289621 1.3911734 1.2436571 1.1435738 -0.091344357 -0.63469696 -0.86372757 -1.6487541 -2.6631656 -4.1826315][-1.3681755 -0.538404 0.40694714 1.1568451 1.6513662 1.6871891 1.7437811 1.7972689 1.9059582 0.30289841 -0.29798794 -0.68312025 -1.6179228 -2.6242285 -4.1630125][-1.5753942 -0.867064 -0.18240786 0.2177434 0.92488575 0.91991043 0.8266964 1.0441694 1.3679152 0.13345623 -0.78241396 -1.5047665 -2.3744545 -3.2661386 -4.6702747][-2.9240789 -2.4200253 -1.8801241 -1.1932325 -0.58020258 -0.24830151 -0.17801428 0.09305191 0.54002666 -0.71905565 -1.6772885 -2.393209 -3.1308193 -3.9727631 -5.4935246][-4.3794613 -3.8543832 -3.2857823 -2.8665152 -2.0911193 -1.7938185 -1.6422343 -1.4680643 -1.2718554 -2.6233673 -3.5371633 -4.2367921 -4.7613363 -5.2619138 -6.229743][-5.5426822 -5.12282 -5.0243096 -5.1538954 -4.5297012 -4.13365 -3.8677423 -4.1446209 -4.1204195 -4.9630527 -5.5295935 -5.583436 -5.9061193 -6.13234 -6.6366196][-6.5503531 -6.6259193 -6.6505632 -6.58817 -6.4256835 -6.1872129 -6.0425029 -5.6542253 -5.3275294 -5.7183075 -5.8515329 -6.04871 -6.0752516 -5.6896229 -5.5433645][-7.1320562 -7.2135563 -7.3575716 -7.1460834 -7.0173512 -7.025425 -6.7250576 -6.77946 -6.9223924 -7.0661983 -7.3217163 -6.7970295 -6.0368528 -5.1173515 -4.4845018][-6.8231826 -6.780426 -6.9689617 -7.065351 -7.3168874 -7.4476266 -7.4356756 -7.7938075 -7.8802238 -7.8380103 -7.6642208 -7.2770934 -6.8866053 -6.2626982 -5.1387415]]...]
INFO - root - 2017-12-16 01:17:06.576147: step 83610, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 44h:01m:14s remains)
INFO - root - 2017-12-16 01:17:12.964287: step 83620, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 43h:19m:29s remains)
INFO - root - 2017-12-16 01:17:19.316199: step 83630, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 43h:45m:24s remains)
INFO - root - 2017-12-16 01:17:25.805347: step 83640, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 43h:24m:46s remains)
INFO - root - 2017-12-16 01:17:32.217657: step 83650, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 45h:15m:02s remains)
INFO - root - 2017-12-16 01:17:38.616879: step 83660, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 44h:12m:55s remains)
INFO - root - 2017-12-16 01:17:44.960869: step 83670, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 44h:31m:07s remains)
INFO - root - 2017-12-16 01:17:51.370252: step 83680, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 43h:06m:09s remains)
INFO - root - 2017-12-16 01:17:57.766289: step 83690, loss = 0.32, batch loss = 0.21 (11.9 examples/sec; 0.673 sec/batch; 46h:31m:20s remains)
INFO - root - 2017-12-16 01:18:04.252630: step 83700, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 44h:15m:01s remains)
2017-12-16 01:18:04.834014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7178841 -4.9416409 -5.8054438 -6.3594851 -6.4569831 -5.9878483 -5.3833447 -4.8185511 -4.2259541 -5.4831505 -4.9581976 -6.7332482 -7.1155071 -9.1578255 -10.22309][-4.3123875 -5.5184255 -6.9959435 -7.42469 -7.9764395 -6.8521743 -5.3462014 -4.5846133 -3.6085329 -6.3268085 -7.0484514 -9.1584044 -9.49268 -11.086198 -11.0829][-4.2241335 -5.1437635 -6.2527413 -6.30715 -6.08725 -5.6939163 -4.92608 -3.6844039 -2.5244446 -4.1662211 -5.0905361 -8.33317 -9.7195129 -11.140797 -11.343777][-4.3893614 -4.7992792 -5.3597245 -5.245389 -4.6718125 -3.1640468 -1.5370297 -1.2946229 -0.522346 -2.5239968 -3.3715606 -6.2329769 -8.31573 -10.457899 -10.992722][-4.5488372 -4.2709293 -4.23071 -3.7411182 -2.9051943 -0.80233 1.362154 2.4750748 2.3856468 -0.43581533 -1.5675588 -4.9861097 -7.1243653 -9.0915251 -9.8954468][-5.0118742 -4.169354 -3.3689213 -2.1406054 -0.85408115 1.077095 3.2796001 4.4513273 4.5659666 1.6564093 -0.42819071 -3.9677985 -5.999527 -7.8337579 -8.6798992][-5.44996 -4.2412643 -3.4971452 -1.28546 0.87723732 2.58634 4.4784184 5.2507868 5.3800278 2.477663 0.51433754 -3.6690845 -6.0982552 -8.08569 -8.6223783][-5.2050314 -4.28227 -3.3827596 -0.69907522 1.5408859 4.0570993 5.9558525 5.9212074 5.6070805 2.1340809 0.58461285 -3.4097929 -6.0050635 -8.3904028 -8.7689867][-5.6148338 -4.6339197 -4.2278433 -1.9860578 0.17404842 2.4244776 4.399828 5.2862358 5.1080389 1.374465 -0.30326748 -3.7502012 -5.8665762 -8.5375738 -9.2686729][-5.7883182 -5.7136149 -5.716116 -4.1843338 -2.5743585 -0.34520531 1.6273432 2.0885086 2.6129704 -1.0075603 -2.8608007 -5.80753 -7.2113419 -9.105257 -9.5199957][-7.1642022 -7.1880522 -7.5229759 -6.834331 -5.5908012 -3.2224426 -1.6135888 -1.0543976 -0.70304775 -3.6347585 -4.6329679 -7.6184125 -8.7628441 -9.9992981 -10.145651][-8.0887785 -8.0924129 -8.429903 -8.2533617 -7.7204146 -6.410284 -4.8929758 -4.2822561 -4.049377 -5.2992334 -5.7247238 -7.5357127 -8.2967691 -9.9913368 -10.178536][-9.8393641 -9.3232021 -8.89571 -8.83048 -8.4540281 -7.8008413 -7.3785057 -7.3176293 -6.6254959 -6.7487736 -6.7291164 -7.82645 -8.0503531 -8.9226265 -8.9186563][-10.284584 -10.165602 -9.7298393 -8.9533873 -8.2944384 -7.5133715 -6.8537965 -7.2317476 -7.6856441 -7.6102791 -7.1818943 -7.5207038 -7.4123011 -7.9833875 -7.4826288][-8.8748989 -9.5202274 -9.6387815 -9.9840775 -9.5264463 -8.31127 -7.6739507 -7.522562 -7.4769611 -7.80407 -7.86857 -7.93317 -7.3335567 -7.1460781 -6.9987879]]...]
INFO - root - 2017-12-16 01:18:11.256874: step 83710, loss = 0.36, batch loss = 0.24 (12.5 examples/sec; 0.640 sec/batch; 44h:14m:18s remains)
INFO - root - 2017-12-16 01:18:17.656490: step 83720, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 43h:34m:15s remains)
INFO - root - 2017-12-16 01:18:24.251167: step 83730, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 45h:53m:33s remains)
INFO - root - 2017-12-16 01:18:30.664515: step 83740, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 44h:39m:05s remains)
INFO - root - 2017-12-16 01:18:37.060356: step 83750, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 45h:22m:42s remains)
INFO - root - 2017-12-16 01:18:43.469942: step 83760, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 44h:39m:28s remains)
INFO - root - 2017-12-16 01:18:49.863069: step 83770, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 43h:57m:46s remains)
INFO - root - 2017-12-16 01:18:56.289543: step 83780, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 44h:26m:34s remains)
INFO - root - 2017-12-16 01:19:02.732443: step 83790, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 44h:59m:58s remains)
INFO - root - 2017-12-16 01:19:09.206969: step 83800, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 45h:51m:58s remains)
2017-12-16 01:19:09.748049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.32062 -2.7165232 -3.5983419 -4.1533413 -4.4201145 -4.4443126 -3.5044694 -2.0847745 -0.82509518 -1.2019587 -3.0669756 -4.7864771 -6.536684 -7.7941318 -8.4714022][-3.3114257 -3.0883455 -3.2907429 -4.11664 -4.8876081 -4.873858 -4.2867432 -3.4508939 -2.1579232 -1.7407913 -3.2789211 -5.0044937 -6.6231847 -7.4656391 -8.13822][-4.0346036 -3.9777062 -3.8808217 -3.5385013 -3.4868279 -3.8534873 -3.7919006 -3.3291483 -2.5244923 -2.2565646 -3.6365867 -4.7626057 -6.1236053 -7.254972 -7.9446864][-5.0294323 -4.2339592 -3.5091138 -3.1258402 -2.2390471 -1.535181 -0.94730282 -0.97633696 -0.6333971 -1.2000194 -3.5915499 -5.1196475 -6.5789604 -7.4693084 -7.9419718][-4.4659991 -3.7934842 -2.7877793 -1.6471982 -0.82106829 0.40263367 1.8452692 2.2968349 2.4621439 0.82138634 -2.3445663 -4.4339414 -6.7363315 -7.6617355 -7.8524046][-3.7148781 -3.0880055 -1.8361931 -0.66671276 0.31830788 2.1943703 3.7769403 4.2081423 4.4156322 2.7083549 -0.94520378 -3.8166246 -6.0909023 -7.2297063 -7.6739173][-3.039691 -2.9422889 -2.0957365 -0.31278324 1.0264578 2.8752155 4.7703381 5.5261707 5.6086187 3.9067287 0.46104431 -2.6371756 -5.3095675 -6.6661468 -7.2265387][-2.454423 -1.7350039 -1.0274324 0.00096893311 1.6866751 3.3674068 4.4202986 5.5062895 6.2827854 4.4831886 0.85657692 -2.1789961 -4.6805868 -6.0278654 -6.671792][-2.2849236 -1.2145867 -0.81772709 -0.13864708 0.79063225 2.157527 3.4510479 4.8447218 5.4962711 3.6130972 -0.0015358925 -2.8736954 -5.2596865 -6.1004167 -6.6262851][-3.7809052 -2.9630027 -2.317935 -1.9241161 -1.0790567 -0.30674314 0.041740894 1.018754 1.6491404 0.63999176 -1.7602959 -3.6874871 -5.7642512 -6.4304867 -7.07864][-6.2746854 -5.9533453 -5.1003332 -3.5727596 -2.7157178 -2.511878 -1.9570584 -1.7002549 -2.0128164 -2.6530948 -4.3648977 -5.4495282 -6.5256481 -7.0469017 -7.3211832][-7.5808697 -7.3063736 -6.6655107 -5.4856133 -4.9493275 -4.2608604 -3.5849977 -3.7084939 -3.7720828 -4.4136877 -6.3218026 -6.2928681 -6.6446714 -7.4224119 -7.5555258][-7.6066985 -7.637094 -7.7029305 -6.9825315 -6.5908394 -6.023098 -5.2326546 -5.1520777 -5.2921171 -5.393074 -6.069859 -5.7729311 -6.1486454 -6.7985468 -6.5948696][-7.0025969 -6.984848 -7.0930266 -6.4204931 -6.0601959 -5.734859 -5.6332817 -5.5561447 -5.0720825 -4.9631281 -5.5854087 -5.4802942 -5.4673834 -5.7250333 -5.56063][-7.1653366 -7.1048985 -7.339251 -7.3489609 -7.1521225 -7.1801949 -7.0249176 -6.8849592 -6.88934 -6.7251983 -6.4442492 -6.2225041 -5.908761 -5.60105 -5.5380135]]...]
INFO - root - 2017-12-16 01:19:16.201023: step 83810, loss = 0.27, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 42h:35m:02s remains)
INFO - root - 2017-12-16 01:19:22.612751: step 83820, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 44h:07m:50s remains)
INFO - root - 2017-12-16 01:19:29.032299: step 83830, loss = 0.25, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 45h:21m:10s remains)
INFO - root - 2017-12-16 01:19:35.452232: step 83840, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 44h:26m:57s remains)
INFO - root - 2017-12-16 01:19:41.850163: step 83850, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 45h:14m:07s remains)
INFO - root - 2017-12-16 01:19:48.318816: step 83860, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.644 sec/batch; 44h:30m:36s remains)
INFO - root - 2017-12-16 01:19:54.755638: step 83870, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 44h:16m:17s remains)
INFO - root - 2017-12-16 01:20:01.292748: step 83880, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 44h:05m:57s remains)
INFO - root - 2017-12-16 01:20:07.678762: step 83890, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 44h:02m:36s remains)
INFO - root - 2017-12-16 01:20:14.068191: step 83900, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 43h:36m:51s remains)
2017-12-16 01:20:14.607629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7081847 -3.5881515 -4.3875446 -4.4483547 -3.8279941 -3.1607723 -3.3652625 -3.1296248 -2.7505469 -3.5372577 -3.608839 -5.2667389 -6.0137749 -6.2934709 -7.192781][-3.1142297 -3.9305751 -4.8655634 -5.0625181 -4.81357 -4.1331186 -3.487843 -3.4652815 -3.584291 -4.3135586 -4.303359 -5.8786764 -6.5061989 -7.0373306 -7.5546112][-3.7375498 -4.0738649 -4.6784229 -4.8159132 -4.9073081 -4.4826212 -4.0007315 -3.8385167 -3.2134495 -4.2480702 -4.735076 -6.4897795 -7.0597296 -7.6747451 -8.2961168][-4.6240082 -4.8246489 -4.9826841 -4.5798216 -4.2039108 -3.8834596 -3.0900331 -2.5645456 -2.4744415 -3.222712 -3.6172185 -5.8224974 -6.8450994 -7.5242677 -7.9530182][-5.6736259 -5.4850874 -5.4394937 -4.7907095 -3.9338233 -2.8381081 -1.4608564 -1.0588436 -0.72309828 -1.9743886 -3.0631189 -5.4041777 -6.4157805 -7.2911077 -8.0599937][-7.2069473 -6.6161127 -5.9799252 -4.5081367 -3.2757883 -1.6368537 0.14485025 1.0908384 1.15765 -1.1178379 -2.8735037 -5.7582817 -6.7576556 -7.3265305 -7.7171278][-7.0485587 -6.4384823 -5.9259033 -4.0396414 -1.7808399 0.27373171 2.3410988 3.0620852 3.1020575 1.1165562 -1.0507774 -4.3960309 -5.8785343 -6.4340258 -6.9506087][-6.5315371 -6.0190873 -5.3274603 -3.3351288 -1.1144753 1.7201862 4.1299686 4.2654247 3.8465729 1.4401493 -0.24401951 -3.2523737 -4.6760063 -5.3394213 -5.7440839][-6.6865964 -6.072166 -5.6225262 -4.1807365 -2.1350579 0.71177292 2.994997 3.6134071 3.4359121 0.79430676 -0.94808292 -3.8616481 -5.0343404 -5.5031757 -6.2857485][-6.6065488 -6.3091707 -6.01635 -4.703608 -3.1855164 -1.2677908 0.69283867 1.2065125 1.2042818 -1.1044164 -3.0661592 -5.5880933 -6.2698803 -6.2216263 -6.6527572][-8.0763483 -7.4634433 -7.0288858 -5.850698 -4.8405209 -3.3752499 -2.223371 -1.7624922 -1.4144821 -3.3086815 -4.0294027 -6.1028748 -6.8840766 -6.5148931 -6.7884068][-7.7702351 -7.341733 -6.9653606 -6.0689955 -5.6721606 -4.9515648 -4.0258565 -3.8630855 -3.8384054 -4.4529448 -4.497756 -5.8519974 -6.3331022 -6.6117172 -6.9320316][-8.780302 -7.912909 -7.3005443 -6.4646845 -5.9836035 -5.76357 -5.3940477 -5.224205 -4.9281063 -5.9463019 -6.0723128 -6.2345457 -6.2981219 -6.3427863 -6.6544275][-8.7190285 -7.8133435 -7.0204883 -6.1599278 -5.6308365 -5.5961552 -5.1563406 -5.2135563 -5.3142338 -5.5729895 -5.7483444 -5.9186563 -5.9469237 -6.1553183 -6.4630423][-8.6688242 -8.3771963 -7.947876 -7.0646291 -6.46803 -6.2383561 -6.3131933 -6.4457884 -6.224369 -6.3437877 -6.5372705 -6.383502 -6.1874619 -5.8721328 -5.5609207]]...]
INFO - root - 2017-12-16 01:20:21.046949: step 83910, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 44h:44m:58s remains)
INFO - root - 2017-12-16 01:20:27.589131: step 83920, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 45h:55m:05s remains)
INFO - root - 2017-12-16 01:20:33.985357: step 83930, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 43h:13m:15s remains)
INFO - root - 2017-12-16 01:20:40.513574: step 83940, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 44h:16m:52s remains)
INFO - root - 2017-12-16 01:20:46.965550: step 83950, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 43h:43m:40s remains)
INFO - root - 2017-12-16 01:20:53.407618: step 83960, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:56m:44s remains)
INFO - root - 2017-12-16 01:20:59.880896: step 83970, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 44h:47m:23s remains)
INFO - root - 2017-12-16 01:21:06.278639: step 83980, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 44h:11m:12s remains)
INFO - root - 2017-12-16 01:21:12.655903: step 83990, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 44h:17m:32s remains)
INFO - root - 2017-12-16 01:21:19.186349: step 84000, loss = 0.30, batch loss = 0.19 (12.1 examples/sec; 0.664 sec/batch; 45h:48m:01s remains)
2017-12-16 01:21:19.825640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9346075 -7.1614408 -7.8017712 -7.7569351 -7.6762276 -7.3850222 -6.3274136 -5.0650945 -4.0652237 -5.4610329 -6.1636686 -5.8713908 -6.0623584 -7.2074394 -7.77674][-6.30694 -6.8919921 -7.586956 -8.262722 -8.7308226 -8.5841742 -7.7674985 -6.6657763 -5.3591785 -6.636878 -7.2362266 -7.0139937 -7.4890094 -8.579319 -9.11978][-5.2251368 -5.7327433 -6.4176865 -7.0507193 -7.6421289 -7.8105545 -7.5930324 -6.8258214 -6.1867352 -7.567142 -7.7743797 -7.595048 -7.7003098 -8.9282341 -9.3266554][-6.36954 -6.1106863 -6.2786131 -6.49548 -6.7936182 -6.1447229 -5.286953 -4.7445393 -4.3362212 -6.1703515 -7.0303655 -7.33991 -7.8230448 -8.675499 -8.9731684][-7.6008797 -7.101584 -6.379324 -5.4729123 -5.1033354 -3.8983026 -3.0718269 -2.5773144 -2.1105151 -4.4053926 -5.7001472 -6.4957075 -7.1078248 -8.2156057 -8.82573][-9.1269379 -8.3673344 -7.2703528 -5.2441645 -3.1504979 -1.3737912 0.11522245 0.54087925 0.65576458 -2.0708532 -4.0749989 -5.1960068 -5.909461 -7.39797 -8.7157192][-9.1996593 -7.8856339 -5.8722186 -3.2853084 -0.59918261 2.1356087 4.09622 4.57463 4.5163708 0.82217693 -2.1133132 -3.6765671 -4.8675537 -6.3395009 -7.1081767][-8.2985916 -7.495729 -6.1048088 -2.9965706 0.701107 3.2276344 5.230793 6.0365582 5.9882803 1.8678446 -1.6620431 -3.6831775 -4.7487774 -6.1426883 -6.8440962][-9.1895752 -8.6668911 -7.4502358 -4.6184015 -1.6594319 0.96244717 3.0197058 3.6410761 3.8289433 0.38332176 -2.7983618 -4.3744993 -5.2880249 -6.2948561 -6.4385662][-10.511915 -9.7343483 -8.4721861 -5.6706066 -2.4930291 -0.42887402 1.1807966 1.7052727 1.3208895 -2.1410828 -4.3347416 -5.5645661 -6.9375849 -7.6355119 -7.3320794][-11.385233 -10.974971 -10.277588 -8.3351221 -6.5661764 -4.088418 -1.9884052 -2.19979 -2.655757 -5.304738 -6.8686056 -7.6619887 -8.3524122 -9.1538963 -9.139451][-11.823149 -11.184437 -10.275917 -9.5600624 -8.7885084 -7.2528114 -6.3704195 -5.6483288 -5.0324407 -7.4151187 -8.5418291 -8.910099 -9.1299839 -9.2585392 -9.0166664][-12.135354 -11.581199 -10.866439 -10.038565 -8.966466 -7.9188862 -7.0592341 -6.8334894 -7.1468925 -9.0040817 -9.7692537 -9.8269348 -9.8848686 -9.6313877 -9.0333261][-10.590528 -10.328197 -10.015093 -9.2604332 -8.4216747 -7.2758484 -6.633203 -6.7782025 -6.952445 -8.18964 -8.7480125 -9.1007442 -9.6860189 -9.5578575 -8.704546][-9.7143583 -9.3319092 -8.9935913 -8.6827087 -8.3303394 -7.4978714 -6.9033947 -7.0379248 -7.3390217 -7.6343365 -7.7387967 -7.5782666 -7.6079288 -7.6761656 -7.4066248]]...]
INFO - root - 2017-12-16 01:21:26.255796: step 84010, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 44h:09m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 01:21:32.723127: step 84020, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 43h:50m:30s remains)
INFO - root - 2017-12-16 01:21:39.095735: step 84030, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 45h:56m:56s remains)
INFO - root - 2017-12-16 01:21:45.514697: step 84040, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.647 sec/batch; 44h:40m:33s remains)
INFO - root - 2017-12-16 01:21:51.939224: step 84050, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 44h:03m:07s remains)
INFO - root - 2017-12-16 01:21:58.422021: step 84060, loss = 0.33, batch loss = 0.21 (12.3 examples/sec; 0.648 sec/batch; 44h:42m:56s remains)
INFO - root - 2017-12-16 01:22:05.023486: step 84070, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 43h:16m:21s remains)
INFO - root - 2017-12-16 01:22:11.379435: step 84080, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 43h:58m:52s remains)
INFO - root - 2017-12-16 01:22:17.748422: step 84090, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 43h:48m:23s remains)
INFO - root - 2017-12-16 01:22:24.164759: step 84100, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 44h:51m:58s remains)
2017-12-16 01:22:24.709703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9677668 -3.9152884 -3.7083869 -4.2418337 -5.2676277 -5.6175437 -5.8142924 -5.5579128 -5.0937614 -6.8136606 -7.4857726 -8.0270405 -8.67532 -9.42666 -9.1829052][-3.192543 -3.8119562 -4.3919511 -4.9236608 -5.5275717 -5.77583 -5.9394026 -5.6777496 -5.4404945 -7.0073752 -7.1213174 -7.8907428 -9.1763916 -9.7777557 -9.5582523][-2.9254909 -3.7414532 -3.8788736 -3.8549798 -4.2530074 -4.1433706 -4.0654535 -3.9732738 -3.7969286 -5.509789 -6.1621108 -6.9569988 -7.8298111 -8.6207685 -8.8990469][-2.2783298 -2.7464581 -3.0837231 -3.0248456 -2.9245267 -2.6391268 -2.3583016 -2.1227517 -1.9071951 -3.9111505 -4.7888751 -6.0480976 -7.452692 -8.1102467 -8.0574675][-3.2897539 -2.8031201 -1.6618009 -1.0172939 -0.83636332 -0.29535055 0.13817549 0.29679585 0.491951 -1.6655536 -2.8902969 -4.3544436 -5.9347105 -7.21399 -7.3221297][-2.5209007 -2.3276033 -1.6947351 -0.52454853 0.91619873 2.0169592 2.6672983 2.8552666 2.7004795 0.30644941 -1.0742235 -2.6946998 -4.8743792 -5.8931108 -5.9898744][-3.058795 -3.0086932 -1.4628015 0.46825981 2.0486374 2.9047718 3.7739487 3.9227304 3.7331038 0.82427311 -1.0471063 -3.0136232 -5.16947 -6.1934896 -6.9263673][-3.1004038 -2.444963 -1.1303701 1.0954676 2.8678989 3.654048 4.0785332 4.178462 3.8985634 1.098279 -0.7713747 -2.7143998 -5.0730009 -6.5466404 -7.176167][-2.2200942 -1.7815418 -0.33680773 1.6374407 2.746974 3.1557026 3.286108 3.3322811 2.9230833 0.32284069 -1.6037393 -3.2254043 -5.3373356 -6.8013105 -7.5804591][-1.7504883 -1.701129 -0.68418884 0.83258724 1.8892641 2.2128677 2.4183683 2.0852127 1.5473995 -0.982676 -2.4666982 -4.0107527 -5.9151278 -7.0223584 -7.6665277][-4.2530413 -3.6542153 -2.6003838 -1.1042161 -0.062829018 0.32189941 0.50632286 0.22533607 0.10503149 -2.1359472 -3.7964528 -4.9818091 -6.5717764 -7.4781356 -8.0871792][-4.9152946 -4.3767838 -3.6400456 -2.7203727 -2.0708423 -1.7954793 -1.6656804 -1.6590557 -1.6232677 -3.0063448 -4.2829981 -4.9135647 -6.2152228 -7.2725248 -7.791719][-5.5350962 -5.604877 -5.5277629 -4.9640522 -4.53158 -4.0885596 -3.622407 -3.7296748 -3.6594176 -4.3736939 -5.4312963 -5.8433433 -6.7252107 -6.9481268 -7.1205955][-5.1521492 -5.3805609 -5.7645125 -5.4092207 -4.7305708 -4.2848516 -4.2049727 -4.1106062 -3.6458201 -4.0222487 -4.5599575 -4.9592886 -5.7135878 -6.3450117 -6.7146673][-5.8627796 -5.3014431 -5.2515249 -5.4491844 -5.5772362 -5.4399815 -5.0599461 -4.8920021 -4.8886595 -4.5916238 -4.8454914 -5.0669937 -5.4284487 -5.9820132 -6.3878045]]...]
INFO - root - 2017-12-16 01:22:31.110145: step 84110, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 44h:57m:45s remains)
INFO - root - 2017-12-16 01:22:37.448167: step 84120, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:01m:31s remains)
INFO - root - 2017-12-16 01:22:43.843654: step 84130, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 43h:10m:23s remains)
INFO - root - 2017-12-16 01:22:50.229076: step 84140, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.664 sec/batch; 45h:46m:58s remains)
INFO - root - 2017-12-16 01:22:56.713539: step 84150, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 45h:20m:39s remains)
INFO - root - 2017-12-16 01:23:03.108121: step 84160, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 43h:23m:10s remains)
INFO - root - 2017-12-16 01:23:09.427377: step 84170, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 43h:33m:40s remains)
INFO - root - 2017-12-16 01:23:15.830990: step 84180, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.638 sec/batch; 44h:00m:59s remains)
INFO - root - 2017-12-16 01:23:22.314000: step 84190, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 42h:55m:35s remains)
INFO - root - 2017-12-16 01:23:28.738458: step 84200, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.642 sec/batch; 44h:15m:08s remains)
2017-12-16 01:23:29.290366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2215195 -5.2675118 -4.8561945 -4.927537 -5.8060293 -6.0679989 -6.3383641 -6.3280573 -5.8835459 -6.2969217 -7.5963869 -7.399344 -7.5681472 -8.6425457 -8.2451115][-4.9678106 -4.3816 -3.9797704 -4.866817 -5.579546 -6.1330156 -6.7141418 -6.2511225 -5.6978121 -6.0556417 -6.651022 -6.2026095 -6.5621567 -7.3400679 -6.7630568][-2.9201636 -2.658865 -2.9073439 -3.8177032 -4.4833574 -5.4328156 -5.8372993 -5.3006549 -4.8119421 -4.8324175 -5.4758077 -4.8941841 -5.0498276 -6.2626538 -6.0763493][-2.6770525 -2.4059005 -2.6287713 -2.6877098 -3.1565537 -3.5130682 -3.1823883 -2.9164262 -2.5182424 -2.6505933 -3.1367517 -3.1895623 -4.0259771 -4.9339275 -4.9847231][-2.4178729 -2.1568642 -1.7260165 -2.0667658 -2.2534766 -1.7604294 -1.3283048 -0.77034283 -0.16382885 -0.55198622 -1.5319171 -1.9589138 -3.2281885 -4.4837704 -4.9857621][-2.4917626 -2.3696518 -1.8576531 -1.3436222 -0.73740959 -0.2435956 0.35498905 1.18711 1.7582722 1.0226536 -0.29589367 -1.6649747 -3.2453823 -4.7107553 -5.1376081][-2.8235674 -1.6308827 -1.0889444 -0.85680437 -0.41269445 0.40607262 1.3038063 1.9627857 2.1699018 1.1444035 -0.690578 -2.3276935 -4.0244508 -5.6280155 -5.781683][-2.5650649 -1.5975204 -1.4044781 -0.77707529 -0.017154217 0.91120815 1.8280621 2.6265049 2.9727316 1.4020796 -0.7541647 -2.459867 -4.1864624 -5.8680382 -6.2005796][-2.7156491 -2.087368 -1.6710935 -0.9857831 -0.15985346 0.64237595 1.2790403 1.9276228 2.0826044 0.94131565 -1.2782626 -3.0463586 -4.4829855 -5.943985 -6.0289116][-4.1357222 -3.078804 -2.68392 -2.0452423 -1.1161366 -0.46976042 0.20879555 0.81936646 0.95968246 -0.48854065 -2.3427296 -3.5306153 -4.645957 -6.1798921 -6.3594522][-5.841136 -5.3776617 -5.0329385 -4.4320679 -3.700629 -2.973125 -2.427042 -2.3289385 -2.3456631 -3.2924552 -4.8169765 -5.2072296 -5.6598988 -6.8220434 -6.7542887][-6.3955507 -6.6505804 -6.7885475 -6.0526524 -5.1446528 -4.5665221 -4.0220795 -3.7708898 -3.8051288 -4.9351764 -5.8514848 -5.9398913 -5.9948282 -6.8321323 -6.9326553][-8.1496973 -8.0102835 -8.0479622 -7.9452825 -7.4413538 -6.523634 -5.6906767 -5.775156 -6.0536051 -6.3768005 -6.9822168 -6.3793521 -6.13456 -6.579422 -6.3243518][-6.6537514 -7.1304188 -7.50338 -7.3475614 -7.0694208 -6.3943481 -5.7869816 -5.8751369 -5.7926822 -6.0791292 -6.2329154 -5.6635046 -5.4159212 -5.6930375 -5.3247604][-6.7024446 -7.1359291 -7.0061331 -7.1041327 -7.1606083 -7.0617409 -6.9677467 -6.7740288 -6.6614485 -6.8912368 -6.7254553 -6.2779675 -6.1128759 -6.0023584 -5.6531129]]...]
INFO - root - 2017-12-16 01:23:35.676111: step 84210, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 43h:37m:35s remains)
INFO - root - 2017-12-16 01:23:42.025760: step 84220, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 43h:50m:31s remains)
INFO - root - 2017-12-16 01:23:48.393129: step 84230, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 44h:04m:03s remains)
INFO - root - 2017-12-16 01:23:54.794803: step 84240, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 44h:57m:42s remains)
INFO - root - 2017-12-16 01:24:01.101082: step 84250, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 43h:57m:20s remains)
INFO - root - 2017-12-16 01:24:07.413409: step 84260, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 43h:25m:55s remains)
INFO - root - 2017-12-16 01:24:13.807768: step 84270, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 44h:21m:09s remains)
INFO - root - 2017-12-16 01:24:20.227113: step 84280, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 44h:42m:45s remains)
INFO - root - 2017-12-16 01:24:26.545270: step 84290, loss = 0.33, batch loss = 0.22 (12.8 examples/sec; 0.624 sec/batch; 43h:00m:46s remains)
INFO - root - 2017-12-16 01:24:33.015455: step 84300, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.614 sec/batch; 42h:18m:47s remains)
2017-12-16 01:24:33.569491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1261015 -5.1482892 -5.0307775 -4.832468 -4.65763 -3.9794164 -3.681077 -3.0346146 -2.3185802 -4.1247368 -4.6778345 -5.4550447 -6.1322746 -6.5550361 -7.3266611][-4.6000462 -5.0208416 -5.165555 -5.1488972 -5.2237153 -4.2359805 -3.5209131 -3.0695724 -2.631083 -4.3117456 -4.5836315 -5.3895311 -6.2540541 -6.6844177 -7.63369][-2.5746527 -3.2006521 -3.6964405 -4.1199112 -4.4074135 -3.7133737 -3.1055899 -2.7603621 -2.3793778 -4.0559359 -4.3359103 -5.0312419 -5.968575 -6.3940668 -7.2836404][-0.85071468 -1.4059057 -1.6946249 -1.7927704 -1.7632008 -1.4779024 -1.2229791 -1.2797832 -1.3061171 -3.3363185 -3.967881 -5.033165 -6.0930567 -6.4682183 -7.1499228][-1.0138602 -0.57769012 -0.25528049 -0.18886328 -0.004049778 0.58213997 0.7921114 0.42221737 -0.0037226677 -2.8355789 -3.9826975 -5.1465225 -6.28687 -6.6393137 -7.215189][-2.0384216 -1.1728816 -0.21293449 0.51059818 1.2989159 2.0983839 2.715085 2.3166542 1.7390862 -1.2802558 -2.7007351 -4.3012424 -5.9634447 -6.6627536 -7.4939194][-2.576942 -1.6309476 -0.23981619 1.1308022 2.4097748 3.3821363 4.0264568 3.5810146 2.9803238 -0.34192848 -2.2187629 -3.8598981 -5.6941156 -6.687243 -7.6312275][-3.1565528 -1.9415817 -0.43931913 1.4847965 2.944314 3.8043795 4.4661713 4.0617476 3.4087353 0.17673874 -1.683877 -3.6572809 -5.7212687 -6.6757522 -7.59204][-4.3859062 -3.0898018 -1.5065999 0.24649906 1.5200434 2.4347553 2.9692497 2.8406954 2.638587 -0.39503717 -1.9474659 -3.5542035 -5.6206512 -6.80237 -7.676353][-5.4133968 -4.4552536 -3.180747 -1.3698535 -0.07353878 0.58290386 1.2589893 1.2941656 1.1510315 -1.5740185 -2.6356268 -4.13674 -5.7347236 -6.6529169 -7.4023628][-6.0962543 -5.4519625 -4.3870697 -3.2350693 -2.2526641 -1.5495696 -1.0822792 -0.95151567 -0.94048643 -3.5857596 -4.6520348 -5.5687423 -6.7093167 -7.3461123 -7.6654234][-7.0451331 -6.3510637 -5.672904 -4.7484951 -4.0090818 -3.5585008 -3.0870647 -3.0110965 -2.9942245 -4.7411976 -5.4117956 -6.1787286 -7.0059743 -7.5458765 -7.9106722][-7.508399 -6.8549023 -6.2111287 -5.8028369 -5.3581381 -4.8707333 -4.6509295 -4.6797953 -4.5249233 -5.7639461 -5.9774647 -6.1612768 -6.6821213 -6.9870496 -7.3164616][-7.2740731 -7.1263795 -6.9303293 -6.7327528 -6.3515773 -5.8999324 -5.7550755 -5.7000961 -5.5112419 -5.904655 -5.8549638 -6.1693583 -6.251009 -6.2208319 -6.3098288][-7.6638827 -7.2172356 -6.8041224 -7.0471091 -6.9916782 -6.7296486 -6.9034729 -7.0301447 -7.0237269 -6.6549191 -6.3031135 -6.3869162 -6.2167511 -5.971725 -5.7370939]]...]
INFO - root - 2017-12-16 01:24:40.129859: step 84310, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 43h:51m:22s remains)
INFO - root - 2017-12-16 01:24:46.442094: step 84320, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 43h:43m:38s remains)
INFO - root - 2017-12-16 01:24:52.844924: step 84330, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:48m:21s remains)
INFO - root - 2017-12-16 01:24:59.222971: step 84340, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 45h:23m:01s remains)
INFO - root - 2017-12-16 01:25:05.541366: step 84350, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 43h:47m:11s remains)
INFO - root - 2017-12-16 01:25:11.876503: step 84360, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 43h:42m:11s remains)
INFO - root - 2017-12-16 01:25:18.271722: step 84370, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 44h:06m:45s remains)
INFO - root - 2017-12-16 01:25:24.612450: step 84380, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 44h:32m:56s remains)
INFO - root - 2017-12-16 01:25:31.013487: step 84390, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 44h:03m:08s remains)
INFO - root - 2017-12-16 01:25:37.399240: step 84400, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 44h:59m:47s remains)
2017-12-16 01:25:37.912061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.920351 -3.2999582 -2.6746144 -2.3653426 -2.1604733 -2.0266619 -1.8491335 -1.7600188 -1.8599863 -1.7808838 -3.8289192 -5.6446486 -6.5779071 -8.107007 -8.4504509][-3.6676316 -3.2371669 -2.8697443 -2.5848632 -2.7056665 -2.6372123 -2.5860887 -2.5853143 -2.4036241 -2.173893 -4.0537477 -5.6588116 -6.5425367 -7.7017822 -7.8085551][-2.817214 -2.6108246 -2.107368 -1.8986635 -1.9548926 -1.8719573 -2.1042027 -2.3145928 -2.4984951 -2.547873 -4.1491776 -5.2550197 -6.0902262 -7.1253428 -7.538682][-1.7399678 -1.0092769 -0.64643049 -0.891994 -1.3076115 -1.3637238 -1.1225448 -1.3621387 -1.7712483 -2.2489538 -4.02227 -5.1560974 -5.8870645 -6.5785451 -6.7969847][-1.6967053 -0.71707964 -0.033982277 -0.15633535 -0.60870409 -0.6028161 -0.4207058 -0.66056347 -0.7277565 -1.4361973 -3.0182056 -3.7779663 -4.7186937 -5.6784878 -5.8995256][-1.0708728 -0.43800163 -0.062694073 -0.18365288 -0.1574316 0.35234261 0.6757431 0.54169846 0.64843941 -0.0265975 -1.2197413 -2.2235656 -3.5463123 -4.9183888 -5.9351454][-1.7971206 -0.7795105 -0.0020599365 0.17361689 0.3315258 0.65189075 0.91148186 1.1512251 1.4024391 0.82142735 -0.42453003 -1.7005811 -3.0963969 -4.4496679 -5.2545495][-2.1766472 -1.2683563 -0.71617842 -0.29231024 0.21114588 0.79672718 1.1803484 1.3470678 1.7166224 1.2822609 0.15097475 -1.298377 -2.4367194 -3.5456181 -4.2585163][-3.6190333 -2.190238 -1.576828 -0.90554667 -0.27209091 0.23977661 0.81199932 1.3951063 1.7457495 1.1487255 -0.12355614 -1.5292077 -2.3184042 -3.3638434 -4.236064][-4.6592565 -3.8770616 -2.6777725 -2.1060548 -1.4647956 -0.99621868 -0.34897661 0.11700296 0.75739956 0.25573635 -1.0926023 -2.3143525 -3.3810019 -4.1833339 -4.8498383][-7.276855 -6.520647 -6.0562019 -5.1652832 -4.0201855 -3.5937471 -3.0456758 -2.6971893 -2.5365391 -3.1180148 -3.9802291 -4.7024155 -5.1718946 -5.6446204 -6.173944][-8.7904348 -8.3093 -7.58193 -6.7635312 -6.3151231 -5.8181167 -5.0411372 -4.7933264 -4.6153917 -4.9861355 -5.69541 -5.9649529 -6.2173529 -6.7600479 -7.09784][-9.0143461 -9.0647783 -8.4687443 -7.6174784 -7.2238121 -6.8870068 -6.4736156 -6.4244781 -6.233109 -6.1536341 -6.5098376 -6.5062857 -6.5893288 -6.8537908 -6.7501354][-8.16054 -8.5506229 -8.4157085 -7.9267726 -7.4071364 -6.778007 -6.5963025 -6.7659607 -6.8519607 -6.9614797 -7.2986593 -7.2976365 -7.1606936 -6.920505 -6.391346][-8.3355312 -8.2300386 -8.2954741 -8.0676432 -7.7073622 -7.2863612 -6.7914023 -6.7774916 -6.8745794 -7.0757012 -7.1695824 -7.1155186 -7.1287904 -7.2170238 -6.8134179]]...]
INFO - root - 2017-12-16 01:25:44.234312: step 84410, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.619 sec/batch; 42h:37m:39s remains)
INFO - root - 2017-12-16 01:25:50.574204: step 84420, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 43h:52m:04s remains)
INFO - root - 2017-12-16 01:25:56.973863: step 84430, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 43h:59m:54s remains)
INFO - root - 2017-12-16 01:26:03.317805: step 84440, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 43h:15m:03s remains)
INFO - root - 2017-12-16 01:26:09.664840: step 84450, loss = 0.30, batch loss = 0.18 (12.0 examples/sec; 0.665 sec/batch; 45h:50m:31s remains)
INFO - root - 2017-12-16 01:26:15.961718: step 84460, loss = 0.29, batch loss = 0.17 (12.9 examples/sec; 0.618 sec/batch; 42h:34m:03s remains)
INFO - root - 2017-12-16 01:26:22.419344: step 84470, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 44h:58m:25s remains)
INFO - root - 2017-12-16 01:26:28.779845: step 84480, loss = 0.23, batch loss = 0.11 (12.8 examples/sec; 0.624 sec/batch; 43h:01m:27s remains)
INFO - root - 2017-12-16 01:26:35.120205: step 84490, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 43h:48m:36s remains)
INFO - root - 2017-12-16 01:26:41.535496: step 84500, loss = 0.30, batch loss = 0.18 (11.9 examples/sec; 0.671 sec/batch; 46h:15m:20s remains)
2017-12-16 01:26:42.061946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7499404 -2.0805311 -1.5897107 -1.4537725 -1.3875213 -1.2693415 -0.80086994 -1.0882559 -1.1991696 -2.9981866 -4.5035849 -6.2516894 -8.7404718 -10.026579 -11.260481][-3.4793706 -3.1117716 -2.7199922 -2.2467537 -1.9671211 -1.8206024 -1.5603476 -1.4474978 -1.2381024 -3.0951896 -4.6782503 -6.6326671 -9.1117334 -10.327695 -11.787472][-4.7336087 -4.0032368 -3.3597488 -2.753417 -1.9031072 -1.1812315 -0.56496572 -0.578156 -0.81240892 -2.4432554 -3.8466611 -5.7038546 -7.9099832 -9.3018093 -11.073507][-5.8382621 -5.00591 -4.1960096 -3.1321807 -2.0520582 -1.0766687 0.10207748 0.89333534 1.0702467 -0.64635515 -2.0350075 -4.1199279 -6.3311772 -7.8124118 -9.4805269][-6.2056918 -5.2421823 -4.3524141 -3.1218972 -2.1249013 -1.1549158 0.11803007 0.87321854 1.4579 0.40739155 -0.78237724 -2.0523729 -4.1359839 -5.9329166 -7.8261061][-5.5797815 -4.7432446 -3.8414507 -2.3282161 -1.3648262 -0.88190985 -0.25999355 0.32586479 0.92319107 0.033936977 -0.4616189 -1.6875763 -2.8518376 -3.6407957 -5.3747215][-4.1113825 -3.1546135 -2.0789533 -0.8201189 0.14609957 0.73161411 1.0334225 1.3436813 1.5673389 0.14391041 -0.57941961 -1.8331003 -3.1831055 -3.9085484 -5.1941957][-2.2597365 -1.0450845 0.010819435 1.2311678 2.0961552 2.7330837 3.1520367 3.1028576 3.0208902 1.3128548 0.19194794 -1.6242414 -3.204031 -3.8131542 -4.9697981][0.32469654 0.74509048 1.6383486 2.3663387 2.7774839 2.8510723 3.1812744 3.186861 2.9702005 1.4317341 0.18202066 -1.7129531 -3.4023232 -4.2894106 -5.2462978][1.2471609 1.8746643 2.0925226 2.1307545 2.0331306 2.6225576 3.1501207 3.0638046 2.942421 0.737627 -0.83437681 -2.5280423 -4.1026049 -4.9664192 -6.0030174][1.015192 1.4046946 1.4823074 2.0108385 1.5751629 1.612318 1.9135523 1.7850285 1.3477335 -0.63811541 -2.1870971 -4.5086384 -6.0181923 -6.465661 -7.2702909][0.0064930916 0.87776756 1.1441889 1.3649073 1.0514088 0.6658659 0.63218117 0.19463587 -0.33730268 -2.0155048 -3.6036034 -5.229373 -6.9640155 -7.8799958 -8.86042][-0.834075 -0.54774714 -1.1510525 -1.0632524 -1.082305 -1.4305811 -1.7060084 -2.0739279 -2.4844494 -3.9717743 -5.2769322 -6.6873035 -8.32867 -8.8425226 -9.6091051][-1.6856508 -1.8197885 -2.4915223 -2.1847372 -2.680654 -2.8384743 -2.8533421 -3.585979 -4.15659 -5.310442 -6.0366154 -6.7272663 -7.6559572 -8.1962481 -8.8259983][-2.7654562 -2.9069738 -3.1827607 -3.6724305 -4.4800406 -4.7646341 -5.0842142 -5.1617327 -5.4178486 -6.020865 -6.5492249 -6.9936404 -7.4243174 -7.5873904 -7.8898125]]...]
INFO - root - 2017-12-16 01:26:48.459196: step 84510, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.625 sec/batch; 43h:02m:37s remains)
INFO - root - 2017-12-16 01:26:54.789605: step 84520, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 44h:24m:46s remains)
INFO - root - 2017-12-16 01:27:01.141734: step 84530, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 43h:32m:16s remains)
INFO - root - 2017-12-16 01:27:07.454685: step 84540, loss = 0.36, batch loss = 0.24 (12.7 examples/sec; 0.629 sec/batch; 43h:20m:01s remains)
INFO - root - 2017-12-16 01:27:13.835924: step 84550, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 44h:19m:01s remains)
INFO - root - 2017-12-16 01:27:20.177440: step 84560, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 44h:30m:36s remains)
INFO - root - 2017-12-16 01:27:26.538854: step 84570, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 44h:16m:38s remains)
INFO - root - 2017-12-16 01:27:33.025263: step 84580, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 43h:54m:29s remains)
INFO - root - 2017-12-16 01:27:39.399398: step 84590, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 43h:47m:23s remains)
INFO - root - 2017-12-16 01:27:45.792141: step 84600, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 44h:37m:55s remains)
2017-12-16 01:27:46.332727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4549832 -4.0289936 -4.0077157 -4.5318193 -5.3798318 -5.6014462 -5.4063058 -5.0052109 -4.2529087 -4.4771757 -5.1545534 -6.6459064 -7.9522905 -9.2727509 -9.7533817][-5.1133671 -4.6215143 -4.772294 -5.6707697 -6.3910689 -6.6546736 -6.3216381 -6.0059137 -5.5274181 -5.6066256 -6.2205849 -7.5350318 -8.4712563 -9.6389971 -10.339749][-4.9564314 -3.8605878 -3.9011879 -4.0732622 -4.5773182 -4.8377 -4.8548474 -4.435091 -3.9846678 -4.5697269 -5.1851664 -6.4532971 -7.612874 -8.8449039 -9.2149725][-4.7377348 -3.4218974 -2.6545792 -2.5340028 -3.0718079 -2.6947255 -2.1744561 -2.3404016 -2.309545 -2.788343 -3.5418863 -4.6988134 -5.6680636 -6.7844653 -7.4044614][-3.7271495 -2.2972059 -1.5169344 -1.2638841 -1.29496 -0.28979397 0.34362221 0.52703571 0.7178154 -0.1931901 -1.2217002 -2.5090213 -3.6642098 -5.0747166 -5.9526668][-3.7994354 -2.7195096 -2.2192168 -1.3597178 -0.63111448 0.47257423 1.6066675 1.9184713 2.1069622 1.0305691 -0.085495949 -1.5253654 -2.8315964 -4.2754917 -5.3681107][-4.1877642 -3.3585458 -2.316987 -1.1449203 0.1058197 1.3374128 2.5217705 2.8405943 2.922513 1.7422123 0.58962536 -1.4005232 -2.9176636 -4.3201571 -5.3434219][-4.2791886 -3.4008894 -2.4149199 -0.8796525 0.7092762 2.3172121 3.745657 3.6649351 3.4689522 2.1070185 0.60244274 -1.5700579 -2.9903755 -4.4442964 -5.4594355][-4.7207766 -3.9873073 -3.1303477 -1.7574258 0.0056667328 1.6805229 2.6998024 3.0513592 2.8609657 1.2690935 -0.21238518 -2.079423 -3.4506431 -4.766654 -5.42062][-5.1082363 -4.6850634 -4.1647463 -3.030201 -1.6500311 -0.60308504 0.58200741 1.0485106 0.8494215 -0.57829809 -1.9409442 -3.5358877 -4.7537851 -5.7147303 -6.4059][-5.8063555 -5.78963 -5.0663328 -4.2296219 -3.2080064 -2.2267714 -1.2936172 -1.1278358 -1.1440892 -2.6144032 -3.9052358 -5.0292606 -6.0661144 -6.70822 -7.1298141][-6.5033627 -5.7295446 -5.2761841 -4.4359016 -3.2533689 -2.4830155 -2.1727734 -2.6557021 -2.6907325 -3.9583557 -5.2227135 -6.2520676 -7.2200251 -7.514482 -8.3548155][-6.1807 -5.9146948 -5.6383295 -4.8110948 -3.9780908 -3.0724287 -3.0544071 -3.3997478 -3.6725116 -4.9512358 -6.1275454 -6.7525148 -7.4043379 -7.7981315 -8.33946][-5.197299 -4.5654755 -4.5520687 -4.1362915 -3.4298534 -2.4234414 -2.1189427 -2.1361213 -2.8345685 -4.348238 -5.1433916 -6.1992579 -7.1885643 -7.463428 -7.3708839][-5.4213352 -4.995121 -4.8054147 -3.9652796 -3.2906399 -2.5011992 -1.9491162 -2.2973342 -3.1190791 -4.0777707 -5.3143716 -6.1555872 -6.3865404 -6.7652688 -7.0981345]]...]
INFO - root - 2017-12-16 01:27:52.681466: step 84610, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 42h:58m:21s remains)
INFO - root - 2017-12-16 01:27:58.985745: step 84620, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.637 sec/batch; 43h:52m:07s remains)
INFO - root - 2017-12-16 01:28:05.353943: step 84630, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 44h:05m:56s remains)
INFO - root - 2017-12-16 01:28:11.758918: step 84640, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 44h:19m:53s remains)
INFO - root - 2017-12-16 01:28:18.149826: step 84650, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 44h:16m:32s remains)
INFO - root - 2017-12-16 01:28:24.512101: step 84660, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:22m:05s remains)
INFO - root - 2017-12-16 01:28:30.827044: step 84670, loss = 0.32, batch loss = 0.20 (12.7 examples/sec; 0.630 sec/batch; 43h:20m:09s remains)
INFO - root - 2017-12-16 01:28:37.204325: step 84680, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 45h:43m:33s remains)
INFO - root - 2017-12-16 01:28:43.706705: step 84690, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 45h:25m:56s remains)
INFO - root - 2017-12-16 01:28:50.226626: step 84700, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 43h:28m:04s remains)
2017-12-16 01:28:50.765616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4551396 -3.5499644 -3.7195103 -3.8097153 -3.838351 -3.7179458 -3.4165611 -3.4760456 -3.4261236 -4.3443966 -4.7284269 -5.754405 -5.7917852 -6.5319057 -6.9393072][-3.4915471 -3.6473956 -3.6651835 -3.680172 -3.6722455 -3.6039138 -3.6007404 -3.838418 -3.8330615 -4.85158 -4.8949852 -5.7531595 -6.0232639 -6.8250861 -7.4594021][-3.2375655 -2.874722 -2.6101041 -2.2567992 -2.0149727 -2.0438719 -2.0927138 -2.4604559 -2.8149991 -3.7434664 -4.1003466 -5.0313907 -5.2866116 -6.1011229 -6.7083077][-1.9801011 -1.3618264 -0.75755215 -0.40841961 -0.43318367 -0.2282486 -0.1956954 -0.63275814 -1.0518904 -2.4549127 -3.1147475 -3.9505484 -4.5161624 -5.0201077 -5.6881809][-1.4082179 -0.71241093 0.035197258 0.36430359 0.2432003 0.38968468 0.57646084 0.30655336 -0.091510296 -1.1386676 -1.5853524 -2.3647532 -2.7819791 -3.6775222 -4.870162][-1.2348552 -0.41841078 0.15749359 0.79498386 0.669014 0.88755894 1.1182938 0.79656219 0.50054836 -0.6626339 -1.1285176 -1.8039756 -2.293366 -3.2306538 -4.3076344][-1.0931416 -0.30859184 0.33124065 1.1294641 1.3385572 1.3671122 1.5484238 1.4343863 1.2778091 0.038723946 -0.56796932 -1.4642262 -1.9399652 -2.9266629 -4.1414471][-0.76444769 0.02712822 0.74079418 1.7464724 2.0838003 2.0747547 2.0457335 1.9498272 1.9791441 0.64143467 -0.082634449 -1.1885781 -1.939991 -2.8154669 -3.9024143][-0.9272089 -0.28318644 0.11860275 0.8701458 1.3100443 1.3311777 1.2797813 1.347435 1.5302992 0.40766335 -0.49123764 -1.8849034 -2.5640392 -3.4220643 -4.5173988][-2.2952042 -1.6738076 -1.1823635 -0.57530165 -0.080483437 0.21390581 0.26028633 0.49104881 0.91412067 -0.10059452 -1.1749558 -2.454999 -3.2981973 -4.0074272 -5.1217585][-3.4838839 -3.0250249 -2.4983826 -2.0093408 -1.480989 -1.349658 -1.2331467 -1.1309805 -1.0247016 -2.069078 -2.8798094 -4.05476 -4.6668482 -5.175416 -5.7528248][-4.6527157 -4.2783346 -4.3259273 -4.4152727 -3.9905865 -3.6365848 -3.6177683 -3.7622187 -3.6614528 -4.4434834 -5.0003443 -5.4886074 -5.6600184 -5.7509365 -6.0773964][-6.2240057 -6.1847668 -5.9808311 -5.7944818 -5.8147216 -5.6165428 -5.8297811 -5.5301809 -5.1935015 -5.4714475 -5.6110115 -5.8015375 -5.7346497 -5.5909882 -5.7617311][-7.1159563 -7.0317 -6.7863922 -6.6394687 -6.5962343 -6.4257793 -6.3436213 -6.6220169 -6.754014 -7.0908232 -7.2147226 -6.8976741 -6.3454595 -5.5125513 -4.8543205][-7.1978927 -7.0215454 -7.0929041 -7.0398397 -7.2719784 -7.2697077 -7.2498527 -7.6464715 -7.7733054 -8.030776 -7.8443427 -7.4740033 -7.2892246 -6.601161 -5.80682]]...]
INFO - root - 2017-12-16 01:28:57.152497: step 84710, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 44h:01m:11s remains)
INFO - root - 2017-12-16 01:29:03.515918: step 84720, loss = 0.23, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 44h:11m:07s remains)
INFO - root - 2017-12-16 01:29:10.112704: step 84730, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 44h:16m:11s remains)
INFO - root - 2017-12-16 01:29:16.577800: step 84740, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 43h:28m:43s remains)
INFO - root - 2017-12-16 01:29:22.949046: step 84750, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 43h:36m:47s remains)
INFO - root - 2017-12-16 01:29:29.460702: step 84760, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 44h:22m:19s remains)
INFO - root - 2017-12-16 01:29:35.826286: step 84770, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 43h:24m:01s remains)
INFO - root - 2017-12-16 01:29:42.183982: step 84780, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 44h:29m:17s remains)
INFO - root - 2017-12-16 01:29:48.669054: step 84790, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 43h:57m:37s remains)
INFO - root - 2017-12-16 01:29:55.218109: step 84800, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 43h:33m:09s remains)
2017-12-16 01:29:55.752437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5841632 -6.914166 -8.197752 -7.7293015 -6.3423963 -3.9746077 -2.6001348 -2.5717711 -2.2092633 -3.690536 -4.1618328 -4.698853 -7.5946026 -7.4385233 -8.2200956][-8.6340914 -8.275877 -7.9788914 -7.900578 -7.4677615 -6.2139339 -4.3173442 -2.4127469 -1.5738215 -3.9573815 -4.9759636 -5.7131128 -8.3445835 -7.7756643 -8.712121][-7.3354082 -8.212677 -8.9232922 -8.3799219 -7.1240087 -6.2999821 -5.1402082 -4.116538 -2.9344416 -4.0834451 -5.3674364 -6.5595131 -8.9380426 -8.2895985 -8.7128048][-7.5785756 -8.0131044 -7.677865 -7.2716012 -6.564476 -5.1935034 -4.1408148 -3.224555 -1.9610267 -3.9708369 -4.4993334 -5.7905917 -9.0427437 -8.5482492 -8.9058867][-8.4349995 -7.9698634 -7.6586556 -6.3404365 -4.4456635 -3.1995468 -2.1563272 -1.1284771 -0.76165438 -3.2134089 -4.6967306 -6.4266305 -9.0076561 -8.4905357 -9.4484968][-7.8095608 -7.5159321 -6.7554727 -5.7063718 -3.6962485 -1.236053 1.4209738 2.363184 1.8293247 -1.5298624 -4.4166694 -6.6200538 -9.1730909 -8.9338989 -9.4959869][-8.3188925 -7.0565352 -5.9884949 -4.6651773 -2.2482119 0.275249 2.7042456 4.0262861 4.3505716 0.87487984 -2.4323 -5.3077374 -9.2107887 -9.1880932 -9.2492018][-7.12412 -7.4673905 -7.3324041 -4.6083164 -1.0650215 0.99291706 3.9142475 5.1883793 5.3435822 2.6695032 -0.11526537 -3.0641346 -7.3445191 -8.14133 -9.129199][-7.1951275 -6.5938182 -6.2881522 -5.7731371 -3.2313895 -0.15197754 2.9640522 4.1350613 4.3440971 1.3680391 -1.333777 -3.4296737 -6.8413696 -7.6087737 -8.7714834][-7.0683403 -7.3130932 -7.0407882 -5.3994637 -3.0526705 -1.0727959 1.432663 2.4107294 2.6709967 -0.84264183 -3.6214046 -5.73416 -8.3741646 -7.3472996 -7.4639888][-6.183682 -6.7825327 -7.46792 -5.9920807 -4.4097943 -2.5888972 -0.45457125 -0.23978996 -0.27861786 -3.3813338 -6.1461482 -7.5669494 -8.8004971 -7.8712091 -7.8782592][-6.2322607 -4.9022655 -4.7600679 -4.9238176 -3.9526269 -2.74859 -1.8398132 -2.4256706 -2.7654963 -3.8103032 -4.7280612 -6.6135116 -8.82494 -8.5258169 -8.5173664][-6.770658 -6.1382985 -5.509059 -4.0759783 -2.6458173 -2.5969257 -2.0973501 -2.5582085 -2.9091311 -4.8728857 -5.6771526 -6.6875191 -8.1573524 -8.3814478 -9.246974][-5.7578516 -5.769299 -6.26477 -5.34766 -3.8378487 -2.2753973 -1.697268 -2.6882386 -3.0259161 -4.5288963 -5.431612 -6.3395243 -7.1207294 -6.5422029 -6.8328848][-7.2624025 -6.1972389 -6.1242442 -6.3356709 -5.9695573 -5.2575221 -4.7457132 -4.4222431 -4.5715075 -4.590332 -4.767786 -6.3656745 -6.9937778 -6.9261 -7.01843]]...]
INFO - root - 2017-12-16 01:30:02.206052: step 84810, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:41m:31s remains)
INFO - root - 2017-12-16 01:30:08.656341: step 84820, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 43h:39m:49s remains)
INFO - root - 2017-12-16 01:30:15.033162: step 84830, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.637 sec/batch; 43h:51m:17s remains)
INFO - root - 2017-12-16 01:30:21.393875: step 84840, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 42h:54m:46s remains)
INFO - root - 2017-12-16 01:30:27.789465: step 84850, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 44h:11m:44s remains)
INFO - root - 2017-12-16 01:30:34.253002: step 84860, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 43h:42m:32s remains)
INFO - root - 2017-12-16 01:30:40.685830: step 84870, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:18m:16s remains)
INFO - root - 2017-12-16 01:30:47.119243: step 84880, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 45h:55m:07s remains)
INFO - root - 2017-12-16 01:30:53.610071: step 84890, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 43h:45m:03s remains)
INFO - root - 2017-12-16 01:31:00.083883: step 84900, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 42h:52m:23s remains)
2017-12-16 01:31:00.641779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.5206027 -0.19034815 -0.0094938278 -0.062160492 0.50549126 0.5544796 0.3387413 0.40014267 0.314456 -0.8733964 -3.1700134 -6.4134603 -8.7132549 -10.068308 -10.834293][-0.39253283 -0.3702383 -0.40993404 -0.1799469 0.19822311 0.57706165 0.81510448 0.42758846 -0.60216427 -2.1575212 -4.1725969 -6.9193144 -8.8342762 -9.3855991 -10.06733][-1.3533325 -0.95128393 -0.792953 -0.32893944 0.29671383 0.8572607 1.4900169 1.3761024 0.62186241 -1.4834952 -4.6247597 -7.5834732 -8.7534418 -9.0267372 -9.11627][-1.8724885 -1.5865135 -1.295053 -0.55623722 0.44701672 1.090539 1.2708778 0.82775974 0.5993824 -0.86593819 -3.5389209 -6.7275667 -8.477438 -8.76338 -8.7290115][-3.4762478 -2.8772111 -2.2875829 -1.4576321 -0.42293835 0.28606367 1.0822124 1.0820227 0.6194706 -1.4974194 -3.889878 -6.7189097 -8.2950811 -8.5424709 -9.2946968][-5.214859 -4.647295 -3.7088468 -2.3662977 -0.82378721 0.20468569 0.81069279 0.82364368 0.38354969 -1.4549489 -3.7083447 -6.5287638 -8.0847836 -8.1774254 -8.5557222][-5.9625759 -4.8681507 -3.6312423 -2.3843212 -0.96225166 0.145082 0.41672516 0.1668129 -0.11602449 -1.7282457 -4.0731764 -6.888124 -8.0238485 -8.287447 -9.0337353][-5.5152445 -5.378747 -4.071991 -2.3919754 -0.94581652 0.06952095 0.3696022 0.29815912 -0.057573318 -1.7027903 -4.0835123 -6.9930186 -8.3407211 -8.6278839 -8.6738157][-5.1950045 -4.4196281 -3.7446771 -2.4448504 -0.86464548 0.1380415 0.85773659 0.61130714 0.20107841 -1.571732 -3.7647676 -6.6139975 -7.92423 -8.1927824 -8.731041][-5.7282887 -5.2420921 -4.4084291 -2.813879 -1.5757356 -0.69638062 0.048191547 0.13390255 0.030560493 -1.6247139 -3.6566114 -5.6545162 -6.6979251 -6.9391122 -7.9144316][-6.2140946 -6.1710782 -5.8589687 -4.4690161 -3.1235189 -2.0100613 -1.1931491 -1.3681531 -1.5284271 -2.6008453 -4.2496805 -5.1634073 -5.9731908 -6.4211226 -6.9002495][-7.1922879 -7.042769 -6.7323351 -6.03582 -4.9906626 -4.0381317 -3.0667996 -3.0802813 -3.233294 -3.5314078 -4.5171647 -5.3101082 -5.5555983 -6.0830393 -6.4460716][-7.7811246 -7.9187403 -7.2861819 -6.517065 -5.8340263 -5.0076318 -4.2173018 -4.2047534 -4.2360115 -4.2278557 -4.3736744 -4.6675997 -4.7012672 -5.1408348 -5.4387054][-7.9554386 -8.1737862 -7.5044975 -6.7676749 -6.2118659 -5.2305865 -5.0006666 -4.8641844 -4.3031583 -4.1035566 -4.206418 -3.9922144 -3.7265015 -4.3474016 -4.674684][-8.2596073 -8.4650412 -8.5521288 -8.1228275 -7.4523339 -6.5723047 -6.2257423 -6.1788669 -6.3441463 -5.7962284 -4.9673219 -4.7511644 -4.321393 -4.5897322 -5.0128384]]...]
INFO - root - 2017-12-16 01:31:07.231806: step 84910, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 43h:35m:24s remains)
INFO - root - 2017-12-16 01:31:13.705751: step 84920, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 46h:08m:50s remains)
INFO - root - 2017-12-16 01:31:20.203961: step 84930, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 45h:22m:05s remains)
INFO - root - 2017-12-16 01:31:26.637260: step 84940, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:18m:32s remains)
INFO - root - 2017-12-16 01:31:33.074105: step 84950, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 45h:06m:10s remains)
INFO - root - 2017-12-16 01:31:39.507840: step 84960, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 44h:28m:29s remains)
INFO - root - 2017-12-16 01:31:45.861244: step 84970, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.617 sec/batch; 42h:26m:44s remains)
INFO - root - 2017-12-16 01:31:52.314637: step 84980, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 45h:16m:56s remains)
INFO - root - 2017-12-16 01:31:58.739719: step 84990, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:45m:51s remains)
INFO - root - 2017-12-16 01:32:05.239061: step 85000, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 43h:59m:53s remains)
2017-12-16 01:32:05.766252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5235205 -2.8363829 -3.2067075 -3.7560284 -4.3044629 -4.5424771 -4.5238104 -4.4528732 -4.470542 -5.153008 -5.3580475 -6.8000822 -7.38015 -8.2636538 -9.2583437][-2.883245 -3.1614647 -3.6342521 -3.9712574 -4.2544389 -4.1163187 -4.0959311 -4.2136555 -4.0479355 -5.2597585 -5.944272 -7.4053388 -8.01159 -8.6862135 -9.69186][-3.6670585 -4.0482473 -4.2668262 -4.2213564 -4.1579819 -3.9544914 -3.6875777 -3.4586692 -2.9392295 -3.9446008 -4.4433136 -5.6021209 -6.0767593 -6.875834 -7.9169607][-4.9353924 -4.6196537 -4.5526724 -3.9324574 -3.2836471 -2.53836 -1.8765273 -1.7397852 -1.4128542 -2.0359974 -2.6438155 -4.4584408 -5.2086987 -5.6626611 -6.3243723][-5.1197939 -4.8776693 -4.0136404 -3.1575041 -2.3927884 -0.96896219 -0.15536404 0.17974234 0.60176563 -0.44518375 -0.95247269 -2.6499009 -3.9842591 -4.9188337 -5.6675558][-4.8607159 -4.6124516 -3.9822075 -2.6508183 -1.345542 0.27175331 1.7178526 2.2468271 2.387886 0.90000343 -0.22141504 -2.0446796 -2.9627295 -3.9169104 -4.5732107][-5.2945681 -4.7470312 -3.938822 -2.312283 -0.71340132 1.1332235 2.6931381 3.374423 3.804471 1.9945068 0.36987305 -2.1712441 -3.7137403 -4.2784672 -4.8920836][-5.1640615 -4.1617842 -3.1034908 -1.7928538 -0.10614443 2.0498543 3.1804209 3.6416206 3.9385204 2.2904196 0.75813675 -2.0120878 -3.8329308 -4.9709892 -5.2014809][-4.0405607 -3.6146388 -3.0023623 -1.4075732 -0.25817156 1.1423492 2.1356897 2.8797531 3.1468239 1.2434759 -0.14922762 -2.2468877 -3.7593477 -5.2767982 -5.9944372][-4.5781584 -4.1507959 -3.8415945 -2.6710997 -1.7553749 -0.54444027 0.31307411 0.82385254 0.93986034 -0.94944239 -1.952497 -4.040585 -4.8046207 -5.6174736 -6.48304][-6.3564177 -6.0213308 -5.7542577 -4.8716755 -3.8976622 -2.7233491 -1.9469347 -1.5503092 -1.4269075 -3.0537548 -4.198566 -6.0059643 -6.6931729 -7.0701861 -7.1853938][-6.8532271 -6.7293119 -6.3613491 -6.0362692 -5.558547 -4.7807255 -4.2820749 -3.887816 -3.6902103 -4.8318076 -5.4997797 -6.4548674 -6.6564307 -7.0682406 -7.5283055][-6.7818294 -6.9604335 -7.1896753 -6.8316174 -6.6957555 -6.4105806 -6.2757931 -6.1302481 -6.0484123 -6.8121347 -7.1130705 -7.4404073 -7.6640573 -7.5625706 -7.592916][-6.6671119 -6.79849 -7.0038681 -7.0121818 -6.7534428 -6.2586207 -6.313899 -6.6744623 -6.7120814 -7.1230145 -7.4569907 -7.5184145 -7.5956025 -7.3368254 -7.0718565][-8.4408531 -8.207696 -8.1443014 -8.0056791 -7.7154322 -7.7721748 -8.1146507 -8.02486 -8.1081228 -8.2245655 -8.1705952 -7.9328465 -7.4809589 -7.0747967 -6.8165536]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-85000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-85000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 01:32:13.117535: step 85010, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:17m:08s remains)
INFO - root - 2017-12-16 01:32:19.570388: step 85020, loss = 0.35, batch loss = 0.24 (12.2 examples/sec; 0.655 sec/batch; 45h:02m:51s remains)
INFO - root - 2017-12-16 01:32:25.952361: step 85030, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:44m:16s remains)
INFO - root - 2017-12-16 01:32:32.515030: step 85040, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:49m:59s remains)
INFO - root - 2017-12-16 01:32:38.956674: step 85050, loss = 0.39, batch loss = 0.27 (12.7 examples/sec; 0.631 sec/batch; 43h:22m:40s remains)
INFO - root - 2017-12-16 01:32:45.315609: step 85060, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 43h:49m:38s remains)
INFO - root - 2017-12-16 01:32:51.712422: step 85070, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:55m:41s remains)
INFO - root - 2017-12-16 01:32:58.152477: step 85080, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 44h:33m:53s remains)
INFO - root - 2017-12-16 01:33:04.656824: step 85090, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 45h:25m:12s remains)
2017-12-16 01:33:09.207077: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 39535848 get requests, put_count=39535853 evicted_count=39000 eviction_rate=0.000986446 and unsatisfied allocation rate=0.000986598
INFO - root - 2017-12-16 01:33:11.144457: step 85100, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:16m:00s remains)
2017-12-16 01:33:11.678139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6327305 -5.1528072 -4.5330925 -4.0112972 -3.6370373 -3.1099229 -2.7043824 -2.6307049 -2.3825455 -3.6778784 -4.8811359 -5.4532318 -6.7848544 -7.4031997 -7.8436575][-5.0380898 -4.5439091 -3.9979262 -3.8119967 -3.2868924 -3.0425568 -3.0358782 -3.0518332 -2.9645119 -4.2090278 -5.1944447 -5.541647 -6.9277797 -7.3775291 -7.8289747][-4.1223316 -3.749578 -3.53088 -3.4538679 -3.2316418 -2.8673854 -2.5258098 -2.5532188 -2.421062 -3.8637109 -5.1678972 -5.5935497 -6.946558 -7.4467282 -7.6606894][-3.8815522 -3.4027777 -3.0269308 -2.8010507 -2.4010353 -2.3253007 -2.2258134 -2.3041878 -2.2723231 -3.6210575 -4.6509705 -5.0154781 -6.4386711 -7.0599151 -7.4174833][-3.8272314 -2.8469467 -1.8035102 -1.4721913 -1.6096077 -1.4848542 -1.1776052 -1.361321 -1.348659 -2.9094853 -3.9817877 -4.3022757 -5.7780638 -6.5378461 -6.5357485][-2.8139954 -2.1316619 -1.6614413 -1.201385 -0.44851732 -0.23506498 -0.34284258 -0.52187109 -0.3592701 -1.8929667 -3.0709572 -3.6581912 -5.14886 -5.7407861 -6.2292709][-2.6261115 -1.9118719 -1.0035167 -0.65353584 -0.2348299 0.24471807 0.69863987 0.69691086 0.60664082 -0.91488647 -2.1227598 -2.7546377 -4.3908939 -5.1264372 -5.6269326][-2.0487585 -1.3637109 -0.94644833 -0.3561306 0.66640186 1.2396069 1.2716827 1.6175222 2.0953293 0.47019196 -1.1326513 -1.9969225 -3.8384807 -4.8835526 -5.2728467][-2.1225185 -1.4654145 -0.58879566 0.13903522 0.52847767 0.90173149 1.6209507 1.8734941 1.9624643 0.34434986 -1.0476823 -1.9488721 -3.9797816 -4.9304605 -5.4278669][-2.9764295 -2.2189751 -1.1868348 -0.32042503 0.48538303 0.96932125 1.0614996 1.0090971 1.3033438 -0.60270405 -1.9704471 -2.9235644 -4.6841946 -5.5754223 -5.9576664][-3.3992486 -2.7754445 -2.6257043 -1.7129011 -0.99900866 -0.35537577 -0.17030239 -0.26031637 -0.39851713 -2.1407022 -3.354877 -4.31448 -5.7172213 -6.540164 -6.4772177][-3.8187783 -3.0145712 -2.7591105 -2.2539396 -1.6944413 -1.1504645 -1.097188 -1.4320626 -1.788115 -3.1595306 -4.4625635 -5.2885761 -6.5215287 -7.097177 -6.9625731][-4.2528343 -3.664824 -3.653326 -3.2867441 -2.7728219 -2.3673391 -2.2104716 -2.7849579 -3.25697 -4.3019085 -5.2153091 -5.5779095 -6.2523069 -6.623476 -6.5199523][-4.868494 -4.2760406 -4.0476866 -4.0072136 -4.3901224 -4.021903 -3.9599235 -4.4104662 -4.4896421 -5.0868959 -5.2742338 -5.588994 -6.1316452 -6.0793867 -5.7104254][-5.3781595 -5.4125271 -5.2116528 -5.2887936 -5.4528213 -5.2421794 -5.4112582 -5.5872989 -5.7541242 -6.0565968 -5.861815 -5.933043 -5.725996 -5.5335274 -5.3101463]]...]
INFO - root - 2017-12-16 01:33:17.992717: step 85110, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 43h:35m:18s remains)
INFO - root - 2017-12-16 01:33:24.446417: step 85120, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:42m:55s remains)
INFO - root - 2017-12-16 01:33:30.901747: step 85130, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 44h:22m:50s remains)
INFO - root - 2017-12-16 01:33:37.244498: step 85140, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 43h:23m:37s remains)
INFO - root - 2017-12-16 01:33:43.676431: step 85150, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 43h:21m:37s remains)
INFO - root - 2017-12-16 01:33:50.172008: step 85160, loss = 0.37, batch loss = 0.25 (12.3 examples/sec; 0.651 sec/batch; 44h:41m:50s remains)
INFO - root - 2017-12-16 01:33:56.612680: step 85170, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 44h:36m:46s remains)
INFO - root - 2017-12-16 01:34:03.008471: step 85180, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 44h:18m:41s remains)
INFO - root - 2017-12-16 01:34:09.475283: step 85190, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 43h:57m:28s remains)
INFO - root - 2017-12-16 01:34:15.915662: step 85200, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.621 sec/batch; 42h:39m:41s remains)
2017-12-16 01:34:16.478698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8561916 -4.4622993 -4.8106661 -4.6435595 -4.6473007 -4.5484786 -3.991996 -3.6333218 -2.4346309 -1.567771 -1.0883322 -5.0562878 -8.6700191 -9.948349 -10.841444][-3.3082404 -3.8605576 -4.942728 -5.4992847 -6.1912351 -6.9338112 -6.7658849 -5.8848224 -4.436429 -2.9451485 -2.14717 -5.6426353 -8.3005171 -9.5691624 -10.783791][-3.3300915 -3.1376505 -4.1997886 -5.0624142 -6.3931446 -7.3287039 -7.9413061 -8.2563982 -6.7775154 -5.031095 -3.976619 -6.3251376 -8.5663433 -9.6779079 -9.8314476][-3.8670564 -3.7784359 -4.1833725 -4.382781 -4.951335 -5.5975032 -6.6050014 -7.1892958 -6.3965755 -4.4172797 -3.0065861 -5.959847 -8.6581059 -9.8946524 -9.9624243][-4.6497555 -3.6752172 -4.0848713 -4.2308784 -4.7400961 -4.5543022 -4.2725458 -4.7194128 -4.306695 -3.3324327 -2.9049745 -6.8190651 -8.8962536 -10.593674 -10.809282][-4.7692466 -4.2626753 -4.2237244 -3.9499598 -3.8648252 -2.8806024 -1.8897204 -1.1142182 -0.32449198 -0.12803507 -0.78884649 -5.7983232 -8.8090076 -9.96813 -10.103774][-3.9437876 -2.8983369 -2.8461633 -2.1401463 -1.3985062 -0.096876144 1.2669954 2.3652048 3.6740217 3.6768694 2.2566071 -3.8201318 -8.2176018 -10.06316 -10.000237][-3.4780664 -1.9658885 -1.5473094 -0.98623848 -0.20970106 1.4781961 2.8698883 4.0121555 5.3023643 5.5271521 4.5039272 -1.6969461 -5.631423 -8.4049788 -9.6493082][-3.0939927 -2.4929385 -2.2934117 -1.477046 -0.85285616 0.43992519 1.7655506 2.9100437 3.3904085 3.3529024 2.1553802 -3.4438014 -6.5346622 -8.4836044 -9.1791782][-3.991395 -3.34903 -3.5741234 -3.1933365 -2.8651276 -1.9668655 -0.36886358 0.57389545 1.2645664 0.27780008 -1.2541637 -5.7540255 -8.5432968 -9.2228489 -9.03852][-7.2311444 -6.4752645 -6.3179059 -5.5603848 -5.2560215 -4.2640715 -3.4281154 -2.4436355 -1.2809649 -1.8340144 -2.7776814 -6.9410095 -9.4887657 -9.968709 -9.6264067][-7.6637759 -8.0701656 -7.8527064 -6.6869574 -6.5037751 -6.0781684 -6.0295944 -5.1327462 -4.2990742 -4.0129809 -4.4360361 -7.3011189 -8.7792015 -9.0033855 -8.6783495][-7.9767952 -8.53192 -8.93711 -8.4405165 -7.9271431 -7.595428 -7.3493228 -7.3202615 -7.2609859 -7.1876321 -6.5274115 -7.9014835 -8.0757132 -8.1132708 -7.9623628][-8.6797256 -8.8472738 -9.0740576 -8.5584755 -7.8531866 -7.6680269 -7.2162848 -7.5500979 -7.9454885 -7.8417583 -7.6834655 -7.6477675 -7.5852761 -7.3223228 -7.1859875][-9.1420355 -10.309139 -10.34564 -9.4718857 -9.2402935 -9.1107235 -8.694602 -8.5778913 -8.6858883 -8.5635033 -8.8011026 -8.534605 -7.6782856 -7.4372497 -7.2063317]]...]
INFO - root - 2017-12-16 01:34:22.966874: step 85210, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 45h:07m:26s remains)
INFO - root - 2017-12-16 01:34:29.325590: step 85220, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 43h:54m:27s remains)
INFO - root - 2017-12-16 01:34:35.803816: step 85230, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 45h:21m:38s remains)
INFO - root - 2017-12-16 01:34:42.193771: step 85240, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 43h:40m:34s remains)
INFO - root - 2017-12-16 01:34:48.640372: step 85250, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 43h:26m:58s remains)
INFO - root - 2017-12-16 01:34:55.080349: step 85260, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 44h:48m:47s remains)
INFO - root - 2017-12-16 01:35:01.475219: step 85270, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 44h:05m:18s remains)
INFO - root - 2017-12-16 01:35:07.961975: step 85280, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 44h:58m:29s remains)
INFO - root - 2017-12-16 01:35:14.481146: step 85290, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:41m:31s remains)
INFO - root - 2017-12-16 01:35:20.953407: step 85300, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 44h:02m:17s remains)
2017-12-16 01:35:21.517129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6245742 -3.1119466 -2.8745723 -3.4975581 -3.9615347 -4.5495672 -4.6440506 -5.2977533 -5.3298607 -5.6532812 -5.3514385 -5.4643049 -6.2126956 -7.9657283 -8.1894617][-2.9211955 -2.5949421 -2.2101603 -2.6433849 -2.8067441 -3.1109037 -3.3798347 -4.3912048 -4.7070408 -6.1731558 -6.5390606 -6.2138987 -6.8965216 -7.9402657 -7.8657274][-1.3599024 -0.77924347 -0.96439695 -1.1433067 -1.6425271 -2.1897283 -2.0651445 -2.4474621 -3.1897607 -5.2111397 -5.3378057 -5.2794113 -5.9735832 -6.7787509 -7.2251248][-1.0668044 -0.17275858 0.31251812 -0.32375956 -0.975368 -1.5274062 -1.9136729 -1.8401985 -2.0189161 -3.485589 -3.3238897 -3.6051221 -4.5424271 -5.2173738 -5.7866888][-2.2914844 -1.2192702 -0.68746328 -0.45146275 -0.45301771 -0.53856516 -0.58169031 -0.81911278 -1.036994 -2.1075864 -1.814743 -2.3980727 -3.2419033 -4.1163578 -5.1071663][-2.788754 -2.0705137 -1.7224298 -1.0951676 -0.5411396 0.34001732 0.73119926 0.57636642 0.43345547 -0.94907522 -0.92212868 -1.6106687 -2.5008307 -3.6385932 -4.0102396][-1.026989 -1.0451522 -0.61311007 -0.148489 0.41789913 1.1950855 1.3147993 1.4214954 1.6032305 -0.19271612 -0.54801178 -1.1145253 -2.0876093 -3.4944358 -3.8593924][0.051543236 -0.02529335 0.57893085 1.2800856 2.006856 2.4199266 2.3885832 2.2640667 2.3426895 0.38395691 -0.1275382 -1.1447868 -2.2050614 -3.9378016 -4.235177][-0.26316547 0.85460377 1.4876719 1.9624395 2.6935616 2.7463665 2.905385 3.002079 2.8674641 0.8797102 0.25187111 -1.0779724 -2.299902 -4.2349052 -4.880043][-0.90910053 0.38716698 1.3551693 2.3348808 2.6559286 2.977663 3.1117496 3.2145452 2.8414125 0.69314575 -0.14063072 -1.1563954 -2.4651556 -4.3525567 -4.8339148][-3.1535068 -2.5824747 -1.8310466 -1.0867982 -0.50210333 -0.1479969 -0.26341677 -0.1592617 -0.34708261 -1.9056153 -3.3137426 -3.5600753 -4.2352266 -5.8915071 -6.1997066][-6.0660753 -5.3104 -5.1493006 -4.51023 -3.7007515 -3.2108135 -3.0092058 -2.9541016 -3.0077043 -4.3559732 -5.36012 -5.3790436 -5.9056211 -6.9195023 -7.2372055][-6.4084206 -5.9213114 -6.1889839 -6.1306806 -6.0656652 -5.5511484 -5.2717342 -5.4932232 -5.31207 -5.7191124 -6.2630315 -6.0114417 -6.3441925 -6.3630762 -6.182117][-5.9043441 -6.0245094 -6.0962605 -6.2427325 -6.5173073 -5.9247575 -5.80301 -5.9088354 -6.0623279 -6.2964215 -6.0474596 -5.5068932 -4.9750605 -5.2496123 -5.5110869][-5.50152 -5.0043478 -5.113596 -5.63401 -6.0596457 -5.7704258 -5.6131358 -5.4270258 -5.76018 -5.7230248 -5.5924573 -5.2744226 -4.8535852 -4.5743589 -4.1507206]]...]
INFO - root - 2017-12-16 01:35:27.970313: step 85310, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 45h:27m:45s remains)
INFO - root - 2017-12-16 01:35:34.529380: step 85320, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 44h:59m:32s remains)
INFO - root - 2017-12-16 01:35:40.977204: step 85330, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 46h:10m:58s remains)
INFO - root - 2017-12-16 01:35:47.377213: step 85340, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 42h:56m:03s remains)
INFO - root - 2017-12-16 01:35:53.755552: step 85350, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 44h:01m:02s remains)
INFO - root - 2017-12-16 01:36:00.153830: step 85360, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 43h:03m:59s remains)
INFO - root - 2017-12-16 01:36:06.561663: step 85370, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 42h:57m:50s remains)
INFO - root - 2017-12-16 01:36:13.041604: step 85380, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 43h:22m:24s remains)
INFO - root - 2017-12-16 01:36:19.414417: step 85390, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 44h:26m:26s remains)
INFO - root - 2017-12-16 01:36:25.814940: step 85400, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 43h:18m:27s remains)
2017-12-16 01:36:26.372625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7414565 -5.4650984 -5.2477341 -4.9879236 -4.6380062 -4.268115 -3.9502032 -3.6126218 -3.3984208 -3.4621882 -4.06133 -5.33749 -6.10537 -6.9513173 -7.3346109][-4.7113004 -4.9783382 -4.9471154 -4.5321217 -4.378521 -4.0186048 -3.7915928 -3.5552278 -3.5016727 -3.704869 -4.6849918 -5.820528 -6.6946888 -7.7091031 -7.6700006][-3.6003847 -4.1852913 -4.33183 -3.8027837 -3.2418098 -2.9917111 -2.9372439 -2.7430353 -2.4558663 -2.7619348 -3.935864 -5.2407379 -5.9437366 -6.93186 -6.96243][-3.2572484 -3.1215572 -3.1766162 -3.2640886 -2.8673673 -1.9773993 -1.3552527 -1.2522216 -0.98626232 -1.2134085 -2.5838494 -4.1110396 -4.8977385 -5.9864388 -6.4898462][-2.9163642 -2.8465142 -2.6388607 -2.4958611 -2.0728045 -1.0895705 -0.20081186 -0.035057545 0.040259838 -0.38522816 -1.4312983 -2.8990755 -4.0100613 -5.3305035 -5.7788615][-2.7204189 -2.5402946 -2.1534715 -1.5081067 -0.84348917 0.093958855 0.86187649 1.0881386 0.94530773 0.061855793 -1.0125251 -2.2946873 -3.0150542 -4.5427303 -5.4504032][-3.5469599 -2.9228125 -2.0680909 -1.0701289 -0.077014446 0.8565321 1.5856771 1.7494364 1.6471729 0.81669331 -0.020253181 -1.5667214 -2.8022985 -4.0115213 -4.9618196][-3.8323414 -2.9554152 -2.112165 -1.2772584 0.17836523 1.1133022 1.7183857 1.9571009 1.8377619 1.1803484 0.58677769 -0.83256626 -2.2305441 -4.1454849 -5.3288679][-3.567904 -3.1607594 -2.4190145 -1.6240177 -0.3206687 0.85995388 1.4213428 1.7528801 1.8313017 1.2562675 0.70850277 -0.77145386 -2.3293056 -3.8655381 -5.2969389][-3.732605 -3.3493571 -2.8309636 -1.9189577 -0.99297619 -0.19518805 0.10843134 0.52531624 0.93875504 0.052176952 -0.67971516 -2.0181484 -3.0535555 -4.3542652 -5.0834656][-5.14662 -4.54987 -3.8687143 -2.9593463 -2.3020406 -1.6283035 -1.4659824 -1.2774711 -0.98169041 -1.4985323 -2.2235603 -3.3514051 -4.12424 -4.9464331 -5.4417419][-5.9521589 -5.0907397 -4.2996125 -3.4814606 -2.7167735 -2.1073194 -2.0481834 -1.8215518 -1.7867422 -2.7902198 -3.3905401 -3.6158614 -4.4944782 -5.2553015 -5.6000347][-6.3666916 -4.9444351 -4.1164503 -3.3019114 -2.8899913 -2.2496381 -2.0824738 -2.2796698 -2.4265003 -3.2886868 -4.0094128 -4.1347861 -4.1864777 -4.5255632 -4.9372082][-5.9895549 -4.8020973 -3.9268711 -3.4245019 -3.0173583 -2.3235478 -2.3391337 -2.7150545 -2.9817257 -3.4490695 -3.6478939 -4.0797548 -4.0607595 -3.6077933 -3.7990727][-6.789587 -5.6293635 -5.0861263 -4.1128297 -3.4172459 -2.8192258 -2.9158778 -3.0733113 -3.4471774 -3.5453897 -3.6594124 -4.0037408 -4.183959 -4.1782417 -4.3633151]]...]
INFO - root - 2017-12-16 01:36:32.794028: step 85410, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 43h:48m:55s remains)
INFO - root - 2017-12-16 01:36:39.228500: step 85420, loss = 0.32, batch loss = 0.21 (12.4 examples/sec; 0.644 sec/batch; 44h:11m:37s remains)
INFO - root - 2017-12-16 01:36:45.655331: step 85430, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 46h:11m:52s remains)
INFO - root - 2017-12-16 01:36:52.031993: step 85440, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 42h:52m:26s remains)
INFO - root - 2017-12-16 01:36:58.398645: step 85450, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 45h:01m:52s remains)
INFO - root - 2017-12-16 01:37:04.810523: step 85460, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 43h:39m:17s remains)
INFO - root - 2017-12-16 01:37:11.306255: step 85470, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 43h:33m:02s remains)
INFO - root - 2017-12-16 01:37:17.725144: step 85480, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 43h:06m:11s remains)
INFO - root - 2017-12-16 01:37:24.139629: step 85490, loss = 0.26, batch loss = 0.15 (11.5 examples/sec; 0.695 sec/batch; 47h:41m:43s remains)
INFO - root - 2017-12-16 01:37:30.514422: step 85500, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 44h:03m:44s remains)
2017-12-16 01:37:31.030577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9875202 -4.6008334 -5.2884893 -6.0667896 -6.9981785 -7.4046483 -6.626997 -6.0042896 -4.6675839 -3.5840878 -3.6801176 -4.5449286 -5.4205189 -6.3551545 -6.9506712][-4.29362 -4.3509188 -5.02823 -5.9612379 -6.8802252 -7.6458764 -7.7212272 -7.1163664 -5.3117237 -4.3128424 -4.2935104 -4.8912249 -5.5872688 -6.5414786 -7.3737106][-4.6700454 -4.527451 -4.7184234 -5.0039597 -5.7538261 -6.5338173 -6.4054718 -6.1327848 -5.348918 -4.8262625 -4.7920227 -5.5547237 -6.6376381 -7.2782307 -7.6096377][-4.6812439 -4.6401367 -4.2658386 -3.8911436 -3.3954854 -3.4763241 -3.1906171 -3.3313813 -2.3877773 -2.2066536 -3.3154392 -4.7465677 -6.2039862 -7.1979485 -7.8373156][-4.7079592 -3.9733448 -2.8763371 -1.8878875 -1.5807285 -1.114264 -0.37660742 -0.075464725 0.69199657 0.54646206 -0.93505049 -2.8969355 -4.8293457 -5.8803596 -6.8936329][-3.9941068 -3.4015293 -2.0533857 -0.73868704 0.2301321 1.0064917 1.3536024 1.5614061 1.7795076 1.3450289 -0.37590981 -2.6104646 -4.4445572 -5.3182611 -5.9988503][-3.2914987 -2.667419 -1.0309463 0.29812145 1.5888805 2.4596519 2.9606924 3.1767197 3.1193008 2.3494806 0.12172556 -1.9131775 -3.66751 -4.7883968 -5.5289621][-2.5918107 -2.0992956 -0.63522053 0.50109577 1.3336267 2.4823484 3.2858629 3.5462551 3.6587076 2.9947815 0.97836685 -1.3153267 -3.389359 -4.4798908 -5.5273566][-3.06148 -2.1203341 -1.6065173 -0.73702097 0.25697184 1.167181 1.4347763 1.8091583 2.2938128 1.647089 -0.3729229 -2.3398285 -4.3661623 -5.5208116 -6.0954504][-4.3175068 -3.7295587 -2.8441191 -2.2005801 -1.8089385 -1.279685 -0.822577 -0.41564465 -0.4912653 -1.1068044 -2.7593713 -3.7465549 -5.2526073 -6.13313 -6.8309188][-6.6370726 -6.4202304 -5.5357451 -4.8455849 -4.3285475 -3.8980386 -3.190629 -2.9550571 -3.0202904 -3.1578183 -4.1429138 -4.4860015 -5.394846 -6.0275784 -6.2468166][-9.2260542 -8.5515757 -8.280612 -7.4821758 -6.5942154 -6.2259817 -5.7203226 -5.6897755 -5.4056249 -5.1850176 -5.7516441 -6.2297654 -6.8174782 -6.6011248 -6.3630424][-10.912733 -11.010418 -10.677599 -9.33499 -8.4235172 -7.6020284 -6.7236085 -6.9974194 -6.908545 -6.5580478 -6.92684 -6.5727954 -6.3251991 -6.2605186 -6.2590017][-10.239349 -9.9483137 -9.3059139 -8.6998806 -8.1239891 -6.9186168 -6.1743536 -6.3671188 -6.3638415 -6.1656518 -6.4947605 -6.3097382 -6.0810752 -5.7979488 -5.5519791][-9.6108246 -9.2128973 -8.7875805 -8.3463812 -7.63901 -6.8669486 -6.3760815 -6.3019519 -6.274323 -6.2392082 -6.2880726 -6.125216 -5.8538871 -6.1528 -5.8201976]]...]
INFO - root - 2017-12-16 01:37:37.432802: step 85510, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 44h:35m:38s remains)
INFO - root - 2017-12-16 01:37:43.921578: step 85520, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 43h:50m:47s remains)
INFO - root - 2017-12-16 01:37:50.363834: step 85530, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 43h:57m:14s remains)
INFO - root - 2017-12-16 01:37:56.766060: step 85540, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 44h:38m:14s remains)
INFO - root - 2017-12-16 01:38:03.129076: step 85550, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.637 sec/batch; 43h:43m:39s remains)
INFO - root - 2017-12-16 01:38:09.541156: step 85560, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 44h:56m:27s remains)
INFO - root - 2017-12-16 01:38:15.903290: step 85570, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.624 sec/batch; 42h:46m:31s remains)
INFO - root - 2017-12-16 01:38:22.440469: step 85580, loss = 0.32, batch loss = 0.21 (12.3 examples/sec; 0.648 sec/batch; 44h:26m:53s remains)
INFO - root - 2017-12-16 01:38:28.843929: step 85590, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 42h:49m:43s remains)
INFO - root - 2017-12-16 01:38:35.277784: step 85600, loss = 0.32, batch loss = 0.21 (13.0 examples/sec; 0.618 sec/batch; 42h:22m:00s remains)
2017-12-16 01:38:35.822232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8385839 -3.4888525 -3.1619372 -2.0604148 -1.1191387 -0.006383419 1.1593838 1.4920206 1.5706825 0.49498367 -0.49598598 -2.8953271 -3.9168618 -5.6666117 -6.6550341][-3.5856147 -3.1103668 -2.5640759 -1.745369 -0.79232168 0.39941025 1.3733664 1.7949114 1.8667955 0.3757906 -0.58950663 -2.7277312 -4.0032959 -5.6957383 -6.1280923][-3.4782963 -2.9957461 -2.7043638 -2.1749516 -1.4914842 -0.44731188 0.74444771 0.94644547 0.94875145 -0.17798328 -0.79581261 -2.8430777 -3.8530877 -5.249608 -5.9851117][-3.7903903 -3.4684892 -3.3671169 -2.6883507 -1.6643696 -0.63792944 0.23901701 0.61061096 1.0855932 -0.40757132 -1.2448487 -3.4217167 -4.4121389 -5.3432703 -5.5955234][-4.0543175 -3.5071659 -2.9822264 -2.335546 -1.5257788 -0.53152132 0.33558083 0.77957153 1.1065426 -0.3136816 -1.2078681 -3.2482944 -4.3189821 -5.5198674 -6.1037154][-4.4616337 -4.0237694 -3.4342928 -2.1957588 -0.93838167 -0.14243793 0.57900906 1.108161 1.2832355 -0.13891602 -0.97866106 -3.0478206 -4.2827578 -5.47532 -6.2048764][-4.5132108 -3.8345819 -3.4360285 -2.5208826 -1.5929308 -0.22280788 1.1477041 1.5585413 1.8596201 0.62406635 -0.42354202 -2.9925981 -4.4998732 -5.6696677 -6.2211838][-4.7604995 -3.6443162 -2.6372662 -1.7128878 -0.5302372 0.84971237 1.6796112 2.2427263 2.9027023 1.2030458 0.060679913 -2.2250733 -3.8296883 -5.5618672 -6.5694671][-3.6954036 -3.2748418 -2.5873947 -1.0275302 0.20136547 1.0184278 1.937439 2.6212158 2.8959732 1.2267866 0.18361568 -2.530704 -4.0860615 -5.4060678 -6.2778912][-4.1553321 -3.2488818 -2.1857209 -1.1556816 -0.13783026 1.4415751 2.4695892 2.4269857 2.5937891 1.0091963 0.052378178 -2.6918344 -4.3416295 -5.9722233 -6.8337336][-4.09729 -3.5012693 -2.9027238 -1.6840997 -0.35091496 0.81258106 1.680891 2.2897463 2.1992092 -0.089413643 -1.2299967 -3.5629907 -5.2131596 -6.743701 -7.3828211][-3.5653386 -3.1743455 -2.5776243 -1.3463192 0.05698204 1.0248613 1.9903288 1.9985867 1.5698433 -0.25967646 -1.8706007 -4.3852139 -5.7954149 -6.7056651 -7.4004335][-4.2952266 -3.6415462 -2.57866 -1.4149857 -0.3720026 0.49664116 1.0989265 1.0332747 0.63421059 -0.93245029 -2.079783 -4.2184715 -5.67776 -6.7494745 -7.3667455][-4.1782017 -3.8664103 -3.3194022 -2.2963281 -1.1713572 -0.59041882 -0.28941393 -0.51321936 -1.1829348 -2.234365 -3.1230793 -4.3188477 -5.1320319 -6.3754878 -7.1111073][-5.7727656 -5.3962379 -4.4574218 -3.8756645 -3.187911 -3.0744486 -3.1614404 -2.9113898 -3.1742444 -4.0198088 -4.6749992 -5.3976688 -6.2422109 -6.680727 -6.7947788]]...]
INFO - root - 2017-12-16 01:38:42.240530: step 85610, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 44h:34m:38s remains)
INFO - root - 2017-12-16 01:38:48.640971: step 85620, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 43h:56m:31s remains)
INFO - root - 2017-12-16 01:38:55.091962: step 85630, loss = 0.24, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 45h:34m:28s remains)
INFO - root - 2017-12-16 01:39:01.472668: step 85640, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 43h:39m:59s remains)
INFO - root - 2017-12-16 01:39:07.951974: step 85650, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 44h:28m:06s remains)
INFO - root - 2017-12-16 01:39:14.360968: step 85660, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 42h:59m:44s remains)
INFO - root - 2017-12-16 01:39:20.718815: step 85670, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 43h:33m:09s remains)
INFO - root - 2017-12-16 01:39:27.156942: step 85680, loss = 0.24, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 44h:56m:30s remains)
INFO - root - 2017-12-16 01:39:33.498017: step 85690, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 43h:37m:42s remains)
INFO - root - 2017-12-16 01:39:39.961750: step 85700, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 44h:49m:11s remains)
2017-12-16 01:39:40.454079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5749631 -4.5667429 -3.4826231 -2.9970117 -2.7784967 -2.8549619 -2.4497986 -2.1228046 -2.1175175 -3.4917555 -4.1950493 -5.4529986 -7.5236616 -8.2027769 -9.6877842][-5.0908031 -4.93299 -4.8700743 -4.8058205 -4.6491985 -4.0569415 -3.0280557 -2.9843869 -2.8316193 -3.9589205 -5.3263268 -6.5139351 -7.936233 -8.7704105 -10.228311][-5.8822427 -4.8569384 -3.383379 -3.1294513 -3.2647843 -3.2139845 -2.76153 -2.2862773 -1.6911416 -3.1657081 -4.5122223 -5.4052191 -7.0829253 -8.2722206 -9.5925665][-4.5212917 -3.7708149 -3.0741067 -1.9324541 -0.97306919 -0.24337578 0.22180986 -0.1904068 -0.53819323 -2.1931348 -3.7403641 -4.8954234 -6.64533 -6.9574876 -7.7045441][-4.2001181 -2.365624 -1.0782323 -0.371737 0.2957058 1.1498318 1.7096748 1.1810522 0.5308075 -1.7580538 -3.4039092 -4.2003107 -5.5217752 -5.7446384 -6.8867474][-3.958961 -2.7935743 -1.8883009 -0.4563241 1.1136141 1.7664137 2.2947884 2.3035488 2.046648 0.14594555 -1.0925198 -2.1912713 -3.9463675 -4.3476977 -5.27569][-3.8490992 -2.8017588 -1.1956463 0.24327374 2.1394253 3.0187769 3.5045204 3.2775774 2.92774 0.99831009 -0.30147457 -1.2384834 -3.2544084 -3.5696936 -4.6962619][-1.4694452 -0.76701355 0.035013676 1.2851048 2.3822489 3.099885 3.4222193 3.247406 2.9614353 0.83464527 -0.72154093 -1.8885593 -3.7062101 -3.8157642 -4.4087715][-1.2352695 -0.16381931 0.5264349 0.30560923 0.9027338 1.5689774 1.8944874 2.3112202 2.3044682 0.52696037 -0.17507029 -1.0708637 -3.3082948 -3.8258193 -5.126359][-1.8347888 -1.6331816 -1.5346694 -0.76705551 -0.09816885 0.49689579 0.69315338 0.79721451 0.808753 -1.0394225 -1.7656164 -2.4628611 -3.5837331 -3.9754086 -4.7468281][-1.2266335 -1.5211563 -1.5952177 -1.1422977 -1.0671606 -1.0312266 -0.9102869 -1.2591386 -1.6341643 -3.0791612 -3.7374024 -4.4351921 -5.9536152 -5.5474572 -5.5156565][-2.2190704 -1.3506947 -0.74423027 -1.4255867 -2.1083145 -2.3674636 -2.3990812 -2.817843 -3.2107487 -4.68341 -5.5493975 -6.3556128 -7.2947278 -7.0311184 -6.80567][-2.7701983 -2.7069798 -2.3388572 -2.9398966 -3.5653362 -3.6348467 -3.6869087 -3.7970235 -4.3405972 -5.9250669 -6.7936611 -6.8665385 -8.0313416 -7.5787153 -7.2296276][-3.7803411 -3.9539907 -3.9952335 -3.5015311 -3.4803262 -3.7468178 -3.7496729 -4.2314034 -4.5847282 -5.1660733 -5.825428 -6.6327319 -7.3579955 -7.2969265 -6.8343916][-6.1389995 -5.3070769 -4.24017 -3.8807907 -4.2012663 -4.2673025 -4.4633865 -5.0051675 -5.5811372 -6.1211882 -6.4875317 -6.7277822 -7.2796278 -7.2080679 -6.9590158]]...]
INFO - root - 2017-12-16 01:39:46.955871: step 85710, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 44h:19m:29s remains)
INFO - root - 2017-12-16 01:39:53.342888: step 85720, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 44h:18m:42s remains)
INFO - root - 2017-12-16 01:39:59.785305: step 85730, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 44h:48m:53s remains)
INFO - root - 2017-12-16 01:40:06.196014: step 85740, loss = 0.29, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 45h:41m:21s remains)
INFO - root - 2017-12-16 01:40:12.704273: step 85750, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 44h:01m:55s remains)
INFO - root - 2017-12-16 01:40:19.147098: step 85760, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 45h:37m:08s remains)
INFO - root - 2017-12-16 01:40:25.646861: step 85770, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 43h:29m:52s remains)
INFO - root - 2017-12-16 01:40:32.098186: step 85780, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 43h:09m:13s remains)
INFO - root - 2017-12-16 01:40:38.459976: step 85790, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 43h:50m:00s remains)
INFO - root - 2017-12-16 01:40:44.826535: step 85800, loss = 0.26, batch loss = 0.15 (13.1 examples/sec; 0.613 sec/batch; 41h:59m:32s remains)
2017-12-16 01:40:45.351615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5822482 -4.1200066 -3.3541827 -3.0401011 -3.0021625 -2.9451261 -2.6351113 -2.499557 -2.4261894 -3.3462181 -4.9043984 -5.0142403 -5.2682076 -5.6521082 -5.9926276][-3.8142867 -3.5368795 -2.848918 -2.6371841 -2.3574953 -2.4166894 -2.6437688 -2.7137928 -2.313539 -3.1637492 -4.0221586 -4.5672817 -5.4810829 -5.4834642 -5.4973903][-3.8057303 -3.7299771 -3.3454447 -2.9382362 -2.5798683 -2.5142937 -2.4067841 -2.6112609 -2.9207635 -3.5482774 -4.4404278 -4.5325079 -5.400826 -5.9196568 -6.128355][-5.1395979 -4.5718555 -4.0132713 -3.7397194 -3.1408849 -2.4460635 -1.8421631 -1.7747436 -1.6393147 -2.7211337 -4.2637625 -5.0414119 -6.0735579 -6.0837307 -6.1435127][-5.9581308 -5.352622 -4.7589359 -4.0512247 -2.8626571 -1.8828502 -1.2704043 -1.299293 -1.3017492 -2.0131369 -3.4541211 -4.6676888 -6.0673327 -6.9349089 -7.0885983][-5.1962457 -4.4682779 -3.813081 -3.2837939 -2.2041063 -0.91815138 0.26889706 0.30245066 0.0020565987 -0.74036455 -2.2071486 -3.3726778 -4.7968187 -5.7681 -6.6856642][-4.0804186 -3.3848958 -2.3005419 -1.4060359 -0.48412561 0.59936333 1.3360968 1.3737726 1.2977419 0.65653992 -0.85802031 -2.0283074 -3.5280371 -4.8029318 -5.7211013][-2.6915035 -1.9841671 -1.402534 -0.38184261 0.697176 1.3837919 2.06001 2.1079788 1.9491749 1.3570032 -0.44013834 -1.5583615 -2.8967032 -3.9775512 -5.0240669][-2.5996437 -1.5225196 -0.63660431 -0.30674314 0.49961662 1.5023432 2.1549997 2.1790142 1.8987122 1.6273079 -0.029695988 -1.1859131 -2.5293207 -3.5307589 -4.3215408][-3.0623846 -2.0916696 -0.72974968 0.11515331 0.801548 1.2053843 1.6177397 1.8746653 2.0228004 1.2734308 0.029587269 -0.30593157 -1.8820987 -2.7718625 -3.7913048][-3.5613585 -2.5103078 -1.758863 -1.1254544 -0.22607517 0.24372768 0.756958 0.68777561 0.84706688 0.22750807 -0.76176548 -0.93900394 -1.9897442 -2.6412687 -3.2013006][-4.3077374 -2.9334064 -1.8364177 -1.2483487 -0.75239897 -0.49280596 -0.209661 -0.24427605 -0.17808104 -0.9371891 -0.90541172 -0.74288082 -2.0660915 -2.8934927 -3.7702427][-4.5950184 -3.8028021 -2.972508 -2.6956596 -2.322226 -1.9128385 -1.6367993 -1.6855221 -1.7587595 -2.1791267 -2.2403321 -1.754252 -2.5741334 -3.4655142 -3.9459012][-4.2520318 -3.8284373 -3.5987511 -3.3459249 -3.2316365 -2.8325047 -2.4497557 -2.6669054 -2.8887625 -3.0166154 -3.3142452 -3.2152081 -3.5757408 -3.8669524 -4.2106876][-4.1262064 -3.3158927 -3.1633739 -3.604403 -3.9871256 -4.0779986 -4.0569105 -4.1746063 -4.2385588 -4.3191175 -4.3370018 -4.6326084 -4.8835506 -4.8724737 -5.2336178]]...]
INFO - root - 2017-12-16 01:40:51.707657: step 85810, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.633 sec/batch; 43h:22m:10s remains)
INFO - root - 2017-12-16 01:40:57.997729: step 85820, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 44h:13m:49s remains)
INFO - root - 2017-12-16 01:41:04.459694: step 85830, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:09m:21s remains)
INFO - root - 2017-12-16 01:41:10.803538: step 85840, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 43h:43m:37s remains)
INFO - root - 2017-12-16 01:41:17.254871: step 85850, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:39m:05s remains)
INFO - root - 2017-12-16 01:41:23.616042: step 85860, loss = 0.29, batch loss = 0.18 (12.9 examples/sec; 0.619 sec/batch; 42h:22m:39s remains)
INFO - root - 2017-12-16 01:41:29.993314: step 85870, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 44h:07m:00s remains)
INFO - root - 2017-12-16 01:41:36.392513: step 85880, loss = 0.30, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 44h:52m:31s remains)
INFO - root - 2017-12-16 01:41:42.831998: step 85890, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 45h:07m:23s remains)
INFO - root - 2017-12-16 01:41:49.257248: step 85900, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 43h:21m:39s remains)
2017-12-16 01:41:49.793999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5559883 -6.0782108 -6.3424177 -5.5651865 -4.3507185 -3.3951025 -2.6282115 -2.2136889 -1.5243711 -1.2583299 -2.7023034 -4.73349 -6.7727675 -7.1431236 -7.79141][-5.863462 -6.5320454 -6.6281581 -6.4388013 -5.75907 -3.7366402 -2.1253858 -1.8365893 -2.0955906 -2.41791 -3.9009826 -4.9785995 -7.1691089 -7.5794668 -8.1485424][-5.7450056 -6.5598593 -6.9316449 -6.548439 -5.6693115 -4.7319732 -2.9079795 -1.4780307 -0.92098713 -1.7943187 -4.8307123 -6.3441849 -7.4009314 -7.4255733 -8.13454][-5.3806839 -6.1872263 -6.2239418 -6.1094971 -5.3313107 -4.0119319 -2.4046993 -1.1682439 0.061399937 -0.26944542 -2.865211 -5.4415636 -7.646224 -7.5762143 -7.660265][-5.4130025 -6.2590694 -6.0726423 -5.5143933 -3.7348812 -2.4239922 -0.85809755 0.056879997 0.22415876 -0.67982006 -2.8999767 -5.3831854 -7.4348192 -8.0261259 -8.1999073][-7.01014 -6.6805062 -5.8523855 -5.4828391 -3.6170816 -1.429708 0.87424374 1.6562958 1.4249392 0.066763878 -3.3504758 -5.7388339 -7.5979967 -8.2048063 -8.5926123][-6.7908239 -6.3774166 -5.6449051 -5.1144619 -2.9490457 -0.53689766 2.5503664 4.2695389 4.191433 2.3819685 -1.676435 -5.4680429 -7.6860557 -8.3249159 -8.7564383][-5.8977914 -6.1777334 -5.5482793 -4.4746513 -2.406817 0.53652859 3.7525597 5.628459 6.3516874 4.708437 0.36303806 -3.8993666 -7.2155638 -8.3017921 -8.38309][-6.0530014 -5.7914343 -5.5817518 -4.5541019 -2.1089478 0.24032068 3.0107956 4.6904287 5.1696539 4.2046585 0.34331036 -3.3778267 -6.6762381 -8.1648026 -8.8363419][-6.2293067 -6.0251184 -5.5790544 -4.7681036 -2.9727163 -0.48522139 2.2381649 3.1022472 3.1737394 1.6223021 -1.1643019 -4.1556468 -6.9154582 -7.7580729 -8.4848328][-5.7741346 -6.4711514 -5.8797112 -4.5723772 -3.2303452 -1.5185218 0.14219904 0.85039139 0.54893589 -2.1585965 -4.8309422 -6.5140948 -7.7167516 -7.7255187 -7.6078014][-5.4941683 -5.3524289 -4.4796457 -3.3793221 -1.8846908 -0.87307119 -0.12755203 -0.014922142 -0.42531586 -2.3693419 -4.4617777 -6.3027363 -7.4589086 -7.8265796 -7.7583423][-6.6152287 -5.4572163 -3.8623979 -2.0716767 -0.65470266 -0.5367012 -0.4317317 -0.63924408 -0.58102036 -2.6127968 -4.1886616 -5.8028259 -7.0555086 -7.7850285 -8.03653][-7.4097404 -6.9320965 -5.8906064 -3.5336175 -1.3592172 -0.48935366 -0.67216492 -2.2629952 -2.7231531 -4.2343488 -4.8358936 -6.0055571 -6.5066729 -6.6611266 -6.9033766][-7.8561549 -7.1745319 -6.2544947 -5.1912308 -3.4383473 -2.196341 -1.6536098 -2.398869 -3.4218097 -4.6742134 -5.526042 -6.416647 -6.6189117 -6.57006 -6.2016592]]...]
INFO - root - 2017-12-16 01:41:56.131115: step 85910, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:38m:29s remains)
INFO - root - 2017-12-16 01:42:02.565718: step 85920, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 44h:58m:47s remains)
INFO - root - 2017-12-16 01:42:09.002492: step 85930, loss = 0.30, batch loss = 0.19 (11.8 examples/sec; 0.678 sec/batch; 46h:24m:12s remains)
INFO - root - 2017-12-16 01:42:15.370464: step 85940, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 43h:42m:22s remains)
INFO - root - 2017-12-16 01:42:21.860925: step 85950, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 43h:01m:57s remains)
INFO - root - 2017-12-16 01:42:28.198166: step 85960, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 43h:02m:36s remains)
INFO - root - 2017-12-16 01:42:34.531790: step 85970, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.636 sec/batch; 43h:34m:59s remains)
INFO - root - 2017-12-16 01:42:40.995090: step 85980, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:07m:46s remains)
INFO - root - 2017-12-16 01:42:47.321256: step 85990, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 42h:47m:57s remains)
INFO - root - 2017-12-16 01:42:53.740833: step 86000, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 43h:31m:58s remains)
2017-12-16 01:42:54.251715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6942539 -4.7831354 -5.3122959 -5.5655994 -5.3123417 -4.4391918 -3.6521072 -3.0027266 -2.3290172 -3.8076119 -5.0943441 -5.9302592 -6.8072081 -7.7258463 -8.6147451][-4.3418837 -4.3686581 -5.1609774 -5.7662778 -5.90955 -5.45308 -4.4022274 -3.3609772 -2.6104455 -3.8110909 -4.6488085 -6.02221 -7.2784934 -8.0944309 -8.905798][-3.3521552 -3.8305156 -3.6706762 -3.9606502 -3.9495263 -3.3730626 -2.8924084 -2.4713583 -1.8255 -2.8530765 -3.6293817 -4.6728892 -5.4858828 -6.6845636 -7.6807771][-1.9606805 -2.2678409 -2.3336678 -1.9204988 -1.5324001 -1.0378051 -0.069068432 0.576622 0.76338577 -1.0303144 -2.3410077 -3.2113848 -4.6425705 -5.8359575 -7.1625013][-2.3631759 -1.2527308 -0.25355959 -0.058434963 0.65084457 1.2050638 1.5710201 1.9060812 1.9256439 -0.24993658 -1.4947848 -2.610188 -3.5276093 -4.410615 -5.7242174][-2.4354968 -1.325109 -0.69149637 0.3417387 1.6089802 2.6854019 3.2415686 3.2546196 3.054966 0.49606705 -1.0862498 -2.59724 -4.0723362 -5.0191412 -5.437706][-3.5233526 -2.2986407 -0.90466118 0.40961456 1.9825363 3.6919994 4.4936514 3.9371424 3.4264717 0.81361866 -1.0704103 -2.8027968 -4.2618122 -5.4136081 -6.3971987][-3.856097 -2.8619328 -1.5827594 0.48715878 1.9866943 3.270093 4.2441931 4.1216326 3.4207687 0.50955677 -1.3755546 -3.4971371 -5.1380281 -6.2827444 -7.1190519][-4.6360631 -3.3992395 -2.1890831 -0.41915751 1.580678 2.1694374 2.6239014 3.2854643 3.3077183 0.24539709 -2.1037488 -4.0030861 -5.4786129 -6.6589632 -7.4421372][-5.3928308 -4.9499598 -4.0241942 -1.8934822 0.37076378 1.2521048 1.4136162 1.6947289 1.897665 -1.0541925 -3.0279016 -4.8799734 -6.0546889 -7.0060496 -7.7027712][-6.5770149 -5.6615114 -4.8859892 -3.7399297 -2.2040129 -1.2311101 -1.1395483 -1.1877098 -0.69838524 -3.3099971 -5.4224968 -6.2704649 -6.9815054 -7.8976641 -8.4337559][-6.8016996 -5.8294554 -4.8467617 -4.0124054 -3.3197918 -2.4350085 -2.6263905 -2.6025829 -2.7655764 -4.4317102 -5.4656663 -6.2705412 -6.7789631 -7.5572639 -8.4463978][-7.4115677 -6.1538143 -4.9991579 -4.5556569 -4.01077 -3.902396 -3.4856558 -3.8712466 -4.1381512 -5.3067017 -6.5382538 -6.5475035 -6.85765 -7.4184566 -7.7193713][-6.9961324 -5.9472475 -5.3306818 -4.9267435 -4.7380848 -4.6276693 -4.7364163 -4.9247651 -5.065197 -6.0098505 -6.1465006 -6.5174594 -6.4657073 -6.501893 -6.9577951][-7.5741472 -6.7902203 -6.2772908 -5.9637465 -6.0740242 -6.0602088 -6.6507416 -7.0029817 -7.1944861 -7.2403908 -7.3728061 -7.2271461 -6.9885783 -6.7155294 -6.492589]]...]
INFO - root - 2017-12-16 01:43:00.725576: step 86010, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 43h:22m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 01:43:07.163998: step 86020, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 43h:37m:56s remains)
INFO - root - 2017-12-16 01:43:13.601975: step 86030, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 43h:04m:40s remains)
INFO - root - 2017-12-16 01:43:19.993734: step 86040, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:35m:28s remains)
INFO - root - 2017-12-16 01:43:26.347015: step 86050, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:36m:24s remains)
INFO - root - 2017-12-16 01:43:32.703460: step 86060, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 44h:12m:34s remains)
INFO - root - 2017-12-16 01:43:39.119119: step 86070, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 44h:23m:38s remains)
INFO - root - 2017-12-16 01:43:45.535248: step 86080, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 42h:51m:13s remains)
INFO - root - 2017-12-16 01:43:51.815401: step 86090, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 43h:32m:04s remains)
INFO - root - 2017-12-16 01:43:58.186621: step 86100, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 43h:35m:00s remains)
2017-12-16 01:43:58.729719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9497747 -4.5906935 -5.0102406 -4.8935652 -4.6345625 -4.0074911 -3.2167172 -2.4394345 -1.8214307 -2.3392072 -2.8135486 -4.3077765 -5.0309916 -6.5832205 -7.9449525][-3.8457985 -4.0955229 -4.706811 -5.3770065 -5.1391897 -4.1902084 -3.3169632 -2.5946631 -2.4106054 -3.0903206 -3.7064195 -5.3575935 -5.9431515 -7.4475136 -8.0198679][-3.2672324 -3.2833467 -3.4890051 -3.9329395 -4.0535283 -3.7237754 -2.954751 -2.290689 -1.8122582 -2.5873284 -3.2149377 -4.4757104 -4.9669976 -6.4592628 -7.331][-3.1530542 -3.4010563 -3.5596704 -3.2235112 -2.7783217 -2.1876893 -1.2823868 -0.71413755 -0.57997131 -1.5789971 -2.4252343 -3.9497473 -4.4035344 -5.7180743 -6.9639807][-4.6204557 -4.6932254 -4.62288 -4.0675631 -3.0905929 -1.550643 -0.34823227 0.3357811 0.49602413 -0.81137466 -2.296617 -4.1227818 -4.6885614 -5.75062 -6.3623867][-5.782589 -5.7863131 -5.5140619 -4.5852938 -2.9507565 -0.60097647 1.1518946 1.8433371 1.6171255 0.009677887 -1.5303106 -3.8350506 -4.9263554 -6.0505309 -6.5629563][-6.1840248 -5.6134081 -5.4314003 -4.7143259 -2.8558545 0.22538424 2.6833086 3.9348221 4.0030584 2.0734396 -0.17432451 -3.305934 -5.1304655 -6.3235979 -6.94689][-6.406353 -5.6875153 -4.7332468 -3.4988198 -1.9571238 0.69122219 3.0365705 3.6734562 3.4175596 1.8078651 0.058542252 -2.8796163 -4.8778849 -6.199645 -6.6581225][-5.1231713 -4.5624533 -3.9816225 -2.8316369 -1.1169257 1.3094101 3.2816668 3.9963284 3.9915094 2.5982542 0.72187042 -2.2976375 -4.1709452 -5.7065816 -6.4423032][-4.09918 -3.4118466 -3.0284276 -2.3369946 -1.3012261 0.63972759 2.3490658 2.7253199 2.8829622 1.0890837 -0.42963028 -2.9088159 -4.6971393 -6.3437176 -6.8770986][-3.8139555 -3.3026872 -2.96451 -2.1997256 -1.5967965 -0.32885504 0.73794746 0.85810852 0.3862381 -1.2436657 -2.4827275 -4.8612843 -5.8454781 -6.5844617 -6.8279781][-2.9687157 -2.8120704 -2.3187685 -1.4817519 -0.99453306 -0.58382845 -0.0054268837 -0.545403 -1.1955438 -2.2693224 -3.1832442 -4.6699743 -6.0639315 -7.4091969 -7.7386708][-3.9941416 -3.2152219 -2.5728388 -1.8679609 -1.5146375 -1.0881763 -1.0112138 -1.5897775 -2.4459457 -3.9620433 -4.5156155 -5.3072081 -5.8493333 -7.2608795 -7.901381][-3.4457889 -2.9948425 -2.4152713 -1.5753779 -1.1777449 -0.80288696 -0.7205348 -1.4627781 -2.7504463 -4.193439 -4.4616952 -4.7804804 -4.9947948 -6.0231023 -6.2415333][-4.8754859 -4.0354176 -2.9794641 -2.5331912 -2.5559082 -2.7597451 -3.1099796 -3.485383 -3.9424577 -4.7388029 -5.1825905 -5.2634263 -5.8009586 -6.0860572 -6.0602269]]...]
INFO - root - 2017-12-16 01:44:05.112998: step 86110, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 45h:20m:26s remains)
INFO - root - 2017-12-16 01:44:11.577027: step 86120, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 43h:09m:09s remains)
INFO - root - 2017-12-16 01:44:17.996167: step 86130, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:37m:14s remains)
INFO - root - 2017-12-16 01:44:24.329219: step 86140, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 44h:28m:11s remains)
INFO - root - 2017-12-16 01:44:30.713900: step 86150, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 43h:41m:54s remains)
INFO - root - 2017-12-16 01:44:37.032556: step 86160, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 43h:24m:54s remains)
INFO - root - 2017-12-16 01:44:43.443555: step 86170, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 42h:53m:30s remains)
INFO - root - 2017-12-16 01:44:49.806578: step 86180, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.616 sec/batch; 42h:10m:46s remains)
INFO - root - 2017-12-16 01:44:56.253311: step 86190, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 44h:07m:21s remains)
INFO - root - 2017-12-16 01:45:02.591785: step 86200, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:34m:45s remains)
2017-12-16 01:45:03.148241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.157752 -3.8259532 -3.2440529 -2.7748814 -2.2308478 -1.5874233 -0.99045324 -0.82087088 -0.94914246 -2.5530372 -3.6612329 -4.6693287 -5.7509794 -6.8407512 -7.6840858][-4.1278381 -3.8057189 -3.2163682 -3.1949267 -2.9709816 -2.4364781 -2.025269 -1.5422359 -1.7469978 -3.6811032 -4.4002233 -5.2234035 -6.62208 -7.6138597 -8.0936947][-4.1522436 -3.734673 -3.2983727 -2.8599854 -2.58432 -2.1665931 -1.2222786 -1.0297985 -1.1701121 -2.574585 -3.5106516 -4.3386507 -5.4818811 -6.6604476 -7.2960982][-4.3194528 -4.1866302 -3.6585894 -2.9130721 -2.43917 -1.8778629 -1.093998 -0.64490652 -0.39957047 -2.0602732 -2.9723749 -3.7162094 -4.6228819 -5.6315022 -6.6755924][-4.4957085 -3.8130689 -2.9164443 -2.4184203 -1.9085875 -1.0571542 -0.10929918 0.073040009 0.090977669 -1.5598025 -2.4823203 -3.2311525 -4.2340517 -5.4287882 -6.1688757][-3.8119185 -3.2967019 -2.664073 -1.8374867 -1.0454884 -0.47144318 0.15830851 0.48145866 0.51881409 -1.2043347 -2.1300659 -3.0646358 -4.19181 -5.3130035 -6.1877346][-3.409636 -2.8814459 -2.0549502 -1.405323 -0.71184254 0.022279263 0.85486794 1.1210108 1.2896204 -0.32724476 -1.4261575 -2.5235252 -3.7890136 -4.9657936 -5.8083997][-2.3085022 -1.6732087 -0.92443371 -0.155087 0.79933929 1.2928219 1.7540016 1.9429464 2.0557079 0.22715616 -1.021122 -2.19342 -3.6186385 -4.7865949 -5.6361551][-1.5197558 -1.1049709 -0.36299515 0.50785732 1.1274557 1.4154701 1.7669268 1.9957123 2.1651564 0.21367598 -1.0397315 -2.2339506 -3.7720191 -4.9586182 -5.7959852][-1.6781173 -0.91003513 -0.30881882 0.29712152 0.95312023 1.0530424 1.1631069 1.1166573 1.0283155 -0.95094013 -2.0121746 -3.4055724 -4.5240192 -5.4235454 -6.046968][-2.6723104 -1.8426132 -1.4951773 -0.8185811 -0.44404268 -0.56021738 -0.58286619 -0.71175575 -0.71868849 -2.7449951 -3.9605572 -4.783205 -5.5598087 -6.2715974 -6.5636024][-3.36164 -2.636549 -2.4557343 -1.8271666 -1.1906204 -1.5300694 -1.7976017 -2.1995926 -2.460856 -3.815814 -4.6465836 -5.4371624 -6.0945249 -6.5072551 -6.6153293][-4.6219664 -4.3392992 -4.4424534 -3.8716273 -3.3762279 -3.1788192 -3.595561 -4.0434489 -4.11605 -4.9733834 -5.609746 -5.7440262 -5.9535837 -6.0194044 -6.0438242][-4.9579973 -4.9787211 -5.2776241 -4.8593216 -4.3147249 -4.0307207 -4.3435497 -4.6269412 -4.7844439 -5.5513115 -5.7275991 -5.6948152 -5.5351472 -5.558825 -5.4757233][-6.0174556 -5.5542178 -5.6722245 -5.7437305 -5.5676975 -5.4877644 -6.0488434 -6.4117446 -6.4227328 -6.0554214 -5.9046841 -5.8948507 -5.6102209 -5.3062844 -5.0146656]]...]
INFO - root - 2017-12-16 01:45:09.637759: step 86210, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:35m:41s remains)
INFO - root - 2017-12-16 01:45:15.968070: step 86220, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 42h:54m:27s remains)
INFO - root - 2017-12-16 01:45:22.391537: step 86230, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 44h:36m:28s remains)
INFO - root - 2017-12-16 01:45:28.780629: step 86240, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 43h:36m:55s remains)
INFO - root - 2017-12-16 01:45:35.125232: step 86250, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.639 sec/batch; 43h:42m:05s remains)
INFO - root - 2017-12-16 01:45:41.549409: step 86260, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 43h:57m:39s remains)
INFO - root - 2017-12-16 01:45:47.933932: step 86270, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.626 sec/batch; 42h:50m:14s remains)
INFO - root - 2017-12-16 01:45:54.367442: step 86280, loss = 0.31, batch loss = 0.20 (12.9 examples/sec; 0.620 sec/batch; 42h:23m:16s remains)
INFO - root - 2017-12-16 01:46:00.699014: step 86290, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 43h:45m:41s remains)
INFO - root - 2017-12-16 01:46:07.189711: step 86300, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 42h:55m:25s remains)
2017-12-16 01:46:07.718283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4341087 -6.7817793 -6.9966168 -6.7560964 -5.69466 -4.3392048 -2.962615 -1.4247012 -0.052313328 -0.50193214 -1.5570703 -4.3203206 -6.5064573 -8.8268185 -10.007858][-5.4547105 -6.0814757 -6.7517657 -7.01953 -6.86707 -5.4330893 -3.7150328 -1.523109 -0.20815754 -0.63654613 -2.021409 -4.0991468 -6.2737741 -8.7775183 -10.34688][-5.0467849 -5.6948757 -6.02444 -5.6511369 -5.1633596 -4.5563369 -3.557879 -1.9942355 -0.51617193 -0.97114086 -2.0831437 -4.0720854 -5.8049755 -7.6271729 -9.071104][-5.1327848 -4.9720507 -4.6691046 -4.0589762 -3.620429 -2.8410468 -1.785121 -1.0907178 -0.39784384 -0.90924597 -1.6640949 -3.8778398 -5.4959869 -7.052474 -7.8487449][-5.3410892 -4.5450134 -3.5907092 -2.6990609 -1.9849744 -0.69580555 0.50966549 0.99489212 1.2656012 0.16107845 -1.3164401 -3.7402625 -5.3293343 -6.9540849 -7.9695487][-5.0846348 -3.8536024 -2.4588232 -1.5286412 -0.29699802 1.0265694 2.5032864 3.1861944 3.475564 2.1418114 0.1276865 -2.7455516 -4.9472585 -6.6504974 -7.5027804][-4.3904305 -3.0191731 -1.6357713 -0.68239307 0.57189655 1.7379799 3.253685 4.1479015 4.7711773 3.3595781 1.1375628 -2.2571878 -4.7843189 -6.6527128 -7.6917624][-3.8734143 -2.3958955 -0.71362782 0.16293287 1.3848448 2.0860415 2.8042555 3.8548651 4.6416664 3.4748058 1.2544661 -2.0133605 -4.7496281 -6.9761295 -8.1128244][-3.0047197 -1.7540073 -0.46112776 0.70387745 2.0521441 2.3844767 2.8297777 3.268096 3.5923586 2.3826189 0.5110178 -2.1562524 -4.6656828 -7.0999737 -8.7051573][-2.3906465 -1.5812054 -0.11749601 0.70396042 1.7677336 2.3497629 2.9940681 3.0614815 3.1222925 1.1298857 -1.0564265 -3.4260097 -5.3953104 -7.0426583 -8.0963545][-1.9592376 -1.4494891 -0.53498411 0.38926315 1.4582281 1.5804996 1.9518957 2.0002155 1.983345 0.028614521 -2.3439236 -4.4971852 -5.7397933 -7.1435614 -8.0670443][-2.6567039 -1.8385558 -0.82078648 0.0919776 0.75759125 0.56617737 0.53248596 0.079843044 -0.3939476 -1.3509502 -2.74724 -4.5193939 -6.2068491 -7.5732918 -8.0827484][-3.9300623 -3.2858143 -2.3508086 -1.261003 -0.28458977 -0.31466007 -0.61609554 -1.2311611 -1.6426797 -2.5120111 -3.6162024 -4.5766115 -6.0007162 -6.9865 -7.7229004][-5.69663 -5.4312563 -4.450758 -3.322495 -2.24994 -1.5757079 -1.7004752 -1.8441787 -2.2507439 -3.1732512 -4.579319 -5.4618521 -6.1862879 -6.7191958 -6.7634482][-6.6521459 -6.3138256 -5.7479663 -5.5067892 -4.8023686 -3.7608688 -3.2527394 -3.1972442 -3.8737564 -4.5419722 -5.4864697 -6.4111261 -6.762116 -6.6488781 -6.5638366]]...]
INFO - root - 2017-12-16 01:46:14.112498: step 86310, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 43h:22m:21s remains)
INFO - root - 2017-12-16 01:46:20.530442: step 86320, loss = 0.31, batch loss = 0.20 (12.0 examples/sec; 0.664 sec/batch; 45h:24m:15s remains)
INFO - root - 2017-12-16 01:46:26.923479: step 86330, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 43h:41m:17s remains)
INFO - root - 2017-12-16 01:46:33.330025: step 86340, loss = 0.34, batch loss = 0.23 (12.0 examples/sec; 0.667 sec/batch; 45h:34m:29s remains)
INFO - root - 2017-12-16 01:46:39.751499: step 86350, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 42h:46m:15s remains)
INFO - root - 2017-12-16 01:46:46.116534: step 86360, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 42h:34m:30s remains)
INFO - root - 2017-12-16 01:46:52.564186: step 86370, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 45h:26m:06s remains)
INFO - root - 2017-12-16 01:46:58.993000: step 86380, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 44h:42m:31s remains)
INFO - root - 2017-12-16 01:47:05.373452: step 86390, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 44h:01m:13s remains)
INFO - root - 2017-12-16 01:47:11.763385: step 86400, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 44h:21m:47s remains)
2017-12-16 01:47:12.296615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21244907 -0.707613 -1.4960923 -2.3710909 -3.1945825 -3.9567611 -4.3922157 -5.0020447 -5.4212885 -6.8275552 -7.5570602 -8.350235 -8.8569031 -8.8193922 -8.9037361][-0.14777327 -1.0682979 -2.5955343 -3.6137328 -4.5668564 -5.2301655 -5.7727475 -6.3927369 -6.7138314 -7.76096 -8.4278908 -9.0939436 -9.4152212 -9.7195864 -9.8466854][-0.46704531 -0.6047616 -1.2355213 -1.9345593 -2.7961326 -3.5975413 -3.7358668 -4.2231903 -4.8433971 -6.7628531 -7.9348865 -8.7709208 -9.43222 -9.7204437 -9.6332][-1.9066525 -2.1799431 -2.0577755 -1.7634201 -1.6860886 -1.4228272 -1.0302048 -1.646246 -2.1163716 -4.0118961 -5.5557828 -7.2215075 -8.4600792 -8.78654 -9.1368074][-3.9963238 -4.2658806 -3.3345761 -2.8311505 -2.0238943 -0.8726306 -0.051579952 -0.077523232 -0.0020737648 -1.9792676 -3.7513893 -5.495923 -6.9085255 -7.4755316 -8.0542545][-5.7576122 -5.47473 -5.3007808 -4.0723753 -2.3932352 -0.83834076 0.88404179 0.88862038 0.66326332 -1.0915165 -2.6507697 -4.2225618 -5.7712913 -6.5203958 -6.9248047][-6.055377 -5.3527403 -4.5521479 -3.1250725 -1.4406509 0.742486 2.3676825 2.6816788 3.2333603 0.85453796 -1.073812 -3.0917606 -5.0259614 -6.1180863 -6.806777][-6.1110535 -4.8664379 -3.8082871 -1.8947048 0.22215414 1.7553921 2.9789839 3.70331 4.001091 1.9950075 0.34530544 -2.290556 -4.1634188 -5.6959391 -6.7701173][-5.6846523 -4.861145 -3.6307783 -1.5884008 0.13465834 1.5294132 2.7599401 2.9968939 3.2581072 1.0531788 -1.1841612 -2.9197302 -4.5378194 -6.0314131 -7.1666512][-7.1539359 -6.2643223 -5.1391659 -3.7364495 -1.9194193 -0.74345112 0.19439411 0.75284004 1.3018513 -0.68990993 -2.6814084 -4.81345 -6.1143813 -7.1574025 -7.7550988][-7.8355975 -7.3450632 -6.5776772 -5.3388262 -4.1750932 -3.0777655 -1.7335243 -1.1793532 -0.54161739 -1.5139532 -3.429491 -4.8967972 -6.04689 -7.2511063 -7.8081331][-8.5215588 -7.8566203 -7.51323 -6.7136364 -6.186996 -5.5453138 -4.748498 -4.2549257 -3.4280529 -4.2622323 -4.8166342 -5.59914 -6.3010316 -7.2413173 -7.6623535][-9.0853443 -8.6250372 -8.5036325 -7.7067676 -7.5414615 -7.0125313 -6.4538722 -6.6396275 -6.23224 -6.2562518 -6.5308943 -6.7875891 -6.9447861 -7.1626954 -7.6435313][-9.2497215 -9.6475515 -8.3716154 -7.7643785 -7.9290276 -7.4626904 -7.3133388 -7.1057978 -6.7671595 -7.2350426 -7.3352141 -7.052875 -6.6989617 -6.7592516 -6.9491587][-10.760883 -9.8570442 -8.8284187 -8.8073406 -8.1680546 -8.0373144 -7.950439 -8.4705982 -8.7490988 -8.06427 -7.52349 -7.35403 -6.7780948 -6.2341542 -5.6889944]]...]
INFO - root - 2017-12-16 01:47:18.689310: step 86410, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:41m:35s remains)
INFO - root - 2017-12-16 01:47:25.039037: step 86420, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 43h:05m:44s remains)
INFO - root - 2017-12-16 01:47:31.440403: step 86430, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 45h:18m:48s remains)
INFO - root - 2017-12-16 01:47:37.804524: step 86440, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 43h:50m:39s remains)
INFO - root - 2017-12-16 01:47:44.267420: step 86450, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 45h:14m:14s remains)
INFO - root - 2017-12-16 01:47:50.723129: step 86460, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 44h:18m:59s remains)
INFO - root - 2017-12-16 01:47:57.126537: step 86470, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 42h:51m:45s remains)
INFO - root - 2017-12-16 01:48:03.513531: step 86480, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 44h:23m:24s remains)
INFO - root - 2017-12-16 01:48:09.880027: step 86490, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 43h:05m:33s remains)
INFO - root - 2017-12-16 01:48:16.197991: step 86500, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 43h:37m:31s remains)
2017-12-16 01:48:16.715289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4104786 -5.9905434 -5.477376 -4.4988723 -3.6317449 -3.2724972 -3.2538605 -3.165885 -2.7276907 -2.8227782 -3.4286385 -4.5614681 -5.0277457 -6.1086016 -6.7876716][-6.3814912 -5.6607656 -5.2190385 -5.0753403 -4.73225 -4.254838 -3.8263402 -3.7338014 -3.8460555 -3.9258785 -4.6116209 -5.9391708 -6.3187084 -7.39458 -7.4707475][-5.6947269 -5.7085714 -5.0639839 -4.3153863 -3.6388526 -3.2402911 -3.099431 -3.0405769 -2.8503833 -2.8048124 -3.8440621 -5.4443645 -5.7644081 -6.684351 -6.7617955][-4.9367685 -4.5658808 -4.1664181 -3.4835076 -2.4522686 -1.4822745 -0.73745537 -0.82917261 -1.0387764 -1.5508103 -2.8420916 -4.7828703 -5.478097 -6.5820971 -6.4818988][-4.6746168 -3.8429279 -2.7011924 -1.8268819 -0.92249584 -0.061333656 0.37912178 0.18280888 0.009469986 -0.79511833 -2.2089047 -3.9819732 -4.9117956 -6.0859618 -5.9847455][-3.8411214 -3.1566782 -1.6639557 -0.47592449 0.69725132 1.3994312 1.7788239 1.4207449 0.93079853 0.25714493 -0.86812782 -2.6586571 -3.3640618 -4.7069483 -5.0266066][-4.3079119 -2.965611 -1.0366726 0.43942451 1.6086712 2.1998081 2.3651533 2.3551283 2.3190689 1.2375927 0.20787907 -1.6134515 -2.8036437 -4.1826715 -4.8104954][-4.2810268 -2.4892588 -0.61128187 1.1673765 2.506115 3.0310049 3.2692938 3.1551657 2.839592 2.0003996 0.83796215 -1.3689256 -2.8996582 -4.3168249 -5.3142653][-3.491941 -2.3412461 -0.84899616 0.53686428 1.6522551 2.5548801 2.8155575 2.7870951 2.6160336 2.1279869 1.4742165 -0.89108467 -2.6960626 -4.3081446 -5.2885208][-3.2821679 -2.5982866 -1.9507732 -1.0158849 -0.069879055 0.74221039 1.0904665 1.4420319 1.6515322 0.69620132 0.32217026 -1.3679676 -2.9370933 -4.78862 -5.9637032][-3.5345464 -3.2953191 -2.829495 -2.3152361 -2.0643272 -1.3283052 -0.87210274 -0.29972267 0.47019768 -0.6454277 -1.8361387 -3.2310333 -4.3298407 -5.2020535 -6.2311072][-3.9969685 -3.7611516 -3.5440674 -2.6706729 -2.0932097 -1.5751905 -1.477705 -1.5730515 -1.577158 -2.1781368 -2.7807269 -4.2019725 -5.6297779 -6.5828204 -6.4333563][-5.2142105 -4.6837835 -4.3501282 -3.7649965 -3.197505 -2.1127472 -1.7123499 -1.7699437 -2.0562539 -3.2593632 -3.8319478 -4.3470364 -5.0424871 -6.3656425 -7.1117935][-5.8633189 -4.7642536 -4.1029081 -3.078742 -2.7868586 -2.3654504 -1.9143443 -1.9406929 -2.0214806 -2.8937001 -3.8096807 -4.5085716 -5.2178016 -5.7519932 -6.1507659][-7.0604405 -5.876421 -4.8755331 -3.9129217 -3.0865917 -2.9504733 -2.6902184 -2.9310732 -3.1248751 -3.5133281 -3.9735711 -4.6528497 -5.4901791 -5.952538 -5.8742504]]...]
INFO - root - 2017-12-16 01:48:23.110245: step 86510, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.648 sec/batch; 44h:15m:00s remains)
INFO - root - 2017-12-16 01:48:29.541531: step 86520, loss = 0.25, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 45h:36m:43s remains)
INFO - root - 2017-12-16 01:48:36.033681: step 86530, loss = 0.27, batch loss = 0.16 (10.8 examples/sec; 0.743 sec/batch; 50h:45m:16s remains)
INFO - root - 2017-12-16 01:48:42.496354: step 86540, loss = 0.29, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 45h:52m:19s remains)
INFO - root - 2017-12-16 01:48:48.934296: step 86550, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 43h:45m:09s remains)
INFO - root - 2017-12-16 01:48:55.279171: step 86560, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 43h:26m:32s remains)
INFO - root - 2017-12-16 01:49:01.679431: step 86570, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 43h:45m:47s remains)
INFO - root - 2017-12-16 01:49:08.080677: step 86580, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 45h:00m:38s remains)
INFO - root - 2017-12-16 01:49:14.483786: step 86590, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 43h:09m:11s remains)
INFO - root - 2017-12-16 01:49:20.854789: step 86600, loss = 0.24, batch loss = 0.12 (12.6 examples/sec; 0.633 sec/batch; 43h:13m:42s remains)
2017-12-16 01:49:21.389823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252151 -4.7299194 -5.1248574 -5.8959169 -6.3623347 -6.7109361 -7.1241822 -6.6691771 -6.0740066 -5.1133223 -4.71805 -4.7829695 -5.3214722 -5.7405233 -5.9866719][-4.7281151 -4.5154614 -4.7762079 -5.1706257 -5.8067036 -7.19144 -7.8541865 -7.9765091 -6.8725529 -6.1941438 -5.4323277 -5.3267384 -5.7435389 -5.9751954 -6.119957][-5.6202068 -5.0423574 -4.7629552 -4.2726688 -4.5879445 -5.5172095 -6.4504223 -6.8421354 -6.481267 -6.4377275 -6.0735393 -6.0629053 -6.5003138 -6.9046006 -6.8121042][-6.3239365 -5.33014 -4.3097095 -3.0248485 -2.4206152 -2.5349464 -2.9030604 -3.4271836 -3.2777557 -3.5027819 -3.84412 -4.5355482 -6.0439548 -6.7102795 -6.9298973][-6.4282136 -4.7260189 -3.0091677 -1.1284471 -0.512753 0.053833008 0.43892479 0.18671799 0.41298103 0.094315529 -0.8415308 -2.623755 -4.929203 -6.2847047 -6.7990069][-5.1557455 -3.6672688 -1.7425857 -0.21319008 0.82592869 1.61798 1.8476219 1.9827261 2.3161879 1.7906322 0.039851665 -2.1446552 -4.4152679 -5.7914929 -6.3456306][-3.9311512 -2.5028973 -0.69140005 0.85555935 2.11839 2.9937582 3.4590712 3.471139 3.2926979 2.3654099 0.16819668 -2.0202823 -4.0883465 -5.5218124 -6.1868095][-3.3928761 -1.7567754 -0.50091314 0.95935631 2.008544 2.5946684 3.2281885 3.2337532 3.4758606 2.5131121 0.45895004 -1.8190536 -3.7826304 -5.401835 -6.1897631][-3.6436248 -2.2657557 -1.3366895 -0.12993431 0.41426373 1.1934261 1.5336323 1.6756563 1.9061222 0.68387318 -1.1820421 -3.120204 -5.2750664 -6.6010156 -6.8776994][-3.9455585 -3.4259696 -2.5013847 -1.7982702 -1.2331424 -0.80098248 -0.36282969 -0.18204498 -0.18906546 -1.4194708 -3.2380466 -4.3534107 -6.2944231 -7.1998529 -7.7936277][-6.2244978 -5.6454473 -4.9766803 -4.1652842 -3.3302503 -2.8233037 -2.382834 -2.6720729 -2.9680986 -3.6491752 -5.0457487 -5.4412947 -6.7798514 -7.1362958 -6.88303][-8.4680052 -7.8818564 -7.3467569 -6.6547852 -6.0239606 -5.3298073 -4.9388146 -5.4703417 -5.3486586 -6.061193 -6.9268112 -7.1135168 -8.3049717 -8.0845833 -6.8766451][-9.9111938 -9.8387051 -9.29383 -8.1669722 -7.2091985 -6.1190648 -5.4667997 -5.5588827 -5.8251462 -6.3716331 -7.1339741 -6.9830685 -7.3687687 -7.1892648 -6.588872][-9.7810211 -10.079027 -9.5442343 -8.5462914 -7.3929739 -6.1216354 -5.24366 -5.0481529 -5.2904696 -5.4864655 -6.0393291 -6.0687361 -6.3757782 -6.2091913 -5.73176][-10.480815 -9.9091759 -9.612978 -9.1762686 -8.15355 -6.9018474 -6.1963811 -5.9227295 -5.9601412 -5.8471746 -5.8744783 -5.9515481 -6.1304965 -6.4430227 -6.3297167]]...]
INFO - root - 2017-12-16 01:49:27.710096: step 86610, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 42h:32m:49s remains)
INFO - root - 2017-12-16 01:49:34.079692: step 86620, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.628 sec/batch; 42h:52m:10s remains)
INFO - root - 2017-12-16 01:49:40.455263: step 86630, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 43h:57m:21s remains)
INFO - root - 2017-12-16 01:49:46.754746: step 86640, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 43h:05m:34s remains)
INFO - root - 2017-12-16 01:49:53.153152: step 86650, loss = 0.35, batch loss = 0.23 (12.5 examples/sec; 0.641 sec/batch; 43h:47m:16s remains)
INFO - root - 2017-12-16 01:49:59.438784: step 86660, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:44m:34s remains)
INFO - root - 2017-12-16 01:50:05.854335: step 86670, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:31m:08s remains)
INFO - root - 2017-12-16 01:50:12.250928: step 86680, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 43h:17m:08s remains)
INFO - root - 2017-12-16 01:50:18.646662: step 86690, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 43h:36m:52s remains)
INFO - root - 2017-12-16 01:50:25.217954: step 86700, loss = 0.33, batch loss = 0.22 (12.4 examples/sec; 0.644 sec/batch; 44h:00m:08s remains)
2017-12-16 01:50:25.764667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6946807 -3.1613054 -2.6192622 -1.6568389 -0.77743912 0.37268639 1.543745 1.8042393 2.0421085 0.50334454 -0.70851421 -2.9457521 -4.2766347 -5.5220757 -6.615509][-2.9763331 -2.6870894 -2.2904434 -1.6929679 -0.64853287 0.66368484 1.7350836 1.9285994 2.1017246 0.44492245 -0.70860672 -2.6042924 -3.9613347 -5.2789068 -6.2022157][-2.9888167 -2.6145205 -2.2408457 -1.8074665 -1.2111044 -0.09353447 0.86291313 1.082777 1.3628712 0.089213848 -0.72321081 -2.5318055 -3.8736787 -5.2418265 -6.0411186][-3.8964262 -3.5958166 -3.3174334 -2.6674891 -1.9515343 -0.98870516 -0.049072266 0.44229603 1.0011959 -0.52477646 -1.5308156 -3.2653036 -4.2610407 -5.23525 -5.8363514][-4.2920089 -3.7124345 -3.3219414 -2.7103515 -1.8424015 -0.81426 -0.087704659 0.22878981 0.5903511 -0.85146618 -1.7247157 -3.3292294 -4.5616608 -5.5122204 -5.8276653][-5.0807943 -4.3710847 -3.6339564 -2.6340828 -1.4815516 -0.46816635 0.3377018 0.77624512 1.016531 -0.67543936 -1.8003669 -3.4982686 -4.8417983 -5.7744823 -6.0561504][-5.6104145 -4.722331 -4.1747408 -3.0374007 -1.5825396 -0.13602734 0.83937263 1.4521227 1.8063736 0.068014622 -1.1308117 -3.4270749 -5.1479449 -6.2874908 -6.7175961][-5.6718206 -4.414197 -3.2416353 -2.2041349 -1.1195674 0.42724228 1.6907902 2.2846594 2.6550722 0.63973045 -0.69626093 -2.6799483 -4.5103335 -6.3845754 -7.1997976][-4.9853611 -4.2711329 -3.1961083 -1.371244 0.066598892 0.97024345 1.855216 2.5806789 3.0712767 0.94974995 -0.61863279 -2.9371376 -4.5106359 -5.8909087 -6.8813658][-5.0834455 -3.9742706 -2.9612308 -1.6824536 -0.3977294 1.281642 2.5156202 2.5362797 2.6726274 0.64166546 -0.51220894 -3.1370373 -5.0886226 -6.3792343 -7.0557938][-4.7472425 -3.911516 -3.1231256 -1.7993932 -0.60957193 0.48988342 1.7322464 2.4304056 2.4533043 -0.2286787 -1.792851 -4.1377563 -5.9327583 -7.3640246 -7.8790073][-4.1778631 -3.8656533 -2.9891963 -1.7017589 -0.3412323 0.7409935 1.6596088 1.9219799 1.7419567 -0.46445847 -2.1890841 -4.5543709 -6.2935629 -7.4602056 -8.228282][-4.4046769 -3.887702 -2.8473339 -1.7140746 -0.37649536 0.761611 1.7299356 1.8489218 1.2594042 -0.66502 -2.3065791 -4.6955848 -6.4557924 -7.4199033 -7.8167844][-4.5005112 -4.567534 -3.9604161 -2.6202936 -1.3954153 -0.54891205 0.13969803 0.35379887 -0.17867374 -1.6305919 -2.7119913 -4.4140172 -5.778595 -6.8440495 -7.351552][-5.0477085 -5.2802768 -4.6980848 -4.2706079 -3.7643828 -3.1573772 -2.6936541 -2.0559812 -2.1209512 -2.9676051 -3.8123732 -4.9127436 -5.9004145 -6.7163749 -7.0809426]]...]
INFO - root - 2017-12-16 01:50:32.103124: step 86710, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 42h:37m:27s remains)
INFO - root - 2017-12-16 01:50:38.512002: step 86720, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:28m:33s remains)
INFO - root - 2017-12-16 01:50:44.865521: step 86730, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 43h:42m:43s remains)
INFO - root - 2017-12-16 01:50:51.375914: step 86740, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 45h:00m:18s remains)
INFO - root - 2017-12-16 01:50:57.825313: step 86750, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.623 sec/batch; 42h:29m:45s remains)
INFO - root - 2017-12-16 01:51:04.212446: step 86760, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 43h:01m:54s remains)
INFO - root - 2017-12-16 01:51:10.591740: step 86770, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 43h:02m:59s remains)
INFO - root - 2017-12-16 01:51:17.028833: step 86780, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.627 sec/batch; 42h:48m:23s remains)
INFO - root - 2017-12-16 01:51:23.383203: step 86790, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 43h:48m:57s remains)
INFO - root - 2017-12-16 01:51:29.776130: step 86800, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 43h:24m:17s remains)
2017-12-16 01:51:30.370622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8120122 -6.7522988 -5.9343119 -4.9422178 -4.2062969 -3.6121845 -3.0711775 -2.66123 -2.2884641 -3.9735398 -4.7989078 -5.97931 -6.9478335 -7.7484241 -8.7016172][-7.1440835 -7.057785 -7.4401097 -7.3921089 -6.9685192 -6.1827908 -5.2933207 -4.5659361 -3.7253742 -4.9129152 -5.3590565 -6.2550888 -7.0708885 -8.2357969 -9.5891171][-7.3212557 -7.3416228 -6.9136219 -6.3589916 -5.9365292 -5.9538078 -5.4171629 -4.9467468 -4.1781111 -5.2079353 -5.994133 -6.5784225 -6.7898211 -7.4792752 -8.4940929][-7.0324135 -6.2510414 -6.0559764 -5.4490538 -4.6180725 -3.8313715 -2.7889934 -2.4569783 -2.4048815 -4.185091 -5.1348505 -6.0870886 -6.9773197 -7.2917352 -7.5461831][-5.0241346 -4.4371166 -3.6722918 -2.9736671 -2.2752538 -1.5356107 -0.71621943 -0.19697237 0.20952749 -1.351758 -2.6193266 -4.2301321 -5.4664 -6.5030684 -7.1942625][-3.06571 -1.6632156 -1.1087079 -0.70776653 -0.17261553 0.50981045 1.4879618 1.460638 1.300828 -0.36212254 -0.86822987 -1.8010907 -3.0122385 -4.4016094 -5.9065409][-3.1615438 -1.5221286 -0.33054495 0.46034336 1.4027319 2.3940048 3.1853437 3.1907358 3.2465057 1.1198664 0.096938133 -1.0552959 -2.4112973 -3.9486747 -5.1870432][-2.3844013 -1.1366024 -0.1961112 0.82638454 2.0469332 2.9677572 3.7608185 3.8869972 4.0013456 2.1295309 1.3475761 -0.13319302 -1.8712196 -3.24433 -4.6122217][-1.709321 -0.76990414 -0.16790009 0.7277298 1.3595953 2.1761265 2.66047 2.8675423 3.3073854 1.7131224 1.2419119 -0.28336477 -1.8908634 -3.0367217 -4.2169409][-1.4150515 -0.757041 -0.71904516 -0.1222105 0.46030045 1.2603178 1.8021183 1.9030876 2.268775 0.76998138 0.33921432 -1.0690622 -2.4533544 -3.1992502 -4.225687][-1.402194 -0.878994 -1.0358477 -0.58651829 -0.27717972 0.44933796 0.65408516 0.80247211 0.72368431 -0.66365719 -1.1964006 -2.1430955 -3.0482826 -3.771065 -4.4414649][-1.9986844 -1.7092452 -1.7627072 -1.6239085 -1.6282363 -1.2218337 -0.74141312 -0.75011206 -0.6876092 -2.252255 -3.0974932 -3.9032741 -4.4362154 -4.8894181 -5.2333126][-3.0202684 -2.5986958 -2.8901372 -2.8392129 -3.2515001 -3.1981573 -3.1191998 -3.0280914 -2.812088 -3.5793123 -3.6938617 -3.8591053 -4.1865597 -4.6170015 -5.1385555][-4.4152193 -4.2472539 -3.6400113 -3.345016 -3.7270157 -4.17987 -4.3491969 -4.6458797 -4.7810745 -5.264575 -5.55807 -5.356575 -4.9425077 -4.8821449 -4.734333][-4.7095985 -4.8607416 -4.7278214 -4.4907842 -4.5387478 -4.4106016 -4.7358809 -5.406765 -5.74555 -5.9832554 -6.11277 -5.9924955 -6.1184297 -6.4594479 -6.4593172]]...]
INFO - root - 2017-12-16 01:51:36.654769: step 86810, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 43h:19m:22s remains)
INFO - root - 2017-12-16 01:51:43.058868: step 86820, loss = 0.31, batch loss = 0.20 (12.8 examples/sec; 0.627 sec/batch; 42h:46m:11s remains)
INFO - root - 2017-12-16 01:51:49.431838: step 86830, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 44h:26m:54s remains)
INFO - root - 2017-12-16 01:51:55.736809: step 86840, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:57m:06s remains)
INFO - root - 2017-12-16 01:52:02.199877: step 86850, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.629 sec/batch; 42h:56m:25s remains)
INFO - root - 2017-12-16 01:52:08.635160: step 86860, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 43h:39m:39s remains)
INFO - root - 2017-12-16 01:52:15.105951: step 86870, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:53m:10s remains)
INFO - root - 2017-12-16 01:52:21.466240: step 86880, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 43h:41m:31s remains)
INFO - root - 2017-12-16 01:52:27.743794: step 86890, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 42h:49m:29s remains)
INFO - root - 2017-12-16 01:52:34.066524: step 86900, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 42h:29m:44s remains)
2017-12-16 01:52:34.536198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654343 -4.1016831 -4.0762777 -4.3085327 -4.0840945 -3.7227194 -3.6041336 -3.6189013 -3.7446451 -4.3725939 -4.9657183 -6.3864489 -6.7279506 -8.2596159 -8.4628525][-3.8446116 -3.9493637 -4.3908129 -4.3728685 -4.0727768 -3.6810694 -3.4631057 -3.4167013 -3.4802098 -4.2786942 -4.9000487 -6.367404 -6.5772257 -8.0294609 -8.3446207][-2.3623838 -2.1003666 -2.4839664 -2.72859 -2.70225 -2.0285082 -1.9573913 -2.0306983 -2.059824 -2.84699 -3.6892791 -4.9652805 -5.310267 -6.87325 -7.5353093][-1.4669228 -1.4486384 -1.3745527 -1.1686835 -1.2199736 -0.95231342 -0.71393728 -0.69670105 -0.98480749 -1.888 -2.9499311 -4.3125949 -4.7920732 -6.3425908 -6.9575267][-1.1339078 -0.83363438 -1.2454109 -1.1601257 -0.85923243 -0.36528158 0.058241367 0.32607651 0.35577202 -0.56204653 -1.6220651 -3.313242 -4.0222006 -5.6992164 -6.3254089][-0.99554443 -0.77214527 -0.818655 -0.59031391 -0.39196777 0.37386894 1.1217899 1.2047958 1.1551609 0.41446686 -0.47068739 -1.9883976 -2.626092 -4.1914678 -4.9500561][-2.5797863 -2.0871706 -1.6887245 -1.1649814 -0.7098093 0.44389057 1.3542223 1.4140596 1.457427 0.75170803 -0.13494539 -1.6949296 -2.4390731 -4.0569153 -5.1134062][-3.6283646 -2.9759049 -2.5635648 -1.6644454 -0.724555 0.46027851 1.4045496 1.7039261 1.8603973 1.268548 0.29380894 -1.248517 -2.0681043 -3.7242804 -4.8850861][-4.1243534 -3.08744 -2.4397254 -1.6929502 -0.74527693 0.35398865 1.2761259 1.6070518 1.9704723 1.4827394 0.51265526 -1.0833545 -2.1709452 -4.0667391 -5.0840845][-5.0906262 -4.2012978 -3.1793137 -2.1804705 -1.3458924 -0.34260988 0.583827 0.88988209 1.3014307 0.78629017 -0.44914579 -1.745307 -2.6705661 -4.1407337 -5.0858641][-6.2369876 -5.5031443 -4.6288991 -3.6248636 -2.8038301 -2.0146847 -1.4442468 -1.2448254 -1.0772138 -1.7669659 -2.8581796 -3.4825363 -3.7018185 -4.6065884 -5.2628736][-7.6206427 -6.848321 -6.1853828 -5.4812689 -5.0475063 -4.39424 -3.7095072 -3.7459283 -3.7482278 -4.1967139 -4.8139534 -5.0220013 -5.1002612 -5.4938364 -6.0083909][-7.7032661 -7.3564987 -7.0698166 -6.6336689 -6.5243573 -6.0367422 -5.738616 -5.9306479 -5.85351 -5.976964 -6.2891212 -5.9047871 -5.8199759 -5.8627534 -5.6975865][-7.4398851 -7.31671 -7.1825213 -7.2295518 -7.3540916 -6.9654422 -6.8205152 -7.040504 -7.0446677 -6.9569578 -7.0495768 -6.6408153 -5.9811511 -5.6918426 -5.6083508][-7.7070251 -7.8374505 -7.7038922 -7.66494 -8.0379353 -7.9998131 -7.8445582 -7.9405456 -7.9109478 -7.8102641 -7.5663962 -7.0728626 -6.4833937 -6.2753096 -6.2870922]]...]
INFO - root - 2017-12-16 01:52:40.918723: step 86910, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 44h:49m:49s remains)
INFO - root - 2017-12-16 01:52:47.365341: step 86920, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.643 sec/batch; 43h:50m:28s remains)
INFO - root - 2017-12-16 01:52:53.778722: step 86930, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:19m:09s remains)
INFO - root - 2017-12-16 01:53:00.200297: step 86940, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 43h:29m:24s remains)
INFO - root - 2017-12-16 01:53:06.557903: step 86950, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 43h:44m:44s remains)
INFO - root - 2017-12-16 01:53:12.901074: step 86960, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 43h:31m:05s remains)
INFO - root - 2017-12-16 01:53:19.371043: step 86970, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 42h:59m:08s remains)
INFO - root - 2017-12-16 01:53:25.686862: step 86980, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 42h:59m:39s remains)
INFO - root - 2017-12-16 01:53:32.157162: step 86990, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 44h:03m:06s remains)
INFO - root - 2017-12-16 01:53:38.580769: step 87000, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 44h:10m:03s remains)
2017-12-16 01:53:39.124365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.969964 -3.9315302 -3.4844494 -3.2426062 -3.1442852 -2.6837025 -2.28867 -2.4502811 -2.3908634 -3.4495721 -4.0070448 -4.71402 -5.5735531 -6.6186557 -7.1500297][-4.7973013 -5.1401463 -5.2740483 -4.9117236 -4.6916714 -4.1992006 -4.1249003 -3.8003898 -3.4118652 -4.8939795 -5.6975408 -6.1316061 -6.7719746 -7.5825624 -7.6921644][-6.0960135 -5.9371276 -6.0046754 -6.0296397 -5.5032825 -5.0818758 -4.6227665 -4.77461 -5.0956168 -6.0931454 -6.4146438 -7.1708565 -8.0910854 -9.0215473 -8.9947033][-6.7330322 -6.6664047 -6.602685 -5.4681563 -4.308435 -3.4270225 -2.8305311 -3.08257 -3.3897953 -4.8695722 -5.5532427 -6.2419395 -7.2562528 -7.9711509 -7.7192149][-7.9296227 -6.6181908 -4.9963107 -4.1533794 -3.1549416 -1.5564809 -0.56183195 -0.782959 -1.1422009 -2.7469549 -3.520411 -4.6228666 -5.4650707 -6.5084896 -6.4943953][-8.3991385 -7.8832173 -6.946135 -4.0839224 -1.762763 -0.23033381 0.96567059 0.74185276 0.17543077 -1.8394327 -2.7523332 -3.7530179 -4.7970963 -5.9461575 -6.38165][-8.9009686 -7.25948 -5.2458143 -3.2059174 -0.9286375 1.3280172 2.7977867 2.4968271 2.1201372 -0.10756159 -1.3775616 -2.7135458 -3.8420758 -4.8244572 -5.1765261][-7.9391637 -6.3276916 -4.8142185 -1.405117 1.7963018 3.5602207 4.3034658 4.19711 4.1852903 1.7466145 0.44222546 -0.80265808 -1.845624 -3.0826564 -3.2711411][-7.6510129 -6.7416091 -5.6863832 -3.2488136 -1.1768842 0.78563786 2.6081161 3.5717316 3.8503189 1.3405991 0.17795897 -1.2549763 -2.0375714 -3.3254433 -3.3296432][-8.4215527 -7.5182948 -6.6192074 -4.3939514 -2.7757931 -1.5880322 -0.17975473 0.66508675 1.0546246 -0.99444723 -2.0936923 -3.0355897 -4.0104742 -4.7089949 -4.5862775][-9.0485306 -9.1328678 -8.78845 -6.9115386 -5.580308 -3.979502 -2.6304908 -2.5367122 -2.0662217 -3.1086135 -4.1038427 -5.1149788 -5.8669639 -6.0213637 -5.6607][-9.6415615 -9.4483252 -8.9737225 -8.5396433 -7.6762471 -5.9546213 -4.7229156 -4.3533907 -3.7608726 -4.9712114 -5.2044172 -5.4087772 -5.5269632 -6.3570471 -6.6702909][-9.6712093 -9.5499477 -9.6096811 -9.0535793 -8.5542488 -7.5999951 -6.4739118 -5.7314219 -5.3723841 -6.2809415 -5.7214603 -5.7250814 -5.8035693 -5.7187271 -5.6705003][-7.5541592 -7.9264369 -7.6977 -7.1577978 -7.0858374 -6.1220455 -5.729548 -5.7691841 -5.7681265 -5.9160185 -5.9026918 -5.7165174 -5.5296421 -5.6553535 -5.3561945][-7.9785304 -7.9509916 -7.765101 -7.8919492 -7.4804926 -6.9535351 -6.3608341 -6.536737 -6.7264867 -6.7127342 -6.9045978 -7.477591 -7.5749049 -7.1637726 -6.7762847]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 01:53:45.594967: step 87010, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 42h:28m:55s remains)
INFO - root - 2017-12-16 01:53:52.012051: step 87020, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 43h:47m:31s remains)
INFO - root - 2017-12-16 01:53:58.380344: step 87030, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:27m:36s remains)
INFO - root - 2017-12-16 01:54:04.736025: step 87040, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 44h:47m:53s remains)
INFO - root - 2017-12-16 01:54:11.177553: step 87050, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 42h:56m:57s remains)
INFO - root - 2017-12-16 01:54:17.542741: step 87060, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 44h:07m:14s remains)
INFO - root - 2017-12-16 01:54:23.935667: step 87070, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 44h:36m:48s remains)
INFO - root - 2017-12-16 01:54:30.404123: step 87080, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.664 sec/batch; 45h:15m:15s remains)
INFO - root - 2017-12-16 01:54:36.881802: step 87090, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 43h:22m:55s remains)
INFO - root - 2017-12-16 01:54:43.282264: step 87100, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 44h:05m:35s remains)
2017-12-16 01:54:43.798997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6390367 -3.9140718 -4.6675253 -5.0415525 -5.4025655 -5.708982 -5.7680235 -5.4324775 -4.7310877 -5.5750179 -6.1178188 -6.7209029 -7.601779 -8.5600452 -9.144537][-2.7094331 -3.6095514 -4.9752789 -6.1003928 -6.6085644 -6.4658031 -6.2668066 -5.6846008 -5.0149775 -6.0850291 -6.5598593 -7.3251605 -8.3394241 -9.4689093 -10.410142][-2.042829 -2.2221503 -3.0227079 -4.0770583 -4.6410465 -4.6525307 -4.4095383 -4.0276794 -3.8000951 -4.9308715 -5.7188272 -6.5303068 -7.4125757 -8.33637 -8.9187164][-2.4938941 -2.8100662 -2.9784889 -3.0044246 -3.0636249 -2.7411962 -2.0556378 -1.8900342 -1.5716319 -3.1096053 -4.19394 -5.3095145 -6.7050867 -7.6465163 -8.3361607][-2.4815164 -2.4673591 -2.2261548 -1.8498154 -1.2824812 -0.37399054 0.23160648 0.35451508 0.25849056 -1.5592618 -2.7451434 -3.8894346 -5.3239021 -6.5906992 -7.4897985][-3.6515203 -3.1411004 -2.3969636 -1.1804767 0.28405809 1.7519369 3.1981287 3.4717264 3.2371845 0.89111328 -0.91196966 -2.4726248 -4.1872339 -5.239377 -5.8963795][-4.5777678 -4.0254831 -2.9949145 -1.0475225 1.0698719 2.8031349 4.0720797 4.2258997 4.0664177 1.3613214 -1.0911026 -3.1940851 -5.0503197 -6.0293455 -6.6958508][-5.1308217 -4.1133232 -2.7120333 -0.26436138 2.3613348 3.9990854 4.6932869 4.3729792 3.8722706 0.99398804 -1.3993192 -3.6760116 -5.4010792 -6.5891676 -7.3497429][-4.9428415 -4.2937136 -3.0624065 -0.90914917 1.4739428 3.2373238 3.8618841 3.8973551 3.6096516 0.48390484 -2.1936121 -4.3889761 -6.1899824 -7.2092834 -7.7986164][-5.96576 -4.9016452 -3.6958387 -1.7936368 0.37928391 1.6650925 2.0618296 2.0922613 1.7216473 -1.2406011 -3.6213589 -5.7749395 -7.0753202 -7.8283038 -8.4891291][-6.9404459 -5.8881316 -4.7717118 -2.8850842 -1.4157891 -0.32771206 0.017807007 -0.43601704 -0.70071554 -3.1272316 -5.2647223 -6.9872985 -7.9394746 -8.4933348 -8.967576][-6.7690744 -5.6905189 -4.6738043 -3.5923877 -2.8976932 -2.2153492 -2.4853926 -2.6392498 -2.675086 -4.6023474 -5.6440697 -6.8144445 -7.5260525 -8.2019444 -8.646677][-6.7501268 -5.7043257 -4.7951527 -4.1413116 -3.8507409 -3.9978633 -3.9506109 -4.4458914 -4.9263887 -5.9145656 -6.7748365 -7.2154603 -7.7310362 -7.9138589 -7.972661][-6.331759 -5.1219516 -4.4860058 -4.1929464 -4.4314089 -4.6402774 -5.1109886 -5.7208319 -5.8941174 -6.7325826 -6.6865921 -6.5821118 -6.7661409 -6.8092246 -7.034399][-6.8851666 -5.8399429 -5.1476288 -4.6955934 -4.9433441 -5.4794078 -6.2065959 -6.9500775 -7.5436831 -7.4122863 -7.0273414 -6.7714458 -6.3520122 -6.2566719 -6.2594309]]...]
INFO - root - 2017-12-16 01:54:50.200777: step 87110, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:54m:32s remains)
INFO - root - 2017-12-16 01:54:56.554629: step 87120, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 43h:58m:13s remains)
INFO - root - 2017-12-16 01:55:02.938745: step 87130, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 43h:28m:11s remains)
INFO - root - 2017-12-16 01:55:09.306989: step 87140, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 44h:02m:18s remains)
INFO - root - 2017-12-16 01:55:15.707719: step 87150, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 43h:08m:37s remains)
INFO - root - 2017-12-16 01:55:22.193166: step 87160, loss = 0.27, batch loss = 0.15 (11.8 examples/sec; 0.680 sec/batch; 46h:18m:55s remains)
INFO - root - 2017-12-16 01:55:28.647562: step 87170, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 44h:02m:46s remains)
INFO - root - 2017-12-16 01:55:34.987571: step 87180, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.618 sec/batch; 42h:06m:49s remains)
INFO - root - 2017-12-16 01:55:41.430886: step 87190, loss = 0.33, batch loss = 0.21 (12.2 examples/sec; 0.656 sec/batch; 44h:41m:42s remains)
INFO - root - 2017-12-16 01:55:47.935684: step 87200, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 42h:39m:27s remains)
2017-12-16 01:55:48.503220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4548416 -2.9989862 -2.6359286 -2.6915507 -3.1271486 -3.1586962 -3.0931854 -3.4129725 -3.7694669 -5.2656689 -6.0999079 -7.3893442 -8.1749611 -8.6598616 -8.8179512][-3.0365782 -3.0327268 -3.0373917 -2.6625242 -2.6287236 -2.9539967 -3.3692088 -3.7302041 -3.6104264 -5.1175885 -6.2201471 -6.8457427 -7.2100215 -7.8478875 -8.6295652][-4.6286912 -3.9140508 -3.7091539 -3.7809217 -3.6515112 -3.3839617 -3.1556063 -3.462594 -3.7377067 -5.4091415 -6.5134993 -7.4312849 -8.1665535 -8.3880568 -8.12915][-6.508183 -5.6978378 -5.1489363 -3.8350303 -2.9251766 -2.895875 -2.9098358 -3.1461349 -3.104907 -4.5733304 -5.6071062 -6.3675256 -7.04465 -7.5871415 -7.7422066][-6.2499647 -4.9362092 -4.0596237 -3.2322512 -2.9404597 -2.0951338 -1.4362431 -1.5496974 -1.2488523 -2.494287 -3.2382188 -4.1563911 -5.03366 -5.9867673 -6.4360337][-4.229547 -3.9557741 -3.8933268 -2.733089 -1.6245093 -0.88489914 -0.46331644 -0.50805855 -0.40233755 -1.8256745 -2.4255519 -2.8346572 -3.2364602 -4.1804814 -4.936893][-3.1932521 -2.2837305 -1.6852589 -1.3017817 -1.027482 -0.11878967 0.51851463 0.56733131 0.78358555 -0.63888168 -1.2559052 -1.9374785 -2.5099707 -3.133954 -4.1133623][-2.0684967 -1.5795708 -0.85062313 0.18668985 0.77048588 1.4331846 1.4858532 1.6395407 2.0621128 0.22314644 -0.97874594 -1.5510678 -2.3017912 -3.365447 -3.9721847][-2.1829257 -0.95536709 0.30915594 0.84481525 0.868227 1.3620367 1.6501598 1.5206614 1.402606 0.18197155 -0.5975461 -1.6859627 -3.0645375 -3.8647745 -4.5122066][-2.3047481 -1.6880503 -1.1943893 -0.47101641 0.54288864 1.1321554 0.9050703 0.39665794 0.28405857 -1.1337333 -2.4396973 -3.2425203 -3.8229151 -4.7672396 -5.2132454][-4.6195211 -3.9248257 -3.2836375 -2.6693559 -2.5981846 -2.1070747 -1.3959999 -1.2533865 -1.6785569 -3.1434207 -4.4260173 -5.2023454 -5.8901482 -6.4646211 -6.5156684][-7.6034808 -6.4876637 -5.7334509 -4.6423311 -4.0487194 -3.9835856 -3.6441441 -3.7675176 -3.9735758 -4.7404881 -6.003931 -6.3001533 -6.225142 -6.6706519 -6.6968646][-8.01023 -7.8152876 -7.420526 -6.0045671 -5.0291224 -4.8406343 -4.72394 -4.663866 -4.7535992 -5.2978 -5.9490418 -6.1592937 -6.4728136 -6.424077 -5.8288589][-6.189774 -6.5256205 -6.7290707 -6.445261 -5.8041167 -5.1730914 -4.5781622 -4.6795197 -4.984046 -5.3285465 -5.7401705 -5.8108788 -5.7924018 -5.7746015 -5.5734739][-7.9453425 -6.4350328 -5.3838396 -5.9541359 -6.2462516 -6.352427 -6.2054272 -6.0767407 -5.815237 -6.19893 -6.4527292 -6.16399 -5.9179535 -5.546237 -5.4483776]]...]
INFO - root - 2017-12-16 01:55:54.891355: step 87210, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 44h:45m:19s remains)
INFO - root - 2017-12-16 01:56:01.316478: step 87220, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 43h:28m:20s remains)
INFO - root - 2017-12-16 01:56:07.741802: step 87230, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 43h:04m:40s remains)
INFO - root - 2017-12-16 01:56:14.085156: step 87240, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 43h:35m:08s remains)
INFO - root - 2017-12-16 01:56:20.495746: step 87250, loss = 0.35, batch loss = 0.23 (12.6 examples/sec; 0.635 sec/batch; 43h:14m:21s remains)
INFO - root - 2017-12-16 01:56:26.917598: step 87260, loss = 0.27, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 45h:46m:26s remains)
INFO - root - 2017-12-16 01:56:33.282713: step 87270, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 44h:25m:51s remains)
INFO - root - 2017-12-16 01:56:39.664213: step 87280, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 42h:46m:43s remains)
INFO - root - 2017-12-16 01:56:45.997927: step 87290, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 43h:41m:53s remains)
INFO - root - 2017-12-16 01:56:52.356579: step 87300, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 43h:08m:59s remains)
2017-12-16 01:56:52.887843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5061836 -3.3258271 -3.2924833 -2.9953594 -2.7177205 -2.2402253 -1.5180373 -0.91520977 -0.31782198 -1.8215427 -3.2576632 -4.9103818 -6.0057745 -7.100656 -8.0618792][-3.3214855 -3.5161633 -3.8232329 -3.9015558 -3.6876922 -3.078413 -2.4200282 -1.9319019 -1.1310625 -2.486413 -3.7439673 -5.3430462 -6.7590461 -7.6271052 -8.2949915][-2.4763045 -2.549284 -2.7419109 -2.6992888 -2.5001621 -2.1085215 -1.5380878 -1.157279 -0.72856092 -2.3057146 -3.9620876 -5.49831 -6.6563807 -7.9067125 -8.6786442][-2.1866007 -2.1232991 -2.1722879 -1.955193 -1.6969237 -1.1809216 -0.720067 -0.66567087 -0.47875595 -2.3336778 -3.8931661 -5.6683769 -7.0164056 -7.7829123 -8.3044748][-2.51187 -2.1469326 -1.6499882 -1.1956954 -0.55034637 0.21721649 0.72627544 0.61873531 0.37733555 -1.7513981 -3.5697193 -5.4394922 -6.7846136 -7.7418103 -8.2511969][-2.8305583 -2.2664132 -1.7018576 -0.83659077 0.19551754 1.3599586 1.9835911 1.8199663 1.4213591 -1.2145238 -3.375576 -5.1590934 -6.5402174 -7.5274363 -8.133338][-3.1847429 -2.5512285 -1.5611744 -0.22446012 1.1932688 2.55927 3.1384544 3.1318378 2.6775579 -0.23712492 -2.8654437 -5.0980406 -6.4415455 -7.4342833 -8.1638308][-3.2617784 -2.4885359 -1.5363874 -0.033389091 1.6893482 3.0483704 3.6452751 3.808466 3.3998184 0.56482887 -2.033998 -4.5786123 -6.2647181 -7.3303671 -7.7882595][-3.5808234 -2.8232713 -1.9597898 -0.56829786 0.94428253 2.057168 2.469388 2.6284828 2.5379286 -0.0055088997 -2.3879375 -4.4920282 -6.1242118 -7.325913 -7.8748779][-4.0074186 -3.3201442 -2.4814706 -1.2394414 0.049717903 1.005682 1.2399435 1.4293804 1.392024 -1.202333 -3.124413 -4.8423176 -6.132699 -7.2586603 -8.0149412][-5.3921328 -4.8531275 -3.9099927 -2.8252139 -1.7274413 -0.7804637 -0.57849884 -0.73021841 -0.94525766 -3.1178517 -4.8778791 -5.9987206 -6.8414083 -7.6092649 -7.91029][-6.3003273 -5.6635294 -4.80571 -4.0069485 -3.1867628 -2.5122657 -2.5576482 -2.8069706 -3.0111861 -4.69691 -5.8121748 -6.5258379 -7.0423174 -7.5642538 -7.6638284][-6.6203284 -6.061657 -5.2891269 -4.531189 -3.812521 -3.743144 -3.8008122 -3.9919631 -4.444993 -5.5502081 -6.6255226 -6.8661447 -7.0692725 -7.1656966 -6.8833714][-6.8155823 -6.31913 -5.6787729 -5.0096464 -4.491148 -4.3905277 -4.6526594 -4.9878054 -5.1549282 -5.9109836 -6.2910929 -6.49683 -6.4975266 -6.597084 -6.4137521][-7.5312381 -7.0296078 -6.6156645 -6.2608023 -5.96554 -6.163867 -6.5017223 -6.928771 -7.2653561 -7.1064477 -6.7501888 -6.4117427 -6.2584963 -6.1936741 -5.9471717]]...]
INFO - root - 2017-12-16 01:56:59.320183: step 87310, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 43h:02m:49s remains)
INFO - root - 2017-12-16 01:57:05.695833: step 87320, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 43h:49m:39s remains)
INFO - root - 2017-12-16 01:57:12.155287: step 87330, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 43h:26m:11s remains)
INFO - root - 2017-12-16 01:57:18.512844: step 87340, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 42h:25m:52s remains)
INFO - root - 2017-12-16 01:57:24.849203: step 87350, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 44h:21m:54s remains)
INFO - root - 2017-12-16 01:57:31.291328: step 87360, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:13m:53s remains)
INFO - root - 2017-12-16 01:57:37.726739: step 87370, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 43h:49m:23s remains)
INFO - root - 2017-12-16 01:57:44.072099: step 87380, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 43h:59m:22s remains)
INFO - root - 2017-12-16 01:57:50.461218: step 87390, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 44h:20m:07s remains)
INFO - root - 2017-12-16 01:57:56.871937: step 87400, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:18m:00s remains)
2017-12-16 01:57:57.394666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.120882 -6.34906 -6.4524655 -6.3333778 -5.3971424 -4.4830127 -2.9616346 -1.4504809 -0.021859169 -0.42989826 -1.4395428 -4.3387308 -6.3351545 -8.4554825 -9.3466358][-4.9909897 -5.594317 -6.3029885 -6.5650768 -6.494132 -5.2302966 -3.6794086 -1.4969106 -0.071256638 -0.095034122 -1.5997138 -4.1037712 -6.0309372 -8.2098961 -9.7306013][-4.9313331 -5.2115626 -5.5160217 -5.2711229 -4.6156869 -3.9164042 -2.8414102 -1.5335574 -0.1785202 -0.566236 -1.4592757 -3.8414686 -5.5430603 -7.1173782 -8.39221][-4.6977997 -4.3385382 -4.1080704 -3.7929144 -3.3797755 -2.5605116 -1.4298062 -0.60065985 0.049219131 -0.35878468 -1.0532937 -3.5587969 -4.9921761 -6.5078535 -7.4244404][-4.692883 -4.08041 -3.2613382 -2.4709096 -1.8757749 -0.62095594 0.45767593 1.1528473 1.4626417 0.512331 -0.8528142 -3.5064464 -5.1290674 -6.5737295 -7.2235875][-4.462678 -3.6936431 -2.457653 -1.4482021 -0.28453732 0.9236784 2.5548668 3.1012592 3.250411 2.0336142 0.21811342 -2.9420176 -4.9208059 -6.3806581 -7.1550679][-4.1689553 -2.9647312 -1.7391505 -0.71546268 0.4178257 1.5183926 2.9152689 3.7812042 4.3813486 2.930439 0.96870232 -2.5158648 -4.5765343 -6.2642107 -6.9961753][-3.7988346 -2.2849603 -0.81591511 0.10614586 1.1467819 1.84132 2.6219015 3.5844116 4.1750212 3.1419067 1.1629839 -2.1762872 -4.53821 -6.3585854 -7.1518259][-3.0657854 -1.8109894 -0.41299486 0.65261936 1.7498217 2.1040859 2.6244526 3.0681419 3.3550291 2.1574049 0.5271492 -2.2820711 -4.4730344 -6.5286655 -7.6468015][-2.2134852 -1.4500184 -0.15789938 0.62782955 1.5161467 1.9674749 2.641861 2.818141 2.9263868 1.0967178 -0.84592676 -3.4642043 -5.064867 -6.5015078 -7.382803][-2.0253143 -1.3721437 -0.67503405 0.16706753 1.0762863 1.2839479 1.7162476 1.8040504 1.8614883 0.032435894 -2.1655116 -4.4209251 -5.389637 -6.7638125 -7.5449929][-2.7722969 -1.9496603 -1.109704 -0.30851269 0.23951483 0.24030113 0.36962223 0.049089909 -0.46086264 -1.3482261 -2.5463943 -4.3845592 -5.8095746 -6.9842491 -7.8845186][-3.832731 -3.1542749 -2.281251 -1.3394361 -0.54403687 -0.52207613 -0.95216179 -1.2932787 -1.677176 -2.6123476 -3.576714 -4.8864493 -6.0166159 -6.7976847 -7.5765886][-5.9518094 -5.5219975 -4.4178238 -3.4710889 -2.19308 -1.5849385 -1.4842768 -1.6544909 -2.338213 -3.2903318 -4.4301414 -5.3674545 -6.2977858 -6.7792106 -6.5863008][-7.4379935 -6.9204454 -6.3270226 -5.7207575 -4.7204027 -3.8631592 -3.197526 -3.247654 -3.9196746 -4.6129179 -5.7431049 -6.5756598 -6.6254597 -6.625031 -6.7378144]]...]
INFO - root - 2017-12-16 01:58:03.881864: step 87410, loss = 0.26, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 45h:46m:29s remains)
INFO - root - 2017-12-16 01:58:10.368565: step 87420, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 44h:21m:54s remains)
INFO - root - 2017-12-16 01:58:16.696436: step 87430, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.626 sec/batch; 42h:34m:55s remains)
INFO - root - 2017-12-16 01:58:23.086721: step 87440, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 43h:20m:56s remains)
INFO - root - 2017-12-16 01:58:29.524122: step 87450, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 44h:15m:37s remains)
INFO - root - 2017-12-16 01:58:35.827444: step 87460, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 42h:35m:17s remains)
INFO - root - 2017-12-16 01:58:42.166006: step 87470, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 43h:37m:44s remains)
INFO - root - 2017-12-16 01:58:48.639409: step 87480, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 44h:54m:38s remains)
INFO - root - 2017-12-16 01:58:55.133779: step 87490, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 44h:13m:32s remains)
INFO - root - 2017-12-16 01:59:01.505966: step 87500, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 43h:14m:32s remains)
2017-12-16 01:59:02.056611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3553934 -3.8367031 -4.2042627 -4.3466554 -4.6030912 -4.5339212 -3.7653277 -2.4700141 -1.6772265 -3.0036712 -4.45533 -4.6208363 -5.5134773 -7.2656145 -7.4601541][-5.0346446 -5.2783051 -5.25381 -5.1239929 -4.936389 -4.5612526 -4.6657782 -4.0957756 -2.6909823 -3.0719948 -4.2185097 -4.9021029 -6.2479925 -7.2119188 -7.415453][-5.0129738 -4.8170638 -5.0428572 -4.9003811 -4.4929361 -3.6762681 -2.547956 -2.1910625 -1.8422165 -2.5539584 -3.4000764 -4.3507895 -6.2597075 -7.2885389 -7.0422573][-5.2463374 -4.6136956 -4.3133593 -3.698415 -3.3865519 -2.7232237 -1.5106668 -0.74038553 -0.51402807 -1.9704833 -3.1650057 -3.1516709 -4.502387 -6.3734694 -6.4671345][-5.6903963 -4.7320147 -3.3310142 -2.7896738 -2.1213536 -1.2985868 -0.5111084 -0.074079037 -0.40262794 -1.9061356 -3.5149412 -3.7947052 -4.9697294 -6.0152268 -6.0039616][-3.6979697 -3.342968 -2.771956 -1.7926369 -1.0141835 -0.27214718 0.57144642 1.1559467 0.74921227 -1.5044708 -3.543129 -4.4898891 -6.5080156 -7.5700088 -6.561347][-2.4079204 -1.7121096 -1.5214572 -0.7323904 0.097176075 0.81048679 1.5445137 1.7490578 2.019228 -0.46882629 -2.7861733 -4.0420747 -6.17127 -8.1198187 -7.8764896][-1.8313417 -1.2892184 -1.0139604 -0.15114832 0.5929327 1.2259464 1.4320536 2.0820284 2.2402124 0.304595 -2.1554961 -4.0573196 -5.8874364 -7.478157 -7.390564][-2.2370076 -1.9722834 -1.5197825 -0.81923819 0.38689232 1.0344429 1.5207396 1.7717752 1.7705536 -0.13592291 -2.2411318 -3.4285388 -5.5899487 -7.4938941 -7.1487255][-3.2183514 -2.5839972 -1.428381 -1.331799 -0.982358 0.079066277 1.0579977 1.4338169 1.6012163 -0.39949417 -2.543036 -3.8926814 -5.3763771 -6.7840662 -6.9729767][-4.1689386 -4.8711681 -4.4993143 -3.4148383 -3.151711 -2.3883696 -1.5742054 -0.91278791 -0.93819094 -2.1358771 -3.7976036 -4.7272563 -5.9340391 -7.0338974 -6.8064084][-6.195797 -6.1445875 -5.9662404 -5.6656356 -4.6785307 -4.3735628 -4.1925535 -3.3680682 -2.8205895 -4.0051928 -5.0860538 -5.0249386 -5.6449327 -6.6499681 -6.6339908][-7.2517948 -7.6277847 -7.7411518 -7.3618917 -6.5986409 -5.8826742 -4.8821664 -4.7065582 -4.8214626 -5.1003437 -5.9517236 -5.5560741 -5.2690268 -5.6185389 -6.1028223][-7.2255464 -7.3548675 -7.6323733 -7.1138468 -6.8447366 -6.4863348 -6.407095 -5.9499416 -5.5477219 -5.1289463 -5.4510012 -5.5768671 -5.9078856 -5.5480866 -5.2507052][-7.8372355 -7.5941353 -7.4284973 -7.2105408 -6.860002 -6.7316537 -6.7645683 -6.6088228 -6.926053 -6.8886886 -6.399766 -5.9194374 -5.77451 -6.0822864 -5.9149895]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-87500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-87500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 01:59:09.753881: step 87510, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 42h:22m:04s remains)
INFO - root - 2017-12-16 01:59:16.177209: step 87520, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 43h:52m:06s remains)
INFO - root - 2017-12-16 01:59:22.507545: step 87530, loss = 0.30, batch loss = 0.18 (13.1 examples/sec; 0.609 sec/batch; 41h:28m:14s remains)
INFO - root - 2017-12-16 01:59:28.900874: step 87540, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:27m:57s remains)
INFO - root - 2017-12-16 01:59:35.381966: step 87550, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 43h:14m:24s remains)
INFO - root - 2017-12-16 01:59:41.791431: step 87560, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 43h:55m:40s remains)
INFO - root - 2017-12-16 01:59:48.170944: step 87570, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 44h:38m:49s remains)
INFO - root - 2017-12-16 01:59:54.524457: step 87580, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 43h:30m:01s remains)
INFO - root - 2017-12-16 02:00:00.942417: step 87590, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 43h:21m:36s remains)
INFO - root - 2017-12-16 02:00:07.313429: step 87600, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.627 sec/batch; 42h:41m:13s remains)
2017-12-16 02:00:07.880000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0494871 -5.9742002 -4.1335125 -3.6953945 -3.8120706 -2.9976416 -2.5889425 -2.6846981 -2.9271045 -5.232461 -5.7916088 -6.8908858 -6.8734703 -8.1796837 -8.8065][-6.4311967 -6.0169353 -6.2715073 -5.8750105 -4.8053951 -3.631084 -2.8607073 -2.8278623 -2.8788567 -4.7981515 -5.4054108 -7.5321774 -8.4353409 -9.4093018 -10.085476][-5.6486464 -4.6736045 -4.3634214 -4.0352535 -4.2466516 -3.1260176 -2.1572394 -2.2338834 -1.6017303 -3.8129852 -4.5945439 -5.6331244 -6.092638 -8.0632744 -9.2795944][-5.8804951 -3.968785 -3.2326493 -2.1402011 -1.7145691 -0.62053823 0.17622089 -0.036502838 -0.13713551 -2.4878888 -3.1149569 -4.6552162 -5.2162952 -5.7280717 -6.6365089][-5.4086952 -3.2918978 -2.162992 -0.79023218 -0.35957289 0.9348917 1.8041534 1.8139172 1.9292879 -0.92120314 -2.3883767 -4.4363785 -4.935 -5.979537 -6.7586622][-4.0713387 -2.6646318 -1.8843927 -0.50643873 0.61252117 1.9603729 2.6417332 2.834115 3.1171093 0.43344975 -0.82055473 -2.9702706 -3.8354921 -5.0635796 -5.7465868][-3.8651028 -2.5431328 -1.1758871 0.25890493 1.218399 2.6847363 3.7506409 3.6766415 3.648674 0.86416531 -0.36470222 -2.6253014 -3.59869 -4.7638183 -5.2522][-3.2891884 -1.8326845 -1.0109062 0.31164932 1.9899969 3.1317606 3.9406939 4.070652 3.739296 1.0063019 -0.342618 -2.5903573 -3.6598063 -5.1667109 -5.8570547][-2.7107468 -1.6448174 -1.0463858 -0.22660637 0.96707344 2.1822977 2.7923899 3.0351629 2.9181404 0.40749836 -0.87180853 -2.7699013 -3.6010118 -5.3529544 -6.248251][-2.7785597 -2.3609891 -1.8572736 -1.2041388 -0.6844306 -0.06622839 0.5392971 1.076622 0.79412174 -1.5892944 -2.8553438 -4.1398897 -4.6697111 -5.8402405 -6.3911839][-2.9776258 -2.2019992 -1.7739816 -1.1815319 -0.96636391 -0.93548441 -0.7191906 -0.70808458 -1.1078758 -3.2766619 -4.65293 -5.2716231 -5.7869253 -6.7049866 -7.3652291][-3.71854 -2.9689288 -2.0997109 -1.8561764 -1.7054482 -1.6052256 -1.6988788 -1.8930378 -2.4201279 -4.7088051 -6.0299783 -6.775526 -7.1887341 -7.122046 -7.3106561][-4.076879 -3.8751175 -3.1344376 -2.7108259 -2.5927052 -2.4297309 -2.152885 -2.1486325 -2.701581 -4.8629746 -6.0914454 -6.9738946 -7.4252758 -7.5094166 -7.9865727][-3.7624595 -3.2086105 -3.0806031 -3.4611092 -3.7682867 -4.0976858 -4.1499372 -4.1528969 -4.1326361 -4.8419447 -5.2338414 -6.1986423 -7.7348137 -7.7725291 -6.9170156][-4.7187786 -3.8465927 -3.1121545 -3.568718 -4.0642271 -4.3545456 -4.4055004 -5.2497897 -5.8206539 -5.4044371 -5.1946888 -5.5767164 -6.020659 -6.3922772 -7.1323681]]...]
INFO - root - 2017-12-16 02:00:14.249851: step 87610, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 42h:59m:58s remains)
INFO - root - 2017-12-16 02:00:20.697324: step 87620, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 45h:07m:01s remains)
INFO - root - 2017-12-16 02:00:27.049213: step 87630, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.615 sec/batch; 41h:48m:53s remains)
INFO - root - 2017-12-16 02:00:33.484009: step 87640, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 44h:15m:18s remains)
INFO - root - 2017-12-16 02:00:39.934013: step 87650, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 43h:19m:06s remains)
INFO - root - 2017-12-16 02:00:46.338130: step 87660, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 43h:31m:05s remains)
INFO - root - 2017-12-16 02:00:52.725936: step 87670, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 42h:58m:37s remains)
INFO - root - 2017-12-16 02:00:59.133636: step 87680, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.640 sec/batch; 43h:32m:51s remains)
INFO - root - 2017-12-16 02:01:05.474385: step 87690, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 42h:48m:18s remains)
INFO - root - 2017-12-16 02:01:11.802964: step 87700, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 43h:59m:19s remains)
2017-12-16 02:01:12.307772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0609269 -2.3855433 -2.3633027 -2.2670722 -2.2927933 -2.5386229 -2.5200691 -2.3913407 -2.4739437 -3.6376438 -4.7958932 -5.8439541 -7.2937508 -8.1772432 -8.73387][-2.6192522 -2.42597 -2.8727107 -2.9612613 -3.1198955 -3.3223524 -3.5789924 -3.6955884 -3.6180725 -4.2029848 -5.1049433 -5.8858333 -7.07162 -7.6018128 -8.0918322][-2.6034083 -2.0213156 -2.1466632 -2.3069229 -2.1028953 -1.9139409 -2.4291935 -2.7770181 -2.7382836 -3.2989836 -4.111886 -4.7053947 -5.9002614 -6.632803 -7.1973181][-1.3604565 -0.46043253 -0.31411409 -0.86439323 -1.0339684 -0.81622219 -0.81183577 -1.3054485 -1.8214841 -2.7016277 -3.6446295 -4.4295073 -5.4608159 -5.938077 -6.37595][-0.74026632 0.031166077 0.44614983 0.30306339 -0.0059146881 -0.0456028 0.0071210861 -0.52722788 -0.94945288 -1.6034417 -2.2405877 -2.7311034 -3.5736604 -4.2286148 -4.9708757][-0.7992487 -0.39884138 0.024256229 0.35816383 0.53689575 1.1040764 1.6748571 1.42587 1.2229071 0.50117683 -0.21667051 -1.0383272 -2.4999228 -3.7149048 -4.6991529][-1.3095875 -0.67682886 -0.20143366 0.31477165 0.70189571 1.2014713 1.7271128 1.7949476 1.8891554 0.82781315 -0.20125294 -0.99658775 -2.4977431 -3.7007627 -4.5465927][-1.7509356 -1.1160851 -0.80680466 -0.12512589 0.49201298 0.94164658 1.6536255 2.0688667 2.3962593 1.6067953 0.70129585 -0.52741146 -2.1103964 -3.290453 -4.2601829][-3.0288696 -2.0380125 -1.3876901 -0.86100817 -0.4633832 0.16913652 0.89307117 1.4879341 2.134511 1.3492765 0.33056927 -0.67464304 -2.367 -3.8181479 -4.7061524][-4.2601728 -3.4910517 -2.7215557 -2.076437 -1.5325146 -0.84340477 -0.32796764 0.13439417 0.78677273 0.21242762 -0.71275282 -1.8708229 -3.7714789 -5.0891738 -5.8316488][-5.4355507 -5.2328625 -4.5848179 -3.8126998 -3.2550263 -2.7629404 -2.2515831 -1.9500113 -1.822669 -2.4955177 -3.4774432 -4.2733707 -5.4355617 -6.4660206 -6.992137][-7.5201759 -7.0242205 -6.6662588 -6.0226774 -5.4853182 -5.1730204 -4.8046551 -4.4645128 -4.1460018 -4.4508691 -5.2217951 -5.6711407 -6.4728937 -7.1389437 -7.5540848][-7.6707559 -7.3652487 -7.3592529 -7.0470252 -6.3388953 -5.9826908 -5.8360682 -5.8177485 -5.8638954 -5.782052 -5.8610573 -6.1915326 -6.5690207 -6.8905926 -7.3152833][-7.0096955 -7.0184107 -7.028657 -7.187973 -6.7665477 -6.2637639 -5.9659362 -5.7175155 -5.6576562 -5.6989775 -6.1284924 -6.3442373 -6.4948921 -6.5184841 -6.6485057][-7.6195617 -7.3769565 -7.4815187 -7.631444 -7.4183197 -6.8551879 -6.4562483 -6.2225122 -6.1611104 -6.0741715 -6.0053596 -5.9483552 -5.9953408 -6.1218143 -6.0333595]]...]
INFO - root - 2017-12-16 02:01:18.642803: step 87710, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 42h:21m:10s remains)
INFO - root - 2017-12-16 02:01:25.045151: step 87720, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.633 sec/batch; 43h:01m:19s remains)
INFO - root - 2017-12-16 02:01:31.418901: step 87730, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 43h:47m:53s remains)
INFO - root - 2017-12-16 02:01:38.060387: step 87740, loss = 0.24, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 45h:23m:45s remains)
INFO - root - 2017-12-16 02:01:44.480146: step 87750, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 43h:12m:43s remains)
INFO - root - 2017-12-16 02:01:50.901179: step 87760, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 43h:55m:12s remains)
INFO - root - 2017-12-16 02:01:57.271405: step 87770, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 43h:03m:35s remains)
INFO - root - 2017-12-16 02:02:03.693924: step 87780, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 43h:39m:07s remains)
INFO - root - 2017-12-16 02:02:10.110608: step 87790, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 43h:42m:10s remains)
INFO - root - 2017-12-16 02:02:16.603317: step 87800, loss = 0.33, batch loss = 0.22 (11.9 examples/sec; 0.672 sec/batch; 45h:42m:04s remains)
2017-12-16 02:02:17.105138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.512537 -5.9474125 -5.9002476 -6.1737404 -6.3257203 -6.9686384 -7.2342544 -6.80264 -6.3223224 -6.4268694 -6.6377654 -6.9858656 -6.8595934 -7.467804 -7.7025576][-5.50447 -5.2434039 -4.9599557 -5.5395503 -6.4118333 -6.9748411 -6.6506047 -6.479187 -6.1071534 -6.3946252 -6.3842897 -7.1163936 -7.4423976 -7.8552556 -7.9577551][-4.2431936 -4.0796471 -3.9922259 -4.3719788 -4.91014 -5.3789678 -5.6243453 -5.1598492 -4.86124 -5.0582056 -5.5382452 -6.3041515 -6.1196055 -6.516531 -6.7185287][-3.7996905 -3.2968097 -3.0740438 -3.3545017 -3.5185022 -3.1421075 -2.4644189 -2.3775449 -2.0438251 -2.3499408 -3.5581574 -4.752243 -5.3064137 -5.6999388 -5.9956684][-3.2237644 -2.5158253 -2.1556501 -2.0082164 -1.8347282 -1.349421 -0.62838173 -0.18156052 0.46264458 -0.050395966 -1.9479327 -3.6218648 -4.3243895 -5.3032646 -6.0888162][-3.0863695 -2.5650468 -2.08805 -1.1343961 -0.20807314 0.77343464 2.022563 2.5466337 2.8011665 1.5320168 -0.73257589 -2.7774434 -4.0354319 -5.3280754 -6.2545948][-3.5521021 -2.1961784 -1.4476147 -0.30901051 1.0271664 2.7603273 4.0768194 4.3869505 4.6386051 2.6734772 -0.51461697 -3.2034588 -4.7197075 -6.1318564 -7.1020808][-3.8835938 -2.3724346 -1.4448996 0.033186913 1.7348185 2.9864769 3.9260464 4.7143183 4.8556175 2.7453136 0.04906559 -3.07582 -4.9474611 -6.6654234 -7.4843678][-4.8825855 -3.5104318 -2.5008097 -0.9115715 0.42074966 1.8040209 3.4409666 3.7013235 3.7780991 1.9946909 -0.81997156 -3.7219918 -5.2895236 -6.9274669 -7.6441455][-6.2335629 -5.4613028 -4.6688824 -3.41852 -2.3590331 -0.81994629 0.6191988 1.1713791 1.6825132 -0.089754105 -2.1937704 -5.0110531 -6.544497 -7.9482741 -8.525321][-7.6031618 -7.452333 -6.9034915 -5.8224225 -5.3725181 -4.0136127 -2.7111392 -2.1260352 -1.5923309 -2.4278417 -3.8801782 -5.9724207 -7.0159206 -8.3115358 -8.5667706][-8.7711811 -8.7397184 -8.070363 -7.2047014 -6.8924084 -6.0714049 -5.3196678 -4.4141741 -3.6572819 -4.3753767 -5.0129128 -6.2771096 -6.8810434 -7.63979 -8.0582952][-9.39943 -9.6999426 -9.540266 -8.7781563 -8.121767 -7.271574 -6.533535 -6.49245 -6.5935326 -6.9320211 -6.9902105 -7.3915968 -7.301868 -7.2786355 -7.2826939][-8.6677294 -8.7617683 -8.5008488 -7.9299097 -7.622581 -6.837986 -6.1848059 -6.2460742 -6.13801 -6.3167038 -6.5082808 -7.0928307 -6.9254632 -6.7166786 -6.6211786][-9.1644764 -8.9333277 -8.8196764 -8.0599155 -7.7043128 -7.4399071 -7.21958 -7.4776483 -7.4375353 -7.3812194 -7.29488 -7.1626878 -6.9827228 -6.7345982 -6.4982767]]...]
INFO - root - 2017-12-16 02:02:23.487518: step 87810, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 44h:25m:03s remains)
INFO - root - 2017-12-16 02:02:30.010078: step 87820, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 44h:58m:20s remains)
INFO - root - 2017-12-16 02:02:36.499360: step 87830, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 45h:00m:23s remains)
INFO - root - 2017-12-16 02:02:42.913581: step 87840, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 43h:39m:31s remains)
INFO - root - 2017-12-16 02:02:49.303504: step 87850, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 45h:30m:54s remains)
INFO - root - 2017-12-16 02:02:55.746088: step 87860, loss = 0.44, batch loss = 0.33 (12.7 examples/sec; 0.631 sec/batch; 42h:54m:33s remains)
INFO - root - 2017-12-16 02:03:02.210641: step 87870, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:41m:01s remains)
INFO - root - 2017-12-16 02:03:08.732279: step 87880, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:46m:10s remains)
INFO - root - 2017-12-16 02:03:15.162433: step 87890, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:45m:05s remains)
INFO - root - 2017-12-16 02:03:21.657704: step 87900, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 42h:56m:38s remains)
2017-12-16 02:03:22.209616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2908621 -5.0957203 -4.6037025 -4.5287242 -4.2318439 -3.3988872 -2.3209195 -1.5242462 -0.60868359 -1.8833857 -2.9511375 -3.7904141 -5.2298932 -6.8702083 -8.3304739][-5.9099169 -5.6786747 -5.7987194 -5.8939724 -5.9505415 -5.5426044 -4.8365269 -4.2139826 -3.2229934 -4.1765041 -5.0650992 -5.4992576 -6.5849657 -7.6174088 -8.7947464][-6.2131777 -5.913434 -5.5003328 -5.2310038 -4.7978039 -4.3159304 -3.8484979 -3.6935093 -3.5583549 -4.9957085 -6.0884247 -6.6079211 -7.5664763 -8.5345144 -9.0288887][-6.6450305 -5.4246535 -4.4298019 -3.4570341 -2.609983 -1.7191682 -0.9712038 -0.88259745 -1.0376768 -3.3822842 -5.2171278 -6.3702936 -7.8274012 -8.6892262 -9.291235][-6.9944811 -6.0507426 -4.5982141 -3.5783558 -2.0527611 -0.5621376 0.62949562 1.0437078 0.87451172 -1.1591716 -3.1587615 -4.7406282 -6.5991316 -7.64108 -8.4512634][-7.0668731 -5.6873269 -4.3216047 -3.0462132 -1.4017234 0.082805634 1.2717857 1.7635975 1.8592825 0.083669186 -1.4930768 -2.795949 -4.7212715 -5.747427 -6.6538124][-5.9757032 -4.9905224 -3.2922859 -1.6303654 -0.070455074 1.3131142 2.2928181 2.5778704 2.7557449 0.83906937 -0.79962492 -1.9462643 -3.9677339 -4.896914 -5.706738][-4.9771433 -3.235775 -1.8811531 -0.08433485 1.4811363 2.3622513 2.970583 3.2645302 3.1889744 1.1370401 -0.453516 -1.7990408 -4.0977211 -5.3152733 -6.0046129][-3.7665823 -2.935173 -2.07938 -0.49037886 1.3381042 2.197546 2.5741043 2.2273979 1.6616669 -0.35740423 -1.1238894 -2.1498675 -4.4678936 -5.5046372 -5.7533965][-3.2268467 -2.8637567 -2.8116536 -1.941329 -1.2862258 -0.54477549 0.32218742 0.099766731 -0.23950052 -1.8650765 -2.8275509 -3.3306031 -4.6016817 -5.6387405 -6.0326939][-2.9558854 -2.4989557 -2.1619539 -1.9653726 -1.7247849 -1.4542942 -1.1322517 -1.345468 -1.5335059 -3.3479323 -4.3682117 -4.7255096 -5.80829 -6.2133174 -5.840415][-4.6455956 -4.2918835 -3.59692 -3.1797023 -2.5905943 -1.8569765 -2.1181016 -2.8089094 -3.3086329 -4.7451239 -5.4328656 -6.0917463 -6.4864664 -6.694078 -6.2245836][-6.1029186 -5.8917956 -5.6037946 -5.35048 -5.0449772 -4.5701723 -4.2675109 -4.4052496 -4.60596 -5.8714786 -6.6722174 -6.7611518 -6.6646204 -6.4212151 -5.7917476][-5.6785588 -5.32179 -4.806056 -4.9795494 -4.7934217 -4.5108848 -4.5426254 -5.0908575 -5.6797829 -6.3529816 -6.3270473 -6.530139 -6.5200543 -6.314033 -5.8901305][-6.2103033 -5.7197123 -5.1623659 -5.304038 -5.1798725 -5.1214914 -5.310226 -5.6936269 -5.9923983 -5.9546719 -6.2444992 -6.2894497 -6.398757 -6.1205664 -5.917954]]...]
INFO - root - 2017-12-16 02:03:28.608636: step 87910, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 42h:41m:53s remains)
INFO - root - 2017-12-16 02:03:35.138909: step 87920, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 43h:35m:57s remains)
INFO - root - 2017-12-16 02:03:41.511262: step 87930, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 43h:04m:18s remains)
INFO - root - 2017-12-16 02:03:47.906527: step 87940, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 43h:18m:58s remains)
INFO - root - 2017-12-16 02:03:54.376276: step 87950, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 43h:08m:12s remains)
INFO - root - 2017-12-16 02:04:00.833264: step 87960, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 42h:37m:02s remains)
INFO - root - 2017-12-16 02:04:07.264637: step 87970, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 43h:30m:22s remains)
INFO - root - 2017-12-16 02:04:13.823366: step 87980, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 44h:22m:51s remains)
INFO - root - 2017-12-16 02:04:20.304821: step 87990, loss = 0.36, batch loss = 0.25 (12.5 examples/sec; 0.638 sec/batch; 43h:18m:43s remains)
INFO - root - 2017-12-16 02:04:26.761025: step 88000, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 44h:05m:54s remains)
2017-12-16 02:04:27.319933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2848511 -3.4673634 -3.6562033 -4.2194448 -4.1467943 -4.249413 -4.2487369 -4.1675587 -4.2729468 -4.0607738 -5.3431377 -7.29529 -7.5511885 -8.4702024 -8.892271][-2.9050674 -2.9862261 -2.9096222 -3.3642492 -3.4608846 -3.6589618 -3.9405146 -3.8384514 -3.9669471 -3.7560406 -4.7157464 -6.6186795 -6.8289056 -7.823566 -8.4083309][-2.0114074 -1.8000937 -1.6924243 -2.0463829 -2.039412 -1.8417382 -1.7635918 -2.0821376 -2.5165 -2.5345817 -3.6295538 -5.4234734 -5.5989552 -6.5138211 -7.0825176][-1.7792501 -1.6628232 -1.597754 -1.4999046 -1.2633557 -0.67848349 -0.1767354 -0.44818306 -0.806036 -0.97876453 -2.3637176 -4.4381475 -4.8506026 -5.8008089 -6.2493258][-1.4498534 -1.3667483 -1.2987971 -1.4335294 -1.3328633 -0.55129766 0.25704479 0.28423738 0.15607166 0.22813749 -0.95291138 -3.0862908 -3.6093087 -4.7986097 -5.4779806][-1.6209588 -1.4451847 -1.1831746 -1.0670691 -0.81299019 0.14465141 1.0013409 0.94789982 0.90268707 0.972744 -0.11295557 -2.1816969 -2.8344774 -4.1129107 -5.1499209][-3.1462703 -2.6695018 -2.112102 -1.5857148 -1.104722 0.10225677 1.2642717 1.3879175 1.5570889 1.5530777 0.34703255 -1.7384229 -2.5299678 -3.8316529 -5.0404587][-4.1509304 -3.7008522 -2.9572334 -2.0105038 -0.95600176 0.27420616 1.2657356 1.4892397 1.7701902 1.7442236 0.63928032 -1.3975616 -2.2250371 -3.6557121 -5.0300837][-4.6497169 -4.0472589 -3.1910157 -2.200069 -1.0938468 0.11082458 1.1623468 1.5385714 1.9588985 1.9648724 0.73129368 -1.4166541 -2.561955 -3.9708598 -5.068141][-4.7528262 -4.1962929 -3.4080086 -2.4272342 -1.558372 -0.47194147 0.58964634 0.84086227 1.1601772 0.991745 -0.22411776 -2.0391002 -2.9528437 -4.3882341 -5.3929052][-5.922502 -5.2962847 -4.562921 -3.5234933 -2.7504287 -2.0312266 -1.292779 -1.3273721 -1.2779245 -1.5546484 -2.2837996 -3.3832049 -3.8611758 -4.9969139 -5.9208865][-7.2890038 -6.8240857 -6.369575 -5.2869868 -4.8310719 -4.1975412 -3.5034842 -3.8170776 -4.1019945 -4.1929779 -4.684289 -5.031364 -5.063632 -5.5315981 -5.9299297][-7.5869212 -7.4381976 -7.3617468 -6.6720963 -6.6285982 -6.2071719 -5.6568689 -5.879106 -6.0328679 -5.8984675 -6.1904497 -6.0194979 -5.52307 -5.428483 -5.2184744][-7.1109147 -7.2482142 -7.4529676 -7.1407733 -7.3246531 -7.1949244 -6.8740516 -6.9048738 -6.9588575 -6.508358 -6.659111 -6.5556755 -6.1326642 -5.8164382 -5.3665791][-7.4722657 -7.3583026 -7.3445935 -7.376688 -7.6525116 -7.6544933 -7.5006218 -7.6807585 -7.8430352 -7.5481772 -7.1617551 -6.6590967 -6.1194553 -6.2028561 -6.3609819]]...]
INFO - root - 2017-12-16 02:04:33.913570: step 88010, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 44h:09m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:04:40.344298: step 88020, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:42m:19s remains)
INFO - root - 2017-12-16 02:04:46.829750: step 88030, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 43h:19m:52s remains)
INFO - root - 2017-12-16 02:04:53.211861: step 88040, loss = 0.35, batch loss = 0.24 (12.7 examples/sec; 0.631 sec/batch; 42h:51m:11s remains)
INFO - root - 2017-12-16 02:04:59.611643: step 88050, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 44h:23m:52s remains)
INFO - root - 2017-12-16 02:05:06.114910: step 88060, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 43h:30m:28s remains)
INFO - root - 2017-12-16 02:05:12.649454: step 88070, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 44h:20m:47s remains)
INFO - root - 2017-12-16 02:05:19.029672: step 88080, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 44h:07m:12s remains)
INFO - root - 2017-12-16 02:05:25.492728: step 88090, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 44h:53m:41s remains)
INFO - root - 2017-12-16 02:05:31.935970: step 88100, loss = 0.34, batch loss = 0.22 (12.2 examples/sec; 0.654 sec/batch; 44h:24m:12s remains)
2017-12-16 02:05:32.467661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9064956 -5.739069 -5.5660448 -5.0782251 -3.953671 -2.7808604 -2.3800912 -2.0554433 -1.5288515 -2.1629696 -3.4364815 -5.6281805 -6.6034184 -7.7764335 -8.28713][-6.3347692 -6.5062289 -6.5348997 -6.2079391 -5.5768414 -4.6620321 -3.4996376 -3.3275633 -3.0210371 -3.5069327 -4.7541 -7.0046296 -7.905508 -8.537405 -8.8254728][-6.3783917 -6.738966 -6.86705 -6.5432329 -6.3155947 -5.5516777 -4.873951 -4.7738008 -3.9353755 -4.1365991 -5.0403814 -6.9584379 -8.1477289 -9.2710571 -9.3063869][-6.5241718 -6.8477073 -6.8348265 -6.3956652 -5.7145824 -4.839467 -4.0867238 -3.7273135 -2.8288116 -3.2406416 -4.278285 -6.5149021 -7.6722693 -8.754137 -9.5570631][-7.2429495 -6.961844 -6.6310215 -5.8357649 -4.6364212 -3.5509725 -2.8586955 -2.6345224 -1.9145017 -2.0622239 -2.9175091 -5.6751909 -7.4966483 -9.2754526 -10.062691][-7.9263844 -7.4829392 -6.7480283 -5.0256863 -3.2116113 -1.8835301 -0.71492672 -0.42411613 0.66298771 0.46509457 -0.87839746 -4.0423131 -6.5665674 -9.1014252 -10.412997][-8.511467 -7.0776405 -5.8758531 -3.5501366 -1.5093546 0.71031475 2.2820215 2.8025208 3.920845 3.6031942 1.8966904 -1.9832015 -5.1222262 -7.6819625 -9.42966][-7.3762951 -7.3221669 -5.973515 -3.0589066 -0.09467268 2.1086597 3.6401052 4.5072775 6.0524082 5.8352537 3.8247786 -0.013908863 -3.6078129 -6.418314 -8.3209534][-6.9880366 -6.287612 -6.0987964 -4.2460413 -1.412209 0.98070145 2.8304138 2.9516821 4.14808 4.4526205 3.4822006 0.27786255 -2.8488116 -5.477603 -7.42802][-6.5333176 -6.4312816 -6.4642344 -4.580864 -2.8804259 -0.91182804 0.73206043 1.3868275 2.6541376 2.0568161 1.0037374 -1.5789704 -3.5623603 -5.4600935 -6.6300073][-5.617125 -5.8102417 -6.0011587 -4.9285288 -3.5326924 -1.9332838 -0.58449507 -0.65625715 0.082139969 -0.82645273 -2.2177143 -4.0491905 -5.7452173 -6.0050297 -6.0470114][-4.408761 -4.1031871 -4.0292797 -3.4476581 -2.6015573 -1.5987382 -0.96429682 -1.4508524 -1.8828917 -3.2437229 -4.5937347 -6.4579606 -7.8574152 -7.5980539 -6.9957356][-4.2618036 -3.8765063 -3.6474819 -3.003664 -2.6040125 -2.1537218 -1.4320579 -0.96059275 -1.2088284 -3.1936512 -4.6694822 -6.4208059 -7.7851734 -8.0251989 -7.8972535][-5.3257122 -4.6816244 -4.052928 -3.4534407 -3.1522937 -2.5867205 -1.8617754 -2.0351481 -1.5888963 -2.5955734 -4.0541344 -5.7633052 -6.7680507 -7.3167329 -7.3768778][-5.4354382 -5.2335854 -5.2135787 -4.5227413 -3.955632 -3.8904343 -4.0155277 -3.676281 -3.7131121 -3.7003975 -4.5607405 -5.6443872 -6.75037 -7.7577314 -7.4872789]]...]
INFO - root - 2017-12-16 02:05:38.921307: step 88110, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:13m:26s remains)
INFO - root - 2017-12-16 02:05:45.386029: step 88120, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 44h:39m:42s remains)
INFO - root - 2017-12-16 02:05:51.814377: step 88130, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 44h:13m:29s remains)
INFO - root - 2017-12-16 02:05:58.239894: step 88140, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 43h:37m:11s remains)
INFO - root - 2017-12-16 02:06:04.713590: step 88150, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 42h:42m:16s remains)
INFO - root - 2017-12-16 02:06:11.209739: step 88160, loss = 0.24, batch loss = 0.12 (12.8 examples/sec; 0.625 sec/batch; 42h:26m:17s remains)
INFO - root - 2017-12-16 02:06:17.573208: step 88170, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 42h:46m:57s remains)
INFO - root - 2017-12-16 02:06:24.063412: step 88180, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 42h:50m:27s remains)
INFO - root - 2017-12-16 02:06:30.560681: step 88190, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 45h:05m:28s remains)
INFO - root - 2017-12-16 02:06:36.997369: step 88200, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 43h:01m:21s remains)
2017-12-16 02:06:37.541737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3491011 -3.2900662 -3.4083686 -3.7697411 -4.5285139 -5.17018 -5.2058024 -5.1047773 -4.7101316 -4.7204804 -5.5427017 -7.1955752 -7.6114807 -7.8253522 -8.0466957][-3.1531944 -3.2972856 -4.1227803 -4.7243032 -4.83612 -4.7898493 -4.7842536 -4.6202283 -4.215631 -4.2554379 -5.1930122 -6.9254704 -7.4979591 -7.5958486 -7.832902][-2.6064892 -3.0170693 -3.7988017 -3.6756148 -3.5828958 -3.2284813 -2.8372097 -2.758451 -2.7691851 -2.862709 -3.8809576 -6.1259747 -7.3687906 -7.745151 -8.1237583][-2.8687968 -2.6448607 -3.549819 -3.7573409 -3.3573155 -2.5691004 -2.0266562 -1.7910137 -1.7379122 -2.1683612 -3.2419834 -5.9787216 -7.3754745 -7.9785328 -8.223033][-2.9571943 -2.3818192 -2.4164758 -2.41504 -2.5352368 -1.426157 -0.31932878 -0.03088665 -0.2387104 -1.2774949 -2.8372335 -5.6948447 -6.9984679 -7.6720362 -7.9716759][-5.3249559 -4.273138 -3.5606956 -3.0879502 -2.2590833 -0.21061182 1.5460625 2.0669746 1.5407314 -0.1863637 -2.1911893 -5.3611679 -7.05489 -7.6785774 -7.8100061][-4.7225313 -4.4441276 -3.7017198 -2.1069903 -0.87525749 1.0301828 3.35153 4.1324015 4.20181 2.7267666 0.21338558 -3.8339512 -5.7404118 -6.6725421 -7.0822544][-3.7658339 -3.6608491 -2.7026811 -0.7747736 1.4727068 3.344552 4.7718573 5.3172436 5.4331675 4.0215826 1.9242306 -2.3735814 -4.8462105 -6.0526972 -6.1562395][-3.5070066 -3.0363817 -2.4590993 -0.58280039 1.1141167 2.8097448 4.18978 4.8211384 4.8076239 3.4516649 1.02707 -2.9800439 -5.1501102 -6.2979593 -6.3882995][-5.5911751 -5.1751156 -4.1406322 -2.4277606 -0.96758842 0.62465858 1.6323566 1.7136946 1.538188 0.32132053 -1.4028897 -4.0634022 -6.3540144 -7.1809821 -7.3050551][-6.8781486 -7.1496549 -6.2491846 -4.6544166 -3.1456513 -1.5724874 -0.49031162 -0.33121586 -0.33802652 -1.8962979 -3.7360661 -5.908298 -7.1846762 -7.3445754 -7.2847881][-8.4478264 -8.6902142 -8.494153 -7.300631 -5.8170013 -4.6245871 -3.7998414 -3.8967748 -3.7969856 -4.788456 -5.8519511 -6.9023824 -7.3592672 -7.38511 -7.6412768][-8.4234743 -8.4980259 -8.5471945 -7.6465697 -6.9657478 -6.1652675 -5.4217663 -5.3332663 -5.4950247 -5.9125018 -6.2568874 -6.6401386 -7.1856518 -7.0741415 -7.1703691][-8.3382473 -8.2162666 -7.6904826 -6.776082 -6.2937717 -5.9084263 -6.173986 -6.7255158 -6.7175083 -6.7081008 -6.7447042 -7.2340174 -7.1853008 -6.6920333 -6.4262261][-10.168518 -9.6188231 -9.0874367 -8.4163713 -7.6806579 -7.3936968 -6.74394 -7.1209769 -8.0953951 -8.2313986 -7.8603339 -7.5413275 -7.1003084 -6.5222974 -5.74369]]...]
INFO - root - 2017-12-16 02:06:43.972051: step 88210, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 43h:44m:09s remains)
INFO - root - 2017-12-16 02:06:50.444902: step 88220, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 42h:52m:28s remains)
INFO - root - 2017-12-16 02:06:56.931417: step 88230, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.637 sec/batch; 43h:15m:15s remains)
INFO - root - 2017-12-16 02:07:03.400961: step 88240, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 43h:37m:03s remains)
INFO - root - 2017-12-16 02:07:09.893580: step 88250, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 43h:06m:18s remains)
INFO - root - 2017-12-16 02:07:16.354685: step 88260, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 43h:50m:35s remains)
INFO - root - 2017-12-16 02:07:22.757958: step 88270, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 43h:37m:19s remains)
INFO - root - 2017-12-16 02:07:29.186545: step 88280, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 42h:11m:09s remains)
INFO - root - 2017-12-16 02:07:35.598347: step 88290, loss = 0.35, batch loss = 0.24 (12.4 examples/sec; 0.646 sec/batch; 43h:47m:52s remains)
INFO - root - 2017-12-16 02:07:41.974976: step 88300, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 42h:35m:12s remains)
2017-12-16 02:07:42.507979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9209971 -2.4389539 -1.6399302 -1.6361246 -2.0236735 -2.762382 -2.3390794 -1.4029188 -1.5258288 -3.2412372 -4.9511662 -6.0409431 -6.7435474 -6.8873148 -6.6429758][-1.8203287 -1.9870205 -2.2447329 -1.8616457 -1.1984539 -1.3610506 -1.6521659 -2.0491109 -1.9321289 -2.8461208 -4.1641474 -5.4111433 -6.4601173 -6.9230247 -7.0810695][-1.809865 -1.4663897 -1.2167039 -1.4718757 -1.5606875 -1.4482718 -1.6194677 -2.1443868 -2.2931337 -3.3791542 -3.9308155 -4.8326216 -6.1264644 -7.079453 -6.7775373][-1.5142174 -0.45680141 -0.52722025 -0.68088579 -1.0311618 -1.2313342 -1.3765869 -1.2344913 -1.5748334 -3.4627829 -4.5101538 -4.6593308 -4.5320253 -5.3697567 -6.2202964][-2.6738877 -1.1884065 -0.0805521 0.44991779 -0.086042881 -0.71790504 -1.1290393 -0.71814013 -0.56827831 -2.0887566 -3.494225 -5.0367212 -5.7933707 -5.7381258 -5.3225636][-0.89142847 -0.3465662 -0.23842239 0.87625694 1.7096634 1.8541746 1.3506908 0.68071365 -0.067002773 -1.483418 -2.3389826 -3.2688756 -4.4094286 -5.257555 -5.2917242][0.5315752 1.7271605 2.2639294 2.2762117 2.2507067 2.6703358 2.8829985 2.4941654 1.8052006 -0.17235613 -1.6168971 -2.6082687 -3.736263 -4.4433265 -4.5527263][0.768075 2.0965233 2.8855219 2.4876223 2.4961767 2.5565386 2.2183428 2.2583961 2.055088 0.36657238 -0.97934246 -2.56955 -3.6686158 -4.7007084 -4.6834745][1.308116 1.9298277 2.4689741 2.6865892 2.4637051 2.097559 1.7165518 1.7149086 1.9591293 0.60605431 -0.75332355 -2.2596498 -3.7596507 -4.5040522 -4.4533496][0.390625 0.54486084 0.71427155 1.2678623 1.9967318 1.9138823 1.3626614 1.2628555 1.1028528 -0.11744404 -1.2111435 -2.8457303 -4.0345774 -4.5853357 -4.6767483][-2.4465747 -1.7299366 -1.546349 -1.1119542 -0.7539854 -0.75133181 -0.55984735 -0.3827343 -0.56806517 -2.2548752 -3.9185908 -4.3345208 -4.8210597 -6.0380836 -6.1322956][-5.1203203 -4.6015539 -4.0494704 -3.0717568 -2.9923468 -3.2661572 -3.6578789 -3.4778671 -3.2799363 -4.0853238 -5.1566839 -5.30105 -6.1131091 -6.8817935 -7.0245037][-7.8314476 -6.9555149 -6.3187523 -5.752481 -5.5683165 -5.0634575 -4.92235 -5.2300739 -5.5065937 -6.1438003 -6.7620997 -6.9231892 -7.1100388 -6.9028525 -6.4801683][-7.9344811 -7.4965658 -6.9927058 -6.5815921 -6.609107 -6.519021 -6.2913446 -6.1047049 -6.0080795 -6.225018 -6.5267029 -6.6201715 -6.78864 -7.0308986 -6.6506524][-7.889657 -6.9474292 -6.4146428 -6.6225424 -7.0495615 -6.824019 -6.4090395 -6.7135596 -7.154202 -6.6413665 -6.37936 -6.2274022 -6.1620655 -6.4086032 -6.3168755]]...]
INFO - root - 2017-12-16 02:07:48.799982: step 88310, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.619 sec/batch; 41h:58m:52s remains)
INFO - root - 2017-12-16 02:07:55.229598: step 88320, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:41m:05s remains)
INFO - root - 2017-12-16 02:08:01.588821: step 88330, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.626 sec/batch; 42h:28m:58s remains)
INFO - root - 2017-12-16 02:08:08.010300: step 88340, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 43h:31m:50s remains)
INFO - root - 2017-12-16 02:08:14.320279: step 88350, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 43h:01m:39s remains)
INFO - root - 2017-12-16 02:08:20.662567: step 88360, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.624 sec/batch; 42h:18m:43s remains)
INFO - root - 2017-12-16 02:08:27.053934: step 88370, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.614 sec/batch; 41h:37m:48s remains)
INFO - root - 2017-12-16 02:08:33.430172: step 88380, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 42h:27m:36s remains)
INFO - root - 2017-12-16 02:08:39.768128: step 88390, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 42h:45m:25s remains)
INFO - root - 2017-12-16 02:08:46.177306: step 88400, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 42h:34m:25s remains)
2017-12-16 02:08:46.652218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0464277 -2.7388759 -2.8617902 -2.8043361 -2.6575222 -2.5923615 -2.1519308 -1.6168876 -1.0292168 -2.3234081 -3.5675044 -4.9137549 -5.9310012 -7.1666179 -7.9609766][-3.0417323 -3.2422132 -3.5592313 -3.7285013 -3.6125026 -3.1119208 -2.6882052 -2.239778 -1.5731239 -2.7431931 -3.8842354 -5.3132591 -6.6290178 -7.6289897 -8.3316059][-2.1944404 -2.0757656 -2.5819054 -2.869379 -2.7841892 -2.5596619 -2.0725017 -1.6364851 -1.3352771 -2.8265352 -4.1532631 -5.3728209 -6.5318589 -7.9033251 -8.6434956][-2.2217526 -1.9705234 -1.8669553 -1.6124578 -1.4177246 -1.0252285 -0.61229944 -0.49043989 -0.46527815 -2.4036465 -4.06637 -5.7128744 -6.9450188 -7.7929764 -8.3174915][-2.5379825 -2.186655 -1.5767879 -1.071084 -0.4905467 0.17039061 0.55574703 0.43401718 0.15757751 -1.9035859 -3.6805849 -5.4531207 -6.7565145 -7.7553797 -8.2437658][-2.6413975 -2.0969214 -1.5879326 -0.76616335 0.071891308 1.0952053 1.6492138 1.420845 1.0799952 -1.3699064 -3.450995 -5.1066322 -6.4380608 -7.5727277 -8.2058][-2.922462 -2.336422 -1.5323429 -0.33701372 0.92510509 2.1616488 2.67875 2.565649 2.1653728 -0.54184628 -3.0006337 -5.0425906 -6.3069024 -7.5526161 -8.3835754][-3.0615067 -2.3653297 -1.4620438 -0.17724371 1.3518076 2.5558214 3.0336494 3.0273438 2.6825676 0.062024593 -2.3891578 -4.7038541 -6.2665477 -7.4649086 -8.0167675][-3.3662825 -2.5124922 -1.861176 -0.71868467 0.71819019 1.6965914 2.1548882 2.228982 2.1260853 -0.24884987 -2.6051264 -4.562295 -6.0671821 -7.4661751 -8.0694532][-3.6857963 -3.163743 -2.3697062 -1.2248526 -0.06483078 0.88590622 1.214015 1.3450909 1.3356047 -1.1628151 -3.113677 -4.7869959 -6.0504975 -7.3710833 -8.1735163][-5.4137344 -4.7950993 -4.0434189 -3.1108918 -2.0143857 -0.99746943 -0.550653 -0.78772545 -0.98666763 -3.0784702 -4.790534 -5.8434725 -6.5656905 -7.6089787 -7.9673429][-6.1594868 -5.5623569 -4.8015203 -4.184629 -3.5159931 -2.7164311 -2.601192 -2.8851328 -3.0508709 -4.6565576 -5.6390018 -6.2256527 -6.69137 -7.4550037 -7.6399069][-6.7543688 -6.3086152 -5.6601334 -5.0386491 -4.4799614 -4.1882868 -3.9527583 -4.1493788 -4.5655613 -5.534657 -6.3392296 -6.4474063 -6.6088314 -6.9155693 -6.7451134][-6.8148756 -6.3740997 -5.754055 -5.2097163 -4.9424496 -4.7538233 -4.8952932 -5.159564 -5.2434263 -5.775126 -5.9667492 -5.9976296 -6.2021527 -6.4172006 -6.300097][-7.8804855 -7.2940392 -6.8280587 -6.5458221 -6.4849296 -6.4352918 -6.6325483 -6.9849143 -7.1978469 -6.9442673 -6.5531654 -6.2881722 -6.1662116 -6.1889811 -5.9976778]]...]
INFO - root - 2017-12-16 02:08:52.944611: step 88410, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 42h:41m:06s remains)
INFO - root - 2017-12-16 02:08:59.233345: step 88420, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:26m:41s remains)
INFO - root - 2017-12-16 02:09:05.602142: step 88430, loss = 0.24, batch loss = 0.13 (12.8 examples/sec; 0.624 sec/batch; 42h:17m:25s remains)
INFO - root - 2017-12-16 02:09:12.035884: step 88440, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 42h:28m:49s remains)
INFO - root - 2017-12-16 02:09:18.457985: step 88450, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.651 sec/batch; 44h:09m:52s remains)
INFO - root - 2017-12-16 02:09:24.831470: step 88460, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.620 sec/batch; 42h:02m:35s remains)
INFO - root - 2017-12-16 02:09:31.144869: step 88470, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 43h:18m:07s remains)
INFO - root - 2017-12-16 02:09:37.604343: step 88480, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:56m:53s remains)
INFO - root - 2017-12-16 02:09:43.994988: step 88490, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 42h:35m:12s remains)
INFO - root - 2017-12-16 02:09:50.320807: step 88500, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 44h:01m:48s remains)
2017-12-16 02:09:50.809614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9109306 -5.5396385 -5.8170285 -5.6293335 -5.0577607 -4.311729 -3.9125454 -3.5055385 -3.565824 -5.6869831 -6.6475744 -7.5312495 -7.6617684 -7.6465697 -7.1670332][-4.1528463 -4.8509316 -5.2378473 -5.2230473 -5.0139751 -4.1003733 -3.6547771 -3.3683448 -4.0263214 -6.1704779 -7.1578851 -8.5914173 -9.4275331 -9.8195486 -8.9794426][-4.0520034 -4.6468363 -4.9484148 -4.7926617 -4.0168533 -3.3329234 -2.6823683 -2.2554355 -2.3481259 -5.0318565 -6.5988665 -8.2660093 -9.102663 -10.082647 -9.902524][-3.7838864 -4.1072931 -4.5386982 -4.4083095 -3.364408 -2.1928644 -1.3718066 -1.1770496 -1.0498128 -3.1691136 -4.3993535 -6.8005805 -8.0799494 -9.4566612 -9.6686974][-3.6810088 -3.3477492 -3.0636 -2.5418458 -1.2669773 -0.23056316 0.42200851 0.30999947 0.19572687 -2.0818963 -3.5843563 -5.4291658 -6.564568 -7.8741193 -8.3386469][-4.049396 -2.903687 -2.0839953 -1.1134048 -0.1270957 1.1373825 1.8229084 1.6585894 1.1033039 -1.092329 -2.4380984 -4.2613473 -4.9834542 -6.0356035 -6.480423][-3.851356 -2.6302848 -1.4477286 0.18877506 1.8073978 2.9786968 3.3474474 3.0640402 2.7688322 0.30594587 -1.3470469 -3.5288825 -4.5958529 -5.3715916 -5.6356363][-3.1072116 -2.070672 -1.1633115 0.591506 2.2194529 3.7662125 4.6121216 4.0627728 3.3650427 0.95217896 -0.55168247 -2.7668509 -3.7982264 -4.8985729 -5.0218239][-3.6769781 -2.5646486 -1.7247658 -0.096351147 1.4868841 3.0495596 3.7259855 3.8301296 3.2060232 0.60220528 -0.87844515 -2.6284456 -3.7408934 -4.8688536 -5.048769][-4.651576 -3.5832276 -3.1365762 -1.9252539 -0.67989922 0.87674046 2.1998415 2.7639933 2.6902847 0.30473852 -1.4666181 -3.2173738 -4.0223589 -5.033494 -5.199615][-5.3706822 -4.9136877 -4.1519194 -2.9324098 -1.9946361 -0.99873495 -0.12956905 0.37680531 0.4724474 -1.263339 -2.2705584 -3.8094952 -4.6475768 -5.6549635 -5.6390257][-5.9319463 -5.4527369 -5.0904331 -4.1533427 -3.3257775 -2.4635768 -1.8988223 -1.582418 -1.5816007 -2.7765422 -3.5490379 -4.4950933 -4.968308 -5.9045172 -6.3644814][-6.6516542 -6.0961471 -5.4799109 -4.6765122 -4.3458314 -3.8457472 -3.3572664 -3.1935267 -3.0689864 -3.6931427 -4.3409624 -4.6968517 -5.0842791 -5.4689612 -6.0454068][-6.8490281 -6.5002413 -5.98525 -4.982543 -4.3785267 -4.0591726 -4.1659112 -4.40473 -4.6322603 -4.3662896 -4.7714233 -4.5420427 -4.2244968 -4.4639492 -4.9345636][-6.8199868 -6.815618 -6.4844356 -5.7288117 -5.1279745 -4.748796 -4.54545 -4.8954325 -5.297657 -4.9955072 -4.989625 -4.8561 -4.8078489 -4.5617619 -4.6375818]]...]
INFO - root - 2017-12-16 02:09:57.102100: step 88510, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 43h:04m:49s remains)
INFO - root - 2017-12-16 02:10:03.497233: step 88520, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 44h:01m:02s remains)
INFO - root - 2017-12-16 02:10:09.829900: step 88530, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 43h:23m:04s remains)
INFO - root - 2017-12-16 02:10:16.159681: step 88540, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:16m:48s remains)
INFO - root - 2017-12-16 02:10:22.521935: step 88550, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 43h:35m:26s remains)
INFO - root - 2017-12-16 02:10:28.888019: step 88560, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 42h:12m:10s remains)
INFO - root - 2017-12-16 02:10:35.266318: step 88570, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.624 sec/batch; 42h:17m:21s remains)
INFO - root - 2017-12-16 02:10:41.734870: step 88580, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 43h:11m:50s remains)
INFO - root - 2017-12-16 02:10:48.131438: step 88590, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 44h:37m:15s remains)
INFO - root - 2017-12-16 02:10:54.458626: step 88600, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.618 sec/batch; 41h:53m:25s remains)
2017-12-16 02:10:54.966293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8877206 -4.0402455 -4.4070807 -5.151742 -6.0517273 -5.618834 -4.9901195 -5.2745976 -5.6114798 -7.4429188 -8.4675283 -8.9296331 -9.6739731 -10.174336 -10.208553][-3.9283543 -4.4326 -4.5687103 -4.4642649 -4.8567162 -4.612958 -4.3072138 -4.0471349 -4.7103791 -7.1076293 -8.3974 -8.967802 -10.056425 -10.102182 -9.9152079][-4.6267042 -4.2616634 -3.6856265 -3.2252111 -2.7879972 -1.9619451 -1.8370471 -2.1869259 -2.7048807 -5.4326258 -6.926116 -7.5405874 -8.7061615 -9.4454679 -9.6222982][-3.8547504 -3.1860347 -2.4159541 -1.7597537 -1.2602754 -0.53273773 0.032282829 0.33739376 -0.3815918 -3.1399541 -4.2978897 -5.571064 -6.8402586 -7.721736 -8.1249247][-3.8154716 -2.4518838 -0.98900175 -0.018995762 0.49024391 1.387146 1.7861853 1.4590597 1.2815094 -1.0361152 -2.581286 -3.4164443 -4.9446573 -6.0470028 -6.8216448][-2.9893045 -2.2740736 -1.1731625 0.065891266 1.1448994 2.1093454 2.4842377 2.3187771 1.7651281 -0.70312405 -1.4949951 -2.5449347 -4.4920731 -5.4608874 -6.5425534][-2.7197332 -1.8902059 -0.37681341 0.082006931 1.1934404 2.2771606 2.53063 2.353035 2.3914824 0.19621706 -0.93066168 -2.0025158 -3.6090021 -4.7036777 -5.7323213][-2.918087 -2.27316 -1.3648453 -0.44130135 0.65063858 1.3512907 1.840126 2.5242968 2.7712517 0.37129307 -0.60985136 -2.0581965 -4.0358424 -5.2741346 -6.274518][-3.1973758 -2.4985704 -1.665082 -0.88160563 -0.023264885 0.75831985 0.82309341 1.0069313 1.4374428 -0.70944023 -1.8227129 -3.0579386 -4.7793427 -5.9615273 -7.0483131][-3.278161 -2.7443056 -2.1311278 -1.0550871 -0.36321068 0.28539705 0.20501471 -0.027088165 0.031671524 -2.1855235 -2.8965569 -3.9527318 -5.4034209 -6.3167887 -7.3242574][-4.7909145 -4.2078457 -3.0497837 -2.4770088 -2.1252136 -1.7521677 -1.9004006 -2.2361064 -2.0004959 -3.9884369 -4.9523129 -4.9548821 -5.6511059 -6.5403872 -7.1360173][-6.618618 -5.49319 -4.9960737 -4.129879 -2.9186249 -2.5189285 -3.0147533 -3.2007613 -3.0600595 -4.9021168 -5.5363455 -5.3224063 -5.9584923 -6.2550449 -6.4950571][-5.8884397 -5.685009 -4.8210526 -4.1435528 -3.7923634 -3.4937987 -3.6667061 -3.9609675 -4.1572084 -5.1785965 -6.1253071 -6.0832934 -6.2514734 -5.8752761 -5.87344][-5.9794307 -5.0681829 -4.47103 -4.0057487 -3.6351862 -3.7010984 -4.1473842 -4.1939545 -3.8793204 -4.7507915 -5.316566 -5.7389441 -6.1634254 -5.9745789 -5.9029441][-6.7039685 -6.1172009 -5.1544418 -4.9362097 -4.9684505 -4.5047922 -4.5863123 -4.8115711 -5.0359039 -4.8931613 -4.8732648 -5.2292094 -5.532722 -5.47414 -5.5260649]]...]
INFO - root - 2017-12-16 02:11:01.380045: step 88610, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 42h:10m:16s remains)
INFO - root - 2017-12-16 02:11:07.907995: step 88620, loss = 0.27, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 45h:04m:15s remains)
INFO - root - 2017-12-16 02:11:14.261804: step 88630, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 43h:18m:30s remains)
INFO - root - 2017-12-16 02:11:20.625042: step 88640, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 42h:23m:29s remains)
INFO - root - 2017-12-16 02:11:27.017459: step 88650, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 42h:48m:20s remains)
INFO - root - 2017-12-16 02:11:33.386134: step 88660, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:16m:57s remains)
INFO - root - 2017-12-16 02:11:39.837981: step 88670, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:34m:46s remains)
INFO - root - 2017-12-16 02:11:46.216342: step 88680, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 43h:10m:16s remains)
INFO - root - 2017-12-16 02:11:52.614793: step 88690, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:41m:10s remains)
INFO - root - 2017-12-16 02:11:59.128837: step 88700, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 45h:18m:20s remains)
2017-12-16 02:11:59.669015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7025356 -4.1440811 -3.5431805 -2.9593935 -2.6291628 -2.3579755 -1.7274728 -1.8582711 -0.61655617 -1.0127602 -1.3857431 -3.3655429 -5.3296852 -6.5397892 -7.6357412][-4.2647572 -3.5064502 -3.2853475 -3.3114128 -3.6243858 -3.6455436 -3.4311695 -3.0357823 -2.1275134 -2.23201 -2.3358393 -3.669982 -4.915215 -5.94878 -6.9684014][-2.8231969 -1.7323389 -1.7090411 -2.2774582 -3.0884881 -3.1596632 -2.9510384 -2.5714073 -1.8172421 -2.4325457 -2.3319349 -3.615675 -5.0543971 -5.4457884 -6.1191487][-2.3782148 -1.5036187 -1.1406932 -1.2353349 -1.8250737 -2.1534185 -2.0353708 -1.3764715 -0.51929665 -1.2672195 -1.6993637 -3.4917421 -4.6272926 -5.1472282 -5.72193][-2.1749878 -1.4686017 -1.2163696 -0.83638668 -0.7718873 -0.36614895 -0.071131229 0.52532959 1.4195967 0.13691425 -0.48559237 -2.7035327 -4.3195977 -5.4620543 -6.4075322][-1.31074 -0.75449896 -0.88972569 -0.59690857 -0.17948723 0.739542 1.5327873 2.2129412 3.0316048 1.7790194 0.7658205 -1.6992216 -3.4895277 -4.6121893 -5.2888503][-1.8022571 -0.89591026 -0.55850983 -0.10740948 0.22096443 1.088232 2.2238789 3.0949726 3.6573133 2.2110987 1.2368765 -1.5273728 -3.6292768 -4.9237337 -6.0538111][-2.5078707 -1.6345038 -0.79723883 0.4332552 1.266737 2.0920076 2.7642994 3.2754917 3.6821051 1.9298887 0.77190495 -1.7351112 -3.5254192 -4.8746548 -5.7060714][-1.8991718 -1.0031605 -0.73376369 0.19981384 0.99441147 2.0209141 2.8461361 2.8209429 2.9038038 1.2634621 0.4250288 -2.4049892 -3.9118166 -4.7518563 -5.6744862][-2.0380573 -1.0037413 -0.34088516 0.30033016 0.882247 1.4224701 1.9265318 1.7750406 1.7309256 -0.13366938 -0.95811796 -2.8357558 -3.8007231 -4.4863577 -4.6996536][-2.8784981 -2.2623262 -1.4489484 -0.53861332 -0.04437685 0.5458765 0.96102619 0.75102234 0.5632515 -1.1327295 -1.9276066 -3.693639 -4.1794472 -4.0162745 -3.926868][-3.7158892 -2.8777871 -2.5931029 -2.1390657 -1.5361705 -1.1338844 -0.74747038 -1.083075 -1.1529822 -1.9599004 -2.5722723 -3.6208119 -3.9963179 -3.7852705 -3.5770864][-4.4567 -3.9441147 -3.5660872 -2.973804 -2.15127 -1.6545839 -1.6495037 -1.8459873 -2.0036211 -2.6784663 -2.7413793 -4.0158296 -4.6538796 -4.101048 -4.2427955][-4.633966 -4.0876393 -3.9035392 -3.7153563 -3.3075175 -2.8646221 -3.0924773 -3.2398796 -3.4358497 -4.2097163 -4.0497446 -4.1672888 -4.4993525 -3.8812695 -4.1079512][-6.133605 -5.8451576 -5.4251604 -5.1982985 -5.2736807 -5.1797156 -5.6621037 -5.7915621 -6.1893821 -6.3794174 -6.1725421 -5.9291415 -5.5256395 -5.3464522 -5.0678396]]...]
INFO - root - 2017-12-16 02:12:06.071920: step 88710, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 43h:17m:03s remains)
INFO - root - 2017-12-16 02:12:12.509593: step 88720, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 44h:09m:49s remains)
INFO - root - 2017-12-16 02:12:18.985503: step 88730, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 44h:16m:27s remains)
INFO - root - 2017-12-16 02:12:25.409983: step 88740, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 43h:25m:02s remains)
INFO - root - 2017-12-16 02:12:31.875352: step 88750, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 43h:08m:42s remains)
INFO - root - 2017-12-16 02:12:38.373774: step 88760, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 44h:10m:02s remains)
INFO - root - 2017-12-16 02:12:44.856411: step 88770, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 44h:50m:27s remains)
INFO - root - 2017-12-16 02:12:51.302209: step 88780, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 44h:16m:03s remains)
INFO - root - 2017-12-16 02:12:57.678893: step 88790, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.625 sec/batch; 42h:17m:08s remains)
INFO - root - 2017-12-16 02:13:04.201586: step 88800, loss = 0.28, batch loss = 0.16 (11.6 examples/sec; 0.688 sec/batch; 46h:33m:21s remains)
2017-12-16 02:13:04.879413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4531965 -2.1400456 -2.3867092 -2.1373363 -1.8933377 -1.8609257 -2.1053624 -2.1684737 -2.2997427 -3.0861382 -3.8826406 -4.9287672 -5.9447908 -7.11839 -8.2879419][-1.9838748 -1.9321976 -2.3284245 -2.2185011 -2.3728185 -2.5412989 -2.7518358 -2.9205761 -2.7863097 -2.9610167 -3.4257941 -4.5005574 -5.4262714 -6.366168 -7.221015][-1.9165902 -1.2909999 -1.2256427 -1.2479577 -1.421072 -1.5070381 -1.793498 -1.9658885 -1.8224669 -2.3844581 -2.9541063 -4.2485709 -4.98812 -5.6165619 -6.356905][-0.91894054 0.19319153 0.64166832 0.30033207 -0.21059561 -0.29062891 -0.31071758 -0.62327957 -1.1511779 -2.0801501 -2.5816507 -3.4956126 -3.9040334 -4.6347151 -5.2796345][-0.63495684 0.39306641 1.0103416 1.0218954 0.84965897 1.0192471 1.1157646 0.5411396 0.00076436996 -0.90583563 -1.3694901 -2.1170926 -2.7879767 -3.9912634 -4.9798594][-0.71064425 0.0066876411 0.46545124 0.7264595 0.96591759 1.6771536 2.3575611 2.0592041 1.7549019 0.81925678 0.12803555 -1.1385994 -2.2986698 -3.7897229 -4.9626532][-1.7118359 -0.71258783 -0.14356422 0.48059368 1.0760775 1.5968466 2.1206665 2.1821785 2.3293304 1.1902494 0.30660486 -0.92546272 -2.1335468 -3.5687571 -4.6743021][-2.3048711 -1.2742219 -0.56375837 0.12769079 0.67177486 1.2028017 1.9277601 2.0926809 2.3791466 1.5383291 0.93770409 -0.29079008 -1.6815567 -2.9970183 -4.2771473][-3.1062584 -2.1091189 -1.024756 -0.54502869 -0.37840271 0.24898052 1.2392159 1.8208551 2.3844604 1.2984924 0.51470566 -0.65777588 -2.0255303 -3.5817099 -4.8723655][-3.7754807 -3.2796431 -2.6120095 -1.9086676 -1.5049567 -1.0192442 -0.25838184 0.21695614 0.76196384 0.11865759 -0.56016779 -1.7293162 -3.0933747 -4.6981287 -5.8909054][-5.2892804 -5.0687084 -4.5467319 -3.8096752 -3.1063313 -2.5847397 -2.060822 -1.7387443 -1.5274248 -2.3153172 -3.067749 -3.7547655 -4.5915403 -5.9438429 -6.947268][-7.3701572 -7.0269556 -6.6796989 -6.028017 -5.4584589 -5.0261726 -4.587801 -4.37243 -4.1808057 -4.3801136 -5.0180941 -5.2752171 -5.8182282 -6.5900459 -7.3387909][-7.7360888 -7.3762965 -7.2258368 -6.936614 -6.5098667 -5.9448614 -5.6023779 -5.526258 -5.488018 -5.3962946 -5.474309 -5.8131533 -6.20326 -6.6623468 -7.0893164][-7.4863667 -7.3848252 -7.3839831 -7.2152448 -7.1656141 -6.4852839 -5.8154764 -5.7714806 -5.7038612 -5.6507888 -5.9631228 -6.0814581 -6.0935707 -6.1839309 -6.4427137][-6.9216337 -6.9023867 -6.8106089 -6.7738791 -6.7030225 -6.4813437 -5.9306931 -5.820755 -5.7602711 -5.660748 -5.578568 -5.6015244 -5.6208839 -5.8407474 -5.9133372]]...]
INFO - root - 2017-12-16 02:13:11.361327: step 88810, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.640 sec/batch; 43h:19m:01s remains)
INFO - root - 2017-12-16 02:13:17.787999: step 88820, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 43h:03m:19s remains)
INFO - root - 2017-12-16 02:13:24.324630: step 88830, loss = 0.25, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:33m:07s remains)
INFO - root - 2017-12-16 02:13:30.705440: step 88840, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 42h:49m:15s remains)
INFO - root - 2017-12-16 02:13:37.121439: step 88850, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.643 sec/batch; 43h:29m:18s remains)
INFO - root - 2017-12-16 02:13:43.545044: step 88860, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 43h:15m:23s remains)
INFO - root - 2017-12-16 02:13:49.954591: step 88870, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 44h:04m:10s remains)
INFO - root - 2017-12-16 02:13:56.395033: step 88880, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 44h:51m:24s remains)
INFO - root - 2017-12-16 02:14:02.778299: step 88890, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 42h:48m:54s remains)
INFO - root - 2017-12-16 02:14:09.176517: step 88900, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.629 sec/batch; 42h:33m:42s remains)
2017-12-16 02:14:09.796494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9192886 -5.2136354 -5.5136557 -5.3072853 -5.2724857 -5.1201749 -4.841445 -4.3847313 -4.0515242 -4.7933555 -5.2737322 -5.0637574 -6.2345586 -7.2094007 -7.2963047][-3.6827717 -4.4675527 -5.3107529 -5.8355231 -5.805366 -5.7723622 -5.6145611 -5.4282789 -4.3847008 -4.7131195 -5.3573112 -5.1929078 -6.1479712 -6.7440276 -6.79539][-2.8080802 -3.1494093 -3.7186112 -5.1838841 -5.4712162 -5.0345764 -4.9357724 -4.76805 -3.93188 -4.7572021 -5.2015276 -4.6008258 -5.6805964 -6.5894594 -6.5094595][-2.678494 -2.4319043 -3.2578158 -3.799684 -3.6480865 -3.370441 -3.2128963 -3.2204981 -3.0135593 -4.0855732 -4.613656 -4.352201 -5.4574614 -6.1043644 -6.220871][-3.0434656 -2.6105714 -2.7403717 -2.7553711 -2.1701417 -1.2936716 -0.53782415 -0.26631355 0.19893456 -1.4238729 -2.7245097 -2.6597781 -4.3691969 -5.6598396 -5.9527812][-3.3810215 -3.118845 -2.720746 -2.1837354 -0.7061367 0.73065376 1.6952934 2.349225 2.5743742 0.74335 -0.76034832 -1.4800014 -4.1022186 -5.8880482 -6.4345579][-4.6205869 -3.4463248 -2.2464843 -1.6122394 -0.27196074 1.0924969 2.7668734 3.6034822 3.9260778 2.1004524 0.25251389 -0.78563786 -3.4248152 -5.4905443 -6.1281705][-4.3035517 -3.4028401 -2.8355536 -1.5142117 0.65453053 2.0278215 2.9584455 3.0367994 3.324687 1.8685265 0.65902519 -0.074913979 -2.7695136 -4.9418297 -5.8299656][-4.600708 -4.2969608 -3.4705672 -2.0362296 -0.72269678 1.1389503 2.689805 2.7320776 3.0551615 0.72295094 -1.1015778 -1.9172244 -4.2154703 -5.8948545 -6.2321906][-5.4414282 -4.24308 -3.4909754 -3.1303253 -1.8538594 -0.54625559 -0.084775925 0.44679356 0.77778149 -1.3570433 -2.3735037 -3.3029556 -5.4524965 -6.741766 -6.9396086][-5.9080267 -5.6456423 -5.067378 -4.1224542 -3.1898351 -2.22822 -1.7304802 -1.7491875 -1.9482794 -3.3406396 -4.3352656 -5.1948929 -6.4941525 -7.498661 -7.5539217][-7.7892318 -6.710886 -6.3736916 -5.9188385 -5.0402203 -4.2246828 -4.00142 -4.1575527 -3.9040434 -5.0123024 -5.9840736 -6.6318755 -7.2659969 -7.7921915 -7.5933933][-8.3849792 -7.7799835 -7.3076043 -6.8282285 -6.2059412 -5.4242477 -5.2108307 -5.4769897 -5.4417043 -5.8767242 -6.5271263 -6.6590824 -7.0482111 -7.3274164 -7.1478853][-8.8184843 -7.9813838 -7.2947369 -6.9780922 -6.2881866 -5.7502723 -5.7996106 -5.8256192 -5.7099819 -6.1879458 -6.4965172 -6.4925065 -6.9106317 -6.8285718 -6.4682608][-7.182045 -7.291811 -7.1898103 -6.6141672 -6.01126 -5.9275112 -5.7184696 -5.7019792 -5.656384 -5.69171 -5.8102374 -5.9053063 -5.9524164 -5.8301735 -5.7807546]]...]
INFO - root - 2017-12-16 02:14:16.160690: step 88910, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:58m:58s remains)
INFO - root - 2017-12-16 02:14:22.537639: step 88920, loss = 0.33, batch loss = 0.21 (12.0 examples/sec; 0.664 sec/batch; 44h:56m:49s remains)
INFO - root - 2017-12-16 02:14:29.049601: step 88930, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.675 sec/batch; 45h:38m:17s remains)
INFO - root - 2017-12-16 02:14:35.587256: step 88940, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 43h:14m:28s remains)
INFO - root - 2017-12-16 02:14:41.954721: step 88950, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 43h:13m:05s remains)
INFO - root - 2017-12-16 02:14:48.495462: step 88960, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 44h:43m:47s remains)
INFO - root - 2017-12-16 02:14:55.055109: step 88970, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 43h:43m:25s remains)
INFO - root - 2017-12-16 02:15:01.613086: step 88980, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.640 sec/batch; 43h:18m:51s remains)
INFO - root - 2017-12-16 02:15:08.005745: step 88990, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 45h:04m:27s remains)
INFO - root - 2017-12-16 02:15:14.447007: step 89000, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 42h:44m:46s remains)
2017-12-16 02:15:14.990003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1559734 -5.9878564 -6.6327577 -7.5584674 -8.251133 -8.2288361 -8.1431417 -7.5950289 -6.6747384 -6.49279 -7.38988 -7.8478203 -8.1765614 -8.85217 -9.26433][-5.5405631 -5.8706856 -6.35343 -7.0680447 -7.8029218 -8.1828213 -8.3975868 -7.8039174 -6.9949741 -6.5956059 -7.0297856 -7.59379 -7.923872 -8.23466 -8.7514725][-5.616087 -5.0853252 -5.1175108 -5.4778347 -6.3697338 -6.6962786 -6.5874748 -6.3529429 -5.9365478 -5.6957726 -6.4759831 -7.1979461 -7.1812983 -7.4460454 -7.7814984][-6.1699834 -5.2646103 -4.5857964 -3.9621167 -4.0041361 -3.9209411 -3.7361126 -3.7301395 -3.3859801 -3.7322762 -4.8413858 -5.8144226 -6.2741184 -6.7739687 -7.1937513][-6.3718877 -5.0817242 -4.1612411 -3.3721108 -2.6279325 -1.8341599 -0.90403414 -0.75850773 -0.80018139 -1.6893611 -2.9655905 -4.4124537 -5.3471613 -6.173718 -6.6509442][-6.530736 -5.2880135 -4.04514 -2.5737367 -1.6149139 -0.2025938 1.298542 1.5392399 1.7670317 0.70960617 -1.1340661 -3.1174054 -4.264327 -5.0687923 -5.7817583][-6.0765872 -4.77837 -3.4087296 -2.0540485 -0.53368092 0.92767715 2.0381193 2.4406281 2.5963259 1.7107458 -0.068362236 -2.3173652 -3.7537391 -4.9215364 -5.6162858][-5.3451462 -3.9844415 -3.1938767 -1.6755838 -0.090745449 1.1981106 2.5671806 2.7041321 2.626009 1.773386 -0.32522011 -2.3794084 -3.4678702 -4.6645265 -5.6750517][-4.9906836 -3.8928232 -2.9694548 -1.7463636 -0.67129326 0.7233057 2.0045681 1.9699326 1.9752932 0.80522728 -1.4271345 -3.3055177 -4.2708015 -5.2411714 -5.7838926][-5.32919 -4.6500258 -3.7029576 -2.6021452 -1.565825 -0.33793354 0.57475662 0.6012888 0.67489719 -0.38057613 -2.2884264 -3.5944047 -4.7097869 -5.9264531 -6.7995086][-5.903141 -5.5501089 -5.0775375 -3.9913442 -2.9040089 -1.7849493 -1.0932832 -0.8430934 -0.77179766 -1.0179396 -2.4500942 -3.3940029 -4.3723016 -5.4182425 -6.3342524][-5.2750587 -5.1075268 -4.6208334 -3.7285051 -2.6396518 -1.9127636 -1.2900372 -1.4481025 -1.3381176 -1.5877743 -2.5405331 -2.9602275 -3.8774278 -4.9183969 -5.4382033][-5.6401353 -5.7194819 -5.2983403 -4.4393282 -3.642312 -2.9484048 -2.1636629 -2.1150031 -2.1604061 -2.2522383 -2.9178758 -3.1035051 -3.7119 -4.6256695 -4.883316][-4.9125347 -5.2724252 -5.2452831 -4.5087438 -4.0673084 -3.4070225 -2.641448 -2.6290417 -2.7319117 -2.81328 -3.7802217 -3.398314 -3.6041346 -4.0451622 -4.3816767][-5.2606664 -5.4957151 -5.8244276 -6.0646338 -5.946959 -5.3403211 -4.7293615 -4.5383477 -4.2629695 -4.1499729 -4.2779865 -4.3953943 -4.649797 -4.3900485 -4.0618305]]...]
INFO - root - 2017-12-16 02:15:21.617062: step 89010, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 45h:10m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:15:28.058623: step 89020, loss = 0.28, batch loss = 0.16 (12.8 examples/sec; 0.626 sec/batch; 42h:19m:18s remains)
INFO - root - 2017-12-16 02:15:34.490618: step 89030, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 42h:57m:45s remains)
INFO - root - 2017-12-16 02:15:40.867235: step 89040, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 43h:04m:07s remains)
INFO - root - 2017-12-16 02:15:47.323332: step 89050, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.653 sec/batch; 44h:08m:15s remains)
INFO - root - 2017-12-16 02:15:53.783610: step 89060, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 43h:23m:45s remains)
INFO - root - 2017-12-16 02:16:00.236981: step 89070, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 44h:45m:11s remains)
INFO - root - 2017-12-16 02:16:06.714219: step 89080, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 43h:31m:04s remains)
INFO - root - 2017-12-16 02:16:13.215104: step 89090, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:29m:58s remains)
INFO - root - 2017-12-16 02:16:19.708208: step 89100, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 43h:22m:24s remains)
2017-12-16 02:16:20.220842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8624539 -7.2318516 -7.2675781 -7.1346383 -6.4585843 -5.650351 -4.7335625 -4.0389614 -3.4218302 -3.1926446 -3.7845771 -4.7692842 -5.7738724 -7.1704855 -7.3796844][-6.0622911 -6.7656326 -6.96265 -6.9903817 -7.35141 -7.1586857 -6.3287621 -5.1529579 -3.6881762 -2.9442205 -3.6703172 -4.658618 -5.9261289 -7.6150217 -8.09805][-3.4058723 -3.7041578 -4.6148586 -5.6517105 -6.2675996 -6.5351238 -5.7200718 -4.1944604 -3.0666738 -2.7619805 -3.3920693 -4.534 -6.1076765 -8.136508 -8.6946707][-0.96213961 -0.86663771 -2.1874795 -3.2314115 -3.8726845 -4.1928654 -3.9318316 -2.9655008 -1.3201056 -1.1118855 -2.0117283 -3.5759521 -5.4110827 -7.56639 -8.4778471][-0.17187881 0.377553 0.31791687 -0.39827633 -1.0974751 -1.0791063 -0.48222876 0.26877975 1.2439623 0.98857975 0.029162407 -2.3768377 -4.7473521 -6.7771335 -7.8083391][0.040188789 0.93709087 1.4744368 1.303484 1.0149193 1.8301983 2.6589117 2.9634991 3.0930204 1.7997351 -0.18346977 -2.5953116 -4.4543204 -6.6340284 -7.5357442][-1.5897145 -1.2340603 -0.11900806 0.95081615 1.7234745 2.7261696 3.9589548 4.6829214 4.7077513 2.6839209 -0.59557724 -3.7168932 -5.7369833 -7.77087 -8.1572609][-2.8361602 -2.9172859 -1.4562392 0.36830807 1.3980074 2.5585232 3.6781301 4.4838486 5.1285896 3.2332163 -0.43296003 -3.6819029 -5.81051 -7.5137024 -7.6583037][-4.7685213 -3.9767079 -3.5209064 -2.3491759 -0.47791958 1.3605814 2.0527906 2.6067457 2.9275875 1.2286348 -1.3265905 -4.1334023 -5.8956361 -7.5610437 -7.4748144][-6.332562 -5.7004013 -5.1226521 -4.6793594 -3.2768292 -1.5100908 -0.56032991 0.21282578 -0.072640419 -1.4850535 -3.7935493 -6.2214212 -7.3018656 -7.9764118 -8.3203459][-9.3371735 -8.43922 -7.9570961 -7.3307691 -6.3846927 -5.6636944 -4.5267344 -3.945797 -3.8929219 -4.631424 -6.8055282 -7.4998784 -8.0338945 -9.1192284 -9.2753124][-9.8788185 -9.520998 -9.11877 -8.8675938 -8.339447 -7.7187347 -7.2639093 -6.77902 -6.16067 -6.3410988 -7.8200927 -8.3345213 -8.4722538 -8.3086033 -8.0716057][-9.9744358 -9.9853926 -9.9018507 -9.7888622 -9.1836958 -8.1831331 -7.6970744 -7.7154613 -7.87749 -7.4620624 -8.6097288 -9.2404 -8.910944 -7.950314 -7.1669846][-9.7842484 -8.96639 -8.2488756 -8.25387 -7.9138894 -7.3952684 -6.9682322 -6.8691754 -6.8193941 -7.2733021 -7.992022 -7.8986521 -7.8490143 -7.9740086 -7.2763238][-10.056589 -9.4820061 -8.5683508 -7.7848163 -7.6301675 -7.1040554 -6.7943368 -7.0727215 -6.9343448 -6.9886942 -6.785264 -6.8955536 -7.149889 -7.3152986 -7.0275803]]...]
INFO - root - 2017-12-16 02:16:26.718025: step 89110, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 43h:50m:53s remains)
INFO - root - 2017-12-16 02:16:33.047924: step 89120, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 43h:50m:50s remains)
INFO - root - 2017-12-16 02:16:39.414071: step 89130, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.632 sec/batch; 42h:41m:59s remains)
INFO - root - 2017-12-16 02:16:45.876961: step 89140, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 42h:43m:53s remains)
INFO - root - 2017-12-16 02:16:52.307978: step 89150, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 43h:46m:05s remains)
INFO - root - 2017-12-16 02:16:58.706695: step 89160, loss = 0.32, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 42h:49m:44s remains)
INFO - root - 2017-12-16 02:17:05.173888: step 89170, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 42h:24m:23s remains)
INFO - root - 2017-12-16 02:17:11.593359: step 89180, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:35m:36s remains)
INFO - root - 2017-12-16 02:17:18.062530: step 89190, loss = 0.35, batch loss = 0.24 (12.6 examples/sec; 0.634 sec/batch; 42h:52m:31s remains)
INFO - root - 2017-12-16 02:17:24.522670: step 89200, loss = 0.28, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 45h:20m:19s remains)
2017-12-16 02:17:25.109976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0297842 -6.651082 -7.2595806 -6.3955679 -4.9220786 -3.9641337 -2.7533526 -2.328001 -1.2293863 -1.9360414 -2.4734182 -4.051199 -5.8404131 -5.8194203 -6.2714968][-6.6331329 -7.0476217 -7.1177731 -6.5568895 -5.461134 -3.6678128 -2.2573271 -1.5465069 -1.115418 -2.3093443 -3.1332197 -4.8674436 -6.7516685 -6.4949389 -7.1988811][-6.7148962 -6.8413754 -7.3788729 -6.6357193 -5.8246708 -4.8908215 -3.0950918 -1.8024039 -0.59123468 -1.8653312 -2.9632196 -5.4872274 -7.3910546 -7.0785618 -7.6023855][-6.3399973 -6.8590407 -6.6579924 -6.3814979 -5.2918696 -3.9624028 -2.6358261 -1.8931708 -1.3387656 -2.0896707 -3.0177488 -5.6435366 -7.5425792 -7.4774179 -7.7696013][-6.2702055 -7.0552964 -6.6559153 -5.7930961 -4.7997007 -3.8381906 -2.6140962 -1.3681397 -1.1860476 -2.9724708 -4.2827735 -6.4697962 -8.3056135 -8.21079 -8.4612017][-6.9282322 -7.2050419 -6.9033756 -5.6473646 -3.6455083 -2.1700392 -0.72838736 -0.28983831 0.29811239 -1.3092189 -3.4912176 -6.5147724 -8.7995386 -8.975256 -9.2138472][-6.0927219 -5.9358673 -5.6190662 -4.5249438 -2.5758319 -1.2826824 0.010839939 0.46021461 0.79202461 -0.25659132 -1.9739776 -4.9746861 -7.8571239 -8.4730034 -8.9380951][-4.4288416 -4.2483015 -3.3665028 -2.1630478 -0.25781536 0.98499966 1.3216238 1.2872591 2.0284882 0.43311977 -1.5709085 -4.5913734 -7.4818754 -8.275773 -9.0237484][-2.7849708 -2.4181952 -1.8962803 -0.78444815 0.48401165 1.4470911 1.7634096 1.7020674 2.2855797 0.68552876 -0.70500088 -3.7525384 -7.3102241 -7.7411017 -9.1188307][-2.242929 -0.85853624 -0.29374027 0.134336 0.92647362 1.7763195 1.9644794 2.0300159 1.8761539 -0.021604538 -1.6181378 -3.8032842 -6.1708488 -7.043159 -7.89482][-0.80085325 -0.52075768 -0.30630922 -0.023277283 -0.18269014 0.2541213 0.75720024 1.5643368 0.63626957 -1.3402233 -3.4558992 -4.8965187 -6.9351625 -7.0653048 -7.0442276][-1.6389942 -1.6535888 -1.395535 -1.2403889 -0.74690151 -0.41888332 0.11375856 0.23199558 -0.039671898 -1.6791425 -3.3873672 -4.9172292 -7.0838013 -6.7485294 -7.11901][-3.1018367 -2.9491973 -3.3719745 -2.9206934 -2.7567854 -2.3741417 -1.3004775 -1.3076715 -0.71296883 -2.791697 -2.930378 -4.0960841 -6.2596822 -6.6273742 -7.2443013][-3.9500155 -3.8609829 -4.0865974 -4.2543068 -4.2496762 -4.25387 -3.6389832 -3.4050379 -3.1648555 -3.9273047 -2.844461 -3.3425436 -4.3579988 -5.0247059 -5.8638535][-5.4391603 -5.0949254 -4.7371397 -5.0755181 -4.8130131 -5.0578623 -5.3717637 -4.7449784 -4.6965466 -4.6242638 -4.5003748 -4.2506676 -4.0795794 -4.6659031 -5.5370169]]...]
INFO - root - 2017-12-16 02:17:31.522588: step 89210, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 42h:54m:22s remains)
INFO - root - 2017-12-16 02:17:37.905713: step 89220, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 43h:56m:56s remains)
INFO - root - 2017-12-16 02:17:44.307845: step 89230, loss = 0.31, batch loss = 0.20 (12.3 examples/sec; 0.650 sec/batch; 43h:53m:37s remains)
INFO - root - 2017-12-16 02:17:50.821623: step 89240, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 44h:41m:46s remains)
INFO - root - 2017-12-16 02:17:57.268340: step 89250, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 42h:15m:17s remains)
INFO - root - 2017-12-16 02:18:03.702974: step 89260, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:33m:07s remains)
INFO - root - 2017-12-16 02:18:10.146203: step 89270, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 44h:06m:08s remains)
INFO - root - 2017-12-16 02:18:16.591643: step 89280, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 42h:54m:36s remains)
INFO - root - 2017-12-16 02:18:22.996167: step 89290, loss = 0.24, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 43h:04m:45s remains)
INFO - root - 2017-12-16 02:18:29.380705: step 89300, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 43h:11m:57s remains)
2017-12-16 02:18:29.911643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2495208 -2.6894531 -2.2395973 -2.3295407 -3.0674648 -3.5724888 -3.6341834 -3.2714357 -3.0299544 -3.9510546 -4.6965895 -4.8048363 -6.1730881 -6.9330964 -7.2537222][-2.6281991 -2.569293 -2.3044062 -2.2514172 -2.4904284 -2.87176 -3.1273232 -3.1456132 -2.8935761 -3.767185 -4.3068266 -4.2407284 -5.7227092 -6.4286022 -6.5814986][-3.0291219 -2.796001 -2.5256424 -2.5872283 -2.5072646 -2.5043492 -2.4629607 -2.6633291 -2.8432651 -3.7475586 -4.3351717 -4.4706607 -6.0188932 -6.5719714 -6.7493062][-3.9501486 -3.6950347 -3.1492109 -2.5867653 -1.98985 -1.9406271 -1.8065896 -1.8408532 -1.7102695 -2.8449988 -3.965426 -4.309382 -6.0233479 -6.9216032 -7.2581692][-4.8166113 -3.9599798 -2.6411767 -2.08678 -1.703476 -0.98106575 -0.41358423 -0.44793224 -0.20734167 -1.5677361 -2.8557792 -3.6922774 -5.7355337 -6.5943537 -6.8813457][-5.584259 -4.5517015 -3.5329142 -2.7746606 -1.1846776 -0.21462297 0.34615421 0.78152084 0.95593643 -0.6199913 -2.093698 -3.0282402 -5.2294908 -6.54449 -7.1411462][-5.9441066 -5.2015586 -3.7521474 -2.4616823 -1.1120319 0.32495689 1.5097923 1.7650557 1.7796125 -0.031695366 -1.7018828 -2.585649 -4.8638239 -6.204669 -6.7593174][-5.7888565 -4.8102551 -4.0548372 -2.8131466 -0.71234131 0.78120232 1.7301369 2.1264038 2.3926649 0.38987064 -1.203372 -2.0497632 -4.5942669 -6.0467863 -6.5347328][-6.0679989 -5.2481589 -3.9243867 -2.1522822 -1.0542312 0.1075654 1.2206593 1.4595375 1.378231 -0.13703489 -1.4573016 -2.4164162 -5.0704522 -6.1120415 -6.4387875][-7.1103377 -6.2035561 -4.9862151 -3.4998899 -2.250421 -0.9514122 -0.092442989 0.064247131 0.183599 -1.6400185 -2.8824773 -3.7319458 -5.6352434 -6.1834526 -6.4548006][-7.7311478 -7.5168905 -6.6750569 -5.3015108 -4.1864576 -3.2720079 -2.5227418 -2.3823328 -2.3849134 -4.4150496 -5.5102196 -5.9240942 -7.0737114 -7.7288074 -7.5762887][-9.09182 -7.9676552 -6.8568158 -6.4554415 -6.0498886 -5.4164753 -4.8072929 -4.7480149 -4.7899547 -6.3920507 -7.0864649 -6.8889112 -7.630713 -7.826365 -7.5397482][-9.4180727 -8.6960478 -8.1290674 -7.338191 -6.4239812 -5.9121189 -5.8017368 -6.0872025 -6.0141993 -7.0213866 -7.266099 -6.5572972 -6.6177564 -6.5840249 -6.2391586][-8.6779766 -8.3951654 -8.39449 -7.9165034 -7.4094458 -6.9737072 -6.9326754 -7.233305 -7.0028553 -7.3205891 -7.1458511 -6.6490498 -6.8740726 -6.4435987 -5.95084][-8.8926735 -8.0230618 -7.5736141 -7.67046 -8.02681 -7.4666276 -7.6513186 -7.9571209 -8.1784658 -8.0914755 -7.4200964 -7.0656738 -6.6756544 -5.9643273 -5.6924591]]...]
INFO - root - 2017-12-16 02:18:36.316919: step 89310, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 43h:58m:22s remains)
INFO - root - 2017-12-16 02:18:42.761820: step 89320, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 43h:02m:23s remains)
INFO - root - 2017-12-16 02:18:49.150791: step 89330, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 42h:19m:59s remains)
INFO - root - 2017-12-16 02:18:55.534494: step 89340, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 43h:22m:47s remains)
INFO - root - 2017-12-16 02:19:02.028757: step 89350, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 42h:56m:41s remains)
INFO - root - 2017-12-16 02:19:08.513729: step 89360, loss = 0.26, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:25m:42s remains)
INFO - root - 2017-12-16 02:19:14.839800: step 89370, loss = 0.30, batch loss = 0.19 (12.8 examples/sec; 0.625 sec/batch; 42h:12m:33s remains)
INFO - root - 2017-12-16 02:19:21.260394: step 89380, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 42h:34m:09s remains)
INFO - root - 2017-12-16 02:19:27.716134: step 89390, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 44h:18m:48s remains)
INFO - root - 2017-12-16 02:19:34.150236: step 89400, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 44h:03m:02s remains)
2017-12-16 02:19:34.664432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3902264 -6.1716638 -5.8010416 -5.68291 -5.4372587 -4.6994162 -3.8174856 -2.6516366 -0.99260521 -0.73689032 -2.3428292 -3.9853325 -4.8450384 -6.2296543 -6.6701422][-5.4396806 -5.7309971 -5.7294784 -5.3093395 -5.2092514 -5.03152 -4.7278857 -3.3369503 -1.8106666 -1.4403176 -2.3498788 -3.8710132 -4.1741071 -5.4770436 -6.4975567][-4.8916092 -5.3993115 -5.4180908 -5.5252895 -5.7794218 -5.0773907 -4.4941816 -3.8151085 -3.0083361 -2.6806951 -3.6134291 -4.8807912 -4.9447222 -5.3245058 -5.8321948][-4.645318 -4.988318 -4.6268473 -4.08192 -4.0537004 -3.6366587 -3.183075 -2.4758906 -1.8876572 -2.2515111 -4.31464 -5.6488609 -5.4575977 -6.0700517 -6.4830956][-4.6686668 -4.1616569 -3.4151239 -2.5067019 -1.8179884 -1.1158886 -0.27351665 0.020543098 -0.13004494 -0.69091225 -2.5581636 -4.8132381 -5.94063 -6.89599 -7.0308113][-3.0117087 -2.0269361 -1.3935404 -0.23205709 1.0673523 1.9181738 2.1737528 2.2566385 2.4796047 1.3859186 -0.96257782 -3.0521483 -4.1836224 -5.9894371 -7.2977347][-2.8890791 -1.9762182 -0.23661804 1.3088617 2.5718822 4.2199841 5.1347885 4.914999 4.0596323 2.393364 -0.329957 -2.8861785 -3.9682014 -5.4087934 -6.2778697][-2.125668 -2.0593348 -1.3876238 0.18721056 1.5650177 3.1654625 4.4068546 4.8762283 4.3675289 2.3066568 -0.84028769 -3.5900507 -4.7470789 -5.9001408 -6.5603695][-3.3172102 -2.7117705 -1.9903526 -0.66370487 0.45384693 1.7869053 2.5090208 2.73565 2.7939405 1.3019094 -1.7371588 -4.2225227 -5.0569296 -6.21697 -6.5851016][-4.8469577 -4.3091288 -3.3420382 -2.2391562 -1.5052891 0.11258125 1.0887871 0.92500782 0.43414116 -0.74892664 -2.3613868 -4.27792 -5.1586151 -5.9443946 -6.2379909][-6.8698812 -6.7138047 -6.0672474 -5.3684511 -4.9762869 -3.5003734 -2.2746339 -2.1786046 -2.3449745 -3.2730589 -4.09379 -4.9520316 -5.2353916 -6.2522497 -7.165266][-8.38895 -8.2904167 -7.6817584 -6.6067996 -5.8890338 -5.4272375 -4.8415709 -4.1247473 -3.9774594 -4.6624422 -5.0685587 -5.6350141 -5.7834883 -6.060008 -6.5943103][-8.4099808 -8.97022 -8.5707693 -7.5951343 -7.1189981 -6.4062119 -5.5035591 -5.29835 -5.3012581 -5.2562571 -5.544775 -5.5717497 -5.5570908 -5.8099437 -6.097158][-7.4209614 -7.94284 -8.1307192 -7.6044974 -7.312367 -6.9841266 -6.4367709 -6.2493076 -6.0145268 -5.8180037 -6.1392941 -6.1549082 -5.9129362 -5.6257925 -5.5478134][-6.53217 -6.6495333 -6.8229332 -6.9438167 -7.0324287 -6.9748378 -7.1506672 -7.403739 -7.2888656 -6.907093 -6.4311657 -6.1283751 -6.3209114 -6.3393464 -6.2056031]]...]
INFO - root - 2017-12-16 02:19:41.131144: step 89410, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 42h:55m:55s remains)
INFO - root - 2017-12-16 02:19:47.549452: step 89420, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 43h:28m:43s remains)
INFO - root - 2017-12-16 02:19:54.099920: step 89430, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 44h:11m:04s remains)
INFO - root - 2017-12-16 02:20:00.460939: step 89440, loss = 0.27, batch loss = 0.16 (13.1 examples/sec; 0.609 sec/batch; 41h:05m:22s remains)
INFO - root - 2017-12-16 02:20:06.876431: step 89450, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 42h:25m:10s remains)
INFO - root - 2017-12-16 02:20:13.318064: step 89460, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 43h:32m:47s remains)
INFO - root - 2017-12-16 02:20:19.677355: step 89470, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 43h:02m:22s remains)
INFO - root - 2017-12-16 02:20:26.071900: step 89480, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 42h:33m:38s remains)
INFO - root - 2017-12-16 02:20:32.541160: step 89490, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 43h:51m:46s remains)
INFO - root - 2017-12-16 02:20:38.996299: step 89500, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 43h:41m:49s remains)
2017-12-16 02:20:39.559267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6655111 -2.6880665 -2.9212346 -3.1049395 -3.193975 -3.2285323 -3.0498571 -2.7108068 -2.3109894 -3.3563118 -4.2221766 -5.1362896 -5.8172746 -6.3331275 -6.8341475][-2.9310656 -2.9035439 -2.9386573 -3.203701 -3.1790881 -2.8881044 -2.6592689 -2.5625606 -2.3293719 -3.3823094 -4.0765905 -5.2077894 -6.0296068 -6.846632 -7.2681909][-2.7786903 -2.6332293 -2.7346554 -2.8139291 -2.6203742 -2.208828 -1.7661529 -1.4875994 -1.3006949 -2.40094 -3.1370978 -4.1343579 -5.2074184 -6.0985842 -6.7457256][-2.8449368 -2.8162403 -2.6747155 -2.3893261 -1.7860756 -1.0392828 -0.45872259 -0.20523214 -0.072408676 -1.4018817 -2.4443769 -3.8019125 -4.9653945 -5.9241252 -6.500545][-3.2300582 -2.8879638 -2.3033605 -1.5926409 -0.67030573 0.13476133 0.80255604 0.97680569 1.0099344 -0.50392818 -1.8256631 -3.2788758 -4.5543356 -5.702909 -6.4563646][-4.2008057 -3.4849296 -2.5529394 -1.5702362 -0.50247335 0.64174652 1.473527 1.5563879 1.463233 -0.30054855 -1.7459455 -3.378202 -4.8359289 -5.834619 -6.4411726][-4.0139084 -3.5161309 -2.5882344 -1.1355934 0.16460419 1.2968912 2.0091391 2.0812588 1.9764566 -0.011454582 -1.7772059 -3.5639005 -4.9888258 -6.1775947 -6.8740025][-3.7701 -2.9642735 -2.0536227 -0.66185713 0.59756756 1.4545574 1.983079 2.0266657 1.8484964 -0.15853453 -1.9261885 -3.8318417 -5.308567 -6.4304962 -6.9825511][-3.9981279 -3.1578717 -2.3148041 -1.0623665 0.037511826 0.82098389 1.2041502 1.4287825 1.4287291 -0.64038897 -2.3935657 -4.2332077 -5.7946253 -7.043057 -7.5129519][-4.4842377 -3.7631528 -2.970973 -1.7535653 -0.76283407 -0.18873072 0.13416243 0.46127129 0.51735592 -1.5864053 -3.1427684 -4.9579906 -6.2651258 -7.5210886 -8.2018929][-5.4765234 -4.737627 -3.870857 -2.843668 -2.4071522 -2.0729022 -1.9355102 -1.774056 -1.6395459 -3.49617 -4.9330835 -6.3002958 -7.2272978 -8.1698084 -8.3510447][-6.0168347 -5.399251 -4.8471274 -4.044559 -3.514226 -3.38734 -3.4123826 -3.3357968 -3.297431 -4.4876938 -5.5667219 -6.6708908 -7.2173986 -8.0319548 -8.0670528][-6.3014827 -5.7769136 -5.2311773 -4.8108387 -4.4986219 -4.4366951 -4.5270596 -4.620297 -4.6581573 -5.445118 -6.2336359 -6.7642217 -7.2158532 -7.6309109 -7.39582][-5.9944482 -5.5548716 -5.2810526 -4.8253613 -4.5588918 -4.5650005 -4.7767096 -4.919323 -4.9397564 -5.61615 -5.909595 -5.9813628 -6.0745869 -6.4761844 -6.4089994][-7.2001295 -6.9294615 -6.3798928 -5.8704066 -5.8527327 -5.9932413 -6.223423 -6.4222708 -6.5785294 -6.5604792 -6.6177263 -6.6127009 -6.4429622 -6.3044534 -6.2021036]]...]
INFO - root - 2017-12-16 02:20:45.900263: step 89510, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 43h:04m:37s remains)
INFO - root - 2017-12-16 02:20:52.288917: step 89520, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:46m:21s remains)
INFO - root - 2017-12-16 02:20:58.718084: step 89530, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 44h:13m:19s remains)
INFO - root - 2017-12-16 02:21:05.056197: step 89540, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 42h:23m:28s remains)
INFO - root - 2017-12-16 02:21:11.458474: step 89550, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 43h:00m:55s remains)
INFO - root - 2017-12-16 02:21:17.846838: step 89560, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.613 sec/batch; 41h:23m:16s remains)
INFO - root - 2017-12-16 02:21:24.316522: step 89570, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.618 sec/batch; 41h:41m:52s remains)
INFO - root - 2017-12-16 02:21:30.648166: step 89580, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:30m:48s remains)
INFO - root - 2017-12-16 02:21:37.074525: step 89590, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 44h:01m:26s remains)
INFO - root - 2017-12-16 02:21:43.513585: step 89600, loss = 0.37, batch loss = 0.25 (12.9 examples/sec; 0.621 sec/batch; 41h:55m:44s remains)
2017-12-16 02:21:44.025796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.93214321 -0.88094759 -1.0645552 -1.0851316 -1.3300447 -1.6658912 -1.7955809 -1.8932433 -1.9941163 -3.9783916 -4.7962 -6.3198919 -7.2673135 -7.6842647 -8.1251431][-1.9485655 -1.9214869 -2.1533279 -2.3002176 -2.5894732 -2.553422 -2.5763974 -2.7297206 -2.7488503 -4.6533256 -5.6897449 -7.303359 -8.0892611 -8.51093 -9.0481][-3.7286017 -3.3168545 -3.3208013 -3.0659871 -2.9734879 -2.8922515 -2.8288016 -2.898912 -3.0639548 -5.0017633 -5.6618986 -7.2504039 -8.2107267 -8.6415482 -9.15073][-4.5033932 -4.0188761 -3.6978312 -2.9830174 -2.5653253 -2.2302623 -1.9462576 -2.0659313 -2.24587 -4.1112075 -4.9650683 -6.7422214 -7.7880669 -8.2471952 -8.6154861][-5.9383659 -4.55715 -3.4681482 -2.6183848 -1.982317 -1.3928218 -1.1160092 -1.1736813 -1.3255639 -2.9766831 -3.7514229 -5.5597525 -6.7980433 -7.4420009 -8.0977983][-6.3477087 -5.0933394 -4.3780332 -2.8775697 -1.5178151 -0.86506462 -0.34224796 -0.4198575 -0.57173538 -2.0990958 -2.8952579 -4.5296373 -5.6224604 -6.3452511 -7.1735458][-6.3965235 -4.7052069 -3.1226535 -1.8559628 -0.62190104 0.1753149 0.73131371 0.86048222 0.87504864 -0.74567842 -1.6049447 -3.1971116 -4.17465 -4.8903551 -5.5770006][-5.5610747 -3.8439004 -2.4564428 -0.94723606 0.61788368 1.1804819 1.4670868 1.5899534 1.6074781 -0.046877861 -0.75096369 -2.4719334 -3.4932518 -3.6923683 -4.2914166][-4.8581114 -3.0626273 -1.5185604 0.098092556 0.91708088 1.1924305 1.4182978 1.3558235 1.3376007 -0.36175871 -1.1148634 -2.6193318 -3.3120751 -3.4781651 -4.1269622][-5.4125161 -3.5130444 -1.6557598 -0.55023289 0.66321373 1.1748037 1.1984253 1.2166538 1.1038532 -1.040679 -1.8905721 -3.1165295 -3.6956689 -3.7437861 -3.9991996][-6.3208084 -4.9001741 -3.7184362 -2.2444978 -1.3061237 -0.96276712 -0.83350945 -0.80090237 -0.88365316 -2.7022219 -3.5919666 -4.7041764 -5.0520134 -4.8275485 -4.7975287][-8.3230219 -7.0665 -5.6025896 -4.2586765 -3.2804518 -2.8404703 -2.5625806 -2.7631445 -2.9379797 -4.3414106 -4.94852 -5.6385918 -5.8503714 -5.5966721 -5.5237207][-9.608469 -8.5407715 -7.2232561 -6.1182866 -5.0520926 -4.3339033 -3.9735177 -4.0260859 -4.2280197 -5.3088164 -5.55879 -5.9036336 -6.2095623 -5.6328821 -5.4619627][-9.7722969 -8.7532434 -7.66681 -6.2222252 -5.2891407 -4.6111851 -4.3711543 -4.5749545 -4.833415 -5.4362168 -5.4845114 -5.5804281 -5.7343264 -5.3634539 -5.0566421][-9.9100914 -9.1972609 -8.64021 -7.5349054 -6.42882 -5.7764239 -5.2792406 -5.4235797 -5.5793257 -5.8713379 -6.0449605 -6.1740317 -6.0245981 -5.6855626 -5.3493881]]...]
INFO - root - 2017-12-16 02:21:50.438966: step 89610, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 43h:16m:32s remains)
INFO - root - 2017-12-16 02:21:56.914891: step 89620, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 42h:42m:19s remains)
INFO - root - 2017-12-16 02:22:03.319731: step 89630, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 43h:32m:53s remains)
INFO - root - 2017-12-16 02:22:09.738927: step 89640, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 43h:56m:38s remains)
INFO - root - 2017-12-16 02:22:16.100446: step 89650, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 41h:54m:52s remains)
INFO - root - 2017-12-16 02:22:22.476684: step 89660, loss = 0.24, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 42h:49m:47s remains)
INFO - root - 2017-12-16 02:22:28.859250: step 89670, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:52m:21s remains)
INFO - root - 2017-12-16 02:22:35.307631: step 89680, loss = 0.34, batch loss = 0.23 (12.6 examples/sec; 0.635 sec/batch; 42h:48m:55s remains)
INFO - root - 2017-12-16 02:22:41.676033: step 89690, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 43h:51m:56s remains)
INFO - root - 2017-12-16 02:22:47.993921: step 89700, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 42h:40m:05s remains)
2017-12-16 02:22:48.545138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3199286 -3.472394 -4.0373993 -4.7587318 -5.6179447 -5.7354679 -5.7824397 -5.8841519 -6.1283441 -7.9420042 -9.1582012 -9.551239 -10.0355 -10.154581 -9.7215319][-3.6076841 -3.623446 -3.8232141 -4.3627448 -4.8549695 -4.8552303 -5.1666346 -5.674222 -6.0467482 -8.1204414 -9.6191263 -9.8966608 -10.485136 -10.126686 -9.524518][-3.5983338 -3.1233702 -2.7397313 -2.7904844 -2.9040565 -3.096673 -3.274457 -4.0082226 -4.5651612 -7.0877442 -8.9273071 -9.2233829 -9.5852165 -9.5861549 -9.0949221][-3.7745013 -2.8836784 -1.9870119 -1.7054639 -1.6095743 -1.4401479 -1.6692462 -2.15126 -2.9229822 -5.2976971 -6.7028003 -7.2702761 -8.2291737 -8.2730026 -7.9424977][-3.7596283 -2.7782125 -1.7880845 -1.04076 -0.40710592 0.20986652 0.27632904 -0.19109297 -0.40650225 -2.5168829 -4.542748 -5.3756866 -6.7401576 -7.4654908 -7.4954333][-3.4606571 -2.627789 -1.9029832 -0.82743025 0.053084373 0.85241127 1.0623503 0.77069664 0.67093849 -1.7383289 -3.1295109 -4.1601372 -5.7558336 -6.5339375 -6.7788582][-2.8454814 -2.3525667 -0.99302244 -0.60012579 0.17896271 1.287837 1.7297745 1.5370274 1.5436554 -0.40560675 -1.9008636 -3.1132197 -4.8637481 -6.1173306 -6.5223436][-2.4965682 -1.8787255 -1.0469542 -0.37266731 0.95086861 1.4570599 1.3644819 2.0098782 2.5046129 0.2638669 -1.5768638 -3.1141415 -4.9276667 -6.3668041 -6.8800898][-2.7481189 -2.0905981 -1.7124696 -0.98343277 0.1244216 0.91942883 1.0529041 1.2578382 1.4343529 -0.6924119 -2.3276129 -3.6890912 -5.5257425 -6.8218474 -7.2568679][-2.3138518 -1.9937677 -1.6401062 -0.8159194 -0.11125755 0.023364544 0.30618429 0.23637486 0.43727303 -1.9018459 -3.221076 -4.321476 -6.0720181 -7.4215989 -8.0046816][-3.9934776 -3.2805424 -2.5968924 -2.0491185 -1.7920537 -1.4373951 -1.430378 -1.2751026 -1.0227761 -3.3371806 -4.8458538 -5.8111625 -6.7103572 -7.8464775 -7.7554574][-5.6327667 -4.7244349 -3.8883429 -3.4051356 -3.1619687 -3.024734 -2.9167428 -2.5448017 -2.4371629 -4.0014057 -5.2017851 -5.6818895 -6.4253535 -7.4628339 -7.4229522][-6.1153331 -5.7739925 -5.0230107 -4.0779519 -3.5215859 -3.6743898 -3.5742111 -3.698246 -3.85329 -4.9372883 -6.1049738 -6.5483 -6.9462113 -7.295989 -6.5506725][-5.9197297 -5.6008024 -4.6354208 -3.8443763 -3.6455979 -3.7817256 -4.1800003 -4.0392213 -3.7169497 -4.6904383 -5.39827 -5.952075 -6.3054566 -6.5883918 -6.4722447][-6.5163383 -5.9692779 -5.0737591 -4.7487335 -4.5197811 -4.3000908 -4.6759286 -4.650115 -4.93692 -5.0474739 -5.0965939 -5.3730025 -5.6026077 -6.0089951 -6.2243462]]...]
INFO - root - 2017-12-16 02:22:55.017052: step 89710, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 44h:52m:03s remains)
INFO - root - 2017-12-16 02:23:01.534537: step 89720, loss = 0.31, batch loss = 0.20 (12.5 examples/sec; 0.639 sec/batch; 43h:05m:14s remains)
INFO - root - 2017-12-16 02:23:07.905785: step 89730, loss = 0.28, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 44h:05m:37s remains)
INFO - root - 2017-12-16 02:23:14.296635: step 89740, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 43h:32m:12s remains)
INFO - root - 2017-12-16 02:23:20.664150: step 89750, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 43h:56m:16s remains)
INFO - root - 2017-12-16 02:23:27.069566: step 89760, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 42h:57m:42s remains)
INFO - root - 2017-12-16 02:23:33.439356: step 89770, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 42h:27m:16s remains)
INFO - root - 2017-12-16 02:23:39.797296: step 89780, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 42h:17m:39s remains)
INFO - root - 2017-12-16 02:23:46.219357: step 89790, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 42h:33m:42s remains)
INFO - root - 2017-12-16 02:23:52.564506: step 89800, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 42h:30m:16s remains)
2017-12-16 02:23:53.159091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2944765 -4.4751291 -4.5471983 -4.8621664 -5.0972328 -5.1216106 -5.1123362 -4.8223705 -4.9230175 -5.4156952 -6.8960195 -8.135169 -9.3843918 -10.318522 -10.961549][-4.1436229 -3.8687856 -4.0135851 -4.4150524 -5.3652797 -5.2392983 -5.7167068 -5.1703334 -4.8920212 -5.3057947 -6.3469524 -7.4324217 -8.92525 -9.6839542 -10.384752][-4.1164694 -3.178134 -3.0446768 -3.2570972 -3.770757 -3.9376543 -4.1853218 -3.7415617 -3.2163329 -3.583498 -4.507432 -5.7476969 -6.7816381 -7.4189034 -8.0747318][-3.2884378 -2.3323622 -2.426466 -2.084579 -2.2111826 -2.0718832 -1.7927499 -1.5022488 -1.2260399 -1.6409473 -2.5480046 -3.5924006 -4.7171588 -5.2765303 -6.0949044][-2.8686676 -1.6832528 -0.94293737 -0.81253195 -0.7289114 0.037328243 0.83701134 1.0049896 1.2782202 0.88903427 -0.48006821 -1.8855929 -3.411674 -4.2217264 -5.1319494][-2.5652533 -1.551713 -1.1161413 -0.52309036 0.32983112 1.334116 2.1750402 2.4713306 2.5580587 1.5603809 -0.2550807 -1.9571643 -3.3827171 -4.3784151 -5.5149078][-3.2768426 -2.3461232 -1.260519 -0.24819231 1.1510105 2.4047222 3.4359856 3.3390303 3.2337484 1.4631042 -0.77649927 -2.7307005 -4.4122338 -5.178484 -6.0114608][-3.6521826 -2.8217854 -1.3959055 0.32231617 1.9263659 2.9161606 3.9180441 3.8642626 3.6178761 1.5417452 -1.03092 -3.0987086 -4.8686886 -5.7582369 -6.3557777][-4.81294 -3.5292411 -1.7392387 -0.34791088 1.2264252 2.1918373 2.9518967 2.892765 2.4235582 0.48176003 -1.6760302 -3.5803027 -5.1680136 -5.6307268 -5.8559666][-4.9146023 -4.16129 -3.2825937 -2.0298963 -0.98669147 -0.12624216 0.65269566 0.36936951 0.081648827 -1.6834569 -3.8074093 -5.1416855 -6.2618494 -6.4344692 -6.7580919][-5.8623433 -5.5262523 -4.8513088 -4.3460841 -3.8694072 -3.4721856 -3.065114 -3.1453819 -3.5240421 -4.959733 -6.0520048 -6.6745858 -7.5224695 -7.6976166 -7.7327342][-7.1092873 -6.3582139 -5.847815 -5.2944975 -4.9499617 -5.1559443 -5.1967516 -5.3395424 -5.2460742 -6.6883783 -8.1083841 -7.7704215 -8.0617142 -8.3022261 -8.6537666][-7.9360218 -7.7143607 -7.5782738 -6.8469577 -6.0911007 -5.88857 -5.7563467 -6.2870259 -6.4996495 -7.2194052 -8.2221336 -7.9558239 -8.1666641 -7.9436817 -7.9791217][-6.3212695 -6.21115 -5.8473215 -5.6747513 -5.2694216 -4.68467 -4.243763 -4.361249 -4.6627531 -5.4082737 -5.9344072 -6.0240932 -6.6691861 -6.4818554 -6.3183537][-4.9420013 -4.9432898 -5.2929072 -5.2387276 -5.0200572 -4.4617915 -3.9922142 -3.961499 -4.1062775 -4.471488 -5.0848894 -5.3214917 -5.4168496 -6.0578723 -6.2702727]]...]
INFO - root - 2017-12-16 02:23:59.486161: step 89810, loss = 0.32, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 42h:35m:27s remains)
INFO - root - 2017-12-16 02:24:05.911949: step 89820, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 42h:55m:29s remains)
INFO - root - 2017-12-16 02:24:12.382402: step 89830, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 42h:43m:50s remains)
INFO - root - 2017-12-16 02:24:18.872114: step 89840, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 44h:46m:30s remains)
INFO - root - 2017-12-16 02:24:25.290328: step 89850, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:13m:51s remains)
INFO - root - 2017-12-16 02:24:31.603514: step 89860, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 44h:13m:35s remains)
INFO - root - 2017-12-16 02:24:37.907916: step 89870, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 41h:53m:29s remains)
INFO - root - 2017-12-16 02:24:44.339997: step 89880, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 42h:45m:03s remains)
INFO - root - 2017-12-16 02:24:50.744585: step 89890, loss = 0.28, batch loss = 0.17 (12.9 examples/sec; 0.619 sec/batch; 41h:44m:44s remains)
INFO - root - 2017-12-16 02:24:57.161438: step 89900, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 44h:48m:56s remains)
2017-12-16 02:24:57.694831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.327044 -2.5577459 -2.7891593 -2.9318066 -2.7176366 -2.5644999 -2.7882791 -2.5211344 -2.5102186 -3.8488371 -4.3502283 -4.9840717 -5.4874096 -6.2760296 -7.2330909][-2.828269 -3.2590203 -3.6501169 -3.8277187 -3.9344995 -3.6614256 -3.3509746 -3.007112 -2.6758995 -3.9926417 -4.816772 -5.5939665 -6.4439411 -7.6302752 -8.2540789][-2.7929621 -2.7562494 -2.7509356 -2.9399209 -2.9104314 -2.7785296 -2.4766469 -2.0622745 -1.990746 -3.1185098 -3.7328837 -4.669981 -5.3712435 -6.5130391 -7.4930472][-1.9341908 -1.2927217 -0.89983177 -0.73526955 -0.79315615 -0.47670984 -0.050891876 0.16233206 0.22848225 -1.0840993 -2.164288 -3.4060259 -4.3979969 -5.5924625 -6.7425528][-1.4248776 -0.63321877 0.12250423 0.14859962 0.0740881 0.50262451 0.86133289 0.99164867 1.1673136 -0.18403959 -1.1646748 -2.2941756 -3.4718289 -4.6747718 -6.105094][-1.8162584 -0.77328634 -0.055925846 0.34569645 0.64315891 1.1624136 1.662075 1.7328682 1.79916 0.21381521 -0.95152426 -2.1176562 -3.1791921 -4.4393787 -5.7555718][-2.4043918 -1.3479834 -0.42971182 0.27640057 0.93886662 1.4623699 1.9200573 2.1479521 2.3227444 0.6650753 -0.58861876 -1.7985148 -3.0328169 -4.4330916 -5.8081145][-2.609714 -1.5534225 -0.6922369 0.21183014 1.1626301 2.1473389 2.8133154 2.9492865 3.003933 1.2598152 -0.065783024 -1.5135584 -2.7912087 -4.2892771 -5.737308][-2.6793752 -1.7947297 -0.660377 0.16582537 0.91817 1.9706984 2.6940708 2.8891697 3.1342764 1.3108683 -0.23679018 -1.612947 -2.813406 -4.3673992 -5.8202205][-3.2363491 -2.4028378 -1.6289062 -0.80901337 -0.20894289 0.73597717 1.5234671 1.7394686 1.9369135 0.36075878 -0.79407787 -2.1813421 -3.3808036 -4.683547 -6.1499166][-4.2678361 -3.5460124 -2.8534756 -2.0849285 -1.4941988 -0.504971 0.50373745 0.62528706 0.63837051 -0.887506 -2.0086384 -3.2140536 -4.0716777 -5.2510591 -6.2876043][-5.758646 -4.9442844 -4.3789005 -3.8696327 -3.4744844 -2.7458282 -2.1176972 -1.8786855 -1.6655784 -2.7822332 -3.2877474 -4.2196431 -4.9553547 -5.4666224 -6.2150507][-6.5643167 -5.7905416 -5.2420387 -4.9581795 -4.6570425 -4.2270856 -3.7373178 -3.716594 -3.7498112 -4.3107109 -4.6337194 -5.2324343 -5.5322981 -6.0292268 -6.3934803][-7.1006174 -6.5902662 -6.1658287 -5.8223171 -5.4760871 -5.20744 -5.0157089 -5.1286077 -5.2260303 -5.841711 -6.0190721 -6.2901549 -6.4432049 -6.3582506 -6.4772835][-7.5050235 -7.1420016 -6.791008 -6.6790328 -6.5443645 -6.3936644 -5.9449039 -5.995719 -6.1417675 -6.2164779 -6.2961555 -6.46588 -6.5857677 -6.5618639 -6.4910512]]...]
INFO - root - 2017-12-16 02:25:04.076131: step 89910, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 42h:44m:59s remains)
INFO - root - 2017-12-16 02:25:10.480040: step 89920, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 43h:07m:30s remains)
INFO - root - 2017-12-16 02:25:16.824246: step 89930, loss = 0.24, batch loss = 0.13 (13.1 examples/sec; 0.610 sec/batch; 41h:04m:51s remains)
INFO - root - 2017-12-16 02:25:23.221283: step 89940, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 43h:37m:35s remains)
INFO - root - 2017-12-16 02:25:29.619720: step 89950, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 42h:45m:09s remains)
INFO - root - 2017-12-16 02:25:35.943204: step 89960, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 42h:14m:21s remains)
INFO - root - 2017-12-16 02:25:42.392769: step 89970, loss = 0.34, batch loss = 0.22 (12.1 examples/sec; 0.660 sec/batch; 44h:27m:35s remains)
INFO - root - 2017-12-16 02:25:48.754568: step 89980, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.623 sec/batch; 41h:58m:10s remains)
INFO - root - 2017-12-16 02:25:55.109302: step 89990, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 42h:52m:05s remains)
INFO - root - 2017-12-16 02:26:01.483286: step 90000, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.622 sec/batch; 41h:54m:54s remains)
2017-12-16 02:26:02.040156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1295109 -2.4718623 -2.4903083 -2.7524362 -2.9449239 -3.4434285 -3.3929257 -2.7833023 -2.497499 -2.6707158 -3.5102167 -4.7250547 -5.8420572 -6.4482222 -7.4651594][-1.6981812 -1.818121 -2.5297918 -3.4366074 -4.1289864 -4.085628 -4.3485489 -4.3592358 -4.2894506 -4.3582411 -5.0079823 -6.0089231 -6.8770933 -7.5224361 -8.6272211][-0.765275 -0.88440752 -1.571795 -2.1150126 -2.6586719 -2.9649992 -3.2833757 -3.2712402 -3.5817647 -4.101738 -4.5477171 -5.3748569 -6.2616911 -6.4666338 -7.7000523][0.84980774 0.31569481 -0.40917063 -1.1208768 -2.008955 -2.2993503 -2.4938488 -2.6981821 -2.6814299 -3.1639519 -4.0770698 -5.2995405 -5.9562979 -6.6061611 -7.5298176][0.58957195 0.38729 0.32163715 -0.13559914 -0.36071825 -0.69933033 -0.79431725 -0.95535612 -0.99705839 -1.6057 -2.2474618 -3.6033425 -5.173255 -5.9800367 -7.3535924][-1.0674911 -0.83847189 -0.44074869 -0.23213577 -0.078484058 0.72072029 1.401803 1.5299721 1.493083 0.61710453 -0.12127829 -1.5418653 -3.0153723 -4.2778053 -5.6281714][-1.8346534 -1.3658338 -0.74538946 -0.21281815 0.57352448 1.3593645 2.4329491 2.7852802 2.9425106 1.7118645 0.54788589 -0.81889629 -2.4649649 -3.5244584 -5.0429163][-2.2071743 -1.7438359 -1.0149512 0.058350563 1.3000422 2.3463373 3.4864931 3.9650078 4.1685743 3.1915693 1.6377583 -0.59374046 -2.1792383 -3.260426 -4.2027149][-2.6858268 -2.1363378 -1.3834338 -0.48249817 0.95188713 2.2266846 3.5090342 3.8347282 3.9319439 2.5542374 0.87446213 -0.95068741 -2.7707219 -3.673996 -4.783349][-3.4497008 -2.8757911 -2.5753341 -1.5889254 -0.49760962 0.61620808 1.5298347 1.9913197 2.4537992 1.0002308 -0.79147482 -2.7940712 -4.3656764 -4.706645 -5.4132233][-4.689496 -4.297121 -4.0669918 -3.2643652 -2.3632412 -1.6067004 -0.74509573 -0.47778797 -0.36044693 -1.4499149 -2.96138 -4.1888084 -5.4841337 -5.8705931 -6.4350677][-5.3880978 -5.4905787 -4.8809729 -4.1579595 -3.6000457 -3.0923157 -2.8811388 -2.9151163 -2.769465 -3.5696235 -4.3585763 -5.1611996 -5.8499341 -5.89896 -6.4935775][-6.5263147 -5.9412961 -5.5749655 -5.2699041 -4.4441395 -4.00883 -3.5515547 -3.8558283 -4.254446 -4.6695142 -5.2899356 -6.1164989 -6.6218972 -6.2268944 -5.7944818][-6.8008056 -6.918879 -6.3257761 -5.4552374 -4.9713063 -4.8393025 -4.5090647 -4.4356723 -4.3312016 -5.175045 -5.9435873 -6.1970568 -6.2850442 -6.114603 -6.0131211][-7.5860338 -7.0159321 -6.8426476 -6.6331172 -6.1954875 -5.608984 -5.507895 -5.8287926 -6.2792821 -6.1861916 -6.1588192 -6.3490677 -6.8400741 -6.1628714 -5.4856453]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-90000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-90000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:26:09.781090: step 90010, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 43h:08m:22s remains)
INFO - root - 2017-12-16 02:26:16.107495: step 90020, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 42h:42m:37s remains)
INFO - root - 2017-12-16 02:26:22.522275: step 90030, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.650 sec/batch; 43h:48m:31s remains)
INFO - root - 2017-12-16 02:26:28.996129: step 90040, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 43h:24m:49s remains)
INFO - root - 2017-12-16 02:26:35.418205: step 90050, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 42h:42m:14s remains)
INFO - root - 2017-12-16 02:26:41.827032: step 90060, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:45m:19s remains)
INFO - root - 2017-12-16 02:26:48.215605: step 90070, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 43h:02m:46s remains)
INFO - root - 2017-12-16 02:26:54.600981: step 90080, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 43h:17m:51s remains)
INFO - root - 2017-12-16 02:27:00.972872: step 90090, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 42h:23m:46s remains)
INFO - root - 2017-12-16 02:27:07.349595: step 90100, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.622 sec/batch; 41h:51m:26s remains)
2017-12-16 02:27:07.884045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2675762 -6.5021043 -6.2945528 -6.4731474 -6.5848069 -6.0393095 -5.6670771 -5.033761 -4.4910035 -4.777566 -5.5742893 -5.6696658 -6.5458169 -7.5683722 -7.9847441][-5.9075451 -6.2830567 -6.7124224 -6.8328028 -6.9435892 -6.7468886 -6.126225 -5.818203 -5.6819315 -6.111783 -6.7832813 -6.9058418 -7.4679055 -7.8165445 -8.0015612][-6.5055947 -6.4556594 -5.5648232 -5.8308563 -5.9082766 -5.7074504 -5.652668 -5.5300317 -5.7383742 -6.0688086 -6.6741366 -7.0619617 -7.34241 -7.9356074 -7.9616203][-5.6855965 -5.5885773 -5.5788832 -4.867835 -4.1565123 -3.6262193 -3.2210879 -3.3661609 -3.6517344 -4.5716357 -5.8021922 -5.7947674 -5.9028845 -6.8985643 -7.7139721][-5.580245 -4.823595 -3.8176134 -3.339499 -2.5231786 -1.6828671 -1.4817705 -1.4479189 -1.100822 -1.7179666 -3.0124178 -3.9166183 -4.3504205 -5.3297119 -6.043385][-3.9576952 -3.5256386 -2.6920304 -1.665874 -0.59238911 0.15583229 0.70674515 0.75520325 1.0659599 0.37523746 -0.70177889 -1.6212687 -2.3444638 -4.3354154 -5.4649239][-3.8142352 -2.9263315 -1.7115421 -0.59517145 0.17676067 0.889204 1.8962135 2.2884598 2.5324364 1.9216309 0.63548756 -0.91898632 -2.0494781 -3.9193983 -5.1874666][-3.27774 -2.7958069 -1.9627972 -0.57361889 0.58933735 1.3823624 1.8222027 2.2943392 3.442277 2.539382 0.64740944 -0.73600197 -1.9507022 -3.9148483 -5.356391][-4.7081537 -3.6113658 -2.3408346 -1.4384952 -0.777915 0.14705181 1.1887102 2.1497288 2.5135536 1.8621988 0.99279976 -0.51334381 -2.1944394 -4.0930567 -5.3076124][-5.380929 -4.5914288 -3.711266 -2.313952 -0.94133711 -0.54505396 -0.53785849 0.37147522 1.5403862 0.39351559 -1.1462603 -2.2607617 -3.0732012 -4.1933327 -5.0563831][-7.08766 -6.1645136 -5.3067341 -4.1098204 -3.04912 -2.3657002 -1.7204027 -1.5919785 -1.957408 -2.7590861 -3.4211717 -3.9326856 -4.6075144 -5.6634469 -6.1935964][-8.3491011 -7.1394682 -5.9054546 -5.46835 -4.5283327 -3.7917838 -3.4615688 -3.3475642 -3.050168 -3.8672266 -5.0935678 -5.1199055 -4.9250174 -5.3014421 -5.309824][-7.6704454 -7.3114824 -7.1537895 -6.5660667 -5.7720308 -4.9005613 -4.1389685 -3.8810306 -3.8938084 -4.2410583 -5.30322 -5.4005394 -5.4423838 -5.6360693 -5.5326033][-7.623621 -6.8886828 -6.0505123 -5.876853 -5.5389023 -4.8120966 -4.3230228 -4.497798 -4.5630302 -4.8514633 -5.0179305 -5.2574072 -5.0697565 -4.9221191 -4.8939891][-6.6032534 -6.5970178 -6.5148444 -6.0260048 -5.6885772 -5.2386761 -4.906951 -4.8061485 -5.0067453 -5.2492714 -5.6584654 -5.8084583 -5.7936473 -5.8971524 -5.6879215]]...]
INFO - root - 2017-12-16 02:27:14.291131: step 90110, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 44h:15m:32s remains)
INFO - root - 2017-12-16 02:27:20.641546: step 90120, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.618 sec/batch; 41h:38m:05s remains)
INFO - root - 2017-12-16 02:27:27.067410: step 90130, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 43h:39m:16s remains)
INFO - root - 2017-12-16 02:27:33.439721: step 90140, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.667 sec/batch; 44h:54m:40s remains)
INFO - root - 2017-12-16 02:27:39.846521: step 90150, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 42h:35m:12s remains)
INFO - root - 2017-12-16 02:27:46.243654: step 90160, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 42h:51m:32s remains)
INFO - root - 2017-12-16 02:27:52.588784: step 90170, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 43h:40m:21s remains)
INFO - root - 2017-12-16 02:27:58.997501: step 90180, loss = 0.33, batch loss = 0.22 (12.6 examples/sec; 0.632 sec/batch; 42h:34m:07s remains)
INFO - root - 2017-12-16 02:28:05.421735: step 90190, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.620 sec/batch; 41h:42m:32s remains)
INFO - root - 2017-12-16 02:28:11.812619: step 90200, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 43h:24m:12s remains)
2017-12-16 02:28:12.327118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2635536 -5.2741032 -5.4426107 -5.66506 -5.8552136 -5.919776 -5.6151996 -5.07963 -4.3142648 -4.1212635 -5.4540491 -6.0353146 -6.745244 -7.1684947 -8.3663464][-4.7882085 -5.4855952 -5.6312904 -5.9714813 -5.9573865 -5.7091236 -5.5745406 -5.3781538 -4.9559221 -4.5428553 -5.6700611 -5.6555462 -6.6676769 -7.06674 -7.4291329][-3.9974463 -3.6655602 -3.8709149 -4.4320316 -4.1360836 -4.0872979 -3.9251804 -3.6631804 -3.8361142 -3.917623 -5.0913067 -5.1605153 -5.8406506 -5.7637949 -6.2212563][-3.5170245 -3.343246 -3.282042 -2.8084288 -2.3487 -2.597014 -2.0819931 -1.9772687 -2.3881431 -2.8883061 -4.1552896 -3.9941657 -4.4085641 -4.8756046 -5.2888432][-3.7566147 -3.4037609 -2.6503062 -1.8153076 -1.0894437 -0.66893053 -0.02154541 -0.39403009 -0.67355251 -0.99722481 -2.332006 -2.2785182 -2.9859452 -3.2508268 -3.7982805][-1.4438691 -1.773356 -1.909914 -1.0366054 0.14264965 1.1624956 1.6415548 1.3784313 1.0479221 0.56262779 -0.93822527 -1.1417174 -1.98457 -2.8481679 -4.1364756][-1.8329453 -1.8397212 -1.0753379 -0.31833076 0.44441032 1.5403328 2.0307426 2.1479645 2.1785288 1.5987806 0.039747238 -0.88260174 -2.4274478 -3.1892595 -4.2026224][-1.8275919 -1.689363 -1.6009111 -0.94811821 0.19298363 1.0839367 1.6466303 1.9777718 2.3745317 2.0601082 0.56158066 -0.97856522 -2.6484513 -3.76343 -4.9675231][-2.2699423 -2.3367357 -1.9341302 -1.0458241 -0.40604162 0.26119137 0.6101265 0.94893074 1.5283136 1.5257664 0.038497448 -1.3249493 -3.2818775 -4.8237119 -5.855948][-4.1526041 -3.5693917 -3.1416521 -2.4504552 -1.1303997 -0.41634274 -0.052317142 0.28240347 0.54602432 0.53296375 -0.83425951 -1.8478317 -3.5214214 -4.9305229 -6.4820647][-8.1900454 -7.3627434 -5.9918442 -4.3130684 -3.1593862 -2.7946496 -2.3281722 -2.1651349 -2.0332661 -2.243607 -3.3175774 -3.7243536 -5.1079226 -6.4818807 -7.303915][-8.3038368 -7.8863754 -7.3201265 -6.5218916 -5.8128505 -5.1560774 -4.3208613 -4.2912159 -3.8199742 -3.5580006 -4.7147083 -5.511013 -6.5872016 -6.922718 -6.7850013][-7.7881465 -7.8225756 -7.0134807 -6.4860058 -6.0393114 -5.5329652 -4.7274451 -4.3956356 -3.8864806 -3.9317524 -5.1404934 -5.4309807 -5.7547112 -6.10612 -6.3098583][-7.0163441 -7.375308 -6.9946847 -6.7789693 -6.2394638 -5.2463369 -4.4929104 -4.6137657 -4.5777512 -4.6723032 -5.6023364 -5.8833313 -5.9577179 -5.5648017 -4.9510431][-6.6945057 -6.9555516 -7.0646977 -7.1757517 -6.8575411 -6.1900845 -5.6628337 -5.5584393 -5.4204936 -5.805582 -6.2645392 -6.2853694 -6.1815405 -5.922051 -5.236517]]...]
INFO - root - 2017-12-16 02:28:18.805443: step 90210, loss = 0.29, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 44h:12m:59s remains)
INFO - root - 2017-12-16 02:28:25.265945: step 90220, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 43h:50m:03s remains)
INFO - root - 2017-12-16 02:28:31.725416: step 90230, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 43h:58m:44s remains)
INFO - root - 2017-12-16 02:28:38.205023: step 90240, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 44h:19m:39s remains)
INFO - root - 2017-12-16 02:28:44.624177: step 90250, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 42h:35m:09s remains)
INFO - root - 2017-12-16 02:28:51.065979: step 90260, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:44m:17s remains)
INFO - root - 2017-12-16 02:28:57.530101: step 90270, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.632 sec/batch; 42h:29m:57s remains)
INFO - root - 2017-12-16 02:29:03.983825: step 90280, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 43h:16m:10s remains)
INFO - root - 2017-12-16 02:29:10.432879: step 90290, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 43h:31m:14s remains)
INFO - root - 2017-12-16 02:29:16.887440: step 90300, loss = 0.24, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 45h:04m:01s remains)
2017-12-16 02:29:17.409525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8722222 -3.4060569 -4.1107864 -3.8377471 -2.5721388 -3.5150418 -2.6508918 -2.7926641 -2.6909242 -1.7024307 -5.3227062 -4.2576189 -5.6977472 -5.11666 -4.4040608][-3.6369486 -2.9159822 -1.9007764 -2.2712188 -2.1055212 -2.3511233 -2.8096404 -3.1522555 -0.97600746 -0.36808157 -3.0198917 -3.8898692 -6.9109769 -4.3883667 -3.7640977][-1.7818799 -2.7708983 -2.6899519 -0.85614729 -0.46574163 -0.84536362 -0.049028397 -0.71392488 -0.91799164 -0.62325239 -2.5159988 -2.4014745 -5.4686766 -5.4016142 -5.1254644][-1.8891454 -0.73672724 -1.892158 -1.4103875 -0.20739555 0.38706398 0.051045895 0.026449203 1.4780436 0.839345 -2.19033 -3.7431898 -5.2205734 -4.14563 -4.4242067][-3.2637181 -2.0946479 0.24963045 0.54843044 -0.22126818 -0.528357 0.37310982 -0.290339 0.53929615 0.907464 -1.7730002 -2.5209384 -5.3334084 -4.9280262 -4.4139161][-0.86775446 -1.1936045 -1.4961638 -0.18568468 2.0229406 2.2540255 2.1488409 1.7695932 1.4315166 1.3175287 -1.4755955 -2.6454287 -4.7873049 -4.4554014 -4.7706409][-1.541419 -1.0080109 0.86852932 0.95701504 1.6617527 2.6628408 4.1049976 4.4159412 4.0692453 2.4280844 -1.5322061 -2.45814 -5.4074411 -4.0743084 -4.1026678][-0.47548246 -0.49674702 0.21544313 2.2349205 3.4031553 3.5108786 3.722641 3.6541691 3.9859219 3.6484165 -0.67108393 -2.1691251 -4.8628483 -4.5887461 -4.7095633][-0.35321951 0.41399193 0.5988245 2.1303959 3.9422379 4.1813745 4.6969624 3.7123995 3.3829718 3.3408957 -0.65058708 -0.867331 -3.8257675 -3.4297981 -4.1134491][-1.978271 -1.9058542 -0.69715405 0.12212467 0.63658333 2.3609676 2.1492853 2.1423407 2.6992426 2.1129837 -0.70256233 -1.2839894 -3.2938895 -3.00663 -3.8374844][-4.9639111 -5.0413094 -4.1019044 -3.2600207 -2.3658953 -1.3159375 -0.78741407 -1.6522808 -1.3786402 -1.1718993 -2.5943723 -2.8472309 -4.5481119 -3.8999908 -4.9505119][-6.5799084 -6.8717852 -6.12119 -5.7581711 -4.3574553 -4.5951247 -4.3339748 -3.302969 -3.3084931 -2.9062157 -3.7918112 -4.4086676 -5.8842826 -5.2645044 -6.1655722][-5.5926652 -7.1163297 -6.8456831 -7.1116643 -5.739624 -5.1823092 -4.9930096 -5.02345 -4.7882423 -4.3239803 -5.1768513 -4.8916426 -5.0024681 -5.7846088 -6.3933144][-5.981215 -5.2192917 -5.1728859 -6.0697689 -6.0331764 -6.1164856 -4.7560959 -4.8388777 -4.7173643 -5.072917 -5.9244781 -5.6904478 -5.7467642 -5.7650981 -6.2675047][-5.4801545 -5.7684708 -5.4913921 -5.6883512 -4.8303027 -6.0173783 -5.7890263 -6.2719593 -5.4876385 -5.6949277 -5.7432165 -5.9376221 -6.5844722 -6.466939 -5.9212532]]...]
INFO - root - 2017-12-16 02:29:23.832147: step 90310, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 44h:09m:24s remains)
INFO - root - 2017-12-16 02:29:30.297213: step 90320, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 42h:31m:02s remains)
INFO - root - 2017-12-16 02:29:36.766989: step 90330, loss = 0.24, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 45h:31m:15s remains)
INFO - root - 2017-12-16 02:29:43.177049: step 90340, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 42h:58m:35s remains)
INFO - root - 2017-12-16 02:29:49.635665: step 90350, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 43h:16m:41s remains)
INFO - root - 2017-12-16 02:29:56.041355: step 90360, loss = 0.26, batch loss = 0.15 (12.9 examples/sec; 0.618 sec/batch; 41h:35m:44s remains)
INFO - root - 2017-12-16 02:30:02.418415: step 90370, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 42h:24m:09s remains)
INFO - root - 2017-12-16 02:30:08.868899: step 90380, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 42h:26m:26s remains)
INFO - root - 2017-12-16 02:30:15.246730: step 90390, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 44h:34m:19s remains)
INFO - root - 2017-12-16 02:30:21.743653: step 90400, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:16m:39s remains)
2017-12-16 02:30:22.256565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1157351 -6.5141106 -7.9047065 -9.44476 -9.0940475 -7.4575105 -5.6349907 -4.5681448 -4.3960309 -5.6709118 -6.7534475 -8.283412 -8.0732841 -8.1590567 -7.9541483][-4.814611 -6.4271984 -8.5820036 -10.006761 -9.7614193 -8.543334 -6.7576094 -5.59834 -5.30854 -6.9431095 -8.0362453 -9.1668224 -9.2925758 -9.3592205 -9.12062][-5.2200933 -6.5593004 -8.4579325 -9.6412239 -9.6186857 -7.9177442 -6.1688313 -5.3868771 -4.919898 -6.8524303 -8.1767006 -9.6911192 -9.5613356 -9.7511854 -9.3241234][-5.9472942 -7.7815895 -9.2695255 -9.4292917 -8.534482 -6.4886522 -4.2652817 -2.9249959 -2.80997 -5.0492992 -6.8849368 -9.1436243 -9.6699085 -9.6946831 -9.599741][-6.2226229 -7.1957684 -7.9840178 -7.876873 -6.5044837 -3.5994296 -1.8131866 -0.5943737 -0.435493 -3.1109543 -5.7121449 -8.4031076 -9.2093058 -9.4233589 -9.66343][-8.0524044 -7.7596035 -7.6245351 -6.1332273 -3.7940323 -0.90478611 1.5669727 2.4317141 1.5888357 -1.322753 -4.6975584 -7.5593891 -8.7588663 -9.1097708 -9.02603][-9.3772907 -8.7018852 -6.8750114 -4.5888467 -1.6752667 2.0757885 4.4453869 4.7630997 3.8731794 -0.068016529 -3.7573819 -6.8029485 -8.0130062 -8.7022848 -8.6544695][-8.8932428 -8.2536926 -7.0399914 -3.4744067 0.59057713 4.0544043 6.092288 6.0207558 5.0135012 1.0737047 -3.2387981 -6.5788589 -7.8526125 -8.3035507 -8.1719236][-8.8557644 -7.7910767 -6.5439448 -3.8144569 -1.08605 2.2823553 4.7542171 4.6870356 3.1080027 -0.40171576 -3.275178 -6.4928932 -7.4909048 -8.3460522 -8.5333061][-10.240312 -9.4402313 -8.3969917 -5.8090372 -3.5749454 -1.3562794 0.25027275 0.59065914 0.68290138 -2.3064704 -5.222549 -7.5906563 -8.7070341 -9.2439861 -8.8659163][-10.984159 -10.781034 -10.132917 -8.3208094 -6.5068569 -4.4450331 -3.324687 -3.6169538 -3.8261108 -5.6751547 -7.8826303 -9.4297009 -9.9536734 -9.7241869 -9.7115946][-10.912722 -10.842034 -10.178816 -9.13897 -8.3753166 -6.8583221 -5.61808 -6.1729689 -6.4588509 -7.8340173 -8.3137007 -9.2458124 -9.6019592 -9.359129 -9.2518682][-10.50984 -9.8757811 -9.3805885 -9.0174627 -8.54333 -8.3391933 -7.9630384 -7.7092485 -7.54805 -8.7403717 -9.1043835 -8.8866177 -8.5863342 -8.1787739 -8.5054312][-9.750638 -8.99488 -7.8125815 -7.8910379 -7.6081705 -7.1574297 -7.115169 -7.5183821 -7.6182718 -7.7177386 -7.810915 -7.8528986 -7.5357323 -7.1392679 -7.0005627][-8.7810345 -8.08394 -7.8956418 -7.350245 -6.6380367 -6.7730823 -7.1100621 -7.3235984 -7.6353755 -7.503593 -6.924468 -6.5351553 -6.1940508 -6.0301981 -6.2320719]]...]
INFO - root - 2017-12-16 02:30:28.700249: step 90410, loss = 0.24, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 42h:18m:30s remains)
INFO - root - 2017-12-16 02:30:35.093962: step 90420, loss = 0.33, batch loss = 0.21 (12.4 examples/sec; 0.643 sec/batch; 43h:15m:26s remains)
INFO - root - 2017-12-16 02:30:41.520300: step 90430, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 43h:35m:29s remains)
INFO - root - 2017-12-16 02:30:47.944620: step 90440, loss = 0.34, batch loss = 0.23 (12.2 examples/sec; 0.658 sec/batch; 44h:14m:09s remains)
INFO - root - 2017-12-16 02:30:54.339663: step 90450, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 43h:56m:11s remains)
INFO - root - 2017-12-16 02:31:00.674058: step 90460, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 42h:50m:08s remains)
INFO - root - 2017-12-16 02:31:07.122581: step 90470, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 43h:20m:51s remains)
INFO - root - 2017-12-16 02:31:13.584184: step 90480, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 43h:40m:46s remains)
INFO - root - 2017-12-16 02:31:20.064351: step 90490, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 43h:07m:59s remains)
INFO - root - 2017-12-16 02:31:26.464624: step 90500, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 43h:36m:13s remains)
2017-12-16 02:31:26.978468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6734529 -8.1582546 -8.0910091 -7.745945 -6.8264527 -5.4229231 -3.5261388 -2.1830831 -0.70000792 -0.96138382 -1.6491199 -3.3101883 -4.7807188 -6.0039682 -6.9799151][-5.9048877 -6.6094322 -7.3053861 -7.5251684 -7.1229343 -6.460773 -5.5579481 -3.9610989 -1.5923262 -1.5467505 -1.7050934 -2.7152081 -3.7279139 -5.0024567 -6.5281587][-5.8467436 -6.1419044 -6.4181881 -6.8554339 -7.3708372 -7.293911 -6.5878568 -5.566431 -4.2217388 -4.1004133 -3.4251232 -3.9148722 -4.4883389 -5.1869793 -6.0938287][-5.357996 -5.5058737 -5.17781 -4.6636128 -4.91702 -4.7935543 -4.7943721 -4.4494996 -3.6930985 -4.5232568 -5.3045816 -5.6993704 -5.4032211 -5.7068758 -6.4450769][-5.8408065 -5.2501388 -4.364253 -3.515789 -3.2606053 -2.7186503 -1.5505939 -1.1660357 -1.4300928 -2.970356 -4.0594068 -5.6433268 -6.7913837 -7.3940954 -7.6524086][-4.4630485 -3.3480864 -2.5047059 -1.0366182 -0.045011044 0.8909502 1.7763805 1.6079254 1.7444267 -0.15298033 -1.9464087 -3.8229909 -5.1915646 -6.5321717 -7.7534637][-3.9651787 -2.4122591 -1.098999 0.15035629 1.5289841 3.475709 4.5850849 4.4111261 4.184783 1.8808517 -0.18358755 -2.7007751 -4.4242282 -6.1816282 -7.2366462][-3.426384 -2.702352 -2.1652102 -0.57109451 0.98521805 2.5357866 3.5247393 3.7908936 3.5745411 1.0362062 -1.2942419 -3.6824408 -5.1826172 -6.2674055 -7.1220827][-3.7896495 -3.1917639 -2.9557347 -1.5770903 -0.57594109 1.1441383 2.1165104 2.3005295 2.4550209 -0.097105026 -2.8242774 -4.93121 -5.7914572 -6.5967836 -7.1302485][-5.5750632 -4.8804827 -4.4661961 -3.2582841 -2.2918406 -0.8663559 0.15447044 0.48865986 0.68897057 -1.3163085 -3.0064154 -4.8175859 -6.1562285 -6.849627 -7.37246][-8.3637381 -7.7105346 -7.0055165 -6.0360088 -5.5883884 -4.1082177 -2.8092432 -3.0226521 -3.085866 -4.069232 -4.5970716 -5.979259 -6.9335327 -7.709568 -8.5311871][-9.5070381 -8.7624769 -8.1224213 -7.4015179 -6.5142212 -5.6681824 -4.8924904 -4.4613461 -4.1697721 -4.9708772 -4.9027271 -5.3304634 -6.0211859 -6.8990579 -7.6130643][-9.4020834 -9.6110773 -9.3341131 -8.4939327 -7.7125731 -6.7345858 -5.6087461 -5.4950914 -5.5801988 -5.7062454 -5.4539633 -5.4687586 -5.6475868 -5.8944125 -6.2838349][-8.6418295 -8.921277 -9.0329094 -8.5069914 -8.1466818 -7.7537804 -6.9615641 -6.5525103 -6.31676 -6.2957249 -6.3557506 -6.4097652 -6.2026329 -6.1013761 -5.7261872][-6.38769 -6.5926466 -7.2751017 -7.2852912 -7.3902144 -7.2527771 -7.0585189 -7.0675712 -6.6660867 -6.3734803 -6.2506866 -6.0536909 -6.2144632 -6.0570474 -5.8036518]]...]
INFO - root - 2017-12-16 02:31:33.379787: step 90510, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 41h:54m:19s remains)
INFO - root - 2017-12-16 02:31:39.833451: step 90520, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 42h:18m:48s remains)
INFO - root - 2017-12-16 02:31:46.282876: step 90530, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 44h:35m:56s remains)
INFO - root - 2017-12-16 02:31:52.756914: step 90540, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 43h:15m:57s remains)
INFO - root - 2017-12-16 02:31:59.179039: step 90550, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.633 sec/batch; 42h:31m:07s remains)
INFO - root - 2017-12-16 02:32:05.676062: step 90560, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 43h:49m:56s remains)
INFO - root - 2017-12-16 02:32:12.085759: step 90570, loss = 0.33, batch loss = 0.22 (12.3 examples/sec; 0.649 sec/batch; 43h:35m:18s remains)
INFO - root - 2017-12-16 02:32:18.499754: step 90580, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:36m:54s remains)
INFO - root - 2017-12-16 02:32:24.927480: step 90590, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 44h:46m:20s remains)
INFO - root - 2017-12-16 02:32:31.377926: step 90600, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 42h:59m:26s remains)
2017-12-16 02:32:31.877797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1922784 -3.0431843 -2.803412 -2.5004697 -2.5518203 -2.4555702 -2.1733832 -1.9701657 -2.1282482 -3.9653683 -4.8014693 -5.5228443 -6.3254266 -6.9412107 -7.5643921][-3.1470995 -3.443707 -3.632019 -3.5031567 -3.4805322 -3.1677971 -2.8353524 -2.7467451 -2.83138 -4.6642704 -5.4268618 -6.2040186 -7.1973829 -7.6515274 -8.0345049][-2.7621779 -2.8019867 -2.9144373 -2.8493266 -2.7918448 -2.7084723 -2.4854717 -2.5762806 -2.6145096 -4.1108894 -4.6428204 -5.2764149 -6.2545285 -7.0363135 -7.7873087][-2.7643251 -2.7186718 -2.7574277 -2.8574615 -2.6970949 -2.3121009 -1.6965518 -1.7394042 -1.885725 -3.3915067 -3.8373775 -4.8170204 -5.9339237 -6.8026443 -7.8111463][-3.6941061 -3.2144356 -3.0202484 -2.8106718 -2.2519374 -1.6064548 -1.1869431 -1.4129353 -1.4514437 -3.0070724 -3.5325689 -4.5140467 -5.6072917 -6.6065722 -7.6258531][-4.5739069 -3.7746508 -3.3169508 -2.7288995 -1.7125525 -0.79525089 -0.27678871 -0.6258173 -0.71144867 -2.3980947 -2.9435477 -4.1345119 -5.4518719 -6.3338232 -7.4504695][-4.8508015 -4.2561159 -3.762593 -2.6625 -1.4290733 -0.14580679 0.71054935 0.43576145 0.24653435 -1.4416227 -1.9987507 -3.2855382 -4.6885118 -5.7002487 -6.6049366][-4.8630219 -3.7879255 -2.8611026 -1.5598426 -0.32197332 0.672081 1.0152121 1.059166 1.2143021 -0.50565434 -1.2031083 -2.4107318 -3.5859756 -4.9107637 -5.9499702][-3.9387159 -3.137249 -2.4412022 -1.0648417 0.00057554245 0.580925 0.75976658 0.69901085 0.61487293 -0.8344636 -1.343071 -2.8648205 -4.059391 -5.1574864 -5.9311404][-3.3191328 -2.7302284 -2.0608134 -1.0521665 -0.50048828 -0.071635723 0.031351566 -0.16839266 -0.24488688 -1.8757663 -2.0953507 -3.0746126 -4.2154975 -5.0958834 -5.8305488][-4.1022525 -3.3388858 -2.7227168 -1.7611079 -1.4223504 -1.1808496 -1.126709 -1.1236382 -1.080853 -2.8980031 -3.5381446 -3.9445655 -4.5180273 -5.346734 -5.8549781][-3.5526934 -3.1510782 -2.5273671 -1.8058648 -1.7290478 -1.8152428 -1.7349024 -1.837203 -1.8048644 -2.7976608 -3.2869496 -4.4603596 -5.1686621 -5.704288 -6.3162112][-4.0344343 -3.7810442 -3.1359305 -2.4348769 -2.2411036 -2.3191752 -2.4179945 -2.2294855 -2.2348094 -3.0804 -3.4683046 -4.5672169 -5.0551996 -5.5522866 -5.9059238][-4.7439556 -4.4993992 -3.8567345 -2.8606658 -2.4165535 -2.2432709 -2.4813576 -2.3851728 -2.0600553 -2.4917021 -2.7576857 -3.524312 -3.9615853 -4.4045668 -4.7427354][-5.389111 -5.2391539 -4.5109882 -3.8710909 -3.6488328 -3.2473636 -3.3851442 -3.3939719 -3.4058247 -3.0292354 -2.9304576 -3.6243987 -4.006907 -4.2454028 -4.43336]]...]
INFO - root - 2017-12-16 02:32:38.285348: step 90610, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 43h:46m:50s remains)
INFO - root - 2017-12-16 02:32:44.713668: step 90620, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 42h:27m:03s remains)
INFO - root - 2017-12-16 02:32:51.126661: step 90630, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:03m:54s remains)
INFO - root - 2017-12-16 02:32:57.635990: step 90640, loss = 0.26, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 41h:51m:26s remains)
INFO - root - 2017-12-16 02:33:04.010888: step 90650, loss = 0.36, batch loss = 0.25 (12.8 examples/sec; 0.627 sec/batch; 42h:05m:37s remains)
INFO - root - 2017-12-16 02:33:10.472794: step 90660, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 42h:59m:39s remains)
INFO - root - 2017-12-16 02:33:16.838959: step 90670, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:18m:20s remains)
INFO - root - 2017-12-16 02:33:23.349900: step 90680, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.642 sec/batch; 43h:09m:23s remains)
INFO - root - 2017-12-16 02:33:29.760431: step 90690, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 43h:49m:08s remains)
INFO - root - 2017-12-16 02:33:36.234669: step 90700, loss = 0.30, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 43h:49m:32s remains)
2017-12-16 02:33:36.792010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0402632 -7.411356 -9.0564528 -10.10133 -10.108272 -10.068792 -8.7736721 -7.2504458 -5.697484 -5.5386257 -5.9895973 -6.61001 -6.8529568 -6.9236927 -6.5814819][-5.3029366 -6.8526516 -8.4674063 -10.13961 -10.94958 -11.091846 -10.296787 -8.7968159 -6.8165913 -6.7486329 -7.8255997 -8.7979708 -8.8599987 -9.0570526 -8.74555][-5.1553264 -6.8573251 -9.0954456 -10.457899 -10.927906 -10.0344 -9.2082672 -8.5343018 -7.4735055 -7.7280431 -8.5431547 -10.037199 -10.906715 -11.030503 -10.481356][-6.5548792 -7.1850529 -8.0698385 -9.04433 -9.7153482 -9.1425228 -7.8719821 -6.07141 -4.8769608 -5.7748122 -7.9253445 -9.8017712 -10.99416 -11.58465 -11.230528][-7.4069047 -7.35639 -7.7343597 -7.6771407 -6.9732203 -5.1444321 -3.1012006 -2.288475 -1.9891315 -3.09627 -5.2512374 -7.9315038 -9.6312828 -10.587799 -10.541791][-6.7740808 -7.1785316 -6.9004703 -5.4640708 -4.2048464 -1.8196173 1.0367613 2.7870541 3.1095428 0.50599861 -2.5010352 -5.6626377 -7.7559495 -9.1595812 -9.5740118][-5.9874864 -5.8794703 -4.3821859 -2.2119632 -0.24289417 2.7290258 4.7981606 5.3668451 5.2265339 2.7401905 -0.99982786 -4.8427563 -7.0301991 -8.1054821 -8.1074982][-6.13502 -5.2562084 -3.151279 -0.59907484 2.1767769 4.784297 6.2031651 6.1201534 5.5491495 2.049839 -1.8396063 -5.0037937 -6.770925 -7.7751112 -7.5134821][-6.6434879 -6.1961675 -5.4409184 -3.2479014 -0.35390472 2.2280579 4.2418976 4.6049948 4.0164747 0.42226219 -3.3191009 -6.4746733 -7.9014564 -8.2909212 -7.6696887][-8.626092 -8.3058643 -7.9215174 -6.149549 -3.7139945 -1.5318975 -0.24152327 -0.23905754 -0.44943953 -3.1966558 -6.1217394 -8.917532 -10.105385 -9.5894995 -8.165947][-10.136073 -10.402933 -10.257172 -9.603014 -8.0111485 -5.8390384 -4.7158918 -4.916544 -5.1976671 -7.0798326 -8.7322512 -10.004721 -10.545829 -10.664309 -10.184212][-11.09561 -11.203415 -10.455596 -10.034384 -9.3904381 -8.479825 -7.5717673 -7.373929 -7.2801604 -8.5356016 -9.520648 -9.5139389 -9.5372438 -9.4658 -9.1724024][-10.201818 -11.147751 -11.135155 -10.446401 -9.6469975 -8.6015825 -7.8137012 -7.3362741 -7.4052496 -8.4509554 -9.7090158 -8.9422121 -8.3708763 -8.0350819 -7.7518892][-9.0945711 -8.85393 -9.304842 -9.009573 -8.3458385 -7.9223509 -7.0965571 -6.9447207 -6.8656378 -6.91147 -8.1159515 -8.1277533 -7.7497892 -6.9602494 -6.085094][-7.3576393 -7.8650546 -8.1165876 -7.6427813 -8.1512022 -7.6220331 -6.9436288 -6.7512383 -6.4397049 -6.8941317 -6.77409 -6.6916623 -7.0203547 -7.2859488 -7.2321162]]...]
INFO - root - 2017-12-16 02:33:43.205651: step 90710, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 42h:44m:38s remains)
INFO - root - 2017-12-16 02:33:49.664285: step 90720, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 44h:34m:50s remains)
INFO - root - 2017-12-16 02:33:56.162257: step 90730, loss = 0.26, batch loss = 0.14 (12.9 examples/sec; 0.622 sec/batch; 41h:45m:35s remains)
INFO - root - 2017-12-16 02:34:02.633524: step 90740, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 41h:56m:11s remains)
INFO - root - 2017-12-16 02:34:09.026404: step 90750, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.628 sec/batch; 42h:08m:55s remains)
INFO - root - 2017-12-16 02:34:15.424338: step 90760, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:33m:59s remains)
INFO - root - 2017-12-16 02:34:21.933560: step 90770, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 43h:21m:40s remains)
INFO - root - 2017-12-16 02:34:28.325914: step 90780, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 42h:15m:20s remains)
INFO - root - 2017-12-16 02:34:34.738006: step 90790, loss = 0.30, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 42h:49m:53s remains)
INFO - root - 2017-12-16 02:34:41.209255: step 90800, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 43h:26m:57s remains)
2017-12-16 02:34:41.724137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0450749 -3.6918442 -3.1969571 -3.136332 -2.080297 -2.0381141 -1.8768005 -1.8241014 -1.060873 -2.2121592 -3.8768172 -4.9565816 -6.4772034 -7.7502413 -8.6925344][-2.7816215 -2.78856 -2.8435998 -2.9672332 -2.850091 -2.7771249 -2.4546232 -1.9613905 -1.8992882 -3.3459272 -4.1633334 -4.8175015 -6.5217285 -7.540473 -8.3862886][-3.2298613 -3.1259041 -2.8879609 -2.4676528 -2.0862041 -1.0505538 -0.45082331 -0.66030455 -1.0038548 -2.2139497 -3.5358348 -4.559319 -5.3930721 -6.2170529 -7.4784689][-4.5247173 -3.503788 -2.8335752 -2.167345 -1.7867451 -1.0969539 -0.31973743 0.11797619 0.36508846 -1.5487623 -2.8606668 -3.3557668 -4.4590273 -5.50893 -6.1608219][-4.604578 -3.3992958 -1.815371 -1.1594696 -0.90753508 -0.44654655 0.33750343 0.24533987 0.17753839 -0.9887104 -1.7107391 -2.8030057 -4.4826536 -5.6632652 -6.2931757][-4.0336075 -2.9503374 -1.7908525 -0.4086256 1.079566 1.7922983 2.1533003 1.7349329 1.2959261 -0.31172657 -1.3246093 -1.8682022 -3.0481682 -4.327034 -5.1941233][-3.9646854 -2.6602159 -0.78333044 0.66945267 1.56075 2.6361456 3.1905441 2.9144154 2.2528629 0.29310703 -0.68828535 -1.3658991 -2.4484882 -3.5396781 -4.4111214][-3.2123423 -1.2391629 0.60063362 2.3401842 3.2411261 4.0845404 3.6930428 3.2337236 2.9377842 0.86439705 -0.35293293 -0.91537762 -1.6911616 -2.3790822 -3.599164][-0.99304724 0.19593573 1.41611 2.7889347 3.0230589 2.8971262 2.732255 2.3817005 2.118269 0.59528351 0.36092186 -0.73055267 -2.3664427 -2.8002939 -3.2768583][-1.1753345 0.12260199 0.46603489 1.0030527 1.1017485 1.1691284 0.69480228 0.4749794 0.83850574 -0.30163288 -1.0298281 -1.0717816 -2.0039034 -2.9159203 -3.7366288][-1.6913466 -1.5138168 -1.8304954 -1.8477521 -2.0002666 -2.1574626 -2.0088611 -1.7454853 -1.3025193 -2.06663 -2.20824 -2.5959315 -4.0894442 -5.1166987 -5.7699623][-2.9933619 -3.1271248 -3.6577134 -4.3141832 -4.5822763 -4.3860035 -3.9431489 -4.2666802 -4.1898937 -4.6081085 -4.9866261 -4.9366713 -5.3640261 -5.8771582 -6.3901596][-3.2514815 -3.990901 -5.6527085 -6.2896771 -6.7574043 -6.6711078 -6.2007704 -5.6352835 -5.5165682 -6.4708753 -6.8443518 -6.8590212 -7.6264062 -7.977921 -7.5689526][-4.4952459 -5.2032843 -6.01979 -6.7135124 -7.3525357 -6.7768412 -6.0459943 -6.1181273 -6.295435 -6.4905658 -6.9299054 -7.523654 -8.0308332 -8.062602 -8.460825][-4.6744909 -5.3854737 -6.7611136 -7.5935597 -8.19217 -7.884356 -7.4374561 -6.7737851 -6.0991216 -6.5013981 -7.0315342 -7.3974857 -7.6901813 -8.0628843 -8.06167]]...]
INFO - root - 2017-12-16 02:34:48.153932: step 90810, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 44h:17m:43s remains)
INFO - root - 2017-12-16 02:34:54.589518: step 90820, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 42h:16m:57s remains)
INFO - root - 2017-12-16 02:35:00.991613: step 90830, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 44h:16m:20s remains)
INFO - root - 2017-12-16 02:35:07.448407: step 90840, loss = 0.28, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 44h:43m:58s remains)
INFO - root - 2017-12-16 02:35:13.800055: step 90850, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 42h:03m:18s remains)
INFO - root - 2017-12-16 02:35:20.270375: step 90860, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.628 sec/batch; 42h:09m:07s remains)
INFO - root - 2017-12-16 02:35:26.676596: step 90870, loss = 0.27, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 43h:51m:10s remains)
INFO - root - 2017-12-16 02:35:33.052038: step 90880, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 42h:33m:24s remains)
INFO - root - 2017-12-16 02:35:39.476716: step 90890, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 43h:34m:34s remains)
INFO - root - 2017-12-16 02:35:45.817939: step 90900, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 42h:06m:51s remains)
2017-12-16 02:35:46.365615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1526823 -5.9199977 -4.3976445 -2.6955762 -1.7152443 -1.1445627 -1.481709 -1.6833391 -1.9570575 -3.8579507 -4.5679216 -5.7489915 -5.6495667 -6.8345213 -7.8098493][-6.0630889 -5.3053594 -4.2225189 -2.9513612 -1.9033394 -1.1563101 -1.8405609 -2.3501835 -2.3932872 -3.8811343 -4.5632463 -6.947722 -7.3833461 -7.5924048 -7.6685505][-4.8963103 -4.1579561 -3.0448213 -1.9994931 -1.3608246 -0.67362642 -0.57219839 -0.832129 -1.4882627 -2.8083925 -3.1945662 -5.035521 -5.6905203 -6.5589213 -6.9937496][-3.8890178 -2.5039783 -1.5071492 -0.84108496 -0.37069035 0.25251913 0.026910305 -0.33815861 -0.49119473 -1.6644187 -2.5223856 -4.5988646 -5.1204147 -5.6615057 -6.35743][-2.6650906 -1.4874797 -0.58045292 0.14071417 0.3935461 0.84345627 1.0333643 0.23431253 -0.39607811 -1.6905279 -2.3131709 -4.1871428 -4.9586687 -5.6194773 -6.183671][-1.8229628 -0.83577585 0.12443924 0.67805004 1.0803061 1.2292881 1.045476 0.48694611 0.31920433 -0.8031745 -1.7546453 -3.3877802 -3.906188 -4.6222248 -5.5823612][-1.5840321 -0.16203308 1.0413237 1.363349 1.6842117 1.5192633 1.2083435 0.8728714 0.48690128 -0.82355785 -1.7984376 -3.7234926 -4.4680252 -5.0421309 -5.65218][-1.4686279 -0.25537682 0.79805565 1.5266695 1.7898932 1.7561893 1.4227686 0.87501717 0.36718559 -0.74954271 -1.5811772 -3.7210722 -4.9805808 -5.5440521 -5.9773893][-3.6306772 -2.10077 -0.50443506 0.62949181 1.0254831 0.98702431 0.60812187 0.56970119 0.35708046 -0.94930792 -1.8111396 -3.7371118 -4.9636774 -5.6648254 -6.2604103][-3.8221753 -3.6203632 -3.0301261 -1.7905502 -1.1948342 -0.71363878 -0.665288 -0.77240705 -0.91695929 -2.1718154 -2.7525783 -3.7316561 -4.6065278 -5.3837833 -5.9459953][-4.7196865 -4.8759403 -4.7286386 -4.0109887 -3.3892598 -2.781795 -2.3352709 -2.2051778 -2.2386408 -3.8910041 -4.265173 -4.7200937 -5.6452484 -6.0240579 -6.164669][-5.9240551 -5.442749 -5.3211856 -5.114995 -4.7876377 -4.3940296 -4.0971966 -3.7910123 -3.5050273 -4.6928449 -5.1111612 -5.1231661 -6.1670065 -6.7049427 -6.4344687][-6.8328 -6.92276 -6.7688327 -6.5574617 -6.1454573 -5.6991663 -5.5596657 -4.9419003 -4.1538672 -4.2673163 -4.6839666 -4.7204537 -5.6625671 -5.8128743 -5.8976836][-6.8499932 -6.8273463 -6.685689 -6.5076261 -6.0874019 -5.4632368 -5.1595058 -5.1784458 -5.118577 -4.6037865 -4.9085689 -4.4530516 -5.0121202 -5.2945929 -5.2999763][-6.3627892 -6.132493 -6.0865479 -6.5066915 -6.3109479 -5.4886522 -5.3082905 -5.7566981 -6.2946963 -5.7109861 -5.66412 -5.4759288 -5.8460369 -5.5104618 -5.0877075]]...]
INFO - root - 2017-12-16 02:35:52.766223: step 90910, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 42h:56m:17s remains)
INFO - root - 2017-12-16 02:35:59.202844: step 90920, loss = 0.30, batch loss = 0.18 (12.2 examples/sec; 0.653 sec/batch; 43h:50m:04s remains)
INFO - root - 2017-12-16 02:36:05.611969: step 90930, loss = 0.26, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 42h:20m:52s remains)
INFO - root - 2017-12-16 02:36:11.948803: step 90940, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 43h:23m:12s remains)
INFO - root - 2017-12-16 02:36:18.287053: step 90950, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 43h:07m:09s remains)
INFO - root - 2017-12-16 02:36:24.714136: step 90960, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 43h:53m:28s remains)
INFO - root - 2017-12-16 02:36:31.168491: step 90970, loss = 0.30, batch loss = 0.19 (12.0 examples/sec; 0.666 sec/batch; 44h:39m:00s remains)
INFO - root - 2017-12-16 02:36:37.594307: step 90980, loss = 0.26, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 43h:58m:10s remains)
INFO - root - 2017-12-16 02:36:44.066520: step 90990, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:01m:56s remains)
INFO - root - 2017-12-16 02:36:50.523354: step 91000, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 43h:26m:51s remains)
2017-12-16 02:36:51.051496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2381916 -5.5516672 -5.4038653 -5.3263969 -4.6630926 -3.7973061 -2.7777772 -1.9079452 -0.99494743 -0.89854336 -1.9078145 -2.525773 -3.2204185 -5.002892 -6.1909757][-4.3555121 -4.7378693 -4.6361971 -4.7667885 -4.4807692 -3.5610127 -3.1144185 -2.7021475 -1.4347157 -1.1673336 -2.2834139 -2.814168 -3.6165066 -5.1317921 -5.8347878][-3.1562881 -3.2426271 -3.1019778 -3.3033204 -3.3771639 -2.8908019 -2.4173746 -1.7712278 -0.83802509 -0.35073662 -1.6288924 -2.4625292 -3.2093592 -4.6541014 -5.6097107][-3.466188 -2.8534431 -2.4180307 -2.603518 -2.236578 -1.914155 -1.300704 -0.50694561 0.019410133 -0.1279068 -1.1892176 -1.9903007 -2.8279309 -4.186728 -5.7100496][-3.7853844 -2.6528907 -1.5238185 -1.4824529 -1.2341661 -0.90838337 -0.39946175 -0.077116013 0.4742775 0.13305807 -1.1876121 -1.9990077 -2.7992449 -4.4438076 -5.4550757][-3.4336472 -2.636116 -2.1378245 -1.4306679 -0.70196009 -0.099395275 0.68550682 0.93057346 1.1923552 0.682806 -0.70910215 -1.7032986 -2.5945024 -4.3094749 -5.5836477][-3.8974049 -3.0874572 -2.3108087 -1.5238857 -0.74013519 -0.2750206 0.26790428 0.74771595 1.2487612 0.72453213 -0.86623049 -1.7203417 -2.7195725 -4.7505779 -5.9410629][-4.23665 -3.633091 -2.8415031 -2.112494 -1.2809148 -0.60080004 0.16238594 0.42025757 0.83688164 0.7114563 -0.84233093 -1.9724026 -3.0084453 -4.7844582 -5.7654963][-3.6907177 -3.0892887 -2.5013633 -1.8050957 -1.3072391 -0.70945406 0.28470612 0.60648537 0.81871319 0.69259453 -0.66814423 -1.8989015 -3.2060723 -4.781146 -5.3903093][-3.3159385 -2.769155 -2.4131026 -1.6079473 -0.6775136 -0.24167347 0.45600605 0.59472179 0.69146252 0.11876345 -1.2386847 -2.0369778 -2.7696004 -3.8629107 -4.4057755][-5.0412703 -4.301897 -3.6350369 -3.0410752 -2.563755 -1.9777546 -1.2037559 -1.0837421 -1.0056443 -1.7717056 -2.8018713 -3.1474648 -3.6238489 -4.1710405 -4.1910753][-5.9874024 -5.5918775 -4.9566374 -4.5234394 -4.2562113 -3.5349374 -2.6923342 -2.5999393 -2.6160073 -2.8676214 -3.2486434 -3.2041564 -3.5754285 -3.9835591 -3.9493389][-6.7327938 -6.5723624 -6.1209044 -6.0481811 -5.9494095 -5.3186903 -4.5601673 -4.3603268 -4.1047487 -3.9052529 -3.7168033 -3.2228079 -3.197217 -3.33042 -3.1606526][-7.0340724 -7.2362347 -6.9718928 -7.28054 -7.5334344 -7.0179381 -6.3486047 -6.002861 -5.2988386 -4.767478 -4.5272012 -4.0314264 -3.3274121 -3.2308307 -3.1371212][-7.7806339 -7.8602939 -7.6895275 -8.1187334 -8.5149689 -8.3773232 -8.0368938 -7.7366896 -6.9900904 -6.2792826 -5.5027976 -4.862143 -4.3439555 -4.1234331 -3.7728577]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:36:57.439522: step 91010, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 42h:50m:21s remains)
INFO - root - 2017-12-16 02:37:03.864863: step 91020, loss = 0.35, batch loss = 0.23 (12.9 examples/sec; 0.618 sec/batch; 41h:27m:23s remains)
INFO - root - 2017-12-16 02:37:10.292179: step 91030, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 43h:19m:40s remains)
INFO - root - 2017-12-16 02:37:16.677580: step 91040, loss = 0.25, batch loss = 0.14 (13.1 examples/sec; 0.610 sec/batch; 40h:56m:50s remains)
INFO - root - 2017-12-16 02:37:23.100588: step 91050, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 43h:56m:06s remains)
INFO - root - 2017-12-16 02:37:29.601907: step 91060, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 43h:55m:46s remains)
INFO - root - 2017-12-16 02:37:36.101412: step 91070, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 43h:03m:11s remains)
INFO - root - 2017-12-16 02:37:42.512231: step 91080, loss = 0.25, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 43h:08m:43s remains)
INFO - root - 2017-12-16 02:37:48.981103: step 91090, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:12m:11s remains)
INFO - root - 2017-12-16 02:37:55.350131: step 91100, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 43h:32m:49s remains)
2017-12-16 02:37:55.884436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3123183 -4.8042488 -5.357933 -5.5569386 -5.2646189 -4.602478 -3.9776349 -3.7721884 -3.3327641 -3.6501012 -4.5162811 -6.2578421 -7.024044 -7.5391521 -8.545188][-4.2051015 -4.8972464 -5.2121634 -5.4218426 -5.6078157 -5.3011322 -5.1967106 -5.0402541 -4.563962 -5.0145979 -5.9917693 -7.03094 -7.6837831 -8.1006088 -8.6643639][-4.3079319 -4.8088346 -5.2511964 -4.9194393 -5.0335426 -4.4737034 -4.5716848 -4.6914625 -4.7775826 -4.9440684 -5.4754968 -6.343071 -6.6316605 -7.581193 -8.1483479][-4.947525 -5.0028214 -4.7558613 -4.239542 -3.46384 -2.5880814 -2.3230653 -2.5576591 -3.0970306 -3.8913877 -4.7184711 -5.6769776 -5.4017649 -6.3230247 -6.6685581][-5.5048676 -5.4294462 -4.6296625 -3.2496557 -2.3752608 -1.0929294 0.10705996 0.26512527 -0.52605104 -1.7029071 -2.8691287 -4.3711205 -4.3582487 -4.4898615 -5.2835846][-5.8805737 -5.4207859 -4.0780811 -2.4006639 -0.75260067 0.77805424 1.7887144 2.1329308 1.9121895 0.82344437 -0.58356142 -2.2247939 -3.0914192 -3.2913456 -3.8387311][-5.5891514 -5.0446048 -4.0397625 -1.8796463 0.058850765 1.9372845 3.319313 2.9301071 2.7562923 2.1044741 0.79850006 -1.0235219 -1.9823589 -2.8666892 -3.740474][-5.6116829 -5.4520969 -3.9705737 -2.0842538 -0.024282932 1.6921101 3.0396185 2.9095802 2.955801 2.2491236 0.99247074 -0.92233562 -2.1889949 -3.6365447 -4.5663681][-5.658123 -5.4007974 -4.6687608 -2.8575516 -1.0615711 0.4141016 1.9804134 2.1691656 2.4176512 2.1387062 0.96398163 -0.94425249 -2.2662539 -3.5574074 -5.2565031][-6.4510078 -6.6032505 -5.9774113 -4.3406515 -2.6938572 -1.151082 0.45315552 0.8386097 1.5597401 1.376749 0.44232082 -1.1596427 -2.658772 -3.6064625 -4.9431076][-6.9323945 -6.9461603 -6.82344 -5.8852024 -4.6033459 -3.7796881 -2.3646746 -1.8607969 -1.0045481 -0.60915756 -0.84133387 -1.7963605 -2.4548745 -3.3752599 -4.9158449][-7.4119596 -6.275898 -6.5966997 -7.0204229 -6.3127213 -5.5776672 -4.739522 -4.2256203 -3.8388247 -3.5285244 -3.2024298 -3.3722453 -3.4332166 -4.0540628 -4.7076941][-9.0009031 -7.9139357 -7.6268907 -7.5360789 -7.3133645 -7.0536928 -6.3065109 -6.1721921 -6.0456238 -5.7827888 -4.8655386 -4.3693104 -4.2005973 -4.4017591 -4.9275494][-7.5565314 -7.4697008 -7.3238845 -7.1707139 -6.6433477 -6.7891293 -6.5298758 -6.4084139 -6.3926449 -6.27337 -5.7276258 -4.9935112 -4.3041277 -4.3189311 -4.9162803][-8.0438519 -8.0995359 -8.3974171 -7.9166365 -7.5624251 -7.8295112 -7.6922393 -7.5805826 -7.6619916 -7.3087478 -6.8411241 -6.2073812 -5.672472 -5.3059897 -5.146596]]...]
INFO - root - 2017-12-16 02:38:02.207436: step 91110, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 42h:20m:09s remains)
INFO - root - 2017-12-16 02:38:08.669551: step 91120, loss = 0.25, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 44h:19m:47s remains)
INFO - root - 2017-12-16 02:38:15.094076: step 91130, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.643 sec/batch; 43h:04m:46s remains)
INFO - root - 2017-12-16 02:38:21.519348: step 91140, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.619 sec/batch; 41h:29m:30s remains)
INFO - root - 2017-12-16 02:38:27.945879: step 91150, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 43h:32m:14s remains)
INFO - root - 2017-12-16 02:38:34.340439: step 91160, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 43h:46m:16s remains)
INFO - root - 2017-12-16 02:38:40.732960: step 91170, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.615 sec/batch; 41h:13m:42s remains)
INFO - root - 2017-12-16 02:38:47.158683: step 91180, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 43h:39m:40s remains)
INFO - root - 2017-12-16 02:38:53.671170: step 91190, loss = 0.24, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 44h:16m:30s remains)
INFO - root - 2017-12-16 02:39:00.062546: step 91200, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 43h:49m:56s remains)
2017-12-16 02:39:00.601015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.527112 -5.5986004 -5.6729441 -5.4684806 -5.4487896 -4.8127551 -4.0281916 -3.4107723 -2.335413 -1.5570116 -2.7653098 -4.7907639 -6.0206695 -6.6831665 -7.831358][-4.4215918 -5.4401574 -5.9432497 -6.7857919 -6.8462596 -6.2182446 -5.632606 -4.6039405 -3.4414487 -3.1825109 -4.3125134 -6.332118 -7.0587959 -8.0112123 -8.4385376][-4.2976875 -4.6732473 -5.3054919 -5.6664562 -5.9330797 -6.0566511 -5.1099925 -4.2749853 -3.4248633 -2.7972946 -3.6489086 -5.9924326 -6.8437219 -7.8482862 -8.3475113][-3.7713275 -4.2613516 -4.1090012 -3.4789248 -3.2326937 -3.3408222 -3.1762271 -2.887578 -2.6534657 -2.3620772 -3.5642095 -5.6018238 -6.0478916 -6.6183953 -7.1824017][-3.3713841 -3.1371789 -2.4743457 -1.7478886 -1.4870863 -1.1040702 -0.77372456 -0.81090212 -0.76892567 -0.7878828 -2.6392331 -4.8267727 -5.599432 -6.1116996 -6.1805835][-1.2038069 -1.3981776 -1.2297969 -0.29140997 0.029750347 0.3123188 0.37062454 0.35482311 0.39489174 0.28136539 -1.4633007 -3.3156238 -3.6455512 -4.5759926 -4.9221778][-2.5178366 -1.5290623 -0.59235287 0.20259047 0.80621624 1.4335823 1.8849306 2.0000029 1.790163 0.97443295 -1.0693846 -3.3174248 -3.7580011 -4.8361435 -5.3036871][-2.6247363 -1.5422578 -0.74306059 0.56050777 1.3533192 1.63274 1.8210087 2.1087809 2.1458683 1.6911812 0.24177265 -2.4956932 -3.547821 -4.6789265 -5.3606224][-3.3337717 -2.6197605 -1.4257078 -0.5111227 0.63918877 1.2977581 1.3515205 1.2106342 0.8850975 0.75784779 -0.56755447 -2.9648929 -3.9019768 -4.7726822 -5.4948425][-4.4372964 -3.2553539 -2.4746552 -1.5860801 -1.1376972 -0.034834385 0.4531765 0.38572884 0.16303539 -0.37147951 -1.7874904 -4.0245 -4.8428583 -5.2694464 -5.2957878][-5.1400423 -4.544837 -3.2159615 -2.9384084 -2.3893437 -1.1976514 -0.51756811 -0.87426662 -1.2703772 -1.9434676 -3.3921952 -5.4667711 -5.7732596 -6.3370314 -6.2332916][-6.3121614 -5.3115721 -4.9149218 -4.2604246 -3.6435671 -3.0727019 -2.4534187 -2.236587 -2.1277018 -2.6468945 -3.6760278 -5.2828374 -6.0375419 -6.0738692 -5.9550838][-6.55641 -6.577529 -6.2293181 -5.6089592 -5.3742542 -4.8719664 -4.2359266 -4.1979585 -4.3716612 -4.5194988 -4.5989313 -5.889 -6.4110575 -6.663075 -6.0346713][-7.45734 -7.1274323 -6.3798304 -5.9767208 -5.7987742 -5.0677552 -4.6088753 -4.6179628 -4.6937275 -4.9619231 -5.0044174 -5.8363028 -6.1568131 -5.6254425 -5.0597687][-8.5908747 -8.3381147 -8.1500835 -7.660459 -7.355155 -6.7582674 -6.5100513 -6.6841478 -6.7827334 -6.9329634 -7.2758512 -7.5252004 -7.7267866 -7.2465577 -6.9389915]]...]
INFO - root - 2017-12-16 02:39:07.016384: step 91210, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 43h:16m:47s remains)
INFO - root - 2017-12-16 02:39:13.427350: step 91220, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.627 sec/batch; 42h:03m:14s remains)
INFO - root - 2017-12-16 02:39:19.791012: step 91230, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 42h:37m:47s remains)
INFO - root - 2017-12-16 02:39:26.222182: step 91240, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 43h:13m:53s remains)
INFO - root - 2017-12-16 02:39:32.655161: step 91250, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 43h:07m:28s remains)
INFO - root - 2017-12-16 02:39:38.960688: step 91260, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 41h:41m:33s remains)
INFO - root - 2017-12-16 02:39:45.352847: step 91270, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 42h:09m:30s remains)
INFO - root - 2017-12-16 02:39:51.745865: step 91280, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.632 sec/batch; 42h:19m:29s remains)
INFO - root - 2017-12-16 02:39:58.199614: step 91290, loss = 0.36, batch loss = 0.24 (12.6 examples/sec; 0.632 sec/batch; 42h:22m:44s remains)
INFO - root - 2017-12-16 02:40:04.575502: step 91300, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 42h:04m:05s remains)
2017-12-16 02:40:05.263751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5095544 -6.3130908 -7.1100373 -7.1842823 -7.1929126 -6.7566814 -6.3515968 -5.4538646 -4.7147 -4.9780111 -6.8847947 -6.9999385 -7.7221026 -8.5697842 -8.2069311][-4.5844307 -5.2633495 -5.3460231 -5.981041 -6.191227 -5.7423468 -5.4529896 -4.9608021 -4.712481 -4.794961 -6.4003448 -6.115355 -6.7972279 -7.6138973 -7.5640645][-2.8343701 -3.2882195 -4.0239248 -4.3725781 -3.9597096 -4.2949109 -4.354888 -4.0918984 -3.81206 -3.9040279 -5.8610525 -5.3081832 -5.5886407 -6.4180741 -6.3760939][-2.6310606 -2.6147556 -2.7026258 -2.7773852 -2.9642344 -2.9603429 -2.5704346 -2.9279189 -3.053452 -3.3513618 -4.903409 -4.3698473 -4.8218141 -5.6347413 -5.8695455][-1.4864922 -1.7257628 -1.75278 -1.8385372 -1.5787053 -1.0522556 -0.98322439 -1.2505097 -1.1371484 -2.02886 -3.6779146 -3.2631755 -3.6866193 -4.5179191 -4.9604721][-0.62730455 -0.84305096 -0.99262333 -0.670043 -0.35255146 -0.0079665184 0.38398361 0.5223074 0.69633007 -0.082509518 -1.7462554 -1.7868104 -2.6334686 -3.6717086 -4.0082989][-1.3753948 -1.0565505 -1.444499 -1.1835546 -0.42148066 0.36086941 0.90705013 0.83947182 0.91336346 0.26817179 -1.2281199 -1.1795068 -2.4691916 -3.3738561 -3.812108][-1.9145708 -1.7770791 -1.5638356 -0.90453625 -0.36497736 0.4673996 1.0793562 1.0364189 1.1172552 0.34568691 -1.4212337 -1.7527575 -2.8725448 -4.0008011 -4.6722541][-2.2819128 -2.0119443 -1.7397575 -1.2105703 -0.23214054 0.73494244 1.5252314 1.5259247 1.6178732 0.81689548 -1.1028466 -1.7236452 -3.0617962 -4.3640871 -4.7203417][-4.8117452 -3.9304821 -3.0393953 -2.2492185 -1.0439625 0.21880293 0.87786293 1.150095 1.4722958 0.45430946 -1.4842868 -1.880198 -2.6680546 -4.0659032 -4.5726924][-8.6440468 -7.6985025 -6.3683567 -4.8642263 -3.5897617 -2.2431912 -1.2063518 -1.0577059 -0.93271828 -1.4047127 -3.0064211 -3.4294581 -3.9389732 -4.8287396 -5.0370131][-9.4006042 -8.7679138 -7.5601821 -6.519887 -5.5428705 -4.3060656 -3.0930829 -3.0433855 -2.5412455 -2.838572 -4.0279217 -4.3208246 -4.5035028 -5.2693486 -5.5164328][-9.4778852 -9.4738913 -8.4236574 -7.6057515 -7.07676 -5.96325 -4.8262925 -4.3675804 -3.9213922 -4.0854673 -4.7904139 -4.7086544 -4.5318117 -4.831707 -4.7288036][-7.9105873 -8.4390593 -8.0072165 -7.487464 -7.21084 -6.6991086 -6.1711469 -5.8528066 -5.4902763 -5.13369 -5.5569954 -5.4612503 -5.017561 -4.8630724 -4.8789468][-8.07039 -8.0201445 -7.8216977 -8.0088 -8.0173922 -7.6685715 -7.2248964 -7.0339704 -6.9848876 -6.8882575 -6.7038546 -6.5902033 -6.4733152 -6.297965 -5.6980505]]...]
INFO - root - 2017-12-16 02:40:11.814031: step 91310, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 42h:56m:48s remains)
INFO - root - 2017-12-16 02:40:18.302030: step 91320, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 44h:12m:48s remains)
INFO - root - 2017-12-16 02:40:24.712847: step 91330, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 43h:00m:17s remains)
INFO - root - 2017-12-16 02:40:31.098748: step 91340, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 42h:23m:44s remains)
INFO - root - 2017-12-16 02:40:37.465483: step 91350, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 42h:16m:43s remains)
INFO - root - 2017-12-16 02:40:43.891822: step 91360, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.624 sec/batch; 41h:48m:48s remains)
INFO - root - 2017-12-16 02:40:50.232867: step 91370, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 42h:25m:41s remains)
INFO - root - 2017-12-16 02:40:56.741637: step 91380, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 42h:28m:30s remains)
INFO - root - 2017-12-16 02:41:03.179018: step 91390, loss = 0.28, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 44h:19m:06s remains)
INFO - root - 2017-12-16 02:41:09.642659: step 91400, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 42h:55m:26s remains)
2017-12-16 02:41:10.211981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18645 -4.8819981 -5.4872103 -5.035635 -4.2276826 -4.3498292 -3.2412205 -2.6634216 -3.2037168 -3.7225807 -5.5416155 -4.1816082 -5.8552322 -5.0152559 -4.4803925][-5.6653776 -4.507844 -2.9951544 -3.4741845 -3.6363893 -3.7826259 -4.495697 -4.34038 -1.5456338 -1.4035974 -4.397521 -4.7262683 -7.0202637 -4.9483624 -4.4593925][-2.3932581 -3.4503837 -3.4029489 -1.6463876 -1.4623585 -1.137867 0.2616291 -0.14856339 -2.0555029 -2.5873322 -2.9679556 -1.7571912 -4.8603663 -5.2328758 -4.7255163][-1.6389112 -0.55168486 -2.1770988 -2.4250989 -1.2770848 -0.1596055 -0.097301483 -0.28151846 1.3298674 0.15099144 -3.0834217 -3.6800475 -4.9686756 -3.9172153 -3.621892][-5.2115173 -2.6472607 0.25792885 -0.015473366 -0.87735891 -0.8435812 0.38192654 -0.23451662 -0.20959234 -0.892231 -2.597156 -2.9232235 -5.2035861 -5.0504351 -4.7026863][-2.6091766 -2.7824426 -2.9795175 -1.0475988 2.6830873 3.605361 3.5325947 2.2410936 1.5026865 0.13765574 -2.2005286 -2.9478889 -4.930459 -4.3489089 -4.8293629][-2.0192637 -1.3622484 0.5766468 0.77289963 1.2343798 2.7043314 4.7083015 5.0187788 4.4056921 1.8562002 -1.6417294 -2.3130913 -4.7472148 -4.3923445 -3.7085512][-0.68967104 -0.96475172 -0.6192708 2.1900635 3.7881289 3.9256868 3.6647892 3.3417282 4.4302979 2.907053 -1.07721 -1.5872011 -4.6679626 -4.8630123 -5.0545287][-1.5010376 -1.0718751 -0.063393593 1.289361 3.4232798 4.763835 4.2750254 3.4395161 3.3916969 2.3830872 -0.13426685 -0.44728565 -3.7107713 -4.0208364 -4.5125303][-3.4168224 -3.2217026 -1.6790314 -0.62025166 0.27233934 1.7978916 2.4061966 3.0097141 3.7112646 2.1470776 -0.24178696 -0.56991959 -3.024518 -3.2033973 -4.247025][-5.2456951 -4.6739841 -4.1040392 -4.1437383 -2.8456249 -2.1880298 -0.96999121 -1.3622808 -0.91555691 -0.059903622 -1.8665705 -2.2443953 -3.9057591 -4.3096538 -4.8553848][-7.3195224 -7.1555605 -6.0186329 -5.8670616 -5.4438162 -5.4010439 -4.28427 -3.621913 -3.6650372 -2.85534 -4.1365938 -4.5079341 -6.3300357 -5.9939928 -6.16923][-5.7756834 -7.4103522 -7.925015 -7.8973451 -6.7101917 -5.1166549 -4.8233838 -4.6489611 -4.135149 -3.3972387 -4.8070307 -4.5380807 -4.7289009 -5.482307 -5.9296293][-6.1496134 -5.4821968 -5.4332342 -6.60024 -7.2381206 -6.9773693 -5.62577 -5.0949125 -5.3215652 -5.795496 -5.7468686 -5.022862 -6.0946584 -5.9001551 -6.1612515][-4.3910208 -5.401639 -5.1454711 -5.1607084 -5.0224085 -5.9951048 -6.4524674 -6.0150695 -5.5886135 -6.1111488 -6.1643915 -6.7078238 -6.9942565 -6.4283762 -6.3752575]]...]
INFO - root - 2017-12-16 02:41:16.578569: step 91410, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 42h:57m:48s remains)
INFO - root - 2017-12-16 02:41:23.033051: step 91420, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.649 sec/batch; 43h:26m:42s remains)
INFO - root - 2017-12-16 02:41:29.536552: step 91430, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 42h:26m:28s remains)
INFO - root - 2017-12-16 02:41:36.019812: step 91440, loss = 0.29, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 44h:06m:26s remains)
INFO - root - 2017-12-16 02:41:42.395802: step 91450, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 42h:11m:14s remains)
INFO - root - 2017-12-16 02:41:48.868175: step 91460, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.614 sec/batch; 41h:05m:17s remains)
INFO - root - 2017-12-16 02:41:55.231730: step 91470, loss = 0.25, batch loss = 0.14 (12.9 examples/sec; 0.621 sec/batch; 41h:32m:50s remains)
INFO - root - 2017-12-16 02:42:01.637523: step 91480, loss = 0.30, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 44h:18m:08s remains)
INFO - root - 2017-12-16 02:42:08.149757: step 91490, loss = 0.26, batch loss = 0.15 (11.7 examples/sec; 0.681 sec/batch; 45h:36m:06s remains)
INFO - root - 2017-12-16 02:42:14.586559: step 91500, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.640 sec/batch; 42h:52m:26s remains)
2017-12-16 02:42:15.105670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6502666 -5.2618127 -6.8958945 -8.3959751 -8.913415 -8.0100012 -6.6886787 -5.0467625 -3.869401 -3.7727027 -4.6382942 -6.5338964 -8.2509556 -8.39166 -7.4704127][-4.3771296 -5.4091635 -7.2720966 -9.0377655 -9.1822281 -8.3141832 -7.0276537 -5.5823507 -4.8369136 -4.6642647 -5.6230626 -7.6511655 -9.5302429 -9.7218 -8.9858141][-3.8737276 -5.4590387 -7.3749571 -9.0009212 -9.1050425 -7.68271 -5.879159 -4.5806417 -4.1564817 -4.5925689 -5.90856 -8.1178007 -10.061832 -10.309412 -9.537179][-3.9427633 -5.7219505 -7.5851536 -8.853 -8.7745924 -6.8278418 -4.5677729 -3.1473246 -2.1867208 -2.8900061 -4.9204364 -7.9199781 -10.063061 -10.230857 -9.5548906][-4.6651297 -5.9500308 -7.1375523 -7.5767207 -6.8210244 -4.8988886 -2.609015 -0.99158764 -0.50471973 -1.8468075 -4.1773462 -7.5071177 -9.9232779 -10.356195 -9.7618961][-7.3645539 -8.0379267 -8.1240206 -7.3570976 -5.0608778 -2.1679926 0.47678375 1.7658739 1.731245 -0.36445332 -3.5622578 -6.820528 -9.5548773 -10.080934 -9.6833677][-8.2266817 -8.5907707 -7.8369651 -6.1806569 -2.9099941 0.99813652 4.7462664 5.8688078 4.9178238 1.9664192 -2.0446382 -6.3293176 -9.0238924 -9.4833136 -8.73938][-7.411149 -7.6063819 -6.9386773 -4.3691778 -0.80336857 3.5649023 6.8773584 7.44627 6.7668724 3.5023355 -0.95582676 -5.1278453 -8.0167894 -8.5270958 -7.9977031][-7.7628455 -7.3864427 -6.7079916 -4.4136391 -1.0867586 2.2045679 5.1694078 5.8926096 4.9647341 2.5971804 -1.2384129 -5.2430029 -8.2679787 -8.8335867 -8.288846][-9.41343 -9.3001661 -8.5935507 -6.6348534 -3.7378378 -0.63651609 1.3963394 2.0986986 1.7264233 -0.93178654 -3.6093326 -6.6974983 -9.2124958 -9.7955456 -8.8668509][-10.568928 -10.887579 -10.663626 -9.0070076 -6.9376807 -4.3764353 -2.8012977 -2.2170682 -2.3899379 -4.4468164 -6.6239538 -9.0498371 -10.189372 -10.197194 -9.180851][-9.5585394 -10.196892 -10.41955 -9.2038574 -7.856143 -6.1269388 -5.1369791 -4.9943504 -5.08144 -6.3787937 -7.1706562 -8.9769945 -9.7330294 -9.59697 -8.6278925][-10.412837 -10.121134 -9.900918 -9.0019817 -8.2360144 -7.5163765 -7.1522641 -7.2134604 -7.106205 -8.46789 -9.1045837 -9.448103 -9.4437742 -9.2944975 -8.3531628][-9.0423765 -9.2250519 -8.4073811 -7.4934607 -6.937387 -6.2992735 -6.6168256 -7.3615642 -7.9011312 -8.1004448 -8.0327845 -8.0615063 -7.9927759 -7.8093977 -7.3319759][-8.8492565 -8.7130861 -8.2595034 -7.4151258 -6.3268666 -6.0568681 -6.7562728 -7.2937765 -7.8535028 -8.2318583 -8.0909128 -7.1023946 -6.3207178 -5.9347582 -5.89561]]...]
INFO - root - 2017-12-16 02:42:21.567139: step 91510, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 42h:12m:44s remains)
INFO - root - 2017-12-16 02:42:28.021761: step 91520, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 43h:00m:24s remains)
INFO - root - 2017-12-16 02:42:34.458336: step 91530, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:25m:25s remains)
INFO - root - 2017-12-16 02:42:40.874809: step 91540, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 42h:31m:16s remains)
INFO - root - 2017-12-16 02:42:47.243106: step 91550, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 43h:16m:17s remains)
INFO - root - 2017-12-16 02:42:53.772567: step 91560, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.630 sec/batch; 42h:10m:15s remains)
INFO - root - 2017-12-16 02:43:00.167034: step 91570, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 42h:37m:48s remains)
INFO - root - 2017-12-16 02:43:06.555490: step 91580, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 42h:50m:00s remains)
INFO - root - 2017-12-16 02:43:13.014398: step 91590, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 42h:59m:44s remains)
INFO - root - 2017-12-16 02:43:19.466693: step 91600, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.623 sec/batch; 41h:39m:20s remains)
2017-12-16 02:43:20.040230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5164757 -3.7026665 -4.2515216 -4.4019022 -4.2661648 -4.0511475 -3.7140839 -3.1822815 -2.407371 -2.9717541 -2.8364043 -3.1104326 -4.1137438 -5.1746826 -5.9093909][-2.638247 -2.7618065 -3.3156562 -3.6262927 -3.5400515 -3.5251741 -3.3940182 -2.9719067 -2.0154524 -2.7820477 -2.6545057 -2.5517845 -3.8233716 -5.4723482 -6.8346786][-2.9566278 -2.2346368 -2.5991316 -2.9168282 -3.041368 -3.3285689 -3.4318728 -3.1847239 -2.7577605 -3.6260161 -3.3849649 -3.7732685 -5.1733575 -6.2062054 -6.9893909][-2.7719507 -2.3823867 -2.6655769 -2.4062181 -1.8264589 -1.9532762 -2.19831 -2.0979028 -1.6633849 -3.2748528 -4.1671257 -4.1327033 -4.9613776 -6.3339791 -7.413456][-2.9859762 -1.9696913 -1.3692298 -0.66977358 -0.64991903 -0.55307961 -0.046880722 -0.13774872 -0.22755575 -1.5660067 -1.9287114 -2.9527621 -5.0453892 -6.3808417 -7.2903895][-2.6904116 -1.7476373 -1.1824832 -0.24128389 0.49189186 1.082037 1.564703 1.1393633 1.2734776 -0.13216114 -1.0792542 -2.0363483 -3.8689473 -5.6173453 -7.1996889][-2.0995665 -1.1383243 0.34048939 1.2723465 1.756258 2.364522 3.1160736 2.9082222 2.8509607 0.71125126 -0.59733248 -1.831419 -3.669251 -5.0984411 -6.3177271][-0.8766923 -0.3068552 0.14933109 1.7693367 3.31042 3.6659384 3.8112669 3.926301 4.2895365 1.767642 -0.18876457 -1.9333849 -3.686687 -4.9443765 -5.9304962][-1.4034953 -0.622077 -0.14594746 1.1668291 2.0844517 2.3942986 2.6662006 3.1650209 3.5895071 1.2753897 -0.41522455 -2.1232815 -4.0533667 -5.2196913 -6.2084417][-3.4469209 -2.7631078 -1.7848392 -0.53070021 0.23370075 0.638814 1.0569582 1.1848974 1.0689192 -0.68059111 -1.7968311 -3.3092384 -5.0340443 -6.134964 -6.9439716][-5.39819 -4.9131813 -4.5385795 -3.7168808 -3.2380128 -2.4174638 -1.4170947 -1.1627564 -1.3951674 -3.4423003 -4.6579609 -5.2766819 -6.3659558 -7.5066371 -7.8618231][-7.9472489 -7.2235107 -6.8621254 -6.4246616 -5.7908287 -5.2474713 -4.7320156 -4.162745 -3.6950753 -5.0447655 -5.9352484 -6.0851693 -6.9487443 -7.7827554 -7.9019532][-8.9261265 -8.8826437 -8.8846836 -8.34223 -7.6977153 -7.1707778 -6.2792735 -6.1692085 -6.1570678 -6.6263723 -6.7452087 -6.6458554 -7.147871 -7.6128826 -7.4445105][-8.2513428 -8.4041386 -8.4659386 -8.2561111 -8.1289148 -7.5865765 -6.8617468 -6.752522 -6.629262 -6.78549 -6.8530993 -6.8994708 -7.1770239 -6.9579992 -6.5233645][-7.9816861 -7.4084339 -7.300056 -7.3664513 -7.5048347 -7.0625253 -6.8043551 -6.98909 -7.0857725 -6.9321985 -6.9753718 -6.8891325 -6.6742983 -6.59729 -6.4775233]]...]
INFO - root - 2017-12-16 02:43:26.462697: step 91610, loss = 0.29, batch loss = 0.17 (13.0 examples/sec; 0.617 sec/batch; 41h:18m:17s remains)
INFO - root - 2017-12-16 02:43:32.819699: step 91620, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 43h:14m:55s remains)
INFO - root - 2017-12-16 02:43:39.342513: step 91630, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 44h:12m:36s remains)
INFO - root - 2017-12-16 02:43:45.795269: step 91640, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 43h:07m:37s remains)
INFO - root - 2017-12-16 02:43:52.193126: step 91650, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 43h:09m:58s remains)
INFO - root - 2017-12-16 02:43:58.590316: step 91660, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 43h:14m:57s remains)
INFO - root - 2017-12-16 02:44:04.980531: step 91670, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.645 sec/batch; 43h:09m:03s remains)
INFO - root - 2017-12-16 02:44:11.340277: step 91680, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 42h:41m:33s remains)
INFO - root - 2017-12-16 02:44:17.840746: step 91690, loss = 0.33, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 42h:51m:00s remains)
INFO - root - 2017-12-16 02:44:24.248162: step 91700, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 42h:46m:50s remains)
2017-12-16 02:44:24.766960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.514214 -2.8638291 -2.9698963 -3.1946025 -3.5385976 -3.8833241 -4.1475935 -4.1371107 -3.9549153 -5.1128769 -6.2212744 -6.7807388 -7.2512965 -7.5956554 -7.992991][-2.6146412 -2.8378654 -3.1229978 -3.6716266 -4.2559352 -4.3767567 -4.6321163 -5.026804 -5.0357652 -6.031157 -6.9387484 -7.5046344 -8.1169 -8.6262465 -8.9253731][-1.7209525 -1.8945627 -2.040256 -2.4098372 -2.7062249 -2.958868 -3.2278986 -3.6883025 -3.8383505 -5.1069622 -6.2961326 -6.9075313 -7.6520028 -8.4418812 -8.9894009][-1.3936367 -1.643373 -1.6157999 -1.5117145 -1.6425738 -1.7085624 -1.8300724 -2.1491485 -2.3633175 -3.7943873 -5.127718 -5.9547062 -6.8236217 -7.7471919 -8.2749643][-1.6524754 -1.4276972 -1.1964164 -1.1398525 -0.83521748 -0.66007948 -0.53462791 -0.67654181 -0.76822662 -2.015667 -3.30892 -4.3371511 -5.4476409 -6.5426922 -7.2006335][-2.7099147 -2.2152271 -1.666152 -1.321414 -0.76654577 -0.18238735 0.34140491 0.50653362 0.79085827 -0.43839455 -1.7773542 -2.7567129 -3.9931107 -5.1630421 -6.1148319][-3.3521924 -2.6563368 -1.8532944 -1.118371 -0.18461609 0.27334785 0.785594 1.316349 1.8949614 0.74466324 -0.62357473 -1.6280932 -2.6485567 -4.0829706 -5.20133][-3.4860969 -2.9439964 -2.2008233 -1.1132331 -0.056725502 0.69572353 1.2279282 1.764864 2.563797 1.6350098 0.3981905 -0.64745283 -1.8094826 -3.1195831 -4.2276878][-3.4788804 -2.826755 -2.1768317 -1.2841797 -0.39985037 0.31716156 1.0556259 1.5568743 2.029685 1.0344601 -0.22043562 -1.2894206 -2.3056726 -3.4824605 -4.402051][-3.7767711 -3.2439356 -2.6723876 -1.8653932 -1.0785961 -0.42621708 0.27257824 0.93761158 1.4318094 -0.13604403 -1.5062218 -2.7005634 -3.8715434 -4.7798929 -5.3890519][-5.12537 -4.4405079 -3.8478851 -3.1489091 -2.5447822 -1.9262204 -1.1302624 -0.71949482 -0.45721388 -1.8039861 -3.2854695 -4.1390543 -4.9147515 -5.7059736 -6.2892337][-5.4492855 -5.1642246 -4.462481 -3.8294258 -3.2314377 -2.4190812 -1.8649549 -1.6811662 -1.450736 -2.755332 -4.0343275 -4.6912842 -5.3806543 -5.9670506 -6.3251877][-6.3370957 -5.7277937 -5.116508 -4.5263543 -3.9219382 -3.288918 -2.9872298 -2.8730588 -2.7686648 -3.6489396 -4.74111 -5.1862917 -5.7796259 -6.2500229 -6.1336117][-6.1002369 -5.5778389 -4.7855463 -4.1302848 -3.6514945 -3.1117558 -2.9810691 -3.0241151 -3.2421055 -4.04652 -4.59184 -5.0538011 -5.395606 -5.8792377 -5.9709311][-7.071866 -6.7319193 -6.1006265 -5.4598169 -4.8295889 -4.1174688 -4.0253868 -4.1347785 -4.3421907 -4.6470022 -5.0481653 -5.5819449 -5.9402385 -6.0089364 -5.9898047]]...]
INFO - root - 2017-12-16 02:44:31.090219: step 91710, loss = 0.35, batch loss = 0.24 (12.5 examples/sec; 0.640 sec/batch; 42h:49m:23s remains)
INFO - root - 2017-12-16 02:44:37.635528: step 91720, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 43h:39m:01s remains)
INFO - root - 2017-12-16 02:44:43.996181: step 91730, loss = 0.25, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 42h:24m:52s remains)
INFO - root - 2017-12-16 02:44:50.386506: step 91740, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 43h:00m:45s remains)
INFO - root - 2017-12-16 02:44:56.881636: step 91750, loss = 0.32, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 42h:55m:56s remains)
INFO - root - 2017-12-16 02:45:03.283767: step 91760, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 43h:18m:21s remains)
INFO - root - 2017-12-16 02:45:09.749153: step 91770, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 41h:59m:17s remains)
INFO - root - 2017-12-16 02:45:16.126677: step 91780, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 42h:04m:08s remains)
INFO - root - 2017-12-16 02:45:22.478852: step 91790, loss = 0.27, batch loss = 0.16 (13.0 examples/sec; 0.616 sec/batch; 41h:09m:28s remains)
INFO - root - 2017-12-16 02:45:28.909260: step 91800, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 42h:49m:32s remains)
2017-12-16 02:45:29.461671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3064127 -5.5063124 -7.5872936 -8.9177141 -9.90661 -10.640922 -10.315576 -9.1624413 -7.6098723 -7.1201448 -6.8312807 -7.5949988 -7.5615563 -7.9695196 -8.60134][-4.7910576 -6.740881 -8.5944691 -9.8218794 -11.33561 -11.757857 -11.279254 -10.499114 -8.9814682 -8.4141655 -8.5205936 -9.6536989 -9.6941977 -9.51071 -9.9483557][-4.5289841 -5.9118137 -7.7071176 -8.3766546 -9.0040874 -9.29779 -9.34717 -9.3362408 -8.1583052 -7.8146682 -8.7243385 -10.816376 -11.034891 -10.79365 -11.3954][-5.9175987 -5.7231627 -6.5379515 -7.0961361 -7.7150826 -7.3198314 -5.6237097 -5.4465542 -4.9854121 -5.1169996 -6.29908 -9.1161823 -10.63818 -11.595396 -11.82642][-5.6748395 -5.3315172 -4.5937328 -4.33903 -4.3019123 -2.9297209 -1.6257 -0.774281 -0.078332424 -1.1300178 -3.3269262 -6.9972029 -9.3225145 -10.808758 -11.651489][-5.9961553 -5.1902189 -4.23755 -2.5745058 -1.6086202 0.44266033 2.9991884 4.0069389 4.2419319 2.1610832 -0.73270416 -5.0796413 -7.7442741 -9.3127918 -10.861689][-5.474597 -5.2007761 -4.1677074 -1.9278069 0.39320755 2.4056826 4.5897894 6.091507 6.7282887 3.95638 0.73040962 -3.3626738 -6.1589818 -7.8696146 -9.1401167][-5.5808344 -5.3274355 -4.2968397 -1.3884377 1.3211012 3.2482243 5.1608658 6.1071377 6.7447853 4.352519 1.1323423 -2.9393797 -5.6243439 -7.3661761 -8.1578674][-6.5857582 -6.200531 -5.8679996 -3.5195174 -1.1362929 1.4250011 3.1033678 3.9759159 4.8342791 2.6174116 -0.69882441 -4.129879 -5.7773762 -6.6816063 -7.7483206][-8.2636786 -7.90015 -7.8182 -6.4757996 -4.6050611 -2.3414636 -0.53325415 -0.34399319 -0.14839745 -2.4211826 -4.7327757 -6.8468785 -7.7309608 -8.1311808 -8.95039][-9.967454 -10.22337 -10.230687 -9.4208813 -8.3080769 -6.1039214 -4.7635822 -4.7519627 -3.9874625 -5.55939 -7.2341428 -8.5447493 -9.4279613 -9.7801981 -10.628318][-10.280853 -10.719109 -10.415514 -9.9067621 -9.3428812 -8.3822641 -6.969276 -6.4983883 -6.3020334 -7.4053411 -7.6589875 -8.1905956 -9.3533745 -9.7144365 -10.294729][-10.371152 -10.686636 -10.53883 -9.8255043 -9.2802639 -8.867115 -8.4576263 -8.1020718 -7.6850715 -8.4367485 -9.0143394 -8.94161 -8.7103529 -8.58103 -9.047967][-8.42009 -9.05323 -9.1437931 -8.411767 -7.6099005 -7.1488442 -7.0097451 -7.0784774 -7.2675419 -7.6151905 -7.6552329 -7.5766516 -7.8718524 -7.9593077 -7.8338][-8.1541624 -8.2392635 -8.8819265 -8.7404985 -8.8039455 -8.1650715 -7.6826777 -7.8025417 -7.5164676 -7.8136249 -8.2151814 -7.8050771 -7.8713832 -8.311883 -8.4064636]]...]
INFO - root - 2017-12-16 02:45:35.835928: step 91810, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 42h:53m:35s remains)
INFO - root - 2017-12-16 02:45:42.260592: step 91820, loss = 0.27, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 44h:04m:16s remains)
INFO - root - 2017-12-16 02:45:48.671396: step 91830, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 43h:15m:11s remains)
INFO - root - 2017-12-16 02:45:55.062112: step 91840, loss = 0.29, batch loss = 0.17 (12.8 examples/sec; 0.627 sec/batch; 41h:55m:08s remains)
INFO - root - 2017-12-16 02:46:01.412214: step 91850, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 42h:30m:54s remains)
INFO - root - 2017-12-16 02:46:07.760395: step 91860, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 43h:08m:44s remains)
INFO - root - 2017-12-16 02:46:14.132016: step 91870, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 43h:22m:45s remains)
INFO - root - 2017-12-16 02:46:20.519062: step 91880, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.632 sec/batch; 42h:14m:09s remains)
INFO - root - 2017-12-16 02:46:26.840332: step 91890, loss = 0.26, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 42h:29m:51s remains)
INFO - root - 2017-12-16 02:46:33.296008: step 91900, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 42h:15m:18s remains)
2017-12-16 02:46:33.830841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7736056 -4.2168627 -4.7826757 -5.5995684 -6.3357806 -5.5562253 -4.9411125 -4.1533651 -3.564311 -5.3764 -6.2333875 -8.80712 -10.676583 -11.46701 -11.85292][-5.4662037 -7.1836934 -8.2704887 -8.5097771 -8.6999741 -7.7867532 -6.4472809 -4.9284749 -3.3856082 -5.0647221 -6.7710152 -9.3186131 -10.619743 -11.587176 -11.985579][-4.56499 -5.9242706 -7.5487943 -8.1864052 -8.0659294 -6.828639 -5.0826306 -3.5674057 -2.032578 -3.4406252 -4.9488478 -7.8018394 -9.2672148 -10.714622 -11.017218][-4.7882786 -5.481473 -6.1384206 -6.4253654 -6.3344088 -5.1584158 -3.0034509 -1.3188162 -0.18360901 -1.8815899 -3.3391914 -6.1874967 -8.04985 -9.2858305 -9.885293][-4.5454493 -4.3710079 -4.3596315 -4.6918983 -4.4486642 -3.1680136 -1.7599177 0.080942631 1.4498549 -0.10019016 -1.879384 -4.8373041 -6.9830375 -8.794241 -9.3892708][-3.3981237 -3.6607242 -3.9768753 -3.1328039 -2.12823 -0.57039022 1.024538 2.1518297 2.4614019 0.73553085 -1.4531817 -4.882494 -7.3057528 -8.402812 -9.10423][-2.9147148 -2.4223533 -2.4511805 -1.8539281 -0.9291749 0.69351292 2.1422358 3.0568371 3.4074211 1.1996031 -1.5599146 -5.4894476 -8.0800753 -9.784132 -10.239176][-3.2273378 -2.9430203 -2.4120722 -1.1892767 -0.11685228 1.1433382 2.3635416 2.6709805 2.8840809 0.75748348 -1.6058517 -5.4033604 -7.8276944 -9.4382544 -10.222655][-2.4480529 -2.1003594 -1.5273948 -0.72034359 0.023609161 0.97022343 1.9030933 2.0321445 1.6843729 -0.66142464 -2.7667336 -5.7086554 -7.3197942 -8.7592344 -9.3733921][-3.010869 -2.9175954 -2.5825486 -1.8318496 -1.1956534 -0.07937336 0.55816746 0.30310249 0.12320995 -2.6302185 -4.5953584 -7.505559 -8.9552193 -9.1648827 -9.1607628][-4.5726709 -4.7318425 -4.6347742 -3.6503744 -3.1706581 -2.3012443 -1.4170594 -1.4546638 -1.7922163 -4.4112606 -5.8119063 -7.65841 -8.3182249 -8.6437521 -8.6184921][-5.9457011 -5.9498024 -6.4091911 -5.9529982 -5.5123682 -4.6650219 -4.179553 -3.9786522 -3.7807159 -5.5827303 -6.3638287 -7.6061168 -7.8836465 -7.736167 -7.5198307][-6.0991397 -6.7155528 -7.2641921 -6.9791913 -6.5879555 -5.95852 -5.4453506 -5.7323208 -6.0882196 -6.9360528 -6.765027 -7.3273635 -7.7216444 -7.5078344 -6.8675261][-6.7260723 -7.0152674 -7.0851851 -6.784605 -6.3607883 -6.0946774 -6.1183772 -5.9857416 -5.7826791 -6.5450349 -6.6594086 -6.9432249 -6.738543 -6.2945743 -6.3223567][-7.3886547 -7.7176394 -7.9988527 -8.3076344 -8.3177528 -7.7201338 -7.2968698 -7.6552682 -8.0025272 -7.8150249 -7.2786374 -6.9242268 -6.6428022 -6.420589 -6.5366611]]...]
INFO - root - 2017-12-16 02:46:40.315168: step 91910, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 41h:46m:36s remains)
INFO - root - 2017-12-16 02:46:46.666406: step 91920, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 42h:22m:10s remains)
INFO - root - 2017-12-16 02:46:52.958657: step 91930, loss = 0.30, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 41h:30m:59s remains)
INFO - root - 2017-12-16 02:46:59.388894: step 91940, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 42h:15m:42s remains)
INFO - root - 2017-12-16 02:47:05.789174: step 91950, loss = 0.32, batch loss = 0.21 (12.2 examples/sec; 0.658 sec/batch; 43h:56m:40s remains)
INFO - root - 2017-12-16 02:47:12.200141: step 91960, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 42h:39m:49s remains)
INFO - root - 2017-12-16 02:47:18.684364: step 91970, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 43h:13m:11s remains)
INFO - root - 2017-12-16 02:47:25.138660: step 91980, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.643 sec/batch; 42h:55m:49s remains)
INFO - root - 2017-12-16 02:47:31.511540: step 91990, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 43h:30m:32s remains)
INFO - root - 2017-12-16 02:47:38.057420: step 92000, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 43h:07m:33s remains)
2017-12-16 02:47:38.642967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1223698 -2.3353043 -2.0222421 -1.9888849 -2.3112755 -2.9594364 -2.9340272 -3.0529294 -2.8501019 -4.3972034 -4.9357414 -5.6074553 -6.6087646 -7.2610865 -8.8155794][-2.450707 -1.4416294 -1.2678938 -1.8059111 -2.7657156 -3.1834469 -3.4099331 -3.8941662 -3.7120206 -4.8551702 -5.5860033 -6.1630344 -6.6094074 -6.7380962 -7.762917][-1.8674288 -0.34802103 -0.055616379 -0.79374981 -1.876142 -2.178503 -2.3197803 -2.5264883 -2.3571658 -3.3505116 -4.6764812 -5.53611 -6.0099063 -6.4443116 -7.1299362][-0.60886335 -0.25071573 0.080388069 -0.2073884 -0.18031645 -0.37238836 -0.11816835 -0.6855855 -0.86179495 -2.2232838 -3.3539715 -4.6863418 -5.7746391 -6.3201604 -7.0574484][-2.0604229 -0.56687164 0.21576166 0.097753048 0.14777899 0.74912643 1.473814 1.3950014 0.95289135 -1.0932374 -2.377244 -3.6463876 -5.0661516 -5.7055678 -6.6793547][-2.0225701 -1.7724714 -1.2734609 -0.68898344 0.0074124336 0.61020279 1.2587175 1.5890055 1.5041876 -0.40090513 -1.1237721 -2.1063137 -3.2972736 -4.6403208 -5.7285852][-3.074923 -2.0017595 -1.4965696 -0.69650984 0.26620483 1.2227974 2.1331387 2.4060974 2.4342833 0.34266853 -0.76414728 -2.0448866 -3.5407372 -4.5360413 -5.6440926][-3.8892703 -2.9920607 -2.0737352 -0.51683187 0.73795986 1.8620167 2.7073879 3.0417004 2.9714546 1.0324898 -0.4513998 -2.0486856 -3.6713228 -4.6856661 -5.4265652][-4.685111 -2.9000783 -2.7325 -1.4452028 -0.1435709 1.3158379 2.1202278 2.3056316 2.2304029 0.14790297 -1.1364217 -2.4312 -3.7203243 -4.567955 -5.1203108][-5.6480255 -4.2867737 -3.6146436 -2.4772224 -2.1103611 -0.75739908 0.531168 0.85544872 1.4034843 -0.67096472 -1.9463701 -2.8146424 -3.808166 -4.3910818 -5.1623278][-5.7998171 -5.147109 -4.424787 -3.2056122 -2.5467315 -1.7204356 -0.75678015 -0.088537216 -0.086158276 -2.3181033 -3.1330223 -3.4869523 -4.4409204 -5.304163 -5.8593984][-5.6552172 -4.294744 -3.9680626 -3.20291 -2.3616261 -1.2871904 -0.56434917 -0.50277662 -1.0540605 -2.8183131 -3.5265698 -3.8780651 -4.7709055 -5.4262018 -5.9279728][-5.7721934 -4.3874483 -3.1516666 -2.306459 -1.7699795 -0.91647339 0.082456589 -0.038047791 -0.4300642 -2.7131228 -4.1238832 -3.9911647 -4.4069672 -5.4215374 -6.4909286][-4.5232973 -3.5460291 -2.5002642 -1.1010838 -0.092017651 0.73989582 1.0675812 1.1277475 0.9396534 -0.65388155 -2.6076546 -3.7822564 -4.5361319 -4.9183426 -5.8538237][-4.1087751 -2.898663 -2.4644113 -1.6617999 -0.55297184 0.26669455 0.83060074 0.46866226 -0.29174566 -1.1506577 -1.9168816 -3.2015252 -4.5809574 -5.4014187 -5.6702671]]...]
INFO - root - 2017-12-16 02:47:45.059245: step 92010, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 42h:58m:53s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:47:51.479757: step 92020, loss = 0.28, batch loss = 0.17 (12.2 examples/sec; 0.657 sec/batch; 43h:53m:28s remains)
INFO - root - 2017-12-16 02:47:57.779529: step 92030, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 43h:00m:50s remains)
INFO - root - 2017-12-16 02:48:04.177105: step 92040, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 43h:04m:58s remains)
INFO - root - 2017-12-16 02:48:10.515134: step 92050, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 42h:32m:56s remains)
INFO - root - 2017-12-16 02:48:16.828493: step 92060, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 42h:51m:06s remains)
INFO - root - 2017-12-16 02:48:23.258863: step 92070, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.652 sec/batch; 43h:31m:40s remains)
INFO - root - 2017-12-16 02:48:29.622705: step 92080, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 42h:59m:50s remains)
INFO - root - 2017-12-16 02:48:36.091577: step 92090, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 42h:55m:47s remains)
INFO - root - 2017-12-16 02:48:42.457182: step 92100, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 42h:52m:17s remains)
2017-12-16 02:48:42.962477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9222753 -4.0706806 -4.2974005 -4.3335686 -4.2763233 -4.1035337 -3.9428022 -3.8220184 -3.5055389 -4.3659835 -4.4053793 -4.9475193 -5.250946 -5.94084 -6.9487553][-3.8072824 -4.0119715 -4.1732273 -4.2479444 -4.1685929 -4.2093706 -4.1955872 -4.2984238 -4.3318343 -5.4792662 -5.6475811 -6.5476785 -7.0015249 -7.303401 -8.136342][-3.8025985 -3.4435105 -2.8904285 -2.3760095 -2.1206102 -2.3665528 -2.525836 -2.9111495 -3.3006825 -4.4837222 -4.8794603 -5.5924644 -5.7973614 -6.2486439 -7.2689][-2.0950623 -1.3280401 -0.61969995 -0.34594917 -0.39022779 -0.55983496 -0.65146065 -1.1824083 -1.5667362 -3.0878906 -3.5796666 -4.3910928 -4.8245468 -5.2941723 -6.1678481][-1.4233394 -0.93074274 -0.17000532 0.28865147 0.089934826 0.14705133 0.33865452 0.061618805 -0.2328124 -1.6086125 -2.09755 -3.0842781 -3.7060537 -4.234971 -5.6936083][-1.018681 -0.21203613 0.11953354 0.56811237 0.66170025 0.77710629 0.98310471 0.70759106 0.4569912 -0.75318623 -1.0629034 -1.7516336 -2.1382937 -2.80088 -4.2053709][-1.0536094 -0.15453196 0.60199833 1.201251 1.3853436 1.4616442 1.666254 1.411273 1.1999168 -0.047807693 -0.55135536 -1.3812742 -1.8003001 -2.4397273 -3.9358954][-0.36495113 0.28976631 0.99205208 1.818058 2.3222046 2.2252121 2.18114 2.0183191 1.9792051 0.64240837 0.0042872429 -1.0481935 -1.6645231 -2.2553725 -3.6268196][-0.77334929 0.058144569 0.59306431 1.0000534 1.539238 1.6043367 1.4321079 1.4535208 1.7149801 0.484787 -0.2575264 -1.4109974 -2.204319 -2.9723344 -4.3715181][-1.8366241 -1.281198 -0.70571566 -0.13484669 0.28135157 0.52118874 0.70506191 0.84996986 1.1552801 0.14496803 -0.74682808 -2.1879697 -3.1436415 -3.6815772 -4.9734349][-3.1590662 -2.6158 -2.0965672 -1.6779265 -1.2273321 -1.128963 -1.0400109 -0.95685911 -0.89346981 -1.9894099 -2.6171699 -3.7592459 -4.5275173 -4.9029655 -5.8156643][-4.311367 -4.0261889 -3.9720054 -4.0162134 -3.7551544 -3.3980947 -3.3739414 -3.4105039 -3.3095078 -4.254468 -4.7876472 -5.2383142 -5.4483228 -5.6352415 -6.0844874][-6.2119641 -6.0523987 -5.864603 -5.7894311 -5.72624 -5.5687265 -5.6588173 -5.3507476 -5.1214314 -5.4076061 -5.325057 -5.7688985 -6.12913 -5.8278337 -5.8251643][-6.9735136 -6.9669213 -6.9592128 -6.5874672 -6.540688 -6.5807629 -6.4766469 -6.702714 -6.7440529 -7.2106867 -7.2295961 -6.8585596 -6.4367771 -5.4153671 -4.6447473][-7.2197838 -6.9933543 -7.2607975 -7.3800459 -7.7393417 -7.7313342 -7.6336312 -7.892519 -8.0421543 -8.0944252 -7.8862033 -7.5562572 -7.1089039 -6.31157 -5.3771873]]...]
INFO - root - 2017-12-16 02:48:49.398358: step 92110, loss = 0.33, batch loss = 0.21 (12.9 examples/sec; 0.622 sec/batch; 41h:31m:45s remains)
INFO - root - 2017-12-16 02:48:55.816485: step 92120, loss = 0.28, batch loss = 0.17 (12.7 examples/sec; 0.631 sec/batch; 42h:07m:21s remains)
INFO - root - 2017-12-16 02:49:02.214720: step 92130, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.642 sec/batch; 42h:51m:12s remains)
INFO - root - 2017-12-16 02:49:08.563912: step 92140, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 42h:25m:45s remains)
INFO - root - 2017-12-16 02:49:14.961003: step 92150, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 43h:57m:32s remains)
INFO - root - 2017-12-16 02:49:21.358470: step 92160, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 42h:49m:04s remains)
INFO - root - 2017-12-16 02:49:27.740167: step 92170, loss = 0.31, batch loss = 0.19 (12.7 examples/sec; 0.628 sec/batch; 41h:57m:24s remains)
INFO - root - 2017-12-16 02:49:34.089312: step 92180, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 42h:29m:03s remains)
INFO - root - 2017-12-16 02:49:40.470032: step 92190, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 42h:15m:34s remains)
INFO - root - 2017-12-16 02:49:46.836206: step 92200, loss = 0.33, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 42h:27m:12s remains)
2017-12-16 02:49:47.377792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5382452 -4.9158177 -4.5541458 -3.9037931 -3.4543433 -2.6054621 -2.2491016 -2.479425 -1.997498 -3.2902765 -4.1577272 -4.1621008 -5.9592981 -6.8217111 -8.3978214][-4.2585487 -4.228395 -4.0382905 -4.4894943 -4.8134146 -4.7574091 -4.2144246 -3.9135063 -3.1436539 -2.9437461 -3.1426239 -3.5027995 -5.3260021 -6.9185762 -9.6325359][-4.5599985 -5.0336704 -4.8449364 -4.29751 -4.2776041 -3.89284 -3.4477525 -3.3753576 -2.4747148 -2.180759 -2.7112207 -2.509047 -3.6279955 -5.1323366 -7.4366579][-4.0682068 -4.6923523 -4.5645671 -4.0964994 -3.3548942 -2.2368436 -1.390852 -0.96167374 -0.084661484 -0.59255695 -1.1083565 -1.2308927 -3.0153856 -4.1378164 -6.2544641][-4.4136329 -4.7037959 -3.9481964 -2.6970663 -1.7207708 -0.37454271 -0.095280647 0.48364449 1.1438608 0.85429859 -0.01909256 -0.15952539 -1.8020482 -3.5828958 -6.1379762][-4.7934732 -4.7494698 -3.8301272 -2.4786229 -0.73808908 0.75706768 1.9175797 2.3583717 2.84414 2.509119 1.3420954 0.73221493 -1.1803708 -2.9021163 -4.6134238][-4.0190125 -4.266088 -3.2534695 -1.7601252 0.17445803 1.7601461 2.7991743 3.3676634 4.0188904 3.3894253 2.2093258 1.517355 -0.96077919 -2.964561 -4.8913436][-2.5496483 -1.9965696 -0.82972574 0.16626406 1.7159691 2.5650558 3.2863264 3.2631969 3.7395439 3.0010633 1.6216402 0.93987465 -0.93519926 -3.330761 -5.7391744][-1.7146516 -0.9523325 -0.17823076 1.1205187 1.6216536 2.2099152 2.8394737 2.9956989 3.4252844 2.4908609 0.82856464 0.18796682 -1.9446516 -3.9527228 -6.12854][-2.0069752 -1.7909689 -1.3145037 -0.37877655 0.287539 0.70169544 0.87231064 1.2374573 2.1008568 1.2561226 0.65042305 0.3119688 -2.2865753 -4.5546412 -6.7098112][-3.6743541 -3.2108579 -2.3535047 -1.5708103 -0.56435537 -0.068705082 0.28099632 0.75380611 0.58356762 -0.65808821 -1.2742615 -1.3848839 -3.0883031 -5.307529 -7.1519971][-5.9267035 -5.5604382 -4.7630963 -3.5669608 -3.0612602 -2.6512117 -2.1382246 -1.3247766 -1.3738046 -1.9586549 -2.2611303 -3.0251908 -4.5617409 -5.7089281 -6.9501009][-8.1589146 -7.2971926 -6.9456415 -6.6025853 -6.0151262 -5.3186483 -4.8927317 -4.4521632 -4.3857861 -4.5734282 -4.4765072 -4.2528334 -4.7415504 -5.1959248 -6.3110824][-9.0692339 -8.53422 -8.2062006 -7.6701818 -6.9174981 -6.4833212 -6.1228681 -5.6902423 -5.7681532 -5.9460063 -5.9358807 -5.6031771 -5.8627138 -6.0498948 -6.4058609][-7.9873538 -8.02721 -7.9688993 -7.2380219 -7.107265 -6.4698977 -6.3297124 -6.2964993 -6.5877576 -6.4857593 -6.2939148 -5.8978672 -6.0969539 -6.4613991 -6.5180798]]...]
INFO - root - 2017-12-16 02:49:53.854840: step 92210, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 44h:33m:48s remains)
INFO - root - 2017-12-16 02:50:00.217787: step 92220, loss = 0.39, batch loss = 0.27 (12.5 examples/sec; 0.638 sec/batch; 42h:33m:39s remains)
INFO - root - 2017-12-16 02:50:06.649880: step 92230, loss = 0.30, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 42h:06m:48s remains)
INFO - root - 2017-12-16 02:50:13.095163: step 92240, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 42h:31m:59s remains)
INFO - root - 2017-12-16 02:50:19.589397: step 92250, loss = 0.25, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 44h:11m:53s remains)
INFO - root - 2017-12-16 02:50:26.002291: step 92260, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 43h:11m:05s remains)
INFO - root - 2017-12-16 02:50:32.368316: step 92270, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 42h:27m:24s remains)
INFO - root - 2017-12-16 02:50:38.763296: step 92280, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 42h:33m:35s remains)
INFO - root - 2017-12-16 02:50:45.142133: step 92290, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.625 sec/batch; 41h:41m:15s remains)
INFO - root - 2017-12-16 02:50:51.539672: step 92300, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 42h:27m:12s remains)
2017-12-16 02:50:52.061893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6118646 -5.4204655 -5.1681385 -4.9019718 -4.384798 -4.2871075 -4.1441908 -3.711215 -3.0200791 -4.5272141 -5.3937306 -5.4452209 -5.1205406 -5.8104181 -6.3672376][-4.8379307 -4.2569885 -3.8638809 -3.6651154 -3.6120429 -3.8408785 -4.08496 -4.1137552 -4.0599189 -5.741643 -6.6579132 -7.1314487 -6.8921862 -7.3030367 -7.2606082][-3.7087905 -3.7133412 -3.7126074 -3.5830536 -3.2995682 -3.2818666 -3.6985745 -3.8186967 -4.3253751 -6.0821013 -6.9109707 -7.508348 -7.6430798 -7.9951944 -7.8647676][-3.5005713 -3.0008354 -2.5447035 -2.6426268 -2.7315378 -2.4695935 -2.5544271 -3.1427498 -3.6479053 -5.4981127 -6.6038375 -7.21311 -7.3638668 -7.92795 -8.2804632][-2.1805148 -1.9787197 -1.9467659 -1.5110722 -1.3344231 -0.83983183 -0.76256084 -1.6106539 -2.785295 -4.5808773 -5.2204418 -6.056263 -6.7183456 -7.6439729 -7.9122066][-1.8792481 -1.2362781 -0.97811604 -0.42556715 0.13451195 0.78207779 0.77367687 0.10988998 -0.653419 -3.1232347 -4.5297918 -5.287858 -5.5857763 -6.6996245 -7.3388162][-1.6699371 -0.92354393 -0.38671255 0.40185261 1.2558355 2.0284958 1.9007645 1.2734795 0.48446655 -1.6405625 -3.030283 -4.1579885 -4.95354 -6.2612872 -6.7930946][-1.7898026 -0.98967648 -0.77125263 -0.00715065 1.6558332 2.3870831 2.4278708 2.1044788 1.7111721 -0.59439564 -2.377666 -3.7262521 -4.592864 -5.7407312 -6.3059483][-2.3373652 -1.4855194 -0.77571392 -0.018079281 0.74316883 1.1221972 0.95635796 0.895998 0.77029037 -0.9596386 -2.1763191 -3.481463 -4.3539991 -5.391964 -5.8867879][-2.7244382 -2.3632565 -1.9082599 -1.3095841 -0.51056004 -0.026434898 0.18555975 0.36160278 0.28617573 -1.4844246 -2.6779976 -3.8905728 -4.6706848 -5.6384091 -6.0845442][-4.5081697 -3.9377441 -3.0325656 -2.3880105 -2.1288419 -1.6534686 -1.1969452 -1.1686091 -1.1383734 -2.6352491 -4.030149 -5.1270876 -5.7130909 -6.6512222 -6.7258077][-5.2399859 -4.7578859 -4.447361 -4.2142887 -3.7684183 -3.442874 -3.210578 -3.0492144 -3.0339718 -3.9213562 -4.8995771 -5.3616886 -5.9160261 -7.0650339 -7.2869415][-6.9601746 -6.421041 -5.9631462 -5.2848482 -5.0662732 -4.6127443 -4.0903025 -4.1542063 -4.2084785 -4.48372 -4.8997 -5.4005318 -5.7703152 -6.4593973 -7.0465112][-7.356328 -7.2742639 -7.108325 -6.7214689 -6.5477781 -6.058959 -5.5622797 -5.3873177 -5.5290184 -5.6127548 -5.451838 -5.3142962 -5.5511227 -5.9649053 -6.1414609][-6.8246961 -6.6616755 -6.1861558 -6.2144475 -6.1949172 -6.007741 -5.8893027 -5.7679048 -5.73625 -5.455512 -5.2340503 -4.9835987 -4.8500166 -4.9994478 -5.3042116]]...]
INFO - root - 2017-12-16 02:50:58.508365: step 92310, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 42h:55m:52s remains)
INFO - root - 2017-12-16 02:51:04.857110: step 92320, loss = 0.24, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 43h:20m:04s remains)
INFO - root - 2017-12-16 02:51:11.262400: step 92330, loss = 0.26, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 43h:18m:24s remains)
INFO - root - 2017-12-16 02:51:17.660080: step 92340, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.631 sec/batch; 42h:04m:12s remains)
INFO - root - 2017-12-16 02:51:24.154152: step 92350, loss = 0.28, batch loss = 0.17 (12.8 examples/sec; 0.624 sec/batch; 41h:35m:50s remains)
INFO - root - 2017-12-16 02:51:30.536784: step 92360, loss = 0.24, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 42h:47m:29s remains)
INFO - root - 2017-12-16 02:51:36.976420: step 92370, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 43h:18m:21s remains)
INFO - root - 2017-12-16 02:51:43.353822: step 92380, loss = 0.25, batch loss = 0.13 (12.8 examples/sec; 0.625 sec/batch; 41h:42m:42s remains)
INFO - root - 2017-12-16 02:51:49.826858: step 92390, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.631 sec/batch; 42h:06m:34s remains)
INFO - root - 2017-12-16 02:51:56.296526: step 92400, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 42h:32m:23s remains)
2017-12-16 02:51:56.793721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3614383 -3.4572606 -2.770165 -1.7361751 -1.4493403 -2.1097646 -3.144063 -3.2417693 -2.9689388 -2.9607081 -3.5558605 -6.3462858 -6.9691725 -8.2098131 -8.308259][-3.897877 -3.9893088 -3.2820244 -2.5421209 -2.2267351 -2.6182685 -3.051724 -3.6961694 -4.0708923 -3.9098973 -4.3295903 -6.9661202 -6.9051013 -8.0229092 -8.4695711][-3.84296 -4.4209118 -4.4080405 -2.9928036 -2.5420737 -2.8069263 -2.9616628 -3.3192787 -3.3090429 -3.2355471 -4.2523489 -6.8918171 -6.6929455 -7.6043444 -8.0475731][-5.2376852 -4.8532872 -3.7752185 -2.6216125 -2.1704111 -1.7399883 -1.7743487 -2.2338438 -2.1968222 -2.3231459 -3.7001605 -6.4295678 -6.2259588 -7.1552615 -7.3074603][-6.0678039 -5.3969035 -4.9563928 -3.8533781 -2.4910016 -1.6719394 -1.5912304 -1.7782702 -1.8994517 -1.9643798 -3.2231584 -6.6125526 -6.541172 -6.9439845 -7.4288216][-6.6842613 -6.3780031 -4.5230665 -3.0096674 -2.6182079 -1.5734043 -1.3208847 -1.6325741 -1.3667059 -1.556251 -2.8600626 -5.4978466 -4.9719787 -6.3870931 -5.9770441][-6.6108332 -4.4543142 -4.0945463 -2.1826248 -0.088965893 0.74623585 1.376296 0.87591743 -0.071960926 -0.5115633 -1.3553886 -4.1665316 -3.8518162 -4.8565207 -4.3332586][-6.170577 -4.7596455 -3.350873 -0.86559057 -0.061058044 1.4771032 1.8310785 1.7199221 2.0162764 0.87999058 -0.2013135 -3.0878325 -2.6815438 -4.0124884 -3.9039342][-6.6712685 -5.7384305 -4.7266197 -2.7098789 -0.76440477 0.40227604 0.77374172 1.1251287 0.71586037 0.55120468 0.43764496 -2.6264348 -2.3629737 -2.9631577 -2.8376017][-7.6415415 -6.8252654 -6.5374403 -4.4277449 -2.9092045 -1.6283884 -0.23910284 -0.087156773 -0.20414829 -0.81198215 -1.5101829 -3.9396875 -3.4847031 -3.8951592 -3.7931039][-10.134882 -9.7678261 -8.4066572 -6.1899161 -4.8926883 -3.0910668 -2.4163184 -2.1758227 -1.9014015 -2.5861568 -3.0720792 -4.937685 -4.6132545 -5.1045713 -4.604701][-9.5489492 -9.8310661 -9.6032839 -7.9942269 -6.1267657 -4.8737869 -3.753041 -3.6440487 -3.7597275 -4.0886 -4.3530712 -5.6193113 -5.3819685 -5.6494713 -4.9114027][-9.6641226 -9.6957092 -10.480803 -8.9497938 -7.8781672 -6.7011571 -5.6196361 -5.4571085 -5.7551656 -6.4673042 -5.5808105 -6.58634 -6.00455 -5.9018917 -5.6452579][-9.8340015 -10.040976 -9.8632259 -8.9798088 -8.7764225 -7.8238182 -7.277637 -7.3595886 -6.7868013 -6.9980845 -6.8124895 -6.7061253 -6.3808928 -6.0260572 -5.441534][-10.136324 -10.158685 -10.017534 -8.6463165 -7.6842122 -7.6862278 -7.0929661 -7.1444058 -7.3796268 -7.1856875 -6.8711891 -6.7594428 -5.89118 -5.4663248 -5.0241885]]...]
INFO - root - 2017-12-16 02:52:03.153464: step 92410, loss = 0.29, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 41h:58m:26s remains)
INFO - root - 2017-12-16 02:52:09.529887: step 92420, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 43h:31m:19s remains)
INFO - root - 2017-12-16 02:52:15.943621: step 92430, loss = 0.25, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 42h:55m:25s remains)
INFO - root - 2017-12-16 02:52:22.375366: step 92440, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 41h:57m:55s remains)
INFO - root - 2017-12-16 02:52:28.855039: step 92450, loss = 0.26, batch loss = 0.15 (12.8 examples/sec; 0.623 sec/batch; 41h:34m:28s remains)
INFO - root - 2017-12-16 02:52:35.408638: step 92460, loss = 0.31, batch loss = 0.19 (12.2 examples/sec; 0.656 sec/batch; 43h:42m:59s remains)
INFO - root - 2017-12-16 02:52:41.921591: step 92470, loss = 0.27, batch loss = 0.15 (12.9 examples/sec; 0.621 sec/batch; 41h:23m:35s remains)
INFO - root - 2017-12-16 02:52:48.286327: step 92480, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 42h:27m:19s remains)
INFO - root - 2017-12-16 02:52:54.745526: step 92490, loss = 0.30, batch loss = 0.18 (12.8 examples/sec; 0.626 sec/batch; 41h:42m:27s remains)
INFO - root - 2017-12-16 02:53:01.196982: step 92500, loss = 0.29, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 43h:25m:18s remains)
2017-12-16 02:53:01.798464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2587452 -6.2735004 -6.9930749 -6.4570351 -5.177588 -4.7472987 -3.0645027 -2.4591594 -2.7460303 -3.6265645 -4.8275337 -6.8842459 -7.3960071 -7.1277642 -7.3603582][-4.0655556 -5.6677752 -7.581387 -7.5205703 -7.4620924 -6.5667868 -5.5542293 -4.712698 -4.4355016 -4.8528538 -5.8292837 -7.4479871 -7.8614821 -8.164958 -8.3776188][-4.2792315 -5.3335094 -6.7375808 -6.9556994 -6.4284515 -5.9480014 -5.4238272 -5.5148311 -4.7625866 -4.9735222 -5.8366346 -7.00097 -7.3871894 -7.5268426 -8.0400629][-4.3442278 -4.1938691 -4.7157626 -4.1814065 -3.336298 -2.8866134 -2.1036434 -2.7188673 -3.1926389 -3.9528966 -5.0540648 -6.20539 -6.8681917 -7.1699357 -7.4403157][-4.3705769 -3.2032142 -2.7101092 -1.5882659 -0.66428852 0.23445415 0.54441643 -0.51155663 -1.2143312 -2.0654111 -3.3388638 -5.0370731 -6.2619348 -6.7732863 -6.8038735][-4.6767683 -3.1846886 -1.824966 -0.55857277 0.4664669 1.797122 2.6085796 1.1614542 0.31812286 -0.77631807 -2.0625253 -4.0279284 -5.4308858 -6.1390142 -6.4326673][-4.9850836 -3.6040406 -1.9086757 0.011329651 1.3905125 2.8118877 3.5956268 2.7006702 1.516304 -0.32761431 -2.0115347 -4.0586386 -5.4291921 -6.0482721 -6.4792194][-4.7778096 -3.5108709 -1.8290291 -0.079905033 1.2349329 2.6634626 3.4054117 3.1918583 2.5187159 -0.13239336 -2.5361567 -4.9210291 -5.9891458 -6.3370209 -6.673193][-5.3548584 -4.5382681 -3.6494703 -1.877749 -0.86862707 0.38848019 1.3139076 1.8497667 2.0069466 -0.28179932 -2.9229393 -5.6859808 -6.814332 -6.7640185 -7.0957603][-6.048214 -5.830689 -5.5987906 -4.1138153 -2.745719 -1.7283554 -1.0086632 -0.34980822 0.18105125 -1.7148671 -3.4756308 -5.9454594 -7.3354855 -7.67595 -8.1341171][-8.4147825 -8.0265923 -7.8477817 -6.7253351 -5.426672 -4.662025 -4.0147429 -3.8207626 -3.7150273 -4.7457733 -5.9844475 -7.1393609 -7.9886241 -8.2484636 -8.6932278][-9.3617916 -8.9358473 -8.763382 -7.8450737 -7.100008 -6.4497828 -5.6956882 -5.6208305 -5.703845 -6.6830711 -6.8808928 -7.400044 -7.514761 -7.8768291 -8.1670589][-10.884377 -10.443588 -10.227437 -9.1386642 -8.0784092 -7.6309204 -7.1845846 -7.3523531 -7.1373777 -7.8082242 -8.30939 -7.7111392 -7.3973179 -7.1877012 -7.160985][-10.31909 -9.7430725 -9.4380512 -8.7071247 -7.8262262 -7.0871687 -6.3196692 -6.7687221 -7.41165 -7.6679435 -7.6925039 -7.1458282 -6.997436 -6.8866329 -6.84374][-10.079813 -9.6787844 -9.7057343 -9.3002119 -9.018384 -8.3158751 -7.1321964 -7.1155658 -7.529057 -8.0727444 -7.4839606 -6.6185088 -6.0492039 -6.6204629 -7.0964041]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-92500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-92500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 02:53:09.143889: step 92510, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 42h:04m:17s remains)
INFO - root - 2017-12-16 02:53:15.514409: step 92520, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 42h:34m:43s remains)
INFO - root - 2017-12-16 02:53:21.900631: step 92530, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 42h:34m:15s remains)
INFO - root - 2017-12-16 02:53:28.403974: step 92540, loss = 0.24, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 43h:06m:31s remains)
INFO - root - 2017-12-16 02:53:34.825417: step 92550, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.633 sec/batch; 42h:09m:59s remains)
INFO - root - 2017-12-16 02:53:41.219191: step 92560, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.632 sec/batch; 42h:07m:04s remains)
INFO - root - 2017-12-16 02:53:47.608519: step 92570, loss = 0.30, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 43h:06m:20s remains)
INFO - root - 2017-12-16 02:53:54.056599: step 92580, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 42h:47m:33s remains)
INFO - root - 2017-12-16 02:54:00.427577: step 92590, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 42h:34m:11s remains)
INFO - root - 2017-12-16 02:54:06.844360: step 92600, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 42h:40m:04s remains)
2017-12-16 02:54:07.404910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0756021 -5.473815 -5.8291278 -5.9848757 -6.1563988 -5.3437328 -4.3443613 -4.0643578 -3.5835977 -4.6089249 -5.1593781 -6.0960426 -6.3445506 -5.9082727 -6.3898458][-5.7733111 -5.7486773 -5.4331055 -4.9716063 -3.9814544 -3.5219312 -3.0159106 -2.2835526 -1.6441555 -2.7407579 -3.2906218 -4.7431507 -5.5952988 -6.1016507 -6.9538789][-5.1261134 -5.4463224 -5.5970993 -4.5553913 -3.7362525 -3.0359077 -2.3561835 -2.4240212 -2.3525352 -3.1748838 -4.5973735 -5.7796025 -5.8047638 -6.2380672 -7.2069535][-6.3132172 -5.9047742 -5.0370932 -3.6228638 -2.4193945 -1.3639169 -0.94602346 -1.0815415 -1.5896239 -3.0257306 -3.8906095 -5.797924 -6.4480877 -6.831923 -7.1478133][-8.2887878 -6.7742653 -4.732655 -2.9045711 -1.679481 -0.410295 0.30797195 0.11177874 -0.17279482 -1.4872069 -2.413774 -4.4656343 -5.6272173 -6.2587385 -7.4803119][-8.2725477 -7.8410683 -6.8108282 -3.9008677 -1.2376814 0.47452354 1.6815128 1.6453838 1.2817297 -0.53570795 -1.5874934 -3.0858469 -3.7937276 -4.6655712 -5.970211][-9.5270557 -7.1561527 -4.4085169 -2.9746056 -1.6456194 0.551198 2.1403351 2.2113714 2.1858177 1.1102896 0.084278584 -2.1611052 -3.0028906 -3.0198984 -3.5781574][-8.2155209 -7.0775146 -6.2887621 -3.1678963 0.096445084 1.0668192 2.017004 2.6519327 2.6566906 1.1321898 0.35031605 -1.2380929 -1.9310527 -2.7299776 -3.5080094][-9.875185 -7.7818675 -5.3943529 -3.6179123 -2.0933428 0.054449558 1.7097712 1.945118 2.4433765 1.5108957 0.2575264 -2.0575428 -3.1574378 -2.7893305 -3.3145704][-9.9933681 -9.7680979 -8.392561 -5.0212989 -2.5747862 -1.3997169 -0.039943695 1.2472019 2.700675 1.1174726 -0.71761894 -2.4706464 -3.8717358 -4.6923232 -4.864584][-11.178309 -11.043509 -10.574665 -8.651125 -6.4914794 -3.7161524 -2.0306511 -1.7889881 -1.2573652 -1.8363633 -2.7692022 -4.734746 -6.2015805 -6.678937 -7.2170219][-12.456741 -11.673836 -10.240732 -9.0206633 -7.9331293 -6.1827564 -4.6085157 -3.37391 -2.4133348 -3.8631852 -5.4701557 -6.3372407 -6.9990463 -7.6211877 -8.2488365][-11.67717 -12.356035 -11.869581 -10.196507 -8.4770308 -7.4522719 -6.3069735 -5.6528416 -5.0299978 -5.3552613 -5.4894066 -6.8566341 -7.3821774 -7.1623125 -7.7223282][-10.549092 -10.89477 -10.835743 -10.347665 -9.7730808 -8.6512728 -7.4298091 -7.0519738 -6.7150393 -6.5190053 -6.4721317 -6.6188307 -6.8210492 -6.6576986 -6.462162][-9.1115971 -9.7308064 -10.908956 -10.39888 -9.62146 -9.0800457 -7.9136019 -7.2564611 -6.818296 -6.9485421 -6.791739 -7.0547333 -7.4307055 -7.3330679 -7.3801351]]...]
INFO - root - 2017-12-16 02:54:13.788503: step 92610, loss = 0.29, batch loss = 0.17 (12.7 examples/sec; 0.632 sec/batch; 42h:04m:59s remains)
INFO - root - 2017-12-16 02:54:20.176093: step 92620, loss = 0.31, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 42h:42m:42s remains)
INFO - root - 2017-12-16 02:54:26.650142: step 92630, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 41h:45m:16s remains)
INFO - root - 2017-12-16 02:54:32.994191: step 92640, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 43h:01m:11s remains)
INFO - root - 2017-12-16 02:54:39.369693: step 92650, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 42h:52m:09s remains)
INFO - root - 2017-12-16 02:54:45.759504: step 92660, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 42h:43m:13s remains)
INFO - root - 2017-12-16 02:54:52.175754: step 92670, loss = 0.34, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 43h:02m:55s remains)
INFO - root - 2017-12-16 02:54:58.563874: step 92680, loss = 0.26, batch loss = 0.15 (13.0 examples/sec; 0.615 sec/batch; 40h:57m:25s remains)
INFO - root - 2017-12-16 02:55:05.052888: step 92690, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 44h:02m:14s remains)
INFO - root - 2017-12-16 02:55:11.439356: step 92700, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 43h:54m:10s remains)
2017-12-16 02:55:11.962971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0256209 -4.2161932 -5.3616552 -5.9955192 -6.3145084 -6.6995716 -6.5942526 -5.9418063 -5.2560725 -4.87409 -5.5574913 -6.5187297 -6.6354594 -6.6873083 -6.852931][-3.2816539 -4.1816177 -5.3134651 -6.0280848 -6.48519 -5.8515816 -5.397315 -5.0896912 -4.7998877 -5.064395 -6.01318 -7.24139 -7.5223179 -7.2733412 -7.188827][-3.0197034 -3.8021436 -4.7044106 -5.1555171 -5.2368889 -4.8699555 -4.5990262 -4.4391546 -4.3054581 -4.6279135 -6.2963538 -7.4275765 -7.5616307 -7.9845753 -7.6421342][-2.9265752 -3.4153805 -3.7029185 -3.7271557 -3.7164648 -3.5200043 -3.1197095 -3.137424 -3.0063004 -3.2789388 -4.832891 -6.4443345 -7.2497387 -7.4785147 -7.474843][-3.9889007 -4.0621724 -4.0781555 -3.208272 -2.17693 -1.3271208 -1.1525469 -1.5166082 -2.2506442 -3.1659551 -4.5401459 -5.8982325 -6.2085376 -6.8974676 -7.1763177][-5.2258434 -5.1614718 -4.3217397 -3.134212 -1.4525595 0.12757015 0.89602661 0.26741791 -0.81984663 -2.2197332 -4.8562961 -6.5834455 -6.8983088 -6.9192805 -6.9347944][-5.0826774 -4.8358731 -4.3404121 -2.6570311 -0.030703068 2.0850973 3.1374292 2.769722 2.0402012 -0.11205864 -3.5355039 -5.8809328 -6.6405492 -6.5803294 -6.5671177][-4.4639425 -4.3399911 -3.7582386 -1.9714227 0.69629765 3.2476263 4.5502768 4.0815582 3.0155582 0.78646755 -1.9674416 -4.7592039 -5.5347242 -6.0107536 -6.1186705][-5.0000567 -4.161633 -3.4889412 -2.2450428 -0.25117159 1.9536085 3.4558668 3.4714756 2.516552 0.45173168 -2.5699792 -4.9763985 -5.72035 -6.2205949 -6.5071154][-6.0197186 -5.6644268 -4.8233528 -3.6957636 -2.266242 -0.65400791 0.77170658 1.4606476 1.4012003 -0.76398563 -4.2355642 -6.6916084 -6.9493365 -7.1143932 -7.2657127][-7.1554623 -6.7621851 -6.294879 -5.4355421 -4.5764155 -3.2861652 -2.3290877 -1.6281919 -1.2402644 -2.2587619 -4.3320169 -6.5660167 -7.184165 -7.5290384 -7.1638422][-6.8824453 -6.6977906 -6.7778254 -6.2803073 -5.9943767 -5.3579168 -4.5128732 -4.2457948 -4.0904274 -4.2747149 -5.0402284 -6.439887 -6.3924007 -7.28029 -7.1441765][-6.7448406 -6.2349424 -5.8216496 -5.8868351 -6.20363 -6.1668596 -6.3082123 -6.1380944 -5.7548351 -5.9615431 -6.2651443 -6.9952583 -7.5222731 -8.06662 -8.1142206][-7.4333739 -6.6303968 -5.667088 -5.3337097 -5.7321854 -6.2643795 -6.8609486 -7.2814493 -7.637177 -7.7307386 -7.2245841 -7.0308723 -6.8857031 -7.1937327 -7.4292178][-8.4084578 -8.5682592 -8.1340551 -7.4731278 -6.9634094 -6.9305034 -7.0034332 -7.2925839 -7.6922879 -7.7941318 -7.8360105 -7.3434424 -6.448504 -6.0848389 -6.1006823]]...]
INFO - root - 2017-12-16 02:55:18.415838: step 92710, loss = 0.34, batch loss = 0.22 (12.8 examples/sec; 0.627 sec/batch; 41h:44m:16s remains)
INFO - root - 2017-12-16 02:55:24.903015: step 92720, loss = 0.26, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 44h:21m:29s remains)
INFO - root - 2017-12-16 02:55:31.350848: step 92730, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 43h:34m:41s remains)
INFO - root - 2017-12-16 02:55:37.763192: step 92740, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 42h:48m:32s remains)
INFO - root - 2017-12-16 02:55:44.180849: step 92750, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.638 sec/batch; 42h:29m:32s remains)
INFO - root - 2017-12-16 02:55:50.606672: step 92760, loss = 0.26, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 44h:08m:34s remains)
INFO - root - 2017-12-16 02:55:57.048280: step 92770, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.638 sec/batch; 42h:28m:25s remains)
INFO - root - 2017-12-16 02:56:03.534430: step 92780, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 42h:37m:22s remains)
INFO - root - 2017-12-16 02:56:09.942363: step 92790, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.641 sec/batch; 42h:38m:56s remains)
INFO - root - 2017-12-16 02:56:16.270267: step 92800, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 41h:45m:44s remains)
2017-12-16 02:56:16.775211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3315644 -2.0202785 -2.15792 -2.3840427 -2.9656529 -3.7083185 -4.1595659 -4.2256937 -3.9594274 -4.9696875 -5.49965 -6.32333 -6.9887328 -7.49295 -8.1114273][-3.3394003 -3.1712818 -3.3565044 -3.5151472 -3.9273462 -4.1417756 -4.3533621 -4.5867405 -4.6011496 -5.4801211 -5.6880884 -6.54644 -7.5170197 -8.050808 -8.7091866][-3.1372085 -2.7763662 -2.7232456 -2.8087912 -2.9240742 -3.1982698 -3.371038 -3.5424657 -3.3575668 -4.5737448 -5.1140218 -6.040071 -6.65458 -7.3053169 -8.2215776][-3.4862452 -2.9851484 -2.8492465 -2.4936404 -2.3747139 -2.0724392 -1.6839385 -1.6669269 -1.6810069 -3.2384682 -4.073761 -5.1852512 -6.1828346 -7.0954046 -7.8844686][-3.2847886 -2.8301258 -2.379107 -1.9780264 -1.6086831 -1.1422815 -0.74381447 -0.65672684 -0.66702271 -2.1405578 -2.9299779 -4.0384359 -4.9826365 -6.0302887 -7.0197511][-3.575995 -2.7248783 -2.0840502 -1.5509725 -1.0596561 -0.40684271 0.19228554 0.21615982 0.32885647 -1.0838661 -1.9228163 -2.8992424 -3.7680421 -4.9959393 -6.1750441][-3.2396684 -2.7726898 -1.9660797 -0.88760138 -0.16727829 0.35114956 0.72697735 0.9076519 1.1552248 -0.21564627 -1.0347743 -2.0755215 -2.9942608 -4.2572374 -5.3658538][-3.0494852 -2.3173018 -1.4910097 -0.34533596 0.56103516 0.94423866 1.1373119 1.3036394 1.5892248 0.30982018 -0.50199604 -1.6048536 -2.7969451 -3.8299904 -4.8114376][-2.9642615 -2.2135782 -1.2719588 -0.17958117 0.47339249 0.7988472 0.77418137 0.95768356 1.303977 0.08757925 -0.80979776 -2.1571388 -3.4159017 -4.5528021 -5.3252788][-3.4640784 -2.7704873 -1.8874321 -0.87703276 0.08233881 0.22942495 0.0931592 0.35883236 0.7335434 -0.80921364 -1.9263115 -3.2330637 -4.4001913 -5.1410069 -5.756928][-4.82951 -4.2116156 -3.6206179 -2.9202185 -2.2466483 -1.7581582 -1.7352576 -1.6372089 -1.3782582 -2.8650432 -3.9378266 -4.954257 -5.6596556 -6.381813 -6.8447852][-5.1435571 -4.4631472 -4.1257682 -3.9199641 -3.607111 -3.3391747 -3.4540162 -3.3528671 -3.2151914 -4.2982125 -5.165391 -5.9722228 -6.474689 -7.1261663 -7.693675][-5.7068195 -5.547112 -5.3001308 -4.9927807 -4.7940836 -4.646925 -4.7650728 -4.7632704 -4.6837978 -5.4624939 -6.1214972 -6.3496351 -6.5245395 -6.7435989 -7.0250988][-5.9397697 -5.4226866 -5.1838331 -5.2718945 -5.0287161 -4.9278989 -5.0219707 -5.2060294 -5.34476 -5.992033 -6.2945242 -6.2959213 -6.2349472 -6.3911443 -6.4420991][-7.0193586 -6.7191138 -6.4835386 -6.3205085 -6.1488452 -6.1065693 -5.957551 -6.0693121 -6.2364731 -6.4064617 -6.5915251 -6.6191859 -6.6485457 -6.4781694 -6.2810645]]...]
INFO - root - 2017-12-16 02:56:23.153704: step 92810, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.638 sec/batch; 42h:28m:13s remains)
INFO - root - 2017-12-16 02:56:29.599995: step 92820, loss = 0.29, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 43h:25m:49s remains)
INFO - root - 2017-12-16 02:56:36.009790: step 92830, loss = 0.34, batch loss = 0.23 (12.6 examples/sec; 0.637 sec/batch; 42h:23m:19s remains)
INFO - root - 2017-12-16 02:56:42.458514: step 92840, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 42h:13m:31s remains)
INFO - root - 2017-12-16 02:56:48.853076: step 92850, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 43h:46m:59s remains)
INFO - root - 2017-12-16 02:56:55.331386: step 92860, loss = 0.25, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 42h:00m:55s remains)
INFO - root - 2017-12-16 02:57:01.771000: step 92870, loss = 0.34, batch loss = 0.23 (12.0 examples/sec; 0.666 sec/batch; 44h:20m:05s remains)
INFO - root - 2017-12-16 02:57:08.196551: step 92880, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.643 sec/batch; 42h:46m:29s remains)
INFO - root - 2017-12-16 02:57:14.661862: step 92890, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 43h:24m:23s remains)
INFO - root - 2017-12-16 02:57:21.124240: step 92900, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 42h:30m:01s remains)
2017-12-16 02:57:21.655158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8443904 -5.4431496 -5.3150482 -5.2296677 -5.3615661 -5.9462233 -5.9544325 -5.2598705 -4.4902668 -4.5833397 -5.0429425 -5.3110485 -5.8125691 -6.0355225 -6.24912][-5.110589 -5.0311728 -4.98882 -5.5296988 -5.7825556 -6.0025682 -6.1007466 -5.7729864 -4.8595791 -4.378746 -4.9458919 -4.9515505 -5.9425025 -6.0776234 -6.1417933][-4.8482952 -4.9894457 -4.7559204 -5.1477957 -5.773366 -5.8572607 -5.4862232 -5.1619759 -5.0216579 -4.9074793 -5.3772249 -5.310533 -5.9286594 -6.0074148 -6.1465607][-4.5518856 -4.0457335 -4.2590146 -3.9973881 -3.6857147 -3.6932056 -3.298758 -2.8875856 -2.5073395 -3.2111864 -4.7178359 -5.3940496 -6.400526 -6.7641416 -7.4150367][-3.8948374 -3.2594194 -2.7569122 -2.5433998 -2.4993544 -1.9139833 -0.63922167 -0.51193523 0.052164078 -0.93455172 -2.7664666 -4.1192713 -5.6807775 -6.7120776 -7.6026616][-3.203887 -3.0463481 -2.207592 -1.098165 -0.52972937 -0.099727154 0.65373135 1.0224037 1.321907 0.3567667 -1.277153 -3.0676036 -4.586854 -5.7679863 -6.9213715][-4.1525211 -3.2334547 -2.264461 -0.9417901 0.55857086 1.6486006 2.5308428 2.6521711 2.4780817 1.8105993 -0.66976595 -1.9937754 -3.6962667 -4.6735182 -5.4715052][-3.2664356 -3.0789132 -1.6983104 -0.30051756 0.82547474 1.8736343 3.1405458 3.0141544 3.0927839 2.3494873 -0.66183472 -2.2643337 -3.6619015 -4.3366714 -5.3625712][-3.3407531 -2.997818 -2.2187529 -0.82753086 0.50656128 1.4557877 1.816927 1.9494848 2.2499752 1.3600616 -0.90711212 -2.4305344 -4.1143045 -4.9812832 -5.6140203][-3.4331136 -3.1318498 -2.5908394 -2.0309291 -0.62212515 0.39505386 0.63265324 0.84267521 0.580513 0.13203049 -1.7879338 -2.7219057 -4.4391918 -5.2672462 -5.9203224][-4.9367843 -4.5987334 -4.0769162 -3.6321788 -2.7382689 -2.0751796 -1.6039462 -1.6218653 -1.7313685 -2.2499294 -3.8819623 -4.6178155 -5.1975231 -5.7021103 -6.2820878][-6.67563 -5.8976855 -5.5894723 -5.418663 -4.8607225 -4.0609388 -3.8394749 -4.3518624 -4.349576 -4.601984 -5.0758424 -5.5791826 -6.076508 -6.1680741 -6.5607839][-7.6084666 -7.262219 -6.8471932 -6.6850352 -6.5236931 -6.0493193 -5.4119349 -5.7478132 -5.9706488 -6.0689583 -6.667037 -5.9179997 -5.9818854 -5.9289951 -6.0573997][-7.6367793 -7.6525187 -7.9196949 -7.5383005 -6.7890463 -6.5996423 -6.2459946 -6.0714064 -5.7632937 -5.8916769 -6.5492353 -6.146441 -6.1418242 -5.9325066 -5.9284706][-9.00646 -8.2508039 -8.1607771 -8.340539 -8.0982552 -7.489306 -7.4627476 -7.6139574 -7.31225 -7.021831 -6.8020763 -6.9333854 -6.6970029 -6.115047 -6.1567025]]...]
INFO - root - 2017-12-16 02:57:28.060261: step 92910, loss = 0.25, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 42h:34m:15s remains)
INFO - root - 2017-12-16 02:57:34.445863: step 92920, loss = 0.30, batch loss = 0.19 (12.7 examples/sec; 0.631 sec/batch; 42h:00m:45s remains)
INFO - root - 2017-12-16 02:57:40.883660: step 92930, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 42h:13m:33s remains)
INFO - root - 2017-12-16 02:57:47.235584: step 92940, loss = 0.28, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 42h:35m:28s remains)
INFO - root - 2017-12-16 02:57:53.615938: step 92950, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 43h:14m:10s remains)
INFO - root - 2017-12-16 02:57:59.975982: step 92960, loss = 0.37, batch loss = 0.26 (12.3 examples/sec; 0.651 sec/batch; 43h:19m:53s remains)
INFO - root - 2017-12-16 02:58:06.356166: step 92970, loss = 0.29, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 42h:41m:14s remains)
INFO - root - 2017-12-16 02:58:12.841625: step 92980, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 44h:17m:36s remains)
INFO - root - 2017-12-16 02:58:19.249823: step 92990, loss = 0.29, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 41h:31m:55s remains)
INFO - root - 2017-12-16 02:58:25.676225: step 93000, loss = 0.34, batch loss = 0.22 (12.6 examples/sec; 0.635 sec/batch; 42h:15m:38s remains)
2017-12-16 02:58:26.207186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0911784 -3.1160078 -3.2144189 -3.64225 -4.5107851 -4.9982629 -5.4712934 -5.6373024 -5.6790171 -6.3282218 -7.48858 -8.4118748 -8.4693832 -8.1374025 -8.252634][-3.9155774 -3.8477135 -3.8456254 -4.3372335 -5.1365571 -5.9034324 -6.1974673 -6.7723994 -6.9168539 -7.3191772 -8.0029087 -8.7027283 -8.767107 -8.27161 -8.9744167][-2.9102697 -3.4055228 -3.449667 -4.0836306 -4.8055549 -5.1593132 -5.2219257 -5.4767141 -6.06739 -6.750865 -7.6250353 -8.4586277 -8.5183077 -8.06819 -8.1739159][-2.2504869 -2.5478091 -2.9362845 -3.3847189 -3.981822 -4.2960539 -3.9266148 -3.9564679 -4.2888136 -5.3463421 -6.4675069 -7.4808493 -7.671813 -7.3905864 -7.4416785][-3.327703 -3.4356284 -3.58396 -3.4814854 -3.5630822 -3.2079687 -2.3462167 -1.9350271 -1.9772501 -3.1246953 -4.6128931 -6.325768 -6.7662425 -6.7926912 -6.9467854][-3.6249261 -3.9132519 -3.7375014 -3.3321443 -3.0025177 -2.0761828 -0.95107412 -0.24451303 0.40494251 -0.50496197 -2.0590868 -3.7104483 -4.7908916 -5.4928823 -5.8972912][-3.7016268 -3.2613277 -3.011992 -2.6026912 -1.5381546 -0.53525877 0.82241154 1.3347235 1.5426083 0.73527431 -0.93208075 -2.7561731 -4.1631985 -5.0334549 -5.6019464][-4.1434517 -3.5806856 -2.875906 -2.127655 -1.1649542 0.12847328 1.6226082 2.1027842 2.2742176 1.2320967 -0.61793137 -2.5911069 -4.1708632 -5.3404307 -6.1136842][-5.3401394 -4.7742262 -3.9121475 -2.7595539 -1.4784927 -0.4335289 0.71392536 1.1358356 1.5087299 0.71223259 -1.6123252 -3.8096426 -5.330996 -6.1487908 -7.0377126][-4.8820534 -4.8133106 -3.9217272 -3.2723966 -2.3126364 -1.3641901 -0.90507841 -0.58221292 -0.53754091 -1.3055105 -2.8121405 -4.7926717 -6.0592704 -6.9224887 -7.8426175][-6.1580706 -5.7969451 -5.1311111 -4.269578 -3.4478726 -2.8173661 -2.2731032 -2.2267284 -2.3435812 -3.1644192 -4.6178246 -6.2061439 -6.9849167 -7.0728974 -7.8855209][-6.5994363 -6.330101 -5.9139538 -5.2857475 -4.7462754 -4.254415 -3.7718496 -3.5592346 -3.3879366 -3.926455 -4.9956675 -6.3026328 -6.5096445 -6.7147675 -7.2070327][-7.0558662 -6.9593182 -6.4311666 -5.8159828 -5.3121777 -4.6201487 -4.3552122 -4.2663183 -4.4683175 -5.2759786 -5.9826403 -6.5720015 -6.9261408 -6.7240229 -6.8928642][-7.0109429 -6.7516141 -6.359025 -5.6603842 -5.1064334 -4.3313985 -4.1758137 -4.0811391 -4.1584597 -4.7419806 -5.3171916 -6.0307651 -6.2471447 -6.2069712 -6.2421646][-7.7415276 -7.4451151 -6.8800473 -6.1866441 -5.4044838 -4.9528484 -4.9539089 -5.0247498 -4.8864326 -5.214283 -5.427053 -5.8500075 -6.3673105 -6.2580018 -6.3122797]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 02:58:32.621966: step 93010, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 42h:53m:11s remains)
INFO - root - 2017-12-16 02:58:39.031439: step 93020, loss = 0.29, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 43h:46m:55s remains)
INFO - root - 2017-12-16 02:58:45.443416: step 93030, loss = 0.29, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 42h:54m:37s remains)
INFO - root - 2017-12-16 02:58:51.799174: step 93040, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 42h:00m:00s remains)
INFO - root - 2017-12-16 02:58:58.126899: step 93050, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 43h:02m:29s remains)
INFO - root - 2017-12-16 02:59:04.520921: step 93060, loss = 0.27, batch loss = 0.15 (12.8 examples/sec; 0.626 sec/batch; 41h:40m:00s remains)
INFO - root - 2017-12-16 02:59:10.958708: step 93070, loss = 0.24, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 41h:21m:51s remains)
INFO - root - 2017-12-16 02:59:17.303626: step 93080, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 42h:37m:41s remains)
INFO - root - 2017-12-16 02:59:23.758804: step 93090, loss = 0.31, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 43h:22m:01s remains)
INFO - root - 2017-12-16 02:59:30.121644: step 93100, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 42h:23m:15s remains)
2017-12-16 02:59:30.618106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.007678 -4.748126 -4.6596661 -4.9207582 -5.045126 -4.1374655 -3.6986592 -3.6568251 -3.2712216 -4.7300711 -5.3047194 -6.6068068 -7.4936743 -8.2626019 -9.245575][-5.1631584 -4.9803195 -5.1988935 -5.7070189 -5.9174776 -5.7039075 -5.3012466 -4.2185249 -3.6610441 -5.3185244 -6.3325739 -7.5788894 -8.4462461 -8.62594 -9.194315][-5.3529444 -4.5115724 -4.6113443 -4.4555407 -4.0795851 -3.75981 -3.3375249 -2.9671612 -2.4523983 -4.0054064 -4.9765773 -6.5255127 -7.3828669 -7.6673484 -8.2550611][-5.1389494 -4.5391941 -3.8353834 -3.0826864 -2.3103924 -1.3693857 -0.60031414 -0.22981071 -0.32414818 -2.6653137 -4.1684155 -5.6798611 -6.880918 -7.1180534 -7.4321418][-4.99678 -3.7107608 -2.4589405 -1.5662608 -0.49376059 0.6891222 1.424468 1.5132942 0.94768143 -1.2962375 -3.0961571 -5.00346 -6.1692104 -6.6204362 -7.32699][-4.9740238 -4.08109 -2.8852687 -1.1427822 0.55673313 2.4192953 3.6686478 3.9302244 3.4297419 0.38255405 -1.9216018 -4.0308666 -5.3138695 -5.677927 -5.829576][-5.3060865 -4.0940051 -2.507575 -0.512537 1.6740351 3.7920475 4.9155951 4.7729521 4.08088 0.96889019 -1.2677302 -3.977793 -5.3668141 -5.6744194 -6.1705379][-5.1538291 -4.3878174 -2.9462781 -0.69818163 1.1689949 3.2938194 4.1173439 4.1552582 3.7437534 0.51845074 -1.7381339 -4.3549061 -5.5942235 -6.0649357 -6.1368041][-5.3522711 -4.4591837 -3.5985699 -1.8928728 -0.41757393 1.5058775 2.4689589 2.9676342 2.858242 -0.0076971054 -1.6448731 -3.9662712 -5.2221727 -5.9413338 -6.6228552][-6.3596926 -5.6415229 -4.5555816 -3.27662 -1.9177599 -0.18259048 0.53109455 0.81002617 0.79336357 -1.2827067 -2.8703213 -4.5392733 -5.2823062 -5.6492958 -6.0149097][-6.7367744 -6.3766308 -5.6725407 -4.3087606 -3.280622 -2.1391864 -1.5517054 -1.4466925 -1.6346273 -3.0251279 -3.7585132 -4.8150711 -5.4178543 -5.4335403 -5.7752452][-7.3870296 -7.0722961 -6.7927527 -6.1892271 -5.4788656 -4.6009541 -4.1130266 -3.9531095 -3.9654207 -5.0999451 -5.5056553 -5.6531663 -5.5424557 -5.2613215 -5.4502931][-6.6361022 -6.93676 -7.3447785 -6.975482 -6.7264585 -5.9421215 -5.0777912 -5.2506251 -5.3773727 -6.0716448 -6.2918262 -6.5896473 -7.0168839 -6.7111411 -6.353313][-6.7776303 -6.453701 -6.2569208 -6.0154362 -5.81575 -5.5133362 -5.5069146 -5.6985984 -5.6438503 -6.0849495 -6.0817819 -5.9932318 -5.8321676 -5.8586988 -6.4146924][-7.2338505 -7.2135749 -7.1571484 -6.7845573 -6.8987956 -6.6453385 -6.1126781 -6.20047 -6.5817661 -6.7305274 -6.6364465 -6.6532326 -6.7648988 -6.7081404 -6.3196483]]...]
INFO - root - 2017-12-16 02:59:36.959293: step 93110, loss = 0.32, batch loss = 0.20 (12.6 examples/sec; 0.634 sec/batch; 42h:07m:48s remains)
INFO - root - 2017-12-16 02:59:43.365122: step 93120, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 43h:04m:10s remains)
INFO - root - 2017-12-16 02:59:49.743697: step 93130, loss = 0.25, batch loss = 0.14 (13.0 examples/sec; 0.613 sec/batch; 40h:45m:41s remains)
INFO - root - 2017-12-16 02:59:56.156100: step 93140, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 42h:39m:23s remains)
INFO - root - 2017-12-16 03:00:02.541144: step 93150, loss = 0.30, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 42h:13m:49s remains)
INFO - root - 2017-12-16 03:00:08.980702: step 93160, loss = 0.30, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 42h:06m:24s remains)
INFO - root - 2017-12-16 03:00:15.490646: step 93170, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.632 sec/batch; 42h:02m:36s remains)
INFO - root - 2017-12-16 03:00:21.932587: step 93180, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 42h:42m:20s remains)
INFO - root - 2017-12-16 03:00:28.361393: step 93190, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 42h:57m:10s remains)
INFO - root - 2017-12-16 03:00:34.834579: step 93200, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 43h:12m:18s remains)
2017-12-16 03:00:35.370141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0666633 -4.6193542 -3.932241 -3.450211 -2.7821846 -2.3136768 -2.3178282 -2.1682849 -1.8225083 -2.6615653 -2.8966231 -4.5334659 -6.1454544 -7.548594 -8.3153343][-5.4681759 -5.1918364 -4.8942957 -4.593152 -4.0850048 -3.5987148 -3.3977127 -2.9007111 -2.6500869 -3.8966272 -4.2981968 -5.4132237 -6.4045649 -7.5488987 -8.7200537][-4.6788507 -4.4133263 -4.206934 -4.0801115 -4.2610426 -4.0474577 -3.5131993 -3.0205741 -2.489747 -3.528955 -4.1222081 -5.2212515 -6.1822538 -7.1872039 -7.941813][-3.3703141 -3.0557199 -3.0455437 -2.6890121 -2.5684834 -2.6294575 -2.5810452 -2.1796384 -1.1680517 -1.9766583 -2.5716882 -3.799459 -4.9951487 -5.8834453 -6.3396139][-3.7381616 -2.3767676 -1.7673621 -1.4757919 -1.2719707 -0.94985628 -1.0130429 -0.99211693 -0.754302 -1.7180238 -1.9429646 -3.0675821 -4.3758683 -5.43033 -6.1060114][-2.2751155 -1.5152841 -0.82944775 -0.35636902 -0.14743614 0.39185429 0.79197693 0.86267757 0.74797153 -1.0231524 -1.9425187 -3.308135 -4.1910162 -4.9753714 -5.6119928][-1.4900241 -0.90166283 -0.14408159 0.630888 1.2638311 1.5270491 1.6016903 1.7181063 1.9510984 -0.080322266 -1.3628531 -3.248302 -4.7444143 -5.4957438 -5.6511092][-0.83803797 -0.058441162 0.74569321 1.2954798 1.9893837 2.4232416 2.5366888 2.4841261 2.5347023 0.73477077 -0.39903975 -2.4359598 -4.2385788 -5.4515748 -5.9977055][-0.7600069 -0.067998886 0.36121845 1.084094 1.8075314 2.2064924 2.2462282 1.9553862 1.597847 -0.018406868 -1.0164795 -3.2739763 -5.0231361 -5.8842564 -6.4007769][-1.401916 -0.96465969 -0.531209 0.20846987 0.79874134 0.90061092 0.9548645 0.684741 0.16912508 -1.6614423 -2.4133677 -4.1076064 -5.4491191 -6.5602779 -6.8613424][-3.6584759 -3.6525183 -3.1490626 -2.4187684 -1.5371346 -1.1653409 -0.77054548 -0.94365025 -1.2208495 -2.8540363 -3.7373857 -4.930872 -5.7772694 -6.4592628 -6.6622276][-5.8531175 -5.8345971 -5.4051495 -4.8030787 -4.2845125 -4.0616207 -3.5795507 -3.4024777 -3.3063583 -4.6031451 -5.5906749 -6.4995866 -7.1598454 -7.2037525 -6.9288664][-7.8079133 -7.3385754 -6.9656286 -6.1098638 -5.3070059 -4.9374728 -4.8376679 -4.9711542 -4.7624049 -5.5210028 -5.8926458 -6.255177 -6.8625493 -7.1226397 -7.00376][-7.9257455 -7.5501776 -7.0191832 -6.6507936 -6.3005447 -5.4762554 -5.4218121 -5.5621023 -5.2203045 -5.5482988 -5.6098738 -5.9258885 -6.1757894 -6.4428139 -6.772963][-9.1950684 -8.7765131 -8.0697165 -7.65459 -7.7623153 -7.127738 -6.7552562 -6.3896074 -6.5682182 -6.7342048 -6.5062861 -6.6970305 -6.5555634 -6.6628561 -6.6816206]]...]
INFO - root - 2017-12-16 03:00:41.786543: step 93210, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 42h:19m:15s remains)
INFO - root - 2017-12-16 03:00:48.149196: step 93220, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 41h:49m:38s remains)
INFO - root - 2017-12-16 03:00:54.497048: step 93230, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 43h:10m:17s remains)
INFO - root - 2017-12-16 03:01:00.976092: step 93240, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 42h:40m:29s remains)
INFO - root - 2017-12-16 03:01:07.469055: step 93250, loss = 0.31, batch loss = 0.19 (12.0 examples/sec; 0.665 sec/batch; 44h:13m:00s remains)
INFO - root - 2017-12-16 03:01:13.887890: step 93260, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.648 sec/batch; 43h:04m:03s remains)
INFO - root - 2017-12-16 03:01:20.380117: step 93270, loss = 0.26, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 44h:53m:57s remains)
INFO - root - 2017-12-16 03:01:26.762188: step 93280, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 42h:39m:38s remains)
INFO - root - 2017-12-16 03:01:33.173441: step 93290, loss = 0.31, batch loss = 0.19 (13.0 examples/sec; 0.615 sec/batch; 40h:52m:06s remains)
INFO - root - 2017-12-16 03:01:39.701917: step 93300, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.630 sec/batch; 41h:52m:24s remains)
2017-12-16 03:01:40.204972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2076325 -4.35878 -5.1004219 -6.4469862 -6.4087887 -6.2030544 -6.5364 -6.3769822 -6.1782103 -7.810288 -6.0555725 -8.64248 -8.339201 -6.6597757 -4.4961905][-1.5636296 -2.5870605 -2.9214902 -3.856519 -3.8312466 -3.7140553 -4.425756 -4.7142744 -5.2219162 -6.8141155 -4.7101569 -7.1926289 -6.809104 -5.8176589 -4.5523758][-0.79794455 -1.3853273 -2.2325802 -2.9839921 -2.6354632 -2.3696914 -3.0131583 -3.3519192 -4.0789285 -5.7703652 -4.1996918 -7.322175 -7.9183755 -8.1685534 -7.9789767][-2.7268543 -2.3589125 -2.7919679 -2.2973094 -2.0382533 -1.9600329 -1.7894139 -1.973506 -2.9522705 -5.2779427 -4.2848577 -7.3612947 -8.6846685 -8.8412142 -9.2579813][-3.7313879 -2.9689622 -3.3514576 -1.9321136 -0.78488636 -0.45842266 0.23011446 0.35366631 -0.48065138 -2.4356008 -1.8686805 -4.6954689 -6.98592 -8.17832 -8.62391][-5.8971119 -4.2162485 -4.4203796 -3.1888752 -1.6976519 -0.6349597 0.9401598 1.4939528 1.2854557 0.41949654 0.96614361 -1.612421 -3.2528849 -5.0099878 -6.312192][-5.0813785 -4.0309453 -3.5132108 -1.4507537 0.056620121 1.7922163 3.4420347 3.0840168 3.0496187 1.8228989 2.4580956 0.36406803 -2.0971994 -3.8164916 -4.9552155][-4.4236097 -3.70194 -2.693172 -0.30579853 2.0685053 4.4197283 6.1456242 6.3553648 6.4937611 4.0195818 4.7056007 2.6827059 0.38619518 -1.2487769 -3.2321568][-4.6557755 -4.1261644 -3.3030877 -1.0192509 0.85233879 2.8488665 4.924386 6.3277063 6.9413185 4.7758818 4.6358852 1.3221512 -1.7570477 -3.2351589 -5.1955595][-6.5038819 -5.857296 -5.3570538 -3.486537 -1.5508132 0.57535267 2.3256168 3.6371288 4.1579218 2.1741343 2.5007372 -1.0403142 -4.1217585 -5.4861646 -6.5363121][-8.3757906 -7.7135768 -7.8388724 -6.1240244 -4.4216042 -2.4675035 -0.615644 0.38588905 1.49648 0.24444723 0.45036221 -2.306406 -5.2746944 -6.8289137 -8.2350311][-8.92409 -8.0916538 -8.5593452 -8.269762 -6.70502 -5.2351274 -4.1050825 -2.7343416 -1.5255384 -2.2242446 -1.4952879 -3.1295457 -4.8980541 -6.2156835 -7.4731727][-8.52357 -8.0571709 -9.0490446 -8.6920719 -7.981286 -7.076478 -6.231432 -5.6912727 -5.021142 -4.8902125 -4.0869446 -4.6301918 -5.1487942 -5.3948889 -6.08937][-8.6582 -8.56217 -8.74281 -8.3916035 -8.4690876 -8.0364542 -7.6168804 -7.5405526 -6.8703508 -7.0360336 -6.7862124 -6.9533114 -6.7967076 -6.1668558 -5.3005853][-9.0449944 -8.9904814 -9.4391212 -8.6796427 -7.87587 -7.6963038 -7.4451547 -7.8858061 -7.6795082 -8.00083 -7.652534 -7.2484932 -7.1001945 -6.5893607 -6.30011]]...]
INFO - root - 2017-12-16 03:01:46.653650: step 93310, loss = 0.28, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 41h:52m:40s remains)
INFO - root - 2017-12-16 03:01:53.028152: step 93320, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 44h:11m:15s remains)
INFO - root - 2017-12-16 03:01:59.522819: step 93330, loss = 0.27, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 43h:56m:38s remains)
INFO - root - 2017-12-16 03:02:05.898389: step 93340, loss = 0.27, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 43h:32m:49s remains)
INFO - root - 2017-12-16 03:02:12.329248: step 93350, loss = 0.29, batch loss = 0.18 (12.5 examples/sec; 0.639 sec/batch; 42h:25m:00s remains)
INFO - root - 2017-12-16 03:02:18.716268: step 93360, loss = 0.31, batch loss = 0.19 (12.8 examples/sec; 0.627 sec/batch; 41h:39m:44s remains)
INFO - root - 2017-12-16 03:02:25.266566: step 93370, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 42h:21m:30s remains)
INFO - root - 2017-12-16 03:02:31.651208: step 93380, loss = 0.33, batch loss = 0.22 (12.5 examples/sec; 0.640 sec/batch; 42h:30m:14s remains)
INFO - root - 2017-12-16 03:02:38.063866: step 93390, loss = 0.37, batch loss = 0.26 (12.2 examples/sec; 0.656 sec/batch; 43h:35m:00s remains)
INFO - root - 2017-12-16 03:02:44.442383: step 93400, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.623 sec/batch; 41h:21m:37s remains)
2017-12-16 03:02:44.961104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3075976 -2.7525249 -3.352037 -4.5276051 -5.7968955 -6.8390117 -7.984272 -9.4162607 -10.601567 -12.105404 -13.646579 -13.726366 -12.799528 -11.524589 -10.546884][-1.0429029 -2.6250477 -3.8096721 -4.5508385 -5.3811255 -6.4063411 -7.4132309 -8.34977 -8.9200993 -9.8582773 -10.988331 -12.230016 -12.81562 -12.481151 -12.436206][-1.9617786 -2.3998652 -3.2463341 -3.5554647 -4.3025484 -4.9346762 -5.4428086 -6.4188094 -7.0166259 -7.7688813 -8.4415264 -9.3735733 -10.318723 -11.202808 -12.695374][-4.4668751 -4.5447369 -4.649332 -3.7735164 -2.6987648 -2.1567245 -2.0277715 -3.0103517 -3.5960579 -4.8453774 -6.2492208 -7.9146748 -8.76959 -9.6296854 -11.438841][-7.7884588 -6.7882214 -5.7926412 -4.9285049 -4.0386376 -2.2311225 -0.51676035 -0.35932875 -0.49038887 -2.6408281 -5.3557997 -7.876761 -8.9208126 -10.005275 -11.497099][-8.7703018 -8.3336048 -7.1847363 -5.1709633 -2.7774429 -0.3886795 1.7853861 2.1298294 2.1810484 -0.28893805 -3.7031391 -7.5054398 -9.9943724 -11.202518 -11.66917][-9.550745 -8.2064276 -6.1411533 -3.2151418 -0.0862937 3.027977 5.7742786 6.2147465 6.4712791 3.5870094 -0.66894293 -5.4431458 -8.8414383 -10.611197 -11.678895][-8.7344513 -8.6971235 -7.2942605 -3.733176 0.54995251 3.5903921 5.9492388 6.8733063 7.2268686 4.8847361 1.4915199 -2.9483476 -6.4367313 -8.7148752 -9.9829092][-9.3764381 -8.8127451 -7.7169132 -4.9913807 -1.6201134 1.1816254 3.7363672 4.3989868 4.5104284 2.6833019 -0.40543032 -3.513464 -5.5097151 -7.1395841 -9.0628529][-10.231953 -10.338308 -9.2738819 -7.1540279 -4.44444 -0.91121912 1.9360552 2.6363783 2.9833765 0.74346828 -2.7726154 -5.8960967 -7.8128643 -8.382102 -8.69775][-11.577676 -11.808982 -11.746933 -10.442944 -8.6022882 -5.9609551 -3.1459875 -1.092597 0.54350471 -0.38958883 -3.2792315 -5.9935207 -7.5106812 -8.5364847 -9.4850559][-11.396903 -11.8356 -11.765356 -10.686544 -9.4741354 -8.3188591 -7.1211224 -5.7194891 -4.0368276 -3.6706152 -4.5457535 -5.50679 -6.5697184 -7.4983258 -8.3279867][-10.561743 -11.079596 -11.424088 -10.870267 -9.9710531 -9.030489 -7.9248023 -7.6958494 -7.6687841 -7.8582087 -8.1806889 -7.9804783 -7.7820745 -7.772882 -7.5406251][-9.002636 -9.1204681 -9.1030855 -8.6505537 -8.3331366 -7.8114848 -7.1991043 -6.9327936 -6.7711353 -7.0914536 -7.8009086 -8.1157045 -8.5453081 -8.4994087 -8.1583757][-9.6968622 -9.2688408 -8.6837549 -8.3250847 -7.8569036 -7.5748239 -7.0648479 -7.2397666 -7.8340487 -8.0812187 -7.905436 -7.4876175 -7.1017489 -6.8007684 -6.5400047]]...]
INFO - root - 2017-12-16 03:02:51.446152: step 93410, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 42h:17m:00s remains)
INFO - root - 2017-12-16 03:02:57.902475: step 93420, loss = 0.29, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:05m:37s remains)
INFO - root - 2017-12-16 03:03:04.274641: step 93430, loss = 0.34, batch loss = 0.23 (12.5 examples/sec; 0.640 sec/batch; 42h:28m:45s remains)
INFO - root - 2017-12-16 03:03:10.741648: step 93440, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 42h:22m:12s remains)
INFO - root - 2017-12-16 03:03:17.160311: step 93450, loss = 0.26, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 41h:43m:49s remains)
INFO - root - 2017-12-16 03:03:23.555520: step 93460, loss = 0.26, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 42h:45m:09s remains)
INFO - root - 2017-12-16 03:03:29.858819: step 93470, loss = 0.27, batch loss = 0.16 (12.9 examples/sec; 0.621 sec/batch; 41h:14m:51s remains)
INFO - root - 2017-12-16 03:03:36.207477: step 93480, loss = 0.27, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 41h:46m:27s remains)
INFO - root - 2017-12-16 03:03:42.589536: step 93490, loss = 0.26, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 42h:33m:06s remains)
INFO - root - 2017-12-16 03:03:49.003394: step 93500, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 43h:07m:42s remains)
2017-12-16 03:03:49.547615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.981462 -3.8714201 -4.6235323 -4.6051383 -4.151484 -3.7530706 -3.9132848 -3.581624 -3.1842709 -3.4335175 -3.6910737 -5.522563 -6.297905 -6.1876721 -7.3937874][-3.3957138 -4.1327581 -5.0869741 -5.2152224 -5.0822973 -4.4796119 -3.8944266 -3.8215709 -3.8648229 -4.1745977 -4.3973 -6.1280808 -6.7689772 -6.9956093 -7.7079463][-3.855612 -4.2756176 -4.9174395 -4.98599 -5.1625042 -4.8746433 -4.38826 -4.0311284 -3.50519 -4.1752558 -4.7725716 -6.6986384 -7.1711435 -7.4780512 -8.3111477][-4.5409093 -4.9041424 -5.1656466 -4.7429724 -4.451735 -4.1998086 -3.3048644 -2.7014189 -2.4966035 -3.09193 -3.8133795 -6.0417724 -6.9278378 -7.2911744 -7.8614988][-5.5529952 -5.6369829 -5.7101216 -5.1090565 -4.2821941 -3.3019986 -1.8288364 -1.1803284 -0.8127017 -1.971807 -3.2893672 -5.744833 -6.6674657 -7.1895442 -8.1055937][-7.1311378 -6.7435651 -6.1061773 -4.8997288 -3.5575423 -2.0489092 -0.2858572 0.58917713 0.67530632 -1.1560726 -3.1937432 -6.1081061 -6.8337555 -7.0406055 -7.7425103][-6.8455124 -6.4009323 -5.947515 -4.3117776 -2.1294265 0.17702341 2.1258001 2.7523413 2.8022652 0.99302197 -1.3827415 -4.7579136 -6.0142393 -6.2892694 -6.98396][-6.2595882 -5.9249992 -5.2425694 -3.6239839 -1.5797596 1.4347906 3.8029938 3.9293432 3.4655685 1.6007652 -0.22244835 -3.3164377 -4.6973677 -5.1560574 -5.8652296][-6.2712021 -5.7208323 -5.2611227 -3.8929834 -1.9375644 0.633976 2.9052267 3.3490896 2.9771481 0.82626343 -1.0571218 -3.8463659 -4.9457426 -5.3238535 -6.3664751][-5.953537 -6.0756087 -5.683538 -4.4202271 -2.9737177 -1.0019712 0.75244617 1.0819073 0.94538212 -1.0435243 -3.0417843 -5.6232839 -6.2947907 -5.8938303 -6.7469358][-7.2112746 -7.0697174 -6.7649136 -5.8402977 -4.6804104 -3.2970343 -2.345242 -1.9324298 -1.6599793 -3.0481291 -3.9599476 -6.1846762 -6.9539661 -6.3598971 -6.7598877][-7.4424095 -7.3035069 -7.0504231 -6.292531 -5.6268215 -4.8714495 -3.983372 -3.8699384 -3.896925 -4.3622251 -4.4685297 -5.8920808 -6.3797183 -6.346951 -6.7302356][-8.6027489 -8.0104055 -7.4450893 -6.6575489 -6.1963062 -6.0504293 -5.8829875 -5.5214939 -5.2894864 -6.3099012 -6.2856112 -6.4457784 -6.485744 -6.5565977 -6.7740035][-8.5141239 -8.0598669 -7.5109725 -6.4121146 -5.9115987 -5.9446821 -5.5940647 -5.66009 -5.5982361 -6.0266933 -6.2667203 -6.3911066 -6.3398376 -6.6137137 -6.7805915][-8.7831793 -8.7441082 -8.1272039 -7.288898 -6.7501893 -6.7263379 -6.9879041 -6.9859343 -6.6238027 -6.6153946 -6.76052 -6.4662247 -6.2801218 -5.97237 -5.7500467]]...]
INFO - root - 2017-12-16 03:03:55.942595: step 93510, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 42h:01m:22s remains)
INFO - root - 2017-12-16 03:04:02.380502: step 93520, loss = 0.23, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 42h:39m:28s remains)
INFO - root - 2017-12-16 03:04:08.829203: step 93530, loss = 0.30, batch loss = 0.19 (12.5 examples/sec; 0.638 sec/batch; 42h:19m:39s remains)
INFO - root - 2017-12-16 03:04:15.207839: step 93540, loss = 0.29, batch loss = 0.17 (12.0 examples/sec; 0.664 sec/batch; 44h:04m:44s remains)
INFO - root - 2017-12-16 03:04:21.613295: step 93550, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.647 sec/batch; 42h:57m:40s remains)
INFO - root - 2017-12-16 03:04:28.054521: step 93560, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 43h:08m:32s remains)
INFO - root - 2017-12-16 03:04:34.412273: step 93570, loss = 0.32, batch loss = 0.21 (12.8 examples/sec; 0.623 sec/batch; 41h:19m:41s remains)
INFO - root - 2017-12-16 03:04:40.813903: step 93580, loss = 0.28, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 42h:31m:26s remains)
INFO - root - 2017-12-16 03:04:47.184276: step 93590, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 42h:49m:40s remains)
INFO - root - 2017-12-16 03:04:53.530938: step 93600, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 42h:05m:37s remains)
2017-12-16 03:04:54.116449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4664955 -3.5164657 -3.7947733 -4.4604006 -4.5047965 -3.7317274 -3.0469637 -2.1471176 -1.7336717 -2.9065571 -3.973043 -4.756197 -5.8819056 -6.72144 -7.4285984][-2.7469096 -3.529635 -4.3953872 -5.724401 -6.5056443 -5.1569576 -3.5295234 -2.7958431 -2.1587124 -2.74013 -4.0142736 -5.3290386 -6.2673397 -7.9202528 -8.6267948][-2.583365 -3.8675532 -5.4152937 -5.949439 -6.2220058 -5.6280861 -4.27127 -3.4753332 -2.7385039 -3.7382696 -4.4970617 -5.5768275 -6.702702 -8.225028 -8.7227964][-3.2300849 -3.9253004 -4.7964087 -4.6373949 -4.3120184 -3.9304965 -3.1271787 -2.5995731 -1.897047 -3.2499647 -4.4207563 -5.3635826 -6.28635 -7.3664703 -7.6237059][-3.8479483 -3.2302179 -3.1068616 -2.0861592 -1.4901104 -0.790586 -0.43466091 -0.64029074 -0.58000708 -1.6185937 -2.7237902 -4.5238361 -5.7180443 -6.746171 -7.1679373][-3.7945025 -2.0515947 -0.8626523 -0.20001984 0.0663867 1.5295048 1.8985634 0.79327488 0.2376256 -0.79608965 -1.8478999 -3.4563475 -4.6782036 -6.1551971 -6.6612053][-3.1576262 -1.7994919 -0.49742746 0.18862581 0.79383087 2.4668608 3.5496807 2.662488 1.4497566 -0.56931353 -1.5619354 -2.8224158 -4.0727444 -5.7498894 -6.7380815][-3.0188751 -2.1269259 -1.1180377 0.072289944 1.0918818 2.3106947 3.6727238 3.8045263 2.9516888 0.4278059 -1.6607041 -3.3579211 -4.3472576 -5.8557034 -6.7969961][-3.5168877 -2.862205 -2.3451962 -0.98741865 0.01645422 0.97461224 1.9942751 2.5007372 2.7797098 1.1211824 -1.0478263 -3.6741815 -5.0948634 -6.4373279 -6.7906971][-4.161952 -3.6481233 -3.4817848 -2.5287275 -1.9075165 -1.1533308 -0.1406498 0.48686504 1.1011925 -0.0041804314 -1.8092709 -4.135798 -5.6697531 -7.2941413 -7.5718293][-6.2624021 -5.6734238 -5.4994712 -4.9142962 -4.5931 -3.7804787 -2.9126692 -2.5792437 -2.5691209 -3.8855865 -4.9520364 -6.1523046 -7.1115952 -8.3809128 -8.615653][-7.6020889 -6.8880658 -6.5877128 -6.1210485 -5.9245191 -5.2316217 -4.5840535 -4.7792854 -4.9744968 -6.1102586 -6.8602533 -7.1829295 -7.674984 -8.8334827 -9.1837034][-9.30723 -8.8455181 -8.6324625 -8.0787611 -7.6011972 -6.751471 -6.2016416 -6.6171083 -6.6041932 -7.3063889 -7.5330181 -7.6798034 -7.7520127 -8.2004061 -8.2314205][-9.3422585 -8.8731527 -8.7587986 -8.6126747 -8.3091888 -7.6354694 -6.6767764 -6.3651032 -6.4947605 -7.2257829 -7.3665204 -7.2771673 -7.2454219 -7.3571639 -6.8642592][-8.5653915 -7.7802038 -8.0560856 -8.3228283 -8.4600773 -8.0069323 -7.3971667 -7.1393838 -6.7760687 -6.9185281 -7.190393 -6.6918516 -6.0008979 -6.0601387 -5.7658348]]...]
INFO - root - 2017-12-16 03:05:00.474592: step 93610, loss = 0.28, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 43h:05m:43s remains)
INFO - root - 2017-12-16 03:05:06.911031: step 93620, loss = 0.27, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 43h:12m:36s remains)
INFO - root - 2017-12-16 03:05:13.385769: step 93630, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.634 sec/batch; 42h:05m:10s remains)
INFO - root - 2017-12-16 03:05:19.828819: step 93640, loss = 0.26, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 43h:47m:10s remains)
INFO - root - 2017-12-16 03:05:26.363486: step 93650, loss = 0.28, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 43h:54m:43s remains)
INFO - root - 2017-12-16 03:05:32.827299: step 93660, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.633 sec/batch; 42h:01m:14s remains)
INFO - root - 2017-12-16 03:05:39.213903: step 93670, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.633 sec/batch; 41h:59m:37s remains)
INFO - root - 2017-12-16 03:05:45.512762: step 93680, loss = 0.26, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 42h:25m:59s remains)
INFO - root - 2017-12-16 03:05:52.019529: step 93690, loss = 0.25, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 42h:58m:41s remains)
INFO - root - 2017-12-16 03:05:58.366333: step 93700, loss = 0.28, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 42h:07m:03s remains)
2017-12-16 03:05:58.889626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.274826 -5.5046425 -6.3714948 -6.7430797 -6.7341161 -6.0705781 -5.365056 -4.6456914 -3.88748 -4.4502983 -4.7334032 -5.3737364 -5.8759313 -5.7562618 -6.2601376][-4.4020867 -5.415031 -6.1289062 -6.2666545 -5.8723178 -5.1529655 -4.4978294 -3.7714062 -3.4344454 -4.7064967 -5.4629517 -6.410574 -6.8388166 -6.6427917 -6.7557898][-4.6938 -5.4750772 -6.0611391 -6.3527389 -6.0889421 -5.3567176 -4.6029358 -3.9882593 -4.0687633 -5.1350803 -5.9418907 -7.3223372 -8.3045263 -8.5101509 -8.8701982][-5.6695518 -5.8388071 -5.9675627 -6.0235214 -5.601438 -4.9223948 -4.1062622 -3.4880037 -3.31842 -4.5171714 -5.6133814 -7.0862694 -8.0443945 -8.5863791 -9.2169914][-5.9546957 -5.7249727 -5.0672436 -4.6078806 -3.998312 -3.1142058 -2.5424786 -2.6347198 -2.8813777 -4.00156 -4.5548444 -5.5521507 -6.3353953 -7.0312967 -8.1150646][-6.6055827 -6.486455 -5.8124008 -4.4963741 -3.1588063 -1.9734273 -1.2766714 -1.1715708 -1.1261353 -2.58806 -3.0942206 -3.6908848 -3.9329245 -4.3051062 -5.7195635][-6.79936 -6.3517818 -5.7533512 -4.4455914 -2.7010503 -1.1544423 0.13628387 0.27318668 0.26758003 -1.0447087 -1.6145887 -2.497345 -3.2780094 -3.5733342 -4.4799571][-7.6746716 -6.7488813 -5.3104868 -3.0755477 -0.97903681 0.1365881 1.6327 2.1333847 2.3470335 1.0254364 -0.032742023 -1.6481028 -2.9979315 -3.6627612 -4.3121848][-7.6775227 -6.9389729 -5.9152093 -3.92188 -2.123064 -0.20906115 1.5657463 2.5393543 3.2912731 1.1869125 -0.04377985 -1.4616456 -2.5761151 -3.168591 -4.1222563][-8.5217619 -7.8479614 -6.8788629 -5.3735056 -3.5624638 -1.6703477 0.47680187 1.8399944 2.8051929 0.82133961 -0.78062105 -2.317975 -3.6312132 -4.4754086 -5.6619759][-8.9463348 -8.8397474 -8.4174652 -7.26838 -6.0584955 -4.4222388 -2.7009554 -1.9026318 -0.8064127 -1.9756494 -3.2020817 -4.6500416 -5.7614088 -6.3248029 -6.8888183][-8.8725624 -8.6395836 -8.56504 -8.1841936 -7.5523491 -6.394321 -5.3713737 -4.9222326 -4.4905462 -4.8450055 -4.7533355 -5.80077 -6.7678103 -7.2913394 -7.8818712][-9.0211391 -8.56481 -8.2205038 -8.1371489 -8.1045227 -7.6980891 -6.9196377 -6.6112132 -6.4108324 -6.6829247 -6.35612 -6.4821062 -6.6084437 -6.7473221 -7.33689][-8.5912437 -8.1018171 -7.61297 -7.2224722 -7.24514 -7.3703208 -7.1733975 -7.1490936 -7.214963 -7.3611455 -7.4939709 -7.5170388 -7.0929708 -6.7877855 -6.9318619][-8.4841318 -8.6886845 -8.3316584 -7.4463339 -6.923327 -6.8766818 -7.073288 -7.554791 -7.9569259 -7.9660535 -8.0447721 -8.2889061 -8.5969887 -8.3832378 -8.0339336]]...]
INFO - root - 2017-12-16 03:06:05.288187: step 93710, loss = 0.29, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 43h:50m:22s remains)
INFO - root - 2017-12-16 03:06:11.775528: step 93720, loss = 0.27, batch loss = 0.16 (12.5 examples/sec; 0.642 sec/batch; 42h:33m:07s remains)
INFO - root - 2017-12-16 03:06:18.207888: step 93730, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 44h:21m:05s remains)
INFO - root - 2017-12-16 03:06:24.640611: step 93740, loss = 0.31, batch loss = 0.20 (12.2 examples/sec; 0.655 sec/batch; 43h:25m:02s remains)
INFO - root - 2017-12-16 03:06:31.080708: step 93750, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 42h:24m:38s remains)
INFO - root - 2017-12-16 03:06:37.479560: step 93760, loss = 0.25, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 41h:42m:39s remains)
INFO - root - 2017-12-16 03:06:43.882678: step 93770, loss = 0.26, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 42h:03m:13s remains)
INFO - root - 2017-12-16 03:06:50.222480: step 93780, loss = 0.25, batch loss = 0.14 (12.8 examples/sec; 0.625 sec/batch; 41h:25m:25s remains)
INFO - root - 2017-12-16 03:06:56.634206: step 93790, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 42h:43m:09s remains)
INFO - root - 2017-12-16 03:07:03.037164: step 93800, loss = 0.29, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 42h:55m:59s remains)
2017-12-16 03:07:03.595469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2442341 -6.5495253 -7.5833039 -8.2804918 -8.8978252 -8.8329115 -8.0482082 -6.8608966 -4.8276687 -5.4468279 -4.5096722 -5.9044814 -6.8993626 -7.091836 -7.574255][-4.83794 -6.1427183 -7.7466645 -9.0797529 -10.380299 -10.597507 -10.068707 -8.539444 -6.55758 -7.3179092 -6.5511241 -7.9763808 -8.9932013 -8.6207848 -8.6502457][-4.4513445 -5.3381481 -7.2363143 -8.4561243 -9.3461266 -9.9656429 -9.6636848 -8.7619286 -7.2083988 -7.8504372 -7.5079761 -9.29877 -9.9859819 -10.079 -10.754207][-6.321691 -6.4453592 -7.5702782 -8.2547436 -8.7258205 -8.1685934 -7.1544318 -6.3438034 -5.3698125 -7.2154946 -7.534729 -9.807395 -10.699244 -10.617669 -10.894886][-7.1795664 -6.94324 -7.4692011 -6.9360857 -6.5659933 -4.9080687 -3.2621484 -2.8986554 -2.4721031 -4.8215837 -5.6777592 -8.4698048 -9.9274578 -10.230167 -10.712236][-9.1355524 -8.0433559 -7.2000418 -5.4940815 -4.1615553 -1.6722989 0.86100674 1.616498 1.9879742 -1.3299122 -3.3471718 -6.6933389 -8.8382473 -9.18222 -9.5800095][-8.68931 -7.8056455 -6.1900988 -3.0628023 -0.43443346 2.2482471 4.636364 5.3306351 5.3293409 1.3329639 -1.004498 -4.5022116 -6.6793838 -7.4087563 -7.9021835][-8.0357819 -6.9199152 -5.3233652 -1.8979549 0.96156788 3.4228077 5.767602 6.4771214 6.2323666 1.8004513 -0.81674623 -4.1614437 -5.7743468 -6.415318 -7.0699358][-8.3517189 -7.9180007 -7.0829859 -4.0976329 -1.2492471 1.344677 2.7690992 3.2942638 3.6148739 -0.39601994 -2.8475261 -5.8504725 -6.8161182 -6.7013 -6.9205337][-9.9857368 -9.5021706 -8.7407255 -6.3700609 -4.3065443 -2.1288972 -0.9640789 -0.60331106 -0.58757257 -4.0067844 -5.8945446 -8.3279705 -8.9335632 -8.3344727 -7.9640474][-11.933774 -11.703426 -11.255836 -9.2641306 -7.3567243 -5.4616327 -4.2598948 -4.1484337 -4.0907774 -7.0532169 -7.9280376 -9.0595741 -9.6887417 -9.47586 -9.19271][-12.08735 -11.540271 -11.407613 -10.639586 -9.5157967 -8.0659447 -7.0557909 -6.4967422 -6.0564504 -8.47186 -9.0235405 -9.3938665 -9.5263643 -9.2514248 -9.0264835][-12.116066 -11.995899 -11.731905 -10.755777 -9.7338047 -8.6479216 -8.0462008 -7.9379511 -7.7329488 -8.9934578 -9.3263988 -9.4461575 -9.1976194 -8.6487207 -8.8699856][-9.8689518 -10.03516 -9.8546991 -9.0422211 -8.2614288 -7.2446589 -6.4586344 -6.83257 -7.0883336 -8.3059721 -8.6119118 -8.7872334 -8.9705324 -7.9673963 -7.9298148][-8.7587681 -8.8671312 -8.6730175 -8.154705 -7.977603 -7.0003023 -6.3829951 -6.8293138 -7.0897956 -7.7684269 -7.9784789 -8.0790873 -8.1288834 -7.590333 -7.417912]]...]
INFO - root - 2017-12-16 03:07:09.971155: step 93810, loss = 0.31, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 42h:37m:47s remains)
INFO - root - 2017-12-16 03:07:16.417033: step 93820, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 42h:53m:33s remains)
INFO - root - 2017-12-16 03:07:22.714010: step 93830, loss = 0.31, batch loss = 0.19 (12.6 examples/sec; 0.634 sec/batch; 42h:01m:16s remains)
INFO - root - 2017-12-16 03:07:29.144660: step 93840, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 42h:04m:53s remains)
INFO - root - 2017-12-16 03:07:35.506735: step 93850, loss = 0.30, batch loss = 0.19 (12.3 examples/sec; 0.652 sec/batch; 43h:12m:50s remains)
INFO - root - 2017-12-16 03:07:41.928429: step 93860, loss = 0.27, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 42h:34m:55s remains)
INFO - root - 2017-12-16 03:07:48.412855: step 93870, loss = 0.28, batch loss = 0.17 (11.8 examples/sec; 0.678 sec/batch; 44h:57m:11s remains)
INFO - root - 2017-12-16 03:07:54.758709: step 93880, loss = 0.25, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:04m:20s remains)
INFO - root - 2017-12-16 03:08:01.186761: step 93890, loss = 0.27, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 42h:07m:32s remains)
INFO - root - 2017-12-16 03:08:07.672106: step 93900, loss = 0.27, batch loss = 0.16 (12.0 examples/sec; 0.666 sec/batch; 44h:08m:34s remains)
2017-12-16 03:08:08.262376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.465971 -1.6698942 -2.2811856 -2.8196177 -3.3142323 -3.9454281 -3.9764214 -3.7659147 -3.4198298 -4.3490896 -5.2037811 -5.8845716 -6.5513191 -7.3241878 -7.6679049][-1.9284239 -2.1114364 -2.6681905 -3.1882577 -3.6533589 -3.8102958 -3.9883349 -4.0016618 -4.0482454 -5.119513 -5.7749639 -6.7116332 -7.6274724 -8.3499546 -8.4948139][-2.2130017 -2.0283117 -2.1693664 -2.3968024 -2.6016059 -2.6707511 -2.6826596 -2.9169111 -3.1288934 -4.3438997 -5.0810103 -5.9598522 -6.72691 -7.519455 -7.8452663][-2.2652707 -2.1837187 -2.1757255 -1.9759765 -1.7866826 -1.5792303 -1.4979134 -1.6221132 -1.8992724 -3.2361536 -4.1651869 -5.1013556 -6.0294533 -6.7768764 -7.0167708][-2.7722697 -2.6707363 -2.4223928 -2.0726123 -1.4964209 -0.85112047 -0.29199314 -0.27992105 -0.33664179 -1.5320783 -2.5268273 -3.7870219 -4.7079196 -5.7448606 -6.303669][-3.5228019 -3.1822109 -2.5824022 -1.9742136 -1.2406025 -0.20935059 0.52160263 0.76286793 0.95165443 -0.12989807 -1.0172868 -1.984148 -2.7131424 -4.0419993 -5.13257][-3.6159091 -3.1810145 -2.651454 -1.7554893 -0.6938796 0.056424141 0.71825314 1.4079857 1.8525562 0.85171032 0.18523932 -0.83600187 -1.8488207 -3.2820005 -4.4946117][-3.3280439 -2.8306217 -2.2054486 -1.311944 -0.32280064 0.59035778 1.2708025 1.7806673 2.2857151 1.4723978 0.67893124 -0.35991955 -1.4760342 -2.9846444 -3.9568267][-3.4961658 -2.7110987 -2.1440907 -1.4392099 -0.57654476 0.38362217 0.98821259 1.2794781 1.5673256 0.79353237 0.15420437 -1.162128 -2.440619 -3.9574273 -4.6327744][-3.3308282 -2.7153087 -2.134212 -1.4125838 -0.795866 -0.1329875 0.37038708 0.63661289 0.85285568 -0.40631533 -1.3819284 -2.6305685 -3.9710412 -4.9700437 -5.5538397][-4.6885271 -4.2293878 -3.8642774 -3.2108331 -2.5616765 -1.9718957 -1.5230231 -1.3675842 -1.2181201 -2.6531196 -3.6582546 -4.6959515 -5.4982939 -6.3934436 -6.7254596][-5.4676905 -5.1925545 -4.6901603 -4.1945114 -3.74905 -3.2385364 -2.8674302 -2.9460592 -2.9743 -4.0357437 -5.1636848 -6.1011643 -6.7760191 -7.1495495 -7.1108642][-6.3453836 -6.1727953 -5.9435635 -5.4751635 -5.0611649 -4.709528 -4.488102 -4.3756771 -4.3612909 -5.2111874 -6.0638862 -6.747149 -7.1565146 -7.2472343 -6.9562588][-6.1657038 -5.8329573 -5.5440068 -5.2575922 -4.8247919 -4.6520004 -4.4987926 -4.5507541 -4.6810908 -5.4668236 -5.878777 -6.1681724 -6.2633896 -6.4756346 -6.2207279][-7.1711359 -6.9885921 -6.6031456 -6.0458279 -5.6872272 -5.562706 -5.5829897 -5.63445 -5.6916952 -5.932446 -6.3189621 -6.5544429 -6.7149439 -6.5643945 -6.2212267]]...]
INFO - root - 2017-12-16 03:08:14.661384: step 93910, loss = 0.27, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 43h:00m:31s remains)
INFO - root - 2017-12-16 03:08:21.015845: step 93920, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.637 sec/batch; 42h:13m:07s remains)
INFO - root - 2017-12-16 03:08:27.440681: step 93930, loss = 0.27, batch loss = 0.16 (12.8 examples/sec; 0.624 sec/batch; 41h:20m:04s remains)
INFO - root - 2017-12-16 03:08:33.897569: step 93940, loss = 0.32, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 42h:29m:33s remains)
INFO - root - 2017-12-16 03:08:40.280406: step 93950, loss = 0.28, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 43h:08m:18s remains)
INFO - root - 2017-12-16 03:08:46.671086: step 93960, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 42h:37m:43s remains)
INFO - root - 2017-12-16 03:08:53.007178: step 93970, loss = 0.30, batch loss = 0.18 (12.9 examples/sec; 0.622 sec/batch; 41h:11m:19s remains)
INFO - root - 2017-12-16 03:08:59.394710: step 93980, loss = 0.31, batch loss = 0.20 (12.7 examples/sec; 0.631 sec/batch; 41h:50m:21s remains)
INFO - root - 2017-12-16 03:09:05.849557: step 93990, loss = 0.28, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 42h:39m:47s remains)
INFO - root - 2017-12-16 03:09:12.337369: step 94000, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.648 sec/batch; 42h:53m:55s remains)
2017-12-16 03:09:12.876550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9800615 -6.0446587 -5.9833994 -6.1040688 -6.4872341 -6.9893565 -7.0145369 -7.2965536 -7.6294613 -8.4003525 -7.9437556 -8.3697681 -7.4041667 -6.406702 -6.1204596][-5.1787519 -5.2226152 -5.225131 -5.3001881 -5.2643032 -5.6926384 -5.9174881 -6.3156395 -6.6040182 -7.4904704 -7.1148024 -7.8554511 -6.907227 -5.5786328 -5.0300822][-4.23775 -3.8546476 -3.5104661 -2.9967432 -2.9372821 -2.9580312 -2.970448 -3.311933 -3.6743712 -5.1660776 -5.1468949 -6.0838165 -5.5133963 -4.9982367 -5.295145][-4.185452 -3.4172401 -3.08458 -1.8880372 -1.184236 -0.44100952 0.17479324 0.02712965 -0.33565807 -1.9065242 -1.8926687 -3.4210362 -3.467411 -3.6800308 -3.8680863][-4.4781814 -3.0540304 -1.8736815 -0.49321079 0.44498634 1.5927563 2.6209974 2.3616657 1.9627047 0.46154308 0.13057327 -1.5942526 -2.3628397 -3.1599951 -4.0816197][-3.6675296 -2.7671762 -2.0320115 -0.17090702 1.213398 2.6722746 3.6899567 3.8106852 4.3170185 2.7980576 2.093133 -0.418159 -1.8279295 -3.0183687 -4.5510788][-4.0949349 -2.8124895 -1.5291228 1.1058655 2.8412924 3.8714123 4.5817318 4.7529736 5.1063137 3.2485971 2.3000507 -0.37508965 -1.7188516 -3.4034581 -5.1804762][-3.9379461 -3.0691328 -1.5801439 0.724555 2.6089125 4.1558609 5.1085758 5.1887264 5.3314838 3.1805582 2.0341902 -0.41872454 -1.9048915 -3.3254738 -4.8674941][-3.8723807 -3.017313 -2.0259666 -0.13375187 0.87054729 2.3057146 3.5967064 3.664032 3.6818466 1.3851509 1.0473547 -1.5440083 -2.7096744 -4.1215649 -5.3827391][-4.2545533 -4.0229321 -3.7140419 -2.3098927 -1.0967536 0.15236616 0.61781883 0.70973587 1.239378 -0.21902895 -0.75599194 -3.2774982 -4.5479174 -5.2431393 -6.1028066][-5.3597059 -4.9948807 -4.7001524 -3.453855 -2.7678208 -1.5300374 -0.78907061 -0.94372416 -0.8412466 -2.0763845 -2.3534908 -3.9012516 -4.4364986 -5.4594922 -6.4569817][-6.4111514 -6.1671934 -6.0481782 -5.278511 -4.5833874 -3.8915215 -3.0458312 -2.6182685 -2.177496 -3.3050585 -3.8205729 -5.2439394 -5.9233265 -6.0405827 -6.6602292][-7.06822 -6.9926586 -7.1037087 -6.3435326 -5.5321884 -4.4197378 -3.7982647 -4.1664448 -3.9085624 -4.0334158 -4.1016688 -4.9162455 -5.7841711 -6.3040113 -6.7050638][-7.4572225 -7.2890792 -7.033987 -6.2389565 -5.65628 -5.0637894 -4.3349409 -3.9578793 -3.8206558 -4.0383792 -4.6043029 -5.2034788 -5.851584 -6.2540617 -6.9957671][-8.5503693 -8.1807117 -7.9729581 -7.4167562 -6.5999031 -5.7236695 -4.9827266 -4.8042817 -4.5027914 -4.5581956 -4.9164314 -5.5638161 -6.2664118 -6.8207822 -7.1355815]]...]
INFO - root - 2017-12-16 03:09:19.242251: step 94010, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 42h:50m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-16 03:09:25.659036: step 94020, loss = 0.27, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 42h:47m:24s remains)
INFO - root - 2017-12-16 03:09:32.055551: step 94030, loss = 0.25, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 42h:09m:43s remains)
INFO - root - 2017-12-16 03:09:38.465129: step 94040, loss = 0.27, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 42h:46m:37s remains)
INFO - root - 2017-12-16 03:09:44.925462: step 94050, loss = 0.29, batch loss = 0.18 (12.6 examples/sec; 0.635 sec/batch; 42h:03m:38s remains)
INFO - root - 2017-12-16 03:09:51.278744: step 94060, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 42h:37m:39s remains)
INFO - root - 2017-12-16 03:09:57.633471: step 94070, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 41h:58m:37s remains)
INFO - root - 2017-12-16 03:10:03.984906: step 94080, loss = 0.28, batch loss = 0.16 (12.6 examples/sec; 0.632 sec/batch; 41h:53m:03s remains)
INFO - root - 2017-12-16 03:10:10.348145: step 94090, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 42h:35m:06s remains)
INFO - root - 2017-12-16 03:10:16.724799: step 94100, loss = 0.25, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 42h:16m:33s remains)
2017-12-16 03:10:17.259302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1287827 -2.2825351 -2.312067 -2.3253465 -2.2454872 -2.1893678 -1.9345999 -1.4881625 -1.5232196 -2.650826 -3.2434025 -4.3806839 -5.7357197 -6.7654333 -8.1146832][-3.2932315 -3.6822519 -3.7058585 -4.0718765 -3.9782734 -3.8751411 -3.4537425 -2.9179168 -2.8459167 -4.1882071 -4.8538761 -6.12978 -7.2763638 -8.3665962 -9.4812555][-4.1900959 -4.2106829 -4.0016656 -3.890928 -3.4688458 -2.7722921 -2.1182671 -1.4828739 -1.2697406 -2.5035677 -3.118156 -4.1827068 -5.3250151 -6.2201815 -7.7808414][-4.59614 -4.3285666 -4.0015087 -3.5294003 -3.0099382 -2.3919606 -1.9503689 -1.3164024 -0.90723515 -2.1273298 -2.899375 -3.8720634 -4.7315702 -5.2667079 -6.3579173][-5.5395994 -5.0575275 -3.9866135 -2.8737717 -1.8638201 -0.71127224 -0.054709435 0.074702263 0.20116711 -1.3277197 -2.2400622 -3.4243741 -4.6207304 -5.085186 -6.4724092][-4.98065 -4.1409416 -3.3465276 -2.1388888 -0.97397661 0.49505329 1.6514626 1.6612177 1.6221104 -0.081962585 -1.0337081 -2.4343376 -3.7246418 -4.3383255 -5.3113737][-4.4613953 -3.2485538 -2.2996736 -0.83659363 0.72260475 2.190835 3.0505362 2.8874922 2.8000622 0.99457932 -0.0078983307 -1.5572205 -3.4556093 -4.3393641 -5.5411777][-3.5094776 -2.4980245 -1.8013229 -0.13332415 1.7952881 3.3796206 4.7672882 4.8546238 4.6053705 2.705884 1.4908113 -0.54515028 -2.7716165 -3.7888212 -5.10744][-2.4977245 -1.9390264 -1.2742081 -0.27864552 1.0294104 2.5346785 3.6861658 4.0457344 4.4435072 2.7435465 1.2362509 -0.69553852 -2.9622006 -4.0113277 -5.4286294][-2.6148577 -2.1341462 -1.8408661 -1.0797577 -0.156497 0.708972 1.5672684 1.8436575 2.1774607 0.42507839 -0.70836687 -2.5151229 -4.1364388 -4.776021 -5.9435434][-3.9747684 -3.8052938 -3.4666834 -2.7940302 -2.2125039 -1.1686468 -0.47266865 -0.41011763 -0.11618614 -1.7082081 -2.876256 -3.9023366 -5.4620743 -5.930923 -6.4834747][-5.7043643 -5.5642047 -5.6972265 -5.2288218 -4.8039141 -4.1620121 -3.0342188 -2.8842292 -2.6302009 -3.6330547 -4.3151565 -5.2932825 -6.1307549 -5.8886147 -6.8688669][-6.6653237 -6.5676446 -6.4609079 -6.0908923 -5.7420912 -5.1687908 -4.3584471 -4.3745613 -4.2193842 -4.9669075 -5.598165 -6.0401926 -6.6004858 -6.4585252 -6.7071419][-7.4310365 -7.0164146 -6.6092482 -6.1956134 -5.5831604 -5.1006908 -4.4550352 -4.5000076 -4.6206455 -5.2264791 -5.6271782 -5.551569 -6.1989951 -6.2131166 -6.3604569][-7.458952 -7.4884605 -7.4159737 -6.9828243 -6.7426424 -6.4902024 -5.8491015 -5.8491287 -5.863699 -6.0660172 -6.2337518 -6.3385706 -6.4073396 -6.0820217 -5.973587]]...]
INFO - root - 2017-12-16 03:10:23.621320: step 94110, loss = 0.28, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 42h:37m:22s remains)
INFO - root - 2017-12-16 03:10:29.943448: step 94120, loss = 0.29, batch loss = 0.18 (11.8 examples/sec; 0.676 sec/batch; 44h:44m:31s remains)
INFO - root - 2017-12-16 03:10:36.360875: step 94130, loss = 0.27, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 42h:05m:49s remains)
INFO - root - 2017-12-16 03:10:42.800564: step 94140, loss = 0.31, batch loss = 0.20 (12.6 examples/sec; 0.637 sec/batch; 42h:11m:40s remains)
INFO - root - 2017-12-16 03:10:49.158435: step 94150, loss = 0.30, batch loss = 0.19 (12.4 examples/sec; 0.648 sec/batch; 42h:52m:13s remains)
INFO - root - 2017-12-16 03:10:55.583836: step 94160, loss = 0.26, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 42h:53m:08s remains)
INFO - root - 2017-12-16 03:11:02.018515: step 94170, loss = 0.31, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 42h:52m:59s remains)
INFO - root - 2017-12-16 03:11:08.534041: step 94180, loss = 0.26, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 43h:07m:29s remains)
INFO - root - 2017-12-16 03:11:14.832610: step 94190, loss = 0.27, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 41h:46m:11s remains)
