INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "257"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-14 03:02:23.797719: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 03:02:23.797790: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 03:02:23.797816: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 03:02:23.797836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 03:02:23.797856: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 03:02:24.147486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-14 03:02:24.147554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-14 03:02:24.147580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-14 03:02:24.147624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-14 03:02:27.665255: step 0, loss = 2.28, batch loss = 2.23 (3.4 examples/sec; 2.340 sec/batch; 216h:07m:32s remains)
2017-12-14 03:02:28.075925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3789659 -4.3788772 -4.378818 -4.378746 -4.3786597 -4.3785634 -4.3784623 -4.3783684 -4.3783021 -4.3782868 -4.3783331 -4.3784418 -4.3785977 -4.3787732 -4.378942][-4.3788347 -4.3787203 -4.3786292 -4.3785181 -4.378386 -4.3782358 -4.3780751 -4.3779311 -4.3778448 -4.3778405 -4.3779244 -4.3780923 -4.37832 -4.3785653 -4.3787918][-4.3787203 -4.3785849 -4.3784623 -4.3783059 -4.3781123 -4.3778839 -4.3776388 -4.3774295 -4.3773255 -4.3773479 -4.3774915 -4.3777447 -4.378067 -4.3783937 -4.3786812][-4.3785915 -4.37844 -4.3782883 -4.3780804 -4.3778124 -4.3774886 -4.3771486 -4.376874 -4.3767648 -4.3768396 -4.3770676 -4.3774261 -4.3778491 -4.378252 -4.378593][-4.3784652 -4.3783007 -4.3781219 -4.3778591 -4.3775134 -4.3771019 -4.3766794 -4.3763547 -4.3762655 -4.3764267 -4.3767614 -4.3772206 -4.3777218 -4.3781734 -4.3785453][-4.3783503 -4.3781729 -4.3779635 -4.3776488 -4.37724 -4.3767672 -4.376287 -4.3759389 -4.3759103 -4.376184 -4.3766193 -4.3771491 -4.377686 -4.3781543 -4.3785353][-4.378243 -4.3780522 -4.3778143 -4.3774633 -4.377028 -4.376543 -4.3760624 -4.3757467 -4.3758221 -4.376204 -4.376699 -4.3772364 -4.3777504 -4.3781953 -4.3785591][-4.3781528 -4.3779497 -4.3776965 -4.37734 -4.3769264 -4.3764977 -4.3761063 -4.3759112 -4.3760967 -4.3765211 -4.3769984 -4.3774691 -4.3779049 -4.3782864 -4.3786082][-4.3780856 -4.3778772 -4.377635 -4.3773131 -4.3769712 -4.376658 -4.3764219 -4.3763757 -4.3766093 -4.3769903 -4.3773823 -4.3777456 -4.3780804 -4.3783884 -4.3786621][-4.37806 -4.3778582 -4.3776464 -4.3773861 -4.3771367 -4.3769503 -4.3768535 -4.3769054 -4.3771286 -4.3774257 -4.3777137 -4.3779669 -4.3782144 -4.3784657 -4.3787045][-4.3780789 -4.3779016 -4.3777366 -4.3775506 -4.3773942 -4.3773055 -4.3772907 -4.3773742 -4.3775477 -4.3777504 -4.3779392 -4.3781061 -4.3782973 -4.3785172 -4.3787355][-4.37813 -4.3779988 -4.3778963 -4.3777885 -4.3777056 -4.3776703 -4.3776793 -4.3777432 -4.3778477 -4.3779612 -4.3780708 -4.3781824 -4.3783445 -4.3785491 -4.3787556][-4.3781781 -4.3781037 -4.378068 -4.378027 -4.37799 -4.3779697 -4.3779607 -4.3779774 -4.37801 -4.3780494 -4.3781066 -4.3781929 -4.3783484 -4.378552 -4.3787575][-4.3781877 -4.3781571 -4.3781724 -4.3781762 -4.3781614 -4.3781323 -4.3780861 -4.3780489 -4.3780208 -4.37801 -4.3780437 -4.3781333 -4.378305 -4.3785233 -4.3787413][-4.3781319 -4.3781233 -4.3781705 -4.3782024 -4.3781967 -4.378149 -4.378067 -4.3779869 -4.37792 -4.3778853 -4.3779221 -4.3780346 -4.3782344 -4.3784785 -4.3787155]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-14 03:02:30.579674: step 10, loss = 6.09, batch loss = 6.02 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - 2017-12-14 03:02:32.820200: step 20, loss = 1.43, batch loss = 1.36 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:55s remains)
INFO - root - 2017-12-14 03:02:35.013911: step 30, loss = 1.49, batch loss = 1.40 (37.6 examples/sec; 0.212 sec/batch; 19h:37m:25s remains)
INFO - root - 2017-12-14 03:02:37.243175: step 40, loss = 0.75, batch loss = 0.67 (36.7 examples/sec; 0.218 sec/batch; 20h:09m:27s remains)
INFO - root - 2017-12-14 03:02:39.440027: step 50, loss = 0.89, batch loss = 0.81 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:29s remains)
INFO - root - 2017-12-14 03:02:41.681825: step 60, loss = 0.93, batch loss = 0.84 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:24s remains)
INFO - root - 2017-12-14 03:02:43.914785: step 70, loss = 0.79, batch loss = 0.71 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:29s remains)
INFO - root - 2017-12-14 03:02:46.132971: step 80, loss = 1.45, batch loss = 1.36 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:00s remains)
INFO - root - 2017-12-14 03:02:48.364797: step 90, loss = 0.87, batch loss = 0.78 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:34s remains)
INFO - root - 2017-12-14 03:02:50.593863: step 100, loss = 0.66, batch loss = 0.58 (34.4 examples/sec; 0.232 sec/batch; 21h:26m:32s remains)
2017-12-14 03:02:50.902556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.69866991 -0.42568398 -0.21943521 -0.07485342 -0.033671856 -0.18979692 -0.34358096 -0.49648404 -0.66151762 -0.80041027 -0.87726307 -0.85855603 -0.77354312 -0.69933271 -0.68378067][-0.46314669 -0.18763709 0.019731522 0.1565454 0.19469428 0.036278725 -0.12039781 -0.27916598 -0.45593834 -0.60900784 -0.68656063 -0.67845273 -0.622771 -0.55939817 -0.54395628][-0.23664927 0.055030346 0.2722187 0.43744922 0.483083 0.32397485 0.1583488 -0.0037162304 -0.17353511 -0.32674098 -0.40147996 -0.39790678 -0.37798238 -0.3555181 -0.36909223][-0.19339633 0.12270641 0.36625791 0.52220035 0.58235216 0.45114636 0.29641056 0.12811637 -0.047282457 -0.1998775 -0.25475931 -0.23759222 -0.23031092 -0.21315908 -0.22544241][-0.19876885 0.11424994 0.36223531 0.52434587 0.62162161 0.536716 0.42018723 0.28014207 0.14164448 0.018512011 -0.020176649 0.012182236 0.0019090176 -0.00016951561 -0.037097216][-0.25291371 0.053673744 0.31013107 0.50037026 0.62941337 0.58017516 0.48355484 0.34995723 0.21690774 0.099737644 0.06839323 0.11555481 0.1249187 0.11760569 0.083080769][-0.32500434 -0.056273937 0.15978193 0.3448236 0.49799848 0.48229504 0.414711 0.29422069 0.17386436 0.067501545 0.045791149 0.1065495 0.13391709 0.14058542 0.11366248][-0.37999606 -0.12520456 0.072061539 0.23770523 0.4032228 0.41714573 0.36230779 0.24648404 0.13002181 0.026589632 0.0048055649 0.072390079 0.10272288 0.12794137 0.10068488][-0.43721128 -0.17826676 0.032405376 0.18746352 0.35378766 0.39472222 0.35528421 0.23408198 0.11069942 -0.0020356178 -0.035988808 0.019247055 0.03738308 0.055057526 0.019423962][-0.41701531 -0.15961814 0.045140266 0.19885445 0.36241603 0.3964746 0.34997177 0.22216988 0.091207266 -0.032683372 -0.081822395 -0.045672178 -0.054619074 -0.053152561 -0.098188877][-0.38091159 -0.13328075 0.059319258 0.21200705 0.37341833 0.40025234 0.34738588 0.21322703 0.073695183 -0.061915874 -0.12629867 -0.11098146 -0.15228605 -0.19497013 -0.26441741][-0.31496882 -0.080332041 0.10170913 0.25109029 0.41147208 0.42334342 0.34834933 0.20982766 0.064253569 -0.080832005 -0.15645552 -0.15557361 -0.22862935 -0.29928827 -0.39862013][-0.26055455 -0.01169014 0.16618705 0.31548572 0.46226573 0.43899298 0.34969497 0.20851922 0.059500456 -0.09030962 -0.1706748 -0.17585421 -0.26712251 -0.3651619 -0.48066902][-0.27168894 -0.0024757385 0.17380524 0.32083941 0.45849657 0.43347836 0.34895086 0.20712805 0.057720423 -0.093453646 -0.17501473 -0.18155909 -0.27719021 -0.38600588 -0.52126503][-0.38183188 -0.16838241 -0.00556159 0.14758682 0.29194069 0.29719496 0.24102092 0.14341807 0.018983841 -0.10737228 -0.17175865 -0.1769948 -0.27276325 -0.39375377 -0.53857207]]...]
INFO - root - 2017-12-14 03:02:53.137443: step 110, loss = 1.18, batch loss = 1.09 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-14 03:02:55.365289: step 120, loss = 1.05, batch loss = 0.96 (36.4 examples/sec; 0.220 sec/batch; 20h:19m:07s remains)
INFO - root - 2017-12-14 03:02:57.609470: step 130, loss = 0.86, batch loss = 0.77 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:36s remains)
INFO - root - 2017-12-14 03:02:59.851864: step 140, loss = 0.56, batch loss = 0.47 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:59s remains)
INFO - root - 2017-12-14 03:03:02.079253: step 150, loss = 1.39, batch loss = 1.30 (35.1 examples/sec; 0.228 sec/batch; 21h:03m:59s remains)
INFO - root - 2017-12-14 03:03:04.309935: step 160, loss = 0.70, batch loss = 0.61 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:25s remains)
INFO - root - 2017-12-14 03:03:06.583925: step 170, loss = 0.73, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:58s remains)
INFO - root - 2017-12-14 03:03:08.823520: step 180, loss = 0.85, batch loss = 0.76 (36.5 examples/sec; 0.219 sec/batch; 20h:15m:23s remains)
INFO - root - 2017-12-14 03:03:11.110711: step 190, loss = 0.59, batch loss = 0.50 (32.6 examples/sec; 0.245 sec/batch; 22h:38m:16s remains)
INFO - root - 2017-12-14 03:03:13.323934: step 200, loss = 0.61, batch loss = 0.52 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:27s remains)
2017-12-14 03:03:13.630314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7611216 -1.6219336 -1.5064348 -1.5578891 -1.72427 -1.9335332 -2.1390524 -2.3318624 -2.5122244 -2.6205702 -2.6599472 -2.6389494 -2.6134334 -2.6127632 -2.637248][-1.7827965 -1.565006 -1.3896723 -1.3926163 -1.5119243 -1.6766169 -1.8664775 -2.0328672 -2.2233124 -2.4052565 -2.5029359 -2.5018797 -2.4756339 -2.4767089 -2.5224025][-1.8122394 -1.4912555 -1.2255927 -1.1194683 -1.1475747 -1.3091258 -1.5326301 -1.7200412 -1.9476721 -2.1440487 -2.2765808 -2.3565063 -2.3768687 -2.3885663 -2.4235656][-1.8350337 -1.4018602 -1.0028377 -0.74811172 -0.69212627 -0.85378456 -1.0826044 -1.2992353 -1.5741465 -1.8554611 -2.0550094 -2.1524062 -2.2379804 -2.33461 -2.4081109][-1.8388925 -1.3108073 -0.82129693 -0.53906608 -0.41451859 -0.49352455 -0.70539236 -0.99104452 -1.3510531 -1.6977224 -1.9418325 -2.0801213 -2.1720774 -2.2929487 -2.3865][-1.7975442 -1.2400039 -0.68751144 -0.33276916 -0.15260553 -0.19204712 -0.420434 -0.71640348 -1.0859916 -1.4973149 -1.804257 -1.9717536 -2.1067495 -2.2306466 -2.332901][-1.8600488 -1.2906938 -0.66678786 -0.22765541 -0.013415575 -0.053653 -0.27496362 -0.55306149 -0.89514208 -1.2549227 -1.5549128 -1.7902908 -1.9947536 -2.1882343 -2.2751989][-1.9529892 -1.365342 -0.739336 -0.25602889 -0.073014736 -0.098877907 -0.28450418 -0.52592707 -0.86698532 -1.1718113 -1.4600102 -1.7188786 -1.9726757 -2.2076039 -2.3212738][-1.9664893 -1.4728711 -0.91006207 -0.41318989 -0.20067096 -0.26164222 -0.42797232 -0.62164378 -0.89997077 -1.1934789 -1.5043569 -1.727851 -1.9341638 -2.189769 -2.372858][-2.0552356 -1.6430349 -1.1703197 -0.71453786 -0.44253755 -0.4720211 -0.5968771 -0.78103924 -1.0442687 -1.2970852 -1.6205655 -1.8889388 -2.065804 -2.2321248 -2.3693113][-2.1360559 -1.7378482 -1.3459718 -0.97592473 -0.77851462 -0.76448941 -0.84193563 -0.95160413 -1.1371278 -1.3962375 -1.7775676 -2.0281587 -2.1959736 -2.3045869 -2.355973][-2.1440177 -1.7717429 -1.4194803 -1.1035155 -0.94485044 -0.98425221 -1.0591196 -1.1132187 -1.2318268 -1.5011388 -1.8521135 -2.0383484 -2.1856062 -2.3029995 -2.3701892][-2.1594031 -1.8378867 -1.5117182 -1.2197788 -1.0812474 -1.1377273 -1.23452 -1.2918046 -1.392608 -1.6583446 -1.9889489 -2.1068714 -2.2279818 -2.3434448 -2.4405932][-2.0172 -1.8331938 -1.5974056 -1.3405119 -1.1834775 -1.2427309 -1.3589119 -1.4131252 -1.5056034 -1.7640818 -2.0465505 -2.1291711 -2.223351 -2.3347945 -2.4583857][-1.9721755 -1.8731807 -1.6376958 -1.3918375 -1.2664819 -1.2979993 -1.3938864 -1.436924 -1.5064926 -1.7782043 -2.0371978 -2.1184213 -2.2445784 -2.3737478 -2.4940987]]...]
INFO - root - 2017-12-14 03:03:15.886832: step 210, loss = 0.66, batch loss = 0.57 (32.7 examples/sec; 0.245 sec/batch; 22h:34m:18s remains)
INFO - root - 2017-12-14 03:03:18.124756: step 220, loss = 0.66, batch loss = 0.58 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:17s remains)
INFO - root - 2017-12-14 03:03:20.400568: step 230, loss = 0.63, batch loss = 0.54 (35.6 examples/sec; 0.225 sec/batch; 20h:45m:17s remains)
INFO - root - 2017-12-14 03:03:22.622493: step 240, loss = 0.79, batch loss = 0.70 (34.4 examples/sec; 0.233 sec/batch; 21h:28m:57s remains)
INFO - root - 2017-12-14 03:03:24.866112: step 250, loss = 0.67, batch loss = 0.59 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:14s remains)
INFO - root - 2017-12-14 03:03:27.142023: step 260, loss = 0.63, batch loss = 0.55 (32.8 examples/sec; 0.244 sec/batch; 22h:29m:28s remains)
INFO - root - 2017-12-14 03:03:29.420936: step 270, loss = 0.67, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 20h:38m:40s remains)
INFO - root - 2017-12-14 03:03:31.648146: step 280, loss = 0.62, batch loss = 0.54 (35.0 examples/sec; 0.228 sec/batch; 21h:04m:11s remains)
INFO - root - 2017-12-14 03:03:33.892325: step 290, loss = 0.71, batch loss = 0.63 (34.4 examples/sec; 0.233 sec/batch; 21h:28m:03s remains)
INFO - root - 2017-12-14 03:03:36.137815: step 300, loss = 0.70, batch loss = 0.62 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:27s remains)
2017-12-14 03:03:36.449839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4308296 -1.3929236 -1.3510249 -1.0971638 -0.83411145 -0.80579162 -0.60867095 -0.72673464 -0.99103951 -1.2603096 -1.4425594 -1.508028 -1.5697279 -1.5280252 -1.7924708][-1.2597483 -1.3452502 -1.2168558 -1.0088921 -0.71898723 -0.67550564 -0.75924373 -0.906831 -0.882004 -1.0216149 -1.1472881 -1.2139632 -1.3384705 -1.4742738 -1.6437714][-0.96240246 -0.87800241 -0.9025116 -0.86499476 -0.46770549 -0.39722872 -0.3040154 -0.39806938 -0.50605488 -0.58888388 -0.75570536 -0.76982117 -0.93632436 -1.1779078 -1.5120444][-0.9867363 -0.74571562 -0.79900217 -0.63863444 -0.69641829 -0.58749342 -0.41825509 -0.53971648 -0.34287786 -0.27259731 -0.42605639 -0.52254605 -0.79536581 -1.2114358 -1.6509966][-0.5776124 -0.59341955 -0.5763185 -0.61142588 -0.61596227 -0.15072036 0.1069448 0.139534 0.50867414 0.39608097 0.29160762 0.19301939 -0.14072466 -0.68609643 -1.2287421][-0.070432663 -0.031777382 0.074663162 0.041826725 0.13567567 0.25345731 0.50111103 0.88865948 1.2706468 1.400141 1.2610748 0.93137741 0.43450189 -0.091614485 -0.67928767][0.13121462 0.11261225 0.076815367 0.32753563 0.42367816 0.74353456 1.1437948 1.5691793 1.9447105 2.0179307 2.1218822 1.9948471 1.3440621 0.6212697 -0.25162649][-0.0808332 -0.12732005 -0.01451993 0.29691052 0.49068832 0.79987383 0.98420954 1.5282428 2.0227592 2.082031 2.1009047 1.8301904 1.3599803 0.71712542 -0.10778666][-0.317513 -0.47124314 -0.4701314 -0.28635955 0.0041553974 0.29078722 0.42523098 0.82543445 1.0824468 1.2235315 1.4336059 1.4152625 1.0327895 0.44220781 -0.19870782][-0.34685826 -0.44849968 -0.36194849 -0.30213237 -0.21907234 -0.058457613 0.14843202 0.44005895 0.64630938 0.74276519 0.84663224 0.74628019 0.41845441 0.20164824 -0.049056292][-0.64881849 -0.58396411 -0.38177943 -0.27940249 -0.22480798 -0.069277048 0.0012981892 0.098530531 0.27545857 0.21770334 0.11855984 0.071644783 -0.1309855 -0.35517359 -0.81746745][-0.86898327 -0.86627388 -0.73706436 -0.4970324 -0.17728496 0.020815849 0.13159323 0.18009567 0.21246815 0.090970278 -0.082576275 -0.17324257 -0.34077454 -0.52276778 -0.61621022][-0.89736819 -0.660367 -0.35639381 -0.076896191 0.2251811 0.40444446 0.43641496 0.22627139 0.051250458 -0.19293523 -0.37605095 -0.50841522 -0.67387271 -0.56420469 -0.44161558][-0.89081621 -0.62286186 -0.32375169 -0.091835022 0.33458924 0.53825974 0.69622207 0.44457102 -0.019731045 -0.37356162 -0.60378003 -0.77101541 -0.88539267 -0.8812964 -1.0495439][-1.3946036 -0.99651885 -0.59410524 -0.13405728 0.37404227 0.68398523 0.96209216 1.0067847 0.6761322 0.34818697 0.031258821 -0.46344495 -0.81654358 -0.96663344 -1.101512]]...]
INFO - root - 2017-12-14 03:03:38.736683: step 310, loss = 0.72, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 21h:08m:53s remains)
INFO - root - 2017-12-14 03:03:40.964729: step 320, loss = 0.61, batch loss = 0.52 (36.0 examples/sec; 0.222 sec/batch; 20h:31m:40s remains)
INFO - root - 2017-12-14 03:03:43.195455: step 330, loss = 0.62, batch loss = 0.54 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:38s remains)
INFO - root - 2017-12-14 03:03:45.456089: step 340, loss = 0.54, batch loss = 0.46 (32.4 examples/sec; 0.247 sec/batch; 22h:46m:24s remains)
INFO - root - 2017-12-14 03:03:47.742044: step 350, loss = 0.49, batch loss = 0.41 (34.9 examples/sec; 0.229 sec/batch; 21h:10m:10s remains)
INFO - root - 2017-12-14 03:03:50.002101: step 360, loss = 0.58, batch loss = 0.50 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:19s remains)
INFO - root - 2017-12-14 03:03:52.278073: step 370, loss = 0.55, batch loss = 0.46 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:51s remains)
INFO - root - 2017-12-14 03:03:54.541184: step 380, loss = 0.58, batch loss = 0.50 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:58s remains)
INFO - root - 2017-12-14 03:03:56.846258: step 390, loss = 0.64, batch loss = 0.56 (35.8 examples/sec; 0.224 sec/batch; 20h:38m:36s remains)
INFO - root - 2017-12-14 03:03:59.123717: step 400, loss = 0.43, batch loss = 0.35 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:05s remains)
2017-12-14 03:03:59.423892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6251082 -2.5442853 -2.3665657 -2.0823135 -1.8115013 -1.614226 -1.44498 -1.359145 -1.2437956 -1.2351505 -1.2993329 -1.3650053 -1.5132576 -1.680391 -1.8957489][-2.2799034 -2.2501807 -2.1337376 -1.9147936 -1.7267433 -1.5655974 -1.4043536 -1.2986739 -1.207593 -1.1773927 -1.1698421 -1.2637767 -1.4506218 -1.6668087 -1.9145184][-2.1796684 -2.1790903 -2.1028357 -1.8466698 -1.5636921 -1.3976552 -1.2853111 -1.1438375 -0.98923874 -0.97619224 -0.97610211 -1.0212598 -1.1817915 -1.4320433 -1.7580923][-2.0273843 -2.1126547 -2.1130075 -1.8885415 -1.5836241 -1.3264039 -1.1499885 -0.99986672 -0.85039425 -0.79429531 -0.76229835 -0.79545879 -0.90182114 -1.140797 -1.4821287][-1.8142754 -1.9364285 -1.9143685 -1.7362311 -1.465472 -1.2037812 -1.0259818 -0.84014821 -0.67140365 -0.61606336 -0.55870175 -0.64127183 -0.79766583 -1.0707287 -1.4430678][-1.5019715 -1.7015985 -1.6557178 -1.4299726 -1.1237673 -0.85864997 -0.68974924 -0.49403906 -0.36036539 -0.28382826 -0.30031872 -0.48627472 -0.77949548 -1.1357671 -1.5522684][-1.326423 -1.5242848 -1.4903241 -1.2307394 -0.77374578 -0.37545252 -0.14771366 0.078656912 0.21919727 0.19439769 0.03404808 -0.23312092 -0.66526604 -1.1003348 -1.5791365][-1.4726161 -1.5833362 -1.4314421 -1.1661074 -0.71299577 -0.25156951 0.090695143 0.39521646 0.56297469 0.60306835 0.48017788 0.19722342 -0.27651286 -0.85481191 -1.4611368][-1.4530206 -1.5825872 -1.4021108 -1.0678004 -0.61483908 -0.099707365 0.30205989 0.62128592 0.80440545 0.87880015 0.72754765 0.40935969 -0.090361118 -0.69949031 -1.380406][-1.4734699 -1.4681482 -1.2784588 -0.90547252 -0.42253685 0.029314041 0.38165402 0.64060664 0.81064296 0.89785123 0.6834476 0.30838346 -0.25685906 -0.94027948 -1.5858266][-1.8373541 -1.6848553 -1.4117414 -1.0546746 -0.58802867 -0.19263053 0.15694427 0.42068982 0.56246161 0.60476685 0.3147881 -0.052125216 -0.62779784 -1.3271426 -1.9902943][-2.1756766 -2.0289626 -1.7347909 -1.3064712 -0.88897133 -0.56038213 -0.21471977 -0.015188694 0.069623709 0.10785317 -0.18222284 -0.5327692 -1.028073 -1.6356725 -2.2282519][-2.4362466 -2.338326 -2.0995171 -1.7323428 -1.3411556 -1.0539404 -0.77909541 -0.59464765 -0.50681329 -0.41346908 -0.64539266 -0.88735461 -1.2676004 -1.7576499 -2.2361887][-2.3592784 -2.3452075 -2.1578794 -1.8667418 -1.5391507 -1.2888433 -1.1144351 -0.99589562 -0.97758889 -0.9221189 -1.1208365 -1.2957506 -1.5616573 -1.9288601 -2.2985542][-2.3551469 -2.4443619 -2.1814306 -1.9257828 -1.5906481 -1.3183748 -1.222874 -1.1442556 -1.2371218 -1.2663586 -1.4974973 -1.6916088 -1.894907 -2.1674795 -2.4486873]]...]
INFO - root - 2017-12-14 03:04:01.686827: step 410, loss = 0.56, batch loss = 0.47 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:04s remains)
INFO - root - 2017-12-14 03:04:03.939564: step 420, loss = 0.52, batch loss = 0.43 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:52s remains)
INFO - root - 2017-12-14 03:04:06.229458: step 430, loss = 0.73, batch loss = 0.64 (34.4 examples/sec; 0.232 sec/batch; 21h:26m:43s remains)
INFO - root - 2017-12-14 03:04:08.481525: step 440, loss = 0.67, batch loss = 0.58 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-14 03:04:10.732359: step 450, loss = 0.67, batch loss = 0.57 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:52s remains)
INFO - root - 2017-12-14 03:04:12.972604: step 460, loss = 0.59, batch loss = 0.49 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-14 03:04:15.265174: step 470, loss = 0.75, batch loss = 0.65 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:52s remains)
INFO - root - 2017-12-14 03:04:17.539114: step 480, loss = 0.66, batch loss = 0.56 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:51s remains)
INFO - root - 2017-12-14 03:04:19.763053: step 490, loss = 0.54, batch loss = 0.44 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:32s remains)
INFO - root - 2017-12-14 03:04:21.998920: step 500, loss = 0.58, batch loss = 0.47 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:14s remains)
2017-12-14 03:04:22.330621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1333373 -1.922017 -1.8974072 -1.9728345 -2.0063269 -2.1008766 -2.1233606 -2.241379 -2.4085186 -2.8569248 -3.5696471 -4.2996435 -4.8239279 -4.919312 -4.6023941][-2.0886457 -2.286196 -2.228369 -2.280246 -2.3403265 -2.5323052 -2.6488707 -2.7778509 -3.0542042 -3.4517651 -4.010931 -4.6450915 -5.0403185 -5.1219378 -4.8724675][-1.8455809 -2.2541063 -2.4335456 -2.3563795 -2.1234035 -2.2357903 -2.4092357 -2.5084414 -2.664216 -3.1161556 -3.7712858 -4.3572903 -4.7157011 -4.7440376 -4.59103][-1.1364174 -1.5759255 -2.1159372 -2.4138803 -2.4282482 -2.1217427 -1.9008734 -1.9098884 -2.0400488 -2.2357962 -2.7573247 -3.3965073 -3.93075 -4.0825491 -4.0654788][-0.44744396 -0.96201086 -1.4015492 -1.8585393 -2.1243606 -1.8950922 -1.2370613 -0.90663934 -0.96833062 -1.3270667 -1.8484935 -2.4653502 -3.1753259 -3.588655 -3.8287263][-0.0051190853 -0.76732159 -1.3882343 -1.7386943 -1.9807684 -1.8592014 -1.187912 -0.45315146 -0.24918985 -0.51119137 -1.1398666 -1.7697536 -2.4818561 -2.9963508 -3.3512554][-0.22881818 -1.1753708 -1.9197613 -2.131175 -1.9067401 -1.4306486 -0.77154446 -0.065306664 0.31466556 0.03010273 -0.5989995 -1.504252 -2.2067995 -2.7816467 -3.1800005][-0.76673341 -1.492189 -2.1198874 -2.1874387 -1.6576035 -0.79413152 0.040359497 0.59657097 0.89206219 0.51635003 -0.12761617 -1.1038905 -1.9092971 -2.5254307 -3.0156505][-1.5232235 -2.1553884 -2.4013884 -2.2660637 -1.8086665 -0.79212523 0.27787232 0.88255429 1.020195 0.44000459 -0.31248331 -1.1714742 -2.0551291 -2.7220724 -3.149394][-2.0799115 -2.6717672 -2.71749 -2.4232464 -1.9178542 -1.1327556 -0.068880558 0.72184849 0.70063758 0.12890506 -0.91617036 -1.7549491 -2.6572368 -3.3275514 -3.5820324][-2.8344748 -3.4857135 -3.50002 -2.8773324 -2.11812 -1.3999033 -0.66896629 -0.086391449 -0.16983891 -0.8365531 -1.7486049 -2.6529045 -3.3510466 -3.915319 -4.0462923][-2.8941133 -3.5053375 -3.4863298 -2.9116662 -2.1164751 -1.3562133 -0.99058306 -0.90975475 -1.2500544 -1.8016877 -2.5575645 -3.3356967 -3.9718657 -4.212378 -4.3385143][-3.08436 -3.277004 -3.1889043 -2.817255 -2.4829104 -1.9383221 -1.5287504 -1.541371 -1.7403905 -2.1419406 -2.8608966 -3.6285605 -4.3147907 -4.4407377 -4.4709911][-3.1391335 -3.5159302 -3.6574769 -3.4501157 -3.0870042 -2.7526174 -2.3025727 -2.1622856 -2.2292619 -2.544385 -2.9793034 -3.6215479 -4.1798129 -4.4535427 -4.4161921][-3.3096941 -3.8217313 -3.884398 -4.0319204 -3.765841 -3.3879032 -2.9829092 -2.851017 -2.9526622 -3.2008293 -3.4702191 -3.7452 -4.0832791 -4.2933187 -4.4903727]]...]
INFO - root - 2017-12-14 03:04:24.592863: step 510, loss = 0.61, batch loss = 0.51 (34.8 examples/sec; 0.230 sec/batch; 21h:13m:49s remains)
INFO - root - 2017-12-14 03:04:26.839913: step 520, loss = 0.70, batch loss = 0.59 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-14 03:04:29.096525: step 530, loss = 0.51, batch loss = 0.41 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:44s remains)
INFO - root - 2017-12-14 03:04:31.360130: step 540, loss = 0.54, batch loss = 0.43 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:50s remains)
INFO - root - 2017-12-14 03:04:33.631174: step 550, loss = 0.71, batch loss = 0.60 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:02s remains)
INFO - root - 2017-12-14 03:04:35.899741: step 560, loss = 0.59, batch loss = 0.49 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:01s remains)
INFO - root - 2017-12-14 03:04:38.166782: step 570, loss = 0.58, batch loss = 0.47 (34.1 examples/sec; 0.235 sec/batch; 21h:38m:20s remains)
INFO - root - 2017-12-14 03:04:40.417219: step 580, loss = 0.56, batch loss = 0.45 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:18s remains)
INFO - root - 2017-12-14 03:04:42.686168: step 590, loss = 0.63, batch loss = 0.52 (34.5 examples/sec; 0.232 sec/batch; 21h:22m:08s remains)
INFO - root - 2017-12-14 03:04:44.959409: step 600, loss = 0.81, batch loss = 0.68 (35.6 examples/sec; 0.224 sec/batch; 20h:41m:31s remains)
2017-12-14 03:04:45.267814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8361287 -2.4621742 -2.5478752 -2.9633989 -3.5386434 -3.0046046 -2.8440146 -2.3519144 -2.0663104 -2.4699197 -2.3881814 -2.4154696 -2.0764544 -1.872142 -1.5841783][-1.9523346 -1.9961823 -2.1460931 -2.0408452 -1.9615145 -1.769451 -1.9727752 -1.5302299 -0.95817244 -0.75816083 -0.781471 -1.4444977 -1.3753479 -1.1676314 -0.85115206][-2.3789008 -2.1612499 -1.8661184 -1.8037052 -2.1148751 -1.7453388 -1.3931705 -0.78135538 -0.3020277 0.038621664 -0.22757649 -1.2941265 -1.6721923 -1.9077291 -1.4950063][-2.9311421 -2.7533977 -3.0281606 -2.5867774 -2.1440363 -1.7073172 -1.3730863 -0.98166645 -0.40449214 -0.16078019 -0.32697558 -1.2745289 -1.933383 -2.6108661 -2.220546][-2.8099773 -2.4723163 -2.3291533 -2.0649705 -2.2429342 -1.8858987 -1.4074397 -1.6503233 -1.5363002 -1.3405312 -0.82909989 -1.2741275 -1.7894959 -2.3675146 -2.3925774][-3.5152693 -3.2295763 -2.6928532 -2.2626665 -2.003809 -2.2299037 -2.2887545 -2.5457721 -2.0020292 -2.1247754 -1.7648375 -1.8199763 -1.8931966 -2.0060194 -2.0607336][-3.8976948 -3.7384682 -3.1207407 -2.7414873 -1.9920466 -2.0523105 -2.2742307 -2.7529323 -2.6377108 -2.1590862 -1.5771872 -1.6263962 -1.8167697 -2.0675628 -1.7210521][-2.7474644 -2.5266688 -2.3843267 -2.5458543 -2.114548 -2.2284317 -2.0702515 -1.9963942 -1.9464922 -1.8097671 -1.7292961 -1.7313375 -1.8119128 -1.9373989 -2.1097121][-2.5612645 -2.19654 -2.0273271 -2.1374245 -2.0344255 -1.9007311 -1.9360309 -2.1717889 -1.9166085 -1.9235622 -1.8509593 -2.1231692 -2.1852403 -2.3502243 -2.228704][-1.8859129 -2.1363058 -2.3391764 -2.4553852 -2.1693306 -1.8024374 -1.7172337 -1.8873188 -1.6594071 -1.5158629 -1.2357303 -1.5117511 -1.9630556 -2.1776206 -2.1445994][-1.3996804 -1.9046303 -2.3277235 -2.4726417 -2.1296465 -1.1361049 -0.56964493 -0.61428261 -0.94679892 -1.4544692 -1.4287307 -1.7677641 -1.7050707 -1.7349933 -1.8048078][-0.66676068 -0.93085432 -1.1989123 -1.4863515 -1.6566828 -1.339774 -1.2670506 -0.97522008 -0.752578 -0.76817179 -0.83997595 -0.93109906 -1.289095 -1.1474823 -1.115803][-0.78078437 -0.88952589 -0.94083023 -0.57650948 -0.6119833 -0.28442121 -0.27426982 -0.4189837 -0.96704865 -0.622138 -0.48689914 -0.24257421 -0.66488814 -1.0263268 -1.2069322][-2.2366669 -1.4650358 -0.76445794 -0.26209283 -0.18699288 0.11557007 0.69298482 0.61289763 0.24857497 -0.17518067 -0.97018647 -0.73156953 -0.87965488 -0.97632873 -1.1093423][-2.7701557 -1.8966241 -1.2959791 -0.58906722 -0.46284056 -0.47950673 -0.27126741 -0.30041194 -0.20248914 -0.02551651 0.14073896 0.15061736 -0.24427724 -0.26705408 -0.80270767]]...]
INFO - root - 2017-12-14 03:04:47.485422: step 610, loss = 0.72, batch loss = 0.59 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-14 03:04:49.778617: step 620, loss = 0.63, batch loss = 0.49 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:24s remains)
INFO - root - 2017-12-14 03:04:52.046737: step 630, loss = 0.59, batch loss = 0.45 (33.8 examples/sec; 0.237 sec/batch; 21h:48m:22s remains)
INFO - root - 2017-12-14 03:04:54.289589: step 640, loss = 0.71, batch loss = 0.58 (35.0 examples/sec; 0.229 sec/batch; 21h:05m:02s remains)
INFO - root - 2017-12-14 03:04:56.567623: step 650, loss = 0.72, batch loss = 0.58 (34.1 examples/sec; 0.235 sec/batch; 21h:38m:15s remains)
INFO - root - 2017-12-14 03:04:58.796443: step 660, loss = 0.71, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:59s remains)
INFO - root - 2017-12-14 03:05:01.085015: step 670, loss = 0.67, batch loss = 0.53 (33.3 examples/sec; 0.240 sec/batch; 22h:07m:53s remains)
INFO - root - 2017-12-14 03:05:03.338818: step 680, loss = 0.67, batch loss = 0.53 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:24s remains)
INFO - root - 2017-12-14 03:05:05.597488: step 690, loss = 0.69, batch loss = 0.55 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:28s remains)
INFO - root - 2017-12-14 03:05:07.878445: step 700, loss = 0.57, batch loss = 0.43 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:38s remains)
2017-12-14 03:05:08.232339: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.60940576 0.24886727 0.027788877 -0.0673275 0.11268544 0.11688066 0.14586496 -0.056722164 -0.52787113 -0.95833373 -1.5075523 -2.1088839 -2.9415989 -3.284215 -3.5128207][0.65116048 0.34644198 0.17977262 0.042560816 0.14085054 0.11996365 0.018206835 -0.30567861 -0.70247114 -1.1293298 -1.6526589 -2.1983354 -2.6908107 -3.2953014 -3.6516685][0.5609436 0.36799741 0.24425387 0.20839453 0.34123302 0.25941634 0.12968707 -0.17925191 -0.63108325 -1.2263694 -1.7299161 -2.2377765 -3.017271 -3.3800085 -3.4116216][0.47580123 0.34250379 0.30473876 0.35268474 0.56195474 0.69548988 0.75924039 0.5498178 0.16075444 -0.34423161 -1.0615587 -1.5251197 -2.2804487 -2.6036785 -2.744381][-0.032515764 0.051092386 0.1698854 0.25900626 0.46874332 0.63083744 0.71734977 0.58455944 0.19958854 -0.22463393 -0.69317675 -1.2209651 -1.8355836 -1.8607023 -1.7894535][0.029215574 0.12374187 0.32449126 0.56154227 0.77601957 0.95921159 1.1719048 0.997031 0.727648 0.418334 0.02648592 -0.44242978 -0.98489666 -0.98511767 -0.85376394][-0.4121387 -0.035057068 0.26204824 0.47908998 0.79540062 1.0667162 1.2559876 1.1426373 0.96609378 0.89020491 0.69811964 0.37596369 -0.012979746 0.089517117 0.17221165][-0.30162621 -0.11178517 0.26719093 0.55780864 0.84925556 1.2077534 1.4279668 1.4009373 1.232866 1.1765976 1.0229864 0.73510027 0.54838109 0.610379 0.62025809][-0.13197398 0.017329216 0.29679346 0.62477994 0.97628427 1.2514508 1.4477904 1.4132407 1.3216271 1.2994258 1.3529465 1.1693921 1.1036875 1.1246295 0.96014118][-0.027591228 0.33739758 0.64299369 0.73832822 0.74532676 0.87126875 0.90162277 0.75774336 0.695328 0.7340889 0.71884537 0.60375261 0.68160868 0.79475975 0.73389125][-0.036417961 0.34865475 0.73662376 0.77562237 0.61586332 0.50483108 0.29351664 0.21601963 0.20100212 0.27238965 0.2986412 0.10454011 0.19012499 0.33432722 0.32275152][-0.54128504 0.046947956 0.42055368 0.32547617 0.28643703 0.14563298 -0.18837929 -0.34192586 -0.36671138 -0.32852149 -0.299263 -0.42461896 -0.19123435 -0.092395544 0.065668583][-1.0900074 -0.40626073 -0.017612696 0.070115328 -0.018601656 -0.13704681 -0.3496964 -0.62047052 -0.773376 -0.69637978 -0.74604237 -0.78930187 -0.4420929 -0.2151022 -0.22252584][-1.8098419 -1.2066562 -0.76429248 -0.51001811 -0.33142877 -0.40278459 -0.78845966 -1.0765049 -1.0203986 -0.95010829 -0.88271153 -0.87512767 -0.44350457 -0.12796164 -0.085687876][-3.0046556 -2.2503581 -1.743304 -1.4558499 -1.2055414 -1.1202425 -1.3392473 -1.6408677 -1.7050653 -1.7156916 -1.5666182 -1.432138 -0.907346 -0.67189574 -0.56975245]]...]
INFO - root - 2017-12-14 03:05:10.472710: step 710, loss = 0.74, batch loss = 0.59 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:57s remains)
INFO - root - 2017-12-14 03:05:12.741205: step 720, loss = 0.80, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:59s remains)
INFO - root - 2017-12-14 03:05:14.996306: step 730, loss = 0.63, batch loss = 0.46 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:39s remains)
INFO - root - 2017-12-14 03:05:17.257306: step 740, loss = 0.64, batch loss = 0.46 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:49s remains)
INFO - root - 2017-12-14 03:05:19.499437: step 750, loss = 0.65, batch loss = 0.47 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:41s remains)
INFO - root - 2017-12-14 03:05:21.818885: step 760, loss = 0.58, batch loss = 0.40 (34.0 examples/sec; 0.235 sec/batch; 21h:39m:27s remains)
INFO - root - 2017-12-14 03:05:24.066336: step 770, loss = 0.61, batch loss = 0.42 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:42s remains)
INFO - root - 2017-12-14 03:05:26.324025: step 780, loss = 1.18, batch loss = 0.99 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:52s remains)
INFO - root - 2017-12-14 03:05:28.583526: step 790, loss = 0.74, batch loss = 0.55 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-14 03:05:30.830522: step 800, loss = 0.66, batch loss = 0.47 (34.6 examples/sec; 0.231 sec/batch; 21h:19m:10s remains)
2017-12-14 03:05:31.202054: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.52261877 0.45413017 0.37232494 0.33383012 0.271132 0.32431626 -0.036080837 -0.64751756 -1.4547758 -2.071528 -2.5200171 -2.9695427 -3.2338436 -3.3107114 -3.1148767][0.45080233 0.55615807 0.52637482 0.38064909 0.27249789 0.21981955 -0.206038 -0.92661011 -1.7825526 -2.5532508 -3.1702907 -3.4999466 -3.7423418 -4.0808573 -4.1969624][0.69408941 0.62848783 0.73390651 0.55899 0.36990333 0.25117707 -0.20535827 -0.89638567 -1.7264097 -2.5119259 -3.2144415 -3.8024817 -4.0130816 -4.0896335 -4.4164028][0.51753926 0.75287604 0.9051249 0.89871979 0.71991444 0.67581224 0.27207804 -0.36124063 -1.0998733 -1.8713677 -2.6778643 -3.268702 -3.8589258 -4.3164949 -4.531579][0.76801348 0.90132976 0.93388343 0.96782684 0.99888349 1.0865331 0.68327212 0.043349028 -0.71892095 -1.5845256 -2.5817406 -3.3931506 -3.9690692 -4.2803826 -4.6164818][0.71657658 0.87377787 0.6259408 0.61374235 0.81598473 1.0132353 0.89212966 0.12023258 -1.1087058 -2.1118309 -2.6792157 -3.1255403 -3.7986941 -4.0691681 -4.1142435][0.44483232 0.375695 0.47983766 0.49652362 0.36035275 0.57013535 0.65787578 0.001979351 -1.0427732 -2.1765509 -3.1914032 -3.6254358 -3.6109581 -3.7479687 -4.1092606][0.13024926 -0.035375595 0.29144192 0.4105804 0.24715281 0.40283465 0.51921868 -0.069651842 -1.0375657 -2.2435827 -3.2530878 -3.7327657 -4.0177555 -4.0529509 -4.1897111][0.62635016 0.3633163 0.14134836 -0.017304897 0.21383905 0.44667077 0.565799 0.14396644 -0.73460937 -1.9889421 -3.2143767 -4.1311207 -4.3617992 -4.6374607 -4.6942487][0.78193355 0.044353485 -0.25930715 -0.33282542 -0.33612943 -0.073749781 0.35153675 0.044542313 -0.82497346 -2.1600266 -3.2243359 -3.964469 -4.4141388 -4.6078491 -4.7071848][-0.0694437 -0.61380279 -0.99072933 -0.87313831 -0.60566509 -0.4550221 -0.18343902 -0.37046909 -0.750646 -1.5920359 -2.7187405 -3.8995998 -4.1851339 -4.2864828 -4.9014463][-0.52120471 -0.67266977 -0.779021 -0.59466195 -0.74280393 -0.5274725 -0.090344667 -0.028250456 -0.30984402 -1.1488711 -2.1655345 -3.3812416 -4.3192897 -4.66374 -5.0453405][-0.91477644 -0.95567942 -1.1145713 -1.1113054 -0.65060472 -0.25497985 -0.20925713 -0.075956345 -0.2428062 -0.79320872 -1.5548064 -2.9302161 -4.232563 -4.9125738 -5.1962013][-0.82908916 -1.3844602 -1.4308358 -1.4580866 -1.4776118 -1.1042463 -0.30347919 0.013201475 -0.28361988 -0.87096143 -1.7664883 -2.8291421 -3.678936 -4.651042 -4.9650383][-1.081828 -1.6200079 -1.8419967 -1.9527206 -1.9606881 -1.9271569 -1.7786988 -1.5034621 -1.0908607 -1.5527179 -2.512157 -3.2795465 -4.1604891 -4.7974234 -5.1773357]]...]
INFO - root - 2017-12-14 03:05:33.446400: step 810, loss = 0.70, batch loss = 0.51 (35.0 examples/sec; 0.229 sec/batch; 21h:04m:13s remains)
INFO - root - 2017-12-14 03:05:35.699839: step 820, loss = 0.76, batch loss = 0.57 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:15s remains)
INFO - root - 2017-12-14 03:05:37.972557: step 830, loss = 0.81, batch loss = 0.61 (32.7 examples/sec; 0.244 sec/batch; 22h:31m:00s remains)
INFO - root - 2017-12-14 03:05:40.214593: step 840, loss = 0.67, batch loss = 0.47 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:42s remains)
INFO - root - 2017-12-14 03:05:42.431300: step 850, loss = 0.71, batch loss = 0.51 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:50s remains)
INFO - root - 2017-12-14 03:05:44.687048: step 860, loss = 0.79, batch loss = 0.59 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-14 03:05:46.962721: step 870, loss = 0.80, batch loss = 0.59 (36.1 examples/sec; 0.221 sec/batch; 20h:23m:43s remains)
INFO - root - 2017-12-14 03:05:49.237470: step 880, loss = 0.74, batch loss = 0.53 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:09s remains)
INFO - root - 2017-12-14 03:05:51.477948: step 890, loss = 0.78, batch loss = 0.57 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:57s remains)
INFO - root - 2017-12-14 03:05:53.724706: step 900, loss = 0.99, batch loss = 0.77 (34.9 examples/sec; 0.229 sec/batch; 21h:07m:06s remains)
2017-12-14 03:05:54.071909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6719437 -2.7632108 -2.574086 -2.5326765 -2.6062164 -2.3871765 -2.1141617 -2.27233 -2.5669019 -2.7563758 -2.6478374 -2.6914721 -2.3198011 -2.0086689 -1.5887721][-2.8078914 -2.558027 -2.1904681 -2.3568604 -2.6329334 -2.2098248 -1.9972104 -1.8624603 -2.0722547 -2.4311013 -2.6063941 -2.4158444 -2.3426692 -2.1770616 -2.0455823][-2.5051363 -2.4675138 -2.1787422 -2.017359 -1.8419027 -1.5438517 -1.6426693 -1.7508585 -1.8760313 -2.1310019 -2.2279522 -2.0722661 -2.097626 -1.9118307 -1.6707669][-2.1670182 -2.19964 -2.15926 -1.9773954 -1.6000814 -1.2181655 -1.2808717 -1.3256891 -1.2736629 -1.9384985 -2.5197752 -2.6763787 -2.7409506 -2.526957 -2.4150176][-1.0995424 -1.2082433 -1.5118783 -1.3555002 -0.99809146 -0.80343783 -0.97647333 -1.0935327 -1.2321405 -1.734499 -2.3587992 -3.0143161 -3.394335 -3.2092645 -2.9832587][-1.1870809 -1.1388227 -1.1713016 -1.0403798 -0.98659754 -0.731778 -0.7246058 -0.85733461 -0.88456035 -1.5838813 -2.407865 -2.767869 -3.0035093 -3.0524163 -3.1706405][-0.64070117 -0.57498431 -0.58012486 -0.38225031 -0.24560857 -0.13473368 -0.38991642 -0.94046187 -1.4241852 -2.008594 -2.3533394 -2.7700262 -3.0521948 -2.915333 -2.9614298][-0.79462361 -0.94588053 -0.99404 -0.517902 -0.050934076 0.2708416 0.089454412 -0.68276656 -0.97977293 -1.5102261 -2.1206706 -2.4911597 -2.8640337 -2.9876826 -3.0056295][-0.15781832 -0.045769453 -0.0967195 -0.09004426 0.16342664 0.50865006 0.66409588 -0.35736513 -1.2200541 -1.5216503 -1.7174711 -2.1142077 -2.9277465 -2.9121366 -2.892158][-0.15595269 -0.14661527 -0.084281206 -0.10171413 -0.25322437 -0.018587828 0.21328163 -0.61212909 -1.1805842 -2.0271649 -2.5982509 -2.5892317 -2.8802407 -2.9886494 -3.2303021][-1.1996417 -0.840538 -0.41477394 -0.43274772 -0.61612856 -0.42433345 -0.27912831 -0.64432704 -1.0912563 -1.8832142 -2.6163139 -2.7267728 -3.3828592 -3.1837969 -2.8950305][-0.82394886 -0.98328578 -0.80664051 -0.6677227 -0.5055021 -0.35827112 -0.39266849 -0.9615885 -1.2280358 -2.0182505 -2.7174873 -2.8616724 -3.6055255 -3.6642375 -3.8081574][-0.5519284 -0.45080948 -0.40219378 -0.46113563 -0.20367527 -0.094861507 -0.11392236 -0.83127487 -1.3529675 -2.1981175 -2.7681348 -3.0854568 -4.0413413 -3.7658 -3.8115411][-0.17858005 -0.36638331 -0.48416448 -0.16996431 0.0043804646 0.027680874 0.13837171 -0.60767686 -1.1214799 -2.0172944 -2.72914 -2.9083672 -3.6688514 -3.4941053 -3.4535594][-0.74156547 -0.42044687 0.083801746 0.17055106 0.26328182 0.53186655 0.5964098 -0.27744675 -0.83923233 -1.6767607 -2.239116 -2.6145463 -3.6013432 -3.5118432 -3.3237514]]...]
INFO - root - 2017-12-14 03:05:56.278975: step 910, loss = 0.85, batch loss = 0.63 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:13s remains)
INFO - root - 2017-12-14 03:05:58.523090: step 920, loss = 0.77, batch loss = 0.55 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:53s remains)
INFO - root - 2017-12-14 03:06:00.773837: step 930, loss = 0.90, batch loss = 0.68 (35.0 examples/sec; 0.228 sec/batch; 21h:01m:30s remains)
INFO - root - 2017-12-14 03:06:03.017675: step 940, loss = 0.86, batch loss = 0.63 (36.3 examples/sec; 0.221 sec/batch; 20h:19m:24s remains)
INFO - root - 2017-12-14 03:06:05.300533: step 950, loss = 0.81, batch loss = 0.58 (33.1 examples/sec; 0.242 sec/batch; 22h:15m:20s remains)
INFO - root - 2017-12-14 03:06:07.578203: step 960, loss = 0.66, batch loss = 0.42 (33.4 examples/sec; 0.240 sec/batch; 22h:05m:07s remains)
INFO - root - 2017-12-14 03:06:09.885279: step 970, loss = 0.76, batch loss = 0.52 (35.0 examples/sec; 0.229 sec/batch; 21h:03m:44s remains)
INFO - root - 2017-12-14 03:06:12.140672: step 980, loss = 0.76, batch loss = 0.53 (35.5 examples/sec; 0.226 sec/batch; 20h:46m:40s remains)
INFO - root - 2017-12-14 03:06:14.397250: step 990, loss = 0.77, batch loss = 0.53 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:40s remains)
INFO - root - 2017-12-14 03:06:16.663984: step 1000, loss = 0.82, batch loss = 0.58 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:06s remains)
2017-12-14 03:06:17.022990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4978769 -1.353742 -1.1720197 -1.1469731 -1.1719968 -0.87159 -1.0389719 -1.2811955 -1.5010539 -1.5722392 -1.7170284 -1.8943725 -1.8073771 -1.8064771 -1.9841423][-1.3434047 -1.0880514 -0.918586 -0.92870963 -0.96334553 -0.71924591 -0.92610323 -1.1573056 -1.3575187 -1.5090531 -1.7241535 -2.0031395 -2.0366058 -2.1148264 -2.282912][-1.2867835 -0.98508275 -0.75803614 -0.73386478 -0.77298796 -0.61309564 -0.92711937 -1.2340126 -1.494911 -1.7008573 -2.0370317 -2.3749189 -2.4377003 -2.5317674 -2.6745152][-0.68575966 -0.44510794 -0.2351104 -0.18680382 -0.26404858 -0.095512629 -0.38551307 -0.78191376 -1.1319776 -1.41817 -1.9063559 -2.3948135 -2.6050153 -2.6828291 -2.7632358][-0.33076632 -0.093626976 0.12033153 0.20434761 0.18221045 0.36375833 0.066945791 -0.40393341 -0.81781971 -1.1673348 -1.6582973 -2.2299442 -2.5442505 -2.7590528 -2.9491532][-0.025221586 0.15200019 0.32021713 0.37983346 0.36970615 0.5095284 0.27130127 -0.21866226 -0.65369332 -1.0425715 -1.5484941 -2.0482838 -2.3736846 -2.650804 -2.84865][0.37206984 0.57094049 0.7141974 0.77750587 0.77246952 0.86419725 0.64596224 0.12986183 -0.33361053 -0.793349 -1.4956679 -2.1169779 -2.56138 -2.8706245 -3.0812249][0.59016967 0.750051 0.93318129 0.97959614 0.95434594 1.0418723 0.8854537 0.34920669 -0.15693188 -0.5713923 -1.2631232 -2.018117 -2.5654161 -2.8884227 -3.1159086][0.65626216 0.73267293 0.89127779 0.98886132 1.0568035 1.1442883 1.0463586 0.48772907 -0.085228443 -0.56770253 -1.2202368 -1.9557289 -2.5764623 -2.9593482 -3.1553767][0.27543688 0.3483541 0.47342634 0.57714534 0.66694212 0.7728591 0.77777362 0.26714706 -0.27070212 -0.75760937 -1.2687883 -1.8609427 -2.5113542 -2.8976305 -3.2893605][-0.19133043 -0.10531592 0.020449638 0.12214422 0.21158028 0.30887175 0.39444041 -0.01716733 -0.47264719 -0.93692183 -1.464504 -2.0425529 -2.7119105 -3.1051497 -3.4569702][-0.31421673 -0.31055272 -0.24367559 -0.14778161 -0.050132036 0.06789422 0.16673589 -0.22338724 -0.66085052 -1.1071771 -1.6458757 -2.4065993 -3.1394994 -3.5932045 -3.9006512][-0.4871527 -0.59508181 -0.574036 -0.52857733 -0.43549705 -0.30926526 -0.1858871 -0.54655242 -0.95551479 -1.3689116 -1.7654834 -2.4524832 -3.2443376 -3.7252817 -4.0939522][-0.591172 -0.75079191 -0.76639307 -0.72067988 -0.659292 -0.54481363 -0.41508126 -0.79104805 -1.2013984 -1.5820576 -1.94114 -2.5524421 -3.3025172 -3.7626982 -4.1531763][-0.62276185 -0.74140859 -0.74548411 -0.73246109 -0.68836248 -0.61069822 -0.51665223 -0.89189446 -1.2887193 -1.6591522 -2.0857837 -2.7548797 -3.4691296 -3.887157 -4.2139254]]...]
INFO - root - 2017-12-14 03:06:19.286465: step 1010, loss = 1.31, batch loss = 1.07 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-14 03:06:21.563807: step 1020, loss = 0.89, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 21h:16m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - 2017-12-14 03:06:23.847784: step 1030, loss = 0.78, batch loss = 0.54 (34.4 examples/sec; 0.232 sec/batch; 21h:23m:08s remains)
INFO - root - 2017-12-14 03:06:26.092793: step 1040, loss = 0.84, batch loss = 0.60 (32.9 examples/sec; 0.243 sec/batch; 22h:22m:17s remains)
INFO - root - 2017-12-14 03:06:28.334672: step 1050, loss = 0.83, batch loss = 0.59 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:03s remains)
INFO - root - 2017-12-14 03:06:30.593165: step 1060, loss = 0.79, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 21h:19m:33s remains)
INFO - root - 2017-12-14 03:06:32.850369: step 1070, loss = 0.79, batch loss = 0.55 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:03s remains)
INFO - root - 2017-12-14 03:06:35.102293: step 1080, loss = 0.73, batch loss = 0.49 (35.0 examples/sec; 0.229 sec/batch; 21h:02m:37s remains)
INFO - root - 2017-12-14 03:06:37.345628: step 1090, loss = 0.77, batch loss = 0.52 (34.7 examples/sec; 0.231 sec/batch; 21h:13m:11s remains)
INFO - root - 2017-12-14 03:06:39.625094: step 1100, loss = 0.71, batch loss = 0.46 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:34s remains)
2017-12-14 03:06:39.951243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2636995 -1.4282473 -1.4239627 -1.3677511 -1.2624449 -1.4065373 -1.6482711 -1.9752488 -2.2161756 -2.348325 -2.5371168 -2.7788987 -2.8584986 -3.2174642 -3.4792769][-0.89015758 -1.3310225 -1.5973799 -1.5349331 -1.527595 -1.5396749 -1.4964721 -1.9272002 -2.3914733 -2.4929667 -2.6702049 -2.9239125 -2.9824681 -3.3408496 -3.6797173][-0.70964587 -1.1504847 -1.2293502 -1.2624876 -1.3764492 -1.458702 -1.5598636 -1.8059382 -1.9243391 -2.1096873 -2.4701903 -2.7691684 -2.9703755 -3.3528068 -3.7536798][-0.21633637 -0.55680931 -0.7066251 -0.78711689 -0.76072574 -0.86972427 -0.96589661 -1.0507119 -1.2972832 -1.4998033 -1.6342677 -1.8016143 -1.9424495 -2.3698816 -2.7493126][0.029317141 -0.2837826 -0.26385295 -0.39090228 -0.46042967 -0.63498354 -0.70328224 -0.76688814 -0.87473166 -0.90804327 -0.95937371 -0.94766665 -0.98803437 -1.3774694 -1.8845267][-0.53817773 -0.47364283 -0.29487836 -0.22425258 -0.21344042 -0.37472749 -0.42358983 -0.27769768 -0.40178311 -0.50386238 -0.56914926 -0.56804025 -0.61128294 -0.93755221 -1.4213734][-0.20120013 -0.17040908 -0.14216375 -0.22485721 -0.2396574 -0.33836651 -0.16340888 0.051466703 0.1060257 0.0054397583 -0.17645419 -0.043370247 -0.21419013 -0.53398895 -1.0983717][-0.18519497 -0.26280046 -0.26011121 -0.45522654 -0.43934977 -0.54664695 -0.37700963 -0.10588133 0.17436123 0.291615 0.31099916 0.52436018 0.35807657 0.09575367 -0.28424764][-0.71210575 -0.6433171 -0.67698419 -0.74303949 -0.56355274 -0.4554255 -0.19691885 0.29562092 0.59219122 0.84372067 0.91908145 0.97960734 0.86202383 0.63694906 0.40229487][-1.2911284 -1.2221963 -1.2263227 -1.3023235 -1.0934777 -1.0139002 -0.72577775 -0.39710951 -0.15381062 -0.0010237694 0.095710039 0.23742795 0.013927937 -0.17907345 -0.35202515][-2.482269 -2.5488729 -2.6148844 -2.6395798 -2.2251997 -1.93458 -1.4961886 -1.070357 -0.7956084 -0.72661173 -0.66620815 -0.6193347 -0.88370991 -1.1463704 -1.3713722][-3.3440547 -3.2371969 -3.2847159 -3.3635178 -3.2421336 -3.0527439 -2.7093441 -2.2651665 -1.87838 -1.6633689 -1.6115912 -1.89064 -2.1421385 -2.4137948 -2.6926622][-3.9653893 -3.872205 -3.7852383 -3.7796373 -3.5191398 -3.3977027 -3.4619904 -3.3010948 -3.0423124 -2.9576075 -3.1765347 -3.3572097 -3.4936965 -3.7538061 -3.9122362][-3.9149952 -3.9899313 -4.0214262 -3.8446827 -3.4079032 -3.2609832 -3.1976035 -3.0570188 -3.0456722 -3.1945972 -3.3820615 -3.3677466 -3.543819 -3.7622819 -4.0706639][-3.8820772 -3.9584298 -3.9463415 -3.9444149 -3.6618757 -3.4107733 -3.2675333 -3.0939655 -2.9083076 -3.0199137 -3.3460226 -3.5464783 -3.796 -3.8679197 -4.0222049]]...]
INFO - root - 2017-12-14 03:06:42.196473: step 1110, loss = 0.92, batch loss = 0.66 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:56s remains)
INFO - root - 2017-12-14 03:06:44.478970: step 1120, loss = 0.86, batch loss = 0.61 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:37s remains)
INFO - root - 2017-12-14 03:06:46.761196: step 1130, loss = 0.72, batch loss = 0.47 (33.7 examples/sec; 0.237 sec/batch; 21h:50m:45s remains)
INFO - root - 2017-12-14 03:06:49.049261: step 1140, loss = 0.83, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:58s remains)
INFO - root - 2017-12-14 03:06:51.286469: step 1150, loss = 0.78, batch loss = 0.52 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:06s remains)
INFO - root - 2017-12-14 03:06:53.532547: step 1160, loss = 0.73, batch loss = 0.47 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:52s remains)
INFO - root - 2017-12-14 03:06:55.872467: step 1170, loss = 0.89, batch loss = 0.63 (32.0 examples/sec; 0.250 sec/batch; 22h:58m:55s remains)
INFO - root - 2017-12-14 03:06:58.106326: step 1180, loss = 0.80, batch loss = 0.54 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:44s remains)
INFO - root - 2017-12-14 03:07:00.327260: step 1190, loss = 0.80, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:41s remains)
INFO - root - 2017-12-14 03:07:02.612547: step 1200, loss = 0.72, batch loss = 0.46 (34.5 examples/sec; 0.232 sec/batch; 21h:19m:16s remains)
2017-12-14 03:07:02.962256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.564199 -0.86750579 -1.1549308 -1.4277284 -1.6167965 -1.7727997 -1.8409175 -1.8859681 -1.7513158 -1.5469532 -1.3615789 -1.1906489 -1.1205592 -1.2689608 -1.3836788][-0.019920826 -0.42664182 -0.92051852 -1.4233973 -1.7176992 -1.7920078 -1.748243 -1.6612391 -1.4871579 -1.3387964 -1.22786 -1.1276915 -1.1392112 -1.2334888 -1.3419566][0.02060008 -0.069999695 -0.32611561 -0.7659471 -1.2182978 -1.5846214 -1.8125306 -1.8808391 -1.7729582 -1.5389214 -1.3695521 -1.242772 -1.1717229 -1.1732125 -1.2154629][0.027704 0.0055464506 -0.1185348 -0.31609952 -0.45629609 -0.6357888 -0.85528421 -1.1657953 -1.4517112 -1.6123129 -1.7468266 -1.6579062 -1.5273867 -1.3994935 -1.2944317][0.26052046 0.084532261 -0.13860881 -0.25658131 -0.30144358 -0.29058826 -0.27027059 -0.35695112 -0.5257231 -0.720804 -1.0693085 -1.2836728 -1.4148126 -1.4373965 -1.3880024][-0.1853168 0.018198013 0.24459004 0.24279761 0.09717679 -0.030072212 -0.22879338 -0.40876365 -0.57378685 -0.67787063 -0.84303808 -0.83555865 -0.77956903 -0.79512823 -0.82216609][-0.5025183 -0.40183675 -0.21643627 0.004355669 0.36462116 0.73928571 0.91846943 0.6729722 0.2530005 0.0098212957 -0.36441195 -0.5779053 -0.58840966 -0.47871959 -0.42367554][-0.60325813 -0.55080712 -0.41521382 -0.31764185 -0.086163044 0.36259484 0.6970675 0.91132307 0.99935746 1.0245717 0.753597 0.46592188 0.23045707 -0.025349855 -0.15487087][-0.65023553 -0.60477412 -0.51170111 -0.4590354 -0.04613471 0.513922 0.84435272 1.010586 0.9905529 1.0487916 1.0749564 1.0575135 0.97446847 0.86326241 0.71887207][-1.0781128 -1.0196669 -0.9067471 -0.8980056 -0.55409431 0.02721858 0.42540336 0.72595215 0.72321343 0.74819255 0.69022465 0.70505118 0.79099536 0.8078661 0.70779705][-1.3214788 -1.3702677 -1.3181565 -1.2410244 -0.89173126 -0.42722678 -0.13498044 0.07575798 0.17243981 0.39365935 0.45092487 0.4915998 0.65243173 0.78634381 0.85264158][-1.4416423 -1.5072546 -1.52684 -1.5619655 -1.3278203 -1.0026376 -0.83205032 -0.64781249 -0.64524412 -0.44683075 -0.16607368 -0.01759398 0.067564249 0.2054615 0.37939477][-1.6824607 -1.6920466 -1.5512717 -1.436273 -1.2986659 -1.2908697 -1.3638345 -1.4469265 -1.5820067 -1.4966055 -1.2411973 -0.9443506 -0.59767818 -0.39570737 -0.26863158][-2.0142794 -1.9407767 -1.8614684 -1.7862828 -1.6395967 -1.5782603 -1.6963545 -1.8538679 -2.105242 -2.283138 -2.1541455 -1.7295239 -1.2675275 -0.94820225 -0.65638804][-2.1646605 -2.1601942 -2.1841981 -2.2259686 -2.2612469 -2.3433759 -2.4827507 -2.5600798 -2.6776764 -2.7583852 -2.6452091 -2.5918818 -2.4520237 -2.2076859 -1.9192882]]...]
INFO - root - 2017-12-14 03:07:05.226274: step 1210, loss = 0.75, batch loss = 0.49 (35.8 examples/sec; 0.224 sec/batch; 20h:35m:10s remains)
INFO - root - 2017-12-14 03:07:07.499774: step 1220, loss = 0.74, batch loss = 0.48 (34.7 examples/sec; 0.231 sec/batch; 21h:13m:00s remains)
INFO - root - 2017-12-14 03:07:09.775292: step 1230, loss = 0.73, batch loss = 0.47 (36.1 examples/sec; 0.221 sec/batch; 20h:22m:09s remains)
INFO - root - 2017-12-14 03:07:12.080316: step 1240, loss = 0.73, batch loss = 0.46 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:00s remains)
INFO - root - 2017-12-14 03:07:14.324552: step 1250, loss = 0.73, batch loss = 0.46 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:12s remains)
INFO - root - 2017-12-14 03:07:16.561215: step 1260, loss = 0.68, batch loss = 0.41 (36.8 examples/sec; 0.218 sec/batch; 20h:01m:42s remains)
INFO - root - 2017-12-14 03:07:18.831375: step 1270, loss = 0.90, batch loss = 0.63 (35.6 examples/sec; 0.224 sec/batch; 20h:39m:05s remains)
INFO - root - 2017-12-14 03:07:21.099316: step 1280, loss = 0.89, batch loss = 0.62 (34.7 examples/sec; 0.230 sec/batch; 21h:11m:23s remains)
INFO - root - 2017-12-14 03:07:23.352078: step 1290, loss = 1.08, batch loss = 0.80 (34.1 examples/sec; 0.234 sec/batch; 21h:33m:32s remains)
INFO - root - 2017-12-14 03:07:25.635169: step 1300, loss = 0.79, batch loss = 0.52 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:25s remains)
2017-12-14 03:07:25.939902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1110439 -1.1678076 -1.1928866 -1.2148986 -1.2026169 -1.2244252 -1.2119763 -1.1697044 -1.1326313 -1.1502097 -1.2471597 -1.4068837 -1.6240928 -1.837804 -2.1465254][-1.0282029 -1.1077176 -1.2250049 -1.2780521 -1.2641605 -1.252741 -1.2364594 -1.2393024 -1.1971974 -1.1733189 -1.2212749 -1.3953569 -1.5869931 -1.7913264 -2.1726067][-1.1099281 -1.182368 -1.2544671 -1.3545957 -1.4199941 -1.407961 -1.3811088 -1.3830413 -1.3853731 -1.3693681 -1.3411245 -1.4458592 -1.5542049 -1.7465087 -2.0636189][-1.0763139 -1.117013 -1.1569273 -1.204659 -1.2169204 -1.2304451 -1.1898812 -1.0914271 -1.0389239 -1.0095867 -0.98086512 -1.0718884 -1.2309672 -1.5020479 -1.8310107][-1.1210318 -1.0339081 -0.9435668 -0.86372352 -0.77330554 -0.84085238 -0.87595165 -0.9243961 -0.9420194 -0.90057528 -0.9464857 -1.1851945 -1.5082029 -1.7147634 -1.9818379][-0.95015979 -0.89844584 -0.8292855 -0.77940166 -0.70212138 -0.65331173 -0.57214391 -0.61393881 -0.6746062 -0.5686605 -0.48325789 -0.67291594 -1.018708 -1.3094814 -1.7141167][-0.67583823 -0.57785511 -0.46212256 -0.41590965 -0.33247197 -0.28492069 -0.2063185 -0.19655871 -0.20715046 -0.10168171 -0.083832145 -0.23777866 -0.51893616 -0.77716589 -1.2474883][-0.46501482 -0.35111916 -0.18602037 -0.12328768 -0.031234503 -0.027963877 -0.024574757 -0.010267854 0.12911022 0.40167987 0.40305912 0.28474247 0.08371675 -0.1638236 -0.52975428][-0.16079247 -0.17897522 -0.10537004 -0.033487439 0.21183074 0.27722061 0.31616175 0.21295083 0.22764289 0.49076784 0.47283018 0.44827425 0.24711764 -0.022966862 -0.26209629][0.090294361 0.011088133 -0.12922812 -0.24007428 -0.15806627 -0.089414835 0.031454325 0.0047709942 0.086639524 0.21673906 0.13993943 0.21451509 0.18822277 0.022366405 -0.32794142][0.23010743 0.045810461 -0.13520098 -0.31564963 -0.17158473 -0.072532058 0.0077508688 0.014984608 0.16594994 0.2875911 0.079505444 -0.071465969 -0.27044344 -0.468462 -0.768337][0.50176203 0.24324882 -0.015628576 -0.28811145 -0.15210938 -0.088920236 -0.024375319 -0.012540579 0.099116921 0.17784464 -0.050178409 -0.30267274 -0.61409116 -1.0641189 -1.5120308][0.82764781 0.44182241 0.010247707 -0.43379462 -0.32534337 -0.27326965 -0.25974274 -0.24068809 -0.11664486 -0.061563373 -0.30500174 -0.55940533 -0.85803866 -1.235954 -1.6286231][0.70439613 0.31205285 -0.15722835 -0.6482265 -0.63361311 -0.61256433 -0.62804186 -0.62860322 -0.52867818 -0.44748354 -0.65888345 -0.77608812 -0.91995311 -1.1705644 -1.3632493][0.33256233 0.018919826 -0.32160926 -0.74440145 -0.721912 -0.811586 -0.90513951 -0.97500551 -0.96177256 -0.86659908 -1.0184782 -1.0556879 -1.1075646 -1.2389181 -1.3319387]]...]
INFO - root - 2017-12-14 03:07:28.182225: step 1310, loss = 0.84, batch loss = 0.55 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:00s remains)
INFO - root - 2017-12-14 03:07:30.451502: step 1320, loss = 0.83, batch loss = 0.54 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:23s remains)
INFO - root - 2017-12-14 03:07:32.717879: step 1330, loss = 0.87, batch loss = 0.58 (34.3 examples/sec; 0.233 sec/batch; 21h:27m:28s remains)
INFO - root - 2017-12-14 03:07:34.944288: step 1340, loss = 0.84, batch loss = 0.55 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:56s remains)
INFO - root - 2017-12-14 03:07:37.170610: step 1350, loss = 0.81, batch loss = 0.52 (36.1 examples/sec; 0.221 sec/batch; 20h:21m:29s remains)
INFO - root - 2017-12-14 03:07:39.391514: step 1360, loss = 0.82, batch loss = 0.53 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:46s remains)
INFO - root - 2017-12-14 03:07:41.679257: step 1370, loss = 0.70, batch loss = 0.41 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:07s remains)
INFO - root - 2017-12-14 03:07:43.924531: step 1380, loss = 0.84, batch loss = 0.54 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:13s remains)
INFO - root - 2017-12-14 03:07:46.216665: step 1390, loss = 0.87, batch loss = 0.57 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:20s remains)
INFO - root - 2017-12-14 03:07:48.453794: step 1400, loss = 0.79, batch loss = 0.48 (34.1 examples/sec; 0.234 sec/batch; 21h:32m:51s remains)
2017-12-14 03:07:48.748764: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28422666 0.1579783 -0.23481643 -0.42770076 -0.41626835 -0.53011 -0.58203208 -0.8044306 -0.9986496 -1.0726542 -1.1703302 -1.1121551 -1.0533897 -1.0671695 -1.0743052][-0.52849472 -0.42917216 -0.49283195 -0.4782511 -0.37836087 -0.50234413 -0.55556846 -0.66655767 -0.729511 -0.76513851 -0.68250179 -0.46288788 -0.43530357 -0.6080966 -0.92018628][-0.23140264 -0.30272245 -0.38239288 -0.59924293 -0.76363182 -0.8751508 -1.0808563 -1.2280877 -1.3289378 -1.3162889 -1.3100957 -1.19345 -1.068408 -1.1068327 -1.3591957][-0.57074869 -0.7091341 -0.92029876 -1.0126652 -1.1876199 -1.2533934 -1.1964254 -0.99677092 -0.94283134 -0.91757959 -1.0341465 -1.0961239 -1.3015167 -1.4732009 -1.725576][-1.195184 -1.1633093 -1.1740751 -1.0635722 -0.79332042 -0.88289142 -0.81169057 -0.75312829 -0.64911675 -0.62333858 -0.75041926 -0.75408769 -0.93892014 -1.3224802 -1.7163719][-0.84970975 -1.1169984 -1.3379459 -1.3686699 -1.0890352 -0.86926889 -0.61449456 -0.46823275 -0.30773485 -0.35520303 -0.80035424 -0.93499511 -1.0367575 -1.3371811 -1.6838255][-1.0243527 -0.89822561 -0.81093752 -0.71041715 -0.52429032 -0.30215764 -0.027361393 0.38967896 0.758584 0.7334199 0.28472567 -0.36479008 -1.0623753 -1.5331651 -1.925504][-0.94665813 -0.96030396 -0.8614887 -0.72764778 -0.61739075 -0.48194659 -0.32539117 -0.12367487 -0.037624 -0.16249609 -0.54213405 -0.81880355 -1.3061613 -1.80102 -2.1438265][-1.3546844 -1.3140986 -1.1663857 -0.86396849 -0.4195447 0.016462207 0.33426714 0.58429694 0.6691215 0.51466513 -0.0026692152 -0.73931587 -1.3392307 -1.5532429 -1.7715241][-1.0353866 -0.71572983 -0.46515143 -0.16635549 0.22316265 0.49804974 0.61074615 0.62407851 0.61260796 0.51080012 0.13297892 -0.27871561 -0.69504964 -0.91931528 -1.4050841][-1.338746 -1.1640365 -0.98739785 -0.73074079 -0.34219587 0.080988765 0.28426671 0.26402879 0.076310873 -0.23843455 -0.60856009 -0.72898257 -1.0093389 -1.1386522 -1.4224975][-1.4576895 -1.3198631 -1.088567 -0.73178303 -0.389032 -0.028679013 0.21906114 0.46348143 0.56596184 0.34890962 -0.047161937 -0.25536966 -0.67410362 -1.1458082 -1.6412355][-1.2784411 -1.2183528 -1.0887251 -0.76663053 -0.41985083 -0.054679394 0.18296981 0.27302408 0.29959869 0.186795 0.012161374 -0.16806471 -0.49735427 -0.65724683 -1.0796537][-1.385534 -1.3531816 -1.146559 -0.85463846 -0.49466896 -0.045383573 0.2756207 0.35048246 0.23770475 0.036123991 -0.13965213 -0.28620875 -0.70524979 -0.94343281 -1.3755126][-1.1816993 -1.1780541 -0.87456226 -0.64645267 -0.4740833 -0.25425422 -0.11372685 0.0031756163 0.0077605247 -0.057810545 -0.11512673 -0.26361942 -0.54365969 -0.92322457 -1.4495717]]...]
INFO - root - 2017-12-14 03:07:51.035777: step 1410, loss = 0.81, batch loss = 0.50 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:02s remains)
INFO - root - 2017-12-14 03:07:53.269475: step 1420, loss = 0.94, batch loss = 0.63 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:17s remains)
INFO - root - 2017-12-14 03:07:55.541496: step 1430, loss = 0.83, batch loss = 0.52 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:45s remains)
INFO - root - 2017-12-14 03:07:57.813956: step 1440, loss = 0.76, batch loss = 0.45 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:31s remains)
INFO - root - 2017-12-14 03:08:00.088370: step 1450, loss = 0.78, batch loss = 0.47 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:39s remains)
INFO - root - 2017-12-14 03:08:02.337699: step 1460, loss = 0.88, batch loss = 0.56 (35.8 examples/sec; 0.223 sec/batch; 20h:31m:49s remains)
INFO - root - 2017-12-14 03:08:04.591079: step 1470, loss = 1.05, batch loss = 0.72 (34.1 examples/sec; 0.235 sec/batch; 21h:35m:51s remains)
INFO - root - 2017-12-14 03:08:06.845072: step 1480, loss = 0.88, batch loss = 0.55 (34.3 examples/sec; 0.234 sec/batch; 21h:28m:22s remains)
INFO - root - 2017-12-14 03:08:09.130306: step 1490, loss = 0.85, batch loss = 0.52 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:58s remains)
INFO - root - 2017-12-14 03:08:11.417109: step 1500, loss = 0.98, batch loss = 0.64 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:20s remains)
2017-12-14 03:08:11.736393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.87315047 -0.93535274 -0.94038105 -1.0821139 -1.2008984 -1.2993865 -1.3983741 -1.6382581 -1.769888 -1.9035109 -1.9163405 -1.9039397 -1.9352682 -1.7860231 -1.6977799][-0.77371132 -0.83596539 -0.81609511 -0.70616615 -0.8413583 -1.1372355 -1.327603 -1.6475477 -1.8083265 -1.9438498 -2.0603986 -2.0102148 -1.9680389 -1.9904743 -2.0929353][-0.92557222 -0.88367 -0.80494475 -0.9432348 -1.1210238 -1.2529907 -1.2987835 -1.5786183 -1.7894077 -1.8226451 -1.846536 -1.9452943 -2.1733792 -2.3020856 -2.3702281][-0.55295205 -0.719905 -0.861184 -0.8226825 -0.92898214 -1.2351871 -1.4274125 -1.4868293 -1.4163561 -1.6152595 -1.7091501 -1.6725311 -1.8265114 -2.1625345 -2.4989417][-0.21599936 -0.23651361 -0.27986276 -0.4047761 -0.49672472 -0.66215205 -0.7711823 -0.95042181 -1.0578082 -1.2020955 -1.4997901 -1.7760301 -1.9626828 -2.2674909 -2.5417666][0.64136243 0.4781518 0.20861244 0.14963722 0.047351956 -0.23180747 -0.46867132 -0.59170294 -0.50000238 -0.57709622 -0.8017031 -0.99462593 -1.278043 -1.6364427 -2.00531][0.549299 0.60925007 0.45623493 0.23222232 -0.052742004 -0.2483604 -0.32127213 -0.42646325 -0.45579314 -0.43897939 -0.51644397 -0.77384317 -1.1578474 -1.5300595 -1.7534823][0.50641131 0.55685949 0.43234587 0.30962992 0.070221782 -0.078055024 -0.15856266 -0.16892612 -0.078279495 -0.15223229 -0.25482023 -0.60909593 -1.022733 -1.4383705 -1.7735513][0.27513123 0.44478083 0.26008368 -0.099768162 -0.43142748 -0.25129211 -0.24070835 -0.38190866 -0.28074479 -0.040668964 -0.014144421 -0.30333591 -0.63149583 -0.90491694 -1.1773531][-0.55229259 -0.51369309 -0.57780671 -0.76135409 -0.92969882 -1.0645933 -0.9818294 -0.89262313 -0.92627937 -0.85932541 -0.85407662 -1.101465 -1.2544119 -1.2545996 -1.2746134][-1.0387502 -0.96255028 -1.0899686 -1.2858126 -1.3903871 -1.3484597 -1.1738639 -1.1382191 -1.1507555 -0.8896848 -0.69480383 -0.95993841 -1.1543741 -1.2157921 -1.2060683][-1.2541465 -1.2360229 -1.2960311 -1.3974478 -1.6783917 -1.6750946 -1.4485934 -1.4184712 -1.3574288 -1.1346151 -0.99522525 -0.99088645 -0.93086296 -0.91170955 -0.95637208][-0.86258459 -0.85207176 -0.98189485 -1.1710619 -1.2572905 -1.1750951 -1.106809 -1.0784538 -0.9403283 -0.8643024 -0.88596666 -1.0509939 -1.0379742 -1.230001 -1.1717234][-0.95903504 -1.0404097 -0.9936977 -1.0286226 -1.2678385 -1.1555076 -1.0297383 -0.94233817 -0.720261 -0.60686243 -0.54193521 -0.60692108 -0.806479 -0.89275068 -0.99776495][-1.3586903 -1.3500578 -1.4055429 -1.4950327 -1.4945874 -1.2760663 -1.1191072 -0.946682 -0.84491169 -0.74975348 -0.63758326 -0.5084579 -0.61841428 -0.86147428 -1.0405955]]...]
INFO - root - 2017-12-14 03:08:14.000149: step 1510, loss = 0.88, batch loss = 0.53 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:56s remains)
INFO - root - 2017-12-14 03:08:16.229610: step 1520, loss = 1.06, batch loss = 0.72 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:19s remains)
INFO - root - 2017-12-14 03:08:18.478917: step 1530, loss = 0.80, batch loss = 0.46 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:46s remains)
INFO - root - 2017-12-14 03:08:20.745947: step 1540, loss = 0.74, batch loss = 0.40 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:03s remains)
INFO - root - 2017-12-14 03:08:22.981845: step 1550, loss = 0.82, batch loss = 0.48 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:52s remains)
INFO - root - 2017-12-14 03:08:25.235930: step 1560, loss = 0.73, batch loss = 0.39 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:17s remains)
INFO - root - 2017-12-14 03:08:27.494641: step 1570, loss = 0.80, batch loss = 0.45 (34.7 examples/sec; 0.231 sec/batch; 21h:13m:08s remains)
INFO - root - 2017-12-14 03:08:29.788148: step 1580, loss = 0.91, batch loss = 0.56 (35.8 examples/sec; 0.224 sec/batch; 20h:33m:30s remains)
INFO - root - 2017-12-14 03:08:32.004973: step 1590, loss = 0.91, batch loss = 0.56 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:59s remains)
INFO - root - 2017-12-14 03:08:34.296799: step 1600, loss = 0.93, batch loss = 0.58 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:47s remains)
2017-12-14 03:08:34.645814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.20743835 0.051024318 0.6957221 1.1406133 1.3213511 1.455694 1.0437131 0.35447693 -0.45964372 -0.94436681 -0.74404812 -0.93835908 -0.99683118 -1.3055003 -1.6043235][0.31513643 0.35286188 0.56691504 0.88495612 1.1862245 1.1168866 0.49696088 -0.070365667 -0.92907792 -1.6717263 -2.1988187 -2.3230152 -2.425971 -2.733608 -2.7190301][0.9800148 0.87300754 0.51042676 0.35950923 0.0074669123 -0.060532928 -0.060826421 -0.57886887 -1.3261708 -2.1733017 -3.3204334 -3.5598826 -3.770762 -3.9634373 -3.840796][0.27964973 0.44565582 0.53726554 0.70330811 0.64505315 0.59211159 0.25828815 -0.41407192 -1.3070908 -2.1677389 -2.7525079 -3.0561881 -3.6314626 -3.6358628 -3.3177595][0.62277937 0.88888693 1.3455498 1.7724257 1.9708405 1.6054478 1.0758281 0.40806198 -0.22305441 -1.2636247 -2.3343663 -2.5363729 -2.70521 -2.7277818 -2.598465][1.5707397 1.5629849 1.7920654 2.3498509 3.0929191 3.1834576 3.0124238 2.3002684 1.1963794 -0.048400521 -1.1039042 -1.6244806 -1.9213535 -2.0128934 -2.0022976][2.438206 2.3538487 2.4725978 3.2499378 3.9983861 4.24273 4.1077271 3.3308113 1.8718483 0.55943084 -0.53744757 -0.84318459 -1.1021204 -1.2005253 -1.1936539][3.2328484 3.2140529 3.3187582 3.6703236 4.2545424 4.530201 4.654829 4.0335217 3.156111 1.9666786 0.70252395 0.12616694 -0.40363991 -0.62019122 -0.91183579][3.4796731 3.5098517 3.3671663 3.7410033 3.9729245 4.6720381 5.1641293 4.8519115 4.3959532 3.0486186 2.0244479 1.3584483 0.68526173 0.23981142 -0.1300019][1.853857 1.8720243 2.0139639 2.0589697 2.0465591 2.64589 2.996289 3.31406 3.2061241 2.357506 1.6637921 1.2340055 0.59657025 0.11150682 -0.34557891][-0.73073792 -0.69721913 -0.49882805 -0.57252228 -0.47397661 -0.077460527 0.412997 0.7868607 1.063566 0.90463805 0.41790676 -0.20955789 -0.73609257 -0.51415491 -0.55454838][-1.9415946 -2.3324888 -2.5614545 -2.2255948 -1.7222427 -1.0222847 -0.21291268 0.1027379 0.45315766 0.58187723 0.21647906 -0.26093078 -0.81658626 -1.0040296 -1.1296543][-2.2100708 -2.472142 -2.6433992 -2.5995288 -2.3953872 -1.4802139 -0.46912682 0.26641655 0.57494473 0.32030892 -0.29156256 -0.84420586 -1.2796216 -1.4596318 -1.5755694][-2.1032381 -2.4426134 -2.5994892 -2.6364589 -2.360177 -1.7385479 -1.0234883 -0.47241557 0.18590903 0.15878081 -0.16918373 -0.73247564 -1.4973781 -1.3734641 -1.4603567][-1.6891065 -2.0451536 -2.3183839 -2.5676017 -2.4874148 -1.8912406 -1.2314456 -0.77854037 -0.3347193 -0.37797737 -0.2899828 -0.53167951 -1.0813565 -1.5748781 -2.0407567]]...]
INFO - root - 2017-12-14 03:08:36.938206: step 1610, loss = 0.81, batch loss = 0.45 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:16s remains)
INFO - root - 2017-12-14 03:08:39.187473: step 1620, loss = 0.84, batch loss = 0.48 (35.0 examples/sec; 0.228 sec/batch; 20h:59m:43s remains)
INFO - root - 2017-12-14 03:08:41.434217: step 1630, loss = 0.82, batch loss = 0.47 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:57s remains)
INFO - root - 2017-12-14 03:08:43.644939: step 1640, loss = 0.81, batch loss = 0.45 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:58s remains)
INFO - root - 2017-12-14 03:08:45.885914: step 1650, loss = 0.89, batch loss = 0.53 (35.0 examples/sec; 0.228 sec/batch; 20h:59m:27s remains)
INFO - root - 2017-12-14 03:08:48.140253: step 1660, loss = 0.93, batch loss = 0.57 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:50s remains)
INFO - root - 2017-12-14 03:08:50.357792: step 1670, loss = 0.84, batch loss = 0.48 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:50s remains)
INFO - root - 2017-12-14 03:08:52.592278: step 1680, loss = 0.95, batch loss = 0.59 (36.3 examples/sec; 0.221 sec/batch; 20h:15m:51s remains)
INFO - root - 2017-12-14 03:08:54.813965: step 1690, loss = 0.76, batch loss = 0.40 (36.6 examples/sec; 0.219 sec/batch; 20h:04m:42s remains)
INFO - root - 2017-12-14 03:08:57.062053: step 1700, loss = 0.81, batch loss = 0.45 (34.4 examples/sec; 0.232 sec/batch; 21h:21m:14s remains)
2017-12-14 03:08:57.403095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1502442 -0.73950791 -0.4436183 -0.99362284 -1.67698 -2.4468074 -3.0176048 -3.0356832 -3.1990745 -3.2419989 -3.2332687 -2.8387613 -2.29574 -1.898765 -1.6512518][-1.901529 -1.2674855 -0.73500907 -0.36810076 -0.42144251 -1.3486965 -2.2122912 -2.700146 -3.1067612 -3.2850647 -3.2283483 -2.8351374 -2.4765346 -1.9123104 -1.5695076][-1.9087465 -1.8652027 -2.0141265 -1.8996046 -1.4868202 -1.7567292 -2.1653326 -2.8702478 -3.29751 -3.6441741 -3.5980709 -3.2558818 -3.2087507 -2.8071918 -2.3909664][-0.14579511 0.14486182 -0.17239416 -0.7428937 -1.118449 -2.2335587 -2.3510284 -2.2942657 -1.9103394 -2.0713358 -2.2386093 -2.1894219 -2.2421193 -2.2927318 -2.297184][0.72322106 1.0077605 1.0191116 1.1656399 0.82825637 0.088161469 -0.3934195 -0.9048599 -1.0901855 -1.0985706 -1.0449345 -1.1747295 -1.4242138 -1.586893 -1.9048789][0.83244276 0.60653639 0.88340783 1.51879 1.84584 1.7432933 1.4798782 0.42336106 -0.56841588 -0.89526713 -1.1235054 -1.3513036 -1.4370751 -1.757496 -2.0320089][1.3572516 1.3440697 1.167804 1.42648 1.214359 1.2995846 1.7945035 1.9634812 2.2092807 1.45032 0.600029 0.13294864 -0.42603719 -1.0359644 -1.6996439][1.7393723 2.0294919 1.9733908 2.1696804 1.8537977 1.6483009 1.3179679 1.6929169 2.6526382 2.4634144 1.8000002 0.81919551 -0.19802511 -0.92244446 -1.6387728][-0.643608 -0.41851962 0.11339033 0.69897914 1.3790691 1.5237556 0.67332029 0.33927655 0.70925212 0.75909638 0.63698578 -0.27886069 -1.1841743 -1.5193657 -2.0648232][-2.1840415 -1.9022061 -1.6836233 -1.7521731 -1.1812601 -0.12498343 0.43774295 0.38068104 0.28656626 0.12699378 -0.31400323 -1.0767021 -1.6631142 -2.0012541 -1.9486108][-3.0761843 -2.7474809 -2.8183227 -2.9496322 -2.6086664 -1.7975785 -1.1562071 -0.79547465 -0.33570981 -0.37176919 -0.82679385 -1.4320853 -1.9484947 -2.4047928 -2.6054873][-4.8284464 -4.1541834 -3.741632 -3.6361537 -3.4233251 -2.7422874 -2.4801147 -2.1451137 -1.5261185 -1.2500536 -1.5282274 -1.9618418 -2.2493911 -2.6100421 -2.9778221][-5.1254206 -4.5258231 -4.1109266 -3.6038556 -3.2614455 -2.5870206 -2.4171538 -2.3915677 -2.1530509 -2.3329821 -2.4486623 -2.4475985 -2.4886186 -2.6456761 -2.7839241][-4.2651329 -4.3326693 -4.1081533 -3.867918 -3.3949485 -2.5825596 -1.8720243 -1.6767079 -1.8395973 -2.1448944 -2.5222595 -2.7568736 -2.6476889 -2.7341318 -2.8575532][-4.984643 -4.7390523 -4.3162055 -4.2812967 -4.2225924 -3.363827 -2.5456004 -2.3928502 -2.4062452 -2.2887709 -2.3950372 -2.6324971 -2.7140305 -2.692977 -2.7630396]]...]
INFO - root - 2017-12-14 03:08:59.661847: step 1710, loss = 0.81, batch loss = 0.45 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:32s remains)
INFO - root - 2017-12-14 03:09:01.907437: step 1720, loss = 0.86, batch loss = 0.50 (37.1 examples/sec; 0.215 sec/batch; 19h:47m:39s remains)
INFO - root - 2017-12-14 03:09:04.176827: step 1730, loss = 0.82, batch loss = 0.46 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:07s remains)
INFO - root - 2017-12-14 03:09:06.420680: step 1740, loss = 0.90, batch loss = 0.53 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:48s remains)
INFO - root - 2017-12-14 03:09:08.655823: step 1750, loss = 0.81, batch loss = 0.44 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:26s remains)
INFO - root - 2017-12-14 03:09:10.912453: step 1760, loss = 0.81, batch loss = 0.44 (34.3 examples/sec; 0.233 sec/batch; 21h:24m:22s remains)
INFO - root - 2017-12-14 03:09:13.164904: step 1770, loss = 0.81, batch loss = 0.44 (34.7 examples/sec; 0.230 sec/batch; 21h:09m:51s remains)
INFO - root - 2017-12-14 03:09:15.434749: step 1780, loss = 0.79, batch loss = 0.42 (33.5 examples/sec; 0.239 sec/batch; 21h:54m:56s remains)
INFO - root - 2017-12-14 03:09:17.695963: step 1790, loss = 0.83, batch loss = 0.46 (34.6 examples/sec; 0.232 sec/batch; 21h:16m:11s remains)
INFO - root - 2017-12-14 03:09:19.930535: step 1800, loss = 0.87, batch loss = 0.50 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:16s remains)
2017-12-14 03:09:20.300401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9734507 -2.4677753 -1.7768673 -1.299821 -1.2733331 -1.7099228 -2.1603806 -2.652611 -2.9175515 -3.4048495 -3.4471228 -3.3371575 -3.526154 -3.6408784 -3.4444876][-1.8222187 -1.2457023 -0.811013 -0.37688589 -0.41303265 -0.88611794 -1.5321448 -2.4432151 -3.0526252 -3.4436266 -3.455744 -3.4601984 -3.353147 -3.6954789 -4.1415558][-1.1290329 -0.88136148 -0.24085379 0.23861706 0.10221207 -0.73933709 -1.7012258 -2.5031843 -2.9958653 -3.6227169 -3.8729768 -3.9626293 -4.2617464 -4.6076174 -4.6583447][-1.167017 -0.73180741 -0.10335326 0.0017925501 -0.20521772 -0.62613153 -1.0082216 -1.8708962 -2.8688772 -3.3060963 -3.3022754 -3.7972429 -4.4522371 -4.634872 -4.6710625][-1.0224192 -0.76147538 -0.51054728 0.10628355 0.47428489 0.0838964 -0.34806716 -0.60761511 -0.831498 -1.1969534 -1.7775573 -1.7696511 -1.9178225 -2.4595237 -2.8658636][-0.44356358 0.052566528 0.74686241 0.9500351 1.4676812 1.2955112 1.21018 0.9954834 0.79813194 0.60062528 0.24670243 0.0052360296 -0.5514878 -0.69055784 -0.46791077][0.071930885 0.32633138 0.81692696 1.393791 2.1482279 1.9083593 1.7070825 1.3913181 1.123431 0.89031577 0.48102045 0.31053567 -0.18398356 -0.62529945 -0.76905572][-0.088787794 -0.38394046 -0.26131117 0.17233372 0.71344614 0.61674404 0.86837244 0.98361444 0.95220447 0.8385272 0.807534 0.75874543 0.14009094 -0.24377215 -0.32348716][-0.59885371 -0.29172993 -0.10422111 -0.038940787 0.17068744 -0.19246089 0.093375683 0.30262184 0.53361273 0.53168392 0.33971381 0.37074232 -0.25983047 -0.7951265 -1.0820003][-0.2837621 -0.40758038 -0.42029798 -0.18592465 -0.26556492 -1.1634821 -0.89158612 -0.63449013 -0.85219914 -1.1906221 -1.5416927 -1.7173928 -2.3765283 -2.7128298 -2.9867487][-0.37311447 -0.42996502 -0.43550038 -0.70901978 -0.868027 -1.2779282 -1.2412124 -1.7101004 -2.4318488 -2.8436499 -3.0542386 -3.0450006 -3.6256113 -3.7911174 -3.6853714][0.17444801 0.23724914 0.19334507 0.087374091 -0.14089859 -0.94861907 -1.1559824 -1.3248835 -2.0578804 -2.6990619 -2.7884607 -3.2336659 -4.0837755 -4.0879469 -4.1586113][-0.044975042 0.010981441 0.087806344 0.02013433 -0.033346534 -0.29182971 -0.41772032 -1.0592632 -1.7486619 -1.7192577 -1.6872116 -2.1867435 -2.902061 -3.3599706 -3.6223321][0.58770752 -0.06403017 -0.5823431 -0.68047106 -0.48212588 -0.60942006 -0.64139545 -0.77907729 -0.89898515 -1.0946335 -1.3825254 -1.4987344 -1.8440819 -1.7584342 -1.932927][0.65325713 0.48051286 0.0774709 -0.41406894 -0.43137741 -0.54682338 -0.38171637 -0.39296234 -0.39828181 -0.19066882 -0.40071261 -0.833122 -1.2210492 -1.2680018 -1.255445]]...]
INFO - root - 2017-12-14 03:09:22.573862: step 1810, loss = 0.76, batch loss = 0.39 (34.1 examples/sec; 0.235 sec/batch; 21h:33m:56s remains)
INFO - root - 2017-12-14 03:09:24.829382: step 1820, loss = 0.85, batch loss = 0.48 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:32s remains)
INFO - root - 2017-12-14 03:09:27.103499: step 1830, loss = 0.82, batch loss = 0.45 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-14 03:09:29.361694: step 1840, loss = 0.78, batch loss = 0.41 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:39s remains)
INFO - root - 2017-12-14 03:09:31.596264: step 1850, loss = 0.83, batch loss = 0.45 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:27s remains)
INFO - root - 2017-12-14 03:09:33.829791: step 1860, loss = 0.86, batch loss = 0.49 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:45s remains)
INFO - root - 2017-12-14 03:09:36.062702: step 1870, loss = 0.75, batch loss = 0.38 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:04s remains)
INFO - root - 2017-12-14 03:09:38.305937: step 1880, loss = 0.85, batch loss = 0.47 (36.3 examples/sec; 0.220 sec/batch; 20h:14m:01s remains)
INFO - root - 2017-12-14 03:09:40.565271: step 1890, loss = 0.86, batch loss = 0.49 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-14 03:09:42.826485: step 1900, loss = 0.80, batch loss = 0.42 (34.9 examples/sec; 0.229 sec/batch; 21h:01m:46s remains)
2017-12-14 03:09:43.189987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.62521 -4.370677 -4.018167 -3.8734348 -4.0802221 -4.060214 -3.8582289 -3.8856273 -4.2376127 -4.8242407 -4.6748276 -3.6861234 -2.7010217 -1.6606129 -0.85417861][-5.4580097 -4.993782 -4.314292 -3.9656672 -3.655179 -3.805861 -3.9809084 -3.814805 -4.1650486 -4.5946283 -4.37922 -3.5291171 -2.7007289 -1.92266 -1.2480345][-5.0747862 -4.6113191 -4.0281324 -3.5147605 -3.0573907 -3.0875695 -3.0071306 -3.3333225 -4.2954006 -4.7192049 -4.8131723 -4.1111488 -3.8988185 -3.3890266 -2.6634071][-3.8017671 -3.3964891 -2.8772926 -2.0306625 -1.2448884 -0.83035076 -0.4770211 -0.61321533 -1.4583189 -3.1162891 -4.0952997 -3.7757525 -3.4273429 -3.3417373 -3.3946857][-0.71596515 -0.0208323 0.65101385 1.3596616 2.047935 2.3319373 2.6539769 2.6364541 1.7695649 0.051199675 -1.7151793 -2.6114676 -3.2142165 -3.2260256 -3.3212471][2.0900881 2.5250053 3.2415419 4.1796303 4.956717 5.0017295 4.6486015 4.2448144 3.2817931 1.3844242 -0.8715151 -1.8406838 -2.7156925 -3.3924451 -3.5914893][2.7183566 3.4254093 3.968792 4.7344809 5.6250987 5.7376251 5.6414962 4.8820848 3.5634708 1.8597081 0.40407491 -0.39895308 -1.2863 -1.917557 -2.0930467][3.5098424 4.2312112 5.0448866 5.6782093 5.815866 5.4649482 5.1117039 4.9426184 4.5705295 3.2579379 1.7934158 0.825907 0.12210631 -0.49068713 -1.0013914][3.7451191 3.8642159 4.2106714 4.9360595 5.4258418 5.4816532 4.744616 4.3495107 3.8248506 3.0366993 2.0919783 1.2609799 0.67406058 0.12901294 -0.16337895][0.80981135 1.3866057 2.0917099 2.091691 2.0740476 1.6048949 1.2536819 1.1723976 0.67181277 0.5331049 0.39295912 0.33271742 -0.2074368 -0.58191967 -0.62213969][-4.1061659 -3.6032033 -3.3328609 -3.1998892 -3.4017792 -3.3632512 -2.7478802 -2.4810348 -2.1513872 -1.8634069 -2.0776792 -2.280947 -2.5976458 -2.7442868 -2.7976241][-7.2494197 -7.1902604 -7.2024817 -7.0244732 -6.541307 -6.2794757 -5.8515306 -5.3393879 -5.149004 -5.0823731 -4.9090643 -4.8665981 -5.4012871 -5.4616432 -4.738369][-8.0634212 -8.1285191 -8.3406143 -8.5739346 -8.7229595 -8.3067837 -7.9091997 -7.8171639 -7.8710122 -7.8372126 -7.8747683 -7.3334751 -6.9640965 -6.6755443 -6.5306053][-7.5648785 -7.7904263 -8.270463 -8.4153175 -8.4255419 -8.4984875 -8.448741 -8.1462688 -7.8690629 -7.697618 -7.5441151 -7.2825718 -7.3149438 -6.8112144 -6.0363617][-8.4768915 -8.8164692 -9.0869427 -9.4042 -9.4749479 -9.2533493 -9.0196714 -8.966177 -8.6622009 -8.2400122 -7.4144883 -6.6982646 -6.293961 -6.1555729 -5.9235191]]...]
INFO - root - 2017-12-14 03:09:45.435928: step 1910, loss = 0.87, batch loss = 0.49 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:36s remains)
INFO - root - 2017-12-14 03:09:47.669622: step 1920, loss = 0.74, batch loss = 0.36 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:07s remains)
INFO - root - 2017-12-14 03:09:49.907091: step 1930, loss = 0.87, batch loss = 0.50 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-14 03:09:52.160447: step 1940, loss = 0.79, batch loss = 0.42 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:29s remains)
INFO - root - 2017-12-14 03:09:54.403235: step 1950, loss = 0.77, batch loss = 0.39 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:18s remains)
INFO - root - 2017-12-14 03:09:56.686333: step 1960, loss = 0.80, batch loss = 0.41 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:25s remains)
INFO - root - 2017-12-14 03:09:58.932979: step 1970, loss = 1.27, batch loss = 0.89 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:21s remains)
INFO - root - 2017-12-14 03:10:01.186781: step 1980, loss = 0.87, batch loss = 0.48 (34.6 examples/sec; 0.231 sec/batch; 21h:12m:21s remains)
INFO - root - 2017-12-14 03:10:03.445670: step 1990, loss = 0.80, batch loss = 0.41 (35.2 examples/sec; 0.228 sec/batch; 20h:53m:34s remains)
INFO - root - 2017-12-14 03:10:05.683877: step 2000, loss = 0.86, batch loss = 0.47 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:44s remains)
2017-12-14 03:10:06.012830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8239093 -2.5410531 -2.3475566 -2.4158545 -2.4360244 -2.0007658 -1.3408897 -1.4013727 -1.9341351 -2.2891309 -2.7282267 -2.4979286 -2.2508137 -2.3490705 -2.3178928][-2.2993453 -2.4583263 -2.3771026 -2.1553998 -1.9902495 -1.972059 -1.8404069 -1.4875935 -1.342347 -1.6272115 -2.0774438 -2.1194234 -2.1526573 -1.8450392 -1.3271763][-2.2904887 -2.3005223 -2.2304778 -2.3187747 -2.3234277 -1.9957237 -1.5723031 -1.4632685 -1.5940812 -1.5655457 -1.5983903 -1.7139633 -1.7831228 -1.6438893 -1.6326642][-2.337492 -2.3749661 -2.3461015 -2.2992568 -2.284811 -2.1286268 -1.6820239 -1.4262588 -1.4066868 -1.3388233 -0.8748036 -0.57682633 -0.51851141 -0.65726137 -0.77481937][-1.3718596 -1.2632115 -1.1542156 -1.0567851 -1.0870206 -0.5832417 -0.40860558 0.31300008 0.45258057 0.12540269 -0.047889471 0.010008335 0.21845257 0.49297845 0.51557148][-1.7850844 -1.5986706 -1.3460822 -1.2708404 -1.2487707 -0.76504219 0.30803311 1.6950856 2.0272565 2.4229012 2.1039124 2.2626619 2.3102722 2.2665019 1.9080974][-0.76410544 -0.70683229 -0.77715266 -0.52124512 -0.29478669 0.023546457 0.733518 1.4661754 2.104445 2.6361084 3.5523305 4.355051 4.312952 4.2229562 3.5251427][-0.90704978 -0.44267988 -0.11153257 0.14716733 0.24563348 0.55797327 1.5703975 2.6691318 3.7426963 4.3963437 4.7931914 4.6267371 4.6027322 4.516449 4.1646428][1.4848181 1.3062843 1.1385704 1.293281 1.4512066 1.6441334 2.1831756 3.2272449 4.2109861 4.1276689 3.7125854 3.3130593 3.2595029 3.1233106 2.6735668][-0.19383883 -0.19102311 -0.14517415 -0.31360173 -0.25831342 -0.22848642 -0.20606768 0.40005338 1.1184069 1.7803084 1.9516805 1.5939449 1.3912927 1.2301422 0.80632079][-1.5949059 -2.048636 -2.5380571 -2.6133318 -2.7527587 -2.2397907 -1.1293848 -0.30379438 -0.36500084 -0.45064151 -0.61575973 -0.61686993 -0.54208767 -0.52846968 -0.56016529][-3.0086727 -3.2786317 -3.424109 -3.6790118 -4.0272107 -3.6440413 -2.8239827 -2.294281 -2.1533058 -1.8631978 -1.9752424 -2.4334958 -2.7804394 -2.7339189 -2.6920409][-2.3984098 -3.0979457 -3.6662955 -3.4952393 -3.341589 -3.5746045 -3.8152061 -3.4137228 -3.1169417 -2.9287829 -2.7924151 -2.5426133 -2.4475837 -2.7389066 -2.9707661][-3.4612131 -3.7274866 -3.4502118 -3.4462471 -3.6027427 -2.9899597 -2.4404664 -2.7981086 -3.2433224 -2.95923 -2.9085584 -2.9702821 -2.8290985 -2.4189003 -2.433352][-3.5923967 -3.6474628 -3.341433 -3.2077894 -3.1984754 -3.2029314 -2.7536488 -2.2041404 -1.7700413 -2.2476487 -2.6694422 -2.3123944 -2.2534165 -2.1452098 -2.0699384]]...]
INFO - root - 2017-12-14 03:10:08.282597: step 2010, loss = 0.74, batch loss = 0.35 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:55s remains)
INFO - root - 2017-12-14 03:10:10.570010: step 2020, loss = 0.80, batch loss = 0.41 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:44s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - 2017-12-14 03:10:12.822198: step 2030, loss = 0.84, batch loss = 0.45 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:37s remains)
INFO - root - 2017-12-14 03:10:15.074949: step 2040, loss = 0.70, batch loss = 0.32 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:24s remains)
INFO - root - 2017-12-14 03:10:17.313748: step 2050, loss = 0.76, batch loss = 0.37 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:33s remains)
INFO - root - 2017-12-14 03:10:19.573038: step 2060, loss = 0.76, batch loss = 0.37 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:32s remains)
INFO - root - 2017-12-14 03:10:21.811260: step 2070, loss = 0.76, batch loss = 0.37 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:38s remains)
INFO - root - 2017-12-14 03:10:24.081121: step 2080, loss = 0.71, batch loss = 0.32 (34.6 examples/sec; 0.231 sec/batch; 21h:14m:32s remains)
INFO - root - 2017-12-14 03:10:26.327816: step 2090, loss = 0.70, batch loss = 0.32 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:21s remains)
INFO - root - 2017-12-14 03:10:28.584698: step 2100, loss = 0.82, batch loss = 0.43 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:32s remains)
2017-12-14 03:10:28.961391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2438653 -0.83590215 -0.90391254 -1.028966 -1.3338492 -2.151485 -3.1533487 -4.2629962 -4.9866047 -4.2299881 -3.4010053 -2.5285408 -1.7819135 -1.2726306 -1.2855338][-1.2509298 -1.0885353 -1.362093 -1.4753897 -1.5765415 -2.1055768 -3.1505313 -4.5904484 -5.2585545 -4.6549368 -3.4747293 -2.5675428 -1.9093261 -1.5752728 -1.6506979][-0.79299748 -1.170166 -1.3854804 -1.4275743 -1.6416253 -2.0550032 -2.6509027 -3.906189 -4.7905884 -5.0551882 -3.8851397 -2.8649888 -1.9587812 -1.8335583 -2.169404][-1.0088081 -1.1581662 -1.1233279 -1.4935746 -1.5264727 -2.1867523 -2.6360104 -3.1811898 -3.752634 -3.7922864 -3.3404312 -2.8779655 -1.9432268 -1.9185606 -2.1628284][-0.59983492 -0.60778463 -0.70735669 -0.89367414 -0.98538303 -1.5064564 -2.2140176 -3.068119 -3.5773616 -3.7167554 -3.137157 -2.7618985 -2.3814721 -2.4389629 -2.8083992][-0.04693532 -0.41566348 -0.59379721 -0.23234367 -0.17067587 -0.41342938 -0.326913 -0.63046956 -1.29449 -1.5882287 -1.588883 -1.8639925 -1.9430647 -2.0984066 -2.3332262][0.26986742 0.26858139 0.17495608 0.17699552 0.28663492 0.20002055 0.02824223 0.0614388 -0.091198206 0.062600136 -0.32224095 -0.5458045 -0.96550059 -1.8205162 -2.5810184][0.68549871 0.74701786 0.58117533 0.82608771 0.65993047 0.94500566 0.88650894 0.38144469 -0.40523493 -0.071133018 -0.0391258 -0.52871263 -0.75013494 -1.2928326 -1.6833837][0.40132689 0.60231233 0.87851739 0.98186564 1.2360287 1.5096738 2.2548113 1.9065497 1.4397531 0.73640561 0.12795055 0.36677313 0.39431834 -0.45572209 -0.78582764][0.790267 0.54078269 0.65912437 0.9995234 1.0529461 1.5095644 1.8430862 1.3752096 0.84359694 0.37470651 -0.22229385 -0.71206009 -1.264807 -1.6894903 -2.4044774][-0.4000411 0.018212795 -0.055635571 -0.48257029 -0.547606 0.18270969 0.77399039 0.834929 0.10465848 -0.41414082 -0.54800069 -0.39199972 -0.58821595 -1.5006032 -2.31486][-1.5794005 -1.0666482 -0.59391117 -0.98505574 -1.3424935 -1.7990017 -1.116197 -0.99618977 -0.33347595 -1.3821299 -1.3175721 -1.1243043 -1.4108794 -1.6798327 -2.0655322][-2.4634025 -1.9723594 -1.3855004 -0.89121395 -0.77026308 -0.595021 -0.92175221 -1.7276359 -2.0831358 -2.1380522 -1.8745002 -1.9862542 -2.3806272 -2.8795307 -3.1479716][-3.4853053 -2.7649422 -2.2519155 -1.6041812 -1.2179263 -0.51784885 -0.72701025 -0.73088276 -1.1038094 -1.7403769 -1.7369252 -1.5975529 -1.6979433 -2.6195323 -3.6917512][-3.8632612 -3.40215 -2.6188316 -2.0284915 -1.0792069 -0.81286955 -0.95878184 -1.2742186 -1.3285371 -1.5947871 -1.3359321 -1.2538812 -1.618053 -1.9804302 -2.8043389]]...]
INFO - root - 2017-12-14 03:10:31.211948: step 2110, loss = 0.78, batch loss = 0.40 (35.8 examples/sec; 0.224 sec/batch; 20h:31m:09s remains)
INFO - root - 2017-12-14 03:10:33.460266: step 2120, loss = 0.75, batch loss = 0.37 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:21s remains)
INFO - root - 2017-12-14 03:10:35.681402: step 2130, loss = 0.82, batch loss = 0.43 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:50s remains)
INFO - root - 2017-12-14 03:10:37.932806: step 2140, loss = 1.15, batch loss = 0.76 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:35s remains)
INFO - root - 2017-12-14 03:10:40.177172: step 2150, loss = 0.80, batch loss = 0.41 (35.3 examples/sec; 0.226 sec/batch; 20h:46m:13s remains)
INFO - root - 2017-12-14 03:10:42.421606: step 2160, loss = 0.70, batch loss = 0.31 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:27s remains)
INFO - root - 2017-12-14 03:10:44.662167: step 2170, loss = 0.73, batch loss = 0.35 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:39s remains)
INFO - root - 2017-12-14 03:10:46.920501: step 2180, loss = 0.72, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:54s remains)
INFO - root - 2017-12-14 03:10:49.133419: step 2190, loss = 0.80, batch loss = 0.41 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-14 03:10:51.367145: step 2200, loss = 0.79, batch loss = 0.40 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:47s remains)
2017-12-14 03:10:51.679771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3154345 -2.779923 -2.7355456 -2.3883724 -2.1645482 -2.0089881 -1.9718366 -2.2351143 -2.2277532 -2.3103514 -2.7004695 -2.2054913 -2.072089 -2.24877 -2.2874775][-2.8411398 -3.0224891 -2.7645607 -2.9771113 -2.7938418 -2.2293341 -1.6814995 -1.4634345 -1.5885227 -2.1490905 -2.87814 -2.8034465 -2.8227699 -2.5391145 -2.3417814][-2.7710419 -2.8797708 -2.8066428 -2.5475242 -2.2254205 -2.5346692 -1.9898149 -1.455498 -1.34758 -1.8373682 -2.5300739 -2.7144794 -2.6270657 -2.6872907 -2.490468][-1.833849 -2.4124231 -2.5491838 -2.4480391 -2.5698881 -2.3961244 -1.6700637 -1.019052 -0.85804915 -1.0424021 -1.5587449 -2.062036 -2.2172637 -2.3979084 -2.3564847][-1.2119253 -1.5057582 -1.4584954 -1.6267112 -1.9898219 -2.0335553 -1.3312852 -0.59491134 -0.39676344 -0.69160116 -1.5755008 -2.0107532 -2.1275685 -2.3338351 -2.3512132][-1.5098836 -1.5285327 -1.6442565 -1.8337799 -1.8058972 -1.695649 -0.97131163 -0.2721566 -0.17462409 -0.67099226 -1.4754469 -1.9809898 -2.5357556 -2.6015472 -2.5370133][-1.780508 -1.885289 -1.9698293 -2.128449 -2.5289209 -2.3010886 -1.465634 -0.25254691 0.34752691 0.041088939 -0.56942022 -1.3168361 -2.0109935 -2.337527 -2.435823][-0.7571398 -0.81254792 -0.921087 -1.0825086 -1.1948185 -0.86980921 -0.09950614 0.91539204 1.1812192 0.940415 -0.32725024 -1.2017169 -1.762784 -1.9869027 -2.1677463][0.0048705339 -0.059933305 -0.086932182 -0.28634107 -0.36440051 -0.024827003 0.6097461 1.1189262 1.395291 1.1016263 -0.26642489 -1.0678204 -1.6646137 -2.0098963 -2.0489516][-0.88508075 -0.85902989 -1.0043678 -1.2200683 -1.3417902 -0.93785727 -0.38946486 0.32046831 0.66269433 0.47087038 -0.44697809 -0.83701551 -1.1106855 -1.2239581 -1.4836253][-1.348491 -1.2879239 -1.3012216 -1.3682531 -1.2532792 -0.85948783 -0.23908007 0.22562826 0.29277337 -0.035326719 -0.89916193 -1.5506532 -1.8503184 -1.6128762 -1.4475082][-0.26811743 -0.46265018 -0.65331912 -0.771392 -0.68565774 -0.2059617 0.40487993 0.872826 0.53994524 -0.11749125 -1.4583094 -2.0160296 -1.83214 -2.0737286 -2.2326925][0.16701663 -0.15529335 -0.32644868 -0.51079595 -0.45568264 -0.020218134 0.39879644 0.59821713 0.47607028 0.047717214 -1.2003683 -1.7849045 -2.3642917 -2.233954 -2.245914][-0.30963826 -0.33715582 -0.22328043 -0.22580302 0.012393236 0.5935353 0.79632318 0.76986182 0.51649606 -0.081672668 -1.0097127 -1.6379197 -1.9530582 -2.5101209 -2.9068246][0.35991848 0.0053601265 -0.21757114 -0.33886266 -0.10540628 0.84306872 1.3338872 1.2252935 0.71151412 -0.0067383051 -0.87519151 -1.4242291 -1.91672 -2.2239625 -2.535166]]...]
INFO - root - 2017-12-14 03:10:53.930145: step 2210, loss = 0.72, batch loss = 0.34 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:26s remains)
INFO - root - 2017-12-14 03:10:56.215786: step 2220, loss = 0.72, batch loss = 0.33 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:50s remains)
INFO - root - 2017-12-14 03:10:58.438742: step 2230, loss = 0.71, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-14 03:11:00.693521: step 2240, loss = 0.71, batch loss = 0.32 (33.5 examples/sec; 0.239 sec/batch; 21h:53m:18s remains)
INFO - root - 2017-12-14 03:11:03.140685: step 2250, loss = 0.65, batch loss = 0.26 (32.8 examples/sec; 0.244 sec/batch; 22h:22m:31s remains)
INFO - root - 2017-12-14 03:11:05.375414: step 2260, loss = 0.75, batch loss = 0.36 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:22s remains)
INFO - root - 2017-12-14 03:11:07.602458: step 2270, loss = 1.08, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:35s remains)
INFO - root - 2017-12-14 03:11:09.876037: step 2280, loss = 0.86, batch loss = 0.47 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:20s remains)
INFO - root - 2017-12-14 03:11:12.137033: step 2290, loss = 0.69, batch loss = 0.31 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:00s remains)
INFO - root - 2017-12-14 03:11:14.397674: step 2300, loss = 0.72, batch loss = 0.33 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:52s remains)
2017-12-14 03:11:14.742536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9577656 -5.1124921 -4.7091265 -4.6112781 -4.1813488 -3.1821134 -1.6430125 -0.98763013 -1.0433466 -1.2179186 -2.3845842 -3.75171 -4.6905918 -4.7302923 -4.5941873][-4.537128 -4.3589692 -4.2389255 -3.9811335 -3.6757364 -2.8421962 -1.7040709 -1.0518785 -1.3531344 -1.8822732 -2.6003633 -3.5344527 -4.1453233 -4.2335982 -4.1863785][-4.03314 -4.2740483 -4.4707651 -3.9646158 -3.7531495 -3.2682557 -2.1216094 -1.5731413 -1.9464524 -2.3256385 -3.1217675 -3.7354021 -3.7748561 -3.5742996 -3.4067206][-4.6271162 -4.328578 -4.1131244 -4.0902262 -3.8168116 -3.3119192 -2.3576202 -1.1569285 -1.5036125 -1.9510643 -2.3764477 -2.8503103 -3.2454743 -2.705564 -1.7976362][-4.0173206 -3.7199557 -3.449038 -3.3711951 -3.47223 -3.1617656 -2.2114248 -1.6533687 -2.0148807 -1.7591367 -2.2771156 -3.3125081 -3.7526565 -3.5561812 -2.532187][-2.7411847 -2.7172236 -2.7134197 -2.1696403 -2.1094182 -2.0624185 -1.2024775 -0.12129664 -0.18419731 -0.69431543 -1.2869 -2.2107222 -2.7886326 -3.4039235 -2.8461204][-2.2596595 -1.9575404 -1.5488529 -1.4334258 -1.508533 -0.895112 0.20436919 1.1103495 1.329986 0.80987227 -0.16009057 -1.1834035 -1.7190526 -2.2239423 -1.9227444][-1.2246257 -0.60894823 -0.11559463 -0.087293744 -0.50703239 0.024160028 0.86220491 2.0036297 2.0853214 1.0647851 0.062041998 -0.55358243 -1.2453806 -2.0229032 -1.9055578][-0.47114861 0.17495763 0.57479012 0.75206244 0.75051153 0.89768684 0.86623967 1.1082939 0.80741274 0.33057559 -0.48336911 -1.6551757 -2.3250651 -2.7172556 -2.585572][-1.1166892 -0.99262327 -0.50646067 -0.34916008 -0.720291 -1.0783755 -0.80825293 -0.37631822 -0.64490223 -0.96224207 -1.057843 -1.6874137 -2.2367527 -2.7508924 -2.8142824][-4.1334085 -3.5939627 -3.3557498 -3.1727033 -3.0794849 -2.9366765 -2.7477894 -2.5321388 -2.9398217 -2.8966002 -2.6107655 -2.4968333 -3.3031063 -3.7188673 -4.0766969][-4.620626 -4.6153555 -4.4455428 -4.286593 -4.2835493 -4.2893176 -4.1411929 -3.8357553 -4.1060734 -4.3798375 -3.9851494 -4.2059565 -4.7317042 -4.9152818 -4.8860855][-5.9598927 -5.6914339 -5.4023809 -5.3644834 -5.57269 -5.3174071 -5.1284595 -5.0713916 -5.0776291 -4.7418933 -4.3505139 -4.7362981 -4.8611374 -5.0256386 -5.0364857][-5.9321423 -6.3853693 -6.6308556 -5.9819665 -6.1544595 -6.4818954 -5.9824972 -5.5863576 -5.4999247 -5.2597008 -4.620142 -4.2111306 -4.59602 -4.903101 -4.8116322][-6.3113832 -6.5323009 -6.666254 -6.7595277 -6.4802575 -5.9192595 -5.1042047 -4.6699429 -5.0473151 -5.0940237 -4.7511263 -4.9104929 -4.9515171 -4.7747664 -4.9071312]]...]
INFO - root - 2017-12-14 03:11:16.975380: step 2310, loss = 0.74, batch loss = 0.35 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:58s remains)
INFO - root - 2017-12-14 03:11:19.220852: step 2320, loss = 0.79, batch loss = 0.40 (35.3 examples/sec; 0.227 sec/batch; 20h:48m:36s remains)
INFO - root - 2017-12-14 03:11:21.500243: step 2330, loss = 0.81, batch loss = 0.42 (35.0 examples/sec; 0.229 sec/batch; 20h:58m:41s remains)
INFO - root - 2017-12-14 03:11:23.753112: step 2340, loss = 0.77, batch loss = 0.38 (34.5 examples/sec; 0.232 sec/batch; 21h:14m:21s remains)
INFO - root - 2017-12-14 03:11:26.030772: step 2350, loss = 0.71, batch loss = 0.32 (35.3 examples/sec; 0.226 sec/batch; 20h:45m:52s remains)
INFO - root - 2017-12-14 03:11:28.257002: step 2360, loss = 0.89, batch loss = 0.50 (36.3 examples/sec; 0.221 sec/batch; 20h:14m:09s remains)
INFO - root - 2017-12-14 03:11:30.485294: step 2370, loss = 0.92, batch loss = 0.53 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:30s remains)
INFO - root - 2017-12-14 03:11:32.769314: step 2380, loss = 0.88, batch loss = 0.49 (33.1 examples/sec; 0.242 sec/batch; 22h:11m:01s remains)
INFO - root - 2017-12-14 03:11:35.011918: step 2390, loss = 0.82, batch loss = 0.43 (36.3 examples/sec; 0.221 sec/batch; 20h:13m:58s remains)
INFO - root - 2017-12-14 03:11:37.227768: step 2400, loss = 0.77, batch loss = 0.38 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:58s remains)
2017-12-14 03:11:37.497389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9099011 -3.7983062 -4.0708661 -4.2378449 -4.2442451 -3.8018978 -3.1189971 -3.3832 -3.7611003 -3.7556467 -3.558733 -3.6652162 -3.9156137 -3.8400192 -3.6423948][-2.4957809 -3.5898409 -3.5346379 -3.5647087 -4.0053568 -4.248291 -4.1501102 -3.776063 -3.212605 -3.4037347 -3.5498772 -3.5426788 -3.3976798 -3.711242 -3.9425406][-3.0748489 -3.0669007 -3.2630932 -3.2073145 -3.3044314 -3.8443139 -3.9140768 -3.9410033 -4.2243638 -4.0476203 -3.435987 -3.3726444 -3.5890083 -3.66261 -3.6273389][-3.9320636 -3.606636 -3.2257643 -3.1204596 -2.9924889 -2.8009143 -2.7186475 -2.7834013 -2.8877258 -3.2785139 -3.515615 -3.1031508 -2.7763622 -2.7327437 -2.485405][-0.15733588 -0.86565876 -1.6060536 -1.6565337 -1.1049528 -0.46258044 0.29424727 0.41362488 -0.11397552 -0.96206719 -1.3530097 -1.8392456 -2.4674644 -2.667135 -2.4385064][0.065743923 0.420493 0.36225474 -0.17696154 -0.43035614 -0.018445253 0.74151981 1.5728902 1.9362046 1.9880315 1.5976607 0.72234023 0.46271384 0.15218222 -0.28156412][-0.42412519 0.37761152 0.50120342 0.73580158 0.88937676 0.83423436 0.9803623 1.3605732 1.8843335 2.2915998 2.9002476 2.3157225 1.6468447 1.245303 1.3227366][0.50232279 0.73291433 1.1281751 1.7237698 2.0696073 2.6713662 3.1063104 2.965559 3.0421772 2.740397 2.454896 1.9115511 2.0426493 1.5256432 0.72458112][1.4172436 1.4689363 1.6420668 2.3849 2.8376079 3.2644644 3.2868924 3.737978 3.97723 3.3371205 3.1331239 2.6509261 2.7712078 2.3874178 2.091948][0.39621603 0.98235595 0.97574461 0.85321176 0.84747255 1.8346764 2.3487654 2.4756699 2.7163358 2.3459978 2.1448565 1.5752441 1.5577036 1.2931956 0.93685782][-1.6089883 -1.8704289 -1.5692705 -1.4014626 -1.7702013 -1.5951438 -0.94621676 0.27644479 1.3225077 1.3935918 1.63475 1.1411895 0.79101884 0.26819551 -0.14007258][-4.2522864 -4.0346346 -3.2601874 -2.8536713 -2.4924638 -1.5196184 -0.81816065 0.032322407 0.7845124 1.3411223 1.9707631 1.610787 1.0095729 0.3661989 -0.52775109][-4.1568646 -4.0785813 -3.6913412 -3.2532341 -2.8951244 -2.1633208 -1.2986238 -0.96477062 -0.70673203 -0.65029418 0.075129747 0.38287938 0.51411045 0.21102488 -0.27380288][-3.0705965 -3.3491077 -3.4087353 -3.3255794 -3.3813767 -2.5155 -1.6287258 -1.3106892 -0.90856504 -0.74488294 -0.66155946 -1.127423 -1.1602359 -1.0077431 -0.35405636][-3.1520195 -3.0506015 -2.598026 -2.2677152 -1.8685024 -1.1376274 -0.3141433 0.31316864 0.34160173 -0.17585576 0.22095883 -0.1161232 -0.91793346 -1.3979571 -1.6878527]]...]
INFO - root - 2017-12-14 03:11:39.727478: step 2410, loss = 0.78, batch loss = 0.39 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:35s remains)
INFO - root - 2017-12-14 03:11:41.946826: step 2420, loss = 0.80, batch loss = 0.41 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:32s remains)
INFO - root - 2017-12-14 03:11:44.173011: step 2430, loss = 0.82, batch loss = 0.43 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:56s remains)
INFO - root - 2017-12-14 03:11:46.413443: step 2440, loss = 0.78, batch loss = 0.39 (37.3 examples/sec; 0.214 sec/batch; 19h:39m:08s remains)
INFO - root - 2017-12-14 03:11:48.710427: step 2450, loss = 0.96, batch loss = 0.57 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:42s remains)
INFO - root - 2017-12-14 03:11:50.939706: step 2460, loss = 0.81, batch loss = 0.43 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:48s remains)
INFO - root - 2017-12-14 03:11:53.165771: step 2470, loss = 0.73, batch loss = 0.34 (34.8 examples/sec; 0.230 sec/batch; 21h:05m:14s remains)
INFO - root - 2017-12-14 03:11:55.417212: step 2480, loss = 0.83, batch loss = 0.44 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-14 03:11:57.658609: step 2490, loss = 0.85, batch loss = 0.46 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:07s remains)
INFO - root - 2017-12-14 03:11:59.877416: step 2500, loss = 0.81, batch loss = 0.42 (36.8 examples/sec; 0.218 sec/batch; 19h:56m:27s remains)
2017-12-14 03:12:00.168467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8923838 -3.7684832 -3.7660255 -3.6456313 -3.9191823 -3.6580276 -3.7786341 -3.7039869 -3.6344481 -3.5122674 -3.6303673 -3.0590878 -2.6631861 -2.3040009 -2.2786388][-4.3463469 -4.1306295 -4.0112128 -3.6507385 -3.9412422 -4.0900011 -4.0118122 -3.6859853 -3.5198069 -3.7626793 -4.0363603 -3.8668342 -3.7944531 -3.5176048 -2.7341619][-4.1464438 -4.7732329 -4.863452 -4.1231685 -3.8536067 -3.8024664 -3.8855052 -3.946218 -3.9997187 -3.9493155 -4.1743989 -4.2046089 -3.9333029 -3.8599181 -3.4034104][-3.7812476 -4.5527706 -4.9236774 -4.3937387 -3.8173733 -3.3634796 -3.1635361 -2.7885492 -2.8149729 -3.0895793 -3.0808539 -2.8396521 -2.8975937 -2.9249971 -2.7636538][-4.0592008 -4.2146358 -3.6345744 -2.7875195 -2.3608956 -1.1974123 -0.39161837 -0.131508 -0.024199247 -0.21310663 -0.73005819 -0.97711796 -1.030939 -1.0099596 -0.99384558][-3.1405358 -3.9010932 -3.9296243 -2.6043961 -1.6099231 -0.47402728 0.92822731 1.8372406 2.2176609 1.8552028 1.0500549 0.60061634 0.56392348 0.60127461 0.54362786][-2.9584208 -3.3077109 -3.4262838 -2.0640872 -0.12147653 1.392027 2.3040361 2.6085272 2.7356958 2.35317 1.6053351 1.309858 1.3397044 1.4587282 1.4275833][-1.8573986 -1.5552037 -1.5676317 -1.1315451 -0.93361747 0.7859515 2.685246 3.0970769 3.3370352 2.6050839 1.6073972 1.179242 1.1759392 1.1267294 0.93597138][-1.7587817 -1.1382536 -0.26201844 0.23750651 0.84631717 1.948352 2.661602 2.9091897 3.1785994 2.4029188 1.2709619 0.43496406 0.37387145 0.36143982 0.21445191][-1.0571392 -0.97645879 -0.82290888 -0.58180654 -0.38464558 0.33712184 0.896325 0.87739408 1.3526846 0.95534551 0.11806905 -0.58072853 -0.85930967 -0.73991668 -0.72923219][-2.1454241 -2.2000866 -2.7342551 -2.587306 -2.193846 -1.0638084 -0.11497784 0.33009875 0.12018335 -0.81701672 -1.8677142 -2.24584 -2.1989453 -1.4776292 -1.0890012][-3.409909 -3.2458725 -2.8355737 -2.83146 -2.9477286 -2.1110752 -1.1789391 -0.99784255 -0.68593478 -1.3258592 -2.1260285 -2.7825902 -3.0066309 -2.579716 -2.2615633][-3.7077618 -3.7494204 -3.6396785 -3.2628412 -3.3559272 -2.7477493 -2.0949876 -2.2627342 -2.2295942 -2.5145116 -2.986886 -3.0494258 -3.0077991 -2.9061074 -2.8649666][-3.8487105 -4.3910747 -4.3502746 -4.0421166 -3.9522948 -3.4933376 -3.1881592 -3.6620541 -3.4585924 -3.2510929 -3.1641273 -3.2013621 -3.5952661 -3.6907134 -3.3668079][-3.8742294 -4.0568256 -4.4350648 -4.8075385 -4.9984932 -4.3142691 -3.3507271 -3.0428419 -2.8292096 -2.8879075 -3.1800144 -2.9321539 -3.2155943 -3.7934773 -3.723681]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-14 03:12:02.696969: step 2510, loss = 0.72, batch loss = 0.33 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:08s remains)
INFO - root - 2017-12-14 03:12:04.930789: step 2520, loss = 0.63, batch loss = 0.25 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-14 03:12:07.174801: step 2530, loss = 0.70, batch loss = 0.31 (35.9 examples/sec; 0.223 sec/batch; 20h:24m:29s remains)
INFO - root - 2017-12-14 03:12:09.422141: step 2540, loss = 0.89, batch loss = 0.50 (36.1 examples/sec; 0.221 sec/batch; 20h:17m:50s remains)
INFO - root - 2017-12-14 03:12:11.667499: step 2550, loss = 0.71, batch loss = 0.32 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:37s remains)
INFO - root - 2017-12-14 03:12:13.877350: step 2560, loss = 0.71, batch loss = 0.32 (36.3 examples/sec; 0.220 sec/batch; 20h:10m:17s remains)
INFO - root - 2017-12-14 03:12:16.135137: step 2570, loss = 0.66, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:17s remains)
INFO - root - 2017-12-14 03:12:18.371338: step 2580, loss = 1.31, batch loss = 0.93 (34.4 examples/sec; 0.233 sec/batch; 21h:20m:07s remains)
INFO - root - 2017-12-14 03:12:20.624064: step 2590, loss = 0.79, batch loss = 0.41 (35.8 examples/sec; 0.224 sec/batch; 20h:29m:49s remains)
INFO - root - 2017-12-14 03:12:22.852157: step 2600, loss = 0.98, batch loss = 0.60 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:57s remains)
2017-12-14 03:12:23.150139: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081011653 0.3639915 0.31371522 0.077563763 -0.14742887 -0.11521602 0.23023582 0.38384247 -0.553501 -1.1841061 -1.2724807 -1.3864111 -1.5578433 -1.161667 -0.5098114][-0.853397 -0.56952381 -0.56769657 -0.57876062 -0.79370832 -1.1876574 -1.0957007 -0.30305958 0.26915503 -0.55932534 -1.6978106 -1.6387503 -1.3675007 -1.1002733 -1.0915291][-0.46352613 -0.72694135 -0.91238391 -1.112015 -1.5528836 -1.8953887 -2.234148 -1.7395098 -1.4845426 -1.0252168 -0.32221961 -0.50794005 -0.92655361 -0.58910811 -0.46200562][-0.86204433 -0.95996988 -1.0854259 -1.0553057 -1.284124 -1.1412852 -1.0627871 -0.86391783 -0.77838695 -0.36864185 0.37320971 1.2171502 2.0167997 1.5966728 1.2544055][-0.082184076 0.160398 0.24038577 0.026611328 -0.10025048 0.33799171 0.76211858 1.2199039 1.0351152 0.41352463 0.34215879 0.57434464 0.87991238 1.6849973 2.5136011][-0.688009 -0.32332885 -0.23552835 -0.20734775 -0.41019988 -0.0012255907 0.27049255 1.7285993 2.5378377 2.1297443 2.0620329 1.8194153 1.7128382 2.2193797 2.3177168][0.21656656 0.51001406 0.49912643 0.48614979 0.36318445 1.0289078 0.98449087 1.6731155 2.0818303 2.6716321 3.3779476 3.2240055 3.5535614 3.2364714 3.2146518][0.32600498 0.73207784 0.95005393 1.0437088 1.2347732 2.1085746 2.4639771 3.2025907 2.8670857 2.6711881 3.0313985 3.2623322 3.030895 2.7824619 2.8565829][1.5223019 2.034143 2.3818657 2.2556326 2.0888686 2.9488647 3.64975 4.2276764 3.8423703 3.2152369 2.6316016 2.1918347 2.3082998 2.3532345 2.270232][1.6647394 2.0254161 1.9456923 2.073267 2.354954 2.8530409 2.5541093 3.2600586 3.0937135 2.7433484 1.9874907 1.2950544 1.3272626 1.4595306 1.4322007][0.67895532 0.93152952 0.94832826 0.91039968 0.70370579 1.5165839 2.2243574 2.5997865 1.9798822 1.424741 1.0158558 1.1747038 0.89945173 0.35278177 0.677145][-0.11847997 -0.12076604 -0.073423266 0.029183626 -0.013220906 0.4004724 0.81301022 1.6731772 1.6806753 1.4841614 1.24495 0.70111752 0.44059563 0.51449442 0.25840259][-1.0790341 -0.7286433 -0.7681632 -0.85974419 -0.75949883 -0.066725612 0.3370173 1.0800662 0.86971188 0.67555022 0.76994324 0.68901634 0.30855656 -0.020456314 -0.63037086][-2.0333066 -1.9336463 -1.7140369 -1.6473054 -1.6487021 -0.83756959 -0.18083203 0.71236968 0.47392678 0.093437552 -0.20626581 -0.2103709 -0.099470973 -0.37694228 -0.83219516][-2.5300078 -2.8455608 -2.666944 -2.3126378 -2.2896137 -1.5447571 -0.953831 0.24892688 0.58002138 0.11245561 -0.60214067 -1.1065848 -1.5902246 -1.7014301 -1.3234665]]...]
INFO - root - 2017-12-14 03:12:25.374658: step 2610, loss = 0.81, batch loss = 0.43 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:01s remains)
INFO - root - 2017-12-14 03:12:27.631125: step 2620, loss = 0.80, batch loss = 0.41 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:59s remains)
INFO - root - 2017-12-14 03:12:29.871453: step 2630, loss = 0.75, batch loss = 0.37 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:00s remains)
INFO - root - 2017-12-14 03:12:32.126290: step 2640, loss = 0.82, batch loss = 0.44 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:35s remains)
INFO - root - 2017-12-14 03:12:34.369770: step 2650, loss = 0.80, batch loss = 0.42 (33.5 examples/sec; 0.239 sec/batch; 21h:53m:30s remains)
INFO - root - 2017-12-14 03:12:36.613743: step 2660, loss = 0.82, batch loss = 0.43 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:19s remains)
INFO - root - 2017-12-14 03:12:38.842622: step 2670, loss = 0.86, batch loss = 0.48 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:25s remains)
INFO - root - 2017-12-14 03:12:41.077735: step 2680, loss = 0.68, batch loss = 0.30 (35.7 examples/sec; 0.224 sec/batch; 20h:30m:38s remains)
INFO - root - 2017-12-14 03:12:43.314112: step 2690, loss = 0.92, batch loss = 0.54 (36.1 examples/sec; 0.222 sec/batch; 20h:18m:18s remains)
INFO - root - 2017-12-14 03:12:45.570485: step 2700, loss = 0.79, batch loss = 0.41 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:27s remains)
2017-12-14 03:12:45.862568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.72516286 -1.1235745 -1.2799501 -1.3615497 -1.4461505 -2.0573926 -1.9925435 -1.2806611 -1.1641068 -2.1792948 -3.6096892 -3.197258 -2.7474964 -3.3141007 -4.4951773][-0.5943017 -1.263401 -2.3130326 -2.3058896 -1.9737667 -2.1729715 -2.1368122 -2.2936897 -2.666877 -2.9051886 -3.4046464 -4.1615839 -4.811923 -4.3789945 -4.1434474][-0.21219552 -1.0376266 -1.1837227 -1.4417344 -1.9006313 -2.6394861 -3.0047603 -3.1644654 -3.230022 -3.4382744 -4.0783858 -4.1852407 -4.2299047 -4.6342769 -4.9113107][0.32370746 -0.40509272 -0.85449922 -1.2417581 -1.8929378 -2.1367831 -1.8365865 -1.6879766 -1.6665648 -1.8074281 -2.2860832 -2.3773882 -2.2474737 -2.4823649 -2.9390631][1.7585298 1.0693558 0.60051024 0.37107289 0.27276504 0.23821485 -0.62852979 -1.4413636 -1.3258743 -1.0626414 -0.981039 -1.0590904 -1.2429676 -1.6885029 -1.9499677][1.860696 2.0866847 2.1306062 2.5080705 2.71589 2.5299664 2.6809611 2.4005919 0.84803045 -0.054088593 -0.66906965 -0.979234 -1.0799043 -1.5890875 -1.9864125][1.0805904 1.5351504 2.0107441 2.1310196 2.0559039 2.2888112 2.6385226 2.4842963 1.9496676 0.94397271 -0.1460346 -0.47963572 -0.79865015 -1.2687986 -1.4526148][0.85095084 0.86010611 1.0647672 1.4304568 1.8503405 1.9626034 2.0142665 2.3872156 2.4171195 2.0112033 0.33363426 -0.9366734 -1.2971659 -1.5073124 -1.69467][-0.11195529 0.19122064 0.72618258 0.96126854 1.4071158 1.6934313 2.3876557 3.2591619 2.8728704 2.14753 1.1003002 0.25331986 -0.28714371 -0.4668355 -0.43711221][-1.0247722 -1.145829 -1.0105152 -0.70925903 -0.595158 -0.160694 0.8152858 0.93810189 0.89404261 0.93868768 -0.21660566 -0.76694143 -0.87871242 -1.7142829 -2.2318158][-2.4857192 -2.3048182 -1.6573573 -1.0396278 -0.73933864 -0.454934 -0.060094357 0.077088714 -0.019299746 -0.1322031 -0.30431569 -0.75629938 -1.1881061 -1.2932096 -1.4579517][-3.6029577 -3.7044144 -2.9876573 -2.2715244 -1.5790937 -1.0720024 -0.51301754 -0.2650311 0.17891705 0.083986878 -0.52198684 -0.66312158 -0.60732222 -1.5236001 -2.2171459][-2.6683531 -2.278347 -1.8450497 -1.4611059 -1.4023094 -1.4323261 -0.78701758 -0.38016069 -0.11399579 -0.43506384 -0.91098183 -1.3806508 -1.90181 -2.1283073 -2.3060098][-2.6587229 -2.6481409 -1.9371799 -1.4474447 -1.5954454 -1.6307 -1.4188442 -1.3727133 -1.0696167 -0.97074318 -1.0310603 -1.1115406 -1.2797749 -1.7954612 -2.5385933][-2.9168918 -3.0677319 -2.1450872 -1.1097962 -0.38527203 -0.03148818 0.28464878 -0.20896924 -0.93312955 -1.223562 -1.5863118 -1.6819587 -1.7111809 -1.5145223 -1.2285259]]...]
INFO - root - 2017-12-14 03:12:48.070704: step 2710, loss = 0.74, batch loss = 0.36 (36.2 examples/sec; 0.221 sec/batch; 20h:15m:24s remains)
INFO - root - 2017-12-14 03:12:50.308363: step 2720, loss = 0.77, batch loss = 0.39 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:38s remains)
INFO - root - 2017-12-14 03:12:52.546676: step 2730, loss = 0.87, batch loss = 0.49 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:57s remains)
INFO - root - 2017-12-14 03:12:54.780923: step 2740, loss = 0.83, batch loss = 0.45 (33.0 examples/sec; 0.242 sec/batch; 22h:10m:25s remains)
INFO - root - 2017-12-14 03:12:57.043808: step 2750, loss = 0.73, batch loss = 0.35 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:16s remains)
INFO - root - 2017-12-14 03:12:59.261065: step 2760, loss = 0.71, batch loss = 0.33 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:31s remains)
INFO - root - 2017-12-14 03:13:01.498762: step 2770, loss = 0.67, batch loss = 0.29 (35.3 examples/sec; 0.227 sec/batch; 20h:45m:06s remains)
INFO - root - 2017-12-14 03:13:03.729822: step 2780, loss = 0.81, batch loss = 0.43 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:20s remains)
INFO - root - 2017-12-14 03:13:05.973782: step 2790, loss = 0.78, batch loss = 0.40 (34.7 examples/sec; 0.231 sec/batch; 21h:06m:39s remains)
INFO - root - 2017-12-14 03:13:08.229286: step 2800, loss = 0.79, batch loss = 0.41 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:23s remains)
2017-12-14 03:13:08.519987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.32456589 0.68031931 1.0167286 0.39629745 0.18750525 -0.23876917 -0.73188114 -0.7937727 -1.2271761 -2.4741688 -5.0804238 -5.2354126 -5.4558516 -6.113637 -6.6118345][-1.4489689 -0.75077522 0.099941254 0.45118403 0.17373729 -0.22218609 0.22532892 0.38809943 -1.016865 -1.8869576 -2.4242525 -2.3096466 -3.1569877 -3.7925954 -3.7049923][-0.3519752 -0.021532893 0.29056573 -0.14222741 -0.79398394 -1.6378384 -1.5553135 -1.2021214 -1.7427709 -1.7003223 -2.3491631 -2.8559391 -3.0999212 -3.2420425 -3.8314214][-1.6090672 -1.1928587 -0.8363781 -0.97507751 -1.6982161 -1.2133863 0.20862985 1.4553325 1.3761921 0.60216737 0.33502674 -0.34210074 -1.4357679 -1.798489 -1.6342752][-0.9892354 -0.55454504 0.60332608 1.53806 1.0670505 0.75999761 1.6331189 2.309212 2.5764921 2.5258496 1.6543286 0.49083304 0.37589788 -0.38026774 -0.20865881][-0.04938519 -0.40197396 -0.72274184 0.2546699 1.6564438 2.441889 3.5146778 4.2630558 3.5523989 2.4573109 1.4474373 0.73875523 0.73862076 0.78413057 1.5548935][-0.63158011 0.14535832 1.8838854 2.0754101 1.9806812 3.8786552 5.1534519 5.531456 4.7901583 4.4066868 3.7573378 3.2868693 2.3941319 2.1260145 2.269757][1.8710794 2.0253921 2.6164954 3.376024 3.9198596 4.7106905 5.1111021 5.8581133 5.7156858 5.1517544 3.0999725 1.386168 0.87731647 0.95806193 1.3183539][2.2335498 3.0467932 3.7090819 3.8730862 3.9770525 5.3159418 6.65709 5.8134832 4.18978 3.1388733 1.7451963 -0.098982215 -0.23562586 0.49878383 1.3308418][2.5397723 2.3304875 2.6496856 2.6095059 2.263675 2.8430974 3.8886402 3.3508923 1.4009364 0.76861215 0.0065712929 -0.55273211 -0.85088682 -0.62404215 -0.18904889][0.075511932 -0.25153279 -0.89433444 -1.6955711 -1.3442395 0.035370708 1.025106 0.45152378 -0.086129427 -0.35285532 -1.4710222 -2.2819445 -2.9252362 -2.2589459 -0.97923851][-0.71855748 -1.2360356 -1.5460587 -2.2058597 -2.417758 -1.0688818 0.36065102 -0.3904562 -1.1517843 -1.5448161 -2.8723576 -3.2786975 -2.6052971 -2.5286722 -2.7597098][-1.1669862 -1.1545526 -1.9471239 -2.6987042 -3.252784 -1.9958901 -0.59296966 -1.1543369 -2.1624994 -2.3723445 -3.2805128 -4.0233879 -4.61307 -4.2112412 -3.1421223][-1.8276628 -2.1139574 -2.8666363 -3.6741505 -3.4675932 -2.0629117 -0.57975531 -1.3228452 -2.4873648 -2.8629339 -4.4105539 -4.2706628 -3.4112792 -3.402442 -3.190618][-4.3163347 -4.4610357 -4.0640087 -4.3631086 -3.7144632 -2.6342916 -1.2314541 -1.6627442 -2.8007407 -3.3097825 -3.9626341 -3.9408674 -3.4104886 -2.5102928 -2.1874464]]...]
INFO - root - 2017-12-14 03:13:10.777269: step 2810, loss = 1.22, batch loss = 0.84 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:26s remains)
INFO - root - 2017-12-14 03:13:13.002157: step 2820, loss = 0.71, batch loss = 0.33 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:43s remains)
INFO - root - 2017-12-14 03:13:15.245601: step 2830, loss = 0.68, batch loss = 0.30 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-14 03:13:17.525079: step 2840, loss = 0.84, batch loss = 0.46 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:30s remains)
INFO - root - 2017-12-14 03:13:19.786704: step 2850, loss = 0.70, batch loss = 0.33 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:46s remains)
INFO - root - 2017-12-14 03:13:22.071791: step 2860, loss = 0.71, batch loss = 0.33 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:46s remains)
INFO - root - 2017-12-14 03:13:24.312857: step 2870, loss = 0.82, batch loss = 0.44 (35.6 examples/sec; 0.224 sec/batch; 20h:32m:50s remains)
INFO - root - 2017-12-14 03:13:26.561981: step 2880, loss = 0.74, batch loss = 0.37 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:08s remains)
INFO - root - 2017-12-14 03:13:28.820611: step 2890, loss = 0.71, batch loss = 0.34 (34.7 examples/sec; 0.231 sec/batch; 21h:07m:16s remains)
INFO - root - 2017-12-14 03:13:31.052271: step 2900, loss = 0.70, batch loss = 0.33 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:31s remains)
2017-12-14 03:13:31.336942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.691245 -2.1664462 -2.6155398 -3.1449866 -3.2608528 -2.5928919 -1.6687382 -1.8238454 -2.2418251 -2.2276964 -1.8538357 -2.1038322 -2.5841684 -2.2403812 -1.9530531][-3.0041878 -3.00722 -3.1312957 -3.3898168 -3.3843751 -3.1072588 -2.8881483 -2.5593195 -1.9626241 -2.4814591 -2.9119897 -3.0225806 -3.1621215 -3.6067147 -3.942461][-3.5413709 -3.8193076 -3.7131219 -3.7498059 -3.6922154 -3.1095181 -2.6397264 -2.8380227 -3.0073657 -2.6422024 -2.0602667 -2.4158807 -3.1085367 -3.3370228 -3.3213234][-3.3241289 -3.6055274 -4.0310917 -4.0875368 -3.7304761 -3.2604513 -3.1234491 -2.7108335 -2.3196106 -2.4501238 -2.40321 -1.9567027 -1.3863331 -2.0882568 -2.9936509][-3.6975226 -4.015749 -3.9678831 -3.8986926 -3.781683 -3.3405206 -2.8296416 -2.4325998 -2.4668841 -2.1427214 -1.7337912 -2.0964785 -2.3650713 -2.3029757 -1.9331228][-4.7092018 -4.5640826 -4.1391912 -3.9729028 -3.7996092 -3.0397949 -2.27873 -1.848188 -1.8677336 -1.8337916 -1.8447623 -1.6025114 -1.2252955 -1.9855094 -2.7282145][-3.2767978 -3.3681965 -3.5875092 -2.9932084 -2.3541591 -1.5512176 -0.88323987 -0.86958915 -1.1046171 -1.0047327 -0.53936255 -0.47082782 -0.75907564 -0.83563077 -0.97377217][-1.9955291 -2.0553882 -2.0261753 -1.6533617 -1.4172759 -0.71960092 -0.17162395 -0.16833329 -0.3477664 -0.49512434 -0.36268592 -0.34457624 -0.31445241 -0.63740015 -0.46279085][-0.94238967 -0.41575825 -0.43980658 -0.34078348 -0.54798388 0.16790664 0.77610505 0.66465604 0.50547373 0.21805489 0.13908386 -0.23316586 -0.32731009 -0.0046659708 0.4944638][-0.983213 -0.86362207 -1.0032359 -1.0909173 -1.4601395 -1.4714221 -1.3919851 -1.2425946 -1.0313036 -0.89359957 -0.63145781 -0.53604674 -0.36127865 0.04876709 0.49320495][-3.0948639 -3.4293256 -3.1887779 -3.4035707 -3.5415719 -2.8993 -2.495713 -2.6640728 -2.6249204 -2.2614043 -1.8979065 -1.4486749 -1.1251743 -0.7771647 -0.63492155][-3.876935 -4.0958161 -4.5434318 -4.6687369 -4.746244 -3.7923193 -2.6898668 -2.2677443 -2.1364839 -2.1381242 -1.9366524 -1.7927051 -1.9038982 -1.6246647 -1.5673902][-4.1271639 -4.4160595 -4.3299589 -4.4010596 -4.8686271 -4.2229705 -3.8340249 -3.4290361 -2.7291684 -2.1974068 -1.5687034 -1.5635344 -1.5598986 -2.0483844 -2.613744][-4.0017724 -4.2017884 -4.791472 -5.0788321 -5.151854 -4.3634562 -3.6168108 -3.2935252 -3.1184397 -2.8633294 -2.3398821 -2.3560631 -2.2781892 -2.2517681 -2.2454171][-4.2250128 -4.4372325 -4.5678754 -5.0634365 -4.8225374 -3.8402419 -2.9707131 -2.6530461 -2.3782616 -2.3017709 -2.3410325 -2.311923 -2.3712821 -2.16883 -1.9039531]]...]
INFO - root - 2017-12-14 03:13:33.571941: step 2910, loss = 0.73, batch loss = 0.36 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:51s remains)
INFO - root - 2017-12-14 03:13:35.827568: step 2920, loss = 0.72, batch loss = 0.34 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-14 03:13:38.090033: step 2930, loss = 0.79, batch loss = 0.42 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:55s remains)
INFO - root - 2017-12-14 03:13:40.330829: step 2940, loss = 0.71, batch loss = 0.34 (35.0 examples/sec; 0.228 sec/batch; 20h:54m:06s remains)
INFO - root - 2017-12-14 03:13:42.586577: step 2950, loss = 0.74, batch loss = 0.37 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:48s remains)
INFO - root - 2017-12-14 03:13:44.830829: step 2960, loss = 0.81, batch loss = 0.44 (30.9 examples/sec; 0.259 sec/batch; 23h:40m:12s remains)
INFO - root - 2017-12-14 03:13:47.077417: step 2970, loss = 0.79, batch loss = 0.42 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:32s remains)
INFO - root - 2017-12-14 03:13:49.327746: step 2980, loss = 0.70, batch loss = 0.33 (35.8 examples/sec; 0.223 sec/batch; 20h:27m:24s remains)
INFO - root - 2017-12-14 03:13:51.571167: step 2990, loss = 0.71, batch loss = 0.34 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:28s remains)
INFO - root - 2017-12-14 03:13:53.814657: step 3000, loss = 0.95, batch loss = 0.58 (34.0 examples/sec; 0.235 sec/batch; 21h:33m:12s remains)
2017-12-14 03:13:54.096791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2308059 -5.6079903 -5.5712729 -5.7122316 -5.7890067 -5.211381 -4.5847015 -4.5063977 -4.6279278 -4.4652729 -4.3315477 -4.3554487 -4.5579548 -4.764286 -4.8700094][-4.8471022 -5.3218803 -5.5301256 -5.5683517 -5.3152003 -5.177093 -4.9869237 -4.612689 -4.427022 -4.526001 -4.7039685 -4.6825171 -4.6198697 -4.4885111 -4.3657994][-4.5079451 -4.83564 -5.181179 -5.6329737 -5.7704191 -5.3781862 -4.88535 -4.8602543 -5.137382 -4.7938952 -4.1949625 -4.1525097 -4.3895092 -4.1818714 -3.6437044][-4.0606012 -4.6798391 -4.8517857 -4.7964983 -4.6837916 -4.6883273 -4.7141027 -4.0444431 -3.6874983 -4.03158 -4.4721651 -4.1436882 -3.6303265 -3.2815518 -3.0039659][-3.3314285 -3.5792537 -3.6751194 -3.781836 -3.7980061 -3.601058 -3.3668818 -3.2304122 -3.5000668 -3.3939848 -3.6385498 -3.9282734 -3.9752991 -3.4902906 -3.2416041][-2.4587734 -2.4684973 -2.4967806 -2.1845253 -2.0279062 -2.5053797 -2.9061475 -3.1020203 -2.9724264 -2.9849224 -2.9316962 -2.9208302 -2.9313469 -2.6716235 -2.4142928][-1.3400936 -1.0571804 -0.91828293 -0.6366353 -0.251297 -0.46844459 -1.0757253 -1.9272832 -2.1077926 -1.9277165 -1.7018876 -1.9295791 -2.2984076 -2.361681 -2.3815358][-0.54767919 -0.72526658 -0.61384737 -0.6155405 -0.76866281 -0.52590442 -0.096267343 -0.26774466 -0.77163577 -1.2584269 -1.4951844 -1.5334153 -1.8435417 -2.1932883 -2.2871017][-0.49334514 -0.57183957 -0.25841165 -0.29132903 -0.46555483 -0.42082775 -0.17674506 -0.13362181 -0.67680669 -1.1756945 -1.4876583 -1.3878835 -1.3612621 -1.7007749 -2.1285772][-1.9226993 -1.9133244 -1.6659966 -1.6345409 -1.9173354 -2.1564944 -1.9537054 -1.9229894 -2.3804035 -2.1650195 -2.0388513 -2.0475514 -2.1960921 -2.3072917 -2.4221215][-3.5791326 -3.3425963 -3.4470305 -3.2820268 -3.4029741 -3.8320599 -3.8414149 -3.3825793 -2.7765977 -2.4302013 -2.4282191 -2.1199317 -2.0187833 -2.3959889 -2.5869405][-5.1137862 -4.9247389 -4.3224974 -4.4107571 -5.0181623 -5.2332535 -4.7266064 -4.5642176 -4.150012 -3.50059 -2.9762616 -3.0240922 -2.8999028 -2.9743276 -2.6018097][-4.0198011 -4.4113178 -4.631516 -4.6974506 -4.4246106 -4.85091 -5.2911086 -5.3856544 -5.3350086 -4.7553158 -4.5144663 -3.4136479 -2.2868574 -2.2418303 -1.9820282][-3.5819588 -3.4128277 -3.5214999 -3.9070289 -4.3199153 -4.274601 -3.6283269 -3.4883704 -3.9801314 -3.8518906 -3.9232445 -3.0254257 -2.142025 -1.9096872 -1.5394349][-3.3092866 -2.9848061 -3.2475348 -3.3611312 -3.3643994 -3.70404 -3.8746126 -3.6585073 -2.8669591 -2.7490516 -3.0612593 -2.6411662 -2.3226962 -2.064501 -1.6948738]]...]
INFO - root - 2017-12-14 03:13:56.356034: step 3010, loss = 0.74, batch loss = 0.37 (36.0 examples/sec; 0.222 sec/batch; 20h:19m:50s remains)
INFO - root - 2017-12-14 03:13:58.583619: step 3020, loss = 0.84, batch loss = 0.48 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:55s remains)
INFO - root - 2017-12-14 03:14:00.821573: step 3030, loss = 0.67, batch loss = 0.30 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - 2017-12-14 03:14:03.074377: step 3040, loss = 0.80, batch loss = 0.44 (36.0 examples/sec; 0.222 sec/batch; 20h:19m:34s remains)
INFO - root - 2017-12-14 03:14:05.339805: step 3050, loss = 0.71, batch loss = 0.34 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:15s remains)
INFO - root - 2017-12-14 03:14:07.575273: step 3060, loss = 1.53, batch loss = 1.16 (34.9 examples/sec; 0.229 sec/batch; 20h:57m:45s remains)
INFO - root - 2017-12-14 03:14:09.808380: step 3070, loss = 0.79, batch loss = 0.42 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:31s remains)
INFO - root - 2017-12-14 03:14:12.080974: step 3080, loss = 0.85, batch loss = 0.48 (35.8 examples/sec; 0.224 sec/batch; 20h:27m:48s remains)
INFO - root - 2017-12-14 03:14:14.311611: step 3090, loss = 0.74, batch loss = 0.38 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:39s remains)
INFO - root - 2017-12-14 03:14:16.606585: step 3100, loss = 0.78, batch loss = 0.42 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:58s remains)
2017-12-14 03:14:16.876526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4760134 -1.5115423 -1.5315421 -1.1687462 -0.93228954 -0.52541625 -0.26651311 -0.59950566 -0.76919639 -0.98195 -1.3062625 -1.1060189 -1.3591061 -2.2031779 -2.5089741][-1.2435943 -1.2090178 -1.4266384 -1.2353852 -1.0689864 -1.1489766 -1.2318915 -1.0684996 -1.0789899 -1.445735 -1.740281 -2.055758 -2.2788689 -2.2349417 -2.3939371][-1.4930587 -1.3709521 -1.1798775 -1.119149 -1.1399341 -1.4586121 -1.4879885 -1.4591501 -1.6532803 -1.7729816 -1.9899482 -2.2309654 -2.7957211 -2.6586952 -2.250962][-0.88735807 -0.99516225 -0.99856496 -0.95680737 -0.73577058 -0.81810391 -0.96845853 -1.0739477 -1.3596087 -1.8406261 -2.1702433 -1.7299715 -1.4638791 -1.8668269 -2.161669][-0.40125537 -0.077089071 0.14170754 0.26463175 0.51086807 0.45119905 0.24411988 0.17493296 0.049740553 -0.37443435 -1.0625222 -1.5910174 -1.7319888 -1.9506724 -1.7569048][0.88007784 0.92012835 0.79380608 0.87504387 0.86802125 0.63585734 0.61378765 0.86097169 0.57900667 -0.02850759 -0.62016368 -1.0347376 -0.93936664 -1.7626863 -2.6338463][1.4321418 1.5436194 1.6709018 1.9394441 2.1846697 2.0180333 1.3108208 0.82801247 0.13770628 0.10810542 -0.25438118 -0.72045267 -1.1409156 -1.4492916 -1.4861929][1.1206973 1.1288662 1.2632191 1.7974105 2.3348918 2.4869466 2.3126235 1.8320994 1.4836142 1.1899583 -0.094525576 -0.98438162 -1.281104 -2.0662785 -2.5309997][1.3669455 1.1671629 0.97800374 0.94262695 1.2424002 1.5493422 2.5419126 2.7645745 2.1993866 1.4845266 0.49300766 -0.69277048 -1.4650002 -1.6440485 -1.9120017][1.3488212 1.1013992 0.80249143 0.6639657 0.71519017 0.83156037 1.1779277 0.91582608 0.70645404 0.43642116 -0.37303197 -0.96489984 -1.4471873 -2.296818 -2.8879046][-1.314357 -1.3949298 -1.1726223 -1.1394817 -1.3962498 -1.552675 -1.1761581 -1.3274021 -1.6957958 -1.2780387 -1.1366563 -1.4740839 -1.7115403 -1.7105463 -1.7028904][-2.1251326 -2.1716216 -2.51786 -2.4641066 -2.334096 -2.1869714 -1.9351296 -1.5612841 -1.0945711 -1.1095123 -1.3955258 -1.6962005 -2.0345032 -2.3581667 -2.1370778][-3.4717755 -3.3018699 -2.6775403 -2.5635619 -2.9232211 -2.6574757 -2.057369 -1.5280367 -1.4417766 -1.1916282 -1.3286449 -1.6453028 -2.1016927 -2.2093425 -2.5490794][-2.6026764 -3.0066352 -3.8222575 -3.6902986 -3.4670153 -3.2094891 -2.876472 -2.7937422 -2.7156806 -2.592016 -2.1096873 -2.0074 -2.3169861 -1.941865 -1.5183854][-3.4099092 -3.134316 -2.53085 -2.9978449 -3.699923 -3.2870917 -2.6174755 -2.4444091 -2.4706287 -2.8114095 -3.1978376 -3.2354493 -2.6951523 -3.1262755 -3.206434]]...]
INFO - root - 2017-12-14 03:14:19.110842: step 3110, loss = 0.73, batch loss = 0.37 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:00s remains)
INFO - root - 2017-12-14 03:14:21.376285: step 3120, loss = 0.79, batch loss = 0.42 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:45s remains)
INFO - root - 2017-12-14 03:14:23.622897: step 3130, loss = 0.90, batch loss = 0.54 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:54s remains)
INFO - root - 2017-12-14 03:14:25.865722: step 3140, loss = 0.76, batch loss = 0.40 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:05s remains)
INFO - root - 2017-12-14 03:14:28.110641: step 3150, loss = 0.77, batch loss = 0.40 (35.3 examples/sec; 0.226 sec/batch; 20h:42m:41s remains)
INFO - root - 2017-12-14 03:14:30.358479: step 3160, loss = 0.84, batch loss = 0.48 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:12s remains)
INFO - root - 2017-12-14 03:14:32.596332: step 3170, loss = 0.82, batch loss = 0.45 (35.3 examples/sec; 0.226 sec/batch; 20h:42m:53s remains)
INFO - root - 2017-12-14 03:14:34.823292: step 3180, loss = 0.74, batch loss = 0.37 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:33s remains)
INFO - root - 2017-12-14 03:14:37.074483: step 3190, loss = 0.72, batch loss = 0.36 (34.4 examples/sec; 0.232 sec/batch; 21h:15m:58s remains)
INFO - root - 2017-12-14 03:14:39.321304: step 3200, loss = 0.67, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:18s remains)
2017-12-14 03:14:39.611383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1805506 -2.3111496 -2.7504454 -2.9244728 -2.9865117 -2.4849424 -1.938426 -1.8835825 -2.0968328 -2.9493968 -4.1799769 -3.788039 -3.247571 -2.76873 -3.115273][-2.6455462 -2.7977622 -3.0824966 -3.1958346 -3.3181815 -2.9550192 -2.5843236 -2.5636668 -2.4383082 -2.9693398 -3.8844166 -4.0709834 -3.9788933 -3.8147733 -3.7716026][-1.722672 -2.5254831 -3.2083116 -3.1664228 -2.8368907 -2.489912 -2.299813 -2.2126122 -2.4857779 -2.9505191 -3.6452303 -3.2502117 -3.0210924 -3.4333119 -4.2832022][-1.9095161 -1.9328792 -2.6013956 -2.9992642 -2.8280394 -2.18008 -1.7069023 -1.2441909 -1.4622767 -2.31785 -2.7974052 -2.2827327 -2.1442683 -2.2383575 -2.7295847][-2.3469229 -1.8889747 -1.6281949 -1.4096344 -1.0850799 -0.37063241 0.3165139 0.70825827 0.59148967 -0.44175637 -1.7230642 -1.964518 -2.1330497 -2.1467128 -2.2098744][-1.1410273 -1.2997788 -1.6148436 -1.0687082 -0.66934085 0.14801037 0.67795265 1.6101671 2.0241222 1.7125279 0.14426458 -0.59416556 -0.75264096 -0.9105289 -1.407726][-0.65266693 -0.92846 -0.9095639 -0.61795247 -0.036282778 1.0752846 2.1852355 2.4435358 2.110992 1.9661905 0.789703 0.37875712 0.20297015 -0.067934275 -0.66457355][-1.7607937 -1.4249158 -0.93373704 -0.55377269 -0.15288305 0.73519886 1.6524581 2.6330462 2.5355902 1.8128506 0.68102634 -0.20737803 -0.85050452 -1.1128478 -1.6350334][-0.94377452 -1.018099 -0.76618659 -0.088402629 0.87861431 1.50359 1.5647861 1.911229 2.3714271 2.0120258 0.17506969 -0.99815369 -1.6168449 -2.0231502 -2.4156923][-1.1253144 -1.7193217 -1.3229048 -1.0132136 -0.87660331 0.34188282 1.0675687 1.334443 0.82189834 0.50129139 -0.84513229 -1.4607329 -2.3263366 -2.7076275 -3.2173607][-2.2997704 -2.9082944 -2.439564 -2.3135207 -2.4630702 -1.4742372 -0.57635117 -0.44106066 -0.37783492 -0.77103925 -1.9076644 -2.2324483 -2.803266 -3.0574532 -3.2155352][-2.8914337 -4.0096512 -4.0076003 -3.3763509 -2.9754915 -2.2254903 -1.6901186 -1.2710249 -1.1031824 -1.4245189 -1.9963526 -2.2947984 -2.8771324 -3.440891 -4.0125971][-3.4318032 -4.4343934 -4.4340982 -4.1628265 -4.0123053 -3.0111771 -2.1641071 -1.8285975 -1.7954546 -2.2555261 -3.353914 -3.0055132 -2.583209 -3.066864 -3.992085][-3.9494596 -4.6505232 -4.2414923 -4.0159678 -4.0888042 -3.5101364 -2.8438895 -2.1518352 -1.8965102 -2.3162079 -3.421608 -3.4281774 -3.0242949 -2.6635151 -2.9869027][-3.5116534 -4.7547574 -4.4990134 -4.0328903 -3.9832525 -3.2834778 -2.7901387 -2.6944554 -2.5651579 -2.5862219 -3.3205609 -3.0378044 -3.1306825 -3.2051694 -3.3705282]]...]
INFO - root - 2017-12-14 03:14:41.841846: step 3210, loss = 0.72, batch loss = 0.35 (36.1 examples/sec; 0.221 sec/batch; 20h:15m:30s remains)
INFO - root - 2017-12-14 03:14:44.096556: step 3220, loss = 0.65, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:04s remains)
INFO - root - 2017-12-14 03:14:46.358654: step 3230, loss = 0.70, batch loss = 0.33 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:29s remains)
INFO - root - 2017-12-14 03:14:48.591545: step 3240, loss = 0.77, batch loss = 0.41 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:47s remains)
INFO - root - 2017-12-14 03:14:50.841245: step 3250, loss = 0.64, batch loss = 0.27 (34.7 examples/sec; 0.230 sec/batch; 21h:04m:49s remains)
INFO - root - 2017-12-14 03:14:53.081425: step 3260, loss = 0.75, batch loss = 0.39 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:48s remains)
INFO - root - 2017-12-14 03:14:55.303834: step 3270, loss = 0.88, batch loss = 0.52 (36.1 examples/sec; 0.221 sec/batch; 20h:14m:35s remains)
INFO - root - 2017-12-14 03:14:57.568637: step 3280, loss = 0.69, batch loss = 0.34 (34.4 examples/sec; 0.233 sec/batch; 21h:16m:46s remains)
INFO - root - 2017-12-14 03:14:59.831777: step 3290, loss = 0.70, batch loss = 0.34 (34.7 examples/sec; 0.230 sec/batch; 21h:03m:52s remains)
INFO - root - 2017-12-14 03:15:02.064238: step 3300, loss = 0.70, batch loss = 0.34 (35.2 examples/sec; 0.227 sec/batch; 20h:47m:03s remains)
2017-12-14 03:15:02.317052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.98821872 -1.0944384 -1.8726647 -2.1469531 -2.2188542 -2.5071545 -2.2718613 -3.1674209 -3.4389081 -4.2916183 -4.4028244 -5.4264269 -5.40611 -5.3476448 -5.6781297][-1.7912425 -2.1716957 -2.222369 -2.4332156 -2.9572554 -2.6807199 -2.7179701 -2.9323242 -2.953114 -3.4216495 -3.7967596 -4.0034461 -4.5277019 -5.1187854 -5.6358705][-1.2747712 -1.251179 -1.7599353 -1.7116114 -2.2406912 -2.4002781 -2.0545163 -2.0885234 -2.9979172 -3.2905357 -4.0677047 -3.9620533 -4.4898167 -4.6871371 -5.8748097][-0.38090777 -1.7721672 -2.5010166 -2.4771123 -2.6405776 -2.0873435 -1.8979547 -2.004344 -2.3001935 -2.6611636 -2.5921118 -2.5655007 -2.9744687 -3.1563382 -2.9901838][-0.75744236 -0.73891413 -0.72895658 -1.2091384 -1.2703196 -1.4045078 -0.82947707 -0.57721877 -0.99916846 -0.95848304 -0.80918825 -0.70997488 -0.84811234 -1.5616984 -1.7945038][0.65480244 0.15150607 -0.31546092 -0.68371093 -0.74513674 -0.44429338 0.40176952 1.0923043 0.9170469 1.0044454 0.37436879 -0.4362545 -0.71486354 -1.0892739 -1.5482504][-1.4338001 -0.89612311 -0.76168144 -0.53454471 -0.63607073 0.31613624 1.4602326 1.2874821 1.2751669 1.7050716 1.6043798 1.5784055 1.2816039 0.61144292 0.26766336][-0.33323073 -0.15005243 -0.29558229 0.11048639 0.15575159 1.0310904 1.6877612 1.5730993 1.7775046 2.148488 1.2546746 0.7580179 0.75117385 -0.60196114 -1.106046][-0.35075498 0.90585005 0.62436879 0.50684774 0.44611275 2.1310868 2.7741852 2.4100652 3.0439444 2.6544361 0.89599478 -0.13776028 -0.80132425 -1.5576791 -1.8047221][-0.44500947 -0.044394135 0.29143655 0.029104471 0.033461213 1.4943017 2.2776494 1.4698325 0.92790878 0.55565345 -0.10077381 -0.69007313 -1.5214787 -1.8884907 -2.0782309][-2.712584 -2.5691934 -2.4911194 -2.9151616 -2.7592785 -1.3481576 -0.6178596 -0.64006007 -0.24388337 -0.79727709 -1.860357 -2.4170763 -3.0845366 -3.3705263 -3.8587594][-3.5383515 -3.858561 -3.5725291 -3.8484654 -3.4574928 -1.7174501 -0.95967406 -1.2986956 -1.4989276 -1.7346368 -2.924742 -3.3365228 -3.9487934 -4.2847409 -5.0457129][-2.9151378 -3.8009968 -4.2634029 -4.4572186 -4.2974 -2.8374555 -1.9923791 -1.9594364 -2.5800762 -3.5437808 -4.009305 -4.1897287 -4.4787579 -4.6461396 -5.213932][-3.5295341 -3.8663645 -3.9630451 -4.3931842 -4.7579126 -3.5759177 -2.9121122 -3.0672889 -3.9477119 -4.5501547 -4.8310156 -5.0510216 -5.2001548 -5.4660339 -5.2088008][-4.4507174 -4.8610516 -4.78301 -4.8222208 -4.5952873 -3.5791349 -2.8251481 -3.5924532 -4.5932736 -5.0018806 -5.4360943 -5.0252471 -5.0569468 -5.0187044 -4.3980293]]...]
INFO - root - 2017-12-14 03:15:04.569430: step 3310, loss = 0.89, batch loss = 0.54 (35.7 examples/sec; 0.224 sec/batch; 20h:30m:50s remains)
INFO - root - 2017-12-14 03:15:06.804785: step 3320, loss = 0.89, batch loss = 0.53 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:55s remains)
INFO - root - 2017-12-14 03:15:09.086874: step 3330, loss = 0.67, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 20h:30m:19s remains)
INFO - root - 2017-12-14 03:15:11.367821: step 3340, loss = 0.74, batch loss = 0.38 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:05s remains)
INFO - root - 2017-12-14 03:15:13.616908: step 3350, loss = 0.73, batch loss = 0.37 (34.6 examples/sec; 0.231 sec/batch; 21h:09m:09s remains)
INFO - root - 2017-12-14 03:15:15.862054: step 3360, loss = 0.67, batch loss = 0.32 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:02s remains)
INFO - root - 2017-12-14 03:15:18.091506: step 3370, loss = 0.84, batch loss = 0.49 (37.1 examples/sec; 0.215 sec/batch; 19h:41m:49s remains)
INFO - root - 2017-12-14 03:15:20.357535: step 3380, loss = 0.65, batch loss = 0.30 (35.0 examples/sec; 0.228 sec/batch; 20h:52m:12s remains)
INFO - root - 2017-12-14 03:15:22.664320: step 3390, loss = 0.85, batch loss = 0.50 (33.6 examples/sec; 0.238 sec/batch; 21h:45m:47s remains)
INFO - root - 2017-12-14 03:15:24.928808: step 3400, loss = 0.95, batch loss = 0.59 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:34s remains)
2017-12-14 03:15:25.191828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5920579 -2.1371312 -2.0476811 -2.1324551 -2.3926544 -1.9733062 -1.7806736 -1.8436804 -1.6112825 -1.2893924 -1.1207013 -0.8632735 -0.44109082 0.041418314 0.45370591][-2.9154229 -2.1717885 -1.6308835 -1.2233434 -1.4791015 -2.0606532 -2.6169524 -2.8133578 -3.1059015 -2.9998417 -2.4588253 -1.7499833 -1.0411887 -1.0054568 -1.1476067][-2.6978292 -2.8426182 -2.611505 -1.804481 -1.0703616 -1.2687297 -2.072001 -2.6450605 -2.8316369 -2.5867171 -2.2110298 -1.7002362 -1.0732857 -1.0189686 -1.5343077][-2.1900918 -1.9267719 -2.4427161 -2.2468586 -1.6885278 -0.98021424 -1.2097461 -1.4342679 -1.5098814 -1.3002586 -0.90085375 -0.43777275 -0.051702261 0.35161126 0.5156852][-1.6204908 -0.96750605 -0.79285 -1.1756529 -1.2702351 -0.16654408 -0.070053577 0.058471322 -0.49893928 -0.51188827 0.087781072 0.15867507 -0.26710069 -0.30522716 0.355487][-1.6921668 -1.0875523 -0.78339326 -0.35114026 -0.20300364 -0.18212342 -0.060244322 0.39291012 0.64308441 0.3955425 0.14077175 0.32617295 0.29111993 -0.334517 -0.35864902][-0.59857571 -0.73943222 -1.0157857 -0.5308671 0.05830729 0.45938718 0.2787441 0.1098057 0.4754144 0.59350336 1.1511177 1.4902333 1.2343749 0.88107121 0.73498929][-0.60186779 -0.6697154 -0.476099 -0.32384861 -0.20753598 0.96997535 1.6984674 1.5218877 1.5033184 1.820433 2.0457478 1.9764897 1.640251 1.2404274 0.98109138][0.86244071 0.74642289 0.66757119 0.050754547 -0.092641234 0.8220855 1.4583696 1.4510716 1.0382591 1.0079097 0.97605336 1.1099647 0.75869167 0.32633674 0.34677064][-0.83617246 -0.86925876 -0.80401111 -0.98911989 -1.4475961 -0.57266915 -0.044556975 -0.12928033 -0.257967 -0.063521147 0.074053168 0.24871504 0.34044158 0.091204286 0.091540337][-1.7319958 -2.5812869 -3.1361067 -3.2777376 -3.7524061 -3.5518944 -2.4320745 -1.6067194 -1.2804654 -1.7381287 -1.9264015 -1.5100585 -1.6516898 -1.6878402 -0.99813861][-1.5773528 -3.0290265 -4.1523266 -4.8148556 -5.0603023 -4.3052731 -3.6054163 -3.5965853 -3.1705613 -2.2427182 -1.2060691 -1.1672122 -2.1001291 -2.7520695 -2.8182473][-1.362431 -2.659296 -3.6864631 -4.3896942 -4.6734052 -4.2175426 -4.0970488 -4.0943189 -4.0761547 -3.2844009 -2.049871 -1.1110789 -0.70091033 -0.93408328 -1.8579938][-1.7595555 -2.1635864 -2.8317742 -4.3788481 -5.3873091 -3.894074 -2.7678156 -2.9720173 -3.6005855 -3.1476154 -2.9022465 -2.8304911 -2.3855221 -1.917735 -1.1095369][-3.1212008 -3.3332059 -2.5861425 -2.764514 -4.0673013 -4.2593856 -3.5568738 -3.5345845 -3.5546954 -3.0391912 -2.4555442 -2.1491508 -1.5312898 -1.471509 -1.4801393]]...]
INFO - root - 2017-12-14 03:15:27.416133: step 3410, loss = 0.70, batch loss = 0.35 (35.7 examples/sec; 0.224 sec/batch; 20h:27m:32s remains)
INFO - root - 2017-12-14 03:15:29.652805: step 3420, loss = 0.74, batch loss = 0.39 (34.3 examples/sec; 0.234 sec/batch; 21h:20m:43s remains)
INFO - root - 2017-12-14 03:15:31.891558: step 3430, loss = 0.75, batch loss = 0.40 (35.6 examples/sec; 0.225 sec/batch; 20h:31m:21s remains)
INFO - root - 2017-12-14 03:15:34.148014: step 3440, loss = 0.83, batch loss = 0.48 (36.0 examples/sec; 0.222 sec/batch; 20h:17m:49s remains)
INFO - root - 2017-12-14 03:15:36.408398: step 3450, loss = 0.75, batch loss = 0.40 (35.6 examples/sec; 0.225 sec/batch; 20h:32m:54s remains)
INFO - root - 2017-12-14 03:15:38.655658: step 3460, loss = 0.63, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:31m:27s remains)
INFO - root - 2017-12-14 03:15:40.899310: step 3470, loss = 0.78, batch loss = 0.43 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:37s remains)
INFO - root - 2017-12-14 03:15:43.181836: step 3480, loss = 0.71, batch loss = 0.36 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:05s remains)
INFO - root - 2017-12-14 03:15:45.442889: step 3490, loss = 0.77, batch loss = 0.42 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:19s remains)
INFO - root - 2017-12-14 03:15:47.668175: step 3500, loss = 0.69, batch loss = 0.34 (35.9 examples/sec; 0.223 sec/batch; 20h:21m:18s remains)
2017-12-14 03:15:47.962747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.91358656 -1.8295816 -2.158792 -2.2511108 -2.5336144 -1.8247923 -1.3485768 -2.1859703 -3.075253 -4.3071971 -5.5333643 -4.6585212 -3.8266304 -4.629436 -5.6426325][-0.0050835609 -0.94045824 -2.1412835 -2.2086735 -1.364079 -1.5768318 -1.751821 -1.2558048 -0.9410615 -2.1437666 -3.2345307 -3.7054665 -3.9274912 -3.9632936 -4.6972518][0.69231045 -0.016931057 -0.30017674 -0.29744804 -0.034240842 -0.38894129 -0.48689473 -0.47426915 -0.48767447 -0.858042 -1.1603909 -1.7536705 -2.5576189 -2.540117 -2.7396827][0.78002965 1.4415473 1.5368999 1.1828896 0.65890539 0.28038323 -0.1542964 -0.96862465 -1.6097002 -2.0816865 -1.9090495 -1.6274459 -1.5996716 -1.5411836 -1.7071143][0.143659 1.1402828 1.2474581 0.97184455 0.11007071 -0.18731546 -0.17806089 -0.11474168 -0.2606622 -1.7330645 -3.3280091 -3.1535957 -2.8350434 -2.7357488 -1.9301574][-0.017876863 0.10059893 0.051209211 0.41410244 0.48389208 0.078747034 -0.10823715 -0.21986973 -0.63017356 -1.1036506 -1.9059517 -2.2350655 -2.9775717 -2.8688371 -2.4002845][0.071428895 -0.17880988 -0.20264077 0.081447482 0.9132241 1.6386496 1.9720672 0.78883946 0.060680389 -0.625098 -1.4909565 -1.6781139 -1.9971907 -2.1844432 -2.3421738][0.24682319 1.2809247 2.678122 3.0116482 3.0134068 2.7176166 2.9299355 3.5332918 2.8479929 0.36324894 -1.927716 -2.9928498 -3.4475021 -4.1762228 -4.4514003][4.1701427 4.0111756 4.4847627 5.6203256 6.7135129 7.3966079 6.4397473 4.8957167 3.5426188 1.0963954 -2.6638174 -4.7729969 -5.5849967 -5.7773414 -5.9389052][3.8238969 3.6650662 4.1192908 4.6478052 5.3940225 6.3384833 5.6795011 3.555882 1.4319483 -0.18463755 -2.0991213 -4.1113763 -5.420424 -6.2255592 -6.4391174][1.3978661 1.1954085 0.68127191 -0.50665736 -0.953149 0.22972572 1.2243429 0.22361314 -0.99131107 -2.8615904 -4.4157062 -4.4991088 -4.7311578 -5.6713991 -5.7379861][-1.2827663 -1.741399 -2.7255111 -2.7221954 -2.2246368 -2.5199966 -2.5614493 -2.494858 -2.586988 -4.1838226 -5.1293292 -4.9736824 -4.8785534 -4.4455495 -4.28162][-0.17611647 -1.1642849 -1.8319641 -2.4140382 -3.2940402 -3.7384343 -3.7394948 -3.8511171 -4.4966869 -5.0355649 -4.534256 -4.2957454 -3.910893 -3.3981867 -2.8739414][-1.2911925 -0.84972453 -0.33237672 -0.97689271 -2.1609926 -3.2994831 -4.2673454 -4.9871678 -5.1857891 -5.8029833 -6.2437449 -5.6606421 -4.8706217 -3.8884726 -3.3582885][-0.68955314 -1.503081 -2.1969366 -1.9032456 -2.0943146 -2.7012072 -3.8026929 -4.6100297 -5.2630038 -5.7706747 -5.2336135 -4.7545662 -4.77715 -4.9170032 -4.8626256]]...]
INFO - root - 2017-12-14 03:15:50.186481: step 3510, loss = 0.76, batch loss = 0.41 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:26s remains)
INFO - root - 2017-12-14 03:15:52.442446: step 3520, loss = 0.70, batch loss = 0.35 (34.4 examples/sec; 0.232 sec/batch; 21h:13m:48s remains)
INFO - root - 2017-12-14 03:15:54.676552: step 3530, loss = 1.11, batch loss = 0.76 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:09s remains)
INFO - root - 2017-12-14 03:15:56.902136: step 3540, loss = 0.79, batch loss = 0.44 (36.6 examples/sec; 0.219 sec/batch; 19h:59m:37s remains)
INFO - root - 2017-12-14 03:15:59.133245: step 3550, loss = 0.66, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:06s remains)
INFO - root - 2017-12-14 03:16:01.395443: step 3560, loss = 0.72, batch loss = 0.37 (35.0 examples/sec; 0.229 sec/batch; 20h:53m:05s remains)
INFO - root - 2017-12-14 03:16:03.692412: step 3570, loss = 0.72, batch loss = 0.38 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:11s remains)
INFO - root - 2017-12-14 03:16:05.924814: step 3580, loss = 0.72, batch loss = 0.38 (35.8 examples/sec; 0.224 sec/batch; 20h:25m:40s remains)
INFO - root - 2017-12-14 03:16:08.158613: step 3590, loss = 0.81, batch loss = 0.46 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:51s remains)
INFO - root - 2017-12-14 03:16:10.446124: step 3600, loss = 0.69, batch loss = 0.35 (34.7 examples/sec; 0.231 sec/batch; 21h:05m:07s remains)
2017-12-14 03:16:10.728864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9990811 -3.4664602 -3.0988853 -2.802803 -2.1617496 -2.0580783 -2.2141232 -2.389282 -2.6306498 -3.0936761 -3.1222816 -2.8066063 -2.8524702 -2.6300452 -2.6567211][-3.8634174 -3.7331569 -3.8873334 -3.4143977 -2.8405232 -2.8699584 -3.0928788 -2.9329062 -2.7785482 -3.0318668 -3.5540643 -3.664103 -3.9236906 -3.8619978 -3.6281404][-3.1211543 -3.0569293 -3.2115307 -3.4030306 -3.344635 -3.2368393 -3.0485339 -3.132618 -3.0849004 -3.2133727 -3.4822221 -3.4201245 -3.5922475 -3.8798783 -4.3080568][-1.9160619 -1.8936672 -2.1602974 -2.3790319 -2.4329333 -2.8057663 -3.3216095 -3.1211407 -2.9529278 -3.0639076 -3.3259773 -3.5586748 -3.468256 -3.6591182 -3.7940311][-0.99285835 -0.79188132 -0.69436729 -0.94203418 -1.1231743 -1.3321735 -1.7161736 -2.0621595 -2.1941779 -2.2321129 -2.3037136 -2.2558932 -2.638114 -2.9271195 -3.2319179][-0.43754065 -0.25560033 0.10135353 0.047065496 -0.071418643 -0.19568384 -0.56574893 -0.64929366 -0.78395939 -1.3434124 -1.6836638 -1.6770933 -1.8035883 -1.9502929 -2.2016258][0.8322804 1.1436329 1.290242 1.3960986 1.6148224 1.5417614 0.89895487 0.56155062 0.30964041 0.09127295 -0.53741014 -0.93802571 -1.4210041 -1.5756627 -1.8061305][0.77446818 1.2060916 1.4263449 1.6411633 1.74349 1.8309064 1.9200704 1.4518268 1.4295452 1.4009101 0.41498089 -0.43168843 -0.75442362 -1.4194877 -1.764964][0.57886386 0.56217837 0.96047282 1.1236358 1.4563258 1.6779835 1.21521 0.67984152 0.73899913 0.69214511 0.32507324 0.13405108 -0.22392058 -0.51263034 -0.92212188][0.1445725 0.27055836 0.29598784 0.012171149 -0.012535572 0.23044157 0.20490861 -0.028685331 -0.32895827 -0.50428855 -0.89070433 -1.0429697 -1.0678816 -1.0289804 -1.0998859][-0.74205816 -0.76633322 -0.73684859 -0.86603242 -0.879142 -0.9135291 -0.6842097 -1.0118887 -1.2618425 -1.1662996 -1.0272272 -1.3713565 -1.4380345 -1.778085 -1.7029899][-2.3234725 -2.1190879 -1.9274291 -2.015244 -1.873781 -1.5511534 -1.2566667 -1.6808506 -2.011137 -1.8244377 -2.1837351 -1.9871689 -1.7379267 -1.5090563 -1.5567553][-3.0156827 -2.9624782 -3.1609662 -3.3670835 -2.702322 -1.9352418 -1.8684047 -1.9472594 -1.8329012 -1.8281151 -1.5686771 -1.9948237 -2.2371998 -1.9715881 -1.7873664][-3.7868042 -4.0685596 -4.1665869 -4.0603676 -3.7763898 -3.2340417 -2.6418841 -2.3033285 -2.0450099 -1.6901507 -1.3005972 -1.1988094 -0.99478728 -0.94728947 -1.3340251][-3.2901065 -3.434732 -3.491004 -3.7699702 -4.2290506 -3.904031 -3.3932457 -3.2215967 -2.1429443 -1.4852574 -1.3028135 -0.73014832 -0.26402009 -0.35409939 -0.21839333]]...]
INFO - root - 2017-12-14 03:16:12.972162: step 3610, loss = 0.79, batch loss = 0.44 (35.8 examples/sec; 0.224 sec/batch; 20h:25m:59s remains)
INFO - root - 2017-12-14 03:16:15.215709: step 3620, loss = 0.68, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:31s remains)
INFO - root - 2017-12-14 03:16:17.492805: step 3630, loss = 0.60, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:15s remains)
INFO - root - 2017-12-14 03:16:19.741111: step 3640, loss = 0.65, batch loss = 0.30 (34.4 examples/sec; 0.232 sec/batch; 21h:13m:28s remains)
INFO - root - 2017-12-14 03:16:22.021211: step 3650, loss = 0.72, batch loss = 0.37 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:55s remains)
INFO - root - 2017-12-14 03:16:24.281840: step 3660, loss = 0.72, batch loss = 0.37 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:59s remains)
INFO - root - 2017-12-14 03:16:26.551414: step 3670, loss = 0.71, batch loss = 0.36 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:41s remains)
INFO - root - 2017-12-14 03:16:28.826869: step 3680, loss = 0.62, batch loss = 0.27 (34.7 examples/sec; 0.230 sec/batch; 21h:02m:55s remains)
INFO - root - 2017-12-14 03:16:31.068343: step 3690, loss = 0.62, batch loss = 0.27 (36.6 examples/sec; 0.218 sec/batch; 19h:57m:00s remains)
INFO - root - 2017-12-14 03:16:33.310760: step 3700, loss = 0.69, batch loss = 0.34 (35.8 examples/sec; 0.224 sec/batch; 20h:25m:09s remains)
2017-12-14 03:16:33.565794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8010538 -3.2802482 -2.7058744 -2.0961766 -1.9110991 -1.9762676 -2.3492713 -2.0492761 -2.3214204 -2.7880454 -3.086663 -3.4843163 -3.7214251 -2.8800774 -2.7371156][-3.3475785 -2.4594123 -1.7897902 -1.8120217 -2.3900495 -3.4014783 -3.8873596 -4.321094 -4.7435608 -5.1946373 -5.7236814 -5.4710455 -5.0538707 -5.0994577 -5.4545655][-2.8536267 -2.9337611 -2.582243 -2.0953405 -2.1305244 -2.8644586 -3.3961537 -4.1783352 -4.7870674 -5.1671181 -5.4943423 -5.5338573 -5.5067272 -4.5647783 -4.2431154][-4.7759218 -3.4702334 -2.3273668 -2.5372014 -2.8724389 -2.0718846 -1.9589612 -2.2884262 -3.1370862 -4.1209745 -4.2640247 -4.1399164 -3.9987705 -3.5125532 -3.8501322][-1.7951192 -1.1057725 -0.62883806 0.051703572 0.36031032 0.0098650455 -0.800423 -1.0452714 -1.497717 -2.0486481 -2.0971148 -2.5332491 -3.1061044 -3.1925173 -3.7491808][-0.71698236 -0.4054091 0.43114233 1.3415697 1.5069718 1.8107545 1.4572618 0.87188864 0.2132535 -0.33372581 -0.82918155 -0.98353773 -1.3549359 -2.0383096 -2.9490066][-0.0764091 0.64641094 1.1020486 1.9924533 2.9102659 3.3821158 3.3424964 3.0602212 2.5110765 1.5051911 0.78351641 0.31426311 -0.36960614 -0.66700387 -1.2954396][0.31047511 1.2555883 2.4795084 2.7837639 2.9281998 3.7709637 4.0439615 3.9256787 4.2889824 3.8277617 2.866303 1.4108589 -0.11401403 -0.73311329 -1.7084607][1.9721141 2.0334074 1.9087937 3.1623921 4.3218431 4.1760054 4.0652022 4.4400229 4.6649389 3.84734 2.6194825 1.1704941 0.10859394 -1.3372416 -3.0233452][-0.079647183 0.16216612 0.64976215 0.80605292 0.66455221 1.6812608 2.6946979 2.2024355 1.5397792 1.627388 1.4054599 0.33900428 -0.83107591 -1.8653228 -2.976481][-3.9258914 -3.4536247 -2.8715918 -2.3957875 -1.5747769 -1.1976411 -0.875587 -0.98169416 -1.4119446 -1.5996388 -1.8397216 -1.6012208 -2.1862638 -2.8195558 -3.5052841][-4.8015471 -5.2541041 -5.0191035 -4.5353932 -3.5706739 -1.6940774 -0.1813556 -0.99632877 -1.8677824 -1.7019197 -1.5477197 -2.74186 -4.2290077 -5.02662 -5.4556837][-3.5458207 -3.3369241 -3.1239402 -2.9957731 -2.88624 -1.9874961 -1.3154491 -0.710019 -0.4848156 -1.6961969 -2.8781314 -2.9266291 -3.3650231 -3.9814863 -4.1358857][-3.25106 -3.4941168 -3.7105012 -3.4002705 -3.0355635 -1.9754552 -1.4316829 -1.8694061 -2.9087737 -2.2559597 -1.6789879 -3.1646433 -4.2006426 -4.2957325 -4.36011][-5.8006177 -4.9698582 -4.3572865 -4.3547363 -4.122149 -2.9970338 -2.0974474 -2.0409451 -2.4659376 -2.8342991 -3.4462991 -3.5301843 -3.6408632 -4.4328837 -4.5890408]]...]
INFO - root - 2017-12-14 03:16:35.787379: step 3710, loss = 0.63, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 20h:34m:09s remains)
INFO - root - 2017-12-14 03:16:38.048924: step 3720, loss = 0.67, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:46s remains)
INFO - root - 2017-12-14 03:16:40.342138: step 3730, loss = 0.69, batch loss = 0.35 (34.6 examples/sec; 0.232 sec/batch; 21h:08m:41s remains)
INFO - root - 2017-12-14 03:16:42.592467: step 3740, loss = 0.74, batch loss = 0.39 (35.5 examples/sec; 0.225 sec/batch; 20h:33m:53s remains)
INFO - root - 2017-12-14 03:16:44.841223: step 3750, loss = 0.69, batch loss = 0.35 (35.2 examples/sec; 0.227 sec/batch; 20h:44m:47s remains)
INFO - root - 2017-12-14 03:16:47.086579: step 3760, loss = 0.79, batch loss = 0.45 (37.1 examples/sec; 0.215 sec/batch; 19h:40m:01s remains)
INFO - root - 2017-12-14 03:16:49.363257: step 3770, loss = 0.90, batch loss = 0.55 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:16s remains)
INFO - root - 2017-12-14 03:16:51.609819: step 3780, loss = 0.77, batch loss = 0.43 (35.8 examples/sec; 0.223 sec/batch; 20h:23m:59s remains)
INFO - root - 2017-12-14 03:16:53.850020: step 3790, loss = 0.61, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:45m:37s remains)
INFO - root - 2017-12-14 03:16:56.131308: step 3800, loss = 1.00, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:58s remains)
2017-12-14 03:16:56.398749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0684323 -5.2610912 -4.7326722 -4.8055868 -4.9599967 -4.7331123 -4.523324 -5.8562884 -6.7428236 -6.5556712 -7.0444231 -7.1291103 -6.8338733 -6.5371623 -6.3224258][-4.5865059 -4.6900511 -4.8337955 -5.0081487 -4.729351 -4.6529088 -4.7811112 -5.2616916 -5.8217773 -6.7598038 -7.0223904 -6.7706614 -6.7484417 -6.6972742 -6.8398166][-5.0674896 -4.7448721 -4.5646515 -4.6857371 -4.274828 -4.09505 -4.01757 -4.6180425 -5.2473087 -5.4756875 -5.7987561 -6.2141275 -6.3605795 -6.8262577 -7.0890703][-5.2588558 -5.1904321 -5.2906795 -4.71713 -4.1147571 -4.2355418 -4.1782055 -4.2780643 -4.4920821 -4.4536805 -4.4575758 -4.6908808 -4.9878259 -5.74187 -6.3731728][-4.1936469 -4.1743412 -3.9691334 -3.873733 -3.431813 -2.8510792 -2.3452628 -2.3575778 -2.9771025 -3.132391 -3.4191232 -3.9928551 -4.3258638 -5.1411538 -5.7871556][-3.5834696 -3.9306698 -3.5600898 -2.7621667 -1.8527781 -1.7528356 -1.6545186 -1.7724204 -2.6936953 -3.3726206 -3.8091822 -4.000772 -4.11377 -4.5715737 -5.3961759][-4.5007935 -3.9416342 -3.225585 -2.5694723 -1.7201898 -0.42089319 1.0101961 0.44252551 -1.1692257 -1.8289636 -1.9991193 -2.4401007 -3.2552872 -3.7539234 -3.8418674][-3.5349717 -3.0369129 -2.4487824 -1.4756579 -0.5250119 -0.013014793 0.65283763 1.1103119 0.51603353 -0.57628632 -1.7695217 -2.5575614 -3.0878844 -4.0484157 -5.0153222][-2.4172647 -1.9583578 -1.4806108 -0.80387533 0.088903785 1.0270904 2.1890745 1.8972992 0.51751006 -0.19333875 -1.0068859 -2.7128072 -3.7382486 -4.396307 -5.2095957][-2.7623291 -2.6440082 -2.7159374 -2.2235286 -1.3968991 -1.2004414 -0.70392895 -0.75515985 -1.6295004 -1.9935164 -2.5997658 -2.7792296 -2.7764 -4.0118971 -5.2886076][-6.4751992 -5.7170253 -5.0570254 -4.851553 -4.4325223 -3.6487532 -2.8832948 -3.1368859 -3.4325523 -3.7379069 -4.1775103 -4.0857086 -4.3211751 -4.5555973 -4.9968877][-7.2589908 -7.044908 -6.9347262 -6.1019664 -5.2350936 -4.4329224 -3.577785 -3.5662236 -3.415411 -3.2443419 -3.423553 -4.0154219 -5.20484 -5.8327627 -6.174119][-5.7638092 -5.2516394 -5.314806 -5.5395889 -5.663012 -4.7092667 -3.6344337 -3.5501127 -3.7856555 -4.60507 -5.4096489 -5.1895084 -5.2900333 -5.6190367 -5.600481][-5.6029406 -5.8414717 -5.9755459 -5.8768568 -5.9838891 -5.9065175 -5.6363826 -5.131803 -4.9759 -5.1854014 -5.5285554 -6.3972049 -7.2981935 -7.3941455 -7.2195678][-5.8272223 -5.7903671 -5.6999345 -5.9980278 -6.0088191 -5.4090261 -4.7570429 -4.8872142 -5.0828309 -5.3334627 -5.5787268 -5.9101686 -6.559948 -7.0612483 -7.4930978]]...]
INFO - root - 2017-12-14 03:16:58.672824: step 3810, loss = 0.67, batch loss = 0.33 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:36s remains)
INFO - root - 2017-12-14 03:17:00.972625: step 3820, loss = 0.69, batch loss = 0.35 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:09s remains)
INFO - root - 2017-12-14 03:17:03.262664: step 3830, loss = 0.66, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:05s remains)
INFO - root - 2017-12-14 03:17:05.518544: step 3840, loss = 0.64, batch loss = 0.30 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:25s remains)
INFO - root - 2017-12-14 03:17:07.767464: step 3850, loss = 0.73, batch loss = 0.39 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:30s remains)
INFO - root - 2017-12-14 03:17:10.061041: step 3860, loss = 0.80, batch loss = 0.46 (34.3 examples/sec; 0.233 sec/batch; 21h:17m:53s remains)
INFO - root - 2017-12-14 03:17:12.319536: step 3870, loss = 0.91, batch loss = 0.57 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:41s remains)
INFO - root - 2017-12-14 03:17:14.561270: step 3880, loss = 0.79, batch loss = 0.45 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:50s remains)
INFO - root - 2017-12-14 03:17:16.789357: step 3890, loss = 0.70, batch loss = 0.36 (36.3 examples/sec; 0.221 sec/batch; 20h:08m:04s remains)
INFO - root - 2017-12-14 03:17:19.083487: step 3900, loss = 0.64, batch loss = 0.30 (34.7 examples/sec; 0.231 sec/batch; 21h:02m:36s remains)
2017-12-14 03:17:19.351536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3696547 -2.607456 -3.3802192 -3.5075738 -3.7257357 -4.118166 -4.4688511 -4.6403055 -4.6741562 -4.8507137 -4.5442152 -4.9313345 -4.9240694 -4.372725 -3.6865106][-2.5900743 -2.5944448 -2.67838 -3.1542826 -4.1149464 -4.693862 -4.9165769 -5.2815 -5.4382172 -5.6746206 -5.6334934 -5.4399929 -5.1029396 -4.981391 -4.9607534][-1.9339356 -1.7241776 -2.1713262 -2.8622785 -3.6778245 -4.2599335 -4.7401724 -5.2240348 -5.4557142 -5.4607821 -5.3595662 -5.7006426 -5.9480672 -5.7844453 -5.5423231][-2.1246963 -2.3801379 -2.4050267 -1.9396626 -2.3123767 -3.2978559 -3.8287027 -4.18801 -4.6685867 -5.1155376 -4.8055382 -4.1145859 -3.8895922 -4.1633816 -4.1447029][-0.1669383 -0.30712974 -0.97148919 -0.54118347 -0.23234797 -0.47184849 -0.91558337 -1.8411624 -2.7708285 -3.3606436 -3.4936237 -3.5390534 -3.871038 -3.441226 -3.1167574][-1.3639712 -1.2758479 -1.5464931 -1.2472477 -0.54821575 -0.45918083 -0.32379854 -0.3840394 -0.85507441 -1.4854444 -2.1241682 -2.529439 -2.573545 -2.7734489 -2.8924165][-0.29050231 -0.49966288 -0.30294597 0.0056122541 0.4337554 1.0734138 1.4706779 1.0979388 0.098656178 -0.51249552 -0.9721244 -1.652804 -2.3311472 -2.4945631 -2.1086888][-0.74470925 -1.0290697 -1.0175869 0.27528977 1.3060696 1.5114059 1.3788135 1.4050283 1.0213501 0.27191114 -0.4394021 -1.0964863 -1.802038 -2.1735806 -2.6367908][1.2097027 1.1041954 1.2615075 1.6425648 2.0758414 2.2318993 2.7922206 2.0748353 1.0655067 0.22702789 -0.42861855 -1.4173197 -1.7782609 -1.9886764 -2.4323225][-0.46463311 -1.0929303 -1.0810615 -0.21370125 0.33472085 0.2299633 0.80618763 0.57091594 -0.068022847 -0.97608978 -1.9363192 -2.6897898 -2.8914447 -3.2212079 -3.9148185][-2.6301575 -2.83085 -2.6145597 -2.5026617 -2.4333158 -2.4690275 -1.7811464 -1.8446722 -2.4825604 -2.735009 -3.1538634 -3.7466335 -4.4266815 -4.4032631 -4.3827043][-3.423944 -3.6326022 -3.445612 -3.4216769 -3.2623706 -3.1288443 -2.7123065 -2.8159115 -3.269861 -3.7604606 -4.5690827 -4.67015 -5.2218885 -5.956161 -6.3829403][-4.054904 -4.118053 -3.8711 -3.7664342 -3.6478133 -3.2018287 -2.8277609 -3.3008301 -3.9552171 -4.0406322 -4.5509081 -5.2647491 -6.0594115 -6.3892808 -6.4272966][-3.7903268 -4.3632059 -4.2333407 -4.4102759 -4.7627449 -4.382266 -3.6099961 -3.6703186 -4.1398153 -4.3950381 -4.8548841 -5.2889462 -5.67212 -6.285924 -6.6504455][-4.7238164 -4.4733858 -4.1321106 -4.2748652 -4.1225705 -4.0558338 -3.710444 -3.6740344 -3.7192669 -4.0639434 -4.6434402 -5.2827864 -5.6668348 -5.8105211 -5.5962677]]...]
INFO - root - 2017-12-14 03:17:21.606799: step 3910, loss = 0.72, batch loss = 0.38 (34.9 examples/sec; 0.229 sec/batch; 20h:54m:57s remains)
INFO - root - 2017-12-14 03:17:23.887739: step 3920, loss = 0.62, batch loss = 0.28 (36.1 examples/sec; 0.221 sec/batch; 20h:12m:56s remains)
INFO - root - 2017-12-14 03:17:26.168976: step 3930, loss = 0.66, batch loss = 0.32 (34.3 examples/sec; 0.233 sec/batch; 21h:17m:55s remains)
INFO - root - 2017-12-14 03:17:28.430871: step 3940, loss = 0.76, batch loss = 0.43 (36.1 examples/sec; 0.222 sec/batch; 20h:14m:47s remains)
INFO - root - 2017-12-14 03:17:30.677900: step 3950, loss = 0.74, batch loss = 0.40 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:52s remains)
INFO - root - 2017-12-14 03:17:32.959641: step 3960, loss = 0.67, batch loss = 0.33 (34.2 examples/sec; 0.234 sec/batch; 21h:21m:22s remains)
INFO - root - 2017-12-14 03:17:35.256619: step 3970, loss = 0.60, batch loss = 0.26 (34.2 examples/sec; 0.234 sec/batch; 21h:21m:47s remains)
INFO - root - 2017-12-14 03:17:37.506409: step 3980, loss = 0.60, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:22s remains)
INFO - root - 2017-12-14 03:17:39.748236: step 3990, loss = 0.71, batch loss = 0.37 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:22s remains)
INFO - root - 2017-12-14 03:17:41.980639: step 4000, loss = 0.82, batch loss = 0.48 (36.0 examples/sec; 0.222 sec/batch; 20h:15m:44s remains)
2017-12-14 03:17:42.276454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.011236 -2.0579534 -2.2682235 -2.8168535 -3.2806947 -2.5047882 -2.6789975 -2.4052863 -2.4924037 -2.696923 -2.8721254 -3.1252339 -3.3236704 -2.5123181 -2.0979574][-1.9467193 -2.747448 -3.1873128 -2.8376818 -2.7236869 -2.5667541 -3.1265974 -3.1454515 -3.2522659 -3.3350582 -3.5371585 -3.8493574 -3.7628121 -3.4274974 -3.3637152][-2.1128907 -2.4438415 -2.4506476 -2.7457426 -3.332345 -3.0691173 -2.8917637 -2.6587751 -3.1555123 -3.7262206 -3.6943378 -3.9980316 -4.116744 -3.920661 -3.3661728][-1.6269031 -1.633289 -1.6485777 -1.8592763 -1.908879 -1.2382123 -1.822638 -2.4838202 -2.8038423 -3.2118902 -3.3388734 -3.1065693 -2.7614195 -3.1148562 -3.0080185][0.45928931 0.14782524 -0.009013176 -0.093123794 -0.12132275 0.88634753 0.37848902 -0.38470459 -0.88507307 -1.6149025 -2.5874095 -2.9856687 -2.9754126 -2.8874998 -2.4460561][-0.83683741 -1.5029354 -0.89240015 -0.37515783 -0.53693891 0.16383338 0.37861347 0.55427909 0.6668539 0.49518847 0.071885586 -0.6722573 -0.85225797 -0.9198103 -0.69943023][-2.3820658 -2.0339246 -1.8111264 -1.6577147 -1.8908759 -0.33300591 0.69805408 1.0372205 0.64717364 0.35376954 0.8272121 1.2494662 1.320205 0.89443207 0.94980931][-2.7802525 -2.4386396 -2.2468276 -1.9477267 -1.7042514 0.27499557 0.73283291 1.6095805 2.2302125 2.3091877 1.8801126 1.9813974 2.1226671 1.7294285 1.3527551][-0.26350546 -0.39766061 -0.54879856 -0.12788939 0.19391537 1.7122762 1.83465 2.4584382 3.5439579 3.2672417 2.129523 1.1882935 0.80109382 0.67410636 0.73194075][-0.31814039 -0.491521 -0.53525305 -0.34919286 -0.70032191 0.90368104 1.0572152 1.2937582 1.7771764 1.9484911 1.3254879 0.54051208 0.17091012 -0.71581221 -1.1721126][-1.2631366 -1.1796867 -1.3364673 -1.5488353 -2.2025666 -1.1492376 -0.78389108 0.043448567 0.34400868 0.0861187 -0.35977912 -0.72230613 -1.1380546 -1.1674139 -1.2232273][-1.6823596 -1.899822 -2.3915348 -2.7426536 -2.7924533 -1.5417535 -1.2365468 -0.91862822 -0.18286908 -0.17034459 -0.4106797 -1.0101336 -1.8286877 -2.6133819 -2.8095436][-1.6470982 -1.7282552 -2.1044147 -2.5761342 -2.99118 -1.6696775 -1.1288204 -0.97790223 -0.476488 -0.42960274 -0.76769137 -1.3052486 -2.2931619 -3.0880437 -3.4580812][-2.0839803 -2.3080275 -2.526912 -2.943444 -3.2884007 -2.2893183 -1.8181274 -1.2560024 -0.76209617 -0.94961774 -1.2954519 -1.7121829 -2.3031793 -2.9490926 -3.4589448][-2.2361197 -2.5873938 -3.3482456 -3.6769056 -3.5833671 -2.4312048 -1.9983193 -1.6992637 -1.0849104 -1.1970359 -1.1411594 -1.5678596 -2.6529441 -3.1953211 -3.2826648]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-10-10
INFO - root - 2017-12-14 03:17:44.537992: step 4010, loss = 0.65, batch loss = 0.32 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:39s remains)
INFO - root - 2017-12-14 03:17:46.807114: step 4020, loss = 0.69, batch loss = 0.35 (35.0 examples/sec; 0.229 sec/batch; 20h:51m:24s remains)
INFO - root - 2017-12-14 03:17:49.101776: step 4030, loss = 0.61, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:17s remains)
INFO - root - 2017-12-14 03:17:51.377457: step 4040, loss = 0.60, batch loss = 0.27 (33.8 examples/sec; 0.237 sec/batch; 21h:36m:59s remains)
INFO - root - 2017-12-14 03:17:53.648200: step 4050, loss = 0.64, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:13s remains)
INFO - root - 2017-12-14 03:17:55.931836: step 4060, loss = 0.65, batch loss = 0.32 (34.6 examples/sec; 0.232 sec/batch; 21h:07m:24s remains)
INFO - root - 2017-12-14 03:17:58.224066: step 4070, loss = 0.86, batch loss = 0.53 (37.3 examples/sec; 0.215 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-14 03:18:00.478309: step 4080, loss = 0.71, batch loss = 0.38 (34.5 examples/sec; 0.232 sec/batch; 21h:10m:22s remains)
INFO - root - 2017-12-14 03:18:02.714384: step 4090, loss = 0.71, batch loss = 0.38 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:22s remains)
INFO - root - 2017-12-14 03:18:05.012235: step 4100, loss = 0.81, batch loss = 0.48 (35.1 examples/sec; 0.228 sec/batch; 20h:45m:44s remains)
2017-12-14 03:18:05.300316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0768256 -2.4530144 -2.6181276 -2.5445302 -2.6298311 -2.536139 -2.2336044 -2.7876551 -3.1789708 -3.3165104 -3.1570902 -3.2574878 -3.2044477 -3.2486176 -3.5462248][-2.1445231 -2.4641898 -2.3390603 -2.4290206 -2.6033759 -2.7562664 -2.8652453 -2.6596508 -2.4432726 -3.0883436 -3.2662268 -3.1537876 -2.7639887 -2.7452655 -2.9360547][-1.5392723 -2.3767939 -2.4271822 -2.529597 -2.6596189 -2.6339204 -2.4785573 -2.59162 -2.9184346 -2.7966375 -2.6798429 -2.8702164 -2.9566121 -3.0247717 -3.1278086][-2.0854125 -2.7551389 -3.0760062 -3.2091916 -3.221595 -3.1419568 -2.6469958 -2.1292932 -2.0285096 -1.9230211 -1.3234231 -0.78078532 -0.70129144 -0.5989902 -0.46740949][-0.93796134 -1.2453879 -1.5079429 -1.6097119 -1.697901 -1.7074471 -1.7338423 -1.6162394 -1.5914657 -1.5076627 -1.1441216 -1.1208386 -1.2634175 -1.2969369 -1.483644][-0.278862 -0.89919019 -0.87593675 -0.96974653 -0.83892536 -0.747897 -0.33551121 0.012207985 0.034510493 -0.45365119 -0.79859447 -1.0921664 -1.0608681 -1.1542966 -1.0637074][-0.21044469 -0.78707325 -0.66352808 -0.68735492 -0.70798612 -0.31854033 0.19834638 0.62778139 0.6052177 0.66789794 0.2627449 -0.061396956 0.22251129 0.46564651 0.38166118][0.23205113 -0.16812503 0.040666103 0.33867502 0.39914632 0.60184693 0.7639246 0.62049079 0.27852058 0.51098657 0.50294948 0.012309313 -0.24366939 -0.39607668 -0.36348355][0.82387018 0.18997025 -0.013964295 0.22613406 0.1947782 0.61332917 0.98254061 0.96533322 0.74487209 0.71298528 0.35777187 0.28619576 0.46251106 0.12696743 -0.32693172][0.016406059 -0.31522965 -0.58332491 -0.58004117 -0.60336959 -0.57793307 -0.46725726 -0.66034031 -0.9521997 -0.649814 -0.49902833 -0.45807624 -0.41545367 -0.4118185 -0.58430481][-3.0216541 -3.312892 -3.2677181 -3.3596401 -3.4094276 -3.011483 -2.2781744 -2.1147664 -1.8771582 -1.4032266 -1.1009276 -1.2328184 -1.4812424 -1.367967 -1.3277061][-3.4890106 -3.3423181 -3.2181592 -3.2497211 -3.2768469 -2.9495869 -2.3191688 -1.8335243 -1.6894017 -1.7608706 -1.7316628 -1.6917384 -1.8182846 -1.7732533 -1.7344253][-2.1202536 -2.024024 -2.2684672 -2.1690171 -2.3118048 -2.0196381 -1.4579908 -0.86636174 -0.55737281 -0.52208328 -0.60532272 -1.1800702 -1.9275351 -2.2300436 -2.2144687][-2.5961449 -2.4854918 -2.6531157 -2.6294286 -2.7817154 -2.6117535 -2.2708561 -1.947922 -1.7301403 -1.24017 -0.87953269 -0.75717485 -0.94629657 -1.6618755 -2.4283843][-2.623956 -2.4239845 -2.2380929 -2.3813915 -2.2745237 -1.8650665 -1.3306234 -1.3741955 -1.5612535 -1.7130657 -1.8668934 -1.7870169 -1.3512783 -0.81706429 -1.0497741]]...]
INFO - root - 2017-12-14 03:18:07.557982: step 4110, loss = 0.63, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:07s remains)
INFO - root - 2017-12-14 03:18:09.783981: step 4120, loss = 0.62, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 20h:05m:24s remains)
INFO - root - 2017-12-14 03:18:12.058791: step 4130, loss = 0.74, batch loss = 0.41 (34.4 examples/sec; 0.233 sec/batch; 21h:14m:02s remains)
INFO - root - 2017-12-14 03:18:14.335505: step 4140, loss = 0.60, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:00s remains)
INFO - root - 2017-12-14 03:18:16.606340: step 4150, loss = 0.67, batch loss = 0.34 (32.2 examples/sec; 0.248 sec/batch; 22h:39m:09s remains)
INFO - root - 2017-12-14 03:18:18.867483: step 4160, loss = 0.73, batch loss = 0.40 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:27s remains)
INFO - root - 2017-12-14 03:18:21.155275: step 4170, loss = 0.63, batch loss = 0.31 (33.6 examples/sec; 0.238 sec/batch; 21h:41m:39s remains)
INFO - root - 2017-12-14 03:18:23.395454: step 4180, loss = 0.61, batch loss = 0.28 (35.8 examples/sec; 0.224 sec/batch; 20h:23m:34s remains)
INFO - root - 2017-12-14 03:18:25.680481: step 4190, loss = 0.78, batch loss = 0.45 (34.4 examples/sec; 0.233 sec/batch; 21h:14m:20s remains)
INFO - root - 2017-12-14 03:18:27.916327: step 4200, loss = 0.67, batch loss = 0.34 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:03s remains)
2017-12-14 03:18:28.212891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.92550778 -1.0305529 -0.91905904 -0.64865303 -0.50407934 -1.0412072 -1.3792864 -1.7006555 -2.0260379 -2.2754805 -2.4604285 -2.7069554 -2.86057 -2.4686158 -2.0284612][-1.3757129 -1.6326107 -1.6644157 -1.3197756 -1.0293376 -0.92387664 -0.889258 -1.1882463 -1.4808481 -2.1933153 -2.9305284 -3.1889927 -3.3006151 -3.451324 -3.4045815][-2.0477011 -2.2499285 -2.3820729 -2.3042316 -2.0793567 -2.1383674 -2.1463878 -1.9469115 -2.05018 -2.2144358 -2.5222948 -3.3004842 -3.8597302 -4.2581978 -4.481184][-1.9375196 -2.2258673 -2.3285773 -2.3713078 -2.3915915 -2.5802746 -2.6136174 -3.1379666 -4.16441 -3.8263431 -3.1563492 -2.5945482 -2.5958498 -3.2125876 -3.4692848][-1.0178882 -1.0217073 -1.0890284 -1.0825198 -1.0033128 -1.3236376 -1.8001637 -2.4592702 -3.2885623 -3.5631495 -3.5278006 -3.2192473 -3.0004625 -2.9075665 -2.714236][-0.4919976 -0.7217263 -0.8361001 -0.83904684 -0.94435036 -1.7132692 -2.16164 -1.4865637 -1.3315934 -1.4651132 -2.0576866 -2.2441702 -2.0362947 -2.24089 -2.2517519][-0.58709371 -0.34120429 -0.19792485 -0.37303126 -0.70104611 -1.3832116 -1.5046569 -1.1564813 -1.0163796 -1.1642516 -1.6835167 -1.6658419 -1.3112395 -2.0574658 -2.8418088][0.34153128 0.31717825 0.23856306 0.46644306 0.73694539 0.13217497 -0.52772069 -0.59150124 -0.82412577 -0.81106234 -1.0552976 -1.9386343 -2.2088895 -2.5322857 -2.3845832][1.5080578 1.7085578 1.8498893 1.4717612 1.4354191 1.2579453 0.85273194 0.33031106 -0.10696638 -0.52356982 -1.6966347 -2.6379285 -2.5573061 -2.719574 -3.3262768][0.5716722 0.55329847 0.60896778 0.23381424 -0.0072044134 -0.65315104 -1.0880326 -1.4739296 -1.9454861 -2.1209011 -2.9402518 -3.2410583 -3.0954008 -3.55937 -4.0740156][-2.3963997 -2.3833678 -2.4617505 -2.7360878 -3.0757594 -3.2729096 -3.085072 -2.96656 -3.2510722 -3.4261103 -3.8114305 -4.3067532 -4.7984037 -4.6254053 -4.5588026][-3.9732025 -4.3912172 -4.6939874 -4.6395874 -4.6059585 -4.7020831 -4.3807392 -4.0169768 -3.8521018 -3.5135932 -3.7387395 -4.7868252 -5.1952629 -5.4420414 -5.9120064][-4.1429653 -4.5963497 -4.7773819 -4.8729267 -5.0370874 -5.1557455 -4.7410545 -4.383976 -4.4545841 -4.43705 -4.6484241 -4.6987839 -4.8800025 -5.4627056 -5.8820405][-4.4631567 -4.7508574 -4.9226656 -5.1836815 -5.3319831 -5.4433727 -5.2066479 -4.9662113 -5.1455631 -5.1009655 -5.0873971 -4.938056 -4.9422874 -5.27809 -5.5834379][-5.1146564 -4.9995041 -5.1420393 -5.1601686 -5.4252033 -5.9008746 -5.8363094 -5.2123089 -5.3820982 -5.6229444 -5.4725676 -5.59804 -5.8829794 -5.4522781 -5.2348509]]...]
INFO - root - 2017-12-14 03:18:30.473127: step 4210, loss = 0.62, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:42s remains)
INFO - root - 2017-12-14 03:18:32.725744: step 4220, loss = 0.58, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 21h:06m:01s remains)
INFO - root - 2017-12-14 03:18:34.987676: step 4230, loss = 0.59, batch loss = 0.27 (35.6 examples/sec; 0.224 sec/batch; 20h:27m:56s remains)
INFO - root - 2017-12-14 03:18:37.252366: step 4240, loss = 0.67, batch loss = 0.35 (36.3 examples/sec; 0.220 sec/batch; 20h:05m:48s remains)
INFO - root - 2017-12-14 03:18:39.532323: step 4250, loss = 0.67, batch loss = 0.35 (33.6 examples/sec; 0.238 sec/batch; 21h:41m:18s remains)
INFO - root - 2017-12-14 03:18:41.764404: step 4260, loss = 0.89, batch loss = 0.57 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:50s remains)
INFO - root - 2017-12-14 03:18:44.000468: step 4270, loss = 0.68, batch loss = 0.35 (36.8 examples/sec; 0.218 sec/batch; 19h:49m:56s remains)
INFO - root - 2017-12-14 03:18:46.261530: step 4280, loss = 0.65, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 20h:52m:52s remains)
INFO - root - 2017-12-14 03:18:48.519469: step 4290, loss = 0.68, batch loss = 0.35 (35.3 examples/sec; 0.227 sec/batch; 20h:41m:02s remains)
INFO - root - 2017-12-14 03:18:50.799398: step 4300, loss = 0.76, batch loss = 0.43 (35.5 examples/sec; 0.225 sec/batch; 20h:33m:09s remains)
2017-12-14 03:18:51.060214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9627056 -5.49623 -5.9021621 -5.9745154 -6.0397778 -6.046382 -5.9900923 -6.4169741 -6.5138388 -5.7058153 -5.0583372 -4.6487617 -4.5425658 -4.6299963 -4.61229][-3.7257962 -4.8393192 -5.5539584 -5.6308889 -5.9799175 -6.2079229 -6.5611448 -6.6144972 -6.3130636 -6.28909 -5.980278 -5.16628 -4.8924446 -4.4973316 -3.8233654][-2.7731993 -3.5159273 -4.1929827 -4.7764587 -5.1161628 -5.0518532 -5.2635856 -5.7643428 -6.3006067 -6.0543261 -5.4287271 -5.1711121 -5.0487146 -4.6501889 -4.0592546][-2.3099627 -2.5767274 -2.76336 -2.8794436 -3.1552067 -3.4865289 -3.9458148 -4.400279 -4.8058419 -5.30814 -5.1204958 -4.1765118 -3.4908633 -3.6511962 -3.6849701][-0.13960505 -0.41099048 -0.18230987 -0.028401971 -0.039159417 -0.33511996 -1.214921 -2.0500937 -2.9491794 -3.544883 -3.5133424 -3.7593565 -4.169415 -3.6774211 -2.9718893][0.44813013 0.87315869 1.2294564 1.5718429 1.9633067 1.8291972 1.1927693 0.494493 -0.48449445 -1.3638821 -1.7266188 -2.3642421 -2.9408245 -3.551163 -3.9784412][0.16307139 0.48634005 1.1319504 1.9014516 2.5900133 2.3001459 1.709933 1.2234187 0.23096871 -0.24732697 -0.22050977 -0.94452012 -1.7817918 -2.5319698 -3.0775611][-2.1938667 -1.3835239 -0.12255394 0.62272573 1.4907098 1.9270158 2.4477856 2.3264468 1.7769489 1.3503125 0.79261112 -0.0854224 -1.0932958 -2.1454828 -3.0436652][-1.150399 -1.0577142 -0.380769 0.877023 2.0108488 2.4464467 3.1351216 3.2845504 3.0216029 2.3739698 1.4459496 -0.035087705 -1.3713681 -2.3281271 -3.1371746][-0.68562233 -1.2739927 -1.2486403 -0.17456686 0.83997583 1.4476938 2.1932561 1.9862161 1.649502 1.3444192 0.4416697 -0.32187748 -1.2175102 -2.3939462 -3.4421253][-2.0263879 -2.4313939 -2.5534873 -2.0480256 -1.679005 -1.1309159 -0.2230103 -0.11556125 -0.14618826 -0.27754462 -0.22614479 -0.66719568 -1.7624042 -2.2588766 -2.6885648][-1.9391488 -2.4529161 -2.6310377 -2.358932 -1.9223715 -1.0876756 -0.34796751 -0.18542647 -0.16572213 -0.33857536 -0.71020162 -1.2295021 -2.089721 -2.6926076 -3.3360753][-1.0623486 -2.0003674 -2.451627 -2.1608796 -1.7549742 -0.88612723 -0.11827064 -0.09771204 -0.2124387 -0.563414 -0.75720561 -1.1804928 -2.2355273 -3.0334682 -3.5128012][-1.8708035 -2.6288452 -3.1261089 -3.1215532 -2.9501467 -2.353013 -1.829173 -1.7167339 -1.7732347 -2.1970308 -2.3481348 -2.3400531 -2.3346159 -2.2549326 -2.3014908][-2.41249 -3.2394414 -3.5678186 -3.5415235 -3.7799263 -3.4591708 -2.8728573 -2.3389068 -2.0157003 -2.1845713 -2.1928871 -2.7530456 -3.3507586 -3.1381371 -2.7847984]]...]
INFO - root - 2017-12-14 03:18:53.328509: step 4310, loss = 0.75, batch loss = 0.43 (33.6 examples/sec; 0.238 sec/batch; 21h:41m:32s remains)
INFO - root - 2017-12-14 03:18:55.583590: step 4320, loss = 0.68, batch loss = 0.35 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:42s remains)
INFO - root - 2017-12-14 03:18:57.825899: step 4330, loss = 0.65, batch loss = 0.32 (36.1 examples/sec; 0.222 sec/batch; 20h:11m:48s remains)
INFO - root - 2017-12-14 03:19:00.075587: step 4340, loss = 0.71, batch loss = 0.39 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:49s remains)
INFO - root - 2017-12-14 03:19:02.315125: step 4350, loss = 0.59, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:01s remains)
INFO - root - 2017-12-14 03:19:04.562345: step 4360, loss = 0.64, batch loss = 0.31 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:38s remains)
INFO - root - 2017-12-14 03:19:06.822494: step 4370, loss = 0.66, batch loss = 0.33 (36.6 examples/sec; 0.219 sec/batch; 19h:56m:26s remains)
INFO - root - 2017-12-14 03:19:09.066028: step 4380, loss = 0.60, batch loss = 0.27 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:19s remains)
INFO - root - 2017-12-14 03:19:11.341142: step 4390, loss = 0.60, batch loss = 0.28 (33.5 examples/sec; 0.239 sec/batch; 21h:47m:46s remains)
INFO - root - 2017-12-14 03:19:13.611402: step 4400, loss = 0.73, batch loss = 0.41 (35.5 examples/sec; 0.225 sec/batch; 20h:31m:19s remains)
2017-12-14 03:19:13.899843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2591839 -3.2851715 -3.9010873 -3.2547054 -2.6356812 -1.4553838 -0.52426589 -0.91187418 -1.6281112 -2.2878766 -3.8993316 -3.5730643 -2.77625 -3.2028198 -3.3587871][-0.086728215 -1.7654251 -2.9019992 -3.018225 -2.8049638 -1.0532196 0.03881681 -0.085372448 -0.4140563 -1.3385561 -2.6566439 -2.7482665 -2.8389554 -2.7473745 -2.4330637][0.53888977 -0.62625372 -1.6510468 -1.747082 -1.5345557 -0.91968167 -0.3310957 -0.50461674 -1.3598919 -1.5704067 -2.3134403 -2.5937815 -2.5474281 -2.6615031 -2.7580633][-1.388061 -2.2595992 -2.7977977 -2.6246076 -2.1648991 -1.3246959 -0.548597 -0.57838285 -0.74102986 -1.7363667 -2.8052564 -1.9643002 -1.4742165 -1.6546508 -1.6680956][-1.097724 -1.3685675 -1.3729712 -1.185802 -0.42466116 0.64757144 1.5095021 2.0685849 1.428244 0.24248827 -0.83222735 -1.055841 -1.1789497 -1.2787447 -1.2846322][-0.046989679 -0.36481082 -0.56615353 0.2142359 0.90463245 2.1103778 3.3291364 3.5480266 3.1462088 2.0798812 0.3780123 0.14024508 0.14179957 -0.21986544 -0.36599171][-0.18517375 -0.3227632 -0.12838948 0.50948679 0.9753443 2.291204 3.642704 4.1150947 3.5025029 2.784904 1.3385965 0.83659089 0.75730288 0.70443332 0.43300498][1.0030981 1.5780197 2.5413857 2.7324281 2.498147 3.6200747 4.4211354 4.1774893 3.0296745 1.871837 0.088653445 -0.66053832 -0.76879811 -1.4561779 -2.1378148][2.0309539 2.1614237 2.1270704 2.5548406 2.7926521 3.5906296 4.0815725 3.2911315 1.7614115 0.93456089 -0.9807381 -2.0181925 -2.2305052 -2.4783304 -2.7945266][0.11795175 -0.10129988 -0.067404509 -0.21794498 -0.60437286 -0.10449958 0.30417573 -0.14662278 -0.84250534 -1.5725269 -2.8916924 -3.0246127 -2.6591718 -2.6778946 -3.0469522][-3.9826446 -3.8306613 -3.0110617 -3.0953453 -3.3368845 -2.2645609 -1.2489575 -1.7120519 -2.1969869 -2.6854563 -4.0950408 -3.763782 -3.2645588 -3.4716539 -3.2056119][-3.3541117 -4.4426351 -5.3561859 -5.1611757 -4.7965026 -3.569999 -2.6926594 -2.5930877 -2.5186603 -2.8477523 -3.5921254 -3.062696 -3.1733043 -2.8868885 -2.6037612][-1.5943264 -3.1453795 -4.0456271 -4.2983279 -4.440012 -3.0963488 -2.1085603 -1.7979478 -2.0530512 -2.4108143 -2.3187406 -2.1488039 -1.7242115 -1.8884981 -3.1627755][-2.5272763 -3.4665608 -3.73833 -3.960587 -3.9318485 -2.9918656 -2.5059664 -2.6883397 -2.7623034 -3.0534539 -3.845541 -3.1790895 -2.8437386 -2.8788862 -2.8285236][-3.2793069 -3.505074 -4.0674419 -4.3999166 -4.11998 -2.9435372 -2.1244287 -1.9850719 -2.3722653 -2.8233185 -3.6889415 -3.6063282 -3.3856812 -2.3599458 -2.039032]]...]
INFO - root - 2017-12-14 03:19:16.196313: step 4410, loss = 0.80, batch loss = 0.48 (34.5 examples/sec; 0.232 sec/batch; 21h:08m:10s remains)
INFO - root - 2017-12-14 03:19:18.491142: step 4420, loss = 0.61, batch loss = 0.28 (35.5 examples/sec; 0.225 sec/batch; 20h:32m:22s remains)
INFO - root - 2017-12-14 03:19:20.714725: step 4430, loss = 0.64, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:10s remains)
INFO - root - 2017-12-14 03:19:23.000030: step 4440, loss = 0.61, batch loss = 0.28 (35.2 examples/sec; 0.227 sec/batch; 20h:43m:13s remains)
INFO - root - 2017-12-14 03:19:25.271301: step 4450, loss = 0.64, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 20h:52m:49s remains)
INFO - root - 2017-12-14 03:19:27.557709: step 4460, loss = 0.66, batch loss = 0.34 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:40s remains)
INFO - root - 2017-12-14 03:19:29.795125: step 4470, loss = 0.63, batch loss = 0.31 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:39s remains)
INFO - root - 2017-12-14 03:19:32.087458: step 4480, loss = 0.62, batch loss = 0.30 (34.4 examples/sec; 0.232 sec/batch; 21h:10m:36s remains)
INFO - root - 2017-12-14 03:19:34.371328: step 4490, loss = 0.62, batch loss = 0.30 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:13s remains)
INFO - root - 2017-12-14 03:19:36.633184: step 4500, loss = 0.60, batch loss = 0.28 (36.1 examples/sec; 0.222 sec/batch; 20h:12m:12s remains)
2017-12-14 03:19:36.915275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.745584 -5.4815598 -4.5670371 -3.1955161 -2.4427831 -2.9834297 -2.683466 -2.8382866 -3.0119612 -2.6750062 -3.6494954 -4.7610774 -5.069315 -6.1509871 -6.3517079][-3.9835157 -3.9865623 -3.3290641 -3.4442425 -3.1582146 -2.8113265 -2.7008171 -3.1011698 -2.9099183 -3.0243559 -3.4699278 -3.3085456 -4.4845657 -5.3430181 -5.3175335][-3.1478324 -3.4635096 -3.6844358 -2.5465441 -2.1444848 -2.5536115 -2.9273152 -3.3063617 -2.6632495 -3.3332918 -3.1843076 -3.0430765 -3.3554258 -3.246521 -4.1842408][-2.2705233 -2.0469143 -2.0220435 -1.9241115 -1.630991 -1.074439 -0.56598079 -1.0960598 -2.2855554 -2.7991722 -2.5604439 -2.409533 -2.5587995 -3.5783148 -3.4759579][-1.6797763 -1.97375 -1.5552695 -0.96685338 0.12104714 0.44981754 0.517944 0.17205036 -1.0427034 -2.1170065 -2.4843659 -2.1551509 -2.382756 -3.0624142 -3.4859729][-0.082407475 -0.41423154 -0.74004424 0.17074454 1.019323 1.168336 1.5835766 1.2309211 0.6166507 -1.2467183 -2.4340086 -2.9812839 -3.1566441 -3.0093069 -3.3420267][-1.6445328 -1.1515 0.10837162 0.18411124 0.98667061 2.2470751 2.7443628 1.9917928 0.69862759 -0.095546126 -1.1880583 -2.1004894 -2.9834828 -3.4150977 -3.9319625][-2.4383347 -1.9680207 -1.2436624 0.683277 2.4107227 2.9017377 2.2722683 2.2905312 1.6438094 -0.22520542 -1.8673474 -2.8506262 -2.1235104 -2.7175884 -3.2393653][-0.86582947 -0.71572709 -0.68382275 0.69721186 1.8879906 2.3673391 2.9340687 2.0463881 0.70277059 0.14352715 -1.205874 -2.3024719 -3.290987 -3.9678349 -4.4107332][-1.8017681 -2.2727973 -1.6483588 -0.49394441 -0.15738142 0.13866031 1.6345495 1.4074222 -0.26875174 -1.83444 -2.1044245 -1.4432602 -1.9623051 -2.8480563 -3.5980587][-3.4133792 -3.5148587 -3.0237684 -2.4180615 -1.2444139 -0.24303746 -0.02763474 -0.91748977 -1.2840755 -1.6979492 -1.7104671 -1.9029212 -3.4050479 -3.5275927 -3.374423][-3.9055724 -3.7285757 -3.605967 -3.3752828 -2.7888823 -1.3698446 0.00092113018 -0.26628149 -1.0892556 -1.9739093 -2.7615957 -3.9347348 -4.6425738 -4.4296937 -5.0581169][-2.9306517 -3.5186529 -3.9160161 -3.3143897 -2.9479978 -2.5671103 -1.7704642 -1.047174 -1.3363874 -1.7297424 -2.3692808 -3.9678738 -4.4175835 -4.616394 -4.6925077][-3.2560682 -2.9745727 -3.0309966 -3.3433189 -3.085525 -3.0720549 -2.6224966 -2.2516317 -1.7514797 -2.4723651 -2.9786091 -3.2151666 -4.1005573 -3.929987 -2.886667][-2.2678928 -1.9360491 -2.3189201 -2.1965089 -1.6640666 -1.3454092 -1.3053331 -1.4114227 -1.6546373 -2.8473997 -4.1933241 -4.0587234 -4.1516309 -3.6416242 -3.2240617]]...]
INFO - root - 2017-12-14 03:19:39.179922: step 4510, loss = 0.67, batch loss = 0.36 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:31s remains)
INFO - root - 2017-12-14 03:19:41.468097: step 4520, loss = 0.72, batch loss = 0.41 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:59s remains)
INFO - root - 2017-12-14 03:19:43.706891: step 4530, loss = 0.70, batch loss = 0.38 (35.7 examples/sec; 0.224 sec/batch; 20h:25m:38s remains)
INFO - root - 2017-12-14 03:19:45.978028: step 4540, loss = 0.62, batch loss = 0.31 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:32s remains)
INFO - root - 2017-12-14 03:19:48.232753: step 4550, loss = 0.98, batch loss = 0.66 (36.2 examples/sec; 0.221 sec/batch; 20h:07m:51s remains)
INFO - root - 2017-12-14 03:19:50.461568: step 4560, loss = 0.62, batch loss = 0.30 (36.0 examples/sec; 0.222 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-14 03:19:52.725764: step 4570, loss = 0.65, batch loss = 0.33 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:44s remains)
INFO - root - 2017-12-14 03:19:54.988400: step 4580, loss = 0.67, batch loss = 0.36 (35.8 examples/sec; 0.224 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-14 03:19:57.238409: step 4590, loss = 0.69, batch loss = 0.37 (34.2 examples/sec; 0.234 sec/batch; 21h:17m:09s remains)
INFO - root - 2017-12-14 03:19:59.500100: step 4600, loss = 0.59, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:20s remains)
2017-12-14 03:19:59.784851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2386551 -4.4975157 -4.7281518 -4.6246877 -4.3390722 -3.3698869 -2.420047 -2.7396178 -3.5009837 -4.6217737 -6.1920567 -6.1153045 -6.2061772 -6.2908845 -5.9849515][-5.5651441 -5.1265025 -4.9846668 -4.50186 -3.9122906 -2.8536477 -1.4992959 -1.6108218 -1.6970134 -3.0341368 -4.3525424 -4.513073 -5.33569 -5.6255708 -5.1237283][-3.8159561 -3.3702736 -3.8974009 -3.9007783 -3.7067862 -2.5270286 -1.3687804 -1.3503947 -1.3932636 -1.9001313 -2.8470075 -3.3097663 -3.7573605 -4.4633918 -4.33336][-4.9420395 -3.8369575 -3.2416582 -2.6952012 -2.6950164 -2.4189706 -1.859036 -0.84547687 -0.74180543 -1.1335922 -1.4357727 -0.93607545 -1.5939573 -2.7052898 -2.2196896][-3.1022544 -1.2766985 0.022103429 0.23278177 0.4543401 0.94712579 1.8000692 1.9943365 0.758664 0.19935429 -1.1203339 -1.6777368 -2.2214768 -2.6214275 -2.2078555][-0.02882731 0.73599422 1.3693186 2.5972338 3.0882797 3.5975084 4.25143 3.8674827 2.9885588 1.6732532 -0.1519469 -1.0527594 -1.3972229 -2.2686136 -2.9664059][1.13014 2.0412326 2.6269403 3.253448 3.6645555 4.5339684 5.7407613 5.8594794 5.0345678 3.1910372 1.5347253 0.82656682 0.18965447 -0.28432238 -0.86866987][1.7416097 2.81148 3.931953 4.2617912 4.3726969 5.1521478 6.6300945 6.8892736 6.1506605 4.4607224 2.2653394 0.75163853 -0.5258621 -1.3968756 -1.879751][1.6278869 2.6139288 3.7231956 4.2937 4.7710056 5.3232341 5.9227471 5.4912281 4.2079997 2.6743784 0.21534741 -1.8628221 -3.1417336 -4.2429457 -4.5728059][-1.9277378 -1.3083532 -0.7105509 0.090225816 1.0435125 1.8242317 2.7727423 2.6616216 1.2905053 0.34188759 -1.3999735 -2.9941492 -3.7526855 -4.42619 -5.7764721][-7.3044944 -6.2880211 -4.9826841 -4.4947495 -3.5817418 -2.147398 -0.49583888 -0.4036212 -1.7260737 -2.1853044 -3.727787 -5.1525416 -5.6048412 -5.6770544 -5.4092207][-7.4981494 -7.01528 -6.4042382 -5.5015979 -4.0767436 -2.5220618 -0.95546424 -0.94043267 -2.0363219 -3.1055422 -4.7913947 -5.957078 -7.0590186 -6.9675064 -6.8955193][-5.59763 -5.1356578 -4.73465 -4.2838535 -3.7505751 -2.2910492 -0.82731104 -0.9210664 -2.1213014 -3.6909349 -5.3682232 -6.6234708 -7.1139064 -6.4187827 -6.2962966][-4.7922773 -4.3261805 -4.1840768 -3.7358909 -3.2978954 -2.7334466 -2.0873597 -2.2259803 -2.8423221 -3.9826436 -5.4151549 -6.2937827 -6.7351508 -6.7836905 -6.5356855][-4.3345881 -3.10004 -2.3399045 -2.577477 -2.5414224 -1.6663722 -0.69401562 -1.6835533 -3.0540442 -3.9632738 -4.8683782 -5.5525918 -5.8334708 -5.0895257 -4.4827409]]...]
INFO - root - 2017-12-14 03:20:02.029672: step 4610, loss = 0.59, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:47s remains)
INFO - root - 2017-12-14 03:20:04.286526: step 4620, loss = 0.61, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 21h:25m:18s remains)
INFO - root - 2017-12-14 03:20:06.547423: step 4630, loss = 0.65, batch loss = 0.34 (37.5 examples/sec; 0.213 sec/batch; 19h:26m:33s remains)
INFO - root - 2017-12-14 03:20:08.804965: step 4640, loss = 0.64, batch loss = 0.32 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:53s remains)
INFO - root - 2017-12-14 03:20:11.061843: step 4650, loss = 0.63, batch loss = 0.32 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:07s remains)
INFO - root - 2017-12-14 03:20:13.362176: step 4660, loss = 0.62, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 20h:13m:07s remains)
INFO - root - 2017-12-14 03:20:15.608673: step 4670, loss = 0.58, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 20h:24m:54s remains)
INFO - root - 2017-12-14 03:20:17.851494: step 4680, loss = 0.59, batch loss = 0.28 (36.4 examples/sec; 0.220 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-14 03:20:20.097703: step 4690, loss = 0.87, batch loss = 0.56 (35.0 examples/sec; 0.228 sec/batch; 20h:48m:09s remains)
INFO - root - 2017-12-14 03:20:22.348977: step 4700, loss = 0.58, batch loss = 0.27 (35.8 examples/sec; 0.224 sec/batch; 20h:21m:18s remains)
2017-12-14 03:20:22.608784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.17414 -2.2499628 -3.130301 -3.9296012 -5.55503 -6.3629808 -6.001812 -5.9500065 -6.09575 -5.2571483 -3.4507933 -2.7909575 -1.6287552 -1.0583227 -0.95166564][-3.2879567 -3.8741889 -4.6428232 -5.0589848 -5.3430891 -5.660162 -6.2529726 -6.6833706 -6.3143349 -6.3873348 -6.003149 -4.47279 -2.3643856 -1.3377199 -0.53711343][-2.7696264 -3.3659205 -4.3589659 -4.6747017 -5.5949359 -5.8866243 -5.9742565 -6.4670095 -6.460887 -5.55952 -4.3098125 -4.1521711 -3.5569224 -2.7748787 -2.1083987][-1.7876318 -1.4356053 -1.6803305 -2.33263 -2.9598804 -3.3407669 -3.589889 -3.9139018 -4.5977712 -4.6625233 -3.4880056 -2.7635417 -1.8346748 -1.2686739 -1.5923932][-1.0107374 -0.54504454 -0.3679961 -0.25720096 -0.30461824 -0.78371596 -1.6926948 -1.6408138 -1.3017828 -1.5760305 -1.676324 -1.0913363 -0.40948975 -0.23398352 -0.25003743][0.086392164 0.69110513 0.3232615 0.099824667 -0.01616919 -0.13634014 -0.12410069 -0.13793731 -0.23434258 -0.26247442 0.14997649 0.12927914 0.15645385 0.05802536 -0.13015413][-1.7514912 -0.96057224 -0.1787945 -0.02025032 -0.24612951 -0.41939914 0.05262351 0.63034582 1.8800561 2.6355674 2.8464806 2.5527847 2.3859317 2.0131533 1.4097731][-1.1512041 -0.26576865 -0.026337862 -0.18536949 0.013256311 0.74131274 1.3265426 1.6582713 2.5842521 3.1295178 3.0820134 3.4076798 3.3503702 2.5498955 2.154845][-1.6641126 -1.5507306 -1.9462119 -1.8857508 -1.6075301 -1.7637391 -1.0799456 -0.022472382 1.0313654 1.842535 2.5036209 2.9405448 2.541425 1.7446442 1.5522413][-2.9833469 -2.7568767 -2.7410889 -2.8904421 -2.9805291 -3.3744128 -3.3702588 -2.9059358 -1.7610506 -0.85357535 0.019551039 1.1384704 1.6898739 1.5100338 1.3607886][-3.3987808 -3.6144853 -3.7527051 -3.5389509 -3.4809844 -3.2776825 -2.6032324 -2.6559756 -2.2805135 -1.2024703 -0.74575496 -0.36839783 -0.43925083 -0.092725635 0.64588094][-4.8883276 -5.1939135 -4.8927941 -4.7067194 -4.6158319 -4.538579 -4.1625748 -3.2545266 -1.8183972 -1.4259405 -1.684045 -1.5733421 -1.4209182 -1.3252032 -1.0250239][-3.6033831 -4.089973 -4.9086947 -4.4382963 -3.6463006 -3.7089112 -3.8777764 -3.7037568 -3.1524274 -3.0773318 -3.2563207 -2.6742854 -1.8752145 -1.6429157 -1.3152773][-2.5481832 -3.1516452 -3.9471562 -4.2699718 -4.25823 -3.6114557 -2.8158317 -2.4987965 -2.2797384 -2.3709018 -1.8951578 -2.0601106 -2.0890198 -2.3215346 -2.174453][-3.8122206 -3.4603214 -3.381453 -3.7943125 -3.9500818 -3.8486161 -3.7550242 -3.5369251 -2.7612631 -2.7392266 -3.0439632 -3.0779071 -2.483326 -2.9198148 -3.1323075]]...]
INFO - root - 2017-12-14 03:20:24.890505: step 4710, loss = 0.65, batch loss = 0.33 (34.7 examples/sec; 0.231 sec/batch; 21h:01m:09s remains)
INFO - root - 2017-12-14 03:20:27.118078: step 4720, loss = 0.63, batch loss = 0.31 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:36s remains)
INFO - root - 2017-12-14 03:20:29.345193: step 4730, loss = 0.60, batch loss = 0.29 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:38s remains)
INFO - root - 2017-12-14 03:20:31.599216: step 4740, loss = 0.54, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 20h:14m:14s remains)
INFO - root - 2017-12-14 03:20:33.898229: step 4750, loss = 0.62, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 20h:25m:26s remains)
INFO - root - 2017-12-14 03:20:36.129218: step 4760, loss = 0.78, batch loss = 0.47 (34.4 examples/sec; 0.232 sec/batch; 21h:08m:40s remains)
INFO - root - 2017-12-14 03:20:38.387429: step 4770, loss = 0.59, batch loss = 0.28 (36.1 examples/sec; 0.222 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-14 03:20:40.666403: step 4780, loss = 0.61, batch loss = 0.30 (35.5 examples/sec; 0.225 sec/batch; 20h:31m:02s remains)
INFO - root - 2017-12-14 03:20:42.956002: step 4790, loss = 0.58, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:54s remains)
INFO - root - 2017-12-14 03:20:45.209949: step 4800, loss = 0.69, batch loss = 0.38 (34.7 examples/sec; 0.231 sec/batch; 21h:00m:51s remains)
2017-12-14 03:20:45.500557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.176187 -3.2140851 -3.156693 -3.3305616 -3.6101198 -3.8284972 -3.7306933 -3.4540648 -3.1460505 -3.6327424 -3.9968014 -3.8900747 -3.5855668 -3.4419513 -3.4883633][-2.3883679 -3.1757326 -3.7263288 -3.5522308 -3.4765503 -3.0422132 -2.8282557 -3.1320195 -3.9087162 -4.0453739 -4.0990052 -3.79 -3.5050321 -3.6267457 -3.7453337][-2.7287979 -2.8073943 -2.8514986 -3.4287052 -3.7611759 -3.3054743 -2.7835107 -2.8663905 -3.223654 -3.8546662 -4.7773623 -4.8159666 -4.4190493 -3.7052798 -3.1371346][-2.3601003 -2.2657163 -2.3907337 -2.2673092 -2.2706797 -2.492579 -2.4133236 -2.2838764 -2.0218575 -2.7722926 -3.6604581 -3.7416296 -4.0346446 -3.8074408 -2.6855192][0.34822774 0.16852379 -0.34008956 -0.55901 -0.73390245 -0.40875435 0.05069685 -0.47663534 -1.0995173 -1.6166841 -2.5929744 -3.3849859 -3.5304689 -3.4940763 -3.2193418][0.06280303 0.23035026 0.43920803 0.40701771 0.30031919 0.34551835 0.79992986 0.72048163 -0.048780322 -0.71954286 -2.0023801 -2.4846694 -2.6961637 -2.8308938 -2.721261][0.028272867 -0.18429148 -0.028595805 0.19750118 0.18937016 0.78895617 1.7686834 1.4822376 0.80576825 0.38232613 -0.65509129 -1.4551396 -1.8238866 -1.6229506 -1.8199733][-0.14923263 0.076271057 0.33920503 0.80391121 0.96822572 0.99691224 1.8832483 2.2798166 2.042563 1.420841 0.40364718 -0.44520462 -0.8739208 -1.1688154 -1.2770183][0.7240417 0.51519895 0.37745523 0.75882387 0.80908751 0.82714224 1.4517522 1.5905452 1.5320947 1.1606097 0.24061966 -0.39585531 -0.47567582 -0.39119065 0.080594063][-0.42519939 -0.24849069 0.04660964 -0.24267983 -0.82719994 -0.53568518 0.26188397 0.26941991 -0.34870374 -0.65684187 -1.0340793 -0.84644306 -0.64070594 -0.60024011 -0.67632461][-1.6284779 -2.1776047 -2.1755111 -2.2859139 -2.508714 -2.194752 -1.5598489 -1.4935837 -1.7340322 -1.6599498 -1.7251264 -2.1050243 -2.3427851 -1.1810713 -0.45144427][-1.2943279 -1.6577934 -1.9605191 -2.2881141 -2.1950028 -2.3436575 -2.0467281 -1.7332267 -1.6183425 -1.5462984 -1.8049874 -2.2094526 -2.5838878 -2.6513991 -1.947813][-1.7150229 -1.3879331 -1.5712819 -1.9731615 -2.0141089 -2.0761051 -1.3824792 -1.2311087 -1.0096947 -1.3190846 -1.9448296 -2.4388306 -2.8822362 -2.4507782 -1.7878999][-2.0861402 -2.1229157 -2.1566758 -2.1130342 -2.1303885 -2.0470536 -1.6203358 -1.348528 -1.2399333 -1.2457325 -1.4679668 -2.4814074 -3.5579824 -3.4699121 -2.904434][-2.1647654 -1.7137939 -1.3255877 -1.3783507 -1.2600336 -0.80839026 -0.17307508 -0.4607023 -0.88797891 -0.58761275 -0.78223395 -1.5384904 -2.3371596 -2.4164538 -2.56601]]...]
INFO - root - 2017-12-14 03:20:47.761955: step 4810, loss = 0.54, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:47s remains)
INFO - root - 2017-12-14 03:20:50.037372: step 4820, loss = 0.56, batch loss = 0.25 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:10s remains)
INFO - root - 2017-12-14 03:20:52.313689: step 4830, loss = 0.56, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:38m:24s remains)
INFO - root - 2017-12-14 03:20:54.602720: step 4840, loss = 0.58, batch loss = 0.27 (34.3 examples/sec; 0.233 sec/batch; 21h:12m:44s remains)
INFO - root - 2017-12-14 03:20:56.873490: step 4850, loss = 0.59, batch loss = 0.28 (35.5 examples/sec; 0.225 sec/batch; 20h:29m:40s remains)
INFO - root - 2017-12-14 03:20:59.132882: step 4860, loss = 0.62, batch loss = 0.31 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-14 03:21:01.381681: step 4870, loss = 1.07, batch loss = 0.77 (36.5 examples/sec; 0.219 sec/batch; 19h:55m:47s remains)
