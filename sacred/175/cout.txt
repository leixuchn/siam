INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "175"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 06:43:48.006590: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:43:48.006628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:43:48.006636: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:43:48.006660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:43:48.006667: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-09 06:43:54.576987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-09 06:43:54.577026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 06:43:54.577034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 06:43:54.577205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 06:44:18.775214: step 0, loss = 0.90, batch loss = 0.69 (0.5 examples/sec; 15.589 sec/batch; 1439h:49m:23s remains)
2017-12-09 06:44:19.773749: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012079893 0.00012879766 0.00013620502 0.00014520508 0.00015258408 0.00015658353 0.00015979109 0.00016009084 0.00015771485 0.00015581418 0.00015079386 0.00014420376 0.00013933 0.00013472921 0.00013009182][0.00012282473 0.00013522553 0.00014811571 0.00016053929 0.00016972749 0.00017538255 0.00018110829 0.00018330441 0.00018065827 0.00017828084 0.000170696 0.00015991471 0.00015139545 0.00014378769 0.00013517428][0.00012722886 0.00014091205 0.00015587411 0.00017278362 0.00018223638 0.00019175955 0.00020474104 0.0002146263 0.00021707121 0.00021462765 0.00020281182 0.00018588189 0.00016914353 0.0001532738 0.00013890247][0.00013142996 0.00014524437 0.00016133949 0.00017915074 0.00019336183 0.0002112346 0.00023626337 0.00025697862 0.00026503272 0.00026025754 0.00024372493 0.00021697563 0.00018977992 0.00016467918 0.00014446542][0.00013570402 0.00015005603 0.00016843203 0.00018814059 0.00021023152 0.00024078216 0.00028055889 0.00031533121 0.00032624698 0.00031471098 0.00028562505 0.00024686143 0.00020913185 0.00017466924 0.00014897311][0.00013808747 0.00015264726 0.00017070909 0.00019359474 0.00022279097 0.00026838685 0.00032586142 0.0003703266 0.00038718857 0.00036130907 0.00031395006 0.00026217641 0.00021711437 0.00017843337 0.00015085464][0.00013925997 0.00015280099 0.0001695977 0.00019265128 0.00023280215 0.0002861337 0.00035906237 0.00042896764 0.00043197419 0.00037171139 0.00030953722 0.00025417126 0.00020877835 0.0001743382 0.00014986392][0.00013828745 0.00015041475 0.00016525162 0.00018957097 0.00023005526 0.00027587826 0.00034130379 0.00039563741 0.00037817121 0.000321121 0.00026977586 0.00022485809 0.0001906407 0.00016561097 0.00014625728][0.00013559601 0.00014604337 0.00016044719 0.00018213918 0.0002137351 0.00024554814 0.00028462865 0.00030085139 0.00027947416 0.00024894415 0.0002197772 0.00019414461 0.00017217621 0.00015567173 0.00014260689][0.00013191432 0.00014030174 0.00015395327 0.00017571927 0.00019663628 0.00021535675 0.00023134207 0.00023226433 0.00021737587 0.00019802118 0.00018404775 0.00017134679 0.00015869338 0.00014984755 0.0001407294][0.00012884682 0.00013440118 0.00014504579 0.00016258653 0.00017639897 0.00018504918 0.00018736208 0.00018596025 0.00017873049 0.00016658478 0.00015992745 0.00015394264 0.00014724508 0.00014351496 0.00013767002][0.00013030095 0.00013434181 0.00013984606 0.00015005881 0.00015720424 0.00016187299 0.000164495 0.00016561009 0.00015977392 0.0001504594 0.00014640823 0.00014346995 0.00013999977 0.00013860072 0.00013466671][0.00013180525 0.00013343028 0.00013586013 0.00014263764 0.00014699135 0.00015145238 0.00015595929 0.00015839175 0.00015477082 0.00014610367 0.0001429203 0.00014014746 0.00013807882 0.00013577861 0.00013323702][0.0001352793 0.00013565291 0.00013884935 0.00014458157 0.00014726828 0.00014979012 0.00015113356 0.00015262941 0.00015062769 0.00014539696 0.00014351134 0.0001404518 0.00013714007 0.00013465619 0.00013262099][0.00013505002 0.00013685433 0.00014022396 0.000143044 0.00014409762 0.00014489359 0.00014771811 0.00015015739 0.00014800583 0.00014477674 0.00014516334 0.00014350716 0.00013932119 0.00013649173 0.00013432272]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 06:44:28.470904: step 10, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:59m:09s remains)
INFO - root - 2017-12-09 06:44:34.949401: step 20, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:29m:22s remains)
INFO - root - 2017-12-09 06:44:41.361552: step 30, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:31m:23s remains)
INFO - root - 2017-12-09 06:44:47.738654: step 40, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:51m:37s remains)
INFO - root - 2017-12-09 06:44:54.049895: step 50, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:37m:26s remains)
INFO - root - 2017-12-09 06:45:00.493420: step 60, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:59m:46s remains)
INFO - root - 2017-12-09 06:45:06.797095: step 70, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:17m:46s remains)
INFO - root - 2017-12-09 06:45:13.060016: step 80, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:59m:29s remains)
INFO - root - 2017-12-09 06:45:19.384982: step 90, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.620 sec/batch; 57h:14m:21s remains)
INFO - root - 2017-12-09 06:45:25.655753: step 100, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:40m:07s remains)
2017-12-09 06:45:26.390529: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00022911548 0.00022824974 0.0002288113 0.00023052287 0.00023494985 0.00024571165 0.00026261297 0.00027613636 0.00028964039 0.00030919685 0.00030828375 0.00029264309 0.00025595864 0.00022194699 0.0001979216][0.0002354383 0.00023521332 0.00023695838 0.00023990648 0.00024416953 0.00025289238 0.00026558278 0.00027733078 0.00029192321 0.000313136 0.00031404433 0.0002978337 0.0002606186 0.00022743204 0.00020199727][0.00023873321 0.00024059997 0.00024383575 0.00024987993 0.00025304867 0.00025924065 0.000269036 0.0002777015 0.00029243637 0.00031170854 0.00031175237 0.0002956551 0.00026065172 0.00022965718 0.00020351319][0.00024468539 0.00024974684 0.00025358953 0.00026002678 0.00026061689 0.00026554969 0.00027291931 0.00027896895 0.0002907188 0.00030896172 0.00031231079 0.00029914393 0.00026456741 0.00023173948 0.00020291517][0.000255653 0.00026471945 0.00027031309 0.00027731617 0.00027609683 0.00028066032 0.0002843609 0.00028388883 0.00029082788 0.0003079235 0.00031307785 0.00030264529 0.00026964527 0.00023867517 0.00020810243][0.000272704 0.00028082365 0.00028113462 0.00028856524 0.00028588306 0.00028989441 0.00029123636 0.00029039319 0.00030023852 0.00031387116 0.00031366124 0.00030087476 0.00027219573 0.00024549337 0.00022003544][0.00028504987 0.00028966865 0.00028644977 0.0002860839 0.00028225145 0.00028725085 0.00029292895 0.00030131044 0.00031122996 0.00031851095 0.00031140845 0.00029706358 0.00027169837 0.00025484161 0.00023609224][0.00028509056 0.0002917551 0.00028881474 0.00028336945 0.00027349539 0.00027592698 0.00028603253 0.00030095066 0.00031764226 0.00031958171 0.00031131358 0.00029464628 0.00027642024 0.00026103749 0.00024660796][0.00028860202 0.00029775532 0.00029505277 0.00028945459 0.000274786 0.00027222614 0.00028196015 0.00029985013 0.00032062814 0.00032937652 0.00033469172 0.00032641628 0.00030878841 0.00028792204 0.00026472451][0.00028288891 0.00029159585 0.00029103787 0.00028655343 0.00027563222 0.0002765457 0.00028401118 0.00029161133 0.00030430342 0.00031518462 0.00033230014 0.00034517338 0.00034166104 0.00033081148 0.00030279998][0.00027681261 0.00028135293 0.00028021962 0.00027709265 0.00027047322 0.00026891311 0.00027090259 0.00027142785 0.00027435357 0.00027504942 0.00029022677 0.00031399837 0.00033072766 0.0003473167 0.00033377187][0.00027089575 0.00026995313 0.00026541331 0.00026428179 0.0002590598 0.00025697303 0.00025718959 0.00025870587 0.00025713263 0.00025452205 0.00025674459 0.00027085879 0.00028750356 0.00031272942 0.00031889728][0.00025663542 0.00025454708 0.00024895149 0.00024343954 0.00024021511 0.0002412013 0.00024741006 0.00026283646 0.00026950354 0.0002592534 0.00024967018 0.0002460964 0.00024908831 0.00026519003 0.00027639087][0.00023532435 0.00023375616 0.0002292946 0.00022364782 0.00022191659 0.000224392 0.00023437951 0.00025288991 0.00027186194 0.00027012848 0.00026138566 0.00025455313 0.00024779 0.0002530725 0.00025857051][0.00020000333 0.00019620611 0.00019209615 0.00019019653 0.00019365965 0.0001943137 0.00020349154 0.00022657425 0.00025043832 0.00026021228 0.00026384139 0.00026141526 0.00025330984 0.0002497129 0.0002454099]]...]
INFO - root - 2017-12-09 06:45:32.817662: step 110, loss = 0.90, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:16m:33s remains)
INFO - root - 2017-12-09 06:45:39.150572: step 120, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:34m:19s remains)
INFO - root - 2017-12-09 06:45:45.568243: step 130, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:34m:11s remains)
INFO - root - 2017-12-09 06:45:52.019845: step 140, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 56h:01m:41s remains)
INFO - root - 2017-12-09 06:45:58.433395: step 150, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.632 sec/batch; 58h:21m:34s remains)
INFO - root - 2017-12-09 06:46:05.007399: step 160, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 62h:04m:47s remains)
INFO - root - 2017-12-09 06:46:11.396205: step 170, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:44m:25s remains)
INFO - root - 2017-12-09 06:46:17.632903: step 180, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 57h:25m:40s remains)
INFO - root - 2017-12-09 06:46:24.039400: step 190, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:17m:42s remains)
INFO - root - 2017-12-09 06:46:30.370789: step 200, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:57m:00s remains)
2017-12-09 06:46:31.151467: I tensorflow/core/kernels/logging_ops.cc:79] [[[7.9141493e-05 8.0327729e-05 8.0095073e-05 8.0247417e-05 8.059579e-05 8.0924816e-05 8.1432976e-05 8.1507518e-05 8.1562743e-05 8.1631137e-05 8.151638e-05 8.1085222e-05 8.0482176e-05 7.9890277e-05 7.9529891e-05][8.1988292e-05 8.3148916e-05 8.3115621e-05 8.3362582e-05 8.3957835e-05 8.46305e-05 8.5470252e-05 8.6069958e-05 8.6206383e-05 8.6509615e-05 8.5915286e-05 8.4519212e-05 8.2900333e-05 8.1341052e-05 8.0353129e-05][8.34556e-05 8.5063177e-05 8.5362379e-05 8.6606204e-05 8.8361521e-05 9.0318565e-05 9.2092465e-05 9.3273469e-05 9.3756425e-05 9.3665607e-05 9.1981507e-05 8.9104949e-05 8.6036227e-05 8.3236053e-05 8.1331433e-05][8.4163963e-05 8.5806532e-05 8.6663342e-05 8.8664136e-05 9.1373215e-05 9.4247436e-05 9.6927215e-05 9.8742581e-05 0.00010102116 0.00010184669 9.9535981e-05 9.5682255e-05 9.0871057e-05 8.6506167e-05 8.3092724e-05][8.3828148e-05 8.4814274e-05 8.60233e-05 8.8693596e-05 9.2033995e-05 9.5739284e-05 0.0001002084 0.00010438545 0.0001086433 0.00010985057 0.00010690698 0.0001015953 9.5239731e-05 8.9583205e-05 8.4727966e-05][8.39257e-05 8.50473e-05 8.6380962e-05 8.9219451e-05 9.3271665e-05 9.9135665e-05 0.00010562543 0.00011318048 0.00011920429 0.00011936526 0.00011431839 0.00010778441 9.9836427e-05 9.2981878e-05 8.6556116e-05][8.4689018e-05 8.7447886e-05 8.9934932e-05 9.3272065e-05 9.860126e-05 0.00010660458 0.00011744534 0.00012879234 0.00013437643 0.00012954455 0.00012252216 0.00011489136 0.00010517663 9.65626e-05 8.8522109e-05][8.6500208e-05 9.043132e-05 9.4613322e-05 9.9768979e-05 0.00010653548 0.00011594846 0.00013008744 0.00014543207 0.00014614243 0.00013681276 0.0001280812 0.00011936121 0.00010859966 9.8944685e-05 9.03862e-05][9.0104149e-05 9.4048577e-05 9.7731274e-05 0.0001026859 0.00010988823 0.00011991752 0.00013172456 0.00014206396 0.00014251241 0.00013407739 0.00012629904 0.00011806587 0.00010809719 9.837035e-05 9.0037618e-05][9.5110023e-05 9.9452831e-05 0.00010133784 0.00010306208 0.00010777887 0.00011584299 0.00012194728 0.00012714656 0.00012780732 0.00012342251 0.00011767949 0.00011048666 0.00010321836 9.6084972e-05 8.8916051e-05][9.8004646e-05 0.00010152087 0.0001020282 0.00010172663 0.00010349469 0.00010855577 0.00011212222 0.00011433642 0.00011440577 0.00011131739 0.00010638364 0.00010120014 9.71863e-05 9.2859285e-05 8.7813765e-05][9.5768475e-05 9.930988e-05 9.9232107e-05 9.6664575e-05 9.70095e-05 0.00010186635 0.00010576952 0.00010638551 0.00010403433 0.00010072041 9.7640295e-05 9.5049028e-05 9.2420581e-05 8.9643778e-05 8.5992659e-05][9.179952e-05 9.5189367e-05 9.3975636e-05 9.0990652e-05 9.1510527e-05 9.594678e-05 9.9659337e-05 9.8139644e-05 9.46875e-05 9.2248316e-05 9.172385e-05 9.0698995e-05 8.8873887e-05 8.6155836e-05 8.4033731e-05][8.6977845e-05 8.9623813e-05 8.8850233e-05 8.8012835e-05 8.8501161e-05 9.0955742e-05 9.1307e-05 8.97178e-05 8.7708104e-05 8.6557848e-05 8.7645225e-05 8.7421737e-05 8.5202715e-05 8.3229766e-05 8.2094142e-05][8.3304993e-05 8.5698717e-05 8.5915555e-05 8.6103384e-05 8.6681626e-05 8.6943575e-05 8.584818e-05 8.4475985e-05 8.3374463e-05 8.3184532e-05 8.3982966e-05 8.37713e-05 8.2809252e-05 8.1753984e-05 8.0880243e-05]]...]
INFO - root - 2017-12-09 06:46:37.565132: step 210, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.621 sec/batch; 57h:17m:24s remains)
INFO - root - 2017-12-09 06:46:43.804545: step 220, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:17m:24s remains)
INFO - root - 2017-12-09 06:46:50.268125: step 230, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:37m:27s remains)
INFO - root - 2017-12-09 06:46:56.682797: step 240, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:34m:54s remains)
INFO - root - 2017-12-09 06:47:03.137961: step 250, loss = 0.90, batch loss = 0.69 (13.1 examples/sec; 0.613 sec/batch; 56h:34m:21s remains)
INFO - root - 2017-12-09 06:47:09.715196: step 260, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:52m:41s remains)
INFO - root - 2017-12-09 06:47:15.894238: step 270, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:33m:23s remains)
INFO - root - 2017-12-09 06:47:22.312028: step 280, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:42m:20s remains)
INFO - root - 2017-12-09 06:47:28.876607: step 290, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:21m:21s remains)
INFO - root - 2017-12-09 06:47:35.116157: step 300, loss = 0.90, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:59m:04s remains)
2017-12-09 06:47:35.785987: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00015322972 0.00016444709 0.00017446163 0.00018513779 0.00019044818 0.00019772652 0.00020959217 0.0002181634 0.00021664747 0.00020367032 0.00018403797 0.00016675981 0.00015212491 0.00014410635 0.00013635655][0.00013954623 0.00014495665 0.00015216492 0.00016484635 0.00017918408 0.00020050407 0.00022469653 0.00023904614 0.00023913269 0.00022720473 0.0002051211 0.00018092568 0.00015814904 0.00014546209 0.00013840117][0.00014400533 0.0001496749 0.00015274154 0.00016527805 0.0001895476 0.0002279568 0.00026788021 0.00029486182 0.00030139819 0.00028422632 0.0002516387 0.00021511893 0.00017813871 0.00015509516 0.00014254464][0.00015529995 0.000161383 0.00016597359 0.00018063742 0.00021277617 0.00026640255 0.00032641043 0.00037915018 0.00040901476 0.00039417003 0.00034690186 0.0002904335 0.00023285275 0.00018603964 0.00015538576][0.00016532188 0.00017306527 0.00017746491 0.00019508628 0.00023052456 0.00029543339 0.00037428513 0.00045594023 0.00052334811 0.00053523877 0.00049106084 0.00041321662 0.00032764091 0.00025260256 0.00019644947][0.00016484095 0.00017471515 0.00018190543 0.00020008483 0.00023556553 0.000304592 0.00039606888 0.00050493452 0.00060555612 0.00066041335 0.00064745737 0.00057409663 0.0004711205 0.00036980317 0.00028226431][0.00014987106 0.00016119893 0.00017236453 0.00018803368 0.00022000627 0.00028850182 0.00038231735 0.00050038216 0.00062485144 0.00071992609 0.00075572287 0.000722575 0.00063612749 0.00052660046 0.00041132388][0.00012700631 0.00013765621 0.00014787503 0.00016354813 0.00019096497 0.00025076279 0.00034253634 0.00045981459 0.00059100386 0.00070399261 0.00077289128 0.00079280511 0.000755265 0.00067068171 0.0005546817][0.00010635041 0.00011353982 0.0001226548 0.0001375234 0.00016427212 0.00021463653 0.00029298075 0.00039854328 0.0005207652 0.00063753279 0.00072215538 0.00078172161 0.00080040039 0.00076617952 0.00067840744][9.5244148e-05 0.00010075983 0.00010562283 0.00011708766 0.00013932669 0.00018240222 0.00024683119 0.00033234319 0.00043075375 0.000535986 0.00062987825 0.00071595097 0.00077354268 0.000793949 0.0007532303][9.2573129e-05 9.751878e-05 0.00010059873 0.00010648202 0.00012238511 0.00015493599 0.00020480363 0.00027157302 0.00034571617 0.00043321939 0.00052731106 0.00061950204 0.00070096186 0.00076427794 0.00078343117][9.6922064e-05 9.9643948e-05 0.00010059593 0.00010454554 0.00011591088 0.00013649718 0.00016971823 0.00021800162 0.00027600146 0.00034405739 0.00042197603 0.00050939329 0.00060028414 0.00069483282 0.00075930153][0.00011084587 0.00011127982 0.00011035512 0.0001128224 0.0001193859 0.00012961982 0.00014601805 0.00017494465 0.00021638304 0.0002683835 0.00033105633 0.00040326727 0.00049342046 0.00060434453 0.0006994264][0.00013591153 0.00013321871 0.00012920602 0.000128021 0.00012982888 0.00013291871 0.00013645271 0.00014974823 0.00017367747 0.00021261349 0.00026179705 0.00032010127 0.00039838671 0.00050203688 0.00060896063][0.0001646301 0.00015812018 0.00015065576 0.00014577968 0.00014361678 0.00014191099 0.00013727753 0.00013948695 0.00015273382 0.00017961815 0.00021598123 0.00026302293 0.00032488132 0.00040654777 0.0004960543]]...]
INFO - root - 2017-12-09 06:47:42.233137: step 310, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:32m:09s remains)
INFO - root - 2017-12-09 06:47:48.771677: step 320, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.687 sec/batch; 63h:23m:53s remains)
INFO - root - 2017-12-09 06:47:55.180621: step 330, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:48m:57s remains)
INFO - root - 2017-12-09 06:48:01.631081: step 340, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:51m:20s remains)
INFO - root - 2017-12-09 06:48:08.084619: step 350, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:55m:17s remains)
INFO - root - 2017-12-09 06:48:14.659834: step 360, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:30m:10s remains)
INFO - root - 2017-12-09 06:48:20.923729: step 370, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:37m:45s remains)
INFO - root - 2017-12-09 06:48:27.244638: step 380, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:56m:07s remains)
INFO - root - 2017-12-09 06:48:33.598049: step 390, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:06m:56s remains)
INFO - root - 2017-12-09 06:48:40.026516: step 400, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 57h:27m:40s remains)
2017-12-09 06:48:40.615085: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.6481422e-05 6.63324e-05 6.6014647e-05 6.58603e-05 6.5685614e-05 6.6521417e-05 6.6839588e-05 6.7826812e-05 6.7931149e-05 6.884188e-05 7.0018541e-05 7.2911855e-05 7.379664e-05 7.4316093e-05 7.4771015e-05][6.7298344e-05 6.8062291e-05 6.6634966e-05 6.6583882e-05 6.7335473e-05 6.9082664e-05 7.0206719e-05 7.1185321e-05 7.1545015e-05 7.2280243e-05 7.4946191e-05 7.8465622e-05 7.924658e-05 7.9565129e-05 7.9954909e-05][6.9169371e-05 7.1099952e-05 7.0458635e-05 7.010276e-05 7.1030481e-05 7.2527975e-05 7.4381707e-05 7.5255615e-05 7.6289471e-05 7.7400429e-05 7.99264e-05 8.3457111e-05 8.4919986e-05 8.5262953e-05 8.4858213e-05][7.7894685e-05 8.0111757e-05 7.8661069e-05 7.8155615e-05 8.0218932e-05 8.166107e-05 8.2876308e-05 8.44192e-05 8.57639e-05 8.61886e-05 8.79645e-05 9.1574882e-05 9.2015624e-05 9.1140166e-05 8.9648245e-05][8.2593026e-05 8.5649088e-05 8.6508117e-05 8.7618006e-05 8.9349109e-05 9.0478672e-05 9.1395079e-05 9.1296861e-05 9.1635658e-05 9.1693204e-05 9.3032941e-05 9.6508011e-05 9.7926022e-05 9.6291173e-05 9.3508883e-05][7.9635371e-05 8.2535953e-05 8.4828673e-05 8.7356e-05 8.8701563e-05 9.009413e-05 9.1517926e-05 9.0952082e-05 9.0434041e-05 8.9612287e-05 9.1111113e-05 9.4876217e-05 9.7406206e-05 9.7487275e-05 9.5503536e-05][8.002406e-05 8.5016953e-05 8.8608962e-05 8.9572059e-05 8.7323562e-05 8.7088512e-05 8.8072149e-05 8.8776455e-05 9.0077119e-05 8.9264751e-05 8.9580339e-05 9.2896655e-05 9.7036362e-05 9.8422839e-05 9.6607379e-05][8.0228e-05 8.615644e-05 9.1309907e-05 9.2797338e-05 9.0480164e-05 8.8215522e-05 8.7250759e-05 8.7098837e-05 8.6658241e-05 8.6962347e-05 8.862601e-05 9.2474693e-05 9.723023e-05 9.9083562e-05 9.7531614e-05][7.8679535e-05 8.4007355e-05 8.9130757e-05 9.1471047e-05 8.981389e-05 8.7470726e-05 8.6113163e-05 8.4984153e-05 8.2948878e-05 8.3028957e-05 8.5924265e-05 9.1469912e-05 9.6934062e-05 9.8880009e-05 9.6437572e-05][8.0649435e-05 8.73979e-05 9.288521e-05 9.3839466e-05 9.1571557e-05 8.85927e-05 8.5744141e-05 8.4104475e-05 8.2006205e-05 8.1623672e-05 8.37366e-05 8.7797867e-05 9.1985625e-05 9.4841249e-05 9.3405251e-05][7.8440309e-05 8.3900872e-05 8.8967739e-05 9.0926485e-05 9.0064525e-05 8.768712e-05 8.4535932e-05 8.2166822e-05 8.0744991e-05 8.0235091e-05 8.1439866e-05 8.4535372e-05 8.7287837e-05 8.9409768e-05 8.8817185e-05][7.7660479e-05 8.1681981e-05 8.5557687e-05 8.7353539e-05 8.8156208e-05 8.6733722e-05 8.4064544e-05 8.05801e-05 7.83489e-05 7.7445417e-05 7.8188765e-05 8.136587e-05 8.4442945e-05 8.6096872e-05 8.5421852e-05][7.3558032e-05 7.727203e-05 8.08478e-05 8.2904517e-05 8.370688e-05 8.2995772e-05 8.1285951e-05 7.8727884e-05 7.6606389e-05 7.5159383e-05 7.5589538e-05 7.7617144e-05 8.0111051e-05 8.1965954e-05 8.2315863e-05][7.2393275e-05 7.6222546e-05 7.8967285e-05 8.0080994e-05 7.9807862e-05 7.839009e-05 7.6933757e-05 7.5338539e-05 7.38738e-05 7.2863848e-05 7.2923584e-05 7.4237621e-05 7.6038792e-05 7.749904e-05 7.78668e-05][7.212218e-05 7.6609133e-05 7.9164631e-05 7.91917e-05 7.7824487e-05 7.5679316e-05 7.4012212e-05 7.2669805e-05 7.1711831e-05 7.1020884e-05 7.0974456e-05 7.1820257e-05 7.2961346e-05 7.3674615e-05 7.3696014e-05]]...]
INFO - root - 2017-12-09 06:48:46.840409: step 410, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:49m:15s remains)
INFO - root - 2017-12-09 06:48:53.275348: step 420, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:34m:56s remains)
INFO - root - 2017-12-09 06:48:59.671066: step 430, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:26m:31s remains)
INFO - root - 2017-12-09 06:49:06.089861: step 440, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 59h:06m:08s remains)
INFO - root - 2017-12-09 06:49:12.501126: step 450, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:26m:19s remains)
INFO - root - 2017-12-09 06:49:19.078176: step 460, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:29m:37s remains)
INFO - root - 2017-12-09 06:49:25.473016: step 470, loss = 0.90, batch loss = 0.69 (13.6 examples/sec; 0.589 sec/batch; 54h:20m:35s remains)
INFO - root - 2017-12-09 06:49:31.707979: step 480, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:43m:41s remains)
INFO - root - 2017-12-09 06:49:38.223382: step 490, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.643 sec/batch; 59h:15m:22s remains)
INFO - root - 2017-12-09 06:49:44.703500: step 500, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:29m:00s remains)
2017-12-09 06:49:45.370697: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00035237745 0.00038030971 0.00039678198 0.00041113587 0.00043636683 0.00047379633 0.00052668352 0.00060043368 0.00069246348 0.00079890486 0.00090914092 0.0010074982 0.0010744476 0.0011047225 0.00109084][0.0004293074 0.00047067067 0.00049542118 0.00051882624 0.00055436173 0.000601888 0.00066696375 0.00074927864 0.0008524892 0.00096741528 0.0010727353 0.0011623212 0.0012222539 0.0012374901 0.0012017668][0.00051283522 0.000569868 0.00060589943 0.00063737808 0.00067970523 0.00073266908 0.00080197316 0.00089061889 0.00099738024 0.0011131604 0.0012100943 0.0012785498 0.0013006292 0.0012673712 0.0011849687][0.00062135531 0.000694813 0.00074000651 0.00077412894 0.00081199 0.00086093292 0.00092724524 0.0010189448 0.0011192266 0.0012298391 0.0013175915 0.0013503736 0.0013228174 0.0012347206 0.0011065212][0.00073437 0.00082262955 0.00087138329 0.00089783204 0.00092372025 0.000970154 0.0010290954 0.0011076445 0.0011975573 0.0012982362 0.0013623432 0.0013573237 0.0012925355 0.0011768625 0.001033666][0.00083363696 0.00090706523 0.00094823213 0.00096213806 0.00098510331 0.0010186656 0.0010625386 0.0011198627 0.0011863606 0.0012590619 0.0013021957 0.0012902532 0.0012316187 0.0011218477 0.00099142373][0.00091687107 0.00095743092 0.00096448173 0.00095433771 0.00095609273 0.00097275694 0.0010021202 0.0010372986 0.0010554434 0.001086511 0.0011312986 0.0011548747 0.001129642 0.0010557597 0.0009491][0.00097069156 0.00097717845 0.00094223686 0.00088825746 0.00083659764 0.00081092917 0.0008206339 0.00084983231 0.00084132038 0.00084933662 0.00089649856 0.00094072206 0.00096257444 0.00094132905 0.0008801872][0.00097768824 0.0009532742 0.00088034343 0.00078752486 0.00068902713 0.00061436382 0.00057711883 0.00057313021 0.0005759006 0.00060007244 0.00065709208 0.00071644981 0.00077018078 0.00078323553 0.00075143442][0.00092474878 0.00087507517 0.00077845738 0.00066206261 0.00054545078 0.00045443754 0.00039479678 0.00037349778 0.00037851997 0.00041335434 0.00047728181 0.00054635783 0.00060396845 0.00063221372 0.000614687][0.00081437174 0.00075025577 0.00064962136 0.00054137415 0.00043665787 0.00035494537 0.00030038238 0.00027730322 0.00028524155 0.0003150175 0.0003696708 0.00043022126 0.00047803775 0.00050106551 0.00048815517][0.00065735489 0.00060262176 0.00052318734 0.00043641424 0.0003596188 0.0002970827 0.000255656 0.00023675132 0.00024426822 0.00027211523 0.00031324406 0.00036088857 0.00039531654 0.00040851784 0.00039695483][0.00051871029 0.00047859777 0.00041799992 0.00035798826 0.00030623004 0.00026561331 0.00023677353 0.00022418619 0.00023023869 0.00025048136 0.00027674437 0.00030456317 0.00032099918 0.0003237334 0.00031213037][0.00041465636 0.0003873907 0.00034414875 0.00030017714 0.00026428374 0.00023736022 0.00021927604 0.00021315717 0.00021365545 0.00022279726 0.00023200734 0.0002425119 0.00024815716 0.00024461598 0.0002374217][0.00032448146 0.00030412502 0.00027599715 0.00024509811 0.00021958546 0.00020132257 0.00019133525 0.00018720234 0.00018342497 0.00018629174 0.00018787413 0.00019202594 0.00019652308 0.00019581169 0.00019391264]]...]
INFO - root - 2017-12-09 06:49:51.521671: step 510, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:42m:10s remains)
INFO - root - 2017-12-09 06:49:57.937517: step 520, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:17m:10s remains)
INFO - root - 2017-12-09 06:50:04.330032: step 530, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:23m:51s remains)
INFO - root - 2017-12-09 06:50:10.904865: step 540, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.695 sec/batch; 64h:03m:46s remains)
INFO - root - 2017-12-09 06:50:17.346143: step 550, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:39m:51s remains)
INFO - root - 2017-12-09 06:50:23.716158: step 560, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:22m:02s remains)
INFO - root - 2017-12-09 06:50:30.051992: step 570, loss = 0.90, batch loss = 0.69 (14.4 examples/sec; 0.556 sec/batch; 51h:16m:08s remains)
INFO - root - 2017-12-09 06:50:36.404945: step 580, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:54m:45s remains)
INFO - root - 2017-12-09 06:50:42.836044: step 590, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:22m:26s remains)
INFO - root - 2017-12-09 06:50:49.362320: step 600, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:05m:25s remains)
2017-12-09 06:50:50.091175: I tensorflow/core/kernels/logging_ops.cc:79] [[[7.9207828e-05 8.1768761e-05 8.1165381e-05 7.98374e-05 7.6921831e-05 7.5074779e-05 7.3932912e-05 7.3454641e-05 7.4484975e-05 7.7354875e-05 8.0733385e-05 8.5776388e-05 9.2332717e-05 9.7783086e-05 0.00010117247][8.2639061e-05 8.5923035e-05 8.6575441e-05 8.62832e-05 8.5128013e-05 8.6057255e-05 8.6671629e-05 8.7482607e-05 8.8793982e-05 8.9812092e-05 8.9892412e-05 9.2457223e-05 9.7008968e-05 9.9838908e-05 0.00010041619][8.6772015e-05 9.1746304e-05 9.4372321e-05 9.7977929e-05 0.00010165673 0.00010586691 0.00010804123 0.00010826901 0.00010766539 0.00010605564 0.00010311543 0.00010358716 0.0001059782 0.0001042193 0.00010056065][9.5210838e-05 0.00010182723 0.00010318164 0.00010640341 0.0001103056 0.00011474525 0.00011741112 0.00011974422 0.00012200611 0.00012299669 0.00012221967 0.00012102762 0.00012014812 0.00011435719 0.0001068522][0.00010484771 0.00010885092 0.00010987587 0.00011423993 0.00012003833 0.00012525952 0.00013069743 0.00013642738 0.0001405386 0.00014165195 0.00013970806 0.00013504524 0.00013046931 0.00012324337 0.00011661821][0.00011475242 0.00011978217 0.00012320446 0.00013003373 0.00013801377 0.00014423249 0.00015108525 0.00015841635 0.0001624807 0.00016219041 0.00015809484 0.00015032727 0.00014382982 0.00013585849 0.00012814581][0.00012738958 0.00013192755 0.00013406145 0.00013854748 0.00014559417 0.00015370376 0.00016130567 0.00016741907 0.00017072821 0.00016953099 0.00016316665 0.0001557877 0.00015127276 0.00014390278 0.00013653004][0.00012885239 0.00013138817 0.00013272655 0.00013554796 0.0001426846 0.00014859234 0.00015422096 0.00016094395 0.00016392351 0.00016521219 0.00016119218 0.0001560017 0.00015205884 0.00014829195 0.00014624123][0.00012674852 0.0001306659 0.00013193178 0.00013461545 0.00014137277 0.00014682415 0.00015140294 0.00015574622 0.00015622428 0.00015670317 0.00015215878 0.00014615509 0.0001429277 0.00014415127 0.00014726301][0.00012386069 0.00012582671 0.00012719625 0.00013051282 0.00013467571 0.00013861846 0.00014251134 0.00014371055 0.00014235919 0.00014327191 0.00014029641 0.00013690883 0.00013607166 0.00013909876 0.00014076999][0.00012038037 0.00012151043 0.0001218792 0.0001228787 0.00012188913 0.00012159586 0.00012169038 0.0001212209 0.00012091652 0.00012265057 0.00012375598 0.00012455169 0.00012626332 0.00012940928 0.00012842343][0.00011866381 0.00011941016 0.00011688939 0.00011411607 0.00010900151 0.0001059454 0.00010397198 0.00010265335 0.00010201302 0.00010272823 0.00010521012 0.0001050253 0.00010572113 0.00010782413 0.00010790857][0.00011723085 0.00011564853 0.00011054997 0.00010603339 0.00010058723 9.6882621e-05 9.4325442e-05 9.223212e-05 9.136289e-05 9.1228809e-05 9.2614762e-05 9.2032555e-05 9.1245078e-05 9.1768808e-05 9.1208916e-05][0.000112414 0.00011026641 0.00010703217 0.00010226912 9.800581e-05 9.4998875e-05 9.1224305e-05 8.7482862e-05 8.6698776e-05 8.7414126e-05 8.7732056e-05 8.52615e-05 8.2403662e-05 8.1659913e-05 8.1152633e-05][0.00010939342 0.00010535635 0.00010034727 9.5968106e-05 9.3466006e-05 9.1840957e-05 9.0114554e-05 8.9417212e-05 8.9230205e-05 8.8797206e-05 8.68439e-05 8.3120431e-05 7.9567144e-05 7.8185629e-05 7.7070945e-05]]...]
INFO - root - 2017-12-09 06:50:56.155934: step 610, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:45m:41s remains)
INFO - root - 2017-12-09 06:51:02.580616: step 620, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:52m:22s remains)
INFO - root - 2017-12-09 06:51:09.060264: step 630, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:14m:10s remains)
INFO - root - 2017-12-09 06:51:15.490596: step 640, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 57h:57m:35s remains)
INFO - root - 2017-12-09 06:51:21.892353: step 650, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 57h:34m:25s remains)
INFO - root - 2017-12-09 06:51:28.151812: step 660, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.614 sec/batch; 56h:37m:04s remains)
INFO - root - 2017-12-09 06:51:34.402923: step 670, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 56h:59m:56s remains)
INFO - root - 2017-12-09 06:51:40.584568: step 680, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.614 sec/batch; 56h:36m:04s remains)
INFO - root - 2017-12-09 06:51:46.827211: step 690, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 56h:56m:21s remains)
INFO - root - 2017-12-09 06:51:53.253238: step 700, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:48m:54s remains)
2017-12-09 06:51:53.937382: I tensorflow/core/kernels/logging_ops.cc:79] [[[9.4866067e-05 9.66435e-05 9.7935452e-05 9.7877004e-05 9.6393749e-05 9.4770163e-05 9.447652e-05 9.4864678e-05 9.5669e-05 9.64988e-05 9.7007651e-05 9.6578551e-05 9.560059e-05 9.4564173e-05 9.31993e-05][9.513877e-05 9.7093143e-05 9.814935e-05 9.8205754e-05 9.7197822e-05 9.5390111e-05 9.56209e-05 9.6143049e-05 9.6911768e-05 9.7571028e-05 9.7684977e-05 9.70775e-05 9.6260876e-05 9.5124044e-05 9.3805749e-05][9.52688e-05 9.7643555e-05 9.7795448e-05 9.7887445e-05 9.762455e-05 9.7547796e-05 9.8094562e-05 9.825833e-05 9.9138204e-05 9.9812576e-05 9.9208839e-05 9.7535652e-05 9.6113065e-05 9.5022078e-05 9.3989984e-05][9.5798023e-05 9.85958e-05 9.78588e-05 9.8746372e-05 0.00010100276 0.00010313798 0.00010363051 0.00010260625 0.00010258879 0.0001022392 0.0001017734 9.9076075e-05 9.652866e-05 9.498703e-05 9.3803734e-05][9.5603478e-05 0.00010008809 0.00010019119 0.00010105429 0.00010726897 0.00011062015 0.00010983465 0.00010690942 0.00010499653 0.00010316059 0.00010207325 9.977811e-05 9.7227334e-05 9.5179363e-05 9.38052e-05][9.4727868e-05 0.00010166218 0.00010532002 0.00010740243 0.00011477746 0.00011762948 0.00011470435 0.00010987261 0.00010733931 0.00010377529 0.00010161015 9.8888471e-05 9.6966236e-05 9.5403295e-05 9.45034e-05][9.3369577e-05 0.00010211183 0.00010913244 0.00011268594 0.00011857464 0.0001203562 0.00011770727 0.00011354884 0.00011133682 0.00010704351 0.00010252491 9.8588505e-05 9.6898264e-05 9.5746829e-05 9.5405558e-05][9.0548056e-05 9.88239e-05 0.0001078781 0.0001126673 0.00011577652 0.00011592228 0.00011606685 0.00011582998 0.00011649301 0.00011087505 0.00010405638 9.9436642e-05 9.7827025e-05 9.6999276e-05 9.6723132e-05][8.79348e-05 9.4551811e-05 0.00010163286 0.00010543394 0.00010729553 0.00010721046 0.00010833292 0.00010895244 0.00011324605 0.0001096736 0.00010299538 9.9839832e-05 9.9348174e-05 9.869301e-05 9.7600634e-05][8.6247339e-05 9.0454785e-05 9.4843104e-05 9.7384342e-05 9.8750861e-05 9.8084172e-05 9.7809352e-05 9.6943688e-05 0.0001010009 0.00010225874 9.9922057e-05 9.9078708e-05 9.9873418e-05 9.857642e-05 9.7320575e-05][8.4917658e-05 8.7479173e-05 8.9314017e-05 9.0276553e-05 9.1767608e-05 9.1665846e-05 9.093812e-05 8.8918147e-05 9.1175396e-05 9.4644631e-05 9.8903707e-05 0.00010006564 9.948355e-05 9.7383272e-05 9.6687232e-05][8.37297e-05 8.5463907e-05 8.6086708e-05 8.562424e-05 8.6735163e-05 8.6537584e-05 8.6696222e-05 8.5785745e-05 8.6636494e-05 9.0552741e-05 9.6839576e-05 9.8666766e-05 9.7704346e-05 9.5818439e-05 9.5137286e-05][8.3262988e-05 8.4225176e-05 8.392617e-05 8.3023842e-05 8.3237341e-05 8.3438114e-05 8.4684827e-05 8.4291853e-05 8.4434592e-05 8.6789754e-05 9.0013811e-05 9.2102113e-05 9.3649425e-05 9.3902359e-05 9.3910014e-05][8.37329e-05 8.4165753e-05 8.3449624e-05 8.2668077e-05 8.2193459e-05 8.1606267e-05 8.1758873e-05 8.1688682e-05 8.1241917e-05 8.1647122e-05 8.2921935e-05 8.4910062e-05 8.8746463e-05 9.1527414e-05 9.2991395e-05][8.40722e-05 8.4399624e-05 8.4042214e-05 8.3682382e-05 8.3435596e-05 8.2286439e-05 8.1361664e-05 8.0716643e-05 7.9363359e-05 7.8646866e-05 8.0071237e-05 8.2249775e-05 8.6317057e-05 9.0401838e-05 9.2576607e-05]]...]
INFO - root - 2017-12-09 06:52:00.098027: step 710, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 61h:00m:58s remains)
INFO - root - 2017-12-09 06:52:06.638226: step 720, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:15m:03s remains)
INFO - root - 2017-12-09 06:52:13.089880: step 730, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 56h:59m:13s remains)
INFO - root - 2017-12-09 06:52:19.510981: step 740, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:19m:23s remains)
INFO - root - 2017-12-09 06:52:26.080223: step 750, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:55m:33s remains)
INFO - root - 2017-12-09 06:52:32.527929: step 760, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:22m:14s remains)
INFO - root - 2017-12-09 06:52:39.061572: step 770, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 57h:38m:16s remains)
INFO - root - 2017-12-09 06:52:45.262826: step 780, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:28m:56s remains)
INFO - root - 2017-12-09 06:52:51.607919: step 790, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:50m:17s remains)
INFO - root - 2017-12-09 06:52:58.079362: step 800, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:39m:36s remains)
2017-12-09 06:52:58.747962: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.8414585e-05 9.6001495e-05 0.00010018028 0.00010440657 0.00010587195 0.0001071433 0.00010751347 0.00010550101 0.00010171847 9.9183431e-05 9.8713681e-05 9.6911768e-05 9.3448682e-05 9.0470625e-05 8.9729059e-05][0.00010065829 0.00011447724 0.00012544697 0.00013761171 0.00014762058 0.0001540497 0.00015386583 0.00014805105 0.00013898665 0.00013230457 0.00012912003 0.00012504512 0.00011922592 0.0001117933 0.00010563148][0.00011591647 0.0001386959 0.00016137204 0.00018848044 0.00021909851 0.00024486388 0.00025117706 0.00023806407 0.00021805511 0.00020308491 0.00019533988 0.00018891595 0.00018111146 0.00016443827 0.00014394402][0.00013167564 0.00016556526 0.00020685841 0.00026143179 0.00032935958 0.00039007436 0.00041312919 0.00039594792 0.00035974485 0.00033393686 0.00032107491 0.00031373254 0.00029829881 0.00026406118 0.00021634022][0.00014894216 0.00019718318 0.00026191535 0.00034712741 0.00045900606 0.0005658915 0.00061581226 0.00059496908 0.00054764358 0.00052032107 0.00050432282 0.00049467117 0.00046989031 0.00041188675 0.00032302536][0.00016287969 0.0002252603 0.00030948195 0.00041879225 0.00056547433 0.00071486138 0.00078905327 0.00077199185 0.00072867307 0.00070599816 0.0006962464 0.00068496977 0.00065349339 0.000571388 0.00044081523][0.00016717355 0.0002351069 0.00032398416 0.0004440581 0.00060679909 0.00077317056 0.00086233328 0.00086057233 0.0008313586 0.00082284026 0.00082758936 0.00082211586 0.000781088 0.00068554503 0.00053174741][0.00016105335 0.00022349511 0.00030143574 0.00041144053 0.00056641409 0.00073190616 0.000826386 0.00083972118 0.00082851981 0.00083438482 0.00084858737 0.00084413297 0.00080566609 0.00071176118 0.00055915164][0.0001471071 0.00019933532 0.00025966595 0.0003479163 0.00047528671 0.00061426 0.0006932274 0.00071447465 0.0007196053 0.00073429727 0.0007505684 0.00075179362 0.00072526821 0.00064411864 0.00050743879][0.00012964924 0.00016873708 0.00021239213 0.00027713124 0.0003653953 0.00046213303 0.00052093947 0.00054284389 0.000553253 0.0005649564 0.00057902787 0.00058327871 0.00056613894 0.00050554669 0.00040187634][0.00011561071 0.00014120045 0.00016973622 0.00021151002 0.00026409721 0.00032116851 0.00035788774 0.00037683404 0.00038563553 0.00039355978 0.00040078175 0.00040406812 0.00039176052 0.00035110442 0.00028752131][0.00010268825 0.00011738935 0.00013438368 0.00015876343 0.00018701528 0.00021504445 0.00023340883 0.00024208683 0.00024889174 0.00025217756 0.00025589261 0.00025837752 0.00025200562 0.00023243473 0.00019861286][9.3787166e-05 0.00010299733 0.00011116453 0.00012370585 0.00013683298 0.00014999778 0.00015751558 0.0001601664 0.00016247598 0.00016338815 0.00016411283 0.0001659931 0.00016308241 0.00015427402 0.00014073905][8.7005727e-05 9.3747112e-05 9.8975281e-05 0.0001043434 0.00010917391 0.00011572674 0.00011985272 0.00011965637 0.00011888549 0.00011792134 0.00011854815 0.00011872112 0.00011611797 0.00011386939 0.00010922824][8.3890787e-05 8.8747722e-05 9.08109e-05 9.2912633e-05 9.4556795e-05 9.7586075e-05 0.00010018602 0.00010028766 0.00010033198 9.9516641e-05 9.9446879e-05 9.8525765e-05 9.7399236e-05 9.6090509e-05 9.4258779e-05]]...]
INFO - root - 2017-12-09 06:53:04.811840: step 810, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.621 sec/batch; 57h:12m:35s remains)
INFO - root - 2017-12-09 06:53:11.193891: step 820, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:39m:24s remains)
INFO - root - 2017-12-09 06:53:17.596491: step 830, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:34m:51s remains)
INFO - root - 2017-12-09 06:53:23.967482: step 840, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:50m:54s remains)
INFO - root - 2017-12-09 06:53:30.271111: step 850, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:17m:22s remains)
INFO - root - 2017-12-09 06:53:36.719783: step 860, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:42m:44s remains)
INFO - root - 2017-12-09 06:53:42.971601: step 870, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.617 sec/batch; 56h:47m:42s remains)
INFO - root - 2017-12-09 06:53:49.172711: step 880, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:26m:27s remains)
INFO - root - 2017-12-09 06:53:55.423153: step 890, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 55h:55m:35s remains)
INFO - root - 2017-12-09 06:54:01.849996: step 900, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:03m:51s remains)
2017-12-09 06:54:02.538588: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0026208679 0.0028885226 0.0030519103 0.0031302294 0.0031168561 0.0030302072 0.0029103572 0.0027736393 0.0026414341 0.0025317126 0.002464785 0.0024527172 0.0024854669 0.002545628 0.0026024103][0.0028094028 0.0031008322 0.0033088282 0.0034398446 0.0034719456 0.0034182072 0.0033138555 0.0031702032 0.0030065682 0.0028758054 0.0028040174 0.00279293 0.0028153122 0.0028561105 0.0028812182][0.0029449309 0.0032376549 0.0034590045 0.0036181645 0.0036810266 0.0036561736 0.0035799406 0.0034501252 0.0032860006 0.0031680239 0.0031130509 0.0031163911 0.0031429066 0.0031678863 0.0031641922][0.0031137953 0.0034043321 0.0036234432 0.0037848358 0.0038530126 0.0038296857 0.0037518952 0.0036282663 0.0034822323 0.0033955034 0.003379259 0.0034169778 0.0034632701 0.0034852684 0.0034558733][0.003250198 0.0035351519 0.0037512092 0.0039160205 0.0039877277 0.0039690635 0.0038985698 0.0037933255 0.0036653036 0.003602315 0.0036145139 0.00369307 0.0037773538 0.0038187222 0.0037794926][0.0032602388 0.0035563654 0.0037766451 0.0039546462 0.00404284 0.0040379111 0.0039872727 0.003906324 0.0038088241 0.0037722292 0.0038154849 0.003930524 0.0040406841 0.0041104513 0.0040835319][0.0031531898 0.0034648781 0.0037053695 0.0038996926 0.0040208688 0.0040549077 0.0040266216 0.0039715166 0.0038962301 0.0038725936 0.0039336518 0.0040717321 0.0042025577 0.0042896951 0.0042850948][0.0028935794 0.0032172194 0.0034887907 0.0037227168 0.0038985403 0.0039862874 0.0040018214 0.0039780014 0.0039231954 0.0039053804 0.0039657126 0.00409978 0.00424176 0.0043528867 0.0043808674][0.0024715886 0.0028149656 0.0031230354 0.0034109165 0.0036469912 0.0038050327 0.003878321 0.0038913812 0.003848661 0.0038262629 0.0038855004 0.0040123616 0.0041552545 0.0042781988 0.0043406677][0.0019634543 0.0022990769 0.0026331372 0.0029587124 0.0032468203 0.0034646005 0.0035895989 0.0036425353 0.003630877 0.00361514 0.0036731958 0.0037892163 0.003925411 0.0040574968 0.0041515427][0.0014244183 0.0017352948 0.0020640492 0.0023970765 0.00271394 0.0029717654 0.0031371841 0.0032251033 0.003256215 0.0032686468 0.0033339558 0.003440435 0.0035664057 0.0036958971 0.0038003202][0.00095111184 0.0012137626 0.0015164375 0.0018267067 0.0021327178 0.0023950136 0.0025891156 0.0027087496 0.0027701363 0.0028134852 0.002886537 0.0029853643 0.0031021934 0.0032362491 0.0033431735][0.00057628605 0.0007669825 0.0010034511 0.0012614796 0.0015299635 0.0017766235 0.0019731738 0.002111027 0.0022010058 0.0022753247 0.0023607921 0.0024535526 0.0025644936 0.0026900056 0.0028043613][0.00034772442 0.00046420205 0.00062070671 0.00080091861 0.0010072346 0.0012054717 0.0013822876 0.001509505 0.0016125888 0.0016959385 0.001782584 0.0018718077 0.0019795867 0.0021064505 0.0022251476][0.00020597538 0.00027025951 0.00036236236 0.00047066412 0.00059744611 0.00073322712 0.00086543476 0.00097246881 0.0010619888 0.0011457484 0.0012263267 0.0013110284 0.0014167499 0.0015422688 0.0016615522]]...]
INFO - root - 2017-12-09 06:54:08.928611: step 910, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 57h:54m:55s remains)
INFO - root - 2017-12-09 06:54:15.238461: step 920, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:34m:22s remains)
INFO - root - 2017-12-09 06:54:21.701422: step 930, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:23m:12s remains)
INFO - root - 2017-12-09 06:54:28.271013: step 940, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:17m:13s remains)
INFO - root - 2017-12-09 06:54:34.581912: step 950, loss = 0.90, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:14m:33s remains)
INFO - root - 2017-12-09 06:54:40.991840: step 960, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:02m:45s remains)
INFO - root - 2017-12-09 06:54:47.422306: step 970, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:36m:24s remains)
INFO - root - 2017-12-09 06:54:53.704357: step 980, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:03m:48s remains)
INFO - root - 2017-12-09 06:54:59.980765: step 990, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:36m:54s remains)
INFO - root - 2017-12-09 06:55:06.441622: step 1000, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:31m:50s remains)
2017-12-09 06:55:07.197559: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00011827057 0.0001234019 0.00012227922 0.00012097457 0.00011829636 0.00011510931 0.00011391361 0.0001158454 0.00011784513 0.00011896166 0.00011953358 0.00011778031 0.00010931255 9.4986324e-05 8.1794642e-05][0.00012891741 0.00013321759 0.00013097854 0.00012942127 0.00012508633 0.00012103157 0.00011876291 0.00012046225 0.0001218509 0.00012256275 0.00012346634 0.00012070074 0.00011046989 9.5717383e-05 8.2321028e-05][0.00013149933 0.00013487908 0.00013268349 0.00013154534 0.00012744234 0.00012310917 0.00012027442 0.00012093858 0.00012090403 0.00012082745 0.0001209117 0.00011568327 0.00010455239 9.0961941e-05 7.9032376e-05][0.00012810339 0.00013091942 0.00012968914 0.00012887464 0.00012526782 0.00012152974 0.00011900782 0.0001195521 0.00011913954 0.00011737044 0.00011534993 0.0001089792 9.8301054e-05 8.5931206e-05 7.5312986e-05][0.00012458906 0.00012592771 0.00012397465 0.00012198185 0.00011957904 0.00011730114 0.00011598878 0.00011664997 0.00011464133 0.00011169192 0.00010960602 0.00010350972 9.2361253e-05 8.1269711e-05 7.1626186e-05][0.00011334559 0.00011252145 0.00011128063 0.00011050034 0.00011019146 0.00011168295 0.00011288436 0.00011494978 0.00011315016 0.00010938785 0.00010609366 9.87888e-05 8.7105407e-05 7.6635646e-05 6.83838e-05][0.00010738111 0.00010563247 0.00010399011 0.00010383464 0.00010415616 0.0001062629 0.00010892381 0.00011129006 0.00010913274 0.00010519587 0.00010100576 9.2919843e-05 8.1560844e-05 7.1810151e-05 6.5165666e-05][0.00010042824 9.8858829e-05 9.7419674e-05 9.6964635e-05 9.782803e-05 0.00010083461 0.00010443582 0.00010743857 0.00010552065 0.00010075612 9.5947726e-05 8.7956068e-05 7.77751e-05 6.9176567e-05 6.3284184e-05][0.00010050131 9.8658827e-05 9.7051365e-05 9.6804441e-05 9.8024888e-05 0.00010041028 0.00010333837 0.00010565716 0.0001033872 9.8860211e-05 9.4590272e-05 8.7005916e-05 7.734033e-05 6.8910194e-05 6.2891013e-05][0.00010292437 0.00010182308 0.00010069677 0.0001005942 0.00010194477 0.00010351259 0.00010507059 0.0001058356 0.00010263866 9.826434e-05 9.440221e-05 8.6799388e-05 7.7214507e-05 6.9107024e-05 6.2877771e-05][0.00010331524 0.00010342087 0.00010256151 0.00010184001 0.00010176426 0.00010214731 0.0001023177 0.00010252553 9.9790581e-05 9.6842363e-05 9.3747491e-05 8.6491316e-05 7.6980075e-05 6.89639e-05 6.2928877e-05][0.00010260253 0.00010235098 0.0001005044 9.87044e-05 9.7671909e-05 9.697066e-05 9.6423384e-05 9.6001684e-05 9.4106574e-05 9.2741655e-05 9.1027148e-05 8.4793326e-05 7.6070071e-05 6.8243862e-05 6.2422514e-05][0.00010031372 0.00010002759 9.8369135e-05 9.6174612e-05 9.3949551e-05 9.211967e-05 9.0491296e-05 8.9225934e-05 8.764556e-05 8.73906e-05 8.701894e-05 8.2201674e-05 7.4295516e-05 6.6748631e-05 6.116624e-05][9.588286e-05 9.5494448e-05 9.4202485e-05 9.1944734e-05 8.885334e-05 8.62012e-05 8.3762439e-05 8.2102815e-05 8.09876e-05 8.1253944e-05 8.15836e-05 7.8106263e-05 7.1645532e-05 6.5041284e-05 5.9940485e-05][8.6532891e-05 8.5900181e-05 8.45384e-05 8.2368249e-05 7.9699028e-05 7.73276e-05 7.5255564e-05 7.4302792e-05 7.3672476e-05 7.4182593e-05 7.50283e-05 7.2618837e-05 6.8058129e-05 6.2973326e-05 5.8863789e-05]]...]
INFO - root - 2017-12-09 06:55:13.503119: step 1010, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 57h:20m:01s remains)
INFO - root - 2017-12-09 06:55:19.768072: step 1020, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:03m:39s remains)
INFO - root - 2017-12-09 06:55:26.312568: step 1030, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:14m:36s remains)
INFO - root - 2017-12-09 06:55:32.822223: step 1040, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:01m:44s remains)
INFO - root - 2017-12-09 06:55:39.300984: step 1050, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:18m:22s remains)
INFO - root - 2017-12-09 06:55:45.894405: step 1060, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:23m:57s remains)
INFO - root - 2017-12-09 06:55:52.454545: step 1070, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:36m:40s remains)
INFO - root - 2017-12-09 06:55:58.755836: step 1080, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:27m:00s remains)
INFO - root - 2017-12-09 06:56:05.066765: step 1090, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.621 sec/batch; 57h:11m:37s remains)
INFO - root - 2017-12-09 06:56:11.495860: step 1100, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:23m:28s remains)
2017-12-09 06:56:12.140691: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00018352467 0.00022918364 0.00026729793 0.00029446612 0.00029619888 0.00028038045 0.00025287934 0.00022240719 0.00018690355 0.00015318567 0.00012692623 0.00010725536 9.2880946e-05 8.2580271e-05 7.4306918e-05][0.00018019252 0.0002262409 0.00026825888 0.00030575349 0.00031823743 0.00030282204 0.00027828515 0.00024961343 0.00021370134 0.00017633774 0.00014482089 0.00011928057 0.00010026574 8.6934815e-05 7.5797368e-05][0.00016733487 0.00021341059 0.00026076337 0.00031017125 0.00033937045 0.00033187034 0.00030695894 0.00027977629 0.00024389748 0.0002017509 0.00016590273 0.00013541229 0.00010955206 9.2171489e-05 7.8169876e-05][0.00014853895 0.00019339581 0.00024259005 0.00029496796 0.00033163611 0.000340677 0.00032925248 0.00030723336 0.00027057817 0.00022755464 0.00018825373 0.00015268274 0.00012168899 0.00010060265 8.3578918e-05][0.00012958782 0.00017066528 0.00021383217 0.00025670938 0.00029414831 0.00031443805 0.00032014371 0.00030613269 0.0002734538 0.0002361131 0.00020094163 0.00016399863 0.0001304106 0.00010759544 8.9319663e-05][0.00011806755 0.00015373525 0.00019163838 0.00022873929 0.00026474608 0.00029103417 0.00030282623 0.000300302 0.00027354175 0.0002396716 0.00020519909 0.00016917032 0.00013508946 0.00011021579 9.0999609e-05][0.00011344034 0.00014157414 0.00017902008 0.00021664711 0.00024734455 0.00026941852 0.00028596551 0.00028918756 0.00026939096 0.00023804967 0.00020668206 0.00017249471 0.00014036712 0.00011266643 9.0550748e-05][0.00010764973 0.00013158849 0.00016373015 0.00019673616 0.00021993474 0.00023913789 0.00025896021 0.00026824733 0.00025511344 0.00023119026 0.00020343732 0.00017203556 0.00014350803 0.00011705457 9.1411457e-05][9.6058669e-05 0.00011372014 0.00013647335 0.00016062228 0.00018024699 0.00019672903 0.00021157374 0.00021977614 0.00021665687 0.00020691019 0.0001864612 0.0001597115 0.00013890621 0.00011610761 9.1452675e-05][8.4806146e-05 9.7026517e-05 0.00011167723 0.00012987 0.00014650881 0.00015901741 0.00016947789 0.00017665398 0.00017961054 0.00017912268 0.00016410735 0.00014170418 0.00012405863 0.00010607872 8.5081439e-05][7.6966593e-05 8.5895452e-05 9.64786e-05 0.00010867826 0.00011974472 0.000132146 0.00014106087 0.00014746966 0.00015194144 0.00015340747 0.00014255819 0.00012493246 0.00010930237 9.3768671e-05 7.76078e-05][7.1441158e-05 7.6952681e-05 8.3256542e-05 9.06955e-05 9.85438e-05 0.00010727669 0.00011535231 0.00012077665 0.00012510328 0.00012656345 0.00012117984 0.00010906722 9.878038e-05 8.5429485e-05 7.2102011e-05][6.8878144e-05 7.2140996e-05 7.4698954e-05 7.7481614e-05 8.08192e-05 8.6120563e-05 9.112672e-05 9.588043e-05 0.00010089879 0.00010223759 9.9253877e-05 9.3322487e-05 8.67483e-05 7.8326484e-05 6.8536479e-05][6.6851273e-05 6.8524969e-05 6.9595379e-05 7.0555907e-05 7.1868555e-05 7.44835e-05 7.6808523e-05 7.8993842e-05 8.2518338e-05 8.5340645e-05 8.5647058e-05 8.3754094e-05 7.9223064e-05 7.3979521e-05 6.6388e-05][6.514891e-05 6.78469e-05 6.8663256e-05 6.87405e-05 6.92856e-05 7.1382448e-05 7.2955321e-05 7.3292955e-05 7.44798e-05 7.7017321e-05 7.9938669e-05 7.9622827e-05 7.7305711e-05 7.386069e-05 6.752435e-05]]...]
INFO - root - 2017-12-09 06:56:18.639425: step 1110, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:28m:30s remains)
INFO - root - 2017-12-09 06:56:24.841816: step 1120, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:03m:03s remains)
INFO - root - 2017-12-09 06:56:31.273591: step 1130, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:19m:08s remains)
INFO - root - 2017-12-09 06:56:37.858377: step 1140, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:07m:48s remains)
INFO - root - 2017-12-09 06:56:44.405381: step 1150, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:46m:43s remains)
INFO - root - 2017-12-09 06:56:50.981064: step 1160, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:44m:09s remains)
INFO - root - 2017-12-09 06:56:57.438137: step 1170, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 57h:33m:18s remains)
INFO - root - 2017-12-09 06:57:03.543136: step 1180, loss = 0.90, batch loss = 0.69 (19.0 examples/sec; 0.421 sec/batch; 38h:42m:01s remains)
INFO - root - 2017-12-09 06:57:09.952902: step 1190, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:47m:46s remains)
INFO - root - 2017-12-09 06:57:16.437848: step 1200, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:43m:31s remains)
2017-12-09 06:57:17.129654: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0010822371 0.0011248831 0.001104157 0.0010658188 0.0010067688 0.00094746478 0.00090620376 0.000886652 0.00087696535 0.000871807 0.0008619352 0.0008390976 0.00080793211 0.00077087834 0.00073459162][0.0012416659 0.0013362988 0.0013314231 0.0012982039 0.0012260946 0.0011478944 0.0010864901 0.0010468726 0.0010216812 0.0010140078 0.0010095963 0.0009926362 0.00096154044 0.00091805047 0.00086917815][0.0013775502 0.0015196799 0.0015367914 0.0014996533 0.0014119826 0.0013164113 0.0012321372 0.0011641496 0.0011155134 0.0010981862 0.0010960778 0.0010923957 0.001070905 0.0010278614 0.00097169145][0.0014929781 0.0016646704 0.0016935987 0.0016597707 0.001570848 0.001459867 0.0013483887 0.0012544398 0.0011809302 0.0011459289 0.0011442781 0.0011512325 0.0011486445 0.0011184723 0.0010648217][0.0015705078 0.0017656286 0.0018060608 0.0017766951 0.0016916104 0.0015753987 0.0014459742 0.0013235789 0.0012247963 0.0011678216 0.0011579447 0.0011753266 0.0011932206 0.0011805306 0.0011412763][0.0016052345 0.0018087511 0.0018589525 0.0018353203 0.0017590603 0.0016491954 0.0015121143 0.0013679679 0.0012393792 0.0011591251 0.0011389748 0.0011620601 0.0012009712 0.0012135366 0.001194293][0.0015761098 0.0017671443 0.0018289158 0.0018222275 0.0017650841 0.0016671942 0.0015376399 0.0013831772 0.0012310507 0.0011259905 0.0010959986 0.0011215206 0.0011753547 0.0012142045 0.0012181964][0.0014955997 0.0016638467 0.0017249209 0.0017354258 0.0017062102 0.0016308986 0.001520875 0.0013776612 0.0012053861 0.0010801414 0.0010392372 0.0010610298 0.001123354 0.0011794544 0.0012093751][0.0013875711 0.0015396205 0.0015981782 0.0016154146 0.0016033421 0.0015375629 0.0014259453 0.0012922357 0.0011376691 0.0010103167 0.000963458 0.00098044507 0.0010422802 0.0011054912 0.0011494567][0.0012709554 0.0014121924 0.001465008 0.0014831362 0.001470421 0.0014016796 0.0012759129 0.0011331087 0.00099931273 0.00089325232 0.00085480046 0.00087326294 0.00093095051 0.00099241163 0.001038125][0.0011411724 0.0012677205 0.0013125389 0.0013255742 0.0013080642 0.0012354911 0.0011042827 0.00095160317 0.00082158938 0.00073622476 0.00071248179 0.00073527842 0.0007879223 0.00084272609 0.00088658056][0.000992012 0.0011081686 0.0011443738 0.0011498523 0.0011259776 0.001048526 0.00091778178 0.000770775 0.0006430503 0.00057099748 0.00055605877 0.00058019254 0.00062702369 0.00067470857 0.00071496383][0.00080924219 0.00090990117 0.00094606052 0.000946115 0.00091394153 0.00084045244 0.00072349346 0.00059300615 0.00048168676 0.00042051842 0.00040596718 0.00041897281 0.00045639896 0.00049761019 0.00053264515][0.00061589258 0.00069280225 0.00071747351 0.00071668648 0.00068485033 0.00062447094 0.00053416891 0.00043394472 0.000348713 0.00029807957 0.00028154947 0.0002842489 0.00030474417 0.00033380472 0.00036348272][0.00041738697 0.00047363367 0.00049416017 0.00049110776 0.00046743196 0.00042942984 0.00037181628 0.00030577171 0.00024786018 0.00021083512 0.00019610216 0.00019238003 0.0002011538 0.00021809098 0.00023768384]]...]
INFO - root - 2017-12-09 06:57:23.675798: step 1210, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.670 sec/batch; 61h:38m:04s remains)
INFO - root - 2017-12-09 06:57:29.992126: step 1220, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:17m:08s remains)
INFO - root - 2017-12-09 06:57:36.364557: step 1230, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:16m:15s remains)
INFO - root - 2017-12-09 06:57:42.924891: step 1240, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:32m:54s remains)
INFO - root - 2017-12-09 06:57:49.496359: step 1250, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 61h:58m:19s remains)
INFO - root - 2017-12-09 06:57:56.013508: step 1260, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:19m:07s remains)
INFO - root - 2017-12-09 06:58:02.880641: step 1270, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:48m:11s remains)
INFO - root - 2017-12-09 06:58:09.219877: step 1280, loss = 0.90, batch loss = 0.69 (16.8 examples/sec; 0.475 sec/batch; 43h:44m:03s remains)
INFO - root - 2017-12-09 06:58:15.575901: step 1290, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:29m:51s remains)
INFO - root - 2017-12-09 06:58:21.974868: step 1300, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:17m:01s remains)
2017-12-09 06:58:22.657237: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.3470325e-05 6.6445369e-05 7.08507e-05 7.6542616e-05 8.1969913e-05 8.6379492e-05 9.2492941e-05 9.8832017e-05 9.9741133e-05 9.2723407e-05 8.4357591e-05 7.7775963e-05 7.1531125e-05 6.5050677e-05 6.1326158e-05][6.6324828e-05 7.0173075e-05 7.5660486e-05 8.2209357e-05 8.7545108e-05 9.0765141e-05 9.54124e-05 0.00010080232 0.00010072842 9.29176e-05 8.4112013e-05 7.7961005e-05 7.1973751e-05 6.5375643e-05 6.1652267e-05][7.062294e-05 7.517523e-05 8.1424361e-05 8.8411784e-05 9.2429837e-05 9.3838877e-05 9.671149e-05 0.00010097747 0.00010042724 9.2677859e-05 8.3904692e-05 7.84057e-05 7.2726769e-05 6.6142507e-05 6.2278334e-05][7.4966505e-05 8.0425511e-05 8.6828375e-05 9.360657e-05 9.5710748e-05 9.5754884e-05 9.72142e-05 0.00010035435 9.9729084e-05 9.2679227e-05 8.4014173e-05 7.8666206e-05 7.3387186e-05 6.6957182e-05 6.2772939e-05][7.9622623e-05 8.5571417e-05 9.1451577e-05 9.7726726e-05 9.8094082e-05 9.681363e-05 9.7593045e-05 0.00010012696 9.9948207e-05 9.3683746e-05 8.5094951e-05 7.9006662e-05 7.3978488e-05 6.7597532e-05 6.3095649e-05][8.2913655e-05 8.9475172e-05 9.4873925e-05 0.00010046097 9.9465135e-05 9.6928488e-05 9.7143689e-05 9.9996127e-05 0.00010078495 9.5214986e-05 8.6834116e-05 8.0570964e-05 7.5315118e-05 6.8532616e-05 6.3670384e-05][8.5600725e-05 9.2324524e-05 9.7159558e-05 0.0001020281 0.0001008652 9.74376e-05 9.7763026e-05 0.00010157161 0.00010320872 9.7812022e-05 8.9286696e-05 8.2676612e-05 7.69257e-05 6.95479e-05 6.4201035e-05][8.7312626e-05 9.3469345e-05 9.7799653e-05 0.00010200799 0.00010111475 9.7632234e-05 9.8787219e-05 0.00010414833 0.0001060265 0.00010054746 9.1645947e-05 8.4731044e-05 7.8254139e-05 7.0271046e-05 6.4505e-05][8.9274814e-05 9.41397e-05 9.8153141e-05 0.0001004955 9.941883e-05 9.6831624e-05 9.9539116e-05 0.00010621789 0.00010778158 0.00010236358 9.3572526e-05 8.6417123e-05 7.8909929e-05 7.059153e-05 6.46514e-05][9.1273694e-05 9.472089e-05 9.7781813e-05 9.8701945e-05 9.7359785e-05 9.6338546e-05 0.00010036282 0.00010673299 0.00010887965 0.00010360328 9.4728959e-05 8.7367152e-05 7.91865e-05 7.0673879e-05 6.4836808e-05][9.13137e-05 9.3883675e-05 9.6029529e-05 9.62873e-05 9.5135532e-05 9.4766e-05 9.8935969e-05 0.00010466626 0.00010702798 0.00010246311 9.3603026e-05 8.657288e-05 7.8711026e-05 7.038351e-05 6.5007524e-05][8.8389686e-05 9.0622671e-05 9.2980292e-05 9.3784292e-05 9.2102564e-05 9.2093178e-05 9.5757066e-05 0.00010071922 0.00010264266 9.8352604e-05 9.0063892e-05 8.370242e-05 7.6732569e-05 6.9254318e-05 6.4883789e-05][8.3599989e-05 8.5125081e-05 8.7493638e-05 8.9018831e-05 8.7800807e-05 8.8286244e-05 9.1802533e-05 9.6185482e-05 9.7207456e-05 9.31096e-05 8.5933854e-05 7.9872072e-05 7.3742056e-05 6.7715308e-05 6.4455737e-05][7.5216783e-05 7.6867895e-05 7.9107231e-05 8.1438986e-05 8.0814934e-05 8.1185659e-05 8.411397e-05 8.7762906e-05 8.8087356e-05 8.5242355e-05 7.9974117e-05 7.5228811e-05 7.0526781e-05 6.607172e-05 6.39048e-05][6.6947941e-05 6.8687834e-05 7.07896e-05 7.3120587e-05 7.333843e-05 7.3664953e-05 7.5449861e-05 7.8278114e-05 7.8431491e-05 7.6470787e-05 7.2996823e-05 6.9883237e-05 6.7022171e-05 6.4426713e-05 6.3082567e-05]]...]
INFO - root - 2017-12-09 06:58:29.027561: step 1310, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:38m:21s remains)
INFO - root - 2017-12-09 06:58:35.379830: step 1320, loss = 0.90, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:36m:05s remains)
INFO - root - 2017-12-09 06:58:41.839808: step 1330, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:03m:17s remains)
INFO - root - 2017-12-09 06:58:48.294164: step 1340, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:36m:10s remains)
INFO - root - 2017-12-09 06:58:54.844349: step 1350, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:22m:53s remains)
INFO - root - 2017-12-09 06:59:01.544024: step 1360, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 70h:37m:20s remains)
INFO - root - 2017-12-09 06:59:08.078958: step 1370, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:40m:41s remains)
INFO - root - 2017-12-09 06:59:14.619161: step 1380, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:51m:10s remains)
INFO - root - 2017-12-09 06:59:20.774456: step 1390, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:41m:12s remains)
INFO - root - 2017-12-09 06:59:27.103647: step 1400, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:16m:25s remains)
2017-12-09 06:59:27.742637: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.003768946 0.003978732 0.0040446767 0.0039671129 0.0037629264 0.0034816968 0.0031999869 0.0029341083 0.0026965688 0.0025073432 0.0024188368 0.0024676232 0.0026614522 0.002940299 0.0031735287][0.0042761862 0.0045306762 0.0046288786 0.0045760004 0.0043993555 0.0041450514 0.0038744789 0.0035596071 0.0032084191 0.0028941226 0.0026781177 0.002605241 0.0027224377 0.0029760404 0.0032590546][0.0045996327 0.00487493 0.0050062858 0.0050103255 0.004897201 0.0047205803 0.0044947285 0.0041452362 0.0036769987 0.0032131036 0.0028342274 0.0025863391 0.0025608742 0.002742819 0.0030544659][0.0047110342 0.0050046546 0.0051743602 0.0052647204 0.0052507436 0.0051831268 0.0050298264 0.0046552476 0.004079842 0.0034744153 0.0029237985 0.0024893556 0.0022929844 0.0023662162 0.0026679584][0.0046013328 0.0049033412 0.0051129935 0.0052984906 0.0053961151 0.0054475064 0.00539629 0.0050086263 0.0043523991 0.0036286816 0.0029195794 0.0023112395 0.0019583332 0.0019282085 0.0021993702][0.004254282 0.0045497641 0.0047918474 0.0050577396 0.0052701109 0.0054343934 0.0054669734 0.0050921445 0.0044026617 0.0035995902 0.0027820405 0.0020627652 0.0016055267 0.0015006312 0.0017456909][0.0036928975 0.0039730608 0.0042450279 0.0045799934 0.0048756367 0.0051193219 0.0051907245 0.0048492523 0.004174402 0.0033607273 0.0025197924 0.0017815621 0.0012960214 0.0011596278 0.001394161][0.0029793058 0.0032425318 0.0035351762 0.0039092423 0.0042611258 0.0045341072 0.0046118251 0.004313759 0.0036952815 0.0029435661 0.0021625471 0.0014879323 0.0010442829 0.00092498551 0.0011617896][0.0022152634 0.0024467716 0.0027352446 0.0031166831 0.0034774803 0.0037454914 0.0038189273 0.0035788855 0.003052769 0.0024114444 0.0017524621 0.0011978301 0.00083672139 0.00075941929 0.0010041664][0.0015235281 0.0017061078 0.0019589409 0.0023048066 0.0026396082 0.0028785577 0.0029490769 0.0027823432 0.002376057 0.0018705435 0.0013610629 0.00093903946 0.00066496868 0.000625244 0.00085650053][0.00096941547 0.0011079136 0.0012998349 0.001575641 0.001849505 0.0020435681 0.0021110957 0.0020074763 0.0017286402 0.0013721556 0.00102427 0.00073111418 0.00053666678 0.00051500247 0.0007019086][0.00057571137 0.00067051913 0.00079164305 0.00097722572 0.00117267 0.00131626 0.0013782049 0.0013336123 0.0011785976 0.00096408889 0.00075645448 0.00057644746 0.00044621271 0.00042819619 0.0005478785][0.00033535346 0.00038936504 0.00044827422 0.00054504676 0.00065848872 0.00075489911 0.00080931728 0.00081403647 0.00075674366 0.00065805315 0.00055370358 0.00045481612 0.00036823316 0.000340844 0.00039394558][0.00021031748 0.00023690394 0.00025672824 0.0002918122 0.00034208322 0.00039539448 0.00043717676 0.0004656236 0.00046709957 0.00044343181 0.0004021104 0.00035439525 0.00029902431 0.00026559297 0.0002692822][0.00014188084 0.00015640192 0.00016058382 0.00016678695 0.0001799626 0.00020423134 0.00023146042 0.00026045716 0.00028186615 0.00029222545 0.00028545773 0.0002669573 0.00023580789 0.00020587025 0.00018903162]]...]
INFO - root - 2017-12-09 06:59:34.149326: step 1410, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:47m:21s remains)
INFO - root - 2017-12-09 06:59:40.711882: step 1420, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 59h:57m:54s remains)
INFO - root - 2017-12-09 06:59:46.924756: step 1430, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:06m:17s remains)
INFO - root - 2017-12-09 06:59:53.318636: step 1440, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:34m:16s remains)
INFO - root - 2017-12-09 06:59:59.735144: step 1450, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:21m:53s remains)
INFO - root - 2017-12-09 07:00:06.279982: step 1460, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 59h:56m:16s remains)
INFO - root - 2017-12-09 07:00:14.193430: step 1470, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 66h:11m:05s remains)
INFO - root - 2017-12-09 07:00:22.147613: step 1480, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:40m:43s remains)
INFO - root - 2017-12-09 07:00:30.580253: step 1490, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.987 sec/batch; 90h:47m:14s remains)
INFO - root - 2017-12-09 07:00:39.231143: step 1500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 77h:17m:20s remains)
2017-12-09 07:00:40.155721: I tensorflow/core/kernels/logging_ops.cc:79] [[[5.4878808e-05 5.3323463e-05 4.9870083e-05 4.9520269e-05 5.1299008e-05 5.1715535e-05 5.2894946e-05 5.3796975e-05 5.3514923e-05 5.4509394e-05 5.5430661e-05 5.5349221e-05 5.4834876e-05 5.3805277e-05 5.1352312e-05][5.4558383e-05 5.3758216e-05 5.1764808e-05 5.2250471e-05 5.3624157e-05 5.3564028e-05 5.4205717e-05 5.5762335e-05 5.5815442e-05 5.6409765e-05 5.7937454e-05 5.6096469e-05 5.5770506e-05 5.4403812e-05 5.3698765e-05][5.27613e-05 5.2944648e-05 5.2591291e-05 5.3192365e-05 5.446573e-05 5.4469921e-05 5.4829768e-05 5.7368539e-05 5.86889e-05 6.0535742e-05 6.363805e-05 6.0882812e-05 5.9482161e-05 5.9799851e-05 5.9656908e-05][5.1632407e-05 5.2111878e-05 5.1446077e-05 5.179357e-05 5.3776479e-05 5.504211e-05 5.7353747e-05 6.25857e-05 6.5503678e-05 6.66436e-05 6.9741727e-05 6.7923735e-05 6.5686276e-05 6.689773e-05 6.7164932e-05][5.0851602e-05 5.191427e-05 5.2282703e-05 5.276606e-05 5.4349883e-05 5.6707839e-05 6.1314575e-05 6.8356421e-05 7.1066934e-05 7.2564268e-05 7.5378484e-05 7.3229188e-05 7.2078641e-05 7.4747659e-05 7.1784591e-05][5.2151263e-05 5.1634182e-05 5.1579045e-05 5.254423e-05 5.4294418e-05 5.8353442e-05 6.4510918e-05 7.0874441e-05 7.3246483e-05 7.614497e-05 8.1287035e-05 8.3176179e-05 8.5380751e-05 8.4678148e-05 7.6915443e-05][5.0991097e-05 5.0541072e-05 5.0903684e-05 5.3100346e-05 5.6023855e-05 6.0589395e-05 6.694784e-05 7.27733e-05 7.6375851e-05 8.01908e-05 8.8188e-05 9.82698e-05 0.00010112915 9.69659e-05 8.4765161e-05][4.9150054e-05 4.9910879e-05 5.0862029e-05 5.3044081e-05 5.634952e-05 6.1658793e-05 6.7525827e-05 7.3491334e-05 7.9642421e-05 8.7413056e-05 9.9028119e-05 0.00011186385 0.00011602786 0.00010894414 9.3440183e-05][5.11143e-05 5.2343908e-05 5.161258e-05 5.1713207e-05 5.4268756e-05 5.9060185e-05 6.51109e-05 7.2527866e-05 8.2102619e-05 9.4834541e-05 0.00010969728 0.0001235588 0.00012635798 0.00011714404 9.719326e-05][5.16984e-05 5.2521493e-05 5.0113813e-05 4.8999864e-05 5.1518145e-05 5.5296041e-05 6.269286e-05 7.24007e-05 8.6043008e-05 0.0001020642 0.00011902655 0.00012873678 0.00012559873 0.00011441958 9.5692718e-05][4.8664871e-05 4.9845061e-05 4.8773705e-05 4.8146379e-05 5.0596718e-05 5.4281911e-05 6.060765e-05 7.0976588e-05 8.7510205e-05 0.00010356602 0.00011904523 0.00012114893 0.00011179585 0.00010052153 8.6987369e-05][4.9039714e-05 5.0133909e-05 4.9201168e-05 4.8825808e-05 5.1856172e-05 5.5056684e-05 5.8669109e-05 6.7664e-05 8.10745e-05 9.3468894e-05 0.00010342941 0.00010249061 9.260996e-05 8.3643783e-05 7.4955671e-05][5.0741153e-05 5.2228082e-05 5.1336268e-05 5.0731127e-05 5.1676274e-05 5.3276533e-05 5.6897406e-05 6.356927e-05 7.29069e-05 8.01041e-05 8.5051295e-05 8.4416824e-05 7.6426048e-05 6.9233778e-05 6.5218272e-05][4.9954571e-05 5.1345072e-05 5.2579744e-05 5.2646676e-05 5.3108786e-05 5.3848489e-05 5.4347314e-05 5.8579426e-05 6.4524756e-05 6.7908368e-05 7.0480812e-05 6.9832167e-05 6.5410975e-05 6.1444764e-05 5.9043421e-05][4.8528775e-05 5.1358606e-05 5.3746553e-05 5.4359683e-05 5.4388838e-05 5.3914402e-05 5.298412e-05 5.41623e-05 5.7694982e-05 5.9971564e-05 6.2389518e-05 6.2094921e-05 6.0820668e-05 5.7548124e-05 5.6193843e-05]]...]
INFO - root - 2017-12-09 07:00:48.825165: step 1510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:13m:01s remains)
INFO - root - 2017-12-09 07:00:57.261469: step 1520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:19m:00s remains)
INFO - root - 2017-12-09 07:01:05.734068: step 1530, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 83h:24m:34s remains)
INFO - root - 2017-12-09 07:01:14.447439: step 1540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 79h:00m:08s remains)
INFO - root - 2017-12-09 07:01:22.947312: step 1550, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:49m:01s remains)
INFO - root - 2017-12-09 07:01:31.356020: step 1560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:48m:50s remains)
INFO - root - 2017-12-09 07:01:39.925583: step 1570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:24m:22s remains)
INFO - root - 2017-12-09 07:01:48.396433: step 1580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:28m:19s remains)
INFO - root - 2017-12-09 07:01:56.963108: step 1590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:04m:05s remains)
INFO - root - 2017-12-09 07:02:05.700700: step 1600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:44m:12s remains)
2017-12-09 07:02:06.564268: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00013621243 0.00019128296 0.00025287442 0.0003114327 0.00035912855 0.0003922758 0.00040953505 0.00041261068 0.00040423445 0.0003903434 0.00037152096 0.00034453889 0.00030886216 0.0002681436 0.00022529716][0.00019528951 0.0002913035 0.00039952155 0.00050362147 0.00059117289 0.00065276562 0.00068640395 0.0006956092 0.00068663486 0.00066683226 0.00063874049 0.00059448689 0.00053141621 0.00045596453 0.00037543662][0.00026555723 0.00041429998 0.00058438914 0.00075289275 0.00090128405 0.0010089655 0.0010705219 0.0010936428 0.0010894747 0.0010669382 0.0010267484 0.00095897721 0.00085797586 0.0007319508 0.00059310877][0.00034453123 0.00055574422 0.00080091611 0.0010519946 0.0012764595 0.0014446354 0.0015451605 0.0015887101 0.0015948988 0.0015712744 0.001518838 0.0014212871 0.0012720001 0.0010808123 0.000868445][0.00041847711 0.00069185707 0.0010135858 0.001350523 0.0016574122 0.0018921449 0.0020355433 0.0021012607 0.0021172157 0.002095551 0.0020346921 0.0019146823 0.0017215131 0.0014671652 0.001179281][0.00046025385 0.00077416445 0.0011517056 0.0015541126 0.0019283892 0.0022224833 0.0024131124 0.0025129328 0.0025558542 0.0025504003 0.0024969236 0.0023664848 0.0021411669 0.0018336645 0.0014769416][0.00047111316 0.00080337684 0.0012086517 0.0016479125 0.0020666397 0.0024076349 0.0026463708 0.0027923647 0.0028753085 0.0028908979 0.0028456973 0.0027160088 0.0024738952 0.0021251261 0.0017113811][0.00045498839 0.00077941123 0.0011802102 0.0016217452 0.0020493809 0.0024060346 0.0026701975 0.0028519905 0.0029690727 0.0030066003 0.0029755097 0.0028543284 0.0026137247 0.002252202 0.0018106228][0.00043202934 0.0007319728 0.0011049691 0.0015142134 0.001910332 0.0022453382 0.0024970553 0.002674581 0.0028005673 0.0028707469 0.0028743062 0.0027810275 0.0025581243 0.0022084026 0.0017726922][0.00043020357 0.00070986291 0.0010536017 0.001426809 0.0017876914 0.0020917803 0.0023199157 0.0024818694 0.0026037986 0.0026846444 0.0027093785 0.0026392771 0.0024394796 0.0021107341 0.0016905391][0.00043652699 0.0007081778 0.0010364511 0.0013899772 0.0017273038 0.0020072809 0.002213958 0.0023603523 0.0024679138 0.0025403788 0.0025658747 0.0025078584 0.0023268797 0.0020170847 0.0016148447][0.00043857 0.00070717518 0.0010271769 0.0013688272 0.0016909422 0.0019528953 0.0021385017 0.0022609977 0.0023452064 0.0024005477 0.0024175646 0.00236162 0.0021939331 0.0019087973 0.001538433][0.00042701213 0.00068317732 0.00098764815 0.001310469 0.0016123269 0.0018540614 0.0020173478 0.002114946 0.0021729551 0.0022028438 0.0022033884 0.0021457 0.0019929484 0.0017361852 0.0014071965][0.00038220335 0.000604213 0.00087256107 0.0011601553 0.0014294026 0.0016422489 0.0017814171 0.0018578863 0.0018939612 0.0019039186 0.0018905853 0.0018325624 0.0016991086 0.0014834105 0.0012081583][0.00029729336 0.00046807886 0.0006806259 0.00091246009 0.0011322671 0.0013076137 0.0014208336 0.001478535 0.0014997548 0.0014977137 0.0014775654 0.0014245525 0.0013167978 0.0011510944 0.00094244879]]...]
INFO - root - 2017-12-09 07:02:15.047712: step 1610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:13m:30s remains)
INFO - root - 2017-12-09 07:02:23.618940: step 1620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:51m:53s remains)
INFO - root - 2017-12-09 07:02:32.105102: step 1630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:27m:57s remains)
INFO - root - 2017-12-09 07:02:40.682737: step 1640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:49m:15s remains)
INFO - root - 2017-12-09 07:02:49.425873: step 1650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 77h:21m:53s remains)
INFO - root - 2017-12-09 07:02:58.264457: step 1660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:21m:36s remains)
INFO - root - 2017-12-09 07:03:06.965215: step 1670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:51m:30s remains)
INFO - root - 2017-12-09 07:03:15.547557: step 1680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:51m:15s remains)
INFO - root - 2017-12-09 07:03:24.271769: step 1690, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:25m:21s remains)
INFO - root - 2017-12-09 07:03:33.046072: step 1700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:33m:57s remains)
2017-12-09 07:03:33.931392: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0065928893 0.0071719461 0.0071240389 0.0065293126 0.0055289073 0.0043345923 0.003130415 0.0021105534 0.001361964 0.00089568482 0.00061789877 0.00044125281 0.00031565331 0.00022738203 0.00017323669][0.0083242385 0.009073887 0.0090801856 0.00843453 0.0072878865 0.0058918265 0.0044510341 0.0031903984 0.0022148027 0.0015539633 0.0011162175 0.00080334482 0.00056046969 0.00037916741 0.00026045914][0.0097292662 0.010655019 0.010772615 0.010180154 0.009034602 0.0075937137 0.0060612569 0.0046543092 0.0034916427 0.0026153536 0.0019539408 0.001419791 0.0009757643 0.00062616821 0.00039122603][0.01063729 0.011710958 0.011976831 0.011540469 0.010560077 0.0092536593 0.0078031104 0.006387223 0.0051135966 0.0040347679 0.0031131417 0.0022888053 0.0015622703 0.00097404694 0.0005725667][0.011019136 0.012164388 0.012578134 0.012382885 0.011686163 0.010664433 0.0094406232 0.0081497636 0.0068651284 0.0056385612 0.0044644484 0.0033272251 0.0022839436 0.0014189926 0.00081351277][0.01099788 0.012141303 0.012671016 0.012721283 0.012356206 0.01169428 0.010792198 0.0097137354 0.0084921541 0.0071719843 0.0057842443 0.0043615471 0.0030224929 0.0018946109 0.0010887866][0.010712192 0.011788637 0.012362682 0.012607215 0.012545894 0.012240428 0.01168954 0.010866767 0.0097519429 0.0083794473 0.0068239742 0.0051786108 0.00361224 0.002275686 0.0013119648][0.010248279 0.01120138 0.011736613 0.012075268 0.012211681 0.012170495 0.011905949 0.011325574 0.010330562 0.0089683244 0.0073409257 0.005584172 0.0038964518 0.0024529123 0.0014157627][0.0095571522 0.010350734 0.010793656 0.011141266 0.011358343 0.011450729 0.011333505 0.010894883 0.010029893 0.008754544 0.0071768868 0.0054692295 0.0038251439 0.0024193053 0.0014125668][0.0085374061 0.0091567468 0.0094873644 0.0097783115 0.00999227 0.0101311 0.010077838 0.0097248536 0.0089717591 0.0078295162 0.0064006858 0.0048694904 0.0034098066 0.0021781973 0.0013054532][0.007131346 0.0075984695 0.0078155166 0.0080129094 0.0081714224 0.0083005372 0.008275683 0.007993862 0.007366654 0.0064135473 0.0052157706 0.003950906 0.0027636965 0.0017774802 0.0010890026][0.0054704836 0.0058016842 0.0059256656 0.0060291495 0.0061270706 0.0062210243 0.0062052729 0.0059968671 0.0055227983 0.0047962903 0.0038887491 0.0029370678 0.0020669745 0.0013501699 0.00085660239][0.0038379193 0.0040672561 0.0041166977 0.0041457126 0.0041798512 0.0042311247 0.0042183739 0.0040791477 0.003758152 0.0032633825 0.0026404336 0.0019989493 0.0014267911 0.00096544024 0.00065282919][0.0024319252 0.0025852779 0.0025871252 0.0025682354 0.0025630668 0.0025742778 0.0025568579 0.0024719152 0.0022830279 0.0019903723 0.0016183581 0.0012480324 0.00093139661 0.000675824 0.00049801][0.0014118062 0.0015198438 0.0015143579 0.0014772281 0.0014439254 0.0014239717 0.0013930155 0.00133473 0.0012314644 0.0010833025 0.00089951255 0.00072850403 0.00058887928 0.00047646504 0.00038784888]]...]
INFO - root - 2017-12-09 07:03:42.750190: step 1710, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 81h:18m:36s remains)
INFO - root - 2017-12-09 07:03:51.550561: step 1720, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:24m:18s remains)
INFO - root - 2017-12-09 07:04:00.234176: step 1730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:09m:04s remains)
INFO - root - 2017-12-09 07:04:08.972824: step 1740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:28m:05s remains)
INFO - root - 2017-12-09 07:04:17.710957: step 1750, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 81h:51m:36s remains)
INFO - root - 2017-12-09 07:04:26.306437: step 1760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:32m:40s remains)
INFO - root - 2017-12-09 07:04:34.840204: step 1770, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 76h:31m:18s remains)
INFO - root - 2017-12-09 07:04:43.251891: step 1780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 77h:24m:15s remains)
INFO - root - 2017-12-09 07:04:51.791462: step 1790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:27m:19s remains)
INFO - root - 2017-12-09 07:05:00.395843: step 1800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 81h:48m:51s remains)
2017-12-09 07:05:01.216003: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0019780276 0.0021013638 0.00215588 0.0021647399 0.002146346 0.0021033809 0.0020484924 0.0019914471 0.0019440185 0.0019107965 0.0018942785 0.00189788 0.0019141961 0.0019336538 0.0019509264][0.002189653 0.0023692676 0.002466691 0.0025081334 0.0025040403 0.0024591333 0.0023875134 0.0023055351 0.0022313709 0.0021771316 0.0021462468 0.002142455 0.0021578649 0.0021830187 0.0022066212][0.0021033168 0.0023433152 0.0024944057 0.0025787288 0.0026050545 0.0025726452 0.0024913209 0.0023891241 0.0022897907 0.0022129042 0.00217181 0.0021647124 0.0021833132 0.0022125775 0.0022405826][0.0017997222 0.002085516 0.002305418 0.0024634006 0.0025436287 0.0025416133 0.0024652444 0.0023482593 0.0022243711 0.0021196262 0.0020600206 0.0020471879 0.0020690842 0.0021043813 0.0021386407][0.0014011238 0.001717942 0.0020031286 0.0022438134 0.0023947407 0.0024406856 0.0023879665 0.0022736478 0.0021329578 0.0020066011 0.0019246576 0.0018997724 0.0019154297 0.0019456777 0.0019768917][0.0010859685 0.0014154558 0.0017472666 0.0020491607 0.0022634568 0.0023665649 0.0023567458 0.0022588563 0.0021127777 0.0019637751 0.0018586371 0.0018096269 0.0018022342 0.001819251 0.0018404259][0.0009057459 0.0012319321 0.0015820801 0.0019218931 0.002186399 0.0023438365 0.0023820752 0.0023183068 0.0021740443 0.002002262 0.001865748 0.0017827275 0.0017466092 0.0017420586 0.0017499712][0.00083408551 0.0011365337 0.0014738437 0.0018220482 0.0021160021 0.0023124744 0.002395262 0.0023687738 0.0022381679 0.0020561467 0.0018841443 0.001762406 0.0016901995 0.0016642998 0.0016608095][0.00078928564 0.0010574068 0.0013610488 0.0016812743 0.0019728218 0.0021892521 0.0023017887 0.0023049817 0.0021922586 0.0020079312 0.0018142598 0.001658794 0.0015594596 0.0015185839 0.001511181][0.00070526911 0.00092973775 0.0011833236 0.0014550543 0.0017134368 0.0019257516 0.0020593791 0.002086103 0.0019956143 0.0018184951 0.0016177986 0.0014399274 0.0013186821 0.0012619079 0.0012466419][0.00057349313 0.00075013383 0.000947358 0.001156139 0.0013596194 0.0015379848 0.001664779 0.0016964008 0.0016341826 0.0014891139 0.0013000608 0.0011218045 0.00099036959 0.00091647956 0.00089665438][0.00041695361 0.00054347457 0.00068276224 0.0008282403 0.00097019435 0.0010935263 0.0011821956 0.001214202 0.001182094 0.0010774854 0.00093260995 0.00078543875 0.00067085231 0.00060119829 0.00058508822][0.00030564779 0.00038341543 0.00047033542 0.00056178856 0.00064609683 0.00071565149 0.00076645793 0.000789541 0.00077513774 0.00071603782 0.00062883459 0.00053467427 0.00045625778 0.00040720796 0.00039470763][0.00031932187 0.00036383962 0.00041254761 0.00045875952 0.00050059333 0.00053719716 0.00056144537 0.00056795857 0.0005580904 0.00053138088 0.00048964546 0.00044306988 0.00040126458 0.00037445678 0.00036662721][0.00047035355 0.0004987006 0.0005226123 0.00054431963 0.00056606426 0.00058180239 0.00058941735 0.00058730255 0.00057843956 0.00056549214 0.00054664706 0.00052832888 0.00051203941 0.00050171424 0.00049483369]]...]
INFO - root - 2017-12-09 07:05:09.779010: step 1810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:06m:01s remains)
INFO - root - 2017-12-09 07:05:18.478882: step 1820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:38m:38s remains)
INFO - root - 2017-12-09 07:05:27.058414: step 1830, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.734 sec/batch; 67h:26m:25s remains)
INFO - root - 2017-12-09 07:05:35.629896: step 1840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 77h:25m:04s remains)
INFO - root - 2017-12-09 07:05:44.248785: step 1850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:17m:54s remains)
INFO - root - 2017-12-09 07:05:52.877336: step 1860, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.788 sec/batch; 72h:24m:13s remains)
INFO - root - 2017-12-09 07:06:01.469722: step 1870, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 77h:28m:12s remains)
INFO - root - 2017-12-09 07:06:09.997557: step 1880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:01m:11s remains)
INFO - root - 2017-12-09 07:06:18.526068: step 1890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:02m:59s remains)
INFO - root - 2017-12-09 07:06:27.166364: step 1900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:52m:32s remains)
2017-12-09 07:06:28.160649: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.7641493e-05 7.1040842e-05 7.3106814e-05 7.5116419e-05 7.7588782e-05 8.0129372e-05 8.2802246e-05 8.4172352e-05 8.465541e-05 8.4477251e-05 8.379091e-05 8.3047416e-05 8.2589257e-05 8.2448707e-05 8.27021e-05][7.5426622e-05 7.9284655e-05 8.0967191e-05 8.2260638e-05 8.339739e-05 8.4558189e-05 8.5798129e-05 8.6159467e-05 8.6187159e-05 8.5864e-05 8.5119544e-05 8.4425723e-05 8.4610911e-05 8.5417487e-05 8.6693071e-05][8.3473577e-05 8.7411012e-05 8.8336274e-05 8.8368404e-05 8.7741108e-05 8.6978391e-05 8.6736014e-05 8.6210639e-05 8.5685308e-05 8.54879e-05 8.5500331e-05 8.5966276e-05 8.7474436e-05 8.983061e-05 9.2524482e-05][9.2344606e-05 9.506652e-05 9.419339e-05 9.2179063e-05 8.9940433e-05 8.7699329e-05 8.6062209e-05 8.5072134e-05 8.4658954e-05 8.5061612e-05 8.6093016e-05 8.8011e-05 9.1190952e-05 9.5253257e-05 9.9629557e-05][9.9161276e-05 0.00010038877 9.7426186e-05 9.3756178e-05 9.0329588e-05 8.7648375e-05 8.5938511e-05 8.5340922e-05 8.5563559e-05 8.65594e-05 8.8534434e-05 9.169949e-05 9.6112606e-05 0.00010116553 0.00010637059][0.00010256545 0.00010271759 9.9280631e-05 9.5343639e-05 9.1869639e-05 8.9690271e-05 8.9102628e-05 8.9641064e-05 9.0762755e-05 9.234529e-05 9.4842573e-05 9.8098993e-05 0.0001021171 0.00010631423 0.00011051232][0.00010480162 0.00010579384 0.00010387252 0.00010213122 0.00010062461 0.00010036351 0.00010113102 0.00010238364 0.0001036622 0.00010451156 0.00010547088 0.00010671901 0.00010838701 0.0001101335 0.00011189722][0.0001057733 0.00010889013 0.00010953614 0.00011066729 0.00011198063 0.00011414201 0.00011671913 0.00011898285 0.0001197379 0.00011854416 0.00011670797 0.00011467344 0.0001131942 0.00011207235 0.00011161247][0.00010545491 0.00011048384 0.00011262978 0.0001151158 0.00011790593 0.00012109968 0.00012383191 0.00012594671 0.00012661991 0.00012515837 0.00012237245 0.00011870576 0.00011538526 0.00011279621 0.00011150787][0.00010440172 0.00011000859 0.00011221667 0.00011434805 0.00011690161 0.00011953801 0.00012167487 0.0001232152 0.00012370653 0.00012278564 0.0001207932 0.0001178457 0.00011512095 0.00011318895 0.00011256041][0.0001014181 0.0001069311 0.00010886863 0.0001106113 0.00011261673 0.00011456008 0.00011625514 0.00011767149 0.00011843342 0.00011840168 0.00011772296 0.0001164694 0.00011528188 0.00011451325 0.00011448404][9.5398209e-05 0.00010073087 0.00010265048 0.00010447876 0.00010645718 0.00010826898 0.00010999011 0.00011165509 0.00011288687 0.00011374497 0.00011423726 0.00011428631 0.00011419954 0.00011415379 0.00011431735][8.5877728e-05 9.1290669e-05 9.3789684e-05 9.6103686e-05 9.8527853e-05 0.00010066218 0.00010260587 0.00010437734 0.00010588309 0.00010706939 0.00010802763 0.00010862199 0.00010896453 0.0001091652 0.00010916539][7.50119e-05 8.0081882e-05 8.2553357e-05 8.4925894e-05 8.7421569e-05 8.9647714e-05 9.1741254e-05 9.3630406e-05 9.5203788e-05 9.6520293e-05 9.7528908e-05 9.8104952e-05 9.8456658e-05 9.8548313e-05 9.8451688e-05][6.4698863e-05 6.8929738e-05 7.1078808e-05 7.3310395e-05 7.5628028e-05 7.7745106e-05 7.9748439e-05 8.1513434e-05 8.302027e-05 8.4170526e-05 8.4970823e-05 8.542923e-05 8.5677981e-05 8.5715459e-05 8.5582673e-05]]...]
INFO - root - 2017-12-09 07:06:36.786216: step 1910, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:05m:16s remains)
INFO - root - 2017-12-09 07:06:45.268465: step 1920, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 77h:33m:39s remains)
INFO - root - 2017-12-09 07:06:53.892994: step 1930, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.923 sec/batch; 84h:46m:20s remains)
INFO - root - 2017-12-09 07:07:02.317947: step 1940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:10m:53s remains)
INFO - root - 2017-12-09 07:07:10.969088: step 1950, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 82h:08m:38s remains)
INFO - root - 2017-12-09 07:07:19.571864: step 1960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:05m:24s remains)
INFO - root - 2017-12-09 07:07:28.239504: step 1970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:47m:19s remains)
INFO - root - 2017-12-09 07:07:36.808300: step 1980, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 82h:37m:58s remains)
INFO - root - 2017-12-09 07:07:45.369294: step 1990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:39m:57s remains)
INFO - root - 2017-12-09 07:07:54.105502: step 2000, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:26m:37s remains)
2017-12-09 07:07:54.962022: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0036701653 0.0038478964 0.0039323648 0.0039724782 0.0039973245 0.0040223822 0.0040486292 0.0040795715 0.0041051521 0.0041133184 0.0040882346 0.0040257694 0.0039465134 0.0038603463 0.0037942608][0.0039959149 0.0042687794 0.0044432166 0.0045633893 0.0046533537 0.0047386871 0.0048247273 0.0049134945 0.0049845246 0.0050096218 0.0049578603 0.0048192125 0.00461774 0.0043920139 0.0041907686][0.0043653054 0.0047853575 0.0051027988 0.0053522997 0.0055504115 0.0057326816 0.005913327 0.0060892929 0.00622487 0.006269916 0.0061723934 0.005913463 0.0055287518 0.0050908942 0.0046788938][0.0049190465 0.0055464394 0.0060631023 0.0064766421 0.0067997011 0.0070848265 0.0073606581 0.007622296 0.0078220852 0.0078805927 0.007714577 0.0072980877 0.0066778883 0.005968614 0.0052845646][0.0056880573 0.0065929908 0.0073721893 0.007988913 0.0084552085 0.0088403579 0.0091869505 0.0094980868 0.0097168591 0.0097435089 0.0094731739 0.0088632815 0.0079756938 0.0069615957 0.0059785116][0.0066067879 0.00785749 0.0089591974 0.0098178433 0.010422784 0.010864655 0.011215823 0.011495262 0.01165682 0.01159666 0.011190251 0.010384019 0.0092458474 0.0079508452 0.0066900905][0.0076045473 0.0092443656 0.010692222 0.011794764 0.012509256 0.012946131 0.01319674 0.01332314 0.01331982 0.013100186 0.0125316 0.011557454 0.010230951 0.0087319212 0.0072686449][0.0084996028 0.010511992 0.012289559 0.013620803 0.014416892 0.014775501 0.014832832 0.014709376 0.014449205 0.014009467 0.013275855 0.012179269 0.010751118 0.00915747 0.0075999321][0.0091040609 0.011401898 0.013443141 0.014956199 0.015781509 0.016001573 0.01581306 0.015390846 0.014839292 0.014175287 0.013308056 0.012157942 0.010732146 0.0091620777 0.0076285307][0.0092458986 0.011685618 0.013871221 0.015469207 0.016269298 0.016342917 0.015923845 0.015232877 0.014433412 0.013594915 0.012660562 0.011546613 0.010221669 0.0087779267 0.0073702633][0.0088702533 0.011230493 0.013344733 0.014878541 0.0156255 0.015611131 0.015053837 0.01421304 0.013277574 0.012360879 0.011441903 0.010440534 0.0092981411 0.008065301 0.0068635247][0.0080555351 0.010125318 0.011966051 0.013320953 0.013988747 0.013951815 0.013387047 0.012534113 0.011601198 0.010717475 0.0098917531 0.0090562617 0.0081427684 0.007166123 0.006212174][0.0069565582 0.0086141471 0.010079284 0.011173821 0.01172775 0.011699732 0.011231069 0.010503229 0.0097025679 0.0089510921 0.0082788095 0.0076392195 0.0069653052 0.0062494865 0.0055484385][0.0058283056 0.0070399498 0.0081009744 0.0089057535 0.0093332911 0.0093369763 0.0090067443 0.0084704179 0.0078705922 0.0073038395 0.0068083107 0.0063614477 0.0059072319 0.0054285638 0.0049577188][0.0048759119 0.0056822766 0.0063739484 0.0069093644 0.0072157048 0.0072527337 0.0070672682 0.0067320471 0.0063369772 0.005951595 0.0056166407 0.0053289845 0.0050502704 0.004761145 0.0044790464]]...]
INFO - root - 2017-12-09 07:08:03.652500: step 2010, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:26m:22s remains)
INFO - root - 2017-12-09 07:08:12.270862: step 2020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:04m:14s remains)
INFO - root - 2017-12-09 07:08:20.871042: step 2030, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:09m:21s remains)
INFO - root - 2017-12-09 07:08:29.586700: step 2040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:11m:28s remains)
INFO - root - 2017-12-09 07:08:38.122612: step 2050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:32m:40s remains)
INFO - root - 2017-12-09 07:08:46.833615: step 2060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:15m:36s remains)
INFO - root - 2017-12-09 07:08:55.556450: step 2070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:45m:56s remains)
INFO - root - 2017-12-09 07:09:04.140619: step 2080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:59m:10s remains)
INFO - root - 2017-12-09 07:09:12.697036: step 2090, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:41m:50s remains)
INFO - root - 2017-12-09 07:09:21.175080: step 2100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:50m:40s remains)
2017-12-09 07:09:22.022624: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00060524192 0.00099329127 0.001408876 0.0017694462 0.0020387445 0.0022019441 0.0022641018 0.0022320207 0.00212623 0.0019488523 0.0017012245 0.0013817193 0.0010263177 0.00069728529 0.0004445978][0.0005328352 0.00090900541 0.0013193407 0.0016951037 0.0019941928 0.0021946805 0.0022968224 0.0022998196 0.0022127163 0.0020355107 0.0017765327 0.0014404074 0.001070139 0.0007234228 0.00045361242][0.00048555114 0.000838459 0.0012242823 0.0015851385 0.0018884416 0.0021101364 0.0022426203 0.0022791252 0.0022122781 0.0020367529 0.0017693406 0.0014328257 0.0010678991 0.00073270791 0.00047110056][0.00047832564 0.0008047435 0.0011607779 0.001498607 0.001796271 0.0020353128 0.0022052049 0.002281239 0.0022429891 0.0020733653 0.0017949009 0.0014455945 0.0010746146 0.00073764566 0.00047591474][0.00049162121 0.00078372151 0.0011007864 0.0014097128 0.001699628 0.0019642178 0.0021777526 0.0023003884 0.0022946626 0.0021318872 0.0018404888 0.0014721444 0.0010861815 0.0007426099 0.00047766772][0.00051410904 0.000767792 0.0010389494 0.0013097598 0.0015858874 0.0018702748 0.0021385672 0.0023184388 0.0023505036 0.0021985446 0.001892707 0.0014987327 0.0010938905 0.00073749915 0.00046947139][0.00055067893 0.00076093955 0.00097603194 0.0011921334 0.0014361377 0.0017310715 0.0020514952 0.0023019996 0.00238155 0.0022483224 0.0019382669 0.0015235011 0.0010986504 0.00072989339 0.00045774702][0.00059747207 0.00075880293 0.00091328844 0.0010655097 0.0012650066 0.0015501808 0.0018993818 0.0022104238 0.0023504258 0.002254054 0.0019584752 0.0015442475 0.001105106 0.00072261528 0.000443828][0.00064366404 0.00076166284 0.00086303317 0.00096038933 0.0011124781 0.0013733429 0.001726693 0.0020727026 0.0022602892 0.0022126872 0.0019461166 0.0015461907 0.0011067746 0.00071819272 0.00043209194][0.00067951373 0.00076051237 0.00082137814 0.00088036968 0.00099470851 0.0012221934 0.0015569212 0.0019110831 0.0021302057 0.0021259424 0.001904301 0.0015345184 0.001110434 0.00072695623 0.00043531397][0.0006878071 0.00074107217 0.00077363505 0.00081003795 0.00089828257 0.0010901875 0.0013883921 0.0017220546 0.001952223 0.0019823876 0.0018135492 0.0014931579 0.0011066176 0.00074396038 0.00045799243][0.00067299476 0.00070381642 0.00071833486 0.00074143137 0.00080818014 0.00096571579 0.0012171989 0.0015113706 0.0017331506 0.0017862744 0.0016635383 0.0014056939 0.0010780814 0.00075720262 0.00049072335][0.00063218514 0.00064792542 0.00065531232 0.00067162636 0.00072468008 0.00084691885 0.0010463613 0.001288183 0.0014779641 0.0015399689 0.0014596115 0.001266974 0.0010131943 0.00075061322 0.00052072288][0.00058006728 0.00058331329 0.00058500841 0.00059652654 0.00063927687 0.00073468161 0.00088645378 0.0010671869 0.0012141342 0.0012702824 0.0012190206 0.0010848781 0.00090461888 0.00071492634 0.0005393087][0.00052665861 0.00052160665 0.00051811774 0.00052990974 0.00056747819 0.00064193824 0.00075242628 0.00087805989 0.00097815273 0.0010193053 0.00098281179 0.00089084031 0.00077158678 0.00065032911 0.00053477148]]...]
INFO - root - 2017-12-09 07:09:30.493830: step 2110, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 77h:05m:36s remains)
INFO - root - 2017-12-09 07:09:39.138223: step 2120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:19m:08s remains)
INFO - root - 2017-12-09 07:09:47.771775: step 2130, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 81h:25m:55s remains)
INFO - root - 2017-12-09 07:09:56.411967: step 2140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:37m:17s remains)
INFO - root - 2017-12-09 07:10:05.168389: step 2150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:19m:12s remains)
INFO - root - 2017-12-09 07:10:13.763751: step 2160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:16m:11s remains)
INFO - root - 2017-12-09 07:10:22.457686: step 2170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:19m:15s remains)
INFO - root - 2017-12-09 07:10:30.958445: step 2180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:33m:23s remains)
INFO - root - 2017-12-09 07:10:39.478590: step 2190, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 68h:26m:20s remains)
INFO - root - 2017-12-09 07:10:48.083170: step 2200, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 74h:22m:13s remains)
2017-12-09 07:10:48.902321: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.4863027e-05 3.7310463e-05 3.7245256e-05 3.7334896e-05 3.7418278e-05 3.7291036e-05 3.7168094e-05 3.7094338e-05 3.70852e-05 3.7181329e-05 3.7048427e-05 3.6975995e-05 3.6774531e-05 3.6519043e-05 3.6211939e-05][3.6531266e-05 3.8751496e-05 3.878861e-05 3.8770486e-05 3.8956729e-05 3.9067105e-05 3.9225764e-05 3.9251834e-05 3.9332117e-05 3.9324193e-05 3.8878359e-05 3.8566519e-05 3.8221602e-05 3.7767139e-05 3.7449776e-05][3.6187681e-05 3.7967824e-05 3.8247854e-05 3.7996295e-05 3.7994381e-05 3.8245671e-05 3.8207058e-05 3.8441729e-05 3.8457372e-05 3.8529652e-05 3.8340047e-05 3.8244158e-05 3.7753838e-05 3.696663e-05 3.6422389e-05][3.4162193e-05 3.5425786e-05 3.5343495e-05 3.5151403e-05 3.4966542e-05 3.492659e-05 3.4792545e-05 3.5610843e-05 3.6212368e-05 3.63895e-05 3.6622943e-05 3.6886449e-05 3.6421268e-05 3.5250348e-05 3.4452038e-05][3.3295262e-05 3.4354111e-05 3.4034863e-05 3.3912547e-05 3.3653807e-05 3.2837233e-05 3.2069765e-05 3.2927579e-05 3.4033772e-05 3.4621946e-05 3.5043129e-05 3.5708261e-05 3.5584781e-05 3.4599776e-05 3.3720396e-05][3.7915881e-05 3.8480466e-05 3.8360202e-05 3.797176e-05 3.7196958e-05 3.6737161e-05 3.6527505e-05 3.7209393e-05 3.8460646e-05 3.8456106e-05 3.9052764e-05 3.9714731e-05 3.9135688e-05 3.7983205e-05 3.7172191e-05][4.2385211e-05 4.192294e-05 4.228839e-05 4.2807646e-05 4.2297943e-05 4.2324529e-05 4.3418506e-05 4.6359091e-05 4.880334e-05 4.7195186e-05 4.5279136e-05 4.4687295e-05 4.3198444e-05 4.1933388e-05 4.0711508e-05][3.7258535e-05 3.6276913e-05 3.5797282e-05 3.5321696e-05 3.39971e-05 3.453128e-05 3.592101e-05 3.8139304e-05 4.0329156e-05 4.0312843e-05 3.9259809e-05 3.8082697e-05 3.6044563e-05 3.4862591e-05 3.4448523e-05][3.0302444e-05 2.9415054e-05 2.8602431e-05 2.763363e-05 2.6772277e-05 2.6747182e-05 2.7106567e-05 2.7421678e-05 2.7772376e-05 2.8403249e-05 2.9567627e-05 3.028695e-05 2.9764846e-05 2.9908859e-05 3.0796269e-05][2.8475564e-05 2.7917735e-05 2.7894288e-05 2.7671964e-05 2.7652162e-05 2.7604163e-05 2.7099335e-05 2.6057807e-05 2.5197218e-05 2.5253115e-05 2.6329491e-05 2.74418e-05 2.7667576e-05 2.7914341e-05 2.9051118e-05][3.1272466e-05 3.1653952e-05 3.1988042e-05 3.16641e-05 3.1299234e-05 3.1041447e-05 3.1145319e-05 3.0514746e-05 3.0252331e-05 3.0153902e-05 3.031011e-05 2.9561254e-05 2.8608187e-05 2.8704706e-05 2.8941411e-05][3.2623058e-05 3.3068056e-05 3.3244716e-05 3.3096592e-05 3.298813e-05 3.257762e-05 3.29198e-05 3.304128e-05 3.3049662e-05 3.3136603e-05 3.2960437e-05 3.2688098e-05 3.1877767e-05 3.1098294e-05 3.0934338e-05][3.1954463e-05 3.2755488e-05 3.3196244e-05 3.3276541e-05 3.3332741e-05 3.3332275e-05 3.3302196e-05 3.2912925e-05 3.2375741e-05 3.2158e-05 3.2086602e-05 3.2243363e-05 3.2362761e-05 3.2258744e-05 3.2418087e-05][3.2234224e-05 3.3031123e-05 3.3051139e-05 3.3139469e-05 3.3197131e-05 3.3395525e-05 3.3544224e-05 3.3198849e-05 3.2588367e-05 3.2155171e-05 3.182028e-05 3.1867996e-05 3.2313103e-05 3.251063e-05 3.2713891e-05][3.3252851e-05 3.4068784e-05 3.3876036e-05 3.3870143e-05 3.4072662e-05 3.4142402e-05 3.434869e-05 3.4441233e-05 3.4133816e-05 3.369624e-05 3.3110904e-05 3.2592907e-05 3.2204916e-05 3.1943302e-05 3.2105912e-05]]...]
INFO - root - 2017-12-09 07:10:57.510515: step 2210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:14m:39s remains)
INFO - root - 2017-12-09 07:11:06.154432: step 2220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:16m:41s remains)
INFO - root - 2017-12-09 07:11:14.604941: step 2230, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 76h:27m:08s remains)
INFO - root - 2017-12-09 07:11:22.924339: step 2240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:27m:37s remains)
INFO - root - 2017-12-09 07:11:31.466279: step 2250, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 74h:17m:57s remains)
INFO - root - 2017-12-09 07:11:40.043112: step 2260, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:56m:45s remains)
INFO - root - 2017-12-09 07:11:48.560471: step 2270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:52m:28s remains)
INFO - root - 2017-12-09 07:11:57.063066: step 2280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:35m:37s remains)
INFO - root - 2017-12-09 07:12:05.828425: step 2290, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.920 sec/batch; 84h:21m:20s remains)
INFO - root - 2017-12-09 07:12:14.361869: step 2300, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:29m:19s remains)
2017-12-09 07:12:15.255679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0014517488 0.0013823996 0.0014843821 0.0018130172 0.0023517348 0.0030282843 0.0037840249 0.0046038125 0.0054849391 0.0063378876 0.0071006417 0.0077158823 0.0081139477 0.008251721 0.0080977678][0.0011418532 0.0011181126 0.0012975701 0.0017636914 0.0024884311 0.0033634349 0.0043113721 0.0053325533 0.0064046672 0.0073970361 0.0082020024 0.0087428214 0.0090065561 0.0089933565 0.0087000625][0.00080785691 0.00084008143 0.0011161609 0.001734176 0.0026321905 0.0036957245 0.0048408392 0.0060772053 0.0073381197 0.0084413346 0.0092377579 0.0096241087 0.0096732108 0.0094366772 0.0089779729][0.00068907847 0.00077532529 0.0011188658 0.0017979265 0.0027761473 0.0039655031 0.005274612 0.0067010405 0.0081128879 0.009272675 0.010016802 0.010274857 0.010142362 0.0097174812 0.0091213845][0.00068295753 0.00082710909 0.0012200609 0.0019466842 0.0029742513 0.0042667454 0.0057248883 0.007306797 0.0088041583 0.0099677378 0.01061843 0.010726437 0.010425218 0.0098490966 0.0091540981][0.00075719546 0.00095166377 0.0013823695 0.0021627003 0.0032849975 0.0047315457 0.0063586342 0.0080581056 0.0095539745 0.010612158 0.011087621 0.010996873 0.010522166 0.0098253693 0.0090591861][0.0010162893 0.0012766669 0.0017857369 0.0026714224 0.0039092568 0.0055076955 0.0072552436 0.0089798793 0.010355222 0.011199676 0.011440066 0.011132304 0.010491401 0.00969241 0.0088857561][0.0015491502 0.0019306799 0.0025452781 0.0035257675 0.0048366403 0.006486299 0.00820302 0.0098023377 0.010973187 0.0115707 0.011583822 0.011095652 0.010332198 0.0094742728 0.0086655449][0.0024161397 0.0029435395 0.0036525559 0.0046608141 0.0059387991 0.0074857255 0.0090264017 0.010389167 0.011303036 0.011662002 0.01148815 0.010892428 0.01008263 0.0092210863 0.00844003][0.0036210676 0.004209179 0.0049110181 0.00588785 0.0070630056 0.0084254332 0.009724522 0.010807389 0.011451993 0.0115869 0.011247553 0.01056734 0.0097430963 0.0089166425 0.0081986673][0.0050815633 0.0056351335 0.0062489491 0.0070708315 0.0080410149 0.009175444 0.010214116 0.011009703 0.011393392 0.011324973 0.010876378 0.010170324 0.0093727456 0.0086150961 0.00798442][0.0066898293 0.0071525467 0.0076180832 0.0082231322 0.0089280019 0.0097528705 0.010479438 0.010987177 0.01113359 0.010909227 0.01040736 0.0097282175 0.0090181082 0.0083591342 0.0078278305][0.0081849135 0.0085402345 0.0088312123 0.0091869589 0.0095893843 0.010063245 0.010458055 0.01068689 0.010647781 0.0103553 0.0098765073 0.0092911208 0.008706254 0.0081605157 0.0077337706][0.0093739266 0.0095888209 0.0096836407 0.0097758947 0.0098804021 0.010033559 0.010156026 0.010192158 0.010055892 0.009758682 0.0093448572 0.0088858707 0.00844385 0.008019696 0.007692493][0.010078248 0.010112081 0.010006732 0.0098945685 0.0097901449 0.0097447736 0.0096990345 0.0096242111 0.0094688181 0.0092138248 0.0088877417 0.0085401488 0.008214822 0.0079212449 0.0076845908]]...]
INFO - root - 2017-12-09 07:12:23.932699: step 2310, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 82h:23m:36s remains)
INFO - root - 2017-12-09 07:12:32.384635: step 2320, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:27m:16s remains)
INFO - root - 2017-12-09 07:12:40.754005: step 2330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:53m:10s remains)
INFO - root - 2017-12-09 07:12:49.239953: step 2340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 79h:05m:20s remains)
INFO - root - 2017-12-09 07:12:57.812467: step 2350, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:26m:56s remains)
INFO - root - 2017-12-09 07:13:06.479057: step 2360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 77h:08m:38s remains)
INFO - root - 2017-12-09 07:13:15.068080: step 2370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:21m:14s remains)
INFO - root - 2017-12-09 07:13:23.587648: step 2380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:59m:42s remains)
INFO - root - 2017-12-09 07:13:32.391272: step 2390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:15m:10s remains)
INFO - root - 2017-12-09 07:13:41.017601: step 2400, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:26m:35s remains)
2017-12-09 07:13:41.899064: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.7967548e-05 5.479539e-05 8.0988815e-05 0.00011467609 0.00014926994 0.00017142938 0.00017391861 0.0001549162 0.00012378374 9.1567847e-05 6.5090353e-05 4.6936912e-05 3.6221492e-05 3.1551761e-05 2.9885263e-05][6.3958694e-05 0.00010921652 0.00018064227 0.00027108137 0.00036245707 0.00042529797 0.00043882435 0.00039709746 0.0003167949 0.00022659318 0.00014741105 9.0319219e-05 5.7177989e-05 4.0970597e-05 3.4668097e-05][0.00011664226 0.00021245507 0.00035918082 0.00054071995 0.00072376814 0.00085983571 0.00090649386 0.00084718334 0.00069942744 0.00051316869 0.00033813485 0.00019991431 0.00011273348 6.498037e-05 4.3840882e-05][0.00019712464 0.00036366654 0.00060676935 0.00090585387 0.0012084866 0.0014504115 0.0015585872 0.0014979389 0.0012849254 0.00098656339 0.00067718362 0.00041211487 0.00022821837 0.00011930155 6.6784196e-05][0.00029088152 0.00053482474 0.00087776163 0.0012954014 0.0017262686 0.0020911379 0.0022915802 0.0022639413 0.0020157085 0.0016186077 0.001165884 0.00074678549 0.00042698934 0.00022260378 0.00011408716][0.00036482245 0.00067123928 0.0010946139 0.001605718 0.0021411092 0.0026187878 0.0029311159 0.0029814246 0.0027521197 0.00230623 0.0017440817 0.0011792345 0.0007072665 0.00037890874 0.00018728417][0.00038704675 0.00072095887 0.0011844556 0.0017468294 0.00234231 0.0029021073 0.0033204542 0.0034894713 0.0033419577 0.0029175791 0.0023095093 0.0016386551 0.0010307678 0.00057261367 0.0002850694][0.00034958319 0.0006658209 0.0011172483 0.0016777463 0.0022924733 0.0028941948 0.0033986745 0.0036975548 0.003681266 0.0033400217 0.0027493821 0.0020322385 0.0013313538 0.00076700456 0.00038793604][0.00026571398 0.0005253196 0.00091526774 0.0014234914 0.0020145841 0.0026235187 0.0031825977 0.0035905554 0.0037218125 0.0035037785 0.0029844076 0.0022783868 0.0015420516 0.00091643527 0.000473383][0.00017705631 0.0003589807 0.00065352611 0.0010673361 0.0015858749 0.002162714 0.0027383019 0.0032194443 0.0034780938 0.0033926917 0.0029768762 0.0023303935 0.0016133784 0.00097836636 0.00051346107][0.00010870445 0.00021702773 0.00040919 0.00070781092 0.0011167669 0.0016132512 0.0021512317 0.002650694 0.0029888912 0.003022718 0.0027265593 0.0021764562 0.0015313362 0.00094108249 0.0005001087][6.7427813e-05 0.00012400121 0.00023257082 0.00041931463 0.0007025188 0.0010809886 0.0015268242 0.0019810165 0.0023327542 0.0024445662 0.0022614861 0.0018352391 0.0013073022 0.00081250287 0.00043782932][4.6904257e-05 7.2306451e-05 0.00012575272 0.00022560106 0.00039424736 0.0006428847 0.00096465857 0.0013154802 0.0016136334 0.0017513804 0.0016618724 0.0013728261 0.00098890834 0.00062121236 0.00034041572][3.6523379e-05 4.6908375e-05 6.9640322e-05 0.00011552201 0.00019999209 0.00033813881 0.00053378032 0.00076065119 0.00096688396 0.0010820155 0.0010549182 0.00088967296 0.00064953667 0.00041268431 0.00023052811][3.0611089e-05 3.3992023e-05 4.219346e-05 5.9941609e-05 9.5324911e-05 0.00015718647 0.00025387251 0.00037083944 0.00048526924 0.00055719324 0.00055727374 0.0004792143 0.000356316 0.00023174334 0.00013526784]]...]
INFO - root - 2017-12-09 07:13:50.499638: step 2410, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 75h:42m:13s remains)
INFO - root - 2017-12-09 07:13:58.996185: step 2420, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 76h:48m:40s remains)
INFO - root - 2017-12-09 07:14:07.335945: step 2430, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:06m:51s remains)
INFO - root - 2017-12-09 07:14:15.793255: step 2440, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 74h:12m:31s remains)
INFO - root - 2017-12-09 07:14:24.405632: step 2450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:15m:18s remains)
INFO - root - 2017-12-09 07:14:33.119942: step 2460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:09m:06s remains)
INFO - root - 2017-12-09 07:14:41.814547: step 2470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:33m:40s remains)
INFO - root - 2017-12-09 07:14:50.361358: step 2480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:54m:10s remains)
INFO - root - 2017-12-09 07:14:59.015222: step 2490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:55m:17s remains)
INFO - root - 2017-12-09 07:15:07.654083: step 2500, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 75h:26m:47s remains)
2017-12-09 07:15:08.496747: I tensorflow/core/kernels/logging_ops.cc:79] [[[4.363199e-05 4.4198547e-05 4.383843e-05 4.3656226e-05 4.3876309e-05 4.3933185e-05 4.3832697e-05 4.3826658e-05 4.3945853e-05 4.3939282e-05 4.3934924e-05 4.3987486e-05 4.4013424e-05 4.4054788e-05 4.4090331e-05][4.36868e-05 4.4564564e-05 4.4079767e-05 4.3710264e-05 4.411173e-05 4.3965745e-05 4.3779793e-05 4.373477e-05 4.3878666e-05 4.4028093e-05 4.4270084e-05 4.4482724e-05 4.4598368e-05 4.4612869e-05 4.4661181e-05][4.1360305e-05 4.2488187e-05 4.24265e-05 4.2407875e-05 4.3253443e-05 4.3109714e-05 4.2903754e-05 4.2765241e-05 4.3081367e-05 4.3717941e-05 4.4248976e-05 4.4702152e-05 4.4971661e-05 4.5148277e-05 4.5115339e-05][3.9721468e-05 4.0755469e-05 4.1527535e-05 4.1889914e-05 4.2412212e-05 4.2453663e-05 4.270942e-05 4.2634056e-05 4.317001e-05 4.45026e-05 4.5295863e-05 4.5585104e-05 4.5971377e-05 4.5919063e-05 4.5695291e-05][3.8107479e-05 3.9142033e-05 4.0250939e-05 4.0887775e-05 4.1095285e-05 4.1740182e-05 4.2633583e-05 4.316651e-05 4.3884356e-05 4.5516346e-05 4.5918678e-05 4.5723827e-05 4.6310743e-05 4.6088026e-05 4.5903224e-05][3.5934419e-05 3.7341975e-05 3.7913611e-05 3.907572e-05 4.0018771e-05 4.1421372e-05 4.2980442e-05 4.3527391e-05 4.4256121e-05 4.5734334e-05 4.6162939e-05 4.5792862e-05 4.6320616e-05 4.6507637e-05 4.6223497e-05][3.56199e-05 3.7447411e-05 3.8066246e-05 3.9716717e-05 4.139464e-05 4.3516207e-05 4.4624918e-05 4.4454944e-05 4.5714274e-05 4.673261e-05 4.6773865e-05 4.5724715e-05 4.5810026e-05 4.6457819e-05 4.5976092e-05][3.5971629e-05 3.8262202e-05 3.922257e-05 4.073017e-05 4.2408785e-05 4.4590655e-05 4.4630215e-05 4.3234053e-05 4.3785141e-05 4.4740751e-05 4.5242581e-05 4.4837361e-05 4.5321169e-05 4.5736728e-05 4.5512934e-05][3.646488e-05 3.8591839e-05 3.9355691e-05 4.0329462e-05 4.1068866e-05 4.2728876e-05 4.2293759e-05 4.0712548e-05 4.1315543e-05 4.2371423e-05 4.3275955e-05 4.3464977e-05 4.4721033e-05 4.51691e-05 4.497006e-05][3.7759215e-05 4.0084109e-05 4.1256884e-05 4.1145337e-05 4.0185478e-05 4.0107116e-05 3.9493134e-05 3.88649e-05 3.9652645e-05 4.0414998e-05 4.1399268e-05 4.2074607e-05 4.3898828e-05 4.475352e-05 4.4797685e-05][4.0346895e-05 4.2288739e-05 4.2613945e-05 4.2223175e-05 4.1189342e-05 3.9951912e-05 3.9117505e-05 3.9133847e-05 3.9935083e-05 4.0452498e-05 4.1457293e-05 4.2051332e-05 4.3620268e-05 4.4718021e-05 4.5056251e-05][4.3512584e-05 4.4564265e-05 4.36013e-05 4.3124659e-05 4.2642045e-05 4.1637897e-05 4.0748288e-05 4.1282336e-05 4.2082029e-05 4.2436142e-05 4.3148015e-05 4.3473083e-05 4.392265e-05 4.4652581e-05 4.5048677e-05][4.4497472e-05 4.516379e-05 4.4233879e-05 4.3537904e-05 4.297257e-05 4.2396648e-05 4.1697909e-05 4.1875843e-05 4.2429354e-05 4.2779495e-05 4.34267e-05 4.4089167e-05 4.4409382e-05 4.4606881e-05 4.4948058e-05][4.3999207e-05 4.4256754e-05 4.3653468e-05 4.3245884e-05 4.2762236e-05 4.240172e-05 4.2186402e-05 4.2294356e-05 4.27622e-05 4.3198779e-05 4.3919143e-05 4.4499007e-05 4.5001303e-05 4.5350615e-05 4.5312605e-05][4.4608991e-05 4.5235887e-05 4.4801258e-05 4.436326e-05 4.379623e-05 4.340132e-05 4.3183216e-05 4.3303131e-05 4.357782e-05 4.3849723e-05 4.4398686e-05 4.482473e-05 4.5228735e-05 4.5458648e-05 4.5244575e-05]]...]
INFO - root - 2017-12-09 07:15:17.116092: step 2510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:52m:18s remains)
INFO - root - 2017-12-09 07:15:25.545041: step 2520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 77h:22m:20s remains)
INFO - root - 2017-12-09 07:15:34.124018: step 2530, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 83h:14m:31s remains)
INFO - root - 2017-12-09 07:15:42.670258: step 2540, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 69h:21m:43s remains)
INFO - root - 2017-12-09 07:15:51.336660: step 2550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:53m:14s remains)
INFO - root - 2017-12-09 07:16:00.063798: step 2560, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.983 sec/batch; 90h:05m:39s remains)
INFO - root - 2017-12-09 07:16:08.711419: step 2570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:06m:46s remains)
INFO - root - 2017-12-09 07:16:17.438034: step 2580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:51m:44s remains)
INFO - root - 2017-12-09 07:16:26.136458: step 2590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:23m:51s remains)
INFO - root - 2017-12-09 07:16:34.644930: step 2600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:16m:07s remains)
2017-12-09 07:16:35.543473: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.9928339e-05 2.0850825e-05 2.0586456e-05 2.0309821e-05 2.016826e-05 2.0436015e-05 2.0629192e-05 2.065935e-05 2.051011e-05 2.0430984e-05 2.028274e-05 2.0151652e-05 2.0101954e-05 2.0141393e-05 2.0480355e-05][1.869964e-05 2.0029656e-05 1.9968473e-05 2.0115032e-05 2.0273073e-05 2.0608131e-05 2.0951175e-05 2.1093354e-05 2.1071275e-05 2.0838761e-05 2.0462081e-05 1.9930321e-05 1.9251234e-05 1.9017236e-05 1.9300282e-05][2.016452e-05 2.154263e-05 2.1020402e-05 2.1452994e-05 2.1920921e-05 2.2697946e-05 2.319405e-05 2.3337747e-05 2.3285487e-05 2.2753709e-05 2.2124343e-05 2.1369793e-05 2.0364554e-05 1.9799245e-05 1.993325e-05][2.0876432e-05 2.2427455e-05 2.224584e-05 2.2603581e-05 2.2905468e-05 2.3529592e-05 2.4308662e-05 2.498936e-05 2.5325233e-05 2.4731904e-05 2.4136796e-05 2.3519562e-05 2.2543194e-05 2.185518e-05 2.1817716e-05][2.2984928e-05 2.4280933e-05 2.3660683e-05 2.2888809e-05 2.2225195e-05 2.2549477e-05 2.3241435e-05 2.4464214e-05 2.5338777e-05 2.5134315e-05 2.4609057e-05 2.399665e-05 2.3202861e-05 2.2461762e-05 2.2436368e-05][3.0668114e-05 3.2791755e-05 3.2188978e-05 3.073327e-05 2.9179104e-05 2.9109109e-05 2.9045394e-05 2.9228413e-05 2.9379884e-05 2.9098625e-05 2.8223763e-05 2.6714672e-05 2.4996571e-05 2.347161e-05 2.2446136e-05][3.6137168e-05 4.1412659e-05 4.3820404e-05 4.4672666e-05 4.4354183e-05 4.4527074e-05 4.4314784e-05 4.3993768e-05 4.2675612e-05 4.0873132e-05 3.8572423e-05 3.589987e-05 3.1737236e-05 2.7258018e-05 2.3869434e-05][3.0628729e-05 3.4685156e-05 3.7237867e-05 4.0027229e-05 4.2765376e-05 4.5841127e-05 4.9226863e-05 5.3031221e-05 5.4598106e-05 5.4282526e-05 5.1706858e-05 4.7929589e-05 4.3503187e-05 3.7559312e-05 3.19965e-05][2.6800673e-05 2.8297374e-05 2.8903032e-05 3.0098865e-05 3.1658918e-05 3.3847176e-05 3.65934e-05 4.0284689e-05 4.3710479e-05 4.6106234e-05 4.7192309e-05 4.6853176e-05 4.5558092e-05 4.2412928e-05 3.84205e-05][2.97302e-05 3.0783009e-05 3.0245461e-05 2.9955208e-05 3.0474966e-05 3.136777e-05 3.1664214e-05 3.2198655e-05 3.2985834e-05 3.41418e-05 3.5756584e-05 3.6781446e-05 3.7239468e-05 3.7213515e-05 3.6275331e-05][3.4563356e-05 3.6855152e-05 3.6384383e-05 3.5653193e-05 3.4877772e-05 3.3808723e-05 3.2426859e-05 3.0979387e-05 2.9868013e-05 2.9715517e-05 2.9867722e-05 3.0031628e-05 3.0268708e-05 3.0506551e-05 3.0389503e-05][3.6485948e-05 3.8545139e-05 3.758715e-05 3.7140529e-05 3.6357193e-05 3.4724722e-05 3.3062362e-05 3.113518e-05 2.9292754e-05 2.7923441e-05 2.6837817e-05 2.6257872e-05 2.6208236e-05 2.6474849e-05 2.6887919e-05][3.9508432e-05 4.0940304e-05 3.9939925e-05 3.932873e-05 3.8812035e-05 3.7609589e-05 3.6515721e-05 3.5186025e-05 3.3262233e-05 3.175504e-05 3.0497711e-05 3.0153464e-05 3.0172003e-05 3.0312596e-05 3.0692321e-05][4.6476784e-05 4.7988429e-05 4.731867e-05 4.7137753e-05 4.6691788e-05 4.5808763e-05 4.531995e-05 4.4592696e-05 4.3158241e-05 4.1796393e-05 4.0600109e-05 4.04482e-05 4.0991759e-05 4.1786505e-05 4.2882897e-05][5.1371168e-05 5.3958098e-05 5.350263e-05 5.3114953e-05 5.27513e-05 5.1829436e-05 5.149376e-05 5.113424e-05 5.0445105e-05 4.9666658e-05 4.9097504e-05 4.8944898e-05 4.9652317e-05 5.05952e-05 5.1696468e-05]]...]
INFO - root - 2017-12-09 07:16:44.174900: step 2610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:41m:52s remains)
INFO - root - 2017-12-09 07:16:52.572939: step 2620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:55m:02s remains)
INFO - root - 2017-12-09 07:17:01.110043: step 2630, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:15m:30s remains)
INFO - root - 2017-12-09 07:17:09.659090: step 2640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:04m:34s remains)
INFO - root - 2017-12-09 07:17:18.235185: step 2650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:49m:33s remains)
INFO - root - 2017-12-09 07:17:26.781572: step 2660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:43m:58s remains)
INFO - root - 2017-12-09 07:17:35.328540: step 2670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:05m:22s remains)
INFO - root - 2017-12-09 07:17:43.860615: step 2680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:12m:46s remains)
INFO - root - 2017-12-09 07:17:52.622518: step 2690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:39m:45s remains)
INFO - root - 2017-12-09 07:18:01.207037: step 2700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:02m:06s remains)
2017-12-09 07:18:02.045455: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0785581e-05 2.1746931e-05 2.1435841e-05 2.0910938e-05 2.0754731e-05 1.9948831e-05 1.9419629e-05 2.0221658e-05 2.1427313e-05 2.2917029e-05 2.3202756e-05 2.3416713e-05 2.3217897e-05 2.2713819e-05 2.2140423e-05][2.7139551e-05 2.8006161e-05 2.7799932e-05 2.8552826e-05 3.17576e-05 3.7695052e-05 4.6782192e-05 5.77601e-05 6.6863926e-05 7.11388e-05 6.7641129e-05 5.855758e-05 4.7247868e-05 3.7452573e-05 3.0926163e-05][3.0696076e-05 3.3001521e-05 3.681931e-05 4.6321675e-05 6.598825e-05 9.7014767e-05 0.00013670255 0.00017703732 0.0002056271 0.00021262573 0.00019455727 0.00015768543 0.0001141354 7.5949734e-05 4.9773615e-05][3.54943e-05 4.4037919e-05 6.1718849e-05 9.7330718e-05 0.00015851264 0.00024438504 0.00034320823 0.00043269328 0.00048835832 0.00049272412 0.00044288632 0.00035497049 0.00025162916 0.00015809585 9.0433576e-05][4.5227e-05 7.0196809e-05 0.00011860694 0.00020600039 0.00033903139 0.00050582842 0.00067792635 0.00081729179 0.00088894792 0.00087735744 0.00078261516 0.00062672439 0.00044603311 0.00027841621 0.00015191453][6.9813177e-05 0.00012298499 0.00022135141 0.00038001087 0.00059621769 0.00084233785 0.0010722224 0.0012401105 0.0013096413 0.0012676994 0.0011217073 0.00089635875 0.00063882227 0.00039973436 0.0002155624][0.00011489652 0.00020456496 0.000359043 0.00058491435 0.00086435158 0.0011597263 0.0014191159 0.0015908655 0.0016379782 0.0015508803 0.0013485068 0.0010675441 0.00075669558 0.00047160033 0.00025244275][0.00018463333 0.00030779568 0.00050457881 0.00077196117 0.0010815389 0.0013880414 0.0016327746 0.0017730755 0.0017739779 0.0016334506 0.0013911837 0.0010813453 0.00075331872 0.00046409614 0.00024670229][0.00027137049 0.00040991549 0.00062120473 0.00089700514 0.0011963791 0.0014666244 0.0016517012 0.0017238032 0.0016755533 0.0015090914 0.0012529186 0.00094730797 0.00064259436 0.00038514574 0.00020216989][0.00035714236 0.00048585306 0.000676071 0.00091509637 0.0011625161 0.0013666961 0.001486019 0.0014984404 0.0014050411 0.0012227684 0.00098128384 0.00071526744 0.00046692707 0.00027010363 0.00013962253][0.00041650955 0.000510132 0.00064843695 0.000822068 0.00099514378 0.0011262423 0.0011833009 0.0011510371 0.0010379679 0.00086437556 0.00066266605 0.00046109105 0.00028840947 0.00016078218 8.2046507e-05][0.00042422651 0.00047420291 0.00055149989 0.00065084163 0.00074786833 0.00081074756 0.00081757287 0.00076446187 0.00066125806 0.00052583753 0.00038404859 0.00025531929 0.00015403741 8.6160508e-05 4.7215417e-05][0.0003934995 0.00040749143 0.00043238292 0.00046929508 0.00050197466 0.00051372737 0.00049315789 0.00043924057 0.00036267636 0.00027565495 0.00019363775 0.00012608345 7.644677e-05 4.6830562e-05 3.1100088e-05][0.00036825475 0.00036106026 0.00035430043 0.00034925307 0.00033925593 0.00031819753 0.00028184144 0.00023387127 0.00018304678 0.0001358306 9.6730742e-05 6.7274334e-05 4.6616366e-05 3.4152548e-05 2.65714e-05][0.00041371869 0.00039631466 0.00036977616 0.00033841835 0.00030037609 0.00025667707 0.00020826446 0.00016063347 0.0001211628 9.2586051e-05 7.42609e-05 6.1934741e-05 5.1425548e-05 4.2556505e-05 3.4720008e-05]]...]
INFO - root - 2017-12-09 07:18:10.793473: step 2710, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 81h:23m:26s remains)
INFO - root - 2017-12-09 07:18:19.437664: step 2720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:49m:06s remains)
INFO - root - 2017-12-09 07:18:28.146692: step 2730, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 83h:33m:35s remains)
INFO - root - 2017-12-09 07:18:36.889164: step 2740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:15m:06s remains)
INFO - root - 2017-12-09 07:18:45.513331: step 2750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:07m:19s remains)
INFO - root - 2017-12-09 07:18:54.246251: step 2760, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:48m:45s remains)
INFO - root - 2017-12-09 07:19:02.978450: step 2770, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 81h:57m:45s remains)
INFO - root - 2017-12-09 07:19:11.650323: step 2780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:13m:08s remains)
INFO - root - 2017-12-09 07:19:20.320290: step 2790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:19m:43s remains)
INFO - root - 2017-12-09 07:19:28.816041: step 2800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:24m:32s remains)
2017-12-09 07:19:29.661574: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013559814 0.014552945 0.01526627 0.015889447 0.016490772 0.017076092 0.017490067 0.017591879 0.017119084 0.015838888 0.013645634 0.010781704 0.0077062263 0.0048719421 0.002629105][0.017454557 0.018578598 0.019368939 0.020033855 0.020638334 0.02117293 0.021445837 0.02130796 0.020473268 0.018732797 0.01599952 0.012583943 0.0090019666 0.0057343836 0.0031506012][0.020957792 0.022110643 0.022903135 0.023549749 0.024064586 0.024448277 0.024484513 0.024008531 0.022753835 0.020543052 0.017344778 0.013535337 0.0096441144 0.0061494745 0.0034082048][0.023630941 0.024801137 0.025484556 0.025970139 0.02631815 0.026493886 0.026260732 0.025443105 0.023796313 0.021191915 0.017671956 0.013649785 0.0096570766 0.0061432105 0.0034313372][0.025574032 0.026717016 0.027270265 0.027546618 0.027667142 0.027581785 0.027044034 0.025880549 0.023860982 0.02092335 0.017192246 0.013098299 0.0091629233 0.005782234 0.0032392223][0.02697102 0.028071741 0.028470606 0.028555926 0.028471904 0.028145436 0.027337117 0.025873562 0.023543764 0.020342639 0.016454581 0.012328599 0.0084666749 0.0052383146 0.0028930155][0.027699037 0.028789479 0.029105147 0.029074783 0.028868033 0.028402003 0.027447505 0.025813378 0.023311771 0.019970624 0.015993344 0.011828145 0.0079777585 0.0048134252 0.0025802434][0.02771032 0.028886124 0.029175533 0.029099977 0.028848767 0.0283528 0.027381506 0.025732035 0.023230674 0.019901099 0.015936486 0.011763802 0.0078920759 0.0047128769 0.0024804284][0.027094055 0.028475214 0.028807329 0.028725972 0.0285128 0.028060278 0.02715745 0.025621576 0.023264205 0.020084063 0.016222108 0.012074999 0.0081599373 0.0048960974 0.0025668752][0.025943333 0.027516667 0.028022809 0.028105972 0.02803959 0.027704693 0.02693853 0.025561087 0.023393283 0.020413801 0.016693654 0.012585596 0.00861745 0.0052372478 0.0027600676][0.024232538 0.025977509 0.026640028 0.026902521 0.027011117 0.026847471 0.02626981 0.025099773 0.023180654 0.020444479 0.016908208 0.012903853 0.0089480728 0.0055090189 0.0029273785][0.021812905 0.023647487 0.024444435 0.024832495 0.025044007 0.024987511 0.02455277 0.023593042 0.0219559 0.019529462 0.016301235 0.012557549 0.00879757 0.0054777442 0.0029444988][0.0187296 0.020534262 0.021410093 0.021842003 0.022045739 0.021995306 0.021620985 0.02080748 0.01941834 0.017339902 0.014543509 0.011275648 0.007969318 0.0050274166 0.0027587959][0.015234502 0.016900644 0.01775494 0.018148081 0.018292299 0.018188959 0.017823873 0.017113721 0.015947929 0.014231168 0.011933789 0.0092704566 0.0065752948 0.0041813725 0.0023325349][0.011584349 0.012996825 0.013772063 0.014122621 0.014215854 0.014069801 0.0137162 0.013090688 0.012124916 0.010763916 0.0089871548 0.0069632689 0.0049390239 0.0031533132 0.0017860812]]...]
INFO - root - 2017-12-09 07:19:38.364472: step 2810, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 83h:08m:06s remains)
INFO - root - 2017-12-09 07:19:46.878689: step 2820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:42m:52s remains)
INFO - root - 2017-12-09 07:19:55.273683: step 2830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:36m:19s remains)
INFO - root - 2017-12-09 07:20:03.838685: step 2840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:41m:13s remains)
INFO - root - 2017-12-09 07:20:12.332115: step 2850, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 83h:07m:46s remains)
INFO - root - 2017-12-09 07:20:21.114484: step 2860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:50m:20s remains)
INFO - root - 2017-12-09 07:20:29.769900: step 2870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:46m:09s remains)
INFO - root - 2017-12-09 07:20:38.356390: step 2880, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 82h:28m:11s remains)
INFO - root - 2017-12-09 07:20:46.969909: step 2890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:22m:21s remains)
INFO - root - 2017-12-09 07:20:55.468460: step 2900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:43m:58s remains)
2017-12-09 07:20:56.401926: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00090929057 0.0011818386 0.0013871465 0.0014983822 0.0014946157 0.0014040809 0.001292538 0.0012476306 0.0013350346 0.0015631047 0.0018933344 0.0022381346 0.0024884052 0.0025413188 0.002371636][0.0016840567 0.0021990675 0.0026022308 0.0028327676 0.002845038 0.002685582 0.0024708095 0.0023517846 0.0024472885 0.0027735364 0.0032813812 0.0038445664 0.0042883852 0.0044239005 0.004171636][0.0026732474 0.0034925051 0.0041494095 0.0045384625 0.0045865122 0.0043546185 0.0040127411 0.0037871432 0.0038595204 0.0042599831 0.0049309088 0.0057156882 0.0063784989 0.0066327634 0.0063156984][0.0037848325 0.0049520344 0.0059041744 0.0064818263 0.0065911375 0.0063039046 0.0058394037 0.0054919212 0.00551168 0.0059510819 0.0067456127 0.0077163097 0.0085746525 0.0089468686 0.0085755829][0.0048941625 0.0064360332 0.0077165575 0.0085157845 0.0087237963 0.0084213614 0.0078681139 0.0074164397 0.0073778448 0.0078250542 0.008683282 0.0097628646 0.010738022 0.011167639 0.010714568][0.0058411313 0.0077393288 0.0093463408 0.010385392 0.010730512 0.010474566 0.0098988982 0.0093918145 0.0093106469 0.0097490782 0.010614745 0.011710443 0.012693689 0.013089575 0.01250554][0.0064401841 0.0086065819 0.010485725 0.011753251 0.012268594 0.012132715 0.011631004 0.0111511 0.011063037 0.011481273 0.01230443 0.013326513 0.014210369 0.014483978 0.013730726][0.0065966519 0.0088854432 0.010932092 0.012385094 0.013089826 0.013131602 0.012778715 0.0123965 0.012335446 0.012731255 0.013479499 0.014376231 0.015106274 0.015215827 0.01429887][0.006295858 0.0085529378 0.010637736 0.01220301 0.013076149 0.013314717 0.013157608 0.012922617 0.012924492 0.013310596 0.013979096 0.014740072 0.015297222 0.01523328 0.014184413][0.0055648573 0.0076415204 0.0096331323 0.011212053 0.012199283 0.012624073 0.012669137 0.012607633 0.01271478 0.013113693 0.013717389 0.014350589 0.014745147 0.0145246 0.013398033][0.0045402809 0.0062928484 0.008040729 0.0094986344 0.010501941 0.0110529 0.011284862 0.01140406 0.011629367 0.012056892 0.012602146 0.013116741 0.013367898 0.013043119 0.011929785][0.00337669 0.0047296244 0.0061318469 0.0073578386 0.0082678907 0.008845794 0.009183608 0.0094317058 0.0097407363 0.010180715 0.010669895 0.011083132 0.011232982 0.010874008 0.0098835742][0.0022732236 0.003215421 0.0042320658 0.0051645539 0.0059062992 0.0064298674 0.0067896955 0.0070856079 0.0074208206 0.0078312019 0.0082471 0.0085670082 0.008655061 0.0083365152 0.0075503672][0.0013818142 0.0019694425 0.0026318841 0.0032676652 0.0038049126 0.0042132242 0.0045229681 0.0047967955 0.0050986153 0.0054477984 0.0057860152 0.0060322997 0.0060894918 0.0058481097 0.0052875127][0.00075837062 0.0010800891 0.0014582397 0.0018382054 0.0021788469 0.0024553249 0.0026774043 0.0028859586 0.0031184745 0.0033847652 0.0036395309 0.0038208286 0.0038678315 0.0037137913 0.0033583781]]...]
INFO - root - 2017-12-09 07:21:04.909941: step 2910, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 76h:17m:26s remains)
INFO - root - 2017-12-09 07:21:13.308779: step 2920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:57m:15s remains)
INFO - root - 2017-12-09 07:21:21.894437: step 2930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:31m:53s remains)
INFO - root - 2017-12-09 07:21:30.556918: step 2940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 77h:03m:36s remains)
INFO - root - 2017-12-09 07:21:39.172415: step 2950, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:58m:57s remains)
INFO - root - 2017-12-09 07:21:47.867766: step 2960, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 82h:12m:36s remains)
INFO - root - 2017-12-09 07:21:56.547317: step 2970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:33m:45s remains)
INFO - root - 2017-12-09 07:22:04.930804: step 2980, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 76h:23m:26s remains)
INFO - root - 2017-12-09 07:22:13.487459: step 2990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:45m:16s remains)
INFO - root - 2017-12-09 07:22:21.844450: step 3000, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:19m:50s remains)
2017-12-09 07:22:22.715434: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.7294005e-05 2.817142e-05 2.7342012e-05 2.5491314e-05 2.3188357e-05 2.0487762e-05 1.8206094e-05 1.6402821e-05 1.4920104e-05 1.4362191e-05 1.4556153e-05 1.6200891e-05 1.8519491e-05 2.1203658e-05 2.2555334e-05][2.7917526e-05 2.8492246e-05 2.7841754e-05 2.6202299e-05 2.3470191e-05 2.051378e-05 1.8090042e-05 1.5927119e-05 1.422803e-05 1.2952911e-05 1.3042885e-05 1.5108188e-05 1.7665214e-05 2.0421143e-05 2.1395655e-05][2.6057194e-05 2.6935359e-05 2.6748759e-05 2.5338461e-05 2.271857e-05 1.9678057e-05 1.6684895e-05 1.3997207e-05 1.1707463e-05 1.0867308e-05 1.0982269e-05 1.3375644e-05 1.618006e-05 1.8793424e-05 1.9993106e-05][2.4131394e-05 2.5347868e-05 2.5280238e-05 2.4548255e-05 2.258574e-05 2.0000396e-05 1.707696e-05 1.374412e-05 1.1291424e-05 1.0553897e-05 1.101154e-05 1.3452449e-05 1.5834074e-05 1.8414274e-05 1.9214665e-05][2.4917364e-05 2.6067308e-05 2.5458405e-05 2.4575838e-05 2.294216e-05 2.1199234e-05 1.9088915e-05 1.5639565e-05 1.315468e-05 1.2725282e-05 1.3179986e-05 1.5100759e-05 1.6843947e-05 1.8829207e-05 1.9080249e-05][2.5973819e-05 2.795806e-05 2.6994534e-05 2.538897e-05 2.4314719e-05 2.4039222e-05 2.3193348e-05 2.0794734e-05 1.8055285e-05 1.7821989e-05 1.8277879e-05 1.9238109e-05 1.9628456e-05 2.0694355e-05 2.0439784e-05][2.5277233e-05 2.8734754e-05 2.8914194e-05 2.7799e-05 2.6727637e-05 2.7741549e-05 3.0843428e-05 2.9968654e-05 2.5932964e-05 2.5063982e-05 2.5530553e-05 2.5906225e-05 2.5237761e-05 2.4715762e-05 2.3686105e-05][2.1741977e-05 2.426896e-05 2.4870209e-05 2.605571e-05 2.6337711e-05 2.7047983e-05 3.0652856e-05 3.1756677e-05 2.8599465e-05 2.7697151e-05 2.8509643e-05 2.8933264e-05 2.8625884e-05 2.8036204e-05 2.862568e-05][1.7668983e-05 1.8483242e-05 1.8301809e-05 1.8738367e-05 1.8866576e-05 1.9017225e-05 2.0404721e-05 2.2264012e-05 2.1720061e-05 2.1714939e-05 2.3123463e-05 2.426709e-05 2.6110723e-05 2.680915e-05 2.8468075e-05][1.4403453e-05 1.4007001e-05 1.2513836e-05 1.1628817e-05 1.1090884e-05 1.0367523e-05 9.9997742e-06 1.0995678e-05 1.162336e-05 1.3570436e-05 1.5748705e-05 1.7981492e-05 2.218052e-05 2.488861e-05 2.5689475e-05][1.1068707e-05 1.0199747e-05 7.8907069e-06 6.3121224e-06 4.9865012e-06 3.831341e-06 3.1198797e-06 3.4303957e-06 4.1559506e-06 6.5317254e-06 9.3582821e-06 1.2776811e-05 1.6833506e-05 2.0091164e-05 2.1651114e-05][1.0095424e-05 8.9011846e-06 6.3241569e-06 4.0622654e-06 2.3624816e-06 8.6044383e-07 -2.5683039e-07 -1.3095269e-07 6.4507913e-07 2.7957249e-06 5.550075e-06 8.515377e-06 1.288993e-05 1.6114911e-05 1.7444432e-05][1.0034182e-05 9.3210292e-06 7.1680224e-06 4.7768954e-06 2.7963724e-06 1.1867996e-06 -2.1615779e-07 -4.7907452e-07 -3.681489e-07 1.2884739e-06 4.1926469e-06 7.0857168e-06 1.0065949e-05 1.2875131e-05 1.5822134e-05][1.0007734e-05 1.007404e-05 8.6449036e-06 6.7991969e-06 4.9619885e-06 3.5189041e-06 2.17673e-06 1.0372351e-06 6.4321648e-07 1.733406e-06 4.0001833e-06 7.5880416e-06 9.8972778e-06 1.1409556e-05 1.4374662e-05][9.6094846e-06 1.0213738e-05 9.3527742e-06 8.2001607e-06 6.9492417e-06 5.5491728e-06 4.3224536e-06 3.1158961e-06 2.1908672e-06 2.2598542e-06 2.8912727e-06 5.9422637e-06 8.008381e-06 9.6874755e-06 1.2234992e-05]]...]
INFO - root - 2017-12-09 07:22:31.392618: step 3010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:23m:49s remains)
INFO - root - 2017-12-09 07:22:40.041149: step 3020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:43m:51s remains)
INFO - root - 2017-12-09 07:22:48.600992: step 3030, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 75h:04m:07s remains)
INFO - root - 2017-12-09 07:22:57.274422: step 3040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:51m:59s remains)
INFO - root - 2017-12-09 07:23:05.878007: step 3050, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:22m:52s remains)
INFO - root - 2017-12-09 07:23:14.567950: step 3060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:27m:59s remains)
INFO - root - 2017-12-09 07:23:23.255887: step 3070, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 75h:21m:15s remains)
INFO - root - 2017-12-09 07:23:31.901575: step 3080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:35m:50s remains)
INFO - root - 2017-12-09 07:23:40.629729: step 3090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:36m:05s remains)
INFO - root - 2017-12-09 07:23:49.247447: step 3100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:11m:46s remains)
2017-12-09 07:23:50.133926: I tensorflow/core/kernels/logging_ops.cc:79] [[[7.6756987e-06 9.9130266e-06 9.7871743e-06 9.7365919e-06 9.8272212e-06 1.00119e-05 1.0339936e-05 1.0587224e-05 1.0645934e-05 1.0826378e-05 1.0204742e-05 1.011214e-05 8.9443347e-06 7.3561678e-06 6.7251458e-06][9.3382405e-06 1.1892764e-05 1.2003162e-05 1.2198492e-05 1.250457e-05 1.2960576e-05 1.3338533e-05 1.3648598e-05 1.3614728e-05 1.3617886e-05 1.2796947e-05 1.2775148e-05 1.1579148e-05 9.4727948e-06 8.81101e-06][1.3109675e-05 1.5553393e-05 1.5691789e-05 1.600539e-05 1.6379876e-05 1.68923e-05 1.7521888e-05 1.8160339e-05 1.78884e-05 1.7524617e-05 1.6467464e-05 1.6143349e-05 1.4456302e-05 1.2064214e-05 1.1169068e-05][1.8296181e-05 2.0809632e-05 2.1102001e-05 2.1364496e-05 2.158853e-05 2.244176e-05 2.2883141e-05 2.3326167e-05 2.3020439e-05 2.250517e-05 2.1624735e-05 2.0994004e-05 1.8961851e-05 1.5976591e-05 1.4522455e-05][2.4417706e-05 2.7038928e-05 2.759623e-05 2.8131995e-05 2.8525646e-05 2.9376519e-05 2.9583003e-05 3.0206109e-05 2.9810362e-05 2.8920978e-05 2.8543487e-05 2.8418537e-05 2.609862e-05 2.145848e-05 1.828964e-05][3.2054915e-05 3.4867429e-05 3.581951e-05 3.6416837e-05 3.66775e-05 3.6877747e-05 3.6885089e-05 3.7730766e-05 3.6935962e-05 3.47058e-05 3.4246637e-05 3.4683915e-05 3.203666e-05 2.6076566e-05 2.0695283e-05][3.2488155e-05 3.4815544e-05 3.5731478e-05 3.6317972e-05 3.663799e-05 3.6118297e-05 3.5834957e-05 3.754341e-05 3.7371858e-05 3.3465127e-05 3.2335076e-05 3.2799791e-05 2.96535e-05 2.3809145e-05 1.8328014e-05][2.8910908e-05 3.0657924e-05 3.1173346e-05 3.1583571e-05 3.16378e-05 3.0407697e-05 2.9234856e-05 3.0082163e-05 3.033159e-05 2.7610062e-05 2.6619891e-05 2.7035792e-05 2.422322e-05 1.9454157e-05 1.4523168e-05][2.1847394e-05 2.3172703e-05 2.3444678e-05 2.3522924e-05 2.3204411e-05 2.2109562e-05 2.0320294e-05 1.9933592e-05 1.9824525e-05 1.8491097e-05 1.8282262e-05 1.9258543e-05 1.7955579e-05 1.533766e-05 1.2357552e-05][1.4926307e-05 1.5687008e-05 1.564686e-05 1.5307349e-05 1.4658261e-05 1.3314333e-05 1.1921249e-05 1.1447824e-05 1.1292839e-05 1.1101882e-05 1.1595155e-05 1.2941644e-05 1.3193821e-05 1.2636243e-05 1.161034e-05][1.1108859e-05 1.1542463e-05 1.1127901e-05 1.0648291e-05 1.0191114e-05 9.0769536e-06 8.0449245e-06 7.7534714e-06 7.3940682e-06 7.4320851e-06 8.2198894e-06 9.31228e-06 1.0055941e-05 1.050561e-05 1.0977958e-05][9.9908575e-06 1.0174968e-05 9.2536429e-06 8.53902e-06 8.4940766e-06 7.84413e-06 7.1776885e-06 7.1347386e-06 7.1315662e-06 7.2922485e-06 7.7855657e-06 8.3116538e-06 8.88724e-06 9.4380739e-06 1.0183758e-05][1.0569558e-05 1.0803305e-05 9.8634046e-06 9.0927861e-06 8.99937e-06 8.5119391e-06 7.9146121e-06 7.6920478e-06 7.8436569e-06 7.9518359e-06 8.2727493e-06 8.7115113e-06 8.8912129e-06 9.0101457e-06 9.4439019e-06][1.138269e-05 1.2044766e-05 1.1256896e-05 1.0752468e-05 1.0627024e-05 1.0202537e-05 9.7139564e-06 9.2591508e-06 9.05108e-06 8.9149617e-06 8.9527312e-06 8.99069e-06 8.9985842e-06 8.7886947e-06 8.8772358e-06][1.1465389e-05 1.2851953e-05 1.2349985e-05 1.2199205e-05 1.2243567e-05 1.1930104e-05 1.1443357e-05 1.1009339e-05 1.0498406e-05 1.0102238e-05 9.8867822e-06 9.6532967e-06 9.5285213e-06 9.0829344e-06 8.8776287e-06]]...]
INFO - root - 2017-12-09 07:23:58.744454: step 3110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:35m:22s remains)
INFO - root - 2017-12-09 07:24:07.394641: step 3120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:02m:46s remains)
INFO - root - 2017-12-09 07:24:16.157514: step 3130, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.984 sec/batch; 90h:03m:42s remains)
INFO - root - 2017-12-09 07:24:24.917830: step 3140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:14m:15s remains)
INFO - root - 2017-12-09 07:24:33.380071: step 3150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:42m:28s remains)
INFO - root - 2017-12-09 07:24:42.069378: step 3160, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 81h:20m:35s remains)
INFO - root - 2017-12-09 07:24:50.717042: step 3170, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:42m:31s remains)
INFO - root - 2017-12-09 07:24:59.297781: step 3180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 81h:05m:45s remains)
INFO - root - 2017-12-09 07:25:07.969155: step 3190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:26m:28s remains)
INFO - root - 2017-12-09 07:25:16.545927: step 3200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:36m:37s remains)
2017-12-09 07:25:17.421328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3183595e-05 -1.1299217e-05 -1.1499538e-05 -1.2089626e-05 -1.1829427e-05 -1.2340228e-05 -1.2709399e-05 -1.264588e-05 -1.2769633e-05 -1.2572589e-05 -1.236696e-05 -1.2455082e-05 -1.3093839e-05 -1.3722933e-05 -1.4547528e-05][3.5216726e-06 8.27606e-06 7.239898e-06 4.2007887e-06 1.8394057e-06 1.542212e-07 -2.0485604e-06 -3.9528168e-06 -5.4291741e-06 -6.5058557e-06 -7.1609247e-06 -8.0176214e-06 -9.7011e-06 -1.0621337e-05 -1.1519449e-05][6.59731e-05 8.425831e-05 8.3421881e-05 7.37862e-05 6.3609259e-05 5.6734643e-05 4.97023e-05 4.0240462e-05 2.999913e-05 1.8979939e-05 9.6207077e-06 2.3514731e-06 -3.1649251e-06 -6.0731618e-06 -8.3869454e-06][0.00022498527 0.00027962963 0.00028694479 0.00027047822 0.00025190497 0.00023916041 0.0002247208 0.00019841967 0.00016139224 0.00011633926 7.3067669e-05 3.8094324e-05 1.4651814e-05 2.751658e-06 -3.853369e-06][0.00051655259 0.00064355094 0.00067935651 0.00066642708 0.000649022 0.00064409716 0.00063251512 0.0005841853 0.00049399439 0.00037096202 0.00024556599 0.00013862002 6.36483e-05 2.4597968e-05 4.7115464e-06][0.00092284649 0.0011552966 0.0012495137 0.0012620935 0.0012658309 0.0012859061 0.0012941824 0.0012246561 0.0010603321 0.00081821566 0.00055477791 0.00032068152 0.00015188413 6.1232349e-05 1.8337778e-05][0.0013480228 0.0017001589 0.0018724287 0.0019287637 0.0019638455 0.0020216969 0.002058706 0.0019768612 0.0017394039 0.0013692414 0.00094642874 0.000556073 0.00026644723 0.00010716 3.3866992e-05][0.0016405398 0.00207935 0.0023166898 0.0024120356 0.0024766189 0.0025643669 0.0026274032 0.0025436673 0.0022597502 0.0017963391 0.0012498917 0.00073790125 0.00035480634 0.00014182666 4.6239758e-05][0.0016540744 0.0021060056 0.0023648206 0.0024797132 0.002562905 0.0026712604 0.0027569442 0.0026844298 0.0023890929 0.0018968496 0.0013116676 0.00076849782 0.00036814486 0.00014806792 4.9303912e-05][0.0013494508 0.0017283497 0.0019521394 0.002065311 0.0021617592 0.0022871571 0.0023931519 0.0023504877 0.0020940029 0.0016482723 0.0011202812 0.00064442988 0.00030481524 0.00012110396 3.9741957e-05][0.0008779437 0.001133899 0.0012870254 0.001375846 0.0014690888 0.001595458 0.0017111695 0.0017087085 0.0015236144 0.0011823972 0.00078387151 0.00043834164 0.00020296586 7.8860641e-05 2.4780085e-05][0.00044837559 0.00058468111 0.00066556671 0.00072170526 0.00079544954 0.00090160436 0.0010039234 0.0010242326 0.00091314351 0.000695911 0.00044792861 0.00024351344 0.00011085453 4.172582e-05 1.1351367e-05][0.00017040613 0.00022495064 0.00025692812 0.00028418648 0.00032930361 0.00039787844 0.00046573312 0.000487037 0.00043317495 0.00032223325 0.00020047443 0.00010499221 4.6149151e-05 1.5713536e-05 1.0313088e-06][3.8768805e-05 5.4966935e-05 6.552304e-05 7.5442091e-05 9.5218682e-05 0.00012726479 0.00016052509 0.00017258055 0.00015189221 0.00010882296 6.4302672e-05 3.0704927e-05 1.0660297e-05 -4.6328205e-07 -6.7278015e-06][-4.4932603e-06 -1.21841e-06 1.110704e-06 3.7970458e-06 9.9065219e-06 2.041056e-05 3.2227377e-05 3.6606732e-05 3.0226314e-05 1.8461527e-05 7.0942042e-06 -1.4124162e-06 -6.2764375e-06 -9.0684516e-06 -1.0715445e-05]]...]
INFO - root - 2017-12-09 07:25:26.000668: step 3210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:42m:12s remains)
INFO - root - 2017-12-09 07:25:34.745904: step 3220, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 83h:08m:16s remains)
INFO - root - 2017-12-09 07:25:43.338526: step 3230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:41m:51s remains)
INFO - root - 2017-12-09 07:25:52.053953: step 3240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:37m:14s remains)
INFO - root - 2017-12-09 07:26:00.613015: step 3250, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 82h:44m:23s remains)
INFO - root - 2017-12-09 07:26:09.426573: step 3260, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:50m:37s remains)
INFO - root - 2017-12-09 07:26:18.014284: step 3270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:50m:43s remains)
INFO - root - 2017-12-09 07:26:26.571360: step 3280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:38m:50s remains)
INFO - root - 2017-12-09 07:26:35.469794: step 3290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:31m:32s remains)
INFO - root - 2017-12-09 07:26:44.091306: step 3300, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.942 sec/batch; 86h:09m:54s remains)
2017-12-09 07:26:45.047341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00037490766 0.00073356129 0.0010924916 0.001379726 0.0015842115 0.0017043884 0.0017427803 0.0017599798 0.0018878931 0.0022272835 0.0028043967 0.0035025908 0.0041111684 0.00440322 0.0042606667][0.00073926145 0.0014412362 0.0021772035 0.0028355366 0.0033841345 0.003796817 0.0040671364 0.0042818594 0.0046539144 0.0053267716 0.0063359728 0.0074869525 0.0084403595 0.0088376887 0.0084635857][0.0011846013 0.0023368651 0.0035975883 0.0048127114 0.0059173978 0.0068374728 0.0075349188 0.0081010386 0.00883043 0.0098976791 0.01135881 0.012936445 0.014167353 0.014559651 0.013802671][0.0016408487 0.0033113596 0.0052118339 0.0071396874 0.0089747952 0.010570818 0.011831416 0.012841162 0.013957895 0.0153884 0.017211234 0.019083634 0.020445684 0.020687005 0.019418385][0.0020356991 0.0042247735 0.006820444 0.0095461449 0.012200753 0.014546798 0.016429346 0.017897332 0.019343326 0.021015391 0.023008587 0.024944684 0.026212266 0.0261434 0.024290113][0.0023237262 0.0049702809 0.0082218749 0.011717078 0.015153916 0.018208494 0.020653633 0.022506492 0.024160163 0.025893819 0.02782296 0.029565621 0.030520314 0.030003663 0.027555814][0.0024631626 0.0054372526 0.0091955941 0.013293708 0.01732919 0.020904504 0.023731943 0.025804555 0.027498143 0.029082932 0.030701995 0.032017991 0.032479461 0.031448394 0.0284941][0.0024187977 0.0054980358 0.0094922967 0.013887239 0.018200656 0.021994304 0.024936194 0.027001945 0.028519388 0.029756136 0.03088361 0.031633362 0.031536166 0.030037023 0.026775153][0.002214266 0.0051213312 0.0089859376 0.013279355 0.017476933 0.021136662 0.023904067 0.025742806 0.026926152 0.027726542 0.028293092 0.02844608 0.02783384 0.026016418 0.022742141][0.0018331223 0.0043157423 0.0076958053 0.011496495 0.015197367 0.018400244 0.020769859 0.022257019 0.023059506 0.023422983 0.023496879 0.023179403 0.022229657 0.020347465 0.017392861][0.0013732525 0.0032753434 0.0059244763 0.008948572 0.011889121 0.014414455 0.01624459 0.017323168 0.017773349 0.017795105 0.017537484 0.016961845 0.015924975 0.014246377 0.011876567][0.000905506 0.002191558 0.0040307664 0.0061728559 0.0082684234 0.010054442 0.011314069 0.011992476 0.01216602 0.011983901 0.01158167 0.010955428 0.010036289 0.0087404354 0.0070730145][0.000497754 0.0012355933 0.0023416681 0.0036747877 0.0049973913 0.0061244019 0.0068936674 0.0072611496 0.007267999 0.0070176041 0.0066198949 0.0060894806 0.0054037743 0.0045391466 0.0035325][0.00021029983 0.000548165 0.0010955506 0.0017960228 0.0025080368 0.0031133129 0.0035169539 0.0036861016 0.0036404782 0.0034377042 0.0031483772 0.0027878743 0.0023622478 0.0018839675 0.0013924215][4.9999151e-05 0.0001656079 0.00037267865 0.00065971643 0.00096222706 0.0012222999 0.0013953283 0.0014606124 0.0014240477 0.0013116937 0.0011603214 0.00097929745 0.00078126037 0.00058289192 0.00040469281]]...]
INFO - root - 2017-12-09 07:26:53.617851: step 3310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:54m:22s remains)
INFO - root - 2017-12-09 07:27:02.049636: step 3320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:56m:58s remains)
INFO - root - 2017-12-09 07:27:10.631488: step 3330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:02m:00s remains)
INFO - root - 2017-12-09 07:27:19.241220: step 3340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:59m:37s remains)
INFO - root - 2017-12-09 07:27:27.639052: step 3350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:49m:46s remains)
INFO - root - 2017-12-09 07:27:36.160893: step 3360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:46m:12s remains)
INFO - root - 2017-12-09 07:27:44.867971: step 3370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:22m:51s remains)
INFO - root - 2017-12-09 07:27:53.467253: step 3380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:41m:05s remains)
INFO - root - 2017-12-09 07:28:02.151894: step 3390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:02m:49s remains)
INFO - root - 2017-12-09 07:28:10.660085: step 3400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:18m:28s remains)
2017-12-09 07:28:11.498559: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0073687062 0.0086989906 0.009871671 0.011190992 0.012969228 0.015336768 0.018127056 0.020857746 0.022986172 0.024124136 0.024221748 0.023539996 0.022473805 0.021317001 0.02013634][0.013714645 0.01610906 0.018196991 0.020422749 0.023179006 0.026574204 0.03032656 0.03376672 0.036165684 0.037003327 0.036241807 0.034275886 0.031720556 0.029071389 0.026566623][0.021294603 0.024995597 0.02829081 0.031749118 0.035746332 0.040277462 0.044892531 0.048746146 0.050969366 0.050968848 0.048765227 0.044913746 0.040279571 0.035593793 0.031323045][0.028800102 0.03389759 0.038588453 0.04350058 0.048898313 0.054565765 0.059846 0.06374722 0.065322071 0.063987553 0.059869613 0.053696014 0.0465902 0.039545149 0.033288565][0.034444902 0.040839329 0.046984557 0.0534617 0.06031457 0.06705001 0.072778054 0.076428905 0.077048667 0.074139096 0.067946389 0.059350986 0.049730752 0.040355384 0.032209948][0.036633886 0.04404762 0.051562242 0.059580941 0.067833774 0.075561829 0.081667461 0.085021235 0.084695831 0.080313481 0.072228551 0.06144724 0.049616575 0.03830668 0.02870357][0.034776181 0.0427058 0.051236983 0.060541008 0.070021428 0.0786771 0.085221283 0.088471435 0.087490708 0.082000814 0.072455466 0.060017128 0.046607535 0.034052204 0.023668831][0.029625943 0.037473783 0.046456508 0.056512717 0.0668098 0.07616619 0.083133884 0.0864625 0.085182264 0.079064168 0.068654224 0.055334363 0.041251831 0.028368235 0.018030042][0.022680476 0.029795801 0.038461939 0.048472654 0.058894753 0.068466179 0.07566645 0.07915774 0.077923283 0.07171195 0.061196975 0.047990374 0.034332003 0.022153206 0.012714611][0.015433894 0.021277362 0.028871531 0.037990719 0.047722466 0.056843255 0.063873708 0.067445166 0.066490047 0.06073761 0.050963577 0.038927894 0.026782695 0.016252216 0.0084053865][0.0090252934 0.013289026 0.019271109 0.026825752 0.035173934 0.04320921 0.049597692 0.05301898 0.0524068 0.047515478 0.0391911 0.029181249 0.019362979 0.011116098 0.0052446686][0.0042958218 0.0069649955 0.011111044 0.016710961 0.023213387 0.029662127 0.034940295 0.03788488 0.037555922 0.033768363 0.027356589 0.019855365 0.012731072 0.0069545675 0.0030468474][0.0014823988 0.0028845123 0.0053686164 0.0090047447 0.01347663 0.018065697 0.021916507 0.0241046 0.023921268 0.02128648 0.016909819 0.011939261 0.0073727379 0.0038262166 0.001570742][0.00033078514 0.00089669693 0.002094188 0.0040491195 0.006631583 0.0093977805 0.011773163 0.013120217 0.013004423 0.011427234 0.00887589 0.0060637137 0.0035786214 0.0017562265 0.00068328111][5.28178e-05 0.00021224862 0.0006439719 0.0014570876 0.0026328783 0.0039594062 0.0051218942 0.0057660849 0.0056927535 0.0049354611 0.00374345 0.0024615135 0.001378115 0.000639878 0.00024251878]]...]
INFO - root - 2017-12-09 07:28:19.988823: step 3410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:28m:09s remains)
INFO - root - 2017-12-09 07:28:28.627789: step 3420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:43m:43s remains)
INFO - root - 2017-12-09 07:28:37.241772: step 3430, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 77h:13m:21s remains)
INFO - root - 2017-12-09 07:28:46.064752: step 3440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:37m:49s remains)
INFO - root - 2017-12-09 07:28:54.614568: step 3450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:22m:07s remains)
INFO - root - 2017-12-09 07:29:03.264539: step 3460, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:58m:08s remains)
INFO - root - 2017-12-09 07:29:11.991886: step 3470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:15m:07s remains)
INFO - root - 2017-12-09 07:29:20.593285: step 3480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:04m:08s remains)
INFO - root - 2017-12-09 07:29:29.335114: step 3490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:38m:18s remains)
INFO - root - 2017-12-09 07:29:37.843194: step 3500, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 67h:01m:15s remains)
2017-12-09 07:29:38.684491: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028656827 0.029006498 0.02863548 0.02819046 0.027787549 0.027496411 0.027373381 0.027433332 0.027715674 0.02822664 0.028784547 0.029241206 0.029415706 0.029419933 0.029240796][0.032576013 0.032914307 0.03241678 0.031851668 0.031375054 0.031074656 0.031018252 0.031219952 0.031690244 0.032363061 0.033054821 0.033533644 0.033702511 0.033653744 0.033402558][0.033913895 0.034280781 0.033681881 0.033011522 0.032469302 0.032178797 0.03221206 0.032567866 0.033221103 0.034078937 0.034935929 0.035530288 0.035763703 0.035743937 0.035501093][0.034020152 0.034521241 0.033918686 0.033200029 0.032606725 0.032308396 0.032394413 0.032838471 0.03359453 0.034571435 0.035571735 0.036316838 0.036684457 0.036756016 0.036560267][0.032980636 0.033715397 0.033238687 0.032590583 0.032014865 0.031723123 0.031836357 0.032301161 0.033054627 0.034041695 0.035112809 0.035990365 0.036516361 0.036714081 0.036585528][0.030607404 0.031575847 0.031344969 0.030895105 0.030443819 0.030230217 0.03038701 0.030839052 0.031525236 0.032427996 0.033472542 0.034406703 0.035059046 0.035389498 0.035369728][0.026705349 0.027835334 0.027911413 0.027762739 0.027562028 0.027522523 0.027761949 0.028206062 0.028801596 0.029570978 0.030506602 0.031412523 0.032138553 0.032620903 0.032806862][0.021447374 0.022630047 0.02302333 0.023244062 0.023390487 0.023598846 0.023950292 0.024378173 0.024861682 0.025460016 0.026218569 0.027023472 0.027791236 0.028459037 0.028970381][0.015410068 0.016636185 0.017333088 0.017939398 0.018482283 0.018985806 0.01945457 0.019844843 0.02017629 0.020533215 0.02101854 0.021662671 0.022443103 0.023337506 0.024270117][0.0097168516 0.010914563 0.011887572 0.012868165 0.013819166 0.014642846 0.01523594 0.015562043 0.015682179 0.015706362 0.015799131 0.016111737 0.016754149 0.017746342 0.018998127][0.005001165 0.0060816938 0.0071892613 0.0084290756 0.0096986033 0.010789883 0.011504345 0.011776758 0.011668121 0.011310413 0.010948597 0.010869132 0.011291117 0.012255875 0.013657939][0.0019868524 0.0027811553 0.0037654308 0.0049775075 0.00627575 0.0074136849 0.0081436206 0.0083614243 0.0080839181 0.0074457107 0.006750524 0.006370991 0.00657819 0.0074174698 0.0087584844][0.0005638412 0.0010053371 0.0016749968 0.0025914982 0.0036121754 0.004516074 0.005094768 0.0052523953 0.0049585598 0.004305556 0.0035678386 0.0031103306 0.0031855062 0.003817365 0.0048709675][0.00012768278 0.00029632752 0.00063328794 0.0011627122 0.0017711099 0.0022947947 0.0026308796 0.0027319409 0.0025330049 0.0020762393 0.0015428537 0.0011982457 0.0012036526 0.0015738696 0.0022234751][2.5165937e-05 7.9371857e-05 0.00021922032 0.00046560203 0.000749595 0.00097380124 0.0011140705 0.0011666336 0.0010802759 0.00085659331 0.0005865 0.00040427624 0.00038303868 0.00052627956 0.000806123]]...]
INFO - root - 2017-12-09 07:29:47.307534: step 3510, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:39m:10s remains)
INFO - root - 2017-12-09 07:29:55.712204: step 3520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:07m:43s remains)
INFO - root - 2017-12-09 07:30:04.258780: step 3530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:09m:43s remains)
INFO - root - 2017-12-09 07:30:12.816560: step 3540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:35m:58s remains)
INFO - root - 2017-12-09 07:30:21.470058: step 3550, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:36m:27s remains)
INFO - root - 2017-12-09 07:30:30.100701: step 3560, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 83h:08m:43s remains)
INFO - root - 2017-12-09 07:30:38.837834: step 3570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:42m:39s remains)
INFO - root - 2017-12-09 07:30:47.320887: step 3580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 77h:10m:26s remains)
INFO - root - 2017-12-09 07:30:55.998834: step 3590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:17m:13s remains)
INFO - root - 2017-12-09 07:31:04.633597: step 3600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:42m:04s remains)
2017-12-09 07:31:05.517495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7762948e-05 -1.7153659e-05 -1.7508635e-05 -1.6695718e-05 -1.3942939e-05 -9.7146039e-06 -6.1886894e-06 -2.81987e-07 5.5522905e-06 8.1785693e-06 6.7128349e-06 3.0473166e-06 -2.0124207e-06 -9.0017274e-06 -1.5001304e-05][-1.6400965e-05 -1.5617803e-05 -1.5993395e-05 -1.4986173e-05 -1.2027325e-05 -6.2711697e-06 -1.3271419e-06 3.935922e-06 8.7046938e-06 1.0724732e-05 8.9089808e-06 4.7224312e-06 -1.3607496e-07 -6.6161228e-06 -1.2331602e-05][-1.7034145e-05 -1.5644804e-05 -1.559253e-05 -1.4953734e-05 -1.2665438e-05 -7.1119357e-06 -9.0612593e-07 4.6774658e-06 9.0804679e-06 1.0174022e-05 8.396164e-06 3.459907e-06 -1.7036364e-06 -7.7611476e-06 -1.2795066e-05][-1.7095175e-05 -1.6098787e-05 -1.646706e-05 -1.6000926e-05 -1.3534569e-05 -8.2245751e-06 -1.839282e-06 4.7159192e-06 1.0089352e-05 1.0992648e-05 8.973002e-06 3.5907651e-06 -1.9096115e-06 -8.372117e-06 -1.289857e-05][-1.8305636e-05 -1.7372771e-05 -1.8485141e-05 -1.7737544e-05 -1.4661651e-05 -9.40013e-06 -3.2478129e-06 3.2892713e-06 9.2047412e-06 1.1069445e-05 9.0110625e-06 4.0320519e-06 -2.1286396e-06 -8.8333763e-06 -1.3335077e-05][-1.8918126e-05 -1.7564304e-05 -1.8948784e-05 -1.8573952e-05 -1.540314e-05 -1.0783508e-05 -5.1308889e-06 9.9754106e-07 6.9995222e-06 9.240699e-06 6.9624366e-06 1.4594552e-06 -5.1479074e-06 -1.1212614e-05 -1.4787118e-05][-1.9020412e-05 -1.7987109e-05 -1.8880273e-05 -1.9140472e-05 -1.6653918e-05 -1.2701508e-05 -7.1712566e-06 -1.7018028e-06 3.2801618e-06 6.4862907e-06 4.2350439e-06 -1.7746715e-06 -8.9555251e-06 -1.4093548e-05 -1.5869948e-05][-2.0721982e-05 -1.9436175e-05 -2.0175688e-05 -2.0736879e-05 -1.9238836e-05 -1.6205711e-05 -1.1134493e-05 -5.1735769e-06 -4.152389e-07 3.6116617e-06 1.7016791e-06 -4.5312045e-06 -1.1433331e-05 -1.7010709e-05 -1.8498889e-05][-2.299243e-05 -2.1232063e-05 -2.1493957e-05 -2.1495485e-05 -2.0345553e-05 -1.7850736e-05 -1.3854406e-05 -7.1843533e-06 -1.9811268e-06 1.9383006e-06 1.8189894e-07 -5.5015917e-06 -1.2126184e-05 -1.8026087e-05 -1.9997435e-05][-2.4246827e-05 -2.1897995e-05 -2.1012998e-05 -2.067937e-05 -1.9526939e-05 -1.7873353e-05 -1.4882717e-05 -9.0638423e-06 -3.7186182e-06 -1.6552804e-07 -2.3478133e-06 -6.8825393e-06 -1.3318906e-05 -1.8390521e-05 -1.9841333e-05][-2.3513861e-05 -2.087398e-05 -1.8957926e-05 -1.8420673e-05 -1.8502193e-05 -1.8172908e-05 -1.6250557e-05 -1.2179051e-05 -7.3779156e-06 -3.7581631e-06 -5.3771873e-06 -9.3242706e-06 -1.4212364e-05 -1.7743208e-05 -1.9418389e-05][-2.3194385e-05 -2.0973264e-05 -1.9060877e-05 -1.8705676e-05 -1.9080788e-05 -1.8845723e-05 -1.7775172e-05 -1.4524765e-05 -1.0626616e-05 -6.9330636e-06 -7.7561563e-06 -1.0882832e-05 -1.4666432e-05 -1.7534941e-05 -1.8813782e-05][-2.3250486e-05 -2.0945761e-05 -2.0135059e-05 -2.0594165e-05 -2.1346041e-05 -2.0674728e-05 -1.8269187e-05 -1.4106554e-05 -1.0164607e-05 -7.9262245e-06 -9.8769524e-06 -1.2881941e-05 -1.6336478e-05 -1.941243e-05 -2.0604566e-05][-2.039623e-05 -1.8225612e-05 -1.7920487e-05 -1.851657e-05 -1.9366482e-05 -1.9140945e-05 -1.6257905e-05 -1.2260796e-05 -8.9541063e-06 -8.3806808e-06 -1.1159886e-05 -1.3735444e-05 -1.6506383e-05 -1.885368e-05 -1.95313e-05][-1.7110917e-05 -1.5466754e-05 -1.5844023e-05 -1.6871425e-05 -1.7883442e-05 -1.7967046e-05 -1.5911326e-05 -1.2936292e-05 -1.1190845e-05 -1.1307755e-05 -1.265488e-05 -1.3950594e-05 -1.5714148e-05 -1.7629325e-05 -1.8177197e-05]]...]
INFO - root - 2017-12-09 07:31:14.213776: step 3610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:23m:10s remains)
INFO - root - 2017-12-09 07:31:22.816451: step 3620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:56m:00s remains)
INFO - root - 2017-12-09 07:31:31.542489: step 3630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 81h:14m:20s remains)
INFO - root - 2017-12-09 07:31:40.188528: step 3640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:05m:19s remains)
INFO - root - 2017-12-09 07:31:48.780965: step 3650, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.727 sec/batch; 66h:23m:30s remains)
INFO - root - 2017-12-09 07:31:57.433831: step 3660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:14m:47s remains)
INFO - root - 2017-12-09 07:32:06.113980: step 3670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 81h:07m:44s remains)
INFO - root - 2017-12-09 07:32:14.751548: step 3680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:32m:10s remains)
INFO - root - 2017-12-09 07:32:23.408293: step 3690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:04m:36s remains)
INFO - root - 2017-12-09 07:32:32.146973: step 3700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:17m:59s remains)
2017-12-09 07:32:32.994452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2974287e-05 -2.110979e-05 -1.7857164e-05 -9.8834644e-06 4.9993396e-06 2.3983433e-05 3.8492755e-05 4.7453024e-05 5.0891729e-05 4.9883791e-05 4.9627008e-05 5.0220697e-05 5.3734941e-05 5.6169534e-05 5.7975369e-05][-1.4932106e-05 -1.1859316e-05 -4.7990179e-06 1.1426098e-05 3.8763319e-05 7.0935275e-05 9.56062e-05 0.00011315678 0.00012318624 0.00012518861 0.00012616848 0.00012741025 0.00012940397 0.00013089892 0.00013218533][-8.1161124e-06 -2.5344125e-06 1.0786833e-05 3.919271e-05 7.9615565e-05 0.00012356203 0.00016027887 0.00018867648 0.00020635787 0.00021333904 0.0002151096 0.00021650553 0.00021697329 0.00021730237 0.00021674765][-4.2827232e-06 3.6508936e-06 2.321491e-05 6.1778235e-05 0.00011114856 0.00015995374 0.00020321734 0.00024051445 0.00026648719 0.00027718372 0.00027805503 0.00027497124 0.00027106673 0.00026780466 0.00026530278][-7.397306e-06 3.2306998e-06 2.7112037e-05 6.6821856e-05 0.00011714002 0.00016460655 0.00020916246 0.00025000254 0.00027887925 0.00029036344 0.00028789457 0.00027764158 0.00026533881 0.00025443692 0.00024901197][-7.081886e-06 5.7357756e-06 3.2199467e-05 7.0957656e-05 0.00011741357 0.00016178509 0.0002031818 0.00024064204 0.00026852492 0.00027623388 0.00026580307 0.0002449259 0.00022066692 0.0001980136 0.00018670216][-3.0158917e-06 1.4016972e-05 4.3753324e-05 8.5150241e-05 0.00012900877 0.00016859276 0.00020474558 0.00023730141 0.00025679567 0.00025790511 0.00023912678 0.00020623875 0.00016914726 0.00013502284 0.00011905482][-3.3343313e-06 1.3637007e-05 4.1434811e-05 7.9154692e-05 0.00011468929 0.00014408217 0.00017052771 0.00019410717 0.00020787019 0.00020413088 0.00018224945 0.00014582329 0.00010669015 7.3599833e-05 5.6429548e-05][-9.6374788e-06 1.4445322e-06 1.885432e-05 4.2912594e-05 6.6279696e-05 8.36408e-05 9.9012861e-05 0.00011412313 0.00012302632 0.00011937102 0.00010142894 7.4106632e-05 4.678758e-05 2.4396679e-05 1.164787e-05][-1.4841466e-05 -9.0327958e-06 -1.2879536e-06 9.065523e-06 2.0273284e-05 2.9646988e-05 3.7840749e-05 4.456997e-05 4.806037e-05 4.543284e-05 3.5202473e-05 2.148949e-05 8.3244013e-06 -1.4307734e-06 -7.0022943e-06][-1.6691731e-05 -1.2053519e-05 -8.22459e-06 -4.1370222e-06 1.0331132e-07 4.0674058e-06 7.7665172e-06 9.7445009e-06 1.0910378e-05 9.5704527e-06 4.9351947e-06 -9.1183028e-07 -5.1701209e-06 -8.7249427e-06 -1.136947e-05][-1.7276176e-05 -1.3607852e-05 -1.116965e-05 -8.8158849e-06 -6.4577689e-06 -4.3955733e-06 -2.9975126e-06 -2.3186949e-06 -2.5202389e-06 -3.4202312e-06 -5.4128031e-06 -8.16692e-06 -1.0165408e-05 -1.1939337e-05 -1.2835604e-05][-1.9513835e-05 -1.672142e-05 -1.5359041e-05 -1.4565318e-05 -1.3796394e-05 -1.2585802e-05 -1.172872e-05 -1.1052209e-05 -1.0638905e-05 -1.0617267e-05 -1.1311036e-05 -1.2573717e-05 -1.3743316e-05 -1.4559515e-05 -1.507275e-05][-2.1031949e-05 -1.8759085e-05 -1.7820774e-05 -1.7707564e-05 -1.7486786e-05 -1.6978498e-05 -1.6597027e-05 -1.6210604e-05 -1.584502e-05 -1.5876478e-05 -1.6269591e-05 -1.6640766e-05 -1.6910464e-05 -1.7194645e-05 -1.7469501e-05][-2.2816679e-05 -2.0915348e-05 -2.027259e-05 -2.0385927e-05 -2.028537e-05 -2.0055755e-05 -2.0032228e-05 -1.9943185e-05 -2.0073196e-05 -2.0194751e-05 -2.0433137e-05 -2.0518757e-05 -2.0409225e-05 -2.0189938e-05 -2.0125291e-05]]...]
INFO - root - 2017-12-09 07:32:41.079249: step 3710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:45m:08s remains)
INFO - root - 2017-12-09 07:32:49.654953: step 3720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:20m:28s remains)
INFO - root - 2017-12-09 07:32:58.295581: step 3730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:27m:53s remains)
INFO - root - 2017-12-09 07:33:06.851467: step 3740, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:35m:04s remains)
INFO - root - 2017-12-09 07:33:15.377284: step 3750, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:04m:34s remains)
INFO - root - 2017-12-09 07:33:23.894234: step 3760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:29m:06s remains)
INFO - root - 2017-12-09 07:33:32.651845: step 3770, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:52m:56s remains)
INFO - root - 2017-12-09 07:33:41.144154: step 3780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:27m:11s remains)
INFO - root - 2017-12-09 07:33:49.777019: step 3790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:33m:07s remains)
INFO - root - 2017-12-09 07:33:58.339875: step 3800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:18m:15s remains)
2017-12-09 07:33:59.175570: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0149791 0.023279088 0.032088637 0.039978046 0.045796588 0.049224921 0.050651532 0.050787505 0.050123077 0.0490353 0.047785416 0.046463095 0.045146782 0.043876659 0.042633146][0.016526483 0.02566 0.035405908 0.044237528 0.050920691 0.054979946 0.056673981 0.056583241 0.055153646 0.05282804 0.050030354 0.04714334 0.044492193 0.042336863 0.040781923][0.01651622 0.025815884 0.035972714 0.045503151 0.053118858 0.058095396 0.0603625 0.060233545 0.058055159 0.054306738 0.049621243 0.044731755 0.040308148 0.036917806 0.034825619][0.014996038 0.023881443 0.034079488 0.044245116 0.0530005 0.05927889 0.062526837 0.062659875 0.059865788 0.054669585 0.047969725 0.040903348 0.03457069 0.029831944 0.02707405][0.012351472 0.02049939 0.030541221 0.041382909 0.051501311 0.059396833 0.063936219 0.064558424 0.061242595 0.054594215 0.045847818 0.036623456 0.028478898 0.02251284 0.019156029][0.0092869783 0.016564908 0.026371175 0.037822776 0.049327984 0.058922704 0.0648493 0.0660485 0.0622889 0.054319404 0.0437566 0.032690063 0.023074249 0.016170343 0.012363531][0.0066245142 0.013102622 0.022644602 0.034547962 0.047160305 0.058140103 0.06515421 0.066697247 0.062428933 0.053321484 0.041382886 0.029095974 0.018657573 0.011309478 0.0072870767][0.004841831 0.010684655 0.019897532 0.031920988 0.045075454 0.056778863 0.064298548 0.065815836 0.060954753 0.05098756 0.0382665 0.025543725 0.015075617 0.0079215216 0.0040542618][0.0038192314 0.0091004288 0.017803611 0.029465154 0.042453088 0.0540897 0.06144933 0.062578872 0.057193022 0.046829015 0.03406832 0.021753095 0.012005142 0.0056045856 0.0022429833][0.0031832927 0.0079270154 0.015862118 0.026620237 0.038675923 0.049431179 0.056001406 0.056512427 0.050877426 0.040760182 0.028809935 0.017718486 0.0092993118 0.004022758 0.0013777947][0.0026464867 0.0066851876 0.013515365 0.022769062 0.033122364 0.042254124 0.047573753 0.047485657 0.042087752 0.033012562 0.022721643 0.013525526 0.0068266094 0.0028225989 0.00091629929][0.0020732398 0.0052618356 0.010665132 0.018010587 0.026134444 0.033171695 0.037020031 0.036483943 0.031788819 0.024387728 0.016333917 0.0094068525 0.0045716628 0.0018227739 0.0005892872][0.00144986 0.0037028689 0.0075476971 0.012789682 0.018559387 0.023442075 0.02591674 0.025201214 0.021565989 0.016164875 0.010519203 0.0058418009 0.002723308 0.0010533781 0.00035893719][0.00081454247 0.0021629781 0.0045288429 0.0078090453 0.011420851 0.014435127 0.015865589 0.015238522 0.012810728 0.00937152 0.0059115188 0.003154326 0.0014081696 0.00053942867 0.00021251956][0.00035947206 0.00098493556 0.0021601452 0.0038646229 0.0057578906 0.0073328456 0.0080485679 0.0076685031 0.006345008 0.0045256 0.0027606629 0.0014150619 0.0006202776 0.00026150909 0.00014207508]]...]
INFO - root - 2017-12-09 07:34:07.550032: step 3810, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:17m:17s remains)
INFO - root - 2017-12-09 07:34:15.960310: step 3820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:02m:15s remains)
INFO - root - 2017-12-09 07:34:24.566013: step 3830, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 81h:34m:31s remains)
INFO - root - 2017-12-09 07:34:33.355752: step 3840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:28m:23s remains)
INFO - root - 2017-12-09 07:34:42.219152: step 3850, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 81h:15m:59s remains)
INFO - root - 2017-12-09 07:34:50.913333: step 3860, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 84h:19m:32s remains)
INFO - root - 2017-12-09 07:34:59.597432: step 3870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:15m:44s remains)
INFO - root - 2017-12-09 07:35:08.187002: step 3880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:35m:03s remains)
INFO - root - 2017-12-09 07:35:16.842444: step 3890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:20m:10s remains)
INFO - root - 2017-12-09 07:35:25.573499: step 3900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:08m:52s remains)
2017-12-09 07:35:26.429430: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.009322457 0.016103307 0.023220934 0.0287681 0.03149318 0.031138495 0.028405357 0.024463246 0.020531084 0.017456567 0.015819434 0.015953584 0.018114928 0.021626132 0.026185865][0.01350127 0.022643713 0.032069102 0.039351638 0.042938393 0.042571694 0.039216496 0.034363858 0.029487986 0.025626888 0.023368744 0.022934821 0.024548383 0.027629996 0.031897616][0.017910417 0.029351143 0.040966697 0.049916163 0.054403454 0.054246869 0.050650638 0.045278758 0.03979611 0.03537178 0.032596558 0.031613473 0.032523166 0.034770753 0.038105402][0.022056796 0.035369456 0.048689228 0.0589675 0.064291857 0.064600162 0.061248839 0.055935029 0.050312512 0.045645632 0.042540852 0.041085672 0.041253787 0.042693704 0.044918887][0.025208177 0.03970325 0.054060817 0.065237 0.071315691 0.072347037 0.069725707 0.065073304 0.059905332 0.055416994 0.05224302 0.050400507 0.049772751 0.050219942 0.051150389][0.026738143 0.0417038 0.056475807 0.068175234 0.07496047 0.076934472 0.075396225 0.07181336 0.067467988 0.063466907 0.060418777 0.058285564 0.056833278 0.056165826 0.055680443][0.026504245 0.041301232 0.055961274 0.067853287 0.075258777 0.078281254 0.078064673 0.0758547 0.072622277 0.069284029 0.0663873 0.063904047 0.0615645 0.059649404 0.057665017][0.02465215 0.038611025 0.05261622 0.064303242 0.07208728 0.076089509 0.077243246 0.076533407 0.074609987 0.072142035 0.0695163 0.0667396 0.0635529 0.060352366 0.056938216][0.021443076 0.033942048 0.046714984 0.05772442 0.065537907 0.070222341 0.07252086 0.073229544 0.072659343 0.071206987 0.069039054 0.066194631 0.0624246 0.058214083 0.053656131][0.017190926 0.027755687 0.038825717 0.048709679 0.056125615 0.061091434 0.064191595 0.066071533 0.0668294 0.06651444 0.064999409 0.062323745 0.058264133 0.053339869 0.048029207][0.012671838 0.021018783 0.030043349 0.038379345 0.044932459 0.049668469 0.053061597 0.055673916 0.057427879 0.058150545 0.057374038 0.055099446 0.051152833 0.046046719 0.040462036][0.0085332487 0.014644042 0.021520067 0.02809114 0.033466969 0.037571184 0.040816035 0.04367125 0.0459724 0.047394097 0.047267191 0.045516491 0.042036608 0.037252892 0.031900849][0.0050830189 0.009124551 0.013901627 0.018657111 0.022682084 0.025923502 0.028704604 0.031398591 0.033807479 0.035560608 0.035927441 0.03478127 0.031986836 0.027978491 0.023331996][0.0025104445 0.0048453421 0.0077642673 0.01081068 0.013515415 0.015812371 0.01795911 0.020245135 0.022454223 0.024207182 0.024846403 0.024209505 0.022170445 0.019081941 0.015458409][0.00093847682 0.002009182 0.0034629235 0.0050806417 0.0065974868 0.0079950662 0.0094495919 0.011152088 0.012931663 0.014456649 0.015179537 0.014922027 0.013579497 0.011421716 0.0088751353]]...]
INFO - root - 2017-12-09 07:35:34.846959: step 3910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 76h:30m:17s remains)
INFO - root - 2017-12-09 07:35:43.402346: step 3920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:46m:35s remains)
INFO - root - 2017-12-09 07:35:52.009030: step 3930, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:51m:26s remains)
INFO - root - 2017-12-09 07:36:00.706433: step 3940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:11m:16s remains)
INFO - root - 2017-12-09 07:36:09.316934: step 3950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:37m:40s remains)
INFO - root - 2017-12-09 07:36:17.718916: step 3960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:50m:22s remains)
INFO - root - 2017-12-09 07:36:26.316741: step 3970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:55m:49s remains)
INFO - root - 2017-12-09 07:36:34.791145: step 3980, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:50m:16s remains)
INFO - root - 2017-12-09 07:36:43.468911: step 3990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:38m:31s remains)
INFO - root - 2017-12-09 07:36:52.082592: step 4000, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:31m:46s remains)
2017-12-09 07:36:52.938041: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045220122 0.044637337 0.041088752 0.035117473 0.02771762 0.0200602 0.01340852 0.0086211357 0.0057996791 0.004359833 0.0035918627 0.0031063252 0.0027167113 0.0023290324 0.001922369][0.047507189 0.047778659 0.045295533 0.040401541 0.033822108 0.026500281 0.019639893 0.014203024 0.010579206 0.0084315781 0.007145653 0.0062881387 0.0055808034 0.0048681549 0.0040951637][0.048454076 0.049936477 0.048966836 0.04566494 0.040457152 0.034024388 0.027442912 0.021705892 0.017442338 0.014580368 0.012677267 0.011288401 0.010089018 0.0088661043 0.0075245309][0.048443731 0.0513441 0.052171566 0.050828841 0.047446698 0.042373635 0.036499158 0.0307879 0.026025793 0.022419881 0.019753003 0.017665604 0.01581922 0.013948491 0.011917449][0.04791313 0.052197091 0.054777883 0.055415545 0.053995941 0.050577052 0.045787971 0.040462669 0.035436735 0.031152021 0.02765434 0.024730304 0.022091506 0.019461941 0.016663022][0.046506628 0.052130423 0.056357212 0.058867313 0.059346672 0.057623282 0.054083932 0.049399953 0.044379991 0.039621063 0.0353782 0.031596 0.028099623 0.024651205 0.021067485][0.04373052 0.05051082 0.056172885 0.060389504 0.06265828 0.062608406 0.060392357 0.05651753 0.051716343 0.046680737 0.041827191 0.037278261 0.03298448 0.028771726 0.024474541][0.039546065 0.047106113 0.053905379 0.059535537 0.063362464 0.064879581 0.063981511 0.061005149 0.056580704 0.051448531 0.046152182 0.040981345 0.036021661 0.031191006 0.026361035][0.033976018 0.041954894 0.049578652 0.056345228 0.061476443 0.064338975 0.064615175 0.0625127 0.05851816 0.05339203 0.047800303 0.042199872 0.036800548 0.0316117 0.026516438][0.027660226 0.035702672 0.043846432 0.051469129 0.057652369 0.061604604 0.062815726 0.06137082 0.057680354 0.052556042 0.046741668 0.040868223 0.035267837 0.030014379 0.024989443][0.020881921 0.02863227 0.037009139 0.045285437 0.052388042 0.057308387 0.059311837 0.058355279 0.054848529 0.049662206 0.04361989 0.037537672 0.031880856 0.026802458 0.022164475][0.014837055 0.021887178 0.030106125 0.038714778 0.0465165 0.052254163 0.054935232 0.054307248 0.05082 0.045426246 0.039092977 0.032832257 0.027229493 0.022529587 0.018558061][0.010246097 0.016451512 0.024226964 0.032806959 0.040962558 0.047228094 0.050338123 0.049860295 0.046271451 0.04061269 0.033992618 0.027578661 0.022093019 0.017863864 0.014704059][0.0074292044 0.01291861 0.020170037 0.028478371 0.036621921 0.043005362 0.046201374 0.045643993 0.041834388 0.035893358 0.029013854 0.022470694 0.017107671 0.013374017 0.011061475][0.0063648834 0.011354445 0.01806861 0.025881927 0.03361398 0.039704651 0.042674635 0.041852981 0.03778835 0.031627178 0.024640441 0.018057255 0.012822813 0.0095218569 0.007954469]]...]
INFO - root - 2017-12-09 07:37:01.375013: step 4010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:35m:43s remains)
INFO - root - 2017-12-09 07:37:10.060253: step 4020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:02m:52s remains)
INFO - root - 2017-12-09 07:37:18.584331: step 4030, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:59m:55s remains)
INFO - root - 2017-12-09 07:37:27.147457: step 4040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:04m:35s remains)
INFO - root - 2017-12-09 07:37:35.655427: step 4050, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 77h:06m:28s remains)
INFO - root - 2017-12-09 07:37:43.980814: step 4060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:36m:12s remains)
INFO - root - 2017-12-09 07:37:52.678607: step 4070, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 81h:33m:03s remains)
INFO - root - 2017-12-09 07:38:00.960687: step 4080, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 75h:50m:03s remains)
INFO - root - 2017-12-09 07:38:09.457057: step 4090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:25m:48s remains)
INFO - root - 2017-12-09 07:38:18.034851: step 4100, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:32m:17s remains)
2017-12-09 07:38:18.864804: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00039401324 0.00043899447 0.00044296813 0.00041587852 0.00038117421 0.00036845094 0.00038834219 0.00042339283 0.00044297913 0.00043127805 0.00040348872 0.00037217309 0.00033899394 0.00029312135 0.0002453654][0.00067649246 0.00075297174 0.00077877671 0.00077095348 0.00075298967 0.00075064914 0.00077676331 0.00080737879 0.00080580672 0.00076388125 0.00071282638 0.00067786768 0.00065560231 0.00062470266 0.00058227673][0.0010830435 0.0011915108 0.0012391079 0.0012458238 0.0012347655 0.0012298028 0.0012419715 0.0012424925 0.0011966556 0.0011124195 0.0010442456 0.0010316839 0.0010589928 0.0010853403 0.0010824893][0.001605015 0.001728253 0.0017692671 0.0017589489 0.0017201286 0.0016819342 0.001654398 0.0016098602 0.0015201147 0.00141095 0.0013502735 0.0013790716 0.0014784607 0.0015927124 0.0016572983][0.0021874092 0.0022992606 0.00230536 0.0022517217 0.0021622388 0.0020710304 0.001993956 0.0019095717 0.0017965866 0.001687001 0.001648639 0.0017229413 0.001883971 0.0020642083 0.0021876961][0.0028090584 0.0028998649 0.0028630684 0.0027640408 0.0026296184 0.0024933061 0.0023789119 0.0022732632 0.0021592239 0.0020620504 0.0020383995 0.0021262758 0.0022999104 0.0024935398 0.0026280847][0.0034448474 0.0035257558 0.0034740402 0.0033636005 0.0032161532 0.0030571106 0.0029145577 0.0027857164 0.0026601353 0.0025562269 0.0025190464 0.0025793996 0.0027152922 0.002867308 0.002958185][0.0039995378 0.0041083186 0.0040689833 0.0039793304 0.003850013 0.0036896181 0.0035165115 0.0033446609 0.0031739576 0.0030304743 0.0029498765 0.0029563478 0.0030320287 0.0031225458 0.0031532983][0.0043491106 0.0045029907 0.0044849478 0.0044197366 0.0043094731 0.0041514337 0.0039528874 0.0037308028 0.003509341 0.0033131912 0.0031784151 0.0031231551 0.0031349817 0.0031672006 0.0031488668][0.00438309 0.004577721 0.0045888131 0.0045476253 0.004458684 0.0043089176 0.0041036727 0.0038598538 0.0036059215 0.0033699225 0.0031904494 0.0030868431 0.0030444916 0.0030207352 0.0029548758][0.0039849142 0.004212704 0.0042741597 0.0042828978 0.00424409 0.0041401098 0.0039740484 0.0037552889 0.0035065454 0.0032539619 0.0030368946 0.0028794662 0.00277045 0.0026746576 0.002547069][0.0031522184 0.003422549 0.0035642961 0.0036605634 0.0037075027 0.0036879696 0.0035953168 0.0034276913 0.0032027606 0.0029501475 0.0027052998 0.0024965317 0.0023199511 0.0021532285 0.0019650895][0.0021174813 0.002382837 0.0025836418 0.0027580652 0.0028903186 0.0029560828 0.0029338272 0.0028189064 0.0026264007 0.0023884771 0.0021398279 0.0019099209 0.0017040074 0.0015144938 0.0013189756][0.0011798925 0.0013874476 0.0015760582 0.0017570567 0.0019109854 0.0020106859 0.0020275482 0.0019559304 0.0018119605 0.0016249715 0.001420525 0.0012276253 0.0010563784 0.00090484053 0.0007586953][0.00054620183 0.00067269243 0.0008008501 0.00093055365 0.0010467413 0.0011270043 0.0011493566 0.0011097229 0.0010187087 0.00089783175 0.00076769048 0.00064727594 0.00054230331 0.00045384429 0.00037391292]]...]
INFO - root - 2017-12-09 07:38:27.333630: step 4110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:49m:22s remains)
INFO - root - 2017-12-09 07:38:36.023542: step 4120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:59m:55s remains)
INFO - root - 2017-12-09 07:38:44.748666: step 4130, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:14m:51s remains)
INFO - root - 2017-12-09 07:38:53.450949: step 4140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 81h:21m:38s remains)
INFO - root - 2017-12-09 07:39:02.194793: step 4150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:34m:01s remains)
INFO - root - 2017-12-09 07:39:10.676451: step 4160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:10m:50s remains)
INFO - root - 2017-12-09 07:39:19.280442: step 4170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:30m:04s remains)
INFO - root - 2017-12-09 07:39:27.814268: step 4180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:45m:59s remains)
INFO - root - 2017-12-09 07:39:36.356938: step 4190, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 75h:54m:25s remains)
INFO - root - 2017-12-09 07:39:44.968614: step 4200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:57m:24s remains)
2017-12-09 07:39:45.807827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9997438e-05 -2.9166342e-05 -2.2284803e-05 1.9093539e-05 0.00016372229 0.00052178022 0.0011549775 0.0018944091 0.0024083634 0.0024136668 0.0019093833 0.0011335619 0.00045014443 7.8041048e-05 -2.5691807e-05][-2.9812334e-05 -2.7543545e-05 -9.3553681e-06 8.4836196e-05 0.00038706209 0.0010839797 0.0022214928 0.0034249926 0.0041470607 0.0040309886 0.003184193 0.0019564284 0.00084459223 0.0002067966 -1.4625963e-05][-2.9050825e-05 -2.408016e-05 1.2456178e-05 0.00018112379 0.00068978674 0.0018059191 0.0035285398 0.0052568647 0.0062246248 0.0060067414 0.0047902665 0.0030388867 0.0014011954 0.00040322347 1.39353e-05][-2.7490376e-05 -1.8989442e-05 4.1720516e-05 0.00029652074 0.0010350127 0.0025948328 0.0049235197 0.0072204187 0.00850318 0.0082688341 0.006719687 0.0043882341 0.0021209009 0.00066772348 6.0581631e-05][-2.6777863e-05 -1.3739227e-05 7.2183495e-05 0.00041169705 0.0013561489 0.0033009502 0.0061509176 0.0089548463 0.010564091 0.01037876 0.008583718 0.0057398584 0.0028583291 0.00093421235 0.00010607459][-2.6692749e-05 -9.1258262e-06 9.7486147e-05 0.00050052395 0.0015849858 0.0037720867 0.0069463551 0.010074998 0.011916992 0.011777266 0.0098183369 0.0066255052 0.0033430457 0.0011043524 0.00013007558][-2.7213693e-05 -6.0001621e-06 0.00011190999 0.0005366843 0.0016478562 0.0038529395 0.00703939 0.01021347 0.01213542 0.012043452 0.010044342 0.0067629847 0.0033916768 0.0011102287 0.00012912888][-2.9188035e-05 -6.2688559e-06 0.00010991156 0.00050885661 0.0015265555 0.00352105 0.0064062369 0.0093339123 0.011182437 0.011164801 0.0093003232 0.0062141768 0.003080531 0.00098752731 0.00011195856][-3.1676173e-05 -9.800191e-06 9.1693291e-05 0.00042776988 0.0012685152 0.002900756 0.005276232 0.0077447183 0.0093733659 0.0094206939 0.0078387521 0.0051974277 0.0025427127 0.00079511886 8.5717271e-05][-3.3655524e-05 -1.5989302e-05 6.1306011e-05 0.000312365 0.00093635358 0.0021461761 0.0039253859 0.0058112619 0.0070927222 0.0071586384 0.0059358678 0.003892428 0.0018691262 0.00056186481 4.9372975e-05][-3.46573e-05 -2.2666216e-05 2.6432514e-05 0.00018948549 0.00059845857 0.0013984969 0.0025969758 0.0038901598 0.00477945 0.0048122527 0.0039389813 0.0025205263 0.0011652396 0.00032256352 1.2604629e-05][-3.4508168e-05 -2.8401195e-05 -4.5416964e-06 8.0567392e-05 0.00030308284 0.00074898521 0.0014384529 0.0022057714 0.0027420078 0.0027512594 0.0021983173 0.0013447 0.00057326478 0.00012967293 -1.3357785e-05][-3.4172805e-05 -3.1360625e-05 -2.3441375e-05 9.3811905e-06 0.00010061229 0.00029216029 0.00060472 0.00096888677 0.0012345597 0.0012424412 0.00096356682 0.00054791546 0.00020054073 2.1359512e-05 -2.4346336e-05][-3.3347784e-05 -3.1902873e-05 -3.05769e-05 -2.2912303e-05 1.1681113e-06 5.6620978e-05 0.00015502631 0.00028090994 0.00038049067 0.0003899388 0.00029244766 0.00014677353 3.2748416e-05 -1.7263752e-05 -2.5893034e-05][-3.1128562e-05 -2.9843846e-05 -3.0015141e-05 -2.9239836e-05 -2.6732843e-05 -1.9889339e-05 -4.8377624e-06 1.7055114e-05 3.7776263e-05 4.4643792e-05 3.0020536e-05 3.0980591e-06 -1.8200066e-05 -2.5344729e-05 -2.6096175e-05]]...]
INFO - root - 2017-12-09 07:39:54.322383: step 4210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:28m:37s remains)
INFO - root - 2017-12-09 07:40:03.124652: step 4220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:09m:19s remains)
INFO - root - 2017-12-09 07:40:11.707389: step 4230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:45m:57s remains)
INFO - root - 2017-12-09 07:40:20.412332: step 4240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:00m:33s remains)
INFO - root - 2017-12-09 07:40:29.266515: step 4250, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 81h:10m:42s remains)
INFO - root - 2017-12-09 07:40:37.848073: step 4260, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 73h:12m:43s remains)
INFO - root - 2017-12-09 07:40:46.499072: step 4270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:31m:55s remains)
INFO - root - 2017-12-09 07:40:54.993083: step 4280, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 75h:12m:58s remains)
INFO - root - 2017-12-09 07:41:03.875653: step 4290, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.993 sec/batch; 90h:33m:01s remains)
INFO - root - 2017-12-09 07:41:12.498348: step 4300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:44m:41s remains)
2017-12-09 07:41:13.327893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6054167e-05 -4.6057979e-05 -4.6046593e-05 -4.6106754e-05 -4.6155736e-05 -4.6129026e-05 -4.6048968e-05 -4.6029494e-05 -4.611606e-05 -4.6240017e-05 -4.6350662e-05 -4.6414942e-05 -4.6347341e-05 -4.6273446e-05 -4.6146812e-05][-4.5998167e-05 -4.5935984e-05 -4.5905181e-05 -4.5981145e-05 -4.5994377e-05 -4.5894285e-05 -4.5712932e-05 -4.5709894e-05 -4.5847737e-05 -4.6064175e-05 -4.6272849e-05 -4.6395806e-05 -4.6314006e-05 -4.6219644e-05 -4.6157973e-05][-4.6315326e-05 -4.6204037e-05 -4.6078883e-05 -4.6082958e-05 -4.6066187e-05 -4.5867357e-05 -4.5616343e-05 -4.5707908e-05 -4.6067278e-05 -4.6461486e-05 -4.6706631e-05 -4.684992e-05 -4.6642697e-05 -4.647304e-05 -4.651042e-05][-4.6598172e-05 -4.6407164e-05 -4.6151643e-05 -4.5993482e-05 -4.5898105e-05 -4.5624787e-05 -4.5341847e-05 -4.5619949e-05 -4.6308167e-05 -4.6888374e-05 -4.7139194e-05 -4.7253376e-05 -4.6932735e-05 -4.662326e-05 -4.6680558e-05][-4.6847552e-05 -4.6586298e-05 -4.6322679e-05 -4.6094709e-05 -4.5900619e-05 -4.5521861e-05 -4.5210225e-05 -4.5705252e-05 -4.6687346e-05 -4.750484e-05 -4.7719015e-05 -4.7645448e-05 -4.7156627e-05 -4.6636604e-05 -4.6576508e-05][-4.7314017e-05 -4.7018832e-05 -4.6818961e-05 -4.658509e-05 -4.6277306e-05 -4.5709534e-05 -4.5415407e-05 -4.6119352e-05 -4.7303627e-05 -4.8250196e-05 -4.8455942e-05 -4.8247115e-05 -4.7454181e-05 -4.6678011e-05 -4.6498135e-05][-4.7764119e-05 -4.7448721e-05 -4.7318834e-05 -4.7049238e-05 -4.6749581e-05 -4.6078509e-05 -4.5678506e-05 -4.6458517e-05 -4.7827511e-05 -4.882909e-05 -4.9131173e-05 -4.8926584e-05 -4.7985792e-05 -4.6932211e-05 -4.6516874e-05][-4.8187714e-05 -4.7836817e-05 -4.7746904e-05 -4.7454905e-05 -4.7252772e-05 -4.6808742e-05 -4.664237e-05 -4.7414778e-05 -4.8621125e-05 -4.9586833e-05 -4.9890841e-05 -4.966015e-05 -4.8679776e-05 -4.7423047e-05 -4.6808185e-05][-4.8405265e-05 -4.803525e-05 -4.7974441e-05 -4.7779413e-05 -4.7678524e-05 -4.7567977e-05 -4.7723079e-05 -4.8506558e-05 -4.954906e-05 -5.0275339e-05 -5.0433133e-05 -5.0106639e-05 -4.9123868e-05 -4.7851587e-05 -4.7100624e-05][-4.8273825e-05 -4.7858983e-05 -4.7930684e-05 -4.8077163e-05 -4.8298534e-05 -4.8518894e-05 -4.8753303e-05 -4.9470702e-05 -5.0354942e-05 -5.0822742e-05 -5.0808467e-05 -5.0477935e-05 -4.9505023e-05 -4.8234957e-05 -4.7410413e-05][-4.7814188e-05 -4.7301448e-05 -4.7541544e-05 -4.8025515e-05 -4.8674247e-05 -4.9058133e-05 -4.9056009e-05 -4.9318103e-05 -4.993975e-05 -5.035592e-05 -5.047935e-05 -5.0345956e-05 -4.9694387e-05 -4.8567643e-05 -4.7730973e-05][-4.7043333e-05 -4.6390836e-05 -4.68065e-05 -4.7591373e-05 -4.8548129e-05 -4.9001668e-05 -4.886563e-05 -4.8805334e-05 -4.9130249e-05 -4.948382e-05 -4.964531e-05 -4.9653529e-05 -4.9368238e-05 -4.86308e-05 -4.7937883e-05][-4.6546465e-05 -4.581211e-05 -4.6198762e-05 -4.7067766e-05 -4.8025384e-05 -4.8404774e-05 -4.8231697e-05 -4.8028727e-05 -4.824093e-05 -4.8676036e-05 -4.8932525e-05 -4.9036204e-05 -4.8991646e-05 -4.8579455e-05 -4.8073896e-05][-4.6232595e-05 -4.5549241e-05 -4.5972785e-05 -4.6842295e-05 -4.7766713e-05 -4.8016766e-05 -4.7710528e-05 -4.7354897e-05 -4.7384248e-05 -4.7788308e-05 -4.819224e-05 -4.8407695e-05 -4.8460228e-05 -4.8299982e-05 -4.8046091e-05][-4.5873763e-05 -4.5382374e-05 -4.6004621e-05 -4.6928137e-05 -4.7828227e-05 -4.8016867e-05 -4.7670252e-05 -4.7132358e-05 -4.6847304e-05 -4.7159949e-05 -4.7602323e-05 -4.7733123e-05 -4.7764323e-05 -4.7743019e-05 -4.7687561e-05]]...]
INFO - root - 2017-12-09 07:41:21.840397: step 4310, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:17m:10s remains)
INFO - root - 2017-12-09 07:41:30.604041: step 4320, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 74h:31m:51s remains)
INFO - root - 2017-12-09 07:41:39.269584: step 4330, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:02m:13s remains)
INFO - root - 2017-12-09 07:41:47.861207: step 4340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:28m:30s remains)
INFO - root - 2017-12-09 07:41:56.483108: step 4350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:30m:32s remains)
INFO - root - 2017-12-09 07:42:04.983984: step 4360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:05m:57s remains)
INFO - root - 2017-12-09 07:42:13.371817: step 4370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:44m:59s remains)
INFO - root - 2017-12-09 07:42:21.893365: step 4380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:01m:03s remains)
INFO - root - 2017-12-09 07:42:30.517116: step 4390, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:11m:42s remains)
INFO - root - 2017-12-09 07:42:39.106048: step 4400, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 81h:35m:11s remains)
2017-12-09 07:42:40.021150: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.2441468e-05 2.3946137e-05 -1.2644989e-05 -3.5753383e-05 -4.4633158e-05 -4.6019279e-05 -4.572382e-05 -4.4531324e-05 -4.1670777e-05 -3.3018237e-05 -7.3020274e-06 3.4548968e-05 6.7493995e-05 7.134625e-05 5.7164012e-05][9.6792821e-05 4.7601206e-05 4.4480112e-07 -2.9605297e-05 -4.2026029e-05 -4.475069e-05 -4.5394892e-05 -4.5533034e-05 -4.3920532e-05 -3.710853e-05 -1.036976e-05 3.5653648e-05 7.4031195e-05 8.0737605e-05 6.711793e-05][0.00013582665 8.1733728e-05 2.9498384e-05 -7.6541619e-06 -2.7378337e-05 -3.5466663e-05 -3.9415641e-05 -4.2389736e-05 -4.2923548e-05 -3.5661524e-05 -9.8274541e-06 3.3241697e-05 6.9505651e-05 7.7682562e-05 6.5260785e-05][0.00019117507 0.00013327079 7.7519508e-05 3.5064164e-05 1.06226e-05 1.3812023e-06 -2.7128917e-06 -8.0481841e-06 -1.1715034e-05 -6.852104e-06 1.218402e-05 4.4815592e-05 7.1700124e-05 7.7538643e-05 6.5246873e-05][0.00029257062 0.00023050849 0.0001678577 0.00011342891 7.4667216e-05 5.7407291e-05 5.1252777e-05 5.314796e-05 5.8290418e-05 7.3371717e-05 9.8677876e-05 0.00013092768 0.00015213664 0.00014923407 0.00012448822][0.000464871 0.00040458172 0.00033740356 0.00027500663 0.00022590083 0.00019955209 0.00018550102 0.00019011965 0.00020409083 0.00024019934 0.00029103883 0.000344427 0.00037086976 0.00035211083 0.0002983046][0.00075644907 0.00070356531 0.000635826 0.00056704064 0.00050774013 0.00047594076 0.0004681056 0.00049510441 0.00053942419 0.00060862547 0.00069357263 0.00077183347 0.0008004902 0.00075072749 0.00063725311][0.0011605449 0.001126628 0.0010671414 0.00099735614 0.000931817 0.00089857413 0.00090577407 0.00096392981 0.0010511106 0.0011598383 0.0012760937 0.0013706831 0.0013920179 0.0013013144 0.0011130885][0.0015142902 0.0015114809 0.0014706966 0.0014139391 0.0013620445 0.001343671 0.0013741628 0.0014589161 0.0015751602 0.0017031158 0.0018265565 0.0019157381 0.00192057 0.0017979784 0.0015499362][0.0016844342 0.0017042364 0.0016778465 0.0016399509 0.0016076259 0.0016033164 0.0016502603 0.0017544832 0.0018867325 0.0020205115 0.0021355534 0.0022097612 0.002198898 0.0020608995 0.0017922278][0.0016666871 0.0017028938 0.0016906244 0.0016690973 0.0016538046 0.0016564027 0.0017050151 0.0018053388 0.0019312428 0.0020557321 0.002157466 0.0022169864 0.0021947762 0.0020565803 0.0017935265][0.0015223566 0.0015731136 0.0015763527 0.0015685846 0.001565086 0.0015737336 0.001616947 0.0016942669 0.0017942528 0.0018973931 0.0019818004 0.0020238808 0.0019896678 0.0018567491 0.0016144568][0.0012196646 0.0012791074 0.0012952295 0.0012970511 0.0012988831 0.0013084181 0.0013389686 0.0013895265 0.0014588126 0.0015328008 0.0015949497 0.0016202022 0.0015808218 0.0014613424 0.0012568115][0.00079829845 0.000846871 0.00086457271 0.00086973363 0.00087252434 0.00087884994 0.00089579297 0.00092469947 0.0009656538 0.0010111586 0.0010500401 0.0010632458 0.0010317445 0.00094220159 0.00079831225][0.00037623622 0.00040518661 0.00041741022 0.000421046 0.00042262726 0.00042537338 0.00043266267 0.00044573407 0.00046434632 0.00048571156 0.00050490897 0.00051170652 0.00049551995 0.0004478959 0.00037327962]]...]
INFO - root - 2017-12-09 07:42:48.398200: step 4410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:12m:38s remains)
INFO - root - 2017-12-09 07:42:56.949766: step 4420, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 76h:21m:28s remains)
INFO - root - 2017-12-09 07:43:05.400704: step 4430, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:11m:22s remains)
INFO - root - 2017-12-09 07:43:13.952839: step 4440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:32m:30s remains)
INFO - root - 2017-12-09 07:43:22.578524: step 4450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:06m:32s remains)
INFO - root - 2017-12-09 07:43:31.379937: step 4460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:06m:37s remains)
INFO - root - 2017-12-09 07:43:39.953885: step 4470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:00m:59s remains)
INFO - root - 2017-12-09 07:43:48.434586: step 4480, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:46m:38s remains)
INFO - root - 2017-12-09 07:43:57.159040: step 4490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:40m:52s remains)
INFO - root - 2017-12-09 07:44:05.814406: step 4500, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:46m:07s remains)
2017-12-09 07:44:06.716379: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.4309727e-05 0.00028020309 0.0010705229 0.002764141 0.0053590429 0.0082315551 0.01042464 0.011104134 0.010059892 0.0077791368 0.00510667 0.0028214881 0.0012840286 0.0004612745 0.00011833106][0.00012652218 0.00066198787 0.0021144722 0.0049613263 0.0091419183 0.01369731 0.017171919 0.018313453 0.016772181 0.013221059 0.0089352606 0.0051297988 0.00245766 0.00094926608 0.00027669777][0.00032161296 0.0013586716 0.0038010585 0.00816012 0.014247927 0.020749163 0.02568238 0.0273571 0.025264435 0.020253036 0.014054807 0.0083624851 0.0042048777 0.0017356689 0.00055618346][0.00065157283 0.0023969552 0.0060926448 0.012191604 0.020294091 0.028739154 0.0350805 0.037263062 0.03463687 0.028168777 0.019985972 0.012238876 0.006387434 0.0027679293 0.00094527024][0.0011273077 0.0036744918 0.0087232254 0.016576033 0.026569411 0.036736451 0.0442633 0.046828225 0.043688316 0.035913881 0.025921533 0.016227484 0.008706755 0.0039046609 0.0013925197][0.0017460849 0.0050115716 0.01121801 0.020505019 0.031953231 0.043368697 0.051732071 0.05458647 0.05110893 0.0423863 0.030990612 0.019708624 0.010769799 0.0049316604 0.001802429][0.0023621155 0.0061126729 0.013046617 0.02317369 0.03539544 0.047399797 0.056135543 0.059154168 0.055600893 0.04650135 0.03438149 0.022137545 0.012247539 0.0056690164 0.0020872038][0.0027870624 0.00666544 0.013746381 0.02398905 0.03623398 0.048180066 0.056880623 0.059967607 0.056544848 0.047552757 0.035389815 0.022943167 0.012771863 0.0059305923 0.0021791637][0.0028702058 0.0064717932 0.013075159 0.022705097 0.034254439 0.045541052 0.053804971 0.056841884 0.053746294 0.045330904 0.033799704 0.021932663 0.012203023 0.0056405361 0.0020468766][0.0025690533 0.0055580572 0.011153633 0.019507563 0.029684374 0.039730098 0.047161017 0.050000258 0.047380153 0.039984606 0.02975996 0.019237459 0.010639161 0.0048655947 0.0017317426][0.0020046069 0.0042163855 0.0084881438 0.015102603 0.023371836 0.031673387 0.037893467 0.040348493 0.03828191 0.032254633 0.023887282 0.01531883 0.0083772726 0.0037657288 0.00130182][0.0013124592 0.002781759 0.0057237726 0.010466488 0.016573662 0.022829764 0.027590234 0.029525103 0.02802352 0.023515344 0.01726488 0.010926403 0.0058672475 0.0025709802 0.00085616217][0.0006873809 0.0015345797 0.0033292621 0.0063697458 0.010423224 0.01467612 0.017957484 0.019309195 0.018292975 0.015225085 0.011018277 0.0068322369 0.0035675513 0.0015063989 0.00047795163][0.00025732981 0.00065507233 0.0015813278 0.0032697283 0.0056310222 0.0081807282 0.010179251 0.01100305 0.010374164 0.00851397 0.006022275 0.0036200187 0.0018121388 0.00072420976 0.00021437943][4.677401e-05 0.00018036336 0.00054782809 0.0012978809 0.0024315945 0.0037164276 0.0047563436 0.0051903264 0.0048616873 0.0039079459 0.0026749903 0.0015383882 0.00072484871 0.00026639819 6.9854847e-05]]...]
INFO - root - 2017-12-09 07:44:15.150071: step 4510, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 73h:34m:37s remains)
INFO - root - 2017-12-09 07:44:23.656002: step 4520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:44m:41s remains)
INFO - root - 2017-12-09 07:44:32.147194: step 4530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:23m:38s remains)
INFO - root - 2017-12-09 07:44:40.568128: step 4540, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:07m:19s remains)
INFO - root - 2017-12-09 07:44:49.009180: step 4550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:35m:51s remains)
INFO - root - 2017-12-09 07:44:57.657056: step 4560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:27m:12s remains)
INFO - root - 2017-12-09 07:45:06.098375: step 4570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:43m:04s remains)
INFO - root - 2017-12-09 07:45:14.589002: step 4580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:01m:23s remains)
INFO - root - 2017-12-09 07:45:23.307342: step 4590, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 81h:28m:40s remains)
INFO - root - 2017-12-09 07:45:32.032415: step 4600, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 80h:32m:34s remains)
2017-12-09 07:45:32.879552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0969695e-05 -4.0287516e-05 -4.012799e-05 -3.9775652e-05 -3.9266644e-05 -3.8517861e-05 -3.790449e-05 -3.7383412e-05 -3.7198362e-05 -3.7469687e-05 -3.7953792e-05 -3.8321887e-05 -3.8629263e-05 -3.8605885e-05 -3.8433176e-05][-3.8882354e-05 -3.7538925e-05 -3.6400615e-05 -3.4516725e-05 -3.2240303e-05 -2.9610761e-05 -2.7453716e-05 -2.6306661e-05 -2.6464921e-05 -2.8116701e-05 -3.0653613e-05 -3.3302596e-05 -3.5130215e-05 -3.569918e-05 -3.560177e-05][-3.6603404e-05 -3.1945419e-05 -2.5931738e-05 -1.7932631e-05 -9.4281422e-06 -1.7238272e-06 3.8457583e-06 5.8540827e-06 3.3342149e-06 -3.4096738e-06 -1.2743141e-05 -2.2431923e-05 -3.0225397e-05 -3.47913e-05 -3.635091e-05][-2.9605268e-05 -1.4264115e-05 7.4468917e-06 3.4084551e-05 6.0140977e-05 8.0791644e-05 9.256781e-05 9.3817034e-05 8.3631232e-05 6.2756058e-05 3.5281431e-05 6.9518792e-06 -1.5859638e-05 -2.9836534e-05 -3.5590354e-05][-1.0793839e-05 2.695329e-05 8.1505255e-05 0.00014643546 0.00020831972 0.00025524848 0.00027966758 0.00027955114 0.0002548614 0.00020690094 0.0001434162 7.6957127e-05 2.1766718e-05 -1.4006291e-05 -3.0822976e-05][1.7675455e-05 9.0151145e-05 0.00019609526 0.00032045835 0.00043773447 0.00052469375 0.00056897575 0.00056841277 0.00052358623 0.00043697486 0.00032024024 0.00019543705 8.873664e-05 1.6674698e-05 -1.9749066e-05][5.3142947e-05 0.00016271428 0.00032268726 0.00051068753 0.00068726757 0.00081805623 0.00088722171 0.000892281 0.00083259569 0.00070946454 0.00053648348 0.00034519992 0.00017569464 5.74089e-05 -5.1935931e-06][7.10253e-05 0.00020274022 0.00039619295 0.00062520028 0.00084203307 0.0010058499 0.0010985247 0.0011165799 0.0010575326 0.000917867 0.00070967496 0.00047009287 0.00025106181 9.3841467e-05 8.475552e-06][5.70342e-05 0.00018044951 0.00036683914 0.00059246062 0.00081195513 0.0009840416 0.001088375 0.001120345 0.0010752133 0.00094538083 0.000738953 0.00049303478 0.00026483944 9.9060962e-05 8.0636528e-06][2.4373854e-05 0.00011819305 0.00026336758 0.00044496579 0.00062812795 0.00077771774 0.00087543228 0.00091442844 0.00088664779 0.00078193704 0.00060728064 0.00039824116 0.00020599441 6.897351e-05 -4.5361739e-06][-7.3717165e-06 5.0550043e-05 0.00014294326 0.00026198154 0.00038718974 0.000494773 0.00057112938 0.00060859695 0.00059746747 0.00052710786 0.00040260737 0.00025333095 0.00011838828 2.577233e-05 -2.1462831e-05][-3.1286363e-05 -3.8404032e-06 4.1369705e-05 0.00010153531 0.00016832011 0.00023041133 0.00027890009 0.00030764603 0.00030848963 0.00027234593 0.00020169219 0.00011569584 3.9457555e-05 -1.0626514e-05 -3.440657e-05][-4.3444819e-05 -3.4807941e-05 -1.9228028e-05 2.5795234e-06 2.8648908e-05 5.5362856e-05 7.843809e-05 9.43219e-05 9.8344077e-05 8.5441287e-05 5.6211305e-05 1.9699153e-05 -1.2281351e-05 -3.2274926e-05 -4.0785784e-05][-4.5252942e-05 -4.4459503e-05 -4.1745861e-05 -3.6883084e-05 -2.9988092e-05 -2.1891428e-05 -1.4416175e-05 -8.7781737e-06 -6.696122e-06 -9.9953613e-06 -1.8322877e-05 -2.8656577e-05 -3.70813e-05 -4.1672891e-05 -4.2834825e-05][-4.4201686e-05 -4.4224325e-05 -4.4122644e-05 -4.3589072e-05 -4.2241481e-05 -4.0667466e-05 -3.9249415e-05 -3.8096747e-05 -3.7728845e-05 -3.8475009e-05 -4.0204639e-05 -4.201748e-05 -4.3212436e-05 -4.3464188e-05 -4.2943168e-05]]...]
INFO - root - 2017-12-09 07:45:41.426096: step 4610, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:03m:22s remains)
INFO - root - 2017-12-09 07:45:49.780023: step 4620, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 74h:48m:42s remains)
INFO - root - 2017-12-09 07:45:58.335463: step 4630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:29m:20s remains)
INFO - root - 2017-12-09 07:46:06.861309: step 4640, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:29m:43s remains)
INFO - root - 2017-12-09 07:46:15.452900: step 4650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:14m:52s remains)
INFO - root - 2017-12-09 07:46:24.136880: step 4660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:31m:05s remains)
INFO - root - 2017-12-09 07:46:32.630159: step 4670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:41m:10s remains)
INFO - root - 2017-12-09 07:46:41.167858: step 4680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 76h:20m:46s remains)
INFO - root - 2017-12-09 07:46:49.764163: step 4690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:52m:44s remains)
INFO - root - 2017-12-09 07:46:58.329705: step 4700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:51m:02s remains)
2017-12-09 07:46:59.201922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0057178647 0.0084715094 0.011698795 0.015090832 0.018368097 0.021393837 0.023804605 0.025577795 0.026666086 0.027333258 0.027471976 0.027215334 0.026600938 0.025698682 0.024440574][0.0070490073 0.010215103 0.013879436 0.017704021 0.021369046 0.024575844 0.027143419 0.028983803 0.030059308 0.030544482 0.030478708 0.030147824 0.029399659 0.028414084 0.027091507][0.00828791 0.011401078 0.014921105 0.018547019 0.022003481 0.025059834 0.027567731 0.0294208 0.030527301 0.031064261 0.031049917 0.030651407 0.029821355 0.028910507 0.027747983][0.0092566889 0.012036348 0.015162839 0.018400585 0.021490276 0.024260683 0.026591813 0.028350202 0.029482007 0.030113947 0.030258076 0.030073505 0.029436002 0.028535478 0.02739156][0.0096400129 0.011966469 0.014615472 0.017397808 0.020082677 0.022528986 0.024620095 0.026279259 0.027345048 0.027980559 0.028193491 0.028140325 0.027642511 0.026832318 0.025757028][0.00912511 0.011074468 0.013332025 0.015727375 0.01804691 0.020165304 0.021974679 0.023388714 0.024376672 0.024880385 0.0250683 0.024972131 0.024460996 0.023669116 0.022571128][0.0074662487 0.0091096209 0.011009986 0.013021592 0.01497352 0.016741822 0.018239966 0.019403446 0.020199778 0.020674085 0.020841427 0.02065534 0.020129828 0.019279316 0.018119084][0.0051227384 0.0064334418 0.007981116 0.0095889485 0.01113342 0.012541494 0.013731704 0.014643007 0.015222198 0.015524655 0.015587845 0.015391146 0.014872543 0.01407082 0.01298704][0.0027113734 0.0036433702 0.0047845677 0.0059570307 0.0070829191 0.0081077516 0.0090042287 0.0096844723 0.010044757 0.010158379 0.010068903 0.0098127946 0.0093382774 0.0086206533 0.0077165645][0.0010153997 0.0015385507 0.0022294838 0.0029468501 0.0036315247 0.004290808 0.0049032154 0.0053651291 0.0055541378 0.0055244155 0.0053451532 0.0050491909 0.0046037766 0.0040320936 0.0034213176][0.00017904741 0.00039387352 0.00070303213 0.0010400113 0.0013697192 0.0017221976 0.0020823944 0.0023634636 0.002460273 0.0024059517 0.0022577737 0.0020326874 0.0017189366 0.0013565128 0.0010276815][-1.1307937e-05 3.9227838e-05 0.00012569074 0.00022707629 0.00032985923 0.00046684736 0.0006247501 0.00075113756 0.00078985846 0.00076041109 0.00069243042 0.00058326777 0.00043771378 0.00028854946 0.00018077827][-2.6063994e-05 -2.2263121e-05 -1.6029568e-05 -4.1055973e-06 1.1000113e-05 3.8935956e-05 7.6601929e-05 0.0001067749 0.00011812592 0.00011616656 0.00011070744 9.5271862e-05 6.9047579e-05 4.3437503e-05 2.89784e-05][-2.6997128e-05 -2.6092181e-05 -2.560165e-05 -2.3818189e-05 -2.1077976e-05 -1.7143167e-05 -1.431907e-05 -1.5325822e-05 -1.5097052e-05 -1.1147036e-05 -3.9330334e-06 2.0033476e-06 6.6168359e-06 9.0379181e-06 1.0438678e-05][-2.9259747e-05 -2.8162496e-05 -2.7976937e-05 -2.6981215e-05 -2.6218331e-05 -2.5206893e-05 -2.4552981e-05 -2.6241476e-05 -2.7232229e-05 -2.5522764e-05 -2.1750253e-05 -1.8944251e-05 -1.4811114e-05 -1.1423275e-05 -8.2194747e-06]]...]
INFO - root - 2017-12-09 07:47:07.671796: step 4710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:45m:02s remains)
INFO - root - 2017-12-09 07:47:15.999676: step 4720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:11m:27s remains)
INFO - root - 2017-12-09 07:47:24.679172: step 4730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:55m:02s remains)
INFO - root - 2017-12-09 07:47:33.310963: step 4740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:53m:17s remains)
INFO - root - 2017-12-09 07:47:41.951544: step 4750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:56m:22s remains)
INFO - root - 2017-12-09 07:47:50.608004: step 4760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:21m:09s remains)
INFO - root - 2017-12-09 07:47:59.197572: step 4770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:29m:41s remains)
INFO - root - 2017-12-09 07:48:07.852034: step 4780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:00m:49s remains)
INFO - root - 2017-12-09 07:48:16.464086: step 4790, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 81h:26m:24s remains)
INFO - root - 2017-12-09 07:48:25.221334: step 4800, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:52m:01s remains)
2017-12-09 07:48:26.159260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9858507e-05 -4.9406259e-05 -4.9548755e-05 -4.998672e-05 -5.0060022e-05 -4.9711714e-05 -4.956315e-05 -4.948449e-05 -4.9510385e-05 -4.9585193e-05 -4.9706134e-05 -4.9855687e-05 -5.0022449e-05 -5.0199345e-05 -5.0438455e-05][-4.9319202e-05 -4.8803053e-05 -4.8603746e-05 -4.8116064e-05 -4.7559344e-05 -4.7319427e-05 -4.7357182e-05 -4.7318139e-05 -4.7702433e-05 -4.8057766e-05 -4.8502756e-05 -4.884501e-05 -4.9005637e-05 -4.9225066e-05 -4.9557628e-05][-4.7034377e-05 -4.6871111e-05 -4.70097e-05 -4.67218e-05 -4.614918e-05 -4.5260917e-05 -4.4990134e-05 -4.5115867e-05 -4.559333e-05 -4.6260357e-05 -4.7314952e-05 -4.8139933e-05 -4.8553258e-05 -4.90354e-05 -4.9396862e-05][-4.2786993e-05 -4.2780473e-05 -4.2661362e-05 -4.2089734e-05 -4.1818708e-05 -4.1464897e-05 -4.115999e-05 -4.1605948e-05 -4.2672447e-05 -4.4170356e-05 -4.5889923e-05 -4.7245274e-05 -4.7810852e-05 -4.8339694e-05 -4.8932525e-05][-4.1447449e-05 -4.035965e-05 -3.964538e-05 -3.8093447e-05 -3.6530069e-05 -3.61398e-05 -3.5830253e-05 -3.6578589e-05 -3.9080634e-05 -4.2217471e-05 -4.4614997e-05 -4.6425619e-05 -4.7127032e-05 -4.7762034e-05 -4.8470221e-05][-4.5964152e-05 -4.2572963e-05 -4.05775e-05 -3.7748756e-05 -3.4254095e-05 -3.1374053e-05 -2.8631948e-05 -2.845929e-05 -3.2131684e-05 -3.7770886e-05 -4.1746876e-05 -4.451022e-05 -4.6226749e-05 -4.7331538e-05 -4.7989604e-05][-5.0325856e-05 -4.6917761e-05 -4.4838049e-05 -4.1951313e-05 -3.8040474e-05 -3.3937613e-05 -2.8700728e-05 -2.418254e-05 -2.5323498e-05 -3.0944095e-05 -3.5978261e-05 -4.0184284e-05 -4.332306e-05 -4.5575976e-05 -4.7198395e-05][-5.143863e-05 -4.9087692e-05 -4.8281796e-05 -4.679443e-05 -4.3727028e-05 -4.0381954e-05 -3.648764e-05 -3.2503951e-05 -3.0109513e-05 -3.00014e-05 -3.0992524e-05 -3.3269236e-05 -3.6806821e-05 -4.0604948e-05 -4.3852113e-05][-5.1244529e-05 -4.9556078e-05 -4.9239236e-05 -4.853847e-05 -4.6639056e-05 -4.4820179e-05 -4.3465414e-05 -4.1717565e-05 -3.9606824e-05 -3.8204264e-05 -3.5313766e-05 -3.2220145e-05 -3.217005e-05 -3.4530662e-05 -3.7858488e-05][-5.1377767e-05 -4.9626055e-05 -4.9676695e-05 -4.9923561e-05 -4.8400216e-05 -4.69991e-05 -4.6710185e-05 -4.5924553e-05 -4.5291261e-05 -4.5046934e-05 -4.3552653e-05 -4.0037507e-05 -3.7373007e-05 -3.5382644e-05 -3.474675e-05][-5.0835297e-05 -4.9247868e-05 -4.9687045e-05 -5.0945131e-05 -5.0676921e-05 -4.9488695e-05 -4.8065394e-05 -4.7011967e-05 -4.6761048e-05 -4.6921006e-05 -4.6966354e-05 -4.6252855e-05 -4.5133525e-05 -4.3026721e-05 -4.02963e-05][-4.8538805e-05 -4.7545927e-05 -4.8927737e-05 -5.060104e-05 -5.1240364e-05 -5.120577e-05 -5.0500465e-05 -4.9267815e-05 -4.7940979e-05 -4.7538229e-05 -4.7694713e-05 -4.8265778e-05 -4.9124534e-05 -4.8727652e-05 -4.7319467e-05][-4.5882211e-05 -4.5063927e-05 -4.6499343e-05 -4.8120612e-05 -4.9361446e-05 -5.022189e-05 -5.0253773e-05 -4.9506762e-05 -4.8346894e-05 -4.7652858e-05 -4.7765236e-05 -4.8545997e-05 -4.9798007e-05 -5.0411574e-05 -5.0605977e-05][-4.3802913e-05 -4.2663753e-05 -4.3501088e-05 -4.4490251e-05 -4.5332657e-05 -4.6543206e-05 -4.7464575e-05 -4.7719881e-05 -4.7338195e-05 -4.6913785e-05 -4.7231937e-05 -4.8306094e-05 -4.9397073e-05 -4.9990773e-05 -5.0416678e-05][-4.2620111e-05 -4.1146883e-05 -4.1273095e-05 -4.1367024e-05 -4.1505449e-05 -4.2434127e-05 -4.341975e-05 -4.44331e-05 -4.4695655e-05 -4.4987275e-05 -4.5526624e-05 -4.6272919e-05 -4.722298e-05 -4.80338e-05 -4.899182e-05]]...]
INFO - root - 2017-12-09 07:48:34.889151: step 4810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:30m:24s remains)
INFO - root - 2017-12-09 07:48:43.499959: step 4820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:34m:05s remains)
INFO - root - 2017-12-09 07:48:52.141135: step 4830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:46m:50s remains)
INFO - root - 2017-12-09 07:49:00.929416: step 4840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:05m:28s remains)
INFO - root - 2017-12-09 07:49:09.600958: step 4850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:48m:18s remains)
INFO - root - 2017-12-09 07:49:18.208312: step 4860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:24m:32s remains)
INFO - root - 2017-12-09 07:49:26.919521: step 4870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:26m:48s remains)
INFO - root - 2017-12-09 07:49:35.563216: step 4880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:54m:36s remains)
INFO - root - 2017-12-09 07:49:44.413355: step 4890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:31m:16s remains)
INFO - root - 2017-12-09 07:49:53.178243: step 4900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:02m:39s remains)
2017-12-09 07:49:54.149807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3387973e-05 -3.2476222e-05 -2.961047e-05 -2.4881148e-05 -1.8677303e-05 -1.334698e-05 -8.2523184e-06 -5.421949e-06 -5.3076583e-06 -6.00762e-06 -6.5349304e-06 -6.4649357e-06 -5.8895603e-06 -6.7567962e-06 -8.733361e-06][-2.171193e-05 -2.0603489e-05 -1.669889e-05 -1.0161821e-05 -1.5961923e-06 6.2096224e-06 1.276908e-05 1.6152597e-05 1.7300146e-05 1.7790531e-05 1.8311446e-05 1.9573381e-05 2.0383748e-05 1.9271036e-05 1.6823156e-05][-1.0872987e-05 -1.0474898e-05 -6.158305e-06 2.929024e-06 1.4582416e-05 2.5509835e-05 3.4937802e-05 4.156572e-05 4.5613626e-05 4.8118243e-05 5.0057257e-05 5.1733346e-05 5.217002e-05 5.0263166e-05 4.7920003e-05][-3.168534e-06 -3.3022443e-06 1.8435894e-06 1.340513e-05 2.8650167e-05 4.4317487e-05 5.7948237e-05 6.8532005e-05 7.4893447e-05 7.9203288e-05 8.2195322e-05 8.4027888e-05 8.3940213e-05 8.2077422e-05 8.0509628e-05][3.7333593e-06 4.3663167e-06 1.1204203e-05 2.5461799e-05 4.3671949e-05 6.2649073e-05 7.8854813e-05 9.1704867e-05 9.9850186e-05 0.00010473072 0.00010741689 0.00010962728 0.00011018744 0.00010911025 0.00010840792][1.0606913e-05 1.4036916e-05 2.3484841e-05 3.9401908e-05 5.8594022e-05 7.7489465e-05 9.4300158e-05 0.00010742452 0.00011551379 0.00012052469 0.0001228714 0.0001252864 0.00012618396 0.00012644674 0.00012715554][1.7099592e-05 2.4830653e-05 3.7793252e-05 5.50653e-05 7.352078e-05 9.0681693e-05 0.00010575237 0.00011694406 0.00012286758 0.00012633653 0.00012804434 0.00013010643 0.00013161398 0.00013289176 0.00013419043][2.4929752e-05 3.6206526e-05 5.1280549e-05 6.771637e-05 8.3070838e-05 9.6778815e-05 0.00010931186 0.00011841415 0.00012233073 0.00012413852 0.00012472281 0.00012637005 0.00012813666 0.00012985358 0.0001309808][3.3516269e-05 4.6599736e-05 6.0604354e-05 7.3897674e-05 8.566802e-05 9.5382849e-05 0.00010464961 0.00011229347 0.00011661283 0.00011782483 0.00011705582 0.00011848912 0.00012076104 0.00012322725 0.00012477674][3.8498496e-05 5.0576018e-05 6.2313389e-05 7.2718896e-05 8.1304279e-05 8.7701359e-05 9.4318668e-05 0.00010053811 0.00010475489 0.00010645141 0.00010707622 0.00011024389 0.00011374392 0.00011856027 0.00012118894][3.7356636e-05 4.8194684e-05 5.7724967e-05 6.5158885e-05 7.03193e-05 7.3804542e-05 7.8158548e-05 8.3896048e-05 8.9140456e-05 9.3522562e-05 9.7431279e-05 0.00010270262 0.00010840293 0.00011658161 0.00012192968][3.2485426e-05 4.0912033e-05 4.7337038e-05 5.1820047e-05 5.4340191e-05 5.6158016e-05 5.9501144e-05 6.60657e-05 7.3819225e-05 8.1808343e-05 8.9612171e-05 9.8847762e-05 0.00010853049 0.00011998988 0.00012758721][2.3988185e-05 3.0615534e-05 3.4614226e-05 3.655014e-05 3.7800019e-05 3.8816135e-05 4.1835527e-05 4.8716618e-05 5.84281e-05 6.9839349e-05 8.2457373e-05 9.7712582e-05 0.00011192033 0.00012677888 0.00013665986][1.2102726e-05 1.8492676e-05 2.1696738e-05 2.3437562e-05 2.5037007e-05 2.6859874e-05 3.033784e-05 3.7161946e-05 4.7416725e-05 6.0736573e-05 7.6779812e-05 9.6061929e-05 0.00011482422 0.00013237493 0.00014386597][-9.3586277e-07 4.7889844e-06 8.1798062e-06 1.0593358e-05 1.3062614e-05 1.6034282e-05 2.0501255e-05 2.7629278e-05 3.7819562e-05 5.1792151e-05 6.9231806e-05 9.0328969e-05 0.00011110369 0.0001295229 0.00014118105]]...]
INFO - root - 2017-12-09 07:50:02.800967: step 4910, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 82h:29m:40s remains)
INFO - root - 2017-12-09 07:50:11.437934: step 4920, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:57m:05s remains)
INFO - root - 2017-12-09 07:50:20.170724: step 4930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:16m:04s remains)
INFO - root - 2017-12-09 07:50:28.792923: step 4940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:19m:35s remains)
INFO - root - 2017-12-09 07:50:37.332630: step 4950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:10m:47s remains)
INFO - root - 2017-12-09 07:50:45.942666: step 4960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:33m:49s remains)
INFO - root - 2017-12-09 07:50:54.415837: step 4970, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 74h:48m:57s remains)
INFO - root - 2017-12-09 07:51:03.005794: step 4980, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:35m:48s remains)
INFO - root - 2017-12-09 07:51:11.654018: step 4990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:13m:40s remains)
INFO - root - 2017-12-09 07:51:19.980381: step 5000, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:33m:52s remains)
2017-12-09 07:51:20.912193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022789519 0.027391287 0.033686928 0.041093111 0.048406143 0.054144066 0.056592952 0.054757524 0.048637439 0.039334726 0.028778736 0.019001538 0.011431364 0.0064849881 0.003728614][0.027467642 0.031041382 0.036155626 0.042448111 0.048851471 0.053950891 0.056042429 0.054074891 0.047785707 0.038288843 0.027559564 0.017680028 0.010131536 0.005360255 0.0028927203][0.033174865 0.035219625 0.038620241 0.043296844 0.048370831 0.052579127 0.054225031 0.052201405 0.046042211 0.036712535 0.026133556 0.01640694 0.0090256436 0.0044402038 0.0022275734][0.039124779 0.03949691 0.041036766 0.043919262 0.047562707 0.050882049 0.052209429 0.050400723 0.044672981 0.035804391 0.025579803 0.0160888 0.0088481707 0.0043410384 0.0022235336][0.043946892 0.042698696 0.042583391 0.044063378 0.046749495 0.049541734 0.050810281 0.049304824 0.044105124 0.035857815 0.026098408 0.016826998 0.0095689939 0.0048968792 0.002596837][0.046626579 0.044034354 0.042520549 0.042946152 0.045136996 0.048008248 0.049835831 0.049125511 0.04478145 0.037271529 0.027949722 0.018762546 0.011267214 0.0061721876 0.0034269076][0.046016503 0.042705137 0.040539894 0.040531207 0.042668644 0.045909382 0.048517402 0.048854094 0.045723587 0.039394066 0.030896971 0.022000356 0.014327196 0.00874055 0.005448035][0.041617632 0.038368713 0.036346611 0.036669176 0.039381813 0.043396469 0.046964325 0.048377775 0.046488158 0.041441485 0.034032546 0.025786737 0.018228641 0.012372778 0.0086698812][0.034046631 0.031478524 0.030133422 0.031168297 0.034680694 0.039614379 0.044230554 0.046790987 0.046233345 0.042660069 0.036586683 0.029359741 0.02237777 0.016740151 0.013022841][0.025485909 0.02376073 0.023117943 0.024779262 0.028910691 0.034545787 0.039992817 0.043523613 0.04419319 0.042007577 0.037379075 0.031464417 0.025490066 0.020576911 0.017253816][0.016963288 0.016101943 0.016156707 0.018186392 0.022407215 0.028116448 0.033830281 0.037972398 0.039632153 0.038709708 0.035558641 0.031165313 0.02659874 0.022827312 0.020310326][0.009712073 0.0093750656 0.0098831588 0.011969739 0.015893506 0.021125477 0.0264777 0.030619079 0.032758284 0.032747518 0.030908799 0.028067764 0.025106765 0.022771925 0.02139258][0.0044041667 0.0044011041 0.0050524492 0.0068637924 0.0100883 0.01440139 0.018930156 0.022602923 0.024779582 0.025269218 0.024356615 0.022782879 0.021237854 0.020266857 0.020036578][0.0014201845 0.0015419298 0.0020577684 0.0032940018 0.0054942872 0.00854659 0.011891919 0.01476817 0.016638758 0.017316706 0.017032035 0.016356163 0.015870471 0.015930597 0.016573282][0.00023815327 0.00033208853 0.00061226805 0.0012762974 0.0025317736 0.0043868418 0.0065303878 0.0084689567 0.0098249633 0.010422096 0.010421912 0.010249237 0.010331462 0.010898896 0.011932733]]...]
INFO - root - 2017-12-09 07:51:29.482500: step 5010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:14m:07s remains)
INFO - root - 2017-12-09 07:51:37.886912: step 5020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:55m:12s remains)
INFO - root - 2017-12-09 07:51:46.404176: step 5030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:05m:29s remains)
INFO - root - 2017-12-09 07:51:55.128394: step 5040, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 82h:29m:10s remains)
INFO - root - 2017-12-09 07:52:03.882085: step 5050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:50m:51s remains)
INFO - root - 2017-12-09 07:52:12.627813: step 5060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 79h:31m:58s remains)
INFO - root - 2017-12-09 07:52:21.234350: step 5070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:50m:11s remains)
INFO - root - 2017-12-09 07:52:29.637550: step 5080, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 81h:28m:38s remains)
INFO - root - 2017-12-09 07:52:38.344440: step 5090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:15m:17s remains)
INFO - root - 2017-12-09 07:52:47.060279: step 5100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:30m:52s remains)
2017-12-09 07:52:47.916580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2125182e-05 -6.0857474e-05 -6.1234008e-05 -6.160124e-05 -6.2108622e-05 -6.2613923e-05 -6.2951076e-05 -6.3236308e-05 -6.3379848e-05 -6.3068073e-05 -6.1882005e-05 -5.9865524e-05 -5.7589681e-05 -5.5225504e-05 -5.3494321e-05][-5.9420112e-05 -5.8446716e-05 -5.8731086e-05 -5.9108905e-05 -5.98561e-05 -6.0659342e-05 -6.1122846e-05 -6.1449246e-05 -6.1602041e-05 -6.1208193e-05 -5.995945e-05 -5.7873622e-05 -5.5547396e-05 -5.3361458e-05 -5.1677176e-05][-5.6428395e-05 -5.5834247e-05 -5.6095341e-05 -5.6458543e-05 -5.7141289e-05 -5.7924572e-05 -5.8587248e-05 -5.9415121e-05 -5.9864451e-05 -5.9534748e-05 -5.8390739e-05 -5.6653545e-05 -5.46473e-05 -5.2718464e-05 -5.0998551e-05][-5.3432057e-05 -5.3106083e-05 -5.3353815e-05 -5.3716303e-05 -5.4409404e-05 -5.5180459e-05 -5.5852859e-05 -5.6916542e-05 -5.7773417e-05 -5.7869907e-05 -5.6958663e-05 -5.5506203e-05 -5.4054959e-05 -5.2534855e-05 -5.1165662e-05][-5.06497e-05 -5.0549064e-05 -5.0927065e-05 -5.1504128e-05 -5.2573672e-05 -5.3721807e-05 -5.4345153e-05 -5.5169981e-05 -5.5852772e-05 -5.6108311e-05 -5.5386972e-05 -5.4156524e-05 -5.3209209e-05 -5.2074112e-05 -5.1097337e-05][-4.8901809e-05 -4.8807582e-05 -4.9344748e-05 -5.0093447e-05 -5.1480682e-05 -5.2916042e-05 -5.3611257e-05 -5.4403648e-05 -5.4855987e-05 -5.5035554e-05 -5.4235472e-05 -5.3270349e-05 -5.2754283e-05 -5.1792857e-05 -5.0883285e-05][-4.91824e-05 -4.8969381e-05 -4.9315044e-05 -4.9805185e-05 -5.0913728e-05 -5.2291074e-05 -5.3144075e-05 -5.4257773e-05 -5.4878234e-05 -5.4953445e-05 -5.3919255e-05 -5.298711e-05 -5.2529344e-05 -5.1699182e-05 -5.0974475e-05][-5.0724651e-05 -5.0429109e-05 -5.0451523e-05 -5.0508741e-05 -5.1049181e-05 -5.2021893e-05 -5.2832194e-05 -5.4182539e-05 -5.5257326e-05 -5.5497545e-05 -5.4582917e-05 -5.3778374e-05 -5.3233773e-05 -5.2455362e-05 -5.2005125e-05][-5.2607513e-05 -5.2413263e-05 -5.2287258e-05 -5.2060735e-05 -5.2057883e-05 -5.2431969e-05 -5.3217547e-05 -5.4860426e-05 -5.6313991e-05 -5.6684345e-05 -5.6184188e-05 -5.5576929e-05 -5.4910943e-05 -5.3993408e-05 -5.3515294e-05][-5.4780892e-05 -5.4700919e-05 -5.4643686e-05 -5.454964e-05 -5.4273085e-05 -5.4125318e-05 -5.4655353e-05 -5.6183257e-05 -5.7612844e-05 -5.8098369e-05 -5.787288e-05 -5.7472073e-05 -5.6815021e-05 -5.5814111e-05 -5.5272103e-05][-5.6706493e-05 -5.6379307e-05 -5.6266639e-05 -5.611567e-05 -5.5875826e-05 -5.5597313e-05 -5.5982968e-05 -5.7236251e-05 -5.8499783e-05 -5.8863356e-05 -5.8788806e-05 -5.8579379e-05 -5.7991205e-05 -5.7219084e-05 -5.68879e-05][-5.8125861e-05 -5.755142e-05 -5.7266472e-05 -5.694478e-05 -5.6749261e-05 -5.6614539e-05 -5.6985071e-05 -5.8041125e-05 -5.8907612e-05 -5.9166669e-05 -5.9209353e-05 -5.9105307e-05 -5.8516605e-05 -5.8032787e-05 -5.7950252e-05][-5.9231479e-05 -5.8497197e-05 -5.8171743e-05 -5.7808131e-05 -5.7489611e-05 -5.7250611e-05 -5.7670037e-05 -5.8587542e-05 -5.9181657e-05 -5.9379079e-05 -5.9526024e-05 -5.9487724e-05 -5.9105812e-05 -5.8765636e-05 -5.8803707e-05][-5.9842518e-05 -5.903206e-05 -5.8883321e-05 -5.8646867e-05 -5.832663e-05 -5.8088783e-05 -5.8282414e-05 -5.9025391e-05 -5.9513048e-05 -5.9730581e-05 -5.9946397e-05 -6.0001523e-05 -5.9782251e-05 -5.9511629e-05 -5.9562943e-05][-6.0312625e-05 -5.9561713e-05 -5.959364e-05 -5.939179e-05 -5.91491e-05 -5.8854021e-05 -5.8773618e-05 -5.9018948e-05 -5.9294714e-05 -5.9575072e-05 -5.9903032e-05 -6.016788e-05 -6.0198734e-05 -6.0171635e-05 -6.0292565e-05]]...]
INFO - root - 2017-12-09 07:52:56.464346: step 5110, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:46m:06s remains)
INFO - root - 2017-12-09 07:53:04.997568: step 5120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:43m:19s remains)
INFO - root - 2017-12-09 07:53:13.653347: step 5130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:23m:23s remains)
INFO - root - 2017-12-09 07:53:22.190020: step 5140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:24m:59s remains)
INFO - root - 2017-12-09 07:53:30.662666: step 5150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:24m:34s remains)
INFO - root - 2017-12-09 07:53:39.554312: step 5160, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:58m:22s remains)
INFO - root - 2017-12-09 07:53:48.246949: step 5170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:59m:23s remains)
INFO - root - 2017-12-09 07:53:56.621080: step 5180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 80h:25m:40s remains)
INFO - root - 2017-12-09 07:54:05.303073: step 5190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:51m:00s remains)
INFO - root - 2017-12-09 07:54:13.900157: step 5200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:55m:09s remains)
2017-12-09 07:54:14.757326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.202708e-05 -6.2109029e-05 -6.2220941e-05 -6.2278043e-05 -6.2284249e-05 -6.2250248e-05 -6.2257852e-05 -6.224664e-05 -6.21678e-05 -6.2155581e-05 -6.2192026e-05 -6.2248393e-05 -6.2298473e-05 -6.2330859e-05 -6.2339852e-05][-6.2038467e-05 -6.2012165e-05 -6.21911e-05 -6.2281775e-05 -6.2223495e-05 -6.20628e-05 -6.1981089e-05 -6.187999e-05 -6.1714876e-05 -6.1717481e-05 -6.183701e-05 -6.19191e-05 -6.1971587e-05 -6.2004328e-05 -6.2009974e-05][-6.2732637e-05 -6.2541963e-05 -6.266858e-05 -6.2849649e-05 -6.2797408e-05 -6.2517327e-05 -6.228713e-05 -6.196092e-05 -6.1605824e-05 -6.1524595e-05 -6.1542378e-05 -6.1598563e-05 -6.1723185e-05 -6.1783838e-05 -6.1808139e-05][-6.3290427e-05 -6.3129592e-05 -6.2911378e-05 -6.28822e-05 -6.2673505e-05 -6.2465959e-05 -6.2445193e-05 -6.2369727e-05 -6.2032224e-05 -6.16945e-05 -6.1427359e-05 -6.1337152e-05 -6.1411367e-05 -6.1546365e-05 -6.161489e-05][-6.3168613e-05 -6.3778396e-05 -6.37429e-05 -6.32102e-05 -6.2275183e-05 -6.1489518e-05 -6.1460865e-05 -6.1986815e-05 -6.2327352e-05 -6.2075473e-05 -6.1371335e-05 -6.1124345e-05 -6.1162689e-05 -6.1210027e-05 -6.1323582e-05][-6.2093277e-05 -6.39573e-05 -6.4911175e-05 -6.4588865e-05 -6.291426e-05 -6.0696308e-05 -5.9270351e-05 -5.9991689e-05 -6.1660277e-05 -6.2238221e-05 -6.1648e-05 -6.1260289e-05 -6.1137587e-05 -6.1089653e-05 -6.1155086e-05][-6.0987597e-05 -6.3277388e-05 -6.504175e-05 -6.5697561e-05 -6.4790445e-05 -6.1871244e-05 -5.7777579e-05 -5.733228e-05 -5.9899612e-05 -6.1688916e-05 -6.1876461e-05 -6.1682586e-05 -6.1420185e-05 -6.1239058e-05 -6.11464e-05][-6.2455591e-05 -6.3171479e-05 -6.4732449e-05 -6.6068154e-05 -6.61715e-05 -6.4120708e-05 -6.0517352e-05 -5.8285666e-05 -5.8855581e-05 -6.0494069e-05 -6.1318831e-05 -6.1710482e-05 -6.1559025e-05 -6.1367507e-05 -6.1276616e-05][-6.4705004e-05 -6.4413638e-05 -6.4961205e-05 -6.6159162e-05 -6.6783294e-05 -6.5858083e-05 -6.3914849e-05 -6.2035724e-05 -6.086718e-05 -6.025712e-05 -6.039891e-05 -6.102413e-05 -6.1157887e-05 -6.1168379e-05 -6.1258936e-05][-6.55865e-05 -6.5247543e-05 -6.5327971e-05 -6.6182081e-05 -6.6853681e-05 -6.6767083e-05 -6.6230212e-05 -6.55022e-05 -6.4570457e-05 -6.285515e-05 -6.1431972e-05 -6.1083731e-05 -6.0956951e-05 -6.0951937e-05 -6.11337e-05][-6.49047e-05 -6.4621054e-05 -6.4497857e-05 -6.5077853e-05 -6.5751941e-05 -6.6035966e-05 -6.6083216e-05 -6.599311e-05 -6.5755536e-05 -6.4672735e-05 -6.3270156e-05 -6.2308653e-05 -6.1557977e-05 -6.1285595e-05 -6.1227984e-05][-6.4202577e-05 -6.3693558e-05 -6.3270578e-05 -6.3352825e-05 -6.3847256e-05 -6.4438282e-05 -6.4658627e-05 -6.4231877e-05 -6.3766362e-05 -6.3378364e-05 -6.3163563e-05 -6.3023428e-05 -6.2449013e-05 -6.182526e-05 -6.137637e-05][-6.423243e-05 -6.3275176e-05 -6.2497587e-05 -6.2251478e-05 -6.2589854e-05 -6.3422536e-05 -6.3836473e-05 -6.2746738e-05 -6.1122279e-05 -6.0225742e-05 -6.0611896e-05 -6.1907187e-05 -6.231555e-05 -6.1980012e-05 -6.1412415e-05][-6.4134721e-05 -6.2861654e-05 -6.1879422e-05 -6.152688e-05 -6.2073887e-05 -6.3274594e-05 -6.4322419e-05 -6.3549829e-05 -6.0810638e-05 -5.7451642e-05 -5.6943645e-05 -5.9271824e-05 -6.1172505e-05 -6.1686733e-05 -6.1584447e-05][-6.3563552e-05 -6.2031773e-05 -6.0864884e-05 -6.0569393e-05 -6.1454863e-05 -6.3160624e-05 -6.4754742e-05 -6.4996828e-05 -6.3018961e-05 -5.9389902e-05 -5.6177567e-05 -5.7184825e-05 -5.9534439e-05 -6.1041152e-05 -6.1590239e-05]]...]
INFO - root - 2017-12-09 07:54:23.373066: step 5210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:54m:26s remains)
INFO - root - 2017-12-09 07:54:31.879741: step 5220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:59m:50s remains)
INFO - root - 2017-12-09 07:54:40.388458: step 5230, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:35m:28s remains)
INFO - root - 2017-12-09 07:54:48.887035: step 5240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:00m:39s remains)
INFO - root - 2017-12-09 07:54:57.605273: step 5250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 76h:04m:36s remains)
INFO - root - 2017-12-09 07:55:06.361353: step 5260, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 75h:32m:10s remains)
INFO - root - 2017-12-09 07:55:15.052890: step 5270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:16m:57s remains)
INFO - root - 2017-12-09 07:55:23.574223: step 5280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:28m:07s remains)
INFO - root - 2017-12-09 07:55:32.165708: step 5290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:39m:44s remains)
INFO - root - 2017-12-09 07:55:40.741183: step 5300, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:08m:26s remains)
2017-12-09 07:55:41.646971: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047647577 0.046939287 0.045953449 0.044852551 0.044312559 0.044262759 0.044675868 0.044744045 0.04430756 0.042573061 0.039916832 0.036936056 0.033775039 0.030907243 0.0277755][0.0588099 0.058126185 0.057049494 0.056167427 0.056154739 0.056674108 0.057657994 0.058120105 0.057776287 0.055572245 0.052197628 0.048539836 0.044843785 0.041657969 0.038375076][0.069922805 0.068974748 0.067748629 0.067142546 0.067674987 0.068814993 0.070375152 0.071229279 0.071027674 0.06860286 0.06480372 0.060618136 0.056549769 0.053218089 0.049953911][0.079638094 0.078656673 0.077436969 0.077138461 0.078080647 0.079659835 0.081473395 0.082393691 0.082195379 0.079749562 0.075816758 0.07133843 0.067122087 0.063715465 0.060563289][0.087222978 0.086476907 0.085389145 0.085338734 0.0864679 0.088072896 0.0898131 0.090546064 0.090226293 0.087735154 0.083881974 0.07936947 0.075171851 0.071781613 0.068782181][0.092157707 0.091822922 0.090936147 0.091035329 0.092183359 0.093651079 0.095091611 0.09548682 0.09486144 0.092378028 0.088642091 0.084223181 0.080209509 0.076919042 0.07422246][0.094217643 0.09437824 0.093832888 0.094030559 0.095007725 0.096207187 0.097334318 0.097439744 0.096499138 0.09398482 0.090408728 0.086344168 0.082735553 0.079752229 0.0775774][0.093612053 0.09452875 0.0944657 0.094811484 0.095592409 0.096480064 0.097158648 0.096928932 0.095704153 0.093229987 0.089907452 0.086159147 0.082888186 0.080354169 0.078815877][0.093048662 0.094550177 0.094893709 0.095463783 0.096075661 0.096578769 0.096651 0.095905341 0.094220847 0.09152855 0.088185593 0.084644489 0.081688829 0.079405583 0.078178227][0.092979155 0.095071994 0.095673859 0.096254811 0.096585929 0.096680976 0.096246704 0.095070407 0.092977375 0.090033978 0.086738333 0.0835149 0.080839723 0.078716584 0.0775414][0.09415105 0.096578114 0.097362526 0.097958058 0.09805283 0.097813286 0.096941464 0.095454149 0.093044333 0.089920022 0.086533628 0.083325796 0.080525443 0.078113742 0.076391377][0.094467759 0.097172707 0.09823937 0.099054016 0.099176407 0.098952249 0.097832538 0.096083246 0.0933422 0.089841895 0.086066529 0.082423076 0.079048678 0.075849354 0.073058777][0.095172189 0.098059654 0.0991529 0.099646442 0.099256366 0.09852884 0.096938767 0.094923541 0.092067063 0.088578396 0.084690452 0.080568857 0.076355241 0.072022095 0.067758664][0.095589817 0.098466285 0.099486865 0.099813968 0.099220648 0.098064125 0.096128866 0.093925804 0.090889029 0.087153934 0.082793094 0.077897251 0.072522111 0.066614404 0.060560398][0.096546493 0.099234939 0.0999598 0.099926904 0.099085622 0.0976203 0.095474735 0.093011037 0.08962781 0.085414782 0.08029709 0.074357681 0.067569964 0.060077913 0.052395619]]...]
INFO - root - 2017-12-09 07:55:50.219036: step 5310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:52m:13s remains)
INFO - root - 2017-12-09 07:55:58.779498: step 5320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 77h:45m:28s remains)
INFO - root - 2017-12-09 07:56:07.329021: step 5330, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 73h:46m:42s remains)
INFO - root - 2017-12-09 07:56:15.799345: step 5340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:59m:40s remains)
INFO - root - 2017-12-09 07:56:24.485989: step 5350, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:57m:02s remains)
INFO - root - 2017-12-09 07:56:33.160619: step 5360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:36m:49s remains)
INFO - root - 2017-12-09 07:56:42.015038: step 5370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 79h:27m:52s remains)
INFO - root - 2017-12-09 07:56:50.474254: step 5380, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:31m:37s remains)
INFO - root - 2017-12-09 07:56:59.182802: step 5390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:35m:59s remains)
INFO - root - 2017-12-09 07:57:07.861347: step 5400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:25m:32s remains)
2017-12-09 07:57:08.739906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1478281e-05 -2.3937217e-05 -2.5312474e-05 -2.5653564e-05 -2.8631533e-05 -2.6277296e-05 -2.6131027e-05 -1.5973623e-05 1.9813007e-05 0.00011530985 0.00030202582 0.00061301817 0.0010653294 0.0016228076 0.0022176821][2.1919383e-05 4.1314976e-05 4.0318737e-05 3.8812235e-05 3.4748264e-05 3.6394726e-05 3.5015117e-05 4.9589282e-05 9.1841e-05 0.0002104131 0.00045195094 0.000882644 0.0015488327 0.0024366658 0.003438453][0.00011474765 0.00015035737 0.00015357649 0.00014804697 0.00013981349 0.00015857964 0.00018129224 0.00022083355 0.00029363623 0.00045964058 0.00077627349 0.001345323 0.0022549164 0.0035218603 0.0049902736][0.00023993768 0.0002969054 0.00031954065 0.00033663024 0.00037036851 0.00044929888 0.00055734365 0.00069011463 0.00084487227 0.0010689548 0.0014406057 0.0021005489 0.0031843346 0.0047508068 0.00659952][0.00035362688 0.00045724976 0.00054783595 0.000662476 0.00083211722 0.0010799764 0.0013718461 0.0016621725 0.0019226018 0.002170068 0.0025029881 0.0030868652 0.0041322568 0.005746644 0.0077310116][0.00044981064 0.00064711442 0.00089048257 0.0012261258 0.0016732558 0.0022137433 0.0027577116 0.0031969945 0.0034758942 0.0036032326 0.0037034128 0.00397653 0.0047213621 0.0061024115 0.0079577062][0.00053683104 0.00087634625 0.0013522863 0.0020077247 0.0028217682 0.003699241 0.004459667 0.004932689 0.0050618774 0.0048818053 0.0045712157 0.0043754783 0.00465855 0.005630496 0.0071847788][0.00063530792 0.001133943 0.0018626719 0.0028404277 0.0039796997 0.0050977012 0.0059301555 0.0062754052 0.0060971444 0.0054954952 0.004726266 0.0040845843 0.0039485893 0.0045323321 0.0057524233][0.00069865311 0.0013063224 0.0022027262 0.0033713356 0.0046660188 0.0058355988 0.00656678 0.0066675283 0.0061497046 0.0051916284 0.0041062045 0.003214272 0.0028505942 0.0031921729 0.0041380236][0.00067220832 0.0012827668 0.0021830569 0.0033290514 0.0045555523 0.0055884351 0.0061222557 0.0060006608 0.0052801394 0.004186023 0.0030459 0.0021560371 0.0017746621 0.0020117071 0.0027450465][0.00052272109 0.0010261204 0.0017742872 0.0027237067 0.0037192143 0.0045109838 0.0048396946 0.0045971787 0.0038697582 0.0028890886 0.0019399773 0.0012535385 0.00099478068 0.0012165991 0.0017986132][0.00031225182 0.00065884768 0.0011813022 0.0018427004 0.0025262802 0.00304254 0.0032109905 0.0029674487 0.0024005356 0.0017037789 0.0010801848 0.00067846163 0.00059721322 0.000861721 0.001372455][0.00012447205 0.00031815883 0.00061623944 0.00099713972 0.0013861629 0.0016680342 0.0017383806 0.0015673279 0.0012251113 0.00084008917 0.00053425349 0.00039999059 0.00050289644 0.00086146005 0.0014014677][1.0417229e-05 9.425349e-05 0.00022455226 0.00039723242 0.00057301117 0.00069602055 0.00072032859 0.0006363042 0.00048380415 0.00033278356 0.00025078788 0.00031051206 0.00057054876 0.0010474824 0.0016641399][-3.8838145e-05 -1.380538e-05 2.4343615e-05 8.2190752e-05 0.0001423198 0.00018418406 0.00019171424 0.00016614323 0.00012385452 9.8110431e-05 0.00013523805 0.00030184336 0.0006557153 0.0012017875 0.0018530888]]...]
INFO - root - 2017-12-09 07:57:17.489851: step 5410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:16m:08s remains)
INFO - root - 2017-12-09 07:57:26.327221: step 5420, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 1.004 sec/batch; 91h:12m:06s remains)
INFO - root - 2017-12-09 07:57:34.949346: step 5430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:54m:33s remains)
INFO - root - 2017-12-09 07:57:43.551790: step 5440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:52m:18s remains)
INFO - root - 2017-12-09 07:57:51.998053: step 5450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:21m:22s remains)
INFO - root - 2017-12-09 07:58:00.444918: step 5460, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 74h:18m:17s remains)
INFO - root - 2017-12-09 07:58:08.839128: step 5470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 75h:22m:46s remains)
INFO - root - 2017-12-09 07:58:17.152683: step 5480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:33m:06s remains)
INFO - root - 2017-12-09 07:58:25.818291: step 5490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:54m:40s remains)
INFO - root - 2017-12-09 07:58:34.409035: step 5500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:50m:41s remains)
2017-12-09 07:58:35.281214: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.001685263 0.0023739948 0.0028301722 0.0029054994 0.0025738054 0.0019231467 0.0011258149 0.00044375603 7.5805037e-05 -4.1086445e-05 -3.6273486e-05 -1.5015685e-06 3.7386773e-05 5.6339173e-05 4.2244581e-05][0.0018455458 0.0025832518 0.0030801964 0.0031663484 0.0027842408 0.0020217842 0.0011442594 0.0004448721 7.7902652e-05 -3.4383724e-05 -1.1823366e-05 5.6472352e-05 0.00013409436 0.00016648465 0.00013185979][0.0017555354 0.0024537803 0.0029098182 0.0029718992 0.00261501 0.0019149231 0.0011066296 0.00044767626 8.91711e-05 -1.9987951e-05 1.1934688e-05 0.00010264308 0.00020424504 0.00023741895 0.00017958274][0.0015864267 0.0022315641 0.0026585034 0.0027423287 0.002444942 0.001826658 0.0010965394 0.00047624475 0.00011317865 -1.1787895e-05 1.2569544e-05 0.00010114419 0.00019637728 0.00021689505 0.00015009483][0.0013635976 0.001939137 0.0023346124 0.0024308171 0.0022078624 0.0017189152 0.0010994327 0.00052979466 0.00015241874 -8.0815007e-06 -1.5002857e-05 4.69352e-05 0.00011372804 0.00012189618 6.9673515e-05][0.0011076669 0.0016224724 0.0019923807 0.0021050053 0.0019451997 0.0015569992 0.0010402321 0.00054140441 0.00017810051 -1.1912343e-06 -3.9591439e-05 -1.4835903e-05 1.5948193e-05 1.920529e-05 -7.2776165e-06][0.00081848638 0.0012765751 0.0016534103 0.0018242561 0.0017368142 0.0014216134 0.00097514916 0.00053557981 0.00019950009 1.6022175e-05 -4.7442263e-05 -5.3554784e-05 -4.6609319e-05 -4.4579305e-05 -5.1620416e-05][0.00052814255 0.00092416763 0.0013226449 0.0015723575 0.0015714703 0.0013184687 0.00092918106 0.00054252712 0.00024590304 6.2553438e-05 -2.9526076e-05 -6.2961408e-05 -6.7620014e-05 -6.7849352e-05 -6.79638e-05][0.00028689325 0.00060452882 0.0010017676 0.0013261194 0.0014261503 0.0012534191 0.000926939 0.00059751107 0.00033870692 0.00014446711 1.0583397e-05 -5.4817447e-05 -6.8090594e-05 -6.9712711e-05 -6.9508373e-05][0.00012312314 0.00036519754 0.00071901671 0.0010690207 0.0012495631 0.0011810482 0.00094908109 0.00069091842 0.0004523533 0.00022528137 4.5584362e-05 -4.6855013e-05 -6.7338959e-05 -6.9315742e-05 -6.9822578e-05][4.7321424e-05 0.00023194612 0.00052858214 0.00085390237 0.0010666489 0.0010752458 0.00092265155 0.00071500987 0.0004858851 0.00024048364 4.6964888e-05 -4.67285e-05 -6.7080793e-05 -6.9524925e-05 -7.0232287e-05][1.9560968e-05 0.00015053348 0.00037951794 0.00066582125 0.00088922813 0.00094795896 0.00084650255 0.00065353746 0.00041596682 0.00018124524 1.642909e-05 -5.4564516e-05 -6.7896704e-05 -7.0069378e-05 -7.0974507e-05][-1.0959928e-05 8.0465346e-05 0.000248933 0.00048072546 0.00069218042 0.00078274176 0.00071978732 0.00054693653 0.00032257559 0.00011660984 -1.1759752e-05 -5.9961254e-05 -6.7599205e-05 -6.9626745e-05 -7.0665192e-05][-4.2646374e-05 1.313055e-05 0.00012118804 0.00028332707 0.00045523819 0.00055311935 0.00052550272 0.00039154742 0.00021038213 5.3649266e-05 -3.4808989e-05 -6.3984895e-05 -6.7808222e-05 -6.9439957e-05 -7.0238137e-05][-6.663581e-05 -4.564556e-05 3.8894941e-06 9.1495815e-05 0.00019929127 0.0002770549 0.00027752283 0.00019845451 8.4992345e-05 -5.9678787e-06 -5.2448402e-05 -6.6278793e-05 -6.8304944e-05 -6.9206624e-05 -6.9737587e-05]]...]
INFO - root - 2017-12-09 07:58:43.961497: step 5510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:42m:06s remains)
INFO - root - 2017-12-09 07:58:52.422584: step 5520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:06m:44s remains)
INFO - root - 2017-12-09 07:59:00.925932: step 5530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:38m:30s remains)
INFO - root - 2017-12-09 07:59:09.390893: step 5540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:48m:46s remains)
INFO - root - 2017-12-09 07:59:18.051689: step 5550, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 82h:34m:14s remains)
INFO - root - 2017-12-09 07:59:26.681865: step 5560, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 81h:14m:47s remains)
INFO - root - 2017-12-09 07:59:35.458131: step 5570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:06m:49s remains)
INFO - root - 2017-12-09 07:59:43.863920: step 5580, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 81h:39m:26s remains)
INFO - root - 2017-12-09 07:59:52.558690: step 5590, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 81h:59m:05s remains)
INFO - root - 2017-12-09 08:00:01.263667: step 5600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:06m:58s remains)
2017-12-09 08:00:02.210393: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039148331 0.039927166 0.040065225 0.039559837 0.038376983 0.036842223 0.035160255 0.033522133 0.032026168 0.030684154 0.029571665 0.028785862 0.028256148 0.027705427 0.026705917][0.041445967 0.042595506 0.043089 0.042813629 0.041795567 0.040558815 0.039367173 0.038099144 0.036895663 0.035865366 0.034959588 0.034110442 0.033398025 0.032642771 0.031397026][0.041449133 0.043007132 0.043887362 0.04401888 0.043438327 0.042650595 0.042005818 0.041388687 0.040759359 0.0401416 0.039385416 0.038519986 0.037607256 0.03652072 0.034916054][0.040369242 0.042394821 0.043768268 0.044441503 0.044446077 0.044270981 0.044199351 0.0442139 0.044174157 0.043970454 0.043434463 0.042506028 0.041324176 0.039792039 0.037769098][0.038932942 0.04141495 0.043353364 0.044591997 0.045208778 0.045560092 0.045917481 0.046306133 0.046652958 0.046748441 0.04635936 0.045408059 0.043998465 0.042106632 0.039646994][0.037393816 0.040402468 0.042829003 0.044675428 0.045893323 0.046656489 0.047209546 0.047763012 0.048181277 0.048340797 0.047998957 0.047008716 0.045445003 0.043331392 0.04065197][0.036260966 0.039624095 0.042387791 0.044702325 0.046276804 0.047346123 0.047975417 0.048430353 0.048750263 0.04876335 0.048381135 0.047316272 0.045760915 0.043644894 0.0409474][0.035179421 0.038622569 0.041484665 0.043897022 0.045646679 0.046797141 0.047478884 0.047855325 0.048050642 0.048003413 0.047493953 0.046429139 0.044940419 0.042947084 0.040408328][0.033501625 0.036699653 0.039381083 0.041678041 0.043367691 0.044569474 0.045267943 0.045675982 0.045945849 0.045960389 0.045466386 0.044487596 0.043143716 0.041301791 0.038932908][0.031534169 0.034207121 0.036437716 0.038456455 0.039979957 0.04112884 0.041994654 0.042498335 0.042812005 0.042835187 0.0422903 0.041331679 0.040006034 0.038349375 0.036242254][0.029426618 0.031577107 0.03337707 0.035009813 0.036340807 0.037351944 0.03813915 0.038642507 0.038887717 0.038830414 0.038238596 0.03731586 0.036114432 0.034659181 0.032863889][0.027424935 0.029157115 0.030590249 0.031858377 0.032952279 0.03371679 0.034224726 0.034484524 0.034550432 0.034346398 0.033787087 0.033050712 0.032138728 0.030969612 0.029510688][0.025563456 0.026986819 0.028055893 0.028963475 0.029640075 0.030082075 0.03024439 0.030236959 0.030120583 0.029794827 0.029353643 0.028804913 0.028147737 0.027282976 0.026211254][0.023689466 0.024844315 0.025662221 0.02633775 0.026762845 0.026932463 0.026874574 0.026711037 0.026474562 0.026103608 0.025710439 0.025281403 0.024795566 0.024207339 0.023501014][0.022270337 0.022948822 0.023312721 0.023680916 0.023907766 0.02391584 0.023801614 0.023630248 0.023444889 0.023162274 0.022867285 0.022578236 0.022275556 0.021925174 0.021508897]]...]
INFO - root - 2017-12-09 08:00:10.647899: step 5610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:48m:30s remains)
INFO - root - 2017-12-09 08:00:19.170201: step 5620, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 65h:47m:54s remains)
INFO - root - 2017-12-09 08:00:27.828159: step 5630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:18m:40s remains)
INFO - root - 2017-12-09 08:00:36.284070: step 5640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:35m:14s remains)
INFO - root - 2017-12-09 08:00:44.817303: step 5650, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 72h:57m:54s remains)
INFO - root - 2017-12-09 08:00:53.477560: step 5660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:24m:11s remains)
INFO - root - 2017-12-09 08:01:02.286474: step 5670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:52m:34s remains)
INFO - root - 2017-12-09 08:01:10.776433: step 5680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:58m:25s remains)
INFO - root - 2017-12-09 08:01:19.281867: step 5690, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 74h:42m:09s remains)
INFO - root - 2017-12-09 08:01:27.983487: step 5700, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 82h:50m:40s remains)
2017-12-09 08:01:28.966850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7333381e-05 -7.7156088e-05 -7.6701515e-05 -7.5559539e-05 -7.4121032e-05 -7.437983e-05 -7.0716167e-05 -6.2829582e-05 -5.5007637e-05 -4.9116308e-05 -4.2822176e-05 -3.9155719e-05 -4.2296422e-05 -4.7971975e-05 -5.1714669e-05][-6.3277643e-05 -6.2091065e-05 -6.0386934e-05 -5.6107594e-05 -3.824028e-05 -6.7598303e-06 6.2176885e-05 0.0001325122 0.000186801 0.00021551961 0.00022743653 0.00022480213 0.00018799769 0.00013915643 0.00010024077][-4.8679736e-05 -3.9370265e-05 -2.0121821e-05 1.5968821e-05 0.00013469991 0.00034230814 0.00063256128 0.00087854714 0.0010168249 0.0010665146 0.0010508376 0.000971539 0.00082262343 0.00066658482 0.000566856][-4.5005792e-05 -3.3476077e-05 -7.2926559e-06 8.6459913e-05 0.00039475627 0.00093918847 0.0016105974 0.002126863 0.0023716204 0.0024538978 0.0024054716 0.0022140131 0.0018949643 0.0015883402 0.001442467][-5.5047298e-05 -4.23437e-05 -1.1949502e-05 0.00013865541 0.00063104345 0.0015199899 0.0025443046 0.0032928817 0.0036124459 0.003693349 0.0036045841 0.0033212998 0.0028827263 0.0025138992 0.0024374635][-6.9228612e-05 -5.7112426e-05 -2.6830996e-05 0.00014027378 0.00070710515 0.0017186458 0.0028412787 0.0036341003 0.0039362023 0.0039976882 0.0039033119 0.0036134755 0.003190944 0.002924779 0.003087702][-7.870212e-05 -6.9758382e-05 -4.6957874e-05 7.8348021e-05 0.00053304539 0.001369515 0.0022909057 0.0029152122 0.0031230079 0.0031409368 0.0030623572 0.0028559337 0.0025918894 0.0025780809 0.0030743019][-8.6008513e-05 -8.0642189e-05 -6.5843225e-05 3.8217404e-06 0.00027142983 0.00078124832 0.0013476645 0.0017155573 0.0018111204 0.001794439 0.00174248 0.0016459544 0.0016043198 0.0018735683 0.0026576035][-8.5549145e-05 -8.3951862e-05 -7.7042489e-05 -4.5858411e-05 7.5548567e-05 0.0003119944 0.00057614059 0.00073995744 0.00076626544 0.00073775428 0.00070669921 0.00068560452 0.00082429266 0.0013240259 0.0023061628][-5.1466668e-05 -6.7161469e-05 -7.705274e-05 -6.9250746e-05 -2.705698e-05 5.7851168e-05 0.00015440174 0.00021229598 0.00021791198 0.00020315497 0.00019304153 0.00021329294 0.00044596696 0.0010590784 0.0021027802][4.5844834e-05 -5.421869e-06 -4.4758315e-05 -5.995074e-05 -4.8757596e-05 -2.1532622e-05 2.6560301e-06 1.767886e-05 2.368362e-05 3.3801742e-05 6.2307794e-05 0.00012254909 0.00038148888 0.0010108568 0.002027411][0.00047234877 0.00029356655 0.00014642089 5.5003271e-05 1.4594872e-05 -3.8799917e-06 -6.9225862e-06 6.9622765e-06 4.7660942e-05 0.00011871512 0.00023771655 0.00036673213 0.00062921364 0.0011773612 0.00204387][0.0010436599 0.00070416555 0.00043096853 0.00026907516 0.00020965062 0.00021001069 0.00024214106 0.00031229877 0.00044966285 0.00067129242 0.00097954285 0.0012931165 0.0016526621 0.0021179607 0.0027615614][0.0023087715 0.0015930765 0.001009786 0.00065079628 0.00052049005 0.0005637444 0.00077651418 0.001092656 0.0015036087 0.0020213719 0.0026103912 0.0031838915 0.0036623932 0.0040169572 0.0043692072][0.0041979114 0.0031656234 0.0022919809 0.0018236398 0.0017746994 0.0020772375 0.00262907 0.0033002121 0.00404387 0.0048204195 0.0055905557 0.0062459689 0.0067035877 0.00694678 0.007020737]]...]
INFO - root - 2017-12-09 08:01:37.623637: step 5710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:18m:00s remains)
INFO - root - 2017-12-09 08:01:46.285776: step 5720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:11m:02s remains)
INFO - root - 2017-12-09 08:01:54.748656: step 5730, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:35m:12s remains)
INFO - root - 2017-12-09 08:02:03.404765: step 5740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:24m:46s remains)
INFO - root - 2017-12-09 08:02:12.160471: step 5750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:32m:13s remains)
INFO - root - 2017-12-09 08:02:20.832281: step 5760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:41m:31s remains)
INFO - root - 2017-12-09 08:02:29.540000: step 5770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:42m:41s remains)
INFO - root - 2017-12-09 08:02:37.993739: step 5780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:11m:54s remains)
INFO - root - 2017-12-09 08:02:46.732799: step 5790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:42m:38s remains)
INFO - root - 2017-12-09 08:02:55.411029: step 5800, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 81h:19m:04s remains)
2017-12-09 08:02:56.282764: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037069116 0.032753438 0.027694715 0.022748902 0.01801393 0.013987332 0.010708338 0.008260374 0.0061465595 0.004370119 0.0029239003 0.0021411721 0.0020097382 0.0023732393 0.0031578571][0.037152924 0.0326574 0.027183685 0.021686168 0.016436834 0.012128667 0.0087604579 0.0064435727 0.0046436759 0.0033133528 0.0023222836 0.00186416 0.001921918 0.002351834 0.00309715][0.037308846 0.032434907 0.026484391 0.020475214 0.014781504 0.010225506 0.0068608541 0.0048059514 0.0033888614 0.0024430444 0.001798926 0.0015810879 0.0017284886 0.0021366235 0.0027916988][0.037238441 0.032405186 0.026213869 0.019949572 0.013979023 0.0093076574 0.0060065789 0.0041723973 0.0030698378 0.0024732631 0.00213321 0.0020689014 0.0022220197 0.0025379451 0.0030467922][0.037027381 0.032521371 0.026518907 0.020543505 0.014768231 0.010284627 0.0071377927 0.0054388144 0.0044572446 0.0039362144 0.0036761991 0.0036938461 0.0038908131 0.0041255024 0.00449406][0.036737464 0.032704271 0.02716847 0.021813354 0.016627261 0.012749211 0.010067604 0.0085720215 0.0075981594 0.0070172306 0.0066919867 0.0066263122 0.00670435 0.0068174568 0.0070636775][0.03628543 0.03272998 0.027755152 0.023036662 0.018487042 0.015269551 0.013091315 0.011899864 0.011043495 0.010468269 0.01010087 0.0099549275 0.009902562 0.0098334178 0.0098678321][0.035163265 0.032244 0.027904861 0.02383215 0.019924918 0.017239384 0.01538963 0.014396736 0.01357799 0.012998267 0.012597748 0.012400021 0.012275898 0.012148067 0.0120898][0.032626867 0.03049765 0.02694024 0.023559632 0.020343611 0.018171731 0.016646797 0.015802922 0.015019603 0.014478094 0.014052209 0.01375376 0.01351482 0.013290733 0.013126751][0.0285083 0.0271124 0.024396043 0.021820817 0.019407187 0.017765636 0.016627021 0.015974279 0.015348251 0.01489854 0.014498828 0.014168303 0.013807313 0.013425204 0.013109849][0.023089677 0.022337528 0.02048659 0.018719824 0.017085036 0.015967472 0.015195535 0.014745667 0.014285708 0.013958134 0.013652728 0.013345194 0.012935257 0.012460938 0.012035922][0.016872272 0.016572574 0.015481986 0.014453884 0.013505244 0.012819157 0.01232687 0.012018724 0.011690054 0.01145072 0.011215727 0.010965292 0.010584208 0.010120883 0.009695176][0.010821127 0.010743443 0.010182884 0.0096704587 0.0091899671 0.0088116061 0.0085078133 0.0082811685 0.0080215754 0.0078285672 0.0076477136 0.0074620442 0.00717617 0.0068288879 0.0065120067][0.0058808285 0.00587481 0.0056247078 0.0054158997 0.005213432 0.0050200322 0.0048453803 0.0046906518 0.0045074215 0.0043615908 0.0042310855 0.0041147792 0.0039415117 0.0037327558 0.0035455753][0.0026530498 0.0026589201 0.0025592286 0.0024950851 0.0024318558 0.002352155 0.0022663192 0.0021804974 0.0020762961 0.0019862168 0.001904313 0.0018403062 0.0017534288 0.0016516758 0.0015652517]]...]
INFO - root - 2017-12-09 08:03:04.971019: step 5810, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:34m:37s remains)
INFO - root - 2017-12-09 08:03:13.626472: step 5820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:00m:38s remains)
INFO - root - 2017-12-09 08:03:22.146300: step 5830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:32m:26s remains)
INFO - root - 2017-12-09 08:03:30.810234: step 5840, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 81h:37m:27s remains)
INFO - root - 2017-12-09 08:03:39.529534: step 5850, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 80h:29m:44s remains)
INFO - root - 2017-12-09 08:03:48.161270: step 5860, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:26m:56s remains)
INFO - root - 2017-12-09 08:03:56.891789: step 5870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 79h:20m:24s remains)
INFO - root - 2017-12-09 08:04:05.280703: step 5880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:39m:16s remains)
INFO - root - 2017-12-09 08:04:14.009853: step 5890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:23m:38s remains)
INFO - root - 2017-12-09 08:04:22.637888: step 5900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:40m:26s remains)
2017-12-09 08:04:23.537881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7304758e-05 -7.5094009e-05 -7.4384094e-05 -7.4142437e-05 -7.4170741e-05 -7.43285e-05 -7.440638e-05 -7.4519317e-05 -7.4566226e-05 -7.4523021e-05 -7.4503645e-05 -7.4981654e-05 -7.5525393e-05 -7.5897e-05 -7.597544e-05][-7.9280158e-05 -7.6322096e-05 -7.4954754e-05 -7.4294825e-05 -7.4195523e-05 -7.4250864e-05 -7.4389638e-05 -7.4491858e-05 -7.438007e-05 -7.4267147e-05 -7.4367454e-05 -7.5001175e-05 -7.56345e-05 -7.61398e-05 -7.6326964e-05][-8.3190127e-05 -7.9865182e-05 -7.8398487e-05 -7.76927e-05 -7.7588338e-05 -7.7658326e-05 -7.7850535e-05 -7.7916186e-05 -7.7640252e-05 -7.7344368e-05 -7.7466e-05 -7.8156037e-05 -7.8827317e-05 -7.9471174e-05 -7.9855585e-05][-8.80243e-05 -8.5086183e-05 -8.3717823e-05 -8.2587707e-05 -8.1825463e-05 -8.1200837e-05 -8.0772719e-05 -8.03967e-05 -8.0150043e-05 -8.0286482e-05 -8.1170365e-05 -8.273375e-05 -8.3968363e-05 -8.4894171e-05 -8.5319625e-05][-9.29479e-05 -9.0658708e-05 -8.9231922e-05 -8.740776e-05 -8.5627078e-05 -8.39438e-05 -8.2701365e-05 -8.1944134e-05 -8.157565e-05 -8.2104045e-05 -8.3859835e-05 -8.6517713e-05 -8.8637389e-05 -9.0297981e-05 -9.10541e-05][-9.6336458e-05 -9.50799e-05 -9.3747818e-05 -9.1310591e-05 -8.8598914e-05 -8.5673237e-05 -8.3006482e-05 -8.1419654e-05 -8.0937709e-05 -8.18548e-05 -8.4555344e-05 -8.8357483e-05 -9.1505921e-05 -9.3854062e-05 -9.5080992e-05][-9.8870682e-05 -9.8679557e-05 -9.7560012e-05 -9.4937772e-05 -9.16678e-05 -8.7867869e-05 -8.4234649e-05 -8.2062194e-05 -8.1387407e-05 -8.2509971e-05 -8.5786865e-05 -9.0387344e-05 -9.4315852e-05 -9.7566233e-05 -9.9577817e-05][-0.0001013475 -0.00010229609 -0.0001014479 -9.8601246e-05 -9.4752664e-05 -9.0053756e-05 -8.5738968e-05 -8.3135383e-05 -8.2365033e-05 -8.3567153e-05 -8.7312e-05 -9.2623857e-05 -9.7253622e-05 -0.00010134428 -0.00010410727][-0.00010229233 -0.00010414294 -0.00010378195 -0.00010093756 -9.6801785e-05 -9.1577487e-05 -8.6924694e-05 -8.3770232e-05 -8.2459686e-05 -8.3611514e-05 -8.7536944e-05 -9.2730217e-05 -9.729345e-05 -0.00010168432 -0.00010492523][-9.9611214e-05 -0.00010209571 -0.00010251826 -0.00010055679 -9.7193217e-05 -9.2811417e-05 -8.8743145e-05 -8.5819418e-05 -8.4239684e-05 -8.4748273e-05 -8.7795372e-05 -9.1830523e-05 -9.5471914e-05 -9.9360652e-05 -0.00010247392][-9.4938121e-05 -9.7874639e-05 -9.9105717e-05 -9.8619712e-05 -9.7090044e-05 -9.447409e-05 -9.1994065e-05 -8.9906287e-05 -8.8258166e-05 -8.785355e-05 -8.9288595e-05 -9.160029e-05 -9.3745228e-05 -9.6607444e-05 -9.9223194e-05][-8.9623369e-05 -9.2832124e-05 -9.5046431e-05 -9.5898875e-05 -9.613263e-05 -9.5463554e-05 -9.4713374e-05 -9.3577095e-05 -9.1791357e-05 -9.0428046e-05 -9.0203495e-05 -9.0625588e-05 -9.1167421e-05 -9.3011542e-05 -9.5004289e-05][-8.3440711e-05 -8.6425978e-05 -8.9237539e-05 -9.1190348e-05 -9.2947565e-05 -9.3842595e-05 -9.4485746e-05 -9.4085473e-05 -9.2135277e-05 -8.9989364e-05 -8.8452376e-05 -8.7262873e-05 -8.6566215e-05 -8.7376713e-05 -8.8728637e-05][-7.7594181e-05 -7.9920494e-05 -8.2862156e-05 -8.5373438e-05 -8.7695254e-05 -8.9574038e-05 -9.1103604e-05 -9.1481255e-05 -8.9873538e-05 -8.7458931e-05 -8.5169726e-05 -8.2994782e-05 -8.1504928e-05 -8.1419887e-05 -8.2037513e-05][-7.278601e-05 -7.4122552e-05 -7.6494245e-05 -7.8847712e-05 -8.1224549e-05 -8.3341409e-05 -8.5124142e-05 -8.5927051e-05 -8.4955849e-05 -8.2874263e-05 -8.05652e-05 -7.8145524e-05 -7.650927e-05 -7.5985728e-05 -7.6149619e-05]]...]
INFO - root - 2017-12-09 08:04:32.128365: step 5910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:24m:06s remains)
INFO - root - 2017-12-09 08:04:40.766827: step 5920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:51m:50s remains)
INFO - root - 2017-12-09 08:04:49.134174: step 5930, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:09m:05s remains)
INFO - root - 2017-12-09 08:04:57.563002: step 5940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:06m:33s remains)
INFO - root - 2017-12-09 08:05:06.152260: step 5950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 75h:41m:40s remains)
INFO - root - 2017-12-09 08:05:14.666931: step 5960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:23m:05s remains)
INFO - root - 2017-12-09 08:05:23.176407: step 5970, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:06m:57s remains)
INFO - root - 2017-12-09 08:05:31.615477: step 5980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:58m:39s remains)
INFO - root - 2017-12-09 08:05:40.134051: step 5990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:46m:08s remains)
INFO - root - 2017-12-09 08:05:48.867412: step 6000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:09m:38s remains)
2017-12-09 08:05:49.723715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4398108e-05 -6.0131293e-05 -7.1671529e-05 -6.7365516e-05 -4.426542e-05 1.0317031e-05 8.3247593e-05 0.00014678168 0.00016979463 0.00013320518 6.4149383e-05 -1.7708982e-05 -6.5092179e-05 -8.142236e-05 -8.1557082e-05][0.00019200952 4.2476895e-05 -4.1150226e-05 -6.7188143e-05 -5.5302087e-05 -2.0436739e-05 2.5014189e-05 6.731227e-05 8.3815277e-05 6.3862768e-05 2.0750886e-05 -3.1018135e-05 -6.3733234e-05 -8.08214e-05 -8.1011443e-05][0.00065472361 0.00035327137 0.00012517293 -7.55387e-06 -5.5638295e-05 -5.1542549e-05 -3.0163035e-05 -4.6097266e-06 1.3703757e-05 1.6780948e-05 4.0477607e-06 -1.9357831e-05 -4.3280015e-05 -7.2665229e-05 -7.9551668e-05][0.0013106545 0.00094942289 0.00059254433 0.00029130507 8.5576321e-05 -1.3235331e-05 -2.6990005e-05 4.6201021e-06 3.9637715e-05 5.7436046e-05 4.795118e-05 7.8611047e-06 -3.3948716e-05 -7.1673094e-05 -7.8549383e-05][0.001759269 0.0014476578 0.00111767 0.000811021 0.00054789282 0.00035209465 0.0002287431 0.00018256673 0.00019300228 0.00021495184 0.00020008086 0.00011771149 3.041912e-05 -5.1263036e-05 -7.705373e-05][0.0022912507 0.0019464348 0.0015589676 0.0011899814 0.00089017389 0.00070240704 0.00061891193 0.00061278703 0.00060557656 0.00054198771 0.00040529942 0.00021837873 6.2833889e-05 -4.5044122e-05 -7.5635253e-05][0.0032824629 0.0029636454 0.0025603189 0.0021829484 0.0018976906 0.0017610912 0.0017528411 0.0017837244 0.0017225216 0.0014755535 0.0010544418 0.00057483307 0.00019876141 -6.7583314e-06 -7.0043447e-05][0.0048678112 0.0045525869 0.0041385428 0.0038241355 0.0037047018 0.0038220764 0.0041010776 0.0042940439 0.0041222735 0.0034630224 0.0024465031 0.0013694866 0.00054620556 0.00010972857 -4.6367524e-05][0.008852493 0.0083627691 0.0076772869 0.00716711 0.0069340742 0.00705651 0.0074264454 0.0076587037 0.0073212935 0.006204416 0.0044947108 0.0026543641 0.0011854093 0.00033905113 3.3393007e-06][0.01494153 0.014398982 0.01345041 0.012610853 0.012022753 0.011804194 0.01181192 0.011618922 0.010750839 0.0089781955 0.006532697 0.0039733378 0.0019036704 0.00063377072 7.9912905e-05][0.021855164 0.021198241 0.019984704 0.018812742 0.017784182 0.017031325 0.016396826 0.015494648 0.013897823 0.011416049 0.0083001079 0.00513309 0.0025510839 0.00090999942 0.0001566651][0.027399087 0.026565997 0.025019435 0.02342033 0.021891445 0.020579398 0.019292826 0.017698262 0.015447299 0.012430107 0.00892277 0.0054884539 0.0027269153 0.00097527244 0.00016996646][0.030173343 0.029081797 0.027165828 0.025160229 0.023162847 0.021342127 0.019506566 0.017389257 0.014713342 0.0114817 0.00800853 0.0047934218 0.0023130826 0.00079476909 0.00011778236][0.029620158 0.028362682 0.02621224 0.023918731 0.021596339 0.01942989 0.017216783 0.014779504 0.011955283 0.0088835508 0.0058846078 0.0033370571 0.0015129617 0.00046783854 3.1975505e-05][0.026417099 0.025140451 0.022966443 0.020565338 0.018093199 0.015735349 0.013342052 0.010845697 0.0082195867 0.0056936382 0.0034993729 0.0018317837 0.0007499035 0.00018177391 -3.4842727e-05]]...]
INFO - root - 2017-12-09 08:05:58.343539: step 6010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 80h:10m:33s remains)
INFO - root - 2017-12-09 08:06:06.950256: step 6020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 74h:54m:51s remains)
INFO - root - 2017-12-09 08:06:15.519373: step 6030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:02m:51s remains)
INFO - root - 2017-12-09 08:06:24.299004: step 6040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:18m:46s remains)
INFO - root - 2017-12-09 08:06:32.889102: step 6050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:43m:41s remains)
INFO - root - 2017-12-09 08:06:41.781964: step 6060, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:19m:21s remains)
INFO - root - 2017-12-09 08:06:50.540650: step 6070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:21m:32s remains)
INFO - root - 2017-12-09 08:06:59.047961: step 6080, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:16m:35s remains)
INFO - root - 2017-12-09 08:07:07.551978: step 6090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:30m:10s remains)
INFO - root - 2017-12-09 08:07:16.181745: step 6100, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:40m:03s remains)
2017-12-09 08:07:17.049273: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0016696383 0.0015989543 0.0015328403 0.0014038887 0.0012298818 0.0010651795 0.00094169588 0.00094196666 0.00098321715 0.0011980783 0.0013676124 0.001495569 0.0014440009 0.0012949113 0.001122745][0.0019269545 0.0018909384 0.0018401208 0.0017703554 0.0016857146 0.0016074457 0.0015492359 0.0015830024 0.001636591 0.0017824378 0.0018860273 0.0019896789 0.0019250918 0.0018008117 0.0016513569][0.0020828873 0.0022181885 0.0023321162 0.0023547297 0.002320502 0.0022684853 0.0022283916 0.0022531913 0.0022662752 0.0023683645 0.0024207132 0.0024667082 0.002348331 0.0021645685 0.0019530338][0.0019517509 0.0023353759 0.0027657524 0.0031747068 0.0035295098 0.0038059743 0.0040070619 0.0041105822 0.0041298503 0.0041355705 0.0041115507 0.0040635117 0.003832753 0.0035703024 0.0033107821][0.0022623616 0.0030545779 0.0040711458 0.0051748604 0.0062169847 0.0071305772 0.0077838511 0.0081235589 0.0082115578 0.0081405258 0.0080017075 0.0077379043 0.0073323753 0.0069350796 0.0066012251][0.0034294066 0.0049672029 0.0070014037 0.0092585273 0.01141887 0.013233655 0.014450841 0.015003665 0.014970605 0.014649487 0.014241563 0.013719131 0.01309517 0.012494381 0.012057884][0.0052810498 0.0077794404 0.011007212 0.014529894 0.017875183 0.020619474 0.022434479 0.023183728 0.022982361 0.022290263 0.021459073 0.020602329 0.0197349 0.018949544 0.018392973][0.00720485 0.010535521 0.014698384 0.019132584 0.023228832 0.026504077 0.028658839 0.029559232 0.029281544 0.028361505 0.027230235 0.026188077 0.025242386 0.024415942 0.023779254][0.008361225 0.012203584 0.016835932 0.021632034 0.025933105 0.029273927 0.031449378 0.032406971 0.032178964 0.031248216 0.03007645 0.029029697 0.028150307 0.027375204 0.026749352][0.0083738063 0.012302791 0.016912796 0.021569172 0.025605217 0.028607216 0.030505707 0.031361349 0.031184196 0.030377304 0.029348105 0.028443573 0.027727764 0.027081234 0.026513943][0.0071924534 0.010835261 0.015080647 0.019323939 0.022927407 0.025499038 0.027051616 0.027708685 0.027512033 0.026770851 0.025838144 0.025014855 0.024346126 0.023738652 0.023190739][0.0052164881 0.0081717726 0.011691466 0.015287733 0.018356729 0.020506876 0.021733571 0.02215622 0.021867346 0.021133538 0.020239443 0.019412525 0.018704372 0.018070275 0.017521534][0.0031611768 0.0051848958 0.0076899673 0.010365794 0.012725485 0.014420805 0.015354579 0.015597272 0.015264059 0.01458618 0.013783674 0.013030971 0.012368338 0.011789559 0.011311119][0.0015291811 0.0026639036 0.0041458621 0.0058116568 0.0073477454 0.0085049961 0.0091422759 0.0092758983 0.008994271 0.0084716287 0.0078622652 0.0072839842 0.0067648017 0.0063239438 0.005973794][0.00055062631 0.0010510837 0.0017446758 0.0025788981 0.0033967302 0.0040545184 0.0044295872 0.0045089363 0.0043443274 0.0040404405 0.0036849591 0.0033401253 0.0030260088 0.0027613386 0.002557266]]...]
INFO - root - 2017-12-09 08:07:25.776325: step 6110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:10m:10s remains)
INFO - root - 2017-12-09 08:07:34.545732: step 6120, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 83h:07m:25s remains)
INFO - root - 2017-12-09 08:07:42.969341: step 6130, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:21m:34s remains)
INFO - root - 2017-12-09 08:07:51.723494: step 6140, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:41m:19s remains)
INFO - root - 2017-12-09 08:08:00.414072: step 6150, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:16m:37s remains)
INFO - root - 2017-12-09 08:08:08.957003: step 6160, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 75h:15m:42s remains)
INFO - root - 2017-12-09 08:08:17.312381: step 6170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:25m:17s remains)
INFO - root - 2017-12-09 08:08:25.808927: step 6180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:47m:14s remains)
INFO - root - 2017-12-09 08:08:34.404724: step 6190, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:39m:04s remains)
INFO - root - 2017-12-09 08:08:43.015457: step 6200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:06m:46s remains)
2017-12-09 08:08:43.947739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00012437243 -0.00012408885 -0.0001245719 -0.00012521063 -0.00012525194 -0.00012512848 -0.00012469303 -0.00012418762 -0.000123493 -0.00012264644 -0.00012163197 -0.00012054852 -0.00011898455 -0.00011725548 -0.00011574789][-0.00012468646 -0.00012385161 -0.00012418086 -0.00012447941 -0.00012413503 -0.000123543 -0.00012273011 -0.00012218482 -0.00012158098 -0.00012105239 -0.00012029916 -0.00011931654 -0.00011788821 -0.00011632018 -0.00011484898][-0.00012499931 -0.00012389198 -0.00012393939 -0.00012367479 -0.00012275344 -0.00012193626 -0.0001209826 -0.00012052644 -0.00012028801 -0.00012003718 -0.00011950383 -0.00011865253 -0.00011741249 -0.00011617444 -0.00011481385][-0.00012430157 -0.00012289989 -0.00012301277 -0.00012232299 -0.00012109191 -0.00012043415 -0.00011952202 -0.00011920083 -0.00011897759 -0.00011874736 -0.00011826797 -0.00011816125 -0.00011708838 -0.00011584655 -0.00011467458][-0.00012376477 -0.00012171283 -0.00012176062 -0.00012063491 -0.00011932655 -0.00011894405 -0.00011874686 -0.00011847248 -0.00011755029 -0.00011754221 -0.00011692257 -0.00011681395 -0.00011599567 -0.00011491765 -0.00011402695][-0.00012474861 -0.00012181766 -0.00012139176 -0.00012030428 -0.0001186818 -0.00011789671 -0.0001176228 -0.00011688485 -0.00011568612 -0.00011562192 -0.00011562508 -0.00011529498 -0.00011446264 -0.00011394777 -0.0001134261][-0.00012498733 -0.00012153297 -0.00012057029 -0.00011993635 -0.00011884433 -0.00011798974 -0.00011802507 -0.00011661965 -0.00011476687 -0.00011427938 -0.0001147408 -0.00011358279 -0.00011259052 -0.00011266761 -0.00011268262][-0.00012379252 -0.00012076354 -0.00012021418 -0.00012037122 -0.00012013636 -0.00011987005 -0.00012070802 -0.00012008502 -0.00011821375 -0.00011638996 -0.00011612815 -0.00011429383 -0.00011254132 -0.00011230657 -0.00011245857][-0.00012528108 -0.00012287183 -0.00012238446 -0.000122108 -0.00012264586 -0.0001234118 -0.00012457852 -0.00012483544 -0.00012342844 -0.0001211007 -0.00011974805 -0.00011717223 -0.00011405563 -0.00011307887 -0.00011301097][-0.0001271684 -0.00012517591 -0.00012474431 -0.0001244936 -0.00012459587 -0.0001255989 -0.00012726281 -0.00012806087 -0.00012730056 -0.00012523597 -0.00012312048 -0.0001204024 -0.00011635269 -0.00011393693 -0.00011363758][-0.00012880409 -0.000127273 -0.00012677768 -0.00012624644 -0.00012598246 -0.00012659733 -0.00012770464 -0.000128276 -0.00012807848 -0.00012648555 -0.00012430082 -0.00012174201 -0.00011807082 -0.00011534906 -0.00011419301][-0.00012930276 -0.00012798126 -0.0001273026 -0.00012645514 -0.00012633381 -0.00012654324 -0.0001266614 -0.00012649782 -0.00012650149 -0.00012551909 -0.00012359166 -0.00012128754 -0.00011855099 -0.00011621062 -0.00011450215][-0.0001260555 -0.00012481409 -0.00012444906 -0.00012404633 -0.0001237659 -0.00012380868 -0.00012383764 -0.00012372149 -0.00012364844 -0.00012290655 -0.0001212964 -0.00011941245 -0.00011782485 -0.00011625546 -0.00011466261][-0.00012200255 -0.00012094689 -0.00012057977 -0.00012014729 -0.00011988113 -0.00011970673 -0.00011970886 -0.0001196849 -0.00011950154 -0.0001190848 -0.00011820933 -0.00011693961 -0.00011614997 -0.0001152671 -0.00011422024][-0.00011741705 -0.0001167751 -0.0001166272 -0.0001163696 -0.00011623837 -0.00011600685 -0.00011585668 -0.00011563533 -0.00011541371 -0.00011530408 -0.00011502196 -0.00011460601 -0.00011432332 -0.00011401363 -0.00011360223]]...]
INFO - root - 2017-12-09 08:08:52.520377: step 6210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:02m:51s remains)
INFO - root - 2017-12-09 08:09:01.015407: step 6220, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 74h:17m:13s remains)
INFO - root - 2017-12-09 08:09:09.553599: step 6230, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.925 sec/batch; 83h:49m:26s remains)
INFO - root - 2017-12-09 08:09:18.107174: step 6240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:09m:54s remains)
INFO - root - 2017-12-09 08:09:26.784258: step 6250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:34m:06s remains)
INFO - root - 2017-12-09 08:09:35.475160: step 6260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:08m:22s remains)
INFO - root - 2017-12-09 08:09:44.115886: step 6270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:13m:23s remains)
INFO - root - 2017-12-09 08:09:52.538728: step 6280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:57m:53s remains)
INFO - root - 2017-12-09 08:10:00.828251: step 6290, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:03m:34s remains)
INFO - root - 2017-12-09 08:10:09.187913: step 6300, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 74h:21m:53s remains)
2017-12-09 08:10:10.015091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.000127643 -0.00012679349 -0.00012694494 -0.00012686408 -0.00012650441 -0.00012601563 -0.00012529266 -0.00012449009 -0.00012380062 -0.00012354847 -0.00012397503 -0.0001240319 -0.00012410946 -0.00012421948 -0.00012387006][-0.0001243619 -0.00012399377 -0.00012419892 -0.0001240086 -0.00012400406 -0.00012377549 -0.00012330744 -0.00012280185 -0.00012244491 -0.00012253711 -0.00012299162 -0.00012306886 -0.00012350657 -0.00012378609 -0.00012311072][-0.00011862819 -0.00011808946 -0.00011827709 -0.00011826537 -0.00011848399 -0.00011870574 -0.00011873449 -0.00011888963 -0.00011921339 -0.00012005933 -0.00012072305 -0.0001208788 -0.0001213241 -0.0001209304 -0.00011954052][-0.00011406518 -0.00011328144 -0.000113353 -0.00011349338 -0.00011387995 -0.00011445723 -0.0001151718 -0.00011593954 -0.0001169746 -0.0001181145 -0.00011884842 -0.00011919166 -0.00011956409 -0.00011892527 -0.00011745049][-0.00011174745 -0.0001102874 -0.0001098857 -0.00010995174 -0.00011016582 -0.00011086465 -0.00011189499 -0.00011287138 -0.00011439984 -0.00011585231 -0.00011680282 -0.0001172964 -0.00011774183 -0.0001174345 -0.00011613242][-0.00011078511 -0.00010899588 -0.00010860135 -0.00010872097 -0.00010893978 -0.00010943443 -0.00011000522 -0.00011105882 -0.00011287785 -0.00011435619 -0.00011545231 -0.00011627845 -0.00011683053 -0.00011665845 -0.00011553423][-0.00011142877 -0.00010956156 -0.00010923282 -0.00010893554 -0.00010863328 -0.00010846291 -0.00010833311 -0.00010905506 -0.00011112265 -0.00011281649 -0.00011396752 -0.00011481803 -0.00011536365 -0.00011519515 -0.00011413141][-0.00011314932 -0.00011147448 -0.00011085595 -0.00011014491 -0.00010921264 -0.00010834084 -0.00010786152 -0.00010855024 -0.00011035662 -0.00011202304 -0.0001131188 -0.00011384273 -0.00011429704 -0.00011407868 -0.0001128809][-0.00011542461 -0.00011428837 -0.00011371258 -0.00011251349 -0.00011134383 -0.00011016015 -0.00010954952 -0.00010992214 -0.00011102842 -0.00011218528 -0.00011305038 -0.00011366659 -0.00011401781 -0.00011374171 -0.00011257473][-0.00011735287 -0.0001164084 -0.00011608341 -0.00011500774 -0.00011390776 -0.00011282962 -0.00011212897 -0.00011216121 -0.00011246398 -0.00011326364 -0.00011396775 -0.00011454281 -0.0001146578 -0.00011416226 -0.00011295339][-0.000119155 -0.00011815796 -0.00011809981 -0.00011707099 -0.0001161447 -0.00011519055 -0.00011439434 -0.00011389039 -0.00011366857 -0.0001140991 -0.00011450981 -0.000114835 -0.00011475858 -0.00011430345 -0.00011313589][-0.00011798684 -0.00011656081 -0.00011661295 -0.00011601843 -0.00011540837 -0.00011478018 -0.00011388063 -0.00011302462 -0.0001125752 -0.00011277398 -0.00011301866 -0.00011313989 -0.00011299491 -0.00011283987 -0.00011196982][-0.00011476056 -0.00011256319 -0.00011245946 -0.00011204148 -0.00011149767 -0.00011098171 -0.00011014118 -0.00010936111 -0.00010902526 -0.00010914396 -0.00010937926 -0.00010938392 -0.00010925445 -0.00010937579 -0.00010908164][-0.00010740281 -0.00010423321 -0.00010385447 -0.0001034272 -0.00010285882 -0.00010227393 -0.00010141766 -0.00010073075 -0.00010029128 -0.00010037186 -0.00010059038 -0.00010062697 -0.00010075387 -0.0001013502 -0.00010197121][-9.581531e-05 -9.175147e-05 -9.1073387e-05 -9.0496673e-05 -8.9833055e-05 -8.9566063e-05 -8.8951405e-05 -8.8225614e-05 -8.7618573e-05 -8.7562927e-05 -8.7610861e-05 -8.7365072e-05 -8.7490094e-05 -8.8521614e-05 -8.9964458e-05]]...]
INFO - root - 2017-12-09 08:10:18.538735: step 6310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:18m:32s remains)
INFO - root - 2017-12-09 08:10:27.073877: step 6320, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:26m:08s remains)
INFO - root - 2017-12-09 08:10:35.560840: step 6330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:17m:08s remains)
INFO - root - 2017-12-09 08:10:44.217088: step 6340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:59m:30s remains)
INFO - root - 2017-12-09 08:10:52.897306: step 6350, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:45m:15s remains)
INFO - root - 2017-12-09 08:11:01.555526: step 6360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:55m:04s remains)
INFO - root - 2017-12-09 08:11:10.270595: step 6370, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 75h:48m:09s remains)
INFO - root - 2017-12-09 08:11:18.671036: step 6380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:24m:58s remains)
INFO - root - 2017-12-09 08:11:27.042950: step 6390, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:31m:28s remains)
INFO - root - 2017-12-09 08:11:35.626018: step 6400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:58m:12s remains)
2017-12-09 08:11:36.486086: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10024448 0.099510975 0.097821839 0.09607435 0.094095282 0.091902792 0.089240976 0.087286033 0.086221166 0.085884556 0.086311661 0.086432911 0.086143382 0.08448185 0.081319205][0.096507482 0.096405938 0.0955223 0.094768822 0.094042368 0.093285665 0.092053786 0.091251805 0.091194674 0.091810264 0.093071632 0.093762212 0.09399195 0.092804268 0.089953385][0.084453352 0.084856488 0.084858008 0.085391112 0.086336687 0.087705664 0.088701047 0.089886881 0.091465533 0.0933048 0.095456138 0.096765861 0.097536124 0.096719138 0.094208971][0.071617812 0.072269946 0.072962783 0.074791387 0.077862233 0.081799276 0.085620411 0.08941073 0.093058415 0.096299335 0.099228233 0.10101656 0.10198227 0.1010924 0.098384947][0.058785971 0.059600815 0.0611571 0.064329766 0.069444753 0.076209284 0.083352558 0.090126991 0.096050531 0.10083356 0.10448251 0.10634707 0.10689195 0.10535052 0.10193656][0.04750682 0.047944862 0.049800728 0.054274622 0.061477114 0.070863433 0.081083141 0.090704456 0.098844983 0.10496946 0.10899193 0.11059529 0.11014546 0.1074011 0.10291341][0.03772803 0.037837617 0.039884631 0.044933245 0.05334552 0.064655669 0.077179253 0.088892765 0.098792024 0.10599317 0.11022533 0.11130439 0.10961638 0.10555848 0.099758513][0.030086176 0.029549163 0.031275831 0.036445465 0.045230795 0.057137225 0.070531592 0.083358504 0.094130777 0.1018149 0.1060034 0.10646763 0.10363427 0.0983875 0.091508165][0.023619764 0.022584895 0.023858458 0.028497411 0.036733508 0.048205312 0.061151158 0.073638409 0.084083937 0.091560371 0.095348 0.095227212 0.091660954 0.085715815 0.078288063][0.018174946 0.016704563 0.017372176 0.021101711 0.02812328 0.038120069 0.049461845 0.060464542 0.069587484 0.076009393 0.079061672 0.078427158 0.074572183 0.068648614 0.06147847][0.012953951 0.01151172 0.011826654 0.014603931 0.020082671 0.027969979 0.036948189 0.045659453 0.052800965 0.057670627 0.059651114 0.058620602 0.055057921 0.049868476 0.043772854][0.0078677656 0.0068698474 0.0070672589 0.0090526938 0.012973545 0.018541962 0.024894252 0.031025082 0.035989985 0.039208855 0.0402426 0.039091602 0.036181208 0.032144617 0.027559387][0.0036620495 0.0031583894 0.0033922293 0.0046790065 0.0071026767 0.010475529 0.014369332 0.018138463 0.021150196 0.023000339 0.023431843 0.022490829 0.020453624 0.017742831 0.014750416][0.0010076596 0.00086105731 0.0010410073 0.0016844247 0.0028675708 0.00450879 0.0064323386 0.0083236983 0.0098386081 0.010734978 0.010881808 0.010311441 0.0091719422 0.0076950276 0.006102012][2.0101739e-05 -1.9669766e-05 4.6516318e-05 0.00025238236 0.00064812839 0.0012263637 0.001951672 0.0026899111 0.003283625 0.0036280339 0.0036704943 0.0034214694 0.0029442895 0.002347874 0.0017359429]]...]
INFO - root - 2017-12-09 08:11:44.940069: step 6410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:59m:42s remains)
INFO - root - 2017-12-09 08:11:53.498766: step 6420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:07m:27s remains)
INFO - root - 2017-12-09 08:12:01.957048: step 6430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:41m:51s remains)
INFO - root - 2017-12-09 08:12:10.727932: step 6440, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:53m:29s remains)
INFO - root - 2017-12-09 08:12:19.459790: step 6450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:36m:35s remains)
INFO - root - 2017-12-09 08:12:27.969614: step 6460, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:10m:05s remains)
INFO - root - 2017-12-09 08:12:36.437621: step 6470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:03m:14s remains)
INFO - root - 2017-12-09 08:12:45.049374: step 6480, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:14m:14s remains)
INFO - root - 2017-12-09 08:12:53.426984: step 6490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:03m:32s remains)
INFO - root - 2017-12-09 08:13:02.098453: step 6500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 80h:49m:40s remains)
2017-12-09 08:13:02.933001: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0068509835 0.0096851829 0.013998195 0.020884253 0.03208575 0.047321834 0.064531274 0.079497471 0.087054588 0.08488334 0.072868966 0.055020269 0.036430635 0.020653162 0.00982318][0.0069761421 0.010076753 0.016578598 0.028078988 0.046283033 0.069924735 0.095367484 0.116626 0.12725824 0.12410495 0.10711871 0.081769921 0.054770086 0.03137153 0.014763852][0.007612729 0.011634802 0.021991929 0.040446859 0.068342425 0.10230358 0.13659121 0.16313282 0.17451733 0.16799851 0.1442496 0.11034156 0.074311882 0.043159816 0.020692246][0.0087967357 0.014161387 0.029193763 0.055364858 0.093541354 0.13793693 0.1808707 0.21205857 0.22328019 0.21256211 0.18146259 0.13870081 0.093467772 0.054557633 0.026406065][0.011219526 0.018711977 0.038422979 0.072174579 0.1194372 0.17255026 0.22176956 0.25566617 0.26559454 0.25018948 0.21247227 0.16213293 0.1094051 0.064063162 0.031359784][0.014696741 0.024538485 0.049080685 0.089186244 0.14308155 0.20204021 0.254289 0.28841981 0.29534462 0.27532002 0.2321751 0.17622168 0.11862111 0.0692285 0.034035377][0.017620303 0.030362319 0.058583435 0.10333581 0.16130228 0.2227798 0.274981 0.30683714 0.3095859 0.28489998 0.2377656 0.17885339 0.11959466 0.069234535 0.034145094][0.018387949 0.034059204 0.064812809 0.11169242 0.17052728 0.23144236 0.28090236 0.30895022 0.30746335 0.27911317 0.22970773 0.17034598 0.11255747 0.064252891 0.03171479][0.016648173 0.033680126 0.064797528 0.11101507 0.16760403 0.2249939 0.2699548 0.29372615 0.28915402 0.25918576 0.21008347 0.15308103 0.099438913 0.055664156 0.027585581][0.013156873 0.029157355 0.057851918 0.1003254 0.15177622 0.20359142 0.24298072 0.26230422 0.25567916 0.22632293 0.18050556 0.12884195 0.081977464 0.044832218 0.02255825][0.010151731 0.023293264 0.047173426 0.083269626 0.12711482 0.17113993 0.20406845 0.21899441 0.211298 0.18413261 0.1439018 0.10019357 0.062171236 0.033235285 0.017653788][0.008336572 0.018058088 0.035917774 0.063878387 0.098310173 0.13299291 0.15863875 0.16929902 0.16152021 0.1380679 0.10516804 0.070964605 0.042625107 0.022340154 0.013104852][0.0062923115 0.012726954 0.025076959 0.044965729 0.069546819 0.094338305 0.11232945 0.11899714 0.11196134 0.09361463 0.069268957 0.045214739 0.026270183 0.013651049 0.0093225706][0.0038937645 0.0078328438 0.015531134 0.028061435 0.043760266 0.0593632 0.070187576 0.073407769 0.067808248 0.05515511 0.039360866 0.024666235 0.013815715 0.0071994592 0.0059494437][0.0014374484 0.0033945467 0.00752949 0.014202034 0.02259342 0.030694183 0.035958592 0.036891233 0.03307607 0.025838345 0.017377462 0.010138809 0.00527837 0.0027187334 0.0030870333]]...]
INFO - root - 2017-12-09 08:13:11.798017: step 6510, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 81h:50m:13s remains)
INFO - root - 2017-12-09 08:13:20.468175: step 6520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:43m:44s remains)
INFO - root - 2017-12-09 08:13:28.968783: step 6530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:54m:36s remains)
INFO - root - 2017-12-09 08:13:37.734747: step 6540, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:11m:28s remains)
INFO - root - 2017-12-09 08:13:46.262853: step 6550, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 75h:15m:04s remains)
INFO - root - 2017-12-09 08:13:54.826387: step 6560, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 81h:26m:14s remains)
INFO - root - 2017-12-09 08:14:03.562668: step 6570, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 1.001 sec/batch; 90h:38m:01s remains)
INFO - root - 2017-12-09 08:14:12.197338: step 6580, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:50m:13s remains)
INFO - root - 2017-12-09 08:14:20.579024: step 6590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:29m:30s remains)
INFO - root - 2017-12-09 08:14:29.292123: step 6600, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 82h:15m:46s remains)
2017-12-09 08:14:30.142110: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15465452 0.16642047 0.17217503 0.17385267 0.17268977 0.16743095 0.16373718 0.16158387 0.16326781 0.16860944 0.17531914 0.18366554 0.18945435 0.19014527 0.18721129][0.16043007 0.1781133 0.19028014 0.19904946 0.20328081 0.20262806 0.20091999 0.19902322 0.2005861 0.20458199 0.21097746 0.21814503 0.22406797 0.22447811 0.2212905][0.16563784 0.19019245 0.21054567 0.2279771 0.23916782 0.24380369 0.24473414 0.24405733 0.24483137 0.2469973 0.25227183 0.25763926 0.26258507 0.26170278 0.25779104][0.17196596 0.2037309 0.23270854 0.25905475 0.27787411 0.28901377 0.29333332 0.29384831 0.29385141 0.29415414 0.2968668 0.29972878 0.30266142 0.30052972 0.29534873][0.17845844 0.21807633 0.2562775 0.29196262 0.31877857 0.33611768 0.34423861 0.34623495 0.34523118 0.34262305 0.34174991 0.34063369 0.34023812 0.33618504 0.32991952][0.18432471 0.23079619 0.27681369 0.32086432 0.3557058 0.38007054 0.39312011 0.39701295 0.39535254 0.39009544 0.38482371 0.37837595 0.37319183 0.36630458 0.3588067][0.18880174 0.24050051 0.29238611 0.3427684 0.38416919 0.41478598 0.43238211 0.43904898 0.4378289 0.43061364 0.42102578 0.40933749 0.39910245 0.38873148 0.37962064][0.19064267 0.24479377 0.30041921 0.35497355 0.40118247 0.43628627 0.45817313 0.46752232 0.46654573 0.45800894 0.44488424 0.42958829 0.415032 0.40220571 0.39231837][0.18902525 0.24320962 0.29935214 0.35481018 0.40281007 0.4400689 0.46447337 0.47592643 0.47611442 0.46718714 0.45256212 0.43549713 0.41924086 0.40558714 0.39560816][0.18294917 0.23520236 0.2896392 0.34339774 0.39038056 0.42710221 0.45188668 0.46424472 0.46517235 0.45659006 0.44234538 0.42548075 0.40969926 0.3963826 0.38775027][0.17098327 0.21918187 0.26943019 0.3190358 0.36280626 0.39708859 0.42034739 0.43213695 0.43325663 0.42570102 0.41293961 0.39810777 0.38448286 0.37342411 0.36722678][0.15452832 0.19725049 0.24188322 0.28601906 0.32456529 0.35425308 0.3741318 0.38386288 0.38414335 0.37706143 0.36617765 0.35424721 0.34396246 0.33640963 0.3337253][0.13493192 0.17055985 0.20801134 0.245106 0.27759254 0.30222726 0.31836203 0.32538939 0.32430044 0.31738138 0.30813029 0.29895562 0.29207334 0.28853118 0.28971866][0.11443414 0.14214997 0.17114091 0.20030703 0.22591282 0.24516688 0.25755313 0.26240867 0.2609483 0.25458577 0.24692263 0.24011452 0.23598491 0.23562503 0.2395646][0.095548123 0.11569121 0.13654287 0.1578863 0.17659135 0.19069773 0.19967766 0.20272328 0.2011281 0.19582243 0.18974979 0.18472962 0.18223582 0.18365794 0.18910474]]...]
INFO - root - 2017-12-09 08:14:38.658665: step 6610, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:44m:23s remains)
INFO - root - 2017-12-09 08:14:47.347502: step 6620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:58m:45s remains)
INFO - root - 2017-12-09 08:14:55.941252: step 6630, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 75h:13m:53s remains)
INFO - root - 2017-12-09 08:15:04.391520: step 6640, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 75h:42m:03s remains)
INFO - root - 2017-12-09 08:15:12.954150: step 6650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 78h:16m:29s remains)
INFO - root - 2017-12-09 08:15:21.508241: step 6660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:51m:00s remains)
INFO - root - 2017-12-09 08:15:30.221747: step 6670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:21m:34s remains)
INFO - root - 2017-12-09 08:15:38.720535: step 6680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:23m:36s remains)
INFO - root - 2017-12-09 08:15:47.228223: step 6690, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 81h:30m:03s remains)
INFO - root - 2017-12-09 08:15:55.784347: step 6700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:50m:35s remains)
2017-12-09 08:15:56.669558: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019140484 0.024122855 0.028926063 0.033510271 0.037963785 0.04238788 0.046575658 0.04970973 0.05112499 0.050190251 0.046307348 0.0395613 0.030553699 0.020971825 0.012474693][0.02147275 0.026209611 0.030044988 0.033232536 0.036404576 0.039864931 0.04344685 0.046221249 0.047339786 0.046131887 0.042124659 0.035395067 0.026701165 0.01769449 0.010030437][0.020794395 0.024900496 0.027501976 0.029113362 0.030788552 0.033176661 0.036082789 0.038336832 0.038980875 0.037355978 0.033242945 0.02697582 0.019497566 0.012249673 0.0064908126][0.018120773 0.021720374 0.023638731 0.024373436 0.025069105 0.026533611 0.028681923 0.030421693 0.030679533 0.0287725 0.024722459 0.019141376 0.013077934 0.0076891668 0.0037680198][0.015073113 0.01827798 0.019924412 0.02040592 0.020600246 0.021230813 0.022293713 0.023003673 0.022565227 0.020497994 0.016897416 0.012400166 0.0079354877 0.0043139928 0.0019095049][0.012850984 0.015718978 0.017295536 0.017906031 0.01802342 0.017965229 0.017735612 0.017031815 0.015544661 0.013149377 0.010081209 0.006854285 0.00402207 0.0019673626 0.00073252735][0.012338251 0.01506257 0.016655102 0.017490838 0.017698823 0.017221304 0.015893271 0.01374051 0.011025857 0.0080790957 0.0053283614 0.0030942617 0.0015188245 0.00057293393 9.1780443e-05][0.01291457 0.015628882 0.017319137 0.018388301 0.018743781 0.018054605 0.016009485 0.012806263 0.0091223512 0.0056729168 0.0030320873 0.0013462986 0.00044265061 3.3484219e-05 -0.00012178063][0.013464763 0.016210182 0.017919179 0.019003769 0.019281412 0.018345935 0.01588464 0.012225217 0.0081995614 0.0046713166 0.0021966565 0.000795183 0.00014840913 -9.0105052e-05 -0.00015965319][0.012492543 0.015099914 0.016609589 0.017365089 0.017249037 0.015989205 0.013449224 0.010046509 0.0065179565 0.0035758279 0.0016041688 0.00052864489 5.1476236e-05 -0.00011786175 -0.00016616489][0.0098425029 0.012085959 0.013296918 0.013614736 0.012993989 0.011414774 0.0090528112 0.006384009 0.003909206 0.0020198887 0.0008359385 0.00022106958 -4.3016436e-05 -0.000136796 -0.00016696763][0.0065124212 0.0082297316 0.0091413278 0.0091942223 0.0083486028 0.0067790486 0.0048629288 0.0030574067 0.0016356392 0.00070802728 0.00020350839 -2.8146984e-05 -0.00012092471 -0.00015558959 -0.00016954256][0.0034691712 0.0046087164 0.0052324445 0.0052087111 0.0045035523 0.0033455228 0.0020938108 0.0010761982 0.00040683351 5.4235716e-05 -9.292936e-05 -0.0001419959 -0.00015792561 -0.0001654059 -0.0001707499][0.0013374334 0.0019101023 0.0022406762 0.0022288936 0.0018498858 0.0012631702 0.00067388534 0.0002389123 -1.2692355e-05 -0.00012478352 -0.00016128988 -0.00016997868 -0.00017321703 -0.00017543627 -0.00017667384][0.00022512019 0.0004108123 0.0005126663 0.00050226343 0.00038721025 0.00021291384 4.6512519e-05 -7.2162744e-05 -0.00013855245 -0.00016742083 -0.00017687047 -0.00017917829 -0.00017954891 -0.00017941846 -0.00017894144]]...]
INFO - root - 2017-12-09 08:16:05.304485: step 6710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:58m:41s remains)
INFO - root - 2017-12-09 08:16:13.919166: step 6720, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:29m:49s remains)
INFO - root - 2017-12-09 08:16:22.264245: step 6730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 75h:11m:46s remains)
INFO - root - 2017-12-09 08:16:30.936768: step 6740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:01m:19s remains)
INFO - root - 2017-12-09 08:16:39.566210: step 6750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:26m:31s remains)
INFO - root - 2017-12-09 08:16:48.187538: step 6760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:36m:22s remains)
INFO - root - 2017-12-09 08:16:56.819182: step 6770, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 79h:22m:41s remains)
INFO - root - 2017-12-09 08:17:05.554513: step 6780, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 79h:57m:57s remains)
INFO - root - 2017-12-09 08:17:14.267455: step 6790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:24m:56s remains)
INFO - root - 2017-12-09 08:17:22.780779: step 6800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:55m:42s remains)
2017-12-09 08:17:23.753444: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049934853 0.065571465 0.080740109 0.093316421 0.10079399 0.10199423 0.096637644 0.087036 0.075604826 0.064990312 0.056877729 0.052853972 0.053576019 0.058106393 0.064421117][0.06228292 0.082947828 0.10354955 0.12139247 0.13304195 0.13654299 0.1314707 0.12035686 0.1062355 0.092479676 0.081163958 0.074574143 0.073349357 0.076974332 0.08318612][0.07260748 0.098502211 0.12502229 0.14861661 0.16484283 0.17131145 0.16741881 0.15572432 0.13976328 0.12286298 0.10786486 0.097535737 0.092951752 0.094117649 0.0986368][0.078537613 0.10885698 0.14100817 0.17051797 0.1920584 0.20242363 0.20087627 0.18989149 0.17312315 0.15372624 0.13502614 0.12018512 0.11086946 0.10762744 0.1086955][0.0797489 0.11324508 0.14952129 0.18409789 0.21066646 0.22531554 0.22708236 0.21793358 0.20160821 0.18078001 0.1589383 0.13948601 0.12467939 0.11592531 0.11196812][0.077347949 0.11270169 0.15157694 0.18974698 0.22027583 0.23899418 0.24430141 0.23766914 0.22270858 0.20137078 0.17730816 0.15382574 0.13369799 0.11885058 0.10879041][0.072343349 0.10816653 0.14807707 0.18828088 0.2216306 0.24350399 0.2520009 0.24823233 0.23512021 0.21409328 0.18879092 0.16228703 0.13770628 0.11708267 0.10099989][0.066327117 0.10103365 0.14042644 0.18094896 0.21546954 0.23912081 0.24978127 0.24842922 0.23721667 0.21736962 0.19204909 0.16396737 0.13634533 0.1111417 0.09014079][0.060247432 0.092392281 0.12934738 0.16819854 0.20182845 0.22562174 0.23715644 0.2374014 0.22779599 0.20952266 0.18526091 0.15725549 0.12833607 0.10056099 0.0765226][0.053528007 0.082282752 0.1153342 0.15017809 0.18043257 0.20234054 0.21327388 0.21431686 0.20619741 0.18997082 0.16778725 0.14134867 0.11319937 0.085446574 0.061012521][0.045629736 0.070248373 0.098358825 0.12768266 0.15288442 0.17122044 0.18032935 0.18133967 0.17447104 0.16056196 0.14124279 0.11772844 0.092334814 0.067118406 0.045050927][0.036626142 0.056341622 0.078858107 0.10218221 0.12206823 0.136463 0.14344414 0.14411207 0.13835697 0.1268978 0.11085683 0.091178037 0.069961093 0.04907579 0.031207265][0.026874114 0.041701987 0.058696266 0.076270841 0.091321781 0.10231747 0.107634 0.10808542 0.10353709 0.094500341 0.081848115 0.066373192 0.049869686 0.033954963 0.020721653][0.017404871 0.027677968 0.039557058 0.051991507 0.062763564 0.0708599 0.075001538 0.075558655 0.072351508 0.065785937 0.056507628 0.0451787 0.033281356 0.022097733 0.013066082][0.009439623 0.015818316 0.023401573 0.031387158 0.038392749 0.043809358 0.046713941 0.047317881 0.045404684 0.041231878 0.035159361 0.027764259 0.020104922 0.013037457 0.0074740043]]...]
INFO - root - 2017-12-09 08:17:32.387130: step 6810, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:19m:13s remains)
INFO - root - 2017-12-09 08:17:40.966595: step 6820, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.820 sec/batch; 74h:11m:03s remains)
INFO - root - 2017-12-09 08:17:49.506799: step 6830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:03m:38s remains)
INFO - root - 2017-12-09 08:17:58.107816: step 6840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:59m:29s remains)
INFO - root - 2017-12-09 08:18:06.575648: step 6850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:14m:24s remains)
INFO - root - 2017-12-09 08:18:15.127959: step 6860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:03m:18s remains)
INFO - root - 2017-12-09 08:18:23.953245: step 6870, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.984 sec/batch; 89h:00m:09s remains)
INFO - root - 2017-12-09 08:18:32.583717: step 6880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:11m:28s remains)
INFO - root - 2017-12-09 08:18:41.113307: step 6890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:32m:02s remains)
INFO - root - 2017-12-09 08:18:49.693039: step 6900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:40m:37s remains)
2017-12-09 08:18:50.499477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00026528956 -0.00026459395 -0.00026455757 -0.00026505752 -0.00026576209 -0.00026611349 -0.0002663823 -0.000266692 -0.0002672633 -0.00026816208 -0.00026905723 -0.0002697648 -0.00027035619 -0.00027063172 -0.00027095462][-0.0002663692 -0.00026552114 -0.00026544248 -0.00026592889 -0.0002666364 -0.00026698847 -0.00026725602 -0.00026798365 -0.00026901145 -0.00026997272 -0.00027046763 -0.00027064388 -0.00027073742 -0.00027081068 -0.00027108993][-0.00026814573 -0.00026751941 -0.00026737541 -0.00026779078 -0.0002687129 -0.00026916352 -0.00026945418 -0.00027036978 -0.00027157052 -0.0002723317 -0.00027244914 -0.00027211272 -0.00027151778 -0.00027113222 -0.00027120739][-0.00027044868 -0.00026987158 -0.00026965202 -0.00027007423 -0.00027091705 -0.00027117325 -0.00027137127 -0.00027215038 -0.00027328628 -0.00027378779 -0.00027372953 -0.00027322816 -0.00027215329 -0.00027138667 -0.00027119243][-0.00027242815 -0.00027199791 -0.0002720537 -0.00027254468 -0.00027303127 -0.00027317923 -0.00027331922 -0.0002734219 -0.00027415372 -0.00027491886 -0.00027476554 -0.00027426364 -0.00027295618 -0.00027164942 -0.00027104688][-0.000274052 -0.00027424269 -0.00027424833 -0.00027469432 -0.00027514732 -0.00027536869 -0.00027527957 -0.00027423858 -0.00027422773 -0.0002753303 -0.00027573298 -0.00027518114 -0.00027384126 -0.0002722785 -0.00027118588][-0.00027550265 -0.00027576124 -0.00027587733 -0.00027644195 -0.00027693884 -0.00027708567 -0.00027635958 -0.00027437089 -0.00027415247 -0.00027572108 -0.00027650309 -0.00027619052 -0.00027480183 -0.0002731205 -0.00027133818][-0.00027670973 -0.0002768627 -0.00027678188 -0.00027718078 -0.00027803439 -0.0002781713 -0.0002765659 -0.00027388622 -0.00027415709 -0.0002763579 -0.00027755264 -0.00027729888 -0.00027614477 -0.00027438218 -0.00027188816][-0.00027764184 -0.0002775001 -0.00027721684 -0.00027794129 -0.00027900984 -0.00027911147 -0.00027725103 -0.0002752833 -0.00027606127 -0.00027760817 -0.00027848454 -0.0002785298 -0.00027729041 -0.00027540923 -0.00027258208][-0.00027878009 -0.00027841225 -0.0002781584 -0.00027880102 -0.00027983604 -0.00027981453 -0.00027862814 -0.00027781705 -0.00027844895 -0.00027931563 -0.00027965996 -0.00027964209 -0.00027843422 -0.00027650519 -0.00027354984][-0.00027899269 -0.00027865556 -0.00027843658 -0.00027875544 -0.00027978394 -0.0002804515 -0.0002800021 -0.00027961488 -0.00027963339 -0.00027995734 -0.00028030871 -0.00028008778 -0.00027909377 -0.00027730805 -0.00027454796][-0.00027854822 -0.0002778653 -0.00027739748 -0.00027747391 -0.00027843128 -0.00027985993 -0.0002803634 -0.0002799691 -0.00027923679 -0.00027970356 -0.000280575 -0.00028037585 -0.00027918461 -0.00027716169 -0.00027466496][-0.00027806964 -0.00027694745 -0.00027634136 -0.00027642932 -0.00027734612 -0.00027884982 -0.00027983985 -0.00027965195 -0.00027886074 -0.00027920768 -0.00028009139 -0.00028018377 -0.00027909668 -0.00027707621 -0.0002746933][-0.00027765398 -0.00027640336 -0.00027595885 -0.00027607693 -0.00027701718 -0.00027834019 -0.00027915504 -0.00027919165 -0.00027832962 -0.00027824022 -0.00027908542 -0.00027933824 -0.00027854697 -0.00027671363 -0.00027445692][-0.00027664474 -0.00027560451 -0.00027537218 -0.0002756113 -0.00027647213 -0.0002775465 -0.00027842168 -0.00027831379 -0.00027714678 -0.00027639818 -0.00027679608 -0.00027723837 -0.00027690461 -0.00027535012 -0.00027360811]]...]
INFO - root - 2017-12-09 08:18:59.114529: step 6910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:48m:08s remains)
INFO - root - 2017-12-09 08:19:07.932291: step 6920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:30m:34s remains)
INFO - root - 2017-12-09 08:19:16.639753: step 6930, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 63h:29m:44s remains)
INFO - root - 2017-12-09 08:19:25.228092: step 6940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:48m:16s remains)
INFO - root - 2017-12-09 08:19:33.901337: step 6950, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 73h:24m:39s remains)
INFO - root - 2017-12-09 08:19:42.438172: step 6960, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:31m:22s remains)
INFO - root - 2017-12-09 08:19:51.150567: step 6970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:47m:22s remains)
INFO - root - 2017-12-09 08:19:59.978795: step 6980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:56m:13s remains)
INFO - root - 2017-12-09 08:20:08.569787: step 6990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 80h:37m:56s remains)
INFO - root - 2017-12-09 08:20:17.209214: step 7000, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 78h:29m:02s remains)
2017-12-09 08:20:18.049736: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0004183269 0.00048250813 0.00048172552 0.00044151026 0.00043164866 0.00044016188 0.0004632241 0.000497908 0.00052800612 0.0005328923 0.00047087009 0.00033224814 0.00012872121 -7.1506161e-05 -0.00020943023][0.0011583263 0.001309854 0.0013355918 0.0013035482 0.0013341993 0.0014044744 0.0015059571 0.0016187725 0.0016967366 0.0016948434 0.0015382075 0.001207595 0.00073169661 0.00025493352 -8.5322506e-05][0.0026208693 0.0028981497 0.0029358331 0.0029003017 0.0029749891 0.0031449273 0.0033742439 0.003590564 0.0036958938 0.0036206215 0.003261358 0.0025969322 0.001682573 0.00077477493 0.00011674248][0.0052642706 0.0057037654 0.0056653745 0.00553189 0.005548019 0.0057045864 0.0059267455 0.0060985251 0.0060804645 0.0057981112 0.0051401369 0.0040933187 0.0027182533 0.0013533052 0.0003461515][0.00932927 0.00985584 0.00955327 0.0090706572 0.0087404344 0.0086130006 0.0085448073 0.0083722379 0.0079648681 0.0072971685 0.0062882523 0.0049361866 0.0032737772 0.0016589726 0.0004673597][0.013842531 0.014297185 0.013549621 0.012519876 0.011591266 0.010850237 0.010162151 0.0093796793 0.0084073637 0.0073029562 0.0060413117 0.0046215267 0.0030086215 0.0014919613 0.00039113159][0.016613819 0.016848776 0.015647544 0.014040384 0.012473478 0.01110626 0.0098183611 0.0084900875 0.0070977556 0.0057547875 0.0044862288 0.0032794694 0.0020381629 0.00093071093 0.00015723577][0.016145077 0.016076183 0.014582723 0.012614649 0.010667248 0.0089569874 0.0074273194 0.0059817391 0.0046015205 0.003406899 0.0024318406 0.0016370416 0.00090947188 0.00030785598 -8.6508124e-05][0.012831186 0.012512272 0.010967704 0.008966621 0.0070268661 0.0054160673 0.0041085961 0.0030063533 0.0020620103 0.0013228692 0.0007947277 0.00042518953 0.00012737326 -9.6250864e-05 -0.00022895608][0.008091446 0.0077096033 0.0064414027 0.0048425035 0.0033600992 0.0022295716 0.0014363081 0.00086951163 0.00044025722 0.00014295749 -3.6020618e-05 -0.00014032195 -0.00020964429 -0.00025247369 -0.00027443466][0.0037919118 0.003523825 0.0027344287 0.0017660172 0.00092740031 0.00036455612 5.0471019e-05 -0.00012101224 -0.00021596131 -0.0002611911 -0.00027480724 -0.00027674052 -0.000277238 -0.00027825509 -0.00028010653][0.0010468768 0.0009217693 0.000601546 0.00022730188 -6.2925275e-05 -0.00021772314 -0.00027018925 -0.00028038892 -0.00028066937 -0.00028026896 -0.00027961709 -0.00027919869 -0.0002790993 -0.00028005175 -0.00028177831][-3.0252908e-05 -5.6917197e-05 -0.00012606573 -0.00020525671 -0.00025867621 -0.00028032556 -0.00028412498 -0.00028399736 -0.00028305277 -0.00028267485 -0.00028241827 -0.00028199342 -0.00028151058 -0.00028247776 -0.00028361249][-0.00028454856 -0.00028374032 -0.0002835092 -0.00028424361 -0.00028485889 -0.00028518328 -0.00028516038 -0.00028478433 -0.00028410787 -0.00028381741 -0.00028362052 -0.00028333958 -0.00028292462 -0.00028341898 -0.00028426736][-0.00028537784 -0.00028469574 -0.00028451459 -0.00028465033 -0.00028476195 -0.00028494679 -0.00028489239 -0.00028450546 -0.00028406802 -0.00028375289 -0.00028369419 -0.00028362087 -0.00028342503 -0.00028366188 -0.00028411936]]...]
INFO - root - 2017-12-09 08:20:26.984967: step 7010, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:21m:45s remains)
INFO - root - 2017-12-09 08:20:35.579997: step 7020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:54m:27s remains)
INFO - root - 2017-12-09 08:20:44.445697: step 7030, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 81h:39m:03s remains)
INFO - root - 2017-12-09 08:20:53.058200: step 7040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:51m:55s remains)
INFO - root - 2017-12-09 08:21:01.712970: step 7050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:25m:34s remains)
INFO - root - 2017-12-09 08:21:10.286106: step 7060, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 74h:54m:04s remains)
INFO - root - 2017-12-09 08:21:18.780089: step 7070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:22m:19s remains)
INFO - root - 2017-12-09 08:21:27.538497: step 7080, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 78h:22m:13s remains)
INFO - root - 2017-12-09 08:21:36.160154: step 7090, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 80h:37m:46s remains)
INFO - root - 2017-12-09 08:21:44.607333: step 7100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:57m:02s remains)
2017-12-09 08:21:45.433744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00032286893 -0.00032012523 -0.00031655331 -0.00031099893 -0.00030558347 -0.00030218318 -0.0003013798 -0.00030204089 -0.00030342114 -0.0003058463 -0.00030830782 -0.00030952686 -0.00030909438 -0.00030841579 -0.00030789769][-0.00031982432 -0.00031813252 -0.00031562807 -0.00031070449 -0.00030494368 -0.00030105378 -0.00030011404 -0.00030125756 -0.00030363724 -0.0003072512 -0.00031063129 -0.00031234219 -0.00031231969 -0.00031146398 -0.00031020088][-0.00031565409 -0.00031543331 -0.00031467684 -0.00031093229 -0.00030570524 -0.00030142273 -0.00029987807 -0.00030072837 -0.00030367906 -0.00030830654 -0.00031249932 -0.00031465915 -0.00031474157 -0.00031346775 -0.00031093802][-0.00031223611 -0.00031316056 -0.00031412928 -0.00031157472 -0.00030697347 -0.00030237489 -0.00029986294 -0.0002997113 -0.00030251883 -0.00030777269 -0.00031263492 -0.00031533433 -0.00031564178 -0.00031399025 -0.00031026802][-0.00031076552 -0.00031250919 -0.00031459826 -0.00031313868 -0.00030930986 -0.00030447525 -0.00030084956 -0.00029945804 -0.00030186912 -0.00030715374 -0.00031218963 -0.00031538235 -0.00031624414 -0.00031464253 -0.00030973781][-0.00031184766 -0.00031362678 -0.00031598867 -0.00031518019 -0.00031196902 -0.00030707277 -0.00030236982 -0.00029985831 -0.00030144327 -0.00030644739 -0.00031185127 -0.00031534361 -0.00031677843 -0.00031527577 -0.00030980349][-0.00031435309 -0.00031600104 -0.00031827664 -0.00031769372 -0.00031468624 -0.00030998458 -0.00030451114 -0.00030084865 -0.00030116391 -0.00030549007 -0.00031066386 -0.00031435172 -0.00031649234 -0.00031564635 -0.00031037879][-0.00031749203 -0.00031896433 -0.00032114974 -0.00032081397 -0.00031801054 -0.00031349284 -0.00030771529 -0.00030265588 -0.00030090174 -0.00030377816 -0.00030817869 -0.00031193352 -0.00031478264 -0.00031493424 -0.00031112155][-0.00032043847 -0.00032200126 -0.00032375677 -0.00032386195 -0.00032170778 -0.00031767722 -0.00031207007 -0.00030644814 -0.00030298383 -0.00030347906 -0.00030640184 -0.000309809 -0.00031302881 -0.000314343 -0.00031249141][-0.00032211444 -0.000323715 -0.00032538109 -0.0003258565 -0.00032460119 -0.00032153708 -0.00031658646 -0.00031101788 -0.00030684445 -0.00030536571 -0.00030584284 -0.00030796745 -0.00031092844 -0.00031289121 -0.00031295128][-0.00032205335 -0.00032357051 -0.00032539765 -0.00032651759 -0.00032626896 -0.00032428815 -0.00032022124 -0.00031520543 -0.00031079742 -0.00030799606 -0.00030639578 -0.00030661596 -0.00030857921 -0.00031028184 -0.00031168247][-0.00032119654 -0.00032316244 -0.00032534453 -0.00032688866 -0.00032727837 -0.00032587268 -0.00032255042 -0.00031813921 -0.00031395341 -0.00031029189 -0.00030740455 -0.00030594063 -0.00030609878 -0.00030695493 -0.00030864595][-0.000320284 -0.00032259297 -0.00032472692 -0.00032635257 -0.00032713223 -0.00032598551 -0.00032309559 -0.00031935287 -0.00031570511 -0.00031197854 -0.0003086077 -0.0003060023 -0.00030442589 -0.00030381198 -0.00030473497][-0.00032047907 -0.00032278342 -0.00032496953 -0.00032628074 -0.00032679286 -0.00032533458 -0.00032257068 -0.00031932225 -0.00031619603 -0.00031282808 -0.00030938754 -0.00030660594 -0.00030453989 -0.0003027035 -0.00030233531][-0.00032002816 -0.0003224439 -0.00032456577 -0.00032550533 -0.00032548798 -0.00032404481 -0.000321666 -0.00031886878 -0.00031625381 -0.0003131729 -0.00030985294 -0.00030722847 -0.00030520471 -0.00030295012 -0.00030141606]]...]
INFO - root - 2017-12-09 08:21:54.211070: step 7110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:59m:56s remains)
INFO - root - 2017-12-09 08:22:02.866140: step 7120, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:58m:10s remains)
INFO - root - 2017-12-09 08:22:11.595773: step 7130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:33m:14s remains)
INFO - root - 2017-12-09 08:22:20.234730: step 7140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:59m:48s remains)
INFO - root - 2017-12-09 08:22:28.951826: step 7150, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:15m:29s remains)
INFO - root - 2017-12-09 08:22:37.382856: step 7160, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:25m:20s remains)
INFO - root - 2017-12-09 08:22:46.059473: step 7170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:29m:12s remains)
INFO - root - 2017-12-09 08:22:54.776957: step 7180, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:04m:53s remains)
INFO - root - 2017-12-09 08:23:03.374170: step 7190, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:59m:51s remains)
INFO - root - 2017-12-09 08:23:11.851026: step 7200, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 75h:40m:43s remains)
2017-12-09 08:23:12.712541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00031392631 -0.00031402305 -0.00031445443 -0.00031481229 -0.00031511424 -0.00031542176 -0.0003155891 -0.00031568715 -0.00031551014 -0.00031508284 -0.00031438988 -0.00031362011 -0.0003129773 -0.00031233695 -0.00031205494][-0.00031438665 -0.00031484533 -0.00031571338 -0.0003166555 -0.00031751627 -0.00031809421 -0.00031841823 -0.00031838732 -0.000317912 -0.00031680596 -0.00031555502 -0.00031434352 -0.00031334098 -0.00031230337 -0.00031163983][-0.000315023 -0.00031601737 -0.00031746604 -0.00031941119 -0.000320988 -0.00032199098 -0.00032247219 -0.00032214622 -0.00032113009 -0.00031942443 -0.00031777183 -0.00031626545 -0.00031483476 -0.0003133206 -0.000312119][-0.00031545488 -0.00031683408 -0.00031882705 -0.00032155131 -0.00032384627 -0.00032539415 -0.00032606846 -0.00032566403 -0.00032440323 -0.00032255184 -0.00032059976 -0.0003185872 -0.000316628 -0.00031470277 -0.00031314051][-0.00031577473 -0.0003172401 -0.00031943421 -0.00032232841 -0.0003249381 -0.00032683846 -0.00032758346 -0.00032706227 -0.00032591072 -0.00032410992 -0.00032228124 -0.00032028582 -0.00031812035 -0.00031610069 -0.00031432102][-0.00031566434 -0.00031698536 -0.00031905633 -0.00032156266 -0.00032391155 -0.00032558187 -0.0003261391 -0.00032581709 -0.000324757 -0.0003230178 -0.0003215741 -0.00031998349 -0.00031822704 -0.00031644828 -0.00031490254][-0.00031523898 -0.0003163137 -0.00031788729 -0.00031969277 -0.00032139404 -0.00032255394 -0.00032291914 -0.00032260426 -0.00032186389 -0.00032047991 -0.00031929367 -0.00031851802 -0.00031749756 -0.00031610669 -0.00031468918][-0.00031428324 -0.00031489664 -0.00031581367 -0.00031672476 -0.00031759471 -0.00031840961 -0.00031856779 -0.00031831197 -0.00031784573 -0.00031720547 -0.00031647398 -0.00031600683 -0.00031539085 -0.00031438275 -0.00031323085][-0.00031335288 -0.00031354287 -0.00031402006 -0.00031444218 -0.00031468156 -0.00031493237 -0.00031511387 -0.0003150423 -0.00031484407 -0.00031467923 -0.00031428813 -0.00031372174 -0.00031299386 -0.0003124426 -0.00031185563][-0.00031291493 -0.00031288824 -0.00031323411 -0.00031364942 -0.00031379177 -0.00031386936 -0.00031407177 -0.00031398423 -0.00031382457 -0.00031363257 -0.00031331836 -0.00031286551 -0.00031234964 -0.0003120265 -0.00031180808][-0.00031259851 -0.00031245506 -0.00031279496 -0.00031319115 -0.00031346723 -0.00031357145 -0.00031375425 -0.00031377544 -0.0003137007 -0.00031339773 -0.00031326179 -0.00031307834 -0.00031284604 -0.00031270381 -0.00031261746][-0.00031240875 -0.00031226245 -0.00031256117 -0.00031292564 -0.00031321871 -0.0003132722 -0.00031337034 -0.00031347197 -0.0003135908 -0.00031343231 -0.00031331871 -0.00031323353 -0.00031317028 -0.00031307942 -0.00031289668][-0.0003121777 -0.00031199577 -0.00031220072 -0.00031243876 -0.000312656 -0.00031276961 -0.00031282904 -0.00031290072 -0.0003129848 -0.00031297642 -0.00031295541 -0.00031288908 -0.00031283934 -0.00031273247 -0.00031257118][-0.00031207374 -0.00031182732 -0.00031192738 -0.00031202106 -0.00031210852 -0.00031221326 -0.000312316 -0.00031236548 -0.00031238105 -0.0003124004 -0.00031244082 -0.0003124403 -0.00031238946 -0.00031230948 -0.00031219053][-0.00031204859 -0.0003117618 -0.00031180627 -0.00031185575 -0.00031187449 -0.00031192196 -0.00031198282 -0.00031199551 -0.0003119827 -0.00031197327 -0.00031198791 -0.00031202377 -0.00031203093 -0.00031199781 -0.0003119497]]...]
INFO - root - 2017-12-09 08:23:21.299660: step 7210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:33m:26s remains)
INFO - root - 2017-12-09 08:23:29.895333: step 7220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:27m:34s remains)
INFO - root - 2017-12-09 08:23:38.634844: step 7230, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.992 sec/batch; 89h:36m:09s remains)
INFO - root - 2017-12-09 08:23:47.176511: step 7240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:33m:11s remains)
INFO - root - 2017-12-09 08:23:55.866436: step 7250, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:16m:14s remains)
INFO - root - 2017-12-09 08:24:04.401300: step 7260, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 76h:55m:04s remains)
INFO - root - 2017-12-09 08:24:12.974694: step 7270, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 75h:59m:41s remains)
INFO - root - 2017-12-09 08:24:21.716049: step 7280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:25m:18s remains)
INFO - root - 2017-12-09 08:24:30.317532: step 7290, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 78h:29m:51s remains)
INFO - root - 2017-12-09 08:24:38.886958: step 7300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:47m:53s remains)
2017-12-09 08:24:39.731729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00029777153 -0.00030337385 -0.00030972756 -0.00031156573 -0.00030834961 -0.00030239526 -0.00029803163 -0.00029573104 -0.00029621337 -0.00029769097 -0.000298549 -0.00029931398 -0.00029898348 -0.00029849494 -0.00029753041][-0.00029590988 -0.00030184642 -0.00030901507 -0.0003112441 -0.00030821571 -0.00030198961 -0.00029739458 -0.00029458216 -0.00029482151 -0.00029608846 -0.00029669024 -0.00029712892 -0.00029675377 -0.00029585898 -0.00029477317][-0.00029586919 -0.0003020555 -0.00030987393 -0.00031236993 -0.00030994526 -0.00030374562 -0.0002989494 -0.00029597938 -0.00029547248 -0.00029618162 -0.00029621346 -0.00029614352 -0.00029534256 -0.0002942608 -0.00029309472][-0.00029643776 -0.00030287818 -0.00031012928 -0.00031297881 -0.00031153558 -0.00030614375 -0.00030145209 -0.00029797171 -0.00029732121 -0.00029752948 -0.00029686635 -0.00029632324 -0.00029518767 -0.00029383413 -0.0002926][-0.00029749735 -0.00030355743 -0.00031034838 -0.0003129415 -0.00031166186 -0.00030714311 -0.00030211819 -0.00029814863 -0.00029697121 -0.00029700223 -0.00029610246 -0.0002954677 -0.00029467524 -0.00029372075 -0.00029270229][-0.00029870245 -0.00030465037 -0.00031082644 -0.00031347646 -0.00031235712 -0.00030801212 -0.00030276302 -0.00029818987 -0.00029611759 -0.0002960425 -0.00029521226 -0.00029472605 -0.00029392709 -0.00029332004 -0.0002925513][-0.00029993756 -0.00030574703 -0.00031130374 -0.0003135046 -0.00031252633 -0.00030841836 -0.0003028654 -0.00029794793 -0.00029571232 -0.00029578054 -0.00029540496 -0.00029529069 -0.00029473202 -0.00029437992 -0.00029439546][-0.00030017545 -0.00030616485 -0.00031125522 -0.00031293576 -0.00031121049 -0.00030734247 -0.00030160288 -0.00029643494 -0.0002937243 -0.00029445888 -0.00029552856 -0.00029664143 -0.00029720471 -0.00029829051 -0.00029968855][-0.00029885388 -0.00030493934 -0.00031058647 -0.00031192478 -0.00030928326 -0.00030459836 -0.00029897387 -0.00029376702 -0.0002907564 -0.00029239757 -0.00029482358 -0.00029768812 -0.00030030153 -0.00030294078 -0.00030583719][-0.000296832 -0.00030322972 -0.00030982716 -0.00031204254 -0.00030983781 -0.00030540355 -0.00029999856 -0.00029512064 -0.00029288916 -0.00029393623 -0.0002966004 -0.00030014507 -0.00030396448 -0.00030788605 -0.00031138276][-0.00029407704 -0.00030084065 -0.00030840206 -0.00031113529 -0.0003098833 -0.00030588906 -0.00030027021 -0.00029516645 -0.00029300805 -0.00029403967 -0.00029628875 -0.00030031524 -0.00030521292 -0.00031021971 -0.00031471669][-0.00029008859 -0.000297458 -0.00030639771 -0.00031014445 -0.00030961161 -0.00030607005 -0.0003001673 -0.00029463152 -0.00029151418 -0.00029203104 -0.0002939743 -0.00029826912 -0.00030367353 -0.00030971048 -0.00031517484][-0.00028652075 -0.00029402407 -0.00030398191 -0.00030923643 -0.000309869 -0.00030710947 -0.0003011981 -0.00029496831 -0.00029118638 -0.00029029851 -0.000291515 -0.000295239 -0.00030064382 -0.00030706622 -0.00031291915][-0.00028501597 -0.00029220351 -0.00030310644 -0.00030979741 -0.00031202548 -0.00031003857 -0.00030432391 -0.00029739877 -0.00029223209 -0.00028956967 -0.000289303 -0.00029169407 -0.00029631305 -0.00030242704 -0.00030875567][-0.00028556114 -0.00029258768 -0.00030327518 -0.00031040161 -0.00031330856 -0.00031220695 -0.00030661057 -0.00029882544 -0.00029231273 -0.00028800272 -0.00028653583 -0.00028780749 -0.00029166797 -0.00029740462 -0.00030430214]]...]
INFO - root - 2017-12-09 08:24:48.470501: step 7310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:49m:50s remains)
INFO - root - 2017-12-09 08:24:57.192952: step 7320, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 80h:22m:58s remains)
INFO - root - 2017-12-09 08:25:05.945505: step 7330, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:56m:26s remains)
INFO - root - 2017-12-09 08:25:14.529304: step 7340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:27m:52s remains)
INFO - root - 2017-12-09 08:25:23.198805: step 7350, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 80h:30m:27s remains)
INFO - root - 2017-12-09 08:25:31.964889: step 7360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:22m:32s remains)
INFO - root - 2017-12-09 08:25:40.775859: step 7370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:56m:36s remains)
INFO - root - 2017-12-09 08:25:49.556006: step 7380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:24m:43s remains)
INFO - root - 2017-12-09 08:25:58.095925: step 7390, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 77h:38m:48s remains)
INFO - root - 2017-12-09 08:26:06.558184: step 7400, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 77h:45m:13s remains)
2017-12-09 08:26:07.427845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0003208507 -0.00032059275 -0.00032098111 -0.00032167978 -0.00032228034 -0.00032323063 -0.00032456321 -0.00032625665 -0.00032873588 -0.00033183105 -0.00033599208 -0.0003403445 -0.00034460658 -0.0003485448 -0.00035135518][-0.00032060235 -0.0003202013 -0.00032072683 -0.00032133167 -0.00032162253 -0.00032253837 -0.00032388206 -0.00032580289 -0.00032851 -0.00033209665 -0.00033681589 -0.0003416744 -0.0003464038 -0.00035067723 -0.00035389466][-0.00032056804 -0.00032004956 -0.00032086219 -0.00032148208 -0.00032151479 -0.00032209756 -0.00032330438 -0.00032525658 -0.00032795183 -0.00033195334 -0.00033707856 -0.00034235849 -0.00034742797 -0.00035180233 -0.00035503384][-0.00032023498 -0.00031960581 -0.00032059074 -0.00032127669 -0.0003212328 -0.00032145082 -0.00032233028 -0.00032397782 -0.0003267001 -0.00033096824 -0.00033622023 -0.00034141168 -0.00034601317 -0.00034981288 -0.00035274253][-0.00032031248 -0.00031964466 -0.00032067008 -0.00032133367 -0.00032131406 -0.00032135661 -0.00032177108 -0.00032308063 -0.00032555411 -0.00032955303 -0.00033443922 -0.00033929682 -0.00034330424 -0.00034648037 -0.00034897341][-0.00032061402 -0.00031978555 -0.00032063317 -0.00032113845 -0.00032104668 -0.00032086793 -0.00032094202 -0.00032183784 -0.00032408789 -0.00032781911 -0.00033224872 -0.00033667276 -0.00034017596 -0.00034284225 -0.00034500874][-0.00032165137 -0.00032068748 -0.00032110905 -0.0003216851 -0.00032159779 -0.0003212528 -0.00032088906 -0.0003208101 -0.00032259186 -0.00032609087 -0.00033013991 -0.00033402626 -0.00033716846 -0.00033940055 -0.00034139687][-0.00032288325 -0.00032195711 -0.00032190708 -0.00032248269 -0.00032274963 -0.00032239419 -0.00032116915 -0.00031976635 -0.00032084447 -0.00032403332 -0.00032815544 -0.00033185535 -0.00033492933 -0.00033674983 -0.00033841829][-0.00032412019 -0.00032348823 -0.00032337528 -0.00032413588 -0.00032459377 -0.00032389237 -0.0003214688 -0.00031892397 -0.0003190451 -0.0003217202 -0.00032566267 -0.00032952757 -0.00033256828 -0.00033410161 -0.00033554094][-0.00032501802 -0.00032486185 -0.00032498868 -0.00032588266 -0.00032623307 -0.00032559197 -0.00032268921 -0.00031925569 -0.00031804509 -0.00031951978 -0.00032260804 -0.00032634265 -0.00032946776 -0.00033085977 -0.00033205067][-0.00032488673 -0.00032511051 -0.00032579596 -0.0003264751 -0.00032652609 -0.00032597041 -0.00032324798 -0.0003197171 -0.00031771438 -0.00031801991 -0.00032010264 -0.00032329635 -0.00032598653 -0.00032733107 -0.00032828498][-0.00032402767 -0.00032443981 -0.00032551074 -0.0003260588 -0.00032621948 -0.00032591569 -0.00032372159 -0.00032059487 -0.00031821427 -0.00031740751 -0.00031821444 -0.00032037302 -0.00032240228 -0.000323553 -0.00032422342][-0.00032291311 -0.00032333683 -0.00032443533 -0.00032488265 -0.00032507296 -0.0003249351 -0.00032337932 -0.00032103254 -0.00031877324 -0.00031753787 -0.00031761965 -0.00031890289 -0.00032027106 -0.00032112387 -0.00032143813][-0.00032150716 -0.000321827 -0.00032273011 -0.00032317464 -0.00032328709 -0.00032308948 -0.00032201933 -0.00032040826 -0.00031870572 -0.00031757384 -0.00031733187 -0.00031796284 -0.00031877207 -0.00031927472 -0.00031941367][-0.00032027691 -0.00032022054 -0.00032082995 -0.00032115035 -0.00032114948 -0.00032103783 -0.000320466 -0.00031951533 -0.00031852419 -0.00031784558 -0.0003176452 -0.00031789974 -0.00031828877 -0.00031855723 -0.00031868051]]...]
INFO - root - 2017-12-09 08:26:16.051932: step 7410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:41m:09s remains)
INFO - root - 2017-12-09 08:26:24.721387: step 7420, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 76h:53m:41s remains)
INFO - root - 2017-12-09 08:26:33.363671: step 7430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:58m:06s remains)
INFO - root - 2017-12-09 08:26:41.928264: step 7440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:39m:24s remains)
INFO - root - 2017-12-09 08:26:50.431795: step 7450, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.824 sec/batch; 74h:21m:59s remains)
INFO - root - 2017-12-09 08:26:59.027880: step 7460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 80h:49m:19s remains)
INFO - root - 2017-12-09 08:27:07.450852: step 7470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:44m:29s remains)
INFO - root - 2017-12-09 08:27:16.228580: step 7480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-09 08:27:24.877529: step 7490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:57m:51s remains)
INFO - root - 2017-12-09 08:27:33.624796: step 7500, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 66h:52m:19s remains)
2017-12-09 08:27:34.490414: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0051593841 0.0037318734 0.0028167139 0.0030002324 0.0046396968 0.0075735408 0.010911975 0.013729078 0.01513501 0.015148003 0.014092937 0.012775517 0.011677693 0.010770908 0.0097430171][0.0085194241 0.007022995 0.0060399137 0.0063742064 0.0087828049 0.012917362 0.017639037 0.021705655 0.023694547 0.023685563 0.022282049 0.020897573 0.019946516 0.019051289 0.017688127][0.011468503 0.010715211 0.01088504 0.012823582 0.017522424 0.024224117 0.031480741 0.037556469 0.040634137 0.040960439 0.03936974 0.037776791 0.036585469 0.0352195 0.03276588][0.013321271 0.014024174 0.016661232 0.022141779 0.031361826 0.042787313 0.05432364 0.063491687 0.0682037 0.0689541 0.067003541 0.064660057 0.062314168 0.059381995 0.054760702][0.014719957 0.017750621 0.024190998 0.034971189 0.050486892 0.068359181 0.08542 0.098351941 0.10496707 0.10604275 0.10328577 0.0991229 0.094284117 0.088378996 0.08029826][0.0167622 0.023130374 0.034600634 0.051726256 0.074107587 0.098564655 0.12087912 0.13710941 0.14521222 0.14629073 0.14237171 0.13571452 0.12728277 0.11720482 0.10477493][0.020413877 0.030532766 0.047171 0.070444874 0.098868363 0.12846339 0.15426378 0.17221443 0.18056354 0.18089123 0.17535941 0.16599584 0.15387185 0.13942944 0.12266053][0.0260636 0.039663743 0.060393564 0.087803572 0.11955675 0.15148593 0.17827168 0.19601578 0.20330684 0.20211954 0.19472776 0.18289366 0.167693 0.1497715 0.1295944][0.032918241 0.049301129 0.072310239 0.10105591 0.1327008 0.1634206 0.18823093 0.203731 0.20865925 0.20527692 0.19589658 0.18216199 0.1649933 0.14507857 0.12323149][0.038404677 0.056572407 0.080080517 0.10753211 0.1360772 0.16265461 0.18315126 0.19479647 0.1964882 0.19050263 0.17912559 0.16407175 0.14613496 0.1260049 0.10462912][0.039784882 0.058145154 0.080304429 0.10455842 0.12824045 0.14901206 0.16387674 0.17082167 0.16902043 0.16060977 0.1478543 0.13253686 0.11540426 0.097107343 0.07848265][0.035886157 0.052457731 0.071472034 0.09113802 0.10914379 0.12380922 0.13312879 0.13581187 0.13138856 0.12175586 0.10904691 0.094979659 0.080300041 0.065545559 0.051259581][0.028019769 0.041272424 0.055957761 0.070443921 0.082880728 0.092097186 0.096900478 0.096607685 0.09109696 0.0818911 0.070827708 0.059406634 0.048298433 0.03789236 0.02840762][0.018643914 0.027985003 0.038113642 0.047686528 0.055335172 0.060292095 0.061994 0.060251158 0.05518629 0.047875494 0.039643023 0.03160236 0.024300978 0.01798534 0.012688479][0.010229221 0.015920918 0.022043884 0.02763241 0.031758174 0.03396339 0.034060016 0.032119893 0.028383046 0.023537328 0.018391671 0.013627589 0.0096156457 0.0064764237 0.0041254479]]...]
INFO - root - 2017-12-09 08:27:43.082554: step 7510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:18m:10s remains)
INFO - root - 2017-12-09 08:27:51.721034: step 7520, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:21m:47s remains)
INFO - root - 2017-12-09 08:28:00.416758: step 7530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 76h:24m:51s remains)
INFO - root - 2017-12-09 08:28:08.885076: step 7540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:01m:12s remains)
INFO - root - 2017-12-09 08:28:17.492761: step 7550, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:20m:57s remains)
INFO - root - 2017-12-09 08:28:25.999554: step 7560, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 76h:56m:52s remains)
INFO - root - 2017-12-09 08:28:34.575948: step 7570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:44m:56s remains)
INFO - root - 2017-12-09 08:28:43.201827: step 7580, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 79h:56m:13s remains)
INFO - root - 2017-12-09 08:28:51.783389: step 7590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:27m:03s remains)
INFO - root - 2017-12-09 08:29:00.598423: step 7600, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:21m:27s remains)
2017-12-09 08:29:01.429971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00046478695 -0.00046398863 -0.0004636017 -0.00046308321 -0.00046190913 -0.00046061637 -0.00045870792 -0.00045548682 -0.0004531336 -0.00045483321 -0.00045740564 -0.000458736 -0.00046072909 -0.00046345778 -0.00046405548][-0.00046406762 -0.00046486486 -0.00046534216 -0.00046458328 -0.00046288141 -0.00046000927 -0.00045070887 -0.00043843882 -0.00043184421 -0.00043700356 -0.00044412428 -0.00044920537 -0.00045578575 -0.00046316092 -0.00046559024][-0.00045825096 -0.00046322611 -0.00046646161 -0.00046666357 -0.00046501547 -0.00045740258 -0.0004281624 -0.00039351807 -0.000376106 -0.00038868809 -0.0004067425 -0.00042294647 -0.00043981246 -0.00045706105 -0.00046563943][-0.00044223346 -0.00045588793 -0.00046548527 -0.00046835895 -0.00046730065 -0.00044886596 -0.00038228219 -0.00030745083 -0.00026906026 -0.00029298867 -0.00033025478 -0.00036839783 -0.000405863 -0.00044111465 -0.00046258315][-0.0004151295 -0.00044167062 -0.00046234558 -0.00046954164 -0.00046930258 -0.00043459257 -0.00031787524 -0.00018831174 -0.00012001084 -0.00015680477 -0.00021818921 -0.00028574411 -0.00035278813 -0.00041550287 -0.0004573075][-0.00038233577 -0.00042277912 -0.00045642894 -0.00046932622 -0.00047022864 -0.0004188263 -0.00025312969 -6.8084453e-05 3.2735581e-05 -1.3582438e-05 -9.713156e-05 -0.00019344423 -0.00029168112 -0.00038497362 -0.00045030384][-0.00035875541 -0.00040846312 -0.00045111481 -0.00046855834 -0.000470445 -0.00040932684 -0.00021282432 9.471958e-06 0.00013345241 8.4133935e-05 -1.2408535e-05 -0.00012716613 -0.00024664094 -0.00036201967 -0.00044443388][-0.00035570763 -0.00040566895 -0.00044973614 -0.00046835304 -0.00047044357 -0.00041076966 -0.0002143778 1.2354576e-05 0.00014116644 9.5814234e-05 -1.2460514e-06 -0.00011719088 -0.00023812248 -0.00035715994 -0.00044253597][-0.00037615915 -0.00041718787 -0.00045386757 -0.00046972244 -0.00047154928 -0.00042447067 -0.00025948975 -6.2060513e-05 5.1799987e-05 1.5139231e-05 -7.0719805e-05 -0.00017096184 -0.00027310435 -0.00037449959 -0.00044674659][-0.00040947081 -0.00043636706 -0.00046073127 -0.00047131698 -0.00047215866 -0.00044176428 -0.0003255643 -0.0001809542 -9.66544e-05 -0.00012321374 -0.00018987749 -0.00026336274 -0.0003343492 -0.000405569 -0.00045496834][-0.00044038074 -0.00045479788 -0.00046723784 -0.00047219804 -0.00047208043 -0.00045555795 -0.00038699439 -0.00029952519 -0.00024797037 -0.00026479035 -0.00030983129 -0.00035551263 -0.00039587234 -0.000436668 -0.00046299378][-0.00045972478 -0.0004657672 -0.00047034628 -0.00047152725 -0.00047033757 -0.00046280798 -0.00043024495 -0.00038790854 -0.00036320795 -0.00037197035 -0.00039750771 -0.00042078528 -0.00043891362 -0.000457394 -0.00046753109][-0.00046812193 -0.00046984994 -0.00047062698 -0.00046956106 -0.00046752961 -0.00046400784 -0.00045198752 -0.00043649413 -0.00042814793 -0.00043260504 -0.00044422562 -0.00045402924 -0.00046065261 -0.00046700586 -0.00046905375][-0.00046905573 -0.00046906024 -0.00046857013 -0.00046674494 -0.00046440586 -0.0004622704 -0.00045859968 -0.00045447383 -0.00045319018 -0.00045558042 -0.00046022586 -0.00046420694 -0.00046679159 -0.0004683341 -0.00046780106][-0.00046633586 -0.00046582823 -0.0004653038 -0.00046363231 -0.00046147636 -0.00045981281 -0.00045852535 -0.000457664 -0.00045814927 -0.00045961482 -0.00046169857 -0.00046372591 -0.00046527333 -0.00046587209 -0.00046505887]]...]
INFO - root - 2017-12-09 08:29:09.966319: step 7610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:55m:02s remains)
INFO - root - 2017-12-09 08:29:18.562656: step 7620, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 74h:27m:51s remains)
INFO - root - 2017-12-09 08:29:27.073805: step 7630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:46m:07s remains)
INFO - root - 2017-12-09 08:29:35.697378: step 7640, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 79h:43m:39s remains)
INFO - root - 2017-12-09 08:29:44.486085: step 7650, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 81h:16m:02s remains)
INFO - root - 2017-12-09 08:29:53.189809: step 7660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:30m:54s remains)
INFO - root - 2017-12-09 08:30:01.780602: step 7670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:08m:04s remains)
INFO - root - 2017-12-09 08:30:10.447485: step 7680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:03m:01s remains)
INFO - root - 2017-12-09 08:30:19.071162: step 7690, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 79h:19m:43s remains)
INFO - root - 2017-12-09 08:30:27.863282: step 7700, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 79h:54m:34s remains)
2017-12-09 08:30:28.612563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.000514961 -0.00051421614 -0.00051357353 -0.000512526 -0.00051128049 -0.00051058445 -0.00050987804 -0.00050910556 -0.0005084544 -0.00050795468 -0.00050722586 -0.00050662551 -0.00050618825 -0.00050596427 -0.00050579477][-0.00051292992 -0.00051292137 -0.00051332056 -0.00051262358 -0.00051187206 -0.000511046 -0.00051010866 -0.00050927105 -0.00050864241 -0.00050794933 -0.00050698459 -0.00050594419 -0.00050535187 -0.00050499785 -0.00050478586][-0.00051112485 -0.00051153143 -0.00051196816 -0.00051139522 -0.00051105046 -0.00051058957 -0.00050957967 -0.0005094459 -0.000509409 -0.00050857605 -0.0005073885 -0.00050611293 -0.00050551281 -0.00050503621 -0.00050462643][-0.00051007757 -0.000509865 -0.00050954486 -0.00050852459 -0.00050835044 -0.00050839048 -0.00050863042 -0.00050937891 -0.00051013852 -0.00050957879 -0.00050836359 -0.00050696661 -0.00050604751 -0.00050537026 -0.00050475565][-0.00050892524 -0.00050759182 -0.00050661474 -0.00050557574 -0.00050530815 -0.000505489 -0.00050682144 -0.00050897652 -0.00051048212 -0.00051051239 -0.00050964736 -0.00050824514 -0.00050720165 -0.00050613307 -0.00050538441][-0.00050785416 -0.00050530396 -0.00050418219 -0.000502862 -0.000502456 -0.00050329295 -0.00050555269 -0.00050813233 -0.00050999195 -0.000510404 -0.00050959352 -0.00050837343 -0.00050766225 -0.00050692697 -0.000506477][-0.00050708593 -0.00050399418 -0.00050235237 -0.00050044607 -0.00050103653 -0.00050313171 -0.00050624076 -0.00050894957 -0.0005114777 -0.00051149091 -0.00051079923 -0.0005098 -0.00050885195 -0.00050805713 -0.00050761807][-0.00050756586 -0.00050418166 -0.00050143636 -0.00049958355 -0.00050053006 -0.00050330278 -0.00050707115 -0.0005102664 -0.0005126202 -0.00051261083 -0.000512115 -0.00051109557 -0.00050990324 -0.00050892716 -0.00050857046][-0.00051008235 -0.00050702883 -0.00050398137 -0.00050199713 -0.00050271419 -0.00050529389 -0.00050876226 -0.00051171728 -0.00051336532 -0.00051342446 -0.00051313965 -0.00051199464 -0.00051060144 -0.00050949416 -0.00050903176][-0.00051328522 -0.00051111646 -0.000508791 -0.00050699728 -0.00050732953 -0.00050880807 -0.00051147235 -0.00051333156 -0.00051428482 -0.00051453058 -0.00051412324 -0.00051237515 -0.00051077968 -0.00050977257 -0.00050918414][-0.00051644369 -0.00051512365 -0.0005135822 -0.00051216461 -0.0005117589 -0.00051249977 -0.00051460706 -0.00051540462 -0.00051555684 -0.000515368 -0.00051450147 -0.000512616 -0.00051099673 -0.00050990545 -0.00050928193][-0.000519126 -0.00051787408 -0.00051654538 -0.00051527662 -0.00051480916 -0.00051528163 -0.0005165239 -0.00051668304 -0.0005160924 -0.00051501481 -0.00051397813 -0.00051231991 -0.00051092496 -0.00050987524 -0.00050935551][-0.00052134308 -0.00052015489 -0.00051909371 -0.000518087 -0.00051736494 -0.00051675492 -0.00051646086 -0.000515559 -0.00051410985 -0.00051281345 -0.00051167753 -0.000510681 -0.00050983741 -0.00050918554 -0.00050885556][-0.00052323024 -0.00052186142 -0.00052140787 -0.00052015425 -0.00051841181 -0.00051668141 -0.000515172 -0.00051361113 -0.00051197613 -0.00051058031 -0.00050962379 -0.00050907757 -0.00050858833 -0.0005083587 -0.00050828996][-0.00052391452 -0.0005221449 -0.00052122 -0.00051945564 -0.00051708479 -0.00051493105 -0.00051311165 -0.00051173725 -0.00051045796 -0.00050948595 -0.0005088576 -0.00050851953 -0.00050826068 -0.0005081016 -0.00050804409]]...]
INFO - root - 2017-12-09 08:30:37.128269: step 7710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:35m:24s remains)
INFO - root - 2017-12-09 08:30:45.785261: step 7720, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 78h:03m:22s remains)
INFO - root - 2017-12-09 08:30:54.438517: step 7730, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 76h:42m:18s remains)
INFO - root - 2017-12-09 08:31:02.891874: step 7740, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 81h:05m:23s remains)
INFO - root - 2017-12-09 08:31:11.597893: step 7750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:12m:58s remains)
INFO - root - 2017-12-09 08:31:20.242098: step 7760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:41m:42s remains)
INFO - root - 2017-12-09 08:31:28.855036: step 7770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:45m:09s remains)
INFO - root - 2017-12-09 08:31:37.499035: step 7780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:50m:47s remains)
INFO - root - 2017-12-09 08:31:45.994599: step 7790, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:15m:08s remains)
INFO - root - 2017-12-09 08:31:54.470742: step 7800, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 74h:17m:30s remains)
2017-12-09 08:31:55.317953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00056810124 -0.0005678875 -0.00056816579 -0.000567853 -0.00056670106 -0.00056396268 -0.00055570033 -0.00053975458 -0.00053025334 -0.00052951404 -0.00053626078 -0.00054536056 -0.00055386854 -0.00056051416 -0.0005641295][-0.00056919752 -0.00056854077 -0.00056832726 -0.00056750193 -0.00056444912 -0.00055529358 -0.00053779513 -0.00050899619 -0.00049459923 -0.00049707526 -0.0005123053 -0.00053111516 -0.00054684625 -0.00055791379 -0.00056376721][-0.00057005323 -0.0005694679 -0.00056942488 -0.00056888669 -0.0005557001 -0.00050732971 -0.00041646097 -0.00032108629 -0.00028724212 -0.0003193251 -0.00038667087 -0.00045789039 -0.00051400962 -0.00054835167 -0.0005632668][-0.00057143613 -0.00057108718 -0.00057156535 -0.00057122827 -0.00054562691 -0.00044496034 -0.00025409763 -5.32213e-05 2.7022441e-05 -4.2127795e-05 -0.00018758816 -0.00033853317 -0.00045491426 -0.00052587612 -0.0005582507][-0.00057296804 -0.00057215343 -0.000571192 -0.00056949316 -0.0005263788 -0.00035610772 -2.514181e-05 0.00033282337 0.00050190074 0.00040006574 0.00014268362 -0.0001317547 -0.00034613471 -0.00047968765 -0.00054463453][-0.00052686728 -0.00053856388 -0.00054551766 -0.00054275367 -0.00047961381 -0.00025618947 0.00016329112 0.000619063 0.000856365 0.0007535746 0.00042563322 5.4347096e-05 -0.00023980325 -0.000424795 -0.00052077125][-0.00043507226 -0.00044398755 -0.00044986355 -0.00042925944 -0.00031605281 -1.6269274e-05 0.00049634493 0.0010481935 0.0013518983 0.0012459485 0.000837072 0.00034478609 -6.45722e-05 -0.00033190672 -0.00047773332][-0.0002655164 -0.00027709454 -0.00026074346 -0.00016551776 8.7532157e-05 0.00055346993 0.0012101203 0.0018538039 0.0022031032 0.0020807348 0.0015680895 0.00090375863 0.00030570896 -0.00011711358 -0.00036867143][0.00029911823 0.00032184657 0.00036321045 0.0005101228 0.00087040622 0.001476316 0.0022606689 0.0029833068 0.0033590586 0.0032117246 0.0025911876 0.0017287866 0.00088821462 0.00024100975 -0.00017539697][0.0012323388 0.0012087126 0.0011468423 0.0012150644 0.0015791452 0.0022304971 0.0030385072 0.0037422108 0.0040807514 0.0038963857 0.0032158312 0.0022459002 0.0012582736 0.00046541885 -5.8122503e-05][0.0026804903 0.002493056 0.0021118545 0.0018101693 0.0018479114 0.0022510211 0.0028824841 0.0034669598 0.0037432283 0.003568684 0.0029569885 0.0020704952 0.001147446 0.00039417337 -0.00010103988][0.0042724935 0.0036962128 0.0027987841 0.00199121 0.001598971 0.0016492507 0.0019868976 0.0023485566 0.002512655 0.0023679563 0.0019190677 0.0012767627 0.0006102238 7.0074806e-05 -0.0002753122][0.005972595 0.0048615886 0.003360328 0.0020196263 0.0012021584 0.00091067812 0.00097353087 0.0011417014 0.0012234766 0.0011303597 0.00086248812 0.00048526231 9.5731753e-05 -0.0002184732 -0.00041523221][0.0075861448 0.0059376666 0.0038902096 0.0021018293 0.00095662015 0.0004163761 0.00028081838 0.00031347526 0.00034667365 0.0003028686 0.00017078459 -1.765968e-05 -0.00021461368 -0.00037598802 -0.00047871735][0.0091059525 0.0070398 0.0045580459 0.0023876065 0.00094914011 0.00018933573 -0.0001083473 -0.00017915154 -0.0001842785 -0.00020095694 -0.00025275457 -0.0003280396 -0.0004078194 -0.00047454837 -0.00051866058]]...]
INFO - root - 2017-12-09 08:32:03.726671: step 7810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:50m:09s remains)
INFO - root - 2017-12-09 08:32:12.262863: step 7820, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 76h:14m:07s remains)
INFO - root - 2017-12-09 08:32:20.921841: step 7830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:39m:24s remains)
INFO - root - 2017-12-09 08:32:29.475726: step 7840, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 79h:24m:06s remains)
INFO - root - 2017-12-09 08:32:38.112627: step 7850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:08m:25s remains)
INFO - root - 2017-12-09 08:32:46.697173: step 7860, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:00m:02s remains)
INFO - root - 2017-12-09 08:32:55.180086: step 7870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:17m:56s remains)
INFO - root - 2017-12-09 08:33:03.811341: step 7880, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 74h:31m:07s remains)
INFO - root - 2017-12-09 08:33:12.278807: step 7890, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:18m:40s remains)
INFO - root - 2017-12-09 08:33:21.028069: step 7900, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 76h:08m:20s remains)
2017-12-09 08:33:21.969526: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34815705 0.35016686 0.3491731 0.34347594 0.33190221 0.31288594 0.2869409 0.25319707 0.21313857 0.1706983 0.13012743 0.096022837 0.069243073 0.050221495 0.038144674][0.42139128 0.42654622 0.42718366 0.42188948 0.40884569 0.3873764 0.35646471 0.31653187 0.26734751 0.21348871 0.16095071 0.11506745 0.078393482 0.052127294 0.035162818][0.4885495 0.4982326 0.50125569 0.4955965 0.48083705 0.456391 0.4218387 0.37732208 0.3223117 0.26152661 0.20064311 0.1464095 0.10202944 0.06849277 0.04580497][0.54277641 0.55636626 0.56164122 0.55623144 0.53993237 0.51294935 0.47503883 0.42715582 0.36851549 0.3046079 0.24067977 0.18269764 0.13407497 0.096513525 0.070201442][0.58851522 0.60519814 0.61125708 0.60564 0.58815503 0.55935788 0.51941323 0.4693689 0.40916479 0.34400743 0.27918857 0.22074507 0.1715634 0.133118 0.1059011][0.62604952 0.647207 0.65508163 0.64946407 0.63022274 0.59953845 0.55818182 0.507741 0.44881567 0.3856101 0.32339349 0.26708838 0.21908662 0.18106332 0.15333514][0.65556878 0.6807074 0.69002157 0.68441468 0.66374439 0.63152063 0.58931506 0.53937024 0.4830102 0.4250713 0.36999792 0.32066175 0.27783448 0.24313453 0.21637444][0.67596531 0.70585704 0.71598214 0.70927125 0.68705404 0.65286088 0.6106478 0.56201375 0.51011276 0.45918381 0.41255361 0.37289125 0.33844432 0.3103919 0.287042][0.68852067 0.7204271 0.73016649 0.72192442 0.69821787 0.66310006 0.621571 0.575476 0.52795094 0.48375484 0.44591376 0.41604495 0.39071715 0.37007478 0.35199627][0.69035512 0.72420788 0.73358858 0.72380906 0.69933569 0.6640774 0.62433594 0.58131629 0.538567 0.50078881 0.47003022 0.44776574 0.4296506 0.4151234 0.40092549][0.67167962 0.70764339 0.71858054 0.70928293 0.68630308 0.65297461 0.61635107 0.57694906 0.53913909 0.50727993 0.48291814 0.46719432 0.45467952 0.44453377 0.43254957][0.63131386 0.66688192 0.67894876 0.6722306 0.653379 0.62487888 0.59429777 0.56076658 0.52894527 0.50180447 0.48217353 0.470766 0.46163055 0.45385355 0.44266498][0.57555509 0.6085763 0.62007725 0.61510122 0.60061425 0.5780797 0.55454969 0.528445 0.50421184 0.48286164 0.46731114 0.45794725 0.45017663 0.44273731 0.43109858][0.50553238 0.53526706 0.54643124 0.54348379 0.53281003 0.51552534 0.49782217 0.47815949 0.46043652 0.44426817 0.43254253 0.42481822 0.41755661 0.40966034 0.39770353][0.42455885 0.45074412 0.4613274 0.46071649 0.45398402 0.44180417 0.42914116 0.41471431 0.40210074 0.38988048 0.38044462 0.37332347 0.36606279 0.35799912 0.34637329]]...]
INFO - root - 2017-12-09 08:33:30.447641: step 7910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:32m:57s remains)
INFO - root - 2017-12-09 08:33:39.306022: step 7920, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 78h:23m:53s remains)
INFO - root - 2017-12-09 08:33:47.981148: step 7930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:28m:09s remains)
INFO - root - 2017-12-09 08:33:56.611657: step 7940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:23m:14s remains)
INFO - root - 2017-12-09 08:34:05.302147: step 7950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:21m:45s remains)
INFO - root - 2017-12-09 08:34:14.102304: step 7960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:25m:10s remains)
INFO - root - 2017-12-09 08:34:22.588045: step 7970, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 78h:40m:20s remains)
INFO - root - 2017-12-09 08:34:31.217314: step 7980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:44m:22s remains)
INFO - root - 2017-12-09 08:34:39.886237: step 7990, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:36m:53s remains)
INFO - root - 2017-12-09 08:34:48.535846: step 8000, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 74h:49m:16s remains)
2017-12-09 08:34:49.461255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00079820864 -0.00080001471 -0.0008025798 -0.0008039308 -0.00080443977 -0.000804758 -0.00080520951 -0.00080520532 -0.00080482796 -0.00080430543 -0.00080309808 -0.0008016791 -0.00080027146 -0.00079895672 -0.00079721957][-0.00079353841 -0.00079490978 -0.00079699524 -0.00079819834 -0.00079867244 -0.00079887372 -0.00079881214 -0.00079825486 -0.00079739158 -0.00079657225 -0.00079545425 -0.00079420616 -0.00079317507 -0.00079227821 -0.00079114386][-0.00079085038 -0.00079130544 -0.00079244329 -0.00079301232 -0.00079311442 -0.00079287536 -0.00079224084 -0.00079121609 -0.00079013524 -0.00078939646 -0.000788567 -0.00078789005 -0.00078747771 -0.00078721752 -0.00078683277][-0.00078934559 -0.00078895752 -0.00078926224 -0.00078931864 -0.00078903342 -0.00078843877 -0.00078740786 -0.00078622327 -0.00078516512 -0.000784575 -0.00078406068 -0.00078376982 -0.00078383478 -0.00078409462 -0.0007843381][-0.00078897882 -0.00078815548 -0.00078796723 -0.00078770233 -0.00078715215 -0.00078631961 -0.00078513706 -0.00078396569 -0.00078295486 -0.0007824357 -0.00078218349 -0.00078209367 -0.00078232668 -0.0007828821 -0.00078353053][-0.00078925624 -0.000788279 -0.00078782014 -0.00078723824 -0.00078648096 -0.00078557734 -0.00078431179 -0.00078313105 -0.00078224472 -0.00078190141 -0.0007819513 -0.00078205339 -0.00078241085 -0.00078307075 -0.00078392262][-0.00078963162 -0.0007885049 -0.00078792643 -0.0007872326 -0.0007864433 -0.00078559038 -0.0007845246 -0.00078341877 -0.00078258337 -0.00078256032 -0.00078290992 -0.0007832846 -0.00078393862 -0.00078481756 -0.00078572496][-0.00079028308 -0.00078896206 -0.00078831124 -0.00078771997 -0.000787014 -0.00078632589 -0.00078560121 -0.00078481372 -0.00078419771 -0.00078440586 -0.00078502914 -0.00078573287 -0.00078681868 -0.00078804436 -0.00078906334][-0.00079085294 -0.0007895194 -0.00078887108 -0.00078836939 -0.00078786095 -0.000787408 -0.000787078 -0.00078684493 -0.00078679429 -0.00078758545 -0.00078860839 -0.00078988605 -0.000791443 -0.000793096 -0.00079416076][-0.00079124357 -0.000790064 -0.00078953779 -0.00078906567 -0.0007886797 -0.000788583 -0.00078888331 -0.00078930281 -0.00078996306 -0.00079146406 -0.00079293596 -0.00079443178 -0.00079608744 -0.00079778093 -0.00079874136][-0.00079171895 -0.00079059025 -0.00079028815 -0.00078992854 -0.00078965916 -0.00078990147 -0.000790958 -0.000792213 -0.00079359673 -0.00079572125 -0.0007974605 -0.00079873187 -0.00080003886 -0.00080134074 -0.00080215605][-0.00079227576 -0.00079111464 -0.00079093152 -0.0007907009 -0.00079061976 -0.00079126051 -0.00079317752 -0.00079544971 -0.0007976038 -0.00080014055 -0.00080176187 -0.00080239034 -0.00080277008 -0.00080349128 -0.0008042446][-0.00079267973 -0.00079146092 -0.0007912186 -0.000791057 -0.00079120544 -0.00079238892 -0.00079507474 -0.0007984488 -0.00080141064 -0.00080409111 -0.00080552511 -0.00080534758 -0.00080480688 -0.00080463151 -0.00080515677][-0.000792706 -0.000791402 -0.00079101964 -0.00079074927 -0.000791005 -0.00079271011 -0.00079632679 -0.0008008299 -0.00080458517 -0.00080744381 -0.0008087086 -0.0008078278 -0.00080633326 -0.00080507225 -0.00080528052][-0.00079170219 -0.00079027977 -0.00078979082 -0.00078941463 -0.00078975991 -0.00079181721 -0.00079618371 -0.00080159784 -0.00080599514 -0.00080924417 -0.00081072794 -0.00080986141 -0.00080780603 -0.0008057162 -0.00080559368]]...]
INFO - root - 2017-12-09 08:34:57.907179: step 8010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:19m:08s remains)
INFO - root - 2017-12-09 08:35:06.650071: step 8020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:13m:41s remains)
INFO - root - 2017-12-09 08:35:15.267994: step 8030, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 75h:30m:21s remains)
INFO - root - 2017-12-09 08:35:23.788285: step 8040, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 66h:03m:43s remains)
INFO - root - 2017-12-09 08:35:32.477321: step 8050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:54m:11s remains)
INFO - root - 2017-12-09 08:35:41.187572: step 8060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:14m:27s remains)
INFO - root - 2017-12-09 08:35:49.892868: step 8070, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:51m:05s remains)
INFO - root - 2017-12-09 08:35:58.503357: step 8080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:02m:48s remains)
INFO - root - 2017-12-09 08:36:07.077387: step 8090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:10m:34s remains)
INFO - root - 2017-12-09 08:36:15.828920: step 8100, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:17m:19s remains)
2017-12-09 08:36:16.822604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00095523347 -0.00095395959 -0.00095452252 -0.0009554805 -0.00095628807 -0.0009569106 -0.00095734256 -0.00095789938 -0.00095809193 -0.00095764088 -0.00095697632 -0.00095603144 -0.00095510087 -0.00095425639 -0.00095357938][-0.00095452124 -0.000953665 -0.00095488696 -0.00095644762 -0.00095784862 -0.00095883454 -0.00095963926 -0.00096027582 -0.00096044806 -0.00095965242 -0.00095850159 -0.00095708942 -0.00095558824 -0.000954203 -0.0009529753][-0.00095532474 -0.00095500593 -0.00095685554 -0.00095942093 -0.00096194015 -0.00096370612 -0.00096491765 -0.00096589 -0.00096588518 -0.00096478348 -0.00096288323 -0.000960402 -0.00095805264 -0.00095600076 -0.00095410971][-0.00095631275 -0.00095647632 -0.00095905032 -0.00096265291 -0.00096611294 -0.00096856937 -0.00097039621 -0.0009711351 -0.00097074063 -0.000969122 -0.00096673222 -0.00096365059 -0.00096068665 -0.00095792848 -0.00095523067][-0.00095715275 -0.00095789612 -0.00096123363 -0.00096598041 -0.0009704929 -0.00097336242 -0.00097519282 -0.00097596337 -0.00097533432 -0.00097339507 -0.00097028259 -0.00096648763 -0.00096306216 -0.00095998327 -0.0009567294][-0.00095777557 -0.000958944 -0.00096268737 -0.00096837262 -0.000973975 -0.00097732444 -0.00097958569 -0.00098059035 -0.00098039221 -0.00097809 -0.00097400014 -0.00096922141 -0.00096530467 -0.0009618894 -0.00095838634][-0.00095830497 -0.00095961784 -0.00096359738 -0.00096996041 -0.00097665854 -0.00098062737 -0.00098356663 -0.00098552811 -0.00098574336 -0.00098357815 -0.000978195 -0.00097239 -0.00096777064 -0.00096380821 -0.00095977413][-0.00095861021 -0.00095975341 -0.00096378924 -0.00097039656 -0.00097784819 -0.00098244508 -0.000985943 -0.00098849949 -0.00098929368 -0.00098717562 -0.00098143343 -0.00097503234 -0.00097000133 -0.00096521457 -0.00096064032][-0.0009581531 -0.00095902022 -0.0009627993 -0.00096920464 -0.0009766859 -0.00098139246 -0.00098527269 -0.000988088 -0.00098947564 -0.00098774722 -0.00098268967 -0.000976579 -0.00097112911 -0.00096591894 -0.000961263][-0.00095733011 -0.00095787423 -0.00096129189 -0.00096743833 -0.00097450375 -0.00097915914 -0.00098314241 -0.00098638725 -0.00098768238 -0.00098678214 -0.00098243123 -0.0009768249 -0.00097168249 -0.00096635445 -0.00096153875][-0.00095633121 -0.00095633656 -0.00095947314 -0.00096505746 -0.00097107131 -0.00097604509 -0.00098071212 -0.00098418712 -0.00098550052 -0.00098464824 -0.00098077371 -0.00097599474 -0.00097128935 -0.00096591742 -0.0009607398][-0.00095510803 -0.00095470471 -0.0009573861 -0.0009614859 -0.00096658885 -0.00097177707 -0.00097680232 -0.000980359 -0.0009817176 -0.0009815176 -0.00097842992 -0.00097415812 -0.00096954155 -0.00096397364 -0.00095908006][-0.0009539852 -0.00095313106 -0.00095506507 -0.00095803163 -0.00096218748 -0.00096717459 -0.00097190909 -0.00097521732 -0.00097692059 -0.00097709056 -0.00097478071 -0.00097068906 -0.00096618134 -0.00096132804 -0.00095713348][-0.00095324207 -0.00095201377 -0.00095346768 -0.00095572113 -0.00095870963 -0.00096230139 -0.00096591638 -0.00096856931 -0.00096999103 -0.00097001373 -0.00096831948 -0.00096526515 -0.00096182909 -0.00095809129 -0.00095492176][-0.00095320848 -0.0009516438 -0.00095262594 -0.00095398643 -0.00095581554 -0.00095790176 -0.0009599615 -0.00096148381 -0.00096206227 -0.00096192677 -0.00096083345 -0.00095903128 -0.00095706119 -0.00095473166 -0.00095290446]]...]
INFO - root - 2017-12-09 08:36:25.297474: step 8110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:34m:11s remains)
INFO - root - 2017-12-09 08:36:33.987179: step 8120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:38m:29s remains)
INFO - root - 2017-12-09 08:36:42.718405: step 8130, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.890 sec/batch; 80h:12m:45s remains)
INFO - root - 2017-12-09 08:36:51.186781: step 8140, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.724 sec/batch; 65h:16m:10s remains)
INFO - root - 2017-12-09 08:36:59.774469: step 8150, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:16m:23s remains)
INFO - root - 2017-12-09 08:37:08.426125: step 8160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:20m:46s remains)
INFO - root - 2017-12-09 08:37:16.979100: step 8170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:18m:13s remains)
INFO - root - 2017-12-09 08:37:25.586299: step 8180, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:07m:40s remains)
INFO - root - 2017-12-09 08:37:34.091034: step 8190, loss = 0.88, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 79h:02m:20s remains)
INFO - root - 2017-12-09 08:37:42.816753: step 8200, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 75h:17m:17s remains)
2017-12-09 08:37:43.686421: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51502943 0.52315319 0.51683241 0.49864176 0.4761121 0.45054045 0.43216619 0.42615762 0.43425536 0.4501915 0.46493322 0.47340477 0.47127271 0.45271263 0.41922951][0.57162142 0.58235961 0.57628721 0.55872387 0.53779626 0.51445812 0.50017405 0.50031877 0.51692146 0.54182941 0.56586635 0.58084804 0.58258927 0.56424993 0.52610242][0.61179978 0.62382179 0.61791468 0.602193 0.58431071 0.56570292 0.55902708 0.56854749 0.59629697 0.63177955 0.6638329 0.68455958 0.6882019 0.66871011 0.62518418][0.63027906 0.64347512 0.63886112 0.62611663 0.61222905 0.59966654 0.60100365 0.62124652 0.65979493 0.70446879 0.74368703 0.76891762 0.77457213 0.75383133 0.70613319][0.6309281 0.64720023 0.64511889 0.63504946 0.62521338 0.61835808 0.62649965 0.6551556 0.70292991 0.75627762 0.80184019 0.83111411 0.83875751 0.81780243 0.7671001][0.61712688 0.63576031 0.63600326 0.62910867 0.62208766 0.6198349 0.63351721 0.66882843 0.72287488 0.782588 0.8339811 0.86660963 0.87473524 0.85361749 0.80091232][0.59496838 0.61659962 0.61824846 0.61226392 0.60526687 0.60516733 0.622183 0.66179752 0.72018236 0.78451347 0.84042162 0.8763836 0.88491511 0.86360377 0.80993545][0.56833977 0.59219813 0.59543556 0.58894533 0.58033317 0.57959014 0.59689665 0.6371488 0.69606966 0.76209879 0.82034194 0.85872889 0.86818057 0.84719533 0.79389614][0.533142 0.55829245 0.56193149 0.55333126 0.54197949 0.53872693 0.55335253 0.59001851 0.64494973 0.70950115 0.76836264 0.80898905 0.821033 0.80255103 0.75338209][0.48902717 0.51229089 0.5144434 0.50372767 0.48919424 0.48259038 0.49237254 0.52274531 0.57095814 0.63077182 0.68792242 0.72965616 0.74425566 0.73079187 0.68881488][0.44095114 0.46018621 0.45837137 0.44393975 0.42558524 0.41481242 0.41929027 0.44244269 0.48281407 0.5359596 0.58933914 0.63060963 0.64757866 0.63974994 0.60651839][0.3936289 0.40780455 0.40175635 0.38293809 0.36060423 0.34514639 0.34392947 0.35984933 0.3923189 0.43813536 0.48630995 0.52566087 0.54452759 0.54200941 0.51763934][0.3465741 0.35635421 0.34753957 0.32704252 0.3030906 0.28451917 0.27842954 0.28731731 0.31105047 0.34795412 0.3890115 0.424205 0.44370598 0.44545078 0.42950448][0.29774126 0.3037397 0.2932961 0.27231321 0.24826738 0.22831614 0.21864347 0.22158268 0.23746961 0.26572764 0.29925162 0.329748 0.34886241 0.35458079 0.34621784][0.24852139 0.25243449 0.24234319 0.22355835 0.20201546 0.18331309 0.17257485 0.17189552 0.18179007 0.20202294 0.22758657 0.25206366 0.2688868 0.27629393 0.27299383]]...]
INFO - root - 2017-12-09 08:37:52.332646: step 8210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:29m:45s remains)
INFO - root - 2017-12-09 08:38:00.981446: step 8220, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 74h:48m:05s remains)
INFO - root - 2017-12-09 08:38:09.613440: step 8230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:38m:37s remains)
INFO - root - 2017-12-09 08:38:18.229156: step 8240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:41m:18s remains)
INFO - root - 2017-12-09 08:38:26.657548: step 8250, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:01m:36s remains)
INFO - root - 2017-12-09 08:38:35.163776: step 8260, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.884 sec/batch; 79h:36m:59s remains)
INFO - root - 2017-12-09 08:38:43.973979: step 8270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:06m:49s remains)
INFO - root - 2017-12-09 08:38:52.805373: step 8280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 80h:36m:05s remains)
INFO - root - 2017-12-09 08:39:01.362059: step 8290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:01m:36s remains)
INFO - root - 2017-12-09 08:39:10.196712: step 8300, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 82h:04m:14s remains)
2017-12-09 08:39:11.075179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010809068 -0.0010807086 -0.0010808372 -0.0010809222 -0.0010809742 -0.0010809634 -0.0010809716 -0.0010810288 -0.0010810586 -0.0010810493 -0.0010810279 -0.0010810185 -0.0010809652 -0.0010809069 -0.0010808177][-0.0010805547 -0.0010804611 -0.00108074 -0.0010809009 -0.0010809083 -0.0010807925 -0.0010807061 -0.0010807097 -0.0010807229 -0.0010806727 -0.0010806684 -0.001080664 -0.0010806256 -0.0010805913 -0.0010804865][-0.0010809731 -0.0010810406 -0.0010814997 -0.0010818077 -0.0010818335 -0.0010815512 -0.0010812045 -0.001081034 -0.0010810102 -0.0010809914 -0.0010810094 -0.0010809785 -0.00108093 -0.0010808404 -0.0010806659][-0.0010816664 -0.0010819173 -0.0010826461 -0.0010832486 -0.0010833655 -0.0010828149 -0.0010819853 -0.0010814429 -0.0010812553 -0.0010812465 -0.0010812584 -0.0010812794 -0.0010812136 -0.001081039 -0.0010807941][-0.0010823826 -0.001082938 -0.0010839181 -0.0010848551 -0.0010849744 -0.0010841431 -0.0010826263 -0.0010810196 -0.0010803143 -0.0010806372 -0.0010811415 -0.0010814391 -0.0010814418 -0.0010811344 -0.0010807994][-0.0010831823 -0.001084095 -0.0010854438 -0.0010865554 -0.0010864792 -0.0010849704 -0.0010819206 -0.0010784722 -0.0010770314 -0.0010783785 -0.0010800393 -0.0010812724 -0.0010816011 -0.0010812694 -0.0010808943][-0.0010841175 -0.0010853541 -0.0010870104 -0.0010882149 -0.0010878578 -0.0010856154 -0.0010810901 -0.0010756629 -0.0010740741 -0.0010767656 -0.001079618 -0.0010813475 -0.0010818213 -0.0010814673 -0.0010811053][-0.0010850109 -0.0010868396 -0.0010889228 -0.0010902864 -0.0010902297 -0.0010884227 -0.0010841986 -0.0010795279 -0.001077965 -0.0010794129 -0.0010812462 -0.0010822214 -0.0010823352 -0.001081902 -0.001081538][-0.001085687 -0.0010881711 -0.0010907595 -0.0010923673 -0.0010931817 -0.0010927219 -0.0010903082 -0.0010875872 -0.0010858873 -0.0010852988 -0.0010849989 -0.0010842627 -0.0010833641 -0.0010825848 -0.0010821088][-0.0010857051 -0.0010885147 -0.0010914603 -0.0010936075 -0.0010950064 -0.0010955039 -0.0010947027 -0.0010933031 -0.0010914261 -0.0010897382 -0.0010882204 -0.0010864295 -0.0010847282 -0.001083398 -0.0010825667][-0.0010849135 -0.0010875924 -0.0010907452 -0.0010932983 -0.0010951154 -0.0010962015 -0.0010962401 -0.0010952691 -0.0010931455 -0.0010911067 -0.0010893331 -0.0010873557 -0.0010854424 -0.0010838301 -0.001082637][-0.0010835467 -0.0010857969 -0.0010889277 -0.001091633 -0.001093621 -0.0010950658 -0.0010955887 -0.001094771 -0.0010927918 -0.0010908725 -0.0010890345 -0.0010871029 -0.0010851958 -0.0010835102 -0.0010822183][-0.0010822555 -0.0010838308 -0.0010864639 -0.0010889588 -0.0010910628 -0.001092806 -0.0010935777 -0.0010930534 -0.0010915451 -0.0010897816 -0.0010878799 -0.0010860128 -0.0010842183 -0.0010826949 -0.001081581][-0.0010814496 -0.0010824993 -0.0010844732 -0.001086427 -0.0010882054 -0.001089706 -0.0010904579 -0.0010901922 -0.001089078 -0.0010876186 -0.0010860517 -0.0010845241 -0.0010831804 -0.0010820669 -0.0010812688][-0.0010810212 -0.0010816491 -0.0010829916 -0.0010843698 -0.0010856728 -0.0010866473 -0.0010870375 -0.0010867388 -0.0010858832 -0.0010848717 -0.0010838229 -0.001082901 -0.0010821767 -0.0010816437 -0.0010812159]]...]
INFO - root - 2017-12-09 08:39:19.477239: step 8310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:08m:34s remains)
INFO - root - 2017-12-09 08:39:28.238732: step 8320, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 77h:52m:12s remains)
INFO - root - 2017-12-09 08:39:36.901848: step 8330, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:09m:01s remains)
INFO - root - 2017-12-09 08:39:45.541089: step 8340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 78h:18m:48s remains)
INFO - root - 2017-12-09 08:39:54.057024: step 8350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:02m:53s remains)
INFO - root - 2017-12-09 08:40:02.529279: step 8360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:32m:43s remains)
INFO - root - 2017-12-09 08:40:11.131355: step 8370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:54m:01s remains)
INFO - root - 2017-12-09 08:40:19.714891: step 8380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:17m:45s remains)
INFO - root - 2017-12-09 08:40:28.202637: step 8390, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 75h:45m:44s remains)
INFO - root - 2017-12-09 08:40:36.670264: step 8400, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 75h:30m:44s remains)
2017-12-09 08:40:37.580601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011007307 -0.0010996481 -0.001099527 -0.0010993956 -0.0010991448 -0.0010986747 -0.0010983752 -0.0010985683 -0.001099124 -0.0010997733 -0.0011002186 -0.0011006217 -0.0011008506 -0.0011007068 -0.0011003197][-0.0010992326 -0.0010981943 -0.0010984252 -0.0010987768 -0.0010989789 -0.001098885 -0.0010985128 -0.0010982731 -0.0010982081 -0.001098211 -0.0010982482 -0.0010983752 -0.0010984206 -0.0010982852 -0.0010979772][-0.0010985333 -0.0010977385 -0.00109839 -0.0010994129 -0.0011004178 -0.0011009893 -0.0011007939 -0.001100095 -0.0010992 -0.001098429 -0.00109783 -0.0010973765 -0.0010968141 -0.0010964959 -0.0010962952][-0.0010980614 -0.0010974145 -0.0010985415 -0.0011004384 -0.0011027737 -0.0011045167 -0.0011051717 -0.001104379 -0.0011025695 -0.0011006742 -0.0010988598 -0.0010973812 -0.0010960845 -0.0010953167 -0.0010950805][-0.0010977614 -0.0010971228 -0.0010987902 -0.0011016254 -0.0011055307 -0.0011088351 -0.0011108766 -0.0011104663 -0.0011079914 -0.0011043889 -0.001100832 -0.0010981103 -0.0010960845 -0.0010948734 -0.0010943415][-0.0010973732 -0.0010966903 -0.00109885 -0.0011027171 -0.0011082104 -0.0011132772 -0.0011168276 -0.0011174259 -0.0011146396 -0.0011093168 -0.0011036112 -0.00109928 -0.0010964938 -0.0010948251 -0.0010939767][-0.0010968349 -0.0010961081 -0.0010986136 -0.0011034736 -0.0011104487 -0.0011170728 -0.0011222244 -0.0011239116 -0.001121442 -0.0011152831 -0.0011075478 -0.0011013502 -0.0010974229 -0.0010951972 -0.0010939497][-0.0010962358 -0.0010953185 -0.0010980385 -0.0011035077 -0.0011116314 -0.001119506 -0.0011261244 -0.0011291421 -0.0011277614 -0.001121812 -0.0011125719 -0.0011043006 -0.0010987106 -0.0010958294 -0.0010942842][-0.0010956131 -0.0010945909 -0.0010973121 -0.0011029511 -0.0011113874 -0.0011197879 -0.0011270513 -0.0011309525 -0.0011309645 -0.0011260443 -0.001116698 -0.0011072914 -0.0011004236 -0.0010968092 -0.0010949797][-0.0010952151 -0.0010941371 -0.0010966599 -0.0011020818 -0.0011100159 -0.0011179721 -0.001124721 -0.0011283578 -0.00112903 -0.0011250291 -0.001116793 -0.0011079563 -0.0011012258 -0.0010975508 -0.0010955803][-0.0010948387 -0.0010938443 -0.001096044 -0.0011008476 -0.001107769 -0.0011148236 -0.0011205728 -0.0011235916 -0.0011242337 -0.0011210076 -0.0011143693 -0.0011070323 -0.0011013475 -0.0010979993 -0.0010962316][-0.001094587 -0.0010935389 -0.0010952926 -0.0010990135 -0.0011044097 -0.0011101046 -0.0011148519 -0.0011174164 -0.0011180432 -0.0011156857 -0.001110959 -0.0011056541 -0.0011012177 -0.0010984787 -0.0010971035][-0.0010944048 -0.0010934093 -0.0010947631 -0.001097296 -0.0011010024 -0.0011051976 -0.0011088334 -0.0011109515 -0.0011115785 -0.0011101415 -0.0011071925 -0.0011037432 -0.0011007773 -0.0010989225 -0.0010983838][-0.0010943322 -0.0010933413 -0.0010943394 -0.001095942 -0.0010982006 -0.0011008262 -0.0011031753 -0.0011046582 -0.0011051695 -0.0011044667 -0.0011030287 -0.0011012392 -0.0010997921 -0.0010992055 -0.0010998111][-0.0010944898 -0.0010932504 -0.0010939742 -0.0010948656 -0.0010960749 -0.0010974584 -0.0010987155 -0.0010995365 -0.0010998573 -0.0010996916 -0.0010991512 -0.0010985214 -0.0010982605 -0.001099047 -0.0011011537]]...]
INFO - root - 2017-12-09 08:40:46.103674: step 8410, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 69h:52m:02s remains)
INFO - root - 2017-12-09 08:40:54.807907: step 8420, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 77h:45m:57s remains)
INFO - root - 2017-12-09 08:41:03.174964: step 8430, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 73h:40m:53s remains)
INFO - root - 2017-12-09 08:41:11.751183: step 8440, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 76h:59m:11s remains)
INFO - root - 2017-12-09 08:41:20.207403: step 8450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:53m:36s remains)
INFO - root - 2017-12-09 08:41:28.844769: step 8460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:07m:04s remains)
INFO - root - 2017-12-09 08:41:37.311642: step 8470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:08m:26s remains)
INFO - root - 2017-12-09 08:41:45.856565: step 8480, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 73h:35m:38s remains)
INFO - root - 2017-12-09 08:41:54.305945: step 8490, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 77h:05m:39s remains)
INFO - root - 2017-12-09 08:42:02.985499: step 8500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:56m:38s remains)
2017-12-09 08:42:03.799742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012194079 -0.001219974 -0.0012203037 -0.0012198748 -0.0012191191 -0.0012184198 -0.0012180438 -0.0012180856 -0.0012183421 -0.001218543 -0.0012187785 -0.0012186551 -0.0012182177 -0.0012175537 -0.0012167818][-0.0012183406 -0.0012189068 -0.0012191884 -0.0012188801 -0.0012182444 -0.0012175317 -0.0012169096 -0.0012166607 -0.0012166154 -0.0012165299 -0.0012165291 -0.001216318 -0.0012160849 -0.0012156296 -0.0012150662][-0.0012182143 -0.0012188363 -0.0012192306 -0.0012190477 -0.0012184188 -0.0012175578 -0.0012167083 -0.0012162136 -0.0012159327 -0.0012154193 -0.0012149857 -0.0012145377 -0.0012142218 -0.0012137973 -0.0012133163][-0.001218622 -0.0012189563 -0.0012192121 -0.0012189065 -0.0012181747 -0.0012170267 -0.0012160128 -0.0012151999 -0.001214738 -0.0012140707 -0.0012135841 -0.0012130772 -0.0012125854 -0.0012120015 -0.0012114227][-0.0012183619 -0.0012170268 -0.0012159686 -0.0012148876 -0.0012137303 -0.0012127338 -0.0012121536 -0.0012118451 -0.001212177 -0.0012123145 -0.0012124572 -0.0012120893 -0.0012116492 -0.0012110036 -0.0012104978][-0.0012163459 -0.0012119734 -0.0012087849 -0.0012065126 -0.0012049088 -0.001204422 -0.0012045754 -0.0012054663 -0.00120708 -0.0012086314 -0.0012103542 -0.0012110929 -0.0012108975 -0.0012102204 -0.0012098775][-0.001212569 -0.001205167 -0.0012000359 -0.0011967592 -0.0011953393 -0.0011967163 -0.0011983794 -0.001200503 -0.0012028805 -0.0012051276 -0.0012078801 -0.0012096685 -0.0012098103 -0.0012093581 -0.0012092738][-0.0012094448 -0.0012001435 -0.0011940079 -0.0011906293 -0.0011899435 -0.0011928645 -0.0011967714 -0.0012004911 -0.0012031803 -0.001205446 -0.0012080071 -0.0012095786 -0.0012098679 -0.0012094794 -0.0012093622][-0.0012083784 -0.0011985421 -0.0011921346 -0.0011892227 -0.0011891391 -0.0011930469 -0.0011982198 -0.0012032751 -0.0012068768 -0.0012089266 -0.0012106207 -0.0012109722 -0.0012106481 -0.0012099297 -0.00120956][-0.0012096183 -0.0012006232 -0.0011946672 -0.0011918955 -0.001191868 -0.0011953653 -0.0012008825 -0.0012067878 -0.0012112204 -0.0012135337 -0.0012144033 -0.0012136076 -0.001212357 -0.0012111535 -0.0012104071][-0.0012125409 -0.0012057195 -0.0012011726 -0.0011992821 -0.0011993877 -0.0012021577 -0.0012068206 -0.0012117777 -0.001215311 -0.0012167623 -0.0012167782 -0.0012155608 -0.001213952 -0.001212638 -0.0012119684][-0.0012164278 -0.0012119659 -0.0012091493 -0.0012079047 -0.0012083997 -0.0012106537 -0.001213718 -0.00121662 -0.001218404 -0.0012185711 -0.0012177167 -0.001216066 -0.0012143877 -0.001213227 -0.0012126897][-0.0012201917 -0.0012174756 -0.0012160324 -0.0012153346 -0.0012154819 -0.0012167424 -0.0012180626 -0.0012188735 -0.0012188747 -0.0012177994 -0.0012163046 -0.0012147179 -0.0012135378 -0.0012128364 -0.0012126239][-0.0012221287 -0.0012209788 -0.0012203703 -0.0012198503 -0.0012194021 -0.0012190406 -0.0012186914 -0.0012178827 -0.0012168138 -0.0012152803 -0.0012138335 -0.0012126927 -0.0012119981 -0.0012117766 -0.0012118439][-0.0012206255 -0.0012203614 -0.00122 -0.0012196695 -0.001218838 -0.0012177328 -0.0012164182 -0.0012151729 -0.0012140026 -0.0012126217 -0.0012116693 -0.0012111783 -0.0012110798 -0.0012112648 -0.0012115624]]...]
INFO - root - 2017-12-09 08:42:12.456058: step 8510, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 79h:43m:32s remains)
INFO - root - 2017-12-09 08:42:20.939429: step 8520, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:47m:14s remains)
INFO - root - 2017-12-09 08:42:29.572792: step 8530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:40m:38s remains)
INFO - root - 2017-12-09 08:42:38.301612: step 8540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 79h:34m:22s remains)
INFO - root - 2017-12-09 08:42:46.926981: step 8550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:00m:53s remains)
INFO - root - 2017-12-09 08:42:55.683099: step 8560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:29m:48s remains)
INFO - root - 2017-12-09 08:43:04.375282: step 8570, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 77h:58m:08s remains)
INFO - root - 2017-12-09 08:43:13.075926: step 8580, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 79h:00m:30s remains)
INFO - root - 2017-12-09 08:43:21.695115: step 8590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:52m:56s remains)
INFO - root - 2017-12-09 08:43:30.360603: step 8600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:43m:29s remains)
2017-12-09 08:43:31.285786: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077629365 0.075686142 0.073374636 0.070729807 0.068459935 0.066830978 0.065423466 0.065597661 0.066147804 0.067342833 0.068632662 0.06908334 0.069585949 0.069071472 0.067601323][0.11549082 0.1149523 0.11316978 0.11061081 0.10822062 0.10650635 0.10496406 0.10453485 0.10469843 0.10578372 0.1069958 0.10794857 0.10886921 0.10916959 0.10802239][0.15320231 0.1537355 0.15211861 0.15003124 0.14796601 0.14630947 0.14488952 0.14485328 0.14564674 0.14726296 0.1491548 0.15092483 0.15250276 0.153133 0.15159191][0.18091154 0.18212628 0.18060316 0.178746 0.17696445 0.17543837 0.17484167 0.17563115 0.1774523 0.18031123 0.18375948 0.18692568 0.1891142 0.19018103 0.18856174][0.19129623 0.1932978 0.19209328 0.19066246 0.18963896 0.18873967 0.18894161 0.1905183 0.1934052 0.19795923 0.20281267 0.20737889 0.21103545 0.2133231 0.21249923][0.18575762 0.18791176 0.18635473 0.18578959 0.18589427 0.186167 0.18755053 0.19050375 0.19520983 0.2012824 0.20758063 0.21427618 0.21963577 0.22324948 0.22371447][0.1651658 0.16699825 0.16536295 0.16496421 0.16551143 0.16715597 0.17044817 0.17548604 0.18223076 0.19100267 0.19997934 0.20909631 0.21614675 0.2210805 0.22264753][0.13411571 0.13432491 0.13162604 0.1311892 0.13227977 0.13492608 0.13993502 0.14770421 0.15819892 0.17063586 0.18306307 0.19549467 0.20493822 0.21144585 0.21339163][0.095193006 0.094449282 0.0914638 0.090631165 0.091384165 0.094975963 0.10154302 0.11110208 0.12395377 0.13977458 0.15615806 0.17146875 0.18301417 0.19126631 0.19427469][0.057915609 0.056085113 0.053419568 0.052988011 0.053938679 0.056932885 0.062814012 0.0724497 0.085499808 0.10199623 0.11935676 0.13620424 0.1496755 0.1593264 0.16364597][0.0274572 0.026066585 0.02473864 0.024773741 0.025625113 0.027937302 0.032139868 0.039202012 0.049279258 0.062990136 0.078302771 0.093485206 0.10644283 0.11662643 0.12248343][0.0081910454 0.0075460449 0.0072952807 0.0079226028 0.0088908086 0.010336201 0.012804081 0.017144317 0.023554178 0.032360081 0.042770095 0.0538732 0.064178586 0.072871409 0.078883044][0.00039168284 4.021381e-05 4.0508923e-05 0.00045367645 0.0010490979 0.0018574507 0.0031625906 0.0053197267 0.0086691054 0.013331577 0.019087618 0.025581924 0.032064959 0.038184684 0.043101493][-0.0010551418 -0.0012053746 -0.0012243894 -0.0011866094 -0.0011412784 -0.00098731217 -0.00066625176 -2.8691837e-05 0.0011150178 0.0029341311 0.0054096235 0.0082810139 0.01141307 0.014684448 0.017737959][-0.0012369637 -0.0012545744 -0.0012555687 -0.0012504513 -0.0012451134 -0.0012417499 -0.0012340384 -0.0011913485 -0.0010290232 -0.00067084312 -9.43155e-05 0.00067113154 0.0016330064 0.0027798864 0.0040273042]]...]
INFO - root - 2017-12-09 08:43:39.936272: step 8610, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 79h:36m:25s remains)
INFO - root - 2017-12-09 08:43:48.391547: step 8620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:58m:44s remains)
INFO - root - 2017-12-09 08:43:57.004521: step 8630, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:44m:56s remains)
INFO - root - 2017-12-09 08:44:05.588510: step 8640, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:08m:24s remains)
INFO - root - 2017-12-09 08:44:14.105654: step 8650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:34m:18s remains)
INFO - root - 2017-12-09 08:44:22.745077: step 8660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:35m:40s remains)
INFO - root - 2017-12-09 08:44:31.466666: step 8670, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 79h:39m:16s remains)
INFO - root - 2017-12-09 08:44:40.190212: step 8680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:23m:54s remains)
INFO - root - 2017-12-09 08:44:48.812532: step 8690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:17m:25s remains)
INFO - root - 2017-12-09 08:44:57.484358: step 8700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:48m:35s remains)
2017-12-09 08:44:58.332384: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0024677392 0.0085002454 0.017578449 0.028854981 0.042198732 0.055378761 0.065056331 0.068883382 0.066410437 0.057655983 0.043726895 0.028263407 0.015137238 0.0061323252 0.0012198125][0.0044745123 0.011842702 0.024743887 0.042763341 0.065537825 0.08955624 0.10897065 0.11822483 0.11553124 0.10076915 0.076963916 0.050234925 0.027269052 0.011522156 0.0028999161][0.0090302583 0.018532 0.036766402 0.06417384 0.099409193 0.13703881 0.16809572 0.18412395 0.18121143 0.15914263 0.12337516 0.082718536 0.046963569 0.021392567 0.0066842921][0.016335862 0.030487202 0.057995643 0.099913664 0.15306628 0.20869181 0.25349796 0.27513483 0.2684606 0.23424524 0.18157074 0.12292113 0.071481958 0.034291875 0.012236626][0.025169972 0.04571972 0.084545426 0.14343828 0.21671578 0.29205865 0.35141677 0.37892628 0.36803287 0.32028964 0.24891798 0.1702228 0.10112116 0.050390307 0.019438375][0.034226522 0.06186901 0.11254068 0.18749417 0.27914041 0.37149012 0.4425512 0.47396186 0.45845091 0.39898428 0.31132278 0.2150934 0.13031086 0.067107379 0.027366979][0.041247878 0.075249821 0.13500954 0.22170709 0.32656196 0.43050236 0.5090574 0.5425812 0.52345121 0.45572603 0.35660043 0.24822399 0.1524172 0.080345675 0.034071531][0.042092353 0.080095269 0.14437038 0.23625076 0.34694514 0.45600113 0.53789604 0.57238394 0.5516724 0.48059681 0.37644997 0.26285663 0.1624099 0.086722143 0.037647083][0.035896108 0.073099941 0.1359155 0.22545291 0.33366787 0.44072351 0.52171147 0.55662155 0.53726 0.46853766 0.36684188 0.25597331 0.15796679 0.084416784 0.036880285][0.026216153 0.058355987 0.1137457 0.19351377 0.29108405 0.38874954 0.46380258 0.49751931 0.48147649 0.42044502 0.32864809 0.22840884 0.13996969 0.07405033 0.031973362][0.019087994 0.042994261 0.086109467 0.14996582 0.22953127 0.31038576 0.37359303 0.40289083 0.39050078 0.34055611 0.26481745 0.18241139 0.11017992 0.057040691 0.023821691][0.013947965 0.03017848 0.059853584 0.10531799 0.16324112 0.22316332 0.27074859 0.29320204 0.28405428 0.24636337 0.18933313 0.12808557 0.075307205 0.03734735 0.014535138][0.0090109063 0.018937353 0.037272412 0.066149414 0.10344493 0.14260134 0.17401658 0.18869278 0.1819099 0.15592711 0.11731104 0.076853506 0.04296298 0.01955465 0.006446152][0.0040380424 0.0094183767 0.019233003 0.035061255 0.05612915 0.078626677 0.09672232 0.10504379 0.10061462 0.084782936 0.06181486 0.038609169 0.019927319 0.00776174 0.0015856954][0.00036685669 0.002557419 0.0068583628 0.01408584 0.023951108 0.034611866 0.043165881 0.047003265 0.044572096 0.036693152 0.025619105 0.014785679 0.0065124789 0.0015768912 -0.00061352912]]...]
INFO - root - 2017-12-09 08:45:07.190877: step 8710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:26m:56s remains)
INFO - root - 2017-12-09 08:45:15.693629: step 8720, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 80h:53m:54s remains)
INFO - root - 2017-12-09 08:45:24.343494: step 8730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:38m:54s remains)
INFO - root - 2017-12-09 08:45:33.031445: step 8740, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:40m:35s remains)
INFO - root - 2017-12-09 08:45:41.566351: step 8750, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 79h:25m:34s remains)
INFO - root - 2017-12-09 08:45:50.150204: step 8760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 77h:45m:33s remains)
INFO - root - 2017-12-09 08:45:58.695961: step 8770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:15m:43s remains)
INFO - root - 2017-12-09 08:46:07.235918: step 8780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:04m:14s remains)
INFO - root - 2017-12-09 08:46:15.858060: step 8790, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 78h:40m:38s remains)
INFO - root - 2017-12-09 08:46:24.517381: step 8800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:05m:55s remains)
2017-12-09 08:46:25.471765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016193509 -0.0016170305 -0.0016145051 -0.0016115066 -0.0016091563 -0.001606899 -0.0016049821 -0.0016043215 -0.0016055073 -0.0016079727 -0.0016099768 -0.0016112764 -0.0016121949 -0.0016128744 -0.0016131605][-0.0016174112 -0.0016151287 -0.0016127406 -0.001610308 -0.0016085076 -0.0016067513 -0.0016050626 -0.001604145 -0.0016046336 -0.0016063891 -0.0016078897 -0.0016087379 -0.0016094443 -0.001610128 -0.0016106246][-0.0016159608 -0.001613979 -0.0016120545 -0.0016100944 -0.0016086099 -0.0016074241 -0.0016062395 -0.0016052292 -0.0016053175 -0.0016064758 -0.0016074174 -0.0016076853 -0.0016081289 -0.001608612 -0.0016091439][-0.0016149498 -0.0016132233 -0.0016117441 -0.0016096802 -0.0016071172 -0.0016022422 -0.0015931968 -0.0015831015 -0.0015820077 -0.0015915709 -0.0016035574 -0.0016076938 -0.0016080937 -0.0016080282 -0.0016082129][-0.0016154344 -0.0016138 -0.0016115643 -0.0016058837 -0.0015917905 -0.0015594692 -0.0015092755 -0.0014602178 -0.0014564183 -0.0015046046 -0.0015687927 -0.0016016026 -0.0016095978 -0.0016091311 -0.0016086339][-0.0016161982 -0.0016143442 -0.0016082372 -0.0015855104 -0.0015249967 -0.0014095434 -0.0012597817 -0.0011434618 -0.0011609436 -0.0013083396 -0.0014858628 -0.0015838227 -0.0016110711 -0.0016104934 -0.0016095107][-0.0016171031 -0.0016131045 -0.0015935238 -0.0015245955 -0.0013646253 -0.0011023977 -0.00081647368 -0.00064949493 -0.00074644643 -0.0010522497 -0.0013803665 -0.0015607635 -0.0016116031 -0.0016113149 -0.0016102367][-0.0016170971 -0.0016095069 -0.0015687409 -0.001433766 -0.0011455335 -0.00072460424 -0.00033169938 -0.00018138869 -0.00041313912 -0.00087927456 -0.0013203402 -0.0015495773 -0.0016112671 -0.0016118257 -0.0016113112][-0.001616996 -0.0016062625 -0.0015468922 -0.00136191 -0.00099000335 -0.00049030851 -8.6093438e-05 -1.2539909e-05 -0.00035896327 -0.00089474011 -0.001343145 -0.0015575436 -0.0016107636 -0.0016120803 -0.0016120375][-0.0016177152 -0.0016080652 -0.0015494967 -0.0013695636 -0.0010165286 -0.00056491513 -0.00024405739 -0.00025105546 -0.000607806 -0.0010748595 -0.0014247323 -0.0015772292 -0.0016105509 -0.0016121123 -0.0016124251][-0.0016177725 -0.0016122479 -0.0015716058 -0.0014457459 -0.0012005772 -0.00089173944 -0.0006904889 -0.00072993536 -0.00099348323 -0.0013006935 -0.0015108425 -0.0015946343 -0.0016107437 -0.0016121288 -0.0016126335][-0.0016168545 -0.0016161044 -0.0015969842 -0.0015328322 -0.0014078896 -0.0012512774 -0.0011538849 -0.0011832812 -0.0013219644 -0.001472789 -0.0015686061 -0.0016046721 -0.0016111758 -0.0016123329 -0.001612768][-0.001615437 -0.001618097 -0.0016133746 -0.0015902889 -0.0015443148 -0.00148626 -0.0014497673 -0.001459819 -0.0015107734 -0.0015650041 -0.0015976409 -0.0016093712 -0.0016116224 -0.0016124487 -0.0016128293][-0.0016136196 -0.0016173334 -0.0016192 -0.0016143555 -0.001603028 -0.0015876126 -0.0015769304 -0.0015770553 -0.0015879964 -0.001601084 -0.0016089929 -0.0016118097 -0.0016121956 -0.0016126275 -0.0016128819][-0.0016124503 -0.0016157735 -0.0016191567 -0.0016201986 -0.0016192613 -0.0016164948 -0.001613646 -0.001611499 -0.0016112676 -0.0016126287 -0.0016132683 -0.0016131655 -0.0016128061 -0.0016128258 -0.0016129162]]...]
INFO - root - 2017-12-09 08:46:34.064680: step 8810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:11m:54s remains)
INFO - root - 2017-12-09 08:46:42.567532: step 8820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 77h:33m:39s remains)
INFO - root - 2017-12-09 08:46:51.354866: step 8830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 78h:03m:49s remains)
INFO - root - 2017-12-09 08:47:00.162882: step 8840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:46m:11s remains)
INFO - root - 2017-12-09 08:47:08.668642: step 8850, loss = 0.91, batch loss = 0.70 (8.9 examples/sec; 0.896 sec/batch; 80h:33m:45s remains)
INFO - root - 2017-12-09 08:47:17.357464: step 8860, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 80h:12m:19s remains)
INFO - root - 2017-12-09 08:47:26.077007: step 8870, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 80h:31m:46s remains)
INFO - root - 2017-12-09 08:47:34.810255: step 8880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:42m:06s remains)
INFO - root - 2017-12-09 08:47:43.357266: step 8890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 76h:14m:44s remains)
INFO - root - 2017-12-09 08:47:52.061127: step 8900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 77h:23m:37s remains)
2017-12-09 08:47:52.978470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017101857 -0.0017066931 -0.0017014823 -0.0016959131 -0.0016907649 -0.0016859954 -0.0016820113 -0.001680381 -0.0016818988 -0.0016858275 -0.0016906798 -0.0016944767 -0.001697377 -0.0016987853 -0.0016980547][-0.001711159 -0.0017081649 -0.0017033985 -0.0016979267 -0.0016927466 -0.0016875791 -0.0016834642 -0.001681849 -0.0016832035 -0.0016872422 -0.001692371 -0.0016970076 -0.0017003457 -0.0017022942 -0.0017021337][-0.0017113286 -0.0017088194 -0.0017046756 -0.0016995808 -0.0016945263 -0.0016899706 -0.001686852 -0.0016857685 -0.0016872772 -0.0016909944 -0.0016957202 -0.001699971 -0.0017030492 -0.0017048529 -0.0017043917][-0.0017077506 -0.0017057277 -0.0017027344 -0.0016987132 -0.0016948353 -0.0016917546 -0.0016898336 -0.0016896457 -0.0016908629 -0.0016937399 -0.0016973828 -0.0017004772 -0.0017023468 -0.0017029424 -0.0017015224][-0.0016998642 -0.0016985094 -0.0016966017 -0.0016941283 -0.0016919728 -0.001690927 -0.0016906935 -0.0016914601 -0.0016926285 -0.0016948405 -0.0016971193 -0.0016983127 -0.0016984966 -0.0016972863 -0.0016947163][-0.0016908246 -0.0016901498 -0.0016895178 -0.00168858 -0.0016882153 -0.0016888512 -0.001690044 -0.0016914267 -0.0016926182 -0.0016938623 -0.0016945349 -0.0016935536 -0.0016916586 -0.0016891538 -0.0016862042][-0.0016826915 -0.001682178 -0.0016823271 -0.0016826121 -0.001683486 -0.0016851347 -0.0016870097 -0.001688638 -0.0016901057 -0.0016908289 -0.0016900027 -0.0016876378 -0.0016845894 -0.0016816831 -0.0016788712][-0.001676406 -0.0016756472 -0.0016761286 -0.0016768856 -0.0016781978 -0.0016797077 -0.0016811687 -0.0016825891 -0.0016843895 -0.0016849551 -0.0016836677 -0.0016810867 -0.0016782673 -0.0016755988 -0.0016730583][-0.0016717367 -0.0016704267 -0.0016709109 -0.0016714764 -0.0016726557 -0.0016739607 -0.0016750124 -0.0016763881 -0.0016780469 -0.0016785188 -0.0016773797 -0.0016752811 -0.0016730078 -0.0016707048 -0.0016685603][-0.0016682612 -0.0016662931 -0.0016663891 -0.0016666992 -0.0016675933 -0.0016686816 -0.0016698336 -0.0016709216 -0.0016719843 -0.0016725869 -0.0016717274 -0.0016700217 -0.0016683358 -0.0016667302 -0.001665166][-0.0016656495 -0.0016632411 -0.0016632347 -0.0016633301 -0.0016639585 -0.0016648336 -0.0016656991 -0.0016664444 -0.0016671832 -0.0016676672 -0.0016670052 -0.0016657775 -0.0016646661 -0.0016635386 -0.0016625378][-0.0016636067 -0.0016613037 -0.0016611131 -0.0016609427 -0.0016612123 -0.0016616667 -0.0016620635 -0.0016624834 -0.0016629851 -0.0016633179 -0.0016628819 -0.0016622321 -0.0016616695 -0.0016610856 -0.0016606334][-0.0016622363 -0.0016599275 -0.0016597566 -0.0016594585 -0.0016594299 -0.0016595286 -0.0016596204 -0.0016598105 -0.0016601731 -0.0016604161 -0.0016602327 -0.0016599813 -0.001659806 -0.0016595804 -0.0016594144][-0.0016614691 -0.0016589345 -0.0016589153 -0.0016586082 -0.001658515 -0.0016585591 -0.0016585742 -0.0016586643 -0.0016588508 -0.0016590456 -0.0016590687 -0.0016589625 -0.0016589054 -0.0016588147 -0.0016587204][-0.0016606289 -0.0016580287 -0.0016581272 -0.0016578787 -0.0016577471 -0.0016577715 -0.0016577863 -0.0016577647 -0.0016579075 -0.0016581026 -0.0016581592 -0.0016581573 -0.0016581637 -0.0016581104 -0.001658036]]...]
INFO - root - 2017-12-09 08:48:01.540288: step 8910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:48m:51s remains)
INFO - root - 2017-12-09 08:48:09.922680: step 8920, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:29m:52s remains)
INFO - root - 2017-12-09 08:48:18.383603: step 8930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:03m:59s remains)
INFO - root - 2017-12-09 08:48:26.873394: step 8940, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 77h:21m:50s remains)
INFO - root - 2017-12-09 08:48:35.281478: step 8950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 75h:08m:37s remains)
INFO - root - 2017-12-09 08:48:43.955125: step 8960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:26m:50s remains)
INFO - root - 2017-12-09 08:48:52.662712: step 8970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:47m:41s remains)
INFO - root - 2017-12-09 08:49:01.262818: step 8980, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 77h:30m:02s remains)
INFO - root - 2017-12-09 08:49:09.715960: step 8990, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 77h:18m:00s remains)
INFO - root - 2017-12-09 08:49:18.202969: step 9000, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 74h:34m:03s remains)
2017-12-09 08:49:19.113203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2415617 0.24186642 0.24624774 0.250712 0.25352848 0.25289774 0.24798433 0.24083157 0.23458089 0.23378766 0.23961487 0.25100854 0.26614976 0.28048298 0.29108575][0.23333025 0.2364428 0.2436713 0.25166869 0.25793177 0.25931594 0.25418907 0.24578992 0.2373905 0.23342659 0.23629501 0.24551277 0.26011056 0.275997 0.28874251][0.22598194 0.23187047 0.24122289 0.25173849 0.2602551 0.2626211 0.2581374 0.24967903 0.23992565 0.23399739 0.23418564 0.24119967 0.25409305 0.26952359 0.28341666][0.22292268 0.23433751 0.24889052 0.26326784 0.27416679 0.27669004 0.272155 0.26269495 0.25069821 0.24143016 0.23799075 0.24182925 0.25130802 0.26438156 0.27737552][0.22472218 0.24141808 0.26017287 0.27760449 0.29077208 0.29432082 0.28950614 0.27835208 0.26348647 0.25108531 0.24332158 0.24265254 0.2482802 0.25852668 0.27017874][0.235043 0.25585905 0.2767469 0.29571915 0.30946314 0.3133854 0.30852798 0.29677016 0.28061172 0.26530012 0.25337958 0.24741456 0.24744806 0.25211042 0.260183][0.2526311 0.27524018 0.29613364 0.315396 0.32937923 0.33335215 0.32805154 0.31599173 0.29918611 0.28082532 0.26465926 0.25369653 0.24812175 0.24643812 0.24920106][0.27204883 0.29779229 0.31914303 0.3371675 0.34892097 0.35156333 0.34604976 0.33411026 0.31690741 0.29718345 0.27839094 0.26231968 0.25091115 0.24254405 0.23932017][0.29026809 0.31747755 0.33839214 0.35656053 0.36839 0.37141255 0.36570886 0.35360402 0.33568418 0.31510103 0.29336709 0.27350813 0.25712365 0.24305619 0.23429231][0.31068319 0.33611694 0.3538411 0.36938906 0.37990579 0.38371268 0.37943557 0.36887491 0.35146439 0.33044967 0.3070761 0.28451586 0.26488534 0.24693465 0.23409872][0.32769069 0.35085514 0.36445713 0.37608337 0.38363677 0.38683176 0.38408279 0.37649029 0.36240697 0.34401587 0.32240286 0.30000997 0.27876583 0.25815997 0.24190563][0.33649671 0.35761565 0.36815476 0.37680247 0.38217324 0.38356265 0.38042179 0.37515897 0.36453596 0.34993368 0.3321121 0.31280679 0.29382262 0.27364472 0.25653842][0.33457151 0.35514528 0.36395204 0.37060648 0.37445047 0.37552634 0.3729814 0.36849463 0.36020371 0.35053104 0.33781165 0.32277584 0.30764055 0.290813 0.27580073][0.32343361 0.34155139 0.34826541 0.35380509 0.35745251 0.3592065 0.35855162 0.35647896 0.35147116 0.34565657 0.33714244 0.3268126 0.31593543 0.30375081 0.29228765][0.29937628 0.31587812 0.3219786 0.32718378 0.33130032 0.33440328 0.33617502 0.33718762 0.33623415 0.33418667 0.33037996 0.32466078 0.31845835 0.31094247 0.3034822]]...]
INFO - root - 2017-12-09 08:49:27.687430: step 9010, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:01m:42s remains)
INFO - root - 2017-12-09 08:49:35.860271: step 9020, loss = 0.89, batch loss = 0.68 (12.3 examples/sec; 0.648 sec/batch; 58h:13m:35s remains)
INFO - root - 2017-12-09 08:49:44.438527: step 9030, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:47m:02s remains)
INFO - root - 2017-12-09 08:49:52.917847: step 9040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:22m:55s remains)
INFO - root - 2017-12-09 08:50:01.299078: step 9050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 75h:26m:03s remains)
INFO - root - 2017-12-09 08:50:09.973241: step 9060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:56m:13s remains)
INFO - root - 2017-12-09 08:50:18.669108: step 9070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:16m:17s remains)
INFO - root - 2017-12-09 08:50:27.386116: step 9080, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:03m:36s remains)
INFO - root - 2017-12-09 08:50:36.026789: step 9090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:22m:48s remains)
INFO - root - 2017-12-09 08:50:44.808949: step 9100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:45m:02s remains)
2017-12-09 08:50:45.689581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018547481 -0.001859515 -0.0018675494 -0.0018740838 -0.0018782988 -0.0018793087 -0.0018764089 -0.001870352 -0.001863201 -0.0018575235 -0.0018529962 -0.0018490204 -0.0018465781 -0.0018449384 -0.0018447185][-0.0018502331 -0.0018536409 -0.0018613082 -0.001868381 -0.0018737216 -0.0018766628 -0.0018762231 -0.0018725968 -0.0018676096 -0.0018638662 -0.0018604936 -0.0018573323 -0.0018556475 -0.0018541947 -0.0018537119][-0.0018474722 -0.0018491326 -0.0018551855 -0.0018618971 -0.0018678885 -0.0018725334 -0.0018749496 -0.0018746628 -0.0018726665 -0.0018708871 -0.0018689524 -0.0018670002 -0.0018663278 -0.0018657717 -0.0018657502][-0.001845073 -0.0018447828 -0.0018488332 -0.0018544262 -0.0018600214 -0.0018654639 -0.0018699308 -0.0018727494 -0.0018737672 -0.0018742062 -0.0018738462 -0.0018731355 -0.0018730314 -0.0018729483 -0.001873897][-0.0018440359 -0.0018419805 -0.0018437196 -0.0018474472 -0.0018517962 -0.0018569238 -0.0018622281 -0.0018669184 -0.0018703068 -0.0018729119 -0.0018745639 -0.0018754137 -0.0018763258 -0.001877213 -0.0018788805][-0.0018434876 -0.001840177 -0.0018400166 -0.0018415401 -0.0018439214 -0.0018476518 -0.001852445 -0.0018575842 -0.0018622511 -0.001866712 -0.0018700315 -0.0018723159 -0.001874208 -0.0018757093 -0.0018781449][-0.0018432381 -0.0018391128 -0.0018379135 -0.0018379707 -0.0018385213 -0.0018404602 -0.0018436497 -0.0018477716 -0.0018526259 -0.0018578711 -0.0018622209 -0.0018654313 -0.0018682851 -0.0018708267 -0.0018738431][-0.0018436369 -0.0018392565 -0.0018375822 -0.001836617 -0.0018362103 -0.0018369654 -0.0018387069 -0.0018416393 -0.0018461691 -0.0018513449 -0.0018555243 -0.0018586392 -0.0018614004 -0.0018638433 -0.0018668044][-0.001844048 -0.0018398017 -0.0018386398 -0.0018376437 -0.0018370136 -0.0018374182 -0.0018385146 -0.0018409163 -0.0018447787 -0.0018489703 -0.0018520625 -0.0018543644 -0.0018566119 -0.0018586019 -0.0018610272][-0.0018446827 -0.0018410354 -0.0018407125 -0.0018404082 -0.0018404243 -0.0018413095 -0.0018431146 -0.0018463085 -0.001849803 -0.0018525665 -0.0018539474 -0.0018548192 -0.0018555806 -0.0018559967 -0.0018573274][-0.001845893 -0.001842923 -0.0018436192 -0.001844582 -0.0018460705 -0.0018479547 -0.0018508802 -0.0018551195 -0.0018586516 -0.0018601079 -0.0018600314 -0.0018594639 -0.0018580778 -0.0018564726 -0.0018560354][-0.0018472431 -0.0018457429 -0.0018477693 -0.0018500836 -0.0018534547 -0.0018570598 -0.0018611449 -0.0018657653 -0.0018691715 -0.0018699679 -0.0018689494 -0.0018668848 -0.0018637388 -0.0018604653 -0.001858419][-0.0018488107 -0.0018485981 -0.0018521919 -0.0018561748 -0.001861385 -0.0018663232 -0.0018709128 -0.0018752883 -0.0018782824 -0.0018788372 -0.0018773366 -0.001874613 -0.0018707208 -0.0018666374 -0.0018636252][-0.0018503928 -0.0018515345 -0.001856907 -0.0018626767 -0.0018692288 -0.0018747963 -0.0018792632 -0.0018828812 -0.0018853259 -0.0018855855 -0.0018842617 -0.0018819325 -0.0018782717 -0.0018744726 -0.0018712824][-0.0018515144 -0.0018534389 -0.0018600104 -0.0018671578 -0.0018745853 -0.0018805763 -0.0018848558 -0.0018878581 -0.0018896289 -0.0018895948 -0.0018887739 -0.0018875432 -0.0018855209 -0.0018828732 -0.0018797528]]...]
INFO - root - 2017-12-09 08:50:54.361437: step 9110, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 77h:41m:14s remains)
INFO - root - 2017-12-09 08:51:03.012162: step 9120, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 76h:39m:22s remains)
INFO - root - 2017-12-09 08:51:11.479469: step 9130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:49m:03s remains)
INFO - root - 2017-12-09 08:51:20.174931: step 9140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:44m:09s remains)
INFO - root - 2017-12-09 08:51:28.712606: step 9150, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 74h:01m:26s remains)
INFO - root - 2017-12-09 08:51:37.408771: step 9160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 75h:35m:09s remains)
INFO - root - 2017-12-09 08:51:46.095664: step 9170, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:18m:46s remains)
INFO - root - 2017-12-09 08:51:54.901989: step 9180, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:43m:57s remains)
INFO - root - 2017-12-09 08:52:03.442677: step 9190, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:06m:00s remains)
INFO - root - 2017-12-09 08:52:12.154238: step 9200, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 75h:39m:28s remains)
2017-12-09 08:52:13.009670: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0058107413 0.0095402906 0.013209943 0.016665429 0.019766286 0.022591924 0.0251351 0.026730653 0.026679335 0.024572026 0.020926867 0.016280396 0.011114224 0.0063609462 0.0026380548][0.0087176356 0.014688398 0.020913113 0.02669736 0.031809375 0.036354713 0.040333446 0.043065928 0.043548893 0.041192397 0.036265511 0.029270148 0.021011323 0.012927239 0.0063940277][0.01324901 0.022062425 0.031603467 0.040513303 0.048320577 0.0550761 0.060646009 0.0641894 0.0644423 0.061072122 0.054592505 0.045581739 0.03451414 0.022766262 0.012695318][0.019421125 0.031092936 0.043754961 0.055557217 0.065781333 0.074531771 0.08172477 0.086369805 0.086670041 0.082287148 0.074174553 0.063073166 0.049306665 0.034079753 0.020326976][0.0259598 0.03991669 0.054522127 0.067760341 0.079085961 0.088821284 0.096796758 0.10222104 0.10303396 0.0988027 0.090537347 0.078910671 0.06385681 0.046037611 0.029069455][0.029917991 0.044750951 0.059789855 0.073248453 0.084739372 0.094641522 0.10295065 0.10904173 0.11070599 0.10720631 0.099677272 0.088814706 0.074138708 0.05552914 0.03672006][0.030452566 0.044464748 0.058408149 0.07122229 0.082615688 0.092985667 0.10192087 0.10908386 0.11202861 0.10975266 0.10331052 0.093348451 0.079493158 0.061117414 0.041733935][0.028296823 0.040363539 0.05240491 0.063876733 0.074906386 0.085632011 0.095599152 0.10397837 0.10840358 0.10765944 0.10238585 0.093265407 0.080128454 0.062447656 0.043431453][0.024538735 0.034078993 0.043565463 0.053145986 0.063166894 0.073750317 0.084301174 0.093848035 0.099788651 0.10080121 0.097080588 0.089003824 0.0765816 0.059801959 0.041835282][0.018938709 0.026089266 0.033139426 0.040601116 0.048976596 0.058447883 0.068570979 0.0783034 0.0852727 0.088011868 0.086133271 0.079671316 0.068578973 0.053352822 0.03715058][0.012246353 0.01702616 0.021894719 0.027400145 0.033857666 0.041530438 0.05024549 0.059316047 0.066558875 0.070423484 0.070207775 0.06558311 0.056442708 0.043563467 0.029955091][0.0061063236 0.0091212625 0.012301167 0.016099613 0.020614313 0.026184892 0.032870054 0.040398192 0.047009822 0.051179234 0.051883232 0.048786573 0.041778896 0.031756379 0.0213043][0.0013776037 0.0030759524 0.0049537579 0.0072950441 0.010032102 0.013517656 0.018062131 0.023596646 0.028844111 0.032442372 0.033404935 0.031384151 0.026436225 0.019511614 0.012502988][-0.0011093481 -0.00041498209 0.00045435363 0.0015680101 0.0029259569 0.0047669532 0.0074156569 0.010935667 0.014528527 0.017106183 0.017854482 0.016604109 0.013581285 0.00954961 0.0056028664][-0.0018801133 -0.001761645 -0.0016225929 -0.0013774631 -0.00096616196 -0.00023673777 0.0010020512 0.002792421 0.0047500944 0.0062351646 0.0066949464 0.0060285414 0.004548084 0.0027101084 0.0010027124]]...]
INFO - root - 2017-12-09 08:52:21.777776: step 9210, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 77h:42m:19s remains)
INFO - root - 2017-12-09 08:52:30.485266: step 9220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:14m:12s remains)
INFO - root - 2017-12-09 08:52:39.062049: step 9230, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 77h:54m:56s remains)
INFO - root - 2017-12-09 08:52:47.602065: step 9240, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 77h:17m:20s remains)
INFO - root - 2017-12-09 08:52:55.946632: step 9250, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 76h:09m:39s remains)
INFO - root - 2017-12-09 08:53:04.629751: step 9260, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 77h:17m:25s remains)
INFO - root - 2017-12-09 08:53:13.267604: step 9270, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 78h:48m:21s remains)
INFO - root - 2017-12-09 08:53:22.060152: step 9280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:26m:27s remains)
INFO - root - 2017-12-09 08:53:30.680363: step 9290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 80h:05m:27s remains)
INFO - root - 2017-12-09 08:53:39.318661: step 9300, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 76h:25m:38s remains)
2017-12-09 08:53:40.158241: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.63179415 0.63086343 0.62737948 0.62263006 0.61765629 0.61297309 0.61108726 0.60809541 0.60377032 0.59704 0.58516032 0.56886911 0.54390377 0.51316792 0.47996545][0.655866 0.65486127 0.65035176 0.64594722 0.64109874 0.63758707 0.63611352 0.63350457 0.62906969 0.62063789 0.60650629 0.58652633 0.55801475 0.52452052 0.4890219][0.66259235 0.66299421 0.65842158 0.65333354 0.64793324 0.64406413 0.64220148 0.63984907 0.63506943 0.62543708 0.60999131 0.58836567 0.55822897 0.52325541 0.48700067][0.6685226 0.66963154 0.6649819 0.66027731 0.65521222 0.65084785 0.64858204 0.64514226 0.6389401 0.6275509 0.61040455 0.58755523 0.55663311 0.52164978 0.48542574][0.67952383 0.68165034 0.67561924 0.66931576 0.66260374 0.65712363 0.65323079 0.64874655 0.64087921 0.6276021 0.60914016 0.58548486 0.55473924 0.5203321 0.4852649][0.69453925 0.69855261 0.69035238 0.68112034 0.671655 0.66352773 0.65689546 0.65054566 0.64111215 0.62669849 0.60723263 0.58312643 0.553149 0.51972973 0.48564544][0.70802635 0.71467847 0.70512837 0.69385785 0.68179125 0.67071736 0.66139495 0.65251946 0.640923 0.62519604 0.60518509 0.58069772 0.55153382 0.51930988 0.48603791][0.71468014 0.72491026 0.71463633 0.70182431 0.68749917 0.67270786 0.6589452 0.64726108 0.6335867 0.61673826 0.59657097 0.57297653 0.54580057 0.51537997 0.48373523][0.71221262 0.72677571 0.71720821 0.70376521 0.6880272 0.67109442 0.65485173 0.63913506 0.62251264 0.60507768 0.58528328 0.5627045 0.537023 0.50917536 0.47984949][0.70306873 0.72067386 0.71073055 0.695972 0.67888755 0.66022545 0.64201814 0.6251356 0.60840845 0.59163839 0.57285911 0.55204916 0.52825183 0.50218219 0.47470704][0.6848824 0.70409518 0.69468659 0.67971909 0.662287 0.64369732 0.62569135 0.60915595 0.59339225 0.578826 0.56240159 0.5434587 0.52129996 0.49647769 0.46988115][0.66063577 0.68171066 0.67367637 0.66043627 0.64498848 0.62829256 0.61248189 0.59786665 0.58405733 0.571462 0.55676138 0.53932649 0.51817679 0.49379677 0.46728811][0.6326201 0.65516883 0.649713 0.63903147 0.62670851 0.61327231 0.60034823 0.58881366 0.577851 0.56766635 0.55475378 0.53873676 0.51834285 0.4939948 0.46750635][0.60493934 0.62762254 0.62399048 0.61678684 0.60864604 0.5997054 0.5916568 0.58390766 0.57640171 0.5689249 0.55805027 0.54343408 0.52363443 0.49907291 0.47183406][0.58010942 0.60193795 0.59919131 0.59472531 0.58997756 0.58491766 0.58108979 0.57810426 0.57489544 0.57000619 0.56112278 0.54794633 0.52886385 0.50426704 0.47640663]]...]
INFO - root - 2017-12-09 08:53:48.750152: step 9310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:44m:08s remains)
INFO - root - 2017-12-09 08:53:57.526658: step 9320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:47m:09s remains)
INFO - root - 2017-12-09 08:54:05.994901: step 9330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 77h:47m:43s remains)
INFO - root - 2017-12-09 08:54:14.575505: step 9340, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 75h:13m:56s remains)
INFO - root - 2017-12-09 08:54:23.119522: step 9350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 77h:27m:00s remains)
INFO - root - 2017-12-09 08:54:31.776656: step 9360, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 75h:57m:20s remains)
INFO - root - 2017-12-09 08:54:40.220784: step 9370, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.823 sec/batch; 73h:52m:37s remains)
INFO - root - 2017-12-09 08:54:48.674613: step 9380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:35m:05s remains)
INFO - root - 2017-12-09 08:54:57.336779: step 9390, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 77h:43m:14s remains)
INFO - root - 2017-12-09 08:55:05.925171: step 9400, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 76h:29m:47s remains)
2017-12-09 08:55:06.760434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0021693059 -0.0021678221 -0.0021677546 -0.0021677588 -0.0021677066 -0.0021676139 -0.0021675483 -0.002167518 -0.002167495 -0.0021673823 -0.0021672961 -0.0021672361 -0.0021671888 -0.0021672298 -0.0021672756][-0.0021680624 -0.0021664912 -0.0021664125 -0.0021664086 -0.0021663702 -0.0021662863 -0.0021662267 -0.002166172 -0.0021661178 -0.002166078 -0.0021660982 -0.0021661564 -0.0021662046 -0.0021662284 -0.0021662307][-0.0021681339 -0.0021665385 -0.0021665869 -0.0021666759 -0.0021666849 -0.0021665725 -0.002166494 -0.0021663446 -0.0021661525 -0.0021660263 -0.0021660193 -0.0021660905 -0.0021661452 -0.0021661781 -0.0021661597][-0.002168074 -0.0021665364 -0.0021668409 -0.0021671921 -0.0021672982 -0.0021671685 -0.0021671546 -0.002166908 -0.002166511 -0.0021662239 -0.00216616 -0.0021661506 -0.0021661154 -0.002166108 -0.0021660647][-0.002168196 -0.0021666675 -0.0021672631 -0.0021681043 -0.0021684614 -0.002168319 -0.0021683713 -0.0021680617 -0.0021675215 -0.0021670249 -0.0021666647 -0.0021663392 -0.0021661336 -0.0021659452 -0.0021659022][-0.0021684372 -0.0021669385 -0.0021678566 -0.002169244 -0.0021702813 -0.0021702496 -0.0021699914 -0.0021695876 -0.0021691816 -0.002168712 -0.0021677674 -0.0021669057 -0.0021662537 -0.002165765 -0.0021657257][-0.0021686184 -0.0021672153 -0.0021684894 -0.0021705807 -0.0021726496 -0.002173326 -0.0021725788 -0.0021718955 -0.0021720871 -0.0021716042 -0.0021696314 -0.0021678987 -0.0021665727 -0.0021657324 -0.0021655622][-0.0021692349 -0.0021678025 -0.0021693462 -0.0021722927 -0.0021755586 -0.0021768224 -0.0021759882 -0.002175313 -0.0021759579 -0.0021751237 -0.0021720312 -0.0021692219 -0.002167064 -0.0021658123 -0.0021653748][-0.002171173 -0.0021696957 -0.0021712424 -0.0021746217 -0.0021787893 -0.0021810979 -0.0021814387 -0.0021808173 -0.0021806026 -0.0021788161 -0.0021749504 -0.0021709183 -0.002167841 -0.0021660584 -0.0021652961][-0.0021741858 -0.0021727115 -0.0021741567 -0.0021776243 -0.0021820453 -0.0021853119 -0.0021870462 -0.0021866488 -0.002185354 -0.0021825072 -0.0021779577 -0.0021727893 -0.0021688154 -0.0021665106 -0.0021654312][-0.0021770692 -0.0021753728 -0.0021764294 -0.0021797074 -0.0021837861 -0.0021874194 -0.0021899161 -0.002189935 -0.0021882153 -0.0021846534 -0.0021794229 -0.0021737972 -0.0021695127 -0.0021669557 -0.0021656326][-0.0021791898 -0.0021773819 -0.0021780673 -0.0021806462 -0.002183716 -0.0021868504 -0.002189144 -0.0021893561 -0.0021875894 -0.0021836904 -0.0021784718 -0.0021733802 -0.0021695422 -0.0021671068 -0.0021657606][-0.0021796464 -0.0021777505 -0.0021781011 -0.0021798315 -0.0021818262 -0.0021839882 -0.0021855766 -0.0021855794 -0.002183947 -0.0021802909 -0.0021759879 -0.0021720196 -0.0021689874 -0.0021669916 -0.0021658584][-0.0021773714 -0.0021754974 -0.0021757917 -0.0021767882 -0.0021780254 -0.0021793498 -0.0021802736 -0.0021801176 -0.0021788597 -0.0021760578 -0.002173004 -0.0021702112 -0.0021680689 -0.0021666498 -0.0021658663][-0.0021729108 -0.0021708549 -0.0021711206 -0.0021717201 -0.0021725292 -0.0021733597 -0.002173977 -0.0021739285 -0.0021730945 -0.0021714107 -0.0021696992 -0.0021681373 -0.0021669124 -0.0021661322 -0.002165749]]...]
INFO - root - 2017-12-09 08:55:15.607836: step 9410, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 86h:14m:02s remains)
INFO - root - 2017-12-09 08:55:24.263319: step 9420, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 78h:27m:24s remains)
INFO - root - 2017-12-09 08:55:32.759629: step 9430, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 77h:41m:01s remains)
INFO - root - 2017-12-09 08:55:41.555100: step 9440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:31m:43s remains)
INFO - root - 2017-12-09 08:55:49.926264: step 9450, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 76h:52m:47s remains)
INFO - root - 2017-12-09 08:55:58.577190: step 9460, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 77h:56m:32s remains)
INFO - root - 2017-12-09 08:56:07.223319: step 9470, loss = 0.90, batch loss = 0.70 (8.2 examples/sec; 0.981 sec/batch; 88h:01m:29s remains)
INFO - root - 2017-12-09 08:56:15.867410: step 9480, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:11m:40s remains)
INFO - root - 2017-12-09 08:56:24.437348: step 9490, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 75h:50m:45s remains)
INFO - root - 2017-12-09 08:56:33.087614: step 9500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:01m:43s remains)
2017-12-09 08:56:33.945839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0021971224 -0.0021966989 -0.0021970908 -0.0021971338 -0.0021957287 -0.0021946339 -0.0021955885 -0.0021970584 -0.0021981557 -0.002197349 -0.0021963732 -0.0021951066 -0.0021938041 -0.0021919026 -0.002189406][-0.0021985616 -0.0021987695 -0.0021992042 -0.0021986298 -0.0021962558 -0.0021949306 -0.002195718 -0.0021969553 -0.0021984424 -0.0021975983 -0.0021969385 -0.0021961639 -0.0021949946 -0.0021927003 -0.0021898071][-0.0022005308 -0.0022008964 -0.0022014785 -0.0022005506 -0.0021976612 -0.0021958007 -0.0021959604 -0.0021968954 -0.0021993783 -0.0021989078 -0.0021984263 -0.0021983241 -0.0021972468 -0.0021945848 -0.0021914335][-0.0022016531 -0.0022020533 -0.0022024654 -0.0022014885 -0.0021986743 -0.0021962989 -0.0021959716 -0.0021972037 -0.0021997574 -0.0022003434 -0.0022005513 -0.0022008363 -0.0021996156 -0.0021964847 -0.0021929527][-0.0022009267 -0.0022013423 -0.0022017595 -0.0022008719 -0.0021982733 -0.0021953438 -0.0021946027 -0.0021965669 -0.0021998815 -0.0022016154 -0.0022021024 -0.0022025772 -0.0022016054 -0.0021980866 -0.0021939543][-0.0021981893 -0.0021982484 -0.0021988349 -0.0021981169 -0.0021958842 -0.0021935944 -0.0021931718 -0.0021963716 -0.0022004896 -0.0022027125 -0.0022036538 -0.0022035453 -0.0022020722 -0.002198356 -0.0021940467][-0.0021949236 -0.0021946256 -0.0021955261 -0.0021953925 -0.0021937804 -0.0021926425 -0.002192823 -0.0021959448 -0.002200712 -0.0022035935 -0.0022042887 -0.0022035127 -0.0022016612 -0.0021981958 -0.0021943341][-0.002192402 -0.0021917981 -0.0021929946 -0.002193192 -0.002191833 -0.0021921918 -0.0021938754 -0.0021971404 -0.0022009036 -0.0022031164 -0.0022030289 -0.0022017213 -0.0021996947 -0.0021967622 -0.0021934244][-0.002190144 -0.0021890809 -0.002190521 -0.0021914211 -0.0021905561 -0.0021915466 -0.0021945008 -0.0021980545 -0.0022005618 -0.002201343 -0.0022002922 -0.00219862 -0.0021965343 -0.002193881 -0.0021910428][-0.0021886779 -0.0021871689 -0.0021884872 -0.0021893973 -0.0021893738 -0.0021905834 -0.0021936193 -0.0021969983 -0.0021990349 -0.0021989779 -0.0021966256 -0.0021940744 -0.0021922602 -0.0021901948 -0.0021881447][-0.0021874469 -0.0021854071 -0.0021864369 -0.002187273 -0.0021876933 -0.0021886185 -0.0021914211 -0.0021947299 -0.0021960251 -0.00219506 -0.0021924528 -0.002190192 -0.0021883473 -0.0021867575 -0.002185557][-0.0021868527 -0.0021845549 -0.0021851598 -0.0021857049 -0.0021861277 -0.0021866662 -0.0021889666 -0.002191538 -0.0021923843 -0.0021911794 -0.0021886153 -0.0021868125 -0.0021857417 -0.002184795 -0.0021839517][-0.0021867182 -0.0021840183 -0.0021843151 -0.0021844716 -0.0021847135 -0.0021852441 -0.0021868136 -0.0021888101 -0.0021893387 -0.0021881352 -0.0021860406 -0.0021846402 -0.0021841743 -0.0021837126 -0.002183164][-0.0021867377 -0.0021836576 -0.0021836914 -0.0021838003 -0.002183879 -0.0021842171 -0.0021851845 -0.0021865643 -0.0021869941 -0.0021861312 -0.0021847305 -0.002183883 -0.0021836362 -0.0021832606 -0.0021829561][-0.0021868097 -0.0021835796 -0.0021832283 -0.0021834755 -0.0021836145 -0.0021836201 -0.0021839384 -0.0021847144 -0.00218509 -0.0021845812 -0.0021837794 -0.0021834008 -0.0021834245 -0.0021833281 -0.0021831973]]...]
INFO - root - 2017-12-09 08:56:42.593714: step 9510, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:17m:16s remains)
INFO - root - 2017-12-09 08:56:51.275966: step 9520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:53m:42s remains)
INFO - root - 2017-12-09 08:56:59.829726: step 9530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:51m:47s remains)
INFO - root - 2017-12-09 08:57:08.636564: step 9540, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 80h:29m:40s remains)
INFO - root - 2017-12-09 08:57:17.120526: step 9550, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 79h:07m:28s remains)
INFO - root - 2017-12-09 08:57:25.734549: step 9560, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 78h:33m:27s remains)
INFO - root - 2017-12-09 08:57:34.420859: step 9570, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 76h:41m:12s remains)
INFO - root - 2017-12-09 08:57:43.060391: step 9580, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 77h:21m:12s remains)
INFO - root - 2017-12-09 08:57:51.430759: step 9590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:24m:16s remains)
INFO - root - 2017-12-09 08:58:00.055768: step 9600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:25m:10s remains)
2017-12-09 08:58:00.882477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020324518 -0.0017207542 -0.0011801855 -0.00043614744 0.00040052179 0.0010396941 0.0013928474 0.0011666622 0.00066844723 -5.6903111e-05 -0.00077523675 -0.0013695399 -0.0018190211 -0.0020906185 -0.0022160944][-0.0017455892 -0.0013539488 -0.00061220548 0.00046119816 0.0016414826 0.0025293352 0.0029473635 0.0025398298 0.0017386954 0.00066706724 -0.00032588933 -0.0011281616 -0.0017146672 -0.0020588881 -0.0022095481][-0.00059026747 0.00033768266 0.001753883 0.0035305009 0.005282382 0.0064672525 0.0068535279 0.0061568 0.004894759 0.0032050861 0.0015286126 7.7765668e-05 -0.0010368919 -0.001745648 -0.0020993187][0.0029899632 0.0050704973 0.0078044361 0.010833375 0.013537141 0.01520074 0.015584888 0.014489925 0.01247111 0.0096857026 0.00655683 0.0035253 0.00095375185 -0.00080184429 -0.0017739676][0.010102715 0.014057839 0.018617664 0.023150232 0.026890393 0.029071368 0.029457115 0.027884429 0.024796065 0.020313652 0.014902951 0.0093583819 0.0044187084 0.00085218041 -0.0012217528][0.020921974 0.026738346 0.032887764 0.038564187 0.04288438 0.045184422 0.045254532 0.042935912 0.038462639 0.031916115 0.023834556 0.015445996 0.0079020336 0.0024353024 -0.00072821579][0.033875253 0.040712632 0.047387443 0.053202789 0.057192143 0.058809564 0.057912752 0.054404493 0.048358742 0.039853517 0.02955237 0.019032277 0.0097172558 0.0031163753 -0.00057650148][0.045554619 0.052434076 0.058289286 0.06294246 0.065420076 0.065372765 0.062786028 0.057701603 0.050187115 0.040417086 0.029214185 0.018235818 0.0089038014 0.0025679823 -0.00080504862][0.052400086 0.058196884 0.062122252 0.064693242 0.065006047 0.062861994 0.058373567 0.051829781 0.043478128 0.033677254 0.023290968 0.013724327 0.0060696458 0.0011662191 -0.0012748142][0.051795673 0.055782847 0.057382524 0.057717673 0.056015026 0.052187867 0.046453688 0.039362174 0.031366907 0.02294003 0.014793046 0.00784542 0.0026902873 -0.00036696019 -0.0017512309][0.043432605 0.045536142 0.045302417 0.044112954 0.04136103 0.037021916 0.031356022 0.025037378 0.018591961 0.012454608 0.0070677358 0.0028677592 3.9784703e-05 -0.0014675709 -0.0020583426][0.030229263 0.030843943 0.029689189 0.027994864 0.025326146 0.021686997 0.017302006 0.012757964 0.0084871184 0.0047762347 0.001828711 -0.00023530028 -0.0014531895 -0.002011348 -0.0021891268][0.016777009 0.01660165 0.015396241 0.013996664 0.012126384 0.00978286 0.0071143848 0.0045043407 0.002222612 0.00040960638 -0.00088070508 -0.0016661203 -0.0020522077 -0.0021934935 -0.0022265331][0.0066190078 0.006230691 0.0054109506 0.0045959791 0.0036291052 0.0024954728 0.0012653756 0.00012302725 -0.00080923375 -0.0014842639 -0.00190993 -0.0021267564 -0.0022104 -0.0022319891 -0.0022345735][0.00084537943 0.00057344954 0.0001884473 -0.00014542812 -0.00049915537 -0.00088596833 -0.0012851889 -0.0016353005 -0.0019009935 -0.002075366 -0.0021745218 -0.0022184097 -0.0022333274 -0.0022365646 -0.0022362669]]...]
INFO - root - 2017-12-09 08:58:09.606990: step 9610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:44m:26s remains)
INFO - root - 2017-12-09 08:58:18.470747: step 9620, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 80h:15m:29s remains)
INFO - root - 2017-12-09 08:58:27.005034: step 9630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:24m:32s remains)
INFO - root - 2017-12-09 08:58:35.665035: step 9640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:16m:59s remains)
INFO - root - 2017-12-09 08:58:44.332610: step 9650, loss = 0.90, batch loss = 0.70 (10.7 examples/sec; 0.744 sec/batch; 66h:44m:36s remains)
INFO - root - 2017-12-09 08:58:53.182554: step 9660, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 82h:01m:38s remains)
INFO - root - 2017-12-09 08:59:01.992297: step 9670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 75h:01m:19s remains)
INFO - root - 2017-12-09 08:59:10.798528: step 9680, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:20m:33s remains)
INFO - root - 2017-12-09 08:59:19.353687: step 9690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:36m:46s remains)
INFO - root - 2017-12-09 08:59:28.143986: step 9700, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 81h:07m:18s remains)
2017-12-09 08:59:29.028602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0023021253 -0.0022987516 -0.002296899 -0.0022950356 -0.0022930696 -0.0022909942 -0.0022889823 -0.0022873189 -0.0022862973 -0.0022857853 -0.0022857292 -0.0022862891 -0.0022871704 -0.0022884654 -0.0022901015][-0.0023063491 -0.0023021973 -0.0022996259 -0.0022970366 -0.0022942929 -0.0022912459 -0.0022881536 -0.002285236 -0.0022833361 -0.0022822968 -0.0022819904 -0.0022826549 -0.002284222 -0.0022865233 -0.0022891203][-0.0023129736 -0.0023087549 -0.0023059368 -0.0023030289 -0.0022998794 -0.0022960566 -0.0022918682 -0.0022875331 -0.0022843452 -0.0022822863 -0.0022812318 -0.0022816465 -0.0022836442 -0.0022870165 -0.0022906202][-0.0023200063 -0.0023156018 -0.0023127906 -0.0023097107 -0.0023062457 -0.0023022126 -0.002297567 -0.0022923967 -0.0022879939 -0.0022850852 -0.002283569 -0.0022834765 -0.0022850926 -0.0022887252 -0.0022933916][-0.0023261257 -0.0023219024 -0.002319233 -0.0023163613 -0.0023131911 -0.0023094337 -0.0023048017 -0.0022994597 -0.0022945139 -0.0022907825 -0.0022883455 -0.0022870821 -0.0022879343 -0.0022913977 -0.0022967511][-0.0023305661 -0.0023268273 -0.0023246373 -0.0023223646 -0.0023200135 -0.0023172298 -0.0023134833 -0.0023085775 -0.0023040602 -0.0022996538 -0.0022958966 -0.0022928785 -0.0022923925 -0.0022948987 -0.0022999805][-0.002332255 -0.002329065 -0.0023274377 -0.0023261078 -0.0023249839 -0.0023235085 -0.002320993 -0.0023173613 -0.0023138155 -0.0023091757 -0.0023043645 -0.0023000508 -0.0022984575 -0.002299712 -0.0023036045][-0.0023332303 -0.0023299451 -0.0023284007 -0.002327706 -0.0023277064 -0.0023275078 -0.0023266058 -0.0023249984 -0.0023229057 -0.0023186589 -0.002313287 -0.0023078711 -0.0023045251 -0.002303432 -0.0023049843][-0.0023317602 -0.0023287958 -0.002327841 -0.0023279013 -0.0023289956 -0.0023299831 -0.0023303062 -0.0023300701 -0.0023289409 -0.00232515 -0.00231951 -0.0023130663 -0.0023078909 -0.0023042562 -0.0023033603][-0.0023283549 -0.0023258543 -0.0023255202 -0.0023263718 -0.0023281123 -0.0023296806 -0.0023306734 -0.002331424 -0.0023307055 -0.0023271311 -0.0023214414 -0.0023145746 -0.0023081987 -0.0023027204 -0.0022997318][-0.0023224559 -0.0023203914 -0.0023204391 -0.0023216621 -0.0023236424 -0.0023254033 -0.0023265607 -0.0023274575 -0.0023270943 -0.0023239523 -0.0023187282 -0.002312337 -0.002305934 -0.0022998382 -0.0022956491][-0.002314857 -0.0023129734 -0.0023132812 -0.0023144342 -0.0023162591 -0.0023179345 -0.0023188016 -0.002319545 -0.002319131 -0.0023165382 -0.0023121256 -0.0023068131 -0.0023012049 -0.0022956729 -0.0022914102][-0.0023054928 -0.0023036287 -0.00230403 -0.0023050276 -0.0023065021 -0.0023078984 -0.0023085747 -0.0023091005 -0.0023087729 -0.0023067761 -0.0023033754 -0.0022993267 -0.0022950247 -0.0022906899 -0.0022870428][-0.0022963784 -0.0022939788 -0.0022942275 -0.0022949968 -0.0022961874 -0.0022973304 -0.0022979269 -0.0022983204 -0.0022980783 -0.0022965767 -0.0022942643 -0.0022915713 -0.0022887788 -0.0022859864 -0.0022836723][-0.0022893716 -0.0022860789 -0.0022858542 -0.0022862535 -0.0022869944 -0.0022878402 -0.0022884612 -0.0022890295 -0.0022892074 -0.0022886174 -0.0022876102 -0.0022862924 -0.0022848377 -0.0022834081 -0.0022821419]]...]
INFO - root - 2017-12-09 08:59:37.699159: step 9710, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 74h:48m:30s remains)
INFO - root - 2017-12-09 08:59:46.518302: step 9720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:16m:45s remains)
INFO - root - 2017-12-09 08:59:55.197722: step 9730, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:24m:47s remains)
INFO - root - 2017-12-09 09:00:03.811785: step 9740, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 77h:23m:15s remains)
INFO - root - 2017-12-09 09:00:12.450360: step 9750, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 64h:07m:33s remains)
INFO - root - 2017-12-09 09:00:21.048307: step 9760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:08m:32s remains)
INFO - root - 2017-12-09 09:00:29.765781: step 9770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:06m:43s remains)
INFO - root - 2017-12-09 09:00:38.211126: step 9780, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 76h:20m:07s remains)
INFO - root - 2017-12-09 09:00:46.611258: step 9790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:55m:07s remains)
INFO - root - 2017-12-09 09:00:55.446814: step 9800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:15m:28s remains)
2017-12-09 09:00:56.356493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002384522 -0.002383539 -0.0023836021 -0.0023839427 -0.0023841592 -0.0023841511 -0.002383675 -0.0023828668 -0.002381851 -0.0023807629 -0.0023797746 -0.0023791816 -0.0023791434 -0.0023795981 -0.0023796174][-0.0023870862 -0.0023860685 -0.0023861094 -0.002386508 -0.0023867884 -0.0023867942 -0.0023863434 -0.002385421 -0.0023841844 -0.0023827825 -0.0023815054 -0.0023807769 -0.0023807182 -0.0023812824 -0.0023813485][-0.0023872054 -0.0023861225 -0.0023861383 -0.0023865516 -0.0023868629 -0.0023869511 -0.0023865746 -0.0023856096 -0.0023841946 -0.0023824954 -0.0023808978 -0.0023798575 -0.0023796014 -0.0023800868 -0.0023802004][-0.0023849828 -0.0023838764 -0.002383966 -0.0023843953 -0.0023846978 -0.0023847579 -0.0023843015 -0.0023832473 -0.0023817003 -0.002379826 -0.0023780081 -0.002376748 -0.002376308 -0.0023766889 -0.0023770088][-0.0023820654 -0.0023808312 -0.0023810156 -0.0023814978 -0.0023818156 -0.0023818903 -0.0023814321 -0.0023804312 -0.0023789417 -0.002377185 -0.0023754546 -0.0023741687 -0.0023736251 -0.0023737503 -0.0023739834][-0.0023778414 -0.0023764074 -0.0023766514 -0.0023771408 -0.0023774493 -0.0023773944 -0.0023768051 -0.0023758505 -0.0023745622 -0.0023732018 -0.0023719582 -0.0023710888 -0.0023706246 -0.0023705426 -0.0023705366][-0.0023734435 -0.0023717517 -0.002372042 -0.0023724742 -0.0023726963 -0.0023725471 -0.002371859 -0.0023708029 -0.0023695864 -0.0023685172 -0.002367716 -0.002367262 -0.002367036 -0.0023669577 -0.0023668974][-0.0023712136 -0.0023692437 -0.0023694518 -0.0023698481 -0.0023701815 -0.0023702655 -0.002369775 -0.0023687494 -0.0023674779 -0.0023663198 -0.0023654723 -0.0023649919 -0.0023647512 -0.0023646448 -0.0023646064][-0.0023707957 -0.0023688639 -0.0023690993 -0.0023694944 -0.002369853 -0.0023700469 -0.0023697955 -0.0023690327 -0.0023679787 -0.0023668977 -0.0023659242 -0.0023652036 -0.0023647218 -0.0023645116 -0.002364449][-0.00237087 -0.0023689959 -0.002369154 -0.002369391 -0.0023696348 -0.0023697922 -0.00236973 -0.0023693095 -0.0023686881 -0.0023679866 -0.0023670853 -0.0023661396 -0.0023653039 -0.0023648199 -0.002364648][-0.0023707815 -0.0023688755 -0.0023690013 -0.0023690672 -0.002369151 -0.0023691824 -0.0023691831 -0.0023690192 -0.0023687871 -0.002368479 -0.0023678357 -0.0023668974 -0.002365811 -0.0023650206 -0.0023647067][-0.0023706655 -0.0023688185 -0.0023689135 -0.002368846 -0.0023688432 -0.00236888 -0.0023689172 -0.0023688618 -0.0023688539 -0.0023688329 -0.0023684674 -0.0023677119 -0.0023667205 -0.0023657593 -0.0023652446][-0.0023704453 -0.0023686045 -0.0023687405 -0.0023686313 -0.0023685854 -0.0023686434 -0.002368693 -0.002368649 -0.0023687005 -0.0023688036 -0.0023686583 -0.0023681899 -0.0023674583 -0.0023665552 -0.0023659675][-0.0023703962 -0.0023684255 -0.0023685521 -0.0023684748 -0.0023684269 -0.0023684807 -0.0023685282 -0.0023684832 -0.0023685088 -0.0023685803 -0.0023685703 -0.0023684204 -0.0023679573 -0.0023672467 -0.0023667871][-0.0023702113 -0.0023681207 -0.00236823 -0.0023682052 -0.0023682071 -0.0023683093 -0.0023683715 -0.0023683303 -0.002368307 -0.0023683789 -0.0023684932 -0.0023686083 -0.0023684064 -0.0023679119 -0.0023676215]]...]
INFO - root - 2017-12-09 09:01:04.961265: step 9810, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 73h:50m:54s remains)
INFO - root - 2017-12-09 09:01:13.735309: step 9820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:31m:40s remains)
INFO - root - 2017-12-09 09:01:22.212013: step 9830, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.825 sec/batch; 73h:58m:36s remains)
INFO - root - 2017-12-09 09:01:30.968777: step 9840, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 81h:30m:11s remains)
INFO - root - 2017-12-09 09:01:39.613098: step 9850, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 77h:05m:39s remains)
INFO - root - 2017-12-09 09:01:48.313033: step 9860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:20m:44s remains)
INFO - root - 2017-12-09 09:01:57.108595: step 9870, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 81h:13m:13s remains)
INFO - root - 2017-12-09 09:02:05.699688: step 9880, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:15m:35s remains)
INFO - root - 2017-12-09 09:02:14.098254: step 9890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:45m:08s remains)
INFO - root - 2017-12-09 09:02:22.417674: step 9900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:53m:55s remains)
2017-12-09 09:02:23.289595: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015544387 0.024623325 0.034673728 0.043751467 0.050292183 0.053800821 0.054313779 0.052030344 0.046772659 0.038913716 0.029620171 0.02028103 0.012096794 0.0059778257 0.00204643][0.026283858 0.039467074 0.054616921 0.068962209 0.079938509 0.086308151 0.087672755 0.08427994 0.076285176 0.064491026 0.0505262 0.036265854 0.023469277 0.013455803 0.0065535209][0.039300613 0.057293005 0.078092352 0.098189794 0.11399949 0.12338764 0.12540185 0.12042762 0.10917957 0.09306702 0.074268252 0.055014528 0.037409626 0.023102734 0.01268976][0.051446337 0.074459448 0.10085139 0.12642147 0.14675409 0.15898499 0.1616382 0.1550376 0.14042351 0.11999973 0.096593767 0.072801284 0.0509437 0.032768484 0.0190546][0.059642624 0.086880222 0.1178675 0.14786899 0.17189553 0.18648949 0.18980964 0.18211281 0.16497511 0.14113833 0.11405741 0.086706981 0.061596252 0.040452294 0.024181617][0.062535174 0.092488363 0.12625518 0.15889068 0.18528558 0.2017639 0.20617974 0.19847029 0.18021753 0.15436783 0.12486453 0.095139235 0.067976855 0.045051351 0.027308548][0.060714737 0.091878682 0.12667812 0.16003722 0.18702962 0.20421149 0.20951416 0.20259269 0.18461789 0.15829031 0.12784903 0.097158208 0.069333993 0.046039369 0.028132504][0.056421008 0.087093018 0.12117084 0.15355925 0.17954291 0.19613199 0.20157889 0.19542484 0.17843895 0.15287784 0.12295672 0.092789337 0.065769419 0.04355979 0.026801029][0.051587965 0.080332443 0.11206757 0.14184567 0.16522962 0.17987964 0.1845284 0.17871219 0.16288877 0.13888244 0.11071569 0.082538605 0.057775635 0.037967283 0.023443324][0.046301667 0.072074063 0.10020144 0.12615485 0.14585976 0.15756738 0.16053286 0.15455496 0.13997473 0.11824133 0.093005948 0.068123862 0.04677923 0.030300234 0.018666897][0.039131746 0.061026894 0.084645607 0.10605529 0.1217197 0.1303754 0.13163882 0.12552002 0.11238655 0.093521148 0.072100326 0.051453337 0.034287285 0.021618417 0.013118194][0.029782116 0.046964243 0.065420389 0.081955083 0.093617745 0.099475816 0.0994243 0.093645871 0.08257854 0.067339666 0.050503738 0.034712426 0.022065444 0.013234543 0.0076827076][0.019676927 0.031765398 0.044829074 0.056527697 0.064650334 0.06843508 0.067816466 0.063028269 0.054555964 0.043340497 0.03131967 0.020387169 0.01198234 0.0064627039 0.0032461798][0.010521065 0.018020473 0.026221968 0.033580925 0.038664062 0.040932298 0.040339779 0.037068445 0.031474207 0.024222359 0.01662817 0.0099096559 0.0049412707 0.0018703472 0.00021724636][0.0037109321 0.0076702903 0.012130899 0.016201142 0.019049056 0.020309489 0.019955695 0.018103085 0.014966412 0.010956123 0.00683571 0.0032826085 0.000753568 -0.00072305184 -0.0014622688]]...]
INFO - root - 2017-12-09 09:02:31.785430: step 9910, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:08m:26s remains)
INFO - root - 2017-12-09 09:02:40.367934: step 9920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:57m:48s remains)
INFO - root - 2017-12-09 09:02:48.915510: step 9930, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 70h:56m:05s remains)
INFO - root - 2017-12-09 09:02:57.523491: step 9940, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 76h:03m:32s remains)
INFO - root - 2017-12-09 09:03:06.227792: step 9950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:12m:06s remains)
INFO - root - 2017-12-09 09:03:14.589103: step 9960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:36m:38s remains)
INFO - root - 2017-12-09 09:03:23.227136: step 9970, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 75h:06m:31s remains)
INFO - root - 2017-12-09 09:03:31.884737: step 9980, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:47m:02s remains)
INFO - root - 2017-12-09 09:03:40.466175: step 9990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:06m:07s remains)
INFO - root - 2017-12-09 09:03:49.133315: step 10000, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 74h:45m:22s remains)
2017-12-09 09:03:49.981975: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014911964 0.014732507 0.014318111 0.014043008 0.013504525 0.013022547 0.012689786 0.012487534 0.012401689 0.012422721 0.012686537 0.012845783 0.01295443 0.012609771 0.011529029][0.018236225 0.018017963 0.017297927 0.0168214 0.01611276 0.015455029 0.014890411 0.014576041 0.014429153 0.014394415 0.014545406 0.014686364 0.01484407 0.014510234 0.013378103][0.019838223 0.019719172 0.018844912 0.018045317 0.016898261 0.016016765 0.015259796 0.014666491 0.014206623 0.014049091 0.014269351 0.014592613 0.014896656 0.0147035 0.013692847][0.020027945 0.020127071 0.019332597 0.018282233 0.016721273 0.015322614 0.014077852 0.013294709 0.012722383 0.012423065 0.012688269 0.013297076 0.014028325 0.014244529 0.013569403][0.019698244 0.019735858 0.01873933 0.01747027 0.015647016 0.013779682 0.012117437 0.011056392 0.010349631 0.010116144 0.010523044 0.011346471 0.012363237 0.012982694 0.012869734][0.018724408 0.019059295 0.017750949 0.016044956 0.013814957 0.011678327 0.0098841647 0.0086027766 0.0077718729 0.0075703086 0.0081089763 0.0091558341 0.01037291 0.011208862 0.011457908][0.017359603 0.017936669 0.016405465 0.014445473 0.011849619 0.0094778016 0.0075915 0.0062719872 0.005431789 0.0052427249 0.005888639 0.0070793433 0.0084018419 0.009346948 0.009763347][0.015762042 0.016289236 0.014649215 0.012509159 0.0096984748 0.0072837528 0.0053602308 0.0039965948 0.0031732831 0.0031366243 0.0039591845 0.0052246125 0.0065806257 0.00767646 0.0083011193][0.013281454 0.013593373 0.011963102 0.0098789856 0.0071382029 0.0047981711 0.0030012496 0.0018690357 0.0013353878 0.0015071549 0.0024222976 0.0037455822 0.0051487628 0.0063751023 0.0071303258][0.010054004 0.010226967 0.0085323146 0.0064984672 0.0040224446 0.0020669659 0.00071550813 1.6544946e-06 -0.00012888433 0.00024579652 0.0011405589 0.0023377731 0.0036423167 0.0048900507 0.0056701694][0.0069394894 0.0069721118 0.0053330911 0.0034698306 0.0014066908 -7.3680654e-05 -0.00090832682 -0.0011325814 -0.00090666872 -0.00046077813 0.00025130156 0.0011566093 0.0021815838 0.0031530934 0.003685585][0.0040480504 0.0041318974 0.0028099508 0.0012273567 -0.0004262391 -0.00144503 -0.0018187684 -0.0017414161 -0.0014576912 -0.0011193536 -0.00061668828 3.641122e-05 0.00077618589 0.0013935489 0.0016680823][0.0019551783 0.0019290529 0.00087510981 -0.00038056029 -0.0015215144 -0.0021391625 -0.0022918447 -0.002187514 -0.0019812477 -0.001753064 -0.0014524884 -0.0010782169 -0.00064670935 -0.00029464555 -0.00012414996][0.00038089743 0.00024391711 -0.00055318954 -0.0014382495 -0.0021219347 -0.0024312898 -0.0024710319 -0.0024200887 -0.0023361691 -0.0022267504 -0.0020546014 -0.0018579152 -0.001668104 -0.0015205297 -0.001410068][-0.00082985463 -0.00097332243 -0.0015088224 -0.0020469311 -0.0023940797 -0.0025193596 -0.0025305355 -0.0025209868 -0.0025009357 -0.0024678987 -0.002417095 -0.0023605593 -0.0023043633 -0.0022426844 -0.0021672072]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 09:03:59.146242: step 10010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:27m:46s remains)
INFO - root - 2017-12-09 09:04:07.585200: step 10020, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 74h:56m:51s remains)
INFO - root - 2017-12-09 09:04:16.154602: step 10030, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 79h:29m:04s remains)
INFO - root - 2017-12-09 09:04:24.638420: step 10040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:55m:58s remains)
INFO - root - 2017-12-09 09:04:33.168201: step 10050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:17m:41s remains)
INFO - root - 2017-12-09 09:04:41.654667: step 10060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:16m:54s remains)
INFO - root - 2017-12-09 09:04:50.343544: step 10070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 79h:10m:42s remains)
INFO - root - 2017-12-09 09:04:59.065152: step 10080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:03m:02s remains)
INFO - root - 2017-12-09 09:05:07.576149: step 10090, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 76h:11m:48s remains)
INFO - root - 2017-12-09 09:05:16.271529: step 10100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:42m:39s remains)
2017-12-09 09:05:17.159490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025401325 -0.0025276663 -0.0025182054 -0.0025125884 -0.0025102759 -0.0025070456 -0.0025010286 -0.002492877 -0.002484194 -0.0024772135 -0.0024737986 -0.002474349 -0.0024784619 -0.0024835349 -0.0024863183][-0.0025321648 -0.002515903 -0.0025033921 -0.002495975 -0.0024930239 -0.0024898741 -0.0024834154 -0.0024735054 -0.0024623773 -0.0024527723 -0.0024476212 -0.0024483388 -0.00245485 -0.0024644143 -0.002473362][-0.0025242635 -0.0025035075 -0.0024868783 -0.0024767863 -0.002472003 -0.0024675005 -0.0024598676 -0.0024486128 -0.002436365 -0.002426394 -0.0024220189 -0.0024249111 -0.0024348556 -0.0024495351 -0.0024654765][-0.0025175819 -0.0024921352 -0.0024703287 -0.0024557093 -0.0024473749 -0.0024402591 -0.0024318094 -0.0024211537 -0.0024098493 -0.0024011072 -0.0023987966 -0.0024051801 -0.0024195171 -0.0024393776 -0.0024611545][-0.0025132976 -0.0024832801 -0.0024557319 -0.0024355929 -0.0024226217 -0.0024131644 -0.0024046486 -0.0023950937 -0.0023854917 -0.0023789061 -0.0023787492 -0.0023881102 -0.0024065832 -0.0024311699 -0.0024569293][-0.0025126527 -0.00248017 -0.0024479078 -0.00242265 -0.0024061787 -0.0023954706 -0.0023866878 -0.0023779911 -0.0023692762 -0.0023640546 -0.0023650955 -0.002375751 -0.0023955624 -0.0024213595 -0.0024484566][-0.0025144529 -0.0024813644 -0.0024472412 -0.0024192524 -0.0024007917 -0.0023897302 -0.00238184 -0.0023740432 -0.0023656737 -0.002360078 -0.0023601491 -0.0023691447 -0.0023868557 -0.0024105133 -0.0024362113][-0.0025171232 -0.0024848757 -0.0024512471 -0.00242346 -0.0024054872 -0.0023963058 -0.0023913193 -0.0023862394 -0.0023781252 -0.0023704551 -0.0023665626 -0.0023705142 -0.0023825269 -0.002401358 -0.0024238708][-0.0025207729 -0.0024914721 -0.0024620148 -0.00243853 -0.0024252702 -0.0024214974 -0.0024195923 -0.0024154452 -0.0024069855 -0.0023959575 -0.0023860873 -0.0023820882 -0.0023859313 -0.0023982194 -0.0024169814][-0.0025273459 -0.0025035557 -0.0024798345 -0.0024624676 -0.0024542543 -0.0024538469 -0.002454489 -0.0024517416 -0.0024439064 -0.0024306679 -0.0024156151 -0.0024036805 -0.0023995284 -0.0024051387 -0.0024207761][-0.0025380144 -0.0025214832 -0.0025054093 -0.0024942623 -0.0024891109 -0.0024897899 -0.0024909799 -0.0024886602 -0.0024811206 -0.0024673368 -0.0024493141 -0.0024321028 -0.0024220133 -0.0024224208 -0.0024343613][-0.0025497023 -0.0025403483 -0.0025314582 -0.0025247349 -0.0025213293 -0.002520931 -0.0025206206 -0.0025176972 -0.0025102929 -0.0024969385 -0.0024791488 -0.0024599852 -0.0024470021 -0.0024447155 -0.0024533912][-0.0025574567 -0.0025536066 -0.002549523 -0.0025457274 -0.0025431588 -0.0025419281 -0.0025398585 -0.002535827 -0.002528304 -0.0025161062 -0.0024998009 -0.0024830143 -0.0024713203 -0.002468643 -0.0024752517][-0.0025607636 -0.0025596768 -0.002558365 -0.0025565187 -0.0025546737 -0.002552741 -0.0025496613 -0.0025452275 -0.0025381716 -0.0025274327 -0.0025142867 -0.002501756 -0.002493599 -0.0024922143 -0.0024969776][-0.0025610211 -0.00256092 -0.0025607059 -0.0025599098 -0.0025587976 -0.0025566781 -0.0025536458 -0.0025494341 -0.0025431749 -0.0025352656 -0.0025265946 -0.0025192245 -0.0025146825 -0.002513781 -0.0025166341]]...]
INFO - root - 2017-12-09 09:05:25.951938: step 10110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 79h:10m:13s remains)
INFO - root - 2017-12-09 09:05:34.686194: step 10120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:20m:27s remains)
INFO - root - 2017-12-09 09:05:43.437180: step 10130, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:12m:21s remains)
INFO - root - 2017-12-09 09:05:52.048524: step 10140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 79h:12m:16s remains)
INFO - root - 2017-12-09 09:06:00.822597: step 10150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:20m:44s remains)
INFO - root - 2017-12-09 09:06:09.444738: step 10160, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 77h:28m:02s remains)
INFO - root - 2017-12-09 09:06:18.270304: step 10170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:13m:33s remains)
INFO - root - 2017-12-09 09:06:26.939755: step 10180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:15m:40s remains)
INFO - root - 2017-12-09 09:06:35.642650: step 10190, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.988 sec/batch; 88h:28m:22s remains)
INFO - root - 2017-12-09 09:06:44.341466: step 10200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:00m:28s remains)
2017-12-09 09:06:45.216471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002530915 -0.0025294188 -0.0025295189 -0.0025293746 -0.0025292188 -0.0025294162 -0.0025285326 -0.0025236583 -0.0025062871 -0.002412355 -0.002260736 -0.0021643445 -0.0021818141 -0.00226395 -0.0023131352][-0.0025241165 -0.0025257212 -0.0025262218 -0.0025262102 -0.0025255419 -0.0025252677 -0.002525741 -0.002522581 -0.0024849584 -0.0023572696 -0.0021589508 -0.001993642 -0.0019714264 -0.0020846906 -0.002178669][-0.0025128492 -0.0025213547 -0.0025222441 -0.0025246327 -0.0025236073 -0.0025239566 -0.0025232388 -0.0025119577 -0.0024457262 -0.0022704934 -0.0019961516 -0.0017136179 -0.0015902495 -0.0016957116 -0.0018627216][-0.002495995 -0.0025121055 -0.0025122575 -0.0025185118 -0.0025135649 -0.002515398 -0.0025043117 -0.00246158 -0.0023318734 -0.0020659736 -0.0016778389 -0.0012509315 -0.00098924618 -0.0010374655 -0.0012780234][-0.0024732368 -0.0024931927 -0.0024922953 -0.0025009 -0.0024807914 -0.0024640665 -0.0024073306 -0.002293525 -0.0020457886 -0.0016432434 -0.0011298277 -0.00059046131 -0.00021189172 -0.00018703146 -0.00048241066][-0.0024418386 -0.0024602695 -0.0024614078 -0.0024587708 -0.0024002143 -0.0023197434 -0.0021622519 -0.00192563 -0.0015326458 -0.0010044618 -0.00041180477 0.00016520033 0.00058523426 0.00065456075 0.00032923394][-0.0024044076 -0.0024137432 -0.0024165378 -0.0023918536 -0.0022698008 -0.0020609479 -0.0017612741 -0.001382998 -0.00088495133 -0.00030521047 0.00027911318 0.00080858474 0.0011922424 0.0012660013 0.0009247472][-0.0023804542 -0.0023723813 -0.0023652178 -0.0023001675 -0.0021025639 -0.0017566454 -0.0013391015 -0.00086097885 -0.00035672728 0.0001835844 0.00068001566 0.0011115372 0.0014111078 0.0014661746 0.0011303516][-0.00238781 -0.0023596184 -0.0023294936 -0.0022230532 -0.0019667256 -0.0015395145 -0.0010878077 -0.00060698111 -0.00019433047 0.00024562958 0.00061121932 0.00093103922 0.0011383393 0.0011789182 0.00087411003][-0.0024271712 -0.0023879854 -0.0023389498 -0.0022077479 -0.0019415687 -0.00153393 -0.001135541 -0.00074139494 -0.00046825758 -0.0001572296 7.8097684e-05 0.00030197389 0.00043700333 0.00047354586 0.00022815075][-0.0024780671 -0.0024435529 -0.0023955163 -0.002279059 -0.0020575549 -0.0017422705 -0.0014515381 -0.0011913036 -0.0010427115 -0.00085320568 -0.00072223018 -0.00057488144 -0.000486149 -0.00044277171 -0.00061465427][-0.0025163507 -0.0024958658 -0.0024638968 -0.0023883914 -0.0022459833 -0.0020529451 -0.0018787247 -0.0017413797 -0.0016727895 -0.0015789449 -0.00151486 -0.0014237624 -0.001363289 -0.0013169098 -0.0014169684][-0.0025332693 -0.0025244735 -0.0025117323 -0.0024789418 -0.0024105364 -0.0023195348 -0.0022382995 -0.0021820613 -0.0021548693 -0.0021199312 -0.0020924718 -0.0020428698 -0.0020036872 -0.0019641761 -0.0020104814][-0.0025353518 -0.0025316009 -0.0025290411 -0.0025203116 -0.0024985624 -0.0024702253 -0.002444013 -0.002428354 -0.0024195511 -0.0024107657 -0.0024001009 -0.0023781802 -0.0023576692 -0.0023338764 -0.002349165][-0.0025308351 -0.0025275527 -0.0025280307 -0.0025275715 -0.0025242858 -0.0025202432 -0.0025166804 -0.0025153391 -0.002514598 -0.0025134317 -0.0025097758 -0.0025027457 -0.0024952996 -0.0024853975 -0.0024881386]]...]
INFO - root - 2017-12-09 09:06:54.033439: step 10210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:46m:30s remains)
INFO - root - 2017-12-09 09:07:02.726408: step 10220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 76h:45m:02s remains)
INFO - root - 2017-12-09 09:07:11.437852: step 10230, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 79h:19m:07s remains)
INFO - root - 2017-12-09 09:07:20.021730: step 10240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:18m:36s remains)
INFO - root - 2017-12-09 09:07:28.702901: step 10250, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 75h:52m:48s remains)
INFO - root - 2017-12-09 09:07:37.159445: step 10260, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 75h:33m:09s remains)
INFO - root - 2017-12-09 09:07:45.884119: step 10270, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 78h:11m:16s remains)
INFO - root - 2017-12-09 09:07:54.533995: step 10280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 80h:04m:55s remains)
INFO - root - 2017-12-09 09:08:02.998047: step 10290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 76h:42m:14s remains)
INFO - root - 2017-12-09 09:08:11.741297: step 10300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:42m:45s remains)
2017-12-09 09:08:12.680558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025429833 -0.0025407351 -0.0025407295 -0.0025408766 -0.0025410959 -0.0025413386 -0.0025415048 -0.002541614 -0.002541852 -0.0025421616 -0.0025425183 -0.0025431749 -0.0025440685 -0.0025447626 -0.0025451288][-0.0025403877 -0.0025378377 -0.00253782 -0.0025381416 -0.0025384929 -0.0025387469 -0.002538861 -0.0025388154 -0.0025388589 -0.0025391101 -0.0025395832 -0.0025404538 -0.0025417265 -0.0025426832 -0.0025432566][-0.0025403986 -0.0025370996 -0.0025369583 -0.0025373057 -0.0025376806 -0.0025379015 -0.0025379311 -0.0025378503 -0.0025378494 -0.0025380631 -0.0025386629 -0.0025395418 -0.0025406005 -0.0025416974 -0.0025424184][-0.0025401288 -0.0025367611 -0.0025362861 -0.0025366165 -0.002536844 -0.0025364899 -0.0025358687 -0.0025356773 -0.0025359595 -0.0025365516 -0.0025373087 -0.0025381106 -0.0025390442 -0.0025401141 -0.0025408985][-0.0025388568 -0.002536152 -0.0025358505 -0.002535694 -0.0025351271 -0.0025339569 -0.0025330982 -0.0025330293 -0.0025336528 -0.002534745 -0.0025356859 -0.0025364263 -0.0025373688 -0.0025384424 -0.0025394002][-0.0025320074 -0.0025344668 -0.0025352628 -0.0025344521 -0.0025286956 -0.0025190006 -0.0025141316 -0.0025173496 -0.0025257708 -0.0025315569 -0.002534155 -0.0025350093 -0.0025358298 -0.0025366866 -0.002537474][-0.0025227191 -0.0025334158 -0.0025349236 -0.0025328386 -0.0025198632 -0.0024999415 -0.0024866334 -0.0024914702 -0.002511417 -0.0025265482 -0.0025334728 -0.0025352302 -0.0025356167 -0.002535972 -0.0025358649][-0.002461334 -0.0025096631 -0.0025300614 -0.0025321858 -0.0025164031 -0.0024897496 -0.0024716624 -0.002478472 -0.0025038046 -0.002524442 -0.0025346451 -0.0025377304 -0.0025376265 -0.0025370407 -0.0025362195][-0.0023880526 -0.0024845649 -0.0025251873 -0.0025354254 -0.002526765 -0.00250749 -0.0024934639 -0.0024964125 -0.002514187 -0.0025308502 -0.0025399069 -0.0025429272 -0.0025422317 -0.0025401823 -0.0025380447][-0.0021902651 -0.0024236119 -0.0025187766 -0.0025421435 -0.002541773 -0.0025351907 -0.002529796 -0.0025309005 -0.0025381737 -0.0025456017 -0.0025494997 -0.0025498734 -0.0025478043 -0.0025445197 -0.0025407637][-0.0017214125 -0.0022395388 -0.0024713397 -0.0025400459 -0.0025512809 -0.0025517691 -0.0025517291 -0.0025529529 -0.0025559973 -0.0025580705 -0.0025578879 -0.002555836 -0.0025521105 -0.0025476411 -0.0025430326][-0.00096004771 -0.001924475 -0.0023812752 -0.002525598 -0.0025525778 -0.0025566872 -0.0025584891 -0.0025603585 -0.0025625583 -0.002563169 -0.0025613913 -0.002558118 -0.0025537673 -0.0025489144 -0.0025444545][0.0001291798 -0.0013734201 -0.0021878695 -0.002480987 -0.0025467838 -0.002554948 -0.0025565834 -0.0025589808 -0.0025614907 -0.0025624696 -0.0025605354 -0.0025571431 -0.0025530637 -0.0025485472 -0.0025445477][0.0012720325 -0.00068886415 -0.0019087656 -0.0024023978 -0.0025322733 -0.0025509207 -0.0025526332 -0.0025551838 -0.0025581839 -0.0025594195 -0.0025582425 -0.0025551058 -0.0025510616 -0.0025472408 -0.0025438773][0.0021070694 -0.00010998431 -0.0016440412 -0.0023200484 -0.0025160224 -0.0025471519 -0.0025489535 -0.0025508008 -0.0025537251 -0.002555155 -0.0025542628 -0.0025515752 -0.0025486527 -0.0025456348 -0.0025429358]]...]
INFO - root - 2017-12-09 09:08:21.332123: step 10310, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 74h:20m:52s remains)
INFO - root - 2017-12-09 09:08:30.051831: step 10320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:52m:48s remains)
INFO - root - 2017-12-09 09:08:38.799020: step 10330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:34m:27s remains)
INFO - root - 2017-12-09 09:08:47.468509: step 10340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 78h:03m:45s remains)
INFO - root - 2017-12-09 09:08:56.202795: step 10350, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 81h:02m:53s remains)
INFO - root - 2017-12-09 09:09:04.815776: step 10360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:17m:46s remains)
INFO - root - 2017-12-09 09:09:13.432172: step 10370, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:35m:45s remains)
INFO - root - 2017-12-09 09:09:22.243180: step 10380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:16m:48s remains)
INFO - root - 2017-12-09 09:09:30.820723: step 10390, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 76h:22m:44s remains)
INFO - root - 2017-12-09 09:09:39.492647: step 10400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 77h:53m:36s remains)
2017-12-09 09:09:40.396514: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0050085108 0.0071449177 0.010280597 0.013636169 0.016465463 0.017896552 0.017560866 0.01565567 0.012716121 0.00962814 0.0067882473 0.004417635 0.0025270595 0.0010880968 -2.3976201e-05][0.010902647 0.014602111 0.019727953 0.025152072 0.029592693 0.031586856 0.030510515 0.026674844 0.021083143 0.015252511 0.010119367 0.0062357532 0.0035768305 0.0018741949 0.00072220666][0.019776538 0.026586536 0.03526324 0.044118553 0.051168911 0.054317661 0.052640431 0.046556447 0.037552219 0.027761668 0.018741198 0.011580679 0.0065998044 0.0034743138 0.0016274541][0.031785496 0.042935658 0.056107383 0.06901525 0.0789917 0.083531171 0.081531659 0.073566638 0.061364744 0.047327138 0.033693705 0.02214627 0.013450508 0.0074457126 0.0036571687][0.04582407 0.061874542 0.079605862 0.096156828 0.10842351 0.11387404 0.11149874 0.10207737 0.087467834 0.07024996 0.052735757 0.03696426 0.024183329 0.014654002 0.0081729162][0.060753766 0.081600793 0.10324661 0.12250359 0.13620715 0.14221762 0.13966094 0.12932296 0.11299521 0.093290806 0.072642647 0.053356864 0.036938619 0.024007168 0.0146617][0.074892931 0.099508911 0.12354837 0.14384082 0.15778692 0.16404229 0.16185226 0.15182181 0.13528192 0.1145616 0.091969512 0.069973625 0.050448194 0.034486711 0.022483237][0.087133527 0.11365771 0.13812834 0.1577836 0.17089458 0.1769252 0.17534606 0.16648932 0.15114568 0.13106123 0.10825217 0.085107565 0.063690566 0.045411617 0.031158138][0.094901316 0.12116058 0.14418994 0.16201846 0.17376131 0.17946222 0.17876035 0.17168123 0.15855303 0.14047821 0.1191038 0.096583828 0.074855916 0.055569239 0.039977584][0.097097 0.12141564 0.14193316 0.15746935 0.16771555 0.17310159 0.17336689 0.16820562 0.15766647 0.14239869 0.12361391 0.10313259 0.082675822 0.063812263 0.048127737][0.093473442 0.11491682 0.13261896 0.14599112 0.15505065 0.16023816 0.16130124 0.15778448 0.14966063 0.13722183 0.12148192 0.1038532 0.085772529 0.06863784 0.05409921][0.084549949 0.10268702 0.1176839 0.12927967 0.13746308 0.14248425 0.14412904 0.14194338 0.13590303 0.12623052 0.11371745 0.099349052 0.084355824 0.069786042 0.057279993][0.072595142 0.086986832 0.098960243 0.10851917 0.11561566 0.12029684 0.1223001 0.1211701 0.1169441 0.10986754 0.10064564 0.089902587 0.078486063 0.067197211 0.05746318][0.059422944 0.070164047 0.079186864 0.086604565 0.092338189 0.096346356 0.098455958 0.098255105 0.095734455 0.091139153 0.084991783 0.07774131 0.0699225 0.062083576 0.055271253][0.047005024 0.054430734 0.060628969 0.065813117 0.069972955 0.073106527 0.075104214 0.075617731 0.0746329 0.072280079 0.068952866 0.064822577 0.060300048 0.05569483 0.051566593]]...]
INFO - root - 2017-12-09 09:09:49.101537: step 10410, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:05m:05s remains)
INFO - root - 2017-12-09 09:09:57.740718: step 10420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:16m:40s remains)
INFO - root - 2017-12-09 09:10:06.482019: step 10430, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 81h:16m:04s remains)
INFO - root - 2017-12-09 09:10:15.100102: step 10440, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 77h:42m:50s remains)
INFO - root - 2017-12-09 09:10:23.934470: step 10450, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:23m:44s remains)
INFO - root - 2017-12-09 09:10:32.460133: step 10460, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:28m:49s remains)
INFO - root - 2017-12-09 09:10:41.119640: step 10470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:13m:44s remains)
INFO - root - 2017-12-09 09:10:49.769156: step 10480, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 76h:45m:05s remains)
INFO - root - 2017-12-09 09:10:58.169308: step 10490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:04m:08s remains)
INFO - root - 2017-12-09 09:11:06.661411: step 10500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:19m:16s remains)
2017-12-09 09:11:07.530550: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013557847 0.010254811 0.0076527027 0.0062741628 0.0068316692 0.0095504876 0.01617931 0.02484964 0.035095308 0.043317307 0.050746117 0.060106564 0.0753717 0.099479362 0.12807705][0.020976067 0.016020644 0.012426727 0.010300625 0.01020726 0.013267227 0.020835984 0.03120809 0.044507645 0.057860833 0.071245126 0.085977532 0.10220186 0.12496116 0.1495166][0.032104835 0.024267258 0.018563023 0.015324178 0.015247696 0.0188931 0.028005453 0.04191909 0.060066219 0.078927085 0.098386638 0.11706465 0.13300554 0.15199225 0.1701241][0.047748368 0.036122736 0.02751267 0.022803951 0.022972789 0.028714683 0.041345838 0.05981192 0.082380593 0.10634647 0.12996484 0.15097958 0.1654931 0.17919636 0.19067232][0.066894338 0.051675141 0.040264815 0.034078643 0.034698382 0.042950746 0.059522882 0.082399189 0.10905927 0.13602708 0.16141407 0.18223196 0.19411452 0.20275745 0.20786372][0.085454106 0.067960165 0.055132408 0.048408397 0.049447536 0.059918713 0.079951055 0.10687469 0.13642807 0.16460191 0.18830772 0.20599905 0.21347754 0.21655847 0.21557471][0.09672223 0.079922982 0.068580948 0.063024826 0.065064706 0.077216908 0.099163793 0.12830265 0.15866168 0.18542972 0.20539916 0.21791033 0.22003351 0.21724546 0.21075866][0.096748814 0.0829728 0.075217538 0.072975457 0.077304788 0.090935983 0.11374535 0.14307642 0.17150751 0.1945684 0.20927151 0.21544656 0.21309467 0.2052184 0.19454876][0.08539328 0.0759434 0.072563782 0.073998988 0.080572546 0.095664687 0.11859567 0.14634159 0.17133304 0.18945481 0.19840942 0.19865359 0.19196299 0.18051688 0.16765885][0.065934159 0.060825329 0.061301563 0.065571129 0.073529884 0.08853472 0.10977477 0.13455305 0.15531644 0.1683566 0.17216313 0.16782787 0.15818001 0.14500217 0.13129404][0.0437945 0.042169988 0.044999495 0.050550256 0.05853644 0.071966529 0.089970656 0.11031073 0.1260926 0.13440946 0.13448134 0.12771122 0.11719132 0.10449708 0.09219645][0.024314964 0.024814459 0.028410062 0.033773676 0.040669277 0.051377185 0.065189734 0.080160573 0.090878695 0.095440567 0.093568414 0.0866689 0.077293038 0.06686157 0.057427164][0.010296003 0.011422359 0.014387949 0.018590014 0.023835273 0.031654745 0.041390337 0.0512967 0.057722628 0.059610583 0.057087172 0.051519208 0.044596665 0.037326142 0.031069163][0.0020796729 0.0029257073 0.0046936991 0.0073014563 0.010695429 0.015730582 0.021767762 0.027594021 0.031074338 0.03150196 0.029271867 0.025426965 0.020998828 0.016695248 0.013130312][-0.0015263071 -0.001162534 -0.000415331 0.00080572837 0.0025546255 0.0051834071 0.0082357675 0.011034116 0.01250993 0.012367853 0.010927145 0.0088042449 0.0065287761 0.0044764094 0.0028346728]]...]
INFO - root - 2017-12-09 09:11:16.045737: step 10510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:28m:22s remains)
INFO - root - 2017-12-09 09:11:24.655230: step 10520, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:06m:56s remains)
INFO - root - 2017-12-09 09:11:33.206405: step 10530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:06m:51s remains)
INFO - root - 2017-12-09 09:11:41.756620: step 10540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:19m:10s remains)
INFO - root - 2017-12-09 09:11:50.422124: step 10550, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 74h:46m:54s remains)
INFO - root - 2017-12-09 09:11:59.039796: step 10560, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:58m:13s remains)
INFO - root - 2017-12-09 09:12:07.736136: step 10570, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 78h:55m:06s remains)
INFO - root - 2017-12-09 09:12:16.461990: step 10580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 79h:10m:04s remains)
INFO - root - 2017-12-09 09:12:25.099397: step 10590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:26m:05s remains)
INFO - root - 2017-12-09 09:12:33.741285: step 10600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:44m:05s remains)
2017-12-09 09:12:34.651278: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0043078121 0.0084973769 0.013826791 0.019154694 0.023465035 0.026346432 0.027822427 0.027755998 0.026393564 0.02391935 0.021178747 0.018501481 0.016131509 0.014125509 0.012473568][0.003044398 0.00673901 0.01157646 0.01664691 0.021030424 0.024057509 0.025599603 0.025318729 0.023350075 0.020079352 0.016330345 0.012792962 0.0099341013 0.0078252032 0.0063105691][0.0012770332 0.00409899 0.0079020848 0.012117065 0.016112477 0.019243371 0.021193909 0.021397553 0.019678541 0.01625292 0.011964423 0.0077769635 0.0044858996 0.0023608212 0.0011360908][-0.00025385851 0.0017371108 0.0045485273 0.0079450784 0.011554263 0.014838686 0.017290395 0.018184917 0.017124541 0.014100438 0.0098069077 0.0053281197 0.0017306758 -0.0004629551 -0.0014816375][-0.0012928095 -4.1916035e-05 0.0018683551 0.0045183171 0.0077718133 0.01122511 0.014265373 0.016012322 0.01581314 0.013445087 0.0094156591 0.0048296861 0.00096614892 -0.0013599723 -0.00230013][-0.0018245846 -0.0011205642 6.9300178e-05 0.0019834167 0.0047514876 0.0082178228 0.01177643 0.014420429 0.015199929 0.013597347 0.0099143712 0.0052916259 0.0011950866 -0.0013418281 -0.0023552566][-0.0020918536 -0.0017428016 -0.001105549 0.00011808262 0.0022364166 0.0053385692 0.0090256929 0.012304073 0.013946118 0.013114154 0.0099055842 0.0054520462 0.0013365359 -0.0012668108 -0.0023252382][-0.0022865059 -0.0020773311 -0.0017435665 -0.001060842 0.00033044536 0.0027310974 0.0060363617 0.00938145 0.011469091 0.011245634 0.0086154044 0.0046523148 0.000930537 -0.0014132591 -0.0023507974][-0.0024753243 -0.002361821 -0.0021568798 -0.0017295575 -0.00084922148 0.00079887826 0.0032935473 0.0060317069 0.0079483949 0.0080180932 0.0060391165 0.0029158853 2.2063032e-06 -0.0017649257 -0.0024231381][-0.0025495563 -0.0025079416 -0.0024113464 -0.0021661555 -0.0016173925 -0.00057614734 0.0010425982 0.0028996924 0.0042316434 0.0043044784 0.0029485375 0.00082611851 -0.0010720027 -0.0021432422 -0.0024965254][-0.0025733234 -0.0025629546 -0.0025263347 -0.0024093485 -0.0021157661 -0.0015317335 -0.00063878181 0.00038228557 0.0010980894 0.0010997059 0.00029525184 -0.00088998757 -0.0018817207 -0.0023883083 -0.0025288155][-0.0025752743 -0.0025735449 -0.0025662126 -0.0025295231 -0.0024213269 -0.0021894795 -0.0018166819 -0.0013832155 -0.0010906201 -0.0011073132 -0.0014585479 -0.0019472829 -0.002329079 -0.0025057744 -0.0025455188][-0.0025746559 -0.0025735786 -0.002573282 -0.0025666228 -0.0025428631 -0.0024869889 -0.0023930364 -0.002281965 -0.0022055139 -0.0022144073 -0.0023099491 -0.0024336076 -0.002518259 -0.0025503221 -0.0025505736][-0.0025736236 -0.0025726457 -0.0025737071 -0.002574425 -0.002574587 -0.002572848 -0.0025686459 -0.0025632738 -0.0025592116 -0.0025587862 -0.0025599927 -0.0025613103 -0.0025623676 -0.002561532 -0.0025527808][-0.0025730112 -0.0025718242 -0.0025729358 -0.0025740569 -0.002575373 -0.0025757139 -0.0025746622 -0.0025728943 -0.0025713276 -0.0025711975 -0.0025713311 -0.0025705996 -0.0025694233 -0.0025675264 -0.0025624589]]...]
INFO - root - 2017-12-09 09:12:43.232119: step 10610, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 77h:35m:22s remains)
INFO - root - 2017-12-09 09:12:51.923888: step 10620, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 82h:37m:03s remains)
INFO - root - 2017-12-09 09:13:00.629984: step 10630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:11m:30s remains)
INFO - root - 2017-12-09 09:13:09.275135: step 10640, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 77h:22m:55s remains)
INFO - root - 2017-12-09 09:13:17.987184: step 10650, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.909 sec/batch; 81h:13m:42s remains)
INFO - root - 2017-12-09 09:13:26.449927: step 10660, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:43m:57s remains)
INFO - root - 2017-12-09 09:13:34.990270: step 10670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:20m:06s remains)
INFO - root - 2017-12-09 09:13:43.506903: step 10680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 77h:52m:52s remains)
INFO - root - 2017-12-09 09:13:52.116717: step 10690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:07m:29s remains)
INFO - root - 2017-12-09 09:14:00.810222: step 10700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:21m:50s remains)
2017-12-09 09:14:01.689273: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018153353 0.020692647 0.022890268 0.024652177 0.02597302 0.026757054 0.027143335 0.027103551 0.027032083 0.027007284 0.026974207 0.02721661 0.027505392 0.027654333 0.027413705][0.020573542 0.022671722 0.024376459 0.025734453 0.026831586 0.027626516 0.027900808 0.027870286 0.02782499 0.027638331 0.027515436 0.027634224 0.02779286 0.027909305 0.027728651][0.021674015 0.023322372 0.024630636 0.025923453 0.027014302 0.027896658 0.028217131 0.028203445 0.027992493 0.02766702 0.027508467 0.027393255 0.027525788 0.027837468 0.027752375][0.021121912 0.022671385 0.023814686 0.025108121 0.0263093 0.027414594 0.02804099 0.028020883 0.027838022 0.027477201 0.027238635 0.027074857 0.027217375 0.027496893 0.027563352][0.019755859 0.021268466 0.022356275 0.023706239 0.024880087 0.02595821 0.026604589 0.026804205 0.026399503 0.026139218 0.025891079 0.02585016 0.026087768 0.026486577 0.027028028][0.018355237 0.019860132 0.020888582 0.022060849 0.023186369 0.024179377 0.024867851 0.025169875 0.025128674 0.024783751 0.024436425 0.024328176 0.024390802 0.024670793 0.025092589][0.016981974 0.018476447 0.0195591 0.020600488 0.021637442 0.0225327 0.023180975 0.023490421 0.023634559 0.023587527 0.023397023 0.023232786 0.023212405 0.023277383 0.023325428][0.015211621 0.016829168 0.018049322 0.019152069 0.020097742 0.020961415 0.021606803 0.021962393 0.022143388 0.02220558 0.022129495 0.022046845 0.022026189 0.021935716 0.021777479][0.012939567 0.014527954 0.015821472 0.017080696 0.018199712 0.01904905 0.019751508 0.020269116 0.020585189 0.020790527 0.020768721 0.020796509 0.020725289 0.020644031 0.020417681][0.0091367159 0.010793462 0.012267334 0.013714917 0.015082182 0.016346086 0.017310856 0.018198201 0.018967867 0.019486662 0.01969474 0.019728323 0.019719709 0.019528661 0.019187765][0.0044614603 0.0059150173 0.0074523105 0.009158317 0.010850685 0.012486551 0.014021927 0.015492504 0.016740695 0.017719446 0.018284144 0.018518509 0.018513778 0.018305246 0.017948477][0.00041421643 0.0013755029 0.0025681765 0.0040558232 0.0057651852 0.0076471339 0.0096737407 0.011672657 0.013467897 0.014957875 0.016091568 0.016848557 0.017121883 0.017056581 0.016808297][-0.0014758528 -0.0011412969 -0.00062516215 0.00023136172 0.0014345716 0.0030218265 0.0050247437 0.0072292075 0.0094654243 0.011618314 0.01330144 0.014650041 0.015422333 0.0156808 0.015593993][-0.0021454939 -0.0020527737 -0.0018817531 -0.0016022625 -0.0010861099 -0.00015179627 0.0012907041 0.0032239831 0.0054944376 0.0079025943 0.010123009 0.011968534 0.013284301 0.013892137 0.014031496][-0.0023690108 -0.0022978131 -0.0021820585 -0.0020348669 -0.0018473056 -0.0015099196 -0.00079006364 0.000512945 0.0023177681 0.0044547673 0.0067737685 0.009021502 0.010768499 0.01180041 0.012271924]]...]
INFO - root - 2017-12-09 09:14:10.367024: step 10710, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 76h:56m:51s remains)
INFO - root - 2017-12-09 09:14:19.150125: step 10720, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 74h:29m:35s remains)
INFO - root - 2017-12-09 09:14:27.853725: step 10730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:26m:42s remains)
INFO - root - 2017-12-09 09:14:36.622443: step 10740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:55m:05s remains)
INFO - root - 2017-12-09 09:14:45.398981: step 10750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:49m:43s remains)
INFO - root - 2017-12-09 09:14:53.937820: step 10760, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 77h:57m:48s remains)
INFO - root - 2017-12-09 09:15:02.652007: step 10770, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:10m:27s remains)
INFO - root - 2017-12-09 09:15:11.266365: step 10780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:27m:54s remains)
INFO - root - 2017-12-09 09:15:19.797644: step 10790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:47m:03s remains)
INFO - root - 2017-12-09 09:15:28.471843: step 10800, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:57m:19s remains)
2017-12-09 09:15:29.341600: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12726402 0.12733141 0.12803072 0.1292537 0.13000442 0.12960947 0.12777607 0.12411982 0.11888687 0.11239669 0.10589524 0.098483145 0.089373857 0.076592013 0.060078152][0.13600507 0.1364982 0.13715665 0.13830188 0.13930729 0.13935222 0.13799006 0.13503504 0.13029903 0.12438154 0.11853778 0.11187147 0.10328715 0.0901501 0.072206713][0.13664815 0.13797383 0.13895532 0.14005229 0.14101335 0.14121389 0.14015044 0.13748154 0.13304275 0.12770694 0.12271097 0.11730794 0.10991472 0.097540878 0.079595283][0.13418213 0.13641267 0.137622 0.13856354 0.13903196 0.13863277 0.13718268 0.13444498 0.13030387 0.12570542 0.1218808 0.11814018 0.11240014 0.10136502 0.084163867][0.12755993 0.13120201 0.13295205 0.13372202 0.133559 0.13250077 0.13060378 0.12777223 0.12407695 0.1205384 0.11806586 0.11586289 0.11155232 0.10199144 0.086051][0.11496245 0.12060335 0.12370564 0.1251578 0.12494509 0.12362508 0.12161627 0.11914622 0.11635929 0.11408391 0.11272693 0.11146296 0.10796038 0.099407747 0.084656343][0.096042804 0.10342479 0.10800219 0.11084785 0.11169551 0.11130564 0.11021838 0.10904191 0.10786576 0.10703146 0.10652977 0.10548377 0.10179838 0.093325719 0.07933718][0.072607145 0.080613256 0.086423889 0.090498708 0.092473343 0.093542896 0.094124556 0.094890207 0.095755927 0.09674374 0.09733817 0.096414864 0.092278823 0.0835592 0.070171282][0.04779416 0.055214938 0.061185606 0.065813251 0.068705872 0.070886008 0.072856978 0.075438768 0.078258783 0.080875732 0.082494646 0.081925437 0.077853292 0.069457188 0.057259828][0.025691379 0.0312421 0.036268976 0.040459529 0.04350473 0.046125319 0.048875757 0.052359261 0.056234583 0.059878305 0.062328272 0.062415041 0.059086263 0.051973414 0.041972008][0.010265644 0.01355866 0.01682408 0.019824494 0.022299947 0.024595134 0.027202383 0.030561978 0.034394935 0.038024239 0.040645145 0.0412761 0.039084855 0.033904832 0.0266574][0.0015710664 0.0030394418 0.0046608504 0.0063353088 0.0078981612 0.0094664795 0.011335528 0.013797384 0.016704272 0.01955119 0.021771055 0.022654925 0.02161661 0.018571263 0.01419308][-0.0019576848 -0.0015789116 -0.001065545 -0.00043773698 0.00025789835 0.0010462559 0.0020554324 0.0034000741 0.0050380975 0.0067046229 0.0080715818 0.00874186 0.0083950469 0.0070367176 0.0050007105][-0.0025496343 -0.0025389686 -0.0025094436 -0.002447169 -0.0023182111 -0.0021172215 -0.0018146154 -0.0013756771 -0.000777912 -0.00012401538 0.00042269379 0.00069475546 0.00060984748 0.00020874874 -0.00041104783][-0.0025535137 -0.0025484057 -0.0025453684 -0.0025439444 -0.0025399206 -0.0025356694 -0.0025186064 -0.0024795418 -0.0023987712 -0.0022933751 -0.002197599 -0.00214594 -0.0021457144 -0.0021896227 -0.0022696264]]...]
INFO - root - 2017-12-09 09:15:37.995232: step 10810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:43m:36s remains)
INFO - root - 2017-12-09 09:15:46.653034: step 10820, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:54m:48s remains)
INFO - root - 2017-12-09 09:15:55.372218: step 10830, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 79h:05m:29s remains)
INFO - root - 2017-12-09 09:16:03.979390: step 10840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:29m:21s remains)
INFO - root - 2017-12-09 09:16:12.795513: step 10850, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:13m:39s remains)
INFO - root - 2017-12-09 09:16:21.294283: step 10860, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 75h:13m:42s remains)
INFO - root - 2017-12-09 09:16:30.040747: step 10870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:20m:28s remains)
INFO - root - 2017-12-09 09:16:38.560985: step 10880, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:21m:29s remains)
INFO - root - 2017-12-09 09:16:47.024476: step 10890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:22m:08s remains)
INFO - root - 2017-12-09 09:16:55.904243: step 10900, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:32m:37s remains)
2017-12-09 09:16:56.793742: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038520798 0.036711633 0.036059558 0.035230141 0.033524383 0.030807508 0.027736714 0.024575869 0.021507338 0.018277595 0.014775753 0.010882352 0.0071942536 0.0038407499 0.0012031083][0.061510876 0.05982906 0.059218068 0.05805783 0.055444226 0.051303964 0.046250388 0.04068185 0.034935594 0.028948022 0.022588687 0.016214773 0.010518111 0.0057408717 0.0021497139][0.088891454 0.08800213 0.087944575 0.086922526 0.084018886 0.079214245 0.073170267 0.06622196 0.058156952 0.048992939 0.038680743 0.028172305 0.018624393 0.010758759 0.004887078][0.11507764 0.11465628 0.11522087 0.11485747 0.11268719 0.10846575 0.10264259 0.095439143 0.086281434 0.075094521 0.061464362 0.046663269 0.032251548 0.019739121 0.009971261][0.13795282 0.13649935 0.13655601 0.13645583 0.13544613 0.13300006 0.12906864 0.12317364 0.11421759 0.10188952 0.085754007 0.067396939 0.048457064 0.031112729 0.0168137][0.15242137 0.14854322 0.14719822 0.14745162 0.14842673 0.14899077 0.14809646 0.14456002 0.13703971 0.12492019 0.10776069 0.087039806 0.064407244 0.042751335 0.024058035][0.15311222 0.14595379 0.14215155 0.14231052 0.14595108 0.1509773 0.15509278 0.15612581 0.15214035 0.14205624 0.1253458 0.10340272 0.078002147 0.05269286 0.030180907][0.14023657 0.12894213 0.1222996 0.12223981 0.12816778 0.13731191 0.14685781 0.15381658 0.15544143 0.14996499 0.13641091 0.11554059 0.089033067 0.061144106 0.035576031][0.12118059 0.10552319 0.095198467 0.09359394 0.10028957 0.11206776 0.12582098 0.13817847 0.14605381 0.14674646 0.13836914 0.12093047 0.095691755 0.06720864 0.040014353][0.10348944 0.084405862 0.070477977 0.066518471 0.072016582 0.0837905 0.099281587 0.11536141 0.12867908 0.13561188 0.1332486 0.12050071 0.098113939 0.070799723 0.043555103][0.091134459 0.069490679 0.05177553 0.044542205 0.047715865 0.058314715 0.074178338 0.09247876 0.1097911 0.12195805 0.12519729 0.11739618 0.098864913 0.0740052 0.047833864][0.086678386 0.063657 0.042881645 0.03224745 0.032239687 0.040543243 0.055560112 0.074687012 0.094402321 0.11006439 0.11731628 0.11335188 0.0982354 0.075905405 0.051199988][0.087122217 0.065107167 0.043614518 0.03100224 0.0280397 0.033407312 0.046046373 0.063879617 0.083631553 0.10067738 0.11044428 0.10955406 0.097748555 0.07819438 0.055208605][0.088934913 0.069515973 0.049041864 0.035906374 0.031251922 0.034295186 0.044201937 0.059347056 0.077085748 0.093302965 0.10359555 0.10452728 0.095518723 0.0788092 0.05786971][0.09079463 0.074593335 0.056133159 0.043433733 0.037764035 0.038782284 0.045730233 0.057159439 0.070984147 0.084010154 0.092709355 0.094085343 0.087304682 0.073700979 0.055687986]]...]
INFO - root - 2017-12-09 09:17:05.516031: step 10910, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 78h:29m:29s remains)
INFO - root - 2017-12-09 09:17:14.194256: step 10920, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 75h:23m:08s remains)
INFO - root - 2017-12-09 09:17:22.989952: step 10930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:02m:10s remains)
INFO - root - 2017-12-09 09:17:31.523863: step 10940, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 74h:29m:09s remains)
INFO - root - 2017-12-09 09:17:40.172193: step 10950, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.823 sec/batch; 73h:32m:54s remains)
INFO - root - 2017-12-09 09:17:48.649568: step 10960, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:41m:41s remains)
INFO - root - 2017-12-09 09:17:57.139342: step 10970, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:40m:54s remains)
INFO - root - 2017-12-09 09:18:05.726643: step 10980, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 80h:13m:35s remains)
INFO - root - 2017-12-09 09:18:14.299330: step 10990, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:18m:48s remains)
INFO - root - 2017-12-09 09:18:22.929916: step 11000, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:32m:58s remains)
2017-12-09 09:18:23.900650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026135377 -0.0025958817 -0.0025500066 -0.002464368 -0.0023436556 -0.0022207126 -0.002136047 -0.0021108561 -0.0021613797 -0.0022782336 -0.0024263381 -0.0025449102 -0.0026017779 -0.0026120122 -0.0026130083][-0.0025904176 -0.0025236169 -0.0023835523 -0.0021645292 -0.0019135904 -0.0017228578 -0.0016627453 -0.0017242229 -0.0018823633 -0.0021049059 -0.0023401505 -0.0025149686 -0.0025947881 -0.0026098923 -0.0026110592][-0.0025112035 -0.0023002662 -0.0019078256 -0.0013653897 -0.00083293009 -0.00052113039 -0.00053528184 -0.000805647 -0.0012226909 -0.0016943755 -0.0021284933 -0.0024300811 -0.0025726459 -0.002607936 -0.0026111393][-0.0023338937 -0.0018309485 -0.00095810229 0.00016410416 0.0011771095 0.0017153507 0.0016434269 0.0011010987 0.00026389956 -0.00068693515 -0.0015691527 -0.002183361 -0.0025013036 -0.0025983262 -0.0026104269][-0.0020399047 -0.0011354072 0.00037958543 0.0022509017 0.003871476 0.0047288816 0.0047004223 0.0039787861 0.0027160172 0.0011369884 -0.00045829266 -0.0016511333 -0.0023301332 -0.0025677886 -0.0026071086][-0.0016840377 -0.00040691206 0.0017070777 0.0042702276 0.0064527281 0.0076472918 0.0077847792 0.0070910649 0.0055862525 0.0034460209 0.0010605759 -0.00086662464 -0.0020520757 -0.0025105986 -0.0026007821][-0.001401804 5.5643264e-05 0.0024861235 0.0054326067 0.0079519665 0.0093999412 0.0097715193 0.0093049742 0.0078427438 0.0054320367 0.0024818464 -7.3733972e-05 -0.0017419176 -0.0024387268 -0.0025934426][-0.0013341261 3.5561156e-05 0.0023625598 0.0052362205 0.0077629136 0.009316029 0.0099020125 0.0097299339 0.0085231271 0.00620629 0.0031511893 0.00035671634 -0.0015442614 -0.0023849751 -0.0025892064][-0.0015375329 -0.00047535636 0.0013807451 0.0037598147 0.0059588971 0.0074403314 0.0081536947 0.0082315225 0.0073667075 0.0054310709 0.0027446237 0.00020290748 -0.0015671758 -0.0023797625 -0.002588989][-0.0019084059 -0.0012334154 -1.8903986e-05 0.0016276713 0.0032573265 0.0044808798 0.0051856469 0.0054046242 0.0048707183 0.0034714027 0.001467291 -0.00045502395 -0.0018002 -0.0024273896 -0.0025936575][-0.0022721377 -0.0019278368 -0.0012901835 -0.00036044861 0.00063601043 0.0014765693 0.0020387049 0.0022820963 0.0020114242 0.0011534258 -0.00010020891 -0.0012962922 -0.002120212 -0.002499921 -0.0026002016][-0.002503288 -0.0023706232 -0.0021133558 -0.0017047024 -0.0012256926 -0.00076855742 -0.00041639386 -0.00023169722 -0.00033433316 -0.00076358148 -0.0014009894 -0.0019958066 -0.0023889774 -0.002562555 -0.0026070692][-0.00259767 -0.0025629979 -0.002489584 -0.0023602063 -0.0021913496 -0.0020068411 -0.0018432789 -0.0017443418 -0.0017687065 -0.0019335145 -0.002182344 -0.0024056584 -0.002542783 -0.0025981653 -0.0026115221][-0.0026168919 -0.0026128872 -0.00260076 -0.0025755323 -0.0025372421 -0.0024870171 -0.0024352944 -0.002400456 -0.0024028218 -0.0024469309 -0.0025137088 -0.002570461 -0.002600912 -0.0026109002 -0.0026126208][-0.0026153522 -0.0026147487 -0.0026142704 -0.0026123377 -0.0026080082 -0.0026010121 -0.0025925331 -0.002586646 -0.00258687 -0.0025938675 -0.0026038038 -0.0026114981 -0.0026140381 -0.0026137275 -0.0026131524]]...]
INFO - root - 2017-12-09 09:18:32.438416: step 11010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:01m:54s remains)
INFO - root - 2017-12-09 09:18:41.020043: step 11020, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 78h:17m:26s remains)
INFO - root - 2017-12-09 09:18:49.632554: step 11030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:05m:05s remains)
INFO - root - 2017-12-09 09:18:58.160081: step 11040, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 70h:12m:38s remains)
INFO - root - 2017-12-09 09:19:06.907334: step 11050, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:00m:54s remains)
INFO - root - 2017-12-09 09:19:15.380246: step 11060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:49m:43s remains)
INFO - root - 2017-12-09 09:19:24.032581: step 11070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:04m:28s remains)
INFO - root - 2017-12-09 09:19:32.474061: step 11080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:19m:19s remains)
INFO - root - 2017-12-09 09:19:40.888980: step 11090, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 74h:57m:47s remains)
INFO - root - 2017-12-09 09:19:49.660099: step 11100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:17m:03s remains)
2017-12-09 09:19:50.531519: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27299947 0.27069396 0.26463714 0.25665957 0.24808554 0.24076785 0.23381881 0.22984277 0.22799525 0.22777912 0.22997896 0.23393673 0.24021067 0.24701773 0.25208268][0.27625096 0.27597556 0.27048066 0.2625356 0.25340277 0.24633083 0.24028988 0.23769169 0.23856789 0.24108249 0.24563661 0.2501668 0.25681216 0.26323134 0.26816967][0.2656517 0.26605085 0.26088691 0.25328562 0.24374303 0.23652546 0.2318763 0.23194374 0.2359806 0.24239713 0.25050709 0.25817251 0.26599556 0.27250528 0.27697632][0.25008184 0.25097683 0.24566692 0.23814504 0.22933213 0.22310038 0.2200447 0.22348623 0.23124871 0.24166679 0.2537353 0.26495764 0.27496123 0.28198159 0.28617567][0.22783637 0.22997451 0.22584295 0.21954757 0.21148302 0.20672259 0.20652094 0.21338229 0.22430415 0.23859473 0.25400972 0.26843932 0.2805523 0.28824219 0.29252377][0.1983519 0.20232043 0.20043372 0.19701928 0.1917119 0.1894511 0.19132154 0.20061524 0.21440859 0.23121224 0.24852236 0.26518589 0.27836561 0.28619727 0.29009396][0.16185549 0.16904964 0.17094539 0.17112269 0.16977662 0.17129232 0.17563066 0.18641122 0.200492 0.21786016 0.23578012 0.25250185 0.26511183 0.27244163 0.27549723][0.12299483 0.1319418 0.1381052 0.14331445 0.14692818 0.15231845 0.15951967 0.17181441 0.18506169 0.20076704 0.21609749 0.23016591 0.24056225 0.2456466 0.24665762][0.0848663 0.095671326 0.10507617 0.11355857 0.12133036 0.13079146 0.14020295 0.15226516 0.16326194 0.17669097 0.18852779 0.19829647 0.20461749 0.20634088 0.20564175][0.051887631 0.062657453 0.073492266 0.083931811 0.093896732 0.10425393 0.11409798 0.12495089 0.13375129 0.14357491 0.15115084 0.15757126 0.16065902 0.15981896 0.1569792][0.027861806 0.036792226 0.046721525 0.056948021 0.066886045 0.076576449 0.085209548 0.0934875 0.099644944 0.10593954 0.11034613 0.1136151 0.11416739 0.11227078 0.10871036][0.012130853 0.018470198 0.026123598 0.034483496 0.04278066 0.050671645 0.0578824 0.064042725 0.067756839 0.070714936 0.072379515 0.073262073 0.072431542 0.069805041 0.066380791][0.003150902 0.0066333488 0.01138373 0.0170464 0.02319487 0.029178202 0.034627486 0.038812961 0.041087553 0.042145003 0.041858979 0.040765218 0.038625322 0.035927489 0.033096883][-0.0010182181 0.00038793054 0.0026625849 0.0057199765 0.0092856735 0.012845086 0.01607237 0.018442385 0.019682333 0.019703174 0.018734632 0.017194325 0.015190975 0.013043039 0.011035147][-0.0024232222 -0.0020532273 -0.0013120404 -0.00014484883 0.0013648428 0.0029277715 0.004302498 0.0052627195 0.0056806915 0.0054947203 0.0047882022 0.0037782888 0.0026390164 0.0015431119 0.00061282585]]...]
INFO - root - 2017-12-09 09:19:59.222932: step 11110, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 78h:02m:06s remains)
INFO - root - 2017-12-09 09:20:07.915929: step 11120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:31m:54s remains)
INFO - root - 2017-12-09 09:20:16.613570: step 11130, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:58m:18s remains)
INFO - root - 2017-12-09 09:20:25.279945: step 11140, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:37m:14s remains)
INFO - root - 2017-12-09 09:20:33.862809: step 11150, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 79h:29m:30s remains)
INFO - root - 2017-12-09 09:20:42.399522: step 11160, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:39m:17s remains)
INFO - root - 2017-12-09 09:20:51.258898: step 11170, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 80h:19m:53s remains)
INFO - root - 2017-12-09 09:21:00.005979: step 11180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:43m:29s remains)
INFO - root - 2017-12-09 09:21:08.622012: step 11190, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:06m:09s remains)
INFO - root - 2017-12-09 09:21:17.320320: step 11200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 79h:02m:04s remains)
2017-12-09 09:21:18.196995: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13460355 0.1171587 0.10203488 0.085422494 0.070248835 0.057169434 0.046071008 0.038201623 0.030486174 0.024290211 0.018838981 0.014151546 0.0098509118 0.0058362111 0.0027476873][0.17258507 0.15463701 0.13866384 0.11948496 0.10115641 0.084247284 0.068586044 0.05633007 0.044598076 0.03526023 0.027062157 0.020125275 0.014084528 0.0089468686 0.0049643815][0.2259905 0.2078993 0.19125865 0.17087813 0.15133426 0.13182756 0.11217095 0.095273823 0.078483663 0.063622542 0.049241159 0.036585338 0.025216201 0.015936175 0.0085637048][0.29063523 0.27339554 0.25551015 0.23463537 0.21508932 0.19442403 0.17217657 0.15149081 0.12957175 0.10750899 0.084116876 0.062796481 0.043173485 0.027306557 0.014758742][0.35407814 0.34068212 0.32482538 0.30492866 0.28516573 0.26341525 0.23910244 0.21450251 0.18771723 0.15862332 0.12635016 0.095919296 0.067302033 0.043620668 0.024107009][0.4095543 0.40303218 0.3921448 0.37616938 0.35835829 0.33621737 0.3094013 0.28003713 0.2470964 0.20980731 0.16834036 0.12824646 0.090257108 0.058949754 0.033476327][0.4475261 0.44640672 0.43967718 0.42825547 0.41425863 0.39440611 0.36853111 0.33638635 0.29856235 0.25416845 0.20422629 0.15605934 0.10994894 0.071693927 0.041130174][0.47417337 0.47448295 0.46744981 0.45735654 0.44427383 0.42629728 0.40235904 0.37026334 0.33077148 0.28240487 0.2271156 0.1726366 0.1208216 0.077854887 0.043828722][0.48437175 0.48736015 0.47989714 0.46984923 0.45654625 0.43783394 0.41364756 0.38032213 0.3400619 0.29026043 0.23360543 0.17758211 0.12442963 0.080174811 0.044734847][0.48610213 0.49049819 0.48253459 0.47206604 0.45787203 0.43759695 0.41198123 0.37653506 0.333323 0.28154704 0.22455633 0.16908202 0.11714481 0.074762106 0.04192058][0.49284887 0.49817973 0.48771721 0.47563219 0.46006835 0.43827915 0.41095027 0.37365553 0.32875398 0.27589852 0.21895519 0.16411507 0.11370219 0.0736308 0.043873873][0.5066883 0.51301968 0.49910462 0.48360416 0.46483025 0.43979275 0.40964589 0.36930326 0.32206532 0.26783127 0.21168877 0.1597833 0.11296783 0.077460356 0.052246798][0.52521932 0.53509915 0.52051085 0.50380629 0.4823637 0.4543913 0.42126355 0.37899658 0.330704 0.27636954 0.22137958 0.17004927 0.1244788 0.091055408 0.067861743][0.53341812 0.54644573 0.53316158 0.517097 0.49689406 0.47046658 0.43878934 0.39828727 0.35255215 0.30130941 0.24938643 0.20055068 0.15668415 0.12394953 0.10074817][0.54574156 0.55994838 0.54711103 0.53129834 0.510698 0.48486525 0.45407382 0.41665444 0.37506288 0.32923535 0.2835817 0.240387 0.20148699 0.17022805 0.1468201]]...]
INFO - root - 2017-12-09 09:21:26.936874: step 11210, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 77h:13m:16s remains)
INFO - root - 2017-12-09 09:21:35.612367: step 11220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:08m:00s remains)
INFO - root - 2017-12-09 09:21:44.352143: step 11230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:25m:58s remains)
INFO - root - 2017-12-09 09:21:53.004211: step 11240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:47m:35s remains)
INFO - root - 2017-12-09 09:22:01.384107: step 11250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 74h:40m:46s remains)
INFO - root - 2017-12-09 09:22:09.842069: step 11260, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 75h:45m:21s remains)
INFO - root - 2017-12-09 09:22:18.591121: step 11270, loss = 0.90, batch loss = 0.69 (8.2 examples/sec; 0.972 sec/batch; 86h:46m:12s remains)
INFO - root - 2017-12-09 09:22:27.331041: step 11280, loss = 0.89, batch loss = 0.69 (8.2 examples/sec; 0.980 sec/batch; 87h:25m:28s remains)
INFO - root - 2017-12-09 09:22:35.948429: step 11290, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.924 sec/batch; 82h:29m:08s remains)
INFO - root - 2017-12-09 09:22:44.474868: step 11300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:57m:08s remains)
2017-12-09 09:22:45.336034: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095635593 0.10099563 0.10323577 0.10184671 0.0982956 0.092454769 0.085991085 0.078578413 0.071178123 0.063717984 0.05551343 0.046800692 0.036802106 0.026531637 0.016776718][0.1185573 0.12789685 0.13306858 0.13381571 0.13148366 0.12580706 0.11893377 0.11060474 0.10247763 0.094220527 0.084570885 0.073007621 0.05934472 0.0445996 0.029527649][0.14843392 0.16344637 0.17268603 0.17583522 0.17439324 0.16899408 0.16165215 0.15205379 0.14261132 0.13253887 0.1204794 0.10576545 0.0879747 0.068271607 0.047519144][0.18137337 0.2022662 0.21537407 0.22027865 0.21918789 0.21296346 0.2048596 0.19501008 0.18526083 0.17485891 0.16158257 0.14451876 0.12278945 0.097575851 0.069970481][0.20698522 0.23355423 0.25021023 0.25764921 0.2582359 0.2532427 0.24544573 0.23566525 0.22598517 0.21486944 0.2006135 0.18172501 0.15682305 0.12682517 0.093619458][0.21856752 0.24985461 0.2700561 0.27997014 0.2825394 0.27912885 0.27264559 0.2639789 0.25512695 0.24450839 0.23027702 0.21032608 0.18342067 0.15024482 0.11255845][0.21837406 0.25253573 0.27560675 0.28846389 0.293209 0.29124087 0.2854588 0.27679121 0.26799387 0.25669253 0.24211845 0.22164169 0.19400384 0.159797 0.12069441][0.21000952 0.24525368 0.26963806 0.2842809 0.29119784 0.29084516 0.28572208 0.2765511 0.26646307 0.25370705 0.23759654 0.21590643 0.18789081 0.15443951 0.11643305][0.20426889 0.23681696 0.25969043 0.27311251 0.27851102 0.27768061 0.27134588 0.2608152 0.24909241 0.23450027 0.21744113 0.19529448 0.16798979 0.13660511 0.10188488][0.19747657 0.2266894 0.24661928 0.25798509 0.26122147 0.25806746 0.24881802 0.23588111 0.22154993 0.20564002 0.18824585 0.1668077 0.14157268 0.11304722 0.082633078][0.19471392 0.21982975 0.2355234 0.24339959 0.24371661 0.23837775 0.2269869 0.21183422 0.1950893 0.17729308 0.15942587 0.13852349 0.11481221 0.089249648 0.063173652][0.20054488 0.22133581 0.23205876 0.23586014 0.23331043 0.22631595 0.21491498 0.19946347 0.18187726 0.16240941 0.14284554 0.12058439 0.0968785 0.072969653 0.04958408][0.21428414 0.23126443 0.2363041 0.23493131 0.22878237 0.21999986 0.20875427 0.19436206 0.17849496 0.16067666 0.14164731 0.11883474 0.094250925 0.069892235 0.04632289][0.23174681 0.24581476 0.24649133 0.24142386 0.23299299 0.22305034 0.21138841 0.19800472 0.18324187 0.16645236 0.14749035 0.12495186 0.10062304 0.075869024 0.051157754][0.24456455 0.25743178 0.2556861 0.24817888 0.23820022 0.22801037 0.21715184 0.20551275 0.19225858 0.17696197 0.15852128 0.13647959 0.11199223 0.086388282 0.060006522]]...]
INFO - root - 2017-12-09 09:22:53.795291: step 11310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:56m:39s remains)
INFO - root - 2017-12-09 09:23:02.484065: step 11320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:59m:23s remains)
INFO - root - 2017-12-09 09:23:11.188679: step 11330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 77h:41m:02s remains)
INFO - root - 2017-12-09 09:23:19.952106: step 11340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:37m:06s remains)
INFO - root - 2017-12-09 09:23:28.510364: step 11350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:47m:47s remains)
INFO - root - 2017-12-09 09:23:37.030173: step 11360, loss = 0.91, batch loss = 0.70 (11.1 examples/sec; 0.719 sec/batch; 64h:07m:56s remains)
INFO - root - 2017-12-09 09:23:45.732462: step 11370, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.871 sec/batch; 77h:39m:47s remains)
INFO - root - 2017-12-09 09:23:54.303653: step 11380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:00m:37s remains)
INFO - root - 2017-12-09 09:24:02.713094: step 11390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:21m:28s remains)
INFO - root - 2017-12-09 09:24:11.292915: step 11400, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 78h:01m:34s remains)
2017-12-09 09:24:12.172316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026393072 -0.0026373991 -0.0026370997 -0.0026371763 -0.0026371384 -0.0026159543 -0.0025431088 -0.0024402761 -0.0023323612 -0.002300275 -0.0023658224 -0.0024662507 -0.0025537584 -0.0026078958 -0.0026321795][-0.0026372613 -0.0026349269 -0.0026345854 -0.002633906 -0.0026271071 -0.0025826334 -0.0024585729 -0.0022957427 -0.0021607729 -0.0021609261 -0.0022825161 -0.0024320455 -0.0025472036 -0.0026099107 -0.0026333593][-0.0026326063 -0.0026223981 -0.002582072 -0.0024763548 -0.0022573373 -0.0019296552 -0.0015654998 -0.0013062651 -0.0012962734 -0.0015539436 -0.0019419638 -0.0022803156 -0.0024968488 -0.0025990009 -0.002632485][-0.0026257122 -0.0025361539 -0.0022269511 -0.0015757962 -0.00055009313 0.00068727648 0.0017663515 0.0022758679 0.0019404055 0.00093739224 -0.0003283429 -0.0014024174 -0.00209796 -0.0024513146 -0.0025903203][-0.0024404093 -0.0021447428 -0.001275648 0.00038451934 0.002758468 0.0054234294 0.0076218722 0.00856922 0.0077751977 0.0055777542 0.0028092205 0.00037683826 -0.0012568977 -0.0021284367 -0.0024945491][-0.0018904045 -0.0012084262 0.00041553192 0.0031877127 0.0068500908 0.010749674 0.013878017 0.015207113 0.014071197 0.01078139 0.0064787986 0.0025382424 -0.00019891723 -0.0017079597 -0.0023656136][-0.0012422018 -0.00011412287 0.002121401 0.0056073591 0.0099297138 0.014329775 0.017722864 0.019077055 0.017686877 0.013800208 0.0086324764 0.0038028588 0.00040526991 -0.0014800992 -0.00230024][-0.00089959567 0.00051901024 0.0029644063 0.0064797485 0.010619402 0.014707104 0.017734889 0.018789902 0.017238669 0.013332835 0.0082170982 0.0034860964 0.00019614282 -0.0015943284 -0.0023484307][-0.0011804814 0.0001221688 0.0022046845 0.0050871121 0.0083855307 0.011581104 0.013850187 0.014455969 0.012922071 0.0095831342 0.0054337317 0.0017582406 -0.00069640228 -0.0019683982 -0.0024700381][-0.0018534251 -0.0010006992 0.00036436948 0.0022326629 0.0043502683 0.0064123254 0.0078454167 0.0081108175 0.0069097877 0.0046042446 0.0019324515 -0.00028324407 -0.0016716555 -0.0023375135 -0.0025756883][-0.0023610564 -0.0019854442 -0.0013631114 -0.00048214989 0.00055714417 0.0016088057 0.0023363046 0.002425374 0.0017405427 0.00053946069 -0.00074989 -0.0017370997 -0.0023050942 -0.0025510909 -0.0026280244][-0.0025895874 -0.002477478 -0.002279283 -0.0019838575 -0.0016161726 -0.0012286145 -0.00095998379 -0.00094009377 -0.001211602 -0.0016543637 -0.0020953207 -0.0024046411 -0.0025660982 -0.0026278507 -0.0026440721][-0.0026482926 -0.0026293646 -0.0025911364 -0.0025271673 -0.0024385909 -0.0023386243 -0.002267539 -0.0022626454 -0.0023317479 -0.0024399278 -0.0025404817 -0.0026048056 -0.0026348559 -0.0026450511 -0.0026466888][-0.0026538437 -0.0026529573 -0.0026509862 -0.0026445384 -0.0026326207 -0.0026182458 -0.0026073609 -0.0026047847 -0.0026116078 -0.0026240237 -0.0026359009 -0.002642825 -0.002645903 -0.0026466653 -0.0026456874][-0.0026509936 -0.0026507257 -0.0026512074 -0.0026512044 -0.0026504658 -0.0026497438 -0.0026488586 -0.0026475568 -0.0026463484 -0.0026456369 -0.0026464087 -0.002646348 -0.0026458094 -0.0026449864 -0.0026433545]]...]
INFO - root - 2017-12-09 09:24:20.673432: step 11410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:56m:07s remains)
INFO - root - 2017-12-09 09:24:29.319712: step 11420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:44m:41s remains)
INFO - root - 2017-12-09 09:24:38.094789: step 11430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:22m:46s remains)
INFO - root - 2017-12-09 09:24:46.622912: step 11440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:09m:38s remains)
INFO - root - 2017-12-09 09:24:55.163779: step 11450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:00m:49s remains)
INFO - root - 2017-12-09 09:25:03.895839: step 11460, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 76h:48m:42s remains)
INFO - root - 2017-12-09 09:25:12.423768: step 11470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:41m:38s remains)
INFO - root - 2017-12-09 09:25:21.098298: step 11480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:51m:27s remains)
INFO - root - 2017-12-09 09:25:29.416343: step 11490, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 77h:21m:38s remains)
INFO - root - 2017-12-09 09:25:38.118128: step 11500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:50m:17s remains)
2017-12-09 09:25:38.991263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026414453 -0.0026399347 -0.0026395496 -0.0026392587 -0.0026391558 -0.0026392166 -0.0026394008 -0.0026397416 -0.002640117 -0.0026403333 -0.0026404336 -0.0026404161 -0.0026402043 -0.0026400108 -0.0026399225][-0.00264031 -0.0026387402 -0.0026382948 -0.0026379128 -0.0026377575 -0.0026377384 -0.0026378168 -0.0026380932 -0.0026384424 -0.0026388043 -0.0026390408 -0.0026391861 -0.0026391 -0.0026389726 -0.0026389302][-0.0026402236 -0.0026386732 -0.0026382823 -0.0026378748 -0.0026376836 -0.0026375949 -0.0026375796 -0.0026377558 -0.0026379961 -0.0026383412 -0.00263868 -0.0026389689 -0.0026389631 -0.002638981 -0.0026389854][-0.0026396741 -0.002638279 -0.0026380231 -0.0026376462 -0.0026373987 -0.0026371568 -0.0026370187 -0.0026370515 -0.0026372119 -0.0026375332 -0.0026378925 -0.0026383307 -0.0026385358 -0.0026387894 -0.0026388988][-0.0026390429 -0.002637788 -0.002637754 -0.0026374911 -0.0026372038 -0.0026368152 -0.002636573 -0.0026365498 -0.0026365989 -0.0026368052 -0.0026371593 -0.0026376646 -0.0026380564 -0.0026385335 -0.0026387463][-0.0026379451 -0.0026368781 -0.0026372627 -0.0026373202 -0.0026370955 -0.0026365202 -0.0026359931 -0.0026358769 -0.0026359956 -0.0026362413 -0.0026366075 -0.0026371633 -0.0026376483 -0.0026382834 -0.0026385963][-0.0026364387 -0.0026356094 -0.00263652 -0.0026370711 -0.00263701 -0.0026362422 -0.0026352943 -0.0026349241 -0.0026349016 -0.0026351756 -0.0026357588 -0.0026365181 -0.0026372068 -0.0026379353 -0.0026384131][-0.0026353505 -0.0026343693 -0.002635648 -0.0026366755 -0.0026371747 -0.0026365456 -0.0026351628 -0.0026342738 -0.0026341558 -0.0026345667 -0.0026353414 -0.0026362492 -0.002637008 -0.0026377765 -0.0026383034][-0.002635027 -0.0026334266 -0.002634827 -0.0026363065 -0.0026372932 -0.0026370869 -0.002635411 -0.0026337563 -0.0026333518 -0.0026339341 -0.0026349528 -0.0026360229 -0.0026368066 -0.0026375796 -0.0026381791][-0.0026356543 -0.0026334489 -0.0026345549 -0.0026361151 -0.0026375812 -0.0026378075 -0.0026364129 -0.002634513 -0.0026334717 -0.0026338021 -0.0026347616 -0.0026358832 -0.0026367733 -0.0026374455 -0.0026380324][-0.0026363984 -0.0026339188 -0.0026343493 -0.0026356552 -0.0026373463 -0.0026380797 -0.0026372003 -0.002635583 -0.0026344296 -0.0026341495 -0.0026348804 -0.0026359111 -0.002636787 -0.0026373968 -0.0026379677][-0.0026376366 -0.0026349528 -0.0026348783 -0.0026354685 -0.0026368177 -0.0026379344 -0.0026374543 -0.0026362082 -0.00263517 -0.002634591 -0.0026347095 -0.0026354992 -0.0026364282 -0.0026371724 -0.0026378275][-0.0026389561 -0.0026363425 -0.0026359234 -0.0026358345 -0.0026365875 -0.0026376995 -0.0026379791 -0.0026373647 -0.0026365488 -0.002635838 -0.002635502 -0.0026356713 -0.0026363577 -0.0026371605 -0.0026378168][-0.0026397386 -0.0026371346 -0.0026365588 -0.0026361169 -0.0026362743 -0.0026369712 -0.0026375791 -0.0026375835 -0.0026370138 -0.0026363954 -0.0026360876 -0.0026360038 -0.0026364883 -0.0026372431 -0.0026378932][-0.0026401514 -0.0026377116 -0.0026372161 -0.0026367675 -0.0026366522 -0.0026368706 -0.0026372736 -0.0026375046 -0.00263731 -0.0026369735 -0.0026367444 -0.0026366387 -0.0026369027 -0.0026374697 -0.0026380138]]...]
INFO - root - 2017-12-09 09:25:47.579020: step 11510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:18m:04s remains)
INFO - root - 2017-12-09 09:25:56.164026: step 11520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:23m:12s remains)
INFO - root - 2017-12-09 09:26:04.834745: step 11530, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 78h:24m:07s remains)
INFO - root - 2017-12-09 09:26:13.523228: step 11540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:27m:34s remains)
INFO - root - 2017-12-09 09:26:21.842026: step 11550, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 73h:59m:37s remains)
INFO - root - 2017-12-09 09:26:30.491135: step 11560, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:02m:40s remains)
INFO - root - 2017-12-09 09:26:38.803432: step 11570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:22m:40s remains)
INFO - root - 2017-12-09 09:26:47.461984: step 11580, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.893 sec/batch; 79h:35m:25s remains)
INFO - root - 2017-12-09 09:26:56.036939: step 11590, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 77h:40m:26s remains)
INFO - root - 2017-12-09 09:27:04.747659: step 11600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:20m:45s remains)
2017-12-09 09:27:05.579972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0022486255 -0.0012545889 0.00035698432 0.0022344063 0.0040605017 0.0050942558 0.0056532589 0.0058002924 0.0057932413 0.0054069725 0.0046326164 0.0034951922 0.0020089131 0.0004890766 -0.000761518][-0.0019223872 -0.0003742415 0.0021625464 0.0052581974 0.0082197208 0.0099905338 0.011011869 0.011725133 0.012465209 0.012591207 0.011579843 0.0094127581 0.0065152594 0.0035223803 0.00096317194][-0.0013093736 0.0014547603 0.0059778872 0.011565475 0.016815793 0.020199105 0.021977115 0.022954052 0.023867806 0.023993827 0.022457702 0.019251386 0.014709724 0.00964627 0.0050257985][-0.00065633096 0.0035882914 0.010957396 0.020329658 0.029171066 0.035625868 0.039859008 0.0427797 0.044765662 0.044894382 0.04204933 0.036369946 0.028628947 0.020010129 0.011922777][-0.00018395693 0.0051287995 0.014840888 0.027613644 0.040280677 0.050720282 0.058481961 0.064432465 0.068493932 0.069404617 0.065896 0.058092009 0.047178552 0.034479775 0.022116559][6.8068737e-05 0.005292621 0.01558263 0.029847445 0.045007769 0.058645155 0.070006885 0.079298526 0.0855213 0.0874007 0.083726108 0.074842654 0.062183753 0.04706509 0.031801865][0.00044187484 0.0043799262 0.013053488 0.026189175 0.041336853 0.056429178 0.070445225 0.082771055 0.091412939 0.094664373 0.091306888 0.082114711 0.06887617 0.053006027 0.036763296][0.001493312 0.0035983403 0.0094734859 0.019523662 0.032466963 0.04694647 0.062080882 0.076566577 0.087578535 0.09272597 0.090592727 0.082005329 0.068907708 0.053090803 0.036930345][0.0024300967 0.0034511597 0.0066092624 0.012916131 0.022154847 0.034061238 0.04820231 0.0630667 0.075489886 0.08242479 0.082058072 0.074975111 0.063075908 0.048301522 0.033164069][0.0015350715 0.0022409689 0.003902656 0.0073334859 0.012878736 0.02108822 0.032115858 0.044939566 0.056688279 0.064090714 0.065195128 0.06021142 0.050639685 0.038315397 0.025645394][-0.00021972833 0.00031065289 0.0011320279 0.0026948438 0.0053177024 0.0098200329 0.016784027 0.025781136 0.034696922 0.040885184 0.042534452 0.039617356 0.033110838 0.024478355 0.015653808][-0.0019796537 -0.0015841544 -0.001163421 -0.00059437496 0.00032831705 0.0021675741 0.005435254 0.010261042 0.015463104 0.019333193 0.02057877 0.019108169 0.015512595 0.010760337 0.0060497182][-0.002666455 -0.0026238349 -0.0024983082 -0.0022863988 -0.002015186 -0.0015403212 -0.00047310768 0.001373596 0.0034986632 0.0051600407 0.0056921174 0.0051008961 0.0036575743 0.0018097186 7.4016396e-05][-0.002675039 -0.0026731989 -0.0026729589 -0.0026724502 -0.0026659567 -0.0026097659 -0.00242913 -0.0020853137 -0.0016004847 -0.0011758534 -0.0010607444 -0.001220251 -0.0015311221 -0.0018920678 -0.0022288708][-0.0026765335 -0.0026748092 -0.0026739824 -0.002673005 -0.0026717405 -0.0026711582 -0.0026657165 -0.0026459855 -0.0026167736 -0.0025910903 -0.0025850513 -0.0025952489 -0.0026073777 -0.0026216817 -0.0026356385]]...]
INFO - root - 2017-12-09 09:27:14.356128: step 11610, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 76h:41m:52s remains)
INFO - root - 2017-12-09 09:27:23.010638: step 11620, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 76h:42m:55s remains)
INFO - root - 2017-12-09 09:27:31.677409: step 11630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 79h:58m:39s remains)
INFO - root - 2017-12-09 09:27:40.486886: step 11640, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:19m:20s remains)
INFO - root - 2017-12-09 09:27:49.169250: step 11650, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:05m:36s remains)
INFO - root - 2017-12-09 09:27:57.920054: step 11660, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 77h:02m:39s remains)
INFO - root - 2017-12-09 09:28:06.513455: step 11670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:29m:13s remains)
INFO - root - 2017-12-09 09:28:15.249925: step 11680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:27m:02s remains)
INFO - root - 2017-12-09 09:28:23.807667: step 11690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:40m:03s remains)
INFO - root - 2017-12-09 09:28:32.344261: step 11700, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:03m:08s remains)
2017-12-09 09:28:33.269921: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.509821 0.52655476 0.53973013 0.54793912 0.54763192 0.54383218 0.53616661 0.52284688 0.50683534 0.4899635 0.47956583 0.47219369 0.46494272 0.45284697 0.44031152][0.51227754 0.52884436 0.541932 0.55144137 0.55383193 0.55219817 0.54638129 0.53704405 0.52424014 0.51041269 0.50177372 0.49480221 0.48782644 0.47675267 0.46505573][0.49692675 0.51419139 0.52717143 0.5358876 0.53850085 0.53971905 0.53663552 0.53077126 0.52151954 0.51152939 0.50587893 0.50109696 0.4949834 0.48515841 0.47399265][0.47990036 0.49600908 0.50697738 0.51499122 0.51763451 0.51943105 0.51780671 0.51559162 0.51062858 0.50512785 0.502157 0.50056589 0.49667177 0.48828107 0.4776][0.45649734 0.47513223 0.48542824 0.49101892 0.49224898 0.49499089 0.49630818 0.49743849 0.49589157 0.49498531 0.49544492 0.49632284 0.49373344 0.4861441 0.47590876][0.42467213 0.44715467 0.45895281 0.46384421 0.46558237 0.46862215 0.47169131 0.47604421 0.47879687 0.4824222 0.48443589 0.48786861 0.48685429 0.48015288 0.469251][0.38228717 0.40918002 0.42372862 0.43092507 0.43491563 0.43998903 0.44595203 0.4538489 0.45996162 0.46636033 0.47070694 0.47618294 0.47595364 0.46968684 0.45881778][0.32940614 0.36006054 0.37854224 0.38889933 0.39672062 0.40628928 0.41749617 0.42844796 0.4371472 0.44635972 0.45236915 0.45840451 0.4588508 0.45349994 0.44305736][0.26993534 0.30240145 0.32367674 0.33758938 0.34990668 0.36454123 0.38115516 0.39661965 0.4084979 0.41925815 0.42624858 0.43272287 0.43370023 0.42940047 0.42100534][0.20871961 0.24017149 0.26368183 0.28121078 0.29828218 0.31790429 0.33910358 0.35919765 0.37466288 0.3873333 0.39613265 0.40310019 0.40496927 0.40144148 0.39492238][0.15193687 0.17898282 0.2026812 0.22380887 0.24611355 0.27064523 0.295952 0.31905743 0.33680087 0.35093227 0.36114073 0.36948416 0.37394765 0.374089 0.37054959][0.10503745 0.12723191 0.14806721 0.16909416 0.19424698 0.22296566 0.25198483 0.27715155 0.2963092 0.31141081 0.32292998 0.33284473 0.33994582 0.3447524 0.34605393][0.070210949 0.085196555 0.10148502 0.12080605 0.14545311 0.17526042 0.20563751 0.2319518 0.25238287 0.26861411 0.28200585 0.29444882 0.30525333 0.31417108 0.31955007][0.048668541 0.054571211 0.063403912 0.078242518 0.10001022 0.1280258 0.15710205 0.18330203 0.2050956 0.22341515 0.24018228 0.25649026 0.27140239 0.28419974 0.2934213][0.041129947 0.037175611 0.037057314 0.044367768 0.059962917 0.083134875 0.10886454 0.13325235 0.15562499 0.1767965 0.1983491 0.21981269 0.23971422 0.25635061 0.26846111]]...]
INFO - root - 2017-12-09 09:28:41.721161: step 11710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:02m:21s remains)
INFO - root - 2017-12-09 09:28:50.399570: step 11720, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 75h:54m:18s remains)
INFO - root - 2017-12-09 09:28:59.170869: step 11730, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:05m:24s remains)
INFO - root - 2017-12-09 09:29:07.927462: step 11740, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 76h:09m:38s remains)
INFO - root - 2017-12-09 09:29:16.536059: step 11750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:12m:20s remains)
INFO - root - 2017-12-09 09:29:25.354130: step 11760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:46m:30s remains)
INFO - root - 2017-12-09 09:29:33.965234: step 11770, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:20m:07s remains)
INFO - root - 2017-12-09 09:29:42.709997: step 11780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:36m:50s remains)
INFO - root - 2017-12-09 09:29:51.292283: step 11790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:10m:53s remains)
INFO - root - 2017-12-09 09:29:59.988395: step 11800, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 80h:06m:33s remains)
2017-12-09 09:30:00.835193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.554197 0.53162718 0.5089249 0.48560995 0.46141338 0.437919 0.41317907 0.39144668 0.37269035 0.36020038 0.3530696 0.34970698 0.34688586 0.33899677 0.32786343][0.56713128 0.54641092 0.524037 0.50058478 0.47617432 0.45213762 0.4257648 0.40145022 0.38018334 0.36621073 0.35772038 0.35284552 0.34931552 0.34162143 0.33102447][0.57238597 0.55340475 0.53141761 0.50948828 0.48619786 0.46259117 0.43625227 0.41150132 0.38906693 0.37293252 0.36177176 0.35448393 0.34903386 0.34042066 0.32929766][0.58067518 0.56608981 0.54659182 0.52633286 0.50419468 0.4818728 0.45687926 0.43312654 0.41033748 0.39326939 0.38030234 0.36943552 0.35956478 0.34740633 0.33369532][0.58924365 0.58111358 0.5648219 0.54783273 0.52861547 0.50907886 0.48694327 0.46488744 0.44244096 0.42366698 0.40773124 0.3926127 0.37768269 0.36074129 0.34304896][0.59806985 0.59352189 0.57825917 0.56388718 0.54805237 0.53327978 0.5168801 0.49940598 0.47993284 0.46156225 0.44348755 0.42375386 0.40229517 0.37966657 0.35679856][0.60153359 0.60191905 0.58723468 0.57328415 0.55975038 0.54864746 0.53695357 0.52442151 0.50936937 0.49331218 0.47515571 0.45316282 0.42781961 0.39983848 0.37189865][0.59894907 0.60355335 0.58979845 0.576825 0.56465369 0.55567646 0.54737341 0.53863978 0.52745712 0.51321703 0.4957104 0.47308132 0.44532588 0.41434976 0.38287613][0.58207875 0.59239107 0.58220237 0.5707978 0.56026769 0.55374837 0.5478406 0.54072976 0.53094417 0.51855153 0.50254917 0.4803434 0.45250717 0.42059997 0.38821769][0.55616432 0.57066929 0.56418836 0.5563187 0.54891819 0.54439586 0.54009569 0.53347397 0.52424836 0.51187789 0.49567702 0.47449034 0.44790956 0.41734517 0.38587394][0.52350634 0.53998566 0.53636986 0.53134704 0.52640253 0.5239917 0.52082163 0.51579881 0.50729007 0.49588537 0.48067057 0.46051663 0.43580779 0.40716398 0.37772298][0.48665056 0.50481987 0.50395739 0.50092679 0.4974215 0.49543658 0.49248877 0.48802057 0.48062271 0.47058183 0.45675457 0.43859267 0.41660631 0.39111224 0.36468115][0.44261202 0.461383 0.46246353 0.46203429 0.46087232 0.46014902 0.45811433 0.45417142 0.4479506 0.43941987 0.42741856 0.4116227 0.3926104 0.37079385 0.348337][0.40132147 0.41820481 0.41993973 0.42053148 0.42093003 0.42135739 0.42067313 0.41788864 0.41325054 0.40642741 0.39665562 0.383649 0.36811042 0.35052446 0.3326678][0.36649284 0.38068449 0.38088205 0.38113779 0.38150212 0.38213107 0.3819938 0.38049573 0.37767664 0.37254953 0.36514023 0.35522264 0.34344786 0.33039412 0.31734928]]...]
INFO - root - 2017-12-09 09:30:09.603650: step 11810, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 78h:22m:44s remains)
INFO - root - 2017-12-09 09:30:18.312403: step 11820, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 79h:58m:24s remains)
INFO - root - 2017-12-09 09:30:26.942657: step 11830, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:09m:32s remains)
INFO - root - 2017-12-09 09:30:35.526934: step 11840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:13m:37s remains)
INFO - root - 2017-12-09 09:30:43.980006: step 11850, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 69h:20m:38s remains)
INFO - root - 2017-12-09 09:30:52.753512: step 11860, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 76h:53m:02s remains)
INFO - root - 2017-12-09 09:31:01.270010: step 11870, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 75h:21m:58s remains)
INFO - root - 2017-12-09 09:31:09.942421: step 11880, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 75h:53m:02s remains)
INFO - root - 2017-12-09 09:31:18.402866: step 11890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:04m:00s remains)
INFO - root - 2017-12-09 09:31:26.958459: step 11900, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 74h:04m:53s remains)
2017-12-09 09:31:27.942219: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010688156 0.012738422 0.01553859 0.01891065 0.022329573 0.025052823 0.026519133 0.02625541 0.024117801 0.020110315 0.014658067 0.008674575 0.0034017893 -0.00017785234 -0.0019673293][0.0088517983 0.01162561 0.015421528 0.01993715 0.024415629 0.027923185 0.029785564 0.02957763 0.02716738 0.022596985 0.0163734 0.0096248407 0.0038021151 -6.5053115e-05 -0.0019457367][0.00553524 0.0086772311 0.013165263 0.01870532 0.024335599 0.028826816 0.031224871 0.031074585 0.028350113 0.023317415 0.016673632 0.0096621048 0.003738313 -0.00013239984 -0.0019814065][0.0027490966 0.0058306237 0.010435149 0.016362976 0.022670364 0.02795185 0.030939491 0.031006509 0.028107686 0.02275279 0.015877515 0.008882666 0.0031802608 -0.00042536366 -0.0020853183][0.00079788873 0.0034713037 0.0076976274 0.013452586 0.019874383 0.025455069 0.028721755 0.028911209 0.025978491 0.020618249 0.013926854 0.00737768 0.0022392645 -0.00087749981 -0.0022381968][-0.00021544006 0.0018032063 0.005321925 0.010560691 0.016791295 0.022435181 0.02580338 0.02600369 0.023046315 0.017815745 0.01154851 0.0056702388 0.0012447769 -0.0013216502 -0.0023794135][-0.00065391045 0.00075891963 0.0035245284 0.0080909524 0.013940362 0.019522795 0.022962628 0.023199327 0.020265367 0.015228178 0.0094308741 0.0042130281 0.00043949811 -0.0016642297 -0.0024825][-0.00093963975 7.8249723e-05 0.0022371863 0.0061020819 0.011363175 0.016644971 0.020055955 0.020435715 0.017735414 0.013057126 0.0077661271 0.0031297472 -0.0001209327 -0.0018792064 -0.0025344847][-0.0013080919 -0.00048423931 0.0012558028 0.00441079 0.0088417288 0.013463005 0.016591787 0.017082272 0.014794625 0.010721685 0.0061163404 0.0021269261 -0.00061514159 -0.0020567093 -0.0025720017][-0.0016344447 -0.0010349755 0.00025927532 0.0026309583 0.0060285707 0.0096571781 0.012205115 0.012686731 0.010916764 0.0076816394 0.0040226881 0.00089690904 -0.0011966602 -0.0022538945 -0.0026125421][-0.001988909 -0.0015939331 -0.00075708353 0.0008009274 0.0030677528 0.0055395891 0.0073217065 0.007675461 0.0064425 0.0041673942 0.0016302974 -0.00047755358 -0.0018231903 -0.002456198 -0.0026511855][-0.0023706101 -0.002164305 -0.0017266989 -0.0008682115 0.00042152731 0.0018644142 0.0029189922 0.003127038 0.0023826063 0.001026514 -0.00044046016 -0.0016037278 -0.0022956622 -0.0025929355 -0.0026735174][-0.0026296761 -0.0025455353 -0.0023753196 -0.002031656 -0.0014920057 -0.00085076841 -0.00035626534 -0.00023900066 -0.00057037757 -0.0011827397 -0.0018275601 -0.0023095759 -0.0025678356 -0.0026641008 -0.0026835424][-0.0026918089 -0.0026797261 -0.0026430839 -0.00255245 -0.00240091 -0.0022155913 -0.0020687054 -0.0020265291 -0.0021192883 -0.0022962498 -0.0024790727 -0.00260663 -0.0026673297 -0.0026863387 -0.0026875008][-0.0026916082 -0.0026903283 -0.0026897362 -0.0026851264 -0.0026732658 -0.0026518097 -0.0026300126 -0.0026183913 -0.0026240197 -0.0026425007 -0.0026634182 -0.0026790348 -0.0026870868 -0.0026898331 -0.0026890466]]...]
INFO - root - 2017-12-09 09:31:36.638678: step 11910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:08m:48s remains)
INFO - root - 2017-12-09 09:31:45.257291: step 11920, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 75h:29m:25s remains)
INFO - root - 2017-12-09 09:31:53.841136: step 11930, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:54m:06s remains)
INFO - root - 2017-12-09 09:32:02.318677: step 11940, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 76h:33m:18s remains)
INFO - root - 2017-12-09 09:32:10.904548: step 11950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:43m:37s remains)
INFO - root - 2017-12-09 09:32:19.388504: step 11960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:53m:15s remains)
INFO - root - 2017-12-09 09:32:27.921297: step 11970, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:28m:53s remains)
INFO - root - 2017-12-09 09:32:36.505199: step 11980, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 74h:06m:21s remains)
INFO - root - 2017-12-09 09:32:44.935583: step 11990, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 74h:00m:04s remains)
INFO - root - 2017-12-09 09:32:53.640113: step 12000, loss = 0.91, batch loss = 0.70 (8.9 examples/sec; 0.895 sec/batch; 79h:38m:29s remains)
2017-12-09 09:32:54.590388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026420178 -0.0026390618 -0.0026386306 -0.0026385523 -0.0026384862 -0.0026385405 -0.0026385968 -0.0026386967 -0.0026387956 -0.0026387672 -0.002638733 -0.0026387125 -0.002638703 -0.0026387607 -0.0026388254][-0.0026398504 -0.0026366597 -0.0026362406 -0.0026361533 -0.0026359993 -0.0026358895 -0.0026357367 -0.0026356808 -0.0026356895 -0.0026358108 -0.002635963 -0.0026361465 -0.0026363756 -0.0026365248 -0.0026366424][-0.0026402064 -0.0026369623 -0.002636652 -0.0026365346 -0.0026362189 -0.0026357565 -0.0026352294 -0.0026348508 -0.0026346496 -0.0026347763 -0.0026350569 -0.0026355002 -0.0026359963 -0.0026362785 -0.0026364625][-0.0026401742 -0.0026368855 -0.0026365682 -0.0026363614 -0.0026358676 -0.0026351607 -0.0026343269 -0.0026337232 -0.0026334017 -0.0026335213 -0.0026339195 -0.0026346231 -0.0026354508 -0.0026359602 -0.0026363011][-0.0026402415 -0.002636781 -0.0026364175 -0.0026361858 -0.0026356047 -0.002634777 -0.0026336471 -0.0026327816 -0.0026323139 -0.0026323202 -0.0026327525 -0.0026336142 -0.0026347558 -0.0026355467 -0.0026360927][-0.0026402657 -0.0026366604 -0.0026363693 -0.0026362631 -0.0026358434 -0.0026351812 -0.0026340266 -0.0026329623 -0.00263229 -0.0026319895 -0.0026321181 -0.002632888 -0.0026341265 -0.0026351335 -0.0026358452][-0.0026400958 -0.0026364494 -0.0026363111 -0.0026363793 -0.0026361817 -0.0026357619 -0.0026347616 -0.0026337027 -0.0026328906 -0.0026323223 -0.0026320908 -0.0026325746 -0.0026336792 -0.0026347269 -0.0026355146][-0.0026399062 -0.0026362641 -0.0026361693 -0.0026363928 -0.0026364229 -0.002636285 -0.0026355258 -0.0026345539 -0.0026336957 -0.0026329313 -0.0026324566 -0.0026326745 -0.0026335621 -0.0026345253 -0.002635282][-0.0026396934 -0.0026361106 -0.0026360706 -0.0026364033 -0.0026366562 -0.002636862 -0.002636441 -0.0026356678 -0.0026348687 -0.0026339274 -0.0026331791 -0.0026330859 -0.0026336673 -0.0026344869 -0.002635122][-0.002639595 -0.002636004 -0.0026361374 -0.0026366236 -0.0026370913 -0.0026375884 -0.0026375889 -0.0026371211 -0.0026364494 -0.0026354855 -0.00263456 -0.0026340883 -0.0026342073 -0.0026347546 -0.0026351572][-0.0026394082 -0.0026359351 -0.0026362038 -0.0026366226 -0.0026370643 -0.002637554 -0.0026377635 -0.0026375472 -0.0026370925 -0.0026363858 -0.0026355982 -0.0026350759 -0.0026349155 -0.0026351616 -0.0026353095][-0.0026392967 -0.0026358997 -0.0026361069 -0.0026363656 -0.0026366247 -0.002636895 -0.0026370683 -0.002636973 -0.0026367267 -0.0026362962 -0.0026358468 -0.0026355342 -0.0026353712 -0.0026354925 -0.0026355386][-0.0026394385 -0.0026358836 -0.002636004 -0.0026360815 -0.0026361337 -0.0026362117 -0.0026363223 -0.0026362685 -0.0026361658 -0.0026360112 -0.0026358811 -0.0026357968 -0.0026356853 -0.0026357723 -0.0026357912][-0.0026395712 -0.0026358906 -0.002636007 -0.002636011 -0.0026359942 -0.0026359993 -0.0026360408 -0.0026359768 -0.0026359288 -0.0026358741 -0.0026358534 -0.0026358496 -0.0026358112 -0.0026358885 -0.0026359155][-0.0026396578 -0.0026359281 -0.0026359835 -0.0026359824 -0.0026359481 -0.0026359195 -0.0026359349 -0.0026358932 -0.0026358324 -0.0026357942 -0.0026358049 -0.002635835 -0.0026358371 -0.0026359109 -0.0026359691]]...]
INFO - root - 2017-12-09 09:33:03.316695: step 12010, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 78h:06m:41s remains)
INFO - root - 2017-12-09 09:33:11.925241: step 12020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:37m:49s remains)
INFO - root - 2017-12-09 09:33:20.411528: step 12030, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.843 sec/batch; 75h:02m:00s remains)
INFO - root - 2017-12-09 09:33:29.094542: step 12040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:31m:14s remains)
INFO - root - 2017-12-09 09:33:37.746539: step 12050, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 79h:06m:32s remains)
INFO - root - 2017-12-09 09:33:46.324315: step 12060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:14m:07s remains)
INFO - root - 2017-12-09 09:33:54.803008: step 12070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:23m:54s remains)
INFO - root - 2017-12-09 09:34:03.378178: step 12080, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 74h:11m:40s remains)
INFO - root - 2017-12-09 09:34:11.982019: step 12090, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:35m:11s remains)
INFO - root - 2017-12-09 09:34:20.641892: step 12100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:34m:57s remains)
2017-12-09 09:34:21.487740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027089079 -0.0027099131 -0.0027072094 -0.0026965456 -0.0026730532 -0.0026450558 -0.0026333304 -0.0026501503 -0.0026788835 -0.0026971535 -0.0027005621 -0.0026983628 -0.0026961362 -0.0026944047 -0.0026933376][-0.0027062509 -0.002707117 -0.0027027715 -0.0026864745 -0.0026539785 -0.0026180153 -0.0026040697 -0.0026257748 -0.002663828 -0.0026926154 -0.0027019018 -0.0027024611 -0.0027010795 -0.0026995796 -0.0026982112][-0.0027026911 -0.0027038946 -0.0026994532 -0.0026817494 -0.0026449848 -0.0026017264 -0.0025794825 -0.0025967637 -0.0026379249 -0.0026777349 -0.0026977945 -0.00270448 -0.0027051002 -0.0027042772 -0.0027035272][-0.0026979023 -0.0027003211 -0.0026982357 -0.0026831408 -0.0026461857 -0.00259259 -0.0025481917 -0.0025438673 -0.0025805489 -0.0026369926 -0.0026788348 -0.002699839 -0.0027062083 -0.0027069305 -0.0027068586][-0.0026930154 -0.0026970527 -0.0026985193 -0.0026880796 -0.0026514195 -0.0025827081 -0.0025042403 -0.0024664085 -0.0024940758 -0.0025712126 -0.0026431659 -0.0026871904 -0.0027040811 -0.0027069524 -0.0027078663][-0.0026899127 -0.0026946936 -0.0026987426 -0.0026919271 -0.0026582377 -0.0025842919 -0.0024880827 -0.0024302846 -0.0024517779 -0.0025388193 -0.002625651 -0.0026790206 -0.0026995046 -0.002703388 -0.0027054059][-0.0026899008 -0.0026941379 -0.0026989561 -0.002695652 -0.0026708795 -0.0026127703 -0.0025323126 -0.00247847 -0.0024908357 -0.0025593869 -0.0026326906 -0.0026784351 -0.0026952268 -0.0026984455 -0.0027003342][-0.0026927539 -0.0026955777 -0.0027000816 -0.002699855 -0.0026874852 -0.0026568065 -0.0026126774 -0.0025813116 -0.0025867657 -0.0026231434 -0.0026617737 -0.0026851243 -0.0026929649 -0.0026947544 -0.00269627][-0.0026973134 -0.0026982883 -0.0027011726 -0.0027016362 -0.0026968687 -0.0026848523 -0.0026667607 -0.0026522512 -0.0026507475 -0.0026624631 -0.0026772134 -0.0026867022 -0.0026906214 -0.0026922098 -0.0026934962][-0.0027028618 -0.002702683 -0.0027035405 -0.0027025 -0.0026986154 -0.0026923297 -0.0026845124 -0.0026792611 -0.002677684 -0.0026797582 -0.0026829452 -0.0026856607 -0.0026878999 -0.0026895846 -0.0026908563][-0.0027073699 -0.0027068122 -0.0027064662 -0.0027038914 -0.0026985989 -0.0026915313 -0.0026842856 -0.002680243 -0.0026789955 -0.0026799161 -0.0026814232 -0.0026833338 -0.0026848782 -0.0026861795 -0.0026873059][-0.0027096758 -0.0027092486 -0.0027086937 -0.0027054285 -0.002699302 -0.0026916661 -0.0026842575 -0.002680239 -0.0026793038 -0.002679917 -0.0026807555 -0.0026818698 -0.0026828484 -0.0026837066 -0.0026844069][-0.0027097908 -0.0027097459 -0.0027094122 -0.0027060767 -0.0026998497 -0.0026923188 -0.0026853157 -0.0026810193 -0.0026794667 -0.0026795315 -0.00268008 -0.0026807962 -0.0026815697 -0.002682331 -0.0026827112][-0.0027083748 -0.0027087054 -0.0027086488 -0.0027057223 -0.0026997495 -0.0026922734 -0.0026859411 -0.0026816812 -0.0026796423 -0.0026791571 -0.0026793859 -0.0026799552 -0.0026806949 -0.0026814248 -0.0026817683][-0.0027060052 -0.0027068101 -0.0027072264 -0.0027052609 -0.00270025 -0.0026934096 -0.0026875092 -0.002683386 -0.0026808272 -0.0026795471 -0.0026791748 -0.0026794078 -0.0026800483 -0.0026807669 -0.0026812293]]...]
INFO - root - 2017-12-09 09:34:30.291092: step 12110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:24m:58s remains)
INFO - root - 2017-12-09 09:34:38.996899: step 12120, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 76h:52m:49s remains)
INFO - root - 2017-12-09 09:34:47.846593: step 12130, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:10m:01s remains)
INFO - root - 2017-12-09 09:34:56.522940: step 12140, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 78h:17m:46s remains)
INFO - root - 2017-12-09 09:35:05.246504: step 12150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:01m:17s remains)
INFO - root - 2017-12-09 09:35:13.940570: step 12160, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:01m:54s remains)
INFO - root - 2017-12-09 09:35:22.468920: step 12170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:45m:45s remains)
INFO - root - 2017-12-09 09:35:31.052417: step 12180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:15m:42s remains)
INFO - root - 2017-12-09 09:35:39.605571: step 12190, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 75h:47m:06s remains)
INFO - root - 2017-12-09 09:35:48.478849: step 12200, loss = 0.90, batch loss = 0.70 (8.9 examples/sec; 0.900 sec/batch; 80h:02m:59s remains)
2017-12-09 09:35:49.309633: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.5216673 0.51440912 0.50648957 0.50134236 0.49767202 0.49322936 0.48847744 0.4824928 0.47587281 0.469358 0.46397939 0.46126094 0.45704579 0.45051935 0.44481537][0.54501474 0.54049695 0.53494483 0.53186136 0.52895552 0.52508891 0.52060771 0.51640642 0.51010346 0.50351012 0.4977175 0.49297786 0.48681739 0.47743741 0.46921206][0.5598318 0.559903 0.55687261 0.55606616 0.55381954 0.551603 0.54848045 0.5460825 0.54050714 0.53414154 0.52785337 0.52139193 0.51333439 0.500952 0.48929346][0.57740349 0.58268028 0.58321983 0.58368427 0.58373815 0.58330315 0.58183324 0.58078134 0.57638854 0.57123715 0.56493813 0.55730587 0.54727632 0.53210121 0.51616406][0.5921551 0.60436064 0.60838825 0.61057878 0.61205482 0.61361116 0.61427134 0.61518747 0.61309826 0.61010933 0.60532272 0.59747177 0.58554566 0.56748354 0.54660779][0.6013301 0.62057078 0.62902254 0.63432527 0.63717341 0.64028013 0.64251125 0.64476866 0.64502221 0.64494807 0.64331335 0.63650948 0.62371343 0.60348213 0.57785577][0.61110419 0.6362716 0.64803851 0.65494645 0.65880203 0.66165638 0.66318285 0.6661483 0.66788435 0.67080927 0.67258286 0.66842204 0.65654576 0.63504 0.60587859][0.61056954 0.64204586 0.65665126 0.66442525 0.66827816 0.67008752 0.67032325 0.6713419 0.67323709 0.67785817 0.68236119 0.68154514 0.67225683 0.65221137 0.62209743][0.59955472 0.63463145 0.65134287 0.66050243 0.66452557 0.66534972 0.66408956 0.66238093 0.6625793 0.66718733 0.67344522 0.67531794 0.6690498 0.65204149 0.62384665][0.58087885 0.6163699 0.63242131 0.64173019 0.64460713 0.64383775 0.64068967 0.63722378 0.6364308 0.64030832 0.64701551 0.65057218 0.64717364 0.63379127 0.60933673][0.55547577 0.58982038 0.60387 0.61169589 0.61346871 0.61180133 0.60735691 0.60253918 0.60059828 0.60361141 0.60956573 0.61350751 0.61200273 0.60214037 0.5825243][0.5284251 0.56031269 0.571777 0.576922 0.57746142 0.57521993 0.57054377 0.56528038 0.56249696 0.56425512 0.56856281 0.571721 0.57093215 0.56381279 0.54898149][0.49919122 0.52834076 0.53793675 0.54210585 0.54271543 0.5407784 0.53688329 0.53176171 0.52835518 0.52864826 0.5310483 0.5327881 0.53178579 0.52631521 0.51530617][0.47467148 0.49934995 0.5063554 0.51004267 0.5112403 0.51070476 0.50886416 0.5058167 0.50325662 0.50273138 0.50349385 0.503645 0.50181925 0.49695614 0.48848337][0.45452777 0.47589806 0.48036709 0.48278058 0.48402578 0.48434994 0.48396346 0.48310137 0.48222837 0.48200426 0.48175839 0.48074543 0.47823444 0.473724 0.4670448]]...]
INFO - root - 2017-12-09 09:35:57.951102: step 12210, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 77h:49m:12s remains)
INFO - root - 2017-12-09 09:36:06.546363: step 12220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:55m:39s remains)
INFO - root - 2017-12-09 09:36:15.181363: step 12230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:40m:51s remains)
INFO - root - 2017-12-09 09:36:23.659528: step 12240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:27m:30s remains)
INFO - root - 2017-12-09 09:36:32.130642: step 12250, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:57m:32s remains)
INFO - root - 2017-12-09 09:36:40.569708: step 12260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:17m:43s remains)
INFO - root - 2017-12-09 09:36:49.068510: step 12270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:30m:44s remains)
INFO - root - 2017-12-09 09:36:57.783124: step 12280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:23m:00s remains)
INFO - root - 2017-12-09 09:37:06.225068: step 12290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:27m:01s remains)
INFO - root - 2017-12-09 09:37:14.990914: step 12300, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:18m:54s remains)
2017-12-09 09:37:15.899729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026984217 -0.0026854868 -0.0026459706 -0.0025668959 -0.0024629338 -0.0023781946 -0.0023611747 -0.0024336728 -0.0025495973 -0.0026460413 -0.0026858419 -0.0026914317 -0.002691716 -0.002692163 -0.002692536][-0.0026837077 -0.0026134211 -0.0024328171 -0.002119239 -0.0017459585 -0.0014733203 -0.0014530958 -0.0017271652 -0.0021411562 -0.0024864515 -0.0026523634 -0.0026899318 -0.0026929225 -0.0026934568 -0.0026938636][-0.0026135535 -0.0023498607 -0.001757102 -0.00082323805 0.00020304858 0.00086855819 0.000814382 1.9401778e-06 -0.0011366711 -0.0020750943 -0.0025511135 -0.0026802409 -0.0026939071 -0.0026947935 -0.0026950378][-0.0024221684 -0.0017044614 -0.0002323708 0.0019198456 0.0041312464 0.005403291 0.0050644376 0.0031732651 0.00069815875 -0.0013073802 -0.0023467049 -0.0026549951 -0.0026931148 -0.0026944862 -0.0026947428][-0.0020656688 -0.00057566515 0.0022864332 0.0062416564 0.010095865 0.012093892 0.01119299 0.0076774363 0.0033066375 -0.00019023335 -0.0020307512 -0.002612273 -0.0026915919 -0.0026934957 -0.0026940731][-0.0015807074 0.00089542381 0.0054270248 0.011432033 0.017045995 0.019700188 0.018018849 0.012621332 0.0061680381 0.0010742981 -0.0016407692 -0.0025544693 -0.0026897227 -0.00269263 -0.0026934696][-0.001116222 0.0022670147 0.0082499832 0.015956715 0.022940012 0.025989125 0.02351667 0.016490543 0.0083928667 0.0020819078 -0.0012919978 -0.002491483 -0.0026863038 -0.0026910221 -0.0026926324][-0.00091948023 0.0028084731 0.0092482567 0.017373754 0.024546226 0.027433738 0.024502732 0.016976735 0.00862675 0.0022232775 -0.0011839772 -0.0024564669 -0.0026832679 -0.0026894286 -0.0026917646][-0.00111826 0.0021695658 0.007762725 0.014696656 0.020664893 0.02284297 0.020020854 0.013478599 0.0065376819 0.0013479656 -0.0014001776 -0.0024771639 -0.0026822912 -0.0026887481 -0.0026911232][-0.0016345816 0.00058999145 0.0043552737 0.0089680329 0.012856774 0.014143938 0.01207695 0.0076540834 0.0031636734 -8.2074665e-05 -0.0018070999 -0.0025385174 -0.0026834342 -0.0026889765 -0.00269099][-0.0021731805 -0.0010584847 0.00083786296 0.0031545172 0.0050898953 0.0056988024 0.0046066777 0.0023693051 0.00016652909 -0.0013787033 -0.0022178125 -0.0026083172 -0.0026872274 -0.002690749 -0.0026913849][-0.0025260518 -0.0021520576 -0.0015064003 -0.00070910994 -3.2842159e-05 0.00019852724 -0.00016001589 -0.000912286 -0.0016621815 -0.0021949634 -0.0025048661 -0.0026596687 -0.0026903141 -0.0026916908 -0.0026914731][-0.0026672555 -0.002601061 -0.0024818822 -0.0023302585 -0.0021944633 -0.0021360465 -0.0021906402 -0.0023219986 -0.0024656726 -0.0025699625 -0.0026434197 -0.0026833827 -0.0026910745 -0.0026911402 -0.0026908414][-0.0026933642 -0.0026913416 -0.0026858286 -0.0026755875 -0.0026626382 -0.0026512539 -0.0026487277 -0.0026542568 -0.0026659013 -0.0026764974 -0.00268577 -0.0026906342 -0.002691553 -0.0026915486 -0.0026914072][-0.0026908158 -0.002691139 -0.0026927139 -0.002692916 -0.0026916284 -0.0026885413 -0.0026859751 -0.0026850968 -0.0026859038 -0.0026873492 -0.0026893024 -0.002690518 -0.00269088 -0.0026912116 -0.0026913092]]...]
INFO - root - 2017-12-09 09:37:24.636509: step 12310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:44m:41s remains)
INFO - root - 2017-12-09 09:37:33.308207: step 12320, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:12m:44s remains)
INFO - root - 2017-12-09 09:37:41.949880: step 12330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 77h:11m:05s remains)
INFO - root - 2017-12-09 09:37:50.622430: step 12340, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 74h:05m:35s remains)
INFO - root - 2017-12-09 09:37:59.029633: step 12350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:42m:08s remains)
INFO - root - 2017-12-09 09:38:07.597177: step 12360, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 80h:04m:00s remains)
INFO - root - 2017-12-09 09:38:16.201639: step 12370, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 80h:32m:00s remains)
INFO - root - 2017-12-09 09:38:24.958394: step 12380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:27m:14s remains)
INFO - root - 2017-12-09 09:38:33.363146: step 12390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:25m:01s remains)
INFO - root - 2017-12-09 09:38:42.063978: step 12400, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 78h:21m:10s remains)
2017-12-09 09:38:42.955771: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0059930384 0.008197967 0.010654728 0.011298174 0.011621931 0.01062151 0.0087047061 0.0065173209 0.0044942433 0.0034949186 0.0025125404 0.0016603528 0.00065164804 -0.00047440408 -0.0012923762][0.0071272496 0.0099944174 0.012830293 0.014072736 0.014786283 0.013832291 0.011291516 0.0082009183 0.0052289842 0.0034391268 0.001859796 0.00094739208 0.00026251655 -0.00014514057 -0.00071552326][0.011505469 0.014345975 0.0170716 0.018376123 0.018948959 0.017875353 0.015371258 0.01199209 0.0083828848 0.0052278526 0.0024704782 0.0011527294 -0.00026761275 -0.00084406079 -0.0013793284][0.019245623 0.022221845 0.02473361 0.025638707 0.025556197 0.023800809 0.020624561 0.016749403 0.012444711 0.0086272312 0.0055247396 0.003782687 0.0010747996 -0.00030391105 -0.0014756313][0.02721356 0.030763943 0.033404104 0.034839813 0.034993194 0.032861922 0.02904452 0.024430418 0.01931777 0.014619774 0.010385799 0.0072327973 0.0035828261 0.0011662391 -0.00071086967][0.035405573 0.03853156 0.040731244 0.042226639 0.042836916 0.041628614 0.038692676 0.034444377 0.029079627 0.023467848 0.017639428 0.012620591 0.0073243324 0.0035593223 0.00061990181][0.044618931 0.047226392 0.048411764 0.04916165 0.049212016 0.047969338 0.045511976 0.042339232 0.038332529 0.033506438 0.027681643 0.020820688 0.013169951 0.0067841327 0.0017567172][0.052828018 0.055069115 0.055092786 0.05475387 0.05399891 0.052541822 0.050307337 0.047767233 0.04474001 0.040832084 0.035713468 0.028701536 0.020331768 0.012227952 0.0045072651][0.058919579 0.060787976 0.059590247 0.058416013 0.05674535 0.055298038 0.053704031 0.052231476 0.0502722 0.047219388 0.042379685 0.035193097 0.026271105 0.016921083 0.0077511705][0.061752334 0.064640664 0.062984027 0.061414964 0.059225589 0.05772524 0.056562882 0.055935279 0.05503428 0.053182632 0.049212392 0.042173132 0.032998968 0.022954822 0.012442593][0.062642522 0.066922821 0.065183088 0.063834168 0.061954312 0.060680442 0.059821531 0.05996418 0.06013649 0.0590921 0.055839647 0.049538359 0.040901143 0.031033643 0.019899514][0.061390013 0.067584991 0.06608279 0.065271452 0.063936554 0.062893271 0.062163837 0.0625756 0.0633313 0.063191563 0.060933203 0.055861343 0.048105787 0.038765334 0.027840059][0.060291931 0.067038685 0.066108271 0.066268004 0.065706193 0.064710841 0.063781053 0.064171068 0.065143526 0.065622963 0.064374372 0.060597688 0.054098021 0.045676269 0.035584386][0.058164392 0.065444395 0.065562092 0.06673149 0.066759564 0.0659894 0.064750634 0.0650511 0.0662125 0.06727092 0.066863216 0.064080246 0.058217313 0.05011411 0.04060977][0.055863455 0.062043808 0.062349677 0.063988574 0.06450475 0.063959487 0.0629528 0.063280687 0.064837389 0.066747308 0.06757091 0.066232041 0.061771609 0.054646362 0.046004575]]...]
INFO - root - 2017-12-09 09:38:51.586789: step 12410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:23m:28s remains)
INFO - root - 2017-12-09 09:39:00.268149: step 12420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:47m:40s remains)
INFO - root - 2017-12-09 09:39:09.047064: step 12430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:26m:41s remains)
INFO - root - 2017-12-09 09:39:17.753242: step 12440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:20m:35s remains)
INFO - root - 2017-12-09 09:39:26.454250: step 12450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:57m:07s remains)
INFO - root - 2017-12-09 09:39:35.044414: step 12460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:32m:19s remains)
INFO - root - 2017-12-09 09:39:43.618294: step 12470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:28m:02s remains)
INFO - root - 2017-12-09 09:39:52.254433: step 12480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:38m:49s remains)
INFO - root - 2017-12-09 09:40:00.649776: step 12490, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:12m:22s remains)
INFO - root - 2017-12-09 09:40:09.196845: step 12500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:56m:21s remains)
2017-12-09 09:40:10.043655: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17528601 0.16666816 0.15346512 0.13464887 0.11333327 0.091136135 0.070595883 0.054028008 0.042187605 0.034566164 0.030355548 0.029180853 0.029114278 0.026855854 0.023545196][0.21239872 0.19746873 0.17713828 0.15142398 0.12356889 0.096116468 0.072801463 0.056619145 0.046572044 0.041150514 0.038991891 0.038446236 0.037269417 0.033616167 0.027526852][0.23192626 0.21148472 0.18606497 0.15597251 0.12522516 0.096224248 0.072700359 0.058032881 0.050836716 0.048602853 0.050338134 0.052370306 0.051789559 0.046855278 0.038885657][0.23720059 0.21109422 0.18134305 0.1483378 0.11739059 0.090744331 0.0717021 0.061914757 0.060179532 0.063429289 0.069177486 0.073216662 0.072889946 0.067386381 0.058387429][0.23534232 0.20303254 0.16788985 0.13221665 0.10243775 0.080330841 0.069050983 0.06820029 0.075796321 0.086757272 0.097861566 0.10481942 0.10529246 0.099892452 0.090681173][0.23770781 0.2014304 0.16277198 0.12555929 0.097550184 0.080517739 0.076540127 0.083405264 0.09853024 0.11550571 0.13066249 0.13942528 0.14063896 0.1358982 0.12668338][0.24291016 0.20741163 0.16961288 0.13386188 0.10946331 0.098126248 0.10082511 0.11381522 0.13259876 0.15174952 0.16773538 0.17623548 0.17744809 0.17328148 0.1650247][0.25136346 0.21942945 0.18494622 0.15453675 0.13578808 0.1293786 0.13659665 0.15276866 0.17274699 0.19074821 0.20424564 0.21033154 0.20991373 0.20632999 0.19902244][0.26714131 0.2385467 0.20717835 0.18182874 0.16792503 0.16500115 0.17499018 0.191483 0.2102221 0.22605249 0.23649672 0.24037589 0.23921284 0.23659031 0.22987899][0.28468311 0.26114854 0.23378383 0.21283887 0.20234653 0.20093772 0.20997524 0.22365935 0.23897089 0.25122276 0.25822139 0.2607426 0.26047456 0.25989166 0.25547948][0.29901716 0.2828373 0.26208249 0.24574311 0.23818989 0.23669091 0.2431474 0.25194505 0.26171333 0.26843944 0.2706221 0.27021208 0.26877537 0.26895049 0.26579106][0.30779403 0.297659 0.28269988 0.2711584 0.26620176 0.2644679 0.26877248 0.27335352 0.27850965 0.27950934 0.27702466 0.27386859 0.27114788 0.27058637 0.26719221][0.30615833 0.30235213 0.29303408 0.28474 0.2805146 0.27818385 0.28055587 0.28208557 0.28453895 0.28185213 0.27681366 0.27159914 0.26810259 0.26698545 0.26301917][0.29003602 0.29066274 0.28525922 0.28004634 0.2776238 0.27554592 0.27690092 0.27668634 0.27747703 0.27313852 0.26749215 0.26165682 0.25842586 0.25737771 0.25370598][0.26889938 0.27101436 0.26697758 0.26234394 0.25994065 0.25703132 0.25653192 0.25431946 0.25339693 0.24882188 0.24382249 0.23958363 0.23829521 0.23898542 0.23696837]]...]
INFO - root - 2017-12-09 09:40:18.784114: step 12510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:20m:48s remains)
INFO - root - 2017-12-09 09:40:27.367868: step 12520, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 72h:46m:04s remains)
INFO - root - 2017-12-09 09:40:35.865206: step 12530, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:09m:37s remains)
INFO - root - 2017-12-09 09:40:44.584646: step 12540, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:40m:42s remains)
INFO - root - 2017-12-09 09:40:53.264148: step 12550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:17m:44s remains)
INFO - root - 2017-12-09 09:41:01.745891: step 12560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:10m:41s remains)
INFO - root - 2017-12-09 09:41:10.316410: step 12570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:16m:25s remains)
INFO - root - 2017-12-09 09:41:18.987290: step 12580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:57m:18s remains)
INFO - root - 2017-12-09 09:41:27.604203: step 12590, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 64h:27m:57s remains)
INFO - root - 2017-12-09 09:41:36.320030: step 12600, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 75h:50m:26s remains)
2017-12-09 09:41:37.136231: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015600824 0.019016942 0.026753694 0.037093315 0.052544948 0.068940111 0.090137631 0.11274063 0.13445644 0.15042031 0.16224802 0.17325249 0.18368551 0.19149189 0.19196866][0.014168887 0.016487835 0.02199162 0.032076173 0.049527291 0.074680924 0.10661409 0.13936868 0.16813904 0.18512535 0.1927242 0.19834843 0.20405541 0.20893554 0.20717253][0.013878408 0.015204358 0.019511426 0.029985467 0.050588962 0.082135051 0.12213513 0.16462615 0.19880655 0.21767831 0.22193782 0.22254804 0.2233263 0.22389969 0.21854556][0.014406838 0.015946766 0.020947179 0.032227315 0.0546656 0.091459841 0.13650356 0.18453471 0.22123435 0.24265656 0.24775727 0.24781795 0.24632935 0.24337888 0.23466617][0.01562788 0.017439431 0.023761597 0.0382174 0.063231662 0.10245086 0.14958745 0.19985156 0.23719268 0.2609103 0.26668897 0.26966625 0.26899403 0.26512387 0.25488198][0.017975712 0.021465367 0.030478092 0.047566976 0.074486993 0.11568975 0.16210602 0.21181704 0.24870147 0.27343765 0.28018245 0.28586194 0.28613973 0.28279263 0.2716004][0.021309983 0.027476816 0.039950583 0.06034885 0.089617923 0.13081057 0.17536247 0.22113568 0.25571239 0.2808179 0.2904239 0.29825008 0.29945058 0.29525754 0.28233656][0.029489225 0.037948944 0.053042721 0.07554289 0.10485259 0.14305781 0.18326589 0.22513205 0.25693125 0.28243598 0.29513079 0.30669975 0.30969808 0.30590621 0.29057324][0.039870169 0.05137141 0.067780443 0.090286836 0.11715408 0.15105376 0.18598391 0.22174451 0.25021577 0.27529684 0.29074076 0.30540752 0.31083357 0.30821514 0.29233098][0.049696289 0.0631964 0.080195732 0.10213983 0.12557833 0.1541058 0.18282999 0.21252145 0.23677267 0.25919574 0.27440006 0.29041436 0.29835346 0.29836023 0.28398448][0.055992 0.070682615 0.087206379 0.10666461 0.12637547 0.15014282 0.17282321 0.19656277 0.21633661 0.23507355 0.24964093 0.26454636 0.27362838 0.27638593 0.26522791][0.056912158 0.071009934 0.085966744 0.10303301 0.11900286 0.13752081 0.15467992 0.17334165 0.18880481 0.20480558 0.21811277 0.23156026 0.24116994 0.24554117 0.23759492][0.05207422 0.064542361 0.076969415 0.090266168 0.10197128 0.11538863 0.12775636 0.14244689 0.15477379 0.16793659 0.17885776 0.19121304 0.20101865 0.20686771 0.20199914][0.041040268 0.051283218 0.060953472 0.070721321 0.078666151 0.087779127 0.096021853 0.10625718 0.11540346 0.12578298 0.13487341 0.14499548 0.15330426 0.15915458 0.15729333][0.029020635 0.036333539 0.043324839 0.050071709 0.055147618 0.060863975 0.06570673 0.072142631 0.07816983 0.085520431 0.092246994 0.0999281 0.10644835 0.11169765 0.1116389]]...]
INFO - root - 2017-12-09 09:41:45.855401: step 12610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 75h:26m:22s remains)
INFO - root - 2017-12-09 09:41:54.623119: step 12620, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 80h:07m:09s remains)
INFO - root - 2017-12-09 09:42:03.362242: step 12630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:03m:53s remains)
INFO - root - 2017-12-09 09:42:12.021182: step 12640, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 78h:17m:51s remains)
INFO - root - 2017-12-09 09:42:20.696585: step 12650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:32m:13s remains)
INFO - root - 2017-12-09 09:42:29.212180: step 12660, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 74h:42m:56s remains)
INFO - root - 2017-12-09 09:42:37.588241: step 12670, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 78h:26m:24s remains)
INFO - root - 2017-12-09 09:42:46.283432: step 12680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:01m:09s remains)
INFO - root - 2017-12-09 09:42:54.754947: step 12690, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 62h:24m:18s remains)
INFO - root - 2017-12-09 09:43:03.177907: step 12700, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 74h:24m:24s remains)
2017-12-09 09:43:04.141569: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0091132168 0.0093098516 0.0085992794 0.0073641483 0.0057412963 0.0048486972 0.0043156752 0.0046201833 0.0049339896 0.0061985897 0.0077760741 0.011311606 0.015207198 0.02085566 0.028005544][0.004031905 0.0040381085 0.0037824048 0.0027884089 0.0017947329 0.0012772453 0.00088747591 0.00098457118 0.0008058683 0.0014685795 0.0028252832 0.0062600039 0.010503022 0.016462473 0.02355876][0.0011986413 0.0016626639 0.0022967609 0.0021591752 0.001976846 0.0016958613 0.0010365951 0.00029797805 -0.00042564305 4.4567278e-05 0.0020989573 0.0067049339 0.012496506 0.019254612 0.0260559][0.00037162798 0.0013413897 0.0023033496 0.0023418006 0.0024931694 0.0023658208 0.0021081378 0.0012505848 0.001109171 0.0025436783 0.0064830929 0.013218944 0.021021649 0.028973961 0.035619657][-0.00012157345 0.0012942322 0.0023288804 0.0030221448 0.0028941066 0.0027447112 0.0027443117 0.0024143935 0.0039481157 0.0075968159 0.01426131 0.023550732 0.032963693 0.041677535 0.048343163][-0.00064837257 0.00071845343 0.0017558688 0.0025701241 0.0026944124 0.0027351507 0.003177827 0.0037998238 0.0070471112 0.013177493 0.022139832 0.032601345 0.042482018 0.051256604 0.057394117][-0.0012867813 -0.00029193168 0.00068179518 0.0014666694 0.0015444851 0.001945345 0.0028317033 0.0043359995 0.00856645 0.015893634 0.025461879 0.035756405 0.045035247 0.052713308 0.058002912][-0.0019966268 -0.001363133 -0.00072553824 1.7801765e-05 0.00052042864 0.0010939876 0.0021230478 0.0037482085 0.0077124471 0.014491611 0.022796134 0.031793777 0.039691113 0.04613987 0.050689857][-0.0024298998 -0.0021225521 -0.0017767012 -0.0012891946 -0.00083949463 -0.00023725023 0.00073460117 0.0019963782 0.0049999719 0.010050277 0.016183177 0.022934753 0.028706959 0.033714231 0.037383303][-0.0026322734 -0.0025081576 -0.002373588 -0.0021077085 -0.0018324452 -0.0014693976 -0.00084757234 -7.5371237e-05 0.0018032158 0.0049563423 0.0088861212 0.013206025 0.016803602 0.020078609 0.022514014][-0.0027027891 -0.0026656443 -0.0026250191 -0.002517832 -0.0024051729 -0.0022451752 -0.0019443128 -0.0015513285 -0.000587604 0.0010467789 0.0031539851 0.0054522343 0.0073065273 0.0091020623 0.010346039][-0.002725394 -0.0027189823 -0.0027080341 -0.0026773985 -0.0026508877 -0.0025969208 -0.0024850757 -0.0023189923 -0.0019229836 -0.0012453708 -0.00034514372 0.00061208312 0.0013532753 0.0020987189 0.0025521431][-0.0027302227 -0.0027296513 -0.0027275202 -0.0027221984 -0.0027170209 -0.0027054588 -0.0026756895 -0.0026152711 -0.0024873109 -0.0022617932 -0.0019589057 -0.0016463295 -0.0014126281 -0.0011722503 -0.0010616734][-0.0027328988 -0.0027329954 -0.0027320308 -0.0027325058 -0.002731289 -0.0027276997 -0.0027204088 -0.0027015936 -0.0026716194 -0.0026143524 -0.0025375977 -0.002460028 -0.0024044861 -0.0023484817 -0.0023342352][-0.0027298846 -0.002728665 -0.0027275253 -0.0027278792 -0.0027273626 -0.0027256683 -0.0027225516 -0.0027174745 -0.0027123047 -0.0027011537 -0.0026860456 -0.0026710634 -0.0026579066 -0.0026435074 -0.0026372653]]...]
INFO - root - 2017-12-09 09:43:12.540701: step 12710, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 74h:06m:17s remains)
INFO - root - 2017-12-09 09:43:21.146249: step 12720, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 74h:57m:21s remains)
INFO - root - 2017-12-09 09:43:29.801280: step 12730, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 78h:42m:37s remains)
INFO - root - 2017-12-09 09:43:38.323100: step 12740, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 74h:00m:40s remains)
INFO - root - 2017-12-09 09:43:46.835883: step 12750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:10m:56s remains)
INFO - root - 2017-12-09 09:43:55.388595: step 12760, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 66h:03m:31s remains)
INFO - root - 2017-12-09 09:44:03.877862: step 12770, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 67h:47m:13s remains)
INFO - root - 2017-12-09 09:44:12.437032: step 12780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:50m:07s remains)
INFO - root - 2017-12-09 09:44:20.994398: step 12790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:24m:02s remains)
INFO - root - 2017-12-09 09:44:29.699965: step 12800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:58m:23s remains)
2017-12-09 09:44:30.678562: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37108624 0.3682909 0.36499658 0.35954109 0.35253668 0.34271827 0.33129358 0.31914136 0.30670282 0.29468292 0.2833879 0.27549309 0.26935443 0.26488787 0.26066148][0.38120058 0.3821421 0.3813656 0.37910289 0.37513119 0.36770466 0.3574641 0.34581721 0.33372962 0.32124159 0.30848157 0.29844186 0.29013252 0.28392828 0.27832532][0.38092619 0.38581631 0.38871184 0.38882312 0.387039 0.38242528 0.37411278 0.36314994 0.35163075 0.33945814 0.32633087 0.3148326 0.30497095 0.29712677 0.28995645][0.37854081 0.38770992 0.39468366 0.3979851 0.39814156 0.39504784 0.38746512 0.37680638 0.36446878 0.35229626 0.33936834 0.32727548 0.3165406 0.30744019 0.29952431][0.3738769 0.38705578 0.39674684 0.40307733 0.40561861 0.40358648 0.39563963 0.38428161 0.371085 0.35798377 0.34484938 0.3332375 0.32258385 0.31321517 0.30545643][0.36488551 0.38145664 0.39297986 0.40077075 0.40432435 0.40276489 0.39526653 0.38284346 0.36936691 0.35656855 0.34406531 0.33310118 0.32328412 0.31469342 0.30746356][0.35015318 0.36791974 0.37984315 0.38946578 0.39462951 0.39354959 0.38696638 0.37505141 0.36222544 0.34958088 0.33809543 0.32834649 0.31947625 0.31187609 0.3052783][0.33485517 0.35257378 0.36416385 0.37380961 0.3797558 0.37967792 0.37396777 0.36350402 0.35109803 0.33895758 0.32789978 0.31863376 0.3105551 0.30363262 0.29791877][0.31459782 0.3326928 0.3441956 0.35515448 0.36305243 0.36435083 0.36009216 0.3509264 0.339218 0.32704368 0.3155821 0.30646175 0.29912475 0.29287112 0.28821996][0.29343086 0.31081134 0.32128426 0.33267236 0.34189329 0.34532043 0.343618 0.3365573 0.32661703 0.31497967 0.30331975 0.29391187 0.28609002 0.2803354 0.27625456][0.27253434 0.28866994 0.29797935 0.30836517 0.31746942 0.32228589 0.3223581 0.31784663 0.31020829 0.30053622 0.29004014 0.28081122 0.2733207 0.26781094 0.26393393][0.25529003 0.26949188 0.27696711 0.28551197 0.29348981 0.29879779 0.3002525 0.29792532 0.29261804 0.2851356 0.27630347 0.26807728 0.26121557 0.25634146 0.25284031][0.24187075 0.2550064 0.26117817 0.26780817 0.27433154 0.27915344 0.2810871 0.27984473 0.27609295 0.2704272 0.26318225 0.25625971 0.25046074 0.246524 0.24380116][0.23109996 0.24313051 0.24804045 0.25331959 0.258765 0.26277149 0.26488897 0.26460132 0.26242438 0.2587077 0.25351623 0.24823447 0.24389225 0.24083094 0.23869422][0.2263727 0.23691857 0.23989436 0.24294651 0.24624187 0.2488123 0.25023293 0.25033605 0.24924298 0.24718049 0.24395274 0.24042204 0.23732592 0.2351665 0.23357202]]...]
INFO - root - 2017-12-09 09:44:39.312421: step 12810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:39m:09s remains)
INFO - root - 2017-12-09 09:44:47.947414: step 12820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 76h:27m:51s remains)
INFO - root - 2017-12-09 09:44:56.655026: step 12830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:25m:28s remains)
INFO - root - 2017-12-09 09:45:05.447376: step 12840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:24m:43s remains)
INFO - root - 2017-12-09 09:45:14.083028: step 12850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:15m:24s remains)
INFO - root - 2017-12-09 09:45:22.777806: step 12860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 75h:31m:42s remains)
INFO - root - 2017-12-09 09:45:31.384395: step 12870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:39m:24s remains)
INFO - root - 2017-12-09 09:45:39.876632: step 12880, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:30m:53s remains)
INFO - root - 2017-12-09 09:45:48.619727: step 12890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:59m:20s remains)
INFO - root - 2017-12-09 09:45:57.158791: step 12900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:16m:40s remains)
2017-12-09 09:45:58.035454: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.05372759 0.051982533 0.050645743 0.04958804 0.048281204 0.047645871 0.047029506 0.0458204 0.04498649 0.043293126 0.041349225 0.038574606 0.03580229 0.033493396 0.03124458][0.035668071 0.034057986 0.033313513 0.032923564 0.032571979 0.033410165 0.034076504 0.033827726 0.033505693 0.032075617 0.030226551 0.027359072 0.024713581 0.02280367 0.021137111][0.027969413 0.026479773 0.025627315 0.025307445 0.025203843 0.025705103 0.025533775 0.025458418 0.02505024 0.023532912 0.021951078 0.020112643 0.018865475 0.017662462 0.016318809][0.021153005 0.020135274 0.020532943 0.021322455 0.022151072 0.02304429 0.023140112 0.023007518 0.02172731 0.020052833 0.018481987 0.016755512 0.015884776 0.01528354 0.014565615][0.022234077 0.022050938 0.023273479 0.025145954 0.027095368 0.028357165 0.028231053 0.027071189 0.024215076 0.021585304 0.019236907 0.017456368 0.016903397 0.016580462 0.015119651][0.022851935 0.02452689 0.027954262 0.032611415 0.037391387 0.040774684 0.041735742 0.039488096 0.03471918 0.028950624 0.024371423 0.020470873 0.018291118 0.016469425 0.014013903][0.025303306 0.026371766 0.029951695 0.036185842 0.043258954 0.04901354 0.050907254 0.048640668 0.042888157 0.035979856 0.030416965 0.025286151 0.022285223 0.019726055 0.01679272][0.038998704 0.041149944 0.044914976 0.0515191 0.058942266 0.064386435 0.065642051 0.062015541 0.054393921 0.045381561 0.037894245 0.032517597 0.029552367 0.027570793 0.025666138][0.065865263 0.068428807 0.071658678 0.077342935 0.083867058 0.089019462 0.090114646 0.086479411 0.079160005 0.070219874 0.061990939 0.056194987 0.05276712 0.050304603 0.048258048][0.096380204 0.10082571 0.10555384 0.11222627 0.11921106 0.124504 0.125878 0.12231246 0.11500411 0.1059069 0.096566923 0.089321114 0.08429499 0.080957562 0.078140862][0.1270662 0.13282174 0.13909471 0.14735363 0.15556043 0.16085178 0.1625462 0.15895826 0.15128066 0.14199983 0.13281529 0.12517078 0.1186482 0.1140307 0.11032052][0.15213199 0.15735582 0.16279575 0.17187601 0.1814962 0.18870938 0.19204342 0.19025926 0.18373443 0.1745322 0.1647148 0.15619028 0.14854556 0.14289409 0.13845347][0.17595433 0.1813616 0.18698975 0.19549146 0.20424557 0.21064012 0.21312797 0.21057448 0.20416431 0.19559038 0.18679796 0.17861763 0.17167188 0.16691901 0.16332211][0.19563133 0.20126702 0.20651722 0.21381505 0.22117601 0.22635175 0.2279339 0.22539891 0.21992463 0.21299629 0.20546782 0.19866322 0.19292666 0.18789434 0.18361433][0.21155876 0.2173605 0.22204581 0.22783735 0.23335388 0.23687005 0.23734407 0.23456089 0.22960128 0.2238678 0.21828556 0.21366785 0.20920222 0.20504904 0.20118771]]...]
INFO - root - 2017-12-09 09:46:06.837202: step 12910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:49m:01s remains)
INFO - root - 2017-12-09 09:46:15.482602: step 12920, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 76h:39m:17s remains)
INFO - root - 2017-12-09 09:46:24.241618: step 12930, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 75h:21m:44s remains)
INFO - root - 2017-12-09 09:46:33.081274: step 12940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:11m:59s remains)
INFO - root - 2017-12-09 09:46:41.896338: step 12950, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 78h:49m:59s remains)
INFO - root - 2017-12-09 09:46:50.712806: step 12960, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 79h:57m:04s remains)
INFO - root - 2017-12-09 09:46:59.283011: step 12970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:12m:38s remains)
INFO - root - 2017-12-09 09:47:07.772492: step 12980, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:48m:12s remains)
INFO - root - 2017-12-09 09:47:16.187941: step 12990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:59m:26s remains)
INFO - root - 2017-12-09 09:47:24.635787: step 13000, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 77h:06m:55s remains)
2017-12-09 09:47:25.527653: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16866401 0.20346166 0.2384454 0.26983941 0.29662746 0.31616738 0.32807663 0.3315396 0.32820627 0.32133597 0.31137908 0.30104432 0.28954464 0.27795845 0.26319954][0.19293439 0.2347938 0.27553871 0.31071109 0.34025669 0.36216941 0.37582943 0.38113743 0.37976247 0.37435851 0.36598063 0.3560991 0.34467632 0.33182937 0.31617263][0.20427419 0.24868345 0.29070345 0.32614744 0.35520691 0.37658146 0.39090306 0.39794472 0.40007737 0.39848429 0.39414713 0.38796738 0.37870559 0.36715713 0.35139638][0.20424263 0.24818 0.2877171 0.31998789 0.34571743 0.36522743 0.37952757 0.38936427 0.39617318 0.40042076 0.4028413 0.40236282 0.39700869 0.38810104 0.37373227][0.19238655 0.23316523 0.26770756 0.29493794 0.3163403 0.33339286 0.34808132 0.36065361 0.37233445 0.38382456 0.3936013 0.39938748 0.39882582 0.39387295 0.38188151][0.17047152 0.20522903 0.2326968 0.25372091 0.27049324 0.28541958 0.29996449 0.31478211 0.33107373 0.3481406 0.36371067 0.37455979 0.37935024 0.37837142 0.36913249][0.14348359 0.16879897 0.18731312 0.20116973 0.21198311 0.22330514 0.23681283 0.25288931 0.27221933 0.29380453 0.31477436 0.33088174 0.3404949 0.34293842 0.33723539][0.11726468 0.13258287 0.14081006 0.14654863 0.15030135 0.15695663 0.16720277 0.18251935 0.2035103 0.22789505 0.25273919 0.27354127 0.28837886 0.29513481 0.29260278][0.092204727 0.098806396 0.099146239 0.096614182 0.092562713 0.093209915 0.099549279 0.11312776 0.13320096 0.15867983 0.18586513 0.21051443 0.22944704 0.23998542 0.24089365][0.068512894 0.069469355 0.064712696 0.057419103 0.049617391 0.046298809 0.048928294 0.05938752 0.076850995 0.099740162 0.1253047 0.15012175 0.17010981 0.18241863 0.18519847][0.045495193 0.044075012 0.038690019 0.031142622 0.023456417 0.019635748 0.020592734 0.027731346 0.040486634 0.058450412 0.07902091 0.099699974 0.11719504 0.1286118 0.13163371][0.027595213 0.025395822 0.020783443 0.015348704 0.01054141 0.00810311 0.0089182 0.013866839 0.022738781 0.035367433 0.049993716 0.065200455 0.077961244 0.086303346 0.088342465][0.015809184 0.013911421 0.010545031 0.0068500722 0.0038598923 0.002423506 0.0030378851 0.0064386707 0.012687552 0.021480983 0.031437363 0.041569572 0.050026156 0.0552092 0.055999164][0.007278434 0.0063004596 0.004407079 0.0021930749 0.00035441527 -0.000520807 -0.00017715292 0.001920827 0.0059796888 0.011736726 0.018136822 0.02436607 0.029320449 0.032059427 0.031892668][0.0018886484 0.0015387407 0.00071489858 -0.00029101688 -0.0012257013 -0.001824343 -0.0017998922 -0.00073271524 0.0013907331 0.0043843016 0.007713086 0.010881779 0.013286849 0.01439168 0.014066837]]...]
INFO - root - 2017-12-09 09:47:34.185517: step 13010, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 75h:53m:07s remains)
INFO - root - 2017-12-09 09:47:42.990549: step 13020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:44m:32s remains)
INFO - root - 2017-12-09 09:47:51.732636: step 13030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:09m:36s remains)
INFO - root - 2017-12-09 09:48:00.396713: step 13040, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 77h:02m:47s remains)
INFO - root - 2017-12-09 09:48:09.185217: step 13050, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 82h:00m:02s remains)
INFO - root - 2017-12-09 09:48:17.929269: step 13060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:37m:19s remains)
INFO - root - 2017-12-09 09:48:26.411087: step 13070, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:40m:52s remains)
INFO - root - 2017-12-09 09:48:35.003244: step 13080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:44m:30s remains)
INFO - root - 2017-12-09 09:48:43.499537: step 13090, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 74h:44m:46s remains)
INFO - root - 2017-12-09 09:48:51.942114: step 13100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:52m:13s remains)
2017-12-09 09:48:52.812282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027145115 -0.0027323463 -0.0026841627 -0.0025348065 -0.0023274112 -0.0019645139 -0.0015804241 -0.0012621892 -0.0011000619 -0.0011281224 -0.0012488242 -0.0014361345 -0.0017102633 -0.0020459988 -0.0023618431][-0.0025782424 -0.0025828732 -0.0024789884 -0.0022436345 -0.0019411028 -0.0014761699 -0.00096520851 -0.00050425739 -0.00017862883 -0.00010107667 -0.0001664653 -0.00038622459 -0.00080582965 -0.0013568826 -0.0019324941][-0.0023729224 -0.0023122786 -0.0020847551 -0.0016859032 -0.001228436 -0.00062892376 2.9656803e-05 0.0006402724 0.0011393358 0.0013239319 0.0013477919 0.0010945983 0.00052268803 -0.00027298648 -0.0011851155][-0.0021702163 -0.001966022 -0.001543752 -0.00089712511 -0.000203704 0.00058230269 0.0013985564 0.0021351855 0.002759825 0.0030007935 0.0030791857 0.0027320343 0.0019998509 0.00097381929 -0.00024205726][-0.0020920632 -0.0017240316 -0.0010575408 -8.3625317e-05 0.00092176883 0.0019444353 0.0029193116 0.0037545175 0.0044242674 0.0046501155 0.0046856352 0.0041729384 0.003266497 0.002044152 0.00062390021][-0.0021632113 -0.0017109487 -0.00083828578 0.00044760294 0.0017783041 0.0030709291 0.00422038 0.0051491172 0.0058086333 0.0059765256 0.0058787852 0.0051644868 0.0040919255 0.0027122577 0.0011761049][-0.002279605 -0.0018652299 -0.00092376047 0.00054917717 0.0021306248 0.0036389637 0.0049300566 0.0059639639 0.0066048787 0.0067170556 0.0064606625 0.0055856709 0.0043804022 0.0028901496 0.0012935875][-0.0023487725 -0.0020424325 -0.001183635 0.00028269598 0.0019673673 0.0035847146 0.00493726 0.0060146186 0.0066047129 0.0066790641 0.0063019437 0.00534441 0.0040568477 0.0025265208 0.00094009214][-0.0023644315 -0.0021533461 -0.0014587024 -0.00015722658 0.0014639655 0.0030445945 0.0043322658 0.0053272913 0.0057947286 0.0058185752 0.0053631454 0.0043963538 0.0030998709 0.0016365 0.000184929][-0.0024100465 -0.002244988 -0.0017071598 -0.00064710132 0.00078062783 0.0021936311 0.0033050054 0.0040896581 0.00436177 0.0042835907 0.0037670238 0.0028534671 0.0016543572 0.00039395527 -0.00077302312][-0.0025135078 -0.0023801846 -0.0019767503 -0.0011698278 -2.3734989e-05 0.0011257492 0.00200027 0.0025197794 0.0025841296 0.0023802929 0.0018395367 0.0010454224 6.7502493e-05 -0.00087198918 -0.0016615399][-0.0026314349 -0.0025403749 -0.0022649746 -0.0017110739 -0.00088935334 -5.1128445e-05 0.00057070446 0.00085906126 0.00078139524 0.00050644879 1.7020619e-05 -0.000588828 -0.001264391 -0.0018480401 -0.002282026][-0.0027154679 -0.00267057 -0.0025143416 -0.002188829 -0.0016841026 -0.0011561112 -0.00076805823 -0.00063588214 -0.00075806142 -0.0010089745 -0.0013649262 -0.0017470164 -0.0021257959 -0.0024177791 -0.0026061016][-0.0027551602 -0.0027424472 -0.0026755608 -0.0025216953 -0.0022671316 -0.0019903001 -0.001787068 -0.0017389414 -0.0018360644 -0.0019990788 -0.0021980337 -0.0023870403 -0.0025527258 -0.0026658436 -0.0027283675][-0.002768524 -0.0027681445 -0.0027491753 -0.0026955819 -0.0025957746 -0.0024801029 -0.0023953659 -0.002382766 -0.0024349398 -0.0025103826 -0.002591515 -0.0026593721 -0.0027108712 -0.0027425045 -0.0027593845]]...]
INFO - root - 2017-12-09 09:49:01.423613: step 13110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:08m:16s remains)
INFO - root - 2017-12-09 09:49:09.959115: step 13120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:20m:35s remains)
INFO - root - 2017-12-09 09:49:18.609359: step 13130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:09m:02s remains)
INFO - root - 2017-12-09 09:49:27.198921: step 13140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:55m:01s remains)
INFO - root - 2017-12-09 09:49:35.955984: step 13150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:30m:23s remains)
INFO - root - 2017-12-09 09:49:44.532680: step 13160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:29m:19s remains)
INFO - root - 2017-12-09 09:49:52.975966: step 13170, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 76h:44m:59s remains)
INFO - root - 2017-12-09 09:50:01.340197: step 13180, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 75h:04m:25s remains)
INFO - root - 2017-12-09 09:50:09.811039: step 13190, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 74h:11m:56s remains)
INFO - root - 2017-12-09 09:50:18.160021: step 13200, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:34m:08s remains)
2017-12-09 09:50:19.008391: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35434368 0.35955846 0.36289179 0.36319104 0.36200753 0.35958651 0.35483694 0.34761494 0.33873174 0.32864851 0.3166745 0.30202246 0.28694114 0.27097079 0.25338709][0.38877895 0.39997926 0.4075968 0.41150182 0.41284102 0.41170666 0.40735292 0.39870533 0.38784438 0.37450939 0.3594602 0.34137452 0.32205263 0.302628 0.28246704][0.41454455 0.43216905 0.44439298 0.45084736 0.45430917 0.45438588 0.449599 0.43986714 0.42674443 0.41092378 0.39228219 0.36986715 0.34627783 0.32322663 0.30096003][0.43091697 0.45475924 0.47188225 0.48173648 0.48688149 0.48698235 0.48097059 0.4685522 0.45119107 0.43195719 0.4099375 0.38497698 0.35904965 0.33395973 0.31160748][0.44192243 0.47015023 0.49060962 0.50289756 0.50956362 0.50914955 0.50074136 0.48505208 0.46340686 0.4407427 0.41595265 0.38965425 0.3633419 0.33891028 0.31827679][0.44674268 0.47897735 0.50201684 0.5159629 0.52339214 0.52209324 0.510905 0.49132448 0.4655484 0.43916988 0.41246891 0.38652012 0.36173734 0.33966014 0.3220236][0.44654933 0.4819133 0.5059815 0.52130616 0.52857047 0.52629495 0.51255077 0.48882133 0.45964172 0.43061891 0.40335318 0.37886411 0.35755667 0.33962229 0.32594162][0.44071716 0.47800806 0.50292689 0.51822728 0.52414107 0.51969856 0.50303626 0.47665632 0.44501907 0.41559073 0.3896437 0.36821949 0.35171834 0.33822632 0.32823941][0.42603996 0.46338955 0.48803779 0.50371063 0.50956178 0.50405192 0.48654836 0.45956063 0.42770734 0.39854255 0.37399775 0.35639933 0.34460506 0.3358357 0.32974976][0.40385762 0.43911585 0.46162593 0.47675166 0.48313442 0.47899348 0.46350411 0.43904847 0.41089997 0.38434064 0.36293125 0.34866038 0.34050295 0.33547273 0.33160028][0.37649646 0.40930378 0.42952737 0.44315565 0.44928426 0.44700143 0.43531415 0.41608906 0.39384738 0.37251744 0.3554616 0.34441382 0.33859906 0.33519882 0.33191764][0.344284 0.37370521 0.39108318 0.403471 0.40961894 0.40906709 0.40144211 0.38819444 0.37225842 0.35665235 0.34432882 0.33626527 0.33204275 0.32940379 0.32624459][0.30864763 0.33426562 0.34900472 0.35954645 0.36556017 0.36643323 0.36223263 0.35406342 0.34368861 0.333279 0.32490572 0.31961128 0.3167426 0.31444132 0.31109682][0.27004117 0.29106355 0.30270797 0.31178844 0.31809926 0.32103291 0.32045862 0.31684279 0.31128383 0.30487886 0.29945737 0.29583567 0.29365405 0.2916061 0.28829286][0.23521648 0.25213698 0.26080087 0.26783836 0.273309 0.27704677 0.27847344 0.27799731 0.27603418 0.27290767 0.27002656 0.2677775 0.26614571 0.26413488 0.26089585]]...]
INFO - root - 2017-12-09 09:50:27.487535: step 13210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:07m:43s remains)
INFO - root - 2017-12-09 09:50:36.060994: step 13220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:36m:51s remains)
INFO - root - 2017-12-09 09:50:44.727997: step 13230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:38m:04s remains)
INFO - root - 2017-12-09 09:50:53.419202: step 13240, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 79h:51m:16s remains)
INFO - root - 2017-12-09 09:51:02.007253: step 13250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:12m:03s remains)
INFO - root - 2017-12-09 09:51:10.651838: step 13260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:04m:43s remains)
INFO - root - 2017-12-09 09:51:19.321901: step 13270, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 79h:19m:36s remains)
INFO - root - 2017-12-09 09:51:27.848541: step 13280, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 75h:45m:42s remains)
INFO - root - 2017-12-09 09:51:36.439910: step 13290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:41m:11s remains)
INFO - root - 2017-12-09 09:51:44.898112: step 13300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:55m:11s remains)
2017-12-09 09:51:45.825158: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.52586311 0.51680034 0.4992387 0.47674179 0.45201686 0.43459433 0.4271664 0.4304089 0.44484803 0.46466854 0.48624873 0.50813466 0.52978486 0.55080146 0.56540257][0.5240894 0.51296437 0.49305025 0.46791011 0.44135639 0.42126557 0.41142422 0.41349861 0.42612493 0.44645455 0.47006714 0.49492341 0.52188486 0.54916495 0.56917113][0.51834887 0.50544435 0.48264718 0.45244223 0.4196631 0.39267316 0.37614715 0.37304965 0.38241959 0.40289435 0.43084726 0.46315989 0.49809316 0.5328002 0.55837351][0.51800394 0.50251949 0.47452214 0.43722355 0.39653113 0.36122584 0.33728707 0.32849121 0.335106 0.35702351 0.389752 0.42964807 0.47286341 0.51429492 0.54370821][0.52018863 0.50309473 0.47026533 0.42554775 0.37574476 0.33163247 0.30027595 0.28737617 0.29272398 0.31680489 0.35483533 0.40151742 0.45086506 0.49645931 0.52862906][0.52737957 0.50954539 0.47262666 0.42200014 0.36450037 0.31174409 0.27378914 0.25711089 0.26175416 0.28829652 0.3312422 0.38365364 0.43739924 0.48527724 0.5189001][0.53602916 0.5187993 0.47971436 0.42453286 0.362128 0.3043001 0.26236683 0.24444698 0.24964271 0.27857172 0.3247771 0.38027766 0.4354057 0.48252046 0.51579672][0.54159576 0.52673274 0.48796394 0.43355763 0.37104884 0.31208858 0.26936322 0.25038514 0.25517529 0.2841067 0.33048066 0.38553703 0.4387486 0.48295328 0.5143919][0.53739047 0.52619958 0.49100375 0.442498 0.38603738 0.33249167 0.29416493 0.2769323 0.28221688 0.30913502 0.3521499 0.40261719 0.45015267 0.48920834 0.51709026][0.52509969 0.52026594 0.49224323 0.45391217 0.4077149 0.36323464 0.33153573 0.3180522 0.32392314 0.347685 0.38522872 0.42850116 0.46805915 0.49963626 0.52182156][0.50702471 0.50942975 0.49071065 0.46384743 0.43076831 0.39813587 0.37532881 0.36656946 0.37298083 0.39310572 0.42331257 0.45735261 0.48724648 0.51001263 0.52525842][0.48320785 0.49235785 0.48320416 0.46821204 0.44824719 0.42828292 0.41463429 0.41080657 0.41755348 0.43341523 0.45580569 0.47977033 0.49948761 0.51310605 0.520882][0.45995009 0.47314918 0.47119603 0.46653181 0.45784071 0.44806078 0.44188654 0.44136471 0.44695076 0.45770314 0.47197974 0.48631611 0.4966639 0.50198561 0.5030542][0.43878725 0.45339325 0.45551467 0.45722654 0.45648235 0.45408279 0.45283672 0.4543736 0.45845702 0.46403974 0.47057748 0.47637716 0.4789131 0.47754395 0.47311631][0.42045152 0.43603644 0.44054976 0.44486824 0.44774312 0.44901949 0.44952825 0.45019418 0.45076546 0.4507781 0.45009333 0.44824269 0.44417971 0.43751252 0.42895442]]...]
INFO - root - 2017-12-09 09:51:54.325032: step 13310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:21m:37s remains)
INFO - root - 2017-12-09 09:52:02.897211: step 13320, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 75h:12m:45s remains)
INFO - root - 2017-12-09 09:52:11.664626: step 13330, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 74h:30m:21s remains)
INFO - root - 2017-12-09 09:52:20.234219: step 13340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:07m:58s remains)
INFO - root - 2017-12-09 09:52:28.941286: step 13350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:25m:36s remains)
INFO - root - 2017-12-09 09:52:37.626779: step 13360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:52m:24s remains)
INFO - root - 2017-12-09 09:52:46.199170: step 13370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:03m:38s remains)
INFO - root - 2017-12-09 09:52:54.650915: step 13380, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 74h:35m:19s remains)
INFO - root - 2017-12-09 09:53:03.103703: step 13390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:05m:19s remains)
INFO - root - 2017-12-09 09:53:11.710523: step 13400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 76h:40m:20s remains)
2017-12-09 09:53:12.619103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027777781 -0.0027768232 -0.0027769152 -0.0027770882 -0.0027772516 -0.00277709 -0.0027764868 -0.0027756765 -0.0027742442 -0.0027726716 -0.0027718609 -0.0027719147 -0.0027719878 -0.0027719997 -0.0027724628][-0.0027769518 -0.0027760186 -0.0027762114 -0.0027766186 -0.0027772456 -0.0027777727 -0.0027776607 -0.0027767306 -0.0027748107 -0.0027731487 -0.0027725024 -0.0027724635 -0.0027721857 -0.0027718707 -0.0027720002][-0.0027771178 -0.0027760945 -0.0027764014 -0.0027771392 -0.0027784477 -0.0027799362 -0.0027805085 -0.0027796589 -0.0027775529 -0.0027761222 -0.0027753946 -0.0027747808 -0.0027737045 -0.0027729205 -0.0027728514][-0.0027771401 -0.0027760509 -0.0027765159 -0.0027776898 -0.0027798451 -0.0027823551 -0.0027835935 -0.0027829586 -0.0027811979 -0.0027798973 -0.0027789567 -0.0027776379 -0.0027755757 -0.002774084 -0.0027736465][-0.0027770931 -0.0027759105 -0.0027765662 -0.0027782328 -0.0027812256 -0.0027846084 -0.0027862699 -0.0027859502 -0.0027844924 -0.0027832824 -0.0027821737 -0.0027800992 -0.0027772654 -0.0027751497 -0.0027742512][-0.0027769753 -0.0027756386 -0.0027765685 -0.002778824 -0.0027826151 -0.0027864566 -0.0027883414 -0.0027882226 -0.0027871341 -0.0027861176 -0.0027847965 -0.0027821274 -0.0027789311 -0.0027763457 -0.002774888][-0.0027766805 -0.0027752738 -0.0027764239 -0.0027792742 -0.0027837588 -0.0027876238 -0.0027895784 -0.0027898224 -0.0027891097 -0.0027882089 -0.0027866047 -0.0027835 -0.0027800929 -0.0027773161 -0.0027754349][-0.0027762584 -0.0027747608 -0.0027761485 -0.0027792521 -0.002784098 -0.0027878769 -0.0027896671 -0.0027899174 -0.0027896587 -0.0027891821 -0.0027872233 -0.0027840098 -0.0027805485 -0.0027777418 -0.0027755767][-0.0027756635 -0.0027740798 -0.0027757681 -0.0027788256 -0.0027836007 -0.0027870822 -0.0027883782 -0.0027885507 -0.002788638 -0.0027887882 -0.0027867653 -0.0027834261 -0.002780007 -0.0027773187 -0.0027751885][-0.0027750977 -0.0027733967 -0.002775223 -0.0027781373 -0.0027824789 -0.0027854349 -0.0027862745 -0.0027861726 -0.0027864841 -0.0027870121 -0.002785292 -0.0027821332 -0.0027787478 -0.0027762889 -0.0027744274][-0.0027745725 -0.0027728213 -0.0027746251 -0.0027773983 -0.0027809986 -0.0027835136 -0.0027841437 -0.0027836638 -0.0027838782 -0.0027847625 -0.002783298 -0.0027804254 -0.0027773778 -0.0027753413 -0.0027737871][-0.0027742309 -0.0027724225 -0.0027739389 -0.0027762933 -0.0027792766 -0.0027813388 -0.0027817637 -0.002781145 -0.002781047 -0.0027817308 -0.0027806449 -0.0027784014 -0.0027760591 -0.0027744796 -0.0027732265][-0.0027741748 -0.0027721403 -0.0027733282 -0.0027750977 -0.0027772561 -0.0027787932 -0.0027790375 -0.0027783285 -0.0027779867 -0.0027784195 -0.0027777641 -0.0027762582 -0.0027747443 -0.0027736614 -0.0027727764][-0.0027742451 -0.0027720442 -0.0027728139 -0.0027740372 -0.0027753785 -0.0027763054 -0.002776254 -0.0027757192 -0.0027755459 -0.0027757473 -0.0027753762 -0.002774548 -0.0027736605 -0.0027729964 -0.0027724646][-0.002774335 -0.0027721024 -0.0027724763 -0.002773298 -0.0027740528 -0.0027744689 -0.0027743815 -0.002774108 -0.0027740807 -0.002774176 -0.0027739455 -0.0027734123 -0.0027728421 -0.0027725028 -0.0027722963]]...]
INFO - root - 2017-12-09 09:53:21.265792: step 13410, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:04m:19s remains)
INFO - root - 2017-12-09 09:53:29.917982: step 13420, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 72h:22m:57s remains)
INFO - root - 2017-12-09 09:53:38.744750: step 13430, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:26m:54s remains)
INFO - root - 2017-12-09 09:53:47.446276: step 13440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:05m:31s remains)
INFO - root - 2017-12-09 09:53:56.211294: step 13450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 78h:26m:29s remains)
INFO - root - 2017-12-09 09:54:04.938012: step 13460, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 75h:39m:50s remains)
INFO - root - 2017-12-09 09:54:13.468008: step 13470, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 81h:11m:10s remains)
INFO - root - 2017-12-09 09:54:21.914777: step 13480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:49m:06s remains)
INFO - root - 2017-12-09 09:54:30.572666: step 13490, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 76h:39m:09s remains)
INFO - root - 2017-12-09 09:54:39.192782: step 13500, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 75h:08m:28s remains)
2017-12-09 09:54:40.023347: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01383013 0.014319162 0.014734724 0.016069483 0.017530311 0.018823572 0.021794636 0.027598612 0.036093682 0.045038104 0.051324047 0.053759843 0.053618014 0.050961781 0.047265437][0.013769797 0.013998306 0.013930584 0.014999132 0.016001288 0.016698673 0.019382998 0.02516022 0.034861758 0.045482155 0.0536196 0.057255339 0.057486117 0.054772019 0.050634164][0.013676251 0.013791074 0.013386036 0.014486629 0.015076927 0.015749637 0.018394593 0.024270516 0.035107728 0.047067821 0.056252535 0.060415633 0.060993269 0.0582683 0.053916533][0.013734917 0.013047366 0.012234075 0.01320131 0.013780997 0.015043315 0.018079702 0.024661016 0.036530595 0.0495761 0.059407085 0.063559711 0.063688308 0.060823143 0.05622635][0.014195391 0.012729399 0.011519389 0.011935703 0.012103612 0.013417476 0.016873624 0.023753949 0.036552623 0.050415758 0.061215572 0.065786853 0.066201746 0.063436672 0.0588055][0.01466079 0.012607413 0.010983626 0.011068288 0.010830695 0.012123683 0.01610171 0.02404616 0.037732221 0.051284611 0.061742127 0.065887846 0.066081248 0.063430242 0.059178][0.016903639 0.013510014 0.011112633 0.010916551 0.01087923 0.013214915 0.018189516 0.027528081 0.041433871 0.054271352 0.063864723 0.066928737 0.066230208 0.063087441 0.058767498][0.02248209 0.018155415 0.014747745 0.014107154 0.014376442 0.018115468 0.024671949 0.035242662 0.048659783 0.059639126 0.06703648 0.068531714 0.066886261 0.063570291 0.058932226][0.031668313 0.02675331 0.02315275 0.022353284 0.023221042 0.027628839 0.03494427 0.045012187 0.056628123 0.065174177 0.069991663 0.06983541 0.067374811 0.06394612 0.059445407][0.040946234 0.036609523 0.033761412 0.033510491 0.034922957 0.039263688 0.045892689 0.054195207 0.062765226 0.068595365 0.071278416 0.070025921 0.06758558 0.064255744 0.059611704][0.050643116 0.046664793 0.043773364 0.043545656 0.045014907 0.048889391 0.054463696 0.061085626 0.0671878 0.070750423 0.07182654 0.069817342 0.067332171 0.064221956 0.059436526][0.058121908 0.054744687 0.05202524 0.05155009 0.052206941 0.054776784 0.05867802 0.063619748 0.068016015 0.070652805 0.071099043 0.069047064 0.0666383 0.063558206 0.058630276][0.063773893 0.060588904 0.0582466 0.057220697 0.056895647 0.057795696 0.059812278 0.062656157 0.065582 0.067219272 0.067571849 0.06626159 0.064627886 0.062103219 0.057595339][0.065469719 0.062974423 0.061024204 0.060010016 0.059196752 0.058595248 0.058994316 0.060181767 0.061752439 0.0625917 0.062853262 0.061331853 0.060140632 0.058221795 0.054528598][0.063508287 0.061648488 0.060115166 0.059198406 0.058002505 0.056834765 0.056403957 0.056669634 0.057318445 0.057581402 0.057640124 0.056060903 0.054620281 0.052464232 0.04888811]]...]
INFO - root - 2017-12-09 09:54:48.580940: step 13510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:02m:40s remains)
INFO - root - 2017-12-09 09:54:57.265899: step 13520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:30m:27s remains)
INFO - root - 2017-12-09 09:55:05.835200: step 13530, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 75h:39m:13s remains)
INFO - root - 2017-12-09 09:55:14.743147: step 13540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:38m:11s remains)
INFO - root - 2017-12-09 09:55:23.552959: step 13550, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:11m:13s remains)
INFO - root - 2017-12-09 09:55:32.248233: step 13560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:18m:05s remains)
INFO - root - 2017-12-09 09:55:40.831255: step 13570, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:40m:59s remains)
INFO - root - 2017-12-09 09:55:49.293983: step 13580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:07m:00s remains)
INFO - root - 2017-12-09 09:55:57.817327: step 13590, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:37m:57s remains)
INFO - root - 2017-12-09 09:56:06.256159: step 13600, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:24m:08s remains)
2017-12-09 09:56:07.126168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027861665 -0.0027807311 -0.0027767005 -0.0027734835 -0.002771036 -0.0027696472 -0.002769344 -0.0027704106 -0.0027725252 -0.0027753504 -0.002778477 -0.0027820684 -0.0027851802 -0.0027869104 -0.002786051][-0.0027872138 -0.0027814987 -0.0027770919 -0.0027734009 -0.0027702993 -0.0027682306 -0.002767521 -0.0027682106 -0.0027700753 -0.0027729108 -0.0027764034 -0.002780386 -0.0027839735 -0.0027862792 -0.002786017][-0.0027872133 -0.0027821879 -0.0027781113 -0.0027745937 -0.002771636 -0.0027694439 -0.0027683889 -0.0027685207 -0.0027697047 -0.0027718625 -0.0027749785 -0.0027786228 -0.0027819907 -0.0027843527 -0.0027847227][-0.0027853537 -0.0027815164 -0.0027785948 -0.0027760542 -0.0027738574 -0.0027720656 -0.002770856 -0.0027703207 -0.002770514 -0.0027715457 -0.0027734309 -0.0027760095 -0.0027786312 -0.0027807686 -0.0027817453][-0.0027818559 -0.0027791506 -0.0027778656 -0.0027769543 -0.0027761417 -0.0027753555 -0.002774559 -0.0027736449 -0.0027728907 -0.0027723508 -0.0027723403 -0.0027731643 -0.0027746032 -0.002776203 -0.0027774035][-0.0027775248 -0.0027757157 -0.0027760174 -0.0027769974 -0.0027780554 -0.0027789383 -0.002779033 -0.0027780915 -0.0027763688 -0.0027742737 -0.0027723678 -0.0027712479 -0.0027713892 -0.0027719862 -0.0027730118][-0.002773576 -0.0027726691 -0.0027741839 -0.0027766777 -0.0027796137 -0.0027822005 -0.0027833942 -0.0027827725 -0.0027803436 -0.0027770048 -0.0027736407 -0.0027708912 -0.0027695091 -0.0027689275 -0.0027696784][-0.0027709524 -0.0027705955 -0.0027730162 -0.0027766014 -0.0027807686 -0.0027845998 -0.0027868378 -0.0027866485 -0.0027839355 -0.0027798191 -0.0027754172 -0.002771443 -0.0027688355 -0.0027673198 -0.0027677382][-0.00276948 -0.0027692674 -0.0027719855 -0.0027759029 -0.0027804428 -0.0027846731 -0.0027873914 -0.002787852 -0.0027855958 -0.0027814484 -0.0027765904 -0.0027720202 -0.0027689799 -0.0027670646 -0.0027669864][-0.0027687002 -0.00276828 -0.0027708181 -0.0027745545 -0.0027788703 -0.0027828331 -0.0027854631 -0.0027863251 -0.0027848308 -0.0027813993 -0.0027770654 -0.00277282 -0.0027698374 -0.0027677757 -0.0027671773][-0.0027684914 -0.0027676856 -0.0027696397 -0.0027725692 -0.0027761746 -0.002779481 -0.0027818482 -0.0027830112 -0.0027826233 -0.0027806214 -0.0027776903 -0.0027745918 -0.0027720283 -0.0027698765 -0.0027686604][-0.0027683678 -0.0027672278 -0.0027686006 -0.0027705312 -0.0027730006 -0.0027754507 -0.0027775401 -0.0027789793 -0.0027795914 -0.0027793278 -0.0027782947 -0.0027767515 -0.0027750928 -0.0027730155 -0.002771226][-0.0027680565 -0.002766523 -0.0027673857 -0.0027684711 -0.0027699859 -0.0027717266 -0.0027734 -0.0027750423 -0.0027767192 -0.0027784479 -0.0027796843 -0.0027802738 -0.0027799655 -0.002778522 -0.0027761294][-0.0027676029 -0.0027656686 -0.0027661489 -0.0027667235 -0.0027675619 -0.0027686921 -0.0027699941 -0.0027720286 -0.0027747294 -0.0027781271 -0.0027814906 -0.0027842314 -0.002785428 -0.002784577 -0.0027817315][-0.0027673647 -0.0027650907 -0.0027652779 -0.002765497 -0.0027659263 -0.0027667221 -0.0027678825 -0.0027702684 -0.0027736081 -0.0027782442 -0.0027832789 -0.0027876252 -0.0027901125 -0.0027898145 -0.0027867253]]...]
INFO - root - 2017-12-09 09:56:15.649082: step 13610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:54m:10s remains)
INFO - root - 2017-12-09 09:56:24.356612: step 13620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:07m:36s remains)
INFO - root - 2017-12-09 09:56:33.004623: step 13630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:28m:40s remains)
INFO - root - 2017-12-09 09:56:41.758977: step 13640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:16m:51s remains)
INFO - root - 2017-12-09 09:56:50.552016: step 13650, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 78h:18m:36s remains)
INFO - root - 2017-12-09 09:56:59.171274: step 13660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:13m:38s remains)
INFO - root - 2017-12-09 09:57:07.804653: step 13670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 73h:51m:18s remains)
INFO - root - 2017-12-09 09:57:16.344799: step 13680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:32m:08s remains)
INFO - root - 2017-12-09 09:57:24.801826: step 13690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:18m:01s remains)
INFO - root - 2017-12-09 09:57:33.347705: step 13700, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 77h:55m:14s remains)
2017-12-09 09:57:34.270781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026435158 -0.0026105857 -0.0025295671 -0.002402341 -0.0022611979 -0.0021474292 -0.0020922627 -0.002104 -0.0021711024 -0.0022682701 -0.0023679049 -0.0024509893 -0.0025109609 -0.0025537976 -0.0025908013][-0.0026114336 -0.0025782997 -0.0024874348 -0.0023290492 -0.0021311026 -0.0019423678 -0.0018095068 -0.0017573666 -0.0017883655 -0.0018827352 -0.0020067284 -0.0021278684 -0.0022290172 -0.0023127669 -0.0023881148][-0.0026029465 -0.0025791603 -0.0024826929 -0.0022991216 -0.0020512217 -0.0017898219 -0.001574285 -0.0014511533 -0.0014381504 -0.0015164807 -0.001642719 -0.0017753736 -0.0018920478 -0.0019922969 -0.0020810482][-0.0026306303 -0.0026020037 -0.0024858667 -0.0022679185 -0.0019734246 -0.0016584713 -0.001390176 -0.0012217879 -0.0011768499 -0.0012381696 -0.0013591518 -0.0014918164 -0.0016085346 -0.0017059764 -0.0017884636][-0.0026755568 -0.0026352848 -0.0024982989 -0.0022504504 -0.0019147058 -0.0015528003 -0.0012453317 -0.001052641 -0.00099563331 -0.0010503111 -0.0011662466 -0.0012952962 -0.0014078479 -0.0014960039 -0.0015605905][-0.0027101736 -0.0026597579 -0.0025143921 -0.0022543054 -0.0019005849 -0.0015201631 -0.0011956391 -0.00098725548 -0.0009173915 -0.00096125796 -0.0010648385 -0.0011773048 -0.0012718948 -0.0013436148 -0.0013957439][-0.0027178717 -0.0026740343 -0.0025474487 -0.002310372 -0.0019770414 -0.0016101762 -0.0012916841 -0.0010852796 -0.0010101668 -0.0010369953 -0.0011107578 -0.0011860674 -0.0012454933 -0.0012918006 -0.0013325014][-0.0027221132 -0.0026837923 -0.0025885389 -0.0024058926 -0.0021373106 -0.0018294455 -0.0015513239 -0.0013629077 -0.0012871847 -0.0012973925 -0.0013384997 -0.0013702094 -0.0013880606 -0.0014070059 -0.0014389117][-0.0027516619 -0.0027200638 -0.0026536263 -0.002529806 -0.0023423242 -0.0021173672 -0.0019049384 -0.0017561722 -0.0016927999 -0.0016942029 -0.0017144532 -0.0017210268 -0.0017161252 -0.0017201819 -0.0017478091][-0.0027887484 -0.0027673624 -0.0027279158 -0.0026569816 -0.0025465421 -0.002406677 -0.0022677425 -0.0021662817 -0.0021212355 -0.0021199191 -0.0021304283 -0.0021289675 -0.0021196548 -0.0021222497 -0.0021471556][-0.0028143874 -0.0028042076 -0.0027854799 -0.0027524203 -0.0026991197 -0.0026281725 -0.0025543442 -0.002499324 -0.0024757187 -0.0024757066 -0.0024809358 -0.0024780382 -0.0024715597 -0.0024748018 -0.0024931019][-0.0028191358 -0.00281759 -0.0028126389 -0.0028018525 -0.0027821281 -0.002754047 -0.0027236617 -0.0027001931 -0.0026892193 -0.0026869234 -0.0026870845 -0.0026848048 -0.0026818507 -0.0026835797 -0.0026916887][-0.0028158894 -0.00281721 -0.0028178231 -0.0028165118 -0.0028115846 -0.0028030509 -0.0027929819 -0.0027842312 -0.0027792244 -0.0027761734 -0.0027746845 -0.0027734835 -0.0027730707 -0.0027741625 -0.0027764519][-0.0028125569 -0.0028139772 -0.0028160962 -0.002817553 -0.0028171528 -0.0028145292 -0.0028108016 -0.0028063864 -0.0028027112 -0.0027998749 -0.002798751 -0.002798581 -0.0027994374 -0.0028006793 -0.0028013592][-0.0028100824 -0.0028106617 -0.0028125709 -0.0028143004 -0.0028147728 -0.0028136491 -0.0028115751 -0.0028087979 -0.0028061094 -0.002803731 -0.0028029676 -0.0028033354 -0.0028043953 -0.0028057809 -0.0028065385]]...]
INFO - root - 2017-12-09 09:57:42.834989: step 13710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:30m:00s remains)
INFO - root - 2017-12-09 09:57:51.505870: step 13720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:17m:47s remains)
INFO - root - 2017-12-09 09:58:00.241122: step 13730, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:09m:40s remains)
INFO - root - 2017-12-09 09:58:09.044328: step 13740, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 79h:24m:54s remains)
INFO - root - 2017-12-09 09:58:17.619772: step 13750, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 78h:01m:14s remains)
INFO - root - 2017-12-09 09:58:26.406976: step 13760, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 77h:33m:32s remains)
INFO - root - 2017-12-09 09:58:34.955783: step 13770, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 67h:40m:57s remains)
INFO - root - 2017-12-09 09:58:43.153668: step 13780, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 66h:51m:58s remains)
INFO - root - 2017-12-09 09:58:51.792376: step 13790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 78h:50m:51s remains)
INFO - root - 2017-12-09 09:59:00.368246: step 13800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:32m:47s remains)
2017-12-09 09:59:01.216524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028090545 -0.0028090419 -0.0028098268 -0.002810501 -0.0028101266 -0.0028091026 -0.0028078202 -0.0028067545 -0.0028061853 -0.0028054777 -0.0028044106 -0.0028033592 -0.0028026146 -0.002802101 -0.002801615][-0.0028104139 -0.0028108242 -0.0028123322 -0.0028133378 -0.0028126726 -0.0028109609 -0.0028092002 -0.0028076679 -0.0028069774 -0.0028061399 -0.0028048272 -0.0028035091 -0.0028025233 -0.0028016893 -0.0028007645][-0.0028135546 -0.0028145283 -0.0028166512 -0.0028172869 -0.0028158135 -0.0028132719 -0.0028111883 -0.0028097846 -0.002809529 -0.0028091141 -0.0028081585 -0.0028068267 -0.0028055382 -0.0028042137 -0.0028025941][-0.0028167502 -0.0028177942 -0.0028195765 -0.0028193237 -0.0028163823 -0.0028125204 -0.0028104649 -0.0028104535 -0.0028116493 -0.0028124214 -0.0028118477 -0.0028108242 -0.0028092358 -0.0028073143 -0.0028049736][-0.002819302 -0.0028198601 -0.0028210685 -0.0028195351 -0.0028147257 -0.0028097676 -0.0028081331 -0.0028094375 -0.0028126109 -0.0028149786 -0.0028151034 -0.0028144899 -0.0028132459 -0.0028108994 -0.0028077944][-0.0028215419 -0.0028213246 -0.0028217817 -0.0028188422 -0.0028123052 -0.0028065962 -0.0028059 -0.0028086705 -0.0028130445 -0.0028165802 -0.0028174073 -0.0028172175 -0.0028161365 -0.0028138086 -0.0028103071][-0.002823126 -0.0028226583 -0.0028223337 -0.002818288 -0.0028109332 -0.0028054402 -0.002805484 -0.002808498 -0.0028127716 -0.0028167537 -0.0028182671 -0.0028186673 -0.0028180331 -0.0028157472 -0.0028119551][-0.0028240848 -0.0028237216 -0.0028229977 -0.0028189211 -0.0028124945 -0.002808145 -0.0028084056 -0.002810155 -0.00281285 -0.0028155919 -0.00281708 -0.0028179311 -0.0028179088 -0.0028159255 -0.002812461][-0.0028245102 -0.0028249871 -0.00282489 -0.0028217703 -0.0028168901 -0.0028134889 -0.0028129802 -0.0028127416 -0.0028128051 -0.0028136503 -0.0028147614 -0.0028157136 -0.0028159127 -0.0028144212 -0.0028116615][-0.0028239423 -0.0028251363 -0.0028259836 -0.0028243836 -0.0028210368 -0.0028179626 -0.0028159497 -0.0028140575 -0.0028120403 -0.0028113041 -0.0028117485 -0.0028122752 -0.0028126459 -0.002811688 -0.0028096335][-0.002821977 -0.0028235451 -0.0028251493 -0.002824408 -0.0028217242 -0.0028183027 -0.0028151227 -0.002812152 -0.0028091711 -0.0028075497 -0.0028073546 -0.0028076679 -0.0028082491 -0.0028078696 -0.0028067685][-0.0028184408 -0.0028197437 -0.0028215451 -0.0028215447 -0.0028193262 -0.0028160482 -0.0028124191 -0.0028090493 -0.0028058931 -0.0028037336 -0.0028029806 -0.0028032251 -0.0028037638 -0.0028039294 -0.0028038002][-0.0028147609 -0.002815403 -0.0028168354 -0.0028171449 -0.002815566 -0.0028125648 -0.0028090938 -0.0028060724 -0.0028033271 -0.0028011997 -0.0028003415 -0.0028004993 -0.0028008344 -0.0028011713 -0.0028015519][-0.0028099774 -0.0028096989 -0.0028106943 -0.0028109739 -0.0028099206 -0.0028077315 -0.0028051841 -0.0028030255 -0.0028009862 -0.002799277 -0.0027985396 -0.0027985997 -0.0027988937 -0.0027992579 -0.0027998791][-0.0028058896 -0.0028048104 -0.0028049031 -0.0028048588 -0.0028041319 -0.0028028742 -0.002801511 -0.0028002032 -0.0027988914 -0.0027978267 -0.0027973547 -0.0027975049 -0.0027977824 -0.0027981212 -0.0027986106]]...]
INFO - root - 2017-12-09 09:59:09.833758: step 13810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:23m:24s remains)
INFO - root - 2017-12-09 09:59:18.428472: step 13820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:18m:30s remains)
INFO - root - 2017-12-09 09:59:26.939727: step 13830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:59m:13s remains)
INFO - root - 2017-12-09 09:59:35.557979: step 13840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:11m:19s remains)
INFO - root - 2017-12-09 09:59:44.319018: step 13850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:07m:42s remains)
INFO - root - 2017-12-09 09:59:53.082010: step 13860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 77h:11m:51s remains)
INFO - root - 2017-12-09 10:00:01.878380: step 13870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:45m:24s remains)
INFO - root - 2017-12-09 10:00:10.347613: step 13880, loss = 0.91, batch loss = 0.70 (10.7 examples/sec; 0.747 sec/batch; 66h:05m:42s remains)
INFO - root - 2017-12-09 10:00:19.008973: step 13890, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 75h:59m:02s remains)
INFO - root - 2017-12-09 10:00:27.474301: step 13900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:41m:35s remains)
2017-12-09 10:00:28.298740: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053269345 0.054655563 0.055527527 0.056978818 0.058030762 0.057288766 0.055141471 0.050832815 0.045659073 0.039354827 0.033694312 0.029067153 0.025477434 0.02285658 0.021337921][0.058600143 0.059168145 0.059194371 0.060519733 0.061716855 0.061872661 0.060493022 0.056684986 0.052259773 0.045854837 0.03997061 0.035352912 0.031362176 0.028666964 0.026864136][0.065395236 0.0670233 0.0679697 0.070327573 0.072834179 0.073906533 0.072765909 0.069718964 0.0657658 0.060167588 0.053358026 0.047997948 0.043139484 0.038948532 0.035649318][0.072592407 0.077542506 0.081385404 0.087025784 0.092460111 0.095595211 0.096209273 0.093862742 0.090514548 0.084305435 0.076543637 0.070099421 0.063847058 0.058605317 0.054270722][0.093214013 0.10109846 0.10797895 0.11531728 0.12227076 0.12799114 0.1307676 0.12999496 0.12799636 0.1222819 0.1141944 0.1062592 0.0982372 0.0913857 0.085778862][0.12596595 0.13615468 0.14518727 0.15400618 0.16152824 0.16819543 0.17161623 0.17134438 0.16954909 0.16366671 0.15560941 0.14693166 0.13809921 0.13042314 0.12406639][0.16596495 0.17779538 0.18775044 0.19682664 0.20347103 0.20901839 0.21169129 0.21115649 0.20975949 0.20497315 0.19815756 0.19017126 0.18167412 0.17413241 0.16787417][0.20557691 0.21720479 0.22671102 0.23505485 0.24041277 0.24465314 0.2469767 0.24711412 0.24627252 0.24288583 0.23767154 0.23062526 0.22289339 0.21594782 0.20963521][0.23948556 0.24997765 0.2579352 0.26502377 0.26913315 0.2724258 0.27372938 0.27374783 0.27307439 0.27096426 0.26801804 0.26308167 0.25765085 0.25173962 0.24635041][0.26421604 0.27330419 0.27881649 0.28338751 0.28569207 0.28805733 0.28822476 0.28849396 0.28850344 0.287949 0.28683224 0.28364736 0.28050032 0.27580497 0.27166745][0.2765412 0.28516003 0.28867403 0.2920182 0.29383427 0.2949661 0.29412508 0.29390702 0.2940616 0.29467171 0.29535714 0.29469669 0.29440451 0.29240391 0.29093245][0.27935252 0.28821656 0.29042345 0.29331908 0.29565179 0.29654837 0.29547966 0.29505402 0.29543355 0.29668775 0.2987504 0.30054837 0.30334914 0.30454409 0.30623344][0.2786777 0.28777286 0.28881842 0.29096124 0.29308167 0.29414272 0.29322964 0.29263029 0.292894 0.2946012 0.29834446 0.30276039 0.3088237 0.31371048 0.31873012][0.28041863 0.28964022 0.28926733 0.29032069 0.29162285 0.29177022 0.29039213 0.28916132 0.28891593 0.29076627 0.29601121 0.30285078 0.31160402 0.31981143 0.32816333][0.27816933 0.28584641 0.2832405 0.28149065 0.28090528 0.28038821 0.27866307 0.27630216 0.2754817 0.27740046 0.28290689 0.29062539 0.30065989 0.31180692 0.32267937]]...]
INFO - root - 2017-12-09 10:00:36.825425: step 13910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:11m:01s remains)
INFO - root - 2017-12-09 10:00:45.433104: step 13920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:20m:28s remains)
INFO - root - 2017-12-09 10:00:54.148762: step 13930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:27m:49s remains)
INFO - root - 2017-12-09 10:01:02.843968: step 13940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:51m:32s remains)
INFO - root - 2017-12-09 10:01:11.489474: step 13950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:23m:27s remains)
INFO - root - 2017-12-09 10:01:20.108743: step 13960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 75h:42m:25s remains)
INFO - root - 2017-12-09 10:01:28.514793: step 13970, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 72h:57m:41s remains)
INFO - root - 2017-12-09 10:01:36.872294: step 13980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:11m:07s remains)
INFO - root - 2017-12-09 10:01:45.269651: step 13990, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 75h:34m:51s remains)
INFO - root - 2017-12-09 10:01:53.816870: step 14000, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 76h:02m:49s remains)
2017-12-09 10:01:54.795893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002812865 -0.0028110303 -0.002811474 -0.0028123688 -0.0028135055 -0.0028147367 -0.0028158496 -0.0028170021 -0.0028178063 -0.0028182517 -0.0028186466 -0.002819506 -0.0028212948 -0.0028247857 -0.002829928][-0.0028118324 -0.0028099562 -0.002810705 -0.0028119583 -0.0028129653 -0.0028139881 -0.0028146741 -0.0028152727 -0.0028155323 -0.0028156748 -0.0028162468 -0.0028177381 -0.0028207002 -0.0028257652 -0.0028322027][-0.002812922 -0.0028111709 -0.0028121686 -0.0028134605 -0.0028142924 -0.0028146799 -0.0028147236 -0.002814637 -0.0028143628 -0.0028142687 -0.0028152179 -0.002817496 -0.0028220692 -0.0028284558 -0.0028359909][-0.0028138531 -0.002812335 -0.0028133718 -0.0028144654 -0.0028149213 -0.0028147546 -0.0028139837 -0.0028130668 -0.0028123339 -0.0028121672 -0.0028136407 -0.0028172426 -0.0028233223 -0.002830737 -0.002838915][-0.0028147174 -0.0028130563 -0.0028139225 -0.0028143567 -0.002814119 -0.0028132838 -0.0028120745 -0.0028108526 -0.0028101411 -0.0028105911 -0.0028131355 -0.0028182089 -0.0028253407 -0.0028332851 -0.0028414687][-0.0028153078 -0.00281335 -0.0028135739 -0.0028131611 -0.00281205 -0.0028106945 -0.0028094503 -0.0028083252 -0.0028081653 -0.0028098766 -0.0028139092 -0.0028202226 -0.0028280364 -0.0028365515 -0.0028438468][-0.0028155388 -0.0028131257 -0.0028125744 -0.0028112128 -0.0028093741 -0.0028075424 -0.0028062549 -0.0028054076 -0.002806101 -0.0028093681 -0.0028147558 -0.0028217547 -0.0028298919 -0.0028377934 -0.0028437132][-0.0028153134 -0.002812532 -0.0028113422 -0.0028091297 -0.0028065313 -0.0028044642 -0.0028032097 -0.0028028318 -0.0028046216 -0.0028092631 -0.0028154931 -0.0028227821 -0.0028302202 -0.0028368635 -0.0028412344][-0.0028146661 -0.002811596 -0.002809922 -0.0028072284 -0.0028041198 -0.0028020327 -0.0028011277 -0.0028015831 -0.0028044251 -0.0028096915 -0.0028160757 -0.0028228671 -0.0028288418 -0.0028337794 -0.0028365501][-0.0028139038 -0.0028106216 -0.0028088167 -0.0028059906 -0.0028028849 -0.0028008409 -0.0028004255 -0.0028018169 -0.002805101 -0.0028101187 -0.0028158613 -0.002821435 -0.0028256818 -0.00282887 -0.002830453][-0.0028129206 -0.0028096337 -0.0028080742 -0.0028056796 -0.0028030074 -0.0028013454 -0.0028013741 -0.0028030474 -0.0028060423 -0.0028100861 -0.0028147106 -0.0028186629 -0.0028212466 -0.0028231097 -0.0028237365][-0.0028120389 -0.0028091767 -0.0028081341 -0.002806294 -0.002804213 -0.0028030514 -0.0028033177 -0.0028046872 -0.0028070605 -0.0028101658 -0.0028133842 -0.0028157425 -0.0028172042 -0.0028179998 -0.0028178226][-0.0028113518 -0.0028088943 -0.0028084023 -0.0028072186 -0.002805792 -0.0028052046 -0.0028055895 -0.0028065294 -0.0028081879 -0.0028102584 -0.00281211 -0.0028132857 -0.0028137709 -0.0028137441 -0.002813153][-0.002811098 -0.0028085702 -0.0028085669 -0.00280802 -0.0028073606 -0.0028071983 -0.0028075604 -0.0028080577 -0.0028089252 -0.0028099304 -0.0028106475 -0.0028109387 -0.002810929 -0.0028106666 -0.0028101765][-0.0028109704 -0.0028082412 -0.0028085257 -0.002808461 -0.0028083462 -0.0028083664 -0.0028085466 -0.0028086808 -0.0028088959 -0.002809125 -0.0028092174 -0.0028092021 -0.0028091744 -0.0028090517 -0.0028088663]]...]
INFO - root - 2017-12-09 10:02:03.301872: step 14010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:53m:11s remains)
INFO - root - 2017-12-09 10:02:11.920609: step 14020, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:22m:52s remains)
INFO - root - 2017-12-09 10:02:20.381454: step 14030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:09m:18s remains)
INFO - root - 2017-12-09 10:02:28.847130: step 14040, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:59m:45s remains)
INFO - root - 2017-12-09 10:02:37.435490: step 14050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:54m:29s remains)
INFO - root - 2017-12-09 10:02:46.029421: step 14060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:22m:08s remains)
INFO - root - 2017-12-09 10:02:54.581086: step 14070, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 74h:07m:42s remains)
INFO - root - 2017-12-09 10:03:02.929294: step 14080, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:20m:57s remains)
INFO - root - 2017-12-09 10:03:11.452913: step 14090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:20m:12s remains)
INFO - root - 2017-12-09 10:03:19.993580: step 14100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:27m:28s remains)
2017-12-09 10:03:20.897665: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0059241452 0.0061143036 0.0058987462 0.0055252877 0.0051240935 0.0046926765 0.0042507043 0.0037985169 0.0033508153 0.0029401714 0.0024833726 0.0018168641 0.00087984418 -0.00024301768 -0.0013169355][0.00886317 0.0094543751 0.0093344972 0.00909386 0.008944083 0.0089960676 0.0091125295 0.0091653429 0.0090172738 0.0086066 0.0078341775 0.006542414 0.0046933666 0.0025049332 0.00038630073][0.0172022 0.018177468 0.017646642 0.017013464 0.016500689 0.016404398 0.016439654 0.016578682 0.016538698 0.016157728 0.015171209 0.013320321 0.010443654 0.0068298853 0.0030992352][0.031776898 0.033159826 0.031991158 0.030386085 0.02870203 0.027578771 0.026559446 0.025729457 0.024790069 0.023672251 0.021977797 0.019307977 0.015410408 0.010644017 0.00568601][0.0469389 0.048285551 0.046026479 0.043213326 0.040086553 0.037451856 0.034899473 0.032670673 0.030399824 0.02816553 0.02557683 0.022152053 0.017538797 0.012109131 0.0065816003][0.057341628 0.058243178 0.054867834 0.050665282 0.045939948 0.04174906 0.037525397 0.033635724 0.029824091 0.026400775 0.023084303 0.019415768 0.014988061 0.010046791 0.0051767514][0.060317177 0.060894947 0.05684936 0.051659074 0.045682684 0.039985411 0.034166824 0.028825296 0.023770003 0.019551609 0.016012242 0.01272318 0.00922606 0.00559638 0.0022158045][0.057027355 0.057206448 0.052892249 0.047066059 0.040178765 0.033477757 0.02669966 0.020562349 0.015117826 0.010934032 0.007855624 0.0054689632 0.0032916684 0.0012098202 -0.000577352][0.04808569 0.048463605 0.044389877 0.038462132 0.031312495 0.024200965 0.017324606 0.01147732 0.0067598321 0.0035013109 0.0014758641 0.00021895161 -0.00072837737 -0.0015598675 -0.0022068832][0.035156727 0.035514563 0.032008477 0.026708933 0.020354876 0.014063096 0.0083886664 0.0039931238 0.000939857 -0.00087654847 -0.0018081823 -0.0022312438 -0.0024673329 -0.0026525126 -0.0027724502][0.020902639 0.021168096 0.01863838 0.014730422 0.010121282 0.00569943 0.0020041279 -0.00051928149 -0.0019015252 -0.0025444461 -0.002776579 -0.0028383823 -0.0028475833 -0.0028522622 -0.0028547966][0.0086985324 0.0087607391 0.0073605138 0.0052118511 0.0027278354 0.00041348627 -0.0013835239 -0.0024016029 -0.002762934 -0.0028444354 -0.0028535996 -0.0028543505 -0.0028543081 -0.0028549312 -0.00285478][0.001055337 0.0010967879 0.00060559437 -0.00017542229 -0.0010705487 -0.0018956506 -0.0025064019 -0.0027852412 -0.0028435353 -0.0028508122 -0.0028506459 -0.0028503027 -0.0028499281 -0.0028500853 -0.0028499353][-0.0022979691 -0.0022541122 -0.0023016632 -0.0024101923 -0.0025500276 -0.0026869532 -0.0027926357 -0.0028448096 -0.0028509719 -0.0028517852 -0.0028505852 -0.002849174 -0.0028480075 -0.0028479313 -0.0028473185][-0.0028489756 -0.0028420291 -0.0028403753 -0.0028407283 -0.0028433951 -0.0028460475 -0.0028494741 -0.0028523114 -0.0028537733 -0.0028534841 -0.0028512303 -0.0028492145 -0.0028473323 -0.0028471693 -0.0028473071]]...]
INFO - root - 2017-12-09 10:03:29.508913: step 14110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:33m:46s remains)
INFO - root - 2017-12-09 10:03:38.177780: step 14120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:00m:49s remains)
INFO - root - 2017-12-09 10:03:46.948024: step 14130, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 79h:55m:12s remains)
INFO - root - 2017-12-09 10:03:55.723090: step 14140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:18m:03s remains)
INFO - root - 2017-12-09 10:04:04.300608: step 14150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:38m:21s remains)
INFO - root - 2017-12-09 10:04:12.707184: step 14160, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:23m:22s remains)
INFO - root - 2017-12-09 10:04:21.346446: step 14170, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 76h:50m:20s remains)
INFO - root - 2017-12-09 10:04:29.685528: step 14180, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:34m:43s remains)
INFO - root - 2017-12-09 10:04:38.083262: step 14190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:00m:39s remains)
INFO - root - 2017-12-09 10:04:46.728202: step 14200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:11m:47s remains)
2017-12-09 10:04:47.599385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025868232 -0.002429032 -0.0018350563 -0.00060500344 0.0012125862 0.00304587 0.004054755 0.0037758159 0.002272978 0.00042236573 -0.0011305953 -0.0019973461 -0.0023271393 -0.0024993639 -0.0026255217][-0.0014656617 -0.0011291517 3.7927181e-05 0.0022351593 0.0051622321 0.007831797 0.0090807527 0.0083526149 0.00579909 0.0026297523 -0.00014955085 -0.0017815394 -0.0023952494 -0.0025501577 -0.0026209683][-0.00022256281 0.00089995051 0.0031217611 0.0067470181 0.011275316 0.015174313 0.016839085 0.015581951 0.0116711 0.0066550612 0.0020389953 -0.00096485124 -0.0022405977 -0.0025956985 -0.0026885192][0.0003488136 0.0023948038 0.0062338286 0.01203208 0.01870349 0.024142697 0.026527524 0.024968071 0.019636456 0.012349406 0.00529951 0.00040715281 -0.0018552206 -0.0025700498 -0.0026930796][0.00080157816 0.0033926838 0.0086201448 0.016479049 0.025436739 0.032844752 0.036463056 0.0351395 0.028950494 0.019643612 0.0099590309 0.0027220552 -0.0010223537 -0.0023957554 -0.0026740043][0.0010272125 0.0042719031 0.010503452 0.019744214 0.030437864 0.039717261 0.044971183 0.044599131 0.038357448 0.027774207 0.015920628 0.0062422086 0.00057901721 -0.0018448492 -0.0025190245][0.00058969366 0.003863405 0.010588681 0.020892126 0.032984883 0.043839853 0.050690137 0.05173637 0.046399206 0.035708919 0.022652267 0.011044307 0.003408228 -0.00045502908 -0.0018862615][0.00020252564 0.0029873876 0.0093522724 0.019566469 0.03215025 0.0441284 0.052464727 0.055093657 0.051219374 0.041532431 0.028678332 0.016296204 0.0072497739 0.0020673699 -0.00028489158][2.2678869e-05 0.0021716715 0.00749566 0.016680533 0.028754428 0.040942065 0.050202973 0.05425005 0.052076314 0.044079959 0.03247055 0.020531841 0.01104717 0.0050384174 0.0018570754][-0.00038465369 0.001255237 0.005421869 0.012929039 0.023447456 0.03480744 0.044216525 0.049216755 0.048627652 0.042595979 0.032887988 0.022307383 0.013325196 0.0071369144 0.0034545944][-0.0010820442 7.7206176e-05 0.0030886699 0.0087675732 0.017198764 0.026798809 0.035320181 0.040477071 0.04105198 0.03699141 0.029599048 0.021042328 0.013267547 0.0074813715 0.0037136134][-0.0016857567 -0.0010537753 0.000814385 0.0046329573 0.01074484 0.018162064 0.025160065 0.029732838 0.030779015 0.028227212 0.023034863 0.016756568 0.010743203 0.0059723617 0.0026492258][-0.0021531335 -0.0018502466 -0.00089025567 0.0012824712 0.0050994908 0.010089358 0.015148945 0.018728714 0.019868517 0.018451396 0.015147697 0.011000391 0.0068684067 0.0034401054 0.00094898674][-0.0025018202 -0.002385627 -0.0019711333 -0.000915647 0.0011164267 0.0039851745 0.0070958831 0.0094575156 0.01036495 0.0096844807 0.0078402087 0.0054543479 0.0030051395 0.000902846 -0.00067093689][-0.0026964701 -0.0026710993 -0.0025402617 -0.0021225847 -0.0012122494 0.00019353768 0.0018264763 0.0031481711 0.0037199529 0.0034423159 0.002547754 0.001363026 0.00011703814 -0.00098287663 -0.0018252383]]...]
INFO - root - 2017-12-09 10:04:56.107537: step 14210, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:15m:31s remains)
INFO - root - 2017-12-09 10:05:04.581708: step 14220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:21m:25s remains)
INFO - root - 2017-12-09 10:05:13.055922: step 14230, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 74h:35m:35s remains)
INFO - root - 2017-12-09 10:05:21.407963: step 14240, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 74h:00m:55s remains)
INFO - root - 2017-12-09 10:05:30.137164: step 14250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:13m:13s remains)
INFO - root - 2017-12-09 10:05:38.890383: step 14260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:28m:53s remains)
INFO - root - 2017-12-09 10:05:47.613077: step 14270, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:36m:34s remains)
INFO - root - 2017-12-09 10:05:56.079644: step 14280, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:24m:51s remains)
INFO - root - 2017-12-09 10:06:04.695351: step 14290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 76h:02m:12s remains)
INFO - root - 2017-12-09 10:06:13.097652: step 14300, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 74h:00m:57s remains)
2017-12-09 10:06:13.966886: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.299206 0.29633814 0.29047051 0.28480151 0.27586308 0.26571304 0.25027251 0.23184507 0.21093121 0.19124545 0.17416185 0.15982945 0.14747016 0.13673885 0.12622757][0.324517 0.32372063 0.31950176 0.31482768 0.30525491 0.2926577 0.2720896 0.24635549 0.21672446 0.18977478 0.16740416 0.15046759 0.13749515 0.12796663 0.12013351][0.34362623 0.34465927 0.34080622 0.33612677 0.32475507 0.30871615 0.2831355 0.25085035 0.21365999 0.17918788 0.15078518 0.13091902 0.11757739 0.10994755 0.10605134][0.35902718 0.36405259 0.36153868 0.35547569 0.34094477 0.31992829 0.28794274 0.24864483 0.20462413 0.16473094 0.13245885 0.11059915 0.097060435 0.091281712 0.090941392][0.36294931 0.37357488 0.37415445 0.36911488 0.353579 0.32911211 0.29196912 0.24656221 0.19634193 0.15148115 0.11613622 0.093431152 0.0804067 0.076064147 0.078630969][0.36366442 0.37844563 0.38091728 0.37751985 0.36289856 0.33743143 0.29759771 0.24830833 0.19354181 0.14509574 0.10758138 0.083885252 0.070924625 0.068074748 0.0727143][0.364617 0.383262 0.38685867 0.38397098 0.37023231 0.345224 0.30509797 0.25450402 0.19784693 0.14795835 0.10925268 0.084984019 0.071703382 0.068555623 0.073029377][0.3632049 0.38538793 0.38947 0.38681307 0.37389165 0.34959096 0.31027877 0.26090544 0.20554851 0.15686911 0.11918509 0.094773471 0.080835886 0.076417528 0.078881346][0.35500324 0.38053969 0.38634691 0.38511756 0.37429479 0.35247147 0.3157931 0.26917368 0.21635476 0.1703828 0.13495694 0.11165748 0.097483955 0.091116659 0.090336226][0.33862266 0.36537746 0.37244278 0.37425813 0.36823303 0.35187891 0.32081652 0.27957234 0.23140977 0.18878807 0.15559629 0.13345696 0.11857121 0.10970382 0.10505932][0.32256818 0.34798804 0.35399988 0.35701227 0.35454252 0.34364361 0.32021496 0.28698149 0.24610223 0.20825319 0.17759575 0.15595335 0.13963924 0.12821102 0.11945456][0.3033247 0.32750639 0.33257076 0.33528647 0.33407891 0.32761353 0.31116506 0.28613183 0.25366369 0.22233275 0.19540584 0.17478202 0.1574261 0.14332342 0.13082832][0.2781738 0.30043408 0.30462685 0.30729482 0.30737856 0.3039349 0.29318979 0.27590483 0.251956 0.22706389 0.20422885 0.18542269 0.1679683 0.15228602 0.1372285][0.24721169 0.26666638 0.2700361 0.27229154 0.27300727 0.27214423 0.26657122 0.25612792 0.24002416 0.22159603 0.20340246 0.18687004 0.17023301 0.1542758 0.13837312][0.21323304 0.22945714 0.23159367 0.23297642 0.23365054 0.23397565 0.23172502 0.22629586 0.21684308 0.20445485 0.19108574 0.17779128 0.16342343 0.14882709 0.1336486]]...]
INFO - root - 2017-12-09 10:06:22.496923: step 14310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:52m:14s remains)
INFO - root - 2017-12-09 10:06:31.142368: step 14320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 76h:47m:24s remains)
INFO - root - 2017-12-09 10:06:39.844833: step 14330, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:33m:37s remains)
INFO - root - 2017-12-09 10:06:48.637463: step 14340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:19m:23s remains)
INFO - root - 2017-12-09 10:06:57.179719: step 14350, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:28m:01s remains)
INFO - root - 2017-12-09 10:07:05.761569: step 14360, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 78h:49m:09s remains)
INFO - root - 2017-12-09 10:07:14.562622: step 14370, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:27m:59s remains)
INFO - root - 2017-12-09 10:07:23.312067: step 14380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:43m:32s remains)
INFO - root - 2017-12-09 10:07:31.642923: step 14390, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 76h:54m:49s remains)
INFO - root - 2017-12-09 10:07:40.228049: step 14400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:22m:57s remains)
2017-12-09 10:07:41.098789: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011796904 0.011145169 0.014449895 0.019786637 0.026873272 0.033004537 0.037325904 0.039661855 0.03903988 0.036402311 0.033046737 0.030870926 0.030615509 0.030027717 0.029673114][0.015751019 0.016685981 0.021899913 0.030107655 0.040973861 0.050202202 0.057413526 0.06115862 0.061166778 0.057713088 0.052698005 0.049183246 0.048051443 0.047423881 0.047611792][0.020748012 0.025417592 0.035201803 0.049827583 0.066837884 0.082180373 0.0943256 0.10140198 0.10343181 0.099093646 0.092482574 0.087545946 0.085954912 0.08517763 0.085876927][0.027266428 0.037854996 0.055062555 0.079395726 0.1053142 0.129668 0.14897305 0.16109647 0.16603228 0.16215271 0.15532099 0.14940941 0.14720184 0.14683254 0.14883353][0.034525275 0.052777842 0.07995908 0.11698606 0.15491888 0.19024746 0.21816234 0.23698929 0.24625902 0.24387307 0.23735507 0.23097122 0.22845043 0.22749904 0.23007724][0.042375457 0.067941785 0.10511074 0.1546203 0.20497434 0.25241321 0.28983036 0.31598851 0.33041546 0.33220378 0.32814643 0.32295296 0.32062355 0.31944367 0.32220694][0.048997656 0.080019094 0.12463654 0.18315744 0.2430592 0.30018675 0.34568882 0.37836027 0.3972463 0.40377733 0.40356961 0.40156484 0.40106314 0.40026242 0.40285504][0.053660247 0.087366343 0.13629223 0.19892913 0.26350856 0.32496715 0.37408191 0.4100568 0.43156084 0.44235775 0.44569016 0.44733858 0.44950518 0.45067489 0.45465431][0.051862661 0.0846551 0.13270307 0.19350791 0.257447 0.31833023 0.3669605 0.40257889 0.42392784 0.43701571 0.44241288 0.44672117 0.45162585 0.45601004 0.46252355][0.043799452 0.07213816 0.11399782 0.16717319 0.22420287 0.27871042 0.32274657 0.35481584 0.37459159 0.38733828 0.39324415 0.39945903 0.40693218 0.41453049 0.42400664][0.032072186 0.05370171 0.085910283 0.12722497 0.17215294 0.21594357 0.2517018 0.27761143 0.29383644 0.305057 0.31123611 0.31878889 0.32805502 0.33886555 0.35149485][0.0199158 0.034573942 0.056609996 0.0852458 0.1165966 0.14761144 0.17306502 0.19165106 0.20357348 0.21164136 0.21686429 0.22422092 0.23398639 0.24637689 0.26087245][0.0096266875 0.018366214 0.031684898 0.049140334 0.068367526 0.087663442 0.10341538 0.11505865 0.12261716 0.12770975 0.13126169 0.13714144 0.14524804 0.15662894 0.17068639][0.0020150887 0.0060655456 0.012658973 0.021696184 0.0316957 0.041746713 0.050073668 0.05644976 0.06069015 0.063437574 0.065596707 0.069807984 0.075713046 0.084492557 0.095646076][-0.0017558716 -0.00047992193 0.0018681013 0.0052868128 0.009229932 0.013115057 0.016583681 0.019307222 0.021339547 0.022763692 0.024113752 0.02695483 0.030601546 0.036330253 0.044013105]]...]
INFO - root - 2017-12-09 10:07:49.779412: step 14410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:05m:41s remains)
INFO - root - 2017-12-09 10:07:58.592456: step 14420, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 79h:30m:51s remains)
INFO - root - 2017-12-09 10:08:07.370632: step 14430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:01m:57s remains)
INFO - root - 2017-12-09 10:08:16.150609: step 14440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:40m:19s remains)
INFO - root - 2017-12-09 10:08:25.013119: step 14450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:21m:59s remains)
INFO - root - 2017-12-09 10:08:33.685674: step 14460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:41m:14s remains)
INFO - root - 2017-12-09 10:08:42.586020: step 14470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:05m:41s remains)
INFO - root - 2017-12-09 10:08:51.343724: step 14480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:08m:43s remains)
INFO - root - 2017-12-09 10:08:59.706325: step 14490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:45m:44s remains)
INFO - root - 2017-12-09 10:09:08.360584: step 14500, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:30m:21s remains)
2017-12-09 10:09:09.246909: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11757272 0.12562825 0.13886932 0.15786839 0.17610539 0.19212437 0.2039596 0.21137391 0.21462436 0.21519065 0.21270937 0.20837587 0.20323989 0.1986901 0.19509846][0.099759825 0.11012977 0.12585844 0.14754629 0.16945015 0.19088846 0.20708802 0.21900816 0.22585975 0.22797425 0.22643664 0.2224558 0.21697804 0.21143012 0.20637429][0.076013893 0.087243475 0.10364693 0.12664247 0.15171075 0.17744955 0.19886298 0.21563071 0.22582023 0.23074456 0.23018412 0.22699644 0.22135286 0.21600258 0.21101683][0.054642528 0.066122748 0.081998587 0.10551447 0.1319889 0.1604102 0.18624094 0.20741235 0.22096828 0.2291595 0.23023193 0.2285471 0.2230221 0.21765709 0.21177366][0.037607607 0.048147935 0.062858693 0.085490152 0.11200434 0.14261602 0.17128128 0.19544582 0.21296729 0.22405939 0.22750661 0.22681181 0.2204081 0.21390811 0.20621279][0.026825892 0.034838621 0.04695832 0.066812962 0.092246227 0.12310581 0.15286522 0.17948948 0.1999882 0.21363559 0.21937014 0.21888117 0.21143895 0.20251875 0.19282942][0.018959414 0.024112925 0.033061933 0.049354266 0.072435647 0.10181402 0.13158679 0.1594076 0.18202801 0.19877522 0.20644887 0.2053431 0.19564851 0.1829524 0.17058842][0.012792228 0.016084101 0.022055656 0.034355152 0.053951241 0.080298804 0.10827231 0.13548413 0.15964422 0.17924021 0.18957561 0.18863055 0.17608882 0.15866621 0.14255887][0.0071700057 0.0093485275 0.013232283 0.022340769 0.037619665 0.05980207 0.084560283 0.10994948 0.13451621 0.15596956 0.16866596 0.16830802 0.15402898 0.13295613 0.11278781][0.0036766471 0.005081079 0.0071473373 0.013132006 0.023795085 0.041188143 0.061670557 0.0845155 0.10858499 0.13120231 0.1453678 0.1452737 0.13011232 0.10678992 0.084254079][0.0024874362 0.0030647505 0.003763506 0.00741197 0.014580604 0.027679225 0.043845609 0.063716769 0.085942619 0.10821499 0.12249027 0.12252106 0.10710817 0.082722694 0.059352666][0.0015655307 0.0021539421 0.0021557787 0.0042516654 0.0087802811 0.018765906 0.032085575 0.050052594 0.070394874 0.090812311 0.10369706 0.10333948 0.088110484 0.063991055 0.04083655][0.00049598678 0.001242832 0.0013891354 0.0029798278 0.0058848169 0.013014676 0.023877993 0.039913297 0.058975004 0.078081556 0.089835793 0.089276448 0.07483495 0.051987205 0.029793231][-0.00070490292 -5.878089e-05 0.00029899203 0.0016700055 0.0036101018 0.0087289931 0.017757919 0.032530308 0.050801095 0.068801887 0.079936139 0.0798177 0.066905387 0.0460075 0.025301078][-0.00096341106 -0.00072456105 -0.00079843635 -0.00010086945 0.00095736305 0.0045712562 0.012200165 0.025727989 0.04344042 0.060942166 0.072079383 0.072763667 0.061562538 0.042832896 0.023914462]]...]
INFO - root - 2017-12-09 10:09:17.827331: step 14510, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 77h:16m:19s remains)
INFO - root - 2017-12-09 10:09:26.617092: step 14520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:20m:49s remains)
INFO - root - 2017-12-09 10:09:35.345492: step 14530, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:25m:12s remains)
INFO - root - 2017-12-09 10:09:43.974354: step 14540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:51m:28s remains)
INFO - root - 2017-12-09 10:09:52.472071: step 14550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:52m:15s remains)
INFO - root - 2017-12-09 10:10:00.934145: step 14560, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 73h:52m:55s remains)
INFO - root - 2017-12-09 10:10:09.504520: step 14570, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:36m:01s remains)
INFO - root - 2017-12-09 10:10:18.199529: step 14580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 75h:58m:49s remains)
INFO - root - 2017-12-09 10:10:26.439298: step 14590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:30m:28s remains)
INFO - root - 2017-12-09 10:10:34.982852: step 14600, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 77h:07m:20s remains)
2017-12-09 10:10:35.815941: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033769015 0.038086385 0.042677205 0.046381969 0.048552573 0.048018046 0.045412891 0.040402204 0.033458829 0.026718181 0.020247377 0.015581654 0.012316855 0.010470228 0.0087723974][0.034973055 0.040334918 0.046756975 0.053279575 0.05818909 0.059462395 0.057259187 0.051237866 0.042562846 0.033020597 0.023917826 0.017306833 0.012961067 0.011211414 0.010231255][0.045776933 0.053053305 0.061556045 0.070615239 0.078079768 0.081574939 0.079636335 0.072858505 0.061593879 0.047376137 0.033403713 0.022126643 0.014277674 0.0099223237 0.0078846551][0.058158368 0.070532672 0.084097877 0.098435894 0.11034216 0.11691743 0.11577863 0.10819801 0.092690729 0.071920805 0.050661631 0.032621406 0.019334301 0.010921001 0.00806101][0.0774748 0.094688423 0.1127587 0.13188398 0.14790727 0.15850461 0.15955649 0.15233085 0.13527934 0.10942703 0.08076252 0.054080117 0.033727389 0.02008339 0.014997948][0.11317511 0.13475721 0.15546447 0.17553972 0.19186859 0.20300746 0.20587952 0.19997168 0.18316668 0.15534481 0.12203728 0.08859013 0.061312769 0.042530984 0.035073973][0.15964861 0.18543307 0.20828091 0.22768249 0.24254186 0.2521463 0.25483736 0.24927527 0.23338334 0.20665087 0.17148413 0.13405786 0.10169694 0.078109846 0.067461528][0.20752239 0.23511356 0.25765848 0.27569526 0.28906429 0.29764986 0.30096689 0.29706034 0.28281307 0.25821352 0.22377962 0.18523251 0.149368 0.12219753 0.10894351][0.2450131 0.27193961 0.29250079 0.30854997 0.32021609 0.32798123 0.33201402 0.33029696 0.31956437 0.29955104 0.26873934 0.2325151 0.19708836 0.16906677 0.15390041][0.2669839 0.29160947 0.3086279 0.32161638 0.33093569 0.33764058 0.34185126 0.34200394 0.334824 0.3200013 0.29559481 0.265665 0.23495464 0.20973016 0.19436325][0.27197933 0.29389125 0.30696681 0.31657413 0.32325995 0.3279573 0.3309879 0.33159557 0.32687429 0.31668526 0.29910719 0.27719328 0.25386202 0.2339984 0.22135921][0.2616649 0.28036979 0.28979591 0.29670548 0.30149904 0.30473027 0.30668783 0.30669722 0.3032828 0.29638544 0.28466672 0.26999924 0.25418362 0.24066332 0.23174998][0.23956685 0.25524819 0.26144844 0.266122 0.26945245 0.27133292 0.27231109 0.27183861 0.2693935 0.26495433 0.2576603 0.24899599 0.23952773 0.23132084 0.22561452][0.2051314 0.21705496 0.2204757 0.22362743 0.22623795 0.2281353 0.22929364 0.22952963 0.22821802 0.22560377 0.22145909 0.21674645 0.21151505 0.2068304 0.20347503][0.1624056 0.17076282 0.17216322 0.17368552 0.17513143 0.17627621 0.17708227 0.17749111 0.17714842 0.17621267 0.17441231 0.1722319 0.16968289 0.16730334 0.16530412]]...]
INFO - root - 2017-12-09 10:10:44.316861: step 14610, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:36m:38s remains)
INFO - root - 2017-12-09 10:10:52.919056: step 14620, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 76h:08m:12s remains)
INFO - root - 2017-12-09 10:11:01.596516: step 14630, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:28m:33s remains)
INFO - root - 2017-12-09 10:11:10.223855: step 14640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:51m:42s remains)
INFO - root - 2017-12-09 10:11:18.761021: step 14650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:59m:33s remains)
INFO - root - 2017-12-09 10:11:27.503900: step 14660, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:50m:10s remains)
INFO - root - 2017-12-09 10:11:36.314375: step 14670, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.903 sec/batch; 79h:42m:44s remains)
INFO - root - 2017-12-09 10:11:44.997606: step 14680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 76h:35m:21s remains)
INFO - root - 2017-12-09 10:11:53.435477: step 14690, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 76h:46m:23s remains)
INFO - root - 2017-12-09 10:12:02.039198: step 14700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:42m:13s remains)
2017-12-09 10:12:02.956942: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026650831 0.028448764 0.029188082 0.0292846 0.029044021 0.028673772 0.028193206 0.027912578 0.027572878 0.027194468 0.026793065 0.026463654 0.026112709 0.0258343 0.025558613][0.033752628 0.03597777 0.037033118 0.037451275 0.037385281 0.037131157 0.037004 0.03698609 0.036907025 0.036864068 0.036511142 0.036289204 0.036010891 0.035828475 0.0357467][0.035254143 0.037438624 0.038561307 0.039085522 0.039209034 0.03908848 0.039075427 0.039133206 0.039071314 0.039071888 0.039066993 0.039213169 0.039255634 0.03955508 0.039958041][0.034502018 0.036258217 0.037090577 0.037301566 0.037279595 0.036890846 0.036749247 0.036777414 0.03683529 0.037131883 0.037418623 0.037969276 0.038305521 0.038811397 0.039271884][0.032853894 0.034360055 0.034709331 0.03444086 0.033986293 0.033070017 0.032495197 0.032075461 0.031992856 0.032164924 0.032496456 0.033343513 0.034005612 0.034834277 0.0353878][0.029561218 0.030666914 0.030443825 0.02981836 0.029073939 0.028011397 0.027261535 0.026787242 0.02656978 0.026417803 0.026463339 0.026789874 0.027197771 0.027742334 0.028135853][0.022445578 0.0227399 0.021860544 0.020730415 0.019670827 0.01870368 0.018238453 0.018107863 0.018073406 0.018077327 0.018169034 0.018194964 0.018138373 0.017908594 0.017766031][0.0137415 0.013051975 0.011682563 0.010293134 0.0091452226 0.0082794474 0.0079384269 0.0079204449 0.00814147 0.008423117 0.0087862434 0.0090066548 0.009056258 0.0087792808 0.0083066495][0.0052254442 0.0041029332 0.0027410041 0.0016265132 0.00085926126 0.00042405422 0.00031240168 0.00036986615 0.00052365218 0.00067152455 0.00086854491 0.001035962 0.0011056454 0.0010562844 0.00087956665][0.00035558525 -0.00061079464 -0.0015288551 -0.0020984448 -0.002380216 -0.002492185 -0.0025048242 -0.0024642651 -0.0024096672 -0.0023645866 -0.0023153266 -0.0022860365 -0.0022892049 -0.0023378851 -0.0024180156][-0.0013738865 -0.0020535614 -0.0026214374 -0.0028368521 -0.002887914 -0.00289329 -0.0028955752 -0.0028982353 -0.0029010577 -0.00290028 -0.0028989892 -0.0028958388 -0.0028925035 -0.0028883715 -0.0028851833][-0.0018691742 -0.0023560582 -0.0027825786 -0.0028729283 -0.0028932323 -0.0028940109 -0.0028953589 -0.002897446 -0.002899495 -0.0028996253 -0.0028991504 -0.0028958409 -0.002892918 -0.002889096 -0.0028867158][-0.0019419505 -0.0023865134 -0.0027690926 -0.0028823048 -0.0028944414 -0.002895145 -0.0028961611 -0.0028972672 -0.0028983916 -0.002898518 -0.0028980596 -0.0028962386 -0.0028950893 -0.0028922935 -0.0028905207][-0.0017659185 -0.0023170251 -0.0027709017 -0.0028915911 -0.0028939354 -0.0028940646 -0.0028944064 -0.0028947091 -0.0028951883 -0.0028960616 -0.0028964039 -0.0028957708 -0.0028954404 -0.0028946521 -0.0028941534][-0.0015012595 -0.0021116738 -0.0026982857 -0.0028898111 -0.0028966838 -0.0028964023 -0.0028961499 -0.0028956959 -0.0028956572 -0.0028956395 -0.0028954498 -0.0028940153 -0.0028926069 -0.0028922553 -0.0028919573]]...]
INFO - root - 2017-12-09 10:12:11.629326: step 14710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:17m:50s remains)
INFO - root - 2017-12-09 10:12:20.344178: step 14720, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 74h:06m:55s remains)
INFO - root - 2017-12-09 10:12:29.084082: step 14730, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 71h:38m:29s remains)
INFO - root - 2017-12-09 10:12:37.601776: step 14740, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 73h:40m:46s remains)
INFO - root - 2017-12-09 10:12:46.117572: step 14750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:59m:52s remains)
INFO - root - 2017-12-09 10:12:54.772640: step 14760, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:49m:07s remains)
INFO - root - 2017-12-09 10:13:03.461139: step 14770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:22m:35s remains)
INFO - root - 2017-12-09 10:13:12.120640: step 14780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:23m:20s remains)
INFO - root - 2017-12-09 10:13:20.504168: step 14790, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 62h:32m:02s remains)
INFO - root - 2017-12-09 10:13:28.980551: step 14800, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 77h:01m:30s remains)
2017-12-09 10:13:29.870076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029149582 -0.0029188618 -0.00291969 -0.0029185633 -0.0029167561 -0.0029153172 -0.0029146548 -0.0029150196 -0.002915384 -0.0029159202 -0.0029165342 -0.0029176604 -0.0029187591 -0.002919161 -0.0029190576][-0.0029146622 -0.0029175663 -0.0029183652 -0.0029168308 -0.0029153768 -0.0029140247 -0.0029131849 -0.002913343 -0.0029137202 -0.0029143577 -0.0029149805 -0.0029155496 -0.0029160809 -0.0029161139 -0.0029157281][-0.0029154839 -0.0029173859 -0.002918347 -0.0029172718 -0.0029160329 -0.0029145763 -0.0029132047 -0.0029127556 -0.0029132836 -0.0029137512 -0.0029141798 -0.0029145656 -0.0029145645 -0.0029141195 -0.0029133761][-0.0029166432 -0.0029175875 -0.002918028 -0.0029172017 -0.0029154334 -0.0029131067 -0.0029109884 -0.0029102941 -0.0029110678 -0.0029119488 -0.0029127104 -0.0029130904 -0.0029132389 -0.0029128299 -0.0029121279][-0.0029158958 -0.0029163763 -0.00291553 -0.0029127349 -0.0029090294 -0.0029048435 -0.0029016049 -0.002901179 -0.0029030512 -0.0029061462 -0.0029090685 -0.0029111204 -0.0029122205 -0.0029122788 -0.0029119037][-0.0029126925 -0.0029113297 -0.0029083265 -0.0029028475 -0.0028962146 -0.0028895165 -0.0028847603 -0.0028845207 -0.0028883745 -0.0028945403 -0.0029011979 -0.0029063127 -0.002909624 -0.0029113039 -0.0029116161][-0.0029079418 -0.002904546 -0.0028990989 -0.0028914053 -0.0028831274 -0.0028745034 -0.0028677469 -0.002866931 -0.0028721951 -0.0028810813 -0.0028916243 -0.0029004933 -0.0029060759 -0.0029090876 -0.0029107467][-0.0029046603 -0.0029017364 -0.0028953436 -0.0028864474 -0.0028756782 -0.0028658991 -0.0028578201 -0.002855917 -0.0028612204 -0.0028716337 -0.0028850054 -0.0028966989 -0.0029038803 -0.0029075185 -0.0029094263][-0.0029069844 -0.0029053998 -0.0028997227 -0.0028906465 -0.0028783181 -0.0028658784 -0.0028571612 -0.0028549111 -0.0028598693 -0.0028705325 -0.0028842431 -0.0028967853 -0.0029048368 -0.0029087674 -0.002910411][-0.0029125684 -0.0029125896 -0.0029085553 -0.0029001576 -0.0028880483 -0.0028748938 -0.0028659757 -0.0028635687 -0.0028676824 -0.0028778266 -0.0028899761 -0.0029010705 -0.0029083316 -0.002912011 -0.0029132115][-0.0029184821 -0.0029186597 -0.0029165479 -0.0029105868 -0.0029009762 -0.0028909252 -0.0028833984 -0.0028808506 -0.0028833838 -0.002889914 -0.0028984321 -0.0029062189 -0.0029116543 -0.0029148762 -0.0029156422][-0.0029232011 -0.0029230583 -0.0029217787 -0.0029181291 -0.0029122122 -0.0029059055 -0.0029007099 -0.0028983096 -0.0028986626 -0.0029015734 -0.0029066547 -0.0029111574 -0.0029149354 -0.0029168283 -0.0029172674][-0.002925653 -0.0029249615 -0.0029246749 -0.002922507 -0.0029190304 -0.0029155549 -0.0029126802 -0.0029110264 -0.0029103346 -0.0029099006 -0.0029113113 -0.0029136571 -0.0029157458 -0.0029166497 -0.0029166548][-0.0029249566 -0.0029231613 -0.0029229631 -0.0029221524 -0.0029211252 -0.0029203636 -0.0029194125 -0.0029181207 -0.0029168748 -0.0029152641 -0.0029145754 -0.0029139379 -0.002914587 -0.002915022 -0.0029144839][-0.0029224914 -0.0029195345 -0.0029187731 -0.002918554 -0.0029185181 -0.0029190052 -0.0029195321 -0.0029193817 -0.002918391 -0.0029162832 -0.0029146778 -0.0029130715 -0.0029128124 -0.0029122119 -0.0029115425]]...]
INFO - root - 2017-12-09 10:13:38.457721: step 14810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:55m:01s remains)
INFO - root - 2017-12-09 10:13:47.140649: step 14820, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:04m:08s remains)
INFO - root - 2017-12-09 10:13:56.190644: step 14830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:10m:06s remains)
INFO - root - 2017-12-09 10:14:04.900381: step 14840, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:21m:10s remains)
INFO - root - 2017-12-09 10:14:13.487223: step 14850, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:04m:17s remains)
INFO - root - 2017-12-09 10:14:22.002254: step 14860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:14m:56s remains)
INFO - root - 2017-12-09 10:14:30.607797: step 14870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 73h:53m:55s remains)
INFO - root - 2017-12-09 10:14:39.201520: step 14880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:06m:28s remains)
INFO - root - 2017-12-09 10:14:47.817479: step 14890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:43m:37s remains)
INFO - root - 2017-12-09 10:14:56.183528: step 14900, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 74h:02m:44s remains)
2017-12-09 10:14:57.044284: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088548437 0.08474534 0.082123913 0.080573358 0.08077012 0.083222285 0.087475479 0.091029167 0.093345761 0.094754815 0.096312471 0.097825117 0.09919928 0.099308968 0.097938962][0.085191973 0.07912948 0.075046368 0.0725912 0.071883366 0.074263528 0.079419784 0.084384657 0.087957546 0.09100239 0.094239675 0.0962187 0.098201722 0.099037856 0.098796695][0.077439591 0.069293618 0.063727096 0.060462818 0.059524417 0.06220936 0.068003736 0.074043304 0.078789644 0.08301007 0.08732786 0.090197474 0.093158476 0.094762906 0.095576614][0.070182957 0.060899466 0.053804923 0.049437035 0.048443925 0.05168812 0.058334842 0.06542109 0.071091793 0.0756166 0.079908319 0.083028227 0.086442254 0.088474624 0.0904161][0.061364494 0.051952381 0.044406164 0.039311435 0.038074329 0.041684411 0.048831347 0.056692716 0.063632838 0.068903781 0.073430747 0.076324344 0.079671592 0.082043231 0.084464364][0.049341686 0.040592697 0.033429004 0.028628899 0.027617309 0.030908873 0.037874587 0.046371747 0.054310385 0.060476236 0.065750808 0.069238186 0.0730305 0.075479023 0.078011729][0.035994351 0.028520627 0.022463478 0.018181605 0.01740166 0.020132171 0.026126282 0.033777192 0.041778803 0.048610888 0.0546099 0.059283122 0.064350419 0.06786938 0.071155645][0.026421005 0.019848917 0.014556631 0.010767568 0.010002754 0.01149526 0.015666401 0.021366673 0.028211171 0.034096062 0.039876636 0.045073465 0.05103559 0.056170452 0.061241359][0.023403468 0.017373405 0.012329246 0.0081837717 0.0062327217 0.0061333086 0.0084294546 0.012112205 0.017312475 0.022309139 0.02749176 0.031806588 0.036999024 0.042468004 0.048735611][0.024796424 0.018827591 0.013243817 0.0086818337 0.0059168441 0.0041986015 0.004364986 0.0057961587 0.0087939082 0.012160621 0.016148882 0.019387029 0.023455532 0.02917094 0.036745377][0.029148018 0.022697525 0.015920857 0.010053466 0.0062720133 0.00374461 0.0026908654 0.002595959 0.0038222808 0.0057017682 0.0083947284 0.0098458147 0.01211729 0.017772794 0.026058497][0.03476575 0.027762227 0.019754704 0.012339673 0.0070357565 0.0035688754 0.0015616778 0.00084257848 0.0012970318 0.0025113781 0.0046957117 0.0053173257 0.0063755633 0.011407776 0.019949447][0.038909603 0.032254335 0.023968004 0.01596416 0.0096456055 0.004989882 0.0017541519 0.00026496267 3.1423522e-05 0.00074646715 0.0023661472 0.0025641185 0.003417515 0.0088514285 0.017865032][0.039761316 0.033445705 0.025352953 0.017685588 0.011416352 0.006695685 0.00291293 0.0010405194 0.00031738798 0.00038361363 0.0014376433 0.0017232362 0.0030608627 0.0092773261 0.019222241][0.038930006 0.0331359 0.025558861 0.018241333 0.012047006 0.0075954637 0.0037469256 0.0017264166 0.00045603327 0.00022782991 0.00062678521 0.0010639324 0.0029620046 0.0098643014 0.020983551]]...]
INFO - root - 2017-12-09 10:15:05.503741: step 14910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:29m:15s remains)
INFO - root - 2017-12-09 10:15:14.091430: step 14920, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:15m:36s remains)
INFO - root - 2017-12-09 10:15:22.827380: step 14930, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 75h:34m:46s remains)
INFO - root - 2017-12-09 10:15:31.426072: step 14940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:42m:21s remains)
INFO - root - 2017-12-09 10:15:40.076884: step 14950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 73h:50m:41s remains)
INFO - root - 2017-12-09 10:15:48.900750: step 14960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 77h:51m:36s remains)
INFO - root - 2017-12-09 10:15:57.613400: step 14970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:39m:42s remains)
INFO - root - 2017-12-09 10:16:06.302152: step 14980, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 79h:45m:43s remains)
INFO - root - 2017-12-09 10:16:14.714797: step 14990, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 76h:09m:13s remains)
INFO - root - 2017-12-09 10:16:23.055125: step 15000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:04m:56s remains)
2017-12-09 10:16:23.972314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029155223 -0.0028206049 -0.0024795886 -0.0016784243 -0.00041432818 0.0010914293 0.0024713331 0.003237535 0.0031494347 0.0021840043 0.00080006057 -0.00055431458 -0.001621304 -0.0022835727 -0.0026742008][-0.0028776778 -0.0026478672 -0.0019920447 -0.00059892121 0.0015342946 0.0040571708 0.0063595437 0.0076567559 0.0075691259 0.0060500475 0.0037668932 0.0014271436 -0.00046960544 -0.0017141355 -0.0024451786][-0.0026815396 -0.0020594457 -0.00062457332 0.0020305619 0.0058138939 0.010105082 0.013860554 0.015908701 0.01572535 0.013298569 0.0095824869 0.0055946903 0.0022001348 -0.00023309607 -0.0017662634][-0.0020080404 -0.00053436868 0.0023612885 0.0070633069 0.013203816 0.019751724 0.025136819 0.02787327 0.027384285 0.023779301 0.018273341 0.01214198 0.0066740457 0.0024582127 -0.00039832504][-0.00045613828 0.0023591255 0.0073130615 0.01458599 0.023351071 0.032101285 0.03879068 0.041827727 0.040749185 0.035860058 0.028541407 0.020187849 0.012437395 0.0061194235 0.0015903793][0.0018136592 0.0062319888 0.013403615 0.023155747 0.034136765 0.04443026 0.051721867 0.054581739 0.052750669 0.0467455 0.037961453 0.02778491 0.018034957 0.0097922655 0.0036778769][0.0043240059 0.010153308 0.019068487 0.030514089 0.042701498 0.053487446 0.060558517 0.0628643 0.060319904 0.053607427 0.043999817 0.03279594 0.021816684 0.012322851 0.0051502893][0.0061134472 0.012687335 0.022294717 0.034137517 0.046246838 0.056504395 0.062799871 0.064466275 0.061476227 0.054589577 0.04490184 0.033618312 0.022463653 0.012730911 0.005373694][0.0063636443 0.012746181 0.02177937 0.032595973 0.0433818 0.052288584 0.057534955 0.058700506 0.055708867 0.04927545 0.040300693 0.029929714 0.019724835 0.010866468 0.0042727571][0.0049584657 0.010273293 0.017657282 0.026356975 0.034957185 0.041999929 0.046082273 0.046890169 0.044286449 0.038883455 0.031393558 0.022847302 0.014548738 0.0074608605 0.0023286243][0.0025548788 0.006313351 0.011492857 0.017544305 0.023532797 0.028447574 0.031307522 0.031841945 0.029904073 0.025949791 0.020494875 0.01436271 0.0085208109 0.0036465575 0.0002452163][0.00013167039 0.0023364055 0.005379607 0.008929451 0.012467246 0.015387336 0.017105907 0.017417619 0.016216129 0.013776025 0.010422429 0.0067139585 0.0032587706 0.00046117813 -0.0013960587][-0.0016587526 -0.00064049917 0.00078494009 0.0024625317 0.0041531 0.0055541592 0.00638326 0.0065273624 0.0059309518 0.0047240956 0.0030759864 0.0012900739 -0.00033057458 -0.0015909411 -0.0023741266][-0.002582456 -0.0022430148 -0.0017547809 -0.0011683839 -0.00057011307 -7.2920928e-05 0.00022181473 0.00026896759 4.7576847e-05 -0.00039335852 -0.00098444126 -0.0016073281 -0.0021549743 -0.0025595545 -0.0027917209][-0.002888887 -0.002825591 -0.0027226855 -0.0025897992 -0.0024512147 -0.0023361784 -0.002269269 -0.0022606654 -0.0023137934 -0.0024143818 -0.0025448464 -0.0026774367 -0.0027905488 -0.0028692831 -0.0029103656]]...]
INFO - root - 2017-12-09 10:16:32.506009: step 15010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:33m:04s remains)
INFO - root - 2017-12-09 10:16:41.106879: step 15020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:09m:06s remains)
INFO - root - 2017-12-09 10:16:49.663891: step 15030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:15m:10s remains)
INFO - root - 2017-12-09 10:16:58.242302: step 15040, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:18m:23s remains)
INFO - root - 2017-12-09 10:17:06.771023: step 15050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:01m:49s remains)
INFO - root - 2017-12-09 10:17:15.441696: step 15060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 74h:27m:15s remains)
INFO - root - 2017-12-09 10:17:24.099534: step 15070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:10m:58s remains)
INFO - root - 2017-12-09 10:17:32.567101: step 15080, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 73h:15m:20s remains)
INFO - root - 2017-12-09 10:17:41.062526: step 15090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:22m:01s remains)
INFO - root - 2017-12-09 10:17:49.298906: step 15100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:14m:01s remains)
2017-12-09 10:17:50.121547: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47474256 0.47261268 0.46481356 0.45627955 0.44797927 0.44240963 0.43886453 0.43461642 0.43059048 0.42848244 0.42861843 0.43268746 0.43963563 0.45009837 0.46358442][0.48337558 0.48884389 0.48831153 0.48467416 0.48115623 0.47848129 0.47537157 0.46949214 0.46135846 0.45153433 0.44222715 0.43603858 0.43307588 0.43643954 0.44694579][0.46822232 0.48425385 0.49519879 0.50216329 0.50765681 0.51049674 0.51032746 0.5032891 0.4892621 0.46957767 0.4488937 0.42944902 0.41303453 0.40557629 0.40862224][0.4458448 0.47614235 0.501598 0.52174664 0.53963542 0.55119193 0.55497158 0.54527181 0.523699 0.49170932 0.45598212 0.42136511 0.39134717 0.37134653 0.36438295][0.41410831 0.4581055 0.49751538 0.53251785 0.56288069 0.58360523 0.59118545 0.57860839 0.54850537 0.50248367 0.45068365 0.40034974 0.35715011 0.32665467 0.31202906][0.36870342 0.42352161 0.47457224 0.52192312 0.56326336 0.59099394 0.600832 0.58513343 0.54682839 0.48886025 0.42396253 0.36082107 0.30698919 0.2694118 0.2510221][0.31019905 0.36980763 0.42733881 0.48323521 0.53235096 0.56672889 0.57853138 0.55971241 0.51513553 0.44917324 0.37600666 0.3044745 0.24582191 0.20545144 0.18606795][0.24469182 0.30164129 0.35891861 0.41779467 0.47025084 0.50859821 0.52145272 0.50211394 0.45500216 0.38561097 0.30967891 0.23695153 0.17976964 0.14110884 0.12370429][0.17894778 0.22724749 0.27834809 0.33311588 0.38262057 0.42121261 0.43435317 0.41622773 0.37076333 0.30516192 0.23451579 0.16735686 0.11705059 0.08449167 0.071934544][0.12387408 0.15860955 0.19787224 0.24230409 0.28382272 0.31708205 0.32793665 0.3130804 0.27410895 0.21845263 0.15964326 0.10530607 0.066665776 0.042359833 0.03456996][0.084980123 0.10505816 0.1300126 0.16037196 0.19038695 0.21525912 0.22315989 0.21174641 0.18141067 0.13962786 0.096250005 0.057467069 0.031750686 0.016673736 0.013150973][0.060893241 0.068589009 0.080093883 0.096440606 0.11457057 0.13023452 0.13543937 0.12789822 0.10742194 0.079782665 0.051268313 0.027074056 0.012031931 0.0042342423 0.0033679949][0.049106039 0.046894517 0.046727512 0.051320635 0.058980007 0.066581123 0.069301777 0.065028161 0.053657163 0.038392525 0.022779979 0.01004741 0.0025220886 -0.000700237 -0.00058716931][0.044526186 0.035080768 0.026742088 0.023126254 0.023204437 0.025043357 0.025888339 0.02399971 0.019215932 0.012843791 0.0064653675 0.0014809808 -0.0013277222 -0.0023659836 -0.0021755826][0.0448688 0.0309319 0.017477173 0.0090918057 0.0051533193 0.0042871404 0.0043429448 0.0037888312 0.0025178264 0.000807476 -0.00083599961 -0.0020167672 -0.0025872667 -0.0027826668 -0.002739206]]...]
INFO - root - 2017-12-09 10:17:58.808040: step 15110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:36m:03s remains)
INFO - root - 2017-12-09 10:18:07.505099: step 15120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:22m:47s remains)
INFO - root - 2017-12-09 10:18:16.178756: step 15130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:41m:38s remains)
INFO - root - 2017-12-09 10:18:24.796917: step 15140, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 73h:46m:52s remains)
INFO - root - 2017-12-09 10:18:33.525852: step 15150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:51m:59s remains)
INFO - root - 2017-12-09 10:18:42.215844: step 15160, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 76h:04m:58s remains)
INFO - root - 2017-12-09 10:18:50.826549: step 15170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:03m:45s remains)
INFO - root - 2017-12-09 10:18:59.388823: step 15180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:54m:13s remains)
INFO - root - 2017-12-09 10:19:07.998792: step 15190, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:17m:20s remains)
INFO - root - 2017-12-09 10:19:16.289591: step 15200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:16m:11s remains)
2017-12-09 10:19:17.168460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027839793 -0.0029381118 -0.0029440457 -0.0029452983 -0.002945317 -0.0029410566 -0.0029239675 -0.0028804117 -0.002797619 -0.0026799976 -0.0025648193 -0.002511617 -0.0025607576 -0.00269287 -0.0028314914][-0.0027161336 -0.0029310763 -0.0029391998 -0.002940723 -0.002942035 -0.0029404974 -0.0029304551 -0.0029085712 -0.0028748508 -0.0028383227 -0.0028160051 -0.0028250408 -0.0028405525 -0.0028879922 -0.0029172066][-0.0026805876 -0.0029214483 -0.0029343585 -0.0029361183 -0.0029376985 -0.0029371087 -0.0029257259 -0.0028985823 -0.0028501782 -0.0027949116 -0.0027641985 -0.00278461 -0.0028254243 -0.0029110403 -0.0029296174][-0.002545011 -0.0028943985 -0.0029245596 -0.0029251231 -0.0029264593 -0.0029217827 -0.0028713613 -0.0027605668 -0.0025890765 -0.002412745 -0.0023240463 -0.0024244718 -0.0025708887 -0.0027924925 -0.0028951352][-0.0022088322 -0.0028169374 -0.0029171461 -0.0029088922 -0.0028858369 -0.0028216268 -0.0026374953 -0.0022737745 -0.0016984255 -0.0010953044 -0.00073162047 -0.00090076285 -0.0014450933 -0.0022080874 -0.0027152977][-0.0016268006 -0.002658606 -0.0029070682 -0.00287933 -0.0027851195 -0.0025400242 -0.0019708462 -0.0009389983 0.00051283976 0.0019418334 0.002725323 0.0022578582 0.00085126539 -0.00089505105 -0.0022194225][-0.0012140535 -0.002492954 -0.0028784976 -0.0028020949 -0.002563972 -0.0019863439 -0.00078707282 0.0012114563 0.0037974685 0.0060922448 0.0070219226 0.0058908444 0.0032783251 0.00038667722 -0.0017759865][-0.00097570149 -0.0023296629 -0.0027884778 -0.0026297935 -0.002198664 -0.0012661039 0.000553248 0.0034266722 0.0070235115 0.010022824 0.01094828 0.0090462342 0.0051742513 0.0012179094 -0.0015609006][-0.00075580273 -0.0021942123 -0.0027362918 -0.0025703921 -0.0020263963 -0.0008774898 0.0012560061 0.0044962931 0.00837235 0.011434939 0.012117003 0.009777301 0.0054336097 0.0012008494 -0.0016321911][-0.00078707095 -0.0021505165 -0.0027406937 -0.0026522288 -0.0021730135 -0.0011113595 0.00085829291 0.0037939053 0.0071432805 0.0095854495 0.0098367715 0.00755208 0.0037403808 0.00022913166 -0.0019864589][-0.0011751723 -0.0022819513 -0.0027979994 -0.0027971703 -0.0025063311 -0.0017881974 -0.000403631 0.001661517 0.0039332826 0.0054454552 0.0053854892 0.0036728957 0.0011113312 -0.0011015242 -0.0024187225][-0.0017621097 -0.0025110682 -0.0028602665 -0.0028908676 -0.002762048 -0.0023980525 -0.0016562818 -0.0005355943 0.0006688647 0.0014029511 0.0012487115 0.00024879538 -0.0010842261 -0.0021448354 -0.0027292594][-0.0023554789 -0.0027411776 -0.0029095146 -0.0029324058 -0.0028937552 -0.0027656737 -0.002486561 -0.0020517393 -0.0015847648 -0.0013163239 -0.0014069101 -0.001815345 -0.0023172419 -0.0026882757 -0.0028788592][-0.002727814 -0.0028736484 -0.0029337704 -0.0029444741 -0.0029376717 -0.0029069721 -0.0028334933 -0.0027133741 -0.0025819954 -0.0025083097 -0.0025393281 -0.0026575734 -0.0027930492 -0.0028857838 -0.0029291541][-0.0028854629 -0.0029262493 -0.0029426008 -0.0029471281 -0.0029472518 -0.0029434015 -0.0029320368 -0.0029121519 -0.0028893331 -0.0028754051 -0.0028801626 -0.002899487 -0.0029206066 -0.002933813 -0.0029398082]]...]
INFO - root - 2017-12-09 10:19:25.859310: step 15210, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 77h:56m:10s remains)
INFO - root - 2017-12-09 10:19:34.567503: step 15220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 74h:33m:52s remains)
INFO - root - 2017-12-09 10:19:43.168083: step 15230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:03m:04s remains)
INFO - root - 2017-12-09 10:19:51.819234: step 15240, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 76h:55m:14s remains)
INFO - root - 2017-12-09 10:20:00.478545: step 15250, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:20m:19s remains)
INFO - root - 2017-12-09 10:20:09.128014: step 15260, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 76h:21m:06s remains)
INFO - root - 2017-12-09 10:20:17.758182: step 15270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:21m:51s remains)
INFO - root - 2017-12-09 10:20:26.349922: step 15280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:52m:23s remains)
INFO - root - 2017-12-09 10:20:35.034296: step 15290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:02m:10s remains)
INFO - root - 2017-12-09 10:20:43.170716: step 15300, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 75h:08m:38s remains)
2017-12-09 10:20:44.015975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029318964 -0.0029298174 -0.0029295241 -0.0029295117 -0.0029296558 -0.0029302738 -0.00293139 -0.002932596 -0.0029341301 -0.0029354389 -0.0029364175 -0.0029369723 -0.0029373984 -0.0029378843 -0.0029380089][-0.0029319243 -0.0029296065 -0.0029291522 -0.0029289396 -0.0029288069 -0.002929121 -0.0029300242 -0.0029310796 -0.0029324652 -0.0029338016 -0.0029349034 -0.0029355502 -0.0029361215 -0.0029366151 -0.0029367537][-0.0029331036 -0.0029306163 -0.0029299976 -0.0029295867 -0.0029292025 -0.0029292481 -0.0029299303 -0.002930816 -0.0029320258 -0.00293323 -0.002934362 -0.0029350938 -0.0029357537 -0.0029362473 -0.0029363919][-0.0029338216 -0.0029312156 -0.0029304679 -0.002929908 -0.00292933 -0.0029291802 -0.0029296086 -0.0029302994 -0.0029312742 -0.0029323117 -0.0029334477 -0.0029342906 -0.0029350186 -0.0029355336 -0.0029357022][-0.002934224 -0.0029314575 -0.0029306014 -0.0029298959 -0.0029291168 -0.0029287215 -0.0029288679 -0.0029292936 -0.0029300058 -0.0029309082 -0.002932061 -0.0029330554 -0.0029338587 -0.0029344328 -0.0029346636][-0.002934278 -0.0029314039 -0.0029304649 -0.0029296966 -0.0029288058 -0.002928176 -0.0029280118 -0.0029282023 -0.0029286749 -0.0029293336 -0.0029304253 -0.002931573 -0.0029324833 -0.0029331332 -0.0029334838][-0.002933671 -0.0029307902 -0.0029298447 -0.0029290754 -0.0029282286 -0.0029275904 -0.0029273448 -0.0029274691 -0.0029278493 -0.0029283415 -0.0029293243 -0.0029304612 -0.0029313704 -0.00293202 -0.0029324598][-0.0029325855 -0.0029296756 -0.0029287685 -0.0029280712 -0.0029274023 -0.0029269569 -0.0029268279 -0.0029270086 -0.0029273722 -0.0029278011 -0.0029286488 -0.0029296349 -0.0029304598 -0.0029310922 -0.0029315841][-0.0029309855 -0.0029281676 -0.002927379 -0.002926867 -0.0029264921 -0.0029264032 -0.0029265478 -0.0029268945 -0.0029273813 -0.0029278591 -0.0029286023 -0.0029294209 -0.0029301043 -0.0029306263 -0.0029310551][-0.002929694 -0.0029270244 -0.0029264372 -0.0029261927 -0.0029261839 -0.0029264588 -0.0029269098 -0.0029275159 -0.0029281471 -0.0029286456 -0.0029292423 -0.0029298349 -0.0029303127 -0.0029306682 -0.0029309543][-0.002929118 -0.0029266661 -0.0029263685 -0.0029263829 -0.0029266963 -0.0029272591 -0.0029278889 -0.002928613 -0.002929261 -0.0029297057 -0.0029301126 -0.0029304978 -0.0029308069 -0.0029310386 -0.0029312249][-0.0029294884 -0.0029272614 -0.0029271794 -0.0029273417 -0.0029277867 -0.0029284109 -0.0029290612 -0.0029297338 -0.0029303278 -0.0029307378 -0.0029310572 -0.0029313387 -0.002931539 -0.0029316624 -0.0029317588][-0.0029305164 -0.0029283403 -0.0029284162 -0.0029286349 -0.0029290523 -0.0029296065 -0.0029301909 -0.0029307504 -0.0029312607 -0.0029316372 -0.0029319122 -0.0029321124 -0.00293222 -0.0029322761 -0.0029323099][-0.0029319765 -0.0029297643 -0.0029298642 -0.0029300223 -0.0029303262 -0.0029307536 -0.002931213 -0.0029316652 -0.0029320577 -0.0029323585 -0.0029325406 -0.0029326479 -0.0029327059 -0.002932745 -0.0029327739][-0.0029335725 -0.0029312321 -0.0029312966 -0.0029313921 -0.0029315702 -0.0029318235 -0.0029321199 -0.0029324256 -0.002932667 -0.0029328479 -0.0029329581 -0.0029330216 -0.0029330684 -0.0029331062 -0.002933135]]...]
INFO - root - 2017-12-09 10:20:52.560437: step 15310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:20m:59s remains)
INFO - root - 2017-12-09 10:21:01.186738: step 15320, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:05m:05s remains)
INFO - root - 2017-12-09 10:21:09.784661: step 15330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:59m:29s remains)
INFO - root - 2017-12-09 10:21:18.458541: step 15340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 77h:28m:59s remains)
INFO - root - 2017-12-09 10:21:27.142337: step 15350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:36m:22s remains)
INFO - root - 2017-12-09 10:21:35.989187: step 15360, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 77h:35m:49s remains)
INFO - root - 2017-12-09 10:21:44.607370: step 15370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:42m:03s remains)
INFO - root - 2017-12-09 10:21:53.093230: step 15380, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:38m:23s remains)
INFO - root - 2017-12-09 10:22:01.587791: step 15390, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 75h:48m:18s remains)
INFO - root - 2017-12-09 10:22:09.796309: step 15400, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 76h:16m:56s remains)
2017-12-09 10:22:10.623932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029574041 -0.0029556546 -0.0029556537 -0.0029557536 -0.0029557222 -0.0029554961 -0.0029553757 -0.0029554218 -0.0029556733 -0.002955999 -0.0029565913 -0.0029573967 -0.0029582155 -0.0029587841 -0.0029588654][-0.002956247 -0.00295447 -0.0029544542 -0.0029547012 -0.0029547103 -0.002954538 -0.0029543526 -0.0029543675 -0.0029543946 -0.002954704 -0.0029555841 -0.002956857 -0.0029579832 -0.0029584109 -0.002958596][-0.0029564789 -0.0029549331 -0.0029550272 -0.0029556672 -0.0029560667 -0.0029561361 -0.0029560495 -0.0029559396 -0.0029556986 -0.0029556863 -0.0029564365 -0.0029576158 -0.0029586023 -0.0029590828 -0.0029592821][-0.0029566423 -0.0029553447 -0.0029557054 -0.0029565273 -0.0029570325 -0.0029571739 -0.0029572255 -0.0029570181 -0.0029567454 -0.0029568914 -0.0029576826 -0.0029589715 -0.00295993 -0.0029605846 -0.0029607331][-0.0029571115 -0.0029561855 -0.0029566369 -0.0029575494 -0.002958168 -0.0029583629 -0.0029583962 -0.0029583366 -0.0029582838 -0.0029587119 -0.0029592861 -0.0029604696 -0.00296166 -0.0029623762 -0.0029623697][-0.0029574577 -0.00295689 -0.0029572803 -0.0029583138 -0.0029590304 -0.0029591459 -0.0029589173 -0.0029586973 -0.0029590791 -0.0029600495 -0.0029608514 -0.0029618843 -0.0029629432 -0.0029636892 -0.0029636472][-0.002957782 -0.002957518 -0.0029580921 -0.0029593231 -0.0029604365 -0.0029604144 -0.0029594537 -0.0029590209 -0.0029600803 -0.002961199 -0.0029620137 -0.0029631311 -0.0029639737 -0.0029642293 -0.002964488][-0.002958047 -0.0029578451 -0.0029588605 -0.0029604137 -0.0029618924 -0.0029618656 -0.0029605546 -0.0029604987 -0.002961908 -0.0029629525 -0.0029637225 -0.0029644019 -0.0029649176 -0.0029647048 -0.0029652966][-0.0029579047 -0.0029579613 -0.0029593816 -0.0029609855 -0.0029626798 -0.0029629569 -0.0029624451 -0.0029628894 -0.0029640233 -0.0029646398 -0.002964918 -0.0029657059 -0.0029663674 -0.0029658191 -0.0029661658][-0.0029574798 -0.0029576025 -0.002959416 -0.0029612123 -0.0029629469 -0.00296398 -0.0029648491 -0.0029658696 -0.0029660971 -0.0029658345 -0.0029661958 -0.0029668543 -0.002967109 -0.0029665686 -0.0029666238][-0.0029570838 -0.0029573247 -0.0029593641 -0.0029612617 -0.0029631702 -0.0029650279 -0.0029667364 -0.0029679826 -0.0029679746 -0.0029673921 -0.0029674799 -0.0029673572 -0.002966674 -0.0029658719 -0.0029657474][-0.0029571184 -0.0029572228 -0.002959525 -0.002961739 -0.0029638715 -0.0029657502 -0.002967526 -0.0029686773 -0.0029685125 -0.0029678815 -0.0029674498 -0.0029664522 -0.0029650547 -0.0029639315 -0.0029635525][-0.0029575306 -0.0029575564 -0.0029596679 -0.0029619571 -0.0029642098 -0.0029660759 -0.0029673229 -0.0029682277 -0.0029683849 -0.002967882 -0.0029667506 -0.0029650193 -0.0029633096 -0.0029619602 -0.0029609394][-0.0029582842 -0.0029580458 -0.0029597334 -0.0029617376 -0.002963488 -0.0029653676 -0.0029662508 -0.0029667884 -0.0029671825 -0.002966871 -0.0029659923 -0.0029645287 -0.0029629618 -0.0029616556 -0.0029603483][-0.0029593576 -0.0029586514 -0.0029593436 -0.0029602689 -0.0029613653 -0.0029628142 -0.0029636859 -0.0029643951 -0.00296518 -0.0029655045 -0.0029651781 -0.0029644165 -0.002963406 -0.0029624063 -0.0029612831]]...]
INFO - root - 2017-12-09 10:22:19.511481: step 15410, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 75h:49m:46s remains)
INFO - root - 2017-12-09 10:22:28.316402: step 15420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:34m:29s remains)
INFO - root - 2017-12-09 10:22:36.976885: step 15430, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 78h:53m:36s remains)
INFO - root - 2017-12-09 10:22:45.667422: step 15440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:25m:10s remains)
INFO - root - 2017-12-09 10:22:54.392918: step 15450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:31m:14s remains)
INFO - root - 2017-12-09 10:23:03.014872: step 15460, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:10m:02s remains)
INFO - root - 2017-12-09 10:23:11.764751: step 15470, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:17m:29s remains)
INFO - root - 2017-12-09 10:23:20.596660: step 15480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:30m:39s remains)
INFO - root - 2017-12-09 10:23:29.196312: step 15490, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:15m:14s remains)
INFO - root - 2017-12-09 10:23:37.521613: step 15500, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 76h:32m:21s remains)
2017-12-09 10:23:38.388358: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082460545 0.083070166 0.08240512 0.0813344 0.080145612 0.079032257 0.077782772 0.076336056 0.074752055 0.073579364 0.072705984 0.072121277 0.071643509 0.070873 0.071169078][0.087769568 0.090005957 0.090863407 0.090931915 0.090782017 0.090075962 0.089097179 0.087803192 0.086479068 0.085335754 0.084319815 0.083640687 0.082976274 0.08208327 0.081623413][0.082832411 0.08623416 0.088442162 0.09002836 0.091372922 0.091986753 0.092268392 0.091959469 0.091536909 0.090863049 0.090224355 0.089421436 0.088488147 0.087443955 0.086881779][0.070665695 0.074855641 0.07838425 0.081926055 0.08550632 0.088391721 0.090862073 0.092412628 0.093317918 0.093515635 0.093079396 0.092310481 0.091273606 0.090137333 0.089090295][0.052163418 0.057067048 0.061975915 0.067081653 0.072641067 0.078212552 0.083752066 0.088285036 0.091411322 0.09298221 0.093242936 0.092410512 0.090993688 0.0894627 0.088290408][0.031898685 0.0362092 0.041064367 0.047078907 0.054140043 0.061656293 0.069611944 0.077255577 0.083508745 0.08728002 0.088703163 0.088377692 0.087037586 0.0851092 0.083702169][0.015524171 0.018420838 0.02197345 0.026916863 0.033295359 0.041209072 0.05032878 0.059404965 0.067531019 0.073626556 0.077303268 0.07840018 0.077754669 0.076255716 0.075375445][0.0053518428 0.0068627088 0.0088744815 0.011885967 0.01618091 0.022158459 0.029780768 0.038536228 0.047201611 0.054229707 0.059281867 0.062157955 0.063226335 0.062869571 0.062752046][0.00032715173 0.0010170187 0.0019608843 0.0034144185 0.0056797084 0.0091325659 0.013967646 0.020092167 0.026796555 0.033300433 0.038729336 0.042384043 0.044551261 0.04561137 0.046962332][-0.0018337066 -0.0014730765 -0.0010618779 -0.0005076977 0.00039935485 0.001915066 0.0042876564 0.007567123 0.011563053 0.015913576 0.019925881 0.023197327 0.025584228 0.027323445 0.02925425][-0.0027826431 -0.0026573103 -0.0024656642 -0.0021894537 -0.0018012702 -0.0012550589 -0.00037362473 0.00096698431 0.0027614818 0.0049054832 0.0070512951 0.00899821 0.010632563 0.012197955 0.014054928][-0.0029223207 -0.0029137016 -0.0029018209 -0.0028736172 -0.0028014772 -0.0026521464 -0.0023737089 -0.0019325966 -0.001344964 -0.00063591497 0.00016427692 0.00098508038 0.0017524641 0.0025802068 0.0036180657][-0.0029336382 -0.0029282554 -0.0029289555 -0.0029348556 -0.0029408555 -0.0029394908 -0.002922263 -0.002876888 -0.0027921472 -0.0026638608 -0.0024881922 -0.0022846076 -0.0020497851 -0.0017621579 -0.0013813734][-0.0029445251 -0.0029392419 -0.0029375982 -0.0029379898 -0.0029404094 -0.002942675 -0.0029431204 -0.0029395863 -0.0029341083 -0.0029274907 -0.0029203591 -0.0029195168 -0.002914258 -0.0028989604 -0.0028619845][-0.002951964 -0.0029483787 -0.0029463125 -0.0029451973 -0.00294524 -0.0029449526 -0.0029434776 -0.0029377551 -0.0029311641 -0.0029231904 -0.0029157617 -0.0029137998 -0.0029118517 -0.0029119896 -0.0029109998]]...]
INFO - root - 2017-12-09 10:23:47.067820: step 15510, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 78h:47m:22s remains)
INFO - root - 2017-12-09 10:23:55.726580: step 15520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:52m:34s remains)
INFO - root - 2017-12-09 10:24:04.429922: step 15530, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 75h:49m:28s remains)
INFO - root - 2017-12-09 10:24:13.312008: step 15540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:09m:32s remains)
INFO - root - 2017-12-09 10:24:22.021139: step 15550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:31m:47s remains)
INFO - root - 2017-12-09 10:24:30.553273: step 15560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 77h:42m:46s remains)
INFO - root - 2017-12-09 10:24:39.098713: step 15570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:42m:22s remains)
INFO - root - 2017-12-09 10:24:47.780698: step 15580, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 77h:04m:26s remains)
INFO - root - 2017-12-09 10:24:56.300455: step 15590, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.856 sec/batch; 75h:18m:46s remains)
INFO - root - 2017-12-09 10:25:04.278736: step 15600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:07m:24s remains)
2017-12-09 10:25:05.152232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029706811 -0.0029679269 -0.0029670927 -0.0029668726 -0.0029667115 -0.0029659555 -0.0029635183 -0.0029600258 -0.0029564896 -0.0029519624 -0.0029470832 -0.0029430052 -0.0029392305 -0.002935204 -0.0029317965][-0.0029685311 -0.0029665 -0.0029662964 -0.0029665185 -0.0029665488 -0.0029662212 -0.0029648533 -0.0029626791 -0.0029604181 -0.0029570374 -0.0029528469 -0.0029485256 -0.0029435763 -0.0029378163 -0.0029323131][-0.0029648303 -0.0029633562 -0.0029639879 -0.0029646917 -0.0029648223 -0.0029645625 -0.0029640624 -0.0029632777 -0.0029627031 -0.0029609445 -0.0029579694 -0.0029537373 -0.0029482569 -0.0029410578 -0.0029339362][-0.0029606251 -0.00295953 -0.0029604889 -0.0029611313 -0.0029610237 -0.0029604002 -0.0029599152 -0.0029601303 -0.002960975 -0.0029604824 -0.0029584523 -0.0029550057 -0.0029495307 -0.0029418806 -0.0029342542][-0.0029548153 -0.0029545436 -0.0029558362 -0.0029562113 -0.0029554549 -0.0029543864 -0.0029536744 -0.0029544954 -0.0029558062 -0.0029561615 -0.0029550768 -0.0029525284 -0.0029476569 -0.0029406387 -0.0029337013][-0.0029460865 -0.0029462376 -0.0029480271 -0.0029488725 -0.0029481917 -0.0029470278 -0.0029462983 -0.0029473838 -0.0029487798 -0.0029495959 -0.0029493137 -0.0029478995 -0.0029441754 -0.0029384294 -0.0029326989][-0.0029365134 -0.0029363241 -0.0029386927 -0.0029401535 -0.0029401679 -0.002939313 -0.002939014 -0.0029402475 -0.0029415172 -0.0029423812 -0.0029427488 -0.0029421863 -0.0029398496 -0.0029356966 -0.0029314081][-0.002928931 -0.0029277774 -0.0029298367 -0.0029316789 -0.0029325143 -0.0029325022 -0.0029327872 -0.0029340475 -0.0029349746 -0.002935529 -0.0029360198 -0.002936068 -0.0029349271 -0.0029325022 -0.0029296821][-0.0029254274 -0.0029233224 -0.0029250374 -0.0029267496 -0.0029276942 -0.002928199 -0.0029288363 -0.0029297059 -0.0029301236 -0.0029301855 -0.002930359 -0.0029306696 -0.0029304451 -0.002929379 -0.0029280125][-0.0029246155 -0.0029216814 -0.0029230185 -0.0029243531 -0.0029252374 -0.0029260276 -0.0029266968 -0.0029272123 -0.0029270216 -0.0029265839 -0.0029266467 -0.0029270556 -0.0029273317 -0.002927307 -0.0029270975][-0.002925206 -0.0029217666 -0.0029226863 -0.0029236742 -0.0029246181 -0.0029252903 -0.0029258521 -0.002926169 -0.0029256516 -0.0029248239 -0.002924629 -0.0029251182 -0.0029256211 -0.0029260928 -0.0029263857][-0.002926375 -0.0029229622 -0.0029234141 -0.0029240067 -0.0029246625 -0.0029250765 -0.0029255154 -0.0029256705 -0.0029250483 -0.0029242691 -0.0029239003 -0.0029244104 -0.0029251254 -0.0029256702 -0.0029257794][-0.0029279906 -0.0029244942 -0.0029247794 -0.0029249971 -0.0029253338 -0.0029255964 -0.0029258456 -0.0029258947 -0.0029253822 -0.0029248255 -0.0029244449 -0.0029248062 -0.0029253818 -0.0029256158 -0.0029253692][-0.002929152 -0.0029254782 -0.0029255576 -0.0029256707 -0.0029257531 -0.0029258037 -0.0029258775 -0.0029258572 -0.002925578 -0.0029253184 -0.0029250323 -0.00292518 -0.0029253983 -0.0029254332 -0.0029249825][-0.0029299723 -0.0029263217 -0.0029262663 -0.0029262691 -0.0029262388 -0.0029260737 -0.0029259506 -0.0029259319 -0.0029258612 -0.0029257191 -0.002925535 -0.0029256283 -0.0029257964 -0.0029256223 -0.0029250218]]...]
INFO - root - 2017-12-09 10:25:13.783975: step 15610, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.894 sec/batch; 78h:39m:15s remains)
INFO - root - 2017-12-09 10:25:22.494353: step 15620, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 74h:20m:40s remains)
INFO - root - 2017-12-09 10:25:31.252393: step 15630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:04m:51s remains)
INFO - root - 2017-12-09 10:25:40.015816: step 15640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:09m:29s remains)
INFO - root - 2017-12-09 10:25:48.703412: step 15650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:47m:04s remains)
INFO - root - 2017-12-09 10:25:57.103608: step 15660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:18m:42s remains)
INFO - root - 2017-12-09 10:26:05.714345: step 15670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:38m:27s remains)
INFO - root - 2017-12-09 10:26:14.461778: step 15680, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 78h:12m:32s remains)
INFO - root - 2017-12-09 10:26:23.066660: step 15690, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:05m:51s remains)
INFO - root - 2017-12-09 10:26:31.266885: step 15700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:31m:12s remains)
2017-12-09 10:26:32.224256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029696014 -0.0029672531 -0.0029675879 -0.0029680931 -0.0029686624 -0.0029691849 -0.0029695211 -0.0029694568 -0.0029691486 -0.0029689407 -0.0029687984 -0.0029687218 -0.0029684706 -0.0029681963 -0.0029681528][-0.0029677346 -0.0029653397 -0.002965943 -0.0029669637 -0.0029680808 -0.0029688161 -0.0029689011 -0.0029685351 -0.0029681029 -0.0029678445 -0.0029676429 -0.0029672177 -0.0029665388 -0.002966237 -0.0029661527][-0.0029680349 -0.0029660084 -0.0029669665 -0.0029684217 -0.00296972 -0.0029703118 -0.0029699209 -0.0029692028 -0.002968461 -0.0029683011 -0.0029682212 -0.0029679383 -0.0029670894 -0.0029662803 -0.0029660123][-0.0029683386 -0.002966766 -0.0029678782 -0.0029694231 -0.0029704485 -0.002970163 -0.0029684987 -0.0029672917 -0.0029673167 -0.0029683681 -0.0029691991 -0.00296926 -0.0029684864 -0.0029672831 -0.0029666][-0.0029692 -0.0029676664 -0.0029688163 -0.0029699802 -0.0029698943 -0.0029675618 -0.0029637082 -0.0029620454 -0.0029635571 -0.0029669402 -0.0029696715 -0.0029708275 -0.002970529 -0.0029692664 -0.0029680405][-0.0029705004 -0.002968973 -0.0029693395 -0.0029691358 -0.0029670785 -0.0029620603 -0.0029552602 -0.0029531 -0.0029575382 -0.002964519 -0.0029700126 -0.0029729623 -0.0029731563 -0.002971662 -0.0029695088][-0.0029717896 -0.0029700615 -0.0029693812 -0.0029674862 -0.0029627811 -0.0029544218 -0.00294614 -0.0029440941 -0.0029514043 -0.0029615019 -0.0029691318 -0.0029740436 -0.0029752092 -0.0029736608 -0.0029708825][-0.0029728466 -0.0029709409 -0.0029693788 -0.002966234 -0.0029592698 -0.0029501761 -0.0029430306 -0.0029428897 -0.0029504392 -0.0029609159 -0.0029698154 -0.0029747165 -0.0029761337 -0.0029744064 -0.0029710867][-0.0029736243 -0.0029713337 -0.0029693507 -0.0029659038 -0.002958958 -0.0029512695 -0.002946855 -0.0029485496 -0.0029556016 -0.0029647772 -0.002972343 -0.0029764336 -0.0029772397 -0.0029749414 -0.0029712718][-0.0029741977 -0.0029712135 -0.0029691509 -0.0029662729 -0.0029609983 -0.002955812 -0.0029543766 -0.0029572919 -0.00296336 -0.002970109 -0.002975245 -0.0029778832 -0.0029773011 -0.0029744632 -0.0029707828][-0.0029741314 -0.0029713328 -0.0029690356 -0.0029671344 -0.0029644463 -0.0029627816 -0.0029629979 -0.0029657965 -0.0029705467 -0.0029748308 -0.0029776637 -0.0029783929 -0.0029766092 -0.0029735 -0.0029699774][-0.0029741616 -0.0029723421 -0.002971539 -0.0029706073 -0.0029697602 -0.002969323 -0.0029699428 -0.0029719446 -0.0029749176 -0.002977039 -0.0029778695 -0.0029770681 -0.0029748252 -0.0029719113 -0.0029688622][-0.0029742285 -0.0029733472 -0.0029743265 -0.0029744697 -0.0029744618 -0.0029748506 -0.0029751027 -0.0029757726 -0.0029769121 -0.0029774683 -0.0029769707 -0.0029753109 -0.0029729356 -0.0029701928 -0.00296768][-0.0029738268 -0.0029731784 -0.0029752119 -0.0029768355 -0.002977649 -0.0029779891 -0.0029779507 -0.0029775952 -0.0029773014 -0.0029769149 -0.0029754483 -0.0029733717 -0.0029711872 -0.0029690063 -0.0029670068][-0.0029724177 -0.0029714226 -0.0029733188 -0.0029750213 -0.0029763367 -0.0029771496 -0.0029771263 -0.002976347 -0.0029755519 -0.0029747081 -0.002973255 -0.0029713835 -0.0029695726 -0.00296786 -0.0029664261]]...]
INFO - root - 2017-12-09 10:26:40.956326: step 15710, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:16m:22s remains)
INFO - root - 2017-12-09 10:26:49.635997: step 15720, loss = 0.90, batch loss = 0.70 (8.9 examples/sec; 0.902 sec/batch; 79h:20m:19s remains)
INFO - root - 2017-12-09 10:26:58.237595: step 15730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 73h:08m:30s remains)
INFO - root - 2017-12-09 10:27:06.886079: step 15740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:27m:11s remains)
INFO - root - 2017-12-09 10:27:15.614413: step 15750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:37m:11s remains)
INFO - root - 2017-12-09 10:27:24.234579: step 15760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:12m:04s remains)
INFO - root - 2017-12-09 10:27:32.922246: step 15770, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 73h:49m:24s remains)
INFO - root - 2017-12-09 10:27:41.738418: step 15780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:58m:12s remains)
INFO - root - 2017-12-09 10:27:50.473321: step 15790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:09m:19s remains)
INFO - root - 2017-12-09 10:27:58.699588: step 15800, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 77h:25m:16s remains)
2017-12-09 10:27:59.554145: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0067997379 0.008884524 0.012976468 0.019668026 0.027551794 0.036376249 0.043625664 0.047431149 0.04728223 0.045762815 0.043888431 0.04355976 0.044817023 0.046562463 0.047858082][0.0076609459 0.009887496 0.015023528 0.023587633 0.034694578 0.048051253 0.058574393 0.065323129 0.066981241 0.066147394 0.064497463 0.0631787 0.062011253 0.060838651 0.059566859][0.0089535518 0.011561766 0.018085016 0.02951544 0.043929379 0.060402166 0.073413767 0.082134649 0.084995188 0.085441679 0.0838406 0.081951059 0.078421377 0.074652232 0.071033359][0.010759397 0.014733296 0.023131032 0.037167866 0.054136656 0.072584733 0.086510345 0.095817186 0.099076591 0.099912941 0.098199263 0.096623145 0.091920167 0.087016307 0.082155265][0.013344254 0.019137751 0.02988285 0.046161883 0.0641299 0.082465909 0.095864251 0.10503571 0.10838047 0.10943109 0.10752965 0.10596075 0.10130963 0.096324645 0.091721982][0.015866119 0.024168499 0.036745124 0.054003153 0.071901619 0.089068912 0.10089152 0.10898451 0.1121284 0.11376683 0.11223364 0.11087894 0.10648655 0.10298182 0.099104479][0.018268934 0.028656354 0.042862356 0.060697865 0.078146085 0.093458369 0.10355868 0.1104366 0.1127794 0.1143877 0.11369631 0.11312913 0.10974113 0.10759272 0.10474301][0.020744577 0.03236144 0.046969276 0.063953362 0.079917341 0.093620047 0.10264008 0.10847417 0.11079621 0.11295516 0.11369707 0.11401498 0.1117689 0.11064468 0.10852296][0.02354015 0.035259336 0.048957705 0.063731782 0.077124842 0.088548467 0.09646675 0.10233613 0.10566492 0.10875886 0.11112952 0.11319877 0.11209007 0.11216652 0.11090812][0.025206218 0.036562096 0.048727918 0.06063797 0.071022183 0.079912961 0.086367883 0.091815248 0.0960672 0.10054448 0.10479553 0.10856572 0.10972486 0.11105453 0.11098456][0.02589491 0.03609689 0.046146955 0.055425324 0.063058831 0.069415815 0.074277714 0.079042874 0.083538078 0.088766381 0.094019637 0.099278972 0.1026333 0.10568272 0.10721321][0.025648514 0.034175724 0.04205364 0.049001738 0.054481097 0.059159026 0.062784992 0.066644527 0.070905909 0.076157324 0.08161851 0.087459527 0.092108965 0.096552253 0.099305116][0.02502368 0.031653479 0.037440587 0.042440955 0.046297383 0.049706649 0.052474409 0.055844881 0.059496533 0.064453624 0.069623277 0.07523521 0.080249235 0.085193291 0.088799223][0.02393017 0.02873352 0.032514229 0.035917852 0.038556259 0.041207913 0.043547168 0.046645664 0.050109129 0.054681886 0.059125844 0.064043969 0.068565242 0.073163465 0.076890074][0.022022646 0.025274938 0.02762647 0.029660171 0.0313235 0.033387672 0.035448652 0.038420137 0.041624095 0.045785218 0.049750708 0.053831473 0.057278614 0.060973842 0.06411121]]...]
INFO - root - 2017-12-09 10:28:08.459368: step 15810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:58m:13s remains)
INFO - root - 2017-12-09 10:28:17.131574: step 15820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:10m:42s remains)
INFO - root - 2017-12-09 10:28:25.808441: step 15830, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 76h:58m:18s remains)
INFO - root - 2017-12-09 10:28:34.457513: step 15840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:16m:25s remains)
INFO - root - 2017-12-09 10:28:43.026639: step 15850, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 72h:31m:01s remains)
INFO - root - 2017-12-09 10:28:51.688431: step 15860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 76h:06m:49s remains)
INFO - root - 2017-12-09 10:29:00.191213: step 15870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:32m:36s remains)
INFO - root - 2017-12-09 10:29:08.646715: step 15880, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:58m:38s remains)
INFO - root - 2017-12-09 10:29:17.278409: step 15890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:31m:52s remains)
INFO - root - 2017-12-09 10:29:25.524098: step 15900, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:08m:03s remains)
2017-12-09 10:29:26.360487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029876463 -0.0029858879 -0.002985432 -0.0029851934 -0.0029852155 -0.00298526 -0.0029852621 -0.0029852476 -0.0029852281 -0.0029851142 -0.0029850411 -0.0029849843 -0.0029849387 -0.002985 -0.0029852418][-0.002985582 -0.0029837952 -0.0029832255 -0.0029829636 -0.0029829277 -0.0029830087 -0.00298315 -0.0029831764 -0.0029830826 -0.0029828728 -0.0029827373 -0.0029827394 -0.0029829403 -0.0029833531 -0.0029837007][-0.0029860884 -0.0029843727 -0.0029839294 -0.0029836106 -0.002983558 -0.0029836139 -0.002983728 -0.0029836458 -0.0029833007 -0.00298281 -0.0029824651 -0.0029824839 -0.0029827813 -0.0029832944 -0.0029837594][-0.002986647 -0.0029853848 -0.0029849792 -0.0029832581 -0.002980645 -0.0029781961 -0.002977612 -0.0029795223 -0.0029818204 -0.0029829449 -0.0029831885 -0.0029831734 -0.0029832907 -0.0029836691 -0.0029840663][-0.0029876991 -0.0029868607 -0.0029849042 -0.0029731165 -0.0029494949 -0.0029238139 -0.002911269 -0.0029195244 -0.0029384769 -0.0029566546 -0.0029700713 -0.0029782823 -0.0029824935 -0.0029841687 -0.0029845731][-0.0029891469 -0.0029883089 -0.0029794588 -0.0029348664 -0.00284068 -0.0027313111 -0.0026658159 -0.0026774546 -0.0027404076 -0.0028184077 -0.0028892048 -0.0029406205 -0.00296967 -0.0029815943 -0.0029847887][-0.002989844 -0.0029886826 -0.0029656892 -0.0028590921 -0.0026286447 -0.0023449429 -0.00214931 -0.0021328826 -0.0022594628 -0.0024559118 -0.0026612151 -0.0028261228 -0.0029263031 -0.0029706226 -0.0029837175][-0.0029897373 -0.0029877876 -0.0029471854 -0.0027721177 -0.002386861 -0.0018856724 -0.0014954711 -0.0013915463 -0.0015539437 -0.0018883465 -0.0022836896 -0.0026274428 -0.002846777 -0.0029479766 -0.0029794967][-0.0029896642 -0.0029866451 -0.0029378089 -0.0027325186 -0.0022703335 -0.0016360255 -0.0010797852 -0.00084524578 -0.00096640689 -0.0013702086 -0.0019140517 -0.0024219207 -0.0027603542 -0.0029217277 -0.0029734464][-0.0029898672 -0.0029869457 -0.0029462865 -0.0027739424 -0.0023735643 -0.0017928664 -0.0012193699 -0.00089072669 -0.00090368954 -0.0012515903 -0.0017988937 -0.0023428861 -0.0027204114 -0.0029068824 -0.0029696047][-0.0029898926 -0.0029879902 -0.0029652382 -0.0028631764 -0.002611428 -0.002221588 -0.0017864734 -0.0014773243 -0.001407012 -0.001620976 -0.002027452 -0.0024544995 -0.0027598466 -0.0029152427 -0.0029701784][-0.0029889029 -0.0029884211 -0.0029807659 -0.0029403593 -0.0028288418 -0.0026403004 -0.0024006267 -0.0021993937 -0.0021198797 -0.0022129351 -0.0024322816 -0.0026746809 -0.00285108 -0.0029424436 -0.0029761966][-0.0029872924 -0.0029872518 -0.0029870181 -0.0029787011 -0.0029469843 -0.0028855684 -0.0027948751 -0.0027073536 -0.0026628794 -0.0026891651 -0.0027699338 -0.002863544 -0.0029328843 -0.0029696538 -0.0029838672][-0.002985982 -0.0029856481 -0.0029866491 -0.0029871843 -0.0029835536 -0.0029726431 -0.0029520835 -0.0029291592 -0.002914818 -0.0029181556 -0.0029357066 -0.002958012 -0.0029753605 -0.0029847547 -0.0029884728][-0.0029848954 -0.00298418 -0.0029848935 -0.0029859543 -0.0029867811 -0.0029870216 -0.002985704 -0.0029836695 -0.0029822015 -0.0029824183 -0.0029836961 -0.0029863971 -0.0029885573 -0.0029894712 -0.0029896295]]...]
INFO - root - 2017-12-09 10:29:35.230671: step 15910, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 84h:56m:29s remains)
INFO - root - 2017-12-09 10:29:44.189524: step 15920, loss = 0.90, batch loss = 0.69 (7.9 examples/sec; 1.018 sec/batch; 89h:32m:52s remains)
INFO - root - 2017-12-09 10:29:52.987060: step 15930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:01m:35s remains)
INFO - root - 2017-12-09 10:30:01.609796: step 15940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:10m:34s remains)
INFO - root - 2017-12-09 10:30:10.299311: step 15950, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:20m:17s remains)
INFO - root - 2017-12-09 10:30:19.070456: step 15960, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 73h:09m:21s remains)
INFO - root - 2017-12-09 10:30:27.842350: step 15970, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 78h:20m:39s remains)
INFO - root - 2017-12-09 10:30:36.389179: step 15980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:49m:06s remains)
INFO - root - 2017-12-09 10:30:45.150937: step 15990, loss = 0.89, batch loss = 0.68 (8.6 examples/sec; 0.927 sec/batch; 81h:32m:23s remains)
INFO - root - 2017-12-09 10:30:53.417856: step 16000, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 71h:26m:02s remains)
2017-12-09 10:30:54.382488: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0099358354 0.0099845016 0.0098054428 0.0098286355 0.009712493 0.0093953842 0.0089877807 0.0083500715 0.0074884351 0.0062033823 0.0047117146 0.003268755 0.0019405317 0.00070220954 -0.00044135074][0.01101278 0.01078578 0.010563089 0.010428824 0.01028508 0.010305056 0.010174679 0.0098947007 0.00936423 0.008486893 0.0072900914 0.0058172029 0.0043255906 0.0029424219 0.0015707896][0.010871996 0.01038505 0.0099764923 0.0095325122 0.0092284782 0.0091425683 0.0090627344 0.0090067387 0.0088021047 0.0084217833 0.0077520888 0.0067909481 0.005662539 0.00440945 0.0029999088][0.010408552 0.00986419 0.0092898775 0.0086167315 0.0080701821 0.0076994514 0.0074634859 0.0073278844 0.0071862312 0.0070364391 0.0067679351 0.0063167876 0.0055979416 0.0045825527 0.0033382494][0.0092097344 0.0084920833 0.0076757865 0.0069087949 0.0062134471 0.0057532014 0.0054415967 0.005235455 0.0051590558 0.0051868083 0.0052341148 0.0050865747 0.0046641105 0.0039278478 0.0029322724][0.0072993254 0.0065021487 0.0056359023 0.0047816485 0.0039683264 0.0034607206 0.0031112856 0.0029333606 0.0029092259 0.0030707158 0.0033359029 0.0035084006 0.0034030378 0.0029385639 0.0022178339][0.0052822474 0.0045483215 0.0037359945 0.0028869915 0.0020835409 0.0014804718 0.0010890071 0.000915593 0.00087755313 0.0010556858 0.0014122617 0.0017717807 0.0018734261 0.0016068891 0.0011291816][0.0033199554 0.0026039486 0.0018724687 0.0010989276 0.00039223 -0.00021230662 -0.00058229431 -0.00084897596 -0.00095337 -0.00082035386 -0.0005110011 -0.00020537712 -5.3400407e-05 -0.00013658148 -0.00037368969][0.0015326557 0.00086394651 0.00016400521 -0.00046681776 -0.000989506 -0.0013823333 -0.0016486492 -0.0018393315 -0.001911595 -0.001891847 -0.0017665616 -0.0016310337 -0.0015684897 -0.0016127934 -0.0017416065][0.00039270404 -0.00023597991 -0.0008823087 -0.0013843852 -0.0017464845 -0.0019480082 -0.002098494 -0.0021808716 -0.0022482565 -0.0022616819 -0.0022541985 -0.0022833191 -0.0023472374 -0.0024385809 -0.0025265808][-0.0005346057 -0.0010606139 -0.0015772363 -0.0019250585 -0.0021178285 -0.0021784387 -0.0022374962 -0.0022474853 -0.0022349292 -0.0021816809 -0.0022176076 -0.00223955 -0.0023212938 -0.0024252189 -0.0025366344][-0.0011526456 -0.0016079687 -0.002051278 -0.0022693677 -0.0023456991 -0.0023952962 -0.0024535633 -0.0024886257 -0.0025281154 -0.0025357667 -0.0025651513 -0.0024874022 -0.0025201929 -0.0025435134 -0.0025895832][-0.0010307659 -0.0013800187 -0.0017897447 -0.0020061485 -0.0020757359 -0.0021358184 -0.0022339283 -0.0023040469 -0.0023605034 -0.0024297265 -0.0025143684 -0.0025739593 -0.0026285297 -0.002654782 -0.00269564][-0.00055020512 -0.00082324725 -0.0011570817 -0.0013358763 -0.0014314583 -0.0015334381 -0.0016473132 -0.0016979378 -0.0017831643 -0.001862572 -0.0019642971 -0.0020976975 -0.002178113 -0.002320393 -0.0024327191][-0.00035456871 -0.00056577474 -0.000770048 -0.00079538161 -0.00086780963 -0.0010369651 -0.001145207 -0.0011645438 -0.0012459229 -0.0013940121 -0.0015483388 -0.0016969041 -0.0017811931 -0.0019526142 -0.0021078454]]...]
INFO - root - 2017-12-09 10:31:02.901970: step 16010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:56m:59s remains)
INFO - root - 2017-12-09 10:31:11.565903: step 16020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:02m:58s remains)
INFO - root - 2017-12-09 10:31:20.081219: step 16030, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 76h:22m:46s remains)
INFO - root - 2017-12-09 10:31:28.718278: step 16040, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:47m:36s remains)
INFO - root - 2017-12-09 10:31:37.584911: step 16050, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:47m:33s remains)
INFO - root - 2017-12-09 10:31:46.461478: step 16060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:10m:33s remains)
INFO - root - 2017-12-09 10:31:54.972962: step 16070, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 75h:19m:08s remains)
INFO - root - 2017-12-09 10:32:03.713280: step 16080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:49m:36s remains)
INFO - root - 2017-12-09 10:32:12.402026: step 16090, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 73h:51m:45s remains)
INFO - root - 2017-12-09 10:32:20.678522: step 16100, loss = 0.89, batch loss = 0.68 (11.1 examples/sec; 0.721 sec/batch; 63h:23m:13s remains)
2017-12-09 10:32:21.462958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029721267 -0.0029703933 -0.0029704585 -0.0029706471 -0.0029707535 -0.0029707761 -0.0029707816 -0.0029707968 -0.0029708012 -0.0029707071 -0.002970621 -0.0029705076 -0.0029703821 -0.0029702974 -0.0029701635][-0.00297078 -0.0029690065 -0.0029691036 -0.0029693425 -0.0029695267 -0.0029696156 -0.0029696706 -0.0029696929 -0.0029696906 -0.0029696657 -0.0029696242 -0.0029695428 -0.0029694643 -0.0029693972 -0.0029692783][-0.0029706876 -0.0029688764 -0.0029690403 -0.0029693274 -0.0029695546 -0.002969716 -0.0029698575 -0.0029699169 -0.0029699167 -0.0029698745 -0.0029697635 -0.0029695854 -0.002969448 -0.0029693798 -0.0029692794][-0.0029702471 -0.0029682352 -0.0029684103 -0.0029687413 -0.0029689856 -0.0029692156 -0.0029694813 -0.0029696627 -0.0029696927 -0.0029696436 -0.0029695213 -0.0029692331 -0.0029690294 -0.0029689923 -0.0029689441][-0.0029696194 -0.0029672587 -0.0029673406 -0.0029676445 -0.002967898 -0.002968271 -0.0029687108 -0.0029690198 -0.0029691239 -0.0029690918 -0.0029688585 -0.0029684349 -0.0029681975 -0.0029682107 -0.0029682233][-0.002969316 -0.0029666938 -0.0029666335 -0.0029667774 -0.0029668955 -0.0029671611 -0.0029675907 -0.0029679143 -0.0029680687 -0.0029681278 -0.0029678682 -0.0029674019 -0.002967146 -0.0029672051 -0.0029673441][-0.002969496 -0.0029667562 -0.002966485 -0.0029664245 -0.0029663923 -0.0029665697 -0.0029669055 -0.0029672484 -0.0029674843 -0.0029676203 -0.0029673281 -0.0029667064 -0.0029663753 -0.0029664224 -0.0029665693][-0.0029703293 -0.0029675423 -0.0029670768 -0.0029668256 -0.0029665942 -0.0029665786 -0.0029667527 -0.0029670831 -0.0029673434 -0.0029675539 -0.002967238 -0.002966482 -0.0029660386 -0.0029660463 -0.0029661425][-0.0029715563 -0.002969139 -0.0029687108 -0.0029682515 -0.0029678592 -0.0029675458 -0.0029671916 -0.0029672387 -0.0029674433 -0.0029675176 -0.0029671465 -0.0029664263 -0.0029659194 -0.0029658927 -0.0029659311][-0.00297287 -0.0029709367 -0.0029708208 -0.0029704885 -0.002970061 -0.0029695907 -0.0029690608 -0.0029685458 -0.0029681916 -0.0029680883 -0.0029675693 -0.0029669162 -0.0029664619 -0.0029663588 -0.0029663253][-0.0029740243 -0.0029726094 -0.0029729635 -0.0029728631 -0.0029725439 -0.0029720622 -0.0029715549 -0.0029708473 -0.0029700773 -0.0029695872 -0.0029688731 -0.0029680652 -0.0029674519 -0.0029671739 -0.0029669763][-0.0029739242 -0.002972838 -0.0029735609 -0.0029737887 -0.0029737828 -0.0029736077 -0.0029732722 -0.0029727449 -0.0029719833 -0.0029713372 -0.002970475 -0.0029693996 -0.0029684438 -0.0029678585 -0.0029674871][-0.0029729947 -0.002971861 -0.0029727016 -0.0029731831 -0.0029734615 -0.0029735065 -0.0029734217 -0.0029731228 -0.0029725374 -0.0029719418 -0.0029710731 -0.002969997 -0.002968936 -0.0029682098 -0.0029677476][-0.0029717765 -0.0029703178 -0.0029710624 -0.0029715959 -0.0029719784 -0.0029721272 -0.0029721879 -0.0029719986 -0.0029715612 -0.002971112 -0.0029705076 -0.0029697218 -0.002968933 -0.002968299 -0.0029678291][-0.0029711071 -0.0029692235 -0.0029697774 -0.002970156 -0.0029704892 -0.0029706629 -0.0029707418 -0.0029706652 -0.0029704394 -0.0029702045 -0.0029698208 -0.002969329 -0.0029687996 -0.0029683115 -0.0029679036]]...]
INFO - root - 2017-12-09 10:32:29.963339: step 16110, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 74h:31m:49s remains)
INFO - root - 2017-12-09 10:32:38.390768: step 16120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:54m:47s remains)
INFO - root - 2017-12-09 10:32:46.894237: step 16130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:28m:56s remains)
INFO - root - 2017-12-09 10:32:55.334490: step 16140, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:48m:28s remains)
INFO - root - 2017-12-09 10:33:03.894127: step 16150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:56m:05s remains)
INFO - root - 2017-12-09 10:33:12.488700: step 16160, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:32m:33s remains)
INFO - root - 2017-12-09 10:33:20.938389: step 16170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:22m:32s remains)
INFO - root - 2017-12-09 10:33:29.457665: step 16180, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 77h:43m:30s remains)
INFO - root - 2017-12-09 10:33:38.113450: step 16190, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:53m:22s remains)
INFO - root - 2017-12-09 10:33:46.634567: step 16200, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 77h:04m:04s remains)
2017-12-09 10:33:47.360795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030110586 -0.0030097323 -0.0030097405 -0.0030099554 -0.0030101442 -0.0030104124 -0.0030106411 -0.0030107466 -0.0030106716 -0.0030102532 -0.0030098574 -0.003009578 -0.0030093659 -0.0030092516 -0.0030091826][-0.0030090965 -0.003007639 -0.0030077458 -0.0030081258 -0.0030085738 -0.0030090667 -0.0030093556 -0.0030091153 -0.0030085016 -0.0030079223 -0.003007707 -0.0030077288 -0.0030077191 -0.0030076921 -0.0030077109][-0.0030094665 -0.0030081626 -0.0030083735 -0.0030089663 -0.0030096537 -0.0030100164 -0.003009347 -0.0030074329 -0.0030048161 -0.0030030853 -0.0030033498 -0.0030049451 -0.0030066362 -0.0030074774 -0.0030078364][-0.0030096665 -0.0030085666 -0.0030090569 -0.0030099938 -0.0030108411 -0.0030104124 -0.0030073477 -0.0030012911 -0.0029937245 -0.0029892358 -0.0029906488 -0.0029962461 -0.0030021172 -0.0030056485 -0.0030070818][-0.0030100064 -0.0030090644 -0.0030098604 -0.003011236 -0.0030119698 -0.0030099989 -0.0030029302 -0.0029901681 -0.0029760147 -0.0029684461 -0.0029722638 -0.0029841112 -0.0029959043 -0.0030033365 -0.0030064525][-0.0030103764 -0.0030095866 -0.0030106658 -0.003012347 -0.0030127675 -0.0030089396 -0.0029979541 -0.0029796311 -0.0029608428 -0.0029521799 -0.002958237 -0.0029750215 -0.0029915618 -0.0030020219 -0.0030066858][-0.0030108627 -0.003010184 -0.0030114357 -0.0030132707 -0.003013663 -0.0030091349 -0.0029972403 -0.0029787119 -0.0029618321 -0.0029554933 -0.0029616533 -0.0029772564 -0.0029933276 -0.0030036676 -0.0030084064][-0.0030114127 -0.0030107943 -0.0030121631 -0.0030141356 -0.003014873 -0.0030113645 -0.0030019658 -0.002988437 -0.0029771805 -0.0029736282 -0.002978218 -0.0029885978 -0.0029998112 -0.0030073877 -0.0030108155][-0.0030116744 -0.0030111484 -0.003012561 -0.0030146244 -0.0030158318 -0.0030139564 -0.0030086015 -0.0030014033 -0.0029954952 -0.0029937767 -0.0029965201 -0.0030019085 -0.0030073039 -0.0030110576 -0.0030126111][-0.0030116348 -0.0030111514 -0.0030124721 -0.0030144795 -0.0030159578 -0.0030156903 -0.0030138153 -0.0030112185 -0.0030091526 -0.0030086266 -0.0030098285 -0.0030116311 -0.0030129196 -0.0030136651 -0.00301337][-0.0030112718 -0.0030107405 -0.0030119589 -0.0030136008 -0.0030151196 -0.0030159373 -0.003016151 -0.003016101 -0.0030161461 -0.0030161333 -0.0030161757 -0.0030160011 -0.0030154467 -0.0030145375 -0.0030131661][-0.003010822 -0.0030102353 -0.0030112509 -0.00301242 -0.0030137182 -0.0030149189 -0.0030160544 -0.003017073 -0.0030177538 -0.0030178598 -0.0030174297 -0.0030164458 -0.0030151478 -0.0030136567 -0.0030121324][-0.0030103533 -0.0030095128 -0.00301025 -0.003011178 -0.0030121512 -0.0030131722 -0.003014222 -0.0030151864 -0.003015802 -0.0030158572 -0.003015307 -0.003014226 -0.0030130257 -0.0030118374 -0.0030107684][-0.0030100385 -0.0030088231 -0.0030092224 -0.0030098388 -0.0030104776 -0.0030111247 -0.0030117719 -0.00301235 -0.0030126949 -0.0030126611 -0.0030122818 -0.0030115857 -0.0030108523 -0.0030101924 -0.0030095866][-0.0030098134 -0.0030084199 -0.0030086511 -0.0030089738 -0.0030093354 -0.0030096616 -0.0030098998 -0.0030101151 -0.0030102374 -0.0030101922 -0.0030099978 -0.0030096676 -0.0030093377 -0.0030090415 -0.0030087596]]...]
INFO - root - 2017-12-09 10:33:55.757573: step 16210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:30m:04s remains)
INFO - root - 2017-12-09 10:34:04.223843: step 16220, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 72h:45m:38s remains)
INFO - root - 2017-12-09 10:34:12.630343: step 16230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:52m:35s remains)
INFO - root - 2017-12-09 10:34:21.247306: step 16240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:08m:51s remains)
INFO - root - 2017-12-09 10:34:29.712511: step 16250, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 77h:06m:08s remains)
INFO - root - 2017-12-09 10:34:38.289050: step 16260, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 75h:34m:26s remains)
INFO - root - 2017-12-09 10:34:47.010535: step 16270, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 74h:48m:58s remains)
INFO - root - 2017-12-09 10:34:55.540766: step 16280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:13m:51s remains)
INFO - root - 2017-12-09 10:35:04.238331: step 16290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:58m:37s remains)
INFO - root - 2017-12-09 10:35:12.847440: step 16300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:45m:39s remains)
2017-12-09 10:35:13.678921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029888311 -0.002987416 -0.0029878123 -0.002987945 -0.002987494 -0.0029868316 -0.0029859587 -0.0029855957 -0.0029853322 -0.0029853261 -0.0029854516 -0.0029854614 -0.002985491 -0.0029852157 -0.0029852127][-0.0029876742 -0.0029867154 -0.0029874516 -0.0029880209 -0.0029883343 -0.0029878193 -0.0029860213 -0.0029845438 -0.0029835168 -0.0029833131 -0.0029834311 -0.0029833247 -0.0029832576 -0.0029830309 -0.0029831261][-0.0029881836 -0.0029879671 -0.0029894649 -0.0029907587 -0.0029912919 -0.00298985 -0.0029855282 -0.0029814767 -0.0029798222 -0.0029807824 -0.0029821442 -0.002982513 -0.0029826877 -0.0029827794 -0.0029831086][-0.0029887652 -0.0029895026 -0.0029917879 -0.0029933213 -0.0029919199 -0.0029846316 -0.0029710534 -0.0029596062 -0.0029595513 -0.0029688049 -0.00297816 -0.0029818609 -0.0029827494 -0.0029830805 -0.0029834311][-0.0029893769 -0.0029911052 -0.0029935981 -0.0029925876 -0.0029810355 -0.0029513778 -0.0029095358 -0.002881967 -0.0028935422 -0.0029322468 -0.0029666489 -0.0029800534 -0.0029833689 -0.0029841324 -0.0029842977][-0.0029900118 -0.0029925278 -0.0029943637 -0.0029860127 -0.0029506814 -0.0028762487 -0.0027855861 -0.0027390362 -0.0027821662 -0.0028754708 -0.0029496741 -0.0029765661 -0.002983307 -0.0029853273 -0.002985361][-0.0029905925 -0.0029937888 -0.0029939406 -0.0029734205 -0.0029030775 -0.0027712754 -0.0026270607 -0.0025711998 -0.0026614976 -0.0028184119 -0.0029333844 -0.0029732711 -0.0029833331 -0.002986344 -0.0029863836][-0.00299166 -0.0029951404 -0.0029931215 -0.0029606903 -0.0028608744 -0.0026880158 -0.002514977 -0.0024652153 -0.0025937608 -0.0027898047 -0.0029247175 -0.0029712268 -0.0029836886 -0.0029870416 -0.0029868691][-0.0029928389 -0.0029966198 -0.0029934673 -0.002956426 -0.0028494443 -0.0026745498 -0.0025110382 -0.0024784119 -0.0026133447 -0.0028022481 -0.0029278565 -0.0029715297 -0.0029842083 -0.0029874304 -0.002987064][-0.0029945148 -0.0029985169 -0.002996091 -0.0029651406 -0.0028777013 -0.0027395517 -0.0026170802 -0.0026021234 -0.0027097571 -0.0028502881 -0.0029416839 -0.0029746622 -0.0029852574 -0.0029879061 -0.0029875224][-0.0029960787 -0.0030000559 -0.0030000964 -0.0029815049 -0.0029276917 -0.0028438042 -0.002772457 -0.0027666348 -0.0028297298 -0.0029086368 -0.0029591725 -0.0029789417 -0.0029862085 -0.00298816 -0.0029876085][-0.0029968361 -0.0030005719 -0.0030023993 -0.0029950521 -0.0029711234 -0.0029332552 -0.0029007902 -0.0028969855 -0.0029217172 -0.0029532916 -0.0029738392 -0.0029833876 -0.0029872449 -0.0029881832 -0.0029875038][-0.0029963839 -0.0029990328 -0.0030013989 -0.0030000946 -0.0029936389 -0.0029818537 -0.0029707816 -0.002966614 -0.0029702513 -0.0029774394 -0.0029832043 -0.0029868316 -0.0029879976 -0.0029879215 -0.002986961][-0.0029948936 -0.0029963292 -0.0029983972 -0.0029992263 -0.00299904 -0.0029966212 -0.0029930361 -0.0029895816 -0.0029874127 -0.0029869457 -0.0029874789 -0.0029881229 -0.0029880465 -0.0029875294 -0.0029864169][-0.0029930873 -0.0029934878 -0.002995007 -0.0029958922 -0.0029966433 -0.0029963215 -0.0029951578 -0.0029932109 -0.0029912042 -0.0029896887 -0.0029886647 -0.0029880637 -0.002987514 -0.0029868786 -0.0029860102]]...]
INFO - root - 2017-12-09 10:35:21.741490: step 16310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 74h:37m:20s remains)
INFO - root - 2017-12-09 10:35:30.394701: step 16320, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 77h:26m:09s remains)
INFO - root - 2017-12-09 10:35:39.036607: step 16330, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 78h:03m:07s remains)
INFO - root - 2017-12-09 10:35:47.722700: step 16340, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:43m:01s remains)
INFO - root - 2017-12-09 10:35:56.468537: step 16350, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.921 sec/batch; 80h:52m:12s remains)
INFO - root - 2017-12-09 10:36:05.099789: step 16360, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 71h:14m:13s remains)
INFO - root - 2017-12-09 10:36:13.720798: step 16370, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.826 sec/batch; 72h:34m:31s remains)
INFO - root - 2017-12-09 10:36:22.547618: step 16380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:45m:57s remains)
INFO - root - 2017-12-09 10:36:31.227230: step 16390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:21m:29s remains)
INFO - root - 2017-12-09 10:36:39.691743: step 16400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:20m:52s remains)
2017-12-09 10:36:40.638611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029939509 -0.0029893206 -0.0029861878 -0.0029831755 -0.0029804977 -0.0029787417 -0.0029780206 -0.002978367 -0.0029801494 -0.0029821943 -0.00298386 -0.0029848574 -0.0029858176 -0.0029863741 -0.00298665][-0.0029976773 -0.0029922372 -0.0029883564 -0.0029846497 -0.0029811859 -0.0029788206 -0.0029775817 -0.0029774967 -0.0029789591 -0.002980907 -0.0029823973 -0.0029833838 -0.0029844202 -0.0029851208 -0.0029853813][-0.0030018238 -0.0029961788 -0.002992071 -0.0029881839 -0.0029844423 -0.0029817198 -0.0029799617 -0.002979269 -0.0029800443 -0.0029814416 -0.0029825349 -0.0029833945 -0.0029843734 -0.0029850644 -0.002985327][-0.0030029472 -0.0029974442 -0.0029934295 -0.00298972 -0.0029863694 -0.0029837869 -0.0029817708 -0.0029805612 -0.0029805377 -0.0029813088 -0.0029821219 -0.0029831026 -0.0029841897 -0.0029849093 -0.0029852239][-0.003000871 -0.0029956407 -0.0029920412 -0.0029890309 -0.0029867047 -0.0029847585 -0.0029828446 -0.0029811866 -0.00298037 -0.0029807033 -0.0029815356 -0.0029826812 -0.0029839072 -0.0029846733 -0.0029849939][-0.0029970082 -0.0029920116 -0.0029889871 -0.0029868905 -0.0029856281 -0.002984318 -0.0029825564 -0.0029806918 -0.0029797188 -0.0029801761 -0.0029812434 -0.0029826092 -0.0029838243 -0.002984429 -0.0029846744][-0.0029929576 -0.0029881333 -0.0029856467 -0.002984219 -0.0029835403 -0.0029825673 -0.0029807608 -0.0029789221 -0.0029786497 -0.0029797999 -0.0029812253 -0.0029826167 -0.002983703 -0.0029841855 -0.0029844486][-0.0029898956 -0.0029852404 -0.0029829398 -0.0029817191 -0.0029809875 -0.0029801351 -0.0029785389 -0.0029772043 -0.0029778439 -0.0029796595 -0.0029813175 -0.0029827859 -0.0029838204 -0.002984202 -0.0029844434][-0.0029883592 -0.0029838469 -0.002981534 -0.0029802469 -0.0029793086 -0.0029786155 -0.0029775973 -0.0029770324 -0.0029780045 -0.0029801305 -0.0029820548 -0.0029834576 -0.0029843138 -0.0029846875 -0.0029848914][-0.0029885822 -0.002984093 -0.0029817629 -0.0029802865 -0.0029791037 -0.0029783943 -0.0029780155 -0.0029781342 -0.0029791356 -0.0029811168 -0.0029831282 -0.0029843377 -0.0029850181 -0.002985405 -0.0029855538][-0.0029894363 -0.0029849228 -0.0029827156 -0.0029809305 -0.002979619 -0.0029789358 -0.0029788779 -0.0029792138 -0.0029801321 -0.0029818488 -0.0029837503 -0.002984857 -0.0029853981 -0.0029858791 -0.0029860721][-0.0029897955 -0.002985415 -0.0029833685 -0.0029816541 -0.0029803484 -0.0029797677 -0.002979802 -0.0029799698 -0.0029807284 -0.0029821338 -0.0029837606 -0.0029847876 -0.0029853119 -0.0029859913 -0.002986311][-0.0029892961 -0.0029852502 -0.0029835948 -0.0029821398 -0.002981069 -0.0029806546 -0.0029807633 -0.0029807682 -0.0029813072 -0.002982331 -0.0029836455 -0.0029845412 -0.0029851631 -0.0029859694 -0.0029863077][-0.0029876335 -0.0029838963 -0.0029827717 -0.0029818353 -0.0029811841 -0.0029809491 -0.0029810453 -0.002981036 -0.0029814416 -0.002982093 -0.002983276 -0.0029841335 -0.0029849161 -0.0029857685 -0.0029861096][-0.0029859194 -0.00298235 -0.0029817082 -0.0029811712 -0.0029808793 -0.0029807189 -0.0029807889 -0.0029808122 -0.0029811489 -0.0029816658 -0.002982738 -0.0029836774 -0.0029845852 -0.0029854712 -0.0029858227]]...]
INFO - root - 2017-12-09 10:36:48.832624: step 16410, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:14m:11s remains)
INFO - root - 2017-12-09 10:36:57.436494: step 16420, loss = 0.88, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 76h:50m:54s remains)
INFO - root - 2017-12-09 10:37:06.087284: step 16430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:02m:03s remains)
INFO - root - 2017-12-09 10:37:14.779903: step 16440, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 76h:26m:36s remains)
INFO - root - 2017-12-09 10:37:23.236024: step 16450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:49m:57s remains)
INFO - root - 2017-12-09 10:37:31.584259: step 16460, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.825 sec/batch; 72h:23m:30s remains)
INFO - root - 2017-12-09 10:37:40.334226: step 16470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:23m:52s remains)
INFO - root - 2017-12-09 10:37:48.997207: step 16480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 77h:06m:34s remains)
INFO - root - 2017-12-09 10:37:57.643591: step 16490, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 74h:27m:05s remains)
INFO - root - 2017-12-09 10:38:06.117857: step 16500, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:20m:58s remains)
2017-12-09 10:38:06.984882: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22122738 0.22670224 0.23180859 0.23381713 0.23183192 0.22509857 0.21457377 0.20014508 0.18309237 0.1671951 0.15422347 0.14547248 0.14259246 0.14574078 0.15276276][0.2236609 0.23477459 0.24592885 0.25342837 0.25562423 0.2511912 0.24079016 0.22512008 0.20556022 0.18633987 0.17022587 0.15905471 0.15533064 0.15935537 0.16889805][0.22264752 0.23914218 0.25560337 0.2682071 0.27493066 0.27364853 0.2652775 0.24898784 0.2278609 0.20563339 0.18592265 0.17147224 0.1653529 0.16878136 0.17941241][0.21960333 0.2404975 0.26097268 0.27773151 0.28865314 0.29067042 0.28426993 0.26902661 0.24766101 0.22338989 0.2008801 0.18329851 0.1745618 0.17549355 0.18533126][0.21489562 0.23909591 0.26260132 0.28223726 0.29641241 0.30187175 0.29894426 0.28575474 0.26519251 0.24023315 0.21487628 0.19393697 0.18148372 0.17981884 0.18775062][0.20863244 0.23498431 0.26034158 0.28213671 0.29835862 0.30650103 0.30670014 0.29690018 0.27926096 0.25536326 0.22986338 0.20669614 0.19084963 0.18499036 0.18899694][0.20319863 0.23064651 0.25652647 0.27911618 0.29623753 0.30625981 0.30892429 0.30264106 0.28880888 0.2681483 0.24441904 0.22123693 0.20367388 0.19401106 0.19331792][0.19831331 0.22648586 0.25251982 0.27490026 0.2916294 0.30208665 0.306028 0.30241385 0.2924456 0.27629685 0.25618327 0.2347911 0.21677472 0.20423265 0.19861844][0.19170159 0.22013988 0.24598545 0.26784936 0.28418398 0.29424208 0.29853171 0.2967681 0.28985628 0.27779204 0.2616688 0.24343856 0.22670832 0.21287563 0.203586][0.18410638 0.21153158 0.23613288 0.25733253 0.27303112 0.28264704 0.28737926 0.28701675 0.28242376 0.27352524 0.26093617 0.24566397 0.23057695 0.21691674 0.20569305][0.17431293 0.20116279 0.22454618 0.24448296 0.25909266 0.268127 0.272671 0.27285373 0.26980197 0.26358995 0.25432983 0.24234173 0.22975758 0.21714337 0.20531572][0.16317612 0.18884809 0.21099034 0.22984536 0.2434866 0.25165758 0.25566956 0.25603133 0.25364727 0.24906676 0.24217491 0.23305652 0.2230247 0.21248728 0.20158789][0.15040439 0.17347477 0.19348507 0.21096244 0.2239956 0.23182407 0.23564826 0.23607464 0.23411845 0.23053876 0.22522885 0.21852197 0.2106227 0.20215887 0.19278976][0.13639133 0.15615319 0.17337078 0.18878619 0.20074731 0.20850794 0.21277909 0.21380478 0.21257363 0.20966795 0.20535268 0.19999687 0.1937121 0.1871344 0.17967831][0.12360316 0.13959159 0.15329994 0.16606948 0.17624921 0.18317968 0.18727148 0.18867674 0.18810643 0.18602769 0.18279733 0.17865774 0.17387898 0.16896138 0.16335599]]...]
INFO - root - 2017-12-09 10:38:15.159120: step 16510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 73h:03m:28s remains)
INFO - root - 2017-12-09 10:38:23.708826: step 16520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:32m:47s remains)
INFO - root - 2017-12-09 10:38:32.460785: step 16530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:20m:08s remains)
INFO - root - 2017-12-09 10:38:41.171567: step 16540, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 74h:19m:01s remains)
INFO - root - 2017-12-09 10:38:49.870316: step 16550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:42m:01s remains)
INFO - root - 2017-12-09 10:38:58.533844: step 16560, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 72h:33m:07s remains)
INFO - root - 2017-12-09 10:39:07.260572: step 16570, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 77h:09m:07s remains)
INFO - root - 2017-12-09 10:39:16.095842: step 16580, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 76h:49m:46s remains)
INFO - root - 2017-12-09 10:39:24.839032: step 16590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:08m:07s remains)
INFO - root - 2017-12-09 10:39:33.352278: step 16600, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 77h:23m:36s remains)
2017-12-09 10:39:34.228195: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4493213 0.45794243 0.45951355 0.45576519 0.45168352 0.448221 0.44660434 0.4467549 0.4479382 0.44948369 0.44929904 0.45045337 0.45279357 0.45605779 0.46106476][0.42536849 0.43809375 0.44419062 0.44414532 0.44416571 0.44516775 0.4483971 0.45212629 0.45616215 0.46100435 0.46258998 0.46337423 0.46306279 0.46170154 0.46137223][0.38630769 0.4016937 0.41133741 0.4165037 0.42367816 0.43123883 0.4407962 0.4499267 0.45683283 0.46178538 0.4626632 0.46160528 0.45747557 0.45083618 0.44451863][0.34854808 0.36789143 0.38154894 0.390278 0.40246031 0.41573152 0.43115571 0.44329771 0.45089892 0.45441467 0.45223483 0.44705909 0.43703526 0.4239516 0.41121772][0.31949529 0.34244987 0.3613885 0.37581408 0.392103 0.40807742 0.42516568 0.43781805 0.44389477 0.44304553 0.43489379 0.42252895 0.40439609 0.3839339 0.36422455][0.30049935 0.32950878 0.35401049 0.3739785 0.39378679 0.41031834 0.42578405 0.43380195 0.43378642 0.42457229 0.40774888 0.38623473 0.35964486 0.33165711 0.30557966][0.29375085 0.32946178 0.35985214 0.38452947 0.40599313 0.42115441 0.43222079 0.43324652 0.42375031 0.40306917 0.37436682 0.34139988 0.30568829 0.27120748 0.24027534][0.30040625 0.34054467 0.37272057 0.39958143 0.42001697 0.43187359 0.4361127 0.42809647 0.40828088 0.3763974 0.33691454 0.29307678 0.24865559 0.20862266 0.17449337][0.31720421 0.36059535 0.3934181 0.41920605 0.4364776 0.44284552 0.43856475 0.42023286 0.38945484 0.34737843 0.29877648 0.24746507 0.1978931 0.15382019 0.11816889][0.33450109 0.38009351 0.41262615 0.43693939 0.45038351 0.45115569 0.43861932 0.41161028 0.37185249 0.32187864 0.26667091 0.20982237 0.15730849 0.11247564 0.078306265][0.349189 0.39462182 0.4249846 0.44618958 0.45535812 0.45114154 0.43204394 0.39882374 0.35313979 0.29897389 0.24107593 0.18308471 0.13136801 0.089012861 0.057830069][0.35856423 0.40165257 0.42831245 0.444734 0.44919708 0.44066715 0.41776106 0.38201812 0.3347888 0.28070328 0.22382507 0.1685258 0.12100478 0.082971893 0.055931982][0.359552 0.39852628 0.42017785 0.43151441 0.43095872 0.41867056 0.39413634 0.35921374 0.31454718 0.26458743 0.21334369 0.16467185 0.12340349 0.090796016 0.06776052][0.35025594 0.38408834 0.40055451 0.40749645 0.40365711 0.38951284 0.36497045 0.33270526 0.29284045 0.24989194 0.20716307 0.16762462 0.13441189 0.1079787 0.088817328][0.33259323 0.3606365 0.37196288 0.3748062 0.36853394 0.35458171 0.33248198 0.3040916 0.27037832 0.2357167 0.20250426 0.17278086 0.14799894 0.12834512 0.11343995]]...]
INFO - root - 2017-12-09 10:39:42.839577: step 16610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:42m:09s remains)
INFO - root - 2017-12-09 10:39:51.594201: step 16620, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 75h:46m:17s remains)
INFO - root - 2017-12-09 10:40:00.401372: step 16630, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:46m:54s remains)
INFO - root - 2017-12-09 10:40:09.162100: step 16640, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 78h:07m:37s remains)
INFO - root - 2017-12-09 10:40:17.862236: step 16650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:29m:05s remains)
INFO - root - 2017-12-09 10:40:26.624793: step 16660, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 78h:18m:26s remains)
INFO - root - 2017-12-09 10:40:35.331919: step 16670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:22m:00s remains)
INFO - root - 2017-12-09 10:40:44.017432: step 16680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:01m:47s remains)
INFO - root - 2017-12-09 10:40:52.802120: step 16690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:46m:13s remains)
INFO - root - 2017-12-09 10:41:01.388052: step 16700, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:09m:31s remains)
2017-12-09 10:41:02.187161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030262426 -0.0030242689 -0.0030242729 -0.0030242912 -0.0030241448 -0.0030221466 -0.0030119834 -0.0029846611 -0.0029346934 -0.0028706859 -0.0028201023 -0.002814614 -0.0028608434 -0.0029323786 -0.002992474][-0.0029926563 -0.0030111778 -0.0030172546 -0.0030189317 -0.0030192926 -0.0030150621 -0.00299428 -0.0029469898 -0.0028738708 -0.0027966895 -0.0027520782 -0.0027692586 -0.002841281 -0.0029298237 -0.0029952151][-0.0029009646 -0.0029264023 -0.0029485153 -0.002952304 -0.0029326333 -0.0028956174 -0.0028467195 -0.0027891551 -0.0027289733 -0.0026926186 -0.0027086493 -0.0027827695 -0.0028830427 -0.0029674284 -0.003012896][-0.0028127073 -0.002803447 -0.0027782691 -0.0027052 -0.0025576518 -0.002340077 -0.002104552 -0.001935759 -0.0018957994 -0.002002476 -0.0022252209 -0.0025039245 -0.0027582757 -0.0029284232 -0.003005981][-0.0027135045 -0.0026295504 -0.0024481239 -0.0021124347 -0.0016075682 -0.0010060738 -0.00045481534 -0.00011358433 -8.2071405e-05 -0.0003763244 -0.000924814 -0.0015844655 -0.0021902854 -0.0026277825 -0.0028751746][-0.0025366438 -0.0023802011 -0.0020230245 -0.0013611418 -0.00039316108 0.00072420994 0.0017123404 0.0023022923 0.0023581781 0.0018986771 0.0010390654 -4.0736515e-05 -0.0011202814 -0.0019990965 -0.0025785179][-0.0024552627 -0.0022182846 -0.0016798285 -0.00073089823 0.00058623636 0.0020464244 0.0033217743 0.0041142022 0.0042718719 0.0038107061 0.0028422214 0.0015331651 9.3407696e-05 -0.0012152537 -0.0021821836][-0.0023883148 -0.002087316 -0.0014656312 -0.00041293097 0.0010045874 0.0025444408 0.0038755881 0.0047202157 0.0049451035 0.0045685545 0.0036759595 0.0023751538 0.0008254915 -0.00069386978 -0.0018974236][-0.0024423297 -0.0021019531 -0.0014864061 -0.0005095033 0.0007791589 0.002171095 0.0033646161 0.0041136667 0.004311474 0.0039931596 0.0032267561 0.002081322 0.00067004608 -0.00075862673 -0.0019187038][-0.0026408285 -0.0023304278 -0.0018025995 -0.0010144748 -1.3833633e-05 0.0010546343 0.0019782349 0.0025678896 0.0027270662 0.0024612681 0.0018201023 0.0008790968 -0.00024744985 -0.0013533055 -0.0022260898][-0.0028638851 -0.0026717908 -0.0023238794 -0.0017908047 -0.0011127824 -0.00039812643 0.00020855153 0.00058340328 0.0006617019 0.00044313283 -3.1447271e-05 -0.00068802829 -0.0014260096 -0.0021054819 -0.0026084792][-0.002988887 -0.0029116685 -0.0027545874 -0.0024928232 -0.0021377516 -0.0017482124 -0.0014111411 -0.0012086116 -0.0011834258 -0.0013322448 -0.0016188301 -0.0019819876 -0.0023542221 -0.0026679046 -0.0028802191][-0.003029228 -0.003010052 -0.0029640358 -0.0028773979 -0.0027486549 -0.0025971022 -0.0024604665 -0.0023792984 -0.0023758595 -0.0024474445 -0.0025713693 -0.0027135392 -0.0028439967 -0.0029425481 -0.00300225][-0.0030361025 -0.0030348655 -0.0030286137 -0.0030117116 -0.0029812739 -0.002941295 -0.002903041 -0.0028804147 -0.0028808657 -0.0029029225 -0.0029381025 -0.0029746369 -0.00300428 -0.0030234687 -0.0030333358][-0.0030344995 -0.0030361023 -0.0030378355 -0.0030375463 -0.0030337928 -0.0030267858 -0.0030192393 -0.0030148972 -0.0030149855 -0.0030192065 -0.0030255695 -0.0030310822 -0.0030342422 -0.0030349866 -0.0030348576]]...]
INFO - root - 2017-12-09 10:41:10.748835: step 16710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:16m:56s remains)
INFO - root - 2017-12-09 10:41:19.460061: step 16720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:15m:50s remains)
INFO - root - 2017-12-09 10:41:28.251999: step 16730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:43m:51s remains)
INFO - root - 2017-12-09 10:41:36.822582: step 16740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:51m:23s remains)
INFO - root - 2017-12-09 10:41:45.413633: step 16750, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:15m:01s remains)
INFO - root - 2017-12-09 10:41:54.108613: step 16760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 74h:11m:33s remains)
INFO - root - 2017-12-09 10:42:02.832877: step 16770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:59m:38s remains)
INFO - root - 2017-12-09 10:42:11.500655: step 16780, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 77h:25m:43s remains)
INFO - root - 2017-12-09 10:42:20.257630: step 16790, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 74h:36m:30s remains)
INFO - root - 2017-12-09 10:42:28.751905: step 16800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:12m:43s remains)
2017-12-09 10:42:29.634376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030516263 -0.0030502505 -0.0030484814 -0.0030394697 -0.0030117074 -0.0029520288 -0.0028581833 -0.0027571514 -0.0027013314 -0.0027308939 -0.0028445423 -0.0029688894 -0.0030362008 -0.0030504665 -0.0030504409][-0.0030500805 -0.0030477529 -0.0030404795 -0.0030123009 -0.0029426934 -0.0028136109 -0.0026349544 -0.0024734363 -0.0024264567 -0.0025248104 -0.0027250871 -0.0029264013 -0.0030290887 -0.0030487783 -0.0030486272][-0.0030488486 -0.0030389163 -0.0030062075 -0.002915394 -0.0027283018 -0.0024330695 -0.0020959992 -0.0018645928 -0.0018904713 -0.0021545605 -0.0025261145 -0.002847271 -0.0030095498 -0.0030467673 -0.0030485874][-0.0030417708 -0.0029997053 -0.0028774748 -0.0025964405 -0.0020911016 -0.0013961869 -0.00074558216 -0.00044444087 -0.00068409462 -0.0013110272 -0.0020484393 -0.0026280852 -0.0029375423 -0.0030365172 -0.0030488819][-0.003012347 -0.0028847125 -0.0025374417 -0.0018029731 -0.00058915722 0.00092395116 0.0021487661 0.0025100582 0.0018264884 0.00050689396 -0.00095066661 -0.0020887104 -0.0027501816 -0.0030060732 -0.0030493147][-0.0029152131 -0.0026263155 -0.0018748409 -0.00035708235 0.0020196172 0.0047774315 0.0067877378 0.0071340557 0.0057342825 0.0033771894 0.00084412051 -0.0011815785 -0.0024263992 -0.0029474394 -0.0030472754][-0.0026891767 -0.0021810667 -0.00090748211 0.0015806458 0.0053294366 0.0094377268 0.012193305 0.012363245 0.01008446 0.0065642139 0.0028626109 -0.00015593972 -0.0020582757 -0.0028766147 -0.0030436206][-0.0023394804 -0.0016135741 0.00012005633 0.0034004948 0.0081841676 0.01315557 0.016222451 0.016011337 0.012963267 0.0085931793 0.0041318513 0.000480616 -0.0018364359 -0.0028321347 -0.003040622][-0.0020137024 -0.0011584776 0.00075847632 0.0042670136 0.0092335371 0.014141228 0.016896961 0.016237976 0.012858843 0.0083463872 0.0039106188 0.0003412799 -0.001901189 -0.0028441974 -0.0030391319][-0.0019392783 -0.0011103838 0.00062124082 0.00366937 0.0078618443 0.011812344 0.013794368 0.012853233 0.0097678313 0.005923993 0.0023096381 -0.00050095445 -0.0022162152 -0.0029061667 -0.0030409915][-0.00218877 -0.0015430208 -0.00028568506 0.0018366119 0.0046771662 0.0072423769 0.0083734412 0.007484016 0.0052096732 0.0025509731 0.00017204555 -0.0015819622 -0.002598718 -0.002976831 -0.003043127][-0.002600478 -0.0022151656 -0.001502819 -0.00034350692 0.001163953 0.0024761949 0.0029756962 0.0023815734 0.0010760012 -0.00036704983 -0.0015892345 -0.0024275477 -0.0028777409 -0.0030252817 -0.0030463529][-0.0029118226 -0.0027522079 -0.0024608313 -0.0019939034 -0.0014027313 -0.0009012972 -0.00073463563 -0.0010043255 -0.0015480081 -0.0021218259 -0.0025790967 -0.0028663261 -0.003007052 -0.0030449866 -0.0030483289][-0.0030406381 -0.003002424 -0.0029257424 -0.002801209 -0.0026460104 -0.0025137695 -0.0024724249 -0.0025485442 -0.0026953807 -0.002844753 -0.0029546174 -0.0030163594 -0.0030438181 -0.0030488789 -0.0030484896][-0.0030553793 -0.0030517892 -0.0030451783 -0.0030311185 -0.0030110145 -0.0029921033 -0.0029847373 -0.0029923313 -0.00300865 -0.0030266515 -0.00303928 -0.0030458798 -0.0030486868 -0.0030488549 -0.0030483897]]...]
INFO - root - 2017-12-09 10:42:38.091224: step 16810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 77h:48m:51s remains)
INFO - root - 2017-12-09 10:42:46.744873: step 16820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 75h:02m:13s remains)
INFO - root - 2017-12-09 10:42:55.448132: step 16830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:16m:45s remains)
INFO - root - 2017-12-09 10:43:04.149603: step 16840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:34m:03s remains)
INFO - root - 2017-12-09 10:43:12.928505: step 16850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:47m:38s remains)
INFO - root - 2017-12-09 10:43:21.826577: step 16860, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:07m:54s remains)
INFO - root - 2017-12-09 10:43:30.636178: step 16870, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 75h:08m:23s remains)
INFO - root - 2017-12-09 10:43:39.393988: step 16880, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 78h:27m:34s remains)
INFO - root - 2017-12-09 10:43:48.260765: step 16890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:48m:38s remains)
INFO - root - 2017-12-09 10:43:56.970219: step 16900, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 78h:23m:24s remains)
2017-12-09 10:43:57.826385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0008242412 -0.00087658875 -0.00088124652 -0.00086905272 -0.00075181806 -0.00065103988 -0.00055510295 -0.0004647153 -0.00041142083 -0.00028789579 -0.00013727066 7.1740476e-05 0.00017693103 8.698157e-05 -0.0002409874][-0.0015936957 -0.0015943979 -0.0015347046 -0.0014689383 -0.0012798057 -0.0010920903 -0.00089857727 -0.00073180394 -0.00059633865 -0.00045573129 -0.00032089441 -0.00013807416 -4.466041e-05 -0.0001413764 -0.00048219971][-0.0020595347 -0.0019924243 -0.0018590608 -0.0016961736 -0.0014110783 -0.0011274612 -0.000851467 -0.00063126604 -0.00044001709 -0.00029204134 -0.0002205756 -0.00012513576 -9.850855e-05 -0.00022706343 -0.00059244339][-0.0022011218 -0.0020754489 -0.0018751085 -0.0015778436 -0.0011734219 -0.0007762129 -0.00043682219 -0.00018527918 9.8864548e-06 0.00015228637 0.00015748991 0.00014876062 9.6594449e-05 -6.8346271e-05 -0.00044859573][-0.0021764473 -0.0019865474 -0.0016967137 -0.0012465557 -0.00072630728 -0.00022706832 0.0001543269 0.00044063968 0.00061411224 0.0007484958 0.00072415429 0.00064870529 0.00052542123 0.00032668211 -4.7446694e-05][-0.0021571303 -0.001874127 -0.0014597017 -0.00087463553 -0.00026547071 0.00030230894 0.0007052 0.0010338274 0.0011975302 0.0013411024 0.0013198957 0.0012379743 0.0010692605 0.00084008719 0.00047582644][-0.0022222791 -0.0018442572 -0.0013092414 -0.0006433099 1.7534476e-06 0.00060620275 0.0010351816 0.00141273 0.001597821 0.0017737162 0.0017801023 0.0017389304 0.0015456963 0.0012957018 0.00092434115][-0.0023189841 -0.0019104907 -0.0013391903 -0.00067207613 -3.9649662e-05 0.00057583395 0.0010319657 0.0014467093 0.0016757364 0.0018950047 0.0019559904 0.0019784209 0.0017882672 0.0015396823 0.0011520612][-0.0023960238 -0.0020318502 -0.0015172709 -0.00092111412 -0.00034621288 0.00025554164 0.00072851218 0.0011434003 0.0014105306 0.0016677242 0.0017954565 0.0018801989 0.0017225069 0.0014882802 0.0011091477][-0.002486936 -0.0022005129 -0.0017819938 -0.0012912386 -0.00079352874 -0.00023334287 0.00023741135 0.0006301098 0.00090836897 0.0011776548 0.0013539989 0.0014795109 0.0013706547 0.0011662396 0.0008214761][-0.002627037 -0.0024244748 -0.0021172466 -0.0017397994 -0.0013071998 -0.00079893041 -0.00032931147 5.0596427e-05 0.00032086787 0.00056276261 0.00073482771 0.00084922579 0.00077798148 0.00061806315 0.00033957511][-0.002784845 -0.0026566333 -0.0024765488 -0.002227091 -0.0018719228 -0.0014079218 -0.00092319795 -0.0005218212 -0.00024621119 -4.5483e-05 7.0676208e-05 0.00012127031 4.9779657e-05 -6.7746965e-05 -0.00026731566][-0.0028922155 -0.0028138978 -0.002747454 -0.0026268498 -0.0023912264 -0.0019934056 -0.0015133184 -0.0010718005 -0.00076281442 -0.00059168716 -0.0005329831 -0.00056709768 -0.00066962512 -0.000765254 -0.00089195161][-0.0029399316 -0.0028838422 -0.0028707173 -0.0028401085 -0.0027397741 -0.0024478904 -0.0020305284 -0.001581323 -0.0012410892 -0.0010708922 -0.0010366535 -0.0011191785 -0.0012400999 -0.0013288717 -0.0014054074][-0.0029731474 -0.0029257913 -0.002902315 -0.0028970926 -0.0028881913 -0.0027122879 -0.0023926608 -0.0019799285 -0.0016451868 -0.0014670013 -0.0014260539 -0.0015035549 -0.001607025 -0.0016595917 -0.0016882848]]...]
INFO - root - 2017-12-09 10:44:06.256478: step 16910, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 75h:07m:18s remains)
INFO - root - 2017-12-09 10:44:14.757714: step 16920, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:53m:55s remains)
INFO - root - 2017-12-09 10:44:23.389592: step 16930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:35m:02s remains)
INFO - root - 2017-12-09 10:44:32.185871: step 16940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:06m:44s remains)
INFO - root - 2017-12-09 10:44:40.930785: step 16950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:49m:25s remains)
INFO - root - 2017-12-09 10:44:49.641663: step 16960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:35m:27s remains)
INFO - root - 2017-12-09 10:44:58.574797: step 16970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:17m:54s remains)
INFO - root - 2017-12-09 10:45:07.301153: step 16980, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:15m:26s remains)
INFO - root - 2017-12-09 10:45:15.965750: step 16990, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 75h:44m:01s remains)
INFO - root - 2017-12-09 10:45:24.533586: step 17000, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:01m:12s remains)
2017-12-09 10:45:25.353409: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0039665764 0.0088400152 0.020037338 0.036652967 0.05477272 0.070147134 0.0800479 0.083181933 0.079430819 0.069688372 0.056542955 0.042340271 0.028579032 0.016627137 0.0077667376][0.011963952 0.018689347 0.034039788 0.057597965 0.084657781 0.10953017 0.12733905 0.1352565 0.13210961 0.11895279 0.099005535 0.075713262 0.052293427 0.031690426 0.016142555][0.020734148 0.03081586 0.051439956 0.082654513 0.11940508 0.15471929 0.18177184 0.19602667 0.19506377 0.17986032 0.15387554 0.12144408 0.087104909 0.055539876 0.030622212][0.028604632 0.042367123 0.069255054 0.10941843 0.15728857 0.20482075 0.24288841 0.26463687 0.26612514 0.2484711 0.21585132 0.17371304 0.12788199 0.0844388 0.04902675][0.033491306 0.050466418 0.082836546 0.13133097 0.19025989 0.25079998 0.30172449 0.33343571 0.33980763 0.32153866 0.28313223 0.23119049 0.17319012 0.1168933 0.069908679][0.033759296 0.052633412 0.088603705 0.14292882 0.21035352 0.28162098 0.34397233 0.38554269 0.39858389 0.38266727 0.34221089 0.28414118 0.21668866 0.14923769 0.091438681][0.029988801 0.048507825 0.084943682 0.14120507 0.21266103 0.2904335 0.36118519 0.411341 0.43156806 0.42023781 0.38123459 0.3212502 0.24867471 0.17395461 0.10848211][0.024049044 0.040307723 0.073986553 0.12836535 0.19993752 0.27995846 0.35531148 0.41166276 0.43839291 0.43233493 0.39666495 0.33786219 0.26409277 0.18638031 0.11734678][0.018303277 0.030926934 0.059528321 0.10847113 0.17561704 0.25328639 0.32906637 0.38834509 0.41956612 0.41842249 0.38711339 0.331637 0.26013353 0.18385892 0.11587939][0.012709482 0.021891687 0.044526815 0.085692614 0.14475901 0.21537347 0.28674138 0.34495229 0.37822616 0.38116634 0.35483083 0.30463222 0.23848936 0.16781521 0.10510695][0.0087096263 0.014380388 0.030652894 0.062695131 0.11108042 0.17102481 0.23350175 0.28637257 0.31849092 0.32403132 0.30300587 0.2599943 0.20253369 0.14133514 0.087508261][0.0064004865 0.0096508991 0.020083098 0.042700969 0.078894094 0.12548293 0.17587802 0.2200928 0.24814944 0.2545622 0.23850381 0.20385538 0.15731017 0.10825223 0.065790333][0.0036450811 0.005966153 0.012554044 0.027379818 0.052361235 0.085634232 0.12246262 0.15560156 0.17721622 0.18267639 0.17080222 0.1446757 0.10996911 0.074077442 0.043673262][0.00079707755 0.0022440208 0.0063525112 0.015506333 0.031193445 0.052835368 0.077393554 0.099799484 0.11449056 0.11824138 0.10995571 0.091693759 0.06799411 0.044245765 0.024719385][-0.0013366238 -0.00069125229 0.0014595725 0.0063703656 0.015160175 0.027693147 0.042193219 0.055675574 0.064650573 0.066924535 0.061766341 0.050480157 0.036210395 0.022334641 0.011237195]]...]
INFO - root - 2017-12-09 10:45:34.077994: step 17010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:08m:55s remains)
INFO - root - 2017-12-09 10:45:42.739958: step 17020, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.856 sec/batch; 74h:58m:14s remains)
INFO - root - 2017-12-09 10:45:51.166715: step 17030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:12m:22s remains)
INFO - root - 2017-12-09 10:45:59.821529: step 17040, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 73h:17m:32s remains)
INFO - root - 2017-12-09 10:46:08.544594: step 17050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:55m:14s remains)
INFO - root - 2017-12-09 10:46:17.316030: step 17060, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:55m:05s remains)
INFO - root - 2017-12-09 10:46:25.937384: step 17070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:12m:48s remains)
INFO - root - 2017-12-09 10:46:34.567644: step 17080, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 73h:12m:12s remains)
INFO - root - 2017-12-09 10:46:43.355300: step 17090, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 79h:50m:04s remains)
INFO - root - 2017-12-09 10:46:51.887104: step 17100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:44m:43s remains)
2017-12-09 10:46:52.883225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0016066679 0.0013562245 0.0011137277 0.00087163923 0.00060252775 0.00032429956 9.18312e-05 -4.4412445e-05 -7.7460892e-05 -6.7364657e-05 -0.00010782829 -0.00029695756 -0.00068630441 -0.0012420521 -0.00185419][0.00460185 0.0042587877 0.0039003366 0.0035432274 0.0031574168 0.0027573844 0.0024139942 0.0022073425 0.00217961 0.0022701868 0.0023180479 0.002123598 0.0015472914 0.000607454 -0.00051277387][0.0071365004 0.0066924375 0.006332851 0.006108311 0.0059618987 0.0058314493 0.0057060812 0.0056148032 0.0055929162 0.0056096641 0.0055265818 0.0051345211 0.0042585675 0.0028990977 0.0012627235][0.0079890247 0.0074234847 0.0071605053 0.0072824135 0.007673081 0.0081453435 0.0085562551 0.0088308118 0.0089670969 0.0089459056 0.0086598415 0.0079560662 0.0067195697 0.0049884748 0.0029777293][0.007055833 0.0063544391 0.0062230108 0.0067761671 0.0078333141 0.0090499055 0.010097332 0.010768712 0.011026725 0.01090715 0.010408065 0.0094699683 0.0080441376 0.0061963797 0.0041227429][0.0048306119 0.0040782369 0.0040876288 0.0050091194 0.0066458536 0.0085425852 0.010200676 0.011251045 0.011580788 0.011267822 0.010442229 0.0092113819 0.0076538967 0.0058859233 0.0040551419][0.0021284334 0.0015102867 0.0017190524 0.0029221778 0.0049302094 0.0072574569 0.0093099484 0.010593855 0.010905573 0.010323244 0.009076694 0.0074543552 0.0057130782 0.0040669693 0.0026237671][-0.00031715422 -0.0006644174 -0.00027103256 0.0010269214 0.0030826079 0.0054594409 0.0075496286 0.00880566 0.0089673037 0.0081156539 0.0065557193 0.0046981722 0.0029105619 0.0014573643 0.0004092725][-0.0019358119 -0.0020070123 -0.0015654066 -0.00045715785 0.0012512584 0.0032309638 0.0049467329 0.0058872942 0.0058034509 0.0048039053 0.0032460168 0.0015660604 0.00011083763 -0.0009260294 -0.0015493113][-0.0027403352 -0.0026934771 -0.0023657733 -0.0016436619 -0.00053496775 0.00075356918 0.0018403628 0.0023475848 0.00211148 0.0012588087 0.00010511884 -0.0010141546 -0.0018797751 -0.0024153152 -0.0026783608][-0.0030139468 -0.0029724 -0.0028153129 -0.0024758645 -0.0019438694 -0.0013282235 -0.00083701382 -0.00067031081 -0.00088980538 -0.0013891974 -0.0019708734 -0.0024652984 -0.0027967717 -0.0029702659 -0.0030375598][-0.0030746795 -0.00306088 -0.0030155787 -0.0029107961 -0.0027405466 -0.0025460185 -0.0024034837 -0.002380505 -0.0024852252 -0.002662668 -0.002836193 -0.0029574428 -0.0030225262 -0.0030497294 -0.0030593029][-0.0030762942 -0.0030750483 -0.0030714427 -0.0030573057 -0.0030303726 -0.0029977064 -0.0029736026 -0.0029705812 -0.0029878793 -0.0030148528 -0.003036777 -0.0030472968 -0.0030505885 -0.0030520475 -0.0030549043][-0.0030731289 -0.0030727233 -0.0030743016 -0.003073965 -0.0030720122 -0.0030693638 -0.003065866 -0.0030628489 -0.0030598689 -0.0030578026 -0.0030557089 -0.0030535129 -0.0030518919 -0.003050914 -0.0030521909][-0.0030719764 -0.0030707689 -0.0030723074 -0.0030733508 -0.0030736735 -0.0030730944 -0.0030716327 -0.003070236 -0.0030687735 -0.003066957 -0.0030638957 -0.0030602966 -0.003056502 -0.0030532326 -0.0030521934]]...]
INFO - root - 2017-12-09 10:47:01.216780: step 17110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:00m:07s remains)
INFO - root - 2017-12-09 10:47:10.031759: step 17120, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 77h:04m:34s remains)
INFO - root - 2017-12-09 10:47:18.766991: step 17130, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.856 sec/batch; 74h:56m:41s remains)
INFO - root - 2017-12-09 10:47:27.464520: step 17140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:28m:42s remains)
INFO - root - 2017-12-09 10:47:36.000322: step 17150, loss = 0.88, batch loss = 0.67 (9.6 examples/sec; 0.835 sec/batch; 73h:10m:38s remains)
INFO - root - 2017-12-09 10:47:44.435439: step 17160, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 75h:16m:30s remains)
INFO - root - 2017-12-09 10:47:53.122827: step 17170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:42m:31s remains)
INFO - root - 2017-12-09 10:48:01.656685: step 17180, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:15m:59s remains)
INFO - root - 2017-12-09 10:48:10.207407: step 17190, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 77h:59m:15s remains)
INFO - root - 2017-12-09 10:48:18.618066: step 17200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:16m:16s remains)
2017-12-09 10:48:19.500698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00305125 -0.0030486488 -0.0030483126 -0.0030483084 -0.0030484158 -0.0030486714 -0.0030488796 -0.0030491264 -0.0030492707 -0.0030491424 -0.0030488942 -0.0030486092 -0.0030483881 -0.0030482637 -0.0030482607][-0.0030494421 -0.0030466409 -0.0030463082 -0.0030463815 -0.0030465913 -0.0030469142 -0.0030472463 -0.0030475389 -0.0030477042 -0.0030477063 -0.0030475289 -0.0030472646 -0.0030470146 -0.003046832 -0.0030467187][-0.0030496963 -0.0030467499 -0.0030464313 -0.0030465266 -0.0030467766 -0.0030471536 -0.0030475932 -0.0030479282 -0.0030481119 -0.003048104 -0.0030478698 -0.0030475066 -0.0030471776 -0.0030469492 -0.0030467834][-0.0030499217 -0.0030469021 -0.0030465845 -0.0030467166 -0.0030470192 -0.0030474674 -0.0030479664 -0.003048331 -0.0030484963 -0.0030484034 -0.003048073 -0.003047606 -0.0030471727 -0.0030468868 -0.003046718][-0.003050176 -0.0030470877 -0.003046813 -0.0030470004 -0.0030473685 -0.0030478591 -0.0030483825 -0.0030487559 -0.0030488709 -0.0030486893 -0.0030482477 -0.0030476833 -0.0030471752 -0.0030468383 -0.0030466605][-0.0030504283 -0.0030472854 -0.0030470628 -0.0030472954 -0.0030477021 -0.0030482102 -0.0030487338 -0.003049091 -0.003049152 -0.0030488896 -0.0030483529 -0.0030477147 -0.0030471568 -0.0030467738 -0.0030465974][-0.0030505066 -0.0030474486 -0.0030473238 -0.0030476241 -0.003048043 -0.003048531 -0.0030490125 -0.0030493159 -0.0030492719 -0.0030489338 -0.0030483496 -0.003047694 -0.0030471142 -0.0030467259 -0.0030465825][-0.0030505 -0.0030476179 -0.0030476078 -0.0030479638 -0.003048403 -0.0030488505 -0.0030492488 -0.0030494775 -0.0030493189 -0.003048894 -0.0030482996 -0.0030476684 -0.003047097 -0.0030467231 -0.0030466008][-0.0030504009 -0.0030477275 -0.0030478218 -0.0030482237 -0.003048698 -0.0030490961 -0.0030493813 -0.0030495133 -0.0030492784 -0.0030488274 -0.0030482349 -0.0030476574 -0.0030471492 -0.003046816 -0.0030466858][-0.0030502733 -0.003047704 -0.0030478861 -0.0030482945 -0.003048765 -0.0030491133 -0.0030492882 -0.0030493594 -0.0030491082 -0.0030486728 -0.0030481361 -0.0030476225 -0.0030472025 -0.0030469093 -0.0030467843][-0.0030499741 -0.0030474681 -0.0030478053 -0.0030482071 -0.0030486365 -0.003048881 -0.0030489257 -0.003048904 -0.0030486293 -0.0030482267 -0.0030478083 -0.0030474223 -0.0030470996 -0.0030468984 -0.0030468288][-0.0030496763 -0.0030471981 -0.0030476274 -0.0030480283 -0.0030484274 -0.0030485881 -0.0030485156 -0.0030484113 -0.0030481312 -0.0030477573 -0.0030474761 -0.003047249 -0.0030470185 -0.0030468758 -0.0030468369][-0.0030496675 -0.003046975 -0.0030474453 -0.0030478446 -0.0030482085 -0.0030483273 -0.0030481943 -0.0030480395 -0.0030477636 -0.0030474088 -0.0030472085 -0.0030471082 -0.003046965 -0.0030468397 -0.0030468204][-0.0030497264 -0.0030468383 -0.0030472919 -0.0030476889 -0.0030480216 -0.0030481289 -0.0030479941 -0.0030477927 -0.0030475054 -0.0030471729 -0.0030470069 -0.0030469478 -0.0030468581 -0.0030467648 -0.0030467715][-0.0030498912 -0.0030468057 -0.003047212 -0.0030476025 -0.0030479063 -0.0030480067 -0.0030478975 -0.0030476765 -0.0030473864 -0.0030470474 -0.00304688 -0.0030468183 -0.0030467284 -0.0030466565 -0.0030466709]]...]
INFO - root - 2017-12-09 10:48:28.052325: step 17210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:33m:21s remains)
INFO - root - 2017-12-09 10:48:36.516592: step 17220, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 73h:00m:12s remains)
INFO - root - 2017-12-09 10:48:45.152486: step 17230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:29m:15s remains)
INFO - root - 2017-12-09 10:48:53.938953: step 17240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:49m:47s remains)
INFO - root - 2017-12-09 10:49:02.674162: step 17250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:37m:05s remains)
INFO - root - 2017-12-09 10:49:11.374139: step 17260, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:58m:17s remains)
INFO - root - 2017-12-09 10:49:20.028292: step 17270, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 78h:00m:55s remains)
INFO - root - 2017-12-09 10:49:28.673919: step 17280, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:35m:30s remains)
INFO - root - 2017-12-09 10:49:37.467101: step 17290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 75h:13m:18s remains)
INFO - root - 2017-12-09 10:49:46.077104: step 17300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:51m:33s remains)
2017-12-09 10:49:46.900312: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00029462902 -0.00015464309 -0.00096021895 -0.0017534323 -0.0023954965 -0.0027790873 -0.0029691586 -0.0030508484 -0.0030799382 -0.0030838707 -0.00308438 -0.003083922 -0.0030840044 -0.0030833553 -0.0030835138][0.00078142015 0.00041959342 -0.00034773 -0.0011698073 -0.0019452434 -0.0025145398 -0.0028571091 -0.003010801 -0.0030676783 -0.0030822419 -0.003084708 -0.0030843702 -0.0030848412 -0.0030841923 -0.0030845937][0.0011613178 0.00094438577 0.00027589384 -0.000522417 -0.0013764483 -0.0020992039 -0.0026251688 -0.0029176604 -0.0030446667 -0.0030794821 -0.0030851855 -0.0030854419 -0.0030861527 -0.0030863283 -0.0030868233][0.0015195622 0.001433122 0.00090131955 0.00018159044 -0.00070546195 -0.0015640128 -0.0022817121 -0.0027377116 -0.002976143 -0.0030639123 -0.00308494 -0.003086454 -0.0030868768 -0.0030869781 -0.0030880303][0.0017113592 0.0017413935 0.0013239882 0.00071817171 -9.5837517e-05 -0.0009695082 -0.0018138327 -0.0024474349 -0.0028497516 -0.0030265593 -0.0030818598 -0.0030872452 -0.0030863164 -0.0030857469 -0.0030863716][0.0017685383 0.0018248325 0.0014588544 0.00093214447 0.00021623378 -0.00058200327 -0.0014192262 -0.0021394733 -0.0026806616 -0.0029674228 -0.0030745731 -0.0030881953 -0.0030862628 -0.0030845643 -0.0030849839][0.001721682 0.0017103083 0.0013393904 0.00082949596 0.00017557689 -0.00052724709 -0.0012805068 -0.0019841776 -0.0025733267 -0.0029220397 -0.0030665519 -0.0030875949 -0.0030856384 -0.0030834887 -0.0030832745][0.0015140893 0.0013820464 0.00098772207 0.00047135726 -0.00015657092 -0.00080536096 -0.0014734147 -0.0020886017 -0.0026108213 -0.0029285653 -0.0030652988 -0.0030861818 -0.0030851895 -0.0030833634 -0.0030829597][0.0011274363 0.00085903518 0.00040206709 -0.00013185572 -0.00073711481 -0.0013310051 -0.0018954313 -0.0023828477 -0.0027684197 -0.0029835303 -0.0030720257 -0.0030852188 -0.0030854072 -0.0030846416 -0.0030850011][0.00055681006 0.00017726864 -0.000348381 -0.00089520519 -0.0014646949 -0.0019759389 -0.0024025089 -0.0027217823 -0.0029390841 -0.0030426474 -0.0030808009 -0.0030850065 -0.0030850023 -0.0030853392 -0.0030868715][-3.5963021e-05 -0.00047744461 -0.0010974521 -0.0016610306 -0.0021655078 -0.0025476841 -0.0028066849 -0.0029608454 -0.0030441734 -0.0030753417 -0.0030847746 -0.0030851252 -0.0030850109 -0.0030859078 -0.0030872608][-0.00049840566 -0.00095488387 -0.001635196 -0.0021936565 -0.0026184206 -0.0028791067 -0.0030116055 -0.0030633176 -0.0030813671 -0.0030847513 -0.003085552 -0.0030852014 -0.0030846642 -0.0030852258 -0.0030857895][-0.00055378629 -0.0010239873 -0.0018017425 -0.00240253 -0.0028102021 -0.0030067756 -0.003073371 -0.003083776 -0.0030856463 -0.0030858321 -0.0030855683 -0.0030858594 -0.0030852549 -0.0030852114 -0.0030854505][-0.00032001175 -0.00076099415 -0.0015924752 -0.0022640256 -0.0027547451 -0.0029978494 -0.0030801224 -0.0030854596 -0.0030858174 -0.003085745 -0.0030852875 -0.0030847315 -0.0030830202 -0.0030837574 -0.0030837657][7.2365627e-05 -0.00036613992 -0.0012635516 -0.0020284706 -0.0026255213 -0.0029465565 -0.00307326 -0.0030846577 -0.0030852235 -0.0030852887 -0.0030842531 -0.0030832638 -0.0030815392 -0.0030818288 -0.0030808498]]...]
INFO - root - 2017-12-09 10:49:55.492197: step 17310, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 75h:52m:14s remains)
INFO - root - 2017-12-09 10:50:04.204280: step 17320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:46m:07s remains)
INFO - root - 2017-12-09 10:50:12.939653: step 17330, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 77h:24m:58s remains)
INFO - root - 2017-12-09 10:50:21.715904: step 17340, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:28m:24s remains)
INFO - root - 2017-12-09 10:50:30.524123: step 17350, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 79h:29m:11s remains)
INFO - root - 2017-12-09 10:50:39.215403: step 17360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:57m:16s remains)
INFO - root - 2017-12-09 10:50:47.902810: step 17370, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:52m:00s remains)
INFO - root - 2017-12-09 10:50:56.664329: step 17380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 77h:57m:04s remains)
INFO - root - 2017-12-09 10:51:05.386324: step 17390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:28m:22s remains)
INFO - root - 2017-12-09 10:51:13.871170: step 17400, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:38m:40s remains)
2017-12-09 10:51:14.741669: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13518013 0.13880694 0.14233093 0.14533544 0.14763567 0.14869648 0.14857821 0.14735803 0.14430249 0.14045545 0.13725558 0.13670269 0.13902555 0.14364883 0.14889745][0.099581011 0.10361212 0.10896916 0.11472332 0.12018719 0.12450625 0.1269602 0.12773459 0.12614767 0.1227337 0.12030878 0.12041564 0.12434475 0.13164254 0.13958867][0.067526095 0.071792386 0.078788124 0.08728946 0.095932782 0.1032391 0.10808382 0.11022373 0.10950387 0.1065371 0.10371927 0.10387113 0.10862475 0.1174587 0.12785836][0.043173421 0.047905233 0.056164447 0.067071535 0.07848116 0.088580489 0.09579511 0.099253863 0.099200264 0.096015044 0.092058294 0.090672433 0.094709508 0.10393848 0.11613927][0.027229484 0.032474086 0.042063303 0.055146173 0.069104366 0.082040705 0.09135773 0.096036918 0.095980689 0.0916171 0.085453779 0.082038663 0.084108353 0.0926283 0.10521761][0.017774131 0.023542156 0.034234926 0.049222488 0.065655835 0.081247993 0.092820272 0.099061452 0.098837987 0.093066312 0.084788717 0.078688487 0.078537636 0.085741214 0.097802937][0.013129711 0.019136148 0.030443156 0.047080804 0.065606885 0.083836325 0.097073123 0.10459939 0.10420728 0.097837433 0.088089794 0.080118939 0.078031942 0.083631046 0.094383582][0.011556428 0.017944643 0.029661106 0.047269121 0.067530714 0.088130333 0.10330863 0.11195058 0.11189622 0.10485853 0.093738928 0.0845089 0.080898717 0.085367948 0.095013842][0.010409076 0.016876969 0.028489856 0.046032853 0.066793315 0.088421874 0.10512205 0.11548244 0.11705438 0.1109706 0.099892378 0.089552596 0.084213726 0.087120011 0.095153853][0.0080841547 0.014108116 0.024621923 0.040605616 0.060154982 0.080954984 0.097873375 0.10912397 0.11226907 0.10788148 0.098017983 0.0881648 0.082395881 0.083609022 0.089657918][0.0046859328 0.0094033377 0.017770557 0.030788088 0.047211848 0.065049946 0.080342107 0.091171317 0.095280521 0.09280134 0.0853257 0.077187933 0.071830489 0.071944259 0.076091237][0.0017867589 0.0049218303 0.010610862 0.019754527 0.031721782 0.045120049 0.057091415 0.065916918 0.069846332 0.068758957 0.063762389 0.058021422 0.053946979 0.053701263 0.056414347][-0.0003425146 0.0015355267 0.0049701855 0.010579661 0.018176738 0.02687799 0.035004314 0.041200347 0.044308253 0.043995827 0.041004825 0.037321243 0.034542445 0.034201715 0.035895366][-0.0018410941 -0.00087050931 0.00093637523 0.0039526671 0.0081583429 0.013084102 0.017886538 0.021733435 0.023862852 0.024032334 0.022565354 0.020492971 0.018800668 0.018422835 0.019280734][-0.0027128002 -0.0023209779 -0.001554509 -0.00020406814 0.0017741483 0.0041677188 0.0065951981 0.0085980892 0.0097746914 0.0099629862 0.0093321549 0.0084040966 0.0076244334 0.0074229138 0.007837791]]...]
INFO - root - 2017-12-09 10:51:23.328571: step 17410, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 78h:48m:29s remains)
INFO - root - 2017-12-09 10:51:31.930536: step 17420, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:34m:50s remains)
INFO - root - 2017-12-09 10:51:40.706257: step 17430, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 74h:53m:35s remains)
INFO - root - 2017-12-09 10:51:49.473248: step 17440, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.914 sec/batch; 79h:59m:36s remains)
INFO - root - 2017-12-09 10:51:58.098671: step 17450, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 76h:16m:10s remains)
INFO - root - 2017-12-09 10:52:06.763815: step 17460, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.904 sec/batch; 79h:05m:58s remains)
INFO - root - 2017-12-09 10:52:15.265123: step 17470, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 74h:45m:44s remains)
INFO - root - 2017-12-09 10:52:23.778579: step 17480, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 77h:56m:29s remains)
INFO - root - 2017-12-09 10:52:32.378984: step 17490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:16m:10s remains)
INFO - root - 2017-12-09 10:52:40.910077: step 17500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:58m:20s remains)
2017-12-09 10:52:41.851664: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095822237 0.098505847 0.096283071 0.09047763 0.0806185 0.067936763 0.05364424 0.040409103 0.030200766 0.023428114 0.019406797 0.017466564 0.016688764 0.016604353 0.015944963][0.1377393 0.14089306 0.13742474 0.12986074 0.1175195 0.10180195 0.083734326 0.066491216 0.052497722 0.042456262 0.035993204 0.03213938 0.030569071 0.030156037 0.029352633][0.18549205 0.19015358 0.18629289 0.17747796 0.16301045 0.14420384 0.12192133 0.099594496 0.08085379 0.066992491 0.058041789 0.052662045 0.050700389 0.050271776 0.049427234][0.22600862 0.23326924 0.23136419 0.22405869 0.21088867 0.19261946 0.16913009 0.14351374 0.1200119 0.10127769 0.08838433 0.079975434 0.076418743 0.07508073 0.073475689][0.25271866 0.26165873 0.26172602 0.25748652 0.24794349 0.23264952 0.21099725 0.1855409 0.16025446 0.13870943 0.12273712 0.11145885 0.10564863 0.1022919 0.098673053][0.26469469 0.27477798 0.27587649 0.27386305 0.26755825 0.25603747 0.2380216 0.21481958 0.1900117 0.16762219 0.14984974 0.13661857 0.1290171 0.12404932 0.11874247][0.26349649 0.2736434 0.27432141 0.27272087 0.26808655 0.25919041 0.2443535 0.22428627 0.20136784 0.17948848 0.16124257 0.14738119 0.13889241 0.13309899 0.12710965][0.24980491 0.25914741 0.25862494 0.25622284 0.25191456 0.24458727 0.23234625 0.21521257 0.19437525 0.17337795 0.15524679 0.14139882 0.13275304 0.12705103 0.1216817][0.22088943 0.22905502 0.2273162 0.22423454 0.22033282 0.21455075 0.20489919 0.1906902 0.17226413 0.15279993 0.13548264 0.12219879 0.11395255 0.1089016 0.10479761][0.17815478 0.18447115 0.18183812 0.1784482 0.17517863 0.1710306 0.16406469 0.15323693 0.13806865 0.1214252 0.10620856 0.094683625 0.0876958 0.083846584 0.081272826][0.1288188 0.13269214 0.12926951 0.12588692 0.12312454 0.12013286 0.11526506 0.10738665 0.095744759 0.082858518 0.071185671 0.062620834 0.057762835 0.055754922 0.055105202][0.0811605 0.082527377 0.079035357 0.075862326 0.073440351 0.07118576 0.067752488 0.06226876 0.054120041 0.045237135 0.037465196 0.032310121 0.029922711 0.029669018 0.030453971][0.04167591 0.041712575 0.039278328 0.037131377 0.035454 0.033908326 0.031631876 0.028077776 0.023001939 0.017709931 0.013394063 0.010837922 0.010053188 0.010621911 0.01182878][0.014426094 0.01422143 0.013129476 0.012225829 0.011561617 0.010890633 0.0098393671 0.0081389248 0.0057500335 0.0033537713 0.0014706431 0.00049331435 0.0003873948 0.00084247766 0.0015960173][0.00075302715 0.0006120652 0.00034546247 0.00019663107 0.00010108715 3.4822151e-06 -0.000240342 -0.00066852383 -0.0012772104 -0.0019254095 -0.0024062977 -0.0026369558 -0.0026501971 -0.0025310353 -0.0022909085]]...]
INFO - root - 2017-12-09 10:52:50.388629: step 17510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:05m:37s remains)
INFO - root - 2017-12-09 10:52:58.763849: step 17520, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:02m:22s remains)
INFO - root - 2017-12-09 10:53:07.456703: step 17530, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 74h:57m:24s remains)
INFO - root - 2017-12-09 10:53:16.232258: step 17540, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 74h:40m:13s remains)
INFO - root - 2017-12-09 10:53:24.818608: step 17550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:21m:09s remains)
INFO - root - 2017-12-09 10:53:33.611922: step 17560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:02m:29s remains)
INFO - root - 2017-12-09 10:53:42.447217: step 17570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:07m:39s remains)
INFO - root - 2017-12-09 10:53:51.116292: step 17580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 76h:29m:54s remains)
INFO - root - 2017-12-09 10:53:59.843504: step 17590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:55m:18s remains)
INFO - root - 2017-12-09 10:54:08.381385: step 17600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 77h:12m:40s remains)
2017-12-09 10:54:09.305416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030868337 -0.0030770747 -0.0030695233 -0.0030640606 -0.0030605879 -0.0030579788 -0.0030559225 -0.0030538964 -0.0030522859 -0.00305017 -0.0030487727 -0.0030480076 -0.0030484279 -0.0030499182 -0.0030533923][-0.0030964706 -0.0030886424 -0.0030827867 -0.0030785922 -0.0030758216 -0.0030736746 -0.0030718995 -0.0030701065 -0.003068459 -0.0030661337 -0.0030641872 -0.0030626792 -0.0030622738 -0.0030629509 -0.0030656592][-0.0031065312 -0.0031006485 -0.0030961731 -0.003093122 -0.0030912431 -0.003089794 -0.0030887 -0.0030876698 -0.0030866372 -0.0030848342 -0.0030831515 -0.003081545 -0.0030805741 -0.0030804076 -0.0030818721][-0.0031130773 -0.003108887 -0.0031055706 -0.0031031342 -0.0031016557 -0.0031006883 -0.003100039 -0.0030994 -0.0030988613 -0.0030975488 -0.0030962841 -0.0030948443 -0.0030939877 -0.0030935619 -0.0030942846][-0.0031155301 -0.0031131057 -0.0031114859 -0.0031101548 -0.0031089471 -0.0031077713 -0.0031068642 -0.003106385 -0.0031060968 -0.0031054772 -0.0031047638 -0.0031039168 -0.0031033496 -0.003102974 -0.0031036043][-0.0031125515 -0.0031112866 -0.0031111152 -0.0031105222 -0.0031096253 -0.0031085028 -0.0031074698 -0.0031072355 -0.0031073126 -0.0031072823 -0.0031072674 -0.0031075203 -0.0031081901 -0.0031088439 -0.0031101573][-0.0031061054 -0.0031051084 -0.0031053396 -0.0031052679 -0.0031048567 -0.0031041796 -0.0031034979 -0.0031034688 -0.0031036965 -0.0031042064 -0.0031050509 -0.0031063149 -0.0031080421 -0.0031098728 -0.0031117231][-0.0031003342 -0.0030983468 -0.0030985305 -0.0030983444 -0.0030982709 -0.0030975789 -0.0030970313 -0.003097011 -0.0030972569 -0.0030981589 -0.0030996359 -0.0031017824 -0.003104625 -0.0031075 -0.0031098365][-0.0030981372 -0.0030953342 -0.0030951544 -0.0030942145 -0.0030933581 -0.0030925502 -0.0030917407 -0.0030914252 -0.0030913688 -0.0030920561 -0.0030935802 -0.0030957772 -0.0030987957 -0.0031021459 -0.0031050474][-0.0030971041 -0.0030945805 -0.0030947167 -0.0030939535 -0.0030928659 -0.003092139 -0.0030914962 -0.0030908925 -0.003090502 -0.0030906908 -0.0030914557 -0.0030927947 -0.0030949677 -0.0030977353 -0.0031004122][-0.0030966769 -0.0030943686 -0.0030948976 -0.0030945807 -0.0030940373 -0.0030938264 -0.0030938364 -0.0030933418 -0.0030927064 -0.0030925055 -0.003092923 -0.0030935057 -0.0030945833 -0.0030961973 -0.0030978597][-0.0030969244 -0.0030948857 -0.00309561 -0.0030955118 -0.0030954489 -0.0030957703 -0.0030962322 -0.003096082 -0.0030956082 -0.0030955833 -0.0030958091 -0.0030957451 -0.0030959321 -0.0030963311 -0.0030968415][-0.0030974199 -0.0030957307 -0.0030965917 -0.0030967945 -0.0030968178 -0.0030971898 -0.0030975137 -0.0030976764 -0.0030976827 -0.0030978618 -0.003098279 -0.0030984329 -0.003098428 -0.0030983118 -0.003098134][-0.0030991721 -0.0030980231 -0.0030987342 -0.0030985016 -0.0030980729 -0.003098225 -0.0030983407 -0.0030980208 -0.0030977987 -0.0030980473 -0.0030984364 -0.0030984581 -0.0030983663 -0.0030982734 -0.00309799][-0.0031004576 -0.0030996623 -0.0031004329 -0.0031002965 -0.0030999568 -0.0030995146 -0.0030989677 -0.0030986848 -0.0030983314 -0.0030983619 -0.0030984294 -0.0030987321 -0.0030987118 -0.0030985465 -0.0030980746]]...]
INFO - root - 2017-12-09 10:54:17.859660: step 17610, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 72h:49m:40s remains)
INFO - root - 2017-12-09 10:54:26.536907: step 17620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:56m:48s remains)
INFO - root - 2017-12-09 10:54:35.140050: step 17630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:28m:20s remains)
INFO - root - 2017-12-09 10:54:43.850639: step 17640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:07m:13s remains)
INFO - root - 2017-12-09 10:54:52.343892: step 17650, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 76h:49m:53s remains)
INFO - root - 2017-12-09 10:55:00.895819: step 17660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 77h:12m:39s remains)
INFO - root - 2017-12-09 10:55:09.444038: step 17670, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 72h:08m:35s remains)
INFO - root - 2017-12-09 10:55:17.903844: step 17680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:25m:42s remains)
INFO - root - 2017-12-09 10:55:26.580693: step 17690, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 78h:11m:35s remains)
INFO - root - 2017-12-09 10:55:35.009288: step 17700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:12m:23s remains)
2017-12-09 10:55:35.877869: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053600188 0.056189943 0.057359111 0.053680539 0.046780515 0.037120938 0.026172906 0.018243028 0.012162506 0.0083978474 0.0061465679 0.004747251 0.0050394917 0.007120668 0.012607157][0.070860349 0.074748941 0.076742359 0.0728357 0.063536741 0.050411839 0.035538379 0.023447881 0.014565147 0.010100265 0.0081037413 0.0069166115 0.0065106619 0.0073105991 0.010551311][0.096295416 0.10213691 0.1051672 0.10112398 0.089021049 0.071739405 0.051206436 0.032745436 0.018472288 0.010787096 0.007057189 0.0059456816 0.005845082 0.0068978881 0.009222093][0.13397463 0.14148922 0.14551577 0.14166145 0.12723632 0.10549052 0.077925131 0.051764011 0.030402606 0.017260479 0.00991942 0.0067374967 0.0054711318 0.0053021554 0.0058062794][0.17551257 0.18356621 0.18819834 0.18461452 0.16894142 0.14355841 0.10987965 0.075241253 0.044842046 0.024549043 0.012594389 0.0074679079 0.0056100665 0.0049942946 0.0044107558][0.21265796 0.22227067 0.22726476 0.22393993 0.20652963 0.17833099 0.13981955 0.098233044 0.060093191 0.032623962 0.016482664 0.0090345014 0.0063272715 0.0053912634 0.004517979][0.24000797 0.25015828 0.25458708 0.25124115 0.23424804 0.2054982 0.16520856 0.11888072 0.075176328 0.041932181 0.021617677 0.011770948 0.0080001578 0.0068311859 0.0058401767][0.25964895 0.26911432 0.27140057 0.26630643 0.24926753 0.2206597 0.18042883 0.13286626 0.086735591 0.04982936 0.02652299 0.01460393 0.010241525 0.0092545943 0.0092168478][0.27530631 0.28485313 0.28543976 0.27839252 0.26105487 0.23202835 0.19202866 0.1443731 0.098037653 0.060492869 0.036492068 0.024561597 0.020364637 0.020192647 0.020975882][0.28274381 0.2946682 0.29489851 0.28717279 0.27069825 0.24267468 0.20461868 0.15908843 0.11438143 0.078084357 0.054638188 0.043608014 0.040591106 0.042206474 0.044687539][0.2849735 0.30052379 0.30151707 0.2950027 0.28057897 0.25544626 0.22115731 0.18032345 0.13981344 0.10657592 0.085633874 0.076126881 0.0745478 0.077564634 0.081506923][0.28538573 0.30339855 0.30414766 0.29823986 0.28557092 0.26417804 0.23528251 0.20057523 0.16595353 0.13736515 0.11993334 0.11248002 0.11212431 0.11630785 0.12142543][0.28611779 0.30598155 0.3064163 0.30037591 0.28838152 0.26972529 0.24511862 0.21610993 0.18712579 0.16321199 0.14904174 0.14253679 0.14239427 0.1459585 0.15090418][0.28003764 0.30096775 0.30302653 0.29835668 0.28833348 0.27215612 0.25070807 0.22499776 0.20018525 0.18004179 0.16819511 0.16249827 0.16243815 0.16572054 0.17055763][0.27037141 0.29026765 0.29315028 0.28925163 0.28078142 0.26680338 0.24805315 0.2246666 0.20196548 0.18447772 0.17345423 0.16809727 0.16843812 0.17195648 0.17788489]]...]
INFO - root - 2017-12-09 10:55:44.406105: step 17710, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 60h:59m:17s remains)
INFO - root - 2017-12-09 10:55:52.978047: step 17720, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:32m:11s remains)
INFO - root - 2017-12-09 10:56:01.808323: step 17730, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 79h:03m:10s remains)
INFO - root - 2017-12-09 10:56:10.529423: step 17740, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 75h:06m:09s remains)
INFO - root - 2017-12-09 10:56:19.302655: step 17750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:59m:03s remains)
INFO - root - 2017-12-09 10:56:27.996642: step 17760, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:40m:10s remains)
INFO - root - 2017-12-09 10:56:36.481607: step 17770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:43m:59s remains)
INFO - root - 2017-12-09 10:56:44.933291: step 17780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:31m:30s remains)
INFO - root - 2017-12-09 10:56:53.456918: step 17790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:51m:01s remains)
INFO - root - 2017-12-09 10:57:01.952269: step 17800, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 76h:51m:50s remains)
2017-12-09 10:57:02.877775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031360805 -0.003134951 -0.003135263 -0.0031356856 -0.0031359161 -0.0031359817 -0.0031360802 -0.0031360856 -0.0031361089 -0.0031364516 -0.0031370816 -0.0031378674 -0.0031389492 -0.0031404905 -0.0031422295][-0.0031356548 -0.0031343696 -0.0031347736 -0.0031354835 -0.0031361419 -0.0031364916 -0.0031368255 -0.0031369587 -0.0031369263 -0.0031370716 -0.0031374437 -0.0031381131 -0.0031390346 -0.0031403331 -0.0031420013][-0.0031368197 -0.0031352004 -0.0031357266 -0.0031367138 -0.0031378136 -0.0031386397 -0.0031394246 -0.0031397918 -0.0031396798 -0.0031394535 -0.0031393338 -0.0031394612 -0.0031398556 -0.0031407217 -0.0031420719][-0.0031381883 -0.0031361463 -0.0031367091 -0.0031379405 -0.0031394404 -0.0031410058 -0.0031422677 -0.0031428135 -0.0031425343 -0.0031419334 -0.0031414011 -0.0031408325 -0.0031406407 -0.0031411396 -0.0031421513][-0.0031399871 -0.0031374586 -0.0031375033 -0.0031387426 -0.003140562 -0.0031428218 -0.0031445539 -0.0031453734 -0.0031452624 -0.0031444083 -0.0031433627 -0.0031420863 -0.0031411431 -0.0031411855 -0.0031418453][-0.0031421408 -0.0031395408 -0.0031389648 -0.0031398383 -0.0031415997 -0.0031442575 -0.00314616 -0.0031471185 -0.0031473259 -0.0031463515 -0.0031449371 -0.0031431492 -0.0031414949 -0.0031409308 -0.0031411112][-0.0031439129 -0.0031414228 -0.0031404756 -0.0031408928 -0.0031423143 -0.0031447974 -0.0031463976 -0.0031475432 -0.003148139 -0.0031472854 -0.0031459334 -0.0031439585 -0.0031417871 -0.0031407168 -0.0031404819][-0.0031441287 -0.0031419974 -0.0031409482 -0.0031408777 -0.0031417701 -0.0031435194 -0.0031444395 -0.0031453795 -0.0031464924 -0.00314632 -0.0031455993 -0.0031440083 -0.0031419268 -0.0031406172 -0.0031400954][-0.0031432998 -0.0031415992 -0.0031406458 -0.0031403198 -0.0031407448 -0.003141992 -0.0031426076 -0.0031432635 -0.0031441106 -0.0031443413 -0.0031440714 -0.0031431157 -0.0031414363 -0.0031401718 -0.0031395871][-0.003142142 -0.0031406996 -0.0031399932 -0.0031398027 -0.0031400223 -0.0031407881 -0.0031413783 -0.0031418658 -0.0031422614 -0.0031423804 -0.0031422393 -0.0031415303 -0.0031402896 -0.0031393273 -0.0031388751][-0.0031408505 -0.0031394882 -0.0031392286 -0.0031390251 -0.0031390926 -0.0031394877 -0.0031400369 -0.0031405536 -0.0031407175 -0.0031407597 -0.0031407601 -0.0031401843 -0.0031394209 -0.0031388737 -0.0031385398][-0.0031393769 -0.0031380302 -0.003138114 -0.0031379433 -0.0031380339 -0.0031382984 -0.003138579 -0.0031389438 -0.0031390961 -0.0031392304 -0.003139188 -0.0031388728 -0.0031384919 -0.0031382684 -0.0031381007][-0.0031376898 -0.0031364001 -0.0031366774 -0.0031367005 -0.0031368353 -0.0031371049 -0.0031371627 -0.0031372348 -0.003137392 -0.0031375517 -0.0031374625 -0.0031372609 -0.0031371934 -0.0031372411 -0.0031373228][-0.0031367014 -0.003135232 -0.0031355782 -0.0031356735 -0.0031359387 -0.0031362828 -0.0031363703 -0.0031363235 -0.0031364257 -0.0031365405 -0.003136442 -0.0031362455 -0.0031362246 -0.0031363682 -0.0031365433][-0.003136775 -0.0031348676 -0.0031350567 -0.0031351226 -0.0031354262 -0.003135826 -0.0031359848 -0.0031359037 -0.0031359922 -0.0031361524 -0.0031361317 -0.0031359587 -0.0031359254 -0.0031361021 -0.00313634]]...]
INFO - root - 2017-12-09 10:57:11.503580: step 17810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:06m:16s remains)
INFO - root - 2017-12-09 10:57:19.908067: step 17820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 74h:17m:31s remains)
INFO - root - 2017-12-09 10:57:28.651370: step 17830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:05m:33s remains)
INFO - root - 2017-12-09 10:57:37.305796: step 17840, loss = 0.88, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 74h:41m:49s remains)
INFO - root - 2017-12-09 10:57:46.006381: step 17850, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 78h:24m:13s remains)
INFO - root - 2017-12-09 10:57:54.728005: step 17860, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 78h:12m:19s remains)
INFO - root - 2017-12-09 10:58:03.450771: step 17870, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 74h:58m:58s remains)
INFO - root - 2017-12-09 10:58:12.217920: step 17880, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 77h:26m:26s remains)
INFO - root - 2017-12-09 10:58:20.899778: step 17890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:32m:40s remains)
INFO - root - 2017-12-09 10:58:29.514351: step 17900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:45m:18s remains)
2017-12-09 10:58:30.358475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00062441523 -0.0012951781 -0.0019180825 -0.002372242 -0.0026225997 -0.0026833352 -0.0026302098 -0.0025890223 -0.0026214097 -0.0027192598 -0.0028418794 -0.0029610058 -0.0030549364 -0.0031107937 -0.0031371317][-0.0018261635 -0.0022028764 -0.0025388652 -0.0027817609 -0.0029298626 -0.0029842977 -0.0029777947 -0.0029654547 -0.0029811084 -0.0030187066 -0.0030587318 -0.0030954261 -0.0031224734 -0.0031374758 -0.0031430696][-0.0026269169 -0.0027820973 -0.0029155526 -0.0030118793 -0.0030755224 -0.0031044753 -0.0031085887 -0.0031049473 -0.0031090789 -0.0031204736 -0.0031311368 -0.0031392034 -0.003142979 -0.0031441124 -0.0031430109][-0.0030020869 -0.0030470993 -0.0030849089 -0.0031123494 -0.0031314935 -0.0031406295 -0.0031432214 -0.0031427406 -0.0031442156 -0.0031477935 -0.0031498952 -0.0031508396 -0.0031489108 -0.0031466254 -0.0031440859][-0.0031246881 -0.0031334639 -0.0031412682 -0.0031471895 -0.0031518163 -0.0031545395 -0.0031558897 -0.0031560324 -0.0031561819 -0.0031574315 -0.0031567353 -0.0031548184 -0.003151569 -0.0031485818 -0.0031455811][-0.0031471276 -0.0031511497 -0.0031533926 -0.0031545963 -0.0031562306 -0.0031576667 -0.0031591039 -0.003158889 -0.0031584715 -0.0031581759 -0.0031567621 -0.0031551502 -0.0031519553 -0.003148637 -0.0031459816][-0.0030791149 -0.0031116798 -0.003132781 -0.0031466237 -0.0031505148 -0.0031518606 -0.0031478289 -0.0031331549 -0.0031127357 -0.0030896396 -0.0030812013 -0.0030845972 -0.0030974152 -0.0031122966 -0.003125919][-0.0029360754 -0.0029849929 -0.003022335 -0.0030603758 -0.0030894445 -0.0031105895 -0.0031165418 -0.0030989945 -0.0030534992 -0.0029978252 -0.0029611243 -0.0029492071 -0.0029731093 -0.0030157475 -0.0030617565][-0.0027394518 -0.0028639783 -0.0029438727 -0.0029823328 -0.0029897108 -0.0029765305 -0.0029560544 -0.0029107458 -0.0028256325 -0.0027328129 -0.002684182 -0.002699811 -0.0027669317 -0.0028627049 -0.0029628514][-0.002291745 -0.0025747027 -0.0027781304 -0.0028638269 -0.002883598 -0.0028643715 -0.0028068856 -0.0026758974 -0.002480661 -0.0022805838 -0.0021712161 -0.0022080918 -0.0023665181 -0.0025822977 -0.0027916967][-0.0014915724 -0.0019175913 -0.0022505526 -0.0024312078 -0.002507681 -0.0024905538 -0.0024053487 -0.0022329057 -0.0019742097 -0.0016757762 -0.0014948737 -0.0015411039 -0.0017937517 -0.0021471726 -0.0024997417][-0.00051599555 -0.001038308 -0.0014609846 -0.0017057641 -0.0018224018 -0.0018292558 -0.0017398873 -0.0015417449 -0.0012495575 -0.00092928251 -0.00074142776 -0.00080376165 -0.0011459973 -0.0016377113 -0.0021446496][0.00012451597 -0.00040339422 -0.00081905839 -0.001073387 -0.0011955847 -0.0012023791 -0.0011041423 -0.00092451531 -0.00066658785 -0.00039184745 -0.00024714833 -0.00034894422 -0.00074452744 -0.0013106366 -0.0019096754][-0.00025392626 -0.00064869458 -0.00095354719 -0.0011459836 -0.001243366 -0.0012650338 -0.0012020341 -0.0010700384 -0.00088041625 -0.000685595 -0.00058895117 -0.00068385061 -0.0010276367 -0.0015241818 -0.0020510226][-0.0012785909 -0.0015204313 -0.0017014632 -0.0018150947 -0.0018698868 -0.0018755767 -0.0018286812 -0.001751328 -0.0016436442 -0.001533167 -0.0014792635 -0.0015465694 -0.0017721696 -0.0020969578 -0.0024398006]]...]
INFO - root - 2017-12-09 10:58:39.162950: step 17910, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 76h:31m:01s remains)
INFO - root - 2017-12-09 10:58:47.486006: step 17920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:39m:04s remains)
INFO - root - 2017-12-09 10:58:56.062160: step 17930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:50m:12s remains)
INFO - root - 2017-12-09 10:59:04.397086: step 17940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:02m:33s remains)
INFO - root - 2017-12-09 10:59:13.020336: step 17950, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 75h:32m:27s remains)
INFO - root - 2017-12-09 10:59:21.654558: step 17960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 74h:15m:41s remains)
INFO - root - 2017-12-09 10:59:30.021477: step 17970, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 72h:36m:17s remains)
INFO - root - 2017-12-09 10:59:38.514280: step 17980, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 76h:49m:32s remains)
INFO - root - 2017-12-09 10:59:47.286171: step 17990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:02m:34s remains)
INFO - root - 2017-12-09 10:59:55.948897: step 18000, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 67h:46m:11s remains)
2017-12-09 10:59:56.877731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031599053 -0.0031586746 -0.0031585465 -0.0031581479 -0.0031576557 -0.0031574268 -0.003157448 -0.0031577721 -0.0031581169 -0.0031584683 -0.003159391 -0.0031612203 -0.003163971 -0.0031674779 -0.0031707005][-0.0031589472 -0.0031576601 -0.0031577307 -0.0031572517 -0.0031566308 -0.0031562396 -0.0031562487 -0.003156591 -0.0031570196 -0.0031575675 -0.0031587423 -0.0031609179 -0.0031640246 -0.0031679107 -0.0031716956][-0.0031595023 -0.0031584662 -0.0031587468 -0.003158414 -0.0031578573 -0.0031573679 -0.003157015 -0.0031567567 -0.0031567703 -0.0031573346 -0.0031585714 -0.0031606692 -0.0031637717 -0.0031678074 -0.0031718197][-0.0031598809 -0.0031592213 -0.0031600625 -0.0031602602 -0.003159951 -0.0031589966 -0.0031568748 -0.0031540466 -0.0031524911 -0.0031534228 -0.0031562357 -0.0031593959 -0.0031627966 -0.003166375 -0.0031702435][-0.0031599943 -0.0031599819 -0.0031615871 -0.0031621782 -0.003160374 -0.003153082 -0.0031379496 -0.0031196962 -0.0031108712 -0.0031176656 -0.0031357722 -0.0031513886 -0.0031601444 -0.0031642008 -0.0031679438][-0.003160266 -0.0031607593 -0.0031625638 -0.0031602725 -0.0031449983 -0.0031026863 -0.0030281476 -0.0029497067 -0.0029189109 -0.0029582772 -0.0030424143 -0.0031136486 -0.0031502203 -0.0031615929 -0.0031657508][-0.0031606532 -0.003161388 -0.0031612753 -0.0031451674 -0.0030805417 -0.0029263648 -0.0026786742 -0.0024407818 -0.0023640411 -0.0025032987 -0.0027667163 -0.0029936589 -0.0031159986 -0.003156286 -0.0031644653][-0.003161157 -0.0031615652 -0.0031549234 -0.0031030227 -0.0029253308 -0.0025304931 -0.0019309785 -0.0013873562 -0.0012319486 -0.0015728809 -0.0021860986 -0.00272793 -0.0030349696 -0.0031444971 -0.0031637715][-0.0031621496 -0.0031616422 -0.0031422926 -0.0030301709 -0.0026734788 -0.0019122281 -0.00079146074 0.00019210158 0.00046195672 -0.00017056172 -0.0012853392 -0.0022979325 -0.0028964963 -0.0031238443 -0.0031633114][-0.0031630988 -0.0031611989 -0.00312738 -0.0029481456 -0.0023985524 -0.0012462034 0.00042906124 0.0018757626 0.0022792469 0.0013502098 -0.00028157607 -0.0017977603 -0.0027251192 -0.0030967691 -0.0031629892][-0.0031640066 -0.0031612031 -0.0031187397 -0.0028995113 -0.002234611 -0.00084524485 0.0011692753 0.0028891333 0.003385355 0.0022929197 0.00037522288 -0.0014451921 -0.0025933099 -0.0030735068 -0.0031621261][-0.0031641598 -0.0031619873 -0.003123089 -0.0029166257 -0.0022843743 -0.00095067592 0.00099281291 0.0026422127 0.0031373922 0.0021033289 0.00027881679 -0.0014676406 -0.0025881948 -0.0030697263 -0.0031608962][-0.0031633968 -0.0031629447 -0.0031381298 -0.0029913937 -0.0025269487 -0.0015260107 -4.4125365e-05 0.0012197809 0.0016120754 0.0008256787 -0.00057080737 -0.0018935698 -0.0027349587 -0.0030935116 -0.0031600932][-0.0031619892 -0.0031627894 -0.003153556 -0.0030801641 -0.002827636 -0.0022559147 -0.0013774586 -0.00060325 -0.00035349187 -0.00083173136 -0.0016886832 -0.0024672227 -0.002936776 -0.0031259139 -0.0031596103][-0.003160187 -0.0031611791 -0.0031611689 -0.0031396456 -0.0030489326 -0.0028216364 -0.002442566 -0.0020798091 -0.0019561858 -0.0021767179 -0.0025741342 -0.0029082466 -0.0030863732 -0.0031490366 -0.0031593759]]...]
INFO - root - 2017-12-09 11:00:05.525833: step 18010, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:28m:39s remains)
INFO - root - 2017-12-09 11:00:13.816966: step 18020, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 72h:04m:43s remains)
INFO - root - 2017-12-09 11:00:22.317295: step 18030, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 72h:55m:08s remains)
INFO - root - 2017-12-09 11:00:30.913762: step 18040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:12m:46s remains)
INFO - root - 2017-12-09 11:00:39.600754: step 18050, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 75h:55m:14s remains)
INFO - root - 2017-12-09 11:00:48.451630: step 18060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:32m:46s remains)
INFO - root - 2017-12-09 11:00:57.115524: step 18070, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 78h:07m:41s remains)
INFO - root - 2017-12-09 11:01:05.680098: step 18080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:55m:39s remains)
INFO - root - 2017-12-09 11:01:14.406751: step 18090, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 80h:12m:33s remains)
INFO - root - 2017-12-09 11:01:22.999480: step 18100, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 60h:17m:56s remains)
2017-12-09 11:01:23.882968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031736062 -0.0031735273 -0.0031740665 -0.0031744805 -0.0031744053 -0.00317395 -0.0031737052 -0.0031730381 -0.0031701992 -0.0031608832 -0.0031388979 -0.0031004807 -0.0030536437 -0.0030233823 -0.0030214651][-0.0031727569 -0.0031727876 -0.003173383 -0.0031739385 -0.0031743231 -0.0031739762 -0.003172193 -0.0031651552 -0.003143128 -0.0030901639 -0.0029918693 -0.0028568655 -0.002736262 -0.002701547 -0.0027668756][-0.003172715 -0.0031724744 -0.0031725196 -0.0031722619 -0.0031704488 -0.0031615046 -0.0031341938 -0.0030705235 -0.0029438669 -0.0027266163 -0.0024198852 -0.0020802508 -0.0018436202 -0.0018488343 -0.002097083][-0.0031716973 -0.003170073 -0.0031656642 -0.0031562778 -0.0031362846 -0.0030910154 -0.0029871857 -0.0027717967 -0.0023991645 -0.0018606038 -0.0012033165 -0.00056340033 -0.00019263662 -0.00030631339 -0.00086680311][-0.0031667289 -0.0031586019 -0.0031427478 -0.0031128784 -0.0030594722 -0.002949656 -0.0027054136 -0.0022319416 -0.0014753052 -0.00047981134 0.0006165423 0.0015699994 0.0020361571 0.0017511295 0.0008102837][-0.0031613873 -0.0031403161 -0.0030967146 -0.00301956 -0.0028980183 -0.0026919609 -0.0022786371 -0.0015057924 -0.00031148642 0.0011682524 0.0026770998 0.0038826354 0.0043820236 0.0038825274 0.0025452161][-0.0031556168 -0.0031191567 -0.0030416264 -0.0029012118 -0.0026838749 -0.0023428074 -0.0017210522 -0.00063422113 0.00097276596 0.0028901137 0.00476605 0.0061798356 0.0066773975 0.0059630871 0.0042447397][-0.0031535528 -0.0031101997 -0.003011405 -0.0028221474 -0.0025119011 -0.0020232028 -0.0012005265 0.00013717357 0.0020246962 0.0042111478 0.0062851822 0.0078141931 0.008334266 0.0075378604 0.0055884337][-0.003158285 -0.0031216354 -0.0030284349 -0.0028303144 -0.002476159 -0.0018916437 -0.00093197846 0.00054499367 0.0025214106 0.0047212718 0.0067372173 0.0082141776 0.00874072 0.00803063 0.0061187688][-0.003163246 -0.0031407073 -0.0030747084 -0.0029171619 -0.0026066802 -0.0020525032 -0.0011271473 0.00026731985 0.0020593405 0.0039833542 0.0056775734 0.00689815 0.0073635988 0.0068574743 0.0052710762][-0.0031460035 -0.0031468272 -0.003114799 -0.0030181189 -0.0027999822 -0.0023757853 -0.0016355322 -0.00051423977 0.00090024248 0.0023718104 0.0036015743 0.0044444823 0.0047521926 0.0044204304 0.0032954989][-0.003043442 -0.0030797063 -0.003087136 -0.0030489608 -0.002927389 -0.0026571108 -0.0021541594 -0.0013729943 -0.00039751409 0.00059425016 0.0013726065 0.0018396191 0.0019459985 0.0016962774 0.00097333547][-0.0027982139 -0.002885544 -0.0029531587 -0.0029818194 -0.0029506374 -0.0028095511 -0.0025025774 -0.0020076409 -0.0013943494 -0.00078594661 -0.000350588 -0.00016437354 -0.000226771 -0.00046760519 -0.00092574046][-0.0025051695 -0.00263008 -0.0027690365 -0.0028689874 -0.0029109234 -0.0028504326 -0.00265746 -0.0023318604 -0.0019459843 -0.0015827742 -0.0013688859 -0.0013652424 -0.0015455119 -0.001800839 -0.002115516][-0.0023609232 -0.0024854075 -0.0026639421 -0.0028098142 -0.0029001092 -0.0028902746 -0.0027660278 -0.0025412054 -0.0022867529 -0.0020663086 -0.00197216 -0.0020512361 -0.0022680832 -0.0025060496 -0.0027256329]]...]
INFO - root - 2017-12-09 11:01:32.593772: step 18110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:46m:50s remains)
INFO - root - 2017-12-09 11:01:41.071480: step 18120, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 68h:57m:10s remains)
INFO - root - 2017-12-09 11:01:49.550272: step 18130, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.829 sec/batch; 72h:25m:30s remains)
INFO - root - 2017-12-09 11:01:58.191755: step 18140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:52m:10s remains)
INFO - root - 2017-12-09 11:02:06.908462: step 18150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:50m:56s remains)
INFO - root - 2017-12-09 11:02:15.439112: step 18160, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 74h:23m:20s remains)
INFO - root - 2017-12-09 11:02:23.927897: step 18170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:10m:32s remains)
INFO - root - 2017-12-09 11:02:32.830795: step 18180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:51m:35s remains)
INFO - root - 2017-12-09 11:02:41.619925: step 18190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:55m:34s remains)
INFO - root - 2017-12-09 11:02:50.235300: step 18200, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 65h:06m:03s remains)
2017-12-09 11:02:51.093479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031424705 -0.0031397352 -0.0031390609 -0.0031384353 -0.003137978 -0.0031375559 -0.0031370872 -0.0031369638 -0.0031372025 -0.0031375857 -0.0031379943 -0.0031382828 -0.003138355 -0.00313831 -0.0031381925][-0.0031404942 -0.0031375724 -0.0031368437 -0.003136137 -0.0031356642 -0.0031352444 -0.0031349023 -0.0031349428 -0.0031352569 -0.0031356905 -0.0031361713 -0.0031365659 -0.0031366635 -0.0031365866 -0.0031363873][-0.003140569 -0.0031375976 -0.0031368136 -0.0031359771 -0.0031354278 -0.0031350211 -0.0031348658 -0.0031350658 -0.0031355233 -0.0031360227 -0.0031365859 -0.0031370344 -0.003137121 -0.0031370022 -0.0031366991][-0.0031406868 -0.0031376602 -0.0031368721 -0.003135944 -0.0031353091 -0.0031348842 -0.0031348327 -0.0031351806 -0.0031358926 -0.0031365377 -0.0031371391 -0.0031374744 -0.0031374106 -0.0031371943 -0.0031368064][-0.0031409203 -0.0031379629 -0.0031373729 -0.0031365508 -0.0031358767 -0.003135253 -0.0031349829 -0.0031351903 -0.003136032 -0.0031368078 -0.0031374781 -0.0031377992 -0.0031375086 -0.0031370248 -0.0031365398][-0.003141209 -0.0031385259 -0.0031383615 -0.0031379673 -0.0031374658 -0.0031367086 -0.0031359971 -0.003135511 -0.003135975 -0.0031366216 -0.0031373203 -0.0031376116 -0.0031371277 -0.0031363377 -0.0031356039][-0.0031414905 -0.0031392719 -0.003139786 -0.0031403159 -0.0031405417 -0.0031399988 -0.0031388791 -0.0031373848 -0.0031366658 -0.0031363657 -0.0031366469 -0.0031366823 -0.0031360898 -0.0031352665 -0.0031343964][-0.0031416505 -0.0031399648 -0.0031413489 -0.0031432302 -0.0031447771 -0.0031449455 -0.0031437413 -0.0031412519 -0.0031386951 -0.0031365007 -0.003135503 -0.0031350034 -0.003134435 -0.003133964 -0.0031333221][-0.0031415343 -0.00314038 -0.0031427855 -0.0031462046 -0.0031495334 -0.0031510696 -0.0031503735 -0.0031475816 -0.003143474 -0.0031389196 -0.0031357082 -0.0031340655 -0.0031332285 -0.0031329838 -0.0031327528][-0.0031413727 -0.0031404849 -0.0031437571 -0.0031484317 -0.0031534554 -0.0031564618 -0.0031567253 -0.0031540934 -0.0031489485 -0.0031426193 -0.0031372427 -0.003134157 -0.0031326422 -0.0031324835 -0.0031327351][-0.0031411471 -0.0031402486 -0.0031440579 -0.0031495153 -0.0031556834 -0.0031601537 -0.0031616664 -0.0031597097 -0.0031543623 -0.0031470438 -0.0031400514 -0.0031353647 -0.0031328993 -0.003132463 -0.0031329321][-0.0031409292 -0.0031397978 -0.0031434894 -0.0031488996 -0.0031552825 -0.0031604031 -0.0031629931 -0.0031622881 -0.0031577444 -0.0031508147 -0.0031434721 -0.0031378807 -0.0031345545 -0.0031334646 -0.0031337431][-0.0031407678 -0.0031390828 -0.0031421161 -0.0031466789 -0.003152462 -0.0031574923 -0.0031607717 -0.0031613775 -0.0031583384 -0.0031528145 -0.0031464167 -0.0031409757 -0.0031370593 -0.0031351969 -0.0031349638][-0.0031406647 -0.0031384118 -0.0031405652 -0.0031438803 -0.0031483378 -0.0031525476 -0.0031558864 -0.0031574168 -0.0031559628 -0.0031522948 -0.0031475457 -0.0031430076 -0.0031392463 -0.0031369426 -0.0031361545][-0.0031407143 -0.0031379992 -0.003139283 -0.0031413434 -0.0031442668 -0.0031473036 -0.0031501141 -0.0031518077 -0.0031514845 -0.0031495446 -0.0031465995 -0.0031432521 -0.0031401562 -0.0031378926 -0.0031367834]]...]
INFO - root - 2017-12-09 11:02:59.823735: step 18210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:41m:44s remains)
INFO - root - 2017-12-09 11:03:08.349328: step 18220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:00m:04s remains)
INFO - root - 2017-12-09 11:03:16.756287: step 18230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:55m:55s remains)
INFO - root - 2017-12-09 11:03:25.434359: step 18240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:40m:40s remains)
INFO - root - 2017-12-09 11:03:34.057545: step 18250, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 72h:26m:06s remains)
INFO - root - 2017-12-09 11:03:42.896825: step 18260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:56m:38s remains)
INFO - root - 2017-12-09 11:03:51.592457: step 18270, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 74h:47m:53s remains)
INFO - root - 2017-12-09 11:04:00.303348: step 18280, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 76h:00m:02s remains)
INFO - root - 2017-12-09 11:04:08.896473: step 18290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:36m:55s remains)
INFO - root - 2017-12-09 11:04:17.308707: step 18300, loss = 0.88, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 74h:12m:25s remains)
2017-12-09 11:04:18.087833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031425108 -0.0031384015 -0.0031389736 -0.0031404854 -0.0031424104 -0.0031443613 -0.0031457972 -0.0031469425 -0.0031475604 -0.0031478775 -0.003148122 -0.0031484407 -0.003148776 -0.0031488722 -0.0031485052][-0.0031404633 -0.0031362535 -0.003136711 -0.0031382279 -0.0031401706 -0.0031418237 -0.0031428935 -0.0031437804 -0.0031443033 -0.0031446912 -0.0031451243 -0.0031456379 -0.0031461704 -0.0031464135 -0.0031461613][-0.0031407552 -0.0031366863 -0.0031367103 -0.0031379573 -0.0031395336 -0.003140613 -0.0031410472 -0.0031415422 -0.0031419313 -0.0031424218 -0.003143094 -0.0031440293 -0.0031450263 -0.0031455618 -0.0031454945][-0.0031409743 -0.0031369689 -0.0031367471 -0.0031376351 -0.0031388921 -0.0031394663 -0.0031392926 -0.0031392975 -0.0031393878 -0.0031398395 -0.0031407194 -0.0031420649 -0.0031434854 -0.0031443771 -0.0031445746][-0.0031416588 -0.0031375356 -0.0031370209 -0.0031375315 -0.0031383273 -0.00313846 -0.0031377994 -0.0031373969 -0.0031372812 -0.0031377131 -0.0031387033 -0.0031402283 -0.0031419222 -0.0031431571 -0.003143671][-0.0031425371 -0.0031381659 -0.0031373254 -0.0031374339 -0.0031376795 -0.0031374982 -0.0031365585 -0.0031359578 -0.003135832 -0.0031363249 -0.0031374739 -0.0031391396 -0.003140915 -0.0031423441 -0.0031430845][-0.0031426819 -0.0031382381 -0.0031371573 -0.0031371443 -0.0031371266 -0.0031368367 -0.0031358514 -0.00313517 -0.0031349673 -0.0031354525 -0.0031366092 -0.0031382274 -0.0031399608 -0.0031414521 -0.003142345][-0.003142528 -0.0031380469 -0.0031368476 -0.0031367226 -0.0031368041 -0.0031366018 -0.0031357454 -0.0031349994 -0.0031348029 -0.0031353459 -0.0031363138 -0.0031375969 -0.003139141 -0.003140558 -0.0031414842][-0.003143613 -0.0031390025 -0.0031378893 -0.0031377976 -0.0031379415 -0.0031378726 -0.0031373166 -0.0031365671 -0.0031361352 -0.003136327 -0.0031369058 -0.0031377161 -0.0031388421 -0.0031399515 -0.0031407778][-0.0031471984 -0.003142423 -0.0031411874 -0.0031410102 -0.0031412626 -0.0031412807 -0.0031410106 -0.0031404516 -0.0031396002 -0.0031390979 -0.0031391552 -0.0031393103 -0.0031396963 -0.0031401429 -0.0031405096][-0.0031543504 -0.0031496268 -0.0031481106 -0.0031478025 -0.0031480838 -0.0031484046 -0.0031484531 -0.003148119 -0.0031473013 -0.0031464279 -0.0031458552 -0.0031449152 -0.0031439431 -0.0031428996 -0.0031419084][-0.0031619149 -0.0031571793 -0.0031554562 -0.0031549863 -0.0031553009 -0.0031559796 -0.0031564028 -0.003156543 -0.0031561623 -0.0031555097 -0.0031545381 -0.003152627 -0.0031503739 -0.0031477471 -0.0031450957][-0.00317051 -0.0031658914 -0.0031638839 -0.0031631475 -0.003163188 -0.0031638681 -0.0031644795 -0.0031649738 -0.0031650413 -0.0031646111 -0.0031637389 -0.0031615156 -0.0031582997 -0.0031543942 -0.0031502685][-0.0031791278 -0.0031751613 -0.0031732556 -0.0031725429 -0.0031725941 -0.0031732931 -0.00317406 -0.0031745555 -0.0031747038 -0.0031745373 -0.0031738731 -0.0031715319 -0.0031674784 -0.0031624159 -0.0031567847][-0.0031855579 -0.0031824403 -0.0031808671 -0.0031804568 -0.0031806156 -0.0031814175 -0.0031824014 -0.0031831034 -0.0031835067 -0.0031836366 -0.00318305 -0.0031806033 -0.0031759269 -0.0031697664 -0.0031627615]]...]
INFO - root - 2017-12-09 11:04:26.753488: step 18310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:42m:17s remains)
INFO - root - 2017-12-09 11:04:35.317529: step 18320, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 76h:51m:07s remains)
INFO - root - 2017-12-09 11:04:43.916887: step 18330, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 78h:51m:28s remains)
INFO - root - 2017-12-09 11:04:52.588299: step 18340, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 78h:17m:22s remains)
INFO - root - 2017-12-09 11:05:01.186414: step 18350, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:01m:17s remains)
INFO - root - 2017-12-09 11:05:09.926849: step 18360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:54m:01s remains)
INFO - root - 2017-12-09 11:05:18.546995: step 18370, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:45m:40s remains)
INFO - root - 2017-12-09 11:05:27.147199: step 18380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:02m:07s remains)
INFO - root - 2017-12-09 11:05:35.916425: step 18390, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 76h:21m:03s remains)
INFO - root - 2017-12-09 11:05:44.538132: step 18400, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 77h:35m:18s remains)
2017-12-09 11:05:45.438361: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012474369 0.013506431 0.014567444 0.015933989 0.017336169 0.018740766 0.019912995 0.020568322 0.021116139 0.021606715 0.022030026 0.022238927 0.022082238 0.021516642 0.020630952][0.012465458 0.013514727 0.014697945 0.016061803 0.017524589 0.01893457 0.020225462 0.021195892 0.021997524 0.022696631 0.023487302 0.024273766 0.024502339 0.024063611 0.023102438][0.010068468 0.011362771 0.012804367 0.014227817 0.01560653 0.016773595 0.017771564 0.018550176 0.019245807 0.020115389 0.021102486 0.022086892 0.022502379 0.022527823 0.021761382][0.006839578 0.0085053649 0.010191821 0.011705805 0.01311679 0.014168739 0.014984455 0.015493603 0.015979934 0.016751252 0.017729413 0.018704651 0.019350016 0.019579286 0.019193737][0.0038012404 0.0055006817 0.0072586732 0.0089227324 0.010393928 0.011458598 0.01222152 0.012693979 0.013100661 0.01357143 0.014252499 0.015013985 0.01558193 0.015804954 0.015623853][0.0021429292 0.0035207656 0.0049636159 0.0063541029 0.0076555386 0.0087313326 0.0095039215 0.0099625811 0.010286743 0.010645315 0.011167053 0.011643507 0.012027103 0.01220371 0.012161525][0.0020681859 0.0029184313 0.0037573788 0.0045806272 0.0053756777 0.0060877819 0.0066495175 0.0069957022 0.0073664719 0.007758107 0.0082997419 0.0088246325 0.0093261208 0.0096051758 0.0096723614][0.0028352092 0.0030105228 0.0030592449 0.0031414127 0.0032309082 0.0033650992 0.003510175 0.0036645939 0.0040075677 0.0045204954 0.0053353067 0.0060997866 0.0068432586 0.0072712414 0.0074641854][0.0038265043 0.0033528639 0.0026185305 0.0019260438 0.0013342721 0.00088421139 0.00056736777 0.00047293073 0.00075084041 0.0013005773 0.0021296304 0.0030212814 0.0038968942 0.0044112932 0.0045821909][0.0042974846 0.0034047377 0.0021090975 0.00089245127 -0.00015445938 -0.00097426516 -0.0015591073 -0.0018542885 -0.0017564689 -0.0014083437 -0.00080981944 -0.00011259061 0.00057830219 0.0010406503 0.0012589947][0.0041617844 0.003097641 0.0016039491 0.00025464408 -0.00088568311 -0.0018306534 -0.0025132115 -0.0028252057 -0.0028239784 -0.0027063526 -0.0024691992 -0.0021444771 -0.0017869935 -0.0015740211 -0.0014696479][0.0034524503 0.0023528358 0.00085299113 -0.000364542 -0.0013837399 -0.0022192986 -0.0028012302 -0.0030980953 -0.0031732749 -0.0031541074 -0.0030986683 -0.0030238226 -0.0029336922 -0.0028593489 -0.002810573][0.0016901649 0.00090141105 -0.00022030529 -0.0011880698 -0.0019752076 -0.0025685553 -0.0029632892 -0.0031454135 -0.0031855798 -0.0031874478 -0.0031881442 -0.0031850864 -0.0031815718 -0.0031766696 -0.0031729932][-0.00042516878 -0.0010227312 -0.0017337626 -0.0022669178 -0.0026619202 -0.0029418126 -0.0031081685 -0.0031761532 -0.0031871942 -0.0031867004 -0.0031884303 -0.0031856848 -0.0031852762 -0.0031824892 -0.0031805991][-0.00209478 -0.0024307983 -0.0027600185 -0.0029613746 -0.0030859422 -0.0031517858 -0.00317971 -0.0031888925 -0.0031887968 -0.003186811 -0.0031878366 -0.0031869086 -0.0031873137 -0.0031843325 -0.0031809083]]...]
INFO - root - 2017-12-09 11:05:54.050849: step 18410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:21m:52s remains)
INFO - root - 2017-12-09 11:06:02.458779: step 18420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:59m:59s remains)
INFO - root - 2017-12-09 11:06:10.853525: step 18430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 74h:07m:15s remains)
INFO - root - 2017-12-09 11:06:19.571060: step 18440, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 75h:53m:17s remains)
INFO - root - 2017-12-09 11:06:28.092836: step 18450, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.816 sec/batch; 71h:12m:16s remains)
INFO - root - 2017-12-09 11:06:36.648569: step 18460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:50m:45s remains)
INFO - root - 2017-12-09 11:06:45.349077: step 18470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:03m:17s remains)
INFO - root - 2017-12-09 11:06:53.913737: step 18480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:00m:26s remains)
INFO - root - 2017-12-09 11:07:02.387833: step 18490, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:26m:09s remains)
INFO - root - 2017-12-09 11:07:10.952893: step 18500, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 72h:13m:21s remains)
2017-12-09 11:07:11.757468: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54560941 0.53255057 0.52115756 0.50697225 0.49300528 0.48349869 0.47479504 0.4636564 0.4546749 0.44768414 0.44560257 0.4487204 0.45795122 0.47089127 0.482288][0.55025023 0.53990865 0.5313549 0.52049708 0.50973523 0.50112832 0.49215484 0.48219833 0.47298715 0.46569046 0.46212319 0.46340394 0.47153363 0.48322245 0.49368176][0.55097765 0.54711276 0.54285794 0.5353893 0.52709407 0.51839268 0.50881314 0.49810213 0.48782176 0.47876784 0.47312039 0.47215468 0.47820309 0.487412 0.49633402][0.55010605 0.55377585 0.55443895 0.55174655 0.54598767 0.53951919 0.52964061 0.51728952 0.50499797 0.49360356 0.48586318 0.48255664 0.4861857 0.49395829 0.50184071][0.54452521 0.55519336 0.56089938 0.56255525 0.55875683 0.55388367 0.54553 0.53328949 0.52074927 0.50807112 0.4986335 0.49316442 0.49463102 0.50102097 0.50721055][0.53570145 0.552529 0.56204784 0.5679338 0.56698424 0.56336743 0.5558241 0.54407895 0.53164762 0.51909524 0.50942254 0.50266004 0.50253934 0.50737762 0.51214683][0.52874708 0.54847735 0.55932951 0.56721884 0.568046 0.56601357 0.55910337 0.5487324 0.53701329 0.52488267 0.51573455 0.50914657 0.50896841 0.51290447 0.51671636][0.51616353 0.54071569 0.55336344 0.56302136 0.56603295 0.56488484 0.55923879 0.54872668 0.53661543 0.52465415 0.51560432 0.50965995 0.5092442 0.51294351 0.51716787][0.50497317 0.52928078 0.53936636 0.54996443 0.55487591 0.55571449 0.55238461 0.54346257 0.53289151 0.5207597 0.51181608 0.50663865 0.50696933 0.51204205 0.51767492][0.4953177 0.51792049 0.52560019 0.53319854 0.53760141 0.53837168 0.53597784 0.52938366 0.52067477 0.51130432 0.50382704 0.5004555 0.50264251 0.50949025 0.51681578][0.48598605 0.50566918 0.50975323 0.51440936 0.51762527 0.51885265 0.51722074 0.51186389 0.50490707 0.49785984 0.49265546 0.49204805 0.49708164 0.50658751 0.51691908][0.47682491 0.49480021 0.49607933 0.49729294 0.49842358 0.49868739 0.49696088 0.4924078 0.48706844 0.48251554 0.47990176 0.48262233 0.4910737 0.50373536 0.517391][0.46860123 0.48561478 0.48517463 0.48458543 0.4842034 0.48259065 0.48013908 0.47498879 0.46988279 0.466845 0.46692359 0.47317716 0.48530018 0.50131536 0.51831561][0.47062093 0.48434952 0.48090488 0.47780186 0.47495314 0.47133979 0.46761355 0.46271363 0.45818344 0.45632595 0.45885414 0.46855843 0.48431894 0.50362921 0.52312034][0.47070339 0.48442098 0.48018596 0.47512692 0.47001359 0.46473312 0.45979649 0.45408443 0.44953641 0.44879803 0.45372179 0.46606728 0.48471516 0.50670785 0.528189]]...]
INFO - root - 2017-12-09 11:07:20.302573: step 18510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:30m:45s remains)
INFO - root - 2017-12-09 11:07:29.050487: step 18520, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:12m:15s remains)
INFO - root - 2017-12-09 11:07:37.659754: step 18530, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 77h:00m:37s remains)
INFO - root - 2017-12-09 11:07:46.301716: step 18540, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 77h:36m:50s remains)
INFO - root - 2017-12-09 11:07:54.968406: step 18550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:22m:39s remains)
INFO - root - 2017-12-09 11:08:03.612751: step 18560, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:16m:54s remains)
INFO - root - 2017-12-09 11:08:12.312966: step 18570, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 75h:33m:41s remains)
INFO - root - 2017-12-09 11:08:21.002310: step 18580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 75h:04m:48s remains)
INFO - root - 2017-12-09 11:08:29.532741: step 18590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:51m:18s remains)
INFO - root - 2017-12-09 11:08:38.219224: step 18600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:17m:09s remains)
2017-12-09 11:08:39.113938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031803662 -0.0031768042 -0.0031762302 -0.0031759574 -0.0031757555 -0.003175756 -0.0031757024 -0.0031756049 -0.0031753171 -0.0031748118 -0.003174444 -0.0031751317 -0.0031775534 -0.0031815944 -0.0031870403][-0.0031785157 -0.0031747441 -0.0031740975 -0.0031737969 -0.0031736211 -0.0031736156 -0.0031737015 -0.0031737797 -0.0031737094 -0.0031735227 -0.0031733359 -0.0031738223 -0.0031756712 -0.0031788102 -0.0031829879][-0.0031791546 -0.0031753751 -0.0031747273 -0.0031744686 -0.0031743839 -0.0031744265 -0.0031746374 -0.0031748135 -0.0031749536 -0.0031749904 -0.0031749068 -0.0031751839 -0.00317637 -0.0031783662 -0.0031809143][-0.0031791611 -0.0031754198 -0.0031748642 -0.0031746929 -0.0031747511 -0.0031749979 -0.0031753897 -0.0031757972 -0.0031762191 -0.0031765043 -0.0031764484 -0.0031764272 -0.0031769453 -0.003177894 -0.0031790065][-0.0031787425 -0.0031751734 -0.0031748302 -0.0031747289 -0.0031749185 -0.0031752833 -0.0031757841 -0.0031763455 -0.0031769739 -0.0031774787 -0.0031775043 -0.0031773185 -0.0031773886 -0.0031775658 -0.0031776782][-0.0031783339 -0.0031748856 -0.0031748195 -0.0031749045 -0.0031751671 -0.0031755483 -0.0031759019 -0.003176321 -0.0031768349 -0.0031773888 -0.0031775157 -0.0031773713 -0.0031773385 -0.0031773071 -0.003177044][-0.00317794 -0.0031749972 -0.003175328 -0.0031758088 -0.0031760943 -0.0031763292 -0.00317623 -0.0031761422 -0.0031762933 -0.0031765064 -0.0031765082 -0.0031765178 -0.0031766214 -0.0031766815 -0.0031764437][-0.0031776787 -0.0031751751 -0.00317635 -0.0031777697 -0.0031786363 -0.0031789991 -0.0031784663 -0.0031776067 -0.0031766796 -0.0031759366 -0.0031751629 -0.003174881 -0.0031750838 -0.0031753548 -0.0031754775][-0.0031774687 -0.00317558 -0.0031779485 -0.0031810775 -0.0031838038 -0.0031853407 -0.0031849076 -0.00318303 -0.0031802747 -0.0031775425 -0.003175122 -0.0031738174 -0.0031735934 -0.003173905 -0.0031743015][-0.0031774295 -0.0031760244 -0.0031795867 -0.0031843486 -0.0031892143 -0.0031927857 -0.0031935535 -0.0031915761 -0.0031874408 -0.0031824214 -0.0031775816 -0.0031745564 -0.003173324 -0.0031733182 -0.003173745][-0.003177572 -0.0031767094 -0.0031814107 -0.0031873833 -0.003193751 -0.0031990847 -0.0032013031 -0.0032001159 -0.0031957894 -0.0031894075 -0.0031825253 -0.0031773541 -0.0031746454 -0.0031737667 -0.0031739476][-0.0031784207 -0.0031775362 -0.0031827337 -0.0031892359 -0.0031964555 -0.0032027604 -0.003206081 -0.0032058724 -0.0032020034 -0.0031953952 -0.0031876548 -0.0031810496 -0.0031767725 -0.0031747033 -0.0031743383][-0.0031794328 -0.0031779918 -0.0031828287 -0.0031888569 -0.0031958981 -0.0032023885 -0.0032062973 -0.0032067569 -0.0032035674 -0.0031975983 -0.0031902187 -0.0031833248 -0.0031782854 -0.0031754181 -0.0031745445][-0.0031797932 -0.0031776384 -0.003181387 -0.0031860834 -0.0031919356 -0.0031974535 -0.0032009857 -0.0032018886 -0.0031998926 -0.0031953047 -0.0031891786 -0.003183133 -0.0031784177 -0.0031754817 -0.0031743969][-0.003179305 -0.0031763967 -0.0031789502 -0.0031821539 -0.0031862538 -0.0031902292 -0.0031928769 -0.0031938283 -0.00319288 -0.0031900064 -0.0031857542 -0.0031814307 -0.0031778882 -0.0031753574 -0.0031742421]]...]
INFO - root - 2017-12-09 11:08:47.557406: step 18610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 74h:36m:07s remains)
INFO - root - 2017-12-09 11:08:56.061028: step 18620, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:08m:02s remains)
INFO - root - 2017-12-09 11:09:04.436719: step 18630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:58m:02s remains)
INFO - root - 2017-12-09 11:09:12.979139: step 18640, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 75h:28m:29s remains)
INFO - root - 2017-12-09 11:09:21.454818: step 18650, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 72h:57m:10s remains)
INFO - root - 2017-12-09 11:09:29.976460: step 18660, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 73h:10m:36s remains)
INFO - root - 2017-12-09 11:09:38.611024: step 18670, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 73h:24m:05s remains)
INFO - root - 2017-12-09 11:09:47.289020: step 18680, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 76h:50m:35s remains)
INFO - root - 2017-12-09 11:09:55.984179: step 18690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:01m:56s remains)
INFO - root - 2017-12-09 11:10:04.708847: step 18700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:33m:09s remains)
2017-12-09 11:10:05.596038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032343445 -0.0032338258 -0.0032347483 -0.0032356782 -0.0032367061 -0.0032385706 -0.0032405206 -0.0032418899 -0.0032420324 -0.0032409572 -0.0032397215 -0.0032374479 -0.0032342786 -0.0032320181 -0.0032316518][-0.0032333278 -0.003232487 -0.0032330079 -0.0032338467 -0.0032350435 -0.0032364668 -0.0032381692 -0.0032399877 -0.0032414384 -0.0032423392 -0.0032427 -0.0032418941 -0.0032402186 -0.0032385588 -0.0032384396][-0.00323473 -0.0032343967 -0.003234966 -0.0032359716 -0.003237027 -0.0032376389 -0.0032381914 -0.0032395963 -0.0032416198 -0.0032433565 -0.003244712 -0.0032450908 -0.0032445358 -0.00324366 -0.0032434997][-0.00323672 -0.0032374824 -0.0032388498 -0.0032401709 -0.0032408473 -0.0032406049 -0.0032397746 -0.003239952 -0.0032415297 -0.0032433807 -0.0032452722 -0.0032463321 -0.0032462406 -0.0032459139 -0.0032463367][-0.0032388358 -0.0032408228 -0.003243325 -0.0032450228 -0.0032450282 -0.0032436799 -0.0032414072 -0.0032399457 -0.0032400026 -0.0032411225 -0.0032425588 -0.0032436389 -0.00324387 -0.0032438722 -0.0032447483][-0.0032401215 -0.0032427979 -0.003246231 -0.0032481332 -0.0032476387 -0.0032453882 -0.0032419972 -0.0032388952 -0.0032371683 -0.0032370819 -0.0032376994 -0.0032381064 -0.0032381536 -0.0032380605 -0.0032389837][-0.0032403616 -0.0032431062 -0.0032468755 -0.0032487374 -0.003248092 -0.0032450857 -0.0032408081 -0.0032362582 -0.0032330791 -0.0032318463 -0.0032316002 -0.0032315992 -0.0032317163 -0.0032316486 -0.0032324926][-0.0032392456 -0.0032419078 -0.0032454673 -0.0032473246 -0.0032467286 -0.003243342 -0.0032383949 -0.0032332016 -0.0032293077 -0.0032271552 -0.0032262697 -0.0032259894 -0.003226334 -0.0032268357 -0.003228015][-0.0032371345 -0.0032391744 -0.00324243 -0.0032438238 -0.0032432317 -0.003239786 -0.0032349629 -0.0032298812 -0.0032259952 -0.0032237023 -0.003222754 -0.0032225908 -0.0032232094 -0.0032241133 -0.0032252464][-0.0032343324 -0.0032355934 -0.0032381308 -0.0032388924 -0.0032381366 -0.0032351781 -0.00323094 -0.0032265286 -0.0032232585 -0.0032214154 -0.0032208741 -0.0032209717 -0.0032216655 -0.0032226897 -0.0032235833][-0.0032316 -0.0032317471 -0.0032336016 -0.0032339266 -0.0032329869 -0.0032305422 -0.0032272656 -0.0032238015 -0.0032213784 -0.0032200573 -0.0032198862 -0.003220289 -0.0032209605 -0.0032217661 -0.0032223742][-0.0032299233 -0.0032290688 -0.0032301007 -0.0032301797 -0.0032292518 -0.0032271559 -0.0032246849 -0.003222171 -0.0032204851 -0.0032196022 -0.0032196641 -0.0032201945 -0.0032208895 -0.0032215882 -0.0032219859][-0.0032293706 -0.0032279703 -0.0032282579 -0.0032279934 -0.0032271119 -0.0032255186 -0.0032235673 -0.0032218536 -0.0032207423 -0.0032202005 -0.00322029 -0.0032206951 -0.0032210904 -0.0032214243 -0.0032214569][-0.0032293471 -0.0032276297 -0.0032275319 -0.0032269743 -0.00322622 -0.0032249978 -0.0032235922 -0.0032223978 -0.0032216238 -0.0032213246 -0.0032213386 -0.0032215891 -0.0032217579 -0.0032218597 -0.0032217591][-0.0032289962 -0.0032273522 -0.003227358 -0.0032269021 -0.0032262832 -0.0032252267 -0.0032240818 -0.003223201 -0.003222577 -0.0032223654 -0.0032223468 -0.0032224513 -0.0032225233 -0.0032225109 -0.0032223654]]...]
INFO - root - 2017-12-09 11:10:14.132599: step 18710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:04m:01s remains)
INFO - root - 2017-12-09 11:10:22.641200: step 18720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:22m:00s remains)
INFO - root - 2017-12-09 11:10:31.118091: step 18730, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 70h:08m:38s remains)
INFO - root - 2017-12-09 11:10:39.768081: step 18740, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 74h:00m:01s remains)
INFO - root - 2017-12-09 11:10:48.670244: step 18750, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:13m:33s remains)
INFO - root - 2017-12-09 11:10:57.350386: step 18760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-09 11:11:06.175367: step 18770, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 77h:25m:17s remains)
INFO - root - 2017-12-09 11:11:14.800476: step 18780, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 76h:48m:04s remains)
INFO - root - 2017-12-09 11:11:23.405013: step 18790, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 72h:56m:39s remains)
INFO - root - 2017-12-09 11:11:32.068102: step 18800, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 77h:20m:42s remains)
2017-12-09 11:11:32.995141: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49886078 0.49867529 0.49646693 0.49356034 0.49163041 0.49038914 0.48829588 0.48464125 0.47845393 0.47159702 0.46317884 0.45477286 0.44741663 0.44086379 0.43505272][0.50822622 0.50945282 0.50835681 0.50655895 0.5054056 0.50398248 0.50145596 0.49644926 0.48888889 0.47868392 0.4669039 0.45435965 0.44309905 0.43481836 0.42829123][0.50543159 0.50810522 0.50826067 0.50761288 0.50774395 0.50620133 0.50271875 0.49538597 0.48480827 0.47116402 0.45492661 0.43870169 0.42465284 0.4146103 0.40769488][0.501426 0.5059697 0.50764376 0.50776237 0.50818551 0.50639004 0.50273228 0.49396744 0.48136565 0.46489102 0.4458636 0.42652956 0.40905359 0.39597991 0.38779572][0.49201232 0.49888444 0.5013029 0.50270051 0.50426883 0.50258982 0.49880272 0.4902004 0.4776006 0.45973009 0.4389711 0.41730237 0.39747408 0.38132057 0.37104255][0.47225374 0.48106769 0.48425353 0.48691881 0.48965168 0.48967198 0.48743844 0.47955576 0.46752387 0.45050517 0.43029848 0.40735596 0.38573611 0.36762854 0.3550097][0.44017121 0.45097458 0.45436358 0.4584429 0.46303469 0.46508023 0.46482852 0.45945802 0.45008841 0.43480533 0.41576564 0.39379215 0.37245235 0.35317361 0.33812711][0.39731216 0.40928918 0.41306594 0.41896445 0.42591009 0.43085685 0.43354163 0.43129885 0.4252876 0.41345757 0.39754397 0.37703595 0.35603791 0.33648953 0.31945997][0.34515524 0.35754535 0.3622438 0.37072089 0.38079318 0.38890508 0.39457378 0.3960301 0.39398339 0.38578242 0.37274075 0.35428163 0.33406374 0.31360897 0.29405823][0.2886655 0.30002826 0.30537164 0.31630537 0.32958645 0.34098807 0.34968206 0.35425356 0.35538003 0.35056651 0.34018973 0.32420865 0.30534396 0.2851896 0.26400906][0.23425753 0.24298587 0.24778762 0.25981691 0.2750535 0.28866065 0.299444 0.30631715 0.30981073 0.30725423 0.298624 0.2844657 0.2666128 0.24670365 0.22424577][0.18643887 0.19159125 0.194769 0.20575523 0.22063619 0.23432551 0.24565829 0.25365654 0.25848836 0.25752178 0.25069013 0.23907258 0.22310764 0.20409808 0.18153392][0.14646521 0.14751256 0.1481832 0.15684006 0.16931331 0.18105143 0.19115806 0.19899791 0.20440455 0.20479603 0.19999009 0.19081567 0.17713004 0.15966484 0.13812357][0.11995249 0.11633557 0.11307946 0.11728965 0.12521091 0.13308249 0.14012644 0.14626704 0.1509676 0.15202785 0.14900778 0.14240959 0.13151352 0.11676871 0.097884752][0.1076401 0.10039777 0.092877433 0.091554493 0.093613446 0.096333578 0.099208944 0.10265564 0.10589286 0.10707778 0.10542949 0.10104154 0.0929128 0.081285343 0.066023715]]...]
INFO - root - 2017-12-09 11:11:41.659723: step 18810, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 75h:07m:26s remains)
INFO - root - 2017-12-09 11:11:50.150837: step 18820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 74h:48m:53s remains)
INFO - root - 2017-12-09 11:11:58.786293: step 18830, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 66h:46m:36s remains)
INFO - root - 2017-12-09 11:12:07.488138: step 18840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:30m:15s remains)
INFO - root - 2017-12-09 11:12:16.128756: step 18850, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:14m:00s remains)
INFO - root - 2017-12-09 11:12:24.804648: step 18860, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:14m:04s remains)
INFO - root - 2017-12-09 11:12:33.539000: step 18870, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 76h:09m:14s remains)
INFO - root - 2017-12-09 11:12:42.177997: step 18880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:44m:51s remains)
INFO - root - 2017-12-09 11:12:50.751718: step 18890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:47m:31s remains)
INFO - root - 2017-12-09 11:12:59.260126: step 18900, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:15m:32s remains)
2017-12-09 11:13:00.118271: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51380587 0.49409777 0.47161993 0.45190895 0.43405515 0.42134139 0.41178754 0.40276217 0.39173415 0.37776688 0.3590607 0.33641875 0.30983672 0.28374386 0.25904447][0.58061481 0.56492132 0.54494 0.52800143 0.51204455 0.50022191 0.48936856 0.47687206 0.4603416 0.43860883 0.41012558 0.37568077 0.33818707 0.30226907 0.2699945][0.6287148 0.619925 0.60554057 0.59396756 0.58276832 0.57373577 0.56217873 0.54560804 0.52069074 0.48783621 0.44684035 0.400077 0.35156405 0.30719849 0.26930991][0.66038162 0.6613608 0.65495044 0.65008223 0.64447081 0.63905418 0.62721 0.60525382 0.57099491 0.52551687 0.47021559 0.40970376 0.34973624 0.29807368 0.25695589][0.67230123 0.68397504 0.68599272 0.6880368 0.68824434 0.68623728 0.67416185 0.64728224 0.60404366 0.54705924 0.47909752 0.40632474 0.33672062 0.27925411 0.23689632][0.67203063 0.69005269 0.69711649 0.70401996 0.70872384 0.70888829 0.69659466 0.66636521 0.61627918 0.55016404 0.47189763 0.38932174 0.31220189 0.2513423 0.20997137][0.66684586 0.68899328 0.69704413 0.7047317 0.71096504 0.71201181 0.69926184 0.66643196 0.6117397 0.53971648 0.45449936 0.36590677 0.28464285 0.22262049 0.18387628][0.65557218 0.680795 0.688712 0.69480693 0.69959223 0.69865757 0.6835475 0.6485002 0.59088022 0.51540434 0.42749411 0.3377001 0.25668344 0.19658953 0.16201431][0.63688719 0.66470033 0.67331648 0.67853612 0.6808188 0.67684138 0.659202 0.62200069 0.56300884 0.48728848 0.40053883 0.31303421 0.23536557 0.179065 0.14884326][0.60692686 0.636281 0.64559388 0.65050596 0.65178257 0.64544076 0.62543452 0.58694834 0.52824706 0.45529139 0.373473 0.29276273 0.22255394 0.17242502 0.14702328][0.56764621 0.59701318 0.60630208 0.61034787 0.60992438 0.60163575 0.58108258 0.54347408 0.48859507 0.42212588 0.34898832 0.27840617 0.21768761 0.17572905 0.15497036][0.51585817 0.54544646 0.55511361 0.55921054 0.55825979 0.54938632 0.52937591 0.49470121 0.4464525 0.38917878 0.32789519 0.27009109 0.22124586 0.18815313 0.17169972][0.45656541 0.48531669 0.49600518 0.50020963 0.4987984 0.48969549 0.47085509 0.43999112 0.39933372 0.35321191 0.30543905 0.26178756 0.22564194 0.20162405 0.18926178][0.39255258 0.4194653 0.43116668 0.43671176 0.43671241 0.42897525 0.41313908 0.38737792 0.3550767 0.32009307 0.28511703 0.25450981 0.22967739 0.21372426 0.20505832][0.332148 0.35535887 0.36564824 0.37159067 0.37256771 0.36705273 0.35489833 0.33532083 0.31199217 0.28736472 0.26408395 0.24451412 0.22938825 0.219717 0.2138598]]...]
INFO - root - 2017-12-09 11:13:08.599852: step 18910, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 78h:23m:21s remains)
INFO - root - 2017-12-09 11:13:16.988156: step 18920, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 66h:19m:56s remains)
INFO - root - 2017-12-09 11:13:25.502398: step 18930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:09m:45s remains)
INFO - root - 2017-12-09 11:13:33.930232: step 18940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:26m:27s remains)
INFO - root - 2017-12-09 11:13:42.595738: step 18950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:04m:06s remains)
INFO - root - 2017-12-09 11:13:51.280457: step 18960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 73h:53m:59s remains)
INFO - root - 2017-12-09 11:13:59.997654: step 18970, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 73h:57m:35s remains)
INFO - root - 2017-12-09 11:14:08.611291: step 18980, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 73h:29m:34s remains)
INFO - root - 2017-12-09 11:14:17.245845: step 18990, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 74h:17m:52s remains)
INFO - root - 2017-12-09 11:14:25.852189: step 19000, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 76h:14m:31s remains)
2017-12-09 11:14:26.761697: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038841829 0.055196237 0.072695307 0.087978482 0.10126296 0.10879461 0.10773143 0.097822674 0.083465829 0.067292549 0.048798203 0.030265685 0.01517436 0.0051295366 -0.0002075152][0.060795009 0.087249666 0.11690225 0.14532745 0.1694462 0.18518762 0.18807262 0.17605935 0.1535591 0.12418251 0.091026545 0.058201525 0.031157929 0.01257177 0.0021547151][0.08823362 0.12864654 0.17519762 0.22224553 0.26297325 0.29071292 0.29908004 0.28449914 0.25161675 0.20543097 0.15266649 0.10052389 0.05684128 0.025850799 0.0075091161][0.12214755 0.17931128 0.24634235 0.31547177 0.37564602 0.41770077 0.43294498 0.41531092 0.37005693 0.30369386 0.22762397 0.15247759 0.088966139 0.043526717 0.015289254][0.16319062 0.23721643 0.32399219 0.41429448 0.49398381 0.55090988 0.57382178 0.55526751 0.49926069 0.41302416 0.31251472 0.21259198 0.12739775 0.065024927 0.025039222][0.21127546 0.3002322 0.40209278 0.50690329 0.5991255 0.66533595 0.69362456 0.67440373 0.61065531 0.50965285 0.38967362 0.26897386 0.16453795 0.086874887 0.035602532][0.259276 0.35908896 0.47016433 0.58130205 0.67722952 0.74503446 0.77305859 0.75183314 0.68341321 0.57428479 0.44256118 0.3085247 0.19124757 0.10298306 0.043562125][0.30478054 0.40920848 0.52046609 0.628194 0.71946669 0.78248245 0.80673552 0.78269106 0.71187317 0.60015762 0.46447814 0.32565656 0.20309216 0.11013632 0.047118034][0.34289467 0.44629753 0.55112356 0.64891249 0.72930163 0.78300172 0.80111134 0.77447009 0.70345068 0.59314209 0.45930946 0.32218668 0.2007461 0.10837778 0.046089258][0.37176219 0.46919274 0.56269932 0.6468513 0.71435678 0.7573384 0.76821607 0.738732 0.66839 0.56174648 0.43325803 0.30240312 0.18706308 0.099424995 0.041342489][0.38882121 0.47707316 0.55696023 0.62669528 0.68104762 0.7135092 0.71770012 0.68540519 0.61607486 0.51404881 0.39293763 0.2710664 0.16467582 0.0849589 0.033671852][0.385213 0.46249014 0.52936906 0.586142 0.62945354 0.65356559 0.65235847 0.6183545 0.55100483 0.45463091 0.34264114 0.23196189 0.13706075 0.067395017 0.024519531][0.363479 0.42706031 0.47961408 0.52404809 0.55786538 0.57566333 0.57163894 0.53872705 0.47591472 0.38785234 0.28756821 0.1902502 0.10849761 0.050103355 0.016112639][0.32649609 0.37570193 0.41439438 0.44685474 0.47166958 0.48409522 0.47821879 0.44790345 0.39215684 0.31576496 0.23015071 0.14838839 0.081289493 0.034891158 0.0094470512][0.27465603 0.30994257 0.33608574 0.35776573 0.37461463 0.3825314 0.37605026 0.34985545 0.30338749 0.24116361 0.17267367 0.10831182 0.056698311 0.022271568 0.0044964664]]...]
INFO - root - 2017-12-09 11:14:35.217703: step 19010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:58m:21s remains)
INFO - root - 2017-12-09 11:14:43.701218: step 19020, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 58h:57m:03s remains)
INFO - root - 2017-12-09 11:14:52.358134: step 19030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:30m:28s remains)
INFO - root - 2017-12-09 11:15:00.839556: step 19040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:22m:29s remains)
INFO - root - 2017-12-09 11:15:09.561548: step 19050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:55m:39s remains)
INFO - root - 2017-12-09 11:15:18.181308: step 19060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:34m:59s remains)
INFO - root - 2017-12-09 11:15:26.801428: step 19070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:57m:34s remains)
INFO - root - 2017-12-09 11:15:35.454571: step 19080, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:12m:10s remains)
INFO - root - 2017-12-09 11:15:44.072638: step 19090, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 73h:17m:30s remains)
INFO - root - 2017-12-09 11:15:52.748002: step 19100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:32m:24s remains)
2017-12-09 11:15:53.595311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032577838 -0.0032507116 -0.0031991971 -0.0029771361 -0.0024015412 -0.0013892434 -0.00015113922 0.00076960679 0.00074542454 -0.00021331897 -0.001465653 -0.0024206648 -0.0030006347 -0.0032371259 -0.0032564949][-0.003256463 -0.0032389022 -0.0031316185 -0.0027388225 -0.0018154948 -0.00029530074 0.001459233 0.0026450646 0.0024919743 0.0010787197 -0.00072013843 -0.0020515332 -0.0028511279 -0.003200046 -0.00325421][-0.0032478368 -0.0031831786 -0.0029172604 -0.0021511689 -0.00056803995 0.0018545752 0.0045566834 0.0063789873 0.006274736 0.0042681731 0.0014692084 -0.00084195239 -0.0023640459 -0.003069113 -0.0032428396][-0.0031981308 -0.0029717262 -0.002260098 -0.000603565 0.002334103 0.006350134 0.010547524 0.013358742 0.013406072 0.010541724 0.0060572843 0.0018483577 -0.0012055773 -0.0027288343 -0.0032003552][-0.0029275734 -0.0022676927 -0.00054221507 0.002871376 0.0081238132 0.014466226 0.020478271 0.024228867 0.024129182 0.019897642 0.013028858 0.0061005778 0.00075122807 -0.0021038055 -0.0031082639][-0.00217269 -0.00061237765 0.0029701497 0.0092264991 0.017795891 0.02704997 0.034949031 0.039322216 0.038530741 0.03222004 0.022117753 0.011612125 0.0033112131 -0.0012727298 -0.0029746194][-0.00075439108 0.0022240998 0.008414112 0.018188942 0.030283349 0.042010926 0.050839849 0.054778233 0.052525874 0.043823376 0.030537056 0.016661324 0.0056701726 -0.00049733114 -0.0028423211][0.0012076898 0.0058412678 0.014734361 0.027689302 0.042377044 0.05520751 0.063471682 0.065791614 0.061514392 0.050665256 0.035181195 0.019280823 0.0068352832 -0.00013139099 -0.0027764803][0.0031393012 0.0090833791 0.019755078 0.034348842 0.049746707 0.0619433 0.0683777 0.068434529 0.062165573 0.050024927 0.034026485 0.018183531 0.0061666653 -0.0003915343 -0.002818685][0.0042615347 0.010643227 0.021451406 0.035504017 0.049495786 0.059608661 0.063650608 0.061609685 0.054128867 0.042111356 0.027564444 0.013892591 0.0039926171 -0.0011524567 -0.0029486041][0.0037670764 0.00953839 0.018876759 0.03058034 0.041726161 0.049073592 0.050864607 0.047466625 0.039946556 0.029547783 0.018086839 0.0080493465 0.0012348755 -0.0020531146 -0.0030969521][0.002101454 0.0064697717 0.013301551 0.0216459 0.029291132 0.03380898 0.033962756 0.030278316 0.023965286 0.016323695 0.0087366067 0.0026708506 -0.0011059227 -0.0027412563 -0.0031961808][3.9882725e-05 0.0027351161 0.0068503558 0.011797681 0.016172746 0.01843263 0.017836912 0.014866317 0.010556286 0.0059518036 0.0019040247 -0.00096656475 -0.0025454424 -0.0031207772 -0.0032469225][-0.0017809229 -0.00044713635 0.0015329183 0.0038578385 0.0058094072 0.0066187051 0.005954877 0.0041679572 0.0019378373 -0.00013500452 -0.0017107773 -0.0026683819 -0.0031155474 -0.0032410119 -0.0032579431][-0.0028689643 -0.0023784183 -0.0016501405 -0.00083238888 -0.00020286371 -3.7148828e-05 -0.0004195082 -0.001155898 -0.0019461542 -0.0025750133 -0.0029751814 -0.003173223 -0.0032464266 -0.003258527 -0.0032582451]]...]
INFO - root - 2017-12-09 11:16:02.121704: step 19110, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 73h:51m:19s remains)
INFO - root - 2017-12-09 11:16:10.652313: step 19120, loss = 0.89, batch loss = 0.68 (11.3 examples/sec; 0.711 sec/batch; 61h:54m:06s remains)
INFO - root - 2017-12-09 11:16:19.272420: step 19130, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 78h:58m:40s remains)
INFO - root - 2017-12-09 11:16:27.482495: step 19140, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 73h:03m:12s remains)
INFO - root - 2017-12-09 11:16:36.099856: step 19150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:17m:53s remains)
INFO - root - 2017-12-09 11:16:44.541073: step 19160, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 72h:14m:48s remains)
INFO - root - 2017-12-09 11:16:53.085468: step 19170, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.916 sec/batch; 79h:46m:06s remains)
INFO - root - 2017-12-09 11:17:01.830266: step 19180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:40m:58s remains)
INFO - root - 2017-12-09 11:17:10.388734: step 19190, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:36m:40s remains)
INFO - root - 2017-12-09 11:17:19.105662: step 19200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 75h:39m:45s remains)
2017-12-09 11:17:19.986741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00078598526 0.00055293553 0.0024666106 0.0045066806 0.00607276 0.0067265281 0.0064349496 0.0055394717 0.004371393 0.0033019884 0.0024146438 0.0015083989 0.00048854854 -0.00059979688 -0.0016264237][0.0020324094 0.0039257705 0.0064301691 0.0089353416 0.010722421 0.011341443 0.010785647 0.0095124748 0.0079751723 0.0065335338 0.0052572577 0.0039488208 0.0024302336 0.00077717239 -0.0007873287][0.0076524322 0.011158153 0.015165353 0.018725866 0.0209346 0.021407707 0.020305444 0.018321175 0.016131157 0.014091466 0.012149952 0.010030482 0.0074469368 0.0044834088 0.0015801135][0.016502529 0.023062984 0.029899685 0.035539661 0.038812913 0.039447114 0.037814163 0.034944247 0.031745378 0.02860113 0.025321282 0.021545371 0.016929258 0.011576461 0.0062648584][0.027594136 0.038411606 0.049233191 0.05798896 0.0631883 0.064665325 0.062890254 0.059119321 0.054459956 0.049520835 0.0440832 0.037727132 0.030176332 0.021576744 0.01307375][0.038622752 0.05411005 0.069400184 0.081887573 0.0897708 0.092917249 0.091775492 0.087561056 0.081361584 0.074103162 0.065839931 0.05623107 0.045187522 0.032910727 0.02091556][0.046501495 0.065743938 0.084847637 0.100761 0.11142901 0.11667605 0.11683875 0.11284958 0.10553852 0.09611515 0.085042991 0.072290614 0.058006771 0.042541794 0.027661711][0.04942679 0.070448644 0.091620356 0.10969807 0.12244727 0.12956579 0.1312575 0.12805955 0.12045706 0.10972789 0.096684583 0.081662714 0.065067209 0.047534551 0.031035209][0.046773568 0.067216441 0.0881867 0.10655776 0.12007539 0.12824786 0.13117379 0.12906191 0.12208012 0.11132191 0.097703084 0.0818548 0.064400524 0.04639991 0.02991947][0.039339684 0.057197839 0.075908169 0.092693307 0.10539725 0.11341465 0.11684829 0.11570833 0.10996053 0.10036453 0.087732308 0.07280349 0.056306988 0.039630957 0.024791162][0.028945321 0.04284098 0.05776497 0.071502 0.082130454 0.088935517 0.0920504 0.09150219 0.087215878 0.079580717 0.069201015 0.056796961 0.043036882 0.02933746 0.017430592][0.01793281 0.027418943 0.03785231 0.047697291 0.055471092 0.06046848 0.062780961 0.062449716 0.059482031 0.054085519 0.046649896 0.037736915 0.027838981 0.018106073 0.0097983517][0.0084920432 0.014086828 0.0203568 0.026407685 0.031281058 0.034401242 0.035795003 0.035517007 0.033627626 0.030245138 0.025637342 0.020186152 0.014190042 0.008365389 0.0034970059][0.0018503405 0.0045904214 0.0077341353 0.010840088 0.013390617 0.015010832 0.015677866 0.015438279 0.014372928 0.01256172 0.010159705 0.0073941643 0.0044416068 0.0016500554 -0.00060058082][-0.0017258368 -0.00069731777 0.00053779129 0.0018037762 0.002875349 0.0035655117 0.0038298098 0.0036877012 0.0031913351 0.0024008094 0.0014029271 0.00031145313 -0.00079220324 -0.0017888024 -0.0025414433]]...]
INFO - root - 2017-12-09 11:17:28.374462: step 19210, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 77h:02m:45s remains)
INFO - root - 2017-12-09 11:17:36.929503: step 19220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 74h:40m:49s remains)
INFO - root - 2017-12-09 11:17:45.308164: step 19230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:04m:47s remains)
INFO - root - 2017-12-09 11:17:53.739655: step 19240, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 75h:48m:58s remains)
INFO - root - 2017-12-09 11:18:02.314977: step 19250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 75h:34m:25s remains)
INFO - root - 2017-12-09 11:18:10.911084: step 19260, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 78h:14m:38s remains)
INFO - root - 2017-12-09 11:18:19.498243: step 19270, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.821 sec/batch; 71h:26m:53s remains)
INFO - root - 2017-12-09 11:18:28.210680: step 19280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:48m:31s remains)
INFO - root - 2017-12-09 11:18:37.076309: step 19290, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.938 sec/batch; 81h:36m:05s remains)
INFO - root - 2017-12-09 11:18:45.841902: step 19300, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 78h:39m:56s remains)
2017-12-09 11:18:46.690136: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21209915 0.21097556 0.20812592 0.20194221 0.19425528 0.18384235 0.17392789 0.16549484 0.15748818 0.14974166 0.14092359 0.13160275 0.12019043 0.10782135 0.096477754][0.19303842 0.19377637 0.1927143 0.18828791 0.18224418 0.17332608 0.16481186 0.15703134 0.14952111 0.14225644 0.13419665 0.12577325 0.11540265 0.10425707 0.093876429][0.17791507 0.18110418 0.1813747 0.17820688 0.17369764 0.16586454 0.15862106 0.15177158 0.14529067 0.13826706 0.13019431 0.12241203 0.11280592 0.10267808 0.09284281][0.166268 0.17212954 0.17462936 0.1736417 0.17104752 0.16563664 0.16009919 0.15391369 0.14798963 0.14110981 0.13294834 0.12418912 0.11387148 0.10363721 0.093715586][0.15624313 0.16390041 0.16833682 0.17004365 0.16988012 0.16686808 0.16339074 0.15875739 0.15387642 0.14691547 0.13848245 0.12899768 0.11814327 0.10720994 0.096394919][0.15217951 0.16157623 0.16717899 0.17045893 0.17188476 0.17101987 0.16974039 0.16647132 0.16225122 0.15544522 0.1464069 0.13555227 0.1231705 0.11123025 0.099672891][0.15856552 0.16895485 0.1749931 0.178857 0.18100064 0.18123648 0.18111381 0.17896655 0.17500849 0.16793318 0.15779136 0.14504004 0.13017844 0.11583069 0.10263915][0.17510626 0.18550619 0.1905254 0.19385491 0.1956193 0.19624285 0.19591819 0.19374608 0.189441 0.18187982 0.17066306 0.15587491 0.13864134 0.12171347 0.10631335][0.1936481 0.20475054 0.20935717 0.21222977 0.21360077 0.21360354 0.21245432 0.20961834 0.20428592 0.19580041 0.18327154 0.16687548 0.14764655 0.12849741 0.11095928][0.20374097 0.21540989 0.22041568 0.2235643 0.22542517 0.22571643 0.22418286 0.22029571 0.21392024 0.2051096 0.19210911 0.17509402 0.15489778 0.13441762 0.11530226][0.20500067 0.21592346 0.220416 0.22444929 0.22733036 0.22845738 0.22751148 0.22388449 0.21733411 0.20830002 0.19552249 0.17892829 0.15876229 0.13796496 0.11816324][0.20137854 0.21128152 0.21491355 0.21845227 0.22131951 0.22303723 0.22302732 0.22020473 0.21465416 0.20653851 0.19470511 0.17890821 0.1592402 0.13882267 0.1190253][0.19473082 0.20368639 0.20650913 0.20958695 0.21194389 0.21357574 0.21389517 0.21210223 0.20788656 0.20087306 0.19021851 0.1755762 0.15695508 0.1372501 0.11798644][0.18613614 0.19398709 0.19590299 0.19834769 0.2003656 0.20180771 0.2022378 0.20094348 0.19758181 0.19154486 0.18220063 0.16892724 0.15180959 0.13345397 0.11545957][0.17583971 0.18286377 0.18402234 0.18562719 0.1869835 0.18802808 0.18832216 0.18717264 0.18427104 0.17895959 0.17063862 0.15870737 0.14333916 0.12684576 0.11077174]]...]
INFO - root - 2017-12-09 11:18:55.185155: step 19310, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:12m:18s remains)
INFO - root - 2017-12-09 11:19:03.911506: step 19320, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:51m:03s remains)
INFO - root - 2017-12-09 11:19:12.480765: step 19330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:38m:19s remains)
INFO - root - 2017-12-09 11:19:20.893815: step 19340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:28m:45s remains)
INFO - root - 2017-12-09 11:19:29.463797: step 19350, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 74h:13m:38s remains)
INFO - root - 2017-12-09 11:19:37.805523: step 19360, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 74h:19m:14s remains)
INFO - root - 2017-12-09 11:19:46.409436: step 19370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 74h:33m:05s remains)
INFO - root - 2017-12-09 11:19:55.220989: step 19380, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.913 sec/batch; 79h:23m:22s remains)
INFO - root - 2017-12-09 11:20:03.896419: step 19390, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 74h:38m:43s remains)
INFO - root - 2017-12-09 11:20:12.641005: step 19400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 75h:15m:50s remains)
2017-12-09 11:20:13.456595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020568988 -0.0020466766 -0.0013381646 0.00016388926 0.002402063 0.0051982412 0.0079415431 0.010023533 0.010998164 0.010605221 0.0091111427 0.0068821851 0.0045602322 0.002357401 0.00037475466][-0.0002735341 -0.00044182804 0.00035017356 0.0023235283 0.0054684198 0.0094994418 0.01365824 0.016933151 0.018738398 0.018540857 0.016579527 0.013292177 0.0096080322 0.006003988 0.0027867039][0.00307428 0.0032759849 0.0049021272 0.0081630666 0.012971386 0.018906824 0.025004294 0.029933687 0.032891992 0.033074688 0.030609004 0.025845379 0.019981768 0.013890039 0.00825924][0.0077438438 0.0092069209 0.012815807 0.018578807 0.02613957 0.03468449 0.043005571 0.049597558 0.05367998 0.054177724 0.050925117 0.044202328 0.0354397 0.025983902 0.016975699][0.013417874 0.017146897 0.023929281 0.03335496 0.044684671 0.056662418 0.067816943 0.076324068 0.081350826 0.081766114 0.077121809 0.067573793 0.0548035 0.040884729 0.027575744][0.019249827 0.026376456 0.037532192 0.051675353 0.067491114 0.083183028 0.097101316 0.10732704 0.113029 0.11308197 0.10675992 0.094155304 0.077025406 0.058074251 0.039779186][0.024262143 0.035407603 0.051528003 0.070698716 0.090867691 0.10965209 0.12532663 0.13614288 0.14152423 0.14059696 0.13253766 0.11730219 0.09646745 0.073197551 0.050506495][0.028378502 0.043097135 0.063362293 0.086431123 0.10945962 0.12962458 0.14545013 0.15549111 0.15954928 0.15715487 0.14764902 0.13094877 0.10811817 0.0823401 0.056858569][0.031123221 0.04839452 0.071121015 0.096056536 0.11977828 0.13921884 0.1533854 0.16138004 0.16361122 0.15987113 0.14969021 0.13287805 0.10977849 0.0835182 0.057294331][0.032584988 0.050828055 0.073799409 0.098124124 0.12016429 0.1370046 0.14834857 0.15375152 0.15413922 0.1495768 0.1398029 0.12429668 0.10259303 0.077569149 0.052338179][0.03172629 0.049451936 0.070825942 0.092656016 0.11138978 0.12441297 0.13210605 0.13467148 0.13353965 0.12876011 0.12017526 0.10705448 0.088249587 0.066167787 0.04365021][0.027906911 0.043383781 0.061404221 0.079226434 0.09369801 0.10271051 0.10704013 0.10723089 0.10498865 0.10049293 0.093680434 0.083598159 0.068783566 0.051084306 0.032796673][0.021029804 0.033102285 0.0466993 0.059727095 0.069752991 0.075271241 0.077066757 0.07583224 0.073171549 0.069326878 0.064313874 0.057291083 0.046900678 0.034340777 0.021210792][0.012569005 0.020772599 0.029826097 0.038212247 0.044281133 0.047161084 0.047433697 0.045792613 0.043456215 0.040685132 0.037479814 0.03312704 0.026726298 0.018967785 0.010831954][0.0045394981 0.0090981172 0.014136188 0.018741446 0.021942798 0.023246253 0.022980247 0.021691866 0.020145629 0.018572837 0.01695513 0.014795084 0.011580378 0.0075894753 0.0033781584]]...]
INFO - root - 2017-12-09 11:20:22.205149: step 19410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:53m:04s remains)
INFO - root - 2017-12-09 11:20:31.012978: step 19420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:37m:44s remains)
INFO - root - 2017-12-09 11:20:39.572918: step 19430, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 77h:00m:45s remains)
INFO - root - 2017-12-09 11:20:48.245046: step 19440, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 76h:08m:20s remains)
INFO - root - 2017-12-09 11:20:57.005568: step 19450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:23m:57s remains)
INFO - root - 2017-12-09 11:21:05.704716: step 19460, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:52m:02s remains)
INFO - root - 2017-12-09 11:21:14.363973: step 19470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:15m:09s remains)
INFO - root - 2017-12-09 11:21:22.968662: step 19480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 76h:41m:43s remains)
INFO - root - 2017-12-09 11:21:31.466858: step 19490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:19m:31s remains)
INFO - root - 2017-12-09 11:21:39.976459: step 19500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 73h:00m:17s remains)
2017-12-09 11:21:40.817266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003285486 -0.0032832089 -0.0032829526 -0.00328279 -0.0032825849 -0.0032824154 -0.0032822145 -0.0032820187 -0.0032818743 -0.0032817519 -0.0032816445 -0.0032815656 -0.0032815093 -0.0032814466 -0.0032815144][-0.0032836425 -0.0032811477 -0.0032808082 -0.0032806604 -0.00328046 -0.0032802762 -0.0032800809 -0.0032799267 -0.0032797882 -0.0032796226 -0.0032794683 -0.0032793272 -0.003279228 -0.0032791717 -0.0032791963][-0.003284358 -0.0032819035 -0.0032817048 -0.0032817547 -0.0032816501 -0.0032813244 -0.003280954 -0.0032805963 -0.0032802429 -0.0032799123 -0.0032796417 -0.003279496 -0.0032794292 -0.0032793898 -0.003279373][-0.0032850855 -0.0032828485 -0.0032829938 -0.0032834008 -0.0032835333 -0.0032832937 -0.003282829 -0.003282147 -0.0032812681 -0.0032804266 -0.0032798005 -0.0032794247 -0.0032792671 -0.0032792112 -0.0032792042][-0.0032870607 -0.0032854418 -0.0032861661 -0.0032871619 -0.0032876942 -0.00328754 -0.0032867619 -0.0032854455 -0.003283768 -0.0032820322 -0.0032805838 -0.0032796047 -0.0032791072 -0.0032789328 -0.0032789405][-0.0032908712 -0.0032901461 -0.0032916644 -0.0032931124 -0.0032935771 -0.0032930549 -0.0032917468 -0.0032897338 -0.0032871533 -0.0032844439 -0.0032819936 -0.003280171 -0.0032791328 -0.0032787339 -0.0032787018][-0.0032957427 -0.0032955487 -0.0032972957 -0.0032986435 -0.0032988822 -0.0032980733 -0.0032963706 -0.0032937131 -0.0032903538 -0.0032866772 -0.0032833582 -0.0032808424 -0.003279258 -0.0032786166 -0.0032785318][-0.0033003406 -0.0033007853 -0.0033030505 -0.0033048175 -0.0033054368 -0.0033046594 -0.0033024664 -0.0032986721 -0.0032940968 -0.0032893904 -0.0032851999 -0.0032819982 -0.0032797665 -0.0032786818 -0.0032784047][-0.0033039041 -0.0033051807 -0.0033081893 -0.003310638 -0.0033118164 -0.0033110683 -0.0033080715 -0.0033032442 -0.0032976607 -0.003292151 -0.0032872867 -0.0032833603 -0.0032804755 -0.00327887 -0.0032783083][-0.0033068473 -0.0033084846 -0.0033115691 -0.0033138127 -0.0033148257 -0.0033139605 -0.0033110112 -0.0033061444 -0.003300196 -0.0032943315 -0.0032889575 -0.0032845689 -0.0032812683 -0.0032792862 -0.0032784352][-0.0033082142 -0.0033091013 -0.003311449 -0.0033128115 -0.003313211 -0.0033123381 -0.0033099495 -0.0033059474 -0.0033007276 -0.0032952153 -0.0032899284 -0.0032854993 -0.0032821174 -0.0032799754 -0.0032789016][-0.0033063812 -0.0033063446 -0.0033078603 -0.0033086226 -0.0033086671 -0.0033078773 -0.0033060247 -0.0033029413 -0.0032988468 -0.0032942109 -0.0032896423 -0.0032857996 -0.0032827652 -0.0032806727 -0.0032794788][-0.0033019367 -0.0033011439 -0.0033022291 -0.0033027059 -0.0033026012 -0.0033018398 -0.0033004018 -0.0032980291 -0.0032948665 -0.0032913927 -0.0032879859 -0.0032850374 -0.0032825987 -0.0032808268 -0.0032797283][-0.0032960803 -0.0032944062 -0.0032949494 -0.0032951126 -0.0032948055 -0.0032940742 -0.0032929112 -0.0032912586 -0.0032892039 -0.0032870593 -0.0032850003 -0.0032831451 -0.003281571 -0.0032803542 -0.0032795651][-0.0032900032 -0.0032875817 -0.0032877023 -0.003287659 -0.0032873738 -0.0032868239 -0.0032860413 -0.0032850236 -0.0032839009 -0.0032828227 -0.0032818629 -0.003281015 -0.0032802667 -0.0032796778 -0.003279285]]...]
INFO - root - 2017-12-09 11:21:49.303780: step 19510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:00m:28s remains)
INFO - root - 2017-12-09 11:21:57.919549: step 19520, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 73h:59m:59s remains)
INFO - root - 2017-12-09 11:22:06.355452: step 19530, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 74h:33m:20s remains)
INFO - root - 2017-12-09 11:22:15.071027: step 19540, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:49m:14s remains)
INFO - root - 2017-12-09 11:22:23.803547: step 19550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:07m:53s remains)
INFO - root - 2017-12-09 11:22:32.461026: step 19560, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:47m:38s remains)
INFO - root - 2017-12-09 11:22:41.175498: step 19570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:54m:56s remains)
INFO - root - 2017-12-09 11:22:49.727994: step 19580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:36m:30s remains)
INFO - root - 2017-12-09 11:22:58.622613: step 19590, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 77h:18m:57s remains)
INFO - root - 2017-12-09 11:23:07.246695: step 19600, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 75h:42m:51s remains)
2017-12-09 11:23:08.241048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033130466 -0.003312723 -0.0033047528 -0.0032428096 -0.0030770025 -0.0028153716 -0.0025373194 -0.002346375 -0.0023347496 -0.0025082778 -0.0027761813 -0.0030292992 -0.0032024207 -0.0032875019 -0.0033099719][-0.0033129454 -0.0033088182 -0.0032557184 -0.003044693 -0.0025887324 -0.0019619642 -0.0013838389 -0.0011067188 -0.0012568331 -0.0017549387 -0.0023642145 -0.0028621994 -0.0031578445 -0.0032801866 -0.0033091954][-0.0033107053 -0.0032699562 -0.0030565388 -0.0024444275 -0.0013134074 0.00010184245 0.0012930098 0.0017472489 0.0012782149 0.0001182605 -0.0012248768 -0.0022982189 -0.0029360752 -0.0032135963 -0.0032976696][-0.0032950344 -0.0031383839 -0.0025483405 -0.0011561969 0.0011297485 0.003814006 0.0059841378 0.0067562154 0.0058341054 0.0036520415 0.0011006894 -0.0010144238 -0.0023624748 -0.0030206828 -0.0032584174][-0.0032443411 -0.0028672148 -0.0016687034 0.00084278989 0.0046257367 0.0088363336 0.012125776 0.013253217 0.011809698 0.0084373793 0.0044151461 0.00093249208 -0.0014413758 -0.0026997193 -0.0031902157][-0.0030526714 -0.0023990101 -0.00053430581 0.0031189977 0.0083320495 0.01390198 0.018096171 0.019432038 0.017452111 0.012992476 0.007634664 0.0028723024 -0.00050242408 -0.0023711775 -0.0031205411][-0.0027887707 -0.0019105389 0.00039570802 0.004749313 0.010788578 0.017076839 0.021664275 0.023003025 0.020659592 0.015568483 0.009449455 0.0039518066 5.96256e-06 -0.0022034403 -0.0030885772][-0.0026342992 -0.0016918525 0.00063302671 0.0049316743 0.010829153 0.016896121 0.021234861 0.022411706 0.02006034 0.015075815 0.0090882145 0.0036986049 -0.00015178905 -0.0022794318 -0.0031102146][-0.0026827711 -0.0018879963 8.7881926e-06 0.003518984 0.008365945 0.013369589 0.016932415 0.017869793 0.015893297 0.011737933 0.0067506405 0.002279321 -0.00086081587 -0.002542617 -0.0031701112][-0.002858751 -0.0023200992 -0.0010702764 0.0012750679 0.0045859478 0.008077383 0.010596487 0.011272509 0.0098881889 0.0069663776 0.0034681696 0.00036100531 -0.0017639427 -0.002853395 -0.0032353869][-0.0030731065 -0.0027810382 -0.0021216711 -0.0008548568 0.0010008027 0.0030372234 0.0045598811 0.0050049238 0.0042099655 0.0024882078 0.00043643825 -0.0013515702 -0.0025280463 -0.0030979628 -0.0032828453][-0.0032338707 -0.0031155506 -0.0028506536 -0.0023197704 -0.0014968879 -0.00053276611 0.00023873663 0.00050675427 0.00015452295 -0.00067081605 -0.0016538334 -0.0024859679 -0.0030054217 -0.0032399409 -0.0033089262][-0.0033089078 -0.0032773407 -0.0032039252 -0.0030432451 -0.0027722111 -0.0024249908 -0.0021178648 -0.0019838288 -0.0020917668 -0.0023933768 -0.00275703 -0.0030545227 -0.0032287906 -0.0033008421 -0.0033193389][-0.0033251666 -0.003322107 -0.0033124692 -0.0032839475 -0.003226026 -0.0031395189 -0.0030513473 -0.0030018827 -0.003019789 -0.0030950245 -0.003189232 -0.0032637601 -0.0033048042 -0.003319809 -0.0033226761][-0.0033253671 -0.0033248372 -0.003324932 -0.003323751 -0.0033180173 -0.0033061276 -0.0032912095 -0.0032802457 -0.003279638 -0.0032897172 -0.0033043253 -0.0033159934 -0.0033220421 -0.00332376 -0.0033236174]]...]
INFO - root - 2017-12-09 11:23:16.660454: step 19610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:26m:34s remains)
INFO - root - 2017-12-09 11:23:25.352949: step 19620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:33m:10s remains)
INFO - root - 2017-12-09 11:23:33.874741: step 19630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:49m:26s remains)
INFO - root - 2017-12-09 11:23:42.526946: step 19640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:33m:01s remains)
INFO - root - 2017-12-09 11:23:51.215526: step 19650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:37m:06s remains)
INFO - root - 2017-12-09 11:24:00.009876: step 19660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:35m:00s remains)
INFO - root - 2017-12-09 11:24:08.744412: step 19670, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 74h:58m:31s remains)
INFO - root - 2017-12-09 11:24:17.525818: step 19680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:37m:52s remains)
INFO - root - 2017-12-09 11:24:26.209448: step 19690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:15m:58s remains)
INFO - root - 2017-12-09 11:24:34.824030: step 19700, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 71h:25m:32s remains)
2017-12-09 11:24:35.661481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033221405 -0.0033155698 -0.0033106562 -0.0033061998 -0.0033065649 -0.0033094019 -0.0033108608 -0.0033115519 -0.003315351 -0.003321978 -0.0033279338 -0.0033294545 -0.003328807 -0.00332763 -0.0033260363][-0.0033231585 -0.0033169799 -0.0033116785 -0.0033056661 -0.0033037893 -0.0033024179 -0.0032981616 -0.0032944384 -0.0033000922 -0.0033128478 -0.0033225641 -0.0033261182 -0.0033261694 -0.0033256516 -0.0033242877][-0.0033254866 -0.0033193319 -0.0033136618 -0.0033026538 -0.0032949527 -0.0032878458 -0.0032783917 -0.0032710836 -0.003281028 -0.0033018647 -0.003317558 -0.0033232078 -0.0033237522 -0.003323857 -0.0033229871][-0.003328416 -0.0033225722 -0.0033156117 -0.0033058156 -0.0032897214 -0.0032726582 -0.0032576956 -0.0032501572 -0.0032640777 -0.003290456 -0.0033113619 -0.0033206015 -0.0033218905 -0.0033223263 -0.0033220295][-0.0033317297 -0.0033261415 -0.0033186276 -0.003305139 -0.0032816834 -0.0032507486 -0.0032292765 -0.0032283904 -0.0032497589 -0.0032826434 -0.0033077376 -0.0033197727 -0.0033220961 -0.0033220996 -0.003321399][-0.0033351954 -0.0033290051 -0.0033195238 -0.003297966 -0.0032633613 -0.0032233323 -0.0032025161 -0.0032104836 -0.0032410014 -0.0032785472 -0.00330729 -0.0033197417 -0.0033217166 -0.003321172 -0.0033196716][-0.0033362773 -0.0033279152 -0.0033129952 -0.0032808133 -0.0032381546 -0.0032007378 -0.0031905593 -0.0032100747 -0.0032472769 -0.0032856902 -0.0033112927 -0.0033203613 -0.0033213645 -0.0033202348 -0.0033175659][-0.0033322491 -0.0033167843 -0.0032948356 -0.0032554697 -0.0032157132 -0.0031902045 -0.0031931177 -0.0032238595 -0.0032646598 -0.0032984666 -0.0033158604 -0.0033200895 -0.0033201373 -0.0033183249 -0.0033152732][-0.0033183098 -0.0032912239 -0.0032636581 -0.0032290451 -0.00320418 -0.0031976213 -0.0032147602 -0.0032514986 -0.0032877589 -0.0033105817 -0.0033184325 -0.00331931 -0.0033182476 -0.0033157924 -0.0033124215][-0.0032997262 -0.0032647427 -0.0032329233 -0.0032100414 -0.0032010744 -0.0032138568 -0.0032420838 -0.0032785395 -0.0033049127 -0.003316429 -0.0033191873 -0.0033183151 -0.0033162669 -0.0033133612 -0.0033099637][-0.003286429 -0.0032510478 -0.003219916 -0.0032073255 -0.003211759 -0.0032384556 -0.0032708435 -0.0033003781 -0.0033156646 -0.0033189296 -0.0033181603 -0.0033161519 -0.003313676 -0.0033110883 -0.0033083186][-0.0032894823 -0.0032590441 -0.0032342931 -0.0032311713 -0.0032410766 -0.0032667662 -0.0032935145 -0.0033119365 -0.0033188763 -0.0033188376 -0.0033164942 -0.0033138923 -0.0033114119 -0.0033095372 -0.0033071579][-0.0033020726 -0.0032841817 -0.0032695257 -0.0032701385 -0.0032792941 -0.0032956963 -0.0033091987 -0.0033164597 -0.0033179719 -0.0033167775 -0.0033137181 -0.0033109928 -0.0033088247 -0.0033075993 -0.0033061805][-0.0033132774 -0.0033049949 -0.0033005998 -0.0033011974 -0.0033053791 -0.0033120108 -0.0033156206 -0.0033162946 -0.0033155063 -0.0033136709 -0.0033106939 -0.0033080147 -0.0033061458 -0.003305756 -0.003305028][-0.0033158709 -0.0033128932 -0.0033126692 -0.0033124061 -0.0033130369 -0.0033133824 -0.0033143256 -0.0033137514 -0.0033126133 -0.00331048 -0.0033083258 -0.0033061274 -0.0033044727 -0.0033040335 -0.0033038731]]...]
INFO - root - 2017-12-09 11:24:44.250773: step 19710, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:14m:46s remains)
INFO - root - 2017-12-09 11:24:52.954798: step 19720, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 71h:59m:31s remains)
INFO - root - 2017-12-09 11:25:01.316325: step 19730, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 71h:57m:49s remains)
INFO - root - 2017-12-09 11:25:09.639066: step 19740, loss = 0.89, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 66h:18m:28s remains)
INFO - root - 2017-12-09 11:25:18.178600: step 19750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:59m:58s remains)
INFO - root - 2017-12-09 11:25:26.805745: step 19760, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:40m:14s remains)
INFO - root - 2017-12-09 11:25:35.526276: step 19770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:55m:13s remains)
INFO - root - 2017-12-09 11:25:44.242141: step 19780, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 72h:33m:59s remains)
INFO - root - 2017-12-09 11:25:52.968244: step 19790, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 79h:37m:41s remains)
INFO - root - 2017-12-09 11:26:01.682936: step 19800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:37m:42s remains)
2017-12-09 11:26:02.516697: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01062946 0.01968235 0.029784575 0.040095117 0.048770797 0.054058932 0.054575324 0.049829263 0.040960267 0.029513039 0.017933417 0.0084646214 0.002137569 -0.0013075126 -0.0028094039][0.011579481 0.024181465 0.039793186 0.057110306 0.072823085 0.083660871 0.086883448 0.081141882 0.06797874 0.05012745 0.031760383 0.016391406 0.00585471 5.7520578e-05 -0.0024269097][0.012322258 0.029258341 0.052119363 0.078589581 0.10313369 0.12001416 0.12565261 0.11824745 0.10035515 0.075705826 0.049952563 0.02783739 0.011939818 0.0026564531 -0.0016162829][0.014732024 0.036465541 0.067366406 0.10395439 0.13855416 0.16258094 0.17140004 0.16224582 0.13864192 0.10572866 0.071092345 0.041065566 0.019045744 0.0058416771 -0.00051619858][0.018755676 0.04517157 0.083246738 0.12816443 0.1708978 0.20079108 0.21267717 0.20282044 0.17506997 0.13523769 0.092488229 0.054780036 0.026609031 0.0093320115 0.00071257283][0.023979977 0.054011408 0.096811995 0.14708196 0.19489516 0.22834155 0.24210018 0.23170175 0.20123333 0.15664016 0.10818327 0.064992718 0.0323924 0.012123778 0.0017386617][0.031313185 0.063508362 0.10810062 0.15994461 0.20898962 0.24339411 0.25754756 0.24640708 0.2141221 0.16668102 0.1150896 0.069132745 0.03459822 0.013190806 0.0021438242][0.04002839 0.072542876 0.11598193 0.16542666 0.21161443 0.24400301 0.25681213 0.24467938 0.21155688 0.16348314 0.11167119 0.066096552 0.032488834 0.012071898 0.0017033487][0.048577983 0.079124466 0.11811432 0.16159265 0.20163271 0.22942571 0.23928329 0.226134 0.19347626 0.14738071 0.098673448 0.056788072 0.02678352 0.0091678267 0.00058542495][0.053911466 0.080085941 0.11227354 0.14744571 0.17919408 0.20039578 0.20584075 0.1916384 0.1609602 0.11978085 0.077656433 0.04268707 0.018644065 0.005222125 -0.00085956696][0.055276811 0.074198142 0.09740071 0.12278435 0.14533627 0.15956958 0.16089508 0.14669009 0.11998893 0.086299233 0.053303845 0.027150199 0.010180308 0.0014267184 -0.0021196913][0.0527334 0.0636582 0.0771848 0.092359878 0.10554589 0.11291327 0.11097264 0.098275423 0.077511027 0.053093813 0.030506331 0.01359402 0.003420695 -0.0012780908 -0.0028840594][0.045326058 0.049671907 0.055272114 0.061916817 0.067183778 0.068919666 0.065001473 0.055046819 0.041089524 0.026111301 0.013223697 0.0042390926 -0.00065951957 -0.0026187664 -0.0031582923][0.033463303 0.033876404 0.034776464 0.036209 0.03661013 0.035033036 0.030706298 0.023913074 0.016051086 0.0086116288 0.0028385941 -0.0008064846 -0.0025615296 -0.0031382402 -0.0032540231][0.019183388 0.017967245 0.017132971 0.016609622 0.015442794 0.013317877 0.010214992 0.0065420736 0.0030216288 0.00017457316 -0.0017282518 -0.0027623416 -0.0031802626 -0.0032810045 -0.0032900376]]...]
INFO - root - 2017-12-09 11:26:11.056773: step 19810, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:43m:23s remains)
INFO - root - 2017-12-09 11:26:19.758038: step 19820, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 72h:54m:07s remains)
INFO - root - 2017-12-09 11:26:28.216052: step 19830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:56m:00s remains)
INFO - root - 2017-12-09 11:26:36.811728: step 19840, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 71h:05m:14s remains)
INFO - root - 2017-12-09 11:26:45.199220: step 19850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:51m:08s remains)
INFO - root - 2017-12-09 11:26:53.877895: step 19860, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:10m:44s remains)
INFO - root - 2017-12-09 11:27:02.364576: step 19870, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 73h:30m:51s remains)
INFO - root - 2017-12-09 11:27:10.958932: step 19880, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 72h:40m:23s remains)
INFO - root - 2017-12-09 11:27:19.637543: step 19890, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:43m:38s remains)
INFO - root - 2017-12-09 11:27:28.320337: step 19900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 74h:19m:18s remains)
2017-12-09 11:27:29.164731: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03599396 0.038492292 0.04071388 0.042574279 0.04372083 0.044861972 0.045970313 0.048381854 0.053482652 0.062039115 0.073467694 0.085582919 0.095893331 0.102752 0.1051109][0.043986768 0.047284689 0.050651725 0.0535089 0.055580936 0.056949787 0.057843182 0.060210422 0.064959265 0.072858371 0.08343862 0.094816521 0.104716 0.11102415 0.11271122][0.050509617 0.0547853 0.058954738 0.062788181 0.065418027 0.066517659 0.066715047 0.067602038 0.07060957 0.076495856 0.0853794 0.095338292 0.10452721 0.11108084 0.11338714][0.054778188 0.059590109 0.064133659 0.068767756 0.072198264 0.07334549 0.072746694 0.071949296 0.072824255 0.076375939 0.082601316 0.090855688 0.099366575 0.10644919 0.11033694][0.055685669 0.061126143 0.066227891 0.071245722 0.074893557 0.07663431 0.076063864 0.07444194 0.073873609 0.075376667 0.079410523 0.08555112 0.092779793 0.099568918 0.10463019][0.054051194 0.0596738 0.064704567 0.069825083 0.073666126 0.075574957 0.075355552 0.074078575 0.073384337 0.074185 0.0770367 0.081669889 0.087368429 0.093439862 0.098790564][0.049908072 0.055427082 0.059896648 0.064353988 0.067614645 0.06956739 0.06972231 0.068981357 0.068883017 0.070282735 0.073364168 0.07742127 0.082701504 0.0883143 0.093474485][0.040689524 0.045764312 0.049738642 0.053413585 0.055887755 0.05735819 0.057745956 0.057762653 0.058491286 0.060813621 0.0646571 0.069487616 0.075365804 0.081279509 0.086883731][0.026839692 0.030901315 0.034210529 0.037190925 0.039207172 0.040564939 0.041099448 0.04143152 0.042719167 0.045662377 0.050172262 0.056019057 0.063169293 0.070260443 0.076712][0.012700692 0.015329221 0.017699936 0.020031268 0.021643385 0.022770178 0.023236554 0.023904953 0.025360329 0.0282276 0.032914311 0.03935577 0.047226582 0.055423841 0.062976748][0.0025450867 0.0038993214 0.005387899 0.0067519755 0.0077962861 0.0088513326 0.0094724465 0.010281051 0.011527743 0.014109684 0.018311914 0.024184415 0.031643283 0.039638191 0.047223873][-0.0022118976 -0.0017426992 -0.0011133992 -0.00045013567 0.00026409491 0.00096976804 0.0015243944 0.0023699794 0.0034758416 0.005298804 0.0082449708 0.012687355 0.018421533 0.024899419 0.031417578][-0.0033182569 -0.0032558662 -0.0031553814 -0.0030819308 -0.0028997611 -0.0025927206 -0.0022129402 -0.0017157842 -0.00098576793 0.00022035628 0.0020794519 0.0047498117 0.0081826784 0.012444336 0.016931707][-0.0033345958 -0.003330983 -0.0033273243 -0.0033199925 -0.0032976945 -0.0032655997 -0.0032113481 -0.0030592799 -0.0028034495 -0.0023532256 -0.0015558834 -0.00034795096 0.0012373962 0.0033541578 0.0057316278][-0.0033314736 -0.0033293879 -0.0033284526 -0.0033255545 -0.0033215978 -0.0033148262 -0.0033060925 -0.0032968328 -0.0032887745 -0.0032385802 -0.0030811471 -0.0027778517 -0.0022676871 -0.0015141681 -0.00060464954]]...]
INFO - root - 2017-12-09 11:27:37.714375: step 19910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:07m:05s remains)
INFO - root - 2017-12-09 11:27:46.364859: step 19920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:41m:47s remains)
INFO - root - 2017-12-09 11:27:54.953766: step 19930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:26m:37s remains)
INFO - root - 2017-12-09 11:28:03.496730: step 19940, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.821 sec/batch; 71h:15m:09s remains)
INFO - root - 2017-12-09 11:28:11.880187: step 19950, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:49m:56s remains)
INFO - root - 2017-12-09 11:28:20.570439: step 19960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:56m:42s remains)
INFO - root - 2017-12-09 11:28:29.139785: step 19970, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:46m:51s remains)
INFO - root - 2017-12-09 11:28:37.959139: step 19980, loss = 0.89, batch loss = 0.69 (8.1 examples/sec; 0.988 sec/batch; 85h:47m:14s remains)
INFO - root - 2017-12-09 11:28:46.585898: step 19990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:43m:25s remains)
INFO - root - 2017-12-09 11:28:55.241682: step 20000, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 74h:46m:38s remains)
2017-12-09 11:28:56.108833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003355318 -0.0033534656 -0.0033531177 -0.0033530791 -0.0033530886 -0.0033532337 -0.003353308 -0.0033533594 -0.0033536404 -0.0033538684 -0.0033539396 -0.0033539091 -0.0033538281 -0.0033535936 -0.0033533333][-0.0033537007 -0.0033516292 -0.0033512823 -0.0033512737 -0.0033513517 -0.0033514374 -0.0033514674 -0.0033515131 -0.0033516155 -0.0033517934 -0.0033518281 -0.003351728 -0.0033516069 -0.0033514055 -0.0033512658][-0.0033538318 -0.0033519259 -0.0033516726 -0.0033515042 -0.0033514688 -0.0033513468 -0.0033515161 -0.003351772 -0.0033519834 -0.0033519655 -0.0033517492 -0.0033515026 -0.0033511727 -0.00335086 -0.0033507193][-0.0033542691 -0.0033525941 -0.0033523373 -0.0033520311 -0.0033517964 -0.0033512511 -0.0033511221 -0.0033517072 -0.0033523217 -0.003352558 -0.0033524898 -0.0033520556 -0.0033514432 -0.003350911 -0.0033506663][-0.0033547273 -0.0033533718 -0.003353291 -0.0033529764 -0.0033522176 -0.0033504341 -0.0033484921 -0.0033484281 -0.00335003 -0.0033515468 -0.0033526111 -0.0033527603 -0.0033521852 -0.0033514597 -0.0033509897][-0.0033547778 -0.0033540106 -0.0033545773 -0.0033542172 -0.0033529277 -0.0033490073 -0.0033432553 -0.0033408906 -0.0033434583 -0.0033477952 -0.0033509866 -0.0033526074 -0.0033528102 -0.0033522118 -0.0033516297][-0.0033542879 -0.0033539375 -0.0033553389 -0.0033556407 -0.003354239 -0.0033487277 -0.0033393507 -0.0033323686 -0.0033342559 -0.0033419249 -0.0033484171 -0.003352039 -0.0033532782 -0.0033530556 -0.0033524435][-0.0033535943 -0.0033534584 -0.0033552798 -0.0033566023 -0.0033559576 -0.0033510439 -0.0033412192 -0.0033311842 -0.0033298244 -0.0033377649 -0.0033465498 -0.0033516616 -0.003353945 -0.0033541499 -0.0033533135][-0.0033530507 -0.0033525939 -0.0033546053 -0.0033566819 -0.0033572363 -0.0033543231 -0.0033472884 -0.003339 -0.0033352068 -0.0033393598 -0.0033471345 -0.0033526272 -0.003355026 -0.0033551787 -0.0033541282][-0.0033526893 -0.0033517426 -0.0033537031 -0.0033563296 -0.0033579164 -0.0033567247 -0.0033527676 -0.0033484495 -0.0033452243 -0.0033461468 -0.0033504402 -0.003354335 -0.0033561997 -0.0033560831 -0.003354681][-0.0033523827 -0.0033510302 -0.0033525901 -0.0033552609 -0.0033577187 -0.0033583143 -0.00335668 -0.0033551222 -0.0033535587 -0.0033535683 -0.0033546956 -0.0033561168 -0.0033568291 -0.0033563327 -0.003354918][-0.003352273 -0.0033504735 -0.0033516681 -0.0033540602 -0.0033567562 -0.0033587273 -0.0033589581 -0.0033587264 -0.0033580521 -0.003357738 -0.0033575199 -0.003357084 -0.0033566328 -0.0033557394 -0.0033545166][-0.0033525103 -0.0033504129 -0.0033511277 -0.0033528621 -0.003355355 -0.0033578791 -0.0033593902 -0.0033599064 -0.0033597739 -0.003359349 -0.0033584745 -0.003357152 -0.0033559029 -0.0033547771 -0.0033536502][-0.003352708 -0.0033504972 -0.0033509065 -0.0033520989 -0.0033539222 -0.0033561694 -0.0033578295 -0.0033587385 -0.0033589364 -0.0033586225 -0.0033576705 -0.0033563634 -0.0033550479 -0.0033538444 -0.0033529808][-0.0033529096 -0.0033507305 -0.0033508632 -0.0033514984 -0.0033525997 -0.0033540765 -0.0033554649 -0.0033563066 -0.0033565743 -0.0033563825 -0.0033558875 -0.003354945 -0.0033539084 -0.0033530588 -0.0033524574]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 11:29:05.397154: step 20010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:24m:51s remains)
INFO - root - 2017-12-09 11:29:14.189225: step 20020, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 78h:44m:53s remains)
INFO - root - 2017-12-09 11:29:22.693569: step 20030, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:48m:17s remains)
INFO - root - 2017-12-09 11:29:31.463749: step 20040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:32m:51s remains)
INFO - root - 2017-12-09 11:29:40.070174: step 20050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:12m:24s remains)
INFO - root - 2017-12-09 11:29:48.878513: step 20060, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 78h:45m:23s remains)
INFO - root - 2017-12-09 11:29:57.488427: step 20070, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:07m:31s remains)
INFO - root - 2017-12-09 11:30:05.972402: step 20080, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 69h:52m:50s remains)
INFO - root - 2017-12-09 11:30:14.462745: step 20090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:58m:38s remains)
INFO - root - 2017-12-09 11:30:23.281434: step 20100, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 74h:09m:32s remains)
2017-12-09 11:30:24.075689: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0034112621 0.0033816427 0.0034865586 0.0038197155 0.0041403947 0.0040093875 0.0037514435 0.0030845969 0.0021241338 0.00092059677 -0.00029590703 -0.0013459804 -0.0021625436 -0.0027283709 -0.0030902289][0.0066903653 0.0070825657 0.0072102631 0.0072560832 0.0069643408 0.0061159879 0.005236092 0.0039508 0.002572716 0.0010642807 -0.0003100601 -0.0014303075 -0.0022708646 -0.0028166664 -0.0031456421][0.01061526 0.011777433 0.012408522 0.012657405 0.012193892 0.010795701 0.0093089687 0.0071566505 0.0049009593 0.0025695593 0.0005950632 -0.000881311 -0.002020211 -0.002738189 -0.0031475066][0.016791359 0.019059103 0.020705923 0.021740159 0.021688996 0.020232631 0.018312117 0.015110937 0.011491973 0.0075532822 0.0040502828 0.0013295428 -0.00081262225 -0.0021779868 -0.0029740008][0.024848729 0.028424148 0.031469557 0.033761993 0.034697238 0.033940863 0.032192182 0.028291777 0.023264643 0.017203795 0.011358826 0.0064112786 0.0023347952 -0.00047983904 -0.0022829045][0.035503231 0.03963542 0.043548506 0.0471665 0.049445 0.049619306 0.048333317 0.044279121 0.038306888 0.030310262 0.021838531 0.014083347 0.0074560158 0.0026287443 -0.00072073634][0.048287757 0.052590754 0.056477539 0.060340062 0.063257165 0.064201988 0.063416325 0.059342943 0.052821696 0.043559361 0.032960102 0.0225856 0.013306285 0.0064000245 0.0013939661][0.059475295 0.06421499 0.068221144 0.072037168 0.074728034 0.0755122 0.074622534 0.070461184 0.063583456 0.053546853 0.041664183 0.029531803 0.018304858 0.0097482689 0.0033795496][0.066183656 0.071284689 0.075315267 0.079071283 0.081524834 0.081965826 0.0806265 0.076176889 0.069020666 0.058720723 0.046329297 0.033351522 0.021191172 0.011819791 0.0047381409][0.0678385 0.072735667 0.076407142 0.079830542 0.081995316 0.082199454 0.080647074 0.0762842 0.069188841 0.059055325 0.046770535 0.033817686 0.021647612 0.012221883 0.0051056407][0.064909019 0.069254816 0.072152853 0.075005963 0.0768619 0.07692837 0.075435981 0.071478345 0.064912133 0.055414591 0.043783598 0.031522464 0.020085791 0.011219664 0.0045705037][0.057703521 0.0614212 0.063627884 0.065846145 0.067396492 0.067475371 0.066164285 0.062670827 0.056743305 0.048149213 0.037631739 0.026670273 0.016659888 0.0089296894 0.003216658][0.046405684 0.049700439 0.051568009 0.053429969 0.054811984 0.054960676 0.053837597 0.050757345 0.045548551 0.03809623 0.029181868 0.020114735 0.012048883 0.0058878735 0.0014420266][0.03238235 0.035253774 0.036972851 0.038604081 0.039792936 0.039936949 0.038961951 0.036357641 0.032131854 0.026332945 0.019605996 0.012945159 0.007146908 0.0027883339 -0.00027611712][0.0187809 0.021092217 0.02261176 0.023999181 0.024960205 0.025039464 0.024170434 0.022116905 0.019036671 0.015077933 0.01067619 0.0064576268 0.0028579494 0.0001950562 -0.001640674]]...]
INFO - root - 2017-12-09 11:30:32.593959: step 20110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:09m:49s remains)
INFO - root - 2017-12-09 11:30:41.268131: step 20120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 76h:41m:20s remains)
INFO - root - 2017-12-09 11:30:49.864084: step 20130, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 74h:15m:16s remains)
INFO - root - 2017-12-09 11:30:58.496172: step 20140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:18m:43s remains)
INFO - root - 2017-12-09 11:31:06.942094: step 20150, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 73h:54m:46s remains)
INFO - root - 2017-12-09 11:31:15.669454: step 20160, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 78h:35m:04s remains)
INFO - root - 2017-12-09 11:31:24.257810: step 20170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:00m:35s remains)
INFO - root - 2017-12-09 11:31:33.107005: step 20180, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 75h:25m:26s remains)
INFO - root - 2017-12-09 11:31:41.794051: step 20190, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 74h:45m:25s remains)
INFO - root - 2017-12-09 11:31:50.620758: step 20200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:07m:05s remains)
2017-12-09 11:31:51.435296: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022042783 0.02328809 0.023942787 0.024164606 0.024256688 0.024485173 0.024649156 0.024742363 0.024674386 0.024501882 0.024179509 0.02400684 0.023568662 0.023160012 0.022894036][0.024687376 0.026072262 0.026756776 0.027183225 0.027536429 0.027852541 0.028131025 0.02829195 0.028327288 0.028262908 0.028100641 0.02776617 0.027253583 0.026924103 0.026508767][0.024227304 0.025420263 0.025934342 0.026314076 0.026712453 0.027065385 0.027350519 0.027406743 0.027296109 0.027074277 0.026818043 0.026457939 0.025989419 0.025716366 0.025468875][0.023945132 0.024974804 0.025172895 0.025305277 0.025575494 0.025801253 0.025865871 0.0256154 0.025194287 0.024699628 0.024308207 0.023956666 0.023600766 0.023312043 0.02309766][0.023920059 0.025119584 0.025123082 0.024830868 0.024742553 0.024495095 0.023955937 0.023123235 0.022156017 0.021194909 0.020493845 0.019965399 0.019553665 0.019189024 0.018991267][0.023692312 0.025017878 0.024679914 0.024024077 0.023293855 0.022425722 0.02124133 0.019681549 0.018007992 0.01645642 0.015294883 0.014401913 0.013743568 0.013254045 0.013075422][0.022036867 0.023453785 0.022716384 0.021678291 0.020485051 0.018946005 0.017091841 0.015052455 0.012913175 0.010911306 0.0093728937 0.0081235152 0.0072177262 0.0066414722 0.0065370821][0.018537542 0.019552929 0.01865615 0.017375855 0.01591262 0.014240025 0.012317797 0.010026418 0.0076419129 0.0054385341 0.0037689093 0.0024484564 0.0015532426 0.0010444173 0.00098300865][0.012419431 0.013034152 0.012225023 0.011134522 0.0098679047 0.008381797 0.006727004 0.00479411 0.0028344514 0.0011031474 -0.00022948347 -0.0011835373 -0.0017742859 -0.0021168815 -0.0021919857][0.0054417294 0.0056496705 0.0051478366 0.0044673164 0.0036281184 0.0026466753 0.0015343423 0.00029425882 -0.00088830967 -0.0018751336 -0.0025442792 -0.0029418988 -0.0031275223 -0.0031668886 -0.0030762481][7.9693506e-05 7.8843441e-05 -0.00014997832 -0.00046785991 -0.00085372757 -0.0012957221 -0.0017842853 -0.0022785207 -0.0027028972 -0.0029841582 -0.0031182151 -0.0031636241 -0.0031441241 -0.003094468 -0.0029130678][-0.0025750408 -0.0025787232 -0.0026365356 -0.0027216633 -0.0028241321 -0.0029413272 -0.0030706923 -0.0031899158 -0.0032700717 -0.0032958861 -0.0032786727 -0.0032080649 -0.0030721663 -0.0028436319 -0.0024454151][-0.0033336517 -0.003322809 -0.0033104976 -0.0032999185 -0.0032882011 -0.0032782897 -0.0032695744 -0.0032870255 -0.0033020517 -0.0033063611 -0.0033015027 -0.0032278553 -0.0030602349 -0.0027357426 -0.0021970444][-0.0033351858 -0.0033241687 -0.0033149419 -0.0032982109 -0.0032768229 -0.0032631492 -0.0032606162 -0.0032636151 -0.0032494511 -0.0032711988 -0.0032829985 -0.0032190317 -0.0030822605 -0.0028026185 -0.0023548966][-0.00334173 -0.0033382839 -0.0033344193 -0.003327888 -0.003316473 -0.0033011378 -0.0032801453 -0.003260969 -0.0032523714 -0.0032596753 -0.0032674514 -0.0032460662 -0.0031832911 -0.0030181813 -0.002764683]]...]
INFO - root - 2017-12-09 11:32:00.051877: step 20210, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:41m:11s remains)
INFO - root - 2017-12-09 11:32:08.748708: step 20220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 76h:39m:44s remains)
INFO - root - 2017-12-09 11:32:17.393224: step 20230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:48m:39s remains)
INFO - root - 2017-12-09 11:32:26.069154: step 20240, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 78h:00m:06s remains)
INFO - root - 2017-12-09 11:32:34.633806: step 20250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:00m:05s remains)
INFO - root - 2017-12-09 11:32:43.246732: step 20260, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 73h:33m:14s remains)
INFO - root - 2017-12-09 11:32:51.955819: step 20270, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 76h:28m:30s remains)
INFO - root - 2017-12-09 11:33:00.701545: step 20280, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 75h:50m:09s remains)
INFO - root - 2017-12-09 11:33:09.455181: step 20290, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 76h:45m:27s remains)
INFO - root - 2017-12-09 11:33:18.350465: step 20300, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:18m:50s remains)
2017-12-09 11:33:19.159421: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1002484 0.10321932 0.10860911 0.11854102 0.13226464 0.14813404 0.16293637 0.17207953 0.17160884 0.16077937 0.14158204 0.11678051 0.09098991 0.068461671 0.054115582][0.058952715 0.06173753 0.0677682 0.0792234 0.09599074 0.1160282 0.13465372 0.1458098 0.14428033 0.13019983 0.10808263 0.082314253 0.057500217 0.037500639 0.026081473][0.036296695 0.037333209 0.043872584 0.05889878 0.081372373 0.10781161 0.13189811 0.14551999 0.1422924 0.1234004 0.095703796 0.065822162 0.039310735 0.019787204 0.0097548058][0.03784376 0.040479284 0.050043531 0.070195846 0.099342421 0.13298631 0.16213118 0.17676826 0.16994374 0.14377543 0.10720336 0.069203764 0.037023839 0.014535502 0.0034854098][0.054822877 0.064137161 0.081531078 0.11031227 0.1466219 0.18529429 0.216095 0.22866763 0.21578422 0.18022282 0.13258961 0.0843073 0.044335272 0.016750643 0.0028378267][0.079257108 0.099395946 0.12722227 0.16471635 0.20676412 0.24759161 0.27583697 0.28236678 0.26094651 0.21522099 0.1567484 0.098766111 0.051668253 0.019697167 0.0034424674][0.10019808 0.13153282 0.16903403 0.21275775 0.25702503 0.29627156 0.31953305 0.31872794 0.28906855 0.23516592 0.16891854 0.10472153 0.053765956 0.020145837 0.0033606195][0.11577333 0.15461972 0.197347 0.24244559 0.28397954 0.3170895 0.33260375 0.3242819 0.28930834 0.23210597 0.16401477 0.099261895 0.04911473 0.017452959 0.0023286368][0.12351803 0.16398491 0.20558561 0.24568452 0.27922332 0.30260915 0.30901995 0.29425675 0.25739756 0.2026547 0.14010368 0.082038678 0.038417868 0.012339128 0.00069436734][0.12111863 0.15819648 0.19337617 0.2238238 0.2461191 0.25817698 0.25586271 0.23696858 0.20211825 0.15514921 0.10412891 0.058357079 0.02530258 0.0067117931 -0.00094636856][0.10637148 0.13658909 0.16264433 0.18216752 0.19336754 0.19576886 0.18771183 0.16840695 0.13932198 0.10355879 0.066853851 0.035290327 0.013514146 0.00208854 -0.0021653702][0.0817698 0.10378674 0.12095434 0.1313404 0.13436747 0.13074763 0.12061504 0.10425729 0.083128072 0.059302442 0.036307644 0.017473316 0.0051393406 -0.00086343894 -0.0028430074][0.054064319 0.06814348 0.078046612 0.082388408 0.0811909 0.075584844 0.066546321 0.054847673 0.041547343 0.027833613 0.015491785 0.0059877327 0.00017033261 -0.002411772 -0.0031430696][0.02991941 0.03787287 0.042845841 0.044009227 0.041644298 0.036752924 0.0303708 0.023235 0.015998041 0.0092816614 0.0037801054 -7.0286915e-05 -0.0022016759 -0.0030393675 -0.0032391441][0.012594962 0.016517159 0.018710477 0.018725872 0.016823567 0.013691964 0.010099625 0.006543803 0.003331383 0.00067737768 -0.0012564885 -0.0024424903 -0.0030144386 -0.0032089727 -0.0032604658]]...]
INFO - root - 2017-12-09 11:33:27.828394: step 20310, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 76h:15m:20s remains)
INFO - root - 2017-12-09 11:33:36.560405: step 20320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:28m:07s remains)
INFO - root - 2017-12-09 11:33:45.009248: step 20330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 75h:36m:08s remains)
INFO - root - 2017-12-09 11:33:53.426787: step 20340, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:50m:41s remains)
INFO - root - 2017-12-09 11:34:01.868927: step 20350, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 75h:02m:41s remains)
INFO - root - 2017-12-09 11:34:10.447147: step 20360, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 75h:42m:40s remains)
INFO - root - 2017-12-09 11:34:18.986344: step 20370, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.812 sec/batch; 70h:22m:24s remains)
INFO - root - 2017-12-09 11:34:27.812588: step 20380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:34m:21s remains)
INFO - root - 2017-12-09 11:34:36.489714: step 20390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:03m:09s remains)
INFO - root - 2017-12-09 11:34:45.234663: step 20400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:08m:45s remains)
2017-12-09 11:34:46.159998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033347846 -0.0033267089 -0.003316483 -0.0033055115 -0.0032960372 -0.0032903894 -0.0032888816 -0.0032905855 -0.0032947098 -0.0032998926 -0.0033064175 -0.0033134439 -0.0033203163 -0.0033263953 -0.0033322854][-0.0033364894 -0.0033291678 -0.0033196739 -0.0033092187 -0.0033002 -0.0032950614 -0.0032940756 -0.0032965371 -0.0033016312 -0.0033076373 -0.0033146564 -0.0033217242 -0.0033277972 -0.0033323879 -0.0033362489][-0.003340058 -0.0033336733 -0.0033256712 -0.0033166974 -0.0033089521 -0.0033048608 -0.0033047157 -0.0033077265 -0.0033128432 -0.0033186255 -0.0033250395 -0.0033307376 -0.0033349383 -0.0033373372 -0.0033392578][-0.0033444648 -0.0033395432 -0.0033333094 -0.0033262731 -0.0033200395 -0.0033169049 -0.0033169957 -0.0033196653 -0.0033239124 -0.0033284591 -0.0033332123 -0.0033366333 -0.0033382867 -0.0033386792 -0.003338428][-0.0033477326 -0.0033443167 -0.0033399521 -0.0033349325 -0.0033303765 -0.0033278461 -0.0033275429 -0.003329016 -0.0033315469 -0.0033342782 -0.003337157 -0.0033385374 -0.0033382315 -0.0033368706 -0.0033352436][-0.0033489855 -0.003346842 -0.0033442061 -0.0033407039 -0.0033374068 -0.0033352871 -0.0033344422 -0.0033344466 -0.0033348827 -0.0033355001 -0.0033363898 -0.0033359316 -0.0033339043 -0.0033314952 -0.0033293089][-0.0033466665 -0.0033453058 -0.0033440008 -0.003341604 -0.0033396734 -0.0033381018 -0.0033369258 -0.003335949 -0.0033350377 -0.0033343094 -0.0033335323 -0.0033315406 -0.0033286663 -0.0033261171 -0.0033240933][-0.003342255 -0.0033414725 -0.003341377 -0.0033402515 -0.0033390329 -0.003337617 -0.0033360454 -0.0033343791 -0.0033329171 -0.0033317034 -0.0033306105 -0.0033284095 -0.0033257173 -0.0033236663 -0.0033219706][-0.0033368606 -0.0033362187 -0.0033367341 -0.0033362645 -0.0033354119 -0.0033338782 -0.003331874 -0.0033299532 -0.0033283636 -0.0033271371 -0.0033262658 -0.0033248707 -0.0033231566 -0.0033219117 -0.0033208947][-0.0033314347 -0.0033302261 -0.0033306174 -0.0033301336 -0.0033295224 -0.0033283147 -0.003326586 -0.0033249618 -0.0033238218 -0.0033230737 -0.00332268 -0.0033219843 -0.0033211003 -0.0033204514 -0.003320229][-0.0033274661 -0.0033252826 -0.00332513 -0.0033243434 -0.0033238255 -0.0033230765 -0.0033220712 -0.0033211748 -0.0033207659 -0.0033206178 -0.0033207564 -0.0033206909 -0.0033202418 -0.00332003 -0.0033202067][-0.0033254649 -0.003322927 -0.0033223066 -0.0033213424 -0.0033207652 -0.0033203992 -0.0033200206 -0.0033197508 -0.0033200129 -0.0033204767 -0.0033209708 -0.0033212418 -0.003321189 -0.0033211519 -0.0033213536][-0.0033256463 -0.0033232535 -0.0033225818 -0.0033214951 -0.0033205634 -0.0033199892 -0.0033196071 -0.0033192949 -0.0033196618 -0.0033202877 -0.0033209624 -0.0033215333 -0.0033220237 -0.0033225107 -0.0033230719][-0.0033264549 -0.0033243443 -0.0033239312 -0.0033229091 -0.0033218008 -0.0033205778 -0.0033196 -0.0033188374 -0.0033188392 -0.0033193983 -0.003320016 -0.0033208411 -0.003321609 -0.0033223087 -0.003323101][-0.0033273902 -0.0033256609 -0.0033254658 -0.0033243299 -0.0033229836 -0.0033215028 -0.0033202206 -0.0033190488 -0.0033188302 -0.0033193293 -0.003319805 -0.0033204695 -0.0033210681 -0.0033220011 -0.0033229953]]...]
INFO - root - 2017-12-09 11:34:54.673015: step 20410, loss = 0.89, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 79h:30m:33s remains)
INFO - root - 2017-12-09 11:35:03.359754: step 20420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:11m:02s remains)
INFO - root - 2017-12-09 11:35:12.000065: step 20430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:18m:37s remains)
INFO - root - 2017-12-09 11:35:20.803946: step 20440, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:58m:09s remains)
INFO - root - 2017-12-09 11:35:29.534203: step 20450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:48m:06s remains)
INFO - root - 2017-12-09 11:35:38.089906: step 20460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:26m:52s remains)
INFO - root - 2017-12-09 11:35:46.618206: step 20470, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 73h:59m:19s remains)
INFO - root - 2017-12-09 11:35:55.145156: step 20480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:08m:09s remains)
INFO - root - 2017-12-09 11:36:03.632092: step 20490, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 76h:17m:56s remains)
INFO - root - 2017-12-09 11:36:12.205561: step 20500, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 78h:36m:18s remains)
2017-12-09 11:36:13.066668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033362675 -0.0033366354 -0.0033396673 -0.0033428262 -0.0033465589 -0.0033502455 -0.0033541983 -0.0033579597 -0.00336154 -0.0033646238 -0.0033670263 -0.0033676063 -0.0033684818 -0.0033681779 -0.0033671975][-0.0033355535 -0.0033349437 -0.0033367842 -0.0033384974 -0.0033411158 -0.00334395 -0.0033479084 -0.0033520502 -0.0033567133 -0.0033605024 -0.0033635411 -0.0033648091 -0.0033659055 -0.0033660729 -0.0033655136][-0.0033371756 -0.0033355309 -0.0033362184 -0.003336244 -0.0033372417 -0.0033391521 -0.0033422613 -0.0033462893 -0.0033511736 -0.0033555536 -0.0033591785 -0.0033607173 -0.0033621378 -0.00336264 -0.0033624254][-0.0033399973 -0.0033376715 -0.0033370992 -0.0033357688 -0.0033353593 -0.0033357334 -0.0033381372 -0.0033417142 -0.0033460092 -0.0033504206 -0.003354101 -0.0033556966 -0.0033575729 -0.0033588165 -0.0033590312][-0.0033427468 -0.003340092 -0.0033389104 -0.0033364177 -0.0033347392 -0.0033338154 -0.0033351765 -0.0033375015 -0.0033409193 -0.0033445626 -0.0033479081 -0.0033498688 -0.0033519138 -0.0033540688 -0.0033551834][-0.003346507 -0.0033430483 -0.0033409153 -0.003337638 -0.0033345765 -0.0033328268 -0.0033330214 -0.0033340193 -0.0033361823 -0.0033384352 -0.0033405253 -0.0033423917 -0.0033442995 -0.0033468613 -0.0033491219][-0.0033494325 -0.0033458292 -0.0033431733 -0.0033391195 -0.0033354436 -0.0033334235 -0.0033324498 -0.0033322559 -0.0033334338 -0.0033345609 -0.0033357074 -0.0033373174 -0.0033383879 -0.0033404948 -0.0033426939][-0.0033504174 -0.0033464963 -0.0033434685 -0.003339696 -0.0033362268 -0.0033340883 -0.0033329714 -0.0033324552 -0.0033337795 -0.0033346179 -0.0033351847 -0.0033357984 -0.0033355723 -0.0033363763 -0.0033375917][-0.0033509219 -0.0033468618 -0.0033440161 -0.0033407211 -0.0033379672 -0.0033363681 -0.0033357944 -0.003336482 -0.0033382205 -0.003338783 -0.00333848 -0.0033379279 -0.0033360524 -0.0033346119 -0.0033341784][-0.0033503391 -0.0033459158 -0.0033432408 -0.003340512 -0.0033388461 -0.0033381835 -0.0033389525 -0.0033413139 -0.0033436986 -0.0033442643 -0.0033437819 -0.0033424685 -0.0033395176 -0.0033363719 -0.0033344286][-0.0033496921 -0.0033447666 -0.0033419505 -0.0033396266 -0.0033385025 -0.0033387698 -0.0033405016 -0.0033441263 -0.0033473293 -0.0033485927 -0.0033482665 -0.0033463426 -0.0033426806 -0.0033384594 -0.0033352766][-0.0033487994 -0.0033437007 -0.0033408282 -0.0033390308 -0.0033381595 -0.0033388196 -0.0033409619 -0.0033445968 -0.0033478434 -0.0033495133 -0.0033498174 -0.0033483813 -0.0033454325 -0.0033413598 -0.0033380829][-0.0033462711 -0.0033422783 -0.0033399113 -0.003338062 -0.003337218 -0.0033378953 -0.0033394229 -0.0033421749 -0.0033450613 -0.0033468655 -0.0033477198 -0.003347195 -0.00334572 -0.0033426411 -0.0033401002][-0.0033417486 -0.0033393665 -0.0033381667 -0.003336984 -0.0033365136 -0.0033365716 -0.0033371302 -0.0033387933 -0.003341147 -0.0033433016 -0.0033448306 -0.003345638 -0.0033453845 -0.00334386 -0.0033424401][-0.0033348717 -0.0033339898 -0.0033344587 -0.0033352037 -0.003335749 -0.0033360212 -0.0033362529 -0.0033370117 -0.0033380571 -0.0033396159 -0.0033414157 -0.0033428562 -0.0033437938 -0.0033439544 -0.0033439503]]...]
INFO - root - 2017-12-09 11:36:21.590321: step 20510, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:48m:16s remains)
INFO - root - 2017-12-09 11:36:30.257053: step 20520, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 77h:26m:20s remains)
INFO - root - 2017-12-09 11:36:38.765567: step 20530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:38m:16s remains)
INFO - root - 2017-12-09 11:36:47.544541: step 20540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:06m:49s remains)
INFO - root - 2017-12-09 11:36:56.092107: step 20550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:37m:11s remains)
INFO - root - 2017-12-09 11:37:04.691384: step 20560, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 76h:33m:31s remains)
INFO - root - 2017-12-09 11:37:13.400882: step 20570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:30m:17s remains)
INFO - root - 2017-12-09 11:37:22.157117: step 20580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 74h:37m:10s remains)
INFO - root - 2017-12-09 11:37:30.876911: step 20590, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 76h:02m:41s remains)
INFO - root - 2017-12-09 11:37:39.613041: step 20600, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 77h:45m:32s remains)
2017-12-09 11:37:40.487628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00336789 -0.0033667861 -0.0033661244 -0.0033652068 -0.0033645604 -0.0033642189 -0.0033625287 -0.0033593182 -0.0033561394 -0.0033536446 -0.0033520185 -0.0033514255 -0.0033520139 -0.0033534402 -0.0033542102][-0.0033649497 -0.003363983 -0.0033634934 -0.0033630081 -0.0033629921 -0.0033634654 -0.0033624922 -0.0033598612 -0.0033573112 -0.0033556614 -0.0033547767 -0.0033545615 -0.003355332 -0.0033566358 -0.003356941][-0.0033622228 -0.0033618906 -0.0033619015 -0.0033620209 -0.0033622836 -0.0033626119 -0.0033614673 -0.003359332 -0.0033575329 -0.0033570884 -0.0033574163 -0.0033575266 -0.0033580207 -0.003358982 -0.0033588589][-0.0033596812 -0.0033607308 -0.0033616894 -0.0033624733 -0.0033625476 -0.0033620256 -0.0033601767 -0.0033580421 -0.0033568318 -0.0033573897 -0.0033589564 -0.0033594684 -0.0033593532 -0.0033594735 -0.0033586088][-0.0033569564 -0.0033590253 -0.0033608 -0.0033621953 -0.0033623648 -0.0033612917 -0.0033590063 -0.0033572011 -0.0033565045 -0.0033575925 -0.0033595206 -0.0033600205 -0.0033590621 -0.0033581818 -0.0033566346][-0.0033536165 -0.0033559145 -0.0033582486 -0.0033601858 -0.0033606407 -0.003359844 -0.0033582181 -0.0033572749 -0.0033569238 -0.0033580463 -0.0033598123 -0.0033601979 -0.0033585813 -0.003356932 -0.0033549019][-0.0033503273 -0.003352653 -0.0033551566 -0.0033570374 -0.0033576728 -0.0033574221 -0.0033568719 -0.0033569317 -0.0033569315 -0.003357609 -0.003358898 -0.003359013 -0.0033574014 -0.0033558148 -0.0033542113][-0.0033474695 -0.0033497841 -0.0033522027 -0.0033538956 -0.0033548484 -0.0033556416 -0.0033561559 -0.003357219 -0.0033575997 -0.003357722 -0.0033581501 -0.0033576414 -0.003356026 -0.0033546905 -0.0033539701][-0.0033435735 -0.003345364 -0.0033479815 -0.00335009 -0.0033519331 -0.0033539701 -0.0033558826 -0.0033577485 -0.0033579187 -0.0033572265 -0.0033566679 -0.0033556421 -0.0033542924 -0.0033534078 -0.0033534053][-0.0033384529 -0.0033395358 -0.003342309 -0.0033448234 -0.003347534 -0.0033505482 -0.0033532076 -0.0033552288 -0.003355341 -0.0033540179 -0.0033524886 -0.0033513778 -0.0033506844 -0.0033504744 -0.0033513152][-0.0033324459 -0.0033329176 -0.0033354976 -0.0033380624 -0.0033412948 -0.0033448476 -0.0033477293 -0.0033496637 -0.0033496011 -0.0033480318 -0.0033461878 -0.0033450173 -0.0033448373 -0.0033454816 -0.0033470872][-0.0033275953 -0.0033274402 -0.0033296577 -0.0033319839 -0.00333503 -0.0033384294 -0.00334102 -0.0033426723 -0.003342442 -0.0033409975 -0.0033393071 -0.0033380107 -0.0033379039 -0.0033388406 -0.0033407356][-0.0033242474 -0.0033231596 -0.0033246428 -0.0033264873 -0.0033288223 -0.0033314978 -0.0033336626 -0.003334906 -0.0033347353 -0.003333756 -0.0033324359 -0.0033313341 -0.003331156 -0.0033318757 -0.0033333714][-0.0033220712 -0.0033199589 -0.0033207668 -0.0033219431 -0.003323731 -0.0033256577 -0.0033272114 -0.0033280372 -0.0033279387 -0.0033273131 -0.0033265129 -0.0033257995 -0.0033255746 -0.0033260286 -0.0033268589][-0.0033214162 -0.0033185808 -0.0033185461 -0.0033190295 -0.0033201282 -0.0033213107 -0.0033222612 -0.0033226961 -0.0033226488 -0.003322341 -0.0033219005 -0.0033214577 -0.0033211694 -0.003321199 -0.0033215017]]...]
INFO - root - 2017-12-09 11:37:48.990047: step 20610, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:08m:28s remains)
INFO - root - 2017-12-09 11:37:57.791364: step 20620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:08m:15s remains)
INFO - root - 2017-12-09 11:38:06.356720: step 20630, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 76h:35m:31s remains)
INFO - root - 2017-12-09 11:38:15.175208: step 20640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:45m:27s remains)
INFO - root - 2017-12-09 11:38:23.796191: step 20650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:22m:01s remains)
INFO - root - 2017-12-09 11:38:32.512236: step 20660, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:01m:29s remains)
INFO - root - 2017-12-09 11:38:41.260360: step 20670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:36m:07s remains)
INFO - root - 2017-12-09 11:38:49.943663: step 20680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:15m:04s remains)
INFO - root - 2017-12-09 11:38:58.694257: step 20690, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 77h:55m:22s remains)
INFO - root - 2017-12-09 11:39:07.451189: step 20700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:05m:37s remains)
2017-12-09 11:39:08.499795: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0017927657 0.0028570821 0.0034142146 0.0033502188 0.0027779178 0.0019599185 0.0011297655 0.00041983183 -0.00018276926 -0.00071096071 -0.0012035579 -0.0016341953 -0.0019358405 -0.0019894782 -0.0016798221][0.0041134334 0.0056626904 0.0066287541 0.0067855692 0.0061915261 0.0051034275 0.0038542263 0.0026577534 0.0015925528 0.00065881619 -0.00017229398 -0.0008812407 -0.00141097 -0.0016585523 -0.0015390259][0.0067697656 0.0089782141 0.010557376 0.011189377 0.010847716 0.0096944794 0.0080906525 0.0063146842 0.0045814882 0.0029772129 0.0015374846 0.0002989769 -0.00067567243 -0.0013034667 -0.0015355463][0.0092448033 0.012175195 0.014452444 0.015717354 0.01586042 0.014916319 0.013186 0.010963624 0.0085812546 0.0062256097 0.0040272186 0.0020710751 0.00045164255 -0.00074166548 -0.0014707753][0.011179054 0.014711771 0.017571209 0.019412329 0.020076623 0.01950193 0.017880572 0.015484389 0.012697291 0.0097761732 0.0069265855 0.0042807735 0.0019720839 0.00012483937 -0.0011823925][0.012584407 0.016434185 0.019601554 0.021775145 0.022790466 0.022554552 0.0211611 0.018839564 0.015952582 0.012777923 0.0095564071 0.0064446633 0.0035890769 0.0011540712 -0.00069756689][0.01365507 0.017487064 0.02062916 0.022819106 0.023930889 0.023891183 0.022731818 0.020630728 0.017876409 0.014725795 0.011414735 0.0080937035 0.0049088295 0.002065877 -0.00018734881][0.014423568 0.01795928 0.020819373 0.022788858 0.023796713 0.023816269 0.022860561 0.021047534 0.018568274 0.015638527 0.012459436 0.0091426857 0.005827527 0.0027606245 0.00026295241][0.014751915 0.017834233 0.020293724 0.021948604 0.022766866 0.022769846 0.021979805 0.020472988 0.018359156 0.015792921 0.012909055 0.0097695682 0.0065077008 0.0033977819 0.00079823239][0.014505779 0.017075136 0.019128375 0.020490112 0.021118186 0.021068558 0.020389732 0.019162238 0.017455107 0.015352655 0.012900477 0.010115508 0.0071081282 0.0041538328 0.0016083214][0.013785461 0.015818426 0.017480604 0.018577343 0.019017767 0.01887271 0.018231196 0.017210383 0.015874773 0.014253886 0.012319589 0.010042591 0.0075011454 0.00493048 0.0026440294][0.01288043 0.014355872 0.015583389 0.016367422 0.016576137 0.01627933 0.015594874 0.014682837 0.013636254 0.012440702 0.011035185 0.0093444828 0.0074066878 0.0053956765 0.0035603251][0.011894498 0.012836155 0.013604242 0.014025766 0.013961818 0.013470121 0.01268613 0.011798227 0.01093615 0.010066058 0.0091268066 0.0080019273 0.006690349 0.0053078863 0.004020324][0.010847239 0.011302747 0.011620734 0.011674577 0.011352267 0.010699516 0.0098431744 0.0089718625 0.0082381219 0.0076096826 0.0070293495 0.0063650026 0.005587393 0.0047635357 0.00397829][0.00973695 0.0097942948 0.009736985 0.0094964746 0.0089999773 0.00827281 0.0074165468 0.00659496 0.0059581609 0.0055064461 0.0051860246 0.0048670056 0.0044911155 0.0040805773 0.00366217]]...]
INFO - root - 2017-12-09 11:39:17.130133: step 20710, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 73h:55m:50s remains)
INFO - root - 2017-12-09 11:39:25.665828: step 20720, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 73h:34m:17s remains)
INFO - root - 2017-12-09 11:39:34.257662: step 20730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:23m:02s remains)
INFO - root - 2017-12-09 11:39:42.909727: step 20740, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 72h:19m:10s remains)
INFO - root - 2017-12-09 11:39:51.494342: step 20750, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 75h:13m:50s remains)
INFO - root - 2017-12-09 11:40:00.187189: step 20760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:57m:21s remains)
INFO - root - 2017-12-09 11:40:08.931568: step 20770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:16m:02s remains)
INFO - root - 2017-12-09 11:40:17.779145: step 20780, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 73h:33m:13s remains)
INFO - root - 2017-12-09 11:40:26.463294: step 20790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:03m:31s remains)
INFO - root - 2017-12-09 11:40:35.275741: step 20800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:02m:39s remains)
2017-12-09 11:40:36.128281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033138709 -0.0033105544 -0.0033100445 -0.0033098916 -0.0033097223 -0.0033096743 -0.0033096687 -0.0033097526 -0.0033098911 -0.003310051 -0.0033102089 -0.0033103141 -0.0033103845 -0.0033103661 -0.0033103214][-0.0033117349 -0.003308235 -0.0033077174 -0.0033074471 -0.003307068 -0.0033067821 -0.0033065607 -0.0033065225 -0.0033066759 -0.0033069854 -0.0033073165 -0.0033075067 -0.0033076028 -0.0033076035 -0.0033075679][-0.0033118562 -0.0033082778 -0.0033076874 -0.0033072026 -0.0033065276 -0.0033059421 -0.0033054699 -0.0033053409 -0.0033056336 -0.0033062319 -0.0033068415 -0.0033072326 -0.0033074727 -0.0033075539 -0.0033075381][-0.0033115831 -0.0033079172 -0.003307167 -0.0033064377 -0.0033054943 -0.0033046885 -0.003304065 -0.003303902 -0.0033044082 -0.0033053632 -0.0033062834 -0.0033068918 -0.0033073092 -0.0033075102 -0.0033075386][-0.0033111591 -0.0033073132 -0.0033063679 -0.0033054089 -0.0033042522 -0.0033032792 -0.0033025648 -0.0033024168 -0.0033030971 -0.003304329 -0.0033055344 -0.0033063318 -0.0033069237 -0.0033072785 -0.0033073863][-0.0033108685 -0.0033068131 -0.0033056852 -0.0033045434 -0.0033032314 -0.0033021441 -0.0033014198 -0.0033014165 -0.003302274 -0.0033036473 -0.003304956 -0.0033058187 -0.0033064948 -0.0033069444 -0.0033071202][-0.0033105845 -0.0033063863 -0.0033052173 -0.003304047 -0.0033026959 -0.0033015611 -0.0033008673 -0.0033010482 -0.0033019918 -0.0033033784 -0.0033046717 -0.0033055278 -0.0033062145 -0.003306705 -0.0033069346][-0.0033103614 -0.0033061383 -0.0033049916 -0.0033039362 -0.003302777 -0.00330176 -0.0033011506 -0.0033013923 -0.0033022864 -0.0033035097 -0.0033046566 -0.0033054457 -0.0033060862 -0.0033065567 -0.0033067956][-0.0033103772 -0.0033062929 -0.0033053802 -0.0033045504 -0.0033037281 -0.0033030217 -0.00330262 -0.0033028317 -0.0033034745 -0.0033043369 -0.0033051921 -0.0033058033 -0.0033063195 -0.0033067036 -0.0033069113][-0.0033105807 -0.0033066496 -0.0033060785 -0.0033055192 -0.0033050268 -0.0033046489 -0.00330447 -0.0033046827 -0.0033050606 -0.0033055404 -0.003306027 -0.0033063842 -0.0033067048 -0.0033069362 -0.0033070818][-0.0033107793 -0.0033069705 -0.0033068068 -0.0033065251 -0.0033062366 -0.0033059982 -0.0033059067 -0.0033060827 -0.0033062838 -0.0033064832 -0.0033067379 -0.0033069553 -0.0033071246 -0.003307218 -0.0033072818][-0.0033108748 -0.0033071828 -0.0033072622 -0.0033071879 -0.003307048 -0.0033068606 -0.0033067623 -0.0033068322 -0.0033069123 -0.0033069858 -0.0033071293 -0.0033072748 -0.0033073828 -0.0033074329 -0.0033074573][-0.0033109642 -0.0033072482 -0.0033074357 -0.0033074734 -0.0033074513 -0.0033073288 -0.003307231 -0.0033072284 -0.0033072559 -0.0033072638 -0.0033073262 -0.0033074131 -0.0033074897 -0.0033075239 -0.0033075279][-0.0033110569 -0.0033072289 -0.0033074457 -0.0033075272 -0.0033075667 -0.0033075197 -0.0033074429 -0.0033074087 -0.0033073958 -0.0033073875 -0.0033074087 -0.003307445 -0.0033074904 -0.0033075185 -0.003307519][-0.0033111323 -0.0033072 -0.003307319 -0.0033073963 -0.0033074527 -0.0033074589 -0.0033074256 -0.0033074121 -0.0033074124 -0.0033074107 -0.0033074133 -0.0033074236 -0.0033074454 -0.0033074599 -0.0033074552]]...]
INFO - root - 2017-12-09 11:40:44.587010: step 20810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:05m:45s remains)
INFO - root - 2017-12-09 11:40:53.354815: step 20820, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:17m:40s remains)
INFO - root - 2017-12-09 11:41:01.883974: step 20830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:03m:39s remains)
INFO - root - 2017-12-09 11:41:10.613032: step 20840, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:10m:14s remains)
INFO - root - 2017-12-09 11:41:19.012355: step 20850, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 71h:10m:50s remains)
INFO - root - 2017-12-09 11:41:27.679995: step 20860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:13m:05s remains)
INFO - root - 2017-12-09 11:41:36.254873: step 20870, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 73h:51m:42s remains)
INFO - root - 2017-12-09 11:41:44.906994: step 20880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:59m:55s remains)
INFO - root - 2017-12-09 11:41:53.450494: step 20890, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 72h:25m:54s remains)
INFO - root - 2017-12-09 11:42:02.029362: step 20900, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 75h:46m:44s remains)
2017-12-09 11:42:02.878479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033416147 -0.0033351239 -0.0033303732 -0.0033267324 -0.0033243503 -0.0033231776 -0.0033229317 -0.0033229964 -0.0033230223 -0.0033229666 -0.0033229904 -0.0033228365 -0.0033225964 -0.0033224525 -0.0033223382][-0.003338818 -0.0033325569 -0.0033285164 -0.0033256127 -0.0033237 -0.0033227908 -0.0033226376 -0.0033225191 -0.00332225 -0.0033218991 -0.0033215 -0.0033210488 -0.0033207154 -0.0033206025 -0.0033205822][-0.0033377451 -0.0033322792 -0.0033292596 -0.0033270905 -0.0033257643 -0.0033252351 -0.0033252807 -0.0033249927 -0.0033243268 -0.0033235769 -0.0033225694 -0.0033215352 -0.0033208716 -0.0033205433 -0.0033204381][-0.0033347651 -0.0033304936 -0.0033288056 -0.0033280752 -0.00332817 -0.0033287015 -0.0033292442 -0.0033289911 -0.0033278789 -0.003326321 -0.0033244407 -0.0033225748 -0.0033212691 -0.003320581 -0.0033203841][-0.0033313683 -0.0033282836 -0.0033281229 -0.0033290831 -0.0033307797 -0.0033325907 -0.0033339763 -0.0033342475 -0.0033330913 -0.0033308642 -0.0033279057 -0.0033248563 -0.0033225687 -0.0033211745 -0.0033205012][-0.0033284565 -0.0033263497 -0.0033274968 -0.0033297497 -0.0033327355 -0.003335783 -0.0033380196 -0.003338868 -0.0033379 -0.0033353055 -0.0033315057 -0.0033274193 -0.0033241296 -0.0033220225 -0.003320856][-0.0033267462 -0.0033249555 -0.0033267338 -0.0033298798 -0.0033339697 -0.0033380643 -0.0033408725 -0.0033418855 -0.0033410806 -0.0033383539 -0.0033340049 -0.0033293173 -0.0033253659 -0.0033227645 -0.0033212865][-0.0033259646 -0.0033242553 -0.0033262081 -0.0033297006 -0.0033341781 -0.003338705 -0.0033416206 -0.0033428639 -0.003342241 -0.0033393041 -0.0033346454 -0.0033298999 -0.0033259396 -0.003323192 -0.0033215291][-0.0033255154 -0.0033238449 -0.0033257271 -0.0033288919 -0.0033329893 -0.003337305 -0.0033402916 -0.0033417805 -0.0033414199 -0.0033385989 -0.003334142 -0.003329755 -0.0033260202 -0.003323395 -0.00332169][-0.0033253294 -0.0033233608 -0.0033248896 -0.003327267 -0.0033304547 -0.0033341828 -0.0033370319 -0.00333861 -0.0033385325 -0.0033364894 -0.0033328405 -0.0033290731 -0.0033257313 -0.0033233417 -0.0033218211][-0.0033251916 -0.0033228521 -0.0033239957 -0.0033256614 -0.0033279071 -0.003330454 -0.0033324577 -0.0033335967 -0.0033335956 -0.00333237 -0.0033299415 -0.003327287 -0.0033249119 -0.0033231617 -0.0033220546][-0.0033251685 -0.0033225184 -0.0033231904 -0.0033241608 -0.0033254588 -0.0033269643 -0.0033281725 -0.0033288221 -0.0033288072 -0.0033281483 -0.0033268149 -0.0033252733 -0.003323908 -0.0033229245 -0.0033222674][-0.0033252691 -0.0033223636 -0.003322683 -0.0033230465 -0.0033235615 -0.0033242132 -0.0033247657 -0.0033250975 -0.0033251483 -0.0033249245 -0.0033243746 -0.0033236989 -0.0033230595 -0.0033225992 -0.0033222521][-0.0033254903 -0.0033224002 -0.0033225524 -0.0033226183 -0.003322779 -0.0033229962 -0.003323186 -0.0033233555 -0.003323409 -0.0033233298 -0.0033230975 -0.0033228351 -0.003322585 -0.0033224036 -0.0033222735][-0.0033257254 -0.0033225205 -0.0033225603 -0.003322555 -0.003322585 -0.0033226474 -0.0033226896 -0.003322741 -0.0033227385 -0.0033226702 -0.0033225811 -0.0033225035 -0.0033224404 -0.0033223869 -0.0033223652]]...]
INFO - root - 2017-12-09 11:42:11.449061: step 20910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:48m:19s remains)
INFO - root - 2017-12-09 11:42:20.105279: step 20920, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:59m:08s remains)
INFO - root - 2017-12-09 11:42:28.652072: step 20930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:47m:30s remains)
INFO - root - 2017-12-09 11:42:37.301505: step 20940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:19m:43s remains)
INFO - root - 2017-12-09 11:42:45.810621: step 20950, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 66h:39m:35s remains)
INFO - root - 2017-12-09 11:42:54.351939: step 20960, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 77h:32m:04s remains)
INFO - root - 2017-12-09 11:43:03.000189: step 20970, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:27m:07s remains)
INFO - root - 2017-12-09 11:43:11.617746: step 20980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:04m:50s remains)
INFO - root - 2017-12-09 11:43:20.283478: step 20990, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:03m:38s remains)
INFO - root - 2017-12-09 11:43:29.012206: step 21000, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:22m:31s remains)
2017-12-09 11:43:30.060034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003366478 -0.0033649583 -0.0033642717 -0.003364064 -0.0033644405 -0.003365255 -0.0033664892 -0.0033673733 -0.0033672878 -0.0033645141 -0.0033582724 -0.0033477019 -0.003334807 -0.0033203559 -0.0033045134][-0.003365211 -0.0033636512 -0.003362776 -0.003362149 -0.0033617846 -0.0033617003 -0.0033623152 -0.0033633788 -0.0033655162 -0.0033659651 -0.0033642671 -0.0033592961 -0.003352361 -0.0033437319 -0.0033338906][-0.0033650482 -0.0033639011 -0.0033630305 -0.0033618284 -0.0033602002 -0.0033580854 -0.0033562821 -0.0033557341 -0.0033590044 -0.0033628526 -0.0033653686 -0.0033644293 -0.0033612219 -0.0033572479 -0.0033521152][-0.0033653916 -0.003364692 -0.0033637278 -0.0033614952 -0.0033573078 -0.0033508465 -0.0033441486 -0.0033408629 -0.0033453256 -0.0033534933 -0.0033607411 -0.0033640445 -0.0033647963 -0.0033635781 -0.0033610857][-0.0033659309 -0.0033657125 -0.0033645097 -0.0033605169 -0.0033526283 -0.003340248 -0.0033265259 -0.0033187843 -0.0033244495 -0.0033389223 -0.0033518549 -0.0033588829 -0.0033620885 -0.0033630596 -0.003362528][-0.0033663488 -0.003366753 -0.0033653821 -0.0033597904 -0.0033483305 -0.0033301155 -0.0033099966 -0.0032982172 -0.003302956 -0.0033223412 -0.0033417596 -0.0033529382 -0.0033576414 -0.0033587837 -0.0033585515][-0.0033669602 -0.0033677714 -0.0033662552 -0.0033599194 -0.0033460981 -0.003324345 -0.0033007551 -0.0032863677 -0.0032891107 -0.0033092881 -0.0033330759 -0.0033486 -0.0033551008 -0.0033558814 -0.0033551317][-0.0033679369 -0.0033689488 -0.0033676154 -0.0033614936 -0.0033478858 -0.0033270079 -0.0033041839 -0.0032889848 -0.0032886609 -0.0033056415 -0.0033294759 -0.0033463303 -0.003353744 -0.0033548141 -0.0033540484][-0.0033691677 -0.0033699111 -0.003368702 -0.0033636119 -0.0033526542 -0.0033367805 -0.0033194991 -0.0033071947 -0.0033051993 -0.0033154534 -0.0033332198 -0.0033474721 -0.0033548214 -0.0033559885 -0.0033550067][-0.003369641 -0.003370421 -0.003369577 -0.0033658608 -0.0033585948 -0.0033485095 -0.0033375919 -0.00332996 -0.0033283802 -0.0033333991 -0.0033433274 -0.0033520993 -0.0033570887 -0.0033572728 -0.0033552337][-0.0033696038 -0.0033702727 -0.0033700264 -0.00336812 -0.0033639439 -0.0033579338 -0.0033519194 -0.0033477922 -0.0033472779 -0.0033496623 -0.0033538614 -0.0033574854 -0.0033589534 -0.0033563254 -0.0033517806][-0.0033691847 -0.0033696671 -0.0033699388 -0.0033693255 -0.0033671847 -0.003363912 -0.0033601434 -0.0033577415 -0.0033576719 -0.0033590614 -0.0033606486 -0.0033605576 -0.0033581394 -0.0033519962 -0.0033442131][-0.0033688019 -0.0033689647 -0.0033693791 -0.0033694929 -0.0033686156 -0.0033670836 -0.0033649879 -0.0033640293 -0.0033646545 -0.0033657386 -0.0033659826 -0.0033636258 -0.0033576109 -0.003347493 -0.0033363125][-0.0033688736 -0.003368753 -0.0033688815 -0.0033690056 -0.0033686578 -0.0033679616 -0.0033668794 -0.0033667707 -0.0033680918 -0.0033696217 -0.003369564 -0.0033662089 -0.0033586267 -0.0033465493 -0.0033345011][-0.0033689516 -0.0033688382 -0.0033687081 -0.003368397 -0.0033674687 -0.0033663718 -0.0033656124 -0.003365963 -0.0033679218 -0.0033704168 -0.0033711519 -0.0033684126 -0.0033620142 -0.0033521252 -0.0033421549]]...]
INFO - root - 2017-12-09 11:43:38.446882: step 21010, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 76h:15m:06s remains)
INFO - root - 2017-12-09 11:43:47.110420: step 21020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 71h:15m:36s remains)
INFO - root - 2017-12-09 11:43:55.794197: step 21030, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 64h:09m:12s remains)
INFO - root - 2017-12-09 11:44:04.351577: step 21040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:11m:33s remains)
INFO - root - 2017-12-09 11:44:13.011136: step 21050, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 71h:04m:37s remains)
INFO - root - 2017-12-09 11:44:21.473244: step 21060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:49m:18s remains)
INFO - root - 2017-12-09 11:44:30.159373: step 21070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:35m:02s remains)
INFO - root - 2017-12-09 11:44:38.761458: step 21080, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:34m:20s remains)
INFO - root - 2017-12-09 11:44:47.458641: step 21090, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 73h:30m:29s remains)
INFO - root - 2017-12-09 11:44:56.052725: step 21100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:31m:44s remains)
2017-12-09 11:44:57.001431: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43892786 0.42684051 0.41476423 0.40230167 0.389862 0.38190645 0.37588918 0.37072188 0.36575809 0.363907 0.3647331 0.36663112 0.36746737 0.36692554 0.36422712][0.44427279 0.43246967 0.42088509 0.40897608 0.39627767 0.38808933 0.38207832 0.37884751 0.37553808 0.37608638 0.37985167 0.38336456 0.3857801 0.38616374 0.38433498][0.44453505 0.43551669 0.42623255 0.41549543 0.40383473 0.39465427 0.38755494 0.38379493 0.38016951 0.38161415 0.38649136 0.39176804 0.39592797 0.39729944 0.39646482][0.44873819 0.44261774 0.43407398 0.42386377 0.41200432 0.402145 0.39286342 0.38678351 0.38237742 0.38382447 0.3896513 0.39666831 0.40284458 0.40660653 0.40744248][0.44986355 0.44781184 0.44072896 0.42985117 0.4175052 0.40624475 0.39499435 0.38683206 0.38114125 0.38276377 0.38932881 0.39822623 0.40692881 0.41331825 0.41607389][0.44527784 0.44862264 0.44435635 0.43522605 0.42218736 0.40925622 0.39632297 0.38542655 0.37774768 0.37861049 0.38566059 0.39583898 0.4067556 0.41589436 0.42076534][0.44227293 0.4494037 0.4460063 0.43743807 0.42425844 0.41037378 0.39536193 0.38310561 0.37408367 0.37348151 0.38013434 0.39142 0.40440297 0.41525659 0.42209628][0.43658906 0.44893306 0.44758993 0.43970045 0.42683265 0.41169325 0.39479232 0.37978393 0.36844313 0.36581054 0.370975 0.38253734 0.39685446 0.40942395 0.41803473][0.42703176 0.44240379 0.44192415 0.43599182 0.42471823 0.4105987 0.3944335 0.37848726 0.36647713 0.36192262 0.3653135 0.37599257 0.39014527 0.40365371 0.41339946][0.41390187 0.43153939 0.43277878 0.42837039 0.41965893 0.40700769 0.39224061 0.37821367 0.36702535 0.36243588 0.36441296 0.37342855 0.38613307 0.39862639 0.40797028][0.39864269 0.41765106 0.42050257 0.41860849 0.41277924 0.40338728 0.39219752 0.38086286 0.37134728 0.3671993 0.36790696 0.37465215 0.38474509 0.39492652 0.40277657][0.38451612 0.40387812 0.40756443 0.40729076 0.40454036 0.3991448 0.3915436 0.38336867 0.37618855 0.37327915 0.37317306 0.37757036 0.38458198 0.39190072 0.39769813][0.370946 0.3905578 0.39546132 0.39721766 0.39744806 0.39531228 0.39141646 0.38601288 0.38079607 0.37893063 0.37851134 0.3807393 0.38457021 0.38879022 0.39223275][0.36052504 0.37887225 0.38301644 0.3863672 0.38896695 0.38994822 0.38972816 0.38796502 0.38596782 0.38533676 0.3848753 0.38540456 0.38642028 0.38768739 0.38868767][0.34993365 0.36741665 0.37117565 0.37441608 0.37751275 0.38017005 0.38218564 0.38336805 0.38406411 0.38506967 0.38540611 0.38499543 0.38414517 0.38329077 0.38244775]]...]
INFO - root - 2017-12-09 11:45:05.620116: step 21110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:23m:23s remains)
INFO - root - 2017-12-09 11:45:14.340969: step 21120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:01m:09s remains)
INFO - root - 2017-12-09 11:45:22.839974: step 21130, loss = 0.89, batch loss = 0.68 (11.0 examples/sec; 0.730 sec/batch; 63h:09m:02s remains)
INFO - root - 2017-12-09 11:45:31.575081: step 21140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:48m:00s remains)
INFO - root - 2017-12-09 11:45:40.255708: step 21150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:44m:24s remains)
INFO - root - 2017-12-09 11:45:48.717970: step 21160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:26m:53s remains)
INFO - root - 2017-12-09 11:45:57.457559: step 21170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:48m:44s remains)
INFO - root - 2017-12-09 11:46:06.117075: step 21180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:12m:23s remains)
INFO - root - 2017-12-09 11:46:14.770213: step 21190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:35m:02s remains)
INFO - root - 2017-12-09 11:46:23.418100: step 21200, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 73h:26m:58s remains)
2017-12-09 11:46:24.304926: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0040487051 0.0042187031 0.0045965826 0.0051773861 0.0058579408 0.0066595832 0.0075066341 0.0083549162 0.0090660388 0.0097694118 0.010576625 0.011397025 0.011985496 0.012140008 0.011775452][0.0074182851 0.0088763954 0.010555417 0.012322497 0.013995236 0.015548265 0.016900323 0.018046139 0.018800505 0.019330427 0.019877203 0.020457428 0.020783404 0.020552117 0.019709287][0.012104832 0.015600471 0.019244319 0.022602696 0.025389386 0.027622573 0.029341459 0.030642498 0.031413388 0.031827517 0.032164771 0.0324562 0.032290708 0.031223986 0.029303379][0.01793178 0.02345915 0.028934088 0.033689607 0.03733287 0.039907057 0.041661736 0.042822432 0.043419804 0.043589812 0.043583449 0.043486435 0.042880591 0.041249227 0.038575787][0.023801392 0.030965023 0.0374685 0.042696018 0.046546288 0.049143218 0.0508324 0.051802907 0.052172165 0.052053548 0.051671732 0.051122528 0.050062042 0.048021231 0.04490139][0.028057318 0.035893194 0.042740889 0.047983363 0.051581856 0.053872827 0.055452429 0.0563703 0.056668378 0.0563826 0.055660505 0.054629311 0.053130273 0.050860871 0.047586292][0.029665835 0.037246808 0.043702073 0.048633154 0.0520255 0.0542246 0.055828642 0.056820434 0.05713892 0.056693748 0.05567643 0.0542911 0.052492615 0.050034385 0.046618991][0.027792092 0.034572404 0.040270332 0.044700105 0.047851168 0.050103478 0.051950507 0.053241663 0.053795341 0.053449087 0.052390609 0.050773483 0.048625272 0.045811836 0.042120703][0.022648839 0.028257892 0.033042192 0.036955807 0.039919157 0.042229407 0.044206221 0.045721062 0.046597648 0.046569142 0.045656607 0.043930024 0.0414179 0.038110051 0.034008749][0.015422158 0.019601952 0.023368565 0.026725411 0.029492941 0.03178902 0.033713706 0.035201292 0.036169264 0.036384221 0.03571292 0.034021676 0.031332213 0.027770987 0.023530513][0.00783174 0.010597845 0.013354847 0.016051354 0.018483842 0.02057828 0.022225503 0.023375778 0.024054779 0.024197569 0.023672655 0.022233449 0.019833395 0.016613211 0.012887206][0.0016703818 0.0032050789 0.0049481979 0.0068358583 0.0086847944 0.010304241 0.011486573 0.012163796 0.012435323 0.012383442 0.011974399 0.010985034 0.0093229618 0.0070760623 0.0045371205][-0.0018127193 -0.0011763619 -0.00035703345 0.00062532607 0.0016684511 0.0026131051 0.0032776368 0.0035972304 0.0036614744 0.0035735124 0.0033602433 0.0028752491 0.002035829 0.00088840793 -0.00038055796][-0.00317282 -0.002997915 -0.0027420714 -0.0024065748 -0.0020191805 -0.0016443396 -0.0013668332 -0.0012287323 -0.0012048439 -0.0012436723 -0.0013140619 -0.0014755634 -0.0017714128 -0.0021771316 -0.0026068098][-0.0033473074 -0.0033392727 -0.0033194714 -0.00327867 -0.0032178215 -0.0031500109 -0.0030957488 -0.0030647209 -0.0030490146 -0.0030383707 -0.0030322701 -0.0030442928 -0.003084233 -0.0031491965 -0.0032202797]]...]
INFO - root - 2017-12-09 11:46:32.776460: step 21210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:40m:09s remains)
INFO - root - 2017-12-09 11:46:41.459510: step 21220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:49m:07s remains)
INFO - root - 2017-12-09 11:46:49.978799: step 21230, loss = 0.91, batch loss = 0.70 (11.8 examples/sec; 0.679 sec/batch; 58h:41m:52s remains)
INFO - root - 2017-12-09 11:46:58.621403: step 21240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:33m:03s remains)
INFO - root - 2017-12-09 11:47:07.360447: step 21250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:28m:43s remains)
INFO - root - 2017-12-09 11:47:15.885910: step 21260, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 78h:07m:09s remains)
INFO - root - 2017-12-09 11:47:24.433309: step 21270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:59m:38s remains)
INFO - root - 2017-12-09 11:47:32.978840: step 21280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:23m:58s remains)
INFO - root - 2017-12-09 11:47:41.450720: step 21290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 74h:19m:36s remains)
INFO - root - 2017-12-09 11:47:49.993819: step 21300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:32m:48s remains)
2017-12-09 11:47:50.819794: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14369273 0.14060481 0.14484669 0.15396152 0.16685106 0.18084659 0.19376567 0.1974626 0.18830714 0.16546397 0.13250414 0.09570875 0.061306432 0.034655634 0.017179647][0.12286356 0.12373599 0.13207296 0.14530247 0.16175658 0.17853735 0.1933174 0.19831204 0.18963845 0.1663783 0.13279569 0.095641479 0.061063305 0.034335461 0.01688938][0.1035149 0.10906821 0.12240066 0.14060801 0.16102022 0.17980132 0.19503006 0.19983013 0.19058445 0.16645858 0.1321808 0.094878718 0.06052459 0.034046452 0.016706774][0.090825386 0.10025056 0.1179639 0.14087039 0.16515541 0.18631385 0.20178939 0.20493522 0.19394822 0.16808961 0.13274169 0.094892077 0.060480066 0.034135457 0.016727023][0.087434851 0.098458193 0.11809838 0.14296293 0.16893214 0.19080751 0.2058282 0.20831591 0.196623 0.16984224 0.13367799 0.095380522 0.060973212 0.034607526 0.017009584][0.0874031 0.09994214 0.12081354 0.14650941 0.17241633 0.19330882 0.20680143 0.2077646 0.19528817 0.16845509 0.13290323 0.095184274 0.061158635 0.035015464 0.017356765][0.087715812 0.099922128 0.12035853 0.14643346 0.17221349 0.19217089 0.20418254 0.20382455 0.19058292 0.16380382 0.12894578 0.092326805 0.059620805 0.03449804 0.017345609][0.089101896 0.098828837 0.11629117 0.13912284 0.16212623 0.18025005 0.19148248 0.19149226 0.17956859 0.15511006 0.12279537 0.088471629 0.05740542 0.033341281 0.016856657][0.097328454 0.10094192 0.11221924 0.13031745 0.14971052 0.1654688 0.17541094 0.17608248 0.16624931 0.14478169 0.11546671 0.083859004 0.055015892 0.0322764 0.016434958][0.11304739 0.11104777 0.11556815 0.12705643 0.14106178 0.15372869 0.16258435 0.1639466 0.15570557 0.13682383 0.1099503 0.080327846 0.052922316 0.031233799 0.016112657][0.13098253 0.12523906 0.12479411 0.13160606 0.14114013 0.15009974 0.15685542 0.15783325 0.15021333 0.13261841 0.10703763 0.078548454 0.051843114 0.030595496 0.015819719][0.14838906 0.13963224 0.13533378 0.13823138 0.14511862 0.15241313 0.15774147 0.15775108 0.14957084 0.13198951 0.10641595 0.078027822 0.051431075 0.030207034 0.015548533][0.16716793 0.15502669 0.14616846 0.14500812 0.1490998 0.15457974 0.15915886 0.15914442 0.15112904 0.1332393 0.10708709 0.078189574 0.051230837 0.029868253 0.015244512][0.18156148 0.16891791 0.15829481 0.15451537 0.15577391 0.1588885 0.16170472 0.1602075 0.15115565 0.13269337 0.10645915 0.077560879 0.050677884 0.029520132 0.015017224][0.18939629 0.1779152 0.16753797 0.16239394 0.16158597 0.16256358 0.1629616 0.15937825 0.1485005 0.12913719 0.10289156 0.074428819 0.048403054 0.028174572 0.01448255]]...]
INFO - root - 2017-12-09 11:47:59.519942: step 21310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:17m:11s remains)
INFO - root - 2017-12-09 11:48:08.115322: step 21320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:59m:51s remains)
INFO - root - 2017-12-09 11:48:16.852212: step 21330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 75h:26m:26s remains)
INFO - root - 2017-12-09 11:48:25.405596: step 21340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:17m:24s remains)
INFO - root - 2017-12-09 11:48:34.132131: step 21350, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 73h:12m:34s remains)
INFO - root - 2017-12-09 11:48:42.934623: step 21360, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 73h:07m:59s remains)
INFO - root - 2017-12-09 11:48:51.637617: step 21370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:52m:44s remains)
INFO - root - 2017-12-09 11:49:00.374709: step 21380, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 76h:26m:05s remains)
INFO - root - 2017-12-09 11:49:09.141653: step 21390, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 75h:54m:29s remains)
INFO - root - 2017-12-09 11:49:17.838306: step 21400, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:38m:07s remains)
2017-12-09 11:49:18.739374: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0058167162 0.0029778243 0.000575779 -0.00086269923 -0.0016712286 -0.0017366788 -0.001329405 -0.00013462687 0.0019745105 0.0047041136 0.0075163017 0.010118553 0.011679929 0.01170698 0.010031335][0.010240419 0.0063752043 0.0030965344 0.0011747577 0.00013905345 5.5429991e-05 0.00048797973 0.0020716167 0.0049242009 0.0086844843 0.012524921 0.015975639 0.017984416 0.018000951 0.015721412][0.01464198 0.010083996 0.0061084968 0.0038064928 0.0026562461 0.0026563415 0.0033161067 0.0055526248 0.00941913 0.014291029 0.019039745 0.023076562 0.025231929 0.024954712 0.021898644][0.018064044 0.013179699 0.0091026668 0.0070398632 0.0063212807 0.006662345 0.0078456923 0.010908825 0.015851656 0.021693645 0.027021937 0.031166418 0.0329369 0.031933002 0.027900597][0.020658102 0.015751055 0.011922812 0.010491801 0.01078465 0.012113065 0.014409164 0.018543312 0.024609363 0.031171015 0.036597662 0.04023261 0.041004524 0.038773671 0.033472095][0.022625662 0.018052772 0.014874626 0.014315385 0.015850697 0.018400975 0.022147374 0.027510593 0.034682769 0.0417474 0.046954602 0.049601194 0.048936754 0.045157257 0.038385883][0.024137471 0.020086179 0.01766848 0.01799644 0.020723328 0.024407664 0.029424654 0.035800274 0.043911353 0.051321931 0.056271367 0.057970665 0.055925481 0.050656512 0.042460579][0.025280681 0.021829735 0.020016996 0.021025436 0.024612717 0.029068423 0.03483336 0.041589566 0.050049372 0.057542223 0.062359523 0.0634914 0.060613487 0.054384567 0.045168635][0.02575681 0.022756288 0.021272175 0.022511065 0.026305584 0.03096495 0.036829282 0.043390807 0.051603895 0.058834385 0.063557737 0.064617008 0.061632019 0.055194065 0.045638766][0.024995537 0.022250645 0.020678587 0.021575751 0.024907356 0.029090615 0.034369618 0.040286303 0.047812473 0.054620285 0.059307855 0.060699094 0.058259606 0.052362878 0.043254178][0.022361904 0.019723687 0.017853614 0.018076137 0.020449147 0.023704916 0.028008988 0.032981467 0.039434794 0.04552637 0.050051052 0.051892705 0.050348133 0.045539759 0.037617568][0.017899033 0.01542205 0.013340171 0.012875214 0.014147777 0.01627693 0.019386135 0.023254232 0.028433407 0.033572085 0.037667848 0.03974615 0.039031029 0.035475332 0.029204035][0.012272233 0.010164905 0.0082022026 0.0073409961 0.0077444944 0.0088951159 0.010864804 0.013552528 0.017293446 0.021239001 0.024595452 0.026580662 0.026484139 0.024150509 0.019704005][0.0066326135 0.0050768559 0.003535396 0.0026408294 0.0025582719 0.0030443463 0.0041392259 0.0058098496 0.0082177874 0.010887619 0.013274357 0.014822297 0.014990088 0.013634501 0.010864332][0.0020766924 0.0010997462 8.7616732e-05 -0.00059510116 -0.00082700863 -0.00070362538 -0.00019694516 0.0006944458 0.0020422423 0.0036199428 0.0050935983 0.006106039 0.0063184616 0.0056270245 0.0041247951]]...]
INFO - root - 2017-12-09 11:49:27.193279: step 21410, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 72h:38m:21s remains)
INFO - root - 2017-12-09 11:49:36.000908: step 21420, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:51m:45s remains)
INFO - root - 2017-12-09 11:49:44.617370: step 21430, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 73h:01m:30s remains)
INFO - root - 2017-12-09 11:49:53.129803: step 21440, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 71h:51m:03s remains)
INFO - root - 2017-12-09 11:50:01.822820: step 21450, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 74h:37m:18s remains)
INFO - root - 2017-12-09 11:50:10.343025: step 21460, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 73h:13m:42s remains)
INFO - root - 2017-12-09 11:50:19.004631: step 21470, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:44m:54s remains)
INFO - root - 2017-12-09 11:50:27.640398: step 21480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:41m:37s remains)
INFO - root - 2017-12-09 11:50:36.251548: step 21490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:48m:58s remains)
INFO - root - 2017-12-09 11:50:44.751725: step 21500, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 74h:38m:22s remains)
2017-12-09 11:50:45.563886: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35658377 0.37246695 0.38210469 0.38335964 0.37931553 0.37080067 0.36220571 0.35301057 0.34556863 0.34108078 0.33644339 0.33078206 0.32160598 0.30887327 0.29297975][0.37009409 0.38898647 0.40003109 0.40206331 0.39844045 0.39095339 0.3840909 0.37716305 0.37318972 0.37196913 0.37044773 0.3656598 0.35602117 0.34236455 0.32425982][0.37276962 0.39386758 0.40615529 0.40816274 0.40352866 0.39628145 0.39175093 0.38835689 0.38848251 0.39054817 0.391615 0.38788605 0.37756604 0.36230442 0.34180075][0.37066764 0.39329839 0.40543661 0.40689996 0.40160063 0.39378855 0.38978234 0.38954127 0.39276686 0.39704308 0.40032226 0.39775014 0.3876569 0.37114361 0.34953275][0.36510184 0.38914061 0.40094826 0.40128687 0.39535382 0.38728645 0.38339785 0.3839539 0.38844618 0.3940295 0.39757273 0.39552566 0.38587558 0.369376 0.34792426][0.35504261 0.37936008 0.39120835 0.39099219 0.38472873 0.37662515 0.372764 0.37328202 0.37711388 0.381481 0.38423625 0.38188058 0.37222663 0.35678932 0.33661693][0.34116995 0.36521587 0.37552267 0.37425911 0.36761171 0.35952571 0.35514021 0.35516781 0.35832655 0.36204138 0.3638353 0.36169302 0.35335067 0.33921874 0.3210243][0.32486421 0.3471939 0.35584354 0.35360956 0.34591392 0.33725142 0.3314161 0.32970563 0.33114815 0.33330077 0.3345165 0.33252507 0.3260397 0.31457511 0.29898414][0.30371109 0.3249374 0.33243856 0.33034021 0.32275742 0.31342435 0.30537218 0.30126414 0.3005901 0.30116695 0.301698 0.30139112 0.29778692 0.28925839 0.27662158][0.28198013 0.30079338 0.30711275 0.30590609 0.29929388 0.28953597 0.28021231 0.27464503 0.27237019 0.27156702 0.27147061 0.27244595 0.27101922 0.26526719 0.25531128][0.25862056 0.27561346 0.28157234 0.28078908 0.27518365 0.2661539 0.256509 0.25037462 0.24725342 0.24595219 0.24585679 0.24741757 0.24736843 0.24339369 0.23527279][0.23527008 0.25064877 0.25589442 0.25609845 0.25159273 0.24382931 0.23473173 0.2285493 0.22545063 0.22447339 0.22475377 0.22634736 0.22702405 0.22405234 0.21738225][0.21306247 0.22680011 0.23168889 0.23307648 0.23026189 0.22377762 0.21547753 0.20923325 0.20609149 0.20459792 0.20447809 0.20564753 0.20621094 0.20422086 0.19897878][0.19112808 0.20321196 0.20760804 0.21002221 0.2089082 0.20431188 0.19842193 0.19391841 0.1918759 0.19064906 0.19048768 0.19139576 0.19166136 0.18991986 0.18564922][0.17525786 0.18458843 0.18704298 0.18878993 0.18848336 0.18492866 0.1811052 0.17861271 0.17797592 0.17731386 0.17724696 0.17832077 0.17865717 0.17754456 0.1745187]]...]
INFO - root - 2017-12-09 11:50:53.859457: step 21510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:25m:18s remains)
INFO - root - 2017-12-09 11:51:02.461789: step 21520, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 73h:56m:58s remains)
INFO - root - 2017-12-09 11:51:11.203980: step 21530, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 77h:14m:38s remains)
INFO - root - 2017-12-09 11:51:19.973759: step 21540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:57m:05s remains)
INFO - root - 2017-12-09 11:51:28.705324: step 21550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:58m:22s remains)
INFO - root - 2017-12-09 11:51:37.273342: step 21560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:39m:17s remains)
INFO - root - 2017-12-09 11:51:45.969105: step 21570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:40m:12s remains)
INFO - root - 2017-12-09 11:51:54.551757: step 21580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:31m:54s remains)
INFO - root - 2017-12-09 11:52:03.349106: step 21590, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:43m:09s remains)
INFO - root - 2017-12-09 11:52:12.108007: step 21600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:39m:02s remains)
2017-12-09 11:52:12.971746: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.5027 0.49092373 0.47958893 0.467745 0.45454654 0.4409008 0.4284651 0.41512507 0.39839751 0.37913164 0.35680684 0.33457881 0.31242281 0.29279473 0.27536187][0.51882434 0.50787 0.49600205 0.48454621 0.47226444 0.45975646 0.44685838 0.43401918 0.41694537 0.39665937 0.37269026 0.34714782 0.32249242 0.30073535 0.28148761][0.52104259 0.51290303 0.50298315 0.49354216 0.48320231 0.47192642 0.45933193 0.44639778 0.42863306 0.40702465 0.3807928 0.35323182 0.32655174 0.3031829 0.28245851][0.52437276 0.52026004 0.51290107 0.50518382 0.49637392 0.4871791 0.4754262 0.46186307 0.44347441 0.42025998 0.39238447 0.36151436 0.33176222 0.30649444 0.28442252][0.52718836 0.52683312 0.52127719 0.5152759 0.50819182 0.4994379 0.48731542 0.4738622 0.45461929 0.43042707 0.40055591 0.36846328 0.33717692 0.3100242 0.28704375][0.52446306 0.52814811 0.52452052 0.5203858 0.51436317 0.50630069 0.49423569 0.47977591 0.45916098 0.43384555 0.40320706 0.37030134 0.3382743 0.31088662 0.28832784][0.51638949 0.52461654 0.52211124 0.51875985 0.51269621 0.50450891 0.49166048 0.47657114 0.45633167 0.43141365 0.40189528 0.36975586 0.33868924 0.31126231 0.28899783][0.50861144 0.51904237 0.51746631 0.5148946 0.50840962 0.49956623 0.48636302 0.47019118 0.44953877 0.42471579 0.39592269 0.36525527 0.335225 0.30868068 0.28735024][0.50418717 0.51720506 0.51551908 0.51218849 0.50413817 0.49320018 0.47859141 0.46135962 0.44060695 0.41610876 0.38879666 0.35972461 0.33121639 0.30563143 0.28541243][0.487777 0.50228405 0.50063992 0.49829736 0.49099714 0.4794814 0.46402794 0.44684157 0.42660165 0.40268975 0.37655887 0.34993294 0.32402703 0.30048338 0.28193349][0.46272415 0.47857332 0.47766525 0.47541073 0.46833408 0.45770034 0.44399363 0.42737037 0.40869015 0.38673756 0.36363408 0.33929518 0.31553143 0.29457307 0.2776272][0.43546316 0.45024881 0.44866359 0.44596 0.43917111 0.42975095 0.41808656 0.40410268 0.38850972 0.37001705 0.35020593 0.32885846 0.30790815 0.28903303 0.27357852][0.40723792 0.42072231 0.41971532 0.41779262 0.41242146 0.40483922 0.39534682 0.3842802 0.37034172 0.35411596 0.33682832 0.31805038 0.29941151 0.28276706 0.26930866][0.38060144 0.393828 0.39337391 0.3921977 0.38861525 0.38285074 0.37566909 0.36717036 0.3559868 0.3422817 0.32720837 0.31066367 0.29413876 0.27928042 0.26724917][0.35956624 0.37146524 0.37034476 0.36910012 0.36684591 0.36339402 0.35887322 0.35270846 0.34398958 0.33279043 0.31985652 0.30502981 0.29012191 0.27660337 0.26565239]]...]
INFO - root - 2017-12-09 11:52:21.587671: step 21610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:48m:41s remains)
INFO - root - 2017-12-09 11:52:30.419407: step 21620, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:25m:33s remains)
INFO - root - 2017-12-09 11:52:39.308334: step 21630, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 79h:08m:35s remains)
INFO - root - 2017-12-09 11:52:47.779757: step 21640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:09m:03s remains)
INFO - root - 2017-12-09 11:52:56.453229: step 21650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:56m:13s remains)
INFO - root - 2017-12-09 11:53:05.063855: step 21660, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 78h:05m:12s remains)
INFO - root - 2017-12-09 11:53:13.812905: step 21670, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:55m:27s remains)
INFO - root - 2017-12-09 11:53:22.437559: step 21680, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 72h:20m:08s remains)
INFO - root - 2017-12-09 11:53:31.105205: step 21690, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:39m:33s remains)
INFO - root - 2017-12-09 11:53:39.773260: step 21700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:50m:49s remains)
2017-12-09 11:53:40.581296: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.269804 0.23673216 0.20488854 0.17467473 0.14889993 0.12959491 0.11707863 0.11027575 0.10707229 0.1044217 0.10084575 0.095758632 0.088953182 0.08073961 0.071648225][0.26835886 0.23335473 0.20151152 0.17190859 0.14814095 0.13149761 0.12263042 0.12039292 0.12142888 0.12208179 0.11995145 0.11426552 0.10511279 0.09355326 0.080662556][0.2522465 0.21433306 0.1818444 0.15395416 0.13481176 0.12433298 0.12323643 0.12873662 0.13640942 0.14118819 0.14068642 0.13449278 0.12284227 0.10740496 0.090109691][0.22815885 0.18784022 0.15488653 0.12881695 0.11527324 0.11234961 0.12021124 0.1336574 0.14868149 0.1586834 0.16065477 0.15423718 0.14015464 0.1210518 0.099730961][0.2042212 0.1616603 0.1288588 0.10572948 0.097902589 0.10178026 0.1170213 0.13785703 0.15897907 0.17281576 0.17703518 0.17114891 0.15535671 0.13265367 0.10770425][0.18726353 0.14430249 0.11157271 0.090437435 0.086972408 0.095901653 0.11659137 0.14217268 0.16694961 0.18371385 0.18924806 0.18288474 0.16549358 0.1404583 0.11337458][0.18118718 0.13876629 0.10659315 0.0872808 0.086038508 0.097320177 0.12057234 0.14791985 0.17367272 0.19084361 0.19636516 0.18961719 0.17118408 0.14482488 0.11663792][0.18870135 0.1478146 0.11625125 0.097599819 0.095837027 0.10659789 0.12896241 0.15495849 0.1790458 0.19511269 0.19939885 0.19128497 0.17183864 0.1452736 0.1171347][0.21027508 0.17152819 0.14066361 0.1218902 0.11876796 0.12703547 0.14526103 0.16644177 0.18586639 0.19826232 0.19977966 0.18992254 0.16986339 0.14342709 0.11577373][0.23473 0.20162481 0.17480077 0.15733336 0.15284115 0.15711844 0.16938634 0.18396792 0.19632411 0.20286159 0.1996621 0.18695037 0.16574749 0.13952208 0.11285827][0.25272343 0.22723797 0.20633128 0.19225223 0.18830855 0.18970723 0.19610481 0.2029451 0.20762235 0.20764147 0.1991089 0.18270046 0.15975255 0.13406011 0.10881921][0.26088586 0.24270736 0.22750682 0.2178214 0.21500863 0.21480183 0.21679504 0.21756501 0.21578462 0.20949572 0.19608219 0.1765461 0.15223552 0.12685096 0.1026562][0.26438934 0.25271675 0.2417904 0.23471688 0.2320812 0.22969472 0.22724304 0.22302791 0.21646471 0.20588283 0.18953001 0.16824126 0.14358409 0.11883765 0.095952116][0.25733891 0.25226468 0.24599546 0.24120265 0.23882699 0.23485187 0.22931926 0.22118279 0.21092306 0.19760031 0.17955059 0.15759735 0.13348801 0.11006787 0.08907564][0.23804145 0.23767763 0.23516817 0.23258062 0.23079357 0.22671014 0.2203301 0.21061146 0.19834393 0.18359335 0.16503865 0.14360994 0.12098886 0.099730425 0.081402257]]...]
INFO - root - 2017-12-09 11:53:49.110621: step 21710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:55m:47s remains)
INFO - root - 2017-12-09 11:53:57.796718: step 21720, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:31m:25s remains)
INFO - root - 2017-12-09 11:54:06.409027: step 21730, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:36m:50s remains)
INFO - root - 2017-12-09 11:54:14.795979: step 21740, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 71h:49m:30s remains)
INFO - root - 2017-12-09 11:54:23.377119: step 21750, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 73h:44m:25s remains)
INFO - root - 2017-12-09 11:54:31.781078: step 21760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:09m:04s remains)
INFO - root - 2017-12-09 11:54:40.477994: step 21770, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:34m:30s remains)
INFO - root - 2017-12-09 11:54:49.242617: step 21780, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 78h:27m:13s remains)
INFO - root - 2017-12-09 11:54:57.996544: step 21790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:58m:48s remains)
INFO - root - 2017-12-09 11:55:06.648673: step 21800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:57m:13s remains)
2017-12-09 11:55:07.516195: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010766722 0.012724001 0.014681518 0.016742762 0.017978858 0.018004522 0.017224418 0.015609141 0.013354747 0.010497006 0.0079187471 0.0054988754 0.0032473649 0.0011573983 -0.00040576607][0.011060966 0.013524614 0.017347541 0.022099232 0.026266145 0.028579533 0.028749704 0.026469296 0.021982346 0.016532145 0.011859519 0.0081360918 0.0051158885 0.0025372095 0.00058846129][0.0087853335 0.012084862 0.01824509 0.026619017 0.035001647 0.040507514 0.042227272 0.039443653 0.032571871 0.02355844 0.015343005 0.0091470182 0.0046833358 0.0015790658 -0.00039877254][0.010764781 0.014987895 0.023041589 0.034587998 0.046669692 0.055970248 0.059740111 0.056091681 0.046160039 0.033026293 0.020871183 0.011279851 0.0046352153 0.00075950194 -0.0012676339][0.0149701 0.02304039 0.035496224 0.05124186 0.066626385 0.077516377 0.080612384 0.074665964 0.061221875 0.043863159 0.028104696 0.015685253 0.0070006349 0.0015519252 -0.0012758889][0.020911479 0.032569151 0.049360927 0.0698122 0.089521438 0.1030362 0.1059744 0.09705209 0.078740858 0.056394476 0.035866763 0.019598247 0.0088299718 0.0026642112 -0.00042840419][0.037161008 0.051544905 0.070415765 0.092046477 0.11231983 0.1260536 0.12883262 0.11912235 0.098953851 0.073403984 0.048804726 0.028403454 0.013925543 0.0050088591 0.00042360649][0.064072408 0.080676466 0.10009256 0.1204724 0.13861671 0.14988595 0.15073407 0.13987648 0.1190403 0.092547148 0.066036925 0.042676777 0.024709245 0.012315121 0.004730158][0.098084971 0.11526253 0.1332694 0.15113263 0.16612282 0.17465688 0.17394796 0.16272555 0.14214382 0.11565502 0.087819181 0.061997309 0.040509745 0.024028892 0.01261301][0.13073516 0.14888828 0.16592614 0.18117458 0.19310689 0.199365 0.19796009 0.18767722 0.16905545 0.14468332 0.11788634 0.09132731 0.066790767 0.045645345 0.028830063][0.15988427 0.17741869 0.19178677 0.20428981 0.21376292 0.2186611 0.21787247 0.210123 0.19554321 0.17570828 0.15254571 0.12765737 0.10201798 0.077195495 0.054568864][0.17804962 0.1952181 0.20692992 0.21587773 0.222298 0.22563753 0.22503297 0.21967435 0.2096376 0.19554153 0.17788409 0.15691693 0.13264172 0.10632414 0.0795558][0.18280463 0.1981789 0.20754106 0.21411474 0.2183034 0.22019647 0.21953772 0.21607605 0.20958346 0.20046587 0.18816228 0.17184439 0.15068717 0.12528411 0.097039059][0.17134336 0.18398233 0.19105649 0.19563906 0.19865555 0.20027032 0.20006603 0.19819129 0.1945468 0.18933094 0.18132235 0.16933139 0.15200004 0.12936196 0.10236298][0.1480861 0.15766095 0.16248482 0.16521397 0.16706127 0.16821709 0.1682025 0.16738036 0.16573694 0.16340294 0.15892301 0.15096624 0.13787372 0.11911077 0.095322967]]...]
INFO - root - 2017-12-09 11:55:16.078333: step 21810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:07m:35s remains)
INFO - root - 2017-12-09 11:55:24.805159: step 21820, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 75h:26m:24s remains)
INFO - root - 2017-12-09 11:55:33.288691: step 21830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 74h:57m:45s remains)
INFO - root - 2017-12-09 11:55:41.809179: step 21840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 72h:49m:46s remains)
INFO - root - 2017-12-09 11:55:50.341836: step 21850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:38m:48s remains)
INFO - root - 2017-12-09 11:55:58.727824: step 21860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:48m:22s remains)
INFO - root - 2017-12-09 11:56:07.286359: step 21870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:43m:19s remains)
INFO - root - 2017-12-09 11:56:15.828482: step 21880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:37m:28s remains)
INFO - root - 2017-12-09 11:56:24.437903: step 21890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:54m:46s remains)
INFO - root - 2017-12-09 11:56:32.919753: step 21900, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 72h:14m:11s remains)
2017-12-09 11:56:33.839427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033579052 -0.0033568491 -0.0033565494 -0.0033559864 -0.0033557646 -0.0033554011 -0.0033555313 -0.0033554647 -0.003355633 -0.0033556963 -0.0033561655 -0.0033574649 -0.0033600796 -0.0033641276 -0.0033635842][-0.0033566689 -0.00335561 -0.0033552186 -0.003354568 -0.0033542293 -0.0033537766 -0.0033538346 -0.0033537713 -0.0033540893 -0.0033543287 -0.003355087 -0.0033565068 -0.0033590805 -0.0033629332 -0.0033622461][-0.0033565175 -0.0033553615 -0.0033549806 -0.0033543003 -0.003353992 -0.0033536041 -0.0033536851 -0.0033536851 -0.0033539527 -0.0033541084 -0.0033547343 -0.0033560179 -0.0033584663 -0.0033621436 -0.0033617744][-0.003356446 -0.003355375 -0.0033548973 -0.0033542519 -0.0033538183 -0.0033534574 -0.0033535413 -0.0033535913 -0.0033538493 -0.0033540269 -0.0033544933 -0.0033556095 -0.0033575234 -0.0033595676 -0.0033614133][-0.0033563976 -0.0033554193 -0.0033548416 -0.0033541953 -0.0033537012 -0.0033533338 -0.003353314 -0.0033533692 -0.003353639 -0.0033538321 -0.0033542528 -0.0033552316 -0.0033567809 -0.0033563967 -0.0033612004][-0.0033561399 -0.0033552991 -0.0033547385 -0.003354158 -0.003353663 -0.0033533187 -0.0033532295 -0.003353212 -0.0033534293 -0.00335365 -0.0033541082 -0.003355009 -0.0033562831 -0.0033555746 -0.0033608906][-0.0033556724 -0.0033549576 -0.003354595 -0.0033541084 -0.0033538179 -0.0033534802 -0.0033532602 -0.003353127 -0.0033531911 -0.0033533385 -0.0033536733 -0.0033545203 -0.0033558109 -0.0033525017 -0.0033609597][-0.0033551138 -0.0033544423 -0.0033541068 -0.00335375 -0.0033535117 -0.0033532681 -0.0033530353 -0.0033528993 -0.003352918 -0.0033530167 -0.0033532239 -0.003354012 -0.0033553084 -0.0033548484 -0.0033611769][-0.0033545832 -0.0033539382 -0.0033536656 -0.0033534197 -0.0033532791 -0.0033530325 -0.0033528097 -0.0033526262 -0.0033525953 -0.0033526972 -0.0033529459 -0.0033537759 -0.0033552737 -0.0033557427 -0.0033615178][-0.0033540584 -0.0033534924 -0.0033533329 -0.0033532386 -0.0033531792 -0.0033530453 -0.0033529692 -0.003352843 -0.0033528269 -0.0033529329 -0.0033531589 -0.0033539839 -0.0033559727 -0.003359566 -0.0033615653][-0.0033539261 -0.0033533953 -0.003353389 -0.0033534449 -0.0033534917 -0.003353413 -0.0033533766 -0.00335325 -0.0033532584 -0.0033533222 -0.0033535026 -0.0033543089 -0.0033563604 -0.0033600847 -0.0033617935][-0.0033542193 -0.0033536276 -0.0033535087 -0.0033535934 -0.0033536772 -0.0033536607 -0.0033537783 -0.003353657 -0.0033537354 -0.0033537366 -0.0033539205 -0.0033547143 -0.0033567687 -0.0033604912 -0.0033620102][-0.0033548533 -0.0033542223 -0.003353975 -0.0033539818 -0.0033540088 -0.0033539464 -0.0033541394 -0.0033539855 -0.0033539801 -0.0033525093 -0.0033543066 -0.0033550556 -0.0033570586 -0.0033607064 -0.0033615329][-0.0033555126 -0.0033548519 -0.0033544987 -0.0033544318 -0.0033543834 -0.0033543406 -0.0033544637 -0.0033542796 -0.0033541748 -0.0033506271 -0.0033546202 -0.0033553864 -0.0033572898 -0.0033607017 -0.0033608819][-0.00335603 -0.0033552959 -0.0033549992 -0.0033548102 -0.0033547035 -0.0033546435 -0.003354698 -0.0033544784 -0.0033542505 -0.0033506972 -0.003354819 -0.0033556148 -0.0033574679 -0.0033609439 -0.0033606968]]...]
INFO - root - 2017-12-09 11:56:42.247906: step 21910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:47m:10s remains)
INFO - root - 2017-12-09 11:56:50.898815: step 21920, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:39m:52s remains)
INFO - root - 2017-12-09 11:56:59.734287: step 21930, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 77h:49m:05s remains)
INFO - root - 2017-12-09 11:57:08.286587: step 21940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:26m:44s remains)
INFO - root - 2017-12-09 11:57:16.842898: step 21950, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:52m:27s remains)
INFO - root - 2017-12-09 11:57:25.207426: step 21960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:35m:02s remains)
INFO - root - 2017-12-09 11:57:33.873826: step 21970, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:35m:54s remains)
INFO - root - 2017-12-09 11:57:42.541778: step 21980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:01m:49s remains)
INFO - root - 2017-12-09 11:57:51.230624: step 21990, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 74h:11m:50s remains)
INFO - root - 2017-12-09 11:57:59.771781: step 22000, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:56m:37s remains)
2017-12-09 11:58:00.709782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033509533 -0.0033471533 -0.003312642 -0.0031509588 -0.0027054935 -0.0019067586 -0.0009857181 -0.00043272949 -0.000632792 -0.0014919062 -0.0024417462 -0.003053481 -0.0033023723 -0.0033430182 -0.0033439856][-0.0033483284 -0.0033361234 -0.003251137 -0.0029189689 -0.0020927102 -0.00069373427 0.00086642965 0.0017805167 0.0014448541 1.9598287e-05 -0.0016080715 -0.0027287726 -0.0032285182 -0.0033356324 -0.0033399686][-0.0033440806 -0.0033020535 -0.0030838386 -0.0023656958 -0.0007571734 0.001790642 0.0045154588 0.0060694842 0.0054810867 0.0030289914 0.00013290835 -0.0019982462 -0.0030298331 -0.0033127486 -0.0033383758][-0.0033318384 -0.0032148096 -0.0027135813 -0.0012696043 0.0016887554 0.006087184 0.010596119 0.013096675 0.012118898 0.0080980621 0.0032013811 -0.00060683955 -0.0025897538 -0.0032368873 -0.0033320526][-0.0032932532 -0.0030482789 -0.0021002931 0.00039654365 0.0051749786 0.011916985 0.018573131 0.022147078 0.020656744 0.014735978 0.0073698936 0.0014112762 -0.001875909 -0.0030861935 -0.0033169719][-0.0031814573 -0.002783892 -0.0013303689 0.0022835664 0.0088810734 0.017836757 0.02642997 0.030937385 0.028983865 0.021358011 0.011680732 0.0036081222 -0.0010461786 -0.0028908113 -0.0032970121][-0.0029909923 -0.0024607526 -0.00063939276 0.0037045751 0.01138657 0.02154314 0.031095352 0.036024757 0.033833712 0.025350658 0.014413437 0.0050883181 -0.00045571639 -0.0027412737 -0.0032818106][-0.0027982872 -0.0022110231 -0.00033119251 0.0040115928 0.011540362 0.021339517 0.03045401 0.035109781 0.032981239 0.024826629 0.014230192 0.0050896639 -0.00042355224 -0.0027226903 -0.0032793249][-0.0027139098 -0.0021733958 -0.00057854247 0.0030179888 0.00919644 0.017191865 0.024603488 0.028357752 0.02656297 0.01982866 0.011089734 0.0035510149 -0.00098222843 -0.0028492196 -0.0032921981][-0.0027934287 -0.0023838817 -0.0012801494 0.0011637758 0.0053665368 0.010821654 0.01587807 0.0183882 0.017049996 0.012315976 0.0062764687 0.0011512355 -0.0018579819 -0.0030492879 -0.0033136746][-0.0029899662 -0.0027373012 -0.0021264225 -0.00079342397 0.0015218195 0.0045585744 0.0073793717 0.0087257065 0.0078601306 0.0050920714 0.001690496 -0.0010895468 -0.0026449147 -0.0032172049 -0.0033293418][-0.0032073357 -0.0030847404 -0.0028171865 -0.0022458718 -0.0012461767 8.0009922e-05 0.0013044931 0.0018392687 0.0013655231 7.5148419e-05 -0.0014073157 -0.0025398082 -0.0031214948 -0.0033097647 -0.00333921][-0.0033276235 -0.0032955657 -0.0032152322 -0.0030358538 -0.0027162295 -0.0022880961 -0.0018998116 -0.0017553077 -0.0019483287 -0.0023864065 -0.0028430959 -0.0031584366 -0.0033010987 -0.0033384603 -0.0033425756][-0.0033452963 -0.0033448527 -0.0033370352 -0.0033068892 -0.003242916 -0.0031526648 -0.003073609 -0.0030523071 -0.00310431 -0.003199145 -0.0032823696 -0.003328114 -0.0033427449 -0.0033439936 -0.0033437365][-0.0033436923 -0.0033441565 -0.0033459023 -0.0033450765 -0.0033410338 -0.0033337832 -0.0033255445 -0.0033214677 -0.0033246782 -0.0033334065 -0.0033412937 -0.0033444723 -0.0033443591 -0.0033438909 -0.0033437409]]...]
INFO - root - 2017-12-09 11:58:09.275419: step 22010, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.969 sec/batch; 83h:32m:45s remains)
INFO - root - 2017-12-09 11:58:17.972992: step 22020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:56m:26s remains)
INFO - root - 2017-12-09 11:58:26.677728: step 22030, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 76h:21m:33s remains)
INFO - root - 2017-12-09 11:58:35.102232: step 22040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:45m:21s remains)
INFO - root - 2017-12-09 11:58:43.634033: step 22050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:12m:24s remains)
INFO - root - 2017-12-09 11:58:52.001028: step 22060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 72h:44m:52s remains)
INFO - root - 2017-12-09 11:59:00.635451: step 22070, loss = 0.89, batch loss = 0.69 (8.6 examples/sec; 0.926 sec/batch; 79h:49m:06s remains)
INFO - root - 2017-12-09 11:59:09.234320: step 22080, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:30m:15s remains)
INFO - root - 2017-12-09 11:59:18.006601: step 22090, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 71h:53m:03s remains)
INFO - root - 2017-12-09 11:59:26.426442: step 22100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:37m:43s remains)
2017-12-09 11:59:27.347038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033345188 -0.0033307935 -0.0033305069 -0.0033300975 -0.0033297029 -0.0033298179 -0.0033302119 -0.0033313062 -0.0033334838 -0.0033366226 -0.0033407838 -0.003344829 -0.0033489165 -0.0033523014 -0.0033535266][-0.0033311634 -0.0033272952 -0.0033268325 -0.0033263704 -0.00332595 -0.0033258749 -0.0033264458 -0.0033274223 -0.003329505 -0.0033328256 -0.0033371977 -0.0033425447 -0.0033487042 -0.0033534041 -0.003355158][-0.0033313232 -0.0033273436 -0.0033267608 -0.0033265483 -0.0033261643 -0.0033257715 -0.0033261946 -0.0033267522 -0.0033281285 -0.0033305087 -0.0033344191 -0.0033405486 -0.0033478821 -0.0033539487 -0.0033561585][-0.0033313546 -0.0033272291 -0.0033266433 -0.0033265851 -0.003326447 -0.0033262619 -0.0033264186 -0.0033262582 -0.0033261038 -0.0033265122 -0.0033288426 -0.0033348612 -0.0033435402 -0.0033513831 -0.0033548779][-0.0033324764 -0.0033280023 -0.0033263152 -0.003325735 -0.0033254134 -0.0033255175 -0.0033251727 -0.0033233091 -0.0033206528 -0.0033179654 -0.0033193796 -0.0033261937 -0.0033360776 -0.0033457105 -0.0033509959][-0.0033346778 -0.0033288368 -0.0033253503 -0.003323731 -0.0033227641 -0.003322032 -0.0033201915 -0.0033166709 -0.0033119309 -0.0033077737 -0.003308794 -0.0033161819 -0.0033275983 -0.0033392208 -0.0033463354][-0.0033368769 -0.0033293294 -0.0033246283 -0.0033234372 -0.003323145 -0.0033216379 -0.003318209 -0.0033138895 -0.003308408 -0.0033036263 -0.0033030892 -0.0033097216 -0.0033214106 -0.0033335332 -0.003341475][-0.0033371607 -0.0033299006 -0.0033277061 -0.0033291876 -0.0033301758 -0.0033285869 -0.0033244516 -0.0033190548 -0.0033126161 -0.0033067996 -0.003305058 -0.0033100282 -0.0033201065 -0.0033313273 -0.0033392499][-0.0033407449 -0.0033363095 -0.003336502 -0.0033391539 -0.0033408133 -0.0033392841 -0.00333476 -0.0033287462 -0.0033224577 -0.0033172024 -0.0033153214 -0.0033186439 -0.0033261527 -0.0033351469 -0.0033417812][-0.003346049 -0.0033437326 -0.0033452739 -0.0033481 -0.0033495815 -0.0033483584 -0.0033442187 -0.0033390813 -0.0033338699 -0.0033299811 -0.0033288852 -0.00333104 -0.0033359325 -0.0033419556 -0.0033465317][-0.0033483268 -0.0033472278 -0.0033494169 -0.0033518649 -0.0033534816 -0.0033531236 -0.0033505161 -0.0033472481 -0.0033439184 -0.0033415975 -0.0033412571 -0.0033425794 -0.0033449579 -0.0033482804 -0.0033509345][-0.0033469053 -0.0033469088 -0.0033494758 -0.0033521203 -0.0033542586 -0.003354877 -0.0033537371 -0.0033521277 -0.0033504956 -0.0033491151 -0.0033489577 -0.0033501326 -0.0033514807 -0.0033527529 -0.0033533669][-0.00334227 -0.003342004 -0.0033444494 -0.0033471731 -0.0033496455 -0.0033511147 -0.0033516234 -0.0033517175 -0.0033511997 -0.0033504043 -0.0033506595 -0.0033517473 -0.0033525431 -0.0033531371 -0.0033525876][-0.0033361118 -0.003334607 -0.0033362228 -0.0033382277 -0.003340242 -0.0033419507 -0.0033434248 -0.0033443649 -0.0033445836 -0.0033447838 -0.0033457165 -0.0033470883 -0.0033478278 -0.003347931 -0.0033468951][-0.0033297094 -0.003326752 -0.0033276719 -0.0033288791 -0.0033302491 -0.0033315737 -0.0033328005 -0.0033337791 -0.0033343586 -0.0033349285 -0.0033359621 -0.0033373595 -0.0033382287 -0.0033383584 -0.0033375632]]...]
INFO - root - 2017-12-09 11:59:35.748218: step 22110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:35m:28s remains)
INFO - root - 2017-12-09 11:59:44.228511: step 22120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:40m:22s remains)
INFO - root - 2017-12-09 11:59:52.638478: step 22130, loss = 0.88, batch loss = 0.67 (9.8 examples/sec; 0.818 sec/batch; 70h:32m:57s remains)
INFO - root - 2017-12-09 12:00:00.992424: step 22140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:25m:17s remains)
INFO - root - 2017-12-09 12:00:09.676262: step 22150, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:30m:48s remains)
INFO - root - 2017-12-09 12:00:18.363413: step 22160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:55m:41s remains)
INFO - root - 2017-12-09 12:00:26.787233: step 22170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:58m:59s remains)
INFO - root - 2017-12-09 12:00:35.477570: step 22180, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:30m:23s remains)
INFO - root - 2017-12-09 12:00:44.187438: step 22190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:08m:00s remains)
INFO - root - 2017-12-09 12:00:52.830757: step 22200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:47m:19s remains)
2017-12-09 12:00:53.772399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033645744 -0.0033630042 -0.0033631036 -0.0033632072 -0.0033628824 -0.0033622463 -0.0033615651 -0.0033612004 -0.00336109 -0.0033610966 -0.0033611194 -0.0033611914 -0.0033614242 -0.0033618854 -0.0033624312][-0.0033656193 -0.0033642503 -0.0033645737 -0.0033648103 -0.003364559 -0.0033637288 -0.0033625194 -0.0033614736 -0.0033607131 -0.0033604898 -0.0033605313 -0.003360871 -0.003361973 -0.0033630033 -0.0033637378][-0.0033563478 -0.0033545787 -0.0033579585 -0.0033628687 -0.0033666822 -0.0033668021 -0.0033652941 -0.0033635418 -0.0033622533 -0.0033617823 -0.0033619509 -0.0033631462 -0.0033651544 -0.003366675 -0.0033674533][-0.0032886432 -0.0032832394 -0.0033029707 -0.0033329786 -0.0033594109 -0.0033686226 -0.0033692471 -0.0033674263 -0.0033656578 -0.0033651197 -0.0033658773 -0.0033675723 -0.0033696142 -0.0033706396 -0.003371042][-0.0030980497 -0.0030856782 -0.0031490647 -0.0032464026 -0.0033307543 -0.0033648354 -0.0033723849 -0.0033721686 -0.003370937 -0.0033706841 -0.0033715705 -0.0033725936 -0.0033737486 -0.0033739361 -0.0033736979][-0.0027938704 -0.0027716528 -0.0029055625 -0.003110385 -0.0032837742 -0.0033551576 -0.00337308 -0.0033754883 -0.0033754727 -0.0033758192 -0.0033766627 -0.003377045 -0.0033772388 -0.0033761871 -0.003374632][-0.0025174047 -0.0024887063 -0.0026875853 -0.0029901972 -0.0032419804 -0.0033443058 -0.0033702697 -0.0033749784 -0.0033765575 -0.003377934 -0.0033791633 -0.0033793424 -0.0033784595 -0.0033761759 -0.003373347][-0.0024448507 -0.0024204468 -0.0026409228 -0.002969671 -0.00323699 -0.0033431782 -0.0033695621 -0.0033749703 -0.0033771116 -0.0033785442 -0.0033799021 -0.0033799645 -0.0033782548 -0.0033751905 -0.0033718706][-0.0026322433 -0.0026198598 -0.0028022067 -0.003066364 -0.003274003 -0.0033518912 -0.0033705009 -0.0033745018 -0.0033762848 -0.0033774851 -0.0033785047 -0.0033782257 -0.0033759382 -0.0033729081 -0.0033700273][-0.0029439942 -0.0029406764 -0.0030514959 -0.0032067713 -0.0033235832 -0.0033632412 -0.0033714315 -0.0033730634 -0.0033736047 -0.0033745733 -0.0033748124 -0.0033735486 -0.0033712285 -0.0033688827 -0.0033669104][-0.0032083578 -0.0032089502 -0.0032531088 -0.0033124255 -0.0033545513 -0.0033674142 -0.0033691572 -0.0033691493 -0.0033691537 -0.0033694599 -0.0033688671 -0.0033673309 -0.0033655928 -0.003364169 -0.003363007][-0.0033361865 -0.0033350198 -0.0033433975 -0.0033558011 -0.0033638265 -0.003366072 -0.0033657071 -0.003365115 -0.0033649472 -0.0033650713 -0.0033644447 -0.0033632941 -0.0033620845 -0.0033611311 -0.0033605823][-0.0033676785 -0.0033653451 -0.0033646931 -0.0033643283 -0.003363882 -0.0033634405 -0.0033629413 -0.0033627241 -0.0033626924 -0.0033626247 -0.0033621239 -0.0033612493 -0.0033604514 -0.0033598489 -0.0033596593][-0.0033653243 -0.0033626349 -0.0033619853 -0.003361692 -0.0033614063 -0.0033612123 -0.0033610137 -0.0033610838 -0.0033612454 -0.0033611464 -0.0033609108 -0.0033605341 -0.0033600323 -0.0033596209 -0.003359447][-0.0033635595 -0.0033611043 -0.0033604014 -0.003360139 -0.003359928 -0.0033598754 -0.0033598382 -0.0033598328 -0.0033598405 -0.0033597893 -0.0033597781 -0.0033597304 -0.0033595401 -0.003359348 -0.0033592011]]...]
INFO - root - 2017-12-09 12:01:02.238883: step 22210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:41m:29s remains)
INFO - root - 2017-12-09 12:01:10.937974: step 22220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:10m:34s remains)
INFO - root - 2017-12-09 12:01:19.702755: step 22230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:55m:06s remains)
INFO - root - 2017-12-09 12:01:28.199918: step 22240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:53m:30s remains)
INFO - root - 2017-12-09 12:01:36.831621: step 22250, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:20m:38s remains)
INFO - root - 2017-12-09 12:01:45.294134: step 22260, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 71h:41m:46s remains)
INFO - root - 2017-12-09 12:01:53.674683: step 22270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:09m:27s remains)
INFO - root - 2017-12-09 12:02:02.252169: step 22280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:43m:39s remains)
INFO - root - 2017-12-09 12:02:10.988056: step 22290, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 74h:27m:42s remains)
INFO - root - 2017-12-09 12:02:19.704561: step 22300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:22m:36s remains)
2017-12-09 12:02:20.610840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033633055 -0.0033605141 -0.0033601972 -0.0033586759 -0.0033084194 -0.0031038402 -0.0026643428 -0.0021116016 -0.0016823758 -0.0015957924 -0.0018905618 -0.00243112 -0.0029521727 -0.0032630716 -0.0033528039][-0.0033603639 -0.0033599434 -0.0033601047 -0.0033572891 -0.003264975 -0.0029294931 -0.0022606284 -0.0014362442 -0.00079360162 -0.00061785826 -0.00098821172 -0.0017864526 -0.0026166155 -0.0031502983 -0.0033331153][-0.0033313287 -0.0033443738 -0.0033529033 -0.0033500427 -0.0032023329 -0.0027029796 -0.0017634019 -0.0006273794 0.00026761158 0.00057278038 0.00016005686 -0.000891489 -0.0020896783 -0.0029406718 -0.0032920251][-0.0032767286 -0.00329692 -0.0033159032 -0.0033155933 -0.0031100675 -0.0024416992 -0.0012388148 0.00020202296 0.0013697513 0.0018443677 0.0014140767 0.00014324463 -0.0014396233 -0.0026560607 -0.0032310155][-0.003183343 -0.0032167097 -0.0032538392 -0.0032536983 -0.0029925876 -0.00219162 -0.0007836048 0.00092887855 0.0023813325 0.003060797 0.0026338689 0.0011624799 -0.00078466185 -0.0023543681 -0.0031606159][-0.0030816905 -0.0031154179 -0.0031615242 -0.0031543982 -0.0028524124 -0.0019752134 -0.00043891417 0.0014816525 0.0031914685 0.0040821945 0.0036776362 0.0020344781 -0.00022711791 -0.0020952621 -0.003097435][-0.002987158 -0.0030214873 -0.0030778826 -0.0030675579 -0.0027299984 -0.001814525 -0.00020940113 0.00184591 0.003749463 0.0048110969 0.0044238586 0.002644473 0.00016093673 -0.0019148184 -0.0030512861][-0.0029508737 -0.0029578856 -0.0030046077 -0.0030016378 -0.0026404085 -0.0017232446 -0.0001155301 0.0019784698 0.0039736107 0.0051120939 0.0047320779 0.0028898523 0.00031845807 -0.0018381568 -0.0030292422][-0.0029869981 -0.0029636302 -0.0029704471 -0.002952056 -0.0025938365 -0.0017172003 -0.0001733508 0.00183226 0.0037596228 0.0048544845 0.0044748029 0.0026812565 0.00019387668 -0.0018926826 -0.0030386797][-0.0030757163 -0.0030301758 -0.0030028543 -0.0029516132 -0.0026063009 -0.0018177475 -0.00042766449 0.001374874 0.0030951884 0.004036393 0.0036476662 0.0020044388 -0.00022883783 -0.0020843712 -0.0030818805][-0.0031745979 -0.0031239868 -0.003083392 -0.0030069847 -0.0027061491 -0.0020393461 -0.00087334658 0.00063289958 0.0020489886 0.002778779 0.0023865502 0.00097212871 -0.00088182837 -0.0023800898 -0.0031516263][-0.0032536522 -0.0032102326 -0.0031812829 -0.0031072993 -0.0028792256 -0.0023744951 -0.0014789629 -0.00030616485 0.00077779242 0.0012905307 0.00091628474 -0.00021902192 -0.0016270522 -0.0027082826 -0.0032283983][-0.0033081579 -0.0032736764 -0.0032601748 -0.0032057962 -0.0030651425 -0.0027355386 -0.0021239428 -0.0012942469 -0.00053030858 -0.00020038406 -0.00051837252 -0.0013454594 -0.0023068227 -0.0029924824 -0.003292592][-0.0033345027 -0.0033140013 -0.0033132981 -0.0032828955 -0.0032162904 -0.0030353121 -0.002670201 -0.002152276 -0.0016725389 -0.0014784454 -0.0017087738 -0.0022396292 -0.0028137008 -0.0031875405 -0.0033321718][-0.0033491475 -0.0033346387 -0.0033386743 -0.0033286898 -0.0033068995 -0.0032292551 -0.0030481459 -0.0027709375 -0.002507268 -0.0024063769 -0.0025448287 -0.0028342048 -0.0031248126 -0.0032945233 -0.0033511198]]...]
INFO - root - 2017-12-09 12:02:29.133625: step 22310, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 75h:58m:13s remains)
INFO - root - 2017-12-09 12:02:37.797331: step 22320, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 74h:05m:06s remains)
INFO - root - 2017-12-09 12:02:46.454440: step 22330, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:38m:40s remains)
INFO - root - 2017-12-09 12:02:54.842252: step 22340, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:16m:49s remains)
INFO - root - 2017-12-09 12:03:03.631079: step 22350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:55m:09s remains)
INFO - root - 2017-12-09 12:03:12.380288: step 22360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:35m:13s remains)
INFO - root - 2017-12-09 12:03:20.921852: step 22370, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:51m:04s remains)
INFO - root - 2017-12-09 12:03:29.598557: step 22380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:28m:22s remains)
INFO - root - 2017-12-09 12:03:38.272141: step 22390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:40m:29s remains)
INFO - root - 2017-12-09 12:03:46.946490: step 22400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:45m:30s remains)
2017-12-09 12:03:47.820088: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50294834 0.50163841 0.49638587 0.48779243 0.47685295 0.46163183 0.44616252 0.42938688 0.4155634 0.40093857 0.39055017 0.38831252 0.38992107 0.39222842 0.3959021][0.51134455 0.51621497 0.51643485 0.5148564 0.50940746 0.49839696 0.48538056 0.47105214 0.45825496 0.44304535 0.43258217 0.42964721 0.43208003 0.43528441 0.4390747][0.51352149 0.526626 0.53536 0.54124135 0.54263675 0.53722209 0.527958 0.51609087 0.50383443 0.48869115 0.47687167 0.47165671 0.47285748 0.47485045 0.47747061][0.51594704 0.53808707 0.55634224 0.57027888 0.5795911 0.58110589 0.57733208 0.56810337 0.5562042 0.54133326 0.52702063 0.51763773 0.51436681 0.51280594 0.512395][0.51540047 0.54761207 0.57510459 0.59773976 0.61492193 0.62221378 0.62396938 0.618226 0.60741997 0.59246862 0.57615346 0.56227553 0.55326742 0.547136 0.54273051][0.511021 0.55196261 0.58679944 0.61752492 0.6429196 0.65666425 0.66265482 0.66078764 0.65275592 0.63913137 0.6214413 0.60400808 0.589636 0.57863313 0.5698396][0.50613469 0.55205667 0.590579 0.62473 0.65450066 0.67535931 0.68817288 0.69065487 0.68656707 0.67631537 0.65943515 0.63937318 0.62071168 0.60513759 0.59231132][0.50013787 0.54748571 0.58821404 0.6241762 0.65693951 0.68245816 0.70024538 0.70801789 0.70737404 0.70011157 0.68436 0.66330814 0.64210117 0.62385809 0.60862905][0.48987433 0.53770351 0.578033 0.6147632 0.64912307 0.67668909 0.69730705 0.70864505 0.71173835 0.70709366 0.69369888 0.67544413 0.65564382 0.63745004 0.62118167][0.47607183 0.52329004 0.56112772 0.59582764 0.62807494 0.65449411 0.67503154 0.68794113 0.69435877 0.69367057 0.68546206 0.67287606 0.65819085 0.64392632 0.62968045][0.45886338 0.50357813 0.53762913 0.567673 0.59494007 0.61819071 0.63672024 0.64972657 0.658584 0.66217697 0.660258 0.65458441 0.64705604 0.63823777 0.62742543][0.43657538 0.47703567 0.506725 0.53209764 0.55438191 0.57228619 0.58763409 0.60028934 0.61020315 0.61699188 0.62047273 0.62179691 0.62103605 0.61790419 0.61096317][0.41030428 0.4470771 0.47210991 0.492889 0.51038885 0.52334666 0.53463453 0.54483378 0.55479079 0.56310123 0.57043469 0.57695931 0.58186221 0.58368188 0.58052784][0.38496131 0.41678777 0.43600103 0.45227274 0.46562418 0.47430822 0.4825674 0.4907738 0.50041634 0.50990319 0.51958656 0.5295704 0.53804082 0.54321438 0.54296488][0.36281279 0.38961586 0.40402249 0.41540912 0.42423704 0.42980194 0.43505856 0.44110897 0.44937816 0.45849764 0.46867073 0.47906452 0.48850065 0.49494806 0.49630332]]...]
INFO - root - 2017-12-09 12:03:56.403763: step 22410, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:51m:47s remains)
INFO - root - 2017-12-09 12:04:04.990274: step 22420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:24m:25s remains)
INFO - root - 2017-12-09 12:04:13.799989: step 22430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:36m:45s remains)
INFO - root - 2017-12-09 12:04:22.295470: step 22440, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:17m:29s remains)
INFO - root - 2017-12-09 12:04:31.053736: step 22450, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 75h:56m:22s remains)
INFO - root - 2017-12-09 12:04:39.779750: step 22460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 77h:12m:13s remains)
INFO - root - 2017-12-09 12:04:48.338342: step 22470, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 70h:35m:19s remains)
INFO - root - 2017-12-09 12:04:56.959324: step 22480, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:16m:51s remains)
INFO - root - 2017-12-09 12:05:05.712533: step 22490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:10m:03s remains)
INFO - root - 2017-12-09 12:05:14.412030: step 22500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:26m:27s remains)
2017-12-09 12:05:15.319325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033489675 -0.0033469943 -0.0033470753 -0.0033467794 -0.0033465696 -0.0033460092 -0.0033453933 -0.003344632 -0.0033438103 -0.0033428119 -0.0033418043 -0.0033411097 -0.0033407582 -0.0033416264 -0.0033439519][-0.0033465251 -0.0033446858 -0.0033447728 -0.0033444939 -0.0033443402 -0.003343829 -0.0033433181 -0.0033427165 -0.0033419756 -0.0033410951 -0.0033402066 -0.0033395581 -0.0033392978 -0.0033403523 -0.0033431728][-0.0033459116 -0.0033440981 -0.0033442383 -0.0033440818 -0.0033440113 -0.0033436476 -0.0033433435 -0.0033427728 -0.0033421689 -0.0033413279 -0.0033404042 -0.0033394578 -0.0033389661 -0.0033399949 -0.0033429638][-0.0033454502 -0.0033435861 -0.0033437081 -0.0033436406 -0.0033436161 -0.003343381 -0.0033433973 -0.0033429612 -0.0033424401 -0.0033415558 -0.003340584 -0.0033393877 -0.0033387134 -0.0033396701 -0.0033427211][-0.0033447687 -0.0033428622 -0.0033429791 -0.0033429842 -0.0033430122 -0.0033428576 -0.0033428967 -0.0033425707 -0.0033420911 -0.0033411456 -0.0033400571 -0.0033387211 -0.0033380636 -0.00333906 -0.0033424066][-0.0033439933 -0.0033419232 -0.0033419842 -0.0033421027 -0.0033422385 -0.0033422152 -0.0033423938 -0.0033421081 -0.0033417596 -0.0033409684 -0.0033399893 -0.0033387775 -0.0033382415 -0.0033394967 -0.003342988][-0.003343374 -0.0033410583 -0.0033411223 -0.0033412152 -0.0033412725 -0.0033412399 -0.0033415144 -0.003341465 -0.0033416329 -0.0033416727 -0.0033415605 -0.0033408739 -0.0033406545 -0.0033421877 -0.0033456555][-0.0033430767 -0.0033404727 -0.0033404962 -0.0033405467 -0.0033405409 -0.0033404739 -0.0033408708 -0.0033412753 -0.0033420338 -0.0033427987 -0.003343767 -0.0033440837 -0.0033445549 -0.0033462394 -0.0033494271][-0.0033428678 -0.0033401751 -0.0033401221 -0.0033401025 -0.0033401675 -0.0033403106 -0.0033410066 -0.0033421756 -0.0033436567 -0.0033454234 -0.0033475822 -0.0033488972 -0.0033497456 -0.0033511182 -0.003353185][-0.0033426217 -0.0033399321 -0.0033397661 -0.0033396941 -0.003339909 -0.0033403516 -0.0033414871 -0.0033433915 -0.0033456471 -0.0033482644 -0.0033512139 -0.0033531024 -0.0033537326 -0.0033543503 -0.0033547874][-0.0033423165 -0.00333965 -0.0033394485 -0.0033393144 -0.003339669 -0.0033404857 -0.0033422897 -0.0033451968 -0.003348185 -0.0033514306 -0.0033549098 -0.0033566104 -0.0033562679 -0.0033553916 -0.0033542507][-0.0033419409 -0.003339337 -0.0033390576 -0.0033389404 -0.0033394829 -0.0033409477 -0.0033437067 -0.0033476658 -0.0033515259 -0.0033551937 -0.0033586565 -0.003359518 -0.0033577785 -0.0033548949 -0.0033519911][-0.0033413945 -0.0033386517 -0.0033382017 -0.0033381239 -0.0033389837 -0.003341241 -0.0033450923 -0.003350406 -0.0033550682 -0.0033588195 -0.0033614936 -0.0033611481 -0.00335734 -0.0033521936 -0.0033472201][-0.003340909 -0.0033380536 -0.0033374715 -0.0033374112 -0.0033386229 -0.0033415242 -0.0033462788 -0.0033526008 -0.0033580519 -0.0033617574 -0.0033634175 -0.0033615138 -0.0033558549 -0.0033488043 -0.0033419775][-0.0033408802 -0.0033379542 -0.0033372848 -0.0033372557 -0.0033385414 -0.0033417337 -0.0033470523 -0.0033541841 -0.0033602044 -0.0033639835 -0.0033652033 -0.0033625525 -0.0033557611 -0.0033470695 -0.0033388447]]...]
INFO - root - 2017-12-09 12:05:23.878611: step 22510, loss = 0.88, batch loss = 0.67 (9.0 examples/sec; 0.892 sec/batch; 76h:46m:58s remains)
INFO - root - 2017-12-09 12:05:32.700033: step 22520, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 77h:40m:21s remains)
INFO - root - 2017-12-09 12:05:41.308511: step 22530, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:12m:22s remains)
INFO - root - 2017-12-09 12:05:49.862525: step 22540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 76h:04m:56s remains)
INFO - root - 2017-12-09 12:05:58.588363: step 22550, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:47m:37s remains)
INFO - root - 2017-12-09 12:06:07.283322: step 22560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 74h:20m:55s remains)
INFO - root - 2017-12-09 12:06:15.673411: step 22570, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:51m:27s remains)
INFO - root - 2017-12-09 12:06:24.306581: step 22580, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 72h:00m:37s remains)
INFO - root - 2017-12-09 12:06:33.001876: step 22590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:06m:04s remains)
INFO - root - 2017-12-09 12:06:41.746006: step 22600, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 75h:00m:50s remains)
2017-12-09 12:06:42.596792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033370513 -0.0033359972 -0.0033378457 -0.0033399295 -0.003341472 -0.0033418946 -0.00334088 -0.0033394485 -0.0033382361 -0.0033374699 -0.0033371472 -0.0033372366 -0.0033376925 -0.0033382159 -0.00333859][-0.0033346096 -0.0033331395 -0.0033348787 -0.0033369511 -0.0033387337 -0.0033396613 -0.0033392415 -0.0033379833 -0.0033364866 -0.0033356117 -0.0033354727 -0.0033357898 -0.0033363653 -0.0033369055 -0.0033372839][-0.0033342051 -0.0033325097 -0.003334065 -0.0033359784 -0.0033375805 -0.0033384492 -0.0033384506 -0.0033378808 -0.0033369141 -0.0033360478 -0.0033357372 -0.003335977 -0.0033365067 -0.0033369728 -0.0033372657][-0.0033341174 -0.0033323856 -0.0033337823 -0.0033354396 -0.0033366038 -0.0033370531 -0.0033369248 -0.0033366464 -0.00333621 -0.0033357828 -0.0033356349 -0.0033358862 -0.0033364822 -0.0033369963 -0.0033373071][-0.0033350375 -0.003332973 -0.0033340743 -0.0033354445 -0.0033363544 -0.003336515 -0.0033362024 -0.0033358475 -0.0033354526 -0.0033351164 -0.0033350345 -0.0033353909 -0.0033361113 -0.0033367716 -0.0033372091][-0.0033362997 -0.0033339348 -0.0033348571 -0.0033358824 -0.0033365076 -0.0033366454 -0.003336445 -0.0033361567 -0.003335651 -0.0033350249 -0.0033347 -0.0033348892 -0.0033355495 -0.0033362708 -0.0033368834][-0.0033374503 -0.0033347448 -0.0033355893 -0.0033366771 -0.0033373367 -0.0033373423 -0.0033370683 -0.0033367721 -0.0033363455 -0.0033357223 -0.0033350976 -0.0033348624 -0.003335231 -0.0033358831 -0.0033365733][-0.00333789 -0.003334936 -0.0033356147 -0.0033369954 -0.0033381979 -0.0033384995 -0.0033379872 -0.0033372203 -0.0033367905 -0.0033362403 -0.0033355753 -0.0033351588 -0.0033352987 -0.0033357847 -0.0033364217][-0.0033383796 -0.0033351334 -0.0033355388 -0.0033367323 -0.0033381344 -0.0033390166 -0.0033390692 -0.0033383071 -0.0033372482 -0.0033362964 -0.0033357022 -0.003335325 -0.0033354599 -0.0033358717 -0.0033364003][-0.0033402282 -0.0033365048 -0.0033360026 -0.0033364128 -0.0033373111 -0.0033384529 -0.0033393076 -0.0033392655 -0.0033382627 -0.0033368573 -0.0033358368 -0.0033354305 -0.0033355968 -0.0033360298 -0.0033365288][-0.0033439964 -0.0033393018 -0.0033375942 -0.0033368031 -0.0033368343 -0.0033375022 -0.0033384052 -0.0033389695 -0.0033387111 -0.003337662 -0.0033365362 -0.0033359269 -0.0033359712 -0.0033363253 -0.0033367616][-0.0033488071 -0.0033429936 -0.003339919 -0.0033379591 -0.0033369905 -0.00333678 -0.003337078 -0.0033375314 -0.0033376708 -0.0033372939 -0.0033367088 -0.0033364643 -0.0033365532 -0.0033368231 -0.0033371572][-0.0033541864 -0.0033473843 -0.0033430341 -0.0033396778 -0.0033374028 -0.0033363188 -0.0033359928 -0.00333617 -0.003336231 -0.0033359909 -0.0033358717 -0.0033361579 -0.0033365621 -0.0033370042 -0.0033373951][-0.003359298 -0.0033518781 -0.0033465372 -0.0033418422 -0.0033383865 -0.0033366997 -0.003336095 -0.0033360014 -0.0033358089 -0.0033354112 -0.0033351493 -0.0033354384 -0.0033360533 -0.0033367635 -0.0033373425][-0.0033631977 -0.0033550295 -0.0033490607 -0.0033438157 -0.003339797 -0.0033375367 -0.0033367195 -0.0033365644 -0.0033363944 -0.0033359719 -0.0033354589 -0.0033353814 -0.0033358226 -0.0033365241 -0.0033371893]]...]
INFO - root - 2017-12-09 12:06:51.246362: step 22610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 72h:57m:12s remains)
INFO - root - 2017-12-09 12:06:59.842640: step 22620, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:14m:04s remains)
INFO - root - 2017-12-09 12:07:08.425858: step 22630, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 71h:06m:43s remains)
INFO - root - 2017-12-09 12:07:16.748022: step 22640, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:28m:28s remains)
INFO - root - 2017-12-09 12:07:25.405766: step 22650, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 77h:53m:39s remains)
INFO - root - 2017-12-09 12:07:33.933057: step 22660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:18m:25s remains)
INFO - root - 2017-12-09 12:07:42.305135: step 22670, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 72h:07m:48s remains)
INFO - root - 2017-12-09 12:07:50.807102: step 22680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:00m:54s remains)
INFO - root - 2017-12-09 12:07:59.297942: step 22690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:43m:56s remains)
INFO - root - 2017-12-09 12:08:07.935369: step 22700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:54m:00s remains)
2017-12-09 12:08:08.929022: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25164086 0.2607421 0.26799572 0.27367556 0.27720022 0.27875212 0.27675444 0.26981738 0.26099548 0.24897081 0.23186387 0.21217184 0.19300519 0.17722163 0.16897766][0.26597813 0.28234059 0.29571041 0.30805066 0.31566441 0.3210313 0.3208549 0.31490457 0.305205 0.29122967 0.27073979 0.24887431 0.22641827 0.2077077 0.19700909][0.2707836 0.29407987 0.31396812 0.3329702 0.34553075 0.35619164 0.35852319 0.3561779 0.34859666 0.33317706 0.31235716 0.28737319 0.26183587 0.23934506 0.22496757][0.26826882 0.29669505 0.32262915 0.3479597 0.36635333 0.38238847 0.38892394 0.39132416 0.38589263 0.373067 0.35235542 0.3261241 0.29830876 0.27234033 0.25464502][0.2604368 0.29218829 0.32135355 0.35094762 0.37361196 0.39433482 0.40593237 0.41320148 0.41081187 0.40141624 0.38259223 0.35826561 0.33128852 0.30456775 0.28563467][0.24762776 0.2804825 0.312424 0.34477663 0.37107617 0.39526176 0.41047636 0.42262971 0.42349595 0.41741326 0.40099853 0.38075635 0.35655412 0.33161494 0.31227821][0.23202674 0.26444769 0.29656681 0.33043626 0.35895917 0.38629028 0.40435192 0.41948688 0.42262706 0.42021924 0.40711379 0.39021045 0.3693803 0.34756875 0.33025482][0.21691214 0.24737453 0.27903253 0.31210908 0.3417865 0.37040636 0.38980898 0.40550548 0.41033483 0.41022435 0.40054345 0.38744134 0.3714903 0.3546215 0.34056693][0.20115618 0.22847933 0.25793496 0.2903285 0.32009709 0.34854198 0.36940014 0.38457456 0.38970405 0.39098185 0.38318011 0.37393716 0.36205798 0.35069549 0.3408989][0.18465847 0.20910954 0.23567367 0.26548913 0.29370683 0.32176241 0.34275845 0.35737693 0.36375159 0.3651312 0.35924375 0.35196447 0.34357542 0.33670425 0.33062953][0.16733295 0.18804759 0.2109309 0.23743124 0.26299858 0.28917032 0.3090629 0.32303044 0.33087346 0.3323687 0.32910138 0.32355744 0.31768292 0.31319654 0.30919003][0.15196323 0.16871123 0.18671285 0.20776322 0.22945096 0.25239477 0.27059218 0.28407383 0.29257232 0.29568958 0.29454035 0.28948352 0.28500557 0.28106609 0.27957439][0.13831657 0.15247054 0.16696492 0.18315892 0.19982864 0.21740973 0.23290646 0.24453332 0.25288013 0.25649956 0.25601634 0.25312924 0.24964067 0.2465665 0.24662906][0.12364902 0.13570985 0.14704594 0.15989521 0.17246926 0.18527672 0.19699915 0.20622328 0.21330491 0.21672246 0.21683507 0.2151714 0.21287039 0.2107809 0.21227573][0.10926934 0.11931676 0.1280012 0.13781488 0.14693514 0.15617186 0.1645584 0.17094783 0.17615449 0.17854585 0.17884165 0.17788981 0.17627031 0.17497313 0.1778122]]...]
INFO - root - 2017-12-09 12:08:17.263890: step 22710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:39m:14s remains)
INFO - root - 2017-12-09 12:08:25.947688: step 22720, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 74h:13m:11s remains)
INFO - root - 2017-12-09 12:08:34.747811: step 22730, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 73h:03m:06s remains)
INFO - root - 2017-12-09 12:08:43.307036: step 22740, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 65h:40m:34s remains)
INFO - root - 2017-12-09 12:08:51.954659: step 22750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:32m:19s remains)
INFO - root - 2017-12-09 12:09:00.546768: step 22760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:08m:46s remains)
INFO - root - 2017-12-09 12:09:08.882133: step 22770, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 72h:57m:46s remains)
INFO - root - 2017-12-09 12:09:17.446860: step 22780, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.840 sec/batch; 72h:17m:08s remains)
INFO - root - 2017-12-09 12:09:25.928553: step 22790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:47m:40s remains)
INFO - root - 2017-12-09 12:09:34.482810: step 22800, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:53m:00s remains)
2017-12-09 12:09:35.325606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033786569 -0.0033773291 -0.0033762462 -0.0033750362 -0.0033741796 -0.0033735831 -0.0033734404 -0.0033731668 -0.0033732746 -0.0033734497 -0.0033735286 -0.0033736182 -0.0033735621 -0.0033735221 -0.0033734338][-0.0033789163 -0.0033779782 -0.0033768972 -0.0033754446 -0.0033743037 -0.0033734338 -0.0033731894 -0.0033727558 -0.00337271 -0.0033727228 -0.0033725982 -0.0033725749 -0.0033724795 -0.0033723167 -0.0033721307][-0.0033802798 -0.0033796723 -0.0033785647 -0.0033770709 -0.0033757205 -0.0033747347 -0.0033743132 -0.0033736092 -0.0033733107 -0.0033730709 -0.0033727062 -0.0033725221 -0.0033723891 -0.0033721724 -0.0033719819][-0.0033808586 -0.0033805494 -0.00338009 -0.003379121 -0.0033777603 -0.0033766273 -0.0033754194 -0.0033739563 -0.003373248 -0.0033732296 -0.0033732613 -0.0033731661 -0.0033730071 -0.0033725768 -0.0033722196][-0.0033800816 -0.0033808297 -0.0033815338 -0.0033811762 -0.003379581 -0.0033770099 -0.0033730078 -0.0033687209 -0.0033667656 -0.0033680305 -0.0033706247 -0.003372581 -0.0033735621 -0.0033734646 -0.0033728231][-0.0033796253 -0.0033808379 -0.0033823806 -0.0033828965 -0.0033805361 -0.0033743847 -0.0033634536 -0.0033515156 -0.00334585 -0.0033501543 -0.0033597697 -0.0033682918 -0.0033727461 -0.0033741996 -0.0033736448][-0.0033804858 -0.003381141 -0.0033828137 -0.0033835222 -0.0033799254 -0.003367306 -0.0033442259 -0.0033181082 -0.0033055558 -0.0033160939 -0.0033387796 -0.0033594144 -0.0033704138 -0.0033742052 -0.0033740897][-0.0033807356 -0.0033807959 -0.003382026 -0.0033827503 -0.0033776357 -0.0033573096 -0.0033186877 -0.0032738755 -0.0032516855 -0.0032703588 -0.0033100254 -0.0033471372 -0.0033671448 -0.0033742564 -0.0033744783][-0.0033798637 -0.0033797137 -0.0033809838 -0.0033813471 -0.0033752255 -0.0033493268 -0.0032988416 -0.0032388521 -0.0032062719 -0.0032291317 -0.0032834364 -0.0033354182 -0.0033639241 -0.0033740669 -0.0033747454][-0.0033787726 -0.0033780932 -0.003379293 -0.003379917 -0.003374021 -0.0033484283 -0.0032971231 -0.0032345895 -0.0031967051 -0.0032149849 -0.0032725916 -0.0033299259 -0.0033620333 -0.0033736432 -0.0033746115][-0.0033774753 -0.0033762082 -0.0033770318 -0.0033780567 -0.0033745272 -0.0033558463 -0.0033160395 -0.0032660691 -0.0032325541 -0.0032421502 -0.0032866725 -0.0033345544 -0.0033627797 -0.0033733174 -0.0033743177][-0.003376113 -0.0033744096 -0.0033746348 -0.00337586 -0.0033750567 -0.0033656675 -0.0033432406 -0.0033135712 -0.0032921683 -0.0032942072 -0.0033178357 -0.0033466702 -0.003365377 -0.0033728143 -0.003373628][-0.0033747565 -0.0033728818 -0.0033726743 -0.0033736923 -0.0033748753 -0.0033722785 -0.0033638522 -0.0033518702 -0.0033420497 -0.0033403926 -0.0033481442 -0.003360132 -0.0033688245 -0.0033725479 -0.0033730052][-0.0033738927 -0.0033720527 -0.0033717155 -0.0033724071 -0.003373899 -0.003374489 -0.0033730988 -0.0033704867 -0.0033677614 -0.0033658836 -0.0033662671 -0.0033689218 -0.0033711807 -0.0033723612 -0.0033724352][-0.0033735689 -0.0033718953 -0.0033715027 -0.0033718441 -0.0033728585 -0.00337394 -0.0033745696 -0.0033745083 -0.0033742264 -0.0033734315 -0.0033725258 -0.00337225 -0.0033722208 -0.0033722285 -0.0033720955]]...]
INFO - root - 2017-12-09 12:09:43.931127: step 22810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:17m:09s remains)
INFO - root - 2017-12-09 12:09:52.630217: step 22820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:46m:37s remains)
INFO - root - 2017-12-09 12:10:01.239743: step 22830, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 72h:47m:57s remains)
INFO - root - 2017-12-09 12:10:09.640258: step 22840, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 70h:59m:32s remains)
INFO - root - 2017-12-09 12:10:18.020443: step 22850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:59m:56s remains)
INFO - root - 2017-12-09 12:10:26.748360: step 22860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 75h:03m:14s remains)
INFO - root - 2017-12-09 12:10:35.424924: step 22870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:44m:07s remains)
INFO - root - 2017-12-09 12:10:43.990044: step 22880, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 75h:34m:49s remains)
INFO - root - 2017-12-09 12:10:52.769682: step 22890, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 76h:53m:44s remains)
INFO - root - 2017-12-09 12:11:01.372277: step 22900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:13m:42s remains)
2017-12-09 12:11:02.190809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033702403 -0.0033693882 -0.0033686042 -0.00336619 -0.0033624661 -0.0033586065 -0.0033559147 -0.0033554747 -0.0033572158 -0.0033605311 -0.0033639548 -0.0033666166 -0.0033682296 -0.0033687314 -0.0033688312][-0.003364237 -0.0033622412 -0.0033584607 -0.0033523256 -0.0033462336 -0.0033424038 -0.003341763 -0.0033443335 -0.0033489016 -0.003354155 -0.0033585846 -0.0033617378 -0.0033639327 -0.0033651802 -0.0033657621][-0.0033531317 -0.0033451614 -0.003334763 -0.0033227787 -0.0033121551 -0.0033070282 -0.0033099884 -0.0033204465 -0.0033345791 -0.0033481172 -0.0033577587 -0.0033629306 -0.0033651548 -0.0033658904 -0.0033660212][-0.0033344002 -0.0033180804 -0.0032976319 -0.0032763176 -0.0032594944 -0.0032519379 -0.0032562162 -0.0032730429 -0.0032990861 -0.0033264137 -0.0033477137 -0.0033598896 -0.0033648587 -0.0033662184 -0.0033661542][-0.0033117882 -0.0032824127 -0.0032512816 -0.0032225854 -0.0032010344 -0.0031896112 -0.0031925598 -0.0032127989 -0.0032482361 -0.0032899957 -0.003325969 -0.0033492297 -0.0033602486 -0.0033641248 -0.0033648633][-0.0032902115 -0.0032509603 -0.0032081979 -0.0031685464 -0.003138778 -0.0031230696 -0.0031255318 -0.0031486931 -0.0031913144 -0.0032440901 -0.0032941212 -0.0033306482 -0.0033509179 -0.0033592682 -0.0033618934][-0.0032488548 -0.0032065075 -0.0031628734 -0.0031229961 -0.0030913849 -0.0030714329 -0.0030683102 -0.0030871467 -0.0031298366 -0.0031887265 -0.0032498122 -0.0033000584 -0.0033329222 -0.0033502972 -0.0033579718][-0.0032001256 -0.0031622862 -0.0031258673 -0.003093225 -0.0030667512 -0.0030486053 -0.0030430574 -0.003054929 -0.0030877667 -0.0031404048 -0.003203938 -0.003264593 -0.0033102981 -0.00333888 -0.0033537825][-0.0031722717 -0.0031438794 -0.0031177716 -0.00309449 -0.0030746174 -0.0030603879 -0.0030551823 -0.0030630613 -0.0030873411 -0.0031283691 -0.0031818214 -0.0032386708 -0.003287971 -0.0033235564 -0.0033445195][-0.0031834308 -0.0031661023 -0.0031507304 -0.0031363447 -0.0031234068 -0.0031132333 -0.0031086644 -0.0031135457 -0.0031304522 -0.0031593991 -0.0031981305 -0.003241081 -0.0032815118 -0.0033136518 -0.0033353167][-0.0032235838 -0.0032144759 -0.0032066384 -0.0031991571 -0.0031926571 -0.0031881034 -0.0031859169 -0.0031885821 -0.0031974742 -0.0032139278 -0.0032366468 -0.0032635005 -0.0032910448 -0.0033140089 -0.003330295][-0.0032734172 -0.0032683082 -0.003264237 -0.0032604535 -0.0032570474 -0.0032548185 -0.0032539144 -0.0032550716 -0.0032590365 -0.003267068 -0.003278024 -0.0032911818 -0.0033050182 -0.0033175368 -0.0033267417][-0.0033114464 -0.0033077919 -0.0033053588 -0.0033035085 -0.0033016207 -0.0033002091 -0.0032994365 -0.0032995334 -0.003300658 -0.0033037735 -0.0033084052 -0.0033138355 -0.0033193629 -0.0033246605 -0.0033285154][-0.0033384212 -0.0033349369 -0.0033330277 -0.003331858 -0.0033309283 -0.0033301907 -0.0033297485 -0.0033297008 -0.0033298517 -0.0033307152 -0.0033325742 -0.0033350519 -0.0033376631 -0.0033402902 -0.0033422539][-0.0033586749 -0.0033555734 -0.0033540097 -0.0033533212 -0.0033527212 -0.0033523173 -0.0033518667 -0.0033516674 -0.0033514819 -0.0033517901 -0.0033526069 -0.0033539836 -0.0033559967 -0.0033582733 -0.0033602403]]...]
INFO - root - 2017-12-09 12:11:10.751454: step 22910, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 75h:43m:44s remains)
INFO - root - 2017-12-09 12:11:19.497257: step 22920, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 77h:14m:40s remains)
INFO - root - 2017-12-09 12:11:28.238723: step 22930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:03m:57s remains)
INFO - root - 2017-12-09 12:11:36.898004: step 22940, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:17m:27s remains)
INFO - root - 2017-12-09 12:11:45.350062: step 22950, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:03m:23s remains)
INFO - root - 2017-12-09 12:11:54.085206: step 22960, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 73h:50m:36s remains)
INFO - root - 2017-12-09 12:12:02.856557: step 22970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:04m:06s remains)
INFO - root - 2017-12-09 12:12:11.370835: step 22980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:59m:35s remains)
INFO - root - 2017-12-09 12:12:20.104929: step 22990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:11m:53s remains)
INFO - root - 2017-12-09 12:12:28.726365: step 23000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:02m:38s remains)
2017-12-09 12:12:29.593117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26152292 0.26277897 0.26614243 0.2723465 0.2808857 0.28690177 0.28826913 0.28175086 0.26658648 0.24430226 0.21749927 0.19136009 0.16630185 0.14288272 0.11910693][0.22879091 0.23298664 0.24036275 0.25175607 0.26585019 0.27671215 0.28185898 0.27736917 0.26359311 0.24166155 0.21415922 0.18601686 0.15899567 0.13401212 0.10917153][0.18641976 0.19176243 0.20228143 0.21868861 0.23847808 0.25530142 0.26533446 0.26419052 0.25219247 0.22989222 0.20067242 0.16972144 0.14010032 0.11342078 0.088759229][0.14990975 0.15764081 0.17152599 0.19230007 0.216624 0.23807001 0.2514866 0.25225779 0.24085502 0.21727996 0.18570879 0.15097657 0.11797675 0.089261711 0.064508304][0.12125832 0.13246164 0.15076798 0.17639925 0.20515525 0.22998445 0.24553426 0.24677168 0.23403491 0.20816456 0.17336263 0.13498548 0.099109523 0.068802558 0.044527519][0.10568939 0.12132189 0.14382228 0.17322059 0.20454659 0.2304589 0.24575172 0.24600947 0.23133521 0.20273936 0.16536976 0.12480737 0.087493874 0.056804724 0.033404026][0.10488811 0.12408422 0.14941953 0.18079154 0.21261659 0.23790926 0.251444 0.24996553 0.23356923 0.20339026 0.16497947 0.12393607 0.086631 0.05631908 0.033764448][0.11717959 0.13883361 0.16429722 0.19336535 0.22141483 0.2429124 0.25338238 0.25090432 0.23517442 0.2071069 0.17112736 0.13207565 0.09617085 0.066634692 0.04452939][0.13309108 0.15707892 0.18206255 0.20730866 0.22932702 0.24463603 0.2504566 0.24654843 0.23234814 0.2088359 0.17857993 0.14503172 0.11306093 0.08542718 0.063721657][0.14673217 0.17024232 0.19175757 0.21122704 0.22634074 0.23498292 0.2362531 0.23141751 0.22031657 0.20327519 0.18106706 0.15530363 0.12931007 0.10516009 0.084467582][0.15712808 0.17801116 0.19438855 0.20682363 0.21421759 0.21597475 0.21263395 0.20677373 0.19861205 0.1879922 0.1744004 0.15765479 0.13910098 0.11962466 0.10087411][0.16289546 0.17892893 0.18816124 0.19240519 0.19183964 0.18718907 0.18025084 0.17413099 0.16924976 0.16515926 0.15997438 0.15191554 0.14061385 0.12604295 0.10945738][0.16117893 0.1719659 0.17391315 0.16962904 0.16076458 0.1495899 0.13926597 0.13317803 0.1316503 0.13374119 0.13646546 0.13672353 0.13241048 0.12275648 0.10876021][0.14882033 0.15429777 0.15033658 0.14010912 0.12582554 0.1104525 0.097945206 0.091560245 0.0917224 0.0973964 0.10502782 0.11098082 0.11203994 0.10674819 0.095660962][0.12851195 0.12925415 0.12117678 0.10764473 0.090825096 0.073855825 0.060597248 0.054003339 0.05444096 0.060824312 0.069932446 0.078234747 0.082161553 0.080105469 0.072086141]]...]
INFO - root - 2017-12-09 12:12:38.075705: step 23010, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 73h:18m:59s remains)
INFO - root - 2017-12-09 12:12:46.707135: step 23020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:22m:56s remains)
INFO - root - 2017-12-09 12:12:55.389225: step 23030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:20m:34s remains)
INFO - root - 2017-12-09 12:13:03.925002: step 23040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:10m:44s remains)
INFO - root - 2017-12-09 12:13:12.396505: step 23050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:35m:00s remains)
INFO - root - 2017-12-09 12:13:21.045936: step 23060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:31m:03s remains)
INFO - root - 2017-12-09 12:13:29.635708: step 23070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:24m:50s remains)
INFO - root - 2017-12-09 12:13:38.080955: step 23080, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:31m:15s remains)
INFO - root - 2017-12-09 12:13:46.730598: step 23090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:38m:50s remains)
INFO - root - 2017-12-09 12:13:55.490761: step 23100, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 77h:00m:42s remains)
2017-12-09 12:13:56.421493: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31509939 0.31984857 0.32470682 0.32683468 0.32236734 0.31083614 0.2917347 0.26932019 0.24524926 0.22483633 0.20913617 0.19945835 0.19422147 0.19280535 0.19183539][0.36978063 0.38055924 0.3899059 0.39416152 0.39121053 0.38095784 0.3617999 0.33691841 0.30872133 0.28389674 0.26357129 0.250038 0.241673 0.238206 0.23607925][0.41670945 0.43386257 0.44794998 0.4562175 0.45698026 0.44856423 0.43006408 0.4036966 0.37226352 0.34216589 0.31631166 0.2980589 0.28556108 0.27858236 0.27299854][0.44670373 0.46958846 0.48824605 0.50034761 0.50556725 0.5008657 0.48403525 0.45700017 0.42276174 0.38809958 0.35680237 0.33238503 0.31458247 0.30348623 0.29479733][0.45800614 0.48499557 0.50687188 0.52342641 0.534549 0.53481972 0.52163541 0.49475959 0.45858881 0.41974297 0.38318831 0.35359293 0.33102658 0.31542906 0.30316144][0.45780319 0.48736435 0.51048183 0.53001839 0.54545707 0.550024 0.53947645 0.51379126 0.47696081 0.4349077 0.39418539 0.3594017 0.33165413 0.31187102 0.29675281][0.44760653 0.478691 0.5024091 0.52403969 0.54241055 0.550118 0.5420717 0.51649344 0.47828275 0.43350351 0.38931394 0.34996173 0.31801653 0.29410914 0.27662367][0.43850359 0.46812513 0.48916665 0.51103789 0.53042287 0.53898633 0.53130633 0.50514811 0.46574825 0.41786978 0.37023687 0.32660526 0.29061806 0.2635583 0.24444407][0.43183076 0.45747635 0.47280183 0.49020565 0.5059523 0.51228583 0.5027293 0.47539151 0.43423325 0.38435638 0.3348653 0.28852493 0.25029808 0.22144105 0.20226285][0.42284417 0.44507298 0.45373926 0.4643091 0.4737629 0.47473148 0.46105602 0.43056837 0.38688824 0.33527163 0.28426668 0.23727039 0.19923812 0.17079537 0.15351003][0.41015381 0.42800352 0.42991066 0.43323886 0.43509531 0.42955974 0.41011938 0.37581748 0.3297514 0.27707839 0.22672676 0.18120311 0.14540084 0.11980223 0.10674929][0.39093313 0.40345094 0.39949334 0.39719018 0.39395902 0.38385582 0.36027274 0.32239816 0.27391282 0.22070412 0.17148715 0.12921332 0.097913571 0.077191651 0.069633536][0.3695845 0.37579536 0.36585733 0.35828057 0.35072097 0.33755222 0.31194991 0.27338082 0.22515161 0.17327604 0.12624368 0.088013507 0.061403334 0.046110924 0.044308696][0.3434968 0.34395912 0.32976043 0.31808537 0.30709377 0.29240137 0.26646128 0.22918722 0.18315047 0.13443728 0.091131 0.057299189 0.035614341 0.025309036 0.028674172][0.31266832 0.31100783 0.29556769 0.28170466 0.26820844 0.25215882 0.22635329 0.19111668 0.14827251 0.10375717 0.065151647 0.036252957 0.019670352 0.014486839 0.022617515]]...]
INFO - root - 2017-12-09 12:14:04.899528: step 23110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:46m:46s remains)
INFO - root - 2017-12-09 12:14:13.606055: step 23120, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:50m:49s remains)
INFO - root - 2017-12-09 12:14:22.217591: step 23130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:45m:25s remains)
INFO - root - 2017-12-09 12:14:30.868489: step 23140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:46m:30s remains)
INFO - root - 2017-12-09 12:14:39.346229: step 23150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:37m:12s remains)
INFO - root - 2017-12-09 12:14:47.983491: step 23160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:08m:11s remains)
INFO - root - 2017-12-09 12:14:56.471850: step 23170, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:46m:06s remains)
INFO - root - 2017-12-09 12:15:05.007618: step 23180, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 73h:08m:39s remains)
INFO - root - 2017-12-09 12:15:13.787474: step 23190, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 74h:28m:27s remains)
INFO - root - 2017-12-09 12:15:22.504814: step 23200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:50m:17s remains)
2017-12-09 12:15:23.439138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027462109 -0.0021911906 -0.001467739 -0.00079736928 -0.00046945619 -0.0006870504 -0.001316607 -0.0020756475 -0.0026737698 -0.0029961828 -0.0030513424 -0.0029052268 -0.0025728061 -0.001890854 -0.00057006977][-0.0022951388 -0.0013994717 -0.00027550734 0.00080103078 0.0013757723 0.0011166173 0.00014726375 -0.0010477377 -0.0020585028 -0.0027052294 -0.0029432094 -0.0029092976 -0.0026810509 -0.0022175666 -0.0012559602][-0.0015321379 -2.9780902e-05 0.0018141887 0.003543074 0.0044110958 0.0039947135 0.002396235 0.00045101717 -0.0012227569 -0.0022393093 -0.0026035584 -0.0025998405 -0.0024443413 -0.0022281813 -0.0017262152][-0.00032973476 0.0019667449 0.0047025457 0.0072051808 0.0084211491 0.0077306079 0.0053624548 0.0024894418 -3.4252182e-05 -0.0015614942 -0.0020953221 -0.0020658514 -0.0018442775 -0.0016626976 -0.0013600665][0.0013628502 0.0045603095 0.00821076 0.011413142 0.012816897 0.011731183 0.0085027274 0.0046042651 0.0012171168 -0.00078676431 -0.0014622002 -0.001347936 -0.00093624345 -0.00058412855 -0.00020532357][0.0034858037 0.007593683 0.012163332 0.015968107 0.017336408 0.015615854 0.011424738 0.0065123755 0.0023322923 -0.0001005244 -0.000853505 -0.00054809777 0.00019194605 0.00087135681 0.0015352063][0.0062010055 0.011095088 0.01635634 0.020382319 0.021217881 0.018488171 0.013269553 0.0075782696 0.002951781 0.00037850346 -0.00030676671 0.00026267883 0.0014140767 0.0025825312 0.003709649][0.0091743665 0.014436653 0.01983677 0.023491874 0.023475667 0.01966968 0.013673 0.0076801125 0.0030884321 0.0007129719 0.00025779638 0.0011687067 0.0028137497 0.0046160771 0.0063217469][0.01212163 0.01702486 0.02177025 0.02461179 0.02386893 0.019469341 0.013329817 0.0075053154 0.0032398521 0.0011740935 0.000949641 0.0021471938 0.0042724768 0.0067402376 0.0090402961][0.014444444 0.01849659 0.022110982 0.024007279 0.022911711 0.018620959 0.013017695 0.0077626631 0.0039655827 0.0021113746 0.0019882491 0.0033874947 0.0058962703 0.0088715665 0.011575139][0.015891541 0.018840635 0.021161471 0.022265978 0.021227939 0.017583616 0.012941137 0.00847262 0.0051920088 0.0035032697 0.0033781433 0.0047766604 0.0074308123 0.010585506 0.013469508][0.0161543 0.01806842 0.019280642 0.019891562 0.019154765 0.016415175 0.012948088 0.0093654972 0.0066125505 0.0049847104 0.0046835328 0.0058397129 0.0083409706 0.011409576 0.014262579][0.015023496 0.01605772 0.016377997 0.016770354 0.016473582 0.014782907 0.012544015 0.0099119414 0.0077698436 0.0062366016 0.0057434626 0.0065012686 0.0085265376 0.011084511 0.01351044][0.012864798 0.013127655 0.012718728 0.012885705 0.012907829 0.012147776 0.01098642 0.0093105827 0.0078275464 0.0064882832 0.0059186341 0.006295939 0.0077350345 0.0095960274 0.011346634][0.0093953479 0.0093470374 0.0086894045 0.00869792 0.0087892758 0.008589 0.0080455318 0.0071120132 0.0061902972 0.0051277811 0.0045701843 0.0046405448 0.0055117738 0.0066911792 0.0078273127]]...]
INFO - root - 2017-12-09 12:15:32.031512: step 23210, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:28m:53s remains)
INFO - root - 2017-12-09 12:15:40.813124: step 23220, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 75h:01m:49s remains)
INFO - root - 2017-12-09 12:15:49.407616: step 23230, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 71h:29m:15s remains)
INFO - root - 2017-12-09 12:15:58.084335: step 23240, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.887 sec/batch; 76h:14m:16s remains)
INFO - root - 2017-12-09 12:16:06.728318: step 23250, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:18m:14s remains)
INFO - root - 2017-12-09 12:16:15.466459: step 23260, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 75h:29m:58s remains)
INFO - root - 2017-12-09 12:16:24.142860: step 23270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:03m:07s remains)
INFO - root - 2017-12-09 12:16:32.843691: step 23280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:50m:44s remains)
INFO - root - 2017-12-09 12:16:41.488172: step 23290, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 74h:42m:29s remains)
INFO - root - 2017-12-09 12:16:50.228258: step 23300, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:04m:37s remains)
2017-12-09 12:16:51.099117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13989431 0.14115596 0.14071085 0.13970935 0.13829842 0.13741887 0.1372416 0.1374546 0.13827543 0.13958132 0.14072125 0.14012237 0.13576448 0.12650372 0.11011518][0.14423694 0.14542726 0.14364706 0.14126986 0.13791192 0.13504504 0.13313563 0.13254723 0.13331285 0.13567756 0.13890444 0.14086077 0.13896015 0.13179815 0.11671927][0.14258887 0.14360112 0.14072108 0.13683724 0.13115093 0.12580095 0.1214977 0.11935095 0.11969548 0.12317955 0.12873568 0.13400401 0.13568442 0.13168366 0.11897044][0.13842402 0.13933823 0.13562784 0.13054201 0.12297888 0.11536473 0.1086303 0.1047207 0.1042852 0.10850929 0.11630449 0.12503012 0.13042329 0.12986855 0.12022278][0.13191822 0.13300855 0.12852488 0.12246566 0.11358441 0.10430651 0.095783904 0.090516254 0.089452684 0.094182439 0.10352899 0.11474907 0.12309711 0.12554057 0.1186654][0.12046996 0.12221108 0.11800939 0.11189417 0.10273003 0.092772566 0.083369091 0.077347964 0.075973555 0.080906361 0.091028281 0.10340611 0.11314722 0.11731992 0.1126034][0.10380252 0.10601489 0.10211234 0.096955508 0.089084975 0.079794832 0.070838168 0.065017246 0.0638013 0.068653688 0.078617513 0.09074109 0.10043997 0.10502505 0.10145223][0.0821738 0.084746368 0.081832953 0.078026265 0.071762577 0.064208925 0.056748193 0.051830389 0.051278014 0.05596786 0.06510181 0.0759003 0.084478289 0.088567138 0.08561758][0.057382967 0.059758224 0.058192827 0.056107756 0.051893897 0.046183482 0.040379759 0.036885206 0.0371341 0.041429847 0.049287595 0.058061782 0.064874582 0.068030372 0.065548554][0.034104995 0.0357418 0.03506346 0.034024537 0.031503387 0.027952896 0.024154561 0.022019124 0.022683151 0.026344791 0.03245829 0.038900018 0.043871306 0.046099313 0.044184331][0.016178738 0.01680351 0.016575556 0.016276382 0.014982396 0.01295404 0.010829075 0.0098816454 0.010663061 0.013306473 0.017490875 0.021855718 0.025263822 0.026806843 0.025621748][0.00450781 0.0047386754 0.0047165295 0.0046029044 0.0040669609 0.0032422107 0.0024142878 0.0021625098 0.0027759525 0.0043919589 0.0068371054 0.0094379364 0.011573166 0.012663535 0.012125512][-0.001224064 -0.0011488835 -0.0011143845 -0.0011149654 -0.0012900704 -0.0015934827 -0.0018354981 -0.0017854918 -0.0013562776 -0.00052347663 0.00065577147 0.0019088525 0.0029956067 0.0036682293 0.0035299587][-0.0031877586 -0.0031688916 -0.0031419806 -0.0031138775 -0.0031202624 -0.0031503441 -0.0031748342 -0.0031378709 -0.002988952 -0.0026992932 -0.0022886675 -0.0018449627 -0.0014473079 -0.0011787596 -0.0012096777][-0.0033215987 -0.003312981 -0.0033111842 -0.0033108327 -0.0033133419 -0.0033165128 -0.0033171414 -0.0033087456 -0.0032788049 -0.0032219661 -0.0031405021 -0.0030481638 -0.002953412 -0.0028777025 -0.0028801896]]...]
INFO - root - 2017-12-09 12:16:59.512435: step 23310, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 72h:19m:29s remains)
INFO - root - 2017-12-09 12:17:08.288035: step 23320, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 77h:59m:17s remains)
INFO - root - 2017-12-09 12:17:16.998798: step 23330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 75h:39m:39s remains)
INFO - root - 2017-12-09 12:17:25.732106: step 23340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:08m:56s remains)
INFO - root - 2017-12-09 12:17:34.344779: step 23350, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 75h:29m:06s remains)
INFO - root - 2017-12-09 12:17:43.141041: step 23360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:35m:56s remains)
INFO - root - 2017-12-09 12:17:51.700220: step 23370, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 74h:44m:27s remains)
INFO - root - 2017-12-09 12:18:00.257997: step 23380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:14m:29s remains)
INFO - root - 2017-12-09 12:18:08.866479: step 23390, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 68h:54m:23s remains)
INFO - root - 2017-12-09 12:18:17.393184: step 23400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:46m:34s remains)
2017-12-09 12:18:18.266443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033751139 -0.0033744192 -0.0033754597 -0.0033761754 -0.0033762732 -0.0033755894 -0.0033747265 -0.0033741551 -0.0033742306 -0.0033743996 -0.003374764 -0.0033752169 -0.0033751815 -0.003374632 -0.003373842][-0.0033761468 -0.0033752804 -0.0033758308 -0.0033754632 -0.0033738683 -0.0033711521 -0.0033683295 -0.00336698 -0.0033673707 -0.0033690906 -0.0033713151 -0.0033737018 -0.003375344 -0.0033755256 -0.0033746511][-0.0033794374 -0.0033781009 -0.0033777256 -0.0033753191 -0.0033710019 -0.0033652366 -0.003359721 -0.0033565778 -0.0033562786 -0.0033595536 -0.0033645814 -0.0033700573 -0.0033743158 -0.0033760071 -0.0033756474][-0.0033834754 -0.0033818097 -0.0033798045 -0.0033749647 -0.0033673197 -0.0033575643 -0.0033480392 -0.0033416385 -0.003340398 -0.0033455091 -0.003354748 -0.0033642973 -0.0033716082 -0.003375398 -0.003376344][-0.0033858377 -0.0033838984 -0.00338061 -0.0033734634 -0.0033621977 -0.0033475384 -0.0033335122 -0.0033236928 -0.0033212882 -0.0033283522 -0.0033418753 -0.0033558954 -0.0033672124 -0.0033742932 -0.0033768713][-0.0033859429 -0.0033841585 -0.0033806376 -0.0033713982 -0.0033567424 -0.0033383011 -0.003320697 -0.0033087141 -0.0033054317 -0.0033128583 -0.0033280472 -0.0033456411 -0.0033604184 -0.0033703235 -0.0033746667][-0.0033842511 -0.0033830188 -0.0033790453 -0.0033687651 -0.0033532025 -0.0033340033 -0.0033151673 -0.003301943 -0.0032976591 -0.0033037439 -0.0033181387 -0.0033367549 -0.0033539359 -0.0033663961 -0.0033726075][-0.0033813862 -0.0033797249 -0.0033760059 -0.0033668678 -0.0033529568 -0.0033355635 -0.0033178269 -0.0033044645 -0.0032986598 -0.00330247 -0.0033147214 -0.0033321769 -0.0033496825 -0.0033629152 -0.0033704014][-0.0033793484 -0.0033769717 -0.0033735486 -0.0033663705 -0.0033550505 -0.0033404338 -0.0033248165 -0.0033123977 -0.0033058561 -0.0033072238 -0.0033164108 -0.0033312908 -0.0033473663 -0.003360538 -0.0033684252][-0.0033787633 -0.0033766082 -0.0033737822 -0.003368309 -0.0033596437 -0.0033479335 -0.0033355148 -0.0033248009 -0.0033178392 -0.0033168246 -0.0033229182 -0.0033343751 -0.0033479529 -0.003360034 -0.0033672708][-0.0033778492 -0.0033763759 -0.0033748639 -0.0033716923 -0.0033658163 -0.0033573403 -0.0033474721 -0.0033385735 -0.0033322116 -0.0033301362 -0.0033332095 -0.0033407977 -0.0033509354 -0.0033602696 -0.0033659548][-0.0033762129 -0.0033752634 -0.0033748003 -0.0033735402 -0.0033704762 -0.0033654827 -0.0033588812 -0.003351778 -0.0033465361 -0.0033438227 -0.0033449344 -0.0033493612 -0.0033561287 -0.0033624943 -0.0033667553][-0.0033749596 -0.0033739505 -0.0033738811 -0.0033736855 -0.0033725691 -0.0033703626 -0.0033669265 -0.0033625897 -0.0033585485 -0.0033561983 -0.0033560754 -0.0033582014 -0.0033616929 -0.0033651178 -0.0033678773][-0.0033742615 -0.0033728895 -0.003372679 -0.0033728129 -0.0033723647 -0.0033714361 -0.0033698778 -0.0033682664 -0.0033662803 -0.0033646526 -0.0033643288 -0.0033649986 -0.0033662815 -0.003367732 -0.0033688697][-0.0033735433 -0.0033721495 -0.003371974 -0.0033719814 -0.0033717158 -0.0033711093 -0.0033701898 -0.0033694238 -0.0033688087 -0.0033681802 -0.003367573 -0.0033676911 -0.0033684757 -0.0033693591 -0.0033697954]]...]
INFO - root - 2017-12-09 12:18:26.651912: step 23410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:07m:35s remains)
INFO - root - 2017-12-09 12:18:35.165936: step 23420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:01m:33s remains)
INFO - root - 2017-12-09 12:18:43.733578: step 23430, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 71h:58m:31s remains)
INFO - root - 2017-12-09 12:18:52.430883: step 23440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:33m:35s remains)
INFO - root - 2017-12-09 12:19:01.018371: step 23450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:54m:29s remains)
INFO - root - 2017-12-09 12:19:09.515300: step 23460, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:33m:35s remains)
INFO - root - 2017-12-09 12:19:18.169015: step 23470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 71h:53m:53s remains)
INFO - root - 2017-12-09 12:19:26.840954: step 23480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 75h:58m:35s remains)
INFO - root - 2017-12-09 12:19:35.502533: step 23490, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 77h:15m:02s remains)
INFO - root - 2017-12-09 12:19:44.167871: step 23500, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:30m:25s remains)
2017-12-09 12:19:45.005579: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12660974 0.11768026 0.10635429 0.095108181 0.087070785 0.084846132 0.086987689 0.089582928 0.090656213 0.087169006 0.079596512 0.065502107 0.048619136 0.03121887 0.017090246][0.15109459 0.140477 0.12806067 0.11710319 0.11134831 0.11235113 0.11776973 0.12355675 0.12652248 0.12308942 0.1132018 0.094831571 0.071649462 0.047393672 0.027041074][0.16735877 0.15721367 0.14615695 0.13800386 0.13751586 0.14410457 0.15447313 0.16280207 0.16589205 0.16103846 0.14818497 0.1256611 0.096639156 0.065599516 0.038807005][0.16891156 0.16120936 0.15452984 0.1525638 0.16012995 0.17477787 0.19202213 0.20365044 0.2067526 0.19934487 0.18221469 0.15476012 0.1199706 0.082721666 0.050003987][0.15924965 0.15398484 0.15201344 0.15691812 0.17325316 0.19594038 0.21970646 0.235425 0.23991206 0.23115537 0.21103483 0.17966172 0.14036228 0.097891286 0.060103532][0.14718269 0.14325047 0.1442074 0.1539938 0.17659715 0.20538309 0.23457178 0.25401953 0.26050738 0.25208923 0.231065 0.19745515 0.15534991 0.10927419 0.067892537][0.13763583 0.13414118 0.13670433 0.14833708 0.17344403 0.20487231 0.23727719 0.26005125 0.26932558 0.26247722 0.24203253 0.20805046 0.1647218 0.11648555 0.0727968][0.13394561 0.12857123 0.12976606 0.14025626 0.16357209 0.19428721 0.22737001 0.25273192 0.26587164 0.26287037 0.24517262 0.21213734 0.16857111 0.1193939 0.074555792][0.13977665 0.12966141 0.12584193 0.13121437 0.14891154 0.17520392 0.20576876 0.23267189 0.25000814 0.25241885 0.23921749 0.20889103 0.16652021 0.11770482 0.073068194][0.15029137 0.13628983 0.12695621 0.12543379 0.13535547 0.15430146 0.17939982 0.20542473 0.22538255 0.23292755 0.22455788 0.19813304 0.1584345 0.11168192 0.068802409][0.157779 0.14104763 0.12766373 0.12089998 0.1237396 0.13547181 0.15461908 0.17717403 0.19680081 0.20696607 0.20204292 0.17954254 0.14353031 0.10076599 0.061516438][0.16125403 0.14213417 0.12521692 0.11392344 0.11081383 0.11602062 0.1291413 0.14772667 0.1655672 0.17582938 0.17253275 0.15352981 0.1220533 0.084869422 0.051018368][0.15707481 0.13677424 0.11736134 0.10254935 0.094623677 0.094516836 0.10231473 0.11616475 0.13068607 0.13984437 0.13745175 0.12161034 0.095457785 0.065298185 0.038287103][0.14142519 0.12237184 0.10310774 0.087254919 0.07714358 0.07354793 0.077126361 0.086429484 0.096899495 0.10332468 0.10073731 0.0881863 0.068048365 0.045447364 0.025738318][0.11397913 0.098498434 0.082224905 0.068130314 0.058425885 0.053842328 0.055116024 0.06091525 0.067737035 0.0715614 0.068830386 0.059185658 0.044612736 0.028909171 0.015558144]]...]
INFO - root - 2017-12-09 12:19:53.519009: step 23510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:46m:33s remains)
INFO - root - 2017-12-09 12:20:02.095906: step 23520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:46m:37s remains)
INFO - root - 2017-12-09 12:20:10.786410: step 23530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:48m:52s remains)
INFO - root - 2017-12-09 12:20:19.472188: step 23540, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 73h:10m:53s remains)
INFO - root - 2017-12-09 12:20:28.081424: step 23550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 73h:59m:10s remains)
INFO - root - 2017-12-09 12:20:36.645062: step 23560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 74h:07m:14s remains)
INFO - root - 2017-12-09 12:20:45.156762: step 23570, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:32m:28s remains)
INFO - root - 2017-12-09 12:20:53.659261: step 23580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 75h:57m:30s remains)
INFO - root - 2017-12-09 12:21:02.265623: step 23590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:54m:48s remains)
INFO - root - 2017-12-09 12:21:10.959485: step 23600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:54m:15s remains)
2017-12-09 12:21:11.816000: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.53896159 0.58064717 0.61520886 0.63870895 0.64808929 0.64308006 0.62822813 0.608566 0.58353537 0.55516344 0.52782744 0.50401449 0.48551404 0.47281769 0.46775046][0.50766182 0.5572142 0.60032272 0.63224542 0.64884871 0.64960915 0.63855386 0.62420011 0.60273188 0.577052 0.55025 0.52284968 0.49958163 0.4797945 0.46891427][0.46047112 0.515673 0.56733549 0.60721433 0.632663 0.64174235 0.63702464 0.62768322 0.60784507 0.583653 0.556597 0.52641338 0.4992348 0.47482288 0.45948413][0.41109705 0.46916586 0.52529645 0.57143247 0.60582846 0.62473959 0.63035029 0.62443882 0.60620838 0.582782 0.55398273 0.52081609 0.48994496 0.462717 0.4435834][0.35464951 0.41278878 0.46977413 0.520234 0.56055486 0.58776879 0.60114926 0.60352123 0.59139389 0.57017082 0.54277939 0.50939631 0.47623453 0.44576 0.42304465][0.30913624 0.36508349 0.4228842 0.47590104 0.51872736 0.54964644 0.56587762 0.57099205 0.56181389 0.54450405 0.5200457 0.4900524 0.46051851 0.43097165 0.40759948][0.28034678 0.33169633 0.38753092 0.4398801 0.48398471 0.51576853 0.534423 0.54170114 0.53561914 0.5216 0.50124967 0.47661227 0.45123422 0.42536104 0.40348449][0.25312862 0.30030087 0.35318336 0.40293014 0.44739309 0.48132572 0.50430304 0.51530707 0.51419288 0.50540507 0.49011657 0.47166517 0.45122251 0.42822245 0.40789914][0.23343922 0.27669379 0.32638979 0.37526056 0.42036942 0.453632 0.47876167 0.49434879 0.50053525 0.49838603 0.48966494 0.47851259 0.46340114 0.44395372 0.42450559][0.21647698 0.25875923 0.3076728 0.35389683 0.39762324 0.43307871 0.45766959 0.4758704 0.48748165 0.4941152 0.49482226 0.49213088 0.48413876 0.4682582 0.45003426][0.2170693 0.25790238 0.30675116 0.35278934 0.39495823 0.42847633 0.45264244 0.4717558 0.48593354 0.49761671 0.5046162 0.508748 0.507784 0.49840015 0.48390603][0.22853567 0.2711499 0.31927443 0.36456227 0.40603647 0.43832916 0.46220845 0.48217124 0.49841982 0.51236486 0.52371937 0.53299212 0.53711206 0.53248447 0.52134478][0.25022265 0.29610258 0.34388846 0.38951361 0.43014729 0.46202624 0.48585355 0.50485116 0.52064508 0.53520876 0.54810339 0.55966556 0.56686026 0.56618547 0.558374][0.28866255 0.33532318 0.38205165 0.42696425 0.46600312 0.49664527 0.51901013 0.53648973 0.55091476 0.56374145 0.57550532 0.58693117 0.59533054 0.59698689 0.59166026][0.3375062 0.38435251 0.42907369 0.4709172 0.50526392 0.53248447 0.55171674 0.56604958 0.57747096 0.58801365 0.59819639 0.60802412 0.61647385 0.61971277 0.61665249]]...]
INFO - root - 2017-12-09 12:21:20.140524: step 23610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:07m:02s remains)
INFO - root - 2017-12-09 12:21:28.872370: step 23620, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 75h:19m:44s remains)
INFO - root - 2017-12-09 12:21:37.570000: step 23630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:24m:10s remains)
INFO - root - 2017-12-09 12:21:46.197717: step 23640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:49m:31s remains)
INFO - root - 2017-12-09 12:21:54.662859: step 23650, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:10m:03s remains)
INFO - root - 2017-12-09 12:22:03.159911: step 23660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:07m:07s remains)
INFO - root - 2017-12-09 12:22:11.832652: step 23670, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 74h:25m:54s remains)
INFO - root - 2017-12-09 12:22:20.321966: step 23680, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 76h:48m:41s remains)
INFO - root - 2017-12-09 12:22:28.916087: step 23690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:06m:39s remains)
INFO - root - 2017-12-09 12:22:37.722786: step 23700, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 73h:23m:35s remains)
2017-12-09 12:22:38.621011: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011483059 0.013827742 0.015667904 0.016592234 0.016097896 0.014278083 0.011729741 0.0087155439 0.00593632 0.0037628138 0.0023103994 0.0014211503 0.0008966215 0.00063717365 0.00048614829][0.014498654 0.017112833 0.019655067 0.021395732 0.021679886 0.020223435 0.017615233 0.013816954 0.0097994516 0.0062664757 0.0033747298 0.0016370993 0.00042318506 0.00025519682 0.00041993964][0.021075625 0.024889711 0.028324522 0.031244971 0.032828197 0.03219077 0.029416669 0.024641771 0.018832821 0.012610625 0.007066967 0.0031509059 0.00056616869 -0.00050632237 -0.00054637133][0.028326001 0.033846624 0.038779415 0.042847794 0.045510687 0.046174683 0.043987587 0.039006826 0.031823706 0.023560155 0.015635375 0.0090151522 0.0045328187 0.0022355933 0.0019180891][0.039750207 0.046607014 0.052444823 0.057084762 0.059988044 0.061089277 0.059505485 0.055365842 0.04777208 0.038347065 0.028805878 0.020300299 0.014250875 0.010607077 0.010089751][0.054631613 0.061973251 0.067790866 0.072136119 0.074993908 0.076299004 0.074988618 0.07106775 0.063723147 0.053926002 0.043376751 0.03409211 0.027222306 0.023373388 0.022704653][0.06965591 0.07700897 0.082260877 0.085885786 0.08822941 0.089317568 0.088296279 0.084901243 0.078452557 0.069234811 0.058316063 0.048639975 0.040958103 0.036577068 0.035232753][0.07904654 0.085836381 0.0903013 0.093283445 0.095274813 0.096297376 0.095772721 0.093131207 0.08773537 0.079650611 0.0696831 0.060390353 0.052522857 0.047819439 0.046155844][0.081772462 0.087546326 0.090972684 0.09317717 0.094635785 0.095538862 0.09536159 0.093519792 0.089418948 0.0830645 0.075045779 0.06716869 0.060422026 0.056242757 0.054765373][0.07865303 0.083168909 0.085447714 0.086821817 0.087601662 0.087947212 0.087545976 0.086190671 0.08328189 0.078889467 0.073315606 0.067707457 0.062948592 0.05992341 0.058869343][0.070417464 0.074055322 0.075609133 0.07623218 0.076369286 0.07615111 0.0754738 0.074272722 0.072217986 0.069524229 0.066225395 0.062899165 0.060105897 0.058332685 0.05778214][0.058189072 0.061003059 0.062014267 0.062443532 0.062505051 0.06218458 0.061422333 0.060422555 0.059065517 0.057483327 0.055703752 0.053915396 0.052488517 0.051494885 0.051044293][0.042403858 0.044381961 0.045110654 0.045590948 0.045870017 0.045924179 0.045731436 0.045391787 0.044848759 0.044214949 0.043288678 0.042294577 0.041380573 0.040530279 0.039909348][0.026200969 0.027223025 0.02750767 0.027776439 0.028005056 0.028203327 0.028365776 0.028491613 0.028518755 0.028479487 0.028218452 0.027811501 0.027209975 0.026556466 0.025969654][0.012619446 0.012999111 0.013041197 0.013117637 0.01321965 0.013362772 0.013541021 0.013721923 0.013862143 0.013968094 0.013955367 0.013844914 0.013577058 0.013261954 0.012971534]]...]
INFO - root - 2017-12-09 12:22:47.213828: step 23710, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:57m:19s remains)
INFO - root - 2017-12-09 12:22:55.912006: step 23720, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 74h:44m:44s remains)
INFO - root - 2017-12-09 12:23:04.638556: step 23730, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:23m:02s remains)
INFO - root - 2017-12-09 12:23:13.265040: step 23740, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:41m:48s remains)
INFO - root - 2017-12-09 12:23:21.867862: step 23750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:20m:48s remains)
INFO - root - 2017-12-09 12:23:30.544350: step 23760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:00m:08s remains)
INFO - root - 2017-12-09 12:23:38.995948: step 23770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:32m:39s remains)
INFO - root - 2017-12-09 12:23:47.549805: step 23780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:23m:49s remains)
INFO - root - 2017-12-09 12:23:56.248216: step 23790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:55m:15s remains)
INFO - root - 2017-12-09 12:24:04.884349: step 23800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:55m:37s remains)
2017-12-09 12:24:05.793409: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0064489045 0.0081479177 0.010226304 0.010900401 0.011485212 0.011137944 0.0096829534 0.0072417553 0.0045748805 0.0020032316 -3.8326252e-05 -0.0013581903 -0.0022795436 -0.0029908735 -0.0031497863][0.0094897021 0.010425329 0.01203095 0.012928279 0.013953796 0.014077973 0.012602447 0.0098367333 0.0067247082 0.0034386413 0.00059166551 -0.0010671746 -0.0019871951 -0.0025825873 -0.0028156771][0.023669329 0.023692848 0.025372101 0.027423972 0.029406672 0.029186802 0.026416952 0.021598343 0.015564336 0.009147767 0.0036064708 0.00021399744 -0.0014625095 -0.0023667035 -0.0024663834][0.053202741 0.053896908 0.056256406 0.059383091 0.061997164 0.061395671 0.056424517 0.047070578 0.03516715 0.02266415 0.011721943 0.00400597 -0.000314523 -0.0021716971 -0.002549995][0.095505871 0.09822809 0.10279079 0.10764804 0.11065285 0.10846502 0.099537753 0.083876036 0.063920334 0.043154359 0.024782818 0.011096837 0.0026453482 -0.0013388856 -0.0025904288][0.14237987 0.14779074 0.15476221 0.16165106 0.16560495 0.16194253 0.14884606 0.12614955 0.097069465 0.06707684 0.040347219 0.02022115 0.0069391015 0.00016424246 -0.0022325874][0.18492655 0.19148876 0.19934899 0.20729321 0.21185279 0.207284 0.19173147 0.16451292 0.12881899 0.091394179 0.05742747 0.031355623 0.013028374 0.0028410368 -0.0012526258][0.21068476 0.21737076 0.22441159 0.23191279 0.23619159 0.23141113 0.21559961 0.18755895 0.14996919 0.10948611 0.071900077 0.042272307 0.020187207 0.0066516143 0.00047629047][0.21249545 0.21804929 0.22321354 0.22899303 0.23208168 0.227388 0.21308981 0.18750229 0.15259358 0.11399812 0.077370375 0.04769931 0.024445485 0.0089504123 0.0010913266][0.1904057 0.19424994 0.19657631 0.19944088 0.20051363 0.19580215 0.18385501 0.16275351 0.13376233 0.10132491 0.070012413 0.044240426 0.023379192 0.0087771975 0.00088732038][0.14957541 0.15237549 0.15254685 0.1527293 0.15182392 0.14704056 0.13774632 0.12203085 0.10060428 0.076685108 0.05345526 0.034128956 0.018060414 0.0065351496 0.00019066106][0.10110387 0.10307318 0.10223917 0.10097087 0.0990284 0.094846122 0.088378295 0.077990495 0.064224504 0.049100369 0.034429371 0.022178749 0.011828043 0.0043493072 0.00019128295][0.057675052 0.059076544 0.058076877 0.056459107 0.054492071 0.051445495 0.047569145 0.041780069 0.034311511 0.026310524 0.018582555 0.012525616 0.0074352347 0.0042007626 0.0032927236][0.026845245 0.027852388 0.027193947 0.025909038 0.024474213 0.022701027 0.020875739 0.018448923 0.015527252 0.012709625 0.010241369 0.0087398551 0.007833872 0.008101644 0.0098782927][0.0095478389 0.01018128 0.0098277591 0.0090357009 0.0083248951 0.0078499056 0.0080518331 0.00875394 0.0099129826 0.011611136 0.013661221 0.015925713 0.018646061 0.021812402 0.02626496]]...]
INFO - root - 2017-12-09 12:24:14.255013: step 23810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:31m:48s remains)
INFO - root - 2017-12-09 12:24:22.900917: step 23820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:20m:04s remains)
INFO - root - 2017-12-09 12:24:31.684358: step 23830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:04m:36s remains)
INFO - root - 2017-12-09 12:24:40.387422: step 23840, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:49m:04s remains)
INFO - root - 2017-12-09 12:24:48.586672: step 23850, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:17m:15s remains)
INFO - root - 2017-12-09 12:24:57.244110: step 23860, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 75h:40m:35s remains)
INFO - root - 2017-12-09 12:25:05.709949: step 23870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:02m:04s remains)
INFO - root - 2017-12-09 12:25:14.081573: step 23880, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.752 sec/batch; 64h:27m:07s remains)
INFO - root - 2017-12-09 12:25:22.847416: step 23890, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 73h:32m:09s remains)
INFO - root - 2017-12-09 12:25:31.405170: step 23900, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 71h:28m:45s remains)
2017-12-09 12:25:32.304552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29461575 0.29576692 0.29697663 0.29549041 0.29346621 0.29094622 0.2886503 0.28711718 0.28423694 0.28053704 0.27351919 0.26285058 0.24812439 0.22981133 0.21144316][0.32664654 0.32777935 0.32832447 0.32643268 0.32342392 0.3201797 0.31753162 0.31503808 0.31159565 0.30749536 0.30039728 0.2897591 0.27546617 0.2585099 0.24199758][0.34695503 0.34904367 0.34938613 0.34773314 0.34445673 0.34087536 0.33796489 0.33476859 0.33043692 0.32435372 0.31585279 0.3043434 0.28992793 0.27376541 0.25912613][0.36083376 0.3642453 0.36504918 0.36327827 0.359997 0.35618371 0.35291889 0.34897736 0.34334844 0.33568856 0.325666 0.31283313 0.29778743 0.28211513 0.26854652][0.36994234 0.37492087 0.37559912 0.37441894 0.37192321 0.36796379 0.36458552 0.3599714 0.35333008 0.34373555 0.33192217 0.31786475 0.30215374 0.28623357 0.27271187][0.37528688 0.38093752 0.38044274 0.37915853 0.37688741 0.37328985 0.37015876 0.365207 0.35804337 0.34721607 0.33385962 0.31870514 0.30208063 0.28599638 0.27230254][0.37631235 0.38208637 0.37967896 0.377076 0.37401959 0.37046796 0.36715421 0.36188954 0.35420752 0.343197 0.32931855 0.31360653 0.29661292 0.28076112 0.26714796][0.37284479 0.37786612 0.37282112 0.36797619 0.36325586 0.35908726 0.35512942 0.34994012 0.34238818 0.33192068 0.31842494 0.30309412 0.28652766 0.270917 0.25734198][0.361964 0.36545628 0.35774913 0.35060173 0.34390172 0.33839965 0.33336371 0.32740754 0.31960961 0.30988184 0.29764026 0.28386933 0.26888508 0.25487792 0.24239135][0.33864352 0.34062353 0.33045074 0.32107764 0.31239972 0.30516902 0.29868421 0.29200375 0.28413522 0.27522776 0.26472092 0.25392172 0.24200147 0.23056348 0.22053146][0.30078954 0.30047911 0.28814584 0.27635908 0.26567474 0.25726631 0.24989724 0.24277218 0.23529723 0.22854184 0.22117868 0.21394114 0.20603815 0.19909585 0.19301455][0.25341806 0.25077584 0.23704483 0.22349572 0.21152566 0.20213765 0.1944101 0.18747939 0.1810144 0.17610213 0.17196749 0.16835681 0.1651011 0.16327748 0.16244294][0.20124355 0.19659695 0.18268627 0.16888934 0.15697631 0.14762485 0.14023148 0.13377419 0.12846434 0.12491625 0.12325611 0.12270843 0.1237906 0.12666719 0.13113299][0.14692162 0.14037089 0.12715164 0.11457615 0.10409772 0.095884889 0.089654014 0.083971508 0.079816252 0.077488407 0.077634513 0.079758592 0.08462622 0.092436343 0.10268897][0.096522152 0.089321353 0.077689253 0.066694535 0.057992462 0.051657274 0.047182262 0.043362029 0.040883802 0.040280677 0.041914735 0.045531888 0.052362561 0.062827624 0.076983094]]...]
INFO - root - 2017-12-09 12:25:40.909858: step 23910, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.912 sec/batch; 78h:09m:52s remains)
INFO - root - 2017-12-09 12:25:49.715630: step 23920, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 75h:30m:35s remains)
INFO - root - 2017-12-09 12:25:58.450104: step 23930, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 75h:59m:59s remains)
INFO - root - 2017-12-09 12:26:07.127021: step 23940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:54m:26s remains)
INFO - root - 2017-12-09 12:26:15.650291: step 23950, loss = 0.89, batch loss = 0.68 (11.5 examples/sec; 0.696 sec/batch; 59h:40m:02s remains)
INFO - root - 2017-12-09 12:26:24.264452: step 23960, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 74h:00m:35s remains)
INFO - root - 2017-12-09 12:26:32.840047: step 23970, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 70h:33m:17s remains)
INFO - root - 2017-12-09 12:26:41.301807: step 23980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:01m:08s remains)
INFO - root - 2017-12-09 12:26:49.741414: step 23990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:02m:23s remains)
INFO - root - 2017-12-09 12:26:58.362331: step 24000, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:57m:08s remains)
2017-12-09 12:26:59.249054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013207849 -0.0012989941 -0.0012977989 -0.0012901626 -0.0012616634 -0.0011198637 -0.0010678978 -0.0013574131 -0.0018426634 -0.0024460154 -0.0029305583 -0.0032065655 -0.0033283329 -0.0033713486 -0.0033842865][0.00026028813 8.1505161e-05 -0.00014167209 -0.0002253335 -0.00012980774 0.00010771817 0.00022230274 -0.00017456501 -0.00096723577 -0.0019630282 -0.0027225907 -0.0031380553 -0.0033102145 -0.003365522 -0.003381904][0.0020252077 0.0017803141 0.001499715 0.0013724321 0.0013559198 0.0014624817 0.0014297797 0.00087545463 -0.00012362166 -0.0013776687 -0.0024128896 -0.0030169636 -0.0032732768 -0.0033562358 -0.0033779598][0.0032429106 0.0031188347 0.0030363465 0.0029954447 0.0029844057 0.002987514 0.00269643 0.0018284812 0.000493346 -0.0009822303 -0.0021933024 -0.0029222989 -0.0032387641 -0.0033462523 -0.0033749517][0.0037026915 0.0038472188 0.0039833514 0.0041224984 0.0042543095 0.0042372504 0.0038276732 0.0027580617 0.0011270682 -0.00062056328 -0.0020224839 -0.0028610551 -0.0032210154 -0.0033419088 -0.0033746122][0.0034493583 0.0039340779 0.0043772152 0.0047252281 0.0049599847 0.0049949335 0.0046549207 0.0035263717 0.0017650065 -0.00019650394 -0.0017889959 -0.0027629011 -0.0031886068 -0.0033320738 -0.0033712836][0.0031649286 0.0037854284 0.0043385513 0.0048351958 0.0052200044 0.0053620776 0.0051005278 0.0040476304 0.002375291 0.00035370514 -0.0013961575 -0.0025610058 -0.0031114898 -0.0033081011 -0.0033637823][0.00260411 0.0034256063 0.0040611876 0.0045754872 0.0049878648 0.0052571595 0.0051647378 0.0043012192 0.002823917 0.00083803781 -0.00099613727 -0.002330747 -0.0030158232 -0.0032761951 -0.0033535804][0.0011690443 0.0021826921 0.0030544929 0.0037256263 0.004224903 0.0045493757 0.0045748828 0.0039506089 0.0027567723 0.00096654729 -0.00081528118 -0.0021991385 -0.0029544774 -0.0032547403 -0.0033476136][-0.00027963612 0.00049901963 0.0013857579 0.0021879345 0.0028771164 0.0033949481 0.0035872369 0.0031988702 0.002191324 0.000618601 -0.0010001527 -0.0022762646 -0.0029781032 -0.0032615266 -0.003349877][-0.0012730584 -0.00068354537 3.2453798e-05 0.00069219223 0.0013459793 0.0019406208 0.0023291798 0.0021656237 0.0013375685 -2.311077e-06 -0.001423395 -0.0025027688 -0.0030685002 -0.0032910665 -0.0033576335][-0.0018046997 -0.0013230823 -0.00063235173 9.9722762e-05 0.00079139858 0.0012953344 0.0016162626 0.0014805712 0.00078675686 -0.00039371871 -0.0016780399 -0.002635322 -0.0031219996 -0.0033081525 -0.0033624535][-0.0016800259 -0.0012273849 -0.00061304891 7.0743263e-05 0.0007657099 0.0012735596 0.0015760704 0.0014322863 0.00075535686 -0.00041328138 -0.0016950447 -0.0026427147 -0.0031255051 -0.0033086489 -0.0033628086][-0.0012178447 -0.00076091103 -0.00016793632 0.00047836686 0.0011060832 0.0014717982 0.0016743822 0.0014399721 0.00076444517 -0.00033819955 -0.0015938588 -0.0025699432 -0.0030907402 -0.0032961227 -0.0033584512][-0.00037980056 1.5936093e-05 0.00048756087 0.00091471779 0.0014100668 0.0016625284 0.0017344381 0.0013804955 0.00064317021 -0.00044080149 -0.0016125157 -0.0025441754 -0.0030688355 -0.0032837964 -0.0033529568]]...]
INFO - root - 2017-12-09 12:27:07.697083: step 24010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:39m:57s remains)
INFO - root - 2017-12-09 12:27:16.282521: step 24020, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 72h:49m:14s remains)
INFO - root - 2017-12-09 12:27:24.793240: step 24030, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 72h:03m:26s remains)
INFO - root - 2017-12-09 12:27:33.373519: step 24040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:13m:22s remains)
INFO - root - 2017-12-09 12:27:42.105986: step 24050, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:16m:47s remains)
INFO - root - 2017-12-09 12:27:50.599722: step 24060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:44m:00s remains)
INFO - root - 2017-12-09 12:27:59.239548: step 24070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:33m:37s remains)
INFO - root - 2017-12-09 12:28:08.007006: step 24080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:08m:35s remains)
INFO - root - 2017-12-09 12:28:16.520089: step 24090, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 73h:07m:21s remains)
INFO - root - 2017-12-09 12:28:25.153643: step 24100, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:27m:58s remains)
2017-12-09 12:28:26.040405: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12563099 0.12720992 0.12752482 0.12700765 0.12557656 0.12340487 0.12017629 0.1162622 0.11089399 0.10492505 0.098258272 0.091643848 0.085695758 0.08062803 0.077044696][0.12192328 0.12324465 0.12391187 0.12447972 0.12472143 0.12444962 0.12299124 0.12023607 0.11566909 0.10998737 0.10341759 0.0965224 0.089796141 0.083645113 0.078716435][0.11722575 0.11807657 0.11901712 0.12071556 0.12279808 0.12468784 0.12526479 0.12398355 0.12041619 0.11503326 0.10840866 0.10121903 0.093804926 0.08667516 0.080599472][0.11369719 0.11418151 0.11532976 0.11807727 0.12193707 0.12579368 0.12822191 0.12829217 0.12556286 0.12044001 0.11379037 0.10634188 0.098397575 0.090485014 0.083498515][0.11162335 0.1120209 0.11335336 0.11678545 0.12195731 0.12715738 0.13075499 0.13178316 0.12958601 0.12465254 0.11814252 0.11073489 0.10267923 0.094459921 0.08702635][0.11009595 0.11074558 0.11228624 0.11619335 0.1219376 0.12769829 0.13181221 0.1331955 0.13124724 0.12652855 0.12035009 0.11321352 0.105497 0.097541139 0.0902337][0.10791823 0.1088606 0.11044526 0.11445298 0.12006329 0.12572356 0.12984546 0.13141218 0.12979271 0.12555824 0.12004255 0.11357576 0.10656717 0.099246226 0.0923934][0.1046593 0.10575031 0.10705374 0.11061133 0.11552917 0.12055724 0.12433759 0.12601726 0.12503307 0.12158272 0.11695544 0.11141914 0.10543057 0.098990694 0.092831209][0.099991143 0.10099591 0.10171536 0.10447997 0.10835484 0.11250345 0.11566319 0.1173906 0.11719539 0.11478688 0.11135133 0.10696088 0.10215441 0.0967398 0.091364063][0.094008714 0.094975084 0.095134385 0.096903734 0.099691093 0.1031012 0.10583374 0.10755242 0.10790196 0.10637609 0.10394908 0.10069477 0.097061723 0.0928026 0.088351384][0.086669676 0.087841317 0.087829806 0.088880308 0.090730689 0.093367085 0.095702544 0.097324692 0.098177478 0.097447425 0.095909305 0.093645521 0.091015689 0.087834068 0.084353685][0.078734286 0.080224231 0.080281511 0.080960087 0.082208909 0.084243745 0.086342953 0.087897174 0.089021377 0.089114718 0.088549256 0.087283231 0.085634962 0.08332926 0.080670677][0.0707729 0.072839722 0.073363625 0.074112751 0.075268567 0.076873578 0.078698769 0.080056466 0.081170075 0.081716739 0.081832618 0.08136142 0.08044228 0.078857645 0.076887719][0.06219621 0.064920008 0.066176735 0.067393161 0.068773277 0.0702858 0.07199344 0.073426314 0.074740335 0.075634494 0.076279432 0.076306567 0.075802319 0.07466124 0.073218085][0.053760011 0.056804888 0.058566872 0.060254239 0.061938249 0.063684419 0.065545335 0.067291111 0.0688975 0.070064 0.071069472 0.071407244 0.071175382 0.070364356 0.069402121]]...]
INFO - root - 2017-12-09 12:28:34.430030: step 24110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:18m:44s remains)
INFO - root - 2017-12-09 12:28:43.070062: step 24120, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 73h:47m:11s remains)
INFO - root - 2017-12-09 12:28:51.719952: step 24130, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.847 sec/batch; 72h:35m:31s remains)
INFO - root - 2017-12-09 12:29:00.416558: step 24140, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 76h:38m:04s remains)
INFO - root - 2017-12-09 12:29:08.998469: step 24150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:04m:47s remains)
INFO - root - 2017-12-09 12:29:17.498195: step 24160, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:04m:19s remains)
INFO - root - 2017-12-09 12:29:26.127449: step 24170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:46m:10s remains)
INFO - root - 2017-12-09 12:29:34.756392: step 24180, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 71h:23m:19s remains)
INFO - root - 2017-12-09 12:29:43.281929: step 24190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:02m:55s remains)
INFO - root - 2017-12-09 12:29:51.920962: step 24200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:11m:19s remains)
2017-12-09 12:29:52.799028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033843119 -0.0033835822 -0.0033830111 -0.003381564 -0.0033797296 -0.0033776236 -0.003375801 -0.0033739754 -0.0033732348 -0.0033742054 -0.00337678 -0.0033794167 -0.0033812628 -0.003382314 -0.0033821375][-0.0033852928 -0.0033837983 -0.0033826833 -0.0033803433 -0.0033771452 -0.0033736455 -0.0033710371 -0.0033686324 -0.003367858 -0.0033700636 -0.0033740075 -0.003378544 -0.003381928 -0.0033836444 -0.0033836653][-0.0033868228 -0.0033840775 -0.0033817196 -0.0033776183 -0.0033722366 -0.0033664119 -0.0033618156 -0.0033577755 -0.003357033 -0.0033607346 -0.0033677686 -0.0033757654 -0.0033816891 -0.0033848267 -0.0033850966][-0.0033883199 -0.0033844698 -0.003380388 -0.0033733414 -0.0033640985 -0.0033536991 -0.0033449153 -0.003338513 -0.0033384508 -0.0033443198 -0.0033542812 -0.0033667497 -0.0033777079 -0.0033840949 -0.0033851347][-0.003389769 -0.0033850784 -0.0033791128 -0.00336888 -0.0033536283 -0.0033355812 -0.0033202041 -0.0033099409 -0.0033085772 -0.0033154069 -0.0033303795 -0.003350124 -0.0033683705 -0.0033791417 -0.0033824618][-0.0033904032 -0.0033860938 -0.0033788481 -0.0033652449 -0.0033441707 -0.0033188157 -0.0032967704 -0.00328135 -0.0032765644 -0.0032825531 -0.0033019867 -0.0033293497 -0.0033556747 -0.0033725209 -0.0033789231][-0.0033896244 -0.003386626 -0.0033792248 -0.0033639744 -0.0033398131 -0.0033103237 -0.0032844574 -0.0032644591 -0.0032546171 -0.00325807 -0.0032793207 -0.0033121593 -0.0033457419 -0.0033676494 -0.0033765687][-0.0033883939 -0.0033865392 -0.0033806716 -0.00336684 -0.0033434283 -0.0033143605 -0.0032874679 -0.0032647604 -0.0032514343 -0.0032509943 -0.0032704023 -0.0033042694 -0.0033397474 -0.0033635048 -0.0033736106][-0.0033873038 -0.0033862437 -0.0033822181 -0.0033719111 -0.0033535762 -0.0033300754 -0.003306411 -0.0032839542 -0.0032688396 -0.003265376 -0.0032807044 -0.0033099181 -0.0033416199 -0.0033628677 -0.0033719549][-0.0033857287 -0.0033852714 -0.0033829189 -0.0033766537 -0.0033650962 -0.0033493077 -0.0033327187 -0.0033148113 -0.0033012291 -0.0032958314 -0.00330545 -0.0033259033 -0.0033483943 -0.0033637809 -0.0033700671][-0.0033840616 -0.0033840141 -0.0033830863 -0.0033798248 -0.003374184 -0.0033659735 -0.0033566034 -0.0033453649 -0.0033360377 -0.0033313774 -0.0033364417 -0.0033473296 -0.0033588877 -0.0033665653 -0.0033695549][-0.0033820616 -0.0033821089 -0.0033818302 -0.003380798 -0.0033790842 -0.0033757815 -0.0033716352 -0.0033664538 -0.0033613381 -0.003358434 -0.0033598298 -0.0033636643 -0.003367245 -0.0033694082 -0.0033700231][-0.0033802167 -0.0033796511 -0.0033795175 -0.0033794595 -0.0033795051 -0.0033788874 -0.0033775344 -0.0033757295 -0.0033736094 -0.0033721328 -0.0033712781 -0.0033709994 -0.0033707651 -0.0033706052 -0.0033704718][-0.0033783983 -0.0033772511 -0.0033767773 -0.0033766748 -0.003377314 -0.0033775952 -0.0033771177 -0.0033766539 -0.0033763205 -0.0033756401 -0.0033746047 -0.0033735537 -0.0033724203 -0.0033715407 -0.0033709677][-0.0033767177 -0.0033757307 -0.003374954 -0.0033744592 -0.003374713 -0.0033748474 -0.0033744641 -0.0033744015 -0.0033743854 -0.0033740718 -0.0033736443 -0.0033730411 -0.0033722771 -0.0033716585 -0.0033711833]]...]
INFO - root - 2017-12-09 12:30:01.309177: step 24210, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:45m:28s remains)
INFO - root - 2017-12-09 12:30:10.065847: step 24220, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 70h:36m:53s remains)
INFO - root - 2017-12-09 12:30:18.750646: step 24230, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 76h:19m:16s remains)
INFO - root - 2017-12-09 12:30:27.306719: step 24240, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:15m:09s remains)
INFO - root - 2017-12-09 12:30:35.934471: step 24250, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 69h:57m:15s remains)
INFO - root - 2017-12-09 12:30:44.482814: step 24260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:48m:01s remains)
INFO - root - 2017-12-09 12:30:52.956059: step 24270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:50m:03s remains)
INFO - root - 2017-12-09 12:31:01.714845: step 24280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 77h:03m:07s remains)
INFO - root - 2017-12-09 12:31:10.432364: step 24290, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 76h:48m:42s remains)
INFO - root - 2017-12-09 12:31:19.108241: step 24300, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 75h:28m:38s remains)
2017-12-09 12:31:20.014708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033370091 -0.0033512828 -0.0033367714 -0.0032251158 -0.0029669208 -0.0025053415 -0.0018443331 -0.0010623625 -0.00026196032 0.0005065552 0.001241758 0.0020066123 0.0027891295 0.0036679539 0.0045154318][-0.0032684694 -0.0033151256 -0.0033452671 -0.0032819319 -0.0030994928 -0.0027159061 -0.0020768919 -0.0011721456 -6.783125e-05 0.0010760459 0.002119 0.0030790707 0.0040174411 0.0050163083 0.0059138415][-0.0032059052 -0.0032757847 -0.0033367041 -0.0033171822 -0.0032222527 -0.0029557012 -0.0024302695 -0.0015723921 -0.00041844603 0.00088085281 0.0021605776 0.0033598137 0.0044870926 0.0055811834 0.0065358812][-0.0031931372 -0.0032565657 -0.0033090406 -0.0033020324 -0.0032426328 -0.0030534416 -0.0026560489 -0.0019695982 -0.00098302471 0.00022916752 0.0015769731 0.0029745391 0.0043330197 0.0055759056 0.0066434108][-0.0032239247 -0.0032555428 -0.0032622605 -0.0032172143 -0.0031025759 -0.0028758689 -0.0025009762 -0.0019407993 -0.0011764162 -0.00017935759 0.0010767798 0.00254974 0.0040782248 0.0055061383 0.0067411503][-0.0032300411 -0.0032356863 -0.0031846655 -0.0030441242 -0.0027748407 -0.0023618774 -0.0018288486 -0.0012217527 -0.00054309773 0.00026432169 0.0013150261 0.0026481841 0.0041583148 0.0056487033 0.0069690654][-0.0031618725 -0.0031546704 -0.003053569 -0.0027737645 -0.0022572037 -0.0015219634 -0.00068330579 0.0001090765 0.00081345346 0.0014753281 0.00229033 0.0033289478 0.0045373803 0.0057706395 0.0069041783][-0.0030606415 -0.0030388576 -0.002884811 -0.0024585424 -0.0016869771 -0.00062164874 0.00053259311 0.0015267348 0.0022638389 0.0027535628 0.0032481262 0.0038464488 0.0045660911 0.00530342 0.006013426][-0.0029769188 -0.0029265939 -0.002738121 -0.0022360426 -0.0013212499 -7.0241978e-05 0.0012506293 0.0023358632 0.003037476 0.003330614 0.0034592962 0.0035622916 0.0037541469 0.00399156 0.0042675859][-0.0029709507 -0.0028905193 -0.00269187 -0.0022024778 -0.0013161784 -9.8633347e-05 0.001190572 0.0022298968 0.0028163153 0.0029270793 0.0027688639 0.0025035846 0.0023046941 0.002173794 0.0021367283][-0.0030552205 -0.0029539901 -0.0027683026 -0.0023639251 -0.0016314087 -0.00060103787 0.00049919728 0.0013695736 0.001798375 0.001775627 0.0014709011 0.0010479996 0.00067372434 0.00036925497 0.00016611232][-0.0031884862 -0.0030961973 -0.002941973 -0.0026381689 -0.0021019247 -0.0013416011 -0.0005267947 0.00010426575 0.00036861026 0.00027032662 -4.5423629e-05 -0.00044602528 -0.0008174649 -0.0011338657 -0.0013657401][-0.0033063248 -0.0032495705 -0.003145877 -0.0029457815 -0.0025981786 -0.0021104156 -0.0016030603 -0.0012345626 -0.0011183538 -0.0012343621 -0.0014792969 -0.0017722718 -0.0020459252 -0.0022775419 -0.0024461546][-0.0033646321 -0.0033469219 -0.00330074 -0.0031987398 -0.0030162227 -0.0027620888 -0.0025091362 -0.0023458865 -0.0023244289 -0.0024187749 -0.0025655557 -0.0027273409 -0.0028734086 -0.0029925727 -0.0030722856][-0.0033775344 -0.003376086 -0.00336633 -0.0033343504 -0.0032682209 -0.0031740116 -0.0030845094 -0.0030363989 -0.0030446821 -0.0030928792 -0.0031537276 -0.0032137039 -0.0032638602 -0.0033014107 -0.003321924]]...]
INFO - root - 2017-12-09 12:31:28.605441: step 24310, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 75h:27m:08s remains)
INFO - root - 2017-12-09 12:31:37.336913: step 24320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:42m:08s remains)
INFO - root - 2017-12-09 12:31:46.070443: step 24330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:57m:45s remains)
INFO - root - 2017-12-09 12:31:54.761271: step 24340, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 76h:40m:13s remains)
INFO - root - 2017-12-09 12:32:03.466879: step 24350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:15m:58s remains)
INFO - root - 2017-12-09 12:32:11.880148: step 24360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:34m:26s remains)
INFO - root - 2017-12-09 12:32:20.649531: step 24370, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 74h:13m:46s remains)
INFO - root - 2017-12-09 12:32:29.271257: step 24380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:10m:14s remains)
INFO - root - 2017-12-09 12:32:37.585651: step 24390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:03m:02s remains)
INFO - root - 2017-12-09 12:32:46.052138: step 24400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:07m:01s remains)
2017-12-09 12:32:46.927728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003379382 -0.0033773978 -0.0033767782 -0.0033761249 -0.0033755868 -0.0033750578 -0.0033750162 -0.0033749104 -0.0033749442 -0.003375561 -0.003376811 -0.0033792912 -0.0033815964 -0.0033830123 -0.0033834414][-0.003377036 -0.0033746066 -0.0033744962 -0.0033747044 -0.0033746408 -0.0033746057 -0.0033744439 -0.0033742355 -0.0033739565 -0.0033742823 -0.0033755586 -0.00337833 -0.0033809305 -0.0033825182 -0.0033829482][-0.0033750641 -0.0033727149 -0.0033727777 -0.0033738245 -0.0033744478 -0.0033751079 -0.0033754457 -0.0033749596 -0.00337472 -0.0033752075 -0.0033765282 -0.0033793992 -0.0033818611 -0.0033835166 -0.0033837126][-0.0033717419 -0.0033689281 -0.0033689879 -0.0033702678 -0.0033716168 -0.0033735256 -0.0033750907 -0.0033754222 -0.0033755638 -0.003376191 -0.0033772227 -0.0033800285 -0.0033825377 -0.0033836132 -0.0033833578][-0.0033672978 -0.003362935 -0.0033622142 -0.003363665 -0.00336576 -0.0033687945 -0.0033719938 -0.0033740131 -0.0033754746 -0.0033766138 -0.0033776115 -0.0033801321 -0.0033821764 -0.0033827866 -0.0033818518][-0.0033623788 -0.0033571452 -0.0033556125 -0.0033565578 -0.0033593341 -0.0033637243 -0.0033682976 -0.003371448 -0.0033740357 -0.0033761226 -0.0033777254 -0.0033800474 -0.0033816192 -0.0033813803 -0.003380097][-0.0033589383 -0.0033532383 -0.0033515827 -0.0033521224 -0.0033549618 -0.0033596868 -0.0033651036 -0.0033689381 -0.0033722008 -0.0033748541 -0.0033767663 -0.0033792062 -0.0033807349 -0.0033803971 -0.0033789454][-0.0033582472 -0.0033532039 -0.0033520164 -0.0033531943 -0.0033564395 -0.0033613178 -0.0033667602 -0.0033706049 -0.0033736438 -0.0033756604 -0.00337731 -0.0033794383 -0.0033809978 -0.0033806919 -0.0033791866][-0.0033607983 -0.0033560314 -0.0033556004 -0.0033575238 -0.0033612542 -0.0033665742 -0.0033719686 -0.003375832 -0.0033775417 -0.0033780441 -0.0033786064 -0.0033801629 -0.0033815079 -0.0033812621 -0.0033799775][-0.0033654962 -0.0033613178 -0.0033611241 -0.0033629246 -0.0033664096 -0.0033714594 -0.003376019 -0.0033790474 -0.0033799484 -0.0033798709 -0.0033799713 -0.0033811694 -0.0033826174 -0.0033823652 -0.0033818278][-0.0033720438 -0.0033689423 -0.003368628 -0.0033694177 -0.0033720951 -0.003375815 -0.0033789261 -0.0033803901 -0.0033805459 -0.0033803938 -0.0033806495 -0.0033817913 -0.0033836237 -0.0033839357 -0.0033839468][-0.0033783584 -0.003375798 -0.0033749866 -0.003375171 -0.0033764613 -0.0033783207 -0.0033794083 -0.0033793168 -0.0033787137 -0.0033784551 -0.0033789799 -0.0033806837 -0.0033831873 -0.0033844842 -0.0033850379][-0.0033827284 -0.0033801857 -0.0033783827 -0.0033770595 -0.0033768392 -0.00337724 -0.0033772152 -0.0033763684 -0.0033756527 -0.0033755631 -0.0033762844 -0.0033782404 -0.0033813822 -0.0033837177 -0.0033849692][-0.0033855815 -0.0033825415 -0.003379835 -0.0033771608 -0.0033751384 -0.0033739049 -0.0033730897 -0.0033722892 -0.0033717058 -0.0033717013 -0.003372692 -0.0033749337 -0.0033785151 -0.0033816579 -0.0033838926][-0.0033857641 -0.0033827678 -0.0033795198 -0.0033763377 -0.0033736946 -0.0033717446 -0.0033702927 -0.0033693092 -0.0033689139 -0.0033691053 -0.0033701141 -0.0033720585 -0.0033753738 -0.003378866 -0.0033816781]]...]
INFO - root - 2017-12-09 12:32:55.298578: step 24410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:22m:58s remains)
INFO - root - 2017-12-09 12:33:04.058604: step 24420, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:25m:37s remains)
INFO - root - 2017-12-09 12:33:12.744823: step 24430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:52m:16s remains)
INFO - root - 2017-12-09 12:33:21.413651: step 24440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:29m:49s remains)
INFO - root - 2017-12-09 12:33:30.093665: step 24450, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 73h:55m:06s remains)
INFO - root - 2017-12-09 12:33:38.505924: step 24460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:28m:04s remains)
INFO - root - 2017-12-09 12:33:47.101575: step 24470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:21m:38s remains)
INFO - root - 2017-12-09 12:33:55.767611: step 24480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:29m:43s remains)
INFO - root - 2017-12-09 12:34:04.189066: step 24490, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:58m:09s remains)
INFO - root - 2017-12-09 12:34:12.740548: step 24500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:12m:25s remains)
2017-12-09 12:34:13.696376: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0080216033 0.0080009811 0.0078170327 0.0075992262 0.0072697261 0.0071992474 0.0073691318 0.0077219214 0.0083416225 0.0099040633 0.012969774 0.01760561 0.023247769 0.028982796 0.035154819][0.0083343927 0.0081654154 0.0075311568 0.0067745307 0.0057871286 0.0051343581 0.0045441538 0.0046446044 0.0048805932 0.0061135469 0.0083863605 0.012304639 0.01763778 0.023720093 0.030489534][0.011313322 0.011358152 0.010235385 0.00871891 0.0068047559 0.0053649638 0.0039499803 0.0038143976 0.0037094259 0.0045548296 0.0057847854 0.0086671636 0.013100951 0.018920155 0.025954731][0.016915835 0.017052636 0.015305095 0.01321257 0.010668166 0.0083382064 0.0057190983 0.0045828596 0.0037059055 0.0041173454 0.0041769296 0.0055172639 0.0084442832 0.013853575 0.020770589][0.023327239 0.023961477 0.021640209 0.018848442 0.01579815 0.012874679 0.0093638375 0.0070612337 0.0050738091 0.00457698 0.0036327674 0.0037729719 0.0050785989 0.0095665483 0.015651332][0.027739495 0.028938092 0.026684333 0.023832412 0.020737372 0.017397527 0.013524284 0.010189121 0.0070713228 0.0053689294 0.0036629622 0.002860917 0.0031494142 0.0062603382 0.011226936][0.028791446 0.030535486 0.028640885 0.026160192 0.023702191 0.020596441 0.016774148 0.012909121 0.0093453182 0.0066385837 0.0040296335 0.0023259062 0.0018403737 0.0037769543 0.0079350118][0.027217697 0.029264841 0.027925871 0.02615211 0.024468083 0.02204182 0.019004334 0.015375431 0.011816869 0.0085438462 0.0054520005 0.0028759541 0.0015055567 0.0021756387 0.0056229196][0.023720307 0.025834797 0.024987064 0.023824153 0.022760641 0.020946316 0.018644478 0.015709279 0.012732311 0.0095054051 0.00637375 0.0036113353 0.0022679993 0.0025196567 0.0055206036][0.01863849 0.020568246 0.020074924 0.019362101 0.018721802 0.01745737 0.015842458 0.013723462 0.011538863 0.0089515885 0.0062563121 0.0037243597 0.0025148576 0.0026458765 0.0053506568][0.01252788 0.014078838 0.013823532 0.013391535 0.012925318 0.012067161 0.01097799 0.0097229229 0.0083546275 0.0066536218 0.0046486361 0.00270802 0.0018544958 0.0021652274 0.0045520719][0.0065845735 0.0076753795 0.00755936 0.0072600003 0.006925798 0.0063653374 0.0056664059 0.0049869614 0.0042416332 0.0033549403 0.002090926 0.0008576198 0.00033413083 0.0007153817 0.0025468662][0.0019272619 0.0025725143 0.0025691532 0.0023936399 0.0021696289 0.0018406224 0.0014553743 0.0011201862 0.00075381831 0.00037321961 -0.00027779536 -0.00092188758 -0.0011819161 -0.00088151754 0.00025935285][-0.00094058341 -0.00064035342 -0.00062024221 -0.00071376818 -0.00083653932 -0.0010164576 -0.0012292468 -0.0013778573 -0.0014989509 -0.0015894474 -0.0018714146 -0.0021343012 -0.002253843 -0.0020706072 -0.0014658481][-0.0024579114 -0.0023376332 -0.0023204973 -0.0023746365 -0.0024293324 -0.0025090077 -0.0025991932 -0.0026557175 -0.0026951861 -0.0027086393 -0.0027801208 -0.002848071 -0.0028553952 -0.002767832 -0.0025077169]]...]
INFO - root - 2017-12-09 12:34:22.379491: step 24510, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 73h:57m:09s remains)
INFO - root - 2017-12-09 12:34:30.887232: step 24520, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 72h:35m:08s remains)
INFO - root - 2017-12-09 12:34:39.294855: step 24530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:06m:43s remains)
INFO - root - 2017-12-09 12:34:47.891421: step 24540, loss = 0.88, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 72h:36m:45s remains)
INFO - root - 2017-12-09 12:34:56.629619: step 24550, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:09m:33s remains)
INFO - root - 2017-12-09 12:35:05.138549: step 24560, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:25m:19s remains)
INFO - root - 2017-12-09 12:35:13.795454: step 24570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:21m:20s remains)
INFO - root - 2017-12-09 12:35:22.441925: step 24580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:32m:18s remains)
INFO - root - 2017-12-09 12:35:30.774588: step 24590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 75h:09m:45s remains)
INFO - root - 2017-12-09 12:35:39.273389: step 24600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:15m:52s remains)
2017-12-09 12:35:40.123447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033478308 -0.0033453978 -0.0033449307 -0.0033447184 -0.0033446029 -0.0033445833 -0.0033446802 -0.0033449221 -0.0033452346 -0.0033453964 -0.0033454818 -0.0033455226 -0.0033454853 -0.0033454848 -0.003345462][-0.0033460816 -0.0033433125 -0.0033426282 -0.0033422401 -0.0033419931 -0.0033418706 -0.003341978 -0.0033423505 -0.0033428615 -0.0033432974 -0.0033436145 -0.0033438341 -0.0033439249 -0.0033439444 -0.0033439086][-0.0033460821 -0.0033430504 -0.0033421044 -0.0033414597 -0.0033409318 -0.0033406164 -0.0033407253 -0.0033412583 -0.0033420075 -0.0033426506 -0.0033431819 -0.003343584 -0.0033438008 -0.0033438795 -0.0033438609][-0.0033459957 -0.0033427712 -0.003341571 -0.0033406452 -0.0033398136 -0.0033393602 -0.003339451 -0.0033400436 -0.0033409221 -0.0033417817 -0.0033425468 -0.0033431337 -0.0033434432 -0.0033436 -0.0033436636][-0.0033458679 -0.0033424655 -0.003341123 -0.0033400424 -0.0033390252 -0.0033385246 -0.0033385684 -0.0033390452 -0.0033398652 -0.0033407647 -0.0033416902 -0.0033424338 -0.0033428762 -0.003343204 -0.0033433535][-0.0033457715 -0.0033422455 -0.0033408348 -0.0033396762 -0.0033385975 -0.0033381027 -0.0033380415 -0.0033383328 -0.0033390126 -0.0033398352 -0.0033406867 -0.0033414259 -0.003342005 -0.0033424397 -0.0033427146][-0.0033456197 -0.0033421174 -0.0033407891 -0.0033396406 -0.0033385484 -0.0033380259 -0.0033377935 -0.0033379165 -0.0033386126 -0.0033395011 -0.0033403321 -0.0033410289 -0.0033416077 -0.003341936 -0.003342153][-0.0033455715 -0.0033422925 -0.0033411717 -0.0033401072 -0.003339004 -0.0033383621 -0.0033378743 -0.003337783 -0.0033384438 -0.0033393928 -0.0033402657 -0.0033409507 -0.003341513 -0.0033418173 -0.0033420068][-0.0033454476 -0.003342557 -0.0033417465 -0.0033409167 -0.0033399265 -0.0033391793 -0.0033384981 -0.0033380988 -0.0033384936 -0.0033392888 -0.0033400371 -0.0033407616 -0.003341337 -0.0033416068 -0.003341767][-0.0033453708 -0.0033427561 -0.0033422061 -0.0033415586 -0.003340771 -0.0033400885 -0.0033393535 -0.003338797 -0.0033390084 -0.0033395493 -0.0033400182 -0.0033406164 -0.0033411682 -0.003341398 -0.0033414804][-0.0033453233 -0.0033428161 -0.0033424783 -0.0033418944 -0.00334121 -0.0033405439 -0.0033397805 -0.0033392245 -0.0033393558 -0.0033397279 -0.0033399581 -0.0033403304 -0.0033407691 -0.0033409426 -0.0033410673][-0.003345327 -0.0033429204 -0.0033427065 -0.0033421612 -0.0033415265 -0.0033408604 -0.0033401344 -0.0033396154 -0.0033397179 -0.0033400755 -0.0033403661 -0.0033408049 -0.0033412536 -0.0033414371 -0.0033415118][-0.003345541 -0.0033430627 -0.0033429791 -0.0033425502 -0.003342039 -0.0033414641 -0.0033408597 -0.0033404308 -0.0033404375 -0.0033406033 -0.0033407502 -0.0033411204 -0.0033414634 -0.0033415756 -0.0033415891][-0.0033457747 -0.0033431717 -0.0033431712 -0.0033428557 -0.0033424834 -0.0033420462 -0.0033416038 -0.0033412515 -0.0033411316 -0.0033410813 -0.0033410736 -0.0033412764 -0.003341493 -0.0033415628 -0.0033416233][-0.00334603 -0.0033432513 -0.0033432764 -0.0033430546 -0.0033428441 -0.0033425905 -0.0033423041 -0.0033420643 -0.0033419523 -0.0033418965 -0.0033418557 -0.0033419193 -0.0033420811 -0.0033422287 -0.0033423256]]...]
INFO - root - 2017-12-09 12:35:48.659383: step 24610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:45m:23s remains)
INFO - root - 2017-12-09 12:35:57.464965: step 24620, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.884 sec/batch; 75h:34m:36s remains)
INFO - root - 2017-12-09 12:36:06.038167: step 24630, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 74h:15m:46s remains)
INFO - root - 2017-12-09 12:36:14.765590: step 24640, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 70h:27m:31s remains)
INFO - root - 2017-12-09 12:36:23.568325: step 24650, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.969 sec/batch; 82h:52m:28s remains)
INFO - root - 2017-12-09 12:36:32.176828: step 24660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:18m:20s remains)
INFO - root - 2017-12-09 12:36:40.867900: step 24670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 73h:38m:46s remains)
INFO - root - 2017-12-09 12:36:49.441379: step 24680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:38m:04s remains)
INFO - root - 2017-12-09 12:36:57.818234: step 24690, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 57h:50m:26s remains)
INFO - root - 2017-12-09 12:37:06.491140: step 24700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:43m:37s remains)
2017-12-09 12:37:07.378147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033760751 -0.0033744148 -0.00337429 -0.003374204 -0.0033741407 -0.0033741612 -0.003374194 -0.0033742187 -0.0033741982 -0.0033742171 -0.00337424 -0.0033742508 -0.0033742508 -0.0033742459 -0.0033742562][-0.0033721637 -0.0033700019 -0.0033698692 -0.003370024 -0.00337016 -0.0033703169 -0.0033703977 -0.0033703945 -0.003370265 -0.0033700801 -0.0033699563 -0.0033698718 -0.0033697758 -0.0033697947 -0.0033698732][-0.0033719812 -0.0033699241 -0.0033699479 -0.0033701397 -0.0033702629 -0.0033704021 -0.0033705046 -0.0033704713 -0.0033702436 -0.0033700378 -0.0033698527 -0.00336967 -0.0033694871 -0.0033693621 -0.0033693456][-0.0033717379 -0.0033698459 -0.0033701032 -0.003370506 -0.0033709111 -0.0033712436 -0.0033714722 -0.0033714774 -0.003371191 -0.0033708476 -0.0033704566 -0.0033701074 -0.0033697074 -0.0033694406 -0.0033693206][-0.0033722015 -0.0033704985 -0.0033710785 -0.0033716783 -0.0033722688 -0.0033727388 -0.0033729351 -0.0033726806 -0.0033721873 -0.0033717223 -0.0033711558 -0.0033705716 -0.0033700436 -0.0033696713 -0.0033694424][-0.0033730818 -0.003371675 -0.0033725656 -0.0033733882 -0.0033741952 -0.0033747356 -0.0033749558 -0.0033747989 -0.003374429 -0.0033738879 -0.0033730161 -0.0033718424 -0.0033707975 -0.0033701234 -0.0033697169][-0.0033737584 -0.0033727724 -0.0033738583 -0.0033746103 -0.0033751058 -0.0033753682 -0.0033754229 -0.0033751589 -0.0033749247 -0.0033744541 -0.0033736448 -0.0033725959 -0.0033715868 -0.00337075 -0.0033701023][-0.0033740948 -0.0033733759 -0.0033744215 -0.0033747547 -0.0033746711 -0.0033742113 -0.0033737996 -0.0033738506 -0.0033742073 -0.003374015 -0.0033736092 -0.0033730632 -0.0033722848 -0.0033713195 -0.0033704643][-0.0033743205 -0.0033736199 -0.0033748068 -0.0033753894 -0.0033752611 -0.0033748229 -0.00337449 -0.0033745721 -0.0033752534 -0.0033755482 -0.003375202 -0.0033742783 -0.0033730289 -0.003371767 -0.0033706764][-0.0033746061 -0.0033741116 -0.003375778 -0.0033771235 -0.0033779233 -0.0033786681 -0.0033790253 -0.0033788944 -0.0033788423 -0.0033782332 -0.0033767929 -0.0033748909 -0.0033732897 -0.0033716136 -0.0033700834][-0.0033747195 -0.003374489 -0.0033766106 -0.0033786942 -0.003380426 -0.0033818306 -0.00338289 -0.0033830046 -0.0033826374 -0.0033812427 -0.0033790981 -0.0033764825 -0.0033741083 -0.0033720429 -0.0033703297][-0.0033745633 -0.0033744026 -0.003376611 -0.0033789154 -0.0033811154 -0.0033830283 -0.0033845007 -0.0033849981 -0.0033845047 -0.0033828465 -0.0033802767 -0.0033773705 -0.0033748287 -0.0033727968 -0.0033710639][-0.0033738213 -0.0033731903 -0.0033750273 -0.0033772681 -0.0033794644 -0.0033813955 -0.0033827252 -0.0033831215 -0.0033825685 -0.0033810677 -0.003378903 -0.0033765531 -0.0033744075 -0.0033725249 -0.0033709612][-0.0033730075 -0.0033718406 -0.0033731384 -0.003374604 -0.0033761812 -0.003377615 -0.0033785619 -0.0033788602 -0.0033784809 -0.0033774995 -0.0033759868 -0.0033742909 -0.0033726615 -0.0033712848 -0.0033702864][-0.00337245 -0.0033707151 -0.0033712194 -0.0033717754 -0.0033724571 -0.003373049 -0.0033734576 -0.0033735966 -0.0033735025 -0.003373055 -0.0033722818 -0.0033713931 -0.0033705204 -0.0033698403 -0.0033693328]]...]
INFO - root - 2017-12-09 12:37:15.944277: step 24710, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 76h:53m:03s remains)
INFO - root - 2017-12-09 12:37:24.651312: step 24720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:44m:55s remains)
INFO - root - 2017-12-09 12:37:33.434551: step 24730, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 74h:21m:06s remains)
INFO - root - 2017-12-09 12:37:42.180401: step 24740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:33m:18s remains)
INFO - root - 2017-12-09 12:37:50.919271: step 24750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:52m:33s remains)
INFO - root - 2017-12-09 12:37:59.467201: step 24760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:43m:22s remains)
INFO - root - 2017-12-09 12:38:08.157862: step 24770, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 72h:46m:58s remains)
INFO - root - 2017-12-09 12:38:16.658756: step 24780, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 73h:49m:06s remains)
INFO - root - 2017-12-09 12:38:25.342870: step 24790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:53m:37s remains)
INFO - root - 2017-12-09 12:38:34.085589: step 24800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:53m:09s remains)
2017-12-09 12:38:34.931856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033755687 -0.0033743959 -0.0033743666 -0.0033743703 -0.0033743668 -0.0033743698 -0.0033741884 -0.0033737272 -0.0033731582 -0.0033724669 -0.0033717898 -0.0033711321 -0.0033707896 -0.0033709018 -0.0033711188][-0.0033741856 -0.0033729498 -0.0033730455 -0.0033730825 -0.0033730876 -0.0033730618 -0.0033728941 -0.0033724357 -0.0033717782 -0.00337102 -0.0033703882 -0.0033696888 -0.0033694014 -0.0033693179 -0.0033693663][-0.0033740469 -0.0033726995 -0.0033727745 -0.0033728585 -0.0033729782 -0.0033731239 -0.0033731633 -0.0033728294 -0.0033722341 -0.00337148 -0.0033707647 -0.0033696021 -0.0033690664 -0.0033684145 -0.0033684545][-0.0033734078 -0.0033719335 -0.0033719004 -0.0033720571 -0.0033722948 -0.0033725956 -0.0033730299 -0.0033729638 -0.0033726026 -0.0033721086 -0.0033714888 -0.0033701463 -0.0033686678 -0.0033673896 -0.0033677379][-0.0033731703 -0.0033711898 -0.0033710925 -0.0033712578 -0.0033717512 -0.0033724722 -0.0033735139 -0.003373852 -0.003374039 -0.0033736872 -0.0033727537 -0.0033713717 -0.0033689092 -0.0033671092 -0.0033670098][-0.003371313 -0.0033688848 -0.003369781 -0.0033706257 -0.0033716159 -0.0033731365 -0.0033751833 -0.0033765202 -0.0033774534 -0.0033770811 -0.0033755349 -0.0033728639 -0.0033683958 -0.0033651497 -0.003365865][-0.0033626265 -0.0033594768 -0.003364308 -0.0033692138 -0.0033721656 -0.0033745603 -0.0033774073 -0.0033793605 -0.0033805198 -0.0033797408 -0.003377659 -0.0033729677 -0.0033651714 -0.0033584759 -0.0033578968][-0.003342371 -0.0033383782 -0.0033516425 -0.0033651702 -0.0033736525 -0.0033767691 -0.0033800737 -0.0033825124 -0.0033838954 -0.0033826684 -0.0033800607 -0.0033723586 -0.0033609809 -0.00334943 -0.0033438026][-0.0033165286 -0.0033116369 -0.0033340221 -0.0033590794 -0.0033760888 -0.003380191 -0.0033838942 -0.003386551 -0.0033877471 -0.0033861033 -0.0033827673 -0.0033713789 -0.0033555105 -0.0033386638 -0.0033282889][-0.0032982023 -0.0032931338 -0.0033214148 -0.003355138 -0.0033787754 -0.0033835736 -0.0033876114 -0.00339052 -0.0033919096 -0.0033902081 -0.0033863557 -0.0033738776 -0.0033554477 -0.0033355288 -0.0033190411][-0.0032984987 -0.0032933182 -0.0033239939 -0.0033588009 -0.003382439 -0.0033866165 -0.0033903937 -0.0033933658 -0.0033949669 -0.0033939905 -0.0033904905 -0.0033802383 -0.0033630102 -0.0033447179 -0.0033261557][-0.0033175135 -0.0033122071 -0.0033386229 -0.0033680387 -0.003386372 -0.0033897106 -0.0033931166 -0.0033956883 -0.0033969725 -0.0033962892 -0.0033937446 -0.00338669 -0.0033743009 -0.0033604663 -0.003343852][-0.0033429458 -0.0033391349 -0.0033570386 -0.0033763282 -0.0033874868 -0.0033901632 -0.0033935008 -0.0033960061 -0.0033972922 -0.0033970338 -0.003395397 -0.003391597 -0.0033845415 -0.0033756935 -0.0033643558][-0.0033667723 -0.0033629341 -0.0033719095 -0.0033812886 -0.0033864693 -0.0033890395 -0.0033922428 -0.0033946328 -0.0033958477 -0.003395648 -0.0033941483 -0.0033922957 -0.0033893972 -0.0033849024 -0.0033781552][-0.0033822013 -0.0033783992 -0.0033802781 -0.0033827953 -0.0033842775 -0.0033865285 -0.0033897466 -0.0033922207 -0.0033933425 -0.0033930284 -0.0033915071 -0.003390383 -0.0033901678 -0.0033890246 -0.0033866272]]...]
INFO - root - 2017-12-09 12:38:43.478752: step 24810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:12m:01s remains)
INFO - root - 2017-12-09 12:38:52.196036: step 24820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 73h:26m:47s remains)
INFO - root - 2017-12-09 12:39:00.831050: step 24830, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 76h:37m:46s remains)
INFO - root - 2017-12-09 12:39:09.494924: step 24840, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:32m:08s remains)
INFO - root - 2017-12-09 12:39:18.329651: step 24850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:05m:39s remains)
INFO - root - 2017-12-09 12:39:26.872233: step 24860, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:19m:38s remains)
INFO - root - 2017-12-09 12:39:35.442135: step 24870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:02m:06s remains)
INFO - root - 2017-12-09 12:39:44.011826: step 24880, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 70h:22m:16s remains)
INFO - root - 2017-12-09 12:39:52.518468: step 24890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:12m:22s remains)
INFO - root - 2017-12-09 12:40:00.972960: step 24900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:05m:15s remains)
2017-12-09 12:40:01.822789: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23334144 0.2428768 0.25185806 0.2582106 0.25869974 0.25172049 0.23729159 0.21594472 0.18964718 0.16090208 0.13207826 0.10582186 0.0833094 0.065024368 0.051019978][0.23289841 0.25011024 0.26746628 0.28263015 0.29070315 0.29042915 0.28153071 0.26412058 0.24013138 0.21128468 0.18069667 0.15080406 0.1233687 0.099394426 0.079699866][0.21564567 0.23948745 0.26527804 0.29035482 0.30804247 0.31686878 0.31541958 0.30385786 0.28444943 0.25792134 0.22815967 0.19694731 0.1669874 0.13929814 0.11541267][0.19024315 0.21944056 0.25200889 0.28528696 0.31162247 0.32929412 0.33592483 0.33162358 0.3185322 0.29683223 0.27040744 0.24066333 0.21093084 0.18181147 0.15560713][0.16070038 0.19516386 0.23404512 0.27470583 0.30862173 0.33358052 0.34643865 0.3478885 0.34019661 0.32327378 0.30087483 0.27475247 0.2480084 0.22085619 0.19567475][0.13132523 0.17002131 0.21474187 0.26145336 0.30082759 0.33074623 0.3473295 0.35208884 0.34723198 0.33388126 0.31518313 0.29320523 0.27123046 0.24905908 0.2286243][0.10755177 0.14782643 0.19529791 0.24516636 0.28756228 0.319163 0.33630806 0.34156832 0.33639309 0.3244468 0.30795503 0.2903983 0.27396134 0.25851464 0.24543862][0.0935704 0.13337514 0.18034407 0.22910322 0.27056551 0.30087245 0.31620106 0.31929988 0.31125179 0.29785404 0.2809954 0.26611197 0.25438809 0.24594966 0.24098088][0.0872623 0.12478471 0.16851133 0.21279868 0.24982169 0.27532944 0.28640273 0.28582913 0.27394781 0.25813532 0.239741 0.2257378 0.21671987 0.2140573 0.21645583][0.0837991 0.11680236 0.15466622 0.19208196 0.22248061 0.24148378 0.24694115 0.24162313 0.22598425 0.2077239 0.1881368 0.17466426 0.16729704 0.16827388 0.17575353][0.078336053 0.10493813 0.13479494 0.16347697 0.18584365 0.19787894 0.19814827 0.18923688 0.17171013 0.15262768 0.13328341 0.1206051 0.11454932 0.11745149 0.12723984][0.068276405 0.087982096 0.1093964 0.12930149 0.14396498 0.15009162 0.14680229 0.13627736 0.11935952 0.10152463 0.084176481 0.073063962 0.067979626 0.071130663 0.080652826][0.05302066 0.066572055 0.080835372 0.093508817 0.10209893 0.10437831 0.099811107 0.089994796 0.075991921 0.061432477 0.047612105 0.038443919 0.034056272 0.036166791 0.043317229][0.034557015 0.042820029 0.051369362 0.058748823 0.063341536 0.063582689 0.05931614 0.051636174 0.041589767 0.031470168 0.022271685 0.016151022 0.013043689 0.014040697 0.018454317][0.017129589 0.021388866 0.025780918 0.029516073 0.031652622 0.031354308 0.028479708 0.023582364 0.01754543 0.01158333 0.0065278276 0.0033158385 0.0018250435 0.0023171762 0.0045041828]]...]
INFO - root - 2017-12-09 12:40:10.423685: step 24910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:45m:20s remains)
INFO - root - 2017-12-09 12:40:18.976969: step 24920, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 72h:11m:09s remains)
INFO - root - 2017-12-09 12:40:27.703073: step 24930, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 73h:26m:20s remains)
INFO - root - 2017-12-09 12:40:36.234782: step 24940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:02m:18s remains)
INFO - root - 2017-12-09 12:40:44.734742: step 24950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:35m:27s remains)
INFO - root - 2017-12-09 12:40:53.240498: step 24960, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.939 sec/batch; 80h:14m:18s remains)
INFO - root - 2017-12-09 12:41:01.891987: step 24970, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 76h:03m:02s remains)
INFO - root - 2017-12-09 12:41:10.606433: step 24980, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 74h:00m:50s remains)
INFO - root - 2017-12-09 12:41:19.161751: step 24990, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.959 sec/batch; 81h:57m:15s remains)
INFO - root - 2017-12-09 12:41:27.552206: step 25000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:08m:06s remains)
2017-12-09 12:41:28.442361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0022687 -0.002538777 -0.0026984238 -0.0027958241 -0.0026257646 -0.0023645198 -0.0020720884 -0.0018394712 -0.0014000556 -0.00064768223 0.00080101751 0.0024466892 0.0045276042 0.0068278527 0.0096046068][-0.0016545821 -0.0019443198 -0.0019887579 -0.0019280995 -0.001437682 -0.00087708305 -0.00040910975 -0.00028074184 -0.00019190949 0.00016483758 0.0015143547 0.0036761467 0.0070296628 0.011028374 0.015646331][-0.0010783926 -0.0012556969 -0.0011221548 -0.00064212526 0.00049695745 0.0017432277 0.0026138281 0.0025741411 0.0020383843 0.0016409718 0.0025915112 0.0051806215 0.0098133236 0.015839722 0.022796538][-0.00067252223 -0.000551519 9.28191e-05 0.0014598933 0.0038095368 0.006196578 0.0076995641 0.0073570265 0.0058575738 0.0043379376 0.0046506431 0.007635586 0.013653868 0.02168598 0.030897759][-0.00054359389 -2.6373891e-05 0.0013930458 0.0041812863 0.0082044126 0.01209318 0.014352047 0.013750226 0.011333693 0.0087250983 0.0084583424 0.011836174 0.019024987 0.028733566 0.039737329][-0.00048955786 0.0004295269 0.0027340832 0.0071827834 0.013125713 0.018816404 0.02192504 0.021180894 0.017935041 0.014499544 0.013991897 0.01785304 0.025923707 0.036675375 0.04876104][-0.00041794265 0.00077412557 0.0038530598 0.0098417662 0.017626058 0.025026593 0.02897713 0.028147284 0.024269357 0.0202895 0.019698441 0.0240122 0.032820292 0.0443797 0.057031337][-0.00014280365 0.0013090251 0.0048806593 0.011752295 0.020735506 0.0293837 0.034033373 0.033272531 0.029074412 0.024950314 0.024547257 0.029289991 0.038482416 0.050360218 0.063187823][-7.4825715e-05 0.0015333951 0.0053566871 0.012451659 0.021716628 0.030660167 0.035554171 0.034974422 0.030821089 0.026967846 0.026962012 0.032087032 0.041516107 0.053512547 0.066210859][-0.00031104707 0.0012863385 0.0048622526 0.011314232 0.019759346 0.027987983 0.032608684 0.032318477 0.028720025 0.025579017 0.026029117 0.031244632 0.040428974 0.052200656 0.064572386][-0.00083455327 0.00062636333 0.0036239824 0.0088002756 0.015618601 0.022345267 0.026280429 0.026230581 0.023371845 0.021034181 0.021879297 0.026920937 0.035442077 0.046454038 0.058087893][-0.0016546617 -0.00045710988 0.0018194551 0.0055238623 0.010332301 0.015075217 0.017962128 0.018098526 0.016202345 0.014725617 0.015724853 0.020118132 0.027402394 0.036997348 0.0472682][-0.00245485 -0.0016073209 -7.6737255e-05 0.0022615059 0.0051781433 0.0080406452 0.0098108919 0.010014387 0.0090112677 0.0083534392 0.009403469 0.012848024 0.018408269 0.025879882 0.034110934][-0.0030439412 -0.0025738822 -0.0017300596 -0.00048302533 0.00099632191 0.002388359 0.0032136817 0.0033420883 0.0029511638 0.0028770622 0.0038624601 0.0063392511 0.01019832 0.015395415 0.021221][-0.003311353 -0.0031371377 -0.0028043776 -0.002303343 -0.0017329266 -0.0012172642 -0.00094223232 -0.000900669 -0.00098149013 -0.00082841492 -0.00012727082 0.0013903575 0.0037472397 0.0069858553 0.010698888]]...]
INFO - root - 2017-12-09 12:41:36.954668: step 25010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:20m:32s remains)
INFO - root - 2017-12-09 12:41:45.655808: step 25020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:44m:13s remains)
INFO - root - 2017-12-09 12:41:54.352347: step 25030, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 71h:05m:25s remains)
INFO - root - 2017-12-09 12:42:02.928594: step 25040, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 71h:07m:34s remains)
INFO - root - 2017-12-09 12:42:11.605804: step 25050, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 74h:37m:31s remains)
INFO - root - 2017-12-09 12:42:20.115100: step 25060, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 77h:04m:03s remains)
INFO - root - 2017-12-09 12:42:28.860059: step 25070, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 74h:58m:07s remains)
INFO - root - 2017-12-09 12:42:37.518371: step 25080, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 72h:37m:28s remains)
INFO - root - 2017-12-09 12:42:45.979713: step 25090, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 70h:48m:45s remains)
INFO - root - 2017-12-09 12:42:54.298994: step 25100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:21m:12s remains)
2017-12-09 12:42:55.188089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081034087 0.094724841 0.10582591 0.11246303 0.11260189 0.10758619 0.0984523 0.088138595 0.07817366 0.07081534 0.066529721 0.065065883 0.064895481 0.063586496 0.059359118][0.093751632 0.11097595 0.1259024 0.13603151 0.13879442 0.13531937 0.1265862 0.11614658 0.10540863 0.096785977 0.091033429 0.088262923 0.086862393 0.084003411 0.077865377][0.10137537 0.12250656 0.14208417 0.15699545 0.16422144 0.16426171 0.15774073 0.14826854 0.13730593 0.12723945 0.1194855 0.11418806 0.11005414 0.10449856 0.095698655][0.10673632 0.13146126 0.15554881 0.17563479 0.18800795 0.19236733 0.18910444 0.18144354 0.17068759 0.15888667 0.14830561 0.13959712 0.13207687 0.12322995 0.1113899][0.11013293 0.13840066 0.1667136 0.19147484 0.20875116 0.2174529 0.21754928 0.21198128 0.20189595 0.18896987 0.17582174 0.16363414 0.15240981 0.13995299 0.12503719][0.11172102 0.14294183 0.17454354 0.20300166 0.22418712 0.23629563 0.23938189 0.23600021 0.22709078 0.2137813 0.19882546 0.1840169 0.16969843 0.15404424 0.13623048][0.11188347 0.14488782 0.17832109 0.20871218 0.23198351 0.24606825 0.25119486 0.24958041 0.24196646 0.22889651 0.21312712 0.19685422 0.18066305 0.16302715 0.14329961][0.11098404 0.14457442 0.1784084 0.20894262 0.2323456 0.24660543 0.25211206 0.25114897 0.2441984 0.23160383 0.21588689 0.19941847 0.18287084 0.16464986 0.14435537][0.10898888 0.1418836 0.17468093 0.20387161 0.22587752 0.23874216 0.24308982 0.241478 0.2342415 0.22168861 0.20604627 0.19013286 0.17437617 0.15706176 0.13776326][0.10384094 0.13497457 0.16566913 0.19250335 0.21223153 0.22314273 0.22593378 0.2231067 0.21502678 0.20219822 0.18681169 0.17170554 0.15712443 0.14143766 0.12403271][0.094450511 0.12264034 0.15008323 0.17355447 0.19016238 0.1985624 0.19944617 0.1953302 0.18665351 0.17398137 0.1594476 0.14572924 0.13308023 0.11984828 0.10517732][0.081071556 0.10523392 0.12856397 0.14810188 0.16124086 0.16691165 0.16579325 0.16026141 0.15109359 0.13915007 0.12632534 0.1148582 0.10486022 0.094744384 0.083467916][0.064569458 0.0837796 0.10215181 0.11719029 0.12677641 0.13006258 0.12762447 0.12155608 0.11286611 0.10256325 0.092184737 0.083483554 0.076371804 0.069363624 0.061419226][0.046782475 0.060699604 0.073855519 0.084435895 0.090799838 0.0923246 0.089524783 0.084046386 0.07690265 0.069027081 0.061533563 0.055611167 0.051087126 0.04675138 0.041657895][0.030342847 0.039320376 0.047675274 0.05428445 0.058015127 0.058523946 0.056146637 0.052038696 0.047052812 0.04194472 0.037336249 0.033865761 0.031349402 0.028948445 0.025992768]]...]
INFO - root - 2017-12-09 12:43:03.747989: step 25110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:24m:24s remains)
INFO - root - 2017-12-09 12:43:12.526286: step 25120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:50m:35s remains)
INFO - root - 2017-12-09 12:43:21.121603: step 25130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:56m:32s remains)
INFO - root - 2017-12-09 12:43:29.754639: step 25140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 75h:27m:21s remains)
INFO - root - 2017-12-09 12:43:38.404930: step 25150, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:06m:13s remains)
INFO - root - 2017-12-09 12:43:46.926412: step 25160, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 73h:19m:17s remains)
INFO - root - 2017-12-09 12:43:55.536440: step 25170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:43m:46s remains)
INFO - root - 2017-12-09 12:44:04.069079: step 25180, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.815 sec/batch; 69h:35m:03s remains)
INFO - root - 2017-12-09 12:44:12.677834: step 25190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:34m:51s remains)
INFO - root - 2017-12-09 12:44:21.139835: step 25200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 74h:08m:02s remains)
2017-12-09 12:44:22.093473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003387793 -0.0033863215 -0.0033866966 -0.0033844044 -0.0033716415 -0.0033490527 -0.0033284072 -0.0033194628 -0.0033230213 -0.0033348352 -0.0033494646 -0.0033654887 -0.003378106 -0.003384738 -0.0033874498][-0.0033874307 -0.0033861927 -0.0033862717 -0.0033808136 -0.0033620547 -0.0033311967 -0.0033009702 -0.0032849954 -0.0032873242 -0.0033034196 -0.0033265909 -0.0033507724 -0.0033697584 -0.0033800718 -0.0033853215][-0.0033876305 -0.0033865008 -0.003386036 -0.0033755961 -0.0033459663 -0.0033001164 -0.0032534159 -0.0032246797 -0.0032242036 -0.0032480857 -0.0032860718 -0.003324107 -0.0033539387 -0.003371994 -0.0033812155][-0.0033874097 -0.0033860935 -0.0033829967 -0.0033622687 -0.0033093237 -0.0032299275 -0.0031518717 -0.0031050234 -0.0031044004 -0.0031442605 -0.0032102808 -0.0032770119 -0.0033296959 -0.0033623276 -0.0033764658][-0.0033865604 -0.003384283 -0.0033724948 -0.0033263911 -0.0032206518 -0.003071012 -0.0029262984 -0.002841034 -0.0028437893 -0.0029255431 -0.00305592 -0.0031843714 -0.0032828867 -0.0033420804 -0.0033667879][-0.0033851969 -0.0033805908 -0.0033524923 -0.0032642479 -0.0030805548 -0.0028381289 -0.0026103472 -0.0024742638 -0.002468256 -0.0025801817 -0.0027793208 -0.0029944011 -0.0031748416 -0.0032893352 -0.0033408776][-0.0033774716 -0.0033715926 -0.0033264675 -0.0031943726 -0.002936691 -0.0026100376 -0.002309649 -0.0021257887 -0.002104912 -0.002230623 -0.0024692775 -0.0027449732 -0.0029991432 -0.0031775921 -0.0032731032][-0.0033569173 -0.003352663 -0.0033036647 -0.0031527872 -0.002864829 -0.0024975082 -0.0021552609 -0.0019313763 -0.0018814772 -0.0019879262 -0.0022223722 -0.0025146871 -0.0028067457 -0.0030315744 -0.0031636525][-0.0033277089 -0.003325413 -0.0032866569 -0.0031552548 -0.0028949638 -0.0025434271 -0.0022015339 -0.0019557723 -0.0018691357 -0.0019300173 -0.0021165847 -0.0023745368 -0.002656443 -0.0028912951 -0.0030427794][-0.0033117009 -0.0033052571 -0.0032801938 -0.0031888937 -0.0029949828 -0.002709623 -0.002412037 -0.0021739192 -0.0020602704 -0.0020681478 -0.0021837852 -0.0023744071 -0.0026034759 -0.0028101634 -0.0029552297][-0.0033298319 -0.0033173759 -0.0033007979 -0.0032475407 -0.0031261651 -0.0029299636 -0.0027058311 -0.0025079064 -0.0023919425 -0.0023605297 -0.00241075 -0.0025236881 -0.0026787531 -0.0028324109 -0.0029481051][-0.0033564244 -0.0033439058 -0.003333837 -0.00330847 -0.0032478829 -0.0031376674 -0.0029978459 -0.0028626395 -0.0027668152 -0.0027192058 -0.0027250524 -0.0027760847 -0.0028622397 -0.0029585927 -0.0030333356][-0.0033776849 -0.0033698855 -0.0033639376 -0.0033533156 -0.0033300654 -0.0032847417 -0.0032212075 -0.0031500785 -0.0030883926 -0.0030463766 -0.0030325332 -0.0030460325 -0.0030854181 -0.0031370251 -0.0031741294][-0.0033815466 -0.0033788588 -0.0033784376 -0.0033754252 -0.0033686985 -0.0033566311 -0.0033382133 -0.0033131083 -0.0032855212 -0.0032612868 -0.0032475502 -0.0032473824 -0.0032618011 -0.0032826648 -0.0032954125][-0.0033812462 -0.0033780786 -0.0033780895 -0.0033782222 -0.0033782069 -0.0033778669 -0.0033760311 -0.0033703977 -0.0033620202 -0.0033539352 -0.0033491398 -0.0033477913 -0.0033519678 -0.0033583411 -0.0033613008]]...]
INFO - root - 2017-12-09 12:44:30.564182: step 25210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:17m:56s remains)
INFO - root - 2017-12-09 12:44:39.133744: step 25220, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:37m:28s remains)
INFO - root - 2017-12-09 12:44:47.815305: step 25230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 75h:34m:05s remains)
INFO - root - 2017-12-09 12:44:56.495750: step 25240, loss = 0.91, batch loss = 0.70 (9.6 examples/sec; 0.834 sec/batch; 71h:11m:28s remains)
INFO - root - 2017-12-09 12:45:05.110817: step 25250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 73h:48m:43s remains)
INFO - root - 2017-12-09 12:45:13.748835: step 25260, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 63h:14m:24s remains)
INFO - root - 2017-12-09 12:45:22.576054: step 25270, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 76h:46m:02s remains)
INFO - root - 2017-12-09 12:45:31.296230: step 25280, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 71h:51m:55s remains)
INFO - root - 2017-12-09 12:45:40.000547: step 25290, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 74h:10m:02s remains)
INFO - root - 2017-12-09 12:45:48.635985: step 25300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:15m:35s remains)
2017-12-09 12:45:49.462777: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50762331 0.49602321 0.48000592 0.46243325 0.4464151 0.43324122 0.42250323 0.41319478 0.40464339 0.39404547 0.37898591 0.36338905 0.34642181 0.32898375 0.31110686][0.53286636 0.5237627 0.50959319 0.49416652 0.47868231 0.466513 0.45702657 0.44876081 0.44039449 0.42891294 0.41172552 0.39214727 0.37071717 0.349202 0.32722688][0.54535335 0.54208571 0.53151375 0.51952082 0.5066492 0.49656358 0.48828554 0.48068446 0.47138354 0.45770398 0.43814936 0.41462931 0.38878188 0.36271307 0.33736157][0.5561372 0.56018734 0.5548408 0.54630393 0.53615326 0.52838659 0.52117068 0.51357371 0.50295657 0.48737678 0.4649983 0.437269 0.40652925 0.37584987 0.34688371][0.5571329 0.56880075 0.56996775 0.56708825 0.56209791 0.55654258 0.54977173 0.54160613 0.52805567 0.50944465 0.48368579 0.45270765 0.4189766 0.38527855 0.35432297][0.55022365 0.56919843 0.57543296 0.57732385 0.57620019 0.57255018 0.56759787 0.55888551 0.54427183 0.52337831 0.4955318 0.46270615 0.42669907 0.39136288 0.35927081][0.53486753 0.55919909 0.56926483 0.57521158 0.57745677 0.575993 0.57148385 0.56331646 0.54824078 0.52640414 0.49852362 0.46511474 0.42899752 0.39311168 0.36124042][0.51563686 0.54266888 0.55412215 0.56176251 0.56570715 0.56507587 0.559653 0.55065006 0.53524983 0.51376432 0.48668498 0.45495737 0.42122912 0.38723457 0.35693353][0.49034107 0.51896489 0.53135568 0.54018646 0.54489368 0.54368752 0.5373947 0.52727842 0.51171619 0.49101818 0.46583292 0.43715143 0.40695772 0.37651825 0.34886837][0.45864493 0.48597893 0.49716058 0.50577927 0.51088279 0.5095107 0.50320351 0.49339083 0.47890571 0.46006781 0.43739551 0.41285992 0.38760966 0.36180702 0.338318][0.42723235 0.45165059 0.4600628 0.46661741 0.46964723 0.46757385 0.46198827 0.45358071 0.44204378 0.42640352 0.40830061 0.38828516 0.36755088 0.34636363 0.32660788][0.39650452 0.41749787 0.4229061 0.42691785 0.42874292 0.42672312 0.42145556 0.41428709 0.4050729 0.39318308 0.37908927 0.36340097 0.34735847 0.33068961 0.31502673][0.3660453 0.38428551 0.38785541 0.38985088 0.39039269 0.3877987 0.38319436 0.37704742 0.36988539 0.36104909 0.35054657 0.33911389 0.32715058 0.31502011 0.30321708][0.33677271 0.35299531 0.35563275 0.35665038 0.35694104 0.35491255 0.3518506 0.34738377 0.342646 0.33646375 0.32901269 0.32096916 0.3122707 0.3035278 0.29492354][0.31388414 0.32765451 0.32863939 0.32844374 0.32769725 0.32634854 0.32450768 0.32156658 0.31890008 0.31527069 0.31074905 0.30524129 0.29931206 0.29348364 0.28767151]]...]
INFO - root - 2017-12-09 12:45:58.038193: step 25310, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 76h:10m:05s remains)
INFO - root - 2017-12-09 12:46:06.736505: step 25320, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 76h:45m:05s remains)
INFO - root - 2017-12-09 12:46:15.502758: step 25330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 74h:27m:07s remains)
INFO - root - 2017-12-09 12:46:24.379615: step 25340, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:03m:23s remains)
INFO - root - 2017-12-09 12:46:33.017848: step 25350, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 71h:35m:10s remains)
INFO - root - 2017-12-09 12:46:41.624472: step 25360, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 65h:38m:27s remains)
INFO - root - 2017-12-09 12:46:50.138592: step 25370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:28m:25s remains)
INFO - root - 2017-12-09 12:46:58.717580: step 25380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:48m:18s remains)
INFO - root - 2017-12-09 12:47:07.363895: step 25390, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 75h:13m:59s remains)
INFO - root - 2017-12-09 12:47:15.899076: step 25400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:28m:56s remains)
2017-12-09 12:47:16.803612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033851443 -0.0033847368 -0.0033850253 -0.00338513 -0.0033849587 -0.0033846481 -0.0033842945 -0.0033837566 -0.0033834693 -0.003383341 -0.0033833433 -0.0033834034 -0.0033834691 -0.0033834623 -0.0033834071][-0.0033845424 -0.0033843461 -0.0033848397 -0.0033851913 -0.0033851243 -0.0033847161 -0.0033841212 -0.0033832653 -0.003382674 -0.0033822176 -0.0033820723 -0.0033820374 -0.003382036 -0.0033820609 -0.0033820237][-0.003385626 -0.003385979 -0.0033869147 -0.0033874924 -0.0033874777 -0.0033869056 -0.0033859161 -0.0033847725 -0.0033839336 -0.0033833191 -0.0033828064 -0.0033824092 -0.0033821261 -0.003381937 -0.003381904][-0.0033873038 -0.0033882207 -0.0033890749 -0.0033888381 -0.0033879429 -0.0033865385 -0.0033848537 -0.0033836029 -0.0033833708 -0.003383422 -0.0033831091 -0.0033827096 -0.0033823368 -0.0033820882 -0.0033821233][-0.003389156 -0.003390302 -0.0033902554 -0.0033881257 -0.0033848672 -0.0033812334 -0.0033783747 -0.0033779773 -0.0033799552 -0.0033820102 -0.0033828265 -0.00338294 -0.0033828176 -0.0033825713 -0.0033825813][-0.0033904584 -0.0033916093 -0.0033904365 -0.0033857268 -0.0033781116 -0.0033696729 -0.0033644412 -0.0033657022 -0.0033721454 -0.0033782914 -0.003381578 -0.0033826646 -0.0033831405 -0.0033831256 -0.0033831121][-0.0033908633 -0.0033918696 -0.0033892703 -0.0033810053 -0.0033670131 -0.0033515973 -0.0033434436 -0.0033481487 -0.0033609385 -0.0033728003 -0.0033794718 -0.0033821394 -0.00338304 -0.0033832539 -0.0033832693][-0.0033910749 -0.0033919218 -0.0033882505 -0.0033775966 -0.0033596468 -0.0033399169 -0.0033295376 -0.0033360275 -0.0033523615 -0.0033676326 -0.0033770355 -0.0033812542 -0.0033827396 -0.0033830628 -0.0033830814][-0.00339073 -0.003391301 -0.0033878891 -0.0033784527 -0.0033635509 -0.0033475119 -0.0033383358 -0.0033416685 -0.0033541452 -0.0033675763 -0.0033767947 -0.0033812253 -0.0033827552 -0.0033831825 -0.0033832185][-0.003389643 -0.0033903359 -0.0033883522 -0.0033826465 -0.0033742741 -0.0033650238 -0.003358759 -0.003359342 -0.0033657183 -0.0033736625 -0.003379408 -0.0033822772 -0.0033832686 -0.0033834961 -0.0033834302][-0.0033882388 -0.0033891709 -0.0033888882 -0.0033867059 -0.0033832802 -0.0033793624 -0.0033760034 -0.003375178 -0.0033771207 -0.0033802267 -0.0033823496 -0.0033832288 -0.0033835466 -0.0033834849 -0.0033832667][-0.0033866698 -0.003387715 -0.0033885879 -0.0033884649 -0.0033873841 -0.0033858775 -0.0033840709 -0.0033829906 -0.00338276 -0.0033831957 -0.0033833296 -0.0033832381 -0.0033831808 -0.0033830358 -0.003382819][-0.0033849864 -0.0033855126 -0.0033865634 -0.003387267 -0.0033873057 -0.0033867306 -0.0033855552 -0.0033845541 -0.0033838334 -0.0033833855 -0.0033830141 -0.0033827298 -0.0033826418 -0.003382558 -0.0033824379][-0.0033835426 -0.0033833503 -0.003384325 -0.0033849981 -0.0033852539 -0.0033850097 -0.0033843329 -0.0033835648 -0.0033830043 -0.0033827026 -0.0033825578 -0.0033824539 -0.0033824181 -0.0033823932 -0.0033823303][-0.0033828914 -0.0033823403 -0.0033829503 -0.0033832425 -0.0033833394 -0.0033832802 -0.0033829911 -0.0033827072 -0.0033825382 -0.0033824488 -0.0033823538 -0.0033822965 -0.0033822823 -0.0033822621 -0.003382196]]...]
INFO - root - 2017-12-09 12:47:25.316949: step 25410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:14m:06s remains)
INFO - root - 2017-12-09 12:47:34.083042: step 25420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:16m:23s remains)
INFO - root - 2017-12-09 12:47:42.956392: step 25430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:59m:35s remains)
INFO - root - 2017-12-09 12:47:51.831423: step 25440, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 75h:51m:37s remains)
INFO - root - 2017-12-09 12:48:00.530179: step 25450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 75h:09m:10s remains)
INFO - root - 2017-12-09 12:48:09.251547: step 25460, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:52m:32s remains)
INFO - root - 2017-12-09 12:48:17.897152: step 25470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:26m:33s remains)
INFO - root - 2017-12-09 12:48:26.479896: step 25480, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:51m:37s remains)
INFO - root - 2017-12-09 12:48:34.864748: step 25490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:29m:04s remains)
INFO - root - 2017-12-09 12:48:43.292657: step 25500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:09m:25s remains)
2017-12-09 12:48:44.199148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033723747 -0.0033722341 -0.0033710243 -0.0033653851 -0.0033525482 -0.0033323518 -0.0033090364 -0.0032897715 -0.0032782461 -0.0032752962 -0.0032791966 -0.0032882204 -0.0033018193 -0.0033196758 -0.003339075][-0.0033660932 -0.0033663877 -0.00336558 -0.0033585939 -0.0033418394 -0.0033152429 -0.0032853829 -0.0032618928 -0.0032491924 -0.0032475195 -0.0032543677 -0.0032685297 -0.0032887135 -0.0033135975 -0.0033385931][-0.0033680734 -0.0033661437 -0.0033625741 -0.0033515948 -0.0033305292 -0.0033000056 -0.0032670684 -0.0032413583 -0.0032280197 -0.0032283324 -0.0032394484 -0.003259033 -0.0032844853 -0.0033140113 -0.0033425426][-0.0033702382 -0.0033701877 -0.00336778 -0.0033553098 -0.0033295685 -0.0032934262 -0.0032565806 -0.0032297557 -0.0032186443 -0.0032230937 -0.0032387688 -0.0032632628 -0.00329258 -0.0033240893 -0.0033515138][-0.0033712029 -0.0033703719 -0.0033667313 -0.0033550863 -0.0033310184 -0.0032966309 -0.0032612439 -0.003234298 -0.0032229358 -0.0032286774 -0.0032481176 -0.0032770848 -0.0033090778 -0.0033399716 -0.0033637718][-0.0033732299 -0.0033718944 -0.0033685584 -0.0033563608 -0.0033320643 -0.0032989907 -0.0032660798 -0.0032436447 -0.0032378808 -0.0032476697 -0.003267403 -0.0032943778 -0.0033236335 -0.0033509966 -0.0033707037][-0.0033759046 -0.00337429 -0.0033692538 -0.0033582011 -0.0033369327 -0.0033062568 -0.003274282 -0.0032520031 -0.0032452031 -0.0032549095 -0.003276373 -0.0033043721 -0.0033319737 -0.0033546682 -0.003370218][-0.0033756637 -0.0033760013 -0.003372818 -0.003361251 -0.0033385064 -0.0033079882 -0.0032773679 -0.0032567736 -0.0032509863 -0.0032603547 -0.0032790303 -0.0033031767 -0.0033281019 -0.003349914 -0.0033652496][-0.0033744932 -0.0033731733 -0.0033690108 -0.0033597692 -0.0033403148 -0.0033097286 -0.0032755348 -0.0032512194 -0.0032435637 -0.0032529335 -0.0032741074 -0.0033002978 -0.0033239655 -0.0033427037 -0.003356694][-0.0033754 -0.0033739607 -0.0033695737 -0.0033585106 -0.0033393539 -0.00331118 -0.0032787812 -0.003252984 -0.0032402019 -0.003244132 -0.0032623247 -0.0032881231 -0.0033134266 -0.0033343558 -0.0033496623][-0.0033766967 -0.003375222 -0.003370754 -0.0033596186 -0.003339858 -0.0033129747 -0.0032849407 -0.0032617711 -0.0032485379 -0.0032482494 -0.0032600595 -0.0032811037 -0.0033046978 -0.0033269373 -0.0033441745][-0.0033768378 -0.0033763007 -0.0033739035 -0.0033644359 -0.0033467496 -0.0033209275 -0.0032942127 -0.0032739181 -0.0032643522 -0.0032649152 -0.003274217 -0.0032901193 -0.0033092073 -0.0033287711 -0.003345357][-0.0033782497 -0.0033769854 -0.0033763873 -0.0033701081 -0.003356297 -0.0033345749 -0.0033111477 -0.0032922502 -0.0032835519 -0.0032848581 -0.0032937801 -0.0033086883 -0.003325131 -0.0033413609 -0.0033556561][-0.0033808569 -0.003379775 -0.0033789813 -0.0033744802 -0.003365566 -0.0033501741 -0.003331237 -0.0033140036 -0.0033051374 -0.0033062594 -0.0033149761 -0.0033294298 -0.0033448783 -0.0033580938 -0.0033678922][-0.0033812262 -0.0033814323 -0.0033819091 -0.0033792257 -0.0033730734 -0.0033621471 -0.0033481068 -0.0033353693 -0.0033277459 -0.0033288149 -0.0033362417 -0.0033481924 -0.0033602032 -0.0033698562 -0.003375723]]...]
INFO - root - 2017-12-09 12:48:52.736912: step 25510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:28m:44s remains)
INFO - root - 2017-12-09 12:49:01.149598: step 25520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:13m:20s remains)
INFO - root - 2017-12-09 12:49:09.723987: step 25530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:16m:56s remains)
INFO - root - 2017-12-09 12:49:18.372333: step 25540, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:23m:12s remains)
INFO - root - 2017-12-09 12:49:26.986324: step 25550, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:40m:55s remains)
INFO - root - 2017-12-09 12:49:35.716191: step 25560, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 73h:04m:30s remains)
INFO - root - 2017-12-09 12:49:44.218216: step 25570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:57m:06s remains)
INFO - root - 2017-12-09 12:49:52.723386: step 25580, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 71h:12m:38s remains)
INFO - root - 2017-12-09 12:50:01.319897: step 25590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:24m:21s remains)
INFO - root - 2017-12-09 12:50:10.066743: step 25600, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 72h:38m:35s remains)
2017-12-09 12:50:10.823153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033902868 -0.0033891285 -0.0033884298 -0.0033875345 -0.0033871897 -0.0033868928 -0.0033873003 -0.0033883373 -0.0033903008 -0.0033920989 -0.0033936426 -0.0033931287 -0.0033901231 -0.0033862507 -0.0033817128][-0.0033830716 -0.0033815966 -0.0033811226 -0.0033807533 -0.0033808744 -0.0033813242 -0.0033825196 -0.0033840272 -0.0033863042 -0.0033886426 -0.0033909073 -0.0033916018 -0.0033898321 -0.003387115 -0.0033837843][-0.0033743642 -0.0033725679 -0.0033722974 -0.0033723514 -0.0033727183 -0.0033735898 -0.0033752045 -0.0033769202 -0.003379599 -0.0033827615 -0.0033861969 -0.0033885199 -0.0033889392 -0.0033883704 -0.0033868563][-0.0033652827 -0.0033630452 -0.0033625513 -0.0033625402 -0.0033629204 -0.003363895 -0.0033656943 -0.0033680927 -0.0033717095 -0.0033759256 -0.0033804532 -0.003384294 -0.0033870158 -0.0033888395 -0.0033895236][-0.0033572356 -0.0033543468 -0.0033533538 -0.003352945 -0.0033530006 -0.0033539198 -0.0033559194 -0.0033593278 -0.003364438 -0.0033702066 -0.0033761049 -0.0033817019 -0.0033862542 -0.0033894093 -0.0033913217][-0.0033513941 -0.0033475494 -0.0033458862 -0.0033452013 -0.0033450259 -0.0033459982 -0.0033484583 -0.0033529613 -0.0033596039 -0.003367122 -0.0033741109 -0.0033805748 -0.0033859457 -0.0033894298 -0.0033915462][-0.0033485827 -0.0033439344 -0.0033418771 -0.0033412979 -0.0033408192 -0.0033415817 -0.0033441223 -0.0033493876 -0.003357219 -0.0033664249 -0.0033744874 -0.0033810462 -0.0033860283 -0.0033888987 -0.0033905914][-0.0033474967 -0.0033429288 -0.0033411356 -0.0033408918 -0.0033403737 -0.0033409821 -0.0033436732 -0.0033491745 -0.0033576281 -0.0033672659 -0.0033754993 -0.0033815901 -0.0033857087 -0.0033878507 -0.0033889215][-0.0033468124 -0.0033425679 -0.0033415337 -0.0033415791 -0.0033415845 -0.0033424161 -0.0033453405 -0.0033513661 -0.0033601155 -0.0033698517 -0.0033780688 -0.0033834518 -0.0033861522 -0.0033869045 -0.0033871585][-0.0033465705 -0.0033427151 -0.0033426988 -0.0033433621 -0.0033438564 -0.0033448879 -0.0033481598 -0.003354521 -0.0033631646 -0.0033730594 -0.0033817096 -0.0033873082 -0.0033892076 -0.0033882293 -0.0033870465][-0.0033471638 -0.0033436518 -0.0033442837 -0.0033451561 -0.0033460127 -0.0033472779 -0.0033504718 -0.0033563345 -0.0033643215 -0.0033736492 -0.0033825194 -0.0033886856 -0.0033911562 -0.0033900994 -0.0033882386][-0.0033477815 -0.0033447947 -0.0033458094 -0.0033467507 -0.0033476755 -0.0033489529 -0.0033515689 -0.0033564211 -0.0033631311 -0.0033711772 -0.0033793084 -0.0033856228 -0.0033889117 -0.0033888053 -0.0033875788][-0.0033482029 -0.00334543 -0.0033463282 -0.0033472483 -0.0033480732 -0.0033492509 -0.0033513959 -0.00335523 -0.0033606093 -0.0033671297 -0.0033739645 -0.0033795137 -0.0033828751 -0.0033837673 -0.0033831492][-0.0033481868 -0.0033454269 -0.0033462164 -0.0033468721 -0.0033475566 -0.0033484972 -0.0033501452 -0.0033530442 -0.0033574528 -0.0033628214 -0.003368573 -0.0033732927 -0.003376402 -0.0033776164 -0.0033770059][-0.003347676 -0.0033445626 -0.0033450269 -0.0033456406 -0.0033460858 -0.0033468285 -0.0033482171 -0.0033501883 -0.0033533694 -0.003357528 -0.0033621865 -0.0033661369 -0.003368801 -0.0033700026 -0.0033695588]]...]
INFO - root - 2017-12-09 12:50:19.287217: step 25610, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:03m:35s remains)
INFO - root - 2017-12-09 12:50:28.031922: step 25620, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 76h:59m:53s remains)
INFO - root - 2017-12-09 12:50:36.694515: step 25630, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 74h:31m:55s remains)
INFO - root - 2017-12-09 12:50:45.360479: step 25640, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:32m:48s remains)
INFO - root - 2017-12-09 12:50:54.019051: step 25650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:37m:22s remains)
INFO - root - 2017-12-09 12:51:02.755771: step 25660, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 76h:19m:14s remains)
INFO - root - 2017-12-09 12:51:11.376793: step 25670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:31m:40s remains)
INFO - root - 2017-12-09 12:51:20.159892: step 25680, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 72h:45m:38s remains)
INFO - root - 2017-12-09 12:51:28.745886: step 25690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 74h:48m:28s remains)
INFO - root - 2017-12-09 12:51:37.416415: step 25700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 75h:04m:27s remains)
2017-12-09 12:51:38.177946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013138398 -0.0010044603 -0.00083282706 -0.000786657 -0.000963297 -0.0013364945 -0.0018340971 -0.0023246128 -0.002741866 -0.0030530563 -0.0032553247 -0.0033628307 -0.0033934817 -0.0033952061 -0.0033907655][-0.00073197344 -0.00023338269 0.00011761882 0.00028688321 0.00012080534 -0.00031778822 -0.00093649584 -0.0015966188 -0.0022231252 -0.0027559795 -0.0031282771 -0.0033213098 -0.0033828993 -0.0033906635 -0.0033851813][-0.00064984313 -5.5571785e-05 0.00044071418 0.00077181868 0.00073239021 0.00034214463 -0.00028856634 -0.00099626556 -0.0017041117 -0.0023718793 -0.0029121612 -0.0032402603 -0.0033682499 -0.003389881 -0.0033869278][-0.0011228018 -0.00046668993 0.0001966306 0.00074473396 0.00094318436 0.00074583804 0.00022554421 -0.00044752448 -0.0011914328 -0.0019513177 -0.0026105479 -0.0030679256 -0.0033048897 -0.0033823918 -0.0033910251][-0.0019316293 -0.0012755699 -0.00051896274 0.00019637169 0.00064475043 0.00074342405 0.00053234538 0.00012083445 -0.00047299312 -0.0012447899 -0.002070379 -0.0027616213 -0.0031831677 -0.0033547536 -0.0033920754][-0.0026403698 -0.0020866776 -0.0013476566 -0.00055506174 9.164121e-05 0.00046521146 0.00056691328 0.00045432826 8.1679318e-05 -0.00060908007 -0.0015112573 -0.0023897216 -0.0030099463 -0.0033079523 -0.0033891094][-0.00311177 -0.0027650453 -0.0022011735 -0.0014717327 -0.0007600633 -0.00019943668 0.00016141357 0.00029274821 0.00012727338 -0.00043092645 -0.0013137781 -0.002254257 -0.0029476648 -0.0032906123 -0.0033862162][-0.0032518902 -0.0031016017 -0.0028111502 -0.0023413755 -0.001758123 -0.0011598866 -0.000654608 -0.00035073096 -0.00038459105 -0.00084457942 -0.0016234268 -0.002446516 -0.0030354359 -0.0033146096 -0.0033844151][-0.0033258465 -0.0032544539 -0.0031383296 -0.0029276891 -0.0026016615 -0.0021649203 -0.001697754 -0.0013688935 -0.0013408 -0.0016726605 -0.0022522942 -0.0028287964 -0.0032022456 -0.0033539189 -0.0033842777][-0.0033731819 -0.0033471158 -0.0033033369 -0.0032318207 -0.0031048043 -0.0028880406 -0.0026119733 -0.0023987358 -0.0023698378 -0.0025640798 -0.0028833693 -0.0031691832 -0.0033280291 -0.0033780052 -0.0033847161][-0.0033893716 -0.0033861951 -0.0033772059 -0.0033554754 -0.0033111121 -0.0032374703 -0.0031394546 -0.0030596878 -0.0030570626 -0.0031410216 -0.0032583189 -0.0033439747 -0.0033783691 -0.0033844647 -0.0033856519][-0.0033892179 -0.0033898389 -0.0033903308 -0.0033875119 -0.003378595 -0.0033607322 -0.0033374634 -0.0033216286 -0.0033204495 -0.0033390555 -0.0033643709 -0.0033794916 -0.0033838607 -0.003384704 -0.003385491][-0.0033883979 -0.0033883939 -0.0033889725 -0.0033892395 -0.003388511 -0.003386281 -0.003383778 -0.0033825352 -0.0033820001 -0.0033819065 -0.003382476 -0.0033831014 -0.0033835978 -0.0033840556 -0.0033846071][-0.0033875874 -0.0033872 -0.0033875483 -0.0033876856 -0.003387419 -0.0033863762 -0.0033849813 -0.0033839289 -0.0033830644 -0.0033827825 -0.0033830712 -0.0033834444 -0.0033834213 -0.0033834893 -0.0033837622][-0.0033877597 -0.0033870018 -0.0033870279 -0.0033871641 -0.0033870954 -0.0033865648 -0.0033858612 -0.0033850889 -0.0033843492 -0.0033839319 -0.0033837492 -0.0033838036 -0.0033838637 -0.0033840509 -0.0033841336]]...]
INFO - root - 2017-12-09 12:51:46.612297: step 25710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:04m:42s remains)
INFO - root - 2017-12-09 12:51:55.395261: step 25720, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 72h:31m:12s remains)
INFO - root - 2017-12-09 12:52:04.074849: step 25730, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 73h:12m:14s remains)
INFO - root - 2017-12-09 12:52:12.939584: step 25740, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.989 sec/batch; 84h:15m:58s remains)
INFO - root - 2017-12-09 12:52:21.709703: step 25750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:25m:49s remains)
INFO - root - 2017-12-09 12:52:30.407127: step 25760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:33m:48s remains)
INFO - root - 2017-12-09 12:52:39.151035: step 25770, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 74h:23m:38s remains)
INFO - root - 2017-12-09 12:52:47.849491: step 25780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:44m:29s remains)
INFO - root - 2017-12-09 12:52:56.435691: step 25790, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 75h:13m:32s remains)
INFO - root - 2017-12-09 12:53:05.112958: step 25800, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 76h:46m:19s remains)
2017-12-09 12:53:05.977348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033728732 -0.0033248048 -0.0029838337 -0.0019262609 -6.1559957e-05 0.0021744284 0.0039540604 0.0046567842 0.0040053641 0.0024096656 0.000591608 -0.0009632078 -0.0020326476 -0.0027010408 -0.0030640766][-0.0030636434 -0.0030032957 -0.0022196844 -0.00011465629 0.0032364388 0.0069376761 0.0095997034 0.010174624 0.0084866 0.0053745713 0.0021289715 -0.00039069355 -0.0019250995 -0.0027218333 -0.003088292][-0.0018214006 -0.0015053431 0.00025651162 0.0043623177 0.010503121 0.016985398 0.0214647 0.022184897 0.01900547 0.013335469 0.0073195072 0.0025032635 -0.00058256858 -0.0022264761 -0.002968082][-4.2823609e-05 0.0013679892 0.005110343 0.012304748 0.022393219 0.03281153 0.040093306 0.04152526 0.036806062 0.027803574 0.017667219 0.0089585576 0.0028663825 -0.00069812615 -0.0024588336][0.0019952625 0.0048547043 0.011229564 0.022197589 0.036814719 0.051717684 0.062399209 0.065205529 0.059478536 0.047318459 0.03265414 0.019109508 0.008820245 0.0022446255 -0.0013043622][0.0038482116 0.0083004255 0.017238494 0.031578239 0.050087437 0.068849772 0.082705066 0.0873413 0.081622355 0.067507349 0.049218297 0.031195974 0.016512319 0.0064254627 0.0005631221][0.00484603 0.010200958 0.020760614 0.037202798 0.05805178 0.079196416 0.095356524 0.1019792 0.097513519 0.083480559 0.063759811 0.043025672 0.024984529 0.011712734 0.0033879171][0.0050627124 0.010406109 0.020982312 0.037454408 0.058416251 0.079956442 0.097103924 0.10542092 0.10308797 0.091069855 0.072565921 0.05186376 0.032669712 0.017528107 0.0071787518][0.0042597987 0.0089999512 0.018379519 0.033063963 0.05193055 0.071695469 0.088114165 0.097207636 0.097073123 0.088171281 0.072912805 0.054789949 0.036963791 0.021902647 0.010701705][0.0025391907 0.0062662363 0.013685227 0.025426811 0.040685955 0.056969326 0.070979938 0.079518557 0.080868348 0.075167619 0.064094149 0.050168633 0.035682041 0.022619309 0.012140217][0.00055810274 0.003130873 0.00829722 0.01661652 0.027594771 0.039526351 0.05007413 0.056920577 0.058718927 0.055488307 0.048365057 0.038989861 0.028775057 0.01901778 0.010659973][-0.0011783713 0.00039032334 0.0035433525 0.0087002246 0.015606445 0.023231357 0.030109739 0.034767453 0.036292903 0.03466884 0.030623859 0.025141865 0.018966584 0.012766049 0.0071237721][-0.0024135236 -0.0015724807 0.00011040876 0.0028808529 0.00661835 0.010777395 0.014568391 0.01719778 0.018149467 0.017404357 0.015382742 0.012628078 0.0094859581 0.0062040929 0.0030402646][-0.0030893746 -0.002724628 -0.0019707093 -0.00071504293 0.00098289619 0.0028651583 0.0045745345 0.0057661766 0.0062068431 0.005884612 0.00500491 0.0038386376 0.0025334891 0.0011411982 -0.00027619768][-0.0033278402 -0.0032234394 -0.0029731342 -0.0025298032 -0.0019195413 -0.0012475348 -0.0006449041 -0.00023217173 -8.7802531e-05 -0.00021434086 -0.00053010788 -0.00092378072 -0.0013395855 -0.0017824296 -0.0022585762]]...]
INFO - root - 2017-12-09 12:53:14.532904: step 25810, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 76h:36m:03s remains)
INFO - root - 2017-12-09 12:53:23.216770: step 25820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:10m:19s remains)
INFO - root - 2017-12-09 12:53:31.841474: step 25830, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:33m:20s remains)
INFO - root - 2017-12-09 12:53:40.492699: step 25840, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 69h:44m:25s remains)
INFO - root - 2017-12-09 12:53:49.174213: step 25850, loss = 0.88, batch loss = 0.68 (8.8 examples/sec; 0.911 sec/batch; 77h:37m:03s remains)
INFO - root - 2017-12-09 12:53:57.813636: step 25860, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:55m:48s remains)
INFO - root - 2017-12-09 12:54:06.386158: step 25870, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 76h:50m:14s remains)
INFO - root - 2017-12-09 12:54:15.133152: step 25880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:48m:38s remains)
INFO - root - 2017-12-09 12:54:23.819508: step 25890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:58m:40s remains)
INFO - root - 2017-12-09 12:54:32.492229: step 25900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:29m:52s remains)
2017-12-09 12:54:33.323901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033662179 -0.0033644503 -0.0033641318 -0.0033641781 -0.0033642577 -0.0033643425 -0.003364491 -0.003364725 -0.0033649895 -0.0033651432 -0.0033652796 -0.0033653595 -0.0033653073 -0.0033651537 -0.0033650065][-0.003364214 -0.003362176 -0.0033619043 -0.0033621446 -0.0033624296 -0.003362667 -0.0033629411 -0.0033632128 -0.0033633565 -0.0033633038 -0.0033632438 -0.003363125 -0.0033629567 -0.0033627893 -0.0033627041][-0.0033645527 -0.0033624703 -0.0033623106 -0.0033626824 -0.003363061 -0.0033633648 -0.0033637178 -0.003364057 -0.0033641097 -0.0033639222 -0.0033636265 -0.0033632996 -0.0033630016 -0.003362719 -0.0033625977][-0.0033647544 -0.003362712 -0.0033626275 -0.0033629853 -0.0033634345 -0.0033639909 -0.003364638 -0.0033652082 -0.0033652997 -0.003365007 -0.0033645579 -0.0033640193 -0.0033635222 -0.0033629802 -0.0033627185][-0.0033647094 -0.0033627811 -0.003362881 -0.0033633406 -0.0033640417 -0.0033650773 -0.0033662645 -0.0033672871 -0.0033676797 -0.0033673381 -0.0033667258 -0.0033659327 -0.003364912 -0.0033637767 -0.0033631227][-0.0033650966 -0.0033634522 -0.0033639388 -0.0033648782 -0.0033660741 -0.0033676082 -0.0033693328 -0.0033707423 -0.0033713174 -0.0033710191 -0.0033701987 -0.0033689244 -0.0033670072 -0.0033650887 -0.0033638966][-0.0033656093 -0.0033642438 -0.0033650077 -0.0033664114 -0.0033680329 -0.0033699814 -0.0033721426 -0.0033737705 -0.0033743172 -0.0033740534 -0.0033731384 -0.0033711542 -0.003368462 -0.00336603 -0.0033643504][-0.0033657784 -0.0033645651 -0.0033656417 -0.0033674182 -0.0033694198 -0.00337172 -0.0033740452 -0.0033756045 -0.0033758592 -0.0033752609 -0.0033736592 -0.0033709498 -0.003367909 -0.0033655379 -0.0033639434][-0.0033653486 -0.0033640165 -0.0033650785 -0.00336695 -0.003368889 -0.0033710869 -0.0033733305 -0.0033746879 -0.0033749361 -0.0033743491 -0.0033726674 -0.0033700743 -0.0033674045 -0.0033652384 -0.0033636796][-0.0033652566 -0.0033638736 -0.003364837 -0.0033664142 -0.0033681511 -0.0033701251 -0.0033721686 -0.003373618 -0.0033741472 -0.0033738418 -0.0033724376 -0.003370258 -0.0033678806 -0.0033657807 -0.0033642291][-0.0033655942 -0.0033643658 -0.0033653677 -0.0033668331 -0.003368366 -0.0033700564 -0.00337171 -0.0033729321 -0.0033733572 -0.0033730671 -0.0033718641 -0.003370211 -0.0033682575 -0.0033664061 -0.0033648713][-0.0033656603 -0.0033643423 -0.0033652163 -0.0033664007 -0.0033676587 -0.0033689516 -0.0033702275 -0.0033710757 -0.0033713661 -0.0033711318 -0.0033702115 -0.0033688575 -0.0033673483 -0.0033659041 -0.0033646389][-0.0033653427 -0.0033636922 -0.0033643169 -0.0033651183 -0.0033660149 -0.0033669323 -0.0033677223 -0.0033682035 -0.00336829 -0.0033679842 -0.0033672659 -0.0033663516 -0.00336544 -0.0033645802 -0.003363861][-0.003365028 -0.0033630657 -0.0033634314 -0.0033638573 -0.0033643213 -0.0033648591 -0.0033652687 -0.003365502 -0.0033655518 -0.003365407 -0.0033650694 -0.003364553 -0.0033640594 -0.0033636491 -0.0033632754][-0.0033649048 -0.0033627392 -0.0033628989 -0.0033631031 -0.003363336 -0.0033635846 -0.0033637481 -0.0033638051 -0.0033637811 -0.0033636859 -0.0033635229 -0.0033632489 -0.0033630172 -0.0033628962 -0.003362793]]...]
INFO - root - 2017-12-09 12:54:41.721523: step 25910, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 73h:31m:53s remains)
INFO - root - 2017-12-09 12:54:50.386425: step 25920, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 73h:42m:04s remains)
INFO - root - 2017-12-09 12:54:59.035820: step 25930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:21m:20s remains)
INFO - root - 2017-12-09 12:55:07.776797: step 25940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 75h:16m:12s remains)
INFO - root - 2017-12-09 12:55:16.384932: step 25950, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 75h:36m:49s remains)
INFO - root - 2017-12-09 12:55:25.120151: step 25960, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 74h:00m:48s remains)
INFO - root - 2017-12-09 12:55:33.651395: step 25970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:05m:27s remains)
INFO - root - 2017-12-09 12:55:42.301578: step 25980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:36m:36s remains)
INFO - root - 2017-12-09 12:55:50.952947: step 25990, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 76h:26m:51s remains)
INFO - root - 2017-12-09 12:55:59.639114: step 26000, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 73h:45m:14s remains)
2017-12-09 12:56:00.504224: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.073468558 0.11610764 0.17798682 0.25290546 0.32682061 0.38403198 0.41224524 0.41019964 0.38315019 0.34646979 0.30965775 0.28225 0.26565337 0.26595271 0.28182703][0.0824199 0.12960935 0.19731604 0.27754095 0.35608369 0.41729629 0.44792247 0.44653523 0.4184517 0.37952659 0.33839777 0.3053391 0.28228691 0.2763029 0.28687012][0.08608783 0.13390739 0.20195307 0.28377539 0.36446467 0.42786032 0.46047524 0.46015072 0.43239066 0.392866 0.34991083 0.31291783 0.28398564 0.26983678 0.27165839][0.085638948 0.13279042 0.20015593 0.28233945 0.36426196 0.43110374 0.46706134 0.4680649 0.44062451 0.39979845 0.35437331 0.3131566 0.27853829 0.25746214 0.25081766][0.07835836 0.12502559 0.1914683 0.27353242 0.35577372 0.4251278 0.46389407 0.46792364 0.44227046 0.40011281 0.35195222 0.30574277 0.26568168 0.23816831 0.22459324][0.066613667 0.11065105 0.17489363 0.25609824 0.33858052 0.40962645 0.45005444 0.4564845 0.43218976 0.38864014 0.3379513 0.28723016 0.24345872 0.21167454 0.19403517][0.052503295 0.091782205 0.15058115 0.2279429 0.30832112 0.37924302 0.42093721 0.42923436 0.40639269 0.36306673 0.31101996 0.25758171 0.21222152 0.17868024 0.15978523][0.037609138 0.071369126 0.12254655 0.19303422 0.26764894 0.33537868 0.37665826 0.38681021 0.36672789 0.32490814 0.27308759 0.21949778 0.17451496 0.14146741 0.12424693][0.022620093 0.049777862 0.092184909 0.1527808 0.21807817 0.27947035 0.31829473 0.32967505 0.31330663 0.27530682 0.22662115 0.17542066 0.13296868 0.10237379 0.088115983][0.011624835 0.030653583 0.063176975 0.11144161 0.16505362 0.2172813 0.25129902 0.26257184 0.24962075 0.21748991 0.17507789 0.13005823 0.093300916 0.0674588 0.056815483][0.0068380386 0.01787504 0.039671071 0.074016772 0.11446472 0.15511213 0.18264107 0.19257164 0.18263985 0.156942 0.12280942 0.087270737 0.058702003 0.039310619 0.032750119][0.0042201923 0.0098491739 0.022661166 0.044400748 0.071997531 0.10055791 0.1209715 0.12898703 0.12210657 0.10322066 0.078075908 0.052511718 0.032295804 0.019170931 0.015605778][0.0013296723 0.00415562 0.010974513 0.023407791 0.04027129 0.058162581 0.07165999 0.07719744 0.07294748 0.060415786 0.043881025 0.027475376 0.01468714 0.0066205589 0.0046817847][-0.00133701 -2.883398e-05 0.0029637886 0.0088743176 0.017413832 0.026914477 0.034502719 0.03776677 0.035552189 0.028579561 0.019450786 0.01068358 0.0040795282 7.7026896e-05 -0.00088836532][-0.003037259 -0.0025267387 -0.0013909861 0.0008646904 0.004202161 0.0080921492 0.011240354 0.012580979 0.011627925 0.008743858 0.0050041853 0.001491565 -0.0010338207 -0.0024383436 -0.0027439981]]...]
INFO - root - 2017-12-09 12:56:08.872020: step 26010, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 75h:21m:41s remains)
INFO - root - 2017-12-09 12:56:17.579605: step 26020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:24m:50s remains)
INFO - root - 2017-12-09 12:56:26.284150: step 26030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:31m:35s remains)
INFO - root - 2017-12-09 12:56:34.993809: step 26040, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 76h:16m:23s remains)
INFO - root - 2017-12-09 12:56:43.706551: step 26050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:10m:50s remains)
INFO - root - 2017-12-09 12:56:52.427071: step 26060, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 77h:39m:51s remains)
INFO - root - 2017-12-09 12:57:00.963759: step 26070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:39m:26s remains)
INFO - root - 2017-12-09 12:57:09.571301: step 26080, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.857 sec/batch; 72h:57m:07s remains)
INFO - root - 2017-12-09 12:57:18.257502: step 26090, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:24m:00s remains)
INFO - root - 2017-12-09 12:57:26.651289: step 26100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:03m:46s remains)
2017-12-09 12:57:27.565545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033621797 -0.003363021 -0.0033666121 -0.003370564 -0.0033738231 -0.0033752592 -0.0033750192 -0.0033746946 -0.0033735645 -0.0033722024 -0.0033727924 -0.0033738718 -0.0033747882 -0.0033767589 -0.0033795524][-0.0033670266 -0.0033661658 -0.0033671253 -0.0033679304 -0.0033683653 -0.0033670275 -0.0033653802 -0.0033643481 -0.0033636393 -0.0033638768 -0.0033665081 -0.0033693057 -0.0033718057 -0.003374151 -0.0033773838][-0.0033743987 -0.0033727947 -0.0033714988 -0.0033693288 -0.0033668184 -0.0033628256 -0.0033591175 -0.0033567965 -0.0033568908 -0.0033594393 -0.0033640836 -0.0033685954 -0.0033725067 -0.003375758 -0.0033793121][-0.0033810106 -0.0033788597 -0.003375676 -0.003371733 -0.0033668668 -0.0033606531 -0.0033556714 -0.0033531315 -0.0033548223 -0.0033593059 -0.0033660971 -0.003372096 -0.0033765736 -0.0033797305 -0.0033832288][-0.003386165 -0.0033845436 -0.003381012 -0.0033768215 -0.0033700997 -0.0033621697 -0.003356274 -0.0033540688 -0.0033566812 -0.0033622824 -0.0033702792 -0.0033765282 -0.0033805755 -0.0033834034 -0.003386246][-0.0033878426 -0.003386528 -0.0033840528 -0.0033808667 -0.0033745433 -0.0033669672 -0.0033608396 -0.003358362 -0.00336136 -0.0033674948 -0.0033755293 -0.0033809245 -0.0033840509 -0.0033862195 -0.0033889532][-0.0033865389 -0.00338603 -0.0033857678 -0.0033843482 -0.003379354 -0.0033726997 -0.0033666487 -0.0033645455 -0.0033673064 -0.0033731372 -0.0033799284 -0.0033838504 -0.0033857659 -0.0033874479 -0.0033894253][-0.003384073 -0.003384517 -0.0033858153 -0.0033866169 -0.0033839433 -0.0033788374 -0.0033733614 -0.0033707267 -0.003373075 -0.0033777656 -0.00338311 -0.0033854651 -0.0033865925 -0.0033879483 -0.0033894423][-0.0033825829 -0.0033831208 -0.0033849408 -0.0033876183 -0.0033870055 -0.0033839694 -0.0033793671 -0.0033769326 -0.0033787091 -0.0033817186 -0.0033843836 -0.0033856153 -0.0033867541 -0.0033883627 -0.0033898333][-0.0033823492 -0.0033826046 -0.0033851427 -0.0033886009 -0.0033889974 -0.0033868614 -0.0033825964 -0.0033812071 -0.0033828672 -0.0033841743 -0.0033852025 -0.0033857904 -0.0033873196 -0.003389389 -0.0033903974][-0.0033821058 -0.0033821119 -0.0033846931 -0.0033882419 -0.0033894547 -0.0033880877 -0.0033844658 -0.0033836707 -0.0033848369 -0.0033850283 -0.0033848593 -0.003385657 -0.0033874428 -0.0033897876 -0.0033913476][-0.003382537 -0.0033820153 -0.0033840823 -0.0033872998 -0.0033891264 -0.0033879043 -0.0033847156 -0.0033841957 -0.0033850004 -0.0033846435 -0.0033842358 -0.0033852186 -0.0033870675 -0.0033901313 -0.0033921211][-0.0033831822 -0.0033822893 -0.0033837077 -0.0033859995 -0.0033873541 -0.0033867108 -0.0033849766 -0.003384687 -0.0033852665 -0.003384545 -0.0033839196 -0.0033849587 -0.00338712 -0.0033904277 -0.0033922968][-0.0033831235 -0.0033819331 -0.0033830125 -0.0033846067 -0.0033854258 -0.0033849361 -0.0033838504 -0.003383779 -0.0033844225 -0.0033839494 -0.003383786 -0.0033850169 -0.0033870186 -0.0033899753 -0.0033913911][-0.0033828286 -0.0033813783 -0.0033820926 -0.0033832884 -0.003383565 -0.0033835042 -0.0033828621 -0.0033832057 -0.0033834002 -0.0033832032 -0.0033837291 -0.0033851892 -0.003386785 -0.0033889487 -0.0033896493]]...]
INFO - root - 2017-12-09 12:57:35.752088: step 26110, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 71h:16m:05s remains)
INFO - root - 2017-12-09 12:57:44.403499: step 26120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 75h:02m:30s remains)
INFO - root - 2017-12-09 12:57:52.935497: step 26130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 71h:58m:46s remains)
INFO - root - 2017-12-09 12:58:01.366166: step 26140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:00m:23s remains)
INFO - root - 2017-12-09 12:58:10.047536: step 26150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:01m:13s remains)
INFO - root - 2017-12-09 12:58:18.634188: step 26160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:44m:24s remains)
INFO - root - 2017-12-09 12:58:27.052786: step 26170, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 73h:10m:50s remains)
INFO - root - 2017-12-09 12:58:35.557119: step 26180, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 73h:45m:42s remains)
INFO - root - 2017-12-09 12:58:44.231829: step 26190, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 73h:52m:49s remains)
INFO - root - 2017-12-09 12:58:52.915488: step 26200, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 71h:39m:24s remains)
2017-12-09 12:58:53.731323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033434189 -0.0033396538 -0.0033395521 -0.0033399886 -0.003340502 -0.00334105 -0.0033413621 -0.0033416038 -0.0033416906 -0.0033416278 -0.0033416224 -0.0033416154 -0.0033414774 -0.0033412522 -0.0033409246][-0.0033412757 -0.0033371567 -0.0033371262 -0.0033377998 -0.0033386552 -0.0033394019 -0.0033399938 -0.0033404827 -0.0033408343 -0.0033412729 -0.0033417414 -0.0033419726 -0.0033420131 -0.0033417165 -0.0033408045][-0.0033414788 -0.0033372671 -0.0033373835 -0.0033382734 -0.0033394205 -0.0033404308 -0.0033415074 -0.0033425279 -0.0033436376 -0.0033450015 -0.0033462942 -0.0033471519 -0.003347486 -0.0033470395 -0.0033453149][-0.00334137 -0.0033368657 -0.0033368187 -0.0033375632 -0.0033387986 -0.0033403828 -0.0033426099 -0.003345019 -0.0033476588 -0.0033506474 -0.0033532875 -0.0033549436 -0.0033554977 -0.00335476 -0.0033519778][-0.0033414816 -0.0033365407 -0.0033361805 -0.0033368308 -0.0033382191 -0.0033402913 -0.0033435228 -0.0033475128 -0.0033518267 -0.0033566945 -0.003360946 -0.0033637295 -0.0033645728 -0.0033634335 -0.003359613][-0.0033413321 -0.0033361178 -0.0033354471 -0.0033361064 -0.003337709 -0.0033400194 -0.0033441233 -0.0033494181 -0.0033552747 -0.0033619371 -0.0033679295 -0.0033719393 -0.0033731135 -0.0033714746 -0.0033666284][-0.0033410236 -0.0033357549 -0.0033348377 -0.0033351164 -0.0033365607 -0.0033390003 -0.0033436723 -0.0033501417 -0.0033574461 -0.0033657723 -0.0033730846 -0.0033779775 -0.0033793568 -0.0033772348 -0.0033716834][-0.0033407686 -0.0033355204 -0.0033344764 -0.003334522 -0.0033356338 -0.0033379812 -0.0033424541 -0.0033490157 -0.0033568949 -0.0033666447 -0.0033748613 -0.0033803394 -0.0033819189 -0.0033794856 -0.0033734625][-0.0033411423 -0.0033359821 -0.0033349914 -0.0033350969 -0.0033361216 -0.0033382152 -0.0033423414 -0.0033487387 -0.0033562924 -0.0033653839 -0.0033730778 -0.0033785265 -0.0033802288 -0.0033775608 -0.0033715763][-0.0033414874 -0.003336892 -0.0033361025 -0.0033362019 -0.0033369935 -0.0033386184 -0.0033421325 -0.0033478648 -0.0033547126 -0.0033623059 -0.0033688718 -0.0033734606 -0.0033747221 -0.0033722594 -0.0033667621][-0.003341828 -0.0033374771 -0.003337054 -0.0033370641 -0.0033376205 -0.003338825 -0.0033414946 -0.0033460159 -0.0033515529 -0.0033575336 -0.0033625653 -0.0033657809 -0.00336647 -0.0033643006 -0.0033598226][-0.0033421842 -0.0033377798 -0.0033376943 -0.0033377949 -0.0033381276 -0.0033388722 -0.0033406287 -0.0033436965 -0.0033476963 -0.0033520779 -0.0033554696 -0.0033573983 -0.0033575024 -0.0033556707 -0.0033522598][-0.0033422576 -0.0033377665 -0.0033379421 -0.0033382177 -0.0033385137 -0.0033390049 -0.0033401991 -0.0033422108 -0.0033447836 -0.0033475521 -0.0033494756 -0.0033504206 -0.0033501158 -0.0033486562 -0.0033462804][-0.0033421568 -0.003337488 -0.0033377258 -0.0033380257 -0.0033383952 -0.0033388161 -0.0033395921 -0.0033407938 -0.0033422457 -0.0033435978 -0.0033444448 -0.0033448332 -0.0033444073 -0.0033433938 -0.0033420024][-0.0033420569 -0.0033372082 -0.0033372843 -0.0033374887 -0.0033376664 -0.0033378641 -0.0033382033 -0.00333873 -0.0033393591 -0.0033399814 -0.0033403789 -0.003340512 -0.003340204 -0.0033396892 -0.0033390366]]...]
INFO - root - 2017-12-09 12:59:01.983872: step 26210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:06m:27s remains)
INFO - root - 2017-12-09 12:59:10.608362: step 26220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:48m:09s remains)
INFO - root - 2017-12-09 12:59:19.228428: step 26230, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:54m:35s remains)
INFO - root - 2017-12-09 12:59:27.972905: step 26240, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 76h:22m:44s remains)
INFO - root - 2017-12-09 12:59:36.643387: step 26250, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:29m:53s remains)
INFO - root - 2017-12-09 12:59:45.310593: step 26260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 74h:23m:24s remains)
INFO - root - 2017-12-09 12:59:54.002612: step 26270, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 74h:56m:54s remains)
INFO - root - 2017-12-09 13:00:02.756243: step 26280, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 73h:41m:15s remains)
INFO - root - 2017-12-09 13:00:11.491904: step 26290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:57m:42s remains)
INFO - root - 2017-12-09 13:00:20.135449: step 26300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:25m:10s remains)
2017-12-09 13:00:21.030367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0022013402 -0.0017201651 0.00034094695 0.0042062839 0.0099723991 0.016979564 0.023361843 0.026571322 0.025550772 0.021188833 0.015350816 0.0098354518 0.0056108953 0.0027819858 0.00088708475][-0.00084706489 -0.00015753252 0.0024945643 0.0076701893 0.015750661 0.025586277 0.034409579 0.038734239 0.0372092 0.031169094 0.023115542 0.015540156 0.009616252 0.0054706726 0.0026596654][0.0008760062 0.0022567753 0.0059156995 0.012756513 0.023441304 0.036267053 0.047603518 0.053158376 0.051257487 0.043809228 0.033962511 0.024814513 0.017468382 0.011998763 0.0078530191][0.0028009056 0.0054294104 0.010598225 0.019405156 0.032799859 0.048607852 0.062355541 0.06917493 0.067187466 0.05890223 0.048080578 0.038296495 0.03028148 0.023800962 0.018100724][0.0045915386 0.008647264 0.015581508 0.026392195 0.042147532 0.060598016 0.076665692 0.08512152 0.083853953 0.075762 0.065028407 0.055448338 0.047271974 0.039954331 0.03255577][0.0058283219 0.010967221 0.019269276 0.031523306 0.048773386 0.068949565 0.086851537 0.09724424 0.0977604 0.09128537 0.082036428 0.073680051 0.066034175 0.058161542 0.049002398][0.0062403688 0.011642169 0.020342171 0.032933086 0.050387379 0.071039848 0.0898753 0.10196072 0.10477562 0.10081332 0.09389925 0.08729247 0.080555774 0.072529428 0.06213963][0.0057412973 0.010473922 0.018351607 0.029907316 0.046055987 0.065585323 0.084013209 0.097026348 0.10199368 0.1007791 0.096387371 0.091475904 0.085586727 0.0776409 0.0667182][0.0042949812 0.0078439871 0.014022057 0.023405787 0.03690283 0.053736482 0.07023982 0.082868233 0.089083336 0.090079963 0.08785142 0.084331483 0.079134166 0.071528286 0.060951084][0.0021806359 0.00445466 0.0086028883 0.015302769 0.025401343 0.038431972 0.051704645 0.062561885 0.068768419 0.070972614 0.070260823 0.0678665 0.063423984 0.056607615 0.047292877][-1.2761913e-05 0.0012664902 0.0036440995 0.007760602 0.014381128 0.023298174 0.032736756 0.040869247 0.045953706 0.048281837 0.048290826 0.046698991 0.043195989 0.037745636 0.030520272][-0.0017637591 -0.0011173005 4.73354e-05 0.0021831542 0.0058520287 0.011083041 0.016884595 0.022127621 0.025601342 0.027365722 0.027514704 0.026451971 0.023949716 0.020125587 0.015287795][-0.002880574 -0.0025852742 -0.0020935205 -0.0011830218 0.00044776849 0.0029195887 0.0058482839 0.0086635938 0.010631929 0.0116732 0.011749123 0.011072611 0.00953455 0.0072755357 0.0045885043][-0.003330885 -0.0032529728 -0.0031011293 -0.0028013194 -0.0022426653 -0.0013521751 -0.00021956861 0.0009340113 0.001782693 0.0022337898 0.0022441461 0.0019006478 0.0011736988 0.00016514747 -0.000940728][-0.0033730543 -0.0033727246 -0.0033678145 -0.0033284067 -0.0032193521 -0.003009534 -0.0027120507 -0.0023871902 -0.0021351911 -0.0020100523 -0.0020254399 -0.0021474045 -0.0023653139 -0.0026417014 -0.0029180744]]...]
INFO - root - 2017-12-09 13:00:29.287384: step 26310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 71h:34m:49s remains)
INFO - root - 2017-12-09 13:00:37.942583: step 26320, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:36m:34s remains)
INFO - root - 2017-12-09 13:00:46.615318: step 26330, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:39m:54s remains)
INFO - root - 2017-12-09 13:00:55.258503: step 26340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:08m:18s remains)
INFO - root - 2017-12-09 13:01:03.832983: step 26350, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:32m:11s remains)
INFO - root - 2017-12-09 13:01:12.308137: step 26360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:35m:36s remains)
INFO - root - 2017-12-09 13:01:20.802856: step 26370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:57m:14s remains)
INFO - root - 2017-12-09 13:01:29.461247: step 26380, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 74h:13m:39s remains)
INFO - root - 2017-12-09 13:01:38.049555: step 26390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:46m:55s remains)
INFO - root - 2017-12-09 13:01:46.676596: step 26400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:13m:43s remains)
2017-12-09 13:01:47.620597: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14914647 0.14694872 0.14406916 0.13952568 0.13377103 0.12651171 0.11820517 0.10887127 0.098918818 0.088884808 0.078811489 0.069117226 0.059980921 0.051254433 0.042742915][0.15437841 0.15393086 0.15298396 0.15042442 0.14648266 0.14056005 0.13291867 0.12351516 0.11257595 0.10086349 0.08849249 0.076170944 0.064415321 0.053532466 0.04359689][0.1570539 0.15866749 0.15989989 0.1596573 0.15794009 0.15385175 0.14754403 0.13876942 0.1277609 0.11503273 0.1009236 0.086179242 0.071555443 0.057947673 0.045837488][0.15827571 0.16197141 0.16508089 0.16705775 0.16755731 0.16538514 0.16062158 0.15290159 0.14228785 0.12918232 0.11386593 0.097380571 0.08014816 0.06403669 0.049799953][0.1574043 0.16309413 0.16807574 0.1722393 0.17492637 0.17454609 0.17132607 0.16489412 0.15521099 0.14238417 0.12655503 0.10876976 0.089572005 0.071217842 0.054994706][0.15489241 0.16188584 0.16826102 0.17419665 0.17883134 0.18044339 0.17895882 0.17381966 0.16493703 0.15284786 0.1373836 0.11927957 0.099332124 0.079962924 0.062488504][0.15100081 0.15897167 0.16600938 0.1730545 0.17897512 0.18207404 0.18197456 0.17824009 0.17075686 0.15955189 0.14483085 0.12718752 0.10753172 0.088066436 0.070272043][0.14618896 0.15447575 0.16167323 0.16932531 0.17627616 0.18062076 0.18151957 0.17877565 0.17228295 0.16247939 0.14935169 0.13324262 0.11509836 0.096755259 0.0793856][0.14166598 0.14916602 0.15536885 0.16266333 0.16982967 0.17489468 0.17677198 0.17525908 0.1699442 0.16141285 0.14971448 0.13553245 0.11937655 0.10276697 0.086899944][0.13790959 0.1438553 0.14833155 0.1543808 0.1610755 0.16603442 0.16845594 0.16800234 0.16385356 0.15653503 0.14625786 0.13389406 0.11998808 0.10570201 0.091830865][0.13571282 0.14023018 0.1426937 0.14669847 0.15186842 0.15617362 0.15856005 0.15866467 0.15536773 0.1491695 0.14019299 0.12952127 0.1177496 0.10581555 0.0941608][0.13075869 0.13457136 0.13574281 0.13852718 0.1427685 0.14638005 0.14847916 0.14864698 0.14611755 0.14077182 0.13303743 0.12397707 0.11399084 0.10396488 0.094165429][0.127166 0.12970494 0.12917359 0.13003767 0.13275465 0.13547483 0.13767564 0.13823478 0.13669185 0.13250484 0.12600861 0.11851475 0.11014073 0.10201169 0.094006173][0.12327324 0.12481438 0.1228264 0.12212195 0.12313956 0.12458071 0.12612267 0.12680468 0.12641393 0.12352349 0.11863383 0.11278713 0.10598835 0.0992525 0.092508487][0.11778582 0.11937579 0.11714721 0.11586386 0.11600027 0.11655573 0.11731292 0.11727607 0.11673284 0.11434222 0.11044478 0.10592429 0.10074661 0.095612876 0.090324774]]...]
INFO - root - 2017-12-09 13:01:55.977631: step 26410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:41m:45s remains)
INFO - root - 2017-12-09 13:02:04.501178: step 26420, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 73h:03m:10s remains)
INFO - root - 2017-12-09 13:02:13.022605: step 26430, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 71h:26m:55s remains)
INFO - root - 2017-12-09 13:02:21.751043: step 26440, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:10m:50s remains)
INFO - root - 2017-12-09 13:02:30.506309: step 26450, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.922 sec/batch; 78h:21m:17s remains)
INFO - root - 2017-12-09 13:02:39.302545: step 26460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:57m:30s remains)
INFO - root - 2017-12-09 13:02:47.797355: step 26470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 71h:03m:01s remains)
INFO - root - 2017-12-09 13:02:56.412820: step 26480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:12m:58s remains)
INFO - root - 2017-12-09 13:03:04.925297: step 26490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:56m:53s remains)
INFO - root - 2017-12-09 13:03:13.539266: step 26500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:28m:52s remains)
2017-12-09 13:03:14.374684: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036090419 0.036244437 0.042578436 0.053338252 0.06953083 0.085911959 0.10195716 0.11082448 0.11515729 0.11072833 0.10440738 0.096990742 0.095217071 0.10349105 0.12220339][0.03901438 0.040512938 0.049352497 0.06598863 0.089708053 0.11519234 0.13917638 0.1551182 0.1638585 0.16022232 0.15223077 0.14354709 0.14106441 0.14853039 0.16633075][0.042214468 0.045762159 0.0584647 0.082503267 0.11488917 0.15045767 0.18228839 0.20707016 0.22185688 0.22158934 0.2146704 0.20485385 0.20151745 0.2063736 0.22014152][0.044402957 0.049968448 0.067578517 0.099452958 0.14101245 0.18743294 0.22811007 0.26148248 0.28192517 0.28790802 0.28449741 0.27617836 0.27333531 0.27483636 0.2812542][0.048614454 0.055646911 0.076234795 0.11513158 0.16541409 0.22195444 0.27288085 0.31602904 0.3445048 0.35682157 0.35696477 0.35090429 0.34671044 0.34468487 0.34303871][0.05215358 0.060288012 0.083937041 0.12785818 0.18410644 0.24908455 0.30836919 0.36091331 0.39609471 0.41627863 0.42186561 0.4188647 0.41287646 0.40725586 0.39855132][0.054486938 0.064721607 0.091446593 0.13895492 0.19974463 0.26892772 0.33276507 0.39039576 0.43019745 0.45667464 0.46695542 0.46789831 0.46146381 0.45333213 0.4387646][0.054291639 0.067952648 0.0984417 0.14826956 0.21106628 0.28205472 0.34804523 0.40705013 0.44809437 0.47646454 0.48913503 0.49273333 0.48632696 0.47676608 0.460003][0.056256149 0.073558211 0.1074819 0.15871289 0.22189905 0.2924127 0.35732225 0.41409594 0.45315221 0.48025048 0.4917298 0.49571928 0.48892254 0.47972468 0.46337396][0.06152086 0.083478935 0.12063611 0.17286904 0.23510742 0.30221617 0.36299878 0.41501194 0.45038804 0.47270831 0.48138911 0.48377216 0.47580761 0.46632829 0.45162535][0.067760281 0.094974533 0.13592865 0.18896607 0.24835853 0.31099245 0.36513492 0.40983212 0.43873328 0.45581254 0.46134412 0.46171179 0.45407856 0.44526649 0.43316492][0.073186465 0.10598524 0.15005349 0.20335813 0.25938356 0.31544638 0.36160767 0.39856938 0.42068309 0.43229663 0.43438271 0.43307388 0.42602202 0.41846678 0.40920165][0.078846 0.11578934 0.16161056 0.21368805 0.2651825 0.3148438 0.35308221 0.38206017 0.3978695 0.40471727 0.40374872 0.40080425 0.39486375 0.3896428 0.3836019][0.0829144 0.12138507 0.16663347 0.21647651 0.2631782 0.30575359 0.33686244 0.35890648 0.36950457 0.37295353 0.37022933 0.36624891 0.36099395 0.35822874 0.35502374][0.082374 0.11989143 0.16227703 0.20785333 0.24896623 0.28468621 0.3100526 0.32717663 0.33496958 0.33659881 0.33365464 0.32949951 0.32455394 0.32250065 0.32052439]]...]
INFO - root - 2017-12-09 13:03:22.650881: step 26510, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 67h:51m:31s remains)
INFO - root - 2017-12-09 13:03:31.184746: step 26520, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:37m:53s remains)
INFO - root - 2017-12-09 13:03:39.942226: step 26530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:09m:55s remains)
INFO - root - 2017-12-09 13:03:48.547946: step 26540, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:40m:49s remains)
INFO - root - 2017-12-09 13:03:57.336673: step 26550, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:17m:13s remains)
INFO - root - 2017-12-09 13:04:06.078597: step 26560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:36m:25s remains)
INFO - root - 2017-12-09 13:04:14.576569: step 26570, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 74h:04m:26s remains)
INFO - root - 2017-12-09 13:04:23.109271: step 26580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:42m:32s remains)
INFO - root - 2017-12-09 13:04:31.534695: step 26590, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 73h:31m:21s remains)
INFO - root - 2017-12-09 13:04:40.171757: step 26600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:39m:47s remains)
2017-12-09 13:04:41.063042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033926519 -0.0033921038 -0.0033918102 -0.003386691 -0.0033686636 -0.0033404359 -0.0033123039 -0.0033041132 -0.0033137079 -0.0033386031 -0.0033632373 -0.0033800802 -0.0033882251 -0.0033911355 -0.0033917727][-0.003390074 -0.0033892714 -0.0033829371 -0.0033539678 -0.0032813153 -0.0031801178 -0.0030921353 -0.0030695626 -0.0031123839 -0.0031993571 -0.0032853754 -0.0033450306 -0.0033753167 -0.0033865087 -0.0033890007][-0.0033896517 -0.0033867008 -0.003354504 -0.0032450124 -0.0030154646 -0.0027288145 -0.0025063553 -0.0024633878 -0.0026026163 -0.0028426074 -0.0030802395 -0.0032485698 -0.0033396939 -0.0033769431 -0.0033874381][-0.0033880419 -0.0033760041 -0.0032750312 -0.0029803356 -0.0024297396 -0.001801122 -0.0013557028 -0.0012956825 -0.0016194158 -0.0021363883 -0.0026569574 -0.0030369642 -0.0032550863 -0.0033513987 -0.0033822875][-0.0033843005 -0.0033514309 -0.0031259388 -0.0025317175 -0.0015112713 -0.00042911014 0.00028367736 0.00034097233 -0.00023092912 -0.0011111295 -0.0020166999 -0.0027018846 -0.0031139888 -0.003305051 -0.0033706904][-0.0033786586 -0.003316578 -0.0029436236 -0.0020256978 -0.00054226303 0.00094206329 0.0018620703 0.0018859918 0.0010884872 -0.00011680136 -0.0013773048 -0.0023558727 -0.0029634135 -0.0032531724 -0.003355321][-0.0033748236 -0.0032918719 -0.0028259389 -0.0017283835 -2.5683548e-05 0.0016091892 0.0025766522 0.0025488695 0.0016487075 0.00030552433 -0.0011000531 -0.0022000358 -0.0028906702 -0.0032252318 -0.0033445305][-0.0033767484 -0.0032970374 -0.0028561079 -0.0018420349 -0.00031037256 0.0011194476 0.0019373146 0.0018746064 0.0010587133 -0.00014904281 -0.0013964374 -0.0023610839 -0.0029583299 -0.0032452156 -0.0033468108][-0.0033836477 -0.0033288316 -0.0030147897 -0.0022959784 -0.0012234347 -0.00023706304 0.00030993228 0.00023518084 -0.0003559473 -0.0012155499 -0.0020799888 -0.0027293232 -0.0031176596 -0.0032994507 -0.0033626095][-0.0033897064 -0.0033629113 -0.0031977762 -0.0028152154 -0.0022441358 -0.0017193243 -0.0014360414 -0.0014967103 -0.0018314928 -0.0023029018 -0.0027576629 -0.0030838333 -0.0032690112 -0.0033517941 -0.0033796791][-0.0033923257 -0.0033838337 -0.0033241212 -0.0031792128 -0.0029582055 -0.002751821 -0.0026437254 -0.0026784071 -0.0028207549 -0.003009741 -0.0031815928 -0.0032972512 -0.0033580144 -0.0033830521 -0.0033905073][-0.0033922507 -0.0033904992 -0.0033772832 -0.0033414713 -0.00328481 -0.0032306062 -0.0032038866 -0.0032176685 -0.0032593044 -0.0033101183 -0.00335275 -0.0033789338 -0.0033911115 -0.0033949774 -0.0033950827][-0.0033919369 -0.00339112 -0.003389623 -0.0033846484 -0.0033768325 -0.003369529 -0.0033674133 -0.0033719551 -0.0033796383 -0.0033872325 -0.003392783 -0.0033956349 -0.0033965078 -0.0033962822 -0.0033954121][-0.0033925017 -0.0033906302 -0.0033894964 -0.0033883115 -0.0033877315 -0.0033879387 -0.0033893215 -0.0033911872 -0.0033926447 -0.0033935525 -0.003394142 -0.0033944172 -0.0033943637 -0.0033940345 -0.0033934563][-0.0033933243 -0.0033907087 -0.0033890672 -0.0033877923 -0.0033872253 -0.0033871923 -0.0033878477 -0.0033889217 -0.003389942 -0.0033907441 -0.0033913217 -0.0033917641 -0.00339206 -0.0033922619 -0.0033922179]]...]
INFO - root - 2017-12-09 13:04:49.662980: step 26610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:15m:10s remains)
INFO - root - 2017-12-09 13:04:58.194693: step 26620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:57m:01s remains)
INFO - root - 2017-12-09 13:05:06.820052: step 26630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:21m:17s remains)
INFO - root - 2017-12-09 13:05:15.447643: step 26640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:15m:58s remains)
INFO - root - 2017-12-09 13:05:24.146390: step 26650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 72h:45m:56s remains)
INFO - root - 2017-12-09 13:05:32.757588: step 26660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:44m:30s remains)
INFO - root - 2017-12-09 13:05:41.236529: step 26670, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 71h:23m:40s remains)
INFO - root - 2017-12-09 13:05:49.795641: step 26680, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 73h:23m:18s remains)
INFO - root - 2017-12-09 13:05:58.342801: step 26690, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 73h:12m:34s remains)
INFO - root - 2017-12-09 13:06:06.866932: step 26700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:52m:46s remains)
2017-12-09 13:06:07.720693: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024461925 0.024651013 0.024457654 0.024301356 0.024310818 0.024267696 0.024053959 0.023756837 0.023406807 0.02284511 0.02192452 0.020442294 0.018340649 0.015517673 0.011967771][0.033075534 0.034204856 0.034485977 0.034627046 0.034789603 0.034717873 0.034372397 0.033889513 0.033411223 0.032851677 0.031926334 0.030410387 0.027994104 0.024570186 0.020157315][0.043805357 0.046158597 0.047060426 0.047352724 0.047359347 0.047133554 0.046487972 0.045779459 0.045155469 0.044451073 0.043381579 0.041479707 0.038592864 0.03463101 0.029368639][0.054048084 0.05757238 0.058908824 0.059142452 0.058891017 0.058350366 0.057374705 0.056573987 0.056056742 0.055523086 0.054536942 0.0524366 0.049037121 0.044300448 0.038131718][0.061299298 0.065795861 0.067546308 0.0675528 0.06691879 0.066119112 0.064966522 0.0640894 0.063713208 0.063427173 0.0626342 0.060373638 0.05667169 0.051408172 0.044559784][0.065461606 0.07024619 0.072059952 0.07173875 0.07065203 0.069414333 0.068202063 0.067320108 0.066950344 0.066865847 0.066238813 0.063948423 0.060092095 0.054547433 0.047378037][0.0677863 0.072367914 0.073672965 0.072816133 0.071139522 0.069470644 0.068067871 0.0671962 0.06702061 0.0670611 0.066444837 0.0641298 0.060104776 0.054277733 0.046923589][0.068951845 0.073165 0.073803633 0.072064906 0.069586575 0.0673667 0.065620758 0.064550936 0.06434688 0.064618312 0.0641338 0.061723959 0.057475265 0.051397584 0.043958206][0.068400711 0.072170943 0.071825251 0.069223709 0.06587854 0.062911667 0.060640391 0.059289221 0.059229244 0.059644084 0.059268672 0.056933228 0.052593216 0.046393916 0.039078351][0.065982535 0.069082052 0.067644238 0.063982971 0.059813555 0.056235008 0.05350437 0.051847897 0.051827483 0.052505169 0.052361324 0.050011948 0.045608327 0.039628938 0.032726578][0.061524179 0.063772127 0.061346147 0.056982316 0.052265633 0.048256218 0.045164056 0.043386567 0.043406334 0.044150054 0.04407135 0.041771051 0.0375693 0.032000247 0.025855584][0.054895692 0.056531124 0.05382847 0.049390607 0.044778109 0.040830333 0.03769087 0.03587652 0.035766385 0.036410142 0.036150735 0.033796258 0.029804688 0.024777295 0.019456783][0.046272442 0.047611527 0.045275323 0.041517183 0.037772927 0.034491371 0.031730682 0.030046243 0.029773261 0.030137759 0.029541157 0.027105091 0.02340069 0.018975068 0.014413764][0.036380738 0.037316289 0.035485867 0.032678969 0.030034646 0.02780436 0.025887584 0.024666764 0.024428628 0.02454 0.023736184 0.021394528 0.018096859 0.014328226 0.010480649][0.02655993 0.026987934 0.025448628 0.023405479 0.021771252 0.020612929 0.019692264 0.019237684 0.019366482 0.01953616 0.018788032 0.016785778 0.014043645 0.010962741 0.0077920444]]...]
INFO - root - 2017-12-09 13:06:16.218022: step 26710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:13m:05s remains)
INFO - root - 2017-12-09 13:06:24.690960: step 26720, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 72h:15m:47s remains)
INFO - root - 2017-12-09 13:06:33.266541: step 26730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:32m:09s remains)
INFO - root - 2017-12-09 13:06:42.029913: step 26740, loss = 0.90, batch loss = 0.70 (8.9 examples/sec; 0.895 sec/batch; 76h:01m:00s remains)
INFO - root - 2017-12-09 13:06:50.796682: step 26750, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 73h:47m:04s remains)
INFO - root - 2017-12-09 13:06:59.546478: step 26760, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:37m:39s remains)
INFO - root - 2017-12-09 13:07:08.207170: step 26770, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 65h:36m:01s remains)
INFO - root - 2017-12-09 13:07:16.946068: step 26780, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 71h:41m:03s remains)
INFO - root - 2017-12-09 13:07:25.660446: step 26790, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 75h:13m:26s remains)
INFO - root - 2017-12-09 13:07:34.405906: step 26800, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:06m:00s remains)
2017-12-09 13:07:35.251250: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47807607 0.51289958 0.54320312 0.56626695 0.57946384 0.58635271 0.58907062 0.59033364 0.59223443 0.59196216 0.59169924 0.59217215 0.59539545 0.595877 0.596767][0.48611721 0.5264017 0.56091321 0.58735389 0.60370946 0.61314994 0.61818928 0.62266922 0.62607092 0.62603211 0.62435353 0.62174308 0.62175786 0.61824459 0.6149841][0.47373754 0.52257931 0.56509781 0.59839648 0.61947888 0.63226163 0.63974887 0.64485091 0.64835674 0.64752138 0.64392692 0.63737357 0.63343966 0.62609 0.61842763][0.45420998 0.51007593 0.56319916 0.60592318 0.63364756 0.65216607 0.66220152 0.66810244 0.66944289 0.66471416 0.65660661 0.64496291 0.63571048 0.62307507 0.61023951][0.43381593 0.49576345 0.55481035 0.60702592 0.64515185 0.66939479 0.68375951 0.69009817 0.68971556 0.68093461 0.66595936 0.64667785 0.629241 0.61060768 0.59121966][0.41842633 0.48671088 0.55051649 0.60704595 0.6500749 0.68164617 0.69916254 0.70580059 0.70418876 0.69090527 0.67015058 0.64214385 0.615819 0.58934575 0.56249714][0.40538886 0.47778192 0.54644632 0.60725009 0.65385377 0.6865018 0.70504171 0.71052539 0.70506561 0.68715578 0.6598866 0.62522525 0.59127116 0.55750817 0.52472472][0.3914054 0.46577069 0.53557307 0.59900224 0.64718485 0.6805557 0.69810355 0.69973636 0.69045752 0.66714615 0.63484907 0.59493005 0.55525333 0.51735735 0.481399][0.38047621 0.45300296 0.52150649 0.58444726 0.63253373 0.66616619 0.68330026 0.68376905 0.67208362 0.64535654 0.60837948 0.56421924 0.52100372 0.48056477 0.44299349][0.3742229 0.44450885 0.50883085 0.56914896 0.61487162 0.64707923 0.66313916 0.66387588 0.65211588 0.62450749 0.58697021 0.54116803 0.4965778 0.45607829 0.41975418][0.36420307 0.43423358 0.49792838 0.55415225 0.59566993 0.62547916 0.64036816 0.64040565 0.62828618 0.60240835 0.56696314 0.52348781 0.48196292 0.44374594 0.41128126][0.35103938 0.42030472 0.48286679 0.5391174 0.58040577 0.60674113 0.61949408 0.61876172 0.60603297 0.58080459 0.54755992 0.5087477 0.47256312 0.440462 0.41481802][0.33550733 0.40389425 0.46494195 0.52029866 0.56191486 0.58972061 0.60252434 0.601911 0.59104943 0.56844634 0.53819531 0.50338024 0.47279066 0.44654027 0.42704386][0.31947693 0.38550547 0.44478169 0.49951994 0.54184806 0.57065743 0.5855127 0.58769238 0.57948047 0.56070471 0.53533596 0.50624967 0.48167506 0.46155271 0.44744956][0.30560693 0.36885503 0.42584005 0.47835782 0.51966763 0.5493052 0.56502396 0.57004935 0.56581241 0.5518114 0.53230494 0.509933 0.49189609 0.4776924 0.46863544]]...]
INFO - root - 2017-12-09 13:07:43.864894: step 26810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:14m:24s remains)
INFO - root - 2017-12-09 13:07:52.417623: step 26820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:23m:45s remains)
INFO - root - 2017-12-09 13:08:01.094828: step 26830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:14m:11s remains)
INFO - root - 2017-12-09 13:08:09.831754: step 26840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:23m:08s remains)
INFO - root - 2017-12-09 13:08:18.481886: step 26850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:53m:27s remains)
INFO - root - 2017-12-09 13:08:27.092835: step 26860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:06m:15s remains)
INFO - root - 2017-12-09 13:08:35.724698: step 26870, loss = 0.89, batch loss = 0.68 (11.0 examples/sec; 0.730 sec/batch; 61h:57m:28s remains)
INFO - root - 2017-12-09 13:08:44.438605: step 26880, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 72h:06m:42s remains)
INFO - root - 2017-12-09 13:08:53.212732: step 26890, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 74h:19m:24s remains)
INFO - root - 2017-12-09 13:09:01.892770: step 26900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 73h:01m:06s remains)
2017-12-09 13:09:02.902419: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03838608 0.040891703 0.041993588 0.042520702 0.042423561 0.039987147 0.036516782 0.0315383 0.027158575 0.023248283 0.019828603 0.017783482 0.016378874 0.015641591 0.014769445][0.041259088 0.0437455 0.04439548 0.046052314 0.046673428 0.045402978 0.043213896 0.038594853 0.034511045 0.029678568 0.025597053 0.022636322 0.02043773 0.019396886 0.018042402][0.045900986 0.048190754 0.049041856 0.050866019 0.051781479 0.051113714 0.049000364 0.045122661 0.041134164 0.036177605 0.03131019 0.02699372 0.023757728 0.021331832 0.018995902][0.052337296 0.055405736 0.057172354 0.060358718 0.062850982 0.063963272 0.063443847 0.060233615 0.056665841 0.051289372 0.045060221 0.039053541 0.034053896 0.030448712 0.027073389][0.068789512 0.072348088 0.075663626 0.07996849 0.083638221 0.086204566 0.087033406 0.084871508 0.081895679 0.076646015 0.070124954 0.063165754 0.056913741 0.052164737 0.047740765][0.098537616 0.10266761 0.10672437 0.11155976 0.11580726 0.11905592 0.12052712 0.11924883 0.11622169 0.11095406 0.10447931 0.097185537 0.090358429 0.084721744 0.079788819][0.13981412 0.14436591 0.14825313 0.15237504 0.1558723 0.15856913 0.15961015 0.15864922 0.15580918 0.15083489 0.1447278 0.13774282 0.13084549 0.12464135 0.11935133][0.18301506 0.18764937 0.19104011 0.19419424 0.19651443 0.19820808 0.19854315 0.19791882 0.19550383 0.19065107 0.18475148 0.17774688 0.17042015 0.16360784 0.15774672][0.21498311 0.22041173 0.22380699 0.22685999 0.22910461 0.23093545 0.23068428 0.22958602 0.22693604 0.22238067 0.21632802 0.20901144 0.20207459 0.19551547 0.18975711][0.23120376 0.23729914 0.24056743 0.24371268 0.24607608 0.24828501 0.24834584 0.24752651 0.24452458 0.23993474 0.23404928 0.22682573 0.22030222 0.21436253 0.20984092][0.23238049 0.23979846 0.24337105 0.24652968 0.24883626 0.25079316 0.25091285 0.24991675 0.24677716 0.24241033 0.23712066 0.23109919 0.22615005 0.22206104 0.21943764][0.22428603 0.23249042 0.23570193 0.23870918 0.24082869 0.2421512 0.2420606 0.24099809 0.23788281 0.23355141 0.22880292 0.22413005 0.22077812 0.21853237 0.2178183][0.21059965 0.219678 0.2222489 0.22438368 0.22564101 0.22594033 0.22527917 0.22363263 0.22073865 0.21691331 0.21301644 0.20974343 0.20771678 0.20679413 0.20741503][0.19798206 0.20726061 0.20874646 0.20953132 0.2091614 0.20812133 0.20676817 0.20488292 0.20239374 0.19910184 0.19619805 0.19390072 0.19251548 0.19207482 0.19305667][0.18176462 0.18972549 0.18948381 0.18877679 0.18715487 0.18511723 0.18324241 0.18098436 0.17867914 0.17642412 0.17458059 0.17301734 0.17204076 0.17195795 0.17281395]]...]
INFO - root - 2017-12-09 13:09:11.652571: step 26910, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:25m:34s remains)
INFO - root - 2017-12-09 13:09:20.152332: step 26920, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.864 sec/batch; 73h:22m:22s remains)
INFO - root - 2017-12-09 13:09:28.814071: step 26930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:49m:24s remains)
INFO - root - 2017-12-09 13:09:37.428232: step 26940, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 69h:24m:18s remains)
INFO - root - 2017-12-09 13:09:46.056956: step 26950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:57m:46s remains)
INFO - root - 2017-12-09 13:09:54.739827: step 26960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 72h:22m:07s remains)
INFO - root - 2017-12-09 13:10:03.355357: step 26970, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.743 sec/batch; 63h:02m:33s remains)
INFO - root - 2017-12-09 13:10:11.959894: step 26980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:04m:31s remains)
INFO - root - 2017-12-09 13:10:20.511756: step 26990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:22m:15s remains)
INFO - root - 2017-12-09 13:10:29.080611: step 27000, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 73h:31m:18s remains)
2017-12-09 13:10:30.022513: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13552117 0.13050038 0.13204095 0.14013934 0.15409708 0.17123297 0.18680122 0.19543561 0.19307593 0.17727226 0.14923158 0.11322453 0.076267041 0.0446334 0.021257209][0.13683781 0.13346413 0.13684195 0.147093 0.16251144 0.18091023 0.19694334 0.20572105 0.20278907 0.18595877 0.1566177 0.11903796 0.080588296 0.047481753 0.02298655][0.13822752 0.13703907 0.14244395 0.15466657 0.17154589 0.19039193 0.20564651 0.2129837 0.20852843 0.19084817 0.16092348 0.12292636 0.083953671 0.050212387 0.025016595][0.15023866 0.15191343 0.15905258 0.17222926 0.1889748 0.20710978 0.22150305 0.22775853 0.22195455 0.20302066 0.17154157 0.13183394 0.090754971 0.054945242 0.028035711][0.1674614 0.17299789 0.18190196 0.19541654 0.21122324 0.22784549 0.24058408 0.24537364 0.23820534 0.21786335 0.18460837 0.14271384 0.098972179 0.060546711 0.031471346][0.18096349 0.18981186 0.20018646 0.21350929 0.22794449 0.24248561 0.253118 0.25614938 0.24736965 0.2256254 0.1910893 0.14797306 0.10281569 0.0630642 0.032988884][0.18036443 0.19187972 0.20322347 0.21582618 0.22857031 0.24066308 0.24904498 0.25031245 0.24006161 0.21747929 0.18305865 0.14104939 0.097380623 0.059238821 0.030676614][0.1610837 0.17340693 0.18484321 0.19655725 0.20774531 0.21748602 0.22358215 0.22304267 0.21195838 0.18992075 0.15782994 0.12000225 0.081524849 0.048584908 0.024352806][0.12733352 0.13875589 0.14915058 0.15937793 0.16895461 0.1765874 0.18061224 0.1784932 0.16749856 0.14764509 0.12018628 0.089180976 0.058784235 0.033688508 0.015720019][0.0889139 0.097853623 0.10604454 0.11404881 0.12153558 0.12714466 0.12948281 0.12653393 0.11675549 0.1004803 0.079278193 0.056531921 0.03536493 0.018770896 0.0073717283][0.054927971 0.06078694 0.06626866 0.071560875 0.0764515 0.079983123 0.081001513 0.078142174 0.070705853 0.059098922 0.044802982 0.030205246 0.017394198 0.0078701964 0.001653689][0.028767262 0.032128755 0.03525627 0.038238481 0.040917415 0.042711329 0.042801023 0.040578689 0.035733316 0.028629471 0.02041037 0.012505468 0.0059729386 0.001352373 -0.0014591755][0.010397324 0.012116954 0.013764892 0.01520418 0.016414931 0.017140364 0.01690691 0.015545223 0.01294677 0.0095284209 0.0058099087 0.0024516447 -0.00014142133 -0.0018643735 -0.0028027836][9.8097604e-05 0.00080327364 0.0015448625 0.0021557938 0.0026472732 0.0029223235 0.0027853413 0.0022003581 0.0011947462 3.2351818e-05 -0.0010988533 -0.0020322623 -0.0026921486 -0.003075442 -0.0032398938][-0.0032215295 -0.0031054101 -0.0029346212 -0.0027512335 -0.0025855287 -0.0024506275 -0.0024037031 -0.0024683145 -0.00262469 -0.0028064256 -0.0029730082 -0.0031119424 -0.0032058386 -0.0032532734 -0.0032729025]]...]
INFO - root - 2017-12-09 13:10:38.443019: step 27010, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 73h:10m:08s remains)
INFO - root - 2017-12-09 13:10:47.035196: step 27020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:45m:25s remains)
INFO - root - 2017-12-09 13:10:55.638366: step 27030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:24m:16s remains)
INFO - root - 2017-12-09 13:11:04.217186: step 27040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:14m:45s remains)
INFO - root - 2017-12-09 13:11:12.838983: step 27050, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 70h:40m:43s remains)
INFO - root - 2017-12-09 13:11:21.481570: step 27060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:13m:30s remains)
INFO - root - 2017-12-09 13:11:30.159500: step 27070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:34m:12s remains)
INFO - root - 2017-12-09 13:11:38.767818: step 27080, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 74h:24m:36s remains)
INFO - root - 2017-12-09 13:11:47.466733: step 27090, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 76h:20m:06s remains)
INFO - root - 2017-12-09 13:11:56.128742: step 27100, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 74h:00m:55s remains)
2017-12-09 13:11:57.095500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00092051551 -0.0011222453 -0.0013757823 -0.0015072953 -0.001607489 -0.0018333247 -0.0021378286 -0.0024473143 -0.00278983 -0.0031537837 -0.0033739065 -0.0033733179 -0.0033429072 -0.0032691241 -0.0031394204][-0.00070403609 -0.00068371859 -0.00075742346 -0.00078245369 -0.00090554729 -0.0012606659 -0.0017532759 -0.0022144318 -0.0026216037 -0.0030471089 -0.0033447372 -0.0033667241 -0.0033739263 -0.0033747295 -0.0033753514][-3.6393059e-05 0.00082277413 0.0014550742 0.0016178186 0.0010078773 0.0002152354 -0.00015171338 -0.0003325257 -0.00027543469 -0.00031164917 -0.00055280095 -0.0012565001 -0.0020578466 -0.0028514103 -0.0033545375][0.00093460083 0.0021565682 0.0032131341 0.0036862628 0.0033661411 0.0034603192 0.0043454706 0.0053110039 0.0063720392 0.0068032825 0.006510688 0.004901126 0.0020903246 -0.00079134991 -0.0028610865][0.0016424493 0.0039801919 0.006570762 0.0084233312 0.0090481993 0.00955375 0.010405937 0.011001766 0.011803144 0.012469539 0.011992246 0.0095883319 0.0058402754 0.0017317273 -0.0014294256][0.0047562215 0.0079780873 0.011203371 0.013349332 0.013757514 0.013552271 0.013640147 0.013522953 0.0142674 0.014864149 0.014252285 0.011454467 0.0072502624 0.0026242677 -0.0010669308][0.0052475017 0.0086809834 0.012203902 0.01479765 0.015392689 0.015383594 0.015475785 0.015031517 0.015530106 0.01615108 0.015965888 0.013670303 0.0096168583 0.0047696196 0.00016923598][0.0055412957 0.0090115173 0.012634948 0.015360752 0.016355472 0.016804771 0.017139748 0.016615164 0.01674046 0.017380394 0.017148169 0.014622141 0.010270503 0.0052943192 0.00049711578][0.0053581726 0.0080571333 0.01114578 0.013817485 0.015657635 0.017821094 0.019023033 0.018679364 0.018721126 0.019146711 0.019147938 0.017503949 0.013778538 0.0091580516 0.0037316303][0.0062324796 0.0090371007 0.012214255 0.015265225 0.017592372 0.01928092 0.019931281 0.018997705 0.018291162 0.017696753 0.01737456 0.015239585 0.012359004 0.00958487 0.0059302151][0.0092735393 0.0104454 0.01132705 0.012428075 0.013852755 0.017101526 0.020880993 0.023892624 0.026920006 0.028157666 0.028281357 0.025018653 0.01960336 0.014144963 0.0082113259][0.012321776 0.01342757 0.013935786 0.015677696 0.019348649 0.026841925 0.0365907 0.0467221 0.05453743 0.0581827 0.058074262 0.051666602 0.040841132 0.029608194 0.018887112][0.024270803 0.026239643 0.027538605 0.031015672 0.03832759 0.051364876 0.067911752 0.084792629 0.0987712 0.10658261 0.1068116 0.097274862 0.080290169 0.06182912 0.044951547][0.052158821 0.055248585 0.057003256 0.062496759 0.0738247 0.093141 0.1175101 0.14241678 0.16335168 0.17517501 0.17666861 0.16534021 0.14351627 0.11839169 0.094635874][0.10062772 0.10651469 0.10938969 0.11696278 0.13188443 0.15594265 0.18625987 0.21724516 0.24396139 0.25885898 0.26036236 0.24695009 0.22056708 0.18951759 0.16104406]]...]
INFO - root - 2017-12-09 13:12:05.492525: step 27110, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:13m:46s remains)
INFO - root - 2017-12-09 13:12:14.052103: step 27120, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 75h:56m:00s remains)
INFO - root - 2017-12-09 13:12:22.644177: step 27130, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 74h:26m:49s remains)
INFO - root - 2017-12-09 13:12:31.254673: step 27140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:32m:14s remains)
INFO - root - 2017-12-09 13:12:39.882123: step 27150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 71h:17m:20s remains)
INFO - root - 2017-12-09 13:12:48.617558: step 27160, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 81h:56m:30s remains)
INFO - root - 2017-12-09 13:12:57.325711: step 27170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 75h:37m:31s remains)
INFO - root - 2017-12-09 13:13:05.804020: step 27180, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:33m:34s remains)
INFO - root - 2017-12-09 13:13:14.452665: step 27190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:40m:14s remains)
INFO - root - 2017-12-09 13:13:22.964263: step 27200, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:15m:25s remains)
2017-12-09 13:13:23.807469: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0997119 0.10450566 0.10807712 0.11053549 0.1116713 0.11145335 0.10964658 0.10576222 0.099900641 0.092555419 0.084571771 0.075728342 0.066633135 0.057446726 0.048543345][0.10697599 0.11608216 0.12386233 0.12997505 0.13366058 0.13472778 0.13300827 0.12783164 0.11931999 0.10799146 0.095482185 0.082103737 0.069189556 0.057066329 0.046090398][0.11192767 0.12586811 0.13845989 0.14875439 0.15534449 0.15791059 0.15614417 0.149587 0.13856888 0.12337751 0.1062133 0.0879436 0.070753872 0.055436719 0.042587481][0.114196 0.13226253 0.14935991 0.16381907 0.17338228 0.17779511 0.17630792 0.16888234 0.15572906 0.137369 0.11663392 0.094547234 0.074282631 0.05677681 0.042917352][0.11483702 0.13601139 0.156537 0.17414203 0.18616077 0.19247392 0.19174436 0.18440136 0.17035571 0.15064192 0.12804976 0.10410337 0.082495816 0.064297952 0.050645024][0.11393518 0.13749078 0.16059774 0.18058202 0.19449857 0.20222741 0.20241755 0.1958527 0.18224847 0.16315103 0.14082912 0.11730007 0.096335053 0.079071194 0.066791572][0.11100244 0.13598859 0.16085014 0.18262792 0.19825515 0.20738047 0.20881209 0.20348072 0.19132796 0.1740348 0.1535528 0.13215727 0.11338232 0.098497689 0.088534981][0.10601161 0.13140082 0.15685067 0.17955968 0.19651064 0.20702949 0.21017602 0.20677969 0.19699246 0.18238953 0.16450197 0.14597207 0.13001746 0.11803838 0.11060731][0.098755568 0.12341215 0.1483689 0.17106968 0.18867564 0.20025787 0.20519435 0.20416546 0.19743007 0.18610388 0.17147788 0.15616795 0.14310378 0.13389057 0.12857553][0.089431576 0.11237198 0.13586357 0.15773794 0.17531124 0.18747228 0.19388393 0.19523108 0.19174027 0.18395233 0.17277008 0.16064742 0.15005925 0.14279816 0.13883024][0.078232989 0.098580167 0.11986736 0.1402936 0.15736461 0.16971579 0.17724296 0.18057522 0.17988613 0.17529938 0.16747487 0.15833616 0.14992222 0.14396843 0.14073634][0.067161083 0.084082171 0.10216172 0.12014363 0.13593647 0.14811486 0.15647194 0.16133803 0.16280213 0.1608184 0.15582111 0.14934251 0.14290568 0.13792098 0.13484454][0.056946836 0.070224956 0.08459872 0.09938059 0.11302553 0.12417271 0.13256913 0.13826475 0.14128086 0.14134073 0.13859892 0.13416743 0.12922207 0.12495523 0.12182679][0.047799408 0.057445183 0.067991346 0.079196133 0.090092339 0.099526428 0.10714431 0.11282779 0.11658009 0.11802053 0.1170805 0.11429763 0.11055684 0.10677323 0.10342853][0.040202614 0.046572402 0.05343008 0.061047472 0.068859786 0.076031491 0.082208671 0.08718162 0.090869665 0.092853986 0.0929564 0.091298535 0.088604942 0.085493952 0.082364328]]...]
INFO - root - 2017-12-09 13:13:32.430570: step 27210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:57m:14s remains)
INFO - root - 2017-12-09 13:13:40.983188: step 27220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:53m:54s remains)
INFO - root - 2017-12-09 13:13:49.648453: step 27230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:04m:57s remains)
INFO - root - 2017-12-09 13:13:58.406766: step 27240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:50m:31s remains)
INFO - root - 2017-12-09 13:14:06.932602: step 27250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 70h:51m:37s remains)
INFO - root - 2017-12-09 13:14:15.545684: step 27260, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:57m:15s remains)
INFO - root - 2017-12-09 13:14:24.177680: step 27270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:52m:15s remains)
INFO - root - 2017-12-09 13:14:32.629371: step 27280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:47m:12s remains)
INFO - root - 2017-12-09 13:14:41.301534: step 27290, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:07m:12s remains)
INFO - root - 2017-12-09 13:14:49.760435: step 27300, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.807 sec/batch; 68h:26m:31s remains)
2017-12-09 13:14:50.726499: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.67088127 0.66241741 0.65640962 0.65251815 0.65132231 0.65304369 0.65398926 0.6537931 0.64845771 0.64148575 0.63062465 0.61921912 0.60767472 0.5975399 0.58916616][0.72924912 0.723985 0.71888185 0.71434855 0.71046156 0.70782304 0.70369607 0.69823158 0.68764263 0.67491895 0.65857512 0.64146519 0.62433892 0.61006182 0.5989669][0.76797616 0.76957607 0.76873964 0.76571059 0.76204103 0.75658971 0.74930131 0.73931915 0.723246 0.70309228 0.67860305 0.6542632 0.62910807 0.60839528 0.59212148][0.81562555 0.82567191 0.82895041 0.82768649 0.82319766 0.81489033 0.80441314 0.7896629 0.76739258 0.73911345 0.70507145 0.67108512 0.63530582 0.60460371 0.57955313][0.86071914 0.88087171 0.8890779 0.88923985 0.88469136 0.87359172 0.85951477 0.8396762 0.81078511 0.77264482 0.72757548 0.6815359 0.63310164 0.5902465 0.5542506][0.88829982 0.91619289 0.92667669 0.92744374 0.92321134 0.90947527 0.89230061 0.86663842 0.83106333 0.78432745 0.72899 0.6717307 0.61110693 0.55660605 0.50970531][0.88630116 0.91590714 0.92277211 0.9204793 0.91299748 0.89700478 0.87809926 0.84869277 0.80941361 0.75694841 0.69583285 0.63091922 0.56178224 0.49879965 0.44397697][0.85164887 0.87550277 0.87072074 0.85848171 0.84201247 0.82062531 0.7982344 0.76818365 0.73033434 0.67852455 0.61862808 0.55274367 0.48212886 0.41666332 0.35966051][0.78858954 0.80111772 0.7800439 0.75169164 0.72081316 0.69028121 0.66219705 0.63169158 0.59735805 0.55314243 0.50221676 0.44310594 0.37921262 0.31883621 0.26673579][0.70932025 0.70598674 0.66610515 0.62063187 0.57524294 0.53418267 0.49975047 0.46960881 0.4406839 0.40649667 0.36737615 0.32112953 0.2703416 0.22116804 0.17925954][0.62562579 0.60545254 0.54801917 0.48682031 0.42893761 0.38002256 0.34217376 0.3136892 0.29098788 0.2678133 0.24124008 0.20867899 0.17209193 0.13628605 0.10606676][0.54380363 0.50767016 0.43674469 0.36572021 0.30134234 0.24901885 0.21099946 0.185864 0.16918769 0.1545167 0.13813233 0.11788192 0.094616219 0.071391791 0.052010965][0.46309155 0.4151412 0.3372775 0.26300025 0.19812596 0.14778775 0.1128976 0.091666512 0.07971523 0.071491882 0.0631088 0.052624691 0.040525351 0.028409857 0.018423546][0.37224096 0.3201611 0.24510556 0.17708695 0.12025296 0.078044675 0.050083593 0.034095041 0.026249442 0.022084579 0.018681094 0.014721325 0.010287315 0.0059555573 0.0024626222][0.27597338 0.22710437 0.16309756 0.10839289 0.06521938 0.035265692 0.016763069 0.0070572579 0.002983321 0.0014812304 0.00068529369 -0.00013736472 -0.0010135474 -0.001847177 -0.0025019087]]...]
INFO - root - 2017-12-09 13:14:59.018664: step 27310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:08m:15s remains)
INFO - root - 2017-12-09 13:15:07.297679: step 27320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 73h:32m:48s remains)
INFO - root - 2017-12-09 13:15:15.906947: step 27330, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:52m:33s remains)
INFO - root - 2017-12-09 13:15:24.650128: step 27340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:01m:04s remains)
INFO - root - 2017-12-09 13:15:33.251144: step 27350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:07m:02s remains)
INFO - root - 2017-12-09 13:15:41.934048: step 27360, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 73h:49m:10s remains)
INFO - root - 2017-12-09 13:15:50.579029: step 27370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:51m:51s remains)
INFO - root - 2017-12-09 13:15:58.968488: step 27380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:19m:05s remains)
INFO - root - 2017-12-09 13:16:07.663899: step 27390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:35m:25s remains)
INFO - root - 2017-12-09 13:16:16.341769: step 27400, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 70h:04m:23s remains)
2017-12-09 13:16:17.258382: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069001585 0.064390711 0.059567161 0.05562494 0.052424945 0.05023063 0.0483087 0.046302497 0.044029478 0.041460242 0.038758557 0.036538862 0.034468334 0.032856248 0.032294683][0.08509662 0.079125211 0.073741227 0.070149094 0.068356417 0.067857645 0.067761496 0.067150995 0.065574996 0.063047528 0.059666745 0.056387812 0.053315029 0.051026065 0.050237563][0.096578777 0.090110973 0.085548922 0.083925858 0.085250176 0.088052526 0.090811551 0.092324726 0.091873109 0.089568585 0.085518427 0.080807641 0.076299265 0.073081709 0.071856916][0.10057637 0.09550187 0.093522206 0.09585005 0.10170374 0.10919812 0.11586145 0.12015712 0.12111099 0.11909822 0.11432091 0.10782409 0.1012882 0.09623713 0.0936688][0.097771905 0.095715635 0.097796924 0.1048612 0.11565768 0.12742862 0.13757405 0.14423466 0.14636692 0.14468855 0.13942727 0.13159811 0.12314437 0.11587146 0.11123361][0.092386253 0.094330043 0.10062568 0.11181622 0.12607732 0.14082721 0.15305081 0.1613479 0.1642608 0.16271657 0.15693706 0.14797586 0.13785848 0.128557 0.12181547][0.0879893 0.094233818 0.10444715 0.11874968 0.13488713 0.15048125 0.16296998 0.17124128 0.17381206 0.17183477 0.16517499 0.15519652 0.14389527 0.13328558 0.1249835][0.085672386 0.095492221 0.10842894 0.12438037 0.14074144 0.15543967 0.16632944 0.17302403 0.17443338 0.17135769 0.16355664 0.15290797 0.14091581 0.12974173 0.12047444][0.0839139 0.096598677 0.11130464 0.12736648 0.14229977 0.15458378 0.16254234 0.16631904 0.16543406 0.16053936 0.15162423 0.14058304 0.12852898 0.11756445 0.10805237][0.080561839 0.095157988 0.11054727 0.12541142 0.13764989 0.14614198 0.15015623 0.15034029 0.14677438 0.14005402 0.1302352 0.11917658 0.10779118 0.09749455 0.088310726][0.073711954 0.088809095 0.10360537 0.11634099 0.12524799 0.12964645 0.12963781 0.12608016 0.11966056 0.11142208 0.10130168 0.091057226 0.080951989 0.071923129 0.063900575][0.062207393 0.076230012 0.089138091 0.098877735 0.10417361 0.10489357 0.10163374 0.095408469 0.087283425 0.078391984 0.069103144 0.060516633 0.052544989 0.045528047 0.039218221][0.047414325 0.059122548 0.069287419 0.075801969 0.07786718 0.075796522 0.070522219 0.063157275 0.054918244 0.046877678 0.039390534 0.0329819 0.027446207 0.022701297 0.018495021][0.031815287 0.0406409 0.047913693 0.051721971 0.051560584 0.04800602 0.042159833 0.035160568 0.028193608 0.022088258 0.017040985 0.01309781 0.0099382084 0.00739263 0.0052377349][0.018044833 0.023945866 0.028668623 0.030739583 0.029722199 0.026229 0.021263707 0.015856314 0.010970308 0.0071215257 0.00434151 0.0023711277 0.0009191581 -0.00018559629 -0.0010332693]]...]
INFO - root - 2017-12-09 13:16:25.693415: step 27410, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 74h:05m:47s remains)
INFO - root - 2017-12-09 13:16:34.124392: step 27420, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.731 sec/batch; 61h:59m:23s remains)
INFO - root - 2017-12-09 13:16:42.745714: step 27430, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 72h:06m:49s remains)
INFO - root - 2017-12-09 13:16:51.427105: step 27440, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:04m:05s remains)
INFO - root - 2017-12-09 13:17:00.061206: step 27450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 74h:54m:09s remains)
INFO - root - 2017-12-09 13:17:08.793864: step 27460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:32m:39s remains)
INFO - root - 2017-12-09 13:17:17.521777: step 27470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 70h:48m:02s remains)
INFO - root - 2017-12-09 13:17:25.927633: step 27480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:39m:15s remains)
INFO - root - 2017-12-09 13:17:34.439877: step 27490, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 72h:04m:51s remains)
INFO - root - 2017-12-09 13:17:43.078490: step 27500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:41m:51s remains)
2017-12-09 13:17:43.924216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033576442 -0.0033545529 -0.0033518621 -0.0033497775 -0.0033480632 -0.0033463561 -0.00334444 -0.0033428525 -0.0033419752 -0.0033417498 -0.0033420513 -0.0033438639 -0.0033461775 -0.0033484993 -0.0033502183][-0.0033590777 -0.0033550127 -0.0033514702 -0.0033487603 -0.0033468213 -0.0033450883 -0.0033431614 -0.0033414941 -0.0033403109 -0.003339648 -0.003339825 -0.0033417153 -0.0033441966 -0.0033469987 -0.0033492697][-0.0033624084 -0.0033576717 -0.00335389 -0.0033505962 -0.0033478912 -0.0033458367 -0.0033439109 -0.0033423791 -0.0033409267 -0.0033397849 -0.0033394904 -0.0033411477 -0.0033435288 -0.003346317 -0.0033487992][-0.0033652619 -0.003360111 -0.003355921 -0.0033526137 -0.0033491859 -0.0033461915 -0.003343706 -0.0033421344 -0.0033408541 -0.0033397253 -0.0033392368 -0.0033404764 -0.0033426574 -0.0033455859 -0.0033483068][-0.003367984 -0.0033628191 -0.0033582083 -0.0033546097 -0.0033507687 -0.0033465095 -0.0033427228 -0.0033408294 -0.0033394885 -0.0033385081 -0.0033380801 -0.0033391414 -0.0033410862 -0.0033438355 -0.0033465971][-0.0033701756 -0.0033657474 -0.0033611578 -0.0033576919 -0.0033536677 -0.0033482234 -0.0033430948 -0.003340143 -0.0033381416 -0.0033370741 -0.0033366731 -0.0033374943 -0.0033388166 -0.0033409889 -0.0033433931][-0.003373119 -0.0033694722 -0.0033659826 -0.0033624154 -0.0033581299 -0.0033518807 -0.0033451791 -0.0033403388 -0.0033375395 -0.0033359823 -0.0033350803 -0.0033353548 -0.0033359784 -0.0033374461 -0.0033394075][-0.0033763342 -0.003373839 -0.0033713351 -0.0033675178 -0.0033629639 -0.0033562218 -0.0033481452 -0.00334152 -0.0033376196 -0.0033358121 -0.003334689 -0.0033343332 -0.0033344654 -0.0033354021 -0.0033368538][-0.0033799966 -0.00337819 -0.0033762238 -0.0033724296 -0.0033678568 -0.0033611287 -0.0033524581 -0.0033445992 -0.0033394843 -0.0033371397 -0.0033358519 -0.0033350328 -0.0033346361 -0.0033350885 -0.0033359525][-0.0033834211 -0.003381884 -0.0033800369 -0.0033763028 -0.0033717118 -0.0033651688 -0.0033565836 -0.0033482674 -0.0033424629 -0.0033392806 -0.0033375018 -0.0033362228 -0.0033355367 -0.0033354529 -0.0033356426][-0.0033840393 -0.0033819419 -0.0033800732 -0.0033764783 -0.0033721053 -0.0033660303 -0.0033583269 -0.0033505212 -0.0033447247 -0.0033411598 -0.0033391595 -0.0033378596 -0.0033371821 -0.0033367937 -0.003336499][-0.0033807661 -0.0033782471 -0.0033763244 -0.0033727777 -0.0033687616 -0.003363421 -0.003356813 -0.0033502255 -0.0033452846 -0.0033420664 -0.0033402124 -0.0033392166 -0.0033387481 -0.003338394 -0.0033379109][-0.0033745738 -0.0033716669 -0.00336951 -0.0033662564 -0.0033627069 -0.0033586745 -0.0033535869 -0.0033485617 -0.0033448837 -0.0033424972 -0.0033410853 -0.0033403451 -0.0033401002 -0.0033398969 -0.0033394629][-0.003366672 -0.0033632326 -0.0033612037 -0.0033585685 -0.0033558402 -0.0033528928 -0.0033495026 -0.0033462474 -0.0033439274 -0.0033423575 -0.0033413696 -0.0033408713 -0.00334072 -0.0033405998 -0.0033402313][-0.0033583008 -0.0033545438 -0.0033528465 -0.0033509021 -0.0033491415 -0.0033472695 -0.0033453146 -0.0033435852 -0.0033423547 -0.0033415074 -0.003340964 -0.003340763 -0.0033406781 -0.0033405973 -0.0033403737]]...]
INFO - root - 2017-12-09 13:17:52.276397: step 27510, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 74h:55m:46s remains)
INFO - root - 2017-12-09 13:18:00.883854: step 27520, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 72h:32m:34s remains)
INFO - root - 2017-12-09 13:18:09.467843: step 27530, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 70h:16m:40s remains)
INFO - root - 2017-12-09 13:18:18.122351: step 27540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 73h:15m:00s remains)
INFO - root - 2017-12-09 13:18:26.863649: step 27550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:19m:31s remains)
INFO - root - 2017-12-09 13:18:35.664701: step 27560, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 74h:46m:08s remains)
INFO - root - 2017-12-09 13:18:44.418925: step 27570, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:55m:13s remains)
INFO - root - 2017-12-09 13:18:52.895131: step 27580, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 69h:37m:33s remains)
INFO - root - 2017-12-09 13:19:01.481256: step 27590, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 72h:54m:41s remains)
INFO - root - 2017-12-09 13:19:10.136901: step 27600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:03m:23s remains)
2017-12-09 13:19:11.042874: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071938641 0.072766744 0.073144816 0.072309613 0.071319588 0.070066266 0.069543436 0.070116282 0.071126007 0.072139025 0.072503313 0.071866527 0.069012187 0.062907673 0.05419137][0.066508479 0.06722308 0.067865953 0.067366794 0.066969372 0.066421472 0.066839181 0.068176448 0.069908515 0.071646534 0.0726378 0.072530769 0.070095882 0.064251058 0.0554754][0.060080335 0.060578883 0.061290108 0.061321288 0.061939552 0.062462956 0.064191796 0.066599846 0.069116376 0.071239971 0.072268851 0.072127871 0.069514491 0.063663885 0.054945793][0.055880837 0.05610178 0.05662021 0.056847673 0.058098488 0.05965852 0.062751867 0.06618847 0.069354795 0.071577176 0.072342783 0.071591333 0.0684326 0.062381774 0.053723104][0.056123488 0.056504153 0.056663804 0.056745533 0.057999097 0.059705175 0.063135296 0.067158423 0.070862137 0.072959132 0.073302284 0.0718726 0.068092413 0.06158182 0.052682467][0.059795879 0.060781114 0.060987115 0.060928784 0.061775893 0.063049711 0.065973453 0.069400221 0.072649688 0.074467324 0.07444787 0.072273664 0.068027265 0.061330967 0.052330792][0.064164579 0.065816186 0.066027015 0.065898448 0.066281334 0.06702964 0.069202729 0.071812339 0.074375466 0.075533994 0.074974865 0.072264642 0.067541666 0.060498461 0.051417902][0.067544684 0.070239283 0.070635863 0.0703557 0.070098117 0.070115454 0.071256958 0.072833642 0.074177884 0.07448411 0.073335446 0.070180222 0.065177679 0.058071204 0.049244326][0.067820273 0.071445279 0.072522536 0.072608709 0.072131731 0.07182309 0.072128832 0.072667152 0.0728617 0.07225477 0.070523188 0.06710308 0.062011994 0.054971363 0.046572294][0.066251218 0.069955848 0.071216591 0.071502574 0.071159571 0.070823573 0.070840649 0.0709145 0.070359021 0.069040589 0.066760525 0.063156627 0.05803445 0.051165186 0.0432011][0.063524179 0.066872247 0.067850977 0.06782902 0.067296214 0.06683892 0.06676387 0.0664824 0.065566063 0.064090967 0.061660349 0.058049306 0.053004447 0.046499282 0.039057054][0.05898517 0.061951272 0.062643491 0.062390245 0.061670512 0.060921289 0.060539089 0.059902579 0.058688797 0.05691291 0.05444796 0.051162474 0.046527293 0.040621486 0.033895448][0.051439736 0.053961128 0.054531015 0.054210793 0.053503986 0.052661777 0.052016206 0.051004596 0.049447458 0.047576904 0.045175929 0.042199966 0.038249329 0.033322334 0.027715014][0.040155236 0.042250283 0.042837188 0.042742152 0.042370502 0.041722655 0.041130815 0.040070098 0.038512133 0.036653291 0.034451019 0.031935234 0.0287703 0.0249978 0.020797634][0.027743865 0.029226368 0.029737024 0.029784812 0.029694242 0.02930353 0.028943146 0.028167365 0.027020002 0.025588704 0.023865419 0.021974428 0.019664016 0.017089985 0.01432051]]...]
INFO - root - 2017-12-09 13:19:19.363900: step 27610, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:47m:20s remains)
INFO - root - 2017-12-09 13:19:28.021163: step 27620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:45m:06s remains)
INFO - root - 2017-12-09 13:19:36.469329: step 27630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:25m:44s remains)
INFO - root - 2017-12-09 13:19:44.992433: step 27640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:17m:02s remains)
INFO - root - 2017-12-09 13:19:53.707196: step 27650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:25m:50s remains)
INFO - root - 2017-12-09 13:20:02.299422: step 27660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:23m:31s remains)
INFO - root - 2017-12-09 13:20:11.094037: step 27670, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:08m:11s remains)
INFO - root - 2017-12-09 13:20:19.565742: step 27680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:50m:05s remains)
INFO - root - 2017-12-09 13:20:28.287925: step 27690, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.904 sec/batch; 76h:30m:45s remains)
INFO - root - 2017-12-09 13:20:36.890220: step 27700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:19m:23s remains)
2017-12-09 13:20:37.792052: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.023166962 0.037618469 0.058633402 0.0842556 0.11116557 0.13348918 0.1476191 0.15003841 0.1411411 0.12294929 0.099290319 0.075148337 0.054448076 0.04215268 0.039801888][0.023336522 0.041035026 0.065914176 0.095453128 0.12544318 0.15002985 0.16498858 0.16726071 0.15805002 0.13955437 0.11559834 0.091606371 0.071047477 0.059359334 0.057514694][0.022045799 0.041043203 0.068715557 0.1017939 0.13494749 0.16298194 0.18070692 0.1850682 0.17703193 0.1594201 0.13655467 0.11323203 0.092986465 0.081228718 0.07839112][0.0221458 0.042576019 0.072429292 0.10830958 0.14418258 0.17455465 0.1945 0.20108864 0.19516654 0.17955039 0.15873158 0.13727911 0.11814731 0.10566133 0.10077792][0.023124781 0.045196053 0.077120394 0.11489875 0.15280525 0.18432385 0.20560978 0.21348307 0.20883653 0.1947142 0.17573728 0.15632698 0.13817473 0.12572917 0.11951589][0.024981529 0.048186529 0.081108846 0.12022655 0.15922709 0.19162731 0.21334133 0.22122566 0.21656045 0.20279798 0.18428777 0.16530545 0.14724581 0.13422959 0.12678473][0.02463359 0.0476823 0.080376558 0.11925948 0.15838936 0.19132704 0.21319456 0.2207461 0.21552992 0.20109047 0.1817553 0.16160612 0.14310914 0.1290717 0.11999114][0.021847554 0.042665515 0.072103 0.10829069 0.1447577 0.17590705 0.19681053 0.20372817 0.19825371 0.18384634 0.16460124 0.14412513 0.12506959 0.10985705 0.099032283][0.016920734 0.033345249 0.056888308 0.086662918 0.11685827 0.14307174 0.16017891 0.16549796 0.16032241 0.14727722 0.12994047 0.11113999 0.093508385 0.078875266 0.067781433][0.011205062 0.022048377 0.038179152 0.058951203 0.080339648 0.099158481 0.1111491 0.1143562 0.10971982 0.099028409 0.085093886 0.069991916 0.055940196 0.044171423 0.03502265][0.0056581078 0.011471968 0.020666268 0.032979209 0.045712989 0.056997292 0.063936524 0.065419115 0.061619304 0.053765785 0.043914169 0.033505417 0.024314817 0.016877137 0.011326891][0.0010878085 0.0035955594 0.0078128949 0.013704621 0.019978561 0.025503568 0.028729681 0.029169604 0.026677698 0.022000652 0.016368903 0.010804013 0.006204322 0.0026767175 0.00028790114][-0.0021590735 -0.0012385305 0.00035004551 0.0026147366 0.0051163295 0.0071969675 0.0082972674 0.0082409522 0.0069317268 0.0047360715 0.0022614053 0.00010594027 -0.0014560143 -0.00248723 -0.0030409449][-0.003312673 -0.003175901 -0.0028717667 -0.0023336071 -0.0016899958 -0.0011589453 -0.00090586371 -0.00096534146 -0.0013206739 -0.0018839353 -0.0024551318 -0.0028910586 -0.0031520838 -0.0032848991 -0.003333457][-0.0033631576 -0.0033607262 -0.0033420858 -0.0032907065 -0.0032312744 -0.0031754172 -0.0031493194 -0.0031461392 -0.0031657207 -0.0032114126 -0.0032536306 -0.0032895266 -0.003307187 -0.0033205152 -0.0033308431]]...]
INFO - root - 2017-12-09 13:20:46.242918: step 27710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:44m:22s remains)
INFO - root - 2017-12-09 13:20:54.929235: step 27720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:41m:38s remains)
INFO - root - 2017-12-09 13:21:03.394220: step 27730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:24m:09s remains)
INFO - root - 2017-12-09 13:21:12.129715: step 27740, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 74h:50m:07s remains)
INFO - root - 2017-12-09 13:21:20.780634: step 27750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:04m:40s remains)
INFO - root - 2017-12-09 13:21:29.377241: step 27760, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 76h:09m:44s remains)
INFO - root - 2017-12-09 13:21:38.244154: step 27770, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 73h:47m:20s remains)
INFO - root - 2017-12-09 13:21:46.768548: step 27780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:54m:44s remains)
INFO - root - 2017-12-09 13:21:55.331367: step 27790, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 71h:30m:56s remains)
INFO - root - 2017-12-09 13:22:04.002428: step 27800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:12m:48s remains)
2017-12-09 13:22:04.838171: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016062988 0.013925616 0.011707153 0.010011163 0.0089497315 0.0087233121 0.0088967681 0.0093802446 0.010415507 0.012476154 0.016050968 0.021834774 0.030261222 0.040601023 0.051577363][0.024939334 0.021352515 0.017067956 0.01330648 0.010367526 0.00842834 0.007018677 0.0061504096 0.006129995 0.0073921205 0.010105584 0.014915897 0.022512363 0.032424852 0.043591958][0.037290253 0.033189178 0.027310681 0.021530004 0.016437354 0.01233734 0.0088066179 0.0059610568 0.0043242173 0.0042394977 0.0057988595 0.0093289809 0.015458571 0.024120353 0.034715351][0.051158909 0.047469914 0.040927161 0.033987097 0.027352875 0.021125132 0.015049143 0.0095419548 0.0053644488 0.0031627647 0.0030918436 0.0052307853 0.0097798565 0.016739389 0.026187705][0.064410411 0.06244161 0.056218449 0.048897114 0.04134813 0.033418365 0.025038207 0.016593318 0.0092590423 0.0042853653 0.0020628327 0.0025174951 0.0054072682 0.01056008 0.018416792][0.0738482 0.074371204 0.069810264 0.063400127 0.055915829 0.0469817 0.036818489 0.025768833 0.015317943 0.0071984576 0.0024017456 0.0010830655 0.0023993442 0.0057641054 0.011751711][0.077627383 0.080514386 0.078137554 0.073459931 0.067230985 0.058655862 0.047874909 0.035095688 0.022102924 0.011015682 0.0035580529 0.00037932931 0.00028352812 0.0022524029 0.00651073][0.0752021 0.079877287 0.079437315 0.076808468 0.0725382 0.06544064 0.055662636 0.042832002 0.028569739 0.015310587 0.0055591948 0.00053180265 -0.00090941088 -1.6774517e-05 0.0028875729][0.068365924 0.073650658 0.07433787 0.073170908 0.070598125 0.065463535 0.057709575 0.046397913 0.032770157 0.018993732 0.0078983726 0.0013512361 -0.0011819729 -0.00098284381 0.00115184][0.058142476 0.063002966 0.064056419 0.063639171 0.062247798 0.058924165 0.053442366 0.044569619 0.033022705 0.020472396 0.0095706126 0.0024202277 -0.00076415949 -0.00093672215 0.00084582227][0.045139752 0.049068045 0.049977664 0.049819641 0.049139388 0.047153313 0.043717485 0.037588332 0.029029317 0.018992797 0.0095888013 0.0029028768 -0.00037762197 -0.00072493008 0.00088840467][0.031029906 0.033908147 0.034448318 0.034268279 0.033875246 0.032766666 0.030899182 0.027196903 0.021692645 0.014756368 0.0077774609 0.0024013042 -0.00045510707 -0.00084919715 0.00050115329][0.018154062 0.020030417 0.020240832 0.020024631 0.019706013 0.019068228 0.018148966 0.016232232 0.013251944 0.0092051933 0.0048176059 0.0011369695 -0.00099649327 -0.00137678 -0.00039896718][0.0082418 0.00933335 0.0094214175 0.0092352079 0.0089741424 0.0086118514 0.0081302626 0.0072263973 0.0058706151 0.0039445283 0.0016812019 -0.00040945713 -0.0017347428 -0.0020566508 -0.0014844831][0.0016445119 0.0021970945 0.0022651746 0.0021862544 0.0020617924 0.0018871322 0.0016539979 0.0012784966 0.00075262622 1.793704e-05 -0.0008994597 -0.0018241592 -0.0024691734 -0.0026725135 -0.0024108768]]...]
INFO - root - 2017-12-09 13:22:13.230178: step 27810, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 74h:08m:11s remains)
INFO - root - 2017-12-09 13:22:21.835718: step 27820, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:19m:55s remains)
INFO - root - 2017-12-09 13:22:30.365516: step 27830, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 75h:03m:32s remains)
INFO - root - 2017-12-09 13:22:39.166088: step 27840, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:12m:41s remains)
INFO - root - 2017-12-09 13:22:47.798744: step 27850, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 73h:06m:45s remains)
INFO - root - 2017-12-09 13:22:56.486174: step 27860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 71h:58m:26s remains)
INFO - root - 2017-12-09 13:23:05.218004: step 27870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:40m:01s remains)
INFO - root - 2017-12-09 13:23:13.718599: step 27880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 75h:33m:35s remains)
INFO - root - 2017-12-09 13:23:22.389359: step 27890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:49m:01s remains)
INFO - root - 2017-12-09 13:23:31.141356: step 27900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:06m:23s remains)
2017-12-09 13:23:32.015655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033435246 -0.0033393085 -0.0033385511 -0.0033383425 -0.0033383633 -0.0033386138 -0.0033388862 -0.0033393474 -0.0033396217 -0.0033393779 -0.0033386005 -0.0033378066 -0.0033374573 -0.0033372142 -0.0033371272][-0.00334144 -0.0033366983 -0.0033357891 -0.0033354261 -0.0033353646 -0.0033356778 -0.0033361455 -0.0033367288 -0.0033371346 -0.0033369309 -0.003336173 -0.0033352575 -0.0033347434 -0.0033344864 -0.0033344566][-0.0033415633 -0.0033366848 -0.0033357253 -0.0033351963 -0.003335021 -0.0033353767 -0.0033360634 -0.0033367416 -0.0033372343 -0.0033370915 -0.0033363551 -0.0033353909 -0.0033347236 -0.0033343977 -0.003334315][-0.0033412846 -0.003336322 -0.0033353851 -0.0033348452 -0.0033346734 -0.0033351174 -0.0033360538 -0.0033368906 -0.0033374617 -0.0033374226 -0.003336732 -0.0033357411 -0.003334976 -0.0033346405 -0.0033344661][-0.003340893 -0.0033358184 -0.0033349497 -0.0033343136 -0.0033340903 -0.0033345211 -0.0033355139 -0.003336495 -0.0033371481 -0.003337265 -0.0033366713 -0.0033356894 -0.0033349914 -0.0033347597 -0.0033346259][-0.0033405053 -0.0033355081 -0.0033348144 -0.0033340864 -0.0033337411 -0.0033341264 -0.0033349434 -0.0033358866 -0.0033366145 -0.0033369977 -0.0033365197 -0.0033355521 -0.0033349809 -0.003334793 -0.0033346328][-0.0033395439 -0.0033351355 -0.0033348273 -0.0033341972 -0.003333735 -0.0033339637 -0.0033344245 -0.0033350703 -0.0033357525 -0.0033363136 -0.003335936 -0.003335028 -0.0033346042 -0.0033345111 -0.0033342924][-0.0033382177 -0.0033343448 -0.0033344303 -0.0033340545 -0.0033337446 -0.0033340428 -0.0033341972 -0.0033342713 -0.0033345341 -0.0033349516 -0.0033345816 -0.0033338466 -0.0033336824 -0.0033338845 -0.0033339111][-0.0033372035 -0.0033337909 -0.0033343537 -0.0033340733 -0.0033337872 -0.0033341395 -0.0033340091 -0.0033334636 -0.0033330203 -0.0033329872 -0.0033325301 -0.0033320624 -0.003332319 -0.0033328799 -0.0033334121][-0.0033365206 -0.0033335967 -0.0033345835 -0.0033344824 -0.0033342273 -0.0033345248 -0.0033340415 -0.0033328247 -0.0033316393 -0.0033310738 -0.0033303797 -0.0033301339 -0.0033307606 -0.0033317031 -0.0033327478][-0.0033362105 -0.0033334526 -0.0033347541 -0.0033350079 -0.003334984 -0.0033352452 -0.0033343944 -0.0033325595 -0.0033306845 -0.0033294954 -0.0033284917 -0.0033283681 -0.0033291273 -0.0033302861 -0.0033318175][-0.003336326 -0.0033333038 -0.0033346151 -0.0033350934 -0.0033352906 -0.0033355444 -0.0033345534 -0.0033323364 -0.0033299334 -0.0033281401 -0.0033269846 -0.0033268058 -0.0033275078 -0.0033288817 -0.0033308703][-0.0033367821 -0.0033332608 -0.0033344661 -0.003335044 -0.0033354049 -0.0033357993 -0.0033350636 -0.003332908 -0.0033303273 -0.0033282225 -0.0033269508 -0.0033266221 -0.0033270828 -0.0033283264 -0.0033304903][-0.0033372939 -0.003333299 -0.003334187 -0.0033346247 -0.003335055 -0.0033356631 -0.0033354061 -0.0033337211 -0.0033315166 -0.0033296577 -0.0033283907 -0.0033277643 -0.0033277851 -0.0033285534 -0.0033305027][-0.0033374387 -0.0033329742 -0.0033334743 -0.0033338377 -0.0033345041 -0.0033355374 -0.0033360154 -0.0033353514 -0.0033339655 -0.0033323329 -0.0033310428 -0.0033299439 -0.0033291848 -0.0033291955 -0.0033307718]]...]
INFO - root - 2017-12-09 13:23:40.330707: step 27910, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 70h:29m:36s remains)
INFO - root - 2017-12-09 13:23:48.797727: step 27920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:21m:37s remains)
INFO - root - 2017-12-09 13:23:57.129836: step 27930, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:05m:12s remains)
INFO - root - 2017-12-09 13:24:05.800220: step 27940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:11m:11s remains)
INFO - root - 2017-12-09 13:24:14.430637: step 27950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 70h:51m:58s remains)
INFO - root - 2017-12-09 13:24:23.061400: step 27960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:23m:29s remains)
INFO - root - 2017-12-09 13:24:31.726817: step 27970, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 72h:58m:38s remains)
INFO - root - 2017-12-09 13:24:40.247670: step 27980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:53m:27s remains)
INFO - root - 2017-12-09 13:24:48.956645: step 27990, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 77h:23m:33s remains)
INFO - root - 2017-12-09 13:24:57.632421: step 28000, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 74h:24m:25s remains)
2017-12-09 13:24:58.629997: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4101882 0.39375919 0.37240684 0.34863949 0.32442191 0.30154726 0.28021827 0.26222077 0.24852191 0.23698664 0.22693123 0.21740708 0.20529649 0.18830901 0.16612351][0.42196822 0.40798259 0.38792068 0.36594585 0.34357694 0.32152459 0.30105382 0.28406033 0.27122334 0.26057962 0.25074783 0.24020991 0.22641468 0.20641923 0.18059269][0.42351714 0.41262951 0.39506394 0.37579122 0.3562218 0.33747396 0.31958205 0.30401811 0.29176322 0.28095517 0.26981673 0.25620133 0.23860642 0.21468516 0.18564661][0.41906491 0.41261971 0.39878571 0.38318723 0.36769393 0.35271835 0.33815658 0.32478663 0.31245157 0.29984391 0.28472367 0.26606625 0.24306162 0.21477383 0.18340956][0.40552476 0.40386668 0.39495277 0.38471133 0.37507877 0.36490691 0.35379696 0.34204769 0.32866022 0.31270787 0.29254317 0.26825121 0.24002771 0.20874009 0.17719385][0.38532305 0.3886157 0.38384187 0.37894583 0.37455952 0.36891934 0.36202464 0.35163027 0.33723578 0.31747806 0.29207051 0.26261398 0.23124039 0.1999191 0.17165555][0.36426798 0.37185794 0.37021092 0.3695522 0.36897725 0.36705464 0.36239144 0.35290366 0.33731315 0.31368357 0.28343624 0.25001991 0.21734542 0.18845116 0.16601475][0.34345126 0.35370773 0.35437289 0.35645127 0.35849205 0.35920247 0.35603324 0.34602529 0.32815987 0.301473 0.26781687 0.23257324 0.20087808 0.17633508 0.16139916][0.32543224 0.33593839 0.33657405 0.33930257 0.34194553 0.34276712 0.33920714 0.32880917 0.30921695 0.28014961 0.24481423 0.20977004 0.18092689 0.16229409 0.15566175][0.30762997 0.31571063 0.3141956 0.31567642 0.3171677 0.31708348 0.31255576 0.3007856 0.27936488 0.24924757 0.21397671 0.18072519 0.15638651 0.14457761 0.14628361][0.2938602 0.29746115 0.29092181 0.28855875 0.28709131 0.28494924 0.27893502 0.26644468 0.24413985 0.21357992 0.17881839 0.14775911 0.12772864 0.12220883 0.13124585][0.2796503 0.27883157 0.26787719 0.26103958 0.25644261 0.25234285 0.24436121 0.23044364 0.20689403 0.17643534 0.1425105 0.1139494 0.097713731 0.096577145 0.11011951][0.26107004 0.25544503 0.24054842 0.23079753 0.22474347 0.21992514 0.21185698 0.19724229 0.17290564 0.142508 0.10990567 0.083246872 0.069360942 0.07070493 0.086149916][0.23982506 0.23013221 0.21239154 0.20137285 0.19524951 0.19059677 0.18246831 0.16776246 0.14378315 0.11329208 0.081634894 0.056253832 0.043956619 0.046074387 0.060775738][0.21625125 0.20535743 0.18762256 0.17717363 0.17217271 0.16871886 0.16121762 0.14578196 0.12164202 0.091500193 0.060867954 0.036426269 0.024441918 0.02582016 0.037834633]]...]
INFO - root - 2017-12-09 13:25:07.076324: step 28010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 73h:57m:41s remains)
INFO - root - 2017-12-09 13:25:15.827924: step 28020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:52m:31s remains)
INFO - root - 2017-12-09 13:25:24.331096: step 28030, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:13m:51s remains)
INFO - root - 2017-12-09 13:25:32.993899: step 28040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:38m:38s remains)
INFO - root - 2017-12-09 13:25:41.698406: step 28050, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 74h:16m:01s remains)
INFO - root - 2017-12-09 13:25:50.322785: step 28060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:22m:52s remains)
INFO - root - 2017-12-09 13:25:58.955164: step 28070, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:16m:27s remains)
INFO - root - 2017-12-09 13:26:07.363035: step 28080, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 68h:27m:24s remains)
INFO - root - 2017-12-09 13:26:15.792667: step 28090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:14m:58s remains)
INFO - root - 2017-12-09 13:26:24.440059: step 28100, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 73h:55m:29s remains)
2017-12-09 13:26:25.302172: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051820725 0.052510008 0.05166588 0.0505242 0.049075063 0.047594264 0.045873117 0.044749957 0.043569859 0.042678379 0.041647322 0.040530328 0.038687341 0.035297785 0.031768233][0.078379571 0.079891019 0.078886494 0.077604964 0.076221727 0.075176254 0.074142851 0.072891377 0.071755759 0.070501722 0.069067314 0.068055555 0.066129513 0.063025311 0.059792172][0.12913761 0.13218887 0.13070972 0.12828654 0.12559906 0.12315714 0.12093842 0.11890397 0.11736887 0.11513171 0.11272122 0.11068886 0.10851714 0.10544614 0.10244659][0.18464795 0.19013315 0.18893683 0.18618417 0.18241102 0.17812772 0.17385817 0.16941784 0.16590668 0.16133307 0.157382 0.1543029 0.15175281 0.14973779 0.1483566][0.22530666 0.2332063 0.232352 0.23011167 0.22603834 0.22034262 0.21407807 0.20695725 0.20091479 0.1938953 0.18799664 0.18358538 0.18091765 0.18058227 0.18189159][0.24121961 0.25048724 0.24944936 0.24743919 0.24291159 0.23665574 0.22933464 0.22036296 0.2122533 0.20246968 0.19439927 0.18817432 0.18486762 0.18522656 0.18853432][0.23647116 0.24679555 0.24579166 0.24318507 0.23720297 0.22936283 0.22012256 0.20903361 0.19823301 0.18587855 0.17562048 0.16780004 0.16419514 0.16556114 0.17110614][0.21523675 0.22543509 0.22429776 0.22037977 0.21198952 0.20155841 0.18941465 0.17596336 0.16253179 0.14822623 0.13704532 0.1286601 0.12527439 0.12741804 0.13446721][0.17865293 0.18778406 0.18554559 0.17901526 0.1682879 0.15503785 0.14017002 0.12512934 0.11097149 0.097410172 0.086825192 0.079574063 0.077627346 0.08054512 0.088007346][0.12906033 0.13581584 0.13298774 0.12538631 0.11363167 0.099477187 0.08473783 0.070573092 0.058052387 0.047501817 0.040097434 0.035501122 0.034681689 0.037467096 0.043720428][0.075475357 0.079609871 0.077208035 0.071007445 0.061551083 0.050112452 0.038591783 0.028312806 0.020317454 0.014139636 0.010311862 0.0083958013 0.008646897 0.010475096 0.013732933][0.031511452 0.033291046 0.031926595 0.028429355 0.023127384 0.016830264 0.010710894 0.0055944547 0.001927268 -0.00056729116 -0.0018595966 -0.0021862979 -0.0017785338 -0.0010883873 9.5070573e-05][0.0062745176 0.0068680011 0.0065285591 0.0054446878 0.003748425 0.0017071569 -0.00018170592 -0.0016035545 -0.0024969918 -0.0029944617 -0.0031981436 -0.0032532134 -0.0032608304 -0.003226907 -0.0030956776][-0.0024963389 -0.0023544636 -0.0023271809 -0.0023998804 -0.0025646985 -0.0028036218 -0.0030217289 -0.0031569037 -0.0032108321 -0.0032318474 -0.0032429784 -0.0032532385 -0.0032603382 -0.0032639939 -0.0032653513][-0.0032988868 -0.003284005 -0.003275434 -0.0032699346 -0.0032636495 -0.0032519477 -0.003241075 -0.0032339855 -0.0032300921 -0.0032346731 -0.0032417295 -0.0032536285 -0.003262714 -0.0032682037 -0.003271464]]...]
INFO - root - 2017-12-09 13:26:33.802798: step 28110, loss = 0.91, batch loss = 0.70 (8.9 examples/sec; 0.898 sec/batch; 75h:53m:50s remains)
INFO - root - 2017-12-09 13:26:42.401985: step 28120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:26m:58s remains)
INFO - root - 2017-12-09 13:26:50.970971: step 28130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:33m:38s remains)
INFO - root - 2017-12-09 13:26:59.633872: step 28140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 75h:20m:48s remains)
INFO - root - 2017-12-09 13:27:08.179700: step 28150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:11m:42s remains)
INFO - root - 2017-12-09 13:27:16.948948: step 28160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:46m:59s remains)
INFO - root - 2017-12-09 13:27:25.562924: step 28170, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:22m:06s remains)
INFO - root - 2017-12-09 13:27:34.138112: step 28180, loss = 0.89, batch loss = 0.68 (11.2 examples/sec; 0.715 sec/batch; 60h:25m:55s remains)
INFO - root - 2017-12-09 13:27:42.829197: step 28190, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 70h:48m:56s remains)
INFO - root - 2017-12-09 13:27:51.387408: step 28200, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 71h:47m:50s remains)
2017-12-09 13:27:52.338963: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036387198 0.03804522 0.03682429 0.032949317 0.027380025 0.021312483 0.015868889 0.011928408 0.0096172448 0.008254027 0.0071853455 0.0060245106 0.0046513584 0.0031541309 0.0015397307][0.058235645 0.06083956 0.059122872 0.053521845 0.04548144 0.03662701 0.028533205 0.022540234 0.018835897 0.016584145 0.014881141 0.013062538 0.010877734 0.0083043957 0.0054673087][0.084428817 0.088380888 0.086274013 0.078823693 0.068232596 0.056611881 0.046010315 0.038080513 0.033104487 0.030179746 0.02791135 0.02518722 0.021663068 0.017303105 0.012436409][0.1094816 0.11603185 0.11475579 0.10707565 0.0954881 0.082139708 0.0694169 0.059343044 0.052610986 0.048414666 0.045048449 0.040917538 0.035499405 0.028770391 0.021373589][0.1248477 0.13472591 0.13608059 0.13035508 0.12011133 0.1076503 0.0949978 0.084218428 0.076363862 0.070838846 0.065763637 0.059466336 0.051466487 0.04174215 0.031225357][0.12701681 0.14025611 0.14486545 0.14246593 0.13536097 0.12552893 0.1145326 0.10437302 0.096291609 0.0899843 0.083437622 0.075168625 0.064879335 0.052585289 0.039421044][0.11645658 0.13186343 0.13976102 0.14102101 0.13738687 0.13081317 0.12229605 0.11366374 0.10601252 0.099209838 0.0915233 0.082021654 0.070527822 0.056961633 0.042534783][0.097245567 0.11296089 0.12283219 0.12704794 0.12678762 0.12320697 0.11722548 0.11038704 0.10339752 0.096440442 0.0881152 0.07822004 0.066574767 0.053206094 0.039283376][0.073342942 0.087513439 0.097741209 0.10358632 0.1057982 0.10471892 0.10104364 0.096046753 0.0901195 0.08351814 0.075274087 0.065781228 0.054930694 0.043035217 0.031007957][0.048678551 0.059816573 0.068914264 0.075022779 0.078380048 0.078787155 0.076908723 0.073541217 0.068877712 0.063242719 0.056031518 0.047852729 0.0387575 0.029311694 0.020182846][0.027105648 0.034537356 0.041236844 0.04626907 0.04941022 0.050253037 0.049311381 0.047105212 0.043683723 0.039390307 0.034055796 0.028119784 0.021711739 0.015400595 0.009660772][0.011496942 0.015652264 0.019656876 0.022816829 0.024784321 0.025335645 0.024743222 0.023194073 0.020775963 0.017930031 0.014741874 0.011351022 0.0078877816 0.0046456531 0.0019512179][0.0023297647 0.0041319514 0.0060013179 0.0074667288 0.0083559053 0.0084877294 0.0079995934 0.0070498036 0.0055777254 0.0040675 0.0025773596 0.0012105035 -3.671553e-05 -0.0011590249 -0.0019534524][-0.0017690916 -0.001201238 -0.00051029562 3.4485711e-05 0.00040135672 0.00049452647 0.00032501784 -9.6547883e-06 -0.00068337191 -0.00129377 -0.0018478865 -0.0022459798 -0.002520486 -0.0028062537 -0.0029394813][-0.0025897997 -0.0025288607 -0.0023963859 -0.00223275 -0.0020251255 -0.0018822281 -0.0018335768 -0.0017933405 -0.0019551062 -0.0021254523 -0.0023311819 -0.0024624264 -0.0024949531 -0.0026336913 -0.0026600035]]...]
INFO - root - 2017-12-09 13:28:00.701891: step 28210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:10m:29s remains)
INFO - root - 2017-12-09 13:28:09.319403: step 28220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:46m:54s remains)
INFO - root - 2017-12-09 13:28:17.845602: step 28230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:56m:05s remains)
INFO - root - 2017-12-09 13:28:26.606367: step 28240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:44m:26s remains)
INFO - root - 2017-12-09 13:28:35.364691: step 28250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 73h:32m:34s remains)
INFO - root - 2017-12-09 13:28:43.950387: step 28260, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 72h:21m:09s remains)
INFO - root - 2017-12-09 13:28:52.622947: step 28270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:11m:12s remains)
INFO - root - 2017-12-09 13:29:01.383029: step 28280, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.919 sec/batch; 77h:37m:06s remains)
INFO - root - 2017-12-09 13:29:09.734314: step 28290, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 69h:42m:32s remains)
INFO - root - 2017-12-09 13:29:18.249058: step 28300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:24m:08s remains)
2017-12-09 13:29:19.124577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012413382 0.00056131673 0.0043365238 0.009566796 0.015119213 0.019492436 0.021395523 0.020553967 0.017645665 0.013967437 0.010041157 0.0063334834 0.0032122468 0.00091314246 -0.00071321335][0.0018778562 0.0041213585 0.0091279261 0.0164111 0.024710823 0.031879921 0.035874359 0.035745524 0.031777993 0.025713924 0.018617524 0.011824252 0.0061750356 0.0022294687 -0.0002344707][0.0071254326 0.010263631 0.016973922 0.026892185 0.038715281 0.049624506 0.056795768 0.058429103 0.054111406 0.045669116 0.034649368 0.023408331 0.013458113 0.0060933307 0.0014274193][0.014176861 0.019436197 0.029307518 0.0434313 0.060181249 0.075939372 0.086930066 0.090388358 0.085280165 0.073544875 0.057446428 0.040459163 0.024969559 0.012978622 0.0049582245][0.022076042 0.030973352 0.045692336 0.065737046 0.08887738 0.11059505 0.12613039 0.13182725 0.12602237 0.11052205 0.088295825 0.0640026 0.041268378 0.023046074 0.010431859][0.028890371 0.042986467 0.063820355 0.090355106 0.11944634 0.14600769 0.16474521 0.17191298 0.16562708 0.14734223 0.12036635 0.089791387 0.060277943 0.035549045 0.017684685][0.032247812 0.051838182 0.078993917 0.11161131 0.14537834 0.17472279 0.19447294 0.20157529 0.19451524 0.17470552 0.14522164 0.11099751 0.077039443 0.047528163 0.025446102][0.033844803 0.0573885 0.089078933 0.12576587 0.16208036 0.19218765 0.21130033 0.21728063 0.2091309 0.18845788 0.15817176 0.12284446 0.08732941 0.055778455 0.031767596][0.03515565 0.060556043 0.094146 0.13210417 0.16850208 0.19761346 0.21521239 0.2197928 0.21079284 0.18992637 0.16000503 0.12531863 0.090393618 0.059322368 0.0355675][0.035711166 0.06110758 0.094166383 0.13086368 0.1653195 0.19221088 0.20788467 0.21125589 0.2020261 0.18192017 0.15363096 0.12100024 0.08825174 0.059273515 0.03733911][0.034338024 0.05855358 0.089498051 0.1231689 0.15418656 0.17784543 0.19124404 0.193665 0.185035 0.16686994 0.14162131 0.11254826 0.083347544 0.057560746 0.038178612][0.030738894 0.052298557 0.079757214 0.10936758 0.13640814 0.1568111 0.16816714 0.16998933 0.16241945 0.1468998 0.12553705 0.10099843 0.076424867 0.054752991 0.038538549][0.025765784 0.04341713 0.065953225 0.09048631 0.11320557 0.13066232 0.14059964 0.14243931 0.13637415 0.12368535 0.10626206 0.086434714 0.066855624 0.049799416 0.037155211][0.019365355 0.033092722 0.050457276 0.06943135 0.087240741 0.10125796 0.10964494 0.11168268 0.10742544 0.097795211 0.084389694 0.069214545 0.054547213 0.042124927 0.033110175][0.011690917 0.02155558 0.03411565 0.047968317 0.061091844 0.071519293 0.077862166 0.079530671 0.076673076 0.069971524 0.060619194 0.050133936 0.040156946 0.031960927 0.026151348]]...]
INFO - root - 2017-12-09 13:29:27.410664: step 28310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:08m:27s remains)
INFO - root - 2017-12-09 13:29:36.005860: step 28320, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:41m:01s remains)
INFO - root - 2017-12-09 13:29:44.560530: step 28330, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 64h:41m:25s remains)
INFO - root - 2017-12-09 13:29:53.141618: step 28340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 74h:27m:55s remains)
INFO - root - 2017-12-09 13:30:01.936077: step 28350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 72h:43m:44s remains)
INFO - root - 2017-12-09 13:30:10.550960: step 28360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:09m:22s remains)
INFO - root - 2017-12-09 13:30:19.036900: step 28370, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 72h:12m:52s remains)
INFO - root - 2017-12-09 13:30:27.644359: step 28380, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 69h:28m:58s remains)
INFO - root - 2017-12-09 13:30:35.992877: step 28390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:46m:33s remains)
INFO - root - 2017-12-09 13:30:44.584964: step 28400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 73h:10m:13s remains)
2017-12-09 13:30:45.401958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033773787 -0.003375323 -0.0033752979 -0.0033749454 -0.0033746983 -0.0033747964 -0.00337511 -0.0033755593 -0.0033761531 -0.003376371 -0.0033761812 -0.0033760325 -0.0033755549 -0.0033745624 -0.0033733619][-0.0033743815 -0.0033720559 -0.0033719337 -0.003371526 -0.0033715062 -0.0033717609 -0.0033724026 -0.0033733044 -0.003374724 -0.0033760443 -0.0033765563 -0.0033766453 -0.0033761347 -0.0033745961 -0.003372598][-0.003372604 -0.0033691893 -0.0033686839 -0.0033682403 -0.0033686103 -0.0033692252 -0.0033699726 -0.0033714876 -0.0033743363 -0.0033771605 -0.0033787445 -0.0033791065 -0.0033784348 -0.0033763894 -0.0033735551][-0.0033709689 -0.0033682778 -0.0033679148 -0.0033666997 -0.0033657253 -0.0033648205 -0.003364149 -0.0033658443 -0.0033712971 -0.0033776562 -0.0033815675 -0.003382561 -0.0033817084 -0.0033788828 -0.003375029][-0.0033691528 -0.0033650177 -0.003362685 -0.0033603343 -0.0033580617 -0.003354715 -0.0033518756 -0.0033534346 -0.0033614493 -0.0033727058 -0.0033822141 -0.0033871077 -0.0033868761 -0.003383074 -0.0033777964][-0.0033672103 -0.0033625069 -0.0033585732 -0.0033519671 -0.0033456443 -0.0033400145 -0.0033368506 -0.0033377651 -0.0033458835 -0.0033601073 -0.0033742997 -0.0033850756 -0.0033888686 -0.0033868563 -0.0033812008][-0.0033643865 -0.0033579459 -0.0033522469 -0.0033445258 -0.003337289 -0.003330668 -0.0033260135 -0.0033236754 -0.0033267704 -0.00333796 -0.0033539366 -0.0033708632 -0.0033812858 -0.0033852668 -0.0033830733][-0.0033642773 -0.0033564237 -0.0033491559 -0.0033403733 -0.0033334114 -0.0033284253 -0.0033258433 -0.0033225203 -0.0033200835 -0.0033228279 -0.0033322908 -0.0033495352 -0.0033665837 -0.0033785729 -0.0033823922][-0.0033766993 -0.0033691453 -0.0033609958 -0.0033508963 -0.0033430336 -0.0033372922 -0.0033339409 -0.0033291213 -0.0033244826 -0.0033229238 -0.0033266307 -0.0033379437 -0.0033542272 -0.0033709488 -0.003380578][-0.0033935842 -0.0033879466 -0.0033814132 -0.0033736085 -0.0033664007 -0.0033597816 -0.0033548924 -0.0033484951 -0.0033427549 -0.0033388003 -0.0033388012 -0.0033457265 -0.0033574153 -0.0033713731 -0.0033811035][-0.0034052394 -0.0034020564 -0.0033975423 -0.0033921802 -0.0033873906 -0.0033821827 -0.0033776497 -0.0033724227 -0.0033673304 -0.0033628705 -0.0033605488 -0.003362919 -0.0033693372 -0.0033776604 -0.003383988][-0.003406707 -0.0034045244 -0.0034019626 -0.0033991237 -0.0033966524 -0.0033937434 -0.0033910335 -0.0033879853 -0.0033848893 -0.003381765 -0.0033795268 -0.0033798763 -0.0033822041 -0.0033857862 -0.0033887434][-0.0034040003 -0.0034021672 -0.0034003437 -0.0033982631 -0.0033965418 -0.0033949623 -0.0033937288 -0.00339244 -0.0033916242 -0.0033907343 -0.0033895434 -0.0033891595 -0.0033897865 -0.0033909318 -0.0033925711][-0.0033997309 -0.003398336 -0.0033974014 -0.003396278 -0.0033952591 -0.003394583 -0.0033941877 -0.0033935842 -0.0033930833 -0.0033927176 -0.0033921255 -0.0033916642 -0.0033921083 -0.0033929835 -0.0033944931][-0.0033944896 -0.0033931802 -0.0033928573 -0.0033925553 -0.0033922212 -0.0033922608 -0.0033923932 -0.0033923476 -0.0033922549 -0.0033920959 -0.0033917711 -0.0033913681 -0.003391484 -0.0033921748 -0.0033935574]]...]
INFO - root - 2017-12-09 13:30:54.000061: step 28410, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 73h:55m:02s remains)
INFO - root - 2017-12-09 13:31:02.758521: step 28420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:27m:54s remains)
INFO - root - 2017-12-09 13:31:11.563995: step 28430, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 73h:12m:08s remains)
INFO - root - 2017-12-09 13:31:20.112858: step 28440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 74h:09m:04s remains)
INFO - root - 2017-12-09 13:31:28.724056: step 28450, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 72h:27m:44s remains)
INFO - root - 2017-12-09 13:31:37.289391: step 28460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:36m:13s remains)
INFO - root - 2017-12-09 13:31:45.981890: step 28470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:33m:50s remains)
INFO - root - 2017-12-09 13:31:54.597475: step 28480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:13m:57s remains)
INFO - root - 2017-12-09 13:32:03.088764: step 28490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:28m:26s remains)
INFO - root - 2017-12-09 13:32:11.660404: step 28500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 70h:45m:03s remains)
2017-12-09 13:32:12.513817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034027102 -0.003401316 -0.0034013418 -0.0034015046 -0.0034013467 -0.0033995116 -0.0033945297 -0.0033875408 -0.0033824781 -0.0033828113 -0.0033883208 -0.0033952037 -0.0033998296 -0.0034014895 -0.0034014841][-0.0034028238 -0.0034014552 -0.0034014329 -0.003401329 -0.0034001477 -0.003395315 -0.0033858262 -0.0033743472 -0.003367102 -0.0033685665 -0.0033780234 -0.0033895837 -0.0033976778 -0.0034013567 -0.0034019763][-0.0034038795 -0.0034026583 -0.0034025251 -0.0034014566 -0.0033971877 -0.0033860332 -0.0033678787 -0.0033486653 -0.0033372478 -0.0033410259 -0.0033573995 -0.0033775035 -0.0033927255 -0.0034004669 -0.0034027209][-0.0034045263 -0.0034033509 -0.0034022483 -0.0033972776 -0.003383548 -0.0033572919 -0.0033206167 -0.0032862897 -0.0032685709 -0.0032775868 -0.0033077437 -0.0033458695 -0.0033771947 -0.0033954105 -0.0034023311][-0.0034044089 -0.0034024138 -0.003397475 -0.0033814816 -0.0033466471 -0.003289606 -0.003221435 -0.0031651969 -0.0031413147 -0.0031619067 -0.003217655 -0.0032871785 -0.0033467081 -0.0033835142 -0.0033995039][-0.0034019449 -0.0033973393 -0.0033838782 -0.003347982 -0.0032789994 -0.0031812119 -0.0030783869 -0.0030061849 -0.0029862833 -0.0030259385 -0.0031119261 -0.0032158452 -0.0033073726 -0.0033667213 -0.003393983][-0.0033938696 -0.0033846409 -0.0033586465 -0.0032981804 -0.0031934017 -0.0030629919 -0.0029420829 -0.0028687585 -0.002862178 -0.0029228909 -0.0030336399 -0.0031629098 -0.0032782166 -0.0033544707 -0.0033900987][-0.0033813962 -0.0033658068 -0.0033281667 -0.0032493307 -0.0031255151 -0.0029841287 -0.0028651673 -0.0028031603 -0.0028120587 -0.0028873188 -0.0030093757 -0.0031486035 -0.0032725183 -0.0033537515 -0.0033909772][-0.0033722268 -0.0033512104 -0.0033072864 -0.0032250148 -0.0031058758 -0.0029764678 -0.0028735697 -0.0028258816 -0.002844499 -0.0029233382 -0.0030434397 -0.0031762626 -0.0032908733 -0.0033628256 -0.0033941036][-0.0033735316 -0.0033514479 -0.0033091665 -0.0032365639 -0.0031382705 -0.0030356315 -0.0029567631 -0.0029241736 -0.0029473461 -0.0030206833 -0.0031263186 -0.0032373851 -0.0033269138 -0.0033786783 -0.003398957][-0.0033827242 -0.0033641723 -0.0033309681 -0.0032774592 -0.0032076407 -0.0031370544 -0.0030854084 -0.0030689728 -0.0030932236 -0.0031529502 -0.0032318954 -0.0033081318 -0.0033638619 -0.0033929786 -0.0034024275][-0.0033924757 -0.0033800032 -0.0033596547 -0.0033280309 -0.0032874334 -0.003247259 -0.0032209279 -0.0032177349 -0.0032395674 -0.0032797218 -0.0033256498 -0.0033648466 -0.0033899592 -0.0034011863 -0.0034036154][-0.0033986373 -0.0033925488 -0.0033835534 -0.0033695952 -0.0033518393 -0.0033346054 -0.0033251205 -0.0033276132 -0.0033415074 -0.003361119 -0.0033799163 -0.003393746 -0.0034011062 -0.0034033447 -0.0034029544][-0.0034012801 -0.0033990548 -0.0033965611 -0.0033929585 -0.0033881706 -0.0033835715 -0.0033810521 -0.0033826241 -0.0033881625 -0.00339461 -0.0033993872 -0.00340216 -0.0034028792 -0.0034023339 -0.003401451][-0.00340192 -0.0034009586 -0.0034008771 -0.0034008326 -0.0034006059 -0.0033999712 -0.0033988149 -0.0033983041 -0.0033991563 -0.0034003062 -0.0034009977 -0.0034011933 -0.0034011453 -0.0034007451 -0.0034002247]]...]
INFO - root - 2017-12-09 13:32:20.843964: step 28510, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:50m:17s remains)
INFO - root - 2017-12-09 13:32:29.410149: step 28520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:34m:02s remains)
INFO - root - 2017-12-09 13:32:38.212152: step 28530, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 76h:01m:01s remains)
INFO - root - 2017-12-09 13:32:46.556638: step 28540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:11m:00s remains)
INFO - root - 2017-12-09 13:32:55.170029: step 28550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 75h:20m:16s remains)
INFO - root - 2017-12-09 13:33:03.802832: step 28560, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 74h:39m:23s remains)
INFO - root - 2017-12-09 13:33:12.623435: step 28570, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.872 sec/batch; 73h:35m:19s remains)
INFO - root - 2017-12-09 13:33:21.238275: step 28580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:46m:32s remains)
INFO - root - 2017-12-09 13:33:29.721984: step 28590, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 69h:45m:07s remains)
INFO - root - 2017-12-09 13:33:38.088685: step 28600, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 70h:14m:27s remains)
2017-12-09 13:33:38.906691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028143441 -0.0028731264 -0.0030248133 -0.003117613 -0.0031253691 -0.0032023096 -0.0032320484 -0.0032007461 -0.0030753575 -0.0028726463 -0.0026363218 -0.0024728912 -0.0023888024 -0.0024220753 -0.0023247539][-0.0027554044 -0.0027873209 -0.0028827551 -0.0029454022 -0.0029301569 -0.0030287388 -0.0030444164 -0.0029839852 -0.0028005429 -0.0025264185 -0.0022492076 -0.0020359473 -0.0018543632 -0.0018696516 -0.0016538653][-0.0025909971 -0.0026676368 -0.0027351482 -0.0027625575 -0.0027195322 -0.0028214289 -0.002804715 -0.0026819017 -0.0023574012 -0.0019545075 -0.0015872698 -0.0013275482 -0.0010512767 -0.0010296803 -0.00075313356][-0.0020864429 -0.0023207758 -0.002474959 -0.0025471079 -0.0025137784 -0.0026072906 -0.0025417435 -0.0023264764 -0.0017981117 -0.0011899967 -0.00063428329 -0.00029047579 5.11345e-05 7.1033137e-05 0.0002947473][-0.0011251252 -0.0016455632 -0.0020453171 -0.0022715076 -0.0023234161 -0.0024483167 -0.0023408935 -0.0020199437 -0.0012714313 -0.00041989423 0.00036291312 0.00085209589 0.0012265267 0.0012202472 0.0012479727][2.2214372e-05 -0.00080173858 -0.0015206516 -0.0019732616 -0.0021768734 -0.0023526687 -0.0022219429 -0.0018217195 -0.00092512579 0.00010813237 0.001070891 0.0016958495 0.0020614401 0.002035 0.001901703][0.00091351289 -0.00014475477 -0.0011229168 -0.0017560999 -0.002101148 -0.0023221096 -0.0021969508 -0.0017729187 -0.00083419634 0.00027093221 0.0012742239 0.0019415664 0.0022597439 0.0022159033 0.0020105354][0.0012806784 0.00010092906 -0.0010080938 -0.0017138129 -0.0021220574 -0.0023310443 -0.0022263413 -0.0018568892 -0.00097638858 8.3546853e-05 0.0010252264 0.0016365997 0.0018415074 0.001757713 0.0015221471][0.0011253697 -2.9897783e-05 -0.0011341365 -0.0018239593 -0.0022082669 -0.0023621588 -0.0022881483 -0.0019846733 -0.0012185227 -0.00024995045 0.00059883995 0.001101837 0.0011420362 0.00095948717 0.00064379536][0.00061657839 -0.00032092724 -0.0012819148 -0.0019087193 -0.0022277136 -0.0023223748 -0.0022660478 -0.0020011561 -0.0013375701 -0.00046001538 0.00031404104 0.00074508879 0.00063901348 0.00029939227 -0.0001793846][-6.8521826e-05 -0.00060888822 -0.0012723894 -0.001762914 -0.0019928191 -0.00203234 -0.0019703566 -0.0017070699 -0.0011255026 -0.00034379633 0.00036223419 0.00074211182 0.00056461431 0.00010727649 -0.00051943515][-0.00077405875 -0.00084466324 -0.0011070245 -0.0013545838 -0.0014360882 -0.0014083276 -0.0013234888 -0.0010725795 -0.00057367329 7.5194286e-05 0.00066781929 0.0009784163 0.00081590842 0.00033854926 -0.00031468645][-0.0014074189 -0.0010883291 -0.00095539656 -0.00088993576 -0.00076204631 -0.00062797451 -0.00050749094 -0.00031771557 4.8235059e-05 0.00051878626 0.00093673076 0.0011294088 0.0010150895 0.00062501663 0.0001022513][-0.0019382279 -0.0014184075 -0.0010121998 -0.00066946517 -0.000332291 -7.8278128e-05 6.2485691e-05 0.00016653631 0.00037741149 0.00066490239 0.00088740792 0.00094255619 0.00086380821 0.00061346265 0.00029362156][-0.0023536417 -0.0018336697 -0.0013310667 -0.00084176823 -0.0003683914 -3.123749e-05 0.00010427134 0.00013097958 0.0002052465 0.00034818053 0.00042898278 0.00039167679 0.00032298896 0.00018536532 2.6285648e-05]]...]
INFO - root - 2017-12-09 13:33:47.208164: step 28610, loss = 0.91, batch loss = 0.70 (10.8 examples/sec; 0.741 sec/batch; 62h:33m:06s remains)
INFO - root - 2017-12-09 13:33:55.972338: step 28620, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 75h:18m:36s remains)
INFO - root - 2017-12-09 13:34:04.614135: step 28630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:42m:05s remains)
INFO - root - 2017-12-09 13:34:13.177670: step 28640, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:07m:34s remains)
INFO - root - 2017-12-09 13:34:21.753973: step 28650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:18m:29s remains)
INFO - root - 2017-12-09 13:34:30.461014: step 28660, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:42m:17s remains)
INFO - root - 2017-12-09 13:34:39.083787: step 28670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 70h:35m:21s remains)
INFO - root - 2017-12-09 13:34:47.704738: step 28680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:50m:56s remains)
INFO - root - 2017-12-09 13:34:56.249709: step 28690, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 76h:09m:57s remains)
INFO - root - 2017-12-09 13:35:05.119362: step 28700, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 69h:01m:53s remains)
2017-12-09 13:35:06.055497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033593283 -0.0033571504 -0.0033573143 -0.0033575098 -0.0033576803 -0.0033578712 -0.0033577653 -0.0033575112 -0.003357247 -0.0033570556 -0.0033568093 -0.0033566842 -0.0033566786 -0.003356684 -0.0033566635][-0.0033576209 -0.0033556935 -0.003356305 -0.0033569925 -0.0033575632 -0.0033577797 -0.0033574612 -0.0033567909 -0.0033561529 -0.003355559 -0.0033549196 -0.0033545094 -0.0033543399 -0.0033543652 -0.0033544106][-0.003358471 -0.0033570603 -0.0033584267 -0.00336001 -0.0033612624 -0.0033616808 -0.0033610188 -0.0033596838 -0.0033582496 -0.0033568745 -0.0033555843 -0.0033546875 -0.0033543063 -0.0033543366 -0.0033544526][-0.0033591185 -0.0033584707 -0.0033609062 -0.0033636936 -0.0033660687 -0.0033670182 -0.0033662694 -0.0033640694 -0.0033614228 -0.0033588517 -0.0033565068 -0.0033548812 -0.0033541734 -0.0033542265 -0.0033544644][-0.0033598372 -0.0033598873 -0.0033635297 -0.0033676191 -0.0033713845 -0.0033730431 -0.003372363 -0.0033693453 -0.0033653774 -0.0033613262 -0.0033576859 -0.003355223 -0.0033541142 -0.0033540577 -0.0033543603][-0.0033604698 -0.003361054 -0.0033657984 -0.003371113 -0.0033761938 -0.00337872 -0.00337813 -0.00337439 -0.0033691975 -0.0033638952 -0.0033592873 -0.0033560121 -0.003354382 -0.0033540807 -0.0033542935][-0.003360539 -0.0033615653 -0.0033669488 -0.0033732166 -0.0033792164 -0.0033824216 -0.0033818777 -0.0033777773 -0.0033718403 -0.0033657779 -0.0033605429 -0.0033567031 -0.0033546297 -0.0033540132 -0.0033541587][-0.0033600982 -0.0033613148 -0.003366854 -0.0033733405 -0.0033796909 -0.0033832022 -0.0033828737 -0.0033787983 -0.0033727109 -0.0033665218 -0.0033609208 -0.0033568619 -0.0033545389 -0.0033537499 -0.003353908][-0.0033593967 -0.0033602964 -0.0033654687 -0.0033712806 -0.0033771624 -0.0033806842 -0.0033807792 -0.00337734 -0.0033718657 -0.0033660247 -0.0033607569 -0.0033569364 -0.0033545953 -0.0033537662 -0.0033538467][-0.003358613 -0.0033587862 -0.0033630733 -0.0033676743 -0.0033724098 -0.0033756006 -0.0033759698 -0.0033734513 -0.003369062 -0.0033642121 -0.0033599881 -0.0033567927 -0.0033547939 -0.003354039 -0.0033539548][-0.0033577168 -0.0033570251 -0.0033601394 -0.0033633083 -0.0033666231 -0.0033692564 -0.0033697647 -0.0033680741 -0.0033649413 -0.0033615336 -0.0033584391 -0.0033560877 -0.003354606 -0.0033539878 -0.00335389][-0.0033570114 -0.0033555252 -0.0033574759 -0.0033593522 -0.0033614596 -0.0033632738 -0.0033637341 -0.00336268 -0.0033606933 -0.0033584947 -0.0033564558 -0.0033549827 -0.0033541254 -0.003353761 -0.0033537741][-0.0033567192 -0.0033543929 -0.0033554411 -0.0033563797 -0.0033575674 -0.003358691 -0.0033590987 -0.0033585085 -0.0033573958 -0.0033561657 -0.0033549676 -0.0033541152 -0.0033537103 -0.0033535836 -0.0033537103][-0.0033568302 -0.0033538602 -0.0033543217 -0.0033546851 -0.003355236 -0.0033558751 -0.0033560335 -0.0033556861 -0.0033551357 -0.0033545843 -0.0033539762 -0.0033535678 -0.0033534879 -0.0033535445 -0.0033537161][-0.0033570067 -0.003353744 -0.0033538197 -0.0033539161 -0.0033541648 -0.0033544675 -0.0033544735 -0.0033542397 -0.0033539925 -0.0033537431 -0.0033534484 -0.0033532802 -0.0033533638 -0.003353525 -0.0033537128]]...]
INFO - root - 2017-12-09 13:35:14.555431: step 28710, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 66h:07m:38s remains)
INFO - root - 2017-12-09 13:35:23.168456: step 28720, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 72h:13m:00s remains)
INFO - root - 2017-12-09 13:35:31.643777: step 28730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:10m:12s remains)
INFO - root - 2017-12-09 13:35:40.024626: step 28740, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:41m:34s remains)
INFO - root - 2017-12-09 13:35:48.799285: step 28750, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:46m:08s remains)
INFO - root - 2017-12-09 13:35:57.449471: step 28760, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 68h:34m:07s remains)
INFO - root - 2017-12-09 13:36:06.141075: step 28770, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 74h:11m:55s remains)
INFO - root - 2017-12-09 13:36:14.836798: step 28780, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 73h:55m:44s remains)
INFO - root - 2017-12-09 13:36:23.358537: step 28790, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 72h:57m:11s remains)
INFO - root - 2017-12-09 13:36:32.021495: step 28800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:06m:54s remains)
2017-12-09 13:36:32.862518: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012118125 0.010335553 0.00793162 0.0052137091 0.0024292418 9.52729e-05 -0.0016151489 -0.0026136027 -0.0030832156 -0.0032300602 -0.0032776394 -0.0033451249 -0.0033807703 -0.0033845003 -0.0033867625][0.010719126 0.0095256828 0.0076493337 0.0052529271 0.0027251514 0.00044992031 -0.0012636597 -0.0023531704 -0.0029135284 -0.0031170477 -0.0031842103 -0.0032344768 -0.003284082 -0.0033347437 -0.0033665379][0.0068325126 0.0061698933 0.0049281609 0.0033261892 0.0017454992 0.00042812969 -0.00058596488 -0.0012749268 -0.0017050921 -0.0020033657 -0.0023292056 -0.0026407095 -0.0028896821 -0.0031081536 -0.0032511237][0.0025680163 0.0021253969 0.0012839693 0.00040761638 -0.00015794137 -0.00030649896 -0.00025982154 -0.00019489066 -0.00028910185 -0.0006564681 -0.0013124568 -0.0020495255 -0.0026322016 -0.0030275066 -0.0032408379][0.00046733627 -0.00023613498 -0.0010484757 -0.0016111777 -0.0019288171 -0.0017397513 -0.0013687625 -0.0010210157 -0.00088109681 -0.0011036159 -0.001534172 -0.0021336786 -0.0026122446 -0.0029885755 -0.0032257538][0.0012592296 0.00096247718 0.00038639712 -0.00037218817 -0.0012196589 -0.0018794901 -0.002279999 -0.0023807264 -0.002348667 -0.0023179823 -0.002372267 -0.002538499 -0.002742548 -0.0030013067 -0.0032013939][-0.00098731485 -0.00041991728 0.00010550045 0.00039173733 0.0004245569 0.00013500452 -0.0003778222 -0.00097657973 -0.0015306583 -0.0019885222 -0.0023676162 -0.0027009398 -0.0029639974 -0.0031626513 -0.003278692][-0.000846345 -0.00081986468 -0.00064002909 -0.00043689646 2.494175e-05 0.00043909159 0.00069916807 0.00071673887 0.00045496994 -2.8593699e-05 -0.00074202777 -0.0014418042 -0.0021273461 -0.0027036157 -0.0030896296][0.0011256798 0.00091750221 0.00070177345 0.00060710125 0.0009825821 0.001682369 0.0023511895 0.002845502 0.0028549596 0.0023455382 0.0012705685 -2.4370383e-05 -0.001204229 -0.0021640016 -0.0028033599][0.0044228989 0.0040979506 0.0037107724 0.0035580476 0.0035164575 0.0037257508 0.0040074848 0.0044234558 0.0045848046 0.0042364807 0.0032802783 0.0017676177 0.00012767734 -0.0013623964 -0.002414745][0.0095386375 0.0094861668 0.0090740323 0.0086678881 0.0082043484 0.0075710178 0.0068750293 0.0063388743 0.0057006744 0.0048869192 0.0036937231 0.0022029984 0.00056236237 -0.00097081694 -0.002116696][0.010786833 0.010910286 0.010924585 0.01111789 0.011297731 0.011408702 0.011238659 0.010562327 0.0093108937 0.0075630504 0.0053846785 0.0030240649 0.00083524873 -0.00088089239 -0.0020913521][0.014316645 0.013992531 0.013520582 0.013233384 0.013155689 0.013215316 0.013493022 0.013574192 0.013010295 0.01153541 0.0090942662 0.0060812873 0.0029312582 0.00030517019 -0.0015568534][0.019830225 0.019509763 0.019149642 0.018965596 0.018891061 0.018848116 0.018659662 0.018181834 0.017018573 0.015027512 0.01210784 0.008504279 0.0047818017 0.001551738 -0.00083077536][0.027522381 0.027220285 0.026614364 0.026077699 0.025544476 0.024979739 0.024250712 0.023157662 0.021305345 0.018593056 0.014932971 0.010637669 0.0062516346 0.0024843179 -0.00032319454]]...]
INFO - root - 2017-12-09 13:36:41.498778: step 28810, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 64h:24m:38s remains)
INFO - root - 2017-12-09 13:36:50.165212: step 28820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:26m:54s remains)
INFO - root - 2017-12-09 13:36:58.682747: step 28830, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 69h:05m:03s remains)
INFO - root - 2017-12-09 13:37:06.959086: step 28840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:35m:47s remains)
INFO - root - 2017-12-09 13:37:15.444672: step 28850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:04m:39s remains)
INFO - root - 2017-12-09 13:37:24.110396: step 28860, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:41m:40s remains)
INFO - root - 2017-12-09 13:37:32.561152: step 28870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:09m:04s remains)
INFO - root - 2017-12-09 13:37:41.130586: step 28880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:06m:43s remains)
INFO - root - 2017-12-09 13:37:49.639060: step 28890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:51m:47s remains)
INFO - root - 2017-12-09 13:37:58.330591: step 28900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:52m:34s remains)
2017-12-09 13:37:59.152585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033717225 -0.0033693111 -0.0033691311 -0.0033691193 -0.0033691302 -0.0033692122 -0.0033692832 -0.0033693879 -0.0033694517 -0.0033694289 -0.0033694264 -0.0033694033 -0.0033694031 -0.0033693763 -0.0033693749][-0.0033696813 -0.0033670736 -0.0033668876 -0.0033669223 -0.0033669942 -0.003367089 -0.0033670911 -0.0033670766 -0.0033670694 -0.0033669819 -0.0033668845 -0.003366801 -0.0033667798 -0.0033668026 -0.0033668438][-0.0033700147 -0.0033673295 -0.0033672305 -0.0033673597 -0.0033674277 -0.0033673791 -0.0033671698 -0.0033669339 -0.0033667011 -0.0033665055 -0.0033663902 -0.0033663395 -0.0033663886 -0.0033664543 -0.0033665488][-0.0033700047 -0.0033672992 -0.0033672578 -0.0033674587 -0.0033675926 -0.0033675942 -0.0033674489 -0.0033672357 -0.0033670235 -0.0033667586 -0.0033664692 -0.0033661914 -0.0033661288 -0.0033661376 -0.0033662687][-0.0033700182 -0.0033673171 -0.0033673216 -0.0033676079 -0.0033678727 -0.0033679772 -0.003367926 -0.0033678354 -0.0033676706 -0.0033673618 -0.0033669772 -0.0033665434 -0.0033662328 -0.0033660291 -0.0033661306][-0.0033700429 -0.0033673565 -0.0033674305 -0.0033678149 -0.003368271 -0.0033684364 -0.0033683889 -0.0033683509 -0.0033681635 -0.0033678189 -0.0033674296 -0.0033668494 -0.0033662952 -0.0033659516 -0.003366034][-0.0033699644 -0.0033673178 -0.0033674806 -0.0033679535 -0.0033685146 -0.0033686953 -0.0033685791 -0.0033683751 -0.0033680657 -0.0033675791 -0.0033670438 -0.0033664592 -0.0033660482 -0.0033658356 -0.0033659737][-0.0033698294 -0.0033672662 -0.0033674925 -0.0033680429 -0.0033687309 -0.00336904 -0.0033689938 -0.0033686417 -0.0033680846 -0.0033672797 -0.0033665905 -0.003366027 -0.0033657129 -0.0033656165 -0.0033658447][-0.0033695642 -0.0033671088 -0.0033674191 -0.0033680811 -0.0033689714 -0.003369577 -0.0033697165 -0.0033694881 -0.0033689016 -0.003368048 -0.0033671618 -0.0033664422 -0.0033660198 -0.0033658242 -0.0033659784][-0.0033693614 -0.0033669181 -0.0033672671 -0.0033679495 -0.0033688913 -0.0033696592 -0.0033701335 -0.0033702778 -0.0033699612 -0.0033691579 -0.0033681388 -0.0033672675 -0.0033666689 -0.0033662997 -0.0033663092][-0.003368909 -0.0033664189 -0.0033668417 -0.003367411 -0.0033682303 -0.0033689728 -0.0033695311 -0.00336984 -0.0033697316 -0.0033691842 -0.0033683865 -0.0033676473 -0.0033671397 -0.0033667916 -0.00336667][-0.003368597 -0.0033659847 -0.0033663006 -0.0033667053 -0.0033672776 -0.0033677826 -0.0033681325 -0.0033683723 -0.0033684787 -0.0033682855 -0.0033678778 -0.0033675369 -0.0033673085 -0.0033670564 -0.003366879][-0.0033685598 -0.0033657462 -0.0033659781 -0.0033661765 -0.0033664308 -0.0033666959 -0.00336692 -0.003367096 -0.0033672135 -0.0033672112 -0.0033671437 -0.0033670529 -0.0033670235 -0.0033669402 -0.0033668473][-0.003368685 -0.0033656394 -0.0033658945 -0.0033659809 -0.0033660964 -0.0033662394 -0.0033663637 -0.0033664703 -0.0033665458 -0.0033665809 -0.0033666021 -0.0033666049 -0.0033666503 -0.0033666887 -0.0033667025][-0.0033686759 -0.003365519 -0.0033657521 -0.0033658429 -0.0033659434 -0.0033660559 -0.0033661555 -0.0033662207 -0.003366271 -0.0033663115 -0.0033663476 -0.0033663621 -0.00336641 -0.0033664638 -0.0033665206]]...]
INFO - root - 2017-12-09 13:38:07.667420: step 28910, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 62h:48m:35s remains)
INFO - root - 2017-12-09 13:38:16.243422: step 28920, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.860 sec/batch; 72h:33m:00s remains)
INFO - root - 2017-12-09 13:38:24.736540: step 28930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 70h:36m:32s remains)
INFO - root - 2017-12-09 13:38:32.995380: step 28940, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 72h:22m:07s remains)
INFO - root - 2017-12-09 13:38:41.739203: step 28950, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:31m:15s remains)
INFO - root - 2017-12-09 13:38:50.414144: step 28960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:29m:48s remains)
INFO - root - 2017-12-09 13:38:59.082220: step 28970, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:44m:54s remains)
INFO - root - 2017-12-09 13:39:07.757713: step 28980, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:31m:11s remains)
INFO - root - 2017-12-09 13:39:16.219520: step 28990, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 69h:08m:27s remains)
INFO - root - 2017-12-09 13:39:24.966932: step 29000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:56m:58s remains)
2017-12-09 13:39:25.807876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033847652 -0.0033838295 -0.003383697 -0.0033838088 -0.0033839552 -0.003384127 -0.003384283 -0.0033844358 -0.0033845836 -0.0033848803 -0.0033854349 -0.0033860167 -0.0033860777 -0.003385863 -0.0033856514][-0.0033829112 -0.0033818178 -0.0033816155 -0.0033817384 -0.0033819014 -0.0033821054 -0.0033823575 -0.0033825345 -0.0033826765 -0.0033829918 -0.0033835091 -0.0033840921 -0.003384212 -0.0033841813 -0.0033841117][-0.0033828861 -0.0033818106 -0.0033817834 -0.003381968 -0.0033821438 -0.0033822455 -0.0033823345 -0.0033823939 -0.0033825175 -0.0033828942 -0.0033835198 -0.0033842889 -0.0033846658 -0.003384881 -0.0033846085][-0.0033830008 -0.0033819929 -0.0033821054 -0.0033824572 -0.0033826327 -0.0033825787 -0.0033825003 -0.0033824095 -0.0033824623 -0.0033828223 -0.0033835096 -0.0033844926 -0.0033852418 -0.0033857336 -0.0033851874][-0.0033833028 -0.0033824851 -0.0033827883 -0.0033832868 -0.0033833911 -0.00338316 -0.0033828656 -0.0033826123 -0.0033825957 -0.0033829121 -0.0033835452 -0.0033844658 -0.0033854402 -0.0033860505 -0.0033856765][-0.0033836197 -0.003382972 -0.0033834637 -0.0033841159 -0.0033843045 -0.0033841634 -0.0033838467 -0.0033834667 -0.0033832898 -0.0033833783 -0.003383735 -0.0033845366 -0.0033854996 -0.0033863543 -0.0033865927][-0.0033838877 -0.0033834523 -0.0033841133 -0.0033848942 -0.0033852723 -0.0033853333 -0.0033850481 -0.0033843794 -0.0033837392 -0.0033835811 -0.0033837734 -0.0033844481 -0.0033854088 -0.0033865145 -0.0033872358][-0.0033840693 -0.0033838684 -0.0033848237 -0.0033858274 -0.0033865271 -0.0033867934 -0.0033863287 -0.0033850719 -0.0033838619 -0.0033834968 -0.0033836097 -0.0033841685 -0.0033849641 -0.003385915 -0.0033867352][-0.0033841508 -0.003384114 -0.0033851529 -0.0033864863 -0.003387765 -0.0033883052 -0.0033875164 -0.0033856302 -0.0033840032 -0.0033835254 -0.0033834942 -0.0033836828 -0.0033841538 -0.003384897 -0.0033857406][-0.0033841154 -0.003384013 -0.0033851305 -0.0033867059 -0.0033883885 -0.0033891923 -0.0033883892 -0.0033867119 -0.003385067 -0.0033841114 -0.0033836681 -0.003383487 -0.0033837422 -0.0033843033 -0.0033850842][-0.0033838493 -0.0033837294 -0.0033848756 -0.0033864868 -0.00338828 -0.003389457 -0.0033895141 -0.0033887317 -0.0033873396 -0.0033858023 -0.0033846393 -0.0033840821 -0.0033838341 -0.0033839296 -0.0033846011][-0.0033833813 -0.0033832525 -0.0033844314 -0.0033859911 -0.0033876058 -0.0033889527 -0.0033896463 -0.003389678 -0.0033887427 -0.00338727 -0.0033858425 -0.0033848009 -0.003384195 -0.0033838667 -0.0033841783][-0.0033828516 -0.0033824958 -0.00338357 -0.0033848649 -0.0033862158 -0.0033874849 -0.0033884414 -0.0033889641 -0.0033884991 -0.0033874826 -0.00338629 -0.0033852009 -0.0033845298 -0.0033840188 -0.0033840172][-0.003382514 -0.0033817992 -0.0033824462 -0.0033832965 -0.0033841797 -0.0033852868 -0.0033864453 -0.003387284 -0.0033872456 -0.003386854 -0.0033861792 -0.0033852425 -0.0033843375 -0.0033837138 -0.0033836707][-0.0033823941 -0.0033813326 -0.0033815489 -0.0033820311 -0.0033825671 -0.0033834698 -0.0033846151 -0.0033856314 -0.0033860775 -0.0033863559 -0.0033860914 -0.0033850935 -0.003383863 -0.0033831859 -0.0033830605]]...]
INFO - root - 2017-12-09 13:39:34.379590: step 29010, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 72h:43m:28s remains)
INFO - root - 2017-12-09 13:39:42.912641: step 29020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:49m:26s remains)
INFO - root - 2017-12-09 13:39:51.575882: step 29030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:45m:08s remains)
INFO - root - 2017-12-09 13:40:00.071530: step 29040, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 69h:52m:16s remains)
INFO - root - 2017-12-09 13:40:08.692233: step 29050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:24m:54s remains)
INFO - root - 2017-12-09 13:40:17.396881: step 29060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:03m:54s remains)
INFO - root - 2017-12-09 13:40:26.184294: step 29070, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 69h:09m:07s remains)
INFO - root - 2017-12-09 13:40:34.769166: step 29080, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 69h:36m:31s remains)
INFO - root - 2017-12-09 13:40:43.353532: step 29090, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:00m:33s remains)
INFO - root - 2017-12-09 13:40:52.116673: step 29100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 74h:08m:52s remains)
2017-12-09 13:40:53.052583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033804057 -0.0033793745 -0.0033798725 -0.0033803177 -0.0033786553 -0.0033675614 -0.0033396226 -0.0033008922 -0.0032730822 -0.0032751602 -0.003305438 -0.003344109 -0.0033707398 -0.003382755 -0.0033850423][-0.0033766942 -0.0033752539 -0.0033756758 -0.003375334 -0.0033688834 -0.0033421158 -0.0032823235 -0.0032020058 -0.003144729 -0.0031503672 -0.0032165877 -0.0033002966 -0.0033568004 -0.0033810101 -0.0033845406][-0.0033537236 -0.0033498006 -0.0033429104 -0.0032994838 -0.0031783702 -0.0029689081 -0.0027328264 -0.0025830376 -0.0026066655 -0.0027904676 -0.0030357812 -0.0032365485 -0.0033446667 -0.0033823021 -0.00338675][-0.0033381549 -0.0033145125 -0.003223954 -0.0029356254 -0.0023265609 -0.0014484404 -0.00061904895 -0.00025767135 -0.00057029282 -0.0013767013 -0.0022680764 -0.0029185484 -0.0032512639 -0.003364889 -0.0033874644][-0.0033307292 -0.0032606805 -0.0029716599 -0.0021582744 -0.00059979828 0.0014886763 0.0033577927 0.0041182223 0.0033772704 0.0015350301 -0.000538595 -0.0021140454 -0.0029757344 -0.00330121 -0.003381982][-0.0031975 -0.0030121973 -0.0023813997 -0.00079424237 0.0020262334 0.0055976245 0.0086528752 0.0098219384 0.0085579958 0.005487978 0.0019445457 -0.00087808678 -0.0025211521 -0.00319011 -0.0033685002][-0.0028984719 -0.0024973743 -0.0014552048 0.0008422574 0.004673406 0.0093725212 0.013334684 0.014834279 0.013164571 0.0090798363 0.0042676567 0.00031636166 -0.0020669745 -0.0030771771 -0.0033533582][-0.0025795647 -0.0019836556 -0.00071731396 0.0018486124 0.0060216957 0.01112644 0.015478086 0.017214715 0.015476953 0.01097797 0.0055442755 0.000988794 -0.0018053709 -0.0030097517 -0.0033423863][-0.0024417769 -0.0017753041 -0.0005383133 0.0018201424 0.0056383624 0.010405408 0.014628138 0.016532371 0.015135271 0.0109432 0.0056469035 0.0010832571 -0.0017597594 -0.0029970095 -0.0033392042][-0.0025301203 -0.001915394 -0.00089390553 0.00095129618 0.003975234 0.0079243425 0.011681309 0.013701243 0.012907183 0.0095015205 0.0048780772 0.00075062737 -0.0018700771 -0.0030227376 -0.0033427679][-0.0027094351 -0.0021582125 -0.0013383687 -1.3917219e-05 0.0021116436 0.005014596 0.0080054924 0.00990125 0.0096604712 0.00719965 0.0035321643 0.00011533522 -0.0020991142 -0.0030798663 -0.0033503582][-0.0028555039 -0.0023578804 -0.0016680118 -0.00067272619 0.00082284724 0.0028637708 0.0050520669 0.0065412712 0.006467571 0.0046698567 0.0018829457 -0.00073816441 -0.0024284306 -0.0031647892 -0.0033597718][-0.0028722612 -0.0022901264 -0.0014811221 -0.000455437 0.00084857224 0.0024328274 0.0040048314 0.0049221418 0.0045596887 0.0028842008 0.000554895 -0.0014948051 -0.0027408674 -0.0032483686 -0.0033690983][-0.002677273 -0.0017622869 -0.00042360742 0.0012137746 0.0029927632 0.0046938122 0.0059257713 0.0061494219 0.0050271521 0.0028145313 0.00029346091 -0.0017054527 -0.0028405997 -0.0032767814 -0.0033719093][-0.0022635867 -0.00072006462 0.0016128509 0.0044395044 0.0072726957 0.0095666256 0.010736255 0.010285417 0.0081301611 0.0048489203 0.001459993 -0.0011310768 -0.0026133191 -0.003213037 -0.0033619176]]...]
INFO - root - 2017-12-09 13:41:01.616671: step 29110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:32m:10s remains)
INFO - root - 2017-12-09 13:41:10.089919: step 29120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:39m:13s remains)
INFO - root - 2017-12-09 13:41:18.642677: step 29130, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 73h:09m:51s remains)
INFO - root - 2017-12-09 13:41:27.135588: step 29140, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.716 sec/batch; 60h:20m:42s remains)
INFO - root - 2017-12-09 13:41:35.883914: step 29150, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:01m:46s remains)
INFO - root - 2017-12-09 13:41:44.521420: step 29160, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:27m:28s remains)
INFO - root - 2017-12-09 13:41:53.142710: step 29170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:04m:40s remains)
INFO - root - 2017-12-09 13:42:01.867177: step 29180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:56m:04s remains)
INFO - root - 2017-12-09 13:42:10.466528: step 29190, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.719 sec/batch; 60h:32m:34s remains)
INFO - root - 2017-12-09 13:42:19.011255: step 29200, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 70h:39m:30s remains)
2017-12-09 13:42:19.853835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013701171 -0.0020569405 -0.0025139381 -0.0028223044 -0.0027416786 -0.0021337389 -0.00090844859 0.00065694051 0.0022279313 0.00318289 0.0034989985 0.003468906 0.0033102287 0.0031186808 0.0026304168][-7.6757744e-05 -0.000807157 -0.0014330589 -0.001819491 -0.001718533 -0.0011132194 0.00031701359 0.00229092 0.0042735813 0.0055933627 0.006149163 0.0062798462 0.0063831504 0.0064261644 0.0059791128][0.0011598733 0.00097970758 0.00097522745 0.0010123393 0.001508195 0.0025498373 0.0045410586 0.0070674191 0.0097264182 0.011801086 0.01290028 0.013335032 0.013519673 0.013400724 0.012339355][0.0023797434 0.0032686787 0.0047873897 0.0065933946 0.0088822525 0.011450637 0.014835622 0.018597584 0.022301355 0.025107943 0.02658839 0.026898932 0.026404787 0.025045995 0.02208182][0.0036068773 0.0061335363 0.010022778 0.014592867 0.019890437 0.025302794 0.030974418 0.036291551 0.041042168 0.0442894 0.045678578 0.045182694 0.043092098 0.03940599 0.033529136][0.0045278226 0.0087384246 0.015143614 0.022751443 0.031263437 0.039686859 0.04782502 0.054774508 0.060346339 0.063866965 0.064954288 0.06333492 0.059179064 0.05265009 0.043571986][0.0052010096 0.010618405 0.018895341 0.028793953 0.0395812 0.050042916 0.059723306 0.067625083 0.073442005 0.076855779 0.07754796 0.075025886 0.069223687 0.060466092 0.049033254][0.0055955262 0.011592463 0.020607654 0.031405218 0.042887304 0.053893942 0.063823767 0.071678273 0.076995365 0.079916269 0.08006902 0.076915972 0.070200823 0.06035414 0.047950562][0.0053638513 0.011358341 0.020210087 0.030650061 0.041355949 0.05137448 0.060121406 0.066909947 0.071065135 0.073006853 0.072351977 0.068711631 0.061733965 0.05191046 0.04005713][0.0039184466 0.0092565045 0.01702458 0.026071642 0.03505579 0.0431735 0.049958091 0.055057302 0.057796173 0.058600888 0.057081129 0.053115159 0.046447545 0.037658531 0.027695918][0.0015461813 0.005575228 0.011470772 0.018311368 0.024972158 0.0308243 0.035497427 0.038871635 0.04038398 0.040311582 0.038370479 0.034624726 0.029048912 0.022252375 0.015095446][-0.00071405643 0.0017561852 0.0054455488 0.0097683258 0.013958689 0.017579958 0.020377127 0.022305526 0.022985213 0.0225629 0.020832682 0.017952014 0.014064036 0.0096948408 0.0054570432][-0.0022829692 -0.0010678191 0.00080513535 0.0030303509 0.0051856195 0.0070194495 0.0083965771 0.0093034022 0.0095358118 0.00914094 0.0080063045 0.0062685977 0.0041036038 0.001865271 -0.00011675595][-0.0030720646 -0.0026247385 -0.0018933234 -0.0009945014 -0.00011148723 0.00063837273 0.0011924864 0.0015348434 0.0015822286 0.0013508308 0.00079213083 -1.2933277e-05 -0.00094039785 -0.0018114202 -0.0025026342][-0.0033359565 -0.0032265624 -0.0030257155 -0.0027626138 -0.0024969869 -0.0022685681 -0.0021001794 -0.0020038574 -0.0020044039 -0.0020938036 -0.0022862919 -0.0025485572 -0.0028240422 -0.0030536058 -0.00321462]]...]
INFO - root - 2017-12-09 13:42:28.279061: step 29210, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:03m:48s remains)
INFO - root - 2017-12-09 13:42:36.864834: step 29220, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 76h:24m:54s remains)
INFO - root - 2017-12-09 13:42:45.545198: step 29230, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 72h:17m:27s remains)
INFO - root - 2017-12-09 13:42:54.211639: step 29240, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 73h:05m:20s remains)
INFO - root - 2017-12-09 13:43:02.336301: step 29250, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 69h:49m:26s remains)
INFO - root - 2017-12-09 13:43:11.002297: step 29260, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.838 sec/batch; 70h:33m:17s remains)
INFO - root - 2017-12-09 13:43:19.754968: step 29270, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 73h:22m:16s remains)
INFO - root - 2017-12-09 13:43:28.437465: step 29280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 72h:21m:02s remains)
INFO - root - 2017-12-09 13:43:37.170637: step 29290, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:40m:41s remains)
INFO - root - 2017-12-09 13:43:45.681108: step 29300, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 74h:37m:03s remains)
2017-12-09 13:43:46.542240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033646992 -0.0033623383 -0.0033621718 -0.003362275 -0.0033623965 -0.0033625676 -0.0033627306 -0.0033628209 -0.0033628042 -0.0033626806 -0.0033625634 -0.0033624908 -0.0033623644 -0.0033622799 -0.0033620913][-0.0033628577 -0.0033604344 -0.003360386 -0.003360596 -0.0033608126 -0.0033610649 -0.003361288 -0.0033614524 -0.0033614393 -0.0033614433 -0.0033615036 -0.0033615562 -0.0033615455 -0.0033614798 -0.0033612491][-0.0033627579 -0.0033604209 -0.0033605574 -0.003360841 -0.0033610724 -0.0033612722 -0.0033614293 -0.0033615935 -0.0033615977 -0.0033616379 -0.0033618195 -0.0033619786 -0.0033619995 -0.0033619418 -0.0033616761][-0.0033623879 -0.0033600535 -0.0033603283 -0.0033606943 -0.00336093 -0.0033610952 -0.0033611991 -0.0033612705 -0.0033612896 -0.003361315 -0.0033615581 -0.0033618268 -0.0033619248 -0.0033619117 -0.0033616212][-0.0033617383 -0.0033592805 -0.0033596214 -0.0033599893 -0.0033601418 -0.0033601702 -0.0033601965 -0.0033601285 -0.003360108 -0.0033601648 -0.0033605013 -0.0033609434 -0.0033611846 -0.0033613476 -0.0033611793][-0.0033609404 -0.00335829 -0.0033586358 -0.0033589886 -0.0033590565 -0.003358945 -0.0033588156 -0.0033585464 -0.0033583976 -0.003358474 -0.0033589138 -0.0033594635 -0.0033598985 -0.003360267 -0.0033603648][-0.003359945 -0.0033573443 -0.0033577865 -0.0033584228 -0.0033588456 -0.0033589297 -0.00335858 -0.0033578521 -0.0033574395 -0.0033572942 -0.0033574805 -0.0033578495 -0.0033583324 -0.0033588957 -0.003359281][-0.0033593432 -0.0033569676 -0.0033577275 -0.0033590419 -0.0033604484 -0.0033612654 -0.0033611956 -0.0033601986 -0.0033592735 -0.003358254 -0.0033573683 -0.0033569608 -0.0033570935 -0.0033576463 -0.0033582482][-0.0033591827 -0.0033571857 -0.00335868 -0.0033609271 -0.0033635541 -0.0033656769 -0.0033666175 -0.0033658571 -0.0033641357 -0.0033615788 -0.003358972 -0.0033573194 -0.0033566472 -0.0033567967 -0.0033574346][-0.0033597178 -0.003358148 -0.003360471 -0.0033636913 -0.0033677891 -0.003371615 -0.0033739025 -0.0033735987 -0.0033709982 -0.003366915 -0.0033624847 -0.0033590475 -0.0033570619 -0.0033565238 -0.0033569494][-0.00336071 -0.0033595683 -0.0033626123 -0.0033667975 -0.0033722583 -0.0033775084 -0.0033808807 -0.0033809755 -0.0033780744 -0.0033729712 -0.0033670266 -0.0033618675 -0.003358498 -0.00335706 -0.0033569688][-0.0033621378 -0.0033612405 -0.0033645937 -0.003369322 -0.0033755286 -0.0033815398 -0.0033856353 -0.0033861892 -0.0033834574 -0.0033779508 -0.0033710336 -0.0033647635 -0.0033602859 -0.0033580032 -0.0033573192][-0.0033640163 -0.0033630088 -0.0033662915 -0.0033709074 -0.0033769361 -0.0033828726 -0.0033869604 -0.0033877641 -0.0033852772 -0.0033800013 -0.0033730792 -0.0033666007 -0.0033617686 -0.0033589727 -0.0033578838][-0.0033654771 -0.0033640997 -0.0033669122 -0.0033708536 -0.0033758779 -0.0033808677 -0.0033841676 -0.0033847634 -0.0033826907 -0.0033782257 -0.0033723239 -0.0033666687 -0.0033624077 -0.0033597588 -0.0033585029][-0.0033662147 -0.00336437 -0.0033664384 -0.0033693239 -0.0033729167 -0.0033763377 -0.0033785878 -0.0033789971 -0.0033775319 -0.0033743135 -0.0033699546 -0.0033657472 -0.0033624857 -0.0033602931 -0.0033590731]]...]
INFO - root - 2017-12-09 13:43:55.132036: step 29310, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 70h:00m:31s remains)
INFO - root - 2017-12-09 13:44:03.621969: step 29320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:08m:15s remains)
INFO - root - 2017-12-09 13:44:12.194876: step 29330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:16m:38s remains)
INFO - root - 2017-12-09 13:44:20.822960: step 29340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:43m:26s remains)
INFO - root - 2017-12-09 13:44:29.372561: step 29350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:03m:44s remains)
INFO - root - 2017-12-09 13:44:37.976582: step 29360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:13m:09s remains)
INFO - root - 2017-12-09 13:44:46.729101: step 29370, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:37m:58s remains)
INFO - root - 2017-12-09 13:44:55.531052: step 29380, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 72h:02m:40s remains)
INFO - root - 2017-12-09 13:45:04.222215: step 29390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:22m:09s remains)
INFO - root - 2017-12-09 13:45:12.792197: step 29400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:05m:52s remains)
2017-12-09 13:45:13.710605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033884707 -0.0033840344 -0.0033815724 -0.003381111 -0.003382015 -0.0033835371 -0.0033852246 -0.0033871308 -0.0033893371 -0.0033916736 -0.0033935176 -0.0033949988 -0.0033958419 -0.0033962945 -0.0033964934][-0.0033924803 -0.0033891108 -0.0033873289 -0.0033872728 -0.0033882107 -0.0033896968 -0.0033913525 -0.0033932505 -0.0033954696 -0.0033976315 -0.0033989474 -0.0033999614 -0.0034003241 -0.0034001963 -0.0033997109][-0.0033961688 -0.003394241 -0.0033937222 -0.0033941003 -0.0033948109 -0.0033960408 -0.0033974503 -0.0033989791 -0.0034008224 -0.0034025237 -0.0034033551 -0.0034040299 -0.0034040054 -0.0034036033 -0.0034028664][-0.0033973311 -0.0033966319 -0.0033974417 -0.0033981625 -0.0033990343 -0.0034004869 -0.0034019044 -0.0034029447 -0.0034039163 -0.0034047961 -0.0034047088 -0.0034043917 -0.0034036131 -0.0034026275 -0.0034013155][-0.0033952165 -0.0033955455 -0.0033974382 -0.0033989036 -0.0034004003 -0.0034021551 -0.0034035619 -0.0034041372 -0.0034041726 -0.0034039533 -0.0034025717 -0.0034008638 -0.003399217 -0.0033976214 -0.0033961867][-0.0033909036 -0.0033914782 -0.0033939087 -0.0033961567 -0.0033979125 -0.0033995928 -0.0034008024 -0.0034011421 -0.003400645 -0.0033995518 -0.0033972957 -0.0033947788 -0.0033928561 -0.0033912791 -0.0033900521][-0.0033863748 -0.0033864088 -0.0033887371 -0.0033910938 -0.0033926766 -0.003393735 -0.0033943127 -0.0033945243 -0.0033941497 -0.0033931562 -0.0033910037 -0.0033885376 -0.0033868107 -0.0033856202 -0.0033848193][-0.0033822474 -0.0033813741 -0.003383213 -0.0033851862 -0.0033864956 -0.0033870786 -0.0033868218 -0.0033868251 -0.003386884 -0.0033864207 -0.0033848255 -0.0033829042 -0.003381575 -0.0033807382 -0.0033802418][-0.003379365 -0.0033777421 -0.0033790704 -0.0033805475 -0.0033815058 -0.0033816751 -0.0033807599 -0.003380459 -0.0033807692 -0.0033809051 -0.0033800346 -0.0033788714 -0.0033780083 -0.0033774357 -0.0033770953][-0.0033779941 -0.0033755887 -0.0033763258 -0.0033771612 -0.0033778285 -0.0033777724 -0.0033769384 -0.0033766204 -0.0033770006 -0.0033773798 -0.0033770739 -0.0033765086 -0.0033760087 -0.0033757261 -0.0033756439][-0.0033774788 -0.0033745295 -0.0033748094 -0.0033753261 -0.0033757736 -0.0033755908 -0.0033750341 -0.0033746974 -0.0033750078 -0.003375408 -0.0033752813 -0.0033751149 -0.003375011 -0.0033749861 -0.0033750315][-0.0033774965 -0.003374357 -0.003374347 -0.0033744755 -0.0033747882 -0.0033746685 -0.0033743617 -0.0033740345 -0.0033741808 -0.003374455 -0.0033744634 -0.003374533 -0.0033746418 -0.0033748539 -0.003375][-0.0033776611 -0.0033745766 -0.0033743905 -0.0033743957 -0.0033746504 -0.0033745652 -0.003374368 -0.0033740723 -0.0033740096 -0.0033741635 -0.0033742758 -0.0033744348 -0.0033746674 -0.0033749584 -0.0033751323][-0.0033779356 -0.0033749461 -0.0033746636 -0.0033747049 -0.0033749135 -0.0033748732 -0.0033746723 -0.003374405 -0.0033743067 -0.0033743775 -0.0033744629 -0.0033746003 -0.0033748297 -0.0033751007 -0.00337528][-0.0033781782 -0.0033753461 -0.0033749982 -0.0033750441 -0.0033751826 -0.003375143 -0.0033749596 -0.0033747139 -0.0033746597 -0.0033746215 -0.0033746723 -0.003374808 -0.0033749782 -0.0033752059 -0.0033753626]]...]
INFO - root - 2017-12-09 13:45:22.135738: step 29410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:04m:41s remains)
INFO - root - 2017-12-09 13:45:30.537456: step 29420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:13m:58s remains)
INFO - root - 2017-12-09 13:45:39.294112: step 29430, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.913 sec/batch; 76h:50m:28s remains)
INFO - root - 2017-12-09 13:45:47.948113: step 29440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:44m:09s remains)
INFO - root - 2017-12-09 13:45:56.470396: step 29450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:25m:40s remains)
INFO - root - 2017-12-09 13:46:05.048519: step 29460, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 70h:28m:14s remains)
INFO - root - 2017-12-09 13:46:13.773810: step 29470, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 74h:16m:41s remains)
INFO - root - 2017-12-09 13:46:22.466835: step 29480, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:22m:22s remains)
INFO - root - 2017-12-09 13:46:31.167310: step 29490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:30m:59s remains)
INFO - root - 2017-12-09 13:46:39.796663: step 29500, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 75h:17m:50s remains)
2017-12-09 13:46:40.706848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033433991 -0.003272543 -0.0031822394 -0.0031044905 -0.0030620315 -0.0030609944 -0.0030904156 -0.0031582229 -0.0032434692 -0.0033167298 -0.0033605404 -0.003378578 -0.0033818721 -0.0033782581 -0.0033748569][-0.0032573973 -0.0031146808 -0.0029561466 -0.0028420691 -0.002805935 -0.0028426449 -0.0029213759 -0.0030397172 -0.0031664423 -0.0032722717 -0.0033348445 -0.0033595902 -0.0033635297 -0.0033595429 -0.0033581713][-0.0030588063 -0.0027795346 -0.0025007785 -0.0023272855 -0.0023077219 -0.0024160165 -0.0025885499 -0.0028047469 -0.0030175159 -0.0031938693 -0.0033009201 -0.0033454108 -0.003352883 -0.0033496453 -0.0033504504][-0.0026979558 -0.0022113547 -0.001759271 -0.0014942163 -0.0014713313 -0.0016403614 -0.0019226611 -0.0022914249 -0.0026784858 -0.0030197108 -0.0032365611 -0.0033312244 -0.0033498132 -0.00334727 -0.0033499261][-0.0021947604 -0.0014699155 -0.00083089923 -0.00045584515 -0.00038047181 -0.00054299855 -0.00088971574 -0.001426544 -0.002069423 -0.0026787776 -0.0030907823 -0.003282639 -0.0033307672 -0.0033347164 -0.0033428816][-0.0016746145 -0.00075231772 3.5167672e-05 0.00050786976 0.00067317113 0.00059357844 0.00027036178 -0.00037163496 -0.0012553982 -0.0021514501 -0.0027963237 -0.0031222617 -0.0032307438 -0.003266295 -0.0033033642][-0.0013090358 -0.00028931 0.00056619057 0.0010926127 0.0013465555 0.0013850876 0.0011607669 0.00052436208 -0.00047209207 -0.0015433913 -0.0023622524 -0.0028136594 -0.0030098238 -0.0031167194 -0.0032206492][-0.0012532459 -0.00027174689 0.00055139372 0.0010766573 0.0013885554 0.0015344201 0.0014421255 0.00092221471 -7.0258975e-06 -0.0010635674 -0.001918021 -0.0024362225 -0.0027219492 -0.0029280984 -0.0031239432][-0.0015753348 -0.000760057 -6.0508028e-05 0.00041290815 0.00073902914 0.00095155369 0.0009764079 0.00062943622 -9.2543662e-05 -0.000959747 -0.0016996886 -0.0021960223 -0.0025298668 -0.0028127623 -0.0030743012][-0.0021623818 -0.0015939601 -0.0010810986 -0.00070022163 -0.00040487526 -0.00018081465 -9.0969494e-05 -0.00027831038 -0.00074885623 -0.0013416358 -0.0018742403 -0.0022670366 -0.0025744629 -0.0028579326 -0.0031111604][-0.0027650441 -0.0024475097 -0.0021372088 -0.0018772724 -0.0016527562 -0.0014688498 -0.0013723031 -0.0014512106 -0.0017064706 -0.0020416931 -0.0023589188 -0.0026118397 -0.0028293377 -0.0030354594 -0.0032106854][-0.0031729438 -0.0030388036 -0.0028942607 -0.0027556373 -0.0026235436 -0.0025093737 -0.0024433429 -0.0024669485 -0.002579109 -0.002731608 -0.0028831013 -0.0030111372 -0.0031244915 -0.0032292665 -0.0033118781][-0.0033532432 -0.0033143042 -0.0032669657 -0.0032145723 -0.003159238 -0.0031087857 -0.0030780374 -0.0030822847 -0.003119302 -0.0031714698 -0.0032256802 -0.0032733683 -0.0033143454 -0.0033486555 -0.003372496][-0.00339966 -0.0033927364 -0.0033831832 -0.0033712361 -0.0033572507 -0.0033433661 -0.0033345146 -0.0033343025 -0.0033426355 -0.0033544279 -0.0033673057 -0.0033788439 -0.0033876554 -0.0033930296 -0.003394298][-0.0034023758 -0.0034007572 -0.0033995097 -0.0033981819 -0.0033967304 -0.003395637 -0.0033952484 -0.0033950058 -0.0033964107 -0.0033983744 -0.0034006834 -0.0034023651 -0.0034024657 -0.0034008941 -0.0033983008]]...]
INFO - root - 2017-12-09 13:46:49.206274: step 29510, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:58m:02s remains)
INFO - root - 2017-12-09 13:46:57.766795: step 29520, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 71h:24m:08s remains)
INFO - root - 2017-12-09 13:47:06.217789: step 29530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:02m:56s remains)
INFO - root - 2017-12-09 13:47:14.833327: step 29540, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:59m:10s remains)
INFO - root - 2017-12-09 13:47:23.332653: step 29550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:40m:53s remains)
INFO - root - 2017-12-09 13:47:31.974694: step 29560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:18m:21s remains)
INFO - root - 2017-12-09 13:47:40.838769: step 29570, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.927 sec/batch; 78h:02m:43s remains)
INFO - root - 2017-12-09 13:47:49.324730: step 29580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 72h:00m:20s remains)
INFO - root - 2017-12-09 13:47:57.992101: step 29590, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 74h:09m:13s remains)
INFO - root - 2017-12-09 13:48:06.570622: step 29600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:18m:41s remains)
2017-12-09 13:48:07.433084: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025485549 0.030568875 0.038994048 0.0489662 0.057525359 0.063003294 0.064606957 0.062504977 0.057443522 0.051415417 0.045830142 0.040330105 0.035260044 0.031561919 0.029012349][0.043328017 0.052133672 0.065128013 0.080668166 0.09491457 0.10595783 0.11220165 0.11240911 0.10710433 0.098828442 0.089269087 0.078832775 0.068787061 0.06050447 0.054985471][0.067023277 0.082234822 0.10268318 0.12665656 0.14932157 0.16793966 0.18007804 0.18399766 0.17925739 0.16873133 0.15411654 0.13710445 0.11943118 0.10389653 0.092766158][0.095291533 0.11840672 0.14829223 0.18305683 0.21714111 0.2466512 0.26743469 0.27647296 0.27244312 0.25829241 0.23606242 0.20912512 0.18030798 0.15421878 0.13450459][0.12332255 0.15507179 0.19483361 0.24132971 0.28832245 0.33054689 0.36197993 0.37817138 0.37603107 0.35806692 0.32700926 0.2882342 0.24598499 0.20658374 0.17552696][0.14265309 0.18264157 0.23198575 0.28961572 0.34871665 0.40330213 0.44522294 0.46808532 0.46778139 0.44623533 0.40658215 0.35574374 0.29950196 0.2465435 0.20368363][0.15031771 0.19524328 0.25077543 0.31595674 0.38316053 0.4462353 0.49578714 0.52427214 0.52643734 0.50307971 0.45763248 0.39744952 0.32963216 0.2647959 0.21105395][0.14509141 0.19026855 0.24677169 0.31423855 0.3847959 0.45189381 0.50525761 0.53718221 0.54171008 0.51833934 0.47063997 0.40574068 0.33139369 0.25939307 0.19847748][0.12810171 0.16977856 0.22265841 0.28685772 0.35528764 0.42189017 0.47621998 0.5097878 0.51652718 0.49525335 0.4490453 0.38421884 0.30910528 0.23556851 0.17260505][0.10125951 0.13790242 0.18479867 0.24253976 0.30505973 0.36716247 0.41893977 0.45240861 0.46124104 0.44378254 0.40233681 0.34214818 0.27161306 0.20187752 0.1416965][0.071176745 0.10151342 0.14126614 0.19055709 0.24439208 0.29871595 0.34454265 0.37504914 0.38464153 0.37170732 0.33768818 0.28643641 0.22587623 0.16564968 0.11326187][0.046065785 0.068759382 0.09947592 0.13839431 0.18150714 0.22539325 0.26276538 0.28837529 0.29735151 0.28840059 0.26281574 0.22370802 0.17751837 0.1314355 0.090935312][0.026925713 0.042501554 0.064140469 0.091888629 0.1232178 0.15557523 0.18316232 0.20211507 0.20910574 0.20358963 0.18678826 0.16137283 0.13198943 0.103011 0.076723486][0.013272388 0.023036165 0.036833707 0.054668851 0.075081579 0.096359946 0.11453506 0.12692581 0.13152368 0.12860952 0.11969745 0.10715814 0.093644254 0.080890283 0.068208687][0.0041567148 0.0094019957 0.017241675 0.027530257 0.039459281 0.051970545 0.062481757 0.0696775 0.072618529 0.072062567 0.069622114 0.067396261 0.066595569 0.066835605 0.0648363]]...]
INFO - root - 2017-12-09 13:48:16.227160: step 29610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 71h:38m:52s remains)
INFO - root - 2017-12-09 13:48:24.829626: step 29620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:14m:37s remains)
INFO - root - 2017-12-09 13:48:33.478987: step 29630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:24m:13s remains)
INFO - root - 2017-12-09 13:48:42.186277: step 29640, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 73h:39m:37s remains)
INFO - root - 2017-12-09 13:48:50.671609: step 29650, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 76h:15m:10s remains)
INFO - root - 2017-12-09 13:48:59.344870: step 29660, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:09m:01s remains)
INFO - root - 2017-12-09 13:49:08.099706: step 29670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:55m:49s remains)
INFO - root - 2017-12-09 13:49:16.702335: step 29680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:22m:30s remains)
INFO - root - 2017-12-09 13:49:25.419471: step 29690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:30m:37s remains)
INFO - root - 2017-12-09 13:49:33.911439: step 29700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:03m:43s remains)
2017-12-09 13:49:34.907960: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019514708 0.017685011 0.015676303 0.014230959 0.013582282 0.013811048 0.014454156 0.015054381 0.015187124 0.014519634 0.012948629 0.01067671 0.0080304835 0.0053058672 0.0027447259][0.020550089 0.019193521 0.018365823 0.018820249 0.02036093 0.022799617 0.025192205 0.026633864 0.026770873 0.025387267 0.022777474 0.019112114 0.014896616 0.010560537 0.0065144985][0.020445624 0.02008377 0.020547986 0.02276044 0.026439387 0.031152757 0.035485748 0.038558606 0.039628748 0.03829091 0.034867413 0.029793655 0.023971111 0.017713252 0.011753734][0.019367132 0.019929145 0.021965258 0.026247473 0.032185428 0.039139338 0.045533441 0.050189555 0.051964078 0.050694268 0.046744518 0.04062669 0.0332887 0.025263187 0.017488997][0.023250598 0.024393676 0.027255747 0.032509089 0.039689697 0.047914725 0.055626705 0.061342128 0.063705735 0.062614724 0.058315422 0.051412202 0.042808261 0.033209536 0.023635235][0.033295784 0.035959069 0.039733831 0.045338396 0.052363947 0.0602066 0.067582853 0.073056564 0.07540834 0.074427687 0.07036148 0.0633093 0.0539107 0.042944878 0.031593163][0.0463593 0.051184662 0.056232829 0.062275574 0.068824746 0.075506032 0.081508085 0.085805975 0.087471522 0.086233288 0.082050227 0.0748778 0.065057896 0.053190548 0.040383495][0.059030037 0.065413713 0.071167916 0.077108659 0.0828712 0.088142253 0.092618816 0.095637009 0.0965541 0.095021054 0.090909824 0.083888121 0.074062958 0.062010564 0.048582096][0.068181306 0.075030133 0.080191523 0.08487419 0.088994138 0.0925393 0.095444426 0.09730126 0.097632915 0.096104883 0.092401356 0.086005367 0.076985881 0.065773554 0.052862469][0.070935167 0.077667311 0.081923574 0.084991314 0.087201864 0.088773571 0.089807585 0.090298191 0.089834653 0.088259846 0.08516334 0.079892017 0.072447687 0.062921137 0.051524173][0.06580995 0.07210061 0.075635493 0.077591643 0.078640781 0.078955472 0.078723937 0.077985659 0.076540813 0.074583463 0.071763106 0.06754823 0.061755918 0.054226957 0.044902578][0.053253409 0.058719378 0.061849654 0.0635996 0.064495847 0.064497679 0.064009406 0.062872574 0.061072081 0.058743402 0.055867333 0.052293397 0.04762511 0.041700013 0.034273095][0.037756782 0.041918617 0.044390343 0.045743875 0.046328124 0.046167027 0.045560837 0.044518694 0.043095704 0.041281559 0.039143439 0.036494792 0.032983784 0.028367443 0.022620788][0.022416627 0.025073947 0.026726127 0.027735192 0.028202618 0.028203787 0.027836794 0.02709581 0.026090302 0.024775827 0.02323938 0.021357367 0.018953567 0.015810071 0.012075048][0.011083612 0.012451855 0.013286445 0.013822928 0.014087109 0.014115283 0.013892999 0.013453478 0.012871699 0.012119703 0.011196882 0.010101238 0.0087226648 0.0069932821 0.0050502396]]...]
INFO - root - 2017-12-09 13:49:43.390515: step 29710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:15m:31s remains)
INFO - root - 2017-12-09 13:49:51.747733: step 29720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:37m:50s remains)
INFO - root - 2017-12-09 13:50:00.359668: step 29730, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:10m:07s remains)
INFO - root - 2017-12-09 13:50:08.937131: step 29740, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 74h:09m:31s remains)
INFO - root - 2017-12-09 13:50:17.300804: step 29750, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 70h:24m:50s remains)
INFO - root - 2017-12-09 13:50:25.922703: step 29760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:51m:16s remains)
INFO - root - 2017-12-09 13:50:34.399022: step 29770, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 70h:10m:43s remains)
INFO - root - 2017-12-09 13:50:42.895455: step 29780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 73h:10m:50s remains)
INFO - root - 2017-12-09 13:50:51.439937: step 29790, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 70h:27m:50s remains)
INFO - root - 2017-12-09 13:50:59.925537: step 29800, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:07m:47s remains)
2017-12-09 13:51:00.767692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033876188 -0.0033867718 -0.0033867827 -0.0033869974 -0.0033872954 -0.0033874416 -0.0033872374 -0.0033869727 -0.0033867534 -0.0033864938 -0.0033862151 -0.0033860668 -0.0033861431 -0.0033862139 -0.003386318][-0.0033851268 -0.0033840719 -0.0033844865 -0.0033848735 -0.0033852181 -0.0033854279 -0.0033853012 -0.0033850565 -0.0033846565 -0.0033841091 -0.0033835061 -0.0033831389 -0.0033829689 -0.0033829915 -0.0033831173][-0.0033862418 -0.003385846 -0.0033868568 -0.0033875925 -0.0033880433 -0.0033880121 -0.0033876263 -0.003386945 -0.0033861613 -0.0033855431 -0.0033849867 -0.0033845957 -0.0033841527 -0.0033837692 -0.0033836744][-0.0033874521 -0.0033876686 -0.0033890221 -0.0033898407 -0.0033903285 -0.0033904107 -0.0033898687 -0.0033890647 -0.0033883546 -0.0033877343 -0.0033871371 -0.0033865534 -0.0033858782 -0.0033851592 -0.0033847417][-0.0033887571 -0.0033893702 -0.0033905082 -0.0033908181 -0.0033907157 -0.0033901604 -0.0033891718 -0.003388444 -0.0033880833 -0.0033882416 -0.0033883066 -0.0033880046 -0.0033874616 -0.0033866875 -0.0033860861][-0.0033907022 -0.0033912063 -0.0033920757 -0.0033921949 -0.0033914333 -0.003389868 -0.0033880987 -0.0033870819 -0.003386738 -0.0033873743 -0.0033880773 -0.0033883217 -0.0033883143 -0.0033877804 -0.0033872677][-0.0033924622 -0.0033928275 -0.0033934936 -0.0033935921 -0.0033927551 -0.0033905841 -0.0033880346 -0.0033864621 -0.0033860707 -0.0033864814 -0.0033870679 -0.003387772 -0.0033882577 -0.0033880717 -0.0033878796][-0.0033936154 -0.0033940438 -0.003395488 -0.0033965597 -0.0033966089 -0.0033950196 -0.0033931367 -0.0033920044 -0.0033906442 -0.0033896398 -0.0033892237 -0.0033890794 -0.0033888414 -0.0033882973 -0.0033882153][-0.0033944289 -0.0033949811 -0.0033966291 -0.0033985435 -0.0033995765 -0.0033991898 -0.0033983435 -0.0033977544 -0.0033967022 -0.0033952724 -0.0033938147 -0.0033922943 -0.00339076 -0.0033893851 -0.003388836][-0.0033940938 -0.0033945825 -0.0033966149 -0.0033987912 -0.0034001844 -0.0034008527 -0.0034009691 -0.0034007991 -0.0034001817 -0.0033990212 -0.0033973085 -0.0033951635 -0.0033929534 -0.00339102 -0.0033896863][-0.0033931667 -0.003393505 -0.003395413 -0.0033977125 -0.0033995095 -0.0034004613 -0.0034012813 -0.0034013642 -0.0034009309 -0.0033998955 -0.0033983905 -0.003396217 -0.0033939066 -0.0033918959 -0.0033902691][-0.0033924589 -0.0033916417 -0.0033931492 -0.0033953339 -0.0033972831 -0.00339871 -0.0033996783 -0.0034000354 -0.0033998671 -0.0033991633 -0.0033979178 -0.0033959786 -0.0033938866 -0.003392024 -0.0033905557][-0.0033926761 -0.0033906559 -0.0033912514 -0.0033926317 -0.0033940019 -0.003395369 -0.0033964587 -0.0033970245 -0.0033970836 -0.0033965947 -0.003395689 -0.0033944221 -0.003392837 -0.003391356 -0.0033903641][-0.0033936938 -0.0033907213 -0.003390007 -0.0033901837 -0.0033907259 -0.0033913392 -0.0033920766 -0.0033926861 -0.0033928982 -0.0033927287 -0.0033923704 -0.00339188 -0.003391193 -0.0033904037 -0.00338979][-0.0033941471 -0.0033908105 -0.003389461 -0.0033888996 -0.0033888312 -0.0033889329 -0.0033892081 -0.0033894307 -0.00338953 -0.0033895816 -0.0033895748 -0.0033895252 -0.0033893487 -0.0033891601 -0.0033889939]]...]
INFO - root - 2017-12-09 13:51:09.359023: step 29810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:26m:48s remains)
INFO - root - 2017-12-09 13:51:17.992967: step 29820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:01m:36s remains)
INFO - root - 2017-12-09 13:51:26.699328: step 29830, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 74h:07m:42s remains)
INFO - root - 2017-12-09 13:51:35.431763: step 29840, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:40m:46s remains)
INFO - root - 2017-12-09 13:51:44.079210: step 29850, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 68h:22m:13s remains)
INFO - root - 2017-12-09 13:51:52.764585: step 29860, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:32m:59s remains)
INFO - root - 2017-12-09 13:52:01.337814: step 29870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 69h:52m:09s remains)
INFO - root - 2017-12-09 13:52:10.007565: step 29880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:00m:33s remains)
INFO - root - 2017-12-09 13:52:18.766493: step 29890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:42m:23s remains)
INFO - root - 2017-12-09 13:52:27.263634: step 29900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:37m:22s remains)
2017-12-09 13:52:28.102200: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.029308552 0.028847257 0.027818931 0.026586561 0.027085323 0.03094898 0.039918061 0.053474009 0.068759814 0.082841709 0.091798253 0.093568765 0.086154573 0.071747623 0.054375771][0.041076295 0.038034309 0.03529283 0.033596054 0.035250902 0.041647051 0.054193567 0.071675032 0.090181969 0.10651187 0.11661195 0.11852364 0.10998649 0.093364559 0.073074467][0.061272029 0.055901717 0.051128626 0.049050331 0.051742215 0.060115498 0.075194187 0.095020324 0.11511789 0.13200791 0.14168102 0.14301759 0.13380221 0.11639099 0.094731018][0.087345757 0.080262072 0.07368315 0.071256533 0.07463938 0.083973087 0.099739015 0.11945905 0.13909346 0.15505067 0.16365114 0.16433863 0.15485926 0.13759354 0.11587394][0.11572021 0.10849748 0.10094096 0.097990341 0.10123704 0.11021245 0.12457971 0.14183381 0.15857518 0.17155176 0.17768314 0.17707267 0.16750161 0.15095116 0.13012373][0.13687782 0.13165793 0.12522471 0.12279913 0.12593791 0.13368851 0.14528242 0.15860139 0.17075542 0.1791065 0.18128593 0.17809506 0.16755608 0.15140095 0.13174459][0.14245377 0.14129874 0.13806815 0.13780093 0.1417089 0.14848833 0.1571067 0.16577567 0.17238587 0.17496794 0.17215608 0.16529559 0.1528641 0.13665372 0.11836322][0.1309647 0.13404857 0.13491517 0.13752122 0.14261337 0.14884007 0.15515287 0.15978852 0.16134998 0.15858167 0.15084358 0.14009853 0.12584828 0.10969346 0.092885934][0.10729762 0.11313143 0.11708921 0.12206124 0.12838154 0.134219 0.13868517 0.14006436 0.13777044 0.13100703 0.11980451 0.10668979 0.091864489 0.076767258 0.062237967][0.0782181 0.084960051 0.090814881 0.097112738 0.10367955 0.10852538 0.11134151 0.11031145 0.10551931 0.096702725 0.084451482 0.071347952 0.057983659 0.045456134 0.034213215][0.049207047 0.05547205 0.06167968 0.067982033 0.0739176 0.077452734 0.078505106 0.07594613 0.070470035 0.061833028 0.050911054 0.040077236 0.030054418 0.021298496 0.013876772][0.024911728 0.029671354 0.034857087 0.039992876 0.044442765 0.046459507 0.046302214 0.043547083 0.038908012 0.032298129 0.024621285 0.017604742 0.011640784 0.006761712 0.00282793][0.0084097339 0.011298588 0.014690842 0.017997749 0.020685991 0.021527102 0.020916209 0.018849734 0.015870865 0.01197372 0.0077955145 0.0043370388 0.0016759329 -0.00029197568 -0.0017835324][1.2043631e-05 0.0013269361 0.0029714431 0.004529194 0.0056832265 0.0058381474 0.0052948613 0.0041642003 0.0027339521 0.001086646 -0.00049686013 -0.0015988852 -0.0023128879 -0.0027623477 -0.0030787252][-0.0028980747 -0.0024998961 -0.0019580759 -0.001416239 -0.0010666794 -0.0010583387 -0.0012905891 -0.0016700809 -0.0021109539 -0.0025561284 -0.0029062037 -0.0031002273 -0.0031903267 -0.0032306251 -0.0032549913]]...]
INFO - root - 2017-12-09 13:52:36.766244: step 29910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:26m:48s remains)
INFO - root - 2017-12-09 13:52:45.461307: step 29920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:23m:07s remains)
INFO - root - 2017-12-09 13:52:54.127938: step 29930, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:42m:28s remains)
INFO - root - 2017-12-09 13:53:02.786583: step 29940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:27m:55s remains)
INFO - root - 2017-12-09 13:53:11.347103: step 29950, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 68h:45m:27s remains)
INFO - root - 2017-12-09 13:53:19.925627: step 29960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:33m:12s remains)
INFO - root - 2017-12-09 13:53:28.699443: step 29970, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:56m:33s remains)
INFO - root - 2017-12-09 13:53:37.360363: step 29980, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 75h:49m:15s remains)
INFO - root - 2017-12-09 13:53:46.024270: step 29990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:56m:37s remains)
INFO - root - 2017-12-09 13:53:54.565603: step 30000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:29m:05s remains)
2017-12-09 13:53:55.485697: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11110659 0.10842947 0.11012431 0.11489967 0.12377954 0.1347633 0.1441519 0.14856799 0.14598389 0.139056 0.13010727 0.12267032 0.11471057 0.10655925 0.097934306][0.11071946 0.10463022 0.10330044 0.10500908 0.11150108 0.12022854 0.12813798 0.13175796 0.12871951 0.12278293 0.11521823 0.10922754 0.10358548 0.097370252 0.0901781][0.10206543 0.093461581 0.088790163 0.087226406 0.089984737 0.094863325 0.099843964 0.10146178 0.098127633 0.092631586 0.086750671 0.083069645 0.080115557 0.076895505 0.072914049][0.091888942 0.080147155 0.072450012 0.068279415 0.068159677 0.070275113 0.073121734 0.072593413 0.068575375 0.063434675 0.059065524 0.057268932 0.056908607 0.057188679 0.057030641][0.08071994 0.067002572 0.057029877 0.050753884 0.048307352 0.04814608 0.049226359 0.047577929 0.043766644 0.039346971 0.036493234 0.03654813 0.038908228 0.042360939 0.046018761][0.070212118 0.055729881 0.044892952 0.038086504 0.034794655 0.033647787 0.033481628 0.031157168 0.02781589 0.024174407 0.022730246 0.024115156 0.028400024 0.034067284 0.040851563][0.059331793 0.04528448 0.034331635 0.027883427 0.024287397 0.02306423 0.022729525 0.021277562 0.019540245 0.017645102 0.018168012 0.021014282 0.026933907 0.034530852 0.043827187][0.050169941 0.036962025 0.026694173 0.021250324 0.01836038 0.017586656 0.016397471 0.015767723 0.015036163 0.014649185 0.017095618 0.022006983 0.030269189 0.040434796 0.052274507][0.04143158 0.029982714 0.021669129 0.017810475 0.016090589 0.015773823 0.014655986 0.013972997 0.012982144 0.013371187 0.016499972 0.023409918 0.033911057 0.046984021 0.061270196][0.033991072 0.02514944 0.019269314 0.017271899 0.016869191 0.016966531 0.016150434 0.015310659 0.013532578 0.01373249 0.016873103 0.024427405 0.035603929 0.050374839 0.065511547][0.027749572 0.020328974 0.016119383 0.015950624 0.016906016 0.01738487 0.017060909 0.016682291 0.014979349 0.015331651 0.018637346 0.027165066 0.039428953 0.054758608 0.06965895][0.022808217 0.017462952 0.014973012 0.01617156 0.018451806 0.020398997 0.02147785 0.021767827 0.020659231 0.021029985 0.023655919 0.031326186 0.042683069 0.057062246 0.070811309][0.020664008 0.017481478 0.017044483 0.020384613 0.025084691 0.02917817 0.031612244 0.032170694 0.030606685 0.030025063 0.031159101 0.036634944 0.046078872 0.05821849 0.069830716][0.021753935 0.020105686 0.021346034 0.026739199 0.033742771 0.039978649 0.043656878 0.044096943 0.041780639 0.039359443 0.038701639 0.041935705 0.049048536 0.059053812 0.069072127][0.024315545 0.024489166 0.027388137 0.034581475 0.043147407 0.050164726 0.054065622 0.054230139 0.05112816 0.047038782 0.044350419 0.045275483 0.050094377 0.057927202 0.066406339]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 13:54:04.604292: step 30010, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.744 sec/batch; 62h:32m:28s remains)
INFO - root - 2017-12-09 13:54:13.359920: step 30020, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 70h:03m:04s remains)
INFO - root - 2017-12-09 13:54:22.022862: step 30030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:59m:14s remains)
INFO - root - 2017-12-09 13:54:30.739920: step 30040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:11m:49s remains)
INFO - root - 2017-12-09 13:54:39.294883: step 30050, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 75h:00m:09s remains)
INFO - root - 2017-12-09 13:54:48.053070: step 30060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:10m:07s remains)
INFO - root - 2017-12-09 13:54:56.723489: step 30070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:23m:40s remains)
INFO - root - 2017-12-09 13:55:05.398903: step 30080, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:55m:12s remains)
INFO - root - 2017-12-09 13:55:14.088483: step 30090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:58m:59s remains)
INFO - root - 2017-12-09 13:55:22.556911: step 30100, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:32m:38s remains)
2017-12-09 13:55:23.455435: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24415284 0.25310406 0.25222972 0.24028665 0.22012283 0.19396475 0.16864997 0.14769687 0.13424031 0.12812763 0.12826361 0.13225172 0.13395877 0.13254543 0.12540346][0.27468804 0.28976664 0.29227158 0.28223276 0.26123464 0.23150362 0.20027351 0.17047121 0.14992879 0.13883618 0.13734128 0.14276932 0.14828673 0.15229869 0.14969982][0.30372739 0.32434517 0.33026153 0.3253566 0.30611119 0.27633989 0.24312086 0.20936346 0.18515572 0.17014107 0.16794756 0.17337483 0.18095654 0.18870029 0.19023934][0.32526204 0.35300529 0.36519235 0.366587 0.35303074 0.327728 0.29607311 0.26117614 0.23601812 0.2199928 0.21749282 0.22375455 0.23287745 0.24278551 0.24686368][0.32993627 0.36381823 0.3831729 0.39215192 0.38705528 0.36998034 0.3448801 0.31463411 0.29216853 0.27750236 0.27672538 0.28329173 0.29367045 0.30466038 0.31062704][0.31893855 0.35651335 0.38146538 0.39794183 0.40134269 0.39292008 0.37533379 0.35167131 0.33466747 0.3245295 0.32640183 0.334731 0.34695169 0.35842198 0.365624][0.29648358 0.33623117 0.36560771 0.38788262 0.39867696 0.39853305 0.38752481 0.36957353 0.35630998 0.34956652 0.35368872 0.3641223 0.37776244 0.39087284 0.40021631][0.26792812 0.30854493 0.34150311 0.36780638 0.38422877 0.38926667 0.38209081 0.36772972 0.35535911 0.34953946 0.35350004 0.3647899 0.38060907 0.39603123 0.40904686][0.2385271 0.27984202 0.31581226 0.34477749 0.36476165 0.37232482 0.3662346 0.35076261 0.33489189 0.32629403 0.3262229 0.33573595 0.35232541 0.3713904 0.38910636][0.21388663 0.25499275 0.2916773 0.32171673 0.34228739 0.3493751 0.34159374 0.32313773 0.30255359 0.28774491 0.28171384 0.28674021 0.30145448 0.32168764 0.34257933][0.19704357 0.23678024 0.27219892 0.30076373 0.31926921 0.32368931 0.31222674 0.28909007 0.26256296 0.24057572 0.22723684 0.22628011 0.23691291 0.25627664 0.27806836][0.19021484 0.22663745 0.25832 0.28272343 0.29675791 0.29660502 0.280588 0.25287509 0.2211042 0.19284591 0.17288688 0.16533148 0.1705233 0.18644951 0.20651308][0.18823625 0.22036403 0.2464938 0.26505059 0.2735374 0.26825505 0.24808772 0.21704125 0.18199365 0.14977895 0.12521926 0.11245183 0.11251829 0.12355924 0.13955151][0.18540706 0.21275906 0.23281382 0.24497111 0.2473195 0.23717089 0.21390946 0.18139423 0.14549758 0.11220579 0.085924722 0.070336342 0.066414878 0.072522268 0.083857745][0.17595306 0.19895788 0.21357118 0.21995132 0.21683152 0.20271577 0.17761768 0.14573696 0.11180287 0.080699869 0.055960502 0.040388249 0.034658115 0.03691145 0.043851245]]...]
INFO - root - 2017-12-09 13:55:31.829058: step 30110, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.713 sec/batch; 59h:54m:49s remains)
INFO - root - 2017-12-09 13:55:40.436564: step 30120, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:12m:07s remains)
INFO - root - 2017-12-09 13:55:49.123364: step 30130, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 77h:34m:40s remains)
INFO - root - 2017-12-09 13:55:57.766432: step 30140, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 75h:45m:22s remains)
INFO - root - 2017-12-09 13:56:06.352021: step 30150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:22m:46s remains)
INFO - root - 2017-12-09 13:56:15.081173: step 30160, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 74h:55m:29s remains)
INFO - root - 2017-12-09 13:56:23.854123: step 30170, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 72h:01m:29s remains)
INFO - root - 2017-12-09 13:56:32.773787: step 30180, loss = 0.90, batch loss = 0.69 (7.9 examples/sec; 1.014 sec/batch; 85h:08m:01s remains)
INFO - root - 2017-12-09 13:56:41.485448: step 30190, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 73h:02m:15s remains)
INFO - root - 2017-12-09 13:56:49.922477: step 30200, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 72h:07m:21s remains)
2017-12-09 13:56:50.786129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033839943 -0.003381965 -0.0033809487 -0.0033802502 -0.0033799957 -0.003380561 -0.00338184 -0.0033839745 -0.0033868696 -0.0033898905 -0.0033925832 -0.0033940345 -0.0033944012 -0.0033939383 -0.0033929131][-0.0033830379 -0.0033809107 -0.0033795347 -0.0033785787 -0.00337813 -0.0033788523 -0.0033806178 -0.0033835818 -0.0033874882 -0.003391197 -0.0033941439 -0.0033954727 -0.00339496 -0.0033926035 -0.0033896563][-0.0033835757 -0.0033818879 -0.0033805659 -0.0033794108 -0.0033787019 -0.0033792465 -0.0033813918 -0.0033850924 -0.0033898333 -0.0033942687 -0.0033971092 -0.0033974817 -0.0033955202 -0.0033911085 -0.0033856998][-0.0033840279 -0.003383148 -0.0033825398 -0.0033816353 -0.003380693 -0.0033807745 -0.0033829687 -0.0033870535 -0.0033922971 -0.0033971625 -0.0033996869 -0.0033990082 -0.003395336 -0.0033890866 -0.003381561][-0.003384623 -0.0033847752 -0.0033851776 -0.0033847028 -0.0033836106 -0.0033831936 -0.0033847054 -0.0033886128 -0.0033941644 -0.0033992548 -0.0034015949 -0.0034003176 -0.0033957332 -0.0033883005 -0.0033795543][-0.00338544 -0.0033865869 -0.0033884705 -0.0033889576 -0.0033880195 -0.0033870079 -0.0033872714 -0.0033899311 -0.0033950179 -0.0034002126 -0.0034026962 -0.0034014694 -0.0033968049 -0.003389223 -0.0033801023][-0.0033858507 -0.0033877562 -0.0033909322 -0.0033929897 -0.0033930589 -0.0033917464 -0.0033906656 -0.0033914682 -0.0033955253 -0.0034005346 -0.003403232 -0.0034026538 -0.003398682 -0.0033917075 -0.0033829622][-0.0033858065 -0.0033882021 -0.0033925541 -0.0033959842 -0.0033973139 -0.0033965565 -0.0033946964 -0.0033935918 -0.0033962741 -0.0034005691 -0.0034032797 -0.0034035915 -0.0034006203 -0.0033943516 -0.0033863969][-0.003385311 -0.0033875867 -0.0033924207 -0.0033969346 -0.0033995516 -0.0033993989 -0.0033979672 -0.0033966161 -0.0033973937 -0.0034003698 -0.0034028671 -0.0034038571 -0.0034021437 -0.003397176 -0.0033904358][-0.0033839412 -0.0033859247 -0.0033905702 -0.0033953409 -0.0033987213 -0.0033998215 -0.003399509 -0.0033987802 -0.0033987665 -0.0034005635 -0.0034024357 -0.0034035828 -0.0034027754 -0.0033993686 -0.0033943346][-0.0033818781 -0.0033833811 -0.0033874591 -0.0033919432 -0.0033955609 -0.0033974927 -0.0033980848 -0.0033980603 -0.0033981353 -0.0033990412 -0.0034001996 -0.003401492 -0.0034016031 -0.003399875 -0.0033969313][-0.0033790544 -0.0033795177 -0.003382457 -0.0033860228 -0.0033893869 -0.0033919676 -0.003393349 -0.0033939586 -0.0033941614 -0.003394854 -0.0033957781 -0.0033969961 -0.0033976673 -0.0033974352 -0.0033962629][-0.0033766972 -0.0033761205 -0.0033779459 -0.0033802318 -0.0033825964 -0.0033848444 -0.0033864032 -0.0033873257 -0.003387772 -0.0033882975 -0.0033892717 -0.00339054 -0.0033917581 -0.0033926491 -0.0033928154][-0.0033749496 -0.0033734948 -0.0033743405 -0.0033755873 -0.0033770427 -0.0033785845 -0.0033797703 -0.00338067 -0.0033812809 -0.0033816865 -0.0033823717 -0.0033833093 -0.0033847725 -0.0033862581 -0.0033874179][-0.0033742462 -0.0033722972 -0.0033726085 -0.0033730883 -0.0033737416 -0.0033745035 -0.0033750655 -0.0033755654 -0.0033760469 -0.0033764467 -0.0033769039 -0.0033775526 -0.0033785254 -0.00337968 -0.0033809175]]...]
INFO - root - 2017-12-09 13:56:59.283828: step 30210, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 58h:13m:53s remains)
INFO - root - 2017-12-09 13:57:07.994248: step 30220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:43m:23s remains)
INFO - root - 2017-12-09 13:57:16.531946: step 30230, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 73h:02m:11s remains)
INFO - root - 2017-12-09 13:57:25.142642: step 30240, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 69h:43m:37s remains)
INFO - root - 2017-12-09 13:57:33.604224: step 30250, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.749 sec/batch; 62h:54m:52s remains)
INFO - root - 2017-12-09 13:57:42.186662: step 30260, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 75h:46m:43s remains)
INFO - root - 2017-12-09 13:57:50.896082: step 30270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:25m:10s remains)
INFO - root - 2017-12-09 13:57:59.567289: step 30280, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 73h:20m:06s remains)
INFO - root - 2017-12-09 13:58:08.233248: step 30290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 72h:17m:53s remains)
INFO - root - 2017-12-09 13:58:16.790830: step 30300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:41m:41s remains)
2017-12-09 13:58:17.760423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00075423531 -0.0011804537 -0.00168387 -0.0019577658 -0.0021763525 -0.002387045 -0.0026748101 -0.0029759365 -0.00320107 -0.0033273934 -0.003380833 -0.003387905 -0.0033881739 -0.0033881306 -0.0033887634][0.00016375212 -0.000261707 -0.00085248426 -0.0011722031 -0.0014603715 -0.0017645607 -0.0022357022 -0.002703896 -0.0030803024 -0.0032847675 -0.0033706741 -0.0033856817 -0.003386515 -0.0033866416 -0.0033870463][0.0010331206 0.000657778 3.671553e-05 -0.0002985457 -0.00063920533 -0.00099008647 -0.0016152821 -0.002271835 -0.0028780862 -0.0032061702 -0.0033498167 -0.0033817252 -0.0033854428 -0.0033852814 -0.0033859801][0.00185073 0.0015406448 0.00089830696 0.00055212178 0.00015049777 -0.0002283901 -0.00097378343 -0.0017631041 -0.0025706971 -0.0030557325 -0.0033040391 -0.0033732071 -0.0033850211 -0.0033847159 -0.0033855925][0.0023448083 0.0022124148 0.0016506424 0.0013233537 0.00087655964 0.00047901156 -0.00034441031 -0.0012327023 -0.002214998 -0.002844969 -0.0032149765 -0.0033502793 -0.0033850984 -0.0033859794 -0.0033862873][0.0025728534 0.0026201203 0.0021517661 0.0018589406 0.0013886255 0.0010066086 0.00020218082 -0.00070802215 -0.0017870854 -0.0025606256 -0.003079131 -0.0033074545 -0.0033834861 -0.0033883117 -0.0033878945][0.0024516291 0.0027409142 0.0024425529 0.0021996254 0.0017074829 0.0013313538 0.00056661642 -0.00031232159 -0.0013892301 -0.0022481484 -0.0028999751 -0.0032441271 -0.0033789203 -0.0033899948 -0.0033905774][0.0020396376 0.0025639741 0.0024969347 0.0023460386 0.0018816625 0.0014994668 0.00074897311 -9.6731121e-05 -0.0011159969 -0.0020024008 -0.0027385191 -0.003184495 -0.0033727183 -0.0033897588 -0.0033913511][0.0013328914 0.0020923412 0.0022626345 0.0022348652 0.0018369912 0.0014524281 0.00071408204 -9.4025629e-05 -0.0010493584 -0.001914515 -0.0026610612 -0.0031558746 -0.0033701721 -0.0033904896 -0.0033929702][0.00046980055 0.001395961 0.0017638856 0.0018597224 0.0015263951 0.0011293099 0.00040090363 -0.00035643764 -0.0012269788 -0.0020255907 -0.0027126709 -0.0031798475 -0.0033710466 -0.0033906423 -0.003394197][-0.00043950253 0.00058095227 0.0010533512 0.0012308245 0.00093696942 0.00052063749 -0.00021992344 -0.00091127912 -0.0016698763 -0.0023374106 -0.0028875174 -0.0032461761 -0.003377863 -0.0033903418 -0.0033932936][-0.0012447948 -0.00029891473 0.00021411618 0.00042683096 0.00017744279 -0.00027373014 -0.0010221386 -0.0016482222 -0.0022627772 -0.0027396709 -0.0030982024 -0.0033185196 -0.0033858872 -0.0033912794 -0.0033933867][-0.001928551 -0.0011770823 -0.0007065495 -0.00050244806 -0.00068928069 -0.0011254114 -0.0018212986 -0.0023604506 -0.0028203039 -0.0031003952 -0.003277973 -0.0033704147 -0.0033916191 -0.0033928275 -0.0033936903][-0.0024999236 -0.001988139 -0.0015987029 -0.0014260442 -0.0015432891 -0.0019205342 -0.0024682172 -0.0028767101 -0.003163398 -0.0032941636 -0.0033593581 -0.0033876929 -0.0033928878 -0.0033937169 -0.003394593][-0.0029501459 -0.0026572875 -0.0023989989 -0.0022643912 -0.0023155382 -0.002571265 -0.002925497 -0.0031813746 -0.0033302316 -0.0033754006 -0.003388447 -0.0033917888 -0.0033928503 -0.0033933059 -0.0033935728]]...]
INFO - root - 2017-12-09 13:58:26.433665: step 30310, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.742 sec/batch; 62h:19m:23s remains)
INFO - root - 2017-12-09 13:58:35.321828: step 30320, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 75h:22m:00s remains)
INFO - root - 2017-12-09 13:58:44.088700: step 30330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:04m:14s remains)
INFO - root - 2017-12-09 13:58:52.772189: step 30340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:41m:06s remains)
INFO - root - 2017-12-09 13:59:01.432244: step 30350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:53m:26s remains)
INFO - root - 2017-12-09 13:59:10.066993: step 30360, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:39m:49s remains)
INFO - root - 2017-12-09 13:59:18.769789: step 30370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 71h:53m:54s remains)
INFO - root - 2017-12-09 13:59:27.533765: step 30380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:25m:41s remains)
INFO - root - 2017-12-09 13:59:36.154063: step 30390, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:47m:46s remains)
INFO - root - 2017-12-09 13:59:44.643345: step 30400, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:08m:50s remains)
2017-12-09 13:59:45.539651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003373547 -0.0033715828 -0.0033713738 -0.0033713637 -0.0033713738 -0.0033714604 -0.0033715495 -0.0033716336 -0.0033716764 -0.0033715947 -0.0033715093 -0.0033714159 -0.0033713265 -0.0033713046 -0.0033712904][-0.0033724557 -0.0033703588 -0.0033701903 -0.0033702445 -0.0033703016 -0.0033703889 -0.0033704503 -0.0033704829 -0.0033704666 -0.0033704154 -0.0033703679 -0.0033703186 -0.003370272 -0.0033702464 -0.0033702336][-0.0033725726 -0.0033703689 -0.0033702306 -0.0033703435 -0.0033704445 -0.0033705633 -0.0033706618 -0.0033707242 -0.0033706909 -0.003370618 -0.0033705593 -0.003370495 -0.003370408 -0.0033703111 -0.0033702462][-0.0033723786 -0.003370113 -0.0033699824 -0.0033701342 -0.0033703046 -0.003370509 -0.0033707123 -0.003370865 -0.0033708713 -0.00337082 -0.0033707723 -0.003370692 -0.0033705635 -0.0033704245 -0.0033703404][-0.0033718469 -0.0033694683 -0.0033693768 -0.0033696026 -0.0033698634 -0.0033701612 -0.0033704934 -0.0033707859 -0.003370876 -0.0033708634 -0.0033708357 -0.0033707328 -0.0033705006 -0.0033702708 -0.0033701393][-0.0033711481 -0.0033686284 -0.0033685055 -0.0033687477 -0.003369092 -0.0033695218 -0.0033699975 -0.0033704629 -0.0033706955 -0.0033707947 -0.0033708196 -0.0033707002 -0.0033703917 -0.0033700934 -0.0033698813][-0.0033702678 -0.0033676508 -0.0033674352 -0.0033675584 -0.0033678424 -0.0033682624 -0.0033687814 -0.0033693602 -0.0033697465 -0.0033700117 -0.0033701586 -0.0033701223 -0.0033698445 -0.0033695619 -0.0033693248][-0.0033695451 -0.0033668214 -0.0033664661 -0.0033663674 -0.0033664503 -0.0033667097 -0.0033670801 -0.0033675679 -0.0033679774 -0.0033683518 -0.0033686038 -0.0033686592 -0.0033684832 -0.003368316 -0.0033681579][-0.0033689567 -0.0033663039 -0.0033658892 -0.0033655942 -0.0033654224 -0.0033654019 -0.0033655006 -0.0033657413 -0.0033660545 -0.0033664382 -0.0033667509 -0.0033669018 -0.0033668573 -0.0033668533 -0.0033667909][-0.0033685614 -0.00336603 -0.0033656005 -0.0033651728 -0.0033648026 -0.0033645148 -0.0033643369 -0.0033643316 -0.0033644817 -0.0033647614 -0.0033650058 -0.0033651593 -0.0033652477 -0.0033654035 -0.0033654922][-0.0033684224 -0.0033660152 -0.0033657283 -0.003365254 -0.0033647856 -0.0033643411 -0.0033639534 -0.0033637236 -0.0033637234 -0.0033639518 -0.0033641739 -0.00336434 -0.0033645476 -0.0033648119 -0.0033649721][-0.0033682962 -0.0033659954 -0.00336583 -0.0033653928 -0.003364891 -0.0033643849 -0.00336389 -0.0033635045 -0.003363376 -0.0033634969 -0.0033636605 -0.0033637877 -0.0033640112 -0.0033642838 -0.0033644845][-0.0033682801 -0.0033659332 -0.0033659246 -0.0033656366 -0.0033652422 -0.0033648142 -0.003364393 -0.0033639965 -0.0033638056 -0.0033638156 -0.003363857 -0.0033638852 -0.0033640228 -0.0033642042 -0.0033643383][-0.0033683933 -0.0033659539 -0.0033660964 -0.0033660324 -0.003365858 -0.0033655909 -0.0033652892 -0.00336495 -0.0033647462 -0.0033646591 -0.0033645767 -0.0033645143 -0.0033645604 -0.0033646217 -0.0033646461][-0.0033686047 -0.0033660159 -0.0033662403 -0.0033663558 -0.0033663749 -0.0033663262 -0.0033662193 -0.0033660128 -0.0033658359 -0.0033656994 -0.0033655569 -0.0033654326 -0.0033653798 -0.0033653162 -0.00336524]]...]
INFO - root - 2017-12-09 13:59:53.900993: step 30410, loss = 0.89, batch loss = 0.68 (10.5 examples/sec; 0.762 sec/batch; 63h:58m:18s remains)
INFO - root - 2017-12-09 14:00:02.461999: step 30420, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:09m:51s remains)
INFO - root - 2017-12-09 14:00:11.006973: step 30430, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:05m:30s remains)
INFO - root - 2017-12-09 14:00:19.688045: step 30440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:47m:38s remains)
INFO - root - 2017-12-09 14:00:28.253378: step 30450, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 69h:53m:15s remains)
INFO - root - 2017-12-09 14:00:36.702907: step 30460, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:02m:43s remains)
INFO - root - 2017-12-09 14:00:45.513144: step 30470, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 72h:38m:25s remains)
INFO - root - 2017-12-09 14:00:54.060087: step 30480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 73h:29m:43s remains)
INFO - root - 2017-12-09 14:01:02.709744: step 30490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:03m:58s remains)
INFO - root - 2017-12-09 14:01:11.155288: step 30500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:49m:51s remains)
2017-12-09 14:01:12.001883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033928722 -0.0033899522 -0.003388247 -0.0033871685 -0.0033865704 -0.0033860926 -0.0033856763 -0.0033854828 -0.0033861473 -0.0033876575 -0.0033898223 -0.0033916731 -0.0033922496 -0.0033913164 -0.0033905585][-0.0033898626 -0.0033868609 -0.0033853683 -0.0033829927 -0.0033737549 -0.003360149 -0.003348869 -0.0033489873 -0.0033561932 -0.0033660491 -0.0033724089 -0.0033755798 -0.0033736969 -0.0033738222 -0.0033765447][-0.0033884069 -0.0033859 -0.0033827249 -0.0033597285 -0.0032879666 -0.0031910655 -0.0031203865 -0.0031216743 -0.0031657903 -0.0031861879 -0.0031371852 -0.0030344692 -0.0029587836 -0.0029626507 -0.0030481494][-0.0033882027 -0.0033862037 -0.003373869 -0.0032925536 -0.0030600643 -0.0027389557 -0.0024932148 -0.0024789488 -0.0025708028 -0.0024938094 -0.0021351457 -0.0015845557 -0.0012492205 -0.0012903942 -0.0017107368][-0.0033881429 -0.0033860437 -0.0033649092 -0.0032097369 -0.0027680644 -0.0021615773 -0.0016971541 -0.001666254 -0.0018015573 -0.0015381142 -0.00064993673 0.00059512653 0.0013224997 0.00121306 0.0002827784][-0.0033883711 -0.003386436 -0.0033597965 -0.0031664786 -0.0026338068 -0.0019130579 -0.0013600388 -0.0013205204 -0.0014536199 -0.0010151274 0.00029228558 0.0020498822 0.0030898119 0.0029372617 0.0016547043][-0.0033887576 -0.0033873131 -0.0033658308 -0.0032071667 -0.0027636443 -0.002176295 -0.001721517 -0.0016844742 -0.0017885254 -0.0013614839 -0.00012435042 0.0015205874 0.002523171 0.0023959994 0.0012263234][-0.0033887229 -0.0033878959 -0.0033766269 -0.0032897699 -0.0030412159 -0.0027114348 -0.0024515963 -0.0024258494 -0.0024813369 -0.0021974202 -0.0013776727 -0.00028506992 0.00040454231 0.00033579115 -0.00041301874][-0.0033839068 -0.0033857073 -0.0033841953 -0.0033527706 -0.0032552811 -0.0031248869 -0.0030197781 -0.0030069537 -0.0030290952 -0.0029015583 -0.0025270181 -0.0020204866 -0.0016859774 -0.0017054873 -0.0020398025][-0.0033671446 -0.0033737519 -0.0033808663 -0.003379063 -0.0033570735 -0.0033237329 -0.0032947939 -0.0032899622 -0.0032968137 -0.0032599855 -0.0031431252 -0.0029802304 -0.0028660302 -0.0028652675 -0.0029668279][-0.0033448928 -0.0033556554 -0.0033693872 -0.0033799324 -0.0033839385 -0.0033826015 -0.0033792076 -0.0033784071 -0.0033801892 -0.0033747293 -0.0033535236 -0.0033218346 -0.003297033 -0.0032934002 -0.0033106483][-0.0033363318 -0.0033462099 -0.0033607564 -0.0033744047 -0.0033840458 -0.0033894519 -0.003391908 -0.0033928277 -0.0033937586 -0.003394373 -0.0033934251 -0.0033909348 -0.003387921 -0.0033865881 -0.0033875152][-0.0033510528 -0.0033558391 -0.0033655055 -0.0033753919 -0.0033829249 -0.0033877778 -0.0033907844 -0.003392075 -0.0033929567 -0.003393715 -0.0033941984 -0.0033941427 -0.0033939087 -0.0033941395 -0.0033948002][-0.0033724781 -0.0033736839 -0.0033778904 -0.0033823573 -0.0033851357 -0.0033868002 -0.0033880444 -0.0033886263 -0.0033892896 -0.0033901867 -0.0033910777 -0.0033915087 -0.0033920079 -0.0033927141 -0.0033934151][-0.0033884528 -0.0033880821 -0.0033886114 -0.0033891778 -0.0033883993 -0.0033868053 -0.0033857464 -0.0033853131 -0.0033854174 -0.0033860507 -0.0033870882 -0.0033878672 -0.0033886207 -0.0033895739 -0.003390393]]...]
INFO - root - 2017-12-09 14:01:20.405090: step 30510, loss = 0.89, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 65h:09m:54s remains)
INFO - root - 2017-12-09 14:01:28.945523: step 30520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:58m:41s remains)
INFO - root - 2017-12-09 14:01:37.586355: step 30530, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:10m:11s remains)
INFO - root - 2017-12-09 14:01:46.358968: step 30540, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:08m:13s remains)
INFO - root - 2017-12-09 14:01:55.068098: step 30550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 73h:11m:22s remains)
INFO - root - 2017-12-09 14:02:03.514073: step 30560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 72h:20m:18s remains)
INFO - root - 2017-12-09 14:02:12.174722: step 30570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:38m:49s remains)
INFO - root - 2017-12-09 14:02:20.851938: step 30580, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 74h:29m:13s remains)
INFO - root - 2017-12-09 14:02:29.553370: step 30590, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 73h:28m:21s remains)
INFO - root - 2017-12-09 14:02:38.039966: step 30600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 74h:05m:55s remains)
2017-12-09 14:02:38.975734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014988281 -0.0011043712 -0.00016371394 0.0011998715 0.0031445089 0.0049910126 0.0066002673 0.0075469678 0.0076486133 0.0070046084 0.0057751639 0.0044254921 0.0029146916 0.0014190774 2.4613924e-05][0.0016106095 0.0024666477 0.00411258 0.0063354149 0.0092618605 0.012074807 0.014296391 0.01518616 0.014634893 0.012792085 0.010106651 0.0073291287 0.0046947114 0.0026107633 0.00095113437][0.0043123672 0.0062606409 0.0094954893 0.013802337 0.018934797 0.02366098 0.027031081 0.028166004 0.026933175 0.023571044 0.018968206 0.014109028 0.0094697531 0.0056079132 0.0025530972][0.0069295121 0.010608895 0.016387088 0.023908503 0.03246605 0.040213984 0.045459885 0.047121171 0.045107543 0.040131006 0.033203758 0.025514714 0.017921621 0.011262544 0.0058404179][0.010362011 0.015912194 0.024355182 0.035459198 0.047953922 0.059273168 0.067039825 0.0697661 0.0672912 0.060621917 0.051044658 0.040052768 0.028952044 0.018901559 0.010613061][0.013452019 0.021461109 0.033086035 0.047947578 0.064194679 0.0787076 0.088736773 0.092647828 0.090248 0.082283892 0.070226125 0.056007434 0.041308403 0.027676467 0.016389377][0.014812016 0.024397383 0.038574874 0.056840271 0.076727487 0.094354525 0.10655814 0.11162795 0.10948038 0.10093145 0.087530151 0.071201265 0.053842835 0.03736135 0.023517052][0.015137724 0.024894942 0.039732859 0.059440918 0.081523106 0.10171379 0.11650355 0.12369564 0.12293798 0.11484238 0.101079 0.084009446 0.065530531 0.047652047 0.032232802][0.013936085 0.022885622 0.036821779 0.055848535 0.07796903 0.09931761 0.11642901 0.12651291 0.12865926 0.12290666 0.11078327 0.09462449 0.076487459 0.058818024 0.043315731][0.010918812 0.01845514 0.03038108 0.047100741 0.067544021 0.088653155 0.10728711 0.120253 0.12601994 0.12380208 0.11475123 0.10106619 0.084959023 0.069127306 0.05509695][0.006739975 0.01231465 0.021494178 0.034989711 0.052473549 0.0720126 0.0910811 0.10647136 0.11598545 0.1183637 0.11390327 0.1042184 0.091548011 0.078690827 0.067078583][0.002978977 0.0065403981 0.012655537 0.022306284 0.035945609 0.052752938 0.070843324 0.087365329 0.099849693 0.10639139 0.10653577 0.10136635 0.092827491 0.083582841 0.074978][0.00019714306 0.0023144993 0.0059563117 0.012004086 0.021353655 0.034105577 0.049312729 0.064782307 0.0782343 0.087546691 0.091612011 0.090686604 0.086211815 0.080557317 0.074951269][-0.0016995206 -0.00052176905 0.0014619434 0.0048933486 0.01063048 0.019192711 0.030299567 0.042655841 0.05456987 0.064181 0.070148528 0.072167024 0.071034722 0.068511836 0.065602921][-0.0027165227 -0.0021717802 -0.001248694 0.00045099785 0.0035567207 0.0086009232 0.015676899 0.02416563 0.033016436 0.040888935 0.046626642 0.049750734 0.050558079 0.05014015 0.049213652]]...]
INFO - root - 2017-12-09 14:02:47.271941: step 30610, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.716 sec/batch; 60h:01m:53s remains)
INFO - root - 2017-12-09 14:02:55.771102: step 30620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:48m:58s remains)
INFO - root - 2017-12-09 14:03:04.398976: step 30630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:22m:46s remains)
INFO - root - 2017-12-09 14:03:13.086143: step 30640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:33m:21s remains)
INFO - root - 2017-12-09 14:03:21.708730: step 30650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 72h:07m:58s remains)
INFO - root - 2017-12-09 14:03:30.171530: step 30660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:50m:09s remains)
INFO - root - 2017-12-09 14:03:38.735977: step 30670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:41m:06s remains)
INFO - root - 2017-12-09 14:03:47.317813: step 30680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:30m:43s remains)
INFO - root - 2017-12-09 14:03:55.852042: step 30690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:48s remains)
INFO - root - 2017-12-09 14:04:04.333749: step 30700, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 74h:51m:48s remains)
2017-12-09 14:04:05.196604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033729693 -0.0033704035 -0.0033701756 -0.0033701423 -0.0033702846 -0.0033704424 -0.0033707151 -0.0033710811 -0.0033715602 -0.0033719942 -0.0033723277 -0.0033723591 -0.0033724199 -0.0033723612 -0.0033721777][-0.003371085 -0.0033681176 -0.0033675716 -0.0033674936 -0.0033675958 -0.0033678396 -0.0033681996 -0.0033685092 -0.0033688713 -0.0033692424 -0.0033696597 -0.0033698692 -0.0033699519 -0.0033699635 -0.0033698203][-0.0033717623 -0.0033687148 -0.0033681234 -0.0033680529 -0.0033681411 -0.0033683365 -0.0033685316 -0.0033685085 -0.0033684119 -0.0033686431 -0.0033690429 -0.0033693181 -0.0033694019 -0.0033694208 -0.0033693954][-0.003372947 -0.003370034 -0.0033695085 -0.0033692592 -0.0033693661 -0.0033697074 -0.0033699395 -0.003369973 -0.0033698743 -0.0033700604 -0.003370336 -0.0033703924 -0.0033700131 -0.0033696236 -0.003369289][-0.0033754397 -0.0033728776 -0.0033724802 -0.0033722024 -0.00337246 -0.0033728576 -0.0033732539 -0.0033735472 -0.0033736823 -0.0033737456 -0.0033735705 -0.0033729984 -0.0033718771 -0.0033707975 -0.0033698718][-0.0033784139 -0.0033763503 -0.0033764285 -0.0033765321 -0.0033772781 -0.0033779673 -0.0033784269 -0.0033786544 -0.0033788108 -0.00337867 -0.003378209 -0.0033773226 -0.003375577 -0.0033737423 -0.003372089][-0.0033817345 -0.0033806372 -0.0033813934 -0.003382473 -0.0033835983 -0.0033842139 -0.0033844297 -0.0033844688 -0.003384744 -0.003384935 -0.0033846183 -0.0033831866 -0.003380778 -0.0033781333 -0.0033755382][-0.0033851822 -0.0033856668 -0.003387555 -0.0033894891 -0.0033909967 -0.0033914871 -0.0033914782 -0.0033914924 -0.0033921457 -0.0033928799 -0.0033924335 -0.0033905453 -0.0033874796 -0.0033840237 -0.0033803708][-0.0033878628 -0.0033898309 -0.0033932619 -0.0033963106 -0.0033984536 -0.0033994138 -0.0033996189 -0.0033995679 -0.003400462 -0.0034011549 -0.0034005048 -0.0033983802 -0.003394983 -0.0033910896 -0.0033864523][-0.0033887567 -0.0033921772 -0.003397281 -0.0034017961 -0.0034050883 -0.0034067691 -0.0034070774 -0.0034066972 -0.003407015 -0.0034072008 -0.0034062222 -0.0034039272 -0.0034005218 -0.0033961085 -0.003390691][-0.0033877806 -0.0033920293 -0.0033982967 -0.0034037805 -0.003407659 -0.0034094411 -0.003409615 -0.0034090972 -0.00340901 -0.0034087899 -0.0034074418 -0.0034048718 -0.0034012604 -0.0033966131 -0.003391138][-0.0033857834 -0.0033902961 -0.0033968424 -0.0034022757 -0.0034060099 -0.0034077559 -0.0034078441 -0.0034072618 -0.0034068995 -0.0034063952 -0.0034048867 -0.0034020678 -0.0033982019 -0.00339353 -0.0033884246][-0.003383334 -0.003387094 -0.0033932829 -0.0033985376 -0.0034022226 -0.0034040315 -0.0034042904 -0.0034037575 -0.0034033926 -0.0034024527 -0.0034006396 -0.0033976205 -0.0033936871 -0.0033891057 -0.0033842416][-0.0033810579 -0.0033835245 -0.0033889015 -0.0033936636 -0.0033971041 -0.003398889 -0.0033995672 -0.0033995088 -0.0033992629 -0.0033980417 -0.003395752 -0.0033924871 -0.0033884188 -0.0033839406 -0.0033795435][-0.0033784686 -0.0033795836 -0.0033836442 -0.0033874195 -0.0033904563 -0.0033923795 -0.0033933257 -0.0033935977 -0.0033933928 -0.0033920989 -0.0033897 -0.0033865573 -0.0033829859 -0.0033792628 -0.0033757102]]...]
INFO - root - 2017-12-09 14:04:13.675321: step 30710, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 63h:16m:11s remains)
INFO - root - 2017-12-09 14:04:22.320607: step 30720, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:13m:22s remains)
INFO - root - 2017-12-09 14:04:30.940651: step 30730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:26m:09s remains)
INFO - root - 2017-12-09 14:04:39.389033: step 30740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:50m:48s remains)
INFO - root - 2017-12-09 14:04:47.966320: step 30750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:21m:01s remains)
INFO - root - 2017-12-09 14:04:56.472593: step 30760, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 75h:01m:26s remains)
INFO - root - 2017-12-09 14:05:05.135687: step 30770, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:40m:02s remains)
INFO - root - 2017-12-09 14:05:13.831274: step 30780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 72h:45m:26s remains)
INFO - root - 2017-12-09 14:05:22.521000: step 30790, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 75h:43m:49s remains)
INFO - root - 2017-12-09 14:05:31.007194: step 30800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:30m:44s remains)
2017-12-09 14:05:31.896901: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.55217463 0.54850221 0.54244041 0.53587461 0.52855217 0.52038807 0.51062441 0.50411445 0.49627835 0.48586798 0.47425386 0.46298048 0.45058846 0.43265784 0.41538015][0.57953137 0.58254588 0.58206165 0.58066469 0.57660168 0.569921 0.56108 0.55448622 0.54639661 0.53421557 0.52061081 0.50610405 0.48988619 0.46745047 0.44395092][0.59674883 0.60927832 0.61689943 0.62163073 0.62162805 0.61634934 0.60802931 0.60162729 0.59227526 0.57854748 0.5623 0.54446715 0.52354419 0.49549767 0.46545684][0.60413134 0.62806785 0.64637792 0.65884 0.66350961 0.66061497 0.65266556 0.64472038 0.63277197 0.61657292 0.59738785 0.57630211 0.55192494 0.51979256 0.48514268][0.60636806 0.64023304 0.66657472 0.68697846 0.69694322 0.696521 0.68928641 0.67977679 0.66535717 0.64622617 0.62407362 0.60011607 0.57258904 0.53775942 0.499773][0.60211438 0.64461803 0.67787218 0.703928 0.71706355 0.71807647 0.71189719 0.70147008 0.685694 0.66482079 0.64082569 0.61524868 0.58528262 0.54910207 0.50924706][0.59979409 0.64904332 0.68746972 0.71781152 0.73272288 0.73241884 0.72389942 0.71130651 0.69285327 0.67050135 0.64635414 0.6204955 0.58999807 0.55388635 0.51389891][0.59009618 0.64683729 0.6900636 0.72240394 0.73829776 0.73734683 0.72640604 0.71035647 0.68871409 0.66444826 0.63934851 0.61379528 0.58430254 0.54930431 0.51079518][0.57607937 0.63606727 0.68136632 0.71561688 0.7330007 0.73196936 0.7200762 0.701482 0.67764175 0.65147924 0.62557757 0.60047746 0.5722906 0.53958559 0.50380689][0.56343639 0.62323791 0.66621608 0.69961643 0.716793 0.715712 0.70309377 0.68405396 0.66026735 0.63364989 0.60785389 0.58341885 0.55724007 0.52712166 0.49397933][0.54752916 0.60541362 0.64488208 0.67417532 0.68940842 0.6876775 0.67506516 0.65639424 0.63362813 0.60890865 0.58494544 0.56290931 0.53928661 0.51233011 0.4824428][0.52509093 0.57978362 0.61531872 0.64116395 0.65446055 0.6528331 0.64134705 0.62400794 0.6032837 0.58103758 0.55964333 0.5400092 0.51920819 0.49555704 0.4691523][0.49559447 0.54569536 0.57596177 0.59825683 0.61015517 0.60937548 0.600491 0.5859921 0.56867331 0.54955578 0.53108716 0.51435691 0.49663183 0.47640067 0.45368996][0.46788421 0.50995457 0.53350896 0.55153471 0.56175393 0.56202209 0.55612206 0.54555649 0.53262353 0.51751453 0.5027957 0.48922536 0.47466314 0.45791119 0.43906292][0.43896279 0.47324368 0.48984694 0.50262725 0.51038808 0.51098007 0.50740337 0.50083721 0.49246514 0.48206946 0.47138843 0.46115994 0.45005319 0.43710923 0.42250597]]...]
INFO - root - 2017-12-09 14:05:40.397864: step 30810, loss = 0.89, batch loss = 0.69 (10.8 examples/sec; 0.740 sec/batch; 62h:00m:18s remains)
INFO - root - 2017-12-09 14:05:48.949210: step 30820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:13m:29s remains)
INFO - root - 2017-12-09 14:05:57.424771: step 30830, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:44m:50s remains)
INFO - root - 2017-12-09 14:06:06.068207: step 30840, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:10s remains)
INFO - root - 2017-12-09 14:06:14.714852: step 30850, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 69h:13m:47s remains)
INFO - root - 2017-12-09 14:06:23.057247: step 30860, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.689 sec/batch; 57h:44m:18s remains)
INFO - root - 2017-12-09 14:06:31.591175: step 30870, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 69h:36m:42s remains)
INFO - root - 2017-12-09 14:06:40.182875: step 30880, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:34m:37s remains)
INFO - root - 2017-12-09 14:06:48.817357: step 30890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:27m:37s remains)
INFO - root - 2017-12-09 14:06:57.263021: step 30900, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 59h:23m:39s remains)
2017-12-09 14:06:58.193044: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0048750066 0.0044391309 0.0041917092 0.0043948968 0.0050016944 0.0058288691 0.006585821 0.0070867469 0.0071007186 0.0067967759 0.0069146664 0.0084708538 0.012167789 0.017634438 0.023856591][0.0068617309 0.0063077831 0.00578504 0.0055158674 0.0056585758 0.00636458 0.0075633372 0.0089250952 0.0099998508 0.010606673 0.011310823 0.012925351 0.016037488 0.020216564 0.024686795][0.008070983 0.0072748433 0.0065062805 0.0059548379 0.0059648152 0.0069624935 0.0090062637 0.011665482 0.014145032 0.015872551 0.017077977 0.018349877 0.020192536 0.022392171 0.024584871][0.0091487058 0.0081652831 0.0073320423 0.0068562166 0.0071540833 0.0087700486 0.011764506 0.015692679 0.019470604 0.022220697 0.023750829 0.024377348 0.02448578 0.024182806 0.02363381][0.01011182 0.0092719663 0.0087753162 0.008805085 0.0097391177 0.01210727 0.01593807 0.020710934 0.025167514 0.028338861 0.0297983 0.029604319 0.027908485 0.025073145 0.02178311][0.011027059 0.010663185 0.010899521 0.011869919 0.013825873 0.017105997 0.021569282 0.026583096 0.030929118 0.033746615 0.034619886 0.033453196 0.03019014 0.025303835 0.019769358][0.012717301 0.013265405 0.014588406 0.016703332 0.019653162 0.023492025 0.02791968 0.032310285 0.035726309 0.037618816 0.037726194 0.035844613 0.031646535 0.025524365 0.018494198][0.015970027 0.017687112 0.020149006 0.023242254 0.026845826 0.030745469 0.034519777 0.037613861 0.039474294 0.039988596 0.0391279 0.036720525 0.032245014 0.025920907 0.018543722][0.020396531 0.023143129 0.026475234 0.03017316 0.03389959 0.037331447 0.040100764 0.041809626 0.042196032 0.041451234 0.039763935 0.037111115 0.032948006 0.027289579 0.020587025][0.02588792 0.02923334 0.032865733 0.036542434 0.039896242 0.0425919 0.044392925 0.044992145 0.044336122 0.042807031 0.040764339 0.038281951 0.034877811 0.030422036 0.025062243][0.031752408 0.035280451 0.038653314 0.041778736 0.044379558 0.046247903 0.047304269 0.047346376 0.046393689 0.0448236 0.043028377 0.041158743 0.038843382 0.035886392 0.03220487][0.036774121 0.040502954 0.04351021 0.045840092 0.047413889 0.048390329 0.048823286 0.04868716 0.047980115 0.046955038 0.045943346 0.045074686 0.044033498 0.0426243 0.040651627][0.039456438 0.043419681 0.046165764 0.047879361 0.048735049 0.049188282 0.049263548 0.049147669 0.048822876 0.048529271 0.048497777 0.048790071 0.04908311 0.049117632 0.048689075][0.0388714 0.043239452 0.046149511 0.047808666 0.048523054 0.048963644 0.049103342 0.049191676 0.049134217 0.049300749 0.049919575 0.051142327 0.052645426 0.053999584 0.054905213][0.035061713 0.039946862 0.0434937 0.045796983 0.04712376 0.048132107 0.048780024 0.0492289 0.049378846 0.049683947 0.0504128 0.051922914 0.054011576 0.056269962 0.058311488]]...]
INFO - root - 2017-12-09 14:07:06.570860: step 30910, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.766 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-09 14:07:15.042690: step 30920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:22m:33s remains)
INFO - root - 2017-12-09 14:07:23.572496: step 30930, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 76h:11m:42s remains)
INFO - root - 2017-12-09 14:07:32.150263: step 30940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:07m:06s remains)
INFO - root - 2017-12-09 14:07:40.701256: step 30950, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 71h:37m:19s remains)
INFO - root - 2017-12-09 14:07:49.134511: step 30960, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:28m:25s remains)
INFO - root - 2017-12-09 14:07:57.596235: step 30970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:46m:36s remains)
INFO - root - 2017-12-09 14:08:06.150963: step 30980, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 70h:18m:41s remains)
INFO - root - 2017-12-09 14:08:14.728845: step 30990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:59m:07s remains)
INFO - root - 2017-12-09 14:08:23.279226: step 31000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:18m:31s remains)
2017-12-09 14:08:24.155651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033609641 -0.003358345 -0.0033580288 -0.0033579918 -0.0033580533 -0.0033582821 -0.0033585383 -0.0033587408 -0.0033588696 -0.0033587608 -0.0033585581 -0.0033582917 -0.0033579855 -0.0033577376 -0.0033575008][-0.0033602873 -0.0033574062 -0.0033570142 -0.0033569697 -0.0033571064 -0.0033573981 -0.0033577213 -0.0033579273 -0.0033580414 -0.0033579818 -0.003357738 -0.0033573655 -0.0033569247 -0.0033564898 -0.0033561047][-0.003360919 -0.0033579313 -0.003357464 -0.0033573597 -0.0033575515 -0.0033579266 -0.0033582828 -0.0033584889 -0.0033586579 -0.0033586284 -0.0033583415 -0.0033578868 -0.0033573541 -0.0033568107 -0.0033562956][-0.0033607529 -0.0033575727 -0.0033570034 -0.0033567445 -0.003356888 -0.0033572949 -0.0033577264 -0.0033580135 -0.0033583227 -0.0033583986 -0.0033582014 -0.0033578034 -0.0033573122 -0.0033568046 -0.0033563022][-0.0033599532 -0.0033565268 -0.0033558272 -0.0033554593 -0.0033554272 -0.003355687 -0.0033560228 -0.0033563643 -0.0033568209 -0.0033570759 -0.0033571313 -0.0033569506 -0.0033567136 -0.0033564528 -0.0033561119][-0.0033586766 -0.0033549648 -0.0033542104 -0.0033537694 -0.0033536307 -0.0033537145 -0.0033539713 -0.0033544507 -0.0033550651 -0.0033555198 -0.0033558686 -0.0033560179 -0.0033561268 -0.0033561557 -0.0033560074][-0.0033572093 -0.0033534374 -0.0033527093 -0.0033522304 -0.0033518216 -0.0033515962 -0.003351745 -0.0033523431 -0.0033531424 -0.0033538376 -0.0033544935 -0.003355033 -0.0033554963 -0.0033557971 -0.0033558416][-0.003355949 -0.0033523762 -0.0033518292 -0.0033513338 -0.0033507033 -0.003350198 -0.0033501836 -0.0033507429 -0.0033515918 -0.0033524286 -0.0033533801 -0.0033542912 -0.0033550488 -0.0033555655 -0.0033558197][-0.0033553224 -0.0033519855 -0.0033516719 -0.0033512255 -0.003350541 -0.0033499254 -0.0033498025 -0.0033502802 -0.0033510984 -0.0033519648 -0.0033531007 -0.0033541964 -0.0033550954 -0.0033557052 -0.003356053][-0.003355674 -0.0033525247 -0.0033524726 -0.0033521622 -0.0033515892 -0.0033510216 -0.0033508865 -0.0033513075 -0.0033519857 -0.0033527587 -0.003353826 -0.0033548307 -0.0033556295 -0.0033561469 -0.0033564281][-0.0033566575 -0.0033537431 -0.0033539014 -0.0033536304 -0.0033531818 -0.0033527182 -0.0033526311 -0.0033529971 -0.0033535417 -0.0033541806 -0.0033550672 -0.0033558609 -0.0033563909 -0.0033566731 -0.0033567972][-0.003357522 -0.0033547662 -0.0033550188 -0.0033547857 -0.0033544353 -0.0033541126 -0.0033540523 -0.0033542935 -0.0033546768 -0.0033551625 -0.003355809 -0.0033563492 -0.0033566572 -0.0033567813 -0.0033568318][-0.0033582626 -0.0033553801 -0.0033556779 -0.0033555436 -0.0033553317 -0.0033551238 -0.0033550796 -0.0033552151 -0.0033554435 -0.0033557452 -0.0033561392 -0.0033564314 -0.003356583 -0.0033565988 -0.0033565913][-0.0033588475 -0.0033557625 -0.0033560342 -0.0033559497 -0.0033558288 -0.0033557161 -0.0033557026 -0.0033557839 -0.0033559073 -0.0033560626 -0.003356246 -0.0033563531 -0.0033563706 -0.0033562903 -0.0033562067][-0.0033593418 -0.0033561129 -0.003356263 -0.0033561741 -0.003356064 -0.0033559874 -0.0033559748 -0.003355996 -0.0033560363 -0.0033560891 -0.0033561215 -0.0033561075 -0.00335604 -0.0033559159 -0.0033557971]]...]
INFO - root - 2017-12-09 14:08:32.521958: step 31010, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 63h:58m:41s remains)
INFO - root - 2017-12-09 14:08:41.236655: step 31020, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:31m:44s remains)
INFO - root - 2017-12-09 14:08:49.935171: step 31030, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 74h:19m:55s remains)
INFO - root - 2017-12-09 14:08:58.642158: step 31040, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:45m:35s remains)
INFO - root - 2017-12-09 14:09:07.273598: step 31050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:08m:06s remains)
INFO - root - 2017-12-09 14:09:15.999858: step 31060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:31m:04s remains)
INFO - root - 2017-12-09 14:09:24.484216: step 31070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:38m:47s remains)
INFO - root - 2017-12-09 14:09:33.126160: step 31080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:40m:12s remains)
INFO - root - 2017-12-09 14:09:42.022216: step 31090, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.933 sec/batch; 78h:04m:31s remains)
INFO - root - 2017-12-09 14:09:50.816465: step 31100, loss = 0.89, batch loss = 0.69 (8.1 examples/sec; 0.993 sec/batch; 83h:08m:23s remains)
2017-12-09 14:09:51.733279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003342811 -0.0031906455 -0.0027420362 -0.0019721054 -0.00113099 -0.00058125146 -0.0006039585 -0.0011953067 -0.0020753236 -0.0028229675 -0.0032254027 -0.0033646105 -0.0033861352 -0.0033860973 -0.003381884][-0.0032866772 -0.0029602535 -0.0021278169 -0.00077460986 0.00069855177 0.0016828042 0.0016928061 0.00070541329 -0.00081191747 -0.0021718603 -0.0029834437 -0.0033084708 -0.0033795894 -0.0033831978 -0.0033795142][-0.0030669973 -0.0023247688 -0.00072681508 0.0016947945 0.0042714113 0.0059824688 0.0060204472 0.004319787 0.0016639775 -0.00082730083 -0.0024397324 -0.0031633282 -0.0033622105 -0.003381466 -0.0033802765][-0.002418156 -0.00074877241 0.0023508228 0.006669309 0.011055605 0.013829835 0.013729839 0.010695832 0.0060411189 0.0015815676 -0.0014435893 -0.0028868173 -0.0033243834 -0.0033768916 -0.0033784155][-0.0010192802 0.0022729647 0.0077040847 0.014694588 0.021351669 0.025174065 0.024471546 0.019348247 0.011880998 0.0047693355 -0.00011149794 -0.0025040614 -0.0032695015 -0.0033708601 -0.0033745922][0.0012520941 0.0068371408 0.015281787 0.025409067 0.034400143 0.038900919 0.03694874 0.029049499 0.018228143 0.0081403041 0.0012614971 -0.0021154454 -0.0032106261 -0.0033628659 -0.0033683628][0.0039955024 0.011971115 0.023241645 0.035879746 0.046191588 0.0503315 0.046507694 0.035905465 0.022403264 0.010233178 0.0020887877 -0.0018802293 -0.0031717657 -0.0033571906 -0.0033633141][0.0061821928 0.015645213 0.028307391 0.041627668 0.051503453 0.054218173 0.04858778 0.036504604 0.022222178 0.0098807123 0.0018658491 -0.0019531939 -0.0031762861 -0.003353639 -0.0033592077][0.0066342121 0.015837815 0.027650494 0.039387286 0.047224034 0.048153788 0.041741498 0.030229913 0.017567627 0.0071512759 0.00066279224 -0.0023119072 -0.0032259459 -0.0033534598 -0.003357141][0.0050911345 0.012347583 0.021335969 0.029780885 0.034752872 0.03427219 0.028491719 0.019487372 0.01029985 0.003187312 -0.00098740845 -0.0027871556 -0.0032982172 -0.0033612966 -0.0033616242][0.0022663658 0.0067888638 0.012203507 0.016984696 0.019336687 0.018249242 0.014145756 0.0085467761 0.0033266058 -0.00038621691 -0.00237507 -0.0031519036 -0.003346158 -0.0033647437 -0.0033629802][-0.0005340029 0.0016339861 0.0041032983 0.0060941866 0.0067805042 0.0058284355 0.0036315338 0.0010271159 -0.0011327455 -0.0024923699 -0.0031216585 -0.0033304279 -0.0033712564 -0.0033725067 -0.0033700622][-0.0024660681 -0.0017321099 -0.00095106894 -0.00040208595 -0.00033961236 -0.00080671604 -0.001596677 -0.0024001028 -0.0029645404 -0.003255141 -0.0033547161 -0.0033763675 -0.0033771058 -0.0033761705 -0.00337461][-0.0032452876 -0.0031108141 -0.0029732126 -0.0028880117 -0.0029003348 -0.0030048094 -0.0031498903 -0.0032761768 -0.0033458602 -0.003371214 -0.0033760057 -0.0033772872 -0.0033775121 -0.0033767018 -0.0033751174][-0.003374411 -0.0033690636 -0.0033630123 -0.0033559701 -0.0033494595 -0.0033455731 -0.0033452094 -0.003347357 -0.0033501149 -0.0033524614 -0.0033559622 -0.0033603418 -0.0033648366 -0.0033687043 -0.003371055]]...]
INFO - root - 2017-12-09 14:10:00.027992: step 31110, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 63h:09m:09s remains)
INFO - root - 2017-12-09 14:10:08.672327: step 31120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 70h:16m:40s remains)
INFO - root - 2017-12-09 14:10:17.173840: step 31130, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 75h:34m:22s remains)
INFO - root - 2017-12-09 14:10:25.773275: step 31140, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.838 sec/batch; 70h:07m:09s remains)
INFO - root - 2017-12-09 14:10:34.489039: step 31150, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 75h:23m:35s remains)
INFO - root - 2017-12-09 14:10:43.243924: step 31160, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 75h:30m:07s remains)
INFO - root - 2017-12-09 14:10:51.921476: step 31170, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 74h:41m:53s remains)
INFO - root - 2017-12-09 14:11:00.681305: step 31180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:24m:06s remains)
INFO - root - 2017-12-09 14:11:09.490876: step 31190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:10m:52s remains)
INFO - root - 2017-12-09 14:11:18.234035: step 31200, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 71h:23m:20s remains)
2017-12-09 14:11:19.115868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033877804 -0.0033860402 -0.0033852255 -0.0033845198 -0.003384274 -0.0033845184 -0.0033851806 -0.0033863017 -0.0033882188 -0.0033908559 -0.0033933204 -0.0033947155 -0.003395411 -0.0033955951 -0.003394963][-0.0033865075 -0.0033843254 -0.003382988 -0.0033820104 -0.0033815738 -0.0033819356 -0.0033828048 -0.0033842695 -0.00338698 -0.0033905411 -0.0033935474 -0.0033947709 -0.0033946002 -0.0033938582 -0.0033921802][-0.0033871518 -0.0033849454 -0.0033834083 -0.0033821771 -0.0033815892 -0.0033817987 -0.0033826146 -0.0033844169 -0.0033876363 -0.003391861 -0.0033951709 -0.0033962382 -0.0033951825 -0.0033931085 -0.0033899155][-0.0033875299 -0.00338547 -0.0033839531 -0.0033825831 -0.0033816269 -0.003381537 -0.0033825941 -0.003384907 -0.0033888237 -0.0033939017 -0.0033974734 -0.0033981781 -0.0033959309 -0.0033919497 -0.0033866053][-0.003387783 -0.0033860612 -0.0033848055 -0.0033835545 -0.0033822318 -0.0033817231 -0.0033825713 -0.003385158 -0.0033897709 -0.0033955378 -0.0033994929 -0.0033999591 -0.003396967 -0.0033915557 -0.0033843589][-0.0033878703 -0.003386599 -0.0033860498 -0.0033851131 -0.0033837222 -0.0033826614 -0.0033828516 -0.0033851722 -0.003389617 -0.0033958585 -0.0034003605 -0.0034009884 -0.0033977581 -0.0033922128 -0.0033847191][-0.0033875671 -0.0033868097 -0.0033871343 -0.0033870635 -0.0033860118 -0.0033844449 -0.0033833385 -0.0033847725 -0.0033888824 -0.0033947735 -0.0033999383 -0.0034013814 -0.0033987712 -0.0033937683 -0.0033869185][-0.0033869529 -0.0033865955 -0.0033877564 -0.0033886826 -0.0033885024 -0.0033869664 -0.0033846879 -0.003384734 -0.0033878344 -0.0033932363 -0.0033986003 -0.0034010964 -0.0033999484 -0.0033961 -0.0033904954][-0.0033862619 -0.003386115 -0.0033878295 -0.003389532 -0.0033903492 -0.0033894065 -0.0033875459 -0.0033868172 -0.0033882784 -0.0033923022 -0.0033972284 -0.0034004475 -0.0034010217 -0.0033992426 -0.0033951798][-0.0033855834 -0.0033854349 -0.003387348 -0.003389478 -0.0033910142 -0.003391233 -0.0033901948 -0.003389373 -0.0033897087 -0.0033921849 -0.0033962086 -0.0033996259 -0.0034015048 -0.0034014245 -0.0033994731][-0.0033843259 -0.00338381 -0.0033857233 -0.0033878537 -0.003389922 -0.0033909203 -0.0033909492 -0.0033905318 -0.0033905872 -0.003391925 -0.0033945348 -0.0033975116 -0.0033999553 -0.0034011253 -0.0034011137][-0.0033831391 -0.003381989 -0.0033833508 -0.0033850949 -0.0033869643 -0.003388284 -0.003388939 -0.0033893099 -0.0033896021 -0.0033903073 -0.0033918677 -0.0033941653 -0.0033966207 -0.0033983851 -0.0033994878][-0.0033823659 -0.0033806616 -0.0033813945 -0.0033823368 -0.0033836965 -0.0033850167 -0.0033858409 -0.0033863771 -0.0033867762 -0.0033872072 -0.0033882605 -0.0033900624 -0.0033922289 -0.0033941139 -0.0033954943][-0.0033817713 -0.0033796837 -0.0033798709 -0.0033803191 -0.0033810076 -0.003381775 -0.0033824726 -0.003383063 -0.0033834346 -0.0033837345 -0.0033842735 -0.0033854588 -0.0033869897 -0.0033886689 -0.0033900375][-0.0033815727 -0.0033793638 -0.0033792842 -0.0033793452 -0.0033795447 -0.0033798423 -0.0033800961 -0.003380416 -0.0033806516 -0.0033808951 -0.0033812008 -0.0033819007 -0.0033826898 -0.0033837229 -0.0033848183]]...]
INFO - root - 2017-12-09 14:11:27.585259: step 31210, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.704 sec/batch; 58h:57m:33s remains)
INFO - root - 2017-12-09 14:11:36.212510: step 31220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:48m:56s remains)
INFO - root - 2017-12-09 14:11:44.863074: step 31230, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 69h:50m:09s remains)
INFO - root - 2017-12-09 14:11:53.368375: step 31240, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 71h:03m:21s remains)
INFO - root - 2017-12-09 14:12:01.863742: step 31250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 72h:30m:42s remains)
INFO - root - 2017-12-09 14:12:10.488603: step 31260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:22m:55s remains)
INFO - root - 2017-12-09 14:12:19.132397: step 31270, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 71h:52m:36s remains)
INFO - root - 2017-12-09 14:12:27.865014: step 31280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:45m:19s remains)
INFO - root - 2017-12-09 14:12:36.536636: step 31290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 72h:06m:50s remains)
INFO - root - 2017-12-09 14:12:45.200022: step 31300, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:55m:20s remains)
2017-12-09 14:12:46.023542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033888342 -0.0033882689 -0.00338826 -0.0033871054 -0.0033855138 -0.0033837371 -0.0033817212 -0.0033802718 -0.0033797335 -0.0033799971 -0.0033814216 -0.0033826251 -0.0033827364 -0.0033818756 -0.0033804292][-0.003386291 -0.0033861706 -0.0033864973 -0.0033859098 -0.0033847732 -0.0033830779 -0.0033811734 -0.0033796371 -0.0033791859 -0.0033791894 -0.0033802872 -0.0033813172 -0.0033815224 -0.003381016 -0.003379998][-0.0033848807 -0.00338452 -0.0033848339 -0.0033846225 -0.0033837131 -0.0033820011 -0.0033801764 -0.0033788078 -0.0033782898 -0.0033783575 -0.0033794392 -0.0033805955 -0.0033813717 -0.0033815352 -0.0033812579][-0.0033843084 -0.0033838844 -0.0033839878 -0.0033835331 -0.0033821219 -0.0033801089 -0.0033783258 -0.003377008 -0.0033764383 -0.0033768935 -0.0033781826 -0.0033797584 -0.0033811815 -0.0033820269 -0.0033810877][-0.0033841624 -0.0033833322 -0.0033830986 -0.0033823883 -0.0033808073 -0.0033787598 -0.0033768818 -0.0033756355 -0.0033464318 -0.0033536267 -0.0033172565 -0.0033335609 -0.0033481626 -0.0033575259 -0.0033045984][-0.0033845862 -0.003383541 -0.0033831371 -0.0033821082 -0.0033801626 -0.003377931 -0.0033759526 -0.0033745889 -0.003294338 -0.0032598316 -0.0032143909 -0.0032281808 -0.0031680311 -0.0032085106 -0.0029182816][-0.0033848709 -0.0033837026 -0.0033832716 -0.0033820516 -0.0033800325 -0.0033777228 -0.0033756825 -0.0033741924 -0.0032867067 -0.0031468226 -0.0031113536 -0.0030605658 -0.00295308 -0.0027741259 -0.0019043627][-0.003385084 -0.0033837713 -0.003383314 -0.0033821678 -0.0033805517 -0.0033785789 -0.0033768006 -0.0033752532 -0.0032848439 -0.0031262019 -0.0030754241 -0.0029980105 -0.0025403337 -0.0020106321 -2.9729446e-05][-0.00338542 -0.0033839874 -0.0033834835 -0.0033822304 -0.0033808537 -0.0033793163 -0.0033778872 -0.0033765642 -0.0032858569 -0.0031191523 -0.0030511878 -0.002905715 -0.0019152475 -0.00047190161 0.0034164744][-0.0033855257 -0.00338411 -0.0033837224 -0.0033825617 -0.0033814574 -0.0033801664 -0.0033789219 -0.0033778239 -0.0032903522 -0.0031207607 -0.0029427174 -0.0027375771 -0.0010067993 0.0019408981 0.0079076281][-0.0033850409 -0.0033837806 -0.0033836996 -0.0033829587 -0.0033822604 -0.0033812975 -0.0033804125 -0.0033796856 -0.0033280905 -0.0031620117 -0.0029184239 -0.0023561022 0.00054023881 0.0056370776 0.014532184][-0.0033843536 -0.0033832036 -0.0033833333 -0.0033829454 -0.0033826858 -0.0033821641 -0.0033817571 -0.0033815166 -0.0033819294 -0.0032775139 -0.002946364 -0.0016156193 0.0031692581 0.01195347 0.024894923][-0.0033834944 -0.0033824635 -0.0033826791 -0.0033825426 -0.0033825948 -0.0033824453 -0.0033824665 -0.0033826968 -0.0033834307 -0.00337431 -0.0028568381 -0.00049712672 0.00695719 0.020242617 0.038451307][-0.0033828116 -0.0033817873 -0.0033819706 -0.0033819957 -0.00338213 -0.0033821394 -0.0033823852 -0.0033829813 -0.0033724098 -0.0033515911 -0.0025203691 0.0011319546 0.011342475 0.029532442 0.052856512][-0.0033826476 -0.0033814325 -0.0033812304 -0.0033811536 -0.0033812008 -0.0033812248 -0.0033815699 -0.0033824274 -0.0033705712 -0.0033395309 -0.0022384797 0.0025328309 0.014976554 0.036933981 0.064227745]]...]
INFO - root - 2017-12-09 14:12:54.414393: step 31310, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.731 sec/batch; 61h:10m:20s remains)
INFO - root - 2017-12-09 14:13:03.250713: step 31320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:50m:15s remains)
INFO - root - 2017-12-09 14:13:12.041026: step 31330, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 72h:41m:19s remains)
INFO - root - 2017-12-09 14:13:20.689671: step 31340, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:10m:01s remains)
INFO - root - 2017-12-09 14:13:29.519167: step 31350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:46m:01s remains)
INFO - root - 2017-12-09 14:13:38.261862: step 31360, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:39m:14s remains)
INFO - root - 2017-12-09 14:13:46.791090: step 31370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:53m:09s remains)
INFO - root - 2017-12-09 14:13:55.516922: step 31380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 74h:22m:13s remains)
INFO - root - 2017-12-09 14:14:04.449508: step 31390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 74h:17m:33s remains)
INFO - root - 2017-12-09 14:14:13.144817: step 31400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:41m:06s remains)
2017-12-09 14:14:14.025288: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028639076 0.041278139 0.05308336 0.062427923 0.069514632 0.07401675 0.074592195 0.06905511 0.06188754 0.0554845 0.051524632 0.048546068 0.049567074 0.053641573 0.062516458][0.03544721 0.053496372 0.072123684 0.088902004 0.10219368 0.11122012 0.11444536 0.10885908 0.099453345 0.087643854 0.078423 0.070189022 0.067537732 0.072048411 0.085379891][0.051314712 0.07709901 0.10590454 0.13494976 0.15813446 0.17438963 0.18190341 0.1754458 0.16209757 0.14490494 0.13123505 0.11820751 0.11399067 0.12131903 0.13931562][0.079311363 0.11544745 0.15638593 0.19780408 0.23167188 0.25726634 0.26888248 0.26327237 0.24640369 0.22663932 0.20983149 0.19613284 0.19418155 0.20591526 0.22865947][0.11536104 0.1639867 0.21857138 0.27397051 0.31987688 0.35390303 0.36965588 0.36596465 0.34799257 0.32585612 0.30632648 0.2947298 0.29586011 0.31096569 0.33650738][0.14465766 0.20569704 0.27521184 0.34652814 0.40452111 0.44721457 0.4681451 0.469253 0.45212325 0.43129438 0.41240773 0.40254629 0.40483826 0.42045042 0.44481495][0.1656426 0.23388638 0.31160253 0.39226085 0.45961732 0.5107246 0.538459 0.54719508 0.53794253 0.52587891 0.51165783 0.50617427 0.50894272 0.52212322 0.53980017][0.17583282 0.24490072 0.32276881 0.40364197 0.47272655 0.52783328 0.56067181 0.5784927 0.58129269 0.58285564 0.58032638 0.58385032 0.59082496 0.60125417 0.61179787][0.1728287 0.23836645 0.3102943 0.38431808 0.44834623 0.50114304 0.53525275 0.56018579 0.57497412 0.59093 0.60319382 0.62036425 0.63528347 0.64818013 0.65713447][0.15349455 0.21204929 0.27480662 0.33877259 0.39506811 0.44179556 0.47491977 0.50329047 0.526878 0.55496377 0.58169925 0.6123305 0.63763458 0.65799797 0.67022985][0.12159841 0.16988327 0.22097665 0.27196246 0.31745422 0.35744959 0.38785994 0.41780457 0.44775435 0.4861109 0.52513671 0.56741726 0.60403663 0.63306761 0.65102518][0.088194758 0.12286122 0.1594865 0.19623795 0.23080167 0.26221111 0.2888228 0.31872711 0.35333782 0.39883167 0.44705343 0.49938613 0.54542249 0.58125657 0.60401833][0.05941087 0.0818157 0.10521495 0.12813999 0.15033245 0.17215605 0.19326101 0.221051 0.25613156 0.30382812 0.35677144 0.41444963 0.46621498 0.50720167 0.53364551][0.037224475 0.049970042 0.0631566 0.076137424 0.088678554 0.10177103 0.11660932 0.13902707 0.16998196 0.21444577 0.26484275 0.32159972 0.37322593 0.41619489 0.44404441][0.020283431 0.026695227 0.033462014 0.039667405 0.045709495 0.052766696 0.062400043 0.078765139 0.10331804 0.13973595 0.18179822 0.23063892 0.27647185 0.31553355 0.34108293]]...]
INFO - root - 2017-12-09 14:14:22.434166: step 31410, loss = 0.89, batch loss = 0.69 (11.1 examples/sec; 0.722 sec/batch; 60h:23m:53s remains)
INFO - root - 2017-12-09 14:14:31.299269: step 31420, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 75h:59m:19s remains)
INFO - root - 2017-12-09 14:14:39.975925: step 31430, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 76h:29m:35s remains)
INFO - root - 2017-12-09 14:14:48.574456: step 31440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:17m:52s remains)
INFO - root - 2017-12-09 14:14:57.257625: step 31450, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:32m:17s remains)
INFO - root - 2017-12-09 14:15:06.003943: step 31460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:59m:53s remains)
INFO - root - 2017-12-09 14:15:14.582391: step 31470, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 72h:14m:40s remains)
INFO - root - 2017-12-09 14:15:23.283206: step 31480, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 75h:04m:28s remains)
INFO - root - 2017-12-09 14:15:32.015251: step 31490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:07m:56s remains)
INFO - root - 2017-12-09 14:15:40.732397: step 31500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:30m:26s remains)
2017-12-09 14:15:41.566127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033927641 -0.0033911241 -0.0033907334 -0.0033907657 -0.0033914093 -0.0033921292 -0.0033929374 -0.0033932773 -0.0033930908 -0.0033925548 -0.0033914114 -0.0033897755 -0.0033874284 -0.0033852912 -0.0033830621][-0.0033855955 -0.0033838698 -0.0033836088 -0.0033839834 -0.0033852237 -0.0033865296 -0.0033878703 -0.0033885522 -0.0033883587 -0.0033877124 -0.0033861701 -0.0033839929 -0.0033811014 -0.0033782942 -0.003375693][-0.0033793452 -0.0033773673 -0.0033774453 -0.0033782539 -0.0033799051 -0.0033819815 -0.0033844381 -0.0033861012 -0.0033863282 -0.0033854141 -0.0033831832 -0.0033800562 -0.0033762651 -0.0033725207 -0.00336935][-0.0033714406 -0.003369082 -0.0033694187 -0.0033707207 -0.0033730895 -0.003376656 -0.0033810046 -0.0033841 -0.0033853517 -0.0033847811 -0.0033820851 -0.0033778262 -0.0033729135 -0.0033682347 -0.0033643474][-0.0033653905 -0.0033629979 -0.0033634836 -0.0033652559 -0.0033686687 -0.003373574 -0.0033793391 -0.0033839105 -0.0033866358 -0.0033865622 -0.0033835249 -0.0033785098 -0.0033725139 -0.0033668177 -0.003362118][-0.0033616475 -0.0033591071 -0.0033596244 -0.0033619651 -0.0033664657 -0.0033726308 -0.0033794539 -0.0033855191 -0.0033898577 -0.0033902847 -0.0033868738 -0.003381036 -0.0033739328 -0.0033671665 -0.0033614619][-0.0033599271 -0.0033572568 -0.0033578891 -0.0033607737 -0.0033661642 -0.003373259 -0.0033810022 -0.0033881057 -0.0033933111 -0.0033940875 -0.0033904237 -0.0033838719 -0.0033758264 -0.0033680825 -0.0033615367][-0.0033586205 -0.0033559829 -0.0033568149 -0.0033601706 -0.0033661476 -0.0033739437 -0.0033821822 -0.0033897671 -0.0033953756 -0.0033965029 -0.0033929262 -0.0033862186 -0.003377724 -0.0033693246 -0.0033622226][-0.0033579005 -0.0033555471 -0.0033564367 -0.00335974 -0.0033656878 -0.0033734364 -0.0033817161 -0.0033893769 -0.0033951625 -0.0033966328 -0.0033933721 -0.003386982 -0.003378636 -0.0033701463 -0.0033629728][-0.0033573527 -0.0033547825 -0.0033556873 -0.003358715 -0.0033643516 -0.0033715579 -0.003379171 -0.0033863485 -0.0033918913 -0.0033932847 -0.0033903553 -0.0033845543 -0.0033769906 -0.0033694559 -0.0033629888][-0.0033569643 -0.0033542046 -0.0033550067 -0.0033575827 -0.0033622973 -0.0033684152 -0.0033750697 -0.0033812178 -0.003385918 -0.0033872605 -0.0033850644 -0.0033803843 -0.0033743144 -0.0033681106 -0.0033625606][-0.0033567979 -0.0033538474 -0.0033543888 -0.0033563408 -0.0033596959 -0.0033643735 -0.0033697891 -0.0033746571 -0.0033783517 -0.0033797296 -0.0033784767 -0.0033750944 -0.0033706264 -0.0033657348 -0.0033614608][-0.0033568377 -0.0033536071 -0.003354043 -0.0033553522 -0.0033574069 -0.0033607055 -0.0033647355 -0.0033683984 -0.0033712464 -0.0033725777 -0.0033720243 -0.0033699116 -0.0033667148 -0.0033632154 -0.0033601476][-0.0033572493 -0.0033538188 -0.0033540071 -0.0033547801 -0.0033560738 -0.0033581993 -0.0033608873 -0.0033633814 -0.0033654084 -0.003366498 -0.0033663544 -0.003365204 -0.0033632386 -0.0033610803 -0.0033590489][-0.0033577648 -0.0033541068 -0.0033540986 -0.00335452 -0.003355257 -0.0033564135 -0.0033578912 -0.003359393 -0.0033606782 -0.0033615828 -0.0033618535 -0.0033614126 -0.0033604705 -0.0033593057 -0.0033581092]]...]
INFO - root - 2017-12-09 14:15:49.972033: step 31510, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.750 sec/batch; 62h:40m:36s remains)
INFO - root - 2017-12-09 14:15:58.632784: step 31520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:51m:22s remains)
INFO - root - 2017-12-09 14:16:07.265994: step 31530, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 73h:46m:46s remains)
INFO - root - 2017-12-09 14:16:15.988791: step 31540, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 72h:50m:23s remains)
INFO - root - 2017-12-09 14:16:24.593790: step 31550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:10m:58s remains)
INFO - root - 2017-12-09 14:16:33.246360: step 31560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:48m:49s remains)
INFO - root - 2017-12-09 14:16:41.685890: step 31570, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:36m:48s remains)
INFO - root - 2017-12-09 14:16:50.383440: step 31580, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 74h:03m:59s remains)
INFO - root - 2017-12-09 14:16:59.082553: step 31590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 74h:13m:29s remains)
INFO - root - 2017-12-09 14:17:07.670537: step 31600, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 73h:38m:56s remains)
2017-12-09 14:17:08.599890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034044897 -0.0034057382 -0.0034076395 -0.0034091964 -0.0034104309 -0.0034112551 -0.0034107012 -0.00340953 -0.0034081412 -0.0034065712 -0.003405008 -0.0034039845 -0.0034031891 -0.0034023752 -0.0034011805][-0.0034055798 -0.0034051563 -0.0034053377 -0.0034051833 -0.0034047032 -0.003404066 -0.0034031232 -0.003401862 -0.0034005137 -0.0033996571 -0.0033994631 -0.0034001844 -0.0034010164 -0.0034015465 -0.0034012876][-0.0034068727 -0.0034046597 -0.0034026061 -0.0034000257 -0.0033975551 -0.0033954636 -0.0033937597 -0.0033921227 -0.0033910843 -0.0033913201 -0.0033928934 -0.0033957 -0.0033988981 -0.0034012725 -0.0034023081][-0.0034057775 -0.0034019041 -0.0033978946 -0.0033937695 -0.0033900295 -0.0033874363 -0.0033858579 -0.0033846386 -0.0033844726 -0.003386121 -0.0033895231 -0.0033937788 -0.003398332 -0.0034017239 -0.0034033435][-0.0034019006 -0.0033962429 -0.0033909022 -0.0033858449 -0.0033819913 -0.0033796025 -0.003378717 -0.0033786972 -0.0033799976 -0.0033832542 -0.0033883515 -0.0033941274 -0.0033990322 -0.0034022236 -0.0034034445][-0.0033970452 -0.0033903718 -0.0033845347 -0.0033794953 -0.0033758965 -0.0033738925 -0.0033737395 -0.0033748234 -0.0033776038 -0.0033824169 -0.0033886319 -0.0033945565 -0.003398712 -0.0034010452 -0.0034014951][-0.0033935278 -0.0033870346 -0.0033816667 -0.0033779221 -0.0033756075 -0.0033746869 -0.0033754662 -0.0033775365 -0.0033811526 -0.0033865864 -0.003392295 -0.0033968072 -0.0033994066 -0.0034004897 -0.003400242][-0.0033920391 -0.003386368 -0.0033825669 -0.0033805673 -0.0033800867 -0.0033804167 -0.003382243 -0.003384687 -0.0033879543 -0.0033920761 -0.0033956952 -0.0033981444 -0.0033991139 -0.0033992911 -0.0033989812][-0.0033923518 -0.0033880845 -0.0033858218 -0.003385535 -0.0033864654 -0.0033879075 -0.0033903869 -0.0033927988 -0.0033950282 -0.0033967865 -0.0033979351 -0.003398583 -0.0033984045 -0.0033978943 -0.0033973684][-0.0033953211 -0.0033927232 -0.0033917476 -0.0033921422 -0.0033931285 -0.0033944652 -0.0033961802 -0.0033976585 -0.0033984897 -0.0033985828 -0.0033984075 -0.0033982191 -0.0033975502 -0.0033967926 -0.003396065][-0.0034011279 -0.0033993376 -0.00339853 -0.0033983486 -0.0033983283 -0.00339843 -0.003398824 -0.003398858 -0.0033986142 -0.0033979716 -0.0033975029 -0.0033971539 -0.0033964512 -0.003395767 -0.0033951867][-0.0034052813 -0.0034038133 -0.00340266 -0.0034016885 -0.0034007896 -0.0034000638 -0.0033993095 -0.0033983325 -0.0033971467 -0.0033959784 -0.0033950782 -0.0033945146 -0.0033940922 -0.0033940044 -0.0033940582][-0.0034056366 -0.0034037514 -0.0034024 -0.0034010806 -0.0033997542 -0.0033984722 -0.0033973921 -0.0033961169 -0.0033947346 -0.0033937227 -0.0033933525 -0.0033930456 -0.00339267 -0.0033927367 -0.003393017][-0.0034031586 -0.0034006636 -0.0033992261 -0.0033980035 -0.0033968186 -0.0033957257 -0.0033948384 -0.0033940643 -0.0033933592 -0.00339292 -0.0033929488 -0.0033929951 -0.0033930857 -0.0033932775 -0.0033933595][-0.003399505 -0.0033970394 -0.0033958117 -0.0033949795 -0.0033941919 -0.0033937606 -0.0033934149 -0.0033930594 -0.003392884 -0.0033929711 -0.0033933688 -0.0033938005 -0.003394092 -0.0033943676 -0.003394336]]...]
INFO - root - 2017-12-09 14:17:17.012888: step 31610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:04m:51s remains)
INFO - root - 2017-12-09 14:17:25.520010: step 31620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:16m:04s remains)
INFO - root - 2017-12-09 14:17:34.191108: step 31630, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 68h:58m:19s remains)
INFO - root - 2017-12-09 14:17:42.612038: step 31640, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.823 sec/batch; 68h:47m:04s remains)
INFO - root - 2017-12-09 14:17:51.044846: step 31650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:07m:31s remains)
INFO - root - 2017-12-09 14:17:59.638474: step 31660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:30m:25s remains)
INFO - root - 2017-12-09 14:18:08.169720: step 31670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:54m:56s remains)
INFO - root - 2017-12-09 14:18:16.738445: step 31680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:09m:21s remains)
INFO - root - 2017-12-09 14:18:25.448730: step 31690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:18m:29s remains)
INFO - root - 2017-12-09 14:18:34.148445: step 31700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:19m:38s remains)
2017-12-09 14:18:34.973660: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.7042098 0.71156263 0.71776241 0.72070706 0.72194785 0.72093791 0.71766162 0.70776671 0.68959403 0.66100168 0.62090153 0.56783682 0.49929249 0.4157382 0.32149214][0.72031289 0.73420304 0.7465288 0.75488734 0.7600373 0.762161 0.76022917 0.75096166 0.73288804 0.70552409 0.6667676 0.61379862 0.54436129 0.45832658 0.35942218][0.719669 0.73912841 0.75554162 0.76768118 0.77566195 0.78040129 0.78072506 0.77341235 0.75762063 0.73208463 0.69527185 0.64449966 0.57656872 0.49087691 0.39057097][0.72008395 0.744463 0.76399148 0.77845585 0.7868433 0.79109824 0.79143208 0.78455275 0.77035141 0.74798304 0.71500134 0.6679917 0.60310829 0.51990449 0.42056444][0.71995181 0.74671096 0.76737112 0.7838313 0.79296464 0.7966066 0.79549223 0.78649759 0.77101505 0.74931169 0.71941662 0.677796 0.61923629 0.54257911 0.44861022][0.71895766 0.74630564 0.7661528 0.78200734 0.79059005 0.79329884 0.79020083 0.77904415 0.76168984 0.73951906 0.71126366 0.67454886 0.62419242 0.55738664 0.4720276][0.71035171 0.73655725 0.754366 0.76999414 0.77895671 0.78143692 0.77732593 0.76500648 0.7465086 0.72409618 0.69742393 0.66545987 0.62274331 0.56542712 0.48956543][0.69885349 0.72246158 0.7357884 0.7484647 0.75504565 0.75538689 0.74951351 0.73669952 0.71865088 0.69771838 0.67416894 0.64773172 0.61292714 0.56534624 0.49921691][0.67339331 0.69563478 0.70703405 0.71690017 0.72107226 0.71908951 0.710689 0.69680589 0.67857516 0.65941364 0.63939762 0.61877066 0.59218425 0.55448568 0.49883431][0.63377 0.65298617 0.66090995 0.66825992 0.67184061 0.66935778 0.6599806 0.64621633 0.62876946 0.6110546 0.59383214 0.57805187 0.55897254 0.5308851 0.48584482][0.58713406 0.60355496 0.60801286 0.61183459 0.61311424 0.60972965 0.60035694 0.58816433 0.57305235 0.55751467 0.54269141 0.52996594 0.51622444 0.49527749 0.45919707][0.53080815 0.54583341 0.54849571 0.549975 0.549536 0.54520947 0.53593427 0.5251078 0.5123347 0.49950582 0.48730409 0.477268 0.46708468 0.45104462 0.42237079][0.46353063 0.47725713 0.47953981 0.47921002 0.47754097 0.47235185 0.46327522 0.45342377 0.44239661 0.43186054 0.42182171 0.41394722 0.40672389 0.39528671 0.37359893][0.3805401 0.39295149 0.39613459 0.39619598 0.39548919 0.39113313 0.38343588 0.37465852 0.36515814 0.356447 0.34814167 0.34195709 0.33692172 0.32914534 0.31360587][0.29342696 0.30296269 0.305319 0.30587742 0.30603045 0.30290747 0.29728442 0.29037958 0.2828927 0.27582109 0.26905754 0.26412767 0.26050159 0.25537804 0.24486642]]...]
INFO - root - 2017-12-09 14:18:43.402174: step 31710, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 70h:52m:39s remains)
INFO - root - 2017-12-09 14:18:51.713456: step 31720, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.826 sec/batch; 69h:00m:23s remains)
INFO - root - 2017-12-09 14:19:00.035742: step 31730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:45m:11s remains)
INFO - root - 2017-12-09 14:19:08.933720: step 31740, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.878 sec/batch; 73h:19m:20s remains)
INFO - root - 2017-12-09 14:19:17.411963: step 31750, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.820 sec/batch; 68h:28m:22s remains)
INFO - root - 2017-12-09 14:19:25.789070: step 31760, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 68h:18m:16s remains)
INFO - root - 2017-12-09 14:19:34.239611: step 31770, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 69h:24m:20s remains)
INFO - root - 2017-12-09 14:19:42.870639: step 31780, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 71h:02m:59s remains)
INFO - root - 2017-12-09 14:19:51.494618: step 31790, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 71h:56m:02s remains)
INFO - root - 2017-12-09 14:20:00.075849: step 31800, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 69h:26m:34s remains)
2017-12-09 14:20:01.003468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017107754 -0.0002355394 0.0014767197 0.0029645402 0.003777622 0.0037540218 0.0030395009 0.0019771515 0.00088272546 -7.9411082e-05 -0.00088081323 -0.0015563152 -0.0021277207 -0.0025811931 -0.0029130476][-0.00052595395 0.0016295097 0.00410516 0.0062655462 0.0074984962 0.0075708209 0.00665815 0.0051993979 0.0036107681 0.0021525535 0.0009085068 -0.00015816512 -0.001080093 -0.0018436395 -0.0024401068][0.0015806195 0.0046229828 0.008093413 0.011198211 0.013175715 0.013664128 0.012793005 0.011012937 0.0088152355 0.0065788189 0.0045005111 0.0026154409 0.00093247951 -0.00049306289 -0.001622373][0.0048740683 0.0090437019 0.013777722 0.018144967 0.021240806 0.022513103 0.02194031 0.019896204 0.016922543 0.013539627 0.010121846 0.0068662995 0.0039014623 0.0013908918 -0.00056000194][0.0091739092 0.014737055 0.02100992 0.026946161 0.031493556 0.033872232 0.033808179 0.031494416 0.027509425 0.022539716 0.017218309 0.012004012 0.0072605219 0.0033320643 0.00039607473][0.013570478 0.020668693 0.028565641 0.036122933 0.042145103 0.045632679 0.046041727 0.043393373 0.038306925 0.031623293 0.024234837 0.01687663 0.01022141 0.0048297988 0.00095589855][0.016761124 0.025299646 0.034667559 0.04361178 0.050789721 0.055048138 0.055669863 0.052596509 0.046504974 0.0383491 0.029216036 0.020066397 0.011878752 0.0053992532 0.00093032955][0.017729849 0.027200613 0.0375441 0.04738139 0.055263769 0.059965421 0.060692381 0.057362355 0.050659832 0.041589003 0.031353626 0.021099921 0.012050794 0.0050678551 0.00045277248][0.016392648 0.025874998 0.036309727 0.0462595 0.05426652 0.059136946 0.060072068 0.056938037 0.050301597 0.0411134 0.030601941 0.020076942 0.010937454 0.0040765731 -0.00025645131][0.013539709 0.022069445 0.031607583 0.040802944 0.048312131 0.053038593 0.054238308 0.051693153 0.045747411 0.037205279 0.027247222 0.017300587 0.0088209817 0.0026552414 -0.0010454585][0.010172428 0.017149605 0.025081811 0.032826275 0.03924232 0.043386154 0.044617981 0.042670205 0.037690639 0.030331556 0.021672098 0.013111806 0.0059842104 0.0009914448 -0.001830691][0.0068101557 0.012092874 0.01815941 0.024119124 0.029059172 0.032217439 0.033110812 0.031519808 0.027513789 0.021638928 0.014820298 0.00823947 0.0029277557 -0.00062830746 -0.0025008838][0.0035423937 0.0072711883 0.011568152 0.015770171 0.019176483 0.021215346 0.021570859 0.020151984 0.017069211 0.012788415 0.0080253221 0.0036115453 0.00020227651 -0.001951381 -0.002986653][0.00059596472 0.0029631874 0.0057118805 0.0083850054 0.010475301 0.011586174 0.011533514 0.010340931 0.0081830816 0.0054353895 0.0025781167 8.300133e-05 -0.0017332148 -0.0027986509 -0.0032566998][-0.0016195113 -0.000353697 0.0011566167 0.0026326405 0.0037468716 0.0042527737 0.0040594516 0.0032369492 0.0019615272 0.00048810593 -0.00091738044 -0.0020490047 -0.0028089972 -0.003215886 -0.0033689961]]...]
INFO - root - 2017-12-09 14:20:09.376769: step 31810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:51m:24s remains)
INFO - root - 2017-12-09 14:20:17.921200: step 31820, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:27m:13s remains)
INFO - root - 2017-12-09 14:20:26.610006: step 31830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:11m:46s remains)
INFO - root - 2017-12-09 14:20:35.284487: step 31840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:34m:35s remains)
INFO - root - 2017-12-09 14:20:44.070596: step 31850, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 74h:26m:25s remains)
INFO - root - 2017-12-09 14:20:52.768859: step 31860, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 73h:35m:59s remains)
INFO - root - 2017-12-09 14:21:01.546552: step 31870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:48m:07s remains)
INFO - root - 2017-12-09 14:21:10.269324: step 31880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:22m:22s remains)
INFO - root - 2017-12-09 14:21:18.913715: step 31890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:20m:48s remains)
INFO - root - 2017-12-09 14:21:27.598752: step 31900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:51m:45s remains)
2017-12-09 14:21:28.410293: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12844031 0.13281438 0.13709731 0.1397585 0.14087984 0.14024901 0.1388018 0.13660012 0.13289393 0.1263821 0.11544297 0.099239625 0.078545563 0.056385648 0.035620738][0.13322717 0.1370175 0.1412984 0.1445722 0.14628975 0.14538383 0.14309187 0.13975118 0.13477077 0.1270697 0.11473677 0.097186185 0.075601943 0.053116184 0.032689556][0.13561307 0.13899025 0.14312224 0.14643133 0.14815114 0.14743917 0.14482373 0.14094943 0.13479339 0.1254911 0.11133609 0.092347071 0.069783621 0.047219545 0.027625825][0.13598205 0.14029323 0.14535305 0.14968035 0.15229 0.15200204 0.14904182 0.14378591 0.13569787 0.12437776 0.10845473 0.088240005 0.065343082 0.043008395 0.024393946][0.13307963 0.13822 0.14482652 0.15115544 0.15603203 0.15753424 0.15550984 0.1494659 0.13943428 0.12527703 0.10638782 0.08416602 0.060761806 0.039269343 0.022043295][0.12951703 0.13566725 0.14346819 0.15174669 0.15880641 0.16205801 0.16134186 0.15557759 0.14448266 0.12828453 0.10703038 0.083128743 0.058820702 0.037753608 0.021694593][0.12781939 0.13425127 0.14268535 0.15248968 0.16123548 0.16608134 0.16614147 0.16046347 0.14873768 0.13125381 0.10916446 0.084956206 0.06090131 0.040210459 0.024677176][0.12928048 0.13623925 0.14439926 0.15417455 0.16327736 0.16902581 0.16966958 0.16406117 0.15197365 0.13398258 0.11171741 0.088166736 0.065232895 0.045711894 0.031077892][0.13243097 0.13986999 0.14765395 0.15709789 0.16586156 0.17152727 0.17237066 0.16711412 0.15564053 0.13852428 0.11727841 0.094594128 0.072855912 0.054328054 0.040497459][0.13678962 0.14379935 0.15043244 0.15853557 0.16640621 0.17192191 0.17322065 0.16880864 0.15849797 0.14274031 0.12321272 0.10249744 0.082678296 0.065887526 0.053097196][0.14093496 0.14760649 0.1526991 0.1590509 0.16537911 0.16976129 0.17079517 0.16723834 0.15861803 0.14539205 0.1287962 0.11119054 0.0938813 0.07904084 0.067423172][0.14165248 0.14789121 0.15180056 0.15674645 0.16169873 0.16522424 0.16597505 0.1630924 0.15606916 0.14497806 0.13132519 0.11688697 0.10259672 0.090033479 0.079834826][0.13741927 0.14331008 0.14631517 0.14988878 0.15364207 0.1564298 0.1572323 0.15527566 0.15018676 0.1417931 0.13090371 0.1193247 0.10798368 0.097958066 0.089405388][0.12936205 0.13514444 0.13788567 0.14101633 0.14433816 0.14687206 0.14783405 0.14671594 0.14317815 0.13706893 0.12897111 0.12011702 0.11135694 0.10363157 0.096735172][0.11889612 0.12459846 0.12734629 0.13028122 0.13347684 0.13597192 0.13730678 0.13687816 0.13466133 0.13031717 0.12462799 0.11842954 0.11209092 0.10639815 0.10090833]]...]
INFO - root - 2017-12-09 14:21:36.848498: step 31910, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:48m:13s remains)
INFO - root - 2017-12-09 14:21:45.445098: step 31920, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:45m:59s remains)
INFO - root - 2017-12-09 14:21:54.250161: step 31930, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 73h:06m:15s remains)
INFO - root - 2017-12-09 14:22:02.904940: step 31940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:20m:21s remains)
INFO - root - 2017-12-09 14:22:11.616545: step 31950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:50m:01s remains)
INFO - root - 2017-12-09 14:22:20.228085: step 31960, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:17m:52s remains)
INFO - root - 2017-12-09 14:22:28.735836: step 31970, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 67h:03m:51s remains)
INFO - root - 2017-12-09 14:22:37.603981: step 31980, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 75h:17m:46s remains)
INFO - root - 2017-12-09 14:22:46.322214: step 31990, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:38m:03s remains)
INFO - root - 2017-12-09 14:22:54.993375: step 32000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:50m:41s remains)
2017-12-09 14:22:55.837749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033814628 -0.0031863397 -0.0022019404 -0.00023462484 0.0022290291 0.0041897334 0.0049221907 0.004424613 0.0029289431 0.000998609 -0.00081289397 -0.0021119248 -0.0028797674 -0.0032409106 -0.0033630023][-0.0033622482 -0.0029789885 -0.0013609286 0.0017849526 0.0057838876 0.00901685 0.010208973 0.0091854706 0.0065218089 0.0032891498 0.00036572758 -0.0016452165 -0.0027464251 -0.0032142766 -0.0033574062][-0.0033191855 -0.0025740445 7.6475553e-06 0.0048565064 0.011163894 0.016675025 0.019407045 0.018587314 0.014698599 0.0092482585 0.0039689681 0.0001075156 -0.0020989352 -0.0030511953 -0.0033365474][-0.0032338672 -0.0019377795 0.0021816629 0.0095919883 0.018975224 0.027146259 0.031576898 0.0310515 0.025950072 0.018017868 0.0097451741 0.0032632758 -0.00072406675 -0.0026088343 -0.0032470911][-0.0030428483 -0.00093030022 0.0053454814 0.016438454 0.030564407 0.043128405 0.0504989 0.050505262 0.043306805 0.031423736 0.018616255 0.0081671076 0.0014665902 -0.0018697992 -0.0030757119][-0.0026280428 0.00042175478 0.0091067068 0.024214413 0.043177683 0.060365915 0.071434505 0.072982281 0.06449604 0.048800692 0.030856276 0.015410601 0.0049786298 -0.00055424357 -0.0027302434][-0.0018691354 0.0019716185 0.012642824 0.030976897 0.053802386 0.074806407 0.088671006 0.091427147 0.082286775 0.064004295 0.042137992 0.022497853 0.0087226117 0.001024219 -0.0022467487][-0.00072044227 0.003589913 0.015338774 0.03541394 0.06032186 0.083465181 0.098630749 0.10161419 0.091791652 0.072165184 0.048405796 0.026637349 0.011118017 0.0021767134 -0.0018286539][0.00069366605 0.0049756505 0.016702516 0.036828194 0.061845042 0.085107461 0.099965 0.10235959 0.09189906 0.071925245 0.048114505 0.026507387 0.011175835 0.0022969397 -0.0017498021][0.002009012 0.00575832 0.016281152 0.034769494 0.05793428 0.079320215 0.092459559 0.093507029 0.082631558 0.063417234 0.041450225 0.022241794 0.0089413188 0.0013761285 -0.0020316206][0.0019808805 0.0050213588 0.013595031 0.02920627 0.049097206 0.067317694 0.077922814 0.077478893 0.066560373 0.049142662 0.030558357 0.015307575 0.0052509196 -0.00019505736 -0.002521462][0.00083999033 0.0030812144 0.0094823018 0.021743711 0.037770387 0.052292794 0.06010855 0.058265995 0.047861747 0.033108521 0.018797059 0.0080326423 0.0014804902 -0.0017510629 -0.0029861734][-0.00071103568 0.00093055586 0.0055783819 0.014791316 0.02714752 0.038068473 0.043196015 0.040289972 0.031029126 0.019447969 0.0093526654 0.0025060002 -0.0012081352 -0.0027794605 -0.003265762][-0.0019188161 -0.00072612823 0.002680782 0.0094211642 0.018441204 0.025951438 0.028678821 0.025329461 0.017905358 0.00972737 0.0032747856 -0.00067637279 -0.0025554509 -0.0032110245 -0.0033605767][-0.0027091699 -0.0019644685 0.00048020366 0.0052818209 0.011509305 0.01616987 0.017127993 0.014001126 0.0087014437 0.0035118985 -0.00022188062 -0.0022741272 -0.0031168056 -0.0033513156 -0.0033869855]]...]
INFO - root - 2017-12-09 14:23:04.403828: step 32010, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 73h:23m:14s remains)
INFO - root - 2017-12-09 14:23:12.943582: step 32020, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 74h:02m:18s remains)
INFO - root - 2017-12-09 14:23:21.690469: step 32030, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 74h:08m:55s remains)
INFO - root - 2017-12-09 14:23:30.295487: step 32040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:59m:21s remains)
INFO - root - 2017-12-09 14:23:39.033536: step 32050, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 71h:54m:58s remains)
INFO - root - 2017-12-09 14:23:47.630553: step 32060, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 69h:30m:34s remains)
INFO - root - 2017-12-09 14:23:56.218408: step 32070, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:31m:48s remains)
INFO - root - 2017-12-09 14:24:04.864484: step 32080, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:47m:08s remains)
INFO - root - 2017-12-09 14:24:13.510349: step 32090, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 72h:14m:32s remains)
INFO - root - 2017-12-09 14:24:22.261411: step 32100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:23m:02s remains)
2017-12-09 14:24:23.135922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002791611 -0.0016986269 0.00036814087 0.0035392905 0.0074505387 0.011046013 0.013111547 0.012646981 0.0095732417 0.0048577366 0.00040415977 -0.0023239418 -0.0032755812 -0.0033760427 -0.003376002][-0.0032136494 -0.0028671725 -0.0021529698 -0.00098509365 0.00053884904 0.0020672658 0.0031241893 0.0031753343 0.0020287514 6.7987014e-05 -0.00182675 -0.002970947 -0.0033423144 -0.0033749777 -0.0033748453][-0.0033739845 -0.0033335551 -0.0032264143 -0.0030242126 -0.002728798 -0.0023830491 -0.0020935773 -0.0020141881 -0.0022459603 -0.0026779156 -0.0030936131 -0.0033231694 -0.0033763044 -0.0033759694 -0.0033761868][-0.0033845154 -0.0033834267 -0.0033799428 -0.003367766 -0.0033477161 -0.0033234521 -0.0032996677 -0.0032886376 -0.0033022803 -0.0033333686 -0.0033634549 -0.0033754949 -0.0033778606 -0.0033781058 -0.0033787726][-0.0033852004 -0.003385128 -0.0033851822 -0.0033814923 -0.0033737647 -0.0033642496 -0.0033548411 -0.0033512572 -0.0033541222 -0.0033624212 -0.0033713209 -0.0033766013 -0.0033794488 -0.003380653 -0.003381717][-0.003385803 -0.0033857131 -0.0033858076 -0.0033827208 -0.00337586 -0.0033673339 -0.0033584782 -0.003354508 -0.0033568365 -0.0033631411 -0.0033719498 -0.0033776222 -0.0033811633 -0.0033832754 -0.0033842784][-0.0033864975 -0.0033861098 -0.003386528 -0.0033840847 -0.0033782762 -0.0033713772 -0.0033643725 -0.0033604705 -0.0033611704 -0.0033661192 -0.0033737284 -0.0033788218 -0.0033824618 -0.00338504 -0.0033861306][-0.0033869916 -0.0033864437 -0.0033870472 -0.0033851096 -0.0033802502 -0.0033748732 -0.0033698606 -0.0033673462 -0.0033677842 -0.0033710147 -0.0033766537 -0.0033808846 -0.0033840588 -0.0033866037 -0.0033879501][-0.0033879636 -0.0033876391 -0.0033885585 -0.0033870807 -0.003383158 -0.0033789896 -0.00337548 -0.0033739067 -0.0033740704 -0.0033761612 -0.0033795992 -0.003382554 -0.0033850379 -0.0033875185 -0.0033890337][-0.0033878242 -0.003387739 -0.0033886821 -0.0033876544 -0.0033845268 -0.0033815359 -0.0033791936 -0.003377707 -0.0033773512 -0.003378157 -0.0033800208 -0.0033821622 -0.0033844241 -0.0033866502 -0.0033882596][-0.0033875166 -0.0033873422 -0.0033885629 -0.0033880824 -0.0033858998 -0.0033838372 -0.0033818569 -0.0033803105 -0.0033792038 -0.0033789275 -0.0033798446 -0.0033814795 -0.0033832635 -0.0033850938 -0.0033864458][-0.0033863606 -0.003386233 -0.0033877273 -0.0033876882 -0.0033861797 -0.003384629 -0.003382944 -0.0033814139 -0.0033803557 -0.0033799543 -0.0033801089 -0.0033809249 -0.0033817848 -0.0033830663 -0.0033840346][-0.0033840162 -0.003384114 -0.0033856842 -0.0033860039 -0.0033850861 -0.0033841119 -0.0033829429 -0.0033816034 -0.0033803361 -0.0033794704 -0.0033791291 -0.0033793375 -0.0033799352 -0.0033808304 -0.0033817855][-0.003381649 -0.0033812684 -0.0033824337 -0.0033827212 -0.003382402 -0.0033819464 -0.0033814281 -0.0033806718 -0.0033796844 -0.0033787724 -0.0033784327 -0.0033784569 -0.0033787845 -0.0033792048 -0.0033797135][-0.0033797366 -0.0033787519 -0.0033796935 -0.0033799491 -0.0033799896 -0.0033798481 -0.0033796737 -0.0033794434 -0.0033789896 -0.003378398 -0.003377937 -0.0033776762 -0.0033776492 -0.0033777573 -0.0033779952]]...]
INFO - root - 2017-12-09 14:24:31.534689: step 32110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:29m:50s remains)
INFO - root - 2017-12-09 14:24:40.025745: step 32120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:32m:15s remains)
INFO - root - 2017-12-09 14:24:48.538590: step 32130, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 68h:33m:56s remains)
INFO - root - 2017-12-09 14:24:56.947834: step 32140, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:39m:33s remains)
INFO - root - 2017-12-09 14:25:05.595835: step 32150, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 74h:02m:16s remains)
INFO - root - 2017-12-09 14:25:14.218442: step 32160, loss = 0.90, batch loss = 0.69 (8.4 examples/sec; 0.947 sec/batch; 79h:00m:37s remains)
INFO - root - 2017-12-09 14:25:22.863883: step 32170, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 74h:43m:27s remains)
INFO - root - 2017-12-09 14:25:31.541352: step 32180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:34m:51s remains)
INFO - root - 2017-12-09 14:25:40.221948: step 32190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:12m:36s remains)
INFO - root - 2017-12-09 14:25:48.865418: step 32200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:15m:44s remains)
2017-12-09 14:25:49.688236: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38728684 0.33183867 0.28282791 0.24534415 0.21844524 0.20559165 0.20898829 0.22665597 0.2545875 0.2870037 0.31704652 0.33833992 0.3425175 0.33000308 0.30210108][0.45016482 0.39803544 0.34929654 0.31150588 0.28236043 0.26747844 0.26793966 0.28047162 0.30186343 0.32736179 0.35056275 0.36532232 0.36385837 0.34675157 0.31543878][0.51001543 0.46453476 0.42015675 0.38569993 0.35579816 0.33888239 0.335007 0.34105086 0.3536478 0.3689045 0.38266909 0.38942784 0.38195193 0.36039796 0.32589424][0.56553644 0.52837408 0.48971528 0.458678 0.42979869 0.41221178 0.40475613 0.40439036 0.40822905 0.41328216 0.41736463 0.41593754 0.40276691 0.37692538 0.33878329][0.6118716 0.58433479 0.55216366 0.52587306 0.49903497 0.48191991 0.47137833 0.465428 0.46151555 0.45756584 0.453069 0.44436273 0.42587349 0.39569494 0.35384008][0.64235342 0.62914473 0.60748672 0.58877361 0.56665212 0.55041623 0.53713971 0.52658713 0.51607269 0.5042668 0.49164909 0.47578287 0.45151898 0.41625038 0.36957321][0.65420657 0.65775025 0.64983636 0.64109206 0.62551266 0.61226004 0.59778714 0.58364618 0.56755072 0.54972553 0.53048211 0.5080232 0.47760695 0.43686914 0.385145][0.65157729 0.6711123 0.67736542 0.68008381 0.67435557 0.66540647 0.65074432 0.63536435 0.61583287 0.59357309 0.56841958 0.53992158 0.50370085 0.45668691 0.3990773][0.64222187 0.67615396 0.69546056 0.70816141 0.71277225 0.70895189 0.69622678 0.68071514 0.65939236 0.6348722 0.60559428 0.57241243 0.53103447 0.47791579 0.4144921][0.62968659 0.67405975 0.70373154 0.72575384 0.73965454 0.74116743 0.73116851 0.71591747 0.69216025 0.66464227 0.63120246 0.5948211 0.54936612 0.49200231 0.42476428][0.61342716 0.66338575 0.698696 0.72777474 0.74939328 0.75629848 0.75059861 0.73674327 0.71222574 0.68233216 0.64551693 0.60614532 0.55774945 0.49847719 0.4296819][0.59085983 0.64216876 0.67990476 0.71280164 0.73944259 0.75185114 0.7510187 0.73979318 0.716118 0.68590206 0.64801109 0.60759342 0.55859751 0.49947116 0.43109229][0.55746979 0.60731453 0.64567131 0.68139344 0.71207148 0.72920871 0.73301923 0.72562557 0.70450824 0.67592967 0.63910419 0.59977752 0.55233967 0.49529982 0.42894849][0.52125561 0.56753844 0.60451317 0.64124143 0.67396235 0.6938625 0.70170909 0.69750863 0.6795519 0.65360582 0.61976928 0.58375335 0.5398069 0.48649731 0.42363477][0.48384863 0.52583164 0.55968052 0.59573215 0.62800151 0.65016329 0.6613186 0.65969676 0.64471167 0.62112987 0.59050286 0.55731964 0.51694447 0.46840265 0.4102768]]...]
INFO - root - 2017-12-09 14:25:58.221462: step 32210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:51m:14s remains)
INFO - root - 2017-12-09 14:26:06.822912: step 32220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:35m:51s remains)
INFO - root - 2017-12-09 14:26:15.569095: step 32230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:31m:18s remains)
INFO - root - 2017-12-09 14:26:24.365446: step 32240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:43m:20s remains)
INFO - root - 2017-12-09 14:26:33.216374: step 32250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:27m:21s remains)
INFO - root - 2017-12-09 14:26:41.879470: step 32260, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 74h:11m:03s remains)
INFO - root - 2017-12-09 14:26:50.541316: step 32270, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 73h:44m:09s remains)
INFO - root - 2017-12-09 14:26:59.327261: step 32280, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:58m:19s remains)
INFO - root - 2017-12-09 14:27:08.142071: step 32290, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 72h:21m:22s remains)
INFO - root - 2017-12-09 14:27:16.835639: step 32300, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 73h:26m:11s remains)
2017-12-09 14:27:17.643772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003376998 -0.0033702452 -0.0033670915 -0.0033655753 -0.0033651839 -0.0033656578 -0.0033661521 -0.0033664738 -0.0033660522 -0.0033656796 -0.0033647579 -0.0033640359 -0.003363776 -0.0033639946 -0.003365926][-0.0033785636 -0.0033713249 -0.0033672687 -0.0033653013 -0.0033647702 -0.0033654715 -0.003366363 -0.0033666713 -0.0033660543 -0.0033651919 -0.0033637 -0.0033624009 -0.0033616042 -0.0033615513 -0.0033634575][-0.0033815578 -0.003374639 -0.0033705486 -0.0033684433 -0.0033679316 -0.0033688615 -0.0033698466 -0.0033701088 -0.0033692818 -0.0033678988 -0.0033658626 -0.0033639 -0.0033624042 -0.0033616647 -0.0033632207][-0.0033831415 -0.0033777351 -0.0033746415 -0.0033728729 -0.0033725542 -0.0033735018 -0.0033743845 -0.0033744734 -0.0033733023 -0.0033716105 -0.0033692017 -0.0033666589 -0.0033643183 -0.0033628026 -0.0033637413][-0.0033831042 -0.003379425 -0.0033775796 -0.0033767857 -0.0033769717 -0.0033779112 -0.0033786825 -0.0033783119 -0.0033766534 -0.0033750057 -0.0033727102 -0.0033699267 -0.0033667798 -0.0033646133 -0.0033649558][-0.0033816285 -0.0033797168 -0.0033798767 -0.0033804495 -0.0033818362 -0.0033834076 -0.0033847438 -0.0033841357 -0.0033819075 -0.0033797491 -0.0033769733 -0.0033735177 -0.0033694606 -0.0033664333 -0.0033662813][-0.0033788073 -0.003378741 -0.0033811438 -0.0033836095 -0.0033862749 -0.0033888232 -0.0033907176 -0.003390098 -0.0033874922 -0.0033845438 -0.0033811275 -0.0033769018 -0.003371936 -0.003368217 -0.00336741][-0.0033756897 -0.0033767724 -0.0033806346 -0.0033845576 -0.0033882337 -0.0033912472 -0.0033932328 -0.0033929092 -0.003390457 -0.0033872032 -0.0033833135 -0.003378829 -0.0033736269 -0.0033696692 -0.003368383][-0.0033724282 -0.0033737984 -0.0033781 -0.0033826565 -0.0033871164 -0.0033904512 -0.0033925662 -0.003392603 -0.0033903767 -0.0033870803 -0.0033828591 -0.0033783833 -0.0033736432 -0.0033702755 -0.0033692108][-0.0033699449 -0.0033706252 -0.0033744967 -0.0033788707 -0.0033834518 -0.003386958 -0.0033891946 -0.0033895557 -0.0033876386 -0.003384552 -0.0033804739 -0.003376317 -0.0033721612 -0.003369913 -0.0033695463][-0.0033681106 -0.0033676159 -0.0033706657 -0.0033745233 -0.0033789536 -0.0033825871 -0.003384935 -0.0033856614 -0.0033841517 -0.0033811098 -0.0033772136 -0.0033735333 -0.0033699619 -0.0033684059 -0.0033687325][-0.0033668957 -0.0033652082 -0.0033672927 -0.0033701924 -0.0033736955 -0.0033768506 -0.0033792513 -0.0033803694 -0.00337939 -0.0033769642 -0.0033736012 -0.0033704042 -0.0033675013 -0.0033666997 -0.0033675309][-0.0033659479 -0.0033633602 -0.0033643772 -0.00336595 -0.0033681514 -0.0033701437 -0.003371988 -0.0033732674 -0.003372906 -0.0033715419 -0.0033690189 -0.0033667127 -0.0033647665 -0.0033647062 -0.003366315][-0.0033645125 -0.0033612882 -0.0033617972 -0.0033625211 -0.0033635816 -0.0033647292 -0.0033659684 -0.0033669018 -0.0033669039 -0.0033663905 -0.00336501 -0.0033637937 -0.0033626757 -0.0033629979 -0.0033651267][-0.0033630733 -0.0033594735 -0.0033595124 -0.0033598775 -0.0033604142 -0.0033609951 -0.0033616223 -0.00336241 -0.003362576 -0.0033625574 -0.003362006 -0.0033614738 -0.0033608414 -0.003361234 -0.0033635385]]...]
INFO - root - 2017-12-09 14:27:26.025301: step 32310, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 69h:17m:56s remains)
INFO - root - 2017-12-09 14:27:34.404380: step 32320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:55m:37s remains)
INFO - root - 2017-12-09 14:27:43.028886: step 32330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:47m:58s remains)
INFO - root - 2017-12-09 14:27:51.619028: step 32340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:06m:03s remains)
INFO - root - 2017-12-09 14:28:00.446508: step 32350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:02m:07s remains)
INFO - root - 2017-12-09 14:28:09.033813: step 32360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:35m:57s remains)
INFO - root - 2017-12-09 14:28:17.657565: step 32370, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 69h:25m:34s remains)
INFO - root - 2017-12-09 14:28:26.081258: step 32380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:02m:59s remains)
INFO - root - 2017-12-09 14:28:34.706029: step 32390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:16m:23s remains)
INFO - root - 2017-12-09 14:28:43.266407: step 32400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:07m:50s remains)
2017-12-09 14:28:44.179126: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26823744 0.26894438 0.26965991 0.26910025 0.26732758 0.26484454 0.26155838 0.25737384 0.25211689 0.24636211 0.24020508 0.23551036 0.23106 0.22636488 0.22222535][0.27952155 0.28037834 0.28111672 0.28048769 0.27860168 0.27527303 0.27100524 0.26570022 0.260139 0.2555545 0.25092047 0.24728635 0.24458271 0.24194652 0.23901622][0.2796146 0.28139958 0.28190866 0.28019372 0.27792633 0.27390447 0.26893488 0.26287219 0.257123 0.25301167 0.24985534 0.24794388 0.24747887 0.24682429 0.24537027][0.27175671 0.27522954 0.27632061 0.27491993 0.27291629 0.26854059 0.26293623 0.25635058 0.25066161 0.24671882 0.244373 0.24358667 0.24410519 0.2448481 0.24440157][0.2588813 0.26380023 0.26574364 0.26460376 0.26257703 0.25856355 0.25300762 0.24549022 0.23913895 0.23487495 0.23284936 0.23241316 0.23356548 0.23488744 0.23535272][0.24160437 0.24669853 0.24815241 0.24716413 0.24504566 0.24138291 0.23608084 0.22910728 0.22327632 0.21934009 0.2175452 0.21707951 0.21822335 0.21917972 0.21905398][0.22042224 0.22536652 0.22631462 0.22490171 0.22236206 0.21835193 0.21278897 0.20586005 0.20010619 0.19648629 0.19487344 0.19446823 0.1956012 0.19636481 0.19575223][0.19658412 0.20122822 0.20165378 0.19990629 0.19708131 0.19214788 0.18579592 0.17821488 0.17199457 0.16786356 0.16560021 0.16468763 0.16511907 0.16554104 0.16430947][0.16701956 0.17061144 0.17010297 0.16817957 0.164981 0.15974569 0.15346329 0.14574036 0.1394594 0.13481948 0.13203606 0.13079029 0.13062982 0.13048159 0.12863989][0.13831823 0.13974567 0.13774532 0.13525981 0.1316787 0.12629403 0.11973953 0.11208691 0.10561635 0.10054751 0.097255886 0.0952813 0.094473779 0.094118841 0.092351437][0.11151707 0.110761 0.10726739 0.10410008 0.10020598 0.095057063 0.088784508 0.081362128 0.0745882 0.068738639 0.064735807 0.062075194 0.060879532 0.060526688 0.059318196][0.093071938 0.090794779 0.0863453 0.082384653 0.07785818 0.072691381 0.066588089 0.059374493 0.052273158 0.045751393 0.040941089 0.037677836 0.036160465 0.035762303 0.035311557][0.082974315 0.080184095 0.075524122 0.071409829 0.067086436 0.062404308 0.0571609 0.050743297 0.043955728 0.037412263 0.03233929 0.028535776 0.026514426 0.025530599 0.024936434][0.080813415 0.077793926 0.072723247 0.068514585 0.064347118 0.060333226 0.05624922 0.051374748 0.045982193 0.040484484 0.03594958 0.032265667 0.029972538 0.028526023 0.027569626][0.083057635 0.080192305 0.07478065 0.069787085 0.065479659 0.061743315 0.058566958 0.055238158 0.051669654 0.048247725 0.045269731 0.042808913 0.040978283 0.039605349 0.038581744]]...]
INFO - root - 2017-12-09 14:28:52.532333: step 32410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:08m:59s remains)
INFO - root - 2017-12-09 14:29:01.002334: step 32420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:16m:50s remains)
INFO - root - 2017-12-09 14:29:09.599776: step 32430, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 74h:07m:38s remains)
INFO - root - 2017-12-09 14:29:18.229123: step 32440, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 71h:34m:46s remains)
INFO - root - 2017-12-09 14:29:27.052024: step 32450, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.915 sec/batch; 76h:17m:34s remains)
INFO - root - 2017-12-09 14:29:35.871435: step 32460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:51m:15s remains)
INFO - root - 2017-12-09 14:29:44.622163: step 32470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:33m:24s remains)
INFO - root - 2017-12-09 14:29:53.159572: step 32480, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 72h:22m:00s remains)
INFO - root - 2017-12-09 14:30:01.839739: step 32490, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:51m:46s remains)
INFO - root - 2017-12-09 14:30:10.478728: step 32500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:15m:23s remains)
2017-12-09 14:30:11.323915: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072179876 0.075910896 0.0774426 0.077267185 0.078918584 0.079354666 0.079332709 0.077466488 0.076627858 0.07420215 0.070691645 0.066251405 0.06256935 0.057942163 0.050466333][0.069306932 0.072499774 0.073288321 0.073007055 0.07491637 0.076195478 0.077690952 0.077560671 0.07798519 0.076460563 0.073714979 0.069783486 0.066865779 0.0632154 0.056703944][0.062373981 0.064755619 0.064949475 0.064247854 0.066068724 0.068068489 0.071061529 0.072958022 0.075185992 0.075056568 0.073007464 0.069354489 0.066956356 0.064423859 0.059369847][0.054464526 0.056220446 0.056048747 0.0553502 0.057292189 0.059913076 0.064095706 0.067951448 0.071733877 0.0721612 0.0701305 0.066395208 0.06407328 0.062138788 0.058420632][0.046222057 0.047122005 0.046723507 0.046244141 0.048249897 0.05165758 0.05708009 0.062425859 0.066858955 0.067568682 0.065195374 0.060664281 0.057738766 0.055955451 0.053646497][0.039180994 0.038710397 0.037363369 0.036748864 0.038749322 0.042791307 0.048896875 0.055106264 0.0601099 0.060715593 0.057724673 0.052417327 0.048648864 0.046799287 0.045398403][0.03377749 0.031915471 0.029639717 0.028305709 0.029470436 0.033278324 0.039221063 0.045475435 0.050232798 0.05068529 0.04760959 0.042218987 0.038190149 0.0362882 0.0356283][0.029603828 0.026542097 0.023515008 0.021569021 0.021994103 0.024874726 0.0296476 0.034761027 0.038476039 0.038547706 0.035482064 0.030381437 0.026485391 0.024883246 0.025148027][0.026301252 0.022240371 0.018370068 0.01571906 0.015177384 0.016959196 0.020272683 0.024022238 0.026468394 0.026144227 0.023464933 0.019270947 0.016061068 0.014748266 0.015768649][0.023710741 0.019058706 0.014516503 0.011068432 0.0094679054 0.0099176653 0.011646878 0.013867643 0.015093412 0.014557958 0.01243495 0.0093545541 0.0070793037 0.0063430783 0.0081275366][0.022140849 0.017374549 0.012474608 0.0082327761 0.0055221044 0.0046490296 0.0049975575 0.0058986847 0.0063299132 0.0058871028 0.0045621595 0.0027211574 0.001408089 0.0011466858 0.0032625208][0.021596359 0.017315267 0.012489427 0.0076830727 0.0039451956 0.0018256335 0.001073387 0.000991682 0.00081275916 0.00047512585 -0.00021332549 -0.001078693 -0.001656974 -0.0015983649 0.00047854707][0.021817977 0.018377045 0.014007397 0.0089027584 0.0042730705 0.001108964 -0.00050587952 -0.0012737443 -0.0017952272 -0.0021234532 -0.002402504 -0.0027158936 -0.002880645 -0.0027189828 -0.0010496827][0.021470273 0.019052606 0.015407227 0.010425679 0.0053768787 0.0016043987 -0.00054657366 -0.001760049 -0.0025199237 -0.0029525482 -0.0031675331 -0.003263619 -0.0032828581 -0.0031712428 -0.0020825444][0.02067145 0.018889295 0.015878847 0.011190068 0.0060649277 0.0021010069 -0.00021499069 -0.0015751978 -0.0024590841 -0.0029657346 -0.003208112 -0.003305722 -0.003345442 -0.003318989 -0.0027616513]]...]
INFO - root - 2017-12-09 14:30:19.868310: step 32510, loss = 0.90, batch loss = 0.70 (10.9 examples/sec; 0.731 sec/batch; 60h:54m:19s remains)
INFO - root - 2017-12-09 14:30:28.317898: step 32520, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 70h:19m:08s remains)
INFO - root - 2017-12-09 14:30:36.838011: step 32530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:52m:33s remains)
INFO - root - 2017-12-09 14:30:45.379834: step 32540, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 69h:37m:37s remains)
INFO - root - 2017-12-09 14:30:53.836879: step 32550, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:29m:19s remains)
INFO - root - 2017-12-09 14:31:02.465073: step 32560, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 71h:27m:36s remains)
INFO - root - 2017-12-09 14:31:11.106701: step 32570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:12m:36s remains)
INFO - root - 2017-12-09 14:31:19.422586: step 32580, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 71h:00m:19s remains)
INFO - root - 2017-12-09 14:31:28.020764: step 32590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:03m:37s remains)
INFO - root - 2017-12-09 14:31:36.613233: step 32600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:42m:17s remains)
2017-12-09 14:31:37.505265: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24734966 0.25586641 0.26266271 0.26713449 0.27095437 0.26739174 0.25402498 0.2281841 0.19342181 0.15056784 0.10556055 0.0687824 0.045109645 0.033514295 0.029002219][0.28068924 0.29438472 0.30695504 0.31576657 0.32179397 0.31827706 0.30238312 0.27319318 0.23231889 0.18213917 0.1282248 0.081492983 0.049804263 0.033652782 0.028022729][0.30716372 0.32515705 0.34278375 0.35592854 0.36449337 0.36199325 0.3457427 0.3144865 0.26987886 0.21470547 0.15437783 0.10060743 0.063378513 0.044093713 0.037456751][0.32703543 0.34825224 0.37074131 0.38880503 0.40069258 0.40023988 0.38385403 0.35117167 0.30405915 0.24664699 0.18346062 0.12630598 0.086234756 0.064734265 0.057278674][0.34395123 0.36811626 0.39560002 0.41903883 0.43540889 0.43841916 0.42334402 0.38984734 0.34007919 0.27975762 0.21449415 0.15632688 0.11652584 0.09698353 0.0920528][0.36225259 0.38999844 0.42075592 0.44803041 0.46788293 0.47413838 0.46181715 0.43085891 0.38263071 0.32337075 0.25867146 0.20040157 0.15929851 0.13867582 0.1342451][0.38175654 0.41303521 0.44552839 0.47554076 0.49790323 0.50651258 0.49772245 0.47089398 0.42726898 0.37234336 0.31277621 0.25941968 0.22121707 0.20059218 0.19465236][0.40360481 0.43649328 0.46811187 0.4988057 0.52349424 0.53566533 0.53101951 0.50871915 0.4702276 0.42117327 0.36859828 0.32207596 0.28940272 0.27171814 0.2679314][0.43619078 0.46952641 0.49784875 0.525852 0.54879534 0.561396 0.55957568 0.54299015 0.51134908 0.46951175 0.4245761 0.38553023 0.35915798 0.34556669 0.34371108][0.47464871 0.50834984 0.53310931 0.55735487 0.57678831 0.587273 0.58608919 0.57294464 0.5468924 0.51254445 0.47644198 0.44490421 0.42325914 0.41139388 0.40875721][0.50311869 0.53888035 0.56210655 0.58372778 0.60070556 0.61011881 0.60970116 0.59961677 0.57864189 0.55085772 0.52177417 0.49629092 0.47798562 0.46658611 0.46182024][0.50881881 0.54702705 0.57158542 0.59365338 0.61127847 0.62212545 0.62450886 0.6184606 0.60246176 0.58023041 0.556634 0.53584915 0.52005327 0.50937057 0.502991][0.492477 0.53202128 0.55821741 0.58186996 0.60207146 0.61625439 0.62355816 0.62389404 0.61535382 0.6001882 0.58259094 0.56558222 0.5507527 0.53971672 0.5310483][0.452488 0.49434847 0.52378839 0.55006951 0.572463 0.58971292 0.60236216 0.60909742 0.60963106 0.60331988 0.59375608 0.58172494 0.56831247 0.55612272 0.54406524][0.39492923 0.43861854 0.47168195 0.50183511 0.527987 0.548935 0.56664777 0.5781517 0.58458203 0.58349669 0.57908344 0.57027692 0.55776954 0.5455479 0.53314131]]...]
INFO - root - 2017-12-09 14:31:46.039415: step 32610, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 72h:16m:35s remains)
INFO - root - 2017-12-09 14:31:54.468952: step 32620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:42m:47s remains)
INFO - root - 2017-12-09 14:32:03.100236: step 32630, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 72h:13m:08s remains)
INFO - root - 2017-12-09 14:32:11.745256: step 32640, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 70h:52m:22s remains)
INFO - root - 2017-12-09 14:32:20.340130: step 32650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 71h:37m:45s remains)
INFO - root - 2017-12-09 14:32:29.122741: step 32660, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.904 sec/batch; 75h:19m:08s remains)
INFO - root - 2017-12-09 14:32:37.702012: step 32670, loss = 0.90, batch loss = 0.70 (9.6 examples/sec; 0.832 sec/batch; 69h:18m:31s remains)
INFO - root - 2017-12-09 14:32:46.244397: step 32680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:49m:52s remains)
INFO - root - 2017-12-09 14:32:54.841345: step 32690, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 70h:42m:54s remains)
INFO - root - 2017-12-09 14:33:03.539269: step 32700, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 72h:23m:37s remains)
2017-12-09 14:33:04.442067: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076714769 0.076652959 0.073995918 0.071040943 0.066897295 0.061628357 0.054916658 0.048157278 0.041581411 0.036517397 0.032424025 0.030596847 0.030323708 0.030559117 0.032420009][0.087309346 0.087125525 0.084764026 0.081056 0.076054506 0.069117658 0.060041375 0.050732665 0.041678779 0.034500021 0.028695049 0.025130054 0.024471236 0.025137415 0.027431224][0.093338415 0.093847521 0.09118554 0.086674362 0.080584876 0.07235226 0.061751015 0.050219495 0.038865518 0.029463844 0.022286279 0.0180091 0.016562177 0.017282519 0.02003835][0.095613427 0.0971243 0.095180988 0.090178162 0.083676733 0.074313074 0.062587857 0.049454123 0.0362423 0.024850847 0.015936784 0.010365273 0.00914093 0.010097506 0.012828263][0.095816672 0.098148823 0.096503176 0.091903917 0.085158728 0.075134546 0.063041449 0.048987668 0.034814171 0.021622648 0.011035982 0.0044968715 0.0026232239 0.0031834424 0.0061151627][0.095742784 0.09865015 0.097198136 0.092765175 0.086131193 0.0754304 0.062853038 0.04831579 0.033539776 0.019188177 0.0082237488 0.0012696644 -0.00080194906 -0.00065972772 0.0011682345][0.093847245 0.098385513 0.097105011 0.092920117 0.086236723 0.07534799 0.062706433 0.0475402 0.031839941 0.016757239 0.0058189295 -0.00037885923 -0.0023036525 -0.0025445048 -0.0017997891][0.090717793 0.097129039 0.095975675 0.091456339 0.083951704 0.0728799 0.060233004 0.044752102 0.028738702 0.01381826 0.0037585844 -0.00132017 -0.00270456 -0.0030123356 -0.0029414271][0.085489653 0.093516693 0.092486553 0.087966368 0.080206759 0.0689508 0.055960976 0.040044215 0.023853095 0.01006577 0.0013820545 -0.002314961 -0.0031933927 -0.0032572085 -0.0032419718][0.08044792 0.088502623 0.087460436 0.082665056 0.074425407 0.0629221 0.049643643 0.034057476 0.018603053 0.0063272705 -0.00063008186 -0.002886689 -0.003360997 -0.003363691 -0.0033257329][0.076577984 0.084522143 0.08232253 0.076950423 0.068057418 0.056179926 0.042812787 0.027920021 0.013676394 0.003320493 -0.0018097846 -0.0030828561 -0.0033449982 -0.0033287017 -0.0033305376][0.073501863 0.081613481 0.078897022 0.07243184 0.062146604 0.049672287 0.03620204 0.022096148 0.00954054 0.0012719303 -0.0024299724 -0.0031461131 -0.0033015201 -0.0032697162 -0.0033241657][0.071862347 0.079504415 0.075811744 0.068812683 0.057728395 0.044337425 0.030556465 0.017310066 0.0063800449 -0.0001218305 -0.0027097366 -0.0030781771 -0.00323865 -0.0032233361 -0.0032866986][0.071217112 0.078899369 0.074422084 0.066407524 0.054301005 0.039922338 0.025614126 0.013026385 0.0037503934 -0.0012133757 -0.0029786881 -0.0030949314 -0.0031907565 -0.003153611 -0.0032395546][0.071547478 0.078822508 0.073769838 0.065319359 0.052428771 0.037136883 0.022387182 0.010245048 0.0020355161 -0.0018075492 -0.002932244 -0.0030004776 -0.0030557087 -0.0030513052 -0.0031678397]]...]
INFO - root - 2017-12-09 14:33:13.141669: step 32710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:44m:10s remains)
INFO - root - 2017-12-09 14:33:21.573007: step 32720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:08m:06s remains)
INFO - root - 2017-12-09 14:33:30.272478: step 32730, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 74h:18m:15s remains)
INFO - root - 2017-12-09 14:33:38.850464: step 32740, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:27m:36s remains)
INFO - root - 2017-12-09 14:33:47.428604: step 32750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:14m:01s remains)
INFO - root - 2017-12-09 14:33:55.930696: step 32760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:16m:57s remains)
INFO - root - 2017-12-09 14:34:04.626378: step 32770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:22m:53s remains)
INFO - root - 2017-12-09 14:34:13.317524: step 32780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:10m:43s remains)
INFO - root - 2017-12-09 14:34:22.076828: step 32790, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 72h:55m:52s remains)
INFO - root - 2017-12-09 14:34:30.881779: step 32800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:14m:55s remains)
2017-12-09 14:34:31.701151: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049501434 0.04893 0.048328251 0.048018202 0.047937885 0.047390144 0.045709871 0.042456619 0.0372566 0.030334111 0.022146758 0.013874698 0.0065397136 0.0011257289 -0.001761293][0.056372672 0.055511583 0.054235414 0.053641181 0.053121291 0.052304067 0.050557751 0.0475113 0.0427663 0.036085561 0.027637763 0.018268123 0.0094934339 0.0027350707 -0.0010747076][0.057231504 0.056621887 0.05549369 0.055135466 0.054764133 0.054006062 0.052202392 0.049134474 0.044618215 0.038445368 0.030471839 0.0211667 0.011934292 0.0042731836 -0.00042062346][0.057847198 0.057690594 0.056689374 0.056345612 0.055949084 0.055265974 0.053598352 0.050693389 0.046316769 0.040233441 0.032220464 0.0227468 0.013230806 0.0052432027 0.00012568035][0.058016 0.058227111 0.057185512 0.056738053 0.056145657 0.05534143 0.053617563 0.050875876 0.0467479 0.041129697 0.033595636 0.024349472 0.014554778 0.0060628233 0.00054929568][0.057084575 0.05737789 0.056115363 0.05545919 0.054552939 0.053527534 0.05190669 0.049708828 0.046335209 0.041520294 0.034751821 0.026086753 0.016491627 0.0077667655 0.0016569656][0.052874655 0.053490575 0.052237105 0.051460728 0.050623015 0.050045293 0.049121231 0.047792468 0.045354631 0.041562911 0.035824154 0.028110825 0.019111607 0.010384654 0.0036969793][0.044989958 0.0458785 0.0450456 0.044958379 0.044961426 0.045154553 0.045305755 0.04525201 0.043894954 0.040937405 0.035965964 0.029205814 0.021105183 0.012987945 0.0062746778][0.034710694 0.035681564 0.035084471 0.035646223 0.036539055 0.03782893 0.039135192 0.039952364 0.039466824 0.037469987 0.03351314 0.02799689 0.021236712 0.014356038 0.0083623929][0.02279521 0.023834158 0.023604177 0.024474952 0.02572016 0.027473496 0.029347539 0.030865613 0.031239294 0.030100919 0.027452448 0.023658188 0.018809024 0.013780961 0.00916168][0.011628537 0.012323484 0.012421638 0.013230268 0.014378756 0.016090188 0.017969782 0.019667409 0.02059325 0.020416304 0.019158596 0.016890289 0.013894314 0.010774073 0.0078040585][0.0030746933 0.0035252252 0.0037442415 0.0044090422 0.005392612 0.0067172395 0.0082114125 0.00968113 0.010733575 0.011175504 0.010878481 0.0098514557 0.0082781846 0.006453855 0.0046939673][-0.0018413257 -0.0016329183 -0.0013716305 -0.00091607333 -0.00027136481 0.00054211938 0.0015011479 0.0024781616 0.0032806897 0.0038069871 0.0039381012 0.0036489943 0.0030135454 0.0021891852 0.0013595519][-0.0033507941 -0.0033154155 -0.0032203577 -0.0030519809 -0.0027878925 -0.0024485905 -0.0020371708 -0.0016183547 -0.0012237458 -0.0009094805 -0.00077578938 -0.00079957349 -0.000974369 -0.0012283036 -0.0014792325][-0.0033893383 -0.0033820632 -0.0033722217 -0.0033510623 -0.0033093505 -0.0032439856 -0.0031550832 -0.0030615963 -0.0029681039 -0.002898063 -0.0028594052 -0.0028495253 -0.0028769039 -0.002910204 -0.0029325152]]...]
INFO - root - 2017-12-09 14:34:40.484890: step 32810, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 72h:07m:52s remains)
INFO - root - 2017-12-09 14:34:48.857075: step 32820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:23m:01s remains)
INFO - root - 2017-12-09 14:34:57.547486: step 32830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:22m:13s remains)
INFO - root - 2017-12-09 14:35:06.068602: step 32840, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.828 sec/batch; 68h:54m:35s remains)
INFO - root - 2017-12-09 14:35:14.577451: step 32850, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:09m:46s remains)
INFO - root - 2017-12-09 14:35:23.242771: step 32860, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:38m:24s remains)
INFO - root - 2017-12-09 14:35:31.716285: step 32870, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 71h:09m:28s remains)
INFO - root - 2017-12-09 14:35:40.090079: step 32880, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 68h:47m:34s remains)
INFO - root - 2017-12-09 14:35:48.738801: step 32890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:47m:42s remains)
INFO - root - 2017-12-09 14:35:57.393322: step 32900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:39m:31s remains)
2017-12-09 14:35:58.244187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033108883 -0.0032957245 -0.0032869331 -0.0032854124 -0.0032872218 -0.0032882094 -0.0032895596 -0.003292789 -0.0032982884 -0.0033060606 -0.0033160257 -0.0033273126 -0.0033380706 -0.003346161 -0.0033505079][-0.0032806271 -0.0032630067 -0.0032472673 -0.0032371464 -0.003230144 -0.0032249889 -0.0032229947 -0.003223964 -0.0032290148 -0.00323948 -0.0032547219 -0.0032721483 -0.0032874907 -0.0032968896 -0.0032994144][-0.0032373345 -0.0032041715 -0.0031673596 -0.0031330169 -0.0031051079 -0.0030868277 -0.0030764025 -0.0030733631 -0.0030786367 -0.0030932534 -0.0031165362 -0.0031451515 -0.0031717245 -0.0031891661 -0.0031961864][-0.0031709671 -0.0031056085 -0.0030317528 -0.0029612919 -0.0029048505 -0.0028679278 -0.0028478652 -0.0028424256 -0.0028492673 -0.0028688754 -0.0029004011 -0.0029404531 -0.0029803105 -0.0030123384 -0.003034146][-0.003064557 -0.0029661071 -0.0028586779 -0.0027592038 -0.0026807655 -0.0026289145 -0.0026020897 -0.0025968566 -0.0026078043 -0.0026349116 -0.0026759917 -0.0027280331 -0.0027833818 -0.0028358472 -0.0028826208][-0.0029280719 -0.0028123846 -0.0026906563 -0.0025795426 -0.0024918176 -0.0024321699 -0.0024008136 -0.0023947381 -0.0024097692 -0.0024454449 -0.0024977063 -0.0025652209 -0.0026406846 -0.002718901 -0.0027956371][-0.0028056826 -0.0026966943 -0.0025833778 -0.0024787337 -0.0023928066 -0.002329106 -0.0022906661 -0.0022770842 -0.00228931 -0.0023271982 -0.0023884547 -0.0024725208 -0.0025717712 -0.0026790276 -0.0027868063][-0.0027436733 -0.0026557008 -0.0025630787 -0.0024764074 -0.002401385 -0.0023386106 -0.0022929669 -0.0022681979 -0.0022701574 -0.0023021284 -0.0023661249 -0.0024630092 -0.0025833561 -0.0027159036 -0.0028480291][-0.0027652835 -0.002705866 -0.0026386527 -0.0025721118 -0.0025113146 -0.0024566869 -0.0024118102 -0.0023808088 -0.0023711042 -0.0023926417 -0.0024535963 -0.0025546378 -0.0026842486 -0.0028273095 -0.0029667402][-0.0028622076 -0.00283058 -0.0027858838 -0.00273716 -0.0026909662 -0.0026487729 -0.002611659 -0.0025827941 -0.0025701574 -0.00258569 -0.002640425 -0.0027361745 -0.0028597733 -0.0029929287 -0.0031164021][-0.0030064099 -0.0029967404 -0.0029708114 -0.0029374997 -0.002903742 -0.0028730324 -0.0028463467 -0.0028260322 -0.0028184212 -0.0028341683 -0.0028822965 -0.0029628477 -0.0030622829 -0.0031635643 -0.0032510243][-0.0031610772 -0.003163954 -0.0031504934 -0.0031291745 -0.0031059191 -0.0030849592 -0.003066998 -0.0030553876 -0.0030540675 -0.0030701738 -0.0031082775 -0.0031657163 -0.0032307494 -0.0032906896 -0.0033372217][-0.0032876253 -0.0032956512 -0.003289534 -0.0032771053 -0.003261992 -0.0032481921 -0.0032368384 -0.0032309606 -0.0032329077 -0.0032456445 -0.0032701392 -0.0033021027 -0.0033338561 -0.0033588549 -0.0033752585][-0.0033645427 -0.0033711055 -0.0033687416 -0.003362776 -0.00335511 -0.0033476895 -0.0033413756 -0.0033384766 -0.0033396578 -0.0033456301 -0.0033554519 -0.003366776 -0.0033767787 -0.0033825689 -0.003385491][-0.0033951176 -0.0033980734 -0.003396905 -0.0033948121 -0.0033916598 -0.0033883962 -0.0033854228 -0.0033836754 -0.0033831063 -0.0033837147 -0.0033843003 -0.0033852123 -0.0033855375 -0.0033850542 -0.0033847019]]...]
INFO - root - 2017-12-09 14:36:06.847835: step 32910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:22m:59s remains)
INFO - root - 2017-12-09 14:36:15.239149: step 32920, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 72h:18m:41s remains)
INFO - root - 2017-12-09 14:36:23.794883: step 32930, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 72h:20m:54s remains)
INFO - root - 2017-12-09 14:36:32.300840: step 32940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:21m:08s remains)
INFO - root - 2017-12-09 14:36:41.015822: step 32950, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 74h:57m:04s remains)
INFO - root - 2017-12-09 14:36:49.660036: step 32960, loss = 0.88, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 73h:01m:39s remains)
INFO - root - 2017-12-09 14:36:58.314440: step 32970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:09m:14s remains)
INFO - root - 2017-12-09 14:37:06.811457: step 32980, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 64h:00m:51s remains)
INFO - root - 2017-12-09 14:37:15.553707: step 32990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:07m:53s remains)
INFO - root - 2017-12-09 14:37:24.298677: step 33000, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:40m:01s remains)
2017-12-09 14:37:25.181025: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07526806 0.080193363 0.082568645 0.082533814 0.080226667 0.076536492 0.071279444 0.065005474 0.059196137 0.054609507 0.050718866 0.047588922 0.04455933 0.0420203 0.039165363][0.084435 0.090693921 0.093251333 0.092561521 0.089231886 0.084040731 0.076698862 0.068482414 0.060901463 0.054972582 0.050383963 0.047399074 0.045092747 0.043532223 0.04162056][0.090974391 0.098727725 0.10139406 0.099740967 0.094796658 0.087668136 0.078399926 0.0684931 0.059531242 0.052508414 0.047431387 0.044468179 0.043075934 0.042720526 0.041886196][0.095537506 0.10491835 0.10815009 0.1060769 0.099759929 0.090409644 0.078675129 0.066934183 0.05668477 0.048992939 0.043737613 0.041114278 0.040712558 0.041584805 0.042008385][0.096653529 0.10777017 0.11237489 0.11084095 0.10404415 0.093303353 0.079874612 0.066624559 0.054943465 0.046251729 0.040785775 0.038755283 0.039454095 0.041350313 0.042600997][0.0958091 0.10812908 0.11338932 0.11265918 0.10632294 0.095237955 0.0812407 0.067090556 0.05473049 0.045554645 0.039860152 0.038147874 0.039301962 0.041960947 0.043892942][0.09257324 0.10601243 0.1120536 0.11186337 0.10605159 0.09535592 0.0815176 0.067642413 0.055483162 0.046323843 0.040705241 0.039054967 0.040297866 0.043027565 0.04507187][0.088049382 0.10197361 0.1083948 0.10903227 0.10405261 0.0942096 0.081251308 0.068010427 0.056466624 0.047872454 0.042733356 0.041247368 0.042407252 0.044682324 0.046288379][0.081601761 0.095719919 0.10262095 0.10416816 0.1007438 0.092684731 0.081480384 0.069655739 0.058753416 0.050586239 0.045544855 0.043875657 0.044518307 0.046033785 0.047054462][0.073906191 0.087678462 0.094754234 0.097424217 0.095998935 0.090485737 0.081779107 0.072081596 0.062480003 0.054751128 0.049562916 0.047334127 0.047116958 0.047750309 0.047761][0.066236295 0.079259589 0.0861266 0.089365795 0.089436367 0.086216576 0.0801161 0.072731934 0.0648904 0.058129109 0.0530708 0.050294302 0.049054176 0.048652641 0.04774116][0.057675041 0.06980335 0.07635729 0.07976079 0.080692574 0.079132885 0.075105719 0.069798879 0.063588411 0.058153026 0.053818256 0.051043913 0.049303897 0.048042506 0.046234205][0.048010916 0.058596954 0.06463284 0.068129152 0.069673643 0.069248132 0.066759363 0.063123815 0.058420688 0.054055177 0.050397217 0.048014171 0.046265103 0.044832423 0.042834684][0.0381338 0.046652772 0.051647909 0.054829009 0.056737833 0.057138123 0.055937327 0.0536504 0.050336592 0.047163121 0.044208169 0.042134747 0.040555224 0.039238043 0.037464425][0.029494515 0.036088921 0.039902441 0.042539187 0.044270188 0.044932246 0.044502966 0.043241926 0.04122968 0.039084546 0.036977708 0.035410494 0.03404358 0.033012763 0.031690821]]...]
INFO - root - 2017-12-09 14:37:33.821346: step 33010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:18m:18s remains)
INFO - root - 2017-12-09 14:37:42.032401: step 33020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:59m:06s remains)
INFO - root - 2017-12-09 14:37:50.335845: step 33030, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 68h:10m:12s remains)
INFO - root - 2017-12-09 14:37:58.995748: step 33040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:43m:20s remains)
INFO - root - 2017-12-09 14:38:07.691585: step 33050, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 73h:36m:27s remains)
INFO - root - 2017-12-09 14:38:16.366259: step 33060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:48m:19s remains)
INFO - root - 2017-12-09 14:38:25.153406: step 33070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:35m:02s remains)
INFO - root - 2017-12-09 14:38:33.904692: step 33080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:41m:20s remains)
INFO - root - 2017-12-09 14:38:42.396548: step 33090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:13m:06s remains)
INFO - root - 2017-12-09 14:38:51.013038: step 33100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:43m:50s remains)
2017-12-09 14:38:51.895448: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0031913894 0.0042547612 0.0058442354 0.0080211032 0.010850343 0.013898455 0.01655414 0.017934827 0.017489268 0.015327929 0.012103082 0.0089220759 0.0062268917 0.0045090113 0.0035911321][0.0042580785 0.0055141626 0.007385944 0.010212261 0.01426656 0.019114805 0.023672575 0.0264039 0.02622564 0.023099061 0.018090688 0.012745377 0.0080840206 0.0049421582 0.0032257268][0.0065938635 0.0084542623 0.010987962 0.014551358 0.019418322 0.025377756 0.031189786 0.035101656 0.035626519 0.032396711 0.026323667 0.019031895 0.012339056 0.0074189221 0.0045194244][0.0072170589 0.0093738493 0.012715386 0.01758017 0.024035951 0.031626824 0.038743518 0.043574907 0.044697989 0.04180545 0.03585963 0.028134061 0.020552782 0.014369378 0.010321226][0.0080694761 0.01085487 0.015278112 0.021639405 0.029646067 0.038701523 0.04698018 0.052636664 0.054306086 0.051956672 0.046696492 0.039666388 0.032611374 0.0264015 0.021863393][0.011447494 0.015969872 0.022171892 0.030080622 0.0392442 0.0488715 0.057172336 0.062497668 0.063960634 0.062045641 0.05791755 0.052551072 0.047025245 0.041710235 0.0374324][0.01725517 0.024539186 0.033652261 0.044002783 0.054607436 0.064346582 0.071666092 0.075439148 0.075538829 0.073139526 0.06969136 0.066157 0.062601969 0.058760043 0.055050984][0.024814714 0.035695449 0.048022605 0.060504377 0.071953557 0.081251323 0.087301217 0.089595042 0.088631906 0.086033858 0.083343036 0.0813902 0.079684764 0.07742577 0.074578755][0.032482173 0.047219474 0.062840179 0.077078596 0.08854422 0.096585147 0.10078807 0.10146884 0.099587955 0.096942663 0.095017746 0.094485134 0.0945599 0.094055958 0.092531323][0.039428513 0.057445932 0.075705647 0.091128357 0.10210221 0.10844816 0.11070413 0.10984983 0.1072281 0.10458583 0.10305101 0.10305553 0.10382134 0.10421125 0.10361817][0.042121675 0.062361643 0.082444921 0.098693006 0.10937022 0.11455794 0.11547142 0.11366542 0.11073095 0.10805455 0.1063794 0.10586998 0.10586435 0.1056598 0.10476353][0.039025705 0.059195466 0.079544254 0.096196458 0.10701191 0.11200809 0.11273246 0.11097308 0.10828835 0.10567342 0.10352542 0.10175435 0.10002886 0.098234944 0.096194781][0.032070968 0.049716197 0.068062156 0.083640978 0.094234347 0.099603079 0.10105635 0.10015122 0.098144531 0.0956442 0.092861108 0.089744292 0.086354785 0.083054826 0.079932287][0.023260036 0.0369545 0.051686596 0.064712681 0.07409995 0.07942728 0.081525885 0.081501581 0.080138735 0.077752486 0.074498415 0.070518591 0.066218928 0.062216617 0.058718324][0.014459332 0.02382583 0.034326263 0.044051457 0.051504403 0.0561664 0.058426261 0.058893997 0.057960376 0.055791833 0.052566178 0.048582293 0.044375077 0.040562157 0.037354622]]...]
INFO - root - 2017-12-09 14:39:00.514790: step 33110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:24m:44s remains)
INFO - root - 2017-12-09 14:39:08.662707: step 33120, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 69h:25m:25s remains)
INFO - root - 2017-12-09 14:39:17.111501: step 33130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:04m:07s remains)
INFO - root - 2017-12-09 14:39:25.744953: step 33140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:15m:34s remains)
INFO - root - 2017-12-09 14:39:34.450321: step 33150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 73h:07m:27s remains)
INFO - root - 2017-12-09 14:39:43.099276: step 33160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:33m:48s remains)
INFO - root - 2017-12-09 14:39:51.850434: step 33170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:39m:47s remains)
INFO - root - 2017-12-09 14:40:00.639987: step 33180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:48m:32s remains)
INFO - root - 2017-12-09 14:40:09.118203: step 33190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:54m:49s remains)
INFO - root - 2017-12-09 14:40:17.876206: step 33200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:43m:35s remains)
2017-12-09 14:40:18.730528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033962152 -0.0033953628 -0.0033957004 -0.0033961178 -0.0033962543 -0.0033961881 -0.0033959586 -0.0033959881 -0.0033960044 -0.0033962138 -0.003396929 -0.0033977681 -0.0033985686 -0.0033983637 -0.0033959628][-0.003395434 -0.0033946505 -0.0033950396 -0.0033957046 -0.0033959069 -0.003395569 -0.0033943378 -0.0033937094 -0.0033939222 -0.0033950252 -0.0033965106 -0.003397698 -0.0033980974 -0.0033955716 -0.0033871427][-0.0033961015 -0.0033955569 -0.00339589 -0.003396316 -0.0033952463 -0.0033922535 -0.0033877483 -0.003384751 -0.0033858151 -0.0033903818 -0.0033951504 -0.0033978052 -0.0033964764 -0.0033870863 -0.0033633215][-0.0033976953 -0.0033970915 -0.0033962037 -0.0033934664 -0.0033850712 -0.0033714143 -0.0033561259 -0.0033480208 -0.0033532679 -0.003369163 -0.0033857997 -0.0033948335 -0.0033902477 -0.0033632088 -0.0033034985][-0.0033996885 -0.0033981458 -0.0033934596 -0.0033806213 -0.0033514784 -0.0033069558 -0.0032618016 -0.0032418992 -0.0032615212 -0.0033081453 -0.0033550754 -0.0033816779 -0.0033738483 -0.0033155773 -0.0031930604][-0.0034008638 -0.0033972587 -0.0033852467 -0.0033543885 -0.003288389 -0.0031885086 -0.0030897567 -0.003049525 -0.0030936806 -0.0031949468 -0.0032959711 -0.0033545548 -0.0033458448 -0.0032483663 -0.0030489427][-0.0033997982 -0.0033935856 -0.0033730427 -0.0033211941 -0.0032150536 -0.003054763 -0.0028989662 -0.0028348083 -0.0029032328 -0.0030639002 -0.0032265573 -0.0033226681 -0.0033183147 -0.0031928471 -0.002937617][-0.003398818 -0.0033911301 -0.0033658075 -0.0033024144 -0.0031763304 -0.0029886663 -0.0028081117 -0.0027315677 -0.00280812 -0.0029951793 -0.0031876944 -0.0033054214 -0.0033105959 -0.0031877307 -0.0029318382][-0.0033987956 -0.0033929243 -0.0033712033 -0.0033144164 -0.0032014155 -0.0030361675 -0.0028796177 -0.0028128866 -0.0028790976 -0.0030402006 -0.0032081201 -0.0033143174 -0.0033287485 -0.003239291 -0.0030394553][-0.00339891 -0.0033967723 -0.0033849867 -0.0033479345 -0.0032703644 -0.0031567046 -0.0030508908 -0.0030087535 -0.0030564119 -0.0031626881 -0.0032704717 -0.0033408292 -0.0033583269 -0.0033123672 -0.0031912604][-0.0033986212 -0.0033993418 -0.0033964945 -0.0033803047 -0.0033405852 -0.0032793756 -0.0032221952 -0.0032011473 -0.0032288248 -0.0032836751 -0.0033352715 -0.0033691858 -0.0033816984 -0.003366767 -0.0033127102][-0.0033976226 -0.0033983884 -0.0033993502 -0.0033958582 -0.0033819119 -0.0033575054 -0.0033336175 -0.0033253378 -0.0033367325 -0.003357141 -0.0033752562 -0.0033870523 -0.0033931797 -0.00339124 -0.0033744443][-0.003396628 -0.0033971525 -0.0033985078 -0.0033991789 -0.0033972431 -0.0033914333 -0.0033845385 -0.0033816888 -0.0033841319 -0.0033884996 -0.0033921083 -0.0033949136 -0.0033971604 -0.0033981202 -0.0033953276][-0.0033957551 -0.0033958007 -0.0033971581 -0.003398356 -0.0033991057 -0.0033985437 -0.0033972629 -0.0033962769 -0.0033956675 -0.0033953136 -0.0033952706 -0.0033959914 -0.0033969576 -0.003397929 -0.0033984249][-0.0033954331 -0.003395109 -0.0033960014 -0.0033967867 -0.0033973174 -0.0033972678 -0.0033969106 -0.0033964515 -0.0033956745 -0.0033950957 -0.0033946717 -0.0033950298 -0.0033958212 -0.0033966433 -0.0033973579]]...]
INFO - root - 2017-12-09 14:40:27.290902: step 33210, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 70h:40m:18s remains)
INFO - root - 2017-12-09 14:40:35.643914: step 33220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:11m:52s remains)
INFO - root - 2017-12-09 14:40:44.044599: step 33230, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 69h:23m:53s remains)
INFO - root - 2017-12-09 14:40:52.535621: step 33240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:31m:37s remains)
INFO - root - 2017-12-09 14:41:01.169819: step 33250, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 70h:32m:59s remains)
INFO - root - 2017-12-09 14:41:09.802493: step 33260, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 69h:19m:09s remains)
INFO - root - 2017-12-09 14:41:18.536846: step 33270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:12m:47s remains)
INFO - root - 2017-12-09 14:41:27.163003: step 33280, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.820 sec/batch; 68h:08m:18s remains)
INFO - root - 2017-12-09 14:41:35.668776: step 33290, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:41m:14s remains)
INFO - root - 2017-12-09 14:41:44.306471: step 33300, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 75h:14m:03s remains)
2017-12-09 14:41:45.165769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16567071 0.17474103 0.18880136 0.20539692 0.2234809 0.23752019 0.24201094 0.23426932 0.21333815 0.18143846 0.1420003 0.10104284 0.063615762 0.033644017 0.01300495][0.21122016 0.22706856 0.24673361 0.26610172 0.284487 0.29687035 0.29894283 0.28813544 0.26307449 0.22590509 0.1791755 0.12981439 0.083324321 0.04522239 0.018369325][0.25454956 0.27917454 0.3060644 0.3289263 0.34720472 0.35611415 0.35309258 0.33711025 0.3068901 0.26450026 0.21168491 0.15564691 0.10177014 0.056878682 0.024336839][0.28718275 0.31968641 0.35342515 0.38040915 0.39971837 0.40655136 0.40006661 0.38016716 0.34553185 0.29855153 0.24007779 0.17774592 0.11722698 0.06650573 0.029193766][0.30968562 0.34727955 0.38515294 0.4152061 0.43551737 0.44099912 0.43244845 0.4108035 0.37441421 0.32528022 0.26305115 0.19603264 0.13005912 0.074328616 0.032924075][0.32308421 0.3635605 0.40340206 0.43494681 0.4559108 0.46061295 0.45066687 0.42720279 0.38931546 0.33912891 0.27490821 0.20535584 0.1362949 0.077974357 0.034434091][0.32885489 0.36949548 0.40873519 0.44111109 0.462817 0.46725911 0.45664316 0.4317756 0.39263096 0.34093291 0.27508429 0.20432539 0.13443811 0.07605318 0.03277164][0.32884514 0.36907834 0.40608642 0.43754038 0.45891741 0.46331903 0.45238325 0.42619967 0.38569611 0.33261642 0.26593933 0.19496822 0.12598564 0.069611818 0.0288352][0.32249862 0.36203846 0.39679804 0.4274736 0.44865549 0.45297667 0.44143325 0.41363749 0.371375 0.3168489 0.24988377 0.18000422 0.1136496 0.060767755 0.023769304][0.30958223 0.34849861 0.38184324 0.41211104 0.43351015 0.43806833 0.42586252 0.39621815 0.35183477 0.29550314 0.22843602 0.16065995 0.098270282 0.050249226 0.018150011][0.28911069 0.32723829 0.36018196 0.39071786 0.41289309 0.41827273 0.40602592 0.37491304 0.32863808 0.27090284 0.20442644 0.13945335 0.0816764 0.039250992 0.012535197][0.25981811 0.29698205 0.32954997 0.36025006 0.38340652 0.39057472 0.38007975 0.35019159 0.30432612 0.24645838 0.18137158 0.11966848 0.066729665 0.029674508 0.0078547224][0.22245902 0.25831509 0.2907519 0.3211045 0.34448591 0.35337293 0.34549049 0.31876796 0.27610928 0.22158915 0.16053037 0.10317928 0.055073939 0.022714758 0.0048442287][0.17679004 0.20974201 0.24135876 0.27127498 0.29510176 0.30596706 0.30132544 0.27908534 0.2416009 0.19280849 0.13822187 0.087382153 0.045283739 0.017600797 0.0028951925][0.13018568 0.15792732 0.18621638 0.21356098 0.23634714 0.24866945 0.24781841 0.23128316 0.20075656 0.15978947 0.11367884 0.070822418 0.035635516 0.012963674 0.0013455057]]...]
INFO - root - 2017-12-09 14:41:53.839707: step 33310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:34m:56s remains)
INFO - root - 2017-12-09 14:42:02.111337: step 33320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:47m:31s remains)
INFO - root - 2017-12-09 14:42:10.630720: step 33330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:23m:43s remains)
INFO - root - 2017-12-09 14:42:19.189279: step 33340, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 71h:28m:14s remains)
INFO - root - 2017-12-09 14:42:27.835805: step 33350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:27m:16s remains)
INFO - root - 2017-12-09 14:42:36.479624: step 33360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:53m:17s remains)
INFO - root - 2017-12-09 14:42:45.258022: step 33370, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 76h:01m:44s remains)
INFO - root - 2017-12-09 14:42:53.749595: step 33380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:12m:09s remains)
INFO - root - 2017-12-09 14:43:02.069668: step 33390, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 67h:55m:15s remains)
INFO - root - 2017-12-09 14:43:10.533915: step 33400, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 68h:50m:03s remains)
2017-12-09 14:43:11.368233: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39173064 0.3809613 0.36099645 0.33492082 0.30686539 0.27966198 0.25835839 0.24440445 0.24097647 0.24712004 0.26107904 0.28053698 0.2997382 0.31598124 0.32728788][0.37103349 0.36740768 0.3550126 0.33624673 0.31381145 0.29039457 0.27099854 0.2575756 0.25303093 0.25646579 0.26705486 0.28286126 0.29873878 0.31194237 0.32032889][0.332596 0.33733219 0.33418372 0.32507664 0.311083 0.29395378 0.27840558 0.26634288 0.26111495 0.26147428 0.26710528 0.27642202 0.28604162 0.29367226 0.29759246][0.28314465 0.29700068 0.30451214 0.30629742 0.30260172 0.29431045 0.28482652 0.27564904 0.26991484 0.2666159 0.2661306 0.26753351 0.26893374 0.26909044 0.26666805][0.2289637 0.25069752 0.26878864 0.28242338 0.29007891 0.29189748 0.28993514 0.2853297 0.2800914 0.27312046 0.26625231 0.25903508 0.25147584 0.24288784 0.23283052][0.17901696 0.20676181 0.23386782 0.25826371 0.27644172 0.28759211 0.29279083 0.29297131 0.28897452 0.27959782 0.2673873 0.25254893 0.23659754 0.21952625 0.2017539][0.14233325 0.17586221 0.21046701 0.24304272 0.26940385 0.28805181 0.29840484 0.30174467 0.29823223 0.2871176 0.27068362 0.24965419 0.22677411 0.20247936 0.17846568][0.1228992 0.16042235 0.20012718 0.23818 0.26957768 0.29266864 0.3063361 0.31128642 0.30751207 0.29460311 0.27487192 0.2492824 0.22115535 0.19187281 0.16368282][0.12013125 0.15859665 0.1996852 0.23989426 0.27356383 0.29858169 0.31355312 0.31902087 0.31435582 0.29972905 0.27740374 0.24901737 0.21800381 0.18601327 0.15566304][0.13131347 0.16818114 0.20663092 0.24443747 0.27633652 0.30020812 0.31421134 0.31863222 0.31273589 0.29712591 0.27388585 0.24484597 0.21345422 0.18138196 0.15107644][0.14975755 0.18321192 0.21651983 0.24902548 0.27610278 0.29601172 0.30701211 0.30919182 0.30177832 0.28578392 0.26287952 0.23513907 0.20536858 0.17514706 0.14648364][0.17009445 0.19956565 0.22653306 0.25212771 0.27276725 0.28737774 0.29432684 0.29359031 0.28451672 0.26821759 0.24619217 0.22055998 0.19341022 0.16598381 0.1396421][0.18562302 0.21047048 0.23085648 0.24928324 0.26325914 0.27228659 0.27509084 0.27182975 0.26183373 0.24602069 0.22573757 0.20282024 0.17871746 0.15423949 0.13015][0.1898056 0.21050426 0.22510439 0.2374115 0.24569601 0.24977212 0.24895786 0.24366333 0.23350164 0.21909928 0.20143308 0.18177126 0.1609634 0.13926031 0.11712961][0.18143281 0.19823319 0.20814532 0.21528521 0.21877457 0.21896979 0.21551175 0.20905398 0.19943973 0.18705747 0.17244919 0.15626724 0.13877477 0.11986128 0.099892415]]...]
INFO - root - 2017-12-09 14:43:19.791140: step 33410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:21m:51s remains)
INFO - root - 2017-12-09 14:43:28.227610: step 33420, loss = 0.89, batch loss = 0.68 (14.1 examples/sec; 0.567 sec/batch; 47h:06m:51s remains)
INFO - root - 2017-12-09 14:43:36.656310: step 33430, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 69h:55m:11s remains)
INFO - root - 2017-12-09 14:43:45.041930: step 33440, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 71h:32m:54s remains)
INFO - root - 2017-12-09 14:43:53.640294: step 33450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:17m:34s remains)
INFO - root - 2017-12-09 14:44:02.460773: step 33460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:33m:40s remains)
INFO - root - 2017-12-09 14:44:11.104071: step 33470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:18m:59s remains)
INFO - root - 2017-12-09 14:44:19.932580: step 33480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:29m:21s remains)
INFO - root - 2017-12-09 14:44:28.421618: step 33490, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 65h:57m:29s remains)
INFO - root - 2017-12-09 14:44:37.140375: step 33500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:44m:35s remains)
2017-12-09 14:44:38.102102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003347469 -0.0033436597 -0.0033429305 -0.0033423353 -0.0033415388 -0.0033408713 -0.0033401947 -0.0033394459 -0.0033387905 -0.0033382054 -0.0033378524 -0.0033375612 -0.0033373404 -0.0033371646 -0.0033369497][-0.003345405 -0.0033416033 -0.0033413838 -0.003341483 -0.0033412417 -0.0033408296 -0.0033399642 -0.0033387169 -0.0033373833 -0.0033360033 -0.0033349171 -0.0033342461 -0.0033338242 -0.0033336231 -0.0033335371][-0.0033462681 -0.003343117 -0.0033440515 -0.0033453933 -0.0033462078 -0.0033461486 -0.0033452273 -0.0033432443 -0.0033407165 -0.003338034 -0.0033358051 -0.003334471 -0.0033337087 -0.00333339 -0.0033333949][-0.0033468248 -0.0033446529 -0.0033471882 -0.0033503834 -0.0033527496 -0.0033534889 -0.003352697 -0.0033499366 -0.0033459354 -0.003341466 -0.0033376846 -0.00333528 -0.0033339609 -0.0033334703 -0.0033335097][-0.0033474255 -0.0033462795 -0.0033504509 -0.0033555196 -0.0033595681 -0.003361342 -0.0033608831 -0.0033573827 -0.0033517655 -0.003345385 -0.0033401384 -0.0033366641 -0.0033347516 -0.003334058 -0.0033341013][-0.0033478355 -0.0033474835 -0.0033528318 -0.003359118 -0.003364407 -0.0033670084 -0.0033665581 -0.0033625495 -0.0033557666 -0.0033480711 -0.0033419866 -0.0033379716 -0.0033357767 -0.0033350699 -0.003335251][-0.0033476788 -0.0033477917 -0.0033536705 -0.0033605886 -0.003366567 -0.0033695183 -0.0033691237 -0.0033650622 -0.00335794 -0.0033497489 -0.0033433053 -0.003339167 -0.0033370948 -0.0033365018 -0.0033367546][-0.0033469126 -0.003347161 -0.0033529568 -0.0033597976 -0.003365708 -0.0033685961 -0.0033684103 -0.0033646394 -0.0033579187 -0.0033501103 -0.0033441028 -0.0033403835 -0.0033384399 -0.0033378322 -0.0033380261][-0.0033460453 -0.003345767 -0.0033507177 -0.0033564963 -0.0033615667 -0.003364248 -0.0033643355 -0.0033611611 -0.00335553 -0.0033491254 -0.0033440997 -0.003340988 -0.0033393502 -0.0033387796 -0.0033388897][-0.0033451072 -0.0033439391 -0.0033475806 -0.0033517343 -0.0033555294 -0.0033577161 -0.0033579394 -0.0033555513 -0.0033514837 -0.0033469268 -0.0033431055 -0.0033406769 -0.0033393642 -0.0033389335 -0.0033390895][-0.0033442453 -0.0033420059 -0.0033442886 -0.003346767 -0.0033493817 -0.0033511342 -0.0033515028 -0.0033501668 -0.0033477359 -0.0033447738 -0.0033419821 -0.003340235 -0.003339289 -0.0033389204 -0.003338922][-0.0033436997 -0.0033406657 -0.0033418932 -0.0033431144 -0.0033447554 -0.0033462024 -0.0033468062 -0.0033461861 -0.0033448508 -0.0033430641 -0.0033411793 -0.00333997 -0.0033393234 -0.0033389102 -0.003338648][-0.0033437728 -0.0033399968 -0.0033405425 -0.0033409921 -0.0033419372 -0.0033430331 -0.0033436166 -0.0033434031 -0.0033428131 -0.0033418529 -0.0033405907 -0.0033396306 -0.0033390422 -0.0033384943 -0.0033378897][-0.0033439698 -0.003339773 -0.0033399849 -0.0033401095 -0.0033406559 -0.0033413928 -0.0033417409 -0.0033416182 -0.0033413619 -0.0033408504 -0.0033399558 -0.0033390915 -0.0033385125 -0.003337852 -0.003337071][-0.0033443784 -0.003339967 -0.0033398934 -0.0033397591 -0.0033400012 -0.0033404061 -0.0033405328 -0.0033404194 -0.0033402988 -0.0033399458 -0.0033392187 -0.0033384196 -0.0033377588 -0.0033370007 -0.0033361434]]...]
INFO - root - 2017-12-09 14:44:46.646164: step 33510, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 67h:20m:59s remains)
INFO - root - 2017-12-09 14:44:55.276875: step 33520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:36m:08s remains)
INFO - root - 2017-12-09 14:45:03.235966: step 33530, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.812 sec/batch; 67h:27m:38s remains)
INFO - root - 2017-12-09 14:45:11.707723: step 33540, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 69h:04m:50s remains)
INFO - root - 2017-12-09 14:45:20.175831: step 33550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:41m:41s remains)
INFO - root - 2017-12-09 14:45:28.796107: step 33560, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:03m:52s remains)
INFO - root - 2017-12-09 14:45:37.464518: step 33570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:31m:01s remains)
INFO - root - 2017-12-09 14:45:46.109824: step 33580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:17m:49s remains)
INFO - root - 2017-12-09 14:45:54.799902: step 33590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 69h:47m:36s remains)
INFO - root - 2017-12-09 14:46:03.143509: step 33600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:21m:51s remains)
2017-12-09 14:46:04.053432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034016476 -0.0034034483 -0.0034046567 -0.0034070769 -0.0034068746 -0.0033970806 -0.0033840227 -0.0033746911 -0.0033645288 -0.0033461214 -0.0033188597 -0.0032882737 -0.0032634339 -0.0032426282 -0.0032286611][-0.0034002853 -0.0034019477 -0.0034038397 -0.0034000149 -0.0033953066 -0.0033924589 -0.0033831517 -0.003360786 -0.0033290659 -0.0032904362 -0.0032600982 -0.0032394053 -0.0032198958 -0.0031998239 -0.0031831409][-0.0034004853 -0.0033994415 -0.0033962587 -0.0033901546 -0.0033834176 -0.00336716 -0.0033413272 -0.0033171908 -0.003292124 -0.0032475477 -0.0031931999 -0.0031629456 -0.0031629647 -0.0031690376 -0.0031637386][-0.0034014774 -0.0033999176 -0.0033952678 -0.0033819375 -0.0033515017 -0.0033116732 -0.0032710268 -0.0032441693 -0.0032233577 -0.003199406 -0.0031656029 -0.0031466191 -0.0031467993 -0.0031666036 -0.0031834517][-0.003400187 -0.0033953737 -0.0033878053 -0.0033682012 -0.0033163137 -0.0032431085 -0.0031559509 -0.0030986522 -0.0030706744 -0.0030706646 -0.0030731163 -0.003101578 -0.0031534659 -0.0032100915 -0.0032451209][-0.0033912156 -0.0033791706 -0.0033545566 -0.0033202688 -0.0032460617 -0.0031368379 -0.0030252044 -0.0029655055 -0.002953181 -0.0029759512 -0.0030032308 -0.0030554757 -0.0031403545 -0.0032305529 -0.0032969865][-0.0033742993 -0.00335535 -0.003311167 -0.0032435339 -0.0031375815 -0.0030183308 -0.0029027241 -0.0028488277 -0.0028650882 -0.0029246612 -0.0029960938 -0.0030745985 -0.003173175 -0.0032667876 -0.0033370529][-0.0033316717 -0.003290006 -0.0032254416 -0.0031398761 -0.0030206759 -0.0029075616 -0.0028105448 -0.0027863828 -0.0028421367 -0.0029606642 -0.0030830645 -0.003179444 -0.0032620258 -0.003325586 -0.0033712685][-0.0032367532 -0.0031531986 -0.0030622922 -0.0029751002 -0.0028861312 -0.0028270839 -0.0027758116 -0.0027843146 -0.0028655198 -0.0030195033 -0.0031806661 -0.0032866984 -0.0033462753 -0.0033725994 -0.0033902754][-0.0030742274 -0.0029395386 -0.0028297245 -0.002764937 -0.0027381007 -0.0027594827 -0.00277948 -0.002835382 -0.0029371441 -0.0030949828 -0.0032536169 -0.0033503161 -0.0033900752 -0.0033947206 -0.0033967439][-0.0028985417 -0.0027368045 -0.0026220451 -0.002583636 -0.0026205452 -0.0027229551 -0.0028191488 -0.0029217841 -0.0030315197 -0.0031696907 -0.0032955725 -0.0033666322 -0.0033919103 -0.0033933923 -0.0033939893][-0.0028021368 -0.0026273625 -0.0025153176 -0.0025025855 -0.002589751 -0.0027554794 -0.0029161025 -0.0030515003 -0.0031560084 -0.003258514 -0.0033391966 -0.0033792583 -0.0033906943 -0.0033908596 -0.0033912081][-0.0028457204 -0.0026754364 -0.0025709234 -0.0025664228 -0.0026727498 -0.0028549435 -0.0030351661 -0.0031764533 -0.0032662805 -0.0033326179 -0.003374211 -0.0033901313 -0.0033925353 -0.0033909525 -0.003390129][-0.0029470534 -0.0028200885 -0.0027496219 -0.0027584552 -0.002861554 -0.00302401 -0.003178691 -0.0032844045 -0.0033415661 -0.0033744993 -0.0033910305 -0.0033956994 -0.0033950978 -0.0033932214 -0.0033916172][-0.0031031654 -0.0030069877 -0.0029625827 -0.0029871939 -0.0030785662 -0.00319678 -0.0032940779 -0.0033524362 -0.0033798628 -0.0033921073 -0.0033957488 -0.0033962831 -0.0033958859 -0.0033949213 -0.0033937157]]...]
INFO - root - 2017-12-09 14:46:12.526620: step 33610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:50m:32s remains)
INFO - root - 2017-12-09 14:46:21.018554: step 33620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:51m:18s remains)
INFO - root - 2017-12-09 14:46:29.342235: step 33630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 71h:46m:18s remains)
INFO - root - 2017-12-09 14:46:37.984430: step 33640, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 71h:47m:21s remains)
INFO - root - 2017-12-09 14:46:46.679540: step 33650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:29m:19s remains)
INFO - root - 2017-12-09 14:46:55.281851: step 33660, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 71h:43m:51s remains)
INFO - root - 2017-12-09 14:47:03.731812: step 33670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:53m:43s remains)
INFO - root - 2017-12-09 14:47:12.097289: step 33680, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 69h:05m:38s remains)
INFO - root - 2017-12-09 14:47:20.581316: step 33690, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 66h:51m:12s remains)
INFO - root - 2017-12-09 14:47:29.212460: step 33700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:06m:39s remains)
2017-12-09 14:47:30.081813: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18729967 0.18197927 0.17515588 0.16720675 0.15889233 0.15062799 0.14310831 0.13571101 0.12856838 0.12127098 0.11423211 0.10754161 0.10086536 0.094450817 0.087843359][0.20784912 0.20359567 0.19663271 0.18820886 0.17901853 0.16937891 0.15996274 0.15045016 0.14102685 0.13206822 0.12372296 0.11593295 0.10827394 0.10112116 0.093867324][0.2244181 0.22230707 0.21645011 0.20785855 0.19802845 0.18726777 0.17630741 0.16485204 0.15325354 0.14230508 0.13199782 0.1225781 0.11359818 0.10536702 0.09752088][0.23522189 0.23585647 0.23233156 0.22516522 0.21580963 0.20472851 0.19291848 0.18017201 0.16682547 0.15406005 0.14179733 0.13058694 0.11999078 0.11032485 0.10140081][0.23906513 0.24267238 0.24156155 0.23671564 0.22888111 0.21851972 0.20695764 0.19393383 0.17977515 0.16567324 0.1521313 0.1395071 0.12746033 0.11616454 0.10595924][0.23603486 0.2427441 0.24451312 0.24194708 0.2360673 0.22770531 0.21789914 0.20594235 0.19224218 0.17822297 0.16400433 0.15014623 0.13631015 0.12318675 0.11115867][0.22755474 0.23689952 0.24098453 0.24093252 0.23740605 0.23120394 0.22362082 0.21423014 0.20270187 0.18984005 0.17588906 0.16149722 0.14628233 0.13112521 0.11681304][0.21537751 0.22721347 0.23375124 0.23580636 0.23459597 0.23083045 0.22584724 0.21897586 0.20988415 0.19925635 0.1865951 0.1722503 0.15631872 0.13943467 0.12279382][0.20077308 0.21409705 0.22241159 0.22632429 0.22715226 0.22566849 0.22332576 0.2191193 0.21220542 0.20362702 0.19284259 0.1793801 0.1633659 0.1454335 0.12725592][0.18270956 0.19710159 0.20663756 0.21249467 0.21540697 0.21616858 0.21605377 0.21387158 0.20923457 0.2026685 0.1932603 0.18075497 0.16504742 0.14702734 0.12835975][0.16231167 0.17684954 0.18714957 0.19458346 0.19955586 0.20246777 0.20433828 0.20392312 0.20095424 0.19593351 0.18776904 0.17622928 0.16111445 0.14365996 0.12552655][0.14120759 0.15453456 0.16433914 0.17232858 0.17861889 0.18319334 0.1867249 0.18800685 0.18698618 0.18332446 0.17633674 0.16592315 0.15218076 0.13620691 0.11946665][0.12219989 0.13375859 0.14256136 0.15023272 0.15671094 0.16176863 0.16589129 0.16773663 0.16742849 0.16493459 0.15948452 0.15063378 0.13877948 0.12514348 0.11107352][0.10533843 0.11453232 0.12152899 0.12819019 0.1340559 0.1387482 0.14257038 0.14460021 0.14483142 0.14293042 0.13869357 0.13184322 0.12257491 0.11198984 0.10110192][0.091075547 0.098356746 0.10358668 0.10905439 0.11409994 0.1178327 0.1207658 0.12231356 0.1226069 0.12111839 0.11792913 0.1130022 0.10637133 0.098949261 0.091386676]]...]
INFO - root - 2017-12-09 14:47:38.732157: step 33710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:39m:01s remains)
INFO - root - 2017-12-09 14:47:47.484724: step 33720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:36m:21s remains)
INFO - root - 2017-12-09 14:47:55.950057: step 33730, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 75h:09m:52s remains)
INFO - root - 2017-12-09 14:48:04.719120: step 33740, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 71h:12m:44s remains)
INFO - root - 2017-12-09 14:48:13.296600: step 33750, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 68h:54m:30s remains)
INFO - root - 2017-12-09 14:48:21.857890: step 33760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 71h:35m:02s remains)
INFO - root - 2017-12-09 14:48:30.479943: step 33770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:40m:38s remains)
INFO - root - 2017-12-09 14:48:39.282515: step 33780, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 71h:42m:50s remains)
INFO - root - 2017-12-09 14:48:47.931013: step 33790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:23m:48s remains)
INFO - root - 2017-12-09 14:48:56.376100: step 33800, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:32m:33s remains)
2017-12-09 14:48:57.224827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0024829952 -0.00025459914 0.0038216629 0.0091474662 0.014079371 0.01690807 0.016738184 0.013781329 0.009119316 0.0043658307 0.00063303928 -0.0017142948 -0.0028595044 -0.0032661424 -0.003355599][-0.0022539559 0.00081065693 0.0064994749 0.014116575 0.021178544 0.025053706 0.024550408 0.020156305 0.013558664 0.0070111016 0.0019750919 -0.0011475426 -0.0026718485 -0.0032221864 -0.0033475691][-0.0020786515 0.0017103429 0.008965333 0.018980233 0.028436713 0.033686202 0.033039574 0.027199127 0.018572565 0.010058388 0.0035508724 -0.00046604266 -0.0024361464 -0.0031634471 -0.003339502][-0.0019778814 0.0023863025 0.0112685 0.023882046 0.03597999 0.042850755 0.04225805 0.035066217 0.024379883 0.013755285 0.0055629797 0.00045103021 -0.0021061301 -0.0030784823 -0.0033289238][-0.0020721862 0.0028857219 0.013628268 0.029338196 0.044700157 0.053692736 0.053358026 0.0446687 0.031604648 0.018457891 0.0081928512 0.0016889728 -0.0016478383 -0.0029575727 -0.0033141572][-0.0020501325 0.0033660254 0.015825149 0.034584794 0.05323901 0.064605445 0.064938851 0.055125341 0.03982912 0.024048615 0.011454617 0.0032820932 -0.001038691 -0.0027923838 -0.0032924144][-0.0018383357 0.0037409656 0.017223604 0.0380939 0.059208263 0.072502688 0.073557034 0.063223772 0.046497773 0.028784778 0.014355996 0.0047872551 -0.000416863 -0.0026115174 -0.0032656523][-0.0013929021 0.0039357943 0.017351633 0.038677171 0.060572486 0.074666023 0.076211095 0.066037461 0.04905469 0.030754305 0.015672604 0.0055444054 -6.4941822e-05 -0.0024976793 -0.0032443893][-0.000875772 0.0038701931 0.016094452 0.03591723 0.056473497 0.069798253 0.071351647 0.061945904 0.046068419 0.028851286 0.014668145 0.0051271645 -0.00018901029 -0.0025215233 -0.0032441069][-0.00060197222 0.0032989567 0.013434377 0.030099649 0.047433775 0.058583211 0.059683923 0.051503409 0.03787037 0.023232775 0.011368707 0.0035009177 -0.0008253064 -0.0027011496 -0.0032719332][-0.00054655853 0.0023548908 0.0097354585 0.022014577 0.03485065 0.042973392 0.043457288 0.036932163 0.026412282 0.015405365 0.0067558633 0.0012068746 -0.0017339169 -0.0029559275 -0.0033089835][-0.0013079725 0.00068315607 0.0053838678 0.013184364 0.02135889 0.026418827 0.026444457 0.021866551 0.014799531 0.0076819146 0.0023201562 -0.00094181695 -0.0025593184 -0.0031798102 -0.0033426769][-0.0024501442 -0.0014019909 0.0010106473 0.0050432682 0.009313968 0.011950795 0.011871559 0.0093424451 0.0055486728 0.0018542323 -0.00082066609 -0.002358417 -0.0030615283 -0.0033054645 -0.0033613564][-0.003213967 -0.0028423013 -0.0019297795 -0.00038365554 0.0012871984 0.0023408628 0.0023064993 0.0013104312 -0.00015986455 -0.0015572864 -0.0025359951 -0.0030679198 -0.0032899012 -0.0033584812 -0.0033711626][-0.0033748902 -0.0033287322 -0.0031558489 -0.002830585 -0.0024473015 -0.0021918393 -0.0021888555 -0.0024067112 -0.0027277423 -0.0030261558 -0.0032268376 -0.0033264435 -0.0033624189 -0.0033719614 -0.0033736138]]...]
INFO - root - 2017-12-09 14:49:05.892028: step 33810, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 69h:18m:39s remains)
INFO - root - 2017-12-09 14:49:14.752910: step 33820, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 73h:32m:16s remains)
INFO - root - 2017-12-09 14:49:23.098403: step 33830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 71h:59m:33s remains)
INFO - root - 2017-12-09 14:49:31.660606: step 33840, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 71h:47m:54s remains)
INFO - root - 2017-12-09 14:49:40.374136: step 33850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:03m:29s remains)
INFO - root - 2017-12-09 14:49:48.940699: step 33860, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 68h:19m:51s remains)
INFO - root - 2017-12-09 14:49:57.311665: step 33870, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:06m:41s remains)
INFO - root - 2017-12-09 14:50:05.946243: step 33880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:33m:55s remains)
INFO - root - 2017-12-09 14:50:14.606533: step 33890, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 73h:54m:06s remains)
INFO - root - 2017-12-09 14:50:23.229758: step 33900, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.912 sec/batch; 75h:40m:25s remains)
2017-12-09 14:50:24.093880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034082201 -0.0034073638 -0.0033978168 -0.0033269841 -0.0030550528 -0.0024475562 -0.0016346406 -0.0010966081 -0.0011504025 -0.0017639557 -0.0025213244 -0.0030463452 -0.0032576041 -0.0033533394 -0.0033879553][-0.0034087694 -0.0034059484 -0.0033853061 -0.0032525209 -0.0027799029 -0.0017791064 -0.00049911905 0.00030663214 0.00013074651 -0.000925177 -0.0021672379 -0.0030174041 -0.0033314666 -0.0033944768 -0.0033998648][-0.0034092388 -0.0034035884 -0.0033639835 -0.0031345659 -0.0023707491 -0.00082185445 0.0010893629 0.0022699391 0.0019796141 0.00039733876 -0.001467101 -0.0027870489 -0.0033109733 -0.0033993255 -0.0034020161][-0.0034087976 -0.0034002129 -0.0033328158 -0.0029725281 -0.00184266 0.00035582855 0.0029921 0.00458024 0.0041401172 0.0019700285 -0.00060791778 -0.00248075 -0.0032746291 -0.0034007807 -0.0034033088][-0.003406689 -0.003394505 -0.0032899112 -0.0027607356 -0.0012001796 0.0016948595 0.0050513479 0.0070203613 0.0063934908 0.0035974793 0.00026129116 -0.0021665804 -0.0032391311 -0.0034012313 -0.003404049][-0.0033901064 -0.0033711814 -0.0032228867 -0.0024999166 -0.00048172125 0.0030820421 0.0070731021 0.0093268044 0.0084866695 0.0051158094 0.001060464 -0.0018858438 -0.0032102361 -0.0033993211 -0.0034032064][-0.0033461703 -0.0033102636 -0.0031131373 -0.0022026687 0.0002042132 0.0042535416 0.008638531 0.011015685 0.0099705327 0.0061559188 0.0015656715 -0.0017249422 -0.0032004265 -0.0033978897 -0.0034027938][-0.0032834827 -0.003217526 -0.0029713863 -0.0019322191 0.00067568012 0.0048882873 0.0093092788 0.011615077 0.010424571 0.0064182631 0.0016324106 -0.0017378259 -0.0032113774 -0.0033959213 -0.0034017588][-0.0032278714 -0.0031278729 -0.0028512378 -0.0017930196 0.00073989714 0.0046945321 0.0087312162 0.010744257 0.00950996 0.0056789541 0.0011803582 -0.0019222993 -0.00323675 -0.0033931572 -0.003399858][-0.0032090459 -0.0030886156 -0.0028160156 -0.0018809165 0.00027256715 0.0035761828 0.0068558059 0.00840459 0.0072353021 0.0039618108 0.00023173611 -0.0022667833 -0.0032770205 -0.0033906265 -0.0033974994][-0.0032291694 -0.0031117641 -0.0028799484 -0.0021812362 -0.00063537294 0.0017190515 0.0040393448 0.0050866851 0.0041220943 0.0016698954 -0.00099077215 -0.0026872957 -0.0033243666 -0.0033899171 -0.0033956086][-0.0032959229 -0.0032033613 -0.0030339761 -0.0026097703 -0.0017130753 -0.00033087377 0.0010455442 0.0016477555 0.00098214764 -0.0005659014 -0.0021334332 -0.0030523369 -0.0033599848 -0.0033905411 -0.0033949181][-0.003370343 -0.0033183608 -0.0032141763 -0.0030070415 -0.0026022084 -0.0019691526 -0.0013300497 -0.0010558565 -0.001410729 -0.0021795295 -0.0028948032 -0.0032726622 -0.0033784579 -0.0033910044 -0.0033949348][-0.0033931967 -0.0033794008 -0.0033391051 -0.0032574609 -0.0031192387 -0.0029109081 -0.002709181 -0.0026303474 -0.0027656485 -0.0030337889 -0.0032567203 -0.0033600708 -0.0033854998 -0.0033916882 -0.0033948265][-0.0033926812 -0.0033816448 -0.0033529962 -0.0033055563 -0.0032492618 -0.0031892019 -0.0031581458 -0.0031546017 -0.0031934895 -0.0032698452 -0.0033385993 -0.003373483 -0.00338679 -0.0033918957 -0.0033940824]]...]
INFO - root - 2017-12-09 14:50:32.720652: step 33910, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:41m:30s remains)
INFO - root - 2017-12-09 14:50:41.321702: step 33920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:51m:19s remains)
INFO - root - 2017-12-09 14:50:49.580086: step 33930, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 73h:20m:11s remains)
INFO - root - 2017-12-09 14:50:58.160540: step 33940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:40m:47s remains)
INFO - root - 2017-12-09 14:51:06.817868: step 33950, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 71h:39m:49s remains)
INFO - root - 2017-12-09 14:51:15.412059: step 33960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:38m:25s remains)
INFO - root - 2017-12-09 14:51:24.104969: step 33970, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 75h:22m:51s remains)
INFO - root - 2017-12-09 14:51:32.759905: step 33980, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:46m:32s remains)
INFO - root - 2017-12-09 14:51:41.538385: step 33990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:27m:51s remains)
INFO - root - 2017-12-09 14:51:50.000397: step 34000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:36m:20s remains)
2017-12-09 14:51:50.916889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033473037 -0.003344116 -0.0033439249 -0.0033441002 -0.0033441854 -0.0033442504 -0.0033441854 -0.0033439843 -0.003343801 -0.0033436904 -0.0033436662 -0.0033437745 -0.0033439633 -0.0033440869 -0.0033440213][-0.0033449323 -0.0033412913 -0.0033410559 -0.003341235 -0.0033412722 -0.0033412848 -0.0033411451 -0.0033408871 -0.003340611 -0.003340421 -0.0033403586 -0.0033404492 -0.0033406632 -0.0033408669 -0.0033409137][-0.00334471 -0.003340832 -0.0033404958 -0.0033405472 -0.0033404147 -0.003340251 -0.0033400122 -0.0033396564 -0.0033392892 -0.0033390853 -0.0033390264 -0.0033391768 -0.0033394732 -0.0033397581 -0.0033399321][-0.0033441554 -0.0033400645 -0.0033395533 -0.0033395337 -0.0033393269 -0.0033391323 -0.0033389146 -0.0033385763 -0.003338126 -0.0033378794 -0.0033378138 -0.0033379928 -0.0033383765 -0.0033386943 -0.0033389763][-0.0033437123 -0.0033393241 -0.0033386261 -0.0033384911 -0.0033382406 -0.0033380713 -0.0033379293 -0.0033376322 -0.0033370894 -0.0033367516 -0.0033366054 -0.0033367013 -0.0033370371 -0.0033373781 -0.0033377982][-0.0033435347 -0.0033388948 -0.0033380273 -0.0033377868 -0.0033374073 -0.0033371476 -0.0033369421 -0.0033365674 -0.0033359402 -0.0033355441 -0.003335336 -0.003335367 -0.0033357015 -0.0033360773 -0.0033365795][-0.0033433803 -0.0033386238 -0.003337644 -0.0033372687 -0.0033367644 -0.0033364315 -0.0033362166 -0.0033358531 -0.0033352987 -0.0033349956 -0.0033347937 -0.0033347406 -0.0033349986 -0.0033353339 -0.0033358571][-0.0033430429 -0.0033384115 -0.0033373944 -0.0033369809 -0.0033365262 -0.0033364038 -0.0033365048 -0.0033363532 -0.0033359623 -0.0033356675 -0.003335349 -0.0033350368 -0.0033350403 -0.003335254 -0.0033356762][-0.0033428178 -0.0033382822 -0.0033373581 -0.0033368771 -0.0033364538 -0.0033365788 -0.0033371132 -0.003337302 -0.0033371784 -0.0033369227 -0.0033365793 -0.0033360696 -0.0033357232 -0.0033356822 -0.0033358824][-0.0033428608 -0.0033383057 -0.0033375418 -0.0033370513 -0.0033367155 -0.0033369958 -0.0033377 -0.0033381006 -0.0033381695 -0.003337953 -0.0033375348 -0.0033368405 -0.0033362841 -0.0033361078 -0.0033361726][-0.0033429391 -0.0033383633 -0.0033378 -0.0033373153 -0.0033370219 -0.00333733 -0.0033380773 -0.0033385714 -0.0033388168 -0.0033388068 -0.0033384981 -0.003337827 -0.0033372154 -0.0033370077 -0.0033370175][-0.0033431309 -0.0033386413 -0.0033382403 -0.0033378217 -0.0033375935 -0.00333788 -0.0033385816 -0.0033391146 -0.0033394932 -0.0033396012 -0.0033394231 -0.0033388757 -0.0033382669 -0.0033379532 -0.0033378326][-0.00334352 -0.0033390622 -0.0033388196 -0.0033384969 -0.0033383542 -0.003338617 -0.0033391777 -0.0033396212 -0.0033399616 -0.0033400725 -0.0033399812 -0.0033396219 -0.0033391714 -0.0033388571 -0.0033386666][-0.0033440623 -0.0033395945 -0.0033394862 -0.0033392436 -0.0033391211 -0.0033392445 -0.0033395493 -0.0033397786 -0.0033399102 -0.0033399444 -0.0033398804 -0.0033397104 -0.0033395095 -0.0033393886 -0.0033393656][-0.0033445929 -0.0033401668 -0.0033400909 -0.0033399123 -0.0033398035 -0.003339818 -0.0033399051 -0.003339973 -0.0033400091 -0.0033400382 -0.0033399968 -0.0033399374 -0.0033398669 -0.0033398478 -0.003339868]]...]
INFO - root - 2017-12-09 14:51:59.533229: step 34010, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 70h:29m:41s remains)
INFO - root - 2017-12-09 14:52:08.191839: step 34020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:30m:11s remains)
INFO - root - 2017-12-09 14:52:16.678925: step 34030, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.927 sec/batch; 76h:51m:12s remains)
INFO - root - 2017-12-09 14:52:25.235346: step 34040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:11m:12s remains)
INFO - root - 2017-12-09 14:52:33.681146: step 34050, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:46m:53s remains)
INFO - root - 2017-12-09 14:52:42.376320: step 34060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:01m:29s remains)
INFO - root - 2017-12-09 14:52:51.024391: step 34070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 73h:27m:24s remains)
INFO - root - 2017-12-09 14:52:59.680067: step 34080, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 69h:51m:59s remains)
INFO - root - 2017-12-09 14:53:08.299799: step 34090, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:21m:48s remains)
INFO - root - 2017-12-09 14:53:16.904872: step 34100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:12m:20s remains)
2017-12-09 14:53:17.732249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033685574 -0.0033661888 -0.0033658922 -0.0033658338 -0.0033657609 -0.0033657409 -0.0033657374 -0.0033657639 -0.003365807 -0.0033657516 -0.0033657355 -0.0033656235 -0.0033655323 -0.0033655993 -0.0033656554][-0.0033668543 -0.003364278 -0.0033639986 -0.0033639763 -0.0033639309 -0.0033639139 -0.0033638906 -0.0033638896 -0.0033638922 -0.003363899 -0.003363932 -0.0033639355 -0.0033639697 -0.0033640508 -0.0033641716][-0.003366984 -0.0033643094 -0.0033640675 -0.0033640803 -0.0033640547 -0.0033639767 -0.0033638664 -0.0033637383 -0.003363614 -0.0033635558 -0.0033635264 -0.0033635735 -0.0033636834 -0.0033638312 -0.0033640668][-0.0033667684 -0.0033641013 -0.0033639711 -0.0033641481 -0.0033643048 -0.0033643525 -0.0033643139 -0.0033641802 -0.0033638978 -0.0033636063 -0.0033633241 -0.0033632275 -0.0033632363 -0.0033633474 -0.0033636298][-0.003366488 -0.0033637716 -0.0033637646 -0.00336415 -0.0033645504 -0.0033648387 -0.0033650084 -0.0033649839 -0.0033646051 -0.0033639804 -0.0033632747 -0.0033628051 -0.0033625639 -0.0033625851 -0.0033629504][-0.0033661078 -0.0033633718 -0.0033635516 -0.0033642706 -0.0033651348 -0.003365936 -0.0033665285 -0.0033667551 -0.0033662752 -0.0033652757 -0.0033639928 -0.0033628664 -0.0033621008 -0.0033618484 -0.0033622382][-0.0033656047 -0.0033629523 -0.0033634396 -0.0033646026 -0.003366041 -0.0033674617 -0.0033685232 -0.0033691074 -0.0033686415 -0.0033673763 -0.003365536 -0.0033636694 -0.0033622214 -0.0033615332 -0.0033617758][-0.0033651229 -0.003362604 -0.0033634093 -0.0033650405 -0.0033669956 -0.0033689744 -0.0033704578 -0.0033713013 -0.0033708524 -0.0033693642 -0.0033670943 -0.0033645451 -0.0033625313 -0.0033614442 -0.0033614472][-0.0033647646 -0.0033623585 -0.0033634512 -0.0033654796 -0.0033679463 -0.0033705842 -0.003372631 -0.0033737174 -0.0033732464 -0.0033714287 -0.0033687321 -0.0033655991 -0.0033630806 -0.0033616561 -0.0033614661][-0.0033648261 -0.0033624419 -0.0033637143 -0.0033658789 -0.0033686012 -0.0033716036 -0.0033739454 -0.0033750837 -0.0033745449 -0.0033725416 -0.0033696534 -0.0033662864 -0.0033635462 -0.0033619637 -0.0033615895][-0.0033649071 -0.0033625071 -0.0033638088 -0.0033658075 -0.0033683791 -0.0033712666 -0.0033735433 -0.0033746432 -0.0033741982 -0.003372438 -0.0033697798 -0.0033666338 -0.003364007 -0.0033624382 -0.0033618617][-0.0033649378 -0.0033623877 -0.0033634151 -0.0033649211 -0.003366885 -0.0033690818 -0.0033708592 -0.0033717351 -0.0033715114 -0.003370278 -0.0033683297 -0.003365987 -0.0033639139 -0.003362562 -0.0033618996][-0.0033648473 -0.0033620219 -0.0033626468 -0.0033634752 -0.0033646226 -0.0033658757 -0.0033668973 -0.0033673584 -0.0033672093 -0.0033664622 -0.0033653229 -0.0033639935 -0.0033627742 -0.0033619432 -0.0033614968][-0.0033647337 -0.0033616836 -0.0033619502 -0.0033621693 -0.0033625548 -0.0033629856 -0.0033633607 -0.0033635136 -0.0033634263 -0.0033631215 -0.0033626684 -0.0033621469 -0.0033616242 -0.0033612463 -0.0033610389][-0.0033644943 -0.0033613117 -0.0033613413 -0.0033613022 -0.0033613229 -0.0033613425 -0.0033613741 -0.0033613564 -0.0033612936 -0.003361217 -0.0033610982 -0.0033609783 -0.0033608417 -0.0033607262 -0.0033606675]]...]
INFO - root - 2017-12-09 14:53:26.350535: step 34110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:39m:55s remains)
INFO - root - 2017-12-09 14:53:34.957344: step 34120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:02m:25s remains)
INFO - root - 2017-12-09 14:53:43.301760: step 34130, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.843 sec/batch; 69h:53m:56s remains)
INFO - root - 2017-12-09 14:53:51.984616: step 34140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:38m:13s remains)
INFO - root - 2017-12-09 14:54:00.614920: step 34150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:16m:48s remains)
INFO - root - 2017-12-09 14:54:09.241875: step 34160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:55m:53s remains)
INFO - root - 2017-12-09 14:54:17.802153: step 34170, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:30m:28s remains)
INFO - root - 2017-12-09 14:54:26.309152: step 34180, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.828 sec/batch; 68h:34m:39s remains)
INFO - root - 2017-12-09 14:54:34.877980: step 34190, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:54m:53s remains)
INFO - root - 2017-12-09 14:54:43.470943: step 34200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:21m:37s remains)
2017-12-09 14:54:44.403811: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012661257 0.012194366 0.011606865 0.010993574 0.010411059 0.0098980879 0.0094401985 0.00907458 0.00877432 0.008511289 0.0082154041 0.0079111429 0.0075004669 0.0068748211 0.006017291][0.014592684 0.014466233 0.014129757 0.013666579 0.013045282 0.012322889 0.011548846 0.010866992 0.010326745 0.0099611366 0.0097148987 0.0095419725 0.0092710927 0.0087404139 0.0078951688][0.017754937 0.018196275 0.018259486 0.018002473 0.017351264 0.016401231 0.015267066 0.014216551 0.013413022 0.012923884 0.012644731 0.012492086 0.012245775 0.011718002 0.010775634][0.021761315 0.022839222 0.023357447 0.023343086 0.022738328 0.021652326 0.020307695 0.019055566 0.018136844 0.01761305 0.017276542 0.016971931 0.016481236 0.015669614 0.014379785][0.025390446 0.027047344 0.02796955 0.028198384 0.027767072 0.026758367 0.025459036 0.024268739 0.023442706 0.022981279 0.022570165 0.022007853 0.021082472 0.019754969 0.017934814][0.027811136 0.029911455 0.03115185 0.031608026 0.031364683 0.030504618 0.029367976 0.028346807 0.027691655 0.027315957 0.026867755 0.026062977 0.024711089 0.02286787 0.020546626][0.028817642 0.031054821 0.032356787 0.032910846 0.032773189 0.031966969 0.030868839 0.029922649 0.029407308 0.029168475 0.028835837 0.028060138 0.026592968 0.024529735 0.02197144][0.028552368 0.030711576 0.031843826 0.032239016 0.031925045 0.030916225 0.02964066 0.02860862 0.028185237 0.02821498 0.028232533 0.027804939 0.026608771 0.024722574 0.022300307][0.027198065 0.029137935 0.029927678 0.029934965 0.029190119 0.027745359 0.026117114 0.0249379 0.024620302 0.025059901 0.025662335 0.025884952 0.02527 0.023882702 0.021881374][0.025164029 0.026663257 0.026896469 0.026286999 0.024908073 0.022950115 0.020997627 0.019753512 0.019659422 0.020629764 0.021954384 0.022965467 0.023092559 0.022364223 0.020894963][0.022754261 0.023641849 0.023175951 0.021833681 0.019787082 0.017357474 0.01520281 0.014053513 0.014310769 0.015839767 0.017876478 0.019646544 0.020510072 0.020437259 0.019500932][0.020276714 0.020510893 0.019349633 0.017349089 0.014770222 0.012048952 0.0098514631 0.0088886144 0.0095001748 0.011457919 0.013992946 0.016311871 0.017775606 0.018241839 0.017778393][0.017831601 0.017553905 0.015873935 0.013434927 0.010579782 0.0078043379 0.0057211807 0.0049628345 0.0057795634 0.0079059778 0.01064063 0.013235406 0.015080092 0.015940974 0.015855365][0.015298924 0.014775886 0.012900007 0.010334873 0.0074870689 0.0048571676 0.0029933511 0.0024304821 0.0033173223 0.0053970553 0.0080626383 0.010669335 0.012645009 0.013704355 0.013844649][0.012696058 0.0121824 0.010427301 0.00805767 0.0055026235 0.0031902108 0.0016120591 0.0011918924 0.0020408358 0.0039300788 0.0063503748 0.0087671867 0.010666553 0.011749223 0.011952552]]...]
INFO - root - 2017-12-09 14:54:52.973622: step 34210, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 69h:21m:52s remains)
INFO - root - 2017-12-09 14:55:01.439010: step 34220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:47m:10s remains)
INFO - root - 2017-12-09 14:55:09.856690: step 34230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:59m:42s remains)
INFO - root - 2017-12-09 14:55:18.398891: step 34240, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 68h:51m:46s remains)
INFO - root - 2017-12-09 14:55:27.004819: step 34250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:55m:38s remains)
INFO - root - 2017-12-09 14:55:35.539972: step 34260, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 69h:22m:26s remains)
INFO - root - 2017-12-09 14:55:44.135644: step 34270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:13m:36s remains)
INFO - root - 2017-12-09 14:55:52.855245: step 34280, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 72h:31m:35s remains)
INFO - root - 2017-12-09 14:56:01.525778: step 34290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:45m:50s remains)
INFO - root - 2017-12-09 14:56:10.378741: step 34300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:14m:03s remains)
2017-12-09 14:56:11.209545: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022355597 0.023143502 0.023454878 0.023913382 0.024377849 0.024270037 0.02265738 0.020846449 0.019980684 0.02016167 0.021623062 0.024042221 0.025674922 0.028529743 0.033280075][0.02677393 0.025866047 0.024519818 0.025473671 0.026281813 0.026587946 0.026437836 0.026490759 0.027195936 0.028694717 0.031943969 0.036806174 0.041157342 0.047212549 0.055485755][0.03979969 0.037465826 0.034569472 0.034444295 0.034262598 0.036207393 0.039002754 0.040887408 0.043271042 0.047008552 0.052504897 0.058891892 0.065467596 0.074354455 0.084891237][0.060915627 0.058371186 0.055262864 0.054211833 0.053766266 0.055949368 0.058829814 0.061947942 0.065397739 0.070823304 0.07836362 0.087278292 0.096689887 0.10644816 0.11756138][0.085830368 0.083229333 0.080252707 0.079370394 0.0796913 0.081813619 0.084760666 0.087892935 0.091263816 0.097373165 0.1062329 0.11652339 0.12729456 0.13760631 0.14826742][0.10829708 0.10624841 0.10393041 0.10350796 0.10463257 0.10711836 0.11045688 0.1137372 0.11634272 0.12246273 0.13081121 0.14150262 0.15264137 0.16292894 0.17234743][0.12531802 0.1238806 0.12231907 0.12239634 0.12409834 0.12661296 0.12971702 0.13263258 0.13476579 0.14014119 0.14804824 0.15838161 0.16845965 0.17871964 0.18806434][0.13443825 0.13430449 0.13355021 0.13403681 0.13616426 0.13881458 0.14157569 0.14367558 0.14506643 0.1490864 0.15558848 0.16468766 0.17422286 0.18484266 0.19520032][0.13535517 0.13632926 0.13661487 0.13799034 0.14008985 0.14245616 0.14474876 0.145757 0.14538532 0.14687712 0.15133405 0.15883702 0.16767776 0.17866378 0.1904321][0.12675029 0.12854795 0.12924846 0.13090555 0.1328297 0.13499984 0.13664813 0.13587627 0.13347553 0.13216789 0.13421476 0.13965577 0.1478107 0.15928799 0.17234315][0.10883562 0.11013437 0.1100769 0.11080977 0.11150019 0.11248329 0.1128767 0.11104192 0.10744116 0.10459635 0.10492749 0.10898383 0.11668067 0.12787706 0.14138123][0.082844153 0.083114035 0.08222349 0.08193285 0.081477724 0.081192538 0.08033704 0.077724248 0.073914491 0.070998318 0.071125187 0.074628189 0.08202944 0.092437364 0.10525227][0.054321 0.053697757 0.052390866 0.051509358 0.050445519 0.049420893 0.047954984 0.045399189 0.042202443 0.040025346 0.040311985 0.043469347 0.050136186 0.059104525 0.070180021][0.029496921 0.028484257 0.027259082 0.026335407 0.025321499 0.024320828 0.023049941 0.021255502 0.019210771 0.018018603 0.018472942 0.020900862 0.025903033 0.032659851 0.041197076][0.012366328 0.011514684 0.010647704 0.00993045 0.0092138946 0.0085087735 0.0076718768 0.0066758692 0.0056787906 0.0052532395 0.0057383776 0.0073427334 0.010551092 0.014899336 0.02054319]]...]
INFO - root - 2017-12-09 14:56:19.602921: step 34310, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:12m:50s remains)
INFO - root - 2017-12-09 14:56:28.136932: step 34320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:58m:00s remains)
INFO - root - 2017-12-09 14:56:36.784118: step 34330, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 73h:17m:24s remains)
INFO - root - 2017-12-09 14:56:45.285456: step 34340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 71h:44m:46s remains)
INFO - root - 2017-12-09 14:56:53.928651: step 34350, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 69h:21m:01s remains)
INFO - root - 2017-12-09 14:57:02.479109: step 34360, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:53m:37s remains)
INFO - root - 2017-12-09 14:57:11.155564: step 34370, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:16m:32s remains)
INFO - root - 2017-12-09 14:57:19.864414: step 34380, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 71h:15m:09s remains)
INFO - root - 2017-12-09 14:57:28.667990: step 34390, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 74h:25m:12s remains)
INFO - root - 2017-12-09 14:57:37.423160: step 34400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:14m:31s remains)
2017-12-09 14:57:38.286462: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54723018 0.55128735 0.55769747 0.56592029 0.57426584 0.58059728 0.58222616 0.57984775 0.5712502 0.55763423 0.540587 0.524611 0.50798261 0.48759001 0.46511212][0.57079148 0.57660121 0.58490741 0.59452152 0.60449922 0.6125598 0.61578292 0.61481071 0.60663635 0.59397292 0.5767948 0.55874205 0.53952628 0.5167495 0.49130389][0.58136594 0.58761138 0.59462833 0.60346806 0.61330771 0.62139648 0.62560308 0.62735033 0.62239122 0.61137825 0.594197 0.57519543 0.55445945 0.53010595 0.50218815][0.58643579 0.59379023 0.59924144 0.60542989 0.61367893 0.62159312 0.62675411 0.63000548 0.627618 0.61829174 0.60129565 0.58059847 0.5572297 0.53093725 0.50119293][0.58788276 0.59593189 0.59956723 0.60357833 0.60957128 0.6154179 0.61966991 0.62380785 0.62296551 0.61432087 0.59688938 0.57483786 0.54873109 0.519672 0.48772132][0.58492732 0.59165627 0.59123486 0.59219223 0.5960747 0.60139084 0.60584629 0.61054856 0.61089742 0.60266662 0.5844205 0.55904061 0.52862215 0.49578235 0.46097624][0.58088803 0.58668286 0.58182287 0.57911694 0.57994956 0.58449423 0.58978379 0.595592 0.59708762 0.58833355 0.56802672 0.53837115 0.50218207 0.46287426 0.42351022][0.57035017 0.57744128 0.56978583 0.56488657 0.5638656 0.56743467 0.57182479 0.57747895 0.57933992 0.5695473 0.54698378 0.5123775 0.47000185 0.42380682 0.37930676][0.55440986 0.56360316 0.5562976 0.55074608 0.54911369 0.55290425 0.55729735 0.56124812 0.560493 0.548771 0.52340609 0.48480642 0.43691558 0.38470653 0.33577263][0.53524268 0.54525471 0.538784 0.533869 0.5325402 0.53624904 0.53957176 0.54274249 0.54002839 0.52694005 0.4995589 0.4580811 0.40712708 0.35109273 0.2990039][0.50871545 0.52047348 0.5156 0.5119195 0.51210505 0.51682591 0.52109408 0.52358067 0.51960868 0.50575227 0.47732061 0.43423247 0.38131788 0.32404888 0.27123377][0.47733307 0.49090675 0.48814917 0.4861165 0.48821706 0.49427798 0.49981466 0.50307387 0.49911907 0.4853287 0.4572641 0.41541004 0.36366472 0.30766588 0.25632608][0.44075212 0.45529208 0.45499513 0.455935 0.46127039 0.46988365 0.47758254 0.48127788 0.47727838 0.46411538 0.43747863 0.39827588 0.35021356 0.29924321 0.25313318][0.406358 0.41910279 0.4195863 0.42279395 0.43084165 0.44199815 0.45205322 0.457566 0.45496336 0.44287366 0.41855687 0.38396063 0.34206411 0.29839727 0.25956225][0.37195092 0.38325611 0.38354102 0.38765758 0.39673644 0.40898126 0.42013094 0.42680413 0.426115 0.41624486 0.3959091 0.36696258 0.3329041 0.29817149 0.26791272]]...]
INFO - root - 2017-12-09 14:57:46.830645: step 34410, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:35m:15s remains)
INFO - root - 2017-12-09 14:57:55.499239: step 34420, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 73h:53m:53s remains)
INFO - root - 2017-12-09 14:58:04.154048: step 34430, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:49m:38s remains)
INFO - root - 2017-12-09 14:58:12.853926: step 34440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:38m:15s remains)
INFO - root - 2017-12-09 14:58:21.703120: step 34450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:03m:06s remains)
INFO - root - 2017-12-09 14:58:30.389003: step 34460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 74h:04m:22s remains)
INFO - root - 2017-12-09 14:58:39.100718: step 34470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 69h:01m:33s remains)
INFO - root - 2017-12-09 14:58:47.836922: step 34480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:07m:27s remains)
INFO - root - 2017-12-09 14:58:56.550911: step 34490, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:37m:45s remains)
INFO - root - 2017-12-09 14:59:05.188276: step 34500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:20m:20s remains)
2017-12-09 14:59:06.122059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003377144 -0.0033755375 -0.0033755929 -0.0033757372 -0.0033758364 -0.0033758723 -0.0033759188 -0.0033759412 -0.0033758834 -0.0033756674 -0.0033754518 -0.0033752646 -0.0033750844 -0.0033749999 -0.0033749347][-0.003375883 -0.0033740769 -0.0033742595 -0.0033745281 -0.0033747747 -0.003375004 -0.0033751715 -0.0033751917 -0.0033750662 -0.0033748618 -0.0033746376 -0.0033743542 -0.0033740867 -0.0033739114 -0.0033737794][-0.0033768509 -0.0033752639 -0.0033758925 -0.0033764942 -0.0033769985 -0.0033774725 -0.0033777596 -0.0033777372 -0.0033774809 -0.0033770497 -0.003376456 -0.0033756916 -0.0033749938 -0.003374442 -0.0033739898][-0.0033784402 -0.0033773808 -0.0033785983 -0.0033798937 -0.0033811629 -0.0033823221 -0.0033830313 -0.0033831124 -0.0033826106 -0.0033816274 -0.0033801659 -0.0033784651 -0.003376913 -0.0033756346 -0.0033745994][-0.0033809559 -0.0033807007 -0.0033829643 -0.0033853878 -0.0033876791 -0.003389593 -0.0033906288 -0.0033906307 -0.0033897234 -0.0033880116 -0.0033855273 -0.0033827219 -0.0033801342 -0.0033778427 -0.0033759272][-0.0033840297 -0.0033846223 -0.0033878407 -0.0033911637 -0.0033941108 -0.0033963914 -0.0033974715 -0.003397306 -0.0033961271 -0.003394082 -0.0033909972 -0.0033874244 -0.0033839731 -0.0033807009 -0.0033778618][-0.0033867243 -0.0033880186 -0.0033919339 -0.0033959309 -0.0033994683 -0.0034021363 -0.003403441 -0.0034033251 -0.0034019658 -0.0033994948 -0.0033958268 -0.0033915518 -0.003387341 -0.0033833107 -0.0033798495][-0.0033883625 -0.0033900959 -0.0033943839 -0.0033987153 -0.0034026199 -0.003405615 -0.003407215 -0.003407428 -0.0034062825 -0.0034037344 -0.0033995963 -0.0033947048 -0.00338981 -0.0033851583 -0.0033812928][-0.0033880356 -0.003389911 -0.0033941655 -0.0033985006 -0.0034024022 -0.0034054527 -0.0034072865 -0.0034078956 -0.003407164 -0.0034048953 -0.0034008017 -0.0033957623 -0.0033906407 -0.0033857357 -0.0033818143][-0.0033863178 -0.0033875902 -0.0033910612 -0.0033945236 -0.0033976135 -0.0034000555 -0.0034016222 -0.0034023996 -0.0034022562 -0.0034007984 -0.0033976752 -0.0033935502 -0.0033890968 -0.0033846807 -0.0033811112][-0.0033834851 -0.0033838628 -0.0033863129 -0.0033886442 -0.003390732 -0.003392424 -0.0033936081 -0.0033943849 -0.0033946345 -0.0033939767 -0.0033920417 -0.0033892125 -0.0033858849 -0.0033824835 -0.0033796425][-0.0033803938 -0.0033799615 -0.0033815242 -0.0033830244 -0.0033845017 -0.0033857811 -0.0033867513 -0.0033875394 -0.003387948 -0.0033876519 -0.003386446 -0.0033845857 -0.0033823105 -0.0033798895 -0.0033778343][-0.0033773789 -0.0033760625 -0.0033770481 -0.0033779761 -0.0033788946 -0.0033797151 -0.0033802998 -0.0033808358 -0.0033811356 -0.0033810346 -0.003380402 -0.0033794469 -0.003378218 -0.0033768197 -0.0033755724][-0.0033747803 -0.0033726809 -0.0033731805 -0.0033736171 -0.0033740904 -0.0033745759 -0.0033749165 -0.0033751826 -0.0033752755 -0.0033751712 -0.00337488 -0.0033745186 -0.0033740366 -0.0033734962 -0.0033730457][-0.0033736378 -0.0033709954 -0.0033711456 -0.0033712091 -0.0033713132 -0.0033715088 -0.0033716506 -0.0033717242 -0.003371685 -0.00337159 -0.003371496 -0.0033714417 -0.0033713763 -0.0033713551 -0.0033714247]]...]
INFO - root - 2017-12-09 14:59:14.557035: step 34510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:16m:27s remains)
INFO - root - 2017-12-09 14:59:23.174476: step 34520, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 69h:59m:26s remains)
INFO - root - 2017-12-09 14:59:31.701130: step 34530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:09m:42s remains)
INFO - root - 2017-12-09 14:59:40.071518: step 34540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:35m:03s remains)
INFO - root - 2017-12-09 14:59:48.671519: step 34550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 71h:50m:24s remains)
INFO - root - 2017-12-09 14:59:57.316929: step 34560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:22m:56s remains)
INFO - root - 2017-12-09 15:00:06.081070: step 34570, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 74h:22m:00s remains)
INFO - root - 2017-12-09 15:00:14.770581: step 34580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:24m:35s remains)
INFO - root - 2017-12-09 15:00:23.499213: step 34590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:56m:56s remains)
INFO - root - 2017-12-09 15:00:32.328547: step 34600, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 74h:50m:22s remains)
2017-12-09 15:00:33.163208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003387572 -0.0033855035 -0.0033845208 -0.0033845084 -0.0033843676 -0.0033855678 -0.0033863054 -0.0033872693 -0.0033884908 -0.0033898328 -0.0033914191 -0.0033925849 -0.0033938331 -0.003394776 -0.0033959516][-0.0033905993 -0.0033884363 -0.0033875152 -0.0033875317 -0.0033869937 -0.0033874959 -0.0033870018 -0.003386792 -0.0033868416 -0.0033872116 -0.0033880379 -0.0033887327 -0.003389878 -0.0033906521 -0.0033920661][-0.0033939746 -0.0033919129 -0.0033904002 -0.0033894661 -0.003388135 -0.003388122 -0.003387158 -0.0033866793 -0.0033863259 -0.0033857427 -0.0033856567 -0.0033858046 -0.0033862765 -0.0033868216 -0.0033881618][-0.0033941306 -0.0033920202 -0.0033898142 -0.003388786 -0.0033871913 -0.0033868244 -0.0033854102 -0.0033848304 -0.0033845154 -0.003383443 -0.0033826516 -0.0033823308 -0.0033823522 -0.0033832914 -0.0033846092][-0.0033891022 -0.0033869657 -0.0033852088 -0.0033847778 -0.0033836295 -0.0033835561 -0.0033829061 -0.0033830013 -0.0033826332 -0.0033813431 -0.0033797587 -0.0033792376 -0.0033794502 -0.0033807741 -0.0033821862][-0.0033831277 -0.0033814262 -0.0033808257 -0.0033806164 -0.0033803817 -0.0033814567 -0.0033814702 -0.0033821259 -0.00338229 -0.0033814802 -0.0033802898 -0.0033802104 -0.0033807 -0.0033819654 -0.0033833641][-0.0033796479 -0.0033783161 -0.003378045 -0.0033781247 -0.003379174 -0.003381416 -0.0033827901 -0.0033846078 -0.0033855163 -0.0033859287 -0.0033856055 -0.0033855007 -0.0033859096 -0.0033867834 -0.0033871][-0.0033777924 -0.0033763044 -0.0033764718 -0.0033773812 -0.0033797342 -0.00338373 -0.0033869357 -0.003389715 -0.0033912628 -0.0033917802 -0.0033914989 -0.0033913855 -0.0033912673 -0.0033916004 -0.0033911252][-0.0033768958 -0.003375699 -0.0033771654 -0.0033792756 -0.0033827699 -0.0033878903 -0.0033919935 -0.00339518 -0.0033967053 -0.0033971688 -0.0033969039 -0.0033968191 -0.0033969963 -0.0033973448 -0.0033963036][-0.0033753298 -0.003374232 -0.0033767098 -0.0033797896 -0.0033841045 -0.0033897506 -0.00339414 -0.0033973993 -0.0033994855 -0.0034005453 -0.0034009244 -0.0034012755 -0.0034018098 -0.0034019749 -0.0034006997][-0.0033754986 -0.0033754897 -0.0033789668 -0.0033829482 -0.0033880752 -0.0033937192 -0.0033978703 -0.0033999793 -0.00340145 -0.0034021686 -0.0034027842 -0.0034027491 -0.0034034974 -0.0034040981 -0.0034034387][-0.0033762944 -0.0033771845 -0.0033810239 -0.0033861913 -0.0033922889 -0.0033981337 -0.0034018578 -0.0034035151 -0.0034038099 -0.0034037547 -0.0034038299 -0.0034033563 -0.0034036129 -0.0034047856 -0.0034054534][-0.0033775729 -0.0033795878 -0.0033843778 -0.0033911103 -0.003398177 -0.0034038294 -0.0034073226 -0.0034082129 -0.0034076395 -0.0034066003 -0.0034056967 -0.0034048294 -0.0034048827 -0.0034061091 -0.0034071004][-0.0033800716 -0.0033830255 -0.0033889597 -0.0033965004 -0.003403 -0.0034079126 -0.0034104935 -0.0034108565 -0.0034100485 -0.0034084611 -0.0034069768 -0.003405828 -0.003405466 -0.0034062318 -0.0034071472][-0.0033843254 -0.0033881334 -0.0033940284 -0.0034006017 -0.0034056129 -0.0034090527 -0.0034106274 -0.0034101945 -0.0034089363 -0.0034070676 -0.0034054269 -0.003404435 -0.00340396 -0.0034047908 -0.0034057803]]...]
INFO - root - 2017-12-09 15:00:41.775382: step 34610, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 73h:36m:59s remains)
INFO - root - 2017-12-09 15:00:50.419450: step 34620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:20m:03s remains)
INFO - root - 2017-12-09 15:00:58.865435: step 34630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:02m:59s remains)
INFO - root - 2017-12-09 15:01:07.376893: step 34640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 73h:07m:48s remains)
INFO - root - 2017-12-09 15:01:16.179757: step 34650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:10m:02s remains)
INFO - root - 2017-12-09 15:01:24.886588: step 34660, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 73h:26m:14s remains)
INFO - root - 2017-12-09 15:01:33.584790: step 34670, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 75h:41m:17s remains)
INFO - root - 2017-12-09 15:01:42.247768: step 34680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:31m:28s remains)
INFO - root - 2017-12-09 15:01:50.899404: step 34690, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:20m:52s remains)
INFO - root - 2017-12-09 15:01:59.589729: step 34700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:22m:00s remains)
2017-12-09 15:02:00.497250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00336146 -0.0033593017 -0.0033593292 -0.0033593248 -0.0033593392 -0.0033593609 -0.003359424 -0.0033591702 -0.0033588142 -0.0033585189 -0.0033582076 -0.0033576498 -0.0033571892 -0.0033568796 -0.0033566146][-0.0033599345 -0.0033577208 -0.0033576784 -0.0033576507 -0.0033576123 -0.0033576349 -0.0033577497 -0.0033575539 -0.0033572295 -0.0033568833 -0.0033565909 -0.0033561615 -0.0033558547 -0.0033556863 -0.003355606][-0.0033594565 -0.003357074 -0.0033568784 -0.0033567559 -0.003356616 -0.0033565729 -0.0033567238 -0.0033566372 -0.003356385 -0.0033561033 -0.003355884 -0.0033555191 -0.003355257 -0.0033553017 -0.0033554644][-0.0033592782 -0.0033567194 -0.0033562786 -0.0033559429 -0.0033556449 -0.0033554777 -0.0033554896 -0.0033554749 -0.0033553683 -0.0033552188 -0.0033550586 -0.0033548591 -0.0033547543 -0.0033548272 -0.0033550682][-0.0033589897 -0.0033562682 -0.003355677 -0.0033550239 -0.0033543909 -0.0033539794 -0.0033537513 -0.0033536495 -0.0033535541 -0.003353565 -0.0033536507 -0.0033537156 -0.0033538626 -0.0033539457 -0.0033541203][-0.0033584987 -0.0033557354 -0.0033551618 -0.003354389 -0.0033535617 -0.0033529724 -0.0033525662 -0.0033523326 -0.0033521424 -0.0033521822 -0.0033523608 -0.0033524611 -0.0033526276 -0.003352693 -0.0033527561][-0.0033583092 -0.0033554644 -0.0033551229 -0.0033545061 -0.0033537112 -0.0033530646 -0.0033525066 -0.0033521315 -0.0033518157 -0.0033518318 -0.0033518712 -0.0033518074 -0.0033517785 -0.0033515743 -0.0033514251][-0.0033586111 -0.0033558598 -0.003355884 -0.0033556216 -0.0033552065 -0.0033547725 -0.0033543417 -0.0033537482 -0.0033532311 -0.0033530886 -0.0033528851 -0.0033525478 -0.00335224 -0.0033517214 -0.0033513661][-0.0033603476 -0.0033578172 -0.0033580405 -0.0033579289 -0.0033579411 -0.0033579902 -0.0033579904 -0.0033575937 -0.0033571403 -0.0033571376 -0.0033568242 -0.0033563061 -0.003355813 -0.0033549988 -0.0033544118][-0.0033659695 -0.0033638165 -0.0033641965 -0.0033643085 -0.003364638 -0.0033649567 -0.0033651425 -0.0033648759 -0.0033643316 -0.0033640906 -0.00336359 -0.0033631506 -0.0033626813 -0.0033617842 -0.0033612465][-0.0033728078 -0.0033706711 -0.0033708347 -0.0033708995 -0.0033711605 -0.0033713921 -0.0033715165 -0.0033711696 -0.0033705058 -0.0033700264 -0.0033693931 -0.003368879 -0.00336837 -0.0033675442 -0.0033671013][-0.0033754064 -0.0033729856 -0.0033726275 -0.0033723484 -0.0033723712 -0.0033722559 -0.0033719502 -0.0033713679 -0.0033705751 -0.0033697875 -0.0033689376 -0.0033683951 -0.0033678995 -0.0033672093 -0.003366865][-0.0033734173 -0.0033701854 -0.0033691365 -0.0033685486 -0.0033682922 -0.0033677728 -0.0033669707 -0.0033661181 -0.0033651637 -0.0033641343 -0.0033631795 -0.0033628386 -0.0033626007 -0.0033623266 -0.0033623006][-0.003367743 -0.0033637502 -0.0033621248 -0.0033614445 -0.0033610051 -0.0033602871 -0.0033593618 -0.003358453 -0.0033574668 -0.0033563683 -0.0033553138 -0.0033550025 -0.0033547424 -0.0033544875 -0.0033542796][-0.003357257 -0.0033522476 -0.0033499186 -0.0033491568 -0.0033485652 -0.0033476849 -0.0033467668 -0.0033458045 -0.0033448332 -0.0033437086 -0.0033430522 -0.003342984 -0.0033428527 -0.0033429305 -0.0033429568]]...]
INFO - root - 2017-12-09 15:02:08.977093: step 34710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:51m:36s remains)
INFO - root - 2017-12-09 15:02:17.735046: step 34720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:26m:24s remains)
INFO - root - 2017-12-09 15:02:26.340339: step 34730, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 73h:59m:34s remains)
INFO - root - 2017-12-09 15:02:34.952369: step 34740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:02m:03s remains)
INFO - root - 2017-12-09 15:02:43.616169: step 34750, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 71h:53m:21s remains)
INFO - root - 2017-12-09 15:02:52.360581: step 34760, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 74h:58m:16s remains)
INFO - root - 2017-12-09 15:03:01.054693: step 34770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:48m:57s remains)
INFO - root - 2017-12-09 15:03:09.714829: step 34780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-09 15:03:18.409272: step 34790, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:12m:16s remains)
INFO - root - 2017-12-09 15:03:27.090966: step 34800, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 71h:49m:30s remains)
2017-12-09 15:03:27.978197: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088634662 0.10477774 0.11535056 0.12027175 0.12181218 0.12128045 0.12125323 0.12147973 0.12270051 0.12444192 0.12594007 0.12683427 0.125625 0.12191798 0.11324826][0.089266181 0.10635494 0.11827721 0.12560761 0.12954453 0.13139793 0.13310432 0.13463023 0.13630556 0.13760743 0.13791429 0.13739423 0.13431585 0.12839602 0.11835238][0.080559812 0.096089445 0.10883386 0.11870587 0.1261539 0.13226537 0.1371125 0.14010836 0.14205702 0.14267761 0.14158823 0.13933381 0.13462275 0.12756492 0.11719363][0.06737332 0.081239857 0.094253093 0.106408 0.11793634 0.12900111 0.13770506 0.1431513 0.14574088 0.1454467 0.14262505 0.13821721 0.13142605 0.12288861 0.11202314][0.050796922 0.063724577 0.077622823 0.092747107 0.1089556 0.1247403 0.1371129 0.14461002 0.14738382 0.14574547 0.14114515 0.13410667 0.12460276 0.11410914 0.10243171][0.033439305 0.044853833 0.05979367 0.078501917 0.099370167 0.11926287 0.13461308 0.14325455 0.14570378 0.14269669 0.13570802 0.12531053 0.11288399 0.10002612 0.087384284][0.01864689 0.028595529 0.043760922 0.064288005 0.0880273 0.11046486 0.12729163 0.13619691 0.13793421 0.13307151 0.12343982 0.11032332 0.095392711 0.080699138 0.067975067][0.0083401408 0.016602885 0.030943546 0.051467687 0.075212695 0.09742707 0.11371397 0.12219729 0.1229268 0.11645091 0.10451779 0.089041635 0.072538204 0.057312619 0.045657918][0.0032669222 0.0093977917 0.02144764 0.03941638 0.060237695 0.079630382 0.09367802 0.10070329 0.10010699 0.092585504 0.0797286 0.063896969 0.048054155 0.034417503 0.024846187][0.0014398498 0.0053790049 0.014130734 0.027690521 0.043716822 0.05883605 0.069626108 0.0744142 0.07238847 0.0644439 0.052311659 0.038626537 0.025906596 0.015778227 0.0092892665][0.00033903145 0.0024804831 0.0077649257 0.016564991 0.02746886 0.038034089 0.04543006 0.048064016 0.045130309 0.037851326 0.028094474 0.018186538 0.0098930858 0.0040871627 0.00075063994][-0.0010471991 1.2079021e-05 0.0027142102 0.0074899611 0.013860116 0.020299457 0.02472051 0.025788847 0.022976112 0.017515682 0.011083532 0.005363707 0.0011916487 -0.0013276795 -0.0025534248][-0.002316663 -0.0018557549 -0.00078715407 0.0012636264 0.0042748884 0.0074896258 0.0096926074 0.009969322 0.0080815442 0.0049521741 0.0017009056 -0.0007837899 -0.0022850041 -0.0030063577 -0.0032811633][-0.0032779863 -0.0030886021 -0.0027504154 -0.0021158382 -0.0011079116 2.9974151e-05 0.00082203187 0.00083796214 4.3850159e-05 -0.0011264731 -0.0021722647 -0.002850252 -0.0031904608 -0.0033207142 -0.0033567483][-0.0033780965 -0.0033735482 -0.003349625 -0.0032444168 -0.0030382415 -0.0027953042 -0.0026464162 -0.0026729337 -0.0028445674 -0.0030497704 -0.0032027168 -0.0032921082 -0.003329094 -0.0033428031 -0.0033476343]]...]
INFO - root - 2017-12-09 15:03:36.378241: step 34810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:03m:03s remains)
INFO - root - 2017-12-09 15:03:45.018752: step 34820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:15m:43s remains)
INFO - root - 2017-12-09 15:03:53.600814: step 34830, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 74h:32m:19s remains)
INFO - root - 2017-12-09 15:04:02.096044: step 34840, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 74h:31m:36s remains)
INFO - root - 2017-12-09 15:04:10.672953: step 34850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:11m:10s remains)
INFO - root - 2017-12-09 15:04:19.409123: step 34860, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 71h:19m:18s remains)
INFO - root - 2017-12-09 15:04:28.055626: step 34870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:32m:40s remains)
INFO - root - 2017-12-09 15:04:36.727099: step 34880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 73h:02m:36s remains)
INFO - root - 2017-12-09 15:04:45.465132: step 34890, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 74h:14m:38s remains)
INFO - root - 2017-12-09 15:04:54.188012: step 34900, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 71h:40m:15s remains)
2017-12-09 15:04:55.027449: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076434024 0.087997183 0.10486253 0.12837307 0.15777501 0.19173406 0.22420968 0.25114465 0.26941264 0.27882743 0.2795839 0.27512181 0.26694697 0.25610906 0.23930535][0.07520514 0.092306063 0.11549917 0.14506823 0.17894472 0.21538872 0.24859188 0.27541277 0.29307833 0.30113024 0.29984865 0.29265818 0.28122574 0.26724744 0.24754827][0.07288228 0.094579585 0.12327706 0.1581 0.1952057 0.23330484 0.26588553 0.29129428 0.30715591 0.314261 0.31202433 0.30323806 0.28935203 0.27208504 0.24964003][0.069722049 0.095046788 0.12764952 0.16664323 0.2059911 0.2439971 0.27544308 0.29900724 0.31265062 0.3189387 0.31676272 0.30839625 0.29422015 0.27492926 0.25016484][0.066916436 0.095005684 0.13036083 0.17147633 0.21151409 0.24872319 0.27792439 0.2992821 0.31095847 0.31678292 0.31454548 0.30753544 0.29450172 0.2754007 0.25040317][0.064087793 0.094245575 0.13114168 0.17298345 0.21245311 0.24703903 0.27353397 0.29294139 0.30301663 0.30874392 0.30729574 0.30154869 0.28979826 0.27215591 0.24856453][0.062696084 0.092982493 0.12964156 0.1700749 0.20766583 0.23967992 0.26362821 0.28136557 0.29065251 0.29639059 0.29575998 0.29158935 0.28145573 0.26556304 0.24392602][0.062516674 0.091968045 0.12630218 0.16350417 0.19717966 0.22551484 0.24717666 0.26388043 0.27321365 0.27945468 0.28013334 0.27779415 0.26939926 0.25600067 0.23761411][0.05953446 0.0880104 0.12047219 0.15366268 0.18331972 0.20727368 0.22632189 0.24227521 0.25252658 0.26041579 0.26242808 0.26124939 0.25441748 0.24385604 0.22904782][0.056666382 0.082771786 0.11189061 0.14107868 0.16675365 0.18771172 0.20461594 0.21964608 0.23103517 0.23992762 0.24330208 0.24330431 0.23781753 0.2299075 0.21857578][0.052386325 0.074737966 0.099292986 0.12361432 0.14521815 0.16323571 0.17884496 0.19392702 0.20651083 0.21643813 0.22203872 0.2236875 0.21996678 0.21486257 0.20646699][0.048610982 0.066791065 0.086293526 0.10508458 0.12190532 0.13726108 0.1517241 0.16665608 0.18029885 0.19187875 0.19955702 0.20313932 0.20219916 0.19961782 0.19415393][0.045542393 0.0598951 0.0746653 0.088599108 0.10107359 0.11294896 0.12518194 0.1389015 0.15231349 0.16447081 0.17365913 0.17959791 0.1815701 0.18242058 0.18055002][0.044750009 0.055693068 0.066274613 0.075431906 0.083663821 0.092334844 0.10196793 0.11369309 0.12558906 0.13768092 0.14785607 0.15524049 0.16060618 0.16452907 0.16605148][0.049839228 0.057901468 0.064469457 0.069499046 0.073947392 0.079322018 0.086050466 0.095109291 0.10452318 0.1142701 0.12377719 0.13205937 0.1394114 0.14587243 0.15031959]]...]
INFO - root - 2017-12-09 15:05:03.406118: step 34910, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 71h:17m:15s remains)
INFO - root - 2017-12-09 15:05:12.015928: step 34920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:22m:50s remains)
INFO - root - 2017-12-09 15:05:20.469730: step 34930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:49m:50s remains)
INFO - root - 2017-12-09 15:05:29.056488: step 34940, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 69h:48m:31s remains)
INFO - root - 2017-12-09 15:05:37.641225: step 34950, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:34m:19s remains)
INFO - root - 2017-12-09 15:05:46.285574: step 34960, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 73h:41m:51s remains)
INFO - root - 2017-12-09 15:05:54.976908: step 34970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:21m:55s remains)
INFO - root - 2017-12-09 15:06:03.594479: step 34980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:01m:22s remains)
INFO - root - 2017-12-09 15:06:12.246914: step 34990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:43m:03s remains)
INFO - root - 2017-12-09 15:06:20.950440: step 35000, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 74h:13m:32s remains)
2017-12-09 15:06:21.886353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033632964 -0.0033602181 -0.0033600649 -0.0033602153 -0.0033604116 -0.0033605285 -0.0033604128 -0.0033601979 -0.0033601017 -0.0033600924 -0.0033601199 -0.0033602405 -0.0033605408 -0.0033610819 -0.0033615343][-0.0033610975 -0.0033576512 -0.003357518 -0.0033577385 -0.0033578605 -0.0033577094 -0.00335722 -0.0033567452 -0.0033565545 -0.0033565043 -0.003356454 -0.0033566251 -0.0033569622 -0.0033574891 -0.0033580142][-0.0033615176 -0.0033580656 -0.0033580039 -0.0033581471 -0.0033580537 -0.0033575755 -0.0033568379 -0.0033561434 -0.0033557927 -0.0033557494 -0.0033557781 -0.0033559324 -0.0033562663 -0.0033566977 -0.0033572349][-0.0033616242 -0.0033581504 -0.0033580298 -0.0033581581 -0.0033580982 -0.0033576137 -0.0033568104 -0.0033560065 -0.0033556127 -0.003355385 -0.0033551271 -0.0033551482 -0.0033554463 -0.0033558512 -0.0033564847][-0.0033616733 -0.0033583075 -0.0033582461 -0.0033584279 -0.0033586684 -0.0033583289 -0.0033574491 -0.0033565785 -0.0033559634 -0.0033554186 -0.0033548116 -0.0033545876 -0.0033547881 -0.0033551538 -0.0033559224][-0.0033613639 -0.0033581068 -0.0033581823 -0.0033585813 -0.0033591033 -0.0033589795 -0.003358199 -0.0033572474 -0.0033564284 -0.003355833 -0.0033551566 -0.0033547138 -0.0033545219 -0.0033546465 -0.0033553266][-0.0033606584 -0.0033575972 -0.0033577918 -0.0033584214 -0.0033590975 -0.003359006 -0.0033583194 -0.0033574963 -0.0033568316 -0.003356352 -0.0033556162 -0.0033549359 -0.0033544984 -0.0033544186 -0.0033549804][-0.0033598109 -0.0033568733 -0.0033570337 -0.0033578523 -0.0033588342 -0.0033589085 -0.0033585601 -0.0033582528 -0.0033581303 -0.0033576472 -0.0033564849 -0.0033554356 -0.0033547543 -0.0033545094 -0.003354833][-0.0033590158 -0.0033560731 -0.0033563862 -0.0033573587 -0.0033585452 -0.0033591851 -0.0033596964 -0.0033600319 -0.0033598882 -0.0033590556 -0.003357525 -0.0033560453 -0.0033551638 -0.0033548872 -0.003355158][-0.0033582905 -0.0033551357 -0.0033558239 -0.0033571331 -0.0033585906 -0.0033598191 -0.0033608449 -0.0033612014 -0.0033608442 -0.0033599867 -0.003358372 -0.0033566719 -0.0033555892 -0.0033553482 -0.0033556523][-0.003357853 -0.0033545454 -0.0033554882 -0.0033570076 -0.0033584931 -0.0033597539 -0.0033607853 -0.0033610126 -0.0033603802 -0.0033593525 -0.0033580831 -0.0033568158 -0.0033559627 -0.0033558866 -0.003356257][-0.0033578125 -0.0033541978 -0.0033550705 -0.0033565178 -0.003357803 -0.0033586514 -0.0033593192 -0.0033594971 -0.0033588321 -0.0033578896 -0.0033570225 -0.0033564053 -0.0033561192 -0.0033563152 -0.0033568025][-0.0033580828 -0.0033541815 -0.0033547236 -0.0033557774 -0.003356637 -0.0033571175 -0.0033574791 -0.0033575008 -0.0033569692 -0.0033564486 -0.0033561597 -0.0033560416 -0.0033561822 -0.0033566204 -0.0033572414][-0.0033584842 -0.003354307 -0.0033546309 -0.0033552814 -0.0033557939 -0.0033560181 -0.0033560835 -0.0033560088 -0.0033557564 -0.0033556498 -0.0033557857 -0.0033560754 -0.0033564738 -0.0033570144 -0.0033576558][-0.0033590677 -0.0033548011 -0.003354853 -0.0033552367 -0.0033555483 -0.0033556197 -0.0033555666 -0.0033554451 -0.0033553487 -0.0033554435 -0.0033558202 -0.0033563331 -0.0033569119 -0.003357488 -0.0033580714]]...]
INFO - root - 2017-12-09 15:06:30.243310: step 35010, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.829 sec/batch; 68h:28m:40s remains)
INFO - root - 2017-12-09 15:06:38.673676: step 35020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:47m:43s remains)
INFO - root - 2017-12-09 15:06:47.144288: step 35030, loss = 0.88, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 72h:24m:00s remains)
INFO - root - 2017-12-09 15:06:55.547293: step 35040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:51m:00s remains)
INFO - root - 2017-12-09 15:07:04.227579: step 35050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:00m:10s remains)
INFO - root - 2017-12-09 15:07:12.860319: step 35060, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 71h:27m:49s remains)
INFO - root - 2017-12-09 15:07:21.660316: step 35070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:58m:47s remains)
INFO - root - 2017-12-09 15:07:30.177387: step 35080, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:26m:49s remains)
INFO - root - 2017-12-09 15:07:38.852699: step 35090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:13m:03s remains)
INFO - root - 2017-12-09 15:07:47.448579: step 35100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:45m:01s remains)
2017-12-09 15:07:48.246259: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19910145 0.21096098 0.21969658 0.22369897 0.22258407 0.21681315 0.20851503 0.1988977 0.1894598 0.18106042 0.17363638 0.16845085 0.16460994 0.15890363 0.14979459][0.19606134 0.21086678 0.220609 0.22540155 0.22409518 0.21676861 0.20667778 0.19487393 0.18389146 0.17415032 0.16593093 0.16123736 0.15859883 0.15501605 0.14823423][0.18959384 0.20643532 0.2185563 0.22465825 0.22318113 0.21503662 0.20384786 0.19056678 0.17813012 0.16724005 0.15884756 0.15383668 0.15174614 0.14950214 0.14422743][0.18381263 0.20360841 0.21851681 0.22751269 0.22734307 0.21920297 0.20714466 0.19364089 0.18059571 0.16902059 0.16104463 0.15620595 0.15432294 0.15154381 0.14567003][0.1768893 0.20002304 0.21901599 0.23162924 0.233837 0.22802393 0.21650967 0.20244594 0.18908121 0.17727794 0.16917959 0.16439614 0.1621708 0.15870178 0.15169813][0.17057435 0.19537406 0.21594481 0.23044869 0.23508361 0.23148511 0.2219668 0.20954597 0.1973442 0.18599492 0.17815398 0.17309944 0.16997366 0.16517894 0.15688436][0.16573888 0.19056222 0.21076488 0.22517662 0.23058227 0.22897072 0.22229782 0.21234091 0.20229448 0.19245131 0.18517959 0.18039215 0.17696886 0.17124146 0.16181523][0.15963371 0.18295956 0.20149648 0.2148506 0.22077268 0.22084698 0.21656878 0.20936827 0.20117846 0.19281331 0.18619567 0.18109228 0.17710797 0.17087731 0.16122676][0.14980938 0.17242189 0.19056019 0.20398025 0.21066558 0.21261315 0.21003543 0.20483735 0.19839361 0.19146569 0.18533975 0.18020263 0.17588277 0.16912074 0.15911587][0.14048176 0.16126062 0.17776644 0.19152074 0.19939594 0.20235185 0.20171395 0.19870465 0.19410668 0.1881015 0.1824667 0.17746678 0.1725716 0.16543113 0.15526181][0.13013151 0.14952876 0.16398002 0.17638174 0.18393528 0.18720004 0.1882093 0.18703681 0.1845101 0.18026336 0.17573018 0.17124096 0.16582735 0.15833177 0.14794978][0.11798734 0.13516843 0.14772518 0.15871023 0.16564068 0.16850905 0.16991833 0.17009047 0.16901912 0.1660672 0.16257036 0.1586007 0.15336846 0.14619444 0.13663037][0.10495331 0.11846879 0.12787864 0.13637444 0.14215179 0.14447419 0.14543065 0.14570543 0.14508288 0.14291301 0.14019246 0.1372858 0.13320026 0.12757528 0.12005252][0.089837328 0.099787883 0.10607937 0.11190673 0.11601498 0.11773838 0.11852277 0.11889622 0.11864316 0.11732282 0.11559276 0.11386407 0.11130162 0.10769507 0.10281163][0.082967423 0.09015727 0.093517676 0.09678106 0.099299662 0.10032774 0.10074624 0.10109969 0.10118215 0.10044819 0.099492915 0.098758891 0.097455591 0.095630288 0.09305241]]...]
INFO - root - 2017-12-09 15:07:56.756068: step 35110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:37m:25s remains)
INFO - root - 2017-12-09 15:08:05.390627: step 35120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:46m:16s remains)
INFO - root - 2017-12-09 15:08:13.915224: step 35130, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 68h:39m:17s remains)
INFO - root - 2017-12-09 15:08:22.474332: step 35140, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 71h:27m:58s remains)
INFO - root - 2017-12-09 15:08:31.096068: step 35150, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:01m:36s remains)
INFO - root - 2017-12-09 15:08:39.530180: step 35160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:48m:20s remains)
INFO - root - 2017-12-09 15:08:47.946693: step 35170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:46m:00s remains)
INFO - root - 2017-12-09 15:08:56.651108: step 35180, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 71h:49m:45s remains)
INFO - root - 2017-12-09 15:09:05.316530: step 35190, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 74h:21m:18s remains)
INFO - root - 2017-12-09 15:09:14.042203: step 35200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:46m:45s remains)
2017-12-09 15:09:14.877289: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0052111093 0.00480711 0.0049061077 0.0054926248 0.0078906408 0.011412105 0.015663261 0.019133419 0.020983707 0.021120468 0.019612059 0.017390138 0.015001108 0.013001342 0.011090716][0.0088098748 0.0084235556 0.0087783365 0.010145861 0.013740496 0.018963834 0.025125802 0.030577304 0.033734284 0.034761384 0.033223625 0.030285804 0.026763352 0.023429317 0.02059239][0.013094571 0.012834308 0.013751912 0.016273906 0.021659875 0.029303713 0.037911419 0.045634273 0.050242465 0.052252911 0.0506722 0.046990961 0.04241138 0.038016167 0.034254014][0.015457905 0.015849762 0.018440265 0.023704026 0.032546353 0.043542691 0.054958377 0.064449929 0.070010215 0.072279625 0.07030271 0.065897807 0.060156826 0.054372773 0.049004864][0.016361762 0.017543381 0.022207279 0.030891793 0.04367369 0.058303554 0.072193086 0.083204336 0.089262769 0.091426037 0.088916734 0.083907425 0.077084832 0.069697343 0.06229334][0.016549394 0.019067286 0.026270816 0.038618449 0.055128228 0.072853826 0.088441141 0.099775448 0.10536376 0.10642856 0.10272618 0.097005047 0.08959686 0.08111643 0.071910955][0.016897231 0.020883955 0.030834546 0.046810322 0.0665626 0.086152643 0.10198876 0.11231109 0.11630316 0.11569756 0.11070246 0.10432768 0.096503749 0.087290816 0.076505333][0.017443264 0.022992983 0.035390981 0.054209005 0.076423183 0.096635267 0.11127602 0.11948309 0.12128411 0.11847262 0.11214448 0.10564949 0.098214991 0.089503989 0.078262426][0.017932685 0.025469478 0.040300928 0.061347913 0.08441183 0.10373084 0.11606706 0.12104268 0.11965425 0.11462742 0.10727363 0.10083904 0.094342068 0.087414362 0.0778282][0.02104665 0.029874815 0.045966905 0.067853138 0.090516344 0.10792807 0.11714698 0.11868522 0.11396516 0.10643794 0.097891495 0.091731027 0.087118745 0.083167 0.076855123][0.026696378 0.036493037 0.053156666 0.074753322 0.095846005 0.11041687 0.11581511 0.1132539 0.1048362 0.095136069 0.085879333 0.080677591 0.078656927 0.078881659 0.077317528][0.03514399 0.04485495 0.060435031 0.079991177 0.098198906 0.10923785 0.11096393 0.1050453 0.093734063 0.082593627 0.073176235 0.069291651 0.070167065 0.074909858 0.078632578][0.043945536 0.052028272 0.06457407 0.080247007 0.094609216 0.10225774 0.10132793 0.093372434 0.080776796 0.069745585 0.061258834 0.059624735 0.063519694 0.072228231 0.08045698][0.049281862 0.054302555 0.063159838 0.074838176 0.085395776 0.090238884 0.08778087 0.079269111 0.067164995 0.057593562 0.051298372 0.052520271 0.059455831 0.071228892 0.082604147][0.050050296 0.051477734 0.056579333 0.0647154 0.07269714 0.076144509 0.07359051 0.066219531 0.056082897 0.048754614 0.044854332 0.04867281 0.057772119 0.07127358 0.083873726]]...]
INFO - root - 2017-12-09 15:09:23.365672: step 35210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 73h:47m:22s remains)
INFO - root - 2017-12-09 15:09:31.978138: step 35220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:17m:38s remains)
INFO - root - 2017-12-09 15:09:40.587566: step 35230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:16m:19s remains)
INFO - root - 2017-12-09 15:09:49.033078: step 35240, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 71h:24m:17s remains)
INFO - root - 2017-12-09 15:09:57.420160: step 35250, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 69h:50m:58s remains)
INFO - root - 2017-12-09 15:10:05.900669: step 35260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:40m:39s remains)
INFO - root - 2017-12-09 15:10:14.462539: step 35270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:22m:48s remains)
INFO - root - 2017-12-09 15:10:23.068022: step 35280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 71h:08m:17s remains)
INFO - root - 2017-12-09 15:10:31.690651: step 35290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:14m:28s remains)
INFO - root - 2017-12-09 15:10:40.352464: step 35300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:01m:35s remains)
2017-12-09 15:10:41.206256: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.56558651 0.5599606 0.55407983 0.54792643 0.54179364 0.53505147 0.52912176 0.52380413 0.51961821 0.51503128 0.50989556 0.50467616 0.49647853 0.48274451 0.46312454][0.59971416 0.60007185 0.598121 0.59600675 0.592326 0.58789456 0.58319014 0.57730174 0.57243806 0.56561339 0.55764443 0.54779881 0.53467989 0.51635522 0.49282503][0.61697894 0.62403083 0.62684822 0.62915 0.62796819 0.62454122 0.61935931 0.61316788 0.60678363 0.59736228 0.58626652 0.57240194 0.55545491 0.53337413 0.506276][0.61944675 0.63438123 0.64382678 0.65057784 0.65261686 0.65161628 0.64576834 0.63756746 0.62830937 0.61661077 0.60176277 0.58394426 0.56346881 0.53880703 0.50957686][0.60901004 0.63080132 0.64649862 0.65891147 0.66478366 0.6658597 0.66113836 0.65187383 0.63964236 0.624165 0.60586613 0.58430278 0.56036425 0.53338623 0.5028429][0.58800596 0.61557114 0.63502908 0.6512714 0.6606043 0.663413 0.66010582 0.65061766 0.63667935 0.61811137 0.59669173 0.57289684 0.54674459 0.51855356 0.48847473][0.55477709 0.58567995 0.60633087 0.62388325 0.6344384 0.63880938 0.63647497 0.62788957 0.61394978 0.59464359 0.5726378 0.5485974 0.52308673 0.49603921 0.46780586][0.51573008 0.54803342 0.56960195 0.58708483 0.5979929 0.60247171 0.60011137 0.59150153 0.57641119 0.55732268 0.536032 0.51370925 0.49087265 0.46717948 0.44282496][0.47358224 0.50501388 0.52506256 0.54084831 0.55151421 0.55673939 0.55588835 0.54841775 0.53389996 0.51625621 0.49690777 0.47772729 0.45842451 0.43920356 0.41976997][0.43669364 0.46471751 0.48043454 0.49212158 0.50046533 0.50503659 0.5053 0.50020266 0.48912236 0.47499681 0.45999283 0.44552928 0.4308129 0.4164511 0.40181518][0.40228647 0.42640984 0.43699896 0.443134 0.44749951 0.45011792 0.44936574 0.44572702 0.43837437 0.4305515 0.4224534 0.41480455 0.40674448 0.39809456 0.38840076][0.373632 0.39283139 0.3976725 0.39836577 0.39779145 0.39755961 0.395702 0.39253175 0.38877207 0.38708532 0.38654342 0.3862617 0.38522148 0.38263631 0.37740111][0.35310125 0.36689174 0.36642808 0.36250553 0.35809356 0.35499185 0.35177726 0.34857279 0.34724998 0.35007769 0.35525748 0.36090147 0.36541247 0.367452 0.36566105][0.33517927 0.34322193 0.33750448 0.32991353 0.32255653 0.31749132 0.31373698 0.31149009 0.31283426 0.31908363 0.32859293 0.33877164 0.34751502 0.35302979 0.35381249][0.31712785 0.32156795 0.31221557 0.30171272 0.29219878 0.28535122 0.28095889 0.27929625 0.28266808 0.29096735 0.30301914 0.31592906 0.32732537 0.33515075 0.33783787]]...]
INFO - root - 2017-12-09 15:10:49.713874: step 35310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:49m:04s remains)
INFO - root - 2017-12-09 15:10:58.551704: step 35320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:29m:19s remains)
INFO - root - 2017-12-09 15:11:07.091847: step 35330, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 69h:25m:30s remains)
INFO - root - 2017-12-09 15:11:15.644489: step 35340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 72h:17m:12s remains)
INFO - root - 2017-12-09 15:11:24.214547: step 35350, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 71h:26m:31s remains)
INFO - root - 2017-12-09 15:11:32.616590: step 35360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:48m:16s remains)
INFO - root - 2017-12-09 15:11:41.162909: step 35370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:22m:18s remains)
INFO - root - 2017-12-09 15:11:49.641080: step 35380, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:17m:58s remains)
INFO - root - 2017-12-09 15:11:58.116438: step 35390, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 69h:16m:44s remains)
INFO - root - 2017-12-09 15:12:06.764142: step 35400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:30m:30s remains)
2017-12-09 15:12:07.589514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033813731 -0.0033784208 -0.0033780795 -0.0033783852 -0.0033788818 -0.0033798388 -0.0033812579 -0.0033830036 -0.0033846747 -0.0033858642 -0.0033864421 -0.0033862214 -0.0033851538 -0.0033837848 -0.0033817862][-0.0033795089 -0.0033762231 -0.0033758495 -0.0033762963 -0.00337694 -0.0033782579 -0.0033800611 -0.0033821375 -0.0033843506 -0.0033861089 -0.0033875259 -0.003388061 -0.0033875015 -0.0033859257 -0.0033832064][-0.0033796071 -0.0033764413 -0.0033760492 -0.0033763759 -0.0033771582 -0.003378652 -0.0033806062 -0.003382978 -0.0033858786 -0.0033884917 -0.0033909965 -0.0033925227 -0.0033927821 -0.003391159 -0.0033879203][-0.0033799906 -0.0033769123 -0.0033764909 -0.0033769398 -0.0033777857 -0.0033797137 -0.0033822556 -0.0033851292 -0.0033885494 -0.003392349 -0.0033962058 -0.0033985 -0.0033986287 -0.0033967807 -0.0033932533][-0.0033804602 -0.0033776143 -0.0033774595 -0.003378476 -0.0033798295 -0.0033821866 -0.0033853413 -0.0033884968 -0.0033926035 -0.0033975656 -0.0034022387 -0.0034047447 -0.0034048103 -0.0034032005 -0.0034000843][-0.0033810313 -0.0033787645 -0.0033791102 -0.0033807755 -0.0033828849 -0.0033858321 -0.0033893271 -0.0033927972 -0.0033976762 -0.0034027996 -0.0034071696 -0.003409382 -0.0034094581 -0.003408253 -0.003405852][-0.003381456 -0.0033797957 -0.0033805221 -0.0033829582 -0.0033858407 -0.0033890705 -0.0033928184 -0.0033973488 -0.0034020892 -0.0034064788 -0.0034097033 -0.0034110493 -0.0034112015 -0.0034102383 -0.0034088693][-0.0033816104 -0.003380456 -0.00338169 -0.003384785 -0.0033879098 -0.0033918673 -0.0033966582 -0.0034015758 -0.0034056411 -0.0034078644 -0.0034097966 -0.0034102576 -0.0034101491 -0.0034098888 -0.0034096029][-0.0033814344 -0.0033806164 -0.0033821741 -0.0033856113 -0.0033893057 -0.0033945863 -0.0034001777 -0.0034044918 -0.0034065999 -0.0034069221 -0.0034073079 -0.0034067419 -0.0034064406 -0.0034069784 -0.0034083191][-0.0033811997 -0.0033801538 -0.0033820905 -0.0033856747 -0.00339044 -0.0033965751 -0.0034020992 -0.0034050844 -0.0034055749 -0.0034042625 -0.0034028457 -0.0034014888 -0.0034011351 -0.0034023477 -0.0034052886][-0.0033803645 -0.0033790795 -0.0033813533 -0.0033851941 -0.0033900919 -0.0033958165 -0.0034004694 -0.0034025188 -0.00340198 -0.0033994538 -0.0033970545 -0.0033954198 -0.0033953944 -0.0033973353 -0.0034006955][-0.003379619 -0.0033780707 -0.0033803741 -0.0033839426 -0.0033881729 -0.00339286 -0.0033963069 -0.0033974373 -0.0033961758 -0.0033933774 -0.003391105 -0.003389803 -0.0033901504 -0.0033923073 -0.0033959316][-0.0033793191 -0.0033773687 -0.0033793868 -0.0033820011 -0.0033850449 -0.0033883655 -0.0033905082 -0.003390911 -0.0033897019 -0.003387524 -0.0033858037 -0.0033849052 -0.0033854789 -0.0033876165 -0.0033910673][-0.0033792108 -0.0033766176 -0.0033779489 -0.003379469 -0.00338134 -0.0033833687 -0.0033845115 -0.0033848034 -0.0033839575 -0.0033827014 -0.0033817482 -0.0033813503 -0.0033820202 -0.0033836633 -0.0033864768][-0.0033790751 -0.0033758311 -0.0033762457 -0.0033768846 -0.0033777272 -0.0033786634 -0.0033793473 -0.0033796532 -0.0033795731 -0.003379229 -0.0033789061 -0.0033788506 -0.0033793631 -0.0033805473 -0.0033826274]]...]
INFO - root - 2017-12-09 15:12:16.409852: step 35410, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:23m:27s remains)
INFO - root - 2017-12-09 15:12:24.976211: step 35420, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 71h:15m:03s remains)
INFO - root - 2017-12-09 15:12:33.474075: step 35430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:15m:43s remains)
INFO - root - 2017-12-09 15:12:42.031763: step 35440, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:44m:37s remains)
INFO - root - 2017-12-09 15:12:50.678932: step 35450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:57m:30s remains)
INFO - root - 2017-12-09 15:12:59.327273: step 35460, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 70h:26m:09s remains)
INFO - root - 2017-12-09 15:13:07.961012: step 35470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:31m:39s remains)
INFO - root - 2017-12-09 15:13:16.560002: step 35480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 72h:38m:10s remains)
INFO - root - 2017-12-09 15:13:25.265266: step 35490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:47m:58s remains)
INFO - root - 2017-12-09 15:13:33.858603: step 35500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:32m:40s remains)
2017-12-09 15:13:34.704149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033940389 -0.0033919655 -0.0033916107 -0.0033913441 -0.0033911653 -0.0033912836 -0.0033916177 -0.0033919569 -0.0033922154 -0.0033923532 -0.0033923904 -0.0033924305 -0.0033924079 -0.0033923131 -0.0033921748][-0.0033928072 -0.0033903511 -0.003389704 -0.0033892742 -0.003389067 -0.0033892139 -0.0033894577 -0.0033898726 -0.0033902626 -0.0033904957 -0.003390721 -0.0033909508 -0.0033910261 -0.0033909376 -0.0033908417][-0.0033930261 -0.003390166 -0.0033892596 -0.003388742 -0.0033884668 -0.003388497 -0.0033885879 -0.0033889995 -0.0033895541 -0.0033899357 -0.0033902191 -0.0033903692 -0.0033904379 -0.00339043 -0.0033904144][-0.0033929755 -0.0033899678 -0.0033890679 -0.0033884814 -0.0033881157 -0.0033880617 -0.0033880295 -0.003388417 -0.0033890852 -0.0033893969 -0.0033895473 -0.0033896703 -0.00338981 -0.0033899278 -0.0033897837][-0.0033930612 -0.0033900451 -0.0033892258 -0.0033886007 -0.0033881448 -0.0033879629 -0.003387932 -0.0033882624 -0.0033886069 -0.0033886984 -0.0033889997 -0.0033895886 -0.0033899238 -0.0033897334 -0.0033892218][-0.0033933152 -0.0033904226 -0.003389539 -0.0033888042 -0.0033883585 -0.003388362 -0.003388444 -0.003388559 -0.0033885578 -0.0033890845 -0.0033905602 -0.0033913718 -0.00339148 -0.003390583 -0.0033895127][-0.003393424 -0.0033906165 -0.0033898018 -0.0033892815 -0.0033892156 -0.0033894044 -0.003389599 -0.0033895313 -0.0033898428 -0.00339174 -0.0033938887 -0.0033946314 -0.003394054 -0.0033925264 -0.0033906919][-0.0033933467 -0.0033906447 -0.0033901974 -0.0033901057 -0.0033903944 -0.0033908838 -0.0033914652 -0.0033919914 -0.0033934896 -0.0033960387 -0.0033981251 -0.0033984496 -0.0033974303 -0.0033951032 -0.0033923576][-0.0033932813 -0.0033909171 -0.0033907301 -0.0033909553 -0.0033915646 -0.0033926021 -0.0033939162 -0.0033954789 -0.0033977479 -0.003400459 -0.0034023563 -0.0034023919 -0.0034008359 -0.0033976324 -0.0033941027][-0.0033932924 -0.0033911634 -0.0033912042 -0.0033915893 -0.003392485 -0.0033940002 -0.0033959723 -0.0033984191 -0.0034011216 -0.0034038534 -0.0034057125 -0.0034053994 -0.003403089 -0.0033992745 -0.0033955057][-0.0033931369 -0.0033912321 -0.0033915725 -0.003392033 -0.0033930968 -0.0033947211 -0.0033968824 -0.0033995532 -0.0034021763 -0.0034045849 -0.00340616 -0.0034056231 -0.0034030471 -0.0033993919 -0.0033958554][-0.0033930598 -0.003391379 -0.0033917616 -0.0033922051 -0.0033931625 -0.0033945814 -0.0033965167 -0.0033989083 -0.0034011763 -0.0034031086 -0.0034041828 -0.0034036229 -0.003401425 -0.0033983556 -0.0033954245][-0.0033931588 -0.0033913907 -0.0033916726 -0.0033920279 -0.0033927532 -0.0033937939 -0.0033951416 -0.0033969032 -0.0033986175 -0.00339991 -0.0034005167 -0.0034001614 -0.0033987511 -0.0033965369 -0.0033944692][-0.0033932682 -0.0033913301 -0.0033915157 -0.0033917485 -0.0033922088 -0.0033928556 -0.0033937073 -0.0033947169 -0.0033958035 -0.0033966149 -0.0033969113 -0.0033966422 -0.0033957637 -0.00339449 -0.0033932244][-0.0033933686 -0.0033912661 -0.0033913532 -0.003391441 -0.0033916165 -0.0033919208 -0.0033923481 -0.0033927844 -0.0033932764 -0.0033936962 -0.003393841 -0.0033936657 -0.0033931802 -0.0033926349 -0.0033920971]]...]
INFO - root - 2017-12-09 15:13:43.405317: step 35510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:51m:02s remains)
INFO - root - 2017-12-09 15:13:51.932769: step 35520, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:29m:46s remains)
INFO - root - 2017-12-09 15:14:00.478463: step 35530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:16m:39s remains)
INFO - root - 2017-12-09 15:14:09.182547: step 35540, loss = 0.89, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 63h:40m:51s remains)
INFO - root - 2017-12-09 15:14:17.924852: step 35550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:41m:56s remains)
INFO - root - 2017-12-09 15:14:26.621880: step 35560, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 73h:42m:27s remains)
INFO - root - 2017-12-09 15:14:35.272696: step 35570, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.915 sec/batch; 75h:29m:09s remains)
INFO - root - 2017-12-09 15:14:44.005383: step 35580, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:09m:32s remains)
INFO - root - 2017-12-09 15:14:52.693981: step 35590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 73h:39m:31s remains)
INFO - root - 2017-12-09 15:15:01.295608: step 35600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 73h:57m:02s remains)
2017-12-09 15:15:02.241450: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21371722 0.20351063 0.1882634 0.16991389 0.15093586 0.13271244 0.11687318 0.10359025 0.093063109 0.085329577 0.079930052 0.076354578 0.0758705 0.077413805 0.081772208][0.22800161 0.2164083 0.19913878 0.17868067 0.15767421 0.13756984 0.12007526 0.10507681 0.092713572 0.082081787 0.073789582 0.068690382 0.067021623 0.069826767 0.077123][0.23428418 0.2213919 0.2019062 0.18042338 0.15889311 0.13931058 0.12140193 0.10678416 0.094300419 0.082859024 0.073250636 0.066151813 0.063140713 0.065750182 0.0737811][0.23702565 0.22404978 0.20395966 0.18193226 0.16024759 0.14139469 0.12455482 0.11056823 0.098315522 0.087826081 0.077253617 0.069297358 0.065471843 0.0679214 0.076603025][0.23808628 0.22681601 0.20786372 0.18694548 0.16679882 0.14906171 0.13313712 0.12002629 0.10805868 0.0973586 0.086847119 0.079697832 0.075833261 0.078478955 0.086939186][0.23877321 0.22975117 0.21286625 0.19466014 0.17719187 0.16182421 0.14779101 0.1358097 0.12415875 0.11331171 0.10288762 0.095245272 0.091318712 0.094561338 0.10286922][0.23479536 0.22950603 0.21578093 0.20134611 0.18801127 0.17602806 0.16472258 0.1545973 0.14426704 0.13413222 0.1241846 0.11704633 0.11374868 0.11643767 0.12392267][0.22906578 0.22759578 0.21722661 0.20640211 0.1963149 0.1876357 0.17914057 0.1711223 0.16241743 0.1543223 0.14637867 0.14142992 0.13989793 0.14309555 0.15022683][0.22283864 0.22505301 0.2180687 0.21026023 0.20332493 0.19745934 0.1913459 0.18527687 0.17824467 0.17163976 0.16549027 0.16263078 0.16239958 0.16671845 0.17384794][0.21559596 0.22068888 0.21611552 0.21047612 0.20550936 0.20113316 0.19689783 0.19263303 0.1877148 0.1827502 0.17825422 0.17677324 0.17757843 0.18140675 0.18712266][0.20350122 0.21148814 0.20931199 0.20557545 0.20195617 0.19840348 0.19463226 0.191033 0.18760504 0.18446222 0.18190625 0.18121348 0.18253136 0.18584888 0.19002815][0.18364173 0.19416657 0.19480808 0.19319144 0.19105561 0.18833394 0.1851664 0.18138722 0.17818066 0.17598353 0.17449598 0.17486219 0.17689697 0.18006173 0.18349041][0.15851881 0.17025131 0.17325577 0.17359154 0.1730731 0.17119487 0.16861188 0.165118 0.16197261 0.1599312 0.15878831 0.1594518 0.16175786 0.16510338 0.1685503][0.12930284 0.1407375 0.14492008 0.14658402 0.14728552 0.14642651 0.14477335 0.14193694 0.13930531 0.13744295 0.13642365 0.13701597 0.13921981 0.1425274 0.14586622][0.099375881 0.10940713 0.113745 0.11604398 0.11729414 0.11688783 0.11577264 0.11354327 0.11139023 0.10983239 0.10890262 0.109375 0.11128714 0.1142511 0.11732515]]...]
INFO - root - 2017-12-09 15:15:10.770561: step 35610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:46m:53s remains)
INFO - root - 2017-12-09 15:15:19.238578: step 35620, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 68h:30m:17s remains)
INFO - root - 2017-12-09 15:15:27.892429: step 35630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:45m:55s remains)
INFO - root - 2017-12-09 15:15:36.406117: step 35640, loss = 0.89, batch loss = 0.68 (10.8 examples/sec; 0.743 sec/batch; 61h:15m:09s remains)
INFO - root - 2017-12-09 15:15:45.137482: step 35650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:47m:44s remains)
INFO - root - 2017-12-09 15:15:53.819244: step 35660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:06m:43s remains)
INFO - root - 2017-12-09 15:16:02.511133: step 35670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:19m:20s remains)
INFO - root - 2017-12-09 15:16:11.268307: step 35680, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.903 sec/batch; 74h:29m:15s remains)
INFO - root - 2017-12-09 15:16:19.966630: step 35690, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 68h:21m:10s remains)
INFO - root - 2017-12-09 15:16:28.640093: step 35700, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:31m:18s remains)
2017-12-09 15:16:29.544742: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15039608 0.15112768 0.15113251 0.15035315 0.14785936 0.14375418 0.13989486 0.13546391 0.13019425 0.12279827 0.11851121 0.11605037 0.11308604 0.10830948 0.10462532][0.15993597 0.15795162 0.15544926 0.1527051 0.14917514 0.14565088 0.14309978 0.1407847 0.1373395 0.13286179 0.13089696 0.13011305 0.12846419 0.12452131 0.12072342][0.17023665 0.16509494 0.15894815 0.15364614 0.1480574 0.14440256 0.14188507 0.14030194 0.13991626 0.13884722 0.14012051 0.14131722 0.14099759 0.13843451 0.13524035][0.18375611 0.17661522 0.16796555 0.15882289 0.15016207 0.14363118 0.13893065 0.13752425 0.13758336 0.13943179 0.14337261 0.14793278 0.15083091 0.15042026 0.14863592][0.18971807 0.18251964 0.1717115 0.16030154 0.14938392 0.14104864 0.13454252 0.13194112 0.13329132 0.13695985 0.14193511 0.14872016 0.15356886 0.15629087 0.15666261][0.18886246 0.18098713 0.16905762 0.1558225 0.14251748 0.13236935 0.12489828 0.12281635 0.12583452 0.13231319 0.14082754 0.14978671 0.15706134 0.16180295 0.16317727][0.18455163 0.17554937 0.16129783 0.14625357 0.13074246 0.11839552 0.11000086 0.10900716 0.11436076 0.12368122 0.13651025 0.14944471 0.16047607 0.16721304 0.1695192][0.18086229 0.17338873 0.15748975 0.13920934 0.12051292 0.1052718 0.09504889 0.093516082 0.099997468 0.11203326 0.12877357 0.14614367 0.16120012 0.1708283 0.17449197][0.1799535 0.17223126 0.15456019 0.13307537 0.11072741 0.092947066 0.081648551 0.079960063 0.087263942 0.1016933 0.12167648 0.14231937 0.16036123 0.17218201 0.17754738][0.17398442 0.16695908 0.14755853 0.12394994 0.099998094 0.080993161 0.0695557 0.06878566 0.07771457 0.093970619 0.11578497 0.13837714 0.1580939 0.17109104 0.17713918][0.16500016 0.15814362 0.13895546 0.1163116 0.092774756 0.0736389 0.0633632 0.063627362 0.073152676 0.0904491 0.1126357 0.1354688 0.15535329 0.16840121 0.1745442][0.15455066 0.14958905 0.13133833 0.11132055 0.090848505 0.074212618 0.06544815 0.066986166 0.077462293 0.094546124 0.1160896 0.13801971 0.15614362 0.16760407 0.17230938][0.14731617 0.14485799 0.12999445 0.11307114 0.096404068 0.083356664 0.0768378 0.078910694 0.088703662 0.10482538 0.12414844 0.14357279 0.15891795 0.16774592 0.17049026][0.14252357 0.1425776 0.13091715 0.1185212 0.10624242 0.096581481 0.092658281 0.0957412 0.1054949 0.11932272 0.13571668 0.15194656 0.16388802 0.16986328 0.17008597][0.14009677 0.14207925 0.13363996 0.12423315 0.1147977 0.10750938 0.10472411 0.10799769 0.11647855 0.12795985 0.14134243 0.15435217 0.16349426 0.16706353 0.16581929]]...]
INFO - root - 2017-12-09 15:16:38.193738: step 35710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:38m:09s remains)
INFO - root - 2017-12-09 15:16:46.714020: step 35720, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 69h:18m:26s remains)
INFO - root - 2017-12-09 15:16:55.488205: step 35730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:33m:34s remains)
INFO - root - 2017-12-09 15:17:04.108736: step 35740, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.732 sec/batch; 60h:18m:04s remains)
INFO - root - 2017-12-09 15:17:12.747632: step 35750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:43m:59s remains)
INFO - root - 2017-12-09 15:17:21.338877: step 35760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:10m:46s remains)
INFO - root - 2017-12-09 15:17:29.952127: step 35770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 69h:20m:27s remains)
INFO - root - 2017-12-09 15:17:38.654134: step 35780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 69h:43m:18s remains)
INFO - root - 2017-12-09 15:17:47.538278: step 35790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 73h:01m:03s remains)
INFO - root - 2017-12-09 15:17:56.210613: step 35800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:07m:49s remains)
2017-12-09 15:17:57.089020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00089508272 -0.0020154989 -0.0027613337 -0.0031541642 -0.0033183699 -0.003372221 -0.0033832751 -0.0033820902 -0.0033787987 -0.0033787363 -0.0033793438 -0.0033801445 -0.0033792933 -0.0033796327 -0.0033806411][-0.0016201758 -0.0024632027 -0.0029850351 -0.0032433292 -0.0033460655 -0.0033786774 -0.0033849098 -0.0033846512 -0.0033827878 -0.0033833724 -0.0033837387 -0.003382979 -0.0033807622 -0.0033802676 -0.0033803433][-0.0023386707 -0.0028822373 -0.003183597 -0.0033186646 -0.0033678305 -0.0033804874 -0.003379738 -0.0033773065 -0.0033769456 -0.0033796884 -0.0033827014 -0.0033833829 -0.003382276 -0.0033810432 -0.0033808621][-0.0028627536 -0.0031617479 -0.0033053597 -0.0033589634 -0.0033701682 -0.0033632829 -0.0033513405 -0.0033439931 -0.003347748 -0.0033595758 -0.0033719996 -0.0033794893 -0.0033810979 -0.0033808791 -0.003380517][-0.003169775 -0.0033058145 -0.0033593732 -0.0033676878 -0.0033497603 -0.0033160776 -0.0032818117 -0.0032676472 -0.0032834841 -0.0033178788 -0.0033511112 -0.0033713495 -0.0033790027 -0.0033803913 -0.0033801117][-0.0033159524 -0.0033627939 -0.0033733472 -0.0033566412 -0.0033088916 -0.0032352982 -0.0031685093 -0.0031505781 -0.0031918073 -0.0032626125 -0.0033255534 -0.0033622291 -0.0033770034 -0.0033805638 -0.0033805207][-0.0033683521 -0.0033777612 -0.0033724245 -0.0033414788 -0.0032661369 -0.0031522107 -0.0030517455 -0.0030311695 -0.0031038043 -0.0032139223 -0.0033043739 -0.0033547261 -0.0033744269 -0.0033797759 -0.0033804122][-0.0033784474 -0.0033783468 -0.0033702266 -0.0033350177 -0.003248807 -0.0031195336 -0.003007764 -0.0029874372 -0.0030721035 -0.0031981659 -0.0032978854 -0.0033526807 -0.003373842 -0.0033794672 -0.0033801948][-0.0033786469 -0.0033768672 -0.0033713875 -0.0033417831 -0.0032674172 -0.0031593568 -0.0030725051 -0.00305926 -0.0031257288 -0.0032263212 -0.0033090829 -0.0033563653 -0.0033751498 -0.0033804786 -0.0033809892][-0.0033796194 -0.0033778427 -0.0033750602 -0.0033553478 -0.003302878 -0.0032314528 -0.0031793751 -0.0031770281 -0.0032207256 -0.0032821971 -0.0033323274 -0.0033628531 -0.0033766294 -0.0033809568 -0.0033809887][-0.0033802153 -0.0033788134 -0.0033789938 -0.00336723 -0.0033362182 -0.0032953769 -0.0032697965 -0.0032738943 -0.0033007942 -0.0033324198 -0.0033563066 -0.0033712697 -0.0033792548 -0.0033813724 -0.0033807734][-0.0033807287 -0.0033793261 -0.0033801226 -0.0033754017 -0.0033594249 -0.0033390869 -0.0033287376 -0.003334119 -0.0033486458 -0.0033625562 -0.0033718238 -0.0033772439 -0.0033801589 -0.0033810053 -0.0033803238][-0.0033801785 -0.0033787154 -0.0033795813 -0.0033787042 -0.0033725982 -0.0033652061 -0.0033623166 -0.0033659611 -0.003372241 -0.0033774998 -0.0033794031 -0.0033799449 -0.0033801254 -0.0033800881 -0.0033794979][-0.003379788 -0.0033775137 -0.0033781228 -0.0033788739 -0.00337727 -0.0033759128 -0.003375785 -0.0033782243 -0.0033807145 -0.0033810094 -0.003380439 -0.0033797435 -0.0033790383 -0.0033784881 -0.0033784194][-0.003379208 -0.0033763372 -0.0033768115 -0.0033776572 -0.0033778092 -0.0033783803 -0.0033790898 -0.0033798912 -0.003380178 -0.0033791997 -0.0033782716 -0.003377863 -0.0033779275 -0.003377903 -0.0033780048]]...]
INFO - root - 2017-12-09 15:18:05.710113: step 35810, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 72h:49m:09s remains)
INFO - root - 2017-12-09 15:18:14.226903: step 35820, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 72h:44m:37s remains)
INFO - root - 2017-12-09 15:18:22.806472: step 35830, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 71h:59m:55s remains)
INFO - root - 2017-12-09 15:18:31.537580: step 35840, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:05m:30s remains)
INFO - root - 2017-12-09 15:18:40.091105: step 35850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:02m:24s remains)
INFO - root - 2017-12-09 15:18:48.757903: step 35860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:36m:51s remains)
INFO - root - 2017-12-09 15:18:57.331610: step 35870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:51m:13s remains)
INFO - root - 2017-12-09 15:19:05.889989: step 35880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:19m:27s remains)
INFO - root - 2017-12-09 15:19:14.599354: step 35890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:04m:41s remains)
INFO - root - 2017-12-09 15:19:23.477061: step 35900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:07m:14s remains)
2017-12-09 15:19:24.363944: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026696423 0.025103981 0.02356188 0.021881724 0.019742906 0.017251028 0.014515007 0.011878685 0.0096844593 0.0080594309 0.0074044084 0.0079465378 0.0099166539 0.013014378 0.016929442][0.023664156 0.021528268 0.019519927 0.01745699 0.015112128 0.012562351 0.0099044722 0.0075392351 0.0057014031 0.0044287164 0.0039732242 0.0044622393 0.0061825644 0.0091474317 0.013187999][0.018444372 0.016068637 0.014007231 0.012040615 0.0098847421 0.007621496 0.005364798 0.0034761087 0.0020209921 0.000992975 0.00060891523 0.00095465849 0.002323699 0.0048490893 0.0085381977][0.013232728 0.010613179 0.0085320445 0.0068389652 0.0051724473 0.003530801 0.0019002084 0.00059289229 -0.00043335184 -0.0011710031 -0.0014610048 -0.0013266909 -0.00048065628 0.0013610704 0.00439455][0.0085367635 0.0058141565 0.0038698611 0.0025770657 0.0015593406 0.00072789541 -5.2082352e-05 -0.00064353528 -0.0012144826 -0.0017731675 -0.0021327939 -0.0022971185 -0.0020104591 -0.00095433882 0.0011916112][0.0050574942 0.0023329773 0.00059178728 -0.00027604355 -0.00062580174 -0.00063253241 -0.00056515285 -0.00050967745 -0.00073564635 -0.0012779792 -0.0018297356 -0.0023170931 -0.0025177135 -0.0021510308 -0.00087673776][0.0031798391 0.00048979628 -0.0010739311 -0.0016428147 -0.00151179 -0.00090623996 -0.00016872236 0.0003807887 0.00035343762 -0.00031726575 -0.0011958336 -0.0020211013 -0.0025372098 -0.0025677623 -0.0019144661][0.0025446319 -7.5150747e-06 -0.001429315 -0.0018498302 -0.001508153 -0.00060062949 0.00054251566 0.001452381 0.0016003361 0.00081628677 -0.0004092548 -0.0016173044 -0.0024350462 -0.0027520226 -0.0025506995][0.0024249225 -7.5972639e-06 -0.0013528035 -0.0017149842 -0.001281857 -0.00020492752 0.0011768646 0.0023186964 0.0025959185 0.0018157766 0.00039901328 -0.0010679446 -0.00213106 -0.0026845683 -0.0028362083][0.0027185886 0.00019994215 -0.0012656802 -0.0017033569 -0.001272396 -0.00015986129 0.0012965247 0.0025126503 0.0028728645 0.0021816005 0.00080031087 -0.00069557736 -0.0018100842 -0.0024437332 -0.0027636974][0.0033185652 0.00046859193 -0.001292767 -0.0018575653 -0.001468533 -0.00037212833 0.0010183936 0.0021388645 0.0024347377 0.0018069812 0.00057995575 -0.00075157313 -0.0017480727 -0.0023457254 -0.0026690715][0.0043833144 0.0011767319 -0.00096935243 -0.0018240645 -0.0016680803 -0.00079490594 0.0003390878 0.0012069738 0.0013735273 0.00082142907 -0.00018950738 -0.0012755306 -0.0020616418 -0.0025225356 -0.0027364672][0.0054475674 0.0020779867 -0.00029351446 -0.0013901971 -0.0015466305 -0.0010821696 -0.00043043983 1.4073914e-05 -3.3200253e-05 -0.00050219521 -0.0011909946 -0.0018854657 -0.0023478405 -0.0026510141 -0.0028012395][0.0067730341 0.0033180073 0.0007448101 -0.00067999959 -0.0012365382 -0.0012385335 -0.001067386 -0.001005704 -0.0012053153 -0.0015470624 -0.0019328753 -0.0022815454 -0.0024831118 -0.0026144614 -0.0026692203][0.0085511189 0.0050998162 0.0024344774 0.000770299 -0.00013721059 -0.00058562239 -0.00086784619 -0.0011676461 -0.0015563219 -0.0018858436 -0.0021110447 -0.0022404273 -0.0022835198 -0.0023172111 -0.0023332112]]...]
INFO - root - 2017-12-09 15:19:32.844275: step 35910, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:59m:32s remains)
INFO - root - 2017-12-09 15:19:41.348453: step 35920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:11m:24s remains)
INFO - root - 2017-12-09 15:19:49.915547: step 35930, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 73h:15m:18s remains)
INFO - root - 2017-12-09 15:19:58.555921: step 35940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:41m:33s remains)
INFO - root - 2017-12-09 15:20:07.221697: step 35950, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 69h:28m:11s remains)
INFO - root - 2017-12-09 15:20:16.037633: step 35960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 73h:00m:38s remains)
INFO - root - 2017-12-09 15:20:24.659126: step 35970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:45m:55s remains)
INFO - root - 2017-12-09 15:20:33.410956: step 35980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:06m:57s remains)
INFO - root - 2017-12-09 15:20:42.134506: step 35990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:12m:56s remains)
INFO - root - 2017-12-09 15:20:50.798465: step 36000, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:59m:18s remains)
2017-12-09 15:20:51.726043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017155528 -0.0015077745 -0.0013060849 -0.00074748439 -0.00011148071 0.00076376949 0.001761683 0.0024930963 0.0028542227 0.00259589 0.0019004804 0.00087033 -0.00021748827 -0.0010397322 -0.0015656352][-0.0013237372 -0.00098265032 -0.00061989552 0.00017994107 0.0010161924 0.0022374287 0.0035093573 0.0045793336 0.0052540889 0.0053294143 0.0049227756 0.0039424337 0.0026393814 0.0011302598 -0.00014539366][-0.00090454705 -0.00037457072 0.0003486881 0.0013768987 0.0024423802 0.0038415582 0.0052365409 0.0065542525 0.0073805554 0.0076438747 0.0075249709 0.0067520328 0.0055524874 0.00403014 0.0024022742][-0.00067820633 7.4775424e-05 0.0011123416 0.0024262138 0.0039727353 0.0057083229 0.0073718261 0.0090022972 0.010001283 0.010415691 0.010017366 0.0090425145 0.0076619582 0.006002754 0.0041313497][-0.0012690993 -2.8987415e-06 0.0015559702 0.0034894336 0.0053800084 0.0074071037 0.0090993494 0.010724064 0.011706254 0.012146049 0.011818029 0.011045902 0.0097654359 0.007996561 0.0058325762][-0.0020278748 -0.00068323524 0.0010704023 0.0035679473 0.0061760424 0.0085267536 0.010257394 0.011799886 0.012683808 0.013123802 0.012929655 0.012442925 0.011447522 0.0099267624 0.0078895688][-0.0020100516 -0.0012239842 0.00041911006 0.0029024079 0.0055820327 0.0084886942 0.01060572 0.012282264 0.013096879 0.013565774 0.013474858 0.013130515 0.012328125 0.011148837 0.0094328113][-0.0015452723 -0.00096751517 0.00046378979 0.0027509446 0.0054831197 0.0083650965 0.010786022 0.012714201 0.013624583 0.014160975 0.013975908 0.01373509 0.013093197 0.012069568 0.010767999][-0.0018343288 -0.0010071311 0.000525594 0.0028165963 0.0054982807 0.0083203446 0.010804298 0.012729899 0.013822709 0.014270703 0.014050978 0.013747751 0.013109528 0.012412005 0.01164721][-0.0020526762 -0.00134497 9.6122967e-05 0.002557215 0.0051987823 0.0078591807 0.010345074 0.012404789 0.013838495 0.014599123 0.014722927 0.0143597 0.013527106 0.013035524 0.012222028][-0.0023379857 -0.0018922595 -0.00064764172 0.0015148164 0.0039764103 0.0066443197 0.0091585275 0.011387235 0.012967211 0.014127844 0.014596521 0.014434271 0.013872663 0.013592458 0.013026866][-0.0021579403 -0.0019076099 -0.0011364904 0.00040260539 0.0024012134 0.00482249 0.0071295947 0.0095818751 0.011421848 0.012870657 0.013708699 0.013921111 0.013672818 0.013547434 0.013242193][-0.0020244378 -0.0018407813 -0.001343397 -0.00033140555 0.00096738734 0.0027307996 0.0047722869 0.0072278 0.0094332853 0.011479778 0.012888952 0.013510336 0.013506769 0.013387791 0.013081889][-0.002297522 -0.0020135893 -0.0016860017 -0.0010884386 -0.00012735091 0.0013260755 0.0029096014 0.0049322182 0.0070971837 0.0092051942 0.010977156 0.012169375 0.012591564 0.012922592 0.012893262][-0.0025637131 -0.0024877023 -0.002276706 -0.0018046767 -0.0012061822 -9.2230737e-05 0.0012530808 0.0031110879 0.005002195 0.0072109317 0.0091818944 0.010960863 0.011928906 0.012631907 0.012992694]]...]
INFO - root - 2017-12-09 15:21:00.453296: step 36010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:12m:57s remains)
INFO - root - 2017-12-09 15:21:08.958014: step 36020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:47m:00s remains)
INFO - root - 2017-12-09 15:21:17.534223: step 36030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:13m:58s remains)
INFO - root - 2017-12-09 15:21:26.291983: step 36040, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 75h:06m:55s remains)
INFO - root - 2017-12-09 15:21:34.880983: step 36050, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:15m:31s remains)
INFO - root - 2017-12-09 15:21:43.595316: step 36060, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:48m:21s remains)
INFO - root - 2017-12-09 15:21:52.239013: step 36070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:10m:51s remains)
INFO - root - 2017-12-09 15:22:00.889616: step 36080, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 72h:36m:03s remains)
INFO - root - 2017-12-09 15:22:09.356712: step 36090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:46m:08s remains)
INFO - root - 2017-12-09 15:22:17.842415: step 36100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 71h:23m:02s remains)
2017-12-09 15:22:18.720533: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028855032 0.0025420077 0.0016720151 0.00048593758 -0.00077619124 -0.0016781304 -0.0023155264 -0.0027099049 -0.0028542646 -0.0029452131 -0.0030067309 -0.0031614061 -0.0032589363 -0.0033403635 -0.0033753191][0.0043163793 0.0041516162 0.0034849457 0.0024715913 0.0012452798 3.8062688e-05 -0.0011383491 -0.0021201316 -0.0028014295 -0.0031224973 -0.0031788242 -0.0032527959 -0.0032820026 -0.0033269539 -0.003376747][0.0058278185 0.0056855022 0.0050881277 0.0042474503 0.0032212129 0.0019885758 0.00064424705 -0.00063090748 -0.0019138167 -0.0027705024 -0.0031788796 -0.003308855 -0.0033324398 -0.0033472059 -0.003377113][0.0069361576 0.0069509349 0.00645455 0.005630746 0.0046462291 0.00355769 0.0022873194 0.0010676705 -0.00062574004 -0.0018822382 -0.0027244878 -0.0031856759 -0.0033319443 -0.0033596384 -0.00337769][0.007301697 0.0077132084 0.00746307 0.0067563956 0.005813858 0.0047294027 0.003438679 0.0021980931 0.00045869872 -0.00096054818 -0.0020915219 -0.002815438 -0.003222713 -0.003357477 -0.0033770895][0.0070012226 0.0075037903 0.0075682891 0.0072153 0.0066415039 0.0057451152 0.0045008934 0.0030585979 0.0011172933 -0.00039529894 -0.0015278017 -0.0024121383 -0.0030313928 -0.0033097873 -0.0033765114][0.0066645727 0.0071802516 0.0072461911 0.0071106548 0.0068053924 0.0062655932 0.0053704954 0.0043096351 0.002379756 0.00056337658 -0.000972847 -0.0021141176 -0.0028640348 -0.0032521968 -0.0033764876][0.0063817627 0.0071580629 0.007242505 0.0071586915 0.0069059194 0.0065652658 0.0058930949 0.0051153875 0.0036157859 0.0019600699 0.00030057877 -0.0013113609 -0.002507491 -0.003146773 -0.0033588596][0.0060651167 0.0072831172 0.0074381954 0.0074330508 0.0073325192 0.00720376 0.0067151687 0.0060733319 0.004773892 0.00320346 0.0014961748 -0.00033951271 -0.0018655558 -0.0028830462 -0.0033149528][0.0062170071 0.0075848778 0.007734301 0.0078400839 0.0078455247 0.0078747943 0.0075893425 0.0071578869 0.0061007752 0.0045160851 0.0025650114 0.00051089656 -0.0012815692 -0.0025428713 -0.0031938085][0.0063611069 0.0079891067 0.0081378724 0.0083159953 0.0083426312 0.0083999038 0.0083053559 0.0079218876 0.0069192718 0.0052422718 0.0030485708 0.0010115812 -0.00086219446 -0.0022893585 -0.0030683849][0.0066370466 0.008514531 0.0085422564 0.0086582359 0.0086144879 0.0086671021 0.0086713415 0.0083960816 0.0075995126 0.006046887 0.0036289585 0.0013663503 -0.0007647872 -0.0022253445 -0.003032522][0.0073328693 0.0091512222 0.0090507492 0.0090841129 0.0088881012 0.0088036843 0.0086510535 0.0084227007 0.0077995374 0.0064838384 0.00418011 0.0017601796 -0.00059751072 -0.0021233577 -0.0029821191][0.007427447 0.0089709312 0.0089114606 0.0090494249 0.0088647548 0.0088535827 0.0086250193 0.0083632432 0.0077354242 0.006585672 0.0044605914 0.0020187576 -0.00039869593 -0.0019693442 -0.0028964079][0.0071080984 0.0082145613 0.0081666112 0.00834926 0.0082186237 0.0083148722 0.0080926809 0.00797433 0.0073973839 0.006463272 0.0045752893 0.0022713682 -0.00016219798 -0.001795172 -0.0028079888]]...]
INFO - root - 2017-12-09 15:22:27.350201: step 36110, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:07m:27s remains)
INFO - root - 2017-12-09 15:22:35.849576: step 36120, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.927 sec/batch; 76h:16m:40s remains)
INFO - root - 2017-12-09 15:22:44.256759: step 36130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 71h:59m:06s remains)
INFO - root - 2017-12-09 15:22:53.089454: step 36140, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 1.002 sec/batch; 82h:27m:09s remains)
INFO - root - 2017-12-09 15:23:01.592462: step 36150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:33m:21s remains)
INFO - root - 2017-12-09 15:23:10.302305: step 36160, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 71h:39m:10s remains)
INFO - root - 2017-12-09 15:23:19.030655: step 36170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:04m:51s remains)
INFO - root - 2017-12-09 15:23:27.772989: step 36180, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 72h:46m:14s remains)
INFO - root - 2017-12-09 15:23:36.523566: step 36190, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 70h:37m:44s remains)
INFO - root - 2017-12-09 15:23:45.300103: step 36200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 71h:27m:59s remains)
2017-12-09 15:23:46.161039: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33629313 0.33768806 0.3407231 0.34648159 0.35077333 0.35119358 0.34115189 0.3184233 0.28023627 0.23158911 0.17495143 0.11883863 0.067932509 0.030195646 0.0075038979][0.34235349 0.34617624 0.35130137 0.35853428 0.36462638 0.36628386 0.35721305 0.33488688 0.29648182 0.2466788 0.18796159 0.12964143 0.075724028 0.034655694 0.0091837225][0.33714214 0.34313577 0.35222933 0.36430821 0.37474403 0.37841928 0.36980483 0.34672251 0.30716038 0.25631076 0.19662872 0.13752969 0.08205162 0.039010171 0.011293022][0.33112147 0.33897129 0.35057974 0.3658058 0.38014162 0.3878735 0.38229015 0.36022371 0.32000065 0.26765043 0.2060374 0.14533362 0.087905839 0.043087274 0.013360824][0.32219759 0.33395544 0.34868392 0.36658758 0.38329834 0.39310378 0.38968766 0.36998802 0.33130032 0.27937159 0.21692052 0.15456869 0.095041692 0.047819268 0.015615897][0.31250948 0.32790434 0.34488133 0.36510909 0.38448203 0.39639315 0.39470273 0.37591055 0.33805838 0.28700429 0.22521287 0.16287588 0.10216764 0.052927788 0.018296067][0.3044802 0.32311985 0.34132275 0.36242566 0.38277134 0.39601737 0.39622554 0.37932822 0.34298772 0.29231569 0.23057945 0.16795076 0.10669711 0.056575295 0.020337142][0.30109322 0.32190794 0.33826822 0.35787383 0.37707004 0.39014411 0.39123565 0.37620372 0.342257 0.29370895 0.23361416 0.171301 0.10973796 0.058796555 0.021594619][0.30066383 0.32391864 0.33913204 0.3559989 0.372105 0.38303742 0.3832199 0.36898488 0.33645719 0.29010165 0.23208508 0.17121606 0.1105279 0.059695337 0.022276733][0.30181137 0.32561412 0.33926621 0.35452169 0.36894163 0.37781638 0.3758412 0.36040524 0.32749242 0.28224343 0.2257615 0.16686398 0.10804307 0.058452662 0.021968819][0.30282256 0.32729074 0.33954561 0.35288244 0.36564198 0.37342465 0.37042269 0.35368744 0.31934223 0.27338475 0.21707729 0.15955077 0.10261921 0.055168033 0.020599328][0.30190852 0.32642791 0.33802372 0.35136068 0.36380559 0.37016135 0.36571679 0.34840372 0.31333548 0.26666594 0.20957437 0.15222158 0.096609131 0.051100526 0.0184888][0.30087849 0.323012 0.33280885 0.34488177 0.35688627 0.36377597 0.35989904 0.34236547 0.30705106 0.26029357 0.2031499 0.14645159 0.091842763 0.047665361 0.01667453][0.29597616 0.31558785 0.32380155 0.33488753 0.34632802 0.35313407 0.34969309 0.33301833 0.29863742 0.2523064 0.19540893 0.13977261 0.086657435 0.044377595 0.015113251][0.28309682 0.30173713 0.3096205 0.31928787 0.32912657 0.33572826 0.3323181 0.31618452 0.283393 0.23871179 0.18361202 0.13001767 0.079496242 0.039848045 0.012958556]]...]
INFO - root - 2017-12-09 15:23:54.806060: step 36210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:53m:40s remains)
INFO - root - 2017-12-09 15:24:03.341627: step 36220, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 72h:34m:21s remains)
INFO - root - 2017-12-09 15:24:11.823897: step 36230, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 71h:46m:17s remains)
INFO - root - 2017-12-09 15:24:20.439332: step 36240, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:07m:26s remains)
INFO - root - 2017-12-09 15:24:28.665519: step 36250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 68h:39m:07s remains)
INFO - root - 2017-12-09 15:24:37.150229: step 36260, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 68h:12m:36s remains)
INFO - root - 2017-12-09 15:24:45.783617: step 36270, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 73h:48m:24s remains)
INFO - root - 2017-12-09 15:24:54.450299: step 36280, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:52m:27s remains)
INFO - root - 2017-12-09 15:25:03.100136: step 36290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:27m:24s remains)
INFO - root - 2017-12-09 15:25:11.718712: step 36300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:02m:44s remains)
2017-12-09 15:25:12.551877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012953989 0.001960499 0.007058599 0.013449131 0.019923287 0.02482795 0.026815411 0.025494475 0.021702448 0.016488444 0.010870242 0.0057728123 0.0017992065 -0.00094931922 -0.002535396][0.00047345622 0.0052567758 0.013017755 0.023049418 0.033476487 0.041751705 0.045738868 0.044594456 0.039164122 0.03094833 0.021712024 0.013065409 0.0061228219 0.0012018906 -0.0016954645][0.0042239362 0.011386858 0.022782397 0.037441481 0.052662823 0.064923219 0.071225762 0.070319965 0.062920161 0.050967492 0.037110213 0.023804775 0.012823252 0.0047681006 -0.0002009871][0.0099867443 0.020548085 0.036526892 0.056452323 0.076660328 0.092668377 0.10084642 0.0997603 0.090069495 0.074090093 0.055276472 0.036905676 0.021393823 0.0096620079 0.00204846][0.01687143 0.031434044 0.052316215 0.077391014 0.10211208 0.12124601 0.13076413 0.12920572 0.11724108 0.097426675 0.073869482 0.05054678 0.030540999 0.015119825 0.0047305562][0.023272928 0.041404605 0.066338688 0.09527801 0.12312318 0.14430028 0.154759 0.15300359 0.13972165 0.11741527 0.090432212 0.063173041 0.039286576 0.020519415 0.0074731447][0.027554367 0.047832962 0.074947439 0.10572425 0.13490079 0.15686713 0.16778319 0.16614681 0.15264477 0.12955247 0.10110053 0.071729578 0.045428216 0.024409328 0.0095050735][0.02880992 0.04931223 0.076291054 0.10663702 0.13535704 0.15707399 0.1681298 0.16698425 0.15429512 0.13205966 0.10412319 0.074660465 0.047743939 0.025921999 0.01030428][0.026651932 0.045608025 0.070362568 0.0982018 0.12472566 0.1450831 0.15584576 0.15544802 0.14437589 0.12427966 0.098516576 0.070873179 0.045266524 0.024360098 0.0094703715][0.021804905 0.037835009 0.058830742 0.0825918 0.1054112 0.1231843 0.13283379 0.13295679 0.12384889 0.10680768 0.0846175 0.060558021 0.038172871 0.019942731 0.0072030658][0.015626255 0.02792044 0.044177569 0.062783346 0.080811754 0.095012464 0.10280003 0.10304499 0.095916681 0.082435749 0.064811118 0.045704328 0.028039098 0.013834339 0.0041971132][0.0094901947 0.01799847 0.029358104 0.042463817 0.055212725 0.065298371 0.07076221 0.070810281 0.065534748 0.055734284 0.043057248 0.029492412 0.017186426 0.0075196316 0.0012249427][0.004243305 0.0095107406 0.016603474 0.024805481 0.03276008 0.039002255 0.042246226 0.04201436 0.038384855 0.031957258 0.023865335 0.015427746 0.00800766 0.0023857735 -0.001081015][0.00026254379 0.0030984229 0.0069499938 0.011397314 0.015670434 0.018939244 0.020487031 0.020083224 0.017818719 0.014123658 0.0097025167 0.0053030048 0.0016251893 -0.0010086186 -0.00251152][-0.0021476271 -0.00092582055 0.00078982324 0.0027727017 0.0046441173 0.0060181366 0.0065771667 0.0062384754 0.0050854608 0.003371225 0.0014517151 -0.00034371438 -0.001745748 -0.002676124 -0.0031562396]]...]
INFO - root - 2017-12-09 15:25:21.085978: step 36310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:29m:25s remains)
INFO - root - 2017-12-09 15:25:29.415579: step 36320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 71h:12m:34s remains)
INFO - root - 2017-12-09 15:25:37.957894: step 36330, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 73h:50m:21s remains)
INFO - root - 2017-12-09 15:25:46.673879: step 36340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:50m:06s remains)
INFO - root - 2017-12-09 15:25:55.170323: step 36350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:02m:08s remains)
INFO - root - 2017-12-09 15:26:03.739630: step 36360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:19m:23s remains)
INFO - root - 2017-12-09 15:26:12.148624: step 36370, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 71h:50m:13s remains)
INFO - root - 2017-12-09 15:26:20.745648: step 36380, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 70h:22m:49s remains)
INFO - root - 2017-12-09 15:26:29.320234: step 36390, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 71h:06m:07s remains)
INFO - root - 2017-12-09 15:26:37.741627: step 36400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:59m:33s remains)
2017-12-09 15:26:38.719138: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027568744 0.017199771 0.011316235 0.0093639949 0.010356935 0.012447809 0.014217428 0.014017633 0.012085818 0.0088413749 0.0052608792 0.0021586649 -0.00020570937 -0.0017260133 -0.0026411135][0.024815721 0.016600892 0.013142481 0.013566487 0.016557826 0.019780319 0.021456577 0.020078804 0.016274186 0.011122602 0.0060461676 0.0020907817 -0.00055024726 -0.0020262222 -0.0028038674][0.023116905 0.019390581 0.020666493 0.025630532 0.032339126 0.037850946 0.039933488 0.03717437 0.030435404 0.021576276 0.012748231 0.0057262713 0.0010451612 -0.0015480443 -0.0027593481][0.024910428 0.02790756 0.036309782 0.048301168 0.061226796 0.071318753 0.075424574 0.072025515 0.061931554 0.047514468 0.031875331 0.018119222 0.007961506 0.0016413892 -0.0016150351][0.029260362 0.039496858 0.0564944 0.077819422 0.099434741 0.11657295 0.12513754 0.12297767 0.11028043 0.089778334 0.065265678 0.041554943 0.022274306 0.0090638567 0.0015231476][0.033691924 0.0513697 0.076757967 0.10723591 0.13825291 0.16394846 0.17894912 0.18029858 0.16702621 0.14166088 0.10828561 0.0734795 0.043025 0.020648815 0.0069107059][0.03568938 0.059084926 0.091630153 0.13014793 0.16949679 0.20326833 0.22547835 0.2319048 0.22030038 0.19241321 0.15208358 0.10724333 0.065824971 0.03393748 0.01342268][0.035304114 0.06128953 0.097611949 0.14051095 0.18507694 0.22448647 0.25263894 0.26441476 0.2563878 0.22905158 0.18552436 0.13430493 0.084840596 0.045411378 0.019231595][0.032292556 0.057372261 0.093097731 0.13577013 0.18119906 0.22224931 0.25345874 0.26901379 0.26499927 0.24065925 0.19822747 0.14586437 0.093571261 0.050936971 0.022087945][0.026332863 0.048152514 0.07947173 0.11742728 0.15884982 0.19707966 0.22768715 0.24447231 0.24371848 0.22366674 0.18594363 0.13782629 0.088825323 0.048442114 0.020892562][0.018194782 0.034881182 0.059263933 0.089682437 0.12373089 0.15586004 0.18272145 0.19830804 0.19944988 0.18390939 0.15316837 0.11333745 0.072604716 0.039082818 0.016276687][0.010584512 0.021536827 0.038011491 0.059415 0.084129348 0.10817546 0.12913223 0.14184697 0.14382888 0.13286088 0.11027476 0.080824561 0.050893132 0.026518228 0.010150304][0.0044629844 0.01070381 0.020280372 0.03337089 0.049042385 0.06478437 0.078936182 0.087743253 0.0895229 0.082577251 0.067948833 0.048883937 0.029751753 0.014448014 0.0044112783][3.2119686e-05 0.0030093852 0.0076945461 0.01455954 0.023124544 0.032117907 0.040446136 0.045694754 0.046852935 0.042880706 0.034541413 0.023844505 0.013402429 0.0053139273 0.00022356794][-0.0024536429 -0.0013620141 0.00041616685 0.0032758426 0.0070926463 0.011427368 0.015591222 0.018282369 0.018920774 0.016990989 0.012983308 0.0079985624 0.0033461221 -8.0790371e-05 -0.0021029362]]...]
INFO - root - 2017-12-09 15:26:47.185660: step 36410, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 66h:48m:14s remains)
INFO - root - 2017-12-09 15:26:55.567450: step 36420, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 61h:32m:01s remains)
INFO - root - 2017-12-09 15:27:03.812329: step 36430, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 68h:00m:39s remains)
INFO - root - 2017-12-09 15:27:12.478162: step 36440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:42m:37s remains)
INFO - root - 2017-12-09 15:27:21.040449: step 36450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:04m:09s remains)
INFO - root - 2017-12-09 15:27:29.699419: step 36460, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:55m:39s remains)
INFO - root - 2017-12-09 15:27:38.457299: step 36470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:04m:24s remains)
INFO - root - 2017-12-09 15:27:47.004516: step 36480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 73h:26m:37s remains)
INFO - root - 2017-12-09 15:27:55.675546: step 36490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:07m:46s remains)
INFO - root - 2017-12-09 15:28:04.370500: step 36500, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:56m:11s remains)
2017-12-09 15:28:05.246280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033867497 -0.0033833338 -0.0033814448 -0.0033805829 -0.0033801754 -0.0033800709 -0.0033801256 -0.0033804304 -0.0033807894 -0.003381181 -0.0033815429 -0.0033820667 -0.0033823811 -0.0033827315 -0.003383514][-0.003385439 -0.0033816362 -0.0033794609 -0.0033781794 -0.003377452 -0.0033771857 -0.0033771293 -0.0033774313 -0.0033778523 -0.0033784097 -0.0033789796 -0.0033795631 -0.0033798746 -0.0033802874 -0.0033809685][-0.0033832574 -0.003379544 -0.0033776884 -0.003376506 -0.0033756811 -0.0033755319 -0.0033754604 -0.0033755414 -0.0033757929 -0.0033763014 -0.0033768003 -0.0033774597 -0.0033780304 -0.0033784891 -0.0033791491][-0.0033819256 -0.0033787652 -0.0033771999 -0.003376018 -0.0033747263 -0.0033741363 -0.003373476 -0.0033730776 -0.0033729635 -0.0033731749 -0.0033732655 -0.003373784 -0.003374412 -0.0033749738 -0.003375645][-0.0033781589 -0.0033750883 -0.0033735931 -0.0033721423 -0.0033705633 -0.0033695335 -0.0033684806 -0.003367502 -0.0033669821 -0.0033668485 -0.003366726 -0.0033671111 -0.003367879 -0.0033687116 -0.0033693253][-0.0033706457 -0.0033675 -0.0033666193 -0.0033655586 -0.0033641229 -0.0033635593 -0.0033628587 -0.0033619006 -0.0033613511 -0.0033610472 -0.0033606705 -0.0033605471 -0.0033608216 -0.0033613928 -0.0033617269][-0.0033609413 -0.0033576263 -0.0033572165 -0.0033566805 -0.0033558493 -0.0033557389 -0.0033551997 -0.0033541005 -0.0033537243 -0.0033534884 -0.0033531045 -0.0033530646 -0.0033532896 -0.0033537354 -0.0033538239][-0.0033546328 -0.0033507508 -0.0033502444 -0.0033501505 -0.0033496323 -0.0033496802 -0.003349307 -0.0033484197 -0.0033481496 -0.0033477899 -0.0033472793 -0.0033472658 -0.0033474755 -0.003347998 -0.0033483084][-0.0033535291 -0.0033488763 -0.0033480388 -0.0033477403 -0.0033472639 -0.0033472723 -0.0033472006 -0.0033469014 -0.003346636 -0.0033462006 -0.0033455854 -0.0033452185 -0.0033449316 -0.0033452057 -0.0033455889][-0.0033562868 -0.0033518779 -0.003351039 -0.0033507037 -0.0033504888 -0.0033505131 -0.0033506539 -0.003350565 -0.0033505629 -0.0033504425 -0.0033500236 -0.0033496036 -0.0033492881 -0.0033496339 -0.0033500646][-0.003360989 -0.0033571881 -0.0033566297 -0.0033565962 -0.0033564963 -0.0033565559 -0.0033566887 -0.0033568337 -0.0033569371 -0.0033568144 -0.0033564006 -0.0033560181 -0.0033557499 -0.0033560125 -0.0033564898][-0.0033695253 -0.0033660422 -0.0033651483 -0.0033648508 -0.0033645758 -0.0033644452 -0.0033642829 -0.0033641991 -0.0033641842 -0.0033640307 -0.0033634445 -0.0033629481 -0.003362576 -0.0033628284 -0.0033632657][-0.0033779826 -0.0033747719 -0.0033735808 -0.0033729502 -0.0033724196 -0.0033718573 -0.0033713619 -0.0033712795 -0.0033712054 -0.003371191 -0.0033708564 -0.003370554 -0.0033703055 -0.0033705162 -0.0033709686][-0.0033854309 -0.0033825038 -0.0033810716 -0.0033804809 -0.0033800188 -0.0033792693 -0.00337881 -0.0033787377 -0.0033786902 -0.0033784304 -0.0033777959 -0.0033773796 -0.0033771573 -0.0033773496 -0.0033776355][-0.0033927148 -0.003390468 -0.0033890698 -0.0033883017 -0.0033875222 -0.0033865706 -0.00338595 -0.0033857927 -0.0033856332 -0.0033853417 -0.0033848009 -0.0033845815 -0.0033844491 -0.003384382 -0.0033845373]]...]
INFO - root - 2017-12-09 15:28:13.950028: step 36510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:57m:51s remains)
INFO - root - 2017-12-09 15:28:22.592897: step 36520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 68h:56m:08s remains)
INFO - root - 2017-12-09 15:28:31.047552: step 36530, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 72h:19m:06s remains)
INFO - root - 2017-12-09 15:28:39.620374: step 36540, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 66h:18m:37s remains)
INFO - root - 2017-12-09 15:28:47.839910: step 36550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:15m:10s remains)
INFO - root - 2017-12-09 15:28:56.497617: step 36560, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 72h:44m:16s remains)
INFO - root - 2017-12-09 15:29:05.136888: step 36570, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 70h:45m:06s remains)
INFO - root - 2017-12-09 15:29:13.794952: step 36580, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 68h:31m:52s remains)
INFO - root - 2017-12-09 15:29:22.564173: step 36590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 72h:35m:00s remains)
INFO - root - 2017-12-09 15:29:31.281825: step 36600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:20s remains)
2017-12-09 15:29:32.172742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033617681 -0.0033590663 -0.0033585839 -0.0033583485 -0.0033582267 -0.0033582649 -0.0033584349 -0.0033587182 -0.0033590537 -0.0033592638 -0.0033593965 -0.0033594752 -0.0033594966 -0.0033595241 -0.0033595113][-0.0033595941 -0.0033564549 -0.0033557182 -0.0033552595 -0.0033550067 -0.0033550002 -0.0033552253 -0.0033556633 -0.0033561965 -0.0033567464 -0.0033571688 -0.0033575003 -0.0033577145 -0.0033578086 -0.0033578495][-0.0033595681 -0.0033561329 -0.0033551143 -0.0033543091 -0.0033537704 -0.003353552 -0.0033537194 -0.0033542095 -0.0033549329 -0.0033557659 -0.0033564859 -0.0033570863 -0.00335749 -0.0033576891 -0.0033578193][-0.0033593632 -0.0033556586 -0.0033543315 -0.0033531524 -0.0033522744 -0.0033518397 -0.0033519305 -0.0033524397 -0.0033532938 -0.0033543909 -0.0033554414 -0.0033564044 -0.0033570547 -0.0033574083 -0.003357714][-0.0033591695 -0.0033553268 -0.003353803 -0.0033523603 -0.0033512202 -0.0033506178 -0.0033505608 -0.0033509815 -0.0033517794 -0.0033529261 -0.0033541603 -0.0033553718 -0.0033562663 -0.0033568593 -0.0033574114][-0.003359118 -0.0033551049 -0.0033534735 -0.0033518504 -0.0033506639 -0.0033500048 -0.00334983 -0.0033500693 -0.0033507352 -0.0033518295 -0.003352935 -0.0033540847 -0.0033550351 -0.0033557836 -0.0033565252][-0.0033590752 -0.0033551184 -0.0033535471 -0.0033519019 -0.0033507124 -0.0033499468 -0.0033495799 -0.003349592 -0.0033501028 -0.0033512071 -0.003352253 -0.0033532134 -0.0033540339 -0.0033548349 -0.0033557119][-0.003359172 -0.003355473 -0.0033541459 -0.0033527345 -0.0033516579 -0.0033508539 -0.0033501394 -0.0033497105 -0.0033498295 -0.0033507312 -0.0033517135 -0.0033525524 -0.0033533226 -0.0033542693 -0.003355281][-0.0033592118 -0.0033558276 -0.0033549108 -0.0033538279 -0.003353047 -0.0033523741 -0.0033515308 -0.0033508663 -0.0033506737 -0.0033511883 -0.0033518167 -0.0033524102 -0.0033530663 -0.0033539494 -0.0033548533][-0.0033592836 -0.0033560393 -0.0033554689 -0.0033546621 -0.0033541459 -0.0033536681 -0.0033528646 -0.0033522004 -0.0033519126 -0.0033521741 -0.0033524006 -0.0033525305 -0.0033528393 -0.0033534928 -0.0033541559][-0.0033593262 -0.0033561636 -0.0033558756 -0.0033552474 -0.0033548246 -0.0033543725 -0.0033535969 -0.0033529405 -0.0033526581 -0.0033527822 -0.0033527978 -0.0033526483 -0.0033526793 -0.0033531487 -0.0033536381][-0.003359409 -0.0033563362 -0.0033562304 -0.0033557364 -0.0033553031 -0.0033548395 -0.0033540409 -0.0033533692 -0.0033530444 -0.0033531117 -0.0033531538 -0.0033531096 -0.0033532842 -0.0033537375 -0.0033541161][-0.0033596447 -0.0033565613 -0.003356636 -0.0033563282 -0.0033559871 -0.0033555951 -0.0033548886 -0.00335422 -0.0033538754 -0.0033538155 -0.0033537687 -0.0033537366 -0.0033539499 -0.0033543385 -0.0033545473][-0.0033598826 -0.003356741 -0.0033569112 -0.0033567331 -0.0033565247 -0.0033562705 -0.003355779 -0.0033552691 -0.0033549564 -0.0033547746 -0.0033545324 -0.0033543715 -0.0033544055 -0.003354578 -0.0033546677][-0.0033601348 -0.00335691 -0.0033570402 -0.0033569103 -0.0033567825 -0.0033566563 -0.0033563552 -0.0033560342 -0.0033557811 -0.0033555876 -0.003355334 -0.0033551343 -0.0033550931 -0.0033551995 -0.0033553122]]...]
INFO - root - 2017-12-09 15:29:40.837239: step 36610, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 73h:44m:51s remains)
INFO - root - 2017-12-09 15:29:49.471422: step 36620, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:25m:18s remains)
INFO - root - 2017-12-09 15:29:57.663150: step 36630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:19m:54s remains)
INFO - root - 2017-12-09 15:30:06.398616: step 36640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:35m:46s remains)
INFO - root - 2017-12-09 15:30:14.928820: step 36650, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 73h:05m:31s remains)
INFO - root - 2017-12-09 15:30:23.621922: step 36660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:02m:07s remains)
INFO - root - 2017-12-09 15:30:32.411486: step 36670, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 71h:15m:41s remains)
INFO - root - 2017-12-09 15:30:41.174744: step 36680, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 72h:35m:55s remains)
INFO - root - 2017-12-09 15:30:49.969331: step 36690, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:03m:19s remains)
INFO - root - 2017-12-09 15:30:58.755969: step 36700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:42m:52s remains)
2017-12-09 15:30:59.691032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033946668 -0.0033928766 -0.0033925059 -0.0033921243 -0.0033915157 -0.0033909474 -0.0033908356 -0.0033909106 -0.0033906994 -0.0033905467 -0.0033904775 -0.0033905425 -0.0033903476 -0.003390217 -0.0033903869][-0.00339543 -0.0033930568 -0.0033921977 -0.0033917259 -0.0033909073 -0.0033900258 -0.0033899634 -0.0033902193 -0.0033901895 -0.00339 -0.003390006 -0.003389779 -0.0033891955 -0.0033885106 -0.003388254][-0.0033970445 -0.0033944924 -0.003393502 -0.0033927939 -0.0033920805 -0.0033911243 -0.0033909383 -0.0033910505 -0.0033906731 -0.0033903273 -0.0033903869 -0.0033900826 -0.0033890947 -0.0033880556 -0.0033876267][-0.0033975332 -0.0033949264 -0.0033941942 -0.0033935176 -0.0033929148 -0.0033919273 -0.0033917215 -0.0033918652 -0.003391454 -0.0033913341 -0.0033914265 -0.0033910205 -0.003390016 -0.0033884896 -0.0033873129][-0.0033985183 -0.0033955255 -0.0033947276 -0.003394502 -0.0033941532 -0.0033929767 -0.0033923413 -0.0033920861 -0.0033915138 -0.0033914105 -0.0033917702 -0.0033923874 -0.0033920004 -0.0033904666 -0.0033888109][-0.0034011153 -0.003398163 -0.0033973949 -0.00339748 -0.0033968217 -0.0033951981 -0.003394075 -0.0033934226 -0.0033928533 -0.0033931213 -0.0033942033 -0.003395353 -0.0033950449 -0.0033934966 -0.0033915346][-0.0034039982 -0.0034018115 -0.0034011775 -0.0034009004 -0.0033999688 -0.0033986294 -0.0033975106 -0.0033970359 -0.0033966627 -0.0033969511 -0.0033980331 -0.0033988557 -0.0033983523 -0.0033963018 -0.0033936037][-0.00340547 -0.0034041994 -0.00340382 -0.0034034438 -0.0034032175 -0.0034028452 -0.0034024795 -0.003402666 -0.0034024289 -0.0034022692 -0.0034024704 -0.0034025235 -0.0034014413 -0.0033991083 -0.003395736][-0.0034053177 -0.0034040154 -0.0034037551 -0.0034035016 -0.0034040015 -0.0034048439 -0.0034057244 -0.0034068972 -0.0034074374 -0.0034072543 -0.0034069528 -0.0034063016 -0.0034050406 -0.003402689 -0.0033993735][-0.0034054504 -0.0034041163 -0.0034039454 -0.0034037824 -0.0034043398 -0.0034058914 -0.0034079112 -0.0034101405 -0.0034109717 -0.0034113168 -0.0034109131 -0.0034102059 -0.003408818 -0.0034062814 -0.0034027661][-0.0034048751 -0.0034038196 -0.0034038951 -0.0034039468 -0.0034044653 -0.0034063146 -0.0034094115 -0.0034125908 -0.0034142411 -0.0034148234 -0.0034143475 -0.0034132965 -0.0034111654 -0.0034079137 -0.0034040641][-0.0034031882 -0.0034027991 -0.0034033072 -0.0034035814 -0.0034040979 -0.0034056669 -0.0034088758 -0.0034124393 -0.0034150949 -0.0034161261 -0.0034155005 -0.0034139287 -0.0034110153 -0.0034072248 -0.0034029831][-0.0033999661 -0.0033994152 -0.003399984 -0.0034007179 -0.003401794 -0.0034035156 -0.003406585 -0.0034101326 -0.0034130116 -0.0034141922 -0.0034137091 -0.0034120723 -0.0034092879 -0.0034054965 -0.0034012322][-0.0033963507 -0.0033955469 -0.003396211 -0.003396672 -0.0033978985 -0.0034001532 -0.0034032159 -0.0034068651 -0.0034097421 -0.0034110295 -0.0034108171 -0.0034096972 -0.0034074206 -0.0034039023 -0.0033999139][-0.00339474 -0.0033936452 -0.0033940519 -0.0033943444 -0.0033955218 -0.0033976161 -0.00340033 -0.0034035542 -0.00340617 -0.003407584 -0.0034077473 -0.0034069146 -0.0034050432 -0.0034021041 -0.0033986592]]...]
INFO - root - 2017-12-09 15:31:08.365222: step 36710, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 72h:55m:32s remains)
INFO - root - 2017-12-09 15:31:16.979864: step 36720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:27m:10s remains)
INFO - root - 2017-12-09 15:31:25.345133: step 36730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 68h:08m:01s remains)
INFO - root - 2017-12-09 15:31:34.058959: step 36740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:19m:25s remains)
INFO - root - 2017-12-09 15:31:42.576033: step 36750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:27m:16s remains)
INFO - root - 2017-12-09 15:31:51.233855: step 36760, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 71h:29m:59s remains)
INFO - root - 2017-12-09 15:31:59.961191: step 36770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:59m:47s remains)
INFO - root - 2017-12-09 15:32:08.598477: step 36780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:44m:02s remains)
INFO - root - 2017-12-09 15:32:17.323784: step 36790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:07m:08s remains)
INFO - root - 2017-12-09 15:32:25.926161: step 36800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:54m:18s remains)
2017-12-09 15:32:26.739783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033867212 -0.0033858123 -0.0033861499 -0.0033865497 -0.0033868239 -0.0033870484 -0.0033872782 -0.0033875247 -0.0033876803 -0.0033875587 -0.0033874938 -0.0033873694 -0.0033872961 -0.0033872274 -0.0033870998][-0.0033849031 -0.0033837399 -0.003384066 -0.0033844907 -0.0033848784 -0.0033851394 -0.0033853645 -0.0033853243 -0.0033851054 -0.003384843 -0.0033850428 -0.0033853536 -0.003385663 -0.0033857261 -0.0033855941][-0.0033848872 -0.0033837727 -0.0033841811 -0.003383691 -0.0033779894 -0.0033592468 -0.0033249422 -0.0032884497 -0.0032744869 -0.0032939643 -0.0033327688 -0.003365441 -0.0033813745 -0.0033850297 -0.0033853767][-0.0033844663 -0.0033836202 -0.0033833298 -0.0033729477 -0.00332375 -0.0031955943 -0.0029860372 -0.0027767585 -0.0026971761 -0.0028041753 -0.0030246843 -0.0032266788 -0.0033402639 -0.0033777093 -0.0033844772][-0.0033841571 -0.0033836234 -0.0033809366 -0.0033474704 -0.0032076817 -0.0028636064 -0.0023187613 -0.0017836096 -0.0015779433 -0.0018445812 -0.0024073767 -0.0029381544 -0.0032492981 -0.0033605394 -0.0033827557][-0.003384341 -0.0033838281 -0.0033774041 -0.0033135496 -0.0030586524 -0.0024455679 -0.0014842105 -0.00053804577 -0.00015942659 -0.0006090703 -0.0015951352 -0.0025484199 -0.0031225011 -0.0033365975 -0.0033808106][-0.0033856768 -0.0033852018 -0.0033757449 -0.0032907031 -0.0029548716 -0.0021518648 -0.00089029595 0.00036314502 0.00088561373 0.00031101727 -0.00098388665 -0.0022529927 -0.0030249176 -0.0033174776 -0.0033786916][-0.0033875166 -0.0033869364 -0.003376262 -0.0032872211 -0.0029369788 -0.0020957515 -0.00076329708 0.00057263393 0.001144832 0.00054640742 -0.00082636205 -0.0021794792 -0.0030015002 -0.00331299 -0.003377571][-0.0033887937 -0.0033887534 -0.0033806462 -0.0033076773 -0.0030140518 -0.0023008506 -0.001160834 -1.4499063e-05 0.00047902716 -3.2247743e-05 -0.0012102164 -0.0023699519 -0.0030662862 -0.0033261126 -0.0033787824][-0.003388989 -0.0033895026 -0.0033854919 -0.0033387835 -0.0031400044 -0.002646138 -0.0018461067 -0.0010371322 -0.0006894921 -0.0010561286 -0.0018898258 -0.0027006236 -0.0031768482 -0.0033489605 -0.003381887][-0.0033879138 -0.0033883932 -0.0033881657 -0.0033656557 -0.0032585252 -0.0029836872 -0.0025323424 -0.0020734617 -0.0018782136 -0.0020916993 -0.0025676065 -0.0030219594 -0.0032814883 -0.0033702077 -0.0033852255][-0.003386145 -0.0033864109 -0.0033882372 -0.0033814793 -0.0033377695 -0.0032178704 -0.0030161678 -0.002809739 -0.0027232734 -0.0028228115 -0.0030380085 -0.0032380575 -0.0033482213 -0.0033831545 -0.003387497][-0.0033848477 -0.0033847326 -0.0033868928 -0.00338768 -0.003377269 -0.0033410538 -0.0032761598 -0.0032091788 -0.0031813451 -0.0032134254 -0.0032822331 -0.0033447768 -0.0033783207 -0.0033878894 -0.003388009][-0.003384775 -0.003384321 -0.0033860588 -0.003388257 -0.0033890465 -0.0033844614 -0.0033725447 -0.00335864 -0.0033519622 -0.0033573257 -0.0033699335 -0.0033815098 -0.0033875266 -0.0033886898 -0.0033878428][-0.0033848861 -0.0033843981 -0.0033856235 -0.0033871778 -0.0033890887 -0.0033906081 -0.0033908105 -0.0033897748 -0.0033887571 -0.0033885005 -0.0033887541 -0.0033889762 -0.0033888188 -0.0033881078 -0.0033870772]]...]
INFO - root - 2017-12-09 15:32:35.477059: step 36810, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 73h:44m:42s remains)
INFO - root - 2017-12-09 15:32:44.114924: step 36820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:39m:48s remains)
INFO - root - 2017-12-09 15:32:52.489737: step 36830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:16m:34s remains)
INFO - root - 2017-12-09 15:33:01.238781: step 36840, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:10m:04s remains)
INFO - root - 2017-12-09 15:33:09.654082: step 36850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:41m:08s remains)
INFO - root - 2017-12-09 15:33:18.107968: step 36860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:17m:27s remains)
INFO - root - 2017-12-09 15:33:26.743235: step 36870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:48m:24s remains)
INFO - root - 2017-12-09 15:33:35.426221: step 36880, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 69h:10m:45s remains)
INFO - root - 2017-12-09 15:33:44.075468: step 36890, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:39m:31s remains)
INFO - root - 2017-12-09 15:33:52.704015: step 36900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:46m:25s remains)
2017-12-09 15:33:53.626390: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38034877 0.37107196 0.36041385 0.34913635 0.33859265 0.33016595 0.323824 0.31873122 0.31566429 0.31283081 0.30984795 0.30883896 0.30889803 0.31039059 0.31132889][0.38139564 0.37434578 0.36586055 0.35648951 0.34648627 0.33898497 0.33325425 0.3289074 0.32582098 0.32393026 0.32243243 0.32153335 0.32159337 0.32244125 0.32325596][0.36426607 0.36131504 0.356622 0.3507539 0.34407622 0.33898005 0.33454326 0.33060804 0.32683951 0.32515204 0.3235893 0.32315657 0.32309076 0.32359785 0.32402509][0.34756935 0.34815574 0.3473224 0.34529772 0.34245655 0.33986303 0.33662471 0.33350781 0.32951084 0.3271805 0.32470191 0.32382202 0.32337779 0.32314724 0.32310927][0.33238509 0.3364346 0.33872703 0.339841 0.34143293 0.3419137 0.33991626 0.33661255 0.33133671 0.32760853 0.3232519 0.32055441 0.31817979 0.31657782 0.31601238][0.31722882 0.32335982 0.32777146 0.33177796 0.33618054 0.3384681 0.33770734 0.33420882 0.32751784 0.3220216 0.31554437 0.31027567 0.30523783 0.30158603 0.29978663][0.30113557 0.30769187 0.31163388 0.31653023 0.32207569 0.32559377 0.32549971 0.32136419 0.31337249 0.30502698 0.29590753 0.28788593 0.28002235 0.27451813 0.2723532][0.28398779 0.29017249 0.29253945 0.29607973 0.30066827 0.30384094 0.30352804 0.29808292 0.28898159 0.27809674 0.26607537 0.25428784 0.24281488 0.2359156 0.23308375][0.26041427 0.26469821 0.26505402 0.26639611 0.2684817 0.26992449 0.26830626 0.26156318 0.25073934 0.23760931 0.22342378 0.20905213 0.1955072 0.18715994 0.18429472][0.22811395 0.23029709 0.22787815 0.22643021 0.22561869 0.22398235 0.22002724 0.21153189 0.19989537 0.1861292 0.17159626 0.15696134 0.14383784 0.13575545 0.13386251][0.18494882 0.18434219 0.17947376 0.17535581 0.17166899 0.16826464 0.16330151 0.1545199 0.1432696 0.13071187 0.11807615 0.10535047 0.094704516 0.089368291 0.090064235][0.13879818 0.13565288 0.12889938 0.12339345 0.11855353 0.11386961 0.1083386 0.1006543 0.091551356 0.081313446 0.071461678 0.061947636 0.055020358 0.053831317 0.058507673][0.093485974 0.08955802 0.083122917 0.077581182 0.072430849 0.067569725 0.062482849 0.056130495 0.049204275 0.042190265 0.036158435 0.030531 0.027867731 0.031810429 0.041787904][0.0540331 0.049704835 0.044407148 0.040000748 0.03585754 0.032135636 0.028513245 0.024373373 0.020190503 0.016181676 0.013342071 0.011639118 0.013281995 0.022577297 0.038377002][0.0257955 0.022008205 0.01824051 0.015217861 0.01260918 0.010106919 0.0080295261 0.0061802682 0.0043999492 0.0028455989 0.0023545218 0.0037759296 0.00941908 0.023645954 0.045482285]]...]
INFO - root - 2017-12-09 15:34:02.227739: step 36910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:15m:58s remains)
INFO - root - 2017-12-09 15:34:10.783030: step 36920, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 67h:23m:45s remains)
INFO - root - 2017-12-09 15:34:18.965423: step 36930, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 70h:38m:57s remains)
INFO - root - 2017-12-09 15:34:27.687264: step 36940, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 69h:18m:50s remains)
INFO - root - 2017-12-09 15:34:36.147579: step 36950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:33m:52s remains)
INFO - root - 2017-12-09 15:34:44.853526: step 36960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 69h:56m:30s remains)
INFO - root - 2017-12-09 15:34:53.558083: step 36970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:06m:14s remains)
INFO - root - 2017-12-09 15:35:02.195312: step 36980, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 72h:16m:01s remains)
INFO - root - 2017-12-09 15:35:10.866343: step 36990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:07m:30s remains)
INFO - root - 2017-12-09 15:35:19.338129: step 37000, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:16m:11s remains)
2017-12-09 15:35:20.265246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010683019 0.019525468 0.028098956 0.033657171 0.036058959 0.034406863 0.030805245 0.025285771 0.019096248 0.013106802 0.0081921378 0.0055283653 0.0050237356 0.0056215012 0.007598564][0.0086772814 0.017423121 0.026525844 0.03241178 0.03471389 0.033202726 0.028907578 0.022666877 0.015618694 0.0091465292 0.0043067206 0.0019129694 0.0014193414 0.0018999099 0.0037320948][0.006236312 0.014436384 0.023380971 0.029929191 0.032801338 0.031530462 0.026945045 0.02018345 0.012555943 0.0056874137 0.00092393276 -0.0011790553 -0.0014434783 -0.0012495299 -7.1144663e-05][0.0049079815 0.012668912 0.021688342 0.028791644 0.031895798 0.030604444 0.025807746 0.018686352 0.010782312 0.0037733756 -0.00082673971 -0.0026559909 -0.0029326775 -0.0028042386 -0.002236065][0.0048318664 0.012566109 0.021292044 0.028561015 0.031730447 0.030248445 0.025091078 0.017663829 0.0097253937 0.0028450966 -0.0014909217 -0.003134992 -0.0033419703 -0.0032916854 -0.0031475513][0.005256528 0.013304315 0.022099227 0.028887931 0.031442374 0.029391669 0.023966311 0.016364397 0.0086953882 0.0022863282 -0.0016732544 -0.0031694414 -0.003357549 -0.003361349 -0.0033505789][0.0058946358 0.013900953 0.022327911 0.028272443 0.030043829 0.027258029 0.021534815 0.014221362 0.0072919074 0.0016232126 -0.0018312623 -0.0031751033 -0.0033557909 -0.0033607944 -0.0033583029][0.0060901139 0.013327566 0.020744931 0.025267437 0.026115604 0.022863492 0.017312147 0.010829763 0.0050648944 0.00059517589 -0.0021168615 -0.0031991298 -0.0033586461 -0.0033612787 -0.0033586232][0.0052475026 0.011068778 0.016671557 0.019606074 0.019608855 0.016369009 0.011681537 0.0066376794 0.0024010537 -0.000735431 -0.0025647597 -0.0032606968 -0.0033623183 -0.0033643637 -0.0033619939][0.0032997893 0.007323171 0.01097728 0.012555433 0.012002589 0.0092588421 0.0058012949 0.0024460687 -0.00017934898 -0.0020211346 -0.0030132313 -0.0033311073 -0.003367105 -0.003367953 -0.0033649288][0.00090421387 0.0032644814 0.0053204317 0.0060282145 0.0053213239 0.00333284 0.0011326454 -0.00076425425 -0.0020707315 -0.0028837575 -0.0032633743 -0.0033603706 -0.0033715288 -0.0033718366 -0.00336679][-0.0012130039 1.3061799e-05 0.001030179 0.0012803094 0.00070422865 -0.00049084052 -0.0016659047 -0.0025411271 -0.0030218535 -0.0032662896 -0.0033588873 -0.0033740976 -0.0033762327 -0.0033748318 -0.0033692517][-0.0026172092 -0.0020907538 -0.0016347348 -0.0015792698 -0.0019183158 -0.0024767797 -0.0029569336 -0.0032441202 -0.0033478139 -0.003376415 -0.0033794916 -0.0033786891 -0.0033784423 -0.0033759919 -0.0033720538][-0.003234905 -0.0030841939 -0.0029470716 -0.0029305308 -0.0030384066 -0.003206152 -0.0033237224 -0.0033678871 -0.0033740008 -0.0033788467 -0.003379625 -0.0033794479 -0.0033782846 -0.0033770176 -0.0033742413][-0.0033876209 -0.0033765188 -0.0033642491 -0.0033613679 -0.0033658727 -0.003370998 -0.0033726119 -0.003374845 -0.0033763612 -0.0033787861 -0.0033797922 -0.0033802895 -0.0033794942 -0.0033780483 -0.0033756935]]...]
INFO - root - 2017-12-09 15:35:28.988549: step 37010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:26m:02s remains)
INFO - root - 2017-12-09 15:35:37.512832: step 37020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:58m:21s remains)
INFO - root - 2017-12-09 15:35:45.708218: step 37030, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:05m:35s remains)
INFO - root - 2017-12-09 15:35:54.193699: step 37040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:12m:09s remains)
INFO - root - 2017-12-09 15:36:02.654150: step 37050, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 69h:47m:05s remains)
INFO - root - 2017-12-09 15:36:11.391623: step 37060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:13m:53s remains)
INFO - root - 2017-12-09 15:36:20.057730: step 37070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:11m:45s remains)
INFO - root - 2017-12-09 15:36:28.698525: step 37080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:43m:37s remains)
INFO - root - 2017-12-09 15:36:37.347997: step 37090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:58m:12s remains)
INFO - root - 2017-12-09 15:36:46.056081: step 37100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:32m:01s remains)
2017-12-09 15:36:46.924662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033968529 -0.0033940845 -0.0033928093 -0.0033922568 -0.0033920498 -0.0033920752 -0.0033923336 -0.0033924559 -0.0033925436 -0.0033926878 -0.0033927045 -0.0033924771 -0.0033921837 -0.0033919918 -0.003391854][-0.0033956536 -0.0033924761 -0.003390895 -0.0033902535 -0.0033901576 -0.003390243 -0.0033905325 -0.0033907192 -0.0033910074 -0.00339138 -0.0033915457 -0.0033914121 -0.0033911727 -0.0033908866 -0.0033905813][-0.0033961884 -0.0033931872 -0.0033915762 -0.0033908251 -0.0033908407 -0.0033909534 -0.0033913008 -0.0033915909 -0.0033918603 -0.0033921229 -0.0033923269 -0.0033923744 -0.0033921686 -0.0033918465 -0.0033914009][-0.0033959434 -0.0033930645 -0.0033919041 -0.0033913716 -0.0033909401 -0.003390501 -0.0033903536 -0.0033904838 -0.0033908642 -0.0033916216 -0.0033924328 -0.0033930021 -0.0033931278 -0.0033927755 -0.0033923003][-0.003395288 -0.0033924324 -0.0033908556 -0.0033894049 -0.003387871 -0.0033861643 -0.003384741 -0.0033842176 -0.0033850425 -0.0033871874 -0.0033897611 -0.0033918747 -0.0033930703 -0.0033932668 -0.0033930398][-0.0033943383 -0.0033906882 -0.0033869704 -0.0033824858 -0.0033778334 -0.0033729037 -0.0033692943 -0.003367957 -0.0033699006 -0.0033749377 -0.0033811608 -0.0033865403 -0.0033901236 -0.0033918237 -0.0033920864][-0.003393722 -0.0033872859 -0.0033791976 -0.0033690827 -0.0033589513 -0.0033500432 -0.0033444536 -0.0033435586 -0.0033474527 -0.0033563389 -0.0033676829 -0.0033778283 -0.0033845375 -0.0033873057 -0.003387871][-0.0033916056 -0.0033808502 -0.0033668054 -0.0033509128 -0.0033370464 -0.0033272002 -0.0033227724 -0.0033240779 -0.0033307835 -0.0033428653 -0.0033576663 -0.0033709346 -0.0033793349 -0.003383175 -0.0033839012][-0.0033885303 -0.003373459 -0.0033549224 -0.0033372049 -0.0033242798 -0.0033164609 -0.00331429 -0.0033181699 -0.0033273969 -0.003341502 -0.0033572987 -0.0033709018 -0.0033790576 -0.0033820826 -0.0033813925][-0.0033868379 -0.0033700243 -0.0033513664 -0.0033345567 -0.0033223976 -0.0033154178 -0.0033146639 -0.0033205645 -0.0033325255 -0.0033484257 -0.0033642889 -0.0033761086 -0.0033829452 -0.0033853187 -0.0033843745][-0.00338892 -0.0033730043 -0.0033554938 -0.0033392571 -0.0033271431 -0.0033206607 -0.0033219696 -0.0033296631 -0.0033421954 -0.0033575858 -0.0033723207 -0.00338305 -0.00338931 -0.0033909383 -0.0033896845][-0.0033912684 -0.0033774388 -0.0033622915 -0.0033474383 -0.0033359686 -0.0033303832 -0.0033322193 -0.0033402115 -0.003352022 -0.0033659716 -0.0033791182 -0.0033887469 -0.0033941264 -0.0033954489 -0.003394455][-0.0033898014 -0.0033787026 -0.0033665798 -0.0033542169 -0.0033445214 -0.0033401796 -0.0033419817 -0.0033486651 -0.0033588547 -0.0033700755 -0.0033801743 -0.0033874584 -0.0033913716 -0.0033926291 -0.0033920272][-0.0033820146 -0.0033737794 -0.003364952 -0.0033560763 -0.0033493743 -0.0033463291 -0.0033471687 -0.0033514076 -0.0033581 -0.0033651656 -0.0033713318 -0.0033761137 -0.0033789519 -0.0033798763 -0.0033795624][-0.003369743 -0.003363905 -0.0033577345 -0.0033515212 -0.0033465086 -0.0033438427 -0.0033438106 -0.0033456287 -0.0033491785 -0.0033533978 -0.0033573473 -0.0033600205 -0.0033618815 -0.003363074 -0.0033636331]]...]
INFO - root - 2017-12-09 15:36:55.718146: step 37110, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:20m:44s remains)
INFO - root - 2017-12-09 15:37:04.441726: step 37120, loss = 0.88, batch loss = 0.67 (9.5 examples/sec; 0.846 sec/batch; 69h:23m:53s remains)
INFO - root - 2017-12-09 15:37:12.995428: step 37130, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:45m:11s remains)
INFO - root - 2017-12-09 15:37:21.752133: step 37140, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 73h:40m:24s remains)
INFO - root - 2017-12-09 15:37:30.226941: step 37150, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 68h:27m:49s remains)
INFO - root - 2017-12-09 15:37:38.793658: step 37160, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 70h:00m:28s remains)
INFO - root - 2017-12-09 15:37:47.213859: step 37170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:17m:57s remains)
INFO - root - 2017-12-09 15:37:55.896405: step 37180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:16m:20s remains)
INFO - root - 2017-12-09 15:38:04.585346: step 37190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:38m:41s remains)
INFO - root - 2017-12-09 15:38:13.277040: step 37200, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 73h:30m:32s remains)
2017-12-09 15:38:14.163423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033420958 -0.0033789608 -0.0033800176 -0.0033862863 -0.0033861457 -0.0033857583 -0.0033857215 -0.0033860328 -0.0033696194 -0.0032979355 -0.0031249519 -0.0029497014 -0.0028297366 -0.0028483947 -0.0030442234][-0.0032744075 -0.0033200022 -0.0033189987 -0.0033691018 -0.0033725225 -0.0033835056 -0.0033838882 -0.0033826565 -0.0033278794 -0.0031838054 -0.0028991548 -0.0026460097 -0.0024818757 -0.0025418848 -0.0028211758][-0.0032038928 -0.0032499742 -0.0032418161 -0.0033022696 -0.0033072203 -0.0033631925 -0.0033645718 -0.0033656883 -0.0032268865 -0.0029434673 -0.0024790289 -0.0021203081 -0.0018952447 -0.0019953111 -0.0023498731][-0.0031452011 -0.0031766319 -0.0031554382 -0.0032217754 -0.00322044 -0.0032913506 -0.003289632 -0.0032941732 -0.0030181403 -0.0025351876 -0.0018426264 -0.0013429001 -0.0010360291 -0.0011590209 -0.0015738022][-0.00312096 -0.0031094151 -0.0030814819 -0.0031314786 -0.0031225788 -0.0032066135 -0.0031803478 -0.0031165949 -0.0026510369 -0.0019640368 -0.0010437153 -0.00037554209 2.69101e-05 -0.00010007131 -0.00056666066][-0.0031289558 -0.0030699221 -0.0030415445 -0.0030628908 -0.0030743903 -0.0031252783 -0.0030599388 -0.0028718668 -0.0021975047 -0.001281827 -0.00018085493 0.00063276151 0.001127217 0.0010339967 0.00052989833][-0.0031670392 -0.0030705414 -0.0030260438 -0.0030253283 -0.0030725838 -0.0030912515 -0.0029845727 -0.0026310869 -0.0017727989 -0.00065509393 0.00054588635 0.0014906735 0.0020410479 0.0019830111 0.00143955][-0.0032087916 -0.0031076865 -0.0030571886 -0.0030180665 -0.0030846924 -0.0030873015 -0.0029594775 -0.0024586315 -0.0014972385 -0.00024169241 0.00096644182 0.0020080637 0.0025683017 0.0025424352 0.0019424874][-0.0032569985 -0.0031655931 -0.00310247 -0.0030593493 -0.0031192731 -0.0030980422 -0.0029734294 -0.0024088626 -0.001431149 -0.00012049382 0.0010163006 0.0020766156 0.0025973525 0.0025968356 0.0019547949][-0.0033032533 -0.0032351781 -0.0031722623 -0.00311974 -0.0031535153 -0.0031346735 -0.0030105163 -0.0024594183 -0.0015630262 -0.00031032949 0.00071199751 0.0017238995 0.0021652239 0.0021526068 0.0014964598][-0.003340794 -0.0033004105 -0.0032450063 -0.0031946765 -0.0031977696 -0.003175895 -0.0030654694 -0.0025967262 -0.0018312375 -0.0007335539 0.00013176259 0.0010274015 0.0013634055 0.0013308069 0.00069503114][-0.0033667183 -0.0033463498 -0.0033070017 -0.0032656714 -0.003240109 -0.0032274695 -0.0031336932 -0.0027743736 -0.0021724934 -0.0013075089 -0.000616743 0.00010369695 0.00033705402 0.00027635507 -0.00029930426][-0.003379541 -0.0033705004 -0.0033506933 -0.0033216057 -0.0032914404 -0.003277322 -0.0032052777 -0.0029579629 -0.0025279836 -0.0019204547 -0.0014151253 -0.00089402543 -0.00074493722 -0.00082252966 -0.0012965202][-0.0033848155 -0.00338074 -0.0033704827 -0.0033563504 -0.0033360361 -0.0033252314 -0.0032783314 -0.0031249737 -0.0028514392 -0.0024751942 -0.0021446934 -0.0018139657 -0.0017261154 -0.0018040017 -0.0021468906][-0.0033854831 -0.003382815 -0.0033798714 -0.0033704985 -0.0033595362 -0.0033569478 -0.0033341772 -0.003252449 -0.003106032 -0.0029052366 -0.0027187732 -0.0025348992 -0.0024872359 -0.0025516408 -0.0027660623]]...]
INFO - root - 2017-12-09 15:38:22.767467: step 37210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:07m:07s remains)
INFO - root - 2017-12-09 15:38:31.403560: step 37220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 70h:24m:03s remains)
INFO - root - 2017-12-09 15:38:39.896993: step 37230, loss = 0.89, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 63h:07m:59s remains)
INFO - root - 2017-12-09 15:38:48.648739: step 37240, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 69h:18m:02s remains)
INFO - root - 2017-12-09 15:38:57.145089: step 37250, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 61h:55m:23s remains)
INFO - root - 2017-12-09 15:39:05.750768: step 37260, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 68h:02m:21s remains)
INFO - root - 2017-12-09 15:39:14.252811: step 37270, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 69h:05m:51s remains)
INFO - root - 2017-12-09 15:39:22.941410: step 37280, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 67h:40m:00s remains)
INFO - root - 2017-12-09 15:39:31.636019: step 37290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:48m:44s remains)
INFO - root - 2017-12-09 15:39:40.092279: step 37300, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:41m:12s remains)
2017-12-09 15:39:40.939852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001382615 -0.0016418737 -0.0018138652 -0.0021142419 -0.0023113824 -0.0025682673 -0.0027542994 -0.0027222922 -0.002647165 -0.0023870154 -0.0021248348 -0.0019527806 -0.0019655167 -0.002099527 -0.0022816998][0.0016570466 0.0016842799 0.0017318178 0.0014654181 0.0012188603 0.00073303492 0.00036590453 1.3482757e-05 -0.00013632863 2.3938948e-05 0.00018735905 0.00032805488 0.00020271656 -0.00017188513 -0.00068840291][0.0043584257 0.0045805695 0.0048278449 0.0048681395 0.0049299123 0.0047659343 0.0047001326 0.0042427331 0.0039380658 0.0038097 0.003520543 0.0031007116 0.00231137 0.0014027541 0.00049525476][0.006781986 0.007716951 0.0087418761 0.0095331995 0.010368203 0.01090443 0.011301271 0.011124426 0.010730364 0.010124794 0.009017989 0.00738691 0.0053210612 0.0034623255 0.0017905405][0.0094307633 0.011283102 0.013715873 0.016209725 0.018760391 0.020923713 0.02250527 0.022698492 0.021893539 0.020529648 0.01826198 0.015061641 0.011444056 0.0079349214 0.0047032842][0.010936617 0.014553281 0.019240398 0.024002854 0.028753027 0.033123132 0.036306366 0.037430346 0.036549065 0.03457614 0.0313648 0.026839621 0.02150872 0.015755925 0.010513622][0.01367549 0.019875212 0.027616803 0.03502785 0.041902296 0.047921058 0.052019119 0.053412352 0.052111607 0.049712975 0.0460362 0.040903389 0.034460988 0.026991881 0.019850142][0.018683685 0.027316632 0.037305467 0.046303798 0.054321378 0.061400585 0.066014841 0.067553096 0.066084951 0.063364647 0.059331357 0.053766221 0.046447083 0.037976697 0.029706053][0.02314662 0.033113807 0.044034231 0.053532537 0.061801448 0.0689378 0.073629983 0.075578459 0.074415043 0.071785428 0.067735694 0.061934426 0.054497011 0.045878526 0.037105888][0.027351351 0.036591783 0.046242461 0.05447641 0.06147334 0.067563377 0.071879022 0.074093908 0.073608086 0.071801111 0.068492211 0.063438244 0.056317307 0.048295848 0.040283885][0.030014765 0.036913671 0.043702465 0.049106 0.053868175 0.058102239 0.061781213 0.064042017 0.064234249 0.063303247 0.060902841 0.057057876 0.051216234 0.044785567 0.038621351][0.032754093 0.036162116 0.03889202 0.04069889 0.04271296 0.045050092 0.0477083 0.05001539 0.050954584 0.051008757 0.049818188 0.04706575 0.042724021 0.037963349 0.03391473][0.035906151 0.0360875 0.035051487 0.033536404 0.033234015 0.034091771 0.036189627 0.038643356 0.040119834 0.040984474 0.0407222 0.038747635 0.035582919 0.032254338 0.030028991][0.038705476 0.036730435 0.033725023 0.030758798 0.029854506 0.030579362 0.032710854 0.035300247 0.037036318 0.038193945 0.038055152 0.036381882 0.033563718 0.030911684 0.029407825][0.04228656 0.039376382 0.035672609 0.033038411 0.033014014 0.03475488 0.03765405 0.040733691 0.042732798 0.043678511 0.043014936 0.040767085 0.037501708 0.034754913 0.032913253]]...]
INFO - root - 2017-12-09 15:39:49.447923: step 37310, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 68h:25m:29s remains)
INFO - root - 2017-12-09 15:39:58.311498: step 37320, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 69h:18m:29s remains)
INFO - root - 2017-12-09 15:40:06.774642: step 37330, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 52h:27m:16s remains)
INFO - root - 2017-12-09 15:40:15.508485: step 37340, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:32m:55s remains)
INFO - root - 2017-12-09 15:40:24.343576: step 37350, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:52m:01s remains)
INFO - root - 2017-12-09 15:40:32.716002: step 37360, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 72h:01m:01s remains)
INFO - root - 2017-12-09 15:40:41.292804: step 37370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:18m:13s remains)
INFO - root - 2017-12-09 15:40:49.887752: step 37380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:33m:40s remains)
INFO - root - 2017-12-09 15:40:58.569429: step 37390, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:55m:09s remains)
INFO - root - 2017-12-09 15:41:07.292191: step 37400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:40m:51s remains)
2017-12-09 15:41:08.221077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033289515 -0.003331753 -0.003332404 -0.0033386399 -0.0033389302 -0.0033399332 -0.0033351365 -0.0033298365 -0.0033257285 -0.0033243818 -0.003321493 -0.0033187729 -0.0033176469 -0.0033294964 -0.0033437151][-0.00332718 -0.0033370217 -0.0033468143 -0.0033481347 -0.0033456313 -0.0033463594 -0.0033407498 -0.0033392892 -0.0033347569 -0.0033313662 -0.0033238376 -0.00331627 -0.0033118431 -0.0033209031 -0.0033315474][-0.0033320982 -0.0033431253 -0.0033543897 -0.0033612708 -0.0033620973 -0.0033583122 -0.003353229 -0.0033507729 -0.0033462259 -0.0033436886 -0.003332227 -0.0033181964 -0.0033051968 -0.0033035388 -0.0033089649][-0.0033442569 -0.0033501938 -0.0033539387 -0.0033588402 -0.003361393 -0.0033610375 -0.0033662384 -0.0033588824 -0.0033513873 -0.0033472904 -0.0033349465 -0.0033139144 -0.0032898167 -0.0032802953 -0.0032819943][-0.0033484071 -0.0033529571 -0.0033547904 -0.0033548672 -0.0033617737 -0.0033639921 -0.0033634128 -0.0033666918 -0.0033655453 -0.0033541932 -0.0033435859 -0.0033142546 -0.0032818806 -0.0032674705 -0.0032613599][-0.0033573904 -0.0033565252 -0.0033619269 -0.0033617539 -0.003363509 -0.0033662776 -0.0033648417 -0.0033565904 -0.0033529291 -0.0033488818 -0.003335146 -0.0033104138 -0.0032832376 -0.0032642039 -0.0032543929][-0.0033625751 -0.0033592586 -0.0033591243 -0.003356958 -0.0033575268 -0.0033605024 -0.0033584265 -0.0033478 -0.0033453091 -0.0033329455 -0.0033149351 -0.0032887803 -0.0032665844 -0.0032549447 -0.0032466855][-0.0033691016 -0.003363488 -0.0033589939 -0.0033534591 -0.0033492814 -0.0033470043 -0.0033370396 -0.00332541 -0.0033189186 -0.0033122874 -0.0032982656 -0.0032734722 -0.0032537517 -0.003240325 -0.0032333934][-0.0033667271 -0.0033591439 -0.0033496402 -0.0033392045 -0.0033302279 -0.0033189098 -0.00330785 -0.0032999211 -0.0032964714 -0.0032945331 -0.0032866828 -0.003271837 -0.0032574965 -0.0032423141 -0.0032306735][-0.0033534297 -0.0033428785 -0.003330902 -0.0033182886 -0.003307286 -0.0032946724 -0.0032859507 -0.003279543 -0.0032809682 -0.0032840252 -0.0032798876 -0.003269091 -0.0032549065 -0.003244479 -0.0032354316][-0.0033318235 -0.0033180425 -0.0033054603 -0.0032951173 -0.0032858551 -0.0032760967 -0.0032702154 -0.0032696482 -0.0032723439 -0.0032732722 -0.0032708915 -0.0032651105 -0.0032572006 -0.0032489363 -0.0032411884][-0.0033036461 -0.0032938239 -0.0032859044 -0.0032774059 -0.0032698277 -0.0032625308 -0.0032584125 -0.0032622176 -0.0032667862 -0.0032731947 -0.0032726903 -0.0032702016 -0.003263013 -0.0032600919 -0.0032582902][-0.003291846 -0.00328399 -0.0032780976 -0.0032738261 -0.0032657543 -0.0032608251 -0.0032601119 -0.0032613473 -0.0032640919 -0.0032696447 -0.0032724175 -0.0032727367 -0.0032732228 -0.0032726643 -0.0032711434][-0.0032811926 -0.0032757851 -0.0032716773 -0.0032708354 -0.003270786 -0.0032706729 -0.0032682985 -0.0032649178 -0.0032673939 -0.0032684526 -0.0032711332 -0.0032725949 -0.00327574 -0.0032776149 -0.0032777819][-0.0032746915 -0.0032709145 -0.0032662272 -0.0032628591 -0.0032624218 -0.003264905 -0.0032646309 -0.00326472 -0.003264091 -0.0032654137 -0.0032706745 -0.0032759653 -0.0032812906 -0.0032844702 -0.0032858998]]...]
INFO - root - 2017-12-09 15:41:16.938071: step 37410, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 73h:33m:59s remains)
INFO - root - 2017-12-09 15:41:25.682452: step 37420, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 70h:26m:58s remains)
INFO - root - 2017-12-09 15:41:34.222335: step 37430, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.722 sec/batch; 59h:08m:45s remains)
INFO - root - 2017-12-09 15:41:42.743608: step 37440, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:51m:34s remains)
INFO - root - 2017-12-09 15:41:51.380067: step 37450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:13m:56s remains)
INFO - root - 2017-12-09 15:41:59.839882: step 37460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:06m:45s remains)
INFO - root - 2017-12-09 15:42:08.527577: step 37470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:06m:08s remains)
INFO - root - 2017-12-09 15:42:17.207553: step 37480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 72h:15m:06s remains)
INFO - root - 2017-12-09 15:42:25.929642: step 37490, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 71h:12m:43s remains)
INFO - root - 2017-12-09 15:42:34.588745: step 37500, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 72h:06m:48s remains)
2017-12-09 15:42:35.482630: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01253275 0.012223803 0.011941444 0.012124328 0.01206801 0.011339115 0.0097822314 0.0077606784 0.0058040405 0.0043737087 0.003679361 0.0035552008 0.003736424 0.0038953361 0.0038819646][0.018437345 0.017957127 0.01730107 0.017155837 0.016705349 0.015475692 0.013311202 0.010685942 0.0082273819 0.006452281 0.0055017779 0.0051652677 0.0051567564 0.0051286845 0.0049155559][0.023049107 0.022659294 0.021667097 0.020982983 0.01986842 0.017948978 0.015127853 0.011926156 0.0090199709 0.0068840832 0.0055782162 0.0048737265 0.0044944352 0.0041500228 0.003675373][0.024712369 0.024413303 0.023307679 0.022317227 0.020748574 0.018295856 0.014914002 0.011203455 0.0078726644 0.0053883959 0.0037533825 0.0027133713 0.002026553 0.0014788366 0.00094142067][0.023267936 0.023035321 0.021912068 0.02075655 0.018945336 0.01622092 0.01260736 0.0087434426 0.0053245211 0.0027854738 0.0010932072 -8.99937e-06 -0.00073557347 -0.0012407294 -0.0016375786][0.019370679 0.019264646 0.018286392 0.0171301 0.015301581 0.012594875 0.009095178 0.005465298 0.0023666064 0.00016907742 -0.001220017 -0.0020620693 -0.0025637138 -0.0028567174 -0.0030346725][0.013735579 0.013893189 0.0133026 0.012422914 0.010869187 0.0084799035 0.005428439 0.0023818302 -8.5381558e-05 -0.0017035875 -0.0026154094 -0.0030675861 -0.0032698447 -0.0033529042 -0.003380727][0.0072907582 0.0077018011 0.0076202769 0.007191319 0.006148872 0.0043777935 0.0021144904 -7.1187969e-05 -0.0017317523 -0.0027065708 -0.0031734027 -0.0033428213 -0.0033863741 -0.0033947956 -0.0033965292][0.001553627 0.0020676234 0.0023033933 0.002240292 0.0017360479 0.00073799421 -0.0005730195 -0.0018182325 -0.0026932417 -0.0031448728 -0.003329379 -0.0033821214 -0.0033921439 -0.0033942992 -0.0033954417][-0.0018817596 -0.0015899539 -0.001399881 -0.0013698728 -0.0015533449 -0.0019563886 -0.0024794827 -0.0029485896 -0.0032298074 -0.003347412 -0.0033820956 -0.0033881972 -0.0033903422 -0.0033919769 -0.0033934717][-0.0031677848 -0.0030693049 -0.0029960433 -0.0029705074 -0.0030056285 -0.0030930778 -0.0032110589 -0.003307543 -0.0033562898 -0.0033755084 -0.0033822171 -0.0033852037 -0.0033874675 -0.0033893834 -0.0033917322][-0.0033868845 -0.0033804066 -0.0033753982 -0.0033719169 -0.0033703703 -0.0033719034 -0.0033735961 -0.0033749728 -0.0033756993 -0.0033774602 -0.0033801321 -0.0033825582 -0.0033843976 -0.0033867606 -0.0033896826][-0.0033936948 -0.003390783 -0.003388637 -0.0033860384 -0.0033828146 -0.0033805855 -0.0033782278 -0.003376998 -0.0033762879 -0.0033770758 -0.0033789135 -0.0033808979 -0.0033824989 -0.0033844812 -0.0033869338][-0.0033957101 -0.0033941194 -0.0033929567 -0.0033907141 -0.0033879296 -0.003385456 -0.003382676 -0.003381185 -0.0033799189 -0.0033797598 -0.0033802574 -0.0033811531 -0.0033821526 -0.0033832064 -0.00338433][-0.0033975234 -0.0033963444 -0.0033959183 -0.0033949118 -0.003393206 -0.0033909676 -0.003388298 -0.0033862817 -0.0033839787 -0.0033830251 -0.0033819233 -0.0033811277 -0.0033806863 -0.0033807838 -0.0033809391]]...]
INFO - root - 2017-12-09 15:42:44.129175: step 37510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:26m:33s remains)
INFO - root - 2017-12-09 15:42:52.584828: step 37520, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 68h:26m:53s remains)
INFO - root - 2017-12-09 15:43:01.065741: step 37530, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 59h:23m:59s remains)
INFO - root - 2017-12-09 15:43:09.526872: step 37540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:37m:09s remains)
INFO - root - 2017-12-09 15:43:18.142243: step 37550, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 70h:00m:34s remains)
INFO - root - 2017-12-09 15:43:26.599517: step 37560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:51m:21s remains)
INFO - root - 2017-12-09 15:43:35.226053: step 37570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:46m:18s remains)
INFO - root - 2017-12-09 15:43:43.940247: step 37580, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 69h:46m:59s remains)
INFO - root - 2017-12-09 15:43:52.566999: step 37590, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 69h:21m:21s remains)
INFO - root - 2017-12-09 15:44:01.265743: step 37600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:16m:03s remains)
2017-12-09 15:44:02.269089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.359262 0.34875092 0.33994514 0.33461475 0.33219442 0.33154756 0.33446664 0.33964804 0.34641403 0.35196754 0.35700127 0.36330402 0.37219331 0.3804833 0.38763148][0.3651078 0.35726362 0.3495574 0.34394369 0.33944479 0.33649215 0.33739609 0.34237567 0.34984377 0.3569777 0.36363924 0.37098843 0.38001254 0.38765413 0.39331481][0.3701736 0.36371621 0.35631916 0.34931231 0.34201133 0.33482221 0.33141482 0.33490625 0.34168991 0.34995034 0.35789886 0.36677065 0.37691772 0.38437396 0.38930318][0.36992526 0.36428985 0.35650522 0.34707078 0.33695242 0.32652694 0.321427 0.32296053 0.32871041 0.3377637 0.347141 0.35788593 0.36966538 0.37852463 0.38354787][0.36619365 0.36090359 0.35169113 0.341332 0.32945845 0.31726992 0.31072783 0.31163758 0.3169958 0.32601538 0.33649093 0.3491056 0.36147857 0.37179843 0.37749648][0.35690105 0.35328668 0.34403193 0.33296686 0.32077071 0.30831593 0.30193725 0.30260408 0.30778161 0.31728828 0.3293086 0.34325418 0.35689712 0.36870694 0.37502488][0.34791613 0.34790185 0.33990398 0.32848838 0.31623992 0.30355144 0.29721737 0.29767096 0.30251259 0.31256619 0.32584921 0.34122083 0.35556257 0.36822468 0.37492469][0.33957618 0.3438012 0.33755863 0.32762551 0.31707668 0.30502778 0.2986047 0.29715595 0.29998678 0.30890805 0.32163247 0.33715156 0.3515515 0.36445934 0.37226009][0.33039811 0.33763057 0.33319294 0.32666335 0.31879568 0.30864191 0.30396739 0.30256617 0.30452257 0.31102809 0.32219365 0.336877 0.35048962 0.363025 0.37129971][0.32595077 0.33682162 0.33331892 0.32889071 0.32350713 0.31655169 0.31279683 0.31148458 0.3133755 0.31859842 0.3281785 0.34130916 0.35339424 0.36436945 0.37222031][0.32351881 0.33809045 0.3380641 0.33672407 0.33429855 0.33070534 0.32909259 0.32803664 0.32896084 0.33228129 0.33958033 0.35052606 0.36100864 0.3699846 0.37675944][0.32658949 0.34451565 0.34713551 0.34864736 0.34928006 0.34863481 0.34845403 0.34771308 0.34769619 0.34882963 0.3535904 0.36195627 0.37022117 0.37734681 0.38319618][0.33292109 0.35175028 0.35556871 0.35919851 0.36218953 0.36420277 0.36630872 0.36598322 0.36496395 0.36419961 0.36636835 0.37192684 0.37789163 0.38340095 0.38869837][0.34103069 0.36007497 0.36391467 0.36834913 0.37267232 0.37628052 0.37942928 0.38029695 0.37971082 0.37783054 0.37826279 0.3813037 0.38526636 0.38947684 0.39394331][0.3485038 0.3682566 0.37253469 0.37706655 0.38139635 0.38598821 0.38991255 0.3911202 0.39054406 0.38877437 0.38820839 0.38938954 0.39181966 0.39500508 0.39864987]]...]
INFO - root - 2017-12-09 15:44:10.750771: step 37610, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:51m:09s remains)
INFO - root - 2017-12-09 15:44:19.292369: step 37620, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:53m:49s remains)
INFO - root - 2017-12-09 15:44:27.794475: step 37630, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.727 sec/batch; 59h:33m:00s remains)
INFO - root - 2017-12-09 15:44:36.304140: step 37640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:15m:20s remains)
INFO - root - 2017-12-09 15:44:45.050952: step 37650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:02m:45s remains)
INFO - root - 2017-12-09 15:44:53.742287: step 37660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:20m:49s remains)
INFO - root - 2017-12-09 15:45:02.387812: step 37670, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:45m:09s remains)
INFO - root - 2017-12-09 15:45:11.063613: step 37680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:07m:11s remains)
INFO - root - 2017-12-09 15:45:19.709730: step 37690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:51m:57s remains)
INFO - root - 2017-12-09 15:45:28.375333: step 37700, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 67h:39m:04s remains)
2017-12-09 15:45:29.204192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20493546 0.21447848 0.21831863 0.21775967 0.21426645 0.209263 0.20366625 0.19779992 0.19119784 0.18326351 0.17411494 0.16308402 0.15056913 0.13708125 0.12436468][0.21632223 0.23180774 0.24146508 0.24641298 0.2473578 0.2458875 0.24192365 0.23649228 0.22951862 0.22062795 0.20971188 0.19592384 0.18068627 0.16411237 0.14771859][0.22247604 0.24317938 0.25855768 0.26942474 0.27568024 0.27854383 0.27786219 0.27393386 0.26754317 0.25790137 0.24563552 0.22972792 0.21126887 0.1910926 0.17054413][0.22646624 0.25099087 0.27105543 0.28716096 0.29831967 0.30546412 0.30790985 0.30694044 0.30252516 0.29382414 0.28152192 0.26448 0.24382107 0.22012831 0.19536719][0.22866802 0.25657672 0.28015155 0.29987705 0.31453457 0.32490852 0.33009565 0.33121908 0.32890913 0.32236126 0.31140858 0.2948691 0.27355191 0.2479025 0.21997407][0.22976594 0.25988302 0.28565183 0.30743372 0.32376331 0.33568043 0.34270138 0.34630632 0.34645993 0.34195417 0.3324472 0.31706649 0.29557452 0.26899514 0.23864965][0.2302132 0.26149124 0.28770807 0.30957252 0.32608193 0.3383545 0.34644869 0.35191071 0.35487777 0.35340428 0.34595186 0.33183071 0.31066647 0.28364736 0.25136086][0.23048504 0.26153436 0.2869001 0.3075929 0.32312348 0.33472857 0.34328854 0.35006067 0.35536155 0.35633561 0.35084864 0.33822724 0.31778711 0.29110649 0.25795645][0.22753793 0.2580758 0.28225604 0.30131376 0.31552079 0.3259353 0.33398554 0.34115216 0.34767294 0.35049203 0.34643015 0.33497423 0.31594583 0.29045111 0.25794208][0.21881692 0.24819256 0.27065685 0.28842434 0.30160591 0.31139052 0.31942984 0.32655665 0.33364293 0.3371838 0.33387727 0.32328779 0.30552411 0.2820271 0.25138816][0.20566401 0.23270096 0.25274476 0.26853123 0.28038159 0.28963849 0.29764408 0.30528465 0.31298256 0.31706008 0.31429726 0.30451179 0.28835115 0.26686352 0.23874198][0.18877326 0.21309762 0.23069349 0.24443562 0.25470734 0.26275191 0.26978308 0.27694711 0.28439727 0.2890369 0.28751343 0.27932224 0.26558968 0.24690987 0.22230327][0.16927435 0.19038342 0.20523739 0.21686755 0.22561282 0.23225574 0.23790459 0.243772 0.25015834 0.25453821 0.25369188 0.24739881 0.23650786 0.22147183 0.20141882][0.14694951 0.16474447 0.17695627 0.18636855 0.19336514 0.19858901 0.20294841 0.2076643 0.21298093 0.21685854 0.21666482 0.21211426 0.2040433 0.19261284 0.17739607][0.12206086 0.13647859 0.14601153 0.15332408 0.1586287 0.16243725 0.16556463 0.16927628 0.17364998 0.1771993 0.17787471 0.17527311 0.16994528 0.1619675 0.15125345]]...]
INFO - root - 2017-12-09 15:45:37.648217: step 37710, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:13m:20s remains)
INFO - root - 2017-12-09 15:45:46.185346: step 37720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:27m:24s remains)
INFO - root - 2017-12-09 15:45:54.856619: step 37730, loss = 0.89, batch loss = 0.68 (11.0 examples/sec; 0.724 sec/batch; 59h:18m:35s remains)
INFO - root - 2017-12-09 15:46:03.472905: step 37740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:40m:10s remains)
INFO - root - 2017-12-09 15:46:12.261301: step 37750, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 73h:25m:24s remains)
INFO - root - 2017-12-09 15:46:20.893447: step 37760, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 72h:19m:40s remains)
INFO - root - 2017-12-09 15:46:29.587308: step 37770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:01m:00s remains)
INFO - root - 2017-12-09 15:46:38.250792: step 37780, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 70h:23m:48s remains)
INFO - root - 2017-12-09 15:46:46.763521: step 37790, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 68h:11m:47s remains)
INFO - root - 2017-12-09 15:46:55.321486: step 37800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:36m:35s remains)
2017-12-09 15:46:56.184685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003404849 -0.0034029074 -0.0034024841 -0.0034025921 -0.00340287 -0.00340175 -0.003398658 -0.0033934745 -0.0033876747 -0.0033803284 -0.0033718606 -0.0033620524 -0.0033537056 -0.00334642 -0.0033418634][-0.0034031302 -0.0034012981 -0.003400977 -0.003401136 -0.0034016869 -0.0034018431 -0.0034001786 -0.0033957139 -0.0033894074 -0.0033819752 -0.0033740832 -0.0033654573 -0.0033577487 -0.003350707 -0.0033457361][-0.0034026494 -0.0034009134 -0.0034005083 -0.0034007602 -0.0034016119 -0.0034026487 -0.0034023484 -0.0033991772 -0.003392908 -0.003385298 -0.0033779081 -0.0033705435 -0.0033633958 -0.0033565105 -0.0033515152][-0.0034024988 -0.0034007789 -0.0034005337 -0.0034010655 -0.0034019253 -0.0034031996 -0.003403422 -0.0034008664 -0.0033949104 -0.0033874025 -0.0033804285 -0.0033743312 -0.0033678766 -0.0033615588 -0.0033569832][-0.003402631 -0.0034010687 -0.0034009225 -0.0034015314 -0.0034024543 -0.0034035512 -0.0034032539 -0.0034005698 -0.0033950033 -0.0033883012 -0.0033822355 -0.0033771659 -0.0033713258 -0.0033655558 -0.0033614375][-0.0034029516 -0.0034014597 -0.0034014219 -0.0034020375 -0.0034025516 -0.003403004 -0.0034019486 -0.0033990536 -0.0033940629 -0.0033888363 -0.0033835974 -0.0033789002 -0.0033742115 -0.0033695351 -0.0033661397][-0.0034032015 -0.0034017116 -0.0034018438 -0.0034025076 -0.0034026427 -0.0034020199 -0.0033998766 -0.0033970657 -0.0033937444 -0.0033899476 -0.0033851478 -0.0033809799 -0.0033771347 -0.0033737388 -0.003371021][-0.0034032902 -0.0034019479 -0.0034023146 -0.0034028816 -0.0034026515 -0.0034012499 -0.00339939 -0.0033976897 -0.0033960098 -0.0033928002 -0.0033881224 -0.0033836013 -0.0033801715 -0.0033775577 -0.0033751896][-0.0034033747 -0.003402113 -0.0034024457 -0.0034027265 -0.0034023209 -0.0034012857 -0.0034012161 -0.0034011146 -0.0033998762 -0.00339659 -0.0033919583 -0.0033872421 -0.0033837033 -0.003380744 -0.0033784611][-0.0034031321 -0.0034017989 -0.0034023209 -0.0034026254 -0.0034029926 -0.0034037842 -0.0034049931 -0.0034049263 -0.0034033037 -0.0034002468 -0.0033959425 -0.0033914256 -0.0033877848 -0.0033851932 -0.0033826702][-0.0034028168 -0.0034016594 -0.0034025544 -0.0034036483 -0.0034051114 -0.0034067733 -0.0034081552 -0.0034078688 -0.0034060394 -0.0034032355 -0.0033993372 -0.003395343 -0.0033922752 -0.003389721 -0.0033872277][-0.0034022459 -0.0034013579 -0.0034025635 -0.0034042329 -0.0034061435 -0.0034077673 -0.0034093291 -0.0034091775 -0.0034078655 -0.0034057498 -0.0034023877 -0.0033987816 -0.0033958848 -0.0033932449 -0.0033909192][-0.0034019281 -0.0034010273 -0.0034022529 -0.0034039842 -0.0034058222 -0.0034073968 -0.0034086797 -0.0034087764 -0.0034085268 -0.0034074062 -0.0034049409 -0.003401649 -0.0033988839 -0.0033966654 -0.0033945846][-0.0034011663 -0.0034000226 -0.0034010131 -0.0034023966 -0.0034037195 -0.0034049097 -0.0034058862 -0.0034063593 -0.003406835 -0.0034064795 -0.003405387 -0.0034033994 -0.00340138 -0.00339948 -0.0033980531][-0.0034009535 -0.0033992629 -0.0033997977 -0.0034006156 -0.0034012187 -0.003401984 -0.0034027104 -0.0034032636 -0.0034040962 -0.0034041484 -0.0034038781 -0.0034034688 -0.0034025547 -0.003401499 -0.0034006538]]...]
INFO - root - 2017-12-09 15:47:04.868025: step 37810, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 72h:06m:00s remains)
INFO - root - 2017-12-09 15:47:13.506681: step 37820, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:06m:15s remains)
INFO - root - 2017-12-09 15:47:22.186491: step 37830, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 62h:34m:47s remains)
INFO - root - 2017-12-09 15:47:30.794278: step 37840, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 73h:50m:03s remains)
INFO - root - 2017-12-09 15:47:39.603975: step 37850, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:01m:30s remains)
INFO - root - 2017-12-09 15:47:48.053786: step 37860, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 67h:13m:41s remains)
INFO - root - 2017-12-09 15:47:56.726715: step 37870, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 70h:49m:13s remains)
INFO - root - 2017-12-09 15:48:05.537646: step 37880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:51m:03s remains)
INFO - root - 2017-12-09 15:48:14.248798: step 37890, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:42m:45s remains)
INFO - root - 2017-12-09 15:48:22.932563: step 37900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:45m:09s remains)
2017-12-09 15:48:23.804257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033875545 -0.0033837396 -0.0033817976 -0.0033816085 -0.0033813755 -0.0033815408 -0.0033806518 -0.003379086 -0.0033759675 -0.0033753891 -0.0033774769 -0.003379018 -0.0033798667 -0.0033790369 -0.0033767738][-0.0033811959 -0.0033770569 -0.0033754362 -0.0033751898 -0.0033758928 -0.0033779261 -0.0033798933 -0.0033811866 -0.0033799903 -0.0033796104 -0.0033806567 -0.0033809335 -0.0033802658 -0.0033784756 -0.0033758862][-0.0033749689 -0.0033712015 -0.003369736 -0.0033693176 -0.0033701973 -0.0033726697 -0.0033758716 -0.0033784716 -0.0033791789 -0.0033802451 -0.0033816847 -0.0033814453 -0.0033794835 -0.0033769617 -0.0033741943][-0.0033694978 -0.0033656387 -0.0033639821 -0.003363197 -0.0033634321 -0.0033651786 -0.0033680063 -0.0033707675 -0.0033722925 -0.0033736178 -0.0033756455 -0.0033759149 -0.0033745943 -0.0033727342 -0.0033708662][-0.00336727 -0.003363162 -0.0033613173 -0.0033601865 -0.0033594565 -0.0033594712 -0.0033607665 -0.0033626396 -0.0033644512 -0.0033664193 -0.0033687404 -0.0033696811 -0.0033694771 -0.003368472 -0.0033676112][-0.0033672249 -0.0033635329 -0.0033613446 -0.0033592088 -0.0033568121 -0.0033550654 -0.0033540584 -0.0033540132 -0.0033556309 -0.0033577911 -0.003360468 -0.0033625087 -0.003364048 -0.0033649008 -0.0033648717][-0.0033691037 -0.0033654706 -0.0033631751 -0.00335965 -0.0033547163 -0.0033495123 -0.0033456287 -0.0033441093 -0.0033455279 -0.0033494874 -0.0033552225 -0.0033599338 -0.0033626738 -0.0033638144 -0.0033635772][-0.0033710229 -0.0033688925 -0.0033675325 -0.0033644976 -0.00335901 -0.0033528511 -0.0033482988 -0.0033467461 -0.0033481882 -0.0033526372 -0.0033586379 -0.0033629434 -0.003364624 -0.003364441 -0.0033632994][-0.0033727284 -0.0033721738 -0.003372859 -0.0033716476 -0.0033676855 -0.003362064 -0.0033570679 -0.0033553655 -0.0033569941 -0.003361478 -0.0033675546 -0.003371214 -0.0033712285 -0.0033689411 -0.0033651667][-0.0033723782 -0.0033733568 -0.0033759463 -0.0033774197 -0.0033763081 -0.0033728478 -0.0033691302 -0.0033670664 -0.0033675837 -0.0033707547 -0.0033752732 -0.0033775922 -0.0033764762 -0.0033731589 -0.0033677558][-0.0033703649 -0.0033723481 -0.0033767954 -0.0033803983 -0.0033819063 -0.0033810725 -0.0033791699 -0.0033775887 -0.0033775202 -0.0033793875 -0.003382412 -0.0033834279 -0.0033816015 -0.0033772648 -0.0033709595][-0.003367258 -0.0033691218 -0.0033741551 -0.0033790336 -0.0033823168 -0.0033832914 -0.0033831683 -0.0033825121 -0.0033826062 -0.0033841762 -0.0033856975 -0.0033856465 -0.0033833801 -0.0033786593 -0.0033722338][-0.0033640533 -0.003364888 -0.0033694974 -0.00337472 -0.0033794988 -0.0033826202 -0.0033849948 -0.0033860221 -0.0033862127 -0.0033870847 -0.0033873678 -0.003386186 -0.0033832025 -0.0033783559 -0.0033724003][-0.0033617858 -0.0033614847 -0.0033651681 -0.0033699682 -0.0033752895 -0.0033796644 -0.0033832963 -0.0033851378 -0.003385667 -0.0033856316 -0.0033846472 -0.0033826367 -0.0033795978 -0.0033753626 -0.0033706543][-0.0033605259 -0.0033588589 -0.0033610775 -0.0033644342 -0.0033684659 -0.0033724485 -0.0033761687 -0.0033783284 -0.0033790926 -0.0033792651 -0.0033786662 -0.0033768206 -0.0033738532 -0.0033703193 -0.0033667551]]...]
INFO - root - 2017-12-09 15:48:32.351008: step 37910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 71h:38m:09s remains)
INFO - root - 2017-12-09 15:48:40.911547: step 37920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:37m:31s remains)
INFO - root - 2017-12-09 15:48:49.301736: step 37930, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 59h:13m:57s remains)
INFO - root - 2017-12-09 15:48:57.723509: step 37940, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:07m:43s remains)
INFO - root - 2017-12-09 15:49:06.310024: step 37950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:12m:20s remains)
INFO - root - 2017-12-09 15:49:14.798814: step 37960, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 68h:20m:46s remains)
INFO - root - 2017-12-09 15:49:23.213871: step 37970, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 67h:50m:00s remains)
INFO - root - 2017-12-09 15:49:31.704229: step 37980, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 71h:00m:43s remains)
INFO - root - 2017-12-09 15:49:40.321121: step 37990, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 68h:07m:13s remains)
INFO - root - 2017-12-09 15:49:49.125388: step 38000, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:47m:27s remains)
2017-12-09 15:49:49.999769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028570774 -0.0026660732 -0.0023737182 -0.0021078116 -0.00184967 -0.0016541578 -0.0015342563 -0.0014355397 -0.00139128 -0.0013756971 -0.0014029224 -0.0013624921 -0.0013992784 -0.0015080461 -0.0016525075][-0.0025340091 -0.0022083004 -0.001769039 -0.0013239968 -0.00096480059 -0.00071966974 -0.00062684529 -0.00054747076 -0.00052322051 -0.00055109919 -0.00065326458 -0.00071766274 -0.00078225252 -0.00080612488 -0.0009503609][-0.0021681958 -0.0017416287 -0.0012653959 -0.000797736 -0.00044520455 -0.00022912817 -0.00010172976 -2.0989683e-05 -1.9308878e-05 -4.3835957e-05 -0.00018896139 -0.00029228092 -0.00036422419 -0.00035086623 -0.00049108174][-0.0017904687 -0.0013245414 -0.00094919279 -0.00057613058 -0.00025256653 -6.4102e-05 3.1747855e-05 7.0725568e-05 5.6149904e-05 4.1790772e-05 -2.2169668e-05 -7.8801531e-05 -0.00013320125 -0.0001344902 -0.0002957515][-0.001640582 -0.0011882107 -0.00079611968 -0.00047675171 -0.00022925646 -5.6857476e-05 1.5463913e-05 7.4917916e-06 -3.0016759e-05 -7.1457587e-05 -0.00013118214 -0.00015098043 -0.00018761936 -0.0001947335 -0.00029219803][-0.0017661639 -0.001372627 -0.0010355525 -0.0006798699 -0.00034582312 -0.00014355732 -9.8018674e-05 -8.8762492e-05 -0.00012872298 -0.00021445449 -0.0002862378 -0.00032043271 -0.00036578393 -0.00039459835 -0.000510572][-0.0021585738 -0.0018386695 -0.001511377 -0.0011603474 -0.00080036186 -0.00050152373 -0.00036230311 -0.00030433643 -0.00035527139 -0.00043265615 -0.00050890353 -0.00057926495 -0.00064514577 -0.0007318994 -0.00087134494][-0.0025567014 -0.0023571877 -0.0021246 -0.001792484 -0.0014284009 -0.0011207873 -0.00090678246 -0.00079823565 -0.00079258485 -0.000878562 -0.00099133677 -0.0010611266 -0.0011233594 -0.0012535718 -0.0013972181][-0.0029578302 -0.002815431 -0.0026699638 -0.0024844958 -0.0022357823 -0.0019601542 -0.0017560343 -0.0016487599 -0.0016455128 -0.0016839713 -0.0017160889 -0.001768988 -0.0018318294 -0.0019617989 -0.0020845742][-0.0032493025 -0.0031750428 -0.0030977766 -0.0030089458 -0.0028967918 -0.0027610776 -0.002639371 -0.0025660396 -0.0025577829 -0.0025804725 -0.0025913212 -0.002599651 -0.0026337605 -0.0027045496 -0.0027780132][-0.003389874 -0.0033753775 -0.0033539243 -0.0033074287 -0.0032575566 -0.0032109683 -0.0031667212 -0.0031402004 -0.0031392165 -0.0031459171 -0.0031424209 -0.0031388598 -0.0031546438 -0.0031879207 -0.0032231614][-0.0034010722 -0.0033993945 -0.0033992417 -0.0033976417 -0.0033920202 -0.0033789899 -0.0033660447 -0.0033614007 -0.0033632775 -0.0033674627 -0.0033672822 -0.0033663213 -0.0033714594 -0.0033824425 -0.0033931737][-0.0034003556 -0.003398424 -0.0033980531 -0.0033982215 -0.0033984846 -0.0033972985 -0.0033953339 -0.0033939073 -0.0033932785 -0.0033950005 -0.0033966703 -0.0033994636 -0.0034020017 -0.0034045558 -0.0034063174][-0.0033999337 -0.0033982059 -0.0033974275 -0.0033971679 -0.0033972834 -0.0033972331 -0.003396759 -0.0033967844 -0.0033965372 -0.0033978382 -0.0033986627 -0.0034012294 -0.0034028545 -0.0034047852 -0.0034069333][-0.0034005553 -0.0034000184 -0.0033999518 -0.0033999248 -0.0033998413 -0.0033992061 -0.0033989267 -0.003398692 -0.0033984822 -0.0033994666 -0.00339919 -0.0034005637 -0.0033994503 -0.0033983479 -0.0033990764]]...]
INFO - root - 2017-12-09 15:49:58.579831: step 38010, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 70h:38m:08s remains)
INFO - root - 2017-12-09 15:50:07.020474: step 38020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 67h:29m:11s remains)
INFO - root - 2017-12-09 15:50:15.371453: step 38030, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.711 sec/batch; 58h:08m:08s remains)
INFO - root - 2017-12-09 15:50:23.808342: step 38040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:49m:08s remains)
INFO - root - 2017-12-09 15:50:32.445227: step 38050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:05m:36s remains)
INFO - root - 2017-12-09 15:50:41.001712: step 38060, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:09m:40s remains)
INFO - root - 2017-12-09 15:50:49.639790: step 38070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:00m:54s remains)
INFO - root - 2017-12-09 15:50:58.258450: step 38080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:13m:00s remains)
INFO - root - 2017-12-09 15:51:06.804287: step 38090, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:01m:11s remains)
INFO - root - 2017-12-09 15:51:15.501890: step 38100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:05m:49s remains)
2017-12-09 15:51:16.423524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24317184 0.24373975 0.24085103 0.23525107 0.22809935 0.22136119 0.21490464 0.20867814 0.20249113 0.19640899 0.18998677 0.18071239 0.16817835 0.15135209 0.13203694][0.25927213 0.2624175 0.26185378 0.2582522 0.25231779 0.24606796 0.23933531 0.23200782 0.22408363 0.21644527 0.20860319 0.19799462 0.18396278 0.16538095 0.14407772][0.26388568 0.27051917 0.2733807 0.27283818 0.26948288 0.26480734 0.25847781 0.25052258 0.24119374 0.23164046 0.22178322 0.20938459 0.19350861 0.17323476 0.15032774][0.25966781 0.27028438 0.27714503 0.28050697 0.28060836 0.27842739 0.27316248 0.26490024 0.25455484 0.24345335 0.23147628 0.21723582 0.19959016 0.17783357 0.15360521][0.24899894 0.26334357 0.27422836 0.28174019 0.2857936 0.28661814 0.28314725 0.27535111 0.26435348 0.25190303 0.23797965 0.22168279 0.2021624 0.17917442 0.1540595][0.23323034 0.250741 0.26492092 0.27600104 0.283665 0.28758389 0.28626502 0.27953982 0.26860294 0.25532541 0.23976502 0.22174287 0.2008563 0.17719381 0.15190575][0.21498916 0.23429276 0.25050226 0.264305 0.27463296 0.28090423 0.28164321 0.2762143 0.26585567 0.25214407 0.2356908 0.21661097 0.19503388 0.17158285 0.14709282][0.19688641 0.21667904 0.23350622 0.24841815 0.26010355 0.26773116 0.26973963 0.26535994 0.25573969 0.24209157 0.22539043 0.20598958 0.18451887 0.16208541 0.13915181][0.17908393 0.19803344 0.21429059 0.22897147 0.24070765 0.24870071 0.25146466 0.24802452 0.23914507 0.22602212 0.20977923 0.19087811 0.17053926 0.14986368 0.129273][0.16112138 0.17814809 0.19253612 0.20604229 0.21705262 0.22449572 0.22743984 0.22495565 0.21729866 0.20519356 0.19021285 0.17284465 0.15423244 0.13591099 0.11804964][0.14293493 0.1576089 0.16965039 0.18120398 0.19068339 0.1970956 0.19978166 0.19799961 0.19163314 0.1811991 0.16811214 0.15306865 0.13700758 0.12135941 0.10622673][0.12506774 0.13731715 0.14705528 0.15644969 0.16410087 0.16910817 0.17117029 0.16952997 0.16418712 0.15554243 0.144816 0.13247502 0.11925624 0.10654535 0.094147369][0.10808332 0.11802091 0.12562181 0.13296373 0.13903496 0.14280167 0.14398484 0.14228716 0.13763991 0.13048978 0.1218409 0.11218983 0.10195083 0.092119969 0.082518533][0.090956107 0.098532781 0.1042105 0.10981371 0.11462504 0.11757968 0.11839992 0.11683055 0.11278521 0.1071972 0.10056867 0.09342052 0.086042836 0.078965589 0.072190344][0.075313464 0.080802776 0.084523126 0.088506289 0.092083894 0.094339266 0.0949919 0.093812965 0.090817384 0.086749911 0.082020737 0.077020615 0.071962684 0.06732583 0.062949374]]...]
INFO - root - 2017-12-09 15:51:25.004778: step 38110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:44m:03s remains)
INFO - root - 2017-12-09 15:51:33.446724: step 38120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:36m:03s remains)
INFO - root - 2017-12-09 15:51:41.700218: step 38130, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 55h:16m:58s remains)
INFO - root - 2017-12-09 15:51:50.215280: step 38140, loss = 0.88, batch loss = 0.68 (11.0 examples/sec; 0.727 sec/batch; 59h:27m:05s remains)
INFO - root - 2017-12-09 15:51:58.790277: step 38150, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 67h:52m:32s remains)
INFO - root - 2017-12-09 15:52:07.319494: step 38160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:47m:35s remains)
INFO - root - 2017-12-09 15:52:16.151491: step 38170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:39m:39s remains)
INFO - root - 2017-12-09 15:52:24.842997: step 38180, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 72h:40m:00s remains)
INFO - root - 2017-12-09 15:52:33.504103: step 38190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:06m:19s remains)
INFO - root - 2017-12-09 15:52:42.155518: step 38200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:29m:16s remains)
2017-12-09 15:52:43.091563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033936221 -0.0033916193 -0.0033762769 -0.00328352 -0.0030066855 -0.0024904162 -0.0018113428 -0.0010515361 -0.00049718656 -0.00040008244 -0.00074866787 -0.0013635552 -0.0020639631 -0.0026736618 -0.0030849234][-0.0033766951 -0.0033606433 -0.0032906511 -0.0030213508 -0.002392994 -0.0014487295 -0.00039138971 0.000598697 0.0011359616 0.0010063211 0.00029880623 -0.00068641268 -0.0016883356 -0.0025042733 -0.00302425][-0.0032869442 -0.0032000886 -0.0029097502 -0.0021779034 -0.00078605418 0.0010297094 0.0028765569 0.0043172585 0.0048623439 0.0043221517 0.0029048375 0.0011045979 -0.00061334134 -0.0019441119 -0.0027805017][-0.0029218774 -0.0025679455 -0.0017471436 -0.00012960564 0.0025230376 0.0057653673 0.0090001067 0.011356146 0.012171139 0.011147654 0.00862822 0.0053842887 0.002209289 -0.00030442956 -0.0019664792][-0.0019959982 -0.0010701269 0.0007266067 0.0037017455 0.0080459155 0.013163862 0.018302448 0.022068843 0.0235647 0.022218782 0.018415332 0.013167652 0.0076955771 0.0031097736 -0.00013466948][-0.00050409208 0.0013274834 0.0043941187 0.0090069994 0.015164024 0.02221885 0.029350631 0.034764681 0.037313215 0.035973564 0.031031735 0.023607541 0.015370657 0.008073641 0.00264091][0.0011394247 0.0040015015 0.0083897822 0.014592108 0.022230212 0.030699208 0.039265096 0.046028655 0.049656082 0.048629645 0.043016229 0.033862531 0.023158427 0.013249145 0.005602113][0.0022575096 0.0059070047 0.01124087 0.018501375 0.026847344 0.035745252 0.044673759 0.051962905 0.056220766 0.0555886 0.049890425 0.03999047 0.027983673 0.016533878 0.0075027021][0.00227897 0.0061155465 0.011633102 0.018968675 0.026944093 0.0350978 0.043152433 0.049889136 0.054043852 0.053742133 0.048584066 0.03923 0.02763601 0.016393334 0.0074305665][0.0012052846 0.0045251651 0.0093376776 0.015680306 0.022335298 0.028865919 0.035141256 0.040447462 0.04381622 0.043668538 0.039524589 0.031859756 0.022277255 0.012924115 0.0054486357][-0.00041365274 0.0019454178 0.005448224 0.010061434 0.014807421 0.019295674 0.023472518 0.027018238 0.029296376 0.029196568 0.026303917 0.020944647 0.014242533 0.0077007515 0.0025016628][-0.001892411 -0.00054976507 0.0015323614 0.004301602 0.0071382364 0.0097387675 0.012070951 0.014031171 0.015278318 0.015180872 0.013464988 0.010335934 0.0064502945 0.0026849937 -0.00026455591][-0.00285027 -0.0022644838 -0.0013028448 4.288042e-06 0.0013618465 0.0025838218 0.0036381478 0.0045042485 0.00503897 0.00496352 0.0041419151 0.0026829974 0.00089553883 -0.00081596361 -0.0021265796][-0.0032761821 -0.0031013414 -0.00278569 -0.0023382471 -0.0018576203 -0.001426134 -0.0010666193 -0.00078211981 -0.00061505241 -0.000654934 -0.00094731059 -0.0014476199 -0.0020487083 -0.0026136441 -0.0030331609][-0.0033873073 -0.0033606938 -0.0033019779 -0.0032083297 -0.0030981619 -0.0029964542 -0.002913838 -0.0028500846 -0.0028131651 -0.0028227367 -0.00288806 -0.0029962398 -0.0031238031 -0.0032427658 -0.0033294403]]...]
INFO - root - 2017-12-09 15:52:51.746730: step 38210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:56m:08s remains)
INFO - root - 2017-12-09 15:53:00.464663: step 38220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:22m:06s remains)
INFO - root - 2017-12-09 15:53:09.230632: step 38230, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.826 sec/batch; 67h:33m:22s remains)
INFO - root - 2017-12-09 15:53:17.825998: step 38240, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 59h:53m:14s remains)
INFO - root - 2017-12-09 15:53:26.594947: step 38250, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 73h:57m:25s remains)
INFO - root - 2017-12-09 15:53:35.078614: step 38260, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 71h:18m:24s remains)
INFO - root - 2017-12-09 15:53:43.811539: step 38270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:14m:14s remains)
INFO - root - 2017-12-09 15:53:52.298026: step 38280, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:18m:25s remains)
INFO - root - 2017-12-09 15:54:00.805782: step 38290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:11m:09s remains)
INFO - root - 2017-12-09 15:54:09.359494: step 38300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:09m:44s remains)
2017-12-09 15:54:10.291659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033594272 -0.0033562207 -0.0033561487 -0.0033564006 -0.0033565555 -0.0033569052 -0.0033565846 -0.0033560481 -0.003355294 -0.0033544719 -0.0033537706 -0.0033545622 -0.0033570677 -0.003361719 -0.003367078][-0.0033578076 -0.0033541366 -0.0033537738 -0.0033539624 -0.0033540926 -0.0033543177 -0.0033540362 -0.0033536481 -0.0033528286 -0.0033520334 -0.0033516472 -0.0033529904 -0.003356159 -0.0033616752 -0.0033683155][-0.0033583886 -0.0033543978 -0.0033536479 -0.0033535275 -0.0033536025 -0.0033538358 -0.0033538439 -0.0033538849 -0.0033532027 -0.0033522851 -0.0033517769 -0.0033532691 -0.0033563285 -0.0033616389 -0.0033685975][-0.0033585611 -0.0033547499 -0.0033539704 -0.0033537827 -0.0033539615 -0.0033543813 -0.0033547974 -0.0033552498 -0.0033546879 -0.003353646 -0.003352839 -0.0033540055 -0.0033564297 -0.0033608621 -0.0033672724][-0.0033589716 -0.0033551704 -0.0033547187 -0.0033547103 -0.0033552907 -0.0033562013 -0.0033573015 -0.0033581194 -0.0033577445 -0.003356636 -0.0033554602 -0.0033555948 -0.0033567406 -0.0033600579 -0.0033654275][-0.0033596463 -0.0033560984 -0.0033563273 -0.0033571955 -0.0033585706 -0.0033602898 -0.0033622356 -0.0033634917 -0.0033633902 -0.0033624175 -0.0033606687 -0.0033595217 -0.0033589043 -0.0033601134 -0.0033636917][-0.0033608531 -0.0033579452 -0.0033589413 -0.0033609015 -0.0033636997 -0.0033666922 -0.003369638 -0.0033714704 -0.0033721027 -0.0033715272 -0.0033691407 -0.0033665362 -0.0033639423 -0.003362736 -0.0033640736][-0.0033625211 -0.0033603583 -0.0033623278 -0.0033654955 -0.0033695118 -0.0033735794 -0.003377354 -0.003379954 -0.003381259 -0.0033810365 -0.0033784632 -0.0033749212 -0.0033709439 -0.0033679325 -0.0033670885][-0.0033658103 -0.0033640144 -0.003366739 -0.00337114 -0.003376133 -0.0033810213 -0.0033854949 -0.0033889059 -0.003390796 -0.0033910018 -0.0033887425 -0.0033846488 -0.0033797475 -0.0033752222 -0.0033723756][-0.0033710338 -0.003369105 -0.0033720005 -0.0033769377 -0.0033825915 -0.0033879473 -0.0033929688 -0.003396824 -0.0033992692 -0.0033999614 -0.0033981977 -0.003394257 -0.0033890111 -0.0033832381 -0.0033781759][-0.0033761689 -0.0033737819 -0.0033763479 -0.0033813445 -0.0033872162 -0.0033931658 -0.0033986713 -0.00340285 -0.0034055219 -0.003406385 -0.003405106 -0.0034018231 -0.00339668 -0.0033900924 -0.003383287][-0.003381181 -0.0033778455 -0.0033792781 -0.0033838612 -0.003389755 -0.0033959253 -0.0034016704 -0.0034060709 -0.0034090413 -0.0034101771 -0.0034093484 -0.0034066532 -0.0034021868 -0.0033956585 -0.0033877743][-0.0033855273 -0.0033819638 -0.0033827513 -0.0033863781 -0.0033915276 -0.0033973693 -0.0034025435 -0.0034064797 -0.0034092688 -0.00341056 -0.0034102949 -0.003408327 -0.003404534 -0.0033983747 -0.0033903364][-0.0033885664 -0.0033848656 -0.0033854204 -0.0033880393 -0.0033920968 -0.003396845 -0.0034013174 -0.0034046294 -0.0034066653 -0.0034072036 -0.0034068062 -0.0034053205 -0.0034022436 -0.0033972953 -0.0033904496][-0.0033901376 -0.0033866731 -0.0033867694 -0.0033882866 -0.0033910307 -0.0033944284 -0.0033972308 -0.0033991856 -0.0034000836 -0.0033999945 -0.0033992757 -0.0033980585 -0.0033964231 -0.0033935932 -0.0033891581]]...]
INFO - root - 2017-12-09 15:54:18.910677: step 38310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:21m:42s remains)
INFO - root - 2017-12-09 15:54:27.486558: step 38320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:33m:54s remains)
INFO - root - 2017-12-09 15:54:36.055736: step 38330, loss = 0.89, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 60h:12m:36s remains)
INFO - root - 2017-12-09 15:54:44.684124: step 38340, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:05m:39s remains)
INFO - root - 2017-12-09 15:54:53.148164: step 38350, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:33m:33s remains)
INFO - root - 2017-12-09 15:55:01.682763: step 38360, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.714 sec/batch; 58h:19m:27s remains)
INFO - root - 2017-12-09 15:55:10.254718: step 38370, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 70h:02m:14s remains)
INFO - root - 2017-12-09 15:55:18.969411: step 38380, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 69h:27m:44s remains)
INFO - root - 2017-12-09 15:55:27.620511: step 38390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:36m:07s remains)
INFO - root - 2017-12-09 15:55:36.328611: step 38400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:23m:37s remains)
2017-12-09 15:55:37.291936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033822425 -0.0033800378 -0.0033811266 -0.0033840621 -0.0033880621 -0.0033924286 -0.0033963076 -0.0033996091 -0.0034022313 -0.0034046776 -0.0034065058 -0.0034062858 -0.00340498 -0.0034029675 -0.0034004662][-0.0033866456 -0.0033837145 -0.0033834917 -0.003384789 -0.0033875711 -0.0033913192 -0.003395098 -0.0033984627 -0.0034016492 -0.0034049456 -0.0034074609 -0.0034078031 -0.0034066637 -0.0034044147 -0.003401242][-0.0033933062 -0.0033904421 -0.0033889187 -0.0033889166 -0.0033901453 -0.003392386 -0.0033946412 -0.0033972275 -0.0034005209 -0.0034042543 -0.0034077917 -0.0034092 -0.003408748 -0.0034067514 -0.0034032997][-0.0033991924 -0.003397102 -0.0033960151 -0.0033955888 -0.0033956307 -0.0033960417 -0.0033967108 -0.003398088 -0.0034005567 -0.0034041558 -0.0034079796 -0.0034095924 -0.0034094006 -0.0034079119 -0.0034048855][-0.0034029237 -0.0034015491 -0.0034011439 -0.0034008508 -0.0034005109 -0.0034002012 -0.0033996855 -0.0034002243 -0.0034016566 -0.0034045004 -0.0034078972 -0.0034094076 -0.0034091359 -0.0034076003 -0.0034048418][-0.0034044785 -0.0034038995 -0.00340419 -0.0034045798 -0.0034043943 -0.0034042161 -0.0034038592 -0.0034039007 -0.0034044979 -0.0034061815 -0.0034085356 -0.0034093342 -0.0034087116 -0.0034068564 -0.0034040925][-0.0034026399 -0.0034029228 -0.0034042797 -0.0034053274 -0.0034060024 -0.0034064678 -0.0034068471 -0.0034072169 -0.0034076998 -0.003408876 -0.0034106621 -0.0034109245 -0.0034094381 -0.0034068013 -0.003403157][-0.0033989218 -0.0033995784 -0.0034018913 -0.0034041684 -0.0034064604 -0.0034076697 -0.0034083966 -0.0034094581 -0.0034102059 -0.0034109894 -0.0034123745 -0.0034124269 -0.0034103054 -0.0034071524 -0.0034029698][-0.0033951069 -0.0033954412 -0.0033980657 -0.0034012953 -0.0034044366 -0.0034067258 -0.0034085631 -0.0034106101 -0.0034121722 -0.0034126348 -0.0034130444 -0.0034124358 -0.0034098283 -0.0034064655 -0.0034022329][-0.0033924773 -0.0033917597 -0.0033940633 -0.0033973539 -0.0034009484 -0.0034040536 -0.003406554 -0.0034088613 -0.0034104839 -0.0034113524 -0.0034117443 -0.0034110474 -0.0034085615 -0.0034051291 -0.0034013577][-0.0033906796 -0.0033894812 -0.0033912172 -0.0033942361 -0.0033976869 -0.0034008441 -0.0034037179 -0.0034067268 -0.0034087023 -0.0034092322 -0.0034092362 -0.0034083847 -0.0034062094 -0.0034031942 -0.0034001188][-0.0033892675 -0.0033878388 -0.0033889117 -0.0033913078 -0.0033945048 -0.003397448 -0.0033999977 -0.0034031826 -0.0034051619 -0.0034057486 -0.0034059261 -0.0034053486 -0.0034034171 -0.0034005465 -0.0033977812][-0.0033880728 -0.0033861145 -0.0033867073 -0.003388464 -0.0033908323 -0.0033933304 -0.0033957786 -0.0033987786 -0.0034009046 -0.0034019309 -0.003402086 -0.0034017095 -0.0033999563 -0.0033975507 -0.0033955493][-0.0033878568 -0.0033854027 -0.0033853038 -0.0033860151 -0.0033874391 -0.0033893578 -0.003391349 -0.0033939928 -0.0033961285 -0.0033974443 -0.0033981218 -0.0033981076 -0.0033969388 -0.0033952626 -0.0033937374][-0.0033879329 -0.0033854581 -0.0033850791 -0.0033852612 -0.0033857564 -0.0033867606 -0.0033880186 -0.0033899322 -0.003391779 -0.0033933595 -0.0033945974 -0.0033949406 -0.0033943981 -0.0033932789 -0.0033922482]]...]
INFO - root - 2017-12-09 15:55:45.848526: step 38410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:28m:27s remains)
INFO - root - 2017-12-09 15:55:54.439485: step 38420, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:39m:47s remains)
INFO - root - 2017-12-09 15:56:02.891745: step 38430, loss = 0.89, batch loss = 0.68 (10.8 examples/sec; 0.742 sec/batch; 60h:36m:35s remains)
INFO - root - 2017-12-09 15:56:11.490101: step 38440, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 70h:52m:12s remains)
INFO - root - 2017-12-09 15:56:20.010934: step 38450, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 73h:09m:02s remains)
INFO - root - 2017-12-09 15:56:28.607725: step 38460, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.726 sec/batch; 59h:18m:13s remains)
INFO - root - 2017-12-09 15:56:37.379086: step 38470, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 73h:24m:05s remains)
INFO - root - 2017-12-09 15:56:46.103888: step 38480, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 72h:21m:16s remains)
INFO - root - 2017-12-09 15:56:54.760859: step 38490, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:45m:13s remains)
INFO - root - 2017-12-09 15:57:03.362009: step 38500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 71h:29m:16s remains)
2017-12-09 15:57:04.250474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033041276 -0.0031177436 -0.0028268048 -0.0024712579 -0.0021524807 -0.0020152568 -0.0021641213 -0.002469 -0.0027776014 -0.0029938614 -0.0030630534 -0.0030923649 -0.0031701138 -0.0032787134 -0.0033664193][-0.0031685468 -0.0028356069 -0.0023665992 -0.001813574 -0.0013403276 -0.0011414457 -0.0013380328 -0.0017864738 -0.0022760853 -0.0026130634 -0.0027119447 -0.002756682 -0.0028959108 -0.0031201784 -0.0033059341][-0.0029050983 -0.0023093221 -0.0015344041 -0.00064465147 0.00013027899 0.00053281244 0.00033034035 -0.00035780226 -0.0012122847 -0.0018418176 -0.0020765457 -0.0021931473 -0.0024388388 -0.0028402146 -0.0031850627][-0.0025008651 -0.0015284141 -0.00031867926 0.001037701 0.0022549978 0.0029913089 0.0028412128 0.0018753954 0.00052420655 -0.00054632709 -0.0010445456 -0.0013361869 -0.0017682138 -0.002426422 -0.0029926582][-0.0020172594 -0.0006211435 0.0010877778 0.0029875454 0.0047748322 0.0059868339 0.0059518209 0.0046582725 0.0027150149 0.0010686298 0.00016767113 -0.00037823385 -0.001020463 -0.0019326797 -0.0027423371][-0.0015932 0.00013903133 0.0022740304 0.0046361033 0.0069246972 0.008578185 0.0087081119 0.0071880603 0.0047467034 0.0025643203 0.0012374786 0.00040949252 -0.00042338809 -0.0015213659 -0.002515445][-0.0013532119 0.00051953504 0.0028979662 0.005539272 0.0081003634 0.0099673318 0.010178788 0.0085339369 0.0058217319 0.003307825 0.0016827998 0.00068752421 -0.00021891692 -0.001354957 -0.00240938][-0.0013737576 0.00040006661 0.0027662111 0.005428372 0.007978213 0.0097837979 0.009957266 0.00830446 0.005571452 0.0030130376 0.0013143546 0.00030778442 -0.00053835521 -0.0015498276 -0.0024980986][-0.0016365004 -0.00016091927 0.0019304384 0.004340528 0.0066048931 0.0081223976 0.0081694219 0.0066372566 0.0041561038 0.001854389 0.00031869835 -0.000543867 -0.0012319041 -0.0020202021 -0.0027439273][-0.0020812922 -0.0010170175 0.00059068343 0.0025094938 0.0042970595 0.0054169735 0.00533483 0.00405858 0.0020569386 0.00025228574 -0.00094520091 -0.0015846759 -0.0020655158 -0.0025845002 -0.0030375002][-0.0025811857 -0.0019306906 -0.00089030969 0.00040566223 0.0016164146 0.0023303884 0.0021836651 0.0012621903 -0.00012496812 -0.0013192429 -0.0021020281 -0.0024958351 -0.0027673354 -0.0030375533 -0.0032555207][-0.0030129675 -0.0026866938 -0.0021352647 -0.0014167654 -0.00073996349 -0.00036488357 -0.00050123781 -0.0010581894 -0.001852904 -0.00249091 -0.0028978761 -0.0030887141 -0.0032016246 -0.0032987567 -0.0033685965][-0.0032784087 -0.0031518484 -0.0029302523 -0.0026280838 -0.0023423797 -0.0021932675 -0.0022802565 -0.0025392631 -0.0028841058 -0.0031322907 -0.0032815826 -0.0033447808 -0.0033735349 -0.0033915658 -0.0034017076][-0.003391589 -0.0033546989 -0.0032868811 -0.0031946886 -0.003112206 -0.0030753529 -0.0031138475 -0.003199399 -0.0033011485 -0.0033636836 -0.0033961111 -0.0034063826 -0.0034082462 -0.0034072811 -0.0034052946][-0.0034088984 -0.003405645 -0.0033963991 -0.003380924 -0.0033664368 -0.0033593974 -0.0033689498 -0.0033857995 -0.00340194 -0.0034080765 -0.0034099363 -0.0034104125 -0.0034095163 -0.0034075845 -0.0034051659]]...]
INFO - root - 2017-12-09 15:57:12.762398: step 38510, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 69h:13m:42s remains)
INFO - root - 2017-12-09 15:57:21.331206: step 38520, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 70h:16m:42s remains)
INFO - root - 2017-12-09 15:57:29.966072: step 38530, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 59h:25m:46s remains)
INFO - root - 2017-12-09 15:57:38.614553: step 38540, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:17m:15s remains)
INFO - root - 2017-12-09 15:57:47.235328: step 38550, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 73h:45m:33s remains)
INFO - root - 2017-12-09 15:57:55.820014: step 38560, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 59h:10m:59s remains)
INFO - root - 2017-12-09 15:58:04.523557: step 38570, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 67h:14m:09s remains)
INFO - root - 2017-12-09 15:58:13.257920: step 38580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:39m:20s remains)
INFO - root - 2017-12-09 15:58:21.878556: step 38590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:54m:41s remains)
INFO - root - 2017-12-09 15:58:30.549434: step 38600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 73h:01m:31s remains)
2017-12-09 15:58:31.359664: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00056969724 0.0024465562 0.0041224854 0.0067600645 0.012212594 0.020947821 0.031428989 0.040701821 0.047018521 0.049156703 0.047346573 0.042414147 0.035107814 0.026597423 0.017704636][-9.7266166e-05 0.0016559723 0.0041967537 0.0089479182 0.01764231 0.0299657 0.043507043 0.054928057 0.062364455 0.065054849 0.063492633 0.058472794 0.050175365 0.03912624 0.027174575][-0.0010060607 0.00075439061 0.0041701393 0.011116801 0.022971245 0.038645502 0.055053752 0.068718635 0.077570878 0.081020996 0.079677112 0.074131519 0.064825669 0.052236792 0.037896991][-0.0019437638 -0.00018760492 0.0042649042 0.012940009 0.027360141 0.045643829 0.064216293 0.079813465 0.090008162 0.094420925 0.093633652 0.088159077 0.078378253 0.064576134 0.048107781][-0.0024312651 -0.00074515538 0.0041237166 0.014252898 0.030382434 0.05020554 0.0701495 0.087245576 0.098372847 0.10341007 0.10284761 0.097493947 0.087803438 0.07372722 0.056064151][-0.002220341 -0.00082096667 0.004147972 0.01477765 0.031821784 0.052498288 0.073255874 0.091163434 0.10251877 0.10744388 0.10624175 0.10040513 0.090678595 0.076794952 0.05900484][-0.0014593718 -0.00033008959 0.004164435 0.014402454 0.03107108 0.05174651 0.072846025 0.090922624 0.10191996 0.10551628 0.10250951 0.095443986 0.085452087 0.072175324 0.055324879][-0.00067012594 0.00023819553 0.004004675 0.012812198 0.027863277 0.047232039 0.067400262 0.084561892 0.094354443 0.096039034 0.091131315 0.082673371 0.072305754 0.059952315 0.045041572][-0.00051180646 0.00022841641 0.00312241 0.01004849 0.022380983 0.038798302 0.056270212 0.07076592 0.078305893 0.078072384 0.071917713 0.063125215 0.053381719 0.042658832 0.030642895][-0.00073745358 -0.00041901297 0.0013844909 0.006181783 0.015056784 0.02739824 0.040600285 0.051280688 0.056145389 0.054453868 0.048269238 0.040490575 0.032487374 0.024352154 0.016061844][-0.0014071723 -0.0011851832 -0.00028194231 0.0023528852 0.007490783 0.01521512 0.023575982 0.030306341 0.032878436 0.030898081 0.02586784 0.020073541 0.014535065 0.0094400691 0.0048986655][-0.0023270599 -0.0021489738 -0.0018064963 -0.00074146129 0.0015436916 0.0053875716 0.0097129177 0.013324565 0.014579047 0.013160617 0.010021617 0.0065646404 0.0034940138 0.0010114468 -0.00085004629][-0.0030915977 -0.002924969 -0.002780084 -0.0024852473 -0.0017885513 -0.00049350783 0.0011307306 0.002633488 0.0031423706 0.0024976644 0.0010818678 -0.00042237807 -0.0016459303 -0.0024807071 -0.0029604882][-0.0033832991 -0.0033791414 -0.0033569885 -0.0032652956 -0.0030926364 -0.0028013259 -0.0024138947 -0.00205496 -0.0019261306 -0.0021042698 -0.0024801488 -0.00285042 -0.0031177041 -0.003266667 -0.0033319129][-0.0033822178 -0.0033807429 -0.0033817596 -0.0033826048 -0.0033822597 -0.0033748935 -0.0033581352 -0.0033388115 -0.0033240994 -0.0033208812 -0.0033229808 -0.0033281939 -0.0033361742 -0.0033458669 -0.0033576596]]...]
INFO - root - 2017-12-09 15:58:40.068332: step 38610, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 72h:14m:51s remains)
INFO - root - 2017-12-09 15:58:48.792611: step 38620, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:38m:54s remains)
INFO - root - 2017-12-09 15:58:57.329283: step 38630, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 59h:04m:16s remains)
INFO - root - 2017-12-09 15:59:06.114232: step 38640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:16m:28s remains)
INFO - root - 2017-12-09 15:59:14.693305: step 38650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:04m:51s remains)
INFO - root - 2017-12-09 15:59:23.232964: step 38660, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 61h:36m:18s remains)
INFO - root - 2017-12-09 15:59:31.940158: step 38670, loss = 0.90, batch loss = 0.70 (8.4 examples/sec; 0.950 sec/batch; 77h:30m:40s remains)
INFO - root - 2017-12-09 15:59:40.495486: step 38680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:36m:02s remains)
INFO - root - 2017-12-09 15:59:49.098049: step 38690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:39m:32s remains)
INFO - root - 2017-12-09 15:59:57.695452: step 38700, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 69h:08m:50s remains)
2017-12-09 15:59:58.556435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1516959e-05 -0.0011473459 -0.0020433364 -0.0026100227 -0.0027532759 -0.0024154326 -0.0015469915 -0.00035634986 0.00079298369 0.0014973299 0.0014894202 0.00073742843 -0.00045210845 -0.0016238779 -0.0024802489][-0.00032531191 -0.0013885435 -0.0022501785 -0.0027605887 -0.0028055902 -0.0023055868 -0.0011756262 0.00033349893 0.0017888646 0.0027200284 0.0028155951 0.0020319337 0.00065822038 -0.00078810449 -0.0019369026][-0.0011453421 -0.0020031976 -0.0026479862 -0.0029797708 -0.002857 -0.0021893708 -0.00087003736 0.00083019235 0.0024700018 0.0035670742 0.0037718292 0.0030417291 0.0016532412 0.00010417448 -0.0012202091][-0.0020803381 -0.0026369062 -0.0029965888 -0.003115912 -0.0028305808 -0.0020472929 -0.000642224 0.0011235431 0.0028451828 0.0040629935 0.0043869447 0.003789718 0.0025542059 0.0010840578 -0.00025938358][-0.002781745 -0.0030520936 -0.0031643966 -0.0031095527 -0.0027157373 -0.0018863741 -0.0004770048 0.0012845404 0.0030303949 0.0043334989 0.00478571 0.0043879068 0.0034226489 0.0021618095 0.00093009719][-0.0031569563 -0.0032155791 -0.0031531565 -0.0029786816 -0.0025258213 -0.0016881034 -0.00029765023 0.001454632 0.0031962933 0.0045278268 0.0050776824 0.0048925458 0.0042234445 0.0032101336 0.0021531584][-0.0032953953 -0.0032232979 -0.0030497368 -0.0027944581 -0.0023023107 -0.0014499763 -5.7983678e-05 0.0016909337 0.0033780679 0.0046370635 0.0051894579 0.0051461635 0.00469907 0.0039061739 0.0030377496][-0.0033318279 -0.0032000463 -0.0029738613 -0.0026723326 -0.0021554809 -0.0012791422 0.00011387374 0.0018203787 0.0033610195 0.004420762 0.0048435871 0.0048093731 0.0044694208 0.0038554452 0.0031817572][-0.0033519375 -0.0032215612 -0.003006598 -0.0027083906 -0.0022067374 -0.0013480787 -3.121933e-05 0.0015188165 0.0027949556 0.0035561875 0.0037580768 0.0036296898 0.0033146713 0.0028596765 0.0023781138][-0.0033771186 -0.0032877724 -0.0031333303 -0.0028957494 -0.0024763776 -0.0017299311 -0.00062244991 0.00062524457 0.0015475147 0.0019845248 0.0019660306 0.001734562 0.0014368205 0.0011253732 0.00081523345][-0.0033983914 -0.0033555781 -0.0032737644 -0.0031286746 -0.0028417883 -0.0022995486 -0.0015110937 -0.00066369469 -0.00010580476 7.2490191e-05 -7.0844544e-05 -0.00032803998 -0.00057075592 -0.000759952 -0.00093694241][-0.0034092702 -0.0033958047 -0.0033660335 -0.0033011336 -0.0031480105 -0.0028331243 -0.0023780209 -0.0019164338 -0.0016495783 -0.0016174058 -0.0017664196 -0.001961889 -0.0021156596 -0.0022130543 -0.0022984152][-0.0034131196 -0.0034102383 -0.0034040627 -0.0033846996 -0.0033252896 -0.0031876096 -0.0029842837 -0.0027904061 -0.0026944645 -0.0027099054 -0.0028018334 -0.0029062356 -0.0029790411 -0.0030197476 -0.0030528947][-0.0034138833 -0.0034134362 -0.003413283 -0.0034099752 -0.0033948056 -0.0033534062 -0.0032891768 -0.0032320635 -0.0032091532 -0.0032221912 -0.0032562492 -0.0032908525 -0.0033127416 -0.0033253778 -0.0033350603][-0.0034141021 -0.0034138663 -0.0034139818 -0.003413989 -0.0034126374 -0.0034057663 -0.003393136 -0.0033822302 -0.003378039 -0.003381181 -0.0033873888 -0.0033924456 -0.0033948666 -0.0033976836 -0.0034010892]]...]
INFO - root - 2017-12-09 16:00:07.137139: step 38710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:24m:04s remains)
INFO - root - 2017-12-09 16:00:15.902188: step 38720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:29m:45s remains)
INFO - root - 2017-12-09 16:00:24.554969: step 38730, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 61h:31m:36s remains)
INFO - root - 2017-12-09 16:00:33.444612: step 38740, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 70h:46m:23s remains)
INFO - root - 2017-12-09 16:00:42.003272: step 38750, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 72h:26m:34s remains)
INFO - root - 2017-12-09 16:00:50.706341: step 38760, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 72h:39m:06s remains)
INFO - root - 2017-12-09 16:00:59.239001: step 38770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 72h:04m:44s remains)
INFO - root - 2017-12-09 16:01:07.892555: step 38780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 71h:04m:25s remains)
INFO - root - 2017-12-09 16:01:16.543082: step 38790, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:15m:29s remains)
INFO - root - 2017-12-09 16:01:25.279839: step 38800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 71h:22m:09s remains)
2017-12-09 16:01:26.246826: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027763084 0.035796914 0.04162756 0.044538561 0.044138193 0.042029306 0.039080471 0.036178119 0.033065263 0.030077927 0.027185895 0.024246505 0.020851437 0.01659294 0.011734774][0.042261504 0.05402001 0.063497514 0.069453627 0.071139745 0.069888592 0.066603169 0.062485758 0.057789583 0.053256787 0.048667386 0.043683827 0.037733778 0.030388355 0.022157129][0.060378354 0.076603837 0.090471923 0.10014284 0.10450855 0.10404175 0.09988641 0.093703754 0.086766452 0.080432095 0.074329577 0.067634985 0.0591789 0.048350658 0.036017023][0.080534264 0.10185997 0.12095197 0.13543148 0.1433842 0.14413396 0.13873993 0.12959759 0.11931241 0.1099912 0.10130432 0.092207409 0.080952123 0.066821516 0.050840519][0.1001804 0.12646808 0.15039891 0.16942729 0.18080388 0.18258758 0.17577334 0.16340779 0.14962645 0.13725872 0.12612325 0.11487676 0.10125075 0.084180132 0.064935647][0.1157599 0.14547366 0.17239119 0.19416225 0.20736174 0.20924178 0.20073625 0.18550856 0.16867357 0.1537707 0.14111575 0.12908159 0.11483619 0.096754745 0.075980708][0.12353207 0.15458526 0.18213604 0.20447496 0.21775439 0.21863304 0.20804809 0.190118 0.17079081 0.1543113 0.14126326 0.13008201 0.11733131 0.10068848 0.080855623][0.12362862 0.15368275 0.17965807 0.20037751 0.21188073 0.21082455 0.19806631 0.17798348 0.15692325 0.13955896 0.12703249 0.11794572 0.1083582 0.095277138 0.078651093][0.11663695 0.14405288 0.1667399 0.18404841 0.19250776 0.18916377 0.1749035 0.15400475 0.13267988 0.11565882 0.10436174 0.097824067 0.09199027 0.083342344 0.070948973][0.10307823 0.12662251 0.14503387 0.15794356 0.16280676 0.15740091 0.14284079 0.12295128 0.10325456 0.088033721 0.078828037 0.074912354 0.072514132 0.067903183 0.059658337][0.084299624 0.10290291 0.1164527 0.12484185 0.12650914 0.11997353 0.10642342 0.089079022 0.072550215 0.060354095 0.05385343 0.052490041 0.05283574 0.051426739 0.046680972][0.062683307 0.075917445 0.084756777 0.089142352 0.088380978 0.081737339 0.070340186 0.056677639 0.044189245 0.035519417 0.031604297 0.032008007 0.034071293 0.034825962 0.0328809][0.041353967 0.049744915 0.054728787 0.056147274 0.053920176 0.047958825 0.039327823 0.029870056 0.021719828 0.016502623 0.014716405 0.01603068 0.018677916 0.020459123 0.020323146][0.022116609 0.026729796 0.029102221 0.028995402 0.026617225 0.022237528 0.016750613 0.011294249 0.0069549689 0.0045466237 0.0041948305 0.0056717945 0.0079627689 0.0096941283 0.010246853][0.007709749 0.0097407307 0.010591224 0.01009852 0.0084890006 0.0061130887 0.0035327508 0.0012577258 -0.00035791332 -0.0010513451 -0.00080024381 0.00028000283 0.0017275915 0.0028590008 0.0033624249]]...]
INFO - root - 2017-12-09 16:01:34.802801: step 38810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:11m:24s remains)
INFO - root - 2017-12-09 16:01:43.557783: step 38820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:32m:06s remains)
INFO - root - 2017-12-09 16:01:51.955948: step 38830, loss = 0.91, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 61h:30m:59s remains)
INFO - root - 2017-12-09 16:02:00.351889: step 38840, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 67h:06m:24s remains)
INFO - root - 2017-12-09 16:02:08.791949: step 38850, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 72h:57m:03s remains)
INFO - root - 2017-12-09 16:02:17.542310: step 38860, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 72h:52m:19s remains)
INFO - root - 2017-12-09 16:02:26.056945: step 38870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:18m:38s remains)
INFO - root - 2017-12-09 16:02:34.619031: step 38880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:41m:57s remains)
INFO - root - 2017-12-09 16:02:43.104787: step 38890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:36m:37s remains)
INFO - root - 2017-12-09 16:02:51.684467: step 38900, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 73h:14m:51s remains)
2017-12-09 16:02:52.545547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011959558 -0.0012874193 -0.0014612409 -0.0018970768 -0.0023219672 -0.0026144404 -0.0027783616 -0.0026371479 -0.0025736922 -0.0027415829 -0.0029105262 -0.003050793 -0.003168948 -0.0032681099 -0.0033180583][-0.0012768896 -0.0011085179 -0.0010532171 -0.0012107952 -0.0015140037 -0.0019790605 -0.0023771864 -0.0024688172 -0.0027142433 -0.0029321271 -0.0030293108 -0.0030385461 -0.0030351703 -0.0030458153 -0.0030342164][-0.0022557792 -0.0020930492 -0.0018991326 -0.0017152284 -0.0016262328 -0.0017129639 -0.0018625908 -0.0021318682 -0.0024576762 -0.0026613656 -0.0028031094 -0.002903857 -0.0029988326 -0.0030802018 -0.0031439718][-0.0021905941 -0.0021077788 -0.0019574559 -0.0018015433 -0.0017074186 -0.0015122953 -0.0013876725 -0.0014001429 -0.0015171253 -0.0017253055 -0.0018991588 -0.0020789602 -0.0022006948 -0.0022660743 -0.0023051875][-0.0020593244 -0.0020666798 -0.0020047487 -0.001990573 -0.001976294 -0.0018737066 -0.0017658073 -0.0015300756 -0.001378987 -0.0013902867 -0.0014934775 -0.0017048673 -0.0019384266 -0.0021890015 -0.0023857409][-0.00085796393 -0.00060342788 -0.000371031 -0.00026061456 -0.0003204348 -0.00049141655 -0.00071858382 -0.0009431981 -0.0011695491 -0.0013424247 -0.0014909112 -0.0019078205 -0.0022411323 -0.002568441 -0.0027530147][-3.8558152e-05 0.00049204915 0.00088678231 0.0011920647 0.0011468178 0.00083729322 0.00033973553 -0.00017227372 -0.00071984297 -0.0011487096 -0.0014728182 -0.0017171891 -0.0019020899 -0.002014474 -0.0019279897][0.0022530712 0.0027293323 0.0030784749 0.0031127629 0.0028811919 0.0023422686 0.0015934033 0.000832672 9.4457529e-05 -0.00042406726 -0.00083519472 -0.00086297863 -0.00070153526 -0.00021712319 0.0003733167][0.0052030673 0.0057028616 0.0061194133 0.0063158888 0.0059959274 0.0053700041 0.00454292 0.0036860292 0.002864117 0.0021231554 0.0016732144 0.0016423643 0.001938171 0.002431514 0.0029689914][0.0079329815 0.0086409356 0.0092456639 0.0096792448 0.0097196521 0.0092687681 0.0085394606 0.0076711429 0.0067674536 0.005946218 0.0054188864 0.0052883755 0.0054415828 0.0056298971 0.0057873065][0.0098872436 0.010710436 0.01157176 0.012379519 0.013002456 0.013122034 0.012793367 0.012118776 0.011213587 0.010243477 0.0094037773 0.0087870685 0.0083687864 0.0079030171 0.0073848013][0.0110493 0.011884767 0.01279389 0.013656368 0.014350596 0.014592347 0.014365238 0.01374781 0.012806664 0.011678287 0.010540773 0.0094776759 0.0085080769 0.007472177 0.0064063752][0.011526014 0.01209625 0.012711904 0.013225616 0.013550441 0.013570818 0.013399635 0.012942002 0.012164589 0.011075741 0.0098307068 0.0084942738 0.0071491385 0.0057090139 0.0042498112][0.012042969 0.012297955 0.012539837 0.012603835 0.012547915 0.012263716 0.011904986 0.011414968 0.010654937 0.0095403614 0.0081832726 0.0066641 0.0051106084 0.003463757 0.0018445512][0.012494505 0.012348405 0.012202439 0.011944954 0.011705122 0.011203137 0.010658115 0.010014572 0.0091098938 0.0078427792 0.0063170083 0.0046347734 0.0029628843 0.0012957442 -0.00020144437]]...]
INFO - root - 2017-12-09 16:03:01.150225: step 38910, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 72h:41m:15s remains)
INFO - root - 2017-12-09 16:03:09.883924: step 38920, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 69h:54m:52s remains)
INFO - root - 2017-12-09 16:03:18.619994: step 38930, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 61h:32m:33s remains)
INFO - root - 2017-12-09 16:03:27.280017: step 38940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:58m:23s remains)
INFO - root - 2017-12-09 16:03:35.743422: step 38950, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 71h:08m:01s remains)
INFO - root - 2017-12-09 16:03:44.370779: step 38960, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 66h:18m:23s remains)
INFO - root - 2017-12-09 16:03:52.942826: step 38970, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 72h:10m:46s remains)
INFO - root - 2017-12-09 16:04:01.586221: step 38980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:42m:44s remains)
INFO - root - 2017-12-09 16:04:10.252127: step 38990, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 67h:40m:54s remains)
INFO - root - 2017-12-09 16:04:18.781869: step 39000, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 67h:44m:06s remains)
2017-12-09 16:04:19.599809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00339376 -0.0033930442 -0.0033930014 -0.0033931278 -0.003393146 -0.0033931024 -0.0033930391 -0.0033931192 -0.0033933041 -0.0033934787 -0.0033936531 -0.0033938277 -0.0033939239 -0.0033938806 -0.0033936852][-0.0033910058 -0.0033900065 -0.0033899669 -0.0033900898 -0.0033901494 -0.0033902165 -0.0033902421 -0.0033904593 -0.0033907623 -0.0033909425 -0.003391102 -0.0033912386 -0.0033912954 -0.0033912263 -0.003391054][-0.0033910195 -0.0033900286 -0.0033900561 -0.0033900442 -0.003389962 -0.003389857 -0.0033898463 -0.0033900617 -0.0033902978 -0.0033903983 -0.0033905376 -0.0033906908 -0.0033906491 -0.0033905397 -0.0033904857][-0.0033913727 -0.003390505 -0.0033904135 -0.0033904321 -0.0033904691 -0.0033905176 -0.0033906447 -0.0033906589 -0.0033904389 -0.0033901213 -0.003390094 -0.0033901404 -0.0033900151 -0.0033898682 -0.0033899278][-0.0033921993 -0.0033917089 -0.0033915853 -0.0033913853 -0.0033911811 -0.003391278 -0.0033915259 -0.003391406 -0.0033907515 -0.0033900265 -0.0033898109 -0.0033898284 -0.0033896088 -0.0033894479 -0.0033895944][-0.0033935062 -0.0033932007 -0.0033933166 -0.0033932161 -0.0033930843 -0.0033925027 -0.0033917236 -0.0033909376 -0.0033898086 -0.0033890328 -0.0033889308 -0.0033890647 -0.0033889918 -0.003388955 -0.0033891622][-0.0033946109 -0.0033943215 -0.0033947502 -0.0033947057 -0.0033942577 -0.0033929006 -0.0033907196 -0.0033880814 -0.0033857014 -0.0033850388 -0.0033861997 -0.0033874935 -0.0033881902 -0.0033885171 -0.0033887557][-0.0033956408 -0.0033943441 -0.0033942135 -0.0033937336 -0.0033927937 -0.0033906931 -0.0033875031 -0.0033830449 -0.0033790455 -0.0033781859 -0.0033811296 -0.0033846437 -0.0033868982 -0.0033880004 -0.0033885324][-0.0033990075 -0.0033956058 -0.0033926775 -0.0033900463 -0.0033879145 -0.0033852409 -0.0033819252 -0.0033773684 -0.0033741328 -0.003373631 -0.0033766222 -0.0033813061 -0.0033852789 -0.003387426 -0.0033884833][-0.003404828 -0.0033996869 -0.0033936871 -0.0033877317 -0.0033828048 -0.0033782439 -0.0033751463 -0.0033729794 -0.0033719828 -0.0033728653 -0.0033761235 -0.003380619 -0.0033844819 -0.0033869888 -0.0033882747][-0.0034094776 -0.0034044769 -0.0033975367 -0.0033885271 -0.0033795177 -0.003372971 -0.0033710797 -0.0033725156 -0.0033749526 -0.00337741 -0.0033803161 -0.0033837387 -0.0033860365 -0.0033872915 -0.0033880882][-0.003412259 -0.0034078255 -0.0034008219 -0.0033908305 -0.0033800735 -0.0033719416 -0.0033707302 -0.0033746401 -0.0033801007 -0.0033844572 -0.0033871757 -0.0033889066 -0.0033891734 -0.0033887147 -0.0033884528][-0.0034128574 -0.0034104555 -0.00340402 -0.0033941837 -0.0033833552 -0.003376106 -0.0033754008 -0.0033797568 -0.0033857769 -0.0033899581 -0.0033923145 -0.0033926668 -0.0033917618 -0.0033903865 -0.0033892882][-0.0034119643 -0.003411615 -0.003407584 -0.0034001058 -0.0033906209 -0.0033828688 -0.0033810707 -0.003384308 -0.0033891117 -0.0033926978 -0.003393922 -0.0033936196 -0.0033924449 -0.0033909993 -0.0033898228][-0.0034094446 -0.0034112046 -0.003410063 -0.0034062257 -0.0033995295 -0.0033919646 -0.0033870377 -0.003387196 -0.0033899609 -0.0033920307 -0.0033928207 -0.0033920908 -0.0033910233 -0.0033900805 -0.0033893327]]...]
INFO - root - 2017-12-09 16:04:28.112919: step 39010, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.829 sec/batch; 67h:33m:41s remains)
INFO - root - 2017-12-09 16:04:36.777999: step 39020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:06m:39s remains)
INFO - root - 2017-12-09 16:04:45.265014: step 39030, loss = 0.89, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 62h:22m:53s remains)
INFO - root - 2017-12-09 16:04:53.961801: step 39040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:52m:52s remains)
INFO - root - 2017-12-09 16:05:02.540334: step 39050, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 70h:52m:04s remains)
INFO - root - 2017-12-09 16:05:11.296008: step 39060, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 71h:15m:27s remains)
INFO - root - 2017-12-09 16:05:19.982465: step 39070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:59m:13s remains)
INFO - root - 2017-12-09 16:05:28.507933: step 39080, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 70h:27m:59s remains)
INFO - root - 2017-12-09 16:05:36.912482: step 39090, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.826 sec/batch; 67h:18m:06s remains)
INFO - root - 2017-12-09 16:05:45.438642: step 39100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:42m:12s remains)
2017-12-09 16:05:46.354839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2618843 0.31311783 0.36067536 0.39738065 0.42604524 0.44562805 0.45823 0.4654907 0.4669089 0.46641535 0.4628301 0.46278349 0.46088856 0.45916614 0.45554402][0.2709997 0.32502505 0.37569848 0.41442427 0.44389072 0.46508905 0.47908661 0.48673293 0.4876487 0.48634282 0.48106512 0.47784126 0.473854 0.47117493 0.46791437][0.26432317 0.31836268 0.36859968 0.40803075 0.43893233 0.46023464 0.47428876 0.48288617 0.48448548 0.48218805 0.47499043 0.47029102 0.4659633 0.46347889 0.46067122][0.24953638 0.30441839 0.35580605 0.39657244 0.42867723 0.45243841 0.46856612 0.47801739 0.48030993 0.47879371 0.47158051 0.46526581 0.46030065 0.45818883 0.45628756][0.23113558 0.28700414 0.33879393 0.38165855 0.41588348 0.44100842 0.45856023 0.47075832 0.47526619 0.47373757 0.46626925 0.45837477 0.45314649 0.44985983 0.44744903][0.21074562 0.26651856 0.31762874 0.36115322 0.39549482 0.42128497 0.44000638 0.45374912 0.46038288 0.45966235 0.45377645 0.44561151 0.43928412 0.43409118 0.43117824][0.18951695 0.2426917 0.29144171 0.33357605 0.36595812 0.39140907 0.40995827 0.42471352 0.43333715 0.43423626 0.42997503 0.42126685 0.41431403 0.40789104 0.40298495][0.16745034 0.21626258 0.26028782 0.29885975 0.32827076 0.35102442 0.36774659 0.38249269 0.39183906 0.39304978 0.38883144 0.38002038 0.37306604 0.36500645 0.358482][0.14255914 0.18453775 0.22246535 0.25465792 0.27782711 0.29584602 0.30896732 0.32101521 0.32867888 0.3302564 0.32737038 0.31933269 0.31219885 0.30336803 0.29637221][0.11610588 0.1485412 0.17727517 0.20126848 0.21771675 0.22916043 0.23707493 0.24488595 0.25009865 0.25072977 0.24856736 0.24300486 0.23773009 0.23015687 0.22327541][0.0879805 0.11073812 0.13042508 0.1458859 0.15566005 0.16140343 0.1647047 0.16813459 0.17045881 0.17080854 0.1694012 0.16633876 0.16367491 0.15907028 0.15406306][0.059220959 0.073657595 0.085701361 0.094519839 0.09964025 0.10164926 0.10260706 0.10350811 0.10405288 0.10358074 0.10257215 0.10158557 0.10066906 0.09851931 0.095657326][0.033590384 0.041439421 0.047482174 0.051296998 0.053046983 0.053396638 0.053353142 0.053134222 0.053160317 0.052991904 0.052587286 0.052220304 0.052053094 0.051610153 0.050337762][0.013872724 0.0171419 0.019344024 0.020374414 0.020347213 0.019784953 0.019196903 0.018764693 0.018650118 0.018570326 0.018560557 0.018634563 0.018731104 0.018725509 0.018429277][0.0022282994 0.0031210741 0.0035479174 0.0034165059 0.0029458564 0.0023956273 0.0019329737 0.0016170286 0.0014942053 0.0015262708 0.0016125937 0.0017117665 0.0018314391 0.0018903937 0.0018466844]]...]
INFO - root - 2017-12-09 16:05:54.865943: step 39110, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 66h:41m:46s remains)
INFO - root - 2017-12-09 16:06:03.453216: step 39120, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.904 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-09 16:06:11.908490: step 39130, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 58h:41m:43s remains)
INFO - root - 2017-12-09 16:06:20.524037: step 39140, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 71h:08m:28s remains)
INFO - root - 2017-12-09 16:06:29.169037: step 39150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:01m:45s remains)
INFO - root - 2017-12-09 16:06:37.815032: step 39160, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 68h:18m:36s remains)
INFO - root - 2017-12-09 16:06:46.204642: step 39170, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:25m:07s remains)
INFO - root - 2017-12-09 16:06:54.933328: step 39180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:36m:30s remains)
INFO - root - 2017-12-09 16:07:03.647900: step 39190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:04m:57s remains)
INFO - root - 2017-12-09 16:07:12.329379: step 39200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:58m:30s remains)
2017-12-09 16:07:13.220650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033704729 -0.0033682284 -0.0033688762 -0.0033703789 -0.0033719738 -0.0033729526 -0.0033737379 -0.0033741831 -0.0033744494 -0.0033744944 -0.0033744613 -0.0033743023 -0.0033739808 -0.00337346 -0.0033728967][-0.0033686771 -0.0033658799 -0.0033663013 -0.0033676187 -0.0033690743 -0.0033699896 -0.0033707803 -0.0033711994 -0.0033715123 -0.003371676 -0.0033716869 -0.003371628 -0.0033712038 -0.0033705903 -0.0033699442][-0.0033690198 -0.0033657481 -0.0033656782 -0.0033666198 -0.0033677085 -0.003368421 -0.0033691702 -0.0033695556 -0.0033698762 -0.0033701905 -0.0033703838 -0.0033704145 -0.0033700329 -0.0033694338 -0.0033688142][-0.0033694489 -0.0033657916 -0.0033651695 -0.0033656387 -0.0033662864 -0.0033667437 -0.0033673348 -0.0033675821 -0.0033677716 -0.0033681833 -0.0033687183 -0.0033690352 -0.0033687854 -0.0033683411 -0.0033678373][-0.0033703565 -0.0033663139 -0.0033653048 -0.0033653826 -0.0033656189 -0.0033657164 -0.0033660498 -0.0033659898 -0.0033659362 -0.0033664049 -0.0033672359 -0.0033679628 -0.0033681153 -0.0033682031 -0.0033680382][-0.0033713563 -0.0033671879 -0.0033660089 -0.00336592 -0.0033659127 -0.0033658028 -0.0033658873 -0.0033654885 -0.0033653539 -0.003365912 -0.0033669418 -0.0033678797 -0.0033684608 -0.0033690722 -0.0033693765][-0.0033719381 -0.0033677572 -0.0033666391 -0.0033666033 -0.0033665262 -0.0033663779 -0.0033663288 -0.0033656589 -0.003365455 -0.0033660706 -0.0033671027 -0.0033681125 -0.0033689847 -0.003369957 -0.0033706555][-0.0033723528 -0.00336854 -0.003367689 -0.0033678727 -0.0033678804 -0.0033676755 -0.0033674114 -0.0033664659 -0.0033659548 -0.0033664505 -0.0033674221 -0.0033683432 -0.0033692459 -0.0033704331 -0.0033714497][-0.003372879 -0.003369523 -0.0033690357 -0.0033695747 -0.0033698804 -0.0033698613 -0.0033697421 -0.0033689167 -0.003368105 -0.0033681302 -0.003368688 -0.003369316 -0.003370055 -0.0033711386 -0.0033721391][-0.0033739435 -0.0033710892 -0.0033709244 -0.0033715738 -0.0033721004 -0.0033722613 -0.0033723081 -0.0033717647 -0.0033711141 -0.003370787 -0.0033708729 -0.0033711493 -0.0033716313 -0.0033723463 -0.003373119][-0.0033750781 -0.0033725044 -0.0033725367 -0.0033730746 -0.0033735232 -0.0033737672 -0.0033739679 -0.0033737561 -0.0033734585 -0.0033731777 -0.003373089 -0.0033731188 -0.0033733067 -0.0033736667 -0.0033741437][-0.0033760674 -0.003373727 -0.0033738294 -0.0033741228 -0.003374316 -0.0033744304 -0.0033745964 -0.0033744967 -0.0033743551 -0.0033741728 -0.00337404 -0.0033740315 -0.0033741191 -0.0033743484 -0.003374693][-0.0033765917 -0.0033743063 -0.0033744443 -0.00337456 -0.0033745628 -0.00337457 -0.0033746907 -0.0033746155 -0.0033745705 -0.0033744972 -0.0033744422 -0.0033744548 -0.003374544 -0.003374734 -0.0033749423][-0.0033766537 -0.0033744266 -0.0033745782 -0.0033746061 -0.0033745845 -0.0033746115 -0.0033746986 -0.0033746948 -0.0033747389 -0.0033747768 -0.0033748068 -0.0033748536 -0.0033749004 -0.0033749936 -0.0033750779][-0.0033769414 -0.003374686 -0.003374798 -0.0033747992 -0.0033747754 -0.00337477 -0.0033747966 -0.0033748129 -0.0033748564 -0.0033749065 -0.0033749572 -0.0033749999 -0.003375022 -0.0033750471 -0.003375083]]...]
INFO - root - 2017-12-09 16:07:21.826681: step 39210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:52m:32s remains)
INFO - root - 2017-12-09 16:07:30.473724: step 39220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:50m:36s remains)
INFO - root - 2017-12-09 16:07:39.086321: step 39230, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 60h:43m:13s remains)
INFO - root - 2017-12-09 16:07:47.816134: step 39240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:08m:11s remains)
INFO - root - 2017-12-09 16:07:56.426928: step 39250, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 73h:35m:47s remains)
INFO - root - 2017-12-09 16:08:05.169438: step 39260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:51m:10s remains)
INFO - root - 2017-12-09 16:08:13.717629: step 39270, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 70h:42m:05s remains)
INFO - root - 2017-12-09 16:08:22.272479: step 39280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:37m:27s remains)
INFO - root - 2017-12-09 16:08:30.991100: step 39290, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 73h:22m:47s remains)
INFO - root - 2017-12-09 16:08:39.664734: step 39300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 68h:57m:39s remains)
2017-12-09 16:08:40.522198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00037288712 -0.00016613025 4.5471825e-07 0.00020303577 -5.3087249e-05 -0.00041238009 -0.00079170964 -0.0013227549 -0.0015991 -0.0010670044 -0.00063102692 0.00019641942 0.00083472626 0.0015451661 0.0022989865][-0.00068280171 -0.00057292986 -0.00041839154 -0.00029665814 -0.00041903043 -0.00055428175 -0.00087527046 -0.0013637105 -0.0017049048 -0.0012782179 -0.00067987596 0.00013091601 0.00085515622 0.0015384578 0.0024047215][-0.00090057286 -0.00084823719 -0.00068417005 -0.00059816451 -0.00049447292 -0.00062594377 -0.00079920166 -0.0012661836 -0.0017552698 -0.0014735318 -0.00089033321 -7.5157965e-05 0.00078432634 0.0016157562 0.0025835433][-0.00077797379 -0.0006803968 -0.00039307517 -0.00040576421 -0.00022385409 -0.00029553147 -0.00045749848 -0.0010120701 -0.0015628976 -0.0015869437 -0.0010297752 -1.0925578e-05 0.0008381228 0.0017593927 0.0028600693][-0.00045775622 -0.00016252045 0.00019039377 0.00013111578 0.00027101184 7.563876e-05 -0.00014738762 -0.00077734655 -0.0013935815 -0.0016754976 -0.001259214 -0.00026270445 0.00071282964 0.0018739817 0.0030408655][-0.00016818079 0.000272013 0.00066820486 0.00064992625 0.00065520778 0.00036388705 0.00011632056 -0.00056133186 -0.0012801671 -0.0018149848 -0.0016307324 -0.00058164122 0.0004220861 0.00177899 0.0030057649][7.6611992e-05 0.00067611923 0.00092556933 0.00096440315 0.00082188984 0.00037485128 9.2927134e-05 -0.00062951772 -0.0014228427 -0.0019371952 -0.0017615723 -0.00078654592 0.00028020376 0.0015104082 0.0027066236][0.00023669214 0.00092659518 0.00093494216 0.0010253391 0.00068748835 0.00021359348 -0.00025827461 -0.0010963012 -0.0017872446 -0.0020853656 -0.0018412272 -0.00075013144 0.00020496058 0.0014694263 0.002566142][0.00038465974 0.0010098522 0.00075035263 0.00083524967 0.00022597658 -0.00020944886 -0.000799821 -0.001454117 -0.0022414858 -0.0024795542 -0.0020980164 -0.0010583657 -1.4168676e-05 0.001279023 0.0023694951][0.0004663337 0.00084510352 0.00052476674 0.00042021391 -0.00035503833 -0.00073823985 -0.0013253863 -0.0018161351 -0.0024849954 -0.0026611406 -0.002452855 -0.0016441315 -0.00054439879 0.00071171578 0.0018303618][0.00029674452 0.00054256478 0.00023734407 -1.7508632e-05 -0.00079836627 -0.0011959237 -0.0016665494 -0.0019518661 -0.0025689378 -0.0027032134 -0.0026935516 -0.0021257265 -0.0012540875 -4.7905603e-05 0.00099648489][-8.9521986e-05 0.00018309965 -7.39058e-05 -0.00047923648 -0.0011008708 -0.0014695956 -0.0018246323 -0.0020690914 -0.0025767433 -0.0027926345 -0.0029663639 -0.0025309923 -0.0018749308 -0.00082226074 3.5140663e-05][-0.00062523037 -0.0003125607 -0.00050827838 -0.00087385532 -0.0013553244 -0.0016433574 -0.0019996297 -0.0022339709 -0.00264017 -0.0027835059 -0.0030628678 -0.0028521321 -0.0024250881 -0.0015689925 -0.00083887018][-0.0012007 -0.000896096 -0.000990327 -0.001203455 -0.0015737006 -0.0018936929 -0.0021672617 -0.0023820593 -0.0026857606 -0.0028382009 -0.00313942 -0.0030549837 -0.0028851049 -0.0022958973 -0.0016958052][-0.0015741535 -0.0013456475 -0.0013688675 -0.0015264796 -0.0018377721 -0.002076597 -0.0022752043 -0.0024704556 -0.0027122856 -0.0029332596 -0.0031375424 -0.0031943971 -0.0031666777 -0.00283342 -0.0024285368]]...]
INFO - root - 2017-12-09 16:08:49.063932: step 39310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:21m:09s remains)
INFO - root - 2017-12-09 16:08:57.802289: step 39320, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 73h:20m:38s remains)
INFO - root - 2017-12-09 16:09:06.336707: step 39330, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 59h:43m:11s remains)
INFO - root - 2017-12-09 16:09:14.938505: step 39340, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 71h:59m:17s remains)
INFO - root - 2017-12-09 16:09:23.212223: step 39350, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:11m:55s remains)
INFO - root - 2017-12-09 16:09:31.937478: step 39360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:23m:28s remains)
INFO - root - 2017-12-09 16:09:40.370819: step 39370, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 68h:55m:25s remains)
INFO - root - 2017-12-09 16:09:49.081684: step 39380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:19m:21s remains)
INFO - root - 2017-12-09 16:09:57.674986: step 39390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:07m:20s remains)
INFO - root - 2017-12-09 16:10:06.406895: step 39400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:49m:17s remains)
2017-12-09 16:10:07.380846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033918137 -0.0033912382 -0.0033887236 -0.0033613313 -0.0032535351 -0.0029978466 -0.0026040059 -0.0022401172 -0.0021520487 -0.0024422724 -0.0029198753 -0.0032733679 -0.0033825834 -0.0033876288 -0.0033875483][-0.0033924025 -0.0033913383 -0.0033765042 -0.0032872362 -0.0030075179 -0.0024380321 -0.0016606111 -0.0010373099 -0.00098900124 -0.0016097339 -0.0024991287 -0.0031379561 -0.0033639302 -0.0033865203 -0.0033863056][-0.0033929402 -0.0033839464 -0.0033157072 -0.0030353398 -0.0023098812 -0.0010201214 0.00054064905 0.0016106204 0.0014918251 0.00016801315 -0.0015428992 -0.0027771569 -0.0032892516 -0.0033837028 -0.0033864363][-0.0033911346 -0.0033509249 -0.0031256611 -0.0023773196 -0.00068509718 0.0020271856 0.0050144861 0.0068216869 0.0063418257 0.0037097917 0.000443995 -0.0019820374 -0.0030974925 -0.0033730411 -0.0033853168][-0.0033792374 -0.0032626921 -0.0027142726 -0.0011071349 0.0022003362 0.0071127261 0.012174468 0.015002476 0.013975035 0.0094206445 0.0037682834 -0.00060549704 -0.0027482589 -0.0033525222 -0.0033832998][-0.0033251892 -0.0030840575 -0.0020626911 0.000702234 0.0060516405 0.01360435 0.021056728 0.025026845 0.02333685 0.016516663 0.007970253 0.0011563953 -0.002291108 -0.0033249743 -0.003380008][-0.0032012707 -0.0028088987 -0.0013033557 0.0025460185 0.0097075868 0.019509036 0.028946636 0.033882983 0.031679075 0.02296767 0.011847625 0.0027759043 -0.0018832055 -0.0033033949 -0.0033772604][-0.0030315167 -0.0025169225 -0.00072534918 0.0036608512 0.011637442 0.022378139 0.032613076 0.037947584 0.035540584 0.026004856 0.01366804 0.0035035592 -0.0017201176 -0.0032979839 -0.0033753449][-0.0029021269 -0.0023514358 -0.00062290952 0.0034668539 0.010838849 0.0207397 0.030209256 0.035205647 0.033014689 0.024117222 0.012514785 0.0029601189 -0.0018951774 -0.0033146383 -0.0033763445][-0.0029066263 -0.002425336 -0.0010730557 0.002028977 0.0076403036 0.015262173 0.022676488 0.026682954 0.025024319 0.01800306 0.0088458247 0.0013890802 -0.0023165469 -0.0033414362 -0.0033803335][-0.0030506093 -0.0027127857 -0.0018726739 -7.3155388e-07 0.0034240596 0.00819491 0.012977239 0.015654609 0.014638707 0.010067197 0.0041484688 -0.00055456627 -0.002796445 -0.0033655351 -0.0033835676][-0.0032534935 -0.0030733545 -0.0026694834 -0.0017946252 -0.00017754035 0.0021583545 0.0045952643 0.0060197036 0.0055350023 0.0031763911 0.00017408235 -0.002125416 -0.0031512592 -0.0033811477 -0.0033876759][-0.0033716774 -0.0033226779 -0.0031965955 -0.0029093283 -0.0023637866 -0.0015374005 -0.0006375364 -9.1502909e-05 -0.00026204903 -0.0011398522 -0.0022216761 -0.0030112537 -0.0033300731 -0.0033881152 -0.003389417][-0.0033918277 -0.0033898023 -0.0033773209 -0.0033307595 -0.0032208809 -0.0030371915 -0.0028269021 -0.0026928596 -0.0027282948 -0.0029285511 -0.00316381 -0.0033268968 -0.003382693 -0.003389409 -0.0033895157][-0.0033900843 -0.0033901464 -0.0033918412 -0.0033914694 -0.0033881282 -0.0033801191 -0.0033682026 -0.0033569175 -0.0033547529 -0.0033646335 -0.0033786346 -0.0033879059 -0.0033890235 -0.0033889967 -0.0033890991]]...]
INFO - root - 2017-12-09 16:10:16.132927: step 39410, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 71h:12m:30s remains)
INFO - root - 2017-12-09 16:10:24.856282: step 39420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:49m:34s remains)
INFO - root - 2017-12-09 16:10:33.536224: step 39430, loss = 0.89, batch loss = 0.68 (11.1 examples/sec; 0.722 sec/batch; 58h:48m:12s remains)
INFO - root - 2017-12-09 16:10:41.978707: step 39440, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 68h:53m:34s remains)
INFO - root - 2017-12-09 16:10:50.461034: step 39450, loss = 0.91, batch loss = 0.70 (10.8 examples/sec; 0.738 sec/batch; 60h:04m:40s remains)
INFO - root - 2017-12-09 16:10:59.144756: step 39460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 73h:16m:32s remains)
INFO - root - 2017-12-09 16:11:07.700565: step 39470, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 67h:48m:34s remains)
INFO - root - 2017-12-09 16:11:16.392702: step 39480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:33m:14s remains)
INFO - root - 2017-12-09 16:11:25.089151: step 39490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:27m:47s remains)
INFO - root - 2017-12-09 16:11:33.571149: step 39500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:09m:32s remains)
2017-12-09 16:11:34.394583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003401716 -0.0034021721 -0.0034028338 -0.003403405 -0.003403096 -0.0034026455 -0.0034015654 -0.0033997679 -0.0033956885 -0.0033893506 -0.0033807156 -0.0033720469 -0.003365448 -0.003360298 -0.0033578998][-0.0034018394 -0.0034023803 -0.0034028308 -0.0034034906 -0.0034039682 -0.0034034147 -0.0034012115 -0.003398509 -0.0033934843 -0.0033861771 -0.003377123 -0.0033679078 -0.00336092 -0.003355802 -0.0033537226][-0.0034052934 -0.00340541 -0.0034052292 -0.0034054143 -0.0034051628 -0.0034037163 -0.0034004084 -0.0033965281 -0.0033903297 -0.003382148 -0.0033729323 -0.0033635888 -0.0033564249 -0.0033519776 -0.0033503009][-0.0034073931 -0.0034069135 -0.003406499 -0.0034063153 -0.0034054194 -0.0034027945 -0.0033981577 -0.0033930494 -0.0033857776 -0.0033771417 -0.0033679442 -0.0033590379 -0.0033523035 -0.0033480898 -0.0033471335][-0.0034084448 -0.0034077181 -0.0034068893 -0.0034062029 -0.0034041549 -0.0033997516 -0.0033935611 -0.0033867252 -0.0033786609 -0.003369991 -0.0033616903 -0.0033538975 -0.0033487214 -0.0033458355 -0.0033459177][-0.0034097889 -0.0034088965 -0.003407568 -0.0034060357 -0.0034020985 -0.0033955257 -0.0033876894 -0.0033800958 -0.0033714208 -0.0033626913 -0.0033548372 -0.0033481689 -0.0033448318 -0.0033434127 -0.0033451386][-0.0034098809 -0.0034093081 -0.0034080141 -0.0034054804 -0.003399526 -0.0033919851 -0.0033836567 -0.0033751498 -0.0033659793 -0.0033569327 -0.0033490378 -0.0033431808 -0.0033407458 -0.0033405253 -0.0033436606][-0.0034076429 -0.003407028 -0.0034061659 -0.0034036457 -0.0033973805 -0.0033903164 -0.0033829312 -0.0033739593 -0.003363587 -0.0033534714 -0.0033451773 -0.0033390417 -0.0033370033 -0.0033381223 -0.0033423724][-0.003403801 -0.0034032515 -0.00340313 -0.0034009216 -0.0033957302 -0.003390081 -0.0033836446 -0.0033750948 -0.0033644321 -0.003353673 -0.0033447808 -0.0033381174 -0.0033357528 -0.0033368743 -0.0033416508][-0.0034001779 -0.0033998645 -0.0034007039 -0.0033998047 -0.0033962009 -0.0033918582 -0.0033856351 -0.0033772041 -0.0033668606 -0.0033561909 -0.0033478066 -0.0033414538 -0.0033390345 -0.003339746 -0.0033443547][-0.0033978198 -0.0033975516 -0.003399235 -0.003399651 -0.003398072 -0.0033955334 -0.0033902829 -0.0033825918 -0.0033726317 -0.003362302 -0.0033549485 -0.0033487445 -0.0033453221 -0.0033451167 -0.003348816][-0.0033973246 -0.0033966629 -0.003398309 -0.0033996184 -0.0033999537 -0.0033992843 -0.003395638 -0.0033896591 -0.0033811359 -0.0033714478 -0.0033644268 -0.0033584218 -0.0033549885 -0.0033539012 -0.003356294][-0.0033970154 -0.0033960524 -0.003397357 -0.0033991528 -0.0034006471 -0.0034012233 -0.0033995507 -0.0033956578 -0.00338948 -0.0033817475 -0.0033752248 -0.003369546 -0.0033662138 -0.0033646312 -0.003365865][-0.0033958603 -0.0033946803 -0.0033955856 -0.0033971802 -0.0033990052 -0.0034005241 -0.0034002222 -0.0033985858 -0.0033949662 -0.0033902347 -0.0033855883 -0.0033807117 -0.0033777033 -0.0033756124 -0.0033757288][-0.0033943944 -0.0033926398 -0.0033930968 -0.0033943017 -0.0033958156 -0.0033977181 -0.0033989586 -0.0033992184 -0.0033983095 -0.0033960922 -0.003393675 -0.0033909874 -0.0033888193 -0.0033866339 -0.0033861296]]...]
INFO - root - 2017-12-09 16:11:42.908666: step 39510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:00m:04s remains)
INFO - root - 2017-12-09 16:11:51.380569: step 39520, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 68h:49m:46s remains)
INFO - root - 2017-12-09 16:12:00.013763: step 39530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:38m:25s remains)
INFO - root - 2017-12-09 16:12:08.651482: step 39540, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 67h:32m:32s remains)
INFO - root - 2017-12-09 16:12:17.284873: step 39550, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.847 sec/batch; 68h:53m:54s remains)
INFO - root - 2017-12-09 16:12:25.855278: step 39560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:45m:56s remains)
INFO - root - 2017-12-09 16:12:34.337743: step 39570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:32m:35s remains)
INFO - root - 2017-12-09 16:12:43.059965: step 39580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:22m:26s remains)
INFO - root - 2017-12-09 16:12:51.740851: step 39590, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 68h:55m:01s remains)
INFO - root - 2017-12-09 16:13:00.388679: step 39600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:31m:50s remains)
2017-12-09 16:13:01.271667: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54432708 0.55665761 0.56203425 0.56320709 0.56214368 0.55680037 0.54421926 0.5246501 0.49818853 0.46350482 0.42244524 0.37700272 0.33200556 0.28809568 0.24854195][0.54784441 0.55797726 0.56101292 0.56013185 0.55666232 0.54963887 0.53585368 0.5148766 0.48685682 0.44987929 0.40509802 0.35599354 0.30698636 0.25940773 0.21824419][0.53762811 0.54540563 0.54697293 0.54706806 0.54451078 0.53832185 0.5241701 0.50241625 0.47206244 0.43298903 0.38688672 0.33659956 0.28720459 0.24076782 0.20183173][0.53056246 0.53619653 0.53646231 0.53627384 0.53448755 0.5298987 0.51761824 0.49681625 0.4667201 0.42772606 0.38168165 0.33325613 0.28703216 0.24551755 0.2115178][0.52742624 0.5326674 0.53138512 0.53148907 0.53019887 0.52607095 0.51543385 0.49681482 0.46884713 0.43202949 0.38956764 0.34564668 0.304907 0.27015203 0.24216504][0.52604216 0.53363019 0.53412694 0.5330745 0.53096873 0.52809316 0.5191378 0.50267023 0.47750786 0.44606611 0.41076952 0.37488359 0.34249166 0.31563014 0.2938877][0.52411813 0.5358634 0.53869551 0.54049468 0.53974116 0.53718019 0.52960038 0.51700217 0.49697855 0.47244 0.44555897 0.41932687 0.39583874 0.37618756 0.35929778][0.52157849 0.53981745 0.54575545 0.54917812 0.54975712 0.54930562 0.54479337 0.53629357 0.52223188 0.50535637 0.48714381 0.4692668 0.45275465 0.43862015 0.42550537][0.52129149 0.5466885 0.55831927 0.56483781 0.567955 0.56914371 0.56673747 0.56154293 0.55209756 0.54186684 0.53091776 0.51967996 0.50831425 0.49795297 0.48696423][0.51605117 0.54956913 0.56682408 0.577736 0.58495635 0.58931816 0.58945173 0.5860979 0.58006245 0.57464212 0.56874913 0.56217766 0.55378872 0.54478741 0.53385454][0.50634426 0.54350793 0.56456506 0.58004582 0.59217888 0.60031664 0.60457057 0.60518932 0.60281229 0.600765 0.59794348 0.59417868 0.58686429 0.57759696 0.56615609][0.49687085 0.53619969 0.55773354 0.57475173 0.58980411 0.6020723 0.61069059 0.61491019 0.61661816 0.61800772 0.61687189 0.61285496 0.60439914 0.59435475 0.58240288][0.48747543 0.52746952 0.54942805 0.56612593 0.5814628 0.59456241 0.60558635 0.611731 0.61573589 0.6184597 0.61814326 0.61442745 0.60540175 0.59465331 0.58279139][0.47847345 0.51690191 0.53750628 0.55380869 0.56931078 0.5821116 0.59295648 0.59906161 0.60271138 0.60511917 0.6044175 0.60079521 0.59242707 0.58270657 0.57207149][0.46703476 0.50242877 0.51971406 0.53382391 0.54704666 0.55841565 0.56901658 0.57492936 0.57888764 0.58054715 0.57939577 0.57601124 0.56863248 0.56046718 0.55168486]]...]
INFO - root - 2017-12-09 16:13:10.007477: step 39610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:07m:27s remains)
INFO - root - 2017-12-09 16:13:18.563480: step 39620, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 67h:41m:39s remains)
INFO - root - 2017-12-09 16:13:27.263588: step 39630, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:36m:13s remains)
INFO - root - 2017-12-09 16:13:35.838318: step 39640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:40m:39s remains)
INFO - root - 2017-12-09 16:13:44.489096: step 39650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:40m:48s remains)
INFO - root - 2017-12-09 16:13:53.025322: step 39660, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 69h:05m:51s remains)
INFO - root - 2017-12-09 16:14:01.614004: step 39670, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.933 sec/batch; 75h:55m:06s remains)
INFO - root - 2017-12-09 16:14:10.176084: step 39680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:08m:28s remains)
INFO - root - 2017-12-09 16:14:18.584590: step 39690, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 70h:06m:48s remains)
INFO - root - 2017-12-09 16:14:27.207152: step 39700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:57m:34s remains)
2017-12-09 16:14:28.149619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033747035 -0.0033721144 -0.0033717116 -0.0033719484 -0.0033725263 -0.0033733447 -0.0033740937 -0.0033747335 -0.0033753421 -0.003375872 -0.0033762604 -0.0033763759 -0.003376375 -0.0033762171 -0.0033760099][-0.0033731849 -0.0033703102 -0.0033696233 -0.0033694759 -0.0033698196 -0.0033703598 -0.0033708669 -0.0033714853 -0.0033721267 -0.0033726997 -0.0033731631 -0.0033735186 -0.0033737831 -0.0033737761 -0.0033737065][-0.0033741375 -0.0033711584 -0.0033702124 -0.0033697244 -0.0033697514 -0.003370048 -0.0033703728 -0.003370848 -0.0033714133 -0.0033720555 -0.0033725782 -0.0033729619 -0.0033731987 -0.003373181 -0.0033730934][-0.0033756185 -0.0033726639 -0.0033718077 -0.0033712909 -0.0033711931 -0.0033714208 -0.00337156 -0.0033718012 -0.0033720376 -0.003372347 -0.0033725684 -0.0033727619 -0.0033727339 -0.0033725039 -0.0033722883][-0.0033770227 -0.0033743742 -0.0033736473 -0.0033735174 -0.0033739039 -0.0033745249 -0.0033749326 -0.0033751582 -0.0033750269 -0.0033745712 -0.0033740033 -0.0033734886 -0.0033727128 -0.0033720012 -0.0033716247][-0.0033789063 -0.0033764881 -0.0033760676 -0.0033764844 -0.0033778131 -0.0033794257 -0.0033807326 -0.0033813505 -0.0033809508 -0.0033796474 -0.0033780513 -0.0033763747 -0.0033744196 -0.0033729747 -0.0033720373][-0.0033811508 -0.0033790465 -0.0033790632 -0.0033802646 -0.0033825031 -0.0033849715 -0.0033869783 -0.0033877958 -0.0033872519 -0.0033854961 -0.0033831664 -0.0033806372 -0.0033778546 -0.0033756064 -0.0033738471][-0.0033840635 -0.0033823494 -0.0033829354 -0.0033849142 -0.0033876032 -0.0033903909 -0.003392519 -0.0033933755 -0.0033927897 -0.003391003 -0.0033886426 -0.0033856933 -0.0033824553 -0.0033796113 -0.0033772355][-0.0033884966 -0.003387233 -0.0033880207 -0.0033899904 -0.0033924086 -0.0033948577 -0.003396668 -0.0033975269 -0.0033974394 -0.0033964659 -0.0033946096 -0.0033918184 -0.0033885967 -0.0033855333 -0.0033827124][-0.0033939241 -0.0033927632 -0.0033932806 -0.0033945928 -0.0033961914 -0.0033978708 -0.003399092 -0.0033998415 -0.003400438 -0.003400634 -0.0033999535 -0.003397997 -0.0033953977 -0.0033925613 -0.0033896887][-0.0033984603 -0.003397098 -0.0033972233 -0.0033978815 -0.003398706 -0.0033993863 -0.003399862 -0.0034002268 -0.0034011095 -0.0034023032 -0.0034031356 -0.0034028885 -0.0034018017 -0.0033998466 -0.0033972303][-0.0034022175 -0.0034007949 -0.0034006955 -0.0034008257 -0.0034007949 -0.0034004522 -0.00339991 -0.0033996794 -0.0034002892 -0.0034020629 -0.0034044147 -0.0034061372 -0.003407028 -0.0034064478 -0.003404612][-0.0034060548 -0.0034045435 -0.0034040695 -0.0034034764 -0.0034025635 -0.0034015784 -0.0034005672 -0.0034000389 -0.0034005521 -0.0034023393 -0.0034051063 -0.0034077396 -0.0034096646 -0.003410185 -0.0034093761][-0.0034085785 -0.0034069757 -0.0034059787 -0.0034049407 -0.0034037267 -0.0034025796 -0.003401546 -0.0034011898 -0.0034017847 -0.0034032667 -0.0034052928 -0.0034074134 -0.0034092851 -0.0034101852 -0.0034100523][-0.0034074273 -0.0034059489 -0.0034049477 -0.0034042166 -0.0034034816 -0.00340283 -0.0034023058 -0.0034021933 -0.0034025477 -0.0034032448 -0.0034040357 -0.0034049186 -0.0034059021 -0.0034065302 -0.0034064644]]...]
INFO - root - 2017-12-09 16:14:36.742222: step 39710, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 68h:49m:14s remains)
INFO - root - 2017-12-09 16:14:45.560973: step 39720, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 74h:41m:44s remains)
INFO - root - 2017-12-09 16:14:54.180281: step 39730, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:13m:10s remains)
INFO - root - 2017-12-09 16:15:02.860680: step 39740, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 72h:51m:36s remains)
INFO - root - 2017-12-09 16:15:11.598997: step 39750, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 70h:28m:26s remains)
INFO - root - 2017-12-09 16:15:20.051205: step 39760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:44m:34s remains)
INFO - root - 2017-12-09 16:15:28.616872: step 39770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:25m:01s remains)
INFO - root - 2017-12-09 16:15:37.195905: step 39780, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 67h:42m:09s remains)
INFO - root - 2017-12-09 16:15:45.807919: step 39790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:07m:18s remains)
INFO - root - 2017-12-09 16:15:54.491998: step 39800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:02m:44s remains)
2017-12-09 16:15:55.358486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00089767738 -0.0016520311 -0.0020092395 -0.0019020862 -0.0012955873 -0.00024791784 0.0010615245 0.0024360011 0.0032217239 0.0027878161 0.0017240795 0.00023318385 -0.000900113 -0.0018737946 -0.0024881214][0.0029101844 0.0015741137 0.00070410618 0.00066267909 0.0015446057 0.0032737118 0.0053331256 0.0072042029 0.0078769717 0.006355104 0.003790227 0.0010963075 -0.00050451886 -0.0015031965 -0.0020764463][0.0067669349 0.0058302991 0.0051753893 0.0056457836 0.0074116467 0.010617713 0.014293496 0.017172202 0.017564263 0.014136325 0.0087025315 0.0034166898 0.00032715593 -0.0011156308 -0.001651364][0.010487854 0.0095809121 0.00952268 0.011425941 0.015470329 0.021750152 0.028339112 0.033037394 0.03315784 0.02714506 0.017674118 0.0083532436 0.0024162184 -0.00033783633 -0.0012611619][0.015520188 0.015174822 0.015539874 0.018900022 0.02566353 0.035612125 0.045841854 0.052779935 0.05261359 0.043913409 0.030110316 0.016169103 0.0063592875 0.0011264666 -0.00090648187][0.01797308 0.018744336 0.021188891 0.027792173 0.038434964 0.052351967 0.065772019 0.074473128 0.0739361 0.062833704 0.045052528 0.02638551 0.012165138 0.0036956694 -0.00016127503][0.019011201 0.02104599 0.02620955 0.036399946 0.051129516 0.068954885 0.085344724 0.095398024 0.094448641 0.081600331 0.060800727 0.038114887 0.019644575 0.0074867262 0.001253682][0.021218691 0.024889128 0.032766536 0.045855287 0.063512318 0.083527863 0.10126671 0.11197133 0.1111132 0.097659424 0.075307861 0.050096974 0.028395394 0.012802166 0.0038594331][0.027795436 0.032685336 0.042205457 0.056849033 0.07570447 0.096308015 0.11383825 0.12403762 0.12296776 0.1098221 0.087589934 0.061570611 0.037804767 0.019336892 0.0076858][0.038110208 0.04438762 0.054706842 0.069037266 0.087102689 0.10655521 0.12254653 0.13147306 0.13004278 0.11777508 0.096724965 0.071265489 0.046990231 0.027133198 0.013598313][0.053617503 0.060300719 0.07021147 0.083527587 0.099432267 0.11617293 0.12919718 0.13558498 0.13283047 0.12100039 0.1014997 0.077683583 0.054495752 0.034921784 0.0207361][0.073832124 0.081017807 0.089151382 0.099391825 0.11135515 0.12393185 0.13288432 0.13571128 0.13100348 0.11942239 0.10171622 0.080422081 0.059707563 0.041958287 0.028551372][0.099794641 0.10676551 0.11209518 0.11780453 0.12397643 0.1307296 0.13429715 0.13295074 0.12587202 0.11408185 0.098195918 0.079950579 0.062480446 0.047456522 0.035756297][0.12599196 0.13233112 0.13452528 0.13570903 0.13632108 0.13736545 0.13550237 0.12989858 0.12011956 0.10770836 0.093107946 0.077493943 0.063127421 0.051036838 0.041593712][0.15015922 0.1546983 0.15306969 0.14991549 0.14553989 0.14159748 0.13517241 0.12643498 0.11515464 0.10242293 0.08902014 0.075896434 0.064434491 0.055213992 0.047921769]]...]
INFO - root - 2017-12-09 16:16:04.170058: step 39810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:31m:44s remains)
INFO - root - 2017-12-09 16:16:12.987648: step 39820, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 69h:47m:29s remains)
INFO - root - 2017-12-09 16:16:21.676170: step 39830, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 68h:52m:34s remains)
INFO - root - 2017-12-09 16:16:30.257904: step 39840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:06m:16s remains)
INFO - root - 2017-12-09 16:16:38.777241: step 39850, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 69h:58m:06s remains)
INFO - root - 2017-12-09 16:16:47.239612: step 39860, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:21m:48s remains)
INFO - root - 2017-12-09 16:16:55.847811: step 39870, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 57h:40m:03s remains)
INFO - root - 2017-12-09 16:17:04.515788: step 39880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:10m:40s remains)
INFO - root - 2017-12-09 16:17:13.220463: step 39890, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:24m:30s remains)
INFO - root - 2017-12-09 16:17:21.862795: step 39900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:59m:35s remains)
2017-12-09 16:17:22.733418: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0025305548 0.010229243 0.02181375 0.035997018 0.04993052 0.0592566 0.060606487 0.05401494 0.042386241 0.028385531 0.015010946 0.0051757656 -0.00017834478 -0.0025294649 -0.0032679837][0.0068174396 0.016645294 0.033012744 0.054391075 0.076686084 0.093564756 0.099092782 0.091512434 0.074090146 0.051287707 0.02875039 0.011655773 0.0018973085 -0.0021400806 -0.0032379888][0.01756651 0.031300902 0.053902928 0.08344695 0.11453105 0.13903198 0.14913909 0.14108737 0.1177261 0.084831238 0.050765641 0.023418542 0.0066058859 -0.00079727848 -0.0030543243][0.035277292 0.056008868 0.087038495 0.12543878 0.16459945 0.1951424 0.20831092 0.19887485 0.1688306 0.12496005 0.078327961 0.039504565 0.013976702 0.0016902587 -0.0026223138][0.056956775 0.087073617 0.12816919 0.17613527 0.22301741 0.25844562 0.27367702 0.26254395 0.22571564 0.17024623 0.10994107 0.058402997 0.023112796 0.0050475551 -0.0019569658][0.077056073 0.11690654 0.167524 0.22352427 0.2759639 0.31440687 0.33083802 0.31871614 0.27694586 0.21251923 0.14057581 0.077443 0.032768231 0.0087267617 -0.0011450034][0.092187069 0.13956667 0.19711545 0.25819948 0.31326562 0.35260165 0.36933371 0.35692853 0.31304973 0.24359246 0.16407482 0.092713326 0.04094797 0.012010591 -0.00036846078][0.09919291 0.15035388 0.21054852 0.27262014 0.32739037 0.36619624 0.38298023 0.37159693 0.32861158 0.25852147 0.17655906 0.10146022 0.046054874 0.014166032 0.00018501375][0.098017365 0.14733776 0.20469519 0.26348627 0.31550017 0.3531419 0.3705979 0.36157936 0.32189709 0.25501 0.1755593 0.10169334 0.046663277 0.014469788 0.00028923131][0.090275295 0.13263763 0.18167202 0.23274571 0.2793377 0.31491262 0.33329353 0.32781574 0.29380339 0.23375009 0.16110548 0.092915975 0.042284332 0.012714867 -7.1494142e-05][0.076853968 0.10933916 0.14708643 0.18758291 0.22619629 0.25767931 0.27576703 0.27371791 0.24686855 0.19692813 0.13545367 0.077271663 0.034301538 0.0095499735 -0.00077633956][0.060519822 0.082426772 0.1082399 0.13733816 0.16680121 0.19242349 0.20844875 0.20887172 0.18941346 0.15103537 0.10331416 0.057938442 0.024715729 0.0059218109 -0.0016062804][0.043345086 0.05652985 0.072102793 0.0908475 0.11114185 0.12980008 0.14222312 0.14368482 0.13085505 0.10402828 0.070387252 0.038339794 0.015221328 0.0024873954 -0.0023471043][0.027298559 0.034149081 0.042279251 0.053083207 0.065626286 0.077837855 0.086480394 0.088027224 0.080176443 0.063041858 0.04159585 0.02134829 0.0071602883 -0.00030069076 -0.0029050955][0.013633956 0.016576257 0.020116754 0.025447592 0.032111969 0.039081682 0.044335406 0.045639597 0.041555043 0.031985506 0.019930497 0.0088049266 0.001463403 -0.0020974143 -0.0031971792]]...]
INFO - root - 2017-12-09 16:17:31.327889: step 39910, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:34m:53s remains)
INFO - root - 2017-12-09 16:17:39.841574: step 39920, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:28m:58s remains)
INFO - root - 2017-12-09 16:17:48.549102: step 39930, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.914 sec/batch; 74h:15m:18s remains)
INFO - root - 2017-12-09 16:17:57.186861: step 39940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 71h:51m:28s remains)
INFO - root - 2017-12-09 16:18:05.840654: step 39950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 67h:38m:26s remains)
INFO - root - 2017-12-09 16:18:14.524401: step 39960, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 72h:50m:10s remains)
INFO - root - 2017-12-09 16:18:23.203362: step 39970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:48m:23s remains)
INFO - root - 2017-12-09 16:18:31.574370: step 39980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:15m:55s remains)
INFO - root - 2017-12-09 16:18:40.104118: step 39990, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 70h:49m:58s remains)
INFO - root - 2017-12-09 16:18:48.691192: step 40000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:11m:22s remains)
2017-12-09 16:18:49.552560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030715056 -0.0028676037 -0.0025058063 -0.0017823633 -0.00019736402 0.0023275092 0.0052031646 0.0074380995 0.0083041769 0.0074495054 0.0048863064 0.0017991224 -0.00065383269 -0.0018183463 -0.0018716685][-0.0030550191 -0.0028980176 -0.0025621466 -0.0019026992 -0.00023958599 0.0023824028 0.0053794519 0.0078165028 0.0089712385 0.0083386507 0.0060039549 0.0028184243 0.00020909635 -0.0010780822 -0.0010021261][-0.003115634 -0.0030054022 -0.0026640291 -0.0020121313 -0.00041867048 0.0020540273 0.0049289516 0.0074114879 0.0088416282 0.0086615719 0.0067499219 0.0039233919 0.0014584353 0.00021075085 0.0004246512][-0.00308525 -0.0030628808 -0.0028070034 -0.00225456 -0.00077942247 0.0015338706 0.0042806077 0.0068134335 0.0084926486 0.008808475 0.0075297826 0.0051528974 0.0030362019 0.0019943295 0.0023132083][-0.002991467 -0.0029721111 -0.0027975324 -0.0023943277 -0.0010654503 0.001091186 0.0037333965 0.00626115 0.0081232265 0.0089075863 0.0082054213 0.0063792975 0.0046413816 0.0037893853 0.0040885303][-0.002935888 -0.0028751565 -0.0027063477 -0.0023743366 -0.0011946657 0.00079101603 0.0033196053 0.0057568508 0.007631701 0.0087254234 0.008583663 0.007460251 0.0062757866 0.0057600825 0.0060863351][-0.003025725 -0.0028904777 -0.0026983363 -0.0023853225 -0.0013335694 0.00048259739 0.0028221067 0.0051024077 0.0068551148 0.0079842266 0.0082309693 0.007697836 0.00708013 0.0069554984 0.0074829874][-0.0030878407 -0.002960152 -0.0027923756 -0.0025036396 -0.0016149346 -1.3939571e-06 0.0021250146 0.0042010257 0.0057650176 0.0068804217 0.0073522232 0.0073495512 0.0072288667 0.0074135954 0.0079356711][-0.0030827636 -0.002955006 -0.0028373029 -0.0026425992 -0.0019704022 -0.00061783707 0.0012436858 0.00306996 0.0044013481 0.0053882529 0.0060255541 0.0062849321 0.0065104314 0.0068693724 0.0073619527][-0.0031807406 -0.0030380657 -0.0029201512 -0.0027423403 -0.002213513 -0.0011583969 0.00034790114 0.0018456471 0.0029064752 0.0036677932 0.0042038732 0.004643762 0.0050735115 0.0055414685 0.0059949928][-0.0032688391 -0.0031707659 -0.0030534707 -0.0028876704 -0.0024753392 -0.0016346048 -0.00048718764 0.00062166434 0.0013590981 0.0018457482 0.0022203997 0.0025977176 0.0029929895 0.0034809471 0.0039518951][-0.0033604947 -0.0032948812 -0.0032107567 -0.0030652194 -0.0027500829 -0.0021495109 -0.0013444067 -0.000598036 -0.00014407933 9.3134819e-05 0.000288632 0.00053685438 0.00082076853 0.0011665048 0.0015503927][-0.0034077391 -0.0033872519 -0.0033362433 -0.003248597 -0.0030716248 -0.0027167127 -0.0022353237 -0.0017787163 -0.001520006 -0.0014332552 -0.0013623044 -0.001226926 -0.0010302088 -0.00082348683 -0.00058177579][-0.0034140749 -0.0034136698 -0.0034078534 -0.0033760415 -0.003301092 -0.0031593379 -0.0029585434 -0.0027690607 -0.0026660387 -0.002634285 -0.0026095232 -0.0025555755 -0.0024410025 -0.0023189844 -0.0021723951][-0.00341371 -0.0034135238 -0.0034137715 -0.0034120518 -0.0034015269 -0.0033677858 -0.0033152527 -0.0032717325 -0.0032569012 -0.0032674172 -0.0032772208 -0.0032601515 -0.0032148487 -0.0031560722 -0.0030719796]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 16:18:58.881624: step 40010, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 68h:39m:37s remains)
INFO - root - 2017-12-09 16:19:07.608656: step 40020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:00m:30s remains)
INFO - root - 2017-12-09 16:19:16.146068: step 40030, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.739 sec/batch; 60h:00m:32s remains)
INFO - root - 2017-12-09 16:19:24.725157: step 40040, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 70h:29m:42s remains)
INFO - root - 2017-12-09 16:19:33.124600: step 40050, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 66h:56m:16s remains)
INFO - root - 2017-12-09 16:19:41.514840: step 40060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:33m:15s remains)
INFO - root - 2017-12-09 16:19:50.007134: step 40070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:38m:28s remains)
INFO - root - 2017-12-09 16:19:58.501750: step 40080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:23m:30s remains)
INFO - root - 2017-12-09 16:20:07.171754: step 40090, loss = 0.90, batch loss = 0.69 (8.4 examples/sec; 0.949 sec/batch; 77h:05m:11s remains)
INFO - root - 2017-12-09 16:20:15.921121: step 40100, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 68h:59m:21s remains)
2017-12-09 16:20:16.810922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034080611 -0.0034061379 -0.0034056064 -0.0034060411 -0.0034067789 -0.0034073959 -0.0034083538 -0.0034090041 -0.003409534 -0.003410168 -0.0034107978 -0.0034111913 -0.0034109091 -0.0034106385 -0.0034108716][-0.0034086094 -0.0034072776 -0.003406697 -0.0034065943 -0.003406591 -0.0034065512 -0.0034068595 -0.0034067926 -0.0034068916 -0.0034073442 -0.0034076853 -0.0034079603 -0.003408063 -0.0034079445 -0.0034078779][-0.0034096688 -0.0034087463 -0.0034078371 -0.0034069514 -0.0034065647 -0.0034059773 -0.0034053819 -0.0034044362 -0.0034038615 -0.0034038287 -0.0034036096 -0.0034036725 -0.0034037083 -0.0034037824 -0.0034036026][-0.0034064634 -0.0034054352 -0.0034040019 -0.0034027549 -0.0034031591 -0.0034029232 -0.0034023395 -0.0034009768 -0.0033997609 -0.0033991346 -0.0033989069 -0.003398973 -0.0033989807 -0.0033987975 -0.0033983134][-0.0034028292 -0.0034014406 -0.0033998517 -0.0033993002 -0.0033997239 -0.0033999209 -0.0033994701 -0.0033979083 -0.0033966128 -0.0033958603 -0.0033955888 -0.0033955385 -0.0033955765 -0.0033954298 -0.0033946645][-0.003400295 -0.0033990082 -0.0033985539 -0.0033991239 -0.0033998117 -0.0034000063 -0.0033989325 -0.0033969516 -0.0033953045 -0.0033943513 -0.0033938065 -0.00339346 -0.0033931127 -0.0033924463 -0.003392125][-0.003400754 -0.0034007747 -0.0034015961 -0.0034029977 -0.0034037489 -0.0034031377 -0.0034012534 -0.0033984806 -0.0033963523 -0.0033952412 -0.0033939781 -0.0033923169 -0.0033897129 -0.0033878544 -0.0033887965][-0.0034030667 -0.0034041998 -0.0034058683 -0.0034078241 -0.0034085964 -0.0034073277 -0.0034049004 -0.0034015754 -0.0033988671 -0.0033971297 -0.0033949125 -0.0033911448 -0.0033865396 -0.003385155 -0.0033872684][-0.0034040883 -0.0034058108 -0.0034080641 -0.0034105757 -0.0034112842 -0.0034099123 -0.0034076336 -0.0034048236 -0.0034025 -0.003400024 -0.0033969858 -0.003392352 -0.0033880696 -0.0033867629 -0.0033890808][-0.00340431 -0.0034064739 -0.0034090073 -0.0034113247 -0.00341175 -0.0034105217 -0.0034085731 -0.0034065102 -0.0034050404 -0.003402669 -0.0034001598 -0.0033968533 -0.0033940941 -0.0033937406 -0.003395678][-0.003403252 -0.0034057349 -0.0034084164 -0.0034105647 -0.0034110202 -0.0034100548 -0.0034081726 -0.0034063582 -0.0034054772 -0.0034044667 -0.0034036052 -0.0034023793 -0.003401564 -0.0034018704 -0.0034032695][-0.0034015584 -0.0034035915 -0.0034059999 -0.0034077426 -0.003408297 -0.0034075011 -0.0034060732 -0.0034049843 -0.0034048078 -0.0034050017 -0.003405408 -0.0034055407 -0.0034059973 -0.003406689 -0.0034073642][-0.0033992385 -0.0034003251 -0.0034023079 -0.0034041498 -0.0034048248 -0.0034042762 -0.0034032415 -0.0034028711 -0.00340322 -0.0034040269 -0.0034053964 -0.0034064492 -0.0034074343 -0.0034085202 -0.0034089186][-0.0033975008 -0.00339772 -0.0033991178 -0.0034003353 -0.0034010285 -0.0034010848 -0.0034003982 -0.0033999125 -0.0034001898 -0.0034015689 -0.0034039365 -0.0034060266 -0.0034072914 -0.0034083743 -0.0034091179][-0.0033961793 -0.0033957895 -0.0033964387 -0.0033970578 -0.0033975702 -0.0033979416 -0.0033975085 -0.0033969942 -0.0033971644 -0.0033989991 -0.0034017691 -0.003404472 -0.0034067032 -0.0034083193 -0.0034095584]]...]
INFO - root - 2017-12-09 16:20:25.338597: step 40110, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:54m:29s remains)
INFO - root - 2017-12-09 16:20:34.129714: step 40120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:00m:51s remains)
INFO - root - 2017-12-09 16:20:42.909836: step 40130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:31m:41s remains)
INFO - root - 2017-12-09 16:20:51.368528: step 40140, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 68h:34m:39s remains)
INFO - root - 2017-12-09 16:21:00.015967: step 40150, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:55m:53s remains)
INFO - root - 2017-12-09 16:21:08.507835: step 40160, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 72h:09m:49s remains)
INFO - root - 2017-12-09 16:21:17.152022: step 40170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:28m:54s remains)
INFO - root - 2017-12-09 16:21:25.768168: step 40180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:13m:52s remains)
INFO - root - 2017-12-09 16:21:34.200706: step 40190, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 66h:32m:46s remains)
INFO - root - 2017-12-09 16:21:42.663055: step 40200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:50m:42s remains)
2017-12-09 16:21:43.545904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033590843 -0.0033559769 -0.0033555522 -0.0033555431 -0.0033556225 -0.0033561029 -0.0033567189 -0.0033572118 -0.0033574565 -0.0033573727 -0.003357284 -0.0033567809 -0.003356209 -0.003355836 -0.0033555711][-0.0033571315 -0.0033538088 -0.0033533368 -0.0033532109 -0.0033531867 -0.0033535603 -0.0033541648 -0.003354646 -0.0033549422 -0.0033551531 -0.0033553236 -0.0033549736 -0.0033544037 -0.0033539031 -0.003353596][-0.0033572891 -0.0033538826 -0.0033533527 -0.0033530635 -0.003352908 -0.0033530896 -0.0033535378 -0.0033539247 -0.0033543995 -0.0033548973 -0.0033552633 -0.0033551152 -0.0033546246 -0.0033540509 -0.0033536339][-0.0033574973 -0.0033541203 -0.0033535433 -0.0033531555 -0.0033528579 -0.0033528479 -0.0033530905 -0.0033533038 -0.0033539063 -0.003354647 -0.0033551436 -0.0033550733 -0.0033547042 -0.0033543038 -0.0033539131][-0.0033578298 -0.0033544463 -0.0033539878 -0.00335367 -0.0033532998 -0.003352924 -0.00335281 -0.0033529159 -0.0033535035 -0.0033542814 -0.0033547988 -0.0033548314 -0.0033546113 -0.0033544481 -0.0033542737][-0.0033579895 -0.0033547261 -0.0033545534 -0.0033545746 -0.0033543725 -0.0033537492 -0.0033532276 -0.00335305 -0.0033533887 -0.0033539219 -0.0033541436 -0.0033540926 -0.0033539475 -0.0033540372 -0.003354094][-0.0033579953 -0.0033548484 -0.0033550325 -0.0033553757 -0.0033554856 -0.0033547159 -0.003353718 -0.003353107 -0.0033531114 -0.0033533436 -0.0033531794 -0.0033529683 -0.0033529352 -0.0033532542 -0.0033534702][-0.0033579054 -0.0033548728 -0.0033554756 -0.0033563352 -0.0033569562 -0.0033563511 -0.003355128 -0.0033540761 -0.0033534847 -0.0033531939 -0.0033526283 -0.0033522747 -0.0033522891 -0.0033526917 -0.0033530688][-0.00335775 -0.0033547736 -0.0033557403 -0.0033570982 -0.0033582589 -0.0033581329 -0.0033570714 -0.0033557962 -0.0033545666 -0.0033536567 -0.0033527294 -0.0033522032 -0.0033520991 -0.0033524563 -0.0033528246][-0.0033574824 -0.0033545787 -0.0033557608 -0.0033573557 -0.0033588104 -0.0033589688 -0.0033581343 -0.0033568533 -0.0033552614 -0.0033539042 -0.0033527038 -0.003352124 -0.003352032 -0.0033523634 -0.0033526197][-0.0033571746 -0.00335437 -0.0033556363 -0.0033572565 -0.0033587045 -0.003358871 -0.0033582125 -0.0033570873 -0.0033555129 -0.0033542002 -0.0033530814 -0.0033528323 -0.0033528768 -0.0033531813 -0.0033531953][-0.0033571238 -0.0033542949 -0.0033555031 -0.0033569147 -0.0033581222 -0.0033581613 -0.0033574784 -0.0033563937 -0.0033551126 -0.0033541175 -0.0033533452 -0.0033536265 -0.0033539787 -0.0033544293 -0.0033544961][-0.0033572218 -0.0033542479 -0.0033554423 -0.0033568167 -0.0033578242 -0.0033578353 -0.0033572647 -0.0033562824 -0.0033554982 -0.0033551631 -0.0033552309 -0.003356196 -0.0033569378 -0.0033576784 -0.0033576987][-0.0033573222 -0.0033541885 -0.0033552691 -0.0033565385 -0.0033574747 -0.00335764 -0.0033573792 -0.0033567345 -0.0033566784 -0.0033573972 -0.0033588067 -0.0033610312 -0.0033628291 -0.0033643011 -0.0033643798][-0.0033573825 -0.003354081 -0.0033549462 -0.0033561236 -0.0033571289 -0.00335771 -0.0033579355 -0.0033580114 -0.0033589364 -0.0033609129 -0.0033637127 -0.003367296 -0.00337023 -0.0033723416 -0.00337222]]...]
INFO - root - 2017-12-09 16:21:52.115762: step 40210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:38m:44s remains)
INFO - root - 2017-12-09 16:22:00.807084: step 40220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:45m:21s remains)
INFO - root - 2017-12-09 16:22:09.385228: step 40230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:07m:24s remains)
INFO - root - 2017-12-09 16:22:18.065187: step 40240, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 71h:43m:00s remains)
INFO - root - 2017-12-09 16:22:26.867934: step 40250, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 72h:55m:01s remains)
INFO - root - 2017-12-09 16:22:35.270873: step 40260, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.758 sec/batch; 61h:31m:12s remains)
INFO - root - 2017-12-09 16:22:43.882217: step 40270, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 71h:37m:18s remains)
INFO - root - 2017-12-09 16:22:52.364517: step 40280, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:11m:01s remains)
INFO - root - 2017-12-09 16:23:01.304322: step 40290, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:30m:42s remains)
INFO - root - 2017-12-09 16:23:10.106556: step 40300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:42m:25s remains)
2017-12-09 16:23:10.968146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003407371 -0.0034056569 -0.00340445 -0.0034043267 -0.003405056 -0.0034064543 -0.0034075414 -0.0034072082 -0.0034050939 -0.0034010652 -0.0033962417 -0.003391447 -0.00338796 -0.0033872081 -0.0033893152][-0.003403953 -0.0034031044 -0.0034030369 -0.0034038539 -0.003405357 -0.003407232 -0.0034080343 -0.0034068308 -0.0034034364 -0.0033981749 -0.0033923606 -0.0033869131 -0.003383134 -0.0033822143 -0.0033844481][-0.003400336 -0.003400208 -0.0034013479 -0.0034032082 -0.0034056457 -0.0034082809 -0.0034090956 -0.0034072436 -0.0034031554 -0.00339711 -0.0033905345 -0.0033846523 -0.0033805142 -0.00337916 -0.0033806954][-0.0033953241 -0.0033959982 -0.0033985188 -0.0034015297 -0.0034048441 -0.0034080977 -0.0034091398 -0.003407354 -0.003403154 -0.0033968871 -0.0033902233 -0.0033841336 -0.0033796842 -0.0033775712 -0.0033781743][-0.0033895792 -0.0033908628 -0.0033945215 -0.0033985665 -0.0034027861 -0.0034064457 -0.0034077915 -0.0034063596 -0.003402415 -0.0033963744 -0.0033898228 -0.0033838667 -0.0033793151 -0.003376602 -0.0033762136][-0.0033835983 -0.0033849087 -0.0033888766 -0.0033933374 -0.0033979309 -0.003402018 -0.0034039412 -0.0034033633 -0.0034002219 -0.0033948731 -0.0033889846 -0.0033833773 -0.0033787887 -0.0033755987 -0.0033742806][-0.0033790751 -0.0033796683 -0.0033833426 -0.0033876016 -0.0033919879 -0.00339601 -0.0033984382 -0.0033989921 -0.003396865 -0.0033924489 -0.0033873592 -0.0033822111 -0.0033778211 -0.0033743894 -0.0033724098][-0.0033759687 -0.0033758385 -0.0033788371 -0.0033824223 -0.003385751 -0.0033889625 -0.0033916552 -0.00339316 -0.0033921343 -0.00338895 -0.0033850255 -0.0033807405 -0.0033767389 -0.0033733414 -0.0033710089][-0.0033731346 -0.003372472 -0.003374713 -0.0033774772 -0.0033801687 -0.0033829918 -0.0033854356 -0.0033873967 -0.0033876714 -0.0033861247 -0.0033836286 -0.003380253 -0.003376656 -0.003373286 -0.0033707614][-0.0033712245 -0.0033700836 -0.003371686 -0.0033737319 -0.0033758581 -0.0033783063 -0.0033807287 -0.003383297 -0.0033848824 -0.003385029 -0.003384083 -0.0033818139 -0.0033786921 -0.0033753274 -0.003372371][-0.0033697074 -0.0033682862 -0.0033695335 -0.0033712022 -0.0033729342 -0.0033753202 -0.0033781587 -0.0033813529 -0.0033840043 -0.0033856002 -0.0033860949 -0.0033848176 -0.0033821282 -0.00337878 -0.0033754604][-0.0033691842 -0.0033675055 -0.0033684177 -0.0033696939 -0.0033712042 -0.0033736294 -0.0033767279 -0.0033803643 -0.0033837541 -0.0033864337 -0.0033878295 -0.0033873136 -0.0033851685 -0.0033822001 -0.0033791014][-0.0033688725 -0.0033668627 -0.0033673234 -0.0033683293 -0.0033697581 -0.0033722888 -0.0033755368 -0.0033794523 -0.0033834493 -0.0033868267 -0.0033889366 -0.0033891476 -0.0033877525 -0.0033853797 -0.0033827925][-0.0033683756 -0.0033662361 -0.0033663067 -0.0033669767 -0.0033683972 -0.0033708047 -0.003374049 -0.0033779906 -0.0033821275 -0.0033856744 -0.0033881287 -0.0033890877 -0.0033885662 -0.0033872174 -0.0033856125][-0.0033682736 -0.00336589 -0.003365603 -0.0033658913 -0.0033670869 -0.0033691651 -0.0033720483 -0.0033756075 -0.0033793594 -0.0033826772 -0.0033854446 -0.0033870903 -0.0033876656 -0.00338765 -0.003387183]]...]
INFO - root - 2017-12-09 16:23:19.795757: step 40310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:59m:09s remains)
INFO - root - 2017-12-09 16:23:28.420625: step 40320, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 69h:30m:51s remains)
INFO - root - 2017-12-09 16:23:37.189889: step 40330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 70h:12m:33s remains)
INFO - root - 2017-12-09 16:23:45.749551: step 40340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:26m:09s remains)
INFO - root - 2017-12-09 16:23:54.498477: step 40350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:33m:49s remains)
INFO - root - 2017-12-09 16:24:03.070083: step 40360, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 62h:42m:53s remains)
INFO - root - 2017-12-09 16:24:11.802124: step 40370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 69h:47m:32s remains)
INFO - root - 2017-12-09 16:24:20.297663: step 40380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:56m:35s remains)
INFO - root - 2017-12-09 16:24:28.770935: step 40390, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 65h:45m:44s remains)
INFO - root - 2017-12-09 16:24:37.319132: step 40400, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 69h:34m:42s remains)
2017-12-09 16:24:38.209834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030325928 -0.0024997867 -0.0018515819 -0.0013225705 -0.0010053392 -0.0010045846 -0.0012958138 -0.0017712207 -0.0022957195 -0.0027565793 -0.0030815124 -0.0032680482 -0.0033544817 -0.0033852698 -0.003392722][-0.0024151239 -0.0013177826 -0.00016666413 0.00056027784 0.00073975162 0.00036317809 -0.00036964077 -0.0012354085 -0.0020303214 -0.0026398834 -0.003036571 -0.0032528802 -0.0033498395 -0.0033832521 -0.0033912009][-0.001017875 0.0010954745 0.0031499474 0.0043319464 0.0044432944 0.0035837998 0.002149523 0.00053280429 -0.00093441806 -0.0020559439 -0.0027756686 -0.0031596252 -0.0033261545 -0.0033805096 -0.0033921923][0.0013022141 0.0047573559 0.0081010824 0.010221047 0.010700245 0.0095889 0.0073752981 0.0046262061 0.0019075091 -0.00034608948 -0.0019004875 -0.0027897048 -0.0032044773 -0.0033534029 -0.003390156][0.0039167665 0.0087198243 0.013508706 0.01698829 0.018417878 0.017579554 0.014839693 0.010873429 0.0065608378 0.0026777319 -0.00021074875 -0.001995425 -0.0029029234 -0.003266433 -0.0033729584][0.006148098 0.011934777 0.017954089 0.022860557 0.025587501 0.025524193 0.022723958 0.01786 0.012076024 0.0065016905 0.0020863337 -0.00082292431 -0.0024078409 -0.003102751 -0.0033339132][0.0072350698 0.013473454 0.020272564 0.026348693 0.030369794 0.031338751 0.028925369 0.023713822 0.016978355 0.010112784 0.0043952856 0.00043789949 -0.0018294217 -0.0028901352 -0.0032750051][0.0069632288 0.013025084 0.019969186 0.026653718 0.031607874 0.033569902 0.031855237 0.026876278 0.019883733 0.012410463 0.0059511447 0.0013373483 -0.0013908306 -0.0027183874 -0.0032244322][0.0054759197 0.010766742 0.01715658 0.02368583 0.028897302 0.031447254 0.030467061 0.026158346 0.019633748 0.012422682 0.0060643815 0.0014596137 -0.0013057797 -0.0026765317 -0.0032112382][0.0031954132 0.0072980598 0.012453896 0.017966675 0.022611333 0.025173036 0.0247635 0.021411564 0.016027702 0.0099717863 0.0046027731 0.000710577 -0.0016308147 -0.002790868 -0.0032428664][0.00068256329 0.0034391268 0.0070312312 0.01100397 0.01447218 0.016508752 0.01639745 0.014094747 0.010274877 0.0059635267 0.0021607412 -0.00057572545 -0.0022067439 -0.0030005891 -0.003300905][-0.0013632113 0.00017197034 0.0022431246 0.004607005 0.0067310967 0.0080251321 0.0080234027 0.0066832397 0.0044295387 0.0018990182 -0.00030544726 -0.0018666483 -0.0027757317 -0.0032013445 -0.003352823][-0.0026316277 -0.0019474867 -0.0009871698 0.00014357432 0.0011827012 0.0018276039 0.0018398927 0.0011982138 0.00012417207 -0.001063603 -0.0020759455 -0.0027713364 -0.0031586858 -0.0033288128 -0.0033834612][-0.0032028134 -0.0029785493 -0.0026431708 -0.0022303236 -0.001840434 -0.0015959524 -0.0015902676 -0.0018275783 -0.0022176323 -0.0026396704 -0.0029866779 -0.0032140512 -0.0033325918 -0.0033802481 -0.0033933716][-0.003371407 -0.0033253112 -0.0032463234 -0.0031403271 -0.0030353661 -0.002968723 -0.0029667511 -0.0030275411 -0.0031258212 -0.0032297787 -0.0033108259 -0.0033598903 -0.0033832809 -0.0033919571 -0.0033937572]]...]
INFO - root - 2017-12-09 16:24:46.790264: step 40410, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 70h:08m:36s remains)
INFO - root - 2017-12-09 16:24:55.269842: step 40420, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 67h:58m:19s remains)
INFO - root - 2017-12-09 16:25:03.740728: step 40430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:11m:11s remains)
INFO - root - 2017-12-09 16:25:12.320480: step 40440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:37m:48s remains)
INFO - root - 2017-12-09 16:25:21.008148: step 40450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:31m:11s remains)
INFO - root - 2017-12-09 16:25:29.617778: step 40460, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:58m:37s remains)
INFO - root - 2017-12-09 16:25:37.853120: step 40470, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 69h:50m:35s remains)
INFO - root - 2017-12-09 16:25:46.261173: step 40480, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 68h:37m:00s remains)
INFO - root - 2017-12-09 16:25:54.669990: step 40490, loss = 0.88, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 68h:08m:43s remains)
INFO - root - 2017-12-09 16:26:03.168013: step 40500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:59m:03s remains)
2017-12-09 16:26:04.143499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033990059 -0.0033980249 -0.0033967292 -0.0033936324 -0.0033887592 -0.0033841885 -0.0033802246 -0.0033776292 -0.0033761773 -0.0033754939 -0.0033750995 -0.003374873 -0.0033746362 -0.0033745174 -0.0033744122][-0.0033970072 -0.0033963292 -0.0033951681 -0.003392614 -0.0033881818 -0.0033834227 -0.003378775 -0.0033758 -0.0033741698 -0.0033730778 -0.0033726355 -0.0033725079 -0.003372509 -0.0033725521 -0.0033725312][-0.003394933 -0.0033941695 -0.0033938165 -0.0033924719 -0.0033886519 -0.0033837794 -0.0033793438 -0.0033764893 -0.0033745954 -0.0033731319 -0.0033727651 -0.0033727558 -0.0033728327 -0.0033729749 -0.0033730273][-0.003390515 -0.0033892661 -0.0033895392 -0.003389105 -0.003386491 -0.0033827082 -0.0033791829 -0.0033768511 -0.0033749759 -0.0033734976 -0.0033732427 -0.0033733044 -0.003373181 -0.0033731868 -0.0033732688][-0.0033848498 -0.0033832786 -0.0033838951 -0.0033843492 -0.0033830435 -0.003380815 -0.003378636 -0.0033771717 -0.0033753663 -0.0033740541 -0.00337421 -0.0033742243 -0.0033737232 -0.0033734706 -0.0033734888][-0.0033801531 -0.0033783 -0.0033791014 -0.0033803375 -0.0033805333 -0.0033801382 -0.0033793733 -0.0033784739 -0.0033763999 -0.0033751873 -0.0033753552 -0.0033748548 -0.0033737684 -0.0033731849 -0.0033729591][-0.0033769093 -0.0033750597 -0.0033763319 -0.0033782911 -0.003379755 -0.0033812325 -0.0033819634 -0.0033807268 -0.0033785303 -0.0033775452 -0.0033771442 -0.0033756525 -0.0033737745 -0.0033730313 -0.0033724394][-0.0033752383 -0.003373594 -0.0033753086 -0.0033779964 -0.0033808758 -0.0033840744 -0.0033854619 -0.0033835582 -0.0033817785 -0.0033813065 -0.0033797026 -0.003376432 -0.0033737 -0.0033725773 -0.0033716892][-0.0033743267 -0.0033730934 -0.0033752411 -0.0033786348 -0.0033827845 -0.0033871827 -0.0033889713 -0.0033876777 -0.0033872074 -0.0033857604 -0.0033819391 -0.0033773209 -0.0033736834 -0.0033718373 -0.0033704943][-0.0033734185 -0.0033724143 -0.0033751396 -0.0033790225 -0.0033838656 -0.0033888549 -0.0033920684 -0.0033930372 -0.0033927253 -0.0033894258 -0.0033837387 -0.0033784471 -0.0033741505 -0.0033713947 -0.0033694103][-0.003372557 -0.0033714511 -0.0033742751 -0.0033781689 -0.0033832823 -0.003388497 -0.0033929893 -0.0033953201 -0.0033948491 -0.0033911215 -0.0033852679 -0.0033797375 -0.003374784 -0.0033713654 -0.0033689342][-0.003371659 -0.0033703279 -0.0033729747 -0.0033768034 -0.0033818048 -0.0033867189 -0.00339128 -0.0033937397 -0.0033936175 -0.0033907844 -0.0033855767 -0.0033801082 -0.0033752918 -0.0033718222 -0.0033690808][-0.0033708687 -0.0033692159 -0.0033717069 -0.0033750408 -0.0033791934 -0.0033832174 -0.0033869289 -0.0033890936 -0.0033893299 -0.003387341 -0.00338349 -0.0033791033 -0.0033749954 -0.0033716941 -0.0033689735][-0.0033707572 -0.003368625 -0.003370641 -0.0033729922 -0.0033758986 -0.0033788434 -0.0033814549 -0.0033829464 -0.0033831699 -0.0033819978 -0.0033795671 -0.003376557 -0.0033735204 -0.0033709204 -0.0033688147][-0.0033710802 -0.0033682832 -0.003369418 -0.0033707363 -0.0033723614 -0.0033740862 -0.0033756027 -0.0033765128 -0.0033768145 -0.0033763894 -0.0033752401 -0.0033736033 -0.0033718732 -0.003370319 -0.0033690445]]...]
INFO - root - 2017-12-09 16:26:12.648350: step 40510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:19m:11s remains)
INFO - root - 2017-12-09 16:26:21.297148: step 40520, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:28m:13s remains)
INFO - root - 2017-12-09 16:26:30.077929: step 40530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:24m:26s remains)
INFO - root - 2017-12-09 16:26:38.781645: step 40540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:11m:11s remains)
INFO - root - 2017-12-09 16:26:47.412317: step 40550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:44m:55s remains)
INFO - root - 2017-12-09 16:26:55.918341: step 40560, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 69h:15m:42s remains)
INFO - root - 2017-12-09 16:27:04.254617: step 40570, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 69h:05m:38s remains)
INFO - root - 2017-12-09 16:27:12.779953: step 40580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:01m:09s remains)
INFO - root - 2017-12-09 16:27:21.392450: step 40590, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.857 sec/batch; 69h:28m:38s remains)
INFO - root - 2017-12-09 16:27:29.900921: step 40600, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 67h:04m:17s remains)
2017-12-09 16:27:30.752443: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.089475326 0.087919436 0.085832968 0.083408155 0.080509037 0.07712812 0.073403023 0.069371574 0.065253638 0.060854878 0.056810129 0.053206187 0.050355528 0.048439346 0.047304515][0.091013908 0.090183221 0.08903373 0.087796018 0.085966818 0.083336323 0.08003822 0.076161519 0.071864605 0.066988334 0.062329415 0.057926804 0.054213677 0.051457375 0.049500961][0.0912178 0.091521293 0.091552086 0.091322929 0.090404227 0.088556379 0.085764535 0.082148291 0.077828206 0.072633289 0.067333944 0.062065933 0.057393998 0.053644579 0.05071016][0.089244023 0.090655737 0.091930211 0.092888929 0.093100592 0.092302479 0.090288639 0.086928554 0.082669869 0.077472307 0.071696073 0.06568294 0.060140971 0.055499673 0.051692408][0.087198354 0.089266509 0.091334924 0.093268804 0.094469264 0.094849691 0.093615942 0.090900376 0.087125838 0.082117036 0.076290943 0.069766767 0.063637778 0.058090221 0.053366791][0.085164964 0.087852664 0.0905778 0.093312904 0.095471196 0.096680149 0.096312381 0.094248042 0.090793356 0.086213641 0.0804957 0.073913351 0.067536779 0.061534118 0.056294944][0.083274864 0.086394906 0.0893173 0.09243913 0.095188573 0.09698461 0.097283028 0.095868312 0.092950061 0.088668995 0.083194971 0.0768186 0.070472874 0.064513847 0.059216063][0.081141 0.084313653 0.087055847 0.090065069 0.092858762 0.094918139 0.095638916 0.094850726 0.092495926 0.088813908 0.08379931 0.07786642 0.07190299 0.06624876 0.061186116][0.078204781 0.081399061 0.083879262 0.0866256 0.08928588 0.09138111 0.092274919 0.091854528 0.089950554 0.086833425 0.082357764 0.07697574 0.07154 0.06643454 0.061989915][0.074769236 0.077945367 0.080148779 0.082553275 0.084980264 0.086851262 0.087682925 0.087340824 0.085623115 0.082717337 0.078618154 0.073816337 0.068996996 0.064594969 0.060982842][0.070972979 0.073728032 0.075462945 0.077542685 0.079630524 0.081225209 0.082041249 0.081702046 0.079932228 0.077031456 0.073013768 0.0685449 0.064179085 0.060551 0.057890084][0.068134181 0.07048694 0.071590669 0.072962925 0.074387453 0.075572319 0.076041505 0.075547293 0.073740117 0.070707314 0.066707216 0.062341172 0.058256648 0.055104494 0.053080712][0.066644534 0.068483971 0.068993971 0.069660269 0.070356421 0.070911191 0.070887648 0.070036083 0.067929953 0.0647472 0.060635332 0.056231033 0.052256368 0.04927469 0.047598522][0.064328477 0.066173166 0.06646277 0.066820242 0.067165747 0.067313664 0.066892356 0.065620013 0.063206896 0.059759665 0.05545241 0.050881721 0.046754628 0.043658625 0.041938048][0.061858248 0.063713782 0.063951343 0.064270087 0.064502306 0.064462468 0.063837081 0.062341373 0.05973405 0.05607615 0.051593978 0.046804551 0.0423459 0.038775761 0.036462177]]...]
INFO - root - 2017-12-09 16:27:39.340224: step 40610, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.826 sec/batch; 66h:57m:34s remains)
INFO - root - 2017-12-09 16:27:47.999282: step 40620, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 70h:39m:45s remains)
INFO - root - 2017-12-09 16:27:56.640103: step 40630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 72h:40m:27s remains)
INFO - root - 2017-12-09 16:28:05.385929: step 40640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:19m:34s remains)
INFO - root - 2017-12-09 16:28:14.095230: step 40650, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 71h:03m:32s remains)
INFO - root - 2017-12-09 16:28:22.772488: step 40660, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 67h:27m:29s remains)
INFO - root - 2017-12-09 16:28:31.337742: step 40670, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 72h:55m:45s remains)
INFO - root - 2017-12-09 16:28:39.909529: step 40680, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 72h:54m:02s remains)
INFO - root - 2017-12-09 16:28:48.525162: step 40690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:06m:56s remains)
INFO - root - 2017-12-09 16:28:57.111528: step 40700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:10m:18s remains)
2017-12-09 16:28:57.941662: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0013085941 0.0012479525 0.0011244656 0.00087954919 0.00052279769 9.4485236e-05 -0.00034481869 -0.00071782875 -0.0010015455 -0.0011206928 -0.0012600969 -0.0015203435 -0.001718572 -0.0017651406 -0.0017929226][-0.00083600241 -0.00075478316 -0.000658056 -0.00059704832 -0.00060314662 -0.0007045269 -0.00088235294 -0.0011276484 -0.0014096559 -0.001681031 -0.0020107212 -0.0023401789 -0.0025935106 -0.0026208975 -0.0026448378][-0.000841476 -0.00076516066 -0.000704658 -0.00068307994 -0.0007037851 -0.00078573707 -0.00091693015 -0.0010761081 -0.0013155148 -0.0016329486 -0.0019736825 -0.0022423049 -0.0025683881 -0.0028975124 -0.0030785757][-0.001118114 -0.0011244339 -0.0010450049 -0.00090350537 -0.00075492961 -0.0005565586 -0.00034051086 -0.00015074061 -2.1470943e-05 -8.6218817e-05 -0.00026686629 -0.0007137144 -0.0011869059 -0.0015256733 -0.0017674579][0.000714883 0.00055163796 0.00040303566 0.00027058134 7.1225222e-05 -0.00020106882 -0.00051898835 -0.00075630262 -0.00091887964 -0.001030402 -0.0010522085 -0.001078919 -0.0011401314 -0.0014492567 -0.0017976327][0.000510399 0.00061766175 0.00064972905 0.00066561042 0.00071532954 0.0005689899 0.00027549639 -0.00013544061 -0.00055964477 -0.00098947878 -0.0013918241 -0.0017699547 -0.0021353089 -0.0024862948 -0.0028049571][0.002816627 0.0030619821 0.0031621021 0.0031405443 0.0030514987 0.0027025335 0.0022214975 0.0017336879 0.0012225911 0.00069474219 0.00017573079 -0.00035142084 -0.00094467262 -0.0016033588 -0.0021853088][0.00817023 0.0086515956 0.0087103266 0.0086328406 0.008303307 0.0076489775 0.0067092273 0.0056076571 0.0044780467 0.0034608019 0.0027064166 0.0020157967 0.0013704693 0.00072802952 6.1487313e-05][0.013932617 0.014861237 0.014953203 0.014936376 0.014586562 0.013829628 0.012702528 0.011403376 0.0099774329 0.0086245649 0.0074575637 0.006470358 0.0056399507 0.0048502721 0.0041238032][0.016849177 0.018189088 0.018489799 0.018734528 0.018663324 0.018118815 0.017167453 0.015994318 0.014675958 0.013435189 0.012301194 0.011320751 0.010414249 0.0095382761 0.0086896894][0.016934264 0.018404284 0.018854273 0.019256713 0.019442726 0.019253919 0.018709723 0.017967414 0.017089481 0.016203655 0.015334293 0.014516108 0.013707748 0.01285892 0.011937804][0.015777761 0.017121956 0.017491611 0.017868815 0.01808433 0.01801572 0.017740656 0.017406879 0.017028561 0.016547242 0.016082298 0.015638895 0.015118882 0.014454748 0.013603529][0.014620895 0.015579654 0.015862156 0.016193509 0.016407797 0.016357254 0.016107082 0.015786236 0.015467143 0.015176158 0.014897401 0.014620354 0.014269236 0.01378049 0.01312012][0.012293426 0.012719896 0.012865709 0.013123821 0.013355566 0.013434874 0.01337255 0.013181763 0.012882194 0.012550686 0.01224109 0.012003824 0.011732758 0.011398856 0.010959029][0.0081416881 0.0083949408 0.008454781 0.0085904468 0.0087416479 0.0088561848 0.0089053418 0.0088741351 0.0087559307 0.0085606854 0.0083556678 0.0081638517 0.00796204 0.0077570016 0.00746328]]...]
INFO - root - 2017-12-09 16:29:06.572560: step 40710, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:58m:19s remains)
INFO - root - 2017-12-09 16:29:15.441164: step 40720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:18m:51s remains)
INFO - root - 2017-12-09 16:29:24.073023: step 40730, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 71h:38m:24s remains)
INFO - root - 2017-12-09 16:29:32.722656: step 40740, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 66h:56m:15s remains)
INFO - root - 2017-12-09 16:29:41.325459: step 40750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:25m:31s remains)
INFO - root - 2017-12-09 16:29:50.052662: step 40760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:50m:06s remains)
INFO - root - 2017-12-09 16:29:58.737230: step 40770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:09m:01s remains)
INFO - root - 2017-12-09 16:30:07.318825: step 40780, loss = 0.88, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 72h:49m:37s remains)
INFO - root - 2017-12-09 16:30:16.068798: step 40790, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 72h:28m:40s remains)
INFO - root - 2017-12-09 16:30:24.668048: step 40800, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 67h:40m:57s remains)
2017-12-09 16:30:25.505954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032802746 -0.0032673259 -0.0032640435 -0.0032659878 -0.0032681087 -0.0032699471 -0.0032691187 -0.0032652887 -0.0032590397 -0.0032590991 -0.0032701776 -0.0032900639 -0.0033157216 -0.0033422571 -0.0033632866][-0.003256917 -0.0032388309 -0.0032259924 -0.0032225689 -0.0032184494 -0.0032188057 -0.00321791 -0.003212878 -0.0032076426 -0.0032128859 -0.0032304546 -0.00325699 -0.0032895086 -0.0033233904 -0.003354009][-0.0032326803 -0.0032073378 -0.0031894576 -0.003182325 -0.0031788826 -0.003176203 -0.0031746321 -0.0031714514 -0.0031695422 -0.0031778552 -0.0031982337 -0.0032287564 -0.0032661872 -0.003304848 -0.003338208][-0.0032186408 -0.0031874795 -0.0031623833 -0.0031458058 -0.0031366795 -0.0031345524 -0.0031361128 -0.0031367056 -0.0031428016 -0.0031576585 -0.0031823874 -0.0032194038 -0.0032620609 -0.00330355 -0.0033352564][-0.0032255221 -0.0031917475 -0.0031595652 -0.0031321044 -0.0031144193 -0.0031048451 -0.0031029284 -0.0031099534 -0.0031260224 -0.0031519744 -0.003187133 -0.0032236683 -0.0032615711 -0.0032998677 -0.0033300181][-0.0032201 -0.003185872 -0.0031547644 -0.0031258729 -0.0031051056 -0.0030924927 -0.0030880731 -0.0030926585 -0.0031149236 -0.0031478568 -0.0031822682 -0.0032160606 -0.0032514129 -0.003287955 -0.0033216327][-0.0032173884 -0.0031834026 -0.0031512792 -0.0031224734 -0.0031024588 -0.0030918026 -0.0030919316 -0.0031002094 -0.0031183683 -0.0031448714 -0.0031759148 -0.0032070794 -0.0032407385 -0.0032772706 -0.0033118885][-0.0032171956 -0.0031862024 -0.0031565668 -0.0031282164 -0.0031082216 -0.0030983449 -0.0031013968 -0.0031138756 -0.0031337107 -0.0031571384 -0.0031814063 -0.0032060966 -0.0032349478 -0.0032686044 -0.003302488][-0.0032195947 -0.0031932974 -0.0031662751 -0.0031383152 -0.0031168433 -0.0031057389 -0.0031085443 -0.0031228377 -0.0031420444 -0.0031647473 -0.0031879279 -0.0032094277 -0.0032330493 -0.0032605224 -0.0032907925][-0.0032200916 -0.0031961219 -0.0031697506 -0.0031438021 -0.0031241446 -0.0031156146 -0.0031180221 -0.0031325442 -0.003151838 -0.0031750822 -0.0031952225 -0.0032097497 -0.0032236015 -0.0032413513 -0.0032655532][-0.0032082063 -0.0031881623 -0.0031668656 -0.0031465483 -0.003131011 -0.003125668 -0.0031316055 -0.0031462915 -0.0031602078 -0.0031740209 -0.0031868897 -0.0031992157 -0.0032141325 -0.003231497 -0.0032563596][-0.0032264055 -0.0032103406 -0.0031922215 -0.0031724321 -0.0031564576 -0.0031501651 -0.0031540252 -0.0031639591 -0.0031750549 -0.003188435 -0.0031971871 -0.0032028933 -0.0032060046 -0.0032144298 -0.0032334758][-0.0032150673 -0.0032122191 -0.0032062388 -0.0032004784 -0.0031915447 -0.0031868345 -0.0031845458 -0.0031819141 -0.0031810799 -0.0031855796 -0.0031876513 -0.0031876876 -0.0031859279 -0.0031895179 -0.0032044738][-0.0031991582 -0.00320184 -0.0032019694 -0.0031991224 -0.0031964246 -0.0031991729 -0.0032033825 -0.0032028302 -0.0032000369 -0.0031971834 -0.0031892867 -0.0031785993 -0.0031676863 -0.0031654381 -0.00317655][-0.0031920332 -0.003192245 -0.003191185 -0.0031893239 -0.0031889179 -0.0031941484 -0.0031997184 -0.0032050426 -0.0032057702 -0.0032014069 -0.0031868627 -0.0031711895 -0.0031574527 -0.0031504019 -0.0031536263]]...]
INFO - root - 2017-12-09 16:30:34.110331: step 40810, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 68h:22m:56s remains)
INFO - root - 2017-12-09 16:30:42.699060: step 40820, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:11m:31s remains)
INFO - root - 2017-12-09 16:30:51.422518: step 40830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:15m:03s remains)
INFO - root - 2017-12-09 16:31:00.244271: step 40840, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 74h:28m:44s remains)
INFO - root - 2017-12-09 16:31:08.986606: step 40850, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 71h:18m:00s remains)
INFO - root - 2017-12-09 16:31:17.868923: step 40860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 70h:08m:37s remains)
INFO - root - 2017-12-09 16:31:26.527196: step 40870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:58m:21s remains)
INFO - root - 2017-12-09 16:31:35.082280: step 40880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:39m:34s remains)
INFO - root - 2017-12-09 16:31:43.656102: step 40890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:13m:49s remains)
INFO - root - 2017-12-09 16:31:52.374398: step 40900, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 67h:42m:02s remains)
2017-12-09 16:31:53.289188: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20343325 0.19752759 0.19054404 0.18478683 0.18052536 0.17857429 0.17888862 0.17989385 0.18194571 0.18418786 0.18817753 0.19223893 0.19585994 0.19890186 0.20214045][0.19996876 0.19429126 0.18726698 0.18115345 0.17682141 0.17447814 0.17407268 0.17406988 0.17541322 0.17758471 0.1817764 0.18619776 0.19082567 0.19574031 0.20145157][0.18953216 0.1837711 0.17650345 0.17064339 0.16675341 0.16431272 0.16328174 0.16211703 0.16267669 0.16418871 0.16754389 0.17197204 0.17737883 0.18389151 0.19120216][0.17880565 0.17340049 0.16665539 0.16104326 0.15715417 0.15482411 0.15329376 0.15115729 0.15069613 0.15164274 0.15477479 0.15888232 0.16462123 0.17251581 0.18167019][0.16875508 0.16401479 0.15803936 0.15291685 0.14935696 0.146826 0.14459148 0.14171445 0.14043865 0.14029932 0.14236796 0.14634374 0.15261465 0.16101325 0.17094007][0.15787324 0.15435073 0.14866403 0.14424133 0.14145043 0.13937744 0.13728109 0.13413259 0.13189989 0.13102707 0.13219005 0.13543636 0.14126348 0.15007383 0.16062796][0.14613491 0.14411664 0.13904265 0.1346136 0.13170624 0.12990314 0.12805495 0.12518406 0.12318515 0.12225702 0.12312501 0.12620553 0.13232312 0.14071515 0.15111569][0.13394454 0.13259366 0.1276212 0.12388288 0.12158061 0.1199906 0.11850053 0.11625487 0.11494888 0.1141468 0.11480904 0.11787575 0.12364339 0.13185433 0.14179817][0.1177295 0.11681482 0.11254638 0.10920266 0.1075371 0.10694247 0.10671142 0.10590999 0.10550193 0.10577787 0.10708511 0.10998417 0.11483563 0.12195054 0.13036609][0.097118072 0.09662766 0.093006343 0.090381354 0.089543387 0.09002836 0.091194071 0.092267521 0.0934424 0.0951394 0.097368233 0.10031705 0.10429367 0.10955993 0.11562537][0.073543079 0.0733967 0.070706218 0.069307081 0.069757134 0.071604975 0.0741364 0.076887965 0.079817072 0.083016746 0.086107522 0.089207627 0.092391975 0.095963113 0.099832788][0.051407732 0.050994631 0.049223404 0.049008038 0.050604861 0.05360011 0.057436798 0.061527885 0.065589793 0.069825232 0.073580392 0.076843463 0.079449743 0.081827037 0.083926752][0.033462644 0.0327237 0.031374436 0.032037113 0.034345157 0.037849464 0.042200878 0.046921924 0.051623981 0.056138366 0.060081527 0.063217975 0.065286033 0.066656768 0.06746836][0.019552467 0.018390751 0.01724723 0.017962696 0.020286899 0.02349093 0.027370555 0.031569216 0.035841238 0.040037394 0.043513078 0.046339352 0.047991931 0.04865497 0.048663959][0.010082276 0.0086283162 0.0074959258 0.00791847 0.0095095085 0.011857287 0.014612745 0.017628133 0.020659387 0.02382696 0.026555076 0.028537054 0.029566862 0.02973019 0.02942251]]...]
INFO - root - 2017-12-09 16:32:01.822956: step 40910, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:54m:36s remains)
INFO - root - 2017-12-09 16:32:10.554766: step 40920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:27m:54s remains)
INFO - root - 2017-12-09 16:32:19.223811: step 40930, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 72h:18m:04s remains)
INFO - root - 2017-12-09 16:32:27.871142: step 40940, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 71h:36m:20s remains)
INFO - root - 2017-12-09 16:32:36.568576: step 40950, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 68h:30m:02s remains)
INFO - root - 2017-12-09 16:32:45.240919: step 40960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:40m:36s remains)
INFO - root - 2017-12-09 16:32:53.705132: step 40970, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 65h:25m:52s remains)
INFO - root - 2017-12-09 16:33:02.344974: step 40980, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.696 sec/batch; 56h:21m:51s remains)
INFO - root - 2017-12-09 16:33:11.003009: step 40990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:56m:53s remains)
INFO - root - 2017-12-09 16:33:19.646457: step 41000, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 69h:47m:52s remains)
2017-12-09 16:33:20.568757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027622262 0.036034163 0.04327726 0.047609244 0.049000103 0.047532618 0.043766506 0.037896842 0.030589173 0.022834636 0.015553834 0.0095469179 0.0050061308 0.0019120497 1.5469268e-06][0.030553071 0.040904555 0.049944535 0.05556481 0.057550989 0.055891883 0.051306874 0.044118352 0.035074748 0.025397833 0.01635305 0.00911391 0.0040210988 0.000972403 -0.00059562852][0.03237218 0.044392504 0.055011224 0.061783124 0.0644526 0.063084558 0.058396924 0.050712131 0.040764596 0.029749246 0.019134643 0.010472744 0.0043707807 0.00077721593 -0.0010413111][0.033351637 0.046453264 0.05835538 0.066315994 0.069777235 0.068823084 0.064328529 0.05668674 0.046608474 0.035216995 0.023893364 0.014251756 0.0070027085 0.0023486426 -0.00026827608][0.033727173 0.04747605 0.06018937 0.069172159 0.073865779 0.074190222 0.070741512 0.063785478 0.053930536 0.042321313 0.030482085 0.020213455 0.012168515 0.0064140614 0.0024232792][0.034006119 0.048226524 0.061538462 0.07120432 0.076769039 0.078339145 0.076505736 0.071217254 0.06273488 0.051783685 0.039789405 0.028716171 0.019495424 0.012372345 0.0068913884][0.033731975 0.048378579 0.0622144 0.072376423 0.078664266 0.081286 0.080922619 0.0775033 0.070955008 0.061564546 0.050472908 0.03948655 0.02954716 0.020960247 0.013564115][0.03266266 0.047571298 0.061692808 0.072069556 0.07875257 0.082272455 0.083409183 0.081812575 0.077052295 0.069203362 0.059221383 0.048928741 0.03923548 0.030300731 0.021877209][0.031012671 0.045700531 0.059818383 0.070304386 0.077225044 0.08128605 0.083461739 0.083360791 0.080130786 0.07364475 0.064774148 0.055304658 0.046124104 0.037398476 0.028817253][0.028845401 0.043140087 0.056930114 0.067163885 0.074061878 0.078348 0.081160754 0.082102224 0.080148563 0.075006157 0.06728974 0.058686517 0.050081756 0.041768454 0.033401787][0.025946949 0.039696675 0.053073931 0.063003466 0.0695489 0.07351 0.076261222 0.077594146 0.076523475 0.072644033 0.066316687 0.058936853 0.051160548 0.043411851 0.035546735][0.022037067 0.034734391 0.047319602 0.056810733 0.063081115 0.06677787 0.069299974 0.0706316 0.069966659 0.066923358 0.06174149 0.055596855 0.048889 0.042009134 0.034898795][0.017661074 0.028538996 0.039593268 0.048143398 0.05382441 0.057156146 0.059435185 0.060704913 0.06033219 0.057980526 0.053822976 0.048800323 0.043171126 0.037292406 0.031152116][0.013086889 0.021821946 0.030887412 0.038038015 0.042822406 0.045483373 0.04714933 0.048028141 0.047780581 0.046121195 0.043137439 0.039431754 0.03502848 0.0301952 0.025058936][0.0082775969 0.014900597 0.021882754 0.027459478 0.031194231 0.03316481 0.034246605 0.034662303 0.034355134 0.033181984 0.031187257 0.028718146 0.0256483 0.022067491 0.018106831]]...]
INFO - root - 2017-12-09 16:33:29.200317: step 41010, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 72h:36m:25s remains)
INFO - root - 2017-12-09 16:33:37.913308: step 41020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:13m:39s remains)
INFO - root - 2017-12-09 16:33:46.682945: step 41030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:39m:56s remains)
INFO - root - 2017-12-09 16:33:55.310425: step 41040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:45m:59s remains)
INFO - root - 2017-12-09 16:34:04.122848: step 41050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 68h:11m:06s remains)
INFO - root - 2017-12-09 16:34:12.956990: step 41060, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.878 sec/batch; 71h:03m:39s remains)
INFO - root - 2017-12-09 16:34:21.476184: step 41070, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 64h:54m:30s remains)
INFO - root - 2017-12-09 16:34:29.938038: step 41080, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 56h:49m:51s remains)
INFO - root - 2017-12-09 16:34:38.617963: step 41090, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 71h:28m:36s remains)
INFO - root - 2017-12-09 16:34:47.236742: step 41100, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:45m:04s remains)
2017-12-09 16:34:48.139232: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049620036 0.05386731 0.05592167 0.0561238 0.055007745 0.053300448 0.051290609 0.049122736 0.046823781 0.044639654 0.042544331 0.040395781 0.038118362 0.036002211 0.034278061][0.050179012 0.05549369 0.058713563 0.060090329 0.059963558 0.058887698 0.057123072 0.054902032 0.052334279 0.049771056 0.047196254 0.044420023 0.041299667 0.0381802 0.035567656][0.048155986 0.054473247 0.059064548 0.06202485 0.063480891 0.0637243 0.062820554 0.060964938 0.0584401 0.055665176 0.052715875 0.049283348 0.045140333 0.040679354 0.036704075][0.044287778 0.051416788 0.057380024 0.062014874 0.065333582 0.067325018 0.067870006 0.066869766 0.064774536 0.062005308 0.058778819 0.054744706 0.04962359 0.043812118 0.03823087][0.039808705 0.047433525 0.054538105 0.060641754 0.065660872 0.069360368 0.071526 0.07178551 0.070522174 0.068012714 0.064593032 0.059970211 0.054022212 0.047076289 0.040064942][0.035710517 0.043507379 0.051330831 0.058460012 0.064703308 0.069688335 0.073227152 0.07477551 0.074524105 0.072539866 0.069093123 0.064042129 0.057436347 0.049664658 0.041659579][0.032283425 0.040049072 0.048121065 0.05574942 0.0626128 0.06830696 0.072728544 0.075266048 0.075937472 0.074592791 0.07137619 0.066184618 0.059254434 0.051068582 0.042579621][0.029501254 0.037098404 0.04504659 0.052696507 0.059628386 0.065511733 0.070291393 0.073331706 0.074535094 0.073662184 0.070760347 0.065718122 0.058897197 0.050789244 0.042390186][0.026925407 0.034225821 0.041809544 0.049136475 0.055735506 0.061407004 0.06614279 0.069360137 0.070794046 0.070114776 0.067417659 0.0627206 0.056372639 0.048825953 0.040971711][0.024258925 0.030988613 0.037941664 0.044727147 0.05083023 0.056088112 0.060489435 0.063541465 0.064920373 0.064306535 0.061811369 0.057570312 0.051938515 0.045258842 0.038239393][0.021427138 0.027318697 0.03336519 0.039301142 0.044657327 0.049276434 0.053170409 0.055921074 0.057165734 0.05657576 0.054325998 0.050627619 0.045844004 0.040207919 0.034208886][0.018557061 0.023369489 0.028254265 0.033080216 0.037440032 0.041156903 0.044309121 0.046585497 0.047672603 0.047258988 0.045442216 0.04237796 0.03847973 0.033949025 0.029097436][0.015770573 0.019413076 0.022979511 0.026517063 0.029707251 0.032385781 0.034628086 0.036269851 0.037135497 0.036991592 0.035775933 0.033511039 0.030587925 0.027150126 0.023466978][0.013178194 0.01569118 0.017981131 0.020270867 0.022311168 0.023980482 0.025323149 0.026296992 0.026868273 0.02691693 0.026287049 0.024889689 0.022968436 0.020603403 0.018086532][0.011054341 0.012634117 0.013855485 0.015091574 0.0161971 0.017097922 0.017769361 0.018197987 0.018444266 0.018510656 0.018271411 0.017590104 0.016537497 0.01514338 0.013643868]]...]
INFO - root - 2017-12-09 16:34:56.711309: step 41110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:56m:26s remains)
INFO - root - 2017-12-09 16:35:05.395691: step 41120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:52m:09s remains)
INFO - root - 2017-12-09 16:35:13.832655: step 41130, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 66h:04m:58s remains)
INFO - root - 2017-12-09 16:35:22.284245: step 41140, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 72h:54m:40s remains)
INFO - root - 2017-12-09 16:35:31.043765: step 41150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:53m:02s remains)
INFO - root - 2017-12-09 16:35:40.024556: step 41160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:07m:29s remains)
INFO - root - 2017-12-09 16:35:48.735437: step 41170, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 68h:30m:24s remains)
INFO - root - 2017-12-09 16:35:57.152479: step 41180, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 61h:17m:54s remains)
INFO - root - 2017-12-09 16:36:05.862875: step 41190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 68h:30m:26s remains)
INFO - root - 2017-12-09 16:36:14.485437: step 41200, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:10m:28s remains)
2017-12-09 16:36:15.455318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033465934 -0.0033427028 -0.003342309 -0.0033423009 -0.0033423426 -0.0033426022 -0.0033429519 -0.0033433577 -0.0033437547 -0.0033441419 -0.0033445759 -0.003344981 -0.003345334 -0.0033456595 -0.0033460159][-0.0033447307 -0.0033404366 -0.0033399365 -0.0033398769 -0.0033398373 -0.0033399472 -0.0033401612 -0.0033404557 -0.0033407777 -0.0033411449 -0.0033415977 -0.0033420236 -0.0033424078 -0.0033427812 -0.0033431679][-0.0033456916 -0.0033414043 -0.0033409013 -0.0033407465 -0.0033405535 -0.0033403842 -0.0033403011 -0.0033403311 -0.0033404923 -0.0033408389 -0.0033413486 -0.0033418725 -0.0033423447 -0.0033427903 -0.0033432147][-0.0033460353 -0.0033418522 -0.0033413761 -0.0033411239 -0.0033408119 -0.0033404334 -0.0033401283 -0.0033399877 -0.0033401011 -0.0033404226 -0.003340919 -0.0033415013 -0.0033420578 -0.0033425798 -0.0033430639][-0.0033461163 -0.0033420981 -0.0033417698 -0.003341584 -0.0033412455 -0.0033407183 -0.0033402108 -0.0033398576 -0.0033397393 -0.0033398687 -0.0033402597 -0.0033408173 -0.003341425 -0.0033420394 -0.0033426112][-0.0033459782 -0.0033422145 -0.0033422359 -0.0033423402 -0.003342062 -0.0033413612 -0.0033405519 -0.0033398308 -0.0033393204 -0.0033391023 -0.0033392936 -0.0033398117 -0.0033404632 -0.0033411575 -0.0033417826][-0.003345337 -0.0033421046 -0.003342652 -0.00334322 -0.0033431717 -0.0033423589 -0.0033412168 -0.0033400555 -0.0033389933 -0.0033382669 -0.0033381302 -0.0033385004 -0.0033391251 -0.0033398755 -0.003340564][-0.0033444883 -0.0033418159 -0.0033429819 -0.0033442334 -0.0033447258 -0.0033441756 -0.00334286 -0.0033411728 -0.0033393728 -0.003337916 -0.0033372084 -0.0033372429 -0.0033377362 -0.0033384522 -0.00333914][-0.0033436411 -0.003341211 -0.0033429011 -0.0033447698 -0.0033459053 -0.003345869 -0.0033447735 -0.0033429146 -0.0033405907 -0.003338452 -0.0033370908 -0.0033366613 -0.0033368955 -0.0033374596 -0.0033380585][-0.0033431705 -0.0033405307 -0.0033424126 -0.0033445691 -0.0033461608 -0.0033466935 -0.0033460693 -0.0033444157 -0.0033420164 -0.0033395961 -0.0033378492 -0.0033370124 -0.0033369097 -0.0033372438 -0.0033376862][-0.0033430185 -0.0033400496 -0.0033419197 -0.0033439805 -0.0033456751 -0.0033465442 -0.0033463587 -0.0033451451 -0.0033431286 -0.0033409197 -0.0033391812 -0.0033381793 -0.0033378336 -0.0033379439 -0.0033382142][-0.0033432161 -0.0033396729 -0.0033412273 -0.0033430099 -0.003344621 -0.0033456478 -0.00334584 -0.0033451596 -0.0033437414 -0.0033420566 -0.0033406445 -0.003339695 -0.0033392739 -0.0033392615 -0.003339435][-0.0033438723 -0.0033397067 -0.0033407856 -0.0033420476 -0.0033433209 -0.0033442581 -0.0033446557 -0.0033444418 -0.0033437337 -0.0033427589 -0.0033418252 -0.0033411507 -0.0033408238 -0.0033408208 -0.0033409831][-0.0033443314 -0.0033397593 -0.003340391 -0.00334115 -0.0033420324 -0.0033427915 -0.0033433093 -0.0033434655 -0.0033433302 -0.0033429936 -0.0033425665 -0.0033421963 -0.0033419945 -0.003342014 -0.0033421919][-0.003344706 -0.0033398862 -0.0033401526 -0.003340594 -0.00334115 -0.0033417169 -0.0033422094 -0.0033425165 -0.0033426669 -0.0033426685 -0.0033425631 -0.003342432 -0.0033423782 -0.0033424688 -0.0033426704]]...]
INFO - root - 2017-12-09 16:36:23.998734: step 41210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:02m:36s remains)
INFO - root - 2017-12-09 16:36:32.555285: step 41220, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 71h:25m:25s remains)
INFO - root - 2017-12-09 16:36:41.049077: step 41230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:23m:49s remains)
INFO - root - 2017-12-09 16:36:49.369968: step 41240, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.829 sec/batch; 67h:03m:37s remains)
INFO - root - 2017-12-09 16:36:58.019183: step 41250, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:59m:29s remains)
INFO - root - 2017-12-09 16:37:06.665476: step 41260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:31m:40s remains)
INFO - root - 2017-12-09 16:37:15.347506: step 41270, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 68h:10m:02s remains)
INFO - root - 2017-12-09 16:37:23.945481: step 41280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:13m:29s remains)
INFO - root - 2017-12-09 16:37:32.397212: step 41290, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:42m:48s remains)
INFO - root - 2017-12-09 16:37:40.916303: step 41300, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:38m:32s remains)
2017-12-09 16:37:41.747954: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0072781979 0.0028898548 0.00084644137 0.0005650809 0.0013282667 0.0025441376 0.0034524584 0.0036890861 0.003409727 0.0037345598 0.0065107909 0.012837101 0.022653399 0.032835055 0.039779164][0.01514609 0.0081974994 0.0043803914 0.0033371951 0.003955272 0.0051330025 0.0057810196 0.0054235091 0.0043659885 0.0037337826 0.0054857121 0.01065235 0.01901455 0.027951291 0.034293722][0.027244743 0.017074758 0.010467179 0.0077397362 0.0075609642 0.0083001386 0.0083631705 0.0071212514 0.0050330996 0.0031185267 0.0032668309 0.0063517834 0.012201825 0.018764967 0.023644147][0.041409388 0.028082104 0.018874127 0.014807379 0.014242569 0.015030054 0.014923196 0.012879418 0.0092481226 0.0053112046 0.0029444131 0.0032434459 0.0062936167 0.010497386 0.014082475][0.053531408 0.039247356 0.02901301 0.024774607 0.024859967 0.026563818 0.026800908 0.024091566 0.018649021 0.012060925 0.0065851496 0.0037232237 0.0039175628 0.0060099452 0.0084077735][0.058307067 0.046917982 0.039513811 0.038577627 0.042187016 0.046973843 0.048883181 0.045685384 0.037293795 0.026069066 0.015403656 0.0077780229 0.0044103242 0.0042652488 0.0055644377][0.053489897 0.047882419 0.0466241 0.051971897 0.061752833 0.071724534 0.0767178 0.073631205 0.061960861 0.045004345 0.027720101 0.014198158 0.0064735641 0.0036572176 0.003655252][0.041144229 0.041891187 0.047541637 0.059709333 0.0761261 0.0917103 0.10057197 0.09858641 0.084651366 0.062857069 0.039498437 0.020381507 0.0084383879 0.0029856162 0.0015788227][0.026451597 0.031135665 0.041098587 0.057356477 0.077649213 0.096561171 0.1081823 0.10766546 0.093617663 0.070318021 0.044498403 0.022852182 0.0087785814 0.0018721004 -0.00042061671][0.014819501 0.019844977 0.029732125 0.045147821 0.064404689 0.0826847 0.094540954 0.095207162 0.083354786 0.062725738 0.039364178 0.019584574 0.0066172979 0.00018746895 -0.0020486703][0.0071252533 0.010832258 0.017826712 0.028817819 0.042883907 0.056705832 0.066076659 0.0671584 0.058830142 0.043820441 0.026672106 0.012210105 0.0029448401 -0.001449534 -0.0028916395][0.0018809857 0.0037863501 0.0074462588 0.013472181 0.021499142 0.029734103 0.035490312 0.036345158 0.031604961 0.022853198 0.012826648 0.0045319465 -0.00048250309 -0.0026298696 -0.0032281443][-0.00097720441 -0.00031556678 0.000974013 0.0032855845 0.0066433386 0.010303173 0.01290231 0.013293184 0.011203425 0.007334 0.0029494897 -0.0005499234 -0.0024821269 -0.0031817483 -0.0033327856][-0.0015729307 -0.0017358285 -0.0018792355 -0.0016068181 -0.00072167558 0.00044745277 0.00128495 0.0014014565 0.0007351581 -0.00049121352 -0.0018088202 -0.0027635163 -0.00321774 -0.003335105 -0.003354294][-9.6519478e-05 -0.00068388111 -0.0016983505 -0.0025914963 -0.0029038833 -0.0027667233 -0.0025782217 -0.0025092878 -0.0025965429 -0.0028151912 -0.0030638888 -0.003239112 -0.0033216549 -0.0033448867 -0.0033555394]]...]
INFO - root - 2017-12-09 16:37:50.284978: step 41310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:48m:14s remains)
INFO - root - 2017-12-09 16:37:58.940754: step 41320, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 66h:13m:49s remains)
INFO - root - 2017-12-09 16:38:07.622596: step 41330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:09m:12s remains)
INFO - root - 2017-12-09 16:38:16.251713: step 41340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:17m:36s remains)
INFO - root - 2017-12-09 16:38:24.824016: step 41350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:34m:11s remains)
INFO - root - 2017-12-09 16:38:33.345209: step 41360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:17m:44s remains)
INFO - root - 2017-12-09 16:38:41.895350: step 41370, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 66h:05m:47s remains)
INFO - root - 2017-12-09 16:38:50.366782: step 41380, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 73h:12m:25s remains)
INFO - root - 2017-12-09 16:38:58.820061: step 41390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:53m:05s remains)
INFO - root - 2017-12-09 16:39:07.449022: step 41400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:42m:16s remains)
2017-12-09 16:39:08.325628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003373106 -0.003371309 -0.0033717835 -0.0033716904 -0.0033703658 -0.0033689197 -0.0033678568 -0.003366946 -0.0033663209 -0.0033664927 -0.0033672468 -0.0033673947 -0.0033674715 -0.0033676508 -0.003367648][-0.0033712678 -0.0033701528 -0.0033714566 -0.0033715332 -0.0033704273 -0.0033688562 -0.0033671756 -0.0033654252 -0.0033643174 -0.003364377 -0.0033652189 -0.0033656752 -0.0033659975 -0.0033664433 -0.00336662][-0.0033700035 -0.0033695218 -0.0033713621 -0.003371841 -0.0033712138 -0.0033696753 -0.0033674438 -0.0033650438 -0.0033635157 -0.0033635106 -0.0033643781 -0.0033652338 -0.003365881 -0.0033666654 -0.0033669483][-0.0033690548 -0.0033689735 -0.0033712292 -0.003372035 -0.0033718334 -0.0033702827 -0.0033679127 -0.0033653209 -0.0033636992 -0.0033637066 -0.003364726 -0.003366109 -0.0033671476 -0.0033680806 -0.0033684887][-0.0033680187 -0.0033681507 -0.0033706976 -0.0033719824 -0.00337243 -0.003371242 -0.0033692531 -0.0033669723 -0.0033660163 -0.0033663625 -0.0033674727 -0.0033689348 -0.0033700115 -0.0033706296 -0.0033707637][-0.0033679795 -0.0033678187 -0.0033705209 -0.0033721109 -0.0033730422 -0.003372441 -0.0033713635 -0.0033701917 -0.0033704184 -0.0033712971 -0.0033722527 -0.0033734585 -0.0033740196 -0.0033740515 -0.0033737952][-0.00336874 -0.0033676173 -0.0033697051 -0.0033710364 -0.0033720923 -0.0033721835 -0.0033723898 -0.0033730424 -0.0033745794 -0.0033755645 -0.0033756697 -0.0033752616 -0.0033738192 -0.0033724667 -0.0033717763][-0.0033706971 -0.003369329 -0.0033710508 -0.0033719337 -0.0033731649 -0.0033740168 -0.003374971 -0.0033767603 -0.0033787049 -0.0033787184 -0.0033772509 -0.0033749319 -0.0033718066 -0.0033694091 -0.0033684792][-0.0033725183 -0.0033714108 -0.0033727759 -0.0033737819 -0.0033751591 -0.0033764632 -0.0033781461 -0.0033806905 -0.0033828055 -0.0033823263 -0.0033802411 -0.0033771635 -0.0033733831 -0.0033710364 -0.0033702743][-0.0033737726 -0.0033726366 -0.0033735279 -0.0033742683 -0.0033753049 -0.0033765603 -0.0033786253 -0.003381609 -0.00338402 -0.0033840169 -0.0033825145 -0.0033797934 -0.0033765549 -0.0033746928 -0.003374395][-0.0033752497 -0.0033740234 -0.0033745752 -0.0033747498 -0.0033751116 -0.0033755791 -0.0033770995 -0.0033795782 -0.0033814025 -0.0033814674 -0.0033806248 -0.0033792641 -0.0033777975 -0.0033773433 -0.0033781584][-0.0033772204 -0.0033759645 -0.0033757542 -0.0033753312 -0.00337505 -0.0033746392 -0.0033752269 -0.0033764255 -0.003377388 -0.003377303 -0.0033770122 -0.0033768639 -0.003376995 -0.00337811 -0.003379809][-0.0033793326 -0.0033786562 -0.0033783466 -0.0033776953 -0.0033769058 -0.0033759307 -0.003375767 -0.0033758918 -0.0033763093 -0.0033763184 -0.003376262 -0.0033764767 -0.003376974 -0.00337822 -0.0033795317][-0.0033806451 -0.0033807803 -0.0033805429 -0.0033799326 -0.003379208 -0.0033783035 -0.0033778388 -0.0033774702 -0.0033772998 -0.0033770648 -0.0033766425 -0.0033764711 -0.0033767547 -0.0033775726 -0.0033783943][-0.0033818309 -0.003381985 -0.0033817186 -0.0033812462 -0.0033807284 -0.0033802218 -0.0033798383 -0.0033793859 -0.0033790269 -0.00337867 -0.0033781906 -0.0033776821 -0.0033772944 -0.0033772748 -0.0033774558]]...]
INFO - root - 2017-12-09 16:39:16.828047: step 41410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:22m:04s remains)
INFO - root - 2017-12-09 16:39:25.402856: step 41420, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:32m:27s remains)
INFO - root - 2017-12-09 16:39:34.122350: step 41430, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 72h:43m:41s remains)
INFO - root - 2017-12-09 16:39:42.853939: step 41440, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:35m:32s remains)
INFO - root - 2017-12-09 16:39:51.585816: step 41450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:05m:13s remains)
INFO - root - 2017-12-09 16:40:00.288571: step 41460, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 68h:42m:09s remains)
INFO - root - 2017-12-09 16:40:09.058945: step 41470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:49m:15s remains)
INFO - root - 2017-12-09 16:40:17.658438: step 41480, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:06m:37s remains)
INFO - root - 2017-12-09 16:40:26.194664: step 41490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:40m:50s remains)
INFO - root - 2017-12-09 16:40:34.850660: step 41500, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.874 sec/batch; 70h:36m:58s remains)
2017-12-09 16:40:35.735739: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00083870161 0.0010322607 0.0010965476 0.0011824211 0.0012816093 0.0013640611 0.0013347957 0.0011635348 0.00088353269 0.00056665903 0.00021021068 -0.00028970558 -0.00086729811 -0.0014474469 -0.0020107045][0.0021530502 0.0024661091 0.0025014374 0.0025731875 0.002598627 0.0024970914 0.0022233375 0.0018247892 0.0013835323 0.00091861608 0.00042351172 -0.00020462228 -0.00088464329 -0.0015722832 -0.0022013574][0.0025342111 0.002956911 0.0029505556 0.0030023381 0.0029828646 0.0027856012 0.0023495136 0.0017271065 0.0010431581 0.00040076836 -0.00016037212 -0.00077292207 -0.0014107358 -0.0020623875 -0.002620107][0.0027399836 0.0029037083 0.0027802326 0.0027622613 0.0026860847 0.0024294551 0.0019406253 0.0012442651 0.00043757446 -0.00033457018 -0.00097576552 -0.0015585339 -0.0021002095 -0.002624878 -0.00301901][0.0026602219 0.0025692494 0.0022402066 0.0020727166 0.0018806425 0.0015778271 0.0010805004 0.00040067523 -0.00039367611 -0.0011807797 -0.0018381199 -0.0023558815 -0.0027495592 -0.0030679798 -0.003265548][0.0022564866 0.001895186 0.0013936758 0.0010448107 0.00069660973 0.00035546697 -9.4173476e-05 -0.00066688331 -0.0013451939 -0.0020392793 -0.0026049984 -0.0029808457 -0.0031947503 -0.0033212528 -0.0033760255][0.0013915785 0.0010188874 0.00042258319 -7.0008449e-05 -0.00053990539 -0.00092399493 -0.001321689 -0.0017377218 -0.0022263832 -0.0027286145 -0.0031066597 -0.0033018191 -0.0033693169 -0.0033894528 -0.0033932624][0.00014287024 -0.00023306836 -0.00082405214 -0.0013284679 -0.0017645843 -0.0021139106 -0.0024144594 -0.0026520505 -0.0029128341 -0.003160882 -0.0033261525 -0.0033841589 -0.003390681 -0.0033901786 -0.003390905][-0.0012960027 -0.0016101779 -0.0020932346 -0.0024622343 -0.0027274725 -0.0029222523 -0.0030624839 -0.003166121 -0.0032647611 -0.0033356729 -0.0033789952 -0.003390477 -0.0033909953 -0.0033891404 -0.0033881878][-0.0025485312 -0.0027752167 -0.00305701 -0.0032269014 -0.003316297 -0.003361332 -0.0033776178 -0.0033696592 -0.0033584216 -0.0033559005 -0.0033690636 -0.0033834702 -0.0033908887 -0.0033890898 -0.0033869063][-0.003158818 -0.0032552588 -0.0033528414 -0.0033909662 -0.0033979565 -0.0033841964 -0.0033480127 -0.0032919368 -0.003256859 -0.0032716955 -0.0033295839 -0.0033744355 -0.0033905907 -0.0033893089 -0.0033867199][-0.0033833298 -0.0033940452 -0.0034007188 -0.0034034452 -0.0033959267 -0.0033646522 -0.0032934474 -0.0031936383 -0.0031424714 -0.0031859833 -0.0032920383 -0.0033644617 -0.0033904361 -0.0033914456 -0.0033878877][-0.0033967742 -0.0033966238 -0.0034000615 -0.0034031316 -0.0033938286 -0.0033589462 -0.003282072 -0.0031743597 -0.0031192161 -0.0031648101 -0.0032789155 -0.0033613406 -0.0033915753 -0.003393098 -0.0033889685][-0.003396854 -0.0033974901 -0.0034005295 -0.0034014247 -0.0033947183 -0.0033745959 -0.0033344575 -0.0032748042 -0.0032440524 -0.0032691627 -0.0033308882 -0.0033776823 -0.0033948724 -0.0033950186 -0.0033918247][-0.0033962938 -0.0033969411 -0.0033994764 -0.0034015481 -0.0034013996 -0.0033963185 -0.0033835424 -0.0033665989 -0.0033561124 -0.0033630156 -0.0033809682 -0.003393423 -0.0033966904 -0.0033957064 -0.003392875]]...]
INFO - root - 2017-12-09 16:40:44.278037: step 41510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:39m:49s remains)
INFO - root - 2017-12-09 16:40:52.975034: step 41520, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:52m:45s remains)
INFO - root - 2017-12-09 16:41:01.544509: step 41530, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 66h:01m:49s remains)
INFO - root - 2017-12-09 16:41:09.849499: step 41540, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 72h:38m:05s remains)
INFO - root - 2017-12-09 16:41:18.562692: step 41550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:36m:49s remains)
INFO - root - 2017-12-09 16:41:27.281573: step 41560, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 69h:56m:01s remains)
INFO - root - 2017-12-09 16:41:35.934877: step 41570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:49m:38s remains)
INFO - root - 2017-12-09 16:41:44.488357: step 41580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:59m:31s remains)
INFO - root - 2017-12-09 16:41:53.037244: step 41590, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:43m:39s remains)
INFO - root - 2017-12-09 16:42:01.595619: step 41600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:54m:34s remains)
2017-12-09 16:42:02.468092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034097175 -0.0034081258 -0.0034076932 -0.0034076308 -0.0034078879 -0.0034078737 -0.00340734 -0.003406646 -0.0034065247 -0.0034066546 -0.0034066604 -0.0034062511 -0.0034056313 -0.0034051894 -0.0034049745][-0.0034094192 -0.0034083342 -0.0034081344 -0.0034080222 -0.003407202 -0.0034041863 -0.0033995351 -0.00339635 -0.0033973556 -0.0034011018 -0.0034046846 -0.0034061724 -0.0034059044 -0.0034050567 -0.0034045507][-0.0034090804 -0.0034081894 -0.0034079552 -0.0034066208 -0.0034006711 -0.0033870121 -0.0033692569 -0.003358532 -0.0033634773 -0.0033797896 -0.0033959385 -0.0034044387 -0.0034065361 -0.0034060332 -0.0034052343][-0.0034092625 -0.0034085689 -0.00340765 -0.0034021123 -0.0033824509 -0.0033429931 -0.0032950928 -0.0032685017 -0.0032832858 -0.0033279127 -0.0033720261 -0.003396875 -0.0034051866 -0.0034062418 -0.0034056501][-0.0034100246 -0.0034095454 -0.0034072688 -0.0033936983 -0.0033498323 -0.0032674703 -0.0031712404 -0.0031197814 -0.0031507644 -0.0032407383 -0.0033305758 -0.0033832157 -0.0034024282 -0.0034061314 -0.0034060734][-0.0034116025 -0.003410738 -0.003405723 -0.0033810947 -0.0033081358 -0.0031753527 -0.0030230044 -0.0029431614 -0.0029926747 -0.0031352038 -0.0032787737 -0.0033649676 -0.0033977819 -0.0034049654 -0.0034058692][-0.003412771 -0.0034119959 -0.0034046567 -0.0033711772 -0.0032752973 -0.0031040951 -0.0029089365 -0.0028045676 -0.0028681722 -0.0030524326 -0.0032374023 -0.0033489489 -0.0033920421 -0.0034024657 -0.0034047123][-0.0034110625 -0.003411981 -0.0034046355 -0.0033685428 -0.0032679411 -0.0030902459 -0.0028881736 -0.0027791099 -0.0028451686 -0.0030359332 -0.0032270737 -0.0033426313 -0.0033880358 -0.0034003474 -0.0034037058][-0.0034051419 -0.0034100998 -0.0034055035 -0.0033753507 -0.003290718 -0.0031415087 -0.0029758872 -0.0028929613 -0.0029508476 -0.0031053089 -0.0032575722 -0.0033495196 -0.0033873492 -0.0033993463 -0.0034033319][-0.0033938538 -0.0034062769 -0.0034067293 -0.0033877671 -0.0033316172 -0.003232684 -0.0031268925 -0.003081524 -0.0031259737 -0.0032241181 -0.0033150711 -0.0033684657 -0.0033915471 -0.0034004166 -0.0034038017][-0.0033801741 -0.0034011917 -0.0034066641 -0.0033990652 -0.0033712038 -0.003322196 -0.0032719467 -0.0032543088 -0.0032801339 -0.0033271308 -0.0033665549 -0.0033881629 -0.0033977369 -0.0034023677 -0.0034046879][-0.0033716317 -0.0033976259 -0.0034061207 -0.0034055847 -0.0033967849 -0.0033803598 -0.0033641271 -0.0033592817 -0.0033684694 -0.0033830707 -0.00339441 -0.003400225 -0.0034029426 -0.003404557 -0.0034054804][-0.0033750709 -0.0033978564 -0.0034058229 -0.0034074679 -0.0034061947 -0.0034031062 -0.0034000238 -0.003399156 -0.0034007349 -0.0034027186 -0.0034042192 -0.0034051083 -0.0034055049 -0.0034056844 -0.0034056921][-0.0033865883 -0.0034004315 -0.0034055607 -0.0034068918 -0.0034071153 -0.0034070525 -0.0034068828 -0.0034068259 -0.0034066832 -0.0034062667 -0.0034059223 -0.0034057118 -0.00340553 -0.003405337 -0.0034053666][-0.0033977197 -0.0034033656 -0.0034056259 -0.0034061971 -0.0034063268 -0.0034064006 -0.0034061898 -0.0034058997 -0.0034056453 -0.00340533 -0.0034051691 -0.0034050229 -0.0034049617 -0.0034049714 -0.0034052029]]...]
INFO - root - 2017-12-09 16:42:11.167066: step 41610, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 71h:01m:41s remains)
INFO - root - 2017-12-09 16:42:19.871983: step 41620, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:59m:50s remains)
INFO - root - 2017-12-09 16:42:28.682965: step 41630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 72h:57m:02s remains)
INFO - root - 2017-12-09 16:42:37.432066: step 41640, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 71h:40m:35s remains)
INFO - root - 2017-12-09 16:42:46.053175: step 41650, loss = 0.91, batch loss = 0.70 (9.9 examples/sec; 0.810 sec/batch; 65h:24m:46s remains)
INFO - root - 2017-12-09 16:42:54.891585: step 41660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:40m:14s remains)
INFO - root - 2017-12-09 16:43:03.753613: step 41670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:55m:39s remains)
INFO - root - 2017-12-09 16:43:12.346043: step 41680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:10m:59s remains)
INFO - root - 2017-12-09 16:43:20.942098: step 41690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:14m:58s remains)
INFO - root - 2017-12-09 16:43:29.642532: step 41700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:46m:03s remains)
2017-12-09 16:43:30.475041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033970319 -0.0033951309 -0.0033947036 -0.0033943024 -0.0033942715 -0.0033949537 -0.0033956289 -0.0033961486 -0.0033963856 -0.0033963944 -0.0033962438 -0.0033957765 -0.00339527 -0.0033951034 -0.0033949264][-0.003396607 -0.003394603 -0.0033945264 -0.0033945562 -0.00339486 -0.0033956866 -0.003396706 -0.0033970082 -0.0033968051 -0.0033965579 -0.0033959493 -0.0033955504 -0.0033952012 -0.0033949895 -0.0033948219][-0.0033979085 -0.0033959737 -0.0033963448 -0.0033966282 -0.0033973332 -0.00339855 -0.0033997667 -0.0033999148 -0.0033993891 -0.0033985709 -0.0033976112 -0.0033969574 -0.0033962433 -0.0033955004 -0.0033950221][-0.0033997402 -0.0033981015 -0.0033988815 -0.0033994089 -0.0034004371 -0.0034015656 -0.0034023894 -0.0034021353 -0.0034014739 -0.0034008836 -0.0034000291 -0.0033992021 -0.0033982364 -0.0033967702 -0.0033958717][-0.0034018182 -0.0034007446 -0.0034020781 -0.0034029016 -0.003403384 -0.0034036275 -0.0034033451 -0.0034020867 -0.0034013034 -0.00340156 -0.003401353 -0.0034010825 -0.0034000552 -0.0033982121 -0.0033967893][-0.0034043218 -0.0034038597 -0.0034049111 -0.0034050096 -0.0034045465 -0.0034036383 -0.0034024112 -0.0034007602 -0.0034001444 -0.0034013451 -0.0034020771 -0.0034019086 -0.003401201 -0.0033993216 -0.0033975546][-0.0034076194 -0.0034071379 -0.0034066977 -0.0034054115 -0.003403611 -0.0034014508 -0.0033992324 -0.0033981132 -0.0033987842 -0.00340072 -0.0034023528 -0.0034026732 -0.0034021263 -0.0034002154 -0.0033979055][-0.0034099643 -0.0034085435 -0.0034062881 -0.0034032322 -0.0033996755 -0.0033967709 -0.0033945611 -0.0033943004 -0.003396366 -0.0033991928 -0.0034020494 -0.0034032988 -0.0034030559 -0.0034014171 -0.003398824][-0.003411334 -0.0034082998 -0.0034043666 -0.0033998333 -0.0033952484 -0.0033918978 -0.0033896989 -0.0033901322 -0.0033932258 -0.0033971847 -0.0034012874 -0.0034035502 -0.0034042378 -0.0034027684 -0.00339995][-0.0034126614 -0.0034086185 -0.0034035856 -0.0033978315 -0.00339282 -0.0033885457 -0.0033861294 -0.0033864505 -0.0033899271 -0.0033948061 -0.0034002566 -0.0034036774 -0.0034050858 -0.0034042113 -0.0034013102][-0.0034145636 -0.0034101328 -0.0034044962 -0.0033980815 -0.0033923674 -0.0033876062 -0.0033848993 -0.0033846747 -0.0033873068 -0.0033922498 -0.0033984985 -0.0034028718 -0.0034048858 -0.0034048276 -0.0034023062][-0.0034162127 -0.0034114965 -0.0034058231 -0.0033993062 -0.0033930812 -0.0033882405 -0.0033850241 -0.0033839077 -0.0033858586 -0.0033905394 -0.0033965958 -0.0034013523 -0.0034038753 -0.0034041882 -0.0034024962][-0.0034170651 -0.0034123261 -0.0034071717 -0.0034009609 -0.0033949902 -0.0033898556 -0.0033860803 -0.0033846502 -0.0033859299 -0.0033901818 -0.0033960172 -0.0034009507 -0.0034036958 -0.0034041682 -0.0034030369][-0.0034168698 -0.0034124078 -0.0034078255 -0.0034020762 -0.0033965954 -0.0033912787 -0.0033875306 -0.0033862111 -0.0033874733 -0.003391458 -0.0033969567 -0.0034018166 -0.0034042099 -0.00340458 -0.0034033074][-0.0034163445 -0.0034122101 -0.0034081247 -0.0034030788 -0.003398262 -0.0033932587 -0.0033897592 -0.0033883704 -0.0033895872 -0.0033936377 -0.0033988398 -0.0034033693 -0.0034055847 -0.0034056646 -0.0034039859]]...]
INFO - root - 2017-12-09 16:43:38.941648: step 41710, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 68h:38m:45s remains)
INFO - root - 2017-12-09 16:43:47.719126: step 41720, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.907 sec/batch; 73h:15m:41s remains)
INFO - root - 2017-12-09 16:43:56.340102: step 41730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:28m:47s remains)
INFO - root - 2017-12-09 16:44:05.032453: step 41740, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 69h:22m:53s remains)
INFO - root - 2017-12-09 16:44:13.783917: step 41750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:28m:22s remains)
INFO - root - 2017-12-09 16:44:22.507092: step 41760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:43m:57s remains)
INFO - root - 2017-12-09 16:44:31.165824: step 41770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:16m:11s remains)
INFO - root - 2017-12-09 16:44:39.887575: step 41780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:04m:15s remains)
INFO - root - 2017-12-09 16:44:48.535852: step 41790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:35m:15s remains)
INFO - root - 2017-12-09 16:44:57.289956: step 41800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:43m:03s remains)
2017-12-09 16:44:58.225236: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35462731 0.34655261 0.33913234 0.33375111 0.328776 0.3246195 0.31965086 0.31429568 0.30743992 0.29954612 0.29226297 0.28597915 0.27974313 0.27265295 0.26640496][0.36669424 0.36033192 0.35433912 0.35001945 0.3461206 0.34307778 0.33899719 0.33460349 0.32733762 0.31820503 0.30854949 0.29936206 0.29007238 0.27959692 0.27114272][0.37077579 0.36869064 0.36551109 0.36256731 0.35953146 0.35683969 0.35204196 0.34727338 0.33951372 0.32902092 0.31712723 0.30479923 0.29198343 0.27795592 0.26700276][0.36433896 0.3687897 0.37061968 0.37085822 0.36947611 0.36721727 0.36217961 0.3560313 0.34679514 0.33442476 0.31978029 0.3045212 0.28845817 0.27180302 0.25868112][0.35109329 0.36082789 0.36774349 0.37135124 0.37248144 0.37206969 0.36835119 0.36209133 0.35137206 0.33762273 0.32058835 0.30200312 0.28270164 0.26327205 0.24786796][0.33534649 0.35030392 0.36067882 0.36758536 0.37099034 0.37248546 0.37071908 0.36443102 0.35320732 0.3383837 0.31984881 0.29867125 0.2766346 0.25486472 0.23708013][0.32088554 0.33998039 0.35312006 0.3620173 0.36739436 0.37049815 0.36962473 0.36435094 0.35341686 0.33832517 0.31877276 0.29641625 0.27315897 0.25009188 0.2306717][0.30919984 0.33242086 0.34829056 0.35918266 0.36623016 0.37038493 0.36968374 0.36384946 0.35248974 0.3372176 0.3177869 0.29570743 0.27289972 0.25016618 0.2301937][0.30110896 0.32616416 0.34348455 0.35611743 0.36508846 0.37067577 0.37163812 0.36659887 0.35568106 0.34059349 0.32160637 0.30058542 0.27903238 0.2576265 0.23837179][0.30064762 0.3257598 0.34205279 0.35476035 0.36411235 0.37052852 0.37268814 0.36922371 0.35975951 0.34624571 0.32924828 0.3100723 0.29054239 0.27092379 0.25271279][0.30384934 0.32822588 0.34261709 0.35438266 0.36292335 0.36896923 0.37150535 0.36944064 0.36213726 0.35047075 0.33609647 0.31991684 0.30315334 0.28612095 0.26981929][0.30824205 0.33188879 0.34434739 0.35420415 0.36107907 0.36626366 0.36849371 0.3670949 0.36131543 0.3519232 0.34018794 0.3271035 0.31352198 0.29948995 0.28552884][0.30980667 0.33187464 0.34233952 0.35044119 0.35582438 0.35944846 0.36091858 0.3594186 0.35461485 0.34710142 0.33815065 0.32824817 0.3178612 0.30686435 0.29538372][0.30721197 0.32639408 0.33377776 0.33975381 0.34364647 0.34611112 0.34727341 0.34635839 0.34308672 0.3376345 0.33109167 0.32391319 0.31619871 0.30776709 0.29851294][0.2982426 0.31501991 0.31971356 0.32351884 0.32561883 0.32698953 0.32771862 0.32742441 0.32551652 0.32181585 0.31749612 0.31243229 0.30680409 0.30038908 0.29297051]]...]
INFO - root - 2017-12-09 16:45:06.936273: step 41810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:05m:55s remains)
INFO - root - 2017-12-09 16:45:15.597847: step 41820, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 71h:50m:07s remains)
INFO - root - 2017-12-09 16:45:24.298494: step 41830, loss = 0.89, batch loss = 0.69 (9.9 examples/sec; 0.804 sec/batch; 64h:57m:21s remains)
INFO - root - 2017-12-09 16:45:32.987463: step 41840, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:36m:44s remains)
INFO - root - 2017-12-09 16:45:41.620034: step 41850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:13m:25s remains)
INFO - root - 2017-12-09 16:45:50.279287: step 41860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 68h:44m:36s remains)
INFO - root - 2017-12-09 16:45:58.953222: step 41870, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:22m:04s remains)
INFO - root - 2017-12-09 16:46:07.576633: step 41880, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 71h:20m:55s remains)
INFO - root - 2017-12-09 16:46:16.043279: step 41890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:50m:21s remains)
INFO - root - 2017-12-09 16:46:24.666508: step 41900, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 72h:23m:39s remains)
2017-12-09 16:46:25.544833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00128279 -0.00091580069 -0.0004219783 3.2112002e-06 0.00011840835 -4.7824578e-05 -0.00046815048 -0.00099377078 -0.0014332279 -0.0016358552 -0.0017295436 -0.0016852273 -0.0016619407 -0.0015488601 -0.0014628037][-0.00087353471 -0.00034632627 0.00025636447 0.00086869462 0.0012647908 0.0012310355 0.00089351111 0.00031555374 -0.00032202783 -0.00075623184 -0.0010069986 -0.0010178168 -0.0010243799 -0.00091237342 -0.00073609431][-0.0004899737 0.00016713166 0.00085569895 0.0016357719 0.0022360587 0.0023485736 0.0020578171 0.0015177482 0.000842829 0.00026792428 -0.00011402625 -0.00019799219 -0.00026063598 -0.00014549773 2.9574614e-05][-4.4318382e-05 0.00075816805 0.0017113334 0.0027071715 0.0033637772 0.003553387 0.0033692939 0.0028023764 0.0020033412 0.0014793554 0.0012360921 0.0013088116 0.0012595507 0.00133803 0.0014696594][0.00029716594 0.001286512 0.0023683065 0.0035909691 0.0043815551 0.0046218792 0.00428819 0.0036090512 0.0028460154 0.002353268 0.002307764 0.002702913 0.0029736187 0.0030879101 0.0031636755][0.00037911022 0.0015515678 0.00276248 0.0041380459 0.0050948 0.0053336462 0.0048181582 0.0039359024 0.0031256338 0.0028402733 0.0030793773 0.0037875955 0.0043729665 0.0046227649 0.00479199][0.00021005096 0.001479035 0.0029655707 0.0044987379 0.0055349628 0.0056966925 0.005086503 0.0041020913 0.0033858207 0.00325701 0.0036383995 0.0045438251 0.0053526126 0.0057838736 0.0060257372][-0.0001022825 0.0011786364 0.0028543838 0.0044195438 0.0053461166 0.0053912094 0.004809943 0.004052666 0.0035366195 0.0034607633 0.0039576776 0.0049752844 0.0060296953 0.0065311277 0.0067846291][-0.00052263308 0.000705349 0.0024085662 0.0037273115 0.004435068 0.0044517387 0.0041696951 0.0036934048 0.0032927843 0.003459438 0.0040803384 0.0052210875 0.0063093286 0.0068268413 0.0070693903][-0.0010932428 1.0044314e-06 0.0013876229 0.002491704 0.0030320648 0.0030061451 0.0028834988 0.0026241045 0.0025743984 0.0030399968 0.0037775466 0.0048459247 0.0059491973 0.0065437295 0.0066964841][-0.0018678043 -0.0010586693 -5.0238334e-05 0.000780273 0.0011838984 0.0011343702 0.0011798518 0.001246779 0.0015149156 0.0022343893 0.0031007493 0.0041362038 0.0051895324 0.0057776673 0.0058103641][-0.0025444953 -0.0019570363 -0.0012768791 -0.00075850589 -0.00051746494 -0.00051062624 -0.0003047518 -5.8396952e-05 0.00037692324 0.0012121804 0.002132477 0.0031581616 0.0041174218 0.0046990328 0.0046234643][-0.002728051 -0.0023751636 -0.00196444 -0.0016465252 -0.0015478905 -0.0013743809 -0.0011217704 -0.00068347179 -0.0001112083 0.00071604387 0.0015829836 0.0024014686 0.0031692984 0.0035542364 0.003236979][-0.0026062967 -0.002262366 -0.0019137799 -0.0016654544 -0.0014496292 -0.0011270244 -0.00085698254 -0.00044641038 3.41665e-05 0.00080447621 0.0015252994 0.0021704871 0.0026504502 0.002713613 0.0022360862][-0.0024562622 -0.0021451442 -0.0017878466 -0.0014256339 -0.0011370995 -0.00072299177 -0.00026459363 0.00028729788 0.00084985024 0.0016161513 0.0023329842 0.0027578443 0.0029258055 0.0026059172 0.0019368872]]...]
INFO - root - 2017-12-09 16:46:34.087576: step 41910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:48m:16s remains)
INFO - root - 2017-12-09 16:46:42.718175: step 41920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:26m:08s remains)
INFO - root - 2017-12-09 16:46:51.385263: step 41930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 67h:17m:18s remains)
INFO - root - 2017-12-09 16:47:00.011847: step 41940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:37m:49s remains)
INFO - root - 2017-12-09 16:47:08.629234: step 41950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:23m:33s remains)
INFO - root - 2017-12-09 16:47:17.032114: step 41960, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 64h:51m:02s remains)
INFO - root - 2017-12-09 16:47:25.829818: step 41970, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:25m:47s remains)
INFO - root - 2017-12-09 16:47:34.278487: step 41980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 68h:21m:14s remains)
INFO - root - 2017-12-09 16:47:42.838896: step 41990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:08m:47s remains)
INFO - root - 2017-12-09 16:47:51.529035: step 42000, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 70h:56m:23s remains)
2017-12-09 16:47:52.425745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033643991 -0.0033618291 -0.0033618484 -0.0033621923 -0.0033625707 -0.0033629364 -0.0033631886 -0.0033633758 -0.0033634815 -0.003363454 -0.0033635041 -0.003363729 -0.0033639763 -0.0033642091 -0.0033643492][-0.0033626347 -0.0033598386 -0.0033598149 -0.0033601469 -0.0033605192 -0.0033608058 -0.0033609432 -0.0033610004 -0.0033609862 -0.0033609839 -0.0033611064 -0.0033613488 -0.0033616396 -0.0033619741 -0.0033622009][-0.0033628356 -0.0033598517 -0.0033597094 -0.0033599229 -0.0033601515 -0.0033602833 -0.0033602752 -0.0033602188 -0.0033601127 -0.0033600731 -0.0033602149 -0.00336053 -0.0033609301 -0.0033613662 -0.0033616843][-0.0033630282 -0.003359928 -0.0033596368 -0.0033597122 -0.0033598088 -0.0033598307 -0.0033597476 -0.0033596212 -0.0033594233 -0.0033593019 -0.0033594104 -0.0033597446 -0.0033601923 -0.0033607029 -0.0033611185][-0.0033631511 -0.0033599418 -0.0033595371 -0.0033594857 -0.0033594777 -0.0033594302 -0.00335931 -0.0033591397 -0.0033588482 -0.0033586344 -0.0033586733 -0.0033589907 -0.0033594605 -0.0033600237 -0.0033605185][-0.0033631898 -0.003359885 -0.0033594277 -0.0033593171 -0.0033592735 -0.0033592407 -0.0033591515 -0.0033589823 -0.0033586198 -0.0033583005 -0.0033582265 -0.003358434 -0.0033588347 -0.0033593958 -0.0033599327][-0.0033631 -0.0033598146 -0.0033594002 -0.0033593383 -0.0033593604 -0.0033594486 -0.003359464 -0.0033593581 -0.0033589632 -0.0033585303 -0.0033582875 -0.0033582926 -0.00335853 -0.0033590032 -0.0033595][-0.0033633842 -0.0033602719 -0.0033599804 -0.0033600207 -0.0033601781 -0.0033604044 -0.003360535 -0.0033604836 -0.0033600808 -0.0033595511 -0.0033591036 -0.0033588652 -0.0033588775 -0.0033591525 -0.0033594936][-0.0033641851 -0.0033613164 -0.0033612535 -0.0033614379 -0.0033617534 -0.0033621197 -0.0033623753 -0.0033623907 -0.0033620184 -0.0033614188 -0.0033607744 -0.0033602803 -0.0033600151 -0.0033600121 -0.0033601206][-0.0033651257 -0.0033624009 -0.0033625525 -0.0033628207 -0.003363204 -0.0033636449 -0.0033639506 -0.0033640012 -0.0033636834 -0.00336307 -0.0033623492 -0.0033617008 -0.0033612396 -0.0033610335 -0.0033609644][-0.0033660387 -0.0033634219 -0.0033637215 -0.0033639974 -0.0033643672 -0.0033647812 -0.0033650564 -0.0033650964 -0.0033648356 -0.0033642822 -0.0033635932 -0.0033629409 -0.0033624433 -0.0033621548 -0.0033619832][-0.0033668561 -0.0033643239 -0.0033646405 -0.0033648696 -0.0033651593 -0.003365468 -0.0033656578 -0.0033656603 -0.0033654487 -0.0033650047 -0.0033644249 -0.0033638764 -0.0033634498 -0.0033631658 -0.0033629672][-0.003367628 -0.003364976 -0.0033652794 -0.0033654207 -0.0033655972 -0.0033657863 -0.0033659025 -0.0033658829 -0.003365723 -0.0033654138 -0.003365 -0.0033646123 -0.0033642966 -0.0033640715 -0.0033639194][-0.0033677416 -0.0033649458 -0.003365197 -0.0033652633 -0.0033653362 -0.0033654342 -0.0033654887 -0.0033654633 -0.0033653595 -0.003365183 -0.0033649497 -0.0033647248 -0.003364532 -0.0033644021 -0.0033643311][-0.003367516 -0.0033645916 -0.0033647346 -0.0033647434 -0.0033647446 -0.0033647649 -0.0033647621 -0.0033647236 -0.0033646608 -0.0033645893 -0.0033644882 -0.0033644033 -0.0033643483 -0.0033643418 -0.0033643658]]...]
INFO - root - 2017-12-09 16:48:01.088353: step 42010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:50m:15s remains)
INFO - root - 2017-12-09 16:48:09.493303: step 42020, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:49m:08s remains)
INFO - root - 2017-12-09 16:48:17.956321: step 42030, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 68h:58m:05s remains)
INFO - root - 2017-12-09 16:48:26.474136: step 42040, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 69h:51m:54s remains)
INFO - root - 2017-12-09 16:48:35.129471: step 42050, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 68h:30m:06s remains)
INFO - root - 2017-12-09 16:48:43.718628: step 42060, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 67h:52m:34s remains)
INFO - root - 2017-12-09 16:48:52.266603: step 42070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:54m:39s remains)
INFO - root - 2017-12-09 16:49:00.878236: step 42080, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 62h:05m:04s remains)
INFO - root - 2017-12-09 16:49:09.209969: step 42090, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 67h:58m:56s remains)
INFO - root - 2017-12-09 16:49:17.775948: step 42100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:43m:54s remains)
2017-12-09 16:49:18.632629: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39846244 0.37969458 0.36766052 0.36327469 0.3764345 0.40800989 0.45339772 0.51046652 0.56671357 0.61787468 0.65794843 0.68483794 0.69953483 0.70397007 0.70782185][0.40344357 0.40376908 0.40613148 0.41255826 0.43143904 0.4641777 0.507953 0.55808514 0.60748285 0.651909 0.68580717 0.706147 0.71658427 0.72155166 0.72590768][0.41391346 0.43752271 0.4584485 0.47557697 0.4997448 0.53143144 0.56983382 0.60850716 0.64582652 0.67862004 0.70194477 0.71308112 0.716177 0.71898264 0.72259343][0.44088295 0.48819807 0.52680707 0.55378753 0.58153129 0.61148649 0.6427626 0.66832197 0.692721 0.71213394 0.72311604 0.72361368 0.7190035 0.7170496 0.71828085][0.476644 0.54160345 0.59349561 0.62893474 0.65850359 0.68453985 0.70881 0.72416317 0.73761427 0.7442525 0.74468082 0.73587078 0.72403008 0.7167241 0.71383226][0.509111 0.58464664 0.6412456 0.67950177 0.70684206 0.72852957 0.74729967 0.7552529 0.76141214 0.75946534 0.75339276 0.738803 0.72347432 0.71393991 0.70964885][0.52723366 0.60177827 0.65446365 0.689508 0.71158272 0.72778803 0.74004269 0.74427605 0.74744362 0.74244857 0.73419136 0.7181446 0.70412886 0.69565684 0.69368958][0.52272856 0.58874166 0.63185287 0.65853059 0.67351437 0.683622 0.69048887 0.69217193 0.69337481 0.68925518 0.68390071 0.67284465 0.6655581 0.66237408 0.6651293][0.48972756 0.54003227 0.567396 0.58214849 0.58902156 0.59428382 0.59768 0.59927261 0.60235417 0.60178691 0.60213482 0.59974766 0.60188532 0.6071701 0.61707228][0.43870997 0.46535408 0.47059402 0.47084713 0.46875325 0.4680939 0.47052571 0.47416183 0.48064938 0.48604605 0.49421042 0.50205761 0.51330256 0.52840471 0.54665083][0.38232467 0.3850247 0.36891031 0.352545 0.3395358 0.33281904 0.33261392 0.33668178 0.34504768 0.35609883 0.37119174 0.38830104 0.40862006 0.43213505 0.45736489][0.32878333 0.3117694 0.27921888 0.25061536 0.23018754 0.21918155 0.21694575 0.22020747 0.22768934 0.2391953 0.25528479 0.2758795 0.29951981 0.32637891 0.35429794][0.27980995 0.24830136 0.20488258 0.16911775 0.14471567 0.13176151 0.12841135 0.13087015 0.1371253 0.1468261 0.16081876 0.17896043 0.19990443 0.22399181 0.249086][0.22599819 0.18654191 0.14000437 0.1028192 0.078211859 0.065271735 0.061400626 0.063594088 0.068234771 0.075754568 0.086723581 0.10092227 0.117331 0.13589552 0.15523708][0.17084523 0.13126822 0.087869927 0.055050496 0.03383651 0.023292337 0.020149758 0.021777656 0.024812546 0.029596154 0.036750659 0.045755614 0.056580782 0.069156349 0.08237499]]...]
INFO - root - 2017-12-09 16:49:27.394801: step 42110, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 71h:18m:59s remains)
INFO - root - 2017-12-09 16:49:36.006197: step 42120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:43m:00s remains)
INFO - root - 2017-12-09 16:49:44.755127: step 42130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:45m:07s remains)
INFO - root - 2017-12-09 16:49:53.365898: step 42140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:00m:09s remains)
INFO - root - 2017-12-09 16:50:02.079375: step 42150, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 68h:58m:04s remains)
INFO - root - 2017-12-09 16:50:10.732526: step 42160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:38m:23s remains)
INFO - root - 2017-12-09 16:50:19.380227: step 42170, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 67h:55m:49s remains)
INFO - root - 2017-12-09 16:50:28.074848: step 42180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:15m:43s remains)
INFO - root - 2017-12-09 16:50:36.455368: step 42190, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:19m:22s remains)
INFO - root - 2017-12-09 16:50:45.016692: step 42200, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 66h:37m:30s remains)
2017-12-09 16:50:45.876382: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36073312 0.36018604 0.35495642 0.34541121 0.33253586 0.31887242 0.30459183 0.29041469 0.2774162 0.26496559 0.25296149 0.24008657 0.22731341 0.21278761 0.19779174][0.37968311 0.38610828 0.38623074 0.38158605 0.37259606 0.36150715 0.34925392 0.33614358 0.3235805 0.31093457 0.29766378 0.28254092 0.26561391 0.24624868 0.22535881][0.39346761 0.40700698 0.41337946 0.41408995 0.40915573 0.40161762 0.3930307 0.38284937 0.3721109 0.35987976 0.34538946 0.32808089 0.30683231 0.28160915 0.25352052][0.40760446 0.42826918 0.44075325 0.44610271 0.44527596 0.44191232 0.43691975 0.42986423 0.42111152 0.41001815 0.39516887 0.3758643 0.35085005 0.31978786 0.2845566][0.41834703 0.44493672 0.46284249 0.47238091 0.47576368 0.47554427 0.47378671 0.46961933 0.46275997 0.45347103 0.4390516 0.41946691 0.39187551 0.35643312 0.31494945][0.42176792 0.45353207 0.47532663 0.48862436 0.49525794 0.49874058 0.50094384 0.49971065 0.494903 0.48694628 0.47301012 0.45264828 0.42293411 0.384209 0.3383401][0.4186866 0.45351493 0.47752076 0.49337211 0.50277865 0.50921416 0.51449472 0.5161972 0.513889 0.50750041 0.49482942 0.47491586 0.44446167 0.40371615 0.35476598][0.41016141 0.4465805 0.47198573 0.48913887 0.49982259 0.50811654 0.51497281 0.51871878 0.5180788 0.51337379 0.50216669 0.48267588 0.4527176 0.41121811 0.36100933][0.3912175 0.42800513 0.45334628 0.47079641 0.48272902 0.49254397 0.50057364 0.505926 0.50725681 0.50435221 0.49457756 0.47618473 0.44718227 0.40643057 0.35704136][0.363329 0.39826351 0.42211089 0.43979156 0.45301688 0.46383303 0.47295606 0.47984394 0.48272488 0.4809458 0.47224709 0.45539761 0.42801595 0.38955736 0.34304458][0.32902563 0.36018226 0.38114214 0.39703974 0.40992945 0.42144266 0.43200722 0.44057027 0.44556603 0.4454762 0.43852973 0.42318118 0.39804098 0.36298451 0.3208648][0.29383275 0.3203426 0.33760652 0.35104683 0.36269116 0.37248397 0.38201448 0.39108166 0.39753863 0.39921412 0.39417464 0.38197613 0.36079943 0.33040327 0.29396334][0.25908154 0.28035817 0.29334015 0.30367133 0.3131687 0.32076454 0.32845911 0.3365356 0.34285176 0.345408 0.34227222 0.33302486 0.31625715 0.29201597 0.2630609][0.22669388 0.24332377 0.25237572 0.25962153 0.26653755 0.27166834 0.27735603 0.2838608 0.28925484 0.29179713 0.2900514 0.28397158 0.27187166 0.25424245 0.2331534][0.20134869 0.21410385 0.21991573 0.22493954 0.22968028 0.23261003 0.23593819 0.240241 0.24396029 0.2455225 0.24447727 0.24076657 0.23293377 0.22124146 0.20722657]]...]
INFO - root - 2017-12-09 16:50:54.452414: step 42210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:30m:04s remains)
INFO - root - 2017-12-09 16:51:03.167535: step 42220, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 70h:33m:09s remains)
INFO - root - 2017-12-09 16:51:11.884201: step 42230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:50m:59s remains)
INFO - root - 2017-12-09 16:51:20.471764: step 42240, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 70h:21m:58s remains)
INFO - root - 2017-12-09 16:51:29.220028: step 42250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:21m:34s remains)
INFO - root - 2017-12-09 16:51:37.862219: step 42260, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 70h:47m:02s remains)
INFO - root - 2017-12-09 16:51:46.421792: step 42270, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 72h:39m:10s remains)
INFO - root - 2017-12-09 16:51:54.955658: step 42280, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 67h:00m:11s remains)
INFO - root - 2017-12-09 16:52:03.307363: step 42290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 68h:23m:45s remains)
INFO - root - 2017-12-09 16:52:11.884754: step 42300, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:30m:17s remains)
2017-12-09 16:52:12.762942: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028149511 0.0031352083 0.0031507893 0.0026683325 0.0020304259 0.0012219029 0.00042005768 -0.00041751913 -0.0013008327 -0.0020989038 -0.0026768162 -0.0030382085 -0.0032503633 -0.0033529042 -0.0033922342][0.005276585 0.0052127652 0.0048204362 0.0040268106 0.0032664363 0.0024426186 0.0016280625 0.00065086619 -0.00044563203 -0.0014356801 -0.002234254 -0.0027791669 -0.003132469 -0.0033177878 -0.003387399][0.011375545 0.010824787 0.0097557325 0.0084022926 0.0072945133 0.0061542615 0.0050661061 0.0036161148 0.0018735875 0.00025075907 -0.0011311397 -0.0021049646 -0.0028123786 -0.0032074263 -0.0033652026][0.022633068 0.021692852 0.019815715 0.01770509 0.016039724 0.014371424 0.012676393 0.010217804 0.0071744667 0.0041937456 0.0014939166 -0.00052590249 -0.0020588334 -0.0029347092 -0.0033090459][0.03887767 0.037781149 0.035168085 0.032132719 0.029700838 0.027250137 0.024589766 0.020619329 0.01562858 0.01057845 0.0058181174 0.0021180019 -0.00075180782 -0.0024471735 -0.003206569][0.060199127 0.059528455 0.056480676 0.052584779 0.04906258 0.045077696 0.040556803 0.034163155 0.026376992 0.018474743 0.011050745 0.005245816 0.00077476981 -0.0018710358 -0.0030819131][0.083285265 0.083939008 0.081068181 0.076499417 0.071611434 0.065556049 0.058419697 0.048898529 0.03776462 0.026618224 0.016342122 0.00832428 0.0022421286 -0.001346159 -0.0029737093][0.10148286 0.10359691 0.10096861 0.095653214 0.089107946 0.080660582 0.070778333 0.058458224 0.044692557 0.031234588 0.019170497 0.0098615251 0.0029408971 -0.0011111947 -0.0029230444][0.10951088 0.11232446 0.10938874 0.10299396 0.094560079 0.084035195 0.0721123 0.058265012 0.043642178 0.029895667 0.018031556 0.0090082306 0.0024740298 -0.0013125078 -0.0029668142][0.10588293 0.10814268 0.10416359 0.096553423 0.086556256 0.074716642 0.061971404 0.048394214 0.034956772 0.022972843 0.013158689 0.0059558721 0.00094193243 -0.0019001008 -0.0030859397][0.092044644 0.092746839 0.087394 0.079002805 0.068596743 0.057076111 0.045352884 0.033769682 0.023077125 0.014083077 0.0071670841 0.0023224449 -0.0008765813 -0.002586321 -0.0032307322][0.071292512 0.07005211 0.063667469 0.055382226 0.045972086 0.036383614 0.02722059 0.018880472 0.011745812 0.0061133616 0.0020843067 -0.000594903 -0.0022423347 -0.0030601961 -0.003323578][0.048424743 0.045644522 0.039059717 0.031801343 0.024466526 0.017726758 0.011823492 0.006989602 0.0032620754 0.00054736412 -0.0012471343 -0.0023669202 -0.0029927513 -0.0032802364 -0.0033598654][0.027945153 0.024691982 0.019132683 0.01377429 0.009028336 0.00513974 0.0021123646 -2.6945723e-05 -0.0014308679 -0.0023292354 -0.0028691152 -0.0031740929 -0.0033175047 -0.0033703672 -0.0033802874][0.012482614 0.0099067846 0.0062413868 0.0031262159 0.00073126773 -0.00097932783 -0.0021178757 -0.002759947 -0.0030804763 -0.0032415658 -0.0033256412 -0.00336803 -0.0033830772 -0.0033874458 -0.0033890221]]...]
INFO - root - 2017-12-09 16:52:21.391545: step 42310, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 68h:46m:02s remains)
INFO - root - 2017-12-09 16:52:29.922233: step 42320, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 67h:41m:36s remains)
INFO - root - 2017-12-09 16:52:38.293381: step 42330, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:32m:01s remains)
INFO - root - 2017-12-09 16:52:46.850491: step 42340, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.909 sec/batch; 73h:16m:58s remains)
INFO - root - 2017-12-09 16:52:55.549615: step 42350, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 67h:08m:07s remains)
INFO - root - 2017-12-09 16:53:04.115326: step 42360, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 70h:56m:19s remains)
INFO - root - 2017-12-09 16:53:12.778819: step 42370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 71h:08m:49s remains)
INFO - root - 2017-12-09 16:53:21.446153: step 42380, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 69h:42m:50s remains)
INFO - root - 2017-12-09 16:53:29.869164: step 42390, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 72h:57m:53s remains)
INFO - root - 2017-12-09 16:53:38.606150: step 42400, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 71h:42m:36s remains)
2017-12-09 16:53:39.487069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026778765 -0.0017520372 0.00094944215 0.0058224816 0.012174706 0.018370418 0.022747295 0.024852116 0.02420591 0.020815644 0.015574777 0.0096888607 0.0050244238 0.0017167276 -0.00047594984][6.19553e-05 0.0019696828 0.0062869955 0.01346803 0.022836033 0.032339904 0.039653983 0.043517418 0.042825386 0.037385855 0.028732633 0.019047527 0.010635695 0.0042557996 0.00014170073][0.0076763555 0.012901005 0.021518914 0.033902213 0.049358968 0.065596513 0.079072185 0.08727257 0.08788918 0.079949647 0.065132521 0.046932522 0.029472863 0.014978112 0.0050226348][0.020775402 0.033358634 0.050395478 0.071985446 0.0972551 0.12341137 0.14536227 0.15888083 0.16091101 0.1494866 0.1259636 0.095131271 0.063571922 0.036205329 0.016079787][0.035823442 0.058012035 0.086490832 0.1200429 0.15720268 0.19531447 0.22769871 0.24767449 0.25079289 0.23491713 0.2012101 0.15531152 0.10632955 0.0629624 0.030573478][0.050842553 0.082800359 0.12309322 0.16958804 0.21988569 0.27070382 0.31421268 0.34172174 0.34700516 0.32723618 0.28339055 0.22215492 0.15476465 0.09378662 0.047427952][0.063354194 0.1029648 0.15262713 0.20948856 0.27046153 0.33165869 0.38402921 0.41754612 0.42485341 0.40278506 0.35169497 0.27850884 0.19617118 0.12071279 0.062484808][0.069443211 0.11285895 0.16725181 0.22907655 0.294767 0.36024261 0.4159508 0.45161334 0.45961338 0.4369154 0.38338912 0.30559263 0.21717656 0.13521382 0.070994049][0.066380039 0.10843948 0.16104889 0.22053879 0.28344682 0.3457936 0.39814475 0.43115747 0.43809161 0.4166176 0.36645547 0.29308584 0.20912473 0.13101906 0.069430552][0.054897789 0.09030652 0.13510907 0.18584345 0.23921144 0.29183117 0.33559316 0.362846 0.36783528 0.34943515 0.30742288 0.2461267 0.17576943 0.11009729 0.058263283][0.040384058 0.066385508 0.099478744 0.13744831 0.17775972 0.21739694 0.24977666 0.26969066 0.27295092 0.2587994 0.22724676 0.18163989 0.12946732 0.080691159 0.042076118][0.025818195 0.04266645 0.06417001 0.088883534 0.11525589 0.14129421 0.16231863 0.17493346 0.17650741 0.16680396 0.14583232 0.11587514 0.081977829 0.050377641 0.025332607][0.012926928 0.022771446 0.0350629 0.048906062 0.063682251 0.0783246 0.089931473 0.096333034 0.096278518 0.089975253 0.077772312 0.060848951 0.042063467 0.0247226 0.011177266][0.0034187345 0.008254678 0.014203611 0.020764915 0.027636725 0.03426142 0.039307747 0.041691016 0.040942706 0.0374106 0.031437952 0.023599535 0.01521368 0.0076570176 0.0020176896][-0.0016332454 4.7965907e-05 0.0022313839 0.0047250632 0.0072971387 0.0096780621 0.011355169 0.011891013 0.011213671 0.0096103558 0.0073204767 0.0045939041 0.0018337334 -0.00052470714 -0.0021288064]]...]
INFO - root - 2017-12-09 16:53:48.264968: step 42410, loss = 0.89, batch loss = 0.68 (8.3 examples/sec; 0.962 sec/batch; 77h:33m:18s remains)
INFO - root - 2017-12-09 16:53:56.920147: step 42420, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 69h:08m:39s remains)
INFO - root - 2017-12-09 16:54:05.653059: step 42430, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:18m:45s remains)
INFO - root - 2017-12-09 16:54:14.105754: step 42440, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 66h:39m:11s remains)
INFO - root - 2017-12-09 16:54:22.790948: step 42450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:50m:01s remains)
INFO - root - 2017-12-09 16:54:31.211917: step 42460, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 67h:28m:36s remains)
INFO - root - 2017-12-09 16:54:39.627213: step 42470, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:48m:57s remains)
INFO - root - 2017-12-09 16:54:48.233387: step 42480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:08m:53s remains)
INFO - root - 2017-12-09 16:54:56.554099: step 42490, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 68h:39m:28s remains)
INFO - root - 2017-12-09 16:55:05.129779: step 42500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:44m:06s remains)
2017-12-09 16:55:06.112944: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15247303 0.15836564 0.1580579 0.15410013 0.1482587 0.14112037 0.1337495 0.12537323 0.11810073 0.11350964 0.11173877 0.11384653 0.11861512 0.12562959 0.13390018][0.15814108 0.16433936 0.16368848 0.15824963 0.1503389 0.13996272 0.12914567 0.11755652 0.10805352 0.10206249 0.099680878 0.10189246 0.10713974 0.11601866 0.1264383][0.15165828 0.15826377 0.15761623 0.15108116 0.14085518 0.12785703 0.11390293 0.0993864 0.0881744 0.081017375 0.077688143 0.079117596 0.084119566 0.093700983 0.10518196][0.14243925 0.14839907 0.14678495 0.1388922 0.12658903 0.11066414 0.094213605 0.078376532 0.066320509 0.058278862 0.054409649 0.05517175 0.05978201 0.0691298 0.080807738][0.13578674 0.14010707 0.13650341 0.12640631 0.11190052 0.094028972 0.075859673 0.058946069 0.046089992 0.037822146 0.033454616 0.033235718 0.037054844 0.045613721 0.05683437][0.13267541 0.13516118 0.12940982 0.11687799 0.10034171 0.080996826 0.06178413 0.044278935 0.030745966 0.021495981 0.016199557 0.015161064 0.017813135 0.024834815 0.034796026][0.13036701 0.13083223 0.12314356 0.10903759 0.091731809 0.072260857 0.053177 0.035845667 0.022175647 0.012587278 0.0065303696 0.0039904062 0.00479592 0.0094126007 0.017056119][0.12474195 0.12367173 0.11436672 0.099558078 0.08265686 0.064473808 0.046785709 0.030726654 0.01790314 0.0087667471 0.0026624871 -0.00051442324 -0.0011657639 0.0008433077 0.0051642768][0.11167218 0.10996417 0.10069041 0.086831369 0.071560875 0.055982821 0.040620714 0.026610235 0.015234762 0.0071849125 0.0018430061 -0.0012562084 -0.0025024242 -0.00220731 -0.00062160497][0.090782136 0.089116365 0.081192262 0.069837295 0.057608027 0.045236591 0.032931019 0.021578008 0.012205434 0.0055514723 0.0011292461 -0.0014894372 -0.0027569372 -0.0031005116 -0.0028052521][0.06638547 0.065345027 0.059662085 0.051485952 0.042624019 0.033518344 0.024262363 0.015585127 0.0085238023 0.0034782097 8.6943619e-05 -0.0019456864 -0.0029372959 -0.003262375 -0.0032928297][0.04253529 0.041945856 0.038254086 0.032972798 0.02712073 0.021065071 0.014850629 0.0090697948 0.0044544889 0.0011499443 -0.0010808429 -0.0024384346 -0.0030916329 -0.0032961275 -0.0033196565][0.022627421 0.022247422 0.020089224 0.01690984 0.0133284 0.0096561592 0.0060421815 0.0028619377 0.00042978674 -0.0012529125 -0.0023502784 -0.0029823896 -0.0032589852 -0.0033297306 -0.0033316908][0.0072710197 0.0070490921 0.0060515311 0.004585675 0.0029377092 0.0013222753 -0.00014411961 -0.0013367117 -0.0021912665 -0.0027521143 -0.0030886969 -0.0032639508 -0.0033326757 -0.0033454117 -0.0033381407][-0.00079297484 -0.00086704618 -0.0011581781 -0.0015826836 -0.0020501262 -0.0024703783 -0.0028123443 -0.003057783 -0.0032123267 -0.0032990563 -0.0033383893 -0.0033528463 -0.0033561836 -0.0033522565 -0.0033427344]]...]
INFO - root - 2017-12-09 16:55:14.576553: step 42510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 66h:57m:44s remains)
INFO - root - 2017-12-09 16:55:23.226876: step 42520, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 68h:48m:47s remains)
INFO - root - 2017-12-09 16:55:32.046110: step 42530, loss = 0.89, batch loss = 0.68 (8.1 examples/sec; 0.986 sec/batch; 79h:25m:01s remains)
INFO - root - 2017-12-09 16:55:40.691347: step 42540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:37m:22s remains)
INFO - root - 2017-12-09 16:55:49.354259: step 42550, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 71h:12m:02s remains)
INFO - root - 2017-12-09 16:55:58.001357: step 42560, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 72h:19m:39s remains)
INFO - root - 2017-12-09 16:56:06.620044: step 42570, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 69h:34m:47s remains)
INFO - root - 2017-12-09 16:56:15.225293: step 42580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:55m:00s remains)
INFO - root - 2017-12-09 16:56:23.509625: step 42590, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:20m:04s remains)
INFO - root - 2017-12-09 16:56:32.220530: step 42600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:48m:38s remains)
2017-12-09 16:56:33.109224: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052333716 0.048229124 0.042524092 0.035621133 0.029750714 0.02566864 0.024542062 0.025336007 0.025100408 0.022597931 0.01762441 0.011149164 0.004691341 -2.7594855e-05 -0.0024774061][0.087350681 0.081039146 0.070593789 0.058524489 0.048355345 0.041960776 0.04067605 0.042560615 0.043092903 0.039800368 0.032049872 0.021649631 0.010982048 0.0028456475 -0.0016294179][0.13354167 0.12581158 0.11162036 0.094675668 0.080197029 0.071285248 0.069149584 0.070590951 0.069436975 0.062598839 0.0496472 0.033767495 0.017874077 0.0059381826 -0.00072093238][0.18512179 0.17861083 0.16210461 0.14194411 0.12473198 0.11339523 0.1089077 0.1070248 0.10089267 0.087588288 0.067480318 0.045007136 0.023768419 0.0084138941 -9.0282876e-05][0.23656723 0.23266971 0.21688364 0.19642919 0.17822455 0.16509902 0.15691964 0.14845823 0.1334912 0.1109938 0.082116857 0.052842148 0.027110929 0.0093637118 -6.7332294e-06][0.28304252 0.28248954 0.26941207 0.25192165 0.23519918 0.22041193 0.20694451 0.18957111 0.16348986 0.12991983 0.091465935 0.055785898 0.026858017 0.0084835505 -0.0004658855][0.31755272 0.32152352 0.31175032 0.29772595 0.28341708 0.26815972 0.24951799 0.22245288 0.18491307 0.14062282 0.093879253 0.053520512 0.023516199 0.0062684137 -0.0012257809][0.33736336 0.34576866 0.33950317 0.32860136 0.31566337 0.29911491 0.27546787 0.24028906 0.19327834 0.14062697 0.088698439 0.046685509 0.018162273 0.0034794693 -0.0020132794][0.33909881 0.35312515 0.35124221 0.34417081 0.33274075 0.31421745 0.28479657 0.24204423 0.18794486 0.13061704 0.077452242 0.0371735 0.012207676 0.00086104567 -0.002651074][0.32799023 0.34554079 0.3467719 0.3430346 0.33319521 0.31330723 0.27946758 0.23102367 0.17239824 0.11358375 0.062599406 0.026822384 0.00677232 -0.0011149792 -0.003053677][0.30841964 0.3270593 0.32951227 0.32745102 0.31818262 0.29688913 0.26004893 0.2087795 0.14942537 0.092851281 0.046996552 0.017461041 0.0026017379 -0.0023364834 -0.0032359816][0.28146574 0.29870805 0.3007313 0.2985855 0.288499 0.26575792 0.22770654 0.17714036 0.12139004 0.070916846 0.032587942 0.0099707525 -0.00016386062 -0.0029465985 -0.0032936444][0.24692516 0.26094231 0.26118121 0.25687912 0.24442625 0.22028607 0.18336508 0.13762496 0.089960314 0.049152102 0.020082356 0.00441669 -0.001823315 -0.0031952939 -0.003307713][0.20538343 0.21618322 0.21487205 0.20813209 0.19335508 0.16873668 0.13502108 0.096912012 0.059929952 0.030232826 0.010447525 0.00074271369 -0.0026960683 -0.0032810941 -0.0033182492][0.15840706 0.16526979 0.16255853 0.1545736 0.13970941 0.11758957 0.090082377 0.061448358 0.035582297 0.016138524 0.0040209536 -0.001394978 -0.0030963216 -0.0033098836 -0.0033286673]]...]
INFO - root - 2017-12-09 16:56:41.777129: step 42610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:32m:58s remains)
INFO - root - 2017-12-09 16:56:50.682820: step 42620, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:57m:07s remains)
INFO - root - 2017-12-09 16:56:59.437979: step 42630, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 77h:24m:48s remains)
INFO - root - 2017-12-09 16:57:08.218695: step 42640, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:26m:56s remains)
INFO - root - 2017-12-09 16:57:16.817898: step 42650, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 68h:34m:40s remains)
INFO - root - 2017-12-09 16:57:25.587907: step 42660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:06m:23s remains)
INFO - root - 2017-12-09 16:57:34.301379: step 42670, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:35m:58s remains)
INFO - root - 2017-12-09 16:57:43.009439: step 42680, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 70h:40m:58s remains)
INFO - root - 2017-12-09 16:57:51.441655: step 42690, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:51m:27s remains)
INFO - root - 2017-12-09 16:58:00.112206: step 42700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:53m:54s remains)
2017-12-09 16:58:00.972369: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030206857 0.028068447 0.027997484 0.029489847 0.032743052 0.037739929 0.043865114 0.050069693 0.054687336 0.056768216 0.056053091 0.05258951 0.046511121 0.038094733 0.02815379][0.046596598 0.047916193 0.051017322 0.05531545 0.06076904 0.067404717 0.074506328 0.081201419 0.085719429 0.087161131 0.085395053 0.080432162 0.072133139 0.060378689 0.045799606][0.069946848 0.077492304 0.085770018 0.094119154 0.10243173 0.11070556 0.11842351 0.12529634 0.12923007 0.12913182 0.12464926 0.11596503 0.10337982 0.0868256 0.066871][0.098058946 0.1132734 0.12761083 0.13987529 0.15006115 0.15833932 0.16496225 0.17064331 0.17350534 0.17234029 0.16619909 0.15493302 0.1387348 0.11752186 0.091859959][0.12700596 0.14820062 0.16701293 0.1823923 0.19373696 0.20109636 0.20598893 0.20996375 0.2115515 0.20953308 0.20242052 0.18957482 0.17104858 0.1465982 0.11661717][0.15105818 0.17547393 0.19597442 0.21198671 0.22336459 0.23089455 0.23539481 0.23835872 0.23940694 0.23728943 0.22991952 0.21628839 0.19630337 0.16965137 0.13665988][0.1667206 0.19110912 0.21072529 0.22603381 0.23729672 0.24473079 0.24948345 0.25301775 0.25434831 0.25241628 0.2451499 0.23162796 0.21134865 0.18388231 0.14947988][0.17016281 0.19261169 0.20955172 0.22299698 0.23357049 0.24125524 0.24674363 0.25056389 0.25197002 0.25036573 0.24362661 0.23096451 0.21142317 0.18462315 0.15085794][0.15974157 0.17972906 0.19362761 0.20470499 0.21384628 0.22111091 0.226842 0.23087269 0.23176567 0.22952478 0.22268042 0.21088061 0.19303107 0.16874027 0.13798635][0.13573036 0.15251127 0.16363299 0.17260389 0.1803629 0.1869901 0.1925309 0.19622581 0.19659078 0.19375943 0.18650289 0.17516971 0.15888192 0.1375832 0.1112711][0.10245294 0.115356 0.12409192 0.13145448 0.13799077 0.14364241 0.14827211 0.15106988 0.15101735 0.14810213 0.14145464 0.1313625 0.1173903 0.099775471 0.078754783][0.066655509 0.075563669 0.0821992 0.08816617 0.093557045 0.0979869 0.10120813 0.10265031 0.10196891 0.099219933 0.09411151 0.086663812 0.076460704 0.063596889 0.04859722][0.034948129 0.040437482 0.045115598 0.049530406 0.053498067 0.056585807 0.058492504 0.058950827 0.05800572 0.055818126 0.052443281 0.047836177 0.041703451 0.03395956 0.024967931][0.012455622 0.015248494 0.017932743 0.020560218 0.022948937 0.024788855 0.025904836 0.026175378 0.025646828 0.024439881 0.022640038 0.020306697 0.017262124 0.013497038 0.00919885][0.00072771218 0.0018147114 0.0029167733 0.0040163198 0.0050680549 0.005942103 0.0065679778 0.0068279332 0.0066899559 0.0061561693 0.0052932133 0.0042434311 0.0029718317 0.0015707049 0.00011994084]]...]
INFO - root - 2017-12-09 16:58:09.446668: step 42710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:38m:49s remains)
INFO - root - 2017-12-09 16:58:18.054762: step 42720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:49m:37s remains)
INFO - root - 2017-12-09 16:58:26.700438: step 42730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:57m:51s remains)
INFO - root - 2017-12-09 16:58:35.212238: step 42740, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 68h:23m:25s remains)
INFO - root - 2017-12-09 16:58:43.918519: step 42750, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 67h:09m:28s remains)
INFO - root - 2017-12-09 16:58:52.585072: step 42760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:18m:06s remains)
INFO - root - 2017-12-09 16:59:01.207622: step 42770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:53m:46s remains)
INFO - root - 2017-12-09 16:59:09.904072: step 42780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:49m:45s remains)
INFO - root - 2017-12-09 16:59:18.242132: step 42790, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 59h:01m:21s remains)
INFO - root - 2017-12-09 16:59:26.888486: step 42800, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:12m:10s remains)
2017-12-09 16:59:27.830153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003381704 -0.0033790211 -0.0033779773 -0.0033772185 -0.0033764504 -0.0033759787 -0.0033758536 -0.0033761368 -0.0033767237 -0.0033772043 -0.0033777282 -0.0033784744 -0.0033790786 -0.0033796637 -0.0033801708][-0.0033815566 -0.0033790446 -0.003378026 -0.00337697 -0.0033761493 -0.0033756825 -0.0033754646 -0.0033757908 -0.0033763915 -0.0033771417 -0.0033780076 -0.0033790709 -0.0033800728 -0.0033809852 -0.0033816698][-0.0033835908 -0.0033814351 -0.0033804649 -0.003379355 -0.0033785915 -0.0033780297 -0.0033778374 -0.0033782332 -0.00337898 -0.0033799484 -0.0033810341 -0.0033824481 -0.0033838695 -0.0033851417 -0.0033860977][-0.003385135 -0.0033834849 -0.0033828279 -0.0033818909 -0.0033812714 -0.0033808113 -0.0033809633 -0.0033817035 -0.0033827154 -0.0033838928 -0.0033852 -0.0033867937 -0.003388356 -0.0033898356 -0.0033908563][-0.0033863611 -0.0033852342 -0.0033848567 -0.0033840504 -0.0033836074 -0.003383358 -0.003383944 -0.0033850619 -0.0033864412 -0.003387884 -0.0033893292 -0.0033906163 -0.0033917776 -0.0033928703 -0.0033934338][-0.0033875026 -0.0033865187 -0.0033863438 -0.0033856027 -0.0033853047 -0.0033853343 -0.0033861657 -0.0033876305 -0.003389427 -0.0033910472 -0.003392119 -0.0033927984 -0.0033933893 -0.0033938291 -0.0033941418][-0.0033879953 -0.0033872442 -0.0033869266 -0.0033860377 -0.0033856558 -0.0033859282 -0.0033870572 -0.0033886761 -0.0033905834 -0.0033918961 -0.0033923117 -0.0033921439 -0.0033917257 -0.0033916773 -0.0033923495][-0.0033885543 -0.0033879692 -0.0033876223 -0.0033867653 -0.0033860488 -0.003386162 -0.0033869874 -0.0033882624 -0.0033899564 -0.0033903655 -0.0033897841 -0.0033885068 -0.0033870174 -0.0033863976 -0.0033873348][-0.0033891432 -0.0033889785 -0.0033889604 -0.0033879965 -0.0033869417 -0.0033864654 -0.0033866605 -0.0033868267 -0.003387406 -0.003386759 -0.0033847571 -0.0033822369 -0.0033801531 -0.0033796614 -0.0033810241][-0.0033914102 -0.0033921376 -0.0033922705 -0.0033910968 -0.0033894768 -0.0033878514 -0.0033864169 -0.0033850174 -0.0033843915 -0.003382724 -0.0033800763 -0.0033770863 -0.0033751929 -0.0033756115 -0.0033778013][-0.0033945206 -0.0033961015 -0.0033966757 -0.0033950617 -0.0033923052 -0.003389335 -0.0033867073 -0.0033842402 -0.0033830227 -0.0033813969 -0.0033790288 -0.0033767296 -0.0033760089 -0.0033773205 -0.0033800891][-0.0033968585 -0.0033987844 -0.0033997677 -0.0033982606 -0.0033950477 -0.0033914256 -0.0033883271 -0.0033854237 -0.0033840393 -0.00338278 -0.0033815494 -0.0033805005 -0.0033805743 -0.0033822283 -0.0033846516][-0.0033966845 -0.0033983458 -0.0033999614 -0.0033992375 -0.0033964287 -0.00339336 -0.0033907886 -0.0033886712 -0.0033877196 -0.0033872079 -0.00338674 -0.0033863753 -0.0033870724 -0.00338828 -0.0033896936][-0.0033947362 -0.0033960966 -0.0033981735 -0.0033984981 -0.0033971835 -0.0033954622 -0.0033939616 -0.0033928894 -0.0033926426 -0.0033924445 -0.0033922475 -0.0033919949 -0.0033924289 -0.003393237 -0.003393942][-0.0033913094 -0.0033922768 -0.0033947793 -0.0033962736 -0.0033964682 -0.0033959972 -0.0033953758 -0.0033948096 -0.0033947567 -0.0033946491 -0.0033944543 -0.003394122 -0.0033942035 -0.0033945334 -0.0033946703]]...]
INFO - root - 2017-12-09 16:59:36.389589: step 42810, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 67h:34m:30s remains)
INFO - root - 2017-12-09 16:59:45.034410: step 42820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:47m:58s remains)
INFO - root - 2017-12-09 16:59:53.870807: step 42830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:20m:41s remains)
INFO - root - 2017-12-09 17:00:02.545169: step 42840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:12m:10s remains)
INFO - root - 2017-12-09 17:00:11.153012: step 42850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:10m:37s remains)
INFO - root - 2017-12-09 17:00:19.980663: step 42860, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 68h:59m:12s remains)
INFO - root - 2017-12-09 17:00:28.642052: step 42870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:21m:31s remains)
INFO - root - 2017-12-09 17:00:37.356101: step 42880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:46m:24s remains)
INFO - root - 2017-12-09 17:00:45.686520: step 42890, loss = 0.89, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 63h:04m:23s remains)
INFO - root - 2017-12-09 17:00:54.140278: step 42900, loss = 0.89, batch loss = 0.68 (8.5 examples/sec; 0.936 sec/batch; 75h:16m:17s remains)
2017-12-09 17:00:55.006291: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.080993779 0.072467163 0.063687764 0.054335672 0.045539625 0.03798053 0.031886011 0.028168371 0.025735466 0.02511517 0.024432935 0.023574317 0.022107134 0.020493792 0.0181521][0.11055519 0.10307236 0.094398335 0.084138006 0.074172981 0.064734012 0.056538433 0.050106242 0.045311242 0.042698402 0.040424094 0.038659852 0.035988156 0.032751396 0.02842834][0.15633443 0.14988327 0.14041816 0.12954694 0.11871793 0.10815015 0.098923422 0.091039844 0.085234486 0.081060074 0.077006266 0.073090747 0.067945518 0.062013328 0.054529104][0.21478188 0.20946068 0.19957817 0.18771982 0.17570274 0.16421863 0.15433829 0.14614321 0.14022428 0.13526039 0.1303459 0.12528303 0.11853271 0.1099989 0.099529184][0.277422 0.27360773 0.26357874 0.25071007 0.23750506 0.22512732 0.21474892 0.20651557 0.20103224 0.1968976 0.19253248 0.18771318 0.18000135 0.16988929 0.15720642][0.33253983 0.33013409 0.32046577 0.30715418 0.29351467 0.28135347 0.27161717 0.26451495 0.26063693 0.25810817 0.25491244 0.25071216 0.24312396 0.23261063 0.21891388][0.36831656 0.36816683 0.35953695 0.34691921 0.33385903 0.32220179 0.31339356 0.30809548 0.30635163 0.30599964 0.30466482 0.30186376 0.29527849 0.28515136 0.27124453][0.38156098 0.3838149 0.37629032 0.36485055 0.35274643 0.3417207 0.33336931 0.32923323 0.32871473 0.32997003 0.33036247 0.3291271 0.32434589 0.3157931 0.3032333][0.36922267 0.37450555 0.36902371 0.3594507 0.34880805 0.33869687 0.33050218 0.32600611 0.324771 0.32594404 0.32679468 0.32671368 0.32392347 0.31780016 0.30791694][0.33406377 0.34134486 0.33772758 0.33057564 0.32208583 0.31320578 0.30530548 0.29950377 0.29635102 0.29569173 0.29537719 0.29509279 0.29347378 0.2900421 0.28349757][0.279627 0.28752345 0.28516853 0.28009877 0.27371627 0.26622447 0.25892785 0.25241119 0.24768135 0.24492909 0.24268349 0.2413314 0.239851 0.23783366 0.23360638][0.21270739 0.21988733 0.21830706 0.21488874 0.21037702 0.20460624 0.1985634 0.19244447 0.18734035 0.18361633 0.18056713 0.17870039 0.17734623 0.17601384 0.17330436][0.14617439 0.15160258 0.15045859 0.14819705 0.14515556 0.14103302 0.13638037 0.13120486 0.12645058 0.12259007 0.11934897 0.11727497 0.11597273 0.11519384 0.11374131][0.088935234 0.092466496 0.091692 0.090322584 0.088491537 0.085852139 0.082737744 0.078942791 0.075202107 0.071988948 0.069174565 0.067308061 0.066141725 0.065622993 0.06488765][0.0466356 0.048351929 0.047713075 0.046881791 0.045856215 0.044352531 0.042544715 0.040255211 0.037897181 0.035718761 0.033713538 0.032303598 0.031356357 0.030908471 0.030448711]]...]
INFO - root - 2017-12-09 17:01:03.472638: step 42910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:39m:18s remains)
INFO - root - 2017-12-09 17:01:12.117314: step 42920, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 67h:53m:02s remains)
INFO - root - 2017-12-09 17:01:20.748564: step 42930, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:28m:39s remains)
INFO - root - 2017-12-09 17:01:29.255563: step 42940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:31m:12s remains)
INFO - root - 2017-12-09 17:01:37.881681: step 42950, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 70h:40m:25s remains)
INFO - root - 2017-12-09 17:01:46.589922: step 42960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:32m:07s remains)
INFO - root - 2017-12-09 17:01:55.314523: step 42970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:02m:56s remains)
INFO - root - 2017-12-09 17:02:03.939446: step 42980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:37m:05s remains)
INFO - root - 2017-12-09 17:02:12.597338: step 42990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:08m:12s remains)
INFO - root - 2017-12-09 17:02:21.199924: step 43000, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 73h:20m:19s remains)
2017-12-09 17:02:22.048489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032141723 -0.0032132296 -0.00321353 -0.0032132817 -0.0032137237 -0.0032154659 -0.0032157465 -0.003217923 -0.0032196939 -0.0032214555 -0.0032229233 -0.0032201863 -0.003217634 -0.0032175479 -0.0032192138][-0.0032280975 -0.0032292693 -0.0032306663 -0.0032314807 -0.0032320721 -0.0032327464 -0.0032318034 -0.0032324709 -0.0032336714 -0.0032337476 -0.0032321759 -0.0032298355 -0.0032274891 -0.003227849 -0.0032285799][-0.0032577731 -0.0032595133 -0.003261266 -0.0032631734 -0.0032648195 -0.0032659073 -0.0032651538 -0.0032646284 -0.0032644789 -0.0032629212 -0.0032604001 -0.0032583331 -0.0032561382 -0.0032537107 -0.0032517877][-0.003290676 -0.0032929825 -0.00329398 -0.0032945215 -0.0032946991 -0.0032953138 -0.0032943292 -0.0032928037 -0.0032920688 -0.0032906916 -0.0032888595 -0.0032883291 -0.0032869405 -0.0032843584 -0.0032815982][-0.0033196525 -0.0033219093 -0.003322318 -0.0033214316 -0.0033210104 -0.0033204735 -0.0033186744 -0.0033176395 -0.0033178616 -0.0033177971 -0.0033174171 -0.0033180856 -0.0033172471 -0.0033148443 -0.0033111824][-0.0033516272 -0.0033526453 -0.0033518267 -0.0033493971 -0.0033463291 -0.0033432806 -0.0033404566 -0.0033397824 -0.0033420459 -0.0033444227 -0.0033458709 -0.0033471868 -0.003347005 -0.0033451363 -0.0033417689][-0.0033754013 -0.0033736338 -0.0033706997 -0.0033676047 -0.0033644966 -0.0033615571 -0.0033596859 -0.0033602866 -0.0033623986 -0.0033644354 -0.003367434 -0.0033695481 -0.0033694019 -0.0033685048 -0.0033663095][-0.0033903336 -0.0033873124 -0.0033834481 -0.00337955 -0.0033759407 -0.0033728858 -0.003371204 -0.0033730471 -0.00337733 -0.0033807729 -0.0033829492 -0.00338373 -0.0033829117 -0.0033811904 -0.0033794045][-0.0033986263 -0.003395665 -0.0033921653 -0.0033890095 -0.0033859883 -0.0033833282 -0.0033815717 -0.00338305 -0.0033873329 -0.0033908091 -0.0033925176 -0.0033925923 -0.0033917362 -0.0033909483 -0.0033899471][-0.0034021712 -0.0033996834 -0.0033981008 -0.0033969961 -0.0033958249 -0.0033943523 -0.0033938186 -0.00339532 -0.003398611 -0.0034011239 -0.0034018736 -0.0034013067 -0.003400248 -0.0033992026 -0.0033983216][-0.0034048415 -0.0034025756 -0.0034006736 -0.0033997102 -0.0033992843 -0.003398343 -0.0033984932 -0.0033998475 -0.0034024795 -0.0034045032 -0.0034050804 -0.0034046681 -0.0034039998 -0.0034034394 -0.0034028264][-0.0034052476 -0.0034032743 -0.0034017444 -0.0034006634 -0.0033998783 -0.003398601 -0.0033982056 -0.0033984967 -0.0033999153 -0.0034011591 -0.00340185 -0.0034020157 -0.0034020268 -0.003402336 -0.0034021058][-0.003405608 -0.0034039845 -0.0034027533 -0.0034014091 -0.0034003083 -0.0033989048 -0.0033979097 -0.0033972317 -0.003397394 -0.0033981497 -0.0033993295 -0.0034000536 -0.0034005961 -0.0034011162 -0.0034010683][-0.0034037989 -0.0034019228 -0.0034005719 -0.0033993642 -0.0033975944 -0.0033965011 -0.0033959658 -0.003395437 -0.0033954023 -0.0033958531 -0.0033969581 -0.0033973637 -0.003397722 -0.0033981497 -0.0033985602][-0.0033985979 -0.0033967996 -0.0033960931 -0.0033954396 -0.0033937998 -0.0033926102 -0.0033918493 -0.00339151 -0.0033913371 -0.0033916095 -0.003392749 -0.0033930931 -0.0033936389 -0.0033937483 -0.0033938424]]...]
INFO - root - 2017-12-09 17:02:30.719911: step 43010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:38m:06s remains)
INFO - root - 2017-12-09 17:02:39.452450: step 43020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:37m:16s remains)
INFO - root - 2017-12-09 17:02:48.181618: step 43030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:58m:21s remains)
INFO - root - 2017-12-09 17:02:56.736225: step 43040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:33m:19s remains)
INFO - root - 2017-12-09 17:03:05.323012: step 43050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 67h:50m:51s remains)
INFO - root - 2017-12-09 17:03:13.772753: step 43060, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 66h:03m:55s remains)
INFO - root - 2017-12-09 17:03:22.182334: step 43070, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 67h:32m:29s remains)
INFO - root - 2017-12-09 17:03:30.744219: step 43080, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 69h:21m:16s remains)
INFO - root - 2017-12-09 17:03:39.152358: step 43090, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 64h:58m:21s remains)
INFO - root - 2017-12-09 17:03:47.671267: step 43100, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 69h:27m:57s remains)
2017-12-09 17:03:48.565631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033654335 -0.0033628063 -0.0033624324 -0.00336239 -0.0033623984 -0.0033625094 -0.0033626568 -0.0033628375 -0.0033629918 -0.0033630864 -0.0033631329 -0.0033631276 -0.0033630731 -0.0033629974 -0.0033629336][-0.0033637742 -0.0033608512 -0.0033604624 -0.0033604135 -0.0033603958 -0.0033604321 -0.0033605064 -0.0033606344 -0.0033607646 -0.0033608552 -0.0033609096 -0.0033609034 -0.0033608477 -0.0033607981 -0.0033607439][-0.0033641106 -0.0033610445 -0.0033605709 -0.0033603986 -0.0033601825 -0.0033599858 -0.0033599008 -0.0033599683 -0.0033601332 -0.0033603322 -0.0033605497 -0.0033606931 -0.0033607583 -0.0033607611 -0.0033607157][-0.0033642612 -0.0033611436 -0.0033605506 -0.003360227 -0.0033597562 -0.0033593357 -0.0033591115 -0.0033591199 -0.0033592954 -0.0033596156 -0.0033600586 -0.0033603967 -0.0033606032 -0.0033607027 -0.0033607215][-0.0033643963 -0.0033612493 -0.0033604878 -0.0033598824 -0.0033590256 -0.0033582556 -0.0033577795 -0.0033576505 -0.003357878 -0.0033583962 -0.0033591446 -0.0033598016 -0.0033602931 -0.0033605753 -0.0033607071][-0.0033644224 -0.0033611676 -0.0033603015 -0.003359477 -0.0033582989 -0.0033571771 -0.0033563995 -0.0033560311 -0.0033562258 -0.0033568756 -0.0033579175 -0.0033589448 -0.0033598002 -0.0033603492 -0.0033606272][-0.003364444 -0.0033612279 -0.0033604142 -0.0033595995 -0.0033583066 -0.0033569031 -0.0033557417 -0.0033550414 -0.0033550311 -0.0033555361 -0.0033566928 -0.0033580267 -0.003359254 -0.003360085 -0.0033605304][-0.003364241 -0.0033612789 -0.00336066 -0.0033599597 -0.0033586598 -0.0033571194 -0.0033557357 -0.0033548111 -0.0033546132 -0.0033549278 -0.0033559664 -0.0033573373 -0.0033587511 -0.003359783 -0.0033603942][-0.0033639881 -0.00336128 -0.0033610922 -0.00336054 -0.0033592514 -0.003357664 -0.0033562076 -0.0033551413 -0.0033547815 -0.0033550011 -0.0033558535 -0.0033570684 -0.003358478 -0.00335959 -0.0033602766][-0.0033638459 -0.0033612684 -0.0033615502 -0.0033613495 -0.0033604379 -0.0033590314 -0.0033575168 -0.00335618 -0.0033555413 -0.0033556002 -0.0033562146 -0.0033571981 -0.0033585087 -0.0033596051 -0.0033602759][-0.0033637679 -0.0033612335 -0.0033619441 -0.0033622151 -0.0033619564 -0.0033611262 -0.0033597858 -0.0033583234 -0.0033573343 -0.0033569897 -0.0033572156 -0.0033578735 -0.003358911 -0.0033598209 -0.0033603788][-0.0033637066 -0.0033610847 -0.0033618992 -0.0033623215 -0.0033624177 -0.0033621385 -0.0033613 -0.0033601902 -0.0033592498 -0.0033587215 -0.0033586493 -0.003358955 -0.0033595811 -0.0033602063 -0.0033605646][-0.0033637653 -0.0033608996 -0.0033616221 -0.0033619462 -0.0033619811 -0.0033618265 -0.0033613725 -0.0033607325 -0.0033601481 -0.0033597972 -0.0033597078 -0.0033598503 -0.0033601823 -0.0033605383 -0.0033607138][-0.0033638442 -0.0033606878 -0.0033611814 -0.0033613136 -0.0033612815 -0.0033611718 -0.003360956 -0.0033606745 -0.0033604605 -0.0033603264 -0.0033603245 -0.0033604254 -0.0033605734 -0.003360749 -0.0033608249][-0.0033639758 -0.0033606186 -0.0033608663 -0.0033609089 -0.0033608908 -0.0033608386 -0.0033607616 -0.0033606859 -0.0033606363 -0.0033605867 -0.0033606063 -0.0033606689 -0.0033607257 -0.0033607974 -0.0033608433]]...]
INFO - root - 2017-12-09 17:03:57.214014: step 43110, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 70h:43m:23s remains)
INFO - root - 2017-12-09 17:04:05.836820: step 43120, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:38m:22s remains)
INFO - root - 2017-12-09 17:04:14.451908: step 43130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:20m:25s remains)
INFO - root - 2017-12-09 17:04:22.946777: step 43140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:56m:50s remains)
INFO - root - 2017-12-09 17:04:31.795887: step 43150, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:40m:03s remains)
INFO - root - 2017-12-09 17:04:40.412180: step 43160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:44m:50s remains)
INFO - root - 2017-12-09 17:04:49.088394: step 43170, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 67h:51m:44s remains)
INFO - root - 2017-12-09 17:04:57.769510: step 43180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:57m:04s remains)
INFO - root - 2017-12-09 17:05:06.368369: step 43190, loss = 0.89, batch loss = 0.68 (10.7 examples/sec; 0.746 sec/batch; 59h:58m:19s remains)
INFO - root - 2017-12-09 17:05:14.765471: step 43200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:49m:38s remains)
2017-12-09 17:05:15.700522: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045947619 0.041251361 0.03464324 0.027094334 0.020058369 0.014379717 0.01126888 0.011938222 0.014860807 0.019255798 0.023311192 0.026538158 0.028004488 0.0276942 0.025775444][0.05875361 0.051801346 0.042810574 0.032496169 0.022724073 0.014873315 0.0099419216 0.0087896753 0.010647346 0.014919708 0.019109925 0.022571458 0.024518289 0.024837872 0.023490006][0.069627419 0.059822693 0.047908947 0.035158195 0.023634033 0.014269188 0.0080516264 0.0058947587 0.00695082 0.010444734 0.013874734 0.017071553 0.019024462 0.019437348 0.018226689][0.0776694 0.066350348 0.052203409 0.037714384 0.024741499 0.014357479 0.0073087877 0.0039046309 0.0036305934 0.0053907409 0.0075368127 0.0098297959 0.011240209 0.011732815 0.0112202][0.082393892 0.071222238 0.056171387 0.04087922 0.027037347 0.015839718 0.008057192 0.0037865564 0.0025739311 0.0028457055 0.0035877381 0.0044636466 0.0052797897 0.0054006735 0.0049119359][0.083983786 0.07430885 0.059532978 0.044044748 0.029824005 0.018021217 0.0096621374 0.00446349 0.0025048489 0.0020137057 0.0018649553 0.0018204474 0.0017610057 0.001648285 0.0014167614][0.0823959 0.074638896 0.060698159 0.045674898 0.0312545 0.018832654 0.0096301138 0.0038808063 0.0018120101 0.0011917977 0.00095158862 0.00080082333 0.00039657904 0.00016321964 -0.00045589823][0.075864226 0.069577307 0.0571339 0.043728493 0.030437915 0.018719209 0.0096217217 0.0039476524 0.0016338667 0.0010550823 0.00095496792 0.0010928228 0.0005666581 4.1330466e-05 -0.00061563868][0.066782489 0.060890235 0.049783997 0.038096413 0.027043348 0.017347481 0.0098329075 0.0052812668 0.0035148337 0.003269115 0.0031904024 0.0029852013 0.0021949103 0.0010414356 -0.000281204][0.058152027 0.052389063 0.0422023 0.032522492 0.02411749 0.017051265 0.011772947 0.0091909487 0.0084539875 0.0084534651 0.0084395669 0.0078644566 0.0066986373 0.0051199282 0.0038043829][0.049433924 0.043690898 0.034607932 0.026995771 0.020363869 0.015151201 0.011233657 0.0089140488 0.0079761064 0.0076792892 0.0084195929 0.0091261659 0.0098198215 0.010110129 0.0099702468][0.039485469 0.034051605 0.026030703 0.021050947 0.017510433 0.0151574 0.013934135 0.01359432 0.013892366 0.013475448 0.013951827 0.014784377 0.01520914 0.015405478 0.015411748][0.033483297 0.028312916 0.021589555 0.018192427 0.016464557 0.016254935 0.017552236 0.019221241 0.021356054 0.022696491 0.024487035 0.025185714 0.02512368 0.024734668 0.02423536][0.03004021 0.026180321 0.021573935 0.020223089 0.020909298 0.023150347 0.026339557 0.029650288 0.033003323 0.035621129 0.037489183 0.037746064 0.037248015 0.036168061 0.034857094][0.029286059 0.027636519 0.025858995 0.027344769 0.030277571 0.034281716 0.038107842 0.041791014 0.044700354 0.046402723 0.047452807 0.047196373 0.046278633 0.044566311 0.043206416]]...]
INFO - root - 2017-12-09 17:05:24.254473: step 43210, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 67h:46m:37s remains)
INFO - root - 2017-12-09 17:05:32.734980: step 43220, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 67h:06m:33s remains)
INFO - root - 2017-12-09 17:05:41.347477: step 43230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:43m:06s remains)
INFO - root - 2017-12-09 17:05:49.828281: step 43240, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 71h:14m:28s remains)
INFO - root - 2017-12-09 17:05:58.591055: step 43250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:47m:37s remains)
INFO - root - 2017-12-09 17:06:07.153889: step 43260, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 69h:42m:33s remains)
INFO - root - 2017-12-09 17:06:15.774657: step 43270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:08m:32s remains)
INFO - root - 2017-12-09 17:06:24.297193: step 43280, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 68h:26m:04s remains)
INFO - root - 2017-12-09 17:06:32.639613: step 43290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 67h:22m:36s remains)
INFO - root - 2017-12-09 17:06:40.949094: step 43300, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:01m:38s remains)
2017-12-09 17:06:41.792472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033819561 -0.0033788483 -0.0033779691 -0.0033771044 -0.0033764427 -0.0033757531 -0.0033752015 -0.003375029 -0.0033750017 -0.0033752862 -0.0033754886 -0.003375645 -0.0033758727 -0.003376249 -0.0033767712][-0.0033811145 -0.0033777282 -0.00337655 -0.0033755456 -0.0033747207 -0.0033740031 -0.0033733509 -0.0033730802 -0.0033728862 -0.0033729363 -0.0033731069 -0.0033733244 -0.0033736613 -0.0033740846 -0.0033747444][-0.0033833252 -0.0033800381 -0.0033787105 -0.0033776155 -0.0033766718 -0.0033759025 -0.003375194 -0.003374811 -0.0033744937 -0.0033743463 -0.0033743191 -0.0033744662 -0.0033747533 -0.0033751684 -0.0033758378][-0.0033859657 -0.0033828756 -0.0033815252 -0.0033806365 -0.0033797603 -0.0033792316 -0.003378626 -0.0033782038 -0.0033778059 -0.0033775582 -0.0033775098 -0.0033775768 -0.003377906 -0.003378507 -0.0033794877][-0.003388542 -0.0033856847 -0.0033846302 -0.0033841378 -0.0033838761 -0.0033838176 -0.0033835452 -0.0033834155 -0.0033832008 -0.0033830241 -0.0033829582 -0.0033828586 -0.0033831578 -0.0033837971 -0.003384761][-0.0033896656 -0.0033871729 -0.0033865178 -0.0033867131 -0.003387193 -0.0033876721 -0.0033879387 -0.0033882286 -0.0033884239 -0.0033886589 -0.0033889171 -0.0033888314 -0.0033893199 -0.0033900863 -0.00339091][-0.0033886596 -0.0033864384 -0.0033863513 -0.0033875529 -0.0033888172 -0.0033900475 -0.0033909746 -0.0033916861 -0.0033920638 -0.0033924396 -0.0033928875 -0.0033930829 -0.0033939495 -0.0033948498 -0.0033956752][-0.0033883734 -0.0033866982 -0.0033874204 -0.0033895471 -0.0033917215 -0.0033940629 -0.0033961842 -0.0033976627 -0.0033982021 -0.0033983234 -0.0033982925 -0.0033981658 -0.003398913 -0.003399577 -0.0033999621][-0.0033895625 -0.00338873 -0.0033904181 -0.0033934107 -0.0033965257 -0.0033999884 -0.0034029719 -0.0034051249 -0.0034062059 -0.0034064774 -0.0034060976 -0.0034054031 -0.0034052595 -0.0034049454 -0.0034042625][-0.0033910647 -0.0033908531 -0.0033934047 -0.0033969835 -0.0034007095 -0.0034044627 -0.0034074085 -0.0034095615 -0.0034108453 -0.0034114376 -0.0034112362 -0.0034108781 -0.0034104683 -0.0034093095 -0.0034073622][-0.0033930789 -0.0033932382 -0.003396167 -0.0033998305 -0.0034034934 -0.0034067354 -0.0034091685 -0.0034107154 -0.003411554 -0.0034120521 -0.0034121089 -0.00341207 -0.0034117014 -0.0034105272 -0.0034085787][-0.003394413 -0.0033945937 -0.003397255 -0.0034003321 -0.0034031207 -0.0034053039 -0.0034069058 -0.0034077261 -0.0034083342 -0.0034087468 -0.0034089254 -0.0034091012 -0.0034089386 -0.0034082427 -0.0034070103][-0.0033925723 -0.0033922042 -0.0033942577 -0.003396451 -0.0033984431 -0.0034000725 -0.0034014245 -0.0034021956 -0.0034029202 -0.003403326 -0.0034034215 -0.0034036594 -0.0034036972 -0.0034034343 -0.0034029505][-0.0033883576 -0.0033871091 -0.003388356 -0.003389704 -0.0033910384 -0.0033922156 -0.0033932463 -0.0033940799 -0.0033949376 -0.0033955059 -0.0033957134 -0.0033960589 -0.0033963034 -0.0033963665 -0.0033964217][-0.0033842972 -0.0033820351 -0.0033824285 -0.0033830649 -0.0033837729 -0.003384308 -0.0033848595 -0.0033854223 -0.0033859445 -0.0033863988 -0.0033866297 -0.0033870549 -0.0033874291 -0.0033878281 -0.0033882281]]...]
INFO - root - 2017-12-09 17:06:50.424844: step 43310, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 68h:54m:54s remains)
INFO - root - 2017-12-09 17:06:58.899611: step 43320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:46m:23s remains)
INFO - root - 2017-12-09 17:07:07.256083: step 43330, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 65h:57m:47s remains)
INFO - root - 2017-12-09 17:07:15.596241: step 43340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:20m:21s remains)
INFO - root - 2017-12-09 17:07:24.281157: step 43350, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 71h:13m:32s remains)
INFO - root - 2017-12-09 17:07:32.919661: step 43360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:46m:47s remains)
INFO - root - 2017-12-09 17:07:41.614947: step 43370, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 68h:29m:19s remains)
INFO - root - 2017-12-09 17:07:50.369489: step 43380, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 70h:09m:11s remains)
INFO - root - 2017-12-09 17:07:59.071110: step 43390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:33m:33s remains)
INFO - root - 2017-12-09 17:08:07.259607: step 43400, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:59m:17s remains)
2017-12-09 17:08:08.117160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033845387 -0.0033819028 -0.0033804446 -0.0033785913 -0.0033766665 -0.0033751708 -0.0033743244 -0.0033740362 -0.0033741123 -0.0033742881 -0.0033743188 -0.0033742753 -0.0033741975 -0.003374368 -0.0033744574][-0.0033829669 -0.0033806604 -0.0033795231 -0.0033779379 -0.0033761796 -0.0033748983 -0.0033742357 -0.0033741577 -0.0033743593 -0.0033745018 -0.0033742988 -0.0033737058 -0.0033731426 -0.0033726809 -0.0033724667][-0.0033819673 -0.0033798835 -0.0033792504 -0.003378168 -0.0033769181 -0.0033761966 -0.0033762108 -0.0033771037 -0.0033779747 -0.0033785722 -0.0033782923 -0.0033770825 -0.0033755708 -0.0033739028 -0.0033727563][-0.0033806397 -0.0033784963 -0.0033780914 -0.0033774367 -0.0033768339 -0.0033770574 -0.0033780683 -0.0033799461 -0.0033817021 -0.0033830334 -0.0033829107 -0.0033813685 -0.0033788241 -0.00337582 -0.0033735621][-0.0033798919 -0.0033770117 -0.0033764506 -0.0033762211 -0.0033764557 -0.0033775633 -0.003379415 -0.0033820353 -0.0033847215 -0.0033867285 -0.0033872058 -0.0033859275 -0.0033827748 -0.0033785522 -0.0033750921][-0.0033802758 -0.0033765545 -0.0033756434 -0.0033759137 -0.0033768928 -0.0033783983 -0.0033803831 -0.0033831096 -0.0033861 -0.0033886035 -0.0033898221 -0.0033890756 -0.0033860586 -0.0033812781 -0.0033769908][-0.0033825627 -0.0033789144 -0.0033782616 -0.0033789042 -0.0033800201 -0.0033813249 -0.0033824574 -0.0033839226 -0.0033860763 -0.0033884603 -0.003389864 -0.0033896528 -0.0033871473 -0.0033826833 -0.0033783438][-0.0033856777 -0.0033830069 -0.0033832188 -0.0033842195 -0.0033854004 -0.003386172 -0.0033861892 -0.0033860593 -0.0033864267 -0.0033872663 -0.0033878966 -0.003387786 -0.003385955 -0.003382371 -0.0033785575][-0.0033876412 -0.0033863715 -0.0033878211 -0.0033896184 -0.0033911776 -0.0033918757 -0.0033911981 -0.0033894007 -0.0033874607 -0.0033859941 -0.0033853047 -0.0033847368 -0.0033833045 -0.0033805713 -0.0033775831][-0.0033878041 -0.003387806 -0.0033905546 -0.0033933665 -0.0033954442 -0.0033964415 -0.0033952745 -0.0033920999 -0.003388471 -0.0033852551 -0.0033832076 -0.0033815948 -0.0033799424 -0.0033779109 -0.0033758336][-0.0033860423 -0.0033869296 -0.0033908049 -0.0033944773 -0.0033970901 -0.0033981481 -0.0033965912 -0.003392776 -0.0033882055 -0.003384108 -0.0033810961 -0.0033787 -0.0033768141 -0.0033752816 -0.0033740175][-0.0033827946 -0.0033836509 -0.00338782 -0.0033916822 -0.0033944417 -0.0033955136 -0.0033940694 -0.0033904854 -0.0033861264 -0.0033821913 -0.0033789766 -0.0033762613 -0.0033743759 -0.0033732497 -0.0033725712][-0.0033792234 -0.0033791743 -0.0033828113 -0.0033862761 -0.0033887706 -0.0033898517 -0.0033888959 -0.0033861515 -0.0033827275 -0.0033796246 -0.0033769303 -0.0033745649 -0.0033730278 -0.0033722764 -0.0033719209][-0.0033767768 -0.0033757198 -0.0033782828 -0.0033806711 -0.0033824756 -0.0033834334 -0.0033830134 -0.0033812525 -0.0033789554 -0.0033768977 -0.0033750315 -0.0033733509 -0.0033723526 -0.0033719363 -0.0033717277][-0.0033756406 -0.0033737128 -0.0033750946 -0.0033764185 -0.0033775021 -0.0033780732 -0.0033778993 -0.0033769503 -0.0033756308 -0.003374469 -0.0033734497 -0.0033725516 -0.0033720303 -0.0033718168 -0.0033717328]]...]
INFO - root - 2017-12-09 17:08:16.579656: step 43410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:37m:59s remains)
INFO - root - 2017-12-09 17:08:25.227690: step 43420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:23m:36s remains)
INFO - root - 2017-12-09 17:08:33.856884: step 43430, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:22m:46s remains)
INFO - root - 2017-12-09 17:08:42.368691: step 43440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:50m:57s remains)
INFO - root - 2017-12-09 17:08:51.086774: step 43450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:11m:43s remains)
INFO - root - 2017-12-09 17:08:59.715068: step 43460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:18m:18s remains)
INFO - root - 2017-12-09 17:09:08.390120: step 43470, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 72h:15m:05s remains)
INFO - root - 2017-12-09 17:09:17.035136: step 43480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:07m:56s remains)
INFO - root - 2017-12-09 17:09:25.796588: step 43490, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:26m:13s remains)
INFO - root - 2017-12-09 17:09:34.112257: step 43500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:02m:19s remains)
2017-12-09 17:09:34.961362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034017712 -0.0034005665 -0.0034003411 -0.0034004131 -0.003400642 -0.0034009884 -0.0034011875 -0.0034012333 -0.003401353 -0.0034011395 -0.0034007379 -0.0034003779 -0.0034001796 -0.003400001 -0.003399841][-0.0034004191 -0.0033991321 -0.0033990098 -0.0033992135 -0.0033994422 -0.0033999269 -0.0034002536 -0.0034003558 -0.0034005565 -0.0034004028 -0.0033998718 -0.0033993796 -0.0033991425 -0.003398844 -0.0033984645][-0.0034011335 -0.0033999847 -0.0034001793 -0.0034006394 -0.0034009763 -0.0034015011 -0.0034020185 -0.0034020187 -0.0034018238 -0.0034016925 -0.0034010913 -0.0034005721 -0.003399889 -0.0033995132 -0.0033990932][-0.0034019337 -0.003401272 -0.0034021125 -0.003402903 -0.0034030387 -0.003403279 -0.0034035444 -0.0034032967 -0.0034028601 -0.0034024455 -0.00340185 -0.0034014636 -0.0034007796 -0.0034001176 -0.0033995889][-0.0034032033 -0.0034029838 -0.0034040213 -0.0034045975 -0.0034042692 -0.0034035714 -0.0034029987 -0.0034021286 -0.0034015656 -0.0034014932 -0.0034017176 -0.0034019535 -0.0034016196 -0.0034010296 -0.0034003286][-0.0034042052 -0.0034040974 -0.0034051517 -0.0034055905 -0.0034046273 -0.0034030236 -0.0034011255 -0.0033992715 -0.0033985763 -0.0033994622 -0.0034007116 -0.0034016753 -0.0034015924 -0.0034010794 -0.0034003388][-0.0034044385 -0.0034044816 -0.0034055249 -0.003405655 -0.0034041752 -0.0034014746 -0.003397685 -0.0033942692 -0.003394292 -0.0033974415 -0.0034001272 -0.0034016094 -0.0034015202 -0.003400841 -0.0034001104][-0.0034042497 -0.0034042522 -0.0034053703 -0.0034058639 -0.0034044401 -0.003401269 -0.0033964077 -0.0033928035 -0.0033943881 -0.0033982736 -0.0034012664 -0.0034027898 -0.0034025074 -0.0034012478 -0.0034001684][-0.0034038357 -0.0034038872 -0.0034053551 -0.0034063368 -0.0034058217 -0.0034033875 -0.0033993926 -0.0033967367 -0.0033979914 -0.0034010063 -0.0034031987 -0.0034041172 -0.003403401 -0.0034019281 -0.0034005914][-0.0034037069 -0.0034038702 -0.0034055922 -0.0034070706 -0.0034075589 -0.0034063396 -0.003404221 -0.0034027437 -0.0034030757 -0.0034040848 -0.0034046739 -0.0034046031 -0.0034035768 -0.003402041 -0.0034007016][-0.0034032355 -0.0034032215 -0.0034049472 -0.0034065526 -0.003407672 -0.0034081158 -0.0034076837 -0.0034070602 -0.0034067181 -0.0034060376 -0.0034052476 -0.0034042059 -0.0034028161 -0.0034012115 -0.0034000708][-0.0034019644 -0.0034016604 -0.0034031449 -0.0034047477 -0.0034061803 -0.0034073361 -0.0034079116 -0.0034079028 -0.0034072772 -0.003406066 -0.0034046415 -0.0034029905 -0.0034013172 -0.0034001046 -0.0033994033][-0.0034006776 -0.0033996764 -0.0034005861 -0.0034017793 -0.0034031568 -0.0034044383 -0.0034053244 -0.0034055139 -0.0034051167 -0.0034041181 -0.0034026161 -0.0034010161 -0.0033997288 -0.0033990801 -0.0033987402][-0.0033998571 -0.0033983474 -0.0033987912 -0.0033994038 -0.0034004024 -0.0034013293 -0.0034019041 -0.0034019563 -0.0034017658 -0.0034011665 -0.0033999376 -0.0033989504 -0.0033984056 -0.0033982592 -0.0033981944][-0.0033994538 -0.0033977649 -0.003398061 -0.0033984235 -0.0033989933 -0.003399499 -0.0033996752 -0.0033994971 -0.0033993106 -0.0033989393 -0.0033982191 -0.0033977816 -0.0033976526 -0.0033978142 -0.0033979248]]...]
INFO - root - 2017-12-09 17:09:43.449002: step 43510, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 68h:43m:45s remains)
INFO - root - 2017-12-09 17:09:52.035961: step 43520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:21m:01s remains)
INFO - root - 2017-12-09 17:10:00.718139: step 43530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:09m:05s remains)
INFO - root - 2017-12-09 17:10:09.287563: step 43540, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 69h:10m:23s remains)
INFO - root - 2017-12-09 17:10:18.023874: step 43550, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 71h:49m:33s remains)
INFO - root - 2017-12-09 17:10:26.766600: step 43560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:32m:27s remains)
INFO - root - 2017-12-09 17:10:35.515662: step 43570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:11m:50s remains)
INFO - root - 2017-12-09 17:10:44.198314: step 43580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:51m:55s remains)
INFO - root - 2017-12-09 17:10:52.862545: step 43590, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:55m:49s remains)
INFO - root - 2017-12-09 17:11:01.258361: step 43600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:13m:23s remains)
2017-12-09 17:11:02.161856: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1431333 0.14832577 0.15277016 0.15606181 0.15886214 0.16135371 0.16115627 0.15938839 0.15551263 0.15105827 0.14514546 0.13930841 0.13345034 0.12742415 0.12340847][0.15627442 0.16068694 0.16403997 0.1669046 0.16923769 0.17119887 0.17121457 0.17019258 0.16770107 0.16477782 0.16148339 0.15851037 0.15512724 0.15080097 0.14835764][0.1577718 0.16159609 0.16483943 0.16743462 0.17017184 0.17281872 0.17343438 0.17365892 0.173044 0.17217349 0.17111023 0.17063037 0.16981955 0.16754165 0.166214][0.15206461 0.15633492 0.16022947 0.16373307 0.16732413 0.17096831 0.17354757 0.17549014 0.17637445 0.17707597 0.1782058 0.18008667 0.18130182 0.18076281 0.1805058][0.1453564 0.15022425 0.15473065 0.15995149 0.16504145 0.17037308 0.17458443 0.17799763 0.18067251 0.18254079 0.18500569 0.18780328 0.19014309 0.19135466 0.19196929][0.1412563 0.1468952 0.15150088 0.15765905 0.16341393 0.16988894 0.17578246 0.18033071 0.18381593 0.18606688 0.18915404 0.19309664 0.19618727 0.1982834 0.19935232][0.13933234 0.14513113 0.14921907 0.15575106 0.16156338 0.16851659 0.17429268 0.17876892 0.18207411 0.18457672 0.18818644 0.19226681 0.19548397 0.19784449 0.19949989][0.13422224 0.13962179 0.14285558 0.14821163 0.15326674 0.15953326 0.16406184 0.1675096 0.16996326 0.17190206 0.17507246 0.17854021 0.18221955 0.1847094 0.18631524][0.12162581 0.12606829 0.12806635 0.13155612 0.13495184 0.13965724 0.14232893 0.14417964 0.14526482 0.14621133 0.14836325 0.15059815 0.15334576 0.15495391 0.15595423][0.10062589 0.10329749 0.10364382 0.10527652 0.10649852 0.10931404 0.11005325 0.11046381 0.11041637 0.11019175 0.11117092 0.11180644 0.1131786 0.11363476 0.11396492][0.073816687 0.075052217 0.074272022 0.074026205 0.074015662 0.074647814 0.073497437 0.0729244 0.07223627 0.071595095 0.071604751 0.071129814 0.071500845 0.070878945 0.070400476][0.046247121 0.046281517 0.045396473 0.044731878 0.044098973 0.043448728 0.041849881 0.040523559 0.039192509 0.038346473 0.038001407 0.037370194 0.037366752 0.036638249 0.036201891][0.023994548 0.023352271 0.02219085 0.021200527 0.020312706 0.019332174 0.018167553 0.016815949 0.015598087 0.014738189 0.014210712 0.013812156 0.013671758 0.013190482 0.013007214][0.0078941351 0.0072268043 0.0064237015 0.0057111513 0.0050461153 0.0043457951 0.0035610797 0.0027544808 0.0020294164 0.001486494 0.001116887 0.00088294991 0.00078401645 0.00070899189 0.00081335823][-0.00074157771 -0.00098226033 -0.0012229532 -0.001428321 -0.0016228108 -0.0018503566 -0.002117279 -0.002397893 -0.0026560843 -0.0028726228 -0.0030449091 -0.0031641978 -0.0032297238 -0.0032577987 -0.0032017087]]...]
INFO - root - 2017-12-09 17:11:10.674062: step 43610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:27m:33s remains)
INFO - root - 2017-12-09 17:11:19.269710: step 43620, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:34m:39s remains)
INFO - root - 2017-12-09 17:11:27.919809: step 43630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:26m:09s remains)
INFO - root - 2017-12-09 17:11:36.471360: step 43640, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 71h:44m:01s remains)
INFO - root - 2017-12-09 17:11:45.265275: step 43650, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 70h:39m:04s remains)
INFO - root - 2017-12-09 17:11:53.901244: step 43660, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 72h:15m:59s remains)
INFO - root - 2017-12-09 17:12:02.497856: step 43670, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 65h:42m:03s remains)
INFO - root - 2017-12-09 17:12:11.164849: step 43680, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 66h:10m:49s remains)
INFO - root - 2017-12-09 17:12:19.849706: step 43690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:44m:47s remains)
INFO - root - 2017-12-09 17:12:28.396035: step 43700, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 71h:21m:58s remains)
2017-12-09 17:12:29.344790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032441402 -0.0031907533 -0.0031373915 -0.0030536428 -0.0029824283 -0.002922937 -0.0028752945 -0.0028489337 -0.0028118726 -0.0027967286 -0.0027753916 -0.0027785492 -0.0027906296 -0.0027893893 -0.0028045][-0.0032434645 -0.0031810848 -0.0030986443 -0.0030235194 -0.002979123 -0.0029119162 -0.0028719287 -0.0028473837 -0.0028177435 -0.0028182887 -0.0027897093 -0.0027768258 -0.0027613763 -0.0027624196 -0.0027793802][-0.0032947913 -0.0032311787 -0.0031474195 -0.0030580317 -0.0029860488 -0.0029546348 -0.0029276186 -0.002906993 -0.0028863568 -0.0028904085 -0.0028773895 -0.0028813791 -0.0028547498 -0.0028358931 -0.0028218082][-0.0033070778 -0.0032475889 -0.0031770898 -0.0030885546 -0.003025003 -0.002976886 -0.0029548123 -0.0029620603 -0.0029747267 -0.0029943974 -0.00298619 -0.0029844758 -0.0029674694 -0.0029422585 -0.002910174][-0.0033240027 -0.0032598833 -0.0031861588 -0.0031172317 -0.0030726437 -0.0030454327 -0.0030382809 -0.0030336082 -0.0030421705 -0.0030590757 -0.0030679931 -0.0030864533 -0.0030666182 -0.0030387889 -0.0029908994][-0.0033632661 -0.0033101826 -0.0032357627 -0.0031786598 -0.0031159129 -0.0031152498 -0.0030959824 -0.0031135443 -0.0031283677 -0.0031464179 -0.0031650141 -0.0031648395 -0.0031442486 -0.0031014322 -0.0030468872][-0.0033714136 -0.0033381775 -0.0032923908 -0.0032267878 -0.00321001 -0.0031971368 -0.0032069543 -0.0031960406 -0.003190831 -0.0032222539 -0.0032283904 -0.0032386875 -0.0032137395 -0.0031675736 -0.0030987996][-0.0033831291 -0.0033564984 -0.003317303 -0.0032726699 -0.0032413714 -0.0032423437 -0.0032561722 -0.0032812818 -0.0033064997 -0.0033083297 -0.0033146373 -0.0033005374 -0.0032674347 -0.0032155211 -0.0031412779][-0.0033845352 -0.0033673632 -0.0033534255 -0.0033269585 -0.0032991602 -0.0032746335 -0.0032759118 -0.0033027525 -0.0033287222 -0.0033427645 -0.00334454 -0.0033341579 -0.0033150883 -0.0032610453 -0.003179197][-0.0033994755 -0.0033863431 -0.0033701111 -0.0033559022 -0.0033461263 -0.0033274577 -0.0033171989 -0.0033236714 -0.0033329329 -0.003345313 -0.0033433659 -0.0033536123 -0.0033262209 -0.0032807116 -0.0032098845][-0.0034053836 -0.003390982 -0.0033778055 -0.0033660431 -0.0033550432 -0.0033503126 -0.0033350145 -0.0033299082 -0.0033383581 -0.0033366173 -0.0033608538 -0.0033465035 -0.0033251327 -0.003284666 -0.0032068435][-0.0033781868 -0.0033773859 -0.00337944 -0.0033586046 -0.0033473938 -0.0033445049 -0.0033443277 -0.0033415977 -0.0033297569 -0.0033367423 -0.0033474944 -0.003339851 -0.003310919 -0.0032570267 -0.0031867407][-0.0033798772 -0.0033665537 -0.0033604205 -0.0033628752 -0.0033552113 -0.0033406205 -0.0033287667 -0.0033274936 -0.0033350445 -0.0033240251 -0.003332342 -0.0033268337 -0.0032972074 -0.0032358628 -0.0031475404][-0.003381226 -0.0033696627 -0.0033634428 -0.0033467712 -0.0033376766 -0.0033290936 -0.0033325937 -0.0033345607 -0.0033244507 -0.003321945 -0.0032992633 -0.003287859 -0.0032584195 -0.0031968539 -0.0031227476][-0.0033874528 -0.0033757284 -0.0033651823 -0.0033555063 -0.0033361162 -0.0033239473 -0.0033235284 -0.003322046 -0.0033268109 -0.0033181729 -0.0033073118 -0.0032867936 -0.0032395916 -0.0031863376 -0.0031009878]]...]
INFO - root - 2017-12-09 17:12:37.977040: step 43710, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 69h:23m:01s remains)
INFO - root - 2017-12-09 17:12:46.433653: step 43720, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.814 sec/batch; 65h:15m:24s remains)
INFO - root - 2017-12-09 17:12:55.170052: step 43730, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 70h:37m:04s remains)
INFO - root - 2017-12-09 17:13:03.716303: step 43740, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:59m:39s remains)
INFO - root - 2017-12-09 17:13:12.340110: step 43750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:49m:15s remains)
INFO - root - 2017-12-09 17:13:20.946307: step 43760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:15m:58s remains)
INFO - root - 2017-12-09 17:13:29.521073: step 43770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:56m:00s remains)
INFO - root - 2017-12-09 17:13:38.114433: step 43780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:43m:03s remains)
INFO - root - 2017-12-09 17:13:46.771955: step 43790, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:01m:33s remains)
INFO - root - 2017-12-09 17:13:55.095506: step 43800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:21m:37s remains)
2017-12-09 17:13:55.944124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033912784 -0.0033894377 -0.0033887338 -0.0033884784 -0.0033881681 -0.0033881639 -0.0033885008 -0.0033891096 -0.0033906307 -0.0033935276 -0.003396895 -0.0033999186 -0.0034007914 -0.0033973104 -0.0033891674][-0.0033898733 -0.0033877327 -0.0033867992 -0.0033866311 -0.003386684 -0.0033869315 -0.0033872013 -0.0033874607 -0.0033886426 -0.0033912382 -0.003395021 -0.0033982208 -0.0033986631 -0.0033940398 -0.0033834707][-0.0033909359 -0.003389128 -0.0033887082 -0.0033888454 -0.0033889462 -0.0033888395 -0.0033889 -0.003388915 -0.0033895958 -0.0033917965 -0.0033955548 -0.0033983598 -0.0033979951 -0.0033925944 -0.0033804479][-0.0033929416 -0.0033913753 -0.003390643 -0.0033904158 -0.0033900402 -0.0033895066 -0.0033891341 -0.0033892908 -0.0033900519 -0.0033921348 -0.0033958026 -0.0033989965 -0.0033985798 -0.0033928885 -0.0033809196][-0.0033949069 -0.0033934498 -0.0033922438 -0.0033913243 -0.0033900116 -0.0033887092 -0.0033875473 -0.0033870065 -0.0033878584 -0.0033903676 -0.0033943655 -0.0033982412 -0.0033993367 -0.0033952293 -0.0033857436][-0.003396794 -0.0033955856 -0.00339403 -0.0033916687 -0.0033884938 -0.0033846993 -0.00338062 -0.0033787275 -0.003379416 -0.0033832197 -0.0033890181 -0.0033953611 -0.0033995451 -0.0033980326 -0.0033899932][-0.0033984762 -0.0033971821 -0.0033953928 -0.0033920254 -0.0033868421 -0.0033788572 -0.0033708515 -0.0033665851 -0.0033671758 -0.0033725174 -0.003380585 -0.0033905352 -0.0033978489 -0.0033988189 -0.0033925164][-0.0033996126 -0.003398254 -0.0033960668 -0.0033911939 -0.0033832812 -0.0033723351 -0.0033608575 -0.0033540812 -0.003353985 -0.0033601047 -0.0033702319 -0.0033834719 -0.0033937043 -0.0033970978 -0.003393369][-0.0034001118 -0.0033990827 -0.0033968133 -0.0033908829 -0.0033809347 -0.0033680757 -0.0033559224 -0.0033496453 -0.0033502202 -0.0033574079 -0.0033677979 -0.0033802122 -0.0033905627 -0.00339564 -0.0033944722][-0.0033997071 -0.0033990054 -0.0033975211 -0.0033921881 -0.0033836698 -0.0033726266 -0.0033624498 -0.0033575776 -0.0033589038 -0.0033655346 -0.0033735693 -0.0033823194 -0.0033904798 -0.0033959968 -0.0033968498][-0.0033986424 -0.0033985516 -0.0033983157 -0.0033952768 -0.0033896705 -0.0033823033 -0.0033754907 -0.0033728264 -0.003372757 -0.0033763961 -0.0033814174 -0.0033867471 -0.003392647 -0.0033970291 -0.00339858][-0.0033968394 -0.003397746 -0.0033988254 -0.0033977132 -0.0033950612 -0.0033917471 -0.0033878786 -0.0033859748 -0.0033847247 -0.0033855194 -0.0033879483 -0.0033907129 -0.0033940759 -0.0033971793 -0.0033991742][-0.003394413 -0.0033952335 -0.0033972093 -0.0033980296 -0.0033976049 -0.00339634 -0.0033943411 -0.0033927662 -0.0033903683 -0.0033901487 -0.003390383 -0.0033910021 -0.0033926445 -0.0033945828 -0.0033965048][-0.0033919017 -0.0033917886 -0.0033931832 -0.0033946026 -0.0033954238 -0.0033957772 -0.0033946547 -0.0033934438 -0.0033916698 -0.0033912102 -0.0033904887 -0.0033900952 -0.0033904624 -0.0033910861 -0.0033923364][-0.0033897371 -0.0033885429 -0.0033889527 -0.0033899662 -0.0033908826 -0.00339123 -0.0033904095 -0.0033901648 -0.0033886353 -0.0033882172 -0.0033876766 -0.0033876393 -0.0033878644 -0.0033882905 -0.0033890987]]...]
INFO - root - 2017-12-09 17:14:04.688638: step 43810, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 68h:40m:06s remains)
INFO - root - 2017-12-09 17:14:13.289669: step 43820, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:23m:57s remains)
INFO - root - 2017-12-09 17:14:21.746686: step 43830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:09m:57s remains)
INFO - root - 2017-12-09 17:14:30.178546: step 43840, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:18m:57s remains)
INFO - root - 2017-12-09 17:14:38.955906: step 43850, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:19m:16s remains)
INFO - root - 2017-12-09 17:14:47.702019: step 43860, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 69h:19m:01s remains)
INFO - root - 2017-12-09 17:14:56.414869: step 43870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:58m:40s remains)
INFO - root - 2017-12-09 17:15:05.316192: step 43880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:52m:31s remains)
INFO - root - 2017-12-09 17:15:14.037618: step 43890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:26m:15s remains)
INFO - root - 2017-12-09 17:15:22.441933: step 43900, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 58h:28m:31s remains)
2017-12-09 17:15:23.303266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030090532 -0.0031836447 -0.0032059646 -0.0033290128 -0.0033418946 -0.0033449894 -0.0032966924 -0.0032588635 -0.0031277402 -0.0028066316 -0.0023472048 -0.0018293965 -0.001407143 -0.0013459786 -0.0015438517][-0.0028436263 -0.0030215832 -0.0030596291 -0.0031966111 -0.0032724736 -0.0032365308 -0.0031272084 -0.0030300121 -0.002831589 -0.0024558369 -0.00187329 -0.0012034641 -0.00060842792 -0.00052367384 -0.00091330614][-0.0028114431 -0.0029253864 -0.0029387285 -0.0030258943 -0.0031319126 -0.0029181372 -0.0027294727 -0.0024384912 -0.0020937675 -0.0016399343 -0.0009134002 -0.00011721067 0.00067457743 0.00071759941 0.00026579015][-0.002791865 -0.0028330749 -0.0028933352 -0.0028448598 -0.0028241412 -0.0022556747 -0.0018737662 -0.0011829981 -0.00065422151 5.7725934e-05 0.0010258402 0.0019714097 0.0028808559 0.0029037078 0.0025438722][-0.00266588 -0.0026268752 -0.0027512962 -0.0025017865 -0.0021513798 -0.0011637358 -0.00041420641 0.00080380822 0.0015740055 0.0028066374 0.0040641488 0.0052908645 0.0063122497 0.006335285 0.0061681918][-0.0024007212 -0.0023075368 -0.0025338577 -0.0021056165 -0.0013330681 0.00018803612 0.0015179846 0.0032552877 0.0043703495 0.0062321867 0.0078264661 0.00943736 0.010459771 0.010642441 0.010760657][-0.0021148641 -0.0019352619 -0.0021421183 -0.0017214115 -0.00062934472 0.0012587951 0.0032265563 0.0054820781 0.0070863874 0.009561114 0.011358514 0.013469595 0.01447018 0.014997361 0.015335435][-0.0019913719 -0.0017301652 -0.0018156526 -0.0015196824 -0.00044176378 0.0015996257 0.004077679 0.006675892 0.008819472 0.011738831 0.013790464 0.016380796 0.017496191 0.018552324 0.01906033][-0.0021115425 -0.001785267 -0.0017234626 -0.001555685 -0.0006772622 0.001155037 0.0036735386 0.0064165783 0.0090293251 0.012153452 0.014476607 0.017430607 0.018873349 0.020601952 0.021414544][-0.0024195942 -0.0021044267 -0.0019290595 -0.0018200879 -0.0012380371 0.0002122228 0.0024171779 0.0049495255 0.0077140597 0.01076537 0.013359958 0.016457105 0.018368604 0.020695427 0.021979915][-0.0027970118 -0.0025389164 -0.0023522957 -0.0022171699 -0.0018197226 -0.00082689035 0.00075540505 0.0028203861 0.0053956523 0.0081294477 0.010827079 0.0137845 0.016127951 0.018857952 0.020730808][-0.0031239025 -0.0029597026 -0.0028293126 -0.002643819 -0.0023744882 -0.0017583427 -0.00076958933 0.000679126 0.0027416414 0.004986973 0.0075547616 0.010174679 0.012731789 0.015528183 0.017868504][-0.0033210695 -0.0032434966 -0.0031784878 -0.0030123198 -0.0028311717 -0.0024662977 -0.0019610915 -0.0010784119 0.00035976223 0.0020317882 0.0041928617 0.00638465 0.0088619273 0.011453131 0.013972407][-0.0033918447 -0.00336963 -0.0033392487 -0.0032456063 -0.0031491925 -0.0029424927 -0.0027240103 -0.0022736876 -0.0014481158 -0.00033509056 0.0012668031 0.00298279 0.005122541 0.0073121698 0.0096414443][-0.0034022313 -0.0033982205 -0.0033874908 -0.0033495545 -0.003294474 -0.0031827043 -0.0031191525 -0.0029401213 -0.002590853 -0.0019637023 -0.00095387548 0.00025839894 0.0018874453 0.0035673804 0.0054529742]]...]
INFO - root - 2017-12-09 17:15:31.688772: step 43910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:12m:34s remains)
INFO - root - 2017-12-09 17:15:40.234514: step 43920, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 67h:01m:41s remains)
INFO - root - 2017-12-09 17:15:48.899548: step 43930, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:45m:17s remains)
INFO - root - 2017-12-09 17:15:57.411011: step 43940, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 70h:37m:07s remains)
INFO - root - 2017-12-09 17:16:06.093421: step 43950, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 69h:03m:59s remains)
INFO - root - 2017-12-09 17:16:14.792136: step 43960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:26m:42s remains)
INFO - root - 2017-12-09 17:16:23.541376: step 43970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:07m:55s remains)
INFO - root - 2017-12-09 17:16:32.308434: step 43980, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 69h:52m:43s remains)
INFO - root - 2017-12-09 17:16:40.944488: step 43990, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 70h:22m:58s remains)
INFO - root - 2017-12-09 17:16:49.626327: step 44000, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.740 sec/batch; 59h:19m:16s remains)
2017-12-09 17:16:50.335694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033651388 -0.0033617541 -0.0033606605 -0.0033598254 -0.0033588279 -0.0033576859 -0.0033566717 -0.003356236 -0.003355749 -0.003355524 -0.0033558668 -0.003356992 -0.0033583902 -0.003359901 -0.0033615388][-0.0033630142 -0.0033594375 -0.0033584866 -0.0033579892 -0.0033573173 -0.0033563471 -0.0033553485 -0.0033547108 -0.0033538709 -0.0033533715 -0.0033534248 -0.0033543794 -0.0033557059 -0.0033572505 -0.0033590849][-0.0033630799 -0.0033593734 -0.0033585622 -0.0033583324 -0.0033579664 -0.0033573282 -0.0033566242 -0.0033559178 -0.003354717 -0.0033538018 -0.0033534367 -0.0033539871 -0.0033550488 -0.0033564775 -0.0033584328][-0.0033630589 -0.003359322 -0.0033585688 -0.0033585776 -0.00335858 -0.003358443 -0.0033582496 -0.0033576994 -0.0033563504 -0.0033551913 -0.003354507 -0.0033544991 -0.0033550172 -0.0033561746 -0.0033580903][-0.0033629872 -0.0033591378 -0.0033584805 -0.003358667 -0.0033590174 -0.0033595327 -0.0033599723 -0.0033597855 -0.0033586014 -0.003357484 -0.0033564575 -0.0033557811 -0.0033556616 -0.003356368 -0.0033579487][-0.00336293 -0.0033590375 -0.0033584482 -0.0033587241 -0.0033591934 -0.003360108 -0.0033612177 -0.0033613844 -0.0033605234 -0.0033594936 -0.0033582903 -0.0033570377 -0.0033563781 -0.0033566908 -0.0033578775][-0.0033626554 -0.003358789 -0.0033582211 -0.0033584491 -0.0033589497 -0.00336017 -0.0033617616 -0.0033623779 -0.0033617951 -0.0033609793 -0.0033597015 -0.0033581671 -0.0033570935 -0.0033571122 -0.0033580388][-0.0033622819 -0.0033583853 -0.003357671 -0.0033575941 -0.0033579988 -0.0033594235 -0.0033612817 -0.0033622081 -0.0033621024 -0.0033617537 -0.0033606971 -0.00335913 -0.0033579543 -0.0033577897 -0.003358413][-0.0033616484 -0.0033576961 -0.0033569168 -0.0033565972 -0.0033570284 -0.0033585238 -0.0033604288 -0.0033615634 -0.0033620005 -0.0033620647 -0.003361136 -0.0033597294 -0.0033586819 -0.0033584363 -0.0033588342][-0.0033612798 -0.0033573606 -0.0033566498 -0.0033563464 -0.0033567913 -0.0033581203 -0.0033597606 -0.0033610081 -0.0033618438 -0.0033620407 -0.0033611897 -0.0033600465 -0.0033592992 -0.0033590447 -0.0033593304][-0.0033611201 -0.0033572984 -0.0033567846 -0.0033566011 -0.003356915 -0.0033578509 -0.003358973 -0.0033600482 -0.0033609085 -0.0033611692 -0.0033606249 -0.0033600058 -0.0033597404 -0.0033596237 -0.0033599082][-0.0033610342 -0.003357376 -0.0033569448 -0.0033566628 -0.0033567559 -0.0033572686 -0.0033578409 -0.0033586354 -0.0033594219 -0.0033598002 -0.0033597464 -0.0033597397 -0.0033598931 -0.0033599739 -0.003360342][-0.0033611997 -0.0033574712 -0.0033571217 -0.0033567157 -0.0033565634 -0.0033566682 -0.0033569541 -0.003357525 -0.0033581364 -0.0033585837 -0.0033589229 -0.003359304 -0.0033597529 -0.0033600766 -0.0033605802][-0.003361522 -0.0033577085 -0.0033574908 -0.0033571022 -0.0033568097 -0.0033566682 -0.0033567341 -0.003357132 -0.0033575061 -0.0033579394 -0.0033584668 -0.0033590505 -0.0033596153 -0.0033601888 -0.0033608438][-0.003361753 -0.0033579755 -0.0033578391 -0.003357559 -0.0033573548 -0.0033571704 -0.0033571951 -0.0033575087 -0.0033577592 -0.0033580987 -0.0033585674 -0.0033591343 -0.0033596749 -0.00336029 -0.00336098]]...]
INFO - root - 2017-12-09 17:16:58.801627: step 44010, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 72h:31m:21s remains)
INFO - root - 2017-12-09 17:17:07.508307: step 44020, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 72h:32m:33s remains)
INFO - root - 2017-12-09 17:17:16.124858: step 44030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:26m:42s remains)
INFO - root - 2017-12-09 17:17:24.671109: step 44040, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 73h:01m:18s remains)
INFO - root - 2017-12-09 17:17:33.436566: step 44050, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 69h:19m:30s remains)
INFO - root - 2017-12-09 17:17:42.097984: step 44060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:45m:20s remains)
INFO - root - 2017-12-09 17:17:50.881784: step 44070, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.910 sec/batch; 72h:56m:37s remains)
INFO - root - 2017-12-09 17:17:59.529560: step 44080, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 69h:28m:19s remains)
INFO - root - 2017-12-09 17:18:08.246596: step 44090, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 70h:57m:33s remains)
INFO - root - 2017-12-09 17:18:17.101405: step 44100, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:43m:22s remains)
2017-12-09 17:18:17.839549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034082625 -0.0034067237 -0.0033924028 -0.0033141049 -0.0030701887 -0.0025551571 -0.0017965091 -0.0010642502 -0.00074748951 -0.0010262351 -0.0016935964 -0.002380582 -0.0028799614 -0.0032078146 -0.003366058][-0.0034044292 -0.0033979958 -0.003354857 -0.0031861849 -0.0027313624 -0.0018425839 -0.00058185146 0.00063338154 0.0011999325 0.00079622841 -0.00033722445 -0.0015986883 -0.0025449516 -0.0031066504 -0.0033441964][-0.0033554498 -0.0033400541 -0.0032042442 -0.0027924171 -0.0018854089 -0.00031675049 0.0017655382 0.0037373153 0.004726165 0.0042224554 0.0024677718 0.00030143815 -0.0015206961 -0.0026960752 -0.0032368][-0.0031635142 -0.0030696704 -0.002703378 -0.0018165457 -0.00016601081 0.0023607544 0.0054595806 0.0082904231 0.00972943 0.0090867 0.0065823719 0.0032611031 0.0002109881 -0.001919501 -0.0029954128][-0.0028397762 -0.0025895517 -0.0018184991 -0.00021140859 0.0024306749 0.0060789976 0.010247584 0.013935284 0.015812119 0.014988982 0.011621024 0.0069516711 0.0024403327 -0.00085574645 -0.0026336522][-0.0025110058 -0.0020736596 -0.00082210451 0.0016225672 0.0053822715 0.010215196 0.015426583 0.019882286 0.022146065 0.02119395 0.017056052 0.011053111 0.0049869604 0.00038697524 -0.0022031271][-0.0022277012 -0.0017125231 -0.00012307358 0.0030152646 0.007775153 0.01366724 0.019725082 0.024675114 0.027110033 0.026018795 0.021352448 0.014406796 0.007162692 0.0015002594 -0.0018015938][-0.0019846098 -0.0014870296 0.00020229258 0.003682771 0.0090256166 0.015533374 0.022004785 0.027056824 0.029403169 0.028139595 0.023235532 0.015926229 0.0082118176 0.0020862103 -0.0015754242][-0.0019238091 -0.0014563981 0.00014590123 0.0035425227 0.0088724876 0.015351604 0.021660512 0.026375636 0.028402913 0.027008321 0.022235278 0.015212429 0.0078040268 0.0019198845 -0.001624917][-0.0021182545 -0.001727478 -0.00037406012 0.00256424 0.0072766263 0.013039918 0.018625131 0.022682404 0.024309736 0.022930328 0.018683888 0.012567951 0.0061616311 0.0011072701 -0.0019234171][-0.0024428796 -0.002152472 -0.0011801913 0.001014922 0.0046746125 0.0092597082 0.013766044 0.017004784 0.018232923 0.01700416 0.013559175 0.008745661 0.0037775862 -8.3515188e-05 -0.0023585453][-0.0027621896 -0.0025554255 -0.0019602641 -0.00056718313 0.0018850558 0.0050947024 0.0083507262 0.01072301 0.011597967 0.010625858 0.0080897594 0.0046727033 0.0012396106 -0.0013361706 -0.0027991042][-0.0030733459 -0.0029120091 -0.0025611096 -0.0017716329 -0.00033784145 0.0016173406 0.0036758224 0.0052031297 0.0057298234 0.0050140107 0.0033143482 0.0011453431 -0.00092606712 -0.0023727193 -0.0031392311][-0.0033111693 -0.0032178944 -0.0030208726 -0.002617657 -0.0018911402 -0.00086009759 0.0002592192 0.001081404 0.0013076516 0.00082707894 -0.00016753352 -0.0013388477 -0.0023729687 -0.0030169813 -0.0033246325][-0.0033999577 -0.0033779994 -0.0033102052 -0.0031434023 -0.0028348421 -0.0023871297 -0.0018986849 -0.0015536027 -0.0014932033 -0.0017505008 -0.0022156429 -0.0027106931 -0.0031016786 -0.0033077758 -0.0033929569]]...]
INFO - root - 2017-12-09 17:18:26.245717: step 44110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:42m:38s remains)
INFO - root - 2017-12-09 17:18:34.890333: step 44120, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 67h:23m:40s remains)
INFO - root - 2017-12-09 17:18:43.545562: step 44130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:20m:00s remains)
INFO - root - 2017-12-09 17:18:51.947196: step 44140, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 68h:50m:10s remains)
INFO - root - 2017-12-09 17:19:00.632699: step 44150, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:35m:21s remains)
INFO - root - 2017-12-09 17:19:09.279120: step 44160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:21m:13s remains)
INFO - root - 2017-12-09 17:19:18.077692: step 44170, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:41m:22s remains)
INFO - root - 2017-12-09 17:19:26.772368: step 44180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:33m:46s remains)
INFO - root - 2017-12-09 17:19:35.328290: step 44190, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:00m:24s remains)
INFO - root - 2017-12-09 17:19:43.994611: step 44200, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 72h:17m:37s remains)
2017-12-09 17:19:44.866983: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3498618 0.34003055 0.3281129 0.31599262 0.30361116 0.29243436 0.2811774 0.27053005 0.25949806 0.24813703 0.23648766 0.22602791 0.21612878 0.20686099 0.19804989][0.3754651 0.36888182 0.35935551 0.34882987 0.33676621 0.32543504 0.31378746 0.30197546 0.28947958 0.27622771 0.26227441 0.24834001 0.2343221 0.22187108 0.2097943][0.39364859 0.39247885 0.38714987 0.37993008 0.37002122 0.35890073 0.34633514 0.33332267 0.31909782 0.30369329 0.28723696 0.27020812 0.25230244 0.2356011 0.21975628][0.41048509 0.4148781 0.41439837 0.41080558 0.40283743 0.39220262 0.37915856 0.3649804 0.34921324 0.33188742 0.313328 0.2930454 0.2712785 0.250285 0.23058283][0.42507774 0.43545911 0.43896386 0.43844268 0.43274009 0.42232764 0.40831217 0.39283508 0.37568858 0.35735178 0.33731094 0.31548014 0.2911911 0.26637864 0.24259473][0.43410844 0.44967484 0.45660263 0.45825517 0.45400786 0.44394624 0.4285973 0.41140214 0.39309916 0.37431765 0.35387912 0.33130035 0.30601352 0.27916643 0.25282356][0.43693671 0.456831 0.46584013 0.46867961 0.46477723 0.45408386 0.43709436 0.41871437 0.39968717 0.38125455 0.36196545 0.34034556 0.31565216 0.28834254 0.26055521][0.43459141 0.45639277 0.46514684 0.46763512 0.46308619 0.45138967 0.43308249 0.41347429 0.39416137 0.37625468 0.35815507 0.33876419 0.31626832 0.29021534 0.26303509][0.4267467 0.45001462 0.45867005 0.4597719 0.45367149 0.44044083 0.42140284 0.40133086 0.38163307 0.36452544 0.348016 0.33056894 0.31024528 0.28653952 0.26106372][0.40847322 0.4331803 0.44180286 0.44253874 0.43681088 0.42382523 0.40465936 0.38407937 0.36484867 0.34859386 0.33295843 0.31748852 0.29968417 0.27851933 0.25548375][0.3810885 0.40482169 0.41261053 0.41321796 0.40778852 0.3964704 0.37987715 0.36104181 0.34354356 0.3290562 0.31528184 0.30089536 0.28445926 0.266077 0.24605751][0.34858763 0.3705039 0.37675509 0.37719834 0.37283835 0.36360329 0.3498162 0.33396047 0.3196407 0.30740225 0.295715 0.28336 0.26894084 0.25284818 0.23558125][0.31165469 0.33041745 0.33526602 0.33586088 0.332851 0.32612413 0.31590366 0.30409181 0.29293668 0.28308222 0.27351123 0.2633118 0.25121111 0.23781171 0.22365376][0.27373967 0.2895261 0.29332674 0.29437867 0.29324338 0.28931215 0.2828826 0.27461118 0.26661083 0.25935897 0.25199974 0.24379216 0.23406786 0.22353525 0.21262114][0.24014609 0.25266814 0.25452313 0.25540766 0.25528 0.25371376 0.2505312 0.24578083 0.24118055 0.23645438 0.23139192 0.22528321 0.21799731 0.21027914 0.20219702]]...]
INFO - root - 2017-12-09 17:19:53.160687: step 44210, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 69h:45m:34s remains)
INFO - root - 2017-12-09 17:20:01.830256: step 44220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 69h:01m:17s remains)
INFO - root - 2017-12-09 17:20:10.478589: step 44230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:10m:40s remains)
INFO - root - 2017-12-09 17:20:18.939505: step 44240, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 69h:14m:49s remains)
INFO - root - 2017-12-09 17:20:27.539495: step 44250, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 70h:14m:28s remains)
INFO - root - 2017-12-09 17:20:35.896676: step 44260, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:23m:47s remains)
INFO - root - 2017-12-09 17:20:44.356627: step 44270, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 68h:08m:57s remains)
INFO - root - 2017-12-09 17:20:52.964529: step 44280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 68h:45m:24s remains)
INFO - root - 2017-12-09 17:21:01.649546: step 44290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:14m:31s remains)
INFO - root - 2017-12-09 17:21:10.316342: step 44300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:31m:53s remains)
2017-12-09 17:21:11.247817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034108318 -0.0034097321 -0.0034092285 -0.0034087328 -0.0034086688 -0.0034087249 -0.0034088127 -0.0034090586 -0.0034093808 -0.00340954 -0.0034097442 -0.0034099461 -0.0034100434 -0.0034101063 -0.0034101482][-0.0034122011 -0.0034106902 -0.00341003 -0.0034095324 -0.0034096397 -0.0034100558 -0.0034104248 -0.0034109901 -0.0034114048 -0.0034116369 -0.0034117145 -0.0034117741 -0.0034119927 -0.003411903 -0.003411656][-0.0034133515 -0.0034118609 -0.003410998 -0.0034103699 -0.0034100737 -0.0034102877 -0.0034111259 -0.0034120665 -0.0034126409 -0.0034127622 -0.0034127666 -0.0034133135 -0.0034137629 -0.0034136353 -0.0034131345][-0.0034110427 -0.003410554 -0.003410893 -0.0034106954 -0.0034100418 -0.0034094942 -0.0034097575 -0.0034091566 -0.0034079966 -0.0034066662 -0.0034068516 -0.0034089428 -0.0034107731 -0.0034121422 -0.003412598][-0.003404035 -0.0034065624 -0.0034096041 -0.00341017 -0.0034093049 -0.0034062413 -0.0034021577 -0.0033962743 -0.0033906586 -0.0033874749 -0.0033888733 -0.0033944903 -0.0034006443 -0.0034055999 -0.0034088725][-0.0033878337 -0.0033959497 -0.0034037274 -0.0034059288 -0.0034042345 -0.0033952398 -0.0033818611 -0.0033650205 -0.0033507403 -0.0033443295 -0.0033494662 -0.0033625537 -0.0033777005 -0.003390379 -0.0033992771][-0.0033571424 -0.0033748823 -0.0033896079 -0.0033938342 -0.0033882712 -0.0033684417 -0.003339418 -0.003306482 -0.0032798948 -0.0032679948 -0.003279957 -0.0033069 -0.0033386918 -0.0033649297 -0.0033831552][-0.0033084843 -0.0033402478 -0.0033646214 -0.0033706801 -0.003357515 -0.0033247706 -0.0032784443 -0.0032315755 -0.0031962825 -0.0031827625 -0.0032030852 -0.0032449367 -0.0032952242 -0.0033372722 -0.0033657877][-0.0032642572 -0.003305726 -0.0033363714 -0.0033427388 -0.0033218549 -0.00327975 -0.0032271082 -0.0031822459 -0.0031521665 -0.0031448323 -0.0031709103 -0.003219835 -0.0032777458 -0.0033249517 -0.0033562807][-0.003252615 -0.0032928174 -0.003321239 -0.0033250437 -0.0033019751 -0.003262697 -0.0032194622 -0.0031891731 -0.0031735303 -0.0031763297 -0.0032039504 -0.0032469607 -0.0032960966 -0.003334525 -0.0033602118][-0.0032793796 -0.0033090443 -0.0033283646 -0.0033307783 -0.0033142306 -0.0032891959 -0.0032644728 -0.0032506443 -0.0032469525 -0.0032551684 -0.0032776622 -0.0033070287 -0.0033372235 -0.0033592181 -0.0033743561][-0.0033305881 -0.0033456867 -0.0033554472 -0.0033576831 -0.0033507606 -0.0033399896 -0.0033300729 -0.0033267734 -0.003327894 -0.0033344852 -0.0033472914 -0.0033620021 -0.0033763293 -0.0033860044 -0.0033929502][-0.0033744713 -0.0033808185 -0.0033849038 -0.0033862663 -0.0033847834 -0.0033820001 -0.0033790087 -0.0033790248 -0.0033805259 -0.0033833468 -0.0033879098 -0.0033932887 -0.0033985078 -0.0034017207 -0.0034041805][-0.00339562 -0.0033972773 -0.0033995912 -0.0034012459 -0.0034021104 -0.0034023458 -0.0034016902 -0.0034013523 -0.0034013675 -0.0034018136 -0.0034023621 -0.0034034527 -0.0034049058 -0.0034056869 -0.003406541][-0.0034006971 -0.0034001737 -0.0034010415 -0.0034023544 -0.0034036729 -0.0034046886 -0.0034046727 -0.0034045777 -0.0034046562 -0.0034047824 -0.0034047568 -0.0034049659 -0.0034056494 -0.0034056446 -0.0034058918]]...]
INFO - root - 2017-12-09 17:21:19.410822: step 44310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:32m:29s remains)
INFO - root - 2017-12-09 17:21:27.817141: step 44320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:18m:33s remains)
INFO - root - 2017-12-09 17:21:36.474021: step 44330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 70h:19m:04s remains)
INFO - root - 2017-12-09 17:21:45.042700: step 44340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:07m:43s remains)
INFO - root - 2017-12-09 17:21:53.708512: step 44350, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 70h:20m:06s remains)
INFO - root - 2017-12-09 17:22:02.377606: step 44360, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:17m:10s remains)
INFO - root - 2017-12-09 17:22:11.171214: step 44370, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 70h:02m:07s remains)
INFO - root - 2017-12-09 17:22:19.961742: step 44380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:37m:37s remains)
INFO - root - 2017-12-09 17:22:28.614715: step 44390, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 67h:06m:32s remains)
INFO - root - 2017-12-09 17:22:37.219954: step 44400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:22m:27s remains)
2017-12-09 17:22:38.192154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033589709 -0.0033561853 -0.0033567278 -0.0033576435 -0.0033588302 -0.0033598454 -0.0033605495 -0.0033610181 -0.0033613225 -0.0033614247 -0.0033614479 -0.003361386 -0.0033612747 -0.0033611939 -0.0033611679][-0.0033565774 -0.0033535366 -0.0033541622 -0.0033551983 -0.003356558 -0.0033576388 -0.0033583026 -0.0033587113 -0.0033589834 -0.0033590754 -0.0033590752 -0.0033589888 -0.0033588195 -0.0033586959 -0.0033586456][-0.0033566814 -0.003353595 -0.0033542756 -0.0033553115 -0.0033566316 -0.0033576551 -0.0033582302 -0.003358545 -0.0033587809 -0.0033589315 -0.0033589965 -0.0033589376 -0.0033587557 -0.0033585911 -0.0033584749][-0.0033566009 -0.0033534847 -0.0033541569 -0.0033551792 -0.0033564805 -0.003357481 -0.0033580426 -0.0033583592 -0.0033586405 -0.0033588544 -0.0033589685 -0.0033589355 -0.0033587471 -0.0033585513 -0.0033583948][-0.0033565762 -0.0033534463 -0.0033540593 -0.0033550281 -0.0033562877 -0.0033572505 -0.0033577722 -0.0033581094 -0.0033584505 -0.0033587406 -0.0033588968 -0.0033588908 -0.0033587327 -0.0033585182 -0.0033583299][-0.0033565848 -0.0033534046 -0.0033539708 -0.0033549005 -0.0033560956 -0.0033569892 -0.0033574542 -0.0033577781 -0.0033581427 -0.0033584852 -0.003358712 -0.0033587802 -0.0033586673 -0.0033584572 -0.0033582395][-0.0033564374 -0.0033533836 -0.0033539957 -0.0033549131 -0.0033560633 -0.003356863 -0.0033572456 -0.0033575143 -0.0033578502 -0.003358206 -0.0033584761 -0.0033586274 -0.0033586002 -0.003358436 -0.0033581895][-0.0033562705 -0.003353446 -0.0033541224 -0.0033550123 -0.0033561508 -0.0033568903 -0.0033571913 -0.0033573697 -0.0033576179 -0.0033579248 -0.0033581869 -0.0033583776 -0.0033584274 -0.0033583432 -0.003358148][-0.0033559809 -0.0033533191 -0.0033541243 -0.003354996 -0.0033561767 -0.0033569413 -0.0033572144 -0.003357345 -0.0033575194 -0.0033577643 -0.0033579872 -0.0033581487 -0.0033582312 -0.0033582058 -0.0033580558][-0.0033556512 -0.0033530837 -0.0033540684 -0.0033549911 -0.0033562575 -0.0033570658 -0.0033573636 -0.0033575059 -0.0033576335 -0.0033578132 -0.0033579876 -0.0033581161 -0.0033582072 -0.0033581834 -0.0033580181][-0.0033552779 -0.0033526309 -0.0033538151 -0.0033547904 -0.0033561669 -0.003357067 -0.0033574125 -0.0033575888 -0.0033577208 -0.0033578924 -0.0033580568 -0.0033581783 -0.003358312 -0.0033582596 -0.0033580093][-0.0033551303 -0.0033522421 -0.0033535003 -0.003354569 -0.0033560495 -0.0033570116 -0.0033573871 -0.0033575878 -0.003357715 -0.0033579033 -0.0033581187 -0.0033582477 -0.003358393 -0.0033583029 -0.003357948][-0.0033549871 -0.00335183 -0.0033531217 -0.0033542949 -0.0033558393 -0.0033568377 -0.00335726 -0.0033574938 -0.0033576242 -0.003357793 -0.0033579944 -0.003358118 -0.0033582218 -0.00335813 -0.0033578146][-0.0033550498 -0.0033517354 -0.0033529871 -0.0033542127 -0.0033557939 -0.003356799 -0.0033572284 -0.0033574575 -0.003357542 -0.0033576882 -0.0033578726 -0.0033580041 -0.003358098 -0.0033581359 -0.0033581064][-0.0033553743 -0.0033519363 -0.0033530805 -0.0033543245 -0.0033558798 -0.0033568682 -0.0033572956 -0.0033574973 -0.0033575175 -0.0033575834 -0.0033577185 -0.0033578565 -0.003357931 -0.0033581522 -0.0033585543]]...]
INFO - root - 2017-12-09 17:22:46.475875: step 44410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:39m:35s remains)
INFO - root - 2017-12-09 17:22:54.924245: step 44420, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:02m:11s remains)
INFO - root - 2017-12-09 17:23:03.386775: step 44430, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 66h:53m:27s remains)
INFO - root - 2017-12-09 17:23:11.805325: step 44440, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 66h:18m:49s remains)
INFO - root - 2017-12-09 17:23:20.418426: step 44450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:05m:41s remains)
INFO - root - 2017-12-09 17:23:28.971493: step 44460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 69h:57m:23s remains)
INFO - root - 2017-12-09 17:23:37.657105: step 44470, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 69h:38m:12s remains)
INFO - root - 2017-12-09 17:23:46.245449: step 44480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:26m:02s remains)
INFO - root - 2017-12-09 17:23:55.005063: step 44490, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 71h:32m:08s remains)
INFO - root - 2017-12-09 17:24:03.695635: step 44500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:22m:28s remains)
2017-12-09 17:24:04.590664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033710168 -0.0033710769 -0.0033755042 -0.003381127 -0.0033862097 -0.0033900961 -0.0033928037 -0.0033937918 -0.0033927211 -0.0033911348 -0.0033893785 -0.0033882116 -0.0033876875 -0.0033877874 -0.0033880456][-0.0033597876 -0.0033597895 -0.0033651264 -0.003372537 -0.0033793831 -0.003384647 -0.0033884337 -0.0033904423 -0.0033900449 -0.0033882875 -0.0033862328 -0.003384691 -0.0033835624 -0.0033831224 -0.0033831482][-0.0033515643 -0.0033511755 -0.0033567287 -0.003365447 -0.0033739379 -0.0033806006 -0.003385372 -0.0033888675 -0.0033892717 -0.0033879045 -0.0033858512 -0.0033834476 -0.003381609 -0.0033803424 -0.0033798157][-0.0033510644 -0.0033500865 -0.0033546553 -0.0033630976 -0.0033717332 -0.0033785305 -0.0033834919 -0.0033873797 -0.0033881511 -0.0033870013 -0.0033849885 -0.003382524 -0.0033799715 -0.0033780355 -0.003377001][-0.0033597352 -0.0033580256 -0.0033611404 -0.0033679486 -0.0033752436 -0.0033810441 -0.0033844893 -0.00338701 -0.003387246 -0.0033855808 -0.0033830453 -0.0033802718 -0.0033776492 -0.0033755864 -0.0033742841][-0.0033713388 -0.0033693621 -0.003370977 -0.0033760208 -0.00338147 -0.0033858588 -0.0033873133 -0.003388383 -0.0033878267 -0.0033852176 -0.003381992 -0.003379124 -0.0033762206 -0.0033739058 -0.0033724769][-0.0033806574 -0.0033779379 -0.0033783843 -0.0033819589 -0.0033856994 -0.003388742 -0.0033892365 -0.0033891092 -0.0033878533 -0.003385368 -0.0033825352 -0.0033795759 -0.0033765435 -0.00337376 -0.0033720406][-0.0033863003 -0.0033839808 -0.003384127 -0.0033863487 -0.0033885641 -0.0033902435 -0.0033898016 -0.003388823 -0.0033869348 -0.0033844507 -0.0033817978 -0.0033793214 -0.0033767226 -0.0033738909 -0.0033719246][-0.0033892805 -0.0033874365 -0.0033872707 -0.0033887762 -0.0033898905 -0.0033902486 -0.0033891618 -0.0033877331 -0.0033855571 -0.0033825431 -0.0033796756 -0.0033778534 -0.0033758266 -0.0033732173 -0.0033714408][-0.00338901 -0.0033872933 -0.003387263 -0.0033884889 -0.0033890891 -0.003388861 -0.0033874365 -0.003385656 -0.0033832223 -0.0033807061 -0.0033783894 -0.0033766236 -0.003374781 -0.0033729663 -0.0033717114][-0.0033854984 -0.0033834418 -0.0033833233 -0.0033844702 -0.003385559 -0.0033856004 -0.0033842702 -0.0033830174 -0.0033808192 -0.0033785081 -0.0033767035 -0.0033752441 -0.0033737405 -0.0033723132 -0.0033711335][-0.0033809866 -0.0033784923 -0.0033780572 -0.0033788159 -0.003379476 -0.0033797123 -0.0033790721 -0.0033783522 -0.0033766639 -0.0033748383 -0.0033736913 -0.0033731565 -0.0033725337 -0.0033714396 -0.0033702631][-0.0033778043 -0.0033750923 -0.003374734 -0.0033750283 -0.0033751351 -0.0033748783 -0.0033739898 -0.0033736441 -0.0033725442 -0.0033711288 -0.0033702774 -0.0033704413 -0.0033704995 -0.0033702506 -0.0033693083][-0.0033763929 -0.0033737191 -0.0033733554 -0.003373526 -0.0033734804 -0.003372899 -0.0033718906 -0.0033712306 -0.0033702694 -0.0033693665 -0.0033688417 -0.0033686315 -0.0033686399 -0.0033690468 -0.0033686804][-0.0033764041 -0.0033739137 -0.0033737188 -0.0033737314 -0.0033733922 -0.0033725288 -0.0033715034 -0.0033704548 -0.0033692194 -0.0033686641 -0.0033683593 -0.0033685239 -0.0033687283 -0.0033691328 -0.003368919]]...]
INFO - root - 2017-12-09 17:24:12.953136: step 44510, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:21m:33s remains)
INFO - root - 2017-12-09 17:24:21.595945: step 44520, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 69h:09m:35s remains)
INFO - root - 2017-12-09 17:24:30.248876: step 44530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:23m:42s remains)
INFO - root - 2017-12-09 17:24:38.762475: step 44540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:44m:19s remains)
INFO - root - 2017-12-09 17:24:47.391182: step 44550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:44m:13s remains)
INFO - root - 2017-12-09 17:24:55.939785: step 44560, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 66h:56m:21s remains)
INFO - root - 2017-12-09 17:25:04.478204: step 44570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:56m:46s remains)
INFO - root - 2017-12-09 17:25:13.222542: step 44580, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 70h:20m:31s remains)
INFO - root - 2017-12-09 17:25:21.809569: step 44590, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:00m:59s remains)
INFO - root - 2017-12-09 17:25:30.464643: step 44600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:29m:15s remains)
2017-12-09 17:25:31.288455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033621991 -0.0033591315 -0.0033588696 -0.0033588994 -0.0033589494 -0.003359115 -0.0033593085 -0.0033594857 -0.0033596198 -0.0033596724 -0.003359721 -0.0033597187 -0.0033597094 -0.0033596812 -0.003359644][-0.0033601481 -0.0033567047 -0.0033563359 -0.0033563608 -0.0033564437 -0.0033565597 -0.0033567119 -0.0033568821 -0.003357053 -0.0033571252 -0.0033571625 -0.0033571606 -0.0033571327 -0.0033571215 -0.0033571359][-0.0033602195 -0.0033566186 -0.0033561564 -0.0033560814 -0.0033560647 -0.0033560069 -0.0033560393 -0.0033561455 -0.0033562756 -0.0033564097 -0.0033565115 -0.003356596 -0.0033566698 -0.0033567841 -0.0033569199][-0.0033599148 -0.003356199 -0.0033556847 -0.0033554947 -0.0033553771 -0.0033552621 -0.0033552432 -0.003355271 -0.0033553191 -0.0033554784 -0.00335556 -0.0033557105 -0.0033559105 -0.0033561767 -0.0033565026][-0.0033595117 -0.0033557031 -0.0033553129 -0.0033552349 -0.0033551631 -0.0033550048 -0.0033549587 -0.0033548472 -0.003354646 -0.0033546363 -0.0033546342 -0.0033547867 -0.0033550612 -0.0033554931 -0.0033560307][-0.0033592118 -0.0033553571 -0.0033550828 -0.0033551482 -0.0033552221 -0.0033550239 -0.0033548421 -0.0033546125 -0.0033542369 -0.0033540663 -0.0033539061 -0.0033539736 -0.0033543333 -0.0033548714 -0.0033555415][-0.0033586631 -0.0033551536 -0.0033551657 -0.0033555478 -0.0033558516 -0.00335569 -0.0033553343 -0.0033549324 -0.0033543636 -0.0033539529 -0.0033534998 -0.0033534141 -0.0033537482 -0.0033543103 -0.0033550139][-0.0033582815 -0.0033552647 -0.003355721 -0.0033564717 -0.0033570603 -0.0033571222 -0.003356687 -0.0033560568 -0.0033551876 -0.0033545136 -0.0033537399 -0.0033533892 -0.003353541 -0.0033540451 -0.0033546975][-0.0033581506 -0.0033554903 -0.003356426 -0.0033575627 -0.0033584451 -0.0033587632 -0.0033583052 -0.003357518 -0.003356471 -0.0033555673 -0.0033545897 -0.003353968 -0.0033539017 -0.0033542158 -0.0033547115][-0.0033584137 -0.0033558691 -0.0033571119 -0.003358426 -0.0033595173 -0.0033600365 -0.0033598053 -0.0033590693 -0.0033579876 -0.003356914 -0.0033558058 -0.003355006 -0.0033547108 -0.0033548034 -0.0033550749][-0.0033588933 -0.0033563669 -0.0033577248 -0.0033589741 -0.0033600533 -0.0033606065 -0.0033604384 -0.0033597795 -0.003358813 -0.0033577476 -0.0033566565 -0.0033558942 -0.0033555422 -0.0033554772 -0.0033555215][-0.0033596845 -0.0033569837 -0.0033581667 -0.0033591983 -0.0033600477 -0.0033604857 -0.003360216 -0.0033596116 -0.0033587976 -0.0033579248 -0.0033570477 -0.0033565066 -0.0033562249 -0.00335611 -0.0033560465][-0.0033603914 -0.0033572875 -0.0033582302 -0.0033589506 -0.0033594822 -0.0033597318 -0.0033594533 -0.0033590107 -0.0033583571 -0.0033577215 -0.0033571913 -0.0033568917 -0.0033567324 -0.0033566419 -0.0033565648][-0.0033608654 -0.0033574966 -0.0033582123 -0.00335869 -0.003358979 -0.0033591087 -0.0033588898 -0.0033585306 -0.0033579634 -0.0033574726 -0.0033572146 -0.0033571054 -0.0033570325 -0.0033569729 -0.0033569364][-0.0033611285 -0.0033575774 -0.0033579974 -0.0033583031 -0.0033584742 -0.00335852 -0.0033583457 -0.003358043 -0.0033576125 -0.0033573222 -0.0033572237 -0.0033572267 -0.0033572055 -0.0033571937 -0.0033572023]]...]
INFO - root - 2017-12-09 17:25:39.648471: step 44610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:26m:15s remains)
INFO - root - 2017-12-09 17:25:48.283798: step 44620, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 70h:20m:28s remains)
INFO - root - 2017-12-09 17:25:56.969737: step 44630, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 65h:29m:26s remains)
INFO - root - 2017-12-09 17:26:05.397083: step 44640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:45m:30s remains)
INFO - root - 2017-12-09 17:26:13.907769: step 44650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:57m:43s remains)
INFO - root - 2017-12-09 17:26:22.258949: step 44660, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 66h:00m:36s remains)
INFO - root - 2017-12-09 17:26:30.987713: step 44670, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 72h:54m:37s remains)
INFO - root - 2017-12-09 17:26:39.655791: step 44680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:13m:07s remains)
INFO - root - 2017-12-09 17:26:48.359494: step 44690, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 71h:32m:42s remains)
INFO - root - 2017-12-09 17:26:57.007272: step 44700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:16m:38s remains)
2017-12-09 17:26:57.851547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033803154 -0.0033777053 -0.003376368 -0.0033757573 -0.0033751861 -0.0033747645 -0.0033745347 -0.0033745843 -0.0033745836 -0.0033741912 -0.0033736557 -0.003372825 -0.0033722222 -0.0033717579 -0.0033710848][-0.003378093 -0.0033761377 -0.00337529 -0.003375141 -0.0033750823 -0.0033751426 -0.0033752248 -0.0033755144 -0.0033757878 -0.0033756837 -0.0033752415 -0.0033747163 -0.0033745382 -0.0033739498 -0.0033731719][-0.0033770807 -0.0033754841 -0.0033748189 -0.0033747316 -0.0033747929 -0.003374926 -0.0033751866 -0.0033756753 -0.0033762162 -0.0033763633 -0.0033760711 -0.0033754907 -0.0033751307 -0.0033743572 -0.0033734895][-0.0033700743 -0.0033685144 -0.0033679439 -0.0033676287 -0.0033676759 -0.0033678578 -0.0033683672 -0.0033690343 -0.0033696806 -0.0033699707 -0.0033695796 -0.0033689402 -0.0033684303 -0.003367515 -0.0033666198][-0.0033639581 -0.0033622165 -0.0033615583 -0.0033607413 -0.003360241 -0.0033600389 -0.0033601685 -0.0033605823 -0.0033611648 -0.0033613471 -0.003360884 -0.0033600673 -0.0033595029 -0.0033587234 -0.0033580784][-0.0033596668 -0.0033575296 -0.0033565664 -0.003355273 -0.0033543056 -0.0033535396 -0.0033530944 -0.0033531582 -0.0033537364 -0.0033540756 -0.0033537359 -0.0033531259 -0.0033524721 -0.0033518146 -0.0033512944][-0.0033593231 -0.0033570675 -0.0033562626 -0.003355097 -0.0033536614 -0.003352293 -0.0033513145 -0.0033510895 -0.0033515 -0.0033518877 -0.0033519242 -0.0033515405 -0.0033508616 -0.0033501657 -0.0033497137][-0.0033616154 -0.003359329 -0.0033585089 -0.0033569443 -0.0033550677 -0.0033531443 -0.00335151 -0.003350721 -0.0033507845 -0.0033511093 -0.003351232 -0.0033511103 -0.0033506337 -0.00335016 -0.0033499089][-0.0033612412 -0.0033596014 -0.0033594351 -0.0033582798 -0.0033565753 -0.0033547152 -0.0033530719 -0.003351968 -0.003351571 -0.0033515792 -0.0033516542 -0.0033516244 -0.0033513124 -0.0033509512 -0.0033506718][-0.003360797 -0.0033595187 -0.003359661 -0.0033589783 -0.003357552 -0.0033556952 -0.0033539354 -0.0033526514 -0.0033518951 -0.0033516923 -0.0033519648 -0.0033521345 -0.0033520442 -0.0033518365 -0.0033515636][-0.0033593762 -0.0033577092 -0.00335763 -0.0033570526 -0.0033558807 -0.0033543578 -0.0033529755 -0.003352019 -0.0033515738 -0.0033516949 -0.0033520034 -0.0033521913 -0.0033520903 -0.0033519387 -0.0033517261][-0.003358714 -0.00335702 -0.0033570391 -0.003356582 -0.0033554542 -0.0033542237 -0.0033532113 -0.0033523901 -0.0033518712 -0.0033518658 -0.0033520262 -0.0033521324 -0.0033520435 -0.003351917 -0.0033517554][-0.00335788 -0.0033558947 -0.0033554565 -0.0033546132 -0.0033533587 -0.0033522286 -0.0033513822 -0.0033507061 -0.0033502181 -0.0033501883 -0.0033502504 -0.0033503487 -0.0033503631 -0.0033503028 -0.0033501522][-0.0033568337 -0.0033545087 -0.003353982 -0.0033532334 -0.0033521964 -0.0033512271 -0.0033505866 -0.0033501331 -0.0033498525 -0.0033498895 -0.0033500025 -0.003350182 -0.0033502665 -0.0033502076 -0.0033500448][-0.0033561338 -0.0033537289 -0.0033531727 -0.0033524998 -0.0033515571 -0.0033507429 -0.0033500986 -0.0033496707 -0.003349466 -0.00334947 -0.0033495168 -0.0033496083 -0.0033496311 -0.0033495699 -0.0033494702]]...]
INFO - root - 2017-12-09 17:27:06.249619: step 44710, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 62h:14m:52s remains)
INFO - root - 2017-12-09 17:27:14.531181: step 44720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:59m:56s remains)
INFO - root - 2017-12-09 17:27:23.088353: step 44730, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:26m:32s remains)
INFO - root - 2017-12-09 17:27:31.523534: step 44740, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 69h:35m:46s remains)
INFO - root - 2017-12-09 17:27:40.294288: step 44750, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:34m:22s remains)
INFO - root - 2017-12-09 17:27:49.030068: step 44760, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:04m:57s remains)
INFO - root - 2017-12-09 17:27:57.779657: step 44770, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 76h:47m:31s remains)
INFO - root - 2017-12-09 17:28:06.455330: step 44780, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 71h:01m:23s remains)
INFO - root - 2017-12-09 17:28:15.167471: step 44790, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 70h:29m:01s remains)
INFO - root - 2017-12-09 17:28:23.820685: step 44800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:44m:21s remains)
2017-12-09 17:28:24.775582: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19982314 0.19502686 0.18705128 0.17509611 0.16096069 0.14566451 0.13446401 0.12942627 0.12896316 0.13829537 0.15722074 0.18703045 0.21561986 0.24185723 0.26667848][0.21989653 0.22312669 0.22164115 0.21239629 0.19998696 0.18255934 0.1661751 0.1544725 0.14679256 0.14999253 0.16383816 0.19013596 0.21742016 0.24410002 0.27114281][0.24584478 0.25717291 0.26287061 0.26209351 0.25637069 0.24127036 0.22420181 0.20697543 0.19209376 0.18458575 0.1886622 0.20575343 0.22616595 0.24878438 0.27387127][0.28088868 0.29942966 0.31283358 0.32057247 0.32307461 0.31426716 0.30048355 0.28061268 0.261123 0.24483846 0.23845804 0.2434597 0.25421557 0.26923195 0.28800812][0.33440122 0.3592715 0.37812814 0.39320076 0.40371493 0.40180492 0.39222744 0.3720232 0.34826076 0.32496241 0.30850065 0.30097079 0.30064136 0.30432647 0.31555402][0.39950046 0.42761379 0.4499557 0.47151762 0.48956925 0.49658617 0.49364236 0.47510785 0.44872755 0.41812855 0.39124045 0.36969203 0.35518774 0.34629315 0.347113][0.46117839 0.49303865 0.51891977 0.54568326 0.56994724 0.58393353 0.5869211 0.57207125 0.54438818 0.50800461 0.47199544 0.43800318 0.41008613 0.38780811 0.37798998][0.51295197 0.5473249 0.57420021 0.60434818 0.63217223 0.65158856 0.65999264 0.64810854 0.62167728 0.58242905 0.54086804 0.4976401 0.45846647 0.42464817 0.4041737][0.5448854 0.58168942 0.610568 0.64243495 0.67266077 0.69567007 0.70713711 0.69811022 0.67239612 0.63263631 0.58870566 0.54004943 0.49389449 0.45144767 0.42270246][0.55950767 0.59796691 0.62564492 0.65981919 0.69182122 0.71694523 0.73033959 0.72290039 0.69814175 0.65728337 0.61130774 0.55925572 0.50808215 0.45975167 0.42390892][0.56488246 0.600362 0.62363404 0.6558553 0.68623674 0.71340877 0.72816515 0.72272307 0.69910747 0.65775818 0.60900086 0.55205286 0.49548233 0.44257414 0.401554][0.5686233 0.59913385 0.615281 0.6411702 0.66432244 0.687495 0.69925845 0.69381273 0.67034632 0.62863135 0.57745224 0.5176273 0.45762137 0.40143684 0.35669294][0.5674113 0.59266025 0.60046792 0.61811721 0.63316524 0.64861584 0.65396994 0.64488721 0.61981946 0.57740271 0.524906 0.46310097 0.40174767 0.34466627 0.29830259][0.56230783 0.58163387 0.58078355 0.58815843 0.5929817 0.59937388 0.59721631 0.58322716 0.55507505 0.5113678 0.45800716 0.39604878 0.33494273 0.27879512 0.23344855][0.55205542 0.56682348 0.55927849 0.55669862 0.55028951 0.54637188 0.53481114 0.51428735 0.48247281 0.43703693 0.38354117 0.32297865 0.26420161 0.21048741 0.16740988]]...]
INFO - root - 2017-12-09 17:28:33.324235: step 44810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:06m:27s remains)
INFO - root - 2017-12-09 17:28:41.731941: step 44820, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:08m:18s remains)
INFO - root - 2017-12-09 17:28:50.202535: step 44830, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 68h:05m:41s remains)
INFO - root - 2017-12-09 17:28:58.728138: step 44840, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 71h:27m:39s remains)
INFO - root - 2017-12-09 17:29:07.439542: step 44850, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:01m:57s remains)
INFO - root - 2017-12-09 17:29:16.155726: step 44860, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:48m:31s remains)
INFO - root - 2017-12-09 17:29:24.846417: step 44870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 70h:56m:34s remains)
INFO - root - 2017-12-09 17:29:33.635875: step 44880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:20m:37s remains)
INFO - root - 2017-12-09 17:29:42.366367: step 44890, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 72h:02m:48s remains)
INFO - root - 2017-12-09 17:29:50.998554: step 44900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:09m:37s remains)
2017-12-09 17:29:51.900029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017578095 -0.002251674 -0.0025999444 -0.0027240547 -0.0026852554 -0.0026209652 -0.0025536339 -0.0024961801 -0.0025590889 -0.0027744395 -0.0030397251 -0.0032317997 -0.0033323769 -0.0033778276 -0.0033959625][-0.000861316 -0.0017175514 -0.002351529 -0.002677341 -0.0027094621 -0.0025548085 -0.0022867951 -0.0020361631 -0.0020615393 -0.00238088 -0.0028008576 -0.0031155902 -0.0032849631 -0.0033618927 -0.0033912219][0.0016015484 -0.00015654066 -0.0014650803 -0.0021992603 -0.0023843143 -0.0021636919 -0.0016968991 -0.0012455767 -0.0012441606 -0.0017175814 -0.0023793131 -0.002901277 -0.0031947927 -0.0033298389 -0.0033810625][0.0066427803 0.0033313683 0.00077125756 -0.00072031911 -0.0011892221 -0.00093507138 -0.00028839917 0.00032173889 0.00028929114 -0.00046057114 -0.0015397021 -0.0024519297 -0.0029944787 -0.0032552127 -0.003357318][0.013696237 0.0086149042 0.0045386292 0.0020463746 0.0011824197 0.0014373353 0.002263797 0.0030118343 0.002893365 0.0017102701 -4.4358661e-05 -0.0016233439 -0.0026155082 -0.0031076469 -0.0033089581][0.020710129 0.014256256 0.0089581646 0.0055485126 0.0042672893 0.0044713244 0.0054757381 0.00637474 0.0061678076 0.0044949381 0.0019303211 -0.00048930594 -0.0020791274 -0.0028934784 -0.0032375641][0.025411259 0.018260445 0.012350749 0.0083768116 0.0067585222 0.0068452796 0.0079782279 0.0090450859 0.0088325189 0.0068178708 0.0036087148 0.00049811043 -0.0015973472 -0.0026952182 -0.0031715173][0.026558582 0.019263934 0.013263855 0.0091330912 0.0073232995 0.0072488226 0.0083855474 0.0095686261 0.0094631212 0.0074399654 0.0040813265 0.00079319673 -0.0014415826 -0.0026259432 -0.0031460219][0.024291867 0.017220577 0.011458253 0.0074965153 0.00565781 0.0054249624 0.0063998671 0.0075404905 0.0075861467 0.0059080068 0.0030078501 0.00018049101 -0.0017344856 -0.0027445841 -0.0031858415][0.019586932 0.013106151 0.0078931479 0.0043901158 0.0027050348 0.0023738034 0.0030634841 0.003978001 0.0041155028 0.0029718471 0.00093965651 -0.0010093576 -0.0023072863 -0.002977333 -0.0032664919][0.013734581 0.00827172 0.0039746203 0.0012039193 -0.00014508562 -0.00048133545 -0.00010038028 0.00048050424 0.00061173178 -1.4148187e-05 -0.0011377649 -0.0021854821 -0.0028610716 -0.0031963396 -0.0033369914][0.0079382742 0.0038498919 0.00075365324 -0.0011371907 -0.0020456291 -0.0023009968 -0.0021481276 -0.0018706631 -0.0017982718 -0.002065686 -0.0025338728 -0.0029531743 -0.0032102994 -0.0033303613 -0.0033784732][0.0030989638 0.00047645811 -0.0013958984 -0.0024623291 -0.0029579636 -0.0031065666 -0.0030694152 -0.0029763523 -0.0029520898 -0.0030329004 -0.00316842 -0.0032849358 -0.0033524849 -0.0033813543 -0.0033933602][-0.00029447326 -0.0016773571 -0.0025910288 -0.0030694413 -0.0032798559 -0.0033435216 -0.0033421149 -0.0033238784 -0.0033201114 -0.0033347211 -0.0033572256 -0.0033763202 -0.003388539 -0.003394237 -0.0033972582][-0.0022276388 -0.0027921749 -0.0031322427 -0.003294626 -0.0033626962 -0.0033842321 -0.0033883995 -0.00338801 -0.0033887741 -0.0033897154 -0.0033902032 -0.0033906375 -0.00339269 -0.0033956487 -0.0033975907]]...]
INFO - root - 2017-12-09 17:30:00.479084: step 44910, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:17m:36s remains)
INFO - root - 2017-12-09 17:30:08.964266: step 44920, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 66h:59m:17s remains)
INFO - root - 2017-12-09 17:30:17.474136: step 44930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:18m:16s remains)
INFO - root - 2017-12-09 17:30:26.093457: step 44940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:05m:24s remains)
INFO - root - 2017-12-09 17:30:34.919765: step 44950, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 72h:03m:18s remains)
INFO - root - 2017-12-09 17:30:43.688452: step 44960, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 71h:12m:05s remains)
INFO - root - 2017-12-09 17:30:52.587211: step 44970, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 73h:45m:35s remains)
INFO - root - 2017-12-09 17:31:01.460426: step 44980, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 68h:17m:27s remains)
INFO - root - 2017-12-09 17:31:10.290754: step 44990, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:30m:28s remains)
INFO - root - 2017-12-09 17:31:19.001209: step 45000, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 65h:49m:53s remains)
2017-12-09 17:31:19.900870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0021534895 -0.00059471768 0.0012498987 0.0026525166 0.0030152444 0.0023704995 0.0011301734 -0.00035891403 -0.0017160622 -0.0026735873 -0.0031621805 -0.0033319113 -0.0033746271 -0.0033775452 -0.003377954][-0.00031248014 0.0026812553 0.0057761245 0.00794013 0.0082493443 0.0068759914 0.0044376054 0.0017360733 -0.00059189228 -0.0021958807 -0.003009846 -0.0033077963 -0.003372489 -0.0033760658 -0.0033760571][0.0047187968 0.010691851 0.016493419 0.020369185 0.020942934 0.018353637 0.013522401 0.0080364533 0.0032190192 -0.00021397625 -0.0021690652 -0.0030535753 -0.0033276002 -0.0033768155 -0.0033773766][0.014011595 0.024301196 0.034014191 0.040582459 0.0418373 0.037623636 0.02931568 0.019537069 0.010588782 0.0038938746 -0.00028274069 -0.0024066526 -0.0031898357 -0.0033716233 -0.0033781379][0.026779322 0.04162088 0.055235092 0.064461477 0.0664709 0.060594324 0.04853604 0.033941414 0.020177318 0.009470256 0.0023756525 -0.0014625904 -0.0029850667 -0.0033601257 -0.0033744243][0.039876495 0.058170393 0.074425273 0.085223824 0.087464005 0.079964839 0.064759165 0.046233475 0.028549127 0.014456591 0.0047827861 -0.00060678786 -0.0028053676 -0.0033477806 -0.003369262][0.0487255 0.068420224 0.085398488 0.096357465 0.09831626 0.089770451 0.072891578 0.052399732 0.032817725 0.017015055 0.0059835766 -0.00020938017 -0.0027334187 -0.0033362058 -0.0033584598][0.049861241 0.068509258 0.084264 0.094190292 0.095645487 0.087054014 0.070523135 0.05059712 0.031641997 0.016293989 0.0055534397 -0.0004202663 -0.0027999508 -0.0033363793 -0.0033552039][0.042640656 0.058108889 0.071014695 0.079075329 0.080131762 0.07273753 0.05862774 0.041701794 0.025701597 0.012782559 0.0038103219 -0.0010809565 -0.0029551727 -0.0033429863 -0.0033553126][0.030231718 0.041383203 0.050679922 0.056540355 0.057343133 0.051921979 0.041498665 0.029020447 0.017313154 0.0079299249 0.0015150546 -0.0018847829 -0.0031236585 -0.0033536456 -0.0033598272][0.017231943 0.02414665 0.02997965 0.03374745 0.034348145 0.03096747 0.024351433 0.016444286 0.0091070887 0.0033006705 -0.00057860534 -0.0025670913 -0.0032519829 -0.0033644058 -0.0033664615][0.0070261392 0.010701137 0.013873212 0.015994977 0.016391691 0.014595153 0.011010613 0.0067583569 0.0028833961 -0.00011980953 -0.0020629158 -0.0030198563 -0.003328532 -0.0033721251 -0.0033718494][0.00072652241 0.002333208 0.0037627839 0.0047561112 0.0049614194 0.0041712611 0.0025846167 0.00073605683 -0.00090225041 -0.0021353043 -0.0029004216 -0.0032589568 -0.003365936 -0.0033781854 -0.0033770532][-0.0022397912 -0.0017103235 -0.0012230799 -0.0008701859 -0.00079192547 -0.0010512264 -0.0015726525 -0.0021651359 -0.0026730113 -0.0030426388 -0.0032604218 -0.0033554048 -0.0033801924 -0.0033815536 -0.0033802337][-0.0032071136 -0.0030968562 -0.0029883625 -0.0029012624 -0.0028778582 -0.0029303019 -0.0030379526 -0.003157276 -0.0032573056 -0.0033289401 -0.0033679532 -0.0033821762 -0.0033843839 -0.0033834751 -0.0033823068]]...]
INFO - root - 2017-12-09 17:31:28.556334: step 45010, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 69h:35m:38s remains)
INFO - root - 2017-12-09 17:31:36.896307: step 45020, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 68h:03m:14s remains)
INFO - root - 2017-12-09 17:31:45.491686: step 45030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:36m:54s remains)
INFO - root - 2017-12-09 17:31:54.082936: step 45040, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 67h:18m:33s remains)
INFO - root - 2017-12-09 17:32:02.721509: step 45050, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 66h:52m:06s remains)
INFO - root - 2017-12-09 17:32:11.386460: step 45060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:01m:23s remains)
INFO - root - 2017-12-09 17:32:19.980350: step 45070, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 67h:23m:49s remains)
INFO - root - 2017-12-09 17:32:28.560846: step 45080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:10m:46s remains)
INFO - root - 2017-12-09 17:32:37.244292: step 45090, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 70h:35m:45s remains)
INFO - root - 2017-12-09 17:32:46.030338: step 45100, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 68h:21m:46s remains)
2017-12-09 17:32:46.986586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034007323 -0.0034066846 -0.003405611 -0.0034061023 -0.0034071931 -0.0034081861 -0.0034088828 -0.0034087985 -0.0034077908 -0.0034074 -0.0034075058 -0.0034076704 -0.0034072921 -0.0034071368 -0.0034072078][-0.0033998282 -0.0034056527 -0.0034041668 -0.0034044336 -0.0034055265 -0.0034065931 -0.0034074762 -0.0034074963 -0.0034069556 -0.0034071696 -0.003407358 -0.0034072576 -0.0034067351 -0.0034065389 -0.00340662][-0.003400251 -0.0034056199 -0.0034038969 -0.0034040282 -0.003405093 -0.0034060471 -0.003406903 -0.00340719 -0.0034072015 -0.0034077156 -0.0034079612 -0.0034071936 -0.0034059766 -0.0034055414 -0.0034057794][-0.0033990087 -0.0034061745 -0.0034046383 -0.0034048192 -0.003405713 -0.0034060597 -0.0034065181 -0.0034068266 -0.0034071442 -0.0034077768 -0.0034079398 -0.0034069074 -0.0034052099 -0.0034047919 -0.0034055077][-0.003400313 -0.0034075384 -0.0034063242 -0.0034063645 -0.0034070155 -0.0034069724 -0.003406758 -0.0034065957 -0.0034067361 -0.0034074041 -0.0034074383 -0.0034063866 -0.0034045309 -0.0034043305 -0.0034056632][-0.0034025095 -0.0034090164 -0.003408097 -0.0034080574 -0.0034083065 -0.0034078665 -0.0034069316 -0.0034064793 -0.003406459 -0.003407123 -0.0034072755 -0.003406619 -0.0034050858 -0.0034051712 -0.0034064674][-0.0034029719 -0.0034096693 -0.0034089917 -0.0034088304 -0.0034083705 -0.0034076218 -0.0034065566 -0.0034060497 -0.003406164 -0.0034070688 -0.0034076264 -0.0034074006 -0.0034063761 -0.0034066336 -0.0034075317][-0.0034023416 -0.0034098895 -0.0034092702 -0.0034089985 -0.0034080213 -0.0034070772 -0.0034058925 -0.0034053677 -0.003405448 -0.0034064888 -0.0034072306 -0.0034071079 -0.0034066287 -0.0034069989 -0.003407636][-0.0034022622 -0.0034097373 -0.0034090818 -0.0034087375 -0.0034076422 -0.0034065652 -0.0034052492 -0.0034050236 -0.0034053945 -0.00340636 -0.0034067917 -0.0034067817 -0.0034068527 -0.0034071472 -0.0034075365][-0.0034014862 -0.0034092988 -0.0034086318 -0.00340838 -0.0034075463 -0.0034063433 -0.0034051216 -0.0034052611 -0.0034058543 -0.0034065044 -0.0034067316 -0.0034068576 -0.003407097 -0.0034073843 -0.0034076269][-0.0034014015 -0.0034091757 -0.003408419 -0.0034082939 -0.0034077533 -0.003406602 -0.0034056685 -0.003406099 -0.0034067181 -0.0034073328 -0.0034075263 -0.0034074052 -0.0034073323 -0.0034074273 -0.0034075452][-0.0034017672 -0.0034089366 -0.0034082115 -0.0034082488 -0.0034079563 -0.0034071193 -0.0034063347 -0.0034067647 -0.003407497 -0.0034080932 -0.0034080138 -0.0034075512 -0.0034072772 -0.0034072332 -0.0034074069][-0.0034021051 -0.0034085803 -0.0034078143 -0.0034078271 -0.00340764 -0.0034069209 -0.0034063561 -0.0034067973 -0.0034073617 -0.0034076977 -0.0034074874 -0.0034071663 -0.0034070062 -0.0034070392 -0.0034072872][-0.0034013672 -0.0034083447 -0.003407439 -0.0034073936 -0.0034073463 -0.0034070234 -0.0034066413 -0.0034068588 -0.003407147 -0.0034073945 -0.0034072462 -0.0034070474 -0.0034069647 -0.0034070569 -0.0034072683][-0.0034010815 -0.0034081771 -0.0034072206 -0.0034070823 -0.0034070406 -0.0034069116 -0.0034066716 -0.0034066967 -0.0034068448 -0.0034069365 -0.0034068667 -0.0034068297 -0.003406855 -0.0034069715 -0.0034071384]]...]
INFO - root - 2017-12-09 17:32:55.402760: step 45110, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 65h:21m:16s remains)
INFO - root - 2017-12-09 17:33:03.954952: step 45120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:25m:46s remains)
INFO - root - 2017-12-09 17:33:12.605191: step 45130, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 68h:37m:03s remains)
INFO - root - 2017-12-09 17:33:21.039846: step 45140, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 66h:16m:47s remains)
INFO - root - 2017-12-09 17:33:29.746338: step 45150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:48m:25s remains)
INFO - root - 2017-12-09 17:33:38.334188: step 45160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:31m:26s remains)
INFO - root - 2017-12-09 17:33:46.841335: step 45170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:23m:06s remains)
INFO - root - 2017-12-09 17:33:55.422144: step 45180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:25m:21s remains)
INFO - root - 2017-12-09 17:34:03.952913: step 45190, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:14m:21s remains)
INFO - root - 2017-12-09 17:34:12.521929: step 45200, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:31m:11s remains)
2017-12-09 17:34:13.399386: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.62996137 0.69449127 0.7439791 0.77700692 0.79423422 0.79382211 0.77450866 0.74308407 0.70186657 0.65132672 0.58729863 0.51512861 0.43918458 0.36643654 0.30032498][0.6438818 0.70996737 0.76150566 0.7967276 0.8170386 0.81880105 0.80150843 0.77048635 0.72824526 0.67510408 0.607583 0.53107774 0.45045355 0.37378746 0.30471992][0.65227908 0.71596748 0.76523846 0.79967546 0.82069361 0.8242318 0.80948395 0.78077179 0.73910129 0.68379366 0.61307818 0.53322148 0.44983166 0.37123913 0.30153725][0.66927677 0.72900516 0.77424747 0.80567151 0.82481748 0.82822889 0.81498921 0.78784943 0.74608856 0.68960404 0.61713922 0.53506929 0.44924679 0.36901775 0.29907024][0.68990242 0.74379545 0.78307283 0.81075168 0.82776433 0.83046228 0.81886762 0.79383367 0.75296 0.6956746 0.62194818 0.53854144 0.45103469 0.3689824 0.29798406][0.704133 0.75172907 0.78357369 0.80786967 0.82367575 0.82667196 0.81733644 0.79421639 0.7544843 0.69706422 0.6235472 0.54036736 0.45257068 0.37029839 0.29920596][0.7065683 0.74710417 0.77121961 0.79180306 0.80702776 0.81190974 0.80611008 0.785758 0.74850518 0.69251335 0.62029374 0.53835225 0.45155835 0.3697775 0.29898185][0.69664532 0.73048478 0.74708122 0.7640577 0.77831489 0.78432655 0.78011334 0.76220876 0.72858489 0.6758908 0.6075837 0.52935827 0.44631839 0.36727172 0.29817897][0.67545229 0.7046923 0.71647334 0.73086417 0.7446363 0.75192976 0.74883753 0.73164517 0.6997807 0.65098578 0.58770233 0.514662 0.43678597 0.3619701 0.29571944][0.64121389 0.6670872 0.675579 0.68904203 0.70415366 0.71363872 0.71225947 0.69659817 0.66747642 0.6222254 0.56350517 0.49613121 0.42410335 0.35441557 0.29193741][0.59728312 0.62056917 0.62770408 0.64066315 0.65635931 0.6676107 0.66820687 0.65434557 0.62757814 0.58697015 0.53380084 0.47207746 0.40591648 0.34233153 0.28484043][0.54571038 0.56774533 0.5745346 0.58681768 0.60268807 0.61560404 0.618427 0.60807073 0.58571404 0.55041569 0.50256765 0.44650072 0.38602149 0.32748082 0.27461907][0.48673704 0.50818425 0.51585811 0.52848786 0.54509747 0.55910444 0.56378037 0.55681258 0.538272 0.5084312 0.46671569 0.41748735 0.363835 0.31161422 0.26422977][0.42219129 0.44114378 0.44929254 0.46341586 0.4816353 0.49805629 0.50584793 0.50361717 0.49006006 0.46535078 0.42941266 0.38672343 0.34001049 0.29407939 0.25257176][0.35666314 0.37123087 0.37793717 0.39119917 0.40954351 0.42715496 0.43851256 0.44169423 0.43421295 0.416245 0.38780889 0.35273457 0.31361085 0.27464727 0.23941243]]...]
INFO - root - 2017-12-09 17:34:21.743030: step 45210, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.734 sec/batch; 58h:34m:41s remains)
INFO - root - 2017-12-09 17:34:30.279619: step 45220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:17m:13s remains)
INFO - root - 2017-12-09 17:34:38.970308: step 45230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:39m:02s remains)
INFO - root - 2017-12-09 17:34:47.540334: step 45240, loss = 0.88, batch loss = 0.67 (10.6 examples/sec; 0.757 sec/batch; 60h:23m:19s remains)
INFO - root - 2017-12-09 17:34:56.211353: step 45250, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.857 sec/batch; 68h:22m:30s remains)
INFO - root - 2017-12-09 17:35:04.802999: step 45260, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:16m:15s remains)
INFO - root - 2017-12-09 17:35:13.562994: step 45270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:02m:24s remains)
INFO - root - 2017-12-09 17:35:22.242152: step 45280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:29m:40s remains)
INFO - root - 2017-12-09 17:35:30.944359: step 45290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:44m:18s remains)
INFO - root - 2017-12-09 17:35:39.694462: step 45300, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 70h:39m:04s remains)
2017-12-09 17:35:40.531193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033918275 -0.0033898298 -0.0033889092 -0.0033886721 -0.0033883383 -0.0033875434 -0.0033838809 -0.0033747503 -0.0033619313 -0.0033529894 -0.003357077 -0.0033747437 -0.0033850665 -0.0033868679 -0.0033871867][-0.0033900179 -0.0033903797 -0.0033891192 -0.0033881094 -0.0033850668 -0.0033765472 -0.003357898 -0.0033312547 -0.003309577 -0.0033106457 -0.0033377481 -0.0033732892 -0.0033852223 -0.0033866491 -0.003386393][-0.0033112718 -0.0033184472 -0.0033484725 -0.0033781885 -0.003373744 -0.0033387195 -0.0032751861 -0.0031988106 -0.0031513711 -0.0031756302 -0.0032555768 -0.0033453556 -0.0033826227 -0.0033882845 -0.0033877811][-0.0032460557 -0.0032506958 -0.0032355019 -0.0032144086 -0.0031919631 -0.0031557498 -0.0030713943 -0.0029580067 -0.002916479 -0.0030250605 -0.0031913151 -0.0033374857 -0.0033852656 -0.0033896826 -0.0033889257][-0.0026990066 -0.0026178057 -0.0025311655 -0.0024305182 -0.0022931697 -0.0021254437 -0.0019819422 -0.0019525279 -0.0021369851 -0.0025385378 -0.0029692822 -0.0032777004 -0.003381321 -0.0033896149 -0.0033892039][-0.0016331944 -0.0011576125 -0.00065283664 -0.00018710131 0.00018409686 0.00044967444 0.00056941574 0.0003998233 -0.00023749145 -0.0012889921 -0.0023442111 -0.0030422127 -0.00333315 -0.003386267 -0.0033875292][0.00015789457 0.0010714461 0.0019781291 0.0028254236 0.0035192068 0.0040001259 0.0041710157 0.0037734574 0.0025580316 0.00066979369 -0.0012636411 -0.0025963318 -0.0032217191 -0.0033744529 -0.0033851555][0.0021845251 0.003453033 0.0045639244 0.0055460487 0.0063298703 0.0068966388 0.00715813 0.0067185196 0.0051927189 0.0026898952 -1.4738878e-05 -0.0020116041 -0.0030440274 -0.0033457058 -0.0033795247][0.0035192287 0.0048861909 0.0059651625 0.006859696 0.0075304988 0.0080240071 0.0083060861 0.0079277381 0.0064297221 0.0037977914 0.00077236257 -0.0015999753 -0.0029058845 -0.0033225513 -0.0033746858][0.0034208102 0.0046791988 0.0056148819 0.0063378471 0.0068399576 0.0072064218 0.0074445768 0.0071344818 0.0058350707 0.0034492412 0.00062448718 -0.0016366109 -0.0029095 -0.0033214043 -0.0033766115][0.0019798684 0.0029810835 0.0037204493 0.0042616273 0.0045690346 0.0047533312 0.0049036522 0.0046985061 0.0037384056 0.0018890663 -0.00032836711 -0.0020782147 -0.00304361 -0.0033411789 -0.0033810425][-0.00011053658 0.00054824492 0.0010600663 0.0014292812 0.001618206 0.0017025955 0.0017704731 0.0016292045 0.0010097125 -0.00017532264 -0.0015799563 -0.0026487275 -0.0032102354 -0.0033658142 -0.0033847129][-0.0019299784 -0.0015863201 -0.0013010239 -0.0010899091 -0.00098126172 -0.00093350792 -0.00089115091 -0.00095572649 -0.001275494 -0.0018802221 -0.0025776064 -0.0030801124 -0.0033255031 -0.0033823524 -0.0033870661][-0.0029265161 -0.0027952667 -0.0026790213 -0.0025901704 -0.0025422154 -0.00251931 -0.0024997804 -0.0025220006 -0.0026447724 -0.0028709038 -0.0031255223 -0.0032999681 -0.0033768138 -0.0033888691 -0.0033876915][-0.0033062724 -0.0032707057 -0.0032367893 -0.0032096724 -0.0031929561 -0.0031820771 -0.0031729506 -0.0031776158 -0.0032122447 -0.0032718359 -0.0033353411 -0.0033751908 -0.0033894866 -0.003389284 -0.0033875857]]...]
INFO - root - 2017-12-09 17:35:49.030457: step 45310, loss = 0.90, batch loss = 0.70 (11.2 examples/sec; 0.714 sec/batch; 56h:56m:49s remains)
INFO - root - 2017-12-09 17:35:57.629621: step 45320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:52m:25s remains)
INFO - root - 2017-12-09 17:36:06.099716: step 45330, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 64h:57m:53s remains)
INFO - root - 2017-12-09 17:36:14.481579: step 45340, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.713 sec/batch; 56h:53m:22s remains)
INFO - root - 2017-12-09 17:36:23.040785: step 45350, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:19m:23s remains)
INFO - root - 2017-12-09 17:36:31.633727: step 45360, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 68h:28m:44s remains)
INFO - root - 2017-12-09 17:36:40.638700: step 45370, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 71h:19m:01s remains)
INFO - root - 2017-12-09 17:36:49.328306: step 45380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:52m:05s remains)
INFO - root - 2017-12-09 17:36:58.003226: step 45390, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 70h:52m:44s remains)
INFO - root - 2017-12-09 17:37:06.553804: step 45400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:02m:50s remains)
2017-12-09 17:37:07.464216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034081147 -0.0034068036 -0.0034061507 -0.0034057596 -0.003405442 -0.0034055531 -0.0034055989 -0.0034055149 -0.0034053507 -0.0034053607 -0.0034053137 -0.0034052997 -0.0034054543 -0.003405964 -0.0034063046][-0.00340549 -0.0034041428 -0.0034033959 -0.0034031109 -0.0034028136 -0.0034028031 -0.0034027903 -0.00340278 -0.0034027218 -0.0034027705 -0.0034028632 -0.0034031798 -0.0034034792 -0.0034037137 -0.0034037698][-0.0034015067 -0.0034003397 -0.0033998124 -0.0033995756 -0.003399425 -0.0033993479 -0.0033991514 -0.0033989642 -0.0033987341 -0.0033986121 -0.0033985416 -0.003398808 -0.0033990042 -0.0033989877 -0.0033988387][-0.0033950417 -0.0033938535 -0.0033935097 -0.0033934347 -0.0033933814 -0.0033933383 -0.0033932196 -0.003393013 -0.0033927383 -0.0033923408 -0.0033919588 -0.0033919569 -0.003391989 -0.0033918817 -0.003391675][-0.0033889536 -0.0033873972 -0.0033869725 -0.0033867832 -0.0033866181 -0.0033865375 -0.0033864919 -0.0033864123 -0.0033862146 -0.003385904 -0.003385542 -0.0033854013 -0.0033852351 -0.0033850968 -0.0033849822][-0.0033848144 -0.0033826909 -0.00338211 -0.0033817238 -0.0033814337 -0.0033811929 -0.0033811887 -0.0033811694 -0.0033809573 -0.0033807543 -0.0033805126 -0.0033804914 -0.0033804418 -0.0033804479 -0.0033806721][-0.0033829419 -0.0033803724 -0.003379633 -0.0033792546 -0.0033788707 -0.0033783137 -0.0033780395 -0.0033778115 -0.0033774795 -0.0033773277 -0.0033773165 -0.0033774506 -0.0033773982 -0.0033774818 -0.0033778837][-0.0033817627 -0.0033790607 -0.0033784197 -0.003378147 -0.0033777042 -0.0033773168 -0.0033768255 -0.0033763486 -0.0033757549 -0.0033755107 -0.0033756269 -0.0033758818 -0.00337575 -0.0033756278 -0.0033758706][-0.0033803687 -0.0033776059 -0.0033770115 -0.003376832 -0.0033763256 -0.003375907 -0.0033754713 -0.0033748962 -0.0033740092 -0.0033735083 -0.0033737118 -0.0033740874 -0.0033740536 -0.0033740867 -0.0033744823][-0.0033797666 -0.0033771154 -0.0033766355 -0.0033765193 -0.0033760066 -0.0033756497 -0.0033753766 -0.0033749794 -0.0033740171 -0.0033732525 -0.0033731929 -0.0033734583 -0.0033734536 -0.0033735759 -0.0033739943][-0.0033803151 -0.0033778585 -0.0033774585 -0.0033773424 -0.0033771307 -0.0033770218 -0.0033769934 -0.0033767261 -0.0033759314 -0.0033749933 -0.0033745011 -0.0033744175 -0.0033742636 -0.0033743337 -0.0033746245][-0.0033824686 -0.0033802495 -0.0033798083 -0.0033796208 -0.0033795305 -0.0033796667 -0.003379781 -0.0033796271 -0.0033790546 -0.0033781074 -0.0033773438 -0.0033769486 -0.003376568 -0.0033765715 -0.0033766651][-0.003386101 -0.0033840681 -0.0033836018 -0.0033833024 -0.003383294 -0.0033836951 -0.0033838877 -0.0033837697 -0.0033833131 -0.0033824255 -0.0033815233 -0.0033808772 -0.0033803093 -0.0033800127 -0.0033796823][-0.0033906919 -0.0033885122 -0.0033878726 -0.003387383 -0.0033874076 -0.0033876309 -0.003387735 -0.0033875038 -0.0033870214 -0.0033861788 -0.0033854428 -0.0033850125 -0.0033847252 -0.0033845119 -0.0033841087][-0.0033956359 -0.0033935716 -0.0033930638 -0.0033924181 -0.0033919474 -0.0033917886 -0.0033916789 -0.003391302 -0.003390864 -0.0033902656 -0.0033896645 -0.0033894635 -0.0033895113 -0.0033895667 -0.0033893813]]...]
INFO - root - 2017-12-09 17:37:16.163928: step 45410, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:24m:15s remains)
INFO - root - 2017-12-09 17:37:24.615794: step 45420, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 66h:07m:41s remains)
INFO - root - 2017-12-09 17:37:33.395073: step 45430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:32m:07s remains)
INFO - root - 2017-12-09 17:37:42.149653: step 45440, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.760 sec/batch; 60h:36m:03s remains)
INFO - root - 2017-12-09 17:37:50.823654: step 45450, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 65h:31m:38s remains)
INFO - root - 2017-12-09 17:37:59.504754: step 45460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:22m:34s remains)
INFO - root - 2017-12-09 17:38:08.403269: step 45470, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 71h:09m:46s remains)
INFO - root - 2017-12-09 17:38:17.187371: step 45480, loss = 0.88, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 71h:30m:31s remains)
INFO - root - 2017-12-09 17:38:25.866464: step 45490, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 67h:10m:13s remains)
INFO - root - 2017-12-09 17:38:34.330260: step 45500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:22m:39s remains)
2017-12-09 17:38:35.187792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033757549 -0.0033733994 -0.0033732078 -0.0033733817 -0.0033736972 -0.0033739847 -0.0033741931 -0.0033744252 -0.0033745992 -0.003374604 -0.0033749172 -0.0033752997 -0.0033758588 -0.0033766974 -0.0033775053][-0.0033740425 -0.0033714457 -0.0033711207 -0.0033712178 -0.0033714238 -0.0033716573 -0.0033719102 -0.0033723374 -0.0033726846 -0.0033730376 -0.0033737738 -0.0033747591 -0.0033758904 -0.0033771044 -0.0033782357][-0.003374448 -0.0033718664 -0.003371533 -0.0033716029 -0.0033717565 -0.0033719656 -0.0033722036 -0.0033726103 -0.0033729828 -0.0033734462 -0.0033744797 -0.0033758667 -0.003377341 -0.0033788208 -0.0033801598][-0.0033751721 -0.0033727456 -0.0033726743 -0.0033729977 -0.0033733959 -0.0033737102 -0.0033739037 -0.0033741165 -0.0033743484 -0.0033747468 -0.0033758155 -0.003377334 -0.0033789212 -0.0033804472 -0.0033816567][-0.0033764411 -0.0033742771 -0.0033748082 -0.0033758436 -0.0033768788 -0.0033774558 -0.0033775277 -0.0033772814 -0.003376954 -0.00337677 -0.0033773861 -0.0033787172 -0.0033802453 -0.0033816493 -0.0033825876][-0.0033780215 -0.003376412 -0.0033779768 -0.0033800916 -0.0033821377 -0.0033832737 -0.0033832826 -0.0033824884 -0.0033812297 -0.0033799883 -0.0033797279 -0.003380582 -0.0033818707 -0.0033829769 -0.003383586][-0.0033800716 -0.0033794532 -0.0033822493 -0.0033856733 -0.0033888484 -0.0033906521 -0.0033906722 -0.00338937 -0.0033872598 -0.0033849049 -0.0033835722 -0.0033836106 -0.0033844884 -0.0033853168 -0.0033856][-0.0033823671 -0.0033829671 -0.0033871203 -0.0033918964 -0.0033961516 -0.0033986818 -0.0033990648 -0.0033975972 -0.0033946922 -0.0033912871 -0.0033889764 -0.0033881601 -0.0033884561 -0.0033888523 -0.0033888095][-0.0033844765 -0.0033861459 -0.003391345 -0.0033972787 -0.0034026774 -0.0034061472 -0.0034072322 -0.003405845 -0.0034024713 -0.0033983707 -0.0033952007 -0.0033935681 -0.0033933087 -0.0033935902 -0.0033935667][-0.0033857648 -0.0033880842 -0.003394037 -0.0034005868 -0.0034067733 -0.0034110774 -0.0034129089 -0.003411819 -0.0034083549 -0.0034040147 -0.003400398 -0.0033981449 -0.0033976675 -0.0033981423 -0.0033986676][-0.0033857217 -0.0033882461 -0.0033944803 -0.0034009758 -0.0034071968 -0.0034119817 -0.0034144467 -0.0034139033 -0.0034109184 -0.0034070457 -0.0034034578 -0.0034010108 -0.0034007761 -0.0034019402 -0.0034034634][-0.0033842886 -0.0033862796 -0.0033919844 -0.0033979979 -0.0034038988 -0.0034088353 -0.003411809 -0.003412114 -0.0034100416 -0.0034070166 -0.0034039456 -0.0034018469 -0.0034019509 -0.0034039244 -0.0034065191][-0.0033821287 -0.0033830989 -0.0033879403 -0.0033932584 -0.0033989276 -0.003404049 -0.0034073463 -0.0034083894 -0.0034073072 -0.0034050581 -0.0034024643 -0.0034008715 -0.003401438 -0.0034041281 -0.0034078844][-0.003379836 -0.0033798022 -0.003383796 -0.0033884514 -0.0033938431 -0.0033988475 -0.0034021388 -0.0034036248 -0.0034033228 -0.0034016198 -0.0033994468 -0.0033982396 -0.0033992638 -0.0034026038 -0.0034072194][-0.0033778278 -0.0033768767 -0.0033800604 -0.0033840535 -0.0033889876 -0.0033936165 -0.0033967795 -0.0033982473 -0.0033982212 -0.0033968994 -0.0033949958 -0.0033939977 -0.0033951639 -0.0033985726 -0.0034034175]]...]
INFO - root - 2017-12-09 17:38:43.718708: step 45510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:59m:58s remains)
INFO - root - 2017-12-09 17:38:52.069427: step 45520, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 64h:46m:32s remains)
INFO - root - 2017-12-09 17:39:00.721819: step 45530, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 66h:40m:01s remains)
INFO - root - 2017-12-09 17:39:09.397742: step 45540, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 68h:24m:33s remains)
INFO - root - 2017-12-09 17:39:18.018342: step 45550, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 70h:55m:33s remains)
INFO - root - 2017-12-09 17:39:26.844913: step 45560, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 68h:56m:39s remains)
INFO - root - 2017-12-09 17:39:35.538335: step 45570, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 71h:45m:30s remains)
INFO - root - 2017-12-09 17:39:44.183679: step 45580, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:37m:15s remains)
INFO - root - 2017-12-09 17:39:52.897287: step 45590, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 70h:23m:03s remains)
INFO - root - 2017-12-09 17:40:01.601507: step 45600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 71h:57m:13s remains)
2017-12-09 17:40:02.521192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0042244568 0.0052460954 0.0058702314 0.0062404554 0.0063885991 0.006570518 0.0067761093 0.0069518546 0.0071950853 0.0073163258 0.0072940905 0.0069306344 0.0063447161 0.0060037458 0.0057239449][0.010703585 0.012369727 0.013278826 0.013608371 0.01351562 0.013373951 0.013287551 0.013310985 0.013284427 0.01321865 0.013016104 0.012530967 0.011907341 0.011516161 0.011410127][0.015058598 0.016986988 0.017999791 0.018337738 0.018289898 0.018143207 0.018026909 0.017773625 0.017453201 0.016987361 0.016283 0.015490658 0.014713823 0.014217417 0.013936357][0.015664285 0.017405041 0.018379224 0.018814886 0.019036865 0.019201003 0.019340897 0.019321477 0.019010533 0.01832371 0.01737023 0.016516205 0.015775897 0.015145701 0.014849422][0.013904064 0.015398444 0.016217116 0.016575206 0.016851848 0.017297767 0.017686846 0.017966915 0.017989522 0.017612895 0.016828626 0.015810302 0.015051833 0.014522321 0.014269152][0.012024428 0.013518267 0.014520473 0.015246614 0.016018165 0.016863804 0.017513052 0.017706264 0.017333483 0.016434746 0.015207547 0.014002807 0.01299419 0.012409239 0.012084567][0.0092022307 0.010565637 0.011856234 0.013252149 0.01473252 0.016157715 0.0170704 0.017244747 0.016491229 0.014917192 0.013086927 0.011541206 0.010162561 0.0090498105 0.0079978183][0.0048429472 0.0055988887 0.006814938 0.00843587 0.010130032 0.01187529 0.012860513 0.013098099 0.012254552 0.01071587 0.0089966664 0.0074870642 0.0061180303 0.0050192103 0.0038946781][0.0030971549 0.0036380712 0.004254817 0.0051762667 0.0061506974 0.00732975 0.0080949971 0.0083100069 0.0074758725 0.0061461795 0.0046074707 0.0033230202 0.0022419111 0.00143417 0.00069980905][0.0031886406 0.0031732197 0.0030923823 0.0031856897 0.0032940663 0.0036914877 0.0039530564 0.0041133538 0.0034236561 0.0023666869 0.0011603252 0.0003044277 -0.00040311553 -0.00095644523 -0.0012101533][0.0020930956 0.0017807677 0.0013147763 0.00080463779 0.0003429947 7.12662e-05 -0.00012651249 1.2188684e-06 -0.00037739333 -0.00096503017 -0.0015603723 -0.0018658765 -0.0020806836 -0.0022538169 -0.0022278281][0.0023479082 0.0023955109 0.0021824224 0.0019014042 0.0017134908 0.0017533696 0.0018296165 0.0017978267 0.0013154668 0.00076522049 6.4832158e-05 -0.00058180816 -0.00096142967 -0.0011985779 -0.0014565245][0.0045685051 0.0046546515 0.0043993415 0.004085999 0.0038771902 0.0040746918 0.0042930273 0.0042645419 0.0037834255 0.0032135469 0.0023820328 0.0016108907 0.0012963566 0.0010495607 0.00074759428][0.0056627383 0.0061132098 0.0063989358 0.006394417 0.0064507443 0.0063848607 0.0061128335 0.0054617357 0.0043799952 0.003482891 0.0024494145 0.0016631426 0.0012804086 0.0013942777 0.0014496439][0.0041341158 0.005553144 0.007002634 0.0084801428 0.0098312218 0.010518991 0.010815796 0.010549291 0.009920923 0.0087395348 0.0073814318 0.00620763 0.0056341626 0.0052057989 0.004582176]]...]
INFO - root - 2017-12-09 17:40:11.202604: step 45610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:38m:19s remains)
INFO - root - 2017-12-09 17:40:19.537631: step 45620, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.812 sec/batch; 64h:42m:24s remains)
INFO - root - 2017-12-09 17:40:28.158329: step 45630, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 69h:02m:08s remains)
INFO - root - 2017-12-09 17:40:36.812173: step 45640, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:12m:59s remains)
INFO - root - 2017-12-09 17:40:45.429818: step 45650, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 72h:08m:40s remains)
INFO - root - 2017-12-09 17:40:54.215503: step 45660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:45m:11s remains)
INFO - root - 2017-12-09 17:41:02.958828: step 45670, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 68h:25m:18s remains)
INFO - root - 2017-12-09 17:41:11.760529: step 45680, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:25m:08s remains)
INFO - root - 2017-12-09 17:41:20.423358: step 45690, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.908 sec/batch; 72h:19m:26s remains)
INFO - root - 2017-12-09 17:41:29.042754: step 45700, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.820 sec/batch; 65h:20m:15s remains)
2017-12-09 17:41:29.923387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033815382 -0.0033823973 -0.0033848258 -0.0033866831 -0.0033880188 -0.0033882589 -0.0033871981 -0.0033841447 -0.003377774 -0.0033675535 -0.0033545552 -0.003339391 -0.0033239177 -0.0033107095 -0.0033015867][-0.003376584 -0.0033759249 -0.0033772232 -0.0033788036 -0.0033807175 -0.0033828944 -0.0033848109 -0.0033857939 -0.003383436 -0.0033766024 -0.0033654552 -0.0033498872 -0.0033333036 -0.0033178285 -0.0033061311][-0.0033741933 -0.0033726559 -0.0033727067 -0.0033733165 -0.0033747568 -0.0033772367 -0.0033806607 -0.0033842183 -0.003385439 -0.0033824577 -0.0033738951 -0.0033601148 -0.00334383 -0.00332761 -0.0033142769][-0.0033734 -0.0033713928 -0.003370753 -0.0033704124 -0.0033707852 -0.0033721961 -0.0033751582 -0.0033792721 -0.0033817897 -0.0033811054 -0.0033750476 -0.0033640147 -0.0033495815 -0.0033338559 -0.0033196621][-0.0033733239 -0.0033710534 -0.0033701216 -0.0033691169 -0.0033692489 -0.0033699814 -0.0033719924 -0.0033756655 -0.0033778581 -0.0033771212 -0.0033718108 -0.0033628503 -0.0033504462 -0.003335976 -0.0033222314][-0.0033736858 -0.0033711726 -0.0033701418 -0.003369139 -0.0033694059 -0.003369845 -0.0033713421 -0.0033740185 -0.003374848 -0.0033730986 -0.0033680422 -0.0033602484 -0.0033489941 -0.0033360892 -0.0033237441][-0.00337414 -0.0033715996 -0.0033708655 -0.00337036 -0.0033708601 -0.0033712985 -0.0033721675 -0.0033742324 -0.0033746774 -0.0033719242 -0.0033666973 -0.0033586791 -0.003347687 -0.00333574 -0.0033247489][-0.0033746944 -0.0033729025 -0.0033728145 -0.0033730178 -0.0033739721 -0.0033746036 -0.0033756127 -0.0033770895 -0.0033768443 -0.0033731915 -0.003366482 -0.0033582116 -0.0033478306 -0.0033373968 -0.0033279439][-0.0033751591 -0.0033741591 -0.0033751868 -0.0033766383 -0.0033785317 -0.0033798411 -0.0033809443 -0.0033816823 -0.0033804565 -0.0033756865 -0.0033682275 -0.0033593837 -0.0033498809 -0.0033411125 -0.003333335][-0.003375337 -0.0033749815 -0.0033770332 -0.0033799505 -0.003383074 -0.0033851471 -0.0033866419 -0.003387023 -0.0033849685 -0.0033796469 -0.0033725323 -0.0033646482 -0.0033568088 -0.0033499093 -0.0033438844][-0.0033755698 -0.0033758464 -0.0033790371 -0.0033832903 -0.0033877066 -0.0033907099 -0.0033928684 -0.003393057 -0.003390559 -0.0033851792 -0.0033790788 -0.0033725938 -0.003366346 -0.0033613998 -0.0033569727][-0.0033750995 -0.0033760248 -0.0033799782 -0.0033851448 -0.003390304 -0.003393888 -0.0033965481 -0.0033967912 -0.0033946217 -0.0033903166 -0.0033855513 -0.0033806332 -0.0033761223 -0.0033721295 -0.0033685141][-0.0033747754 -0.0033759163 -0.0033798104 -0.0033850493 -0.0033904181 -0.0033942899 -0.0033973353 -0.0033981539 -0.0033971819 -0.0033945895 -0.0033913804 -0.0033878731 -0.0033840532 -0.003381314 -0.0033788793][-0.0033740706 -0.0033747472 -0.0033779992 -0.0033827699 -0.0033878295 -0.0033918421 -0.0033951874 -0.0033968221 -0.003397146 -0.0033961795 -0.003394929 -0.0033922009 -0.0033884717 -0.0033861645 -0.0033840283][-0.0033740539 -0.0033741833 -0.0033763782 -0.0033798872 -0.0033838437 -0.0033873757 -0.0033906605 -0.0033928489 -0.0033942803 -0.0033944829 -0.0033942871 -0.003392213 -0.0033893068 -0.0033873629 -0.0033857781]]...]
INFO - root - 2017-12-09 17:41:38.370304: step 45710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:27m:18s remains)
INFO - root - 2017-12-09 17:41:46.911006: step 45720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:03m:55s remains)
INFO - root - 2017-12-09 17:41:55.467143: step 45730, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 69h:22m:58s remains)
INFO - root - 2017-12-09 17:42:04.047682: step 45740, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 66h:32m:02s remains)
INFO - root - 2017-12-09 17:42:12.590430: step 45750, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 69h:54m:42s remains)
INFO - root - 2017-12-09 17:42:21.253369: step 45760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:21m:30s remains)
INFO - root - 2017-12-09 17:42:29.872859: step 45770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:37m:10s remains)
INFO - root - 2017-12-09 17:42:38.286221: step 45780, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.829 sec/batch; 66h:01m:58s remains)
INFO - root - 2017-12-09 17:42:46.810236: step 45790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:56m:51s remains)
INFO - root - 2017-12-09 17:42:55.382544: step 45800, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.814 sec/batch; 64h:47m:37s remains)
2017-12-09 17:42:56.380335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033996815 -0.0033744662 -0.0031887507 -0.0026950568 -0.0018786122 -0.00098145264 -0.00032323948 -9.6852658e-05 -0.00023204507 -0.00045076991 -0.00041373237 5.8366684e-05 0.00089917681 0.0017275705 0.0022040519][-0.0033490071 -0.0033050748 -0.0030084066 -0.0022290377 -0.00095534115 0.00045542209 0.0015178754 0.0019313309 0.0017931985 0.0015085826 0.0016264387 0.0025205852 0.0040830988 0.0056903465 0.0067152735][-0.0031911167 -0.0031149168 -0.0026431673 -0.0014170783 0.00060232286 0.0029303145 0.0048294971 0.0057432726 0.00565847 0.0050764782 0.0049183369 0.0059011774 0.0079748053 0.010444727 0.012389429][-0.0028675562 -0.0027322513 -0.0019676439 -5.6249555e-06 0.0032028372 0.0069116261 0.0099603236 0.011400301 0.011099897 0.009825564 0.0089832554 0.0096966177 0.01204372 0.015213298 0.018076722][-0.0023983859 -0.00215171 -0.00098133436 0.001915524 0.0065762578 0.011881934 0.01609008 0.017793598 0.016836606 0.014387049 0.012372222 0.012331942 0.014469309 0.018017083 0.021782739][-0.0019427614 -0.001537802 3.6433572e-05 0.0037357945 0.0095087625 0.015864547 0.02057883 0.021972641 0.020055134 0.016418686 0.013295311 0.012329353 0.013900334 0.017393326 0.021680504][-0.0017709731 -0.0011810833 0.00064330851 0.0046900893 0.010791513 0.017266162 0.021668887 0.022294626 0.019405263 0.01492646 0.011191708 0.0096885394 0.010772586 0.013939956 0.01823576][-0.0020597605 -0.0013535856 0.00043647131 0.00421919 0.0097668916 0.015482577 0.019023832 0.018829828 0.01536409 0.010647823 0.0069364561 0.0053788451 0.0061562462 0.008874706 0.012805128][-0.0026198158 -0.0019589285 -0.00042796624 0.0026499636 0.0070230309 0.011403848 0.013851056 0.013132716 0.0097848587 0.0055931639 0.0024093159 0.0011095267 0.0017120794 0.0039280141 0.0072452351][-0.0031426137 -0.0027069421 -0.0016556255 0.00044520618 0.0034012587 0.006269535 0.0076832138 0.0068531418 0.0042873966 0.0013287391 -0.00082489545 -0.0016742479 -0.0012561055 0.000260049 0.0026469838][-0.0033644042 -0.0031879297 -0.0026682194 -0.0015654986 3.0939234e-05 0.0015374322 0.0021835647 0.0015846693 0.00012162211 -0.0014382838 -0.0025221193 -0.0029442455 -0.0027870459 -0.0020467239 -0.00071028923][-0.0034034792 -0.0033741165 -0.0032227526 -0.0028398554 -0.0022283616 -0.0016450936 -0.0014134392 -0.0016760223 -0.0022454679 -0.0028074484 -0.0031621519 -0.003282313 -0.0032429697 -0.0030212335 -0.0025301264][-0.0034062823 -0.0034041451 -0.0033823629 -0.0033175983 -0.0031980735 -0.0030740602 -0.0030212612 -0.0030736385 -0.0031880108 -0.0032983867 -0.0033660161 -0.0033836076 -0.0033774674 -0.0033442588 -0.0032496138][-0.0034058753 -0.0034051482 -0.0034050031 -0.0034043547 -0.003402221 -0.0033984911 -0.0033949669 -0.0033926785 -0.0033925783 -0.0033949076 -0.0033986541 -0.0034027742 -0.0034054208 -0.0034064318 -0.0034039037][-0.0034054725 -0.0034047656 -0.0034052073 -0.0034057456 -0.0034051826 -0.0034032268 -0.0033997975 -0.003396312 -0.0033942433 -0.0033948722 -0.0033974075 -0.0034008846 -0.0034041088 -0.0034060723 -0.0034060825]]...]
INFO - root - 2017-12-09 17:43:05.112282: step 45810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:52m:52s remains)
INFO - root - 2017-12-09 17:43:13.581927: step 45820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:27m:46s remains)
INFO - root - 2017-12-09 17:43:22.172772: step 45830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 69h:18m:45s remains)
INFO - root - 2017-12-09 17:43:30.848972: step 45840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:22m:08s remains)
INFO - root - 2017-12-09 17:43:39.367764: step 45850, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.923 sec/batch; 73h:28m:15s remains)
INFO - root - 2017-12-09 17:43:48.070736: step 45860, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 70h:41m:07s remains)
INFO - root - 2017-12-09 17:43:56.760652: step 45870, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 71h:17m:38s remains)
INFO - root - 2017-12-09 17:44:05.422533: step 45880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:42m:16s remains)
INFO - root - 2017-12-09 17:44:14.133877: step 45890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:03m:06s remains)
INFO - root - 2017-12-09 17:44:22.876130: step 45900, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:37m:07s remains)
2017-12-09 17:44:23.835611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028363233 -0.0029847983 -0.0031329803 -0.0032446007 -0.0032800161 -0.003236878 -0.0031283761 -0.0030452616 -0.0030210926 -0.0030863129 -0.0031888252 -0.0032878618 -0.0033492972 -0.0033777896 -0.0033873159][-0.0014367388 -0.0019165955 -0.0024407133 -0.0028403653 -0.0029832639 -0.0029110713 -0.0027145217 -0.0025708308 -0.0025514048 -0.0026930424 -0.00291659 -0.0031433264 -0.0032886975 -0.0033592633 -0.0033837161][0.001569204 0.00042428728 -0.00086875958 -0.0018884802 -0.0022957544 -0.0021897075 -0.0017886121 -0.0014841511 -0.0014720198 -0.0018229877 -0.0023705298 -0.0028756424 -0.0031873505 -0.0033320785 -0.0033799834][0.006279089 0.0043640113 0.00203289 0.0002021729 -0.00051069632 -0.00022992259 0.00060394639 0.001186023 0.0010980465 0.00024309685 -0.0010145458 -0.0021516574 -0.0028817081 -0.0032328025 -0.0033579161][0.011446619 0.00920292 0.00621028 0.0038522249 0.003134442 0.0039240243 0.0054724752 0.0064139138 0.0060474179 0.0042743739 0.001751835 -0.00056917733 -0.0021598791 -0.0029719062 -0.0032891596][0.015739216 0.01374376 0.010789515 0.0086166449 0.0085339118 0.010329446 0.012841962 0.014160268 0.013337849 0.010282978 0.00600919 0.0019594771 -0.00095803454 -0.0025160182 -0.0031611526][0.017822392 0.016696414 0.014578348 0.013402738 0.014594659 0.017799582 0.021405937 0.023049982 0.021650163 0.017152702 0.010945259 0.0049273814 0.00047246972 -0.0019638808 -0.003001827][0.016943434 0.017020734 0.016318412 0.016751774 0.0196275 0.024442161 0.029119372 0.031004954 0.028998639 0.023166465 0.015244484 0.0074619823 0.0016749627 -0.0015102465 -0.002875726][0.01347621 0.014597271 0.015380213 0.017533833 0.022037212 0.028176319 0.033540651 0.035469368 0.032956321 0.026261497 0.017337508 0.0085860528 0.0021575876 -0.0013543337 -0.0028423802][0.0085847192 0.010234026 0.012034476 0.015333299 0.020763157 0.027495831 0.033020925 0.034831766 0.032086678 0.025252778 0.016391011 0.00785134 0.0017315047 -0.0015506608 -0.0029089821][0.0039302725 0.0054777595 0.0074699349 0.010965912 0.016296728 0.022613805 0.027613714 0.029127534 0.026523294 0.020411531 0.012740383 0.0055572516 0.00058371061 -0.0020081466 -0.0030439084][0.00032239128 0.0014596647 0.003110894 0.0060171667 0.010351148 0.01542482 0.019397398 0.020539371 0.018399227 0.013614958 0.0078328867 0.0026263986 -0.00081679877 -0.0025369413 -0.003189709][-0.0019389805 -0.0012933286 -0.00021056738 0.0017654896 0.0047299485 0.00820262 0.010919789 0.011663729 0.01014366 0.0069035622 0.0031555849 -4.2444794e-05 -0.0020315014 -0.0029682508 -0.0032992074][-0.0030169764 -0.0027489872 -0.002206088 -0.0011431091 0.0005166023 0.0024939585 0.0040625036 0.0044712196 0.0035573179 0.0017058547 -0.00031699892 -0.0019151649 -0.0028324185 -0.0032309173 -0.0033589113][-0.0033570747 -0.0032855095 -0.0031014462 -0.0026895211 -0.0019801245 -0.0010859568 -0.00034805271 -0.00016009761 -0.00059714052 -0.0014396678 -0.00229461 -0.00290517 -0.0032227454 -0.0033468225 -0.0033823189]]...]
INFO - root - 2017-12-09 17:44:32.425524: step 45910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:46m:24s remains)
INFO - root - 2017-12-09 17:44:40.974610: step 45920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:48m:10s remains)
INFO - root - 2017-12-09 17:44:49.473258: step 45930, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 67h:55m:20s remains)
INFO - root - 2017-12-09 17:44:58.320123: step 45940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:46m:09s remains)
INFO - root - 2017-12-09 17:45:06.823939: step 45950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:30m:58s remains)
INFO - root - 2017-12-09 17:45:15.625761: step 45960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:53m:54s remains)
INFO - root - 2017-12-09 17:45:24.241872: step 45970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:40m:07s remains)
INFO - root - 2017-12-09 17:45:32.912025: step 45980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:44m:33s remains)
INFO - root - 2017-12-09 17:45:41.623772: step 45990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:32m:06s remains)
INFO - root - 2017-12-09 17:45:50.361325: step 46000, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.809 sec/batch; 64h:21m:58s remains)
2017-12-09 17:45:51.214333: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4481132 0.44051996 0.43333125 0.42772231 0.42151976 0.41624931 0.40930933 0.40207043 0.39510593 0.38696671 0.37818182 0.36738533 0.3545801 0.34156704 0.32849172][0.47014067 0.46273324 0.454731 0.44850123 0.44103512 0.43288267 0.42290336 0.41275546 0.40270373 0.39146882 0.37968856 0.36584592 0.35013723 0.33435366 0.31868845][0.47510955 0.46899635 0.46121615 0.45499489 0.446811 0.43761593 0.42643514 0.4144249 0.40227383 0.3885701 0.37425578 0.35736102 0.33882141 0.32053915 0.30279416][0.47022641 0.46700552 0.46239117 0.457687 0.45070058 0.44122761 0.42880893 0.41504171 0.40004775 0.38458636 0.36790732 0.34922865 0.32856119 0.30798286 0.28835011][0.45526382 0.45592472 0.45433211 0.45181271 0.44758233 0.43988651 0.42810822 0.41338205 0.39641228 0.37876329 0.35938415 0.33848843 0.31569466 0.29371205 0.27328557][0.43492016 0.43904108 0.43971914 0.43977091 0.43905491 0.43420884 0.4253439 0.41123664 0.39389363 0.37533459 0.35372469 0.33067974 0.3056089 0.2824485 0.26133564][0.40731949 0.41441134 0.41669479 0.41882473 0.42108274 0.41973177 0.41468996 0.40379485 0.38882861 0.37097934 0.34906021 0.32498839 0.29879209 0.27471814 0.25300398][0.37484792 0.38493344 0.39016116 0.39406258 0.39894515 0.40066993 0.39834583 0.39013168 0.37719294 0.36128753 0.3405484 0.3170014 0.29131809 0.26767167 0.24688733][0.34440321 0.35790041 0.36612087 0.37156802 0.37846556 0.38294113 0.38313338 0.3767162 0.36511067 0.35060769 0.33077663 0.30849689 0.28434169 0.26210496 0.24324039][0.31721261 0.33195755 0.34014764 0.3467792 0.35505688 0.3614648 0.36409295 0.36043766 0.35157198 0.33836961 0.32021752 0.2992402 0.27704933 0.25719759 0.24100585][0.28905421 0.30468616 0.313475 0.32143283 0.33062688 0.33834547 0.34277755 0.34208149 0.33643568 0.3258431 0.31042841 0.29212698 0.27275509 0.25541949 0.24170139][0.26273641 0.27892858 0.28910589 0.2984933 0.30828151 0.31634858 0.32163668 0.3230201 0.31935513 0.31133246 0.2991969 0.28457278 0.26904312 0.25465363 0.24354327][0.24337947 0.26023075 0.27128854 0.2812649 0.29140469 0.29968318 0.30552876 0.30701864 0.30421534 0.29840279 0.28898987 0.27749404 0.26531413 0.25448313 0.24639444][0.22931735 0.24482381 0.25472215 0.26473054 0.27473965 0.2832109 0.28987756 0.29287648 0.29233244 0.28885254 0.28240591 0.27424544 0.26553822 0.25785446 0.252239][0.21898016 0.2332295 0.24176583 0.251168 0.26079947 0.26923564 0.27610865 0.28049147 0.28212956 0.28093168 0.27744895 0.27268428 0.26745278 0.26278993 0.25937161]]...]
INFO - root - 2017-12-09 17:46:00.032493: step 46010, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 71h:09m:08s remains)
INFO - root - 2017-12-09 17:46:08.467520: step 46020, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 66h:57m:17s remains)
INFO - root - 2017-12-09 17:46:17.139620: step 46030, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 66h:44m:41s remains)
INFO - root - 2017-12-09 17:46:25.802692: step 46040, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:46m:13s remains)
INFO - root - 2017-12-09 17:46:34.436172: step 46050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:51m:25s remains)
INFO - root - 2017-12-09 17:46:43.147421: step 46060, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:45m:01s remains)
INFO - root - 2017-12-09 17:46:51.764843: step 46070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 69h:34m:27s remains)
INFO - root - 2017-12-09 17:47:00.573024: step 46080, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 70h:21m:18s remains)
INFO - root - 2017-12-09 17:47:09.284641: step 46090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:28m:24s remains)
INFO - root - 2017-12-09 17:47:17.932176: step 46100, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 68h:20m:26s remains)
2017-12-09 17:47:18.793154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034073703 -0.0034076592 -0.0034086404 -0.0034092383 -0.0034095775 -0.0034096111 -0.0034097091 -0.0034099966 -0.0034101314 -0.0034103638 -0.0034106323 -0.0034109275 -0.0034111855 -0.0034115361 -0.0034119617][-0.0034042441 -0.0034038476 -0.0034042106 -0.00340462 -0.0034050252 -0.0034052446 -0.0034054106 -0.0034055826 -0.003405913 -0.0034061822 -0.0034065086 -0.0034065847 -0.0034069396 -0.0034072583 -0.0034075063][-0.0034017209 -0.0034006257 -0.0034005893 -0.0034006143 -0.0034006324 -0.0034005435 -0.003400746 -0.0034010573 -0.0034014017 -0.0034018387 -0.0034020143 -0.0034018895 -0.0034023225 -0.00340292 -0.0034032105][-0.0033997493 -0.0033977746 -0.0033973523 -0.0033969656 -0.0033966412 -0.0033964415 -0.0033965642 -0.0033968536 -0.0033973563 -0.0033978669 -0.0033979565 -0.0033979348 -0.0033984387 -0.0033989807 -0.0033995702][-0.0033988955 -0.00339651 -0.0033955025 -0.0033947991 -0.0033944678 -0.0033943248 -0.0033944761 -0.0033949129 -0.003395742 -0.0033964978 -0.003396411 -0.0033963455 -0.0033965842 -0.0033971462 -0.0033975961][-0.0033984617 -0.0033958233 -0.0033945555 -0.0033937078 -0.0033929702 -0.0033923364 -0.0033920263 -0.0033927504 -0.0033939229 -0.0033951635 -0.0033954724 -0.0033959139 -0.0033961693 -0.0033965658 -0.003397003][-0.0033973311 -0.0033940272 -0.003391766 -0.0033897692 -0.003388097 -0.0033872686 -0.0033869948 -0.0033888421 -0.0033915914 -0.0033937381 -0.0033947625 -0.0033955823 -0.0033961583 -0.0033966948 -0.0033971604][-0.0033972657 -0.0033932149 -0.003390155 -0.0033866884 -0.0033835731 -0.0033815508 -0.0033812379 -0.0033838346 -0.0033878575 -0.0033914561 -0.0033938522 -0.0033949285 -0.0033957756 -0.0033965537 -0.003397305][-0.003396024 -0.0033910698 -0.0033869802 -0.0033828868 -0.0033796437 -0.0033775948 -0.0033780239 -0.0033812465 -0.0033855422 -0.0033899068 -0.0033931162 -0.0033947029 -0.0033954319 -0.0033959751 -0.0033966308][-0.0033967788 -0.003390983 -0.0033857166 -0.0033806178 -0.0033773507 -0.0033764383 -0.0033781345 -0.0033817699 -0.0033858924 -0.0033897159 -0.0033928945 -0.003394525 -0.0033952144 -0.003395811 -0.0033963853][-0.0033984166 -0.003392953 -0.003387905 -0.0033830034 -0.0033798015 -0.0033793251 -0.0033811505 -0.0033845531 -0.0033879152 -0.0033909925 -0.0033933523 -0.0033946997 -0.0033955469 -0.0033961 -0.0033966354][-0.0034012785 -0.0033978045 -0.0033939895 -0.0033900382 -0.0033870628 -0.0033863422 -0.0033874975 -0.0033899734 -0.00339235 -0.0033945208 -0.0033960543 -0.0033971362 -0.0033977223 -0.0033982196 -0.0033988259][-0.003403774 -0.0034022015 -0.0034004389 -0.0033979993 -0.0033955092 -0.0033941269 -0.0033944352 -0.003395997 -0.0033973462 -0.0033988007 -0.0034001176 -0.0034011053 -0.0034015405 -0.0034020413 -0.0034025908][-0.0034054373 -0.0034046143 -0.0034040513 -0.0034031065 -0.0034014334 -0.0034001914 -0.0034000447 -0.0034008594 -0.0034015442 -0.0034023933 -0.0034034033 -0.0034043423 -0.0034048622 -0.0034052846 -0.0034058259][-0.0034067645 -0.0034062304 -0.0034063789 -0.0034060883 -0.0034046138 -0.0034035954 -0.0034033956 -0.003403896 -0.0034043717 -0.0034049724 -0.0034059493 -0.0034068131 -0.0034072883 -0.0034076341 -0.0034080725]]...]
INFO - root - 2017-12-09 17:47:27.366217: step 46110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:14m:27s remains)
INFO - root - 2017-12-09 17:47:35.888462: step 46120, loss = 0.88, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 69h:03m:59s remains)
INFO - root - 2017-12-09 17:47:44.546710: step 46130, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 66h:27m:24s remains)
INFO - root - 2017-12-09 17:47:53.116173: step 46140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:26m:13s remains)
INFO - root - 2017-12-09 17:48:01.516500: step 46150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:14m:20s remains)
INFO - root - 2017-12-09 17:48:10.111371: step 46160, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.829 sec/batch; 65h:56m:47s remains)
INFO - root - 2017-12-09 17:48:18.975318: step 46170, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:32m:01s remains)
INFO - root - 2017-12-09 17:48:27.706846: step 46180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:31m:08s remains)
INFO - root - 2017-12-09 17:48:36.612637: step 46190, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 70h:21m:59s remains)
INFO - root - 2017-12-09 17:48:45.333628: step 46200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 70h:18m:56s remains)
2017-12-09 17:48:46.280590: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49811697 0.49291563 0.48325092 0.47015026 0.45565253 0.44153082 0.42923626 0.41989905 0.41006 0.39354289 0.36154532 0.314718 0.25479779 0.18872681 0.12921327][0.48575279 0.48251191 0.47480705 0.46430519 0.45140296 0.43769947 0.42500418 0.41504747 0.40372804 0.38445288 0.34993425 0.30139214 0.24106792 0.17582074 0.11745787][0.45186633 0.45500335 0.45306775 0.44770679 0.43902141 0.42843303 0.41649535 0.40587589 0.39273745 0.370505 0.33264196 0.28171861 0.22140051 0.15801163 0.10194082][0.41075236 0.42273155 0.42983404 0.43284169 0.43124837 0.425202 0.41582793 0.405466 0.39056316 0.36495173 0.32302684 0.26854971 0.20618829 0.14316334 0.088268444][0.36476263 0.38641906 0.40399671 0.41735578 0.42462915 0.42595568 0.42116198 0.4116731 0.39529818 0.36616609 0.32031167 0.26193267 0.19706438 0.13338296 0.0782901][0.31782103 0.34963971 0.37766546 0.40062025 0.41634697 0.42425084 0.42376858 0.41619474 0.3987408 0.36716864 0.31858262 0.25773317 0.19127238 0.12725556 0.072392672][0.272323 0.3134931 0.35106736 0.3822735 0.404332 0.41676605 0.4187417 0.41211966 0.39417753 0.36164397 0.31251013 0.251525 0.18541455 0.12230179 0.068280041][0.23811887 0.28583914 0.32913542 0.36451906 0.38870841 0.40198261 0.40333825 0.39590231 0.37731576 0.34556574 0.29859474 0.24035636 0.17734155 0.11679675 0.064797223][0.2162614 0.26593944 0.31021979 0.34543198 0.36749029 0.37696275 0.37433869 0.36433667 0.34544849 0.31578246 0.273423 0.22116241 0.16402406 0.1087433 0.060708821][0.20430511 0.25089711 0.29000956 0.31923974 0.33496475 0.33843467 0.33068815 0.31752169 0.29862687 0.27275535 0.23741086 0.19385028 0.14590079 0.098416343 0.056296386][0.19949795 0.23901099 0.26866943 0.28803259 0.29427624 0.28948289 0.27579638 0.259824 0.24213383 0.22120711 0.19422947 0.16095769 0.12365242 0.085642874 0.050958008][0.19018407 0.22048657 0.23970401 0.24892874 0.24674122 0.23558575 0.21797323 0.20052993 0.18485092 0.16919878 0.15046594 0.12704811 0.099969812 0.07116656 0.0436319][0.17097595 0.19076404 0.19973168 0.20037347 0.19239287 0.17783767 0.15964232 0.14349732 0.13122082 0.12097293 0.10949511 0.094652221 0.076516367 0.056055948 0.035393663][0.13737057 0.14805919 0.14969505 0.14540745 0.13558751 0.1215312 0.10613358 0.093489937 0.0854897 0.080273636 0.074688643 0.066452377 0.055276629 0.041781005 0.027178971][0.098843984 0.10242505 0.099350281 0.09284991 0.083310254 0.0720892 0.061191663 0.05305649 0.04919412 0.048033398 0.046993565 0.043815855 0.037878968 0.029567439 0.019757181]]...]
INFO - root - 2017-12-09 17:48:55.008261: step 46210, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 68h:29m:19s remains)
INFO - root - 2017-12-09 17:49:03.564223: step 46220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:50m:27s remains)
INFO - root - 2017-12-09 17:49:11.886048: step 46230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:52m:32s remains)
INFO - root - 2017-12-09 17:49:20.560475: step 46240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:19m:33s remains)
INFO - root - 2017-12-09 17:49:29.099702: step 46250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 69h:00m:48s remains)
INFO - root - 2017-12-09 17:49:37.549565: step 46260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:38m:06s remains)
INFO - root - 2017-12-09 17:49:46.212288: step 46270, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 71h:04m:15s remains)
INFO - root - 2017-12-09 17:49:54.879833: step 46280, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:10m:32s remains)
INFO - root - 2017-12-09 17:50:03.399832: step 46290, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 65h:37m:03s remains)
INFO - root - 2017-12-09 17:50:12.137747: step 46300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:12m:29s remains)
2017-12-09 17:50:13.025512: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.59601581 0.5871852 0.57885021 0.56798875 0.55407625 0.53465962 0.50854349 0.47406596 0.43304369 0.38859648 0.34141704 0.29277509 0.24070624 0.18898232 0.13720338][0.65402514 0.64722747 0.64135861 0.63357931 0.62238824 0.60383183 0.57730305 0.541898 0.49898654 0.45173827 0.39966834 0.34438598 0.28363785 0.2221957 0.16069505][0.68790275 0.684275 0.68123966 0.67744684 0.67088556 0.6564244 0.633034 0.59901828 0.55523139 0.50423282 0.44550887 0.38220206 0.31280151 0.24311258 0.17406288][0.70235753 0.70277363 0.70196551 0.70154315 0.69861567 0.68852192 0.66954976 0.63924313 0.59770137 0.545387 0.4810667 0.40893811 0.32935739 0.25100294 0.17563654][0.69460452 0.70126265 0.70462173 0.70768231 0.70813221 0.70150679 0.686486 0.66119748 0.62341988 0.57157338 0.504032 0.42528567 0.3372103 0.25072345 0.16950959][0.67235506 0.6842882 0.69143951 0.69925642 0.70476764 0.70294434 0.69254684 0.67126817 0.63664752 0.5858475 0.51613533 0.43217835 0.33791697 0.246216 0.1614961][0.644733 0.6624006 0.67162883 0.6823923 0.69174814 0.69558477 0.69102168 0.67491704 0.64381528 0.59392989 0.52294743 0.43537351 0.3365733 0.24083781 0.15376642][0.62258524 0.6459499 0.65611774 0.66650337 0.67524719 0.68053049 0.67876744 0.6672889 0.64048177 0.59340328 0.52364469 0.43489677 0.33427611 0.23659192 0.14831491][0.60985506 0.63817483 0.64909852 0.65763217 0.66371423 0.66722417 0.66439706 0.65327632 0.62790877 0.58401775 0.51750743 0.43084654 0.33125108 0.23347022 0.14555217][0.601351 0.63486105 0.64641386 0.65249836 0.6549533 0.65456235 0.648235 0.6349287 0.6085844 0.56637138 0.5032959 0.42136255 0.32638079 0.23153822 0.145714][0.598076 0.63264912 0.64184213 0.64370894 0.64177167 0.63706219 0.62689596 0.61024165 0.58258057 0.54268217 0.48392323 0.4078328 0.31893072 0.22951557 0.14778583][0.60573483 0.63614726 0.63792062 0.63180894 0.62158418 0.61000717 0.59468669 0.57524 0.54770392 0.51113111 0.45853359 0.39029062 0.30907065 0.22578019 0.14841217][0.61888617 0.64325982 0.63503814 0.6179406 0.59674579 0.57534319 0.55240804 0.52871013 0.50102794 0.46840191 0.42272046 0.36324009 0.29139885 0.21649916 0.14537355][0.61924672 0.63907278 0.62323469 0.59590924 0.56338859 0.53201842 0.50168705 0.47346473 0.44515038 0.41572034 0.37669221 0.32634684 0.26467 0.1994679 0.13654579][0.59832925 0.61320806 0.59103352 0.55657303 0.51648772 0.47708929 0.44034162 0.40890843 0.38089716 0.35462934 0.32206976 0.28052339 0.22943681 0.17487782 0.12131206]]...]
INFO - root - 2017-12-09 17:50:21.474716: step 46310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:23m:45s remains)
INFO - root - 2017-12-09 17:50:30.102756: step 46320, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 71h:40m:46s remains)
INFO - root - 2017-12-09 17:50:38.691897: step 46330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 69h:43m:40s remains)
INFO - root - 2017-12-09 17:50:47.430508: step 46340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 69h:46m:22s remains)
INFO - root - 2017-12-09 17:50:56.028558: step 46350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:28m:37s remains)
INFO - root - 2017-12-09 17:51:04.719938: step 46360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 66h:55m:49s remains)
INFO - root - 2017-12-09 17:51:13.464723: step 46370, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:32m:15s remains)
INFO - root - 2017-12-09 17:51:22.143928: step 46380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:58m:44s remains)
INFO - root - 2017-12-09 17:51:30.731668: step 46390, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 66h:59m:08s remains)
INFO - root - 2017-12-09 17:51:39.195957: step 46400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:45m:06s remains)
2017-12-09 17:51:40.036175: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015478453 0.021251155 0.0246193 0.026506152 0.025794121 0.023519989 0.020254554 0.016932011 0.014851222 0.013996571 0.01376226 0.01434347 0.016000668 0.018605469 0.019996671][0.016102754 0.021425281 0.024518959 0.025661124 0.024170388 0.020836834 0.016403275 0.012787269 0.01028901 0.0086694639 0.0085096154 0.00927126 0.011324897 0.014368858 0.016702456][0.016270002 0.021162251 0.023420857 0.023788601 0.02135733 0.017035164 0.012301106 0.0082820244 0.0053596809 0.0036395951 0.003496089 0.0040620961 0.0060061547 0.0088989809 0.011831485][0.016663572 0.021022605 0.02269737 0.022239592 0.018796468 0.013754444 0.0086858664 0.0045737443 0.0020460552 0.00031864853 9.5965574e-05 0.00043674395 0.0020587631 0.0044843419 0.0076781795][0.016930345 0.021081926 0.02195766 0.020903947 0.016355814 0.010860041 0.0057422542 0.0019340557 -0.00017083157 -0.0016143053 -0.0019198135 -0.0017811464 -0.00056376448 0.0014141193 0.0045829988][0.016932869 0.020623714 0.020809671 0.019210979 0.014041653 0.0083593223 0.0033253883 5.9489626e-05 -0.0014173843 -0.0024496072 -0.0027962928 -0.0028702545 -0.0021581538 -0.00056872005 0.0024244636][0.016484411 0.019778732 0.019000785 0.016667031 0.01122023 0.0058599566 0.0014791996 -0.0010370456 -0.0021721963 -0.0028011487 -0.0030929926 -0.0031476691 -0.002809624 -0.0015494609 0.0012480216][0.015319694 0.018001048 0.016568786 0.013377326 0.0079636425 0.003317981 -0.00014024391 -0.001868585 -0.0026427805 -0.0029659127 -0.0032344211 -0.0031819786 -0.0030478514 -0.0019592075 0.0006122198][0.013169076 0.015087664 0.013008088 0.009667417 0.0049060979 0.0010174252 -0.0014495549 -0.002494724 -0.0030466809 -0.003064445 -0.0032504387 -0.0031068339 -0.0030048275 -0.0020879151 0.00026585604][0.0098471269 0.01102645 0.0089787673 0.0057813344 0.0020470498 -0.00080133649 -0.0024685946 -0.0029552144 -0.0032336307 -0.0030895 -0.0032474205 -0.0030561173 -0.0029934277 -0.0021049366 0.00011862069][0.0057511623 0.0063743428 0.0048674108 0.002303286 -0.00025996496 -0.0021529335 -0.002989206 -0.0031040427 -0.0032849906 -0.003166439 -0.0032697325 -0.0030927488 -0.0029682061 -0.0021016849 -3.7308317e-05][0.0018255203 0.0022565024 0.0013513919 -0.0003214546 -0.0018573178 -0.002858981 -0.0032447781 -0.003224374 -0.0033145265 -0.003215272 -0.003297501 -0.0031868906 -0.0030221809 -0.0023194251 -0.00045508146][-0.0010342568 -0.00076483632 -0.0011932515 -0.0020589526 -0.0028239512 -0.0032181686 -0.0033242672 -0.0032389825 -0.003316693 -0.0033036929 -0.0033258223 -0.003293023 -0.0031433986 -0.0026090473 -0.0011789787][-0.0025833193 -0.002472986 -0.002647368 -0.0029928465 -0.003246411 -0.0033421237 -0.0033475314 -0.0033218474 -0.0033396955 -0.0033596293 -0.0033547694 -0.0033572277 -0.0032400349 -0.0030117081 -0.0020498387][-0.0032160252 -0.0031792477 -0.0032280665 -0.0033119719 -0.0033583385 -0.003359996 -0.003364553 -0.003359251 -0.0033498525 -0.0033612803 -0.0033526639 -0.0033562265 -0.0033261445 -0.0032242092 -0.0027522261]]...]
INFO - root - 2017-12-09 17:51:48.672238: step 46410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:20m:13s remains)
INFO - root - 2017-12-09 17:51:57.301393: step 46420, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 70h:05m:56s remains)
INFO - root - 2017-12-09 17:52:05.807786: step 46430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:03m:25s remains)
INFO - root - 2017-12-09 17:52:14.491692: step 46440, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 66h:54m:13s remains)
INFO - root - 2017-12-09 17:52:22.973395: step 46450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 71h:00m:17s remains)
INFO - root - 2017-12-09 17:52:31.603043: step 46460, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:22m:44s remains)
INFO - root - 2017-12-09 17:52:40.221315: step 46470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:21m:02s remains)
INFO - root - 2017-12-09 17:52:48.860286: step 46480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:13m:09s remains)
INFO - root - 2017-12-09 17:52:57.458887: step 46490, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 67h:23m:10s remains)
INFO - root - 2017-12-09 17:53:05.980965: step 46500, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 66h:01m:21s remains)
2017-12-09 17:53:06.867423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033860798 -0.0033841417 -0.0033839939 -0.0033842046 -0.0033844144 -0.0033845783 -0.0033845264 -0.0033844085 -0.0033844393 -0.003384287 -0.0033841203 -0.0033839545 -0.0033839445 -0.0033840034 -0.0033839438][-0.0033844227 -0.0033824097 -0.0033824313 -0.0033828805 -0.0033832635 -0.003383372 -0.0033832367 -0.0033830153 -0.003382945 -0.0033827294 -0.0033825289 -0.003382358 -0.0033823249 -0.0033823177 -0.0033822344][-0.0033843275 -0.0033824474 -0.0033827687 -0.0033836523 -0.0033843415 -0.0033845855 -0.0033844076 -0.0033841571 -0.0033839478 -0.0033835506 -0.0033830521 -0.0033826153 -0.0033823615 -0.0033822551 -0.0033821512][-0.0033841766 -0.0033824716 -0.0033831776 -0.0033846274 -0.0033859904 -0.003386704 -0.0033866831 -0.003386359 -0.0033860642 -0.003385399 -0.0033844956 -0.0033835017 -0.0033828884 -0.003382531 -0.0033822632][-0.0033842188 -0.0033825194 -0.0033835338 -0.0033857166 -0.0033878218 -0.003389162 -0.0033893913 -0.0033891711 -0.003388992 -0.0033881709 -0.0033867515 -0.0033851503 -0.0033840102 -0.0033831552 -0.0033826043][-0.0033843836 -0.0033827785 -0.003384128 -0.003386952 -0.0033895932 -0.0033914046 -0.0033919835 -0.0033919245 -0.0033919585 -0.0033911662 -0.0033893369 -0.0033871294 -0.0033853285 -0.0033839922 -0.0033831177][-0.0033846349 -0.0033833575 -0.0033850493 -0.0033883217 -0.0033914761 -0.0033937362 -0.0033948207 -0.0033952289 -0.0033955418 -0.0033945991 -0.0033923264 -0.003389332 -0.0033866169 -0.003384724 -0.0033835901][-0.0033850321 -0.0033841743 -0.0033862509 -0.0033899504 -0.0033936487 -0.0033966 -0.0033984792 -0.0033995421 -0.0033998925 -0.0033982075 -0.0033951669 -0.0033914957 -0.003387955 -0.0033853462 -0.0033839548][-0.003385439 -0.0033850283 -0.00338767 -0.0033916889 -0.003395767 -0.0033993244 -0.0034022972 -0.0034038955 -0.0034037651 -0.0034012308 -0.0033974857 -0.0033929995 -0.0033886565 -0.0033854609 -0.0033838933][-0.0033859254 -0.0033858451 -0.0033889585 -0.003393417 -0.0033976792 -0.003401638 -0.0034052252 -0.0034070283 -0.0034061219 -0.0034026857 -0.0033983483 -0.0033933932 -0.0033885918 -0.0033850693 -0.0033834847][-0.0033861613 -0.003386409 -0.0033900484 -0.0033945977 -0.0033989181 -0.003402838 -0.0034063042 -0.0034077689 -0.003406306 -0.0034023707 -0.0033975884 -0.0033925707 -0.0033879124 -0.0033844435 -0.0033828693][-0.0033861257 -0.0033863676 -0.003390023 -0.0033943986 -0.0033987593 -0.0034025584 -0.0034054811 -0.0034064597 -0.003404768 -0.0034008212 -0.0033960165 -0.0033913858 -0.003387176 -0.0033838993 -0.0033823948][-0.0033859445 -0.0033857336 -0.0033889643 -0.003392779 -0.0033968247 -0.0034003537 -0.0034026941 -0.0034033887 -0.0034018152 -0.0033982936 -0.00339396 -0.0033897795 -0.0033862262 -0.0033834216 -0.003382049][-0.0033854991 -0.0033847061 -0.0033871625 -0.0033900342 -0.0033932333 -0.0033960787 -0.0033978836 -0.0033985295 -0.0033974319 -0.0033946654 -0.0033912745 -0.0033880668 -0.003385331 -0.0033831319 -0.0033819668][-0.0033850605 -0.0033837797 -0.0033853059 -0.0033871322 -0.0033891846 -0.0033909972 -0.0033921737 -0.0033925772 -0.003391827 -0.0033901366 -0.0033880859 -0.003386138 -0.0033843503 -0.0033829436 -0.0033821538]]...]
INFO - root - 2017-12-09 17:53:15.256481: step 46510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:20m:55s remains)
INFO - root - 2017-12-09 17:53:23.614161: step 46520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:40m:54s remains)
INFO - root - 2017-12-09 17:53:32.143016: step 46530, loss = 0.89, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 61h:58m:42s remains)
INFO - root - 2017-12-09 17:53:40.830318: step 46540, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 72h:42m:42s remains)
INFO - root - 2017-12-09 17:53:49.487374: step 46550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:23m:47s remains)
INFO - root - 2017-12-09 17:53:58.007230: step 46560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:39m:02s remains)
INFO - root - 2017-12-09 17:54:06.389804: step 46570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:26m:18s remains)
INFO - root - 2017-12-09 17:54:14.962594: step 46580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:44m:55s remains)
INFO - root - 2017-12-09 17:54:23.443502: step 46590, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:33m:57s remains)
INFO - root - 2017-12-09 17:54:32.042334: step 46600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:00m:04s remains)
2017-12-09 17:54:32.876058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033885017 -0.0033788288 -0.003368252 -0.0033580649 -0.0033501731 -0.0033456446 -0.003343482 -0.0033436737 -0.0033452909 -0.003347534 -0.0033498644 -0.0033516416 -0.00335324 -0.0033538449 -0.0033537759][-0.0033911166 -0.0033835454 -0.0033751677 -0.0033665928 -0.0033597136 -0.0033556223 -0.0033537736 -0.0033541811 -0.0033556838 -0.0033573923 -0.003359281 -0.0033610531 -0.00336264 -0.0033634074 -0.0033633967][-0.0033949376 -0.0033908428 -0.0033859115 -0.0033805724 -0.0033754741 -0.0033725379 -0.003371028 -0.0033712375 -0.003371984 -0.0033730054 -0.0033743279 -0.0033749738 -0.0033754455 -0.0033758974 -0.0033762769][-0.0033949974 -0.0033938934 -0.0033924968 -0.0033906768 -0.0033882444 -0.0033867303 -0.0033857105 -0.0033857825 -0.0033855243 -0.0033855692 -0.0033860519 -0.0033859038 -0.003385291 -0.0033847801 -0.0033851811][-0.0033921734 -0.0033928824 -0.0033941874 -0.0033956345 -0.0033958005 -0.0033956293 -0.0033952137 -0.0033949164 -0.0033937986 -0.0033926072 -0.0033917944 -0.0033910752 -0.0033904538 -0.0033900768 -0.0033907718][-0.0033870521 -0.0033878614 -0.0033907234 -0.0033943278 -0.0033963961 -0.0033973297 -0.0033969409 -0.0033962179 -0.003394474 -0.0033924363 -0.0033903783 -0.0033893667 -0.0033890321 -0.0033892319 -0.0033901567][-0.0033836875 -0.0033827417 -0.003385426 -0.0033897923 -0.003393139 -0.0033945756 -0.003394539 -0.003393519 -0.0033910179 -0.0033882135 -0.0033851352 -0.0033834123 -0.0033825205 -0.0033826383 -0.0033838949][-0.0033837571 -0.0033815757 -0.0033833585 -0.0033870479 -0.0033907488 -0.0033929278 -0.003393173 -0.0033919315 -0.0033887958 -0.0033852572 -0.0033814348 -0.0033789077 -0.0033776553 -0.0033774276 -0.0033788304][-0.0033865003 -0.0033842325 -0.0033855103 -0.0033885166 -0.0033925022 -0.003395319 -0.0033960883 -0.0033954573 -0.0033921907 -0.0033877087 -0.0033825461 -0.0033790525 -0.0033775854 -0.0033769892 -0.0033783023][-0.0033880512 -0.0033858668 -0.0033867462 -0.0033895443 -0.0033936296 -0.0033967453 -0.0033983977 -0.0033986927 -0.0033963937 -0.0033919658 -0.0033863429 -0.0033813783 -0.0033790958 -0.003377686 -0.0033781996][-0.0033879583 -0.0033853992 -0.0033853531 -0.0033872819 -0.0033908195 -0.0033939322 -0.0033961548 -0.0033976468 -0.0033962841 -0.0033929066 -0.0033879115 -0.0033830635 -0.0033799545 -0.0033778609 -0.003377391][-0.0033861273 -0.0033827936 -0.0033816854 -0.0033824886 -0.0033850947 -0.0033880952 -0.003390735 -0.003392698 -0.0033925257 -0.0033905578 -0.0033871953 -0.0033828889 -0.0033796043 -0.0033772078 -0.0033762634][-0.0033829859 -0.0033795456 -0.0033785002 -0.0033783424 -0.0033795873 -0.0033818281 -0.003384203 -0.0033857189 -0.0033858642 -0.0033849855 -0.0033830495 -0.0033801249 -0.0033781875 -0.003376466 -0.0033755621][-0.0033804607 -0.0033770483 -0.0033759298 -0.0033755158 -0.0033762432 -0.0033772707 -0.0033784267 -0.0033790253 -0.0033792402 -0.0033788139 -0.0033771587 -0.0033750257 -0.0033741717 -0.0033733211 -0.0033724727][-0.0033802702 -0.003377489 -0.0033767712 -0.0033759605 -0.0033759268 -0.0033756425 -0.0033757016 -0.0033755363 -0.0033753638 -0.0033749212 -0.0033739158 -0.0033727966 -0.0033725763 -0.0033722529 -0.003371533]]...]
INFO - root - 2017-12-09 17:54:41.594182: step 46610, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 68h:50m:27s remains)
INFO - root - 2017-12-09 17:54:50.088870: step 46620, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.717 sec/batch; 56h:55m:55s remains)
INFO - root - 2017-12-09 17:54:58.719199: step 46630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:32m:30s remains)
INFO - root - 2017-12-09 17:55:07.331371: step 46640, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:45m:32s remains)
INFO - root - 2017-12-09 17:55:15.888881: step 46650, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.884 sec/batch; 70h:09m:52s remains)
INFO - root - 2017-12-09 17:55:24.504858: step 46660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:21m:39s remains)
INFO - root - 2017-12-09 17:55:33.292265: step 46670, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 71h:05m:13s remains)
INFO - root - 2017-12-09 17:55:41.990495: step 46680, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:25m:57s remains)
INFO - root - 2017-12-09 17:55:50.697305: step 46690, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 65h:52m:36s remains)
INFO - root - 2017-12-09 17:55:59.455551: step 46700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:09m:55s remains)
2017-12-09 17:56:00.322281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00047095539 0.0013482003 0.0048191543 0.0094422139 0.013834244 0.017824417 0.02106519 0.024024475 0.026055768 0.025363492 0.020807393 0.013047718 0.0054778312 0.00013228576 -0.0024165928][-0.00070103421 0.001157721 0.0047293236 0.0095857792 0.014519334 0.018884068 0.02212204 0.02495395 0.026870005 0.026290685 0.022158638 0.01456284 0.0066364356 0.0008337535 -0.002154246][-0.0012966061 0.00041438616 0.0036942405 0.0081197005 0.012621166 0.016552312 0.019653179 0.022564629 0.024730626 0.024751481 0.021516412 0.014902607 0.0073901159 0.0014417991 -0.0018827239][-0.0022273785 -0.00077397819 0.0019154162 0.0054778433 0.0092267264 0.01254528 0.015281083 0.01804512 0.02061794 0.02163234 0.019633381 0.014250426 0.0074645975 0.001752059 -0.0016933517][-0.0029441176 -0.0021400023 -0.00027048215 0.0024081049 0.0053419541 0.00795456 0.010389719 0.013111927 0.01587907 0.017495655 0.016550913 0.012514642 0.00680465 0.0016681664 -0.0016432649][-0.0032374149 -0.0028769937 -0.0018767586 -0.00038251234 0.0015297551 0.0035368977 0.0057621929 0.0084741414 0.011504702 0.013647823 0.013399862 0.010317663 0.0056189536 0.0012047165 -0.001752681][-0.0032390929 -0.003067588 -0.0026928275 -0.002079627 -0.0011799831 -4.6161236e-05 0.0017294227 0.0043880525 0.0077069639 0.010319352 0.010629617 0.0082306694 0.0043069394 0.00055793533 -0.0019643889][-0.0033551427 -0.0032866804 -0.0030579385 -0.0028120098 -0.0026330526 -0.0022269413 -0.0011456383 0.00099803088 0.0042017032 0.0070728981 0.0078885239 0.0061411695 0.0029828302 -0.0001301649 -0.0022059605][-0.0033856947 -0.0033981453 -0.0033060377 -0.003123713 -0.0030897777 -0.0029897464 -0.0025041718 -0.001056408 0.0015686057 0.0042016208 0.0052470346 0.0040943138 0.0016993382 -0.000768485 -0.0024200962][-0.0033907017 -0.0033948692 -0.003388135 -0.0033128883 -0.0032308097 -0.0032084738 -0.0029511664 -0.0020172722 -8.0143567e-05 0.0020097429 0.0029466487 0.0021649469 0.00041188672 -0.001443862 -0.0026668184][-0.0033910291 -0.0033898582 -0.0033887494 -0.0033875417 -0.0033464183 -0.003290646 -0.0030941758 -0.0024346705 -0.0009978211 0.0005194447 0.0011837124 0.00059098843 -0.00072651007 -0.0020669284 -0.0029076324][-0.0033862072 -0.0033847706 -0.0033842018 -0.0033838239 -0.0033824183 -0.0033676981 -0.003233898 -0.0028217575 -0.0019068752 -0.00096008508 -0.00055758492 -0.00097110146 -0.0018255957 -0.0026451675 -0.0031289032][-0.0033825305 -0.003381256 -0.0033807987 -0.0033804732 -0.0033799352 -0.0033713323 -0.0033256849 -0.0031466729 -0.0027070181 -0.0022552218 -0.0020581204 -0.0022704953 -0.0026856097 -0.0030688909 -0.0032821768][-0.0033790406 -0.0033793321 -0.0033795759 -0.0033793356 -0.0033787417 -0.0033788888 -0.0033766979 -0.0033201196 -0.0031886543 -0.0030792197 -0.003033818 -0.0031077 -0.0032255019 -0.0033204069 -0.0033681896][-0.0033834698 -0.0033804541 -0.0033798453 -0.0033790187 -0.0033788565 -0.0033788101 -0.0033790278 -0.0033788227 -0.0033735659 -0.0033661285 -0.0033638927 -0.0033695714 -0.0033772546 -0.0033825585 -0.0033852933]]...]
INFO - root - 2017-12-09 17:56:09.083091: step 46710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:01m:45s remains)
INFO - root - 2017-12-09 17:56:17.645414: step 46720, loss = 0.89, batch loss = 0.69 (10.9 examples/sec; 0.736 sec/batch; 58h:25m:00s remains)
INFO - root - 2017-12-09 17:56:26.286646: step 46730, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 67h:06m:31s remains)
INFO - root - 2017-12-09 17:56:34.875698: step 46740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:21m:51s remains)
INFO - root - 2017-12-09 17:56:43.337445: step 46750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:14m:19s remains)
INFO - root - 2017-12-09 17:56:51.952953: step 46760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:33m:52s remains)
INFO - root - 2017-12-09 17:57:00.651906: step 46770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:12m:04s remains)
INFO - root - 2017-12-09 17:57:09.376867: step 46780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 69h:21m:05s remains)
INFO - root - 2017-12-09 17:57:18.115067: step 46790, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:26m:33s remains)
INFO - root - 2017-12-09 17:57:26.996592: step 46800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:23m:50s remains)
2017-12-09 17:57:27.835253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030255355 -0.0031619382 -0.0032195088 -0.0031293072 -0.0029044626 -0.0026633153 -0.002572421 -0.0026619232 -0.0028545242 -0.0030621197 -0.0032305853 -0.0033157158 -0.0032298465 -0.0028926651 -0.0023526894][-0.0029924088 -0.0030979544 -0.0031027419 -0.0028949911 -0.0024843849 -0.00207505 -0.0018973222 -0.0020278385 -0.0023472332 -0.0027083722 -0.0030242249 -0.0031919936 -0.0031202803 -0.002683881 -0.0019053121][-0.0030085447 -0.0030376818 -0.0029646577 -0.0026095482 -0.0019535488 -0.0013131506 -0.0010039913 -0.0011307101 -0.0015725213 -0.0021183549 -0.0026410539 -0.0029480443 -0.0029537037 -0.0024726852 -0.0014952703][-0.0030589579 -0.0029915606 -0.0028548297 -0.0023905518 -0.0015100057 -0.00060326816 -0.00010318332 -0.00018421886 -0.00071196747 -0.0014124045 -0.0021357457 -0.00261117 -0.0027457757 -0.0022594316 -0.001160735][-0.0031420183 -0.0030126248 -0.0028396803 -0.0023400611 -0.0013754726 -0.00028121215 0.00044351304 0.00050187972 -2.8033042e-05 -0.00081626186 -0.0016927484 -0.0023083352 -0.0025312451 -0.0020335517 -0.000869317][-0.0032382435 -0.0031082372 -0.002937553 -0.0024964793 -0.0016066388 -0.00049426267 0.00036554341 0.00060129422 0.00018182769 -0.00056916941 -0.001478089 -0.0021476345 -0.0023668292 -0.0017830668 -0.00055332319][-0.0033203575 -0.0032281338 -0.0030923467 -0.0027705578 -0.0020978318 -0.0011570924 -0.00031236256 5.3670257e-05 -0.00019326434 -0.00077794911 -0.0015697506 -0.0021510157 -0.0022416913 -0.0015063582 -0.00021176971][-0.0033654857 -0.0033276926 -0.0032401427 -0.003041157 -0.0026195098 -0.001978273 -0.0013273172 -0.00094460952 -0.0010059385 -0.0013436032 -0.0018924788 -0.0022698361 -0.0021545538 -0.0012519378 8.0711208e-05][-0.0033767659 -0.0033740366 -0.003334461 -0.0032378475 -0.0030217392 -0.0026705249 -0.0022755044 -0.0019877497 -0.0019422546 -0.002050092 -0.0023041698 -0.0024155194 -0.0020842713 -0.0010805584 0.00024293899][-0.0033745542 -0.0033908659 -0.0033784357 -0.0033394487 -0.0032544876 -0.0031083557 -0.0029161612 -0.0027410313 -0.0026627413 -0.0026262342 -0.002653101 -0.0025516346 -0.0020507588 -0.00098455139 0.000328416][-0.0033767698 -0.0033904347 -0.0033917231 -0.003381605 -0.0033543753 -0.0033098944 -0.0032348342 -0.0031335759 -0.0030514349 -0.0029572682 -0.0028655161 -0.0026509874 -0.0020621419 -0.00094865309 0.00039602607][-0.0033784129 -0.0033862 -0.0033891345 -0.0033902538 -0.0033856349 -0.003375029 -0.0033425104 -0.003276262 -0.0031938893 -0.0030911693 -0.0029818704 -0.0027338902 -0.0020975708 -0.00095117581 0.00044704485][-0.0033810521 -0.0033825231 -0.0033844013 -0.0033862465 -0.0033873592 -0.0033866402 -0.0033711207 -0.0033182183 -0.0032401579 -0.0031527227 -0.0030655572 -0.0028434214 -0.0022330929 -0.0010924113 0.00031548692][-0.0033803228 -0.0033805664 -0.0033814895 -0.0033823857 -0.0033831871 -0.0033830337 -0.0033757894 -0.003344659 -0.0032812131 -0.0032156426 -0.0031570778 -0.0029891145 -0.0024665995 -0.0014449907 -0.0001684553][-0.0033802192 -0.0033798751 -0.0033801075 -0.0033804951 -0.0033808139 -0.0033809703 -0.0033784967 -0.0033635958 -0.0033291574 -0.003283442 -0.0032400747 -0.0031403678 -0.0027805553 -0.00200002 -0.0009822736]]...]
INFO - root - 2017-12-09 17:57:36.491686: step 46810, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 68h:53m:34s remains)
INFO - root - 2017-12-09 17:57:44.962263: step 46820, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.719 sec/batch; 57h:02m:05s remains)
INFO - root - 2017-12-09 17:57:53.540247: step 46830, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 69h:37m:12s remains)
INFO - root - 2017-12-09 17:58:01.954801: step 46840, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 67h:49m:29s remains)
INFO - root - 2017-12-09 17:58:10.635206: step 46850, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 70h:37m:54s remains)
INFO - root - 2017-12-09 17:58:19.323670: step 46860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:45m:43s remains)
INFO - root - 2017-12-09 17:58:27.899323: step 46870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:05m:30s remains)
INFO - root - 2017-12-09 17:58:36.502877: step 46880, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:26m:33s remains)
INFO - root - 2017-12-09 17:58:45.155316: step 46890, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-09 17:58:53.775214: step 46900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:15m:33s remains)
2017-12-09 17:58:54.679620: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26913568 0.27309105 0.27634898 0.2784144 0.27726164 0.2750816 0.27117264 0.26648363 0.26037902 0.25683379 0.25868833 0.26316035 0.26616266 0.26616177 0.26389432][0.24779661 0.24946056 0.25115082 0.25405347 0.25492969 0.25451225 0.25460136 0.25277132 0.25107184 0.25148207 0.25692472 0.2660197 0.27345672 0.27872106 0.28144822][0.22401682 0.22267835 0.22174115 0.22380786 0.22449784 0.22501431 0.22519577 0.2279949 0.23299134 0.23904981 0.24986546 0.26351297 0.27684969 0.28613344 0.2926122][0.20546775 0.2018678 0.19816235 0.19810022 0.19688204 0.19661199 0.1969139 0.20178415 0.20992829 0.2222205 0.23931913 0.25834784 0.27703378 0.290732 0.30092081][0.19902077 0.19505242 0.189534 0.18698129 0.18370964 0.18193433 0.18175153 0.18634292 0.1963466 0.21144292 0.23060568 0.25249761 0.27322572 0.28918087 0.30122387][0.20115416 0.1977783 0.19102193 0.18695307 0.18269247 0.17903653 0.1779359 0.18171844 0.19235805 0.20801312 0.22756721 0.25012428 0.27095515 0.28730378 0.29870874][0.20508268 0.20232671 0.19457644 0.18886796 0.18306704 0.17913166 0.17787237 0.18219605 0.19271204 0.20848359 0.22735511 0.248612 0.26751468 0.28241292 0.29261273][0.20930617 0.20875244 0.2018293 0.19598028 0.18990302 0.18653116 0.18525577 0.19037114 0.19992636 0.21436979 0.23089792 0.2484736 0.26345322 0.27497646 0.2829645][0.20736994 0.2095968 0.20483835 0.20086178 0.19684932 0.19443633 0.19484775 0.19938199 0.20759606 0.21999857 0.23308024 0.24715377 0.25842357 0.266864 0.27281424][0.1982194 0.20163557 0.19869712 0.19714279 0.19609956 0.19634771 0.19851889 0.20358112 0.21121721 0.22097452 0.2314565 0.24223308 0.25010991 0.25642741 0.26143563][0.17963085 0.18178146 0.17993997 0.18015787 0.18180932 0.18464267 0.189234 0.19585696 0.20383862 0.21298696 0.22218056 0.2307073 0.23667596 0.241995 0.24694185][0.1532294 0.15199071 0.14840212 0.14834173 0.15111546 0.1562302 0.16323824 0.17107242 0.17967972 0.18915451 0.19822106 0.20613079 0.21130213 0.21688104 0.22360981][0.12198674 0.11750512 0.11196839 0.11080635 0.11317046 0.11827955 0.12522879 0.13271964 0.14140117 0.15075576 0.16004886 0.16829748 0.17436367 0.18125765 0.19025087][0.088043846 0.08140976 0.074853912 0.072998434 0.074844666 0.078983977 0.084568642 0.090583719 0.098070033 0.10647217 0.11532556 0.12366281 0.13043123 0.13849472 0.14971706][0.059544954 0.051394191 0.044832885 0.042779498 0.043888744 0.046933919 0.051216755 0.055626638 0.061153751 0.067556538 0.074703358 0.081779249 0.087872155 0.095529132 0.1071071]]...]
INFO - root - 2017-12-09 17:59:03.160138: step 46910, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 65h:49m:22s remains)
INFO - root - 2017-12-09 17:59:11.698476: step 46920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:51m:02s remains)
INFO - root - 2017-12-09 17:59:20.207163: step 46930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:11m:14s remains)
INFO - root - 2017-12-09 17:59:28.800554: step 46940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:24m:39s remains)
INFO - root - 2017-12-09 17:59:37.270998: step 46950, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 70h:24m:04s remains)
INFO - root - 2017-12-09 17:59:45.961935: step 46960, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:06m:48s remains)
INFO - root - 2017-12-09 17:59:54.600890: step 46970, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 65h:52m:18s remains)
INFO - root - 2017-12-09 18:00:03.466468: step 46980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:46m:27s remains)
INFO - root - 2017-12-09 18:00:12.125016: step 46990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:23m:26s remains)
INFO - root - 2017-12-09 18:00:20.868169: step 47000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:33m:02s remains)
2017-12-09 18:00:21.719882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032555507 -0.0033192912 -0.0033659867 -0.0033845573 -0.0033940992 -0.003396746 -0.0033979428 -0.0033964086 -0.003392037 -0.00338748 -0.0033889548 -0.0033933253 -0.003395705 -0.0033957646 -0.0033963791][-0.0032394428 -0.0032923215 -0.0033423295 -0.0033642191 -0.003377307 -0.0033838728 -0.0033908319 -0.0033920798 -0.0033898202 -0.0033878421 -0.0033910868 -0.0033955018 -0.0033962603 -0.0033957881 -0.0033957209][-0.0032255717 -0.0032834793 -0.0033242048 -0.0033314866 -0.0033408152 -0.0033482255 -0.0033602191 -0.0033679942 -0.0033730746 -0.0033773661 -0.0033868966 -0.0033944882 -0.0033964133 -0.0033960782 -0.0033950435][-0.00323224 -0.0032729257 -0.0032918183 -0.0032929676 -0.0032890942 -0.0032842802 -0.0032954852 -0.0033098143 -0.0033252877 -0.0033420189 -0.0033632468 -0.0033811866 -0.0033922479 -0.0033955697 -0.0033952142][-0.0032723744 -0.0032952242 -0.0032824255 -0.0032607829 -0.0032264998 -0.003213356 -0.003215177 -0.0032282432 -0.0032493165 -0.0032817209 -0.0033218111 -0.003357366 -0.0033829359 -0.0033940934 -0.0033948342][-0.003310597 -0.003313944 -0.0032805146 -0.0032461139 -0.0031858019 -0.0031551253 -0.0031395417 -0.0031507029 -0.0031734523 -0.0032187998 -0.0032764606 -0.0033321192 -0.0033735544 -0.0033918726 -0.0033938822][-0.0033358252 -0.0033365302 -0.0032956277 -0.0032353164 -0.0031489751 -0.0031068064 -0.0030864021 -0.0030977179 -0.0031175129 -0.0031748959 -0.0032469789 -0.0033184655 -0.0033694056 -0.0033896561 -0.0033924719][-0.0033516714 -0.003337469 -0.0032868709 -0.0032132508 -0.0031228915 -0.0030789005 -0.0030527443 -0.0030661789 -0.0030931956 -0.0031592723 -0.0032389686 -0.0033180539 -0.0033710198 -0.0033892081 -0.0033929851][-0.0033589676 -0.0033311367 -0.0032696282 -0.003189079 -0.0030946878 -0.003058065 -0.0030407696 -0.0030651358 -0.0030966341 -0.0031660686 -0.003249825 -0.0033280384 -0.0033755344 -0.0033899306 -0.0033947157][-0.0033502923 -0.0033007043 -0.0032269175 -0.0031483725 -0.0030650173 -0.0030473208 -0.003045042 -0.0030844994 -0.00312553 -0.0031956695 -0.0032740897 -0.0033428271 -0.0033795037 -0.0033897851 -0.0033954086][-0.0033287308 -0.0032439509 -0.0031588324 -0.0030841762 -0.0030163361 -0.0030281465 -0.0030422318 -0.0030947658 -0.0031479006 -0.0032241116 -0.003299122 -0.0033569038 -0.0033823282 -0.0033903541 -0.0033964943][-0.00331207 -0.0032149069 -0.0031195122 -0.0030483126 -0.0029954521 -0.0030254426 -0.0030548284 -0.0031200834 -0.0031822692 -0.0032587091 -0.0033246966 -0.0033690541 -0.0033860586 -0.0033930496 -0.0033988131][-0.0033094478 -0.0032199176 -0.0031308248 -0.0030633407 -0.0030182975 -0.0030566296 -0.00309668 -0.0031686022 -0.0032338281 -0.0033002398 -0.0033509058 -0.0033801377 -0.0033905383 -0.0033956408 -0.003399252][-0.0033242928 -0.0032557971 -0.0031832294 -0.0031253872 -0.0030862936 -0.0031205104 -0.0031645568 -0.0032339043 -0.0032900514 -0.0033398515 -0.0033719679 -0.0033877806 -0.0033934969 -0.0033963185 -0.0033975423][-0.0033484059 -0.0033010989 -0.0032481123 -0.0032039504 -0.0031735285 -0.0031969764 -0.0032357548 -0.0032945427 -0.0033360305 -0.003366373 -0.0033830237 -0.0033911038 -0.003395 -0.0033962326 -0.0033959656]]...]
INFO - root - 2017-12-09 18:00:30.574237: step 47010, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.993 sec/batch; 78h:46m:29s remains)
INFO - root - 2017-12-09 18:00:39.310278: step 47020, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 71h:58m:51s remains)
INFO - root - 2017-12-09 18:00:47.764074: step 47030, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 67h:21m:47s remains)
INFO - root - 2017-12-09 18:00:56.141304: step 47040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:42m:51s remains)
INFO - root - 2017-12-09 18:01:04.668371: step 47050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:51m:11s remains)
INFO - root - 2017-12-09 18:01:13.287457: step 47060, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 64h:20m:43s remains)
INFO - root - 2017-12-09 18:01:21.870133: step 47070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:21m:57s remains)
INFO - root - 2017-12-09 18:01:30.422081: step 47080, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 67h:26m:41s remains)
INFO - root - 2017-12-09 18:01:39.150611: step 47090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:54m:55s remains)
INFO - root - 2017-12-09 18:01:47.761869: step 47100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:04m:35s remains)
2017-12-09 18:01:48.605199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00335918 -0.0033558498 -0.0033554984 -0.0033554584 -0.0033554577 -0.0033556186 -0.0033557748 -0.0033559296 -0.0033560609 -0.0033561289 -0.0033561771 -0.0033561666 -0.0033561504 -0.0033561322 -0.0033561592][-0.0033571555 -0.0033534523 -0.0033530665 -0.0033530379 -0.003353022 -0.0033530993 -0.0033531568 -0.0033531678 -0.0033531869 -0.0033532071 -0.0033532516 -0.0033532013 -0.0033531534 -0.0033532185 -0.0033533245][-0.0033574593 -0.0033537019 -0.0033533766 -0.0033533336 -0.0033532935 -0.0033533073 -0.0033533024 -0.0033532043 -0.003353087 -0.0033530418 -0.0033530046 -0.0033528707 -0.00335274 -0.00335277 -0.0033529398][-0.0033573331 -0.0033536102 -0.0033533713 -0.0033534341 -0.0033535678 -0.0033537955 -0.0033539853 -0.0033539569 -0.0033537666 -0.0033534691 -0.0033531091 -0.0033526076 -0.0033522011 -0.0033521121 -0.0033523093][-0.0033570211 -0.0033533329 -0.0033532861 -0.0033536421 -0.0033542302 -0.003355043 -0.0033558505 -0.0033561843 -0.0033560614 -0.0033554442 -0.0033544768 -0.0033531743 -0.0033520123 -0.0033515429 -0.0033516516][-0.003356362 -0.0033527585 -0.0033530421 -0.003353914 -0.0033552188 -0.0033570016 -0.00335881 -0.0033598354 -0.0033598791 -0.0033588663 -0.0033571017 -0.0033547194 -0.0033524949 -0.0033513193 -0.0033510316][-0.0033553243 -0.0033519992 -0.0033526714 -0.0033541436 -0.0033562288 -0.0033589811 -0.0033616954 -0.0033634973 -0.0033638652 -0.0033627274 -0.0033603068 -0.0033569462 -0.0033537005 -0.0033515966 -0.0033506702][-0.0033543676 -0.0033514078 -0.0033524355 -0.0033543892 -0.0033570996 -0.0033605755 -0.0033639388 -0.0033662557 -0.0033669705 -0.0033658079 -0.0033629816 -0.0033589869 -0.0033550186 -0.00335221 -0.0033507484][-0.0033537759 -0.0033509491 -0.0033523296 -0.0033547375 -0.0033578991 -0.0033617648 -0.0033654389 -0.0033680403 -0.0033689733 -0.0033676953 -0.0033645898 -0.0033603471 -0.0033561178 -0.0033529757 -0.0033511876][-0.0033536004 -0.003350816 -0.0033524269 -0.0033549652 -0.0033582128 -0.0033620596 -0.0033656927 -0.0033683006 -0.0033692042 -0.0033679851 -0.0033650529 -0.0033610645 -0.003357098 -0.0033540193 -0.0033520991][-0.0033541953 -0.0033512646 -0.0033527915 -0.0033550144 -0.0033578721 -0.0033612081 -0.0033642231 -0.0033664536 -0.003367323 -0.0033664131 -0.0033640908 -0.0033608163 -0.0033575629 -0.0033548835 -0.0033530416][-0.0033554474 -0.003352135 -0.0033532907 -0.003354839 -0.0033568726 -0.0033591152 -0.0033611085 -0.003362664 -0.00336339 -0.003362939 -0.0033614975 -0.003359427 -0.0033573108 -0.0033554272 -0.0033540323][-0.0033568728 -0.0033530644 -0.0033537492 -0.0033545014 -0.0033555115 -0.0033565769 -0.0033575287 -0.0033582768 -0.003358721 -0.0033586135 -0.0033580104 -0.0033571306 -0.0033561874 -0.0033552591 -0.0033545033][-0.0033577827 -0.0033537047 -0.0033541564 -0.0033544076 -0.0033547627 -0.0033550542 -0.003355342 -0.0033556412 -0.0033558477 -0.0033558731 -0.003355657 -0.0033553778 -0.0033550276 -0.0033546053 -0.0033541853][-0.00335758 -0.0033533399 -0.0033535832 -0.0033536204 -0.003353674 -0.0033536642 -0.0033536756 -0.0033537112 -0.0033537559 -0.0033537564 -0.0033536558 -0.0033535443 -0.0033534088 -0.0033532153 -0.003352999]]...]
INFO - root - 2017-12-09 18:01:57.341424: step 47110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:11m:53s remains)
INFO - root - 2017-12-09 18:02:05.959324: step 47120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:11m:58s remains)
INFO - root - 2017-12-09 18:02:14.555668: step 47130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 70h:04m:03s remains)
INFO - root - 2017-12-09 18:02:22.843231: step 47140, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 65h:07m:04s remains)
INFO - root - 2017-12-09 18:02:31.339133: step 47150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:24m:09s remains)
INFO - root - 2017-12-09 18:02:39.875903: step 47160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:23m:12s remains)
INFO - root - 2017-12-09 18:02:48.493987: step 47170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:50m:43s remains)
INFO - root - 2017-12-09 18:02:57.104195: step 47180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:28m:49s remains)
INFO - root - 2017-12-09 18:03:05.701164: step 47190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:34m:31s remains)
INFO - root - 2017-12-09 18:03:14.525770: step 47200, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 68h:18m:46s remains)
2017-12-09 18:03:15.391733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033779549 -0.0033760034 -0.003375415 -0.00337442 -0.0033734289 -0.0033728005 -0.0033722797 -0.0033719174 -0.0033715654 -0.0033712292 -0.0033709123 -0.0033705486 -0.0033702597 -0.0033699351 -0.0033696478][-0.0033814013 -0.0033790856 -0.003378097 -0.0033763335 -0.0033746387 -0.0033734685 -0.0033724352 -0.0033715381 -0.0033707267 -0.0033701826 -0.003369817 -0.0033694094 -0.003369072 -0.0033688021 -0.0033684678][-0.0033868386 -0.0033843007 -0.0033826481 -0.003380172 -0.0033775736 -0.0033758176 -0.0033742634 -0.0033729856 -0.0033718466 -0.0033711519 -0.003370597 -0.003370004 -0.0033696534 -0.0033693938 -0.0033689784][-0.0033917842 -0.0033889494 -0.0033865168 -0.0033834614 -0.0033804616 -0.00337842 -0.0033764 -0.0033748029 -0.0033733037 -0.003372327 -0.0033716438 -0.00337084 -0.003370235 -0.0033697712 -0.003369251][-0.0033967302 -0.0033939185 -0.0033910263 -0.0033874661 -0.0033840409 -0.0033815757 -0.003379374 -0.0033773356 -0.0033751884 -0.0033739987 -0.0033732119 -0.0033720741 -0.0033709996 -0.0033702126 -0.0033695321][-0.0034007952 -0.0033984908 -0.0033955767 -0.0033922326 -0.0033887629 -0.0033863788 -0.0033840465 -0.0033819058 -0.0033795394 -0.003377605 -0.0033759549 -0.0033739428 -0.0033721896 -0.0033709938 -0.0033700736][-0.0034028708 -0.0034010457 -0.003398414 -0.0033955707 -0.00339291 -0.0033910121 -0.003388914 -0.0033868072 -0.0033843666 -0.0033817238 -0.0033789626 -0.0033760222 -0.0033733973 -0.0033715032 -0.0033701917][-0.0034031125 -0.0034020324 -0.0034002529 -0.003398357 -0.0033967749 -0.003395478 -0.0033938927 -0.0033916265 -0.0033889515 -0.0033860574 -0.003382575 -0.0033787205 -0.0033750155 -0.0033722303 -0.0033703709][-0.0034024948 -0.0034021414 -0.0034009716 -0.0033996219 -0.0033987265 -0.0033981276 -0.0033972778 -0.0033957765 -0.0033936403 -0.0033906547 -0.0033867634 -0.0033819207 -0.0033771242 -0.0033733253 -0.0033708042][-0.0034013651 -0.0034014517 -0.0034006394 -0.0033989472 -0.0033978384 -0.0033978203 -0.0033978405 -0.0033976049 -0.0033963132 -0.0033936198 -0.0033896021 -0.0033843056 -0.0033787943 -0.0033742939 -0.0033712434][-0.0033988564 -0.0033988543 -0.0033981579 -0.0033962363 -0.0033949115 -0.003394807 -0.0033951341 -0.003395505 -0.0033950438 -0.0033929043 -0.0033893653 -0.0033845971 -0.0033793696 -0.0033747852 -0.0033715544][-0.0033949057 -0.0033950023 -0.0033942575 -0.003392352 -0.0033908172 -0.0033900316 -0.0033899988 -0.0033903269 -0.0033904358 -0.0033890877 -0.0033862889 -0.0033824926 -0.0033782306 -0.0033743631 -0.0033714634][-0.0033899487 -0.003389372 -0.0033886016 -0.0033865185 -0.0033845943 -0.0033834146 -0.0033830178 -0.0033831787 -0.0033836511 -0.0033832598 -0.0033816781 -0.0033790397 -0.003375964 -0.003373143 -0.0033708289][-0.0033840907 -0.0033824591 -0.0033813405 -0.0033793317 -0.0033776159 -0.003376578 -0.0033761007 -0.0033763645 -0.0033770006 -0.0033772523 -0.0033766774 -0.0033752157 -0.0033732993 -0.0033714392 -0.0033698936][-0.0033783538 -0.0033759843 -0.0033749258 -0.0033734615 -0.0033723428 -0.0033717456 -0.0033714625 -0.0033716641 -0.00337225 -0.0033726569 -0.0033725013 -0.0033718385 -0.0033709027 -0.0033699567 -0.0033690946]]...]
INFO - root - 2017-12-09 18:03:24.004999: step 47210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 70h:33m:15s remains)
INFO - root - 2017-12-09 18:03:32.693398: step 47220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:48m:47s remains)
INFO - root - 2017-12-09 18:03:41.236378: step 47230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:07m:54s remains)
INFO - root - 2017-12-09 18:03:49.794890: step 47240, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 58h:41m:08s remains)
INFO - root - 2017-12-09 18:03:58.387903: step 47250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:04m:43s remains)
INFO - root - 2017-12-09 18:04:07.052651: step 47260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:27m:09s remains)
INFO - root - 2017-12-09 18:04:15.804847: step 47270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:47m:19s remains)
INFO - root - 2017-12-09 18:04:24.565598: step 47280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:40m:55s remains)
INFO - root - 2017-12-09 18:04:33.289050: step 47290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:13m:52s remains)
INFO - root - 2017-12-09 18:04:41.907308: step 47300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:47m:38s remains)
2017-12-09 18:04:42.757725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028126379 -0.0016732601 0.0015078376 0.0063460981 0.011981911 0.017173387 0.020789851 0.0219395 0.020286869 0.016778646 0.012415464 0.0080741467 0.004376309 0.0013946823 -0.0008049109][-0.00090970681 0.00098138978 0.00597936 0.013697398 0.022954792 0.03181123 0.038255703 0.040656008 0.038481846 0.03271015 0.02493027 0.016864009 0.0098359985 0.0043369252 0.00045309193][0.0032130789 0.0071757808 0.015710933 0.028204232 0.043057483 0.057444029 0.068187453 0.0727439 0.07030604 0.061779905 0.049231928 0.035266504 0.022356071 0.011871764 0.0042733084][0.0088789193 0.016488567 0.030354545 0.049430341 0.0714338 0.092517346 0.10821603 0.11516087 0.11232214 0.10074311 0.082815975 0.061887484 0.041579228 0.02434724 0.011292925][0.015540234 0.027604422 0.047846209 0.0747418 0.1051501 0.134051 0.15568615 0.16570352 0.16265431 0.14761318 0.12353963 0.094642542 0.065667152 0.040299878 0.020625347][0.021790249 0.038376041 0.064672627 0.098797575 0.13699484 0.17329447 0.20081905 0.21454114 0.212581 0.19581077 0.16710453 0.13116302 0.093733639 0.059751589 0.032552328][0.025850918 0.045642924 0.075991556 0.11488518 0.15811853 0.1993297 0.23094162 0.24774717 0.24770556 0.23142911 0.20134933 0.16182062 0.11893983 0.0783767 0.04483581][0.026541583 0.047106888 0.078235954 0.1180982 0.16240506 0.2049322 0.23813406 0.25700173 0.2594057 0.24565293 0.21753468 0.17851983 0.13438238 0.091056913 0.054106642][0.023658473 0.042189818 0.070325844 0.10674711 0.1476821 0.18777628 0.22008792 0.23989192 0.24501407 0.23526561 0.21166834 0.17670007 0.13543162 0.093598507 0.057046715][0.018471293 0.032920253 0.055341307 0.084991142 0.1190521 0.1534206 0.18227763 0.20148453 0.20872675 0.20338002 0.18571654 0.15727453 0.12222172 0.085649073 0.05297431][0.012118313 0.022042476 0.037778128 0.059160832 0.084510632 0.11114686 0.13457324 0.1513685 0.15938069 0.15769748 0.14605136 0.12523074 0.098401532 0.069649361 0.043425672][0.0061766971 0.012025219 0.021646094 0.035185739 0.051845364 0.070167854 0.087155484 0.1002216 0.10740159 0.1078283 0.10106214 0.087403864 0.069060206 0.048996508 0.030383456][0.001315539 0.0043919841 0.0094978707 0.0168895 0.026362354 0.037274551 0.047946002 0.056707412 0.061996724 0.063079461 0.059487771 0.051470108 0.040426083 0.028265953 0.016938694][-0.001921386 -0.00066071423 0.0015848267 0.0049547274 0.0094529958 0.014862376 0.020398838 0.025201138 0.02829691 0.029168494 0.027434492 0.023313316 0.017643126 0.011539945 0.0059664994][-0.0032178345 -0.0029108557 -0.0022675232 -0.0011583285 0.00049960264 0.0026022464 0.004837255 0.0068556387 0.0082084863 0.0086132092 0.0079028979 0.0062398231 0.0039969105 0.0016707389 -0.00036931341]]...]
INFO - root - 2017-12-09 18:04:51.420495: step 47310, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:46m:55s remains)
INFO - root - 2017-12-09 18:05:00.149604: step 47320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:21m:28s remains)
INFO - root - 2017-12-09 18:05:08.591112: step 47330, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 68h:19m:28s remains)
INFO - root - 2017-12-09 18:05:17.083669: step 47340, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 62h:04m:54s remains)
INFO - root - 2017-12-09 18:05:25.464664: step 47350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 68h:00m:24s remains)
INFO - root - 2017-12-09 18:05:34.030365: step 47360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:20m:31s remains)
INFO - root - 2017-12-09 18:05:42.666700: step 47370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:35m:59s remains)
INFO - root - 2017-12-09 18:05:51.050157: step 47380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:08m:54s remains)
INFO - root - 2017-12-09 18:05:59.621480: step 47390, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 68h:28m:39s remains)
INFO - root - 2017-12-09 18:06:08.263228: step 47400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:00m:42s remains)
2017-12-09 18:06:09.128065: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10203931 0.10245922 0.10143278 0.09916079 0.096046418 0.092371657 0.08863911 0.085250668 0.082582228 0.080952018 0.080087654 0.079732195 0.079683281 0.079622343 0.078951746][0.097390361 0.097626872 0.096516274 0.094191723 0.091005489 0.087179825 0.083464384 0.08000014 0.077110142 0.07492324 0.0733526 0.072373822 0.071974196 0.071901813 0.07159669][0.088363849 0.087960072 0.086757131 0.084898785 0.0825189 0.079675123 0.076800562 0.073761389 0.07081233 0.068035871 0.065643094 0.06385421 0.062943809 0.06269107 0.062498417][0.0761668 0.075675033 0.0752002 0.074782275 0.074209653 0.073199078 0.071724035 0.069426246 0.066506267 0.063099653 0.059872467 0.057138275 0.055502281 0.054748345 0.054265946][0.060783308 0.061143156 0.062406067 0.064409927 0.0665513 0.068092488 0.068501458 0.067247614 0.064531051 0.060598146 0.056506429 0.052666746 0.050054103 0.048463039 0.047408536][0.043763917 0.045556866 0.04895338 0.053681593 0.058789205 0.06317547 0.065898918 0.066199765 0.064161032 0.06007373 0.055251703 0.050277997 0.046422794 0.043671437 0.041848835][0.027724862 0.030647475 0.035596136 0.042366415 0.049901891 0.056934677 0.062129553 0.064410992 0.063594416 0.059869055 0.054613668 0.04864886 0.043542195 0.039586544 0.036964193][0.015070243 0.018287 0.023664467 0.031214342 0.040102661 0.049100529 0.056539591 0.060930673 0.06160612 0.058597617 0.053241644 0.046626087 0.040533885 0.035523474 0.03217978][0.0066256188 0.0093565891 0.014228575 0.021527966 0.030766841 0.040782094 0.049636118 0.05549594 0.057296883 0.054989114 0.049772024 0.04293685 0.036330614 0.030702334 0.026939813][0.0020447166 0.0040381607 0.0080370139 0.014560651 0.023414364 0.033459961 0.04260974 0.048841745 0.050929982 0.048862617 0.043724552 0.036961187 0.0302906 0.024589408 0.020833114][4.3882057e-05 0.0015198973 0.0047759814 0.01043513 0.018433008 0.027657809 0.035997618 0.041438021 0.042874746 0.040446062 0.035268575 0.028810397 0.022562996 0.017407997 0.014195943][-0.00087994849 0.00043130643 0.0032552707 0.0081587881 0.015055105 0.022881925 0.029653035 0.033546947 0.033758823 0.030665558 0.025470674 0.019570453 0.014204783 0.010103629 0.007861698][-0.0015531726 -0.00029344554 0.0022671439 0.0065503558 0.012286134 0.018452855 0.023286056 0.025352588 0.024255341 0.020655103 0.015787689 0.01085711 0.0067815622 0.0040088771 0.0028862429][-0.0020329338 -0.00090336939 0.0013553577 0.0049687885 0.0095026046 0.013966769 0.016879674 0.017288433 0.01523192 0.011606687 0.0075539313 0.0039766217 0.0013876872 -7.9098623e-05 -0.00025192718][-0.0019788384 -0.0010257424 0.0008187925 0.00363231 0.006897537 0.0097207651 0.01100801 0.01027747 0.0078847883 0.0047760215 0.0018564335 -0.00032892264 -0.00164421 -0.0021625245 -0.0017893312]]...]
INFO - root - 2017-12-09 18:06:17.688853: step 47410, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:43m:45s remains)
INFO - root - 2017-12-09 18:06:26.300026: step 47420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:33m:40s remains)
INFO - root - 2017-12-09 18:06:34.758428: step 47430, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 66h:42m:13s remains)
INFO - root - 2017-12-09 18:06:43.487051: step 47440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:58m:36s remains)
INFO - root - 2017-12-09 18:06:51.624846: step 47450, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 63h:46m:10s remains)
INFO - root - 2017-12-09 18:07:00.300329: step 47460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 71h:08m:12s remains)
INFO - root - 2017-12-09 18:07:09.029393: step 47470, loss = 0.88, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 70h:37m:05s remains)
INFO - root - 2017-12-09 18:07:17.630594: step 47480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:33m:50s remains)
INFO - root - 2017-12-09 18:07:26.216526: step 47490, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.819 sec/batch; 64h:52m:17s remains)
INFO - root - 2017-12-09 18:07:35.033773: step 47500, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 68h:51m:06s remains)
2017-12-09 18:07:35.902002: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.029494476 0.031575747 0.033767216 0.036150303 0.037781779 0.038619682 0.038884126 0.039128006 0.03884735 0.038642466 0.037820753 0.036469363 0.034560028 0.03134831 0.027736058][0.029506259 0.032082397 0.035070382 0.038023103 0.040064849 0.04129646 0.041416246 0.041477386 0.041224979 0.041279163 0.040701423 0.039269429 0.037135191 0.033831771 0.029830027][0.02579096 0.028664961 0.032121334 0.035784312 0.038675919 0.040411618 0.040736642 0.041020054 0.040839806 0.041379251 0.041210748 0.040004984 0.037839144 0.034432672 0.030419547][0.021559792 0.024581367 0.028226994 0.032546807 0.036234859 0.038829722 0.039881922 0.040525012 0.040842623 0.041899689 0.042186793 0.041407611 0.039260406 0.035787292 0.03152708][0.016303308 0.019768355 0.023957733 0.028794091 0.033260528 0.036783975 0.038639847 0.040018007 0.041190989 0.042967439 0.043838356 0.043474536 0.041307442 0.037870202 0.033311322][0.010263647 0.013960385 0.018713752 0.024166854 0.029462203 0.033893518 0.036893256 0.03945737 0.041561622 0.044156898 0.045826852 0.045869585 0.043665282 0.040199134 0.035215855][0.0044867704 0.0078824749 0.012559324 0.018427171 0.024479773 0.029855728 0.034056716 0.037959449 0.04142724 0.0451062 0.047288615 0.047595125 0.045566835 0.041901071 0.036289908][0.0011124632 0.0031297405 0.0065576322 0.012231764 0.018421356 0.02444651 0.029731769 0.035019524 0.039850838 0.044591166 0.047406677 0.048153352 0.046170022 0.042272072 0.036289271][-0.00092984084 0.00013275398 0.0022693945 0.006457218 0.011681267 0.017939983 0.023866348 0.030091755 0.03597863 0.041696347 0.045161042 0.046382561 0.044753756 0.04095732 0.03453229][-0.0015095152 -0.001331161 -0.00044724019 0.0022074829 0.0060608974 0.011269606 0.016737381 0.023339847 0.029747713 0.035798687 0.039635833 0.041510232 0.040594537 0.037278507 0.030902468][-0.0015817358 -0.0015533594 -0.0013875666 -0.00027142372 0.0018379905 0.0056693628 0.01023041 0.015997307 0.021768089 0.027706677 0.031684488 0.034024056 0.033926807 0.031472206 0.025824554][-0.0019068589 -0.0018921561 -0.001739159 -0.0012012748 -0.00020711892 0.0020221865 0.0051348824 0.0097785341 0.014575482 0.019438472 0.022918431 0.025608972 0.026284503 0.024774937 0.020212494][-0.0026620831 -0.0023363684 -0.00212646 -0.001919415 -0.0015104989 -0.00025288132 0.001762104 0.0050667115 0.0086301444 0.012329512 0.015163711 0.017619519 0.018620446 0.017910203 0.01455645][-0.0032268725 -0.0031276671 -0.0029259932 -0.002682907 -0.0024166114 -0.001852231 -0.00074770488 0.0013564841 0.0036593436 0.0062016603 0.00839074 0.010421692 0.011404874 0.011029238 0.0086670667][-0.0033114087 -0.003323077 -0.0033220309 -0.0032951352 -0.0031516417 -0.0028278981 -0.0022466478 -0.0012493255 -8.1114704e-05 0.0013780117 0.0028362039 0.0044168821 0.0053283386 0.0050916607 0.0035267526]]...]
INFO - root - 2017-12-09 18:07:44.644582: step 47510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:36m:57s remains)
INFO - root - 2017-12-09 18:07:53.308007: step 47520, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 68h:17m:24s remains)
INFO - root - 2017-12-09 18:08:01.804543: step 47530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:11m:29s remains)
INFO - root - 2017-12-09 18:08:10.392209: step 47540, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:40m:19s remains)
INFO - root - 2017-12-09 18:08:18.831001: step 47550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 68h:04m:40s remains)
INFO - root - 2017-12-09 18:08:27.469217: step 47560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:33m:43s remains)
INFO - root - 2017-12-09 18:08:36.080163: step 47570, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 67h:59m:20s remains)
INFO - root - 2017-12-09 18:08:44.711760: step 47580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:38m:07s remains)
INFO - root - 2017-12-09 18:08:53.413470: step 47590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:48m:46s remains)
INFO - root - 2017-12-09 18:09:02.002587: step 47600, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 68h:12m:34s remains)
2017-12-09 18:09:02.839402: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0021423625 0.005988366 0.013579243 0.02545191 0.040346961 0.055477295 0.06715662 0.072887935 0.072004877 0.065817609 0.056403857 0.045847636 0.035904482 0.027704632 0.021965342][0.0032791186 0.0067968275 0.013987507 0.025428584 0.039585963 0.053449571 0.063183062 0.066358089 0.062655039 0.053945947 0.042930327 0.031924985 0.022528362 0.015493258 0.01100622][0.0046732081 0.0083636818 0.015550324 0.026635639 0.040023956 0.052676737 0.060762346 0.061907172 0.056010079 0.045301139 0.032969952 0.021553669 0.01259979 0.0065834131 0.0032096889][0.0050903158 0.0093586762 0.017183853 0.028697334 0.042096749 0.054219346 0.061314218 0.061134577 0.053772509 0.041700464 0.028348994 0.016470397 0.0076274062 0.002130619 -0.00062656635][0.0058743609 0.010474072 0.019058714 0.031398728 0.045303587 0.057505552 0.064297721 0.063542977 0.055410478 0.04248 0.028203893 0.015563249 0.0063119717 0.00077810162 -0.0018340907][0.0071499916 0.011973481 0.021047188 0.034093857 0.048760261 0.061659887 0.068971366 0.068545587 0.060489908 0.047249056 0.032178558 0.018420395 0.00803698 0.0016032399 -0.0015741485][0.0073976228 0.012290611 0.021614835 0.035149246 0.050509598 0.064386427 0.072794847 0.0736419 0.066878006 0.054478787 0.039343417 0.02455098 0.012538656 0.0043868329 -0.00016259612][0.0066684112 0.011486776 0.020813027 0.034525756 0.050391868 0.065099694 0.074693069 0.07722386 0.072370395 0.061712302 0.047494065 0.032400422 0.019009098 0.0088939145 0.00246541][0.005548683 0.0098929964 0.018623324 0.031900298 0.047670603 0.0627736 0.073385529 0.07741943 0.0744462 0.06569346 0.05292378 0.038375612 0.024519887 0.013178659 0.0052619409][0.0040888116 0.0078973714 0.015376647 0.02707701 0.041451134 0.0558005 0.066565216 0.0717029 0.070516378 0.063733958 0.052758709 0.039485913 0.026250293 0.01488939 0.0065670786][0.0018994242 0.0050126659 0.011041072 0.020488845 0.032359898 0.044659652 0.054444209 0.059869029 0.060025379 0.055184014 0.046386268 0.035216697 0.023745529 0.013628961 0.0060428381][-0.0001429026 0.0019004771 0.0061039468 0.012968359 0.021927079 0.031513318 0.039484121 0.044355378 0.045218773 0.042109389 0.035684109 0.027173134 0.018258339 0.010280869 0.0042437427][-0.0015648545 -0.00038907793 0.0021577838 0.0065288208 0.012507581 0.019172695 0.024995305 0.028826848 0.029892396 0.028109673 0.023873558 0.018062381 0.011890492 0.0063126003 0.0020649249][-0.0024727688 -0.0018664631 -0.00051316759 0.0019300187 0.0054418202 0.0095206387 0.013253109 0.01586977 0.016809016 0.015916722 0.013392265 0.0098073082 0.0059643593 0.0025107714 -8.7384833e-05][-0.0030301977 -0.0027597758 -0.0021070531 -0.000889427 0.00089493045 0.0030306045 0.0050890204 0.0066393716 0.0073153153 0.0069533121 0.0056226254 0.0036298323 0.0014601715 -0.0004572391 -0.0018270409]]...]
INFO - root - 2017-12-09 18:09:11.393870: step 47610, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 68h:26m:17s remains)
INFO - root - 2017-12-09 18:09:19.950909: step 47620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:10m:32s remains)
INFO - root - 2017-12-09 18:09:28.479521: step 47630, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 65h:50m:59s remains)
INFO - root - 2017-12-09 18:09:36.951727: step 47640, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 66h:35m:12s remains)
INFO - root - 2017-12-09 18:09:45.189172: step 47650, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 65h:02m:52s remains)
INFO - root - 2017-12-09 18:09:53.956790: step 47660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:56m:42s remains)
INFO - root - 2017-12-09 18:10:02.688185: step 47670, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:15m:42s remains)
INFO - root - 2017-12-09 18:10:11.272916: step 47680, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 69h:23m:17s remains)
INFO - root - 2017-12-09 18:10:19.725673: step 47690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:27m:34s remains)
INFO - root - 2017-12-09 18:10:28.301067: step 47700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:18m:35s remains)
2017-12-09 18:10:29.198168: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015382448 0.014380038 0.0121573 0.010451904 0.0089387493 0.0076291766 0.0062648309 0.0050325347 0.0038481879 0.0028068104 0.0018570202 0.0010047045 0.00020877714 -0.00067062583 -0.0015580687][0.018457286 0.017607862 0.015365407 0.013594391 0.011909319 0.01042746 0.0088128867 0.0073421355 0.0059128031 0.0046389895 0.0034392613 0.002265807 0.0010719893 -0.00021497742 -0.0014500143][0.020507293 0.019892879 0.017807081 0.016126778 0.014472945 0.013037153 0.011369496 0.0097887926 0.0081468746 0.0066294875 0.0051159561 0.0035209761 0.0018119344 4.4714427e-05 -0.0015176906][0.021237398 0.020973003 0.019136505 0.01766366 0.016209714 0.015028916 0.013526529 0.011990526 0.010183105 0.008380238 0.0064513832 0.0043663718 0.0021454056 -5.8431178e-06 -0.0017452718][0.020600829 0.020782815 0.019229552 0.018008087 0.016837858 0.016037749 0.014852175 0.013486421 0.011587171 0.0095154187 0.0071514146 0.0045908857 0.0019468532 -0.00041138357 -0.0021112422][0.018601976 0.019284936 0.018069033 0.017193787 0.01638278 0.016022641 0.015216298 0.014082685 0.01214003 0.0098124621 0.0070345318 0.0040911408 0.0012188402 -0.0010895235 -0.0025368594][0.015385414 0.016529704 0.015781816 0.015361227 0.015003761 0.015089862 0.014642075 0.013733052 0.011794826 0.0092587983 0.0061556268 0.0029926554 0.00015468569 -0.001857476 -0.0029274256][0.011391807 0.012808055 0.012533689 0.012622345 0.012797164 0.013304651 0.013200646 0.012504583 0.010606614 0.0079541914 0.0047156727 0.0015967607 -0.00094735855 -0.0025182164 -0.0031963023][0.0072192205 0.0086205807 0.0087039145 0.0091968682 0.0097784018 0.010578756 0.01076593 0.010271558 0.008521595 0.0059568612 0.0028899012 0.00012512854 -0.0019029002 -0.0029737947 -0.0033328806][0.0034641752 0.0045894925 0.0048750713 0.0055573145 0.0063305655 0.0072048483 0.0075189192 0.0071747554 0.0056902571 0.0034822619 0.00094027142 -0.0012034834 -0.0026044324 -0.0032267165 -0.0033800637][0.00048143254 0.0012212426 0.0015527543 0.002207625 0.0029545866 0.0037084736 0.0040148236 0.0037820672 0.0026443985 0.00099348719 -0.0008197336 -0.0022302759 -0.0030415636 -0.0033402743 -0.0033909506][-0.0016050301 -0.0012080609 -0.00092347781 -0.00042604259 0.00014882465 0.00068020052 0.0009136477 0.00075648515 -5.6058634e-06 -0.0010534532 -0.0021385802 -0.0029021183 -0.0032741136 -0.0033828218 -0.0033946792][-0.0027875581 -0.0026329746 -0.0024601421 -0.0021632053 -0.0018037527 -0.0014859441 -0.0013340309 -0.0014237587 -0.001843349 -0.0023851469 -0.0029105232 -0.0032355885 -0.0033661209 -0.0033952803 -0.003396482][-0.0032860283 -0.00324797 -0.0031756898 -0.0030504193 -0.0028964789 -0.0027535893 -0.0026714203 -0.0027050457 -0.0028727083 -0.0030793429 -0.0032676433 -0.0033680534 -0.0033979174 -0.0034005602 -0.0033974685][-0.0034108604 -0.0034034338 -0.0033846721 -0.0033503217 -0.003308238 -0.0032666535 -0.0032411984 -0.003249658 -0.0032898702 -0.003340391 -0.0033819659 -0.0034017349 -0.0034051065 -0.003403527 -0.0034010091]]...]
INFO - root - 2017-12-09 18:10:37.818472: step 47710, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 70h:46m:33s remains)
INFO - root - 2017-12-09 18:10:46.452103: step 47720, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 70h:57m:43s remains)
INFO - root - 2017-12-09 18:10:54.985141: step 47730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:56m:35s remains)
INFO - root - 2017-12-09 18:11:03.659332: step 47740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:33m:40s remains)
INFO - root - 2017-12-09 18:11:12.075485: step 47750, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 67h:27m:40s remains)
INFO - root - 2017-12-09 18:11:20.723621: step 47760, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:27m:10s remains)
INFO - root - 2017-12-09 18:11:29.459064: step 47770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:37m:48s remains)
INFO - root - 2017-12-09 18:11:38.128370: step 47780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:36m:44s remains)
INFO - root - 2017-12-09 18:11:46.542642: step 47790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:02m:09s remains)
INFO - root - 2017-12-09 18:11:55.071510: step 47800, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 65h:23m:24s remains)
2017-12-09 18:11:56.019668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.1212569e-05 0.0029078433 0.0090843048 0.018249743 0.028436173 0.037174463 0.042055368 0.04373008 0.043540604 0.042731524 0.042786151 0.045515798 0.052448794 0.063688017 0.076200984][0.0022357223 0.0050740521 0.012035721 0.023034438 0.035940658 0.0473488 0.054285478 0.056743439 0.055391569 0.051971309 0.049047887 0.049903437 0.056390319 0.068794519 0.083637334][0.0057035573 0.0089442618 0.017518871 0.031475652 0.04835001 0.06370233 0.073876031 0.077679195 0.075176008 0.06823568 0.060435176 0.056704685 0.059603222 0.070043437 0.08460255][0.0089678355 0.013535948 0.024695408 0.042744689 0.064531185 0.084747218 0.0988708 0.104559 0.10124307 0.09069562 0.077230208 0.066990726 0.06361869 0.069056027 0.080516271][0.011454204 0.017676834 0.031653095 0.054184396 0.081369668 0.10741422 0.12652296 0.13504857 0.13179903 0.11816563 0.098902166 0.081166983 0.069998592 0.068622082 0.075103343][0.012534284 0.020366874 0.036802951 0.063180722 0.095014095 0.1266779 0.15089412 0.16326798 0.16164577 0.14681487 0.12346262 0.099166378 0.080493279 0.071950562 0.07253135][0.012372323 0.020962052 0.038427245 0.066804424 0.10139024 0.13716431 0.16560394 0.18201476 0.18310097 0.16932207 0.14478321 0.1168525 0.093075022 0.078789033 0.073782876][0.011392915 0.019632433 0.036235817 0.063835278 0.098224491 0.13515159 0.16566859 0.18515377 0.18931736 0.17840636 0.1555627 0.12756164 0.10215655 0.084840387 0.075977214][0.0096247038 0.016585819 0.030613076 0.054587472 0.085443035 0.1198186 0.14937666 0.16989274 0.1766143 0.1694355 0.15045846 0.12563975 0.10199727 0.084647618 0.074227519][0.0071656294 0.012291436 0.022758931 0.041223451 0.065890372 0.09435486 0.11982602 0.13870752 0.14649795 0.14278454 0.12870222 0.10906662 0.089611813 0.074658111 0.064854376][0.0039101797 0.0073472094 0.014265263 0.026710834 0.043996882 0.064538591 0.083619975 0.098516084 0.10559421 0.10430975 0.09509667 0.081427045 0.067404658 0.056218103 0.048472025][0.00090790167 0.0028299941 0.0066616191 0.01375384 0.024084549 0.036847275 0.049219143 0.059265912 0.064508475 0.06435632 0.058938075 0.050498806 0.041613009 0.034365907 0.02921458][-0.0015106358 -0.00055722939 0.0012376499 0.004547297 0.009622612 0.016202189 0.022904728 0.028619308 0.031859502 0.032087736 0.029305659 0.024722029 0.019807547 0.015747458 0.012883366][-0.0029308412 -0.0025712075 -0.0019022848 -0.00067787897 0.0012809895 0.0039552269 0.0068378793 0.0094190966 0.010987119 0.011197213 0.010001796 0.0079429941 0.0057019517 0.0038259709 0.0025126846][-0.0033534218 -0.0033032917 -0.0031592844 -0.0028642584 -0.0023420937 -0.0015553607 -0.00065463595 0.00019362569 0.00074024172 0.0008386136 0.00045615458 -0.00024465914 -0.0010039429 -0.0016409146 -0.0020779921]]...]
INFO - root - 2017-12-09 18:12:04.531748: step 47810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:48m:05s remains)
INFO - root - 2017-12-09 18:12:13.032325: step 47820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:07m:37s remains)
INFO - root - 2017-12-09 18:12:21.491428: step 47830, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:46m:58s remains)
INFO - root - 2017-12-09 18:12:30.244348: step 47840, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 69h:42m:02s remains)
INFO - root - 2017-12-09 18:12:38.619108: step 47850, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 68h:43m:48s remains)
INFO - root - 2017-12-09 18:12:47.358590: step 47860, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 66h:00m:36s remains)
INFO - root - 2017-12-09 18:12:56.081532: step 47870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:35m:51s remains)
INFO - root - 2017-12-09 18:13:04.816256: step 47880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:06m:10s remains)
INFO - root - 2017-12-09 18:13:13.599644: step 47890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:26m:26s remains)
INFO - root - 2017-12-09 18:13:22.319356: step 47900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:27m:24s remains)
2017-12-09 18:13:23.195998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033629963 -0.0033602719 -0.0033601844 -0.003360338 -0.0033604526 -0.0033606072 -0.0033607348 -0.0033608428 -0.003360912 -0.0033608032 -0.0033607003 -0.0033606475 -0.0033606058 -0.0033605339 -0.0033604512][-0.0033608114 -0.0033580009 -0.003358044 -0.0033583392 -0.0033585478 -0.0033587618 -0.0033588961 -0.0033589795 -0.0033590205 -0.0033589853 -0.003358962 -0.0033589785 -0.0033589993 -0.0033589904 -0.0033589771][-0.0033608063 -0.0033580267 -0.0033581825 -0.0033585466 -0.003358793 -0.0033590207 -0.0033591134 -0.0033590836 -0.0033590472 -0.0033590931 -0.0033591697 -0.0033592503 -0.0033593564 -0.0033594114 -0.0033594263][-0.0033605017 -0.0033577248 -0.0033579557 -0.0033583504 -0.0033586642 -0.0033589664 -0.0033590952 -0.0033590314 -0.0033589466 -0.0033590074 -0.0033590491 -0.0033590537 -0.0033591448 -0.0033592144 -0.0033592316][-0.0033597816 -0.0033568325 -0.0033571012 -0.0033576291 -0.0033581581 -0.0033586121 -0.0033587925 -0.0033586798 -0.0033584612 -0.0033583627 -0.003358281 -0.0033581953 -0.0033582563 -0.0033583818 -0.0033585525][-0.0033591955 -0.0033559767 -0.0033561897 -0.0033567597 -0.0033573564 -0.0033578998 -0.0033581872 -0.0033581536 -0.0033577483 -0.0033573718 -0.0033570118 -0.0033566756 -0.003356474 -0.003356494 -0.0033567869][-0.0033589003 -0.0033555103 -0.0033556179 -0.0033562051 -0.0033569564 -0.0033577955 -0.0033582854 -0.0033582181 -0.0033576796 -0.0033569636 -0.0033561371 -0.0033551517 -0.003354389 -0.0033540893 -0.0033542896][-0.0033590873 -0.0033559073 -0.0033561881 -0.0033570833 -0.0033583073 -0.0033596449 -0.0033604833 -0.0033605814 -0.0033601092 -0.0033590714 -0.0033575222 -0.0033556595 -0.0033540316 -0.0033530053 -0.0033527685][-0.0033597625 -0.003356993 -0.0033578607 -0.0033595061 -0.0033614959 -0.0033635313 -0.003365081 -0.0033658934 -0.0033655318 -0.0033638973 -0.0033614878 -0.0033587657 -0.0033561464 -0.0033541531 -0.0033531212][-0.0033607516 -0.0033584314 -0.003360033 -0.0033623122 -0.0033648377 -0.0033673884 -0.0033694205 -0.0033707423 -0.0033705893 -0.0033689002 -0.0033662994 -0.0033630554 -0.0033595986 -0.0033566509 -0.0033547077][-0.0033615725 -0.0033595078 -0.0033616063 -0.0033641222 -0.0033668012 -0.0033695968 -0.0033719928 -0.003373699 -0.0033739659 -0.0033726695 -0.0033703619 -0.0033671316 -0.0033633322 -0.003359806 -0.0033571196][-0.0033619436 -0.0033600105 -0.00336223 -0.0033646293 -0.0033671746 -0.0033698576 -0.0033722327 -0.0033739207 -0.0033743663 -0.0033735079 -0.0033716892 -0.0033688974 -0.0033654291 -0.003362027 -0.0033591636][-0.0033618414 -0.003359657 -0.0033616424 -0.0033635723 -0.0033656049 -0.0033677127 -0.0033696252 -0.0033710294 -0.0033714015 -0.0033708673 -0.0033696373 -0.0033676499 -0.0033650962 -0.0033624673 -0.0033600882][-0.0033615816 -0.0033589618 -0.0033603825 -0.0033615648 -0.0033627744 -0.0033640531 -0.0033652594 -0.0033661511 -0.0033664629 -0.0033663516 -0.003365844 -0.0033647595 -0.0033632449 -0.0033615623 -0.0033599476][-0.0033612878 -0.0033581532 -0.003358874 -0.0033594044 -0.0033599471 -0.0033605562 -0.0033611893 -0.0033616389 -0.0033618773 -0.0033619578 -0.0033618745 -0.0033614216 -0.0033607017 -0.0033598987 -0.0033590251]]...]
INFO - root - 2017-12-09 18:13:31.757279: step 47910, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 68h:56m:36s remains)
INFO - root - 2017-12-09 18:13:40.328361: step 47920, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 68h:05m:41s remains)
INFO - root - 2017-12-09 18:13:48.821126: step 47930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:46m:20s remains)
INFO - root - 2017-12-09 18:13:57.326999: step 47940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:59m:22s remains)
INFO - root - 2017-12-09 18:14:05.659138: step 47950, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 71h:35m:33s remains)
INFO - root - 2017-12-09 18:14:14.378003: step 47960, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 71h:05m:08s remains)
INFO - root - 2017-12-09 18:14:23.056835: step 47970, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:23m:46s remains)
INFO - root - 2017-12-09 18:14:31.649193: step 47980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:10m:16s remains)
INFO - root - 2017-12-09 18:14:40.317283: step 47990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:27m:53s remains)
INFO - root - 2017-12-09 18:14:49.105132: step 48000, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:56m:37s remains)
2017-12-09 18:14:50.055605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032457386 -0.0032784028 -0.0033127146 -0.0033380887 -0.0033562144 -0.0033680887 -0.0033765049 -0.0033833939 -0.003386603 -0.0033933716 -0.0033932594 -0.0033857245 -0.0033757512 -0.0033668051 -0.0033595713][-0.0032215654 -0.0032520927 -0.0032859587 -0.0033128648 -0.0033302736 -0.003341997 -0.0033489352 -0.0033567648 -0.003364085 -0.0033700771 -0.003372764 -0.0033707991 -0.0033639278 -0.00335772 -0.0033522437][-0.0031608348 -0.0031982581 -0.0032385937 -0.0032666803 -0.0032880851 -0.0033049616 -0.003317069 -0.0033259322 -0.0033292337 -0.0033303644 -0.0033303481 -0.0033316922 -0.003330169 -0.0033251371 -0.0033188444][-0.0031081827 -0.003139603 -0.0031761832 -0.0032065425 -0.003232982 -0.0032568634 -0.0032760019 -0.0032891242 -0.0032967771 -0.0033008792 -0.0033038615 -0.0033047907 -0.003300888 -0.0032944535 -0.0032911429][-0.0030959926 -0.0031224112 -0.0031550834 -0.0031857362 -0.0032126589 -0.0032367741 -0.0032566898 -0.00327279 -0.0032834129 -0.0032918695 -0.0032993839 -0.0033025271 -0.0033022645 -0.003298054 -0.0032967879][-0.0031226163 -0.0031521553 -0.0031839006 -0.0032168464 -0.0032463395 -0.0032692561 -0.0032864255 -0.0032992361 -0.0033096906 -0.0033211231 -0.0033279683 -0.003330464 -0.0033298598 -0.0033286125 -0.0033317837][-0.003161557 -0.0031951016 -0.0032274243 -0.0032617059 -0.0032909019 -0.0033110976 -0.0033241659 -0.0033354822 -0.0033471312 -0.00335806 -0.003363966 -0.0033666973 -0.0033663656 -0.0033677167 -0.0033702734][-0.0032069178 -0.0032459688 -0.0032822622 -0.003314941 -0.0033398937 -0.0033539224 -0.0033610924 -0.0033672252 -0.0033729519 -0.0033786343 -0.0033818593 -0.003384541 -0.0033851364 -0.003386297 -0.0033873108][-0.0032523994 -0.0032876204 -0.0033247883 -0.0033560963 -0.0033747072 -0.003381086 -0.0033818113 -0.0033819047 -0.0033825745 -0.0033829827 -0.0033835378 -0.0033853415 -0.0033859552 -0.0033868898 -0.0033871226][-0.003280072 -0.003314937 -0.0033490488 -0.003374686 -0.0033892728 -0.0033940263 -0.0033920677 -0.0033881986 -0.0033859813 -0.0033839589 -0.0033828777 -0.0033837543 -0.0033844546 -0.0033855757 -0.0033858151][-0.0032988712 -0.0033294214 -0.003357657 -0.003379822 -0.0033916619 -0.0033962291 -0.0033952347 -0.0033915937 -0.003388775 -0.0033854593 -0.0033839978 -0.0033839084 -0.0033840979 -0.0033850255 -0.0033853422][-0.0033032631 -0.003328484 -0.0033605129 -0.0033829561 -0.0033939623 -0.0033964489 -0.0033960915 -0.0033937979 -0.0033913346 -0.0033881445 -0.0033862835 -0.0033849357 -0.0033843787 -0.003384314 -0.0033849082][-0.0032812976 -0.0033036633 -0.0033379854 -0.003361956 -0.0033814188 -0.0033933325 -0.003396099 -0.0033946305 -0.0033931029 -0.0033909206 -0.0033888484 -0.003387101 -0.0033858616 -0.0033851038 -0.0033851299][-0.0032519721 -0.0032773409 -0.0033188444 -0.0033446704 -0.0033655784 -0.0033809 -0.0033911325 -0.00339528 -0.0033956526 -0.0033939118 -0.0033921162 -0.0033895331 -0.003387027 -0.0033854481 -0.0033840796][-0.0031976406 -0.0032321666 -0.0032805684 -0.0033160213 -0.0033484579 -0.003371957 -0.003386332 -0.0033935236 -0.0033963723 -0.0033962585 -0.0033949022 -0.0033925455 -0.0033894749 -0.003386454 -0.0033838865]]...]
INFO - root - 2017-12-09 18:14:58.555004: step 48010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:43m:19s remains)
INFO - root - 2017-12-09 18:15:07.286909: step 48020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:17m:47s remains)
INFO - root - 2017-12-09 18:15:15.880558: step 48030, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 67h:46m:51s remains)
INFO - root - 2017-12-09 18:15:24.541876: step 48040, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 69h:20m:34s remains)
INFO - root - 2017-12-09 18:15:32.878258: step 48050, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 61h:56m:56s remains)
INFO - root - 2017-12-09 18:15:41.438765: step 48060, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 66h:56m:45s remains)
INFO - root - 2017-12-09 18:15:50.039560: step 48070, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 65h:22m:41s remains)
INFO - root - 2017-12-09 18:15:58.781875: step 48080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:36m:28s remains)
INFO - root - 2017-12-09 18:16:07.639117: step 48090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:15m:26s remains)
INFO - root - 2017-12-09 18:16:16.288228: step 48100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 69h:37m:21s remains)
2017-12-09 18:16:17.120408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033522113 -0.0033311804 -0.0032849316 -0.0032054847 -0.0030616566 -0.0027700386 -0.0023435871 -0.0018272666 -0.0013787446 -0.0010893904 -0.0010382163 -0.001280227 -0.0017146243 -0.002195592 -0.0025466008][-0.0031752756 -0.0030788812 -0.0029058431 -0.002664722 -0.0023272817 -0.0018628279 -0.0013194222 -0.00075778482 -0.00036248146 -0.00020200084 -0.00032425206 -0.00073141092 -0.0013318968 -0.0019237013 -0.0023332837][-0.0026872382 -0.0022851203 -0.0016409904 -0.00088116108 -1.2395903e-06 0.00095347781 0.0018176129 0.0024390835 0.0026664673 0.0025742268 0.0020568124 0.0011470155 -2.1620188e-05 -0.0011035951 -0.0019126859][-0.0017819175 -0.00062937359 0.0010813284 0.003079236 0.0052462164 0.0073130354 0.0089237774 0.0097842282 0.00977115 0.00915749 0.007905825 0.0060967179 0.003884207 0.0018017073 0.00012603239][-0.00042397738 0.0016656755 0.004851168 0.00872738 0.012884224 0.016725322 0.019602783 0.020989381 0.020844966 0.019591656 0.017393079 0.014409829 0.010909808 0.0075389864 0.0046746852][0.0012050201 0.0042930073 0.0089734793 0.014811426 0.021058686 0.026747817 0.030951964 0.033033133 0.032992356 0.03130056 0.028359359 0.024377648 0.0197345 0.015112642 0.011082566][0.0027140086 0.0066928864 0.012584846 0.019906551 0.027663296 0.034603998 0.039640002 0.042123124 0.042126976 0.040138222 0.036675572 0.032021232 0.026611455 0.021101924 0.016268389][0.0035317987 0.0079714386 0.014321003 0.022100795 0.030215712 0.037347555 0.042391554 0.044750474 0.044586182 0.04239355 0.038691111 0.033778742 0.028093083 0.022288363 0.017254939][0.0030655693 0.0073126592 0.013165703 0.0201478 0.027271938 0.033372119 0.037493654 0.039170876 0.038604856 0.036282718 0.032698289 0.02813988 0.022969212 0.017749274 0.013346517][0.0013873479 0.0047474485 0.0093028322 0.014621161 0.019895965 0.024239192 0.026969688 0.027770171 0.026842292 0.024635989 0.021605274 0.017994748 0.014051015 0.010183234 0.0070373425][-0.00064740796 0.0014775391 0.0043577417 0.0076770559 0.010892332 0.013429217 0.01486518 0.015014365 0.014053823 0.012347924 0.010254933 0.0079497285 0.0055607967 0.0033118087 0.0015556174][-0.0022043632 -0.0011589362 0.00027758791 0.0019211713 0.0034824098 0.00465957 0.0052423943 0.0051448466 0.0044826893 0.0034808386 0.0023567528 0.0012082811 8.60095e-05 -0.00092138723 -0.0016719381][-0.0030305064 -0.0026472206 -0.0021047671 -0.0014792315 -0.00089165359 -0.00046924618 -0.00029595289 -0.0004023728 -0.00073126261 -0.0011726904 -0.0016278613 -0.0020543039 -0.0024409865 -0.0027650096 -0.0029923583][-0.0033287895 -0.0032334626 -0.0030915972 -0.0029244553 -0.0027673333 -0.0026577567 -0.0026220761 -0.0026693696 -0.0027793522 -0.0029127484 -0.0030390632 -0.0031457627 -0.0032327408 -0.0032984898 -0.003340224][-0.0033938531 -0.0033814888 -0.003360722 -0.0033337008 -0.0033075614 -0.0032892053 -0.0032845128 -0.0032946717 -0.0033155475 -0.0033375882 -0.0033549869 -0.0033679004 -0.0033765645 -0.0033822653 -0.0033858654]]...]
INFO - root - 2017-12-09 18:16:25.634365: step 48110, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:24m:15s remains)
INFO - root - 2017-12-09 18:16:34.314921: step 48120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:38m:05s remains)
INFO - root - 2017-12-09 18:16:42.889090: step 48130, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:54m:59s remains)
INFO - root - 2017-12-09 18:16:51.511097: step 48140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:26m:09s remains)
INFO - root - 2017-12-09 18:16:59.944225: step 48150, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 62h:47m:12s remains)
INFO - root - 2017-12-09 18:17:08.407392: step 48160, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 67h:23m:40s remains)
INFO - root - 2017-12-09 18:17:17.086262: step 48170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:42m:29s remains)
INFO - root - 2017-12-09 18:17:25.881419: step 48180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:26m:11s remains)
INFO - root - 2017-12-09 18:17:34.609866: step 48190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:15m:23s remains)
INFO - root - 2017-12-09 18:17:43.528950: step 48200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:01m:04s remains)
2017-12-09 18:17:44.361860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033503994 -0.0033471084 -0.0033470329 -0.0033470837 -0.0033470863 -0.0033471838 -0.0033472814 -0.0033473477 -0.0033474059 -0.0033473619 -0.0033472187 -0.0033470138 -0.0033468804 -0.0033467491 -0.003346575][-0.0033478751 -0.003344221 -0.0033441444 -0.0033442648 -0.00334434 -0.0033444497 -0.0033445598 -0.0033446548 -0.0033447682 -0.0033447607 -0.0033446501 -0.0033444024 -0.003344205 -0.0033440664 -0.0033438753][-0.0033476958 -0.0033439405 -0.0033438853 -0.0033439943 -0.0033440681 -0.0033441242 -0.0033441645 -0.003344171 -0.0033442955 -0.0033443579 -0.0033442914 -0.0033441293 -0.0033440534 -0.0033439347 -0.0033437076][-0.0033472825 -0.0033434446 -0.0033433472 -0.0033434096 -0.0033433954 -0.0033433523 -0.003343353 -0.0033433351 -0.0033434567 -0.0033435873 -0.003343709 -0.0033438031 -0.0033439249 -0.0033439391 -0.0033437794][-0.0033468618 -0.0033428993 -0.0033427773 -0.003342727 -0.0033425293 -0.003342316 -0.0033421356 -0.0033419556 -0.0033419689 -0.0033422022 -0.0033425903 -0.0033430518 -0.003343537 -0.0033438308 -0.0033438809][-0.0033465023 -0.0033424865 -0.0033423938 -0.0033422515 -0.0033418324 -0.0033412778 -0.0033407537 -0.0033402713 -0.0033400464 -0.0033402841 -0.0033409346 -0.0033417845 -0.0033426448 -0.0033432809 -0.0033436185][-0.003346002 -0.0033421509 -0.0033420916 -0.003341841 -0.0033411384 -0.0033401283 -0.0033391016 -0.0033382117 -0.0033377635 -0.0033380876 -0.0033390603 -0.003340323 -0.003341546 -0.0033425023 -0.0033431097][-0.0033455859 -0.00334199 -0.0033420112 -0.0033416958 -0.003340754 -0.0033393619 -0.0033378815 -0.0033365937 -0.0033358543 -0.0033361563 -0.0033372927 -0.0033388163 -0.0033402978 -0.0033414839 -0.0033423654][-0.0033455384 -0.0033421135 -0.0033423142 -0.0033420112 -0.0033409141 -0.0033393363 -0.0033375886 -0.0033360631 -0.0033350657 -0.003335234 -0.0033363188 -0.0033378091 -0.0033393195 -0.0033406294 -0.0033416846][-0.0033456164 -0.0033423004 -0.00334277 -0.0033425754 -0.003341597 -0.0033400683 -0.0033383598 -0.0033368 -0.0033356873 -0.0033356063 -0.0033363833 -0.0033375793 -0.0033388762 -0.0033400175 -0.0033410648][-0.0033458339 -0.0033425184 -0.003343191 -0.0033431014 -0.0033423498 -0.0033411027 -0.0033397116 -0.0033383693 -0.0033373393 -0.0033370347 -0.0033374121 -0.0033381616 -0.0033390489 -0.0033398864 -0.0033407896][-0.0033460436 -0.0033425619 -0.0033434159 -0.0033434725 -0.0033430483 -0.0033422336 -0.0033412168 -0.0033402252 -0.0033393756 -0.0033388734 -0.0033388105 -0.0033390294 -0.0033394231 -0.0033399302 -0.0033406541][-0.0033461645 -0.003342414 -0.0033433717 -0.00334366 -0.0033436336 -0.003343367 -0.0033428182 -0.0033421558 -0.0033414231 -0.003340706 -0.0033402541 -0.0033399956 -0.0033399642 -0.0033401633 -0.0033406606][-0.0033459435 -0.0033420476 -0.0033430997 -0.0033435915 -0.0033438993 -0.0033440264 -0.0033438751 -0.0033434574 -0.0033428043 -0.0033420443 -0.003341394 -0.0033408366 -0.0033405265 -0.0033404678 -0.0033407235][-0.0033457177 -0.0033415919 -0.0033425777 -0.0033431998 -0.0033437249 -0.0033441407 -0.0033443044 -0.0033441272 -0.0033436348 -0.0033429086 -0.0033421514 -0.0033413668 -0.0033408715 -0.0033406515 -0.0033407584]]...]
INFO - root - 2017-12-09 18:17:53.036825: step 48210, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 67h:31m:06s remains)
INFO - root - 2017-12-09 18:18:01.620749: step 48220, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 68h:23m:20s remains)
INFO - root - 2017-12-09 18:18:10.069655: step 48230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:08m:58s remains)
INFO - root - 2017-12-09 18:18:18.878502: step 48240, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 67h:47m:34s remains)
INFO - root - 2017-12-09 18:18:27.536814: step 48250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 68h:35m:50s remains)
INFO - root - 2017-12-09 18:18:36.083111: step 48260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:14m:30s remains)
INFO - root - 2017-12-09 18:18:44.641581: step 48270, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:51m:00s remains)
INFO - root - 2017-12-09 18:18:53.313494: step 48280, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:16m:25s remains)
INFO - root - 2017-12-09 18:19:01.857565: step 48290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:53m:35s remains)
INFO - root - 2017-12-09 18:19:10.516900: step 48300, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 71h:02m:12s remains)
2017-12-09 18:19:11.432786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032083096 -0.0032950083 -0.0033576782 -0.0033667688 -0.0033677944 -0.0033688291 -0.0033699276 -0.0033210339 -0.0028203428 -0.001333609 0.0010761749 0.0036445293 0.0054512974 0.0060766153 0.0053206114][-0.0028593645 -0.0029431127 -0.0030845236 -0.0032571249 -0.0033486374 -0.0033679015 -0.0033577029 -0.0031249211 -0.0020885621 0.00043322076 0.0040754769 0.0076726042 0.0099644689 0.01041705 0.0088443449][-0.0027243961 -0.0027705426 -0.0028784098 -0.0030388427 -0.0031771478 -0.0031340988 -0.0027154526 -0.0013538464 0.0019187895 0.0078114662 0.015199126 0.022039376 0.026204392 0.026611961 0.022856288][-0.0024592227 -0.002322884 -0.0022538183 -0.0023262461 -0.0023933609 -0.0017746943 0.00027626147 0.00495297 0.013589651 0.026582725 0.041447137 0.05444736 0.061688177 0.061244875 0.052949496][-0.0008068874 -5.6379009e-05 0.00045437925 0.00065952027 0.00089503964 0.0026093116 0.007767099 0.018271346 0.03520732 0.058005847 0.08232709 0.10241866 0.11250452 0.10981673 0.094895266][0.00046113716 0.0019105845 0.0030625775 0.0037997528 0.0049426286 0.0087226238 0.018222872 0.035775702 0.061846286 0.09461686 0.12773599 0.15363055 0.16492319 0.15843311 0.1358905][0.0014265189 0.0034808018 0.0051118582 0.0061871316 0.0081291255 0.013623132 0.026656147 0.050070185 0.083467595 0.12384868 0.16312528 0.19239445 0.20326389 0.19251178 0.16320243][0.0013642325 0.0037784991 0.005842573 0.0073067071 0.0099402675 0.016887624 0.0322541 0.0585768 0.095132738 0.13832232 0.17910124 0.20812659 0.216935 0.20269586 0.16939852][0.00062556937 0.0027757636 0.0048188036 0.0065430012 0.0096234642 0.017035868 0.032709315 0.058826309 0.093971163 0.13444595 0.17164765 0.19700137 0.20272323 0.18664664 0.15330715][-0.0005072772 0.0011528884 0.0027558971 0.004213613 0.0070943963 0.013909795 0.027781762 0.05037323 0.080127656 0.11361735 0.14339298 0.16251846 0.16495833 0.1494357 0.12026022][-0.0017517231 -0.000654679 0.00050218124 0.0016338169 0.0038234266 0.00896088 0.019389883 0.03633095 0.058457777 0.082984574 0.10418674 0.11690742 0.11695389 0.1038581 0.081348754][-0.0026380785 -0.0020483083 -0.0013385667 -0.00055700378 0.00091496389 0.0042557432 0.010963658 0.021903668 0.036176484 0.051836297 0.064995363 0.072302267 0.071196795 0.061712146 0.046622809][-0.0031543907 -0.0029286006 -0.0025897278 -0.0021239794 -0.0012313803 0.00068885786 0.0044495817 0.01058577 0.018597532 0.027310479 0.034410432 0.037961334 0.03662974 0.030678231 0.02191693][-0.0033375293 -0.0032927331 -0.0031997338 -0.0030176509 -0.0026076797 -0.0017013567 6.97209e-05 0.0029858039 0.0068141874 0.010922115 0.014131774 0.015496966 0.014480234 0.011361459 0.0071218032][-0.0033739624 -0.0033707153 -0.0033663076 -0.0033401784 -0.0032325534 -0.0029293327 -0.0022898382 -0.0011975432 0.0002608737 0.0017973403 0.0029282272 0.0032897904 0.00274591 0.0014804108 -9.51828e-05]]...]
INFO - root - 2017-12-09 18:19:20.091397: step 48310, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:11m:23s remains)
INFO - root - 2017-12-09 18:19:28.796108: step 48320, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:56m:36s remains)
INFO - root - 2017-12-09 18:19:37.494432: step 48330, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.913 sec/batch; 72h:03m:04s remains)
INFO - root - 2017-12-09 18:19:46.369673: step 48340, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 70h:24m:26s remains)
INFO - root - 2017-12-09 18:19:54.993806: step 48350, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 68h:36m:29s remains)
INFO - root - 2017-12-09 18:20:03.647555: step 48360, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:30m:16s remains)
INFO - root - 2017-12-09 18:20:12.359368: step 48370, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 65h:28m:18s remains)
INFO - root - 2017-12-09 18:20:21.004289: step 48380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:13m:10s remains)
INFO - root - 2017-12-09 18:20:29.589886: step 48390, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 66h:50m:31s remains)
INFO - root - 2017-12-09 18:20:38.331862: step 48400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 68h:25m:01s remains)
2017-12-09 18:20:39.269766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001707534 -0.0026449859 -0.0029934305 -0.0031245467 -0.003051782 -0.0029984731 -0.0026758718 -0.0012067705 0.0026951937 0.0074622175 0.012666559 0.01427025 0.014799239 0.016762594 0.019260736][-9.844522e-05 -0.0013338116 -0.0018065776 -0.0023712625 -0.00239467 -0.0024361715 -0.0021608756 -0.00039700838 0.0042148316 0.010819358 0.018955244 0.023963341 0.027197195 0.031906139 0.036635384][0.0016628795 0.00044651632 5.5529876e-05 -0.00082513038 -0.00090349768 -0.0012627495 -0.000914213 0.0012187671 0.0069589168 0.015574068 0.027137456 0.036581658 0.04399744 0.05245126 0.059624761][0.0029491268 0.0020681939 0.0020560685 0.0011389351 0.0011735109 0.00068476866 0.0014600146 0.004444153 0.011915081 0.023259927 0.038522106 0.053237252 0.066330642 0.078428686 0.087501772][0.0033975469 0.0030537236 0.0035817034 0.0028229163 0.003165934 0.0029215778 0.0047522746 0.0094952751 0.019522088 0.033636179 0.052143469 0.072086409 0.090432845 0.10633814 0.11664013][0.0030180418 0.0033523508 0.0042014513 0.0037354471 0.0043124147 0.0045892186 0.0080623925 0.015457971 0.028646529 0.045773961 0.06703385 0.090592921 0.11269515 0.13082124 0.14111309][0.002004599 0.0030041698 0.0038434749 0.003715134 0.0042997422 0.0051327255 0.010228227 0.020456279 0.03685474 0.056800716 0.079884082 0.10521476 0.12885374 0.14757881 0.15706843][0.0017965299 0.0024780657 0.0027228377 0.0030358087 0.0034102441 0.0046666684 0.010640889 0.022837779 0.041478038 0.063470684 0.08753594 0.11365591 0.13708779 0.15492383 0.16257221][0.0011504006 0.0018022682 0.0018463975 0.0020233539 0.0018530486 0.0033512881 0.0091256071 0.021589931 0.040392023 0.063013315 0.087144025 0.11279357 0.13481843 0.15108117 0.15701447][0.00018710736 0.0010684447 0.0013080684 0.0014996987 0.0010073779 0.0019329342 0.006169484 0.017040532 0.033741802 0.054733325 0.077319913 0.10152377 0.1215715 0.13567191 0.14036579][-0.0010457323 -0.00021704938 0.0002391513 0.00089117815 0.00061887014 0.0010899527 0.00352516 0.011123114 0.023636559 0.040962666 0.060337439 0.081551522 0.09859062 0.1106931 0.11421112][-0.0025228148 -0.0019921944 -0.0012883758 -0.00056647277 -0.00031773467 0.00020240992 0.0013592246 0.0058035115 0.013720563 0.025789225 0.040184692 0.057049967 0.070619345 0.080160677 0.082770087][-0.0031464195 -0.0031165329 -0.0026410006 -0.0022101402 -0.001734601 -0.0012173126 -0.00059642293 0.0016863209 0.0058142366 0.012899648 0.022057867 0.033517547 0.042931437 0.049801424 0.051520426][-0.0033884172 -0.0033671553 -0.0032313338 -0.0032316637 -0.0029467605 -0.0026280326 -0.0022012065 -0.0012663382 0.00036351639 0.0038381398 0.0086981989 0.015164781 0.02076124 0.024871396 0.025637429][-0.0033893441 -0.0033876952 -0.0033806483 -0.0033768658 -0.0033451568 -0.0033525394 -0.0032070819 -0.0028861081 -0.0022896733 -0.0010807256 0.00079514529 0.0037220938 0.0064476514 0.0083242031 0.0084946211]]...]
INFO - root - 2017-12-09 18:20:47.767025: step 48410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:12m:57s remains)
INFO - root - 2017-12-09 18:20:56.419613: step 48420, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 70h:45m:49s remains)
INFO - root - 2017-12-09 18:21:04.975088: step 48430, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 70h:11m:38s remains)
INFO - root - 2017-12-09 18:21:13.549321: step 48440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:19m:30s remains)
INFO - root - 2017-12-09 18:21:22.101631: step 48450, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 68h:52m:08s remains)
INFO - root - 2017-12-09 18:21:30.517539: step 48460, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:16m:23s remains)
INFO - root - 2017-12-09 18:21:39.248227: step 48470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:28m:38s remains)
INFO - root - 2017-12-09 18:21:47.816609: step 48480, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:41m:32s remains)
INFO - root - 2017-12-09 18:21:56.299015: step 48490, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 66h:22m:20s remains)
INFO - root - 2017-12-09 18:22:04.874581: step 48500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:22m:49s remains)
2017-12-09 18:22:05.757406: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016602699 0.015238734 0.014436211 0.013949005 0.013188045 0.012269266 0.011405846 0.010481751 0.0093439985 0.00785825 0.0066013276 0.00494209 0.0036659618 0.0026046233 0.0019196356][0.022342751 0.020172909 0.019003715 0.018596156 0.018190676 0.01775906 0.017215664 0.016591204 0.015507551 0.01394794 0.01251913 0.010544095 0.0089301411 0.0075658388 0.0064433548][0.028821314 0.025739845 0.023970364 0.023247553 0.022945262 0.023238529 0.023457587 0.023409987 0.022560928 0.020983137 0.019341826 0.0176387 0.016092364 0.014637474 0.013259448][0.034996197 0.030668387 0.02790417 0.026576171 0.026193539 0.026979472 0.028226772 0.02974174 0.030272277 0.029544739 0.028228102 0.026474861 0.024807872 0.023321582 0.021780036][0.038962264 0.033367906 0.02938786 0.027249806 0.026765361 0.02812629 0.03056349 0.033651706 0.035830766 0.03673565 0.036493838 0.035398263 0.034100529 0.032575417 0.030835116][0.039222509 0.03330766 0.028937789 0.025990682 0.025093429 0.026650539 0.029946247 0.0345564 0.038353678 0.040952958 0.042061504 0.041996222 0.041450657 0.040224034 0.038514521][0.037659891 0.03117227 0.026161825 0.022932442 0.021991037 0.023521304 0.027105033 0.032290246 0.037450969 0.0413158 0.043593127 0.044486787 0.044727862 0.044267159 0.043035012][0.035329226 0.028618129 0.023070468 0.019040052 0.017581737 0.018904926 0.02265374 0.028123895 0.033642627 0.038233265 0.041314691 0.042940862 0.043858722 0.043865267 0.043055438][0.032810889 0.02606939 0.019981707 0.015368449 0.013345521 0.014312701 0.017957784 0.023388347 0.028834367 0.03324943 0.03655453 0.038594745 0.039904896 0.040152065 0.039714493][0.029047357 0.022773808 0.016758854 0.011887776 0.0095533766 0.010236233 0.013594348 0.018623706 0.0234804 0.027281594 0.030191882 0.032071687 0.033403415 0.0337536 0.033488087][0.023491673 0.018160842 0.012905911 0.0083698928 0.0060100914 0.0064511676 0.0092852954 0.013508443 0.017439153 0.020280385 0.022410061 0.023767814 0.024852367 0.025215385 0.025215462][0.01686707 0.012431752 0.008155033 0.00480869 0.0030844305 0.0031706588 0.0051428336 0.0083237384 0.011074908 0.012879137 0.014123833 0.014809949 0.015490279 0.015788166 0.016050175][0.010235878 0.0068282788 0.0036974684 0.0014202674 0.00028361264 0.00047368603 0.0017822594 0.0036085867 0.004958089 0.0058389255 0.006387535 0.0065843691 0.0068704984 0.0070829676 0.007543392][0.0047911289 0.0024618516 0.00043261214 -0.000967829 -0.0016928918 -0.0016734691 -0.001015367 -0.00011462695 0.00053507742 0.00079340767 0.00090582762 0.00092119817 0.00094944006 0.0010762266 0.0014603175][0.00026999484 -0.00089434348 -0.0018825199 -0.0025463705 -0.0028881049 -0.0029065686 -0.0026938717 -0.0024410198 -0.0023022904 -0.0022790767 -0.0022957528 -0.0022943246 -0.0022053733 -0.0020801472 -0.00182006]]...]
INFO - root - 2017-12-09 18:22:14.295112: step 48510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:40m:51s remains)
INFO - root - 2017-12-09 18:22:22.921774: step 48520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:54m:13s remains)
INFO - root - 2017-12-09 18:22:31.416646: step 48530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:22m:31s remains)
INFO - root - 2017-12-09 18:22:40.048491: step 48540, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:28m:24s remains)
INFO - root - 2017-12-09 18:22:48.479748: step 48550, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:10m:36s remains)
INFO - root - 2017-12-09 18:22:56.941057: step 48560, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 68h:23m:03s remains)
INFO - root - 2017-12-09 18:23:05.688938: step 48570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:36m:29s remains)
INFO - root - 2017-12-09 18:23:14.392131: step 48580, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 69h:15m:46s remains)
INFO - root - 2017-12-09 18:23:23.176059: step 48590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:31m:45s remains)
INFO - root - 2017-12-09 18:23:31.837066: step 48600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:32m:59s remains)
2017-12-09 18:23:32.661397: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0095685245 0.0082443338 0.0069353608 0.0058048666 0.00482916 0.0040679108 0.0034734218 0.0029862116 0.0025546767 0.0021688633 0.0018185477 0.0014918402 0.0011263199 0.00059889513 -6.2054954e-05][0.015518857 0.013887869 0.012028272 0.010313825 0.0088075 0.0075932611 0.0066414503 0.0058882274 0.0053021917 0.0048572021 0.0045421282 0.0042562224 0.0039356863 0.0033749437 0.0026066054][0.02032421 0.019149475 0.01741717 0.015517671 0.013679124 0.012060316 0.010752292 0.0097161587 0.0089477375 0.0083459811 0.0078840973 0.007431211 0.0069576139 0.0063140308 0.0055266013][0.022279765 0.021962985 0.020887563 0.019540606 0.018065866 0.016588494 0.015293011 0.014162976 0.013225663 0.0123291 0.011459372 0.010502184 0.0095910793 0.0087060891 0.0078850929][0.021168085 0.021847136 0.02181243 0.021351893 0.020511318 0.019505648 0.018483914 0.017468983 0.016464459 0.015270617 0.013894418 0.012336697 0.010955729 0.0098625077 0.0090769148][0.018046554 0.019651383 0.020651219 0.021144027 0.021070043 0.020508649 0.019687617 0.018777633 0.017677234 0.016170377 0.014313791 0.012241794 0.010511718 0.00934613 0.0087075122][0.014043374 0.016362058 0.018218054 0.019540906 0.020114798 0.020013312 0.019408235 0.018439665 0.017065259 0.015159134 0.012846895 0.010366611 0.0084299706 0.0072352067 0.0067110015][0.00938317 0.012274962 0.014878441 0.016906552 0.018049885 0.018340616 0.017918201 0.016960856 0.015414281 0.013131268 0.010438752 0.0076594735 0.0054723015 0.0042039724 0.0036862448][0.0047010696 0.00760731 0.010576585 0.013165781 0.014892031 0.015646875 0.015494075 0.01456164 0.012872683 0.010357906 0.0074703051 0.0046189791 0.0025076184 0.0012257739 0.00075847772][0.0010108694 0.0032205635 0.0058587212 0.0084563447 0.010479831 0.011620155 0.011762347 0.010955501 0.0092809238 0.0068081254 0.0041142106 0.0015540875 -0.00020673266 -0.0011813587 -0.0015066941][-0.0011750048 0.0001152996 0.0018601369 0.0038084465 0.0055387048 0.0066718291 0.0069357641 0.0063150572 0.0048999526 0.002894656 0.00082173967 -0.0010018528 -0.0021283417 -0.0027002711 -0.0028346654][-0.0024009906 -0.001770811 -0.00085292384 0.00028275582 0.0013880196 0.0021332128 0.0022729763 0.001792371 0.00080300751 -0.00046067382 -0.0016676816 -0.0025922526 -0.0030810339 -0.0032810124 -0.0033159617][-0.003099879 -0.0028229235 -0.0024290113 -0.0019114199 -0.0013634739 -0.00098079047 -0.00093012257 -0.001217267 -0.0017447321 -0.0023442674 -0.0028581207 -0.0031865074 -0.0033342722 -0.0033775552 -0.0033848619][-0.0033708429 -0.0033034408 -0.0031895596 -0.0030281339 -0.0028515724 -0.0027268881 -0.0027114819 -0.0028236143 -0.0030056806 -0.0031839821 -0.0033125216 -0.0033728741 -0.003387582 -0.0033881629 -0.003388647][-0.0034063254 -0.0033965919 -0.0033832537 -0.0033675311 -0.0033510069 -0.0033395255 -0.003340177 -0.0033553408 -0.0033747931 -0.003387328 -0.0033922547 -0.0033933453 -0.0033928636 -0.003393007 -0.0033938352]]...]
INFO - root - 2017-12-09 18:23:41.512447: step 48610, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:49m:05s remains)
INFO - root - 2017-12-09 18:23:50.183558: step 48620, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 67h:46m:05s remains)
INFO - root - 2017-12-09 18:23:58.698255: step 48630, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 68h:05m:31s remains)
INFO - root - 2017-12-09 18:24:07.315933: step 48640, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 68h:53m:42s remains)
INFO - root - 2017-12-09 18:24:15.718920: step 48650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:04m:21s remains)
INFO - root - 2017-12-09 18:24:24.145526: step 48660, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:50m:03s remains)
INFO - root - 2017-12-09 18:24:32.721776: step 48670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:14m:54s remains)
INFO - root - 2017-12-09 18:24:41.285182: step 48680, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:24m:48s remains)
INFO - root - 2017-12-09 18:24:49.996029: step 48690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:21m:23s remains)
INFO - root - 2017-12-09 18:24:58.592780: step 48700, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 65h:18m:42s remains)
2017-12-09 18:24:59.471949: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28030553 0.26556572 0.24925296 0.23303869 0.21703888 0.20101966 0.18405803 0.16670011 0.14938483 0.13267219 0.11895751 0.10867552 0.10090912 0.094573088 0.086649187][0.29969108 0.28706759 0.27270928 0.25834036 0.24317972 0.22744398 0.20918831 0.18911284 0.16804895 0.14735873 0.12956642 0.11504061 0.10361879 0.094712064 0.084786236][0.315026 0.30487621 0.29237789 0.27925244 0.26470581 0.24939547 0.23105574 0.21022083 0.18718861 0.16378951 0.14230111 0.12331051 0.10731542 0.094067052 0.080916442][0.32567737 0.31888726 0.30854419 0.29655179 0.28256512 0.26728186 0.2490098 0.22859223 0.20576939 0.18173315 0.15797573 0.13469742 0.11324564 0.094267674 0.076347448][0.32806331 0.32418486 0.31620988 0.30601922 0.29354051 0.27932802 0.26230845 0.24336371 0.22161414 0.19768573 0.17249613 0.14572759 0.11908955 0.094042167 0.070939168][0.3247284 0.32251427 0.31579351 0.30733168 0.29691094 0.28473449 0.27047235 0.25427678 0.23465952 0.21182658 0.18539456 0.15539561 0.12384429 0.093127623 0.065303616][0.31449547 0.31395319 0.30835953 0.30079725 0.29199636 0.28251007 0.27168146 0.25940478 0.24353848 0.22278795 0.19634114 0.16375971 0.12792446 0.0925242 0.061089285][0.29807684 0.29870355 0.2937189 0.28709418 0.28015411 0.27293333 0.26504663 0.25663695 0.2445204 0.22672339 0.2015923 0.16818298 0.13022609 0.092211828 0.058747083][0.27606255 0.27923033 0.2766217 0.27232841 0.26771221 0.26249444 0.2568177 0.25075403 0.24120143 0.22584839 0.202381 0.1694449 0.13098775 0.092087336 0.057902515][0.25112259 0.25746819 0.2584888 0.25761664 0.25608489 0.25344351 0.25033095 0.24593009 0.23788767 0.224065 0.20188646 0.17001298 0.13206986 0.093921043 0.060357757][0.22371458 0.23417588 0.23992567 0.242874 0.24502021 0.24536878 0.24445432 0.24087186 0.23398037 0.22157447 0.20086139 0.17091013 0.13495855 0.099141039 0.067102022][0.19453868 0.20855953 0.218745 0.225647 0.2314932 0.23396894 0.23481117 0.23255005 0.22690637 0.21603422 0.19791415 0.17182048 0.14007279 0.10835734 0.079446651][0.16577256 0.18227737 0.19559315 0.2057948 0.21481201 0.21926436 0.22114302 0.21936233 0.21454869 0.20559445 0.19087991 0.17016193 0.14500128 0.11919094 0.095133856][0.1371858 0.15549386 0.17146079 0.18416972 0.19509891 0.20059021 0.2034426 0.20191836 0.19769822 0.19031896 0.17899527 0.16451499 0.14671762 0.12847418 0.11115748][0.10785857 0.12561905 0.14147361 0.15484264 0.16675955 0.17350997 0.17742419 0.1769398 0.17402302 0.16895111 0.16156019 0.15300003 0.14288402 0.13322225 0.12393745]]...]
INFO - root - 2017-12-09 18:25:07.968495: step 48710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:59m:02s remains)
INFO - root - 2017-12-09 18:25:16.632236: step 48720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:37m:11s remains)
INFO - root - 2017-12-09 18:25:25.160386: step 48730, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 66h:16m:19s remains)
INFO - root - 2017-12-09 18:25:33.837050: step 48740, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:02m:32s remains)
INFO - root - 2017-12-09 18:25:42.561979: step 48750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:21m:22s remains)
INFO - root - 2017-12-09 18:25:51.145095: step 48760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:06m:49s remains)
INFO - root - 2017-12-09 18:25:59.816701: step 48770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:01m:20s remains)
INFO - root - 2017-12-09 18:26:08.392606: step 48780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:30m:22s remains)
INFO - root - 2017-12-09 18:26:17.004026: step 48790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:16m:04s remains)
INFO - root - 2017-12-09 18:26:25.494339: step 48800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:07m:38s remains)
2017-12-09 18:26:26.373148: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00067475694 -0.00024273177 -0.0016157327 -0.0027039186 -0.0032338123 -0.0033771612 -0.0033759351 -0.0033771028 -0.003377358 -0.0033776606 -0.0033782318 -0.0033788125 -0.0033787347 -0.0033789615 -0.0033785254][0.0028594283 0.0018449596 -0.00031391275 -0.0021340058 -0.0030815287 -0.0033536318 -0.0033517489 -0.0033753524 -0.0033752625 -0.0033751633 -0.0033754387 -0.0033758921 -0.0033763954 -0.0033769314 -0.0033767691][0.0058421036 0.0042955745 0.00123909 -0.0014283091 -0.002917513 -0.0033575473 -0.003355914 -0.0033760786 -0.0033753586 -0.0033751575 -0.0033746434 -0.0033748015 -0.0033751251 -0.0033757903 -0.0033758408][0.0082812076 0.0062468424 0.002459463 -0.00087322621 -0.0027749329 -0.0033604859 -0.0033650792 -0.0033778371 -0.0033766164 -0.0033759361 -0.0033753572 -0.0033748357 -0.0033747694 -0.0033748872 -0.0033751642][0.009861215 0.0074789803 0.0031825842 -0.0005718919 -0.002714009 -0.0033594603 -0.0033670571 -0.0033786632 -0.0033779123 -0.0033772192 -0.0033762448 -0.0033751705 -0.0033747063 -0.003374008 -0.0033741726][0.010618348 0.007896306 0.0033697111 -0.00045257853 -0.0026237776 -0.003324457 -0.0033705533 -0.0033778676 -0.0033782108 -0.003378066 -0.0033771673 -0.0033761305 -0.0033759559 -0.0033746317 -0.0033747051][0.010184781 0.0072515151 0.0028057944 -0.00068940851 -0.0026640943 -0.0033117638 -0.0033731817 -0.0033759607 -0.0033763172 -0.0033775482 -0.003376744 -0.0033757344 -0.0033757477 -0.0033750113 -0.0033749021][0.0094672237 0.006554706 0.0021107092 -0.00113184 -0.0028265361 -0.0032945527 -0.0033236737 -0.0033712373 -0.0033758096 -0.0033767431 -0.0033766483 -0.0033760658 -0.0033755195 -0.0033752532 -0.0033752527][0.0087353019 0.006168819 0.0019752339 -0.0009502389 -0.0026169657 -0.0031974898 -0.0033212223 -0.0033718757 -0.0033753528 -0.0033763426 -0.003376714 -0.0033763249 -0.0033757775 -0.0033755689 -0.0033754702][0.00801482 0.0059526721 0.0019354059 -0.0007050659 -0.0022256053 -0.0028540255 -0.0031572729 -0.0033523648 -0.0033756217 -0.0033758301 -0.0033761233 -0.00337612 -0.0033761545 -0.0033760099 -0.0033756192][0.007813965 0.0064620487 0.0028420466 0.00025695749 -0.0013934406 -0.0023130383 -0.0028956649 -0.0032641436 -0.0033606633 -0.003376588 -0.0033758045 -0.003375516 -0.0033752886 -0.0033758411 -0.0033757533][0.0080943834 0.0075090844 0.004527349 0.0021894041 0.0003520383 -0.0011534994 -0.002370677 -0.0031123478 -0.0033555208 -0.0033753768 -0.0033749093 -0.0033747414 -0.0033740832 -0.0033748131 -0.0033747146][0.0085945316 0.0088945972 0.0066397004 0.0046918141 0.0027621191 0.00059304968 -0.0014700708 -0.0028175646 -0.0033337509 -0.0033738764 -0.0033739307 -0.0033736611 -0.0033725922 -0.0033734618 -0.0033734164][0.0096563529 0.010449907 0.0088420743 0.0073091853 0.0052785715 0.0025959967 -0.00034029176 -0.0023691491 -0.0032480715 -0.0033712857 -0.0033718515 -0.0033717423 -0.0033714045 -0.00337243 -0.0033724997][0.01075565 0.01191646 0.010916618 0.0097322911 0.0076752268 0.0045422483 0.00086579542 -0.0018309786 -0.003109613 -0.0033694855 -0.0033708608 -0.0033700478 -0.0033693244 -0.0033704636 -0.0033712876]]...]
INFO - root - 2017-12-09 18:26:34.967143: step 48810, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:14m:17s remains)
INFO - root - 2017-12-09 18:26:43.547799: step 48820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:55m:23s remains)
INFO - root - 2017-12-09 18:26:52.130848: step 48830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:48m:32s remains)
INFO - root - 2017-12-09 18:27:00.803350: step 48840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:29m:15s remains)
INFO - root - 2017-12-09 18:27:09.451135: step 48850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:05m:43s remains)
INFO - root - 2017-12-09 18:27:17.935331: step 48860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 67h:08m:20s remains)
INFO - root - 2017-12-09 18:27:26.714935: step 48870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:56m:12s remains)
INFO - root - 2017-12-09 18:27:35.415814: step 48880, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 68h:29m:40s remains)
INFO - root - 2017-12-09 18:27:44.122102: step 48890, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:31m:04s remains)
INFO - root - 2017-12-09 18:27:52.862805: step 48900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:09m:34s remains)
2017-12-09 18:27:53.711476: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067707911 0.083090946 0.094443366 0.1008855 0.103108 0.1025558 0.10116464 0.10015982 0.10038654 0.10333078 0.10693703 0.1129297 0.11831085 0.12371986 0.12717098][0.067224823 0.082556181 0.093714215 0.099902153 0.1014781 0.10050575 0.098647214 0.0973259 0.09740866 0.10026603 0.10497288 0.11180754 0.11907393 0.12625764 0.13133857][0.057208177 0.07080625 0.080743954 0.086173624 0.087638564 0.086426847 0.084254257 0.0829563 0.083217151 0.086356722 0.09127523 0.098737605 0.10686643 0.11547751 0.12257589][0.043800425 0.05537045 0.064114146 0.068862304 0.069802634 0.068063028 0.0653991 0.063470133 0.063572317 0.06684199 0.07222072 0.0803576 0.08920233 0.098662876 0.10664932][0.030378431 0.039550222 0.046790659 0.050747149 0.051213175 0.048808996 0.045446377 0.042852145 0.042339325 0.044999551 0.050288349 0.058389489 0.067103356 0.076473325 0.084715433][0.019243093 0.025908887 0.031339094 0.034263186 0.034141537 0.031379368 0.0274834 0.023963584 0.022580516 0.024103306 0.02838885 0.035063595 0.042557657 0.050808065 0.058239132][0.010214032 0.014656077 0.018235123 0.01998839 0.019441091 0.016864665 0.013279603 0.0098737478 0.0080900956 0.008447228 0.011023565 0.015299187 0.020495996 0.026504217 0.032410104][0.0036975977 0.00646451 0.0086258436 0.0095729837 0.0089353379 0.00699816 0.0043986756 0.0018247545 0.00023529399 -2.8686132e-05 0.00096109859 0.0028354779 0.00544036 0.0088385921 0.012706083][-0.00057698879 0.00089977472 0.0020010548 0.0024484966 0.002067365 0.0010615985 -0.00029676827 -0.0016641468 -0.0025578602 -0.0028862557 -0.0027482468 -0.0023189648 -0.001530482 -0.00024030451 0.0015318606][-0.0025246562 -0.0019043686 -0.0014223063 -0.0012281442 -0.001372511 -0.0017631039 -0.0023084753 -0.0028524534 -0.003206165 -0.0033542633 -0.003374344 -0.0033516402 -0.0032436061 -0.0029799361 -0.0025502876][-0.0032325031 -0.003058129 -0.0029013334 -0.0028250513 -0.0028348172 -0.0029133086 -0.003051399 -0.0032091544 -0.0033191962 -0.0033630612 -0.0033700194 -0.0033701141 -0.0033673232 -0.0033585702 -0.003337367][-0.0033741831 -0.0033377132 -0.0032903317 -0.0032582576 -0.0032461346 -0.0032515721 -0.0032756415 -0.0033147011 -0.0033509317 -0.0033682212 -0.0033738592 -0.0033736529 -0.0033712224 -0.0033683544 -0.0033659278][-0.0033888882 -0.0033764506 -0.0033595657 -0.003345442 -0.0033359041 -0.0033318796 -0.0033368452 -0.0033500206 -0.0033639278 -0.00337206 -0.0033771398 -0.003378547 -0.0033758581 -0.0033715914 -0.0033670829][-0.0033936782 -0.0033914442 -0.0033877189 -0.003383358 -0.0033791859 -0.0033734967 -0.0033727207 -0.0033746974 -0.003378914 -0.0033833464 -0.003385975 -0.0033855601 -0.00337877 -0.0033692152 -0.0033599022][-0.0033938223 -0.0033921942 -0.0033912854 -0.003388416 -0.0033860204 -0.0033816446 -0.0033801564 -0.0033799589 -0.0033804041 -0.0033825517 -0.0033833438 -0.0033811438 -0.0033740329 -0.0033641781 -0.003354576]]...]
INFO - root - 2017-12-09 18:28:02.309145: step 48910, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:22m:20s remains)
INFO - root - 2017-12-09 18:28:10.947318: step 48920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:37m:46s remains)
INFO - root - 2017-12-09 18:28:19.487027: step 48930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 65h:51m:12s remains)
INFO - root - 2017-12-09 18:28:27.947174: step 48940, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 64h:53m:27s remains)
INFO - root - 2017-12-09 18:28:36.385226: step 48950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:08m:08s remains)
INFO - root - 2017-12-09 18:28:44.812579: step 48960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:52m:32s remains)
INFO - root - 2017-12-09 18:28:53.493269: step 48970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:32m:47s remains)
INFO - root - 2017-12-09 18:29:02.112046: step 48980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:25m:29s remains)
INFO - root - 2017-12-09 18:29:10.630424: step 48990, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 66h:07m:53s remains)
INFO - root - 2017-12-09 18:29:19.176843: step 49000, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 65h:20m:06s remains)
2017-12-09 18:29:20.102460: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11037757 0.10822865 0.10227014 0.096975289 0.090034135 0.081536487 0.07079035 0.058773611 0.044959817 0.031187676 0.018985242 0.010835364 0.0064005097 0.004608592 0.003455935][0.11867505 0.11421685 0.10619379 0.10006966 0.0932373 0.08538492 0.073960178 0.05985393 0.042993918 0.026699938 0.013242667 0.00504613 0.0011393265 -0.00023771636 -0.00086074811][0.12197448 0.11595076 0.10689289 0.10043868 0.09395311 0.08711455 0.076266639 0.061913136 0.043560144 0.025615633 0.010852431 0.0022611986 -0.0015798834 -0.0026824444 -0.0029643858][0.12272257 0.11527032 0.10596851 0.10000067 0.094825439 0.089618184 0.079956993 0.066130877 0.047237195 0.028346214 0.012338144 0.0027640709 -0.0017660978 -0.003059658 -0.0032971972][0.11874962 0.11080966 0.10182328 0.096935973 0.093790777 0.0911231 0.083934791 0.071631968 0.05298686 0.033385757 0.015901878 0.0048876693 -0.0008838193 -0.0028339804 -0.0032637271][0.10963098 0.10208517 0.093686886 0.090463534 0.089919716 0.090226464 0.086038664 0.076061055 0.058715433 0.039020758 0.020288665 0.0075734146 0.00025524176 -0.0025300158 -0.0032036132][0.0958394 0.089029491 0.081609823 0.080311462 0.082604587 0.086075723 0.085013121 0.077582866 0.062070075 0.042855065 0.023615427 0.0097494246 0.0012616753 -0.0022335663 -0.0031388854][0.078515522 0.072498374 0.066342272 0.067099825 0.07219737 0.078422673 0.080007859 0.074988253 0.061581362 0.04363234 0.024907518 0.010723689 0.0018037972 -0.0020533921 -0.0030830745][0.060257491 0.054729149 0.049782325 0.052262619 0.05957197 0.06780652 0.0712919 0.067976072 0.056516293 0.040482026 0.023353621 0.010027727 0.0016019414 -0.0021085909 -0.0030546663][0.043234855 0.038165502 0.034353446 0.038027547 0.046330534 0.055243455 0.059425991 0.057185989 0.047723617 0.034094974 0.019319735 0.0077728769 0.00059581036 -0.0024238145 -0.0030708513][0.028516866 0.02485086 0.022227224 0.026043016 0.033990066 0.042169344 0.045946717 0.044272065 0.036769286 0.025938861 0.014111669 0.0048760306 -0.00064597488 -0.0027569532 -0.0031490391][0.016683389 0.014226018 0.012916875 0.016576555 0.023400765 0.029975083 0.032887977 0.031583387 0.025985591 0.017962217 0.009151889 0.0023308024 -0.0015976012 -0.0030000829 -0.0032294248][0.00835159 0.0065736817 0.0060443515 0.0090355324 0.014277273 0.019013267 0.021025881 0.020018853 0.016107891 0.010769483 0.00489063 0.00032545137 -0.0022225715 -0.0031139655 -0.0032547009][0.0027943631 0.0015964564 0.0016030041 0.0037039313 0.0071401279 0.010108692 0.011310397 0.010546573 0.0079987552 0.0046934774 0.0011962629 -0.0014091614 -0.0028148759 -0.0032347608 -0.0032310104][-3.8401922e-05 -0.000762057 -0.00063195638 0.000579976 0.0025090538 0.0041771517 0.004820439 0.004217268 0.002646558 0.00082934136 -0.00087828049 -0.0020936369 -0.0027425317 -0.0029692326 -0.0030184402]]...]
INFO - root - 2017-12-09 18:29:28.438135: step 49010, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 66h:46m:14s remains)
INFO - root - 2017-12-09 18:29:36.978230: step 49020, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:36m:11s remains)
INFO - root - 2017-12-09 18:29:45.534140: step 49030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:11m:08s remains)
INFO - root - 2017-12-09 18:29:54.017274: step 49040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:13m:47s remains)
INFO - root - 2017-12-09 18:30:02.582979: step 49050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:35m:42s remains)
INFO - root - 2017-12-09 18:30:11.145116: step 49060, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 67h:00m:44s remains)
INFO - root - 2017-12-09 18:30:19.822641: step 49070, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 70h:39m:04s remains)
INFO - root - 2017-12-09 18:30:28.489758: step 49080, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:33m:55s remains)
INFO - root - 2017-12-09 18:30:37.270944: step 49090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:35m:03s remains)
INFO - root - 2017-12-09 18:30:45.852032: step 49100, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:52m:44s remains)
2017-12-09 18:30:46.689131: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.043237582 0.042790316 0.039823964 0.035473991 0.030720429 0.026461441 0.023058495 0.02075145 0.019130625 0.017705018 0.015981317 0.01356022 0.010374518 0.0067486754 0.0032353031][0.039015319 0.037925098 0.034446932 0.029644562 0.024565956 0.020125315 0.01682662 0.014844454 0.013728422 0.012794162 0.011426158 0.009253541 0.00648511 0.0034942494 0.00082847034][0.034205493 0.032680649 0.028798578 0.023734028 0.018655835 0.014469313 0.011618577 0.010032056 0.009216777 0.0084018894 0.0071818493 0.0053440211 0.0031835746 0.0009552727 -0.00091796578][0.029976463 0.02948845 0.026676701 0.022523738 0.018189134 0.014568017 0.012243341 0.011126605 0.010600463 0.0097896671 0.0082960147 0.006002699 0.0034145673 0.00094439345 -0.0010122948][0.029601796 0.030363603 0.029045505 0.026576577 0.023858014 0.021729164 0.020628147 0.020337567 0.020157028 0.019183315 0.017022403 0.013556979 0.0093899183 0.0052321786 0.0017568229][0.033594508 0.03534884 0.03491164 0.033410843 0.031800359 0.030886723 0.030999729 0.031714659 0.032109134 0.031093614 0.028320277 0.023817703 0.018176991 0.012226363 0.0067829369][0.03784911 0.040042132 0.039781544 0.038312946 0.036633648 0.035781503 0.036135007 0.037264559 0.038118422 0.037493579 0.034919851 0.030272001 0.024084158 0.017166592 0.010521863][0.040013194 0.042822041 0.042916413 0.04152992 0.039653502 0.038406223 0.03823939 0.0389007 0.039490607 0.038920071 0.036624834 0.032230791 0.026055679 0.018828362 0.011657191][0.038963437 0.042568576 0.04347603 0.042686466 0.041048542 0.039531291 0.038624134 0.038427081 0.038229235 0.037264213 0.035027742 0.0310285 0.02526667 0.018323129 0.01127869][0.034334566 0.038349222 0.040164441 0.040290661 0.039325852 0.037862282 0.036447268 0.035442423 0.034430124 0.033027742 0.030640693 0.026895657 0.021652497 0.015393765 0.0090985028][0.026349984 0.029912384 0.031904176 0.032599632 0.032360207 0.0314837 0.030329326 0.029104272 0.027699083 0.025961732 0.023486897 0.020073028 0.015609884 0.010551987 0.0056151636][0.016484508 0.019138092 0.02071193 0.021372242 0.021363884 0.020895809 0.020166557 0.019260319 0.018111832 0.016617801 0.014562477 0.011876117 0.0085993512 0.00511544 0.0018918815][0.0071111796 0.0086946553 0.0096591441 0.010062363 0.010058878 0.009800883 0.009410182 0.0089105517 0.0082421135 0.0073314039 0.0060638208 0.0044328654 0.0025279808 0.00061283889 -0.0010396368][0.00076784496 0.0014686186 0.0018999714 0.0020842806 0.0020901922 0.0019893039 0.001838912 0.0016419154 0.0013645899 0.00095922058 0.00038399477 -0.00034511578 -0.0011703691 -0.0019613551 -0.0025999555][-0.0021951031 -0.0019634524 -0.0018196678 -0.0017590601 -0.0017591921 -0.0017942655 -0.0018405878 -0.0018962526 -0.0019754139 -0.00209816 -0.0022808798 -0.0025136776 -0.0027730775 -0.0030105021 -0.0031886236]]...]
INFO - root - 2017-12-09 18:30:55.345723: step 49110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:57m:50s remains)
INFO - root - 2017-12-09 18:31:04.238996: step 49120, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 71h:12m:57s remains)
INFO - root - 2017-12-09 18:31:12.818935: step 49130, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:02m:18s remains)
INFO - root - 2017-12-09 18:31:21.193857: step 49140, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 64h:44m:00s remains)
INFO - root - 2017-12-09 18:31:29.544187: step 49150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:58m:11s remains)
INFO - root - 2017-12-09 18:31:38.306631: step 49160, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 58h:44m:43s remains)
INFO - root - 2017-12-09 18:31:46.902397: step 49170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:50m:17s remains)
INFO - root - 2017-12-09 18:31:55.419043: step 49180, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 65h:22m:42s remains)
INFO - root - 2017-12-09 18:32:03.923062: step 49190, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 66h:22m:13s remains)
INFO - root - 2017-12-09 18:32:12.553134: step 49200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:40m:15s remains)
2017-12-09 18:32:13.551719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033650729 -0.0033622836 -0.0033614032 -0.0033608892 -0.0033604698 -0.0033603397 -0.0033603881 -0.0033603418 -0.0033603755 -0.0033602514 -0.0033603467 -0.0033606109 -0.0033609485 -0.0033612605 -0.0033615327][-0.003362661 -0.0033596521 -0.0033588817 -0.0033584551 -0.0033579608 -0.0033576342 -0.0033573655 -0.0033569748 -0.0033567171 -0.0033565147 -0.0033567296 -0.0033571844 -0.0033577413 -0.0033583003 -0.0033589106][-0.0033622973 -0.0033592505 -0.0033585727 -0.0033581364 -0.0033576277 -0.0033570973 -0.003356386 -0.003355657 -0.0033553068 -0.0033548148 -0.0033546593 -0.0033549487 -0.003355582 -0.003356444 -0.0033573932][-0.0033616631 -0.003358739 -0.0033579729 -0.003357409 -0.0033571171 -0.0033565029 -0.003355454 -0.0033545634 -0.0033541555 -0.0033534756 -0.0033529587 -0.0033530944 -0.0033538321 -0.0033549916 -0.0033562477][-0.0033611341 -0.0033582635 -0.0033577869 -0.0033572842 -0.0033570419 -0.0033564987 -0.0033553971 -0.0033543741 -0.0033537711 -0.0033529322 -0.0033522088 -0.0033521834 -0.0033529548 -0.0033543336 -0.0033557888][-0.0033615229 -0.0033586819 -0.0033583851 -0.0033578286 -0.0033573888 -0.003356711 -0.0033556798 -0.0033546055 -0.0033538244 -0.0033529231 -0.003352263 -0.0033521517 -0.0033528972 -0.003354297 -0.0033558127][-0.0033621481 -0.0033591888 -0.0033589166 -0.0033581008 -0.0033572586 -0.0033563569 -0.0033553836 -0.0033544381 -0.0033536181 -0.0033528395 -0.0033524041 -0.003352389 -0.0033532307 -0.0033546665 -0.0033561753][-0.0033626412 -0.0033594363 -0.0033589143 -0.0033578428 -0.003356637 -0.0033554425 -0.0033544761 -0.0033538102 -0.0033532064 -0.003352758 -0.0033527277 -0.00335306 -0.0033540505 -0.0033554723 -0.0033568721][-0.003362481 -0.0033591317 -0.0033583432 -0.0033570773 -0.003355725 -0.0033545953 -0.0033539627 -0.0033536644 -0.0033532986 -0.0033532397 -0.0033536446 -0.0033542311 -0.0033552246 -0.0033565522 -0.0033577937][-0.003361824 -0.0033583529 -0.0033577222 -0.0033567781 -0.0033555739 -0.0033545692 -0.003354029 -0.0033537736 -0.0033534684 -0.0033535264 -0.0033540095 -0.0033546812 -0.003355724 -0.003356989 -0.0033581189][-0.0033611334 -0.0033577087 -0.0033572849 -0.0033565408 -0.0033555531 -0.0033546404 -0.0033540223 -0.0033536863 -0.003353439 -0.0033535331 -0.0033540016 -0.0033547634 -0.0033558437 -0.0033570367 -0.0033580188][-0.0033604952 -0.00335726 -0.0033570102 -0.0033564733 -0.003355741 -0.0033551124 -0.0033546758 -0.003354358 -0.003354074 -0.0033541971 -0.003354637 -0.0033552898 -0.003356175 -0.0033571515 -0.0033579895][-0.0033603965 -0.0033572582 -0.0033572055 -0.0033570146 -0.0033567066 -0.0033564591 -0.0033562118 -0.0033559103 -0.0033556076 -0.0033556174 -0.0033557911 -0.0033560384 -0.0033565394 -0.0033572996 -0.0033579906][-0.0033607995 -0.0033576665 -0.0033578393 -0.0033579271 -0.0033579487 -0.0033578561 -0.0033576291 -0.0033573085 -0.0033570016 -0.0033568463 -0.0033567655 -0.0033567543 -0.0033570332 -0.0033575939 -0.003358125][-0.0033615322 -0.0033582977 -0.0033585867 -0.0033587348 -0.0033587974 -0.0033587466 -0.0033585457 -0.0033582693 -0.0033580197 -0.0033577997 -0.0033576388 -0.00335757 -0.0033577564 -0.0033581061 -0.0033584505]]...]
INFO - root - 2017-12-09 18:32:22.035504: step 49210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:04m:48s remains)
INFO - root - 2017-12-09 18:32:30.621191: step 49220, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 69h:33m:51s remains)
INFO - root - 2017-12-09 18:32:39.122257: step 49230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:14m:49s remains)
INFO - root - 2017-12-09 18:32:47.759613: step 49240, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 68h:37m:18s remains)
INFO - root - 2017-12-09 18:32:56.299448: step 49250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 68h:21m:16s remains)
INFO - root - 2017-12-09 18:33:04.990371: step 49260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:47m:33s remains)
INFO - root - 2017-12-09 18:33:13.576025: step 49270, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:24m:01s remains)
INFO - root - 2017-12-09 18:33:22.224319: step 49280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:20m:59s remains)
INFO - root - 2017-12-09 18:33:30.880847: step 49290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:37m:45s remains)
INFO - root - 2017-12-09 18:33:39.488269: step 49300, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 69h:21m:54s remains)
2017-12-09 18:33:40.392500: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18890253 0.18916568 0.1890036 0.18818061 0.18867081 0.19374384 0.20464167 0.22415479 0.24982794 0.28375676 0.31943503 0.35432783 0.38563752 0.4105857 0.42727253][0.20020346 0.20723806 0.21514986 0.22353961 0.23245972 0.2432531 0.25726104 0.27721733 0.30069798 0.32907274 0.35830817 0.38498327 0.40793517 0.42465636 0.43607551][0.19748148 0.21284521 0.23123865 0.2504563 0.27001095 0.28897393 0.30746093 0.32634228 0.34541771 0.366446 0.38539088 0.40083453 0.41278002 0.41979817 0.42375007][0.18476249 0.20634639 0.2335383 0.2644119 0.29536164 0.32273486 0.34678924 0.36683887 0.38246852 0.39510593 0.4042179 0.40861821 0.40894502 0.40609491 0.40294537][0.16097596 0.18784708 0.22260985 0.2625944 0.3032127 0.339572 0.36926544 0.39054048 0.40324041 0.40942043 0.40930739 0.40244526 0.39277074 0.38113958 0.37173524][0.13213174 0.16020904 0.19818319 0.24542263 0.29421681 0.33784014 0.37234461 0.39445 0.4050191 0.40416566 0.39509714 0.37891889 0.36085838 0.34224084 0.32770345][0.10347241 0.12901291 0.16588128 0.21468754 0.26767033 0.31631041 0.35434821 0.37762472 0.38571495 0.37890649 0.3619163 0.33718359 0.311789 0.28750554 0.26947773][0.076258861 0.097150013 0.12971278 0.17585902 0.22773874 0.2774837 0.31703112 0.34082606 0.3474102 0.33662945 0.313932 0.28258678 0.25081825 0.22188911 0.20071913][0.053436391 0.068422981 0.094693542 0.1346859 0.18141931 0.22758155 0.26477033 0.28743932 0.29263356 0.27986151 0.25447422 0.22046559 0.18637303 0.15565419 0.1328813][0.036200516 0.044909332 0.063429549 0.094374068 0.13220598 0.17079261 0.20234434 0.22199622 0.22577783 0.21315603 0.18883759 0.15688379 0.12543899 0.097654834 0.077240378][0.02238559 0.026517082 0.038043611 0.058982089 0.085727535 0.11392689 0.13744709 0.15241145 0.15492971 0.14470868 0.12483658 0.099542968 0.074999526 0.053703718 0.038369585][0.011735831 0.013156737 0.019463036 0.032042541 0.048615232 0.066508465 0.081821166 0.091662765 0.092983656 0.085766874 0.071853496 0.054701127 0.038250979 0.024479169 0.014919086][0.0041063773 0.0042437236 0.0070163636 0.013302418 0.022005547 0.031677235 0.040284596 0.045969766 0.046775736 0.042551968 0.034305625 0.024418138 0.015052837 0.0075761704 0.002622101][-8.493429e-05 -0.00093541993 -0.00024815346 0.0022909511 0.00590147 0.0099113369 0.013507792 0.015949868 0.016336359 0.01451462 0.010918796 0.0066807531 0.0027679177 -0.00018054387 -0.0019722697][-0.0015713356 -0.0027388288 -0.0030116716 -0.0024316013 -0.0014904529 -0.00036595413 0.00069338363 0.0014242036 0.0015383251 0.0010460201 8.5181091e-05 -0.0010225228 -0.0020094796 -0.0026973477 -0.0030499233]]...]
INFO - root - 2017-12-09 18:33:48.982251: step 49310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:26m:26s remains)
INFO - root - 2017-12-09 18:33:57.653504: step 49320, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 67h:15m:27s remains)
INFO - root - 2017-12-09 18:34:06.288107: step 49330, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:15m:32s remains)
INFO - root - 2017-12-09 18:34:14.944208: step 49340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:19m:30s remains)
INFO - root - 2017-12-09 18:34:23.417167: step 49350, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 64h:44m:47s remains)
INFO - root - 2017-12-09 18:34:31.813956: step 49360, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:42m:15s remains)
INFO - root - 2017-12-09 18:34:40.353806: step 49370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:10m:00s remains)
INFO - root - 2017-12-09 18:34:48.967115: step 49380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:47m:59s remains)
INFO - root - 2017-12-09 18:34:57.544288: step 49390, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 64h:50m:42s remains)
INFO - root - 2017-12-09 18:35:06.229948: step 49400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:19m:18s remains)
2017-12-09 18:35:07.151262: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34400809 0.341473 0.34235945 0.34448195 0.34634858 0.3474063 0.34532529 0.33927479 0.32662666 0.31111726 0.29191777 0.27036163 0.24997751 0.23196121 0.21875685][0.38273183 0.38417313 0.38817796 0.39322668 0.39705482 0.39778829 0.3944791 0.38650411 0.37097365 0.35161436 0.3288047 0.3050397 0.28186378 0.26107779 0.24576402][0.41198587 0.41871843 0.42670724 0.43500894 0.44091782 0.44229263 0.43765566 0.4272427 0.40928662 0.38675162 0.361014 0.33452109 0.30819991 0.28425258 0.2663987][0.43167233 0.44368562 0.45622966 0.4679727 0.47601697 0.47836289 0.47352386 0.46163493 0.44197288 0.4172478 0.38999304 0.36104727 0.33184847 0.30462191 0.2839618][0.44333935 0.4604722 0.47654769 0.49157602 0.50195867 0.50498146 0.50023478 0.48744357 0.46722916 0.44189131 0.41419011 0.384131 0.35342044 0.32409027 0.30132833][0.4463869 0.46768162 0.48635069 0.50400031 0.51618618 0.52031583 0.51631963 0.50359648 0.48301518 0.45754203 0.4296326 0.39968252 0.36848283 0.3384786 0.31440887][0.44373319 0.46681362 0.48546636 0.50359219 0.51612574 0.52069134 0.51701391 0.50496006 0.48549667 0.46101552 0.43392742 0.40493095 0.37395957 0.34456044 0.32051951][0.43240762 0.45613977 0.47348702 0.49011135 0.50155252 0.50585681 0.502539 0.49162361 0.47355935 0.45101145 0.42626694 0.3996346 0.37100026 0.34343144 0.32052511][0.4120234 0.43376374 0.4477635 0.46126953 0.4706009 0.47363877 0.47063196 0.46182296 0.4465681 0.42717481 0.4053573 0.382043 0.35670027 0.33185896 0.31114125][0.37909532 0.39886537 0.40996951 0.42010725 0.42761409 0.42994645 0.4274717 0.42004263 0.40708697 0.39118594 0.37235051 0.35267404 0.33094192 0.30989283 0.29271671][0.33691892 0.35323152 0.36115721 0.36831662 0.37375256 0.37553677 0.37362105 0.3679578 0.35763913 0.34476283 0.32897556 0.3126609 0.29461935 0.27737784 0.26379713][0.28800723 0.30105281 0.30621877 0.31040287 0.3138589 0.31476957 0.31337586 0.30969995 0.30221298 0.29247463 0.28026107 0.26722348 0.25272024 0.23855624 0.2286353][0.23712891 0.24754007 0.2509363 0.25329781 0.25534934 0.255537 0.2546286 0.25240225 0.24733321 0.24041697 0.23159507 0.22157145 0.21047193 0.1994202 0.19244884][0.18944103 0.19733289 0.19947547 0.20075312 0.20193954 0.20190582 0.20142505 0.20010911 0.19684498 0.19234575 0.18604119 0.17849094 0.17015964 0.16201371 0.15780219][0.147284 0.15366583 0.15532197 0.15629004 0.15721402 0.15736496 0.15716165 0.15642056 0.15419839 0.15126011 0.1472555 0.14228556 0.1367344 0.13120702 0.12944749]]...]
INFO - root - 2017-12-09 18:35:15.748376: step 49410, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 66h:54m:44s remains)
INFO - root - 2017-12-09 18:35:24.228131: step 49420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:02m:42s remains)
INFO - root - 2017-12-09 18:35:32.882151: step 49430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:43m:22s remains)
INFO - root - 2017-12-09 18:35:41.366517: step 49440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:10m:34s remains)
INFO - root - 2017-12-09 18:35:49.985069: step 49450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:06m:56s remains)
INFO - root - 2017-12-09 18:35:58.694182: step 49460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:51m:32s remains)
INFO - root - 2017-12-09 18:36:06.818528: step 49470, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 70h:42m:12s remains)
INFO - root - 2017-12-09 18:36:15.444160: step 49480, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:58m:06s remains)
INFO - root - 2017-12-09 18:36:24.049800: step 49490, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 67h:39m:52s remains)
INFO - root - 2017-12-09 18:36:32.671823: step 49500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:36m:27s remains)
2017-12-09 18:36:33.639252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033467291 -0.0033438797 -0.0033422315 -0.0033394475 -0.0033353958 -0.0033301678 -0.0033255513 -0.0033227771 -0.0033215149 -0.0033214376 -0.003321924 -0.0033226674 -0.0033231752 -0.0033234709 -0.0033236009][-0.0033424785 -0.0033393521 -0.0033381023 -0.0033356678 -0.0033317767 -0.003326698 -0.0033222055 -0.0033194465 -0.003318005 -0.0033177375 -0.00331818 -0.0033188402 -0.0033192767 -0.0033195149 -0.0033196588][-0.0033392815 -0.0033359644 -0.00333552 -0.0033343532 -0.0033314233 -0.0033271515 -0.0033232647 -0.0033206609 -0.0033190106 -0.0033182965 -0.0033183787 -0.0033187387 -0.0033190688 -0.0033192504 -0.0033194514][-0.0033346128 -0.0033310452 -0.0033312731 -0.0033315574 -0.0033301283 -0.0033276125 -0.0033249238 -0.003322796 -0.0033210542 -0.0033198686 -0.0033193503 -0.0033191119 -0.0033191219 -0.0033190723 -0.0033192544][-0.0033299264 -0.0033258263 -0.0033267119 -0.0033281504 -0.0033282805 -0.0033279078 -0.0033270712 -0.0033258006 -0.0033241955 -0.0033224213 -0.0033209871 -0.0033198015 -0.0033192039 -0.0033187722 -0.003318886][-0.0033264842 -0.0033221182 -0.0033236262 -0.0033262649 -0.0033280952 -0.0033298288 -0.0033307821 -0.0033303874 -0.003328684 -0.0033261846 -0.0033234383 -0.003321006 -0.0033196155 -0.0033187254 -0.0033186288][-0.0033246092 -0.0033204011 -0.0033225289 -0.0033263639 -0.0033297085 -0.0033331772 -0.0033353204 -0.0033356557 -0.0033339628 -0.0033308051 -0.0033266814 -0.0033227778 -0.0033203226 -0.0033188406 -0.0033184637][-0.0033234016 -0.0033196497 -0.0033220621 -0.0033266211 -0.0033309923 -0.0033357779 -0.0033388666 -0.0033396438 -0.0033380073 -0.0033346771 -0.0033295809 -0.0033242854 -0.0033206858 -0.0033185529 -0.0033179238][-0.00332237 -0.0033186933 -0.0033212875 -0.0033258614 -0.0033306174 -0.0033360696 -0.0033398997 -0.0033409379 -0.0033392818 -0.0033361565 -0.0033308722 -0.0033249052 -0.0033206921 -0.0033182802 -0.0033174639][-0.0033221312 -0.003318155 -0.0033203578 -0.0033244155 -0.0033290503 -0.0033344228 -0.0033383779 -0.0033398187 -0.0033387719 -0.0033362894 -0.0033316088 -0.0033261823 -0.0033221014 -0.0033196225 -0.0033184236][-0.0033235094 -0.0033188909 -0.0033202188 -0.0033229387 -0.0033265403 -0.0033310417 -0.0033346915 -0.003336618 -0.0033369267 -0.0033360571 -0.0033330873 -0.0033293345 -0.0033259725 -0.0033234551 -0.0033216143][-0.003326254 -0.003320944 -0.0033209997 -0.00332183 -0.0033235832 -0.0033264204 -0.0033291955 -0.0033315837 -0.0033335425 -0.0033347991 -0.0033346321 -0.0033336151 -0.0033316342 -0.0033292202 -0.0033266312][-0.0033309774 -0.0033247138 -0.0033233464 -0.0033219848 -0.0033214192 -0.0033224132 -0.0033239457 -0.0033263857 -0.0033297457 -0.0033329951 -0.003335777 -0.0033378415 -0.003338075 -0.003336349 -0.0033332552][-0.0033370226 -0.0033301024 -0.0033274416 -0.0033241052 -0.0033214134 -0.0033203536 -0.0033205531 -0.0033230165 -0.0033275571 -0.0033325173 -0.0033377856 -0.0033423312 -0.0033447433 -0.0033439158 -0.0033404157][-0.003342472 -0.0033352792 -0.0033318151 -0.0033269806 -0.0033224567 -0.0033197151 -0.0033189093 -0.0033213445 -0.0033265504 -0.003332746 -0.003339784 -0.0033462835 -0.0033499084 -0.0033494269 -0.0033457743]]...]
INFO - root - 2017-12-09 18:36:42.151153: step 49510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:34m:07s remains)
INFO - root - 2017-12-09 18:36:50.757773: step 49520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:27m:07s remains)
INFO - root - 2017-12-09 18:36:59.421735: step 49530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:29m:59s remains)
INFO - root - 2017-12-09 18:37:08.000362: step 49540, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 71h:38m:23s remains)
INFO - root - 2017-12-09 18:37:16.519800: step 49550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:14m:12s remains)
INFO - root - 2017-12-09 18:37:25.208051: step 49560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:02m:07s remains)
INFO - root - 2017-12-09 18:37:33.700951: step 49570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:42m:06s remains)
INFO - root - 2017-12-09 18:37:42.350452: step 49580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:45m:46s remains)
INFO - root - 2017-12-09 18:37:50.986460: step 49590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:29m:13s remains)
INFO - root - 2017-12-09 18:37:59.519001: step 49600, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 65h:10m:59s remains)
2017-12-09 18:38:00.350488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033979791 -0.0033962538 -0.0033957083 -0.0033955593 -0.0033957069 -0.0033958389 -0.0033951439 -0.0033933225 -0.0033902992 -0.0033862269 -0.0033819026 -0.0033784278 -0.0033761563 -0.0033750525 -0.0033749682][-0.0033910424 -0.0033896093 -0.0033897867 -0.0033901075 -0.0033903536 -0.0033903411 -0.0033893357 -0.0033873813 -0.0033845319 -0.0033809869 -0.00337736 -0.0033744308 -0.0033725591 -0.0033717463 -0.0033718538][-0.0033850004 -0.0033834586 -0.0033839925 -0.0033845268 -0.0033848237 -0.0033848835 -0.0033840125 -0.0033823124 -0.0033800644 -0.0033774644 -0.003374777 -0.0033726168 -0.0033712548 -0.0033705933 -0.0033705418][-0.0033795086 -0.0033772674 -0.0033774655 -0.0033777996 -0.0033780518 -0.003378218 -0.0033777724 -0.0033768283 -0.0033755228 -0.0033740124 -0.0033723835 -0.0033711409 -0.003370441 -0.0033699982 -0.003369937][-0.0033767342 -0.0033738627 -0.0033736026 -0.00337349 -0.0033733742 -0.0033732366 -0.0033728858 -0.0033725081 -0.0033721579 -0.0033718236 -0.0033713095 -0.0033709134 -0.0033706452 -0.0033703041 -0.0033700322][-0.0033764646 -0.0033733966 -0.0033729477 -0.0033725149 -0.0033721616 -0.0033718273 -0.0033714678 -0.0033714222 -0.0033715512 -0.0033719544 -0.0033721686 -0.0033722713 -0.0033720846 -0.0033716292 -0.0033710455][-0.0033780586 -0.0033752124 -0.0033748683 -0.0033742688 -0.0033736106 -0.0033727637 -0.0033723088 -0.0033723277 -0.0033725949 -0.0033733193 -0.003373896 -0.0033743072 -0.0033741537 -0.0033735579 -0.0033727796][-0.0033800083 -0.0033776562 -0.0033775191 -0.0033767554 -0.0033756911 -0.003374364 -0.0033738257 -0.0033739731 -0.003374279 -0.0033751221 -0.0033758956 -0.0033765088 -0.0033764511 -0.0033757649 -0.0033748322][-0.0033816011 -0.0033798667 -0.0033801987 -0.0033796765 -0.0033785196 -0.0033769137 -0.0033763282 -0.0033764218 -0.0033765393 -0.0033772085 -0.0033780301 -0.0033787037 -0.0033786115 -0.0033778728 -0.0033768911][-0.0033815037 -0.003380086 -0.0033807778 -0.0033806146 -0.0033798381 -0.0033785067 -0.0033781286 -0.0033782676 -0.0033783917 -0.0033790576 -0.0033797526 -0.0033802192 -0.0033799834 -0.0033792593 -0.0033782767][-0.0033802714 -0.00337889 -0.003379917 -0.0033801794 -0.0033798728 -0.0033789577 -0.0033786816 -0.00337873 -0.0033788031 -0.0033792586 -0.0033796532 -0.0033797889 -0.0033794187 -0.003378761 -0.0033778844][-0.0033787065 -0.003376994 -0.0033780388 -0.0033784546 -0.0033783638 -0.0033777433 -0.0033775887 -0.0033776197 -0.0033776967 -0.0033780374 -0.0033782548 -0.0033782686 -0.003377967 -0.0033774285 -0.0033766422][-0.0033769468 -0.00337497 -0.0033759368 -0.0033764062 -0.0033765521 -0.0033762883 -0.0033761903 -0.0033761887 -0.0033762844 -0.003376516 -0.0033765612 -0.003376462 -0.0033761999 -0.0033756997 -0.0033750045][-0.0033748862 -0.0033724238 -0.00337316 -0.0033736031 -0.0033739377 -0.0033740487 -0.0033741067 -0.0033741696 -0.0033743042 -0.0033744446 -0.0033744418 -0.0033743605 -0.003374188 -0.0033738075 -0.0033732932][-0.0033735721 -0.0033705719 -0.0033709051 -0.0033711747 -0.0033714473 -0.0033716557 -0.0033717947 -0.003371926 -0.0033720676 -0.0033722 -0.0033722539 -0.0033722518 -0.0033722019 -0.003371998 -0.0033717027]]...]
INFO - root - 2017-12-09 18:38:08.919215: step 49610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:55m:44s remains)
INFO - root - 2017-12-09 18:38:17.547749: step 49620, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:51m:46s remains)
INFO - root - 2017-12-09 18:38:26.228763: step 49630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:46m:43s remains)
INFO - root - 2017-12-09 18:38:34.712336: step 49640, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:25m:52s remains)
INFO - root - 2017-12-09 18:38:43.502676: step 49650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:08m:51s remains)
INFO - root - 2017-12-09 18:38:52.075846: step 49660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:00m:57s remains)
INFO - root - 2017-12-09 18:39:00.606378: step 49670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:26m:30s remains)
INFO - root - 2017-12-09 18:39:09.266744: step 49680, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:25m:41s remains)
INFO - root - 2017-12-09 18:39:18.039737: step 49690, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 66h:38m:51s remains)
INFO - root - 2017-12-09 18:39:26.673287: step 49700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:18m:05s remains)
2017-12-09 18:39:27.598782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033827987 -0.0033793398 -0.0033780367 -0.0033777438 -0.0033782676 -0.0033790788 -0.003379897 -0.0033807352 -0.0033819466 -0.0033835177 -0.0033857408 -0.0033885075 -0.0033910142 -0.0033929525 -0.0033945071][-0.003384352 -0.0033805787 -0.0033790239 -0.0033787456 -0.0033788292 -0.0033793661 -0.0033802176 -0.0033815764 -0.0033833205 -0.0033854356 -0.0033882237 -0.0033911413 -0.0033933851 -0.0033946559 -0.0033952466][-0.0033900458 -0.0033867625 -0.0033846961 -0.0033838628 -0.0033833771 -0.0033836614 -0.0033846514 -0.0033861992 -0.0033882866 -0.0033908072 -0.0033936794 -0.0033957493 -0.0033966454 -0.0033964308 -0.0033957718][-0.00339607 -0.0033935546 -0.00339149 -0.0033903872 -0.0033897304 -0.0033900836 -0.0033913227 -0.0033931152 -0.0033949546 -0.0033968478 -0.0033988396 -0.0033995353 -0.0033986131 -0.0033968815 -0.0033955802][-0.0034000108 -0.0033983372 -0.0033968815 -0.0033961302 -0.003395722 -0.0033961462 -0.0033974252 -0.0033988128 -0.0034002187 -0.0034013805 -0.0034023535 -0.0034016185 -0.0033990024 -0.0033960836 -0.0033941411][-0.0034006529 -0.003399479 -0.0033990336 -0.0033987479 -0.0033986045 -0.0033988168 -0.0033996953 -0.0034010005 -0.0034024701 -0.003403102 -0.0034035614 -0.0034022373 -0.0033988631 -0.0033954943 -0.0033936086][-0.0033987756 -0.0033976971 -0.0033979996 -0.0033984035 -0.0033987365 -0.0033983849 -0.0033982473 -0.0033987043 -0.0034001369 -0.0034013274 -0.0034022653 -0.0034014441 -0.0033984485 -0.0033954091 -0.0033936736][-0.0033951234 -0.0033942456 -0.0033947641 -0.0033953474 -0.0033956284 -0.0033952168 -0.0033937697 -0.0033929865 -0.0033952324 -0.0033974494 -0.0033989765 -0.003399085 -0.003397136 -0.0033951688 -0.0033940687][-0.003390417 -0.0033889788 -0.0033898244 -0.0033905627 -0.0033910295 -0.0033909772 -0.0033900985 -0.0033897064 -0.0033912926 -0.0033936603 -0.0033957961 -0.0033966536 -0.003395807 -0.0033945404 -0.0033935839][-0.0033862037 -0.0033840467 -0.0033848577 -0.0033858821 -0.0033865403 -0.003386619 -0.0033864435 -0.0033864025 -0.0033872274 -0.0033891057 -0.0033913786 -0.0033927672 -0.0033928219 -0.0033923355 -0.0033915888][-0.003383223 -0.0033806106 -0.0033812537 -0.0033820702 -0.0033825615 -0.0033826984 -0.0033829247 -0.003383331 -0.0033840232 -0.0033853818 -0.003387097 -0.0033883697 -0.003388803 -0.0033886661 -0.0033882146][-0.003381151 -0.0033780243 -0.0033783859 -0.0033789249 -0.0033792912 -0.0033795862 -0.0033800413 -0.0033805843 -0.0033811429 -0.0033821138 -0.0033833224 -0.0033841913 -0.00338439 -0.0033841785 -0.0033837019][-0.0033801768 -0.0033765123 -0.0033766211 -0.0033768523 -0.0033771312 -0.0033775482 -0.0033780588 -0.0033786464 -0.0033793407 -0.0033800146 -0.0033805529 -0.0033807766 -0.0033806812 -0.00338023 -0.003379833][-0.0033802374 -0.0033761614 -0.003375968 -0.0033758883 -0.0033759414 -0.0033762681 -0.0033766357 -0.003377198 -0.0033776928 -0.00337816 -0.0033786031 -0.0033785664 -0.0033785033 -0.0033782655 -0.0033779859][-0.0033806791 -0.0033765307 -0.0033761547 -0.003375828 -0.003375662 -0.0033757507 -0.0033759808 -0.0033762273 -0.003376513 -0.0033768835 -0.0033773137 -0.00337747 -0.0033776441 -0.0033775785 -0.0033773493]]...]
INFO - root - 2017-12-09 18:39:36.098484: step 49710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:24m:07s remains)
INFO - root - 2017-12-09 18:39:44.682336: step 49720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:36m:43s remains)
INFO - root - 2017-12-09 18:39:53.291171: step 49730, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 69h:56m:37s remains)
INFO - root - 2017-12-09 18:40:01.957369: step 49740, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 69h:16m:27s remains)
INFO - root - 2017-12-09 18:40:10.435879: step 49750, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 65h:58m:34s remains)
INFO - root - 2017-12-09 18:40:19.067837: step 49760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 66h:14m:31s remains)
INFO - root - 2017-12-09 18:40:27.444569: step 49770, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:23m:42s remains)
INFO - root - 2017-12-09 18:40:36.294598: step 49780, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 71h:09m:57s remains)
INFO - root - 2017-12-09 18:40:44.927567: step 49790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:47m:06s remains)
INFO - root - 2017-12-09 18:40:53.596156: step 49800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 66h:28m:25s remains)
2017-12-09 18:40:54.628159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033887215 -0.0033864579 -0.0033857333 -0.0033858584 -0.0033857531 -0.0033858637 -0.0033864747 -0.0033869708 -0.0033878391 -0.0033881187 -0.0033884444 -0.0033889676 -0.0033893129 -0.00338964 -0.0033897753][-0.0033863371 -0.0033841422 -0.0033838125 -0.0033841536 -0.0033846258 -0.0033852495 -0.0033861278 -0.0033866689 -0.0033873091 -0.0033876325 -0.0033878128 -0.0033879725 -0.0033879476 -0.0033878563 -0.003387521][-0.0033844977 -0.0033826637 -0.0033826535 -0.0033832525 -0.0033843589 -0.0033856551 -0.0033870316 -0.003387755 -0.0033881881 -0.0033879282 -0.0033870703 -0.0033861906 -0.0033851706 -0.0033840982 -0.0033828083][-0.003376842 -0.0033750855 -0.0033760108 -0.0033779643 -0.0033805775 -0.003383232 -0.0033857427 -0.0033872551 -0.0033879047 -0.0033872414 -0.0033856686 -0.0033835946 -0.0033812004 -0.0033786716 -0.0033761016][-0.0033714548 -0.0033698641 -0.0033717121 -0.0033749489 -0.0033788409 -0.0033826937 -0.0033860835 -0.0033883904 -0.0033895485 -0.0033891329 -0.0033872721 -0.0033841399 -0.003380717 -0.0033770711 -0.0033732506][-0.0033694457 -0.003368286 -0.0033712231 -0.0033758827 -0.0033809177 -0.0033857313 -0.00338984 -0.0033927355 -0.0033940079 -0.0033936189 -0.0033912796 -0.003387294 -0.0033825936 -0.0033778031 -0.0033732792][-0.0033706492 -0.0033701756 -0.0033742641 -0.0033800339 -0.0033858535 -0.0033912158 -0.003395654 -0.0033986366 -0.0033998669 -0.0033991598 -0.0033963847 -0.0033918263 -0.003386073 -0.0033799852 -0.0033742252][-0.0033731905 -0.003373354 -0.0033777556 -0.0033839394 -0.0033900645 -0.0033954275 -0.0033996713 -0.0034022951 -0.0034030764 -0.0034019323 -0.0033989728 -0.0033943194 -0.0033883485 -0.0033817242 -0.0033751957][-0.0033744425 -0.0033749735 -0.003379812 -0.0033862721 -0.0033920982 -0.003397176 -0.0034009451 -0.0034026979 -0.0034026336 -0.0034013088 -0.0033989027 -0.0033949683 -0.0033897164 -0.0033836486 -0.0033773754][-0.0033755754 -0.00337603 -0.0033806111 -0.00338641 -0.0033914973 -0.0033960682 -0.0033995078 -0.0034008308 -0.0034005281 -0.0033994108 -0.0033974468 -0.0033941059 -0.0033894761 -0.0033841056 -0.0033784618][-0.0033764958 -0.0033765647 -0.0033804381 -0.0033852945 -0.0033897501 -0.0033939581 -0.0033972429 -0.003398726 -0.0033985185 -0.0033972771 -0.003395116 -0.0033918172 -0.0033874477 -0.0033826281 -0.0033779389][-0.0033786849 -0.0033782953 -0.0033809342 -0.0033842032 -0.003387522 -0.0033908894 -0.0033935157 -0.0033947765 -0.0033946079 -0.0033933057 -0.0033909988 -0.0033878747 -0.0033842034 -0.0033803252 -0.003376775][-0.0033790104 -0.0033778672 -0.0033792425 -0.0033812548 -0.0033834078 -0.0033858009 -0.0033876346 -0.0033884398 -0.003388145 -0.0033870386 -0.0033852535 -0.0033829913 -0.0033805273 -0.0033780404 -0.0033758157][-0.0033782595 -0.0033762148 -0.0033766837 -0.0033777696 -0.0033789398 -0.003380368 -0.0033813063 -0.0033818639 -0.0033816008 -0.00338081 -0.0033796588 -0.0033784439 -0.0033771354 -0.0033757293 -0.0033744047][-0.0033768946 -0.003374357 -0.0033741696 -0.003374507 -0.0033749638 -0.0033757859 -0.0033761994 -0.0033768266 -0.0033769046 -0.0033765722 -0.0033759184 -0.0033753226 -0.0033745808 -0.0033735349 -0.0033725181]]...]
INFO - root - 2017-12-09 18:41:03.174608: step 49810, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 67h:32m:40s remains)
INFO - root - 2017-12-09 18:41:11.767114: step 49820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:51m:18s remains)
INFO - root - 2017-12-09 18:41:20.503205: step 49830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:42m:58s remains)
INFO - root - 2017-12-09 18:41:29.121052: step 49840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:59m:18s remains)
INFO - root - 2017-12-09 18:41:37.542931: step 49850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:38m:50s remains)
INFO - root - 2017-12-09 18:41:46.165440: step 49860, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:40m:14s remains)
INFO - root - 2017-12-09 18:41:54.789440: step 49870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:31m:58s remains)
INFO - root - 2017-12-09 18:42:03.445382: step 49880, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 70h:33m:39s remains)
INFO - root - 2017-12-09 18:42:12.139258: step 49890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:11m:00s remains)
INFO - root - 2017-12-09 18:42:20.968789: step 49900, loss = 0.90, batch loss = 0.70 (8.9 examples/sec; 0.895 sec/batch; 70h:13m:12s remains)
2017-12-09 18:42:21.873903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00337987 -0.0033766797 -0.0033764343 -0.0033778164 -0.0033810516 -0.0033851122 -0.0033893932 -0.0033932819 -0.0033968131 -0.0033980915 -0.0033969253 -0.0033946848 -0.0033918384 -0.0033893525 -0.0033886516][-0.0033861587 -0.00338358 -0.0033826993 -0.0033832248 -0.003385067 -0.0033869671 -0.0033891029 -0.0033918249 -0.0033952033 -0.0033972959 -0.0033972973 -0.003396323 -0.0033942899 -0.0033917732 -0.0033908524][-0.0033914957 -0.0033894088 -0.0033883536 -0.0033881182 -0.0033886528 -0.003389131 -0.0033900014 -0.0033915164 -0.0033940736 -0.0033963248 -0.0033973581 -0.0033977847 -0.003396689 -0.0033949779 -0.0033940615][-0.0033913055 -0.0033902067 -0.0033892673 -0.0033887161 -0.0033881906 -0.0033869795 -0.0033868558 -0.003387467 -0.0033897189 -0.0033922268 -0.0033940943 -0.00339577 -0.0033956608 -0.0033947036 -0.0033940435][-0.0033877206 -0.0033871986 -0.003386928 -0.0033863506 -0.0033852626 -0.003383182 -0.0033820609 -0.0033819021 -0.003383378 -0.00338611 -0.0033889294 -0.0033912067 -0.0033917262 -0.00339134 -0.0033910424][-0.0033839461 -0.0033837096 -0.0033841967 -0.0033835305 -0.0033821883 -0.0033803585 -0.00337887 -0.0033774143 -0.0033779168 -0.0033806718 -0.0033837503 -0.0033858933 -0.0033867629 -0.0033867436 -0.0033863417][-0.0033819629 -0.0033817119 -0.0033826472 -0.0033826167 -0.0033817813 -0.0033803054 -0.0033781535 -0.0033753444 -0.003375066 -0.0033772476 -0.0033796725 -0.0033815019 -0.0033823398 -0.0033822278 -0.0033817627][-0.0033831459 -0.0033829056 -0.0033845666 -0.0033852656 -0.0033852279 -0.0033843527 -0.0033812809 -0.0033772236 -0.0033757011 -0.0033766832 -0.0033779822 -0.0033789524 -0.0033796586 -0.0033797144 -0.0033792583][-0.0033862477 -0.0033861066 -0.0033879611 -0.0033895273 -0.0033904202 -0.0033893811 -0.003385145 -0.0033803892 -0.0033778651 -0.0033773598 -0.0033776492 -0.0033784914 -0.0033794527 -0.0033796337 -0.0033794509][-0.0033890116 -0.0033884523 -0.003389904 -0.0033917914 -0.0033931995 -0.0033919716 -0.0033879487 -0.0033836551 -0.0033804008 -0.0033786355 -0.0033780381 -0.0033785899 -0.0033792006 -0.0033793719 -0.0033795193][-0.0033922445 -0.0033908831 -0.0033916011 -0.0033931856 -0.0033942803 -0.0033933639 -0.003390339 -0.0033869231 -0.0033832984 -0.0033808823 -0.0033797678 -0.0033800201 -0.0033801754 -0.0033799435 -0.0033800262][-0.0033956815 -0.0033936338 -0.0033933776 -0.0033943977 -0.0033952533 -0.0033946673 -0.0033927739 -0.0033902056 -0.0033869927 -0.0033846169 -0.0033835857 -0.0033835582 -0.0033832379 -0.0033825019 -0.0033820416][-0.0033996031 -0.0033975814 -0.0033969381 -0.0033973015 -0.0033975276 -0.0033970724 -0.0033962142 -0.0033949155 -0.003392685 -0.0033908272 -0.003389678 -0.0033889997 -0.0033879853 -0.0033865764 -0.0033853701][-0.0034015041 -0.003399777 -0.0033989311 -0.0033986755 -0.0033985111 -0.003398655 -0.0033987605 -0.0033987332 -0.0033983039 -0.0033977579 -0.003396871 -0.0033956836 -0.0033939255 -0.0033919311 -0.0033899145][-0.0034026308 -0.003401455 -0.0034010266 -0.0034006145 -0.0034002524 -0.0034005886 -0.0034013512 -0.0034022231 -0.0034028955 -0.0034033775 -0.0034029298 -0.00340189 -0.0034000187 -0.0033977169 -0.0033951919]]...]
INFO - root - 2017-12-09 18:42:30.421155: step 49910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:10m:27s remains)
INFO - root - 2017-12-09 18:42:39.089622: step 49920, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:17m:48s remains)
INFO - root - 2017-12-09 18:42:47.712508: step 49930, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 66h:02m:03s remains)
INFO - root - 2017-12-09 18:42:56.292703: step 49940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 70h:01m:01s remains)
INFO - root - 2017-12-09 18:43:04.779190: step 49950, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:39m:55s remains)
INFO - root - 2017-12-09 18:43:13.262602: step 49960, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 66h:44m:03s remains)
INFO - root - 2017-12-09 18:43:21.707373: step 49970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:57m:54s remains)
INFO - root - 2017-12-09 18:43:30.438240: step 49980, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:34m:41s remains)
INFO - root - 2017-12-09 18:43:39.172137: step 49990, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 68h:53m:25s remains)
INFO - root - 2017-12-09 18:43:47.925477: step 50000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:02m:40s remains)
2017-12-09 18:43:48.869736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003398956 -0.0033957881 -0.0033745759 -0.0032279177 -0.0028606064 -0.0023023169 -0.00181904 -0.0016375236 -0.0017678146 -0.0021824851 -0.0025828872 -0.0029654792 -0.0032355394 -0.0033577369 -0.0033935034][-0.0033956044 -0.0033909883 -0.0033304829 -0.0030195597 -0.0023712153 -0.0014721628 -0.00077011134 -0.00054783653 -0.0008246568 -0.0015033029 -0.002183402 -0.0027786416 -0.0031719333 -0.0033424834 -0.0033924533][-0.0033564155 -0.0033621753 -0.0032318197 -0.0026449678 -0.0015301337 -6.4319232e-05 0.0010436766 0.0014210611 0.00098021119 -3.4885714e-05 -0.0011680482 -0.0021941219 -0.0029239086 -0.0032694421 -0.0033790525][-0.0032485041 -0.0032534727 -0.0030097221 -0.0020393622 -0.00028162054 0.0019604533 0.0036806951 0.0043857591 0.0038482358 0.00248885 0.00072893105 -0.0010015545 -0.0023444477 -0.0030683037 -0.0033306249][-0.0030787461 -0.0030639444 -0.0026404033 -0.0011855795 0.0013332881 0.004463424 0.0069300011 0.0080995858 0.0075864028 0.005951412 0.0035080609 0.00088111148 -0.0013407706 -0.0026823932 -0.003226365][-0.0028758086 -0.0028179134 -0.0021565938 -0.0001994397 0.0030700036 0.0070108026 0.010179084 0.011845127 0.011538035 0.0098257456 0.0068218336 0.0032588143 -2.9464718e-06 -0.0021374286 -0.003068683][-0.0026995214 -0.0025976792 -0.00170719 0.0006588262 0.0045037409 0.008992061 0.012649247 0.014713235 0.014723692 0.013148485 0.0098671233 0.0055655446 0.0013421846 -0.0015767679 -0.0029035681][-0.0025620223 -0.0024411157 -0.0014165477 0.0011477445 0.0052306764 0.0098779555 0.013728544 0.016036855 0.016400864 0.015099319 0.011792887 0.0070742015 0.0022193009 -0.0012188635 -0.0028026947][-0.002516309 -0.0023721699 -0.0013512259 0.0011091323 0.0050083315 0.009406969 0.013129804 0.015508382 0.016163152 0.015163131 0.012001921 0.007237575 0.002264864 -0.001229112 -0.0028178939][-0.0025884947 -0.0024334881 -0.001535501 0.00056953193 0.003906725 0.007689815 0.010990806 0.013252363 0.014046254 0.013265451 0.010356085 0.0059471289 0.0014373192 -0.0016102666 -0.0029419742][-0.0027522594 -0.0025946896 -0.0019055681 -0.00031971792 0.0022173522 0.00516105 0.0078320587 0.0097748023 0.010498159 0.0098232431 0.0072874781 0.00361509 5.2814372e-05 -0.0021970007 -0.0031145981][-0.0029776241 -0.0028323783 -0.0023583244 -0.0013038404 0.00038544042 0.0024157206 0.0043331087 0.0057921493 0.0063064117 0.0056972262 0.0036849107 0.0010064922 -0.0013841535 -0.0027556685 -0.0032637219][-0.0031885344 -0.0030769457 -0.0027838789 -0.002174122 -0.0012009807 1.2730481e-05 0.0011880144 0.002099409 0.002367574 0.0018664727 0.00048797228 -0.0011542339 -0.0024695185 -0.0031354981 -0.0033531766][-0.0033360256 -0.0032711888 -0.0031217018 -0.0028279838 -0.0023536007 -0.0017420456 -0.0011492583 -0.00069455709 -0.00060655549 -0.0009426598 -0.0017065722 -0.0025086463 -0.0030730828 -0.0033200565 -0.0033890961][-0.0033903816 -0.0033725207 -0.0033199044 -0.0032069732 -0.0030243422 -0.0027860408 -0.0025549927 -0.0023838046 -0.0023719403 -0.0025391576 -0.002855188 -0.0031434768 -0.003318026 -0.0033820707 -0.0033971823]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 18:43:57.973684: step 50010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:43m:33s remains)
INFO - root - 2017-12-09 18:44:06.855806: step 50020, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 69h:40m:37s remains)
INFO - root - 2017-12-09 18:44:15.595748: step 50030, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.915 sec/batch; 71h:46m:24s remains)
INFO - root - 2017-12-09 18:44:24.051725: step 50040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:05m:43s remains)
INFO - root - 2017-12-09 18:44:32.525374: step 50050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:37m:51s remains)
INFO - root - 2017-12-09 18:44:41.014535: step 50060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 65h:55m:45s remains)
INFO - root - 2017-12-09 18:44:49.522120: step 50070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:18m:22s remains)
INFO - root - 2017-12-09 18:44:58.183876: step 50080, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:29m:45s remains)
INFO - root - 2017-12-09 18:45:06.872749: step 50090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:49m:19s remains)
INFO - root - 2017-12-09 18:45:15.472078: step 50100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:59m:16s remains)
2017-12-09 18:45:16.319347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003370848 -0.003368366 -0.0033684876 -0.0033697376 -0.0033721952 -0.0033764753 -0.0033826726 -0.0033895664 -0.0033962743 -0.0034019495 -0.0034058688 -0.0034062651 -0.00340337 -0.0033995218 -0.0033954545][-0.0033688121 -0.0033661085 -0.0033661537 -0.0033671495 -0.0033690287 -0.0033725414 -0.0033779666 -0.0033840879 -0.0033902451 -0.0033954405 -0.0033992629 -0.0034001719 -0.0033981476 -0.0033950019 -0.003391284][-0.0033690841 -0.0033662359 -0.0033659288 -0.0033663684 -0.0033673714 -0.0033696655 -0.0033738 -0.0033788551 -0.0033841331 -0.0033886384 -0.0033922079 -0.0033937029 -0.0033928242 -0.0033906144 -0.0033875492][-0.0033692233 -0.0033663139 -0.0033657269 -0.0033656592 -0.0033658855 -0.0033670331 -0.0033698352 -0.0033737379 -0.0033780718 -0.0033818018 -0.0033852595 -0.0033874633 -0.0033879289 -0.0033872609 -0.0033854239][-0.0033696808 -0.0033664303 -0.003365492 -0.0033649732 -0.0033644908 -0.0033647614 -0.0033665569 -0.0033693148 -0.0033726364 -0.0033756751 -0.0033791326 -0.0033821473 -0.0033840607 -0.0033853059 -0.0033852346][-0.0033700163 -0.003366509 -0.003365438 -0.0033646412 -0.0033637045 -0.0033633313 -0.0033643802 -0.003366332 -0.0033688722 -0.0033714813 -0.0033751838 -0.0033792076 -0.0033825268 -0.0033854067 -0.0033868512][-0.0033701041 -0.0033665642 -0.0033654871 -0.0033645246 -0.0033633818 -0.0033627043 -0.0033632498 -0.0033647069 -0.003366902 -0.0033695772 -0.0033737333 -0.0033786909 -0.0033832265 -0.0033873867 -0.0033898321][-0.0033700245 -0.0033665837 -0.0033655432 -0.00336453 -0.0033633308 -0.0033625092 -0.0033626896 -0.0033639306 -0.0033662554 -0.0033694308 -0.0033742143 -0.0033799747 -0.0033854763 -0.0033904756 -0.0033934428][-0.0033698955 -0.0033665672 -0.0033656787 -0.0033646547 -0.0033634184 -0.0033623895 -0.0033621583 -0.0033632587 -0.0033657649 -0.0033695369 -0.0033748688 -0.0033810525 -0.0033870277 -0.0033922957 -0.0033952922][-0.0033698932 -0.0033666415 -0.0033658259 -0.0033648429 -0.0033635027 -0.0033622347 -0.0033615148 -0.0033624459 -0.0033649893 -0.0033688894 -0.0033742143 -0.003380124 -0.0033857832 -0.0033905562 -0.0033930747][-0.0033699155 -0.0033666708 -0.0033659127 -0.003364908 -0.0033634156 -0.0033619471 -0.0033608708 -0.0033613082 -0.0033635558 -0.0033672375 -0.0033719656 -0.0033771142 -0.0033819338 -0.0033857992 -0.0033877129][-0.0033700725 -0.0033669698 -0.00336623 -0.0033653737 -0.0033641527 -0.0033627937 -0.0033616982 -0.0033616759 -0.0033632515 -0.0033661004 -0.0033696087 -0.0033733142 -0.0033767947 -0.0033794218 -0.0033806926][-0.0033703304 -0.0033674513 -0.003366896 -0.0033666748 -0.0033661413 -0.0033654673 -0.0033649234 -0.0033649195 -0.0033658228 -0.0033675106 -0.0033695477 -0.0033715267 -0.0033733537 -0.0033745503 -0.0033749782][-0.0033707903 -0.0033681125 -0.0033681754 -0.0033689309 -0.0033696559 -0.0033704406 -0.0033711579 -0.0033718653 -0.0033725123 -0.003373039 -0.0033734022 -0.0033734306 -0.003373258 -0.0033726336 -0.0033718736][-0.003371214 -0.0033689379 -0.0033698727 -0.0033719216 -0.0033742904 -0.0033766746 -0.0033787065 -0.0033802723 -0.0033809375 -0.0033804674 -0.0033793536 -0.0033778504 -0.003376171 -0.0033740727 -0.0033722834]]...]
INFO - root - 2017-12-09 18:45:25.007035: step 50110, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 70h:09m:53s remains)
INFO - root - 2017-12-09 18:45:33.569275: step 50120, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 66h:38m:15s remains)
INFO - root - 2017-12-09 18:45:42.268411: step 50130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:13m:59s remains)
INFO - root - 2017-12-09 18:45:50.838430: step 50140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:00m:30s remains)
INFO - root - 2017-12-09 18:45:59.431532: step 50150, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 72h:01m:06s remains)
INFO - root - 2017-12-09 18:46:08.036202: step 50160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:03m:44s remains)
INFO - root - 2017-12-09 18:46:16.453981: step 50170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:39m:43s remains)
INFO - root - 2017-12-09 18:46:25.070424: step 50180, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 68h:09m:20s remains)
INFO - root - 2017-12-09 18:46:33.702986: step 50190, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.856 sec/batch; 67h:09m:48s remains)
INFO - root - 2017-12-09 18:46:42.342272: step 50200, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:47m:55s remains)
2017-12-09 18:46:43.275213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034115154 -0.0034108977 -0.0034112236 -0.0034115359 -0.0034117054 -0.0034114993 -0.0034114486 -0.0034114188 -0.003411269 -0.0034109587 -0.0034106218 -0.0034107831 -0.003411721 -0.0034127063 -0.0034127661][-0.0034096208 -0.003409093 -0.0034093347 -0.00340974 -0.0034101671 -0.0034100027 -0.0034099468 -0.0034100018 -0.0034099137 -0.0034093207 -0.003409025 -0.0034093424 -0.0034105473 -0.0034116828 -0.0034110332][-0.003409683 -0.0034093452 -0.0034097598 -0.0034103324 -0.003410426 -0.0034098087 -0.003409236 -0.0034086653 -0.0034086937 -0.0034080592 -0.0034080551 -0.003408659 -0.0034100036 -0.0034110928 -0.0034096579][-0.0034099673 -0.0034099144 -0.0034107256 -0.0034111766 -0.0034106013 -0.0034085675 -0.0034061768 -0.0034040667 -0.0034037067 -0.0034047181 -0.0034064604 -0.0034080511 -0.0034097338 -0.0034105314 -0.0034078336][-0.0034100967 -0.003410208 -0.0034110944 -0.0034107524 -0.0034087517 -0.0034039617 -0.0033977085 -0.0033923679 -0.0033921273 -0.0033968086 -0.0034024788 -0.0034063673 -0.0034088253 -0.0034096772 -0.003406212][-0.0034097165 -0.0034102574 -0.0034113941 -0.0034099361 -0.003405448 -0.0033966168 -0.0033852553 -0.0033758127 -0.0033756765 -0.0033851846 -0.0033959909 -0.0034029929 -0.0034065691 -0.0034075337 -0.0034040369][-0.0034085964 -0.0034093924 -0.00341055 -0.0034086246 -0.0034022338 -0.0033901583 -0.0033737184 -0.003360224 -0.0033603476 -0.0033742278 -0.0033893411 -0.0033990187 -0.0034038017 -0.0034048597 -0.0034007863][-0.0034076977 -0.0034085766 -0.0034096886 -0.003407754 -0.0034009786 -0.0033876221 -0.0033699831 -0.0033558349 -0.0033568663 -0.0033713025 -0.0033866288 -0.0033975672 -0.0034025637 -0.0034032171 -0.003398133][-0.0034071077 -0.0034078464 -0.0034092492 -0.003407879 -0.003402381 -0.0033910216 -0.0033778751 -0.0033682969 -0.0033685232 -0.0033783722 -0.0033908517 -0.0033998364 -0.0034040078 -0.0034029041 -0.003395552][-0.0034065871 -0.0034073142 -0.0034088036 -0.0034084772 -0.0034056103 -0.0033985912 -0.0033907262 -0.0033857224 -0.0033849473 -0.0033901986 -0.0033981444 -0.0034035246 -0.0034060488 -0.0034030387 -0.003392671][-0.0034065244 -0.003407158 -0.00340883 -0.0034098185 -0.0034094122 -0.0034068385 -0.0034029533 -0.0034003798 -0.0033991744 -0.0034009833 -0.0034048269 -0.003407279 -0.0034080937 -0.0034035312 -0.0033908603][-0.0034062171 -0.003406537 -0.0034082383 -0.0034100858 -0.0034114544 -0.0034113654 -0.0034103866 -0.0034090737 -0.0034072523 -0.003406968 -0.0034083938 -0.003409775 -0.003409944 -0.0034049703 -0.0033918023][-0.0034046855 -0.0034048732 -0.0034068369 -0.0034090593 -0.0034109815 -0.0034120081 -0.003412172 -0.0034115603 -0.0034103233 -0.0034095747 -0.0034098011 -0.0034106497 -0.0034110488 -0.0034068876 -0.0033949572][-0.0034002413 -0.0034007165 -0.0034038112 -0.0034066592 -0.0034088164 -0.003410347 -0.0034107049 -0.0034100795 -0.003409286 -0.0034090409 -0.00340915 -0.0034098921 -0.0034106446 -0.0034084278 -0.0033986][-0.003391969 -0.0033940051 -0.0033995591 -0.0034040592 -0.0034064048 -0.0034076173 -0.0034080055 -0.0034076076 -0.0034071242 -0.0034067528 -0.0034068972 -0.0034079468 -0.0034094506 -0.0034090881 -0.003401841]]...]
INFO - root - 2017-12-09 18:46:51.927316: step 50210, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 67h:51m:19s remains)
INFO - root - 2017-12-09 18:47:00.513346: step 50220, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 67h:01m:30s remains)
INFO - root - 2017-12-09 18:47:08.969485: step 50230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:30m:32s remains)
INFO - root - 2017-12-09 18:47:17.401114: step 50240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:36m:40s remains)
INFO - root - 2017-12-09 18:47:25.845040: step 50250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:02m:57s remains)
INFO - root - 2017-12-09 18:47:34.298717: step 50260, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 66h:02m:26s remains)
INFO - root - 2017-12-09 18:47:42.686960: step 50270, loss = 0.89, batch loss = 0.68 (10.7 examples/sec; 0.745 sec/batch; 58h:22m:37s remains)
INFO - root - 2017-12-09 18:47:51.382773: step 50280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:03m:27s remains)
INFO - root - 2017-12-09 18:48:00.067528: step 50290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:21m:27s remains)
INFO - root - 2017-12-09 18:48:08.758845: step 50300, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:23m:26s remains)
2017-12-09 18:48:09.608083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033215145 -0.0033274493 -0.0033321318 -0.0033271355 -0.0033324696 -0.0033441863 -0.003345775 -0.0033434145 -0.0033385979 -0.0033433565 -0.0033487312 -0.0033496993 -0.0033499631 -0.0033502986 -0.0033514008][-0.0033116012 -0.00331141 -0.0033061269 -0.0032981962 -0.0033039446 -0.003303631 -0.0033053025 -0.0033034529 -0.0033028077 -0.0033158921 -0.0033240633 -0.0033315462 -0.0033341723 -0.0033381891 -0.0033440785][-0.003305594 -0.0032933902 -0.003295019 -0.0032937527 -0.0033045653 -0.00331284 -0.0033125093 -0.0033134441 -0.0033121086 -0.0033201049 -0.0033271285 -0.0033322542 -0.0033322116 -0.0033361069 -0.0033416178][-0.0032567249 -0.003269488 -0.0032740235 -0.0032646847 -0.0032826946 -0.0032987024 -0.0033156162 -0.003326711 -0.0033249534 -0.0033330973 -0.0033365234 -0.003340458 -0.0033399023 -0.0033415325 -0.0033440667][-0.0031683871 -0.0031782347 -0.0031918434 -0.0031966656 -0.0032106382 -0.0032185873 -0.0032351003 -0.003263684 -0.0032820429 -0.0033061819 -0.0033120324 -0.0033166758 -0.0033232665 -0.003329634 -0.0033400268][-0.0030508027 -0.0030521466 -0.0030562847 -0.0030469371 -0.0030441829 -0.0030559357 -0.0030768849 -0.0031074272 -0.0031414055 -0.0031923477 -0.00322912 -0.0032528576 -0.0032693648 -0.0032880642 -0.0033173678][-0.0028636395 -0.0028554341 -0.0028603196 -0.0028549661 -0.0028399348 -0.0028306027 -0.0028407858 -0.0028896516 -0.0029470967 -0.0030190505 -0.0030840931 -0.0031365117 -0.0031843104 -0.0032239291 -0.0032749963][-0.0026752749 -0.0026552239 -0.0026637856 -0.0026630082 -0.0026489955 -0.0026404748 -0.0026458791 -0.0026886349 -0.002756472 -0.0028533819 -0.0029450469 -0.0030203555 -0.0030949302 -0.0031543351 -0.0032282406][-0.002594864 -0.0025591685 -0.0025702827 -0.0025768413 -0.0025686095 -0.0025690862 -0.0025816336 -0.0026226528 -0.0026817268 -0.0027724989 -0.0028762077 -0.0029714615 -0.0030583157 -0.0031213793 -0.0032001373][-0.0026645365 -0.0026195033 -0.0026369612 -0.0026532065 -0.0026491689 -0.0026465014 -0.0026554225 -0.0026905616 -0.0027431694 -0.0028192792 -0.0029115346 -0.0030004834 -0.0030811997 -0.0031391534 -0.0032094349][-0.002841718 -0.0027998025 -0.0028206375 -0.0028465926 -0.0028546182 -0.0028562234 -0.002862019 -0.00288236 -0.0029177307 -0.0029721158 -0.0030391768 -0.0030998231 -0.0031523376 -0.0031970483 -0.0032518408][-0.0030493049 -0.0030190016 -0.0030355544 -0.0030596871 -0.0030721512 -0.0030776726 -0.0030851248 -0.0030978876 -0.0031185378 -0.0031485283 -0.0031866797 -0.0032182352 -0.0032431651 -0.0032693297 -0.0033032915][-0.0032325697 -0.0032171188 -0.003224331 -0.0032371068 -0.0032447022 -0.0032490145 -0.0032540017 -0.0032609829 -0.0032704859 -0.0032820706 -0.0032964761 -0.0033082818 -0.0033188427 -0.0033305418 -0.0033449964][-0.0033334927 -0.0033265848 -0.00332763 -0.0033323169 -0.0033358871 -0.0033382468 -0.0033404185 -0.0033425498 -0.0033449938 -0.0033469868 -0.0033504511 -0.0033542165 -0.003358874 -0.0033625516 -0.0033652994][-0.0033670638 -0.0033632698 -0.0033627748 -0.0033648796 -0.0033668471 -0.0033678233 -0.0033681318 -0.0033675916 -0.003367166 -0.0033667425 -0.0033670648 -0.0033683246 -0.0033698522 -0.0033703782 -0.0033695356]]...]
INFO - root - 2017-12-09 18:48:18.186158: step 50310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:49m:00s remains)
INFO - root - 2017-12-09 18:48:26.910620: step 50320, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:45m:16s remains)
INFO - root - 2017-12-09 18:48:35.562843: step 50330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 68h:43m:58s remains)
INFO - root - 2017-12-09 18:48:43.973553: step 50340, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:50m:26s remains)
INFO - root - 2017-12-09 18:48:52.375467: step 50350, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 66h:18m:53s remains)
INFO - root - 2017-12-09 18:49:01.134275: step 50360, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 71h:17m:37s remains)
INFO - root - 2017-12-09 18:49:09.780731: step 50370, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 68h:42m:01s remains)
INFO - root - 2017-12-09 18:49:18.235170: step 50380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:39m:11s remains)
INFO - root - 2017-12-09 18:49:26.762905: step 50390, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 64h:35m:59s remains)
INFO - root - 2017-12-09 18:49:35.383921: step 50400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:38m:41s remains)
2017-12-09 18:49:36.262551: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0039571412 0.0027567351 0.0021263172 0.0018094701 0.0017935683 0.001686774 0.0016761802 0.0023253758 0.0029531245 0.0038860559 0.0049477229 0.00621407 0.0072135944 0.0077379514 0.0081342608][0.0039467989 0.0033232907 0.003028637 0.0029549755 0.0030434474 0.0028519665 0.0024158817 0.00258149 0.0027662998 0.0030143934 0.0033188008 0.0036426655 0.00385361 0.0035838883 0.0030551704][0.0019808968 0.0017331874 0.0016219851 0.0016070076 0.0016837919 0.0012801567 0.0006806578 0.0006069534 0.0006561128 0.00079054968 0.0010605652 0.0011259576 0.00096620363 0.00036698789 -0.00036434736][0.00081761531 0.00072327093 0.0006611601 0.0006326891 0.00065085455 0.00051186024 0.00038301316 0.00041804579 0.00043281517 0.00039815134 0.00020764046 -9.3127368e-05 -0.00054069958 -0.0010527815 -0.001485654][0.0031460843 0.0031207653 0.0030855706 0.0030287209 0.0030307926 0.0027615095 0.0025111104 0.0021814809 0.0015347071 0.00079044397 4.390697e-05 -0.00056151743 -0.0011767524 -0.0017411435 -0.0020628101][0.0017816054 0.0017836096 0.0020778223 0.0024500608 0.0025935031 0.0023011968 0.0020332618 0.0014664752 0.00055830344 -0.00018729991 -0.00069190841 -0.00065511069 -0.00069922069 -0.00046390155 -0.00010473654][0.0048239492 0.0045883572 0.0045460453 0.0046819197 0.0044998219 0.0041411677 0.0038000687 0.0033225531 0.0027211003 0.0022555904 0.0020005365 0.0019022964 0.0018427328 0.0017774457 0.001746326][0.0055643655 0.0057416502 0.0059723323 0.0060374271 0.0058212625 0.0054469304 0.0049566114 0.0043884618 0.0036475125 0.0031287742 0.0025901713 0.0022002703 0.0017353424 0.0012552307 0.00089156465][0.0046878317 0.0048892843 0.0051699085 0.0053595537 0.0054103024 0.0054543391 0.0054567168 0.0051047057 0.00449163 0.0040628035 0.0035726351 0.0029568749 0.0023384623 0.0019559262 0.0020030502][0.0091748415 0.0088276556 0.0083516482 0.00829276 0.0084523037 0.0086617908 0.0089086108 0.0087689841 0.0082611479 0.0075239306 0.0067985496 0.0062951269 0.0059435088 0.0062351488 0.0072148275][0.018596588 0.017845448 0.016658064 0.016322002 0.016511858 0.016850712 0.017724451 0.018356489 0.018364826 0.017515261 0.01656756 0.015638057 0.015097689 0.015598463 0.017163992][0.030731736 0.030455491 0.029544761 0.028723983 0.028211934 0.027697481 0.027613683 0.027574491 0.027371401 0.026853608 0.02628758 0.025910882 0.02594885 0.026604934 0.028163994][0.03904723 0.038609181 0.037536081 0.036682051 0.036035564 0.035482418 0.035225034 0.034875926 0.034568384 0.034089658 0.0336878 0.03335683 0.033444498 0.034177516 0.03547135][0.043822065 0.043133419 0.04209796 0.04122711 0.040404703 0.039574567 0.039004643 0.038644407 0.038416 0.038226675 0.038194727 0.038353268 0.038619798 0.0389652 0.039550662][0.043650281 0.0433753 0.042735539 0.042166494 0.04134934 0.040493414 0.039754085 0.039173692 0.038716353 0.038455792 0.03854841 0.038793005 0.039116547 0.039468061 0.039667748]]...]
INFO - root - 2017-12-09 18:49:44.690516: step 50410, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 64h:44m:45s remains)
INFO - root - 2017-12-09 18:49:53.333947: step 50420, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 70h:27m:28s remains)
INFO - root - 2017-12-09 18:50:01.905747: step 50430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:35m:03s remains)
INFO - root - 2017-12-09 18:50:10.267188: step 50440, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 64h:39m:40s remains)
INFO - root - 2017-12-09 18:50:18.733780: step 50450, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:23m:54s remains)
INFO - root - 2017-12-09 18:50:27.170462: step 50460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:00m:28s remains)
INFO - root - 2017-12-09 18:50:35.768965: step 50470, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 65h:31m:30s remains)
INFO - root - 2017-12-09 18:50:44.196195: step 50480, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 67h:51m:57s remains)
INFO - root - 2017-12-09 18:50:52.830505: step 50490, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 67h:54m:55s remains)
INFO - root - 2017-12-09 18:51:01.631694: step 50500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:55m:11s remains)
2017-12-09 18:51:02.632688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032559067 -0.0027119929 -0.0012442358 0.0017093921 0.0065900069 0.01342527 0.021370741 0.029125249 0.034989092 0.037471145 0.035597842 0.030196635 0.022933438 0.015392931 0.0086813094][-0.0030828789 -0.0025855061 -0.00099591888 0.0024886949 0.0088092778 0.018513128 0.030770833 0.043404117 0.053057935 0.057108298 0.054546356 0.046811063 0.036357205 0.025234343 0.015247609][-0.0024115806 -0.0020729061 -0.00050608604 0.0035385748 0.011728557 0.025136871 0.042677846 0.060688339 0.0739044 0.078809246 0.074925005 0.064598024 0.050965711 0.036353696 0.023018368][-0.001127359 -0.00092928857 0.00071332208 0.0056713456 0.01632995 0.034090847 0.05730198 0.080514781 0.096582353 0.10132219 0.095115945 0.081462316 0.064263642 0.046148747 0.029730132][0.00059948792 0.0008445459 0.002874661 0.0091225654 0.022473272 0.044369329 0.072254986 0.099124312 0.11658127 0.12030388 0.11149736 0.094608761 0.07409364 0.052947428 0.033983629][0.0024004311 0.0028604052 0.0055441866 0.013303492 0.029211821 0.054367639 0.085188448 0.1135809 0.13062328 0.1323933 0.12088735 0.1011268 0.077876583 0.054574635 0.034147412][0.003980563 0.0047058892 0.0080397464 0.017062081 0.03471195 0.061733484 0.093704194 0.12193801 0.13755892 0.13718258 0.12324788 0.10101441 0.075611375 0.05098971 0.030239055][0.0047112172 0.0058733076 0.00971673 0.019380383 0.037576023 0.064731218 0.096046858 0.12291957 0.13698003 0.13512373 0.11967617 0.0958021 0.068901516 0.043750908 0.023695018][0.0038233304 0.0052582156 0.0093817338 0.019048136 0.036544334 0.06217774 0.091458462 0.11642781 0.1293119 0.1270052 0.11143696 0.087298349 0.060090803 0.035241816 0.016545443][0.0022858472 0.0034715526 0.0070414608 0.015565298 0.031107688 0.0539551 0.080265455 0.1030953 0.11544411 0.11395296 0.099869706 0.077135846 0.051065803 0.027484452 0.010599827][0.0010017667 0.0017606376 0.0043150624 0.010942889 0.023467852 0.042323887 0.064735785 0.085107677 0.097235166 0.097510941 0.086346038 0.066677883 0.043252315 0.021806872 0.0068585938][8.1680715e-05 0.00068438356 0.0022970263 0.0067534856 0.015682017 0.029864347 0.047590643 0.064796247 0.076421894 0.078780733 0.071361512 0.055978019 0.036500592 0.018075036 0.0050947024][-0.001132014 -0.00049261539 0.0006688626 0.0035400402 0.009335394 0.018876389 0.031455997 0.044708379 0.054921329 0.058829986 0.055172224 0.044629876 0.029927528 0.015181737 0.0043096608][-0.0021573098 -0.0017538021 -0.000992971 0.00076786638 0.0042045824 0.010063825 0.018202385 0.027395805 0.035305273 0.039572749 0.038711671 0.032582019 0.022736693 0.011980935 0.0034628923][-0.0027745645 -0.00261498 -0.0022729123 -0.0014258262 0.00034385989 0.0034806191 0.00803891 0.013659762 0.019175302 0.023055656 0.023898356 0.021103296 0.015318058 0.008264686 0.0022077144]]...]
INFO - root - 2017-12-09 18:51:11.075635: step 50510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:50m:43s remains)
INFO - root - 2017-12-09 18:51:19.717504: step 50520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:03m:56s remains)
INFO - root - 2017-12-09 18:51:28.503611: step 50530, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 70h:11m:29s remains)
INFO - root - 2017-12-09 18:51:37.011937: step 50540, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.703 sec/batch; 55h:01m:40s remains)
INFO - root - 2017-12-09 18:51:45.349541: step 50550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:14m:39s remains)
INFO - root - 2017-12-09 18:51:53.967158: step 50560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:13m:33s remains)
INFO - root - 2017-12-09 18:52:02.586662: step 50570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:29m:55s remains)
INFO - root - 2017-12-09 18:52:11.080823: step 50580, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 67h:19m:23s remains)
INFO - root - 2017-12-09 18:52:19.534322: step 50590, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:20m:12s remains)
INFO - root - 2017-12-09 18:52:28.164427: step 50600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:09m:36s remains)
2017-12-09 18:52:29.046391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033750641 -0.0032706729 -0.0026707896 -0.00099005387 0.0019999433 0.0056576319 0.0086275125 0.0095836641 0.0080596684 0.0048667593 0.0015813049 -0.00079165329 -0.0021966584 -0.0030041896 -0.0033240803][-0.0033609569 -0.0031216175 -0.0020515139 0.000695538 0.0054100603 0.011160578 0.015958015 0.017727222 0.015683822 0.010831907 0.0054692058 0.0012343314 -0.0014143034 -0.0027956385 -0.0032923147][-0.002962054 -0.002509187 -0.00027282396 0.0050622188 0.013723936 0.024083689 0.032824326 0.036471147 0.033591345 0.025544811 0.015824394 0.0073133623 0.0014167635 -0.0018156564 -0.0030811985][-0.0016314809 -0.00042720092 0.0041453727 0.014105508 0.029438788 0.047246456 0.062101983 0.068527587 0.064189158 0.05100495 0.034122795 0.018433593 0.0069015371 0.00027875649 -0.0025368345][0.00036547706 0.0033217387 0.011822174 0.028358558 0.052411146 0.079408146 0.1014676 0.11092409 0.10440975 0.0844992 0.058259074 0.033207212 0.014314832 0.0031934951 -0.0017440049][0.0024253738 0.0077973129 0.021095729 0.044885326 0.077909619 0.11386941 0.14270186 0.15476015 0.14576787 0.1189059 0.083065249 0.0484225 0.021999449 0.00625249 -0.0009024865][0.004110978 0.011593076 0.028818665 0.058195867 0.0979084 0.14022389 0.17357504 0.18705757 0.17593259 0.14385134 0.10091176 0.05924318 0.02736274 0.0083255125 -0.00036523794][0.0052041784 0.013760833 0.032726292 0.064256236 0.10618713 0.15023112 0.18458377 0.19811103 0.1860421 0.15204604 0.10653248 0.062374428 0.028671527 0.0086821485 -0.000345123][0.0052988064 0.013726687 0.031792045 0.061437666 0.10061067 0.14148644 0.17320742 0.18540704 0.17375223 0.1415371 0.098547712 0.057015393 0.025559764 0.0072200634 -0.00083985482][0.0042361151 0.01143813 0.026542477 0.05118189 0.083646223 0.11746364 0.14368966 0.15356152 0.14344972 0.11608431 0.079851575 0.045173902 0.0192848 0.00458756 -0.001609573][0.0025014903 0.0078017488 0.018802037 0.036866836 0.060775697 0.085756883 0.10508378 0.11208507 0.10408986 0.083280556 0.056177426 0.030639367 0.011961673 0.0017231547 -0.0023607824][0.0005763059 0.0039929422 0.010975944 0.022566114 0.038000867 0.054211736 0.066703856 0.07098183 0.065312408 0.051307023 0.033499446 0.017101444 0.0054516764 -0.000647841 -0.0029106452][-0.0012097422 0.00073329383 0.0046203956 0.011106042 0.019759798 0.02886861 0.035805106 0.037958693 0.034383174 0.026154807 0.016036298 0.0070084836 0.00084848912 -0.0021879962 -0.0032136324][-0.0025006568 -0.001589352 0.00023525278 0.0033261783 0.0074807731 0.011853699 0.015101394 0.015943505 0.013968099 0.0098298788 0.0049831821 0.00086723664 -0.0017760915 -0.0029766283 -0.0033367982][-0.0031523488 -0.0028474904 -0.0021798229 -0.00099580013 0.00062149647 0.0023268901 0.0035588231 0.0038046993 0.002922063 0.0012371496 -0.000612312 -0.0020681776 -0.0029245235 -0.0032744405 -0.0033669712]]...]
INFO - root - 2017-12-09 18:52:37.488885: step 50610, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 64h:54m:33s remains)
INFO - root - 2017-12-09 18:52:46.030051: step 50620, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:18m:36s remains)
INFO - root - 2017-12-09 18:52:54.651408: step 50630, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:29m:01s remains)
INFO - root - 2017-12-09 18:53:03.399031: step 50640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:18m:53s remains)
INFO - root - 2017-12-09 18:53:11.634745: step 50650, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 65h:58m:38s remains)
INFO - root - 2017-12-09 18:53:20.005434: step 50660, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:19m:26s remains)
INFO - root - 2017-12-09 18:53:28.514998: step 50670, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:58m:07s remains)
INFO - root - 2017-12-09 18:53:36.947324: step 50680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 67h:54m:09s remains)
INFO - root - 2017-12-09 18:53:45.590551: step 50690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:08m:01s remains)
INFO - root - 2017-12-09 18:53:54.327725: step 50700, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.851 sec/batch; 66h:37m:31s remains)
2017-12-09 18:53:55.173265: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.064918369 0.063519962 0.061435029 0.058614016 0.055844411 0.053138763 0.050598845 0.048164297 0.045579236 0.043285042 0.041214347 0.040514458 0.03995866 0.039199945 0.038461566][0.067059837 0.065898813 0.062914655 0.059441939 0.055994947 0.052754693 0.049975753 0.047531519 0.044577524 0.041768197 0.039378583 0.038837895 0.0386514 0.038307793 0.037376836][0.066658266 0.065431841 0.062329322 0.058271177 0.05395944 0.050051566 0.046949368 0.044266768 0.041064531 0.037866727 0.035335176 0.035011258 0.035371847 0.035306651 0.034373008][0.065894462 0.064977586 0.06178616 0.057138506 0.052315161 0.047862876 0.044245955 0.04108778 0.037496086 0.03415475 0.031766787 0.031568807 0.032002844 0.032261882 0.031519104][0.064676441 0.064566508 0.061232083 0.056207631 0.05069048 0.045696158 0.041771255 0.038051292 0.0339629 0.030264543 0.028016487 0.028007979 0.029000593 0.029363828 0.028491089][0.063150287 0.063621245 0.059987627 0.054618917 0.048686251 0.043280989 0.038956758 0.034761004 0.030438801 0.026388818 0.024079142 0.024413271 0.02568423 0.026528947 0.025838107][0.060452238 0.06142047 0.057712816 0.052038755 0.045654003 0.040007137 0.035572007 0.031300515 0.026711192 0.022591284 0.020468874 0.020909125 0.022429729 0.023468114 0.022908311][0.05676638 0.058274694 0.054382913 0.048686855 0.042216975 0.036455594 0.031793583 0.027637236 0.023449022 0.019490719 0.01729691 0.017539904 0.018944485 0.019936513 0.019497713][0.051363118 0.053523153 0.049867339 0.044362511 0.038133882 0.032723021 0.028174197 0.024102844 0.020117048 0.016614404 0.014628336 0.014499547 0.015290746 0.015915496 0.015442731][0.045308072 0.047423389 0.043786023 0.038688928 0.033119082 0.028281374 0.024164172 0.020606494 0.017139481 0.01401531 0.012042059 0.011591078 0.011916204 0.012065253 0.011337363][0.038548831 0.040615141 0.037008129 0.032311626 0.027271211 0.023112642 0.019539881 0.016563395 0.013852125 0.011333441 0.0095677273 0.0088275624 0.0086807637 0.008322305 0.0072658658][0.032239482 0.033536632 0.030136975 0.025948552 0.021582816 0.018025853 0.014932754 0.012579998 0.01057421 0.0087244594 0.0073256828 0.0065071043 0.0060949279 0.0053152358 0.0039163427][0.026655504 0.027537592 0.023916082 0.019859415 0.016035451 0.013108147 0.010724776 0.0090796864 0.00793651 0.0068204794 0.005777996 0.0048763761 0.0041965041 0.0031281488 0.0015533327][0.022423163 0.02236056 0.018553372 0.014645878 0.011080871 0.0086531518 0.007120423 0.0062796529 0.0059383651 0.0055169938 0.0049590748 0.0041269632 0.0032106664 0.0018836318 0.00015450176][0.020413231 0.01957491 0.01518745 0.011174983 0.0076869745 0.0054964442 0.0045884755 0.0045403754 0.0048540253 0.0050149495 0.0048753014 0.0042642513 0.0032754315 0.0018028517 -0.00010697101]]...]
INFO - root - 2017-12-09 18:54:03.783716: step 50710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:10m:04s remains)
INFO - root - 2017-12-09 18:54:12.367823: step 50720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:14m:52s remains)
INFO - root - 2017-12-09 18:54:20.800931: step 50730, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 65h:45m:50s remains)
INFO - root - 2017-12-09 18:54:29.230392: step 50740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:32m:21s remains)
INFO - root - 2017-12-09 18:54:37.489299: step 50750, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 70h:16m:17s remains)
INFO - root - 2017-12-09 18:54:46.117941: step 50760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:05m:50s remains)
INFO - root - 2017-12-09 18:54:54.773174: step 50770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:44m:50s remains)
INFO - root - 2017-12-09 18:55:03.429850: step 50780, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 65h:54m:49s remains)
INFO - root - 2017-12-09 18:55:12.142082: step 50790, loss = 0.91, batch loss = 0.70 (9.6 examples/sec; 0.833 sec/batch; 65h:11m:37s remains)
INFO - root - 2017-12-09 18:55:20.886427: step 50800, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 67h:08m:44s remains)
2017-12-09 18:55:21.821378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033807906 -0.0033794523 -0.0033795312 -0.0033796735 -0.0033798234 -0.0033797428 -0.003379409 -0.0033789885 -0.0033787517 -0.0033787689 -0.003378974 -0.0033791799 -0.0033794248 -0.0033796122 -0.0033797221][-0.0033792665 -0.0033779352 -0.0033782097 -0.0033787522 -0.0033793477 -0.0033795414 -0.0033791044 -0.0033783857 -0.0033778567 -0.0033776388 -0.0033775072 -0.0033775042 -0.0033778073 -0.0033780287 -0.0033781985][-0.0033794865 -0.0033783312 -0.0033791224 -0.003380548 -0.0033819932 -0.0033828048 -0.0033823552 -0.0033812651 -0.0033802972 -0.0033793084 -0.0033784644 -0.0033779643 -0.0033780052 -0.0033781151 -0.003378286][-0.0033796714 -0.0033789335 -0.0033807086 -0.0033834276 -0.0033860079 -0.00338753 -0.0033869608 -0.0033851878 -0.0033831985 -0.0033813254 -0.0033797307 -0.0033787005 -0.0033783293 -0.0033781205 -0.003378131][-0.0033800183 -0.0033798164 -0.0033825429 -0.0033865536 -0.0033900088 -0.0033919769 -0.0033914289 -0.0033888831 -0.0033857694 -0.0033831431 -0.0033812576 -0.0033798423 -0.003378822 -0.0033781496 -0.0033778041][-0.0033805629 -0.0033808644 -0.0033844949 -0.0033898281 -0.0033940033 -0.003396458 -0.0033962962 -0.0033933774 -0.0033891464 -0.0033854118 -0.0033827883 -0.0033808118 -0.0033791396 -0.0033780942 -0.0033774734][-0.00338102 -0.0033819133 -0.0033865855 -0.0033929355 -0.0033975476 -0.0034003204 -0.0034007623 -0.003398122 -0.0033930994 -0.0033881806 -0.0033843073 -0.0033813557 -0.0033791626 -0.0033775927 -0.0033766315][-0.0033812188 -0.0033825396 -0.003388013 -0.0033949488 -0.0033998261 -0.0034028103 -0.0034038406 -0.0034019153 -0.0033970475 -0.0033910035 -0.0033856907 -0.0033817065 -0.0033788902 -0.0033768206 -0.0033756564][-0.0033811836 -0.0033827231 -0.0033885157 -0.0033952279 -0.0033999868 -0.0034029707 -0.0034046292 -0.0034034604 -0.0033991074 -0.0033927811 -0.0033867522 -0.0033821345 -0.0033787207 -0.003376161 -0.0033748453][-0.0033809473 -0.003382209 -0.0033875145 -0.0033936719 -0.0033981898 -0.0034011763 -0.0034028247 -0.0034022275 -0.0033986003 -0.0033927413 -0.0033868444 -0.0033822551 -0.0033787689 -0.0033761642 -0.0033748287][-0.0033803927 -0.0033812514 -0.0033858239 -0.0033912717 -0.0033956342 -0.003398546 -0.0034000531 -0.0033994424 -0.0033960831 -0.0033909343 -0.0033857112 -0.0033814816 -0.0033784087 -0.0033761912 -0.0033749877][-0.003379771 -0.003379965 -0.0033834076 -0.0033878207 -0.0033919739 -0.0033947034 -0.0033958759 -0.003395234 -0.0033924228 -0.0033880421 -0.003383683 -0.00338015 -0.0033777836 -0.0033761219 -0.0033751326][-0.0033794409 -0.0033788451 -0.0033812143 -0.003384422 -0.0033879594 -0.0033902917 -0.0033911548 -0.0033906556 -0.0033886221 -0.0033851727 -0.0033816502 -0.0033790807 -0.0033774101 -0.0033762057 -0.0033755361][-0.003379277 -0.0033779256 -0.0033793312 -0.0033813992 -0.003383857 -0.0033855643 -0.003386287 -0.0033859946 -0.0033846805 -0.0033824223 -0.0033802052 -0.0033786108 -0.0033774562 -0.0033766234 -0.0033761971][-0.0033792229 -0.0033773528 -0.0033779903 -0.0033790881 -0.0033804788 -0.00338154 -0.0033820265 -0.0033819277 -0.0033812628 -0.0033800977 -0.0033789671 -0.0033781515 -0.0033775067 -0.0033770087 -0.0033767195]]...]
INFO - root - 2017-12-09 18:55:30.436706: step 50810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:11m:37s remains)
INFO - root - 2017-12-09 18:55:39.187155: step 50820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:17m:06s remains)
INFO - root - 2017-12-09 18:55:47.748599: step 50830, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 64h:54m:11s remains)
INFO - root - 2017-12-09 18:55:56.380941: step 50840, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:50m:43s remains)
INFO - root - 2017-12-09 18:56:04.645217: step 50850, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 64h:43m:38s remains)
INFO - root - 2017-12-09 18:56:13.159695: step 50860, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:58m:39s remains)
INFO - root - 2017-12-09 18:56:21.834685: step 50870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:40m:16s remains)
INFO - root - 2017-12-09 18:56:30.354750: step 50880, loss = 0.90, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 58h:20m:24s remains)
INFO - root - 2017-12-09 18:56:39.122278: step 50890, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 69h:12m:12s remains)
INFO - root - 2017-12-09 18:56:47.809731: step 50900, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:55m:06s remains)
2017-12-09 18:56:48.681850: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20591283 0.20454329 0.20146029 0.19255605 0.18040246 0.16521317 0.14868625 0.13253401 0.11841653 0.11039277 0.10722986 0.1123061 0.12289278 0.13646013 0.15225738][0.23431523 0.23832208 0.24139586 0.23792465 0.22909026 0.21558081 0.19921042 0.18228826 0.16664883 0.15759951 0.15375619 0.15804994 0.16756588 0.18047021 0.19483377][0.26207247 0.27190143 0.28048152 0.28321263 0.2806989 0.27110747 0.25755003 0.24219008 0.22794631 0.21859999 0.21326126 0.2150982 0.22093779 0.22916667 0.23831587][0.28969 0.30497149 0.31917313 0.32851103 0.33214897 0.32755992 0.31906945 0.30661887 0.29412085 0.2850211 0.27865198 0.27715349 0.27718684 0.27861097 0.27995762][0.31118906 0.33201927 0.35114598 0.36734894 0.3774572 0.37927362 0.37588415 0.36748591 0.35729507 0.3473593 0.33916095 0.33330163 0.32733241 0.32070115 0.31362349][0.32663757 0.35197318 0.37544385 0.39643198 0.41205463 0.41983154 0.42211902 0.4171223 0.40829512 0.39714804 0.38600704 0.37489539 0.36226743 0.34830126 0.33298004][0.33868942 0.36739558 0.39247689 0.41590968 0.43506077 0.44762194 0.45457244 0.45264518 0.444789 0.43174306 0.41670698 0.39964667 0.38061565 0.35985148 0.33769113][0.35182607 0.38229772 0.40686572 0.42981258 0.44891828 0.4627226 0.47119176 0.47017461 0.46161738 0.4464002 0.42798048 0.40673241 0.38347211 0.3578791 0.33155969][0.36930579 0.39875636 0.42064214 0.44147357 0.45816383 0.47103888 0.47861108 0.47785589 0.46844044 0.45105049 0.43029448 0.40589678 0.37944746 0.35019886 0.32061023][0.38654098 0.41509324 0.43400756 0.45159724 0.46598476 0.47721067 0.48289171 0.48128152 0.47097018 0.45195636 0.42842475 0.40062797 0.37135038 0.33958447 0.30768302][0.40190747 0.42802957 0.44232941 0.45622796 0.46750164 0.47761506 0.48300469 0.48127937 0.47069529 0.45077431 0.42568278 0.3949458 0.36276832 0.32962972 0.29679456][0.40818247 0.43108752 0.44076827 0.45112178 0.459365 0.46724859 0.4705987 0.46846712 0.45816016 0.43830377 0.41287422 0.38128662 0.34814298 0.31405181 0.28021434][0.41110089 0.4313004 0.43756944 0.44457623 0.45018584 0.45519471 0.45554143 0.45182315 0.44063807 0.42105433 0.39597696 0.36557835 0.33388251 0.30087525 0.26845178][0.40888247 0.4266994 0.43059948 0.43530303 0.43941596 0.44254458 0.4407444 0.43497869 0.42261666 0.40257898 0.37752295 0.34832931 0.31907871 0.28916571 0.25973785][0.40430829 0.42159393 0.42504138 0.4281342 0.4308331 0.43138519 0.42665437 0.41737112 0.40202123 0.38084081 0.35592076 0.32901654 0.30353737 0.27853808 0.25432044]]...]
INFO - root - 2017-12-09 18:56:57.296852: step 50910, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 67h:39m:57s remains)
INFO - root - 2017-12-09 18:57:05.900694: step 50920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:53m:00s remains)
INFO - root - 2017-12-09 18:57:14.409630: step 50930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:42m:24s remains)
INFO - root - 2017-12-09 18:57:23.011215: step 50940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:24m:11s remains)
INFO - root - 2017-12-09 18:57:31.391909: step 50950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:53m:16s remains)
INFO - root - 2017-12-09 18:57:40.016827: step 50960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:09m:17s remains)
INFO - root - 2017-12-09 18:57:48.523771: step 50970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 66h:57m:02s remains)
INFO - root - 2017-12-09 18:57:57.240418: step 50980, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.867 sec/batch; 67h:47m:07s remains)
INFO - root - 2017-12-09 18:58:05.827733: step 50990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:16m:29s remains)
INFO - root - 2017-12-09 18:58:14.580455: step 51000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:34m:03s remains)
2017-12-09 18:58:15.406693: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12906115 0.11679375 0.10311422 0.08852759 0.07466504 0.061639354 0.050822739 0.043274213 0.037654113 0.034394924 0.03206446 0.031751681 0.03150662 0.029213699 0.026108423][0.18805279 0.17502932 0.15913695 0.14074327 0.12261987 0.10522088 0.090129137 0.07772138 0.068305038 0.062086083 0.057234019 0.054266777 0.051772047 0.048152905 0.043828238][0.25740451 0.24408334 0.22655441 0.20715816 0.18752235 0.16850573 0.15126598 0.13682707 0.12548317 0.1159712 0.10812738 0.1020618 0.096927315 0.090971492 0.084376939][0.33630139 0.32150066 0.30185136 0.2806077 0.25908825 0.23950471 0.22249633 0.20887752 0.19820718 0.18781428 0.17855561 0.16958898 0.16139156 0.15293744 0.143783][0.41284057 0.39934388 0.38096824 0.36098289 0.34026814 0.32138222 0.30472472 0.29153508 0.28083479 0.26976097 0.25936404 0.24796942 0.2369519 0.22616507 0.21360186][0.47082227 0.46177113 0.44758868 0.43157479 0.41439575 0.3986336 0.38420942 0.37272325 0.3625412 0.35096434 0.33924308 0.326091 0.31297073 0.29996741 0.28415993][0.50036204 0.4979018 0.49008065 0.48022234 0.46864659 0.4577466 0.44716072 0.43848297 0.4296101 0.41824055 0.40581512 0.39143869 0.37649548 0.36117 0.3429639][0.50780416 0.51201719 0.51024544 0.50622195 0.49984384 0.49300966 0.48572305 0.47983181 0.47256404 0.46205139 0.44966394 0.4349497 0.41894609 0.40190837 0.38203436][0.49451098 0.5051946 0.5089398 0.50920737 0.50649661 0.50182927 0.49620858 0.49096796 0.48381755 0.47393262 0.46194622 0.44796187 0.4323535 0.4153679 0.39553368][0.45915946 0.47543207 0.48329434 0.48626366 0.48545945 0.4802981 0.47364265 0.46628493 0.4572176 0.44665545 0.43460631 0.42213079 0.40842703 0.39354473 0.37606639][0.40022555 0.41934794 0.42964125 0.43460009 0.43501148 0.42963013 0.42202637 0.41255143 0.4015902 0.38975525 0.37749514 0.36610308 0.35414362 0.341502 0.32694703][0.32037711 0.33837798 0.34873095 0.35412022 0.35499209 0.34996089 0.34251782 0.33290544 0.32225206 0.3108913 0.29972559 0.28980088 0.27991372 0.26979262 0.25853258][0.23404324 0.24836549 0.25690481 0.26125225 0.26171684 0.25690687 0.24987596 0.24085395 0.23135819 0.22179466 0.21292269 0.20544082 0.19831267 0.19107573 0.18297097][0.15213102 0.16208676 0.1682957 0.17131938 0.17139952 0.16727503 0.16131032 0.15366377 0.14603612 0.13874641 0.13245787 0.12748848 0.12295887 0.11841781 0.11328588][0.085645638 0.091673516 0.095622428 0.097558893 0.0975007 0.094586544 0.090325244 0.0848309 0.079551347 0.074618123 0.070620239 0.0676114 0.064997837 0.062385604 0.059466809]]...]
INFO - root - 2017-12-09 18:58:24.072239: step 51010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:55m:27s remains)
INFO - root - 2017-12-09 18:58:32.736933: step 51020, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 70h:12m:01s remains)
INFO - root - 2017-12-09 18:58:41.311670: step 51030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:40m:12s remains)
INFO - root - 2017-12-09 18:58:50.079503: step 51040, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:15m:27s remains)
INFO - root - 2017-12-09 18:58:58.365042: step 51050, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:01m:49s remains)
INFO - root - 2017-12-09 18:59:06.914940: step 51060, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:25m:17s remains)
INFO - root - 2017-12-09 18:59:15.559038: step 51070, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.894 sec/batch; 69h:51m:55s remains)
INFO - root - 2017-12-09 18:59:24.332039: step 51080, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 69h:26m:16s remains)
INFO - root - 2017-12-09 18:59:32.945768: step 51090, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 64h:48m:38s remains)
INFO - root - 2017-12-09 18:59:41.528655: step 51100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 68h:58m:39s remains)
2017-12-09 18:59:42.425281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033608889 -0.0033579217 -0.003357748 -0.003357819 -0.0033575704 -0.0033571844 -0.0033563327 -0.0033550963 -0.0033538677 -0.0033529738 -0.0033526367 -0.0033525878 -0.0033529582 -0.0033538381 -0.0033549329][-0.0033589525 -0.0033557666 -0.0033555208 -0.0033556186 -0.003355382 -0.0033549711 -0.0033540896 -0.0033528847 -0.0033517766 -0.0033509508 -0.003350649 -0.003350571 -0.0033509608 -0.0033519645 -0.00335316][-0.0033591632 -0.0033558731 -0.0033555825 -0.0033555813 -0.0033553231 -0.0033548351 -0.0033539685 -0.0033529005 -0.0033519641 -0.0033511971 -0.0033508153 -0.0033508975 -0.0033514497 -0.0033525554 -0.0033538216][-0.0033594398 -0.0033560887 -0.0033556174 -0.0033554067 -0.003354982 -0.0033543843 -0.0033534882 -0.003352494 -0.0033517047 -0.0033509829 -0.0033507068 -0.003350999 -0.00335183 -0.0033530081 -0.0033542817][-0.0033596866 -0.003356216 -0.0033555313 -0.0033550891 -0.0033543834 -0.0033535496 -0.0033525846 -0.0033515478 -0.0033508569 -0.0033503077 -0.0033502567 -0.003350762 -0.0033518539 -0.0033531322 -0.0033543406][-0.0033599464 -0.0033563359 -0.0033554332 -0.0033546528 -0.0033535569 -0.003352352 -0.0033511349 -0.00334997 -0.0033494339 -0.0033492057 -0.0033495056 -0.0033502944 -0.0033515343 -0.0033529084 -0.0033541417][-0.0033599916 -0.0033562982 -0.0033553159 -0.003354277 -0.0033528621 -0.0033512437 -0.003349633 -0.0033482786 -0.0033479747 -0.0033481556 -0.0033488213 -0.0033498916 -0.00335116 -0.003352517 -0.0033538023][-0.003359834 -0.0033561343 -0.0033551354 -0.0033540407 -0.0033524635 -0.0033505422 -0.0033487473 -0.0033472986 -0.0033469668 -0.0033473838 -0.0033484283 -0.0033497924 -0.0033510965 -0.0033524397 -0.00335368][-0.0033593317 -0.0033556309 -0.0033547094 -0.0033537741 -0.0033523552 -0.0033506404 -0.003349131 -0.0033479873 -0.0033474304 -0.0033476742 -0.0033487119 -0.0033501142 -0.003351395 -0.0033526877 -0.0033538432][-0.0033587606 -0.0033550884 -0.0033544442 -0.0033537091 -0.0033526181 -0.003351476 -0.0033506567 -0.0033500711 -0.0033495305 -0.0033494055 -0.0033500446 -0.0033511054 -0.0033521086 -0.0033531813 -0.0033541569][-0.0033583515 -0.0033547957 -0.0033545138 -0.0033540493 -0.0033534339 -0.0033529818 -0.0033527203 -0.0033525273 -0.003352117 -0.0033517804 -0.0033518921 -0.0033523473 -0.0033528493 -0.0033535748 -0.0033543569][-0.0033581995 -0.0033546065 -0.003354708 -0.0033547075 -0.0033545934 -0.003354643 -0.0033547112 -0.0033546074 -0.0033541629 -0.0033537296 -0.0033535133 -0.0033534362 -0.0033534486 -0.0033537489 -0.0033542255][-0.0033583327 -0.003354637 -0.0033548884 -0.0033551271 -0.0033553215 -0.0033556286 -0.0033558735 -0.0033558444 -0.0033554547 -0.0033549722 -0.0033545385 -0.0033540763 -0.0033536733 -0.0033535198 -0.0033536446][-0.003358487 -0.0033547794 -0.0033551813 -0.0033554507 -0.0033557322 -0.0033561629 -0.0033565338 -0.0033566079 -0.0033562838 -0.0033557005 -0.0033550328 -0.0033541773 -0.0033533352 -0.0033527359 -0.0033526036][-0.0033587446 -0.0033549306 -0.003355294 -0.0033554945 -0.0033557231 -0.0033560568 -0.0033564442 -0.0033565764 -0.0033563233 -0.0033557655 -0.0033549941 -0.0033538765 -0.0033527198 -0.0033517971 -0.003351449]]...]
INFO - root - 2017-12-09 18:59:50.947847: step 51110, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 68h:55m:51s remains)
INFO - root - 2017-12-09 18:59:59.568001: step 51120, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 64h:46m:32s remains)
INFO - root - 2017-12-09 19:00:08.246507: step 51130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:57m:06s remains)
INFO - root - 2017-12-09 19:00:16.882297: step 51140, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 68h:05m:02s remains)
INFO - root - 2017-12-09 19:00:25.225054: step 51150, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 62h:22m:44s remains)
INFO - root - 2017-12-09 19:00:33.973187: step 51160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:21m:27s remains)
INFO - root - 2017-12-09 19:00:42.703573: step 51170, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 70h:58m:04s remains)
INFO - root - 2017-12-09 19:00:51.379427: step 51180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:42m:00s remains)
INFO - root - 2017-12-09 19:00:59.939935: step 51190, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.921 sec/batch; 71h:58m:46s remains)
INFO - root - 2017-12-09 19:01:08.616035: step 51200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:57m:25s remains)
2017-12-09 19:01:09.514567: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068935715 0.076821946 0.082595445 0.086716436 0.087967575 0.083313018 0.076738611 0.067171156 0.060030483 0.053289972 0.047749914 0.043355368 0.04083652 0.03907565 0.037525039][0.078440346 0.087921493 0.094709352 0.099762648 0.10124603 0.0974041 0.091584735 0.082288116 0.073942363 0.066687725 0.060446694 0.054214373 0.049826868 0.046890255 0.044885017][0.094947934 0.10786557 0.11805212 0.12579292 0.13005289 0.12797856 0.12221634 0.11225023 0.10286319 0.095004141 0.087754704 0.081118666 0.076478489 0.072713494 0.06965936][0.12491216 0.14027523 0.15326226 0.16340604 0.16908944 0.16978508 0.16536973 0.15679756 0.14823274 0.14083543 0.1333061 0.12558684 0.12085939 0.11671329 0.11292144][0.16870633 0.18606921 0.20086135 0.21337068 0.22097646 0.2235616 0.22098717 0.21381253 0.20612219 0.19846091 0.19072312 0.18245932 0.17686462 0.1724254 0.16818862][0.22367187 0.24260662 0.25896147 0.27253705 0.28070721 0.28334045 0.28074631 0.2740145 0.26655647 0.25868022 0.25064278 0.24231371 0.23579675 0.23050496 0.22469646][0.27601475 0.29636931 0.31374121 0.32881659 0.33818492 0.3415454 0.33884564 0.33221966 0.32435846 0.31512061 0.30531749 0.29503837 0.28692266 0.28016648 0.27246812][0.32334405 0.34502265 0.3613534 0.37548506 0.38418356 0.38734117 0.38348529 0.37640062 0.36855584 0.35839489 0.34696367 0.33416441 0.32365477 0.31416035 0.30458632][0.35950494 0.38139635 0.39615208 0.40808067 0.41604561 0.41937962 0.4150942 0.40677485 0.39736012 0.38584131 0.37309125 0.35866216 0.34683833 0.33640769 0.32707122][0.38727662 0.41002369 0.42337596 0.43367592 0.4406946 0.4440681 0.43975842 0.43066162 0.41977683 0.40701473 0.39351013 0.37880212 0.36671588 0.35652933 0.34838894][0.40722138 0.43310541 0.44643047 0.4568077 0.46384516 0.46722448 0.46293682 0.45309213 0.440832 0.42686981 0.41287494 0.39832425 0.3865597 0.37734088 0.3709375][0.41913995 0.44707415 0.45975989 0.46985617 0.47670659 0.479827 0.47573686 0.46562925 0.45236427 0.43736163 0.42302814 0.4091922 0.39837623 0.39098254 0.38692778][0.42869654 0.45767909 0.46891618 0.47731593 0.48244983 0.48406604 0.47948265 0.46908814 0.45541507 0.44050372 0.42668083 0.41436055 0.40509155 0.39970368 0.3978126][0.41758066 0.44403553 0.45245373 0.45857868 0.4618119 0.46108702 0.45608923 0.4473618 0.43608797 0.42468524 0.41486883 0.40708876 0.40123805 0.39718118 0.39601529][0.4032577 0.42758673 0.43337452 0.43747509 0.43994403 0.43774188 0.43251479 0.42337641 0.41280079 0.40203974 0.39271715 0.386098 0.38106397 0.37790594 0.37723231]]...]
INFO - root - 2017-12-09 19:01:18.122868: step 51210, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:06m:06s remains)
INFO - root - 2017-12-09 19:01:26.836309: step 51220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 69h:02m:19s remains)
INFO - root - 2017-12-09 19:01:35.541001: step 51230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:27m:01s remains)
INFO - root - 2017-12-09 19:01:44.250402: step 51240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:40m:17s remains)
INFO - root - 2017-12-09 19:01:52.785901: step 51250, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 69h:04m:37s remains)
INFO - root - 2017-12-09 19:02:01.532072: step 51260, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:27m:22s remains)
INFO - root - 2017-12-09 19:02:10.161596: step 51270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:29m:45s remains)
INFO - root - 2017-12-09 19:02:18.781619: step 51280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:57m:17s remains)
INFO - root - 2017-12-09 19:02:27.076025: step 51290, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 64h:38m:28s remains)
INFO - root - 2017-12-09 19:02:35.598976: step 51300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:42m:02s remains)
2017-12-09 19:02:36.479994: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10848728 0.1219164 0.13411015 0.148374 0.16634023 0.18829314 0.21241711 0.23386134 0.2466124 0.24537191 0.22937758 0.20018479 0.16384722 0.12727404 0.095178373][0.11382715 0.13846846 0.16350108 0.1888961 0.21401221 0.23898907 0.26180145 0.27852333 0.28538549 0.27910396 0.25943917 0.22870228 0.1923268 0.1560912 0.12478121][0.12156418 0.15726213 0.19439471 0.23227867 0.26610059 0.29360011 0.31389356 0.32441217 0.32423419 0.31092387 0.286977 0.25546324 0.22057879 0.18707183 0.15857235][0.13588427 0.18013638 0.22653371 0.27378246 0.3137503 0.34379208 0.36290351 0.3692624 0.36496824 0.34866846 0.32430682 0.29398754 0.261067 0.2295194 0.20215261][0.16028534 0.20973182 0.26104358 0.31216431 0.35407618 0.38433972 0.401879 0.40570122 0.39915821 0.38197258 0.35927197 0.33303764 0.30559927 0.27871373 0.25424922][0.19348378 0.24570306 0.29706883 0.34693 0.38618848 0.4132517 0.42730877 0.42878664 0.42116308 0.40446767 0.38441211 0.36295116 0.34196362 0.32077208 0.29969174][0.23058367 0.28379452 0.33186555 0.37541893 0.40834662 0.42999831 0.43944752 0.43869752 0.43115509 0.41661534 0.39933342 0.38236871 0.36741677 0.35143372 0.33248571][0.2606101 0.31421131 0.35853177 0.39516628 0.42074519 0.43614593 0.44074702 0.4387053 0.43201014 0.41977373 0.40616071 0.39396492 0.38403168 0.37114441 0.35263303][0.27064189 0.32356963 0.36545324 0.39702174 0.41734928 0.42830175 0.43024939 0.42791376 0.42232043 0.41384465 0.40425938 0.39583069 0.389236 0.37820038 0.35899392][0.25489184 0.30526602 0.34417602 0.37259254 0.39106187 0.40137547 0.40440649 0.40479848 0.40246218 0.39738691 0.39048007 0.38376769 0.37837034 0.36689547 0.34607214][0.21556354 0.26088274 0.29646343 0.32313815 0.34143138 0.35396558 0.36092231 0.36694145 0.36989117 0.3694655 0.36555859 0.35911059 0.35167247 0.33673388 0.31260452][0.16400592 0.20118864 0.23203596 0.25667545 0.27556926 0.29125181 0.30461732 0.31843439 0.32855296 0.33399108 0.33353788 0.32745731 0.31670648 0.29698053 0.268724][0.11088111 0.138487 0.16296007 0.18423851 0.2030946 0.22163528 0.24062173 0.26092309 0.27769697 0.28884494 0.29232618 0.28817472 0.27632043 0.25416726 0.22393493][0.064674266 0.083103716 0.1005323 0.11748029 0.13473241 0.15397963 0.17576918 0.19904009 0.21909219 0.23311648 0.23923284 0.23668316 0.22523202 0.20404705 0.17592125][0.030311493 0.041018672 0.051964365 0.064027235 0.077929042 0.09515924 0.11591141 0.13839899 0.15794976 0.17171729 0.17818925 0.1763123 0.16584478 0.14697954 0.12314804]]...]
INFO - root - 2017-12-09 19:02:45.010365: step 51310, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 68h:53m:13s remains)
INFO - root - 2017-12-09 19:02:53.500774: step 51320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:08m:20s remains)
INFO - root - 2017-12-09 19:03:02.262082: step 51330, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:40m:19s remains)
INFO - root - 2017-12-09 19:03:10.956874: step 51340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:54m:34s remains)
INFO - root - 2017-12-09 19:03:19.172036: step 51350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:57m:02s remains)
INFO - root - 2017-12-09 19:03:27.607940: step 51360, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 64h:50m:08s remains)
INFO - root - 2017-12-09 19:03:36.143257: step 51370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:56m:00s remains)
INFO - root - 2017-12-09 19:03:44.726258: step 51380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:41m:58s remains)
INFO - root - 2017-12-09 19:03:53.312848: step 51390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:23m:28s remains)
INFO - root - 2017-12-09 19:04:02.033970: step 51400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:20m:38s remains)
2017-12-09 19:04:02.892905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034131736 -0.0034128278 -0.0034131943 -0.0034140528 -0.0034149259 -0.0034156169 -0.0034160861 -0.0034163399 -0.0034163569 -0.003416317 -0.0034161997 -0.0034159923 -0.0034156456 -0.0034150793 -0.0034145257][-0.0034141608 -0.0034140018 -0.003414453 -0.0034153261 -0.0034158651 -0.0034159031 -0.0034154195 -0.0034144744 -0.0034133471 -0.0034125745 -0.0034122912 -0.0034126751 -0.0034133324 -0.0034137731 -0.0034140551][-0.0034157615 -0.0034158335 -0.0034162863 -0.003416589 -0.0034160719 -0.0034143864 -0.0034118965 -0.0034090069 -0.0034062576 -0.0034045209 -0.0034043831 -0.003405795 -0.003408137 -0.0034103566 -0.0034121687][-0.0034171415 -0.0034174935 -0.0034178789 -0.0034174284 -0.0034152805 -0.0034109994 -0.0034054758 -0.0033999712 -0.0033955418 -0.0033932261 -0.0033935506 -0.0033968717 -0.003401842 -0.0034065384 -0.0034101515][-0.0034178845 -0.0034183953 -0.0034186672 -0.0034172838 -0.0034127973 -0.0034056534 -0.0033969865 -0.0033888558 -0.003382785 -0.0033801184 -0.0033814202 -0.0033866588 -0.0033941702 -0.0034014031 -0.0034072704][-0.0034176391 -0.0034180416 -0.0034184132 -0.0034160905 -0.00340942 -0.0033995898 -0.0033884512 -0.0033784169 -0.00337157 -0.0033686154 -0.0033706597 -0.0033775035 -0.0033873962 -0.0033967905 -0.0034043475][-0.0034168896 -0.003417474 -0.0034180717 -0.0034153529 -0.0034073922 -0.0033957944 -0.0033832914 -0.0033723251 -0.0033649658 -0.0033617981 -0.0033642389 -0.0033720687 -0.0033830411 -0.0033938221 -0.0034028068][-0.0034163638 -0.0034169806 -0.0034178535 -0.0034152439 -0.0034074974 -0.0033963411 -0.0033836316 -0.0033723302 -0.003365108 -0.0033626552 -0.0033655108 -0.003373222 -0.0033838477 -0.0033944687 -0.0034031041][-0.003415759 -0.0034165054 -0.0034180866 -0.0034167077 -0.0034103624 -0.0034006364 -0.0033890635 -0.003378497 -0.0033720145 -0.0033705565 -0.0033738005 -0.0033810588 -0.0033901655 -0.0033991234 -0.0034057465][-0.003415606 -0.0034169725 -0.0034191755 -0.0034188356 -0.0034143382 -0.0034068604 -0.0033970901 -0.0033879986 -0.0033826027 -0.0033818558 -0.0033853764 -0.0033914838 -0.0033986294 -0.0034050827 -0.0034093133][-0.0034158218 -0.0034177224 -0.0034198875 -0.0034200372 -0.0034171992 -0.0034116313 -0.0034045423 -0.0033979905 -0.0033942619 -0.0033939681 -0.0033970335 -0.0034013807 -0.0034062846 -0.0034098206 -0.0034117736][-0.0034154661 -0.0034172013 -0.0034193019 -0.0034200081 -0.0034184335 -0.0034145447 -0.0034097987 -0.0034056352 -0.0034034562 -0.0034039421 -0.0034061754 -0.0034090036 -0.0034116008 -0.0034130034 -0.0034132558][-0.0034149515 -0.0034162945 -0.0034179131 -0.0034187259 -0.00341795 -0.003415517 -0.0034124712 -0.0034098586 -0.0034089214 -0.0034096935 -0.0034112369 -0.0034130039 -0.0034140167 -0.0034141885 -0.0034135326][-0.0034139063 -0.0034147711 -0.0034159487 -0.0034166216 -0.0034162491 -0.0034148151 -0.0034130306 -0.0034116521 -0.0034112907 -0.0034120171 -0.0034128851 -0.0034137843 -0.0034141296 -0.003413866 -0.0034128963][-0.0034127668 -0.003413175 -0.0034139645 -0.0034146523 -0.0034146614 -0.0034137312 -0.0034123368 -0.0034111885 -0.0034109706 -0.0034115848 -0.003412355 -0.0034127985 -0.0034128635 -0.0034125755 -0.0034117706]]...]
INFO - root - 2017-12-09 19:04:11.460950: step 51410, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 70h:06m:44s remains)
INFO - root - 2017-12-09 19:04:20.006887: step 51420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:34m:21s remains)
INFO - root - 2017-12-09 19:04:28.631805: step 51430, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:03m:51s remains)
INFO - root - 2017-12-09 19:04:37.432185: step 51440, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:15m:43s remains)
INFO - root - 2017-12-09 19:04:45.804290: step 51450, loss = 0.89, batch loss = 0.68 (10.9 examples/sec; 0.735 sec/batch; 57h:21m:12s remains)
INFO - root - 2017-12-09 19:04:54.679549: step 51460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:34m:49s remains)
INFO - root - 2017-12-09 19:05:03.530823: step 51470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:02m:28s remains)
INFO - root - 2017-12-09 19:05:12.092323: step 51480, loss = 0.88, batch loss = 0.67 (9.5 examples/sec; 0.844 sec/batch; 65h:50m:46s remains)
INFO - root - 2017-12-09 19:05:20.599885: step 51490, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 67h:16m:33s remains)
INFO - root - 2017-12-09 19:05:29.309016: step 51500, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.922 sec/batch; 71h:56m:07s remains)
2017-12-09 19:05:30.166668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00046665571 -0.00041001081 -0.0004264575 -0.00043080444 -0.00043123867 -0.00042667636 -0.00041721412 -0.00039665587 -0.00038440223 -0.00035565905 -0.00032489398 -0.00033845846 -0.00036216341 -0.00040916842 -0.00045119156][0.0017634185 0.0018903182 0.0018751437 0.0018719488 0.0018576074 0.001843997 0.0018318726 0.001837685 0.0018191019 0.0018267485 0.0018436441 0.0018257096 0.0017910639 0.0017253321 0.0017070773][0.003309543 0.0035354195 0.0035796494 0.0035878646 0.0035509195 0.0034841981 0.0033963893 0.003312161 0.0032189677 0.0031475711 0.0031034008 0.0030566293 0.0029963071 0.0029083637 0.0028673348][0.0036889461 0.0039814441 0.0040634153 0.0041039959 0.0040600635 0.0039372211 0.0037522626 0.003553122 0.0033563373 0.0032025941 0.0031125064 0.003066222 0.0030379833 0.0029916477 0.0029797552][0.0032139521 0.003570229 0.0036999492 0.0037529177 0.0036948172 0.0035323624 0.0032793649 0.0029983472 0.0027375107 0.0025141567 0.0023963258 0.0023615567 0.0023861222 0.0024193733 0.002458133][0.0024407294 0.0027529842 0.0028820357 0.00294353 0.0029029262 0.0027418772 0.0024881491 0.0022054554 0.0019125447 0.0016735282 0.0015312179 0.0014770802 0.001522878 0.001594532 0.0016842152][0.00089646294 0.0011514558 0.0012853618 0.0013755194 0.0013864588 0.0012887216 0.0011111752 0.00089315162 0.00064050709 0.00040710578 0.00025097094 0.00019307551 0.00021402445 0.00024742167 0.00029890449][-0.0010029625 -0.000842409 -0.00071363291 -0.00060267304 -0.00054490543 -0.00057146954 -0.00066554383 -0.00080274255 -0.00099338009 -0.0011941954 -0.0013544788 -0.0014493158 -0.0014743509 -0.0014759875 -0.0014734594][-0.0026146939 -0.0025192881 -0.0024282136 -0.0023396143 -0.0022766269 -0.0022587467 -0.0022849736 -0.0023411037 -0.002437681 -0.0025567641 -0.0026655379 -0.0027435133 -0.0027842168 -0.0028070505 -0.0028140545][-0.0033325059 -0.00330472 -0.0032690414 -0.0032310076 -0.003200256 -0.0031838692 -0.0031850252 -0.003200789 -0.0032316556 -0.0032744457 -0.0033170134 -0.0033491524 -0.0033658219 -0.0033729495 -0.0033742464][-0.0033801307 -0.0033774208 -0.0033772481 -0.0033767312 -0.0033760569 -0.0033772478 -0.003378615 -0.0033791626 -0.003379703 -0.0033808877 -0.0033825305 -0.0033823082 -0.0033807233 -0.0033784849 -0.0033768381][-0.0033800914 -0.0033778348 -0.0033786017 -0.0033800942 -0.0033820819 -0.0033853336 -0.003388148 -0.0033902084 -0.0033905467 -0.0033889445 -0.0033865098 -0.00338382 -0.0033813538 -0.0033790087 -0.0033774832][-0.0033807885 -0.0033783827 -0.0033786357 -0.0033795014 -0.0033807005 -0.0033829128 -0.0033847657 -0.0033864048 -0.0033869932 -0.0033862153 -0.0033849606 -0.0033833277 -0.003381883 -0.0033799773 -0.0033788274][-0.0033816893 -0.0033790453 -0.0033788 -0.0033789598 -0.0033793724 -0.0033807345 -0.0033815794 -0.0033827065 -0.0033831871 -0.0033831466 -0.0033825906 -0.003382039 -0.0033811759 -0.0033798194 -0.0033789845][-0.0033832777 -0.0033809261 -0.0033804695 -0.0033799636 -0.0033796038 -0.0033797892 -0.0033797214 -0.003380195 -0.0033808129 -0.0033810732 -0.0033811969 -0.003381365 -0.0033809994 -0.003380022 -0.0033793054]]...]
INFO - root - 2017-12-09 19:05:38.883160: step 51510, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 70h:30m:55s remains)
INFO - root - 2017-12-09 19:05:47.537399: step 51520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:43m:28s remains)
INFO - root - 2017-12-09 19:05:56.257063: step 51530, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 67h:26m:08s remains)
INFO - root - 2017-12-09 19:06:04.919791: step 51540, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 67h:58m:52s remains)
INFO - root - 2017-12-09 19:06:13.247933: step 51550, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.737 sec/batch; 57h:33m:15s remains)
INFO - root - 2017-12-09 19:06:21.345200: step 51560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 65h:28m:32s remains)
INFO - root - 2017-12-09 19:06:29.903693: step 51570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:21m:42s remains)
INFO - root - 2017-12-09 19:06:38.604828: step 51580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 66h:46m:02s remains)
INFO - root - 2017-12-09 19:06:47.207407: step 51590, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 69h:55m:54s remains)
INFO - root - 2017-12-09 19:06:55.867400: step 51600, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:14m:26s remains)
2017-12-09 19:06:56.739420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034092634 -0.0034053519 -0.0034009505 -0.0033963006 -0.0033937229 -0.0033936903 -0.0033936256 -0.0033842367 -0.0033756713 -0.0033644433 -0.0033567422 -0.0033408247 -0.0033312291 -0.0033150092 -0.0033077102][-0.0034079049 -0.0034031819 -0.0033979623 -0.0033929907 -0.0033901257 -0.003388488 -0.0033843715 -0.003378523 -0.0033677397 -0.0033603951 -0.0033514386 -0.0033352366 -0.0033210425 -0.0033022668 -0.0032941089][-0.0034074169 -0.0034021887 -0.0033968058 -0.00339224 -0.0033879895 -0.0033864696 -0.003384456 -0.003376011 -0.003371527 -0.0033588368 -0.003354026 -0.003344571 -0.0033314587 -0.0033104853 -0.0032962894][-0.0034071519 -0.003401346 -0.0033967448 -0.0033923646 -0.0033899636 -0.003387724 -0.0033848563 -0.0033808532 -0.0033707705 -0.0033641087 -0.0033602947 -0.0033527208 -0.0033427624 -0.0033256141 -0.0033113926][-0.0034065275 -0.0034005097 -0.003395668 -0.0033916351 -0.0033892996 -0.0033886794 -0.003387097 -0.0033823531 -0.0033764397 -0.00336937 -0.003367221 -0.0033619814 -0.0033532754 -0.003338225 -0.0033260933][-0.0034050427 -0.003399147 -0.0033942354 -0.0033899725 -0.0033874288 -0.0033866854 -0.003386301 -0.0033833866 -0.0033772727 -0.0033735088 -0.0033734792 -0.0033696138 -0.0033615136 -0.003350476 -0.0033375721][-0.003402927 -0.0033973381 -0.0033926177 -0.0033886861 -0.0033857715 -0.0033843163 -0.0033839496 -0.0033820628 -0.0033798879 -0.003380171 -0.0033808171 -0.003377445 -0.0033684901 -0.0033557436 -0.0033468278][-0.0034007651 -0.0033956221 -0.0033916351 -0.0033880465 -0.003385396 -0.0033838931 -0.0033832376 -0.0033817331 -0.0033807566 -0.0033837557 -0.0033884591 -0.003387484 -0.0033792504 -0.0033681539 -0.0033585902][-0.0033982468 -0.0033938331 -0.0033908042 -0.0033877047 -0.00338532 -0.00338411 -0.0033839005 -0.0033834591 -0.0033835005 -0.0033873795 -0.0033940803 -0.0033955579 -0.0033919527 -0.0033838009 -0.0033759121][-0.0033956538 -0.0033922086 -0.0033902789 -0.0033878356 -0.00338617 -0.0033856044 -0.0033857718 -0.0033859247 -0.003388189 -0.0033923255 -0.0033968012 -0.0034011444 -0.0034013374 -0.0033978801 -0.0033932193][-0.0033930973 -0.003390515 -0.0033897278 -0.0033884782 -0.0033877136 -0.0033879073 -0.0033885136 -0.0033896074 -0.0033927378 -0.0033970971 -0.0034013567 -0.0034044972 -0.0034059591 -0.0034055915 -0.0034031717][-0.0033919243 -0.0033900707 -0.0033900072 -0.00338983 -0.0033900407 -0.0033907997 -0.0033920086 -0.0033940922 -0.003397634 -0.0034015181 -0.0034049591 -0.003407842 -0.0034094537 -0.0034094627 -0.0034081787][-0.0033910484 -0.0033895955 -0.0033902274 -0.0033910524 -0.0033923541 -0.003393953 -0.0033959274 -0.0033987672 -0.0034024718 -0.0034060415 -0.0034088797 -0.0034111962 -0.003412473 -0.0034123904 -0.0034113287][-0.0033905737 -0.0033892142 -0.0033901408 -0.0033916952 -0.0033939551 -0.0033966582 -0.0033996971 -0.0034030774 -0.0034068308 -0.0034102686 -0.003412853 -0.0034149773 -0.0034163229 -0.0034165948 -0.0034160903][-0.00338997 -0.0033885574 -0.0033898875 -0.003392027 -0.0033950121 -0.003398753 -0.0034025898 -0.0034060956 -0.0034098418 -0.0034131345 -0.0034157052 -0.0034179322 -0.003419644 -0.0034206039 -0.0034208542]]...]
INFO - root - 2017-12-09 19:07:05.367514: step 51610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:31m:41s remains)
INFO - root - 2017-12-09 19:07:14.003128: step 51620, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:30m:46s remains)
INFO - root - 2017-12-09 19:07:22.672338: step 51630, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 72h:26m:12s remains)
INFO - root - 2017-12-09 19:07:31.295702: step 51640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:11m:21s remains)
INFO - root - 2017-12-09 19:07:39.761280: step 51650, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 63h:52m:09s remains)
INFO - root - 2017-12-09 19:07:48.145597: step 51660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:50m:40s remains)
INFO - root - 2017-12-09 19:07:56.786407: step 51670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 67h:20m:27s remains)
INFO - root - 2017-12-09 19:08:05.363654: step 51680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:55m:17s remains)
INFO - root - 2017-12-09 19:08:14.012490: step 51690, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:06m:14s remains)
INFO - root - 2017-12-09 19:08:22.703305: step 51700, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 69h:26m:55s remains)
2017-12-09 19:08:23.670790: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10752109 0.10731417 0.1012606 0.089658678 0.075072937 0.059607618 0.045266259 0.033859402 0.025213262 0.019453196 0.01513901 0.012261443 0.010355142 0.0091746533 0.0079518612][0.12597194 0.1281082 0.12357895 0.112648 0.097806215 0.081033736 0.064282633 0.049387977 0.037135076 0.027745834 0.020472582 0.015110465 0.011313588 0.00900529 0.0072319536][0.14064041 0.14823757 0.14919582 0.14301932 0.13144292 0.11573958 0.097831532 0.079470359 0.062467434 0.04778887 0.035359915 0.025314186 0.017804792 0.012712784 0.0090802656][0.15205711 0.16640709 0.17494597 0.17620927 0.17067154 0.15863509 0.14156418 0.12114784 0.10021625 0.080648407 0.06288626 0.047512919 0.03490844 0.025000323 0.017608194][0.16403392 0.18580753 0.20270827 0.21235323 0.21395068 0.2070642 0.19254053 0.171817 0.14778715 0.12334475 0.10022788 0.079447463 0.061671212 0.046623629 0.034562539][0.17540669 0.2045031 0.22929451 0.24689442 0.25573173 0.25477833 0.24441142 0.22585703 0.20136681 0.17482051 0.14796263 0.12230049 0.098782711 0.077756532 0.060006633][0.18202044 0.21634084 0.2471675 0.27164447 0.28725216 0.29228246 0.28712034 0.27330115 0.25175473 0.22632347 0.1985587 0.17059991 0.14321283 0.11691219 0.093502045][0.18301691 0.21875674 0.25161827 0.27928239 0.29924443 0.309869 0.31093657 0.30334124 0.28697476 0.26569641 0.24008498 0.21298918 0.18454452 0.15582705 0.12892903][0.17903164 0.21279633 0.24391909 0.27100983 0.29185697 0.30543792 0.3111546 0.30962116 0.29966193 0.28403276 0.26313666 0.23940268 0.21291544 0.18500674 0.15741701][0.16690563 0.19690827 0.2245207 0.24893525 0.26906574 0.28395757 0.29268804 0.29538032 0.29072943 0.28073382 0.26495555 0.24542467 0.22228117 0.19725972 0.17180125][0.14734946 0.17246161 0.19566107 0.21690737 0.23558614 0.25095579 0.26145139 0.26686978 0.26629037 0.26079473 0.24940619 0.23391634 0.21494848 0.19387898 0.17212851][0.12345651 0.14318435 0.16150761 0.17912631 0.19586445 0.21092156 0.22251573 0.22962274 0.23177411 0.2293569 0.2214 0.20958515 0.19495983 0.17871004 0.1617787][0.10079157 0.11636943 0.1307494 0.14466597 0.15861565 0.17208092 0.18334047 0.1905541 0.19349873 0.19236815 0.18658961 0.17750426 0.16645992 0.15505263 0.14383325][0.080867708 0.093845926 0.1059818 0.11795327 0.13021073 0.14156011 0.15156181 0.15780333 0.16040911 0.15906852 0.15379374 0.14597099 0.13726623 0.12945743 0.12298247][0.066469878 0.07838694 0.089463875 0.10051224 0.11156969 0.12121657 0.12960912 0.13418403 0.13553335 0.13314644 0.12745601 0.11999559 0.11232243 0.1067614 0.10361784]]...]
INFO - root - 2017-12-09 19:08:32.279175: step 51710, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 65h:07m:59s remains)
INFO - root - 2017-12-09 19:08:40.922595: step 51720, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 66h:41m:46s remains)
INFO - root - 2017-12-09 19:08:49.431249: step 51730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 64h:41m:10s remains)
INFO - root - 2017-12-09 19:08:57.951671: step 51740, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.824 sec/batch; 64h:17m:03s remains)
INFO - root - 2017-12-09 19:09:06.493733: step 51750, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.723 sec/batch; 56h:23m:54s remains)
INFO - root - 2017-12-09 19:09:15.029632: step 51760, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:55m:46s remains)
INFO - root - 2017-12-09 19:09:23.771047: step 51770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:15m:13s remains)
INFO - root - 2017-12-09 19:09:32.530327: step 51780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:03m:23s remains)
INFO - root - 2017-12-09 19:09:41.115198: step 51790, loss = 0.89, batch loss = 0.68 (10.8 examples/sec; 0.742 sec/batch; 57h:51m:56s remains)
INFO - root - 2017-12-09 19:09:49.742923: step 51800, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 67h:45m:01s remains)
2017-12-09 19:09:50.633830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033638512 -0.0033609311 -0.0033610817 -0.0033614647 -0.0033619553 -0.0033625378 -0.0033630666 -0.0033636659 -0.0033642079 -0.0033645423 -0.0033647646 -0.0033650652 -0.0033653956 -0.0033656277 -0.0033657213][-0.0033620561 -0.0033588896 -0.0033589378 -0.0033593078 -0.0033597387 -0.0033601637 -0.0033604642 -0.0033609057 -0.003361406 -0.0033618433 -0.0033622992 -0.00336284 -0.0033633567 -0.0033637013 -0.0033638678][-0.0033625867 -0.0033592659 -0.0033591979 -0.0033594391 -0.0033596486 -0.0033597497 -0.0033597071 -0.0033598929 -0.0033602496 -0.0033606463 -0.0033612195 -0.0033620468 -0.0033628889 -0.0033634882 -0.0033638068][-0.0033628822 -0.0033593916 -0.0033591469 -0.0033591886 -0.00335914 -0.0033589567 -0.003358715 -0.0033587979 -0.0033591725 -0.0033595844 -0.0033601881 -0.0033611881 -0.003362328 -0.0033632133 -0.0033637565][-0.0033632545 -0.0033595893 -0.0033591304 -0.0033589378 -0.0033586337 -0.0033582572 -0.0033578756 -0.0033579646 -0.0033584149 -0.0033587816 -0.0033593138 -0.0033604014 -0.0033617388 -0.0033628712 -0.0033636782][-0.003363526 -0.003359721 -0.0033590135 -0.0033586156 -0.0033581986 -0.0033577846 -0.0033574982 -0.0033577026 -0.0033581865 -0.0033585839 -0.0033590866 -0.0033600782 -0.0033613127 -0.003362522 -0.00336352][-0.0033642568 -0.0033606957 -0.0033600361 -0.0033594456 -0.0033587487 -0.003358095 -0.003357609 -0.0033578253 -0.0033583704 -0.0033589455 -0.0033595837 -0.00336034 -0.0033611972 -0.0033621271 -0.0033630321][-0.0033652189 -0.0033620321 -0.0033617506 -0.0033613171 -0.0033605306 -0.0033595217 -0.0033584437 -0.003358131 -0.0033585089 -0.0033591161 -0.0033599287 -0.0033607879 -0.0033615052 -0.0033621464 -0.0033627809][-0.0033660799 -0.0033635644 -0.0033639683 -0.0033639511 -0.003363248 -0.0033621222 -0.0033608249 -0.0033597816 -0.0033594153 -0.0033596258 -0.0033603169 -0.003361085 -0.0033617304 -0.0033621727 -0.0033625551][-0.0033669446 -0.0033649495 -0.0033659397 -0.0033662096 -0.003365607 -0.0033645413 -0.0033630321 -0.0033616102 -0.00336067 -0.0033603823 -0.0033606435 -0.0033610435 -0.0033615166 -0.0033618296 -0.0033620792][-0.00336755 -0.0033661395 -0.0033678035 -0.0033681728 -0.0033673088 -0.0033659604 -0.0033639746 -0.0033622074 -0.0033611597 -0.0033607858 -0.0033608999 -0.0033610584 -0.003361332 -0.003361433 -0.0033615108][-0.0033677949 -0.0033665567 -0.0033686159 -0.0033691868 -0.0033682757 -0.0033665719 -0.0033640885 -0.0033620154 -0.0033608563 -0.0033604365 -0.0033606512 -0.00336086 -0.0033611564 -0.0033611634 -0.0033611169][-0.003367217 -0.00336567 -0.0033677351 -0.0033684971 -0.0033679111 -0.0033665153 -0.0033643264 -0.0033622736 -0.0033610004 -0.0033603993 -0.0033604146 -0.0033605059 -0.0033608256 -0.0033609311 -0.0033609099][-0.003366254 -0.0033642186 -0.0033659036 -0.0033665856 -0.0033662985 -0.0033654526 -0.0033639865 -0.00336235 -0.003361203 -0.0033606139 -0.0033603711 -0.0033602139 -0.0033604053 -0.0033605665 -0.0033605993][-0.0033651206 -0.003362424 -0.0033636107 -0.0033641167 -0.0033640931 -0.0033637227 -0.0033628747 -0.0033617949 -0.0033610319 -0.0033606053 -0.0033603243 -0.00336008 -0.0033601876 -0.0033603385 -0.0033603795]]...]
INFO - root - 2017-12-09 19:09:59.124395: step 51810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:09m:03s remains)
INFO - root - 2017-12-09 19:10:07.817273: step 51820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:11m:58s remains)
INFO - root - 2017-12-09 19:10:16.430290: step 51830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:26m:15s remains)
INFO - root - 2017-12-09 19:10:25.070427: step 51840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 65h:21m:06s remains)
INFO - root - 2017-12-09 19:10:33.579286: step 51850, loss = 0.89, batch loss = 0.68 (10.8 examples/sec; 0.743 sec/batch; 57h:54m:47s remains)
INFO - root - 2017-12-09 19:10:42.069878: step 51860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:01m:10s remains)
INFO - root - 2017-12-09 19:10:50.471837: step 51870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:55m:11s remains)
INFO - root - 2017-12-09 19:10:59.029646: step 51880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:47m:08s remains)
INFO - root - 2017-12-09 19:11:07.955744: step 51890, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 70h:16m:04s remains)
INFO - root - 2017-12-09 19:11:16.390866: step 51900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 68h:52m:36s remains)
2017-12-09 19:11:17.362149: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17502256 0.15124771 0.12036549 0.090135708 0.062898524 0.042870261 0.029226089 0.020481717 0.014326589 0.0096413735 0.0059258947 0.0029538504 0.00023278454 -0.0017257344 -0.0027879216][0.23875545 0.21099968 0.17367168 0.13660531 0.10223319 0.075696945 0.055671044 0.04123934 0.029967526 0.020794187 0.013217656 0.0071168011 0.0022183082 -0.0011666371 -0.0026981086][0.30232677 0.27532366 0.236076 0.19551121 0.15592928 0.12341285 0.096277051 0.074832715 0.057060089 0.042004332 0.028775251 0.017415777 0.0080742957 0.0014783826 -0.0018697163][0.35193554 0.33011788 0.29500744 0.25722095 0.21820317 0.18402004 0.15225255 0.12433504 0.098689914 0.075221427 0.053262025 0.033901632 0.018080393 0.0066213729 0.00011491822][0.37664866 0.36298373 0.33598602 0.30585033 0.27310157 0.24275994 0.211579 0.1814298 0.15054244 0.11950371 0.087909684 0.05842796 0.033336628 0.014634259 0.0033256572][0.37113923 0.36567464 0.34832066 0.32860297 0.3058736 0.28365833 0.2580559 0.23046649 0.19853131 0.16313878 0.12402591 0.085586049 0.051325459 0.024721624 0.0077018561][0.33717236 0.33847627 0.3301664 0.32081702 0.30879423 0.29637608 0.27906042 0.25737396 0.22835685 0.19274618 0.15034889 0.10671317 0.066185653 0.033678364 0.011900868][0.27884406 0.28556016 0.28597918 0.28564233 0.28295457 0.27947718 0.27058282 0.25561389 0.23134363 0.19895528 0.15799901 0.11434045 0.072480544 0.038146254 0.01432208][0.20651491 0.2166144 0.22262208 0.22891161 0.23386128 0.23748992 0.23567899 0.22694747 0.20817931 0.18078618 0.14459756 0.10538701 0.06724339 0.035836164 0.013691293][0.1350145 0.14572237 0.15405603 0.16286446 0.1710133 0.17837755 0.1813646 0.17766453 0.16454616 0.14357722 0.11499079 0.083555661 0.052709252 0.027504716 0.0098907286][0.077052593 0.086256042 0.094065443 0.1021787 0.1097619 0.1167748 0.12065508 0.11976933 0.11162025 0.097566687 0.077989534 0.05620604 0.034660574 0.017068999 0.0050252387][0.038302083 0.044881966 0.050860014 0.056571983 0.061672978 0.066114649 0.068416588 0.067891113 0.062760055 0.054421552 0.0430433 0.030396497 0.017886499 0.0075982464 0.00079173525][0.014871301 0.0188663 0.022884214 0.02647415 0.029532285 0.031751368 0.0325667 0.031678692 0.028343232 0.023817446 0.018065538 0.011943889 0.0060150065 0.0011860749 -0.0018149011][0.002265936 0.0041523278 0.0062375022 0.0081761982 0.0098031955 0.010864738 0.011225423 0.010590454 0.0088762287 0.0067506079 0.0042837188 0.0019013092 -0.00032454077 -0.002036666 -0.0029847042][-0.002465501 -0.0019023342 -0.0011933367 -0.00047840667 0.00015823264 0.00060345815 0.00081498944 0.0006216981 7.3590549e-05 -0.0006716249 -0.0014492618 -0.0021072598 -0.0026987498 -0.003104398 -0.003301098]]...]
INFO - root - 2017-12-09 19:11:25.919291: step 51910, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:06m:01s remains)
INFO - root - 2017-12-09 19:11:34.512293: step 51920, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 66h:18m:56s remains)
INFO - root - 2017-12-09 19:11:42.937720: step 51930, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 64h:17m:17s remains)
INFO - root - 2017-12-09 19:11:51.387411: step 51940, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 64h:57m:47s remains)
INFO - root - 2017-12-09 19:11:59.970354: step 51950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:18m:51s remains)
INFO - root - 2017-12-09 19:12:08.300699: step 51960, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 67h:28m:30s remains)
INFO - root - 2017-12-09 19:12:16.998082: step 51970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:20m:54s remains)
INFO - root - 2017-12-09 19:12:25.538773: step 51980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:53m:20s remains)
INFO - root - 2017-12-09 19:12:34.095776: step 51990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:53m:52s remains)
INFO - root - 2017-12-09 19:12:42.661387: step 52000, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 68h:51m:17s remains)
2017-12-09 19:12:43.560125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033869352 -0.0033840295 -0.003384792 -0.0033862202 -0.0033876051 -0.0033889294 -0.003390451 -0.0033916789 -0.0033923259 -0.0033923909 -0.0033921434 -0.0033917378 -0.0033907527 -0.0033891166 -0.0033876048][-0.0033864137 -0.0033835468 -0.0033842851 -0.00338583 -0.0033869767 -0.0033877539 -0.0033887352 -0.0033896081 -0.00339014 -0.00339061 -0.0033906687 -0.00339056 -0.0033897904 -0.0033883159 -0.0033869462][-0.0033876202 -0.00338497 -0.0033854954 -0.003386341 -0.0033868125 -0.0033864107 -0.0033858786 -0.0033859964 -0.0033862803 -0.0033870596 -0.003388217 -0.003389322 -0.0033896891 -0.0033893217 -0.003388782][-0.0033887862 -0.0033864833 -0.0033862798 -0.0033858283 -0.0033848139 -0.0033825522 -0.0033802297 -0.0033789354 -0.0033783903 -0.0033799147 -0.0033831387 -0.0033861524 -0.0033883431 -0.0033895688 -0.0033901641][-0.0033904177 -0.0033881473 -0.0033866637 -0.0033843191 -0.003381025 -0.0033767049 -0.0033722252 -0.0033693297 -0.0033682911 -0.003371391 -0.003377069 -0.0033823622 -0.0033863503 -0.0033889359 -0.0033905108][-0.0033926854 -0.00339021 -0.0033874719 -0.00338291 -0.0033772825 -0.0033717258 -0.0033658771 -0.0033610456 -0.0033592226 -0.0033632272 -0.0033707225 -0.0033781859 -0.003384782 -0.003389409 -0.0033922461][-0.0033959607 -0.0033934936 -0.0033895276 -0.0033828954 -0.003376099 -0.0033696648 -0.0033631292 -0.0033574188 -0.0033554423 -0.0033593487 -0.0033674713 -0.0033763351 -0.0033844588 -0.0033904905 -0.0033941863][-0.0033982967 -0.0033964543 -0.0033925388 -0.00338561 -0.0033777317 -0.0033707586 -0.003364203 -0.0033591765 -0.0033573003 -0.0033606079 -0.0033686091 -0.0033777223 -0.0033860451 -0.003391739 -0.0033951867][-0.0033989211 -0.0033979064 -0.0033952463 -0.0033897338 -0.0033827198 -0.0033753302 -0.0033690068 -0.003365125 -0.0033640086 -0.0033673912 -0.0033744683 -0.0033821065 -0.0033888482 -0.0033930568 -0.0033954973][-0.0033982371 -0.0033978252 -0.003397526 -0.0033942906 -0.0033894307 -0.0033832297 -0.0033771265 -0.0033735968 -0.003373621 -0.003377293 -0.0033823659 -0.0033878228 -0.0033921115 -0.0033944959 -0.003395651][-0.0033955788 -0.0033956857 -0.0033976487 -0.0033972166 -0.0033949891 -0.0033915553 -0.0033873622 -0.0033838376 -0.0033829834 -0.0033853913 -0.0033888631 -0.0033921595 -0.00339424 -0.0033949981 -0.0033952966][-0.0033917842 -0.0033923688 -0.0033959255 -0.0033980913 -0.0033987821 -0.0033981435 -0.0033956417 -0.0033929662 -0.0033912521 -0.0033916291 -0.0033928319 -0.0033941772 -0.0033948878 -0.003394492 -0.0033935595][-0.0033891143 -0.0033906926 -0.0033950689 -0.0033978138 -0.0033999342 -0.0034007614 -0.0033996929 -0.0033975525 -0.0033953423 -0.0033945662 -0.0033944235 -0.0033943586 -0.0033938477 -0.0033926752 -0.0033914461][-0.0033896868 -0.0033912307 -0.0033952245 -0.0033981546 -0.0034005279 -0.0034014722 -0.0034003567 -0.0033982543 -0.0033956124 -0.0033941211 -0.0033934382 -0.0033926985 -0.0033914526 -0.0033900542 -0.0033888137][-0.0033912384 -0.0033928079 -0.0033957302 -0.0033985672 -0.0034002445 -0.003399991 -0.0033981907 -0.0033954002 -0.0033924102 -0.00339023 -0.0033891723 -0.0033887948 -0.0033878281 -0.0033869373 -0.0033861115]]...]
INFO - root - 2017-12-09 19:12:52.134940: step 52010, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:21m:10s remains)
INFO - root - 2017-12-09 19:13:00.667519: step 52020, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.868 sec/batch; 67h:38m:01s remains)
INFO - root - 2017-12-09 19:13:09.346349: step 52030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:08m:37s remains)
INFO - root - 2017-12-09 19:13:17.848611: step 52040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:37m:29s remains)
INFO - root - 2017-12-09 19:13:26.450324: step 52050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:52m:19s remains)
INFO - root - 2017-12-09 19:13:34.621269: step 52060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:57m:56s remains)
INFO - root - 2017-12-09 19:13:42.934007: step 52070, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:10m:50s remains)
INFO - root - 2017-12-09 19:13:51.666052: step 52080, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 69h:40m:56s remains)
INFO - root - 2017-12-09 19:14:00.371146: step 52090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:52m:55s remains)
INFO - root - 2017-12-09 19:14:08.796050: step 52100, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 66h:07m:44s remains)
2017-12-09 19:14:09.646288: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012314804 0.012629228 0.012652433 0.012564717 0.012625175 0.012832211 0.013175763 0.013405703 0.013394343 0.012778405 0.011506886 0.0093628 0.0065077739 0.0034098423 0.00066364952][0.016680516 0.016855368 0.016723571 0.016528474 0.016485905 0.016609475 0.016832575 0.01694187 0.016802104 0.016075063 0.014612719 0.012147035 0.0088767046 0.0052992012 0.0020207067][0.019834988 0.019463476 0.018741097 0.018096594 0.017817045 0.017908018 0.018197596 0.018427324 0.018338785 0.017612366 0.016021403 0.01338115 0.0098895626 0.0060779275 0.0026231997][0.020661205 0.019649502 0.018286817 0.017156811 0.016572557 0.016580977 0.016974362 0.01746049 0.017650435 0.017166898 0.015730256 0.01321019 0.0098131513 0.0060862126 0.0027065587][0.019862305 0.018264653 0.016376218 0.014932275 0.01415045 0.01411134 0.014522269 0.015096189 0.015421622 0.015108909 0.013876557 0.011662835 0.0086845569 0.0054091467 0.0024006704][0.017742619 0.015890336 0.013836058 0.012244521 0.011432088 0.01140725 0.011825656 0.012388734 0.01261675 0.012224264 0.011011899 0.0090657277 0.0065681431 0.0039042218 0.0014827347][0.014467778 0.012550613 0.01047335 0.0089151319 0.008156335 0.0082078082 0.00870496 0.0092977164 0.0094631091 0.0090024136 0.0077723255 0.0060229311 0.0039763907 0.0019129256 0.00010450743][0.010310329 0.0085643595 0.0066514695 0.0052681826 0.0046798224 0.0048852852 0.0055039683 0.0061642071 0.0063139554 0.0058077965 0.0046263244 0.0030922235 0.0014706289 -5.7527563e-05 -0.0013124181][0.005590491 0.004180274 0.0026471277 0.0016149294 0.0013216815 0.0017968181 0.0026082483 0.0033239292 0.0034814954 0.0029952892 0.0019597623 0.00071250577 -0.000498384 -0.0015724261 -0.0023895376][0.0014475014 0.0004338501 -0.00062847417 -0.0011905443 -0.0011122611 -0.00042001647 0.00044107577 0.0010650146 0.0011397603 0.00067170546 -0.0001373908 -0.0010378552 -0.0018621997 -0.002546845 -0.003020274][-0.0013793113 -0.0019315274 -0.0024363515 -0.0025312607 -0.0021763775 -0.0015092387 -0.00084746163 -0.00051068515 -0.00059751724 -0.0010104601 -0.001576566 -0.0021498718 -0.0026645283 -0.0030515208 -0.0032788822][-0.0027256149 -0.0029187989 -0.0030439734 -0.0028987592 -0.0024995683 -0.0019909684 -0.0016062385 -0.0015104687 -0.0016840076 -0.002031706 -0.0024246266 -0.002784275 -0.0030773603 -0.0032698489 -0.0033645045][-0.0033101721 -0.0033283641 -0.0032690307 -0.0030443715 -0.002676517 -0.0022882889 -0.002054424 -0.0020513304 -0.0022381428 -0.0025135879 -0.0028104619 -0.0030799787 -0.0032709446 -0.0033692333 -0.0033964613][-0.0033906281 -0.0033858952 -0.0033389248 -0.0031979457 -0.0029668151 -0.0027226263 -0.0025718878 -0.0025629627 -0.0026690036 -0.0028358931 -0.00302633 -0.0032031359 -0.0033294626 -0.0033899629 -0.0034033509][-0.0034044012 -0.0034026187 -0.0033907108 -0.0033386541 -0.0032409709 -0.0031299323 -0.003049864 -0.0030305632 -0.0030613206 -0.003126472 -0.0032115802 -0.0032957662 -0.0033624922 -0.0033971982 -0.00340689]]...]
INFO - root - 2017-12-09 19:14:18.182486: step 52110, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 64h:32m:46s remains)
INFO - root - 2017-12-09 19:14:26.842705: step 52120, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 70h:33m:19s remains)
INFO - root - 2017-12-09 19:14:35.597904: step 52130, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:25m:53s remains)
INFO - root - 2017-12-09 19:14:44.254634: step 52140, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 67h:25m:07s remains)
INFO - root - 2017-12-09 19:14:52.936444: step 52150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 68h:35m:22s remains)
INFO - root - 2017-12-09 19:15:01.046165: step 52160, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 63h:22m:33s remains)
INFO - root - 2017-12-09 19:15:09.670429: step 52170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:34m:12s remains)
INFO - root - 2017-12-09 19:15:18.447793: step 52180, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 67h:30m:50s remains)
INFO - root - 2017-12-09 19:15:27.031891: step 52190, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 64h:24m:06s remains)
INFO - root - 2017-12-09 19:15:35.636178: step 52200, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 70h:25m:26s remains)
2017-12-09 19:15:36.611466: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45240268 0.43003979 0.40393114 0.37905005 0.35894221 0.34702995 0.34261575 0.34185567 0.34015757 0.3366957 0.33263025 0.32945618 0.32366541 0.31626147 0.30738026][0.48350653 0.46243778 0.43685821 0.41268548 0.39265612 0.38012391 0.37385419 0.36978984 0.36527771 0.35917646 0.35275316 0.34829843 0.34195134 0.33528537 0.3273136][0.51021683 0.4924455 0.46881673 0.44602174 0.42512631 0.41073114 0.40113318 0.39243823 0.38319194 0.37249884 0.36404279 0.35816035 0.35117695 0.34455934 0.3378256][0.53612691 0.52415055 0.50495839 0.48432133 0.4633193 0.44569996 0.43093818 0.41634503 0.4009358 0.38563776 0.37509426 0.36808777 0.36115709 0.35456994 0.3481859][0.55867338 0.553736 0.53868109 0.52084583 0.50050271 0.4813827 0.46327093 0.44275427 0.42111918 0.40120712 0.38769794 0.37952119 0.37168646 0.36531249 0.35945785][0.57096744 0.5735926 0.56234831 0.54691327 0.52740568 0.507748 0.48782843 0.46496558 0.44111639 0.41861415 0.40305808 0.39308989 0.38393328 0.37660316 0.37026206][0.5634982 0.57286179 0.5653165 0.55360097 0.53726357 0.52011228 0.501856 0.4803921 0.457797 0.43615457 0.42060548 0.40962678 0.39969075 0.39088714 0.38309172][0.54081911 0.555778 0.55179816 0.54344809 0.53037119 0.5162015 0.50097233 0.48346949 0.46432427 0.44591045 0.4321005 0.42159662 0.41126588 0.40121847 0.39212337][0.50515896 0.52331984 0.5221746 0.51769596 0.50920624 0.49953419 0.48909539 0.4763872 0.46181786 0.4473252 0.4351469 0.4253239 0.41465914 0.40380272 0.39363757][0.45925468 0.47865993 0.47911987 0.47800335 0.47391772 0.46919441 0.46393582 0.45649311 0.44684547 0.43600249 0.42579487 0.41624171 0.40514794 0.39389396 0.3829239][0.40546522 0.42462322 0.42664319 0.42852652 0.42883459 0.42814252 0.42705372 0.42409888 0.41870484 0.4113169 0.40310571 0.3944732 0.38408655 0.37261036 0.36093536][0.34880656 0.36652926 0.36967248 0.37350097 0.3767595 0.37949836 0.3818495 0.38196486 0.37888891 0.37377837 0.36731398 0.35997063 0.35086527 0.34055841 0.32965851][0.29350173 0.30870166 0.31213558 0.31650242 0.32111377 0.32529876 0.32901436 0.33054432 0.32894006 0.32536882 0.32024702 0.31439731 0.30696216 0.29836023 0.28887844][0.2389663 0.25129685 0.25459096 0.25875694 0.26363012 0.26848412 0.27280864 0.27491319 0.27413368 0.27166656 0.26772308 0.2631962 0.25755739 0.25101084 0.24358752][0.18977602 0.19967136 0.20236763 0.20578481 0.20995419 0.21413454 0.21799183 0.22002679 0.21980031 0.21823601 0.21538779 0.21221519 0.20834978 0.20387757 0.19852]]...]
INFO - root - 2017-12-09 19:15:45.139717: step 52210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:05m:01s remains)
INFO - root - 2017-12-09 19:15:53.633450: step 52220, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 66h:44m:15s remains)
INFO - root - 2017-12-09 19:16:02.183941: step 52230, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 63h:17m:29s remains)
INFO - root - 2017-12-09 19:16:10.900334: step 52240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:08m:35s remains)
INFO - root - 2017-12-09 19:16:19.595296: step 52250, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.908 sec/batch; 70h:40m:00s remains)
INFO - root - 2017-12-09 19:16:28.069329: step 52260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:21m:39s remains)
INFO - root - 2017-12-09 19:16:36.796570: step 52270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 67h:18m:59s remains)
INFO - root - 2017-12-09 19:16:45.579106: step 52280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:30m:59s remains)
INFO - root - 2017-12-09 19:16:54.257140: step 52290, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 69h:54m:18s remains)
INFO - root - 2017-12-09 19:17:02.731632: step 52300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:54m:52s remains)
2017-12-09 19:17:03.573771: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037758749 0.044705644 0.054753769 0.0668389 0.080142379 0.093015552 0.1049204 0.11208197 0.11368475 0.10904872 0.099360764 0.083747923 0.064749666 0.04505191 0.02783226][0.048814 0.056291707 0.067976594 0.084827848 0.10599329 0.12859932 0.15041602 0.16572677 0.17269994 0.16869991 0.15546632 0.13253486 0.10395242 0.074053757 0.046961971][0.061661392 0.069868818 0.084331267 0.10634577 0.13603494 0.16919023 0.20179226 0.22595693 0.23882391 0.23688765 0.22116527 0.19157116 0.15349166 0.11251266 0.074245453][0.078784131 0.087086514 0.10435182 0.13238649 0.17151177 0.21566948 0.25921017 0.29214159 0.31063369 0.31024066 0.29158831 0.25555626 0.20838737 0.15667368 0.10691383][0.10168395 0.11015587 0.12965208 0.16249925 0.20969263 0.26306716 0.31569836 0.35586467 0.37914312 0.38070491 0.3601023 0.31937122 0.26485187 0.20400952 0.14387716][0.12575713 0.13419998 0.15480655 0.19055605 0.24264574 0.30172977 0.36033794 0.40511876 0.43122375 0.434112 0.41292188 0.37068495 0.31266457 0.24653268 0.17932573][0.14865178 0.15551133 0.17489041 0.21022096 0.26284471 0.32302797 0.38381115 0.43080115 0.458478 0.46272779 0.44274163 0.40203485 0.34425262 0.27700281 0.20668472][0.16537596 0.17035453 0.18634078 0.21772832 0.26650834 0.3235842 0.38241321 0.42841032 0.45615008 0.46194884 0.44492844 0.40835652 0.35457325 0.29019919 0.22080356][0.16794193 0.17173733 0.18374483 0.21036722 0.252899 0.30359176 0.35729057 0.40042955 0.42782018 0.43490106 0.42134315 0.3898775 0.34177119 0.28271738 0.21745199][0.15137017 0.15528099 0.16460028 0.1863713 0.22162668 0.26451117 0.31079811 0.34907213 0.37488988 0.38350523 0.37422594 0.34858891 0.30737084 0.25554639 0.19735828][0.12033154 0.12404728 0.13128376 0.1484465 0.1762376 0.21061625 0.24841219 0.28053331 0.30324578 0.31248415 0.30724448 0.287807 0.2544843 0.21154326 0.16297366][0.084090665 0.087336108 0.092784509 0.10554734 0.12604532 0.1517017 0.1801921 0.20525493 0.22391397 0.23260647 0.23026559 0.21633175 0.19121416 0.15810628 0.12065097][0.049800176 0.052667409 0.056977406 0.0661402 0.080323882 0.098134123 0.11779826 0.13546985 0.14915007 0.15617079 0.15538366 0.14591183 0.12832016 0.10488527 0.078671545][0.022263603 0.024483975 0.02797371 0.034477435 0.043873109 0.055400398 0.067954086 0.079471849 0.088536575 0.093246371 0.092804737 0.086648688 0.075234093 0.06009271 0.043609764][0.0052506654 0.0065295659 0.0087820636 0.012729352 0.018220106 0.024829943 0.03181693 0.038290489 0.043417454 0.0460887 0.045817062 0.042297557 0.035958152 0.0277984 0.019168997]]...]
INFO - root - 2017-12-09 19:17:12.364858: step 52310, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:23m:54s remains)
INFO - root - 2017-12-09 19:17:21.071278: step 52320, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:19m:32s remains)
INFO - root - 2017-12-09 19:17:29.709943: step 52330, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 65h:40m:21s remains)
INFO - root - 2017-12-09 19:17:38.474604: step 52340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:25m:54s remains)
INFO - root - 2017-12-09 19:17:47.189347: step 52350, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:04m:34s remains)
INFO - root - 2017-12-09 19:17:55.513477: step 52360, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 70h:00m:47s remains)
INFO - root - 2017-12-09 19:18:04.048100: step 52370, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 65h:41m:26s remains)
INFO - root - 2017-12-09 19:18:12.625175: step 52380, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:31m:48s remains)
INFO - root - 2017-12-09 19:18:21.293858: step 52390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:28m:48s remains)
INFO - root - 2017-12-09 19:18:30.016184: step 52400, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:42m:52s remains)
2017-12-09 19:18:31.008256: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0052926922 0.005393615 0.0058199679 0.0066832751 0.0077774739 0.0089189028 0.0098451031 0.0099451747 0.0095090056 0.0082974359 0.0066187391 0.0050067818 0.0039817989 0.0035724661 0.0033166327][0.0085137794 0.00849374 0.0086587667 0.0091837337 0.009962623 0.01109019 0.011843274 0.01186349 0.011416891 0.010120614 0.0082753384 0.0063539166 0.0049564773 0.0043215761 0.0038311172][0.010751781 0.011169921 0.011747645 0.012640649 0.013719354 0.015120184 0.015883079 0.015909333 0.015522001 0.014162194 0.01242597 0.010370473 0.0086097782 0.0074427119 0.0061562331][0.012071732 0.013462991 0.015191272 0.017247507 0.019260351 0.02130786 0.022193903 0.022316629 0.022187132 0.021511007 0.020378578 0.018601485 0.016873293 0.015499437 0.0133892][0.013657853 0.0165929 0.020121012 0.023958314 0.027278295 0.030012896 0.03095598 0.031288952 0.031328127 0.031389412 0.031450327 0.030765662 0.029826941 0.028334511 0.025628779][0.017197927 0.021805583 0.027265482 0.032905821 0.037835136 0.041482314 0.042796578 0.043201417 0.043332331 0.043753728 0.044221647 0.044154562 0.043980714 0.042952336 0.040222894][0.02467218 0.030416472 0.037041508 0.043682307 0.049442787 0.053446271 0.055193458 0.055925038 0.056409325 0.05711925 0.057907782 0.058050692 0.057933897 0.056941919 0.054144848][0.03353928 0.039670553 0.0465033 0.053250011 0.059019551 0.06298323 0.065048076 0.06607382 0.066920362 0.067996778 0.068983428 0.069113463 0.068989135 0.067921668 0.065065816][0.041940339 0.047401208 0.053295169 0.059184767 0.064062066 0.067328 0.069177322 0.070364244 0.0715692 0.073069125 0.074378312 0.074968308 0.075206742 0.074255668 0.071760207][0.04771699 0.052246604 0.056662992 0.061112069 0.064670332 0.0668813 0.068083368 0.06888245 0.0698193 0.071117237 0.072371945 0.073319621 0.073931977 0.073580928 0.072024211][0.051084891 0.054163419 0.056685209 0.0594439 0.061617702 0.06271968 0.063099988 0.063185334 0.063317411 0.063766114 0.064412579 0.065256618 0.066007338 0.066255532 0.065717161][0.051374286 0.0530603 0.053670913 0.05474171 0.055614978 0.055815265 0.055469356 0.054758396 0.053878456 0.053180028 0.052909825 0.053172745 0.053762525 0.054412469 0.05472802][0.048793528 0.049550962 0.048551366 0.04819547 0.0480351 0.047586095 0.046660922 0.045212016 0.043198209 0.041215554 0.039813917 0.039352763 0.039715633 0.040634241 0.041732322][0.044391591 0.044630963 0.042724729 0.041575275 0.04085945 0.04005798 0.038770121 0.036752671 0.033959575 0.030928267 0.028476795 0.027260723 0.027276024 0.028252574 0.029727461][0.038393009 0.038171094 0.03580483 0.034339327 0.033371985 0.032400481 0.030998787 0.028768638 0.025745424 0.022343732 0.019456709 0.017906206 0.017740805 0.018809156 0.020499693]]...]
INFO - root - 2017-12-09 19:18:39.599250: step 52410, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 66h:17m:32s remains)
INFO - root - 2017-12-09 19:18:48.299090: step 52420, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 64h:47m:16s remains)
INFO - root - 2017-12-09 19:18:57.040964: step 52430, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 69h:54m:48s remains)
INFO - root - 2017-12-09 19:19:05.712514: step 52440, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 65h:01m:16s remains)
INFO - root - 2017-12-09 19:19:14.422949: step 52450, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 66h:11m:55s remains)
INFO - root - 2017-12-09 19:19:22.782219: step 52460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:40m:47s remains)
INFO - root - 2017-12-09 19:19:31.426335: step 52470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:44m:57s remains)
INFO - root - 2017-12-09 19:19:39.913113: step 52480, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 65h:21m:43s remains)
INFO - root - 2017-12-09 19:19:48.554885: step 52490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:31m:09s remains)
INFO - root - 2017-12-09 19:19:57.052625: step 52500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:29m:35s remains)
2017-12-09 19:19:57.915553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.427869 0.42618474 0.42428952 0.41873151 0.41149938 0.40249965 0.39283618 0.38290355 0.37561741 0.37122235 0.36902747 0.36911803 0.368055 0.36483148 0.35722157][0.43151128 0.43213591 0.43211803 0.42878059 0.42368397 0.41603252 0.40697375 0.39783812 0.390731 0.3860175 0.38324061 0.38188764 0.37954104 0.37538409 0.36713257][0.42461738 0.42867884 0.43068579 0.42911169 0.42568314 0.41947711 0.41087103 0.40190366 0.39459559 0.38907772 0.38504955 0.382111 0.37855902 0.37334803 0.36432624][0.41450685 0.42215815 0.42674354 0.4267334 0.42446518 0.41925517 0.4115172 0.4021267 0.39408198 0.38754603 0.38187706 0.37699991 0.37174389 0.36565351 0.35604653][0.40306243 0.41336286 0.4195877 0.42111865 0.41971412 0.414474 0.40665081 0.39710456 0.3882302 0.38056812 0.3738797 0.36790833 0.3618165 0.3546629 0.34476984][0.39341614 0.40652549 0.41342339 0.41609269 0.41502294 0.40949082 0.40173444 0.39154112 0.38118586 0.37212482 0.36414239 0.35654029 0.34879181 0.34057635 0.33063528][0.38318574 0.39783344 0.40406719 0.40660536 0.40492404 0.39904013 0.39024243 0.38036254 0.36980245 0.35970202 0.35085279 0.34216246 0.33344641 0.32419431 0.31418979][0.3738288 0.39009228 0.39614171 0.39792243 0.39514688 0.38814494 0.37817487 0.36689118 0.35477802 0.343577 0.3335261 0.32395408 0.31467095 0.30474716 0.29446879][0.36179617 0.37818164 0.38352421 0.38469294 0.38111705 0.37327129 0.36256894 0.35045749 0.33710647 0.32444817 0.31327257 0.3028864 0.29304051 0.28294852 0.27270508][0.34710577 0.36285087 0.36671913 0.367065 0.36248052 0.35362506 0.34194383 0.32938328 0.31531623 0.30200768 0.29034898 0.27978948 0.27011365 0.26013657 0.2504518][0.32872394 0.34349152 0.34592882 0.34527025 0.33999103 0.33120435 0.31919053 0.30622256 0.29176745 0.27827287 0.2664322 0.2554321 0.24553567 0.23635979 0.2279444][0.30707917 0.32054061 0.3218489 0.32009324 0.3147583 0.30601159 0.29434386 0.28180861 0.2681129 0.25528836 0.24368817 0.23346759 0.22444856 0.21607211 0.2085005][0.28623229 0.29815459 0.29890031 0.296934 0.29197881 0.28385261 0.27322072 0.26122293 0.24841484 0.23653188 0.22573236 0.2161485 0.20770217 0.20023379 0.19347912][0.26440516 0.27435419 0.27392724 0.27185181 0.26761755 0.26074383 0.2519955 0.2419613 0.23126453 0.22121903 0.21183871 0.20348153 0.19607247 0.18956222 0.1837882][0.24529077 0.25384456 0.25238651 0.24977952 0.24558696 0.23968816 0.2322616 0.22409084 0.21571976 0.20785195 0.20052448 0.19375317 0.18768865 0.1823518 0.17765917]]...]
INFO - root - 2017-12-09 19:20:06.509294: step 52510, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 66h:25m:04s remains)
INFO - root - 2017-12-09 19:20:15.193353: step 52520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:08m:47s remains)
INFO - root - 2017-12-09 19:20:23.779743: step 52530, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 65h:24m:46s remains)
INFO - root - 2017-12-09 19:20:32.582455: step 52540, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 65h:53m:56s remains)
INFO - root - 2017-12-09 19:20:41.306644: step 52550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:30m:25s remains)
INFO - root - 2017-12-09 19:20:49.712880: step 52560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:10m:06s remains)
INFO - root - 2017-12-09 19:20:58.347308: step 52570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:32m:59s remains)
INFO - root - 2017-12-09 19:21:06.744826: step 52580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:31m:11s remains)
INFO - root - 2017-12-09 19:21:15.317552: step 52590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 67h:58m:51s remains)
INFO - root - 2017-12-09 19:21:23.728936: step 52600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:42m:10s remains)
2017-12-09 19:21:24.529225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033828598 -0.0033799291 -0.0033793449 -0.0033791673 -0.0033791757 -0.0033794239 -0.003379754 -0.0033800111 -0.0033801266 -0.0033801417 -0.0033800108 -0.0033797598 -0.0033795033 -0.0033792949 -0.0033789345][-0.0033810511 -0.0033778325 -0.0033773258 -0.0033772627 -0.0033773594 -0.0033777361 -0.0033781414 -0.0033782953 -0.0033782313 -0.0033781426 -0.0033779249 -0.0033775608 -0.003377202 -0.0033768269 -0.0033763696][-0.0033810451 -0.0033778667 -0.0033774443 -0.0033774998 -0.0033777298 -0.0033781459 -0.003378521 -0.0033784797 -0.0033782064 -0.0033779235 -0.0033775759 -0.0033771892 -0.0033767621 -0.0033762609 -0.0033758308][-0.0033808621 -0.0033776413 -0.0033772655 -0.0033773591 -0.003377656 -0.0033781144 -0.0033785263 -0.00337861 -0.003378401 -0.0033779275 -0.0033773081 -0.003376676 -0.0033761628 -0.0033757405 -0.0033754429][-0.0033807044 -0.0033773829 -0.0033771549 -0.0033775377 -0.0033779608 -0.0033782709 -0.0033787049 -0.0033788278 -0.0033786006 -0.0033779356 -0.0033770262 -0.0033762797 -0.0033757507 -0.0033754164 -0.0033753759][-0.0033806437 -0.0033773712 -0.0033773466 -0.0033779242 -0.0033783906 -0.0033787936 -0.0033796958 -0.0033801671 -0.0033799438 -0.0033788709 -0.0033773975 -0.0033762006 -0.0033756075 -0.003375486 -0.0033756639][-0.0033805317 -0.0033772867 -0.0033775875 -0.0033785084 -0.0033792972 -0.0033799531 -0.0033813997 -0.0033823464 -0.0033820341 -0.0033806844 -0.0033786814 -0.0033768427 -0.0033758429 -0.003375628 -0.0033758753][-0.0033804718 -0.0033775063 -0.003378035 -0.0033795005 -0.0033809117 -0.0033820632 -0.0033839459 -0.0033853885 -0.0033852186 -0.0033833391 -0.003380395 -0.0033776513 -0.0033760853 -0.0033755915 -0.0033758245][-0.0033801729 -0.0033773968 -0.0033782977 -0.0033803252 -0.0033826365 -0.0033848626 -0.0033873888 -0.0033889094 -0.0033884009 -0.0033858616 -0.0033821987 -0.0033787144 -0.0033765768 -0.003375747 -0.0033758406][-0.0033800188 -0.0033773847 -0.003378669 -0.0033811787 -0.0033842814 -0.0033874093 -0.0033902549 -0.0033916042 -0.0033907068 -0.0033876821 -0.0033835685 -0.0033797417 -0.003377276 -0.0033760881 -0.00337588][-0.0033800327 -0.0033772723 -0.0033786346 -0.0033810998 -0.0033843648 -0.0033876759 -0.0033905176 -0.0033916223 -0.0033905047 -0.003387552 -0.0033836714 -0.0033801205 -0.0033777012 -0.0033764604 -0.0033761002][-0.0033801254 -0.0033771449 -0.0033783342 -0.0033803403 -0.0033831249 -0.0033860109 -0.0033884915 -0.0033896181 -0.0033887213 -0.0033862095 -0.0033829741 -0.0033800248 -0.0033779617 -0.0033768192 -0.003376361][-0.003380469 -0.0033772562 -0.0033782055 -0.0033796155 -0.0033815717 -0.0033836772 -0.0033854195 -0.0033862202 -0.0033855811 -0.0033839203 -0.0033816597 -0.0033795109 -0.0033779419 -0.0033770078 -0.0033765908][-0.0033808595 -0.0033774565 -0.0033780497 -0.0033788688 -0.0033799359 -0.0033810313 -0.0033819743 -0.0033824346 -0.003382094 -0.0033810788 -0.0033797761 -0.0033786457 -0.0033778071 -0.0033772623 -0.0033769729][-0.0033812686 -0.0033777433 -0.0033778951 -0.0033781289 -0.0033784409 -0.0033787505 -0.0033790558 -0.0033791871 -0.0033789647 -0.0033785142 -0.0033780315 -0.0033777328 -0.0033775214 -0.0033773549 -0.00337727]]...]
INFO - root - 2017-12-09 19:21:32.974377: step 52610, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:30m:56s remains)
INFO - root - 2017-12-09 19:21:41.390293: step 52620, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 65h:44m:28s remains)
INFO - root - 2017-12-09 19:21:49.860966: step 52630, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 64h:13m:27s remains)
INFO - root - 2017-12-09 19:21:58.603167: step 52640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:23m:51s remains)
INFO - root - 2017-12-09 19:22:07.377935: step 52650, loss = 0.89, batch loss = 0.68 (8.1 examples/sec; 0.982 sec/batch; 76h:22m:06s remains)
INFO - root - 2017-12-09 19:22:15.804487: step 52660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 67h:13m:23s remains)
INFO - root - 2017-12-09 19:22:24.461931: step 52670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:46m:37s remains)
INFO - root - 2017-12-09 19:22:32.954743: step 52680, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 64h:05m:54s remains)
INFO - root - 2017-12-09 19:22:41.501709: step 52690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:43m:24s remains)
INFO - root - 2017-12-09 19:22:50.168784: step 52700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:53m:21s remains)
2017-12-09 19:22:51.030654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033935278 -0.0033949937 -0.0033971984 -0.0033980343 -0.0033971833 -0.0033944319 -0.0033905637 -0.0033864526 -0.0033826372 -0.0033794416 -0.0033773729 -0.0033768585 -0.0033769761 -0.0033771279 -0.0033768881][-0.0033846546 -0.0033854637 -0.0033874954 -0.0033882943 -0.0033875769 -0.0033852379 -0.0033819834 -0.0033785186 -0.0033753733 -0.0033729025 -0.003371503 -0.0033713854 -0.0033716334 -0.0033718413 -0.00337174][-0.0033778315 -0.0033774357 -0.0033789324 -0.0033795559 -0.003378958 -0.0033770346 -0.0033744532 -0.0033716154 -0.0033692552 -0.003367539 -0.0033666862 -0.0033669353 -0.0033674212 -0.00336776 -0.0033677742][-0.0033724206 -0.0033709037 -0.0033717635 -0.0033720729 -0.0033715765 -0.0033703751 -0.0033686734 -0.0033666748 -0.0033650692 -0.0033640049 -0.003363509 -0.0033636938 -0.0033640643 -0.0033643993 -0.0033645071][-0.0033690678 -0.0033665218 -0.0033668038 -0.0033669462 -0.0033669705 -0.0033668869 -0.0033664526 -0.0033655767 -0.0033647241 -0.0033639609 -0.0033631986 -0.0033626019 -0.0033622119 -0.0033621427 -0.0033621087][-0.0033674182 -0.0033641011 -0.0033640664 -0.0033644021 -0.003365333 -0.0033669663 -0.0033684643 -0.0033691351 -0.0033690373 -0.0033681656 -0.0033663909 -0.0033642622 -0.003362295 -0.0033611795 -0.0033606282][-0.0033670766 -0.0033633781 -0.0033631078 -0.0033637832 -0.0033657649 -0.0033693276 -0.0033728401 -0.0033752234 -0.0033759659 -0.0033749188 -0.0033719251 -0.0033678473 -0.0033639986 -0.0033615569 -0.0033601862][-0.0033669889 -0.0033631797 -0.0033628247 -0.003363916 -0.0033669146 -0.0033720983 -0.0033773654 -0.003381425 -0.0033832972 -0.0033822388 -0.0033781428 -0.0033723598 -0.0033667597 -0.0033627758 -0.0033603965][-0.0033665053 -0.0033628636 -0.0033626193 -0.003364203 -0.0033679658 -0.0033741565 -0.00338078 -0.0033861885 -0.0033889748 -0.0033879383 -0.003383192 -0.00337628 -0.0033693186 -0.0033640745 -0.0033607977][-0.0033654901 -0.0033620535 -0.0033621311 -0.0033642089 -0.0033684631 -0.0033751051 -0.0033824574 -0.0033885648 -0.0033918433 -0.0033908002 -0.0033859028 -0.0033785775 -0.0033709661 -0.0033650105 -0.003361159][-0.0033641597 -0.0033609215 -0.0033613932 -0.0033636161 -0.0033678235 -0.0033740553 -0.0033809396 -0.0033867476 -0.0033900351 -0.0033892954 -0.0033849145 -0.0033781962 -0.0033711365 -0.0033654091 -0.0033614652][-0.0033627802 -0.0033600135 -0.0033606079 -0.0033625532 -0.0033659665 -0.0033708285 -0.0033762455 -0.0033809587 -0.0033838162 -0.0033834353 -0.0033801345 -0.0033749598 -0.0033694219 -0.0033647914 -0.003361359][-0.0033617069 -0.0033590936 -0.0033596738 -0.0033611727 -0.0033637292 -0.0033671258 -0.003371052 -0.0033744532 -0.0033765626 -0.0033763882 -0.0033740578 -0.0033704531 -0.0033665916 -0.0033632626 -0.0033606931][-0.0033610647 -0.003358345 -0.003359003 -0.0033601196 -0.0033618687 -0.003363994 -0.0033665311 -0.0033687013 -0.0033699973 -0.0033699097 -0.0033684496 -0.0033662582 -0.0033637823 -0.0033616025 -0.0033598761][-0.0033610372 -0.0033581115 -0.0033587888 -0.003359487 -0.0033605138 -0.0033616091 -0.0033629532 -0.0033641018 -0.0033647886 -0.0033647674 -0.0033640105 -0.0033628915 -0.0033616093 -0.0033604112 -0.0033593846]]...]
INFO - root - 2017-12-09 19:22:59.492318: step 52710, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 63h:48m:32s remains)
INFO - root - 2017-12-09 19:23:07.974546: step 52720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:48m:51s remains)
INFO - root - 2017-12-09 19:23:16.605805: step 52730, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:52m:57s remains)
INFO - root - 2017-12-09 19:23:25.272476: step 52740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:24m:04s remains)
INFO - root - 2017-12-09 19:23:33.713324: step 52750, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 61h:52m:54s remains)
INFO - root - 2017-12-09 19:23:42.047978: step 52760, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 54h:33m:50s remains)
INFO - root - 2017-12-09 19:23:50.777895: step 52770, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 65h:54m:16s remains)
INFO - root - 2017-12-09 19:23:59.540233: step 52780, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 69h:30m:26s remains)
INFO - root - 2017-12-09 19:24:08.224895: step 52790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:18m:59s remains)
INFO - root - 2017-12-09 19:24:16.905171: step 52800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 68h:59m:37s remains)
2017-12-09 19:24:17.761890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2466594 0.23567702 0.22007293 0.20434088 0.19012405 0.17808697 0.16944884 0.16391225 0.16104008 0.15850897 0.15567842 0.15161812 0.1464922 0.14129755 0.1372849][0.241472 0.23412102 0.22366484 0.2128123 0.20334734 0.19502772 0.18886089 0.18480642 0.18306823 0.18153542 0.17999178 0.17653829 0.17182277 0.166924 0.16238925][0.22182858 0.2181804 0.21145567 0.20502283 0.20111784 0.19730395 0.19542548 0.19362715 0.19321512 0.19341026 0.19301818 0.19084717 0.18730544 0.18330784 0.17951225][0.19991818 0.19797948 0.19342108 0.19040897 0.19012533 0.19060512 0.19214861 0.19240932 0.19380024 0.19533294 0.19660451 0.19632521 0.19475923 0.19239943 0.18964063][0.17151268 0.17107119 0.16837455 0.16809244 0.17086858 0.17491427 0.17930873 0.18253322 0.1856249 0.18890356 0.19197087 0.19433239 0.19533153 0.19474493 0.19354497][0.13521025 0.13648193 0.13605279 0.13905405 0.14539519 0.1537043 0.16189316 0.1677897 0.17305794 0.17764845 0.18260095 0.18685049 0.19038996 0.19192247 0.1919464][0.096005313 0.097919986 0.099903978 0.10641009 0.11683946 0.13021889 0.14319801 0.15323558 0.1603101 0.165125 0.17071457 0.1754338 0.18016329 0.18232562 0.183444][0.061454654 0.064289071 0.068256833 0.077467754 0.090760909 0.10846553 0.12516765 0.13846433 0.14710772 0.15239462 0.15735421 0.16115026 0.1648329 0.16664702 0.16751465][0.034898493 0.039051339 0.045444004 0.056907665 0.072270676 0.091956228 0.10982133 0.12385198 0.13188055 0.13639815 0.13952093 0.14100727 0.14215593 0.14221728 0.14171508][0.017764295 0.022664661 0.030733882 0.043577425 0.060585633 0.080550171 0.097813353 0.11039038 0.11614995 0.11819353 0.11769833 0.11580127 0.11346711 0.11098782 0.10922334][0.0095200306 0.014519172 0.023274038 0.036401976 0.053203695 0.07187672 0.087209657 0.096666336 0.099417083 0.0980268 0.093582459 0.087959886 0.08267688 0.078334175 0.075307734][0.0064896494 0.011589668 0.020573368 0.033591967 0.049703091 0.06618192 0.078748032 0.0850951 0.084791265 0.079863846 0.071892172 0.063084066 0.0549973 0.048874304 0.045003239][0.0052248631 0.010517946 0.019562067 0.032055005 0.046940167 0.061276183 0.071211606 0.074606694 0.071742751 0.064379029 0.054100662 0.043131255 0.033500567 0.026278852 0.021738885][0.0029137118 0.0076434957 0.015724668 0.026723081 0.0395715 0.051520437 0.059307769 0.060920909 0.056918282 0.048859365 0.038428668 0.027635893 0.018394709 0.011573556 0.0071827937][0.00051671849 0.0039507709 0.010188949 0.018813184 0.029090272 0.038509723 0.044252966 0.04478398 0.040690511 0.033430897 0.024590308 0.015827466 0.00864798 0.0035855125 0.00038741622]]...]
INFO - root - 2017-12-09 19:24:26.262075: step 52810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:24m:31s remains)
INFO - root - 2017-12-09 19:24:34.982062: step 52820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:52m:17s remains)
INFO - root - 2017-12-09 19:24:43.606689: step 52830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 67h:44m:50s remains)
INFO - root - 2017-12-09 19:24:52.332110: step 52840, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 67h:11m:16s remains)
INFO - root - 2017-12-09 19:25:01.035385: step 52850, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:35m:50s remains)
INFO - root - 2017-12-09 19:25:09.402897: step 52860, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:34m:48s remains)
INFO - root - 2017-12-09 19:25:17.862948: step 52870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:19m:50s remains)
INFO - root - 2017-12-09 19:25:26.620090: step 52880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:53m:04s remains)
INFO - root - 2017-12-09 19:25:35.330047: step 52890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:22m:24s remains)
INFO - root - 2017-12-09 19:25:44.089685: step 52900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:30m:13s remains)
2017-12-09 19:25:44.936707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003393498 -0.0033948466 -0.0033985879 -0.0033976771 -0.0033897951 -0.00338017 -0.00337827 -0.0033844607 -0.0033928275 -0.0033988527 -0.0034037582 -0.0034086544 -0.0034126835 -0.003415005 -0.0034163094][-0.0033922496 -0.0033934826 -0.0033954612 -0.0033805431 -0.0033401083 -0.0032943278 -0.0032739881 -0.00328797 -0.0033162527 -0.0033417852 -0.0033624391 -0.0033821589 -0.0033987472 -0.003408754 -0.0034126954][-0.0033916312 -0.0033922666 -0.0033885662 -0.0033475119 -0.0032622549 -0.0031687263 -0.00312124 -0.0031459501 -0.0032023061 -0.0032567291 -0.003300278 -0.0033419959 -0.0033776865 -0.0033986438 -0.0034064364][-0.0033909313 -0.0033903918 -0.0033779568 -0.0033059677 -0.0031728998 -0.0030344303 -0.0029634088 -0.0029948903 -0.0030755915 -0.003159147 -0.0032283568 -0.003294738 -0.0033522258 -0.0033868176 -0.0033998687][-0.0033903834 -0.0033893925 -0.0033743863 -0.0032865235 -0.0031287288 -0.0029656487 -0.0028931596 -0.0029341802 -0.0030257097 -0.0031215553 -0.0032020209 -0.0032809444 -0.0033467617 -0.0033848512 -0.0033982955][-0.0033901508 -0.003389145 -0.0033812718 -0.003308998 -0.0031626325 -0.0030033682 -0.0029365264 -0.0029770331 -0.0030629416 -0.0031452002 -0.0032177281 -0.003293559 -0.0033557029 -0.00339007 -0.0034003577][-0.0033899606 -0.003388437 -0.0033855324 -0.0033471321 -0.0032540106 -0.0031449215 -0.0030975002 -0.0031268306 -0.0031888385 -0.0032389394 -0.0032846471 -0.0033358918 -0.0033782406 -0.0033997705 -0.0034033053][-0.0033898696 -0.0033878274 -0.0033870228 -0.0033716857 -0.0033322931 -0.0032864558 -0.003265677 -0.0032803407 -0.0033095034 -0.00333135 -0.003352507 -0.0033760327 -0.0033950156 -0.0034032129 -0.0034024874][-0.0033896985 -0.0033873674 -0.0033871054 -0.0033849503 -0.0033769782 -0.0033678443 -0.0033638363 -0.0033689605 -0.0033773535 -0.0033835084 -0.0033895434 -0.0033956647 -0.0034001146 -0.003400787 -0.0033987893][-0.0033896109 -0.0033870116 -0.0033863788 -0.0033858712 -0.0033856779 -0.0033860642 -0.0033869215 -0.0033885268 -0.0033905194 -0.0033928282 -0.003394556 -0.0033957595 -0.0033965288 -0.0033958314 -0.0033941569][-0.0033896668 -0.0033868654 -0.0033860074 -0.0033851275 -0.0033846979 -0.0033848602 -0.00338536 -0.0033864386 -0.0033877254 -0.0033891129 -0.0033902945 -0.0033910573 -0.0033917276 -0.003391183 -0.0033901534][-0.0033897092 -0.003386911 -0.0033859627 -0.0033849298 -0.0033842803 -0.0033843815 -0.0033845452 -0.0033851638 -0.0033859846 -0.0033868067 -0.0033875727 -0.003387965 -0.0033883122 -0.0033880577 -0.0033875285][-0.0033901357 -0.0033874006 -0.0033862642 -0.0033849666 -0.0033841406 -0.0033839536 -0.0033839969 -0.0033843669 -0.0033850197 -0.0033856507 -0.003386291 -0.0033865601 -0.0033866889 -0.0033865145 -0.0033861098][-0.0033908864 -0.0033881865 -0.0033867322 -0.0033852779 -0.0033842584 -0.0033837946 -0.0033836691 -0.0033840579 -0.0033846674 -0.0033852817 -0.0033857715 -0.0033859233 -0.0033858104 -0.003385629 -0.0033854262][-0.0033916687 -0.0033889604 -0.003387409 -0.0033858495 -0.0033846279 -0.0033837489 -0.0033835494 -0.0033840197 -0.0033846686 -0.0033852356 -0.0033855184 -0.0033854411 -0.0033851606 -0.0033848977 -0.0033848945]]...]
INFO - root - 2017-12-09 19:25:53.410746: step 52910, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 69h:00m:15s remains)
INFO - root - 2017-12-09 19:26:02.118649: step 52920, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 68h:33m:13s remains)
INFO - root - 2017-12-09 19:26:10.771058: step 52930, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 68h:43m:16s remains)
INFO - root - 2017-12-09 19:26:19.553894: step 52940, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 69h:28m:50s remains)
INFO - root - 2017-12-09 19:26:28.179016: step 52950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:01m:55s remains)
INFO - root - 2017-12-09 19:26:36.787790: step 52960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:10m:41s remains)
INFO - root - 2017-12-09 19:26:45.263369: step 52970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 67h:08m:48s remains)
INFO - root - 2017-12-09 19:26:53.879980: step 52980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:12m:43s remains)
INFO - root - 2017-12-09 19:27:02.495642: step 52990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:17m:13s remains)
INFO - root - 2017-12-09 19:27:11.114414: step 53000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:03m:01s remains)
2017-12-09 19:27:12.001129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003391695 -0.0033904084 -0.0033913809 -0.0033920445 -0.0033448837 -0.0030620757 -0.0022246311 -0.00077523268 0.0012827686 0.0036541952 0.0056791427 0.0063605504 0.0048842169 0.0021506341 -0.00067004724][-0.003389739 -0.0033883792 -0.0033896349 -0.0033815752 -0.0032145861 -0.0025298232 -0.00082298438 0.0019856507 0.00581431 0.0099689765 0.013106612 0.013610646 0.010625443 0.0058175381 0.0010750487][-0.0033846309 -0.00338249 -0.0033836649 -0.0033217384 -0.0028189733 -0.0012205641 0.0022609162 0.0077236071 0.014865881 0.022202017 0.027239591 0.027456606 0.022066 0.013783926 0.0055830088][-0.0032610244 -0.0032550839 -0.0032843694 -0.0030290077 -0.0017315013 0.0016305374 0.0081414618 0.017823145 0.029856885 0.04149466 0.048722751 0.048217263 0.0395377 0.026516331 0.01331735][-0.0023308494 -0.0021433579 -0.0021599997 -0.0016895062 0.00075100758 0.0066797929 0.017311893 0.032372 0.050125778 0.066259533 0.075293787 0.073487714 0.06098219 0.042630665 0.023710703][-0.0011271341 -0.00055707712 -0.00020782999 0.0010486895 0.005221366 0.014178341 0.029184805 0.049739249 0.073044591 0.093279146 0.10360748 0.10013666 0.083736025 0.059997849 0.035151202][0.00011274382 0.0010911261 0.002042643 0.0042474503 0.010118203 0.021795519 0.04053938 0.065725766 0.093536578 0.11693989 0.12788491 0.12268564 0.10312538 0.075198822 0.045594815][0.0009655843 0.0023454993 0.0039596641 0.007213166 0.014523832 0.027967243 0.048926085 0.076781742 0.10693531 0.13159247 0.14213955 0.13541786 0.11396915 0.083787106 0.051693514][0.0011566987 0.0028217777 0.0049327482 0.0088124741 0.016822536 0.030802703 0.052027959 0.079967752 0.10966374 0.13320567 0.14217782 0.13439471 0.11288489 0.083161704 0.051578894][0.00063482882 0.002358685 0.0047026947 0.0086232331 0.016222034 0.029156709 0.0485501 0.073860273 0.10016208 0.12040665 0.12726317 0.11938925 0.099760912 0.073181495 0.045159154][-0.00066621648 0.0007762399 0.002925202 0.0064976607 0.012977853 0.023580626 0.039296031 0.059799671 0.080865674 0.096827567 0.10170602 0.094790757 0.078588426 0.057011709 0.034561992][-0.0019042699 -0.00095933885 0.00066385115 0.0033417947 0.008034993 0.015625075 0.026873879 0.041594014 0.056739233 0.068284243 0.071743712 0.066616639 0.054687537 0.038881727 0.022672283][-0.0028323033 -0.0023331868 -0.0014273792 0.00030054408 0.0033536947 0.0081449766 0.015204223 0.024468526 0.034059945 0.041421253 0.043711971 0.040500075 0.032756589 0.022437377 0.012024012][-0.0032773667 -0.0031222184 -0.0027386735 -0.001955329 -0.00041539641 0.0021670309 0.0059692236 0.010952241 0.01617655 0.02025073 0.021599984 0.019862698 0.015525155 0.0097497264 0.0040774476][-0.003389535 -0.0033731975 -0.0032960577 -0.0030676879 -0.0025362689 -0.001534846 9.0083107e-05 0.0022778159 0.0045465585 0.0063480986 0.007030555 0.006325081 0.0043444224 0.0017102633 -0.00072215078]]...]
INFO - root - 2017-12-09 19:27:20.339913: step 53010, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 67h:13m:33s remains)
INFO - root - 2017-12-09 19:27:28.912573: step 53020, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:35m:42s remains)
INFO - root - 2017-12-09 19:27:37.506592: step 53030, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:16m:52s remains)
INFO - root - 2017-12-09 19:27:45.943798: step 53040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 65h:08m:40s remains)
INFO - root - 2017-12-09 19:27:54.578245: step 53050, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 65h:58m:11s remains)
INFO - root - 2017-12-09 19:28:03.136941: step 53060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:53m:26s remains)
INFO - root - 2017-12-09 19:28:11.751172: step 53070, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 68h:02m:19s remains)
INFO - root - 2017-12-09 19:28:20.455243: step 53080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 69h:17m:05s remains)
INFO - root - 2017-12-09 19:28:29.154398: step 53090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:23m:05s remains)
INFO - root - 2017-12-09 19:28:37.762421: step 53100, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 67h:40m:46s remains)
2017-12-09 19:28:38.629476: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031437427 0.034957513 0.03777232 0.038958117 0.039357767 0.037209224 0.033670995 0.028800758 0.023016829 0.017071927 0.011782186 0.0070375735 0.0032200136 0.00046851672 -0.0010875489][0.051289074 0.058727305 0.0651329 0.06876082 0.070604868 0.068121515 0.062474079 0.05403671 0.043778766 0.032770015 0.022298109 0.013171948 0.0064098667 0.0018385758 -0.00066185067][0.08403179 0.097363643 0.1088252 0.11572959 0.11885457 0.11548258 0.10709655 0.094460711 0.07905452 0.061975643 0.044583354 0.028666334 0.016030835 0.0071158018 0.0017951855][0.1357096 0.15818284 0.17683077 0.18862136 0.19318403 0.18787143 0.17459868 0.15513432 0.13177776 0.10603331 0.079140149 0.053937357 0.03320026 0.01785739 0.0076391306][0.19805065 0.23017123 0.25618377 0.27291539 0.27921715 0.27321234 0.25643912 0.23143163 0.20072883 0.16626878 0.12877575 0.092099153 0.059932455 0.034855761 0.017050195][0.25646585 0.29624641 0.32790104 0.34842178 0.35645041 0.350637 0.33228374 0.30454898 0.2694923 0.22941191 0.18397255 0.13769326 0.094379716 0.058309317 0.030853784][0.2943736 0.33801544 0.37195987 0.39418846 0.40411785 0.40005004 0.38296461 0.3560957 0.32074437 0.27926865 0.230207 0.17838247 0.12701188 0.081909291 0.045532048][0.30097702 0.34322941 0.37587595 0.39823666 0.40993524 0.40860316 0.39544487 0.37271172 0.34109798 0.30242464 0.25449225 0.20222212 0.14790834 0.098177761 0.056187849][0.27675769 0.31344146 0.34077343 0.36026767 0.37256798 0.3746092 0.36651477 0.34956244 0.32393867 0.29102498 0.24809653 0.20024611 0.14888404 0.10057317 0.058510929][0.22935982 0.2586292 0.27902827 0.29369843 0.30386519 0.30732697 0.30389354 0.29337907 0.27500433 0.24915034 0.21389025 0.17374074 0.12974089 0.087872446 0.050975308][0.16803667 0.18944637 0.20303594 0.21260735 0.21962227 0.2226177 0.22136669 0.21577175 0.20445567 0.18662173 0.16090307 0.13078724 0.09740109 0.06528233 0.036869984][0.10595568 0.11944972 0.12744771 0.13277332 0.13656443 0.13823229 0.13776533 0.13481566 0.12838703 0.11750074 0.10130261 0.081824943 0.060186394 0.039242886 0.020874659][0.054378904 0.061485425 0.065501638 0.067956381 0.06952142 0.069967024 0.069361709 0.067558825 0.064046875 0.058124267 0.049474929 0.039181404 0.02794197 0.017075393 0.0077649187][0.019245189 0.02232931 0.024137456 0.025124865 0.025558855 0.025412569 0.024729326 0.023509039 0.021644214 0.018985322 0.015363935 0.011240293 0.0069576856 0.0030126374 -0.00015696394][0.00194274 0.002868057 0.0035334064 0.0039500147 0.00413897 0.004080141 0.0038082621 0.0033254654 0.0026526295 0.0018119712 0.00081056147 -0.00021191849 -0.0012244871 -0.0020899912 -0.0027432777]]...]
INFO - root - 2017-12-09 19:28:47.175303: step 53110, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 67h:34m:20s remains)
INFO - root - 2017-12-09 19:28:55.907810: step 53120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:06m:58s remains)
INFO - root - 2017-12-09 19:29:04.548089: step 53130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:08m:12s remains)
INFO - root - 2017-12-09 19:29:13.224401: step 53140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:01m:35s remains)
INFO - root - 2017-12-09 19:29:21.951988: step 53150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:17m:23s remains)
INFO - root - 2017-12-09 19:29:30.456309: step 53160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:55m:06s remains)
INFO - root - 2017-12-09 19:29:39.028044: step 53170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:25m:53s remains)
INFO - root - 2017-12-09 19:29:47.876051: step 53180, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.922 sec/batch; 71h:30m:07s remains)
INFO - root - 2017-12-09 19:29:56.633687: step 53190, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:48m:53s remains)
INFO - root - 2017-12-09 19:30:05.381827: step 53200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:03m:31s remains)
2017-12-09 19:30:06.430282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003357145 -0.003353595 -0.0033528926 -0.00335258 -0.003352033 -0.003351433 -0.0033514597 -0.0033518062 -0.0033526123 -0.0033540195 -0.0033558712 -0.0033575743 -0.0033589776 -0.0033594852 -0.0033588773][-0.0033570367 -0.0033535329 -0.0033527585 -0.0033522241 -0.003350981 -0.0033493713 -0.0033486928 -0.0033485165 -0.0033487843 -0.0033499571 -0.0033517824 -0.0033534588 -0.0033548526 -0.0033554856 -0.0033553881][-0.0033595185 -0.003356786 -0.0033561417 -0.003354898 -0.0033526185 -0.0033499086 -0.0033483512 -0.0033474923 -0.0033472171 -0.0033480891 -0.0033496956 -0.0033512397 -0.0033524118 -0.0033531119 -0.0033534854][-0.0033621257 -0.0033603061 -0.0033600575 -0.0033583979 -0.0033553848 -0.0033518253 -0.0033493773 -0.0033478257 -0.0033469279 -0.0033470883 -0.0033481261 -0.0033493787 -0.0033503852 -0.0033513168 -0.0033520677][-0.0033638384 -0.0033627525 -0.0033634582 -0.0033624561 -0.0033597816 -0.0033561701 -0.003353344 -0.0033511911 -0.0033493864 -0.0033482765 -0.00334827 -0.0033490008 -0.0033497491 -0.0033508528 -0.0033519419][-0.0033640671 -0.0033636778 -0.0033655697 -0.0033662003 -0.0033652729 -0.0033630214 -0.003360338 -0.0033574416 -0.0033544195 -0.0033515962 -0.0033499736 -0.0033495643 -0.0033498947 -0.0033509915 -0.0033523829][-0.0033629651 -0.0033631425 -0.0033666561 -0.0033695879 -0.0033713165 -0.0033713474 -0.0033693705 -0.0033654755 -0.0033608552 -0.0033561678 -0.0033525887 -0.0033505966 -0.003350294 -0.0033512919 -0.0033531091][-0.0033610137 -0.0033616149 -0.0033667374 -0.0033723714 -0.0033772897 -0.003380131 -0.0033792341 -0.0033749158 -0.0033685665 -0.0033615623 -0.0033557003 -0.0033520916 -0.0033509042 -0.0033517 -0.0033538188][-0.0033586053 -0.003359542 -0.0033657518 -0.0033737018 -0.0033815105 -0.0033871843 -0.0033880833 -0.0033842453 -0.0033769617 -0.0033678974 -0.0033596703 -0.0033541948 -0.003351965 -0.0033521997 -0.0033542125][-0.0033564458 -0.0033572945 -0.0033639856 -0.0033730629 -0.0033825224 -0.0033902086 -0.0033929432 -0.0033902638 -0.0033830462 -0.0033730085 -0.0033634391 -0.0033565017 -0.003353151 -0.0033524013 -0.0033539054][-0.0033550439 -0.003355592 -0.0033622831 -0.0033712243 -0.0033807247 -0.0033888358 -0.003392718 -0.0033912321 -0.0033848574 -0.0033751538 -0.0033655341 -0.0033578479 -0.0033535247 -0.0033519214 -0.0033527382][-0.0033541538 -0.0033542472 -0.0033599676 -0.0033675949 -0.0033759351 -0.0033831862 -0.0033873916 -0.0033870237 -0.0033819471 -0.0033739069 -0.0033654256 -0.0033581196 -0.00335337 -0.0033512108 -0.0033512679][-0.0033536414 -0.0033527333 -0.0033568868 -0.0033626948 -0.0033694934 -0.0033755237 -0.0033793065 -0.0033797645 -0.0033764436 -0.0033704378 -0.0033634577 -0.00335725 -0.0033529638 -0.0033506646 -0.0033501766][-0.003353599 -0.0033515494 -0.0033540665 -0.0033578305 -0.0033628268 -0.0033673898 -0.0033705763 -0.0033716299 -0.0033699984 -0.0033658666 -0.0033606456 -0.0033558633 -0.0033524227 -0.0033501543 -0.0033494597][-0.0033537878 -0.0033507082 -0.003351998 -0.0033540225 -0.0033571566 -0.0033602938 -0.0033626908 -0.0033639474 -0.0033633728 -0.0033609651 -0.0033575809 -0.003354379 -0.0033518393 -0.00334995 -0.0033491724]]...]
INFO - root - 2017-12-09 19:30:14.671661: step 53210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:50m:16s remains)
INFO - root - 2017-12-09 19:30:23.410562: step 53220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:07m:24s remains)
INFO - root - 2017-12-09 19:30:32.155731: step 53230, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 69h:48m:52s remains)
INFO - root - 2017-12-09 19:30:40.880511: step 53240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:20m:18s remains)
INFO - root - 2017-12-09 19:30:49.574351: step 53250, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 65h:24m:14s remains)
INFO - root - 2017-12-09 19:30:58.156663: step 53260, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 69h:09m:20s remains)
INFO - root - 2017-12-09 19:31:06.804301: step 53270, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 69h:32m:23s remains)
INFO - root - 2017-12-09 19:31:15.475172: step 53280, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:14m:51s remains)
INFO - root - 2017-12-09 19:31:24.285139: step 53290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 68h:59m:59s remains)
INFO - root - 2017-12-09 19:31:32.891502: step 53300, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 65h:15m:16s remains)
2017-12-09 19:31:33.740282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00341295 -0.003411785 -0.0034119892 -0.0034122774 -0.0034124185 -0.0034125743 -0.0034128672 -0.0034133028 -0.0034135222 -0.0034136386 -0.0034135685 -0.0034133375 -0.0034131457 -0.0034129152 -0.0034125603][-0.0034122944 -0.0034110369 -0.0034112008 -0.0034114334 -0.0034115629 -0.003411775 -0.0034120951 -0.0034126262 -0.0034129356 -0.0034132637 -0.0034132707 -0.003413172 -0.0034130786 -0.0034128462 -0.0034125][-0.0034128351 -0.0034111624 -0.0034107815 -0.0034106227 -0.00341042 -0.0034103971 -0.0034106355 -0.0034112355 -0.0034115743 -0.0034118481 -0.0034118439 -0.003411798 -0.0034118711 -0.0034119878 -0.00341209][-0.0034126211 -0.0034100185 -0.0034084171 -0.0034072509 -0.0034062604 -0.0034057691 -0.0034056946 -0.0034062818 -0.0034068378 -0.0034072655 -0.0034074083 -0.0034074695 -0.003407967 -0.0034088597 -0.0034100506][-0.0034101224 -0.0034060988 -0.0034031398 -0.0034009747 -0.00339924 -0.0033981707 -0.0033978943 -0.0033983707 -0.0033987875 -0.0033992257 -0.003399462 -0.0034000294 -0.0034015677 -0.0034035826 -0.003405564][-0.00340657 -0.0034021505 -0.0033986894 -0.0033961001 -0.0033940598 -0.0033928961 -0.0033927059 -0.0033931793 -0.0033934636 -0.0033935884 -0.0033938955 -0.0033948754 -0.00339681 -0.0033991127 -0.003401055][-0.0034042124 -0.0034001917 -0.0033969565 -0.0033945525 -0.0033926587 -0.0033917124 -0.0033915024 -0.0033916575 -0.0033914538 -0.0033912146 -0.0033913697 -0.0033922782 -0.0033940445 -0.003396173 -0.0033979029][-0.0034025544 -0.0033991018 -0.0033964796 -0.0033947846 -0.0033936223 -0.0033930598 -0.003392885 -0.0033926615 -0.003391695 -0.0033905536 -0.0033901962 -0.0033909436 -0.0033925641 -0.0033948619 -0.0033971157][-0.0033997637 -0.003397 -0.0033955832 -0.0033951616 -0.0033950964 -0.0033952103 -0.0033951481 -0.003394553 -0.0033933155 -0.0033918039 -0.003390891 -0.0033909958 -0.003392309 -0.0033948699 -0.0033978219][-0.0033958724 -0.0033939672 -0.0033941746 -0.003395553 -0.0033970287 -0.0033980876 -0.0033985581 -0.0033983025 -0.0033972564 -0.0033958743 -0.0033948966 -0.0033948808 -0.0033959697 -0.0033983975 -0.0034013051][-0.0033927441 -0.0033922056 -0.0033942931 -0.0033972703 -0.0033999083 -0.0034019838 -0.0034031468 -0.003403397 -0.0034028878 -0.0034020904 -0.0034016115 -0.0034018627 -0.0034028541 -0.0034046143 -0.0034068029][-0.0033917853 -0.003392146 -0.0033952862 -0.003399323 -0.0034030378 -0.0034058497 -0.0034077121 -0.0034085677 -0.003408714 -0.003408567 -0.0034088432 -0.0034095498 -0.0034102376 -0.0034110397 -0.0034122241][-0.0033931285 -0.003393708 -0.0033970424 -0.0034014469 -0.0034057715 -0.0034093731 -0.0034120744 -0.003413758 -0.0034145555 -0.0034149361 -0.0034151517 -0.0034154097 -0.0034157105 -0.0034162859 -0.0034169033][-0.0033959087 -0.0033968268 -0.0034002676 -0.0034049321 -0.0034097985 -0.0034139969 -0.0034171361 -0.0034186889 -0.0034193504 -0.0034196225 -0.0034194165 -0.0034191366 -0.0034191196 -0.0034193222 -0.0034193965][-0.003401835 -0.0034028434 -0.0034057575 -0.0034097403 -0.0034138749 -0.0034174952 -0.0034200344 -0.0034211522 -0.0034214274 -0.00342138 -0.0034211343 -0.003420878 -0.0034206826 -0.003420328 -0.0034197273]]...]
INFO - root - 2017-12-09 19:31:42.425596: step 53310, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 69h:45m:46s remains)
INFO - root - 2017-12-09 19:31:51.225048: step 53320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:16m:14s remains)
INFO - root - 2017-12-09 19:31:59.781986: step 53330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:09m:39s remains)
INFO - root - 2017-12-09 19:32:08.408738: step 53340, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 68h:06m:04s remains)
INFO - root - 2017-12-09 19:32:17.200289: step 53350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:39m:36s remains)
INFO - root - 2017-12-09 19:32:25.667785: step 53360, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 67h:38m:34s remains)
INFO - root - 2017-12-09 19:32:34.137176: step 53370, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:31m:42s remains)
INFO - root - 2017-12-09 19:32:42.824617: step 53380, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 69h:42m:12s remains)
INFO - root - 2017-12-09 19:32:51.467962: step 53390, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:31m:43s remains)
INFO - root - 2017-12-09 19:33:00.119029: step 53400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:29m:08s remains)
2017-12-09 19:33:00.940303: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077014565 0.080401562 0.081848815 0.0813242 0.07925038 0.076461278 0.073200308 0.069244385 0.065523587 0.062877 0.061150238 0.060372382 0.059545159 0.05809186 0.057034656][0.076189354 0.079767317 0.081761912 0.081731133 0.080226511 0.077250749 0.073318288 0.0683699 0.06382902 0.060635813 0.058444805 0.056889065 0.055329561 0.053703751 0.052163579][0.067675382 0.070135832 0.071340941 0.070875041 0.06933336 0.066611707 0.06302765 0.058542561 0.054128107 0.05065963 0.048318811 0.046777684 0.0454195 0.044053458 0.042614147][0.057972029 0.058360618 0.057568204 0.055471029 0.052973364 0.050055563 0.04690909 0.043382917 0.040068056 0.037430666 0.035686482 0.034713689 0.0340134 0.033331823 0.032433979][0.048645746 0.046908468 0.044120092 0.040402897 0.036709372 0.033279561 0.030363025 0.027582239 0.025290577 0.023543131 0.022738324 0.022618879 0.022628339 0.022586213 0.022172319][0.040926907 0.037408285 0.032775667 0.027637742 0.022905111 0.019006761 0.016001012 0.013596732 0.011913681 0.01099658 0.010970531 0.011375506 0.011796238 0.012092212 0.011998402][0.036027417 0.03130234 0.025598424 0.019596845 0.01439169 0.01031947 0.0072899684 0.0051263971 0.0037464702 0.0031656057 0.0032339164 0.0034979873 0.0037548488 0.0038277491 0.0036453365][0.035295963 0.029304655 0.0225343 0.016221097 0.011103866 0.0074066808 0.0048864214 0.0030992541 0.0018771423 0.0010964719 0.00078333518 0.00054515689 0.00021084445 -0.00025424827 -0.00078448444][0.036529958 0.030439274 0.02324236 0.016535098 0.011573662 0.0085812919 0.0070165358 0.0060576238 0.00538955 0.0045512803 0.0036843333 0.0025427185 0.0012222829 -0.00022188528 -0.0015219132][0.03724445 0.032118898 0.025505867 0.018969687 0.014097445 0.011473034 0.010744891 0.010668968 0.010745859 0.010116246 0.0090702921 0.007224679 0.0048321029 0.0022166513 -0.00012046006][0.038427562 0.034587149 0.028808905 0.022498485 0.017608063 0.01507359 0.014771657 0.015364243 0.016184488 0.015988072 0.014901005 0.01243208 0.0090296268 0.00521437 0.0017495637][0.039397418 0.037235197 0.032797419 0.027418848 0.022980768 0.020635717 0.020752763 0.021728238 0.022722909 0.022357939 0.020729115 0.017456178 0.012913685 0.007886718 0.003396773][0.040366214 0.039239243 0.035683215 0.031043502 0.02699401 0.025210069 0.025727602 0.026893523 0.02788881 0.027357293 0.025263511 0.021172414 0.015667511 0.0097289793 0.0045087514][0.037503872 0.036802419 0.033978622 0.030329918 0.027352367 0.026577363 0.027786708 0.029237969 0.030028267 0.029108165 0.026514584 0.022014026 0.01614261 0.0099776089 0.0046926923][0.031158475 0.030242519 0.027865926 0.025120063 0.02340899 0.023847628 0.025873357 0.027862633 0.028697265 0.027581995 0.02462359 0.01993873 0.014182081 0.0084615014 0.0037340762]]...]
INFO - root - 2017-12-09 19:33:09.467379: step 53410, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 70h:06m:29s remains)
INFO - root - 2017-12-09 19:33:18.134920: step 53420, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 68h:37m:11s remains)
INFO - root - 2017-12-09 19:33:26.801062: step 53430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:03m:09s remains)
INFO - root - 2017-12-09 19:33:35.544975: step 53440, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 69h:55m:04s remains)
INFO - root - 2017-12-09 19:33:44.153699: step 53450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:20m:55s remains)
INFO - root - 2017-12-09 19:33:52.669393: step 53460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:20m:22s remains)
INFO - root - 2017-12-09 19:34:01.135666: step 53470, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 66h:48m:03s remains)
INFO - root - 2017-12-09 19:34:09.674268: step 53480, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 64h:31m:18s remains)
INFO - root - 2017-12-09 19:34:18.214315: step 53490, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:56m:59s remains)
INFO - root - 2017-12-09 19:34:26.945226: step 53500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:48m:38s remains)
2017-12-09 19:34:27.840788: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14530967 0.1410754 0.13577543 0.1302176 0.12542331 0.11956748 0.11304116 0.1062251 0.099988647 0.093690686 0.086656205 0.0786904 0.06934277 0.057503659 0.044918891][0.17310078 0.16868263 0.16363299 0.15873004 0.15471773 0.15003966 0.14471638 0.13862355 0.13256423 0.12580533 0.1174861 0.10734635 0.094954155 0.079231091 0.061926827][0.20158824 0.19792418 0.19376937 0.18951477 0.186398 0.18307711 0.17862475 0.17300527 0.1669212 0.1598686 0.15042169 0.13840084 0.12348269 0.10418779 0.081967451][0.22726212 0.22462822 0.22170451 0.218328 0.21582405 0.21312207 0.20941912 0.20474699 0.19858049 0.19156361 0.18177752 0.16872148 0.15175612 0.12913033 0.10277601][0.24537003 0.24433096 0.24271131 0.24073665 0.23974946 0.23753807 0.23434487 0.2298108 0.22346444 0.21583696 0.20528875 0.19218099 0.17394902 0.14973512 0.12076851][0.25359869 0.25415727 0.25369212 0.25281641 0.25278968 0.2514368 0.24942291 0.24550493 0.23924267 0.23152655 0.22070284 0.20680042 0.18734445 0.16216522 0.13177721][0.25302735 0.25520074 0.25554824 0.25563011 0.25658044 0.25610366 0.25553185 0.25268945 0.2476376 0.24039163 0.23006801 0.21636206 0.19671468 0.17140624 0.14066431][0.24693689 0.25066888 0.25154251 0.25215402 0.25325158 0.25326979 0.25303966 0.25105244 0.2467545 0.24087462 0.23195355 0.21930696 0.20088826 0.17690517 0.14789939][0.23962368 0.24390492 0.24448179 0.24524675 0.24633878 0.24647716 0.24689916 0.24563016 0.24201368 0.23702289 0.22955818 0.21850622 0.20171423 0.18025096 0.15450405][0.23246191 0.23769283 0.23823735 0.2389788 0.23984456 0.23979776 0.24030016 0.23866034 0.23562691 0.23109713 0.22456582 0.21538043 0.20082916 0.1830789 0.16152993][0.22157413 0.22866051 0.23033679 0.23174554 0.23278289 0.23355187 0.23477849 0.23341094 0.2308488 0.22700955 0.22178347 0.21378824 0.20191191 0.18762892 0.17013811][0.20786883 0.21629591 0.21898034 0.22187395 0.22448006 0.22669412 0.22913116 0.2292383 0.22803642 0.22520666 0.22127283 0.21484274 0.20540306 0.19404359 0.18036537][0.19409233 0.20302622 0.20633917 0.21014009 0.21382836 0.21759482 0.22144733 0.22276534 0.2230904 0.22182654 0.21975444 0.21501839 0.20805483 0.19976152 0.18969637][0.17919838 0.18885726 0.19313262 0.19795264 0.20290352 0.20764703 0.21264367 0.2148913 0.21658199 0.2160778 0.21549314 0.21251898 0.20805022 0.20270921 0.1958387][0.15989232 0.17039596 0.17605601 0.18239924 0.18880719 0.19423431 0.19980308 0.20296474 0.20584996 0.20613854 0.20628472 0.20492269 0.20247607 0.19924843 0.1947479]]...]
INFO - root - 2017-12-09 19:34:36.311959: step 53510, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.753 sec/batch; 58h:22m:12s remains)
INFO - root - 2017-12-09 19:34:44.922231: step 53520, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 68h:03m:41s remains)
INFO - root - 2017-12-09 19:34:53.590442: step 53530, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 63h:53m:39s remains)
INFO - root - 2017-12-09 19:35:02.409411: step 53540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:07m:24s remains)
INFO - root - 2017-12-09 19:35:11.164873: step 53550, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 68h:10m:09s remains)
INFO - root - 2017-12-09 19:35:19.787188: step 53560, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 69h:18m:25s remains)
INFO - root - 2017-12-09 19:35:28.316708: step 53570, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 67h:41m:08s remains)
INFO - root - 2017-12-09 19:35:36.918961: step 53580, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:14m:52s remains)
INFO - root - 2017-12-09 19:35:45.432845: step 53590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 68h:32m:47s remains)
INFO - root - 2017-12-09 19:35:54.072130: step 53600, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:17m:05s remains)
2017-12-09 19:35:54.957875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034013081 -0.0034004177 -0.0034003633 -0.0034003195 -0.0034002981 -0.0034002622 -0.0034001532 -0.0034001095 -0.0034000953 -0.0034001502 -0.0034002671 -0.0034004394 -0.0034005626 -0.0034005793 -0.0034005193][-0.0034001749 -0.0033993344 -0.0033994676 -0.0033995092 -0.0033994277 -0.003399261 -0.0033990582 -0.0033989206 -0.0033988231 -0.0033987819 -0.003398807 -0.0033988282 -0.003398851 -0.0033988054 -0.003398696][-0.0034017377 -0.0034013346 -0.0034018462 -0.0034020818 -0.0034020257 -0.003401868 -0.0034016583 -0.0034013491 -0.0034009698 -0.0034005882 -0.0034001064 -0.00339961 -0.0033991628 -0.0033988329 -0.0033985891][-0.0034041738 -0.0034040823 -0.0034047342 -0.003404991 -0.0034050453 -0.0034050066 -0.0034047931 -0.0034043845 -0.0034038241 -0.0034029994 -0.0034019784 -0.0034008732 -0.0033998822 -0.0033991423 -0.0033986568][-0.003406585 -0.0034063207 -0.0034064958 -0.0034063535 -0.0034060371 -0.0034059554 -0.0034058576 -0.0034058371 -0.0034055063 -0.0034047724 -0.0034037158 -0.0034024871 -0.0034011374 -0.003399983 -0.003399062][-0.0034086304 -0.0034077589 -0.003407178 -0.0034061288 -0.003405248 -0.0034048546 -0.0034048273 -0.0034051032 -0.0034052124 -0.0034049256 -0.0034042471 -0.0034031656 -0.0034018273 -0.0034005283 -0.0033993053][-0.0034109449 -0.003409683 -0.003408717 -0.0034072977 -0.0034061337 -0.0034058173 -0.0034059186 -0.0034062909 -0.0034066166 -0.0034063614 -0.003405472 -0.0034042858 -0.0034028266 -0.003401293 -0.0033999358][-0.0034136 -0.003412588 -0.003411822 -0.0034106597 -0.0034096676 -0.0034094448 -0.0034093212 -0.0034093242 -0.0034091244 -0.00340807 -0.0034064578 -0.0034048401 -0.0034031291 -0.0034013733 -0.0033999907][-0.0034150216 -0.0034142216 -0.0034133827 -0.0034123235 -0.0034114027 -0.0034111587 -0.0034111612 -0.0034113943 -0.0034112 -0.0034100241 -0.0034082804 -0.0034062094 -0.0034038047 -0.0034015123 -0.0033998892][-0.0034152914 -0.0034141093 -0.0034128916 -0.0034115864 -0.0034107717 -0.0034105177 -0.0034106998 -0.0034110914 -0.0034110683 -0.0034103461 -0.0034088718 -0.0034068942 -0.0034046036 -0.00340238 -0.0034004548][-0.0034144756 -0.0034131892 -0.0034121235 -0.0034112048 -0.0034106716 -0.003410548 -0.003410629 -0.0034108525 -0.0034107657 -0.0034101822 -0.0034089361 -0.0034072024 -0.0034052024 -0.0034031081 -0.003401093][-0.0034131629 -0.0034121086 -0.0034114555 -0.0034109631 -0.0034107147 -0.0034104211 -0.0034101936 -0.003410063 -0.0034096816 -0.0034090139 -0.0034078851 -0.0034064553 -0.0034047619 -0.0034029221 -0.0034011959][-0.003411148 -0.0034102625 -0.0034100125 -0.0034095924 -0.0034091182 -0.0034086409 -0.0034082038 -0.0034077291 -0.0034071526 -0.0034063903 -0.003405496 -0.0034043561 -0.0034030413 -0.0034017903 -0.0034006811][-0.0034078173 -0.0034066238 -0.0034064292 -0.003405941 -0.0034054155 -0.0034048366 -0.0034043652 -0.0034039319 -0.0034035207 -0.0034031686 -0.0034026734 -0.0034019959 -0.0034013195 -0.0034006594 -0.003400035][-0.0034043153 -0.00340304 -0.0034029221 -0.0034025263 -0.0034021472 -0.0034016736 -0.0034013393 -0.003400981 -0.0034006832 -0.0034004969 -0.0034002804 -0.0034000387 -0.0033998226 -0.0033996373 -0.0033994543]]...]
INFO - root - 2017-12-09 19:36:03.576266: step 53610, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:56m:18s remains)
INFO - root - 2017-12-09 19:36:12.073344: step 53620, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 64h:45m:25s remains)
INFO - root - 2017-12-09 19:36:20.658337: step 53630, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 66h:43m:03s remains)
INFO - root - 2017-12-09 19:36:29.294372: step 53640, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:35m:52s remains)
INFO - root - 2017-12-09 19:36:37.928250: step 53650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:27m:00s remains)
INFO - root - 2017-12-09 19:36:46.359263: step 53660, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:20m:26s remains)
INFO - root - 2017-12-09 19:36:54.696267: step 53670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 66h:29m:18s remains)
INFO - root - 2017-12-09 19:37:03.065048: step 53680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:36m:44s remains)
INFO - root - 2017-12-09 19:37:11.614659: step 53690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:26m:38s remains)
INFO - root - 2017-12-09 19:37:20.263686: step 53700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:46m:26s remains)
2017-12-09 19:37:21.179234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033556484 -0.0029964282 -0.0015821869 0.0013860615 0.0058613541 0.010244656 0.012799427 0.012975896 0.010813841 0.0071760593 0.0032739609 0.000205911 -0.0017217277 -0.0027844498 -0.0032331564][-0.0030895479 -0.0024978134 -0.00027534459 0.0041245455 0.01055141 0.016871162 0.02081413 0.021455916 0.018534619 0.013244046 0.0073235556 0.0024776037 -0.00069239247 -0.0024543679 -0.0031669387][-0.0016459882 -0.00040471321 0.0036823626 0.011116299 0.021313544 0.031309329 0.037990678 0.0396368 0.03551602 0.027156014 0.017189305 0.0084405262 0.0022606312 -0.0013350693 -0.002873061][0.0020385364 0.0050625196 0.012597596 0.024911627 0.04071546 0.056085553 0.066758513 0.069769807 0.063740827 0.050570484 0.034202944 0.019044049 0.0077381339 0.00088835764 -0.0022268216][0.007527764 0.014133438 0.027172545 0.046316512 0.069342159 0.091355361 0.10670295 0.11087973 0.10188632 0.082135119 0.057240129 0.033540796 0.015391914 0.004131915 -0.0012186123][0.0131774 0.024774604 0.044653665 0.071509138 0.10227011 0.13135457 0.15169908 0.15704802 0.14470369 0.11754948 0.0830777 0.049774893 0.024002915 0.0078532211 -1.9080704e-05][0.0174798 0.033899974 0.060051478 0.093717687 0.13113664 0.1662603 0.19085555 0.19713046 0.1818027 0.14813662 0.10529473 0.063619845 0.031285528 0.01099875 0.00101516][0.019463949 0.038941134 0.068799682 0.10632196 0.14748369 0.18626364 0.21367267 0.22072595 0.20367627 0.16597624 0.11795834 0.071181804 0.035009347 0.012482416 0.0014718825][0.018831227 0.038645234 0.068532363 0.10592056 0.14705032 0.18625651 0.21439496 0.2218048 0.20452039 0.1661308 0.11743085 0.070223421 0.034013566 0.011799012 0.0011732944][0.015804879 0.033469357 0.059999563 0.093427978 0.13063565 0.1667174 0.19306456 0.20009926 0.1840234 0.14844026 0.10377152 0.060972638 0.028599912 0.00919069 0.00023679761][0.011432306 0.025184127 0.0460191 0.072772764 0.10303293 0.1328515 0.15488692 0.16061719 0.1469247 0.11721906 0.080593705 0.046089932 0.020464167 0.0055621807 -0.0009764058][0.006746728 0.01617747 0.030664066 0.049709458 0.07158491 0.093397535 0.10953549 0.1133521 0.10269882 0.080561735 0.054009367 0.029594636 0.011895673 0.0019961188 -0.0020737341][0.0024908644 0.0081573743 0.017010344 0.028899284 0.042674758 0.056453932 0.0665724 0.068592437 0.061276242 0.046888348 0.030207252 0.015327299 0.0048660785 -0.00070938934 -0.0028220071][-0.00068699708 0.0021873035 0.0068091555 0.013202922 0.020676961 0.028093154 0.033376057 0.034112673 0.029774221 0.02182758 0.012999453 0.005415814 0.00028414605 -0.0023047316 -0.0031972029][-0.0025242444 -0.0013608465 0.00059216726 0.0033847687 0.0066818418 0.009938241 0.012172338 0.012333108 0.010260875 0.0067271404 0.0029799829 -0.00010227831 -0.0020929677 -0.0030377659 -0.00333179]]...]
INFO - root - 2017-12-09 19:37:29.688391: step 53710, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 64h:18m:12s remains)
INFO - root - 2017-12-09 19:37:38.173015: step 53720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:08m:40s remains)
INFO - root - 2017-12-09 19:37:46.886737: step 53730, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-09 19:37:55.554950: step 53740, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.861 sec/batch; 66h:42m:13s remains)
INFO - root - 2017-12-09 19:38:04.177514: step 53750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:11m:43s remains)
INFO - root - 2017-12-09 19:38:12.672699: step 53760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:08m:40s remains)
INFO - root - 2017-12-09 19:38:21.175681: step 53770, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 64h:43m:14s remains)
INFO - root - 2017-12-09 19:38:29.568944: step 53780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:52m:21s remains)
INFO - root - 2017-12-09 19:38:38.145821: step 53790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:00m:43s remains)
INFO - root - 2017-12-09 19:38:46.849197: step 53800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:30m:42s remains)
2017-12-09 19:38:47.745683: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.007076636 0.0075778104 0.0077004889 0.0077352421 0.0076868013 0.00745201 0.0070205033 0.0066727018 0.0065309079 0.0065960279 0.0066270623 0.0064383093 0.005956159 0.005221202 0.0041900761][0.0078077065 0.008217467 0.00837214 0.0085209562 0.0086605735 0.0086108688 0.0083187092 0.00810289 0.00806454 0.0082232049 0.00835473 0.0082428353 0.0077513326 0.0069118561 0.0056557013][0.0071382364 0.0073858025 0.0074924687 0.0077338144 0.0080201467 0.0082777431 0.0083104307 0.0084620947 0.00877445 0.0092073148 0.00946293 0.0093479762 0.0087453127 0.0077576274 0.0063868407][0.0057065291 0.0058713295 0.0060342858 0.0063879304 0.0069584241 0.0075469278 0.0080030113 0.0085990131 0.009269217 0.010010159 0.010416921 0.010310496 0.0096196551 0.008500265 0.0070291758][0.0041700266 0.0043584174 0.0046542827 0.0052545285 0.006166826 0.0071395421 0.007952882 0.0088916961 0.0098872818 0.010876718 0.011364087 0.011249186 0.010504231 0.0092913508 0.0076966323][0.0030854007 0.0033587071 0.0037576354 0.00450572 0.0057032295 0.0070092333 0.0082127526 0.0094167395 0.01054596 0.011584852 0.012118404 0.012008834 0.011309071 0.010077335 0.0084200371][0.0025307785 0.0028768976 0.0033892256 0.0042161345 0.0055203512 0.0069485595 0.0084045939 0.009790523 0.011058893 0.012089881 0.012573823 0.012463061 0.011800273 0.010586703 0.008948491][0.0025935231 0.0028099159 0.003191625 0.0039108852 0.0051454175 0.0066225883 0.0081889695 0.0095936889 0.010846707 0.011801454 0.012310864 0.012253698 0.011665748 0.010552809 0.0089938557][0.0027062793 0.00290832 0.0032060321 0.0036787514 0.0046395687 0.0059605073 0.0074470947 0.0088522173 0.010082108 0.010961574 0.011389791 0.011258529 0.01064021 0.0096022366 0.0081718992][0.0028218648 0.0029488693 0.003156889 0.0034570794 0.0041555148 0.0052486807 0.0065501053 0.0077977711 0.0088447751 0.0096155936 0.0099521149 0.0097195217 0.0090212161 0.0079356125 0.006555452][0.0026551534 0.002697537 0.0028781632 0.0031337503 0.0036687676 0.004551474 0.0056063281 0.0066486979 0.0074919341 0.0080553163 0.0081982985 0.0078262053 0.0070488229 0.0059427889 0.0046164189][0.00191111 0.0019684511 0.0021810916 0.0024578671 0.0029189114 0.0036358226 0.0044803964 0.0053273365 0.0059734946 0.0063328845 0.0062969718 0.0058221789 0.00503073 0.0039861193 0.0027895554][0.00058583985 0.00066025485 0.000922526 0.0012644732 0.001732405 0.0023361857 0.0029868477 0.0035844196 0.0039863307 0.0041779988 0.0040667662 0.003616737 0.0029277226 0.0020262033 0.0010244704][-0.0010223109 -0.0010245978 -0.00082351663 -0.00052669365 -0.00011740974 0.00037786062 0.000866228 0.0012536112 0.0014526809 0.0014761796 0.0013039594 0.00096387812 0.00048807706 -0.00013122475 -0.00081420061][-0.0024492485 -0.0024939103 -0.0023764004 -0.0021708736 -0.0018833139 -0.0015375996 -0.0012238775 -0.0010218688 -0.0009794347 -0.0010414263 -0.0011865268 -0.0014057206 -0.0016780689 -0.0019925195 -0.0023220638]]...]
INFO - root - 2017-12-09 19:38:56.381442: step 53810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:49m:57s remains)
INFO - root - 2017-12-09 19:39:04.877261: step 53820, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 67h:50m:43s remains)
INFO - root - 2017-12-09 19:39:13.484208: step 53830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:17m:06s remains)
INFO - root - 2017-12-09 19:39:21.954338: step 53840, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:31m:01s remains)
INFO - root - 2017-12-09 19:39:30.489404: step 53850, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 64h:41m:33s remains)
INFO - root - 2017-12-09 19:39:38.855435: step 53860, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 68h:39m:16s remains)
INFO - root - 2017-12-09 19:39:47.302091: step 53870, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 65h:57m:54s remains)
INFO - root - 2017-12-09 19:39:56.000486: step 53880, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 66h:39m:42s remains)
INFO - root - 2017-12-09 19:40:04.591681: step 53890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:55m:15s remains)
INFO - root - 2017-12-09 19:40:13.224433: step 53900, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 69h:29m:36s remains)
2017-12-09 19:40:14.199512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033557811 -0.0033526279 -0.0033524926 -0.0033526286 -0.0033527794 -0.0033529648 -0.0033531326 -0.0033532323 -0.0033532947 -0.0033533087 -0.0033533261 -0.003353361 -0.003353382 -0.0033533669 -0.0033533678][-0.0033531559 -0.003349548 -0.0033493375 -0.0033493594 -0.0033493054 -0.003349249 -0.0033491938 -0.0033491557 -0.0033491938 -0.0033492392 -0.0033493198 -0.0033493852 -0.0033494502 -0.0033494425 -0.0033494639][-0.0033527445 -0.0033489536 -0.0033487102 -0.003348626 -0.0033483438 -0.0033480017 -0.0033476648 -0.003347395 -0.0033472585 -0.0033472381 -0.0033473782 -0.0033475775 -0.0033477983 -0.0033478604 -0.0033479466][-0.0033529617 -0.0033490495 -0.003348727 -0.0033485214 -0.0033481356 -0.0033476371 -0.0033470492 -0.0033465498 -0.0033461621 -0.0033459014 -0.0033459554 -0.0033461656 -0.0033464786 -0.0033467447 -0.0033470788][-0.0033533203 -0.0033495312 -0.0033494269 -0.0033493435 -0.0033491063 -0.0033486648 -0.0033479093 -0.0033470793 -0.0033462839 -0.0033456222 -0.0033453524 -0.0033454017 -0.0033456883 -0.003346005 -0.0033464611][-0.0033539566 -0.0033504555 -0.0033506711 -0.0033508888 -0.0033509221 -0.003350663 -0.00334992 -0.0033488835 -0.0033476856 -0.0033465433 -0.0033458045 -0.0033454974 -0.0033456059 -0.0033459144 -0.0033463961][-0.0033543713 -0.0033512833 -0.0033520106 -0.0033528202 -0.0033533908 -0.0033535485 -0.0033530821 -0.0033520032 -0.0033504446 -0.0033487389 -0.0033474318 -0.0033466369 -0.003346418 -0.0033465947 -0.0033470497][-0.0033539294 -0.0033512949 -0.0033525287 -0.0033539366 -0.0033551727 -0.0033558847 -0.0033557944 -0.0033549326 -0.0033532782 -0.0033511443 -0.00334921 -0.0033478136 -0.0033470697 -0.0033470225 -0.0033473407][-0.0033536102 -0.0033511487 -0.0033526779 -0.0033544067 -0.0033560563 -0.0033572342 -0.0033575851 -0.0033570807 -0.0033556398 -0.0033535426 -0.0033514332 -0.0033497275 -0.0033484758 -0.0033479352 -0.0033478225][-0.0033529175 -0.0033503885 -0.0033519969 -0.0033538216 -0.0033555825 -0.0033569117 -0.0033575953 -0.0033574821 -0.0033564491 -0.0033547184 -0.0033528595 -0.0033511908 -0.0033496802 -0.0033487265 -0.0033481752][-0.0033527853 -0.0033500874 -0.003351551 -0.0033530707 -0.0033545448 -0.0033556193 -0.0033563762 -0.0033565292 -0.0033560139 -0.0033548444 -0.0033534982 -0.0033521315 -0.0033507186 -0.0033495638 -0.0033486721][-0.0033526395 -0.0033494276 -0.0033504381 -0.0033513424 -0.0033523112 -0.0033530658 -0.0033537047 -0.0033540176 -0.0033539622 -0.0033533685 -0.0033526209 -0.0033518132 -0.0033508479 -0.0033499422 -0.0033490951][-0.0033525855 -0.0033486572 -0.0033490106 -0.003349415 -0.0033499543 -0.003350348 -0.0033507028 -0.0033510176 -0.0033512467 -0.0033511738 -0.0033509564 -0.0033506418 -0.0033502912 -0.0033499631 -0.0033493973][-0.0033528036 -0.0033484197 -0.0033483214 -0.0033482914 -0.003348479 -0.0033485179 -0.0033486038 -0.0033488651 -0.0033491929 -0.0033493892 -0.0033494513 -0.0033495128 -0.0033495955 -0.0033496888 -0.0033495072][-0.0033530989 -0.0033485086 -0.0033481221 -0.003347863 -0.003347784 -0.0033475868 -0.00334757 -0.0033477619 -0.0033480807 -0.0033483049 -0.0033484672 -0.0033488076 -0.0033492378 -0.0033497082 -0.0033499447]]...]
INFO - root - 2017-12-09 19:40:22.689542: step 53910, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 67h:16m:31s remains)
INFO - root - 2017-12-09 19:40:31.189410: step 53920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:07m:25s remains)
INFO - root - 2017-12-09 19:40:39.801069: step 53930, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 65h:07m:45s remains)
INFO - root - 2017-12-09 19:40:48.511163: step 53940, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 67h:39m:17s remains)
INFO - root - 2017-12-09 19:40:57.294124: step 53950, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 67h:38m:03s remains)
INFO - root - 2017-12-09 19:41:05.908337: step 53960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:29m:04s remains)
INFO - root - 2017-12-09 19:41:14.444178: step 53970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:06m:56s remains)
INFO - root - 2017-12-09 19:41:22.951284: step 53980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 66h:13m:33s remains)
INFO - root - 2017-12-09 19:41:31.439344: step 53990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 66h:52m:51s remains)
INFO - root - 2017-12-09 19:41:40.203100: step 54000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:44m:25s remains)
2017-12-09 19:41:41.059625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033710992 -0.0033685022 -0.0033683258 -0.0033680145 -0.0033676648 -0.0033673551 -0.0033671758 -0.0033667686 -0.0033666592 -0.0033666361 -0.0033666827 -0.0033665667 -0.0033665113 -0.0033664086 -0.0033662978][-0.0033698662 -0.0033671411 -0.0033669206 -0.0033665991 -0.003366244 -0.0033660356 -0.0033658748 -0.0033655546 -0.0033655528 -0.0033654934 -0.0033655465 -0.0033655018 -0.0033654522 -0.0033652813 -0.0033651178][-0.0033696152 -0.0033668776 -0.0033666363 -0.0033663476 -0.0033660589 -0.0033658736 -0.0033657502 -0.0033656338 -0.0033656778 -0.0033655374 -0.0033654496 -0.0033652137 -0.0033650524 -0.0033647551 -0.0033645174][-0.0033692131 -0.0033665409 -0.0033663684 -0.0033661013 -0.0033658787 -0.0033657539 -0.0033655036 -0.0033652841 -0.0033652696 -0.0033649695 -0.003364668 -0.0033643083 -0.0033639804 -0.0033636079 -0.0033633343][-0.0033685581 -0.0033659886 -0.0033657341 -0.0033654808 -0.0033652687 -0.0033651483 -0.0033648671 -0.0033647537 -0.0033645809 -0.003364115 -0.0033637804 -0.0033633739 -0.00336295 -0.0033625003 -0.0033622081][-0.0033679598 -0.0033654075 -0.0033651725 -0.0033649353 -0.0033646571 -0.0033645278 -0.0033643311 -0.0033642035 -0.0033638072 -0.0033631946 -0.0033627413 -0.0033621558 -0.0033616426 -0.0033612335 -0.0033610144][-0.0033674855 -0.0033648969 -0.0033646647 -0.0033643513 -0.0033639895 -0.0033637595 -0.0033634994 -0.0033633308 -0.0033628186 -0.003362169 -0.0033616698 -0.0033610447 -0.0033604777 -0.0033601301 -0.0033600624][-0.0033678729 -0.0033651215 -0.0033646326 -0.0033641662 -0.0033635572 -0.0033631113 -0.003362844 -0.0033627588 -0.0033624414 -0.0033621895 -0.0033619758 -0.0033616177 -0.0033612396 -0.0033610195 -0.0033610619][-0.0033703963 -0.0033679039 -0.0033674084 -0.00336681 -0.0033662063 -0.0033658505 -0.0033655604 -0.0033655255 -0.0033654456 -0.0033656114 -0.0033657458 -0.0033656359 -0.0033654422 -0.0033651697 -0.0033650477][-0.0033772904 -0.0033749118 -0.0033740208 -0.0033730282 -0.0033720718 -0.0033714273 -0.00337083 -0.0033707332 -0.0033708597 -0.0033714517 -0.0033720089 -0.003372424 -0.0033727032 -0.0033728778 -0.0033730082][-0.003384308 -0.0033815422 -0.0033800232 -0.0033783931 -0.0033767822 -0.0033756269 -0.0033745705 -0.0033741686 -0.003374248 -0.0033748564 -0.0033755521 -0.0033762155 -0.0033769463 -0.0033776881 -0.003378384][-0.0033891238 -0.0033856865 -0.003383341 -0.0033810474 -0.0033789384 -0.003377331 -0.0033756213 -0.0033748276 -0.003374757 -0.003375238 -0.0033759146 -0.0033767449 -0.0033777826 -0.0033787847 -0.0033798062][-0.0033923537 -0.0033882721 -0.0033854397 -0.0033826758 -0.0033800937 -0.0033781112 -0.0033759016 -0.0033745428 -0.0033740054 -0.0033741849 -0.0033748229 -0.0033758581 -0.0033773649 -0.0033788169 -0.0033802823][-0.0033942063 -0.0033896645 -0.003386376 -0.0033834476 -0.0033807652 -0.0033786134 -0.0033760609 -0.0033741645 -0.0033730331 -0.0033725987 -0.0033728858 -0.0033737922 -0.0033754199 -0.0033772192 -0.0033791508][-0.0033934053 -0.0033879904 -0.0033840784 -0.0033809706 -0.0033781969 -0.0033757938 -0.0033731307 -0.0033708559 -0.00336912 -0.0033680231 -0.0033679695 -0.0033688077 -0.0033705817 -0.0033728054 -0.0033753391]]...]
INFO - root - 2017-12-09 19:41:49.738651: step 54010, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 67h:53m:11s remains)
INFO - root - 2017-12-09 19:41:58.409517: step 54020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:02m:21s remains)
INFO - root - 2017-12-09 19:42:07.106044: step 54030, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:11m:08s remains)
INFO - root - 2017-12-09 19:42:15.793141: step 54040, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:04m:52s remains)
INFO - root - 2017-12-09 19:42:24.533894: step 54050, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 69h:37m:12s remains)
INFO - root - 2017-12-09 19:42:33.151175: step 54060, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 57h:20m:14s remains)
INFO - root - 2017-12-09 19:42:41.583280: step 54070, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 56h:04m:05s remains)
INFO - root - 2017-12-09 19:42:49.753703: step 54080, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:18m:31s remains)
INFO - root - 2017-12-09 19:42:58.383912: step 54090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:40m:03s remains)
INFO - root - 2017-12-09 19:43:06.968051: step 54100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:50m:43s remains)
2017-12-09 19:43:07.851517: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25505894 0.26047686 0.26091313 0.2563822 0.24764362 0.23390551 0.21711367 0.19853228 0.18105023 0.16637136 0.1551688 0.14947385 0.14841622 0.15168032 0.15621923][0.24744508 0.25521341 0.25833324 0.25674224 0.25026727 0.23836452 0.22366197 0.206371 0.19042647 0.17691571 0.16771288 0.1632943 0.16300033 0.16706949 0.17158629][0.23478009 0.24456121 0.24980727 0.25078127 0.24711129 0.23914461 0.22707243 0.21314965 0.20039162 0.18989648 0.18225865 0.17839703 0.17886563 0.18219887 0.18567668][0.22407591 0.23591085 0.24330999 0.24666664 0.24546975 0.24113183 0.23253778 0.22142848 0.21115485 0.20311403 0.19708134 0.1936823 0.19430113 0.19715349 0.19989882][0.2169527 0.2302427 0.23936957 0.24487816 0.24639101 0.24442476 0.23882176 0.23065272 0.22303715 0.21653958 0.21108483 0.20836809 0.20888488 0.21133392 0.2132394][0.21234074 0.22733867 0.23798582 0.24535733 0.24878412 0.24891758 0.2459061 0.24085344 0.23562127 0.23059227 0.22596756 0.22330041 0.22331035 0.22449605 0.22572549][0.21232261 0.22830242 0.23918736 0.24751985 0.251839 0.25355333 0.25223678 0.2492702 0.24594909 0.24190271 0.23827358 0.23621044 0.23655757 0.23712716 0.23783639][0.21705498 0.23273833 0.2424103 0.24939299 0.25272077 0.25456455 0.25370836 0.25231329 0.25048745 0.24830297 0.24630243 0.24548881 0.24681807 0.24742326 0.24774122][0.22192934 0.23774649 0.24699137 0.25323692 0.25591153 0.25654331 0.25576892 0.25447869 0.25280809 0.25064415 0.2495755 0.2502813 0.25203016 0.25316644 0.2534804][0.22541034 0.24041003 0.24816102 0.25337765 0.25561991 0.25538373 0.2541917 0.25299823 0.25146082 0.24907199 0.24828017 0.249742 0.25190255 0.25336614 0.253714][0.22760585 0.24139678 0.24750128 0.2509861 0.25165087 0.25059378 0.24840847 0.24635038 0.24404992 0.24169464 0.24114624 0.24239464 0.24470647 0.24665841 0.24763685][0.22674726 0.23902731 0.24307986 0.24493474 0.2442369 0.24184778 0.23805377 0.23411869 0.23023708 0.22680055 0.22581781 0.22695805 0.22982131 0.23312807 0.23546532][0.22200947 0.23251681 0.23474047 0.23495421 0.23306245 0.22931999 0.22426802 0.21834071 0.21245882 0.20766512 0.20536104 0.20588365 0.20875387 0.21335046 0.21757852][0.21194391 0.22078912 0.22165392 0.22114918 0.21880938 0.21438076 0.2083777 0.20092124 0.19317837 0.18639022 0.1823258 0.18162693 0.18417107 0.18952395 0.19568282][0.19857492 0.20572872 0.20532744 0.20398363 0.20105001 0.19627587 0.18964382 0.181395 0.17265128 0.16481811 0.15958446 0.15773073 0.15979651 0.16532813 0.17260891]]...]
INFO - root - 2017-12-09 19:43:16.560057: step 54110, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.877 sec/batch; 67h:48m:03s remains)
INFO - root - 2017-12-09 19:43:25.001019: step 54120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:46m:07s remains)
INFO - root - 2017-12-09 19:43:33.689457: step 54130, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:46m:07s remains)
INFO - root - 2017-12-09 19:43:42.421574: step 54140, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 69h:46m:54s remains)
INFO - root - 2017-12-09 19:43:51.216071: step 54150, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 67h:15m:02s remains)
INFO - root - 2017-12-09 19:43:59.734388: step 54160, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.711 sec/batch; 55h:00m:37s remains)
INFO - root - 2017-12-09 19:44:08.463028: step 54170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:10m:40s remains)
INFO - root - 2017-12-09 19:44:16.791764: step 54180, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 65h:04m:29s remains)
INFO - root - 2017-12-09 19:44:25.392625: step 54190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:33m:14s remains)
INFO - root - 2017-12-09 19:44:33.974410: step 54200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:59m:58s remains)
2017-12-09 19:44:34.902415: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15703423 0.19626799 0.23498921 0.27216604 0.30254152 0.33076453 0.35687426 0.3843376 0.41252142 0.44090778 0.46833575 0.48906368 0.49911511 0.49357465 0.47446474][0.16933872 0.21722881 0.26555532 0.30966625 0.34552753 0.37576348 0.40137807 0.42539337 0.44803309 0.47304747 0.49720848 0.51627862 0.5249061 0.52099824 0.50463885][0.17669573 0.23354164 0.29277778 0.34765026 0.39220533 0.42690748 0.45143664 0.47081873 0.48641419 0.50456494 0.52217108 0.53671896 0.54220855 0.53768069 0.52169693][0.17737575 0.23970604 0.30736822 0.37213588 0.42616892 0.46711615 0.49533862 0.51297051 0.52235591 0.53341967 0.542772 0.55327839 0.55524337 0.54795033 0.53208125][0.17376009 0.24027646 0.31296429 0.38667974 0.44940406 0.49620411 0.52703488 0.54534125 0.55249286 0.55664486 0.55791962 0.56329209 0.56089431 0.55213785 0.53633738][0.17080916 0.23927128 0.31523386 0.39464071 0.46244669 0.51413476 0.54764766 0.56557196 0.56967711 0.57047474 0.56582528 0.56399184 0.55594832 0.54539567 0.53005159][0.16639769 0.23522738 0.313017 0.39482635 0.46713784 0.52289 0.55802459 0.57620114 0.57798344 0.57518649 0.56431913 0.55886388 0.54827821 0.5362497 0.52112377][0.16233684 0.23006633 0.30761254 0.39146903 0.46485206 0.5234502 0.56077665 0.57985026 0.58164412 0.57503915 0.56103265 0.5502876 0.53648257 0.52281171 0.50863457][0.16219626 0.22832663 0.3046577 0.38625151 0.45935777 0.51887333 0.55640572 0.57635051 0.57912093 0.57303119 0.55953705 0.54693657 0.53181845 0.51729149 0.50411892][0.16394097 0.22855632 0.30357361 0.38336608 0.45569471 0.51431781 0.5513925 0.57177663 0.57660782 0.57219672 0.56058359 0.54707152 0.53144425 0.5170303 0.50417793][0.16211839 0.22445881 0.29692152 0.37467158 0.44559789 0.50472552 0.5437237 0.56701142 0.57651317 0.57594442 0.56852031 0.55500412 0.5388 0.52303332 0.50940531][0.15800215 0.21908303 0.29071474 0.36705127 0.43648642 0.49526882 0.53381646 0.55907351 0.57214105 0.57687128 0.57469618 0.56530279 0.55295652 0.53685284 0.5224815][0.15258546 0.21324502 0.28360796 0.35955867 0.42810369 0.48514062 0.52322274 0.54931986 0.56502849 0.57397848 0.57749641 0.57216805 0.56341451 0.55055344 0.53671044][0.14599767 0.20569211 0.27481154 0.34869236 0.41674513 0.47474849 0.51326823 0.53996992 0.55745023 0.57022834 0.57618928 0.57526374 0.5701828 0.56012768 0.54674274][0.13642937 0.19582242 0.26302356 0.336486 0.40305793 0.46014753 0.49954295 0.52695608 0.54548943 0.55844516 0.565418 0.56666315 0.5644154 0.55753666 0.5459069]]...]
INFO - root - 2017-12-09 19:44:43.547644: step 54210, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 67h:00m:45s remains)
INFO - root - 2017-12-09 19:44:51.997122: step 54220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 66h:29m:25s remains)
INFO - root - 2017-12-09 19:45:00.705702: step 54230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:45m:16s remains)
INFO - root - 2017-12-09 19:45:09.392613: step 54240, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 65h:25m:52s remains)
INFO - root - 2017-12-09 19:45:18.042586: step 54250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:04m:20s remains)
INFO - root - 2017-12-09 19:45:26.446980: step 54260, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 64h:44m:03s remains)
INFO - root - 2017-12-09 19:45:34.993543: step 54270, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 69h:24m:04s remains)
INFO - root - 2017-12-09 19:45:43.433552: step 54280, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:41m:30s remains)
INFO - root - 2017-12-09 19:45:52.097578: step 54290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:03m:31s remains)
INFO - root - 2017-12-09 19:46:00.666456: step 54300, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:45m:10s remains)
2017-12-09 19:46:01.617528: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.060719758 0.063814469 0.061153494 0.051794458 0.037451509 0.022755129 0.011486036 0.0047120377 0.0015718837 0.00015350082 -0.00059349206 1.273863e-05 0.0014773572 0.0033258998 0.0052426867][0.085208513 0.090060242 0.08726427 0.07514751 0.056104153 0.035379738 0.018649461 0.0081047975 0.0027021405 0.00017227978 -0.00080319378 7.3368428e-05 0.0022322887 0.005312209 0.0085561723][0.11146351 0.11817008 0.11538582 0.10140663 0.0787181 0.052722812 0.030339492 0.014836527 0.0059899595 0.0016762966 9.7742537e-05 0.00095871207 0.004060002 0.0086436719 0.013839142][0.13906401 0.1475848 0.14483054 0.12937474 0.10398088 0.073865533 0.046662167 0.026619036 0.013658509 0.0064312946 0.0032588353 0.0038079724 0.007906884 0.014351316 0.021842794][0.16385491 0.17447039 0.17238432 0.15670261 0.13024108 0.098016582 0.067319423 0.043135028 0.026179006 0.015723821 0.010415047 0.0098749595 0.014402997 0.022428224 0.032151464][0.18191475 0.19465965 0.19484805 0.18153414 0.15693094 0.12535769 0.0933517 0.065846436 0.044318944 0.029306548 0.020230517 0.017375832 0.02128448 0.030149044 0.041858748][0.191752 0.20589468 0.20838487 0.19857681 0.17801039 0.14959562 0.1185993 0.08969859 0.064985245 0.045399163 0.031585544 0.024888914 0.026310571 0.0343135 0.046707653][0.19478835 0.20987986 0.21407923 0.20727214 0.19053721 0.16579768 0.13713489 0.10819073 0.08126419 0.058197744 0.040083546 0.029397981 0.027696554 0.033614155 0.045150433][0.19400075 0.20965141 0.21466692 0.20985627 0.19537987 0.17306148 0.14613903 0.1175741 0.089822322 0.064747676 0.043634236 0.02997716 0.024914445 0.027963312 0.0372647][0.19226494 0.20813243 0.21294442 0.20843387 0.19460672 0.17275557 0.14606392 0.11731397 0.089045227 0.063280836 0.041594487 0.026686644 0.019478304 0.01950009 0.02553536][0.19166616 0.20729758 0.21069258 0.20505944 0.18977171 0.16628343 0.13806595 0.10839365 0.0799107 0.054418076 0.033618771 0.019612951 0.012093732 0.010309857 0.013454352][0.19164081 0.20635669 0.20669292 0.1976506 0.1791769 0.15279317 0.1225263 0.092199691 0.064413339 0.040901881 0.022842351 0.011251951 0.0050513884 0.0029391639 0.0039891321][0.18603608 0.19916508 0.19678874 0.184622 0.16262184 0.13347228 0.10202963 0.072489992 0.046941519 0.026723402 0.012275418 0.0039257053 -8.0249505e-05 -0.001385059 -0.0011105726][0.17163576 0.18217456 0.17736334 0.16321848 0.13997008 0.1106547 0.080398366 0.053516041 0.031536207 0.015420843 0.0049091345 -0.00029680599 -0.0023775098 -0.0029715821 -0.0029476164][0.14673628 0.1542922 0.14807649 0.13386485 0.11199746 0.085609496 0.059277661 0.036939029 0.019554067 0.0076841516 0.00065485411 -0.0021826187 -0.0030384501 -0.0032146908 -0.0032289198]]...]
INFO - root - 2017-12-09 19:46:10.138894: step 54310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:31m:37s remains)
INFO - root - 2017-12-09 19:46:18.660334: step 54320, loss = 0.89, batch loss = 0.68 (10.0 examples/sec; 0.800 sec/batch; 61h:47m:44s remains)
INFO - root - 2017-12-09 19:46:27.157294: step 54330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 66h:06m:39s remains)
INFO - root - 2017-12-09 19:46:35.774023: step 54340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:04m:16s remains)
INFO - root - 2017-12-09 19:46:44.402107: step 54350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 66h:40m:06s remains)
INFO - root - 2017-12-09 19:46:53.043762: step 54360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:10m:32s remains)
INFO - root - 2017-12-09 19:47:01.604090: step 54370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:36m:53s remains)
INFO - root - 2017-12-09 19:47:10.330923: step 54380, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 69h:57m:40s remains)
INFO - root - 2017-12-09 19:47:19.130252: step 54390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:01m:48s remains)
INFO - root - 2017-12-09 19:47:27.834031: step 54400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:53m:57s remains)
2017-12-09 19:47:28.692039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033865077 -0.0033835866 -0.0033835806 -0.0033836428 -0.0033836598 -0.0033837466 -0.0033839231 -0.0033840924 -0.0033842248 -0.0033842677 -0.0033843592 -0.0033842917 -0.0033843189 -0.003384328 -0.0033844502][-0.0033849422 -0.0033817922 -0.0033817664 -0.003381876 -0.0033819356 -0.0033820493 -0.0033822074 -0.0033822786 -0.0033822886 -0.0033822667 -0.0033822278 -0.0033821051 -0.0033820476 -0.0033821131 -0.0033823149][-0.003385111 -0.0033818705 -0.0033817156 -0.0033818563 -0.0033819575 -0.0033821478 -0.0033823214 -0.0033823084 -0.00338218 -0.0033820253 -0.0033817214 -0.0033813505 -0.0033811484 -0.0033812274 -0.0033814544][-0.0033851506 -0.0033818239 -0.0033815315 -0.0033816937 -0.0033818707 -0.0033821978 -0.0033824353 -0.0033824064 -0.0033821613 -0.0033818972 -0.0033815983 -0.0033812043 -0.0033808085 -0.0033806378 -0.0033807086][-0.0033850877 -0.0033817315 -0.0033813845 -0.0033816015 -0.0033818751 -0.0033824055 -0.0033827566 -0.0033828376 -0.0033827659 -0.0033826621 -0.0033824882 -0.0033820095 -0.0033812854 -0.003380524 -0.0033801754][-0.0033849734 -0.0033816912 -0.0033813498 -0.0033816288 -0.0033820036 -0.0033826632 -0.0033831676 -0.0033836868 -0.0033841748 -0.0033847368 -0.0033848484 -0.0033840672 -0.0033826823 -0.003381094 -0.0033799922][-0.0033846195 -0.0033814609 -0.003381192 -0.0033814714 -0.0033820062 -0.0033829284 -0.003383924 -0.0033852756 -0.0033871122 -0.0033890312 -0.0033893527 -0.0033878 -0.0033852584 -0.0033825408 -0.0033804385][-0.0033842693 -0.0033811755 -0.0033808958 -0.0033811957 -0.0033821212 -0.0033836844 -0.0033853825 -0.003388206 -0.0033922552 -0.0033951574 -0.0033952268 -0.0033927164 -0.0033888342 -0.0033848849 -0.0033816674][-0.0033840111 -0.0033809748 -0.0033807929 -0.0033812895 -0.0033828595 -0.0033854602 -0.0033888337 -0.0033938109 -0.0033992338 -0.0034023984 -0.0034020664 -0.0033985537 -0.0033933732 -0.003388091 -0.0033836754][-0.0033837815 -0.0033808236 -0.0033809438 -0.0033818041 -0.0033840986 -0.0033880123 -0.0033934109 -0.0033998394 -0.0034058038 -0.0034090714 -0.0034083086 -0.0034038539 -0.0033976317 -0.0033913371 -0.0033857678][-0.0033836158 -0.0033807715 -0.0033812341 -0.0033825017 -0.0033854984 -0.0033904568 -0.0033969749 -0.0034038683 -0.0034097899 -0.0034130141 -0.0034118963 -0.0034071261 -0.0034005614 -0.0033936151 -0.0033873194][-0.0033835182 -0.0033806793 -0.0033814129 -0.0033830013 -0.0033862614 -0.0033915164 -0.0033980298 -0.0034045014 -0.0034099461 -0.0034128029 -0.0034117205 -0.0034072867 -0.003401079 -0.0033943453 -0.0033880938][-0.003383948 -0.0033809543 -0.0033817303 -0.0033833273 -0.0033862765 -0.0033911937 -0.0033968575 -0.0034024741 -0.0034072327 -0.0034096946 -0.003408948 -0.0034051463 -0.0033997765 -0.003393804 -0.0033882132][-0.0033846421 -0.0033815559 -0.0033822912 -0.0033836116 -0.0033859788 -0.0033897995 -0.0033943607 -0.0033990131 -0.0034029719 -0.0034052429 -0.0034048872 -0.0034018951 -0.0033974901 -0.0033925574 -0.0033879415][-0.0033853359 -0.0033822134 -0.0033827599 -0.0033838416 -0.0033856302 -0.0033883092 -0.0033917387 -0.003395302 -0.0033982543 -0.0034000562 -0.0033998671 -0.0033977935 -0.0033945229 -0.0033908554 -0.0033873734]]...]
INFO - root - 2017-12-09 19:47:37.362593: step 54410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:22m:44s remains)
INFO - root - 2017-12-09 19:47:45.967151: step 54420, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.828 sec/batch; 63h:56m:09s remains)
INFO - root - 2017-12-09 19:47:54.603821: step 54430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:27m:06s remains)
INFO - root - 2017-12-09 19:48:03.253143: step 54440, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 64h:59m:59s remains)
INFO - root - 2017-12-09 19:48:11.989094: step 54450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:11m:27s remains)
INFO - root - 2017-12-09 19:48:20.692310: step 54460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:51m:45s remains)
INFO - root - 2017-12-09 19:48:29.359651: step 54470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:57m:56s remains)
INFO - root - 2017-12-09 19:48:37.921774: step 54480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:36m:56s remains)
INFO - root - 2017-12-09 19:48:46.403925: step 54490, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 65h:39m:14s remains)
INFO - root - 2017-12-09 19:48:54.796178: step 54500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:28m:24s remains)
2017-12-09 19:48:55.654419: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14861439 0.20304953 0.25930864 0.30665842 0.33955976 0.35346159 0.34789395 0.323175 0.2826049 0.23099376 0.17439879 0.11960807 0.073003784 0.038772855 0.016936544][0.15076457 0.20396841 0.26171288 0.3130224 0.35000047 0.3678804 0.36616021 0.344631 0.30660361 0.25701672 0.20097646 0.144546 0.093653835 0.053499363 0.025518866][0.1426288 0.19027077 0.24325863 0.29219061 0.33094576 0.35409746 0.3594425 0.34524018 0.31408688 0.270479 0.21859072 0.16426453 0.11272359 0.069447249 0.036974952][0.1250886 0.16535302 0.21176733 0.25720653 0.29617119 0.32323933 0.33672771 0.33323646 0.31341505 0.27892408 0.23370534 0.18307242 0.13184316 0.085910447 0.049039748][0.10204632 0.13489647 0.1745552 0.21581337 0.25388068 0.28416821 0.30448657 0.31047028 0.30195242 0.27914888 0.2445451 0.20126259 0.15348879 0.10635272 0.065198][0.07899069 0.10478 0.13734634 0.17378697 0.21060206 0.24316373 0.26891273 0.28290424 0.28420135 0.27136812 0.24687928 0.21258521 0.1707249 0.1253629 0.08223287][0.060363464 0.079415821 0.10566098 0.13760206 0.17203803 0.20512846 0.23431827 0.25439936 0.26353657 0.25913176 0.24317729 0.21639952 0.1799849 0.13721767 0.093828708][0.051808491 0.066107348 0.087379865 0.11572038 0.14807785 0.18021743 0.20991063 0.23240738 0.24640821 0.24783045 0.23771185 0.21555525 0.18288073 0.14234532 0.099572651][0.053398691 0.064002559 0.081198193 0.10650925 0.13609946 0.16532113 0.19255012 0.21388046 0.22797677 0.23081054 0.22339706 0.20467527 0.1757144 0.13874596 0.098814569][0.060994871 0.069436759 0.08316844 0.10459594 0.1300209 0.15467994 0.17665356 0.19295202 0.20305714 0.20362298 0.19576891 0.17882343 0.15402716 0.12312526 0.089690946][0.068077654 0.073920682 0.084119588 0.1008535 0.1208218 0.13993584 0.1558497 0.16595857 0.17029457 0.16714033 0.15762627 0.14165 0.12070679 0.096430555 0.070858762][0.0703717 0.074282013 0.080861278 0.092810415 0.10718839 0.12043677 0.13036679 0.13548106 0.13556373 0.12952146 0.11902644 0.10464814 0.087303922 0.068234161 0.049045675][0.06667839 0.069828369 0.073961318 0.081828207 0.091426931 0.099815086 0.10512456 0.10647745 0.10386391 0.096622445 0.086016521 0.073166378 0.0589859 0.04443831 0.03047953][0.055888329 0.058433536 0.061102174 0.065933086 0.07154309 0.076211907 0.078263395 0.077358454 0.073395774 0.066080093 0.056504522 0.045657642 0.034670681 0.024198314 0.015002066][0.040090855 0.041392259 0.042677328 0.045213368 0.048218094 0.050512113 0.050757352 0.048640743 0.04441442 0.038223356 0.030723931 0.022807978 0.015452852 0.0092116147 0.0042671114]]...]
INFO - root - 2017-12-09 19:49:04.252584: step 54510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:00m:21s remains)
INFO - root - 2017-12-09 19:49:12.866086: step 54520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:16m:53s remains)
INFO - root - 2017-12-09 19:49:21.428326: step 54530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:06m:49s remains)
INFO - root - 2017-12-09 19:49:30.133593: step 54540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:41m:04s remains)
INFO - root - 2017-12-09 19:49:38.833770: step 54550, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 67h:59m:35s remains)
INFO - root - 2017-12-09 19:49:47.539411: step 54560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:43m:43s remains)
INFO - root - 2017-12-09 19:49:56.170390: step 54570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:27m:11s remains)
INFO - root - 2017-12-09 19:50:04.644708: step 54580, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 65h:14m:25s remains)
INFO - root - 2017-12-09 19:50:13.491002: step 54590, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 69h:54m:03s remains)
INFO - root - 2017-12-09 19:50:22.144847: step 54600, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 65h:59m:18s remains)
2017-12-09 19:50:23.035958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033712587 -0.0033689183 -0.0033686277 -0.0033685863 -0.0033685337 -0.0033685404 -0.0033685789 -0.0033686284 -0.0033686517 -0.0033685311 -0.0033684347 -0.0033683677 -0.003368306 -0.0033683334 -0.0033683716][-0.0033693383 -0.0033668671 -0.0033666319 -0.0033666405 -0.003366634 -0.0033666629 -0.0033667025 -0.0033667348 -0.0033667369 -0.003366689 -0.0033666538 -0.0033666263 -0.003366618 -0.0033666559 -0.0033667206][-0.0033692918 -0.0033667523 -0.0033666035 -0.0033666498 -0.0033666268 -0.0033665951 -0.00336656 -0.0033664997 -0.0033664231 -0.0033663549 -0.003366299 -0.003366279 -0.0033663076 -0.0033663935 -0.0033665057][-0.0033691644 -0.0033665833 -0.0033665211 -0.0033666289 -0.0033665893 -0.0033665034 -0.0033663895 -0.0033662182 -0.0033660482 -0.0033659462 -0.0033658824 -0.0033658983 -0.0033659886 -0.0033661644 -0.0033663595][-0.0033690522 -0.0033663996 -0.0033664573 -0.0033666512 -0.0033665602 -0.0033663346 -0.0033660582 -0.0033656987 -0.0033653623 -0.0033651888 -0.0033651316 -0.0033652319 -0.0033654796 -0.0033658382 -0.0033661621][-0.0033691402 -0.0033662855 -0.0033663572 -0.0033665858 -0.0033663674 -0.0033659122 -0.0033653765 -0.0033647341 -0.0033642002 -0.0033639702 -0.0033639821 -0.0033642768 -0.0033647928 -0.0033654273 -0.0033659332][-0.0033693847 -0.003366353 -0.0033663386 -0.0033664929 -0.0033660871 -0.0033653777 -0.0033645947 -0.003363699 -0.003362996 -0.0033627555 -0.0033629094 -0.0033634386 -0.0033641704 -0.0033650137 -0.0033656745][-0.0033695991 -0.0033664824 -0.0033663241 -0.0033663877 -0.0033658107 -0.0033648829 -0.0033638955 -0.0033628063 -0.0033620484 -0.0033619094 -0.0033622298 -0.0033629553 -0.0033638533 -0.0033647937 -0.0033654906][-0.0033697 -0.0033665625 -0.0033663225 -0.0033662547 -0.0033655809 -0.0033645853 -0.0033635353 -0.0033624561 -0.003361783 -0.003361695 -0.0033621024 -0.0033629492 -0.0033638845 -0.0033647693 -0.0033653907][-0.0033697914 -0.0033666012 -0.0033662876 -0.0033660638 -0.003365397 -0.0033645078 -0.003363583 -0.0033626994 -0.0033621511 -0.0033620554 -0.0033624626 -0.0033632563 -0.0033640997 -0.0033648633 -0.0033654005][-0.0033696247 -0.0033665427 -0.003366265 -0.0033659795 -0.0033654207 -0.0033647381 -0.0033640747 -0.0033635143 -0.0033631984 -0.0033631478 -0.0033634896 -0.003364095 -0.0033646959 -0.003365207 -0.0033655751][-0.0033693477 -0.0033664755 -0.003366264 -0.003366011 -0.0033656266 -0.003365214 -0.003364848 -0.0033646279 -0.003364546 -0.0033645465 -0.0033647579 -0.0033651204 -0.0033654417 -0.0033656573 -0.0033658345][-0.0033691721 -0.0033663041 -0.0033662564 -0.0033661041 -0.0033659213 -0.003365725 -0.0033655169 -0.0033654673 -0.0033654943 -0.0033655101 -0.0033656224 -0.0033658324 -0.0033659854 -0.0033660459 -0.0033660783][-0.0033690343 -0.0033661101 -0.0033661919 -0.0033661525 -0.0033661386 -0.0033661183 -0.003366045 -0.0033660789 -0.0033661257 -0.0033660734 -0.0033660589 -0.0033661106 -0.0033661285 -0.0033661115 -0.0033661115][-0.0033691758 -0.0033661898 -0.0033662419 -0.0033662764 -0.0033663467 -0.003366403 -0.003366424 -0.0033664927 -0.0033665309 -0.0033664296 -0.0033662966 -0.0033662058 -0.0033661122 -0.0033660419 -0.003366028]]...]
INFO - root - 2017-12-09 19:50:31.650148: step 54610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:02m:18s remains)
INFO - root - 2017-12-09 19:50:40.214535: step 54620, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:59m:54s remains)
INFO - root - 2017-12-09 19:50:48.668138: step 54630, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:17m:14s remains)
INFO - root - 2017-12-09 19:50:57.392521: step 54640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:12m:29s remains)
INFO - root - 2017-12-09 19:51:05.957514: step 54650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:32m:27s remains)
INFO - root - 2017-12-09 19:51:14.645374: step 54660, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 68h:34m:36s remains)
INFO - root - 2017-12-09 19:51:23.305428: step 54670, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 68h:43m:30s remains)
INFO - root - 2017-12-09 19:51:31.832192: step 54680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 67h:12m:17s remains)
INFO - root - 2017-12-09 19:51:40.414286: step 54690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:17m:08s remains)
INFO - root - 2017-12-09 19:51:49.139115: step 54700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:14m:34s remains)
2017-12-09 19:51:50.074803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033999532 -0.0034006189 -0.0034004902 -0.0033990014 -0.0033969437 -0.0033935206 -0.0033912575 -0.0033918384 -0.003394187 -0.0033964608 -0.0033954792 -0.0033915995 -0.003384013 -0.0033736415 -0.0033617751][-0.0034004741 -0.0034010257 -0.003401536 -0.0034015102 -0.0034008983 -0.0033986974 -0.0033970508 -0.0033973034 -0.0033978533 -0.0033981805 -0.0033953963 -0.0033895113 -0.0033806767 -0.0033689388 -0.0033552749][-0.0034016308 -0.0034019358 -0.0034029938 -0.0034037046 -0.003403838 -0.003402702 -0.0034011947 -0.0034017016 -0.0034018834 -0.0034013514 -0.0033972536 -0.0033897378 -0.0033793694 -0.0033659902 -0.0033502067][-0.0034019728 -0.003402119 -0.0034031179 -0.0034039177 -0.0034040834 -0.0034034105 -0.0034019486 -0.0034022324 -0.0034019544 -0.0034012771 -0.0033972037 -0.003389312 -0.0033787151 -0.0033642489 -0.0033479473][-0.0034016431 -0.0034017656 -0.0034025628 -0.0034029924 -0.0034027547 -0.003401577 -0.003399892 -0.0033997654 -0.0033992575 -0.0033982326 -0.0033940978 -0.0033870817 -0.0033769794 -0.0033629488 -0.00334722][-0.0034003968 -0.0034008571 -0.0034017311 -0.0034021274 -0.0034012645 -0.0033989879 -0.0033966959 -0.0033960165 -0.0033955895 -0.0033948275 -0.003391318 -0.0033852446 -0.0033757293 -0.0033622293 -0.0033471726][-0.0033974024 -0.0033980773 -0.003399434 -0.0034003006 -0.0033999914 -0.0033980652 -0.0033957851 -0.0033951707 -0.0033948175 -0.0033940135 -0.0033904265 -0.0033841478 -0.0033746401 -0.0033616724 -0.0033476264][-0.0033960082 -0.0033967104 -0.0033985164 -0.0033996194 -0.0033993088 -0.0033974091 -0.003395339 -0.0033951043 -0.0033950824 -0.0033942314 -0.0033904489 -0.0033839049 -0.0033739633 -0.0033610766 -0.0033477903][-0.0033973672 -0.0033978052 -0.0033991146 -0.0034000515 -0.0033997961 -0.003398513 -0.0033973141 -0.0033971297 -0.0033973595 -0.0033954466 -0.003390677 -0.0033832311 -0.0033726573 -0.0033600668 -0.0033472287][-0.0034007339 -0.0034004913 -0.0034009926 -0.0034016189 -0.0034019651 -0.0034015616 -0.0034011921 -0.0034010757 -0.0034006576 -0.0033973511 -0.0033912004 -0.003382534 -0.0033712813 -0.0033586035 -0.003346194][-0.0034032818 -0.0034026541 -0.0034029193 -0.0034035789 -0.0034041977 -0.0034043686 -0.0034044224 -0.0034041114 -0.003402706 -0.0033980689 -0.0033908533 -0.0033813438 -0.0033693663 -0.0033562696 -0.0033435598][-0.0034027118 -0.0034017065 -0.0034019377 -0.0034028273 -0.0034037286 -0.0034037223 -0.0034032648 -0.0034021991 -0.0033995553 -0.0033938885 -0.0033858737 -0.0033761002 -0.0033644713 -0.0033523873 -0.0033407705][-0.0033988617 -0.0033978131 -0.0033980492 -0.0033986918 -0.0033993712 -0.0033990464 -0.0033978953 -0.00339568 -0.0033919998 -0.0033856083 -0.003376876 -0.0033675774 -0.0033575268 -0.0033466849 -0.0033363726][-0.0033930263 -0.0033922836 -0.0033927078 -0.0033937017 -0.0033945062 -0.0033939395 -0.0033920002 -0.0033888076 -0.0033840782 -0.0033767058 -0.0033675989 -0.0033585657 -0.0033500595 -0.003340825 -0.0033324424][-0.0033885494 -0.0033881976 -0.0033891299 -0.0033905923 -0.0033912854 -0.0033901597 -0.0033871988 -0.0033828472 -0.0033764297 -0.0033683018 -0.00335967 -0.003351483 -0.0033441209 -0.0033364994 -0.0033297858]]...]
INFO - root - 2017-12-09 19:51:58.686526: step 54710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:54m:06s remains)
INFO - root - 2017-12-09 19:52:07.100543: step 54720, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 62h:52m:16s remains)
INFO - root - 2017-12-09 19:52:15.331880: step 54730, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 63h:27m:39s remains)
INFO - root - 2017-12-09 19:52:23.833745: step 54740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:19m:18s remains)
INFO - root - 2017-12-09 19:52:32.522439: step 54750, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 68h:20m:00s remains)
INFO - root - 2017-12-09 19:52:41.187531: step 54760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:33m:27s remains)
INFO - root - 2017-12-09 19:52:49.761184: step 54770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:45m:33s remains)
INFO - root - 2017-12-09 19:52:58.362758: step 54780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:56m:37s remains)
INFO - root - 2017-12-09 19:53:06.967555: step 54790, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.856 sec/batch; 66h:02m:17s remains)
INFO - root - 2017-12-09 19:53:15.474040: step 54800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:14m:46s remains)
2017-12-09 19:53:16.343451: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058140911 0.055037517 0.049621295 0.043228626 0.036094517 0.029055638 0.0224784 0.016322736 0.010816597 0.0063388227 0.0029362265 0.00059386319 -0.0011333358 -0.0017706563 -0.001541866][0.094899721 0.090713188 0.083607048 0.075184606 0.065404795 0.055135045 0.044059809 0.033104435 0.022780877 0.014033567 0.0072339429 0.0025350938 -0.00033488055 -0.0018975312 -0.0023598173][0.15158169 0.14516662 0.13411045 0.12125049 0.10687803 0.092154354 0.075994849 0.059655033 0.043577336 0.029396208 0.017630208 0.0084944582 0.0022695584 -0.0010481526 -0.0020785062][0.21444628 0.20880309 0.19695055 0.18226579 0.16480143 0.14553712 0.12278923 0.09884052 0.074249342 0.051578775 0.032250825 0.017169885 0.0068923468 0.0010612567 -0.0010698887][0.26555288 0.262007 0.25165278 0.23902062 0.2227307 0.20318769 0.17747667 0.14808728 0.11522332 0.082920641 0.053847812 0.030179234 0.013609754 0.0038720213 0.00014736596][0.29220384 0.29218444 0.28482854 0.27540472 0.26184624 0.24478595 0.21951881 0.18839172 0.15076353 0.11165228 0.074777648 0.043596398 0.021090539 0.0070367018 0.001340687][0.29104182 0.29591084 0.29308444 0.2873067 0.27719787 0.26306129 0.23956981 0.2093924 0.17048481 0.12862192 0.087570556 0.052362636 0.026355006 0.0094188051 0.0023371398][0.26623523 0.27608326 0.27828121 0.27598548 0.26886442 0.25660375 0.23496568 0.2066267 0.16889174 0.12823589 0.087855771 0.053533684 0.027933309 0.010908086 0.004052815][0.22417355 0.23734339 0.24352621 0.24431375 0.24054033 0.23018236 0.21096735 0.18517035 0.15085822 0.11411424 0.077549115 0.047624003 0.025881598 0.012435811 0.0077379122][0.16836528 0.18238337 0.19020604 0.19349983 0.19314964 0.18590966 0.17102711 0.1498881 0.122052 0.092161775 0.062505782 0.039270964 0.023489291 0.015736438 0.014941545][0.10830228 0.12030759 0.12782118 0.13187423 0.13325097 0.12936199 0.11947648 0.10500911 0.085925736 0.065421231 0.045614842 0.030910766 0.02256714 0.021192329 0.025062626][0.055609059 0.063401654 0.069206282 0.072848648 0.074944712 0.0737585 0.068380065 0.060262434 0.049559738 0.038799588 0.028907651 0.0229453 0.022283614 0.027336253 0.035868764][0.019809209 0.023684515 0.026937177 0.029312083 0.031204576 0.03146207 0.029398093 0.026120471 0.021856776 0.018061731 0.015497956 0.016153447 0.020915544 0.03013706 0.041573286][0.0019261336 0.0034404921 0.004914456 0.0062287441 0.0075060297 0.0082016876 0.0078018429 0.00714124 0.0062810257 0.0060447622 0.007005142 0.010183019 0.016616426 0.026290622 0.037458867][-0.0031470098 -0.0029618987 -0.0027114518 -0.0023950839 -0.0020520464 -0.0016160266 -0.0015293695 -0.0014764501 -0.0012459303 -0.00049933116 0.0011562936 0.0040096594 0.00901847 0.016212601 0.024399996]]...]
INFO - root - 2017-12-09 19:53:24.921791: step 54810, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 67h:24m:07s remains)
INFO - root - 2017-12-09 19:53:33.545798: step 54820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:39m:37s remains)
INFO - root - 2017-12-09 19:53:42.007899: step 54830, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:48m:59s remains)
INFO - root - 2017-12-09 19:53:50.552655: step 54840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:22m:49s remains)
INFO - root - 2017-12-09 19:53:59.213292: step 54850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:33m:26s remains)
INFO - root - 2017-12-09 19:54:07.797290: step 54860, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:44m:27s remains)
INFO - root - 2017-12-09 19:54:16.145940: step 54870, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 63h:28m:07s remains)
INFO - root - 2017-12-09 19:54:24.487259: step 54880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:20m:38s remains)
INFO - root - 2017-12-09 19:54:32.969935: step 54890, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 66h:34m:16s remains)
INFO - root - 2017-12-09 19:54:41.531527: step 54900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 66h:04m:06s remains)
2017-12-09 19:54:42.427389: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013812423 0.011601639 0.0093587246 0.0075780628 0.00622165 0.0056356369 0.0056327982 0.0062023224 0.007096678 0.0081720995 0.0091944141 0.0099377679 0.010293939 0.0096052224 0.0084873168][0.022369074 0.019180587 0.015899222 0.013203857 0.011261484 0.010507487 0.010747012 0.011984216 0.013724811 0.015635155 0.017414076 0.018634899 0.019159721 0.018527137 0.017168907][0.036005586 0.032245595 0.028264677 0.024917832 0.022589967 0.021702779 0.022119189 0.023751851 0.025961706 0.028356014 0.030462706 0.031780761 0.032157168 0.031508826 0.029993748][0.053713918 0.050154321 0.046064097 0.042349383 0.039674062 0.038415279 0.038550306 0.039882265 0.041746017 0.043870293 0.045636725 0.046531394 0.046414781 0.045483906 0.043784][0.071475089 0.0687764 0.065038204 0.061143048 0.058020115 0.056027874 0.055315915 0.055613942 0.05632608 0.057395492 0.058178272 0.058234256 0.057424139 0.056032512 0.054168139][0.084633753 0.0830877 0.079919189 0.076062463 0.072494395 0.069564588 0.067630976 0.066395052 0.065475583 0.064993113 0.064534 0.063585594 0.062030733 0.060178667 0.058217481][0.091745235 0.09128315 0.0885431 0.084662855 0.080482513 0.076404557 0.072979443 0.06998197 0.067344666 0.06524615 0.063555628 0.061672896 0.059553906 0.057356618 0.055309348][0.094037674 0.094167039 0.0913236 0.087072968 0.081940234 0.076411769 0.071232811 0.066419952 0.062189788 0.058653425 0.055891667 0.053313483 0.050875157 0.048537396 0.046551503][0.093181029 0.093404509 0.089784667 0.084446087 0.077799633 0.070537485 0.063464791 0.056890052 0.051306937 0.046704069 0.043217842 0.040304326 0.037844438 0.035670504 0.033931781][0.089813851 0.089440174 0.084338836 0.0772819 0.068764925 0.059672944 0.050933752 0.043169346 0.036802638 0.031820521 0.028203035 0.025436021 0.023290461 0.021561375 0.020247338][0.083143845 0.081486322 0.074512891 0.0655691 0.055419408 0.045163535 0.035729349 0.027824575 0.021702858 0.017307157 0.0143134 0.012152729 0.010590804 0.0094347047 0.0085881716][0.072631039 0.069344245 0.060755461 0.050599325 0.039862424 0.029721053 0.020986298 0.014223902 0.0093799885 0.0062368652 0.0042892564 0.0030034787 0.0021350312 0.0015122227 0.0010695404][0.058786344 0.054100368 0.044863444 0.034787107 0.02491493 0.016315069 0.0095035583 0.004699396 0.001603455 -0.00013184128 -0.0010536916 -0.0015932572 -0.0019259346 -0.0021550194 -0.0023104721][0.043414168 0.038086008 0.029467413 0.020816933 0.013011226 0.0068328939 0.0024165094 -0.00036646333 -0.0019278504 -0.0026551264 -0.0029532183 -0.0030846859 -0.0031525323 -0.0031984537 -0.003226713][0.028593661 0.023596263 0.016675819 0.010298377 0.0050140461 0.0012586855 -0.0011211096 -0.0024194191 -0.0030248058 -0.0032417458 -0.0032912309 -0.0033018128 -0.0033051355 -0.0033104226 -0.0033137309]]...]
INFO - root - 2017-12-09 19:54:51.106677: step 54910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:52m:33s remains)
INFO - root - 2017-12-09 19:54:59.830900: step 54920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:19m:02s remains)
INFO - root - 2017-12-09 19:55:08.378424: step 54930, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 67h:53m:38s remains)
INFO - root - 2017-12-09 19:55:17.022228: step 54940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:50m:42s remains)
INFO - root - 2017-12-09 19:55:25.658462: step 54950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:04m:55s remains)
INFO - root - 2017-12-09 19:55:34.239132: step 54960, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 69h:23m:56s remains)
INFO - root - 2017-12-09 19:55:42.789948: step 54970, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 67h:18m:36s remains)
INFO - root - 2017-12-09 19:55:51.189354: step 54980, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 64h:41m:11s remains)
INFO - root - 2017-12-09 19:55:59.924934: step 54990, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:34m:56s remains)
INFO - root - 2017-12-09 19:56:08.596949: step 55000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:47m:33s remains)
2017-12-09 19:56:09.510656: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19354409 0.20672321 0.22476311 0.24366495 0.26127654 0.27511388 0.28397477 0.28960353 0.28943825 0.28775585 0.28201842 0.2743673 0.263796 0.25155362 0.23841681][0.21262597 0.22552258 0.24361037 0.26302439 0.28233686 0.29801282 0.30856761 0.31503111 0.31520495 0.31279427 0.30553013 0.295205 0.2814939 0.26623315 0.25005764][0.21765931 0.23003796 0.24710059 0.26552516 0.28495598 0.30144373 0.31316856 0.32075351 0.32192239 0.31981829 0.31225771 0.30122131 0.28646976 0.27008206 0.2524114][0.21578756 0.22810154 0.24456035 0.2628262 0.28192461 0.29891893 0.3117899 0.32052755 0.32301074 0.32115826 0.31366342 0.30234352 0.28704363 0.26993588 0.2511445][0.21165715 0.22489837 0.24118806 0.25938123 0.27857885 0.29507762 0.30799413 0.31643477 0.31884623 0.31630611 0.3084358 0.29687738 0.28121421 0.26358515 0.24399978][0.2048597 0.21910344 0.23480655 0.25261912 0.27153039 0.28759503 0.30053297 0.3082273 0.31005388 0.3060973 0.29725391 0.284563 0.26804498 0.24976429 0.22991316][0.19842717 0.21311481 0.22735354 0.24315761 0.2606461 0.27569351 0.28842613 0.29460052 0.29522046 0.28966662 0.2789962 0.2644892 0.24620645 0.22668885 0.20633431][0.19192022 0.20472325 0.21603027 0.22967178 0.24463965 0.25795731 0.26915795 0.27442792 0.27396235 0.26627091 0.25356466 0.23659062 0.21665211 0.19602291 0.17510967][0.18905069 0.19936834 0.20603716 0.21562056 0.22661966 0.2367221 0.24440466 0.24691924 0.24379344 0.23381276 0.21925844 0.20025752 0.17933793 0.15847258 0.13819349][0.19156575 0.19878429 0.20065072 0.20523025 0.21091476 0.21615554 0.21877119 0.21692955 0.20944887 0.19614156 0.17904566 0.15888277 0.13821685 0.11832517 0.10005916][0.19892526 0.20397045 0.20203714 0.20209728 0.20244831 0.20188092 0.1987185 0.19124815 0.17827694 0.16066331 0.14019279 0.11853991 0.098205276 0.080291487 0.065011196][0.20863394 0.21235463 0.20710304 0.20366418 0.19970812 0.19391295 0.18486205 0.17156625 0.15314965 0.13068709 0.10661688 0.083710685 0.063984446 0.048035473 0.03560945][0.2175512 0.2192425 0.2111721 0.20469254 0.19720705 0.18757647 0.17396154 0.1556899 0.1325298 0.10621736 0.079635106 0.056115404 0.037660304 0.024270657 0.014990849][0.22512293 0.22382788 0.21225393 0.20208523 0.19064806 0.17701219 0.15944114 0.13750041 0.11150374 0.083627567 0.057044137 0.035070378 0.019123182 0.0089071095 0.002987698][0.22920538 0.2241836 0.2079245 0.19279918 0.17638168 0.15841484 0.13738652 0.11355259 0.087661251 0.061792329 0.038570944 0.020500422 0.0083849235 0.0015705687 -0.0016339833]]...]
INFO - root - 2017-12-09 19:56:18.095006: step 55010, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 67h:08m:21s remains)
INFO - root - 2017-12-09 19:56:26.824295: step 55020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:40m:23s remains)
INFO - root - 2017-12-09 19:56:35.378553: step 55030, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:19m:23s remains)
INFO - root - 2017-12-09 19:56:44.014781: step 55040, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 68h:56m:59s remains)
INFO - root - 2017-12-09 19:56:52.502182: step 55050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:57m:01s remains)
INFO - root - 2017-12-09 19:57:01.244833: step 55060, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 68h:08m:20s remains)
INFO - root - 2017-12-09 19:57:09.704213: step 55070, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:00m:57s remains)
INFO - root - 2017-12-09 19:57:18.173543: step 55080, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 64h:27m:30s remains)
INFO - root - 2017-12-09 19:57:26.971224: step 55090, loss = 0.88, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 65h:34m:05s remains)
INFO - root - 2017-12-09 19:57:35.629165: step 55100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:58m:17s remains)
2017-12-09 19:57:36.595634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033966426 -0.0033950205 -0.0033947763 -0.0033946843 -0.0033946382 -0.003394716 -0.00339476 -0.0033948561 -0.0033949011 -0.00339478 -0.0033946917 -0.0033948028 -0.0033954144 -0.0033965551 -0.0033983791][-0.0033949763 -0.0033933518 -0.0033932645 -0.00339336 -0.0033934976 -0.0033936636 -0.0033937606 -0.0033938207 -0.0033938289 -0.0033937804 -0.0033937232 -0.003393766 -0.0033941017 -0.0033947346 -0.0033959332][-0.0033950123 -0.0033933856 -0.0033935236 -0.0033938051 -0.0033941062 -0.003394377 -0.0033945609 -0.0033947283 -0.0033947926 -0.0033947339 -0.0033945844 -0.0033943832 -0.0033942654 -0.0033943057 -0.0033948177][-0.0033951045 -0.0033935378 -0.0033939236 -0.0033944352 -0.0033950473 -0.0033957148 -0.0033963178 -0.0033967972 -0.0033969569 -0.0033967523 -0.003396285 -0.0033956026 -0.0033948016 -0.0033942007 -0.0033939839][-0.0033952158 -0.003393729 -0.0033943933 -0.0033953674 -0.0033966503 -0.0033980417 -0.0033992049 -0.0034000869 -0.0034002955 -0.0033997509 -0.0033987004 -0.0033973779 -0.003395841 -0.0033945709 -0.0033937441][-0.003395336 -0.0033939818 -0.0033949767 -0.0033965427 -0.0033985125 -0.003400476 -0.0034020566 -0.0034031642 -0.0034034194 -0.0034025589 -0.0034010196 -0.0033991286 -0.0033970405 -0.0033951513 -0.003393787][-0.0033954503 -0.0033943127 -0.0033956778 -0.003397845 -0.0034003675 -0.0034027644 -0.003404798 -0.0034062664 -0.0034066075 -0.0034055717 -0.0034036008 -0.0034010953 -0.0033982822 -0.0033957385 -0.0033938631][-0.0033955048 -0.0033945737 -0.0033962452 -0.0033988017 -0.0034016937 -0.0034046024 -0.0034072113 -0.0034092069 -0.0034098073 -0.0034086802 -0.0034060567 -0.0034027323 -0.0033992243 -0.0033961404 -0.00339386][-0.0033953178 -0.0033946086 -0.0033964631 -0.0033992298 -0.0034024585 -0.0034058902 -0.003409104 -0.0034114895 -0.003411995 -0.0034105417 -0.003407425 -0.0034035668 -0.0033997064 -0.0033963523 -0.0033938838][-0.0033949441 -0.0033943073 -0.0033962305 -0.0033990978 -0.0034025596 -0.0034061144 -0.0034093491 -0.0034115005 -0.0034117294 -0.0034099757 -0.0034066753 -0.0034029428 -0.0033992357 -0.0033960596 -0.0033936971][-0.0033944119 -0.0033937162 -0.0033956037 -0.0033982969 -0.0034015148 -0.0034045968 -0.0034073666 -0.0034090113 -0.0034089708 -0.0034072285 -0.0034042618 -0.0034010203 -0.0033979283 -0.0033953246 -0.0033933057][-0.0033940205 -0.0033931786 -0.0033947991 -0.0033970487 -0.0033996096 -0.0034020636 -0.0034040988 -0.003405161 -0.0034049002 -0.0034034667 -0.0034012143 -0.0033987737 -0.0033963914 -0.0033943611 -0.0033928121][-0.0033937728 -0.003392587 -0.003393821 -0.0033953949 -0.0033971847 -0.0033988429 -0.0034001446 -0.0034007165 -0.0034004441 -0.0033994955 -0.0033980096 -0.003396323 -0.0033946249 -0.0033932221 -0.0033921613][-0.0033935527 -0.0033918829 -0.0033926594 -0.0033935721 -0.0033945902 -0.0033955425 -0.003396285 -0.0033965816 -0.0033964454 -0.003395912 -0.0033950186 -0.0033939483 -0.0033929059 -0.0033920722 -0.0033914561][-0.0033934095 -0.003391349 -0.003391677 -0.0033920682 -0.003392502 -0.0033928866 -0.0033931835 -0.0033932526 -0.0033931802 -0.0033929558 -0.0033925609 -0.0033920554 -0.0033915348 -0.0033911355 -0.003390861]]...]
INFO - root - 2017-12-09 19:57:45.122069: step 55110, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:00m:56s remains)
INFO - root - 2017-12-09 19:57:54.022290: step 55120, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:19m:10s remains)
INFO - root - 2017-12-09 19:58:02.611482: step 55130, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 63h:44m:36s remains)
INFO - root - 2017-12-09 19:58:11.309866: step 55140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:17m:17s remains)
INFO - root - 2017-12-09 19:58:19.943982: step 55150, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 65h:41m:05s remains)
INFO - root - 2017-12-09 19:58:28.663813: step 55160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 68h:05m:10s remains)
INFO - root - 2017-12-09 19:58:37.198139: step 55170, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:17m:39s remains)
INFO - root - 2017-12-09 19:58:45.663777: step 55180, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 64h:53m:08s remains)
INFO - root - 2017-12-09 19:58:54.125879: step 55190, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 63h:34m:17s remains)
INFO - root - 2017-12-09 19:59:02.811739: step 55200, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:23m:00s remains)
2017-12-09 19:59:03.812408: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13724779 0.13846114 0.13798976 0.13683449 0.1359378 0.13514058 0.13487808 0.13433778 0.13387251 0.13297999 0.1316179 0.12973614 0.12670198 0.12239268 0.11706937][0.13821544 0.14091545 0.14211974 0.14210929 0.14177798 0.14118007 0.1408027 0.14022718 0.13961633 0.13909753 0.13832717 0.13640508 0.13291429 0.12813456 0.12239464][0.13225308 0.13701537 0.1401436 0.14166789 0.1424858 0.14210711 0.1419493 0.14119458 0.14051072 0.13974009 0.13904668 0.13707714 0.13384628 0.12958603 0.12420785][0.1257389 0.13261794 0.13764259 0.14087464 0.14292052 0.14342998 0.14324422 0.14192216 0.14061998 0.13959365 0.13896394 0.13704349 0.13417374 0.13079068 0.12663326][0.1179584 0.1274417 0.13482101 0.1396589 0.142442 0.14365762 0.14353365 0.14197445 0.13994353 0.13825096 0.1373653 0.13560466 0.13396864 0.1318738 0.12933603][0.11060512 0.12176894 0.13016869 0.13635638 0.14018597 0.14167109 0.14151812 0.1397624 0.13738264 0.1349297 0.13335037 0.13187319 0.1310509 0.13038878 0.12998527][0.10431141 0.11649842 0.12542662 0.13186254 0.13580199 0.13734682 0.13675392 0.13502006 0.13238122 0.12979323 0.12794182 0.12670304 0.12668581 0.12718409 0.12822062][0.098014519 0.11027963 0.1187827 0.1253823 0.12944873 0.13088323 0.12989677 0.1281749 0.12570436 0.12266995 0.1201602 0.11903888 0.11922908 0.12028295 0.12225337][0.089941762 0.10134514 0.10866456 0.11401622 0.11751703 0.11893813 0.11799414 0.11623542 0.11420445 0.11190567 0.10964848 0.10815144 0.10819679 0.10907726 0.11126407][0.076891266 0.08651343 0.092269152 0.096542664 0.099170759 0.10033671 0.099395663 0.0980634 0.096646309 0.094869673 0.093347475 0.092171922 0.092246123 0.093060434 0.095182084][0.060532309 0.067933284 0.072140984 0.074893571 0.076450147 0.077176668 0.076262593 0.074884944 0.07378035 0.072940461 0.072299905 0.071817778 0.072601773 0.07393568 0.076382644][0.041516218 0.046383388 0.049472485 0.051544961 0.052352682 0.052215915 0.051484548 0.050365858 0.049047887 0.048383243 0.047965877 0.048418678 0.049415585 0.05135379 0.054838773][0.024183668 0.027174436 0.029166048 0.030071272 0.03016592 0.029701078 0.028862761 0.028068883 0.027324438 0.026845811 0.026513521 0.027284665 0.02845489 0.030702176 0.034665205][0.013641473 0.015019918 0.016038889 0.016247371 0.0158849 0.015130784 0.014186693 0.013241926 0.012429664 0.01191397 0.011501719 0.012024297 0.013026617 0.015256304 0.019277938][0.01091079 0.011165536 0.011507624 0.010993281 0.010174992 0.0093893362 0.0083289156 0.0072968053 0.0064407578 0.0056273295 0.0049368646 0.0050921133 0.0057847053 0.0078446493 0.011667345]]...]
INFO - root - 2017-12-09 19:59:12.475764: step 55210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:33m:26s remains)
INFO - root - 2017-12-09 19:59:21.096419: step 55220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:32m:36s remains)
INFO - root - 2017-12-09 19:59:29.811890: step 55230, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 69h:22m:29s remains)
INFO - root - 2017-12-09 19:59:38.423975: step 55240, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 63h:30m:55s remains)
INFO - root - 2017-12-09 19:59:47.204007: step 55250, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:21m:55s remains)
INFO - root - 2017-12-09 19:59:56.026812: step 55260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:40m:16s remains)
INFO - root - 2017-12-09 20:00:04.700716: step 55270, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 68h:44m:59s remains)
INFO - root - 2017-12-09 20:00:13.247621: step 55280, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 69h:46m:36s remains)
INFO - root - 2017-12-09 20:00:22.064383: step 55290, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.914 sec/batch; 70h:24m:24s remains)
INFO - root - 2017-12-09 20:00:30.779110: step 55300, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 69h:04m:12s remains)
2017-12-09 20:00:31.617405: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21201916 0.22354056 0.23192446 0.23496325 0.23486352 0.23202465 0.22997221 0.22733159 0.22509469 0.22528644 0.22626939 0.22956021 0.23268978 0.23712978 0.23984924][0.22049198 0.23329693 0.24105725 0.24539934 0.24594641 0.24378139 0.2423937 0.2400668 0.238154 0.23806524 0.2404246 0.24420172 0.24790615 0.25250396 0.25536954][0.21564551 0.22760238 0.23442732 0.23889509 0.23909268 0.23785876 0.23796465 0.23679076 0.23609832 0.23675393 0.24040879 0.245477 0.25023478 0.25551203 0.25828573][0.20890762 0.22197671 0.22842403 0.23250818 0.23206927 0.23109142 0.23211269 0.23212779 0.23319128 0.23511608 0.24086827 0.24787784 0.255234 0.26217356 0.26574612][0.20192711 0.21769437 0.22444527 0.22811383 0.22781038 0.22716126 0.22820492 0.22906859 0.23183182 0.23508754 0.24203105 0.25112858 0.26056883 0.26871926 0.27387744][0.19401395 0.21144482 0.21920791 0.22381972 0.22348309 0.22257669 0.22371162 0.22394881 0.22787142 0.23240006 0.24144763 0.25214145 0.26325518 0.27306142 0.27974358][0.17953746 0.19896705 0.20835659 0.2130501 0.2127979 0.21272312 0.21368767 0.21398668 0.21843147 0.22300448 0.23247048 0.24401367 0.25580284 0.26756334 0.27661878][0.15572257 0.17488264 0.18555319 0.19064641 0.19101503 0.1905278 0.19086987 0.19137473 0.19547209 0.20047313 0.21099444 0.22397158 0.23726082 0.25179306 0.26371527][0.12037685 0.13716498 0.14821981 0.15284121 0.15368581 0.15315713 0.15292917 0.15310027 0.15634221 0.16220851 0.17351499 0.18908028 0.20561376 0.22474508 0.24125376][0.0817415 0.0956133 0.10460465 0.10787456 0.10807094 0.1072306 0.10525058 0.10454323 0.10710298 0.11333089 0.12604082 0.14415443 0.16463435 0.18843471 0.20975029][0.046733327 0.05639752 0.06389299 0.06736204 0.067255706 0.066493809 0.063827679 0.062591694 0.063496612 0.068833008 0.080796413 0.0982086 0.12021352 0.14673796 0.17180479][0.020111544 0.026295265 0.031345423 0.033856075 0.034892958 0.035401151 0.033728085 0.032442097 0.032663111 0.036515478 0.0454809 0.060290404 0.080391414 0.10500202 0.13026993][0.0042240033 0.0065284297 0.0091432808 0.011194581 0.012237316 0.013230959 0.012997366 0.013370633 0.013919119 0.015984595 0.022092003 0.031606045 0.0468801 0.067227237 0.0893103][-0.0022612358 -0.0016386105 -0.00083557004 -0.00050768536 -7.346971e-05 0.00068270345 0.0008851781 0.0015660052 0.0020172831 0.0036903115 0.0075168964 0.012877442 0.022298077 0.035056308 0.051128328][-0.0033806704 -0.0033746327 -0.0033730934 -0.0033674038 -0.0033068459 -0.0031007335 -0.0030937579 -0.0027695741 -0.0020807846 -0.001325395 0.00046457513 0.002872606 0.00788088 0.014750695 0.023993436]]...]
INFO - root - 2017-12-09 20:00:40.305547: step 55310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:16m:34s remains)
INFO - root - 2017-12-09 20:00:49.079696: step 55320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:15m:23s remains)
INFO - root - 2017-12-09 20:00:57.847153: step 55330, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 66h:18m:21s remains)
INFO - root - 2017-12-09 20:01:06.518417: step 55340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:08m:37s remains)
INFO - root - 2017-12-09 20:01:15.262211: step 55350, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 68h:09m:10s remains)
INFO - root - 2017-12-09 20:01:23.828231: step 55360, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 67h:18m:16s remains)
INFO - root - 2017-12-09 20:01:32.404406: step 55370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:33m:00s remains)
INFO - root - 2017-12-09 20:01:41.070060: step 55380, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.894 sec/batch; 68h:49m:29s remains)
INFO - root - 2017-12-09 20:01:49.718355: step 55390, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:04m:39s remains)
INFO - root - 2017-12-09 20:01:58.435613: step 55400, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 64h:20m:41s remains)
2017-12-09 20:01:59.296933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033829622 -0.003380233 -0.003380276 -0.0033807924 -0.0033812912 -0.0033818332 -0.0033821256 -0.0033821841 -0.0033818565 -0.003381388 -0.003380937 -0.0033806176 -0.0033802839 -0.0033799824 -0.0033799049][-0.0033801568 -0.0033771405 -0.0033773191 -0.0033779738 -0.0033787291 -0.0033793966 -0.00337965 -0.003379646 -0.0033792569 -0.0033786015 -0.0033780462 -0.0033777326 -0.0033773789 -0.00337706 -0.003376951][-0.0033799252 -0.003376781 -0.0033770984 -0.0033778497 -0.0033787331 -0.0033794548 -0.0033797799 -0.0033798 -0.003379432 -0.0033788912 -0.0033784544 -0.00337826 -0.0033779002 -0.0033774709 -0.0033773845][-0.00338081 -0.0033775817 -0.0033778537 -0.0033787086 -0.0033795638 -0.0033801973 -0.0033806222 -0.0033808332 -0.0033806788 -0.0033801706 -0.0033799307 -0.0033797885 -0.0033792404 -0.0033787126 -0.0033785561][-0.0033821126 -0.0033788867 -0.0033791231 -0.0033799722 -0.0033806402 -0.0033811792 -0.0033818269 -0.0033821936 -0.0033823461 -0.0033819289 -0.0033815848 -0.003381094 -0.003380171 -0.0033795757 -0.0033793224][-0.0033831105 -0.0033800497 -0.0033804672 -0.003381229 -0.0033818011 -0.0033824048 -0.0033831524 -0.0033838227 -0.0033844076 -0.0033843748 -0.0033840344 -0.003382995 -0.0033814206 -0.0033805198 -0.003380151][-0.0033833969 -0.0033806679 -0.0033813368 -0.0033821238 -0.0033828851 -0.0033836053 -0.0033837836 -0.003384874 -0.0033866391 -0.0033872093 -0.0033867923 -0.00338519 -0.0033828621 -0.0033812874 -0.0033805622][-0.0033831834 -0.0033807724 -0.0033818355 -0.0033832961 -0.0033846979 -0.0033856656 -0.0033851373 -0.0033866542 -0.0033892128 -0.0033901713 -0.0033897681 -0.0033877336 -0.00338487 -0.0033827117 -0.0033813326][-0.0033830262 -0.0033808758 -0.0033824125 -0.0033847995 -0.0033872817 -0.0033896649 -0.0033911837 -0.0033924992 -0.0033935742 -0.0033941094 -0.0033935488 -0.0033911171 -0.00338788 -0.0033856058 -0.0033840544][-0.0033831396 -0.0033810341 -0.0033828907 -0.0033858905 -0.0033892321 -0.0033929748 -0.0033963732 -0.0033984741 -0.0033991698 -0.0033988124 -0.00339724 -0.0033937453 -0.0033899443 -0.0033874651 -0.0033857762][-0.0033834241 -0.0033810923 -0.0033829205 -0.0033857753 -0.0033895664 -0.0033940072 -0.0033984475 -0.003401363 -0.0034024112 -0.0034017968 -0.0033995495 -0.0033955257 -0.0033915318 -0.0033892319 -0.0033881334][-0.0033838563 -0.00338115 -0.0033824602 -0.0033848863 -0.0033882984 -0.0033922251 -0.0033964296 -0.0033996864 -0.0034016559 -0.0034018171 -0.0034003821 -0.0033976203 -0.0033947891 -0.0033935097 -0.0033934275][-0.0033842064 -0.0033811936 -0.0033817943 -0.0033833336 -0.0033856989 -0.0033886384 -0.0033918635 -0.0033947853 -0.0033976517 -0.00339962 -0.0034003491 -0.0034005768 -0.0034001234 -0.0034005363 -0.0034013153][-0.0033846211 -0.0033812691 -0.0033812835 -0.0033819953 -0.0033831461 -0.003384833 -0.0033869247 -0.0033893627 -0.0033926244 -0.0033960592 -0.0033988797 -0.003401231 -0.0034031107 -0.0034051114 -0.0034062306][-0.0033849236 -0.003381263 -0.0033809189 -0.0033810982 -0.0033815375 -0.0033823077 -0.0033833252 -0.0033850383 -0.0033880277 -0.0033914512 -0.0033949241 -0.0033984077 -0.0034015852 -0.0034045146 -0.0034062585]]...]
INFO - root - 2017-12-09 20:02:07.875872: step 55410, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 65h:59m:41s remains)
INFO - root - 2017-12-09 20:02:16.446014: step 55420, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 64h:18m:25s remains)
INFO - root - 2017-12-09 20:02:24.922015: step 55430, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 57h:52m:10s remains)
INFO - root - 2017-12-09 20:02:33.452600: step 55440, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:56m:12s remains)
INFO - root - 2017-12-09 20:02:41.995779: step 55450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:30m:11s remains)
INFO - root - 2017-12-09 20:02:50.731379: step 55460, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 63h:52m:17s remains)
INFO - root - 2017-12-09 20:02:59.335636: step 55470, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 67h:23m:00s remains)
INFO - root - 2017-12-09 20:03:07.923254: step 55480, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 56h:23m:38s remains)
INFO - root - 2017-12-09 20:03:16.555229: step 55490, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 65h:41m:12s remains)
INFO - root - 2017-12-09 20:03:25.258186: step 55500, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 68h:09m:56s remains)
2017-12-09 20:03:26.220040: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.62175661 0.61708611 0.60219628 0.58725309 0.56387508 0.52751476 0.46855545 0.38453791 0.28356582 0.18119596 0.096107483 0.039639141 0.010273252 -0.00054634432 -0.0029720648][0.61505735 0.61196917 0.59805977 0.58347136 0.56238157 0.53080827 0.47900105 0.4033092 0.30821031 0.20560066 0.11556565 0.051045835 0.015123365 0.00079697859 -0.002771839][0.55839252 0.55841923 0.55053544 0.54406953 0.53326613 0.51310796 0.47295326 0.40864158 0.32214454 0.22421853 0.13388523 0.064308047 0.022057554 0.0029794278 -0.0024775884][0.4750846 0.47888097 0.48049283 0.48739687 0.49265754 0.4895643 0.46627405 0.41595116 0.33825269 0.24350014 0.15109132 0.076526828 0.028829908 0.005515012 -0.0020248219][0.37920886 0.38781536 0.39999053 0.42159513 0.44467321 0.4606747 0.45498961 0.41851252 0.34973803 0.25892457 0.16540749 0.087079108 0.034717418 0.0078090178 -0.0015790517][0.28044462 0.29699433 0.32175505 0.35867324 0.39873278 0.43117523 0.43935883 0.41362935 0.35191444 0.26498175 0.1720977 0.092872396 0.03827152 0.0092444578 -0.0012996099][0.19146271 0.21480767 0.25142983 0.30259448 0.35786113 0.40311328 0.42001373 0.39984423 0.34177747 0.25862613 0.16886437 0.09194655 0.03832645 0.0094527025 -0.0012568624][0.11852692 0.14551243 0.18991402 0.25039554 0.31568211 0.36883888 0.39095777 0.37430397 0.31906885 0.24005568 0.15524226 0.083903 0.034585938 0.0082284594 -0.0014825931][0.064868882 0.092221431 0.13725212 0.19927114 0.26667023 0.32173187 0.34572187 0.33183771 0.28133935 0.20948094 0.13296352 0.070018925 0.027543139 0.0057654697 -0.0019436909][0.032182772 0.056278262 0.09628389 0.15179832 0.21280503 0.2632888 0.28574297 0.27449542 0.23078893 0.16903396 0.10438562 0.052731793 0.0190961 0.0027983363 -0.0025066258][0.016442603 0.034783635 0.066732712 0.11161986 0.16141509 0.20280236 0.22089773 0.21137233 0.17530957 0.12560591 0.074683405 0.035215635 0.010885838 7.5748889e-05 -0.0029509733][0.010732259 0.023183879 0.045683652 0.078402065 0.11578817 0.14714414 0.16062051 0.15280566 0.12468787 0.087022625 0.049310811 0.021178635 0.0048374664 -0.0017123068 -0.0031878469][0.0070254831 0.015347559 0.030607251 0.052596435 0.077949189 0.099336423 0.10842068 0.10244324 0.082059011 0.055440877 0.029408561 0.010856302 0.00084043737 -0.0026873567 -0.0032904109][0.0027856191 0.0081358179 0.017524546 0.030901467 0.04659814 0.059851803 0.065355875 0.060965821 0.047547955 0.030526267 0.014422301 0.0036692584 -0.0015710561 -0.0031369864 -0.0033177098][-0.0009600874 0.0018321297 0.0069278525 0.014070988 0.022400362 0.029543692 0.032687563 0.03023815 0.022703003 0.013202287 0.00466632 -0.00054010074 -0.002762286 -0.0032847049 -0.0033217561]]...]
INFO - root - 2017-12-09 20:03:34.807085: step 55510, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 64h:56m:22s remains)
INFO - root - 2017-12-09 20:03:43.240790: step 55520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:41m:05s remains)
INFO - root - 2017-12-09 20:03:51.886034: step 55530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:58m:29s remains)
INFO - root - 2017-12-09 20:04:00.533868: step 55540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:54m:41s remains)
INFO - root - 2017-12-09 20:04:09.327586: step 55550, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 68h:10m:48s remains)
INFO - root - 2017-12-09 20:04:17.937431: step 55560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:15m:43s remains)
INFO - root - 2017-12-09 20:04:26.490252: step 55570, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 66h:05m:41s remains)
INFO - root - 2017-12-09 20:04:34.911323: step 55580, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 64h:14m:13s remains)
INFO - root - 2017-12-09 20:04:43.251726: step 55590, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 65h:41m:31s remains)
INFO - root - 2017-12-09 20:04:52.019651: step 55600, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 68h:24m:26s remains)
2017-12-09 20:04:53.088320: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24799687 0.25574309 0.25975609 0.25935444 0.25935414 0.25534031 0.25071898 0.24429804 0.23849203 0.23450747 0.23139532 0.23056005 0.2299557 0.22969748 0.22706716][0.32117584 0.33358294 0.34052122 0.34694561 0.35255423 0.3516039 0.34937114 0.34384215 0.33857062 0.33156127 0.32412151 0.32076916 0.31926325 0.32007334 0.31851622][0.39603257 0.41228226 0.42275804 0.43417835 0.44565216 0.45091119 0.45371187 0.45166919 0.44897974 0.44143692 0.432453 0.42657423 0.42315936 0.42210549 0.41931957][0.45907772 0.48087531 0.49673009 0.51294231 0.52907979 0.54065824 0.54863542 0.54998624 0.55029666 0.54513049 0.53713775 0.530581 0.52532405 0.52304816 0.5194875][0.50087416 0.52735084 0.54851276 0.57088643 0.59273511 0.60973948 0.62208784 0.628113 0.63145983 0.62822461 0.62160212 0.61541313 0.61040872 0.60785121 0.60470796][0.52157962 0.55251527 0.5764454 0.60108018 0.62506819 0.64684027 0.66336495 0.67285383 0.67922562 0.67970365 0.67686105 0.67258441 0.66906595 0.66729152 0.66480041][0.52236128 0.55626166 0.58122718 0.60717136 0.63114065 0.65293467 0.66947174 0.68196279 0.69135541 0.69462383 0.69502807 0.69395322 0.69375676 0.69387764 0.69317132][0.502156 0.53637856 0.56163496 0.5854407 0.60762703 0.62885582 0.64505 0.65732986 0.6658898 0.67140508 0.67559689 0.67733222 0.67998409 0.68263161 0.68450177][0.45896691 0.49367493 0.51804477 0.5408718 0.56136751 0.57980293 0.59374696 0.60419327 0.61111712 0.61673796 0.62161189 0.62549138 0.63063008 0.6368131 0.64193434][0.39493817 0.4263548 0.447279 0.46802095 0.48722094 0.50382918 0.51630086 0.52586508 0.53212422 0.53630644 0.54007381 0.54426193 0.55124843 0.55902755 0.56563127][0.3161993 0.34183818 0.35789764 0.3739987 0.38941893 0.40322271 0.41404715 0.42255285 0.42842096 0.43236145 0.43589672 0.43970898 0.44584265 0.45277965 0.45874706][0.2319755 0.24978575 0.26033661 0.27127784 0.28233251 0.2927784 0.30136722 0.30853388 0.3138068 0.3174569 0.32050139 0.3236649 0.32858777 0.33383453 0.33840114][0.15326786 0.16432019 0.17051964 0.17698103 0.18403691 0.19106342 0.19710332 0.20242986 0.20668715 0.2096516 0.21201751 0.21429864 0.2177878 0.22138165 0.22444133][0.087429337 0.09430597 0.098517925 0.10278726 0.10740005 0.11219352 0.11655045 0.12056719 0.12419912 0.12666026 0.12837255 0.12981576 0.1319989 0.13403569 0.13577913][0.040564548 0.044104245 0.046736721 0.049366552 0.052276861 0.055450309 0.058401983 0.061364055 0.064179316 0.06625697 0.067774191 0.068883292 0.070384786 0.071510039 0.072285138]]...]
INFO - root - 2017-12-09 20:05:01.587307: step 55610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:01m:39s remains)
INFO - root - 2017-12-09 20:05:10.142637: step 55620, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 62h:54m:29s remains)
INFO - root - 2017-12-09 20:05:18.692047: step 55630, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 63h:19m:31s remains)
INFO - root - 2017-12-09 20:05:27.157358: step 55640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:28m:30s remains)
INFO - root - 2017-12-09 20:05:35.839145: step 55650, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 66h:30m:45s remains)
INFO - root - 2017-12-09 20:05:44.518140: step 55660, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 69h:44m:12s remains)
INFO - root - 2017-12-09 20:05:53.202574: step 55670, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 69h:23m:15s remains)
INFO - root - 2017-12-09 20:06:01.864036: step 55680, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 64h:21m:41s remains)
INFO - root - 2017-12-09 20:06:10.306712: step 55690, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 66h:42m:31s remains)
INFO - root - 2017-12-09 20:06:18.837208: step 55700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:27m:15s remains)
2017-12-09 20:06:19.716033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033613977 -0.0033598819 -0.0033603027 -0.0033592463 -0.003357461 -0.0033563597 -0.0033568628 -0.0033589781 -0.0033623297 -0.0033661479 -0.0033697507 -0.0033713353 -0.003370065 -0.0033672557 -0.0033641187][-0.0033568458 -0.0033543902 -0.0033544383 -0.0033539974 -0.0033532297 -0.0033531471 -0.0033550183 -0.003358589 -0.0033637201 -0.0033692569 -0.0033744255 -0.0033767968 -0.0033752806 -0.0033713484 -0.0033666098][-0.0033561767 -0.0033529811 -0.0033523724 -0.0033518798 -0.0033515291 -0.0033522681 -0.003355247 -0.003359946 -0.0033664182 -0.0033734327 -0.0033798448 -0.0033832353 -0.0033821876 -0.0033780567 -0.0033719807][-0.003356155 -0.0033527445 -0.0033516893 -0.003351059 -0.0033508232 -0.003351887 -0.0033553175 -0.0033604836 -0.003367631 -0.0033750567 -0.0033820311 -0.0033862276 -0.0033857981 -0.00338197 -0.0033756895][-0.0033569336 -0.0033535454 -0.0033525431 -0.0033519045 -0.0033518763 -0.0033530078 -0.0033560926 -0.0033606919 -0.0033673297 -0.0033742059 -0.003380598 -0.0033845701 -0.0033846826 -0.0033818011 -0.0033761528][-0.003358342 -0.0033552202 -0.0033543948 -0.0033536581 -0.00335341 -0.0033541503 -0.0033564495 -0.0033599103 -0.0033650636 -0.0033704247 -0.0033756255 -0.0033788497 -0.0033794006 -0.003377961 -0.0033737724][-0.0033595455 -0.0033571229 -0.0033568495 -0.0033563657 -0.0033557622 -0.0033556158 -0.0033566412 -0.0033586188 -0.0033620636 -0.0033658308 -0.0033694473 -0.0033715894 -0.0033721265 -0.0033716003 -0.0033691055][-0.0033605327 -0.0033590333 -0.0033596558 -0.0033596121 -0.0033588968 -0.0033578703 -0.0033570551 -0.0033569357 -0.0033584228 -0.0033605895 -0.003362901 -0.0033641886 -0.0033646261 -0.0033646403 -0.003363563][-0.0033606056 -0.0033593434 -0.0033607543 -0.0033613238 -0.0033610768 -0.0033601432 -0.0033587785 -0.0033571641 -0.0033564249 -0.0033566849 -0.0033575853 -0.0033582908 -0.0033587245 -0.0033588398 -0.0033584184][-0.0033600559 -0.0033585543 -0.00336028 -0.0033611446 -0.0033611907 -0.0033606687 -0.0033595383 -0.003357698 -0.003355918 -0.0033548186 -0.0033544984 -0.0033545578 -0.0033548083 -0.0033550642 -0.0033549995][-0.0033592759 -0.0033572032 -0.0033586547 -0.0033594472 -0.0033596826 -0.0033595171 -0.0033588314 -0.0033575355 -0.0033559685 -0.0033545743 -0.0033535981 -0.00335286 -0.0033526495 -0.0033528178 -0.0033530183][-0.0033580132 -0.0033552193 -0.0033561236 -0.0033564798 -0.0033565229 -0.0033564433 -0.0033560337 -0.0033553431 -0.0033545098 -0.0033535555 -0.0033528479 -0.0033521482 -0.0033517242 -0.003351771 -0.0033520591][-0.0033567313 -0.0033533773 -0.0033537811 -0.00335379 -0.0033537082 -0.0033536288 -0.0033532865 -0.0033528244 -0.0033523422 -0.0033519063 -0.003351585 -0.003351321 -0.0033510728 -0.003351168 -0.0033515047][-0.0033557478 -0.0033520842 -0.0033522272 -0.0033521566 -0.003352121 -0.0033520947 -0.0033518623 -0.0033514576 -0.0033510558 -0.0033507345 -0.0033505308 -0.003350408 -0.0033503403 -0.0033505352 -0.0033508902][-0.0033551455 -0.0033514278 -0.0033514563 -0.0033514006 -0.0033514928 -0.0033515107 -0.0033512919 -0.0033509817 -0.0033507128 -0.0033504907 -0.0033502229 -0.0033500397 -0.0033500665 -0.0033502206 -0.0033504337]]...]
INFO - root - 2017-12-09 20:06:28.190457: step 55710, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 65h:52m:40s remains)
INFO - root - 2017-12-09 20:06:36.672045: step 55720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:13m:58s remains)
INFO - root - 2017-12-09 20:06:45.384289: step 55730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:48m:09s remains)
INFO - root - 2017-12-09 20:06:53.900350: step 55740, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 68h:14m:01s remains)
INFO - root - 2017-12-09 20:07:02.571933: step 55750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:30m:11s remains)
INFO - root - 2017-12-09 20:07:11.160965: step 55760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 66h:02m:23s remains)
INFO - root - 2017-12-09 20:07:19.726220: step 55770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:09m:22s remains)
INFO - root - 2017-12-09 20:07:28.425124: step 55780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:05m:17s remains)
INFO - root - 2017-12-09 20:07:36.831282: step 55790, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 65h:02m:04s remains)
INFO - root - 2017-12-09 20:07:45.304130: step 55800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:26m:20s remains)
2017-12-09 20:07:46.160081: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43769369 0.43800348 0.43103096 0.41337335 0.38797185 0.35288984 0.31537202 0.27769738 0.24494828 0.22099797 0.20639186 0.20013805 0.19569042 0.18942921 0.18166451][0.40170658 0.40446025 0.40133765 0.38948488 0.3694959 0.33845031 0.30192646 0.26184368 0.22451308 0.19521821 0.17479669 0.16376558 0.15650487 0.15049879 0.14440598][0.35776111 0.36639586 0.37134188 0.36841351 0.35688278 0.33343622 0.30057865 0.2610921 0.22007686 0.18485209 0.15764067 0.14003453 0.12911616 0.12138719 0.116336][0.32348683 0.33511448 0.34441724 0.35010791 0.34824377 0.33455163 0.30992231 0.27514777 0.2364084 0.19895604 0.16716501 0.14423555 0.12951082 0.12013058 0.11477688][0.30856782 0.31932262 0.32818633 0.33611143 0.33973712 0.33434489 0.31879058 0.2925902 0.26111424 0.22751531 0.19619446 0.17110378 0.1536478 0.14314073 0.13732637][0.31179559 0.32009724 0.32666498 0.33430547 0.34099874 0.34200421 0.33441928 0.31717744 0.29413366 0.26788452 0.24025308 0.21575868 0.1979298 0.1865976 0.18004777][0.3222689 0.32658592 0.32961974 0.33602324 0.34529769 0.35241923 0.35375091 0.3478573 0.33503532 0.31700915 0.29439977 0.27209666 0.25347897 0.24037904 0.23182589][0.32779574 0.32826436 0.32647151 0.33042651 0.34090465 0.35407397 0.36381057 0.3686358 0.36696407 0.35890183 0.34381628 0.32548627 0.30831903 0.29405597 0.28324169][0.33542511 0.32926464 0.32075682 0.32123119 0.33154273 0.3483904 0.36627042 0.38187459 0.39186269 0.39417541 0.38693327 0.37302342 0.35639024 0.34109998 0.3278][0.34212559 0.33357966 0.32213122 0.3194105 0.32866022 0.34804192 0.37091112 0.39389575 0.41177684 0.4228766 0.42279181 0.41301849 0.39714965 0.38013127 0.36424592][0.35731387 0.34852031 0.33515212 0.33217645 0.34128681 0.36171311 0.38692859 0.41334546 0.4346883 0.44858393 0.45141488 0.44349885 0.42821071 0.40972552 0.3916724][0.37319756 0.368626 0.3583495 0.35731304 0.36619303 0.38623813 0.41188323 0.43834925 0.45923108 0.47295916 0.47593972 0.46758649 0.45100322 0.43098328 0.41141942][0.38644508 0.38707504 0.38098115 0.383393 0.39391258 0.41485158 0.44080222 0.46608141 0.4856106 0.49689263 0.49789625 0.4876343 0.46929979 0.44709712 0.42546773][0.39991084 0.40502545 0.4026604 0.40784174 0.41959003 0.43937883 0.46439922 0.48785383 0.50555867 0.51518631 0.51547283 0.50456011 0.48472765 0.46120328 0.43805149][0.40675905 0.41901207 0.42272472 0.43205753 0.44551849 0.46491611 0.48757514 0.50743586 0.52151775 0.5276472 0.52567512 0.5136736 0.49336237 0.46919057 0.44538581]]...]
INFO - root - 2017-12-09 20:07:54.646420: step 55810, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 63h:57m:09s remains)
INFO - root - 2017-12-09 20:08:03.138098: step 55820, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 67h:13m:40s remains)
INFO - root - 2017-12-09 20:08:11.832041: step 55830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 66h:50m:09s remains)
INFO - root - 2017-12-09 20:08:20.181776: step 55840, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:06m:51s remains)
INFO - root - 2017-12-09 20:08:28.732155: step 55850, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 64h:55m:35s remains)
INFO - root - 2017-12-09 20:08:37.226171: step 55860, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 62h:27m:19s remains)
INFO - root - 2017-12-09 20:08:45.687943: step 55870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:50m:25s remains)
INFO - root - 2017-12-09 20:08:54.332309: step 55880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:01m:31s remains)
INFO - root - 2017-12-09 20:09:02.756091: step 55890, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:19m:43s remains)
INFO - root - 2017-12-09 20:09:11.366362: step 55900, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 70h:02m:40s remains)
2017-12-09 20:09:12.243933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033846975 -0.0033711225 -0.0033471067 -0.0033089512 -0.0032468496 -0.0031530617 -0.0030605895 -0.0030101945 -0.0030091945 -0.0030571376 -0.0031467022 -0.0032466557 -0.0033209547 -0.0033575883 -0.0033745533][-0.0033604994 -0.0032942616 -0.0031859782 -0.003038184 -0.0028463493 -0.0026295551 -0.0024627373 -0.0024138526 -0.002477081 -0.0026338096 -0.0028569684 -0.0030769673 -0.003233612 -0.0033145191 -0.0033535031][-0.0032347851 -0.0029770918 -0.0025841896 -0.0020970497 -0.0015658073 -0.0010982424 -0.00084756338 -0.00089767366 -0.0011974494 -0.001668192 -0.0022212933 -0.002713013 -0.0030478952 -0.00322517 -0.0033089044][-0.0028359727 -0.0020728181 -0.00097147818 0.00031056069 0.0015614724 0.0024862716 0.0027970902 0.0024370647 0.0015504777 0.00036730594 -0.00088134268 -0.0019297707 -0.0026277988 -0.0030151014 -0.003198663][-0.0019551914 -0.00020162645 0.0022158951 0.0049102316 0.0073742745 0.0090223774 0.0093669239 0.0084203323 0.006483688 0.0040550558 0.0016023412 -0.00044615567 -0.001819664 -0.0026077186 -0.0029820944][-0.00054344744 0.0026417088 0.0068857931 0.01147309 0.015514694 0.018073816 0.018428236 0.016697694 0.013372071 0.0093065975 0.0052517597 0.001812184 -0.00054819 -0.0019588415 -0.0026425158][0.0011138513 0.0058369925 0.01198474 0.018508306 0.024151815 0.027640032 0.028050309 0.025553236 0.02083037 0.015093833 0.0093511352 0.0043940544 0.00090686954 -0.0012243898 -0.0022729207][0.0022885317 0.0079403445 0.015183132 0.022780109 0.0292976 0.033298373 0.033810236 0.030969488 0.025538597 0.018905485 0.012171745 0.0062538898 0.0019673456 -0.00070775766 -0.0020445441][0.0024126705 0.0079555642 0.014991609 0.022323091 0.028594622 0.032447562 0.033018276 0.030358067 0.025176367 0.018787915 0.012203155 0.0063422173 0.001995032 -0.00075268419 -0.0021325708][0.0013092116 0.005699479 0.011270164 0.01708059 0.02206257 0.025143946 0.025673268 0.023611104 0.019514035 0.01442034 0.0091189882 0.0043807272 0.00081594964 -0.0014322645 -0.0025373914][-0.00043874932 0.002349728 0.005941323 0.0097410809 0.013044432 0.015111379 0.015499659 0.014131635 0.011399972 0.0080103232 0.0045080706 0.0014246095 -0.0008796847 -0.0023035952 -0.002975292][-0.0020909919 -0.00076770852 0.00099770981 0.002922816 0.00463993 0.0057368977 0.0059517734 0.0052216081 0.0037733961 0.0019986776 0.00021736603 -0.0012909016 -0.0023727668 -0.003003197 -0.0032754389][-0.0030436558 -0.0026154306 -0.0020056302 -0.0013082819 -0.00066865468 -0.00024728267 -0.00016422849 -0.00044531259 -0.00099532679 -0.0016535645 -0.0022839385 -0.0027872482 -0.0031228107 -0.0033008612 -0.0033693567][-0.0033596281 -0.0032996228 -0.0031984104 -0.0030692045 -0.0029412173 -0.0028506117 -0.0028287834 -0.0028821533 -0.0029932023 -0.003123374 -0.0032389842 -0.0033211859 -0.0033668394 -0.003385229 -0.0033896894][-0.0033889811 -0.003388294 -0.0033864791 -0.0033835417 -0.0033801305 -0.0033769312 -0.0033752017 -0.0033748916 -0.0033768064 -0.0033800551 -0.0033835715 -0.0033863606 -0.0033879452 -0.0033886463 -0.003388823]]...]
INFO - root - 2017-12-09 20:09:20.889783: step 55910, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 69h:17m:58s remains)
INFO - root - 2017-12-09 20:09:29.484702: step 55920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:24m:34s remains)
INFO - root - 2017-12-09 20:09:38.263203: step 55930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:50m:11s remains)
INFO - root - 2017-12-09 20:09:46.576591: step 55940, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 63h:53m:01s remains)
INFO - root - 2017-12-09 20:09:55.151841: step 55950, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 68h:46m:38s remains)
INFO - root - 2017-12-09 20:10:03.769887: step 55960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:24m:47s remains)
INFO - root - 2017-12-09 20:10:12.135190: step 55970, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 63h:42m:50s remains)
INFO - root - 2017-12-09 20:10:20.539353: step 55980, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:50m:01s remains)
INFO - root - 2017-12-09 20:10:29.131009: step 55990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:21m:55s remains)
INFO - root - 2017-12-09 20:10:37.925487: step 56000, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:26m:55s remains)
2017-12-09 20:10:38.790255: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20125781 0.22001784 0.23570862 0.24810544 0.26237956 0.28206334 0.30817181 0.33875474 0.37018383 0.40355679 0.43273854 0.46038103 0.48310539 0.500688 0.51437813][0.19662668 0.22242963 0.24576814 0.26683772 0.28789398 0.31076261 0.33813086 0.36833441 0.39854774 0.42698872 0.45142418 0.47194824 0.48777747 0.49978757 0.51001436][0.19097097 0.22604883 0.25833574 0.28834611 0.31686518 0.34345949 0.371417 0.39653927 0.42142889 0.44435993 0.4625982 0.47553724 0.48618636 0.49330884 0.5000872][0.19667266 0.23980829 0.28065595 0.31958458 0.35379389 0.38423672 0.41124398 0.43227345 0.45099869 0.46502772 0.4761166 0.48224053 0.48783082 0.48989111 0.49200711][0.20917262 0.26088658 0.31160381 0.35869196 0.39807776 0.42972639 0.45375192 0.47091609 0.4820177 0.48716387 0.4893342 0.4885639 0.48948732 0.48722273 0.48646849][0.22433449 0.28419182 0.34281725 0.39618269 0.43860656 0.46979776 0.49066406 0.502811 0.50789738 0.50562841 0.50055581 0.49379694 0.49013823 0.48425958 0.48050553][0.23644756 0.30254009 0.3666929 0.4227764 0.46548131 0.49507323 0.51199651 0.51948214 0.51946151 0.5124737 0.50347 0.4928045 0.48693168 0.47942126 0.47406057][0.24158886 0.31098744 0.37804711 0.43492898 0.47594136 0.50288081 0.51604378 0.51937765 0.51407135 0.50393373 0.492735 0.48050153 0.47462365 0.46702296 0.46265453][0.2392671 0.30798295 0.37407285 0.42926985 0.46831268 0.49268124 0.50292945 0.5030753 0.49450096 0.48196802 0.46820477 0.45574987 0.450169 0.44379261 0.44138315][0.226437 0.29244271 0.35538521 0.40724805 0.4436599 0.46563032 0.47402903 0.4719547 0.46128514 0.44741011 0.43210608 0.41965908 0.41317853 0.40812394 0.4073346][0.20408043 0.26456335 0.32214397 0.36953855 0.40234041 0.42193028 0.42910457 0.42671257 0.41763252 0.40399706 0.39018366 0.37832463 0.3719359 0.36731902 0.36580464][0.17793721 0.23056307 0.28087321 0.32242203 0.35099003 0.36707711 0.37266973 0.37056652 0.36283264 0.35172486 0.34102345 0.33182204 0.32683942 0.32282934 0.32211027][0.14818759 0.19156604 0.23338021 0.26818937 0.29237279 0.30563349 0.30976102 0.30753383 0.30122808 0.29297364 0.28510329 0.27902532 0.27643117 0.27426821 0.27433953][0.11821219 0.15178166 0.18427829 0.2115711 0.23061456 0.2406618 0.24359155 0.24176228 0.2378011 0.23280184 0.22815372 0.22465685 0.22389482 0.22344781 0.22398145][0.092792191 0.1179103 0.14164855 0.16175552 0.17567007 0.182247 0.18371566 0.18235059 0.17999537 0.17741582 0.17538092 0.17419775 0.17434746 0.17434222 0.17534514]]...]
INFO - root - 2017-12-09 20:10:47.402479: step 56010, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 64h:39m:58s remains)
INFO - root - 2017-12-09 20:10:56.125127: step 56020, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 69h:07m:07s remains)
INFO - root - 2017-12-09 20:11:04.805874: step 56030, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 66h:04m:28s remains)
INFO - root - 2017-12-09 20:11:13.536962: step 56040, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.759 sec/batch; 58h:15m:31s remains)
INFO - root - 2017-12-09 20:11:22.241656: step 56050, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 64h:41m:29s remains)
INFO - root - 2017-12-09 20:11:31.108428: step 56060, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 70h:02m:27s remains)
INFO - root - 2017-12-09 20:11:39.714303: step 56070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:12m:34s remains)
INFO - root - 2017-12-09 20:11:48.557935: step 56080, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.934 sec/batch; 71h:41m:53s remains)
INFO - root - 2017-12-09 20:11:57.055147: step 56090, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.838 sec/batch; 64h:21m:23s remains)
INFO - root - 2017-12-09 20:12:05.894440: step 56100, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 69h:23m:37s remains)
2017-12-09 20:12:06.805771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033856337 -0.0033834411 -0.0033829031 -0.0033831885 -0.0033833089 -0.0033842085 -0.0033875823 -0.0033923746 -0.0033954261 -0.0033940442 -0.0033860621 -0.0033735626 -0.0033598335 -0.0033480334 -0.0033401214][-0.00338225 -0.0033803249 -0.0033800637 -0.0033806602 -0.0033810786 -0.00338172 -0.0033842442 -0.0033887324 -0.0033925383 -0.0033930112 -0.0033881082 -0.0033791161 -0.0033684182 -0.0033586198 -0.0033524372][-0.0033808702 -0.0033792318 -0.0033792506 -0.0033799785 -0.0033805408 -0.0033810101 -0.0033824528 -0.0033858791 -0.0033903255 -0.0033931248 -0.0033917937 -0.0033866207 -0.0033799119 -0.0033731491 -0.0033684818][-0.003379951 -0.0033786516 -0.0033790569 -0.0033801228 -0.0033809058 -0.0033814886 -0.0033819526 -0.0033836262 -0.0033875755 -0.0033916414 -0.003393258 -0.0033914912 -0.0033880142 -0.0033839517 -0.0033806313][-0.0033795079 -0.0033784488 -0.0033794751 -0.0033813096 -0.0033826854 -0.0033835731 -0.0033834868 -0.0033831077 -0.0033852395 -0.0033891997 -0.0033925318 -0.0033935548 -0.0033927923 -0.0033912573 -0.0033895017][-0.0033794704 -0.003378341 -0.0033795866 -0.0033822635 -0.0033847343 -0.0033861503 -0.0033855226 -0.0033837461 -0.0033836823 -0.0033858106 -0.003388623 -0.003390789 -0.0033917904 -0.003391868 -0.0033912722][-0.0033796818 -0.0033783822 -0.0033798432 -0.0033831242 -0.0033868263 -0.0033889739 -0.0033879357 -0.0033852276 -0.0033833093 -0.0033831904 -0.0033840556 -0.0033852616 -0.0033860474 -0.0033863124 -0.0033861662][-0.0033794483 -0.0033780548 -0.0033798255 -0.003383711 -0.0033885199 -0.0033916803 -0.0033906668 -0.0033869916 -0.0033834819 -0.0033815943 -0.0033807494 -0.0033805417 -0.0033803715 -0.0033796851 -0.0033792907][-0.0033796909 -0.0033781945 -0.0033800178 -0.0033841385 -0.0033894714 -0.0033936573 -0.0033934191 -0.0033895567 -0.0033847138 -0.0033814211 -0.0033797335 -0.0033786977 -0.0033778043 -0.003376645 -0.0033758308][-0.0033800921 -0.0033786411 -0.003380375 -0.0033841725 -0.0033896023 -0.0033944105 -0.0033950731 -0.003391477 -0.0033862169 -0.0033820735 -0.0033799128 -0.0033786683 -0.0033779352 -0.0033769356 -0.0033761449][-0.0033805459 -0.0033792001 -0.0033806879 -0.0033837729 -0.0033885657 -0.0033934033 -0.0033948561 -0.00339223 -0.0033875857 -0.0033830618 -0.0033801429 -0.0033785885 -0.0033780453 -0.0033774101 -0.0033770096][-0.0033809659 -0.0033797089 -0.003381 -0.0033832407 -0.0033870235 -0.0033911995 -0.0033928396 -0.003391233 -0.0033877697 -0.0033836248 -0.0033803773 -0.0033784064 -0.0033777314 -0.0033772152 -0.0033772187][-0.0033813729 -0.0033801268 -0.0033812269 -0.0033827224 -0.0033854009 -0.0033884179 -0.0033896843 -0.00338894 -0.0033866793 -0.0033835159 -0.0033804739 -0.003378568 -0.0033778346 -0.0033771405 -0.0033770688][-0.0033815689 -0.0033799922 -0.0033808206 -0.00338166 -0.0033833582 -0.0033852744 -0.0033863664 -0.003386012 -0.0033846435 -0.0033825983 -0.00338029 -0.00337839 -0.0033773738 -0.0033769784 -0.0033768381][-0.0033810965 -0.0033794215 -0.0033798548 -0.0033803091 -0.0033813224 -0.003382398 -0.0033829273 -0.0033825168 -0.0033817529 -0.0033805924 -0.0033791929 -0.003377781 -0.003377035 -0.0033767603 -0.0033765817]]...]
INFO - root - 2017-12-09 20:12:15.426913: step 56110, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 66h:24m:34s remains)
INFO - root - 2017-12-09 20:12:24.094656: step 56120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 65h:41m:12s remains)
INFO - root - 2017-12-09 20:12:32.459288: step 56130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:26m:25s remains)
INFO - root - 2017-12-09 20:12:41.016703: step 56140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:37m:01s remains)
INFO - root - 2017-12-09 20:12:49.575878: step 56150, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 63h:56m:30s remains)
INFO - root - 2017-12-09 20:12:58.298920: step 56160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:33m:44s remains)
INFO - root - 2017-12-09 20:13:06.874786: step 56170, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 65h:36m:57s remains)
INFO - root - 2017-12-09 20:13:15.656374: step 56180, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 67h:03m:53s remains)
INFO - root - 2017-12-09 20:13:24.197853: step 56190, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.807 sec/batch; 61h:56m:58s remains)
INFO - root - 2017-12-09 20:13:32.865126: step 56200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 66h:50m:51s remains)
2017-12-09 20:13:33.745101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033831773 -0.0033806148 -0.0033797948 -0.0033791803 -0.0033789128 -0.0033788178 -0.0033788106 -0.0033789829 -0.0033793575 -0.0033798583 -0.0033803771 -0.0033808651 -0.0033813724 -0.0033820167 -0.0033827475][-0.0033848712 -0.0033816425 -0.0033804297 -0.0033796153 -0.0033791827 -0.0033790139 -0.0033789857 -0.0033795128 -0.0033803694 -0.0033815387 -0.0033827669 -0.003383961 -0.0033852004 -0.0033865012 -0.0033876123][-0.0033907078 -0.003387203 -0.0033856828 -0.0033842467 -0.0033832015 -0.0033824013 -0.0033820418 -0.0033825536 -0.0033837366 -0.0033854947 -0.0033876754 -0.0033897497 -0.0033919232 -0.0033937858 -0.0033952291][-0.0033980627 -0.003394682 -0.0033930247 -0.0033910489 -0.0033890523 -0.0033871403 -0.0033859618 -0.0033861839 -0.003387522 -0.0033899178 -0.0033928712 -0.0033957798 -0.0033987793 -0.0034013072 -0.0034030406][-0.0034047547 -0.0034019649 -0.0034003367 -0.0033979926 -0.0033950366 -0.0033919141 -0.0033898379 -0.0033894163 -0.0033905851 -0.0033932792 -0.0033967625 -0.0034002136 -0.0034039866 -0.0034069475 -0.0034088965][-0.0034101019 -0.0034076946 -0.0034059896 -0.003403235 -0.0033994848 -0.0033953476 -0.0033920675 -0.0033904903 -0.0033909895 -0.0033937388 -0.0033974815 -0.0034013847 -0.0034056825 -0.0034092132 -0.00341146][-0.0034120947 -0.0034100474 -0.0034081438 -0.0034050932 -0.0034008822 -0.0033960456 -0.0033917439 -0.0033890498 -0.0033888964 -0.0033913397 -0.0033948834 -0.0033987227 -0.0034031351 -0.0034069438 -0.0034097093][-0.0034104423 -0.0034086562 -0.0034067228 -0.0034036036 -0.0033993113 -0.003394274 -0.0033896277 -0.0033864039 -0.0033856588 -0.003387246 -0.0033899439 -0.0033933481 -0.0033974231 -0.0034012375 -0.0034044192][-0.0034054581 -0.003403479 -0.0034015772 -0.0033989674 -0.0033954435 -0.0033912391 -0.0033871895 -0.0033843445 -0.0033831948 -0.0033836835 -0.0033853787 -0.0033879059 -0.0033913648 -0.0033950252 -0.0033984785][-0.0033986371 -0.0033963339 -0.0033946617 -0.0033927623 -0.0033904514 -0.0033876344 -0.0033848113 -0.0033827317 -0.0033815883 -0.0033815494 -0.0033824947 -0.00338399 -0.0033863727 -0.0033892938 -0.0033924761][-0.0033920002 -0.0033894121 -0.0033882069 -0.0033870582 -0.0033857343 -0.0033841692 -0.003382493 -0.0033811261 -0.0033802534 -0.00338013 -0.0033804819 -0.0033811918 -0.0033825487 -0.0033845175 -0.0033868009][-0.0033863357 -0.0033840572 -0.0033834309 -0.0033829296 -0.0033822991 -0.0033815887 -0.0033808115 -0.0033800453 -0.0033795531 -0.0033793936 -0.0033793952 -0.0033795831 -0.003380168 -0.0033811731 -0.0033824772][-0.0033822276 -0.0033803156 -0.003380151 -0.0033800835 -0.0033799217 -0.0033797168 -0.0033794388 -0.0033791247 -0.0033789165 -0.0033787938 -0.0033786874 -0.0033786763 -0.0033788804 -0.0033793326 -0.003379883][-0.0033793624 -0.0033775193 -0.0033777044 -0.0033779324 -0.0033780686 -0.0033781333 -0.0033781459 -0.0033780846 -0.0033780341 -0.0033779829 -0.0033778951 -0.0033777999 -0.0033778076 -0.0033779738 -0.0033780742][-0.0033777661 -0.0033757938 -0.0033761656 -0.0033765177 -0.003376758 -0.0033769275 -0.0033770348 -0.0033770506 -0.0033770776 -0.0033771005 -0.0033770753 -0.0033770355 -0.0033770257 -0.0033770516 -0.0033769491]]...]
INFO - root - 2017-12-09 20:13:42.302522: step 56210, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 66h:25m:20s remains)
INFO - root - 2017-12-09 20:13:50.865691: step 56220, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 65h:26m:24s remains)
INFO - root - 2017-12-09 20:13:59.454309: step 56230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:50m:15s remains)
INFO - root - 2017-12-09 20:14:08.136516: step 56240, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.916 sec/batch; 70h:18m:50s remains)
INFO - root - 2017-12-09 20:14:16.652857: step 56250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:12m:48s remains)
INFO - root - 2017-12-09 20:14:25.337969: step 56260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:56m:45s remains)
INFO - root - 2017-12-09 20:14:33.889290: step 56270, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 66h:32m:39s remains)
INFO - root - 2017-12-09 20:14:42.678617: step 56280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 65h:58m:41s remains)
INFO - root - 2017-12-09 20:14:51.303811: step 56290, loss = 0.89, batch loss = 0.68 (10.7 examples/sec; 0.749 sec/batch; 57h:29m:37s remains)
INFO - root - 2017-12-09 20:15:00.070092: step 56300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 64h:41m:52s remains)
2017-12-09 20:15:00.963127: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030950775 0.032833982 0.03398525 0.03459217 0.034959871 0.034973841 0.034414019 0.033034809 0.031064017 0.028428731 0.024320735 0.019341983 0.013798069 0.0088690929 0.0051867561][0.03495273 0.036993597 0.038371596 0.03935428 0.040213883 0.040683225 0.040570747 0.039611705 0.037659656 0.034593433 0.029639849 0.023559868 0.016817195 0.011024877 0.0067251269][0.03918121 0.041127976 0.042728566 0.04427499 0.045907073 0.047128614 0.047700804 0.047192387 0.045227345 0.041649777 0.0359261 0.028667694 0.020848451 0.014365762 0.0095398007][0.043328322 0.045283511 0.047069363 0.049217612 0.051676709 0.053724814 0.054959241 0.054775875 0.05267207 0.048521 0.042145267 0.034091581 0.025561005 0.018651087 0.013568688][0.047025308 0.048998065 0.051019896 0.053923849 0.057263695 0.060157456 0.061964333 0.061991565 0.059729956 0.054805454 0.04748166 0.0386348 0.029587081 0.02244634 0.017567998][0.050837938 0.052951962 0.05509406 0.058539845 0.062607728 0.066205576 0.068407305 0.068344757 0.065685555 0.060092762 0.051979177 0.042480581 0.033237826 0.025920222 0.021317888][0.054306585 0.05690515 0.059331357 0.063130416 0.067438848 0.071073122 0.073216639 0.072910063 0.069930434 0.063894562 0.055310879 0.045718417 0.036668532 0.029587889 0.025610225][0.057177614 0.060486756 0.063070253 0.066817194 0.070784375 0.073912472 0.0755655 0.074845195 0.071625441 0.065521285 0.057020128 0.047726385 0.039454985 0.033188343 0.030085972][0.059658244 0.063679926 0.066284381 0.06953907 0.072691068 0.074846432 0.075692095 0.07458809 0.071328446 0.065526791 0.057454012 0.048808202 0.041350547 0.035871662 0.033649966][0.061926216 0.066547357 0.069033533 0.071563885 0.073690869 0.074727312 0.074611194 0.072931409 0.069560684 0.0640814 0.056655608 0.048861492 0.042329945 0.037831672 0.036427841][0.063498564 0.0687242 0.071138881 0.072919384 0.073929846 0.073698513 0.072495937 0.070106126 0.066572182 0.061575361 0.055146862 0.048557438 0.043154586 0.03966533 0.038791973][0.063221976 0.068576157 0.070850447 0.072090559 0.072266504 0.0711251 0.069290951 0.066657387 0.063293405 0.058895778 0.053512551 0.048078787 0.043600935 0.040894467 0.040493391][0.061813965 0.066790238 0.068432979 0.068891324 0.068295 0.066665836 0.064654842 0.062144533 0.059207454 0.055657618 0.051599927 0.04743823 0.043889552 0.041650847 0.041332226][0.058997046 0.06335561 0.064301252 0.064007677 0.062754765 0.060732648 0.058671325 0.056616552 0.054505095 0.052141055 0.049593981 0.046909481 0.0445002 0.042592425 0.041956674][0.054438796 0.058056731 0.058326293 0.057457417 0.05581465 0.053785 0.051968321 0.050424878 0.049106404 0.047980916 0.046988476 0.045757361 0.044316031 0.042677313 0.041685633]]...]
INFO - root - 2017-12-09 20:15:09.530862: step 56310, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 68h:08m:02s remains)
INFO - root - 2017-12-09 20:15:18.241781: step 56320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:58m:14s remains)
INFO - root - 2017-12-09 20:15:26.901051: step 56330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 66h:25m:39s remains)
INFO - root - 2017-12-09 20:15:35.453248: step 56340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:46m:43s remains)
INFO - root - 2017-12-09 20:15:44.000731: step 56350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:50m:04s remains)
INFO - root - 2017-12-09 20:15:52.722861: step 56360, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 68h:39m:01s remains)
INFO - root - 2017-12-09 20:16:01.237235: step 56370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:43m:52s remains)
INFO - root - 2017-12-09 20:16:10.014411: step 56380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:56m:32s remains)
INFO - root - 2017-12-09 20:16:18.666378: step 56390, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 52h:47m:39s remains)
INFO - root - 2017-12-09 20:16:27.371976: step 56400, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 66h:17m:14s remains)
2017-12-09 20:16:28.279561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033811631 -0.0033787023 -0.0033795508 -0.0033809622 -0.0033825689 -0.0033842297 -0.0033852449 -0.0033852814 -0.0033845785 -0.0033832286 -0.0033814816 -0.0033802914 -0.0033799312 -0.0033801463 -0.003380704][-0.0033799633 -0.0033777168 -0.0033792446 -0.0033816232 -0.003384429 -0.003386959 -0.0033888698 -0.0033894868 -0.0033887133 -0.0033866093 -0.0033836863 -0.003381294 -0.0033798723 -0.0033794055 -0.0033795356][-0.0033802942 -0.0033785927 -0.0033806411 -0.0033836979 -0.0033873206 -0.0033908035 -0.0033938452 -0.0033951409 -0.0033945038 -0.0033919916 -0.0033880139 -0.0033843769 -0.0033818011 -0.0033806851 -0.0033803924][-0.003380368 -0.0033790884 -0.0033815377 -0.0033852078 -0.0033897383 -0.0033945413 -0.0033987178 -0.0034007034 -0.0034002971 -0.003397505 -0.0033927991 -0.0033880579 -0.0033840653 -0.0033819517 -0.003380907][-0.0033802043 -0.0033790143 -0.0033815773 -0.003385291 -0.0033908582 -0.0033971644 -0.0034023465 -0.003405208 -0.00340518 -0.0034024792 -0.0033971032 -0.0033914482 -0.0033864384 -0.0033832246 -0.003381501][-0.0033799226 -0.0033784036 -0.0033807219 -0.0033843515 -0.0033903695 -0.0033974987 -0.0034033996 -0.0034071312 -0.0034075698 -0.0034050744 -0.0033996259 -0.0033935653 -0.0033878924 -0.0033838714 -0.0033815226][-0.0033796937 -0.0033779384 -0.0033797394 -0.0033830735 -0.0033888135 -0.0033955635 -0.0034017041 -0.0034060732 -0.0034069458 -0.0034049058 -0.0034000489 -0.0033943509 -0.0033885648 -0.0033840879 -0.0033812777][-0.0033795536 -0.0033776846 -0.0033791163 -0.0033818975 -0.0033863343 -0.0033917695 -0.0033973486 -0.0034018252 -0.0034031472 -0.0034018303 -0.0033980147 -0.0033929476 -0.0033875818 -0.0033831629 -0.0033803475][-0.003379852 -0.0033778506 -0.0033792676 -0.0033816928 -0.0033853757 -0.0033896915 -0.0033942079 -0.00339786 -0.0033990836 -0.0033980459 -0.0033949013 -0.0033904565 -0.0033857767 -0.0033817729 -0.0033793536][-0.0033807927 -0.003378723 -0.0033802665 -0.0033827436 -0.0033859834 -0.0033896675 -0.00339331 -0.0033959765 -0.0033964927 -0.0033953858 -0.003392403 -0.0033883606 -0.0033840393 -0.0033805256 -0.0033786583][-0.0033827357 -0.0033809198 -0.0033829578 -0.0033854819 -0.0033887827 -0.0033919229 -0.0033946068 -0.0033961453 -0.0033959802 -0.0033943655 -0.0033912568 -0.003387199 -0.0033829086 -0.0033796509 -0.0033779694][-0.0033855313 -0.0033838069 -0.0033860165 -0.003388792 -0.0033922887 -0.0033954235 -0.003397634 -0.0033984911 -0.0033976126 -0.0033953842 -0.0033921425 -0.0033880894 -0.0033834493 -0.0033798593 -0.0033779535][-0.003388427 -0.0033862689 -0.0033881315 -0.0033909192 -0.0033947597 -0.0033982964 -0.0034009588 -0.0034016187 -0.0034002725 -0.0033977898 -0.0033945297 -0.0033902561 -0.0033848658 -0.0033806348 -0.0033783133][-0.003391475 -0.0033881471 -0.0033890912 -0.0033916319 -0.0033959758 -0.003400398 -0.003403917 -0.0034053521 -0.0034045628 -0.0034022487 -0.0033985521 -0.003393773 -0.0033877783 -0.0033826677 -0.0033794888][-0.0033946421 -0.0033903024 -0.0033895341 -0.0033911255 -0.0033954596 -0.0034006308 -0.0034051128 -0.0034078693 -0.0034086015 -0.0034071147 -0.0034037265 -0.0033988138 -0.0033920512 -0.0033856805 -0.0033811228]]...]
INFO - root - 2017-12-09 20:16:36.916515: step 56410, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:04m:38s remains)
INFO - root - 2017-12-09 20:16:45.569643: step 56420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:38m:23s remains)
INFO - root - 2017-12-09 20:16:54.219683: step 56430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:22m:46s remains)
INFO - root - 2017-12-09 20:17:02.876271: step 56440, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 66h:06m:45s remains)
INFO - root - 2017-12-09 20:17:11.617443: step 56450, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.851 sec/batch; 65h:13m:49s remains)
INFO - root - 2017-12-09 20:17:20.244049: step 56460, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:15m:48s remains)
INFO - root - 2017-12-09 20:17:28.677756: step 56470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:05m:38s remains)
INFO - root - 2017-12-09 20:17:37.410460: step 56480, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 67h:47m:00s remains)
INFO - root - 2017-12-09 20:17:45.963720: step 56490, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.721 sec/batch; 55h:16m:21s remains)
INFO - root - 2017-12-09 20:17:54.490883: step 56500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:13m:43s remains)
2017-12-09 20:17:55.381047: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027792012 0.023249201 0.020350028 0.019667752 0.021291567 0.024111807 0.026797714 0.028214751 0.028441278 0.027690029 0.026995106 0.025794355 0.026471009 0.02826933 0.032243442][0.017854663 0.013164762 0.010116722 0.0095749227 0.01086202 0.013556702 0.016600596 0.019079451 0.020552544 0.021070629 0.021480311 0.021048453 0.022418329 0.024389818 0.028828008][0.009137637 0.0049236268 0.0022053055 0.0017222937 0.0022258216 0.0039327191 0.0060241194 0.0082982415 0.010283919 0.011975982 0.013742995 0.014736257 0.016405493 0.018858332 0.02377676][0.0049339738 0.002219605 0.00059250859 0.00020009885 5.00069e-05 0.00093349139 0.0020275486 0.0037405493 0.005604675 0.0076325061 0.0096607292 0.011417133 0.013739884 0.016984792 0.022227924][0.0027805332 0.0018684163 0.0016062495 0.0014714154 0.001300052 0.0019588463 0.0026531836 0.0040788781 0.0056739254 0.0077106236 0.010095126 0.01283644 0.01611059 0.019805962 0.024982216][0.0015177883 0.0016403317 0.0023086297 0.0028855295 0.0036199742 0.0045978418 0.0057252571 0.0075033195 0.0093949195 0.011730636 0.014530698 0.01788619 0.021878369 0.026185943 0.031245803][0.00087231095 0.0014208092 0.002583693 0.0040405532 0.0060687726 0.0082378983 0.010904045 0.013642598 0.016565438 0.019728603 0.023259632 0.027151883 0.03143638 0.035680942 0.039681688][0.001191729 0.0018578668 0.0032500091 0.0055211661 0.0086967554 0.012226427 0.016626677 0.020848665 0.025240339 0.029138038 0.03334742 0.03751627 0.041351393 0.04467338 0.047318295][0.001817887 0.0024299056 0.0041266996 0.00703001 0.011032652 0.015573585 0.020931862 0.025897203 0.031060362 0.035650548 0.040384833 0.044478256 0.048205283 0.050887685 0.05271633][0.0023316962 0.0031053603 0.0049985135 0.0081856744 0.012361862 0.016994456 0.022081822 0.026832011 0.031725995 0.03617432 0.040818017 0.045009654 0.048767976 0.050812095 0.052195251][0.0014034372 0.0024032884 0.0044564474 0.0075156046 0.011223821 0.015217876 0.019198602 0.022938548 0.02693438 0.030875066 0.034790907 0.03842764 0.041741665 0.043788191 0.045005135][-0.00039802422 0.00067435694 0.0024312271 0.0048640491 0.0077147456 0.010678174 0.013587646 0.016410425 0.019203678 0.022211572 0.02511647 0.027648283 0.029939262 0.031520709 0.032479137][-0.0022654072 -0.001589028 -0.00043898798 0.0011492127 0.0030767771 0.005117286 0.0070936149 0.00896854 0.010783615 0.01255356 0.014369778 0.016086435 0.017763663 0.018892655 0.019354783][-0.0032180836 -0.0029784392 -0.0025347159 -0.0018572455 -0.0009717762 6.1949948e-05 0.0010998358 0.0020505928 0.0029470583 0.003872392 0.0049062241 0.0060466351 0.007116762 0.00799808 0.0083632423][-0.0033944123 -0.0033678561 -0.0033061337 -0.0031844769 -0.0029809368 -0.0027044804 -0.0024013803 -0.0020973962 -0.0017897753 -0.0014452238 -0.0010316723 -0.00051134941 7.9549849e-05 0.00065232883 0.001038858]]...]
INFO - root - 2017-12-09 20:18:03.739884: step 56510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:07m:19s remains)
INFO - root - 2017-12-09 20:18:12.272296: step 56520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:57m:42s remains)
INFO - root - 2017-12-09 20:18:20.865900: step 56530, loss = 0.88, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 65h:21m:31s remains)
INFO - root - 2017-12-09 20:18:29.456648: step 56540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:05m:16s remains)
INFO - root - 2017-12-09 20:18:37.762502: step 56550, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 64h:01m:12s remains)
INFO - root - 2017-12-09 20:18:46.408070: step 56560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:59m:06s remains)
INFO - root - 2017-12-09 20:18:54.927735: step 56570, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:54m:29s remains)
INFO - root - 2017-12-09 20:19:03.522785: step 56580, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:12m:56s remains)
INFO - root - 2017-12-09 20:19:11.867746: step 56590, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 63h:24m:40s remains)
INFO - root - 2017-12-09 20:19:20.233235: step 56600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:29m:00s remains)
2017-12-09 20:19:21.147965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033873983 -0.0033855767 -0.0033853883 -0.003385481 -0.0033854593 -0.0033855271 -0.0033857012 -0.0033857296 -0.0033856113 -0.0033855254 -0.0033854039 -0.0033851748 -0.0033848637 -0.00338472 -0.0033847694][-0.0033859953 -0.0033840155 -0.0033840411 -0.0033843832 -0.0033845769 -0.0033847676 -0.0033850151 -0.0033850407 -0.0033847739 -0.0033845913 -0.003384342 -0.0033839582 -0.0033835478 -0.0033833582 -0.003383308][-0.0033861508 -0.0033841287 -0.0033844721 -0.0033851746 -0.0033858011 -0.0033862835 -0.0033866847 -0.0033866754 -0.0033863445 -0.0033859671 -0.0033852844 -0.0033844344 -0.0033837052 -0.0033832018 -0.0033830197][-0.0033867741 -0.0033848537 -0.0033855683 -0.0033866549 -0.003387626 -0.0033885082 -0.0033891962 -0.003389148 -0.0033887255 -0.0033882407 -0.0033873515 -0.0033859878 -0.0033847503 -0.0033839457 -0.0033835662][-0.0033879702 -0.0033863962 -0.0033874181 -0.003389159 -0.0033906489 -0.0033918754 -0.0033929041 -0.0033930338 -0.0033925083 -0.0033916328 -0.0033902384 -0.0033883338 -0.0033864658 -0.0033850572 -0.0033841722][-0.0033897767 -0.0033885213 -0.0033896058 -0.003391755 -0.0033936352 -0.0033949157 -0.003395892 -0.0033961539 -0.0033956526 -0.0033945425 -0.0033927995 -0.0033906887 -0.0033886582 -0.00338691 -0.00338546][-0.0033922426 -0.0033911169 -0.0033921592 -0.0033944556 -0.0033966 -0.00339774 -0.0033987956 -0.0033990794 -0.0033985549 -0.0033970044 -0.0033950466 -0.0033927681 -0.0033905939 -0.0033885844 -0.0033867226][-0.0033948408 -0.0033938857 -0.0033949648 -0.0033971285 -0.0033991057 -0.0034000368 -0.0034014012 -0.0034022287 -0.0034019104 -0.003399974 -0.0033975823 -0.0033953 -0.00339272 -0.0033900186 -0.0033876377][-0.0033974217 -0.0033964438 -0.0033974405 -0.0033992981 -0.003400886 -0.0034015335 -0.0034030203 -0.0034042951 -0.003404353 -0.0034023542 -0.0033999695 -0.0033978042 -0.003394922 -0.0033918857 -0.0033890794][-0.0033999444 -0.0033985835 -0.003399099 -0.0034004103 -0.0034016208 -0.0034023481 -0.0034040506 -0.0034056914 -0.0034061808 -0.003404516 -0.0034025216 -0.0034007079 -0.0033976941 -0.0033944109 -0.0033915245][-0.003402544 -0.0034006382 -0.003400445 -0.0034009831 -0.0034016462 -0.0034023935 -0.0034040157 -0.0034056068 -0.003406574 -0.0034056443 -0.0034040872 -0.0034021633 -0.0033994478 -0.003396431 -0.0033937793][-0.003405205 -0.0034029204 -0.0034019472 -0.0034015591 -0.0034020469 -0.0034030378 -0.003404747 -0.003406252 -0.0034075819 -0.0034073535 -0.0034058795 -0.0034038781 -0.0034015107 -0.0033987577 -0.0033962235][-0.0034067896 -0.0034043319 -0.003403109 -0.00340214 -0.0034022313 -0.0034031821 -0.0034047866 -0.0034063966 -0.0034075005 -0.0034076916 -0.0034064809 -0.0034049044 -0.0034030066 -0.0034009116 -0.0033987383][-0.0034081193 -0.0034056415 -0.0034041333 -0.0034030108 -0.003402997 -0.0034035316 -0.0034046019 -0.0034058744 -0.003406679 -0.0034071759 -0.0034066287 -0.0034058027 -0.0034047384 -0.0034034764 -0.00340165][-0.0034087643 -0.0034062914 -0.0034047659 -0.0034036895 -0.0034032422 -0.0034032138 -0.0034036096 -0.0034044932 -0.0034054159 -0.0034061146 -0.0034060031 -0.0034060613 -0.0034062979 -0.0034061368 -0.0034047461]]...]
INFO - root - 2017-12-09 20:19:29.631023: step 56610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:06m:10s remains)
INFO - root - 2017-12-09 20:19:38.241674: step 56620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:03m:28s remains)
INFO - root - 2017-12-09 20:19:46.892413: step 56630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:18m:07s remains)
INFO - root - 2017-12-09 20:19:55.585967: step 56640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:49m:13s remains)
INFO - root - 2017-12-09 20:20:03.993475: step 56650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:46m:01s remains)
INFO - root - 2017-12-09 20:20:12.555528: step 56660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:32m:14s remains)
INFO - root - 2017-12-09 20:20:21.137221: step 56670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 65h:34m:05s remains)
INFO - root - 2017-12-09 20:20:29.750066: step 56680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 64h:42m:08s remains)
INFO - root - 2017-12-09 20:20:38.538257: step 56690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:41m:52s remains)
INFO - root - 2017-12-09 20:20:47.159472: step 56700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:37m:05s remains)
2017-12-09 20:20:48.037658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033522055 -0.0033485773 -0.0033481673 -0.003347907 -0.0033475433 -0.0033473375 -0.0033469885 -0.0033466902 -0.0033461857 -0.0033460385 -0.0033459875 -0.0033460304 -0.0033462297 -0.0033463023 -0.0033464][-0.0033507026 -0.0033468073 -0.0033465882 -0.0033465934 -0.0033462169 -0.0033460006 -0.0033456439 -0.0033451456 -0.0033443794 -0.0033440092 -0.003343846 -0.003343707 -0.003343693 -0.0033437021 -0.0033437645][-0.0033516954 -0.0033478937 -0.0033480362 -0.0033482788 -0.0033478704 -0.00334758 -0.0033471908 -0.0033464371 -0.0033454727 -0.0033450066 -0.0033447815 -0.00334452 -0.0033443163 -0.0033441454 -0.0033440487][-0.0033519624 -0.0033482059 -0.0033484432 -0.0033488264 -0.0033485696 -0.0033484672 -0.0033481654 -0.0033474835 -0.0033465794 -0.0033460197 -0.0033456557 -0.0033452453 -0.0033447922 -0.0033445125 -0.0033443784][-0.0033519194 -0.0033481072 -0.0033483733 -0.003348791 -0.0033486367 -0.0033485044 -0.0033481228 -0.0033475687 -0.0033469659 -0.0033465696 -0.0033462541 -0.0033458972 -0.0033454562 -0.0033451226 -0.0033449905][-0.003351165 -0.0033472343 -0.0033475603 -0.003347981 -0.0033477433 -0.0033474509 -0.0033470362 -0.0033466418 -0.0033464925 -0.0033466269 -0.003346818 -0.0033468653 -0.0033466609 -0.0033463659 -0.0033461605][-0.0033501072 -0.0033460869 -0.0033462583 -0.0033465615 -0.0033462634 -0.0033458669 -0.0033455614 -0.0033454227 -0.0033458015 -0.0033463992 -0.0033471093 -0.0033476276 -0.0033477773 -0.0033477338 -0.0033476488][-0.0033493387 -0.0033455149 -0.0033455959 -0.0033457698 -0.0033453831 -0.0033448811 -0.0033445554 -0.0033445454 -0.0033452685 -0.0033461866 -0.0033472206 -0.0033481081 -0.0033486523 -0.0033489631 -0.0033491193][-0.0033495503 -0.0033458357 -0.0033458851 -0.0033459209 -0.0033454341 -0.0033448588 -0.0033444834 -0.0033446301 -0.0033454536 -0.0033464851 -0.0033476462 -0.0033487098 -0.0033494304 -0.0033498581 -0.0033500514][-0.0033508157 -0.0033471684 -0.00334723 -0.0033471403 -0.0033466103 -0.0033460082 -0.0033456548 -0.00334588 -0.0033466071 -0.0033475088 -0.0033485617 -0.0033495161 -0.0033501321 -0.0033504588 -0.0033506053][-0.0033523277 -0.0033486884 -0.0033488688 -0.0033486721 -0.0033482104 -0.0033477121 -0.0033475116 -0.0033477359 -0.0033482322 -0.0033488565 -0.0033496206 -0.0033503014 -0.0033507026 -0.003350907 -0.0033509887][-0.0033535024 -0.0033498551 -0.0033500968 -0.0033499382 -0.0033496479 -0.0033493263 -0.0033492325 -0.003349402 -0.0033496763 -0.0033499841 -0.0033503997 -0.003350764 -0.0033509543 -0.0033510423 -0.0033510751][-0.0033543976 -0.0033506779 -0.003350979 -0.0033508809 -0.0033507191 -0.0033505268 -0.0033504656 -0.0033505431 -0.0033506507 -0.0033507803 -0.0033509666 -0.0033511173 -0.0033511629 -0.0033511482 -0.0033511051][-0.0033550165 -0.0033511689 -0.0033514621 -0.0033514376 -0.0033513641 -0.003351246 -0.0033511829 -0.0033511748 -0.0033511766 -0.0033511829 -0.0033512414 -0.0033512732 -0.0033512521 -0.0033512036 -0.0033511391][-0.0033552998 -0.0033513273 -0.0033515296 -0.0033515343 -0.0033515152 -0.0033514576 -0.0033514122 -0.0033513857 -0.0033513652 -0.003351354 -0.0033513696 -0.0033513587 -0.003351327 -0.0033512758 -0.0033512057]]...]
INFO - root - 2017-12-09 20:20:56.649390: step 56710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:51m:40s remains)
INFO - root - 2017-12-09 20:21:05.297782: step 56720, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:00m:48s remains)
INFO - root - 2017-12-09 20:21:13.816615: step 56730, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:25m:26s remains)
INFO - root - 2017-12-09 20:21:22.465350: step 56740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:27m:35s remains)
INFO - root - 2017-12-09 20:21:30.882708: step 56750, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 66h:15m:52s remains)
INFO - root - 2017-12-09 20:21:39.633319: step 56760, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:36m:24s remains)
INFO - root - 2017-12-09 20:21:48.166216: step 56770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:11m:03s remains)
INFO - root - 2017-12-09 20:21:56.875023: step 56780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:41m:05s remains)
INFO - root - 2017-12-09 20:22:05.653713: step 56790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 68h:26m:38s remains)
INFO - root - 2017-12-09 20:22:14.216925: step 56800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:28m:11s remains)
2017-12-09 20:22:15.043318: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0070731379 0.0073136184 0.0052793324 0.0029177808 0.00043030782 -0.0015315587 -0.0028072295 -0.0033138911 -0.0033931322 -0.0034006822 -0.0033994957 -0.003400875 -0.0034012331 -0.0034017994 -0.0034026185][0.008121 0.0083982954 0.0062471097 0.0037631553 0.0010965636 -0.0010732934 -0.0025693283 -0.0032464983 -0.0033837925 -0.0033990864 -0.00339869 -0.0033989213 -0.0033994112 -0.0034007668 -0.0034003786][0.0089010019 0.009085441 0.0069564367 0.0045280531 0.0018535627 -0.00046434463 -0.0022293394 -0.0031294131 -0.0033624051 -0.0033976452 -0.0033974026 -0.0033978955 -0.0033990894 -0.0033997018 -0.0034001297][0.0095394179 0.0096854782 0.0076180324 0.00526408 0.002645618 0.00025302731 -0.0017472621 -0.0029249236 -0.0033152369 -0.0033968247 -0.0033995502 -0.0033996219 -0.0033996233 -0.0034006767 -0.0034006189][0.010005192 0.010112861 0.0081999023 0.0059982 0.0034405489 0.00096903346 -0.0012361489 -0.0026700727 -0.0032447 -0.0033952834 -0.0034015244 -0.0034021598 -0.0034025956 -0.0034031831 -0.0034036038][0.010358756 0.010382984 0.0085949795 0.0065562185 0.0041051749 0.001616745 -0.00075776642 -0.0024161986 -0.0031622881 -0.0033933988 -0.0034038865 -0.0034049754 -0.0034058804 -0.0034060969 -0.0034062595][0.010350836 0.010404935 0.0087319994 0.0068347752 0.0044419859 0.0019758248 -0.00045015058 -0.002228308 -0.0031001295 -0.0033911909 -0.003405269 -0.0034070157 -0.0034083989 -0.0034091952 -0.0034097983][0.010140919 0.010250904 0.0086349891 0.0067983959 0.0044370173 0.0020082651 -0.00040598982 -0.0021779803 -0.0030760325 -0.0033894815 -0.0034048208 -0.0034065826 -0.003408446 -0.003409669 -0.0034106956][0.0095080864 0.0098748431 0.00825703 0.0064452123 0.004087666 0.0017022484 -0.00062837987 -0.0022828504 -0.0031126649 -0.0033920156 -0.0034044033 -0.0034054033 -0.0034066907 -0.003408262 -0.0034092814][0.0088380333 0.0094399238 0.0077230455 0.0058566229 0.0034766954 0.0011333677 -0.0010594542 -0.0025166254 -0.0031935587 -0.0033978054 -0.0034047165 -0.0034044457 -0.0034053577 -0.0034060581 -0.0034066516][0.0081138387 0.0090472512 0.0072134426 0.00515797 0.0026872705 0.0004041614 -0.0015852879 -0.0027918923 -0.0032773707 -0.003402441 -0.0034047612 -0.003403659 -0.0034041158 -0.0034044133 -0.0034046976][0.0076795025 0.0087741725 0.0068037994 0.0045734569 0.0019935241 -0.00029339758 -0.0021083388 -0.003059756 -0.0033502965 -0.0034057195 -0.0034044744 -0.0034024972 -0.0034021332 -0.0034021388 -0.0034024501][0.0075344173 0.0087872213 0.0066583259 0.004138371 0.0014413886 -0.00080555421 -0.0024528336 -0.0032231265 -0.0033839184 -0.0034055081 -0.0034036525 -0.0034017747 -0.0034002031 -0.0033990266 -0.0033991085][0.0075619221 0.00876929 0.0066316938 0.0040101241 0.0011755272 -0.0011000193 -0.0026529976 -0.0033156979 -0.0033979779 -0.0034038287 -0.0034020287 -0.0034002776 -0.0033989327 -0.003398052 -0.003397218][0.0077899583 0.0088129817 0.0066298312 0.00395911 0.0010838804 -0.0011885334 -0.0027172123 -0.0033545417 -0.0034010019 -0.0034020594 -0.0033999293 -0.0033985469 -0.003397539 -0.003396685 -0.0033964003]]...]
INFO - root - 2017-12-09 20:22:23.769056: step 56810, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 66h:36m:48s remains)
INFO - root - 2017-12-09 20:22:32.420996: step 56820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:01m:14s remains)
INFO - root - 2017-12-09 20:22:41.132157: step 56830, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 68h:31m:08s remains)
INFO - root - 2017-12-09 20:22:49.809301: step 56840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:40m:48s remains)
INFO - root - 2017-12-09 20:22:58.386776: step 56850, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:59m:18s remains)
INFO - root - 2017-12-09 20:23:07.017867: step 56860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 66h:35m:39s remains)
INFO - root - 2017-12-09 20:23:15.598754: step 56870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:03m:17s remains)
INFO - root - 2017-12-09 20:23:24.226333: step 56880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:26m:37s remains)
INFO - root - 2017-12-09 20:23:32.912768: step 56890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:34m:04s remains)
INFO - root - 2017-12-09 20:23:41.490342: step 56900, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 67h:23m:57s remains)
2017-12-09 20:23:42.320999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010559764 -0.00053761294 -0.00034619356 -0.00041627092 -0.00061518606 -0.00082864286 -0.0010257855 -0.0012936031 -0.0017001566 -0.0022022007 -0.0026946 -0.0030779587 -0.00329727 -0.0033789289 -0.0033918365][0.0013082421 0.0021412482 0.0024673345 0.0024559794 0.0022618885 0.0020274844 0.0017340535 0.0011965821 0.00034019281 -0.00074255816 -0.0018028513 -0.0026440739 -0.0031434873 -0.0033448304 -0.0033863992][0.0047970843 0.0059368955 0.0064463411 0.0066850027 0.00677888 0.0068033338 0.00657262 0.0057071159 0.0041334359 0.0020261493 -7.1486458e-05 -0.0017559218 -0.0027932951 -0.0032514283 -0.0033739852][0.008931363 0.010225808 0.010906291 0.01164899 0.012539791 0.013471525 0.013858736 0.012889121 0.010429744 0.006795262 0.0030256321 -9.4226329e-05 -0.0021018519 -0.0030501445 -0.0033439132][0.01256163 0.013745286 0.014549948 0.016016126 0.018195784 0.020664755 0.022281865 0.021662258 0.018456312 0.013113359 0.007298856 0.0023325135 -0.0010070694 -0.0026991605 -0.0032878607][0.014174649 0.01499466 0.015865987 0.018136615 0.02190803 0.026419578 0.029895714 0.030231735 0.026701856 0.019891553 0.012055262 0.0051287282 0.00029384554 -0.0022619534 -0.0032104224][0.012507811 0.012925917 0.01380055 0.016591592 0.021526681 0.027679343 0.032890584 0.03450397 0.031373382 0.024102813 0.01523084 0.0071163038 0.0012654543 -0.0019203293 -0.0031435869][0.0077002784 0.0078528225 0.0086586373 0.011410732 0.016495977 0.023111204 0.029118842 0.031746641 0.02961562 0.023231795 0.014945207 0.0070929769 0.0012884142 -0.0019145462 -0.003142711][0.0019926534 0.0020291412 0.0026389018 0.0047633946 0.0089011239 0.014575174 0.020080725 0.023009976 0.022045067 0.01753911 0.011248292 0.0050351704 0.00033255108 -0.0022599618 -0.0032119497][-0.0018163715 -0.001809038 -0.001478682 -0.00027579116 0.0023101869 0.0061603095 0.010185801 0.012644315 0.012503553 0.0099875666 0.0061306255 0.0021046482 -0.0010313999 -0.0027333491 -0.0032972347][-0.0032631159 -0.0032471272 -0.003109907 -0.002608289 -0.001360635 0.00070550595 0.0030413128 0.0045852447 0.0047084754 0.0035718821 0.0016426642 -0.00049351226 -0.0022068452 -0.0031041328 -0.0033538649][-0.0033865457 -0.0033841587 -0.0033692927 -0.0032444107 -0.0028210543 -0.0020098183 -0.0010214567 -0.00034882105 -0.00025762338 -0.00065877428 -0.0013954374 -0.0022533352 -0.0029525855 -0.0032954174 -0.0033720122][-0.0033903518 -0.0033882651 -0.0033887113 -0.0033579727 -0.0032344328 -0.0029990547 -0.0027113811 -0.0025236192 -0.0025028859 -0.0026169689 -0.0028269184 -0.0030685624 -0.003265338 -0.0033559084 -0.0033748888][-0.0033916128 -0.0033907369 -0.0033917718 -0.0033917271 -0.003379913 -0.0033560635 -0.0033254279 -0.0033048347 -0.0032998987 -0.0033055253 -0.0033232919 -0.003345374 -0.003364804 -0.003374262 -0.003378469][-0.0033927846 -0.0033927318 -0.003394342 -0.0033958876 -0.003396468 -0.0033956491 -0.0033926438 -0.0033882302 -0.0033839103 -0.003379279 -0.0033765673 -0.003376141 -0.0033781598 -0.0033808046 -0.0033834076]]...]
INFO - root - 2017-12-09 20:23:50.912831: step 56910, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 66h:09m:08s remains)
INFO - root - 2017-12-09 20:23:59.623957: step 56920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:29m:03s remains)
INFO - root - 2017-12-09 20:24:08.230629: step 56930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:14m:37s remains)
INFO - root - 2017-12-09 20:24:16.625131: step 56940, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 64h:25m:18s remains)
INFO - root - 2017-12-09 20:24:25.168343: step 56950, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 56h:56m:41s remains)
INFO - root - 2017-12-09 20:24:33.839924: step 56960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:06m:08s remains)
INFO - root - 2017-12-09 20:24:42.387095: step 56970, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:17m:02s remains)
INFO - root - 2017-12-09 20:24:50.963664: step 56980, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:30m:14s remains)
INFO - root - 2017-12-09 20:24:59.410622: step 56990, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 64h:37m:55s remains)
INFO - root - 2017-12-09 20:25:07.933029: step 57000, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 67h:41m:47s remains)
2017-12-09 20:25:08.855623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034048245 -0.0034033963 -0.0034032548 -0.0034033158 -0.0034032904 -0.0034037374 -0.0034042636 -0.0034042085 -0.0034032436 -0.0034021344 -0.0034013432 -0.0034006417 -0.0034004196 -0.0034008329 -0.0034017006][-0.0034043826 -0.0034028043 -0.0034020327 -0.0034017274 -0.0034013595 -0.003401198 -0.0034008347 -0.0034002771 -0.0033995153 -0.0033992378 -0.0033994527 -0.0033994548 -0.0033993428 -0.0033998364 -0.0034006268][-0.0034060469 -0.0034040955 -0.0034028634 -0.0034015728 -0.0033995276 -0.0033971146 -0.0033944317 -0.0033931152 -0.0033937856 -0.0033959597 -0.0033982627 -0.0033995204 -0.0033998191 -0.0034004995 -0.0034013141][-0.0034081228 -0.0034063614 -0.0034042206 -0.0034006061 -0.0033942575 -0.0033844481 -0.0033749288 -0.0033718525 -0.0033770604 -0.0033868784 -0.0033949004 -0.0033990706 -0.0034003565 -0.003401109 -0.0034020133][-0.0034096125 -0.0034079221 -0.0034049316 -0.0033962361 -0.0033797722 -0.0033549231 -0.0033322035 -0.0033261958 -0.0033410012 -0.003365322 -0.003385161 -0.003395909 -0.0034001409 -0.0034015656 -0.0034026355][-0.0034090837 -0.0034078236 -0.0034029961 -0.00338672 -0.0033539154 -0.0033079698 -0.0032678314 -0.0032590935 -0.0032876027 -0.0033314687 -0.0033686939 -0.0033898279 -0.0033987176 -0.0034011845 -0.0034023435][-0.0034053915 -0.0034053992 -0.0033987227 -0.0033736636 -0.0033233773 -0.003257016 -0.0032024584 -0.0031938662 -0.0032356989 -0.0032975473 -0.0033508982 -0.0033822719 -0.0033962012 -0.0034003579 -0.0034020396][-0.0033994946 -0.0034012939 -0.0033934454 -0.0033629963 -0.0033023376 -0.003224775 -0.0031659966 -0.0031598418 -0.0032080931 -0.0032787793 -0.0033395102 -0.0033764036 -0.0033936861 -0.0033991472 -0.0034014194][-0.0033945132 -0.0033980254 -0.0033907727 -0.00336065 -0.0033001308 -0.0032237461 -0.0031695291 -0.00316555 -0.003211329 -0.0032795884 -0.0033389567 -0.0033757286 -0.0033931877 -0.0033992354 -0.0034016438][-0.0033940806 -0.0033982003 -0.0033927786 -0.0033680166 -0.0033174925 -0.0032526739 -0.003206268 -0.0032012623 -0.0032379024 -0.003295257 -0.0033461666 -0.0033789575 -0.0033951467 -0.0034009626 -0.0034031549][-0.0033985067 -0.003402404 -0.0033990624 -0.003382154 -0.0033453156 -0.0032956868 -0.0032573261 -0.0032478897 -0.0032709884 -0.0033139195 -0.0033546579 -0.00338279 -0.0033976189 -0.003403147 -0.0034045291][-0.0034044364 -0.0034069531 -0.0034057158 -0.0033958789 -0.0033720178 -0.0033366436 -0.0033048009 -0.0032907317 -0.003299586 -0.0033277089 -0.0033597974 -0.0033844444 -0.0033986426 -0.0034045384 -0.0034059044][-0.0034088958 -0.0034100169 -0.0034101703 -0.0034057964 -0.0033919429 -0.0033676194 -0.0033403044 -0.0033203892 -0.0033164606 -0.0033308426 -0.0033559783 -0.0033799976 -0.0033956654 -0.0034032387 -0.0034049999][-0.0034103028 -0.0034105105 -0.0034113876 -0.0034101678 -0.0034029107 -0.0033861094 -0.0033619548 -0.0033365819 -0.0033216232 -0.0033257024 -0.0033456965 -0.0033705991 -0.0033892454 -0.0033994326 -0.0034029339][-0.0034081424 -0.003407829 -0.0034092132 -0.0034098034 -0.0034064082 -0.0033947157 -0.0033723782 -0.0033445479 -0.0033234095 -0.0033216691 -0.0033391216 -0.0033644983 -0.0033851035 -0.0033970922 -0.003402398]]...]
INFO - root - 2017-12-09 20:25:17.495870: step 57010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 68h:13m:52s remains)
INFO - root - 2017-12-09 20:25:26.176072: step 57020, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 66h:50m:51s remains)
INFO - root - 2017-12-09 20:25:34.779765: step 57030, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:48m:46s remains)
INFO - root - 2017-12-09 20:25:43.432405: step 57040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:48m:42s remains)
INFO - root - 2017-12-09 20:25:52.054921: step 57050, loss = 0.90, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 59h:27m:28s remains)
INFO - root - 2017-12-09 20:26:00.384764: step 57060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:35m:49s remains)
INFO - root - 2017-12-09 20:26:08.750489: step 57070, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 62h:27m:07s remains)
INFO - root - 2017-12-09 20:26:17.323887: step 57080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:59m:28s remains)
INFO - root - 2017-12-09 20:26:25.953732: step 57090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:33m:00s remains)
INFO - root - 2017-12-09 20:26:34.516269: step 57100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 67h:16m:32s remains)
2017-12-09 20:26:35.413655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003403676 -0.0034027665 -0.0034026669 -0.0034024336 -0.0034022427 -0.0034019086 -0.0034015244 -0.0034012212 -0.0034011477 -0.0034009379 -0.0034007202 -0.0034005197 -0.003400302 -0.0034000808 -0.0033998808][-0.0034031325 -0.0034022504 -0.0034022559 -0.0034021225 -0.0034020185 -0.0034017928 -0.0034015253 -0.0034011716 -0.0034009039 -0.0034005677 -0.0034000657 -0.003399573 -0.0033992962 -0.0033991095 -0.0033989672][-0.003403337 -0.0034025873 -0.0034027155 -0.0034027419 -0.0034028501 -0.0034028154 -0.0034025614 -0.0034021225 -0.0034015498 -0.0034008627 -0.0034000559 -0.0033993693 -0.0033990007 -0.0033989293 -0.0033989109][-0.0034034748 -0.0034028534 -0.003403231 -0.003403628 -0.0034039784 -0.0034040813 -0.0034037633 -0.0034031854 -0.0034022036 -0.0034010466 -0.0033998897 -0.0033990922 -0.0033986871 -0.0033986806 -0.0033987856][-0.0034035773 -0.0034031109 -0.0034037225 -0.0034044962 -0.0034051568 -0.0034053004 -0.0034048841 -0.0034041211 -0.0034027742 -0.0034013551 -0.0033999879 -0.0033991011 -0.0033986631 -0.0033986443 -0.0033988017][-0.0034035677 -0.0034032243 -0.0034040271 -0.0034050667 -0.0034058397 -0.0034059482 -0.0034055039 -0.0034044974 -0.0034029689 -0.0034015 -0.0034002033 -0.0033993768 -0.0033989246 -0.0033988745 -0.0033990568][-0.0034033784 -0.0034031905 -0.0034040646 -0.0034051174 -0.0034057449 -0.0034055714 -0.0034050015 -0.0034040217 -0.0034026061 -0.0034014198 -0.0034003737 -0.0033996403 -0.0033992524 -0.0033992708 -0.0033994655][-0.0034031984 -0.0034029977 -0.0034038883 -0.0034048224 -0.003405201 -0.0034048068 -0.0034044106 -0.0034037617 -0.0034026487 -0.0034015847 -0.0034005821 -0.0034000198 -0.0033996953 -0.0033997216 -0.0033998766][-0.0034029519 -0.0034027176 -0.0034034955 -0.0034041815 -0.0034043023 -0.003404001 -0.0034040816 -0.0034037391 -0.0034028592 -0.0034018869 -0.003400946 -0.0034004035 -0.0034000815 -0.0034000471 -0.0034001451][-0.003402723 -0.0034023141 -0.003402889 -0.0034033705 -0.0034033502 -0.0034032008 -0.0034035153 -0.0034034206 -0.0034027062 -0.0034017905 -0.0034010194 -0.0034005735 -0.0034002687 -0.0034002233 -0.0034003393][-0.0034025412 -0.0034018795 -0.0034022897 -0.0034026669 -0.0034025449 -0.0034023596 -0.0034026557 -0.0034026615 -0.0034020767 -0.0034012988 -0.003400678 -0.0034003544 -0.0034001949 -0.0034002648 -0.0034004319][-0.0034024804 -0.0034016033 -0.0034018531 -0.0034020806 -0.0034019439 -0.0034016964 -0.0034019009 -0.003401869 -0.0034013586 -0.0034007768 -0.0034003833 -0.0034002848 -0.0034002543 -0.003400364 -0.0034004885][-0.0034025954 -0.0034015141 -0.0034016522 -0.003401726 -0.0034016168 -0.0034013861 -0.0034014788 -0.0034013616 -0.0034009661 -0.0034006115 -0.0034004671 -0.0034004545 -0.0034004473 -0.0034004925 -0.0034004769][-0.0034027477 -0.0034014923 -0.0034015679 -0.0034015402 -0.0034014469 -0.0034012545 -0.0034012354 -0.0034011381 -0.0034009079 -0.0034007626 -0.0034007465 -0.0034006757 -0.003400526 -0.0034004252 -0.0034002769][-0.0034029207 -0.003401533 -0.0034015686 -0.003401486 -0.0034013458 -0.0034011586 -0.0034010974 -0.0034010506 -0.0034009423 -0.0034008666 -0.003400794 -0.0034006219 -0.0034003966 -0.0034002443 -0.0034000485]]...]
INFO - root - 2017-12-09 20:26:43.968711: step 57110, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 63h:50m:24s remains)
INFO - root - 2017-12-09 20:26:52.404430: step 57120, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 63h:45m:00s remains)
INFO - root - 2017-12-09 20:27:00.971792: step 57130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:32m:54s remains)
INFO - root - 2017-12-09 20:27:09.729770: step 57140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:29m:28s remains)
INFO - root - 2017-12-09 20:27:18.496506: step 57150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 67h:07m:15s remains)
INFO - root - 2017-12-09 20:27:27.095060: step 57160, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 67h:37m:38s remains)
INFO - root - 2017-12-09 20:27:35.594536: step 57170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 67h:56m:38s remains)
INFO - root - 2017-12-09 20:27:44.260914: step 57180, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:25m:03s remains)
INFO - root - 2017-12-09 20:27:52.900715: step 57190, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:53m:55s remains)
INFO - root - 2017-12-09 20:28:01.367489: step 57200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:53m:24s remains)
2017-12-09 20:28:02.362631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033723619 -0.0033702904 -0.0033703847 -0.0033705207 -0.0033705323 -0.0033706755 -0.0033707181 -0.0033706687 -0.0033705889 -0.0033704962 -0.0033702338 -0.0033697365 -0.0033692324 -0.0033689279 -0.0033687414][-0.0033710701 -0.0033691796 -0.0033698 -0.0033705202 -0.0033710962 -0.0033716294 -0.0033720634 -0.0033721342 -0.00337202 -0.0033718387 -0.0033712417 -0.0033703935 -0.0033694343 -0.0033685144 -0.0033678061][-0.0033712974 -0.0033698133 -0.0033714008 -0.0033732103 -0.0033746276 -0.0033760297 -0.003377344 -0.0033779372 -0.0033780653 -0.003377784 -0.003376655 -0.0033751004 -0.0033731381 -0.0033710275 -0.003369252][-0.0033708797 -0.0033697239 -0.0033723109 -0.0033756215 -0.0033784381 -0.0033813552 -0.0033840647 -0.0033854418 -0.0033857306 -0.0033851839 -0.0033836782 -0.0033811806 -0.0033779205 -0.0033743207 -0.0033713512][-0.0033698049 -0.0033689644 -0.0033724844 -0.0033772679 -0.0033815992 -0.0033861538 -0.0033901511 -0.003392335 -0.0033930654 -0.003392668 -0.0033908424 -0.0033876563 -0.0033831403 -0.0033780062 -0.0033737104][-0.003368139 -0.0033674985 -0.0033718287 -0.0033779177 -0.0033836232 -0.0033896554 -0.0033947774 -0.0033980561 -0.0033995986 -0.0033996704 -0.0033974745 -0.0033935055 -0.003387823 -0.0033813303 -0.0033757389][-0.0033666422 -0.0033660138 -0.0033707591 -0.0033774581 -0.0033840274 -0.0033908479 -0.0033965961 -0.0034009737 -0.0034035519 -0.0034040911 -0.0034018294 -0.0033974398 -0.0033910519 -0.0033837913 -0.0033773915][-0.0033657185 -0.0033650238 -0.0033695076 -0.0033759777 -0.0033825904 -0.0033894409 -0.003395227 -0.003399885 -0.0034030084 -0.0034038415 -0.0034017893 -0.00339736 -0.0033910887 -0.003384213 -0.0033779172][-0.0033654256 -0.0033646012 -0.0033683721 -0.0033736688 -0.003379456 -0.0033854954 -0.0033906079 -0.0033948172 -0.0033979556 -0.0033990021 -0.003397383 -0.0033934484 -0.003388193 -0.0033824795 -0.0033770408][-0.0033658452 -0.0033646435 -0.0033673996 -0.0033711339 -0.0033755056 -0.0033800725 -0.0033841499 -0.0033875997 -0.0033903678 -0.0033914372 -0.0033902519 -0.0033873806 -0.0033832451 -0.0033788846 -0.0033746765][-0.0033667895 -0.0033649255 -0.0033667642 -0.0033691004 -0.003372014 -0.0033750527 -0.003378056 -0.0033807734 -0.00338273 -0.0033833869 -0.0033825515 -0.003380524 -0.0033775987 -0.003374601 -0.0033718869][-0.0033675558 -0.0033651083 -0.0033660762 -0.0033673337 -0.0033690692 -0.0033709612 -0.0033729342 -0.0033745877 -0.0033756169 -0.0033759098 -0.0033754155 -0.0033740567 -0.0033724038 -0.003370858 -0.0033695402][-0.0033678615 -0.0033648559 -0.0033651749 -0.0033655635 -0.0033664333 -0.0033676035 -0.0033688608 -0.0033698303 -0.0033702685 -0.0033704089 -0.0033700978 -0.0033692173 -0.0033684208 -0.0033679721 -0.0033676524][-0.0033677577 -0.0033643059 -0.0033641886 -0.0033639958 -0.0033642321 -0.003365024 -0.0033658824 -0.003366499 -0.0033668678 -0.00336716 -0.0033671258 -0.0033667374 -0.003366533 -0.0033665921 -0.0033667558][-0.0033676263 -0.0033639548 -0.0033636533 -0.0033632244 -0.0033631346 -0.0033637753 -0.0033646163 -0.0033652713 -0.0033658182 -0.0033663579 -0.0033664797 -0.0033663034 -0.0033662342 -0.0033663644 -0.0033665476]]...]
INFO - root - 2017-12-09 20:28:11.089170: step 57210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:16m:16s remains)
INFO - root - 2017-12-09 20:28:19.615874: step 57220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:36m:36s remains)
INFO - root - 2017-12-09 20:28:28.013300: step 57230, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 63h:27m:18s remains)
INFO - root - 2017-12-09 20:28:36.645985: step 57240, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 65h:54m:58s remains)
INFO - root - 2017-12-09 20:28:45.267573: step 57250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:26m:29s remains)
INFO - root - 2017-12-09 20:28:53.740116: step 57260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:14m:15s remains)
INFO - root - 2017-12-09 20:29:02.201410: step 57270, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 64h:27m:19s remains)
INFO - root - 2017-12-09 20:29:10.554411: step 57280, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 63h:15m:38s remains)
INFO - root - 2017-12-09 20:29:19.013173: step 57290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:39m:22s remains)
INFO - root - 2017-12-09 20:29:27.509271: step 57300, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 66h:40m:22s remains)
2017-12-09 20:29:28.339970: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0021973231 0.0035587691 0.0044221235 0.0044416972 0.0034429461 0.0018458206 0.00025744154 -0.00089919334 -0.0016009369 -0.0019840067 -0.0023266263 -0.0026507105 -0.0029268225 -0.0031413415 -0.0032802031][0.0048340177 0.0069090147 0.0081347441 0.0082226936 0.006918245 0.0048131021 0.0025139062 0.000753368 -0.000439147 -0.0011101859 -0.0016321292 -0.0021233049 -0.0025856835 -0.0029380305 -0.0031594862][0.0079363221 0.010759216 0.012199899 0.012286121 0.010666132 0.008129539 0.0051909126 0.0028738312 0.0011995307 0.00014811009 -0.0007569748 -0.0015527916 -0.002208408 -0.0026766937 -0.002980731][0.011666047 0.015194802 0.01680921 0.016763223 0.014739599 0.011703219 0.0080328388 0.0050691953 0.0027925239 0.0013303098 8.2792481e-05 -0.0009891279 -0.0018531496 -0.0024304842 -0.0027849702][0.014842374 0.019010164 0.020880163 0.020853706 0.018592123 0.015137887 0.010825116 0.0071709 0.0042243041 0.0022808311 0.00062176841 -0.0006757204 -0.0016428415 -0.0022437545 -0.0026144318][0.017014073 0.02167803 0.023745771 0.023798015 0.021460541 0.017757619 0.012932248 0.0086307181 0.0050426153 0.0026235271 0.000675346 -0.00068856403 -0.001653773 -0.0021920716 -0.0025221913][0.017803336 0.022877974 0.025288714 0.025474368 0.023050526 0.019118043 0.013906229 0.0090793092 0.004970395 0.0021795372 0.00012592971 -0.0011141112 -0.0018980715 -0.0022860821 -0.0025426475][0.017199263 0.022613494 0.025356635 0.025753533 0.023381509 0.019262092 0.013768632 0.0085708573 0.0041490197 0.0011680031 -0.00078009046 -0.0017502743 -0.0022857958 -0.0025048307 -0.0026751636][0.015110331 0.020651728 0.023610443 0.024216818 0.022030523 0.01796166 0.012557101 0.0073332982 0.0028924306 -5.6885881e-05 -0.0017476432 -0.0024173968 -0.0026945269 -0.0027841814 -0.0028832834][0.011842529 0.01708716 0.020030851 0.020834951 0.019009151 0.015267063 0.010320863 0.0054617925 0.0014074284 -0.0012166726 -0.0025361283 -0.0029274467 -0.0030219881 -0.0030309674 -0.0030858973][0.0079118777 0.012471369 0.015105495 0.015948014 0.014508652 0.011354369 0.0072023803 0.0031046222 -0.00019053812 -0.0021885517 -0.0030394206 -0.0032255943 -0.0032340635 -0.0032176438 -0.0032421821][0.0042709764 0.0076945964 0.0098261312 0.010552716 0.0094440077 0.0069364393 0.00372489 0.0006792692 -0.0015990405 -0.0028588541 -0.0032990142 -0.0033578633 -0.0033398739 -0.0033251909 -0.0033357891][0.0012650581 0.0035240038 0.0049855337 0.0054542338 0.00463307 0.0028159569 0.00060469774 -0.0013503784 -0.0026371814 -0.0032246537 -0.0033786804 -0.0033922514 -0.003385518 -0.0033805277 -0.00338121][-0.00086121913 0.00037846714 0.0012025048 0.0013926493 0.00082064024 -0.00031554862 -0.0015763366 -0.0025726932 -0.0031398234 -0.0033530931 -0.0033947497 -0.0033975029 -0.0033965094 -0.0033964415 -0.0033957271][-0.002293041 -0.0017398894 -0.0013521945 -0.0012987086 -0.0016304484 -0.0022168634 -0.00278354 -0.0031710276 -0.0033435312 -0.003393823 -0.0033983651 -0.0033983716 -0.0033989209 -0.003398987 -0.0033978887]]...]
INFO - root - 2017-12-09 20:29:37.201395: step 57310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:52m:50s remains)
INFO - root - 2017-12-09 20:29:46.039738: step 57320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:37m:14s remains)
INFO - root - 2017-12-09 20:29:54.678943: step 57330, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:27m:12s remains)
INFO - root - 2017-12-09 20:30:03.314381: step 57340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:56m:01s remains)
INFO - root - 2017-12-09 20:30:12.007960: step 57350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:30m:02s remains)
INFO - root - 2017-12-09 20:30:20.618405: step 57360, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.930 sec/batch; 71h:06m:56s remains)
INFO - root - 2017-12-09 20:30:29.300638: step 57370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 65h:40m:37s remains)
INFO - root - 2017-12-09 20:30:37.977280: step 57380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:04m:47s remains)
INFO - root - 2017-12-09 20:30:46.785049: step 57390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:12m:41s remains)
INFO - root - 2017-12-09 20:30:55.357530: step 57400, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 66h:16m:47s remains)
2017-12-09 20:30:56.232676: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026207559 0.035946932 0.047999829 0.064521581 0.081965551 0.10013169 0.11659107 0.1325697 0.14404634 0.15028965 0.15169995 0.15002757 0.1474447 0.14397734 0.13655068][0.023850447 0.034541696 0.050222598 0.071256354 0.093503639 0.11620151 0.13526046 0.15291966 0.16477586 0.17191882 0.17328466 0.17238162 0.17062499 0.16742265 0.15973568][0.021281729 0.031641774 0.049372945 0.074334383 0.1013274 0.1274491 0.14868376 0.16723375 0.17860216 0.18570168 0.18665597 0.18548934 0.18339235 0.18076558 0.17396238][0.018709145 0.028709138 0.047565684 0.074621893 0.10492953 0.13455845 0.15775935 0.17668237 0.18702242 0.19327371 0.1936806 0.19290489 0.19109786 0.18802907 0.1812901][0.015686613 0.024921983 0.044277184 0.072800018 0.1056672 0.1368683 0.16165847 0.18093872 0.19063567 0.1965847 0.19643696 0.19684535 0.19606961 0.19415905 0.18881404][0.013946075 0.02182192 0.039369643 0.068349458 0.10207281 0.13486129 0.16117781 0.18159392 0.19177666 0.19862422 0.19854568 0.19980954 0.19949135 0.19944179 0.19604784][0.01501043 0.020480875 0.035366416 0.06171266 0.093880765 0.12671471 0.15526493 0.17736581 0.19001935 0.19816558 0.19893856 0.20202446 0.20217764 0.20366938 0.20220047][0.01633518 0.020833997 0.033158425 0.056442309 0.085650019 0.11612219 0.14455125 0.16894363 0.18603392 0.19731596 0.20113286 0.20616096 0.20628688 0.20848745 0.20747621][0.014797952 0.01925268 0.030069849 0.049685244 0.074705422 0.10214736 0.1296898 0.15516807 0.17600054 0.19210756 0.20089342 0.20921689 0.21042278 0.21290475 0.21188161][0.01343059 0.016873173 0.024697127 0.040619161 0.061669886 0.085201025 0.11132134 0.13713524 0.16144682 0.18236959 0.1971111 0.21014464 0.2139475 0.21692066 0.2159455][0.012157533 0.014848985 0.020828089 0.032025378 0.047448657 0.066451378 0.089864373 0.11470113 0.14134336 0.166209 0.1870165 0.20476256 0.21227215 0.21706145 0.21641052][0.010763391 0.012379501 0.016420385 0.023146585 0.032759015 0.046908177 0.066181287 0.088763013 0.11582263 0.14299797 0.16937172 0.19324857 0.20643835 0.21362515 0.21426737][0.0089450767 0.010200095 0.012452768 0.016446184 0.022066163 0.030848982 0.044613957 0.063141964 0.087813012 0.1156266 0.14618416 0.17504337 0.19450839 0.20539108 0.20787579][0.0075312015 0.0080875577 0.0094791614 0.011229352 0.013508774 0.018834334 0.028530462 0.041786421 0.061806321 0.087541521 0.11965816 0.15237008 0.1783397 0.19361369 0.20032735][0.0062843906 0.0064760251 0.00648358 0.0070062932 0.0076836441 0.010530351 0.016273698 0.025134981 0.040688131 0.062790953 0.094151825 0.12870692 0.15956505 0.18041615 0.19192715]]...]
INFO - root - 2017-12-09 20:31:04.872975: step 57410, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.918 sec/batch; 70h:09m:18s remains)
INFO - root - 2017-12-09 20:31:13.774898: step 57420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:53m:39s remains)
INFO - root - 2017-12-09 20:31:22.233436: step 57430, loss = 0.91, batch loss = 0.70 (9.6 examples/sec; 0.835 sec/batch; 63h:48m:43s remains)
INFO - root - 2017-12-09 20:31:30.763569: step 57440, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 67h:08m:26s remains)
INFO - root - 2017-12-09 20:31:39.375361: step 57450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:51m:20s remains)
INFO - root - 2017-12-09 20:31:47.821461: step 57460, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 63h:29m:41s remains)
INFO - root - 2017-12-09 20:31:56.198148: step 57470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:03m:22s remains)
INFO - root - 2017-12-09 20:32:04.863142: step 57480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:55m:26s remains)
INFO - root - 2017-12-09 20:32:13.545914: step 57490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:30m:54s remains)
INFO - root - 2017-12-09 20:32:22.183512: step 57500, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:39m:32s remains)
2017-12-09 20:32:23.100555: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00021076435 0.00061601796 0.00083578262 0.0007163852 0.00038729794 0.00020384369 0.00010610092 0.00019683409 0.00033407263 0.00038057799 0.00036023371 0.00054603745 0.00058994186 0.00049717329 5.5812532e-05][0.0011803072 0.0015999589 0.001937866 0.0020079624 0.0018680247 0.0016801714 0.0014082564 0.0013805989 0.0014759589 0.0016957966 0.0019003667 0.0021714845 0.0023163741 0.0022407223 0.0016477534][0.0020464303 0.0022588458 0.0026666343 0.0030352084 0.0033619839 0.0035016828 0.0033378617 0.0032295126 0.0030155049 0.00306526 0.0034792221 0.0039486792 0.0043233251 0.00424866 0.003509586][0.0033293373 0.0032938456 0.0035439588 0.0040763635 0.00497499 0.0058113337 0.0062368913 0.0063073263 0.0056966506 0.0052381828 0.0054056924 0.0056600058 0.0059352694 0.0056972494 0.0048567][0.0041826097 0.0040815668 0.0043906597 0.0053369813 0.0070728688 0.0090063792 0.010455465 0.011106304 0.01034156 0.0092636859 0.0086982679 0.0083390493 0.0080642784 0.0072756978 0.006115417][0.0047645615 0.0047565643 0.00511901 0.0063517895 0.0088020265 0.011968308 0.014853489 0.016655397 0.016516043 0.015304765 0.014073955 0.012898847 0.011816264 0.010253066 0.0086732032][0.005186568 0.005385993 0.0058759255 0.0074167522 0.010440517 0.014591649 0.018838193 0.02191573 0.022795016 0.022168094 0.020695649 0.019012924 0.017301045 0.015227539 0.013362782][0.0059286086 0.0060966462 0.0064920904 0.0080554513 0.011241926 0.015942639 0.021227827 0.02544174 0.027461283 0.027811576 0.026689757 0.025130982 0.02333896 0.021261523 0.019358931][0.0072104791 0.0071878023 0.0073183449 0.00871502 0.011769538 0.016571753 0.022171941 0.026899338 0.029637845 0.030734276 0.030276516 0.029429154 0.028279027 0.026852058 0.025445918][0.0086510656 0.008484805 0.0082591865 0.0091569535 0.011717729 0.016224282 0.021564443 0.026236748 0.029211016 0.030719342 0.030973172 0.031089054 0.031038245 0.030805854 0.030481951][0.010799698 0.010543572 0.010061936 0.010516517 0.012457576 0.016198389 0.020692889 0.024692021 0.027344573 0.02881729 0.029438077 0.030406965 0.031567134 0.032886039 0.034169652][0.014917821 0.014754236 0.014101967 0.01410141 0.015243638 0.017923994 0.021120053 0.023843218 0.025519842 0.026359389 0.026855623 0.028256206 0.030450406 0.033365268 0.0365119][0.022408022 0.022631316 0.0218079 0.021231517 0.02142876 0.022736792 0.024217352 0.025167536 0.025298115 0.025001185 0.024929876 0.026168302 0.028870961 0.032979008 0.037771832][0.033144545 0.033989083 0.032837693 0.031540077 0.030847279 0.030761292 0.030563882 0.029640472 0.028056296 0.026306234 0.025208926 0.025746848 0.028299039 0.032842182 0.038558479][0.044852272 0.046065789 0.044190258 0.041969229 0.04032626 0.03933607 0.03808457 0.035979338 0.033324216 0.030521629 0.028484283 0.028075086 0.029963927 0.034151431 0.039851569]]...]
INFO - root - 2017-12-09 20:32:31.673177: step 57510, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 68h:02m:02s remains)
INFO - root - 2017-12-09 20:32:40.525919: step 57520, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 68h:17m:08s remains)
INFO - root - 2017-12-09 20:32:49.126801: step 57530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:05m:27s remains)
INFO - root - 2017-12-09 20:32:57.911649: step 57540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:39m:57s remains)
INFO - root - 2017-12-09 20:33:06.874332: step 57550, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 62h:29m:16s remains)
INFO - root - 2017-12-09 20:33:15.393927: step 57560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:01m:13s remains)
INFO - root - 2017-12-09 20:33:23.896584: step 57570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:33m:05s remains)
INFO - root - 2017-12-09 20:33:32.735669: step 57580, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.917 sec/batch; 70h:00m:44s remains)
INFO - root - 2017-12-09 20:33:41.406868: step 57590, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 69h:20m:52s remains)
INFO - root - 2017-12-09 20:33:49.674093: step 57600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 68h:51m:58s remains)
2017-12-09 20:33:50.542276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0019819867 -0.0016935766 -0.001409584 -0.0012717887 -0.0011862633 -0.0011581471 -0.0011690876 -0.0012475841 -0.0013051154 -0.0014277312 -0.0015252231 -0.0016322817 -0.0017235298 -0.0018112842 -0.0019536668][-0.0018715421 -0.0015479476 -0.0013270935 -0.0012031137 -0.0010646726 -0.0010433448 -0.0010665772 -0.0011926605 -0.0013074868 -0.0014098976 -0.001475913 -0.0015668998 -0.0016549492 -0.0017950805 -0.0018868421][-0.0019595278 -0.0016763527 -0.0014951422 -0.0013964274 -0.0012962392 -0.0013091613 -0.0013247672 -0.0014656354 -0.0015814339 -0.0016623156 -0.0016998126 -0.0017535873 -0.0018084632 -0.0018804371 -0.0019235582][-0.0021100822 -0.0018106011 -0.0016532285 -0.0016150407 -0.0015618538 -0.0016000937 -0.0016473279 -0.0018269659 -0.0019459133 -0.0020228168 -0.0020031366 -0.0019851173 -0.0020073666 -0.0020490554 -0.0020416002][-0.0023091533 -0.0020095131 -0.0018428449 -0.0017760687 -0.0017973121 -0.0018649122 -0.0019460816 -0.0021457749 -0.0022558626 -0.0023069214 -0.00227973 -0.0022396296 -0.0022417866 -0.0022129198 -0.0021370451][-0.002450532 -0.0021970742 -0.0020547416 -0.0019911691 -0.0020099694 -0.0021007664 -0.0022565143 -0.0024260902 -0.0025527226 -0.0026248766 -0.0025311164 -0.0024483346 -0.0023715254 -0.0023447569 -0.0022341521][-0.0026083668 -0.0023497331 -0.002209133 -0.0021684938 -0.0022694282 -0.0023814428 -0.0024672276 -0.0026215434 -0.0027360085 -0.0028222338 -0.0027643836 -0.0026342748 -0.002541624 -0.0024218801 -0.0022508679][-0.0027686022 -0.0025784 -0.0024312637 -0.0023978164 -0.0024750242 -0.0025682333 -0.0027363177 -0.0028596681 -0.0029293632 -0.0029557457 -0.002889307 -0.002797571 -0.0026800369 -0.0025129982 -0.0023367875][-0.0029462711 -0.0028171395 -0.0027060302 -0.0026964715 -0.0027346327 -0.0028004041 -0.0028991797 -0.0029916726 -0.003117793 -0.0031104025 -0.0030119512 -0.0028920956 -0.0027708469 -0.0026350599 -0.0024470054][-0.0031025475 -0.003023443 -0.0029712226 -0.0029534716 -0.0029599762 -0.003002251 -0.0030870289 -0.003153601 -0.0032086438 -0.0031886206 -0.0031550457 -0.0030732728 -0.0029607154 -0.0028350351 -0.0026427959][-0.0032618823 -0.0031980523 -0.0031505143 -0.0031506745 -0.0031710619 -0.0032010253 -0.0032389644 -0.0032817856 -0.0032948218 -0.0032641245 -0.0032304586 -0.0031877654 -0.0031410935 -0.0030588075 -0.0028752813][-0.003343951 -0.0033142644 -0.0033090343 -0.0032875023 -0.0032902241 -0.003317073 -0.0033416969 -0.0033561187 -0.0033518253 -0.0033362955 -0.0033073309 -0.0032971762 -0.0032782184 -0.0032286369 -0.0031245337][-0.0033517967 -0.0033611062 -0.0033521487 -0.0033534011 -0.0033719295 -0.0033739875 -0.0033919923 -0.0033874186 -0.0033798455 -0.0033708692 -0.0033620617 -0.003359349 -0.0033399286 -0.0033265196 -0.0032586139][-0.0033912912 -0.0033860826 -0.0033876167 -0.0033877869 -0.0033904272 -0.0033885164 -0.0033881683 -0.0033816288 -0.0033730457 -0.0033712343 -0.0033733358 -0.0033771514 -0.0033643539 -0.0033620519 -0.0033220123][-0.0033837133 -0.0033757747 -0.0033748876 -0.0033756725 -0.0033794721 -0.0033785694 -0.0033827375 -0.0033745139 -0.0033804674 -0.0033657693 -0.0033599357 -0.0033696315 -0.003356687 -0.0033544493 -0.0033251054]]...]
INFO - root - 2017-12-09 20:33:59.018708: step 57610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 65h:02m:42s remains)
INFO - root - 2017-12-09 20:34:07.757592: step 57620, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 66h:44m:38s remains)
INFO - root - 2017-12-09 20:34:16.379019: step 57630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:02m:46s remains)
INFO - root - 2017-12-09 20:34:25.201197: step 57640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:57m:18s remains)
INFO - root - 2017-12-09 20:34:33.792059: step 57650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:49m:03s remains)
INFO - root - 2017-12-09 20:34:42.435248: step 57660, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:32m:36s remains)
INFO - root - 2017-12-09 20:34:50.960460: step 57670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:09m:21s remains)
INFO - root - 2017-12-09 20:34:59.652298: step 57680, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 67h:50m:55s remains)
INFO - root - 2017-12-09 20:35:08.426043: step 57690, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 67h:47m:51s remains)
INFO - root - 2017-12-09 20:35:16.997652: step 57700, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 63h:43m:32s remains)
2017-12-09 20:35:17.893299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014352532 0.015382843 0.016196802 0.016567614 0.016048774 0.015062924 0.013794065 0.012267482 0.010778162 0.0094239432 0.0082614422 0.0071946122 0.0062554618 0.0052909264 0.0044374447][0.010758446 0.012222476 0.013580384 0.014722632 0.014987733 0.014739724 0.013982393 0.012818912 0.011415244 0.010041135 0.0087138265 0.0072709648 0.005973679 0.004538022 0.0032401232][0.0079916827 0.0098643564 0.012032538 0.014307611 0.015971886 0.016797971 0.016630685 0.015634755 0.01422249 0.012581419 0.010693743 0.0085016983 0.0065438002 0.0045739524 0.0025899748][0.0068743117 0.0091000143 0.011930213 0.015197711 0.017703967 0.019515349 0.019891949 0.019545231 0.018310271 0.016690275 0.014372151 0.011599494 0.0090496428 0.0063170204 0.0038094453][0.0074243192 0.0099170413 0.013264921 0.017200377 0.020467149 0.022690374 0.02326693 0.023116097 0.02203802 0.020466993 0.017901652 0.014893029 0.01199708 0.0088312952 0.0058562011][0.0082525695 0.011072796 0.015006345 0.019589832 0.023607625 0.026575517 0.027708074 0.028146619 0.027211428 0.025602773 0.022899454 0.019509 0.016111195 0.0123909 0.0086627156][0.0091791106 0.011943163 0.015937477 0.02060396 0.02490315 0.028154651 0.029593037 0.030349461 0.029781872 0.028460702 0.025966547 0.022775339 0.019626085 0.015830019 0.011728034][0.010535607 0.012991887 0.016571367 0.020860583 0.024924848 0.02820584 0.029970093 0.031056533 0.030338245 0.029050952 0.02681442 0.024035228 0.021341704 0.017945932 0.014207561][0.011966571 0.013672018 0.016321627 0.01957549 0.022718402 0.025425861 0.027239449 0.028602814 0.028510744 0.027799129 0.02641659 0.024906684 0.023466168 0.021070752 0.017954562][0.01343976 0.014528401 0.016379613 0.018850084 0.021393482 0.023511581 0.025201742 0.026417837 0.026469866 0.02581148 0.025519602 0.025044527 0.024950327 0.02388083 0.021958146][0.015314499 0.015522148 0.016238289 0.017704664 0.019351114 0.020930452 0.022540616 0.024023259 0.02451143 0.024774142 0.025610046 0.026545191 0.027923847 0.028069673 0.027451845][0.016640637 0.016064804 0.015772315 0.016221935 0.016989937 0.017790519 0.01884903 0.020102525 0.020852171 0.0218708 0.024145095 0.026875731 0.029905153 0.031831041 0.032906484][0.0179355 0.016684944 0.015403692 0.014817029 0.01457284 0.014662445 0.015263051 0.016177826 0.017275132 0.019179357 0.02276847 0.027075671 0.031635288 0.035377759 0.038025592][0.018189814 0.01645636 0.014468292 0.013114804 0.012128848 0.011805729 0.012181733 0.013271945 0.015101472 0.018137647 0.023006583 0.028672894 0.034603428 0.039720777 0.04349336][0.0184583 0.016364127 0.013954831 0.012120781 0.010678378 0.010086375 0.010323629 0.011412776 0.013389558 0.017332921 0.023246504 0.030079106 0.037250377 0.043735117 0.048674837]]...]
INFO - root - 2017-12-09 20:35:26.585150: step 57710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:57m:49s remains)
INFO - root - 2017-12-09 20:35:35.316045: step 57720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:49m:56s remains)
INFO - root - 2017-12-09 20:35:44.139655: step 57730, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 67h:00m:14s remains)
INFO - root - 2017-12-09 20:35:52.951767: step 57740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:14m:19s remains)
INFO - root - 2017-12-09 20:36:01.622450: step 57750, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:00m:20s remains)
INFO - root - 2017-12-09 20:36:10.196583: step 57760, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 64h:52m:21s remains)
INFO - root - 2017-12-09 20:36:18.784780: step 57770, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 68h:04m:36s remains)
INFO - root - 2017-12-09 20:36:27.590505: step 57780, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 67h:09m:30s remains)
INFO - root - 2017-12-09 20:36:36.537702: step 57790, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 68h:41m:28s remains)
INFO - root - 2017-12-09 20:36:45.061682: step 57800, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 65h:43m:36s remains)
2017-12-09 20:36:45.948926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033804693 -0.0033782972 -0.0033782343 -0.0033783542 -0.0033785249 -0.0033788185 -0.00337921 -0.0033795405 -0.003379799 -0.0033798805 -0.0033798767 -0.0033792644 -0.0033781813 -0.0033773368 -0.0033766159][-0.0033778963 -0.0033754935 -0.0033754024 -0.0033754501 -0.0033756089 -0.0033759826 -0.0033765277 -0.0033769479 -0.0033771675 -0.0033774008 -0.0033776828 -0.0033774218 -0.0033765438 -0.0033758821 -0.003375262][-0.003377988 -0.003375256 -0.0033749761 -0.0033747589 -0.0033746371 -0.0033748501 -0.0033753165 -0.0033757128 -0.003375985 -0.0033765221 -0.0033773903 -0.003377818 -0.0033775102 -0.0033773021 -0.0033770495][-0.0033788283 -0.0033758618 -0.0033753072 -0.00337474 -0.0033743817 -0.0033744576 -0.0033747626 -0.0033750129 -0.0033752744 -0.003376001 -0.0033773971 -0.0033787515 -0.0033793312 -0.0033800423 -0.0033802409][-0.0033803107 -0.0033771992 -0.0033764294 -0.0033756474 -0.0033751936 -0.00337497 -0.0033749081 -0.0033750541 -0.0033753659 -0.0033762369 -0.003378042 -0.0033803019 -0.00338207 -0.0033836879 -0.0033843005][-0.0033820258 -0.003378754 -0.0033778087 -0.0033769461 -0.0033765293 -0.0033761733 -0.0033758578 -0.0033759805 -0.0033766842 -0.0033780271 -0.0033803026 -0.0033830914 -0.0033856211 -0.0033877059 -0.0033888323][-0.0033838365 -0.0033805796 -0.0033797899 -0.0033791228 -0.0033788481 -0.0033782443 -0.0033777615 -0.0033778057 -0.00337905 -0.0033808723 -0.0033832476 -0.0033860237 -0.0033888782 -0.0033912896 -0.0033925355][-0.0033846465 -0.0033815124 -0.0033810928 -0.0033810067 -0.003381392 -0.0033810332 -0.0033802772 -0.0033805296 -0.0033822411 -0.003384151 -0.0033862412 -0.0033885313 -0.0033909942 -0.003393237 -0.003394383][-0.0033853559 -0.0033824653 -0.0033826232 -0.003383612 -0.0033851038 -0.0033859729 -0.0033862223 -0.003386569 -0.0033877718 -0.0033889946 -0.0033901569 -0.0033915644 -0.0033931583 -0.0033945169 -0.0033953567][-0.0033869143 -0.0033842821 -0.0033847599 -0.0033866155 -0.0033893911 -0.0033916656 -0.0033932265 -0.00339396 -0.0033943122 -0.0033943211 -0.0033939376 -0.0033937953 -0.0033942931 -0.0033947143 -0.0033952482][-0.0033891066 -0.003386732 -0.0033873233 -0.0033894861 -0.0033929145 -0.0033963483 -0.0033986652 -0.0033997169 -0.0033996126 -0.0033985293 -0.0033966063 -0.0033949993 -0.0033943721 -0.0033939895 -0.0033942075][-0.0033902379 -0.0033880677 -0.0033887834 -0.0033908449 -0.0033943141 -0.0033977632 -0.003400014 -0.0034012343 -0.003400791 -0.003399129 -0.0033964941 -0.0033939788 -0.0033928021 -0.0033921858 -0.0033922237][-0.0033902247 -0.0033882067 -0.003388904 -0.0033907713 -0.0033936899 -0.0033968515 -0.003398871 -0.0034000233 -0.0033996077 -0.0033976424 -0.0033949437 -0.003392101 -0.0033903401 -0.0033894079 -0.0033892919][-0.0033893469 -0.0033873394 -0.0033878873 -0.0033893632 -0.0033915367 -0.0033937166 -0.0033951486 -0.0033956196 -0.0033949357 -0.0033930661 -0.0033907301 -0.003388362 -0.0033867711 -0.003385904 -0.0033857666][-0.0033869268 -0.0033847792 -0.0033851888 -0.0033861585 -0.0033875648 -0.0033889613 -0.0033898193 -0.0033899266 -0.0033891986 -0.0033877564 -0.0033861054 -0.0033845284 -0.0033834402 -0.0033828074 -0.0033827685]]...]
INFO - root - 2017-12-09 20:36:54.592386: step 57810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:56m:39s remains)
INFO - root - 2017-12-09 20:37:03.249937: step 57820, loss = 0.90, batch loss = 0.70 (9.6 examples/sec; 0.837 sec/batch; 63h:51m:19s remains)
INFO - root - 2017-12-09 20:37:11.973009: step 57830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:49m:42s remains)
INFO - root - 2017-12-09 20:37:20.654251: step 57840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:18m:42s remains)
INFO - root - 2017-12-09 20:37:29.401176: step 57850, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 66h:18m:57s remains)
INFO - root - 2017-12-09 20:37:37.979637: step 57860, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 62h:13m:23s remains)
INFO - root - 2017-12-09 20:37:46.636078: step 57870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:17m:46s remains)
INFO - root - 2017-12-09 20:37:55.322257: step 57880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:33m:34s remains)
INFO - root - 2017-12-09 20:38:04.036180: step 57890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:43m:30s remains)
INFO - root - 2017-12-09 20:38:12.719537: step 57900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:30m:14s remains)
2017-12-09 20:38:13.678392: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.53409058 0.53387862 0.53120828 0.52651846 0.51950383 0.51031905 0.50165951 0.48764911 0.47393513 0.45886755 0.44503236 0.43194062 0.41782555 0.40633705 0.39539039][0.574435 0.57658714 0.57498419 0.57306474 0.56823313 0.56174529 0.55459172 0.54148966 0.52725875 0.51134008 0.49704397 0.48188311 0.46669292 0.45480928 0.44318944][0.602097 0.60872835 0.60976869 0.61014384 0.60610634 0.60148734 0.59397948 0.58084404 0.56619924 0.5501945 0.5358991 0.52053213 0.50583053 0.49423993 0.48221973][0.62779778 0.63921785 0.64253265 0.64334172 0.63942486 0.63595027 0.62842339 0.61526018 0.60016865 0.584503 0.57056242 0.55493945 0.54082322 0.52944857 0.51789886][0.64805841 0.66512728 0.67131007 0.67329651 0.67028868 0.66702658 0.65969145 0.64749825 0.63261443 0.61697137 0.60273004 0.587274 0.57296175 0.56108111 0.54971331][0.66513044 0.68547851 0.69245523 0.69523054 0.69276214 0.68934494 0.68235445 0.67176348 0.65794832 0.64345562 0.63005292 0.61520123 0.60119832 0.58937997 0.57802975][0.6757406 0.70018435 0.70774043 0.71070433 0.70916331 0.70617115 0.69939911 0.68946391 0.67628616 0.66320527 0.65041071 0.6368233 0.62401754 0.61253935 0.60164613][0.67539656 0.70322639 0.71073329 0.71385288 0.71301275 0.7106626 0.70495868 0.69546813 0.68265676 0.66981488 0.65713596 0.64488977 0.63385314 0.62443352 0.61490065][0.66203183 0.69186759 0.69872719 0.70182836 0.70180482 0.70004648 0.69571376 0.68791038 0.67738885 0.66595769 0.654363 0.64391971 0.63507688 0.627909 0.62029731][0.63454747 0.66431183 0.66933322 0.671249 0.67090732 0.668914 0.66531569 0.65923315 0.6515848 0.64360422 0.63550436 0.62906843 0.62384892 0.62022752 0.61512572][0.59512335 0.62221843 0.62338656 0.62252361 0.6204102 0.61723214 0.61395258 0.60970318 0.60578191 0.60139167 0.59720528 0.59541827 0.59528828 0.59670049 0.59506255][0.54670537 0.57134104 0.57012957 0.56564164 0.56035286 0.55487984 0.55074036 0.54694927 0.54486716 0.543701 0.54307133 0.54489279 0.54884928 0.5546301 0.55717629][0.49336702 0.51469195 0.51131624 0.50410414 0.49628145 0.48847106 0.48281839 0.47880676 0.47773528 0.4781642 0.47941118 0.48326594 0.4894658 0.49736464 0.50243235][0.43360856 0.4510026 0.44666627 0.43850982 0.42988604 0.42105779 0.41488644 0.41141206 0.41139403 0.41284519 0.41510546 0.41978613 0.42648712 0.43434426 0.43996558][0.37272987 0.38671577 0.38187653 0.37354231 0.36472121 0.35623422 0.3503941 0.34740141 0.347682 0.34949768 0.35201415 0.35593787 0.36143214 0.3679204 0.37288123]]...]
INFO - root - 2017-12-09 20:38:22.245646: step 57910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:25m:14s remains)
INFO - root - 2017-12-09 20:38:30.710358: step 57920, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 63h:05m:06s remains)
INFO - root - 2017-12-09 20:38:39.169129: step 57930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:38m:55s remains)
INFO - root - 2017-12-09 20:38:47.780625: step 57940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 67h:26m:05s remains)
INFO - root - 2017-12-09 20:38:56.658223: step 57950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:09m:02s remains)
INFO - root - 2017-12-09 20:39:05.320303: step 57960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:38m:03s remains)
INFO - root - 2017-12-09 20:39:13.605824: step 57970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:27m:05s remains)
INFO - root - 2017-12-09 20:39:22.353333: step 57980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:00m:21s remains)
INFO - root - 2017-12-09 20:39:30.998484: step 57990, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 65h:39m:12s remains)
INFO - root - 2017-12-09 20:39:39.496561: step 58000, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 64h:20m:09s remains)
2017-12-09 20:39:40.430697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034043856 -0.0034017735 -0.0033999989 -0.0033985684 -0.0033977993 -0.003397831 -0.0033986168 -0.0033999337 -0.0034012743 -0.0034022189 -0.0034023179 -0.0034013498 -0.0033994468 -0.0033972086 -0.0033951423][-0.0034083056 -0.0034050825 -0.0034026103 -0.0034006338 -0.0033996028 -0.0033997768 -0.0034011572 -0.0034032264 -0.0034053461 -0.0034069228 -0.0034071296 -0.0034057016 -0.0034029773 -0.0033998662 -0.0033968515][-0.0034117312 -0.0034081812 -0.0034053326 -0.0034030429 -0.0034018925 -0.0034022522 -0.0034042159 -0.0034070895 -0.0034099515 -0.0034120705 -0.0034124348 -0.003410653 -0.0034072436 -0.0034032825 -0.0033992929][-0.0034131394 -0.0034095459 -0.0034065733 -0.0034041659 -0.0034029633 -0.0034034741 -0.0034058641 -0.0034094304 -0.0034130674 -0.003416 -0.0034168605 -0.0034151003 -0.0034112728 -0.0034065703 -0.0034016308][-0.0034114942 -0.00340801 -0.003405245 -0.0034029712 -0.003401933 -0.0034026881 -0.0034054511 -0.0034095361 -0.0034137792 -0.0034175548 -0.0034192475 -0.0034180193 -0.0034143007 -0.0034092914 -0.0034036529][-0.0034070571 -0.0034038813 -0.0034016878 -0.003399784 -0.0033989535 -0.0034000273 -0.0034031193 -0.0034074576 -0.0034120383 -0.0034164982 -0.0034190365 -0.0034186267 -0.0034155468 -0.0034108271 -0.0034050038][-0.0034015775 -0.0033986745 -0.0033971095 -0.0033958002 -0.00339524 -0.0033962703 -0.0033993428 -0.0034038448 -0.0034088828 -0.0034138903 -0.0034172228 -0.0034176619 -0.0034153841 -0.003411206 -0.0034055999][-0.0033971025 -0.003394237 -0.0033929802 -0.0033921679 -0.0033919932 -0.0033930072 -0.0033957213 -0.0034001498 -0.0034057102 -0.0034114523 -0.0034154549 -0.0034164423 -0.0034147049 -0.0034109117 -0.0034054695][-0.0033943243 -0.0033913136 -0.0033900877 -0.0033895425 -0.0033898405 -0.0033911294 -0.0033937374 -0.0033978564 -0.0034033356 -0.0034092662 -0.0034136944 -0.0034150376 -0.0034135866 -0.0034099803 -0.003404713][-0.0033935481 -0.0033903471 -0.003389009 -0.0033885313 -0.0033890689 -0.0033906114 -0.0033933716 -0.0033974049 -0.0034024317 -0.0034077575 -0.0034118486 -0.00341315 -0.003411812 -0.0034083861 -0.0034034932][-0.0033942002 -0.0033908626 -0.00338947 -0.0033889231 -0.0033893748 -0.0033909099 -0.0033935851 -0.0033974424 -0.0034019558 -0.0034064474 -0.0034097312 -0.0034106309 -0.0034092963 -0.0034061298 -0.0034018019][-0.0033950687 -0.0033918421 -0.0033905024 -0.0033899096 -0.0033902344 -0.0033915567 -0.0033939232 -0.0033972641 -0.0034010308 -0.0034044906 -0.0034068828 -0.0034073151 -0.0034059237 -0.003403099 -0.0033995344][-0.0033954363 -0.0033925322 -0.0033914011 -0.00339084 -0.0033909739 -0.0033919469 -0.0033937816 -0.0033963316 -0.0033991034 -0.0034015167 -0.0034030934 -0.0034032715 -0.0034020639 -0.003399821 -0.003397146][-0.003395072 -0.00339255 -0.0033918081 -0.0033913741 -0.0033914207 -0.003392016 -0.003393183 -0.003394831 -0.0033966119 -0.0033980855 -0.0033989842 -0.0033990319 -0.0033982124 -0.003396739 -0.0033950189][-0.0033943879 -0.0033922228 -0.0033918682 -0.0033915851 -0.0033915825 -0.00339189 -0.003392492 -0.0033933534 -0.0033942841 -0.0033950745 -0.0033955404 -0.0033955388 -0.0033950617 -0.0033942456 -0.0033933292]]...]
INFO - root - 2017-12-09 20:39:49.050594: step 58010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:24m:17s remains)
INFO - root - 2017-12-09 20:39:57.485932: step 58020, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.816 sec/batch; 62h:12m:55s remains)
INFO - root - 2017-12-09 20:40:06.053112: step 58030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:28m:46s remains)
INFO - root - 2017-12-09 20:40:14.700599: step 58040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:02m:04s remains)
INFO - root - 2017-12-09 20:40:23.280808: step 58050, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 66h:22m:12s remains)
INFO - root - 2017-12-09 20:40:31.807689: step 58060, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 63h:10m:08s remains)
INFO - root - 2017-12-09 20:40:40.204884: step 58070, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.919 sec/batch; 70h:05m:20s remains)
INFO - root - 2017-12-09 20:40:49.063671: step 58080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:52m:00s remains)
INFO - root - 2017-12-09 20:40:57.766214: step 58090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:06m:07s remains)
INFO - root - 2017-12-09 20:41:06.405487: step 58100, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.705 sec/batch; 53h:45m:08s remains)
2017-12-09 20:41:07.346051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030734679 -0.0031229174 -0.0031748943 -0.0032116435 -0.0032446277 -0.003268847 -0.0032910788 -0.0033023651 -0.0033022368 -0.0032841798 -0.0032464887 -0.0031865456 -0.0031219572 -0.0030723468 -0.003050983][-0.0029647609 -0.0030120185 -0.0030667123 -0.0031093755 -0.0031513856 -0.0031876969 -0.0032224907 -0.003242834 -0.0032476506 -0.0032295336 -0.00319291 -0.0031367452 -0.003079836 -0.003036658 -0.0030198656][-0.0028635375 -0.002900898 -0.0029501556 -0.0029925222 -0.0030385468 -0.0030807927 -0.003122332 -0.0031504114 -0.0031606348 -0.0031457897 -0.003114637 -0.0030705375 -0.0030294247 -0.0029985025 -0.0029905695][-0.0027875169 -0.0028077429 -0.002849869 -0.0028895065 -0.0029335469 -0.002974547 -0.0030156039 -0.0030452351 -0.0030568065 -0.0030473559 -0.0030268994 -0.0029996315 -0.002977096 -0.0029610824 -0.0029641513][-0.002739666 -0.0027405238 -0.0027752477 -0.0028113292 -0.0028521894 -0.0028895808 -0.0029250637 -0.0029513794 -0.0029637162 -0.0029595839 -0.002949266 -0.0029370624 -0.0029303292 -0.0029289175 -0.0029419172][-0.0027204752 -0.002701618 -0.00272698 -0.0027581109 -0.0027947822 -0.002827147 -0.0028571005 -0.0028791323 -0.0028917603 -0.0028942353 -0.0028941771 -0.0028926053 -0.0028960328 -0.0029056834 -0.0029267089][-0.0027296362 -0.0026927034 -0.0027078325 -0.0027305791 -0.002759486 -0.00278531 -0.0028097869 -0.0028300886 -0.0028443031 -0.0028526564 -0.0028615827 -0.0028700677 -0.0028801456 -0.0028955322 -0.0029211654][-0.0027635184 -0.0027129338 -0.0027184775 -0.0027322085 -0.0027510349 -0.0027688537 -0.0027879493 -0.0028069385 -0.0028228322 -0.0028363646 -0.0028527854 -0.0028691692 -0.0028824829 -0.0029002172 -0.0029281678][-0.0028191237 -0.0027593726 -0.0027579344 -0.0027649691 -0.0027754321 -0.0027852198 -0.0028000332 -0.0028167889 -0.0028346276 -0.0028518992 -0.002872051 -0.0028914588 -0.0029049776 -0.0029214351 -0.0029495382][-0.0028895647 -0.0028277966 -0.0028219873 -0.0028243503 -0.0028297105 -0.0028322884 -0.0028428584 -0.0028574392 -0.0028764282 -0.0028957282 -0.0029172087 -0.0029361509 -0.0029470907 -0.0029600824 -0.0029831824][-0.0029684214 -0.0029100648 -0.0029024433 -0.0029018861 -0.002903203 -0.0029024279 -0.0029088575 -0.0029204949 -0.0029384203 -0.0029572081 -0.0029777954 -0.002994105 -0.0030024694 -0.0030096509 -0.003025701][-0.0030481976 -0.0029961895 -0.0029881035 -0.0029854851 -0.0029838379 -0.0029817736 -0.0029850653 -0.0029934817 -0.0030088583 -0.0030255015 -0.0030428926 -0.0030555059 -0.0030618161 -0.0030650585 -0.0030735903][-0.0031223125 -0.0030781818 -0.0030696364 -0.0030652368 -0.0030621001 -0.0030605297 -0.0030626042 -0.0030679516 -0.0030788584 -0.0030911358 -0.0031038229 -0.0031119955 -0.003116905 -0.0031192035 -0.0031238915][-0.0031867316 -0.003151723 -0.0031440682 -0.0031388737 -0.0031350951 -0.0031337154 -0.0031348004 -0.0031380972 -0.0031446265 -0.0031503898 -0.0031559442 -0.0031607854 -0.0031642471 -0.0031672784 -0.0031707238][-0.0032388158 -0.0032129996 -0.0032070326 -0.0032028616 -0.0031992712 -0.0031971335 -0.0031953421 -0.0031970141 -0.0031999042 -0.0031992556 -0.0031996188 -0.0032014265 -0.0032029329 -0.0032059895 -0.0032095402]]...]
INFO - root - 2017-12-09 20:41:15.966417: step 58110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:59m:16s remains)
INFO - root - 2017-12-09 20:41:24.817606: step 58120, loss = 0.90, batch loss = 0.69 (8.2 examples/sec; 0.971 sec/batch; 74h:00m:38s remains)
INFO - root - 2017-12-09 20:41:33.418580: step 58130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 64h:18m:18s remains)
INFO - root - 2017-12-09 20:41:42.071214: step 58140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:10m:11s remains)
INFO - root - 2017-12-09 20:41:50.844946: step 58150, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 66h:08m:11s remains)
INFO - root - 2017-12-09 20:41:59.720010: step 58160, loss = 0.89, batch loss = 0.68 (8.3 examples/sec; 0.970 sec/batch; 73h:53m:17s remains)
INFO - root - 2017-12-09 20:42:08.081609: step 58170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:54m:23s remains)
INFO - root - 2017-12-09 20:42:16.759094: step 58180, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.909 sec/batch; 69h:14m:13s remains)
INFO - root - 2017-12-09 20:42:25.425682: step 58190, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.904 sec/batch; 68h:52m:47s remains)
INFO - root - 2017-12-09 20:42:34.021054: step 58200, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 57h:59m:13s remains)
2017-12-09 20:42:34.962658: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4418005 0.43100947 0.41904402 0.4085626 0.39970365 0.39274934 0.38807765 0.38539147 0.38465741 0.38102058 0.37262842 0.361822 0.34559378 0.32527405 0.30002823][0.43034607 0.42241639 0.41413006 0.40739337 0.40181911 0.39801729 0.3953307 0.39474195 0.3951093 0.39220631 0.3845233 0.3725791 0.35516977 0.33316737 0.30629584][0.407326 0.40244472 0.39684862 0.3929 0.39018661 0.38946071 0.39011475 0.39146286 0.39279512 0.39098004 0.383448 0.37093869 0.35295859 0.33073503 0.30378422][0.38289833 0.38238037 0.38057411 0.37963894 0.37910631 0.38026318 0.38255671 0.3851319 0.3871375 0.38617986 0.37979737 0.36789268 0.34999096 0.32784143 0.30132762][0.36187112 0.3653923 0.36713812 0.3690719 0.37143189 0.3742052 0.377017 0.37930474 0.38113803 0.38043192 0.37485772 0.36404631 0.34738904 0.32617289 0.30026105][0.34979388 0.35672367 0.36049667 0.36376229 0.36689746 0.37053856 0.37399676 0.37610993 0.37785816 0.37795535 0.37384111 0.36438933 0.34881681 0.32838753 0.30308264][0.34850785 0.357133 0.36099067 0.363691 0.36549628 0.36768484 0.36978358 0.37228814 0.37460762 0.37627563 0.37470484 0.36766297 0.35433683 0.3348726 0.30964655][0.35312292 0.36467412 0.36845908 0.37021512 0.3701542 0.36949909 0.36934707 0.3701795 0.37251496 0.37545922 0.37577632 0.37122747 0.35980755 0.3412452 0.3160381][0.35790414 0.3729018 0.37835833 0.3808772 0.3802188 0.37756631 0.37423906 0.3727462 0.37385976 0.37671942 0.3783758 0.37587178 0.36625752 0.34846595 0.32321256][0.36204174 0.37884748 0.38456371 0.38689864 0.3861095 0.38229224 0.37738293 0.37432775 0.37456927 0.37752193 0.37970868 0.37829036 0.36973941 0.35278413 0.32755655][0.36052561 0.37793148 0.38313052 0.3849951 0.38353503 0.3793045 0.37428325 0.37121189 0.37139404 0.37436822 0.37673089 0.37587434 0.36796108 0.351713 0.32701844][0.35250631 0.36955908 0.37366277 0.3745409 0.37237334 0.36830467 0.36381239 0.36087462 0.36101264 0.36356843 0.365453 0.36438075 0.35704052 0.34212217 0.31910577][0.33713481 0.35376275 0.35789436 0.35911623 0.35746172 0.35384849 0.34985113 0.34676611 0.34625322 0.34764063 0.34825367 0.34631091 0.33896551 0.32530919 0.30472863][0.31491235 0.33009461 0.333723 0.33535305 0.33502984 0.3329415 0.33055836 0.32838565 0.32794383 0.32834935 0.32785329 0.32511494 0.3178297 0.30558446 0.28779936][0.28956228 0.30260912 0.30507123 0.30626348 0.30621356 0.30522645 0.30444077 0.30337816 0.30332696 0.30328265 0.30228019 0.29918164 0.29243 0.28212342 0.26771486]]...]
INFO - root - 2017-12-09 20:42:43.731323: step 58210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 67h:53m:32s remains)
INFO - root - 2017-12-09 20:42:52.469712: step 58220, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 64h:50m:58s remains)
INFO - root - 2017-12-09 20:43:01.177142: step 58230, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 68h:20m:31s remains)
INFO - root - 2017-12-09 20:43:10.013752: step 58240, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:45m:13s remains)
INFO - root - 2017-12-09 20:43:18.662984: step 58250, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 63h:47m:55s remains)
INFO - root - 2017-12-09 20:43:27.245957: step 58260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:41m:39s remains)
INFO - root - 2017-12-09 20:43:35.768720: step 58270, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 65h:13m:52s remains)
INFO - root - 2017-12-09 20:43:44.489013: step 58280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:11m:12s remains)
INFO - root - 2017-12-09 20:43:53.094013: step 58290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:43m:51s remains)
INFO - root - 2017-12-09 20:44:01.592946: step 58300, loss = 0.89, batch loss = 0.68 (11.4 examples/sec; 0.703 sec/batch; 53h:33m:58s remains)
2017-12-09 20:44:02.497413: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057960253 0.085316457 0.10986744 0.12543632 0.12712419 0.11402589 0.0903032 0.062639095 0.036903478 0.016743207 0.0041782251 -0.0016352078 -0.0032336577 -0.0033386284 -0.0033448064][0.064452812 0.096918792 0.12734009 0.14805368 0.15259713 0.13919869 0.1121584 0.079135552 0.047554921 0.022381952 0.0064475071 -0.001093948 -0.0031963575 -0.0033329122 -0.00334251][0.063647717 0.097396992 0.1302467 0.15364456 0.16036868 0.14808364 0.12095843 0.086550824 0.05269656 0.025205605 0.0076173451 -0.00077748834 -0.0031757238 -0.0033310908 -0.0033422143][0.055188503 0.08652252 0.11775123 0.14067316 0.14833587 0.13828899 0.11389154 0.081968687 0.050145749 0.024171965 0.0074187303 -0.0007653425 -0.0031703261 -0.0033310582 -0.0033427179][0.04064548 0.06662181 0.093346357 0.11351895 0.12114277 0.1140198 0.094583049 0.068124831 0.041279987 0.019476011 0.0055735763 -0.0011903397 -0.0031988313 -0.0033304156 -0.0033419416][0.024741892 0.043665145 0.06371545 0.079268478 0.085733823 0.081461579 0.06774801 0.048338406 0.028400065 0.012491846 0.0026877753 -0.0019326704 -0.0032449511 -0.0033288286 -0.0033390883][0.011824176 0.023578353 0.036222987 0.046004597 0.050210331 0.047848113 0.039493572 0.027259843 0.014698716 0.0051377397 -0.00032203365 -0.0026935679 -0.0032892635 -0.0033259047 -0.0033333327][0.0032449386 0.00915758 0.01550729 0.020208333 0.022168044 0.021024819 0.016815422 0.010581323 0.0043030409 -8.9316629e-05 -0.0022975584 -0.0031317438 -0.0033099893 -0.0033248009 -0.003329861][-0.0011715468 0.0010594984 0.0033483023 0.0048763016 0.0054027606 0.004891553 0.0032856383 0.00090942229 -0.0013222948 -0.0026494216 -0.00315433 -0.0032957573 -0.003316459 -0.0033233853 -0.0033281739][-0.0028895431 -0.0023575607 -0.0018635556 -0.0016113279 -0.0015528725 -0.0016842234 -0.0020815162 -0.0026413489 -0.0031014776 -0.003283022 -0.0033054615 -0.0033104892 -0.0033173887 -0.0033230458 -0.0033275755][-0.0032971208 -0.0032248695 -0.0031611104 -0.0031308217 -0.003122831 -0.0031369741 -0.0031853872 -0.00325083 -0.0033009339 -0.0033149568 -0.003313506 -0.0033152658 -0.0033211908 -0.0033256994 -0.0033290635][-0.0033542551 -0.0033473838 -0.0033431095 -0.0033406005 -0.0033392806 -0.0033387637 -0.0033361905 -0.0033338619 -0.0033297571 -0.0033256349 -0.0033224849 -0.0033230213 -0.0033274663 -0.0033312812 -0.0033335239][-0.0033583357 -0.0033570006 -0.0033574393 -0.0033574663 -0.0033580614 -0.0033559604 -0.0033510025 -0.0033454688 -0.0033392115 -0.0033347511 -0.003331536 -0.0033321634 -0.003335607 -0.0033381858 -0.0033398392][-0.0033560246 -0.0033559147 -0.0033573441 -0.0033574719 -0.0033584225 -0.0033587122 -0.0033564137 -0.0033524449 -0.0033476311 -0.0033437635 -0.0033404287 -0.0033394853 -0.0033409887 -0.0033424 -0.003343296][-0.003353823 -0.0033538397 -0.0033557 -0.0033561704 -0.0033568039 -0.0033579622 -0.003357884 -0.0033560735 -0.0033531478 -0.0033503578 -0.0033476274 -0.0033453775 -0.0033442988 -0.0033440844 -0.0033443712]]...]
INFO - root - 2017-12-09 20:44:11.045492: step 58310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:10m:53s remains)
INFO - root - 2017-12-09 20:44:19.439921: step 58320, loss = 0.89, batch loss = 0.68 (8.5 examples/sec; 0.943 sec/batch; 71h:50m:18s remains)
INFO - root - 2017-12-09 20:44:28.096666: step 58330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:18m:24s remains)
INFO - root - 2017-12-09 20:44:36.779565: step 58340, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.829 sec/batch; 63h:07m:04s remains)
INFO - root - 2017-12-09 20:44:45.591784: step 58350, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 69h:14m:54s remains)
INFO - root - 2017-12-09 20:44:54.344135: step 58360, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:24m:24s remains)
INFO - root - 2017-12-09 20:45:02.737203: step 58370, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 66h:07m:38s remains)
INFO - root - 2017-12-09 20:45:11.348659: step 58380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:02m:09s remains)
INFO - root - 2017-12-09 20:45:20.062419: step 58390, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 66h:08m:04s remains)
INFO - root - 2017-12-09 20:45:28.657884: step 58400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:26m:31s remains)
2017-12-09 20:45:29.438595: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0061602835 0.011127837 0.016457535 0.021518286 0.024547637 0.025757493 0.02591051 0.0252404 0.024612525 0.022927808 0.020124722 0.016800821 0.01247482 0.0077712378 0.0034793008][0.0096362019 0.016555604 0.024423651 0.032194346 0.037065029 0.039778005 0.0413213 0.042650744 0.044100545 0.043437481 0.040081482 0.034382291 0.02676923 0.018042719 0.0097521571][0.016295174 0.027305957 0.041101422 0.055288147 0.065359958 0.072049737 0.076149337 0.078957781 0.080110669 0.078115277 0.072339617 0.06266509 0.049958661 0.0352863 0.02149255][0.027396414 0.043991443 0.06579452 0.089117661 0.10773227 0.12171547 0.1312855 0.13849381 0.14139135 0.13767602 0.12694804 0.1095638 0.087658569 0.062823229 0.039639764][0.041818924 0.065400138 0.0962609 0.12867586 0.15654472 0.17818083 0.19339457 0.20469612 0.20977062 0.20546575 0.19041149 0.16572976 0.1343724 0.098583326 0.064526461][0.055708434 0.084860414 0.12330849 0.16358666 0.19871987 0.22591658 0.24491864 0.25841936 0.2644183 0.25921825 0.24136075 0.21214315 0.17459403 0.13107668 0.088569187][0.062710121 0.093586862 0.13572957 0.17966375 0.21866629 0.2486949 0.26909825 0.282343 0.2876128 0.28160083 0.26262513 0.23195544 0.19319984 0.14805017 0.10268496][0.058549847 0.087100066 0.12775859 0.17060754 0.21007524 0.24117617 0.26222923 0.2742534 0.27753228 0.26965415 0.24990025 0.22001427 0.1839392 0.14244471 0.10056671][0.046070222 0.06802205 0.10214366 0.14023009 0.176553 0.20588005 0.22678255 0.2380462 0.24000059 0.23067918 0.21133491 0.18428341 0.15301742 0.11825819 0.083873][0.031489044 0.045162007 0.06999933 0.099701993 0.12937541 0.15388873 0.17211178 0.18143027 0.18239553 0.17343649 0.1567242 0.135022 0.11102145 0.085255429 0.0601485][0.0191375 0.025901433 0.041374661 0.061410934 0.081986345 0.099170685 0.11241115 0.11914369 0.11909114 0.1114594 0.098977856 0.083989866 0.068038039 0.051308669 0.03555328][0.0092172232 0.011805449 0.019775996 0.031234333 0.042929705 0.052525196 0.059960164 0.063641846 0.062683955 0.057032734 0.049077742 0.040522821 0.031932693 0.023097832 0.015231343][0.0022709642 0.0030153093 0.0063722329 0.011532793 0.016679911 0.020813674 0.023812469 0.025038902 0.023771873 0.020443164 0.016384669 0.012503788 0.0088856351 0.0053654136 0.0025980838][-0.0017733134 -0.001993323 -0.0011609169 0.00039844029 0.0020494454 0.0032459956 0.0040321527 0.004194716 0.00350825 0.0022554549 0.00092908251 -0.00012908806 -0.0010142603 -0.0017651307 -0.0022482576][-0.0029557436 -0.00320481 -0.003146088 -0.0029019932 -0.0026171636 -0.0024239169 -0.0022914424 -0.0023111468 -0.0024777476 -0.00271197 -0.0029505535 -0.0031206664 -0.0032403325 -0.0032842876 -0.0032920062]]...]
INFO - root - 2017-12-09 20:45:37.992322: step 58410, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 64h:56m:19s remains)
INFO - root - 2017-12-09 20:45:46.544148: step 58420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:04m:14s remains)
INFO - root - 2017-12-09 20:45:54.996602: step 58430, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 62h:43m:12s remains)
INFO - root - 2017-12-09 20:46:03.494638: step 58440, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:22m:06s remains)
INFO - root - 2017-12-09 20:46:12.062439: step 58450, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 66h:55m:58s remains)
INFO - root - 2017-12-09 20:46:20.636805: step 58460, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 65h:22m:15s remains)
INFO - root - 2017-12-09 20:46:29.167486: step 58470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:02m:27s remains)
INFO - root - 2017-12-09 20:46:37.811920: step 58480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:16m:24s remains)
INFO - root - 2017-12-09 20:46:46.532538: step 58490, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:16m:57s remains)
INFO - root - 2017-12-09 20:46:55.195074: step 58500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:56m:10s remains)
2017-12-09 20:46:56.109721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033528912 -0.0033498202 -0.0033500437 -0.0033506872 -0.003351463 -0.0033522181 -0.0033524921 -0.0033521494 -0.0033511121 -0.0033498462 -0.0033482909 -0.0033470381 -0.0033462581 -0.0033457268 -0.0033456373][-0.0033514327 -0.0033485519 -0.0033495373 -0.0033512225 -0.0033531103 -0.0033546528 -0.0033552202 -0.0033545117 -0.003352643 -0.0033502744 -0.0033476278 -0.0033453188 -0.0033437912 -0.0033430094 -0.0033427018][-0.0033528295 -0.0033507759 -0.0033532414 -0.0033568537 -0.0033604503 -0.0033632882 -0.0033645902 -0.003363665 -0.0033606973 -0.0033568386 -0.0033526693 -0.0033489177 -0.0033461421 -0.0033444185 -0.0033435568][-0.0033538374 -0.0033526653 -0.0033567769 -0.0033624377 -0.0033678722 -0.0033721554 -0.003374483 -0.003373751 -0.0033701807 -0.0033651465 -0.0033596926 -0.0033544155 -0.0033501387 -0.0033468844 -0.003344879][-0.0033546838 -0.0033543864 -0.0033599166 -0.0033674072 -0.0033745323 -0.0033801317 -0.0033832991 -0.0033827417 -0.0033788639 -0.0033731861 -0.0033669323 -0.0033606456 -0.0033550877 -0.0033503866 -0.0033469414][-0.0033552586 -0.0033556242 -0.0033619693 -0.0033708119 -0.0033794986 -0.0033865196 -0.0033908798 -0.003390871 -0.0033869578 -0.0033808802 -0.0033739416 -0.0033668033 -0.0033601017 -0.0033541247 -0.0033493366][-0.0033551431 -0.0033558984 -0.003362478 -0.0033717435 -0.0033814025 -0.0033897341 -0.003395607 -0.0033970505 -0.0033942221 -0.0033885937 -0.0033812008 -0.0033731284 -0.0033651849 -0.0033579723 -0.0033517613][-0.0033544835 -0.0033550723 -0.0033610521 -0.0033697824 -0.0033791512 -0.0033878947 -0.0033949034 -0.0033980794 -0.0033975821 -0.003393705 -0.0033869778 -0.0033786336 -0.0033699237 -0.0033615634 -0.0033540342][-0.003353779 -0.0033534425 -0.0033581434 -0.0033652659 -0.0033735109 -0.0033817529 -0.0033891734 -0.0033938356 -0.0033958433 -0.0033944244 -0.0033891893 -0.0033815806 -0.0033728343 -0.0033639113 -0.0033556763][-0.0033528919 -0.0033513738 -0.003354653 -0.0033601348 -0.0033672503 -0.0033749389 -0.0033823159 -0.00338777 -0.00339148 -0.0033920554 -0.0033881424 -0.0033812907 -0.0033730157 -0.0033644198 -0.0033562821][-0.0033521962 -0.0033494509 -0.0033512625 -0.0033551264 -0.0033610042 -0.0033678825 -0.0033748953 -0.0033807203 -0.0033853538 -0.0033870803 -0.0033843338 -0.0033784786 -0.0033708876 -0.0033630237 -0.0033556132][-0.0033514837 -0.0033477705 -0.0033485556 -0.0033508679 -0.0033551904 -0.0033610095 -0.0033672028 -0.0033726166 -0.0033773729 -0.0033795061 -0.0033777708 -0.0033730981 -0.0033667434 -0.0033600943 -0.0033539312][-0.0033508332 -0.0033463035 -0.003346293 -0.0033473799 -0.0033499121 -0.003354013 -0.0033588735 -0.0033633714 -0.0033675197 -0.00336974 -0.0033691397 -0.0033660824 -0.0033614745 -0.0033564647 -0.0033519019][-0.003350155 -0.0033451475 -0.0033448662 -0.0033451407 -0.0033464115 -0.0033488497 -0.0033520206 -0.0033551937 -0.003358257 -0.0033601308 -0.0033602202 -0.003358617 -0.0033558058 -0.00335254 -0.0033495273][-0.0033497412 -0.0033445649 -0.0033440576 -0.0033439039 -0.0033442748 -0.0033454183 -0.0033471095 -0.0033489598 -0.0033508493 -0.0033522055 -0.0033526488 -0.00335211 -0.0033507503 -0.003349076 -0.0033474097]]...]
INFO - root - 2017-12-09 20:47:04.468532: step 58510, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 64h:12m:59s remains)
INFO - root - 2017-12-09 20:47:13.040563: step 58520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:45m:21s remains)
INFO - root - 2017-12-09 20:47:21.711719: step 58530, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 66h:05m:21s remains)
INFO - root - 2017-12-09 20:47:30.483827: step 58540, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 67h:03m:02s remains)
INFO - root - 2017-12-09 20:47:39.174992: step 58550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:03m:22s remains)
INFO - root - 2017-12-09 20:47:47.773047: step 58560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 65h:40m:39s remains)
INFO - root - 2017-12-09 20:47:56.127286: step 58570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 65h:47m:19s remains)
INFO - root - 2017-12-09 20:48:04.717789: step 58580, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:50m:49s remains)
INFO - root - 2017-12-09 20:48:13.128716: step 58590, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 63h:53m:12s remains)
INFO - root - 2017-12-09 20:48:21.687928: step 58600, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 66h:27m:22s remains)
2017-12-09 20:48:22.535459: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46655327 0.432021 0.40367115 0.38662213 0.38485098 0.39564678 0.41169953 0.42845181 0.4374637 0.43611243 0.42552432 0.40890831 0.38618317 0.35891211 0.33005208][0.45051974 0.42180547 0.40333819 0.39840657 0.41002759 0.43139562 0.4555001 0.47623396 0.48554555 0.48126873 0.4652448 0.44260114 0.41395044 0.38171828 0.34812877][0.42485505 0.40324062 0.39468116 0.40172276 0.42686224 0.45951214 0.49112478 0.51513571 0.52302045 0.5144552 0.49264506 0.46457848 0.43192592 0.39638469 0.35948011][0.39660802 0.38166961 0.38163629 0.39926013 0.4360978 0.47905615 0.51685768 0.54366362 0.55092061 0.53909755 0.51292944 0.48040998 0.44487241 0.40703511 0.36816749][0.3759045 0.36719453 0.37505829 0.40166783 0.44781402 0.49646693 0.53793281 0.56533492 0.570622 0.555258 0.52504605 0.489369 0.45150313 0.41262844 0.37311429][0.3686614 0.3651903 0.37741962 0.4102999 0.46125412 0.51374382 0.5575366 0.58369744 0.58647096 0.56766367 0.53411835 0.49545762 0.45543981 0.41599873 0.3761996][0.3784968 0.38153729 0.39663756 0.4307456 0.48237097 0.534785 0.57717133 0.60101104 0.60120136 0.57938981 0.5426966 0.50081837 0.458915 0.41853431 0.37827533][0.39578947 0.40522462 0.42312536 0.45731544 0.50578427 0.5556215 0.59412509 0.61390609 0.61021811 0.58511126 0.5457781 0.50152707 0.45815027 0.4168818 0.37662831][0.41624868 0.43208313 0.45299932 0.48725322 0.53210682 0.57632637 0.60808873 0.62123883 0.61274606 0.58489263 0.544206 0.49916226 0.45561245 0.41443911 0.37446487][0.43673909 0.45764485 0.48018414 0.51301312 0.55274069 0.58901018 0.6139046 0.62034738 0.6073302 0.57723904 0.53609419 0.49199814 0.44930112 0.40921357 0.3700965][0.45147717 0.47574329 0.49802369 0.52692211 0.55977827 0.5885489 0.60697943 0.60850126 0.59327459 0.56333017 0.52401996 0.48213941 0.44120842 0.40254095 0.36458698][0.45688319 0.48265886 0.50358754 0.52787286 0.5540005 0.57565129 0.58777463 0.58553791 0.56958711 0.541627 0.50579816 0.46738854 0.42944425 0.39314958 0.357163][0.44997475 0.47493267 0.49354693 0.51369673 0.53383964 0.54913384 0.55667585 0.55213606 0.53682423 0.51231062 0.48134077 0.44779846 0.41388151 0.38079664 0.34757116][0.43172878 0.45449823 0.47004718 0.48621577 0.50160849 0.51208764 0.51658255 0.51143491 0.49842089 0.47815794 0.45285988 0.42488715 0.39572772 0.36649707 0.33668861][0.40507597 0.42342547 0.43434623 0.4456737 0.45643112 0.46389824 0.46779448 0.46433944 0.45460498 0.43912286 0.4193604 0.39705512 0.3729766 0.34817752 0.32265186]]...]
INFO - root - 2017-12-09 20:48:30.999986: step 58610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:52m:49s remains)
INFO - root - 2017-12-09 20:48:39.468517: step 58620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:16m:28s remains)
INFO - root - 2017-12-09 20:48:48.163252: step 58630, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 73h:28m:41s remains)
INFO - root - 2017-12-09 20:48:56.925867: step 58640, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:10m:35s remains)
INFO - root - 2017-12-09 20:49:05.525161: step 58650, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 66h:56m:26s remains)
INFO - root - 2017-12-09 20:49:14.425269: step 58660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:56m:36s remains)
INFO - root - 2017-12-09 20:49:22.792937: step 58670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:38m:36s remains)
INFO - root - 2017-12-09 20:49:31.448468: step 58680, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 68h:04m:21s remains)
INFO - root - 2017-12-09 20:49:40.357027: step 58690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:31m:56s remains)
INFO - root - 2017-12-09 20:49:49.065316: step 58700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:17m:11s remains)
2017-12-09 20:49:49.939779: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24264963 0.23410946 0.2231704 0.21063289 0.19901046 0.18523201 0.17049509 0.15183063 0.13036951 0.10738622 0.085412033 0.066374511 0.051406272 0.038880575 0.029314419][0.25586841 0.24615733 0.2343815 0.22144975 0.2102797 0.19835953 0.1854371 0.17008178 0.15075201 0.12905852 0.1070566 0.086963527 0.07027328 0.055942371 0.044647038][0.26364446 0.25176 0.238674 0.22492406 0.21478441 0.20589878 0.19636257 0.1849965 0.16958545 0.15116234 0.13098876 0.11200392 0.0954894 0.080289952 0.067442387][0.26773876 0.25302893 0.23885216 0.22484873 0.21557747 0.2095855 0.20392796 0.19709194 0.1858817 0.1717841 0.15498443 0.13861623 0.12385939 0.11022446 0.097848251][0.2703234 0.25281847 0.2366882 0.22265974 0.21464556 0.21112143 0.20924343 0.20667797 0.19986126 0.1900782 0.17749703 0.16470309 0.15262167 0.14115761 0.13028479][0.2683942 0.24923429 0.232515 0.21871264 0.21252099 0.2123246 0.21444887 0.21675929 0.21527624 0.21066332 0.20271906 0.19324355 0.18358672 0.17392053 0.16344325][0.26280776 0.24371444 0.22581795 0.21246387 0.2071137 0.20916279 0.21509427 0.22246435 0.22681236 0.22742321 0.22446883 0.21916668 0.21301356 0.20589872 0.19743481][0.2547012 0.23468138 0.21507634 0.20092493 0.19587374 0.20018427 0.21073271 0.22335711 0.23317027 0.23914912 0.24071996 0.23945682 0.23660892 0.23240739 0.22625805][0.24615192 0.22625691 0.20454775 0.18958287 0.18466116 0.18962178 0.20200416 0.21788888 0.23215753 0.24267682 0.24876887 0.25212204 0.2531696 0.25214362 0.24884754][0.24074344 0.22059925 0.1973809 0.18067043 0.17457302 0.17858166 0.19129907 0.20878123 0.22621943 0.24038747 0.25014034 0.25686228 0.26078114 0.26196223 0.26068658][0.23494084 0.21670955 0.19387695 0.17615169 0.16941829 0.17152715 0.18316634 0.20040324 0.21892107 0.23521349 0.24746194 0.2566244 0.26271167 0.26580593 0.2665357][0.22660285 0.21140066 0.19076245 0.17391607 0.16680409 0.16706561 0.17727022 0.1931054 0.21160527 0.22832739 0.24182786 0.25257248 0.26016277 0.2649847 0.26779464][0.22003134 0.20710997 0.18842241 0.17167065 0.16427366 0.16284099 0.17196776 0.18662886 0.20465615 0.2207365 0.23393533 0.24510729 0.25342306 0.25955591 0.26403487][0.21646337 0.20653395 0.1901571 0.17391549 0.16543734 0.16253209 0.16956858 0.18151183 0.1974614 0.21207246 0.224316 0.23514046 0.24368986 0.25075161 0.25638455][0.21137643 0.204897 0.19158474 0.17777374 0.16987114 0.16586611 0.17080215 0.18005396 0.19325171 0.20581016 0.21625727 0.22559477 0.23295131 0.2396051 0.24547753]]...]
INFO - root - 2017-12-09 20:49:58.450815: step 58710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 64h:08m:55s remains)
INFO - root - 2017-12-09 20:50:06.866340: step 58720, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 65h:54m:44s remains)
INFO - root - 2017-12-09 20:50:15.476135: step 58730, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 66h:25m:53s remains)
INFO - root - 2017-12-09 20:50:24.099953: step 58740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:22m:15s remains)
INFO - root - 2017-12-09 20:50:32.847874: step 58750, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 67h:23m:01s remains)
INFO - root - 2017-12-09 20:50:41.503913: step 58760, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 66h:47m:27s remains)
INFO - root - 2017-12-09 20:50:49.950613: step 58770, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 57h:53m:02s remains)
INFO - root - 2017-12-09 20:50:58.539933: step 58780, loss = 0.90, batch loss = 0.70 (9.7 examples/sec; 0.829 sec/batch; 63h:00m:14s remains)
INFO - root - 2017-12-09 20:51:07.322531: step 58790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:46m:23s remains)
INFO - root - 2017-12-09 20:51:15.887552: step 58800, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 63h:23m:43s remains)
2017-12-09 20:51:16.802758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033702012 -0.0033686121 -0.0033672482 -0.0033614596 -0.0033567324 -0.0033488914 -0.003347334 -0.0033470674 -0.0033498104 -0.0033550928 -0.0033598258 -0.003366305 -0.0033671439 -0.0033697896 -0.0033723977][-0.0033729 -0.0033682936 -0.0033556989 -0.00332772 -0.003285083 -0.0032347473 -0.003205603 -0.0032033944 -0.0032204334 -0.0032533812 -0.0032996105 -0.0033449831 -0.0033663355 -0.0033710718 -0.0033732667][-0.0032736196 -0.0031589272 -0.0030127645 -0.0028522403 -0.0027150249 -0.0026179987 -0.0026175154 -0.002726041 -0.002880655 -0.0030464483 -0.0031929133 -0.0033108685 -0.0033641232 -0.0033733682 -0.0033753333][-0.0024738265 -0.0017652493 -0.00095175579 -0.00016041915 0.00039528403 0.00061378512 0.00038287602 -0.00024944753 -0.0010651234 -0.0018829382 -0.0025476306 -0.0030364373 -0.0032825861 -0.0033655 -0.0033764814][-0.00040400797 0.0016878315 0.0040187659 0.0062323883 0.0077617476 0.0082966564 0.0075662825 0.0057875589 0.0034809595 0.0011488788 -0.00080448808 -0.0022592004 -0.0030330191 -0.0033329092 -0.0033743952][0.0029594761 0.0071432842 0.011716729 0.016045481 0.019111475 0.020248592 0.018882344 0.015466122 0.010943588 0.0062606744 0.0021930558 -0.00090987887 -0.0026059917 -0.0032776424 -0.0033674482][0.0063746879 0.012568189 0.019255525 0.025595952 0.030189475 0.032003675 0.030120784 0.025207315 0.018593779 0.011604072 0.0053643277 0.00051696482 -0.0021576257 -0.0032202299 -0.0033593548][0.00836698 0.015680265 0.023510629 0.030957857 0.036461487 0.03875472 0.036678273 0.03102568 0.023319986 0.015020456 0.007442141 0.0014536718 -0.0018610971 -0.0031831863 -0.0033537671][0.007994012 0.01499786 0.022455804 0.029593015 0.034983471 0.037358768 0.035531379 0.030268067 0.023005774 0.015016675 0.0075399047 0.0015157408 -0.0018418052 -0.0031823718 -0.0033523957][0.0055092042 0.010944401 0.0166918 0.022229254 0.026495909 0.028466577 0.027160563 0.023187581 0.017667327 0.011467328 0.0055145845 0.00061509828 -0.0021313846 -0.0032219102 -0.0033564575][0.0020588546 0.0053844331 0.00886728 0.012248833 0.014901279 0.01616706 0.015411986 0.013025926 0.0097278329 0.0059553552 0.0022487363 -0.000859702 -0.0026023602 -0.0032831572 -0.0033631367][-0.00098237186 0.0005152279 0.0020605607 0.0035734528 0.004773288 0.0053489003 0.0050015133 0.0039240783 0.0024789001 0.00080686645 -0.00085712108 -0.0022671833 -0.0030432085 -0.0033379886 -0.0033705041][-0.0027163527 -0.0022605848 -0.0017972882 -0.0013365566 -0.00096636126 -0.00079227751 -0.00090920366 -0.0012481345 -0.0016737583 -0.0021691672 -0.0026589164 -0.0030730602 -0.0032905322 -0.0033688012 -0.003376191][-0.0033194623 -0.0032579841 -0.0031964451 -0.0031276369 -0.0030701258 -0.0030425675 -0.0030604615 -0.0031131583 -0.0031705352 -0.003238637 -0.0033001187 -0.0033495582 -0.0033717947 -0.0033775805 -0.0033774704][-0.0033797473 -0.0033775205 -0.0033752481 -0.0033718843 -0.0033696939 -0.0033691395 -0.0033707095 -0.0033732061 -0.0033765379 -0.0033791512 -0.00338003 -0.0033797105 -0.0033790898 -0.0033783093 -0.0033780751]]...]
INFO - root - 2017-12-09 20:51:25.147141: step 58810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:44m:42s remains)
INFO - root - 2017-12-09 20:51:33.545595: step 58820, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 63h:51m:03s remains)
INFO - root - 2017-12-09 20:51:42.100245: step 58830, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:37m:22s remains)
INFO - root - 2017-12-09 20:51:50.770057: step 58840, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 67h:06m:05s remains)
INFO - root - 2017-12-09 20:51:59.430675: step 58850, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:15m:01s remains)
INFO - root - 2017-12-09 20:52:07.994543: step 58860, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 65h:35m:05s remains)
INFO - root - 2017-12-09 20:52:16.476266: step 58870, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 68h:01m:58s remains)
INFO - root - 2017-12-09 20:52:24.951024: step 58880, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 66h:14m:06s remains)
INFO - root - 2017-12-09 20:52:33.507887: step 58890, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 65h:10m:16s remains)
INFO - root - 2017-12-09 20:52:42.043489: step 58900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 65h:21m:47s remains)
2017-12-09 20:52:42.893020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003393793 -0.0033913187 -0.0033910526 -0.0033905802 -0.0033900298 -0.0033897897 -0.0033897697 -0.0033902158 -0.0033907609 -0.0033914442 -0.0033918994 -0.00339162 -0.0033913131 -0.0033908628 -0.0033903755][-0.0033950715 -0.003392762 -0.0033923937 -0.003391681 -0.0033907038 -0.0033899485 -0.0033894011 -0.0033891841 -0.003389318 -0.0033898184 -0.0033904824 -0.0033907413 -0.0033908403 -0.0033905071 -0.0033898447][-0.0033981532 -0.0033960238 -0.0033955427 -0.0033945523 -0.0033931052 -0.003391644 -0.0033903928 -0.0033891595 -0.0033885059 -0.0033891955 -0.0033907916 -0.0033917534 -0.0033922268 -0.0033920126 -0.0033910584][-0.0033979451 -0.0033919855 -0.0033879275 -0.003385524 -0.0033857243 -0.0033879424 -0.0033892803 -0.003388142 -0.0033866039 -0.0033873587 -0.0033895068 -0.0033912247 -0.0033927311 -0.003392898 -0.0033917953][-0.0033891483 -0.0033753614 -0.0033641981 -0.0033577387 -0.0033570414 -0.0033608105 -0.0033663677 -0.0033706445 -0.0033735705 -0.003375621 -0.0033767396 -0.0033784236 -0.0033827503 -0.0033873566 -0.0033899059][-0.0033665588 -0.003334993 -0.003308221 -0.0032964337 -0.0033023767 -0.0033182271 -0.003331325 -0.0033334396 -0.0033236404 -0.0033066112 -0.0032921734 -0.0032932365 -0.0033157831 -0.0033489559 -0.0033756229][-0.0033120678 -0.0032514115 -0.0032024086 -0.0031826068 -0.0031933892 -0.0032191188 -0.0032405809 -0.0032388733 -0.0032032777 -0.0031398835 -0.0030804458 -0.0030713822 -0.0031324041 -0.0032344486 -0.003324725][-0.0032297175 -0.0031105038 -0.0030048338 -0.0029508586 -0.0029547089 -0.0029889494 -0.0030114474 -0.002985826 -0.0028996007 -0.0027732279 -0.0026703349 -0.0026708841 -0.0028055641 -0.0030207508 -0.0032197372][-0.0030833432 -0.0028668183 -0.0026637192 -0.0025379506 -0.0025077511 -0.0025411975 -0.0025751884 -0.0025554029 -0.0024593172 -0.0023133645 -0.00220337 -0.0022320859 -0.0024438542 -0.0027722269 -0.0030853753][-0.002909669 -0.0025686123 -0.0022372035 -0.00201417 -0.0019392276 -0.0019742381 -0.0020335563 -0.0020492529 -0.0019986411 -0.0019122078 -0.0018744958 -0.0019781389 -0.002258623 -0.0026492523 -0.0030167019][-0.002831449 -0.0024157492 -0.0019980608 -0.0017032139 -0.0015900559 -0.0016200659 -0.0016993867 -0.0017555191 -0.0017707285 -0.0017787949 -0.0018435471 -0.0020236506 -0.0023339805 -0.0027142891 -0.0030547502][-0.0029497275 -0.0025743833 -0.0021725232 -0.0018671349 -0.0017336321 -0.0017483542 -0.0018304336 -0.0019157684 -0.0019900075 -0.0020750887 -0.0022029106 -0.0023970446 -0.0026565907 -0.002938706 -0.0031755641][-0.0031510871 -0.0029115607 -0.0026371526 -0.0024115602 -0.0022979225 -0.0022935851 -0.0023488384 -0.0024220815 -0.0025035581 -0.002601254 -0.0027209879 -0.0028646686 -0.0030246591 -0.0031784459 -0.0032978307][-0.0033057877 -0.0031985925 -0.0030687766 -0.0029549284 -0.0028911142 -0.0028815689 -0.0029069278 -0.0029505682 -0.0030067773 -0.00307291 -0.00314418 -0.0032150031 -0.0032805162 -0.0033354077 -0.0033738057][-0.0033809803 -0.0033504851 -0.003312136 -0.0032768133 -0.0032557435 -0.0032508755 -0.0032574537 -0.0032718349 -0.003292327 -0.0033164171 -0.0033403551 -0.0033615124 -0.0033788881 -0.0033920147 -0.0033999986]]...]
INFO - root - 2017-12-09 20:52:51.124773: step 58910, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:21m:46s remains)
INFO - root - 2017-12-09 20:52:59.661630: step 58920, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 65h:31m:55s remains)
INFO - root - 2017-12-09 20:53:08.293144: step 58930, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 65h:33m:26s remains)
INFO - root - 2017-12-09 20:53:16.765763: step 58940, loss = 0.91, batch loss = 0.70 (9.9 examples/sec; 0.811 sec/batch; 61h:37m:03s remains)
INFO - root - 2017-12-09 20:53:25.276932: step 58950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:40m:48s remains)
INFO - root - 2017-12-09 20:53:33.904564: step 58960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:46m:59s remains)
INFO - root - 2017-12-09 20:53:42.649758: step 58970, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.736 sec/batch; 55h:53m:38s remains)
INFO - root - 2017-12-09 20:53:51.254893: step 58980, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 67h:18m:02s remains)
INFO - root - 2017-12-09 20:53:59.874945: step 58990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 67h:28m:03s remains)
INFO - root - 2017-12-09 20:54:08.486192: step 59000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:35m:07s remains)
2017-12-09 20:54:09.382536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033161812 -0.0033840735 -0.00340153 -0.0034062243 -0.0034085908 -0.0034096336 -0.0034099433 -0.003409452 -0.0034084017 -0.0034074562 -0.0034049444 -0.0034018746 -0.0033989022 -0.0033970694 -0.0033958138][-0.0033647246 -0.0033930507 -0.0034002445 -0.0034039943 -0.0034065736 -0.0034083393 -0.00340977 -0.0034105172 -0.0034105703 -0.0034099724 -0.003407802 -0.0034050518 -0.0034017942 -0.0033991835 -0.0033958824][-0.0033934203 -0.0033997898 -0.0034013658 -0.003403516 -0.0034054378 -0.0034077412 -0.0034102248 -0.0034124942 -0.0034138369 -0.0034140721 -0.0034117906 -0.0034066397 -0.0034008028 -0.0034006105 -0.0034015607][-0.0033951683 -0.0034030636 -0.0034027055 -0.0034032187 -0.003404056 -0.0034061854 -0.0034094397 -0.0034128516 -0.0034155464 -0.0034169757 -0.0034158777 -0.003411826 -0.0034061864 -0.0034047703 -0.0034055091][-0.0033511214 -0.0033577378 -0.0033739647 -0.0033935248 -0.0034000599 -0.0034033377 -0.0034066206 -0.0034104548 -0.0034142332 -0.0034167811 -0.003417091 -0.0034149315 -0.0034106725 -0.0034046087 -0.0033995942][-0.0033220325 -0.0033476513 -0.0033441486 -0.0033416022 -0.0033428662 -0.0033601085 -0.0033819762 -0.0033998697 -0.0034085042 -0.0034135587 -0.0034148837 -0.0034143969 -0.003412443 -0.0034090141 -0.0034058357][-0.0031115848 -0.0031200396 -0.003130655 -0.0031484263 -0.0031539921 -0.003174142 -0.003228327 -0.0033154895 -0.0033799941 -0.00340813 -0.0034110309 -0.0034117573 -0.0034111047 -0.0034103664 -0.0034087356][-0.0028048805 -0.0027367282 -0.0026786379 -0.0026429691 -0.002655111 -0.0027447923 -0.002925355 -0.0031561628 -0.0033288696 -0.0034016618 -0.0034067715 -0.0034086504 -0.0034089326 -0.003409263 -0.0034095203][-0.00227731 -0.0021834343 -0.0021502515 -0.0020977044 -0.0021074284 -0.0022388988 -0.0025469789 -0.0029510427 -0.0032603357 -0.0033919446 -0.0034020098 -0.0034048015 -0.0034058688 -0.0034067533 -0.0034083535][-0.0016593622 -0.001589854 -0.001645257 -0.0016606848 -0.0017310012 -0.0019317728 -0.0023269486 -0.0028165434 -0.003203423 -0.0033784083 -0.0033984855 -0.0034013542 -0.0034027842 -0.0034042019 -0.0034062469][-0.0013260834 -0.0012697249 -0.0014450669 -0.0015548985 -0.0016899287 -0.0019154326 -0.0023152689 -0.0028050169 -0.0031924627 -0.0033727223 -0.0033964771 -0.003399153 -0.0034004606 -0.0034018352 -0.0034032848][-0.0014088266 -0.0013615857 -0.0016173603 -0.0018137563 -0.0020008986 -0.0022090254 -0.0025347641 -0.0029261769 -0.0032362719 -0.0033780939 -0.0033960079 -0.0033983502 -0.0033990496 -0.0033997893 -0.003400509][-0.0017614304 -0.001757971 -0.0020376833 -0.0022914591 -0.0025090929 -0.002679432 -0.0028935932 -0.0031306439 -0.003311703 -0.003389549 -0.0033975781 -0.0033991376 -0.0033991602 -0.0033992776 -0.0033991965][-0.0022923436 -0.0023408851 -0.0025722466 -0.0027813886 -0.0029494779 -0.003068245 -0.0031862811 -0.0032930502 -0.0033685684 -0.0033981749 -0.0034001709 -0.0034007665 -0.0033991528 -0.0033965358 -0.0033954829][-0.0028412184 -0.0029051031 -0.0030441699 -0.0031628124 -0.0032510853 -0.0033061174 -0.0033471242 -0.0033766967 -0.0033954389 -0.00340219 -0.0034016487 -0.0033998212 -0.0033931066 -0.0033791207 -0.0033598382]]...]
INFO - root - 2017-12-09 20:54:17.749299: step 59010, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:57m:13s remains)
INFO - root - 2017-12-09 20:54:26.410246: step 59020, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:53m:07s remains)
INFO - root - 2017-12-09 20:54:35.161133: step 59030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:22m:47s remains)
INFO - root - 2017-12-09 20:54:43.878237: step 59040, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 65h:12m:38s remains)
INFO - root - 2017-12-09 20:54:52.638189: step 59050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:32m:44s remains)
INFO - root - 2017-12-09 20:55:01.337302: step 59060, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:45m:40s remains)
INFO - root - 2017-12-09 20:55:09.912056: step 59070, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 57h:19m:29s remains)
INFO - root - 2017-12-09 20:55:18.460902: step 59080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:16m:59s remains)
INFO - root - 2017-12-09 20:55:27.143911: step 59090, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:56m:55s remains)
INFO - root - 2017-12-09 20:55:35.855378: step 59100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 66h:44m:22s remains)
2017-12-09 20:55:36.703368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034024285 -0.0034024294 -0.0034021619 -0.0034008287 -0.0033995428 -0.003399343 -0.0033987777 -0.0033974834 -0.0033971081 -0.0033980478 -0.0033997295 -0.0034004708 -0.003400987 -0.0034005286 -0.003398153][-0.0034006627 -0.0034010068 -0.0034010275 -0.0033999006 -0.0033983435 -0.0033973819 -0.0033967949 -0.003395892 -0.0033956713 -0.0033963211 -0.0033976948 -0.0033982897 -0.0033986683 -0.0033981041 -0.0033960494][-0.0033992878 -0.0033997013 -0.0034000676 -0.0033991965 -0.003397844 -0.0033964955 -0.003395702 -0.00339504 -0.003394735 -0.0033951446 -0.0033959325 -0.0033956366 -0.0033956675 -0.0033957472 -0.0033948384][-0.0033958578 -0.0033959874 -0.0033967164 -0.0033965143 -0.0033957832 -0.0033950782 -0.0033947295 -0.0033946449 -0.0033944747 -0.0033945807 -0.0033944172 -0.0033933797 -0.0033928941 -0.0033930505 -0.0033929476][-0.0033915667 -0.0033917106 -0.0033928221 -0.0033937551 -0.0033937746 -0.00339375 -0.0033938412 -0.0033940517 -0.0033937022 -0.00339321 -0.0033928792 -0.0033917371 -0.0033911422 -0.0033911881 -0.0033913318][-0.0033873983 -0.0033881143 -0.0033901439 -0.0033920542 -0.0033935953 -0.0033946873 -0.0033954168 -0.0033956713 -0.0033948144 -0.0033933735 -0.0033927234 -0.0033917902 -0.0033915581 -0.0033917946 -0.0033919485][-0.0033837629 -0.0033847135 -0.0033877336 -0.0033906456 -0.0033931248 -0.0033953895 -0.0033975381 -0.0033984026 -0.0033972466 -0.003394678 -0.00339329 -0.0033924791 -0.003391858 -0.0033920181 -0.0033923287][-0.0033813755 -0.0033822246 -0.0033857254 -0.0033893085 -0.0033923013 -0.0033954142 -0.003398309 -0.0033999451 -0.0033989113 -0.0033961064 -0.0033940284 -0.0033926109 -0.0033914088 -0.0033914223 -0.0033914326][-0.0033799291 -0.0033804993 -0.0033838698 -0.0033874824 -0.003390617 -0.0033938291 -0.003397282 -0.0033995479 -0.0033992685 -0.0033971532 -0.0033947236 -0.0033925518 -0.0033910957 -0.0033911103 -0.0033909525][-0.0033793852 -0.0033795333 -0.0033827517 -0.0033864973 -0.0033898605 -0.0033932172 -0.0033968787 -0.0033992059 -0.0033991756 -0.0033972221 -0.0033948317 -0.003392084 -0.0033902267 -0.0033905387 -0.0033910293][-0.0033796527 -0.0033793552 -0.003382127 -0.0033859776 -0.0033894014 -0.0033927816 -0.0033962524 -0.0033982564 -0.0033984734 -0.0033968037 -0.0033944256 -0.003391433 -0.0033894847 -0.0033901273 -0.0033911762][-0.0033807764 -0.00337992 -0.0033824397 -0.0033860041 -0.0033891231 -0.0033922405 -0.0033949804 -0.0033964636 -0.0033960445 -0.0033944396 -0.0033927727 -0.0033905425 -0.0033889967 -0.0033896097 -0.0033909371][-0.0033831995 -0.0033822889 -0.0033843808 -0.0033871788 -0.0033896691 -0.0033921439 -0.0033939064 -0.0033944866 -0.0033938896 -0.0033927455 -0.003392217 -0.00339123 -0.0033905348 -0.0033910943 -0.0033920892][-0.0033859494 -0.0033850824 -0.003386871 -0.0033891341 -0.0033911667 -0.0033927539 -0.0033936775 -0.0033936696 -0.003392797 -0.0033916247 -0.0033915795 -0.0033914484 -0.0033913963 -0.0033921583 -0.00339309][-0.0033888149 -0.0033879187 -0.0033892088 -0.0033906745 -0.0033921672 -0.0033928838 -0.0033931208 -0.003392953 -0.0033921646 -0.0033910507 -0.0033912943 -0.0033917367 -0.0033922889 -0.0033937811 -0.003394692]]...]
INFO - root - 2017-12-09 20:55:45.235705: step 59110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:20m:08s remains)
INFO - root - 2017-12-09 20:55:53.902173: step 59120, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:02m:25s remains)
INFO - root - 2017-12-09 20:56:02.366325: step 59130, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 63h:13m:45s remains)
INFO - root - 2017-12-09 20:56:10.732994: step 59140, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 65h:19m:27s remains)
INFO - root - 2017-12-09 20:56:19.423025: step 59150, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:01m:12s remains)
INFO - root - 2017-12-09 20:56:28.000767: step 59160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:26m:19s remains)
INFO - root - 2017-12-09 20:56:36.467921: step 59170, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 49h:50m:20s remains)
INFO - root - 2017-12-09 20:56:45.036723: step 59180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:21m:47s remains)
INFO - root - 2017-12-09 20:56:53.833562: step 59190, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 67h:40m:44s remains)
INFO - root - 2017-12-09 20:57:02.558167: step 59200, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:58m:32s remains)
2017-12-09 20:57:03.432988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034024934 -0.0034009854 -0.0034007141 -0.003400503 -0.003400275 -0.0034000373 -0.0033999472 -0.003399923 -0.0034003211 -0.0034010753 -0.0034023684 -0.0034037356 -0.0034047856 -0.0034058294 -0.0034068357][-0.0034017365 -0.0034002531 -0.003400095 -0.0034000911 -0.0033999556 -0.0033996664 -0.0033996117 -0.0033996936 -0.0034002315 -0.0034014254 -0.0034029849 -0.0034043908 -0.0034054173 -0.0034063116 -0.0034071028][-0.0034023079 -0.0034010366 -0.0034010964 -0.003401418 -0.0034014708 -0.0034014394 -0.0034015777 -0.0034019062 -0.0034026657 -0.0034040504 -0.0034056858 -0.0034069782 -0.003407601 -0.0034079279 -0.0034079645][-0.0034024718 -0.0034014087 -0.0034017155 -0.0034025765 -0.0034030729 -0.00340368 -0.0034043968 -0.0034051768 -0.003406299 -0.00340798 -0.003409541 -0.0034101577 -0.003409663 -0.0034087528 -0.0034073498][-0.0034023419 -0.0034014115 -0.0034019391 -0.0034032515 -0.003404377 -0.0034055931 -0.0034066923 -0.0034079233 -0.0034095184 -0.0034114183 -0.0034127941 -0.0034126879 -0.0034110711 -0.0034086739 -0.0034055498][-0.0034023735 -0.00340166 -0.0034023323 -0.0034038315 -0.0034052269 -0.0034067193 -0.0034081491 -0.0034099577 -0.003412104 -0.0034143406 -0.0034155913 -0.0034148782 -0.0034123193 -0.0034087377 -0.0034045798][-0.0034023121 -0.0034014382 -0.0034022713 -0.0034040869 -0.0034058925 -0.0034076201 -0.0034096525 -0.0034123373 -0.0034150272 -0.0034171538 -0.0034179378 -0.00341667 -0.0034135045 -0.0034093549 -0.0034048783][-0.0034021966 -0.0034012438 -0.0034020985 -0.0034042282 -0.0034065598 -0.0034089948 -0.0034120085 -0.0034155534 -0.00341838 -0.0034199271 -0.0034200773 -0.0034181825 -0.0034144497 -0.0034099345 -0.0034055151][-0.0034021921 -0.0034010753 -0.0034020063 -0.0034045135 -0.0034073044 -0.003410338 -0.0034139305 -0.0034176225 -0.0034199317 -0.0034206479 -0.0034203997 -0.0034183641 -0.0034146488 -0.0034103282 -0.0034062339][-0.0034017516 -0.0034007577 -0.0034020022 -0.0034045307 -0.0034074667 -0.0034105727 -0.0034138286 -0.0034166879 -0.003418138 -0.0034185569 -0.0034183676 -0.003417013 -0.0034141068 -0.0034104728 -0.0034069121][-0.0034011719 -0.0034002268 -0.0034016513 -0.0034042124 -0.0034068841 -0.0034095836 -0.0034122686 -0.003414382 -0.0034153576 -0.00341568 -0.0034160293 -0.0034156849 -0.0034135755 -0.0034106427 -0.0034079633][-0.0034003083 -0.0033992312 -0.003400451 -0.0034027072 -0.0034050252 -0.0034071663 -0.0034094355 -0.0034111959 -0.0034121131 -0.003412602 -0.0034132493 -0.0034135198 -0.003411853 -0.0034096234 -0.0034079039][-0.0033996967 -0.0033982045 -0.003398905 -0.003400323 -0.0034019167 -0.0034036431 -0.0034055144 -0.0034070686 -0.0034081712 -0.0034091682 -0.003410297 -0.0034110595 -0.003410208 -0.0034087824 -0.0034079344][-0.0033988433 -0.0033969546 -0.0033972543 -0.0033979435 -0.0033987814 -0.0033998319 -0.0034011323 -0.0034023351 -0.0034033828 -0.0034046501 -0.0034063747 -0.0034080669 -0.0034084395 -0.0034083074 -0.0034084071][-0.0033979726 -0.0033956987 -0.0033955311 -0.0033956079 -0.003395787 -0.0033961071 -0.0033967716 -0.0033976028 -0.0033986531 -0.003400326 -0.0034028897 -0.0034055107 -0.0034070539 -0.0034082092 -0.0034090804]]...]
INFO - root - 2017-12-09 20:57:11.804045: step 59210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:49m:47s remains)
INFO - root - 2017-12-09 20:57:20.483445: step 59220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:27m:08s remains)
INFO - root - 2017-12-09 20:57:29.078195: step 59230, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 62h:45m:35s remains)
INFO - root - 2017-12-09 20:57:37.512117: step 59240, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 62h:34m:10s remains)
INFO - root - 2017-12-09 20:57:45.917615: step 59250, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:21m:53s remains)
INFO - root - 2017-12-09 20:57:54.467329: step 59260, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 63h:10m:49s remains)
INFO - root - 2017-12-09 20:58:03.097980: step 59270, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:14m:01s remains)
INFO - root - 2017-12-09 20:58:11.573544: step 59280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:48m:53s remains)
INFO - root - 2017-12-09 20:58:20.214070: step 59290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:47m:24s remains)
INFO - root - 2017-12-09 20:58:28.823761: step 59300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:42m:42s remains)
2017-12-09 20:58:29.778691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034069242 -0.0034065663 -0.0034068609 -0.0034069016 -0.0034067582 -0.0034066916 -0.0034068 -0.0034066532 -0.003406286 -0.0034060059 -0.0034058588 -0.0034057987 -0.0034057687 -0.0034058262 -0.0034059505][-0.0034049542 -0.0034046904 -0.0034051926 -0.0034053903 -0.0034052765 -0.0034054455 -0.0034054 -0.0034048962 -0.0034042746 -0.0034038697 -0.0034037167 -0.0034036248 -0.0034035963 -0.0034036182 -0.0034037305][-0.0034055922 -0.0034056494 -0.0034062322 -0.0034065428 -0.00340637 -0.0034065128 -0.003406106 -0.0034049607 -0.0034039591 -0.0034035805 -0.0034036348 -0.0034036823 -0.0034036385 -0.0034036827 -0.0034037924][-0.0034064471 -0.0034069885 -0.0034076327 -0.0034076374 -0.0034072113 -0.0034066113 -0.0034053528 -0.0034033831 -0.0034022515 -0.0034024196 -0.0034032876 -0.0034037484 -0.0034037638 -0.0034037945 -0.0034039509][-0.0034073212 -0.0034082376 -0.003408975 -0.0034085044 -0.0034067987 -0.003404387 -0.0034011353 -0.0033981965 -0.0033976869 -0.0033995067 -0.003401696 -0.0034028722 -0.0034035756 -0.0034039409 -0.0034040811][-0.0034083396 -0.0034094669 -0.0034103149 -0.0034093254 -0.0034057598 -0.0034001414 -0.0033929863 -0.0033887019 -0.0033899324 -0.0033945823 -0.0033987076 -0.0034010618 -0.0034028876 -0.0034038813 -0.0034040855][-0.0034088411 -0.0034101713 -0.0034113098 -0.0034101394 -0.0034053163 -0.0033969097 -0.0033851194 -0.0033789845 -0.0033822041 -0.0033891951 -0.0033951679 -0.0033994839 -0.0034022995 -0.0034037798 -0.0034041319][-0.0034091026 -0.0034105938 -0.0034118623 -0.0034105272 -0.0034057708 -0.0033967427 -0.0033838416 -0.0033770935 -0.0033802022 -0.0033871515 -0.0033940035 -0.003399417 -0.0034026157 -0.003403923 -0.0034041726][-0.0034091068 -0.0034106353 -0.0034120667 -0.0034110977 -0.0034072185 -0.0034000464 -0.0033909327 -0.0033854963 -0.0033867471 -0.0033911688 -0.0033964394 -0.0034010515 -0.0034036771 -0.0034046015 -0.0034044515][-0.0034087019 -0.0034103571 -0.0034117997 -0.0034114362 -0.0034091731 -0.003404991 -0.0033998836 -0.0033966817 -0.0033969847 -0.0033986687 -0.0034013495 -0.0034039428 -0.0034049992 -0.0034051319 -0.0034048138][-0.0034082481 -0.0034098744 -0.0034114548 -0.0034122146 -0.0034117117 -0.0034099391 -0.0034075207 -0.0034060474 -0.00340588 -0.003405445 -0.0034058378 -0.0034063056 -0.0034060392 -0.0034055354 -0.0034049947][-0.0034071845 -0.0034085349 -0.003410399 -0.0034121778 -0.0034129734 -0.00341284 -0.0034121722 -0.0034115897 -0.0034105889 -0.0034093533 -0.003408296 -0.0034072397 -0.0034061975 -0.0034054406 -0.0034048581][-0.0034061223 -0.003406836 -0.0034085463 -0.0034106539 -0.0034121047 -0.0034127049 -0.0034129294 -0.003412643 -0.0034117145 -0.0034101207 -0.003408592 -0.0034070939 -0.0034059151 -0.0034050806 -0.0034045465][-0.0034052148 -0.0034052965 -0.0034065624 -0.0034081349 -0.0034095147 -0.003410313 -0.0034107647 -0.0034105482 -0.0034097319 -0.0034083615 -0.003407086 -0.0034059267 -0.003405079 -0.0034045069 -0.0034041633][-0.0034043521 -0.0034040622 -0.0034047246 -0.0034056129 -0.0034064814 -0.003406954 -0.0034071875 -0.0034070893 -0.0034066972 -0.0034059363 -0.0034051847 -0.0034046024 -0.0034042159 -0.0034039218 -0.0034037516]]...]
INFO - root - 2017-12-09 20:58:38.129793: step 59310, loss = 0.89, batch loss = 0.69 (11.0 examples/sec; 0.726 sec/batch; 55h:05m:55s remains)
INFO - root - 2017-12-09 20:58:46.804603: step 59320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:03m:03s remains)
INFO - root - 2017-12-09 20:58:55.442651: step 59330, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 64h:45m:43s remains)
INFO - root - 2017-12-09 20:59:04.017139: step 59340, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:35m:50s remains)
INFO - root - 2017-12-09 20:59:12.740246: step 59350, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 67h:53m:22s remains)
INFO - root - 2017-12-09 20:59:21.422049: step 59360, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.838 sec/batch; 63h:32m:57s remains)
INFO - root - 2017-12-09 20:59:30.084967: step 59370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:18m:44s remains)
INFO - root - 2017-12-09 20:59:38.478571: step 59380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:42m:27s remains)
INFO - root - 2017-12-09 20:59:47.114201: step 59390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:18m:07s remains)
INFO - root - 2017-12-09 20:59:55.900876: step 59400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:55m:20s remains)
2017-12-09 20:59:56.739260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003349148 -0.0033449146 -0.0033441056 -0.0033436134 -0.00334324 -0.0033433619 -0.0033435428 -0.003343777 -0.0033440068 -0.0033444718 -0.0033447004 -0.0033446867 -0.0033445251 -0.0033442706 -0.0033438597][-0.0033463393 -0.0033416769 -0.0033408224 -0.0033403353 -0.0033399253 -0.0033399693 -0.0033401377 -0.0033405111 -0.0033408885 -0.0033413086 -0.0033414776 -0.0033415442 -0.0033414848 -0.0033414394 -0.0033413612][-0.0033442213 -0.003339367 -0.003338743 -0.0033384459 -0.0033381435 -0.0033381181 -0.0033382294 -0.0033385225 -0.0033388222 -0.003339191 -0.0033394131 -0.0033396045 -0.0033397127 -0.0033399407 -0.0033401796][-0.0033418476 -0.0033369223 -0.0033362664 -0.0033361691 -0.0033361 -0.0033361001 -0.0033361509 -0.0033362887 -0.0033363879 -0.00333659 -0.0033367458 -0.0033369814 -0.0033371549 -0.0033375414 -0.003337922][-0.0033394815 -0.0033345239 -0.003333695 -0.0033334671 -0.0033334389 -0.0033333944 -0.00333336 -0.0033333427 -0.0033333227 -0.0033334151 -0.00333346 -0.0033336452 -0.0033338347 -0.003334366 -0.0033348908][-0.0033380173 -0.0033331171 -0.0033321502 -0.003331695 -0.00333159 -0.0033314936 -0.0033314622 -0.003331373 -0.0033312393 -0.0033311122 -0.0033308945 -0.0033307991 -0.0033308063 -0.0033312587 -0.0033317807][-0.0033375956 -0.0033326421 -0.0033314542 -0.0033306468 -0.0033302645 -0.0033300351 -0.0033300256 -0.0033299548 -0.0033299513 -0.003329786 -0.0033294153 -0.0033290833 -0.003328969 -0.0033291955 -0.0033295541][-0.0033384622 -0.0033335169 -0.0033321604 -0.0033310081 -0.0033304088 -0.003329928 -0.0033297478 -0.0033296428 -0.0033298156 -0.0033298221 -0.0033294994 -0.0033290046 -0.0033286361 -0.0033284843 -0.0033284305][-0.0033408448 -0.003335888 -0.0033344582 -0.00333304 -0.0033322687 -0.0033317404 -0.003331528 -0.0033314007 -0.0033316619 -0.0033316342 -0.0033311604 -0.0033304614 -0.0033298202 -0.0033292556 -0.0033288442][-0.0033458734 -0.0033408233 -0.0033393344 -0.0033376529 -0.0033366722 -0.0033360275 -0.0033357989 -0.0033356992 -0.0033358878 -0.0033357823 -0.0033351067 -0.0033341737 -0.0033332198 -0.0033321877 -0.0033313665][-0.0033550027 -0.0033496439 -0.0033479417 -0.0033460006 -0.0033446085 -0.0033436974 -0.003343327 -0.0033431931 -0.0033433386 -0.0033432166 -0.0033425251 -0.0033414648 -0.0033402212 -0.003338729 -0.0033374194][-0.0033672901 -0.0033619574 -0.003360189 -0.0033582847 -0.003356864 -0.0033558009 -0.0033553252 -0.003355084 -0.0033550274 -0.0033547389 -0.0033538635 -0.0033525981 -0.0033510523 -0.0033492974 -0.0033477519][-0.0033778953 -0.0033729379 -0.0033714047 -0.0033699612 -0.0033690478 -0.0033682198 -0.0033677467 -0.0033675078 -0.0033673742 -0.0033670166 -0.0033661826 -0.0033650126 -0.0033635807 -0.0033620265 -0.0033606915][-0.0033844381 -0.003380165 -0.0033790946 -0.0033783973 -0.0033780914 -0.0033775698 -0.0033773156 -0.0033770455 -0.0033768155 -0.0033763526 -0.0033754848 -0.0033744338 -0.0033732879 -0.0033721128 -0.0033710883][-0.0033870323 -0.0033834712 -0.0033828013 -0.0033828779 -0.0033830921 -0.0033830081 -0.0033830178 -0.0033827496 -0.0033823871 -0.0033817946 -0.0033809436 -0.0033799116 -0.0033788574 -0.0033778723 -0.0033770292]]...]
INFO - root - 2017-12-09 21:00:05.106707: step 59410, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.711 sec/batch; 53h:53m:58s remains)
INFO - root - 2017-12-09 21:00:13.668979: step 59420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:13m:08s remains)
INFO - root - 2017-12-09 21:00:22.088034: step 59430, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 65h:48m:36s remains)
INFO - root - 2017-12-09 21:00:30.688389: step 59440, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 65h:47m:05s remains)
INFO - root - 2017-12-09 21:00:39.318939: step 59450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:22m:44s remains)
INFO - root - 2017-12-09 21:00:47.840774: step 59460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:45m:06s remains)
INFO - root - 2017-12-09 21:00:56.306230: step 59470, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 64h:05m:34s remains)
INFO - root - 2017-12-09 21:01:04.645850: step 59480, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.904 sec/batch; 68h:32m:50s remains)
INFO - root - 2017-12-09 21:01:13.348477: step 59490, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:23m:40s remains)
INFO - root - 2017-12-09 21:01:21.941563: step 59500, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 66h:01m:50s remains)
2017-12-09 21:01:22.808268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034021644 -0.0034005535 -0.003391854 -0.003353761 -0.0031932485 -0.0028571079 -0.0024056437 -0.0019897958 -0.0017656488 -0.0018273015 -0.0021426538 -0.0025683993 -0.0029555662 -0.0032175402 -0.0033492828][-0.0034005849 -0.0033959625 -0.0033679449 -0.0032682733 -0.0029759484 -0.0024538171 -0.0018218458 -0.0012974681 -0.0010758943 -0.0012236715 -0.0016870198 -0.0022751549 -0.0027914718 -0.00313504 -0.0033125642][-0.0033862637 -0.0033613346 -0.0032744056 -0.0030382052 -0.0025335283 -0.0017457075 -0.00085285213 -0.00016215187 6.9058267e-05 -0.0002145383 -0.00090705883 -0.0017566392 -0.0025000526 -0.0029993937 -0.0032622751][-0.0033065134 -0.0031905903 -0.0029204478 -0.002361923 -0.0014043553 -8.9192763e-05 0.0012907146 0.0022923492 0.0025666526 0.0020464978 0.000903595 -0.00048983539 -0.0017353175 -0.0026116297 -0.0031064146][-0.0030294503 -0.0026472607 -0.0019088958 -0.00064357067 0.001217087 0.0035293496 0.0057799378 0.0073189419 0.0076587563 0.0067107147 0.0047409385 0.002309147 5.300995e-05 -0.0016378674 -0.0026754651][-0.0023710085 -0.0014523072 0.00017614732 0.0027147627 0.0061644297 0.010172247 0.013892872 0.01634495 0.016787833 0.015087957 0.011659799 0.0074117016 0.0033655928 0.00021168264 -0.0018239567][-0.0012437499 0.00043308828 0.0032774608 0.0074579269 0.012858998 0.018856511 0.024234449 0.027689207 0.028232371 0.025655378 0.020493731 0.014025515 0.0077350037 0.0027085079 -0.00064137485][4.8977323e-05 0.0024219283 0.0063144937 0.011819411 0.018709486 0.026170511 0.032742709 0.03691965 0.037517894 0.034266628 0.027752325 0.019503446 0.011370905 0.0047992589 0.00036427868][0.000825397 0.0034449713 0.0076415762 0.013446024 0.020643264 0.028418997 0.03532052 0.039793689 0.040534839 0.037192922 0.030289153 0.021425467 0.012607953 0.0054670516 0.000656361][0.00057364325 0.0028662335 0.0065122461 0.011529339 0.017801756 0.024689544 0.030965021 0.035192657 0.036079545 0.033204906 0.026989568 0.018909788 0.010862175 0.0043831225 7.9547288e-05][-0.00062020752 0.0010324002 0.003663386 0.0073159188 0.011981397 0.017226323 0.022141321 0.025565267 0.026389012 0.024221236 0.019403499 0.01315321 0.006995854 0.0021162515 -0.001035444][-0.0019825127 -0.0010185679 0.00056260615 0.0028068128 0.0057402086 0.0090770479 0.012234153 0.014428733 0.014905799 0.013422087 0.010264176 0.0062987506 0.0025082061 -0.0004022778 -0.0021958994][-0.0029235459 -0.0025003878 -0.001760197 -0.00066496967 0.00080084265 0.0024699119 0.0040372908 0.0050860215 0.0052258288 0.0043744268 0.0027440877 0.0008149629 -0.00094392034 -0.0022310764 -0.0029730294][-0.0033339704 -0.0032226238 -0.0029931893 -0.0026211422 -0.0021055876 -0.0015237263 -0.000996948 -0.00067605427 -0.00069163251 -0.0010421348 -0.0016192384 -0.0022431561 -0.0027724844 -0.0031324469 -0.0033185782][-0.0033989928 -0.0033924943 -0.0033633953 -0.003299477 -0.0032006034 -0.0030877721 -0.0029901327 -0.0029371071 -0.0029522525 -0.0030286091 -0.0031365827 -0.0032411027 -0.0033220071 -0.0033725854 -0.0033947595]]...]
INFO - root - 2017-12-09 21:01:31.341123: step 59510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:59m:55s remains)
INFO - root - 2017-12-09 21:01:39.831131: step 59520, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 62h:44m:10s remains)
INFO - root - 2017-12-09 21:01:48.549337: step 59530, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.827 sec/batch; 62h:42m:25s remains)
INFO - root - 2017-12-09 21:01:57.328830: step 59540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:31m:50s remains)
INFO - root - 2017-12-09 21:02:06.048453: step 59550, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 68h:45m:50s remains)
INFO - root - 2017-12-09 21:02:14.742781: step 59560, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 67h:01m:08s remains)
INFO - root - 2017-12-09 21:02:23.525408: step 59570, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:16m:27s remains)
INFO - root - 2017-12-09 21:02:31.821765: step 59580, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 54h:36m:19s remains)
INFO - root - 2017-12-09 21:02:40.381133: step 59590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:31m:40s remains)
INFO - root - 2017-12-09 21:02:48.992716: step 59600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:20m:08s remains)
2017-12-09 21:02:49.858187: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33062443 0.32651886 0.32079342 0.31550372 0.30977577 0.30467138 0.2977154 0.29053265 0.28232843 0.27508971 0.26874167 0.26137453 0.25057134 0.23475051 0.21317062][0.34789973 0.34350574 0.33752683 0.33212656 0.3258003 0.3200835 0.31205565 0.30276963 0.29149371 0.28176817 0.27369633 0.26558813 0.25505149 0.24082075 0.22154665][0.35925087 0.35576391 0.34993336 0.34420562 0.33674219 0.32999477 0.3202641 0.30903333 0.29511133 0.28234503 0.27152207 0.26200134 0.25125656 0.23816849 0.22114778][0.37082872 0.36973736 0.36486018 0.35836908 0.34980223 0.34153658 0.32973006 0.31577715 0.2992202 0.28450891 0.27136543 0.25946686 0.24711755 0.23373811 0.21805349][0.38145059 0.38445392 0.38154384 0.37566879 0.36662397 0.35620716 0.34142426 0.32474014 0.30561677 0.28794733 0.27204728 0.25823987 0.24433498 0.22970857 0.21412267][0.39458671 0.40122235 0.39933038 0.3941561 0.38510424 0.37319812 0.35674059 0.33788818 0.31658068 0.29647276 0.27768505 0.26085189 0.24411932 0.22753708 0.21082003][0.40953621 0.42039391 0.41948044 0.41385952 0.40378478 0.39046758 0.37257916 0.35221228 0.32976463 0.30804679 0.28716272 0.26780549 0.24816605 0.22859164 0.20928216][0.42296821 0.43738586 0.43640149 0.42982057 0.41821581 0.40311798 0.38379768 0.36241642 0.33929241 0.31666109 0.29457575 0.27328482 0.25134826 0.22904427 0.20717613][0.42964771 0.44752085 0.44744271 0.44043237 0.42779717 0.41086566 0.39026979 0.36802477 0.34439209 0.32140729 0.29901862 0.27714422 0.25421634 0.23005834 0.20598786][0.42627704 0.44582587 0.44612533 0.43982297 0.42793536 0.4110702 0.39085865 0.36929438 0.3461391 0.32306287 0.30062354 0.27929083 0.256487 0.2318242 0.20641375][0.41683745 0.43621245 0.43577623 0.42974204 0.41868877 0.40314403 0.38478243 0.36514696 0.34402931 0.32320061 0.302562 0.28216162 0.25981551 0.23575023 0.20982397][0.39924234 0.41777417 0.41712245 0.41210574 0.40231183 0.38872498 0.37244302 0.35516948 0.3365092 0.3184244 0.30063063 0.28305525 0.26337853 0.24110009 0.2157951][0.37099656 0.38935179 0.39020625 0.38768789 0.38062224 0.36957973 0.35552976 0.34059209 0.32403338 0.30795556 0.29259235 0.278106 0.26184362 0.24295644 0.22027533][0.33081058 0.34983006 0.35424745 0.35601044 0.35309765 0.34557244 0.33405823 0.32071832 0.30540019 0.29040012 0.27677056 0.26501471 0.252512 0.23776478 0.21876833][0.284154 0.304185 0.31212792 0.31772807 0.31863937 0.3141956 0.30481321 0.29217309 0.27715307 0.26234612 0.24958175 0.2396533 0.23032428 0.21981686 0.20529938]]...]
INFO - root - 2017-12-09 21:02:58.522750: step 59610, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 61h:35m:05s remains)
INFO - root - 2017-12-09 21:03:07.051478: step 59620, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:19m:56s remains)
INFO - root - 2017-12-09 21:03:15.473688: step 59630, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 63h:07m:27s remains)
INFO - root - 2017-12-09 21:03:23.830370: step 59640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:39m:10s remains)
INFO - root - 2017-12-09 21:03:32.507240: step 59650, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:13m:49s remains)
INFO - root - 2017-12-09 21:03:40.938738: step 59660, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 63h:46m:57s remains)
INFO - root - 2017-12-09 21:03:49.254559: step 59670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:22m:11s remains)
INFO - root - 2017-12-09 21:03:57.758194: step 59680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:15m:23s remains)
INFO - root - 2017-12-09 21:04:06.207512: step 59690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:02m:37s remains)
INFO - root - 2017-12-09 21:04:14.870531: step 59700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:16m:56s remains)
2017-12-09 21:04:15.850328: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.53036922 0.50728589 0.48504478 0.46430603 0.44921577 0.44029126 0.44381469 0.45482194 0.47000548 0.48572454 0.49930936 0.50999922 0.51499104 0.50972354 0.49867374][0.53519452 0.51435894 0.49489033 0.48009706 0.47007197 0.46749663 0.47569922 0.49209493 0.51132876 0.52939492 0.5445925 0.55431145 0.55882406 0.55314422 0.54089582][0.52965558 0.51393795 0.49849957 0.48906308 0.48491362 0.48913169 0.50131309 0.52106857 0.54296237 0.56214172 0.57726914 0.58692497 0.591713 0.58638561 0.573504][0.52538669 0.51533246 0.50591147 0.50306511 0.50448424 0.5139184 0.52958345 0.55157179 0.57352269 0.59164643 0.60602868 0.61510581 0.61937243 0.61518246 0.60364348][0.51733267 0.514923 0.51211131 0.51563495 0.52270854 0.53623176 0.55500269 0.57764208 0.59882319 0.61484528 0.62804651 0.63687444 0.64095491 0.63872027 0.6289928][0.51235914 0.51656228 0.51803672 0.52643514 0.53819585 0.55514377 0.57569736 0.59789544 0.61759239 0.63212651 0.64361346 0.65135664 0.65575892 0.655852 0.64861387][0.51086032 0.51978469 0.52348274 0.53243732 0.54595524 0.56551188 0.58702117 0.60878813 0.62726319 0.64160067 0.65252352 0.66032761 0.66548651 0.66638756 0.66075575][0.51123244 0.52511752 0.53103095 0.54097468 0.55567676 0.57408684 0.5933643 0.61145592 0.62682766 0.63870388 0.64780712 0.6558212 0.66202617 0.66430783 0.66023308][0.5108819 0.52694869 0.53371912 0.54439032 0.55848217 0.57546729 0.59272093 0.60805076 0.62018669 0.62938666 0.63730448 0.64479059 0.65115428 0.65390253 0.6509974][0.50698191 0.52373308 0.52892852 0.5378657 0.55035722 0.56473029 0.57922333 0.59212339 0.60275203 0.61121911 0.61828345 0.6254006 0.63181114 0.63482988 0.63215673][0.49627826 0.51336819 0.51711833 0.52273124 0.53196049 0.54382563 0.55545831 0.56591231 0.57536525 0.58390135 0.59113741 0.59811068 0.60455954 0.60762507 0.60505712][0.47867072 0.49498847 0.49771345 0.50099772 0.50689161 0.51498789 0.52384967 0.53235686 0.54044783 0.5484373 0.55573493 0.56289023 0.5690397 0.57200378 0.56974369][0.45492187 0.47088692 0.47273615 0.47500208 0.47850922 0.483087 0.48857257 0.49433443 0.50023192 0.50617212 0.51261789 0.5192225 0.52508318 0.52836686 0.52733117][0.42797366 0.44218713 0.4428114 0.44400439 0.44583338 0.44878754 0.45257288 0.45687395 0.4615097 0.46605012 0.47135603 0.47655389 0.48140565 0.48449117 0.48437008][0.39847478 0.41199791 0.41216785 0.41198274 0.41289264 0.41454411 0.41681775 0.41953567 0.42263398 0.42622724 0.42991766 0.43374079 0.43753445 0.44024682 0.44074824]]...]
INFO - root - 2017-12-09 21:04:24.300423: step 59710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:06m:27s remains)
INFO - root - 2017-12-09 21:04:32.731426: step 59720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:42m:16s remains)
INFO - root - 2017-12-09 21:04:41.330692: step 59730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:27m:01s remains)
INFO - root - 2017-12-09 21:04:50.099199: step 59740, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 66h:51m:20s remains)
INFO - root - 2017-12-09 21:04:58.607849: step 59750, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 66h:28m:19s remains)
INFO - root - 2017-12-09 21:05:07.295804: step 59760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:30m:57s remains)
INFO - root - 2017-12-09 21:05:15.971055: step 59770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:20m:49s remains)
INFO - root - 2017-12-09 21:05:24.450571: step 59780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:05m:07s remains)
INFO - root - 2017-12-09 21:05:33.017404: step 59790, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:34m:46s remains)
INFO - root - 2017-12-09 21:05:41.730862: step 59800, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:10m:10s remains)
2017-12-09 21:05:42.606367: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19455436 0.19914636 0.19991268 0.19668944 0.19125988 0.1842061 0.17638607 0.16815743 0.15962873 0.15082984 0.1408865 0.1297086 0.11738235 0.10373459 0.089826219][0.21185359 0.22299029 0.2297307 0.23164262 0.22993949 0.22529559 0.218501 0.21006761 0.20014958 0.18902776 0.17606169 0.16116394 0.144544 0.12627016 0.10742017][0.22353885 0.24189852 0.25582805 0.26418108 0.26736104 0.26619849 0.26134729 0.2530891 0.24179055 0.22815318 0.21204206 0.19334401 0.17231077 0.14924355 0.12539127][0.23077135 0.25554219 0.27597642 0.29044458 0.29851854 0.30113208 0.29863611 0.29122517 0.27916655 0.26335633 0.24450359 0.22251464 0.19760512 0.17035976 0.142153][0.23378941 0.26349738 0.28900626 0.30838543 0.32061133 0.32670361 0.3267912 0.32076636 0.30862519 0.29148474 0.27060345 0.24604128 0.21825695 0.18779667 0.15630516][0.23062284 0.26407659 0.29332304 0.31650284 0.33201241 0.34124315 0.34397155 0.33942816 0.32756048 0.30971172 0.28762871 0.2615661 0.23208217 0.19984809 0.16653214][0.22174676 0.25668031 0.28763208 0.31290716 0.3308064 0.34263954 0.34786662 0.34538507 0.33469573 0.31720984 0.29488593 0.26844096 0.23859151 0.20592831 0.1721036][0.20918718 0.24302211 0.27320513 0.298322 0.31698278 0.33026153 0.33749944 0.33716655 0.32827407 0.31234831 0.29123649 0.26565689 0.23664132 0.20503852 0.1720771][0.1940461 0.22494365 0.25226885 0.27540338 0.29340643 0.30695716 0.31536505 0.31696966 0.31025922 0.29670495 0.27781004 0.25452179 0.22792511 0.19860405 0.16768782][0.17630894 0.20349464 0.22708941 0.24730295 0.26353747 0.27635464 0.28514862 0.28815502 0.28374511 0.27296692 0.25707084 0.23670006 0.21304654 0.186605 0.15841009][0.1550905 0.17831883 0.19821195 0.21545604 0.22967458 0.24114077 0.24950129 0.25333333 0.25117549 0.24340492 0.23107755 0.21441743 0.19415785 0.17089255 0.14569405][0.13414259 0.1526331 0.16805719 0.18151908 0.19290788 0.20229533 0.2097621 0.2141162 0.21410988 0.20960493 0.20108546 0.18837331 0.17180359 0.15211649 0.1303785][0.11452886 0.12834887 0.13933788 0.14879951 0.15693553 0.1637264 0.16968124 0.1738492 0.17543836 0.1737828 0.16878991 0.15995945 0.14729176 0.13147615 0.11362652][0.095910639 0.1059217 0.11334027 0.11964174 0.12508343 0.12969071 0.13412119 0.13762666 0.13985491 0.13985665 0.13717943 0.13134177 0.12228817 0.11036449 0.096761644][0.080105931 0.087020874 0.091694571 0.0956926 0.099156983 0.1021416 0.10525462 0.1078874 0.10994824 0.11052429 0.10910302 0.10523809 0.098898575 0.090569273 0.081125908]]...]
INFO - root - 2017-12-09 21:05:51.246015: step 59810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:08m:19s remains)
INFO - root - 2017-12-09 21:05:59.632890: step 59820, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 63h:35m:00s remains)
INFO - root - 2017-12-09 21:06:08.350814: step 59830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:15m:57s remains)
INFO - root - 2017-12-09 21:06:16.898953: step 59840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:07m:41s remains)
INFO - root - 2017-12-09 21:06:25.348076: step 59850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:25m:58s remains)
INFO - root - 2017-12-09 21:06:33.906610: step 59860, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 62h:04m:28s remains)
INFO - root - 2017-12-09 21:06:42.569019: step 59870, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-09 21:06:51.037093: step 59880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:16m:38s remains)
INFO - root - 2017-12-09 21:06:59.445157: step 59890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:29m:10s remains)
INFO - root - 2017-12-09 21:07:08.262748: step 59900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 66h:54m:30s remains)
2017-12-09 21:07:09.188891: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34493071 0.36641425 0.38582605 0.4023225 0.41554826 0.4234865 0.42795533 0.42802772 0.42249891 0.41612 0.40790534 0.40105709 0.39481536 0.38741407 0.38266402][0.35294405 0.37990874 0.40420222 0.42356658 0.43962133 0.4507885 0.45798102 0.45952275 0.45574814 0.44983125 0.44133142 0.43185449 0.42313093 0.41522217 0.41083315][0.34622082 0.37726927 0.40617165 0.4293527 0.44842237 0.46183234 0.47201082 0.47478375 0.472864 0.4676764 0.45930874 0.44935665 0.43991053 0.43275934 0.42831933][0.33360621 0.36785033 0.39955491 0.42674592 0.44964346 0.46643153 0.47845778 0.48348883 0.48317108 0.47771436 0.4690772 0.45848411 0.44897491 0.441196 0.43742412][0.31430542 0.34964117 0.38261351 0.41297415 0.43906671 0.45822731 0.472685 0.47999507 0.4808616 0.47543159 0.46647364 0.45460847 0.4439953 0.43592656 0.43262208][0.29107827 0.32512382 0.3558329 0.38527074 0.41086903 0.43080482 0.44488671 0.453752 0.45738837 0.45368561 0.44576421 0.43424952 0.42423329 0.41574189 0.4118762][0.26781091 0.29716957 0.32313055 0.34831592 0.36851671 0.38495374 0.39613378 0.4046267 0.40959811 0.40830711 0.40334702 0.39437467 0.38726538 0.38049313 0.37707984][0.24791032 0.27199844 0.29136339 0.30951098 0.32215145 0.33135411 0.33732715 0.34226862 0.34586811 0.34513336 0.34226766 0.33695686 0.33293635 0.32876307 0.32663336][0.22978561 0.24859166 0.26067388 0.27044922 0.27444047 0.2753073 0.2734983 0.27297679 0.27397093 0.27320561 0.27190545 0.26955342 0.26888719 0.26692593 0.26558515][0.21102816 0.22395217 0.22868678 0.22989205 0.22515124 0.21794486 0.20968837 0.20472792 0.20270681 0.2014375 0.20144337 0.20152389 0.20301819 0.20265888 0.201683][0.194604 0.20265959 0.20120345 0.19415069 0.18163851 0.16719155 0.15252742 0.14301619 0.13789594 0.13623372 0.13679436 0.13835773 0.14162149 0.14207388 0.14119811][0.17952146 0.18281612 0.17640083 0.16422369 0.14688182 0.12799004 0.11004146 0.0973912 0.088998273 0.085821494 0.086083412 0.088340126 0.092202529 0.093124419 0.09269888][0.16839685 0.16947152 0.16052897 0.14632353 0.12714979 0.10648842 0.087187365 0.073496051 0.0640578 0.059599988 0.059035137 0.061034285 0.064630494 0.065332331 0.064807765][0.15971731 0.16053914 0.15151033 0.13857211 0.12152478 0.10234611 0.084346771 0.070736684 0.060746782 0.055620991 0.054211941 0.055388469 0.058049962 0.058400497 0.05777264][0.15629932 0.15697235 0.14837226 0.13705042 0.12252466 0.1064853 0.091568947 0.08066687 0.072724029 0.068015285 0.066288479 0.06685105 0.068509869 0.068281487 0.067255884]]...]
INFO - root - 2017-12-09 21:07:17.794399: step 59910, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 66h:13m:47s remains)
INFO - root - 2017-12-09 21:07:26.141819: step 59920, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 62h:29m:06s remains)
INFO - root - 2017-12-09 21:07:34.647761: step 59930, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 65h:39m:21s remains)
INFO - root - 2017-12-09 21:07:43.405858: step 59940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:39m:41s remains)
INFO - root - 2017-12-09 21:07:52.138157: step 59950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:37m:05s remains)
INFO - root - 2017-12-09 21:08:00.776709: step 59960, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 66h:49m:06s remains)
INFO - root - 2017-12-09 21:08:09.500242: step 59970, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:23m:40s remains)
INFO - root - 2017-12-09 21:08:18.074519: step 59980, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 65h:35m:27s remains)
INFO - root - 2017-12-09 21:08:26.736289: step 59990, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 68h:48m:34s remains)
INFO - root - 2017-12-09 21:08:35.530748: step 60000, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 67h:09m:19s remains)
2017-12-09 21:08:36.477715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033424692 -0.0033383954 -0.0033379511 -0.003338133 -0.0033383842 -0.0033387381 -0.003339282 -0.0033396508 -0.0033395845 -0.0033394306 -0.0033392604 -0.003339082 -0.0033389346 -0.0033388648 -0.0033389344][-0.0033395309 -0.0033350224 -0.0033346706 -0.0033350929 -0.0033356296 -0.0033362387 -0.0033369055 -0.0033372652 -0.0033370904 -0.0033369279 -0.0033367793 -0.0033366964 -0.0033366317 -0.0033366773 -0.0033366007][-0.00334031 -0.0033357912 -0.0033355684 -0.0033362522 -0.0033372347 -0.0033380878 -0.0033386892 -0.0033388697 -0.0033383416 -0.0033380073 -0.0033376121 -0.0033375465 -0.0033375521 -0.0033376534 -0.0033374426][-0.0033412378 -0.0033370212 -0.0033371744 -0.0033383714 -0.0033398811 -0.0033412513 -0.0033421952 -0.0033421773 -0.0033409342 -0.0033398159 -0.0033388082 -0.0033383761 -0.0033380874 -0.0033380915 -0.0033377788][-0.0033427945 -0.0033395244 -0.0033408219 -0.0033432848 -0.0033459498 -0.0033483556 -0.0033497263 -0.003349337 -0.0033471424 -0.0033442983 -0.0033417426 -0.0033400354 -0.0033390336 -0.0033385002 -0.0033379546][-0.0033451412 -0.003343411 -0.0033462746 -0.0033500169 -0.0033538837 -0.0033573704 -0.0033590645 -0.0033580973 -0.0033547538 -0.003350239 -0.003345652 -0.003342422 -0.0033401072 -0.0033388161 -0.0033379542][-0.0033471049 -0.003347018 -0.0033517678 -0.0033572172 -0.0033625285 -0.0033670163 -0.0033691276 -0.0033675095 -0.0033630314 -0.0033569657 -0.0033504472 -0.0033450786 -0.0033408483 -0.0033387169 -0.0033374722][-0.0033485841 -0.0033500504 -0.00335664 -0.0033641024 -0.0033712096 -0.0033765431 -0.0033791496 -0.0033770653 -0.0033714939 -0.0033638065 -0.003355473 -0.0033479529 -0.0033420811 -0.0033387279 -0.0033369423][-0.0033493843 -0.0033524712 -0.003360722 -0.0033698813 -0.0033784753 -0.0033849152 -0.0033877634 -0.00338516 -0.0033786478 -0.0033696506 -0.0033598284 -0.0033508809 -0.0033440064 -0.0033394666 -0.0033368741][-0.0033492432 -0.0033529813 -0.0033621117 -0.0033715665 -0.0033805938 -0.0033872521 -0.0033898395 -0.00338725 -0.0033806178 -0.0033715675 -0.0033614996 -0.0033526137 -0.0033452825 -0.0033401663 -0.0033370869][-0.0033479244 -0.0033509275 -0.0033595094 -0.0033681875 -0.0033764 -0.0033823608 -0.0033845562 -0.0033820935 -0.0033763519 -0.0033683758 -0.0033595157 -0.0033514402 -0.0033447102 -0.003340059 -0.0033373176][-0.00334582 -0.0033472646 -0.0033543562 -0.0033616733 -0.0033685053 -0.0033731773 -0.0033745191 -0.0033723668 -0.0033676489 -0.0033610859 -0.0033538493 -0.0033475603 -0.0033424692 -0.0033389293 -0.0033367779][-0.0033437142 -0.0033431794 -0.0033482276 -0.0033534835 -0.0033584712 -0.0033618242 -0.0033626542 -0.0033609695 -0.0033575133 -0.0033528223 -0.0033475917 -0.0033432893 -0.0033398711 -0.0033374464 -0.0033359851][-0.0033420306 -0.003339845 -0.0033428513 -0.0033460853 -0.003349303 -0.0033513957 -0.0033519249 -0.0033508437 -0.0033486139 -0.0033456944 -0.0033426192 -0.0033400357 -0.0033379167 -0.0033364878 -0.0033356587][-0.0033408413 -0.0033373118 -0.0033387064 -0.0033402485 -0.0033418376 -0.0033428839 -0.0033432511 -0.0033428245 -0.0033416976 -0.003340319 -0.0033389577 -0.0033377444 -0.0033367376 -0.0033361048 -0.0033357334]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 21:08:45.999635: step 60010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:38m:28s remains)
INFO - root - 2017-12-09 21:08:54.483078: step 60020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:59m:26s remains)
INFO - root - 2017-12-09 21:09:03.094283: step 60030, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:19m:53s remains)
INFO - root - 2017-12-09 21:09:11.896251: step 60040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:11m:37s remains)
INFO - root - 2017-12-09 21:09:20.592347: step 60050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 65h:55m:14s remains)
INFO - root - 2017-12-09 21:09:29.436187: step 60060, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.921 sec/batch; 69h:43m:46s remains)
INFO - root - 2017-12-09 21:09:38.118151: step 60070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:27m:37s remains)
INFO - root - 2017-12-09 21:09:46.518518: step 60080, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 62h:41m:03s remains)
INFO - root - 2017-12-09 21:09:54.903583: step 60090, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 63h:27m:26s remains)
INFO - root - 2017-12-09 21:10:03.477130: step 60100, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:16m:59s remains)
2017-12-09 21:10:04.390030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033592449 -0.0033565764 -0.0033567506 -0.0033570391 -0.0033573643 -0.003357521 -0.0033575846 -0.0033575848 -0.0033575348 -0.0033573145 -0.0033571261 -0.0033570423 -0.0033570495 -0.0033570265 -0.003357149][-0.0033566691 -0.0033538216 -0.0033540863 -0.0033544546 -0.0033548677 -0.0033551448 -0.0033552882 -0.0033553413 -0.0033553441 -0.0033553164 -0.0033552963 -0.0033552696 -0.0033552437 -0.0033552223 -0.0033552782][-0.003356379 -0.0033533054 -0.0033534793 -0.0033537932 -0.0033541813 -0.0033544935 -0.0033547317 -0.0033548826 -0.0033549769 -0.0033550388 -0.0033550735 -0.0033550535 -0.0033550088 -0.0033549622 -0.0033549615][-0.0033555261 -0.0033521042 -0.0033520369 -0.0033522586 -0.0033526288 -0.0033530763 -0.0033535578 -0.0033539203 -0.003354148 -0.0033543387 -0.0033544723 -0.0033544737 -0.0033544207 -0.0033543403 -0.0033543154][-0.0033546188 -0.0033507606 -0.0033503585 -0.0033504411 -0.0033507298 -0.0033512365 -0.003351857 -0.0033524088 -0.0033527836 -0.0033531177 -0.0033533853 -0.0033534218 -0.0033533848 -0.0033533007 -0.003353246][-0.0033538109 -0.0033494562 -0.0033486641 -0.0033484567 -0.0033485165 -0.0033489491 -0.0033495992 -0.0033502285 -0.0033506805 -0.0033511354 -0.0033515079 -0.0033515543 -0.0033514684 -0.00335129 -0.003351154][-0.00335385 -0.0033492073 -0.0033481084 -0.0033475538 -0.003347205 -0.0033473452 -0.0033477596 -0.0033482017 -0.0033484807 -0.0033487957 -0.0033490921 -0.0033491349 -0.0033490686 -0.003348893 -0.0033488532][-0.0033556286 -0.003351372 -0.0033503768 -0.0033495235 -0.003348642 -0.0033480308 -0.0033477044 -0.0033475133 -0.0033472951 -0.0033471547 -0.0033470292 -0.0033468211 -0.0033466918 -0.0033465412 -0.0033466257][-0.003358609 -0.0033554051 -0.0033551354 -0.0033543934 -0.0033532782 -0.0033519871 -0.0033506181 -0.0033493552 -0.0033482972 -0.0033475424 -0.0033469785 -0.0033466073 -0.0033464676 -0.0033463573 -0.003346466][-0.0033621616 -0.0033601776 -0.0033610016 -0.003360756 -0.0033597497 -0.0033581506 -0.0033562018 -0.0033541606 -0.0033521224 -0.0033503773 -0.0033491468 -0.0033485631 -0.0033484725 -0.0033485442 -0.0033488546][-0.0033651085 -0.003364315 -0.0033663686 -0.0033669027 -0.0033663062 -0.0033647635 -0.0033627201 -0.0033604829 -0.0033580798 -0.0033557429 -0.0033538616 -0.0033527983 -0.0033524807 -0.0033526591 -0.0033531836][-0.0033663302 -0.0033663306 -0.0033691316 -0.0033703134 -0.0033703293 -0.0033692524 -0.0033674252 -0.0033652473 -0.003362851 -0.0033603925 -0.0033582191 -0.0033568612 -0.0033563285 -0.0033564446 -0.0033570435][-0.0033651143 -0.0033649176 -0.0033677849 -0.0033691479 -0.0033695209 -0.0033690478 -0.0033678885 -0.0033663176 -0.0033644994 -0.0033625171 -0.0033606575 -0.0033594111 -0.0033588056 -0.003358721 -0.0033590971][-0.003362749 -0.0033617564 -0.0033641092 -0.0033652969 -0.0033657849 -0.0033657667 -0.0033652845 -0.0033645243 -0.0033635281 -0.0033623266 -0.0033611574 -0.0033603003 -0.0033597925 -0.0033596167 -0.0033598081][-0.0033603848 -0.0033584195 -0.0033599713 -0.0033607914 -0.0033612228 -0.0033614761 -0.003361423 -0.0033612156 -0.0033608587 -0.0033603301 -0.0033597748 -0.0033593164 -0.0033589927 -0.0033588388 -0.0033589241]]...]
INFO - root - 2017-12-09 21:10:12.929163: step 60110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:57m:19s remains)
INFO - root - 2017-12-09 21:10:21.428025: step 60120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:13m:10s remains)
INFO - root - 2017-12-09 21:10:30.105929: step 60130, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 65h:02m:51s remains)
INFO - root - 2017-12-09 21:10:38.783822: step 60140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:41m:20s remains)
INFO - root - 2017-12-09 21:10:47.480651: step 60150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:45m:07s remains)
INFO - root - 2017-12-09 21:10:56.241370: step 60160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:37m:03s remains)
INFO - root - 2017-12-09 21:11:05.035203: step 60170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:43m:19s remains)
INFO - root - 2017-12-09 21:11:13.585308: step 60180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:24m:31s remains)
INFO - root - 2017-12-09 21:11:22.125710: step 60190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:14m:53s remains)
INFO - root - 2017-12-09 21:11:30.828599: step 60200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:14m:41s remains)
2017-12-09 21:11:31.776720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033871885 -0.0033837741 -0.0033514 -0.0032132298 -0.0028471616 -0.0022019835 -0.0014238213 -0.00084272353 -0.00075558666 -0.0011949826 -0.001922544 -0.0026142579 -0.0030798647 -0.003304923 -0.0033765947][-0.0033441195 -0.0033067027 -0.0031527737 -0.0027623742 -0.0020194012 -0.00096184737 9.2715258e-05 0.00066656037 0.00044220081 -0.00046981475 -0.0016201312 -0.0025603422 -0.0031030902 -0.0033232325 -0.00337937][-0.002962149 -0.0028557202 -0.0023586636 -0.0012635866 0.00048058596 0.0025744413 0.0042939968 0.0048445677 0.0038953682 0.0018622342 -0.00036872481 -0.0020570531 -0.0029662587 -0.0033069055 -0.0033816565][-0.0020952504 -0.0016533404 -0.0004163722 0.0019949048 0.0055555655 0.0095342454 0.012519814 0.013147889 0.010998847 0.0069594318 0.0026532428 -0.00061620795 -0.0024377769 -0.0031727906 -0.0033643907][-0.0010916083 -5.7938276e-05 0.0023833748 0.0067181503 0.012781681 0.019331338 0.024180515 0.025239697 0.021837287 0.015272729 0.0080307489 0.0022525818 -0.0012242352 -0.0027923328 -0.0032884148][-7.7437144e-05 0.0015401051 0.0052047186 0.011572455 0.020340575 0.029740332 0.036813885 0.038755357 0.034542445 0.025691839 0.015374774 0.0066029439 0.00086329994 -0.0020249889 -0.0030930145][0.00038331747 0.0023007819 0.0068517718 0.014839875 0.025862239 0.037760288 0.047003638 0.050225917 0.046010472 0.03581221 0.023127489 0.011639914 0.0035418321 -0.000918597 -0.0027691352][0.0004650529 0.0022992778 0.006963037 0.01546615 0.027485447 0.040750615 0.051512145 0.056109708 0.05275685 0.042576939 0.028940061 0.015820935 0.00597959 0.000178101 -0.0024208487][0.00010762061 0.0016658527 0.0057822615 0.013630891 0.025135035 0.038229167 0.049310558 0.054748159 0.052586567 0.043543577 0.030555623 0.017424738 0.007114348 0.00075645768 -0.0022218453][-0.00068358588 0.00052872091 0.0037579376 0.010132983 0.019818796 0.031236434 0.041338168 0.046833716 0.045767225 0.038479026 0.027365051 0.015751855 0.0064031156 0.00051947753 -0.0022801864][-0.0016221892 -0.00078572868 0.0014296237 0.005972743 0.013160238 0.021939734 0.030011108 0.034715235 0.034355961 0.029033089 0.020552732 0.011537256 0.0042257747 -0.00037439354 -0.0025466019][-0.0024293233 -0.0019168661 -0.0006063336 0.0021800043 0.0068002287 0.012691293 0.018331166 0.021797597 0.021784868 0.018273637 0.012531674 0.0064271744 0.0015289758 -0.0014921589 -0.0028767041][-0.0029791815 -0.0027139913 -0.0020531833 -0.00059940387 0.0019283888 0.0052942336 0.00864536 0.010801271 0.010900563 0.0088792779 0.0055177826 0.0019809792 -0.00078566885 -0.0024271698 -0.0031428081][-0.0032611419 -0.0031581144 -0.00289019 -0.0022635036 -0.0011100932 0.00050360267 0.0021796564 0.0033049139 0.0033956494 0.0023979892 0.00072852313 -0.0009902555 -0.002281202 -0.0030043675 -0.0032974998][-0.003359582 -0.0033350168 -0.0032575459 -0.0030505816 -0.0026371207 -0.0020190887 -0.0013389671 -0.00085580186 -0.00079507986 -0.0011930082 -0.0018671488 -0.0025420692 -0.0030219089 -0.0032708971 -0.0033616971]]...]
INFO - root - 2017-12-09 21:11:40.321226: step 60210, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 64h:20m:10s remains)
INFO - root - 2017-12-09 21:11:48.777536: step 60220, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:04m:08s remains)
INFO - root - 2017-12-09 21:11:57.237328: step 60230, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 63h:40m:38s remains)
INFO - root - 2017-12-09 21:12:05.899419: step 60240, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 67h:51m:09s remains)
INFO - root - 2017-12-09 21:12:14.562561: step 60250, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 68h:45m:40s remains)
INFO - root - 2017-12-09 21:12:22.962551: step 60260, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 63h:40m:54s remains)
INFO - root - 2017-12-09 21:12:31.506451: step 60270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:43m:50s remains)
INFO - root - 2017-12-09 21:12:40.240865: step 60280, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 66h:32m:06s remains)
INFO - root - 2017-12-09 21:12:48.833116: step 60290, loss = 0.89, batch loss = 0.68 (10.3 examples/sec; 0.775 sec/batch; 58h:35m:34s remains)
INFO - root - 2017-12-09 21:12:57.696792: step 60300, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.913 sec/batch; 69h:03m:59s remains)
2017-12-09 21:12:58.597744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033806802 -0.0033793144 -0.0033799261 -0.0033805887 -0.003381225 -0.0033816854 -0.003381598 -0.0033809454 -0.003380164 -0.0033796979 -0.0033798227 -0.0033800961 -0.0033802618 -0.0033803238 -0.0033805629][-0.0033806895 -0.0033797841 -0.0033811426 -0.0033824223 -0.0033835538 -0.0033840572 -0.00338373 -0.0033825308 -0.0033805037 -0.003378764 -0.0033780981 -0.003378029 -0.0033782264 -0.0033784548 -0.0033788343][-0.0033821743 -0.0033816933 -0.0033839364 -0.0033861895 -0.0033877343 -0.0033883972 -0.0033880996 -0.0033865052 -0.0033835415 -0.0033804558 -0.0033786255 -0.0033778455 -0.0033778395 -0.0033780891 -0.0033785414][-0.0033830605 -0.0033830381 -0.0033861285 -0.0033892184 -0.0033910086 -0.0033918351 -0.0033920698 -0.0033906167 -0.0033871352 -0.0033830653 -0.0033799827 -0.0033781377 -0.0033775971 -0.0033778041 -0.0033783147][-0.0033840607 -0.003384195 -0.003388026 -0.0033915977 -0.0033936121 -0.0033945895 -0.003395383 -0.0033942412 -0.0033907378 -0.0033862644 -0.0033820421 -0.0033789985 -0.0033776034 -0.0033774674 -0.0033780308][-0.0033845878 -0.0033843627 -0.0033884111 -0.0033923418 -0.0033946875 -0.0033956936 -0.0033967223 -0.0033962056 -0.0033931783 -0.0033888852 -0.0033842337 -0.003380476 -0.0033781221 -0.0033773275 -0.0033777575][-0.0033845161 -0.0033841378 -0.0033877015 -0.0033914621 -0.0033940498 -0.0033949991 -0.0033954207 -0.0033950135 -0.0033929592 -0.0033894405 -0.0033850858 -0.003381422 -0.0033785792 -0.0033772215 -0.003377313][-0.00338372 -0.0033831552 -0.003385948 -0.003389451 -0.0033922673 -0.0033928829 -0.0033919893 -0.0033914475 -0.0033904153 -0.0033876349 -0.0033843031 -0.003381466 -0.0033787547 -0.00337724 -0.0033770613][-0.0033824942 -0.0033815058 -0.0033835776 -0.0033865706 -0.0033890205 -0.0033895785 -0.0033887278 -0.0033876726 -0.0033864344 -0.0033844661 -0.0033822865 -0.0033804134 -0.003378588 -0.0033775948 -0.0033772502][-0.0033813408 -0.0033798886 -0.0033814244 -0.0033834269 -0.0033852956 -0.0033860712 -0.003385613 -0.0033840679 -0.0033826691 -0.0033813536 -0.0033800288 -0.0033789105 -0.0033779887 -0.003377612 -0.0033774576][-0.0033806388 -0.0033785671 -0.0033795107 -0.0033807775 -0.0033821496 -0.0033829513 -0.0033828313 -0.0033817098 -0.0033805328 -0.0033793957 -0.0033784125 -0.0033777053 -0.0033774094 -0.0033775356 -0.0033776904][-0.0033805233 -0.0033778825 -0.0033784374 -0.0033792027 -0.003380046 -0.0033806849 -0.0033806721 -0.0033798923 -0.0033791747 -0.0033783147 -0.00337759 -0.0033771649 -0.003377276 -0.0033775547 -0.0033778737][-0.0033807543 -0.0033777412 -0.0033780525 -0.0033784076 -0.0033787971 -0.0033791771 -0.0033790958 -0.0033786641 -0.0033783296 -0.0033778474 -0.0033775344 -0.0033774509 -0.0033776038 -0.0033777803 -0.0033780774][-0.0033812365 -0.0033780234 -0.0033782311 -0.0033783196 -0.0033784218 -0.0033785026 -0.0033784423 -0.00337828 -0.003378147 -0.0033780043 -0.0033779696 -0.0033779209 -0.0033780218 -0.0033782348 -0.0033784227][-0.0033817508 -0.0033785696 -0.003378639 -0.0033786129 -0.003378602 -0.0033786024 -0.0033785882 -0.0033785026 -0.0033784583 -0.0033785051 -0.0033784765 -0.0033783845 -0.0033784634 -0.0033786129 -0.0033786986]]...]
INFO - root - 2017-12-09 21:13:07.198181: step 60310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:29m:57s remains)
INFO - root - 2017-12-09 21:13:15.594367: step 60320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:35m:39s remains)
INFO - root - 2017-12-09 21:13:24.156060: step 60330, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 63h:33m:18s remains)
INFO - root - 2017-12-09 21:13:32.661098: step 60340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:31m:13s remains)
INFO - root - 2017-12-09 21:13:41.172971: step 60350, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 64h:10m:48s remains)
INFO - root - 2017-12-09 21:13:49.770941: step 60360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:46m:22s remains)
INFO - root - 2017-12-09 21:13:58.427939: step 60370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:36m:29s remains)
INFO - root - 2017-12-09 21:14:07.048953: step 60380, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 66h:47m:08s remains)
INFO - root - 2017-12-09 21:14:16.059863: step 60390, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 67h:20m:25s remains)
INFO - root - 2017-12-09 21:14:24.633891: step 60400, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.909 sec/batch; 68h:44m:33s remains)
2017-12-09 21:14:25.558986: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45497724 0.46834987 0.47720259 0.47873059 0.47506282 0.46361297 0.45359266 0.44193131 0.43449116 0.4254747 0.41258195 0.38765356 0.34595513 0.28993249 0.22286074][0.43267518 0.45810103 0.48078272 0.49271896 0.49761623 0.49258852 0.48647466 0.47629255 0.46856451 0.46052825 0.44737867 0.42143959 0.37783644 0.3178457 0.2453727][0.396606 0.43185139 0.46394873 0.48652735 0.50193107 0.50441825 0.50465941 0.49785274 0.4922182 0.48361608 0.46830645 0.43998972 0.39364731 0.331461 0.25633726][0.35019267 0.39646202 0.43877527 0.47194538 0.49734098 0.50870407 0.5140655 0.51139814 0.50783366 0.49882209 0.48197132 0.45090887 0.40189564 0.33634716 0.25897869][0.29668614 0.35252494 0.40537426 0.45001531 0.48632234 0.50726634 0.51944232 0.52187485 0.5201807 0.50933343 0.48998895 0.45565441 0.40362072 0.33471447 0.25578588][0.25203824 0.3137829 0.37379929 0.42771894 0.47308895 0.50321597 0.52209264 0.52877992 0.52841473 0.5159719 0.4943741 0.45580554 0.40033349 0.32911146 0.2495186][0.2265926 0.29258898 0.35714447 0.41591278 0.46580285 0.50027704 0.522511 0.53322315 0.53402847 0.52034891 0.4960838 0.45447144 0.39590839 0.32116088 0.24027459][0.22524138 0.29136491 0.35500658 0.41325036 0.46262476 0.49709517 0.51879913 0.52975994 0.53064245 0.51588118 0.49055672 0.44681212 0.38633367 0.31050995 0.22989303][0.24527718 0.30949569 0.37083495 0.42564264 0.47096309 0.50180596 0.51899344 0.52639979 0.52407843 0.50737989 0.480625 0.43622813 0.37587634 0.29997796 0.22009312][0.28065637 0.34193051 0.39811149 0.44583941 0.48408577 0.508673 0.52113897 0.52321011 0.51653808 0.49769646 0.46997771 0.42555475 0.36488017 0.28995177 0.21160997][0.32839268 0.38339075 0.43056029 0.46908686 0.49869788 0.51459628 0.51988095 0.51576948 0.50604904 0.48710924 0.45941266 0.41657317 0.35759118 0.28455678 0.20718892][0.38022283 0.42650381 0.46160039 0.48852697 0.50828165 0.51604277 0.51628566 0.50894916 0.49819231 0.48009479 0.45349911 0.41291121 0.35556456 0.2839877 0.20710088][0.42553324 0.46133971 0.48443678 0.50082165 0.51212496 0.51469183 0.51194823 0.50219589 0.49152339 0.47489086 0.45044148 0.41196117 0.35671645 0.28733116 0.21072274][0.45998463 0.48875067 0.50298232 0.51246637 0.51779062 0.51688445 0.51256609 0.50266707 0.49267107 0.4772315 0.4538506 0.41727892 0.36353469 0.29462644 0.21756206][0.48479456 0.5068118 0.51302928 0.51702124 0.51869392 0.51679158 0.5119651 0.50341332 0.49498382 0.481606 0.46072224 0.42600897 0.373639 0.30527702 0.2277707]]...]
INFO - root - 2017-12-09 21:14:34.135908: step 60410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:46m:06s remains)
INFO - root - 2017-12-09 21:14:42.512556: step 60420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:00m:47s remains)
INFO - root - 2017-12-09 21:14:51.180386: step 60430, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:09m:22s remains)
INFO - root - 2017-12-09 21:14:59.874403: step 60440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:51m:23s remains)
INFO - root - 2017-12-09 21:15:08.572893: step 60450, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 67h:54m:26s remains)
INFO - root - 2017-12-09 21:15:17.339935: step 60460, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 69h:16m:32s remains)
INFO - root - 2017-12-09 21:15:26.012717: step 60470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:32m:25s remains)
INFO - root - 2017-12-09 21:15:34.592483: step 60480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:36m:09s remains)
INFO - root - 2017-12-09 21:15:43.343834: step 60490, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 63h:55m:11s remains)
INFO - root - 2017-12-09 21:15:52.085509: step 60500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 66h:59m:41s remains)
2017-12-09 21:15:52.936061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0024108335 -0.0024340297 -0.0024773409 -0.0025157698 -0.002548371 -0.0025800464 -0.0026011488 -0.0026063076 -0.0026121049 -0.0026393947 -0.0027082723 -0.0028021554 -0.0029119952 -0.0030474297 -0.0031845029][-0.0023846822 -0.0024203917 -0.0024679548 -0.0024946977 -0.0025058552 -0.0025100426 -0.0024969813 -0.0024747644 -0.0024639787 -0.0024906415 -0.0025677169 -0.0026770739 -0.0028072919 -0.0029671316 -0.003128889][-0.002390244 -0.0024387976 -0.0024995068 -0.0025265012 -0.0025193891 -0.0024885286 -0.0024293743 -0.0023669894 -0.0023332739 -0.0023582452 -0.0024436673 -0.0025696892 -0.0027231071 -0.0029069902 -0.0030891106][-0.0024052833 -0.0024624052 -0.0025368491 -0.0025687292 -0.0025466227 -0.0024772179 -0.0023714898 -0.002271397 -0.0022192907 -0.0022428674 -0.0023346154 -0.0024743429 -0.0026483308 -0.00285616 -0.0030586724][-0.0024360092 -0.0024920073 -0.0025660782 -0.0025929259 -0.0025511261 -0.0024464438 -0.0023101869 -0.0021906048 -0.0021329538 -0.0021543216 -0.0022475305 -0.0023944965 -0.0025832115 -0.0028123483 -0.0030342697][-0.0025006048 -0.0025437227 -0.0026017332 -0.0026068313 -0.0025406573 -0.00241013 -0.002266367 -0.002149357 -0.0020982879 -0.0021162042 -0.0022054673 -0.0023512833 -0.0025461935 -0.0027875775 -0.0030217147][-0.002605689 -0.0026283886 -0.0026630491 -0.002642476 -0.002555165 -0.002410542 -0.0022728224 -0.0021676579 -0.0021299561 -0.0021482175 -0.0022327094 -0.0023713128 -0.002563759 -0.0028028586 -0.0030344715][-0.0027255616 -0.0027319989 -0.0027451469 -0.0027095582 -0.0026203298 -0.002479543 -0.0023542102 -0.0022616473 -0.00223439 -0.0022537564 -0.0023346145 -0.002464846 -0.0026468346 -0.002868474 -0.0030790244][-0.0028268595 -0.0028246483 -0.0028264276 -0.0027908881 -0.0027147511 -0.0025957744 -0.0024919803 -0.0024157953 -0.0023951977 -0.0024137797 -0.0024888231 -0.0026097528 -0.0027758218 -0.0029694214 -0.0031450423][-0.002911414 -0.0029032526 -0.0028999161 -0.0028711662 -0.0028145958 -0.0027280671 -0.0026545911 -0.0026010659 -0.00258714 -0.0026033269 -0.0026694853 -0.0027762449 -0.0029208839 -0.0030791978 -0.0032145556][-0.0030090569 -0.0029937029 -0.0029863745 -0.0029621739 -0.0029224134 -0.0028656309 -0.0028212082 -0.0027911174 -0.0027857795 -0.0028026775 -0.0028572795 -0.0029440103 -0.0030584214 -0.003177295 -0.0032746103][-0.0031334909 -0.0031142589 -0.0031028143 -0.0030823622 -0.0030547089 -0.0030189697 -0.0029948002 -0.0029798946 -0.0029811398 -0.0029996135 -0.0030416253 -0.0031036336 -0.0031808803 -0.0032585012 -0.0033211263][-0.0032584176 -0.0032431555 -0.0032314009 -0.0032154189 -0.0031969328 -0.0031755613 -0.0031638346 -0.0031566198 -0.0031599004 -0.0031745173 -0.0032019338 -0.0032386049 -0.0032809004 -0.003322229 -0.0033551687][-0.0033485712 -0.0033407386 -0.0033330275 -0.0033231094 -0.0033124015 -0.0033007073 -0.0032946886 -0.0032908721 -0.0032927117 -0.0033001115 -0.0033133854 -0.0033295646 -0.0033467577 -0.00336354 -0.0033768949][-0.0033943849 -0.0033910878 -0.0033875739 -0.0033831175 -0.0033785067 -0.0033733957 -0.0033703127 -0.003367692 -0.0033671677 -0.0033679404 -0.0033709714 -0.0033745931 -0.0033788229 -0.003383348 -0.0033874984]]...]
INFO - root - 2017-12-09 21:16:01.619147: step 60510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:22m:42s remains)
INFO - root - 2017-12-09 21:16:10.266156: step 60520, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 67h:31m:10s remains)
INFO - root - 2017-12-09 21:16:18.801936: step 60530, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:10m:42s remains)
INFO - root - 2017-12-09 21:16:27.223653: step 60540, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 61h:58m:04s remains)
INFO - root - 2017-12-09 21:16:35.811557: step 60550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 65h:15m:03s remains)
INFO - root - 2017-12-09 21:16:44.568823: step 60560, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:14m:55s remains)
INFO - root - 2017-12-09 21:16:53.335868: step 60570, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:01m:26s remains)
INFO - root - 2017-12-09 21:17:01.825511: step 60580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 65h:09m:02s remains)
INFO - root - 2017-12-09 21:17:10.603888: step 60590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:59m:33s remains)
INFO - root - 2017-12-09 21:17:19.138660: step 60600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:40m:19s remains)
2017-12-09 21:17:20.010244: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.061161481 0.065985747 0.068376884 0.068978556 0.068201385 0.065908432 0.061549373 0.054651089 0.045237187 0.033994935 0.022301661 0.011889763 0.0041845189 -0.00038190908 -0.0024358213][0.066260159 0.070856459 0.072926342 0.073140666 0.071975984 0.06947659 0.065165237 0.058568068 0.049504235 0.038378727 0.026318224 0.01500712 0.0061624851 0.00060869171 -0.0020372807][0.069070458 0.073505476 0.075557634 0.075844511 0.074776441 0.072354138 0.068042696 0.061430484 0.052408919 0.041380689 0.029302521 0.017625419 0.0080681518 0.0017031531 -0.0015552156][0.07100787 0.075387768 0.077498913 0.077973023 0.077188261 0.075041227 0.07088834 0.064281 0.05516842 0.044026256 0.031791065 0.019775476 0.0096612731 0.00265379 -0.0011253755][0.071637206 0.076280005 0.078743979 0.079625718 0.079252571 0.077384934 0.07332278 0.066618025 0.057339307 0.04605506 0.033687878 0.021433549 0.010913339 0.0034088159 -0.00080216373][0.069938138 0.07518857 0.078350984 0.079969712 0.080232158 0.078719251 0.074720576 0.067843504 0.058327734 0.046881393 0.034430645 0.022098511 0.011431392 0.0037302042 -0.000669769][0.064872392 0.0709609 0.075112551 0.077766642 0.07891687 0.077941895 0.074104235 0.067108959 0.057416655 0.045909509 0.033554103 0.021447388 0.01102113 0.0035045019 -0.00079375226][0.056472447 0.063180082 0.06818416 0.071845591 0.073976912 0.0737382 0.070321433 0.063525341 0.053982358 0.042729069 0.030808227 0.01933383 0.0096259871 0.0027525697 -0.0011063248][0.045822423 0.052671537 0.058046907 0.062256098 0.065073512 0.06557513 0.062852576 0.05669025 0.047777824 0.03726878 0.026281253 0.015940735 0.007430912 0.0015999291 -0.001550081][0.034381848 0.040849432 0.046061534 0.050216269 0.053128913 0.054004166 0.051994421 0.046867013 0.039191741 0.03004971 0.020545568 0.011803561 0.004859996 0.00032135611 -0.0019963556][0.02346121 0.029165367 0.033830944 0.03745744 0.039913274 0.040679343 0.039138272 0.035122782 0.02905306 0.021811826 0.014331726 0.0075911395 0.0024136414 -0.00080716959 -0.0023490037][0.013929885 0.018538231 0.022463556 0.02540097 0.027180435 0.02756384 0.026251124 0.023224117 0.018812556 0.013657542 0.0084176185 0.0037998974 0.0003659958 -0.0016627037 -0.0025586835][0.0063478174 0.0095915459 0.012550237 0.014785063 0.016011696 0.016078088 0.014927559 0.012724916 0.0098023657 0.0065995473 0.0034986597 0.00087471888 -0.00099993474 -0.0020441147 -0.0024538324][0.0010519547 0.0028787691 0.0047249896 0.0062223785 0.0070570819 0.0070620775 0.0062486939 0.0048697954 0.0032844597 0.0018146022 0.000586319 -0.00037303148 -0.001061335 -0.0014674827 -0.0016580704][-0.001788732 -0.0010498429 -0.00021434878 0.00053927465 0.001012231 0.0010647171 0.0006884227 0.00011811825 -0.00032556895 -0.0004130064 -0.00018954161 0.00012498186 0.00032025669 0.00034308131 0.00020583044]]...]
INFO - root - 2017-12-09 21:17:28.606364: step 60610, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:44m:32s remains)
INFO - root - 2017-12-09 21:17:36.922005: step 60620, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:24m:44s remains)
INFO - root - 2017-12-09 21:17:45.356121: step 60630, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:45m:01s remains)
INFO - root - 2017-12-09 21:17:53.979252: step 60640, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:56m:16s remains)
INFO - root - 2017-12-09 21:18:02.556996: step 60650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:52m:42s remains)
INFO - root - 2017-12-09 21:18:11.085438: step 60660, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 64h:27m:13s remains)
INFO - root - 2017-12-09 21:18:19.722744: step 60670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 65h:08m:35s remains)
INFO - root - 2017-12-09 21:18:28.244481: step 60680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:33m:54s remains)
INFO - root - 2017-12-09 21:18:36.883666: step 60690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:13m:41s remains)
INFO - root - 2017-12-09 21:18:45.517396: step 60700, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 67h:53m:53s remains)
2017-12-09 21:18:46.422299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002512123 -0.0026258912 -0.0027663936 -0.0027724125 -0.0029205878 -0.0028209421 -0.0026871 -0.0017913488 -0.00091592525 0.00016193138 0.00065924576 0.00081730471 0.00046803919 0.00043063471 -3.3148099e-06][-0.0022150015 -0.002241781 -0.0023548766 -0.0024304907 -0.0025938964 -0.0023879816 -0.002126087 -0.0010317208 0.00020599458 0.0018735814 0.0029217212 0.0034147294 0.0030855851 0.0027251567 0.0020246452][-0.0019823406 -0.0019394202 -0.0019960268 -0.0020684919 -0.0021527077 -0.0018214714 -0.0012267686 0.00025959709 0.0019461776 0.0039950646 0.0054833209 0.00621379 0.0060339365 0.0054669715 0.0045215422][-0.0019643751 -0.0018734592 -0.0018615632 -0.0018841272 -0.001837426 -0.0012880617 -0.00018534111 0.0017718442 0.003952194 0.006229003 0.0079589933 0.0088725714 0.00886528 0.0081485659 0.0070808092][-0.0020100479 -0.0019350491 -0.001896557 -0.001809684 -0.0015396964 -0.00065860851 0.0010783926 0.0034117831 0.0059380997 0.0081239371 0.0098290965 0.01084769 0.011135757 0.01042462 0.0091349063][-0.0020687552 -0.0021092528 -0.0020723038 -0.0018707952 -0.0012864198 8.6816959e-05 0.0024479546 0.0051411362 0.0076677976 0.0093706064 0.01070705 0.011654177 0.012191895 0.011795261 0.010551448][-0.0021189498 -0.002249303 -0.0022679849 -0.0020198533 -0.0011128283 0.00083415513 0.0037551608 0.0066044107 0.0088734189 0.010000479 0.010772904 0.011353789 0.011944819 0.011870962 0.010933368][-0.00217035 -0.0023960874 -0.0023930334 -0.0020889258 -0.00094832573 0.0015021481 0.0047483398 0.0075747287 0.0094162552 0.0099309934 0.010044168 0.010159704 0.010603962 0.010765132 0.010220785][-0.0022983626 -0.0024838559 -0.0024694973 -0.0021107667 -0.00077548786 0.0019280079 0.0051821535 0.0078450656 0.0092326207 0.0092283534 0.008709982 0.00827408 0.00840684 0.0086532626 0.0085419267][-0.0025087509 -0.002647677 -0.0025719558 -0.0021318123 -0.00078686303 0.0018327974 0.0048611928 0.0072224317 0.0081901615 0.0078275828 0.0068537276 0.0059473915 0.0057076523 0.0058678146 0.00603872][-0.002754844 -0.0028145644 -0.0027418481 -0.0022765598 -0.0010241729 0.0013095287 0.0038761336 0.0057686949 0.0063741235 0.0057948278 0.0046240836 0.0034832659 0.0029879671 0.002980184 0.0032416347][-0.0029728941 -0.0029718468 -0.0029053867 -0.0024951454 -0.0014423537 0.00045959093 0.0024896625 0.0038324904 0.004074309 0.0033709223 0.0022302389 0.0011325383 0.00057568192 0.00045425305 0.0006555242][-0.0031453744 -0.0031005733 -0.0030595353 -0.0027124062 -0.001845399 -0.00039170333 0.0010895396 0.0018626123 0.0017650954 0.0010306204 8.9531764e-05 -0.00077501312 -0.001239588 -0.001393897 -0.0012921663][-0.0032555966 -0.00319031 -0.0031632071 -0.0028994521 -0.0022053707 -0.0011156108 -9.6923904e-05 0.00017937412 -0.00019092439 -0.00087965257 -0.0015477558 -0.0021118135 -0.0024130445 -0.0025387621 -0.0025074084][-0.0033207051 -0.0032655727 -0.0032451651 -0.0030316953 -0.0024870571 -0.0017066337 -0.0010617743 -0.0011096112 -0.0016069699 -0.0021793714 -0.0025916402 -0.0028903245 -0.0030423335 -0.0031140158 -0.0031126791]]...]
INFO - root - 2017-12-09 21:18:54.981918: step 60710, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 67h:51m:50s remains)
INFO - root - 2017-12-09 21:19:03.493794: step 60720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:23m:11s remains)
INFO - root - 2017-12-09 21:19:12.228978: step 60730, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 65h:23m:03s remains)
INFO - root - 2017-12-09 21:19:21.059567: step 60740, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.923 sec/batch; 69h:39m:25s remains)
INFO - root - 2017-12-09 21:19:29.777153: step 60750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:05m:16s remains)
INFO - root - 2017-12-09 21:19:38.503811: step 60760, loss = 0.88, batch loss = 0.67 (8.9 examples/sec; 0.895 sec/batch; 67h:31m:26s remains)
INFO - root - 2017-12-09 21:19:47.306914: step 60770, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 66h:21m:42s remains)
INFO - root - 2017-12-09 21:19:55.926672: step 60780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:08m:59s remains)
INFO - root - 2017-12-09 21:20:04.598615: step 60790, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 62h:31m:35s remains)
INFO - root - 2017-12-09 21:20:13.192267: step 60800, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:06m:46s remains)
2017-12-09 21:20:14.065590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034096115 -0.0034079591 -0.0034074988 -0.003407296 -0.0034071421 -0.0034070828 -0.0034070259 -0.003406866 -0.0034067798 -0.0034068041 -0.0034069247 -0.00340716 -0.0034074418 -0.003407757 -0.0034080402][-0.003409304 -0.0034077729 -0.0034072257 -0.0034069677 -0.0034068516 -0.0034066485 -0.0034057938 -0.0034047256 -0.0034042408 -0.0034039239 -0.0034046033 -0.0034058022 -0.0034066478 -0.003407184 -0.0034074129][-0.0034101112 -0.0034085803 -0.0034077326 -0.0034068772 -0.0034060562 -0.0034046408 -0.0034010059 -0.0033969295 -0.0033948778 -0.0033938072 -0.0033971183 -0.0034021179 -0.0034055857 -0.00340753 -0.0034081873][-0.0034101519 -0.0034059025 -0.0034001004 -0.0033928186 -0.0033841166 -0.0033728264 -0.003359607 -0.0033495023 -0.0033492746 -0.003355718 -0.0033718308 -0.0033902458 -0.0034020259 -0.0034076422 -0.0034094888][-0.0034047824 -0.0033858127 -0.0033564148 -0.0033193533 -0.0032782562 -0.0032324232 -0.0031949796 -0.0031756195 -0.0031903309 -0.0032302551 -0.0032893526 -0.0033483629 -0.0033859487 -0.0034038597 -0.0034098674][-0.0033842938 -0.0033263413 -0.0032364177 -0.0031257754 -0.0030083654 -0.0028884551 -0.0028029778 -0.0027681559 -0.0028135544 -0.0029245517 -0.0030780826 -0.0032296963 -0.003333725 -0.0033862384 -0.0034059316][-0.0033423249 -0.0032250285 -0.0030411587 -0.0028155167 -0.002580632 -0.002351841 -0.0021965308 -0.0021346416 -0.0022102357 -0.0024097047 -0.0027019989 -0.0030044706 -0.0032281026 -0.0033480788 -0.0033956892][-0.0032930933 -0.0031228557 -0.0028552238 -0.0025218339 -0.0021682994 -0.0018262035 -0.0015916185 -0.0014868183 -0.0015662765 -0.0018288858 -0.0022567194 -0.0027271211 -0.0030952184 -0.0032990964 -0.0033823943][-0.0032702524 -0.0030855311 -0.0027921817 -0.0024150643 -0.0019993891 -0.0015867178 -0.0012918618 -0.0011405279 -0.0011962585 -0.0014727216 -0.0019705794 -0.0025450513 -0.0030074609 -0.0032664617 -0.0033733712][-0.0032928071 -0.003139639 -0.0028920341 -0.0025534809 -0.0021588155 -0.0017506774 -0.0014471265 -0.0012780421 -0.0013079601 -0.0015566284 -0.0020285293 -0.00258249 -0.0030271653 -0.0032737101 -0.0033753009][-0.003342906 -0.0032467521 -0.0030837676 -0.0028411951 -0.00253757 -0.0022093856 -0.0019583805 -0.0018151938 -0.0018358056 -0.0020329631 -0.0023970208 -0.0028153292 -0.0031405506 -0.0033156418 -0.0033861359][-0.003386633 -0.003342536 -0.0032603543 -0.0031231712 -0.0029394487 -0.0027334257 -0.0025728857 -0.0024823665 -0.0025026682 -0.0026338333 -0.0028546066 -0.0030951479 -0.0032726075 -0.0033637183 -0.0033991651][-0.0034073729 -0.0033937769 -0.0033641236 -0.0033063223 -0.0032237889 -0.0031273619 -0.0030508889 -0.0030093405 -0.0030242663 -0.003090943 -0.0031903866 -0.0032911235 -0.0033608559 -0.0033949248 -0.0034077596][-0.0034112665 -0.0034084341 -0.0034019339 -0.0033857543 -0.0033613497 -0.0033306307 -0.0033057258 -0.0032926921 -0.0032991176 -0.0033219163 -0.003352107 -0.0033806516 -0.003399251 -0.0034080232 -0.0034110479][-0.0034100693 -0.0034091864 -0.003408785 -0.0034066376 -0.0034029048 -0.0033971434 -0.0033924202 -0.0033900938 -0.0033912715 -0.0033955751 -0.0034008021 -0.003405785 -0.003409103 -0.0034107356 -0.0034110439]]...]
INFO - root - 2017-12-09 21:20:22.769254: step 60810, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 67h:39m:39s remains)
INFO - root - 2017-12-09 21:20:31.303713: step 60820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:58m:19s remains)
INFO - root - 2017-12-09 21:20:39.916627: step 60830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:08m:07s remains)
INFO - root - 2017-12-09 21:20:48.585615: step 60840, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:16m:57s remains)
INFO - root - 2017-12-09 21:20:57.126687: step 60850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:53m:13s remains)
INFO - root - 2017-12-09 21:21:05.797082: step 60860, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 62h:52m:22s remains)
INFO - root - 2017-12-09 21:21:14.543539: step 60870, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:42m:44s remains)
INFO - root - 2017-12-09 21:21:22.978323: step 60880, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 64h:45m:53s remains)
INFO - root - 2017-12-09 21:21:31.660209: step 60890, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 64h:22m:22s remains)
INFO - root - 2017-12-09 21:21:40.089021: step 60900, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:58m:10s remains)
2017-12-09 21:21:41.012297: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38532072 0.3708908 0.3530775 0.33417407 0.31542119 0.29894876 0.28546605 0.27553931 0.27083933 0.269363 0.27108938 0.27478489 0.2780053 0.27931958 0.27751112][0.38008112 0.36569536 0.34822729 0.330689 0.31472349 0.30096889 0.2895593 0.2816546 0.27825981 0.27817926 0.27963734 0.28287506 0.28612542 0.287441 0.28597745][0.36404511 0.34949002 0.33298814 0.31739253 0.30394831 0.2944335 0.28769717 0.28269005 0.28123161 0.28203991 0.28365198 0.28547776 0.28712928 0.2880964 0.28648266][0.34545931 0.33250475 0.31844985 0.305742 0.29595995 0.29077521 0.28814432 0.28769 0.28865731 0.29007727 0.29159322 0.2913585 0.29105496 0.28973022 0.286786][0.32765797 0.31776336 0.30599272 0.29700366 0.29142672 0.29058024 0.2920346 0.29529637 0.29878163 0.30121252 0.30163434 0.29953241 0.29599115 0.29195535 0.28740573][0.31099144 0.30442759 0.29604673 0.29066929 0.28964055 0.29417071 0.30006698 0.30659312 0.31196198 0.3150517 0.31478128 0.30974805 0.30271545 0.29484686 0.2878364][0.29526421 0.29350808 0.28885904 0.28740636 0.29041046 0.29894194 0.30855322 0.3174336 0.32440385 0.3283225 0.32745394 0.3202073 0.3096441 0.29801238 0.28843436][0.280522 0.28414193 0.28410682 0.28655604 0.29263562 0.30347538 0.31449768 0.32469189 0.33249247 0.336419 0.33519134 0.32601011 0.31259838 0.29766995 0.28529361][0.26386654 0.27318585 0.27834788 0.28451291 0.29310369 0.305169 0.31615329 0.32598847 0.33309156 0.33662283 0.33390284 0.32341146 0.30869806 0.29207692 0.2781156][0.24697113 0.26041469 0.26957884 0.27850804 0.28821152 0.30034131 0.31081954 0.31905648 0.32433641 0.32656798 0.322499 0.31072375 0.29464564 0.27824646 0.26466164][0.22855419 0.24553144 0.25775754 0.26910815 0.27998704 0.29166028 0.30106306 0.30780783 0.31126294 0.31158027 0.30635411 0.29391372 0.27766186 0.26149023 0.24836966][0.20956621 0.22847939 0.24305573 0.25673088 0.26883361 0.27966362 0.28755218 0.2922788 0.29345813 0.29155919 0.28477123 0.27216685 0.256443 0.24131311 0.2294437][0.19115706 0.21013677 0.22527789 0.23985536 0.25272486 0.26309079 0.26990202 0.27304754 0.27231285 0.26840574 0.26023567 0.24758711 0.23292971 0.21940936 0.20934738][0.17335342 0.19064991 0.20487224 0.2191851 0.23167348 0.24142657 0.2475239 0.24947938 0.24740168 0.24225856 0.23359881 0.22182818 0.20918196 0.1981899 0.19071758][0.15757488 0.17188667 0.18316522 0.19534987 0.20629591 0.21448684 0.21948443 0.22088489 0.21869053 0.21340258 0.20546174 0.19559561 0.1857181 0.17774756 0.17308001]]...]
INFO - root - 2017-12-09 21:21:49.610818: step 60910, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 66h:40m:47s remains)
INFO - root - 2017-12-09 21:21:58.026873: step 60920, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.691 sec/batch; 52h:08m:36s remains)
INFO - root - 2017-12-09 21:22:06.578016: step 60930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:37m:26s remains)
INFO - root - 2017-12-09 21:22:15.192326: step 60940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 63h:18m:47s remains)
INFO - root - 2017-12-09 21:22:23.964160: step 60950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:35m:39s remains)
INFO - root - 2017-12-09 21:22:32.649478: step 60960, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:52m:50s remains)
INFO - root - 2017-12-09 21:22:41.359905: step 60970, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 62h:45m:10s remains)
INFO - root - 2017-12-09 21:22:49.919424: step 60980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:57m:43s remains)
INFO - root - 2017-12-09 21:22:58.657014: step 60990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:01m:52s remains)
INFO - root - 2017-12-09 21:23:07.290862: step 61000, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:12m:46s remains)
2017-12-09 21:23:08.311191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003383046 -0.0033829685 -0.0033839606 -0.0033840844 -0.0033835426 -0.0033821049 -0.0033797754 -0.0033775684 -0.0033758606 -0.0033746583 -0.0033742269 -0.0033743591 -0.0033747454 -0.0033751549 -0.0033754644][-0.0033808567 -0.0033804327 -0.0033813526 -0.0033816462 -0.0033813654 -0.0033800972 -0.0033778469 -0.0033757244 -0.0033741696 -0.0033730732 -0.0033726639 -0.0033728431 -0.0033732818 -0.0033736778 -0.0033739645][-0.0033799543 -0.0033790909 -0.003379727 -0.0033799422 -0.0033797484 -0.0033786185 -0.003376703 -0.0033749775 -0.0033738341 -0.0033729959 -0.0033726534 -0.0033728487 -0.0033732778 -0.0033736175 -0.0033738522][-0.0033789403 -0.0033775098 -0.0033777624 -0.0033777854 -0.0033775005 -0.0033764895 -0.0033749579 -0.0033737214 -0.0033730238 -0.003372615 -0.0033725477 -0.0033729021 -0.0033734303 -0.0033738408 -0.0033741291][-0.0033778595 -0.0033758506 -0.0033757826 -0.003375632 -0.0033752313 -0.0033743898 -0.0033733167 -0.0033725745 -0.0033722471 -0.0033722722 -0.0033725344 -0.00337305 -0.0033736718 -0.003374167 -0.0033745458][-0.003377114 -0.0033746921 -0.003374448 -0.003374228 -0.0033737875 -0.0033731265 -0.0033723786 -0.0033719072 -0.0033717928 -0.0033720785 -0.003372564 -0.0033731884 -0.0033738865 -0.0033744276 -0.0033748574][-0.0033767002 -0.0033740762 -0.0033737596 -0.003373516 -0.003373083 -0.003372543 -0.0033720157 -0.003371672 -0.0033716443 -0.0033720515 -0.0033726157 -0.0033732576 -0.0033739544 -0.0033744848 -0.0033749242][-0.0033765463 -0.0033738448 -0.0033735074 -0.0033732264 -0.0033727796 -0.0033722746 -0.0033717931 -0.0033714292 -0.0033713686 -0.0033717456 -0.0033723016 -0.0033729211 -0.0033735624 -0.0033740571 -0.0033745253][-0.0033763805 -0.0033737572 -0.003373456 -0.0033731863 -0.0033727523 -0.0033722681 -0.0033717935 -0.0033714031 -0.0033712315 -0.003371465 -0.0033719165 -0.003372445 -0.0033729766 -0.0033733868 -0.0033738022][-0.0033762625 -0.0033738047 -0.0033735898 -0.0033733982 -0.0033730785 -0.0033726555 -0.0033722261 -0.0033718408 -0.0033715495 -0.0033715523 -0.0033717765 -0.0033721339 -0.0033725086 -0.0033727894 -0.0033731111][-0.0033762031 -0.0033738888 -0.0033738518 -0.0033737291 -0.0033735055 -0.0033731409 -0.003372719 -0.0033723237 -0.003371957 -0.0033717914 -0.0033718257 -0.00337205 -0.0033722816 -0.0033724543 -0.0033727146][-0.0033761549 -0.0033739551 -0.0033739684 -0.0033738674 -0.0033736737 -0.0033733384 -0.0033729451 -0.0033725917 -0.0033722438 -0.0033720203 -0.0033719738 -0.0033721155 -0.0033722527 -0.0033723691 -0.0033725866][-0.0033762234 -0.003373923 -0.0033739624 -0.0033738608 -0.00337367 -0.0033733926 -0.0033730769 -0.0033728154 -0.0033725575 -0.0033723745 -0.0033723272 -0.0033724019 -0.0033724837 -0.0033725568 -0.0033727095][-0.0033764045 -0.0033739014 -0.0033739337 -0.0033738641 -0.0033737514 -0.0033735777 -0.0033733777 -0.0033732278 -0.0033730783 -0.0033729658 -0.0033729202 -0.0033729565 -0.0033729973 -0.0033730427 -0.0033731356][-0.0033765081 -0.0033738969 -0.0033739451 -0.0033739153 -0.0033738844 -0.003373821 -0.0033737393 -0.0033736841 -0.0033736264 -0.0033735782 -0.0033735486 -0.0033735689 -0.0033735877 -0.0033736192 -0.0033736792]]...]
INFO - root - 2017-12-09 21:23:16.810710: step 61010, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:46m:01s remains)
INFO - root - 2017-12-09 21:23:25.316040: step 61020, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 60h:58m:34s remains)
INFO - root - 2017-12-09 21:23:33.676739: step 61030, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:42m:59s remains)
INFO - root - 2017-12-09 21:23:42.340511: step 61040, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 62h:11m:54s remains)
INFO - root - 2017-12-09 21:23:51.101373: step 61050, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 64h:47m:24s remains)
INFO - root - 2017-12-09 21:23:59.758751: step 61060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:34m:07s remains)
INFO - root - 2017-12-09 21:24:08.444773: step 61070, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:53m:51s remains)
INFO - root - 2017-12-09 21:24:17.233403: step 61080, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 66h:06m:21s remains)
INFO - root - 2017-12-09 21:24:26.012646: step 61090, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:53m:39s remains)
INFO - root - 2017-12-09 21:24:34.693117: step 61100, loss = 0.88, batch loss = 0.68 (10.3 examples/sec; 0.773 sec/batch; 58h:18m:32s remains)
2017-12-09 21:24:35.516911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033559154 -0.0033523131 -0.0033520155 -0.0033519994 -0.0033520162 -0.0033521571 -0.0033522961 -0.0033524465 -0.0033525699 -0.0033525878 -0.003352558 -0.0033525161 -0.0033524663 -0.0033523859 -0.0033523054][-0.0033537978 -0.0033497715 -0.0033493885 -0.0033493773 -0.0033494143 -0.0033495103 -0.0033496132 -0.0033497361 -0.0033498195 -0.0033497987 -0.0033497496 -0.0033496711 -0.0033495792 -0.0033495133 -0.0033494406][-0.0033540567 -0.0033499016 -0.0033494814 -0.0033494169 -0.0033493622 -0.003349334 -0.003349374 -0.0033494921 -0.00334961 -0.0033496688 -0.0033496798 -0.0033496548 -0.0033496122 -0.0033495482 -0.0033494672][-0.0033539208 -0.0033496532 -0.0033490974 -0.00334892 -0.0033487498 -0.0033486055 -0.0033486306 -0.00334884 -0.0033491272 -0.0033493151 -0.0033494513 -0.0033495368 -0.0033495687 -0.0033495473 -0.0033495021][-0.0033534952 -0.0033490646 -0.0033483636 -0.0033480697 -0.0033477694 -0.0033474995 -0.0033475112 -0.0033478022 -0.0033482043 -0.0033485456 -0.0033488579 -0.0033491547 -0.0033493575 -0.0033494432 -0.0033494707][-0.0033529683 -0.0033483452 -0.0033475116 -0.0033470523 -0.0033466148 -0.0033462569 -0.0033461719 -0.0033464415 -0.003346907 -0.0033474171 -0.003347961 -0.0033485235 -0.0033489773 -0.0033492451 -0.0033493778][-0.0033519985 -0.0033473622 -0.0033464357 -0.0033458364 -0.0033452753 -0.0033448148 -0.0033446769 -0.0033449151 -0.0033453871 -0.0033459864 -0.003346757 -0.0033475948 -0.0033483326 -0.003348818 -0.0033490879][-0.0033509668 -0.0033466255 -0.0033457787 -0.0033451652 -0.0033444881 -0.0033438837 -0.0033436257 -0.0033437568 -0.0033440711 -0.0033446087 -0.0033454455 -0.0033464658 -0.0033474152 -0.0033480935 -0.0033485431][-0.0033503114 -0.003346144 -0.0033455596 -0.0033450562 -0.0033442997 -0.00334364 -0.00334332 -0.0033433568 -0.0033435505 -0.0033439926 -0.0033447412 -0.0033456751 -0.0033466262 -0.0033473466 -0.0033478355][-0.0033500567 -0.0033460541 -0.0033459645 -0.0033457386 -0.0033450881 -0.0033443561 -0.0033439407 -0.0033437219 -0.0033436886 -0.0033439812 -0.0033445547 -0.0033453566 -0.0033462269 -0.0033468781 -0.0033472786][-0.0033502816 -0.0033464401 -0.0033468229 -0.0033469177 -0.0033464967 -0.0033458422 -0.0033452716 -0.0033447531 -0.0033444047 -0.0033444792 -0.0033448567 -0.0033454353 -0.00334619 -0.0033467978 -0.003347158][-0.0033510905 -0.0033472907 -0.0033479561 -0.0033483286 -0.0033482006 -0.0033477773 -0.0033472243 -0.003346544 -0.0033459824 -0.0033457507 -0.0033458059 -0.0033460506 -0.0033465463 -0.0033470385 -0.0033473331][-0.0033522898 -0.0033482139 -0.0033488553 -0.0033491985 -0.0033491936 -0.0033489624 -0.0033485426 -0.0033479431 -0.0033474118 -0.0033471279 -0.0033470294 -0.003347049 -0.0033473284 -0.0033476679 -0.0033478646][-0.0033530928 -0.0033487445 -0.0033492877 -0.0033493282 -0.0033491396 -0.0033488502 -0.0033484388 -0.0033479042 -0.0033475137 -0.0033474457 -0.0033475154 -0.0033475938 -0.003347863 -0.0033481959 -0.003348382][-0.0033536542 -0.0033490784 -0.0033492688 -0.0033490125 -0.0033485759 -0.00334809 -0.0033475789 -0.0033470804 -0.0033468429 -0.0033469494 -0.0033472395 -0.0033475386 -0.0033479314 -0.0033483468 -0.0033486024]]...]
INFO - root - 2017-12-09 21:24:44.228254: step 61110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 67h:22m:58s remains)
INFO - root - 2017-12-09 21:24:52.891974: step 61120, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:17m:12s remains)
INFO - root - 2017-12-09 21:25:01.304274: step 61130, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 63h:29m:59s remains)
INFO - root - 2017-12-09 21:25:09.753925: step 61140, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:53m:02s remains)
INFO - root - 2017-12-09 21:25:18.375106: step 61150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:05m:16s remains)
INFO - root - 2017-12-09 21:25:27.054721: step 61160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:35m:32s remains)
INFO - root - 2017-12-09 21:25:35.764076: step 61170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 66h:49m:11s remains)
INFO - root - 2017-12-09 21:25:44.413400: step 61180, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:41m:13s remains)
INFO - root - 2017-12-09 21:25:53.120618: step 61190, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:38m:32s remains)
INFO - root - 2017-12-09 21:26:01.823460: step 61200, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.829 sec/batch; 62h:27m:57s remains)
2017-12-09 21:26:02.579072: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12486753 0.11240814 0.11073261 0.12056613 0.13640644 0.152051 0.15916021 0.15024041 0.12623157 0.092480145 0.058498707 0.030323694 0.011242843 0.0012487948 -0.0025217091][0.14690837 0.13293694 0.13328283 0.14803107 0.17081901 0.19217601 0.20308708 0.19381481 0.16423702 0.12156527 0.07780432 0.04153822 0.01644518 0.003054837 -0.0021080435][0.15716219 0.14390793 0.15002207 0.17270479 0.20391989 0.23118822 0.24502151 0.2344645 0.19967815 0.14900561 0.096408539 0.052596413 0.021915402 0.0051405309 -0.0016022908][0.15818444 0.14445283 0.15638159 0.18812694 0.22914843 0.26380461 0.28222936 0.27147827 0.23212582 0.17423616 0.11358102 0.062751904 0.026896993 0.0070711644 -0.0011252249][0.15079223 0.13860087 0.15465784 0.19265997 0.24150823 0.28246388 0.30606604 0.29816735 0.25839823 0.19644582 0.12966044 0.073012426 0.03237617 0.0093037691 -0.00057213171][0.1328586 0.12517066 0.14811172 0.1932634 0.24880052 0.29482681 0.32282373 0.31741223 0.27837303 0.21497676 0.14444081 0.083336122 0.038397994 0.012161816 0.00036200369][0.1112524 0.10730927 0.13593736 0.18816307 0.25099856 0.30295587 0.33539173 0.33306497 0.29514807 0.23043776 0.15673916 0.092083067 0.043880656 0.014956528 0.0013513227][0.093140624 0.092722885 0.12495922 0.18111345 0.24882698 0.30618757 0.34425902 0.34604883 0.3102757 0.24511999 0.16878761 0.10052323 0.048784614 0.017216489 0.0020944106][0.080607511 0.08324521 0.11784032 0.17728905 0.24905974 0.3110086 0.35292396 0.35740727 0.322809 0.25680581 0.17810586 0.10696644 0.052652989 0.019063776 0.0026931346][0.0765472 0.08415848 0.12197492 0.18470655 0.259855 0.32430378 0.36668468 0.3702834 0.334094 0.26573443 0.18450451 0.11119382 0.055125318 0.020276429 0.0031321456][0.092732705 0.10460624 0.14636287 0.21151391 0.287263 0.34847683 0.38558435 0.38296738 0.34130308 0.26918861 0.18573408 0.11161938 0.055210367 0.020345667 0.0031797986][0.13314284 0.14920373 0.1923067 0.25770709 0.33120385 0.38685077 0.41562298 0.40286902 0.35173252 0.27242285 0.18508697 0.10961755 0.053392477 0.019420896 0.002912007][0.18568794 0.20432216 0.24694842 0.3099277 0.3780714 0.42553839 0.44353151 0.41970435 0.35919213 0.27326143 0.18276751 0.10656934 0.051036261 0.018172864 0.0024833034][0.24262905 0.26060998 0.29873589 0.35451126 0.41314912 0.44953063 0.456464 0.42287788 0.35546321 0.26613432 0.17540319 0.10086156 0.047429096 0.016436087 0.0018901869][0.29660368 0.31361023 0.346053 0.39041537 0.43440515 0.45569658 0.45000243 0.40774456 0.33629158 0.24766749 0.16080147 0.091005743 0.041829366 0.013893956 0.0010808909]]...]
INFO - root - 2017-12-09 21:26:11.122744: step 61210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:03m:38s remains)
INFO - root - 2017-12-09 21:26:19.905551: step 61220, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 66h:26m:06s remains)
INFO - root - 2017-12-09 21:26:28.467230: step 61230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:59m:57s remains)
INFO - root - 2017-12-09 21:26:37.164268: step 61240, loss = 0.88, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 65h:46m:20s remains)
INFO - root - 2017-12-09 21:26:45.683125: step 61250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:16m:49s remains)
INFO - root - 2017-12-09 21:26:54.160060: step 61260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:49m:28s remains)
INFO - root - 2017-12-09 21:27:02.815558: step 61270, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 66h:18m:21s remains)
INFO - root - 2017-12-09 21:27:11.545779: step 61280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:24m:24s remains)
INFO - root - 2017-12-09 21:27:20.286428: step 61290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:23m:48s remains)
INFO - root - 2017-12-09 21:27:29.065351: step 61300, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 67h:14m:52s remains)
2017-12-09 21:27:29.962104: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0013659068 0.0011874966 0.0011299921 0.00077389437 0.00050453353 -0.00014500669 -0.00067041139 -0.0011459873 -0.0016522801 -0.0020726603 -0.0023860254 -0.00239628 -0.0022597397 -0.0020243544 -0.0017736058][0.00090259849 0.00063215313 0.00044684997 0.0003531822 0.00023122597 -9.8059187e-05 -0.00019113417 -0.00041005854 -0.000715398 -0.0011519867 -0.0014664377 -0.0014814888 -0.0012730905 -0.00090562832 -0.00054315687][0.0016643449 0.0011710112 0.00079575391 0.00036949525 0.00014143693 -5.1786425e-05 3.2805838e-05 -5.4205768e-05 -0.00010759407 -0.00019519334 -0.00018456439 6.1275437e-05 0.00038855453 0.00088733505 0.0013586183][0.0037363984 0.002737175 0.0016398339 0.00073713693 0.0002559144 -0.00010379427 -0.00027067796 -0.00042033195 -0.00019970373 3.2333191e-06 0.00028209575 0.0011447852 0.0022571038 0.0034329358 0.0045072585][0.0059267171 0.0042990763 0.0027017815 0.0015486886 0.00067161978 2.7856557e-05 -0.0001882026 -0.00036372943 -0.0001041803 0.00028664828 0.00098863267 0.0021042477 0.0035111781 0.0050858567 0.0064959619][0.0088622216 0.0074127745 0.00574096 0.0043900749 0.0032227032 0.0023129277 0.001797314 0.0014855636 0.0017928395 0.0024422442 0.0035443783 0.0049217045 0.0064771147 0.0080513684 0.0093410863][0.01218123 0.010782113 0.009269109 0.00798443 0.0066770976 0.00573596 0.0051701916 0.0049990397 0.0053055072 0.0058839014 0.0069906292 0.0082335426 0.0095359273 0.01065868 0.011429652][0.014197405 0.013114676 0.012072356 0.01118685 0.010297021 0.00965529 0.009034914 0.0089252116 0.0091370465 0.009455476 0.010161313 0.010823393 0.011599383 0.01217682 0.012379983][0.017014284 0.016094945 0.015095349 0.014369825 0.013653096 0.012811799 0.012016271 0.011769988 0.011839497 0.011979833 0.012355279 0.0126779 0.013049178 0.01311408 0.012948513][0.018351281 0.017993687 0.017228369 0.016750738 0.016052339 0.015209688 0.014532259 0.014254423 0.014232323 0.014105119 0.01421639 0.014377408 0.0144928 0.0142492 0.013876097][0.018438689 0.018862115 0.018526694 0.018182682 0.017682564 0.017278181 0.016911559 0.01669611 0.016611367 0.016522864 0.016559094 0.016562585 0.016473852 0.016111929 0.015545376][0.017903956 0.018445944 0.018272595 0.018120706 0.017907245 0.017754348 0.017606948 0.017518712 0.017454749 0.017371511 0.017287709 0.017131543 0.016895507 0.016500583 0.015959365][0.018325878 0.019005155 0.018834157 0.018588534 0.01836673 0.018158851 0.01792712 0.017688841 0.017462786 0.017282886 0.017063577 0.01678459 0.016453467 0.016066078 0.015633445][0.019100469 0.019871231 0.019629078 0.01922733 0.018903697 0.018594388 0.018151762 0.017724853 0.017336732 0.017065797 0.016767725 0.01621196 0.01598707 0.015532924 0.015308157][0.019403845 0.020258131 0.020089516 0.019793397 0.019502401 0.019026037 0.018308343 0.017552683 0.01671508 0.016082536 0.015324246 0.014589524 0.014022794 0.013495808 0.013194956]]...]
INFO - root - 2017-12-09 21:27:38.491988: step 61310, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:42m:23s remains)
INFO - root - 2017-12-09 21:27:47.202834: step 61320, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 64h:44m:16s remains)
INFO - root - 2017-12-09 21:27:55.624585: step 61330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:35m:12s remains)
INFO - root - 2017-12-09 21:28:04.169840: step 61340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:02m:57s remains)
INFO - root - 2017-12-09 21:28:12.765578: step 61350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:36m:29s remains)
INFO - root - 2017-12-09 21:28:21.392111: step 61360, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 65h:12m:30s remains)
INFO - root - 2017-12-09 21:28:30.083295: step 61370, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:37m:08s remains)
INFO - root - 2017-12-09 21:28:38.646610: step 61380, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:03m:50s remains)
INFO - root - 2017-12-09 21:28:47.417036: step 61390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:53m:25s remains)
INFO - root - 2017-12-09 21:28:56.086285: step 61400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:47m:31s remains)
2017-12-09 21:28:57.001927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034193641 -0.0034187161 -0.0034172928 -0.003415402 -0.0034142456 -0.0034132767 -0.0034126323 -0.0034119815 -0.0034112304 -0.0034107198 -0.0034098693 -0.003409286 -0.003408795 -0.003408504 -0.0034084327][-0.0034179464 -0.0034184759 -0.0034178656 -0.0034165548 -0.0034157322 -0.0034152968 -0.0034150919 -0.0034144511 -0.0034135019 -0.0034126346 -0.0034114376 -0.0034105275 -0.0034095461 -0.0034087955 -0.003408263][-0.0034166726 -0.0034182807 -0.0034185934 -0.003418 -0.0034171257 -0.0034170779 -0.003417613 -0.0034174076 -0.0034168574 -0.0034160805 -0.0034144833 -0.0034133298 -0.00341179 -0.0034106439 -0.0034095694][-0.0034153583 -0.003417379 -0.0034183175 -0.0034181357 -0.0034170833 -0.0034173673 -0.0034186819 -0.0034189748 -0.0034190768 -0.0034186512 -0.0034173324 -0.0034159748 -0.0034142302 -0.0034126758 -0.0034112241][-0.0034138816 -0.0034161061 -0.0034171343 -0.0034168933 -0.0034154686 -0.0034159934 -0.0034183252 -0.0034192828 -0.0034200198 -0.0034199366 -0.003419132 -0.0034179008 -0.003416179 -0.0034144176 -0.0034129145][-0.0034128348 -0.0034144423 -0.0034162055 -0.0034156118 -0.003413643 -0.003414209 -0.0034172053 -0.0034192472 -0.0034206877 -0.003420681 -0.0034203639 -0.0034191078 -0.0034173795 -0.0034157098 -0.0034141904][-0.0034115496 -0.0034127706 -0.0034155319 -0.0034152644 -0.0034131829 -0.0034136323 -0.0034166505 -0.0034186272 -0.0034205529 -0.0034204968 -0.0034203667 -0.0034191869 -0.0034174686 -0.0034159406 -0.0034143659][-0.0034089519 -0.0034102127 -0.00341395 -0.0034146253 -0.0034139629 -0.0034143142 -0.0034168586 -0.0034182686 -0.0034203033 -0.0034200402 -0.0034198957 -0.0034184395 -0.0034166446 -0.0034151906 -0.0034132102][-0.0034039246 -0.0034058634 -0.0034104038 -0.0034121447 -0.0034136144 -0.0034149226 -0.0034178053 -0.0034187546 -0.0034208479 -0.003420539 -0.0034201383 -0.0034185445 -0.0034166083 -0.0034148814 -0.0034126837][-0.0033967376 -0.0033991816 -0.0034039705 -0.0034070704 -0.0034100006 -0.00341241 -0.0034159748 -0.0034178421 -0.0034206032 -0.0034206312 -0.0034205732 -0.0034192887 -0.0034176933 -0.003415914 -0.0034136756][-0.0033883215 -0.0033910007 -0.0033957874 -0.0033997453 -0.0034034159 -0.0034069 -0.0034112339 -0.0034140379 -0.0034178423 -0.0034191 -0.0034199872 -0.0034195206 -0.0034188065 -0.0034174856 -0.0034153322][-0.0033818472 -0.0033839257 -0.003388002 -0.003392474 -0.0033963383 -0.0034004231 -0.0034054907 -0.0034089757 -0.0034135631 -0.0034159934 -0.0034176866 -0.0034181173 -0.0034183303 -0.0034173022 -0.0034153196][-0.0033801764 -0.0033816253 -0.003384199 -0.0033880831 -0.00339117 -0.0033954303 -0.0034004929 -0.0034048359 -0.003409812 -0.0034127554 -0.0034153806 -0.0034162437 -0.0034168742 -0.0034158076 -0.0034142556][-0.0033833005 -0.0033845222 -0.0033865785 -0.0033896607 -0.0033915464 -0.0033947453 -0.0033991956 -0.0034033626 -0.0034080634 -0.003411467 -0.0034143522 -0.0034154034 -0.003415989 -0.0034148064 -0.0034134134][-0.003390555 -0.003391345 -0.0033930747 -0.0033954473 -0.0033967544 -0.0033987942 -0.0034019768 -0.003405307 -0.0034091922 -0.0034124432 -0.0034151452 -0.0034164104 -0.0034169524 -0.0034161198 -0.0034148307]]...]
INFO - root - 2017-12-09 21:29:05.647383: step 61410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:47m:33s remains)
INFO - root - 2017-12-09 21:29:14.335406: step 61420, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:48m:38s remains)
INFO - root - 2017-12-09 21:29:22.858018: step 61430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:31m:34s remains)
INFO - root - 2017-12-09 21:29:31.363335: step 61440, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 66h:06m:07s remains)
INFO - root - 2017-12-09 21:29:39.905981: step 61450, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 65h:17m:23s remains)
INFO - root - 2017-12-09 21:29:48.540874: step 61460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:06m:29s remains)
INFO - root - 2017-12-09 21:29:57.235899: step 61470, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 67h:45m:35s remains)
INFO - root - 2017-12-09 21:30:05.867766: step 61480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:05m:52s remains)
INFO - root - 2017-12-09 21:30:14.540545: step 61490, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:40m:32s remains)
INFO - root - 2017-12-09 21:30:23.144653: step 61500, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:09m:34s remains)
2017-12-09 21:30:24.026050: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082740694 0.083641157 0.084362343 0.084437132 0.084110975 0.082784787 0.079708055 0.07605809 0.07178881 0.067759536 0.0640151 0.060579345 0.058078796 0.055565208 0.053543855][0.08330857 0.0855569 0.087714158 0.089170486 0.090286121 0.08993616 0.087595046 0.084034048 0.079739325 0.075546794 0.071241483 0.067224838 0.064054288 0.060711123 0.057714224][0.083623715 0.086931013 0.090207689 0.092841275 0.094907 0.095754877 0.094522759 0.091599442 0.087807454 0.08368814 0.078903034 0.074130371 0.06997551 0.06578213 0.061925329][0.084518537 0.088457078 0.092374966 0.095968269 0.098570757 0.10017594 0.099565282 0.097558364 0.093685813 0.089582793 0.084551707 0.079466596 0.074943624 0.070381053 0.066576727][0.086545579 0.091204494 0.095326312 0.099251911 0.10211848 0.1041703 0.10400218 0.10214873 0.098181672 0.093514994 0.087936245 0.082460739 0.077824742 0.073720947 0.070602134][0.087700807 0.092761293 0.097006284 0.10098035 0.10395408 0.10632433 0.10672171 0.10518661 0.10143663 0.096724406 0.091098092 0.08564198 0.081141859 0.07766293 0.07514403][0.087304749 0.0924964 0.096671268 0.10066264 0.10358342 0.10604954 0.10693103 0.10595839 0.10267749 0.09831246 0.093479261 0.088860363 0.084918119 0.082030989 0.080009371][0.085449614 0.090588219 0.094493374 0.098227821 0.10102662 0.10328418 0.10438716 0.10375375 0.10129499 0.097920291 0.094486773 0.091026887 0.087961882 0.08556921 0.083538465][0.081457861 0.086625949 0.090265438 0.093581647 0.096037231 0.097755767 0.098773882 0.098518476 0.097015053 0.094933406 0.093097746 0.090907045 0.0888237 0.087024748 0.085163742][0.0760733 0.080996893 0.08441548 0.087446347 0.089712553 0.091151632 0.091837637 0.0914692 0.090338945 0.088897906 0.087924913 0.086757094 0.085853346 0.085062072 0.083958939][0.0697901 0.074231744 0.077226318 0.079836652 0.081694655 0.083028458 0.083662659 0.083377913 0.0826179 0.0816455 0.081277512 0.08079838 0.0808267 0.0810345 0.080961332][0.064430304 0.068107404 0.070453808 0.072634913 0.074292704 0.07544829 0.075950757 0.075784452 0.075189985 0.074329525 0.074156 0.074176162 0.074878305 0.075888187 0.076853208][0.060136586 0.062930249 0.064578094 0.066210471 0.067533568 0.068618834 0.068952128 0.068768047 0.068239853 0.067485623 0.067384019 0.067748286 0.069048569 0.070672296 0.072375588][0.055343118 0.057387464 0.058421217 0.05968304 0.060807526 0.061825164 0.062243968 0.062161952 0.061767928 0.061129533 0.061063975 0.061578766 0.062985264 0.064821236 0.06681554][0.051216751 0.052801616 0.053364307 0.054193497 0.054992747 0.055739842 0.056075368 0.056055382 0.055826176 0.05546803 0.055531736 0.056094054 0.057342362 0.058949322 0.06071917]]...]
INFO - root - 2017-12-09 21:30:32.555128: step 61510, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 68h:58m:31s remains)
INFO - root - 2017-12-09 21:30:41.077685: step 61520, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 63h:40m:10s remains)
INFO - root - 2017-12-09 21:30:49.444336: step 61530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:47m:46s remains)
INFO - root - 2017-12-09 21:30:58.008310: step 61540, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:05m:47s remains)
INFO - root - 2017-12-09 21:31:06.649005: step 61550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:59m:29s remains)
INFO - root - 2017-12-09 21:31:15.336526: step 61560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:23m:29s remains)
INFO - root - 2017-12-09 21:31:24.049326: step 61570, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 67h:45m:37s remains)
INFO - root - 2017-12-09 21:31:32.117018: step 61580, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.821 sec/batch; 61h:47m:32s remains)
INFO - root - 2017-12-09 21:31:40.533587: step 61590, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 62h:49m:14s remains)
INFO - root - 2017-12-09 21:31:49.180725: step 61600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:00m:09s remains)
2017-12-09 21:31:50.083620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033840192 -0.0033804795 -0.0033799689 -0.0033816672 -0.0033846276 -0.0033883185 -0.0033919618 -0.0033951134 -0.0033975539 -0.003399167 -0.0033986252 -0.0033953921 -0.0033897383 -0.0033836602 -0.0033788923][-0.00339191 -0.0033886484 -0.0033884069 -0.0033898121 -0.0033918144 -0.0033935227 -0.003395092 -0.003396611 -0.0033981476 -0.0033995672 -0.003399848 -0.0033980897 -0.0033948163 -0.0033909706 -0.0033878253][-0.0033967167 -0.0033945858 -0.003394855 -0.003396312 -0.0033978545 -0.0033991917 -0.0034005474 -0.0034015123 -0.0034023321 -0.0034028727 -0.0034022359 -0.0034005407 -0.0033980734 -0.0033953036 -0.0033937062][-0.0033963777 -0.0033943725 -0.0033953311 -0.0033975074 -0.0033996357 -0.003401607 -0.0034034082 -0.0034050304 -0.003405785 -0.0034056385 -0.0034044746 -0.0034024692 -0.0033998645 -0.0033966824 -0.0033946307][-0.0033969588 -0.0033951604 -0.0033963465 -0.0033988487 -0.0034010035 -0.0034025351 -0.0034037575 -0.0034050266 -0.0034055614 -0.0034053475 -0.0034046294 -0.0034031856 -0.0034009754 -0.0033978506 -0.0033953213][-0.0033963711 -0.003394746 -0.0033961502 -0.0033987749 -0.0034006822 -0.0034016601 -0.0034021484 -0.0034028259 -0.0034035272 -0.0034043705 -0.0034046816 -0.0034038774 -0.0034019023 -0.0033988953 -0.0033959402][-0.0033960235 -0.0033948594 -0.0033960869 -0.0033983418 -0.0033999796 -0.0033998571 -0.0033982266 -0.0033969672 -0.0033981139 -0.0034014573 -0.0034034264 -0.0034030275 -0.0034012492 -0.0033983728 -0.0033953604][-0.0033952685 -0.003394396 -0.0033955444 -0.0033974897 -0.0033980103 -0.0033957639 -0.003390752 -0.0033872281 -0.0033903371 -0.0033970657 -0.0034010108 -0.0034016294 -0.0034006669 -0.0033982741 -0.0033951378][-0.0033944494 -0.0033935278 -0.0033947532 -0.0033961802 -0.0033957046 -0.0033910486 -0.0033835103 -0.0033795568 -0.0033846581 -0.003392403 -0.0033976974 -0.0033998329 -0.0033999276 -0.003398007 -0.0033946966][-0.0033941439 -0.0033931993 -0.0033944328 -0.0033954289 -0.0033939222 -0.0033889462 -0.0033828886 -0.0033811429 -0.0033850644 -0.0033904691 -0.0033950815 -0.0033975868 -0.0033981239 -0.0033967658 -0.0033940792][-0.0033941069 -0.0033930307 -0.0033941404 -0.0033947334 -0.003393468 -0.0033906726 -0.0033877161 -0.00338712 -0.0033886477 -0.0033913802 -0.0033941946 -0.0033959798 -0.0033962443 -0.003395238 -0.0033934943][-0.0033941849 -0.0033926577 -0.0033933928 -0.0033939686 -0.0033936678 -0.0033926133 -0.0033915585 -0.0033911227 -0.0033913834 -0.003392464 -0.0033938715 -0.003394417 -0.003394187 -0.0033936712 -0.0033927329][-0.0033937991 -0.0033918512 -0.003392471 -0.0033929714 -0.0033930137 -0.0033927294 -0.0033927758 -0.00339279 -0.0033928938 -0.0033932147 -0.003393634 -0.0033933732 -0.0033929397 -0.0033926012 -0.0033921278][-0.0033934391 -0.0033910398 -0.0033916233 -0.003392067 -0.0033921828 -0.0033919357 -0.00339184 -0.0033915136 -0.0033915658 -0.0033919588 -0.0033922156 -0.0033919504 -0.0033917527 -0.0033917942 -0.0033916119][-0.0033928985 -0.0033901145 -0.0033906244 -0.0033911814 -0.0033912158 -0.0033911036 -0.0033910095 -0.003390827 -0.0033910065 -0.0033913942 -0.0033914822 -0.0033912458 -0.0033910971 -0.0033911637 -0.0033910486]]...]
INFO - root - 2017-12-09 21:31:58.409754: step 61610, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 61h:16m:49s remains)
INFO - root - 2017-12-09 21:32:06.937118: step 61620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:42m:28s remains)
INFO - root - 2017-12-09 21:32:15.448219: step 61630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:13m:00s remains)
INFO - root - 2017-12-09 21:32:24.007739: step 61640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:09m:56s remains)
INFO - root - 2017-12-09 21:32:32.943077: step 61650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:51m:05s remains)
INFO - root - 2017-12-09 21:32:41.640636: step 61660, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:30m:34s remains)
INFO - root - 2017-12-09 21:32:50.326860: step 61670, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:42m:26s remains)
INFO - root - 2017-12-09 21:32:59.038370: step 61680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:06m:09s remains)
INFO - root - 2017-12-09 21:33:07.879189: step 61690, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:36m:52s remains)
INFO - root - 2017-12-09 21:33:16.572117: step 61700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:04m:20s remains)
2017-12-09 21:33:17.414920: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035843749 0.028442102 0.022483552 0.019802926 0.020078033 0.022566374 0.026360732 0.030759202 0.035079908 0.039321486 0.042965118 0.045737058 0.04803944 0.049106508 0.049512859][0.045938417 0.039684359 0.033742368 0.031120474 0.031488575 0.03383974 0.037703793 0.042226575 0.046680342 0.050846137 0.054492906 0.057474688 0.059776425 0.061123677 0.061611965][0.051583827 0.046467453 0.040449124 0.037092447 0.036840059 0.038587693 0.041939579 0.045857731 0.05028221 0.054722637 0.058243942 0.061261468 0.063315421 0.064742066 0.065103821][0.050389476 0.045906927 0.039674543 0.035677869 0.034338523 0.034731876 0.037253462 0.040556584 0.044680461 0.049096096 0.053044811 0.05643345 0.05825153 0.05954998 0.059677094][0.04257996 0.038071778 0.031505734 0.026847262 0.024416681 0.023865337 0.025388254 0.027844498 0.031312134 0.035271198 0.038998686 0.042057287 0.043743052 0.044980768 0.045030482][0.029262422 0.024684133 0.018559875 0.014425818 0.012005104 0.011228477 0.011979305 0.013602044 0.016105002 0.019080287 0.022061178 0.024468198 0.025990693 0.027041014 0.027141033][0.016133578 0.012116787 0.0072699925 0.0041986546 0.0023963575 0.0018157291 0.0021353241 0.0029973497 0.0044007227 0.0061337566 0.007943931 0.0094145825 0.010434991 0.011107253 0.011155629][0.005997505 0.0034308096 0.00063538994 -0.0009645042 -0.0018557764 -0.0021546944 -0.002091941 -0.0018093517 -0.0012855569 -0.00058836443 0.00018712156 0.00084244576 0.0013515989 0.0016943614 0.0017260427][8.8689849e-06 -0.0011463992 -0.0022621993 -0.0028210632 -0.0031133948 -0.0032124782 -0.0032092468 -0.0031501187 -0.003026953 -0.002853425 -0.0026479121 -0.0024609962 -0.0022970797 -0.0021730578 -0.0021469886][-0.0024963676 -0.0028441786 -0.0031470582 -0.0032785481 -0.0033400748 -0.0033618868 -0.0033654072 -0.0033597569 -0.0033424972 -0.0033192257 -0.0032872229 -0.0032518448 -0.0032158454 -0.0031818058 -0.0031702407][-0.0032176753 -0.0032835898 -0.0033365388 -0.0033553937 -0.003362237 -0.0033645625 -0.0033646228 -0.0033657381 -0.0033634959 -0.0033640186 -0.0033610014 -0.0033565355 -0.0033520742 -0.0033458921 -0.0033422927][-0.0033461647 -0.0033467405 -0.0033489945 -0.00335193 -0.0033547489 -0.0033548467 -0.0033542793 -0.0033556183 -0.0033553208 -0.0033562647 -0.0033573078 -0.0033574814 -0.0033574672 -0.0033549019 -0.0033534295][-0.0033537834 -0.003346012 -0.0033420844 -0.0033437295 -0.003346887 -0.0033467528 -0.0033454509 -0.0033449787 -0.0033435703 -0.0033444385 -0.0033457067 -0.0033478457 -0.003349151 -0.0033490905 -0.0033471058][-0.0033506595 -0.0033426199 -0.0033372878 -0.0033375795 -0.0033401775 -0.0033417337 -0.0033416159 -0.0033404392 -0.0033394769 -0.0033381237 -0.0033375495 -0.00333744 -0.0033380936 -0.003338973 -0.0033370808][-0.0033523561 -0.0033447978 -0.0033398252 -0.0033377439 -0.0033373241 -0.0033368096 -0.0033364024 -0.0033369281 -0.003337136 -0.0033355795 -0.0033357223 -0.00333321 -0.00333221 -0.0033316307 -0.0033306121]]...]
INFO - root - 2017-12-09 21:33:25.964201: step 61710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:02m:24s remains)
INFO - root - 2017-12-09 21:33:34.560553: step 61720, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 66h:33m:34s remains)
INFO - root - 2017-12-09 21:33:43.125511: step 61730, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 66h:02m:26s remains)
INFO - root - 2017-12-09 21:33:51.845348: step 61740, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:07m:17s remains)
INFO - root - 2017-12-09 21:34:00.568739: step 61750, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:16m:27s remains)
INFO - root - 2017-12-09 21:34:09.091127: step 61760, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:35m:24s remains)
INFO - root - 2017-12-09 21:34:17.857753: step 61770, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:32m:56s remains)
INFO - root - 2017-12-09 21:34:26.482496: step 61780, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:10m:17s remains)
INFO - root - 2017-12-09 21:34:35.089924: step 61790, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 63h:58m:43s remains)
INFO - root - 2017-12-09 21:34:43.550820: step 61800, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 65h:18m:45s remains)
2017-12-09 21:34:44.388406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033750767 -0.0033730012 -0.0033728664 -0.0033729665 -0.0033731051 -0.0033732671 -0.0033734222 -0.0033735423 -0.0033736303 -0.0033735933 -0.0033735477 -0.003373482 -0.0033733717 -0.0033732802 -0.0033731537][-0.0033725053 -0.0033701544 -0.0033700054 -0.0033700932 -0.003370174 -0.0033701626 -0.0033701598 -0.0033701521 -0.0033701479 -0.003370154 -0.0033702189 -0.0033703248 -0.00337041 -0.0033704608 -0.0033704625][-0.0033718937 -0.0033694163 -0.0033692997 -0.0033694326 -0.0033695165 -0.0033694231 -0.00336934 -0.0033692261 -0.0033690964 -0.0033690184 -0.0033690585 -0.0033692499 -0.0033694583 -0.0033696194 -0.0033697498][-0.0033717076 -0.0033691907 -0.0033690645 -0.0033692352 -0.0033694024 -0.0033694194 -0.0033693863 -0.0033692766 -0.0033691064 -0.0033689335 -0.0033688322 -0.0033689428 -0.0033691768 -0.0033694045 -0.0033696629][-0.0033717374 -0.0033690506 -0.0033689328 -0.0033691938 -0.0033695176 -0.0033697279 -0.0033698119 -0.0033696585 -0.0033693835 -0.0033690191 -0.0033686704 -0.0033685646 -0.00336875 -0.0033690808 -0.0033695004][-0.0033720245 -0.0033692464 -0.0033691833 -0.0033695444 -0.0033700338 -0.0033704671 -0.0033706801 -0.0033705109 -0.0033700843 -0.0033694534 -0.0033688117 -0.0033683975 -0.0033684007 -0.0033686892 -0.0033691677][-0.003372455 -0.0033696773 -0.0033696694 -0.0033701374 -0.00337076 -0.0033713994 -0.0033717647 -0.0033716161 -0.0033710452 -0.0033701456 -0.0033691637 -0.0033683712 -0.0033680648 -0.0033682075 -0.0033686312][-0.003372822 -0.0033700829 -0.0033700601 -0.0033705663 -0.00337132 -0.0033721568 -0.003372683 -0.0033726222 -0.0033720278 -0.0033709621 -0.0033696704 -0.003368486 -0.0033678757 -0.0033678296 -0.0033681353][-0.0033731502 -0.0033705845 -0.0033705467 -0.003371004 -0.003371703 -0.0033725332 -0.0033730632 -0.003373086 -0.0033725936 -0.0033715679 -0.0033702452 -0.0033689104 -0.0033681116 -0.003367896 -0.0033680482][-0.003373354 -0.003370954 -0.0033709528 -0.0033713372 -0.0033718951 -0.0033725549 -0.0033730096 -0.0033730895 -0.003372723 -0.00337189 -0.0033707751 -0.0033695754 -0.0033687793 -0.0033684513 -0.003368404][-0.0033731409 -0.0033708024 -0.0033708932 -0.0033711791 -0.0033715838 -0.0033720797 -0.0033724566 -0.0033726231 -0.0033724506 -0.0033718951 -0.0033710822 -0.0033701463 -0.0033694545 -0.0033690606 -0.0033688468][-0.003372693 -0.0033704138 -0.0033704832 -0.0033706392 -0.0033708387 -0.0033710864 -0.0033713267 -0.0033715067 -0.0033714736 -0.0033711616 -0.0033706718 -0.0033701169 -0.0033696783 -0.0033693565 -0.0033691342][-0.00337241 -0.0033699435 -0.0033700087 -0.0033700522 -0.0033701113 -0.0033702059 -0.0033703314 -0.003370465 -0.0033704834 -0.0033703544 -0.0033701558 -0.0033699244 -0.0033697633 -0.0033696177 -0.0033695034][-0.0033724229 -0.0033698021 -0.0033699113 -0.0033699255 -0.0033699402 -0.0033699812 -0.003370058 -0.0033701574 -0.0033701747 -0.0033701316 -0.0033700741 -0.0033700108 -0.00336996 -0.0033698862 -0.0033698378][-0.0033727149 -0.0033699365 -0.0033700396 -0.0033700492 -0.0033700767 -0.0033701179 -0.0033701712 -0.0033702236 -0.0033702166 -0.003370157 -0.0033701009 -0.003370078 -0.0033700652 -0.003370055 -0.0033700764]]...]
INFO - root - 2017-12-09 21:34:52.846892: step 61810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:49m:33s remains)
INFO - root - 2017-12-09 21:35:01.586677: step 61820, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 65h:32m:07s remains)
INFO - root - 2017-12-09 21:35:10.145891: step 61830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:14m:10s remains)
INFO - root - 2017-12-09 21:35:18.836119: step 61840, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 67h:36m:29s remains)
INFO - root - 2017-12-09 21:35:27.427638: step 61850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:02m:49s remains)
INFO - root - 2017-12-09 21:35:36.206992: step 61860, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:35m:32s remains)
INFO - root - 2017-12-09 21:35:44.967011: step 61870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:36m:59s remains)
INFO - root - 2017-12-09 21:35:53.540656: step 61880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:13m:02s remains)
INFO - root - 2017-12-09 21:36:02.371143: step 61890, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:26m:26s remains)
INFO - root - 2017-12-09 21:36:11.121174: step 61900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:59m:24s remains)
2017-12-09 21:36:12.014117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26452348 0.26326159 0.26484603 0.27108279 0.27515405 0.27103728 0.25264129 0.21820949 0.17115682 0.11979329 0.07226298 0.03552394 0.011967709 0.00076536206 -0.0027055778][0.26632634 0.2599929 0.25941229 0.26691851 0.27435708 0.2751292 0.26065537 0.22898556 0.18236251 0.1294505 0.079450957 0.039887562 0.01420049 0.0015522579 -0.0025424077][0.26098955 0.25167978 0.25029615 0.2587122 0.26870516 0.272663 0.26142702 0.23282482 0.18781407 0.13503163 0.083964005 0.04308572 0.016147463 0.0023996623 -0.0023452109][0.25617748 0.24608713 0.24526525 0.2550208 0.26720604 0.27340686 0.26418728 0.23710968 0.19265769 0.13950162 0.087198712 0.045137294 0.017187452 0.0028349042 -0.0022315327][0.25126091 0.24331537 0.24450082 0.255646 0.2696383 0.27712205 0.26875108 0.24213547 0.19748855 0.14363752 0.089848042 0.046604384 0.017747885 0.0029845012 -0.0022323872][0.24859795 0.24397869 0.24701643 0.25938851 0.274467 0.282101 0.2736201 0.24647756 0.20109573 0.14649852 0.091560051 0.047467213 0.017841421 0.0028599745 -0.0023335987][0.24791157 0.24752107 0.25284019 0.26603532 0.28078878 0.2875247 0.27777976 0.24917713 0.20256901 0.14721829 0.091794327 0.04735931 0.017427 0.0025080342 -0.0024892841][0.24662083 0.25019255 0.25660712 0.27016777 0.28424713 0.28971049 0.27841204 0.24852911 0.20104417 0.14555922 0.090569928 0.046460096 0.016741872 0.0020759853 -0.0026527669][0.24085696 0.24777842 0.25508839 0.2684035 0.28123435 0.28545153 0.27318278 0.24298757 0.1956708 0.1411476 0.087607272 0.044733159 0.015858311 0.0016703571 -0.0027542531][0.22898595 0.23708303 0.24374215 0.25687546 0.26968688 0.27331215 0.26084784 0.23148304 0.18603551 0.13386856 0.082763955 0.0420739 0.01471203 0.0012900431 -0.0028144568][0.20942616 0.21832223 0.22511511 0.23805499 0.250453 0.25443181 0.24340411 0.21589181 0.17325114 0.12438117 0.076680019 0.038662553 0.013165444 0.00087912777 -0.0028420675][0.17899536 0.18871623 0.19737364 0.21168374 0.22568472 0.23158617 0.22294483 0.19858581 0.15960832 0.11443577 0.070322894 0.035022195 0.011569657 0.00038917968 -0.0028848073][0.13838707 0.14852126 0.16019072 0.17729425 0.19492126 0.20472239 0.20055345 0.18073975 0.14630339 0.10537379 0.06475848 0.031923238 0.010277867 4.1888561e-06 -0.0029253461][0.093126707 0.10330451 0.11874954 0.13940614 0.16141307 0.17599894 0.17712909 0.16224369 0.13246804 0.095758386 0.058662284 0.028518116 0.0088157151 -0.00042193569 -0.0029841231][0.052586529 0.06225355 0.079254039 0.10212737 0.12769915 0.14652942 0.15295485 0.14321907 0.11854542 0.086238638 0.052416444 0.024853973 0.0070754858 -0.00095841754 -0.0030653416]]...]
INFO - root - 2017-12-09 21:36:20.528489: step 61910, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 62h:49m:59s remains)
INFO - root - 2017-12-09 21:36:29.200376: step 61920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:59m:41s remains)
INFO - root - 2017-12-09 21:36:37.717802: step 61930, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.858 sec/batch; 64h:27m:24s remains)
INFO - root - 2017-12-09 21:36:46.223060: step 61940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 63h:14m:11s remains)
INFO - root - 2017-12-09 21:36:54.908915: step 61950, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 67h:45m:23s remains)
INFO - root - 2017-12-09 21:37:03.632025: step 61960, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:49m:54s remains)
INFO - root - 2017-12-09 21:37:12.576828: step 61970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:58m:57s remains)
INFO - root - 2017-12-09 21:37:21.163512: step 61980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:06m:06s remains)
INFO - root - 2017-12-09 21:37:29.899081: step 61990, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 65h:26m:31s remains)
INFO - root - 2017-12-09 21:37:38.588725: step 62000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:48m:03s remains)
2017-12-09 21:37:39.453700: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094182692 0.083387487 0.068846025 0.05601478 0.044752073 0.036897786 0.031270072 0.027304227 0.022532137 0.016384065 0.0096505824 0.0038239171 -0.00038465369 -0.0025422112 -0.0032781351][0.12106469 0.11032352 0.09484306 0.081130661 0.068673857 0.05995775 0.05317723 0.047418278 0.039587427 0.029540045 0.018483909 0.0089535732 0.0020673585 -0.0016383483 -0.0030906789][0.14676693 0.13699681 0.12199706 0.10870276 0.095780723 0.086354405 0.078341536 0.071077727 0.060752809 0.047069203 0.031245558 0.016992651 0.0060420865 -0.00019411813 -0.0027648523][0.16920027 0.16233531 0.15024099 0.13913772 0.12704286 0.11749614 0.10792863 0.098306254 0.084456436 0.066360958 0.045272678 0.026069593 0.010915304 0.0018365274 -0.0022846563][0.18412776 0.18136685 0.1737498 0.16619213 0.15616149 0.14733078 0.13700169 0.12578368 0.1088855 0.086508475 0.059865914 0.03537938 0.015850248 0.003861903 -0.001762222][0.18986844 0.19124244 0.18777913 0.1840497 0.17644063 0.16881023 0.1583809 0.14623387 0.12733674 0.10184805 0.07097467 0.042522121 0.01967125 0.0054778112 -0.0013415676][0.18771449 0.19286624 0.19260009 0.19109993 0.18475556 0.17794375 0.1677639 0.15559137 0.13584694 0.1084263 0.075120591 0.044756558 0.020784894 0.0060030567 -0.0011813352][0.17908907 0.18699513 0.18879 0.18801543 0.1813764 0.17419417 0.16402206 0.15248375 0.13299108 0.10524167 0.071653269 0.04159962 0.018832132 0.0051918952 -0.0013440549][0.16385576 0.17421316 0.17710239 0.17615905 0.16861565 0.160292 0.14989802 0.13887483 0.11995039 0.093307115 0.06169647 0.034258679 0.014629163 0.0034046026 -0.0017849385][0.1420946 0.15400149 0.15702291 0.15564342 0.14729008 0.13802978 0.12796012 0.11787722 0.10026675 0.075683832 0.047510266 0.024450529 0.0092112515 0.0010875554 -0.0023702695][0.11741178 0.12894988 0.13087302 0.12866829 0.11972864 0.10997171 0.10056049 0.091937348 0.076701023 0.055609059 0.032374058 0.014546735 0.0039055294 -0.0010641725 -0.0028934553][0.092660047 0.10259943 0.10303362 0.09967906 0.090417787 0.080443837 0.071845524 0.06490887 0.052705079 0.036133207 0.018679151 0.0063409321 -4.3306034e-05 -0.002523215 -0.0032289559][0.069775842 0.0773313 0.076964393 0.073020428 0.064015657 0.054436971 0.046814945 0.041360289 0.03235849 0.020561518 0.0086760661 0.0010479263 -0.0022260481 -0.0031691901 -0.0033359143][0.049023364 0.054284964 0.0536902 0.050127827 0.042669781 0.034385365 0.028054105 0.023870278 0.017778158 0.01008007 0.0026766353 -0.0015836398 -0.0030609157 -0.0033366387 -0.0033583906][0.031095779 0.03397461 0.033193644 0.030418782 0.025126768 0.019088445 0.014579758 0.011881563 0.0084378142 0.0039191935 -0.00034115277 -0.002599434 -0.003265752 -0.0033577548 -0.0033636678]]...]
INFO - root - 2017-12-09 21:37:47.988024: step 62010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:46m:18s remains)
INFO - root - 2017-12-09 21:37:56.691278: step 62020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 62h:16m:05s remains)
INFO - root - 2017-12-09 21:38:05.257567: step 62030, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:44m:33s remains)
INFO - root - 2017-12-09 21:38:13.883914: step 62040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:43m:55s remains)
INFO - root - 2017-12-09 21:38:22.518685: step 62050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:54m:37s remains)
INFO - root - 2017-12-09 21:38:31.090740: step 62060, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 61h:45m:00s remains)
INFO - root - 2017-12-09 21:38:39.736987: step 62070, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:28m:07s remains)
INFO - root - 2017-12-09 21:38:48.240752: step 62080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:01m:35s remains)
INFO - root - 2017-12-09 21:38:57.093551: step 62090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:59m:54s remains)
INFO - root - 2017-12-09 21:39:05.906975: step 62100, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 66h:53m:42s remains)
2017-12-09 21:39:06.850315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00336867 -0.0033660957 -0.0033659372 -0.0033661111 -0.00336632 -0.0033665835 -0.003366872 -0.003367099 -0.0033671518 -0.0033670643 -0.0033668785 -0.0033666913 -0.0033664613 -0.0033663306 -0.0033662415][-0.0033662736 -0.0033634496 -0.0033632477 -0.0033634973 -0.0033639374 -0.0033644205 -0.0033648815 -0.0033651527 -0.0033652526 -0.0033652829 -0.0033651465 -0.0033649555 -0.0033647171 -0.003364512 -0.0033642338][-0.0033657502 -0.0033627 -0.0033624235 -0.0033626421 -0.0033632333 -0.0033640021 -0.0033646296 -0.0033648557 -0.0033649688 -0.0033650226 -0.0033648622 -0.0033646524 -0.0033645483 -0.0033644009 -0.0033639609][-0.0033654298 -0.0033621034 -0.0033616065 -0.003361545 -0.0033621015 -0.0033631881 -0.0033640736 -0.0033643283 -0.0033645085 -0.0033646054 -0.0033644892 -0.0033643884 -0.0033646172 -0.0033646198 -0.003364085][-0.0033658862 -0.0033622207 -0.0033614053 -0.0033608249 -0.0033609585 -0.0033620596 -0.0033630538 -0.0033634785 -0.0033639898 -0.0033643439 -0.0033645055 -0.0033648803 -0.003365624 -0.0033658494 -0.0033651835][-0.0033674319 -0.0033633157 -0.0033619341 -0.0033606528 -0.0033601152 -0.0033608451 -0.0033617441 -0.0033623222 -0.0033631087 -0.0033637665 -0.0033644321 -0.0033655469 -0.0033669623 -0.0033676587 -0.0033671609][-0.0033707267 -0.0033661181 -0.0033638107 -0.0033614894 -0.0033600356 -0.0033600295 -0.003360545 -0.003361122 -0.0033621998 -0.0033631683 -0.003364403 -0.0033663257 -0.0033685295 -0.0033698645 -0.0033697896][-0.0033761642 -0.0033712031 -0.0033677197 -0.0033639255 -0.0033611287 -0.0033599944 -0.0033597881 -0.0033601448 -0.0033613218 -0.0033625865 -0.0033642841 -0.0033668729 -0.0033696655 -0.0033715665 -0.0033719882][-0.0033841245 -0.003379104 -0.0033746085 -0.0033690019 -0.0033642061 -0.0033612174 -0.003359878 -0.0033597592 -0.0033606954 -0.0033620337 -0.003364057 -0.0033669313 -0.0033698343 -0.0033719197 -0.0033727495][-0.0033919758 -0.0033870765 -0.003382127 -0.0033755675 -0.003369157 -0.0033642368 -0.003361494 -0.0033605306 -0.0033607848 -0.0033617371 -0.003363563 -0.0033662098 -0.0033687379 -0.0033706049 -0.0033716222][-0.0033989153 -0.0033945316 -0.0033896565 -0.0033829026 -0.0033756113 -0.0033691067 -0.003364861 -0.0033627388 -0.0033619779 -0.0033622074 -0.0033633574 -0.0033652363 -0.0033670429 -0.0033683807 -0.003369322][-0.0034032352 -0.0033994655 -0.00339489 -0.0033882745 -0.00338073 -0.0033733454 -0.0033679302 -0.0033648082 -0.0033633721 -0.0033630799 -0.0033635872 -0.0033647022 -0.0033657 -0.0033664957 -0.0033671949][-0.0034033358 -0.0034002024 -0.0033962163 -0.0033901504 -0.0033829114 -0.0033755507 -0.0033697065 -0.0033660561 -0.0033642454 -0.0033637395 -0.0033638473 -0.0033642917 -0.0033646047 -0.0033648636 -0.0033651979][-0.0033987339 -0.0033960582 -0.0033931267 -0.0033882526 -0.0033821254 -0.0033756755 -0.0033703048 -0.0033667309 -0.0033649988 -0.0033645572 -0.0033645339 -0.0033645921 -0.0033643602 -0.0033640354 -0.0033638286][-0.0033909667 -0.0033885166 -0.003386664 -0.0033832365 -0.0033786644 -0.0033738185 -0.0033695588 -0.0033666792 -0.0033653479 -0.0033651842 -0.0033653786 -0.0033654412 -0.0033650361 -0.0033644133 -0.0033638743]]...]
INFO - root - 2017-12-09 21:39:15.295823: step 62110, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.740 sec/batch; 55h:36m:04s remains)
INFO - root - 2017-12-09 21:39:23.877293: step 62120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:30m:31s remains)
INFO - root - 2017-12-09 21:39:32.315334: step 62130, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 61h:54m:55s remains)
INFO - root - 2017-12-09 21:39:40.863083: step 62140, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:23m:36s remains)
INFO - root - 2017-12-09 21:39:49.426771: step 62150, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:39m:53s remains)
INFO - root - 2017-12-09 21:39:57.995165: step 62160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:52m:45s remains)
INFO - root - 2017-12-09 21:40:06.616312: step 62170, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:23m:17s remains)
INFO - root - 2017-12-09 21:40:15.172354: step 62180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:35m:49s remains)
INFO - root - 2017-12-09 21:40:23.769864: step 62190, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 65h:58m:22s remains)
INFO - root - 2017-12-09 21:40:32.420474: step 62200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:23m:17s remains)
2017-12-09 21:40:33.352648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033572109 -0.0033537203 -0.0033528209 -0.0033523256 -0.0033521564 -0.0033524451 -0.003352819 -0.0033529191 -0.0033528747 -0.003352694 -0.0033519934 -0.0033512998 -0.0033509261 -0.0033508688 -0.003351036][-0.0033554719 -0.0033516423 -0.0033507219 -0.0033503817 -0.0033505545 -0.0033512851 -0.0033522337 -0.0033529478 -0.0033533608 -0.003353186 -0.0033520986 -0.0033509233 -0.0033498253 -0.0033489249 -0.0033484695][-0.003356132 -0.0033520532 -0.0033511 -0.0033508129 -0.0033510781 -0.0033519759 -0.0033535475 -0.0033551734 -0.0033563967 -0.0033567627 -0.0033558945 -0.0033544712 -0.0033524125 -0.0033504227 -0.0033489587][-0.0033564982 -0.0033523212 -0.0033512893 -0.0033509363 -0.0033511345 -0.0033519969 -0.003353877 -0.003356314 -0.0033584931 -0.0033595788 -0.0033592284 -0.0033579329 -0.0033554565 -0.0033526034 -0.0033502323][-0.0033569359 -0.0033526933 -0.0033516479 -0.0033511976 -0.0033513035 -0.0033519161 -0.0033536644 -0.0033564279 -0.0033591739 -0.0033608477 -0.0033611082 -0.0033602824 -0.0033580025 -0.0033547375 -0.0033517366][-0.00335721 -0.0033529426 -0.0033519755 -0.0033514737 -0.0033513294 -0.0033516234 -0.00335297 -0.0033554852 -0.0033582686 -0.0033603127 -0.0033611758 -0.0033609255 -0.0033590498 -0.0033559382 -0.0033528565][-0.0033572831 -0.0033531082 -0.0033522695 -0.0033517561 -0.0033513173 -0.0033512472 -0.0033520765 -0.00335393 -0.0033562966 -0.0033583816 -0.0033598468 -0.0033602337 -0.0033590312 -0.0033565534 -0.003353778][-0.0033573124 -0.00335333 -0.0033525561 -0.0033520346 -0.0033514749 -0.0033511152 -0.003351297 -0.0033522854 -0.0033538404 -0.0033554344 -0.00335695 -0.0033577222 -0.0033573 -0.0033557825 -0.0033538733][-0.003357114 -0.003353439 -0.0033529778 -0.0033525841 -0.0033520216 -0.0033515205 -0.0033511685 -0.0033513522 -0.003352023 -0.0033527729 -0.0033537142 -0.0033544949 -0.0033546386 -0.0033540833 -0.003353141][-0.0033568291 -0.0033533853 -0.0033532137 -0.003352924 -0.0033524807 -0.0033520237 -0.00335159 -0.0033513044 -0.003351341 -0.0033515336 -0.0033519345 -0.0033524502 -0.0033526584 -0.0033525666 -0.0033522614][-0.0033564165 -0.0033531343 -0.0033532723 -0.0033530889 -0.003352785 -0.00335251 -0.0033522008 -0.0033518383 -0.0033516726 -0.0033516423 -0.003351795 -0.0033520239 -0.0033521415 -0.0033522355 -0.0033522425][-0.0033562821 -0.0033530607 -0.00335326 -0.0033530891 -0.0033529187 -0.0033527692 -0.0033525736 -0.0033523773 -0.0033522851 -0.00335225 -0.0033522798 -0.0033523145 -0.0033523992 -0.003352531 -0.0033526733][-0.0033565371 -0.003353127 -0.0033533294 -0.0033531261 -0.0033530213 -0.0033529564 -0.0033528558 -0.0033528123 -0.0033528686 -0.0033529254 -0.0033529291 -0.0033529277 -0.0033529878 -0.0033531287 -0.0033532721][-0.0033568183 -0.0033532318 -0.0033534791 -0.003353318 -0.0033532563 -0.0033532607 -0.0033532032 -0.0033531897 -0.0033532819 -0.0033534146 -0.0033534202 -0.0033534029 -0.0033534626 -0.003353585 -0.0033536947][-0.0033571583 -0.0033534472 -0.0033536782 -0.0033535815 -0.0033535478 -0.0033535687 -0.0033535045 -0.0033534446 -0.0033534796 -0.003353612 -0.0033536495 -0.003353633 -0.0033537035 -0.0033538002 -0.0033538605]]...]
INFO - root - 2017-12-09 21:40:42.015027: step 62210, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 64h:06m:25s remains)
INFO - root - 2017-12-09 21:40:50.543581: step 62220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:12m:36s remains)
INFO - root - 2017-12-09 21:40:59.208918: step 62230, loss = 0.89, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 57h:53m:41s remains)
INFO - root - 2017-12-09 21:41:08.037414: step 62240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:39m:30s remains)
INFO - root - 2017-12-09 21:41:16.694546: step 62250, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:53m:21s remains)
INFO - root - 2017-12-09 21:41:25.450935: step 62260, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:19m:14s remains)
INFO - root - 2017-12-09 21:41:34.094578: step 62270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 65h:24m:47s remains)
INFO - root - 2017-12-09 21:41:42.636933: step 62280, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 66h:20m:13s remains)
INFO - root - 2017-12-09 21:41:51.312281: step 62290, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:11m:55s remains)
INFO - root - 2017-12-09 21:42:00.100956: step 62300, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:01m:52s remains)
2017-12-09 21:42:01.041808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033957968 -0.0033934058 -0.0033930673 -0.0033929548 -0.0033929932 -0.0033930803 -0.0033930426 -0.0033930514 -0.0033932074 -0.0033933441 -0.0033935765 -0.0033936447 -0.0033936948 -0.0033938652 -0.0033940307][-0.0033949544 -0.0033926147 -0.0033924682 -0.0033925758 -0.0033924603 -0.0033920039 -0.0033912696 -0.0033909555 -0.0033910186 -0.0033913071 -0.0033915613 -0.0033915967 -0.0033918112 -0.0033921753 -0.0033924482][-0.0033967234 -0.0033946179 -0.0033946796 -0.003394926 -0.0033944612 -0.003393522 -0.0033923821 -0.0033916607 -0.0033915867 -0.0033917523 -0.0033918391 -0.0033918356 -0.00339205 -0.0033924247 -0.0033926815][-0.0033983761 -0.0033966617 -0.0033970873 -0.0033974859 -0.0033968273 -0.0033955742 -0.0033942766 -0.0033931683 -0.0033929425 -0.0033930477 -0.0033928098 -0.0033924726 -0.0033926254 -0.00339286 -0.0033929797][-0.0033992755 -0.0033977008 -0.0033985688 -0.0033993116 -0.0033988319 -0.0033977241 -0.0033964997 -0.0033953427 -0.0033950387 -0.0033948247 -0.0033940666 -0.0033933292 -0.0033933495 -0.0033933958 -0.0033933434][-0.0033993437 -0.0033978426 -0.0033991814 -0.0034004149 -0.0034001421 -0.0033992163 -0.0033982559 -0.0033975318 -0.0033973509 -0.0033967088 -0.0033954948 -0.0033943288 -0.0033941229 -0.0033939579 -0.0033937097][-0.0033991374 -0.0033976985 -0.0033993418 -0.0034011204 -0.0034017486 -0.0034013148 -0.0034009519 -0.0034007076 -0.0034000012 -0.0033987449 -0.0033969649 -0.0033952903 -0.0033946291 -0.0033942966 -0.0033940119][-0.0034000541 -0.0033984422 -0.00339976 -0.003401672 -0.0034031658 -0.0034036129 -0.0034038031 -0.0034034266 -0.0034023928 -0.0034005884 -0.0033983767 -0.0033963302 -0.0033951171 -0.0033945211 -0.0033942033][-0.003401408 -0.0033998559 -0.0034007705 -0.003402445 -0.0034041035 -0.0034052446 -0.0034059586 -0.0034056508 -0.0034043824 -0.0034021295 -0.003399903 -0.0033977283 -0.003396044 -0.0033949737 -0.003394481][-0.0034028473 -0.0034012606 -0.0034018508 -0.0034031577 -0.003404659 -0.0034059079 -0.0034069777 -0.003406686 -0.0034050697 -0.0034028341 -0.0034008378 -0.0033989013 -0.0033968727 -0.0033954456 -0.0033948114][-0.0034028939 -0.0034011842 -0.0034016266 -0.0034026224 -0.0034037242 -0.0034047095 -0.0034057484 -0.0034056783 -0.0034043076 -0.0034023572 -0.0034006648 -0.0033989963 -0.0033970636 -0.0033956254 -0.0033950454][-0.0034015607 -0.0033997095 -0.0034000562 -0.0034007675 -0.0034014985 -0.0034021188 -0.0034028736 -0.0034029891 -0.0034020212 -0.0034006035 -0.0033993318 -0.0033982056 -0.0033968629 -0.0033958026 -0.0033954445][-0.0033998047 -0.0033977213 -0.003397895 -0.0033983267 -0.0033987632 -0.0033990676 -0.0033995465 -0.0033998105 -0.0033992573 -0.0033983816 -0.003397648 -0.0033971027 -0.0033964112 -0.0033958901 -0.003395708][-0.0033981078 -0.0033959295 -0.0033959437 -0.0033962019 -0.0033964468 -0.0033966093 -0.0033969667 -0.0033973104 -0.0033970703 -0.0033967004 -0.0033965523 -0.0033964014 -0.0033961688 -0.0033959711 -0.0033958403][-0.0033975337 -0.0033953835 -0.0033953334 -0.0033955635 -0.0033956596 -0.0033957369 -0.0033960245 -0.0033962177 -0.0033960007 -0.003395895 -0.0033959623 -0.0033959469 -0.0033959239 -0.0033958589 -0.0033957551]]...]
INFO - root - 2017-12-09 21:42:09.563081: step 62310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:03m:33s remains)
INFO - root - 2017-12-09 21:42:17.962921: step 62320, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:41m:25s remains)
INFO - root - 2017-12-09 21:42:26.460928: step 62330, loss = 0.88, batch loss = 0.67 (10.5 examples/sec; 0.762 sec/batch; 57h:13m:06s remains)
INFO - root - 2017-12-09 21:42:34.810624: step 62340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:46m:29s remains)
INFO - root - 2017-12-09 21:42:43.343289: step 62350, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 61h:29m:47s remains)
INFO - root - 2017-12-09 21:42:51.976479: step 62360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:04m:32s remains)
INFO - root - 2017-12-09 21:43:00.714216: step 62370, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 0.997 sec/batch; 74h:48m:29s remains)
INFO - root - 2017-12-09 21:43:09.363900: step 62380, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 67h:38m:38s remains)
INFO - root - 2017-12-09 21:43:18.100888: step 62390, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:23m:50s remains)
INFO - root - 2017-12-09 21:43:26.950619: step 62400, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.903 sec/batch; 67h:46m:55s remains)
2017-12-09 21:43:27.842423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003379476 -0.0033777733 -0.0033776304 -0.0033776732 -0.0033778213 -0.0033779619 -0.0033780239 -0.003378026 -0.0033781554 -0.0033781009 -0.0033780539 -0.0033780979 -0.0033781475 -0.0033782411 -0.0033782204][-0.0033778015 -0.0033758804 -0.0033757214 -0.0033756883 -0.0033757892 -0.003375967 -0.0033760159 -0.0033759889 -0.0033761263 -0.003376205 -0.0033762718 -0.0033763442 -0.0033765507 -0.0033766855 -0.0033767328][-0.0033780711 -0.0033760748 -0.0033760117 -0.0033758983 -0.0033757808 -0.0033758651 -0.003375819 -0.0033757773 -0.0033759181 -0.0033759507 -0.003375869 -0.0033760134 -0.0033764048 -0.0033765561 -0.0033766865][-0.0033789729 -0.0033769719 -0.0033768206 -0.0033766634 -0.0033764071 -0.0033762027 -0.0033759428 -0.0033758781 -0.0033760024 -0.0033758231 -0.0033756096 -0.0033757754 -0.0033762539 -0.0033764374 -0.0033765859][-0.0033798015 -0.0033777338 -0.0033776732 -0.0033776944 -0.0033774055 -0.0033769934 -0.0033765745 -0.0033763943 -0.0033762408 -0.0033759885 -0.0033758725 -0.0033761319 -0.0033766238 -0.0033768949 -0.0033770255][-0.0033803228 -0.0033782644 -0.0033782152 -0.003378422 -0.003378215 -0.0033777345 -0.0033770497 -0.0033765025 -0.0033762141 -0.0033760753 -0.0033761209 -0.0033764155 -0.0033769207 -0.0033772576 -0.0033773752][-0.0033805559 -0.0033786877 -0.003378863 -0.0033791736 -0.0033790339 -0.0033786602 -0.003377645 -0.0033765996 -0.0033761549 -0.0033760066 -0.0033759952 -0.0033763007 -0.0033768972 -0.0033772876 -0.0033774246][-0.003380633 -0.003378951 -0.0033792679 -0.0033795931 -0.0033794998 -0.0033790062 -0.0033778017 -0.0033766851 -0.0033760185 -0.0033756483 -0.0033755454 -0.0033757007 -0.0033762823 -0.0033767922 -0.0033771261][-0.0033807342 -0.0033792655 -0.0033796928 -0.0033801731 -0.0033800921 -0.0033793037 -0.0033778416 -0.0033764795 -0.0033755735 -0.0033750841 -0.0033748618 -0.0033748804 -0.0033752413 -0.0033757649 -0.0033761675][-0.0033797671 -0.0033784022 -0.0033788674 -0.0033794467 -0.0033794411 -0.0033786569 -0.003377314 -0.0033759959 -0.0033751959 -0.0033749109 -0.0033747656 -0.0033747731 -0.0033749207 -0.0033752604 -0.003375588][-0.003378269 -0.0033768655 -0.0033776397 -0.0033781617 -0.0033781789 -0.003377602 -0.0033766136 -0.0033754576 -0.0033747628 -0.0033745645 -0.003374452 -0.0033744168 -0.0033745365 -0.003374696 -0.0033747961][-0.0033767759 -0.0033753039 -0.003376174 -0.0033767198 -0.0033768229 -0.0033764879 -0.0033757405 -0.0033748979 -0.0033744092 -0.0033742436 -0.0033741791 -0.0033742322 -0.0033743207 -0.0033743065 -0.0033743018][-0.0033757212 -0.003374025 -0.0033746541 -0.0033749337 -0.0033750131 -0.0033749356 -0.0033745209 -0.0033739847 -0.0033736671 -0.0033737093 -0.0033738294 -0.0033739943 -0.0033740508 -0.003374035 -0.0033740709][-0.0033757337 -0.0033737794 -0.0033741815 -0.0033743144 -0.0033743496 -0.0033743384 -0.0033740886 -0.0033737132 -0.0033734133 -0.00337339 -0.0033734867 -0.0033737884 -0.0033739835 -0.0033740397 -0.0033741295][-0.0033764315 -0.0033741777 -0.0033744161 -0.0033745344 -0.0033745551 -0.0033745046 -0.003374371 -0.0033741111 -0.0033737184 -0.0033735149 -0.0033735693 -0.0033738562 -0.0033740918 -0.0033742443 -0.0033745163]]...]
INFO - root - 2017-12-09 21:43:36.829351: step 62410, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 63h:15m:39s remains)
INFO - root - 2017-12-09 21:43:45.261225: step 62420, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 63h:54m:55s remains)
INFO - root - 2017-12-09 21:43:53.866045: step 62430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 65h:36m:14s remains)
INFO - root - 2017-12-09 21:44:02.412170: step 62440, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:45m:55s remains)
INFO - root - 2017-12-09 21:44:11.069482: step 62450, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 65h:15m:44s remains)
INFO - root - 2017-12-09 21:44:19.684209: step 62460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:30m:45s remains)
INFO - root - 2017-12-09 21:44:28.457120: step 62470, loss = 0.88, batch loss = 0.68 (8.8 examples/sec; 0.912 sec/batch; 68h:23m:42s remains)
INFO - root - 2017-12-09 21:44:37.028418: step 62480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:52m:25s remains)
INFO - root - 2017-12-09 21:44:45.728075: step 62490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:48m:30s remains)
INFO - root - 2017-12-09 21:44:54.480189: step 62500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 66h:47m:23s remains)
2017-12-09 21:44:55.321646: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13343889 0.12936327 0.12494856 0.11986027 0.11470866 0.10966407 0.10560273 0.10247408 0.10055467 0.099241056 0.098352067 0.098590814 0.099368647 0.10056676 0.10129517][0.14104602 0.13675249 0.13243201 0.12799515 0.12372588 0.11985859 0.11651538 0.11378989 0.11168461 0.10986165 0.10805423 0.1065641 0.10592617 0.10644537 0.10719562][0.14051028 0.1367608 0.13290882 0.12908599 0.12600636 0.1232993 0.12106691 0.11927442 0.11757816 0.11549125 0.11275715 0.11001869 0.1081996 0.10797691 0.10892523][0.13484204 0.1317015 0.12840322 0.12547661 0.12371649 0.12268278 0.12193196 0.1215831 0.12083722 0.11905453 0.11584257 0.11206775 0.10917343 0.10799654 0.10903206][0.12548611 0.12319379 0.12074893 0.11894235 0.11862379 0.11901855 0.11995082 0.12115414 0.12163012 0.12035463 0.117163 0.11266756 0.10855947 0.10610224 0.10616826][0.11227535 0.11136598 0.11072473 0.11096363 0.11286804 0.11556271 0.11897492 0.12184536 0.12323983 0.12221515 0.11865242 0.11312871 0.10728284 0.10282437 0.10097782][0.098872513 0.098892763 0.099463567 0.10218863 0.10669904 0.11190137 0.11747316 0.12180458 0.1239868 0.12292397 0.11889265 0.11231162 0.10480647 0.098354526 0.09432067][0.087954827 0.088505343 0.089771554 0.093539312 0.099124946 0.10570744 0.11228929 0.11743014 0.1199686 0.11939499 0.11575397 0.10927542 0.10134554 0.093765937 0.088274479][0.080439895 0.08081083 0.081845731 0.08526285 0.090317182 0.096719138 0.10319087 0.10817885 0.11062714 0.1107854 0.1081758 0.10299786 0.09603288 0.088884994 0.083288521][0.076478064 0.076222286 0.076449275 0.078721687 0.082451262 0.087340288 0.092380367 0.096889921 0.099148072 0.099982038 0.098514527 0.095119722 0.089926817 0.083970875 0.079186767][0.072983265 0.072914243 0.072980739 0.074073777 0.076313108 0.07979349 0.083474793 0.086640261 0.088563107 0.0894044 0.088617623 0.086225331 0.082424536 0.077924065 0.074105822][0.070091933 0.070618495 0.070798323 0.0713618 0.072625466 0.074587017 0.076650463 0.078642137 0.079988472 0.080562145 0.080026746 0.078251183 0.075271264 0.071603879 0.068464458][0.068036482 0.069547482 0.070289306 0.070857331 0.071766511 0.0727445 0.073657185 0.074454226 0.074793763 0.074637242 0.073613904 0.071574256 0.06857498 0.065113075 0.062233694][0.067686327 0.069676541 0.07063622 0.07098946 0.0712415 0.071408123 0.07138969 0.071093626 0.070550047 0.069812194 0.06841623 0.06619864 0.063258834 0.060072288 0.057458032][0.067537658 0.068916894 0.069246329 0.068796344 0.068055794 0.067242838 0.066468887 0.065629557 0.06473051 0.063923769 0.062732466 0.060940184 0.058635917 0.056200039 0.0541997]]...]
INFO - root - 2017-12-09 21:45:04.142829: step 62510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:59m:40s remains)
INFO - root - 2017-12-09 21:45:12.795938: step 62520, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 65h:27m:40s remains)
INFO - root - 2017-12-09 21:45:21.489627: step 62530, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 64h:15m:34s remains)
INFO - root - 2017-12-09 21:45:29.843975: step 62540, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 63h:03m:48s remains)
INFO - root - 2017-12-09 21:45:38.390159: step 62550, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 68h:10m:00s remains)
INFO - root - 2017-12-09 21:45:47.151053: step 62560, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:41m:37s remains)
INFO - root - 2017-12-09 21:45:55.865298: step 62570, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:06m:53s remains)
INFO - root - 2017-12-09 21:46:04.388455: step 62580, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 61h:46m:59s remains)
INFO - root - 2017-12-09 21:46:13.172968: step 62590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:44m:32s remains)
INFO - root - 2017-12-09 21:46:21.878299: step 62600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:47m:21s remains)
2017-12-09 21:46:22.777767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033921169 -0.0033921835 -0.0033910261 -0.0033868868 -0.0033803887 -0.0033743896 -0.0033710445 -0.0033700741 -0.0033707428 -0.0033715551 -0.0033715917 -0.0033707798 -0.0033694333 -0.0033679362 -0.0033665858][-0.0033901315 -0.0033909986 -0.0033909495 -0.0033886102 -0.0033839082 -0.0033789494 -0.0033756753 -0.0033744115 -0.0033747486 -0.0033753766 -0.0033753961 -0.0033745791 -0.0033734073 -0.003372096 -0.0033710247][-0.0033884954 -0.0033898517 -0.0033909099 -0.0033905532 -0.0033881015 -0.0033849429 -0.0033823096 -0.0033808877 -0.0033807433 -0.0033810204 -0.0033806891 -0.0033799375 -0.0033790341 -0.0033781005 -0.00337741][-0.0033854677 -0.0033869115 -0.0033886859 -0.0033895506 -0.0033889068 -0.0033873324 -0.0033854977 -0.0033841617 -0.0033837256 -0.0033837648 -0.0033833981 -0.0033828041 -0.0033821752 -0.0033815035 -0.0033807468][-0.0033812807 -0.0033822812 -0.0033844165 -0.0033861774 -0.0033870377 -0.0033869257 -0.0033859513 -0.0033849862 -0.0033845098 -0.0033845073 -0.0033841664 -0.0033838323 -0.0033834707 -0.0033827643 -0.003381911][-0.0033764543 -0.0033761018 -0.0033778322 -0.0033797952 -0.0033813876 -0.0033822076 -0.0033819946 -0.0033814746 -0.0033810448 -0.0033811 -0.0033808777 -0.0033807522 -0.0033804586 -0.0033795431 -0.0033783733][-0.00337196 -0.0033697379 -0.0033704946 -0.0033718271 -0.003373069 -0.0033739838 -0.0033742795 -0.0033742222 -0.0033740164 -0.0033745335 -0.0033752278 -0.0033759491 -0.0033761906 -0.0033753735 -0.0033741414][-0.0033696475 -0.0033661709 -0.00336589 -0.0033661842 -0.0033665553 -0.0033667835 -0.0033671332 -0.0033677346 -0.0033682953 -0.003369957 -0.0033718923 -0.0033738185 -0.0033748755 -0.0033745035 -0.0033731975][-0.0033688645 -0.0033649986 -0.0033644841 -0.003364102 -0.0033638419 -0.0033635097 -0.0033636491 -0.0033647052 -0.0033661779 -0.0033688662 -0.0033719598 -0.0033748103 -0.0033765321 -0.0033766727 -0.0033754357][-0.0033688515 -0.0033651383 -0.0033648121 -0.0033641832 -0.0033637094 -0.0033630368 -0.0033628934 -0.0033638868 -0.0033657686 -0.0033690289 -0.003372818 -0.0033761521 -0.0033783969 -0.0033789582 -0.0033782809][-0.0033693102 -0.0033657819 -0.0033654391 -0.0033645637 -0.0033637569 -0.0033627979 -0.0033625383 -0.0033634785 -0.0033657048 -0.0033690098 -0.0033729055 -0.0033761694 -0.0033782176 -0.0033787203 -0.00337821][-0.0033699127 -0.0033665949 -0.0033662317 -0.0033653132 -0.0033644726 -0.003363393 -0.0033630838 -0.0033639548 -0.0033660489 -0.0033686513 -0.00337191 -0.0033744099 -0.0033758841 -0.0033762252 -0.0033758311][-0.0033705332 -0.0033675088 -0.0033671234 -0.0033662082 -0.0033654154 -0.0033643362 -0.0033639641 -0.0033643057 -0.0033656685 -0.0033673546 -0.0033695567 -0.003371069 -0.0033719155 -0.0033719947 -0.0033715812][-0.0033707053 -0.0033679346 -0.0033676715 -0.0033667835 -0.0033658843 -0.0033648214 -0.0033640757 -0.0033637555 -0.0033641246 -0.0033650261 -0.0033663858 -0.0033671516 -0.0033674904 -0.0033673753 -0.0033670098][-0.0033707416 -0.0033678331 -0.0033672363 -0.003366305 -0.003365376 -0.0033643877 -0.0033635146 -0.0033630596 -0.0033630668 -0.0033632442 -0.0033637322 -0.0033640414 -0.003364217 -0.0033640333 -0.0033637341]]...]
INFO - root - 2017-12-09 21:46:31.476431: step 62610, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 68h:09m:13s remains)
INFO - root - 2017-12-09 21:46:40.124833: step 62620, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:52m:01s remains)
INFO - root - 2017-12-09 21:46:48.686022: step 62630, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 62h:49m:51s remains)
INFO - root - 2017-12-09 21:46:57.178715: step 62640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:38m:07s remains)
INFO - root - 2017-12-09 21:47:05.834221: step 62650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:34m:45s remains)
INFO - root - 2017-12-09 21:47:14.438080: step 62660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:45m:41s remains)
INFO - root - 2017-12-09 21:47:22.891328: step 62670, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 61h:36m:49s remains)
INFO - root - 2017-12-09 21:47:31.268442: step 62680, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.884 sec/batch; 66h:14m:10s remains)
INFO - root - 2017-12-09 21:47:39.940615: step 62690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:03m:23s remains)
INFO - root - 2017-12-09 21:47:48.397369: step 62700, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 63h:54m:46s remains)
2017-12-09 21:47:49.205139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033106068 -0.0027180759 -0.00096195517 0.0023122753 0.0067716455 0.01138288 0.014813086 0.015652779 0.013587158 0.0095101595 0.0048805056 0.00091651292 -0.0017587903 -0.0030119449 -0.0033450311][-0.0032397374 -0.0022762548 0.00052384171 0.0059063351 0.01352869 0.021816233 0.028241312 0.030272188 0.027132476 0.020153444 0.011946326 0.0047315666 -0.00013191416 -0.0025250162 -0.0032647669][-0.0031260832 -0.0016009719 0.0027135483 0.011049797 0.02306452 0.036535602 0.047458589 0.05174632 0.047769591 0.03717871 0.023881307 0.011619166 0.0030028129 -0.0015175416 -0.0030727747][-0.0029450681 -0.00066448282 0.0057438221 0.018041331 0.035605162 0.055233847 0.071119525 0.077583864 0.0722886 0.057414565 0.038387492 0.02038094 0.0072996439 9.3092676e-06 -0.0027615908][-0.0023345563 0.00067531154 0.0093930429 0.026188821 0.050109942 0.076768063 0.098344281 0.10730463 0.10040909 0.080505461 0.054809194 0.030291015 0.012197586 0.0017896772 -0.0023759743][-0.0011431156 0.0025380214 0.013261978 0.033817541 0.062826261 0.094948754 0.12092994 0.13195442 0.12407003 0.10043582 0.0695183 0.039537434 0.016912058 0.0035410507 -0.001993923][0.00050066807 0.0046112947 0.016464403 0.038987644 0.070502467 0.10506333 0.1328022 0.14447396 0.13593565 0.11053462 0.077189252 0.044645086 0.019684577 0.0046319012 -0.0017552297][0.0020974048 0.0063254628 0.018150518 0.040347703 0.071131378 0.10462624 0.13127685 0.14220802 0.13349538 0.10845906 0.075782709 0.043924574 0.019395871 0.0045577576 -0.0017663534][0.0031599661 0.0070827985 0.017728658 0.037536528 0.064805441 0.094280772 0.11754042 0.12672149 0.11844356 0.095707558 0.066417649 0.038068347 0.016350312 0.0033625627 -0.002068195][0.0034100937 0.0066808434 0.015181202 0.030987641 0.052694805 0.076104261 0.0944468 0.10137159 0.09420418 0.075412035 0.051632181 0.028865596 0.011655139 0.0015693347 -0.0024881361][0.0020612397 0.00473677 0.010851007 0.021977542 0.037168268 0.053533752 0.066264257 0.070782192 0.065224819 0.05140629 0.034324326 0.018258171 0.00638902 -0.00034128828 -0.0028899228][-0.00016566343 0.0016072427 0.0053496445 0.012111582 0.021277456 0.031096572 0.038619544 0.041033018 0.037289977 0.028577691 0.018139422 0.0085767871 0.0017685688 -0.001905217 -0.0031765765][-0.0021925289 -0.0012311006 0.000671244 0.0040232949 0.0085298913 0.013317674 0.016888887 0.017840032 0.015730131 0.011256048 0.0061377073 0.0016599305 -0.0013468133 -0.0028644211 -0.0033227131][-0.0031769592 -0.0028626537 -0.0021686913 -0.00091126212 0.0007792383 0.0025330267 0.0037735503 0.0040020626 0.003094807 0.0013853812 -0.00044571212 -0.0019329009 -0.0028429427 -0.0032659965 -0.0033720317][-0.0033744166 -0.0033304465 -0.0031886757 -0.0029020356 -0.002500318 -0.0020938765 -0.0018345771 -0.0018388636 -0.0021001217 -0.0025085288 -0.002901874 -0.0031781024 -0.0033186318 -0.0033732695 -0.003381982]]...]
INFO - root - 2017-12-09 21:47:57.801832: step 62710, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 62h:55m:26s remains)
INFO - root - 2017-12-09 21:48:06.193899: step 62720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:09m:06s remains)
INFO - root - 2017-12-09 21:48:14.780155: step 62730, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:55m:45s remains)
INFO - root - 2017-12-09 21:48:23.253732: step 62740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:52m:54s remains)
INFO - root - 2017-12-09 21:48:31.776572: step 62750, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:12m:03s remains)
INFO - root - 2017-12-09 21:48:40.351344: step 62760, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 60h:37m:21s remains)
INFO - root - 2017-12-09 21:48:49.073556: step 62770, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 66h:00m:23s remains)
INFO - root - 2017-12-09 21:48:57.546505: step 62780, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 65h:55m:38s remains)
INFO - root - 2017-12-09 21:49:06.037851: step 62790, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 62h:20m:45s remains)
INFO - root - 2017-12-09 21:49:14.534852: step 62800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 65h:16m:19s remains)
2017-12-09 21:49:15.460834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033667402 -0.0033644359 -0.0033642142 -0.0033641953 -0.003364221 -0.0033643055 -0.0033643898 -0.0033645236 -0.0033646498 -0.0033646005 -0.0033645621 -0.0033645253 -0.003364515 -0.0033645707 -0.0033646331][-0.0033627646 -0.0033600105 -0.0033597962 -0.0033597758 -0.0033597483 -0.0033596943 -0.0033596121 -0.0033596074 -0.003359626 -0.0033596233 -0.00335967 -0.003359796 -0.0033599811 -0.003360146 -0.0033602912][-0.0033610323 -0.0033578558 -0.0033575085 -0.0033573341 -0.0033571471 -0.0033569378 -0.0033566584 -0.0033564838 -0.0033563348 -0.0033561403 -0.0033559767 -0.0033559832 -0.0033562165 -0.0033564826 -0.0033568256][-0.003359901 -0.0033565317 -0.003356101 -0.0033558425 -0.0033555334 -0.003355318 -0.003355098 -0.0033548605 -0.0033545347 -0.0033541364 -0.0033537396 -0.0033535433 -0.0033536605 -0.003353935 -0.0033544227][-0.0033598223 -0.003356419 -0.0033561203 -0.0033559075 -0.0033555387 -0.0033552856 -0.0033550442 -0.0033548218 -0.003354342 -0.0033537308 -0.0033531038 -0.0033526123 -0.0033523906 -0.0033524376 -0.0033528972][-0.0033606503 -0.0033573173 -0.003357097 -0.0033569438 -0.0033567138 -0.0033566246 -0.0033565795 -0.00335649 -0.0033559601 -0.0033551734 -0.0033543508 -0.0033535575 -0.0033528898 -0.0033524584 -0.0033525745][-0.0033618887 -0.0033587127 -0.0033585872 -0.0033585674 -0.0033586402 -0.0033589338 -0.0033592621 -0.0033594789 -0.0033589941 -0.0033580989 -0.0033570211 -0.0033559327 -0.0033548572 -0.0033541033 -0.0033540337][-0.0033629497 -0.0033600507 -0.00336014 -0.0033603688 -0.0033607676 -0.0033614112 -0.0033619588 -0.0033623918 -0.0033621422 -0.0033612619 -0.0033599918 -0.0033585129 -0.003357 -0.0033559115 -0.0033556309][-0.0033627481 -0.0033601194 -0.0033604195 -0.0033609031 -0.0033615083 -0.0033622463 -0.0033628778 -0.003363549 -0.0033636705 -0.0033631313 -0.003362139 -0.0033606975 -0.0033589993 -0.0033576265 -0.0033570707][-0.0033616705 -0.0033591792 -0.0033595436 -0.0033602605 -0.0033611313 -0.0033618973 -0.0033624244 -0.0033630729 -0.0033634412 -0.0033633558 -0.0033628582 -0.0033619495 -0.0033606279 -0.0033593536 -0.0033586156][-0.0033608906 -0.0033582845 -0.0033586328 -0.0033592596 -0.0033600654 -0.0033606754 -0.0033610191 -0.0033614635 -0.003361854 -0.003361987 -0.0033617988 -0.0033614158 -0.0033608575 -0.0033602514 -0.0033598056][-0.0033596503 -0.0033571133 -0.0033573501 -0.0033577371 -0.0033582558 -0.0033586472 -0.0033587606 -0.0033589378 -0.0033592791 -0.0033595352 -0.0033595045 -0.0033595329 -0.0033596789 -0.003359807 -0.0033599185][-0.0033590128 -0.0033563103 -0.003356627 -0.0033571012 -0.0033575427 -0.0033576847 -0.0033574819 -0.0033573227 -0.0033574381 -0.0033575022 -0.0033574724 -0.0033577669 -0.0033583564 -0.0033589581 -0.0033595532][-0.003359626 -0.0033567571 -0.003357071 -0.0033575511 -0.00335783 -0.003357704 -0.0033573371 -0.0033569552 -0.0033568877 -0.0033568426 -0.0033569294 -0.0033574395 -0.0033581862 -0.0033589702 -0.0033596645][-0.0033610431 -0.0033581061 -0.0033584728 -0.003358806 -0.0033589022 -0.0033587581 -0.003358461 -0.0033580093 -0.0033577539 -0.0033575823 -0.0033575604 -0.0033578794 -0.0033584672 -0.003359203 -0.0033597879]]...]
INFO - root - 2017-12-09 21:49:24.129406: step 62810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:03m:13s remains)
INFO - root - 2017-12-09 21:49:32.690691: step 62820, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 57h:36m:54s remains)
INFO - root - 2017-12-09 21:49:41.127460: step 62830, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 66h:25m:01s remains)
INFO - root - 2017-12-09 21:49:49.526454: step 62840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:10m:43s remains)
INFO - root - 2017-12-09 21:49:58.168006: step 62850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:46m:20s remains)
INFO - root - 2017-12-09 21:50:06.761010: step 62860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 64h:57m:59s remains)
INFO - root - 2017-12-09 21:50:15.444591: step 62870, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:14m:03s remains)
INFO - root - 2017-12-09 21:50:23.980001: step 62880, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:19m:14s remains)
INFO - root - 2017-12-09 21:50:32.647020: step 62890, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 62h:38m:39s remains)
INFO - root - 2017-12-09 21:50:41.354817: step 62900, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.867 sec/batch; 64h:57m:28s remains)
2017-12-09 21:50:42.275392: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45100805 0.46277809 0.468369 0.46972358 0.46915075 0.46624145 0.4599646 0.44992134 0.43609747 0.42131165 0.40578708 0.3923074 0.3825703 0.3782917 0.37736946][0.4138284 0.43175673 0.4430494 0.44947883 0.4531258 0.45263797 0.44859973 0.43985713 0.42718679 0.41278261 0.39708886 0.38398245 0.3744185 0.37046459 0.37016532][0.34300554 0.36557725 0.38260081 0.39609987 0.40749273 0.41474292 0.41771203 0.41466871 0.40635145 0.3934527 0.37724161 0.36260381 0.35151324 0.34748811 0.34793627][0.26365903 0.28920767 0.31172088 0.33357343 0.35435504 0.37226525 0.38507283 0.3900786 0.387322 0.37610519 0.35864973 0.34027448 0.32533613 0.31817439 0.31693682][0.1850844 0.21151504 0.23812966 0.26753107 0.29852116 0.32792506 0.35114583 0.3644135 0.36663166 0.35596129 0.33547705 0.31178904 0.29147127 0.27863157 0.27380058][0.11835148 0.14199294 0.16954215 0.20388135 0.24203673 0.28034422 0.3125436 0.33289871 0.3383604 0.32626721 0.30213776 0.27274305 0.2463752 0.2283724 0.22053392][0.068897843 0.088045172 0.11333267 0.14812304 0.18877013 0.23130381 0.26791009 0.29183489 0.29882807 0.28562602 0.25839975 0.224139 0.19340441 0.1714149 0.160787][0.036856741 0.050707571 0.072050475 0.10389108 0.1419833 0.18301216 0.21899191 0.24289866 0.2486732 0.23375791 0.20563248 0.17065807 0.13870116 0.11469558 0.10289154][0.019539813 0.028743291 0.045346871 0.071817271 0.10388517 0.13893779 0.16954125 0.18956164 0.19225062 0.17651132 0.14969088 0.11735843 0.08822488 0.065997727 0.05498267][0.010468078 0.016520394 0.028721457 0.0487885 0.073220305 0.0998836 0.12276497 0.13679118 0.13611892 0.12091303 0.097692274 0.071269363 0.048057713 0.030501811 0.02208754][0.0051916419 0.0092546791 0.017864076 0.032040898 0.049211171 0.067535922 0.082814239 0.09117797 0.088356808 0.075292356 0.057302184 0.038074486 0.021740517 0.0099913925 0.0046890397][0.0010506769 0.0037672219 0.0095835915 0.019049648 0.03060556 0.042479649 0.051814694 0.056065213 0.052776076 0.043038305 0.03051989 0.017871985 0.0077496115 0.0010187791 -0.0016847398][-0.0015834948 -0.00010712189 0.0033542986 0.00926115 0.016647143 0.023901662 0.029208364 0.031113865 0.028527094 0.022186464 0.014289614 0.0067846132 0.001278179 -0.001942403 -0.0030952836][-0.0028988854 -0.0022512977 -0.00059015956 0.0024415574 0.0064510526 0.010330169 0.012961814 0.013725661 0.01226156 0.0089176046 0.0047028419 0.00084964535 -0.0017099413 -0.0029644188 -0.0032919764][-0.00328147 -0.0031385887 -0.0025755884 -0.0014269578 0.00015458488 0.0016950669 0.0027271474 0.0030251925 0.0024762859 0.0012025589 -0.00045351521 -0.0019419853 -0.0028485628 -0.0032248548 -0.0032942351]]...]
INFO - root - 2017-12-09 21:50:50.808910: step 62910, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 65h:29m:26s remains)
INFO - root - 2017-12-09 21:50:59.578970: step 62920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:45m:44s remains)
INFO - root - 2017-12-09 21:51:08.048678: step 62930, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 62h:52m:47s remains)
INFO - root - 2017-12-09 21:51:16.455654: step 62940, loss = 0.90, batch loss = 0.70 (9.6 examples/sec; 0.834 sec/batch; 62h:28m:39s remains)
INFO - root - 2017-12-09 21:51:24.987939: step 62950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:59m:08s remains)
INFO - root - 2017-12-09 21:51:33.727173: step 62960, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:10m:57s remains)
INFO - root - 2017-12-09 21:51:42.415120: step 62970, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 64h:14m:07s remains)
INFO - root - 2017-12-09 21:51:50.931907: step 62980, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 62h:50m:01s remains)
INFO - root - 2017-12-09 21:51:59.604281: step 62990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:05m:48s remains)
INFO - root - 2017-12-09 21:52:07.931184: step 63000, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:44m:51s remains)
2017-12-09 21:52:08.755220: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070201509 0.070883192 0.072903655 0.078567468 0.0871534 0.09762352 0.10921939 0.12104329 0.13217139 0.14139824 0.14864206 0.15490793 0.15994728 0.16457051 0.16818607][0.06087409 0.060606003 0.061693266 0.066896312 0.0750653 0.085875928 0.097563893 0.10871688 0.119101 0.12734163 0.13279743 0.1369403 0.13999873 0.14290601 0.14468676][0.053444553 0.054454375 0.056901425 0.061850555 0.06895186 0.079074 0.08947853 0.099668071 0.10821483 0.11526112 0.11958861 0.12140071 0.12211877 0.12197067 0.12111635][0.044446185 0.046566356 0.050304256 0.056945872 0.065574691 0.075783141 0.086091153 0.094707079 0.10117608 0.10525762 0.10689347 0.10599057 0.10420743 0.10206266 0.099677585][0.043355815 0.046312146 0.050475061 0.057695281 0.066763841 0.076907046 0.086389631 0.093574673 0.098751023 0.10074271 0.099676967 0.096304551 0.092201278 0.087745383 0.083639294][0.054321647 0.057660434 0.062138513 0.070116207 0.079791792 0.090358019 0.099943027 0.10616305 0.1099994 0.10988384 0.10676277 0.10086729 0.094099775 0.0874192 0.0818777][0.0725189 0.078123227 0.084714651 0.094611481 0.10590418 0.11775584 0.12747362 0.13333356 0.13603075 0.13417384 0.12926587 0.12127355 0.11280049 0.10414366 0.09715946][0.091578119 0.098196745 0.10536546 0.11562349 0.12743548 0.13933522 0.14891058 0.15489049 0.15742311 0.1557612 0.15120631 0.14319451 0.13405165 0.12452358 0.1164457][0.1074093 0.11406783 0.12095641 0.13170363 0.14398031 0.15603542 0.1658076 0.17212687 0.17456868 0.17322339 0.16845122 0.16084462 0.15179323 0.14223327 0.13422549][0.12308325 0.12981831 0.13537571 0.14437144 0.15495536 0.166119 0.17538896 0.18071052 0.18255892 0.18121091 0.17717223 0.16992573 0.16188994 0.15393378 0.14739411][0.13577333 0.14210474 0.14652041 0.15361007 0.1619669 0.17145956 0.17915517 0.18434185 0.18683721 0.18624483 0.18311471 0.17686315 0.17016579 0.16321468 0.15765654][0.14775875 0.15299049 0.15542004 0.15985967 0.16545939 0.17202215 0.17736867 0.18201375 0.18374653 0.18413265 0.18279023 0.17928858 0.17537914 0.17006153 0.16631678][0.15382762 0.157282 0.15766115 0.15949394 0.16216941 0.16564491 0.16874476 0.17221023 0.17328632 0.17446244 0.17462583 0.17334615 0.17169496 0.16899602 0.16748977][0.14826284 0.15101233 0.15010902 0.15028292 0.15116698 0.15276183 0.15432039 0.15620452 0.15677005 0.15785098 0.15803526 0.15795426 0.15784337 0.15738811 0.15719895][0.12977117 0.13145415 0.13005899 0.12917824 0.12887084 0.12934542 0.12988731 0.13085307 0.13113348 0.13186848 0.13213228 0.13246958 0.13291466 0.13311748 0.13346137]]...]
INFO - root - 2017-12-09 21:52:17.332358: step 63010, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 63h:35m:50s remains)
INFO - root - 2017-12-09 21:52:25.988083: step 63020, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 64h:23m:33s remains)
INFO - root - 2017-12-09 21:52:34.366706: step 63030, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 62h:36m:15s remains)
INFO - root - 2017-12-09 21:52:42.927747: step 63040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:15m:12s remains)
INFO - root - 2017-12-09 21:52:51.457706: step 63050, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 63h:26m:22s remains)
INFO - root - 2017-12-09 21:53:00.041615: step 63060, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:12m:32s remains)
INFO - root - 2017-12-09 21:53:08.603947: step 63070, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:34m:03s remains)
INFO - root - 2017-12-09 21:53:17.243925: step 63080, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:59m:08s remains)
INFO - root - 2017-12-09 21:53:25.959748: step 63090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:19m:07s remains)
INFO - root - 2017-12-09 21:53:34.638270: step 63100, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:17m:50s remains)
2017-12-09 21:53:35.547377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003140714 -0.0032266285 -0.0032982598 -0.0033480544 -0.0033741558 -0.0033849592 -0.0033889562 -0.0033904498 -0.0033905543 -0.0033901536 -0.0033892712 -0.0033887341 -0.0033882875 -0.0033880877 -0.0033880302][-0.0030062394 -0.003138301 -0.0032488937 -0.0033242903 -0.0033633532 -0.0033791286 -0.0033854486 -0.0033879643 -0.0033887436 -0.0033886409 -0.003388047 -0.0033876968 -0.0033869902 -0.0033864975 -0.0033861825][-0.0029467265 -0.0031110323 -0.0032404221 -0.0033199419 -0.003355111 -0.0033660165 -0.003371269 -0.0033766837 -0.0033821047 -0.0033862661 -0.0033883024 -0.0033889522 -0.0033883352 -0.0033876197 -0.0033868302][-0.0029799275 -0.0031445646 -0.0032611138 -0.0033181706 -0.0033271138 -0.0033154876 -0.0033090732 -0.003317557 -0.0033385158 -0.0033619122 -0.0033791056 -0.0033879681 -0.0033901038 -0.0033896933 -0.0033883746][-0.0030873748 -0.0032169803 -0.0032899927 -0.0032979071 -0.003253038 -0.0031914336 -0.0031541144 -0.0031647836 -0.003218919 -0.0032882264 -0.0033448082 -0.0033771035 -0.0033891553 -0.0033910284 -0.0033895802][-0.0032192061 -0.0032943985 -0.0033129919 -0.0032610858 -0.0031459436 -0.003011523 -0.0029241303 -0.0029309597 -0.003028356 -0.0031646583 -0.0032824345 -0.0033537345 -0.0033833683 -0.0033901676 -0.0033898184][-0.0033236153 -0.0033514809 -0.0033291767 -0.00323294 -0.0030622846 -0.0028658346 -0.0027323931 -0.0027305912 -0.0028600125 -0.0030513159 -0.0032226252 -0.0033297017 -0.0033761975 -0.0033887278 -0.0033897196][-0.0033797075 -0.0033815529 -0.0033440653 -0.0032396861 -0.0030613302 -0.0028530534 -0.0027063417 -0.0026975661 -0.0028298709 -0.0030303637 -0.0032111083 -0.0033240381 -0.0033739586 -0.0033881715 -0.00338979][-0.003398563 -0.003392606 -0.0033603641 -0.0032782359 -0.0031370509 -0.0029661364 -0.0028402493 -0.0028295664 -0.0029390461 -0.0031054106 -0.0032511409 -0.0033389246 -0.0033770911 -0.00338797 -0.0033892235][-0.0034025556 -0.0033981006 -0.0033785778 -0.0033277054 -0.0032364167 -0.0031203665 -0.0030310904 -0.0030236603 -0.0031011908 -0.0032151095 -0.0033089337 -0.003361746 -0.0033835173 -0.0033893441 -0.0033894179][-0.0034012713 -0.0034005125 -0.0033932487 -0.0033683139 -0.0033182479 -0.0032508029 -0.0031969137 -0.0031929463 -0.0032400996 -0.0033055805 -0.0033555382 -0.0033811079 -0.0033902873 -0.003391518 -0.0033902598][-0.0033983774 -0.0033995821 -0.0033998135 -0.0033920356 -0.0033708545 -0.0033384627 -0.0033116625 -0.0033089689 -0.0033307381 -0.0033601746 -0.0033809086 -0.0033901096 -0.0033922896 -0.0033915367 -0.0033901124][-0.0033953413 -0.0033966603 -0.0033994715 -0.0033999037 -0.0033947385 -0.0033836106 -0.0033731754 -0.0033704152 -0.0033761032 -0.0033848088 -0.0033906936 -0.0033922638 -0.0033914843 -0.0033900281 -0.0033888083][-0.0033929069 -0.0033933176 -0.003395915 -0.0033983861 -0.0033994492 -0.0033977346 -0.0033949723 -0.0033930023 -0.0033926447 -0.00339293 -0.0033930002 -0.0033922468 -0.0033907278 -0.0033890419 -0.0033880502][-0.003390698 -0.0033900933 -0.003391752 -0.003393854 -0.0033954561 -0.0033963423 -0.0033964626 -0.0033958922 -0.0033950508 -0.0033938184 -0.0033925974 -0.0033914291 -0.0033900193 -0.0033886067 -0.003387697]]...]
INFO - root - 2017-12-09 21:53:44.148336: step 63110, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:49m:35s remains)
INFO - root - 2017-12-09 21:53:52.747385: step 63120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:25m:54s remains)
INFO - root - 2017-12-09 21:54:01.281693: step 63130, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 66h:02m:47s remains)
INFO - root - 2017-12-09 21:54:09.864923: step 63140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:53m:07s remains)
INFO - root - 2017-12-09 21:54:18.628011: step 63150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:57m:01s remains)
INFO - root - 2017-12-09 21:54:27.320902: step 63160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:47m:54s remains)
INFO - root - 2017-12-09 21:54:36.090894: step 63170, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 65h:04m:15s remains)
INFO - root - 2017-12-09 21:54:44.796823: step 63180, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 62h:54m:53s remains)
INFO - root - 2017-12-09 21:54:53.485903: step 63190, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:33m:48s remains)
INFO - root - 2017-12-09 21:55:02.248092: step 63200, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 63h:58m:18s remains)
2017-12-09 21:55:03.174934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034077493 -0.0034060343 -0.0034046217 -0.0034030455 -0.0034011286 -0.0034003705 -0.0034010157 -0.0034011842 -0.0034005363 -0.0033998112 -0.00339948 -0.0033988955 -0.0033986538 -0.0033985893 -0.0033988308][-0.0034043486 -0.003401564 -0.0033987381 -0.0033964335 -0.0033953618 -0.0033954729 -0.0033959083 -0.0033971451 -0.0033968615 -0.003395682 -0.0033948056 -0.003394099 -0.0033941811 -0.0033945423 -0.0033949451][-0.0034026052 -0.0033994333 -0.0033955609 -0.0033926133 -0.0033916968 -0.003393207 -0.0033955411 -0.0033963909 -0.0033960454 -0.0033956934 -0.0033951669 -0.0033940442 -0.0033945341 -0.0033943567 -0.0033948114][-0.0033911124 -0.0033911644 -0.0033894319 -0.003386888 -0.0033857631 -0.0033867578 -0.003388677 -0.0033899371 -0.0033906631 -0.0033905108 -0.0033918752 -0.0033926298 -0.0033944924 -0.0033951188 -0.0033951793][-0.0033765244 -0.0033760173 -0.0033743295 -0.0033715162 -0.0033696354 -0.0033705428 -0.0033745256 -0.0033782893 -0.0033815352 -0.0033843976 -0.0033895776 -0.003392088 -0.003394332 -0.0033937618 -0.0033941963][-0.0033614568 -0.0033598021 -0.0033573415 -0.0033539669 -0.003351694 -0.0033523138 -0.0033562477 -0.0033620892 -0.0033677942 -0.0033737184 -0.0033794933 -0.0033845112 -0.0033880267 -0.0033888239 -0.0033891676][-0.0033491172 -0.0033459233 -0.0033439258 -0.0033417023 -0.0033410653 -0.0033421062 -0.0033452257 -0.0033502008 -0.0033562684 -0.0033630158 -0.0033687905 -0.003374987 -0.0033794197 -0.0033805994 -0.0033814074][-0.0033413258 -0.0033396124 -0.0033387984 -0.0033379069 -0.0033400955 -0.0033430213 -0.0033463007 -0.0033501554 -0.0033548472 -0.0033604726 -0.0033662848 -0.0033708371 -0.0033748106 -0.0033790411 -0.0033801605][-0.0033326955 -0.0033347663 -0.0033390368 -0.0033424641 -0.0033464702 -0.0033498134 -0.0033540525 -0.0033569068 -0.00335879 -0.003362281 -0.0033673211 -0.0033716636 -0.0033759412 -0.0033823412 -0.0033843925][-0.0033269206 -0.0033296898 -0.0033369642 -0.0033448418 -0.0033535145 -0.0033599155 -0.0033636768 -0.0033654529 -0.003365349 -0.0033665744 -0.0033693211 -0.0033742045 -0.0033799908 -0.0033869722 -0.0033910307][-0.0033235874 -0.0033280367 -0.0033377446 -0.0033518551 -0.00336223 -0.003370089 -0.0033757021 -0.0033762055 -0.0033749677 -0.0033720739 -0.0033717991 -0.0033754089 -0.0033832176 -0.0033913392 -0.0033970198][-0.003324969 -0.0033310319 -0.0033445577 -0.003361555 -0.0033732632 -0.003379812 -0.0033828332 -0.0033818325 -0.0033825831 -0.0033776127 -0.003374309 -0.0033773584 -0.0033835364 -0.0033911404 -0.0033980124][-0.0033275313 -0.0033356638 -0.0033492341 -0.0033698303 -0.0033819424 -0.0033883622 -0.0033927404 -0.0033939288 -0.0033897148 -0.0033786821 -0.0033720881 -0.0033724073 -0.003377409 -0.0033847145 -0.0033957239][-0.003330213 -0.0033385244 -0.0033537864 -0.0033730925 -0.0033853538 -0.003391475 -0.003395075 -0.0033956449 -0.0033901664 -0.003380148 -0.0033694801 -0.0033636221 -0.0033646161 -0.0033742208 -0.0033881168][-0.0033294482 -0.0033395824 -0.0033551008 -0.0033713032 -0.00338327 -0.0033886614 -0.0033906689 -0.0033877071 -0.0033797184 -0.0033694378 -0.0033574488 -0.0033508164 -0.0033507922 -0.0033593087 -0.0033772255]]...]
INFO - root - 2017-12-09 21:55:11.808870: step 63210, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 67h:24m:42s remains)
INFO - root - 2017-12-09 21:55:20.599929: step 63220, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 68h:04m:12s remains)
INFO - root - 2017-12-09 21:55:29.018300: step 63230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:29m:45s remains)
INFO - root - 2017-12-09 21:55:37.745243: step 63240, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 66h:10m:11s remains)
INFO - root - 2017-12-09 21:55:46.449318: step 63250, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 65h:48m:34s remains)
INFO - root - 2017-12-09 21:55:54.889458: step 63260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:35m:26s remains)
INFO - root - 2017-12-09 21:56:03.560856: step 63270, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 65h:51m:58s remains)
INFO - root - 2017-12-09 21:56:11.993609: step 63280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 64h:18m:05s remains)
INFO - root - 2017-12-09 21:56:20.673294: step 63290, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:29m:24s remains)
INFO - root - 2017-12-09 21:56:29.373051: step 63300, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 67h:27m:06s remains)
2017-12-09 21:56:30.248884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00085317018 0.0021627054 0.006430814 0.011581189 0.017789559 0.025737321 0.03469041 0.041961972 0.043993853 0.038485415 0.02744199 0.015157094 0.0056138551 -1.0145828e-05 -0.0024766051][-0.0013149192 0.0022006456 0.0082669361 0.016590573 0.026395718 0.037123755 0.047253203 0.054127552 0.054642968 0.046976618 0.033578105 0.019052187 0.0077300677 0.00094898627 -0.0021346319][-0.0013287808 0.0031488349 0.011553477 0.023468312 0.037073739 0.050351359 0.06106171 0.066896975 0.065549105 0.055740811 0.04018556 0.02351946 0.010321013 0.0022023672 -0.001658715][-0.00049212226 0.0053643985 0.016307428 0.031645361 0.048723377 0.064226083 0.075259544 0.079832867 0.076462135 0.064477473 0.046873778 0.02821294 0.013177888 0.0036693437 -0.0010602796][0.0012040904 0.008705575 0.022039466 0.040122457 0.059748888 0.076857962 0.0881179 0.091656774 0.08657214 0.072670855 0.053217206 0.032734182 0.015981 0.0051469235 -0.00043285149][0.0031263565 0.0120652 0.027140629 0.046829224 0.067696787 0.085551485 0.096892551 0.099737212 0.093400836 0.078123949 0.057365302 0.035666261 0.017798623 0.0061079152 -2.1070009e-05][0.0044419505 0.013894167 0.029356213 0.049055256 0.069614485 0.087162368 0.098287612 0.10078064 0.0938645 0.078105271 0.05706469 0.035345107 0.01757977 0.0059921183 -7.0148148e-05][0.0046360595 0.013243733 0.027285246 0.045100074 0.063652784 0.079678737 0.090096191 0.09250699 0.085854314 0.070871182 0.051092625 0.031046042 0.014960801 0.0046542538 -0.0006189628][0.0035466275 0.010291073 0.02144908 0.035770874 0.050825406 0.06413728 0.073160857 0.075545132 0.069989793 0.057150435 0.040295675 0.023587663 0.010572401 0.0025023415 -0.0014529608][0.0015924305 0.006049416 0.013684284 0.023768708 0.03458672 0.044432715 0.051426616 0.053577486 0.049574535 0.039866328 0.02717313 0.014904222 0.0056940466 0.00024404563 -0.0022629676][-0.00044705789 0.0019938622 0.0063889083 0.012466392 0.019226758 0.025615817 0.030387817 0.032089498 0.029635726 0.023256792 0.014951225 0.0071612829 0.0015769259 -0.0015292584 -0.00284173][-0.001998109 -0.00092155626 0.0011513412 0.0042115487 0.0077997791 0.011362018 0.014176823 0.015328493 0.014079666 0.010507019 0.0058792597 0.0016929747 -0.0011459056 -0.0026051709 -0.003156862][-0.0029008295 -0.0025232185 -0.0017529746 -0.00052434416 0.001027962 0.0026753678 0.0040670093 0.0047145095 0.0042091385 0.0025606875 0.00043918844 -0.0014007813 -0.002571058 -0.0031181013 -0.0032960828][-0.0033016209 -0.0031910012 -0.0029696319 -0.0025948291 -0.0020801213 -0.0014856225 -0.00094518438 -0.00066441996 -0.000818009 -0.001409502 -0.0021662908 -0.0027916366 -0.0031606012 -0.0033133102 -0.0033514823][-0.0033967397 -0.0033842756 -0.0033480071 -0.0032695404 -0.0031456116 -0.0029874966 -0.0028343792 -0.0027464831 -0.0027757457 -0.0029223666 -0.0031100432 -0.0032593592 -0.0033419998 -0.0033720993 -0.0033746317]]...]
INFO - root - 2017-12-09 21:56:38.873838: step 63310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:11m:47s remains)
INFO - root - 2017-12-09 21:56:47.461978: step 63320, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 62h:50m:43s remains)
INFO - root - 2017-12-09 21:56:55.966581: step 63330, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 62h:13m:19s remains)
INFO - root - 2017-12-09 21:57:04.439341: step 63340, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 63h:41m:17s remains)
INFO - root - 2017-12-09 21:57:12.989128: step 63350, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 63h:55m:05s remains)
INFO - root - 2017-12-09 21:57:21.607124: step 63360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:17m:24s remains)
INFO - root - 2017-12-09 21:57:30.323683: step 63370, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.817 sec/batch; 61h:05m:27s remains)
INFO - root - 2017-12-09 21:57:38.829085: step 63380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:28m:21s remains)
INFO - root - 2017-12-09 21:57:47.560197: step 63390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:15m:14s remains)
INFO - root - 2017-12-09 21:57:56.144326: step 63400, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 62h:26m:59s remains)
2017-12-09 21:57:57.012602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033870738 -0.0033854502 -0.003385223 -0.0033857431 -0.0033870453 -0.0033890456 -0.00339117 -0.0033930345 -0.0033939374 -0.0033935667 -0.0033920826 -0.0033899292 -0.0033878635 -0.0033862134 -0.0033850917][-0.0033842956 -0.0033825492 -0.0033823419 -0.0033825589 -0.0033833391 -0.0033846793 -0.0033862218 -0.0033876917 -0.003388626 -0.0033888512 -0.0033883601 -0.0033872586 -0.0033860202 -0.003384877 -0.0033840092][-0.0033840467 -0.0033818486 -0.003381534 -0.0033815925 -0.0033819424 -0.0033825263 -0.0033834542 -0.0033844886 -0.0033853059 -0.0033857259 -0.0033858384 -0.0033855208 -0.0033849457 -0.0033843494 -0.0033838551][-0.0033854123 -0.0033826581 -0.0033819806 -0.0033816872 -0.0033817049 -0.0033818095 -0.0033822036 -0.0033828707 -0.00338349 -0.0033839936 -0.0033843711 -0.0033844116 -0.0033842386 -0.0033840253 -0.0033838151][-0.0033887674 -0.0033855482 -0.0033843245 -0.0033835007 -0.0033829913 -0.0033827028 -0.0033827394 -0.0033831683 -0.0033836779 -0.0033841713 -0.0033846386 -0.0033849648 -0.0033849333 -0.0033847902 -0.0033846821][-0.0033935844 -0.0033901588 -0.0033883045 -0.0033867615 -0.0033856537 -0.0033849387 -0.0033846342 -0.0033847471 -0.0033851762 -0.0033856891 -0.0033862928 -0.0033866898 -0.0033866556 -0.0033863347 -0.0033861303][-0.0033987414 -0.0033956342 -0.0033933746 -0.0033911054 -0.0033894787 -0.0033882689 -0.0033874316 -0.0033867534 -0.0033866665 -0.0033870686 -0.0033878535 -0.0033882135 -0.0033881983 -0.0033880349 -0.0033878619][-0.003403197 -0.0034006669 -0.0033986308 -0.003396055 -0.0033941416 -0.0033923606 -0.0033906857 -0.0033893194 -0.0033884796 -0.0033885364 -0.0033890884 -0.0033893418 -0.0033893394 -0.003389247 -0.00338925][-0.0034055358 -0.0034041167 -0.0034029931 -0.0034008906 -0.0033990135 -0.0033970182 -0.0033946603 -0.0033928119 -0.0033915297 -0.0033906412 -0.0033903888 -0.0033901697 -0.0033901907 -0.0033901571 -0.0033900081][-0.0034055966 -0.0034053365 -0.0034054155 -0.0034042133 -0.00340278 -0.0034010063 -0.0033985486 -0.0033961232 -0.0033943309 -0.0033926682 -0.00339151 -0.0033907483 -0.0033905404 -0.0033903485 -0.0033900188][-0.0034027658 -0.0034033128 -0.0034045549 -0.0034044979 -0.0034039868 -0.0034027172 -0.0034004273 -0.0033977807 -0.0033956037 -0.0033934822 -0.0033918407 -0.0033906079 -0.0033898556 -0.0033893529 -0.0033888163][-0.0033974664 -0.0033983453 -0.0034003032 -0.0034013609 -0.0034017698 -0.0034012296 -0.0033995847 -0.00339737 -0.0033949558 -0.0033926298 -0.0033908084 -0.0033893907 -0.0033882794 -0.003387508 -0.0033868947][-0.0033917842 -0.0033923511 -0.0033943807 -0.0033959507 -0.00339688 -0.0033969854 -0.0033961912 -0.0033945707 -0.0033924892 -0.003390332 -0.0033886109 -0.0033873224 -0.0033862898 -0.0033854479 -0.0033847711][-0.0033871271 -0.0033868866 -0.0033885541 -0.0033900212 -0.0033910652 -0.0033914335 -0.0033910437 -0.0033899366 -0.0033884088 -0.0033867951 -0.003385474 -0.0033845534 -0.0033838367 -0.0033832171 -0.0033827764][-0.0033838977 -0.0033827969 -0.0033838307 -0.0033847322 -0.0033854456 -0.0033856444 -0.0033854228 -0.0033847997 -0.0033839208 -0.0033830819 -0.0033823648 -0.0033818909 -0.0033816069 -0.0033814129 -0.00338135]]...]
INFO - root - 2017-12-09 21:58:05.805373: step 63410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:03m:52s remains)
INFO - root - 2017-12-09 21:58:14.469694: step 63420, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:38m:10s remains)
INFO - root - 2017-12-09 21:58:23.012257: step 63430, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.904 sec/batch; 67h:31m:47s remains)
INFO - root - 2017-12-09 21:58:31.522423: step 63440, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:35m:39s remains)
INFO - root - 2017-12-09 21:58:40.170813: step 63450, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:17m:12s remains)
INFO - root - 2017-12-09 21:58:48.831766: step 63460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:38m:22s remains)
INFO - root - 2017-12-09 21:58:57.398641: step 63470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:06m:33s remains)
INFO - root - 2017-12-09 21:59:05.855524: step 63480, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:26m:04s remains)
INFO - root - 2017-12-09 21:59:14.522187: step 63490, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 61h:36m:37s remains)
INFO - root - 2017-12-09 21:59:23.336151: step 63500, loss = 0.89, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 68h:39m:15s remains)
2017-12-09 21:59:24.234055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033829059 -0.003377985 -0.0033739523 -0.0033696387 -0.0033661961 -0.0033641879 -0.0033638272 -0.0033650668 -0.0033673847 -0.0033703248 -0.0033730969 -0.0033756015 -0.0033778534 -0.0033797531 -0.0033815552][-0.0033846565 -0.0033796669 -0.0033749291 -0.003369652 -0.003365739 -0.0033637879 -0.0033635797 -0.0033654079 -0.0033687591 -0.0033726895 -0.0033763817 -0.0033790688 -0.0033810849 -0.0033827152 -0.0033843943][-0.0033872607 -0.0033826558 -0.0033776355 -0.0033717786 -0.0033672317 -0.0033648917 -0.0033649751 -0.0033671574 -0.0033709845 -0.0033754846 -0.0033791449 -0.0033823112 -0.00338448 -0.003386074 -0.0033878756][-0.0033892631 -0.0033852092 -0.0033802281 -0.0033740778 -0.0033687681 -0.0033657968 -0.0033659732 -0.0033684957 -0.003373031 -0.0033775598 -0.0033808381 -0.0033839028 -0.0033858928 -0.0033872607 -0.0033884461][-0.0033901487 -0.0033863166 -0.0033819103 -0.0033758329 -0.0033700278 -0.0033666615 -0.00336721 -0.0033702904 -0.0033752162 -0.0033794015 -0.0033819855 -0.003384487 -0.0033856172 -0.0033861608 -0.0033862893][-0.0033904545 -0.0033870162 -0.0033830025 -0.003377446 -0.0033716874 -0.0033681744 -0.0033693577 -0.0033726261 -0.0033770907 -0.0033803603 -0.0033819408 -0.003382616 -0.0033819422 -0.003381355 -0.0033808926][-0.0033899504 -0.0033865306 -0.0033828181 -0.0033780136 -0.0033726019 -0.0033694122 -0.003370916 -0.0033745326 -0.0033784325 -0.0033801128 -0.0033799806 -0.0033784471 -0.0033756089 -0.0033734844 -0.0033725882][-0.0033876158 -0.0033835238 -0.003380164 -0.0033757337 -0.003371476 -0.003369547 -0.0033718024 -0.0033756243 -0.0033787778 -0.0033789058 -0.0033765165 -0.0033723176 -0.0033666599 -0.0033628473 -0.0033608691][-0.0033831755 -0.0033784166 -0.0033751603 -0.0033714306 -0.0033687665 -0.0033685477 -0.0033722238 -0.0033768583 -0.0033793722 -0.0033776702 -0.0033733889 -0.0033671388 -0.0033592875 -0.0033537953 -0.0033507606][-0.0033786057 -0.0033731018 -0.0033699633 -0.0033667167 -0.0033648727 -0.0033660238 -0.0033709751 -0.0033765412 -0.0033792793 -0.0033768797 -0.0033714974 -0.0033641208 -0.0033548432 -0.0033480232 -0.0033438765][-0.0033751989 -0.0033688382 -0.0033656082 -0.0033625171 -0.003360858 -0.0033623416 -0.0033678766 -0.0033743952 -0.0033777964 -0.0033760578 -0.0033709882 -0.0033631013 -0.0033530609 -0.0033450352 -0.003340245][-0.0033737596 -0.0033672806 -0.0033631797 -0.00335979 -0.0033575555 -0.003358688 -0.0033640896 -0.0033712594 -0.0033759479 -0.003375439 -0.0033711409 -0.0033637893 -0.0033538742 -0.0033451209 -0.0033391097][-0.0033747815 -0.0033685586 -0.0033635201 -0.0033592533 -0.0033562295 -0.00335617 -0.0033608263 -0.0033679747 -0.0033741649 -0.003375568 -0.00337345 -0.0033680135 -0.0033593159 -0.0033504218 -0.0033434047][-0.0033782285 -0.0033722259 -0.0033663705 -0.0033608528 -0.0033562442 -0.0033548714 -0.0033584363 -0.0033649674 -0.0033718147 -0.0033753826 -0.0033758308 -0.0033730362 -0.0033668869 -0.0033592535 -0.0033519077][-0.003381571 -0.0033757782 -0.0033697071 -0.0033632661 -0.0033575268 -0.0033548372 -0.0033568856 -0.0033623739 -0.0033688908 -0.0033739822 -0.0033766082 -0.0033766686 -0.0033735812 -0.0033681262 -0.0033619958]]...]
INFO - root - 2017-12-09 21:59:32.840894: step 63510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 62h:34m:39s remains)
INFO - root - 2017-12-09 21:59:41.599498: step 63520, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 65h:06m:43s remains)
INFO - root - 2017-12-09 21:59:50.115243: step 63530, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 60h:24m:06s remains)
INFO - root - 2017-12-09 21:59:58.725022: step 63540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 66h:21m:00s remains)
INFO - root - 2017-12-09 22:00:07.303471: step 63550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 64h:00m:40s remains)
INFO - root - 2017-12-09 22:00:16.246659: step 63560, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 67h:03m:53s remains)
INFO - root - 2017-12-09 22:00:24.983225: step 63570, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 64h:02m:25s remains)
INFO - root - 2017-12-09 22:00:33.591587: step 63580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:23m:55s remains)
INFO - root - 2017-12-09 22:00:42.161692: step 63590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:34m:24s remains)
INFO - root - 2017-12-09 22:00:50.883657: step 63600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:19m:22s remains)
2017-12-09 22:00:51.762194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033963947 -0.0033952685 -0.0033956573 -0.0033969169 -0.003397888 -0.0033982524 -0.0033990056 -0.0034002902 -0.0034005989 -0.0034008932 -0.0034002364 -0.003399709 -0.0033988727 -0.00339735 -0.0033960489][-0.0033942624 -0.003393962 -0.0033950303 -0.0033969514 -0.0033986343 -0.0033999684 -0.0034014506 -0.0034038378 -0.0034054492 -0.0034062457 -0.003406021 -0.00340539 -0.0034034711 -0.0034005896 -0.0033986678][-0.003391617 -0.0033913946 -0.0033930908 -0.0033955837 -0.0033979309 -0.0033996897 -0.0034017058 -0.0034050171 -0.0034081973 -0.0034100153 -0.0034101575 -0.0034091061 -0.0034060345 -0.0034017747 -0.0033984792][-0.0033904258 -0.0033904025 -0.003392644 -0.0033959271 -0.0033988953 -0.0034008955 -0.0034027977 -0.0034057847 -0.003409096 -0.0034113908 -0.0034122688 -0.0034114248 -0.0034079661 -0.0034029642 -0.0033988655][-0.0033913937 -0.0033916833 -0.0033943276 -0.0033981376 -0.0034020359 -0.0034043516 -0.0034058855 -0.0034078544 -0.0034102788 -0.0034121524 -0.0034133242 -0.0034122581 -0.0034084048 -0.0034033076 -0.003399194][-0.0033923856 -0.0033929448 -0.0033964135 -0.0034013086 -0.003405846 -0.0034083265 -0.0034091792 -0.0034098465 -0.0034108558 -0.0034120171 -0.0034131575 -0.0034115727 -0.0034078709 -0.0034025495 -0.0033980187][-0.0033923772 -0.0033931725 -0.0033974161 -0.0034031037 -0.0034080746 -0.0034107375 -0.0034109796 -0.0034098241 -0.003409663 -0.00341006 -0.0034107498 -0.00340917 -0.0034062255 -0.0034015735 -0.0033969348][-0.0033933271 -0.0033939048 -0.003398051 -0.0034036117 -0.0034084583 -0.0034107666 -0.0034105275 -0.0034092679 -0.0034086835 -0.0034084879 -0.0034085358 -0.0034067989 -0.0034046166 -0.0034004413 -0.0033960068][-0.0033948186 -0.0033952384 -0.0033982031 -0.0034029838 -0.0034070686 -0.0034090392 -0.0034090034 -0.0034076613 -0.003406903 -0.0034063812 -0.0034061284 -0.0034042669 -0.0034023644 -0.0033992862 -0.0033957819][-0.0033934503 -0.0033934023 -0.0033952924 -0.0033985774 -0.0034016806 -0.0034038683 -0.0034044348 -0.0034037614 -0.0034033677 -0.0034025658 -0.0034021181 -0.00339996 -0.0033982231 -0.0033954647 -0.003392309][-0.0033883464 -0.0033876474 -0.0033891485 -0.0033918542 -0.0033945183 -0.0033968086 -0.0033977102 -0.0033973427 -0.0033970971 -0.0033960433 -0.0033949916 -0.0033929518 -0.0033910535 -0.0033885022 -0.0033856062][-0.0033785496 -0.0033769803 -0.0033783964 -0.0033805487 -0.0033826774 -0.003384372 -0.0033849678 -0.0033845569 -0.0033840793 -0.0033828102 -0.0033814276 -0.0033797382 -0.003377785 -0.0033755505 -0.0033733884][-0.0033690713 -0.0033663358 -0.0033671176 -0.0033681358 -0.0033690473 -0.0033700736 -0.0033701917 -0.003369608 -0.0033686268 -0.0033676852 -0.0033666762 -0.0033652824 -0.0033638906 -0.003362369 -0.0033606172][-0.0033595283 -0.0033555927 -0.0033550505 -0.0033553895 -0.0033554414 -0.0033555503 -0.0033552144 -0.0033544425 -0.0033534421 -0.0033526062 -0.0033518306 -0.0033508427 -0.0033498709 -0.0033486674 -0.0033473696][-0.0033538437 -0.003349015 -0.0033479722 -0.0033478679 -0.0033473938 -0.003346619 -0.0033451784 -0.003344239 -0.00334346 -0.0033428855 -0.0033421912 -0.0033413384 -0.0033404676 -0.0033393942 -0.003338357]]...]
INFO - root - 2017-12-09 22:01:00.566982: step 63610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 63h:30m:19s remains)
INFO - root - 2017-12-09 22:01:09.267032: step 63620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:00m:27s remains)
INFO - root - 2017-12-09 22:01:17.766512: step 63630, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 59h:01m:38s remains)
INFO - root - 2017-12-09 22:01:26.378611: step 63640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:39m:46s remains)
INFO - root - 2017-12-09 22:01:35.109193: step 63650, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 62h:10m:20s remains)
INFO - root - 2017-12-09 22:01:43.946684: step 63660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:46m:21s remains)
INFO - root - 2017-12-09 22:01:52.751944: step 63670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:16m:01s remains)
INFO - root - 2017-12-09 22:02:01.477711: step 63680, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 67h:25m:42s remains)
INFO - root - 2017-12-09 22:02:10.178970: step 63690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:05m:13s remains)
INFO - root - 2017-12-09 22:02:18.951046: step 63700, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 67h:13m:33s remains)
2017-12-09 22:02:19.875835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033726231 -0.0033076983 -0.0030157908 -0.0023267986 -0.0013509889 -0.00038270047 0.00018559117 0.00018434972 -0.00034892629 -0.0012508247 -0.0022117251 -0.0029402608 -0.0032855256 -0.0033657756 -0.0033666417][-0.0033404068 -0.0030644527 -0.0022322019 -0.00061893277 0.0013857626 0.003171324 0.0040891469 0.0038201737 0.002509624 0.00058429292 -0.0012784866 -0.0025973916 -0.0032147707 -0.0033627097 -0.003366685][-0.0032306495 -0.0024408731 -0.00040751509 0.0031497495 0.0073114596 0.0108387 0.012522336 0.011764646 0.0088741193 0.0048247669 0.0010178292 -0.0016430564 -0.0029450441 -0.0033276828 -0.0033671889][-0.0030142625 -0.0013460396 0.0026551767 0.0093080262 0.016991366 0.023526074 0.026692679 0.025381625 0.02009479 0.012644731 0.0055205431 0.00037262007 -0.0023049884 -0.0032192944 -0.0033649723][-0.0027385843 -2.4665147e-05 0.0063046385 0.01660653 0.028582418 0.039007291 0.044373441 0.042793524 0.034875143 0.023375608 0.012047729 0.0035017973 -0.0012276822 -0.0030128276 -0.0033568456][-0.0023213602 0.0012223602 0.0095049618 0.023011323 0.038974751 0.053223372 0.060997315 0.059566133 0.049490534 0.034338664 0.018988997 0.0070024291 5.0016213e-05 -0.0027492808 -0.0033447759][-0.0018542144 0.0020519996 0.011221251 0.026347661 0.04465327 0.0614606 0.071186535 0.070397615 0.059423961 0.042208523 0.024263022 0.0098178824 0.0011288996 -0.0025143106 -0.003333742][-0.0015904023 0.0020595661 0.01070943 0.02528579 0.043438561 0.060573213 0.071026094 0.071094722 0.060806535 0.043888919 0.025750093 0.010784261 0.001551375 -0.0024099834 -0.0033267562][-0.0016648947 0.0012247756 0.0081944726 0.020317895 0.035931394 0.051103052 0.0607927 0.061568007 0.053236455 0.038823403 0.022929154 0.00954212 0.0011537217 -0.0024813449 -0.0033279036][-0.0020054835 -8.9141075e-05 0.0046574138 0.013290457 0.024857227 0.036417976 0.044082835 0.045145404 0.039344 0.02878947 0.016826231 0.0065797335 0.00010888209 -0.0026927432 -0.0033361928][-0.0024663422 -0.0013964598 0.0013156657 0.0065340549 0.013868259 0.021423226 0.026580105 0.027525011 0.024051089 0.017434159 0.0097398078 0.003062906 -0.0011499776 -0.0029482115 -0.0033462264][-0.0029395802 -0.0024362039 -0.0011614875 0.0014509226 0.00533471 0.0094876429 0.012400357 0.013034409 0.01128634 0.0078086117 0.0036666903 4.0977728e-05 -0.0022243776 -0.0031624311 -0.003357426][-0.0032622661 -0.0030884997 -0.002625911 -0.00159058 4.7668116e-05 0.0018719884 0.0031801977 0.0034948222 0.00278847 0.0013351867 -0.00042775087 -0.0019735212 -0.0029208024 -0.0032940123 -0.0033642717][-0.0033694599 -0.0033363516 -0.0032300388 -0.0029430226 -0.0024450216 -0.0018624692 -0.0014350701 -0.001323164 -0.0015259886 -0.0019608387 -0.0024980791 -0.0029664084 -0.0032465509 -0.0033499948 -0.0033674119][-0.00337338 -0.0033721616 -0.0033673097 -0.0033336072 -0.003257429 -0.0031593274 -0.0030820128 -0.0030567616 -0.0030819518 -0.003145877 -0.0032311217 -0.0033066014 -0.0033514895 -0.0033662708 -0.0033689023]]...]
INFO - root - 2017-12-09 22:02:28.622678: step 63710, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:15m:35s remains)
INFO - root - 2017-12-09 22:02:37.281182: step 63720, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 65h:07m:04s remains)
INFO - root - 2017-12-09 22:02:45.963406: step 63730, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.743 sec/batch; 55h:26m:19s remains)
INFO - root - 2017-12-09 22:02:54.431252: step 63740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:46m:51s remains)
INFO - root - 2017-12-09 22:03:03.185485: step 63750, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 66h:47m:13s remains)
INFO - root - 2017-12-09 22:03:11.942291: step 63760, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 64h:34m:22s remains)
INFO - root - 2017-12-09 22:03:20.587210: step 63770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:53m:54s remains)
INFO - root - 2017-12-09 22:03:29.145267: step 63780, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 65h:59m:11s remains)
INFO - root - 2017-12-09 22:03:37.868693: step 63790, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 65h:01m:15s remains)
INFO - root - 2017-12-09 22:03:46.585709: step 63800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:53m:53s remains)
2017-12-09 22:03:47.496382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033878202 -0.0033852418 -0.0033852956 -0.0033868826 -0.0033894884 -0.003392285 -0.003393827 -0.0033946491 -0.0033945809 -0.0033939569 -0.0033925176 -0.0033905108 -0.0033885718 -0.0033858074 -0.0033830609][-0.0033861019 -0.0033831878 -0.0033833585 -0.0033858952 -0.0033894035 -0.0033932254 -0.0033958319 -0.0033972391 -0.0033974131 -0.0033966168 -0.0033952203 -0.0033934189 -0.003391477 -0.0033892645 -0.0033867611][-0.0033872516 -0.0033845545 -0.0033851429 -0.0033876956 -0.0033913094 -0.0033957271 -0.0033991102 -0.0034007288 -0.0034014024 -0.0034009039 -0.0033995293 -0.0033979616 -0.0033964219 -0.0033945967 -0.0033922656][-0.0033913099 -0.0033889592 -0.0033895322 -0.0033916142 -0.003394746 -0.003399183 -0.0034025179 -0.0034044485 -0.0034055191 -0.0034058942 -0.0034050145 -0.0034040585 -0.0034030282 -0.0034016368 -0.0033991367][-0.0033947185 -0.0033924852 -0.0033924524 -0.0033933956 -0.0033949018 -0.003397448 -0.0033996955 -0.0034010618 -0.0034024708 -0.0034040746 -0.0034050734 -0.0034058173 -0.0034057696 -0.0034051074 -0.0034024285][-0.0033964256 -0.0033939728 -0.0033933211 -0.0033929134 -0.0033931003 -0.0033932037 -0.0033929481 -0.0033919688 -0.0033932396 -0.0033956494 -0.003398489 -0.0034012243 -0.0034028948 -0.0034040017 -0.0034017719][-0.0033978368 -0.003394231 -0.0033920761 -0.0033897965 -0.0033887373 -0.0033872582 -0.0033855939 -0.0033832141 -0.0033850917 -0.0033888947 -0.0033931548 -0.0033969365 -0.003398669 -0.0034001665 -0.0033972976][-0.0033989293 -0.00339612 -0.003393329 -0.0033891506 -0.0033855063 -0.0033824081 -0.0033802474 -0.0033767479 -0.0033799065 -0.00338628 -0.0033916929 -0.0033957986 -0.0033969223 -0.0033972622 -0.0033931946][-0.0033987407 -0.0033968231 -0.0033945919 -0.0033906731 -0.0033864975 -0.0033822176 -0.0033797759 -0.0033776916 -0.0033812213 -0.0033881727 -0.0033939031 -0.0033974131 -0.0033983858 -0.0033981781 -0.0033940531][-0.0033965504 -0.003395699 -0.0033942049 -0.0033922456 -0.0033894398 -0.0033851641 -0.0033836882 -0.0033828472 -0.0033854702 -0.0033906966 -0.0033949558 -0.0033975923 -0.0033982366 -0.0033980855 -0.0033952196][-0.0033929825 -0.0033923418 -0.0033919492 -0.0033915327 -0.003390441 -0.0033884544 -0.0033882277 -0.0033881229 -0.0033902363 -0.0033932563 -0.0033953583 -0.0033964491 -0.0033963821 -0.0033957406 -0.0033939339][-0.003389531 -0.0033879962 -0.0033879993 -0.0033881594 -0.0033889338 -0.003389329 -0.0033901371 -0.0033909685 -0.0033925362 -0.0033940361 -0.0033951476 -0.003395099 -0.0033938529 -0.0033924861 -0.0033909043][-0.0033862258 -0.0033842192 -0.0033842435 -0.0033843431 -0.0033855538 -0.0033872421 -0.0033889501 -0.0033904898 -0.003391759 -0.0033928449 -0.0033927204 -0.0033912626 -0.0033893632 -0.0033869871 -0.003385334][-0.0033836698 -0.0033812309 -0.0033808837 -0.0033808069 -0.00338173 -0.0033829298 -0.0033844502 -0.0033860102 -0.0033870731 -0.0033882484 -0.0033881669 -0.0033865857 -0.0033842942 -0.0033818146 -0.0033802751][-0.0033823878 -0.0033791922 -0.0033783026 -0.0033776893 -0.0033775556 -0.0033779121 -0.0033791561 -0.0033805724 -0.0033812306 -0.0033819203 -0.0033820146 -0.0033810276 -0.0033796045 -0.0033782916 -0.0033774769]]...]
INFO - root - 2017-12-09 22:03:56.158777: step 63810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:36m:04s remains)
INFO - root - 2017-12-09 22:04:04.942140: step 63820, loss = 0.89, batch loss = 0.68 (8.2 examples/sec; 0.979 sec/batch; 73h:01m:57s remains)
INFO - root - 2017-12-09 22:04:13.508216: step 63830, loss = 0.89, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 57h:17m:58s remains)
INFO - root - 2017-12-09 22:04:21.944046: step 63840, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 66h:47m:03s remains)
INFO - root - 2017-12-09 22:04:30.624551: step 63850, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 64h:29m:22s remains)
INFO - root - 2017-12-09 22:04:39.368756: step 63860, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 65h:17m:59s remains)
INFO - root - 2017-12-09 22:04:48.269762: step 63870, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:54m:29s remains)
INFO - root - 2017-12-09 22:04:56.965397: step 63880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:29m:04s remains)
INFO - root - 2017-12-09 22:05:05.753279: step 63890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:35m:26s remains)
INFO - root - 2017-12-09 22:05:14.612525: step 63900, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:02m:05s remains)
2017-12-09 22:05:15.503747: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1683576 0.22781579 0.28884757 0.34368917 0.3870852 0.41782385 0.43685004 0.448013 0.45070338 0.448242 0.43690822 0.42181939 0.40263364 0.38069344 0.35937336][0.173035 0.2357368 0.29943281 0.355813 0.40071309 0.43222532 0.45146954 0.46148127 0.4620021 0.45519227 0.4385235 0.41557494 0.38736427 0.35626084 0.32687351][0.17630227 0.24007235 0.30505118 0.36332506 0.40991876 0.44205555 0.46139678 0.470167 0.46732056 0.45416451 0.4288792 0.3961806 0.35751835 0.31622612 0.27826163][0.18179919 0.24705052 0.31420282 0.37464243 0.42404827 0.45829204 0.47853613 0.48653916 0.47980133 0.45958751 0.42378455 0.37907642 0.32841933 0.27606156 0.22990477][0.18782671 0.25468341 0.32386109 0.38727355 0.43961698 0.47629791 0.49830651 0.50563687 0.49452341 0.46670458 0.41990149 0.36220926 0.29883885 0.23617555 0.18329476][0.18927309 0.2575351 0.32837662 0.39385241 0.44889593 0.48816302 0.51127082 0.51773947 0.50278527 0.46784502 0.41050848 0.34108782 0.26722112 0.1968789 0.13986228][0.17999569 0.24938303 0.32082978 0.38686469 0.44295591 0.48285714 0.50599951 0.51123959 0.49273774 0.45246693 0.38825652 0.31148618 0.23179661 0.15874898 0.10213777][0.16181102 0.23013583 0.30058521 0.36538789 0.41960505 0.45776531 0.47920284 0.48168403 0.45992914 0.41620031 0.34955266 0.27109265 0.19211794 0.12264219 0.07129439][0.1401037 0.20387214 0.26951516 0.32958594 0.37856165 0.41191784 0.4293851 0.42793289 0.40411294 0.359711 0.2959139 0.2227069 0.15171432 0.091637634 0.049264889][0.11951088 0.17504586 0.23099141 0.28175944 0.32116655 0.34642774 0.35732067 0.35162726 0.32786253 0.28655022 0.23127757 0.16968153 0.1125059 0.0662012 0.03523016][0.10018518 0.14500414 0.18940069 0.22753267 0.25467578 0.26947612 0.27242735 0.26272753 0.24039276 0.20567872 0.16213281 0.11608618 0.075911693 0.045067202 0.026083056][0.08227732 0.11565477 0.14757086 0.173213 0.18890844 0.19381398 0.18976393 0.17772317 0.15801403 0.13132913 0.10059667 0.070603952 0.0470239 0.030913 0.0235334][0.06316974 0.0867527 0.10834493 0.12389426 0.13088883 0.12934096 0.12122601 0.10846394 0.0921766 0.073559575 0.054702412 0.038824011 0.029794203 0.026596339 0.029571196][0.042147096 0.0574285 0.070888989 0.0796389 0.081925772 0.078193873 0.069762677 0.059209406 0.047374539 0.03603692 0.026628936 0.021517027 0.023479156 0.030552158 0.041897237][0.022019101 0.030766336 0.03823708 0.042524043 0.043072239 0.040068105 0.033848077 0.027118323 0.020082753 0.014972506 0.01222088 0.014412157 0.02408924 0.038882755 0.056880239]]...]
INFO - root - 2017-12-09 22:05:24.212878: step 63910, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 67h:15m:57s remains)
INFO - root - 2017-12-09 22:05:32.759375: step 63920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:21m:19s remains)
INFO - root - 2017-12-09 22:05:41.587233: step 63930, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 66h:54m:35s remains)
INFO - root - 2017-12-09 22:05:49.869192: step 63940, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 64h:58m:14s remains)
INFO - root - 2017-12-09 22:05:58.377931: step 63950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:20m:47s remains)
INFO - root - 2017-12-09 22:06:06.997649: step 63960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 63h:40m:09s remains)
INFO - root - 2017-12-09 22:06:15.694045: step 63970, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 65h:21m:12s remains)
INFO - root - 2017-12-09 22:06:24.301615: step 63980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:00m:30s remains)
INFO - root - 2017-12-09 22:06:33.071983: step 63990, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 65h:23m:33s remains)
INFO - root - 2017-12-09 22:06:41.921894: step 64000, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.911 sec/batch; 67h:55m:41s remains)
2017-12-09 22:06:42.814854: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027101884 0.027765607 0.0280635 0.028060144 0.027563702 0.02703115 0.026474215 0.025279446 0.02371639 0.021540524 0.019186627 0.01627725 0.012889089 0.0094279572 0.0063007222][0.028297996 0.029348293 0.030028453 0.030285148 0.030181205 0.029950073 0.029475274 0.028384665 0.026810328 0.024414131 0.021435732 0.017699655 0.013521418 0.0092773978 0.0056101428][0.0249987 0.026029753 0.026941171 0.027557038 0.027845802 0.0279555 0.027974071 0.027490892 0.026341727 0.024159051 0.021150213 0.017197702 0.012686413 0.0081534227 0.0044731051][0.021222785 0.022312002 0.023309503 0.024054121 0.024592169 0.024884807 0.025096484 0.025038388 0.024321375 0.022583246 0.019859014 0.016177343 0.01182122 0.007401607 0.0040516313][0.016677294 0.017978642 0.01923763 0.02021496 0.020926427 0.021366242 0.02165962 0.021664608 0.021116909 0.019790567 0.017555371 0.014348667 0.010453586 0.0066139027 0.003978041][0.011528303 0.012957612 0.014287319 0.015503725 0.016534392 0.017235579 0.017669642 0.01770075 0.017364476 0.01636974 0.014547288 0.01203411 0.0089722276 0.0061093839 0.0043478217][0.0061901826 0.0074510928 0.0087410463 0.010031548 0.011183439 0.012251619 0.013099492 0.013526777 0.013583247 0.013021043 0.011838744 0.010152835 0.0080713509 0.0063249459 0.0054947576][0.0016857518 0.0024102635 0.0032725788 0.0042661261 0.00533578 0.00656195 0.0077066394 0.0087177977 0.0093796588 0.0094694812 0.0089760246 0.0081097884 0.0070552165 0.0063362811 0.0062554311][-0.0014869176 -0.001141608 -0.00067885127 -0.00019093044 0.0004304247 0.0012987696 0.0023864207 0.0035605526 0.0045175916 0.0050051147 0.0050874641 0.0049008923 0.0046112156 0.004588088 0.0050003827][-0.0030901381 -0.0029846712 -0.00281777 -0.002657301 -0.0023893178 -0.0019918205 -0.0013861705 -0.0006388023 3.9072009e-05 0.0005221793 0.00076725357 0.00085149542 0.00097632664 0.0012875397 0.0019092404][-0.0033929308 -0.0033878775 -0.0033801666 -0.0033565834 -0.0032844855 -0.0031613919 -0.0029525012 -0.0026761822 -0.0024026849 -0.0022019711 -0.0020974779 -0.0020200871 -0.001898482 -0.0016804024 -0.0012436374][-0.0033934582 -0.0033916151 -0.0033912831 -0.0033925266 -0.0033929045 -0.0033885252 -0.0033676191 -0.0033329756 -0.0032927627 -0.00325583 -0.0032299981 -0.0031977042 -0.0031443895 -0.0030517937 -0.0028795707][-0.0033936095 -0.0033916207 -0.0033915739 -0.0033930913 -0.0033942091 -0.00339553 -0.003396333 -0.0033981407 -0.0033982829 -0.0033990515 -0.0033986578 -0.0033984752 -0.0033975097 -0.003395047 -0.0033775619][-0.0033955309 -0.0033926161 -0.0033915781 -0.0033927367 -0.0033936407 -0.0033943891 -0.0033947332 -0.0033964203 -0.0033967891 -0.0033978461 -0.0033977495 -0.0033977914 -0.0033977709 -0.0033973609 -0.00339739][-0.0033969188 -0.0033946238 -0.0033940938 -0.0033951479 -0.003395824 -0.003396072 -0.0033960955 -0.0033963113 -0.0033961094 -0.0033962277 -0.0033959222 -0.0033959367 -0.0033956738 -0.0033959027 -0.0033964533]]...]
INFO - root - 2017-12-09 22:06:51.481600: step 64010, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 61h:58m:02s remains)
INFO - root - 2017-12-09 22:07:00.275669: step 64020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 66h:11m:51s remains)
INFO - root - 2017-12-09 22:07:09.034911: step 64030, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 61h:31m:59s remains)
INFO - root - 2017-12-09 22:07:17.329775: step 64040, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 62h:03m:50s remains)
INFO - root - 2017-12-09 22:07:25.907749: step 64050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 62h:38m:19s remains)
INFO - root - 2017-12-09 22:07:34.500602: step 64060, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 62h:44m:40s remains)
INFO - root - 2017-12-09 22:07:43.060038: step 64070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 63h:47m:50s remains)
INFO - root - 2017-12-09 22:07:51.572483: step 64080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:49m:37s remains)
INFO - root - 2017-12-09 22:08:00.178045: step 64090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:35m:03s remains)
INFO - root - 2017-12-09 22:08:08.828835: step 64100, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 63h:38m:58s remains)
2017-12-09 22:08:09.724931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034013807 -0.0034001824 -0.0034005535 -0.0034014103 -0.0034023214 -0.0034032152 -0.0034037612 -0.0034037919 -0.0034033365 -0.0034027852 -0.0034025218 -0.0034022317 -0.0034019938 -0.003401828 -0.0034017407][-0.0033990876 -0.0033975265 -0.0033979302 -0.0033989621 -0.0034001675 -0.0034011989 -0.0034015584 -0.0034010273 -0.0034001148 -0.003399408 -0.0033991996 -0.003399303 -0.0033994049 -0.00339961 -0.003399817][-0.0033990878 -0.0033973588 -0.0033979132 -0.0033990582 -0.0034003342 -0.003401235 -0.0034009225 -0.0033993823 -0.0033977928 -0.00339706 -0.0033972417 -0.0033979865 -0.0033986499 -0.0033993856 -0.0033997525][-0.0033991644 -0.0033976221 -0.0033982261 -0.0033995304 -0.0034009889 -0.0034012254 -0.0033991372 -0.00339581 -0.0033937066 -0.003393407 -0.0033950876 -0.003397061 -0.0033984012 -0.0033993763 -0.0033998555][-0.0033993677 -0.0033978277 -0.0033984836 -0.0033999865 -0.003401333 -0.0033998152 -0.0033947269 -0.0033890726 -0.0033865438 -0.0033882905 -0.0033930049 -0.0033965048 -0.003398333 -0.0033995004 -0.0034000096][-0.0033993898 -0.0033979826 -0.0033987653 -0.0034004187 -0.003401007 -0.0033973646 -0.0033895068 -0.0033821769 -0.0033805883 -0.003386131 -0.0033934191 -0.0033974678 -0.0033993311 -0.0034001188 -0.003400269][-0.0033993025 -0.003398072 -0.0033990003 -0.0034005109 -0.0034003118 -0.0033954473 -0.0033865797 -0.0033799699 -0.003381754 -0.0033897038 -0.0033965691 -0.0034001048 -0.0034014138 -0.0034012399 -0.0034004701][-0.003399479 -0.0033983698 -0.0033990953 -0.0034003677 -0.0033999542 -0.0033956119 -0.003388816 -0.0033857122 -0.0033896109 -0.0033962764 -0.0034010662 -0.0034034797 -0.0034036259 -0.0034020413 -0.0034003726][-0.0033999528 -0.0033987211 -0.0033989481 -0.0033999451 -0.0033999102 -0.0033976182 -0.0033947281 -0.0033946377 -0.0033980927 -0.0034022911 -0.0034047384 -0.0034052827 -0.0034039833 -0.0034017789 -0.0033998226][-0.0034008373 -0.0033990028 -0.0033984901 -0.0033991395 -0.0033997847 -0.0033996073 -0.0033994725 -0.0034007092 -0.0034033991 -0.0034058467 -0.0034061202 -0.0034049226 -0.0034027391 -0.0034006061 -0.0033987961][-0.003401445 -0.0033989449 -0.0033979095 -0.0033983761 -0.0033990559 -0.0033998874 -0.0034012522 -0.0034030692 -0.0034052876 -0.0034064716 -0.0034054636 -0.0034032068 -0.0034009402 -0.003399113 -0.0033974024][-0.0034013127 -0.0033985593 -0.0033972785 -0.0033973539 -0.0033979786 -0.0033990773 -0.0034007721 -0.0034026443 -0.0034042445 -0.0034043377 -0.0034031174 -0.0034010217 -0.0033990305 -0.0033974687 -0.0033960193][-0.0034004985 -0.0033978776 -0.0033965704 -0.0033962317 -0.0033964957 -0.0033976224 -0.0033992967 -0.0034006534 -0.0034014531 -0.003401302 -0.0034003232 -0.0033989255 -0.0033975225 -0.0033961523 -0.003395102][-0.0033991733 -0.0033966536 -0.0033954538 -0.0033949632 -0.0033949723 -0.0033958163 -0.0033971195 -0.0033980676 -0.0033984294 -0.0033983476 -0.0033979984 -0.0033973395 -0.0033964752 -0.003395485 -0.0033948505][-0.0033975928 -0.0033952727 -0.0033942207 -0.003393705 -0.0033935769 -0.0033941811 -0.0033952177 -0.0033959069 -0.0033962564 -0.0033964573 -0.0033964717 -0.0033962934 -0.0033958408 -0.0033953546 -0.0033951525]]...]
INFO - root - 2017-12-09 22:08:18.351761: step 64110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:33m:37s remains)
INFO - root - 2017-12-09 22:08:26.890652: step 64120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:30m:17s remains)
INFO - root - 2017-12-09 22:08:35.315611: step 64130, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 63h:40m:20s remains)
INFO - root - 2017-12-09 22:08:43.757163: step 64140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:34m:26s remains)
INFO - root - 2017-12-09 22:08:52.090075: step 64150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:54m:07s remains)
INFO - root - 2017-12-09 22:09:00.676217: step 64160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-09 22:09:09.191630: step 64170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:44m:02s remains)
INFO - root - 2017-12-09 22:09:17.658740: step 64180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:34m:41s remains)
INFO - root - 2017-12-09 22:09:26.462219: step 64190, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 67h:26m:47s remains)
INFO - root - 2017-12-09 22:09:35.173005: step 64200, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.910 sec/batch; 67h:49m:31s remains)
2017-12-09 22:09:36.025981: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15271547 0.15106143 0.15135199 0.15298922 0.15649049 0.15975362 0.16281673 0.16539742 0.16719474 0.16932206 0.1715156 0.17554067 0.17574149 0.17037535 0.15769161][0.17199095 0.16922475 0.16873769 0.16918111 0.17139471 0.17335321 0.174934 0.17556944 0.17603879 0.17743063 0.17986241 0.18389364 0.18485473 0.18047512 0.16787913][0.17076397 0.16678825 0.16475107 0.16498227 0.16670321 0.16802891 0.16908067 0.1689215 0.16866295 0.16955888 0.17208266 0.17652199 0.17871867 0.17615603 0.16559318][0.16472554 0.16071945 0.15823776 0.1572123 0.15780817 0.15888906 0.1596783 0.15967031 0.15924767 0.1600721 0.16249856 0.16659014 0.16884176 0.16705965 0.15816982][0.15626588 0.15303275 0.15029745 0.14889577 0.14964907 0.15037236 0.1512271 0.15111847 0.15037608 0.15095955 0.15219893 0.15464632 0.15554154 0.15306956 0.14477476][0.14739077 0.14564466 0.14301258 0.1411884 0.14131099 0.14229569 0.14332996 0.14293969 0.14220002 0.14204678 0.14229378 0.14258221 0.1411026 0.13715889 0.12835352][0.14008568 0.13994524 0.13785779 0.13561223 0.13515581 0.13471648 0.13469264 0.13384728 0.13258064 0.13175729 0.13102305 0.13002224 0.12672353 0.12079321 0.11063159][0.13659146 0.1384045 0.13645796 0.13333906 0.13073955 0.12759586 0.12521993 0.12215535 0.11959804 0.11733841 0.1154095 0.11329933 0.10887723 0.10207271 0.091318607][0.13520753 0.13848686 0.13566121 0.13023965 0.12359244 0.11605659 0.10919336 0.10272476 0.09761373 0.09341722 0.090388246 0.087394468 0.083033025 0.076697826 0.067245513][0.13155669 0.13458085 0.12948698 0.12090708 0.10969999 0.097221687 0.085593864 0.075437926 0.067531526 0.06127879 0.056959357 0.054084647 0.051089536 0.046947584 0.040777232][0.12272178 0.12460238 0.11698948 0.10516263 0.090211548 0.074455589 0.059913613 0.047611576 0.0381673 0.0309402 0.026122577 0.023366241 0.021453233 0.019573281 0.016816007][0.11141917 0.1107516 0.10045332 0.086169958 0.069206037 0.052304152 0.037622694 0.0260926 0.017600218 0.011263351 0.007287059 0.0052265637 0.0042427238 0.0034976469 0.0025755924][0.10189523 0.097674459 0.084228247 0.0678532 0.05044394 0.034354046 0.021486318 0.012146978 0.0058287689 0.0015130278 -0.00093811774 -0.0020192009 -0.00232184 -0.0023803441 -0.0024442899][0.090876527 0.082859285 0.066839814 0.049590629 0.033117931 0.019433562 0.009757054 0.0036678414 0.0001828731 -0.0018729646 -0.0028415944 -0.0031168344 -0.0030692746 -0.0030060555 -0.0029906346][0.0787241 0.067599624 0.050002683 0.03330227 0.019077143 0.0086554326 0.0022590489 -0.001079147 -0.0024898411 -0.0031083669 -0.0032662542 -0.0031895125 -0.0030432928 -0.0029619862 -0.0029416159]]...]
INFO - root - 2017-12-09 22:09:44.568544: step 64210, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 62h:37m:16s remains)
INFO - root - 2017-12-09 22:09:53.159522: step 64220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:11m:24s remains)
INFO - root - 2017-12-09 22:10:01.940830: step 64230, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 63h:58m:36s remains)
INFO - root - 2017-12-09 22:10:10.651264: step 64240, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 66h:49m:05s remains)
INFO - root - 2017-12-09 22:10:19.303521: step 64250, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:43m:16s remains)
INFO - root - 2017-12-09 22:10:28.036894: step 64260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:26m:50s remains)
INFO - root - 2017-12-09 22:10:36.635236: step 64270, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 62h:24m:54s remains)
INFO - root - 2017-12-09 22:10:45.035340: step 64280, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 63h:33m:06s remains)
INFO - root - 2017-12-09 22:10:53.607105: step 64290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:09m:51s remains)
INFO - root - 2017-12-09 22:11:02.206544: step 64300, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 62h:05m:04s remains)
2017-12-09 22:11:03.143446: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0039476212 0.0040648617 0.0047380421 0.0054724 0.0062814225 0.0062391376 0.0061350605 0.0052897213 0.0041161478 0.00274925 0.0013228601 8.80789e-05 -0.0010531582 -0.0020842105 -0.0028290395][0.0067555476 0.0065993126 0.0069644973 0.0071977908 0.0073880246 0.0070463717 0.0068642832 0.0061310353 0.0051786732 0.0040399535 0.0027174682 0.0012327477 -0.00018060952 -0.0014847905 -0.0024880613][0.013320688 0.012222858 0.011487407 0.010747907 0.010084872 0.0091714272 0.0082939351 0.0068413876 0.0051818853 0.003485902 0.0019294817 0.000489305 -0.00074096117 -0.0018339144 -0.0026502549][0.02808773 0.026814878 0.0252694 0.023279764 0.021001875 0.018502375 0.01606188 0.013013795 0.0097132772 0.0063211052 0.003375157 0.0011122997 -0.00055501913 -0.0018058931 -0.0026871259][0.052441772 0.051861275 0.050612889 0.048582632 0.045607656 0.041856728 0.037634715 0.031989779 0.025674494 0.018927958 0.012887506 0.0079072285 0.0038070858 0.00069787842 -0.0016066128][0.0848965 0.085373543 0.084896311 0.083773062 0.081324793 0.077437252 0.0725592 0.065317504 0.056216873 0.045253132 0.034441411 0.024605354 0.015670871 0.0082250778 0.0022954948][0.11824928 0.12050949 0.12116399 0.12105147 0.1195906 0.11646581 0.11202645 0.10426557 0.093683518 0.079830028 0.064507626 0.04890646 0.033609174 0.020309443 0.0092599457][0.14246327 0.14641784 0.14821382 0.14927748 0.1490078 0.14684644 0.14324711 0.13601357 0.12534375 0.11022922 0.092142671 0.072361775 0.051779076 0.033015452 0.016933298][0.14986321 0.15524459 0.15805721 0.16013022 0.16084114 0.15957867 0.1567746 0.15055688 0.14086427 0.12634672 0.10784128 0.086523861 0.063467585 0.041707981 0.022567121][0.13825032 0.14402728 0.14697349 0.14926347 0.15028256 0.1495602 0.14750338 0.14275001 0.13488604 0.12252788 0.10587094 0.085882656 0.063661806 0.042251006 0.023237128][0.11015417 0.11571895 0.11859186 0.12077148 0.12174977 0.12121744 0.11951152 0.11589512 0.10987544 0.10033076 0.08707916 0.070906527 0.052697819 0.034887034 0.018981431][0.073611088 0.078092419 0.080571361 0.082485989 0.08345665 0.083270885 0.082127944 0.079704829 0.075578228 0.068959251 0.05968032 0.048342451 0.035624087 0.023122761 0.011992013][0.040210865 0.043244384 0.045062039 0.046503194 0.047321782 0.047379378 0.046758704 0.045340076 0.042863235 0.038911879 0.033381116 0.026666056 0.019188963 0.011833139 0.0053383885][0.0170691 0.018766059 0.019860294 0.020742713 0.021287613 0.021416094 0.021145277 0.020439493 0.019173898 0.017171685 0.0143956 0.011064677 0.007398732 0.0037980096 0.0006542874][0.0041905744 0.0049673524 0.0055268425 0.0059917448 0.0063085025 0.0064338539 0.0063595567 0.0060770232 0.0055440376 0.0047035934 0.0035540985 0.0021966703 0.00072848215 -0.00069828611 -0.0019200548]]...]
INFO - root - 2017-12-09 22:11:11.742155: step 64310, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:30m:41s remains)
INFO - root - 2017-12-09 22:11:20.190119: step 64320, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 61h:08m:56s remains)
INFO - root - 2017-12-09 22:11:28.752262: step 64330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:45m:52s remains)
INFO - root - 2017-12-09 22:11:37.227022: step 64340, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 62h:08m:42s remains)
INFO - root - 2017-12-09 22:11:45.693305: step 64350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 64h:19m:39s remains)
INFO - root - 2017-12-09 22:11:54.342386: step 64360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:39m:05s remains)
INFO - root - 2017-12-09 22:12:03.050279: step 64370, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 61h:52m:44s remains)
INFO - root - 2017-12-09 22:12:11.655568: step 64380, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:46m:32s remains)
INFO - root - 2017-12-09 22:12:20.463402: step 64390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:05m:38s remains)
INFO - root - 2017-12-09 22:12:29.095822: step 64400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:34m:27s remains)
2017-12-09 22:12:29.977874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029655562 -0.0030343942 -0.0029441286 -0.002513079 -0.0016696388 -0.00060976227 0.00044721831 0.0012310434 0.0016408695 0.0016293589 0.001169991 0.00043865293 -0.00045039481 -0.0012836005 -0.0019866454][-0.0022943197 -0.0024941154 -0.0024511716 -0.0020023684 -0.0010686563 0.00016437937 0.0014221759 0.0023381233 0.0027276461 0.0025893033 0.0019164698 0.0009397089 -0.00016986975 -0.0011506628 -0.0019280151][-0.00098729343 -0.0010538159 -0.00079423911 -5.9942249e-05 0.0011597124 0.0026752192 0.004144121 0.0051495619 0.0053751189 0.0048640547 0.0037124848 0.0022185033 0.00064638373 -0.00070227543 -0.0017005691][0.0015010142 0.0022596729 0.0034273628 0.0050571375 0.0070007564 0.009039239 0.010754257 0.011704748 0.011515643 0.01026764 0.0082302177 0.0057545369 0.0032288257 0.0010087935 -0.00066360924][0.0056711202 0.0082386425 0.011351692 0.014852874 0.018320633 0.021441981 0.023762995 0.024829932 0.024215743 0.022005426 0.01865061 0.0145212 0.010161146 0.0060659051 0.0027122677][0.011949773 0.017302038 0.023338167 0.029571278 0.0352804 0.040029395 0.043374144 0.0448752 0.044061191 0.040980373 0.036156032 0.0298908 0.022878967 0.015817566 0.0095589589][0.019387281 0.027967067 0.03733087 0.046582259 0.054731231 0.061240174 0.065809689 0.068051063 0.067441471 0.063919991 0.057926107 0.04962806 0.039738849 0.029141119 0.019174855][0.026189044 0.037529748 0.049652606 0.061338454 0.0713722 0.079122223 0.084552638 0.08747787 0.087338254 0.083935253 0.077457152 0.067902692 0.05586243 0.04223622 0.028859029][0.029951869 0.042758528 0.056203749 0.068899892 0.07957533 0.087571792 0.093114845 0.096273907 0.096572205 0.093692079 0.087501936 0.077824585 0.065054186 0.049974464 0.034721214][0.029012868 0.041518193 0.054429065 0.0664558 0.076414764 0.083646849 0.088531 0.091360524 0.091827 0.089541093 0.08415255 0.075395107 0.0634845 0.049043305 0.034191526][0.023515197 0.034109388 0.044914946 0.054889377 0.063068755 0.0688521 0.072599284 0.074672572 0.074979246 0.073180556 0.068858 0.061723985 0.051903538 0.039917972 0.0275688][0.015354669 0.023015417 0.030781837 0.037929952 0.043774921 0.04783028 0.050332554 0.051576834 0.051590152 0.050163556 0.046977319 0.041813724 0.034769572 0.026275618 0.017632123][0.0072868466 0.011921208 0.016609963 0.020919148 0.024453195 0.02689036 0.028314048 0.028896404 0.028697656 0.027637117 0.025540769 0.022298245 0.01801841 0.0130175 0.0080612935][0.0012670427 0.0034755946 0.0057213027 0.0077993544 0.009521368 0.010713794 0.01137902 0.011585108 0.011364435 0.010699065 0.00954324 0.0078750616 0.0057935324 0.00348315 0.0012912753][-0.001995442 -0.0012404653 -0.00045789056 0.00028389343 0.00091207842 0.0013532673 0.0015883592 0.0016323428 0.001499682 0.0011943381 0.00071922946 8.6345011e-05 -0.00064583845 -0.0014028798 -0.0020792352]]...]
INFO - root - 2017-12-09 22:12:38.667319: step 64410, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.909 sec/batch; 67h:41m:18s remains)
INFO - root - 2017-12-09 22:12:47.398407: step 64420, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 63h:21m:48s remains)
INFO - root - 2017-12-09 22:12:56.075013: step 64430, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 62h:45m:51s remains)
INFO - root - 2017-12-09 22:13:04.674571: step 64440, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:09m:33s remains)
INFO - root - 2017-12-09 22:13:13.226532: step 64450, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 65h:37m:10s remains)
INFO - root - 2017-12-09 22:13:21.768773: step 64460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:05m:57s remains)
INFO - root - 2017-12-09 22:13:30.261425: step 64470, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:42m:59s remains)
INFO - root - 2017-12-09 22:13:38.903901: step 64480, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 67h:21m:01s remains)
INFO - root - 2017-12-09 22:13:47.535220: step 64490, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 63h:11m:13s remains)
INFO - root - 2017-12-09 22:13:56.169879: step 64500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:21m:26s remains)
2017-12-09 22:13:57.028479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034043696 -0.003402363 -0.0034011784 -0.0033999702 -0.0033985251 -0.003396749 -0.003395268 -0.0033944538 -0.0033940221 -0.0033938738 -0.0033943697 -0.0033952992 -0.0033962135 -0.003396377 -0.0033962757][-0.0033975511 -0.0033957826 -0.0033949164 -0.0033939972 -0.0033928331 -0.003391434 -0.0033903047 -0.0033896382 -0.0033892016 -0.0033890018 -0.0033893988 -0.0033900863 -0.0033908973 -0.0033911429 -0.0033912442][-0.0033911148 -0.0033892363 -0.003388345 -0.0033874304 -0.0033863033 -0.0033852593 -0.00338457 -0.0033842882 -0.0033841741 -0.0033842041 -0.0033847347 -0.0033854076 -0.0033861971 -0.0033866549 -0.0033870416][-0.0033864717 -0.0033842765 -0.0033831063 -0.0033820402 -0.0033807231 -0.0033797007 -0.0033790632 -0.0033789752 -0.0033788523 -0.0033788471 -0.0033792423 -0.003379676 -0.0033802395 -0.0033805806 -0.003380907][-0.0033800905 -0.0033779021 -0.0033769843 -0.0033762993 -0.0033756543 -0.0033752285 -0.0033751526 -0.0033754988 -0.0033755649 -0.0033757461 -0.0033762434 -0.0033765829 -0.0033767538 -0.0033767009 -0.0033768553][-0.0033721547 -0.0033696184 -0.0033690468 -0.0033687516 -0.0033684811 -0.0033685449 -0.0033689057 -0.0033696147 -0.0033701432 -0.0033708559 -0.003371614 -0.0033719216 -0.0033718457 -0.0033714611 -0.0033712385][-0.0033639201 -0.0033604975 -0.0033598463 -0.0033596791 -0.0033598337 -0.003360359 -0.0033612365 -0.0033622368 -0.0033631935 -0.0033644254 -0.0033653688 -0.0033656394 -0.0033652396 -0.0033643928 -0.0033635835][-0.0033596349 -0.0033555934 -0.0033545024 -0.00335392 -0.0033538071 -0.0033540998 -0.0033547238 -0.0033555652 -0.0033568174 -0.003358575 -0.0033599059 -0.0033604617 -0.0033599683 -0.0033588528 -0.0033577364][-0.0033592265 -0.0033552372 -0.0033542444 -0.0033535853 -0.0033536898 -0.003353924 -0.0033543648 -0.0033550458 -0.0033561881 -0.0033578337 -0.0033590682 -0.0033594398 -0.003358769 -0.0033577122 -0.0033566381][-0.0033596409 -0.0033558044 -0.0033551159 -0.0033547124 -0.0033551173 -0.0033557382 -0.0033566866 -0.0033577052 -0.0033590104 -0.0033604559 -0.0033613455 -0.0033614598 -0.0033606528 -0.0033595089 -0.0033582791][-0.0033603963 -0.003356644 -0.0033559501 -0.0033555152 -0.003355731 -0.0033561161 -0.0033568034 -0.0033576335 -0.0033589713 -0.0033602689 -0.0033610598 -0.0033612757 -0.0033607902 -0.0033598959 -0.0033588808][-0.0033608058 -0.0033570703 -0.003356036 -0.0033550402 -0.0033546996 -0.0033545149 -0.0033547853 -0.0033554516 -0.0033567171 -0.0033579851 -0.0033588987 -0.0033594568 -0.003359407 -0.0033589932 -0.0033584218][-0.0033611653 -0.0033574037 -0.0033561203 -0.0033547913 -0.0033539939 -0.0033533853 -0.0033533531 -0.0033535818 -0.003354264 -0.0033548763 -0.0033553406 -0.0033557573 -0.0033559871 -0.0033561881 -0.0033563175][-0.003362057 -0.0033582523 -0.0033568258 -0.0033553021 -0.0033542106 -0.0033532658 -0.0033529324 -0.0033530141 -0.0033534239 -0.0033537331 -0.0033539168 -0.0033540984 -0.0033543031 -0.0033545825 -0.003354829][-0.0033628158 -0.0033591716 -0.0033576104 -0.0033560782 -0.003354681 -0.0033537517 -0.0033532842 -0.0033533017 -0.0033536002 -0.0033537447 -0.0033538225 -0.0033538945 -0.0033540414 -0.0033542383 -0.0033543834]]...]
INFO - root - 2017-12-09 22:14:05.578228: step 64510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:48m:32s remains)
INFO - root - 2017-12-09 22:14:14.063732: step 64520, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 63h:13m:15s remains)
INFO - root - 2017-12-09 22:14:22.689115: step 64530, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:33m:11s remains)
INFO - root - 2017-12-09 22:14:31.226980: step 64540, loss = 0.89, batch loss = 0.69 (10.9 examples/sec; 0.731 sec/batch; 54h:24m:08s remains)
INFO - root - 2017-12-09 22:14:39.718688: step 64550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:26m:11s remains)
INFO - root - 2017-12-09 22:14:48.497121: step 64560, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:21m:41s remains)
INFO - root - 2017-12-09 22:14:57.063035: step 64570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:57m:14s remains)
INFO - root - 2017-12-09 22:15:05.484119: step 64580, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:10m:08s remains)
INFO - root - 2017-12-09 22:15:14.104738: step 64590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:16m:17s remains)
INFO - root - 2017-12-09 22:15:22.742897: step 64600, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 66h:36m:08s remains)
2017-12-09 22:15:23.689916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033615669 -0.003358322 -0.0033578996 -0.0033589951 -0.0033588335 -0.0033581669 -0.0033573529 -0.0033562542 -0.0033538118 -0.0033514998 -0.0033502637 -0.0033505326 -0.003351259 -0.003351829 -0.0033536507][-0.003367635 -0.0033649169 -0.0033639248 -0.0033645395 -0.0033645066 -0.0033640407 -0.0033630198 -0.0033619422 -0.0033599064 -0.0033581883 -0.0033575851 -0.0033582884 -0.0033596219 -0.0033610563 -0.0033632279][-0.0033752362 -0.0033727037 -0.0033716226 -0.0033718543 -0.0033719584 -0.0033719258 -0.0033714185 -0.0033707619 -0.0033690925 -0.0033676035 -0.0033670696 -0.003367322 -0.003368587 -0.0033701179 -0.0033729007][-0.0033786793 -0.0033760346 -0.0033750969 -0.0033752366 -0.0033754639 -0.0033755654 -0.0033753742 -0.0033744925 -0.0033727125 -0.0033709749 -0.0033700177 -0.0033694641 -0.0033699786 -0.0033712429 -0.0033743584][-0.0033774357 -0.0033742075 -0.0033731146 -0.0033731225 -0.0033729521 -0.0033735284 -0.0033739558 -0.0033732867 -0.0033719037 -0.0033705477 -0.0033698969 -0.0033689674 -0.0033688033 -0.0033697928 -0.0033733926][-0.0033725169 -0.0033687113 -0.0033676587 -0.003367574 -0.0033671779 -0.0033674468 -0.0033672785 -0.0033667579 -0.0033659718 -0.003365543 -0.0033653877 -0.0033647839 -0.0033648445 -0.0033659041 -0.0033696515][-0.0033654314 -0.003361593 -0.0033608964 -0.0033606663 -0.0033597464 -0.0033586971 -0.0033579136 -0.0033575227 -0.0033574395 -0.0033584947 -0.0033599737 -0.0033603823 -0.0033603425 -0.00336105 -0.0033644978][-0.0033581778 -0.0033548267 -0.0033549059 -0.0033547014 -0.0033529212 -0.0033508 -0.0033489973 -0.0033485228 -0.0033499112 -0.0033525031 -0.0033554118 -0.0033570055 -0.0033575862 -0.0033583171 -0.0033613457][-0.0033539403 -0.0033511545 -0.0033514637 -0.0033510942 -0.0033483373 -0.0033453493 -0.0033431188 -0.0033425353 -0.0033452227 -0.0033494986 -0.0033540709 -0.003357074 -0.0033585785 -0.0033598056 -0.0033625267][-0.0033516213 -0.00334947 -0.0033500846 -0.0033497736 -0.0033474681 -0.0033444255 -0.0033422729 -0.0033421849 -0.0033453503 -0.0033508316 -0.0033571527 -0.0033622491 -0.0033658675 -0.0033682445 -0.0033714203][-0.0033502078 -0.0033482213 -0.0033489822 -0.0033487352 -0.0033467487 -0.0033437414 -0.0033422802 -0.0033425915 -0.0033454273 -0.003351287 -0.0033585578 -0.0033655078 -0.0033712848 -0.0033754047 -0.0033791761][-0.003349605 -0.0033475067 -0.00334816 -0.0033483016 -0.0033462029 -0.003343584 -0.0033424096 -0.0033425803 -0.0033445293 -0.0033497764 -0.0033565282 -0.0033643139 -0.0033718562 -0.0033779496 -0.0033824909][-0.0033482022 -0.0033457174 -0.0033459954 -0.0033461316 -0.0033448781 -0.0033430408 -0.0033419144 -0.0033418052 -0.0033433633 -0.0033476076 -0.0033536591 -0.0033620766 -0.0033707602 -0.0033780769 -0.0033837222][-0.0033487305 -0.0033454236 -0.0033447994 -0.0033448674 -0.0033441104 -0.0033426075 -0.0033410948 -0.0033401919 -0.0033408117 -0.0033439628 -0.0033492919 -0.0033579674 -0.0033671223 -0.0033748029 -0.0033812788][-0.0033520837 -0.0033483063 -0.0033475666 -0.0033477675 -0.003347463 -0.003345839 -0.0033434408 -0.0033414687 -0.0033401574 -0.0033420653 -0.003346723 -0.0033552651 -0.0033644922 -0.0033724313 -0.0033793896]]...]
INFO - root - 2017-12-09 22:15:32.286126: step 64610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:35m:58s remains)
INFO - root - 2017-12-09 22:15:40.792658: step 64620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:53m:22s remains)
INFO - root - 2017-12-09 22:15:49.299506: step 64630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:13m:20s remains)
INFO - root - 2017-12-09 22:15:57.944469: step 64640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:52m:04s remains)
INFO - root - 2017-12-09 22:16:06.247695: step 64650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:58m:47s remains)
INFO - root - 2017-12-09 22:16:14.886349: step 64660, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:34m:52s remains)
INFO - root - 2017-12-09 22:16:23.391243: step 64670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:16m:36s remains)
INFO - root - 2017-12-09 22:16:31.953550: step 64680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:31m:28s remains)
INFO - root - 2017-12-09 22:16:40.765066: step 64690, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:26m:24s remains)
INFO - root - 2017-12-09 22:16:49.465485: step 64700, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.875 sec/batch; 65h:05m:06s remains)
2017-12-09 22:16:50.397771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.000282889 0.00024889922 0.000687815 0.00086658192 0.00073489151 0.00013943901 -0.00074818917 -0.001722872 -0.0023881202 -0.0027038776 -0.0028066542 -0.0028876362 -0.0029686952 -0.0030289073 -0.0030872477][0.00033758162 0.0010614463 0.0016937649 0.0020146116 0.001985884 0.0013777202 0.00035862159 -0.00091503956 -0.0019140416 -0.0024713259 -0.0026641982 -0.0027674667 -0.0028506769 -0.0029253124 -0.0029872255][0.0006166494 0.0014154122 0.0022058513 0.002740076 0.0029530355 0.0024757923 0.0014935506 6.5116677e-05 -0.001197323 -0.0020415215 -0.0023962646 -0.0026029148 -0.0027219092 -0.0028120442 -0.0028869291][0.00049686874 0.0011862081 0.0020101855 0.0027268243 0.0032840429 0.0031156864 0.0023867202 0.0010504804 -0.00025336584 -0.0012628124 -0.0017822611 -0.0021767356 -0.0024141986 -0.0025924623 -0.0027587635][0.00017404812 0.0006122354 0.00132752 0.002094235 0.0029708666 0.0031840026 0.0028676994 0.0018811689 0.0008374888 -8.8836066e-05 -0.00065126433 -0.0012092339 -0.0016187571 -0.0020030029 -0.0023954343][-9.2576258e-05 1.4356337e-06 0.0005307144 0.0012598757 0.002351708 0.002916123 0.0030427196 0.0024997869 0.0018987169 0.0012660588 0.00081380154 0.00023682974 -0.00030323002 -0.00093159429 -0.0016296422][-0.0002252867 -0.0005100572 -0.00019608601 0.00048837415 0.0016754994 0.0025379376 0.0030717005 0.0029541573 0.0027962097 0.0024573731 0.0021692447 0.0017167095 0.0011590894 0.00039914111 -0.00054626772][-0.00030183163 -0.00093398592 -0.00085234246 -0.00023976271 0.00090891519 0.0019946222 0.0028821512 0.0031289903 0.0033667989 0.0032544227 0.0031212543 0.0028257167 0.0023146148 0.001552603 0.00049371854][-0.0005174377 -0.0013272252 -0.0014304577 -0.00097476714 -1.5533064e-05 0.0011552733 0.0022764269 0.0028116729 0.0034040243 0.0035222743 0.0035990223 0.0034569975 0.0029941548 0.0022532076 0.0011473268][-0.00099165644 -0.0017618431 -0.0019455156 -0.0016769121 -0.00098072109 7.5602438e-05 0.0012279421 0.0019380425 0.002789953 0.0032165467 0.0036200548 0.0036738429 0.0032763134 0.0024854236 0.0013178799][-0.0016811654 -0.0022549741 -0.0024168845 -0.0022763321 -0.0018252393 -0.0010202848 -3.5024714e-05 0.00070146308 0.0016550913 0.0024127013 0.0031785963 0.0035067487 0.003222083 0.0023664117 0.0011273308][-0.0023769145 -0.0027178563 -0.0028173898 -0.0027172654 -0.0024525502 -0.0019407113 -0.0012329479 -0.00060493359 0.00031378027 0.0013245626 0.0023777431 0.0029848877 0.0028354053 0.0019712972 0.00070919166][-0.0028766759 -0.0030322433 -0.0030693128 -0.0029805133 -0.002859937 -0.0025906309 -0.0021589287 -0.0017236404 -0.0009133995 0.00021638861 0.0014432219 0.0022306216 0.0022228344 0.0014335846 0.00022241147][-0.0031454908 -0.0031728786 -0.0031511351 -0.0030849962 -0.0030581956 -0.00294268 -0.0027365459 -0.0025160885 -0.0018746214 -0.00075195893 0.00057506864 0.0014662214 0.0016327479 0.00098751369 -0.00011021295][-0.0032696326 -0.0032135802 -0.0031528084 -0.0031044148 -0.0031020769 -0.0030508935 -0.0030017477 -0.0029470588 -0.0025160022 -0.0014815812 -9.9730911e-05 0.00093320408 0.0013690756 0.00095019094 2.7911039e-05]]...]
INFO - root - 2017-12-09 22:16:58.935998: step 64710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:19m:12s remains)
INFO - root - 2017-12-09 22:17:07.717687: step 64720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:36m:16s remains)
INFO - root - 2017-12-09 22:17:16.487981: step 64730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:19m:41s remains)
INFO - root - 2017-12-09 22:17:25.147304: step 64740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:06m:24s remains)
INFO - root - 2017-12-09 22:17:33.601415: step 64750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:26m:28s remains)
INFO - root - 2017-12-09 22:17:42.251818: step 64760, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:02m:41s remains)
INFO - root - 2017-12-09 22:17:50.969400: step 64770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:52m:22s remains)
INFO - root - 2017-12-09 22:17:59.624801: step 64780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 62h:27m:47s remains)
INFO - root - 2017-12-09 22:18:08.339699: step 64790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:53m:52s remains)
INFO - root - 2017-12-09 22:18:17.202364: step 64800, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 64h:07m:08s remains)
2017-12-09 22:18:18.115723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003357278 -0.0033537354 -0.0033532195 -0.0033529731 -0.0033528309 -0.0033528612 -0.0033527913 -0.003352768 -0.0033527263 -0.0033526607 -0.0033526928 -0.0033527128 -0.0033526989 -0.003352622 -0.0033526393][-0.0033540607 -0.0033500674 -0.0033495864 -0.0033494548 -0.0033493272 -0.0033493158 -0.0033492181 -0.00334915 -0.00334907 -0.0033489985 -0.0033490611 -0.0033490688 -0.0033491317 -0.0033492497 -0.0033494525][-0.0033530658 -0.0033488977 -0.0033485319 -0.0033484146 -0.003348239 -0.0033481356 -0.0033479759 -0.0033477789 -0.0033475868 -0.0033475768 -0.0033477237 -0.0033478586 -0.0033481561 -0.0033485519 -0.0033489626][-0.0033518902 -0.0033475987 -0.0033473196 -0.0033473549 -0.0033473508 -0.00334749 -0.0033475179 -0.0033473775 -0.0033470553 -0.0033469726 -0.00334701 -0.0033470378 -0.0033473757 -0.0033479484 -0.003348517][-0.0033511964 -0.0033468602 -0.0033466904 -0.0033468972 -0.0033470984 -0.0033474348 -0.0033475677 -0.0033474676 -0.0033471363 -0.0033469554 -0.0033468688 -0.0033467477 -0.0033469889 -0.003347524 -0.0033481233][-0.003351056 -0.0033467342 -0.0033466907 -0.0033470478 -0.0033473317 -0.0033476087 -0.0033476092 -0.0033475007 -0.0033472565 -0.0033471514 -0.00334709 -0.003346914 -0.0033470539 -0.0033474765 -0.0033479496][-0.0033512444 -0.0033471596 -0.0033472267 -0.0033476416 -0.0033479417 -0.0033481219 -0.003348036 -0.0033478136 -0.0033476327 -0.0033476148 -0.0033476043 -0.003347422 -0.0033475326 -0.0033477983 -0.0033480581][-0.0033517282 -0.0033480173 -0.0033481561 -0.0033485426 -0.0033488544 -0.0033490865 -0.0033490788 -0.0033488865 -0.0033487657 -0.00334881 -0.0033487279 -0.0033484709 -0.0033484127 -0.0033484132 -0.0033484476][-0.0033522516 -0.0033487326 -0.0033489657 -0.00334934 -0.0033496295 -0.0033498448 -0.0033498653 -0.0033496949 -0.0033497147 -0.0033496881 -0.003349493 -0.0033491617 -0.0033490798 -0.0033489305 -0.0033487892][-0.0033525906 -0.0033490083 -0.0033493242 -0.0033495389 -0.0033496888 -0.0033497023 -0.0033496106 -0.0033495338 -0.0033496334 -0.0033496204 -0.0033494891 -0.0033493983 -0.0033494334 -0.0033492201 -0.0033490292][-0.0033527617 -0.0033490567 -0.0033494108 -0.0033494385 -0.0033493978 -0.0033493065 -0.0033491554 -0.0033491652 -0.0033493433 -0.0033494351 -0.0033494818 -0.0033495813 -0.0033496385 -0.0033493815 -0.0033492192][-0.0033527154 -0.0033487675 -0.0033490832 -0.0033490839 -0.0033489985 -0.0033489345 -0.003348832 -0.0033489096 -0.003349124 -0.0033492404 -0.0033493717 -0.0033494441 -0.0033494332 -0.0033492818 -0.0033492448][-0.003352792 -0.0033485326 -0.003348761 -0.0033487286 -0.0033486735 -0.0033486767 -0.0033486385 -0.0033488083 -0.0033490276 -0.0033491086 -0.0033492155 -0.0033491943 -0.0033491324 -0.00334907 -0.0033491836][-0.0033530637 -0.0033486017 -0.0033487678 -0.0033487326 -0.00334869 -0.0033486914 -0.0033486425 -0.003348744 -0.0033488348 -0.0033488302 -0.0033488877 -0.0033488867 -0.0033488485 -0.0033488732 -0.0033490469][-0.0033535543 -0.0033489848 -0.0033490246 -0.0033490001 -0.003348981 -0.0033489682 -0.0033489033 -0.0033489391 -0.0033489529 -0.0033489249 -0.0033489554 -0.0033489985 -0.0033489889 -0.0033490455 -0.0033491943]]...]
INFO - root - 2017-12-09 22:18:26.751725: step 64810, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 66h:45m:08s remains)
INFO - root - 2017-12-09 22:18:35.381658: step 64820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:57m:02s remains)
INFO - root - 2017-12-09 22:18:43.934238: step 64830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:50m:43s remains)
INFO - root - 2017-12-09 22:18:52.670380: step 64840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 62h:38m:57s remains)
INFO - root - 2017-12-09 22:19:01.161216: step 64850, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 64h:42m:45s remains)
INFO - root - 2017-12-09 22:19:09.845645: step 64860, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 61h:19m:34s remains)
INFO - root - 2017-12-09 22:19:18.422377: step 64870, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 62h:53m:10s remains)
INFO - root - 2017-12-09 22:19:26.929233: step 64880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 65h:41m:42s remains)
INFO - root - 2017-12-09 22:19:35.730412: step 64890, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:04m:25s remains)
INFO - root - 2017-12-09 22:19:44.526456: step 64900, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 65h:13m:27s remains)
2017-12-09 22:19:45.383187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033932305 -0.0033180041 -0.003094017 -0.0027389589 -0.0023928219 -0.0022270656 -0.002290728 -0.0025514453 -0.0028660393 -0.0031278525 -0.0032908684 -0.0033682443 -0.0033945893 -0.0034001553 -0.0034008785][-0.0033544831 -0.0031621233 -0.0026955958 -0.0020571034 -0.0014645278 -0.0012243465 -0.0013841239 -0.0018627309 -0.002427499 -0.0028972451 -0.0031933852 -0.0033370033 -0.0033877559 -0.0033989779 -0.0034001144][-0.0032189779 -0.0027434074 -0.0017694371 -0.000567856 0.00047486485 0.00084787305 0.00049782847 -0.00039671874 -0.0014527128 -0.0023512612 -0.0029413868 -0.0032455588 -0.0033645574 -0.0033961162 -0.0034006378][-0.0028824774 -0.0018465244 1.8134015e-05 0.0021857938 0.0039754212 0.0046139872 0.0039873077 0.002414868 0.000500527 -0.0011971286 -0.0023714465 -0.0030162993 -0.0032924474 -0.0033776476 -0.0033937627][-0.0022938196 -0.00041881157 0.0026475408 0.0060846126 0.00882158 0.0098314825 0.00888745 0.0064776987 0.0034325465 0.0006208925 -0.0014220527 -0.0026075272 -0.0031503493 -0.0033339353 -0.0033716792][-0.0015523693 0.0012369219 0.0055048019 0.010171705 0.013801333 0.01518395 0.013967907 0.0107779 0.0066131903 0.0026532023 -0.00032361411 -0.0021148215 -0.0029692221 -0.0032704684 -0.0033287813][-0.0009192985 0.0024920593 0.0074949628 0.012877932 0.017001148 0.018595319 0.017243566 0.013629952 0.0087949876 0.0040995339 0.00048558065 -0.0017411746 -0.0028285596 -0.0032141446 -0.0032723583][-0.000688741 0.0027586629 0.0077197161 0.013036141 0.017097279 0.018678319 0.017359944 0.013794221 0.00897694 0.0042600054 0.00059670606 -0.0016820197 -0.0028015494 -0.0031824091 -0.0032072824][-0.00098572625 0.0018986829 0.0060659321 0.010578442 0.014062615 0.015435431 0.014312234 0.011242399 0.0071059633 0.0030719254 -4.1233143e-05 -0.0019686581 -0.0028963808 -0.0031697885 -0.0031202824][-0.0016736821 0.00031706365 0.0032709178 0.0065528378 0.0091380877 0.01016476 0.0093181683 0.0070197219 0.0039788093 0.0010671134 -0.0011289208 -0.0024586781 -0.0030566447 -0.003167798 -0.0030002031][-0.002441505 -0.0013299529 0.00039557391 0.0023893488 0.0040019313 0.0046405075 0.0040866658 0.002628281 0.00076465239 -0.00095691369 -0.0022034575 -0.0029246572 -0.0031945824 -0.0031541791 -0.0028247884][-0.0030168276 -0.0025331997 -0.0017385735 -0.00077757752 1.8996187e-05 0.00032806327 3.1125033e-05 -0.00070937746 -0.0016136527 -0.0024027349 -0.0029411083 -0.0032306325 -0.0032859312 -0.0031552641 -0.0026527797][-0.0033099926 -0.0031613694 -0.0028985059 -0.002564972 -0.0022840928 -0.0021801081 -0.002296618 -0.0025645364 -0.0028747984 -0.0031233439 -0.0032827249 -0.0033603415 -0.0033337902 -0.0031809269 -0.0025691916][-0.0033965819 -0.0033721072 -0.0033219981 -0.0032518103 -0.0031908331 -0.0031695238 -0.0031973312 -0.0032552255 -0.003318076 -0.0033580353 -0.0033831475 -0.00339364 -0.0033634691 -0.0032320584 -0.0026344964][-0.0034029095 -0.0034016 -0.0033993914 -0.0033954198 -0.0033917893 -0.0033911008 -0.0033934428 -0.0033973244 -0.0034012515 -0.0033983972 -0.003398302 -0.0033989965 -0.0033865869 -0.0033006128 -0.002849303]]...]
INFO - root - 2017-12-09 22:19:54.071793: step 64910, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 66h:21m:23s remains)
INFO - root - 2017-12-09 22:20:02.760037: step 64920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:32m:02s remains)
INFO - root - 2017-12-09 22:20:11.456294: step 64930, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 64h:42m:17s remains)
INFO - root - 2017-12-09 22:20:20.187863: step 64940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:37m:38s remains)
INFO - root - 2017-12-09 22:20:28.669699: step 64950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 65h:41m:17s remains)
INFO - root - 2017-12-09 22:20:37.330918: step 64960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:30m:19s remains)
INFO - root - 2017-12-09 22:20:46.012824: step 64970, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:00m:26s remains)
INFO - root - 2017-12-09 22:20:54.632911: step 64980, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:22m:31s remains)
INFO - root - 2017-12-09 22:21:03.266647: step 64990, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 63h:25m:29s remains)
INFO - root - 2017-12-09 22:21:11.935832: step 65000, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 61h:54m:44s remains)
2017-12-09 22:21:12.792532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033554996 -0.0033572183 -0.0033626207 -0.0033676305 -0.003370696 -0.0033707242 -0.00336756 -0.003362705 -0.0033574675 -0.0033530178 -0.00334972 -0.0033477617 -0.003347066 -0.0033471358 -0.0033475265][-0.0033514672 -0.0033526679 -0.0033580295 -0.0033634608 -0.0033673479 -0.0033681944 -0.0033654114 -0.0033603176 -0.0033547953 -0.0033503217 -0.0033470425 -0.0033450839 -0.0033444129 -0.0033446087 -0.0033450713][-0.0033501517 -0.0033504302 -0.0033552083 -0.0033604228 -0.0033645467 -0.0033662606 -0.0033645043 -0.0033602249 -0.0033550439 -0.0033505615 -0.0033472022 -0.0033452259 -0.0033445552 -0.0033447195 -0.0033451326][-0.0033488185 -0.0033481768 -0.0033521494 -0.0033567369 -0.0033607497 -0.0033631688 -0.0033628079 -0.0033600575 -0.0033560784 -0.0033519124 -0.0033482478 -0.0033458176 -0.003344876 -0.0033447768 -0.0033450618][-0.0033479158 -0.0033464197 -0.0033495273 -0.0033536735 -0.0033578768 -0.0033611476 -0.0033622924 -0.0033611991 -0.0033586025 -0.0033549317 -0.0033506884 -0.0033474138 -0.0033456902 -0.0033450471 -0.0033450581][-0.0033474881 -0.0033453326 -0.0033480101 -0.0033521336 -0.0033568912 -0.0033611059 -0.0033637539 -0.0033642992 -0.0033627362 -0.0033592719 -0.0033543839 -0.0033499564 -0.0033470287 -0.0033455139 -0.0033449915][-0.0033471582 -0.0033449414 -0.0033477445 -0.0033524379 -0.0033583003 -0.0033637376 -0.0033677598 -0.0033693286 -0.0033680606 -0.0033641965 -0.0033584726 -0.003352874 -0.0033486274 -0.0033460425 -0.0033449698][-0.0033465598 -0.0033443689 -0.0033474695 -0.0033529317 -0.0033600433 -0.0033669898 -0.0033722431 -0.0033746227 -0.0033732406 -0.00336859 -0.003362108 -0.0033556086 -0.0033503484 -0.0033468532 -0.0033451617][-0.0033455458 -0.0033433142 -0.0033465675 -0.0033524872 -0.0033602759 -0.0033682294 -0.0033745586 -0.0033778227 -0.0033766569 -0.0033714022 -0.0033642265 -0.0033572228 -0.0033515617 -0.0033475971 -0.0033454155][-0.0033446206 -0.0033421423 -0.0033452127 -0.0033507543 -0.00335816 -0.0033659632 -0.0033725724 -0.0033765791 -0.0033761717 -0.003371424 -0.0033642626 -0.0033572763 -0.0033517012 -0.0033477498 -0.0033455463][-0.00334401 -0.0033412476 -0.0033439784 -0.0033486036 -0.0033548509 -0.003361596 -0.0033675134 -0.0033715072 -0.0033718627 -0.0033683311 -0.0033623448 -0.0033560626 -0.0033509112 -0.0033473864 -0.0033455144][-0.0033441791 -0.0033410583 -0.003343327 -0.0033469377 -0.0033518416 -0.0033573019 -0.0033620317 -0.0033652645 -0.0033659567 -0.0033638098 -0.0033596661 -0.0033547147 -0.0033503186 -0.003347168 -0.0033455309][-0.0033455221 -0.0033418306 -0.0033435083 -0.0033462013 -0.003349999 -0.0033546451 -0.0033587085 -0.003361458 -0.0033622079 -0.0033607706 -0.003357698 -0.0033536507 -0.0033499007 -0.0033470518 -0.0033455261][-0.0033477682 -0.0033440269 -0.0033453805 -0.0033475528 -0.0033507799 -0.0033550491 -0.0033589888 -0.003361796 -0.0033627746 -0.0033615795 -0.0033585804 -0.0033543187 -0.0033504174 -0.0033474772 -0.0033457659][-0.0033514611 -0.0033479861 -0.0033491419 -0.0033511752 -0.003354335 -0.0033586903 -0.0033627255 -0.0033658755 -0.003367329 -0.0033664363 -0.0033631278 -0.0033580973 -0.0033530237 -0.0033490644 -0.0033465538]]...]
INFO - root - 2017-12-09 22:21:21.518305: step 65010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:57m:52s remains)
INFO - root - 2017-12-09 22:21:30.447713: step 65020, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:29m:10s remains)
INFO - root - 2017-12-09 22:21:39.121194: step 65030, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:50m:12s remains)
INFO - root - 2017-12-09 22:21:47.744895: step 65040, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 64h:26m:21s remains)
INFO - root - 2017-12-09 22:21:56.164516: step 65050, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 64h:22m:19s remains)
INFO - root - 2017-12-09 22:22:04.783587: step 65060, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 60h:26m:20s remains)
INFO - root - 2017-12-09 22:22:13.518896: step 65070, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 66h:57m:17s remains)
INFO - root - 2017-12-09 22:22:22.169609: step 65080, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 61h:17m:48s remains)
INFO - root - 2017-12-09 22:22:30.724602: step 65090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:02m:34s remains)
INFO - root - 2017-12-09 22:22:39.096360: step 65100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:08m:00s remains)
2017-12-09 22:22:39.941477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003280598 -0.0026995456 -0.0011564947 0.0018956962 0.0065793712 0.012733908 0.019637059 0.025897721 0.030266659 0.031806413 0.030465385 0.026040578 0.019079259 0.011105303 0.0040377285][-0.0032650307 -0.0025213305 -0.00041664368 0.0039109159 0.010813691 0.020104766 0.030708397 0.040464904 0.047375128 0.04977084 0.047647946 0.040986784 0.030758994 0.019057715 0.0085352911][-0.0032568844 -0.0023769969 0.00030953251 0.0060368935 0.015450148 0.028331595 0.043155029 0.056786917 0.066204146 0.069137812 0.065642387 0.056274816 0.04261633 0.027271703 0.013370004][-0.0030603635 -0.00219581 0.00086795841 0.0077432143 0.019352695 0.035511792 0.054295179 0.071621627 0.083350167 0.086474761 0.081146859 0.06870272 0.05164649 0.033257458 0.016861834][-0.0025105015 -0.0017895448 0.0012576163 0.0085161058 0.021171449 0.039174329 0.060390659 0.080122188 0.093265586 0.09628424 0.089329146 0.074520066 0.055261314 0.035351232 0.018033424][-0.0019241349 -0.0013486287 0.0013058248 0.00796164 0.019983573 0.037545614 0.058643788 0.078605011 0.091900118 0.094616711 0.086810805 0.071113348 0.051639263 0.03240405 0.016264083][-0.0016534898 -0.0012518498 0.00074412 0.0060231928 0.015929502 0.030848553 0.04920952 0.067000352 0.079036906 0.081488423 0.074101135 0.059544683 0.042033758 0.025441788 0.012146412][-0.0017216852 -0.0014748676 -0.00023054192 0.0032932225 0.010233038 0.021105761 0.034918882 0.048749194 0.058443036 0.06069798 0.054933194 0.043322753 0.029494071 0.016812235 0.0071332995][-0.0020978888 -0.0019390818 -0.0012993866 0.00063325348 0.004664002 0.011298242 0.02008035 0.029272879 0.036066394 0.038026836 0.034376588 0.026513211 0.017111229 0.0087028816 0.0025946887][-0.0026759589 -0.0025298165 -0.0022280933 -0.0013742354 0.00050640269 0.0037796998 0.008341752 0.01341174 0.017432606 0.01888557 0.01705667 0.01264853 0.007284441 0.0025895678 -0.0006328437][-0.00316731 -0.00306857 -0.0029298323 -0.0026169228 -0.0019281248 -0.00068981829 0.0011400713 0.0033298095 0.0052288491 0.0060946913 0.00543068 0.003476761 0.0010271806 -0.0010796925 -0.0024316437][-0.0033544567 -0.0033376743 -0.003301875 -0.0032120387 -0.0030165792 -0.0026730918 -0.0021448599 -0.0014643108 -0.00081138453 -0.00045142183 -0.00060327211 -0.0012269537 -0.0020454344 -0.00273467 -0.0031465373][-0.0033697553 -0.0033688068 -0.0033696678 -0.0033641732 -0.0033403346 -0.0032868739 -0.0031943088 -0.0030670627 -0.0029304724 -0.0028367972 -0.0028433492 -0.0029579012 -0.0031221271 -0.0032604383 -0.0033367423][-0.0033694494 -0.0033678552 -0.0033699665 -0.0033722688 -0.0033723849 -0.0033704645 -0.0033668235 -0.0033619904 -0.0033581131 -0.0033569958 -0.0033596279 -0.0033659607 -0.0033723216 -0.0033743004 -0.003373238][-0.0033700704 -0.0033675819 -0.0033683339 -0.0033694578 -0.0033698089 -0.0033687179 -0.0033671355 -0.0033654287 -0.0033645679 -0.0033662023 -0.0033693374 -0.0033724369 -0.0033749973 -0.003375585 -0.0033752373]]...]
INFO - root - 2017-12-09 22:22:48.488095: step 65110, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 62h:47m:36s remains)
INFO - root - 2017-12-09 22:22:56.976574: step 65120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:53m:15s remains)
INFO - root - 2017-12-09 22:23:05.592754: step 65130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:54m:11s remains)
INFO - root - 2017-12-09 22:23:14.264771: step 65140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:22m:37s remains)
INFO - root - 2017-12-09 22:23:22.592365: step 65150, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 51h:44m:32s remains)
INFO - root - 2017-12-09 22:23:31.114222: step 65160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:14m:29s remains)
INFO - root - 2017-12-09 22:23:39.638011: step 65170, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:07m:38s remains)
INFO - root - 2017-12-09 22:23:48.298310: step 65180, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.710 sec/batch; 52h:42m:50s remains)
INFO - root - 2017-12-09 22:23:57.165488: step 65190, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 64h:34m:15s remains)
INFO - root - 2017-12-09 22:24:05.777228: step 65200, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 64h:07m:26s remains)
2017-12-09 22:24:06.653088: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15387817 0.18033728 0.19974102 0.21145278 0.21674761 0.21768466 0.21519233 0.21176058 0.20903672 0.20791402 0.20687969 0.2064122 0.20575744 0.2043142 0.20086323][0.16921027 0.19851622 0.21944714 0.231487 0.23699012 0.23711 0.23348024 0.22904538 0.2255006 0.22345722 0.22201823 0.22119369 0.22032514 0.21778713 0.21387729][0.17030846 0.19983493 0.22226101 0.23437601 0.23964101 0.23976678 0.23635556 0.23190358 0.2281799 0.22665156 0.22561288 0.22546905 0.22549097 0.22413439 0.2210656][0.16814439 0.19773351 0.21985719 0.23118839 0.23671073 0.23692712 0.23425154 0.23068382 0.22838058 0.22842425 0.22914667 0.23134261 0.23267041 0.23290305 0.23103632][0.16568071 0.19493303 0.21602046 0.22644234 0.23080464 0.23104461 0.22909057 0.22689052 0.22683132 0.22913857 0.23247232 0.23671561 0.2397759 0.24147761 0.24118809][0.16410042 0.19274239 0.21221325 0.22108303 0.22401007 0.22271167 0.22122219 0.22080547 0.22271854 0.22724755 0.23317373 0.23925878 0.24300261 0.24552947 0.24568269][0.15778975 0.18607591 0.20445538 0.2122076 0.21356237 0.21220067 0.21068783 0.21081127 0.21365294 0.21918178 0.22611065 0.23223342 0.23694916 0.23941442 0.23921432][0.14332917 0.16958171 0.186164 0.19306463 0.1937408 0.19228084 0.19131014 0.19282235 0.19713978 0.20355751 0.21109204 0.21674514 0.22066957 0.22144707 0.22058886][0.11697789 0.13903679 0.15258843 0.15779845 0.15766893 0.15650633 0.15611282 0.15872295 0.16431247 0.17208223 0.18046474 0.18589845 0.18936568 0.18904546 0.18682325][0.085518152 0.10089226 0.11021308 0.11346851 0.11199488 0.1102188 0.10979769 0.11304852 0.118751 0.12666838 0.13543718 0.14108756 0.14440516 0.14361693 0.14090328][0.053865828 0.063767485 0.069574215 0.070974223 0.068954341 0.066620372 0.065431088 0.067768708 0.0727666 0.079549387 0.086961836 0.092180341 0.095557988 0.09519612 0.092753634][0.026728155 0.032520115 0.035900377 0.036748532 0.035200763 0.033082202 0.031974144 0.033189312 0.03640683 0.040871419 0.045815732 0.049835537 0.052548658 0.052507684 0.050992765][0.0086118383 0.011002547 0.012427185 0.012880458 0.012338057 0.011400366 0.010993022 0.011791671 0.013708919 0.016166819 0.018611958 0.020687783 0.02205264 0.022204766 0.021500643][-0.00043324544 0.00013367017 0.00042084767 0.00048027211 0.00024843728 -8.8778324e-05 -0.00017552241 0.00028291042 0.0011495005 0.0021462112 0.0030779594 0.0038257476 0.0042536808 0.004280542 0.0039585503][-0.0029894642 -0.0029288693 -0.0028695555 -0.002865911 -0.0029091984 -0.002972167 -0.0029701127 -0.0028391383 -0.0026439156 -0.0023957845 -0.002176322 -0.0019997864 -0.0019036828 -0.0019193148 -0.0020103254]]...]
INFO - root - 2017-12-09 22:24:15.369449: step 65210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:21m:05s remains)
INFO - root - 2017-12-09 22:24:23.942589: step 65220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:05m:29s remains)
INFO - root - 2017-12-09 22:24:32.328036: step 65230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:18m:04s remains)
INFO - root - 2017-12-09 22:24:40.848506: step 65240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:28m:54s remains)
INFO - root - 2017-12-09 22:24:49.131637: step 65250, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.703 sec/batch; 52h:11m:12s remains)
INFO - root - 2017-12-09 22:24:57.517232: step 65260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:48m:53s remains)
INFO - root - 2017-12-09 22:25:06.086238: step 65270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:26m:40s remains)
INFO - root - 2017-12-09 22:25:14.554540: step 65280, loss = 0.89, batch loss = 0.68 (10.9 examples/sec; 0.732 sec/batch; 54h:20m:09s remains)
INFO - root - 2017-12-09 22:25:23.156354: step 65290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:44m:12s remains)
INFO - root - 2017-12-09 22:25:31.844330: step 65300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:16m:46s remains)
2017-12-09 22:25:32.721781: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0096259937 0.0096190246 0.0097869709 0.010081228 0.010772767 0.011937134 0.013442807 0.014276335 0.01415433 0.012352319 0.0094848163 0.0058676274 0.0023533909 -0.00043812278 -0.002180659][0.0089827869 0.0085249534 0.0082834763 0.0083636949 0.0092429034 0.011178487 0.013728997 0.015719583 0.016419634 0.015269209 0.012706105 0.0089370683 0.0048706923 0.0012557229 -0.0012913186][0.0055944333 0.0050554937 0.0047861123 0.0049225586 0.0059183389 0.0080346465 0.010945842 0.013549242 0.015036641 0.014810907 0.01312631 0.010131639 0.0064507695 0.0027030541 -0.00032086927][0.00147239 0.0011795077 0.0011653707 0.0014350771 0.0023815685 0.0043443944 0.0071767597 0.010061112 0.012102571 0.012684587 0.011847476 0.0097632324 0.0068367608 0.0034866768 0.00041999877][-0.0015793688 -0.0018163216 -0.0018003599 -0.0014340773 -0.00054015662 0.0011482893 0.0035886171 0.0062986249 0.0084450068 0.0095002819 0.0093798237 0.0081742769 0.0060899053 0.00339537 0.00064752321][-0.0030244554 -0.0031670155 -0.0031651368 -0.0030425803 -0.0026002799 -0.0014131558 0.00051973364 0.002860355 0.0048170611 0.0059683612 0.006252978 0.0057538729 0.0044685788 0.0024801057 0.00024793413][-0.0033696932 -0.003388403 -0.0033887608 -0.0033817736 -0.0032816392 -0.0028011533 -0.0017327406 -0.00017368747 0.0013359899 0.0024575761 0.0029820602 0.0029255308 0.0021764131 0.00080741453 -0.00078164763][-0.0033928081 -0.0033943537 -0.0033929357 -0.0033889129 -0.00337593 -0.0032432578 -0.0028558725 -0.0021766194 -0.0013714544 -0.00060472544 -7.4865995e-05 9.769178e-05 -0.00028866925 -0.0011067437 -0.0020199625][-0.0033836409 -0.0033899378 -0.0033939716 -0.0033929145 -0.0033898128 -0.0033557625 -0.0032652519 -0.0031020129 -0.002847048 -0.002540739 -0.0022446092 -0.002108308 -0.0022618016 -0.002597294 -0.0029448224][-0.0033887315 -0.0033964575 -0.0033974235 -0.0033959432 -0.0033934475 -0.0033900004 -0.0033850272 -0.0033671192 -0.0033277057 -0.0032711595 -0.0032044379 -0.0031857358 -0.0032227687 -0.0032846734 -0.0033346424][-0.003379294 -0.0033975111 -0.0033994692 -0.0033972349 -0.0033938449 -0.0033913674 -0.003390752 -0.0033898617 -0.0033883424 -0.0033849529 -0.0033807738 -0.0033784714 -0.0033778257 -0.003378306 -0.0033783396][-0.0033629409 -0.0033915823 -0.0033948496 -0.0033940997 -0.0033918116 -0.0033898558 -0.0033878803 -0.0033863694 -0.0033872419 -0.003386653 -0.0033848654 -0.00338404 -0.0033826497 -0.0033799915 -0.003377879][-0.0033511161 -0.0033856831 -0.0033905092 -0.0033899606 -0.0033880756 -0.0033864954 -0.0033738485 -0.0033636533 -0.003370347 -0.003378737 -0.0033834202 -0.003386796 -0.00338683 -0.0033818004 -0.0033795121][-0.0033640054 -0.0033853191 -0.0033883324 -0.0033874458 -0.0033849769 -0.0033831783 -0.0033469338 -0.0033101877 -0.003319259 -0.0033478041 -0.0033701314 -0.0033826539 -0.0033871883 -0.0033887869 -0.0033904347][-0.0033740513 -0.0033826735 -0.0033841319 -0.003384253 -0.0033830306 -0.0033803254 -0.0033184616 -0.0032415374 -0.0032374638 -0.0032861335 -0.0033378606 -0.0033719332 -0.0033866365 -0.003390857 -0.003390288]]...]
INFO - root - 2017-12-09 22:25:41.268081: step 65310, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:52m:12s remains)
INFO - root - 2017-12-09 22:25:49.875616: step 65320, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:19m:35s remains)
INFO - root - 2017-12-09 22:25:58.423572: step 65330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 63h:22m:54s remains)
INFO - root - 2017-12-09 22:26:06.840818: step 65340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 64h:17m:03s remains)
INFO - root - 2017-12-09 22:26:15.356624: step 65350, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 65h:57m:45s remains)
INFO - root - 2017-12-09 22:26:23.827444: step 65360, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:16m:06s remains)
INFO - root - 2017-12-09 22:26:32.498686: step 65370, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 64h:40m:27s remains)
INFO - root - 2017-12-09 22:26:40.832959: step 65380, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 50h:33m:04s remains)
INFO - root - 2017-12-09 22:26:49.477125: step 65390, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 66h:13m:58s remains)
INFO - root - 2017-12-09 22:26:58.216668: step 65400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:31m:38s remains)
2017-12-09 22:26:59.068032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00058378559 -0.00055976259 -0.000343743 -0.00030421512 -0.00010335306 6.0799764e-05 0.0004224577 0.00084362621 0.0011817908 0.001364548 0.0012579591 0.00095068221 0.00039329892 -0.00021566451 -0.00083315535][0.0018808672 0.0019399889 0.0022000282 0.0022849182 0.0026007553 0.0028643159 0.00332937 0.0037305818 0.0038539025 0.0035828988 0.002908143 0.002171579 0.0013287 0.00067938562 0.00018126634][0.0045728525 0.0048547992 0.0053594345 0.0058140606 0.0065563479 0.007297094 0.0081022214 0.0086229108 0.0085083069 0.0077099921 0.0063071372 0.00482041 0.0033065847 0.0021519342 0.0013869645][0.0070490334 0.007740682 0.0089311721 0.010462319 0.012425067 0.014364256 0.016064996 0.016969239 0.016627835 0.015060144 0.01251239 0.0097512612 0.0069245873 0.0046548164 0.0030464691][0.0090176361 0.01049218 0.012949659 0.016290985 0.020249803 0.024131449 0.027200744 0.028832572 0.028359955 0.02599643 0.022132669 0.017647244 0.012997529 0.00894822 0.00588334][0.010355277 0.013053909 0.01746171 0.023328535 0.029891858 0.036048077 0.040569868 0.042908069 0.042234112 0.039089505 0.033822019 0.02753208 0.020973727 0.015081175 0.010490256][0.01167765 0.015717749 0.02214475 0.030538762 0.039689127 0.048015606 0.0537832 0.056603178 0.055636451 0.05174889 0.045314632 0.037577458 0.029662991 0.022565058 0.017116418][0.013244966 0.018573331 0.026650827 0.03689095 0.047797017 0.057500836 0.063937157 0.066877209 0.065608643 0.061210737 0.054032728 0.045543194 0.037270412 0.030171143 0.025132949][0.014926977 0.021237234 0.030258307 0.041342206 0.052937925 0.06305258 0.069528855 0.072187878 0.070585027 0.0658262 0.058347166 0.049834989 0.042057376 0.036114838 0.032596171][0.016165745 0.02299582 0.032306433 0.043434329 0.054935947 0.0648157 0.07098572 0.073237658 0.0713701 0.066364028 0.058817782 0.050652098 0.043846287 0.039612569 0.038123325][0.016597578 0.023645325 0.032894112 0.043667737 0.0546935 0.0640146 0.069698595 0.071471281 0.06931518 0.064100064 0.056635235 0.049046513 0.04340237 0.041013658 0.041650366][0.016947733 0.024214145 0.033296198 0.043495312 0.053712875 0.062090859 0.06700749 0.068230055 0.065895021 0.060714595 0.053656742 0.046871755 0.042377137 0.041514821 0.043824557][0.019306378 0.026762549 0.035361469 0.044447988 0.05316624 0.060003862 0.063860424 0.064529583 0.062185846 0.057384245 0.051050443 0.045212485 0.041716687 0.041828014 0.045077372][0.02489935 0.032187961 0.0397443 0.047095358 0.053668104 0.058430571 0.060913861 0.060958557 0.058655128 0.054396126 0.048964139 0.044071086 0.041307531 0.041819759 0.04524114][0.033225041 0.039601695 0.045216579 0.050026156 0.05373555 0.055977892 0.056896087 0.056303572 0.054185521 0.05065842 0.046251785 0.042225562 0.039894905 0.040242963 0.043067764]]...]
INFO - root - 2017-12-09 22:27:07.671897: step 65410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:03m:18s remains)
INFO - root - 2017-12-09 22:27:16.447019: step 65420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:48m:51s remains)
INFO - root - 2017-12-09 22:27:25.249411: step 65430, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 66h:59m:01s remains)
INFO - root - 2017-12-09 22:27:33.960747: step 65440, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:29m:25s remains)
INFO - root - 2017-12-09 22:27:42.702620: step 65450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:47m:53s remains)
INFO - root - 2017-12-09 22:27:51.084213: step 65460, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 64h:51m:18s remains)
INFO - root - 2017-12-09 22:27:59.778072: step 65470, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 63h:51m:53s remains)
INFO - root - 2017-12-09 22:28:08.435540: step 65480, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 50h:27m:45s remains)
INFO - root - 2017-12-09 22:28:17.087814: step 65490, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 65h:51m:13s remains)
INFO - root - 2017-12-09 22:28:25.730367: step 65500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:34m:45s remains)
2017-12-09 22:28:26.567293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033642142 -0.0033614885 -0.0033614694 -0.0033622149 -0.0033634237 -0.003365421 -0.0033679227 -0.0033703214 -0.0033713111 -0.0033702052 -0.0033674072 -0.0033636284 -0.0033602205 -0.0033580286 -0.0033573096][-0.0033629453 -0.0033598295 -0.0033597951 -0.0033607781 -0.0033623942 -0.0033650536 -0.0033687484 -0.0033725707 -0.0033745917 -0.0033739342 -0.0033707377 -0.0033657956 -0.0033606337 -0.0033568523 -0.0033548987][-0.003363417 -0.0033602342 -0.0033603397 -0.0033613704 -0.0033630438 -0.0033660606 -0.0033705777 -0.0033756739 -0.0033789687 -0.0033792101 -0.0033762606 -0.0033708834 -0.00336476 -0.0033596358 -0.0033562982][-0.0033638352 -0.0033608775 -0.0033612156 -0.0033622063 -0.0033636491 -0.0033664706 -0.0033711328 -0.0033769051 -0.0033811305 -0.0033822537 -0.0033800583 -0.0033749761 -0.003368716 -0.0033631322 -0.0033590524][-0.0033644831 -0.0033621031 -0.003362787 -0.0033638005 -0.0033648042 -0.0033664943 -0.0033699565 -0.0033749794 -0.0033791496 -0.0033808325 -0.0033796304 -0.0033755847 -0.0033703228 -0.0033653989 -0.0033615148][-0.0033654019 -0.0033640631 -0.0033655521 -0.0033667986 -0.0033673437 -0.0033675483 -0.0033686988 -0.0033712895 -0.0033740895 -0.0033756623 -0.00337539 -0.0033727484 -0.0033689793 -0.0033653295 -0.0033623481][-0.0033663902 -0.0033665146 -0.0033693684 -0.0033713423 -0.0033715982 -0.0033704992 -0.0033691046 -0.0033682347 -0.0033684871 -0.003369105 -0.0033693016 -0.0033679341 -0.0033657528 -0.0033635157 -0.0033617062][-0.003367254 -0.0033687986 -0.0033734904 -0.0033768278 -0.0033775251 -0.0033757319 -0.0033721235 -0.0033675521 -0.0033642873 -0.0033628163 -0.0033627218 -0.0033622759 -0.0033616563 -0.0033607287 -0.0033599569][-0.0033675022 -0.0033703421 -0.0033767347 -0.0033816907 -0.0033835715 -0.0033819368 -0.0033767065 -0.0033694587 -0.0033629595 -0.0033585383 -0.0033570498 -0.0033568516 -0.003357233 -0.0033574065 -0.0033574486][-0.0033669206 -0.0033703751 -0.0033779161 -0.0033841678 -0.0033873387 -0.0033866516 -0.0033815228 -0.0033733293 -0.0033648503 -0.0033581217 -0.0033549042 -0.00335401 -0.0033544665 -0.0033552009 -0.0033557257][-0.003365577 -0.0033687293 -0.00337676 -0.0033838013 -0.0033880156 -0.0033885308 -0.0033846472 -0.0033769563 -0.003368085 -0.0033603173 -0.0033557033 -0.0033537943 -0.0033536395 -0.0033543922 -0.0033549985][-0.0033639651 -0.0033661106 -0.0033732916 -0.0033801473 -0.0033848146 -0.0033861897 -0.003383789 -0.0033773519 -0.0033694492 -0.0033621688 -0.0033571986 -0.0033546297 -0.0033539103 -0.0033544358 -0.0033549098][-0.003362434 -0.00336307 -0.0033686485 -0.0033743393 -0.0033787121 -0.0033806704 -0.0033796281 -0.003375063 -0.0033689453 -0.003363143 -0.0033586917 -0.0033559296 -0.0033549084 -0.0033552 -0.0033554528][-0.003361102 -0.0033601802 -0.0033640186 -0.003368082 -0.0033714522 -0.0033735128 -0.0033735642 -0.0033709416 -0.0033669106 -0.0033629537 -0.0033596787 -0.0033572898 -0.0033561308 -0.0033560926 -0.0033560996][-0.0033603469 -0.0033581494 -0.0033602642 -0.0033626955 -0.0033648694 -0.003366448 -0.0033669893 -0.0033658282 -0.0033637478 -0.0033615194 -0.0033594067 -0.0033577608 -0.0033567804 -0.0033565625 -0.0033564372]]...]
INFO - root - 2017-12-09 22:28:35.260839: step 65510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:36m:31s remains)
INFO - root - 2017-12-09 22:28:43.903153: step 65520, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:40m:24s remains)
INFO - root - 2017-12-09 22:28:52.648123: step 65530, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:18m:54s remains)
INFO - root - 2017-12-09 22:29:01.286300: step 65540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:39m:42s remains)
INFO - root - 2017-12-09 22:29:10.074279: step 65550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:15m:35s remains)
INFO - root - 2017-12-09 22:29:18.329537: step 65560, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:27m:59s remains)
INFO - root - 2017-12-09 22:29:26.976418: step 65570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:50m:24s remains)
INFO - root - 2017-12-09 22:29:35.584905: step 65580, loss = 0.89, batch loss = 0.68 (10.5 examples/sec; 0.762 sec/batch; 56h:29m:57s remains)
INFO - root - 2017-12-09 22:29:44.153105: step 65590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-09 22:29:52.603414: step 65600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:44m:37s remains)
2017-12-09 22:29:53.523058: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0037593402 0.0050653405 0.0068619149 0.0097219059 0.013961785 0.018954461 0.023200495 0.024904974 0.023303118 0.018668782 0.012430189 0.0062412061 0.0015540824 -0.0010980652 -0.0021761996][0.0046780324 0.006091224 0.0080383234 0.01101442 0.015155809 0.019769154 0.023503277 0.024547935 0.022480803 0.017729856 0.011702805 0.0058537619 0.0014682354 -0.00092363125 -0.0019170376][0.00615054 0.0075355461 0.00939597 0.012086795 0.015629755 0.0194929 0.022444593 0.022852987 0.020674936 0.016269553 0.010991555 0.0059944997 0.00228574 0.00020526 -0.00084911124][0.010113018 0.011440538 0.012952328 0.015034208 0.017598504 0.020323785 0.02217521 0.021869257 0.019386282 0.015123177 0.010267338 0.0057117008 0.0024585004 0.00066789077 -0.00025648507][0.01545886 0.017512705 0.019299919 0.021038286 0.022667345 0.023929266 0.024210468 0.022680582 0.019409331 0.014920091 0.010165039 0.0059392713 0.0028208012 0.000878677 -0.0002893426][0.022273405 0.024573805 0.026274567 0.027824631 0.02904526 0.029542541 0.028863562 0.026312815 0.022062078 0.016802961 0.011507628 0.0070153307 0.0037386471 0.0016565232 0.00036534923][0.032076869 0.034143131 0.035161853 0.035906043 0.036269832 0.03600876 0.034774896 0.03193187 0.027565021 0.022067154 0.016281059 0.011133762 0.0070001585 0.0038984998 0.0016421427][0.041935585 0.044214658 0.044920109 0.045063742 0.044583041 0.043485653 0.041515332 0.038234193 0.033719126 0.028306419 0.022708924 0.017365728 0.012606381 0.0085951779 0.005279243][0.050320774 0.053104665 0.053893816 0.053878125 0.053101577 0.051642958 0.049358204 0.045923132 0.041284375 0.035886854 0.030200569 0.024609635 0.019373924 0.014413293 0.010106989][0.055414926 0.058875304 0.060173154 0.060673244 0.06026873 0.059001394 0.05683146 0.05360477 0.049289342 0.044168025 0.038597282 0.032938328 0.0273378 0.021698434 0.016511789][0.0564397 0.060473286 0.062622868 0.063933313 0.064442225 0.064221844 0.063148342 0.060937561 0.057459772 0.052960746 0.047761951 0.04214472 0.036384 0.030341977 0.02454365][0.052729167 0.057490371 0.060772598 0.063371941 0.065216951 0.066201553 0.066472851 0.065619171 0.063664727 0.060547478 0.056421354 0.051470909 0.045807812 0.039559554 0.033330809][0.044440754 0.049629308 0.053998303 0.058107417 0.061614804 0.064308695 0.066291288 0.067046978 0.066459231 0.064555831 0.061715759 0.057948057 0.053206649 0.047462072 0.041395202][0.034041535 0.039132733 0.043894071 0.048690081 0.053262677 0.057399571 0.06098488 0.063549943 0.064873643 0.0647019 0.063305773 0.060684092 0.057024445 0.052113418 0.046602331][0.023209505 0.02785462 0.032473948 0.037253786 0.04203444 0.046590589 0.050866026 0.054513462 0.057259221 0.058807064 0.059200883 0.058272053 0.056085553 0.052403267 0.047967754]]...]
INFO - root - 2017-12-09 22:30:01.994309: step 65610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:01m:20s remains)
INFO - root - 2017-12-09 22:30:10.749791: step 65620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:46m:45s remains)
INFO - root - 2017-12-09 22:30:19.421560: step 65630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:57m:11s remains)
INFO - root - 2017-12-09 22:30:27.945853: step 65640, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 61h:19m:55s remains)
INFO - root - 2017-12-09 22:30:36.495407: step 65650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:15m:59s remains)
INFO - root - 2017-12-09 22:30:44.795461: step 65660, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 64h:43m:35s remains)
INFO - root - 2017-12-09 22:30:53.375249: step 65670, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 66h:34m:55s remains)
INFO - root - 2017-12-09 22:31:02.056128: step 65680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:39m:09s remains)
INFO - root - 2017-12-09 22:31:10.634166: step 65690, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 64h:55m:49s remains)
INFO - root - 2017-12-09 22:31:19.453287: step 65700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:47m:45s remains)
2017-12-09 22:31:20.262412: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19280541 0.17878196 0.16482604 0.15253003 0.14222676 0.1343413 0.12699455 0.11984993 0.11225808 0.10248764 0.089441776 0.0725844 0.054102153 0.036219943 0.020306688][0.23737711 0.22401024 0.21002516 0.19720677 0.1866985 0.17849651 0.17114535 0.16379432 0.15549782 0.14400364 0.12712559 0.10462026 0.078878239 0.053562336 0.030861298][0.27697331 0.26544604 0.25327924 0.24169539 0.23199266 0.22367357 0.21605751 0.20810738 0.19881678 0.1855097 0.16531356 0.13784249 0.10545804 0.073137969 0.043606985][0.30681446 0.29918829 0.29067498 0.2823188 0.27564695 0.26948157 0.26319396 0.25509858 0.24455556 0.22871484 0.20423712 0.17081359 0.13124728 0.091819458 0.055843413][0.32255292 0.31905448 0.31491771 0.31074384 0.30797938 0.30484203 0.30094734 0.29400957 0.28364488 0.26647124 0.23905788 0.20094284 0.15531878 0.10935849 0.067222059][0.32532188 0.32536021 0.3244687 0.32400763 0.32475325 0.32449773 0.32265535 0.31703442 0.30739245 0.28993058 0.26122069 0.22050984 0.17147405 0.12168851 0.075645462][0.31542003 0.31782764 0.31887671 0.32090905 0.32420352 0.326091 0.32575223 0.3209303 0.31200078 0.29502133 0.26660413 0.22579171 0.176362 0.12579483 0.078678571][0.2954835 0.29810289 0.29872429 0.30092862 0.30397144 0.30542925 0.30451548 0.29981592 0.29171634 0.27601236 0.24976918 0.21180315 0.16585381 0.11862236 0.074230328][0.26614588 0.26824242 0.26677582 0.26760417 0.26897007 0.26896569 0.26638633 0.26090008 0.25313097 0.23934451 0.21663789 0.18355282 0.14386493 0.10296965 0.0643053][0.2278944 0.22615926 0.22069848 0.21889214 0.21817859 0.21632114 0.21248887 0.2071199 0.20044884 0.18938848 0.17133734 0.14523189 0.11408072 0.081644177 0.050723985][0.1848385 0.17819956 0.16783841 0.1628398 0.15993632 0.156833 0.15243208 0.1476562 0.14247854 0.13451762 0.12156574 0.10291018 0.080752455 0.057615444 0.035408564][0.1408617 0.13075869 0.11773876 0.11087444 0.10659203 0.1026024 0.097982548 0.093312405 0.089215919 0.083731167 0.075269274 0.063362777 0.049452335 0.034879223 0.020741582][0.10349645 0.089906693 0.075421818 0.06762071 0.063143775 0.059399724 0.055382233 0.051600758 0.048340637 0.044640828 0.039217129 0.032182746 0.024580836 0.01674569 0.0090900436][0.075950854 0.059849173 0.044708572 0.037377689 0.034110829 0.031875532 0.029427489 0.026966181 0.024725625 0.02222568 0.018688016 0.014563851 0.010596602 0.0066009304 0.0026515012][0.060803518 0.043764986 0.029482184 0.023297541 0.02225519 0.02205826 0.021783082 0.020749783 0.01910097 0.01665465 0.012958104 0.0092014037 0.0058643613 0.0028604895 4.2555155e-05]]...]
INFO - root - 2017-12-09 22:31:29.088174: step 65710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 65h:30m:13s remains)
INFO - root - 2017-12-09 22:31:37.823175: step 65720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:31m:16s remains)
INFO - root - 2017-12-09 22:31:46.391547: step 65730, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 62h:54m:33s remains)
INFO - root - 2017-12-09 22:31:55.086277: step 65740, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:39m:52s remains)
INFO - root - 2017-12-09 22:32:03.779783: step 65750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:44m:30s remains)
INFO - root - 2017-12-09 22:32:12.043863: step 65760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:08m:10s remains)
INFO - root - 2017-12-09 22:32:20.670440: step 65770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:05m:35s remains)
INFO - root - 2017-12-09 22:32:29.472102: step 65780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:07m:43s remains)
INFO - root - 2017-12-09 22:32:38.064252: step 65790, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 66h:40m:39s remains)
INFO - root - 2017-12-09 22:32:46.901614: step 65800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 65h:59m:19s remains)
2017-12-09 22:32:47.738346: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11273059 0.10826641 0.10346959 0.10146088 0.10091712 0.10253846 0.10430864 0.10530122 0.10188557 0.093033418 0.0786022 0.060190961 0.039699763 0.021404393 0.0075932574][0.13932373 0.133616 0.1271313 0.12417062 0.12324417 0.12580158 0.12831692 0.12913981 0.12452418 0.11315313 0.094949767 0.071487121 0.046129253 0.024013238 0.0083909919][0.16162555 0.15565838 0.14879532 0.14606418 0.14585051 0.14941166 0.15167345 0.15086494 0.14276722 0.12713242 0.10420505 0.0764662 0.048295934 0.024530495 0.0086633218][0.17765087 0.17218108 0.16516778 0.16273957 0.16347207 0.16836669 0.17068131 0.16852458 0.15708049 0.13703038 0.10880559 0.076768659 0.046181485 0.022201272 0.0074649118][0.18741828 0.18391749 0.17811178 0.17622212 0.17768343 0.18341476 0.18555853 0.18158165 0.16633873 0.14182149 0.10886673 0.073465288 0.041752264 0.018873258 0.0058009345][0.19064158 0.19004655 0.18597405 0.18561921 0.18835372 0.19435029 0.1957058 0.1895587 0.17087948 0.1422137 0.10566847 0.068353221 0.036470246 0.015049882 0.0039155316][0.18992519 0.19275542 0.19046815 0.19080737 0.19349076 0.19911025 0.19918111 0.19057646 0.16904351 0.13761888 0.099441811 0.061940581 0.03147376 0.012252225 0.0029685672][0.1862227 0.19231652 0.19114567 0.19148238 0.19299096 0.19644807 0.19417801 0.18370189 0.16060488 0.12836014 0.090827912 0.055275243 0.027460435 0.010759103 0.0032643436][0.17925157 0.18835057 0.18765321 0.18674651 0.18614416 0.18680184 0.18219733 0.1704872 0.14743455 0.1166926 0.081966415 0.050173506 0.02593928 0.011687852 0.0055286912][0.16624595 0.17671081 0.17582184 0.17385381 0.17126682 0.16919507 0.16274618 0.15095036 0.12988615 0.1029329 0.073285267 0.046802249 0.026842115 0.014952034 0.0099226031][0.14939372 0.15875196 0.15589312 0.15167247 0.14629123 0.14128895 0.13339452 0.12292881 0.10633745 0.086067244 0.064103059 0.044318553 0.029302737 0.020254951 0.016520785][0.12942195 0.1368649 0.13190313 0.12506779 0.11673319 0.1087135 0.099355116 0.090159021 0.078146726 0.065335959 0.051741958 0.039856527 0.031105353 0.025986172 0.024619773][0.10688968 0.11220077 0.10619589 0.097763322 0.087868847 0.078345291 0.068751983 0.060964674 0.052870885 0.045709714 0.038735751 0.033182081 0.029653838 0.028654622 0.030297419][0.081861712 0.085314669 0.079574175 0.07130181 0.061846621 0.052445132 0.043773249 0.037606325 0.032775983 0.029761871 0.02756699 0.026718954 0.027258648 0.029390289 0.033108182][0.055864129 0.057387087 0.052548993 0.045832735 0.038273331 0.030607847 0.024026455 0.019778892 0.017510187 0.017140236 0.017877568 0.019622259 0.022624137 0.027010903 0.032416936]]...]
INFO - root - 2017-12-09 22:32:56.481724: step 65810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:14m:49s remains)
INFO - root - 2017-12-09 22:33:05.148939: step 65820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:34m:42s remains)
INFO - root - 2017-12-09 22:33:13.803712: step 65830, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 62h:39m:11s remains)
INFO - root - 2017-12-09 22:33:22.542085: step 65840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:31m:23s remains)
INFO - root - 2017-12-09 22:33:31.119558: step 65850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:34m:28s remains)
INFO - root - 2017-12-09 22:33:39.448354: step 65860, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 64h:10m:56s remains)
INFO - root - 2017-12-09 22:33:48.071358: step 65870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:03m:23s remains)
INFO - root - 2017-12-09 22:33:56.683353: step 65880, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.881 sec/batch; 65h:16m:38s remains)
INFO - root - 2017-12-09 22:34:05.196153: step 65890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:42m:09s remains)
INFO - root - 2017-12-09 22:34:14.123921: step 65900, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:16m:33s remains)
2017-12-09 22:34:15.000651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027931409 -0.0024116696 -0.0019631642 -0.0015170759 -0.0012368998 -0.0010632812 -0.000995856 -0.00090472284 -0.00069962908 -0.00044858898 -8.6947577e-05 0.00018889667 0.00039574085 0.00059790094 0.00085468334][-0.003008733 -0.0027176414 -0.0023700432 -0.0020036246 -0.0017113985 -0.0015178864 -0.0013825742 -0.0012958534 -0.0011689325 -0.00091822329 -0.00052420562 -0.00015545962 0.00012688432 0.00037844316 0.00060051656][-0.0032476378 -0.0031001964 -0.0029272381 -0.0027569509 -0.0026180858 -0.0025118424 -0.0024281284 -0.0023140947 -0.0021373045 -0.0018530575 -0.0015143055 -0.0011429363 -0.00078065973 -0.00046626059 -0.0001750784][-0.0033933423 -0.0033541273 -0.0032917054 -0.003228823 -0.0031666148 -0.0031118623 -0.0030595439 -0.0029869957 -0.0028671478 -0.0026434409 -0.0023533036 -0.0020289565 -0.0017044708 -0.0013565577 -0.00099073653][-0.003414799 -0.0034102872 -0.0034033908 -0.0033937846 -0.0033857061 -0.0033771005 -0.0033596423 -0.0033147826 -0.0032344973 -0.0031233225 -0.002952453 -0.0027291954 -0.0024173353 -0.0020437874 -0.0016143601][-0.0034144511 -0.0034110225 -0.0034070741 -0.0034011721 -0.0033920538 -0.0033744739 -0.0033505044 -0.0033237673 -0.0033053677 -0.00328266 -0.0032165216 -0.0031065233 -0.0029046461 -0.002529667 -0.0019904082][-0.0034124132 -0.0034082253 -0.003404266 -0.0033946622 -0.0033764541 -0.0033461605 -0.0033232868 -0.0033037215 -0.0032896074 -0.0032942889 -0.0032850152 -0.0032637818 -0.0031459057 -0.0028241132 -0.0022861529][-0.0034101103 -0.0034061235 -0.0034038732 -0.0033973823 -0.0033870121 -0.003364455 -0.0033311921 -0.0032843214 -0.0032451188 -0.0032399851 -0.0032708836 -0.0033083241 -0.0032490876 -0.0029944009 -0.002507865][-0.0034064916 -0.0034060017 -0.0034017614 -0.0033980005 -0.003394495 -0.003385751 -0.0033719121 -0.0033267317 -0.0032894271 -0.0032669557 -0.0032760364 -0.0033060648 -0.0032439823 -0.0030693109 -0.0026845778][-0.0034070259 -0.0034025002 -0.0033986564 -0.003400407 -0.0033992794 -0.0033956268 -0.0033870689 -0.0033640256 -0.0033408045 -0.0033034673 -0.0032904788 -0.0032837971 -0.0032490212 -0.0031169313 -0.0027904809][-0.0034049999 -0.0034029139 -0.0034024096 -0.0034028215 -0.0034031125 -0.0034049894 -0.0034040867 -0.0034002617 -0.0033891536 -0.0033586153 -0.0033262041 -0.0032884008 -0.0032619585 -0.0031598611 -0.0029332344][-0.0034036273 -0.0034005593 -0.0034025877 -0.0034054464 -0.0034081184 -0.0034102758 -0.0034096166 -0.0034069908 -0.0034004855 -0.0033882784 -0.0033712995 -0.0033478059 -0.0033132487 -0.0032354356 -0.0031127241][-0.0034054408 -0.0034043698 -0.0034052287 -0.0034066876 -0.0034091908 -0.0034099985 -0.003407158 -0.0034028778 -0.0033979123 -0.0033894854 -0.0033846714 -0.0033834148 -0.0033750536 -0.0033454397 -0.0032904618][-0.0034065433 -0.0034058574 -0.0034072795 -0.0034084956 -0.003409774 -0.0034080422 -0.0034025023 -0.0033841 -0.0033535082 -0.0033336752 -0.003339893 -0.0033685563 -0.0033925881 -0.0033962817 -0.0033872998][-0.0034073431 -0.0034059293 -0.003406517 -0.0034070744 -0.0034082895 -0.0034038418 -0.0033886174 -0.0033537955 -0.0033048422 -0.0032729136 -0.0032905238 -0.003343534 -0.0033878526 -0.0034022098 -0.0034017898]]...]
INFO - root - 2017-12-09 22:34:23.570076: step 65910, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 60h:54m:23s remains)
INFO - root - 2017-12-09 22:34:32.195826: step 65920, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:29m:36s remains)
INFO - root - 2017-12-09 22:34:40.824233: step 65930, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 64h:56m:41s remains)
INFO - root - 2017-12-09 22:34:49.473565: step 65940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:10m:20s remains)
INFO - root - 2017-12-09 22:34:58.177714: step 65950, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 65h:42m:21s remains)
INFO - root - 2017-12-09 22:35:06.643936: step 65960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:30m:35s remains)
INFO - root - 2017-12-09 22:35:15.560543: step 65970, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 65h:32m:02s remains)
INFO - root - 2017-12-09 22:35:24.246566: step 65980, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:31m:47s remains)
INFO - root - 2017-12-09 22:35:32.853988: step 65990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:52m:22s remains)
INFO - root - 2017-12-09 22:35:41.483793: step 66000, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:15m:08s remains)
2017-12-09 22:35:42.505834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033808339 -0.0033793484 -0.0033788935 -0.0033786302 -0.0033783924 -0.0033784078 -0.0033786274 -0.0033790956 -0.0033797661 -0.0033802756 -0.0033805629 -0.0033806872 -0.0033808614 -0.0033811145 -0.0033812351][-0.0033785684 -0.0033769319 -0.0033765428 -0.0033763151 -0.0033760085 -0.003375903 -0.0033760131 -0.0033764811 -0.0033772562 -0.0033780811 -0.0033786865 -0.0033790735 -0.0033793233 -0.0033795536 -0.0033796127][-0.0033781573 -0.0033764166 -0.0033760942 -0.0033757444 -0.003375395 -0.003375168 -0.0033753291 -0.003375879 -0.003376744 -0.0033777291 -0.0033785729 -0.0033792087 -0.0033795959 -0.00337982 -0.0033798146][-0.0033776439 -0.0033756352 -0.0033751 -0.0033745938 -0.003374266 -0.0033741633 -0.0033745118 -0.0033752462 -0.0033762262 -0.003377306 -0.0033782411 -0.0033789882 -0.0033794793 -0.0033797852 -0.0033798157][-0.0033776644 -0.0033753528 -0.0033745167 -0.0033735861 -0.0033729326 -0.0033728436 -0.0033732639 -0.003374296 -0.0033755186 -0.003376794 -0.0033778108 -0.0033786786 -0.0033793056 -0.0033797377 -0.0033798302][-0.003378347 -0.0033758674 -0.0033748334 -0.003373584 -0.0033725007 -0.0033719286 -0.00337201 -0.003373008 -0.00337447 -0.0033760751 -0.0033773934 -0.003378487 -0.0033792539 -0.0033797568 -0.0033798704][-0.0033794823 -0.0033771659 -0.0033762893 -0.0033749377 -0.0033733295 -0.0033718797 -0.0033711521 -0.0033716972 -0.0033732031 -0.003375174 -0.0033769186 -0.0033783047 -0.0033791806 -0.0033797431 -0.0033798853][-0.003380927 -0.0033791603 -0.0033789019 -0.0033777291 -0.0033757156 -0.0033733011 -0.0033715956 -0.0033713204 -0.0033722313 -0.0033740967 -0.0033761358 -0.0033778057 -0.0033787845 -0.0033795314 -0.0033797855][-0.003382307 -0.0033811512 -0.003381562 -0.0033809643 -0.0033791079 -0.0033763368 -0.0033736995 -0.0033723672 -0.0033722098 -0.003373178 -0.0033750434 -0.0033769733 -0.0033782783 -0.0033791717 -0.0033795799][-0.0033828535 -0.0033821673 -0.0033833494 -0.0033835873 -0.0033824763 -0.0033799531 -0.0033772017 -0.0033750108 -0.003373544 -0.0033732732 -0.0033743475 -0.0033761065 -0.0033775894 -0.003378693 -0.0033792821][-0.0033821932 -0.0033820092 -0.0033840118 -0.0033852907 -0.0033853266 -0.0033836302 -0.0033811335 -0.0033785619 -0.003376259 -0.0033748285 -0.0033746432 -0.0033756616 -0.0033769568 -0.0033781838 -0.0033790013][-0.0033814386 -0.0033813966 -0.0033840227 -0.0033862244 -0.003387237 -0.0033864679 -0.003384491 -0.0033821256 -0.003379446 -0.0033770329 -0.0033758387 -0.0033759468 -0.0033767854 -0.0033779431 -0.0033788576][-0.0033814542 -0.0033811934 -0.0033839312 -0.0033865778 -0.0033883115 -0.0033881767 -0.0033867364 -0.0033846512 -0.0033819193 -0.0033791226 -0.0033773547 -0.0033769477 -0.0033772455 -0.0033780988 -0.0033788993][-0.0033814635 -0.0033811571 -0.0033836043 -0.0033859687 -0.0033878118 -0.0033881636 -0.0033872153 -0.0033854297 -0.0033829936 -0.00338059 -0.0033787733 -0.0033780069 -0.0033779419 -0.003378469 -0.0033790488][-0.0033813098 -0.0033808411 -0.0033827862 -0.0033847003 -0.00338621 -0.0033867275 -0.0033862293 -0.0033850335 -0.0033832351 -0.0033813629 -0.0033797275 -0.0033787834 -0.003378521 -0.0033788173 -0.0033792052]]...]
INFO - root - 2017-12-09 22:35:51.106626: step 66010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:33m:57s remains)
INFO - root - 2017-12-09 22:35:59.830125: step 66020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 65h:53m:48s remains)
INFO - root - 2017-12-09 22:36:08.465349: step 66030, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 64h:57m:58s remains)
INFO - root - 2017-12-09 22:36:17.026165: step 66040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:16m:34s remains)
INFO - root - 2017-12-09 22:36:25.480681: step 66050, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:05m:45s remains)
INFO - root - 2017-12-09 22:36:33.742365: step 66060, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 56h:55m:38s remains)
INFO - root - 2017-12-09 22:36:42.407994: step 66070, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 62h:46m:53s remains)
INFO - root - 2017-12-09 22:36:51.118946: step 66080, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 62h:32m:02s remains)
INFO - root - 2017-12-09 22:36:59.512461: step 66090, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:03m:27s remains)
INFO - root - 2017-12-09 22:37:08.324957: step 66100, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 66h:01m:04s remains)
2017-12-09 22:37:09.212173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033766541 -0.0033746625 -0.0033744345 -0.0033743922 -0.0033743437 -0.0033743083 -0.0033742643 -0.0033742138 -0.0033741896 -0.0033740986 -0.0033740473 -0.0033740124 -0.0033740094 -0.0033741058 -0.003374184][-0.0033752739 -0.0033730681 -0.0033728885 -0.003372889 -0.003372872 -0.0033727961 -0.003372641 -0.0033724662 -0.0033723509 -0.0033722972 -0.0033723162 -0.0033723947 -0.0033725526 -0.0033727551 -0.0033729142][-0.0033754616 -0.0033731069 -0.0033729859 -0.003373031 -0.0033730324 -0.003372909 -0.003372577 -0.0033722268 -0.0033719868 -0.0033718671 -0.0033718455 -0.0033719682 -0.0033722839 -0.0033725854 -0.0033727787][-0.0033755091 -0.0033731109 -0.0033730206 -0.0033731121 -0.0033731025 -0.0033728725 -0.003372333 -0.0033718 -0.0033714031 -0.0033711984 -0.0033711793 -0.0033714371 -0.0033719833 -0.0033724376 -0.0033727076][-0.0033755798 -0.00337309 -0.0033730068 -0.0033730846 -0.0033729472 -0.0033725381 -0.0033718015 -0.0033711037 -0.0033706094 -0.0033703784 -0.0033704522 -0.0033709004 -0.0033716618 -0.0033722497 -0.0033725915][-0.0033756888 -0.0033731174 -0.0033730166 -0.0033730227 -0.0033727249 -0.0033721433 -0.0033712934 -0.0033704615 -0.0033698948 -0.0033696638 -0.0033698247 -0.0033703956 -0.0033712492 -0.0033719495 -0.003372411][-0.0033756911 -0.00337306 -0.0033728906 -0.0033727717 -0.003372309 -0.003371597 -0.0033707414 -0.0033699279 -0.003369346 -0.0033691765 -0.003369435 -0.0033700783 -0.0033709363 -0.0033716934 -0.0033722522][-0.0033755829 -0.0033729132 -0.0033726559 -0.00337244 -0.0033718883 -0.0033711528 -0.0033704236 -0.0033697435 -0.0033692196 -0.0033691619 -0.0033694708 -0.0033701076 -0.0033708739 -0.0033715796 -0.0033721358][-0.0033752674 -0.0033726732 -0.0033723575 -0.0033720271 -0.0033714487 -0.0033708364 -0.0033703696 -0.0033699498 -0.0033696413 -0.0033697349 -0.0033700338 -0.0033704983 -0.0033709994 -0.0033714939 -0.0033719295][-0.0033749184 -0.0033723845 -0.0033720601 -0.0033717069 -0.003371269 -0.0033709342 -0.0033707984 -0.0033706694 -0.0033705758 -0.0033707537 -0.0033709425 -0.0033711004 -0.0033712399 -0.0033714706 -0.0033717528][-0.0033745952 -0.0033721356 -0.0033719633 -0.0033717032 -0.0033714972 -0.0033714259 -0.003371557 -0.0033716655 -0.0033716692 -0.0033718329 -0.0033718538 -0.0033717437 -0.003371574 -0.0033715677 -0.003371716][-0.0033744308 -0.003372102 -0.0033720601 -0.0033719265 -0.0033719137 -0.0033720233 -0.0033722718 -0.003372482 -0.0033725132 -0.0033726059 -0.0033724902 -0.0033722448 -0.0033719577 -0.0033718189 -0.0033718217][-0.0033745032 -0.0033721647 -0.003372258 -0.0033722271 -0.0033723388 -0.00337256 -0.0033728189 -0.0033730147 -0.0033730445 -0.0033730615 -0.0033728832 -0.0033725977 -0.0033722932 -0.0033720727 -0.0033719847][-0.0033747319 -0.0033722981 -0.0033724543 -0.0033724648 -0.0033725929 -0.0033727894 -0.0033729717 -0.0033730918 -0.0033730946 -0.0033730462 -0.0033728885 -0.0033726492 -0.0033723898 -0.0033722043 -0.0033721335][-0.003375 -0.0033724084 -0.0033725679 -0.0033725984 -0.0033727039 -0.0033728059 -0.0033728918 -0.0033729356 -0.0033728997 -0.0033728294 -0.0033727179 -0.0033725821 -0.0033724247 -0.0033723391 -0.00337235]]...]
INFO - root - 2017-12-09 22:37:17.955697: step 66110, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 64h:17m:18s remains)
INFO - root - 2017-12-09 22:37:26.743839: step 66120, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 63h:15m:17s remains)
INFO - root - 2017-12-09 22:37:35.560951: step 66130, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 0.996 sec/batch; 73h:41m:11s remains)
INFO - root - 2017-12-09 22:37:44.239329: step 66140, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:46m:36s remains)
INFO - root - 2017-12-09 22:37:52.998072: step 66150, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:19m:40s remains)
INFO - root - 2017-12-09 22:38:01.409935: step 66160, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.722 sec/batch; 53h:25m:21s remains)
INFO - root - 2017-12-09 22:38:10.162416: step 66170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:12m:19s remains)
INFO - root - 2017-12-09 22:38:18.951520: step 66180, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.894 sec/batch; 66h:06m:07s remains)
INFO - root - 2017-12-09 22:38:27.490570: step 66190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:59m:22s remains)
INFO - root - 2017-12-09 22:38:36.135913: step 66200, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 62h:09m:00s remains)
2017-12-09 22:38:37.005206: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27483654 0.2837308 0.28579327 0.2822108 0.27538696 0.266077 0.25435185 0.2398726 0.22157471 0.19976173 0.17574993 0.14839052 0.11883095 0.087229662 0.057946861][0.26420885 0.2769323 0.28367284 0.28579131 0.28607884 0.28525349 0.28299335 0.27710557 0.26583287 0.24975991 0.22948588 0.20306204 0.17122981 0.13348643 0.096363991][0.25077391 0.26381078 0.27256623 0.27881926 0.28510624 0.29227662 0.29982787 0.30357337 0.30120111 0.29206762 0.27656847 0.2531679 0.22193491 0.18212853 0.13981901][0.24760684 0.25580236 0.2612088 0.26617596 0.2740885 0.28633362 0.30196509 0.31461743 0.32160997 0.32026851 0.3108913 0.29177228 0.26290146 0.22447208 0.18140948][0.24813846 0.24928236 0.24824959 0.24697334 0.25094306 0.26293769 0.28247148 0.30250281 0.31762213 0.3247419 0.32337606 0.31129035 0.2886906 0.25522748 0.21574111][0.2421221 0.23705573 0.22879915 0.22127777 0.22038537 0.22965755 0.24991131 0.27378294 0.29501855 0.30929536 0.31525213 0.31029829 0.29505974 0.26933017 0.23688988][0.22296892 0.21444663 0.20230187 0.19112185 0.18727113 0.19503774 0.2155742 0.24098504 0.26515052 0.28336346 0.2936807 0.29419535 0.285094 0.2666927 0.241546][0.19265856 0.18234982 0.16802889 0.15577058 0.15194748 0.16035125 0.18156379 0.20813821 0.23386112 0.25395426 0.26578346 0.26855573 0.26245323 0.24903682 0.22954175][0.15092854 0.14185309 0.12859337 0.11710604 0.11506332 0.1256828 0.14839937 0.17629063 0.20325969 0.22505571 0.23843117 0.24215257 0.23691456 0.22505064 0.20819484][0.106575 0.099514358 0.089068137 0.081515871 0.082740247 0.095705278 0.11937521 0.14838012 0.17620775 0.19907655 0.21368942 0.21839586 0.21417522 0.20223688 0.18463425][0.065640993 0.062071577 0.055934172 0.0533514 0.059336849 0.075648956 0.099936336 0.12803128 0.15547197 0.17844273 0.19346976 0.19841528 0.19413112 0.18174005 0.16220829][0.033938177 0.033980366 0.033245623 0.036344886 0.047173861 0.066679835 0.091502815 0.11761744 0.14197145 0.16168264 0.17394325 0.17764989 0.17229781 0.15857005 0.13773263][0.01329066 0.016237624 0.019594993 0.026941059 0.041422587 0.062872544 0.087859526 0.11245381 0.13371065 0.14944643 0.1578318 0.15776663 0.14886156 0.13251857 0.11062326][0.0023348585 0.0055795182 0.010370876 0.019157398 0.034141455 0.055167627 0.078702986 0.10086279 0.11944376 0.13256447 0.13859026 0.13639483 0.12602721 0.10905223 0.087667495][-0.00085313851 0.00072200433 0.0035933156 0.010843789 0.023920897 0.041949205 0.06178394 0.080631971 0.096478745 0.10712302 0.11133711 0.10822976 0.098125227 0.082569681 0.063879654]]...]
INFO - root - 2017-12-09 22:38:45.727072: step 66210, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:48m:08s remains)
INFO - root - 2017-12-09 22:38:54.434918: step 66220, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 65h:36m:26s remains)
INFO - root - 2017-12-09 22:39:03.084317: step 66230, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:16m:52s remains)
INFO - root - 2017-12-09 22:39:11.940921: step 66240, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 66h:01m:43s remains)
INFO - root - 2017-12-09 22:39:20.645541: step 66250, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 62h:24m:30s remains)
INFO - root - 2017-12-09 22:39:28.874445: step 66260, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 62h:46m:18s remains)
INFO - root - 2017-12-09 22:39:37.309160: step 66270, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 62h:05m:14s remains)
INFO - root - 2017-12-09 22:39:45.843411: step 66280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 63h:36m:22s remains)
INFO - root - 2017-12-09 22:39:54.260263: step 66290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:57m:39s remains)
INFO - root - 2017-12-09 22:40:02.826414: step 66300, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 63h:29m:35s remains)
2017-12-09 22:40:03.716323: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019396054 0.021645362 0.023554457 0.025008956 0.025708476 0.025390502 0.024209686 0.022653406 0.021273129 0.02037156 0.019949613 0.019867357 0.019741751 0.01930058 0.018264838][0.021765072 0.02487262 0.027612252 0.029824952 0.031217178 0.031423405 0.030448174 0.028763624 0.026863707 0.025186185 0.023815751 0.022859529 0.022131963 0.021418428 0.020343954][0.021840271 0.02585775 0.029755501 0.033238281 0.035797015 0.036860976 0.036285374 0.034488373 0.031974997 0.02921051 0.026527427 0.024273146 0.022484655 0.021042779 0.019593794][0.019138273 0.023708887 0.028611576 0.033461552 0.037473135 0.03979975 0.039983563 0.03824959 0.035078041 0.03109573 0.026980292 0.023361649 0.020550977 0.018424343 0.016700853][0.013978146 0.018448038 0.023788629 0.029658986 0.035078138 0.038856648 0.040102541 0.038672414 0.03497573 0.029958205 0.024677308 0.020018039 0.016410051 0.013811754 0.011942256][0.0085746543 0.012644824 0.018009929 0.024375707 0.030718854 0.035585217 0.037716672 0.03658659 0.0325684 0.026826987 0.020754641 0.015417892 0.011393947 0.0086528827 0.0068555577][0.004400732 0.0079789767 0.013112945 0.019624377 0.026513541 0.032119937 0.034844175 0.03386027 0.029516479 0.023148643 0.016490711 0.010816581 0.0067882473 0.0042852117 0.0027822743][0.0019394008 0.0048207603 0.0093656331 0.015646374 0.022700191 0.028836774 0.0321832 0.031616047 0.02734128 0.020714547 0.013738308 0.0078844642 0.0039454438 0.0016218235 0.00028101774][0.00071532954 0.0030686327 0.0068538962 0.012384243 0.019071156 0.02540762 0.029423205 0.02975888 0.026297966 0.020210061 0.013486451 0.0077661062 0.0039616749 0.0017073674 0.00046357326][8.9350156e-05 0.0020561086 0.005238669 0.010096604 0.016386531 0.022851881 0.027567703 0.028938787 0.026540084 0.021260409 0.014940253 0.0093328394 0.0054248739 0.00320966 0.0020343217][-6.5575819e-05 0.0016062397 0.0043734536 0.0087478217 0.014781019 0.02154229 0.027163133 0.029828588 0.028771052 0.024490442 0.01860573 0.012935914 0.0085724462 0.0059213527 0.00467002][-0.00025388435 0.0011021176 0.0034028196 0.0073322151 0.013180299 0.020346262 0.0270315 0.031223921 0.031779017 0.028791646 0.02367935 0.018151075 0.013507016 0.010500792 0.0090833427][-0.00049145054 0.00058376114 0.0023042213 0.0055194376 0.010863935 0.018019887 0.025449058 0.031100528 0.03346815 0.032362893 0.028605131 0.023779998 0.019268727 0.016039064 0.014311563][-0.00077125989 0.00012168684 0.0014392615 0.0038831576 0.0082919551 0.014816804 0.022347413 0.028964015 0.032950453 0.033714455 0.031685095 0.028094823 0.024141863 0.020886423 0.018859865][-0.0012441375 -0.00052037975 0.00051692151 0.0023016471 0.0056510968 0.011040416 0.017957631 0.024878114 0.030033225 0.032364648 0.031913053 0.029613515 0.026480349 0.02354054 0.021492139]]...]
INFO - root - 2017-12-09 22:40:12.287497: step 66310, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 62h:10m:17s remains)
INFO - root - 2017-12-09 22:40:20.920549: step 66320, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:49m:07s remains)
INFO - root - 2017-12-09 22:40:29.644363: step 66330, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 61h:40m:50s remains)
INFO - root - 2017-12-09 22:40:38.299393: step 66340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 65h:08m:14s remains)
INFO - root - 2017-12-09 22:40:46.952418: step 66350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:16m:47s remains)
INFO - root - 2017-12-09 22:40:55.428809: step 66360, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 54h:59m:06s remains)
INFO - root - 2017-12-09 22:41:03.730735: step 66370, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 61h:16m:02s remains)
INFO - root - 2017-12-09 22:41:12.242510: step 66380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:43m:12s remains)
INFO - root - 2017-12-09 22:41:20.826253: step 66390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:11m:58s remains)
INFO - root - 2017-12-09 22:41:29.684799: step 66400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:07m:16s remains)
2017-12-09 22:41:30.602443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033978524 -0.0033955104 -0.0033846772 -0.0033473442 -0.0032925659 -0.0032331978 -0.0031001444 -0.0029322356 -0.0028679492 -0.0029878719 -0.0031885004 -0.0033236728 -0.0033803345 -0.0033942361 -0.0033959013][-0.0033932056 -0.0033769414 -0.0033276123 -0.003221943 -0.0030973961 -0.0029903208 -0.0028040393 -0.0026031481 -0.0025655851 -0.0027879486 -0.0030949186 -0.0032927904 -0.0033731188 -0.0033924533 -0.0033946221][-0.0033563231 -0.0032281757 -0.0029776322 -0.0026471904 -0.002389058 -0.0022470849 -0.0020542294 -0.00187286 -0.0019316355 -0.0023493543 -0.0028619049 -0.003205813 -0.0033517084 -0.0033905343 -0.0033958605][-0.0032034381 -0.0027368604 -0.0019407832 -0.0010639136 -0.00052713742 -0.00036580791 -0.00025434932 -0.00017876364 -0.00046159327 -0.0012671573 -0.0022251327 -0.0029361441 -0.0032712345 -0.0033770856 -0.0033963029][-0.0028089634 -0.0017101131 5.0299568e-05 0.0018656447 0.0029033513 0.0031242573 0.0030822454 0.0029648675 0.002299512 0.00081985048 -0.00094778603 -0.0023620534 -0.0030841073 -0.0033382175 -0.003393407][-0.0021163642 -0.00024203258 0.0026989549 0.0056825615 0.0074274335 0.00782169 0.0076185353 0.0072374437 0.00605115 0.0036840707 0.00083372439 -0.0015360919 -0.002803294 -0.0032724515 -0.0033836835][-0.0013244916 0.0011862956 0.0051244115 0.0091348188 0.011627183 0.01234349 0.012099105 0.011498689 0.0097944178 0.0065429127 0.0026061123 -0.00070940237 -0.0025191822 -0.0032022912 -0.0033707749][-0.00078893709 0.0019457212 0.0062878197 0.010770995 0.013756273 0.01483373 0.014724169 0.014088079 0.012097281 0.0082974685 0.0036577659 -0.00023744116 -0.0023627151 -0.0031638739 -0.0033622354][-0.00081088697 0.0016476123 0.0056222575 0.0098271715 0.012838224 0.014157986 0.014295148 0.013831345 0.011914648 0.0081476606 0.0035066351 -0.00033908314 -0.0024097939 -0.0031777576 -0.0033641374][-0.0013721616 0.00045383582 0.0034812435 0.00678165 0.009317195 0.010617066 0.010964248 0.010770558 0.0092449607 0.0061005019 0.0021941194 -0.00097280857 -0.00263667 -0.0032358868 -0.0033752397][-0.0021578874 -0.0010451176 0.00085198064 0.0029950978 0.0047536194 0.0057740984 0.0061811097 0.0062037939 0.0052006077 0.003013816 0.00028853351 -0.0018520765 -0.0029345048 -0.003306556 -0.0033874533][-0.0028272572 -0.0022818944 -0.0013294704 -0.00020668842 0.00077754934 0.001414289 0.0017408852 0.0018517196 0.0013096796 6.4409571e-05 -0.0014744657 -0.0026308526 -0.0031839623 -0.0033616051 -0.0033963283][-0.0032145223 -0.0030145869 -0.0026539438 -0.0022017332 -0.001774866 -0.0014708338 -0.0012853418 -0.0011913767 -0.0014253615 -0.0019785971 -0.0026464164 -0.0031185495 -0.0033290244 -0.0033906824 -0.0034008778][-0.0033663046 -0.0033184444 -0.0032262758 -0.0030975682 -0.0029632708 -0.0028582842 -0.0027853162 -0.0027401932 -0.0028150396 -0.0029911746 -0.0031958597 -0.0033301448 -0.00338552 -0.0033995355 -0.0034007393][-0.0033979346 -0.0033928524 -0.0033805745 -0.0033611024 -0.0033375809 -0.0033155556 -0.0032967629 -0.0032819069 -0.0032960877 -0.003330098 -0.0033675069 -0.003389417 -0.0033974913 -0.003399014 -0.0033985809]]...]
INFO - root - 2017-12-09 22:41:39.205601: step 66410, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 64h:49m:33s remains)
INFO - root - 2017-12-09 22:41:48.021755: step 66420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:30m:29s remains)
INFO - root - 2017-12-09 22:41:56.733093: step 66430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:10m:48s remains)
INFO - root - 2017-12-09 22:42:05.529117: step 66440, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 62h:22m:34s remains)
INFO - root - 2017-12-09 22:42:14.180185: step 66450, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 64h:12m:21s remains)
INFO - root - 2017-12-09 22:42:23.074214: step 66460, loss = 0.88, batch loss = 0.67 (8.5 examples/sec; 0.942 sec/batch; 69h:37m:39s remains)
INFO - root - 2017-12-09 22:42:31.334407: step 66470, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 61h:39m:05s remains)
INFO - root - 2017-12-09 22:42:39.895522: step 66480, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 63h:26m:56s remains)
INFO - root - 2017-12-09 22:42:48.469208: step 66490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:28m:27s remains)
INFO - root - 2017-12-09 22:42:57.033464: step 66500, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 64h:30m:21s remains)
2017-12-09 22:42:57.971656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033437677 -0.0033401793 -0.0033398047 -0.0033394024 -0.0033387386 -0.0033377667 -0.0033368205 -0.0033354773 -0.0033338475 -0.00333207 -0.0033307958 -0.0033302591 -0.0033305918 -0.0033316393 -0.0033331006][-0.0033419633 -0.0033381227 -0.0033378876 -0.0033374066 -0.0033366715 -0.0033355632 -0.0033344035 -0.0033330079 -0.0033315334 -0.0033300647 -0.0033293271 -0.0033289483 -0.0033290796 -0.0033299122 -0.0033310298][-0.0033416811 -0.0033378128 -0.0033377111 -0.0033374657 -0.0033370261 -0.0033359588 -0.0033348857 -0.0033335588 -0.0033322761 -0.0033310435 -0.0033305159 -0.0033302533 -0.0033303157 -0.0033308363 -0.00333133][-0.0033414701 -0.0033375542 -0.003337496 -0.0033373011 -0.00333692 -0.0033360943 -0.0033351467 -0.003333977 -0.0033328854 -0.0033318019 -0.0033313921 -0.003331041 -0.0033310235 -0.0033311869 -0.0033312773][-0.0033411949 -0.003337367 -0.0033372922 -0.0033369693 -0.0033364876 -0.0033355851 -0.0033345628 -0.0033335409 -0.003332569 -0.0033315036 -0.0033309169 -0.0033304468 -0.0033304128 -0.0033303669 -0.0033304533][-0.0033405554 -0.0033367272 -0.0033367921 -0.0033365628 -0.0033360636 -0.0033349516 -0.0033338908 -0.0033329725 -0.0033320731 -0.0033312228 -0.0033308112 -0.0033304412 -0.0033301983 -0.0033300475 -0.0033297986][-0.0033406995 -0.0033369714 -0.0033370531 -0.0033368166 -0.0033361919 -0.0033350436 -0.0033337972 -0.0033325504 -0.0033315225 -0.0033307408 -0.0033304882 -0.0033301585 -0.0033298072 -0.0033295422 -0.0033291609][-0.003340729 -0.0033369819 -0.00333692 -0.003336594 -0.0033361339 -0.0033351746 -0.0033339385 -0.003332502 -0.0033313786 -0.00333066 -0.0033302724 -0.0033299145 -0.0033296163 -0.0033292305 -0.0033288326][-0.0033401221 -0.0033364079 -0.0033363709 -0.0033359795 -0.0033356112 -0.0033351583 -0.0033344002 -0.0033333055 -0.003332363 -0.0033316934 -0.0033311914 -0.0033307513 -0.0033302687 -0.0033298577 -0.0033294267][-0.0033400676 -0.0033365025 -0.0033367958 -0.0033366228 -0.0033367057 -0.0033368208 -0.0033368208 -0.0033365327 -0.0033361013 -0.0033357749 -0.0033352585 -0.0033347353 -0.0033340917 -0.0033334668 -0.0033326889][-0.0033413123 -0.0033379139 -0.003338889 -0.0033395779 -0.0033406606 -0.0033418594 -0.003342855 -0.0033433444 -0.0033434087 -0.0033431652 -0.0033425244 -0.0033417395 -0.0033407742 -0.0033401235 -0.0033392119][-0.003344639 -0.0033423421 -0.0033439654 -0.0033454977 -0.0033471431 -0.003348703 -0.0033499151 -0.0033505841 -0.0033509699 -0.0033508341 -0.0033501596 -0.0033491552 -0.0033482644 -0.0033477438 -0.0033468448][-0.0033503652 -0.0033484991 -0.0033499806 -0.0033513617 -0.0033524707 -0.0033533284 -0.0033539918 -0.0033540446 -0.0033538351 -0.0033533033 -0.0033525024 -0.0033513978 -0.003350467 -0.0033502849 -0.0033499352][-0.0033559883 -0.0033536968 -0.0033541161 -0.0033545461 -0.0033546183 -0.0033544716 -0.0033541226 -0.0033534281 -0.0033527962 -0.0033518963 -0.0033508956 -0.0033496465 -0.0033486183 -0.0033485913 -0.0033487128][-0.0033600598 -0.0033568023 -0.0033556095 -0.0033548563 -0.0033538861 -0.003352813 -0.0033517475 -0.0033505377 -0.0033494565 -0.0033482078 -0.0033469987 -0.0033456313 -0.003344683 -0.0033449957 -0.0033457617]]...]
INFO - root - 2017-12-09 22:43:06.703517: step 66510, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:36m:46s remains)
INFO - root - 2017-12-09 22:43:15.390993: step 66520, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 63h:44m:53s remains)
INFO - root - 2017-12-09 22:43:23.835467: step 66530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 62h:02m:08s remains)
INFO - root - 2017-12-09 22:43:32.346768: step 66540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:30m:41s remains)
INFO - root - 2017-12-09 22:43:41.145069: step 66550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:20m:45s remains)
INFO - root - 2017-12-09 22:43:49.873270: step 66560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 63h:21m:08s remains)
INFO - root - 2017-12-09 22:43:58.277665: step 66570, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:22m:24s remains)
INFO - root - 2017-12-09 22:44:06.951075: step 66580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:49m:18s remains)
INFO - root - 2017-12-09 22:44:15.530938: step 66590, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 64h:55m:23s remains)
INFO - root - 2017-12-09 22:44:24.563491: step 66600, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.993 sec/batch; 73h:22m:51s remains)
2017-12-09 22:44:25.463918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033633059 -0.0033608139 -0.0033609017 -0.0033608433 -0.0033605224 -0.0033605611 -0.0033607616 -0.003360816 -0.0033604263 -0.0033597837 -0.003359131 -0.0033585629 -0.0033577876 -0.0033568004 -0.0033556784][-0.0033612296 -0.0033584922 -0.0033585951 -0.0033583951 -0.0033578314 -0.0033574514 -0.0033571555 -0.0033568291 -0.0033561666 -0.0033554924 -0.0033549631 -0.003354545 -0.0033540558 -0.0033533494 -0.0033522947][-0.0033614999 -0.00335861 -0.0033587085 -0.003358436 -0.003357697 -0.0033568966 -0.0033561222 -0.003355355 -0.0033543603 -0.0033536796 -0.0033532195 -0.0033529722 -0.0033528106 -0.0033524209 -0.0033515592][-0.0033614889 -0.0033583827 -0.0033584458 -0.0033580915 -0.0033573501 -0.0033565133 -0.0033555934 -0.0033546223 -0.0033535061 -0.0033527205 -0.003352236 -0.0033518947 -0.0033517142 -0.0033514788 -0.0033508583][-0.0033613283 -0.0033579937 -0.0033580828 -0.0033578207 -0.0033573338 -0.0033566547 -0.0033559178 -0.0033549257 -0.0033537517 -0.003352748 -0.0033520344 -0.0033514162 -0.0033509275 -0.0033505973 -0.0033501005][-0.0033611713 -0.003357637 -0.0033577457 -0.0033577504 -0.0033575853 -0.0033571308 -0.0033566437 -0.0033557694 -0.0033545708 -0.0033533315 -0.0033522041 -0.0033511778 -0.0033502532 -0.0033496276 -0.0033491689][-0.0033608 -0.0033572228 -0.0033574353 -0.0033577005 -0.0033578372 -0.0033576253 -0.0033573846 -0.0033566875 -0.003355474 -0.0033541142 -0.0033526884 -0.0033513692 -0.0033501703 -0.003349283 -0.0033488886][-0.003360382 -0.003356918 -0.0033571937 -0.0033575813 -0.0033579271 -0.0033578051 -0.0033576437 -0.0033570214 -0.0033558151 -0.0033544833 -0.0033531277 -0.0033517517 -0.0033505529 -0.0033495242 -0.0033490108][-0.0033600745 -0.0033566856 -0.003357135 -0.0033574933 -0.0033578412 -0.0033575806 -0.0033572612 -0.0033566109 -0.0033554055 -0.0033542553 -0.0033532463 -0.0033521955 -0.0033512651 -0.0033503131 -0.0033498327][-0.0033598363 -0.0033564486 -0.0033571031 -0.0033574596 -0.0033576118 -0.0033571059 -0.0033565932 -0.0033559119 -0.0033547834 -0.0033538211 -0.0033530302 -0.0033523517 -0.0033518381 -0.0033510777 -0.00335077][-0.0033598095 -0.0033563315 -0.0033571552 -0.0033574612 -0.0033573785 -0.0033567082 -0.0033561308 -0.0033555434 -0.003354595 -0.0033536823 -0.0033529943 -0.0033524837 -0.0033521727 -0.0033516746 -0.0033514902][-0.0033600435 -0.0033562989 -0.0033571627 -0.0033573988 -0.0033572102 -0.0033565515 -0.0033560195 -0.0033555659 -0.0033548055 -0.0033539073 -0.0033532497 -0.003352741 -0.0033523883 -0.0033521007 -0.0033520751][-0.0033602363 -0.003356216 -0.0033570926 -0.0033573434 -0.0033571895 -0.0033566537 -0.00335621 -0.0033557964 -0.003355182 -0.0033542346 -0.0033535154 -0.0033529229 -0.0033524034 -0.0033521499 -0.0033522546][-0.0033603688 -0.0033561441 -0.0033570249 -0.003357383 -0.0033573741 -0.0033570537 -0.0033566947 -0.0033562891 -0.0033556046 -0.0033545597 -0.0033536614 -0.003352928 -0.0033522584 -0.0033519843 -0.0033521738][-0.003360644 -0.003356274 -0.0033569972 -0.0033574728 -0.0033575748 -0.0033574032 -0.0033570309 -0.0033565576 -0.0033558062 -0.0033547189 -0.0033537089 -0.0033529026 -0.003352209 -0.0033518791 -0.0033520395]]...]
INFO - root - 2017-12-09 22:44:34.258145: step 66610, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 63h:28m:17s remains)
INFO - root - 2017-12-09 22:44:42.952327: step 66620, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:48m:33s remains)
INFO - root - 2017-12-09 22:44:51.662060: step 66630, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 65h:38m:56s remains)
INFO - root - 2017-12-09 22:45:00.373999: step 66640, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:31m:15s remains)
INFO - root - 2017-12-09 22:45:09.076223: step 66650, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 66h:37m:18s remains)
INFO - root - 2017-12-09 22:45:17.829683: step 66660, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 66h:46m:29s remains)
INFO - root - 2017-12-09 22:45:26.297613: step 66670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:39m:15s remains)
INFO - root - 2017-12-09 22:45:34.956050: step 66680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 64h:32m:01s remains)
INFO - root - 2017-12-09 22:45:43.549851: step 66690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:39m:32s remains)
INFO - root - 2017-12-09 22:45:52.367766: step 66700, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 66h:32m:58s remains)
2017-12-09 22:45:53.211093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033976471 -0.003391206 -0.0033864696 -0.0033830765 -0.0033800106 -0.0033777778 -0.0033736865 -0.0033710115 -0.0033707942 -0.0033698771 -0.0033682405 -0.0033647469 -0.0033628396 -0.0033625201 -0.0033632775][-0.0034011013 -0.0033961325 -0.0033918361 -0.0033878747 -0.0033820891 -0.0033752595 -0.0033667528 -0.0033607867 -0.0033575029 -0.0033548595 -0.0033526935 -0.0033515871 -0.0033539 -0.00336009 -0.0033665115][-0.0034013796 -0.0033963406 -0.0033909264 -0.0033840265 -0.0033730869 -0.0033604514 -0.0033458781 -0.0033368326 -0.0033324838 -0.0033318524 -0.0033322605 -0.0033340792 -0.0033414229 -0.0033549657 -0.003368262][-0.003400252 -0.0033953248 -0.00338828 -0.0033774409 -0.0033600058 -0.0033406671 -0.0033217776 -0.00331026 -0.0033058911 -0.0033065539 -0.0033098895 -0.0033160571 -0.0033291446 -0.00334948 -0.0033688163][-0.0034003276 -0.003395272 -0.003387036 -0.0033724376 -0.0033492218 -0.0033225922 -0.0032997921 -0.0032877689 -0.0032839351 -0.0032866059 -0.0032928216 -0.0033016116 -0.0033189233 -0.003344307 -0.0033681537][-0.0034000701 -0.003395708 -0.0033874365 -0.0033719689 -0.0033467051 -0.0033169133 -0.0032913296 -0.0032773181 -0.0032726761 -0.0032753826 -0.0032827593 -0.0032934765 -0.0033125984 -0.0033394187 -0.0033643902][-0.0033991886 -0.0033961495 -0.0033890195 -0.0033756369 -0.0033526884 -0.003324532 -0.0032996228 -0.0032844986 -0.0032781877 -0.0032791377 -0.0032849819 -0.0032954137 -0.0033143137 -0.0033405866 -0.0033641055][-0.003398753 -0.0033966696 -0.0033917134 -0.0033820032 -0.0033646419 -0.0033423882 -0.0033209394 -0.0033051053 -0.0032974249 -0.0032960882 -0.0032997304 -0.0033079847 -0.0033239243 -0.0033466488 -0.0033662957][-0.0033978012 -0.0033968519 -0.0033947818 -0.0033886523 -0.0033770241 -0.003361692 -0.0033462984 -0.0033331353 -0.0033252812 -0.0033233943 -0.0033246069 -0.0033289152 -0.0033417223 -0.0033598873 -0.003373059][-0.0033955867 -0.003395525 -0.0033952869 -0.0033932989 -0.0033878253 -0.0033792572 -0.0033692962 -0.0033592104 -0.0033534539 -0.0033513759 -0.0033502295 -0.0033521166 -0.0033598265 -0.0033708904 -0.0033773843][-0.0033929124 -0.0033926335 -0.0033934177 -0.0033938549 -0.0033929909 -0.0033899173 -0.0033849981 -0.003379514 -0.003375523 -0.003373893 -0.0033708264 -0.0033694892 -0.0033739216 -0.003379388 -0.0033818339][-0.0033901713 -0.0033891362 -0.0033896156 -0.0033910454 -0.0033928403 -0.0033932615 -0.0033920829 -0.0033887448 -0.0033859487 -0.0033827533 -0.0033761361 -0.003371862 -0.0033727766 -0.00337562 -0.0033787638][-0.0033883024 -0.0033862346 -0.0033859303 -0.0033873669 -0.0033894549 -0.0033908114 -0.0033910966 -0.0033895888 -0.0033859757 -0.003379632 -0.0033693756 -0.0033609038 -0.0033589494 -0.0033606256 -0.0033665989][-0.0033869727 -0.0033844032 -0.0033835883 -0.0033850707 -0.003386446 -0.003387637 -0.003386752 -0.0033837419 -0.0033782909 -0.0033675374 -0.003351297 -0.00333738 -0.0033308924 -0.0033332373 -0.0033428753][-0.0033871951 -0.0033846623 -0.0033830812 -0.0033841559 -0.0033852286 -0.0033858647 -0.0033840467 -0.0033801084 -0.003371737 -0.003354314 -0.0033305075 -0.0033093959 -0.0032978561 -0.0032984267 -0.0033091132]]...]
INFO - root - 2017-12-09 22:46:01.810963: step 66710, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:35m:21s remains)
INFO - root - 2017-12-09 22:46:10.224135: step 66720, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:08m:55s remains)
INFO - root - 2017-12-09 22:46:18.710321: step 66730, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 61h:26m:04s remains)
INFO - root - 2017-12-09 22:46:27.359059: step 66740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:36m:50s remains)
INFO - root - 2017-12-09 22:46:36.082360: step 66750, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:20m:54s remains)
INFO - root - 2017-12-09 22:46:44.737287: step 66760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 63h:18m:34s remains)
INFO - root - 2017-12-09 22:46:53.074882: step 66770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 62h:26m:55s remains)
INFO - root - 2017-12-09 22:47:01.621418: step 66780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 64h:11m:04s remains)
INFO - root - 2017-12-09 22:47:10.126840: step 66790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:20m:55s remains)
INFO - root - 2017-12-09 22:47:18.582019: step 66800, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 61h:28m:06s remains)
2017-12-09 22:47:19.435771: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082758985 0.086512849 0.085693784 0.079962507 0.073152483 0.066279516 0.061558269 0.0586965 0.05717399 0.056257546 0.0540362 0.048941474 0.040988304 0.030757261 0.020461261][0.119476 0.1291458 0.13256176 0.12876327 0.12250951 0.11530295 0.11039177 0.10713628 0.10451758 0.10201965 0.096980207 0.087396562 0.072969757 0.055360951 0.037742276][0.16188449 0.18078618 0.19176833 0.19385685 0.19178954 0.18656851 0.18219951 0.1782738 0.17400163 0.16885415 0.15914476 0.14278425 0.11929936 0.09144339 0.063495129][0.20723169 0.23811007 0.26066774 0.27323577 0.27934662 0.27962613 0.2777 0.2727358 0.26471481 0.25373778 0.23621202 0.21031252 0.17537574 0.13497111 0.094582416][0.25573179 0.29942852 0.33523038 0.36036077 0.37666804 0.38358465 0.38437524 0.37835926 0.36603132 0.34811032 0.32156524 0.28531164 0.2382492 0.18440282 0.1303933][0.30055296 0.35636547 0.40417683 0.44051176 0.46582568 0.47834614 0.48091885 0.47313288 0.45610261 0.4315646 0.39708278 0.35215142 0.29508814 0.2301264 0.164526][0.32978913 0.3953155 0.45284769 0.49814779 0.53020245 0.54659575 0.5503155 0.54115242 0.52056515 0.49092805 0.45066246 0.39989763 0.33628708 0.26375335 0.18993182][0.33773619 0.40878353 0.47173443 0.52205932 0.55824739 0.57738215 0.58191496 0.57224596 0.55034834 0.51870084 0.47604555 0.42278722 0.35660541 0.2809599 0.20326526][0.32638583 0.39854062 0.46318719 0.51507843 0.55205816 0.57194024 0.57684404 0.56745219 0.54566395 0.51412237 0.4720735 0.41942242 0.35428312 0.27955979 0.20254526][0.29833221 0.36840418 0.43160078 0.4826254 0.51894188 0.53854489 0.54312927 0.53417951 0.51340353 0.48332241 0.44318593 0.392716 0.33080798 0.2600759 0.18767427][0.25457415 0.31941807 0.37858412 0.42666614 0.46072784 0.47964406 0.48448506 0.47641596 0.45728645 0.42924696 0.39192256 0.3448959 0.28799421 0.22422072 0.16004123][0.20037732 0.25629342 0.30848485 0.35162354 0.38238129 0.39982921 0.40468043 0.39807904 0.38172191 0.356987 0.32399529 0.28237629 0.2327968 0.17851394 0.12512095][0.14554143 0.18971242 0.23196138 0.26763895 0.2933026 0.30825743 0.31268346 0.3074173 0.29406133 0.27356774 0.24637826 0.2121581 0.17211485 0.1294338 0.088605896][0.09681 0.12887459 0.16006182 0.18663694 0.20568873 0.21682706 0.22014637 0.21618953 0.20608564 0.19034187 0.16969597 0.1440247 0.1144671 0.083815783 0.055459052][0.056517284 0.0774308 0.098028168 0.11576807 0.12850682 0.1357673 0.13779119 0.13500521 0.1281012 0.1172593 0.10315101 0.085928254 0.066502377 0.046996932 0.029534003]]...]
INFO - root - 2017-12-09 22:47:27.946854: step 66810, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:40m:54s remains)
INFO - root - 2017-12-09 22:47:36.987768: step 66820, loss = 0.90, batch loss = 0.69 (8.2 examples/sec; 0.972 sec/batch; 71h:44m:11s remains)
INFO - root - 2017-12-09 22:47:45.713717: step 66830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:08m:52s remains)
INFO - root - 2017-12-09 22:47:54.451575: step 66840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 65h:12m:11s remains)
INFO - root - 2017-12-09 22:48:03.163116: step 66850, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:14m:45s remains)
INFO - root - 2017-12-09 22:48:11.963529: step 66860, loss = 0.88, batch loss = 0.67 (9.5 examples/sec; 0.840 sec/batch; 61h:56m:56s remains)
INFO - root - 2017-12-09 22:48:20.424880: step 66870, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 56h:42m:59s remains)
INFO - root - 2017-12-09 22:48:29.060792: step 66880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:33m:02s remains)
INFO - root - 2017-12-09 22:48:37.602084: step 66890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:11m:33s remains)
INFO - root - 2017-12-09 22:48:46.373540: step 66900, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 62h:08m:25s remains)
2017-12-09 22:48:47.305508: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.021776773 0.021191997 0.018642446 0.016205914 0.011878683 0.0066903774 0.0019409356 -0.00092512951 -0.0023484267 -0.0030035237 -0.0033180579 -0.0033865178 -0.0033905411 -0.0033910589 -0.0033929923][0.03152027 0.030581256 0.027545644 0.024841527 0.019422879 0.01237531 0.0052026249 0.00074742525 -0.0017181312 -0.0027641938 -0.0032645385 -0.0033818709 -0.0033865296 -0.0033866451 -0.0033896272][0.0430781 0.041272581 0.037216853 0.03390627 0.027444791 0.018944619 0.0096123172 0.0034105976 -0.00042208354 -0.00220108 -0.0031504496 -0.0033755186 -0.0033833832 -0.0033838951 -0.0033854879][0.054223865 0.052320678 0.047732763 0.04380367 0.036100198 0.025929449 0.014498124 0.0066240225 0.0013343857 -0.0014079558 -0.0029793661 -0.0033689744 -0.0033826579 -0.0033827084 -0.0033841454][0.063175172 0.061754346 0.057085749 0.052873846 0.044370521 0.032981396 0.019909156 0.010353772 0.0034189096 -0.00049862638 -0.0028013829 -0.003362006 -0.0033837098 -0.0033843028 -0.0033840416][0.069349937 0.069165073 0.064611711 0.060156442 0.051048849 0.038938388 0.02500313 0.014047578 0.0055526169 0.00035602809 -0.0026522232 -0.0033585422 -0.0033837727 -0.0033854821 -0.0033871043][0.073092572 0.074319504 0.070372395 0.065744065 0.056009032 0.043385189 0.028960483 0.016924264 0.0071918955 0.00090884813 -0.0025885866 -0.0033611639 -0.0033861692 -0.0033876188 -0.0033884952][0.073536433 0.076382361 0.073362634 0.068677314 0.058638185 0.045793291 0.031085486 0.018408937 0.0079399766 0.00098790391 -0.0026331022 -0.0033684052 -0.0033879317 -0.0033885629 -0.0033889182][0.069973476 0.07467968 0.072693363 0.068219647 0.058368437 0.04574094 0.031132462 0.018276094 0.0075944662 0.00057897414 -0.0027543867 -0.0033791235 -0.0033904407 -0.0033894174 -0.0033899061][0.063244648 0.069183439 0.067687988 0.063642971 0.05454725 0.042719144 0.02876625 0.016414616 0.0062926374 -0.00011837739 -0.0029146897 -0.0033849066 -0.0033892773 -0.003386975 -0.0033889965][0.053976782 0.061030634 0.0596805 0.055921432 0.047739342 0.037052106 0.024282107 0.01319528 0.0043791933 -0.0009110258 -0.0030447682 -0.0033817079 -0.0033841426 -0.0033804085 -0.0033855895][0.044313248 0.051090516 0.049640693 0.046348974 0.039287794 0.029889425 0.018723011 0.0094177043 0.0023152384 -0.0016419265 -0.0031460826 -0.0033758075 -0.0033753479 -0.0033712953 -0.0033802597][0.034312498 0.040490337 0.038995784 0.036129955 0.030418454 0.022573942 0.013310345 0.0058503486 0.00045119878 -0.0022606866 -0.0032315385 -0.0033712557 -0.0033685281 -0.0033651765 -0.0033768753][0.024879249 0.02963057 0.028445968 0.026332581 0.021969441 0.015780982 0.0086586615 0.0029813542 -0.0009225423 -0.0026962163 -0.0032926085 -0.0033731153 -0.0033681931 -0.0033656505 -0.0033771424][0.016359758 0.019350564 0.018364184 0.016944278 0.01394389 0.0095161451 0.0045159394 0.00054377993 -0.0019881709 -0.0030191706 -0.0033415235 -0.003377764 -0.0033740595 -0.0033727959 -0.0033823808]]...]
INFO - root - 2017-12-09 22:48:55.937843: step 66910, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:59m:27s remains)
INFO - root - 2017-12-09 22:49:04.783315: step 66920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 65h:53m:15s remains)
INFO - root - 2017-12-09 22:49:13.568584: step 66930, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 65h:23m:36s remains)
INFO - root - 2017-12-09 22:49:22.239840: step 66940, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 65h:25m:11s remains)
INFO - root - 2017-12-09 22:49:30.948350: step 66950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:50m:54s remains)
INFO - root - 2017-12-09 22:49:39.444406: step 66960, loss = 0.89, batch loss = 0.68 (10.2 examples/sec; 0.788 sec/batch; 58h:08m:10s remains)
INFO - root - 2017-12-09 22:49:47.787728: step 66970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:47m:29s remains)
INFO - root - 2017-12-09 22:49:56.335684: step 66980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:44m:46s remains)
INFO - root - 2017-12-09 22:50:04.833224: step 66990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:31m:35s remains)
INFO - root - 2017-12-09 22:50:13.435718: step 67000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:43m:38s remains)
2017-12-09 22:50:14.329603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034022953 -0.0034014762 -0.0034012888 -0.0034011931 -0.0034012338 -0.0034014171 -0.0034016008 -0.003401642 -0.0034016208 -0.0034014352 -0.0034012236 -0.0034009982 -0.0034008715 -0.0034007751 -0.0034007437][-0.0034002766 -0.0033993442 -0.0033991868 -0.0033991805 -0.0033991968 -0.0033992592 -0.0033992559 -0.00339919 -0.003399116 -0.0033990657 -0.0033990154 -0.0033989416 -0.0033989439 -0.003399058 -0.0033991672][-0.0034004592 -0.0033996452 -0.0033996408 -0.0033997495 -0.003399719 -0.0033996312 -0.003399441 -0.0033991283 -0.0033989421 -0.003398766 -0.0033985877 -0.003398374 -0.0033980676 -0.0033977719 -0.0033974145][-0.0033943353 -0.0033943492 -0.0033947453 -0.0033947732 -0.003394339 -0.0033935485 -0.0033925495 -0.0033916305 -0.0033914486 -0.003392508 -0.0033941208 -0.0033941565 -0.0033916021 -0.0033872935 -0.0033825189][-0.0033647122 -0.0033657122 -0.0033662715 -0.0033657821 -0.0033644296 -0.0033627194 -0.0033610573 -0.003359729 -0.0033606691 -0.0033660175 -0.0033740378 -0.0033792593 -0.0033783989 -0.0033698683 -0.0033587976][-0.0033161414 -0.00331687 -0.0033173752 -0.0033170132 -0.0033158765 -0.0033142956 -0.0033128038 -0.0033115961 -0.0033133894 -0.0033213994 -0.0033336626 -0.0033449118 -0.0033501058 -0.0033464385 -0.0033401484][-0.0032736785 -0.0032734387 -0.0032739178 -0.0032743388 -0.0032744445 -0.0032742261 -0.0032738126 -0.0032731649 -0.0032744757 -0.0032805104 -0.0032905508 -0.0033022745 -0.0033137703 -0.0033252258 -0.0033381204][-0.0032632262 -0.0032628556 -0.0032639781 -0.0032656007 -0.0032671404 -0.0032681802 -0.0032681958 -0.0032672028 -0.0032662 -0.0032673185 -0.0032713197 -0.0032778545 -0.0032890069 -0.003312635 -0.0033428366][-0.0032852767 -0.0032862606 -0.0032894081 -0.003293457 -0.0032960456 -0.0032959364 -0.0032934407 -0.0032903606 -0.0032874676 -0.003286541 -0.0032881957 -0.0032902667 -0.0032967355 -0.0033179852 -0.0033455868][-0.0033060915 -0.0033076659 -0.0033117943 -0.0033167922 -0.0033192541 -0.0033175773 -0.003313395 -0.0033098683 -0.0033077956 -0.0033080394 -0.0033102399 -0.0033109924 -0.0033133966 -0.0033242474 -0.0033362897][-0.0033050217 -0.0033055576 -0.0033087111 -0.0033122988 -0.0033138818 -0.0033120939 -0.0033086622 -0.0033063004 -0.0033053609 -0.0033056105 -0.0033065928 -0.0033065074 -0.0033068676 -0.00330904 -0.0033088583][-0.0032704473 -0.0032679811 -0.0032675448 -0.0032674379 -0.003266989 -0.0032661268 -0.0032658787 -0.00326702 -0.003268752 -0.003269997 -0.0032709024 -0.0032715977 -0.0032713625 -0.0032689718 -0.0032639415][-0.0031704551 -0.003162323 -0.0031571204 -0.0031527602 -0.0031500922 -0.0031500566 -0.0031531078 -0.003158408 -0.0031643924 -0.0031693161 -0.003172348 -0.0031730717 -0.0031704893 -0.0031651959 -0.0031597377][-0.0029322496 -0.0029149791 -0.00290243 -0.0028916129 -0.0028842241 -0.0028819153 -0.0028860071 -0.0028960251 -0.0029090699 -0.0029207836 -0.0029280756 -0.0029296658 -0.0029254274 -0.0029184264 -0.0029113418][-0.0025363276 -0.00250846 -0.0024876422 -0.0024682768 -0.0024538254 -0.0024476608 -0.0024520904 -0.0024674837 -0.0024887188 -0.0025094922 -0.00252488 -0.0025303927 -0.0025258942 -0.002515744 -0.0025045918]]...]
INFO - root - 2017-12-09 22:50:23.217556: step 67010, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 66h:32m:20s remains)
INFO - root - 2017-12-09 22:50:31.871380: step 67020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:08m:49s remains)
INFO - root - 2017-12-09 22:50:40.471256: step 67030, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 62h:17m:03s remains)
INFO - root - 2017-12-09 22:50:49.018315: step 67040, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 61h:15m:51s remains)
INFO - root - 2017-12-09 22:50:57.530510: step 67050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:42m:52s remains)
INFO - root - 2017-12-09 22:51:06.341918: step 67060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:11m:22s remains)
INFO - root - 2017-12-09 22:51:14.990967: step 67070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:48m:05s remains)
INFO - root - 2017-12-09 22:51:23.706092: step 67080, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:10m:42s remains)
INFO - root - 2017-12-09 22:51:32.405628: step 67090, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:42m:06s remains)
INFO - root - 2017-12-09 22:51:41.073220: step 67100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:31m:24s remains)
2017-12-09 22:51:41.963299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11663217 0.14815955 0.18413018 0.22320834 0.26609954 0.30771208 0.34969935 0.3925164 0.43450484 0.47927833 0.52062726 0.56308407 0.59867847 0.63080281 0.65232176][0.13215916 0.17381427 0.22095543 0.27055123 0.32051277 0.36586997 0.40897018 0.44912976 0.48652953 0.52423352 0.55632013 0.58691496 0.61153167 0.63392067 0.64748406][0.14631334 0.19646795 0.25241652 0.3109225 0.36748487 0.41596815 0.45804116 0.49359265 0.52410263 0.55084592 0.5702607 0.58599627 0.59502119 0.60282147 0.604604][0.15753597 0.21463323 0.27836832 0.34386542 0.40521708 0.45731232 0.49955279 0.5300833 0.55307746 0.5689581 0.57597131 0.576141 0.56930524 0.56108814 0.54891771][0.16380484 0.22595082 0.29550049 0.36579373 0.43153098 0.48603016 0.52768821 0.55475336 0.57047737 0.57516378 0.56943393 0.55481732 0.53318989 0.50926292 0.48312268][0.16507646 0.22882776 0.29965746 0.37268835 0.44040775 0.49549347 0.53642768 0.559208 0.56785661 0.5625031 0.54537857 0.5172143 0.48238105 0.44565094 0.40871504][0.16053309 0.22163275 0.28946564 0.36049286 0.42698535 0.48104456 0.51951396 0.53750139 0.53963858 0.52560091 0.49840766 0.46008095 0.41599968 0.37064543 0.32741159][0.14795218 0.20280196 0.2639766 0.32984838 0.39180139 0.44328651 0.47878733 0.49233332 0.48852479 0.4664903 0.43189874 0.38623092 0.3363761 0.28768304 0.24342863][0.12763457 0.17258213 0.22410703 0.28171575 0.33675158 0.38334036 0.41487291 0.42550531 0.4173938 0.39011088 0.35071763 0.30160764 0.2507138 0.20290907 0.16172771][0.10156931 0.13495009 0.17451525 0.22141115 0.2673887 0.30713874 0.33365387 0.34198195 0.33242518 0.30400178 0.26441351 0.21704815 0.17012224 0.12785651 0.093209937][0.07423076 0.096208878 0.12376128 0.15822057 0.19351977 0.22535454 0.24654755 0.25296766 0.24375968 0.21844329 0.18360934 0.14297676 0.104338 0.070882 0.045222372][0.048036352 0.061274886 0.079079494 0.10245845 0.12774682 0.15134348 0.16732545 0.17224114 0.16468561 0.14474732 0.1172758 0.086188108 0.057689976 0.034305181 0.017667407][0.025133722 0.032836337 0.043953609 0.059271879 0.076595612 0.093033 0.10436346 0.10784798 0.10224921 0.088025615 0.0685094 0.047217175 0.02845872 0.013965815 0.0046099555][0.0087431194 0.012780018 0.018880861 0.027604192 0.037805576 0.047978487 0.055187188 0.057594527 0.054281041 0.045735162 0.034099117 0.021673918 0.011163222 0.0035720288 -0.00077542756][-0.00045887381 0.0013320341 0.004164896 0.0082357917 0.0130459 0.01800699 0.021522583 0.022768732 0.021205202 0.017345615 0.012133972 0.0065327603 0.001960444 -0.0011671879 -0.0026478397]]...]
INFO - root - 2017-12-09 22:51:50.705221: step 67110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:45m:23s remains)
INFO - root - 2017-12-09 22:51:59.380603: step 67120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:55m:05s remains)
INFO - root - 2017-12-09 22:52:08.051309: step 67130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:16m:44s remains)
INFO - root - 2017-12-09 22:52:16.822303: step 67140, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:56m:58s remains)
INFO - root - 2017-12-09 22:52:25.565414: step 67150, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 66h:19m:29s remains)
INFO - root - 2017-12-09 22:52:34.183360: step 67160, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 61h:41m:39s remains)
INFO - root - 2017-12-09 22:52:42.480049: step 67170, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:09m:20s remains)
INFO - root - 2017-12-09 22:52:50.868401: step 67180, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:19m:37s remains)
INFO - root - 2017-12-09 22:52:59.423177: step 67190, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 63h:55m:06s remains)
INFO - root - 2017-12-09 22:53:08.012523: step 67200, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:02m:00s remains)
2017-12-09 22:53:08.905364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025801256 -0.0029322088 -0.0031934276 -0.0033602593 -0.0033869313 -0.003397082 -0.0033967861 -0.0033963141 -0.0033957334 -0.0033964573 -0.0033968934 -0.0033974054 -0.0033976308 -0.0033977656 -0.0033978634][-0.0019232902 -0.0025063574 -0.0030271837 -0.0033245506 -0.0033861951 -0.0033949094 -0.0033946126 -0.0033932896 -0.0033922743 -0.0033932715 -0.003393939 -0.0033947888 -0.0033956096 -0.003395983 -0.0033960955][-0.0010108836 -0.001872519 -0.0027228373 -0.0032356977 -0.0033782618 -0.0033950484 -0.0033946557 -0.0033921532 -0.0033903667 -0.0033914144 -0.0033925495 -0.0033935488 -0.0033945667 -0.0033950161 -0.0033951979][3.4601195e-05 -0.0010678556 -0.0022865161 -0.0030772903 -0.0033514772 -0.0033964417 -0.0033967518 -0.0033933567 -0.0033907359 -0.0033910812 -0.0033920009 -0.0033927585 -0.0033937176 -0.0033944319 -0.0033946002][0.0011514702 -0.00014476781 -0.0017206977 -0.0028304788 -0.0032938542 -0.0033949267 -0.003399526 -0.0033960952 -0.0033922228 -0.0033908864 -0.0033915306 -0.0033916568 -0.0033926978 -0.0033937362 -0.00339399][0.0021610304 0.00075542764 -0.0011057747 -0.0025210404 -0.0032053865 -0.0033893199 -0.0034013598 -0.0033977097 -0.0033929122 -0.0033905145 -0.0033904728 -0.0033904694 -0.0033914992 -0.0033929008 -0.0033933278][0.0029700615 0.001527874 -0.00052918796 -0.0021958342 -0.0030958732 -0.003377273 -0.0034024448 -0.0033985 -0.0033930026 -0.0033907115 -0.003389572 -0.0033891071 -0.0033900461 -0.0033922417 -0.0033924046][0.003516203 0.0020794005 -9.0000685e-05 -0.0019169427 -0.0029844132 -0.0033599129 -0.0034031842 -0.0033994634 -0.0033935865 -0.0033914857 -0.0033901855 -0.003389 -0.0033894968 -0.0033916833 -0.0033918468][0.0036799705 0.0023498808 0.00017252145 -0.001707598 -0.0028826389 -0.0033407523 -0.0034040252 -0.0033994163 -0.0033903862 -0.0033877867 -0.0033885187 -0.0033901697 -0.0033909159 -0.0033924382 -0.0033924885][0.0034879379 0.0023458151 0.00025262171 -0.0015770299 -0.0027878948 -0.0033093803 -0.0034013332 -0.003394152 -0.0033786595 -0.0033754013 -0.0033826644 -0.0033922025 -0.0033942566 -0.0033950775 -0.0033947346][0.0029009546 0.0020473858 0.00015947782 -0.001528376 -0.0027078032 -0.0032705865 -0.0033897243 -0.0033821333 -0.0033603855 -0.0033585255 -0.0033757582 -0.0033937569 -0.0033976624 -0.0033980727 -0.0033974529][0.0020545272 0.0014803454 -0.00012149243 -0.0015817184 -0.0026655982 -0.0032368752 -0.0033753545 -0.0033698673 -0.0033445521 -0.0033441728 -0.0033678801 -0.003393145 -0.0033992862 -0.0033998739 -0.0033997002][0.0010479598 0.000728091 -0.00054560718 -0.0017463245 -0.0026894144 -0.0032232525 -0.0033641844 -0.0033636142 -0.0033392147 -0.0033404974 -0.0033658391 -0.0033932065 -0.0034007165 -0.0034016781 -0.0034020164][0.00013923692 -2.8340844e-05 -0.001012014 -0.0019773482 -0.002777057 -0.0032406952 -0.0033626836 -0.0033671297 -0.0033479268 -0.0033495121 -0.003371475 -0.0033950652 -0.0034026802 -0.0034043184 -0.0034045633][-0.00042484212 -0.00054888311 -0.0013671059 -0.0021868413 -0.002893867 -0.0032812611 -0.0033722613 -0.0033794479 -0.0033680026 -0.0033688645 -0.0033825117 -0.0033984825 -0.0034047363 -0.0034068883 -0.0034072902]]...]
INFO - root - 2017-12-09 22:53:17.500649: step 67210, loss = 0.90, batch loss = 0.70 (8.7 examples/sec; 0.915 sec/batch; 67h:24m:17s remains)
INFO - root - 2017-12-09 22:53:26.139486: step 67220, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 64h:04m:24s remains)
INFO - root - 2017-12-09 22:53:34.885741: step 67230, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 66h:24m:55s remains)
INFO - root - 2017-12-09 22:53:43.551459: step 67240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:08m:13s remains)
INFO - root - 2017-12-09 22:53:52.321708: step 67250, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 64h:04m:44s remains)
INFO - root - 2017-12-09 22:54:01.130747: step 67260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:11m:22s remains)
INFO - root - 2017-12-09 22:54:09.751908: step 67270, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 61h:18m:09s remains)
INFO - root - 2017-12-09 22:54:18.398475: step 67280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:36m:27s remains)
INFO - root - 2017-12-09 22:54:27.088957: step 67290, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 64h:49m:35s remains)
INFO - root - 2017-12-09 22:54:35.778429: step 67300, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 65h:59m:56s remains)
2017-12-09 22:54:36.665299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22420016 0.2052248 0.18449938 0.16401421 0.14664955 0.13397551 0.12708491 0.12349045 0.12233803 0.12204075 0.12109614 0.11715019 0.10904528 0.0987783 0.086785488][0.24608441 0.22991945 0.21072978 0.19094554 0.17338321 0.16058163 0.1539984 0.15198784 0.15284005 0.15510786 0.15644547 0.15392703 0.14674629 0.13581336 0.12270407][0.2567735 0.24439563 0.22820325 0.21077992 0.19480883 0.18328243 0.17759782 0.1766506 0.17879769 0.18265404 0.18542293 0.18418507 0.17827827 0.16764911 0.15467401][0.26173538 0.25280926 0.23959695 0.22492361 0.21123978 0.20124803 0.19593899 0.19535585 0.19797371 0.20219615 0.20523003 0.20447768 0.19969934 0.19010942 0.17784096][0.2597467 0.25421345 0.24428667 0.23303267 0.22201338 0.21390574 0.20925723 0.20823175 0.21033382 0.2139414 0.21657433 0.21568745 0.21121456 0.20246081 0.19112121][0.25215125 0.24957415 0.24319431 0.23597328 0.22851543 0.22269516 0.21884543 0.21770746 0.21904945 0.22148123 0.22294307 0.22173752 0.21724598 0.20865436 0.19719581][0.2390202 0.23915693 0.23631237 0.23324217 0.22963998 0.22692828 0.22500892 0.22475737 0.22578681 0.22760935 0.22798172 0.22576986 0.22005832 0.21035616 0.19725591][0.22216725 0.22621965 0.22690055 0.22736013 0.22731334 0.22743413 0.22760652 0.22882928 0.23048635 0.23204479 0.23154067 0.22814834 0.22044185 0.20807897 0.19166869][0.20217986 0.21051858 0.21522598 0.21940672 0.22255391 0.22518037 0.22757196 0.23027669 0.23258288 0.23358759 0.23183019 0.22631793 0.21556692 0.19984715 0.17947054][0.17686976 0.18991616 0.19906192 0.20725301 0.21381505 0.21915847 0.22368358 0.22793886 0.23117045 0.23198619 0.22885753 0.22093503 0.20717672 0.18762662 0.16317852][0.14918011 0.16547517 0.17843898 0.19014487 0.19968325 0.2071809 0.2132372 0.21850346 0.22217217 0.22290292 0.21892385 0.2093462 0.19317289 0.17055157 0.14289227][0.12008724 0.13789687 0.15323591 0.16667828 0.17773998 0.18625726 0.19291188 0.19839844 0.20198934 0.20252104 0.19801648 0.18779173 0.17086172 0.14728682 0.11879656][0.092063978 0.11037228 0.1269539 0.14092574 0.15220541 0.16013198 0.16577229 0.16992812 0.1723699 0.17236766 0.16774385 0.15799464 0.14210011 0.12012649 0.093905665][0.063391119 0.080164067 0.096186534 0.10984006 0.12112363 0.12898897 0.13416645 0.13719688 0.1384597 0.13769166 0.13307056 0.1244501 0.11084695 0.092354752 0.070613421][0.037876606 0.050914463 0.063974433 0.075773254 0.085853167 0.093108319 0.098163188 0.1014119 0.10319905 0.10309119 0.099896818 0.09354344 0.083086744 0.068657652 0.051752277]]...]
INFO - root - 2017-12-09 22:54:45.288189: step 67310, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 66h:04m:15s remains)
INFO - root - 2017-12-09 22:54:53.875612: step 67320, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:31m:22s remains)
INFO - root - 2017-12-09 22:55:02.675688: step 67330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 64h:49m:14s remains)
INFO - root - 2017-12-09 22:55:11.413962: step 67340, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:49m:52s remains)
INFO - root - 2017-12-09 22:55:20.181228: step 67350, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 63h:58m:54s remains)
INFO - root - 2017-12-09 22:55:28.958332: step 67360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:51m:06s remains)
INFO - root - 2017-12-09 22:55:37.423628: step 67370, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:02m:40s remains)
INFO - root - 2017-12-09 22:55:45.842792: step 67380, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 60h:24m:00s remains)
INFO - root - 2017-12-09 22:55:54.306810: step 67390, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 63h:46m:05s remains)
INFO - root - 2017-12-09 22:56:02.989827: step 67400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:06m:10s remains)
2017-12-09 22:56:03.842319: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031322259 0.028714629 0.027227171 0.026090227 0.024908971 0.024056641 0.022920068 0.021846451 0.020884776 0.021107752 0.021013891 0.021241933 0.020390492 0.018349603 0.015826326][0.048631679 0.044591427 0.042135384 0.039916862 0.037584759 0.035591587 0.033325452 0.031504352 0.030270321 0.030795643 0.031381406 0.03190434 0.031567343 0.029374147 0.026231574][0.070417613 0.065266587 0.061816663 0.058334518 0.054582424 0.051111937 0.0473217 0.0444504 0.0424405 0.042803444 0.043988444 0.045286767 0.045686558 0.0433525 0.039705079][0.0918343 0.085802145 0.081816576 0.0774569 0.072538115 0.067673 0.062656552 0.058793791 0.05593941 0.05579799 0.057127472 0.058847956 0.059677228 0.057518657 0.053789128][0.11277368 0.10604635 0.10167707 0.0966649 0.091007769 0.085181065 0.079072826 0.074123934 0.07039009 0.069203719 0.069894686 0.07168144 0.072607525 0.070539631 0.066945642][0.128646 0.12209475 0.1178095 0.11263869 0.10651613 0.10007861 0.093231238 0.08735428 0.082831465 0.0803768 0.080016568 0.081256136 0.081853352 0.079600312 0.076494619][0.1343845 0.128659 0.12517998 0.12071801 0.11504295 0.10921446 0.10280211 0.096792452 0.091298766 0.087024979 0.085346393 0.085278988 0.085197181 0.0824936 0.079524584][0.12864701 0.1237843 0.12134012 0.11861721 0.11478373 0.11082388 0.10611123 0.10091235 0.095557205 0.090208218 0.087191641 0.085408174 0.08389087 0.081027694 0.078406215][0.1151135 0.11119092 0.11018417 0.10943076 0.10776035 0.10623414 0.10405795 0.10050128 0.096075639 0.090153441 0.086088866 0.082934238 0.080233462 0.077623807 0.075682707][0.097435258 0.094746359 0.095140025 0.096281417 0.097084604 0.098038234 0.098261632 0.096550718 0.093369275 0.087683223 0.083341233 0.079558089 0.076287381 0.073620342 0.072057247][0.077615738 0.076165035 0.077646554 0.08065214 0.083404362 0.086242892 0.088492051 0.088694304 0.087100357 0.082243912 0.078182943 0.074417621 0.071007185 0.068698809 0.067640737][0.058532193 0.057911143 0.05992949 0.063785546 0.0678598 0.072087266 0.07610704 0.07771606 0.077715866 0.074445568 0.071357667 0.068225868 0.0653315 0.06391976 0.063267969][0.042547718 0.042422872 0.04464836 0.048956364 0.053951211 0.058760617 0.063494615 0.065827787 0.066907518 0.064938456 0.062601641 0.060356826 0.05813802 0.057931263 0.058006596][0.02978315 0.030056873 0.0324007 0.036797538 0.041892461 0.0467278 0.051239304 0.053785544 0.055041239 0.053839963 0.052483488 0.051350538 0.05032672 0.050555628 0.051347848][0.022076156 0.022354767 0.024259554 0.028506974 0.033264913 0.037836522 0.04175964 0.044103421 0.045311753 0.044909433 0.0445366 0.044389356 0.044501919 0.045434311 0.046413224]]...]
INFO - root - 2017-12-09 22:56:12.512233: step 67410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:10m:49s remains)
INFO - root - 2017-12-09 22:56:21.125607: step 67420, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:43m:33s remains)
INFO - root - 2017-12-09 22:56:29.899520: step 67430, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 64h:19m:20s remains)
INFO - root - 2017-12-09 22:56:38.578203: step 67440, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 64h:13m:33s remains)
INFO - root - 2017-12-09 22:56:47.245268: step 67450, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:47m:18s remains)
INFO - root - 2017-12-09 22:56:55.990617: step 67460, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 65h:59m:05s remains)
INFO - root - 2017-12-09 22:57:04.505642: step 67470, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 66h:03m:01s remains)
INFO - root - 2017-12-09 22:57:13.047152: step 67480, loss = 0.89, batch loss = 0.68 (8.7 examples/sec; 0.914 sec/batch; 67h:18m:30s remains)
INFO - root - 2017-12-09 22:57:21.586843: step 67490, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 61h:01m:35s remains)
INFO - root - 2017-12-09 22:57:30.262155: step 67500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:01m:20s remains)
2017-12-09 22:57:31.133940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033358806 -0.0033411856 -0.0033516947 -0.0033657746 -0.0033789484 -0.0033885539 -0.0033945825 -0.0033977497 -0.003396404 -0.0033940037 -0.0033913683 -0.0033890195 -0.0033872954 -0.0033866416 -0.00338712][-0.0033328105 -0.0033373151 -0.0033470523 -0.0033599788 -0.0033723495 -0.0033818011 -0.0033880114 -0.0033923041 -0.0033925604 -0.0033908708 -0.0033888204 -0.0033871077 -0.0033850907 -0.003383335 -0.0033833391][-0.0033328859 -0.0033357781 -0.00334283 -0.0033533408 -0.0033635418 -0.0033726965 -0.003379724 -0.003385846 -0.0033885816 -0.0033885413 -0.0033879359 -0.0033871774 -0.0033849617 -0.0033824097 -0.0033818164][-0.0033330696 -0.0033343418 -0.0033393295 -0.0033465696 -0.003354616 -0.0033632517 -0.0033712953 -0.0033788634 -0.0033840339 -0.0033868102 -0.0033881827 -0.0033885536 -0.0033871972 -0.0033839981 -0.0033817978][-0.0033345814 -0.0033335774 -0.0033358845 -0.0033406534 -0.0033469265 -0.0033547273 -0.0033630107 -0.0033716918 -0.0033790255 -0.0033840123 -0.0033872137 -0.0033890526 -0.0033888235 -0.0033864845 -0.0033838728][-0.0033377658 -0.0033349025 -0.0033344226 -0.0033362792 -0.0033398324 -0.0033461503 -0.0033549815 -0.003365187 -0.0033747598 -0.0033821189 -0.003387297 -0.0033899094 -0.0033904796 -0.0033888668 -0.00338554][-0.0033396278 -0.0033364256 -0.0033346412 -0.003333929 -0.0033348117 -0.0033394787 -0.0033467989 -0.0033575 -0.003369414 -0.0033793249 -0.0033867192 -0.0033907325 -0.0033924873 -0.0033912379 -0.0033882589][-0.0033396031 -0.0033367444 -0.0033349448 -0.0033336536 -0.0033342706 -0.0033375821 -0.0033432315 -0.0033529378 -0.0033651667 -0.003376672 -0.0033857815 -0.0033912084 -0.003394102 -0.0033937492 -0.0033911783][-0.0033377439 -0.0033356908 -0.0033352224 -0.0033343709 -0.0033348165 -0.0033385605 -0.003344381 -0.0033536647 -0.0033646738 -0.0033763428 -0.0033854416 -0.0033908999 -0.0033939255 -0.0033943676 -0.0033927539][-0.0033353055 -0.003333935 -0.0033344165 -0.0033348198 -0.0033367274 -0.0033413996 -0.00334784 -0.0033572109 -0.003368373 -0.0033792167 -0.0033875916 -0.0033916633 -0.0033938533 -0.0033936664 -0.0033915387][-0.0033326482 -0.0033317951 -0.0033336503 -0.0033362682 -0.0033393952 -0.003344808 -0.0033527813 -0.0033625346 -0.0033735558 -0.0033830113 -0.0033898852 -0.0033930051 -0.0033941031 -0.0033933122 -0.0033909783][-0.0033317045 -0.0033306021 -0.0033333888 -0.003337875 -0.003343276 -0.0033505321 -0.0033593483 -0.0033689314 -0.0033790839 -0.0033873327 -0.0033921814 -0.0033934864 -0.0033936088 -0.0033925714 -0.0033902889][-0.0033338657 -0.0033329616 -0.003336251 -0.0033420518 -0.0033487168 -0.0033572752 -0.003366187 -0.0033754795 -0.0033838584 -0.0033896591 -0.0033932626 -0.003392926 -0.0033918011 -0.0033903616 -0.0033882505][-0.0033400706 -0.0033388687 -0.0033421393 -0.0033486173 -0.003355748 -0.0033640186 -0.0033722443 -0.0033803338 -0.0033868051 -0.0033917907 -0.0033933574 -0.0033921239 -0.0033899688 -0.0033877851 -0.0033861368][-0.0033501803 -0.0033490132 -0.0033524283 -0.0033584044 -0.0033647579 -0.0033719763 -0.0033787561 -0.0033851066 -0.0033896083 -0.0033920165 -0.0033918179 -0.0033903793 -0.0033883615 -0.0033860747 -0.0033841291]]...]
INFO - root - 2017-12-09 22:57:39.652379: step 67510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:05m:04s remains)
INFO - root - 2017-12-09 22:57:48.182027: step 67520, loss = 0.91, batch loss = 0.70 (9.6 examples/sec; 0.830 sec/batch; 61h:05m:14s remains)
INFO - root - 2017-12-09 22:57:56.899317: step 67530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:07m:36s remains)
INFO - root - 2017-12-09 22:58:05.521322: step 67540, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 61h:01m:21s remains)
INFO - root - 2017-12-09 22:58:14.252440: step 67550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 63h:50m:04s remains)
INFO - root - 2017-12-09 22:58:22.915714: step 67560, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:09m:01s remains)
INFO - root - 2017-12-09 22:58:31.469030: step 67570, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:55m:16s remains)
INFO - root - 2017-12-09 22:58:39.911444: step 67580, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 59h:54m:56s remains)
INFO - root - 2017-12-09 22:58:48.424794: step 67590, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 63h:08m:01s remains)
INFO - root - 2017-12-09 22:58:57.161103: step 67600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 66h:25m:01s remains)
2017-12-09 22:58:58.072204: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045215819 0.043621067 0.041301697 0.038504567 0.0354847 0.032146323 0.028741578 0.025457434 0.022132773 0.01881833 0.015292242 0.011827279 0.0087211384 0.0062647797 0.004670185][0.046462871 0.045600917 0.044145156 0.041756596 0.038873214 0.035202116 0.031020878 0.026584238 0.02199419 0.017598471 0.013228631 0.0093891472 0.0060804719 0.0037292878 0.0022902905][0.047631256 0.046945572 0.045543645 0.043542679 0.04086069 0.037348453 0.033237759 0.028428355 0.023116294 0.017561879 0.012190098 0.0075149881 0.0037077994 0.0011316675 -0.00026759296][0.049194038 0.049183641 0.048063688 0.045695528 0.042649776 0.038741171 0.034572706 0.02966539 0.024301384 0.018605508 0.012942361 0.007824989 0.0034971244 0.00051564141 -0.0012778698][0.049316932 0.049947962 0.049390446 0.047687244 0.045155581 0.041453548 0.037273329 0.03274633 0.027642492 0.021894854 0.016051035 0.010550452 0.0056496132 0.0018701043 -0.00065441104][0.048596874 0.049652398 0.049816135 0.049063087 0.0475511 0.044994503 0.041441623 0.036879055 0.031396445 0.025579816 0.019717967 0.014003525 0.0089215562 0.0048026871 0.0016535267][0.047444396 0.049058843 0.049737569 0.04977202 0.04926306 0.047709242 0.045243863 0.041461233 0.036572117 0.03087656 0.024878103 0.018946173 0.013393292 0.0086446209 0.0046873232][0.046046741 0.048224829 0.049441963 0.050070774 0.050121162 0.049045615 0.046814378 0.043655537 0.039478626 0.034552708 0.02922531 0.023721656 0.018276727 0.013036709 0.0082167387][0.0452944 0.047158521 0.048375182 0.049274195 0.049798671 0.049202025 0.047407012 0.044530712 0.040745575 0.036549635 0.031925082 0.027359622 0.022586439 0.017433576 0.012152233][0.044891424 0.046809986 0.048040811 0.048764534 0.049239073 0.048937216 0.047617711 0.045191519 0.042032391 0.038398445 0.034175344 0.029762672 0.02487929 0.01943461 0.013783429][0.04451203 0.046823226 0.04823624 0.0489257 0.04922064 0.048603423 0.047189258 0.044913296 0.042259291 0.039127227 0.035425935 0.031300418 0.02647965 0.020900972 0.014860181][0.041296635 0.044167455 0.046086486 0.047562908 0.048543904 0.048378818 0.047134031 0.044842418 0.04229195 0.039277341 0.035859082 0.032073218 0.027528217 0.022213673 0.016190445][0.038165178 0.040484615 0.042377632 0.04418673 0.04559084 0.046149459 0.045726087 0.044424828 0.042438924 0.039820258 0.036743425 0.033211373 0.028944934 0.02395582 0.01826046][0.035755813 0.037473347 0.038539011 0.039663494 0.040667128 0.042051688 0.043008517 0.043108996 0.042425249 0.041002 0.0387824 0.035454463 0.031406306 0.026743319 0.021435671][0.034627475 0.036141049 0.036951687 0.037673794 0.038333356 0.03921755 0.039834406 0.040094305 0.04030738 0.039902896 0.038994819 0.037087962 0.034225065 0.030270996 0.025420813]]...]
INFO - root - 2017-12-09 22:59:06.805341: step 67610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:07m:38s remains)
INFO - root - 2017-12-09 22:59:15.418548: step 67620, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 64h:00m:05s remains)
INFO - root - 2017-12-09 22:59:24.103578: step 67630, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:37m:08s remains)
INFO - root - 2017-12-09 22:59:32.859883: step 67640, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:13m:02s remains)
INFO - root - 2017-12-09 22:59:41.449683: step 67650, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 64h:09m:38s remains)
INFO - root - 2017-12-09 22:59:50.167684: step 67660, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 65h:28m:43s remains)
INFO - root - 2017-12-09 22:59:58.684885: step 67670, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 65h:42m:18s remains)
INFO - root - 2017-12-09 23:00:07.175940: step 67680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:22m:39s remains)
INFO - root - 2017-12-09 23:00:15.613594: step 67690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:25m:56s remains)
INFO - root - 2017-12-09 23:00:24.222292: step 67700, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 61h:30m:10s remains)
2017-12-09 23:00:25.037890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033594731 0.036648922 0.035765056 0.03479645 0.032792021 0.031018266 0.028275045 0.024615295 0.019268328 0.01345397 0.007247448 0.0021852383 -0.0011917702 -0.0028172524 -0.0032170976][0.040333133 0.043060645 0.041143566 0.039170962 0.036389865 0.033915516 0.030737417 0.027405312 0.022538515 0.016779386 0.010013253 0.0041049263 -0.00019795285 -0.0024458636 -0.0032372009][0.043882962 0.046384338 0.04415245 0.041712996 0.038291309 0.035542287 0.032150209 0.028616888 0.023672862 0.018196432 0.011660768 0.0056935018 0.00080821081 -0.0020389333 -0.0031632988][0.047770016 0.049825393 0.047202528 0.044126712 0.039905157 0.036444906 0.032495335 0.02891867 0.024196263 0.018888721 0.012397787 0.0064085228 0.00129795 -0.001768192 -0.0031005265][0.051373471 0.053477187 0.050374616 0.046494737 0.041312702 0.036818586 0.03197347 0.027844442 0.022990756 0.018042162 0.011985542 0.00626452 0.001253553 -0.0017440565 -0.0030850903][0.055094633 0.056872092 0.052950595 0.048159562 0.042059213 0.036425386 0.030563993 0.025662459 0.020545073 0.015798 0.010226927 0.005067491 0.00057553244 -0.0020272485 -0.0031537802][0.058070384 0.059062742 0.05427739 0.048357897 0.041152485 0.034543972 0.027926954 0.022419656 0.017198171 0.012597581 0.0075941738 0.003146115 -0.00050416542 -0.0024804852 -0.0032626947][0.058169328 0.058568005 0.053252678 0.046408039 0.038250372 0.030589433 0.023256261 0.017543156 0.012690678 0.008610148 0.004470747 0.0010048964 -0.00159819 -0.0028993967 -0.0033474881][0.053724173 0.054604243 0.049027167 0.041708004 0.032958489 0.024567997 0.016911983 0.011197501 0.0069076996 0.0037534034 0.00094657554 -0.0011730925 -0.0025563589 -0.0031991997 -0.003380049][0.046931036 0.048342451 0.042328291 0.034666784 0.025687464 0.017183511 0.0099029122 0.0048770569 0.0016761876 -0.00038117473 -0.0018922727 -0.0027601698 -0.0031973184 -0.0033551762 -0.0033899271][0.039857093 0.041532796 0.03499857 0.02705124 0.018076586 0.010050634 0.0038037708 6.6971639e-05 -0.0017531035 -0.0026357782 -0.00312108 -0.0033336452 -0.0033886712 -0.0033945555 -0.0033937213][0.034474958 0.035991069 0.028753115 0.020614935 0.012080875 0.0049650371 9.3930168e-05 -0.002266814 -0.0030555099 -0.0032903077 -0.0033729903 -0.003394417 -0.0033954128 -0.0033953509 -0.0033952305][0.030871155 0.031661686 0.023700435 0.015582085 0.0076568248 0.0016911121 -0.0017887388 -0.0031489127 -0.0033881443 -0.0033973001 -0.003396994 -0.0033963323 -0.003396078 -0.003395915 -0.0033957609][0.029297682 0.028216263 0.019681171 0.011609795 0.0045245355 -0.0002317843 -0.0025795917 -0.0033252565 -0.0033957551 -0.003397004 -0.0033974708 -0.0033974149 -0.0033971758 -0.0033970424 -0.0033958254][0.02724478 0.02482925 0.016427808 0.0087126074 0.0024153481 -0.0013879021 -0.0029727744 -0.0033769142 -0.003392258 -0.0033947271 -0.0033960985 -0.0033965788 -0.0033969227 -0.003397271 -0.0033960093]]...]
INFO - root - 2017-12-09 23:00:33.821683: step 67710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:37m:55s remains)
INFO - root - 2017-12-09 23:00:42.612664: step 67720, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 62h:52m:36s remains)
INFO - root - 2017-12-09 23:00:51.415051: step 67730, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:00m:59s remains)
INFO - root - 2017-12-09 23:01:00.244845: step 67740, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 63h:15m:17s remains)
INFO - root - 2017-12-09 23:01:09.000844: step 67750, loss = 0.89, batch loss = 0.69 (8.2 examples/sec; 0.978 sec/batch; 71h:55m:35s remains)
INFO - root - 2017-12-09 23:01:17.604669: step 67760, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 61h:09m:08s remains)
INFO - root - 2017-12-09 23:01:25.974452: step 67770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:34m:57s remains)
INFO - root - 2017-12-09 23:01:34.392110: step 67780, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 63h:58m:28s remains)
INFO - root - 2017-12-09 23:01:42.841747: step 67790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:47m:54s remains)
INFO - root - 2017-12-09 23:01:51.416460: step 67800, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:36m:54s remains)
2017-12-09 23:01:52.444920: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037033979 0.04346947 0.053185452 0.065113783 0.077777788 0.088939153 0.096580043 0.098149784 0.092752576 0.0813662 0.065915644 0.048918493 0.032277256 0.018056175 0.007402814][0.040243369 0.04989836 0.064370558 0.082061857 0.10049295 0.11607558 0.12616621 0.12820072 0.12169334 0.10806084 0.089344867 0.068108149 0.046454266 0.027076282 0.012145631][0.041109625 0.054339834 0.073991396 0.098028332 0.12277026 0.14312389 0.15561834 0.15771255 0.14980696 0.13386849 0.11209914 0.086982578 0.060707808 0.036469225 0.01718122][0.040204018 0.057053618 0.081599005 0.11163139 0.14264706 0.16816369 0.18374276 0.18657652 0.17740545 0.15871722 0.1331379 0.10348786 0.072557569 0.044017471 0.021180814][0.036106154 0.056321058 0.08540339 0.1206949 0.15706277 0.18703623 0.20548089 0.20934121 0.19935891 0.17821315 0.14902957 0.11527982 0.080369785 0.048471145 0.023190778][0.030327329 0.051773872 0.083209373 0.12181233 0.161902 0.19522463 0.2159988 0.22089997 0.21050835 0.187584 0.15567906 0.11907962 0.081882171 0.0484928 0.022565657][0.023915824 0.044127025 0.074568674 0.11289981 0.15364629 0.18831187 0.21048348 0.21646447 0.20628963 0.1826632 0.14960128 0.11229391 0.075360455 0.0431973 0.019097665][0.018008905 0.035002042 0.061474591 0.095960811 0.13366719 0.1667065 0.18854663 0.19504845 0.18556623 0.16264829 0.13064454 0.095553346 0.062092662 0.034095816 0.013996011][0.012409322 0.025645267 0.046689238 0.074881472 0.10658304 0.13521107 0.15469702 0.16081667 0.1524971 0.13202423 0.103656 0.073484123 0.045827009 0.023717903 0.0086104022][0.006967687 0.016261868 0.031586278 0.052761942 0.077145584 0.09970735 0.11534068 0.12023674 0.11328395 0.096474044 0.073670015 0.050296195 0.029720454 0.014035435 0.0038718556][0.0030049824 0.0085517252 0.018233793 0.032352604 0.049231716 0.065357327 0.076789565 0.080396727 0.075183168 0.062748231 0.04624154 0.029928716 0.016222017 0.006344323 0.00035588676][0.00030354341 0.0032381972 0.0085529955 0.016724721 0.026897324 0.036995716 0.044359647 0.046716575 0.043268833 0.035111211 0.024493802 0.014421223 0.0064088739 0.0010816061 -0.0018318651][-0.0014760019 -7.1748858e-05 0.0025253978 0.0066917464 0.012052488 0.017564207 0.021675648 0.022937704 0.020827325 0.016024308 0.0099830311 0.0045734718 0.00060549309 -0.0017272712 -0.0028287133][-0.0026668543 -0.0020206124 -0.00087569095 0.0009751895 0.003421074 0.0060093245 0.0079481527 0.00847289 0.0073251547 0.0048751896 0.0019557641 -0.00047175377 -0.0020824331 -0.0028809004 -0.0031849944][-0.0032650109 -0.0030634755 -0.0026540135 -0.0019614496 -0.0010180085 4.2999163e-06 0.00077538309 0.00095604756 0.000429105 -0.00061209314 -0.0017450389 -0.0025848 -0.0030571031 -0.0032380756 -0.0032883312]]...]
INFO - root - 2017-12-09 23:02:01.202257: step 67810, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.890 sec/batch; 65h:25m:09s remains)
INFO - root - 2017-12-09 23:02:09.987888: step 67820, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 65h:41m:11s remains)
INFO - root - 2017-12-09 23:02:18.630746: step 67830, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 63h:34m:01s remains)
INFO - root - 2017-12-09 23:02:27.571431: step 67840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:48m:33s remains)
INFO - root - 2017-12-09 23:02:36.383135: step 67850, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:49m:46s remains)
INFO - root - 2017-12-09 23:02:45.038897: step 67860, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:47m:45s remains)
INFO - root - 2017-12-09 23:02:53.494437: step 67870, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 64h:33m:51s remains)
INFO - root - 2017-12-09 23:03:01.785417: step 67880, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 53h:35m:36s remains)
INFO - root - 2017-12-09 23:03:10.360509: step 67890, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:10m:40s remains)
INFO - root - 2017-12-09 23:03:19.084499: step 67900, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:03m:58s remains)
2017-12-09 23:03:19.942382: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.6126948 0.59380144 0.56522822 0.53223324 0.50002575 0.47428873 0.4577713 0.45248711 0.45540562 0.46349558 0.46780738 0.46425202 0.45011893 0.42860761 0.4060764][0.6951012 0.67925477 0.65036649 0.61548859 0.57989615 0.55145538 0.532175 0.52422953 0.52390838 0.52777886 0.52687806 0.51756525 0.49718356 0.47057226 0.44273794][0.753839 0.74340081 0.71708941 0.68246055 0.64546537 0.61370325 0.59174836 0.57978719 0.57449943 0.5726546 0.565485 0.54999763 0.52398604 0.49303207 0.46295294][0.80026025 0.79533714 0.771109 0.73579448 0.694765 0.65810966 0.6313265 0.613467 0.601172 0.59204304 0.57854122 0.55744845 0.52745891 0.49474347 0.4644759][0.8266834 0.829443 0.80696142 0.77016312 0.72631466 0.68334097 0.64906245 0.62358373 0.60238314 0.58381951 0.56132829 0.53560632 0.50336593 0.47124302 0.44457909][0.831979 0.842721 0.82341182 0.78698838 0.73956412 0.68873608 0.64584857 0.609963 0.57804281 0.54827738 0.51696527 0.48632869 0.45422798 0.42659956 0.40693951][0.8283515 0.84514374 0.8265152 0.78954232 0.73832172 0.6799835 0.62561154 0.57710695 0.53183037 0.49129117 0.45270178 0.42038748 0.39116246 0.37030262 0.36057326][0.82405841 0.84444809 0.82431257 0.78368771 0.72719371 0.66037482 0.59549421 0.534238 0.4751831 0.42280269 0.37569636 0.33946583 0.31285673 0.29924908 0.30102631][0.81626844 0.84098 0.81998718 0.77740973 0.71644342 0.64155912 0.56497896 0.49007106 0.41689867 0.35297656 0.29763514 0.25804907 0.23375063 0.22677441 0.23885176][0.803171 0.83327264 0.81347531 0.77148676 0.70928574 0.62947291 0.54344755 0.4566038 0.37131023 0.29557914 0.23221347 0.18918574 0.16669565 0.16444264 0.18510982][0.78660858 0.81868535 0.80127823 0.76251507 0.70136565 0.61987168 0.52991265 0.43585292 0.34268096 0.2597461 0.1920227 0.14686446 0.12493315 0.12590812 0.15079287][0.76613164 0.79965144 0.78388339 0.74761164 0.68957675 0.61225748 0.52281636 0.42753515 0.33223194 0.24745601 0.17920578 0.13417578 0.11333168 0.114868 0.14034669][0.73128867 0.76780635 0.75792718 0.72781307 0.67680836 0.60668063 0.52368295 0.43319917 0.34171924 0.25932732 0.19217183 0.14724262 0.12580626 0.1256209 0.14795013][0.68170762 0.72010785 0.71786547 0.69789964 0.65840489 0.59940654 0.526246 0.44405532 0.360214 0.28333002 0.22074294 0.17893678 0.15812905 0.15630576 0.17373331][0.62631112 0.66418892 0.66658765 0.65485275 0.62617153 0.58082277 0.52118206 0.45191973 0.37967464 0.3115257 0.25602555 0.2179437 0.19784537 0.19455831 0.20717078]]...]
INFO - root - 2017-12-09 23:03:28.556542: step 67910, loss = 0.89, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 60h:49m:40s remains)
INFO - root - 2017-12-09 23:03:37.066774: step 67920, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 60h:20m:25s remains)
INFO - root - 2017-12-09 23:03:45.635397: step 67930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 61h:25m:18s remains)
INFO - root - 2017-12-09 23:03:54.187927: step 67940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:03m:20s remains)
INFO - root - 2017-12-09 23:04:02.857107: step 67950, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:59m:27s remains)
INFO - root - 2017-12-09 23:04:11.685001: step 67960, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 66h:35m:05s remains)
INFO - root - 2017-12-09 23:04:20.097651: step 67970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:25m:18s remains)
INFO - root - 2017-12-09 23:04:28.810745: step 67980, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:41m:12s remains)
INFO - root - 2017-12-09 23:04:37.276804: step 67990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 63h:32m:35s remains)
INFO - root - 2017-12-09 23:04:45.944152: step 68000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:38m:59s remains)
2017-12-09 23:04:46.795267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0304244 0.033417456 0.0338638 0.03169518 0.027268084 0.022170713 0.016937688 0.013085881 0.010750986 0.010257998 0.011705483 0.014572702 0.019786283 0.028139884 0.039891377][0.031519663 0.036410589 0.038780928 0.038450856 0.034958966 0.02926304 0.02221808 0.016318951 0.011951001 0.01052025 0.01151541 0.014175382 0.019082472 0.027060339 0.038096346][0.030744815 0.037586208 0.041770935 0.042890739 0.040644072 0.035713803 0.028170204 0.020685058 0.014446687 0.011535726 0.011532885 0.013420947 0.017173663 0.023842173 0.033633221][0.028782962 0.037386533 0.043509137 0.046178281 0.044832289 0.040206067 0.032757379 0.0248733 0.017559743 0.013226604 0.011615155 0.012580518 0.015545852 0.020876901 0.029031685][0.02499631 0.034759648 0.04265872 0.046919752 0.047113996 0.043646045 0.036852337 0.029224807 0.02172244 0.016472349 0.013826577 0.013814947 0.015696421 0.01949429 0.026123706][0.020592861 0.030340558 0.03897582 0.044442385 0.046118084 0.044113595 0.039378371 0.033623409 0.027217649 0.021754496 0.018310182 0.01739393 0.018589186 0.020933444 0.02519132][0.016422421 0.0249723 0.03318521 0.038986631 0.041795529 0.0418057 0.039878674 0.036893196 0.032742612 0.028327456 0.025272816 0.023738634 0.023896232 0.024665473 0.027295128][0.012595396 0.019889545 0.027235076 0.032653987 0.035845853 0.037167769 0.037799109 0.038085539 0.037235584 0.034902591 0.032758813 0.031284828 0.031348132 0.03124346 0.032059148][0.00883617 0.014729552 0.020996274 0.025992192 0.029290283 0.03150665 0.033941112 0.036791556 0.038708724 0.0385707 0.038173594 0.037578274 0.037898015 0.037726067 0.038297169][0.0055102315 0.0099113258 0.014683396 0.018841594 0.022072339 0.024927482 0.028714975 0.033730831 0.0381199 0.040271465 0.04103535 0.041170932 0.04198803 0.041715439 0.041634921][0.0024493223 0.0054878434 0.0089720013 0.01216268 0.014834855 0.017750075 0.022277219 0.02851736 0.03458862 0.038724497 0.041067831 0.042079013 0.042758726 0.042251155 0.041824795][0.00014177943 0.0019976983 0.0043130117 0.0064690961 0.0083733173 0.010719582 0.01495339 0.021340823 0.028162258 0.033520374 0.037050381 0.038883328 0.039720282 0.039039966 0.037881158][-0.0015681054 -0.00037252763 0.0011609539 0.0025219775 0.0036770955 0.0052859187 0.0085321935 0.013715528 0.019742794 0.025104292 0.029151661 0.031579651 0.032720912 0.032200933 0.03068251][-0.0026787478 -0.0020382544 -0.001164973 -0.00035772519 0.00037091668 0.0014099593 0.0036241703 0.0073416578 0.011871652 0.016211079 0.019827012 0.022286203 0.023612563 0.023245784 0.021829059][-0.0032063478 -0.0030240952 -0.002679012 -0.0022345427 -0.001796807 -0.00116534 0.00011613406 0.0023854233 0.0053980267 0.0086743 0.011679437 0.013830053 0.015047336 0.014665544 0.013305545]]...]
INFO - root - 2017-12-09 23:04:55.652689: step 68010, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 66h:03m:34s remains)
INFO - root - 2017-12-09 23:05:04.354006: step 68020, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 61h:01m:26s remains)
INFO - root - 2017-12-09 23:05:13.113475: step 68030, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 63h:53m:32s remains)
INFO - root - 2017-12-09 23:05:21.608035: step 68040, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 61h:04m:59s remains)
INFO - root - 2017-12-09 23:05:30.279056: step 68050, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:30m:16s remains)
INFO - root - 2017-12-09 23:05:38.837388: step 68060, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 63h:23m:31s remains)
INFO - root - 2017-12-09 23:05:47.335858: step 68070, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 61h:22m:48s remains)
INFO - root - 2017-12-09 23:05:55.983448: step 68080, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 63h:45m:38s remains)
INFO - root - 2017-12-09 23:06:04.272051: step 68090, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 61h:39m:50s remains)
INFO - root - 2017-12-09 23:06:13.008723: step 68100, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 64h:06m:39s remains)
2017-12-09 23:06:13.861905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033752152 -0.0033731959 -0.0033729589 -0.0033725365 -0.0033721924 -0.0033714981 -0.0033710212 -0.0033700326 -0.0033692627 -0.0033683649 -0.0033678315 -0.0033675714 -0.0033675872 -0.0033681553 -0.0033687903][-0.0033739118 -0.0033722178 -0.0033723009 -0.0033720532 -0.003371858 -0.0033712762 -0.0033709053 -0.0033698746 -0.003368895 -0.0033677993 -0.0033669882 -0.0033664308 -0.0033664275 -0.0033669069 -0.003367549][-0.0033739281 -0.0033725242 -0.0033729151 -0.0033731395 -0.0033734753 -0.0033734411 -0.0033732792 -0.0033722424 -0.003371075 -0.0033695358 -0.0033683658 -0.003367268 -0.0033669067 -0.0033671514 -0.0033677497][-0.0033741191 -0.0033731835 -0.0033740513 -0.0033747256 -0.0033756869 -0.003376436 -0.0033768064 -0.0033760245 -0.0033745612 -0.0033726548 -0.0033708275 -0.0033691518 -0.0033681847 -0.0033677972 -0.0033679812][-0.0033740762 -0.0033737728 -0.0033752294 -0.0033766755 -0.0033785913 -0.0033800609 -0.0033808926 -0.0033804211 -0.003378974 -0.0033767133 -0.0033744895 -0.003372336 -0.0033707216 -0.0033699456 -0.0033693293][-0.0033736515 -0.0033737435 -0.0033758343 -0.0033782062 -0.0033809508 -0.003382754 -0.0033836721 -0.0033829855 -0.0033811543 -0.0033783794 -0.0033763112 -0.0033747603 -0.0033736501 -0.0033732196 -0.0033725835][-0.0033732352 -0.0033737014 -0.003376578 -0.0033796807 -0.003382768 -0.0033844416 -0.0033847347 -0.0033834905 -0.0033809789 -0.0033775223 -0.0033753836 -0.0033747244 -0.0033746324 -0.0033746525 -0.0033740539][-0.003373432 -0.0033742355 -0.0033776835 -0.0033813445 -0.0033844889 -0.0033857315 -0.0033846663 -0.003381714 -0.0033773335 -0.0033725312 -0.0033699274 -0.0033695626 -0.0033707968 -0.0033718264 -0.0033719658][-0.0033733628 -0.0033742008 -0.0033782385 -0.0033822223 -0.0033849627 -0.0033853753 -0.0033824211 -0.0033773547 -0.0033707351 -0.0033644082 -0.0033608838 -0.0033603068 -0.0033616915 -0.0033628899 -0.0033633998][-0.0033724075 -0.0033729847 -0.0033775158 -0.0033821287 -0.0033843929 -0.0033835946 -0.0033787328 -0.0033710576 -0.0033618489 -0.0033533743 -0.0033487047 -0.003347026 -0.003347649 -0.0033486064 -0.0033490804][-0.0033710811 -0.0033713747 -0.0033759556 -0.0033809266 -0.0033827652 -0.003381206 -0.0033746182 -0.0033648456 -0.0033533252 -0.0033424411 -0.0033356585 -0.0033320135 -0.0033311213 -0.0033312831 -0.0033315148][-0.0033698354 -0.0033697386 -0.0033740874 -0.0033790166 -0.0033806989 -0.003378571 -0.0033711069 -0.0033593981 -0.0033455798 -0.0033319863 -0.0033220018 -0.0033157363 -0.0033131414 -0.0033123249 -0.0033121079][-0.0033690189 -0.0033684075 -0.0033723284 -0.0033768862 -0.0033781356 -0.0033757635 -0.0033678042 -0.0033548074 -0.0033388769 -0.0033232619 -0.0033109467 -0.0033023364 -0.0032979217 -0.0032962582 -0.0032956153][-0.0033687295 -0.0033676364 -0.0033708992 -0.0033751049 -0.0033762231 -0.0033736755 -0.0033656245 -0.0033523266 -0.0033355616 -0.0033185175 -0.0033045474 -0.0032945788 -0.0032887997 -0.0032860613 -0.0032855][-0.0033694622 -0.0033680212 -0.0033704985 -0.003374191 -0.0033752688 -0.0033722678 -0.0033640505 -0.0033507554 -0.0033334668 -0.0033156604 -0.0033011972 -0.003291362 -0.003285826 -0.0032835971 -0.0032837121]]...]
INFO - root - 2017-12-09 23:06:22.610127: step 68110, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 61h:05m:57s remains)
INFO - root - 2017-12-09 23:06:31.320942: step 68120, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:46m:12s remains)
INFO - root - 2017-12-09 23:06:40.066766: step 68130, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 64h:25m:50s remains)
INFO - root - 2017-12-09 23:06:48.909694: step 68140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:13m:31s remains)
INFO - root - 2017-12-09 23:06:57.594709: step 68150, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 64h:21m:06s remains)
INFO - root - 2017-12-09 23:07:06.317074: step 68160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:02m:44s remains)
INFO - root - 2017-12-09 23:07:14.865050: step 68170, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 67h:22m:02s remains)
INFO - root - 2017-12-09 23:07:23.477903: step 68180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:44m:29s remains)
INFO - root - 2017-12-09 23:07:31.676128: step 68190, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:28m:42s remains)
INFO - root - 2017-12-09 23:07:40.389033: step 68200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 63h:46m:25s remains)
2017-12-09 23:07:41.364005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032502676 -0.0032562464 -0.0032544718 -0.0032340551 -0.0032122927 -0.00321949 -0.0032195926 -0.0031943577 -0.0031240985 -0.0030497368 -0.0029934382 -0.0030011218 -0.0030408532 -0.0030969968 -0.003180987][-0.0033590151 -0.0033533538 -0.0033496476 -0.0033069011 -0.0032451008 -0.003229083 -0.0031979159 -0.0031311221 -0.0030455177 -0.0029939581 -0.0029565697 -0.0029533054 -0.002977856 -0.0030192763 -0.0031245784][-0.0032784862 -0.0032674836 -0.0032752722 -0.0032821321 -0.0032488324 -0.0032185065 -0.0031728495 -0.00309107 -0.0029700012 -0.0029465584 -0.0029240525 -0.0029358345 -0.0030144446 -0.003066503 -0.0031466968][-0.0032343329 -0.0032372652 -0.0032544811 -0.0032595848 -0.0032226681 -0.0032192352 -0.0031941098 -0.0031476819 -0.0030721934 -0.0030104723 -0.0029102238 -0.0028982 -0.0029737647 -0.0030329803 -0.0031431031][-0.0031780417 -0.0032418228 -0.0032718373 -0.0032653965 -0.0032542602 -0.0032537896 -0.0032246003 -0.0031986476 -0.0031482431 -0.0031269493 -0.003109535 -0.0031031193 -0.0031403487 -0.0031641719 -0.0032023471][-0.0029067784 -0.002968072 -0.0030619954 -0.0031117641 -0.0031319035 -0.0031246666 -0.0031282124 -0.0031483707 -0.0031677487 -0.0032007582 -0.0032256669 -0.0032440214 -0.0032548872 -0.0032913026 -0.0033190979][-0.0028888772 -0.0028905622 -0.0029686592 -0.0029872502 -0.0029960184 -0.0030152765 -0.0029982978 -0.0029955795 -0.0030173622 -0.0030739084 -0.0031486093 -0.0032291715 -0.0032954407 -0.0033295841 -0.0033508122][-0.0028038931 -0.002792354 -0.0028744799 -0.00288255 -0.0029303371 -0.0029736562 -0.0029843219 -0.0030376264 -0.0030809261 -0.0031214848 -0.0031573207 -0.0032048994 -0.0032159423 -0.0032541647 -0.0032932004][-0.00261214 -0.0026077516 -0.0026940454 -0.0027226431 -0.0027755636 -0.002798859 -0.0028033119 -0.0028579959 -0.0029075216 -0.0029946037 -0.0030895302 -0.0031722633 -0.0032065867 -0.0032258588 -0.0032596642][-0.0023855357 -0.002325072 -0.0024290266 -0.00247201 -0.0025426245 -0.0026102639 -0.0026447331 -0.0026796961 -0.0027245416 -0.0027892038 -0.0028778389 -0.0029823938 -0.0030570733 -0.0031177634 -0.0031609167][-0.0022022491 -0.0021036435 -0.0021454059 -0.0021955906 -0.0022638757 -0.0023259688 -0.002392943 -0.0024175323 -0.0024393331 -0.002503182 -0.0025580367 -0.0026535005 -0.0027501532 -0.0028511849 -0.0029521757][-0.0022349576 -0.0020863139 -0.0021260781 -0.0020894948 -0.0020995494 -0.0021033064 -0.0021580723 -0.0022006754 -0.0022421652 -0.0022807154 -0.0023227669 -0.0024008921 -0.0025077537 -0.0026296931 -0.0027621477][-0.0021320982 -0.0020563412 -0.0021225354 -0.00211689 -0.0021346009 -0.0021736859 -0.0022366638 -0.0021775528 -0.0021092412 -0.0020813793 -0.0021316949 -0.002204943 -0.002311029 -0.0024599168 -0.0026256642][-0.0018969364 -0.0017963829 -0.0018274429 -0.0018662439 -0.0019392396 -0.0019769543 -0.0020479131 -0.00210329 -0.0021668097 -0.0021588409 -0.0021487032 -0.0021724915 -0.002233644 -0.0023964485 -0.0025647292][-0.0016346927 -0.0015591709 -0.001637477 -0.0016297003 -0.0016645544 -0.0017570754 -0.0018465089 -0.001910878 -0.0020101592 -0.0020943861 -0.0021732021 -0.0022510444 -0.0023387996 -0.0024173581 -0.0025321462]]...]
INFO - root - 2017-12-09 23:07:49.998131: step 68210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:49m:39s remains)
INFO - root - 2017-12-09 23:07:58.687824: step 68220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:08m:09s remains)
INFO - root - 2017-12-09 23:08:07.462960: step 68230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:38m:11s remains)
INFO - root - 2017-12-09 23:08:16.351615: step 68240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:30m:58s remains)
INFO - root - 2017-12-09 23:08:25.097878: step 68250, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 61h:29m:23s remains)
INFO - root - 2017-12-09 23:08:33.665709: step 68260, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 61h:20m:24s remains)
INFO - root - 2017-12-09 23:08:42.323233: step 68270, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 61h:49m:57s remains)
INFO - root - 2017-12-09 23:08:50.941089: step 68280, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 62h:04m:35s remains)
INFO - root - 2017-12-09 23:08:59.156079: step 68290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:12m:39s remains)
INFO - root - 2017-12-09 23:09:07.719515: step 68300, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:15m:22s remains)
2017-12-09 23:09:08.564693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033880325 -0.0033894335 -0.0033931325 -0.0033971868 -0.0034004191 -0.0034027372 -0.0034035859 -0.003402919 -0.0034005803 -0.0033976662 -0.0033941464 -0.003390156 -0.0033862027 -0.0033831166 -0.003381226][-0.0033856086 -0.0033869043 -0.0033907832 -0.0033952978 -0.0033995365 -0.0034029037 -0.0034049235 -0.0034052795 -0.0034032827 -0.0034001961 -0.0033960838 -0.0033914782 -0.0033868235 -0.0033830162 -0.0033806188][-0.0033842896 -0.0033854363 -0.0033894014 -0.0033944165 -0.0033996208 -0.0034040625 -0.0034070078 -0.0034079864 -0.0034058825 -0.0034022443 -0.0033978252 -0.0033928102 -0.0033875224 -0.0033833485 -0.0033807168][-0.0033833608 -0.00338403 -0.0033878214 -0.0033930205 -0.0033989197 -0.0034040119 -0.0034072425 -0.0034083955 -0.0034060651 -0.00340191 -0.0033971732 -0.0033921394 -0.0033869855 -0.0033829862 -0.003380592][-0.0033825901 -0.0033827238 -0.00338613 -0.0033912302 -0.0033973225 -0.0034024846 -0.003405425 -0.0034063198 -0.0034038867 -0.003399339 -0.0033944857 -0.0033897 -0.0033855243 -0.0033822313 -0.0033802828][-0.0033818912 -0.0033815647 -0.0033844882 -0.0033892456 -0.0033949751 -0.0033995935 -0.0034023777 -0.0034029146 -0.0034006683 -0.0033959365 -0.003391081 -0.0033868114 -0.0033834612 -0.0033810802 -0.0033796751][-0.0033808791 -0.0033802611 -0.0033826667 -0.0033868924 -0.0033920966 -0.0033960645 -0.0033987346 -0.0033993367 -0.0033971663 -0.0033923793 -0.0033875077 -0.0033838181 -0.0033813198 -0.0033797543 -0.0033788879][-0.0033799536 -0.0033790539 -0.0033810711 -0.0033848167 -0.0033893532 -0.0033928452 -0.0033952438 -0.0033960526 -0.0033940773 -0.003389339 -0.0033842954 -0.0033810884 -0.0033793056 -0.0033783598 -0.0033778686][-0.0033793126 -0.0033780809 -0.0033798728 -0.00338325 -0.0033873906 -0.0033904822 -0.0033928666 -0.0033938317 -0.0033920077 -0.0033874838 -0.0033824272 -0.0033792313 -0.0033776532 -0.00337706 -0.0033768171][-0.0033787743 -0.0033773966 -0.0033791016 -0.0033822015 -0.0033862202 -0.0033891443 -0.0033916978 -0.0033930566 -0.0033915096 -0.0033872207 -0.0033821252 -0.0033786795 -0.0033768851 -0.0033762443 -0.0033760041][-0.0033789009 -0.0033774516 -0.0033792919 -0.0033822423 -0.0033860626 -0.0033891739 -0.0033920375 -0.0033936123 -0.0033924356 -0.0033885185 -0.0033834225 -0.0033795023 -0.0033772031 -0.0033761696 -0.0033758031][-0.0033793708 -0.0033780683 -0.0033798139 -0.0033826788 -0.0033862276 -0.0033894645 -0.0033926857 -0.0033944375 -0.0033936817 -0.0033901944 -0.0033853452 -0.0033811817 -0.0033784306 -0.0033769091 -0.0033762793][-0.003380239 -0.0033788676 -0.0033802574 -0.003382599 -0.00338571 -0.0033887886 -0.0033918594 -0.0033936538 -0.0033932764 -0.0033904552 -0.0033862633 -0.0033822607 -0.0033794341 -0.00337763 -0.0033768564][-0.0033811978 -0.003379565 -0.003380443 -0.0033821056 -0.0033844195 -0.0033869536 -0.0033895301 -0.0033911322 -0.0033910873 -0.0033890377 -0.0033858735 -0.003382578 -0.0033800937 -0.0033784029 -0.0033776073][-0.0033821212 -0.0033802544 -0.0033805533 -0.0033813547 -0.0033828861 -0.0033846854 -0.0033864239 -0.0033876656 -0.0033878814 -0.003386718 -0.0033846651 -0.0033822707 -0.0033803477 -0.0033789622 -0.0033782993]]...]
INFO - root - 2017-12-09 23:09:17.256470: step 68310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:36m:38s remains)
INFO - root - 2017-12-09 23:09:26.080558: step 68320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 63h:28m:46s remains)
INFO - root - 2017-12-09 23:09:34.824243: step 68330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:01m:43s remains)
INFO - root - 2017-12-09 23:09:43.598065: step 68340, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 66h:00m:09s remains)
INFO - root - 2017-12-09 23:09:52.337602: step 68350, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 65h:15m:29s remains)
INFO - root - 2017-12-09 23:10:01.011948: step 68360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:12m:27s remains)
INFO - root - 2017-12-09 23:10:09.525853: step 68370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 63h:08m:36s remains)
INFO - root - 2017-12-09 23:10:18.185836: step 68380, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:16m:47s remains)
INFO - root - 2017-12-09 23:10:26.395102: step 68390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:14m:25s remains)
INFO - root - 2017-12-09 23:10:35.017357: step 68400, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 61h:31m:37s remains)
2017-12-09 23:10:35.928467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034015598 -0.0034009556 -0.0034013269 -0.0034013696 -0.0033997861 -0.0033969744 -0.003394586 -0.0033943704 -0.0033959756 -0.0033979614 -0.0033995612 -0.0034003025 -0.0034007465 -0.0034010557 -0.0034014308][-0.0034004746 -0.0033998222 -0.0033998613 -0.0033977912 -0.0033895206 -0.003377103 -0.003368055 -0.0033694513 -0.0033792523 -0.0033897837 -0.0033963758 -0.0033985423 -0.0033990284 -0.0033992061 -0.0033994082][-0.0034011735 -0.0033999998 -0.0033951208 -0.0033798688 -0.0033472697 -0.0033074075 -0.0032846143 -0.0032947732 -0.0033298829 -0.0033662387 -0.0033891373 -0.0033974249 -0.0033990971 -0.0033993777 -0.0033995407][-0.0033994941 -0.0033965926 -0.003371805 -0.0033082196 -0.0032007403 -0.0030872286 -0.0030339104 -0.0030737298 -0.0031791162 -0.0032881214 -0.0033604621 -0.0033915497 -0.0033990806 -0.0033998911 -0.003399916][-0.0033877676 -0.0033832684 -0.0033136173 -0.0031441611 -0.0028809027 -0.0026200227 -0.0025084959 -0.0026079509 -0.0028541763 -0.0031131436 -0.0032905031 -0.0033729458 -0.0033965015 -0.0033999521 -0.0033999798][-0.0033638128 -0.0033594356 -0.0032258627 -0.0029085383 -0.0024335694 -0.0019787406 -0.001790215 -0.0019681621 -0.0024013247 -0.0028655778 -0.0031885814 -0.0033437854 -0.003390759 -0.0033991612 -0.0033999702][-0.0033372657 -0.0033376049 -0.0031543348 -0.0027224061 -0.0020858017 -0.001484331 -0.00123848 -0.0014777698 -0.0020536063 -0.0026746176 -0.0031090118 -0.0033192027 -0.003384012 -0.0033968526 -0.0033988163][-0.0033262898 -0.0033331115 -0.0031473974 -0.0027074651 -0.0020611733 -0.0014522609 -0.0012053659 -0.0014527587 -0.0020398679 -0.0026691998 -0.0031062807 -0.0033164078 -0.0033816118 -0.0033948608 -0.0033970792][-0.0033370561 -0.0033475633 -0.0032059862 -0.00286402 -0.0023559667 -0.0018751762 -0.0016839912 -0.0018880388 -0.0023574319 -0.0028481809 -0.0031800789 -0.0033363984 -0.0033846251 -0.0033952096 -0.0033970636][-0.0033603704 -0.0033704466 -0.0032892525 -0.0030840421 -0.0027720474 -0.0024726074 -0.0023562452 -0.0024926083 -0.0027894592 -0.0030860729 -0.0032773092 -0.0033638852 -0.0033906498 -0.003397115 -0.0033983935][-0.0033835925 -0.0033897199 -0.0033554032 -0.0032607238 -0.0031115257 -0.0029658871 -0.002911377 -0.0029833368 -0.0031300818 -0.0032680144 -0.0033507857 -0.003385951 -0.0033968356 -0.0033999966 -0.0034003365][-0.0033969136 -0.0033999162 -0.0033904738 -0.0033586391 -0.0033051455 -0.0032515123 -0.0032322111 -0.00326059 -0.0033144949 -0.0033615211 -0.0033871327 -0.0033972871 -0.0034005169 -0.0034017465 -0.0034015542][-0.0034016045 -0.0034024091 -0.0034016476 -0.0033951926 -0.0033824842 -0.0033688538 -0.0033638966 -0.0033710748 -0.0033842749 -0.0033947469 -0.0033996499 -0.003401309 -0.0034015598 -0.0034017623 -0.0034014431][-0.0034021426 -0.0034014315 -0.0034018422 -0.0034017095 -0.0034007258 -0.0033991858 -0.0033986112 -0.0033995742 -0.0034011202 -0.0034020522 -0.0034020832 -0.0034018853 -0.0034016313 -0.0034015442 -0.0034013267][-0.0034012771 -0.0034002874 -0.0034004145 -0.003400676 -0.003401251 -0.0034015956 -0.0034018476 -0.0034019733 -0.003401848 -0.0034014634 -0.0034010129 -0.0034008892 -0.0034010948 -0.0034011421 -0.0034011125]]...]
INFO - root - 2017-12-09 23:10:44.581928: step 68410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:47m:18s remains)
INFO - root - 2017-12-09 23:10:53.052580: step 68420, loss = 0.90, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 58h:56m:23s remains)
INFO - root - 2017-12-09 23:11:01.532116: step 68430, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 64h:06m:56s remains)
INFO - root - 2017-12-09 23:11:10.274061: step 68440, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 65h:12m:03s remains)
INFO - root - 2017-12-09 23:11:18.927866: step 68450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:19m:22s remains)
INFO - root - 2017-12-09 23:11:27.573756: step 68460, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:48m:48s remains)
INFO - root - 2017-12-09 23:11:36.204577: step 68470, loss = 0.89, batch loss = 0.68 (11.3 examples/sec; 0.711 sec/batch; 52h:07m:22s remains)
INFO - root - 2017-12-09 23:11:44.788609: step 68480, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 63h:16m:20s remains)
INFO - root - 2017-12-09 23:11:53.013732: step 68490, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.811 sec/batch; 59h:29m:32s remains)
INFO - root - 2017-12-09 23:12:01.495875: step 68500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:31m:15s remains)
2017-12-09 23:12:02.407224: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21458605 0.20371993 0.19012792 0.17579278 0.16232809 0.15070283 0.14038295 0.13189314 0.12475748 0.11837077 0.11165828 0.10417465 0.095996015 0.087658949 0.079272337][0.26193875 0.25340796 0.24079847 0.22643404 0.21210922 0.19953911 0.18768443 0.17717156 0.16741204 0.15824257 0.14850414 0.13812576 0.12724803 0.11693238 0.10707867][0.29952085 0.29570526 0.28701687 0.27545533 0.26325858 0.25157565 0.23937535 0.22719266 0.21419396 0.20073512 0.18603209 0.17128347 0.15680937 0.14390357 0.13228215][0.32377762 0.32600436 0.32320574 0.31669044 0.30912888 0.30087909 0.29073608 0.27819267 0.26245978 0.24443105 0.22383334 0.20278874 0.18247527 0.16545124 0.15102743][0.33318615 0.34078333 0.34360725 0.34322807 0.34162396 0.33784524 0.33103013 0.31955725 0.30245647 0.28053296 0.25451574 0.2274738 0.20129448 0.17926158 0.1610902][0.33251414 0.34459519 0.35136792 0.35552338 0.35885364 0.35930005 0.35618865 0.34661034 0.3294431 0.30483162 0.27423453 0.24178211 0.21005622 0.18302034 0.16078193][0.32587269 0.33987087 0.34794414 0.35469306 0.36131874 0.36524573 0.36533013 0.35815769 0.34206989 0.31618041 0.28264496 0.24587886 0.20948771 0.17791304 0.15166262][0.31759858 0.331577 0.33875212 0.34593648 0.35410619 0.35961351 0.36075348 0.3546153 0.33923805 0.31295612 0.2777662 0.23826255 0.19897604 0.16413203 0.13489127][0.31269372 0.32518083 0.32946125 0.33453712 0.34117323 0.34605059 0.34644946 0.34020624 0.32496086 0.29835108 0.26248154 0.22138824 0.18059573 0.14408742 0.11359154][0.30989909 0.31987891 0.31976679 0.32136771 0.324739 0.3262676 0.3236109 0.31569007 0.2993952 0.27230331 0.23608956 0.19520511 0.15510775 0.11918975 0.089718476][0.30635595 0.31432274 0.31026185 0.30773512 0.30662376 0.30377966 0.29695117 0.28563428 0.26694167 0.23870863 0.2021866 0.16248378 0.12437835 0.091198057 0.064840667][0.29873097 0.30290869 0.29449558 0.28741932 0.28217131 0.27588704 0.26602998 0.25224417 0.2315508 0.2025249 0.16635518 0.12867048 0.093739495 0.064581014 0.042656604][0.28737804 0.28708684 0.274359 0.26337519 0.25497577 0.24647754 0.2350516 0.21967782 0.19757682 0.16798627 0.13243583 0.097140789 0.065868653 0.041460682 0.02477538][0.26944706 0.2655603 0.25024611 0.23725168 0.22765255 0.21880803 0.20802328 0.19272847 0.17013521 0.14054199 0.10603606 0.073172763 0.045265526 0.02526404 0.013424618][0.24418737 0.23892587 0.22346844 0.21070762 0.20181081 0.19416901 0.18447243 0.16963688 0.14722615 0.11829382 0.085407853 0.055149887 0.030701851 0.014747156 0.0070138443]]...]
INFO - root - 2017-12-09 23:12:10.974609: step 68510, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 62h:37m:25s remains)
INFO - root - 2017-12-09 23:12:19.490615: step 68520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:53m:20s remains)
INFO - root - 2017-12-09 23:12:27.976578: step 68530, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 63h:39m:29s remains)
INFO - root - 2017-12-09 23:12:36.634472: step 68540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 65h:29m:57s remains)
INFO - root - 2017-12-09 23:12:45.269667: step 68550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:26m:14s remains)
INFO - root - 2017-12-09 23:12:53.914314: step 68560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:37m:39s remains)
INFO - root - 2017-12-09 23:13:02.588281: step 68570, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 63h:07m:20s remains)
INFO - root - 2017-12-09 23:13:10.970766: step 68580, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 62h:12m:02s remains)
INFO - root - 2017-12-09 23:13:19.181328: step 68590, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:27m:41s remains)
INFO - root - 2017-12-09 23:13:27.852485: step 68600, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 66h:00m:14s remains)
2017-12-09 23:13:28.718047: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052798 0.054632772 0.054725952 0.055264771 0.055133767 0.05241875 0.049207188 0.045541227 0.041851427 0.036819406 0.030841092 0.025241235 0.020202763 0.016316436 0.013693566][0.054960903 0.057548951 0.060174182 0.063802667 0.067205943 0.068261363 0.067599162 0.064526565 0.059360765 0.051647604 0.042518597 0.03420179 0.027050428 0.022054764 0.018927792][0.057904195 0.062224187 0.068289138 0.076737441 0.085492313 0.092362136 0.096277125 0.09528067 0.089449517 0.0786894 0.066073515 0.054297034 0.044424634 0.037825391 0.033706829][0.059770577 0.068699144 0.080954015 0.095620252 0.11126215 0.12566316 0.13515855 0.13710845 0.13178179 0.11959506 0.10420897 0.0883801 0.074881665 0.065415651 0.058948446][0.063752234 0.078243941 0.098302931 0.12144298 0.14544246 0.16694385 0.18138461 0.18656163 0.18190138 0.16803417 0.14973255 0.13009344 0.1126411 0.09899044 0.0888683][0.079081081 0.097931221 0.12371022 0.15348434 0.18447503 0.21246897 0.23207143 0.24018927 0.23678716 0.22233862 0.20153524 0.17781435 0.15582637 0.13749446 0.12283187][0.11076354 0.13313244 0.16207013 0.19531892 0.22951208 0.26083103 0.2833764 0.29370525 0.29126933 0.27634856 0.25345856 0.22611476 0.19956222 0.17637752 0.15696672][0.14957942 0.17601743 0.20717508 0.24166006 0.27630568 0.30723372 0.32948789 0.34006163 0.33818677 0.32299638 0.29835975 0.26750734 0.23674516 0.20851198 0.18354519][0.19467513 0.22186585 0.25203505 0.28455085 0.31655046 0.34509405 0.36534002 0.37456802 0.37151209 0.35605109 0.3301062 0.29646778 0.26165193 0.22799534 0.197942][0.24214092 0.26780951 0.29386941 0.32142913 0.34804896 0.37169638 0.38846257 0.396305 0.39207876 0.37535802 0.34741014 0.31135729 0.27280778 0.23365068 0.19849247][0.28619871 0.309185 0.329544 0.35144949 0.3728714 0.39207283 0.40574875 0.41103914 0.40489021 0.38657108 0.35617787 0.31672424 0.27369055 0.22967318 0.19007693][0.31947869 0.33973387 0.35478246 0.37161902 0.38849303 0.4042151 0.41552457 0.41900098 0.41127795 0.39076388 0.35826793 0.31614378 0.26955345 0.22145958 0.17853947][0.33856225 0.35658246 0.36667818 0.378716 0.39165422 0.40462178 0.41417176 0.41718584 0.40948987 0.3891046 0.35665807 0.31363976 0.26557025 0.21524386 0.17044331][0.34359291 0.35814703 0.36299396 0.37039912 0.37957522 0.3904134 0.39930731 0.40327168 0.39725915 0.37921691 0.34937334 0.30846995 0.26119444 0.21061121 0.16601977][0.33190551 0.34446734 0.34678435 0.35133433 0.35790682 0.36650944 0.37392029 0.3776153 0.37253976 0.3565456 0.3293916 0.29164124 0.24719507 0.19958727 0.15760481]]...]
INFO - root - 2017-12-09 23:13:37.300266: step 68610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:39m:32s remains)
INFO - root - 2017-12-09 23:13:45.822978: step 68620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:41m:33s remains)
INFO - root - 2017-12-09 23:13:54.404672: step 68630, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 61h:25m:33s remains)
INFO - root - 2017-12-09 23:14:02.788925: step 68640, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 60h:23m:19s remains)
INFO - root - 2017-12-09 23:14:11.321972: step 68650, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 61h:04m:13s remains)
INFO - root - 2017-12-09 23:14:19.859801: step 68660, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:40m:09s remains)
INFO - root - 2017-12-09 23:14:28.442105: step 68670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 63h:09m:03s remains)
INFO - root - 2017-12-09 23:14:36.868862: step 68680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:19m:28s remains)
INFO - root - 2017-12-09 23:14:45.105817: step 68690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:05m:52s remains)
INFO - root - 2017-12-09 23:14:53.635301: step 68700, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:16m:18s remains)
2017-12-09 23:14:54.498561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033908235 -0.0033891934 -0.0033893825 -0.0033908938 -0.0033927569 -0.0033941125 -0.0033942175 -0.0033922694 -0.0033887317 -0.0033831482 -0.0033764613 -0.0033699262 -0.0033643374 -0.0033604766 -0.0033582358][-0.0033912405 -0.0033896237 -0.0033905439 -0.0033929837 -0.0033955635 -0.003397448 -0.0033976168 -0.0033952272 -0.0033913017 -0.0033849978 -0.0033773577 -0.0033698138 -0.0033633737 -0.0033587185 -0.0033557364][-0.0033912545 -0.0033899629 -0.0033913227 -0.003394051 -0.0033968566 -0.003398868 -0.0033987204 -0.0033958228 -0.0033913534 -0.0033851231 -0.0033775354 -0.0033701651 -0.0033638461 -0.003359026 -0.0033557026][-0.0033906524 -0.0033899108 -0.0033915436 -0.0033939786 -0.0033960324 -0.0033972082 -0.0033961553 -0.0033927006 -0.0033880975 -0.0033821689 -0.0033752371 -0.0033688324 -0.0033633406 -0.0033588719 -0.0033555392][-0.0033898142 -0.0033892267 -0.0033903089 -0.0033918524 -0.0033928521 -0.0033929651 -0.0033909143 -0.0033872323 -0.0033828428 -0.0033775496 -0.0033717183 -0.0033665823 -0.0033620948 -0.0033583094 -0.0033552919][-0.0033898721 -0.0033890526 -0.0033893252 -0.0033899348 -0.0033895085 -0.0033881909 -0.0033853939 -0.0033817298 -0.003377557 -0.0033729721 -0.0033684713 -0.0033646559 -0.0033609613 -0.0033576337 -0.0033550349][-0.0033877098 -0.0033869944 -0.0033871105 -0.0033869937 -0.0033854763 -0.003383128 -0.0033800118 -0.003376733 -0.0033732925 -0.0033696927 -0.0033663879 -0.0033636589 -0.0033603818 -0.003357159 -0.0033548595][-0.0033820169 -0.0033814616 -0.0033819261 -0.0033817855 -0.0033802779 -0.0033779764 -0.0033753715 -0.0033732515 -0.0033708946 -0.0033682312 -0.0033655597 -0.0033633274 -0.0033601569 -0.0033569967 -0.0033547624][-0.0033746175 -0.0033736897 -0.0033743808 -0.0033744078 -0.0033733491 -0.003371784 -0.003370265 -0.0033693814 -0.003368343 -0.0033668051 -0.003365071 -0.0033630705 -0.0033599318 -0.0033569413 -0.0033547992][-0.0033665132 -0.00336447 -0.0033652298 -0.0033656363 -0.0033653444 -0.0033647148 -0.0033642652 -0.0033643954 -0.003364238 -0.0033636619 -0.0033628196 -0.0033613387 -0.0033588437 -0.0033564349 -0.0033546009][-0.0033608228 -0.0033580146 -0.0033589865 -0.0033595092 -0.0033595879 -0.0033596426 -0.0033597152 -0.0033597988 -0.0033596673 -0.0033594037 -0.0033589781 -0.0033581075 -0.0033566104 -0.0033551462 -0.0033539715][-0.0033574968 -0.0033541638 -0.0033551417 -0.0033556421 -0.0033558828 -0.0033560146 -0.0033556989 -0.0033552558 -0.0033549196 -0.0033548607 -0.0033548309 -0.0033547643 -0.0033543629 -0.0033538127 -0.0033531929][-0.0033561909 -0.003352301 -0.0033529578 -0.0033533457 -0.0033533424 -0.0033531955 -0.0033525543 -0.0033518842 -0.0033514635 -0.0033514649 -0.0033517936 -0.0033521384 -0.003352443 -0.0033526095 -0.0033523887][-0.0033557396 -0.0033516078 -0.0033518611 -0.003351877 -0.0033516455 -0.0033512837 -0.0033505869 -0.003349893 -0.0033496176 -0.003349917 -0.0033504642 -0.0033510074 -0.0033515757 -0.0033520001 -0.0033520691][-0.0033560244 -0.00335184 -0.0033516474 -0.0033514826 -0.003351189 -0.0033509519 -0.0033504157 -0.0033497773 -0.0033497093 -0.003350097 -0.0033506055 -0.0033510076 -0.003351463 -0.0033518961 -0.0033520136]]...]
INFO - root - 2017-12-09 23:15:03.086476: step 68710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:20m:16s remains)
INFO - root - 2017-12-09 23:15:11.736117: step 68720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:52m:53s remains)
INFO - root - 2017-12-09 23:15:20.429304: step 68730, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 65h:08m:40s remains)
INFO - root - 2017-12-09 23:15:28.804930: step 68740, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.840 sec/batch; 61h:34m:25s remains)
INFO - root - 2017-12-09 23:15:37.301236: step 68750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:09m:46s remains)
INFO - root - 2017-12-09 23:15:45.937993: step 68760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 65h:22m:23s remains)
INFO - root - 2017-12-09 23:15:54.631130: step 68770, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:14m:13s remains)
INFO - root - 2017-12-09 23:16:03.160628: step 68780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:28m:27s remains)
INFO - root - 2017-12-09 23:16:11.580279: step 68790, loss = 0.89, batch loss = 0.68 (11.5 examples/sec; 0.698 sec/batch; 51h:09m:52s remains)
INFO - root - 2017-12-09 23:16:19.974176: step 68800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:41m:58s remains)
2017-12-09 23:16:20.917020: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04842582 0.053524122 0.058151059 0.061935224 0.066197373 0.070940189 0.074343488 0.076244079 0.076522514 0.076436006 0.075801022 0.074717388 0.0728318 0.070714176 0.068506889][0.055519652 0.059967123 0.063993677 0.06818565 0.073072262 0.078558452 0.082743421 0.086094178 0.087557159 0.088028565 0.087744392 0.086826861 0.084724575 0.081729613 0.078000978][0.056951392 0.059962481 0.062735036 0.065930247 0.070072882 0.07505127 0.079924069 0.083973125 0.086172573 0.088044778 0.089099 0.089268573 0.087866187 0.085118636 0.0813753][0.056246076 0.058479339 0.060311593 0.062807843 0.066286862 0.07057815 0.074951552 0.07884042 0.082165107 0.084913604 0.086941026 0.087836608 0.087687328 0.085947983 0.083043836][0.05298293 0.054414295 0.055780638 0.057710089 0.060400672 0.064311936 0.068651877 0.0727361 0.076648436 0.079817958 0.08268901 0.084841564 0.085778587 0.08484669 0.083055437][0.046976864 0.047248594 0.047145542 0.048284583 0.050685469 0.054192185 0.058550864 0.063646086 0.069228567 0.073733337 0.077861942 0.080904908 0.082525991 0.082639627 0.081287079][0.03694446 0.036331628 0.035190929 0.035066258 0.036237665 0.03956702 0.044798598 0.0511576 0.058363646 0.064863019 0.070929445 0.07519225 0.07737346 0.077594705 0.075860731][0.02444892 0.023251962 0.021619596 0.021110781 0.021782188 0.024421252 0.029358236 0.036573697 0.044752628 0.052497104 0.05964683 0.064632721 0.067537367 0.067773692 0.0656843][0.011388324 0.010703836 0.0097714746 0.0093219653 0.0097429818 0.011867121 0.01606575 0.022343837 0.029585173 0.037078906 0.044041954 0.049185403 0.052160755 0.0523039 0.0503497][0.0020381955 0.0019215054 0.0018030175 0.0018346312 0.0022298463 0.0034906359 0.0061959904 0.010421102 0.015575826 0.021088853 0.026413409 0.03070352 0.033179227 0.033586994 0.032036696][-0.0022844074 -0.0021108622 -0.0018823288 -0.0015784111 -0.00117869 -0.00056862831 0.00066701486 0.0025738662 0.0051571121 0.0081800921 0.01132535 0.014050778 0.015631486 0.01589047 0.014963746][-0.0033857585 -0.003362692 -0.0032490129 -0.0030701493 -0.0028700428 -0.0026257711 -0.0021969471 -0.0015852768 -0.00068306783 0.00048849755 0.0018303005 0.0030377849 0.003756027 0.0038749927 0.003420817][-0.0033934072 -0.0033898293 -0.0033880472 -0.0033905639 -0.0033929867 -0.003388277 -0.0033557317 -0.0032639457 -0.0030716329 -0.0027673489 -0.002393859 -0.002037884 -0.001808351 -0.0017649407 -0.0019017207][-0.0033978689 -0.0033949297 -0.0033932619 -0.0033950973 -0.0033962645 -0.0033966915 -0.0033964762 -0.0033960936 -0.0033934216 -0.003392566 -0.0033929383 -0.0033924605 -0.0033917774 -0.003392424 -0.0033937178][-0.0034006683 -0.0033983339 -0.003397844 -0.00339839 -0.0033993025 -0.0033986762 -0.0033978771 -0.0033968508 -0.0033941641 -0.0033936664 -0.0033941653 -0.0033951553 -0.0033947965 -0.0033944689 -0.0033956903]]...]
INFO - root - 2017-12-09 23:16:29.435519: step 68810, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:49m:54s remains)
INFO - root - 2017-12-09 23:16:38.083451: step 68820, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 62h:49m:24s remains)
INFO - root - 2017-12-09 23:16:46.805230: step 68830, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:34m:38s remains)
INFO - root - 2017-12-09 23:16:55.439326: step 68840, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:26m:53s remains)
INFO - root - 2017-12-09 23:17:03.844065: step 68850, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 60h:03m:56s remains)
INFO - root - 2017-12-09 23:17:12.430943: step 68860, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:14m:08s remains)
INFO - root - 2017-12-09 23:17:21.114014: step 68870, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 64h:53m:09s remains)
INFO - root - 2017-12-09 23:17:29.565357: step 68880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:17m:24s remains)
INFO - root - 2017-12-09 23:17:38.142666: step 68890, loss = 0.89, batch loss = 0.69 (11.0 examples/sec; 0.730 sec/batch; 53h:29m:24s remains)
INFO - root - 2017-12-09 23:17:46.710053: step 68900, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 62h:53m:08s remains)
2017-12-09 23:17:47.624633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033982019 -0.0033973316 -0.0033974333 -0.0033973637 -0.0033971667 -0.0033969029 -0.0033967711 -0.0033968557 -0.003397075 -0.0033972091 -0.0033972829 -0.0033972971 -0.0033973288 -0.0033973756 -0.0033974699][-0.0033967015 -0.0033961188 -0.003396332 -0.0033962983 -0.0033961439 -0.0033956375 -0.0033951018 -0.0033949406 -0.0033950405 -0.0033951581 -0.0033952915 -0.003395427 -0.0033955651 -0.0033956666 -0.0033958375][-0.0033967565 -0.0033966268 -0.0033971753 -0.0033973067 -0.0033970515 -0.0033961842 -0.0033951961 -0.0033944417 -0.0033940836 -0.0033941555 -0.00339436 -0.003394668 -0.0033950144 -0.0033952894 -0.0033955134][-0.0033966473 -0.0033970259 -0.0033981823 -0.0033987185 -0.0033984692 -0.003397217 -0.003395478 -0.0033938664 -0.0033928575 -0.0033927504 -0.0033929853 -0.0033935511 -0.0033941085 -0.0033945066 -0.0033948261][-0.0033963753 -0.0033972024 -0.0033989316 -0.0033997816 -0.0033990121 -0.0033967544 -0.003393735 -0.0033911313 -0.0033898219 -0.0033901019 -0.0033910591 -0.003392074 -0.0033927932 -0.0033934368 -0.0033939215][-0.0033959788 -0.0033971767 -0.0033991223 -0.0033993134 -0.0033965991 -0.0033915408 -0.0033863632 -0.0033829645 -0.0033828311 -0.0033852956 -0.003388 -0.0033898831 -0.0033910277 -0.0033920452 -0.003392783][-0.0033952526 -0.0033966929 -0.003398638 -0.0033975507 -0.0033916933 -0.0033824632 -0.0033739363 -0.0033695973 -0.0033714052 -0.0033775046 -0.0033833589 -0.0033869271 -0.0033890188 -0.003390556 -0.0033914656][-0.0033944962 -0.0033960005 -0.0033980359 -0.0033964412 -0.0033885431 -0.0033750667 -0.0033627735 -0.003357304 -0.003360291 -0.0033691134 -0.00337827 -0.003384094 -0.0033873869 -0.0033892521 -0.0033901392][-0.0033941204 -0.0033956165 -0.0033979272 -0.0033971341 -0.003390149 -0.0033764029 -0.003361539 -0.0033545352 -0.0033570922 -0.0033661481 -0.0033763421 -0.0033832677 -0.0033871075 -0.0033888128 -0.0033894379][-0.003393841 -0.0033953292 -0.0033977267 -0.0033984848 -0.0033947814 -0.0033851801 -0.0033720452 -0.0033635495 -0.0033636515 -0.0033700168 -0.0033784423 -0.0033846614 -0.0033880288 -0.0033891627 -0.0033893411][-0.0033934114 -0.0033948191 -0.0033974661 -0.0033990915 -0.0033984608 -0.0033941171 -0.0033861247 -0.0033788539 -0.0033766155 -0.0033788488 -0.0033835738 -0.0033874535 -0.0033894377 -0.0033897434 -0.0033894449][-0.0033930517 -0.0033941574 -0.0033968445 -0.0033989325 -0.0033998361 -0.003399024 -0.0033957306 -0.0033913264 -0.00338834 -0.0033877923 -0.0033890044 -0.0033904754 -0.0033909623 -0.0033905255 -0.003389966][-0.0033930335 -0.0033937024 -0.0033960461 -0.0033981325 -0.003399489 -0.0034000722 -0.0033992776 -0.0033971476 -0.0033949346 -0.0033933469 -0.0033927218 -0.0033923395 -0.0033918233 -0.0033910808 -0.0033904719][-0.0033928654 -0.0033930948 -0.0033949446 -0.0033964859 -0.0033977944 -0.0033986575 -0.003398844 -0.0033980056 -0.0033965588 -0.0033952256 -0.0033939588 -0.0033928393 -0.0033919935 -0.0033913546 -0.0033908966][-0.0033926447 -0.0033925227 -0.0033938298 -0.0033947343 -0.0033955756 -0.003396278 -0.00339677 -0.0033964922 -0.0033957339 -0.0033947984 -0.0033937029 -0.0033926268 -0.0033919229 -0.0033914747 -0.0033911858]]...]
INFO - root - 2017-12-09 23:17:56.216189: step 68910, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:59m:32s remains)
INFO - root - 2017-12-09 23:18:04.836955: step 68920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:55m:38s remains)
INFO - root - 2017-12-09 23:18:13.510059: step 68930, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 64h:08m:59s remains)
INFO - root - 2017-12-09 23:18:22.120777: step 68940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:10m:48s remains)
INFO - root - 2017-12-09 23:18:30.861744: step 68950, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 64h:04m:57s remains)
INFO - root - 2017-12-09 23:18:39.453002: step 68960, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:30m:22s remains)
INFO - root - 2017-12-09 23:18:48.262176: step 68970, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 64h:20m:30s remains)
INFO - root - 2017-12-09 23:18:56.776997: step 68980, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:19m:37s remains)
INFO - root - 2017-12-09 23:19:05.264906: step 68990, loss = 0.89, batch loss = 0.68 (11.0 examples/sec; 0.725 sec/batch; 53h:05m:12s remains)
INFO - root - 2017-12-09 23:19:13.652063: step 69000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:22m:22s remains)
2017-12-09 23:19:14.504948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034050019 -0.0034035451 -0.0034036099 -0.0034035938 -0.0034034208 -0.0034027738 -0.0034013863 -0.0034005083 -0.003400774 -0.0034019642 -0.0034028478 -0.0034031258 -0.0034031982 -0.003403344 -0.0034035346][-0.0034026129 -0.0034011649 -0.0034014368 -0.0034015481 -0.0034008289 -0.00339807 -0.0033932608 -0.0033901203 -0.0033913862 -0.003395922 -0.0033997218 -0.0034010261 -0.00340118 -0.0034011928 -0.0034011556][-0.003402944 -0.0034017724 -0.0034019798 -0.0034008522 -0.0033963737 -0.0033860644 -0.0033726564 -0.0033653986 -0.0033706692 -0.0033840393 -0.0033954144 -0.0033999905 -0.0034009817 -0.0034008957 -0.0034009388][-0.0034035065 -0.0034023735 -0.003400065 -0.003389972 -0.0033652689 -0.0033264698 -0.0032896695 -0.0032793046 -0.00330392 -0.003345856 -0.0033801482 -0.003396495 -0.0034009928 -0.0034012441 -0.003401235][-0.0034040934 -0.0034019502 -0.0033920596 -0.0033558847 -0.0032769032 -0.0031653412 -0.0030721731 -0.0030577886 -0.0031329207 -0.0032463407 -0.0033375022 -0.0033837499 -0.0033988745 -0.0034014764 -0.003401825][-0.0034046085 -0.0034003195 -0.0033759004 -0.0032934232 -0.0031205676 -0.00288531 -0.0026972543 -0.0026729647 -0.0028297245 -0.0030646715 -0.0032572837 -0.0033588205 -0.0033938426 -0.0034007016 -0.0034016711][-0.0034046927 -0.0033976091 -0.0033561322 -0.0032215554 -0.0029455135 -0.0025744871 -0.0022792122 -0.0022364827 -0.002476986 -0.0028478426 -0.0031588273 -0.0033271532 -0.0033867336 -0.0033992282 -0.003400998][-0.0034033672 -0.0033933048 -0.0033408944 -0.0031738465 -0.0028336034 -0.0023771108 -0.0020114225 -0.0019498892 -0.00223761 -0.0026974129 -0.0030886217 -0.0033039581 -0.0033812001 -0.0033975241 -0.0033995025][-0.0034022 -0.0033897427 -0.003336835 -0.0031738384 -0.0028430028 -0.0023963209 -0.0020332448 -0.0019687787 -0.0022504008 -0.0027039556 -0.0030909483 -0.0033043798 -0.0033808607 -0.0033972932 -0.0033989209][-0.0034024986 -0.0033903243 -0.0033471335 -0.0032209759 -0.0029663139 -0.0026166837 -0.0023273388 -0.00227874 -0.002506644 -0.0028654106 -0.0031652893 -0.003328179 -0.0033859632 -0.0033985591 -0.003399333][-0.0034042846 -0.00339528 -0.0033674436 -0.0032904882 -0.0031354919 -0.0029202751 -0.0027410137 -0.0027151508 -0.0028628563 -0.0030861115 -0.0032658475 -0.0033606112 -0.0033932028 -0.0033998778 -0.0033997309][-0.0034052192 -0.0034009777 -0.0033884246 -0.0033528318 -0.0032801079 -0.0031795523 -0.0030956655 -0.0030858584 -0.0031586606 -0.0032637536 -0.0033450711 -0.003385873 -0.0033991351 -0.0034011854 -0.003400309][-0.00340464 -0.0034032275 -0.0034002669 -0.0033893322 -0.0033647846 -0.0033304596 -0.0033019548 -0.0032997 -0.0033249701 -0.0033598891 -0.0033862563 -0.0033982832 -0.0034012236 -0.0034008422 -0.0034000985][-0.0034033544 -0.003402546 -0.0034032736 -0.0034020147 -0.0033972445 -0.0033897129 -0.0033830879 -0.0033821522 -0.0033874966 -0.0033949427 -0.0034003081 -0.0034020818 -0.0034016881 -0.0034006259 -0.0033999381][-0.0034018718 -0.0034009134 -0.0034018003 -0.0034027633 -0.0034026797 -0.0034020147 -0.0034013242 -0.00340072 -0.0034006804 -0.00340148 -0.0034021912 -0.0034021179 -0.0034012904 -0.0034003619 -0.0033999535]]...]
INFO - root - 2017-12-09 23:19:23.124915: step 69010, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 61h:54m:45s remains)
INFO - root - 2017-12-09 23:19:31.728402: step 69020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:17m:03s remains)
INFO - root - 2017-12-09 23:19:40.336439: step 69030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:55m:22s remains)
INFO - root - 2017-12-09 23:19:48.856461: step 69040, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 64h:39m:30s remains)
INFO - root - 2017-12-09 23:19:57.685704: step 69050, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:10m:55s remains)
INFO - root - 2017-12-09 23:20:06.543727: step 69060, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:56m:02s remains)
INFO - root - 2017-12-09 23:20:15.184002: step 69070, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 62h:54m:52s remains)
INFO - root - 2017-12-09 23:20:23.725415: step 69080, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:23m:18s remains)
INFO - root - 2017-12-09 23:20:32.082753: step 69090, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 55h:53m:22s remains)
INFO - root - 2017-12-09 23:20:40.621131: step 69100, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 60h:21m:04s remains)
2017-12-09 23:20:41.501626: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19609705 0.20150252 0.20481347 0.20368285 0.19856191 0.18883826 0.17464428 0.15530975 0.13100234 0.10682672 0.08411061 0.065153733 0.051173288 0.043329146 0.0388007][0.24150597 0.25768769 0.27145132 0.27791092 0.27766716 0.26977685 0.25391284 0.23050784 0.19928427 0.16592154 0.13314608 0.10440518 0.082425274 0.068359405 0.059226453][0.29499072 0.32074827 0.34382427 0.35707203 0.3607356 0.35477808 0.33743337 0.310372 0.27412653 0.23482595 0.19489852 0.15841353 0.12965542 0.10955727 0.095681511][0.35508135 0.38758954 0.41743758 0.43664962 0.44322047 0.43749541 0.41882825 0.38803166 0.34718916 0.30295923 0.25784853 0.21645766 0.1833922 0.16003932 0.1440827][0.41381294 0.44951478 0.48199391 0.50381261 0.51221234 0.50703883 0.48815259 0.45700085 0.41514358 0.36927682 0.32132268 0.276895 0.24048094 0.21413919 0.19625625][0.46244127 0.49781525 0.52944881 0.55193734 0.56094646 0.556977 0.54023588 0.51173276 0.47292092 0.42924941 0.3833977 0.33923182 0.30147663 0.27272487 0.2520394][0.49387446 0.52714932 0.55514419 0.57620394 0.58592629 0.583543 0.57064348 0.54674697 0.51276678 0.47362685 0.43204519 0.39160237 0.35616076 0.3275781 0.30614141][0.51344758 0.54269242 0.56538212 0.58407319 0.59326351 0.59232378 0.58281982 0.56325936 0.53467524 0.50102073 0.4655973 0.43022603 0.3982864 0.37173659 0.35080469][0.52672297 0.55248648 0.56947815 0.58489341 0.59353983 0.59391093 0.58768755 0.57297164 0.55015945 0.52093756 0.48975372 0.45858666 0.42955297 0.40480387 0.38395187][0.53116822 0.55598259 0.56877804 0.58103192 0.58923411 0.59134054 0.58793527 0.57722366 0.55884576 0.53378344 0.50570452 0.47662002 0.44889849 0.42407012 0.40278277][0.52321088 0.54838896 0.55894595 0.569461 0.57781649 0.58219433 0.5823372 0.57493758 0.56018543 0.53813916 0.512476 0.48467225 0.45736837 0.43238592 0.4104515][0.49808502 0.5239321 0.533679 0.54372406 0.55314547 0.55936921 0.56220233 0.558084 0.54709953 0.52816331 0.50494081 0.47927764 0.45346174 0.42973304 0.40860981][0.4615199 0.48604137 0.49444082 0.50321 0.51214081 0.51915836 0.52290344 0.520732 0.5127508 0.49761689 0.47840703 0.45627454 0.43351665 0.41249743 0.39356142][0.41330054 0.435344 0.44160715 0.4487707 0.4570705 0.4639568 0.4678663 0.46686283 0.46076033 0.4482041 0.43135443 0.41244519 0.39322919 0.37534419 0.35954484][0.35111988 0.36951697 0.37373558 0.37889105 0.38561639 0.39156044 0.39540654 0.39540073 0.390989 0.3808271 0.36672863 0.35086322 0.3344267 0.31904706 0.30611733]]...]
INFO - root - 2017-12-09 23:20:50.055175: step 69110, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:41m:43s remains)
INFO - root - 2017-12-09 23:20:58.670545: step 69120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:20m:09s remains)
INFO - root - 2017-12-09 23:21:07.276460: step 69130, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 63h:32m:51s remains)
INFO - root - 2017-12-09 23:21:15.955962: step 69140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:07m:40s remains)
INFO - root - 2017-12-09 23:21:24.536819: step 69150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:00m:11s remains)
INFO - root - 2017-12-09 23:21:33.171151: step 69160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:12m:13s remains)
INFO - root - 2017-12-09 23:21:41.700429: step 69170, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 62h:16m:16s remains)
INFO - root - 2017-12-09 23:21:50.105740: step 69180, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:33m:13s remains)
INFO - root - 2017-12-09 23:21:58.738165: step 69190, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.706 sec/batch; 51h:36m:40s remains)
INFO - root - 2017-12-09 23:22:07.009938: step 69200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:41m:09s remains)
2017-12-09 23:22:07.945753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0029336798 -0.0015671956 0.0014657483 0.0062760608 0.012649702 0.019909823 0.027067984 0.032759763 0.036150657 0.0374263 0.037240572 0.035875693 0.032995846 0.028474348 0.022585817][-0.0026635253 -0.00060731289 0.0037971281 0.010924128 0.020634672 0.031910948 0.043276723 0.052721888 0.05906868 0.062294945 0.062939085 0.061444595 0.057339832 0.050342917 0.040767271][-0.0024374253 0.00036145817 0.0064033363 0.016403329 0.030216848 0.046703152 0.063547358 0.07767912 0.087289535 0.092068911 0.092811346 0.090207182 0.084084116 0.074030839 0.060495339][-0.0024515493 0.00064270082 0.007739665 0.020116271 0.037736218 0.059178114 0.081692137 0.10127654 0.1150712 0.12168925 0.12203908 0.11741275 0.1081711 0.094187669 0.076225564][-0.0027724952 -0.00012745056 0.0068942513 0.020114888 0.039775327 0.064486004 0.091192916 0.11521137 0.13246316 0.14054982 0.140255 0.13328771 0.12096339 0.1036174 0.082641631][-0.0030715291 -0.0013101841 0.0044596447 0.016641546 0.036110006 0.061380126 0.089627795 0.11582206 0.13514489 0.14398302 0.1423596 0.13264504 0.11705274 0.097263962 0.075151227][-0.0031577223 -0.0021994913 0.0018733067 0.011629598 0.028536147 0.051456176 0.077687934 0.10283551 0.1218387 0.13056311 0.12809667 0.11668424 0.09936934 0.078901514 0.057917655][-0.00314546 -0.0026409337 -0.000137344 0.0065306993 0.019180508 0.037292734 0.058563672 0.079195246 0.0951003 0.10258859 0.10004896 0.089085542 0.07283745 0.05481267 0.037648626][-0.0031512592 -0.002909929 -0.0016545979 0.0020614297 0.00977733 0.021592278 0.035917226 0.049941663 0.060764618 0.065872468 0.063869156 0.055495672 0.043231506 0.030323459 0.018973129][-0.003291253 -0.0031888464 -0.002732621 -0.0011907788 0.0024250245 0.0084202811 0.015967254 0.023363413 0.028957797 0.03145558 0.03009527 0.025139932 0.01809345 0.011089685 0.0054977424][-0.0033702175 -0.0033484425 -0.0032729907 -0.002880502 -0.0017240624 0.00044953916 0.0033245424 0.0061041443 0.0081161875 0.0089063179 0.0082078706 0.0060800547 0.0031792948 0.00051891035 -0.0013352663][-0.0033772066 -0.0033753067 -0.0033737209 -0.0033487028 -0.0031643021 -0.0027024217 -0.0020255903 -0.0013998235 -0.0010051467 -0.00086291041 -0.0010361075 -0.0015500453 -0.0022540924 -0.0028383723 -0.0031544892][-0.0033735724 -0.003371746 -0.0033715975 -0.0033704846 -0.003363726 -0.0033414918 -0.003303864 -0.0032693611 -0.0032479316 -0.0032347895 -0.0032312889 -0.0032465165 -0.0032752855 -0.0032950651 -0.0032983818][-0.0033707377 -0.00336889 -0.0033691558 -0.0033694177 -0.0033692883 -0.0033672659 -0.0033642789 -0.0033608109 -0.0033562623 -0.0033502735 -0.0033442935 -0.0033391709 -0.0033332126 -0.0033261084 -0.0033187929][-0.0033692706 -0.0033668617 -0.0033669774 -0.0033675123 -0.003368139 -0.0033671972 -0.0033648515 -0.0033624985 -0.0033604056 -0.0033584926 -0.0033569892 -0.0033558526 -0.0033534025 -0.0033479673 -0.0033402282]]...]
INFO - root - 2017-12-09 23:22:16.490030: step 69210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:10m:54s remains)
INFO - root - 2017-12-09 23:22:24.983180: step 69220, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 62h:24m:39s remains)
INFO - root - 2017-12-09 23:22:33.588176: step 69230, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:53m:44s remains)
INFO - root - 2017-12-09 23:22:42.303396: step 69240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:01m:13s remains)
INFO - root - 2017-12-09 23:22:50.953443: step 69250, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 64h:03m:17s remains)
INFO - root - 2017-12-09 23:22:59.641521: step 69260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:41m:52s remains)
INFO - root - 2017-12-09 23:23:08.552348: step 69270, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:52m:05s remains)
INFO - root - 2017-12-09 23:23:16.988663: step 69280, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:15m:33s remains)
INFO - root - 2017-12-09 23:23:25.454745: step 69290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 61h:16m:25s remains)
INFO - root - 2017-12-09 23:23:33.932741: step 69300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:23m:20s remains)
2017-12-09 23:23:34.860691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020738263 -0.0020986348 -0.0021135909 -0.0021238034 -0.0021313601 -0.0021493579 -0.0021751295 -0.0021654645 -0.0021443092 -0.002059794 -0.0019460609 -0.0017732284 -0.0016562595 -0.001509626 -0.0013348376][-0.0020212654 -0.0019097709 -0.0017654723 -0.0015895015 -0.0013763658 -0.0011499091 -0.00097671663 -0.00091305329 -0.00089520891 -0.00086868065 -0.00083762524 -0.00068595796 -0.00059362431 -0.00027410476 6.234087e-05][-0.00094606262 -0.00036479789 0.00019022729 0.00073473505 0.0013314241 0.0019237685 0.0023537276 0.0025549426 0.0026454509 0.0026568968 0.0025614898 0.0025711751 0.0025344372 0.0028648728 0.0032940048][0.0016311773 0.0026080462 0.0034617202 0.0042177234 0.0049872138 0.0057528978 0.0063563595 0.006710927 0.0068642534 0.0068603158 0.0068431767 0.0067613609 0.0066776881 0.0068189381 0.0072885621][0.0050007692 0.0060365004 0.0068184128 0.0073958933 0.00794168 0.0084927762 0.0089805564 0.0093435673 0.0094865318 0.0096075591 0.0097003654 0.0097029675 0.0095973667 0.0096399207 0.0098919664][0.0076421928 0.00828074 0.0085642273 0.0086360862 0.0086827883 0.0087904371 0.0089643123 0.0091476738 0.009334594 0.0095527982 0.0097365724 0.0099478252 0.010002275 0.010077523 0.010167488][0.0083977217 0.00840591 0.0080622137 0.007651642 0.0073070144 0.0070666848 0.0069922833 0.0070410836 0.0071901334 0.0074754944 0.0078278091 0.0081397332 0.0083833579 0.0084549868 0.0084318127][0.0069917841 0.0064391466 0.0056405896 0.0049468493 0.0044297706 0.0040533608 0.0038728307 0.0038621926 0.003990477 0.004244063 0.0046106419 0.0050047357 0.0053302208 0.0054086503 0.0053057987][0.0038988322 0.0030322156 0.0020499884 0.0012941777 0.00077247689 0.00044086855 0.00029599015 0.00029833568 0.00039636274 0.00055116252 0.00074846693 0.000979604 0.0011502441 0.0011973169 0.0010584237][0.00033433991 -0.00042089494 -0.0011901204 -0.0017517192 -0.0021049443 -0.0022945292 -0.0023675279 -0.002353359 -0.0023004597 -0.0022290754 -0.0021461255 -0.002059628 -0.0020050681 -0.0019955959 -0.0020414926][-0.0021687425 -0.0025423821 -0.002894839 -0.0031293691 -0.0032523498 -0.003300576 -0.0033088487 -0.0032971329 -0.0032836578 -0.0032745616 -0.0032711902 -0.0032725846 -0.0032834252 -0.0032998463 -0.0033195761][-0.0031964364 -0.0032754634 -0.00333926 -0.0033704017 -0.0033779352 -0.0033780497 -0.0033775307 -0.003377131 -0.0033768271 -0.0033767782 -0.0033768946 -0.0033758143 -0.00337477 -0.0033735007 -0.0033726308][-0.0033782818 -0.0033770485 -0.0033777044 -0.0033774879 -0.0033770355 -0.0033768909 -0.0033765489 -0.0033762178 -0.0033758716 -0.0033755521 -0.0033754061 -0.0033742036 -0.003373197 -0.0033720003 -0.003371167][-0.0033793668 -0.0033766932 -0.0033766946 -0.0033765123 -0.0033761561 -0.0033762171 -0.0033762315 -0.0033758851 -0.0033752017 -0.003374734 -0.003374168 -0.003373008 -0.0033716592 -0.003370672 -0.0033703286][-0.0033781864 -0.0033751798 -0.0033748366 -0.0033753098 -0.0033756953 -0.0033763626 -0.0033768204 -0.0033770509 -0.0033767603 -0.0033755803 -0.003374452 -0.0033725554 -0.0033705716 -0.0033690692 -0.0033686]]...]
INFO - root - 2017-12-09 23:23:43.336266: step 69310, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 59h:14m:38s remains)
INFO - root - 2017-12-09 23:23:51.815987: step 69320, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:03m:36s remains)
INFO - root - 2017-12-09 23:24:00.736639: step 69330, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 65h:30m:22s remains)
INFO - root - 2017-12-09 23:24:09.420113: step 69340, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:31m:17s remains)
INFO - root - 2017-12-09 23:24:18.252618: step 69350, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:31m:18s remains)
INFO - root - 2017-12-09 23:24:26.940703: step 69360, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:16m:50s remains)
INFO - root - 2017-12-09 23:24:35.600259: step 69370, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 65h:10m:15s remains)
INFO - root - 2017-12-09 23:24:44.244165: step 69380, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 66h:05m:47s remains)
INFO - root - 2017-12-09 23:24:53.015919: step 69390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:15m:04s remains)
INFO - root - 2017-12-09 23:25:01.348737: step 69400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:26m:07s remains)
2017-12-09 23:25:02.319514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033830937 -0.003377283 -0.0033812039 -0.0033875641 -0.003391295 -0.0033816844 -0.003357454 -0.0033147051 -0.0032304982 -0.0030233122 -0.002683463 -0.0021998333 -0.0013054407 0.000780429 0.004926322][-0.0033624589 -0.0033471168 -0.0033491524 -0.0033603278 -0.003368366 -0.0033447223 -0.0032798934 -0.0031816319 -0.003079392 -0.0029458106 -0.002754526 -0.0023805692 -0.0014342638 0.00086979312 0.0052956073][-0.0033304393 -0.0032995918 -0.0032897675 -0.0032955946 -0.0032970197 -0.0032329173 -0.0030649332 -0.0028309175 -0.0026601204 -0.0026189343 -0.0026455771 -0.0024691341 -0.001557635 0.00087187695 0.0054616248][-0.0032842902 -0.0032301871 -0.0031950255 -0.0031730328 -0.0031255011 -0.0029365022 -0.0025352761 -0.0020300685 -0.0017140522 -0.0017893289 -0.0021384957 -0.0022861022 -0.0015211845 0.00093922461 0.0056182137][-0.0032271803 -0.0031435124 -0.0030681167 -0.0029834975 -0.002806393 -0.0023423373 -0.0015074298 -0.000553675 -2.3754081e-05 -0.00026608002 -0.0010822425 -0.0017237972 -0.001283254 0.0010875834 0.0057920022][-0.0031726758 -0.0030642545 -0.0029478737 -0.0027664048 -0.0023558321 -0.0014519237 -8.8100787e-06 0.0015058497 0.002262654 0.0017879738 0.00039507612 -0.00085311569 -0.00085910526 0.0013334395 0.0060236752][-0.0031443068 -0.0030248181 -0.0028796869 -0.002594189 -0.0019094822 -0.00052101887 0.0015226596 0.0035249384 0.0044346619 0.0037202633 0.0018169812 3.5239616e-05 -0.00038226717 0.0016301454 0.0063181734][-0.0031567267 -0.003048125 -0.0028998083 -0.0025456534 -0.001669634 2.131681e-05 0.0023723433 0.0045607472 0.0054774256 0.0046374695 0.0025498776 0.00058005261 -1.724693e-05 0.001890491 0.0066181989][-0.0032080228 -0.0031263398 -0.0030038324 -0.0026569478 -0.0017796893 -0.00014372682 0.002043192 0.0040147249 0.0048035309 0.004045262 0.0022217256 0.000519878 4.4769607e-05 0.001935025 0.0067066448][-0.0032655352 -0.0032202848 -0.0031409259 -0.0028786981 -0.002197348 -0.000950217 0.00068053394 0.0021345362 0.0027271293 0.0022453999 0.001036132 -6.3009327e-05 -0.00019169133 0.0017400871 0.0064897276][-0.0033094634 -0.0032976831 -0.0032657124 -0.0031184154 -0.0027134269 -0.0019773603 -0.0010174084 -0.00013260124 0.00029063178 0.00015013106 -0.00038020196 -0.00082660303 -0.000580641 0.0013723818 0.0059668031][-0.0033341597 -0.0033402394 -0.0033404406 -0.0032900567 -0.0031185653 -0.0027982118 -0.0023604122 -0.0019008488 -0.0015835776 -0.001449327 -0.0014723071 -0.0014448359 -0.00094736763 0.00093938108 0.0052103736][-0.0033472464 -0.0033621881 -0.0033759673 -0.0033706161 -0.0033237368 -0.00323034 -0.0030774737 -0.002852716 -0.0025999881 -0.0023285914 -0.0020898411 -0.0018321071 -0.0012660036 0.00043302728 0.0042423457][-0.0033569643 -0.003373062 -0.0033883727 -0.0033927867 -0.0033867427 -0.00337179 -0.0033244144 -0.0032009797 -0.0029989588 -0.0027177464 -0.0024240089 -0.0021295364 -0.0016346148 -0.00022721686 0.003006458][-0.0033662866 -0.0033794949 -0.0033912004 -0.003392603 -0.0033922221 -0.0033912836 -0.0033713786 -0.0032971581 -0.0031546932 -0.002930548 -0.0026786001 -0.0024399469 -0.0020817365 -0.0010158108 0.0015681752]]...]
INFO - root - 2017-12-09 23:25:10.911249: step 69410, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:08m:13s remains)
INFO - root - 2017-12-09 23:25:19.696022: step 69420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 64h:04m:55s remains)
INFO - root - 2017-12-09 23:25:28.423864: step 69430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:45m:16s remains)
INFO - root - 2017-12-09 23:25:37.051459: step 69440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:41m:00s remains)
INFO - root - 2017-12-09 23:25:45.663684: step 69450, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 66h:13m:36s remains)
INFO - root - 2017-12-09 23:25:54.355306: step 69460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:07m:19s remains)
INFO - root - 2017-12-09 23:26:03.095701: step 69470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:01m:54s remains)
INFO - root - 2017-12-09 23:26:11.526863: step 69480, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 60h:55m:20s remains)
INFO - root - 2017-12-09 23:26:20.216993: step 69490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:57m:30s remains)
INFO - root - 2017-12-09 23:26:28.728422: step 69500, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:53m:50s remains)
2017-12-09 23:26:29.720206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033670252 -0.0033640945 -0.0033637041 -0.0033636056 -0.0033636654 -0.00336394 -0.0033642286 -0.0033645276 -0.0033648415 -0.0033650356 -0.0033649721 -0.003364776 -0.0033645232 -0.0033643751 -0.0033642405][-0.0033648585 -0.0033617052 -0.0033612452 -0.0033612356 -0.0033613513 -0.0033616424 -0.0033619753 -0.0033623928 -0.0033628058 -0.0033629886 -0.0033630109 -0.0033628833 -0.0033626235 -0.0033623555 -0.0033621357][-0.0033654014 -0.0033623155 -0.0033619385 -0.0033619718 -0.0033621727 -0.0033623863 -0.0033626934 -0.0033630705 -0.0033634331 -0.0033636836 -0.0033639176 -0.0033639728 -0.0033637614 -0.0033633891 -0.0033630042][-0.003365726 -0.003362797 -0.0033624582 -0.0033624824 -0.0033627653 -0.0033631749 -0.0033636654 -0.003364065 -0.003364431 -0.0033648182 -0.0033652396 -0.0033653928 -0.0033651509 -0.0033646997 -0.0033641886][-0.0033661749 -0.0033633909 -0.0033632475 -0.0033633513 -0.0033637239 -0.0033643823 -0.0033651504 -0.0033656037 -0.0033657183 -0.003365936 -0.003366397 -0.0033664426 -0.0033661851 -0.0033658496 -0.0033654054][-0.0033667129 -0.0033641327 -0.0033645469 -0.0033653411 -0.0033664359 -0.0033676994 -0.0033690145 -0.0033695267 -0.0033693423 -0.0033688955 -0.0033685695 -0.0033679637 -0.0033674061 -0.0033671344 -0.0033667209][-0.003367092 -0.0033649378 -0.0033661234 -0.0033677395 -0.0033696722 -0.0033718927 -0.0033739062 -0.0033745971 -0.0033739472 -0.0033728795 -0.0033714743 -0.0033698534 -0.0033686173 -0.0033681903 -0.0033678813][-0.0033671523 -0.0033654454 -0.003367278 -0.0033695777 -0.003372282 -0.0033754478 -0.0033783698 -0.0033794234 -0.0033784795 -0.0033766269 -0.0033741686 -0.003371438 -0.0033694603 -0.0033687148 -0.0033683262][-0.0033670063 -0.0033654443 -0.0033676496 -0.0033704089 -0.003373591 -0.0033773642 -0.0033807456 -0.0033822 -0.003381086 -0.0033786981 -0.0033755028 -0.0033720355 -0.0033694624 -0.0033684636 -0.0033680822][-0.0033666422 -0.0033650014 -0.0033675116 -0.0033705549 -0.0033740366 -0.0033778595 -0.0033812253 -0.0033827526 -0.0033816223 -0.00337893 -0.0033751768 -0.0033711137 -0.0033682568 -0.0033671167 -0.003366966][-0.0033661344 -0.0033643292 -0.0033668361 -0.0033698422 -0.0033733428 -0.0033770627 -0.0033801273 -0.0033813126 -0.0033800241 -0.0033773647 -0.0033733745 -0.0033691574 -0.003366234 -0.0033651798 -0.0033652794][-0.0033657108 -0.0033635576 -0.003365678 -0.0033681379 -0.0033710313 -0.0033741833 -0.003376585 -0.0033773843 -0.0033760997 -0.003373716 -0.0033703039 -0.0033665416 -0.0033638247 -0.0033628261 -0.0033632356][-0.0033655658 -0.0033628421 -0.0033643111 -0.0033659828 -0.0033680319 -0.0033702087 -0.0033717528 -0.0033721174 -0.0033710003 -0.0033691342 -0.0033664871 -0.0033636396 -0.0033614046 -0.0033605166 -0.0033612109][-0.0033654014 -0.0033621853 -0.0033630284 -0.0033638719 -0.0033649404 -0.0033660568 -0.0033667972 -0.0033668755 -0.0033660363 -0.0033646452 -0.0033627262 -0.0033606899 -0.0033591345 -0.0033584968 -0.0033591837][-0.0033653474 -0.0033617753 -0.0033621525 -0.0033624629 -0.0033628789 -0.0033633136 -0.0033635723 -0.003363444 -0.003362763 -0.0033616479 -0.0033602088 -0.0033586076 -0.0033574628 -0.003356908 -0.0033573736]]...]
INFO - root - 2017-12-09 23:26:38.046624: step 69510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:05m:59s remains)
INFO - root - 2017-12-09 23:26:46.619577: step 69520, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 63h:43m:52s remains)
INFO - root - 2017-12-09 23:26:55.148213: step 69530, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 62h:23m:36s remains)
INFO - root - 2017-12-09 23:27:03.620254: step 69540, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.879 sec/batch; 64h:12m:48s remains)
INFO - root - 2017-12-09 23:27:12.256867: step 69550, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 65h:32m:02s remains)
INFO - root - 2017-12-09 23:27:20.839185: step 69560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 63h:03m:52s remains)
INFO - root - 2017-12-09 23:27:29.488334: step 69570, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:49m:16s remains)
INFO - root - 2017-12-09 23:27:38.131188: step 69580, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 55h:15m:05s remains)
INFO - root - 2017-12-09 23:27:46.963060: step 69590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:17m:39s remains)
INFO - root - 2017-12-09 23:27:55.385464: step 69600, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 63h:30m:52s remains)
2017-12-09 23:27:56.257729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003198988 -0.0031963738 -0.0031770703 -0.0032080379 -0.0032167872 -0.0032890108 -0.0033236824 -0.0033444325 -0.0033249347 -0.003294932 -0.003268837 -0.0032552963 -0.0032492585 -0.003247445 -0.0032455332][-0.0031787031 -0.0031804803 -0.0031809122 -0.0031757017 -0.0031576974 -0.0031956902 -0.0032043715 -0.0032443653 -0.0032310947 -0.0032121539 -0.0031807537 -0.00316097 -0.0031666467 -0.0031788845 -0.0031876042][-0.0031333643 -0.003113688 -0.003138944 -0.0031521418 -0.0031558322 -0.0031730996 -0.0031706044 -0.0031773751 -0.0031322869 -0.0031302364 -0.0031043084 -0.0030969807 -0.0030979891 -0.0031348984 -0.0032000714][-0.0031343873 -0.0030835548 -0.0030520582 -0.0030681507 -0.0030773471 -0.0030766404 -0.0030588289 -0.003056688 -0.0030096248 -0.0029574616 -0.0028991397 -0.0028855498 -0.0028813512 -0.0029775612 -0.003060682][-0.0030887856 -0.0029957301 -0.0029270698 -0.0028907538 -0.0028720074 -0.002882787 -0.002872831 -0.0027848529 -0.0026809685 -0.002612405 -0.0025471528 -0.0024981662 -0.0024835719 -0.002567156 -0.0026748122][-0.0029408855 -0.0027808133 -0.0026568433 -0.0025826 -0.0025457768 -0.0025368433 -0.0025072861 -0.002403954 -0.0022976082 -0.0021764082 -0.0020933459 -0.0020590136 -0.0021091294 -0.00215057 -0.0022171238][-0.0027596431 -0.002530735 -0.0023533995 -0.0022426606 -0.0021795412 -0.0021509738 -0.0021238448 -0.00205188 -0.001967317 -0.0018628442 -0.0018100372 -0.0017566495 -0.0017784575 -0.0018333327 -0.0018984323][-0.002650314 -0.0023585095 -0.0021411881 -0.0019939383 -0.0019169907 -0.0018851461 -0.0018623432 -0.0018183374 -0.0017653955 -0.0017143512 -0.0017047543 -0.0016709368 -0.0016787841 -0.0016806783 -0.0017203272][-0.002663994 -0.0023391135 -0.0020665477 -0.0018866308 -0.0017732336 -0.0017235145 -0.0017073378 -0.0016881644 -0.0016735365 -0.0016661424 -0.0016865828 -0.0016988172 -0.0017305746 -0.0017038692 -0.0017252465][-0.0027140677 -0.0023890473 -0.0021046079 -0.0019116561 -0.0017777055 -0.0017064931 -0.0016976621 -0.0017077437 -0.001733784 -0.0017497665 -0.0017841554 -0.0018070531 -0.0018426313 -0.001845056 -0.0018615678][-0.0028346321 -0.0025061583 -0.002208129 -0.0019810125 -0.0018363211 -0.0018001668 -0.0017995001 -0.0018309871 -0.0018925717 -0.0019526274 -0.0020118416 -0.0020431322 -0.0020789388 -0.0020828363 -0.002092429][-0.0030309747 -0.0027458225 -0.002467792 -0.0022494392 -0.0021165805 -0.0020600422 -0.0020546764 -0.0021129702 -0.0021824781 -0.0022562416 -0.0023219609 -0.0023816433 -0.0024366234 -0.0024509807 -0.002452103][-0.0032156459 -0.0030382178 -0.002838332 -0.0026608105 -0.0025480711 -0.0024944805 -0.0024838466 -0.002518313 -0.0025780932 -0.0026435265 -0.0027231672 -0.002787885 -0.0028367392 -0.0028582867 -0.0028572043][-0.0033080128 -0.0032489784 -0.0031565712 -0.0030587879 -0.0029877196 -0.0029519773 -0.0029487733 -0.0029739626 -0.0030112301 -0.0030495923 -0.0030925956 -0.0031277461 -0.0031638334 -0.0031732342 -0.0031663205][-0.0033743838 -0.0033570016 -0.0033307925 -0.0032996165 -0.0032721425 -0.0032545156 -0.0032502229 -0.0032576814 -0.0032714738 -0.0032879775 -0.0033060333 -0.0033213964 -0.0033336559 -0.0033354661 -0.0033336533]]...]
INFO - root - 2017-12-09 23:28:04.821213: step 69610, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:07m:16s remains)
INFO - root - 2017-12-09 23:28:13.482193: step 69620, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:30m:37s remains)
INFO - root - 2017-12-09 23:28:22.019335: step 69630, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 61h:13m:20s remains)
INFO - root - 2017-12-09 23:28:30.511514: step 69640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:22m:39s remains)
INFO - root - 2017-12-09 23:28:39.085226: step 69650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:43m:28s remains)
INFO - root - 2017-12-09 23:28:47.814149: step 69660, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 66h:06m:50s remains)
INFO - root - 2017-12-09 23:28:56.207073: step 69670, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.804 sec/batch; 58h:43m:15s remains)
INFO - root - 2017-12-09 23:29:04.735195: step 69680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:10m:19s remains)
INFO - root - 2017-12-09 23:29:13.215250: step 69690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:49m:31s remains)
INFO - root - 2017-12-09 23:29:21.784646: step 69700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:21m:42s remains)
2017-12-09 23:29:22.664203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43809929 0.42662659 0.41424707 0.40292388 0.39489305 0.39005867 0.39147452 0.39695379 0.40281254 0.40670103 0.41159192 0.41821039 0.42581218 0.42896464 0.43246219][0.44176751 0.43533796 0.42568415 0.41690472 0.41124588 0.40880793 0.41174427 0.41850841 0.42626721 0.43088332 0.43609032 0.44169244 0.44764441 0.44970393 0.45151412][0.4482716 0.44756106 0.4417904 0.43575692 0.43170363 0.42977637 0.43206984 0.43766046 0.44492239 0.44908312 0.45345408 0.45845869 0.46350554 0.46534902 0.46623918][0.45968312 0.4639647 0.46147591 0.45808241 0.45634565 0.45474362 0.45623457 0.460248 0.46632081 0.47019997 0.47359931 0.47728595 0.48109517 0.48255092 0.48341641][0.46979859 0.47990111 0.48214465 0.48154968 0.48257533 0.48233905 0.48452985 0.48746625 0.49162292 0.49469519 0.49668467 0.49881625 0.50054306 0.50105089 0.50136083][0.47853094 0.49434009 0.50160384 0.5058102 0.50963628 0.51184648 0.51442927 0.51589477 0.51766706 0.51909196 0.52000409 0.51992172 0.51988429 0.51959515 0.51920712][0.48630759 0.5060907 0.51723152 0.52564847 0.53264827 0.53723776 0.54063237 0.54185295 0.5422653 0.5422526 0.5420059 0.54077947 0.5393731 0.53809369 0.53652716][0.48803774 0.51210189 0.52649766 0.53833479 0.54838473 0.55476743 0.55893576 0.55984604 0.55926168 0.5581795 0.556602 0.55403817 0.55126357 0.5489608 0.546622][0.48333243 0.51010025 0.52656639 0.54161549 0.554914 0.5645805 0.57135433 0.57239425 0.5716607 0.57002389 0.56770134 0.56399775 0.55988193 0.55666345 0.55332345][0.47729483 0.50491041 0.52209848 0.53860044 0.55332154 0.56496668 0.57328439 0.57524019 0.57517695 0.57412153 0.57241273 0.56899756 0.56458646 0.56081063 0.555631][0.46643856 0.49492279 0.512445 0.52908474 0.543685 0.55638689 0.56567711 0.56916684 0.570848 0.57147551 0.57146782 0.56912518 0.56511784 0.56054866 0.55292439][0.45218286 0.48127487 0.49875617 0.51498884 0.5294655 0.54130536 0.55000991 0.55446941 0.55749089 0.55985492 0.5616343 0.5612728 0.55839556 0.55315864 0.54298329][0.43433684 0.46364552 0.48070422 0.49586248 0.50902593 0.51947635 0.52741772 0.53174055 0.53554976 0.53949016 0.543181 0.54485571 0.54311574 0.53716093 0.52400559][0.41752663 0.44498998 0.46007147 0.47320604 0.48392749 0.49259743 0.49958146 0.50419658 0.50877923 0.51412714 0.51985055 0.52361536 0.52287942 0.515694 0.49943095][0.40056819 0.42583141 0.43838134 0.4483988 0.45569879 0.46143219 0.46638244 0.47124517 0.47639233 0.48275349 0.49006855 0.49543726 0.49536791 0.48668933 0.46760613]]...]
INFO - root - 2017-12-09 23:29:31.002330: step 69710, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:20m:20s remains)
INFO - root - 2017-12-09 23:29:39.770882: step 69720, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:45m:53s remains)
INFO - root - 2017-12-09 23:29:48.388007: step 69730, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.856 sec/batch; 62h:26m:44s remains)
INFO - root - 2017-12-09 23:29:57.138886: step 69740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:11m:25s remains)
INFO - root - 2017-12-09 23:30:05.763138: step 69750, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.829 sec/batch; 60h:30m:26s remains)
INFO - root - 2017-12-09 23:30:14.428964: step 69760, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 65h:36m:55s remains)
INFO - root - 2017-12-09 23:30:23.148677: step 69770, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:53m:28s remains)
INFO - root - 2017-12-09 23:30:31.966379: step 69780, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:18m:50s remains)
INFO - root - 2017-12-09 23:30:40.531565: step 69790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:11m:20s remains)
INFO - root - 2017-12-09 23:30:49.094972: step 69800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:24m:00s remains)
2017-12-09 23:30:49.971799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033549753 -0.0033577152 -0.0033019357 -0.0031875842 -0.0030310545 -0.0029056277 -0.0028462666 -0.0028057618 -0.0027319351 -0.0026049362 -0.0024428009 -0.002111074 -0.0011872225 0.00053830468 0.0030237355][-0.0031959922 -0.0031723878 -0.0030341775 -0.0027720344 -0.0024419958 -0.0022093481 -0.0021681897 -0.0022537471 -0.00233457 -0.0023093177 -0.0021632067 -0.0016348388 -0.00029707025 0.0022487303 0.0059071453][-0.002885208 -0.0027456977 -0.0023924569 -0.0018000051 -0.0011448276 -0.0007388799 -0.00078224111 -0.0011992599 -0.0017170142 -0.0020308031 -0.0020122193 -0.0013010104 0.00047090254 0.0038581311 0.0087654386][-0.0024308427 -0.0020368989 -0.0012497122 -2.0873267e-05 0.001236839 0.0019747948 0.0017969424 0.00080466154 -0.00049346872 -0.0014772462 -0.0017603152 -0.00092410343 0.0013143409 0.0055163829 0.011612411][-0.0019382474 -0.0011387079 0.000333332 0.0025393253 0.0047738245 0.0061461134 0.0059458697 0.0042530252 0.0018612694 -0.00017605489 -0.0010477451 -0.00025511626 0.0023719443 0.0072546741 0.014306432][-0.0014762399 -0.0001887891 0.0021147511 0.005478166 0.0089735715 0.011304353 0.011378278 0.0091231409 0.0055634985 0.0021904653 0.00035785721 0.00078153447 0.0035906138 0.0089389347 0.016656345][-0.0010471598 0.00070810528 0.0037868659 0.008189843 0.012901498 0.016286714 0.016942479 0.014504751 0.010053719 0.0053873863 0.0023480288 0.002023577 0.0046754945 0.010252598 0.01834158][-0.00063563208 0.0014368596 0.0050145881 0.010055413 0.0155576 0.019751541 0.021070397 0.018854177 0.014074739 0.008593455 0.0045177713 0.0032732652 0.0054190373 0.010899558 0.019046281][-0.00038531935 0.0017852683 0.00546097 0.010584578 0.016233137 0.020724617 0.022481773 0.020703431 0.0161691 0.010596448 0.0060358318 0.0040904004 0.0055716652 0.010596057 0.018352732][-0.00055060443 0.0014674577 0.0048240116 0.0094792051 0.014642244 0.018874859 0.020703791 0.019375611 0.015479799 0.010478907 0.0061278716 0.0039755171 0.0048633348 0.0090785669 0.01587043][-0.0012020504 0.00043100445 0.0031237456 0.0068722861 0.011064209 0.01458359 0.016164308 0.01520507 0.012140379 0.008146408 0.0045663537 0.0026899704 0.0031730563 0.0063787196 0.011729243][-0.0020949342 -0.0009841607 0.00086365757 0.0034763275 0.0064389715 0.0089770425 0.01013796 0.0094903558 0.0073262285 0.0045061475 0.0019748213 0.00063411449 0.00089517352 0.0030725605 0.0067832433][-0.0028419597 -0.0022447931 -0.0012034604 0.00031662127 0.002074681 0.0036078182 0.0043206746 0.0039399462 0.0026275746 0.00092919241 -0.0005707161 -0.0013628199 -0.001231415 4.2689731e-05 0.0022536821][-0.0032511936 -0.0030161489 -0.0025636577 -0.0018640149 -0.0010289513 -0.00028953794 6.1915256e-05 -0.00011690194 -0.00075424532 -0.0015782189 -0.0022892067 -0.0026665141 -0.0026153987 -0.0020156782 -0.00093894172][-0.0033871909 -0.0033291937 -0.0031927666 -0.0029578535 -0.0026603122 -0.0023911507 -0.0022589776 -0.0023184712 -0.0025438168 -0.0028367019 -0.0030858149 -0.003223768 -0.0032166189 -0.0030165834 -0.0026228775]]...]
INFO - root - 2017-12-09 23:30:58.387031: step 69810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:00m:30s remains)
INFO - root - 2017-12-09 23:31:06.937858: step 69820, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:53m:14s remains)
INFO - root - 2017-12-09 23:31:15.609663: step 69830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:15m:05s remains)
INFO - root - 2017-12-09 23:31:24.240668: step 69840, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 64h:05m:46s remains)
INFO - root - 2017-12-09 23:31:32.848016: step 69850, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 60h:52m:41s remains)
INFO - root - 2017-12-09 23:31:41.445371: step 69860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:31m:53s remains)
INFO - root - 2017-12-09 23:31:50.043616: step 69870, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 62h:29m:12s remains)
INFO - root - 2017-12-09 23:31:58.678688: step 69880, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:30m:44s remains)
INFO - root - 2017-12-09 23:32:07.184202: step 69890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:58m:42s remains)
INFO - root - 2017-12-09 23:32:15.716219: step 69900, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 63h:16m:06s remains)
2017-12-09 23:32:16.622414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033914198 -0.0033881729 -0.0033875485 -0.0033873038 -0.0033870453 -0.0033873669 -0.0033879285 -0.0033888782 -0.0033900819 -0.0033909804 -0.003392281 -0.003393759 -0.0033958564 -0.0033974198 -0.003397394][-0.0033884544 -0.0033850323 -0.0033845652 -0.0033847881 -0.0033850451 -0.0033857997 -0.0033867657 -0.003387905 -0.0033888139 -0.003389274 -0.0033898766 -0.0033903623 -0.0033915578 -0.0033932724 -0.0033945066][-0.0033894046 -0.0033859666 -0.0033856516 -0.0033858581 -0.0033861971 -0.0033870561 -0.0033880365 -0.0033891455 -0.0033898689 -0.003390009 -0.003390163 -0.0033900526 -0.0033904663 -0.0033915951 -0.0033932584][-0.0033901217 -0.0033866866 -0.0033862868 -0.0033865573 -0.0033867126 -0.0033872384 -0.0033882284 -0.0033891397 -0.0033899643 -0.0033904661 -0.0033905848 -0.0033900707 -0.003389664 -0.0033900412 -0.0033912738][-0.0033908791 -0.0033876644 -0.0033868069 -0.0033858914 -0.0033843066 -0.0033833825 -0.0033835394 -0.0033843645 -0.0033855985 -0.0033871855 -0.0033887681 -0.0033894656 -0.0033897895 -0.0033899504 -0.003390522][-0.003391111 -0.0033886177 -0.0033872214 -0.0033843014 -0.0033791175 -0.003374306 -0.0033733679 -0.0033754958 -0.003379269 -0.0033837014 -0.0033873089 -0.0033892856 -0.0033898866 -0.0033900314 -0.0033901413][-0.0033913446 -0.0033893795 -0.0033879946 -0.0033841108 -0.0033762206 -0.0033676261 -0.0033642151 -0.003366319 -0.0033725761 -0.0033806679 -0.0033873022 -0.0033905432 -0.0033910868 -0.0033906572 -0.0033894521][-0.0033910912 -0.003389799 -0.00338968 -0.0033876502 -0.0033813217 -0.0033731223 -0.0033679139 -0.0033686089 -0.0033741961 -0.0033818136 -0.0033886072 -0.003392393 -0.003392698 -0.0033912943 -0.0033889557][-0.0033907159 -0.003389512 -0.0033902612 -0.0033902233 -0.0033873639 -0.0033828814 -0.0033793589 -0.0033791929 -0.0033832209 -0.0033882898 -0.0033931918 -0.0033955418 -0.0033946624 -0.0033924568 -0.0033893427][-0.0033897732 -0.0033889746 -0.0033902556 -0.0033911776 -0.003391207 -0.0033903057 -0.0033895525 -0.0033896139 -0.003391668 -0.0033943527 -0.0033972682 -0.0033982589 -0.0033964626 -0.0033932533 -0.0033895357][-0.0033884533 -0.0033873385 -0.0033891548 -0.0033909681 -0.0033928284 -0.0033942796 -0.0033958175 -0.0033968312 -0.0033982461 -0.0033993025 -0.0033995572 -0.0033985216 -0.0033957462 -0.0033920954 -0.0033884479][-0.003387647 -0.0033856665 -0.0033873469 -0.0033898836 -0.003392793 -0.0033951644 -0.0033975197 -0.0033992764 -0.0034005549 -0.0034005481 -0.0033993402 -0.0033966973 -0.0033931364 -0.0033894149 -0.0033862691][-0.0033867168 -0.0033840989 -0.0033851669 -0.0033872083 -0.0033902274 -0.0033930733 -0.0033957025 -0.0033976578 -0.0033987374 -0.0033983649 -0.00339662 -0.0033936854 -0.0033901765 -0.00338685 -0.0033843277][-0.0033858158 -0.0033824618 -0.003383158 -0.0033842744 -0.0033863096 -0.003388565 -0.0033909266 -0.0033926682 -0.0033935728 -0.0033932305 -0.0033916528 -0.0033893972 -0.0033869385 -0.0033848495 -0.0033832048][-0.0033855494 -0.003381694 -0.0033819827 -0.0033825631 -0.0033836144 -0.0033847224 -0.0033859035 -0.0033869229 -0.0033875026 -0.0033873573 -0.0033865052 -0.0033853857 -0.0033841233 -0.0033830025 -0.0033821515]]...]
INFO - root - 2017-12-09 23:32:25.262881: step 69910, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:49m:56s remains)
INFO - root - 2017-12-09 23:32:34.051295: step 69920, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:52m:41s remains)
INFO - root - 2017-12-09 23:32:42.548636: step 69930, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 60h:50m:04s remains)
INFO - root - 2017-12-09 23:32:51.133628: step 69940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:29m:30s remains)
INFO - root - 2017-12-09 23:32:59.831235: step 69950, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 61h:57m:36s remains)
INFO - root - 2017-12-09 23:33:08.475444: step 69960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:51m:01s remains)
INFO - root - 2017-12-09 23:33:17.164024: step 69970, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:15m:38s remains)
INFO - root - 2017-12-09 23:33:25.914969: step 69980, loss = 0.88, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 65h:18m:49s remains)
INFO - root - 2017-12-09 23:33:34.490547: step 69990, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:22m:42s remains)
INFO - root - 2017-12-09 23:33:43.145101: step 70000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:26m:12s remains)
2017-12-09 23:33:44.045865: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03545019 0.039441664 0.041258954 0.040945832 0.040687557 0.041497976 0.044351097 0.047064681 0.04777883 0.04549206 0.040205903 0.032464389 0.023610823 0.015306771 0.0086031966][0.03688585 0.040211126 0.040246382 0.039129492 0.037855037 0.038206447 0.040949345 0.042505909 0.042855237 0.040095396 0.034771495 0.027116245 0.018621206 0.011115131 0.0055966461][0.048598669 0.05082348 0.049489003 0.046062894 0.043656327 0.042355221 0.043363426 0.043792766 0.043457516 0.039943073 0.033900883 0.026033526 0.017499823 0.0099132285 0.0042482307][0.061278261 0.064527765 0.064441696 0.061932124 0.05917725 0.056596722 0.055573232 0.054065879 0.051595714 0.046793614 0.040091291 0.031557545 0.022055104 0.0132627 0.0064277826][0.071869537 0.075523376 0.0763994 0.077429637 0.0778291 0.077097468 0.076320983 0.074540935 0.070894018 0.063969575 0.054848578 0.044115659 0.032441724 0.021264937 0.011984651][0.089449815 0.0925439 0.093002774 0.093964837 0.095095068 0.095291659 0.096638381 0.096133247 0.093425669 0.087275542 0.077845 0.065659344 0.050862607 0.036115415 0.022941912][0.11216757 0.11475766 0.1145165 0.11459526 0.11478843 0.11481651 0.11584148 0.11596843 0.11423014 0.10894685 0.099936262 0.087435976 0.071367793 0.053968687 0.037163906][0.13084763 0.13282821 0.13220738 0.13243175 0.13240062 0.1320973 0.13258107 0.13288154 0.13152687 0.12615877 0.11723197 0.10504813 0.08894065 0.070481218 0.051630802][0.14437708 0.14680584 0.14607193 0.14654711 0.14646989 0.14639643 0.14633699 0.14695756 0.1460759 0.14122407 0.13304073 0.12131046 0.10560186 0.086857542 0.066919677][0.15588081 0.1587345 0.15809903 0.15785879 0.15783262 0.15784265 0.15763527 0.1581587 0.15736425 0.15322116 0.14565308 0.13432492 0.1188195 0.10014748 0.079585433][0.16272712 0.16743791 0.16864437 0.16905345 0.16884984 0.16844271 0.16776478 0.16801335 0.16711293 0.16359441 0.15656707 0.14571035 0.1305425 0.11192854 0.090674542][0.16108888 0.16618165 0.16891232 0.17110895 0.17275181 0.17351341 0.17268892 0.17213842 0.17077419 0.16787849 0.16193469 0.15220673 0.13806096 0.12045563 0.099668391][0.15930721 0.16178362 0.16322526 0.16603474 0.16900456 0.17091496 0.17165332 0.17226727 0.17150792 0.16814406 0.16257897 0.15383822 0.14050275 0.12390755 0.10387266][0.15353636 0.1552185 0.15549462 0.15660024 0.15906031 0.16144942 0.16402259 0.16651626 0.16774057 0.16721116 0.1630103 0.15531468 0.14254539 0.12649237 0.1066887][0.1465811 0.14891623 0.14842077 0.14760859 0.14793471 0.14842957 0.14897704 0.15087478 0.15215486 0.1539156 0.15337475 0.14846051 0.13941216 0.1255936 0.10752657]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 23:33:53.225030: step 70010, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 61h:03m:00s remains)
INFO - root - 2017-12-09 23:34:01.886117: step 70020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:37m:20s remains)
INFO - root - 2017-12-09 23:34:10.546484: step 70030, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:37m:49s remains)
INFO - root - 2017-12-09 23:34:19.032173: step 70040, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 64h:00m:33s remains)
INFO - root - 2017-12-09 23:34:27.783943: step 70050, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:59m:00s remains)
INFO - root - 2017-12-09 23:34:36.511429: step 70060, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:41m:12s remains)
INFO - root - 2017-12-09 23:34:45.247616: step 70070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:55m:40s remains)
INFO - root - 2017-12-09 23:34:53.924320: step 70080, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:09m:37s remains)
INFO - root - 2017-12-09 23:35:02.437780: step 70090, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 61h:10m:50s remains)
INFO - root - 2017-12-09 23:35:10.752521: step 70100, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 61h:28m:14s remains)
2017-12-09 23:35:11.610405: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.065145351 0.065232739 0.064195335 0.063889556 0.063189529 0.063428685 0.064108618 0.064213969 0.0648782 0.066322252 0.066225268 0.06648574 0.065593854 0.064596906 0.063528948][0.048295759 0.048589453 0.048504483 0.048568685 0.048764896 0.048940167 0.049542036 0.049832143 0.050399564 0.051859219 0.05190663 0.052079257 0.050858092 0.049864814 0.048534773][0.027102286 0.027221167 0.027255574 0.02734852 0.027513983 0.02784087 0.028447488 0.029298926 0.030123776 0.031177143 0.031462509 0.031682096 0.031100797 0.030418297 0.02921894][0.010443441 0.010372579 0.010331126 0.010326421 0.010366387 0.010532942 0.01088186 0.011513108 0.01228136 0.012927186 0.013308268 0.013352931 0.013195833 0.012856334 0.012217768][0.00055343495 0.00054090354 0.00048545119 0.00039360183 0.00028401171 0.00024268054 0.00032393821 0.00052991766 0.00081039476 0.0012528568 0.0015848016 0.0016481976 0.0014933476 0.0011846984 0.0008819995][-0.0027996746 -0.0027039098 -0.0026831194 -0.0027353805 -0.0028550311 -0.0029665148 -0.0030336846 -0.0030547357 -0.0029974915 -0.0027091149 -0.0023079717 -0.002115061 -0.0023382581 -0.0027731562 -0.0031143865][-0.0025701881 -0.0025072556 -0.0025456962 -0.0026353265 -0.0027499953 -0.0028641555 -0.0029478068 -0.0029311904 -0.0026745787 -0.0019928082 -0.0010971385 -0.00066370703 -0.001175829 -0.0021311764 -0.0028624134][-0.0025601701 -0.0024284539 -0.0024275077 -0.0025219398 -0.0026166034 -0.0026685162 -0.0027260308 -0.0026546277 -0.0020692605 -0.00063652475 0.0011140092 0.0019451559 0.0010054612 -0.000714042 -0.0020144468][-0.0025438643 -0.0024727446 -0.0024319256 -0.00246552 -0.00247591 -0.0024247183 -0.0023639626 -0.002030944 -0.00082777347 0.0017081734 0.0045961253 0.0058189565 0.0042756638 0.0016566378 -0.00029595965][-0.0024464815 -0.0024218007 -0.0023832982 -0.0023777664 -0.0023037505 -0.0021597641 -0.0019561236 -0.001256139 0.00075473846 0.0044616004 0.0082694106 0.0096946061 0.007612789 0.0042971484 0.0018124536][-0.0025230704 -0.0024013831 -0.0022469484 -0.0021679825 -0.0020165117 -0.0018248979 -0.0015247989 -0.00052715931 0.0020468 0.0063538542 0.010353214 0.011517182 0.009134328 0.0057127904 0.0031198233][-0.0028707832 -0.0028644411 -0.0027871882 -0.0027503 -0.0026040631 -0.0024283947 -0.0021202536 -0.0010626502 0.0014853596 0.0055428855 0.0089624831 0.0096892314 0.0076988833 0.0049933381 0.0028623824][-0.002950673 -0.0029892167 -0.0029725635 -0.0031146628 -0.0031795076 -0.0031027622 -0.0028944334 -0.0020687929 -0.00020538713 0.0027600925 0.0050825933 0.0054533929 0.0040728385 0.0023057868 0.00089637353][-0.0032149274 -0.003168656 -0.0030143508 -0.0031386933 -0.0032312588 -0.0031930467 -0.003124011 -0.0027973992 -0.0019220378 -0.0003436876 0.00075114681 0.00082488661 0.00014984212 -0.00061962125 -0.0012321107][-0.0029382422 -0.0028975955 -0.0027163073 -0.002864806 -0.0029792346 -0.0029368554 -0.0029117719 -0.00295524 -0.0028764056 -0.0024624341 -0.0021579326 -0.0022073416 -0.0024097259 -0.0025825966 -0.0027283565]]...]
INFO - root - 2017-12-09 23:35:19.982376: step 70110, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:20m:29s remains)
INFO - root - 2017-12-09 23:35:28.630511: step 70120, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 63h:15m:01s remains)
INFO - root - 2017-12-09 23:35:37.352422: step 70130, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 63h:18m:14s remains)
INFO - root - 2017-12-09 23:35:46.089664: step 70140, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.989 sec/batch; 72h:02m:46s remains)
INFO - root - 2017-12-09 23:35:54.718995: step 70150, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 62h:44m:56s remains)
INFO - root - 2017-12-09 23:36:03.383002: step 70160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:46m:27s remains)
INFO - root - 2017-12-09 23:36:11.986064: step 70170, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 63h:57m:19s remains)
INFO - root - 2017-12-09 23:36:20.746083: step 70180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:52m:02s remains)
INFO - root - 2017-12-09 23:36:29.357739: step 70190, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 66h:43m:14s remains)
INFO - root - 2017-12-09 23:36:37.986557: step 70200, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 65h:11m:59s remains)
2017-12-09 23:36:38.850962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033710967 -0.0033651297 -0.0033614025 -0.0033588696 -0.0033578123 -0.0033573564 -0.0033572267 -0.0033574882 -0.0033592742 -0.0033630852 -0.0033682445 -0.0033748676 -0.0033818735 -0.0033867389 -0.003388257][-0.0033650133 -0.0033597697 -0.0033570714 -0.0033558132 -0.00335586 -0.003356365 -0.0033570942 -0.0033577427 -0.003359325 -0.0033624163 -0.0033667292 -0.0033724252 -0.0033788427 -0.00338357 -0.0033855862][-0.0033605378 -0.0033563885 -0.0033552784 -0.0033554693 -0.0033567816 -0.0033582542 -0.003359685 -0.0033608885 -0.0033623457 -0.0033645739 -0.003367461 -0.0033714273 -0.0033761526 -0.0033802318 -0.0033824076][-0.0033569601 -0.0033536821 -0.0033536525 -0.0033551948 -0.0033579527 -0.0033605155 -0.0033627613 -0.0033645919 -0.0033659704 -0.0033670382 -0.003368228 -0.0033706205 -0.0033736362 -0.0033770879 -0.0033794993][-0.00335566 -0.0033529724 -0.0033536882 -0.0033566062 -0.003360641 -0.0033641122 -0.0033666135 -0.0033682548 -0.0033690152 -0.0033688943 -0.0033688326 -0.0033699707 -0.0033719321 -0.0033746562 -0.0033768755][-0.0033563257 -0.0033544505 -0.00335585 -0.0033598305 -0.0033649176 -0.0033691055 -0.0033720122 -0.00337331 -0.0033730064 -0.0033717919 -0.0033710042 -0.0033714089 -0.0033724885 -0.0033741279 -0.0033755449][-0.0033574197 -0.0033560549 -0.0033582544 -0.0033633932 -0.00336958 -0.0033747279 -0.0033780425 -0.0033794139 -0.0033785219 -0.0033768252 -0.0033754071 -0.0033752427 -0.0033755365 -0.0033758092 -0.0033756879][-0.0033585473 -0.0033574707 -0.0033601427 -0.0033657004 -0.0033726043 -0.0033786681 -0.0033824889 -0.0033840192 -0.0033830991 -0.0033814576 -0.0033797876 -0.0033795433 -0.0033789966 -0.0033780322 -0.0033765056][-0.003359254 -0.0033583462 -0.0033610479 -0.0033663814 -0.00337335 -0.00337983 -0.003384307 -0.0033863047 -0.0033857343 -0.0033844796 -0.0033830223 -0.0033824502 -0.0033811922 -0.0033791782 -0.0033763666][-0.0033594766 -0.0033585341 -0.003361067 -0.0033657937 -0.0033722676 -0.0033785922 -0.0033834996 -0.0033862586 -0.0033864544 -0.0033856016 -0.0033843985 -0.0033834539 -0.0033817647 -0.0033790017 -0.0033753414][-0.0033595222 -0.0033582633 -0.0033603946 -0.0033642356 -0.0033694934 -0.0033748848 -0.0033797796 -0.0033835371 -0.0033848407 -0.0033846048 -0.0033837738 -0.0033827757 -0.0033806383 -0.0033772385 -0.0033732771][-0.0033591557 -0.0033574144 -0.0033588998 -0.0033614819 -0.0033652235 -0.0033694308 -0.0033736781 -0.0033776264 -0.0033799149 -0.0033810432 -0.0033811042 -0.00338022 -0.0033779689 -0.0033744501 -0.0033704061][-0.0033589862 -0.0033566987 -0.0033574824 -0.0033586912 -0.0033607662 -0.0033635653 -0.0033667856 -0.0033703155 -0.0033729845 -0.0033748462 -0.0033755824 -0.0033750262 -0.0033732164 -0.0033703768 -0.0033670715][-0.0033586551 -0.0033560684 -0.0033564458 -0.0033569064 -0.0033577417 -0.0033592654 -0.0033614242 -0.0033640116 -0.0033661618 -0.0033677532 -0.0033686827 -0.00336851 -0.0033673279 -0.0033653018 -0.0033628028][-0.0033591501 -0.0033562635 -0.003356202 -0.0033562258 -0.0033562661 -0.003356846 -0.0033580109 -0.0033595886 -0.0033608417 -0.003361742 -0.0033624002 -0.0033625646 -0.0033619974 -0.0033607986 -0.0033592822]]...]
INFO - root - 2017-12-09 23:36:47.447069: step 70210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:11m:09s remains)
INFO - root - 2017-12-09 23:36:56.114500: step 70220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:33m:39s remains)
INFO - root - 2017-12-09 23:37:04.786364: step 70230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 64h:25m:37s remains)
INFO - root - 2017-12-09 23:37:13.391019: step 70240, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:38m:19s remains)
INFO - root - 2017-12-09 23:37:22.207002: step 70250, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:48m:53s remains)
INFO - root - 2017-12-09 23:37:30.857165: step 70260, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 61h:20m:55s remains)
INFO - root - 2017-12-09 23:37:39.703747: step 70270, loss = 0.90, batch loss = 0.69 (7.9 examples/sec; 1.015 sec/batch; 73h:54m:38s remains)
INFO - root - 2017-12-09 23:37:48.578063: step 70280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:24m:20s remains)
INFO - root - 2017-12-09 23:37:57.017123: step 70290, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:52m:22s remains)
INFO - root - 2017-12-09 23:38:05.453122: step 70300, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 60h:11m:15s remains)
2017-12-09 23:38:06.341352: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014247289 0.014703702 0.01590227 0.016392361 0.016592266 0.015509255 0.014210699 0.012251558 0.0099758813 0.0079281321 0.0061759586 0.0050166808 0.0038658169 0.0028687434 0.0020669752][0.016657609 0.017142808 0.01861074 0.019577393 0.020329079 0.019565774 0.01850602 0.016381359 0.013813371 0.011352948 0.0090294993 0.0071209995 0.0053165443 0.0040236088 0.0030057717][0.021148579 0.022013437 0.024095619 0.025854914 0.027546477 0.02758185 0.026965413 0.024600858 0.021504529 0.018074442 0.014185974 0.010548375 0.0071321027 0.0047171358 0.0028071271][0.027492374 0.029668162 0.033220138 0.036583349 0.039771672 0.041178968 0.041344982 0.039067861 0.035585832 0.030902615 0.025114913 0.019109281 0.01331009 0.0087707 0.0051681157][0.036383517 0.039514974 0.044420928 0.049494617 0.054205827 0.056876674 0.057768874 0.055866547 0.052198414 0.046714794 0.039676998 0.031909507 0.024204042 0.017769629 0.012491703][0.049937464 0.053378 0.058689322 0.064578414 0.069976732 0.073192373 0.074306428 0.07256338 0.0687728 0.062827513 0.055171069 0.046668656 0.038123243 0.030727958 0.024500486][0.066104352 0.069205381 0.074182145 0.080107652 0.085604541 0.088709958 0.089641921 0.087830611 0.083948091 0.077785768 0.070038728 0.061728477 0.053330079 0.045776919 0.039153084][0.080556437 0.083011419 0.0870635 0.09236522 0.097409122 0.10017006 0.10055912 0.098604418 0.094791763 0.089182571 0.082132936 0.074620895 0.067324691 0.060490556 0.053928472][0.090517581 0.092014663 0.094560221 0.098705277 0.1027972 0.10510549 0.10538121 0.10381018 0.10072653 0.096084096 0.090333804 0.083889186 0.077572905 0.071383357 0.065011457][0.094361208 0.095140345 0.095809847 0.098144807 0.10086913 0.10248785 0.10254269 0.10130633 0.098997489 0.095601253 0.0913173 0.086452484 0.081564419 0.076438509 0.070970722][0.093689442 0.093564294 0.092138365 0.092466138 0.093444712 0.094006591 0.093643963 0.092462055 0.090778194 0.0884971 0.085777879 0.082535841 0.07917355 0.075550139 0.071493052][0.08941815 0.08832074 0.084930114 0.083466142 0.083229244 0.083111733 0.0825802 0.0816268 0.080400072 0.078860119 0.077040359 0.074986212 0.072767884 0.0703624 0.067696273][0.084194154 0.082461193 0.077711634 0.075084612 0.074071139 0.073640443 0.072986208 0.072153494 0.071206614 0.070031047 0.068683639 0.06729421 0.065877981 0.064340547 0.062695034][0.079641111 0.077712171 0.072240189 0.06907697 0.067769021 0.067295909 0.066725157 0.066114 0.0653196 0.064371549 0.063335054 0.062244408 0.06116008 0.060048707 0.059002127][0.073833846 0.071734123 0.0660355 0.062827647 0.061568066 0.061165616 0.060913451 0.060683161 0.060338087 0.060050853 0.059572853 0.058707431 0.05782225 0.056735154 0.055688914]]...]
INFO - root - 2017-12-09 23:38:14.806990: step 70310, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:06m:47s remains)
INFO - root - 2017-12-09 23:38:23.565434: step 70320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:59m:13s remains)
INFO - root - 2017-12-09 23:38:32.271904: step 70330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:20m:00s remains)
INFO - root - 2017-12-09 23:38:40.889910: step 70340, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:31m:08s remains)
INFO - root - 2017-12-09 23:38:49.498922: step 70350, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:06m:48s remains)
INFO - root - 2017-12-09 23:38:58.242793: step 70360, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:47m:12s remains)
INFO - root - 2017-12-09 23:39:07.049572: step 70370, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 63h:14m:46s remains)
INFO - root - 2017-12-09 23:39:15.717496: step 70380, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.969 sec/batch; 70h:34m:00s remains)
INFO - root - 2017-12-09 23:39:24.234577: step 70390, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:14m:40s remains)
INFO - root - 2017-12-09 23:39:32.678888: step 70400, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:05m:15s remains)
2017-12-09 23:39:33.584699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033928128 -0.0033909301 -0.0033909867 -0.0033913725 -0.0033917578 -0.0033920261 -0.0033922163 -0.0033922845 -0.0033922433 -0.0033918077 -0.0033912847 -0.0033907741 -0.0033902912 -0.0033900631 -0.0033899266][-0.003390508 -0.0033882952 -0.0033882344 -0.00338841 -0.0033885725 -0.0033887322 -0.0033888277 -0.0033888465 -0.003388847 -0.0033886998 -0.0033885329 -0.0033883641 -0.0033882505 -0.003388278 -0.0033883429][-0.0033899122 -0.0033874991 -0.0033874228 -0.0033875946 -0.0033877525 -0.0033878398 -0.0033877839 -0.0033876782 -0.0033876747 -0.0033877646 -0.0033878041 -0.0033876835 -0.0033876363 -0.003387783 -0.0033879026][-0.0033892775 -0.0033869008 -0.0033868449 -0.0033871795 -0.0033875203 -0.0033877359 -0.0033877408 -0.0033876314 -0.0033875962 -0.0033876824 -0.0033877215 -0.0033875194 -0.0033872551 -0.0033872221 -0.0033871965][-0.0033893012 -0.0033869776 -0.0033870221 -0.0033876614 -0.0033883131 -0.0033888521 -0.0033890873 -0.003388949 -0.0033888503 -0.003388912 -0.003388715 -0.0033881944 -0.0033875369 -0.0033870377 -0.0033866062][-0.0033897911 -0.0033874083 -0.0033875457 -0.0033884451 -0.0033893806 -0.0033902728 -0.0033907488 -0.0033908593 -0.0033910261 -0.0033910903 -0.0033905348 -0.0033895045 -0.0033883322 -0.0033873778 -0.0033866744][-0.0033905834 -0.0033882731 -0.0033885774 -0.0033897681 -0.0033912659 -0.0033924526 -0.0033931087 -0.0033934936 -0.0033942554 -0.0033943148 -0.0033932854 -0.0033916419 -0.003389732 -0.0033882463 -0.0033871715][-0.0033912966 -0.0033893394 -0.0033900773 -0.0033918656 -0.0033941516 -0.0033959793 -0.0033971246 -0.0033978226 -0.00339901 -0.0033988068 -0.0033969227 -0.0033942487 -0.0033914284 -0.0033892356 -0.0033875578][-0.0033917339 -0.003390081 -0.0033913369 -0.0033938941 -0.0033972063 -0.0034000073 -0.0034025232 -0.0034044657 -0.0034055721 -0.0034046222 -0.0034018648 -0.0033982417 -0.003394579 -0.0033917592 -0.0033894505][-0.0033915343 -0.0033901327 -0.003391824 -0.0033950924 -0.0033991267 -0.0034028157 -0.0034062287 -0.003409046 -0.0034104288 -0.0034092872 -0.0034060429 -0.0034023144 -0.0033986853 -0.0033957423 -0.0033930514][-0.0033912286 -0.0033897585 -0.0033915825 -0.0033949187 -0.0033992617 -0.0034032394 -0.0034068706 -0.0034098418 -0.0034115817 -0.0034107391 -0.0034076469 -0.0034042317 -0.0034012245 -0.003398936 -0.0033964813][-0.0033916177 -0.003389843 -0.003391261 -0.0033939986 -0.0033978072 -0.003401482 -0.0034049542 -0.0034079223 -0.0034098122 -0.0034094546 -0.0034074211 -0.0034049223 -0.0034026797 -0.003400902 -0.0033986708][-0.0033928221 -0.0033905178 -0.0033911152 -0.0033928314 -0.0033956217 -0.0033986643 -0.0034016483 -0.0034043698 -0.0034062157 -0.0034063393 -0.0034053177 -0.0034037298 -0.0034020036 -0.0034003682 -0.0033985605][-0.0033941055 -0.0033913392 -0.0033912002 -0.0033919245 -0.0033934643 -0.0033953758 -0.0033976419 -0.0033999127 -0.0034015421 -0.0034019956 -0.0034018129 -0.0034012869 -0.0034004729 -0.0033996054 -0.0033983765][-0.0033959076 -0.0033926396 -0.0033915886 -0.0033912028 -0.0033915138 -0.0033922945 -0.0033935627 -0.00339503 -0.0033963381 -0.0033970233 -0.0033973961 -0.0033974047 -0.0033973656 -0.0033972375 -0.00339669]]...]
INFO - root - 2017-12-09 23:39:42.029153: step 70410, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 61h:31m:33s remains)
INFO - root - 2017-12-09 23:39:50.941960: step 70420, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 65h:47m:27s remains)
INFO - root - 2017-12-09 23:39:59.729316: step 70430, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 65h:03m:06s remains)
INFO - root - 2017-12-09 23:40:08.366330: step 70440, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 64h:42m:37s remains)
INFO - root - 2017-12-09 23:40:16.990289: step 70450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 61h:40m:04s remains)
INFO - root - 2017-12-09 23:40:25.582681: step 70460, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:35m:42s remains)
INFO - root - 2017-12-09 23:40:34.204159: step 70470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:46m:54s remains)
INFO - root - 2017-12-09 23:40:42.868449: step 70480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:46m:12s remains)
INFO - root - 2017-12-09 23:40:51.269264: step 70490, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 61h:20m:16s remains)
INFO - root - 2017-12-09 23:40:59.556604: step 70500, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 59h:21m:19s remains)
2017-12-09 23:41:00.403378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034055333 -0.0034045265 -0.0034021817 -0.0033922542 -0.00336591 -0.0033208134 -0.003271977 -0.0032494601 -0.0032686598 -0.0033135943 -0.0033579075 -0.0033850695 -0.0033979395 -0.0034029251 -0.0034032222][-0.0034043354 -0.0034020261 -0.0033940417 -0.0033708287 -0.0033233783 -0.0032558425 -0.0031907163 -0.0031656786 -0.0031969356 -0.0032643266 -0.0033305215 -0.0033741889 -0.003394254 -0.0034008338 -0.0034011134][-0.0034016855 -0.0033937974 -0.0033723381 -0.0033246197 -0.0032456343 -0.0031504526 -0.0030718923 -0.003052542 -0.0031065803 -0.0032058486 -0.0033007483 -0.0033650009 -0.0033929672 -0.00340067 -0.0034009721][-0.0033765081 -0.0033420781 -0.0032799658 -0.0031784209 -0.00304289 -0.0029016568 -0.0028039364 -0.0027991026 -0.0029033318 -0.0030716818 -0.0032303673 -0.0033394857 -0.0033874661 -0.0034011172 -0.0034023381][-0.0032804771 -0.003165835 -0.0030009681 -0.0027858997 -0.0025459847 -0.0023258175 -0.0021960721 -0.0022152122 -0.0024076002 -0.0027100763 -0.0030133945 -0.0032398433 -0.003355382 -0.0033956775 -0.0034023137][-0.0030656857 -0.0027976059 -0.0024509728 -0.002057858 -0.0016729085 -0.0013543216 -0.0011875073 -0.0012382152 -0.001547034 -0.0020431317 -0.0025803698 -0.0030173464 -0.0032718296 -0.0033760369 -0.0033991307][-0.0027613756 -0.0023061186 -0.0017486588 -0.0011632729 -0.00063362368 -0.00022765249 -3.1767413e-05 -0.00011249608 -0.00052424963 -0.0012053158 -0.0019946122 -0.0026880621 -0.0031355449 -0.0033389768 -0.0033920575][-0.002511828 -0.0019267559 -0.0012236191 -0.00051036943 0.00011147978 0.00056964788 0.0007846856 0.00069162971 0.00023030746 -0.00055080769 -0.0015016155 -0.0023858319 -0.0029999223 -0.003299729 -0.0033853152][-0.0024761234 -0.0018919231 -0.0011818239 -0.00045676343 0.00018148473 0.00065385108 0.00088230916 0.00080210227 0.00035629794 -0.00040902779 -0.0013631831 -0.0022799158 -0.0029441325 -0.0032820769 -0.0033821417][-0.0026745256 -0.0022148683 -0.0016332112 -0.0010158746 -0.00044947187 -1.2539094e-05 0.00021499256 0.00016702013 -0.00021128519 -0.00086365175 -0.00167278 -0.0024468834 -0.0030109091 -0.0032999567 -0.0033849229][-0.002975398 -0.0026932873 -0.0023133068 -0.0018833242 -0.0014639783 -0.0011185305 -0.0009211828 -0.00093673053 -0.001213897 -0.0016909259 -0.0022614102 -0.0027843362 -0.0031531772 -0.0033383924 -0.003391386][-0.0032263449 -0.0030941109 -0.0029033402 -0.002669865 -0.0024245575 -0.0022049274 -0.00206679 -0.0020629887 -0.0022309064 -0.002517517 -0.0028406286 -0.0031129497 -0.0032900888 -0.003374286 -0.0033967027][-0.0033611131 -0.0033170751 -0.00324903 -0.00315767 -0.003050958 -0.002945235 -0.0028718219 -0.0028623492 -0.0029389833 -0.0030704245 -0.0032090626 -0.0033123773 -0.0033705644 -0.0033950682 -0.0034004964][-0.0034028008 -0.0033941055 -0.0033796753 -0.0033575548 -0.0033268253 -0.0032917231 -0.0032644069 -0.003257876 -0.0032804008 -0.0033209191 -0.0033619732 -0.0033879096 -0.0033988627 -0.0034019009 -0.0034016969][-0.0034043298 -0.00340348 -0.0034030022 -0.0034015274 -0.0033970738 -0.0033903914 -0.0033841555 -0.0033812362 -0.0033838495 -0.0033904947 -0.0033980836 -0.0034020843 -0.0034024627 -0.0034018198 -0.0034013593]]...]
INFO - root - 2017-12-09 23:41:08.733158: step 70510, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 60h:58m:47s remains)
INFO - root - 2017-12-09 23:41:17.256382: step 70520, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:01m:37s remains)
INFO - root - 2017-12-09 23:41:25.946391: step 70530, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:51m:33s remains)
INFO - root - 2017-12-09 23:41:34.647805: step 70540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 64h:52m:30s remains)
INFO - root - 2017-12-09 23:41:43.401401: step 70550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 62h:36m:46s remains)
INFO - root - 2017-12-09 23:41:52.042903: step 70560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:19m:45s remains)
INFO - root - 2017-12-09 23:42:00.841829: step 70570, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 64h:36m:28s remains)
INFO - root - 2017-12-09 23:42:09.430777: step 70580, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-09 23:42:17.800102: step 70590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:44m:36s remains)
INFO - root - 2017-12-09 23:42:26.131418: step 70600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:25m:56s remains)
2017-12-09 23:42:26.958409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033506581 -0.0033477603 -0.0033475154 -0.0033478497 -0.0033484797 -0.0033492865 -0.003350314 -0.0033514216 -0.0033524404 -0.0033532337 -0.0033535387 -0.00335306 -0.003352202 -0.0033511913 -0.0033500798][-0.0033473265 -0.0033440811 -0.0033438967 -0.0033445165 -0.0033456301 -0.0033470178 -0.0033488851 -0.0033511394 -0.0033536498 -0.0033560516 -0.0033575359 -0.0033574356 -0.0033561462 -0.0033541333 -0.0033516339][-0.0033461624 -0.0033425444 -0.0033423514 -0.0033431649 -0.0033447829 -0.0033468318 -0.0033497703 -0.0033533974 -0.0033575203 -0.0033617821 -0.0033650105 -0.0033659341 -0.00336476 -0.0033620624 -0.0033580833][-0.00334479 -0.0033409589 -0.0033407314 -0.0033417137 -0.003343815 -0.0033467379 -0.0033509322 -0.0033560367 -0.0033620091 -0.0033680932 -0.0033730266 -0.0033750695 -0.0033741624 -0.0033709991 -0.0033655872][-0.0033437021 -0.0033395169 -0.0033390145 -0.0033399884 -0.0033423784 -0.0033460769 -0.0033514034 -0.0033578349 -0.0033652864 -0.0033730019 -0.0033792092 -0.0033822153 -0.003381778 -0.0033784332 -0.0033721204][-0.0033429521 -0.0033380333 -0.0033370741 -0.0033379111 -0.0033404755 -0.0033444974 -0.0033504113 -0.0033576242 -0.0033657581 -0.0033743475 -0.003381317 -0.0033852968 -0.003385647 -0.003382622 -0.0033761158][-0.0033430585 -0.0033371029 -0.0033354782 -0.0033359034 -0.0033383651 -0.0033423472 -0.0033478888 -0.0033548719 -0.0033626826 -0.0033705169 -0.0033767526 -0.003380955 -0.0033819848 -0.0033797724 -0.0033743987][-0.0033448837 -0.0033374396 -0.0033348312 -0.0033347444 -0.0033369635 -0.0033406229 -0.0033454241 -0.0033512576 -0.003357718 -0.0033639718 -0.0033690103 -0.0033726871 -0.0033739076 -0.0033725498 -0.0033687649][-0.0033486767 -0.0033395486 -0.0033355958 -0.0033346915 -0.00333631 -0.0033393337 -0.0033430911 -0.0033475282 -0.0033521103 -0.00335648 -0.0033599469 -0.003362895 -0.0033642361 -0.0033637485 -0.0033615339][-0.0033540784 -0.003343049 -0.0033373584 -0.0033351718 -0.0033358589 -0.0033379018 -0.0033406164 -0.0033435915 -0.0033462951 -0.0033491421 -0.0033514667 -0.0033538779 -0.0033554318 -0.0033561348 -0.0033554919][-0.0033601988 -0.0033479866 -0.0033406983 -0.003337068 -0.003336614 -0.0033375944 -0.0033395002 -0.0033415367 -0.0033429966 -0.0033444145 -0.0033457503 -0.003347812 -0.0033492993 -0.0033506064 -0.0033511233][-0.0033655032 -0.003352751 -0.0033442979 -0.0033393193 -0.0033378352 -0.003337933 -0.0033392475 -0.0033410119 -0.0033420413 -0.0033427319 -0.0033437619 -0.0033457649 -0.0033470748 -0.0033483792 -0.0033491349][-0.0033680955 -0.0033559448 -0.0033474052 -0.0033416743 -0.0033392676 -0.0033387062 -0.0033396711 -0.0033412366 -0.0033422322 -0.003343031 -0.0033442276 -0.0033462222 -0.003347571 -0.0033488623 -0.0033494129][-0.0033676897 -0.0033568132 -0.0033489934 -0.0033433489 -0.0033405761 -0.0033396529 -0.0033403791 -0.0033418112 -0.0033428061 -0.0033438539 -0.0033450313 -0.0033467331 -0.0033478062 -0.0033487785 -0.003349199][-0.0033649846 -0.0033560614 -0.0033498392 -0.003344862 -0.0033422357 -0.003341113 -0.0033415006 -0.0033426019 -0.0033435591 -0.003344665 -0.0033455708 -0.0033468716 -0.0033476516 -0.003348113 -0.0033482728]]...]
INFO - root - 2017-12-09 23:42:35.556378: step 70610, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:13m:49s remains)
INFO - root - 2017-12-09 23:42:43.988331: step 70620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:34m:38s remains)
INFO - root - 2017-12-09 23:42:52.664141: step 70630, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:14m:45s remains)
INFO - root - 2017-12-09 23:43:01.205548: step 70640, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 61h:45m:37s remains)
INFO - root - 2017-12-09 23:43:09.832969: step 70650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:35m:23s remains)
INFO - root - 2017-12-09 23:43:18.482851: step 70660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:14m:08s remains)
INFO - root - 2017-12-09 23:43:27.058507: step 70670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 61h:37m:02s remains)
INFO - root - 2017-12-09 23:43:35.640980: step 70680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:06m:09s remains)
INFO - root - 2017-12-09 23:43:44.113519: step 70690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:20m:31s remains)
INFO - root - 2017-12-09 23:43:52.659227: step 70700, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 65h:20m:23s remains)
2017-12-09 23:43:53.527673: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42836535 0.41979024 0.40589708 0.38687566 0.36628461 0.34649339 0.33209592 0.322948 0.32168353 0.32784942 0.33825842 0.35069183 0.36058682 0.36411384 0.3595655][0.42865089 0.42784527 0.4220891 0.41148132 0.39840034 0.38486457 0.37484917 0.368178 0.36853984 0.37463477 0.38351956 0.39218572 0.39770666 0.39720964 0.38802186][0.42156193 0.42990187 0.43412274 0.43405291 0.42997241 0.42473504 0.42107114 0.41741896 0.41847685 0.42219263 0.42719474 0.4297699 0.42838433 0.42182064 0.40660733][0.41409189 0.43149909 0.44602069 0.45717719 0.46268824 0.46542791 0.46661451 0.46626732 0.46689367 0.46751758 0.46834925 0.46468434 0.45680249 0.44305548 0.42151651][0.40713394 0.43379688 0.45751378 0.4779945 0.49218708 0.50195956 0.50735873 0.50854707 0.50764775 0.50516987 0.50071335 0.49078488 0.47635287 0.45607081 0.42934132][0.39881226 0.43235514 0.46376136 0.49157065 0.51203448 0.52703422 0.5354349 0.5374909 0.53451455 0.52854657 0.5191468 0.50319743 0.48307481 0.45794982 0.42765796][0.38873231 0.42720649 0.46282715 0.49534196 0.52064741 0.53891218 0.54888982 0.55113506 0.54612005 0.53725344 0.52409309 0.50442189 0.48038647 0.45183435 0.41960129][0.37392029 0.41408846 0.45065838 0.4849886 0.512882 0.53335613 0.54424471 0.54762459 0.54244125 0.53214633 0.516157 0.49381691 0.4673925 0.43637612 0.40313673][0.35311168 0.39279145 0.42833796 0.46192536 0.49102724 0.51273495 0.52446896 0.52818859 0.52299815 0.5127725 0.49529713 0.472172 0.44538996 0.41416878 0.38188392][0.33030078 0.36647379 0.39811957 0.42871454 0.45685557 0.47815076 0.49052173 0.49458483 0.4902243 0.48053053 0.46283084 0.44030884 0.41435382 0.38563219 0.35631707][0.3040998 0.33542636 0.36204031 0.38832256 0.41351965 0.43339288 0.44590357 0.45113367 0.44877583 0.4406758 0.42495281 0.40508381 0.38181397 0.35629737 0.33043572][0.27875596 0.30450729 0.32523286 0.34656608 0.3679688 0.38466865 0.39602759 0.40194654 0.40121686 0.39526606 0.38253698 0.36680362 0.34802973 0.32674536 0.30522034][0.25454283 0.27406994 0.28885296 0.30480039 0.32163516 0.33502087 0.34471351 0.35028917 0.35072458 0.34679723 0.33716822 0.32540789 0.31122932 0.29504585 0.27838224][0.23306242 0.24799213 0.257899 0.26892498 0.28067106 0.29028064 0.29765514 0.30184785 0.30224451 0.2995005 0.29321384 0.28537297 0.27572495 0.26479307 0.253348][0.21652645 0.22765194 0.23342016 0.24017492 0.24752632 0.25296563 0.25723553 0.26012605 0.26051119 0.25872973 0.2547856 0.25021815 0.24440588 0.23773651 0.23058911]]...]
INFO - root - 2017-12-09 23:44:02.158914: step 70710, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 61h:59m:45s remains)
INFO - root - 2017-12-09 23:44:10.618449: step 70720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:10m:08s remains)
INFO - root - 2017-12-09 23:44:19.249803: step 70730, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 61h:40m:37s remains)
INFO - root - 2017-12-09 23:44:27.952832: step 70740, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:48m:21s remains)
INFO - root - 2017-12-09 23:44:36.712260: step 70750, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 63h:39m:08s remains)
INFO - root - 2017-12-09 23:44:45.460896: step 70760, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:33m:32s remains)
INFO - root - 2017-12-09 23:44:54.088213: step 70770, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.829 sec/batch; 60h:16m:37s remains)
INFO - root - 2017-12-09 23:45:02.781550: step 70780, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:43m:06s remains)
INFO - root - 2017-12-09 23:45:11.236208: step 70790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 64h:26m:26s remains)
INFO - root - 2017-12-09 23:45:19.664858: step 70800, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 63h:18m:20s remains)
2017-12-09 23:45:20.637600: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11938301 0.087720811 0.058023382 0.035690945 0.021305097 0.01365605 0.0097635426 0.0072366688 0.004747699 0.0022765545 2.4853973e-05 -0.0017521536 -0.0028777444 -0.0033337362 -0.003381565][0.11660438 0.091085449 0.06705448 0.048326317 0.034919385 0.026234258 0.020476576 0.015827214 0.011219835 0.006744259 0.0027460905 -0.00038073608 -0.0023550852 -0.0032206303 -0.0033782958][0.11449498 0.0976902 0.081444435 0.06784045 0.056447227 0.047418963 0.039829317 0.03237883 0.024390021 0.016236547 0.0087474911 0.0027549749 -0.0011371667 -0.0029241941 -0.0033682135][0.11607603 0.10851825 0.1003057 0.092212811 0.083407305 0.074674316 0.065634772 0.055351742 0.043401018 0.03054527 0.018237628 0.0079799108 0.0010125318 -0.0023535383 -0.0033392026][0.11904511 0.12109049 0.12100441 0.11866084 0.11302597 0.10547585 0.095532238 0.082398348 0.065999873 0.047719806 0.029806716 0.014566552 0.0038818631 -0.0015213734 -0.0032834928][0.11895917 0.12855466 0.13500591 0.13806668 0.13648564 0.13170709 0.12258898 0.10814895 0.088356324 0.06528005 0.042009525 0.021715561 0.0070750248 -0.00055883941 -0.0032108342][0.11251676 0.12643529 0.13673091 0.14316005 0.14482577 0.14332622 0.13662666 0.12316565 0.10260863 0.077342272 0.050980229 0.027325278 0.0097384918 0.00028801197 -0.0031394137][0.098033838 0.1133339 0.12506385 0.13297592 0.13674776 0.13779454 0.13382649 0.12287182 0.10417242 0.079905413 0.053699344 0.029523933 0.010989156 0.000737719 -0.0031047361][0.076895677 0.0910365 0.10221519 0.11018209 0.11503372 0.11785787 0.11655829 0.10887148 0.093825273 0.073105536 0.049839392 0.027775103 0.010409301 0.0006239207 -0.0031116407][0.053447053 0.064572029 0.0736744 0.08054968 0.085616574 0.089575224 0.0906034 0.086434007 0.075923949 0.060106926 0.041423365 0.023148695 0.008395764 1.2547011e-05 -0.0031634329][0.032327075 0.03978616 0.046045046 0.051014524 0.05545992 0.059749074 0.062383041 0.061268106 0.05510762 0.044429738 0.030838544 0.017028818 0.0056448421 -0.00085064094 -0.0032336777][0.015934076 0.020287827 0.024086379 0.027397914 0.030953499 0.034725953 0.037779462 0.038450696 0.035524596 0.02914477 0.020166352 0.010682 0.00276374 -0.0017403133 -0.0032942607][0.0051335972 0.0072267298 0.0092679523 0.01141038 0.014156573 0.017261207 0.020087732 0.021341126 0.020185094 0.016673872 0.011201943 0.0052338494 0.0002855768 -0.0024770729 -0.0033364939][-0.00049403566 0.00022638077 0.0010765854 0.0022684846 0.0040967651 0.0063747587 0.0086492142 0.0099744461 0.0097679952 0.0080010677 0.00489078 0.0013658151 -0.001463179 -0.0029605879 -0.0033550032][-0.0024904308 -0.0023952373 -0.0021943594 -0.0016942575 -0.00073808827 0.00062746392 0.0021000116 0.003127814 0.0033264311 0.0025838187 0.0010011762 -0.000948919 -0.0024628961 -0.0032024675 -0.003363753]]...]
INFO - root - 2017-12-09 23:45:29.235093: step 70810, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:04m:00s remains)
INFO - root - 2017-12-09 23:45:37.508683: step 70820, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 60h:29m:17s remains)
INFO - root - 2017-12-09 23:45:46.055259: step 70830, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 60h:55m:09s remains)
INFO - root - 2017-12-09 23:45:54.564450: step 70840, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.812 sec/batch; 59h:00m:57s remains)
INFO - root - 2017-12-09 23:46:03.021340: step 70850, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:44m:20s remains)
INFO - root - 2017-12-09 23:46:11.588107: step 70860, loss = 0.89, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 58h:57m:59s remains)
INFO - root - 2017-12-09 23:46:20.280463: step 70870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:00m:58s remains)
INFO - root - 2017-12-09 23:46:28.947701: step 70880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:19m:49s remains)
INFO - root - 2017-12-09 23:46:37.348093: step 70890, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:38m:13s remains)
INFO - root - 2017-12-09 23:46:45.801048: step 70900, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 60h:40m:58s remains)
2017-12-09 23:46:46.736968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033919821 -0.0033904249 -0.0033910468 -0.0033919488 -0.0033927178 -0.003392729 -0.0033920761 -0.0033915695 -0.0033911997 -0.0033901765 -0.0033889196 -0.00338824 -0.0033881695 -0.0033879827 -0.0033878868][-0.00339048 -0.0033889271 -0.0033896824 -0.0033910607 -0.0033919711 -0.0033921443 -0.003391468 -0.0033911173 -0.0033907907 -0.0033897369 -0.0033883553 -0.0033869061 -0.0033862146 -0.0033859371 -0.0033857967][-0.0033904545 -0.003389087 -0.0033900822 -0.0033915401 -0.0033927173 -0.0033929511 -0.0033922764 -0.0033920247 -0.0033916419 -0.0033911134 -0.0033899387 -0.0033881289 -0.0033870083 -0.0033863618 -0.0033859622][-0.0033906074 -0.0033895897 -0.0033905993 -0.0033918042 -0.0033924978 -0.0033907189 -0.0033878852 -0.003386688 -0.0033867822 -0.0033884593 -0.0033897581 -0.0033894323 -0.0033886146 -0.0033877871 -0.003386901][-0.0033907122 -0.0033899541 -0.0033911071 -0.0033920107 -0.0033902328 -0.0033828185 -0.003372567 -0.0033664692 -0.0033687903 -0.0033768658 -0.0033843147 -0.0033884025 -0.0033896195 -0.0033893138 -0.0033880682][-0.0033905711 -0.0033901301 -0.0033910777 -0.0033909502 -0.0033841091 -0.0033655404 -0.003340099 -0.0033234975 -0.003330552 -0.0033525326 -0.0033736713 -0.0033857792 -0.0033896547 -0.0033901688 -0.003389284][-0.0033901099 -0.0033901371 -0.0033908982 -0.0033892426 -0.0033759263 -0.0033430383 -0.0032973047 -0.0032669527 -0.0032804022 -0.0033219093 -0.0033609539 -0.0033823373 -0.0033892654 -0.003390363 -0.0033895813][-0.0033898477 -0.003389888 -0.0033910277 -0.0033876756 -0.0033700687 -0.0033283299 -0.0032694661 -0.0032292525 -0.0032469439 -0.0033008633 -0.0033509545 -0.0033790467 -0.0033887948 -0.0033903567 -0.003389756][-0.0033898109 -0.003389362 -0.0033905089 -0.0033869639 -0.0033705311 -0.0033323278 -0.0032799749 -0.003244811 -0.0032579708 -0.00330497 -0.0033508614 -0.0033784793 -0.0033887937 -0.0033906538 -0.0033902286][-0.0033897662 -0.0033884493 -0.0033897224 -0.003387782 -0.003377297 -0.0033527086 -0.0033204539 -0.0032986172 -0.0033037413 -0.0033318279 -0.0033620237 -0.003381524 -0.0033891292 -0.003390613 -0.0033901122][-0.0033896358 -0.0033876402 -0.0033889033 -0.0033892216 -0.0033848975 -0.0033743212 -0.0033611946 -0.0033512379 -0.0033513317 -0.0033622244 -0.003376052 -0.0033859983 -0.0033902894 -0.003390908 -0.0033898603][-0.0033896859 -0.0033872356 -0.0033883136 -0.0033898312 -0.0033897094 -0.0033868512 -0.0033837392 -0.0033808292 -0.003379259 -0.003381503 -0.0033857885 -0.0033894281 -0.0033906961 -0.0033902442 -0.0033891795][-0.003389593 -0.00338691 -0.0033876712 -0.0033893606 -0.0033905627 -0.0033906326 -0.0033906035 -0.0033902817 -0.0033892447 -0.0033893015 -0.0033901718 -0.0033909893 -0.0033905406 -0.0033894854 -0.00338869][-0.003389468 -0.0033867087 -0.0033869462 -0.0033882423 -0.0033894202 -0.0033900412 -0.003390308 -0.0033904365 -0.0033905192 -0.003390518 -0.0033904337 -0.0033900936 -0.0033893005 -0.0033886305 -0.0033882253][-0.0033893895 -0.0033866316 -0.0033865052 -0.0033872393 -0.0033880339 -0.0033884342 -0.0033885743 -0.0033889019 -0.0033892952 -0.00338909 -0.0033886698 -0.0033883411 -0.0033879948 -0.0033877932 -0.0033876731]]...]
INFO - root - 2017-12-09 23:46:55.155543: step 70910, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 60h:27m:10s remains)
INFO - root - 2017-12-09 23:47:03.459736: step 70920, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 59h:18m:08s remains)
INFO - root - 2017-12-09 23:47:11.965833: step 70930, loss = 0.90, batch loss = 0.70 (9.7 examples/sec; 0.823 sec/batch; 59h:49m:26s remains)
INFO - root - 2017-12-09 23:47:20.499653: step 70940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:34m:55s remains)
INFO - root - 2017-12-09 23:47:29.032966: step 70950, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 63h:18m:41s remains)
INFO - root - 2017-12-09 23:47:37.586170: step 70960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 64h:39m:12s remains)
INFO - root - 2017-12-09 23:47:46.203501: step 70970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:13m:54s remains)
INFO - root - 2017-12-09 23:47:54.958205: step 70980, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:03m:21s remains)
INFO - root - 2017-12-09 23:48:03.420526: step 70990, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:46m:04s remains)
INFO - root - 2017-12-09 23:48:11.675489: step 71000, loss = 0.89, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 58h:48m:25s remains)
2017-12-09 23:48:12.586674: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20139736 0.22087289 0.23585019 0.2441445 0.24848419 0.24851531 0.24471931 0.24040526 0.23580648 0.23376608 0.2325266 0.23491116 0.23924099 0.24538779 0.24984312][0.24044646 0.26563713 0.28472424 0.29587287 0.30196258 0.30307531 0.30085325 0.29688793 0.29334447 0.29214326 0.29308391 0.29832047 0.30491373 0.31335291 0.31983122][0.25260919 0.28013486 0.3013579 0.3164112 0.32584485 0.33031255 0.33171746 0.33135846 0.33168608 0.33261925 0.33596215 0.34401575 0.35330507 0.36403766 0.37209108][0.24097033 0.27024603 0.29367366 0.31102157 0.32303363 0.33196625 0.33948627 0.34419435 0.34883076 0.35304621 0.36033261 0.37132305 0.38226727 0.39505377 0.40489045][0.21111016 0.24158441 0.26733273 0.28747389 0.30302945 0.31749329 0.330364 0.3410168 0.35092583 0.35936219 0.37010425 0.38237336 0.39541778 0.40958589 0.42090106][0.17114055 0.19910984 0.22391979 0.24626486 0.2653355 0.2857126 0.30547491 0.32416376 0.33996978 0.35320207 0.36753571 0.38027859 0.3944613 0.408664 0.42067528][0.12657446 0.14870641 0.16907951 0.1900373 0.21033213 0.2335078 0.25761816 0.28299206 0.30565259 0.32497606 0.34412554 0.35990036 0.37645489 0.39059681 0.4042474][0.084736817 0.098986261 0.11262026 0.12827316 0.14567721 0.16762276 0.19261317 0.22048275 0.24624854 0.27081212 0.29439911 0.31486455 0.33520573 0.35197711 0.36859831][0.048864912 0.056449022 0.063446596 0.073318996 0.085085176 0.10168985 0.12120675 0.14621145 0.17107961 0.19631825 0.22156978 0.24565496 0.27033728 0.29082811 0.31098527][0.022788327 0.026949579 0.030690087 0.035393037 0.041688427 0.052055024 0.064560242 0.08197502 0.099759385 0.12091027 0.14296192 0.16630538 0.19160274 0.21450102 0.23812778][0.0070380224 0.0094233565 0.011392103 0.014419682 0.017828092 0.023176918 0.02896616 0.038655415 0.049384229 0.063135982 0.0782979 0.096931048 0.11833917 0.13976535 0.16257024][-0.0011044918 0.00015600235 0.001551375 0.0030382962 0.0049762148 0.0076475227 0.010489768 0.015056288 0.019880595 0.027564578 0.036539219 0.049028408 0.063888781 0.08070273 0.09919773][-0.0031156424 -0.0028612898 -0.0027079494 -0.0022834083 -0.0015062997 -0.0004039437 0.00074041612 0.003073626 0.0052701226 0.0089796763 0.013175437 0.019586008 0.027604597 0.037631121 0.049734354][-0.0033564002 -0.0033473156 -0.0033365309 -0.0033122846 -0.0032484485 -0.0032192669 -0.0030207941 -0.0025304463 -0.00167153 -0.00036874157 0.001424812 0.0042458689 0.0075913062 0.012200061 0.017484667][-0.0033660519 -0.0033627362 -0.0033576293 -0.0033481077 -0.003338248 -0.0033325418 -0.0033282305 -0.0033265136 -0.0033283355 -0.0032595838 -0.0029535561 -0.0023387196 -0.0013661988 -3.4102006e-05 0.0015569597]]...]
INFO - root - 2017-12-09 23:48:20.971782: step 71010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:31m:07s remains)
INFO - root - 2017-12-09 23:48:29.433317: step 71020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:51m:02s remains)
INFO - root - 2017-12-09 23:48:38.072977: step 71030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:34m:36s remains)
INFO - root - 2017-12-09 23:48:46.723290: step 71040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:18m:36s remains)
INFO - root - 2017-12-09 23:48:55.436778: step 71050, loss = 0.89, batch loss = 0.69 (8.0 examples/sec; 0.994 sec/batch; 72h:11m:37s remains)
INFO - root - 2017-12-09 23:49:04.073235: step 71060, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 64h:52m:29s remains)
INFO - root - 2017-12-09 23:49:12.762582: step 71070, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 64h:35m:23s remains)
INFO - root - 2017-12-09 23:49:21.590612: step 71080, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 63h:10m:41s remains)
INFO - root - 2017-12-09 23:49:30.239830: step 71090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:57m:58s remains)
INFO - root - 2017-12-09 23:49:38.620103: step 71100, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:21m:14s remains)
2017-12-09 23:49:39.525654: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.040789858 0.056916308 0.076909378 0.10103734 0.12630385 0.14937812 0.1692401 0.18368839 0.1917624 0.19261698 0.18669662 0.173223 0.15295143 0.12876897 0.10346649][0.033987954 0.047098637 0.065340839 0.09081357 0.11951812 0.15093474 0.18080406 0.20413506 0.22006942 0.22520739 0.22179236 0.20765804 0.18570606 0.16026527 0.13430427][0.032511767 0.043074552 0.058995269 0.084548354 0.11535554 0.15440387 0.19174999 0.22456026 0.2492758 0.25990662 0.25979358 0.24551162 0.22302723 0.19580035 0.16858669][0.038082533 0.049345728 0.0658942 0.091520168 0.12364608 0.166665 0.2086461 0.24883127 0.27852452 0.2959561 0.29902697 0.28525975 0.26282281 0.23394896 0.20660588][0.0527338 0.067049205 0.08639349 0.11451974 0.14881887 0.19504632 0.24019243 0.28386721 0.31605229 0.33712128 0.3405869 0.32810846 0.30759922 0.27962425 0.2537263][0.077313565 0.098217353 0.12218179 0.15252532 0.18757817 0.23408346 0.278892 0.32303134 0.355373 0.37642309 0.37719408 0.36571759 0.34732142 0.3228521 0.30181634][0.10413595 0.13212349 0.16164884 0.19537556 0.231797 0.27706116 0.31992477 0.36135334 0.3905122 0.40872735 0.40577155 0.39295864 0.3746241 0.35334727 0.33626285][0.13074844 0.16440803 0.19818231 0.23394145 0.27053374 0.31249505 0.35105437 0.38734215 0.41071811 0.4241634 0.41790017 0.40354022 0.38444325 0.36436805 0.34901488][0.15161946 0.18969719 0.22667736 0.26311818 0.29806066 0.335014 0.36764336 0.39612824 0.4122918 0.4202747 0.41162255 0.3976599 0.37921241 0.3610642 0.34733197][0.16205315 0.20183033 0.2400623 0.2767368 0.30963194 0.3411983 0.36709863 0.38774452 0.39752278 0.39975253 0.38983673 0.37716228 0.36051759 0.34420374 0.33091342][0.15927766 0.19823396 0.23545574 0.2693539 0.29837409 0.3244386 0.34443945 0.35919398 0.36456507 0.36385456 0.35465351 0.3435007 0.32906392 0.3148731 0.30206606][0.14382516 0.17963047 0.21407916 0.24459979 0.26890454 0.28833368 0.30174822 0.3108288 0.31276685 0.31107348 0.30427867 0.29582447 0.28478304 0.27282754 0.26177561][0.11845268 0.14874919 0.17774804 0.20280111 0.2218395 0.23560841 0.24379885 0.24853668 0.24803692 0.24567029 0.24047688 0.23496109 0.22772281 0.2190562 0.21109495][0.086423635 0.1096505 0.13162571 0.15012379 0.16368993 0.17254527 0.17700027 0.17900178 0.17763978 0.17557693 0.17196156 0.16877195 0.16474761 0.15954018 0.1545524][0.055907719 0.071701415 0.086403765 0.098440856 0.10673303 0.11158221 0.11341926 0.11386667 0.1124697 0.1110998 0.10906017 0.10760733 0.10588191 0.10310946 0.10035833]]...]
INFO - root - 2017-12-09 23:49:48.192784: step 71110, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 62h:43m:50s remains)
INFO - root - 2017-12-09 23:49:56.629608: step 71120, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 60h:58m:26s remains)
INFO - root - 2017-12-09 23:50:05.024400: step 71130, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:00m:28s remains)
INFO - root - 2017-12-09 23:50:13.579346: step 71140, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:09m:29s remains)
INFO - root - 2017-12-09 23:50:22.071694: step 71150, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 60h:28m:06s remains)
INFO - root - 2017-12-09 23:50:30.701421: step 71160, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:26m:01s remains)
INFO - root - 2017-12-09 23:50:39.336091: step 71170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:30m:58s remains)
INFO - root - 2017-12-09 23:50:47.984466: step 71180, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 63h:38m:16s remains)
INFO - root - 2017-12-09 23:50:56.653827: step 71190, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 60h:50m:55s remains)
INFO - root - 2017-12-09 23:51:05.092257: step 71200, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:45m:43s remains)
2017-12-09 23:51:06.069498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033957395 -0.0033986715 -0.0034001283 -0.0033976417 -0.0033892035 -0.0033784341 -0.0033754781 -0.0033812649 -0.0033854614 -0.0033741917 -0.0033554144 -0.0033255073 -0.0032664591 -0.0031514158 -0.0030234361][-0.0033953269 -0.0033974287 -0.0033974857 -0.0033909369 -0.003373693 -0.0033545136 -0.0033482802 -0.0033532875 -0.0033603602 -0.0033543454 -0.0033413765 -0.0033185012 -0.0032612872 -0.00314639 -0.0030021351][-0.0033969667 -0.0033982147 -0.0033969409 -0.00338498 -0.0033548037 -0.0033160215 -0.0032911587 -0.0032939196 -0.0033114792 -0.0033215398 -0.0033222311 -0.0032986032 -0.0032425069 -0.0031483104 -0.0030328492][-0.0033979833 -0.0033980836 -0.0033935485 -0.0033685579 -0.0033120236 -0.0032406617 -0.003193889 -0.0031972695 -0.0032329655 -0.0032670975 -0.003286134 -0.0032632502 -0.0032008672 -0.003108707 -0.0030025044][-0.0033954477 -0.0033942079 -0.0033851189 -0.0033423447 -0.0032546509 -0.0031500352 -0.0030874573 -0.0030987607 -0.0031583018 -0.0032190303 -0.0032522802 -0.0032292686 -0.0031615321 -0.0030643877 -0.0029515459][-0.0033862314 -0.0033829585 -0.0033708932 -0.0033151107 -0.0032065306 -0.0030835471 -0.003019351 -0.0030486374 -0.0031277034 -0.0032038589 -0.0032419281 -0.0032192522 -0.0031490407 -0.0030456933 -0.0029342782][-0.0033810469 -0.0033744781 -0.003360274 -0.0033100769 -0.003209596 -0.0030920561 -0.0030315293 -0.0030660005 -0.0031473241 -0.0032285952 -0.0032673643 -0.0032461721 -0.0031803187 -0.0030839893 -0.002981286][-0.0033830465 -0.0033760185 -0.0033640016 -0.0033270344 -0.0032520005 -0.0031619733 -0.0031070851 -0.0031365848 -0.003208467 -0.0032853838 -0.0033222071 -0.0033069663 -0.0032520385 -0.0031771429 -0.003098015][-0.0033890575 -0.0033827752 -0.0033720252 -0.0033494472 -0.0033021043 -0.0032484827 -0.0032123455 -0.0032275415 -0.0032755281 -0.0033323518 -0.0033616773 -0.0033597308 -0.0033325416 -0.003290663 -0.0032402105][-0.0033870898 -0.0033863429 -0.0033832339 -0.0033728129 -0.003353222 -0.0033295737 -0.0033116934 -0.0033187307 -0.0033416112 -0.0033671912 -0.0033791657 -0.003381138 -0.0033765037 -0.0033641588 -0.0033435619][-0.0033831915 -0.0033819848 -0.0033811182 -0.0033787624 -0.0033742411 -0.0033669113 -0.0033604938 -0.0033609406 -0.0033668559 -0.0033735065 -0.0033768173 -0.0033789 -0.0033805093 -0.003380581 -0.0033770683][-0.0033802108 -0.0033786567 -0.003378069 -0.0033768655 -0.0033758159 -0.0033749244 -0.0033741416 -0.0033736126 -0.003373428 -0.0033736618 -0.0033745209 -0.0033756131 -0.0033766297 -0.0033775603 -0.0033785382][-0.0033786525 -0.0033768492 -0.003376313 -0.0033754755 -0.0033750378 -0.0033744306 -0.0033738317 -0.0033734608 -0.0033734292 -0.0033731428 -0.0033731747 -0.0033734264 -0.0033741023 -0.0033748059 -0.00337575][-0.0033783233 -0.0033763209 -0.0033759039 -0.0033753493 -0.0033748895 -0.0033744823 -0.0033739673 -0.0033733174 -0.0033730946 -0.0033729149 -0.0033729665 -0.0033729039 -0.0033732196 -0.003373604 -0.0033741344][-0.0033783538 -0.0033761319 -0.0033756008 -0.0033752515 -0.003374923 -0.0033742257 -0.0033736273 -0.003373197 -0.003372665 -0.0033725295 -0.0033726494 -0.0033725584 -0.0033726948 -0.0033728767 -0.0033732085]]...]
INFO - root - 2017-12-09 23:51:14.827101: step 71210, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:50m:17s remains)
INFO - root - 2017-12-09 23:51:23.591494: step 71220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:45m:39s remains)
INFO - root - 2017-12-09 23:51:32.210780: step 71230, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:05m:02s remains)
INFO - root - 2017-12-09 23:51:40.804061: step 71240, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:29m:37s remains)
INFO - root - 2017-12-09 23:51:49.425203: step 71250, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:14m:07s remains)
INFO - root - 2017-12-09 23:51:58.066716: step 71260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:39m:37s remains)
INFO - root - 2017-12-09 23:52:06.796194: step 71270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:44m:43s remains)
INFO - root - 2017-12-09 23:52:15.558748: step 71280, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 65h:11m:33s remains)
INFO - root - 2017-12-09 23:52:24.086739: step 71290, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 63h:45m:03s remains)
INFO - root - 2017-12-09 23:52:32.800416: step 71300, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:36m:46s remains)
2017-12-09 23:52:33.734891: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034683295 0.042660628 0.050015591 0.054627486 0.055758327 0.053460538 0.047520351 0.040123131 0.031837832 0.024942983 0.019003812 0.014390504 0.011054889 0.0093076155 0.0088966116][0.041359264 0.053505074 0.065641917 0.075011261 0.080535546 0.081296049 0.076699488 0.06769181 0.055746872 0.044058163 0.033069693 0.023754049 0.016676107 0.012607323 0.011171716][0.049925771 0.067064166 0.085044377 0.10171165 0.11399487 0.11987317 0.11875211 0.11025254 0.0962141 0.079852834 0.062721893 0.047085244 0.034543335 0.026413508 0.022768835][0.06601958 0.088927187 0.11275195 0.13602053 0.15382946 0.16535211 0.1683559 0.16180642 0.14768966 0.12886439 0.10765062 0.086470515 0.0682245 0.055567026 0.048767839][0.088797025 0.11816514 0.14793618 0.17681475 0.19967595 0.21626367 0.22346617 0.21991506 0.20684496 0.18677707 0.16246891 0.1367158 0.11361471 0.096900649 0.087044284][0.11389389 0.15019464 0.18604143 0.21970788 0.24632643 0.26662016 0.27714241 0.27705 0.26611888 0.24655859 0.22083099 0.19206573 0.16515277 0.14451329 0.13171685][0.13388972 0.17612813 0.21722144 0.25455409 0.28400061 0.30685034 0.31990635 0.32317597 0.3153736 0.29834196 0.27386543 0.2450563 0.21671474 0.19381173 0.17876126][0.14655058 0.19162148 0.23477069 0.27308485 0.30333582 0.32726419 0.34208435 0.34869611 0.34487078 0.33195269 0.31080693 0.28462926 0.25766608 0.2347232 0.21848007][0.14894129 0.19402748 0.2368733 0.27382952 0.30293128 0.32590291 0.34115565 0.34986475 0.34942943 0.34103289 0.32441485 0.3025423 0.278694 0.257593 0.24178089][0.13878739 0.18128063 0.22168685 0.25581971 0.28279942 0.30386561 0.31862333 0.32808405 0.33002532 0.32531941 0.31281573 0.29522532 0.27497405 0.25623021 0.24142981][0.11764535 0.15522972 0.19116835 0.22137161 0.24520257 0.26373264 0.27723616 0.28626281 0.28940532 0.28712079 0.27802131 0.26413721 0.247179 0.23090242 0.21740794][0.089672163 0.11985107 0.14908637 0.1738745 0.1934929 0.20872121 0.2201997 0.22823198 0.23182489 0.23131733 0.22509618 0.21466178 0.20123298 0.18791415 0.17637645][0.061169125 0.083054222 0.10449825 0.12284017 0.13738969 0.14864458 0.15726189 0.16339403 0.16642134 0.16663082 0.16254956 0.1552166 0.1454028 0.13538939 0.12654842][0.035983838 0.050259747 0.064491913 0.07683371 0.08661636 0.094108731 0.099831164 0.10384782 0.10584071 0.10601243 0.10330416 0.0983676 0.091623761 0.084629409 0.07846351][0.016903318 0.025050672 0.033353467 0.04078687 0.046742085 0.051330984 0.054763526 0.057123724 0.05824725 0.058227241 0.056428976 0.053251334 0.049016666 0.044554539 0.040644586]]...]
INFO - root - 2017-12-09 23:52:42.451146: step 71310, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:59m:08s remains)
INFO - root - 2017-12-09 23:52:50.973117: step 71320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:15m:54s remains)
INFO - root - 2017-12-09 23:52:59.687879: step 71330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:04m:49s remains)
INFO - root - 2017-12-09 23:53:08.385085: step 71340, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:31m:36s remains)
INFO - root - 2017-12-09 23:53:17.098791: step 71350, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 65h:43m:28s remains)
INFO - root - 2017-12-09 23:53:25.692114: step 71360, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:39m:00s remains)
INFO - root - 2017-12-09 23:53:34.324769: step 71370, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:28m:01s remains)
INFO - root - 2017-12-09 23:53:43.004571: step 71380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:28m:56s remains)
INFO - root - 2017-12-09 23:53:51.722966: step 71390, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:17m:59s remains)
INFO - root - 2017-12-09 23:54:00.464733: step 71400, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 64h:48m:10s remains)
2017-12-09 23:54:01.416234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033623816 -0.0033620156 -0.0033651676 -0.0033686345 -0.0033718 -0.0033747121 -0.0033764364 -0.00337633 -0.0033751004 -0.0033734017 -0.0033713859 -0.0033697004 -0.003369051 -0.0033687085 -0.0033683616][-0.0033621192 -0.0033620619 -0.0033659679 -0.0033698911 -0.0033733295 -0.0033762692 -0.0033778935 -0.0033774721 -0.0033759996 -0.003374276 -0.0033723088 -0.0033712429 -0.0033714138 -0.0033719235 -0.0033722785][-0.0033628643 -0.0033631623 -0.0033674014 -0.0033713104 -0.0033744033 -0.0033768897 -0.0033777943 -0.0033768846 -0.0033752644 -0.0033739004 -0.0033727679 -0.0033729635 -0.0033745971 -0.0033765805 -0.0033782283][-0.0033618179 -0.0033622421 -0.0033666748 -0.0033705307 -0.0033733612 -0.0033752185 -0.0033751524 -0.0033734783 -0.0033714622 -0.0033702962 -0.0033702245 -0.0033721905 -0.0033760413 -0.0033800465 -0.0033833706][-0.0033599595 -0.0033603569 -0.0033648065 -0.0033685712 -0.0033710324 -0.0033722771 -0.0033713921 -0.0033691355 -0.0033667167 -0.0033657323 -0.0033665346 -0.0033699905 -0.0033760658 -0.003382179 -0.0033875611][-0.0033567082 -0.003356809 -0.0033610987 -0.0033649183 -0.0033677057 -0.003369058 -0.0033681986 -0.003366095 -0.0033640761 -0.003363438 -0.0033647469 -0.0033692326 -0.003376547 -0.0033840879 -0.0033907988][-0.0033531287 -0.0033527061 -0.003356477 -0.0033605716 -0.0033640412 -0.0033663318 -0.0033665402 -0.0033654473 -0.0033646137 -0.0033646564 -0.0033666231 -0.0033714906 -0.0033788977 -0.0033867273 -0.0033938834][-0.0033506216 -0.0033494022 -0.0033525524 -0.0033567245 -0.0033611576 -0.00336496 -0.0033672675 -0.0033682841 -0.003368987 -0.003369831 -0.0033719793 -0.0033763933 -0.0033829408 -0.0033899958 -0.0033965257][-0.003349558 -0.0033472197 -0.0033493752 -0.003353267 -0.0033583373 -0.0033635667 -0.0033686985 -0.003372821 -0.0033755235 -0.0033773789 -0.0033793962 -0.0033829869 -0.0033876898 -0.0033927131 -0.003397621][-0.0033492334 -0.0033459282 -0.00334714 -0.0033500986 -0.0033548495 -0.0033609539 -0.0033683593 -0.0033753188 -0.0033805126 -0.0033839052 -0.0033861308 -0.0033887345 -0.0033910524 -0.003393285 -0.0033960082][-0.0033494658 -0.0033450425 -0.0033454767 -0.0033472241 -0.0033510963 -0.0033573667 -0.0033657791 -0.0033744823 -0.0033815498 -0.0033864293 -0.0033891064 -0.003390756 -0.0033909008 -0.0033909103 -0.0033915776][-0.0033498367 -0.0033447915 -0.0033443067 -0.0033449735 -0.0033478397 -0.0033533426 -0.0033610964 -0.0033697346 -0.0033773016 -0.0033828025 -0.0033859357 -0.0033872698 -0.0033865112 -0.0033851687 -0.0033843464][-0.0033505564 -0.0033452683 -0.0033442785 -0.0033440888 -0.0033459044 -0.0033500493 -0.0033563362 -0.0033636657 -0.0033704985 -0.0033759871 -0.0033792495 -0.0033805773 -0.003379622 -0.0033776753 -0.0033760373][-0.0033516362 -0.0033464483 -0.0033452862 -0.0033447316 -0.003345567 -0.003348269 -0.0033525503 -0.0033579753 -0.0033633863 -0.0033680508 -0.0033712641 -0.0033726012 -0.0033720124 -0.0033702441 -0.0033683304][-0.0033520698 -0.0033473924 -0.0033465466 -0.0033460238 -0.0033463754 -0.0033478176 -0.0033501573 -0.0033534884 -0.0033571685 -0.0033605229 -0.003363146 -0.0033646757 -0.0033646466 -0.0033635271 -0.0033620002]]...]
INFO - root - 2017-12-09 23:54:10.126665: step 71410, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 63h:45m:08s remains)
INFO - root - 2017-12-09 23:54:18.665769: step 71420, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:22m:53s remains)
INFO - root - 2017-12-09 23:54:27.289507: step 71430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:33m:16s remains)
INFO - root - 2017-12-09 23:54:36.008627: step 71440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:17m:09s remains)
INFO - root - 2017-12-09 23:54:44.670257: step 71450, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 59h:50m:03s remains)
INFO - root - 2017-12-09 23:54:53.357672: step 71460, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:20m:30s remains)
INFO - root - 2017-12-09 23:55:01.943081: step 71470, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 60h:26m:33s remains)
INFO - root - 2017-12-09 23:55:10.765271: step 71480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:09m:17s remains)
INFO - root - 2017-12-09 23:55:19.265219: step 71490, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:59m:01s remains)
INFO - root - 2017-12-09 23:55:27.745502: step 71500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 61h:26m:17s remains)
2017-12-09 23:55:28.624034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033681008 -0.003365383 -0.0033648314 -0.0033645071 -0.00336427 -0.0033642072 -0.0033641967 -0.00336435 -0.0033647062 -0.0033650084 -0.00336525 -0.0033653721 -0.0033653516 -0.0033653451 -0.0033653341][-0.0033664762 -0.00336358 -0.0033629728 -0.0033624885 -0.0033620677 -0.003361797 -0.0033615918 -0.0033616268 -0.0033620317 -0.0033626056 -0.003363112 -0.0033634347 -0.0033636135 -0.0033636757 -0.003363682][-0.0033666953 -0.0033637094 -0.003363095 -0.003362458 -0.0033617886 -0.0033613192 -0.0033609534 -0.0033608673 -0.00336125 -0.0033619381 -0.003362614 -0.0033630694 -0.0033633842 -0.0033635532 -0.0033636149][-0.0033666848 -0.0033637655 -0.0033632002 -0.0033626 -0.003361789 -0.0033611786 -0.0033607997 -0.0033605702 -0.003360762 -0.0033613876 -0.0033620698 -0.0033625523 -0.0033629907 -0.003363363 -0.0033635572][-0.003366509 -0.0033636147 -0.0033632517 -0.0033628745 -0.0033621457 -0.0033615273 -0.0033612119 -0.0033609769 -0.0033609176 -0.0033612477 -0.003361685 -0.0033619946 -0.0033624703 -0.0033630666 -0.0033634463][-0.0033662145 -0.0033633953 -0.0033635159 -0.0033638575 -0.0033635488 -0.0033629725 -0.0033626172 -0.0033624112 -0.0033621544 -0.0033619879 -0.0033618088 -0.003361675 -0.0033621017 -0.0033628389 -0.0033633448][-0.0033657625 -0.0033630661 -0.0033636231 -0.0033647877 -0.00336531 -0.0033647339 -0.0033641374 -0.0033640761 -0.0033639101 -0.0033633434 -0.0033623737 -0.0033617232 -0.0033618899 -0.0033626331 -0.0033632421][-0.0033652496 -0.0033628126 -0.0033637239 -0.0033654582 -0.0033668752 -0.0033666629 -0.0033656545 -0.003365333 -0.0033653593 -0.0033647071 -0.0033631611 -0.0033620107 -0.0033618808 -0.0033624847 -0.0033631239][-0.0033647856 -0.003362553 -0.0033637176 -0.0033656792 -0.0033677053 -0.0033681518 -0.0033670922 -0.0033664419 -0.0033662715 -0.0033655071 -0.0033637614 -0.0033624065 -0.0033620452 -0.003362414 -0.0033630028][-0.0033646079 -0.0033623974 -0.0033636703 -0.0033655169 -0.0033677174 -0.0033687074 -0.0033680303 -0.0033672233 -0.0033666624 -0.003365742 -0.003364082 -0.0033627844 -0.0033622265 -0.0033623204 -0.0033627842][-0.0033646738 -0.0033622666 -0.0033634065 -0.0033649304 -0.0033668317 -0.0033679036 -0.0033676277 -0.0033670368 -0.0033664824 -0.0033655302 -0.0033640717 -0.0033628757 -0.0033621984 -0.0033619881 -0.003362285][-0.0033649981 -0.0033623602 -0.0033631679 -0.0033640768 -0.0033652855 -0.0033660778 -0.0033660659 -0.0033657323 -0.0033654305 -0.0033647281 -0.0033635593 -0.0033624573 -0.0033617937 -0.003361396 -0.0033615215][-0.0033654282 -0.0033624615 -0.003362956 -0.0033632535 -0.0033637411 -0.0033641958 -0.0033643057 -0.0033641644 -0.00336403 -0.0033637085 -0.0033629916 -0.003362125 -0.0033615613 -0.0033611322 -0.0033611059][-0.0033657271 -0.0033625267 -0.0033628009 -0.0033627111 -0.003362746 -0.0033630116 -0.0033630959 -0.0033630023 -0.0033629343 -0.0033629502 -0.0033626975 -0.0033619588 -0.0033614703 -0.003361173 -0.0033610535][-0.0033656673 -0.0033623043 -0.0033624168 -0.0033622631 -0.0033622433 -0.0033624654 -0.0033625094 -0.0033624354 -0.0033624948 -0.0033627781 -0.0033628764 -0.0033624026 -0.0033620938 -0.0033619681 -0.0033618263]]...]
INFO - root - 2017-12-09 23:55:37.317079: step 71510, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:36m:14s remains)
INFO - root - 2017-12-09 23:55:45.906500: step 71520, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 62h:31m:19s remains)
INFO - root - 2017-12-09 23:55:54.436189: step 71530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:59m:00s remains)
INFO - root - 2017-12-09 23:56:02.992694: step 71540, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:30m:07s remains)
INFO - root - 2017-12-09 23:56:11.644888: step 71550, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:23m:21s remains)
INFO - root - 2017-12-09 23:56:20.372771: step 71560, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 64h:47m:11s remains)
INFO - root - 2017-12-09 23:56:28.887311: step 71570, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 59h:23m:04s remains)
INFO - root - 2017-12-09 23:56:37.555551: step 71580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:36m:41s remains)
INFO - root - 2017-12-09 23:56:46.078382: step 71590, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 61h:52m:14s remains)
INFO - root - 2017-12-09 23:56:54.128988: step 71600, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:57m:36s remains)
2017-12-09 23:56:54.996935: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094995432 0.10103647 0.10358895 0.10499427 0.10471302 0.10090803 0.094149366 0.085399836 0.075977504 0.065939769 0.05708234 0.049709663 0.044215932 0.040390812 0.038449261][0.12496965 0.13993652 0.15144424 0.16180995 0.16778898 0.16892998 0.16468729 0.15445875 0.14119786 0.12518114 0.10984172 0.094880916 0.082611777 0.073986411 0.068322934][0.18364577 0.20621878 0.22466122 0.24066487 0.2506268 0.25721857 0.25495738 0.24521866 0.23056208 0.21057932 0.19001079 0.16800848 0.15073113 0.13693099 0.12689008][0.26472279 0.29567942 0.32116994 0.34242469 0.35619318 0.36433586 0.36209306 0.35320497 0.33780217 0.31480917 0.29043475 0.26465616 0.24313521 0.22360158 0.2085413][0.36133075 0.39881381 0.4288269 0.45182875 0.46618068 0.47434369 0.472459 0.464714 0.44964367 0.42770326 0.40331593 0.37585124 0.35197875 0.3295857 0.31124565][0.44934046 0.49236062 0.52577609 0.55033004 0.56614548 0.57541549 0.5757094 0.56914592 0.55486292 0.53535426 0.51145262 0.48455724 0.4607591 0.439095 0.41964674][0.5152396 0.56092346 0.595352 0.62043291 0.63764709 0.64952451 0.65382427 0.65098614 0.64008993 0.6244843 0.60323071 0.5787791 0.55641508 0.53637236 0.51791632][0.54822421 0.59441596 0.627863 0.65318084 0.67265958 0.68770784 0.69684249 0.69879609 0.69211441 0.68026567 0.66180694 0.6407218 0.6207298 0.60347992 0.587926][0.54462904 0.5891872 0.62002218 0.64512217 0.66681331 0.68552911 0.69994044 0.70676666 0.70432204 0.69542986 0.67921209 0.66075569 0.64201874 0.62767196 0.61586457][0.50713789 0.54853249 0.57547849 0.59866053 0.62072152 0.64170367 0.66009492 0.67081112 0.6716888 0.66434091 0.64884454 0.63024956 0.611897 0.59952313 0.59087521][0.43695772 0.4740096 0.49703848 0.51780277 0.53860992 0.55933458 0.57853693 0.59069479 0.59346896 0.586863 0.5726403 0.55510324 0.53795588 0.52662724 0.51990187][0.34692481 0.37700856 0.39470914 0.41124511 0.42848969 0.44589761 0.46264824 0.47432557 0.47787347 0.47265515 0.46110359 0.44639662 0.43196392 0.42200339 0.41695961][0.25060371 0.27305955 0.28546494 0.29705802 0.30952996 0.32218942 0.33481446 0.34396654 0.34691602 0.34306011 0.33424321 0.32292742 0.31169763 0.30383337 0.30010259][0.16045383 0.17561014 0.18384708 0.1912798 0.19928582 0.20726378 0.21540126 0.22139721 0.22325058 0.22045174 0.21439902 0.20658094 0.19871485 0.19307891 0.19036485][0.088415 0.097604364 0.10269836 0.10709166 0.11178018 0.11621089 0.1207623 0.12410988 0.12497658 0.12313582 0.11949196 0.1148259 0.11003845 0.10642176 0.10460942]]...]
INFO - root - 2017-12-09 23:57:03.556562: step 71610, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 60h:13m:47s remains)
INFO - root - 2017-12-09 23:57:12.000855: step 71620, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 52h:26m:32s remains)
INFO - root - 2017-12-09 23:57:20.542152: step 71630, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 61h:09m:27s remains)
INFO - root - 2017-12-09 23:57:29.250298: step 71640, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 64h:21m:50s remains)
INFO - root - 2017-12-09 23:57:37.929492: step 71650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:21m:41s remains)
INFO - root - 2017-12-09 23:57:46.546362: step 71660, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:19m:58s remains)
INFO - root - 2017-12-09 23:57:55.121807: step 71670, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 59h:03m:41s remains)
INFO - root - 2017-12-09 23:58:03.483113: step 71680, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 61h:14m:15s remains)
INFO - root - 2017-12-09 23:58:12.028147: step 71690, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 63h:17m:49s remains)
INFO - root - 2017-12-09 23:58:20.208032: step 71700, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 60h:34m:10s remains)
2017-12-09 23:58:21.113690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031819027 -0.0027566254 -0.002091621 -0.0012656434 -0.0004674322 5.8135949e-05 0.00012185355 -0.00028648251 -0.00095614279 -0.0017297307 -0.0023959903 -0.0028821002 -0.0031974688 -0.00334871 -0.0033969763][-0.0030064452 -0.0023575951 -0.0013622353 -0.00013075559 0.0010378428 0.0017913433 0.0018833696 0.0013167651 0.00035403064 -0.00078078848 -0.0017789266 -0.0025355648 -0.00303421 -0.0032979078 -0.0033889988][-0.0025712419 -0.0014294507 0.00027663074 0.0023427226 0.00429109 0.005573581 0.00584611 0.005077254 0.0035833062 0.0017120021 -8.5872365e-05 -0.0015571422 -0.0025814255 -0.0031480254 -0.0033590673][-0.001563077 0.00056108343 0.0035955647 0.0071182968 0.010367314 0.012527488 0.013119543 0.012060104 0.0097036567 0.006577774 0.0033347949 0.00049314345 -0.0015907723 -0.0028006802 -0.0032821214][0.0003666901 0.0041134888 0.0092603276 0.015028264 0.020223871 0.023644423 0.024602871 0.022970058 0.019184532 0.014059426 0.0085763913 0.0036486841 -3.1589065e-05 -0.0022295555 -0.0031448433][0.003112057 0.0088091213 0.01638036 0.024679404 0.032091703 0.037016679 0.038533 0.0363947 0.031036945 0.023512034 0.015237158 0.0076740831 0.0019589995 -0.0014879663 -0.0029617986][0.0058944589 0.013147298 0.022553274 0.032699417 0.041714117 0.047739554 0.049708292 0.047240384 0.04073574 0.03134872 0.020812746 0.011076555 0.0036702857 -0.00082430523 -0.0027866801][0.0076553402 0.015414441 0.025324015 0.035931204 0.045380544 0.051744118 0.053895455 0.051345564 0.044447504 0.034358203 0.022919912 0.01232942 0.0042876108 -0.00057308446 -0.0027114293][0.0077111321 0.014820952 0.023839224 0.033497959 0.042179905 0.048099536 0.05014658 0.047754649 0.041249178 0.031711683 0.020915456 0.010988101 0.0035417981 -0.00087337662 -0.0027857597][0.006028546 0.011723638 0.018968819 0.026807528 0.033936333 0.03884872 0.040536392 0.038440358 0.032888252 0.024856979 0.015905576 0.0078057451 0.0018541778 -0.0015546596 -0.0029718368][0.0032132787 0.0072047394 0.012323683 0.017936237 0.023091059 0.026677473 0.027864916 0.026205497 0.022012528 0.016098462 0.0096701207 0.0039860997 -7.6712342e-05 -0.0023033852 -0.003169297][0.00025791652 0.0025559922 0.0055527212 0.0088959932 0.011998743 0.014161292 0.014825739 0.01371504 0.011084525 0.0074977269 0.0037196574 0.00048041926 -0.0017529406 -0.0029032128 -0.0033078254][-0.0019459324 -0.00093752379 0.00038757129 0.0018741356 0.0032568218 0.0042072805 0.0044535082 0.0038867726 0.0026589807 0.0010528455 -0.00058067148 -0.0019295759 -0.0028188825 -0.0032436112 -0.0033734259][-0.003101822 -0.0028170953 -0.0024322595 -0.0020003906 -0.0016027559 -0.0013388721 -0.0012860305 -0.001471712 -0.0018382578 -0.0022918358 -0.0027324059 -0.0030747349 -0.003284801 -0.0033732727 -0.003393756][-0.0033849296 -0.003358013 -0.0033159819 -0.0032637371 -0.0032137863 -0.0031843875 -0.0031851723 -0.0032155716 -0.0032639222 -0.0033153016 -0.0033591064 -0.0033858961 -0.0033976568 -0.0033994806 -0.0033981977]]...]
INFO - root - 2017-12-09 23:58:29.829685: step 71710, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 62h:11m:29s remains)
INFO - root - 2017-12-09 23:58:38.418432: step 71720, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:49m:10s remains)
INFO - root - 2017-12-09 23:58:46.986346: step 71730, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 63h:17m:51s remains)
INFO - root - 2017-12-09 23:58:55.672360: step 71740, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:58m:23s remains)
INFO - root - 2017-12-09 23:59:04.285571: step 71750, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:55m:48s remains)
INFO - root - 2017-12-09 23:59:13.034246: step 71760, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 64h:01m:18s remains)
INFO - root - 2017-12-09 23:59:21.890948: step 71770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:41m:25s remains)
INFO - root - 2017-12-09 23:59:30.590605: step 71780, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:03m:57s remains)
INFO - root - 2017-12-09 23:59:39.364441: step 71790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 63h:54m:28s remains)
INFO - root - 2017-12-09 23:59:47.807496: step 71800, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 61h:16m:44s remains)
2017-12-09 23:59:48.797268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030209343 -0.0030784833 -0.0031061822 -0.0030196372 -0.0027805706 -0.0023017239 -0.0013583314 0.00024897535 0.0023654324 0.0046180105 0.0062420648 0.0067631593 0.0061771311 0.0051520718 0.004608037][-0.0028367033 -0.0028883636 -0.0029506115 -0.0029747894 -0.0028915911 -0.0025928831 -0.0019008424 -0.00063473335 0.0011119971 0.0030479247 0.00441481 0.0046708686 0.0038082979 0.0025185628 0.001639561][-0.0029889217 -0.0029815875 -0.0029733228 -0.0029707903 -0.0029335856 -0.0027936515 -0.0024371829 -0.0016307902 -0.00042106281 0.00093262154 0.0017967352 0.0017492955 0.00085909595 -0.00028664037 -0.0011121128][-0.0029723339 -0.0029796818 -0.0029932389 -0.002985063 -0.0029606693 -0.0028837419 -0.0027574629 -0.0022694347 -0.0014924888 -0.00061430107 -0.00011383649 -0.00024841749 -0.00091594132 -0.0017208816 -0.0023377468][-0.0028264446 -0.0028861631 -0.0029344154 -0.0029592786 -0.0029812537 -0.002849522 -0.0027463357 -0.0023251227 -0.0018227171 -0.0012489106 -0.00094898627 -0.00097634527 -0.001308643 -0.0017421801 -0.002150299][-0.0020367657 -0.0021107155 -0.0021496094 -0.0021638121 -0.0022295816 -0.0021809249 -0.0021318218 -0.0019017732 -0.0017296171 -0.0013341 -0.0010275037 -0.00076016062 -0.00070134271 -0.000830441 -0.0011287385][-0.00085314177 -0.00093756639 -0.00096263946 -0.0009849316 -0.0011340077 -0.001173737 -0.0011686329 -0.0012915183 -0.0015154113 -0.0015138125 -0.0014530937 -0.0010873857 -0.00071097887 -0.00050573819 -0.00056927511][-0.00036909105 -0.00041727326 -0.00042311964 -0.00050575589 -0.00073342677 -0.00083603337 -0.00083579728 -0.0010197391 -0.00125623 -0.0014478664 -0.0016766965 -0.0017497457 -0.0016887272 -0.0016696539 -0.0017465183][-0.00054209447 -0.00061980728 -0.00063212309 -0.00071995077 -0.00074938545 -0.00069035916 -0.00056943391 -0.0004825755 -0.00043497956 -0.0007252926 -0.001179392 -0.001678025 -0.0019493632 -0.0021980372 -0.0024694721][-0.00074659893 -0.00087351 -0.00084239594 -0.00066546258 -0.0004126206 -0.00016034185 -1.1928147e-05 -6.5427274e-05 -0.00022095 -0.00060965167 -0.0011018738 -0.0016319129 -0.0019020347 -0.0021155437 -0.0023251083][-0.00099020172 -0.0012808975 -0.0013645121 -0.0013163949 -0.0011943833 -0.0010982167 -0.0011409379 -0.0014081106 -0.0017030298 -0.0015895698 -0.001376732 -0.0013781325 -0.0014754958 -0.0016202488 -0.0018688297][-0.00011203252 -0.0010181959 -0.001491172 -0.0016747244 -0.0016785223 -0.0016374501 -0.0017612056 -0.0018886562 -0.0019478266 -0.0017735705 -0.0015565925 -0.0016998053 -0.0018370575 -0.0019831888 -0.0021377443][0.0033008896 0.0016688628 0.00055204961 -0.00010881689 -0.00040034298 -0.00050680456 -0.00063233008 -0.00065037119 -0.00053277216 -0.00031339563 -0.00021081045 -0.0004747957 -0.00093730981 -0.0015684549 -0.0021383036][0.00742793 0.0049190726 0.0031350313 0.0020430104 0.0014933532 0.0012366727 0.0010235666 0.00095710321 0.0011322654 0.0015224575 0.0017229991 0.0015592079 0.0011071267 0.00043891743 -0.00024664612][0.013582017 0.010602918 0.0081358282 0.0063806241 0.0052965079 0.0046666861 0.0043780887 0.004399064 0.0048432192 0.0055389069 0.0058558192 0.0055385828 0.0047499612 0.0041084541 0.0035107683]]...]
INFO - root - 2017-12-09 23:59:57.460030: step 71810, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 62h:29m:32s remains)
INFO - root - 2017-12-10 00:00:06.158584: step 71820, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:37m:06s remains)
INFO - root - 2017-12-10 00:00:14.698231: step 71830, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 65h:28m:10s remains)
INFO - root - 2017-12-10 00:00:23.473362: step 71840, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:46m:36s remains)
INFO - root - 2017-12-10 00:00:32.137354: step 71850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:29m:40s remains)
INFO - root - 2017-12-10 00:00:40.941310: step 71860, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:27m:56s remains)
INFO - root - 2017-12-10 00:00:49.661849: step 71870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:21m:57s remains)
INFO - root - 2017-12-10 00:00:58.406144: step 71880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:38m:06s remains)
INFO - root - 2017-12-10 00:01:06.915832: step 71890, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 63h:12m:07s remains)
INFO - root - 2017-12-10 00:01:15.171844: step 71900, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 58h:43m:00s remains)
2017-12-10 00:01:16.066047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0022506854 -0.0028443756 -0.0032054847 -0.0033156234 -0.0032616595 -0.0031450221 -0.0030075845 -0.002897877 -0.0028633173 -0.0029709556 -0.0030920531 -0.0032231361 -0.0033070212 -0.0033522756 -0.0033703614][-0.00078139687 -0.0014834329 -0.0022440962 -0.0028564651 -0.0032046861 -0.0032906921 -0.0032233417 -0.0031203725 -0.0030260987 -0.002982985 -0.0029893417 -0.0030959595 -0.0031985813 -0.0033029839 -0.0033589911][0.00047823903 -1.5692785e-07 -0.00072183344 -0.0015596611 -0.0023434283 -0.0028963883 -0.0031459448 -0.0031511602 -0.0030611388 -0.0030013567 -0.0029925066 -0.0030814051 -0.0031814538 -0.003307435 -0.0033632545][0.00021800818 0.00033236248 0.00024618558 -0.00015271641 -0.00084770541 -0.0016388813 -0.0022506972 -0.0025194446 -0.0025200772 -0.0024626576 -0.002458374 -0.0026722276 -0.0029165242 -0.003191401 -0.0033320233][4.8746821e-05 -0.00012393831 -0.00024040462 -0.00033328705 -0.00053592282 -0.00085184351 -0.0012413026 -0.0016416289 -0.0019620673 -0.002206231 -0.0023966627 -0.0027323011 -0.0030151689 -0.003245292 -0.0033516579][0.0011987633 0.00099935359 0.00079020648 0.00059411977 0.00035218569 3.7303893e-05 -0.00032512005 -0.00069326372 -0.0010818839 -0.0015056473 -0.0019456986 -0.0024773921 -0.0029015546 -0.0031816422 -0.0033229715][0.0032659576 0.0031219784 0.0029453167 0.0029550584 0.0030310571 0.0030050825 0.002690336 0.0021196047 0.0013271898 0.00046752184 -0.00038753706 -0.0012820708 -0.002102063 -0.0027467934 -0.0031551365][0.0085349809 0.0081236558 0.0074565383 0.00712034 0.0070396289 0.0070103677 0.0067975037 0.0063415822 0.0055182017 0.0042992868 0.0027208247 0.00092099444 -0.00079723937 -0.0020935643 -0.0029111509][0.016711762 0.01610446 0.014992472 0.014332876 0.01379941 0.013151808 0.012246616 0.011155439 0.009818458 0.0081567168 0.0060690381 0.0035928108 0.00097902049 -0.0012225192 -0.0026342107][0.024706373 0.024282627 0.023169084 0.022574145 0.022010721 0.021102743 0.019643703 0.017691987 0.01536517 0.012715258 0.0096374825 0.0061546387 0.0025671341 -0.00041956571 -0.0023385417][0.030675726 0.030330693 0.029169196 0.02871204 0.028358297 0.027773278 0.026494294 0.024370469 0.02145078 0.01787135 0.01370226 0.0090810861 0.004433603 0.0005498433 -0.0019649151][0.032397453 0.032237981 0.031163534 0.030778753 0.030558156 0.030234763 0.029214004 0.027173674 0.023986097 0.019848626 0.01504953 0.00987577 0.0048499983 0.00069996971 -0.0019318521][0.029861106 0.029887924 0.029154798 0.028880736 0.028598579 0.028210817 0.027117489 0.02498411 0.021658827 0.017391456 0.012636103 0.0077912966 0.003358748 -0.00014212774 -0.0022631641][0.023938643 0.024323735 0.024202591 0.024300335 0.02407461 0.023480007 0.022064464 0.019658701 0.016247036 0.012214024 0.0080306372 0.0041148989 0.00085556204 -0.0014801915 -0.002764516][0.016744321 0.017521242 0.017945161 0.018379215 0.018328849 0.017710993 0.016141621 0.01362888 0.010369682 0.006881251 0.0035450591 0.00073645893 -0.0013178403 -0.0025604181 -0.0031412032]]...]
INFO - root - 2017-12-10 00:01:24.674129: step 71910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:30m:08s remains)
INFO - root - 2017-12-10 00:01:33.215707: step 71920, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 63h:57m:32s remains)
INFO - root - 2017-12-10 00:01:41.875823: step 71930, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:04m:59s remains)
INFO - root - 2017-12-10 00:01:50.477247: step 71940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:22m:41s remains)
INFO - root - 2017-12-10 00:01:59.239536: step 71950, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:45m:43s remains)
INFO - root - 2017-12-10 00:02:08.052199: step 71960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:55m:58s remains)
INFO - root - 2017-12-10 00:02:16.890282: step 71970, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 62h:14m:24s remains)
INFO - root - 2017-12-10 00:02:25.558291: step 71980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:05m:42s remains)
INFO - root - 2017-12-10 00:02:34.340784: step 71990, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:12m:15s remains)
INFO - root - 2017-12-10 00:02:42.534105: step 72000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:38m:20s remains)
2017-12-10 00:02:43.377164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033868779 -0.0033838528 -0.0033820376 -0.0033809249 -0.0033802157 -0.0033796732 -0.0033792292 -0.0033789976 -0.0033791561 -0.003379903 -0.003381809 -0.0033854796 -0.0033892479 -0.0033921995 -0.003393502][-0.0033887778 -0.0033847634 -0.0033819042 -0.0033799545 -0.0033787352 -0.0033780423 -0.0033775279 -0.0033770208 -0.0033769675 -0.0033778055 -0.0033799962 -0.0033838458 -0.0033878051 -0.0033908016 -0.0033918365][-0.0033906254 -0.0033858384 -0.0033822162 -0.0033800644 -0.0033786579 -0.0033778572 -0.0033772734 -0.0033766588 -0.0033763736 -0.0033771752 -0.0033795459 -0.0033833261 -0.0033874295 -0.0033902684 -0.0033913262][-0.0033908898 -0.0033859727 -0.0033821098 -0.0033802253 -0.0033791666 -0.0033786551 -0.0033784003 -0.0033781142 -0.0033776721 -0.0033780893 -0.0033803345 -0.0033842041 -0.0033884642 -0.0033909949 -0.003391745][-0.0033886486 -0.0033846814 -0.0033816621 -0.0033805526 -0.00338015 -0.003380527 -0.0033808576 -0.0033811987 -0.0033809454 -0.0033810877 -0.0033829268 -0.0033862325 -0.0033899348 -0.0033917211 -0.0033919576][-0.0033841264 -0.0033811866 -0.0033795659 -0.0033793168 -0.0033796704 -0.0033803824 -0.0033811545 -0.0033820763 -0.0033824416 -0.0033833792 -0.0033853457 -0.0033882041 -0.0033913502 -0.0033919537 -0.0033907793][-0.0033770702 -0.0033752345 -0.0033745733 -0.00337504 -0.0033763517 -0.0033774707 -0.0033786898 -0.0033802004 -0.003381792 -0.003383429 -0.0033857317 -0.0033889594 -0.0033915387 -0.0033909522 -0.0033881625][-0.0033670114 -0.0033659949 -0.003367129 -0.0033692317 -0.0033717956 -0.0033738902 -0.0033762471 -0.003379229 -0.0033819757 -0.0033842647 -0.003386579 -0.003389712 -0.003391485 -0.003389674 -0.0033852172][-0.0033537024 -0.0033536437 -0.003357094 -0.003361406 -0.0033659805 -0.0033696156 -0.003373431 -0.0033777826 -0.0033820493 -0.003385291 -0.0033882738 -0.00339146 -0.0033918221 -0.0033880977 -0.0033817978][-0.0033387614 -0.0033387966 -0.0033437663 -0.003350128 -0.0033568013 -0.0033629327 -0.0033688571 -0.0033753589 -0.0033813061 -0.0033856905 -0.0033887841 -0.0033915606 -0.0033910265 -0.0033863129 -0.003379043][-0.0033260975 -0.0033262796 -0.0033320845 -0.003339577 -0.0033472138 -0.0033557208 -0.0033638438 -0.0033718848 -0.0033791931 -0.0033851757 -0.0033888652 -0.0033913662 -0.0033899064 -0.003384528 -0.0033769158][-0.0033178825 -0.0033180034 -0.0033247273 -0.0033329709 -0.0033423321 -0.0033522693 -0.0033616112 -0.0033709062 -0.0033786006 -0.0033843759 -0.0033878589 -0.0033897588 -0.0033870777 -0.0033806069 -0.00337318][-0.0033152336 -0.0033146674 -0.0033212644 -0.003329793 -0.0033398215 -0.0033504274 -0.0033605325 -0.0033702212 -0.0033786257 -0.0033848092 -0.0033884759 -0.0033887601 -0.003384721 -0.0033782288 -0.0033708138][-0.0033169889 -0.0033164031 -0.0033224567 -0.0033309693 -0.0033403975 -0.0033508409 -0.0033603467 -0.0033689239 -0.0033758925 -0.0033813582 -0.00338492 -0.003384812 -0.0033806746 -0.0033746138 -0.0033685006][-0.0033230141 -0.003322178 -0.0033270519 -0.0033346463 -0.0033428795 -0.0033515526 -0.0033596614 -0.0033667462 -0.003373055 -0.0033778814 -0.0033801582 -0.0033795992 -0.0033763272 -0.0033710226 -0.0033653697]]...]
INFO - root - 2017-12-10 00:02:51.806486: step 72010, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 62h:04m:26s remains)
INFO - root - 2017-12-10 00:03:00.475017: step 72020, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:49m:02s remains)
INFO - root - 2017-12-10 00:03:08.952447: step 72030, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:56m:06s remains)
INFO - root - 2017-12-10 00:03:17.531809: step 72040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:09m:19s remains)
INFO - root - 2017-12-10 00:03:26.211720: step 72050, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 62h:37m:55s remains)
INFO - root - 2017-12-10 00:03:34.973178: step 72060, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:45m:56s remains)
INFO - root - 2017-12-10 00:03:43.640751: step 72070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:20m:28s remains)
INFO - root - 2017-12-10 00:03:52.299782: step 72080, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 65h:43m:02s remains)
INFO - root - 2017-12-10 00:04:01.002827: step 72090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:49m:18s remains)
INFO - root - 2017-12-10 00:04:09.318485: step 72100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:32m:11s remains)
2017-12-10 00:04:10.326327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033182914 -0.0033144865 -0.0033166814 -0.00332201 -0.0033294314 -0.0033374953 -0.0033455505 -0.0033529317 -0.0033592982 -0.0033662161 -0.0033726359 -0.0033789661 -0.003383863 -0.0033883676 -0.0033925986][-0.0032679862 -0.0032666051 -0.0032720088 -0.003281239 -0.0032919231 -0.0033032568 -0.0033148127 -0.0033256372 -0.0033346904 -0.0033413449 -0.0033481165 -0.0033555159 -0.0033632198 -0.003370954 -0.0033780716][-0.0032133935 -0.0032133937 -0.0032217377 -0.0032341569 -0.0032478031 -0.00326105 -0.0032749102 -0.0032874891 -0.0032981217 -0.0033071719 -0.0033166045 -0.0033251676 -0.0033349174 -0.0033467645 -0.0033592351][-0.0031791658 -0.0031779446 -0.0031865556 -0.0032011669 -0.0032170708 -0.0032319624 -0.0032462319 -0.0032585177 -0.0032686682 -0.003277417 -0.0032864541 -0.003295179 -0.0033049206 -0.0033191436 -0.0033364997][-0.003177705 -0.0031753802 -0.0031824377 -0.0031972039 -0.0032142166 -0.003230118 -0.0032433167 -0.0032534727 -0.003261145 -0.0032673427 -0.0032727718 -0.0032784252 -0.0032849894 -0.0032983636 -0.003316453][-0.0031978211 -0.0031969748 -0.0032043057 -0.0032201495 -0.0032384184 -0.0032546571 -0.0032667955 -0.0032750976 -0.0032798271 -0.0032819889 -0.0032829021 -0.0032829298 -0.0032839205 -0.0032925461 -0.0033075917][-0.0032341033 -0.0032327762 -0.0032398046 -0.0032559361 -0.0032737432 -0.0032886053 -0.0032989313 -0.0033043395 -0.0033065369 -0.0033068508 -0.0033050878 -0.0033024692 -0.0032999783 -0.0033027972 -0.003311825][-0.003279628 -0.0032781442 -0.0032837484 -0.0032971292 -0.003312157 -0.0033244262 -0.0033325779 -0.0033364263 -0.0033376536 -0.003336157 -0.0033329385 -0.0033284172 -0.0033242656 -0.003323982 -0.0033278414][-0.0033250221 -0.0033224423 -0.0033258372 -0.0033358373 -0.0033451512 -0.0033527778 -0.0033581161 -0.0033611988 -0.003362213 -0.0033606547 -0.0033578314 -0.0033542658 -0.003350527 -0.0033484511 -0.0033479081][-0.0033597029 -0.0033559729 -0.003356708 -0.0033606873 -0.0033646834 -0.0033683043 -0.0033709134 -0.0033727661 -0.0033736371 -0.003373119 -0.0033715118 -0.0033694878 -0.0033672322 -0.0033655444 -0.0033640531][-0.0033737742 -0.003368858 -0.0033674568 -0.0033677279 -0.0033687928 -0.003370363 -0.0033716892 -0.0033732897 -0.003374282 -0.0033747088 -0.0033744692 -0.0033737102 -0.0033719325 -0.0033702236 -0.0033689204][-0.0033765177 -0.0033711107 -0.0033688829 -0.0033680901 -0.0033682282 -0.0033689705 -0.0033697814 -0.0033708662 -0.0033716355 -0.0033724019 -0.0033724043 -0.0033720946 -0.0033710378 -0.0033700771 -0.0033695141][-0.0033755375 -0.0033695912 -0.0033673223 -0.0033668284 -0.0033667923 -0.0033672568 -0.0033679449 -0.0033695053 -0.0033708008 -0.0033718552 -0.0033719945 -0.003371581 -0.003370245 -0.0033695591 -0.0033694252][-0.0033760928 -0.0033704373 -0.0033682864 -0.0033674673 -0.0033675781 -0.0033681467 -0.0033689779 -0.0033704762 -0.0033720199 -0.003373456 -0.0033739114 -0.0033736632 -0.0033728674 -0.0033726108 -0.0033724988][-0.0033756779 -0.0033713928 -0.0033704636 -0.0033697993 -0.0033698624 -0.0033702329 -0.0033709938 -0.0033719423 -0.0033726955 -0.0033733803 -0.0033736108 -0.0033746471 -0.0033751333 -0.0033758492 -0.0033763286]]...]
INFO - root - 2017-12-10 00:04:18.950273: step 72110, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 64h:06m:47s remains)
INFO - root - 2017-12-10 00:04:27.685901: step 72120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:47m:13s remains)
INFO - root - 2017-12-10 00:04:36.120527: step 72130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:33m:20s remains)
INFO - root - 2017-12-10 00:04:44.563194: step 72140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:03m:20s remains)
INFO - root - 2017-12-10 00:04:53.156760: step 72150, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 63h:39m:17s remains)
INFO - root - 2017-12-10 00:05:01.682981: step 72160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:33m:44s remains)
INFO - root - 2017-12-10 00:05:10.181872: step 72170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 63h:13m:53s remains)
INFO - root - 2017-12-10 00:05:18.931471: step 72180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:49m:44s remains)
INFO - root - 2017-12-10 00:05:27.701118: step 72190, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:07m:28s remains)
INFO - root - 2017-12-10 00:05:36.267228: step 72200, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:07m:09s remains)
2017-12-10 00:05:37.198888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003346818 -0.0033431961 -0.003344174 -0.0033446089 -0.0033449498 -0.0033453112 -0.0033455333 -0.0033453091 -0.0033447663 -0.0033440322 -0.0033435395 -0.0033431088 -0.0033429391 -0.0033432962 -0.0033438902][-0.0033447132 -0.0033413533 -0.0033426594 -0.003343804 -0.0033447694 -0.0033455351 -0.0033458746 -0.0033453067 -0.0033443742 -0.0033429011 -0.0033415542 -0.0033406303 -0.0033404245 -0.0033409139 -0.0033415409][-0.0033446939 -0.0033416906 -0.0033436595 -0.0033459279 -0.0033478625 -0.003348991 -0.0033493727 -0.0033482122 -0.0033467035 -0.0033443286 -0.0033418552 -0.0033403758 -0.0033403025 -0.0033409165 -0.0033413242][-0.0033440259 -0.0033415291 -0.0033445847 -0.0033482916 -0.0033512628 -0.0033528118 -0.0033529114 -0.0033508644 -0.0033485242 -0.0033447186 -0.0033410119 -0.00333907 -0.0033396317 -0.0033403058 -0.0033405996][-0.0033432764 -0.0033415351 -0.0033458306 -0.003351043 -0.0033550321 -0.0033568719 -0.003356348 -0.0033530984 -0.0033491405 -0.0033436776 -0.0033388238 -0.0033367623 -0.0033381628 -0.00333924 -0.0033399402][-0.0033424855 -0.0033412892 -0.0033467158 -0.0033531031 -0.0033581441 -0.0033610738 -0.0033611741 -0.0033581364 -0.0033533033 -0.0033466907 -0.0033411395 -0.0033383691 -0.0033393677 -0.0033399265 -0.003340668][-0.0033420185 -0.0033413079 -0.0033475524 -0.0033547804 -0.0033607576 -0.0033644519 -0.0033650838 -0.003362091 -0.0033567559 -0.0033497747 -0.0033441719 -0.0033409514 -0.0033412816 -0.0033414601 -0.003342076][-0.0033419062 -0.0033413591 -0.0033479338 -0.0033555881 -0.0033617099 -0.003365604 -0.0033662131 -0.0033634517 -0.0033581126 -0.0033511152 -0.0033458243 -0.0033427936 -0.0033431475 -0.0033428741 -0.0033433994][-0.0033416552 -0.00334119 -0.0033472448 -0.0033544018 -0.0033601937 -0.0033642154 -0.0033649898 -0.0033628454 -0.0033579739 -0.0033513922 -0.0033468043 -0.0033439291 -0.0033438141 -0.0033435198 -0.0033440348][-0.0033421628 -0.0033411917 -0.0033462732 -0.0033525832 -0.003357654 -0.0033614957 -0.0033628417 -0.003361742 -0.0033578624 -0.0033521424 -0.0033485217 -0.0033460723 -0.0033453479 -0.0033448185 -0.003345219][-0.0033431042 -0.0033413752 -0.0033452318 -0.003350097 -0.0033542952 -0.0033577185 -0.0033595418 -0.0033597569 -0.0033576293 -0.003353544 -0.0033509191 -0.003348863 -0.0033475214 -0.0033463733 -0.0033462788][-0.0033443274 -0.003342316 -0.0033449992 -0.0033485754 -0.00335168 -0.0033540595 -0.0033552079 -0.0033553604 -0.0033542463 -0.003351589 -0.0033498353 -0.003348558 -0.0033477617 -0.0033467989 -0.003346422][-0.0033451074 -0.0033430539 -0.0033447475 -0.0033470923 -0.0033491377 -0.0033505573 -0.0033510777 -0.0033512714 -0.0033508176 -0.0033492038 -0.0033480441 -0.0033475296 -0.0033472273 -0.0033464893 -0.0033459614][-0.0033457642 -0.0033434958 -0.003344476 -0.0033458103 -0.0033470441 -0.003347784 -0.0033480455 -0.0033482478 -0.0033481631 -0.0033473847 -0.0033465757 -0.0033464301 -0.003346367 -0.0033458725 -0.0033454758][-0.0033463682 -0.0033437922 -0.0033445307 -0.0033451882 -0.0033457442 -0.0033459621 -0.003346032 -0.0033461032 -0.0033460772 -0.0033457961 -0.0033454639 -0.0033456038 -0.0033457086 -0.0033454555 -0.00334523]]...]
INFO - root - 2017-12-10 00:05:45.826713: step 72210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:20m:06s remains)
INFO - root - 2017-12-10 00:05:54.434882: step 72220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:15m:43s remains)
INFO - root - 2017-12-10 00:06:02.992462: step 72230, loss = 0.88, batch loss = 0.67 (9.0 examples/sec; 0.890 sec/batch; 64h:21m:42s remains)
INFO - root - 2017-12-10 00:06:11.698657: step 72240, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:07m:49s remains)
INFO - root - 2017-12-10 00:06:20.389831: step 72250, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:26m:18s remains)
INFO - root - 2017-12-10 00:06:29.085379: step 72260, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 62h:22m:11s remains)
INFO - root - 2017-12-10 00:06:37.807421: step 72270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:53m:03s remains)
INFO - root - 2017-12-10 00:06:46.461819: step 72280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:27m:46s remains)
INFO - root - 2017-12-10 00:06:55.211291: step 72290, loss = 0.89, batch loss = 0.69 (8.1 examples/sec; 0.985 sec/batch; 71h:13m:23s remains)
INFO - root - 2017-12-10 00:07:03.652515: step 72300, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:01m:07s remains)
2017-12-10 00:07:04.511896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033613029 -0.0033577294 -0.0033566144 -0.0033557229 -0.0033550297 -0.0033545801 -0.0033541494 -0.003354206 -0.0033545922 -0.0033549194 -0.0033564554 -0.0033596854 -0.0033641991 -0.0033700378 -0.0033756024][-0.0033596784 -0.003355775 -0.0033542023 -0.0033531226 -0.003352145 -0.0033513661 -0.0033506243 -0.0033504111 -0.0033506856 -0.0033509277 -0.0033525589 -0.0033563818 -0.0033618188 -0.003369204 -0.0033763419][-0.0033607741 -0.0033568246 -0.0033548176 -0.0033535545 -0.0033519631 -0.0033506749 -0.0033496176 -0.0033491515 -0.0033492628 -0.0033493834 -0.0033509587 -0.0033550924 -0.0033611457 -0.0033695567 -0.0033779473][-0.003361766 -0.0033577788 -0.0033558006 -0.0033542425 -0.0033522598 -0.0033505766 -0.00334935 -0.003348778 -0.0033488993 -0.0033490604 -0.0033505263 -0.0033546889 -0.00336087 -0.0033697044 -0.0033786763][-0.0033627355 -0.0033589038 -0.0033569359 -0.0033553068 -0.0033531177 -0.0033511629 -0.0033498637 -0.003349154 -0.0033491352 -0.0033492469 -0.0033504877 -0.0033543122 -0.003360434 -0.0033694324 -0.0033788562][-0.0033631437 -0.0033594635 -0.0033576491 -0.0033560307 -0.003353731 -0.0033517624 -0.0033505878 -0.0033498344 -0.0033496756 -0.0033497112 -0.0033505668 -0.0033537764 -0.0033594021 -0.0033678922 -0.0033773417][-0.0033627066 -0.003359131 -0.0033576288 -0.0033562705 -0.0033541082 -0.0033521377 -0.003350944 -0.0033501266 -0.0033499089 -0.003349852 -0.0033503838 -0.0033529128 -0.0033579199 -0.0033658058 -0.0033756925][-0.0033614493 -0.00335817 -0.0033570826 -0.0033560046 -0.003354134 -0.0033523038 -0.0033510753 -0.0033501703 -0.0033498441 -0.0033496453 -0.003349815 -0.0033516404 -0.003355962 -0.0033634461 -0.0033737014][-0.0033596242 -0.0033565587 -0.0033560002 -0.0033552381 -0.0033538779 -0.0033524183 -0.0033513247 -0.0033503685 -0.0033497282 -0.0033493114 -0.0033492295 -0.0033503764 -0.0033540421 -0.0033612575 -0.0033718208][-0.0033575266 -0.0033545946 -0.0033546553 -0.0033544984 -0.0033538702 -0.00335309 -0.0033524793 -0.0033516127 -0.0033506581 -0.0033498781 -0.003349313 -0.0033498849 -0.0033530591 -0.0033599369 -0.0033703852][-0.0033556304 -0.0033528563 -0.0033537194 -0.0033542907 -0.0033546283 -0.0033547413 -0.0033545995 -0.0033538588 -0.0033526742 -0.0033515755 -0.0033505897 -0.0033508707 -0.0033538034 -0.0033602188 -0.0033701106][-0.0033544044 -0.0033516667 -0.0033531508 -0.0033544446 -0.0033557676 -0.0033569024 -0.0033574202 -0.0033571359 -0.00335613 -0.003354846 -0.0033536332 -0.0033537035 -0.0033564076 -0.0033621839 -0.0033709481][-0.0033537883 -0.0033508537 -0.0033526698 -0.0033546286 -0.0033568672 -0.0033590419 -0.0033603539 -0.0033607339 -0.0033600235 -0.0033588146 -0.003357569 -0.0033575846 -0.0033600437 -0.0033649965 -0.0033722185][-0.003353396 -0.0033501873 -0.0033519866 -0.0033542444 -0.0033571103 -0.0033601073 -0.0033621909 -0.003363237 -0.0033631134 -0.0033624338 -0.0033615825 -0.0033616871 -0.00336373 -0.0033677097 -0.0033731267][-0.0033535771 -0.0033499431 -0.0033514176 -0.0033535718 -0.0033565606 -0.003359806 -0.0033622764 -0.0033639111 -0.0033644605 -0.0033643898 -0.0033640349 -0.0033644356 -0.0033661614 -0.0033691793 -0.0033729959]]...]
INFO - root - 2017-12-10 00:07:13.323944: step 72310, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 65h:26m:17s remains)
INFO - root - 2017-12-10 00:07:22.034473: step 72320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:04m:46s remains)
INFO - root - 2017-12-10 00:07:30.680752: step 72330, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 63h:27m:56s remains)
INFO - root - 2017-12-10 00:07:39.431023: step 72340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:41m:19s remains)
INFO - root - 2017-12-10 00:07:48.061309: step 72350, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:00m:32s remains)
INFO - root - 2017-12-10 00:07:56.790253: step 72360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:39m:04s remains)
INFO - root - 2017-12-10 00:08:05.544089: step 72370, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:41m:31s remains)
INFO - root - 2017-12-10 00:08:14.188797: step 72380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:08m:15s remains)
INFO - root - 2017-12-10 00:08:22.892597: step 72390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 63h:50m:27s remains)
INFO - root - 2017-12-10 00:08:31.173273: step 72400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:08m:05s remains)
2017-12-10 00:08:32.152721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017976274 -0.0015552436 -0.0013016062 -0.0010801794 -0.00094848149 -0.00096349907 -0.0011602044 -0.0015218358 -0.0019746935 -0.0024056816 -0.0027353985 -0.0029411649 -0.0030645586 -0.0031411315 -0.003169707][-0.0015575539 -0.0013336255 -0.0010900665 -0.0008812421 -0.00076358975 -0.00079006515 -0.0010041036 -0.001395572 -0.0018771425 -0.0023302608 -0.0026642566 -0.0028658849 -0.0029704871 -0.0030212943 -0.0030145713][-0.0013513742 -0.0011627106 -0.00094495015 -0.00077135395 -0.00069854804 -0.00075547933 -0.00095781474 -0.0013121956 -0.0017616857 -0.0022150099 -0.0025791044 -0.0028363965 -0.0029976682 -0.003069741 -0.0030203855][-0.0012284005 -0.0010837996 -0.00087478524 -0.00068284757 -0.000587204 -0.00063105696 -0.00082130358 -0.00114672 -0.0015563173 -0.0019887565 -0.0023765569 -0.0026863457 -0.0028895645 -0.0029635145 -0.0028772531][-0.0010520027 -0.00099069229 -0.00083236373 -0.00063984189 -0.00050378335 -0.00048301672 -0.00060492777 -0.00086793769 -0.0012325724 -0.0016406049 -0.0020292031 -0.0023521862 -0.0025650172 -0.0026291423 -0.002520015][-0.00097349519 -0.00094621023 -0.00080169737 -0.00059809932 -0.00043226103 -0.00036742073 -0.00042679952 -0.000600989 -0.00086215767 -0.0011742979 -0.0014914662 -0.0017771379 -0.0019773897 -0.002054357 -0.0019865111][-0.00094745704 -0.0009264918 -0.00078187441 -0.00057280366 -0.00039825798 -0.00032260758 -0.00035370095 -0.00046691811 -0.00062806578 -0.00081234286 -0.0010079972 -0.0012005013 -0.0013562276 -0.0014398522 -0.0014361937][-0.0010680424 -0.0010381825 -0.00087905116 -0.00065281778 -0.00046008779 -0.00036756857 -0.00038150186 -0.00047159777 -0.00059296051 -0.000713103 -0.00082331011 -0.00092791091 -0.00102488 -0.0010937536 -0.0011352468][-0.0013219868 -0.0013080298 -0.0011412713 -0.00088711246 -0.00065539684 -0.00051784515 -0.00048927055 -0.00053349021 -0.00061347545 -0.00070351129 -0.00080344267 -0.00091507123 -0.0010232159 -0.0011017532 -0.0011455046][-0.0015310957 -0.0015727177 -0.0014442375 -0.0011976687 -0.00094230729 -0.000762088 -0.00067718048 -0.00065707555 -0.0006773558 -0.00073306845 -0.00083059515 -0.00095833396 -0.0010962139 -0.0012202398 -0.0013182664][-0.0015049173 -0.001614845 -0.0015595099 -0.001375695 -0.0011570784 -0.00098430179 -0.00088670896 -0.00085195736 -0.0008679782 -0.00093721319 -0.0010537859 -0.0011897346 -0.0013132214 -0.0014170506 -0.0015104017][-0.0012665871 -0.0014038479 -0.0014318512 -0.0013564383 -0.0012385165 -0.0011441614 -0.0011091069 -0.0011328063 -0.0012056271 -0.0013232287 -0.001471713 -0.0016146712 -0.0017161088 -0.0017757579 -0.0018211865][-0.0012030068 -0.0012751911 -0.0013005717 -0.0012900187 -0.0012781401 -0.0012997205 -0.0013804371 -0.0015171577 -0.0016953134 -0.0018912074 -0.002076108 -0.0022117493 -0.0022711433 -0.0022626196 -0.0022263085][-0.0012457722 -0.0013694225 -0.0014492634 -0.0014863749 -0.0015228873 -0.0016068285 -0.0017651089 -0.001984519 -0.0022293762 -0.0024580404 -0.0026326946 -0.0027261619 -0.0027343396 -0.0026800607 -0.0026000678][-0.0013180939 -0.0014751561 -0.0016187808 -0.0017491165 -0.0018939356 -0.0020732642 -0.0022886975 -0.0025238264 -0.0027488363 -0.0029309716 -0.003047728 -0.0030924333 -0.003074825 -0.0030131331 -0.0029241929]]...]
INFO - root - 2017-12-10 00:08:40.823182: step 72410, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 61h:50m:11s remains)
INFO - root - 2017-12-10 00:08:49.456534: step 72420, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 60h:09m:27s remains)
INFO - root - 2017-12-10 00:08:58.202015: step 72430, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 62h:10m:00s remains)
INFO - root - 2017-12-10 00:09:06.908907: step 72440, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.887 sec/batch; 64h:02m:39s remains)
INFO - root - 2017-12-10 00:09:15.579975: step 72450, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 63h:52m:12s remains)
INFO - root - 2017-12-10 00:09:24.330114: step 72460, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 61h:21m:18s remains)
INFO - root - 2017-12-10 00:09:33.017048: step 72470, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:00m:10s remains)
INFO - root - 2017-12-10 00:09:41.749069: step 72480, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.968 sec/batch; 69h:56m:03s remains)
INFO - root - 2017-12-10 00:09:50.422453: step 72490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:49m:01s remains)
INFO - root - 2017-12-10 00:09:58.778524: step 72500, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 60h:54m:05s remains)
2017-12-10 00:09:59.655791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033019627 -0.0033426892 -0.0029642815 -0.0013844066 0.001783157 0.0058890181 0.010125165 0.013441702 0.015197134 0.014394166 0.011548731 0.0075107571 0.0035674146 0.00051682931 -0.00158328][-0.0027489937 -0.003028136 -0.0024704731 -9.2179514e-05 0.0048135165 0.011513001 0.01879744 0.024882162 0.028379081 0.027777459 0.023362957 0.016673822 0.0097049437 0.0039242143 -0.00010379567][-0.0014235761 -0.0017358625 -0.00059882505 0.003552675 0.011894263 0.023592124 0.036621213 0.047924984 0.054553732 0.054242413 0.047060575 0.035531938 0.022945143 0.011944144 0.0039508268][0.00022396119 0.00020911265 0.0029119116 0.010541139 0.024526427 0.043763988 0.064897455 0.083249316 0.093875475 0.093767323 0.082661562 0.064279251 0.04362708 0.025004568 0.011003401][0.0022120785 0.0029525154 0.0080456417 0.020486843 0.041761473 0.070489548 0.10149124 0.1280801 0.14302593 0.14263441 0.12645473 0.099713095 0.069314957 0.041543338 0.020264624][0.0040063215 0.0056506889 0.01340447 0.030759564 0.059070043 0.096563987 0.13655806 0.17052314 0.18943001 0.18903017 0.16848662 0.13421458 0.094703093 0.058253612 0.029876893][0.005268163 0.0076953629 0.017348023 0.037970472 0.070835158 0.11373303 0.15923437 0.19766027 0.21914728 0.21914339 0.19635276 0.15766698 0.11232381 0.070120655 0.036849331][0.0054628309 0.0082966583 0.018495927 0.039509043 0.072881944 0.11622739 0.16242084 0.20156166 0.22397837 0.22503003 0.20290609 0.16412196 0.11767409 0.074006483 0.039208576][0.00480666 0.0073521016 0.016374143 0.034827169 0.064734556 0.10368617 0.14576997 0.18203525 0.20386934 0.2065676 0.18769006 0.15272477 0.10982633 0.069096886 0.036368892][0.0044015134 0.0060599828 0.012405358 0.026010323 0.049451422 0.080737621 0.11554064 0.14635664 0.16606402 0.17016356 0.15584394 0.12734762 0.091478005 0.057133991 0.029410223][0.0042508645 0.0049640564 0.0083054723 0.016408509 0.032153025 0.054387227 0.080380537 0.10433605 0.12074719 0.12551127 0.11582772 0.094731152 0.067583457 0.041485045 0.020445216][0.0040273322 0.0039652027 0.0048482176 0.0084282942 0.017282939 0.031109726 0.048446294 0.065268323 0.077540584 0.081926391 0.076084152 0.062034927 0.043642938 0.025927374 0.011755719][0.0032062267 0.0026469228 0.0021250986 0.0028854881 0.0068309624 0.01420987 0.024317214 0.034735672 0.042776119 0.046064965 0.042986408 0.034734793 0.023800436 0.013248069 0.0049179718][0.0019366157 0.0010962852 9.0945978e-06 -0.00049357978 0.00072228606 0.0039944211 0.0089717172 0.014448987 0.018854724 0.02079094 0.019382531 0.015253454 0.0097643007 0.0044655846 0.00037606363][0.00035565975 -0.0004980017 -0.0015383997 -0.0022771256 -0.0021794415 -0.0010609189 0.00089006568 0.0031816533 0.0050674053 0.0059248535 0.0054046065 0.003769824 0.0015950084 -0.00050015375 -0.002066555]]...]
INFO - root - 2017-12-10 00:10:08.315773: step 72510, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:16m:25s remains)
INFO - root - 2017-12-10 00:10:16.923398: step 72520, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:43m:28s remains)
INFO - root - 2017-12-10 00:10:25.748643: step 72530, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.967 sec/batch; 69h:51m:30s remains)
INFO - root - 2017-12-10 00:10:34.414406: step 72540, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:08m:11s remains)
INFO - root - 2017-12-10 00:10:42.897567: step 72550, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:06m:06s remains)
INFO - root - 2017-12-10 00:10:51.600807: step 72560, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 62h:27m:11s remains)
INFO - root - 2017-12-10 00:11:00.234063: step 72570, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 59h:52m:45s remains)
INFO - root - 2017-12-10 00:11:08.871836: step 72580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:08m:53s remains)
INFO - root - 2017-12-10 00:11:17.521781: step 72590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:41m:48s remains)
INFO - root - 2017-12-10 00:11:25.921311: step 72600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:36m:49s remains)
2017-12-10 00:11:26.802653: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0083883014 0.0085770888 0.0076368647 0.0060599567 0.00426655 0.0023868259 0.00065577147 -0.00070301653 -0.0016219188 -0.0021064756 -0.0022815042 -0.0023552841 -0.0024279426 -0.0025000365 -0.0025762334][0.01629054 0.016712083 0.015212828 0.012538336 0.0093822107 0.006143271 0.0032028777 0.00092986436 -0.00054761162 -0.0013495109 -0.0016887914 -0.0018513408 -0.0019958008 -0.0021143695 -0.0022265136][0.027446594 0.028602907 0.026802216 0.023007829 0.018096915 0.012824504 0.0078295218 0.0038298916 0.0011251762 -0.00039505865 -0.0010362184 -0.0013116 -0.0014944021 -0.0016037809 -0.0016583325][0.040761739 0.042602636 0.040515423 0.035675574 0.029070737 0.021670714 0.014296334 0.008033446 0.0035403247 0.00083857658 -0.00038985792 -0.00088542909 -0.0010316416 -0.0010051471 -0.00084575149][0.055675991 0.058345433 0.055893742 0.049764022 0.041174069 0.031452041 0.021560008 0.012864918 0.0063376259 0.002212974 0.00020110165 -0.00055979704 -0.00057673268 -0.0002357231 0.00035683089][0.069195151 0.072884127 0.070478618 0.0636027 0.053544655 0.041858707 0.029624661 0.018541357 0.0098524326 0.0040870747 0.0010250036 -0.00022459379 -0.00017994549 0.00055700145 0.0017314211][0.077278949 0.0814925 0.079135545 0.071933441 0.061271213 0.0487732 0.035400882 0.022918684 0.012767661 0.00580116 0.0019463084 0.00044270372 0.00072793546 0.002012898 0.0038043333][0.078614227 0.083022073 0.080849759 0.073798813 0.063280776 0.050982304 0.03776 0.025268031 0.01480945 0.00737979 0.0030658382 0.0013412649 0.0018071809 0.0035277393 0.0057991669][0.073219143 0.0775539 0.075706609 0.069235608 0.059574932 0.04844759 0.036623273 0.025514539 0.016069166 0.0091282455 0.0048374627 0.0029797091 0.0033592258 0.0050711446 0.007284564][0.063220486 0.067732193 0.066671722 0.061314017 0.052993894 0.043516185 0.033680659 0.024717417 0.017106332 0.011397498 0.0077018784 0.0059822183 0.0061921803 0.0075411154 0.00915641][0.051981058 0.056768283 0.05698112 0.053345677 0.046914723 0.039298896 0.031457536 0.024551757 0.01878312 0.014423838 0.011493204 0.0099878628 0.0099684158 0.01074892 0.011418224][0.043809339 0.048999265 0.050377149 0.04830981 0.043551389 0.037485555 0.031209985 0.025846343 0.021437693 0.018058207 0.015689392 0.014358282 0.01406634 0.014166566 0.013715768][0.039990928 0.046059653 0.048809014 0.048193477 0.044710256 0.039470151 0.03377882 0.028899932 0.024931295 0.021841111 0.019579271 0.018158605 0.017523455 0.016985223 0.015557008][0.03976832 0.046654586 0.050552938 0.051163495 0.048662093 0.043785468 0.037990063 0.032636814 0.028132031 0.024551641 0.021960761 0.020420939 0.01968858 0.018923182 0.016980177][0.0413058 0.048828952 0.053547103 0.054949347 0.052942954 0.047926754 0.041461859 0.035038959 0.029463636 0.024994461 0.021813512 0.020061869 0.019388601 0.018808773 0.0169946]]...]
INFO - root - 2017-12-10 00:11:35.453783: step 72610, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 63h:16m:57s remains)
INFO - root - 2017-12-10 00:11:44.185531: step 72620, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 65h:04m:11s remains)
INFO - root - 2017-12-10 00:11:52.670218: step 72630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:08m:51s remains)
INFO - root - 2017-12-10 00:12:01.485442: step 72640, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:36m:26s remains)
INFO - root - 2017-12-10 00:12:10.259197: step 72650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:45m:52s remains)
INFO - root - 2017-12-10 00:12:19.177535: step 72660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:22m:34s remains)
INFO - root - 2017-12-10 00:12:27.831479: step 72670, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 62h:17m:01s remains)
INFO - root - 2017-12-10 00:12:36.559529: step 72680, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.904 sec/batch; 65h:14m:36s remains)
INFO - root - 2017-12-10 00:12:45.341559: step 72690, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:38m:12s remains)
INFO - root - 2017-12-10 00:12:53.786824: step 72700, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:40m:29s remains)
2017-12-10 00:12:54.709624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003359559 -0.0033593418 -0.0033626314 -0.003366356 -0.0033700715 -0.0033739561 -0.0033769419 -0.0033788448 -0.0033801503 -0.0033804637 -0.0033804828 -0.0033801722 -0.0033798392 -0.0033800779 -0.0033809261][-0.0033554984 -0.0033535489 -0.0033555296 -0.0033582842 -0.0033613646 -0.0033647171 -0.0033677202 -0.0033704506 -0.003372554 -0.0033737505 -0.0033743163 -0.0033746865 -0.0033747596 -0.0033753591 -0.0033763864][-0.0033532877 -0.003350117 -0.0033506404 -0.0033519978 -0.003353772 -0.003356036 -0.0033584703 -0.0033612826 -0.0033640612 -0.0033658731 -0.0033669244 -0.0033680219 -0.0033685898 -0.0033692536 -0.0033701463][-0.0033526504 -0.0033482402 -0.0033472839 -0.0033472129 -0.0033476893 -0.0033487049 -0.0033500411 -0.0033524262 -0.003355111 -0.0033567981 -0.0033578121 -0.0033590102 -0.0033596132 -0.0033601592 -0.0033611094][-0.0033529222 -0.003348256 -0.0033467493 -0.0033460113 -0.0033458259 -0.0033460632 -0.0033468243 -0.0033487633 -0.0033509729 -0.0033522197 -0.0033530698 -0.0033538372 -0.003353687 -0.0033535091 -0.0033538472][-0.0033534407 -0.003348704 -0.0033474714 -0.003347046 -0.0033468907 -0.0033468863 -0.0033473603 -0.0033491654 -0.0033511312 -0.0033522125 -0.0033531315 -0.0033537678 -0.0033532565 -0.0033522046 -0.0033513049][-0.0033537839 -0.0033496171 -0.0033489892 -0.0033490683 -0.0033492043 -0.0033492427 -0.0033495531 -0.0033515168 -0.0033536847 -0.0033552279 -0.0033566251 -0.0033577273 -0.0033573604 -0.0033559077 -0.0033542479][-0.0033541194 -0.0033504965 -0.0033506504 -0.0033513804 -0.0033520276 -0.0033525084 -0.0033531673 -0.0033550321 -0.0033572961 -0.003359268 -0.0033611453 -0.0033623902 -0.0033621721 -0.0033606472 -0.0033583897][-0.0033546838 -0.0033519533 -0.0033533361 -0.0033553254 -0.003357362 -0.0033589324 -0.0033603022 -0.0033616985 -0.0033632633 -0.0033649784 -0.0033665029 -0.003367512 -0.0033673481 -0.0033659344 -0.003363204][-0.0033556612 -0.00335385 -0.003356583 -0.003359963 -0.0033636882 -0.0033671148 -0.00336953 -0.003370279 -0.0033704834 -0.003371049 -0.0033717805 -0.0033721046 -0.0033718767 -0.0033706753 -0.0033680103][-0.0033569541 -0.0033560437 -0.0033598067 -0.0033644692 -0.0033695928 -0.0033742343 -0.0033773356 -0.0033776122 -0.0033767924 -0.0033763698 -0.0033758434 -0.0033749994 -0.0033743079 -0.0033729924 -0.0033703542][-0.0033582714 -0.0033582384 -0.0033626813 -0.0033680988 -0.0033740343 -0.0033793666 -0.0033828334 -0.0033828281 -0.0033811596 -0.0033798644 -0.003378192 -0.0033761971 -0.0033747454 -0.0033732252 -0.003370571][-0.0033591436 -0.0033591033 -0.0033637162 -0.0033689735 -0.0033747824 -0.0033802898 -0.0033839946 -0.003384195 -0.0033823077 -0.0033802188 -0.0033773263 -0.0033744366 -0.0033724287 -0.0033706536 -0.0033682087][-0.0033592246 -0.0033585657 -0.0033626396 -0.0033675064 -0.0033729821 -0.0033780511 -0.0033815142 -0.0033817131 -0.0033797554 -0.0033769514 -0.0033733421 -0.0033698371 -0.0033674575 -0.0033656699 -0.0033639297][-0.0033584004 -0.0033571732 -0.0033604114 -0.0033644098 -0.0033689342 -0.0033732406 -0.0033760935 -0.0033762245 -0.003374499 -0.0033715598 -0.0033678864 -0.0033644091 -0.0033619562 -0.0033602419 -0.0033589159]]...]
INFO - root - 2017-12-10 00:13:03.411054: step 72710, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 62h:28m:25s remains)
INFO - root - 2017-12-10 00:13:12.124912: step 72720, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 65h:35m:51s remains)
INFO - root - 2017-12-10 00:13:20.751723: step 72730, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 62h:20m:34s remains)
INFO - root - 2017-12-10 00:13:29.528120: step 72740, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 63h:01m:27s remains)
INFO - root - 2017-12-10 00:13:38.263697: step 72750, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 61h:04m:35s remains)
INFO - root - 2017-12-10 00:13:46.911402: step 72760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:26m:22s remains)
INFO - root - 2017-12-10 00:13:55.672040: step 72770, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 61h:12m:03s remains)
INFO - root - 2017-12-10 00:14:04.342058: step 72780, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:18m:12s remains)
INFO - root - 2017-12-10 00:14:12.952856: step 72790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:07m:50s remains)
INFO - root - 2017-12-10 00:14:21.466521: step 72800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:34m:03s remains)
2017-12-10 00:14:22.364467: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.061878033 0.073494978 0.082026973 0.086005278 0.084606349 0.080480404 0.074411921 0.067078337 0.05879508 0.050689559 0.04365949 0.03744835 0.031457789 0.026623279 0.022411017][0.061289586 0.072702527 0.08103729 0.085149914 0.083956264 0.08021649 0.074604608 0.067716539 0.059580415 0.051547926 0.044485707 0.038048685 0.031820726 0.02660894 0.022181589][0.059766136 0.071056589 0.07952828 0.084246486 0.083969988 0.081338622 0.076726258 0.070769176 0.063615613 0.05624412 0.049581602 0.043483667 0.03729419 0.032027014 0.027287487][0.056923904 0.068248719 0.077292949 0.08321014 0.084644422 0.083693124 0.080190673 0.075364426 0.069303721 0.063068114 0.057269253 0.0518804 0.046450641 0.041342046 0.036745105][0.052466832 0.064047359 0.073860683 0.08130569 0.084805161 0.085840695 0.083898932 0.079874106 0.074748069 0.06937781 0.064544581 0.060047321 0.055310614 0.051022314 0.047105007][0.047209509 0.058553632 0.069011755 0.077972338 0.083775342 0.086780176 0.086306609 0.083291195 0.0789116 0.074262574 0.0701566 0.066727921 0.06308236 0.059732623 0.056774624][0.041803066 0.052572742 0.06312079 0.072970629 0.080405556 0.085067004 0.085937008 0.083783962 0.079875834 0.075810395 0.07245376 0.070077032 0.067815796 0.065933347 0.064484909][0.036391865 0.046343282 0.05662436 0.066665277 0.074915148 0.080574214 0.082317 0.080906108 0.077461563 0.073885135 0.071352318 0.070088439 0.069041058 0.068760008 0.068889841][0.032151233 0.041215129 0.050896928 0.060418084 0.0685236 0.074351 0.076588884 0.075828336 0.072756432 0.069825895 0.068122678 0.0677471 0.06804911 0.068978883 0.07041125][0.030038899 0.038257841 0.047326181 0.05609652 0.063402005 0.068556987 0.070940122 0.070537962 0.067875423 0.065406457 0.064250775 0.064609461 0.065430142 0.067561276 0.069907978][0.030191077 0.03812252 0.046663452 0.054402906 0.060653016 0.064810343 0.066627406 0.066488139 0.064092666 0.062030829 0.061020717 0.061703563 0.0631717 0.065537132 0.068340391][0.0319766 0.040192686 0.048653647 0.0557192 0.060776353 0.063815519 0.064976789 0.064337216 0.062030911 0.06006708 0.059038658 0.059321865 0.060644619 0.063116655 0.066217944][0.033989344 0.042752892 0.051353876 0.058037065 0.062511846 0.065066956 0.065912582 0.064848311 0.062417753 0.060138091 0.058798276 0.058589153 0.05934485 0.061484218 0.064150453][0.035880078 0.045158572 0.053888991 0.060194708 0.064037859 0.065991931 0.066623211 0.0653286 0.062954023 0.060643636 0.058912877 0.058277726 0.058704421 0.060348324 0.062668405][0.036736172 0.046445943 0.055104602 0.061082691 0.06444566 0.06587071 0.066435158 0.065230131 0.063210204 0.061027706 0.059305526 0.058398794 0.058357164 0.059605137 0.061489962]]...]
INFO - root - 2017-12-10 00:14:31.021608: step 72810, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:54m:42s remains)
INFO - root - 2017-12-10 00:14:39.920615: step 72820, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:54m:36s remains)
INFO - root - 2017-12-10 00:14:48.430603: step 72830, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:17m:08s remains)
INFO - root - 2017-12-10 00:14:57.165668: step 72840, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 60h:19m:44s remains)
INFO - root - 2017-12-10 00:15:05.847955: step 72850, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 65h:06m:02s remains)
INFO - root - 2017-12-10 00:15:14.600395: step 72860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:58m:24s remains)
INFO - root - 2017-12-10 00:15:23.248859: step 72870, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 60h:12m:18s remains)
INFO - root - 2017-12-10 00:15:32.018735: step 72880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:50m:18s remains)
INFO - root - 2017-12-10 00:15:40.823126: step 72890, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 63h:20m:07s remains)
INFO - root - 2017-12-10 00:15:49.226591: step 72900, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 63h:01m:45s remains)
2017-12-10 00:15:50.110875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00331001 -0.0033569278 -0.0033778613 -0.0033670634 -0.0033205249 -0.0032399397 -0.0031426735 -0.003061262 -0.0030355256 -0.0030855017 -0.0031891835 -0.0032938418 -0.0033608312 -0.0033894591 -0.0033976485][-0.003314272 -0.0033571138 -0.0033733051 -0.00335061 -0.0032760988 -0.0031458179 -0.0029901937 -0.0028662158 -0.0028353583 -0.0029181254 -0.0030749897 -0.0032316726 -0.0033341611 -0.0033798241 -0.0033931134][-0.0033438751 -0.003370553 -0.0033758353 -0.00333972 -0.0032377553 -0.0030616284 -0.0028565922 -0.0027019922 -0.0026724767 -0.0027851271 -0.0029840609 -0.0031816738 -0.0033127689 -0.0033725405 -0.00339036][-0.0033751063 -0.003386725 -0.0033832996 -0.0033381439 -0.0032180827 -0.0030140693 -0.0027808168 -0.0026112709 -0.002586348 -0.0027199795 -0.0029441831 -0.0031627102 -0.0033068829 -0.0033720636 -0.0033909217][-0.0033952331 -0.0033992145 -0.0033909958 -0.0033399302 -0.0032118938 -0.0029987437 -0.0027593048 -0.0025926556 -0.002580802 -0.0027297409 -0.0029614384 -0.0031780999 -0.0033151181 -0.0033752741 -0.0033918978][-0.003402184 -0.003403431 -0.0033929087 -0.003339997 -0.0032126973 -0.0030074115 -0.0027850033 -0.0026430879 -0.0026541757 -0.0028107739 -0.0030296275 -0.0032198604 -0.0033323918 -0.0033790732 -0.0033913411][-0.0034044094 -0.0034032618 -0.0033897653 -0.0033371917 -0.0032190531 -0.0030374043 -0.0028537523 -0.0027559344 -0.0027964218 -0.0029494294 -0.0031334704 -0.0032773856 -0.0033549122 -0.0033844186 -0.0033917979][-0.0034045994 -0.0034009565 -0.0033857322 -0.0033368473 -0.0032359113 -0.0030923479 -0.0029641348 -0.0029185237 -0.0029798385 -0.003108691 -0.0032391744 -0.0033297318 -0.003374649 -0.0033905166 -0.00339383][-0.0034042601 -0.0034000976 -0.0033860523 -0.0033451873 -0.0032687783 -0.00317132 -0.003100079 -0.0030948156 -0.0031556601 -0.0032439607 -0.0033199128 -0.0033669553 -0.0033887189 -0.0033950817 -0.0033953548][-0.0034038946 -0.0034008364 -0.0033904738 -0.0033620356 -0.0033132923 -0.0032594358 -0.0032297017 -0.0032396584 -0.0032808457 -0.003328376 -0.0033651148 -0.0033866877 -0.00339585 -0.003397614 -0.0033965595][-0.0034036902 -0.0034025032 -0.0033970445 -0.0033813294 -0.0033562602 -0.0033324843 -0.0033232917 -0.0033310703 -0.0033500367 -0.0033703547 -0.003385894 -0.0033951548 -0.0033986918 -0.0033983667 -0.0033967225][-0.003403472 -0.0034036338 -0.0034015109 -0.0033955134 -0.0033859382 -0.0033775894 -0.0033744236 -0.0033762129 -0.0033814742 -0.0033882381 -0.0033945998 -0.0033982745 -0.0033991754 -0.0033982953 -0.0033966808][-0.0034026806 -0.0034027414 -0.0034030119 -0.0034023193 -0.0034004943 -0.0033981509 -0.0033963211 -0.0033950312 -0.0033954103 -0.0033971304 -0.0033987919 -0.0033993842 -0.0033990182 -0.0033978575 -0.003396325][-0.0034012431 -0.0034012522 -0.003402916 -0.00340457 -0.0034053379 -0.0034049682 -0.0034041174 -0.0034021104 -0.0034009605 -0.0034005658 -0.0034002909 -0.003399923 -0.0033989374 -0.00339766 -0.0033960075][-0.0033993293 -0.0033991148 -0.0034012564 -0.0034037728 -0.0034056336 -0.003405717 -0.0034050893 -0.0034032739 -0.003401676 -0.0034003875 -0.0033999074 -0.00339925 -0.0033981181 -0.0033969234 -0.0033955208]]...]
INFO - root - 2017-12-10 00:15:58.821197: step 72910, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 64h:24m:13s remains)
INFO - root - 2017-12-10 00:16:07.554630: step 72920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 64h:10m:25s remains)
INFO - root - 2017-12-10 00:16:16.146885: step 72930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:16m:42s remains)
INFO - root - 2017-12-10 00:16:24.639081: step 72940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 60h:57m:39s remains)
INFO - root - 2017-12-10 00:16:33.343865: step 72950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:40m:00s remains)
INFO - root - 2017-12-10 00:16:41.965916: step 72960, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 59h:36m:18s remains)
INFO - root - 2017-12-10 00:16:50.416838: step 72970, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:29m:22s remains)
INFO - root - 2017-12-10 00:16:59.030174: step 72980, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 59h:13m:38s remains)
INFO - root - 2017-12-10 00:17:07.854961: step 72990, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 63h:35m:20s remains)
INFO - root - 2017-12-10 00:17:16.244021: step 73000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:01m:45s remains)
2017-12-10 00:17:17.168181: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19326079 0.19263333 0.1900059 0.18629833 0.18230098 0.17835121 0.17478956 0.17009856 0.16570859 0.16087687 0.15611054 0.15349784 0.15142131 0.15012048 0.14908241][0.20703082 0.20595285 0.20249517 0.198682 0.19444485 0.19076402 0.18668458 0.18178487 0.17704239 0.17206976 0.16774972 0.16430008 0.16181691 0.15979543 0.15826192][0.21092415 0.20994158 0.20647988 0.20270313 0.19838116 0.19511971 0.19129354 0.18684193 0.18204895 0.17673832 0.17208605 0.16783012 0.1648753 0.16187896 0.15931962][0.21188067 0.21191762 0.20878816 0.20567541 0.20147747 0.19823568 0.1939954 0.18947338 0.18459313 0.17894594 0.17436858 0.16918041 0.16566329 0.16178074 0.15851802][0.20898965 0.21033868 0.20791738 0.20562804 0.20190735 0.19873047 0.19380003 0.18918258 0.18406053 0.17738475 0.17158172 0.16475059 0.16029719 0.15526408 0.15135881][0.20027067 0.2033907 0.20269686 0.20168999 0.19874297 0.19588326 0.1908893 0.18569079 0.17883646 0.17048761 0.16235644 0.15363918 0.14709197 0.14033423 0.13581823][0.18257591 0.18815672 0.189643 0.19074985 0.19039513 0.18893735 0.18461217 0.17909303 0.17048079 0.15997997 0.14818449 0.13622676 0.12661064 0.11792772 0.11299448][0.15636978 0.163566 0.16705443 0.17110579 0.17407656 0.1754456 0.17334504 0.16786847 0.15859224 0.14589998 0.13087301 0.11531735 0.101763 0.091366678 0.08530242][0.1236075 0.13061361 0.13529727 0.14170666 0.14768782 0.15153709 0.15142742 0.14686629 0.13803396 0.12416707 0.10765258 0.090472139 0.075416222 0.064401224 0.058001388][0.08717601 0.093281783 0.098386593 0.10509288 0.11182686 0.11701836 0.11811145 0.11400408 0.10601286 0.093390092 0.078345753 0.062724411 0.049406353 0.040752269 0.036725949][0.054012556 0.057809345 0.061927959 0.067667663 0.07326071 0.077459306 0.078211732 0.075090863 0.068632029 0.058616918 0.047197577 0.035943363 0.026668834 0.022290314 0.022373322][0.027221499 0.029540202 0.032405812 0.036108758 0.03969856 0.042363506 0.0426601 0.040405806 0.035867054 0.02961806 0.023030905 0.016736897 0.011880984 0.011963161 0.016295595][0.0093570426 0.010522017 0.012147964 0.014064841 0.015991254 0.017343031 0.017474843 0.016407797 0.014193319 0.011163415 0.00839289 0.0062822523 0.0052804248 0.0089608757 0.016478552][0.00028074672 0.00070613879 0.0014855315 0.002268838 0.0030028939 0.0037047237 0.003951855 0.0037762963 0.0032502704 0.0025794089 0.0021466946 0.002203508 0.0034384283 0.0093444567 0.019082079][-0.0021856497 -0.0021379897 -0.0019269902 -0.0016492021 -0.0013915445 -0.00099596428 -0.00079958467 -0.00041902461 -0.00011865189 0.00011586701 0.00048467494 0.0015605211 0.0041241469 0.010786363 0.021476097]]...]
INFO - root - 2017-12-10 00:17:25.871821: step 73010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:29m:32s remains)
INFO - root - 2017-12-10 00:17:34.693344: step 73020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:48m:30s remains)
INFO - root - 2017-12-10 00:17:43.301622: step 73030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:54m:33s remains)
INFO - root - 2017-12-10 00:17:51.994496: step 73040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 60h:24m:46s remains)
INFO - root - 2017-12-10 00:18:00.816352: step 73050, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:59m:35s remains)
INFO - root - 2017-12-10 00:18:09.453568: step 73060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:26m:48s remains)
INFO - root - 2017-12-10 00:18:18.153402: step 73070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:39m:01s remains)
INFO - root - 2017-12-10 00:18:26.891221: step 73080, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 63h:39m:52s remains)
INFO - root - 2017-12-10 00:18:35.716369: step 73090, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:21m:37s remains)
INFO - root - 2017-12-10 00:18:44.103876: step 73100, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 63h:33m:04s remains)
2017-12-10 00:18:45.003994: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1967418 0.21617761 0.23480056 0.25222379 0.2667889 0.27910057 0.28687787 0.2911585 0.29159319 0.28867525 0.28328651 0.27585912 0.26825488 0.26153392 0.25737849][0.19462188 0.2155674 0.23564143 0.25406507 0.26980287 0.28168121 0.28861624 0.29142329 0.29007012 0.28490233 0.27626559 0.26464009 0.25309828 0.24366206 0.23856667][0.18590717 0.20725964 0.22755283 0.24574423 0.2609131 0.27188137 0.27801928 0.27894843 0.27520776 0.26712841 0.25542235 0.24034896 0.22511278 0.21312802 0.20711131][0.17654157 0.19819008 0.21780194 0.23548913 0.25020111 0.26021963 0.26531178 0.26473173 0.25935936 0.24884313 0.23437276 0.21609829 0.19859774 0.18566655 0.18016386][0.16537367 0.1874786 0.20663421 0.22321574 0.23690008 0.24643756 0.25099757 0.25008175 0.24395226 0.2320421 0.21625429 0.19699825 0.178057 0.16432446 0.15917492][0.15149313 0.17258798 0.19001698 0.205204 0.21763001 0.22641763 0.2312452 0.23060428 0.22517526 0.21453719 0.19951862 0.18085006 0.16274378 0.1502472 0.14669][0.13437091 0.15329467 0.1681381 0.18052495 0.19050401 0.19819143 0.20307525 0.20382899 0.20064016 0.19255817 0.18080632 0.16511868 0.14971709 0.13931696 0.13741182][0.1140979 0.13017105 0.14195645 0.15099604 0.15831116 0.16398406 0.16830751 0.17014952 0.16952626 0.16533248 0.15724275 0.14632085 0.13545766 0.12795973 0.128185][0.091472462 0.10415001 0.11279814 0.11875793 0.12324859 0.12682407 0.13045159 0.13305257 0.13444032 0.13385968 0.13049784 0.12478904 0.11805553 0.11431671 0.11628217][0.06675487 0.075762793 0.081347808 0.084758461 0.087215655 0.08919578 0.091902219 0.094858952 0.097882479 0.10013042 0.10038069 0.09952601 0.097464167 0.096704453 0.099667422][0.043038554 0.048630815 0.051777031 0.053529687 0.054761708 0.056025304 0.058265571 0.061105751 0.06475728 0.068414338 0.071139134 0.073283434 0.074400648 0.076508194 0.080193967][0.0234298 0.02645418 0.028075008 0.029065115 0.029778251 0.030716734 0.032498624 0.035069641 0.038503706 0.042068813 0.045583777 0.049049977 0.051930066 0.055392195 0.059322439][0.0098780114 0.01134246 0.012190962 0.012858544 0.013499543 0.014332466 0.015650094 0.017535325 0.020037562 0.0228186 0.025806315 0.02906174 0.032243084 0.035844252 0.039557375][0.0026690632 0.0032917212 0.003697593 0.0040830467 0.004512093 0.0050776033 0.00585036 0.0068952972 0.0082607679 0.0099606924 0.012041449 0.014402485 0.016891019 0.019795861 0.022651002][-0.00015055342 0.00012670201 0.00031616166 0.0005150435 0.00073189684 0.00097848871 0.0012804179 0.0016615235 0.0021750645 0.0028748165 0.0038307377 0.0050677257 0.0064809667 0.0081504295 0.0097320508]]...]
INFO - root - 2017-12-10 00:18:53.775366: step 73110, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 61h:53m:05s remains)
INFO - root - 2017-12-10 00:19:02.435197: step 73120, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:42m:52s remains)
INFO - root - 2017-12-10 00:19:11.072411: step 73130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:23m:13s remains)
INFO - root - 2017-12-10 00:19:19.741180: step 73140, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:27m:06s remains)
INFO - root - 2017-12-10 00:19:28.370970: step 73150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:55m:59s remains)
INFO - root - 2017-12-10 00:19:37.014523: step 73160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:04m:32s remains)
INFO - root - 2017-12-10 00:19:45.712829: step 73170, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 62h:06m:42s remains)
INFO - root - 2017-12-10 00:19:54.325700: step 73180, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:45m:13s remains)
INFO - root - 2017-12-10 00:20:03.050549: step 73190, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.912 sec/batch; 65h:42m:42s remains)
INFO - root - 2017-12-10 00:20:11.541569: step 73200, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 64h:26m:49s remains)
2017-12-10 00:20:12.415785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033980664 -0.003397905 -0.0033986508 -0.0033989463 -0.0033987078 -0.0033863073 -0.0033134455 -0.0030287961 -0.0024896376 -0.0017304994 -0.00077007129 0.00071829255 0.0030360811 0.0057136351 0.0074050259][-0.0033933062 -0.0033936754 -0.0033949688 -0.0033960084 -0.0033967542 -0.0033949276 -0.0033174497 -0.0030695824 -0.0025197649 -0.0015164339 -9.903498e-05 0.0022799599 0.005594491 0.00921157 0.011405265][-0.00338195 -0.0033860283 -0.0033907245 -0.003393688 -0.0033931446 -0.0033878884 -0.003323877 -0.0031104377 -0.0026102627 -0.0014710086 0.00044466718 0.0037282796 0.00793208 0.012311714 0.014968438][-0.0033572842 -0.003365777 -0.0033751128 -0.00337914 -0.0033669497 -0.0033409894 -0.0032868774 -0.0031385419 -0.0027143958 -0.0014556982 0.00089907856 0.00491706 0.0098076109 0.014594153 0.017590929][-0.0033179992 -0.0033285005 -0.0033368308 -0.0033303581 -0.0032747258 -0.0031760547 -0.0030718325 -0.0029397584 -0.0025958903 -0.0013022202 0.0013391201 0.0056755878 0.010814989 0.015815176 0.019078655][-0.0032725146 -0.0032784962 -0.0032713166 -0.0032274488 -0.0030754886 -0.0028230995 -0.0025731113 -0.0023490624 -0.0020407734 -0.00083609694 0.0016725601 0.0058897734 0.010916594 0.01582958 0.019136712][-0.0032449206 -0.0032396982 -0.003201403 -0.003090316 -0.002800442 -0.0023385221 -0.0019039566 -0.0015392385 -0.0012566794 -0.0003442294 0.0017520555 0.00536884 0.00995572 0.014506727 0.017761176][-0.0032577142 -0.0032391229 -0.0031685014 -0.002990474 -0.0025867168 -0.0019570885 -0.0013929086 -0.00095429062 -0.00074304547 -0.000192218 0.0012228859 0.0039638188 0.0077519082 0.011722424 0.01474154][-0.0033033134 -0.0032825416 -0.0032012607 -0.0030009856 -0.0025879545 -0.0019455045 -0.0014016549 -0.0010272018 -0.0009468263 -0.00074812723 -6.4237975e-07 0.001752936 0.004571504 0.0077656424 0.010310063][-0.0033600908 -0.0033438269 -0.0032842269 -0.003123143 -0.0028061424 -0.0023261572 -0.0019452423 -0.0017339165 -0.0017480745 -0.0017695865 -0.0014927682 -0.00054855738 0.0012819546 0.0035500138 0.005383932][-0.0033898058 -0.0033823317 -0.0033570176 -0.0032718317 -0.0030989712 -0.0028263936 -0.0026325812 -0.0025592959 -0.0026226952 -0.0026968997 -0.0026383474 -0.0022307313 -0.0012339284 0.0001520135 0.0012115405][-0.0033966452 -0.003395797 -0.0033899574 -0.0033630799 -0.0032996293 -0.0031999196 -0.003140229 -0.0031275947 -0.0031641703 -0.0032132785 -0.0032065094 -0.0030719466 -0.0026415389 -0.0019946583 -0.0015030231][-0.0033965716 -0.0033956848 -0.0033960757 -0.0033914009 -0.003377473 -0.0033560223 -0.0033467514 -0.0033501072 -0.0033632352 -0.0033728175 -0.0033700487 -0.0033437016 -0.0032114573 -0.0029980263 -0.0028376062][-0.0033956154 -0.0033947423 -0.003395356 -0.0033963425 -0.0033977746 -0.0033990722 -0.0034001574 -0.0033998452 -0.0033985772 -0.003397418 -0.0033965572 -0.0033945567 -0.0033713779 -0.0033388415 -0.0033155016][-0.0033949616 -0.0033941767 -0.0033943618 -0.0033951581 -0.0033965891 -0.0033984093 -0.0033997681 -0.0033992117 -0.003398095 -0.0033968156 -0.0033955392 -0.0033949567 -0.0033937655 -0.0033936868 -0.0033931669]]...]
INFO - root - 2017-12-10 00:20:21.063286: step 73210, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 63h:41m:32s remains)
INFO - root - 2017-12-10 00:20:29.614328: step 73220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:08m:27s remains)
INFO - root - 2017-12-10 00:20:37.908024: step 73230, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 60h:29m:31s remains)
INFO - root - 2017-12-10 00:20:46.410217: step 73240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 63h:22m:30s remains)
INFO - root - 2017-12-10 00:20:55.040582: step 73250, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:17m:21s remains)
INFO - root - 2017-12-10 00:21:03.580356: step 73260, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:08m:39s remains)
INFO - root - 2017-12-10 00:21:12.346877: step 73270, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 60h:54m:30s remains)
INFO - root - 2017-12-10 00:21:20.970675: step 73280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:05m:40s remains)
INFO - root - 2017-12-10 00:21:29.657376: step 73290, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 62h:50m:35s remains)
INFO - root - 2017-12-10 00:21:38.040576: step 73300, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.926 sec/batch; 66h:38m:43s remains)
2017-12-10 00:21:38.911856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033816672 -0.0033797175 -0.0033794846 -0.0033794774 -0.0033794993 -0.0033795787 -0.0033796511 -0.0033797312 -0.003379788 -0.0033797189 -0.0033796337 -0.00337957 -0.0033795198 -0.003379538 -0.0033795552][-0.0033802141 -0.003378107 -0.0033778863 -0.0033779112 -0.003377971 -0.0033780418 -0.0033780939 -0.0033781549 -0.0033781715 -0.0033781254 -0.0033780774 -0.0033780602 -0.0033780676 -0.0033780993 -0.0033781328][-0.0033801612 -0.0033780015 -0.0033778173 -0.0033778513 -0.0033779026 -0.0033779314 -0.0033779209 -0.0033779275 -0.003377906 -0.0033778343 -0.0033777866 -0.003377809 -0.0033778639 -0.0033779142 -0.0033779619][-0.003379721 -0.0033775445 -0.0033773764 -0.0033774353 -0.0033775107 -0.003377527 -0.00337744 -0.0033773663 -0.0033773046 -0.003377219 -0.0033771836 -0.0033772858 -0.0033774721 -0.0033776129 -0.0033777119][-0.0033791671 -0.0033768986 -0.003376737 -0.0033768143 -0.0033768746 -0.0033768401 -0.0033766329 -0.0033764609 -0.0033763696 -0.0033763088 -0.0033763349 -0.0033765424 -0.0033768464 -0.0033770539 -0.0033771922][-0.003378168 -0.0033757663 -0.0033756236 -0.0033757351 -0.0033758257 -0.0033757824 -0.0033754935 -0.0033752576 -0.0033750976 -0.0033750241 -0.00337512 -0.0033754455 -0.0033758539 -0.0033761093 -0.0033762658][-0.0033769764 -0.003374476 -0.0033743144 -0.0033743733 -0.003374371 -0.0033742429 -0.0033739021 -0.0033736506 -0.0033734706 -0.0033735044 -0.0033737398 -0.0033741675 -0.0033745992 -0.0033748401 -0.0033749796][-0.0033757626 -0.0033732613 -0.0033730671 -0.0033730222 -0.0033728194 -0.0033725128 -0.0033720885 -0.0033717416 -0.0033715647 -0.0033717465 -0.0033721789 -0.0033727025 -0.0033731745 -0.0033734126 -0.003373547][-0.0033746562 -0.0033722147 -0.0033719395 -0.0033717547 -0.0033713873 -0.0033709719 -0.0033705719 -0.0033702904 -0.0033701453 -0.0033702941 -0.0033707367 -0.0033712771 -0.0033717554 -0.0033719821 -0.0033721416][-0.0033741707 -0.0033716948 -0.0033713621 -0.0033710212 -0.0033704718 -0.0033699702 -0.0033696603 -0.0033695106 -0.003369438 -0.0033696338 -0.0033700392 -0.0033704643 -0.0033707833 -0.0033709456 -0.0033711088][-0.0033738555 -0.0033713356 -0.0033710613 -0.003370648 -0.0033700299 -0.0033694988 -0.0033692673 -0.0033691772 -0.0033691546 -0.0033693986 -0.0033698133 -0.0033701735 -0.0033703891 -0.0033705286 -0.0033707009][-0.0033738846 -0.0033714233 -0.0033711994 -0.0033707866 -0.0033701614 -0.0033696287 -0.0033694105 -0.0033692981 -0.0033693581 -0.0033696203 -0.0033699446 -0.0033702317 -0.0033704049 -0.0033705181 -0.0033706834][-0.003374286 -0.0033716706 -0.0033715714 -0.0033712066 -0.0033706757 -0.0033702161 -0.0033700028 -0.0033698748 -0.0033699903 -0.0033702212 -0.0033705095 -0.0033707607 -0.0033709209 -0.0033709803 -0.0033710557][-0.0033744883 -0.0033718543 -0.0033719421 -0.0033717128 -0.0033713558 -0.0033710029 -0.0033707856 -0.0033706452 -0.0033707069 -0.0033708739 -0.0033710725 -0.0033713032 -0.0033714378 -0.0033714469 -0.00337148][-0.0033744385 -0.0033717218 -0.0033719477 -0.003371815 -0.0033715714 -0.0033712871 -0.0033710764 -0.0033709272 -0.0033709246 -0.0033710951 -0.0033713479 -0.0033716087 -0.003371733 -0.0033717374 -0.0033718203]]...]
INFO - root - 2017-12-10 00:21:47.403895: step 73310, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:04m:59s remains)
INFO - root - 2017-12-10 00:21:55.856681: step 73320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 60h:20m:58s remains)
INFO - root - 2017-12-10 00:22:04.514513: step 73330, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 63h:54m:37s remains)
INFO - root - 2017-12-10 00:22:12.885123: step 73340, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.826 sec/batch; 59h:29m:21s remains)
INFO - root - 2017-12-10 00:22:21.397279: step 73350, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:26m:05s remains)
INFO - root - 2017-12-10 00:22:29.912585: step 73360, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:58m:30s remains)
INFO - root - 2017-12-10 00:22:38.376213: step 73370, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.827 sec/batch; 59h:31m:05s remains)
INFO - root - 2017-12-10 00:22:46.990111: step 73380, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:01m:02s remains)
INFO - root - 2017-12-10 00:22:55.596591: step 73390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:14m:56s remains)
INFO - root - 2017-12-10 00:23:03.937643: step 73400, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:01m:51s remains)
2017-12-10 00:23:04.863845: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028497532 0.0044178935 0.0077731619 0.012023402 0.016749689 0.021016469 0.023918528 0.024708346 0.024161387 0.022888964 0.021198463 0.019214705 0.016763559 0.014189309 0.011383955][0.0065091597 0.0095809679 0.015001075 0.021626765 0.029198939 0.03650672 0.041616887 0.043618206 0.0432371 0.041261531 0.038502071 0.034916766 0.030385749 0.025486255 0.020032983][0.010359532 0.015430413 0.023439119 0.033693507 0.045273073 0.056280274 0.0641096 0.067810312 0.068043165 0.065856189 0.062407605 0.057485372 0.051282186 0.044251524 0.036014035][0.013669381 0.021084715 0.032295894 0.046828855 0.063246109 0.079126187 0.090591691 0.09654399 0.097582296 0.0953755 0.091198072 0.085002862 0.077476487 0.068835847 0.058414187][0.015400328 0.025083374 0.039515402 0.058316238 0.079772748 0.1005005 0.11588469 0.12494631 0.12777659 0.12617265 0.12181877 0.11524339 0.10738255 0.098009534 0.0859791][0.015843295 0.026741965 0.043240264 0.06484326 0.089876384 0.1143191 0.13302229 0.14490084 0.14994225 0.15041371 0.14756182 0.14245613 0.13616441 0.1279269 0.11604268][0.015404999 0.026001336 0.042531848 0.064714476 0.0909682 0.11687862 0.13705753 0.15083721 0.15808159 0.16132268 0.16135208 0.159854 0.15702404 0.15191072 0.14201377][0.01394769 0.023148596 0.037777439 0.0579826 0.082547568 0.10719734 0.12696871 0.1416208 0.15064989 0.15680498 0.16046204 0.16368806 0.16548911 0.16472116 0.15794539][0.011234708 0.018434947 0.030103071 0.046463642 0.067002468 0.088160194 0.10557003 0.11931916 0.12908751 0.13792692 0.14534849 0.15331031 0.16008134 0.16437544 0.16170274][0.0075314529 0.012576077 0.02075796 0.032561582 0.047901709 0.064098127 0.0779283 0.089547627 0.098752677 0.10862485 0.11871108 0.13094951 0.14256549 0.15192953 0.1539305][0.0036311003 0.0067857103 0.011808177 0.019204931 0.029106269 0.040001448 0.049714264 0.058420613 0.06621585 0.075848572 0.087140039 0.10183147 0.1169608 0.1305597 0.13719147][0.00026718178 0.0020631414 0.0047732554 0.0088337269 0.014436387 0.020728832 0.026472047 0.031977694 0.037524533 0.045404352 0.056211427 0.071473472 0.088278048 0.10430513 0.11458644][-0.0021790043 -0.0013197744 -4.9006194e-06 0.0019051631 0.0045679174 0.0076342728 0.010587024 0.013580015 0.01699741 0.022795545 0.031896152 0.045685869 0.061837003 0.078193329 0.09046527][-0.0031094798 -0.0028204343 -0.0023477706 -0.001631055 -0.00062532583 0.00052587246 0.001734081 0.0031042334 0.0050482918 0.0089955973 0.01600579 0.027245356 0.041063182 0.05585508 0.068082474][-0.0032186524 -0.0030877092 -0.0029101702 -0.0026633905 -0.0024008993 -0.0021571415 -0.0018391187 -0.0013629051 -0.000417687 0.0019194426 0.0067876885 0.015097355 0.025831537 0.037776224 0.048370615]]...]
INFO - root - 2017-12-10 00:23:13.673872: step 73410, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.931 sec/batch; 66h:58m:34s remains)
INFO - root - 2017-12-10 00:23:22.501466: step 73420, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 65h:11m:36s remains)
INFO - root - 2017-12-10 00:23:31.227575: step 73430, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 60h:05m:46s remains)
INFO - root - 2017-12-10 00:23:39.829834: step 73440, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 58h:48m:44s remains)
INFO - root - 2017-12-10 00:23:48.705693: step 73450, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:14m:34s remains)
INFO - root - 2017-12-10 00:23:57.431883: step 73460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:17m:15s remains)
INFO - root - 2017-12-10 00:24:06.048168: step 73470, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 60h:02m:53s remains)
INFO - root - 2017-12-10 00:24:14.582503: step 73480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:11m:19s remains)
INFO - root - 2017-12-10 00:24:23.194472: step 73490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:54m:00s remains)
INFO - root - 2017-12-10 00:24:31.388773: step 73500, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 59h:39m:51s remains)
2017-12-10 00:24:32.302638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033885108 -0.0033816667 -0.0033753146 -0.0033759447 -0.0033800844 -0.0033855294 -0.003393146 -0.0033999665 -0.0034071878 -0.0034075775 -0.0034080015 -0.0034083484 -0.00340826 -0.0034081137 -0.0034079633][-0.0034052664 -0.0034044059 -0.0033818437 -0.0033419328 -0.0032740431 -0.0032178166 -0.0031355307 -0.0031234524 -0.0031184109 -0.0031578192 -0.0032254588 -0.0032933843 -0.0033703328 -0.0033814635 -0.0034039267][-0.0034046203 -0.0033383295 -0.003143006 -0.0028936369 -0.0026015083 -0.0023321654 -0.0020086444 -0.0019091968 -0.0019133525 -0.0020963228 -0.0023232149 -0.0025974072 -0.0028988586 -0.0030706367 -0.0032613955][-0.0033770096 -0.0032191807 -0.0029220995 -0.0025583659 -0.0021380479 -0.0016972062 -0.0012223241 -0.0009856259 -0.0008660364 -0.000927157 -0.0010638782 -0.0014282214 -0.0018502895 -0.0022425125 -0.0026664194][-0.0033864237 -0.0032633352 -0.0029724496 -0.0025897911 -0.0021141134 -0.0015836641 -0.00099518639 -0.00053202081 -0.00035580178 -0.00023217779 -0.00022194255 -0.00047874986 -0.00080976589 -0.0012846733 -0.0016766512][-0.003400407 -0.0033145025 -0.0030978075 -0.0027936054 -0.0023982185 -0.0020634048 -0.0016582572 -0.0013181879 -0.0012135631 -0.0011650941 -0.0011972392 -0.001132743 -0.0011482614 -0.0013728745 -0.0015110481][-0.0034094979 -0.0033747358 -0.0032729672 -0.0031245952 -0.0028770259 -0.0026639723 -0.0024348304 -0.0021941774 -0.002032348 -0.0019445777 -0.0019489028 -0.0018970876 -0.0018732219 -0.0019691233 -0.0020439129][-0.0034096681 -0.0033984082 -0.0033649961 -0.0032717555 -0.0030287483 -0.0027289544 -0.0024187181 -0.0020367503 -0.0016177709 -0.0012872522 -0.0010904209 -0.0009676693 -0.00087475404 -0.0010106857 -0.0012115089][-0.0034087338 -0.0034080173 -0.0033981167 -0.0032828378 -0.00291351 -0.0024245935 -0.0018602737 -0.001416696 -0.00082497136 -0.00028407504 2.8025126e-05 0.00023351843 0.000384402 0.00058937515 0.00060739531][-0.0034074613 -0.0034070006 -0.0033726234 -0.0032078475 -0.0028032833 -0.0022677658 -0.0015887888 -0.00086234813 7.68085e-06 0.00083787856 0.0015269092 0.002177181 0.0026427356 0.0030374245 0.003204602][-0.0032440475 -0.0033000584 -0.0033024319 -0.0031267903 -0.0027299854 -0.0021319909 -0.001194986 -2.6148511e-05 0.0014168958 0.0028926141 0.0043057073 0.0055681486 0.0065103816 0.00722723 0.0075640632][-0.003250398 -0.0032758911 -0.0032109329 -0.0029125349 -0.0023028541 -0.0013350409 0.00020477641 0.0022191519 0.0045692921 0.0068707094 0.0089365076 0.010641759 0.011847416 0.012590097 0.012863683][-0.0028506564 -0.0027449948 -0.002421943 -0.0017644393 -0.000548732 0.0013011594 0.0038788354 0.0069069862 0.010130363 0.013125002 0.015558977 0.017226029 0.018207269 0.018705828 0.0187204][-0.0017551896 -0.0012843015 -0.00041693309 0.00098646968 0.0031153548 0.0059377262 0.0093765995 0.013084523 0.016739419 0.01993636 0.022360407 0.023664003 0.024169469 0.024135578 0.02361194][2.2300985e-05 0.0010542443 0.0026284137 0.004786225 0.0075951889 0.010906837 0.014555825 0.018242391 0.021629617 0.024390094 0.02628332 0.027071506 0.026966624 0.026233792 0.0250462]]...]
INFO - root - 2017-12-10 00:24:41.013233: step 73510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:21m:37s remains)
INFO - root - 2017-12-10 00:24:49.540451: step 73520, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 58h:44m:24s remains)
INFO - root - 2017-12-10 00:24:58.165408: step 73530, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 62h:58m:37s remains)
INFO - root - 2017-12-10 00:25:06.714596: step 73540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:07m:14s remains)
INFO - root - 2017-12-10 00:25:15.338260: step 73550, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 60h:30m:28s remains)
INFO - root - 2017-12-10 00:25:24.179837: step 73560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:47m:52s remains)
INFO - root - 2017-12-10 00:25:32.904168: step 73570, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.984 sec/batch; 70h:46m:21s remains)
INFO - root - 2017-12-10 00:25:41.467085: step 73580, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 63h:02m:26s remains)
INFO - root - 2017-12-10 00:25:50.268066: step 73590, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:29m:54s remains)
INFO - root - 2017-12-10 00:25:58.487833: step 73600, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 56h:13m:12s remains)
2017-12-10 00:25:59.425248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032343636 -0.0032671776 -0.0032623038 -0.0032088512 -0.0031291647 -0.0030293264 -0.00294681 -0.0028883542 -0.0028747083 -0.0029013166 -0.002912397 -0.0028927187 -0.002851391 -0.0027896098 -0.0027146756][-0.0031985892 -0.0032373525 -0.0032344188 -0.0031791655 -0.0030910759 -0.0029765198 -0.0028846054 -0.0028251656 -0.0028202815 -0.0028483185 -0.0028744207 -0.0028585037 -0.002806697 -0.0027401831 -0.00266371][-0.0032120841 -0.0032488215 -0.0032294374 -0.003166514 -0.0030791585 -0.0029677066 -0.0028796738 -0.0028273561 -0.0028340383 -0.0028652451 -0.0028853775 -0.0028839877 -0.0028424188 -0.0027775492 -0.0027025689][-0.0032671862 -0.0032960952 -0.0032682226 -0.0031979748 -0.0031055389 -0.0029949634 -0.002905393 -0.0028497353 -0.0028574117 -0.002888747 -0.0029025786 -0.0028938579 -0.0028649666 -0.0028103192 -0.0027535781][-0.0033265844 -0.0033482516 -0.0033273655 -0.0032614535 -0.0031708754 -0.0030618219 -0.0029659735 -0.0029053693 -0.002903502 -0.0029260465 -0.0029368678 -0.0029208669 -0.0028722603 -0.0028358242 -0.0028098016][-0.0033630684 -0.0033742839 -0.0033664659 -0.0033230179 -0.0032573084 -0.0031576147 -0.0030525103 -0.0029773165 -0.0029619818 -0.0029729053 -0.0029715523 -0.0029520858 -0.0029054573 -0.0028740498 -0.0028584162][-0.0033801785 -0.0033779093 -0.003372394 -0.0033533226 -0.0033202146 -0.0032469628 -0.00314974 -0.0030639041 -0.0030310079 -0.0030323844 -0.0030336222 -0.0030221285 -0.0029905941 -0.0029642233 -0.0029431004][-0.00339046 -0.0033868856 -0.0033798716 -0.0033667611 -0.0033492085 -0.0033032005 -0.0032256804 -0.0031399007 -0.0030909101 -0.0030861432 -0.003100316 -0.0031121215 -0.0031096265 -0.0031036341 -0.0030974019][-0.0034028734 -0.0034005241 -0.0033932945 -0.0033854772 -0.0033702743 -0.0033415051 -0.0032884052 -0.0032172573 -0.0031671687 -0.0031637794 -0.0031934686 -0.0032290115 -0.0032487852 -0.0032555016 -0.0032587212][-0.0034021279 -0.0034039109 -0.0034056846 -0.0034037353 -0.003393343 -0.0033736436 -0.0033388617 -0.0032954116 -0.0032622202 -0.0032619343 -0.00329019 -0.0033240267 -0.00334459 -0.0033525687 -0.0033565888][-0.0034005756 -0.0034027111 -0.0034050224 -0.003405446 -0.0034032725 -0.0033964862 -0.0033798481 -0.0033581902 -0.0033425074 -0.0033444138 -0.0033598209 -0.003375937 -0.0033843184 -0.0033863839 -0.0033869594][-0.00339871 -0.0034002659 -0.0034024643 -0.0034033493 -0.0034024853 -0.0033997886 -0.0033941634 -0.0033869168 -0.0033800064 -0.0033794004 -0.0033843399 -0.0033876419 -0.003388505 -0.0033873126 -0.0033866209][-0.0033975025 -0.0033982506 -0.0033999924 -0.003401028 -0.0034009288 -0.0033996587 -0.0033971902 -0.0033950661 -0.0033930445 -0.0033917516 -0.0033914903 -0.0033906526 -0.003389976 -0.0033887159 -0.0033877939][-0.0033969604 -0.0033970459 -0.0033984261 -0.0033994641 -0.0033998578 -0.0033992166 -0.0033973882 -0.0033956515 -0.0033942549 -0.003393071 -0.0033923564 -0.0033917357 -0.0033913425 -0.0033901825 -0.0033892104][-0.0033971814 -0.0033969018 -0.0033976438 -0.003398333 -0.0033988941 -0.003398747 -0.0033977639 -0.0033964436 -0.0033952778 -0.0033940121 -0.0033932263 -0.0033924675 -0.0033926538 -0.003392101 -0.0033919963]]...]
INFO - root - 2017-12-10 00:26:07.957927: step 73610, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 63h:15m:37s remains)
INFO - root - 2017-12-10 00:26:16.552850: step 73620, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 62h:25m:27s remains)
INFO - root - 2017-12-10 00:26:25.253308: step 73630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:40m:39s remains)
INFO - root - 2017-12-10 00:26:33.757577: step 73640, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 62h:11m:02s remains)
INFO - root - 2017-12-10 00:26:42.430221: step 73650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:01m:14s remains)
INFO - root - 2017-12-10 00:26:51.083793: step 73660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:07m:08s remains)
INFO - root - 2017-12-10 00:26:59.904558: step 73670, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 65h:07m:49s remains)
INFO - root - 2017-12-10 00:27:08.454290: step 73680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:05m:48s remains)
INFO - root - 2017-12-10 00:27:17.163623: step 73690, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 61h:43m:54s remains)
INFO - root - 2017-12-10 00:27:25.641013: step 73700, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 54h:08m:22s remains)
2017-12-10 00:27:26.547099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033866276 -0.0033847936 -0.0033848493 -0.0033844062 -0.0033845168 -0.0033841792 -0.0033844376 -0.0033843382 -0.0033844912 -0.0033841832 -0.0033843517 -0.0033840383 -0.0033850493 -0.0033865632 -0.0033866265][-0.0033848011 -0.003382619 -0.0033827161 -0.0033823294 -0.0033825042 -0.0033820204 -0.0033820954 -0.0033819824 -0.0033817252 -0.0033812928 -0.0033812548 -0.0033807536 -0.0033815971 -0.0033828118 -0.0033823543][-0.0033858202 -0.0033839424 -0.003383765 -0.0033835091 -0.0033836844 -0.0033834281 -0.0033832209 -0.0033828237 -0.0033824563 -0.0033817994 -0.0033810018 -0.0033803286 -0.0033808795 -0.003381948 -0.0033813296][-0.0033871548 -0.0033852591 -0.0033842286 -0.003383789 -0.0033829911 -0.0033824174 -0.0033819117 -0.0033816963 -0.00338187 -0.0033815966 -0.0033811741 -0.0033810053 -0.0033814711 -0.0033821443 -0.0033810539][-0.0033887422 -0.0033872037 -0.0033856209 -0.003385256 -0.0033842281 -0.0033836334 -0.003382945 -0.0033828723 -0.0033830926 -0.00338273 -0.0033820581 -0.0033816784 -0.0033818642 -0.0033823668 -0.0033807512][-0.0033905818 -0.0033893262 -0.0033873208 -0.0033868232 -0.0033854572 -0.003384959 -0.00338429 -0.0033841643 -0.0033842616 -0.0033842064 -0.0033835366 -0.0033829538 -0.003382945 -0.0033829855 -0.0033810632][-0.0033920668 -0.0033914556 -0.0033896742 -0.00338898 -0.0033875476 -0.0033872058 -0.0033867164 -0.003386633 -0.0033863396 -0.0033856975 -0.0033846011 -0.0033841063 -0.0033842709 -0.0033838695 -0.0033816528][-0.0033924526 -0.0033926968 -0.0033916754 -0.003390952 -0.00338992 -0.003389884 -0.0033899203 -0.0033899404 -0.0033895844 -0.003388454 -0.0033871378 -0.0033863625 -0.00338605 -0.0033851673 -0.0033824905][-0.0033923211 -0.0033927644 -0.0033925832 -0.0033927022 -0.0033920689 -0.0033921862 -0.0033923935 -0.0033926251 -0.0033923732 -0.0033910649 -0.0033901148 -0.0033887117 -0.003388162 -0.0033867988 -0.0033838456][-0.0033915669 -0.0033924263 -0.0033935236 -0.0033945562 -0.0033947062 -0.0033952303 -0.0033955029 -0.0033958177 -0.0033953798 -0.0033938955 -0.0033923041 -0.0033898379 -0.0033887487 -0.0033870214 -0.0033841345][-0.0033896973 -0.0033905911 -0.0033926503 -0.0033941865 -0.0033953155 -0.003396099 -0.0033965318 -0.0033967542 -0.0033962631 -0.0033945346 -0.0033926305 -0.0033900323 -0.0033887266 -0.0033869816 -0.0033842302][-0.0033873308 -0.0033878498 -0.0033900524 -0.0033921583 -0.0033939858 -0.003395722 -0.0033964713 -0.0033964384 -0.0033958012 -0.0033937481 -0.0033917218 -0.0033891851 -0.0033880735 -0.0033865669 -0.0033839715][-0.0033852661 -0.0033847103 -0.0033863205 -0.0033882945 -0.0033898053 -0.0033913697 -0.0033920435 -0.0033920838 -0.0033919029 -0.0033902421 -0.0033890107 -0.0033870554 -0.003386507 -0.0033857014 -0.00338349][-0.0033832111 -0.0033816262 -0.0033824469 -0.0033838428 -0.0033846567 -0.0033857406 -0.0033861457 -0.003386423 -0.0033863515 -0.00338536 -0.0033847776 -0.0033836635 -0.0033838728 -0.0033838372 -0.003382321][-0.003382398 -0.0033799759 -0.0033801387 -0.0033807028 -0.0033808674 -0.0033813058 -0.0033812777 -0.0033813883 -0.0033812823 -0.0033808246 -0.0033806218 -0.0033802572 -0.0033811973 -0.0033820614 -0.0033815436]]...]
INFO - root - 2017-12-10 00:27:35.269916: step 73710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:39m:32s remains)
INFO - root - 2017-12-10 00:27:43.916477: step 73720, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:30m:40s remains)
INFO - root - 2017-12-10 00:27:52.558459: step 73730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:25m:42s remains)
INFO - root - 2017-12-10 00:28:01.138726: step 73740, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 62h:56m:58s remains)
INFO - root - 2017-12-10 00:28:09.882140: step 73750, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 63h:43m:34s remains)
INFO - root - 2017-12-10 00:28:18.659698: step 73760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:34m:56s remains)
INFO - root - 2017-12-10 00:28:27.328806: step 73770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:58m:34s remains)
INFO - root - 2017-12-10 00:28:36.026981: step 73780, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:20m:14s remains)
INFO - root - 2017-12-10 00:28:44.869397: step 73790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 61h:24m:19s remains)
INFO - root - 2017-12-10 00:28:53.261263: step 73800, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 52h:00m:26s remains)
2017-12-10 00:28:54.150447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0024096821 -0.0021569971 -0.0018964472 -0.0018053495 -0.0017450635 -0.0016747622 -0.0017204385 -0.001717405 -0.0018115906 -0.0019122508 -0.0019099123 -0.001928433 -0.0019053764 -0.0018302543 -0.0017976048][-0.0021641655 -0.0019754351 -0.0018137783 -0.0017705937 -0.0017047832 -0.0016536703 -0.0017237163 -0.0017135795 -0.0015991898 -0.0015811609 -0.0015343017 -0.0014494478 -0.0013421217 -0.0012396928 -0.0011576514][-0.0018067625 -0.0015861556 -0.0014052931 -0.0013839079 -0.001379713 -0.0013799584 -0.001416116 -0.0013972952 -0.00117633 -0.0010589813 -0.0009325128 -0.00083098514 -0.00070775207 -0.00056505273 -0.00038519152][-0.0010942302 -0.00092896773 -0.00084022875 -0.0006654819 -0.00056373235 -0.00063483673 -0.00064585288 -0.00070221303 -0.00059487671 -0.00049551064 -0.00049827457 -0.00037235883 -7.6277414e-05 0.000225225 0.0004840435][-0.00013273559 2.2246037e-05 6.419071e-05 0.00019875565 0.00030107587 0.00030771666 0.00035424321 0.00027087238 0.00025195372 0.00019714003 4.8385e-05 0.00010490557 0.00047473074 0.00099742063 0.0013839991][0.00078955037 0.00098310993 0.0010664773 0.0011430231 0.0012175429 0.0012044481 0.0012453676 0.0012277046 0.0012185292 0.0010381246 0.0007915569 0.00075435895 0.001026612 0.001503661 0.0019792158][0.0015273339 0.0017745306 0.0018789775 0.001920759 0.0019261793 0.0018469298 0.0018293632 0.0019059491 0.0019786016 0.0017196077 0.001437865 0.0013667459 0.0014998082 0.0018039106 0.0021280239][0.0019448183 0.0022807366 0.0023681244 0.002389841 0.0023216957 0.0021743539 0.0021637932 0.0022769033 0.0023839094 0.0022231385 0.0020324902 0.0019496034 0.0019912783 0.0021135488 0.0022244628][0.0019829583 0.0023189804 0.0025112748 0.0025369658 0.0024009685 0.00225007 0.0023198228 0.0025014586 0.0026658943 0.0026487636 0.0025412829 0.0025012649 0.002554355 0.0025075797 0.0023620829][0.0016913575 0.0020013822 0.0022292903 0.0023303584 0.0023304306 0.002286914 0.0024232792 0.0026804127 0.002971756 0.0030952564 0.0031186359 0.0031605915 0.003205999 0.003034685 0.0026911742][0.0009206289 0.0012856394 0.0016343759 0.0017967399 0.0019676851 0.0021042095 0.0024037075 0.002860931 0.0033182076 0.00359154 0.0037445787 0.0038488104 0.0039055005 0.0037298414 0.0033489221][-0.00011780602 0.00036924379 0.00080499914 0.0011610922 0.0014616915 0.0017655597 0.0022785475 0.0029937311 0.0036165712 0.004047283 0.00430566 0.0044532549 0.0045452518 0.0044275308 0.0041136006][-0.0012379694 -0.00084586488 -0.0002895575 0.00014233333 0.00057639577 0.0011262277 0.0018569536 0.0028864781 0.0037321474 0.0044000903 0.0047642142 0.0050018895 0.0051506665 0.0049408274 0.0046326797][-0.0022042508 -0.0018620571 -0.0013756098 -0.0008075533 -0.00020799669 0.00063273194 0.0016558908 0.0028917806 0.0038643114 0.0045633484 0.0049244696 0.0051194252 0.005217487 0.0051307157 0.0049088178][-0.0025747789 -0.002383241 -0.0020382097 -0.0013941599 -0.00067942264 0.00030327635 0.0014865098 0.0029215177 0.0040891012 0.0048644682 0.0052562552 0.0054033305 0.0053909179 0.0052016443 0.0049596243]]...]
INFO - root - 2017-12-10 00:29:02.829282: step 73810, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 62h:13m:36s remains)
INFO - root - 2017-12-10 00:29:11.459507: step 73820, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 61h:24m:24s remains)
INFO - root - 2017-12-10 00:29:20.209034: step 73830, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 62h:35m:33s remains)
INFO - root - 2017-12-10 00:29:28.668988: step 73840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:35m:48s remains)
INFO - root - 2017-12-10 00:29:37.300705: step 73850, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 62h:19m:33s remains)
INFO - root - 2017-12-10 00:29:45.967415: step 73860, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:07m:38s remains)
INFO - root - 2017-12-10 00:29:54.503174: step 73870, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:34m:58s remains)
INFO - root - 2017-12-10 00:30:03.159823: step 73880, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 62h:33m:15s remains)
INFO - root - 2017-12-10 00:30:11.964230: step 73890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:24m:12s remains)
INFO - root - 2017-12-10 00:30:20.401025: step 73900, loss = 0.89, batch loss = 0.68 (11.1 examples/sec; 0.719 sec/batch; 51h:39m:37s remains)
2017-12-10 00:30:21.262554: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40613645 0.39395514 0.38013673 0.36639494 0.35202643 0.33910334 0.3260169 0.31362781 0.30167645 0.29034844 0.27800348 0.26680642 0.25533855 0.24420744 0.23458058][0.4270193 0.4217532 0.41357282 0.40390614 0.39235193 0.38189349 0.36973083 0.35661525 0.34290406 0.32899475 0.3131485 0.29653656 0.27923471 0.26352826 0.24970058][0.44293863 0.44671357 0.44616556 0.44273102 0.43623653 0.4287529 0.41645697 0.4021785 0.38593808 0.36864617 0.34827766 0.32561097 0.30241686 0.28100383 0.26214391][0.45632207 0.46997735 0.47865725 0.48248819 0.48220626 0.47843027 0.46688429 0.451513 0.43273431 0.412006 0.386593 0.35777941 0.32820943 0.30081746 0.27635747][0.46511325 0.48767108 0.50517046 0.51741952 0.52457124 0.52436084 0.51420259 0.49836349 0.47678098 0.452779 0.42291746 0.38911217 0.35372093 0.32027 0.29057232][0.46793732 0.49805433 0.52215874 0.5408119 0.55407256 0.55846822 0.55086583 0.53499418 0.51159811 0.48489496 0.45149323 0.41329744 0.37340695 0.33539751 0.30156404][0.464003 0.49893713 0.52710932 0.5507912 0.56831467 0.57596171 0.57052165 0.55566621 0.53237718 0.504239 0.46976331 0.42965081 0.3873587 0.34605628 0.30916554][0.45657471 0.49263215 0.52084965 0.54611117 0.56530792 0.57418317 0.570061 0.55655628 0.533694 0.50573522 0.47179019 0.43252844 0.39048913 0.34834632 0.31050456][0.44255468 0.47908413 0.50654656 0.530966 0.54949653 0.55854183 0.55623055 0.54451376 0.52261144 0.49567267 0.46310848 0.42531738 0.384098 0.34285614 0.30611703][0.42132127 0.45717958 0.48294589 0.50629038 0.52422154 0.53364438 0.53244394 0.52158868 0.50093931 0.47462109 0.4433744 0.40762731 0.36901632 0.33039665 0.2964516][0.3956441 0.42914942 0.45140168 0.47155103 0.48695618 0.49636421 0.49647862 0.48732397 0.46914583 0.44483778 0.41605684 0.382516 0.347128 0.31272233 0.28274283][0.36553407 0.39595124 0.41468689 0.43131933 0.44348368 0.45080402 0.4507485 0.44319409 0.4275063 0.40600866 0.38102907 0.35223562 0.32173491 0.29240522 0.26769018][0.33234221 0.35884225 0.37393594 0.38638186 0.39514032 0.40004584 0.39916584 0.3926664 0.37924421 0.36146569 0.34075484 0.3174156 0.29304954 0.27013028 0.25170583][0.29740435 0.3199206 0.33168277 0.3411119 0.34787086 0.35156623 0.35081077 0.34536603 0.3346917 0.320943 0.30480316 0.28678691 0.26859429 0.25190568 0.23900451][0.26708844 0.28507048 0.29286551 0.29920354 0.30369183 0.30629072 0.30586794 0.30219683 0.29500547 0.28537035 0.27408493 0.26157928 0.24929357 0.2382914 0.22991942]]...]
INFO - root - 2017-12-10 00:30:29.963611: step 73910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 60h:19m:37s remains)
INFO - root - 2017-12-10 00:30:38.427992: step 73920, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:34m:13s remains)
INFO - root - 2017-12-10 00:30:46.954036: step 73930, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 60h:39m:50s remains)
INFO - root - 2017-12-10 00:30:55.450078: step 73940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:08m:12s remains)
INFO - root - 2017-12-10 00:31:04.211984: step 73950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:01m:09s remains)
INFO - root - 2017-12-10 00:31:12.849509: step 73960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 61h:23m:44s remains)
INFO - root - 2017-12-10 00:31:21.411833: step 73970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:43m:20s remains)
INFO - root - 2017-12-10 00:31:30.038761: step 73980, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 60h:26m:08s remains)
INFO - root - 2017-12-10 00:31:38.704886: step 73990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:33m:13s remains)
INFO - root - 2017-12-10 00:31:47.211112: step 74000, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:13m:45s remains)
2017-12-10 00:31:48.062643: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0098616332 0.0099636624 0.0094679343 0.0079572555 0.0055015078 0.003365387 0.0018452376 0.0013302073 0.0013823153 0.0020674833 0.0031081454 0.0046413485 0.0061780531 0.0073152827 0.0078236237][0.0096465275 0.0097202957 0.0090108 0.0073475167 0.0049116444 0.0028490683 0.0014393611 0.0011783794 0.0013933445 0.0021938023 0.0033879012 0.0051123304 0.0067419214 0.00799514 0.0085757878][0.008546059 0.0086728279 0.0078991251 0.0061407369 0.0039670775 0.0022426185 0.0010551915 0.0010809635 0.0015541373 0.0023425457 0.0036372573 0.0054118484 0.0070589241 0.0084270714 0.00905292][0.0071272245 0.0072897077 0.0065639913 0.0048728408 0.0029071097 0.0015309092 0.00072218035 0.0010699809 0.0018193496 0.0026293041 0.0039832536 0.0057031121 0.0073062275 0.0086414032 0.0092000226][0.0055704536 0.0057482217 0.0050473474 0.0034887122 0.001804413 0.00070638559 0.00024791807 0.00087883906 0.0019230817 0.0028742792 0.0042466139 0.0058690635 0.0073598456 0.0085706608 0.0090009775][0.0038378306 0.004082974 0.0033943926 0.0020562236 0.00063435989 -0.00026296009 -0.00039042858 0.00032892148 0.0016030802 0.0027339549 0.0041174786 0.0056912126 0.0070521692 0.0080419937 0.00829198][0.0019768679 0.0022600291 0.0016188819 0.00058645918 -0.00051504 -0.0012142793 -0.0010969331 -0.0003061716 0.0010273128 0.0023045598 0.0036227021 0.0051134424 0.0063739615 0.0071677491 0.0072685918][0.000382439 0.0006418603 0.00012139464 -0.00064258161 -0.0014997971 -0.0020110386 -0.0017909512 -0.0010107057 0.00029801298 0.0015315881 0.0026790451 0.0040432522 0.0052544335 0.0059013544 0.0059713274][-0.00075351144 -0.00056916336 -0.00089062867 -0.0013885605 -0.0020243244 -0.0023931474 -0.0022725931 -0.0017328489 -0.00063283741 0.0004404888 0.0014403414 0.0026942489 0.0037866256 0.00423601 0.00425509][-0.00084194378 -0.0010195533 -0.0013741718 -0.0017582702 -0.0022340789 -0.0025080843 -0.00253543 -0.0022601327 -0.001512088 -0.00075271865 0.00015214342 0.0013367583 0.0022563471 0.0025453574 0.0025014102][-0.00021752948 -0.00050974521 -0.00094754552 -0.0015571162 -0.0021318751 -0.0024247142 -0.0025861396 -0.002521605 -0.0020916085 -0.0016638658 -0.00094243791 8.7864231e-05 0.00074503268 0.0009086451 0.00088077667][0.00090044062 0.0006156771 0.00014340342 -0.00068327389 -0.0014745723 -0.00200277 -0.0024298923 -0.0025970959 -0.0024783826 -0.0021899249 -0.0016641548 -0.00082591549 -0.00040876446 -0.00024537463 -0.00013199402][0.002320437 0.002081075 0.001612542 0.00071328063 -0.00023596617 -0.0010553866 -0.0018357651 -0.0023462479 -0.0026588656 -0.0025084177 -0.0021096347 -0.0013813511 -0.0010272812 -0.00072789658 -0.00044183712][0.0041256743 0.0038220605 0.0032352342 0.0022827296 0.0012726227 0.00022009085 -0.0008801443 -0.0017962396 -0.0024965035 -0.0025146031 -0.0022399256 -0.0015765259 -0.0010845433 -0.00059925718 -7.3064119e-05][0.0057247682 0.0053764153 0.0047563277 0.0037968822 0.0027412351 0.0014735844 0.00010775449 -0.0011995973 -0.0022076038 -0.0023977351 -0.002172783 -0.0014827868 -0.000645621 6.3952291e-05 0.00081036449]]...]
INFO - root - 2017-12-10 00:31:56.794314: step 74010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:54m:16s remains)
INFO - root - 2017-12-10 00:32:05.487010: step 74020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 59h:21m:11s remains)
INFO - root - 2017-12-10 00:32:14.076119: step 74030, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 61h:07m:38s remains)
INFO - root - 2017-12-10 00:32:22.437142: step 74040, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 61h:41m:51s remains)
INFO - root - 2017-12-10 00:32:31.122863: step 74050, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 62h:20m:35s remains)
INFO - root - 2017-12-10 00:32:39.847776: step 74060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:16m:21s remains)
INFO - root - 2017-12-10 00:32:48.516555: step 74070, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:28m:19s remains)
INFO - root - 2017-12-10 00:32:57.241521: step 74080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 63h:09m:26s remains)
INFO - root - 2017-12-10 00:33:05.994469: step 74090, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:32m:56s remains)
INFO - root - 2017-12-10 00:33:14.481057: step 74100, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:56m:49s remains)
2017-12-10 00:33:15.378351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0023381147 -0.0019667833 -0.0012658925 -0.00022865785 0.00080253254 0.0017025291 0.0023357018 0.0025852055 0.0028452703 0.0031247681 0.0035620437 0.00394239 0.0039525833 0.003287947 0.0017167053][-0.00088729733 5.0616218e-05 0.00163746 0.003772557 0.0057534762 0.0072809476 0.00785744 0.0072326418 0.0060430672 0.0046212357 0.0035191074 0.0027114118 0.0020063317 0.001119632 -0.00017147814][0.00021980121 0.0019994357 0.00506049 0.0092000533 0.013115119 0.016146185 0.01724888 0.015924523 0.012846034 0.0089094043 0.0055120857 0.0029143628 0.0011003918 -0.00025501498 -0.0013999422][0.0014665637 0.0044810977 0.0095443167 0.016178876 0.022700373 0.027977185 0.03020983 0.028367897 0.023264932 0.016498655 0.010237342 0.0051854774 0.0016796 -0.00058843638 -0.0019382646][0.0040343357 0.0080715287 0.015169112 0.024690554 0.034340013 0.042200357 0.045970432 0.044158127 0.037392385 0.027844978 0.018381987 0.010285327 0.0042917216 0.00034205103 -0.0018078458][0.0084433565 0.01348036 0.022368584 0.034265082 0.046629865 0.056937091 0.062360868 0.061116129 0.053556819 0.04224981 0.030276945 0.019187624 0.010193817 0.00374142 -6.0892664e-05][0.012358421 0.017695272 0.02768763 0.041518934 0.056439582 0.06929528 0.076710448 0.077153012 0.070613846 0.05930049 0.045927115 0.032507692 0.020576175 0.01093445 0.0043009035][0.014209628 0.019136453 0.028681247 0.042395178 0.058052137 0.072368316 0.081849925 0.085130237 0.081878632 0.07373476 0.062224306 0.048724372 0.034927584 0.022194855 0.011966789][0.013085715 0.01724143 0.025210189 0.03698175 0.05118791 0.065094478 0.07555981 0.081669278 0.082863368 0.080021694 0.07323724 0.062930137 0.050094172 0.035904218 0.022560215][0.0088945962 0.012012308 0.017940931 0.026786383 0.0378402 0.04936152 0.059462473 0.067701921 0.073433332 0.076720446 0.076420918 0.071715295 0.062284924 0.048726644 0.033575337][0.0040417407 0.0061114505 0.0098790834 0.01552744 0.022908615 0.03121181 0.039616726 0.048299711 0.056993511 0.065227479 0.070929781 0.072292 0.067650415 0.056652524 0.04159699][0.00022860267 0.001410749 0.0034648606 0.00652714 0.010643787 0.01562893 0.021596616 0.029117217 0.03831261 0.048699476 0.058044374 0.063984171 0.06388101 0.05647992 0.04346719][-0.002015966 -0.0014216134 -0.00046879076 0.00091299857 0.0028107904 0.0052720048 0.0087189833 0.013956696 0.021577487 0.031327732 0.041341823 0.049209852 0.052060064 0.048178487 0.038412619][-0.00302828 -0.0027586943 -0.0023939926 -0.0018839156 -0.0011679055 -0.00017352216 0.0014525095 0.00439696 0.0094398977 0.016668146 0.024893628 0.032127362 0.035931028 0.034629405 0.0283944][-0.0033253038 -0.0032329222 -0.0031224547 -0.002965905 -0.0027488172 -0.0024370467 -0.0018395621 -0.00051152962 0.0021935662 0.0065220641 0.01192226 0.017102174 0.020369189 0.020427149 0.017070897]]...]
INFO - root - 2017-12-10 00:33:23.926223: step 74110, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 61h:00m:32s remains)
INFO - root - 2017-12-10 00:33:32.751306: step 74120, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.943 sec/batch; 67h:42m:27s remains)
INFO - root - 2017-12-10 00:33:41.428811: step 74130, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:44m:43s remains)
INFO - root - 2017-12-10 00:33:49.937218: step 74140, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 60h:39m:51s remains)
INFO - root - 2017-12-10 00:33:58.615358: step 74150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:55m:06s remains)
INFO - root - 2017-12-10 00:34:07.315655: step 74160, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:04m:27s remains)
INFO - root - 2017-12-10 00:34:15.979270: step 74170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:49m:50s remains)
INFO - root - 2017-12-10 00:34:24.752446: step 74180, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 64h:36m:06s remains)
INFO - root - 2017-12-10 00:34:33.398405: step 74190, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 59h:58m:20s remains)
INFO - root - 2017-12-10 00:34:41.982467: step 74200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:42m:31s remains)
2017-12-10 00:34:42.829882: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16277181 0.13646138 0.1101456 0.090435 0.081374712 0.084470674 0.0964545 0.11186511 0.12876719 0.14340821 0.15382989 0.16047713 0.16344167 0.16284169 0.1579539][0.15374114 0.12744631 0.10289524 0.085804246 0.080948524 0.088226534 0.10374012 0.1205461 0.13763914 0.15215914 0.16211119 0.16759017 0.16974778 0.16873103 0.1634807][0.14380939 0.11875217 0.098081715 0.085049979 0.085095443 0.095888138 0.11335094 0.1299777 0.14576449 0.158454 0.16667689 0.17082912 0.17208993 0.17036141 0.16481988][0.13800816 0.11599865 0.099000365 0.090580516 0.094999343 0.10798857 0.12606265 0.14138637 0.15495366 0.16484147 0.1705994 0.17269962 0.17228246 0.16925603 0.16334897][0.13596635 0.11779033 0.1054872 0.10188571 0.10976218 0.12328106 0.13961685 0.15278059 0.16344015 0.17054234 0.17317192 0.17273143 0.16990159 0.16498992 0.15861373][0.13717933 0.12343466 0.11563678 0.11629912 0.1256444 0.13833454 0.15215094 0.16272113 0.1707069 0.17489605 0.17446871 0.17101479 0.16517553 0.15792915 0.1510139][0.13949147 0.13025416 0.12634121 0.12980606 0.1391468 0.15007988 0.16082884 0.16945946 0.17500173 0.1766604 0.17339876 0.16711189 0.15830861 0.14887658 0.14135557][0.14050551 0.13562161 0.135217 0.14035745 0.14904796 0.15778954 0.16506027 0.17135653 0.17481422 0.17482089 0.16934471 0.16076638 0.14954 0.13855784 0.13100341][0.13923542 0.1372007 0.13910703 0.14474879 0.1521536 0.15907958 0.16396183 0.16814978 0.16967839 0.16844814 0.16165642 0.15200794 0.13968606 0.12837076 0.12146989][0.13503914 0.13502297 0.13817085 0.14367031 0.14982231 0.15430769 0.15680099 0.15917605 0.15904905 0.15709139 0.15030544 0.14079146 0.12923868 0.11904334 0.11351413][0.12848671 0.12948109 0.13254333 0.13683291 0.14175364 0.14508356 0.14615938 0.14640199 0.14532262 0.14265572 0.13670063 0.12826753 0.11868975 0.11047924 0.10670856][0.1204308 0.12143701 0.12358552 0.12668456 0.13018923 0.13219588 0.13220046 0.13129875 0.12904344 0.12652391 0.12169421 0.11503023 0.10785438 0.10190763 0.099968836][0.11154912 0.11144307 0.11239988 0.11388377 0.1158593 0.11650314 0.11603823 0.1147745 0.11265897 0.11048695 0.10661656 0.10155122 0.096770406 0.092959821 0.092665352][0.10048683 0.099779539 0.09977711 0.10037785 0.10121849 0.10094932 0.10027261 0.0990747 0.097124569 0.095172741 0.0925708 0.089200176 0.086614214 0.08456485 0.085507326][0.089683495 0.088774078 0.088248819 0.088337012 0.088554196 0.088277481 0.08763919 0.086463057 0.08474689 0.083137907 0.081107408 0.079204723 0.078311227 0.077533945 0.079245739]]...]
INFO - root - 2017-12-10 00:34:50.974820: step 74210, loss = 0.88, batch loss = 0.67 (8.8 examples/sec; 0.905 sec/batch; 64h:56m:20s remains)
INFO - root - 2017-12-10 00:34:59.581859: step 74220, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 59h:49m:35s remains)
INFO - root - 2017-12-10 00:35:07.914549: step 74230, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:20m:38s remains)
INFO - root - 2017-12-10 00:35:16.399609: step 74240, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 58h:58m:29s remains)
INFO - root - 2017-12-10 00:35:24.983802: step 74250, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 61h:15m:21s remains)
INFO - root - 2017-12-10 00:35:33.594412: step 74260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:38m:35s remains)
INFO - root - 2017-12-10 00:35:42.143067: step 74270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:04m:35s remains)
INFO - root - 2017-12-10 00:35:50.717027: step 74280, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 61h:45m:51s remains)
INFO - root - 2017-12-10 00:35:59.368423: step 74290, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 61h:50m:41s remains)
INFO - root - 2017-12-10 00:36:07.811463: step 74300, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:23m:55s remains)
2017-12-10 00:36:08.727533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033894042 -0.0033881436 -0.003388155 -0.0033875455 -0.0033648482 -0.00329431 -0.0031957086 -0.0031272 -0.0031268154 -0.0031918157 -0.0032721418 -0.0033354934 -0.003370984 -0.0033848262 -0.003388131][-0.0033879639 -0.0033866276 -0.0033846444 -0.0033626338 -0.0032547936 -0.0030250764 -0.0027525253 -0.0026069859 -0.0026618061 -0.0028687813 -0.0030949961 -0.0032610551 -0.0033483263 -0.0033800264 -0.0033868935][-0.0033882577 -0.0033864633 -0.0033629881 -0.0032197663 -0.0027842936 -0.0020654164 -0.0013467909 -0.0010670219 -0.0013301824 -0.0019501199 -0.0025863694 -0.0030402914 -0.0032775812 -0.0033654617 -0.0033860607][-0.0033888819 -0.0033837995 -0.0032790524 -0.0027838894 -0.0015355079 0.00028809253 0.0019477366 0.0024906078 0.0017655736 0.00023706816 -0.0013292141 -0.0024596825 -0.0030696697 -0.0033117428 -0.0033781379][-0.0033899571 -0.0033729936 -0.0030843604 -0.0019138401 0.00074803387 0.0043801088 0.007524794 0.0084992517 0.0070680184 0.004083978 0.00096239126 -0.0013473041 -0.0026425007 -0.0031886569 -0.0033545836][-0.0033890756 -0.0033478811 -0.002783729 -0.00070675951 0.0037314321 0.0095468089 0.014437132 0.015920771 0.013651108 0.0089222193 0.0039044253 0.0001269416 -0.002051695 -0.0030088881 -0.0033163095][-0.0033725903 -0.0032927906 -0.0024468098 0.00045794272 0.006432686 0.014078077 0.020388303 0.022254366 0.01923177 0.013006508 0.0063899327 0.0013821304 -0.0015420658 -0.0028513763 -0.0032812974][-0.0033505575 -0.0032329068 -0.0022213007 0.0010691753 0.0076685362 0.015991358 0.02276255 0.024671534 0.021260487 0.014412273 0.0071978346 0.0017655015 -0.0013971226 -0.002809685 -0.0032714393][-0.0033318717 -0.0031975934 -0.002231475 0.00079514063 0.006794272 0.014303919 0.020339437 0.021913595 0.018662877 0.012376331 0.0058732117 0.001050066 -0.0017081298 -0.0029112871 -0.003293647][-0.0033289946 -0.0032084847 -0.002475112 -0.00022636284 0.0042507397 0.0098607261 0.014317174 0.015331395 0.012716202 0.0079227593 0.0031105506 -0.00036498415 -0.0022871592 -0.0030898578 -0.0033324191][-0.0033428238 -0.0032599734 -0.0028301151 -0.0015088072 0.0011971577 0.0046323827 0.007336434 0.0078220489 0.0060623959 0.0030676483 0.00019701337 -0.0017933593 -0.0028398877 -0.0032505442 -0.0033655255][-0.0033663195 -0.0033245394 -0.0031378951 -0.002546601 -0.0012625477 0.00040703733 0.0017123555 0.0018679032 0.00092117372 -0.000553536 -0.0018854105 -0.0027598809 -0.003190215 -0.0033456562 -0.0033841927][-0.0033873513 -0.0033757081 -0.0033230959 -0.0031346604 -0.002680494 -0.0020730032 -0.0015991199 -0.001573341 -0.0019496467 -0.0024841935 -0.0029362109 -0.0032149584 -0.0033414408 -0.0033822095 -0.0033900726][-0.0033880826 -0.0033874898 -0.0033840961 -0.0033526416 -0.0032496182 -0.003103547 -0.0029876209 -0.0029861967 -0.0030812407 -0.0032047334 -0.0033035758 -0.0033610044 -0.0033844418 -0.0033903341 -0.0033902016][-0.0033870165 -0.0033861231 -0.0033869981 -0.003386755 -0.003377791 -0.0033598132 -0.0033428147 -0.0033410622 -0.003352691 -0.0033680145 -0.0033804365 -0.0033877261 -0.0033900472 -0.003389885 -0.003388976]]...]
INFO - root - 2017-12-10 00:36:17.296058: step 74310, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 61h:58m:06s remains)
INFO - root - 2017-12-10 00:36:26.119400: step 74320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:32m:11s remains)
INFO - root - 2017-12-10 00:36:34.767842: step 74330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:33m:57s remains)
INFO - root - 2017-12-10 00:36:43.349749: step 74340, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 60h:37m:24s remains)
INFO - root - 2017-12-10 00:36:51.996971: step 74350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:41m:58s remains)
INFO - root - 2017-12-10 00:37:00.578090: step 74360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 60h:14m:48s remains)
INFO - root - 2017-12-10 00:37:09.256005: step 74370, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:28m:20s remains)
INFO - root - 2017-12-10 00:37:17.896018: step 74380, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 61h:44m:17s remains)
INFO - root - 2017-12-10 00:37:26.608705: step 74390, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:08m:01s remains)
INFO - root - 2017-12-10 00:37:35.090549: step 74400, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 58h:58m:01s remains)
2017-12-10 00:37:35.945061: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.008591203 0.0078705978 0.0065614115 0.0048770932 0.0036023518 0.0031228552 0.003274583 0.0037788081 0.0042425329 0.0046323324 0.00481491 0.0047670249 0.0043839971 0.0037769529 0.0031066334][0.014931832 0.013455445 0.011262864 0.0088296942 0.0068754861 0.005793578 0.0053764051 0.0053032404 0.0052338382 0.0052414238 0.0051968293 0.0051471395 0.0048546121 0.004338854 0.0035927745][0.022332108 0.020435777 0.017074682 0.013348678 0.01015179 0.0080356831 0.0068311924 0.0061162608 0.0055412985 0.0051139994 0.0047533447 0.0045232931 0.0041686036 0.0036681991 0.0029195913][0.027930466 0.027089467 0.023552233 0.018744126 0.014067728 0.010439903 0.007998242 0.0064845532 0.0055205887 0.004895526 0.0043750573 0.0039712116 0.0034480921 0.0028121995 0.001953806][0.031946141 0.031426486 0.028402647 0.023784449 0.01841446 0.013446076 0.0095922733 0.006971648 0.0053945165 0.0045688637 0.003966867 0.0034516559 0.0027714006 0.0019902864 0.0010893133][0.036004223 0.035617877 0.032216016 0.027374411 0.021783559 0.016209964 0.01140957 0.0077596111 0.005419259 0.00412644 0.0032760559 0.0025814103 0.0017912516 0.00098193507 0.0001707105][0.037826955 0.038050879 0.034848429 0.030119976 0.024443796 0.018537439 0.013186643 0.0088497438 0.0058378512 0.0039659431 0.002700397 0.0017734871 0.00094123813 0.00026671775 -0.00031214207][0.039089248 0.039671842 0.036293335 0.031587228 0.026024569 0.020104719 0.014522795 0.0097388932 0.006182122 0.0037873534 0.0022002731 0.0011758513 0.00045270007 -5.724607e-06 -0.00031101564][0.04112301 0.042445164 0.038809743 0.033731386 0.027773371 0.021482583 0.015459901 0.010119286 0.0059793387 0.0031552871 0.0014299138 0.00048140413 -2.5862362e-05 -0.00025995052 -0.00038438523][0.044379927 0.046446744 0.042539854 0.036987018 0.030227883 0.023135638 0.016344316 0.010225737 0.0053610522 0.0020282969 0.00015613507 -0.00066942908 -0.00096727139 -0.0010544064 -0.0010633352][0.04837279 0.051120974 0.046994269 0.040682171 0.032863438 0.024711102 0.01693261 0.0099258907 0.0044067809 0.00076252013 -0.0010733085 -0.0017607495 -0.0019557392 -0.00198411 -0.0019567651][0.050281379 0.052729733 0.048113056 0.041092984 0.032497603 0.023781527 0.015564654 0.0083502149 0.0028980838 -0.00047672726 -0.0019853741 -0.0024841879 -0.0026189077 -0.0026344487 -0.0026078946][0.05009165 0.05196033 0.046753488 0.039109111 0.030033452 0.021175073 0.013032071 0.0062253792 0.0013545405 -0.0014280174 -0.0025519473 -0.0028743055 -0.0029462951 -0.0029407851 -0.0029127395][0.046459485 0.04832346 0.043426361 0.03591691 0.026900873 0.01819719 0.010396504 0.0042332392 6.21269e-05 -0.0021324574 -0.002919808 -0.0031110137 -0.0031386116 -0.0031231572 -0.0031023263][0.041151974 0.042616919 0.038052484 0.030933058 0.022422709 0.014330976 0.00734339 0.0021628521 -0.0010915061 -0.0026547543 -0.003163287 -0.00326972 -0.003277882 -0.0032698386 -0.0032602975]]...]
INFO - root - 2017-12-10 00:37:44.564765: step 74410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:44m:55s remains)
INFO - root - 2017-12-10 00:37:53.247657: step 74420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:31m:12s remains)
INFO - root - 2017-12-10 00:38:01.849817: step 74430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:59m:04s remains)
INFO - root - 2017-12-10 00:38:10.366482: step 74440, loss = 0.89, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 56h:04m:17s remains)
INFO - root - 2017-12-10 00:38:18.981793: step 74450, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 59h:03m:35s remains)
INFO - root - 2017-12-10 00:38:27.662453: step 74460, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:09m:56s remains)
INFO - root - 2017-12-10 00:38:36.297693: step 74470, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 61h:21m:56s remains)
INFO - root - 2017-12-10 00:38:45.008140: step 74480, loss = 0.88, batch loss = 0.67 (9.3 examples/sec; 0.863 sec/batch; 61h:49m:16s remains)
INFO - root - 2017-12-10 00:38:53.593696: step 74490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:43m:36s remains)
INFO - root - 2017-12-10 00:39:02.086026: step 74500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:43m:40s remains)
2017-12-10 00:39:03.003853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033917951 -0.003388664 -0.0033868393 -0.0033858207 -0.0033857168 -0.0033859813 -0.0033863194 -0.003386993 -0.0033878116 -0.0033887136 -0.0033898649 -0.0033910612 -0.0033919611 -0.0033923744 -0.0033917583][-0.0033797175 -0.0033716892 -0.0033673064 -0.0033694133 -0.0033744646 -0.0033786318 -0.0033813156 -0.0033831256 -0.0033839892 -0.0033845026 -0.0033852847 -0.0033859666 -0.0033860698 -0.0033858151 -0.0033855755][-0.0033439426 -0.003323959 -0.0033160027 -0.0033290694 -0.0033488367 -0.0033650405 -0.0033730837 -0.0033748245 -0.0033671889 -0.0033488378 -0.0033223641 -0.0032903687 -0.0032608893 -0.0032539761 -0.003277211][-0.0032582833 -0.0032156017 -0.0032047853 -0.0032406405 -0.0032881729 -0.0033289143 -0.0033463282 -0.0033451959 -0.0033057139 -0.0032128769 -0.0030712537 -0.0028951091 -0.00273142 -0.0026814844 -0.0027906126][-0.0031162186 -0.0030446828 -0.0030369861 -0.0031069254 -0.0031911451 -0.0032620579 -0.0032902574 -0.0032803337 -0.0031747171 -0.0029189859 -0.0025128485 -0.0020006481 -0.0015258248 -0.0013792594 -0.0016819228][-0.0029542043 -0.0028586283 -0.0028623585 -0.0029674908 -0.0030834198 -0.0031760125 -0.0032117611 -0.0031862613 -0.0029879713 -0.0024942379 -0.0016857397 -0.00066158012 0.00028165197 0.000569775 -1.8893974e-05][-0.00284905 -0.0027474703 -0.0027654353 -0.0028886814 -0.0030129994 -0.0031058264 -0.0031394444 -0.0030967824 -0.0028121744 -0.0020839809 -0.00086383824 0.00068084314 0.0020855183 0.002501996 0.0016237928][-0.0028576227 -0.0027713119 -0.0027989773 -0.0029129148 -0.0030187981 -0.0030913877 -0.0031154091 -0.0030605779 -0.0027313372 -0.0018723428 -0.00041121314 0.0014320293 0.0030791324 0.0035389636 0.0024920756][-0.0029727605 -0.0029131162 -0.002939468 -0.0030238819 -0.0030974937 -0.0031431245 -0.0031599775 -0.0031070583 -0.0027935633 -0.0019655339 -0.00054925913 0.0012215127 0.0027704036 0.0031665789 0.0021561373][-0.0031358493 -0.0031020497 -0.0031186731 -0.0031674786 -0.0032082545 -0.0032308009 -0.0032410859 -0.0032008549 -0.0029573613 -0.0023059936 -0.0011937222 0.00017764908 0.0013472403 0.0016134956 0.00082160416][-0.0032750538 -0.0032588167 -0.0032653441 -0.0032859254 -0.0033034454 -0.0033125957 -0.0033174516 -0.0032920407 -0.0031372695 -0.0027224729 -0.0020199551 -0.001170198 -0.00046489015 -0.00033066003 -0.00083216815][-0.0033563841 -0.0033494351 -0.0033498642 -0.0033554547 -0.0033615273 -0.0033657406 -0.0033680478 -0.0033547732 -0.0032759672 -0.0030673053 -0.0027185108 -0.0023072902 -0.0019768332 -0.0019295665 -0.0021804776][-0.00338851 -0.0033858316 -0.0033846635 -0.003384724 -0.0033860665 -0.0033880863 -0.0033895571 -0.0033845524 -0.003354436 -0.0032756336 -0.0031463667 -0.0029988268 -0.0028853205 -0.0028753246 -0.0029690876][-0.0033963413 -0.0033951481 -0.0033941474 -0.0033929413 -0.0033921329 -0.0033926445 -0.0033932677 -0.003392197 -0.0033849676 -0.0033658142 -0.0033341013 -0.0032992032 -0.0032740682 -0.0032733651 -0.0032964079][-0.0033955029 -0.0033951669 -0.0033947772 -0.0033935581 -0.0033920032 -0.0033911604 -0.003390704 -0.0033904682 -0.0033902498 -0.0033890719 -0.0033861243 -0.0033827336 -0.0033807568 -0.0033808863 -0.0033834914]]...]
INFO - root - 2017-12-10 00:39:11.410012: step 74510, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:48m:36s remains)
INFO - root - 2017-12-10 00:39:19.887547: step 74520, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.804 sec/batch; 57h:37m:27s remains)
INFO - root - 2017-12-10 00:39:28.343881: step 74530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:39m:49s remains)
INFO - root - 2017-12-10 00:39:36.737432: step 74540, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.722 sec/batch; 51h:42m:25s remains)
INFO - root - 2017-12-10 00:39:45.303190: step 74550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 62h:21m:05s remains)
INFO - root - 2017-12-10 00:39:53.962713: step 74560, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:00m:55s remains)
INFO - root - 2017-12-10 00:40:02.615104: step 74570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 63h:19m:53s remains)
INFO - root - 2017-12-10 00:40:11.194212: step 74580, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 59h:16m:13s remains)
INFO - root - 2017-12-10 00:40:19.763153: step 74590, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:27m:12s remains)
INFO - root - 2017-12-10 00:40:28.295130: step 74600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:55m:52s remains)
2017-12-10 00:40:29.249589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033896477 -0.0033954408 -0.0034005884 -0.0034032091 -0.0034028706 -0.0034002017 -0.003396407 -0.0033928282 -0.003390813 -0.0033900642 -0.0033908249 -0.0033913504 -0.0033900817 -0.0033871934 -0.0033823019][-0.0033877755 -0.0033944028 -0.0034007083 -0.0034042543 -0.003404255 -0.0034011675 -0.0033962661 -0.0033913641 -0.0033880675 -0.0033866267 -0.0033876635 -0.0033888647 -0.0033885539 -0.0033870449 -0.0033832851][-0.0033857287 -0.0033923779 -0.0033994075 -0.0034038385 -0.0034047987 -0.0034022692 -0.0033971909 -0.0033920705 -0.0033884728 -0.003386596 -0.0033872386 -0.0033884579 -0.0033885092 -0.0033877094 -0.0033848253][-0.0033831219 -0.0033892896 -0.0033962161 -0.0034012904 -0.0034031086 -0.0034018389 -0.0033983735 -0.0033943306 -0.0033912738 -0.0033896046 -0.0033901129 -0.0033908682 -0.0033903164 -0.0033891625 -0.0033862125][-0.0033814402 -0.0033869159 -0.003393231 -0.0033984864 -0.0034009493 -0.0034008985 -0.0033991882 -0.0033970398 -0.0033955364 -0.0033948631 -0.0033951525 -0.00339512 -0.003393753 -0.0033916791 -0.0033878481][-0.0033815945 -0.0033862116 -0.0033918393 -0.0033964457 -0.0033985311 -0.003398736 -0.0033981632 -0.0033981202 -0.0033986424 -0.0033994231 -0.0034001742 -0.0033997726 -0.0033972927 -0.0033937201 -0.0033886586][-0.0033840418 -0.0033881194 -0.0033925672 -0.0033958261 -0.0033968436 -0.0033962778 -0.003395587 -0.0033968978 -0.0033996424 -0.0034024138 -0.0034044916 -0.0034045712 -0.0034014459 -0.0033964198 -0.0033900575][-0.0033878891 -0.0033911166 -0.0033944421 -0.003396316 -0.0033963174 -0.0033951472 -0.0033938508 -0.003394994 -0.0033991188 -0.0034037561 -0.0034071521 -0.0034080378 -0.0034048327 -0.0033987896 -0.0033911464][-0.0033919159 -0.0033945569 -0.0033968904 -0.003397526 -0.0033966131 -0.0033953981 -0.0033943153 -0.0033952573 -0.0033988825 -0.0034036962 -0.0034074099 -0.0034086697 -0.0034056469 -0.0033994138 -0.0033913525][-0.0033950878 -0.0033978778 -0.0034001793 -0.0034001975 -0.0033989549 -0.0033979004 -0.0033970326 -0.0033979344 -0.0034006576 -0.003404526 -0.0034074481 -0.0034080807 -0.00340485 -0.0033985006 -0.0033903725][-0.003395803 -0.0033992517 -0.0034023784 -0.003403055 -0.003402363 -0.0034018513 -0.0034016378 -0.0034025768 -0.0034045342 -0.0034069873 -0.0034085365 -0.0034078758 -0.0034038634 -0.0033970862 -0.0033887597][-0.0033928687 -0.003396821 -0.003400886 -0.00340275 -0.0034036029 -0.0034044373 -0.0034054511 -0.0034072157 -0.0034088525 -0.0034100045 -0.0034097808 -0.003407557 -0.0034027859 -0.0033958463 -0.0033876752][-0.003386846 -0.0033906931 -0.0033955236 -0.0033988012 -0.0034011139 -0.0034037184 -0.0034060145 -0.0034083945 -0.0034101075 -0.0034107286 -0.0034096029 -0.003406202 -0.0034005819 -0.0033935725 -0.0033858588][-0.0033805172 -0.0033840262 -0.00338904 -0.0033934014 -0.0033968496 -0.0034001262 -0.0034028892 -0.0034055321 -0.0034074176 -0.0034078483 -0.0034065959 -0.0034029973 -0.0033972037 -0.0033904715 -0.0033835373][-0.0033752187 -0.0033778704 -0.0033823405 -0.0033866744 -0.003390654 -0.0033944645 -0.003397461 -0.0033999295 -0.0034016545 -0.0034019903 -0.0034008007 -0.0033974585 -0.0033923113 -0.0033864283 -0.0033804071]]...]
INFO - root - 2017-12-10 00:40:37.681737: step 74610, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:28m:17s remains)
INFO - root - 2017-12-10 00:40:46.299570: step 74620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:27m:52s remains)
INFO - root - 2017-12-10 00:40:54.848963: step 74630, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 60h:52m:23s remains)
INFO - root - 2017-12-10 00:41:03.280013: step 74640, loss = 0.90, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 56h:05m:08s remains)
INFO - root - 2017-12-10 00:41:11.864252: step 74650, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:15m:58s remains)
INFO - root - 2017-12-10 00:41:20.499429: step 74660, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 63h:08m:13s remains)
INFO - root - 2017-12-10 00:41:29.276964: step 74670, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:47m:22s remains)
INFO - root - 2017-12-10 00:41:37.941239: step 74680, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 63h:57m:51s remains)
INFO - root - 2017-12-10 00:41:46.718177: step 74690, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 64h:49m:16s remains)
INFO - root - 2017-12-10 00:41:55.297857: step 74700, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 60h:42m:59s remains)
2017-12-10 00:41:56.291453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0026460688 -0.0029129297 -0.0030044729 -0.0032232616 -0.003162395 -0.00256935 -0.0012798011 0.000239057 0.0015038655 0.0021339466 0.0020880636 0.0013086132 0.00088651129 0.00091672293 0.0012893945][-0.0021421192 -0.0024143411 -0.0025571482 -0.0028563282 -0.0027686523 -0.0020056372 -0.00032003457 0.0016034546 0.0033061623 0.0044768676 0.0047513042 0.0040016817 0.0036222732 0.0039327014 0.0045874324][-0.0017127887 -0.0018731438 -0.0020190133 -0.0023477403 -0.0020507963 -0.00083595235 0.0015255928 0.0040782364 0.0064066369 0.0082078734 0.0089657623 0.008404294 0.0081179217 0.0087267486 0.0095898919][-0.0014033911 -0.0014480788 -0.0015767347 -0.0017424993 -0.00096698292 0.0011372601 0.0045977654 0.0081921965 0.011348563 0.013793863 0.015062734 0.014691954 0.014483117 0.014997641 0.015748514][-0.0012826207 -0.0012069794 -0.0012483099 -0.0011284323 0.00039822375 0.0038571383 0.0087269759 0.013739789 0.017786657 0.020754062 0.02235359 0.022298193 0.022167273 0.022226099 0.022517711][-0.001228583 -0.0011950433 -0.001144375 -0.00053459033 0.0019674019 0.0070279287 0.013276251 0.019571869 0.024252392 0.027624635 0.029358184 0.029601315 0.029357078 0.028920149 0.028479235][-0.0010392487 -0.0012824151 -0.0011924044 -0.00014497922 0.00331791 0.0099142864 0.017230144 0.02442975 0.029323289 0.032962371 0.034643643 0.035197627 0.034921635 0.034143083 0.032850787][-0.00087442948 -0.001328656 -0.0011228167 4.8922841e-05 0.0040912712 0.011695853 0.019673036 0.027301295 0.032180872 0.036011349 0.037755653 0.038684592 0.038470518 0.037728228 0.035618298][-0.00076647778 -0.0013167216 -0.0011568619 6.3930871e-05 0.0042009009 0.01194147 0.020150488 0.027896585 0.032829575 0.036939032 0.038884036 0.040167484 0.040091198 0.039552495 0.036910743][-0.00096612191 -0.0013610218 -0.001170655 -0.00023077941 0.0033876195 0.010577367 0.018634358 0.026301138 0.031505682 0.035967045 0.038172919 0.03974947 0.039793793 0.039542727 0.036829166][-0.0013399983 -0.0015621172 -0.0013618818 -0.00067744893 0.0021385017 0.0081341462 0.015500745 0.022751877 0.028282588 0.03314973 0.035764579 0.037467267 0.037504759 0.037454862 0.034949318][-0.0018550699 -0.0018463194 -0.001612434 -0.0012258659 0.00067288917 0.005083492 0.011222023 0.017900106 0.023635866 0.02854033 0.031447742 0.033245381 0.033216879 0.03311893 0.030979527][-0.0025090841 -0.0022981535 -0.0020188082 -0.0017239634 -0.00057861093 0.0022674471 0.0068148645 0.012208525 0.017555198 0.02235269 0.025421867 0.02711926 0.027039835 0.026758211 0.024932196][-0.0029917613 -0.002746949 -0.0025556548 -0.0023414195 -0.0016824829 -3.1480333e-05 0.0029158413 0.0067562591 0.011157498 0.015316438 0.018174138 0.019718926 0.019646654 0.019135004 0.017468877][-0.0031964364 -0.0030804237 -0.0030008028 -0.0028644523 -0.0025191989 -0.0017492353 -0.00012648362 0.0022910123 0.0054472378 0.0085570589 0.010821277 0.012083976 0.012051242 0.01146021 0.010031712]]...]
INFO - root - 2017-12-10 00:42:04.775145: step 74710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:12m:22s remains)
INFO - root - 2017-12-10 00:42:13.458510: step 74720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:54m:06s remains)
INFO - root - 2017-12-10 00:42:22.359751: step 74730, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:43m:44s remains)
INFO - root - 2017-12-10 00:42:30.974638: step 74740, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 61h:58m:19s remains)
INFO - root - 2017-12-10 00:42:39.434750: step 74750, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 64h:58m:09s remains)
INFO - root - 2017-12-10 00:42:48.123446: step 74760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:48m:07s remains)
INFO - root - 2017-12-10 00:42:56.574064: step 74770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:24m:20s remains)
INFO - root - 2017-12-10 00:43:05.117376: step 74780, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:08m:50s remains)
INFO - root - 2017-12-10 00:43:13.942140: step 74790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:34m:56s remains)
INFO - root - 2017-12-10 00:43:22.527912: step 74800, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 63h:00m:33s remains)
2017-12-10 00:43:23.459838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034051645 -0.0034029824 -0.0034018166 -0.003401259 -0.0034014012 -0.0034012115 -0.003401177 -0.0034005526 -0.0033996794 -0.0033990503 -0.0033982636 -0.0033974806 -0.0033973171 -0.0033971162 -0.0033967714][-0.0034050574 -0.0034019912 -0.0034010473 -0.0034005675 -0.0034010657 -0.0034014906 -0.0034016212 -0.0034005276 -0.0033993102 -0.0033983393 -0.0033970929 -0.0033961271 -0.0033957043 -0.0033953793 -0.0033948713][-0.0034074241 -0.0034040282 -0.0034030769 -0.0034033605 -0.003404367 -0.003405028 -0.0034047051 -0.0034031144 -0.0034014783 -0.0033998676 -0.003398397 -0.0033970629 -0.0033962976 -0.0033958273 -0.0033952857][-0.0034101459 -0.0034071449 -0.0034071894 -0.0034085026 -0.0034097952 -0.0034098255 -0.0034087205 -0.003406212 -0.0034035654 -0.0034014538 -0.0033995898 -0.0033981169 -0.0033973095 -0.0033966217 -0.0033957353][-0.0034130472 -0.0034110276 -0.0034112951 -0.0034124022 -0.0034133277 -0.00341229 -0.0034099855 -0.0034069612 -0.0034040529 -0.0034017784 -0.0033999297 -0.003398658 -0.0033980161 -0.0033973001 -0.0033963092][-0.0034152777 -0.0034141878 -0.0034145275 -0.003415359 -0.0034151478 -0.0034127627 -0.0034088907 -0.0034048348 -0.0034018476 -0.0034000196 -0.0033988038 -0.00339822 -0.0033981102 -0.0033976051 -0.0033965774][-0.0034160388 -0.0034158118 -0.0034165641 -0.0034171673 -0.0034158533 -0.0034124467 -0.0034070562 -0.0034015859 -0.0033995982 -0.0033988212 -0.0033978838 -0.0033978187 -0.003397848 -0.0033975344 -0.0033967793][-0.0034144025 -0.0034144616 -0.0034155913 -0.0034164111 -0.0034153291 -0.0034119226 -0.0034064669 -0.003401607 -0.0033997037 -0.0033990419 -0.0033986792 -0.0033984969 -0.0033982089 -0.0033981497 -0.0033975728][-0.0034110995 -0.0034111731 -0.0034129752 -0.0034145608 -0.0034143296 -0.0034120912 -0.0034084721 -0.0034052341 -0.0034026562 -0.0034016517 -0.0034010243 -0.0034001418 -0.0033991695 -0.0033986841 -0.0033980813][-0.0034072371 -0.0034072383 -0.0034094553 -0.0034121079 -0.0034128707 -0.0034121841 -0.0034102779 -0.0034083754 -0.0034063291 -0.003404408 -0.0034030853 -0.0034016187 -0.0033998536 -0.0033987118 -0.0033979034][-0.0034039328 -0.0034036241 -0.0034062285 -0.0034097838 -0.0034116437 -0.0034124244 -0.0034119813 -0.0034107515 -0.0034087487 -0.0034063924 -0.0034046704 -0.0034026054 -0.0034001751 -0.0033984568 -0.0033976901][-0.0034019153 -0.0034013314 -0.0034036173 -0.0034074504 -0.0034102073 -0.003411887 -0.0034123939 -0.0034115554 -0.0034095112 -0.0034073372 -0.0034053014 -0.0034029828 -0.0034005078 -0.0033989076 -0.0033980061][-0.0034006089 -0.0033998077 -0.0034015775 -0.003404791 -0.0034078092 -0.0034102495 -0.0034112134 -0.003410832 -0.0034094013 -0.0034075321 -0.0034054527 -0.0034033188 -0.0034014054 -0.0034000771 -0.0033989882][-0.0033996662 -0.0033986589 -0.0033998908 -0.003402405 -0.0034050883 -0.0034073195 -0.003408581 -0.0034088255 -0.0034078981 -0.0034065249 -0.0034050685 -0.0034034934 -0.0034019838 -0.0034007297 -0.0033995793][-0.003398652 -0.0033975448 -0.0033981313 -0.0033996778 -0.0034015479 -0.0034032483 -0.0034042681 -0.0034045991 -0.0034041435 -0.0034033507 -0.0034025242 -0.0034017628 -0.0034010198 -0.003400258 -0.0033993823]]...]
INFO - root - 2017-12-10 00:43:31.987900: step 74810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:10m:50s remains)
INFO - root - 2017-12-10 00:43:40.594847: step 74820, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 61h:02m:01s remains)
INFO - root - 2017-12-10 00:43:49.309576: step 74830, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:26m:37s remains)
INFO - root - 2017-12-10 00:43:58.041142: step 74840, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 61h:31m:15s remains)
INFO - root - 2017-12-10 00:44:06.716559: step 74850, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 62h:20m:42s remains)
INFO - root - 2017-12-10 00:44:15.359895: step 74860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:16m:45s remains)
INFO - root - 2017-12-10 00:44:24.176435: step 74870, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 64h:07m:22s remains)
INFO - root - 2017-12-10 00:44:32.825337: step 74880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:49m:11s remains)
INFO - root - 2017-12-10 00:44:41.527407: step 74890, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 62h:22m:46s remains)
INFO - root - 2017-12-10 00:44:50.030726: step 74900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:00m:29s remains)
2017-12-10 00:44:50.934161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025196928 -0.0025227303 -0.0025308365 -0.0024467269 -0.0023876764 -0.0023316978 -0.0022641886 -0.0021323194 -0.0019063472 -0.0016690085 -0.0014846211 -0.0014914353 -0.0016373686 -0.0019517902 -0.0023635451][-0.0026265862 -0.002640551 -0.00265031 -0.0025607767 -0.0025088808 -0.0024433581 -0.0023603248 -0.0022302847 -0.0020270371 -0.0018098444 -0.0016497856 -0.0016769517 -0.0018365816 -0.0021442862 -0.0025277638][-0.0026152842 -0.0025977171 -0.0025846832 -0.0024885354 -0.0024351464 -0.0023534289 -0.0022769663 -0.0021709376 -0.0020024213 -0.001831172 -0.0017163765 -0.0017790904 -0.0019680802 -0.0022922216 -0.0026578596][-0.0025382335 -0.0024783509 -0.0024108139 -0.0022800732 -0.0021976735 -0.0020938679 -0.0019943593 -0.0018839696 -0.0017388973 -0.0016268932 -0.0015731364 -0.001680846 -0.0019212468 -0.0022882828 -0.0026700185][-0.0022983747 -0.0021858769 -0.0020579987 -0.001885472 -0.0017507703 -0.0016063486 -0.0014663984 -0.0013393576 -0.0011815145 -0.0011098725 -0.0011234928 -0.0013128554 -0.0016375453 -0.00208082 -0.0025226851][-0.0019052327 -0.0017480862 -0.0015715114 -0.0013559181 -0.0011670906 -0.000972237 -0.00077304034 -0.00061181281 -0.00045301695 -0.00043126033 -0.00051791733 -0.00079487823 -0.0012106206 -0.0017477473 -0.002272252][-0.0014855169 -0.0013140815 -0.0011209107 -0.00088109472 -0.00064993463 -0.00041407975 -0.00016344711 4.7218287e-05 0.00023381086 0.00021563633 4.5276014e-05 -0.00032011978 -0.00081627886 -0.0014240325 -0.0020276662][-0.0010636216 -0.00089001912 -0.00069275545 -0.00044600363 -0.00021419697 1.8106541e-05 0.00027776859 0.00050611165 0.00069364929 0.00065008109 0.00042905239 -9.2249829e-06 -0.00055598561 -0.0011916792 -0.0018346768][-0.00078360084 -0.00061587244 -0.00041958573 -0.00019066688 2.3807865e-05 0.00022944668 0.00045394502 0.0006635508 0.00083377794 0.00077969418 0.000539453 7.8175915e-05 -0.00047867885 -0.0011078559 -0.0017551484][-0.00066507212 -0.00052595534 -0.00035110698 -0.00015194807 3.698701e-05 0.00018940237 0.00035196613 0.00051423511 0.000641003 0.0005762286 0.00034354534 -9.8856864e-05 -0.0006197053 -0.0012023053 -0.0018143365][-0.00069837342 -0.0005925002 -0.00046567316 -0.00032575475 -0.00017735409 -7.2526745e-05 3.0234689e-05 0.00012268522 0.00019617588 0.00013225363 -6.70047e-05 -0.00045833131 -0.00092050945 -0.0014420843 -0.0019943216][-0.00096285972 -0.00089517143 -0.00081797712 -0.00073837256 -0.0006419355 -0.00058625429 -0.00053316983 -0.00049305777 -0.00046813395 -0.00053388951 -0.00069551007 -0.0010074007 -0.0013850089 -0.0018228871 -0.0022852076][-0.0014516988 -0.0014213792 -0.0013911878 -0.0013694062 -0.0013202776 -0.0012948283 -0.0012734935 -0.0012671652 -0.0012677531 -0.0013212969 -0.0014411707 -0.0016681575 -0.0019503332 -0.0022813065 -0.0026278717][-0.0020946218 -0.0020891111 -0.002087323 -0.0020927447 -0.0020765883 -0.002067524 -0.00205745 -0.0020555139 -0.0020585158 -0.0020942497 -0.002172749 -0.0023185397 -0.0025061732 -0.0027272729 -0.0029533277][-0.0026749878 -0.0026775592 -0.0026835115 -0.0026988129 -0.0027004629 -0.0027007924 -0.0026978459 -0.0026965651 -0.0026975765 -0.002713548 -0.0027546603 -0.0028322386 -0.0029374084 -0.0030618638 -0.0031842608]]...]
INFO - root - 2017-12-10 00:44:59.381053: step 74910, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 65h:15m:20s remains)
INFO - root - 2017-12-10 00:45:08.137098: step 74920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:58m:01s remains)
INFO - root - 2017-12-10 00:45:16.746585: step 74930, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:54m:46s remains)
INFO - root - 2017-12-10 00:45:25.262266: step 74940, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 59h:46m:20s remains)
INFO - root - 2017-12-10 00:45:33.701606: step 74950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:40m:48s remains)
INFO - root - 2017-12-10 00:45:42.256919: step 74960, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 60h:59m:35s remains)
INFO - root - 2017-12-10 00:45:50.914185: step 74970, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 64h:38m:58s remains)
INFO - root - 2017-12-10 00:45:59.416747: step 74980, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:36m:34s remains)
INFO - root - 2017-12-10 00:46:08.082973: step 74990, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 62h:27m:51s remains)
INFO - root - 2017-12-10 00:46:16.526864: step 75000, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 58h:22m:37s remains)
2017-12-10 00:46:17.422596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002783118 -0.0016142556 0.00032041338 0.0026082227 0.0049128514 0.0068450123 0.0080061154 0.0081567243 0.0073628854 0.0060568834 0.004353716 0.0024376453 0.0006055769 -0.001001342 -0.0021896614][-0.0018223759 0.00014506909 0.0031127112 0.0065886872 0.010104543 0.013014242 0.014693363 0.014812743 0.013610283 0.011439735 0.0085791787 0.0054150671 0.0024868359 4.0803803e-05 -0.0017303322][0.00063829054 0.0043873582 0.00958343 0.015591988 0.021612622 0.026555281 0.029511904 0.029994497 0.028325366 0.024697306 0.019617856 0.013814729 0.0082014473 0.0034335328 -6.5173721e-05][0.0050597684 0.011818795 0.020774007 0.030940244 0.040938403 0.049052823 0.054112505 0.055421975 0.053371962 0.047895659 0.0397133 0.029868385 0.019796718 0.010830638 0.0039213169][0.011344018 0.022075327 0.036067128 0.051778849 0.066978015 0.079221584 0.087116212 0.089735307 0.087367281 0.079738967 0.067819148 0.05292302 0.036980771 0.022230797 0.010440673][0.018071884 0.032798611 0.051809214 0.073032394 0.093435787 0.10991605 0.12093149 0.125421 0.12348723 0.11451899 0.09942732 0.079612747 0.057517048 0.036311004 0.018837024][0.023066493 0.040319722 0.062534727 0.08724013 0.11091368 0.13018925 0.14353602 0.14982527 0.14899562 0.14017412 0.12388964 0.10125118 0.074870989 0.04873779 0.026636779][0.024234083 0.041571409 0.063885331 0.088717423 0.1126262 0.1323828 0.14662144 0.15417261 0.15486544 0.14764281 0.13249843 0.11006249 0.082778856 0.054965049 0.030904969][0.020912023 0.035733618 0.054964211 0.076540962 0.097614832 0.11545745 0.12896091 0.13694756 0.13918632 0.13446151 0.12227425 0.10286041 0.078241013 0.05253052 0.029861318][0.014639035 0.025426615 0.039608732 0.055765882 0.071926005 0.086040616 0.0973516 0.10479275 0.10800105 0.10574769 0.09724506 0.082557529 0.06316717 0.0425009 0.023984665][0.0074516013 0.014054781 0.022940949 0.033281323 0.043953162 0.053632632 0.06184072 0.067771457 0.071041837 0.070582241 0.065554485 0.055981997 0.042831823 0.028572043 0.015599482][0.0016806319 0.0048994189 0.009437087 0.014911553 0.020786261 0.026329359 0.031303894 0.03522227 0.037732031 0.038100682 0.035675511 0.030480282 0.023051802 0.014867106 0.0073268153][-0.0017769244 -0.00057895319 0.0011947968 0.0034426742 0.0059539545 0.0084089078 0.010739507 0.012736451 0.014177335 0.014649589 0.013774885 0.011572722 0.0082883062 0.004621326 0.0012115014][-0.0031660548 -0.0028918139 -0.002422757 -0.001781342 -0.0010380778 -0.00029025669 0.00046305335 0.0011653055 0.0017342505 0.0019963451 0.0018008577 0.0011603723 0.00015597767 -0.00098339957 -0.0020522608][-0.0033749233 -0.0033553632 -0.0033081998 -0.0032290674 -0.0031241907 -0.0030062473 -0.0028750522 -0.0027402502 -0.0026125023 -0.0025370428 -0.0025508651 -0.002649755 -0.00281261 -0.0029993178 -0.0031801905]]...]
INFO - root - 2017-12-10 00:46:25.929182: step 75010, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 63h:53m:28s remains)
INFO - root - 2017-12-10 00:46:34.340210: step 75020, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 60h:37m:30s remains)
INFO - root - 2017-12-10 00:46:42.877027: step 75030, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:12m:33s remains)
INFO - root - 2017-12-10 00:46:51.484412: step 75040, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:41m:42s remains)
INFO - root - 2017-12-10 00:46:59.721954: step 75050, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 61h:40m:54s remains)
INFO - root - 2017-12-10 00:47:08.339970: step 75060, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 61h:41m:12s remains)
INFO - root - 2017-12-10 00:47:16.926856: step 75070, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:00m:52s remains)
INFO - root - 2017-12-10 00:47:25.530154: step 75080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:20m:55s remains)
INFO - root - 2017-12-10 00:47:34.214162: step 75090, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:35m:50s remains)
INFO - root - 2017-12-10 00:47:42.730445: step 75100, loss = 0.89, batch loss = 0.68 (11.2 examples/sec; 0.715 sec/batch; 51h:05m:54s remains)
2017-12-10 00:47:43.659737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034085885 -0.0034091622 -0.0034083978 -0.0034061626 -0.0034034783 -0.0034009751 -0.003398695 -0.0033937492 -0.0033789033 -0.0033441605 -0.0032891911 -0.0032218995 -0.0031589379 -0.0031078246 -0.003072951][-0.0034038613 -0.0034039316 -0.0034041374 -0.0034038355 -0.0034029852 -0.0034011465 -0.0033976783 -0.0033894097 -0.0033692573 -0.0033291855 -0.0032702077 -0.0032028984 -0.0031429937 -0.0030974578 -0.0030668664][-0.003399984 -0.0033992676 -0.0033996773 -0.0034010548 -0.0034018336 -0.0034008732 -0.0033964722 -0.0033834539 -0.0033552519 -0.0033079232 -0.0032445705 -0.0031778603 -0.0031226126 -0.00308295 -0.0030580813][-0.0033979986 -0.003396915 -0.0033971346 -0.0033985137 -0.0033993192 -0.0033983311 -0.0033909536 -0.0033707286 -0.0033330587 -0.0032761323 -0.0032089953 -0.0031452905 -0.0030972054 -0.0030648429 -0.0030460716][-0.0033993225 -0.0033972547 -0.0033966308 -0.0033971004 -0.003396739 -0.0033929991 -0.0033794027 -0.0033493445 -0.0032997015 -0.0032343268 -0.0031660518 -0.0031083322 -0.0030687628 -0.0030451349 -0.0030337588][-0.0034014019 -0.0034001437 -0.003399573 -0.0033985553 -0.0033954824 -0.0033861331 -0.0033628247 -0.0033200886 -0.0032587706 -0.0031881169 -0.0031220745 -0.0030722974 -0.0030410213 -0.003024932 -0.0030200584][-0.0034023356 -0.0034020238 -0.0034022857 -0.0034013211 -0.0033954154 -0.0033786132 -0.0033438301 -0.0032886544 -0.0032194548 -0.0031488156 -0.0030887872 -0.0030468171 -0.0030219592 -0.0030116332 -0.0030114758][-0.003401581 -0.0034019824 -0.0034030664 -0.0034025088 -0.0033946843 -0.0033722583 -0.0033293269 -0.0032670121 -0.0031968763 -0.003130689 -0.0030768055 -0.0030394371 -0.0030171678 -0.0030090404 -0.0030115275][-0.0034009255 -0.0034011516 -0.0034024753 -0.0034023556 -0.003395187 -0.0033731076 -0.0033311183 -0.0032720095 -0.0032065997 -0.0031440377 -0.0030933868 -0.0030575776 -0.0030354448 -0.0030269492 -0.0030288636][-0.0034010264 -0.0034010424 -0.003402296 -0.003402353 -0.003397041 -0.0033810167 -0.0033492651 -0.0033011783 -0.003244089 -0.0031875228 -0.0031399557 -0.0031049249 -0.0030821746 -0.003072504 -0.0030727761][-0.0034022878 -0.0034018443 -0.0034029773 -0.0034034958 -0.0034011558 -0.0033921597 -0.0033721938 -0.003339204 -0.0032969099 -0.0032509482 -0.0032097518 -0.0031772736 -0.0031548929 -0.0031438645 -0.0031416938][-0.0034031558 -0.0034031647 -0.0034043319 -0.0034052581 -0.0034052643 -0.0034015744 -0.0033919469 -0.0033737067 -0.0033477477 -0.0033165973 -0.0032864045 -0.0032605487 -0.0032418938 -0.003230901 -0.0032265279][-0.0034036632 -0.0034037649 -0.0034047922 -0.0034060332 -0.0034070504 -0.0034063882 -0.0034030348 -0.003395247 -0.0033827412 -0.0033662035 -0.0033483848 -0.0033318535 -0.0033185915 -0.0033094818 -0.0033041642][-0.0034035284 -0.0034032047 -0.0034041491 -0.0034053577 -0.0034066676 -0.0034074786 -0.003407415 -0.003405611 -0.0034016708 -0.0033953532 -0.0033876144 -0.0033791559 -0.0033713451 -0.0033651211 -0.003360247][-0.0034037156 -0.0034027586 -0.0034029041 -0.0034034622 -0.00340445 -0.0034056103 -0.0034069265 -0.0034078222 -0.0034081892 -0.0034071424 -0.0034047859 -0.0034006906 -0.0033962927 -0.0033919869 -0.00338803]]...]
INFO - root - 2017-12-10 00:47:52.062480: step 75110, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 61h:29m:38s remains)
INFO - root - 2017-12-10 00:48:00.575942: step 75120, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:18m:00s remains)
INFO - root - 2017-12-10 00:48:09.071119: step 75130, loss = 0.88, batch loss = 0.67 (9.2 examples/sec; 0.868 sec/batch; 62h:03m:50s remains)
INFO - root - 2017-12-10 00:48:17.701275: step 75140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:02m:04s remains)
INFO - root - 2017-12-10 00:48:26.272907: step 75150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 62h:34m:13s remains)
INFO - root - 2017-12-10 00:48:35.015788: step 75160, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 61h:03m:39s remains)
INFO - root - 2017-12-10 00:48:43.638307: step 75170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:57m:12s remains)
INFO - root - 2017-12-10 00:48:52.358445: step 75180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:43m:13s remains)
INFO - root - 2017-12-10 00:49:01.075004: step 75190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:17m:28s remains)
INFO - root - 2017-12-10 00:49:09.620944: step 75200, loss = 0.89, batch loss = 0.68 (10.7 examples/sec; 0.748 sec/batch; 53h:26m:04s remains)
2017-12-10 00:49:10.536857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003147413 -0.0028177542 -0.0023668217 -0.0018640806 -0.0013255877 -0.00070546661 -4.5348657e-05 0.00027272943 1.4546327e-05 -0.00078466 -0.0017916806 -0.0026464185 -0.0031612776 -0.0033651425 -0.0034042634][-0.0030764381 -0.0026666725 -0.002070562 -0.0012965307 -0.00030255481 0.00090387207 0.0022234439 0.0029859042 0.0026864924 0.0012622091 -0.00062190252 -0.002193687 -0.0030587704 -0.0033473074 -0.0033993393][-0.0029655683 -0.0024185977 -0.0015487666 -0.00030176248 0.0013550448 0.0032963695 0.0051136957 0.006067046 0.005608906 0.0036903017 0.0010960705 -0.0012425906 -0.0026928233 -0.003282445 -0.0033959439][-0.0029561599 -0.0022648373 -0.0010035783 0.00095546781 0.0035849407 0.0064752037 0.008891358 0.009877407 0.0088898018 0.0061877454 0.002718163 -0.00036040647 -0.0023204964 -0.0031733087 -0.0033878824][-0.0030487252 -0.0022714091 -0.00063581718 0.0020728072 0.0056379326 0.0093513234 0.012168586 0.013115652 0.011776983 0.0085100811 0.0043085068 0.00055107684 -0.0019457859 -0.0030669491 -0.003378253][-0.003131995 -0.0023548158 -0.00055880123 0.00256786 0.006722711 0.010899805 0.013863115 0.01465846 0.013059365 0.0095356992 0.0051110005 0.0010743362 -0.0016997512 -0.002993868 -0.0033676934][-0.0031908972 -0.0025008644 -0.00080107665 0.0023012219 0.0065778643 0.010944291 0.013967108 0.014683126 0.012927918 0.0093452157 0.0049796291 0.0010260607 -0.0016892505 -0.0029946046 -0.0033654659][-0.0032295173 -0.0026768404 -0.0012461387 0.0014927422 0.0054452065 0.0096867224 0.012772389 0.013592126 0.011961853 0.0085169263 0.0043317406 0.00057958183 -0.0019218873 -0.003068428 -0.0033750755][-0.0032363534 -0.00286585 -0.0018325018 0.00029011327 0.0035899172 0.0074420255 0.010526494 0.01162261 0.010307916 0.0071388176 0.0032620893 -0.00014831987 -0.0022983323 -0.0031914324 -0.0033891925][-0.0031736083 -0.0029865431 -0.0023640054 -0.00089263334 0.0016780633 0.004982383 0.0079306979 0.0091995839 0.0081713721 0.0053258371 0.0018660224 -0.0010181158 -0.0026951805 -0.0033020731 -0.0033979123][-0.0029975651 -0.0028945345 -0.0025497312 -0.0015910794 0.00029568654 0.0029902703 0.0056110127 0.0068316152 0.00592028 0.0033582456 0.00039589964 -0.0018792078 -0.0030407524 -0.0033722091 -0.0034028068][-0.0029546639 -0.0027864443 -0.0024853493 -0.0017779748 -0.00037535862 0.0017292055 0.0038391424 0.0048003178 0.0039247917 0.0016536259 -0.00080617564 -0.002505779 -0.0032414349 -0.0033929038 -0.0034029803][-0.0029956182 -0.0028100265 -0.0024532354 -0.0017742597 -0.00060793594 0.0010208369 0.0025986687 0.0032145323 0.0023451012 0.00036968361 -0.0016334993 -0.002885439 -0.0033341462 -0.003396515 -0.0034009344][-0.0029506318 -0.0027591346 -0.0023803506 -0.0017088845 -0.00067859283 0.00059170905 0.001697141 0.0019844288 0.00110475 -0.00058161072 -0.0021741283 -0.0030899018 -0.003372994 -0.0033963362 -0.0033986415][-0.0027427224 -0.0024969163 -0.0020736589 -0.0014167395 -0.00052636978 0.00042981212 0.0010827973 0.001022137 0.00011083297 -0.001297988 -0.00253854 -0.0032062412 -0.0033888242 -0.0033966545 -0.0033978166]]...]
INFO - root - 2017-12-10 00:49:18.868414: step 75210, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 54h:48m:33s remains)
INFO - root - 2017-12-10 00:49:27.397866: step 75220, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 60h:22m:54s remains)
INFO - root - 2017-12-10 00:49:35.986838: step 75230, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:20m:11s remains)
INFO - root - 2017-12-10 00:49:44.641167: step 75240, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 60h:25m:56s remains)
INFO - root - 2017-12-10 00:49:53.212778: step 75250, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 63h:25m:09s remains)
INFO - root - 2017-12-10 00:50:01.893913: step 75260, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:40m:21s remains)
INFO - root - 2017-12-10 00:50:10.593053: step 75270, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 60h:51m:19s remains)
INFO - root - 2017-12-10 00:50:19.300270: step 75280, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:35m:46s remains)
INFO - root - 2017-12-10 00:50:27.994368: step 75290, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:51m:59s remains)
INFO - root - 2017-12-10 00:50:36.638498: step 75300, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.738 sec/batch; 52h:44m:30s remains)
2017-12-10 00:50:37.526109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033752038 -0.0033726108 -0.0033726913 -0.0033728424 -0.0033729437 -0.0033730448 -0.003372994 -0.003372947 -0.0033727491 -0.0033726667 -0.0033722941 -0.0033721405 -0.0033718743 -0.0033718902 -0.0033718213][-0.0033720376 -0.0033691556 -0.0033692482 -0.0033695898 -0.0033696713 -0.0033698396 -0.0033698005 -0.0033697404 -0.0033695889 -0.0033694934 -0.0033692131 -0.0033691 -0.003368913 -0.0033688075 -0.0033685013][-0.0033706296 -0.0033676892 -0.0033676792 -0.0033679937 -0.0033681176 -0.0033684711 -0.0033684385 -0.0033684378 -0.0033682939 -0.0033682508 -0.0033679178 -0.0033676676 -0.0033672813 -0.0033669446 -0.003366363][-0.0033704643 -0.0033674396 -0.0033674436 -0.0033677171 -0.0033678731 -0.0033682147 -0.0033683935 -0.0033685549 -0.0033686096 -0.0033686506 -0.0033682745 -0.0033676426 -0.0033669053 -0.0033661956 -0.0033652915][-0.0033708403 -0.0033679111 -0.0033676033 -0.0033675854 -0.0033677919 -0.0033684322 -0.0033691607 -0.0033698506 -0.003370293 -0.0033707558 -0.003370526 -0.0033695835 -0.0033681723 -0.0033665732 -0.0033651926][-0.0033725626 -0.0033697844 -0.0033692545 -0.0033690359 -0.0033695409 -0.0033703675 -0.0033714226 -0.0033727325 -0.0033739225 -0.0033748993 -0.0033748099 -0.0033736576 -0.0033714904 -0.0033690636 -0.0033668384][-0.0033756823 -0.0033732045 -0.0033723004 -0.003371811 -0.0033723821 -0.0033730837 -0.0033743947 -0.0033763754 -0.0033786336 -0.0033804309 -0.0033806721 -0.0033793359 -0.0033765894 -0.0033735228 -0.0033703942][-0.0033802013 -0.0033783705 -0.0033774877 -0.0033764858 -0.0033766439 -0.003377096 -0.0033787175 -0.0033814285 -0.00338462 -0.00338692 -0.003387037 -0.0033852442 -0.0033822362 -0.0033793545 -0.0033760481][-0.0033852721 -0.0033834756 -0.0033823997 -0.0033814274 -0.0033817713 -0.0033825403 -0.0033846358 -0.0033876607 -0.0033909155 -0.0033935737 -0.0033938398 -0.0033919753 -0.0033890191 -0.0033862663 -0.0033832097][-0.0033910132 -0.0033889352 -0.0033878542 -0.0033867757 -0.0033870235 -0.0033880731 -0.0033905527 -0.0033933143 -0.0033962966 -0.0033989947 -0.0033995421 -0.0033979283 -0.0033954203 -0.0033929991 -0.0033902582][-0.0033956678 -0.0033937509 -0.0033932482 -0.0033923849 -0.0033924095 -0.0033933863 -0.0033953909 -0.0033976012 -0.0034000801 -0.0034021232 -0.0034022769 -0.0034005449 -0.0033986745 -0.0033967928 -0.0033947884][-0.003398879 -0.003397451 -0.0033978131 -0.0033975209 -0.003397672 -0.0033983591 -0.0033995593 -0.0034005281 -0.0034016364 -0.0034024431 -0.0034019577 -0.0034001912 -0.0033990494 -0.0033978971 -0.0033967865][-0.0034009074 -0.0033999973 -0.0034011432 -0.0034017274 -0.0034021644 -0.0034027363 -0.0034032057 -0.0034027684 -0.0034019914 -0.0034010515 -0.0033993868 -0.0033972461 -0.0033961381 -0.0033953076 -0.0033951588][-0.0034008848 -0.0033999863 -0.0034016427 -0.0034030785 -0.0034038702 -0.0034042543 -0.0034041491 -0.0034027342 -0.0034006394 -0.0033980776 -0.003394953 -0.0033920191 -0.0033901369 -0.003388942 -0.0033891248][-0.0034004629 -0.0033993179 -0.0034007307 -0.0034019314 -0.0034022673 -0.003401818 -0.0034008678 -0.0033989048 -0.0033959912 -0.0033923236 -0.0033881646 -0.0033842679 -0.0033812721 -0.0033794441 -0.0033794739]]...]
INFO - root - 2017-12-10 00:50:46.030559: step 75310, loss = 0.89, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 52h:37m:05s remains)
INFO - root - 2017-12-10 00:50:54.664251: step 75320, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:59m:39s remains)
INFO - root - 2017-12-10 00:51:03.208353: step 75330, loss = 0.90, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 61h:05m:03s remains)
INFO - root - 2017-12-10 00:51:11.832435: step 75340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:51m:57s remains)
INFO - root - 2017-12-10 00:51:20.361055: step 75350, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 62h:54m:51s remains)
INFO - root - 2017-12-10 00:51:28.996633: step 75360, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 63h:02m:14s remains)
INFO - root - 2017-12-10 00:51:37.744603: step 75370, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 64h:24m:14s remains)
INFO - root - 2017-12-10 00:51:46.390586: step 75380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:14m:51s remains)
INFO - root - 2017-12-10 00:51:54.974457: step 75390, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:52m:18s remains)
INFO - root - 2017-12-10 00:52:03.564305: step 75400, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 51h:47m:18s remains)
2017-12-10 00:52:04.515507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00338704 -0.0033843 -0.0033838362 -0.0033836341 -0.0033833862 -0.0033833049 -0.0033833545 -0.0033834372 -0.0033836791 -0.0033839575 -0.0033839759 -0.0033837059 -0.0033834262 -0.0033832318 -0.0033829706][-0.003387105 -0.0033844013 -0.003384148 -0.0033840949 -0.003383568 -0.0033830865 -0.0033826183 -0.0033825287 -0.0033828113 -0.0033831836 -0.0033832015 -0.0033829024 -0.0033824891 -0.0033819387 -0.0033813203][-0.003389592 -0.0033876274 -0.0033882055 -0.0033884689 -0.00338766 -0.0033861231 -0.0033848234 -0.0033846586 -0.0033848432 -0.00338555 -0.0033860018 -0.00338587 -0.0033850567 -0.0033838258 -0.0033824691][-0.0033911876 -0.0033902805 -0.0033917569 -0.0033926317 -0.0033916531 -0.0033892742 -0.0033875662 -0.0033874949 -0.0033880577 -0.0033894402 -0.0033903248 -0.0033901297 -0.0033887418 -0.0033866419 -0.0033842775][-0.0033929748 -0.0033929208 -0.0033952466 -0.0033963558 -0.0033953369 -0.0033923259 -0.0033901632 -0.0033903297 -0.0033915744 -0.0033940682 -0.0033953323 -0.0033952175 -0.0033933844 -0.0033903166 -0.0033866807][-0.0033954242 -0.0033961476 -0.0033991358 -0.0034003125 -0.0033995733 -0.0033965886 -0.0033946475 -0.0033946338 -0.0033967013 -0.0033997677 -0.0034011414 -0.0034009607 -0.0033985153 -0.0033943905 -0.0033895483][-0.0033972429 -0.0033988499 -0.0034024445 -0.0034039149 -0.0034037957 -0.0034017554 -0.0034010152 -0.0034011547 -0.0034032133 -0.0034057356 -0.0034066602 -0.00340618 -0.0034027572 -0.0033977646 -0.0033920344][-0.0033973118 -0.0033991041 -0.0034029293 -0.003405134 -0.0034062443 -0.0034061507 -0.003406723 -0.0034073272 -0.0034089969 -0.0034103896 -0.0034104455 -0.0034086397 -0.0034043281 -0.0033988906 -0.0033930794][-0.0033955772 -0.0033971197 -0.0034010613 -0.0034043188 -0.0034066576 -0.00340812 -0.0034091196 -0.0034097233 -0.0034108807 -0.0034109035 -0.0034098418 -0.0034068995 -0.0034023453 -0.0033972836 -0.0033921439][-0.0033926961 -0.0033935087 -0.0033972997 -0.0034010857 -0.0034043367 -0.0034064555 -0.003407533 -0.0034080057 -0.0034082381 -0.0034072706 -0.0034052411 -0.0034019644 -0.0033977744 -0.0033934293 -0.003389396][-0.0033891483 -0.0033891466 -0.0033921679 -0.0033958619 -0.0033993658 -0.0034020629 -0.0034033768 -0.003403635 -0.003403197 -0.0034014 -0.0033989698 -0.0033957218 -0.0033921849 -0.0033888384 -0.0033859911][-0.0033862409 -0.0033850567 -0.0033870563 -0.0033899106 -0.0033927828 -0.003395434 -0.0033968659 -0.0033970242 -0.0033961986 -0.003394326 -0.0033921716 -0.0033895506 -0.0033870225 -0.003384745 -0.0033830192][-0.0033847287 -0.0033824276 -0.0033834903 -0.0033851054 -0.0033867075 -0.0033884079 -0.0033894696 -0.0033893764 -0.0033887143 -0.0033875005 -0.0033861918 -0.0033846032 -0.0033832553 -0.0033819932 -0.0033811284][-0.0033841396 -0.0033812486 -0.0033813585 -0.0033817415 -0.0033822295 -0.0033830355 -0.0033836134 -0.0033835336 -0.0033832346 -0.0033827175 -0.0033821533 -0.0033815908 -0.0033810555 -0.003380591 -0.0033802749][-0.0033840097 -0.0033807578 -0.003380405 -0.0033802763 -0.0033803573 -0.0033806416 -0.0033809352 -0.0033810555 -0.003381002 -0.0033808164 -0.0033806313 -0.0033805077 -0.0033803934 -0.0033802916 -0.0033802111]]...]
INFO - root - 2017-12-10 00:52:13.070557: step 75410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:07m:29s remains)
INFO - root - 2017-12-10 00:52:21.578587: step 75420, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:19m:20s remains)
INFO - root - 2017-12-10 00:52:30.213961: step 75430, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 61h:21m:03s remains)
INFO - root - 2017-12-10 00:52:38.727784: step 75440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:17m:25s remains)
INFO - root - 2017-12-10 00:52:47.214830: step 75450, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.708 sec/batch; 50h:31m:11s remains)
INFO - root - 2017-12-10 00:52:55.814319: step 75460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:37m:40s remains)
INFO - root - 2017-12-10 00:53:04.446503: step 75470, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 64h:26m:43s remains)
INFO - root - 2017-12-10 00:53:13.002233: step 75480, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 62h:31m:44s remains)
INFO - root - 2017-12-10 00:53:21.561157: step 75490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:47m:25s remains)
INFO - root - 2017-12-10 00:53:30.039580: step 75500, loss = 0.89, batch loss = 0.68 (10.8 examples/sec; 0.742 sec/batch; 52h:59m:33s remains)
2017-12-10 00:53:30.948275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034110618 -0.0034104832 -0.0034104246 -0.0034096197 -0.0034078159 -0.003405818 -0.0034045232 -0.0034044445 -0.0034052641 -0.0034065561 -0.003407896 -0.0034086627 -0.0034088616 -0.0034086851 -0.0034084953][-0.0034094858 -0.0034084143 -0.0034065463 -0.0034010343 -0.0033920652 -0.00338307 -0.0033781091 -0.00337856 -0.0033833147 -0.0033906638 -0.0033983488 -0.0034039868 -0.0034067223 -0.0034073589 -0.0034071568][-0.0034084206 -0.0034041284 -0.0033925637 -0.0033673863 -0.0033314733 -0.0032980428 -0.0032827503 -0.0032901682 -0.0033147451 -0.0033456695 -0.0033735526 -0.0033927069 -0.0034029249 -0.0034067053 -0.0034073016][-0.0033950161 -0.0033782627 -0.0033340727 -0.0032493111 -0.0031365927 -0.0030334219 -0.0029875557 -0.0030162656 -0.0031033806 -0.0032097967 -0.0033007741 -0.0033608389 -0.0033919062 -0.0034046769 -0.0034078325][-0.003348913 -0.003300986 -0.0031813469 -0.0029692731 -0.0026945993 -0.0024373988 -0.0023055277 -0.0023539625 -0.0025600265 -0.0028347131 -0.0030850032 -0.0032589706 -0.003353606 -0.0033940882 -0.0034059442][-0.0032517712 -0.0031550219 -0.0029195885 -0.0025159428 -0.0019928557 -0.001484893 -0.0011796905 -0.0012014643 -0.0015453605 -0.0020718412 -0.0026026354 -0.00300712 -0.0032474238 -0.0033587231 -0.0033963018][-0.0031390302 -0.0029954724 -0.0026482821 -0.0020551505 -0.0012724292 -0.00047703972 6.8568625e-05 0.00015500467 -0.00026520761 -0.0010328987 -0.0018902356 -0.002601231 -0.0030601111 -0.0032906828 -0.0033769733][-0.0030805538 -0.0029206574 -0.0025310223 -0.0018535025 -0.0009298271 5.535339e-05 0.00080460333 0.0010422387 0.00066254591 -0.00020116125 -0.0012600271 -0.002203943 -0.0028571968 -0.0032094545 -0.0033527727][-0.003128953 -0.0029954039 -0.0026630035 -0.0020671159 -0.0012190482 -0.00026678806 0.00052026077 0.00086142984 0.00061451294 -0.00013961713 -0.0011423028 -0.002087049 -0.0027774335 -0.0031705014 -0.0033391642][-0.003237772 -0.0031560166 -0.0029429011 -0.0025442485 -0.001948014 -0.0012403515 -0.00061061932 -0.00027946569 -0.00038367976 -0.0008914487 -0.0016189155 -0.0023342946 -0.0028792396 -0.0032012034 -0.0033452685][-0.0033399586 -0.0033052696 -0.0032077418 -0.0030142798 -0.0027067575 -0.0023190202 -0.0019472765 -0.0017221379 -0.0017359481 -0.0019867234 -0.0023779194 -0.0027777008 -0.003090166 -0.0032791519 -0.0033660755][-0.0033945085 -0.0033858996 -0.0033566763 -0.0032923867 -0.0031816422 -0.0030325518 -0.0028790757 -0.0027742332 -0.0027605181 -0.0028445884 -0.0029915273 -0.0031484058 -0.0032736547 -0.0033506874 -0.0033872947][-0.0034108586 -0.0034100863 -0.0034062553 -0.0033949739 -0.0033716641 -0.0033362918 -0.0032956356 -0.0032634928 -0.0032526737 -0.0032678507 -0.0033018976 -0.0033401309 -0.0033714208 -0.0033909918 -0.0034008911][-0.0034116679 -0.0034110998 -0.0034109538 -0.0034102714 -0.0034081682 -0.0034044175 -0.0033992415 -0.0033943101 -0.0033915769 -0.0033917804 -0.0033951304 -0.0033990829 -0.0034026508 -0.0034052213 -0.0034069447][-0.0034094532 -0.0034088101 -0.0034087356 -0.0034087584 -0.0034085303 -0.0034080883 -0.003407293 -0.0034066837 -0.0034066339 -0.0034073736 -0.0034083459 -0.0034085691 -0.003408615 -0.003408612 -0.0034084639]]...]
INFO - root - 2017-12-10 00:53:39.617460: step 75510, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 63h:07m:34s remains)
INFO - root - 2017-12-10 00:53:48.202565: step 75520, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 64h:02m:51s remains)
INFO - root - 2017-12-10 00:53:56.957739: step 75530, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 63h:20m:29s remains)
INFO - root - 2017-12-10 00:54:05.527197: step 75540, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:39m:52s remains)
INFO - root - 2017-12-10 00:54:14.328837: step 75550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 62h:00m:06s remains)
INFO - root - 2017-12-10 00:54:22.880380: step 75560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 61h:09m:38s remains)
INFO - root - 2017-12-10 00:54:31.461235: step 75570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:31m:22s remains)
INFO - root - 2017-12-10 00:54:40.069580: step 75580, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:04m:50s remains)
INFO - root - 2017-12-10 00:54:48.727742: step 75590, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 60h:38m:11s remains)
INFO - root - 2017-12-10 00:54:57.115330: step 75600, loss = 0.89, batch loss = 0.68 (11.3 examples/sec; 0.709 sec/batch; 50h:36m:59s remains)
2017-12-10 00:54:57.969676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013310718 -0.0022974424 -0.0029236733 -0.0032050977 -0.0032959948 -0.0033185112 -0.0033300349 -0.003347039 -0.003361444 -0.0033755659 -0.0033847848 -0.0033927164 -0.0033979777 -0.0034007239 -0.0034006215][0.00062808278 -0.0010715029 -0.0023437701 -0.0029631546 -0.0031672833 -0.0032039746 -0.0032135095 -0.0032444813 -0.0032813495 -0.0033234197 -0.0033511538 -0.0033753454 -0.003391071 -0.0033990168 -0.0033978638][0.004003834 0.00101238 -0.0012962446 -0.0024541626 -0.002836443 -0.0028727588 -0.0028337226 -0.002855625 -0.0029216548 -0.0030455315 -0.0031615044 -0.0032692994 -0.0033450683 -0.0033845087 -0.0033897327][0.0086329821 0.0041599823 0.00044763042 -0.0014859823 -0.0021095513 -0.0021064046 -0.0019655975 -0.0019517166 -0.0020829993 -0.0023663011 -0.0026769151 -0.0029802676 -0.003209285 -0.0033363465 -0.0033681118][0.013533941 0.0076073622 0.0026401111 -2.881512e-06 -0.00081735058 -0.0007032943 -0.00034508808 -0.00023232307 -0.00043688924 -0.00099791866 -0.0016975348 -0.0023799357 -0.0029172744 -0.0032279717 -0.0033298132][0.017256444 0.010572705 0.0048549278 0.0017689636 0.00093052979 0.0012330895 0.0018577513 0.0021232248 0.0018180052 0.0009479404 -0.00024559465 -0.001435712 -0.0024342916 -0.0030379184 -0.0032746762][0.017621076 0.011464907 0.0061051343 0.0032913217 0.0027169005 0.0032897808 0.0042583663 0.0047113495 0.0043410286 0.0031603568 0.0014042163 -0.00032950426 -0.0018476885 -0.0027986744 -0.0032189863][0.014313616 0.00965069 0.0056315465 0.0036600174 0.0035961375 0.0044862283 0.005724946 0.0063968012 0.0060782311 0.0047928756 0.0027210575 0.00063226814 -0.0013145527 -0.0025711921 -0.0031782063][0.0085357158 0.0056884708 0.0033180153 0.0024301277 0.0029039148 0.0040254882 0.0054158252 0.0062005762 0.0059946757 0.0048400043 0.0028510524 0.00079444121 -0.0012069575 -0.0025269957 -0.0031846326][0.002929921 0.0013733965 0.00027973973 0.00015943008 0.00090063107 0.0020272976 0.0033216521 0.0041409265 0.0041664848 0.003375198 0.0018239219 0.0001443224 -0.0015468962 -0.0026731696 -0.0032333583][-0.00091993692 -0.0015956007 -0.0019637262 -0.0017777903 -0.0010996696 -0.00020463881 0.00077149249 0.0014326673 0.0015655088 0.001152673 0.00016741524 -0.0010013117 -0.002199861 -0.0029687169 -0.0033081234][-0.0027587081 -0.0030192512 -0.0030615078 -0.0028424463 -0.0023682569 -0.0017473982 -0.0010911487 -0.00062422664 -0.00045839441 -0.00060795131 -0.0011621758 -0.0019102952 -0.0027029456 -0.0031855067 -0.0033621332][-0.0033042971 -0.0033611506 -0.003333078 -0.0032033697 -0.0029454809 -0.0026057281 -0.0022535743 -0.0019915206 -0.0018528913 -0.001852782 -0.0021279384 -0.0025900023 -0.0030706727 -0.0033267178 -0.0033895909][-0.0033957716 -0.0033938505 -0.0033724583 -0.00331183 -0.0031910711 -0.0030390332 -0.0028874404 -0.0027584494 -0.0026670648 -0.0026399302 -0.0027658958 -0.0029993739 -0.0032419928 -0.0033658638 -0.0033928524][-0.0033964892 -0.0033959686 -0.0033913057 -0.0033747412 -0.0033403009 -0.003299573 -0.0032620465 -0.00322648 -0.0031880836 -0.003169006 -0.0032111877 -0.0032882588 -0.0033584442 -0.0033902668 -0.0033957954]]...]
INFO - root - 2017-12-10 00:55:06.548817: step 75610, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:16m:15s remains)
INFO - root - 2017-12-10 00:55:15.147262: step 75620, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 61h:45m:52s remains)
INFO - root - 2017-12-10 00:55:23.838506: step 75630, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 62h:00m:02s remains)
INFO - root - 2017-12-10 00:55:32.316749: step 75640, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:09m:20s remains)
INFO - root - 2017-12-10 00:55:40.868397: step 75650, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:46m:12s remains)
INFO - root - 2017-12-10 00:55:49.196579: step 75660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:03m:39s remains)
INFO - root - 2017-12-10 00:55:57.819840: step 75670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-10 00:56:06.522222: step 75680, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:51m:16s remains)
INFO - root - 2017-12-10 00:56:15.207948: step 75690, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:48m:31s remains)
INFO - root - 2017-12-10 00:56:23.763187: step 75700, loss = 0.89, batch loss = 0.68 (10.9 examples/sec; 0.737 sec/batch; 52h:33m:47s remains)
2017-12-10 00:56:24.650003: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0081941159 0.0066382587 0.0053763371 0.0045498991 0.004352862 0.0042587053 0.0043987567 0.003941468 0.0032355145 0.0020328278 0.000980512 0.00017550238 -0.00037964783 -0.00078244787 -0.0011232737][0.013464563 0.012670614 0.011577222 0.010375854 0.0094449269 0.0085120313 0.0077527771 0.0064818067 0.005276965 0.0037391616 0.0023388418 0.0010573252 0.00012018462 -0.00053259358 -0.000985252][0.017951181 0.018390328 0.018389387 0.017767252 0.016520943 0.014494702 0.012311323 0.009482896 0.0068664467 0.0042472454 0.0022846961 0.00097010122 0.00016478822 -0.00017689099 -0.00036669453][0.023378629 0.025199981 0.026743174 0.027429175 0.026913036 0.024712583 0.021302816 0.016668508 0.011993858 0.0077139065 0.0046643075 0.0031256729 0.0025950156 0.0027831204 0.0029614677][0.030060148 0.033375125 0.036644503 0.039151419 0.040208034 0.039091613 0.035996106 0.030734586 0.024303591 0.018149666 0.013491219 0.011095042 0.010143571 0.01042029 0.010485408][0.041196223 0.045773838 0.050271161 0.0542225 0.056979235 0.057437308 0.055357158 0.050316494 0.042982563 0.035381306 0.028907387 0.025140703 0.023052774 0.022435362 0.021469912][0.057209879 0.062807307 0.067752421 0.072226927 0.075713374 0.077012956 0.075629666 0.070940733 0.063303068 0.054844674 0.046814576 0.041220114 0.036983285 0.034390204 0.031636737][0.073995776 0.080513746 0.085471906 0.09002167 0.093554549 0.094774641 0.093234785 0.088373125 0.080435567 0.071381509 0.062150389 0.054617446 0.047762096 0.042450339 0.037283275][0.0883233 0.094850555 0.099260069 0.10354833 0.10680295 0.10757726 0.10548586 0.10013986 0.091868773 0.082305655 0.072012022 0.062510066 0.052846387 0.044452246 0.036721759][0.09644562 0.10262986 0.10613795 0.10991516 0.11287943 0.11339346 0.11092116 0.10530131 0.097107455 0.087494068 0.076538742 0.065285131 0.052922741 0.041466329 0.031272694][0.099519432 0.10453206 0.10660715 0.10974219 0.11252515 0.11325895 0.11094277 0.1056253 0.097900115 0.088488191 0.077046111 0.064140551 0.0494575 0.035508249 0.023606228][0.099640012 0.1031278 0.10334797 0.10530623 0.10745221 0.10818298 0.10595421 0.10089005 0.093518063 0.084208466 0.072366193 0.058323737 0.042440724 0.027525479 0.015522219][0.098018236 0.099991873 0.098209761 0.09850204 0.099357672 0.099447541 0.096766591 0.0915029 0.0840243 0.074525036 0.062543206 0.048335414 0.032848097 0.018881796 0.008478282][0.093692362 0.0945506 0.0910038 0.089639626 0.088878833 0.087627612 0.083804458 0.0777176 0.069600567 0.059823766 0.048271663 0.035248362 0.021981742 0.010779518 0.0031786503][0.08511427 0.085150264 0.080497742 0.077808425 0.075510256 0.072656736 0.06756369 0.060461745 0.051814497 0.042302091 0.032142375 0.021679122 0.011959024 0.0043927394 -0.00025159423]]...]
INFO - root - 2017-12-10 00:56:33.256877: step 75710, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 59h:14m:23s remains)
INFO - root - 2017-12-10 00:56:41.738314: step 75720, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:02m:05s remains)
INFO - root - 2017-12-10 00:56:50.361002: step 75730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:58m:51s remains)
INFO - root - 2017-12-10 00:56:59.039215: step 75740, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 61h:23m:23s remains)
INFO - root - 2017-12-10 00:57:07.579568: step 75750, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 59h:38m:42s remains)
INFO - root - 2017-12-10 00:57:16.012456: step 75760, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:08m:41s remains)
INFO - root - 2017-12-10 00:57:24.761704: step 75770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 59h:52m:42s remains)
INFO - root - 2017-12-10 00:57:33.390491: step 75780, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:15m:41s remains)
INFO - root - 2017-12-10 00:57:41.985331: step 75790, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:49m:30s remains)
INFO - root - 2017-12-10 00:57:50.507225: step 75800, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.733 sec/batch; 52h:14m:21s remains)
2017-12-10 00:57:51.404388: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46345234 0.45537689 0.44308496 0.43056 0.41781285 0.40372139 0.39115995 0.3831031 0.37629613 0.36916739 0.36390477 0.36111435 0.3568466 0.34741005 0.33660084][0.46894598 0.46495745 0.45751807 0.44858617 0.43991143 0.42950282 0.4196347 0.41348156 0.40772977 0.4029704 0.39876449 0.39610094 0.39171842 0.38190421 0.3701463][0.4535394 0.45331615 0.45202741 0.44942757 0.44649982 0.44255915 0.43811187 0.43617955 0.43319184 0.43003842 0.42727068 0.42526788 0.42116046 0.41103059 0.39759][0.42786548 0.42988953 0.4336336 0.43805584 0.444258 0.44820866 0.45078367 0.45431808 0.45512602 0.45382953 0.45125318 0.44968823 0.44514751 0.43455851 0.41966787][0.39996627 0.40323558 0.41032854 0.42086548 0.43497667 0.44832477 0.45882002 0.46923119 0.47419125 0.47505295 0.47271803 0.46921954 0.462338 0.44968009 0.43239617][0.37603238 0.38343456 0.39463353 0.41121188 0.43244284 0.45375592 0.47172865 0.4866437 0.4946408 0.49621651 0.49292254 0.48618254 0.47550434 0.45949322 0.43859863][0.36737582 0.37735611 0.39141706 0.41210216 0.43805718 0.464571 0.48724481 0.50518161 0.51345611 0.51417387 0.50834262 0.49784428 0.48323408 0.46372035 0.43990052][0.37432989 0.38869867 0.40481642 0.4277772 0.45572251 0.48352134 0.50678664 0.52395207 0.53058428 0.52858347 0.51947004 0.5054481 0.48715606 0.46471193 0.43835682][0.39120671 0.40935373 0.42734125 0.45167097 0.48025894 0.50685078 0.52755094 0.54132456 0.54441655 0.5392096 0.52682221 0.51009393 0.48946896 0.46521503 0.43782884][0.41394174 0.43404889 0.45148152 0.4748272 0.50082827 0.52452987 0.54216486 0.55237693 0.55270976 0.54510444 0.5313465 0.51317179 0.49132895 0.46650791 0.43878004][0.43733871 0.45882049 0.47420034 0.49395373 0.51520282 0.533143 0.54585016 0.55177867 0.54967666 0.54099387 0.52748162 0.51039058 0.48959404 0.46590963 0.4393279][0.45389384 0.47529203 0.48816958 0.50340784 0.51931864 0.53167057 0.53975058 0.54247558 0.5389179 0.53060579 0.51833111 0.503018 0.48423198 0.46261242 0.43804806][0.45835626 0.47904742 0.48972809 0.501492 0.51290649 0.52061683 0.52475548 0.52505159 0.52121329 0.51422089 0.50419259 0.49152032 0.47543696 0.45637581 0.43399215][0.44748288 0.46647426 0.47492144 0.4843657 0.49316022 0.49854249 0.501257 0.50101912 0.49833155 0.49313873 0.48554161 0.47564349 0.4623974 0.44593537 0.42565075][0.42589319 0.44220698 0.44767931 0.45388937 0.45971149 0.46356255 0.46601436 0.46638685 0.46517223 0.46196496 0.45678779 0.44942173 0.43869311 0.42478257 0.40695125]]...]
INFO - root - 2017-12-10 00:58:00.090313: step 75810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:57m:35s remains)
INFO - root - 2017-12-10 00:58:08.563046: step 75820, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 58h:49m:22s remains)
INFO - root - 2017-12-10 00:58:17.216520: step 75830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:58m:31s remains)
INFO - root - 2017-12-10 00:58:25.723634: step 75840, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:56m:28s remains)
INFO - root - 2017-12-10 00:58:34.208499: step 75850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:02m:17s remains)
INFO - root - 2017-12-10 00:58:42.702155: step 75860, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:21m:07s remains)
INFO - root - 2017-12-10 00:58:51.382344: step 75870, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 62h:34m:35s remains)
INFO - root - 2017-12-10 00:58:59.905830: step 75880, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:48m:16s remains)
INFO - root - 2017-12-10 00:59:08.531263: step 75890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:46m:54s remains)
INFO - root - 2017-12-10 00:59:17.162193: step 75900, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.707 sec/batch; 50h:23m:09s remains)
2017-12-10 00:59:18.131293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033543564 -0.0033511699 -0.0033512427 -0.0033518064 -0.0033531622 -0.0033552404 -0.0033586735 -0.0033627308 -0.0033664366 -0.0033695844 -0.00337159 -0.0033712848 -0.0033685726 -0.0033646603 -0.0033605807][-0.0033522225 -0.0033485189 -0.0033483771 -0.0033491415 -0.0033510167 -0.0033540623 -0.003359159 -0.0033652901 -0.0033709607 -0.0033757023 -0.003378975 -0.0033793268 -0.0033764036 -0.0033715796 -0.0033661695][-0.0033522486 -0.0033484693 -0.0033480974 -0.0033489303 -0.0033509012 -0.0033545506 -0.0033607509 -0.0033685633 -0.0033760562 -0.0033824793 -0.0033871988 -0.0033885371 -0.0033860183 -0.0033807475 -0.0033744075][-0.0033524081 -0.0033485126 -0.0033477081 -0.0033484129 -0.0033503347 -0.0033544258 -0.0033613557 -0.0033703882 -0.0033791212 -0.0033870251 -0.0033931495 -0.0033956685 -0.003394142 -0.0033892812 -0.0033826607][-0.0033524497 -0.0033485317 -0.0033473619 -0.0033477652 -0.0033492344 -0.0033528423 -0.0033593306 -0.0033682357 -0.0033773954 -0.003385945 -0.0033927993 -0.0033966177 -0.0033966615 -0.0033930796 -0.0033870346][-0.0033525671 -0.0033485354 -0.0033472551 -0.0033472639 -0.0033479941 -0.0033505454 -0.0033556046 -0.0033630766 -0.0033713677 -0.0033797005 -0.0033868346 -0.0033915266 -0.0033927949 -0.0033906759 -0.0033859424][-0.0033526304 -0.0033485654 -0.0033474003 -0.0033471256 -0.0033471668 -0.0033484881 -0.0033516476 -0.0033569257 -0.0033633551 -0.0033703803 -0.003376951 -0.0033819503 -0.0033843652 -0.0033836872 -0.0033805242][-0.003352724 -0.0033486774 -0.0033476797 -0.0033472583 -0.0033469202 -0.003347283 -0.0033487838 -0.0033517831 -0.0033560453 -0.0033615183 -0.0033670557 -0.0033715728 -0.0033744203 -0.0033748243 -0.0033730217][-0.0033528488 -0.0033489466 -0.0033482772 -0.0033478127 -0.0033473012 -0.0033471475 -0.0033474704 -0.0033485484 -0.0033505457 -0.0033538486 -0.0033577825 -0.003361393 -0.0033639083 -0.003364858 -0.0033642012][-0.0033527396 -0.0033491531 -0.0033490928 -0.0033487533 -0.003348252 -0.0033478867 -0.003347649 -0.0033476704 -0.0033480991 -0.0033493154 -0.0033513352 -0.0033534807 -0.0033551466 -0.0033560689 -0.0033559818][-0.0033526625 -0.0033491489 -0.0033497005 -0.0033496 -0.0033492809 -0.0033489387 -0.0033486639 -0.0033483463 -0.0033481063 -0.0033481652 -0.003348704 -0.0033493992 -0.0033500041 -0.0033505477 -0.0033508097][-0.0033526593 -0.0033493389 -0.0033501126 -0.0033500756 -0.0033497592 -0.0033495072 -0.0033493512 -0.0033490576 -0.0033488008 -0.0033485745 -0.0033484886 -0.0033484332 -0.0033484474 -0.0033487116 -0.0033489568][-0.0033529701 -0.0033495419 -0.0033502902 -0.0033502849 -0.0033500763 -0.0033499377 -0.0033498651 -0.00334966 -0.0033495363 -0.0033493994 -0.0033492288 -0.0033489966 -0.0033489398 -0.0033491596 -0.0033493529][-0.0033536437 -0.0033500418 -0.0033506167 -0.0033505813 -0.0033503887 -0.0033503082 -0.0033502858 -0.0033501666 -0.0033501729 -0.003350219 -0.0033501443 -0.0033499543 -0.0033499117 -0.0033501447 -0.0033503117][-0.0033544602 -0.0033506944 -0.0033510698 -0.0033509037 -0.0033506553 -0.003350514 -0.0033505023 -0.0033504642 -0.0033506197 -0.003350833 -0.0033508954 -0.0033508181 -0.0033508039 -0.0033509582 -0.0033510516]]...]
INFO - root - 2017-12-10 00:59:26.637781: step 75910, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 60h:09m:27s remains)
INFO - root - 2017-12-10 00:59:35.092382: step 75920, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:25m:58s remains)
INFO - root - 2017-12-10 00:59:43.703179: step 75930, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:17m:48s remains)
INFO - root - 2017-12-10 00:59:52.445000: step 75940, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:25m:34s remains)
INFO - root - 2017-12-10 01:00:01.152844: step 75950, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 62h:23m:18s remains)
INFO - root - 2017-12-10 01:00:09.724638: step 75960, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 61h:07m:30s remains)
INFO - root - 2017-12-10 01:00:18.449119: step 75970, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:04m:01s remains)
INFO - root - 2017-12-10 01:00:27.104064: step 75980, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 59h:06m:31s remains)
INFO - root - 2017-12-10 01:00:35.845386: step 75990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:48m:43s remains)
INFO - root - 2017-12-10 01:00:44.331156: step 76000, loss = 0.89, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 53h:54m:28s remains)
2017-12-10 01:00:45.213273: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21186237 0.20663455 0.198726 0.18930335 0.17941725 0.16995102 0.16090961 0.15196322 0.14313531 0.13446847 0.12621245 0.11768205 0.10841965 0.098380886 0.087995879][0.25726631 0.25814524 0.25395879 0.24581605 0.23500033 0.22313592 0.21098635 0.19865762 0.18649349 0.17482942 0.16339855 0.1509168 0.13695513 0.12190562 0.10618632][0.30979234 0.32361072 0.32935506 0.32692096 0.31806865 0.30509129 0.2892254 0.27158719 0.25303388 0.23434065 0.21537633 0.19535062 0.17386824 0.15152295 0.12892903][0.35971653 0.38987225 0.40929151 0.41679904 0.41357648 0.401901 0.38396198 0.36152744 0.33622456 0.30954334 0.28134188 0.25158215 0.2202134 0.18809912 0.15646708][0.39821923 0.44385061 0.47619525 0.49433684 0.49851674 0.49064741 0.47287557 0.44768211 0.41700095 0.38303211 0.34592408 0.30674079 0.26570323 0.22421822 0.18378884][0.41862634 0.47633851 0.51883614 0.54481477 0.55511051 0.55152655 0.535653 0.50917816 0.47483572 0.43540558 0.39145917 0.34504342 0.29707643 0.24942103 0.20337874][0.42019257 0.48326617 0.53006142 0.56002885 0.57370186 0.57254577 0.5580616 0.53180146 0.49613336 0.45394811 0.40664479 0.35727307 0.30687225 0.2575241 0.21016034][0.40758997 0.47034925 0.51623684 0.54549664 0.55918 0.55847454 0.54407161 0.51705587 0.48045519 0.43770629 0.39055 0.3420274 0.29366279 0.24715209 0.20292962][0.38226971 0.44085768 0.48268694 0.508224 0.51900768 0.51660323 0.5014376 0.4739688 0.43729505 0.39561003 0.3512176 0.30701685 0.26393571 0.22338967 0.18532676][0.34610742 0.39851415 0.43446308 0.4546901 0.46144977 0.45631477 0.43973914 0.41222021 0.37704626 0.33871689 0.29951146 0.26164082 0.22590764 0.19303569 0.16273275][0.30106848 0.34561521 0.3747696 0.38931596 0.39149645 0.3836008 0.36646247 0.34031487 0.30862322 0.27581629 0.24395177 0.2141552 0.18663064 0.16175202 0.13922106][0.25178403 0.28728369 0.30918884 0.3181771 0.31651 0.30670202 0.29000145 0.26708025 0.24081057 0.21482861 0.1909786 0.16976035 0.15077631 0.13368137 0.11799504][0.20260541 0.22896673 0.24402973 0.24840799 0.24402753 0.23336931 0.21798915 0.19893795 0.17872055 0.15985435 0.14387925 0.13087803 0.11991704 0.10989153 0.10014879][0.15666953 0.17439711 0.18339126 0.1846523 0.1792988 0.16925023 0.15630819 0.14192082 0.12802018 0.11579201 0.10648938 0.10024756 0.09578798 0.091300264 0.085974045][0.11851575 0.12933275 0.1336378 0.13272461 0.12731449 0.11893227 0.10900069 0.099106349 0.090739347 0.0842052 0.080347545 0.078924268 0.0788108 0.078019515 0.075550884]]...]
INFO - root - 2017-12-10 01:00:53.998231: step 76010, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:36m:26s remains)
INFO - root - 2017-12-10 01:01:02.455769: step 76020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:19m:33s remains)
INFO - root - 2017-12-10 01:01:11.216628: step 76030, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 64h:48m:13s remains)
INFO - root - 2017-12-10 01:01:19.813590: step 76040, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 59h:36m:47s remains)
INFO - root - 2017-12-10 01:01:28.566670: step 76050, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 61h:25m:06s remains)
INFO - root - 2017-12-10 01:01:37.188654: step 76060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:42m:07s remains)
INFO - root - 2017-12-10 01:01:45.892723: step 76070, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:34m:18s remains)
INFO - root - 2017-12-10 01:01:54.775537: step 76080, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:30m:05s remains)
INFO - root - 2017-12-10 01:02:03.605954: step 76090, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:57m:36s remains)
INFO - root - 2017-12-10 01:02:12.135911: step 76100, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.717 sec/batch; 51h:05m:31s remains)
2017-12-10 01:02:13.197376: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33579779 0.3266167 0.32455093 0.3308951 0.3390398 0.35035655 0.3617287 0.3770234 0.38842902 0.39719111 0.40138569 0.40035829 0.39477879 0.38204607 0.36673364][0.40772545 0.39670235 0.39096627 0.39380044 0.39756507 0.40450269 0.41101575 0.42070428 0.42612696 0.42970964 0.429537 0.42503887 0.41618225 0.4006477 0.38278824][0.47336078 0.46120396 0.451313 0.4486151 0.44591829 0.44779909 0.44918242 0.45405555 0.45485684 0.45222455 0.446104 0.43575594 0.42113003 0.40110716 0.37961781][0.52407891 0.51133806 0.49864745 0.49140617 0.48457098 0.48248896 0.47947952 0.48094404 0.47781062 0.47025624 0.45789745 0.44049084 0.41861367 0.39201173 0.36574265][0.55905914 0.54731131 0.53337806 0.52259517 0.51381773 0.51065433 0.50605392 0.50484085 0.49738395 0.48425293 0.46428114 0.43833804 0.40731665 0.37313029 0.34072][0.58006805 0.57071054 0.5557155 0.54401338 0.53544438 0.53186423 0.52802277 0.52565879 0.51565439 0.4974618 0.46964961 0.43314689 0.39055339 0.34632093 0.30562747][0.5888226 0.58454496 0.57047808 0.55756533 0.54905075 0.54554683 0.5429849 0.54015595 0.52888787 0.5068686 0.47215894 0.42637396 0.3721076 0.31676814 0.26653808][0.58331549 0.58598125 0.57614893 0.56601018 0.56062531 0.55790144 0.55611557 0.55350006 0.54092193 0.51500946 0.47365847 0.41926661 0.3552044 0.28991014 0.23109992][0.57173604 0.58145392 0.57642466 0.57040453 0.56952763 0.56898105 0.56856287 0.56492108 0.55045414 0.52203918 0.47648689 0.41692889 0.34687847 0.27568695 0.21173681][0.55740291 0.57330579 0.571555 0.569169 0.57093573 0.57173073 0.57361221 0.57001179 0.55516374 0.52585834 0.47978672 0.41974878 0.34876072 0.27735946 0.21302956][0.53707844 0.55724043 0.55775714 0.55845004 0.56180745 0.56345659 0.5659858 0.56282187 0.55066329 0.52431256 0.48252153 0.42719975 0.36128536 0.29477647 0.23405977][0.51152539 0.5332616 0.53554392 0.5380891 0.5423283 0.54465204 0.54734308 0.54539996 0.53602606 0.51480573 0.48080236 0.43482724 0.37915185 0.32181841 0.26865527][0.47877285 0.49976003 0.50295788 0.50705218 0.51272792 0.51671284 0.52087444 0.5210402 0.515171 0.49976993 0.47423145 0.43918663 0.3957462 0.35045263 0.30787206][0.44586098 0.46338877 0.4651053 0.46891254 0.47471642 0.48031214 0.48622277 0.48894814 0.48698327 0.47832996 0.46216974 0.43836164 0.40775061 0.37539542 0.34456986][0.41190305 0.4264729 0.42651498 0.42952451 0.4348706 0.44114453 0.44790986 0.45242196 0.45386335 0.45038885 0.44183412 0.42797053 0.40900165 0.38866255 0.36871207]]...]
INFO - root - 2017-12-10 01:02:21.787748: step 76110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:53m:06s remains)
INFO - root - 2017-12-10 01:02:30.318690: step 76120, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 64h:12m:57s remains)
INFO - root - 2017-12-10 01:02:39.186428: step 76130, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:58m:00s remains)
INFO - root - 2017-12-10 01:02:47.855119: step 76140, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:00m:40s remains)
INFO - root - 2017-12-10 01:02:56.633953: step 76150, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 61h:29m:34s remains)
INFO - root - 2017-12-10 01:03:05.124059: step 76160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:55m:53s remains)
INFO - root - 2017-12-10 01:03:13.842274: step 76170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 62h:15m:21s remains)
INFO - root - 2017-12-10 01:03:22.503170: step 76180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:22m:01s remains)
INFO - root - 2017-12-10 01:03:31.191643: step 76190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:07m:05s remains)
INFO - root - 2017-12-10 01:03:39.789526: step 76200, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.717 sec/batch; 51h:04m:20s remains)
2017-12-10 01:03:40.627405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033686711 -0.0033664915 -0.0033670084 -0.0033689989 -0.003370421 -0.0033717859 -0.0033726019 -0.003373049 -0.0033739028 -0.0033753195 -0.0033761617 -0.003376819 -0.003377876 -0.003379239 -0.0033801643][-0.0033753677 -0.0033730145 -0.0033728904 -0.0033737116 -0.0033743298 -0.0033747992 -0.0033749461 -0.0033744108 -0.0033745677 -0.003375259 -0.0033756215 -0.0033753724 -0.0033755882 -0.0033758704 -0.0033757871][-0.0033806062 -0.0033777743 -0.0033763698 -0.0033757896 -0.0033749703 -0.0033745554 -0.003373751 -0.0033726236 -0.0033725686 -0.0033732227 -0.0033737191 -0.0033734469 -0.0033739705 -0.0033739447 -0.0033732636][-0.0033812113 -0.0033776709 -0.0033752362 -0.0033736075 -0.0033718557 -0.0033704143 -0.0033688869 -0.0033674317 -0.0033673667 -0.0033676722 -0.0033678545 -0.0033673474 -0.0033676967 -0.0033677684 -0.00336706][-0.0033769934 -0.0033732061 -0.0033703244 -0.0033679623 -0.0033656531 -0.003363851 -0.0033619385 -0.0033600922 -0.0033600163 -0.0033604365 -0.0033607807 -0.0033603236 -0.0033611711 -0.0033617602 -0.0033617362][-0.0033696773 -0.0033655835 -0.0033627746 -0.0033599297 -0.0033570223 -0.0033547396 -0.0033527736 -0.0033510644 -0.0033516528 -0.0033527543 -0.0033537294 -0.003354361 -0.0033564467 -0.0033579047 -0.0033589916][-0.0033610689 -0.0033570181 -0.0033551592 -0.0033533163 -0.0033507715 -0.0033490963 -0.0033479917 -0.0033472697 -0.0033483065 -0.0033497752 -0.0033510351 -0.0033522989 -0.0033548626 -0.003357026 -0.0033589355][-0.0033552132 -0.0033513727 -0.0033505987 -0.0033496611 -0.0033479703 -0.0033469412 -0.0033463775 -0.0033461738 -0.0033478469 -0.0033500823 -0.0033515461 -0.0033534488 -0.0033561247 -0.0033581932 -0.0033601394][-0.0033533459 -0.003349764 -0.0033498656 -0.003349687 -0.0033489338 -0.00334866 -0.0033482914 -0.0033481959 -0.0033496772 -0.0033516849 -0.0033527331 -0.0033539266 -0.0033556167 -0.003357125 -0.0033590752][-0.0033520847 -0.0033487459 -0.0033491848 -0.0033492481 -0.0033490837 -0.0033493224 -0.0033491731 -0.0033492025 -0.0033497144 -0.0033508537 -0.0033517077 -0.0033528446 -0.0033540505 -0.0033554235 -0.0033573427][-0.0033514285 -0.0033480977 -0.0033486332 -0.0033489517 -0.0033489638 -0.0033498027 -0.0033501091 -0.0033504949 -0.0033504672 -0.0033507715 -0.0033507906 -0.0033513922 -0.003351901 -0.0033530847 -0.0033548609][-0.0033507359 -0.003347293 -0.003347792 -0.0033484262 -0.0033487419 -0.0033495014 -0.0033498271 -0.0033502476 -0.0033501696 -0.0033503834 -0.0033501561 -0.003350311 -0.0033505396 -0.0033516169 -0.0033531541][-0.0033516737 -0.003348401 -0.0033483363 -0.0033485866 -0.0033488858 -0.0033497072 -0.003349693 -0.0033500001 -0.0033496276 -0.0033493638 -0.00334885 -0.00334864 -0.0033486758 -0.0033498053 -0.003352032][-0.0033517787 -0.0033480627 -0.0033472874 -0.0033472755 -0.003347245 -0.0033479945 -0.0033482264 -0.003348934 -0.0033486669 -0.0033480502 -0.003347507 -0.0033476115 -0.0033481014 -0.003349836 -0.0033527107][-0.0033521676 -0.0033478544 -0.0033464702 -0.0033463421 -0.0033461063 -0.0033465717 -0.0033468304 -0.003347496 -0.0033470704 -0.0033465107 -0.0033462469 -0.0033469428 -0.0033482753 -0.0033508022 -0.0033541843]]...]
INFO - root - 2017-12-10 01:03:49.131013: step 76210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:22m:52s remains)
INFO - root - 2017-12-10 01:03:57.699535: step 76220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:56m:16s remains)
INFO - root - 2017-12-10 01:04:06.363503: step 76230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:04m:46s remains)
INFO - root - 2017-12-10 01:04:14.997793: step 76240, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.854 sec/batch; 60h:46m:31s remains)
INFO - root - 2017-12-10 01:04:23.668174: step 76250, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 61h:06m:22s remains)
INFO - root - 2017-12-10 01:04:32.044003: step 76260, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 61h:19m:04s remains)
INFO - root - 2017-12-10 01:04:40.590123: step 76270, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:56m:35s remains)
INFO - root - 2017-12-10 01:04:49.225802: step 76280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:20m:51s remains)
INFO - root - 2017-12-10 01:04:57.772872: step 76290, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 59h:53m:10s remains)
INFO - root - 2017-12-10 01:05:06.376606: step 76300, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 61h:24m:52s remains)
2017-12-10 01:05:07.351672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033631192 -0.0033601257 -0.0033599755 -0.003359885 -0.0033597997 -0.003359823 -0.0033598756 -0.0033598712 -0.0033600517 -0.0033607734 -0.0033624968 -0.00336539 -0.0033694087 -0.0033738636 -0.0033786227][-0.0033610705 -0.0033578661 -0.0033578505 -0.0033580158 -0.0033581241 -0.0033583443 -0.0033585038 -0.0033584747 -0.0033585466 -0.0033592815 -0.0033613341 -0.0033647644 -0.0033695723 -0.0033753668 -0.0033816774][-0.0033616868 -0.0033586631 -0.0033591024 -0.0033598416 -0.0033605564 -0.0033612007 -0.0033614954 -0.0033613865 -0.0033611848 -0.0033615164 -0.0033630459 -0.0033661947 -0.0033710767 -0.00337749 -0.0033847995][-0.0033623218 -0.0033596682 -0.0033607346 -0.0033622789 -0.0033638887 -0.0033653746 -0.0033662796 -0.0033662131 -0.0033656263 -0.0033653048 -0.0033657413 -0.0033676482 -0.0033715433 -0.0033773021 -0.003384205][-0.0033628945 -0.0033607662 -0.0033628538 -0.0033656489 -0.0033688045 -0.0033717286 -0.0033735302 -0.0033737451 -0.0033724729 -0.0033708981 -0.0033697491 -0.0033696196 -0.0033718224 -0.0033758476 -0.0033809948][-0.0033636491 -0.0033621828 -0.0033653448 -0.0033696145 -0.0033743903 -0.0033787962 -0.0033816081 -0.003382151 -0.0033802651 -0.0033772956 -0.0033743479 -0.0033719996 -0.0033716448 -0.0033731514 -0.003375917][-0.0033646526 -0.0033636938 -0.0033675001 -0.0033728867 -0.003379235 -0.0033850507 -0.0033888456 -0.0033895422 -0.00338719 -0.003382931 -0.003378117 -0.0033736087 -0.0033705207 -0.003369696 -0.0033704028][-0.0033668235 -0.0033655004 -0.0033690538 -0.0033745833 -0.0033818723 -0.0033887615 -0.003393223 -0.0033943527 -0.0033919958 -0.0033867902 -0.0033800772 -0.0033738257 -0.0033691309 -0.0033663036 -0.0033651069][-0.0033701377 -0.0033677504 -0.0033700701 -0.0033746809 -0.0033814518 -0.0033883669 -0.0033933092 -0.0033952717 -0.0033931464 -0.0033877054 -0.0033801682 -0.0033730012 -0.0033673723 -0.0033634377 -0.003360962][-0.0033746408 -0.0033707735 -0.0033712878 -0.0033741319 -0.0033789054 -0.0033843685 -0.0033889241 -0.003391342 -0.0033899383 -0.0033850088 -0.0033780804 -0.0033712403 -0.003365638 -0.0033614207 -0.0033584519][-0.0033796695 -0.0033742352 -0.0033723689 -0.0033726115 -0.0033748893 -0.0033783154 -0.0033814495 -0.0033834539 -0.0033826998 -0.0033792602 -0.0033739686 -0.0033684457 -0.0033637951 -0.0033601453 -0.0033575378][-0.00338383 -0.0033767547 -0.0033725458 -0.0033702608 -0.003370241 -0.0033717519 -0.003373615 -0.0033747482 -0.0033742704 -0.0033721551 -0.0033688161 -0.0033649241 -0.0033614244 -0.0033588535 -0.0033570472][-0.0033861273 -0.0033776897 -0.0033719721 -0.0033679202 -0.0033659679 -0.0033658887 -0.0033665923 -0.0033672568 -0.0033671223 -0.0033659558 -0.003363851 -0.0033613278 -0.0033590989 -0.0033575476 -0.0033564456][-0.0033857028 -0.0033767 -0.003370465 -0.0033658207 -0.003362867 -0.0033618105 -0.0033620303 -0.0033625746 -0.0033628147 -0.0033623329 -0.0033610871 -0.0033595439 -0.0033580961 -0.0033569895 -0.0033562332][-0.0033830772 -0.0033747011 -0.0033689695 -0.0033646806 -0.003361827 -0.0033606589 -0.0033609723 -0.0033618973 -0.0033626852 -0.0033628084 -0.003362084 -0.0033609811 -0.00335951 -0.0033579876 -0.003356833]]...]
INFO - root - 2017-12-10 01:05:15.792431: step 76310, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 61h:49m:56s remains)
INFO - root - 2017-12-10 01:05:24.257981: step 76320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:04m:19s remains)
INFO - root - 2017-12-10 01:05:32.947275: step 76330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:25m:17s remains)
INFO - root - 2017-12-10 01:05:41.496045: step 76340, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 59h:11m:24s remains)
INFO - root - 2017-12-10 01:05:49.964666: step 76350, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 59h:45m:54s remains)
INFO - root - 2017-12-10 01:05:58.612782: step 76360, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 59h:59m:54s remains)
INFO - root - 2017-12-10 01:06:07.153538: step 76370, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 60h:56m:22s remains)
INFO - root - 2017-12-10 01:06:15.824314: step 76380, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:25m:44s remains)
INFO - root - 2017-12-10 01:06:24.494440: step 76390, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:02m:55s remains)
INFO - root - 2017-12-10 01:06:33.134389: step 76400, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:16m:07s remains)
2017-12-10 01:06:34.010397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033912717 -0.0033904526 -0.0033906288 -0.0033909229 -0.0033910153 -0.0033910386 -0.0033910885 -0.0033912282 -0.0033911562 -0.0033909532 -0.0033908365 -0.0033906379 -0.003390485 -0.0033903671 -0.0033903331][-0.0033898368 -0.0033891469 -0.0033895271 -0.0033899779 -0.0033901706 -0.0033901788 -0.0033901758 -0.0033900992 -0.0033897539 -0.0033894936 -0.0033893788 -0.0033891455 -0.0033889108 -0.0033887096 -0.0033886619][-0.0033901813 -0.0033898214 -0.003390813 -0.0033919211 -0.0033925949 -0.003392674 -0.0033923327 -0.0033915583 -0.0033905925 -0.0033898209 -0.0033892242 -0.0033887955 -0.0033885383 -0.0033884963 -0.0033885429][-0.00339009 -0.0033903073 -0.0033923483 -0.0033947646 -0.0033960447 -0.0033960997 -0.0033951928 -0.00339353 -0.003391779 -0.0033904563 -0.0033894109 -0.003388765 -0.0033885567 -0.0033886363 -0.0033887082][-0.0033897208 -0.0033907006 -0.0033940417 -0.0033979195 -0.0034000061 -0.0034002701 -0.0033987442 -0.0033959928 -0.0033930908 -0.0033912074 -0.0033900756 -0.0033893969 -0.0033891413 -0.0033891336 -0.0033891159][-0.003389392 -0.0033912745 -0.0033959285 -0.0034009225 -0.0034038967 -0.0034046115 -0.0034028834 -0.0033991188 -0.0033950768 -0.0033926948 -0.0033917569 -0.00339119 -0.0033907625 -0.0033903823 -0.0033900216][-0.0033894775 -0.0033920666 -0.0033977549 -0.0034036124 -0.0034074935 -0.0034086097 -0.0034067822 -0.0034023719 -0.0033977157 -0.0033951369 -0.0033943544 -0.0033939313 -0.0033932908 -0.0033922317 -0.0033911981][-0.0033897595 -0.0033926873 -0.0033987791 -0.0034052439 -0.0034096402 -0.0034111326 -0.0034095291 -0.0034055037 -0.0034011481 -0.0033986783 -0.0033978135 -0.0033975672 -0.0033965663 -0.0033945914 -0.0033926303][-0.003390054 -0.0033928619 -0.0033986988 -0.0034051288 -0.0034098001 -0.0034117533 -0.003411029 -0.0034083065 -0.0034053433 -0.0034034962 -0.0034024282 -0.0034019495 -0.0034003924 -0.0033975805 -0.0033944573][-0.0033899578 -0.0033924733 -0.0033977483 -0.0034038175 -0.0034086481 -0.0034112514 -0.0034117543 -0.0034107247 -0.0034098048 -0.0034090255 -0.0034082269 -0.0034072641 -0.003404896 -0.0034011274 -0.0033967094][-0.0033902181 -0.0033922489 -0.0033969637 -0.003402611 -0.003407415 -0.0034104898 -0.0034117613 -0.0034120209 -0.0034122693 -0.0034124462 -0.0034122714 -0.0034111326 -0.0034083126 -0.0034038881 -0.0033987693][-0.0033906896 -0.0033921569 -0.0033962433 -0.0034014659 -0.0034062285 -0.0034095093 -0.0034111482 -0.0034119186 -0.0034127238 -0.0034133254 -0.0034134309 -0.0034123282 -0.0034094965 -0.0034050993 -0.0033999162][-0.0033906042 -0.0033913183 -0.0033948435 -0.0033996953 -0.0034043684 -0.0034078588 -0.0034100062 -0.003411137 -0.0034118511 -0.0034121994 -0.0034119822 -0.0034109168 -0.003408256 -0.0034042511 -0.0033996459][-0.0033902633 -0.0033904512 -0.0033934161 -0.0033974713 -0.0034015789 -0.0034051635 -0.0034077384 -0.0034092 -0.0034097859 -0.0034098178 -0.0034092539 -0.0034078951 -0.0034054175 -0.0034019768 -0.0033982103][-0.0033902328 -0.0033900903 -0.0033924051 -0.0033953651 -0.0033984629 -0.0034014028 -0.003403689 -0.0034051067 -0.003405618 -0.0034055221 -0.0034048646 -0.0034034795 -0.0034014569 -0.0033987931 -0.0033959725]]...]
INFO - root - 2017-12-10 01:06:42.545055: step 76410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:15m:30s remains)
INFO - root - 2017-12-10 01:06:51.064388: step 76420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:10m:45s remains)
INFO - root - 2017-12-10 01:06:59.680671: step 76430, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 60h:23m:28s remains)
INFO - root - 2017-12-10 01:07:08.073208: step 76440, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 59h:51m:19s remains)
INFO - root - 2017-12-10 01:07:16.650745: step 76450, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 62h:09m:50s remains)
INFO - root - 2017-12-10 01:07:25.379153: step 76460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:42m:25s remains)
INFO - root - 2017-12-10 01:07:33.877798: step 76470, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:47m:48s remains)
INFO - root - 2017-12-10 01:07:42.506864: step 76480, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:14m:55s remains)
INFO - root - 2017-12-10 01:07:51.162327: step 76490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:45m:09s remains)
INFO - root - 2017-12-10 01:07:59.795138: step 76500, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 63h:05m:38s remains)
2017-12-10 01:08:00.722850: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35890338 0.32626709 0.27923083 0.22623138 0.17162694 0.12299281 0.082922578 0.054972164 0.036184449 0.025728747 0.020214941 0.018008912 0.016746035 0.015024573 0.012531606][0.3949081 0.36427325 0.31868097 0.26619908 0.21001494 0.15733518 0.11059571 0.074830227 0.048412967 0.032012478 0.022599492 0.018449754 0.016655592 0.014899427 0.012414899][0.41831964 0.3928048 0.35087964 0.30123878 0.24702424 0.19409688 0.14462325 0.10428052 0.071772985 0.049361929 0.035328656 0.028675312 0.025708558 0.023004713 0.019061841][0.43885198 0.4182826 0.3807247 0.33489606 0.28293028 0.23014453 0.17923912 0.13602506 0.10048655 0.07535398 0.058840703 0.050107636 0.045020644 0.040059458 0.03308836][0.45621228 0.44000587 0.40626472 0.36480853 0.31691417 0.26659131 0.21630816 0.17202243 0.1350244 0.10843408 0.0912478 0.082355507 0.076483034 0.069052793 0.05753256][0.46901563 0.45460424 0.42267126 0.38565949 0.3434839 0.2990559 0.25401917 0.2130003 0.17800602 0.15220119 0.13491356 0.12463017 0.11630128 0.10523576 0.088326007][0.4712691 0.45927772 0.42809296 0.39353272 0.35519969 0.317169 0.27956194 0.24590252 0.21786471 0.19751213 0.18437161 0.17607045 0.1670399 0.15209039 0.12851556][0.46805111 0.45753461 0.42569825 0.39217329 0.35630372 0.32271272 0.29105806 0.26508853 0.24500726 0.23167627 0.22463615 0.22047384 0.21322039 0.19709875 0.16910456][0.45864806 0.44952974 0.41729638 0.38357177 0.34824279 0.31748486 0.290101 0.2696209 0.25554797 0.24867949 0.24738869 0.24735749 0.24261153 0.22700579 0.1974735][0.43303895 0.42825535 0.39846209 0.36631981 0.33265147 0.30443311 0.28031322 0.26439449 0.25490072 0.25195685 0.25361452 0.25616124 0.25314629 0.23793541 0.20818162][0.39079088 0.38966626 0.36393818 0.33506683 0.30451134 0.27941582 0.25871298 0.2466765 0.24060538 0.2403269 0.24333481 0.24648245 0.24392593 0.22971445 0.20161211][0.33195034 0.33474338 0.31437397 0.2900866 0.26359674 0.24126558 0.22317639 0.21401533 0.21019083 0.21134159 0.21450807 0.21765962 0.21560018 0.20285182 0.17779259][0.2627598 0.26806572 0.25376821 0.23486704 0.21383892 0.19573355 0.18080083 0.17296581 0.1693178 0.1700372 0.17186324 0.1738449 0.17179799 0.16146509 0.14132512][0.18994501 0.19547981 0.18679729 0.17396802 0.15932824 0.14604324 0.13512169 0.12907378 0.12598887 0.12617761 0.12652354 0.12705936 0.12456389 0.11613575 0.10068534][0.12361884 0.1279007 0.12307556 0.11555906 0.10670828 0.098080792 0.091039442 0.087139174 0.08543092 0.085904405 0.086099207 0.086307541 0.084065482 0.077420257 0.065708026]]...]
INFO - root - 2017-12-10 01:08:09.158102: step 76510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:54m:58s remains)
INFO - root - 2017-12-10 01:08:17.700273: step 76520, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 62h:37m:04s remains)
INFO - root - 2017-12-10 01:08:26.351821: step 76530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:35m:21s remains)
INFO - root - 2017-12-10 01:08:34.837593: step 76540, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 60h:01m:47s remains)
INFO - root - 2017-12-10 01:08:43.428552: step 76550, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:53m:37s remains)
INFO - root - 2017-12-10 01:08:52.138323: step 76560, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 60h:08m:32s remains)
INFO - root - 2017-12-10 01:09:00.647107: step 76570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 61h:27m:46s remains)
INFO - root - 2017-12-10 01:09:09.321695: step 76580, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 61h:15m:26s remains)
INFO - root - 2017-12-10 01:09:18.006856: step 76590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:31m:15s remains)
INFO - root - 2017-12-10 01:09:26.632094: step 76600, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:29m:28s remains)
2017-12-10 01:09:27.544868: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027831843 0.015912546 0.0065588905 0.00073134177 -0.0020994823 -0.0031186913 -0.0033533666 -0.0033832123 -0.0033811072 -0.0033776865 -0.0033736636 -0.0033723363 -0.0033717726 -0.003372608 -0.0033735808][0.014183749 0.0071366066 0.0018200746 -0.0013276865 -0.002756767 -0.0032204811 -0.0033125256 -0.0033267471 -0.0033397949 -0.0033543343 -0.0033654196 -0.003372238 -0.003374564 -0.0033753947 -0.0033756015][0.0048136832 0.0013542313 -0.0011401388 -0.0024754852 -0.0029238684 -0.0028572453 -0.00267801 -0.0025970426 -0.0026990622 -0.00289255 -0.0031013517 -0.0032629389 -0.0033476392 -0.0033751782 -0.0033774897][-0.00021201046 -0.0016049761 -0.0025361625 -0.0028614325 -0.0026028925 -0.0019097027 -0.0012306713 -0.00091260253 -0.0011627993 -0.0017468616 -0.0024157511 -0.002954192 -0.0032528234 -0.0033606209 -0.0033771473][-0.0023829571 -0.0028119436 -0.0030534307 -0.0029226025 -0.0022171224 -0.00092783477 0.00034295139 0.00097838021 0.00062521012 -0.00037577772 -0.0015805356 -0.0025733272 -0.0031335135 -0.0033392627 -0.003374761][-0.0031218191 -0.0032101469 -0.0032244297 -0.0029454594 -0.0020778764 -0.00055421609 0.000986832 0.001798515 0.0014572607 0.00029657129 -0.0011607313 -0.0023804291 -0.0030727584 -0.0033270852 -0.0033733572][-0.0033068992 -0.0033154415 -0.0032886392 -0.0030182493 -0.0022362843 -0.00088467286 0.00048299157 0.0012241842 0.00097061391 -5.0185481e-05 -0.0013698402 -0.0024761148 -0.0031008106 -0.0033284072 -0.0033714573][-0.0033485207 -0.0033462173 -0.0033225964 -0.0031228904 -0.0025583655 -0.0016294577 -0.00071101915 -0.00020905072 -0.00036329892 -0.0010673569 -0.0019909381 -0.0027572582 -0.003184278 -0.0033392287 -0.0033709428][-0.0033472711 -0.0033432515 -0.0033314258 -0.0032143954 -0.0028845931 -0.0023687645 -0.0018784734 -0.001617212 -0.0017080962 -0.0021087187 -0.0026251164 -0.0030414104 -0.0032681753 -0.0033515028 -0.0033712413][-0.0033541238 -0.0033508164 -0.0033472604 -0.0032951154 -0.0031414328 -0.0029098818 -0.0026966033 -0.0025885205 -0.002639662 -0.0028289543 -0.0030590675 -0.0032341317 -0.0033266419 -0.0033620663 -0.0033721391][-0.0033612 -0.0033589774 -0.0033590659 -0.0033430618 -0.0032857021 -0.0032023503 -0.0031270091 -0.0030923965 -0.0031183418 -0.0031919566 -0.0032733439 -0.0033293106 -0.0033580181 -0.0033698019 -0.0033737875][-0.0033683742 -0.003366346 -0.0033674559 -0.0033657285 -0.0033507189 -0.0033283981 -0.0033077546 -0.0032997646 -0.0033084722 -0.0033298752 -0.0033506944 -0.00336251 -0.0033687898 -0.0033718494 -0.0033733468][-0.0033719202 -0.0033703926 -0.0033715041 -0.0033732643 -0.0033718946 -0.0033690617 -0.0033667451 -0.0033663171 -0.0033683286 -0.0033718133 -0.0033729156 -0.0033725686 -0.0033721325 -0.0033720492 -0.0033722848][-0.0033723589 -0.0033710641 -0.0033723437 -0.0033738546 -0.0033748718 -0.0033764488 -0.0033779158 -0.003379073 -0.0033793557 -0.0033795983 -0.0033775324 -0.0033756031 -0.003373761 -0.0033726289 -0.0033726564][-0.0033717828 -0.003370425 -0.0033715751 -0.0033730392 -0.0033741777 -0.0033755703 -0.0033770162 -0.0033774821 -0.0033779268 -0.003377378 -0.0033759244 -0.0033746583 -0.0033735242 -0.0033732448 -0.0033730844]]...]
INFO - root - 2017-12-10 01:09:36.092527: step 76610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 60h:20m:03s remains)
INFO - root - 2017-12-10 01:09:44.749184: step 76620, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 60h:49m:51s remains)
INFO - root - 2017-12-10 01:09:53.475378: step 76630, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 63h:55m:44s remains)
INFO - root - 2017-12-10 01:10:01.989840: step 76640, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 58h:42m:16s remains)
INFO - root - 2017-12-10 01:10:10.369726: step 76650, loss = 0.90, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 56h:23m:07s remains)
INFO - root - 2017-12-10 01:10:19.067891: step 76660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:26m:00s remains)
INFO - root - 2017-12-10 01:10:27.626253: step 76670, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:29m:01s remains)
INFO - root - 2017-12-10 01:10:36.285265: step 76680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:42m:31s remains)
INFO - root - 2017-12-10 01:10:44.997261: step 76690, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:19m:10s remains)
INFO - root - 2017-12-10 01:10:53.754567: step 76700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:42m:11s remains)
2017-12-10 01:10:54.647577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034083463 -0.003407334 -0.003407255 -0.0034072686 -0.0034071882 -0.0034071498 -0.0034071025 -0.0034071149 -0.0034070655 -0.0034069081 -0.0034067486 -0.0034068383 -0.0034068809 -0.0034068516 -0.0034068532][-0.0034063174 -0.00340523 -0.003405246 -0.0034053288 -0.0034052748 -0.0034052085 -0.0034051028 -0.0034050355 -0.0034049372 -0.0034048466 -0.003404815 -0.0034048886 -0.0034049139 -0.0034049205 -0.0034049784][-0.0034063642 -0.0034054371 -0.0034056134 -0.0034058222 -0.0034057847 -0.0034057656 -0.0034055787 -0.003405361 -0.0034051677 -0.0034051449 -0.0034052164 -0.0034052934 -0.0034052969 -0.0034052297 -0.0034051877][-0.0034064697 -0.003405642 -0.0034059233 -0.0034062176 -0.0034061617 -0.0034061724 -0.0034059889 -0.0034057577 -0.0034054471 -0.0034052979 -0.0034054527 -0.0034055975 -0.0034056033 -0.0034054569 -0.0034053787][-0.0034066737 -0.0034061028 -0.0034067156 -0.0034073165 -0.003407422 -0.003407527 -0.0034074662 -0.0034072471 -0.0034068809 -0.0034064958 -0.0034064262 -0.0034063184 -0.0034061361 -0.0034058164 -0.0034056474][-0.0034069344 -0.0034067691 -0.0034078369 -0.0034085554 -0.0034084418 -0.003408215 -0.0034079943 -0.0034079722 -0.0034076634 -0.0034072967 -0.0034071803 -0.0034068157 -0.0034065179 -0.0034062078 -0.0034059207][-0.0034071652 -0.0034074076 -0.0034090397 -0.0034098625 -0.0034093927 -0.0034084786 -0.0034074152 -0.0034070336 -0.0034071323 -0.0034070932 -0.0034071952 -0.0034067496 -0.0034066257 -0.0034065486 -0.0034062397][-0.0034074052 -0.0034079093 -0.0034098737 -0.0034109156 -0.0034101936 -0.0034083242 -0.0034056418 -0.0034045887 -0.003405405 -0.0034060215 -0.0034066355 -0.003406601 -0.0034068078 -0.0034068676 -0.0034065947][-0.0034077791 -0.0034085552 -0.0034108344 -0.0034123892 -0.0034122441 -0.0034108832 -0.0034089438 -0.0034077514 -0.0034079819 -0.0034086818 -0.0034090746 -0.003408761 -0.0034083992 -0.0034078397 -0.0034072681][-0.0034081114 -0.0034089626 -0.0034116257 -0.0034140409 -0.0034151967 -0.0034154723 -0.003415022 -0.0034146628 -0.0034144802 -0.0034144304 -0.0034141433 -0.0034130395 -0.0034114048 -0.0034098132 -0.0034084527][-0.003408283 -0.0034093754 -0.0034121561 -0.0034147743 -0.0034165466 -0.0034178752 -0.0034184265 -0.0034187448 -0.0034187627 -0.00341848 -0.0034175564 -0.0034157992 -0.0034135471 -0.0034113158 -0.0034092444][-0.0034083654 -0.0034096267 -0.0034124188 -0.003414942 -0.0034167308 -0.0034181848 -0.0034192654 -0.0034199178 -0.0034199581 -0.0034194097 -0.00341822 -0.003416365 -0.0034140078 -0.0034115554 -0.0034092986][-0.0034082874 -0.0034090986 -0.0034114209 -0.0034136062 -0.003415331 -0.00341681 -0.0034179208 -0.0034186244 -0.0034186502 -0.0034180591 -0.0034168237 -0.0034150432 -0.0034128954 -0.0034106539 -0.003408703][-0.0034079184 -0.0034082152 -0.0034099831 -0.0034117431 -0.0034132423 -0.0034144651 -0.0034153066 -0.0034157636 -0.003415674 -0.0034151054 -0.0034140921 -0.0034126996 -0.0034110509 -0.0034093002 -0.0034078024][-0.0034072564 -0.003406978 -0.0034081072 -0.003409338 -0.0034103987 -0.0034111559 -0.0034116423 -0.0034117734 -0.0034116427 -0.0034111815 -0.0034105333 -0.0034097477 -0.0034087426 -0.0034076646 -0.0034067379]]...]
INFO - root - 2017-12-10 01:11:03.070818: step 76710, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:27m:46s remains)
INFO - root - 2017-12-10 01:11:11.583877: step 76720, loss = 0.89, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 58h:05m:59s remains)
INFO - root - 2017-12-10 01:11:20.142138: step 76730, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 59h:11m:16s remains)
INFO - root - 2017-12-10 01:11:28.795622: step 76740, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:56m:50s remains)
INFO - root - 2017-12-10 01:11:37.332654: step 76750, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 59h:55m:09s remains)
INFO - root - 2017-12-10 01:11:45.930208: step 76760, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-10 01:11:54.476965: step 76770, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:29m:47s remains)
INFO - root - 2017-12-10 01:12:03.240519: step 76780, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:49m:38s remains)
INFO - root - 2017-12-10 01:12:11.994955: step 76790, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 64h:30m:23s remains)
INFO - root - 2017-12-10 01:12:20.708041: step 76800, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:11m:44s remains)
2017-12-10 01:12:21.655094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003391007 -0.0033890184 -0.0033887862 -0.0033887415 -0.0033887241 -0.0033887634 -0.0033888083 -0.0033889036 -0.0033890032 -0.0033890372 -0.0033890244 -0.0033890163 -0.0033890381 -0.0033890689 -0.0033890838][-0.0033895874 -0.0033873247 -0.0033870507 -0.003387037 -0.0033869876 -0.0033869296 -0.0033869031 -0.003386969 -0.0033870591 -0.0033870765 -0.0033870393 -0.0033870083 -0.0033870216 -0.0033870747 -0.003387125][-0.0033898661 -0.003387514 -0.0033872777 -0.0033872584 -0.0033871576 -0.0033869296 -0.0033867562 -0.0033867236 -0.0033867024 -0.0033866742 -0.0033866256 -0.0033866127 -0.0033867205 -0.0033869161 -0.0033870824][-0.0033898933 -0.0033874763 -0.0033872288 -0.0033871979 -0.0033870202 -0.0033866502 -0.0033864253 -0.0033863601 -0.0033862516 -0.0033860961 -0.0033859615 -0.003385955 -0.003386148 -0.0033865136 -0.0033868374][-0.0033899888 -0.0033875832 -0.0033872859 -0.0033871485 -0.0033868589 -0.0033864335 -0.0033861196 -0.003385935 -0.0033856935 -0.0033854225 -0.0033852025 -0.0033852023 -0.0033854777 -0.0033860072 -0.0033865049][-0.0033900107 -0.0033875334 -0.0033871287 -0.0033867967 -0.0033864379 -0.0033860994 -0.0033857217 -0.0033854467 -0.0033851664 -0.003385009 -0.0033847224 -0.0033845843 -0.0033848255 -0.0033854002 -0.0033860395][-0.0033899387 -0.0033873662 -0.0033868339 -0.0033863047 -0.0033858961 -0.0033855273 -0.003385029 -0.0033846996 -0.003384572 -0.0033845666 -0.0033842663 -0.0033840802 -0.003384324 -0.0033849787 -0.0033858111][-0.0033896146 -0.0033870288 -0.0033864412 -0.0033858891 -0.0033853888 -0.0033846642 -0.0033836616 -0.003383284 -0.0033835557 -0.0033839338 -0.0033837652 -0.0033835589 -0.0033838216 -0.0033845173 -0.0033854805][-0.0033892374 -0.0033867126 -0.0033863096 -0.0033859289 -0.0033854847 -0.0033849399 -0.0033840404 -0.003383508 -0.0033835873 -0.0033839073 -0.0033838281 -0.0033835028 -0.0033835801 -0.0033841883 -0.0033851983][-0.0033889967 -0.0033866658 -0.0033865508 -0.0033863278 -0.0033860616 -0.0033861143 -0.0033859829 -0.0033856055 -0.0033853198 -0.0033852933 -0.0033849676 -0.0033842281 -0.0033839936 -0.0033843459 -0.003385264][-0.0033888607 -0.0033866493 -0.0033867441 -0.0033866556 -0.0033867145 -0.0033874004 -0.0033879587 -0.0033877874 -0.0033874519 -0.0033873203 -0.0033867441 -0.003385785 -0.003385216 -0.0033852078 -0.0033858123][-0.0033890125 -0.0033868034 -0.0033869662 -0.003386925 -0.0033870626 -0.003387714 -0.0033881832 -0.0033881881 -0.0033880563 -0.0033880009 -0.0033875827 -0.0033868956 -0.0033864256 -0.0033862595 -0.003386498][-0.0033893089 -0.0033868533 -0.0033870209 -0.0033869632 -0.0033869976 -0.0033872421 -0.0033873601 -0.0033873692 -0.0033873545 -0.0033873038 -0.0033870668 -0.0033867594 -0.0033866244 -0.0033866067 -0.0033867017][-0.0033894866 -0.0033868379 -0.0033870488 -0.0033870398 -0.0033870356 -0.0033870752 -0.0033870426 -0.0033869725 -0.0033869296 -0.0033868374 -0.0033867226 -0.0033866426 -0.0033866491 -0.0033867336 -0.0033868272][-0.0033896125 -0.0033869508 -0.0033870977 -0.0033871017 -0.0033870949 -0.0033871224 -0.0033871217 -0.0033870761 -0.0033870561 -0.0033870179 -0.0033869524 -0.0033868945 -0.0033868775 -0.0033868931 -0.0033869413]]...]
INFO - root - 2017-12-10 01:12:30.030054: step 76810, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 63h:21m:04s remains)
INFO - root - 2017-12-10 01:12:38.548039: step 76820, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 51h:30m:41s remains)
INFO - root - 2017-12-10 01:12:47.299573: step 76830, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:16m:21s remains)
INFO - root - 2017-12-10 01:12:55.879628: step 76840, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 62h:12m:39s remains)
INFO - root - 2017-12-10 01:13:04.528175: step 76850, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:25m:29s remains)
INFO - root - 2017-12-10 01:13:13.254129: step 76860, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 60h:26m:00s remains)
INFO - root - 2017-12-10 01:13:21.725412: step 76870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:54m:03s remains)
INFO - root - 2017-12-10 01:13:30.304499: step 76880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:58m:32s remains)
INFO - root - 2017-12-10 01:13:38.976031: step 76890, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 62h:43m:58s remains)
INFO - root - 2017-12-10 01:13:47.593894: step 76900, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 60h:33m:39s remains)
2017-12-10 01:13:48.457064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003385867 -0.0033835243 -0.0033820127 -0.0033806674 -0.0033794588 -0.0033793582 -0.0033799605 -0.0033807685 -0.0033817408 -0.003382704 -0.0033833771 -0.0033834656 -0.0033832574 -0.003383311 -0.0033831575][-0.0033832376 -0.0033810202 -0.0033793214 -0.003377954 -0.0033769137 -0.0033767568 -0.0033772178 -0.003377934 -0.0033790246 -0.0033801168 -0.00338096 -0.0033811207 -0.0033808085 -0.0033806555 -0.0033802975][-0.0033843629 -0.00338225 -0.0033805042 -0.0033790667 -0.0033779121 -0.0033774183 -0.0033773666 -0.0033776073 -0.0033785119 -0.0033795242 -0.0033804676 -0.0033806236 -0.0033805096 -0.0033802262 -0.0033795803][-0.0033906437 -0.003387657 -0.0033852945 -0.0033829655 -0.0033805452 -0.0033786136 -0.0033770425 -0.0033763133 -0.0033768732 -0.0033780842 -0.0033794255 -0.00338007 -0.0033803466 -0.0033801107 -0.0033797016][-0.0033976517 -0.0033942631 -0.0033913178 -0.0033875371 -0.0033830535 -0.0033787983 -0.0033751258 -0.0033728895 -0.0033725845 -0.0033741863 -0.0033765009 -0.0033780145 -0.0033790085 -0.0033795582 -0.0033798942][-0.0034015393 -0.0033983896 -0.0033952848 -0.003390155 -0.0033832039 -0.0033749794 -0.0033671802 -0.0033625078 -0.0033611732 -0.0033630333 -0.0033664405 -0.0033698208 -0.0033729537 -0.0033761491 -0.003378646][-0.0034030145 -0.0034000827 -0.0033965013 -0.0033899765 -0.0033798802 -0.0033660065 -0.0033511657 -0.0033415416 -0.003338913 -0.0033406534 -0.003345418 -0.0033517003 -0.0033593676 -0.0033684331 -0.0033760492][-0.0034018082 -0.0033992087 -0.0033957628 -0.0033893937 -0.003379016 -0.0033638666 -0.0033463947 -0.0033331432 -0.0033270561 -0.0033250367 -0.0033278542 -0.003335851 -0.0033476837 -0.0033623732 -0.0033746189][-0.0033987937 -0.0033966796 -0.003394193 -0.0033904035 -0.0033837692 -0.0033738285 -0.0033615252 -0.00334987 -0.0033403072 -0.0033332433 -0.0033318056 -0.0033378273 -0.0033493913 -0.0033642505 -0.0033774269][-0.0033942768 -0.0033930077 -0.0033926796 -0.00339189 -0.0033899986 -0.0033852661 -0.0033788914 -0.0033718785 -0.0033639537 -0.0033569075 -0.0033538374 -0.0033570961 -0.0033647197 -0.0033751093 -0.0033847881][-0.0033900521 -0.0033897546 -0.0033910738 -0.0033927264 -0.0033939886 -0.0033928647 -0.0033904456 -0.0033877885 -0.0033837329 -0.0033796022 -0.0033773552 -0.0033788201 -0.003383091 -0.0033884195 -0.0033930675][-0.003387585 -0.0033882558 -0.0033905006 -0.003393024 -0.0033955581 -0.0033964168 -0.0033962904 -0.003396221 -0.0033954913 -0.0033941653 -0.0033925064 -0.00339251 -0.0033943274 -0.00339644 -0.0033983917][-0.0033838258 -0.00338692 -0.0033898226 -0.0033920612 -0.0033943404 -0.0033954529 -0.003396445 -0.003397679 -0.0033990855 -0.0033998089 -0.0033996084 -0.0033992636 -0.003399529 -0.003400356 -0.0034011949][-0.0033766343 -0.0033841147 -0.0033887937 -0.0033906947 -0.0033919062 -0.0033926491 -0.0033939194 -0.0033957681 -0.0033980934 -0.0033997828 -0.0034002918 -0.0034005034 -0.003400855 -0.0034021814 -0.0034030441][-0.0033702194 -0.0033811068 -0.0033874272 -0.0033892733 -0.0033892887 -0.0033890205 -0.0033893096 -0.0033907448 -0.0033927017 -0.003394742 -0.00339558 -0.0033965113 -0.0033981418 -0.0034013062 -0.0034036657]]...]
INFO - root - 2017-12-10 01:13:56.999419: step 76910, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 62h:24m:14s remains)
INFO - root - 2017-12-10 01:14:05.535330: step 76920, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 52h:19m:28s remains)
INFO - root - 2017-12-10 01:14:14.122679: step 76930, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:17m:38s remains)
INFO - root - 2017-12-10 01:14:22.725478: step 76940, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:59m:19s remains)
INFO - root - 2017-12-10 01:14:31.063848: step 76950, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 58h:00m:48s remains)
INFO - root - 2017-12-10 01:14:39.566421: step 76960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:10m:14s remains)
INFO - root - 2017-12-10 01:14:48.112738: step 76970, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:43m:43s remains)
INFO - root - 2017-12-10 01:14:56.691818: step 76980, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:19m:48s remains)
INFO - root - 2017-12-10 01:15:05.319024: step 76990, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:14m:36s remains)
INFO - root - 2017-12-10 01:15:13.912342: step 77000, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 64h:13m:42s remains)
2017-12-10 01:15:14.759471: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27009854 0.24275124 0.21919237 0.20275369 0.19170646 0.19056398 0.20002894 0.22411624 0.26459739 0.31439868 0.37353525 0.4343861 0.49029487 0.52949971 0.555466][0.21751513 0.19537972 0.17906035 0.17232572 0.16985182 0.17742269 0.19576904 0.22611651 0.27169555 0.32416907 0.38426182 0.44587538 0.50239968 0.54344523 0.56940353][0.17167333 0.15760954 0.14867546 0.14899029 0.15491097 0.16995057 0.1944343 0.23036151 0.28066564 0.33681995 0.39584041 0.456083 0.51108205 0.55212307 0.57580167][0.14818759 0.13838039 0.13404906 0.14064556 0.15421142 0.17842235 0.20968932 0.25235626 0.30696973 0.36352548 0.41945153 0.47455379 0.52462065 0.56130385 0.58130187][0.14809525 0.1419414 0.14256032 0.15405513 0.17456123 0.20672065 0.24637322 0.29400694 0.34935212 0.40344691 0.4537586 0.5002892 0.5418089 0.5724448 0.58818895][0.16850588 0.16750281 0.17233492 0.18854174 0.21478616 0.25251523 0.2975212 0.34812009 0.4012185 0.45173594 0.49462435 0.5321843 0.56306207 0.58546764 0.59595269][0.20339917 0.20665976 0.21439922 0.23256509 0.26182243 0.30077493 0.34740409 0.39860967 0.45034248 0.49701518 0.53261369 0.56186277 0.58335966 0.59781343 0.60193938][0.24429505 0.25098029 0.26094204 0.2800419 0.30933112 0.34746411 0.39211625 0.43999121 0.48703566 0.52646422 0.55483049 0.5760783 0.5886296 0.59440994 0.59275782][0.2834101 0.29215667 0.30351916 0.32310468 0.3506346 0.38450325 0.42444402 0.46717176 0.50750619 0.53876632 0.56101507 0.57461381 0.57942033 0.57896721 0.57350188][0.31448656 0.32494706 0.33613467 0.3540808 0.37822998 0.4061574 0.43945673 0.47303286 0.50380349 0.52709919 0.54184169 0.54954475 0.55107135 0.54869717 0.54273975][0.33693552 0.34863114 0.35856479 0.37297484 0.39128065 0.41161361 0.43587348 0.46028003 0.48195642 0.49707073 0.50588387 0.5103302 0.51141691 0.5095706 0.50552791][0.35222375 0.36272892 0.36954144 0.37885752 0.38999724 0.40230969 0.41720986 0.4316847 0.44450164 0.45250642 0.45703277 0.45932704 0.46155205 0.46302417 0.46234167][0.35352841 0.36202893 0.3650943 0.36861607 0.3729167 0.37815991 0.38500258 0.39132535 0.39655668 0.39891785 0.40044442 0.4024387 0.40624622 0.41017529 0.41309083][0.33188742 0.33746359 0.33709037 0.33685303 0.33669358 0.33711037 0.33839014 0.33965093 0.340608 0.34115508 0.34164622 0.34352943 0.34806302 0.35355958 0.35825616][0.2889511 0.29322165 0.29170024 0.28951928 0.28699493 0.28531328 0.28412279 0.28304246 0.28230998 0.2821638 0.2824035 0.28501061 0.29001045 0.29557616 0.30035275]]...]
INFO - root - 2017-12-10 01:15:23.275952: step 77010, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:20m:25s remains)
INFO - root - 2017-12-10 01:15:32.007010: step 77020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:30m:07s remains)
INFO - root - 2017-12-10 01:15:40.505581: step 77030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:48m:13s remains)
INFO - root - 2017-12-10 01:15:49.132481: step 77040, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:54m:49s remains)
INFO - root - 2017-12-10 01:15:57.747911: step 77050, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 61h:57m:34s remains)
INFO - root - 2017-12-10 01:16:06.540782: step 77060, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:00m:23s remains)
INFO - root - 2017-12-10 01:16:14.788965: step 77070, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:24m:16s remains)
INFO - root - 2017-12-10 01:16:23.488230: step 77080, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.894 sec/batch; 63h:23m:54s remains)
INFO - root - 2017-12-10 01:16:32.057044: step 77090, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 60h:17m:34s remains)
INFO - root - 2017-12-10 01:16:40.389310: step 77100, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 59h:04m:45s remains)
2017-12-10 01:16:41.206249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22545165 0.19040672 0.14802712 0.10802828 0.073727004 0.049888868 0.034032438 0.023837889 0.016679456 0.011466371 0.0071433345 0.0032503305 -3.16184e-07 -0.0019887884 -0.0029213368][0.30713782 0.26763102 0.21790963 0.16976918 0.12598568 0.092610516 0.067605421 0.049566992 0.035605505 0.024775507 0.015850022 0.0083560823 0.0024796871 -0.0010953252 -0.0026576575][0.38638476 0.34791797 0.29614556 0.24441692 0.19500574 0.15466447 0.12151259 0.0953488 0.07290864 0.053605724 0.036538213 0.021698663 0.0098729832 0.0020740398 -0.0017547337][0.4485186 0.41759095 0.37121636 0.32275245 0.27371943 0.23093611 0.19238949 0.15911202 0.12757282 0.097823195 0.069702312 0.044380426 0.0234894 0.0086780824 0.00055555371][0.48008198 0.46046871 0.42491588 0.38602218 0.34452125 0.30612358 0.26808336 0.2316889 0.19301286 0.15306279 0.11255054 0.07458397 0.042157162 0.018194318 0.0041867336][0.47205454 0.4643364 0.44217306 0.41648486 0.38737676 0.35894608 0.32741722 0.29308668 0.2515097 0.20459382 0.15378948 0.10465773 0.061459519 0.028600896 0.0084606754][0.423974 0.42627 0.4168863 0.40472382 0.389487 0.3734796 0.35192627 0.32367224 0.28393382 0.23509005 0.17910391 0.12371878 0.074106216 0.035821643 0.011646772][0.34474829 0.35398778 0.35518873 0.35393909 0.35040992 0.34543636 0.33393928 0.31318864 0.27859265 0.23311308 0.17868385 0.12416543 0.074856713 0.036717579 0.012305707][0.24969095 0.26223376 0.27000153 0.27620661 0.28095579 0.28355324 0.27955717 0.26557487 0.23788075 0.19969662 0.15282361 0.10582258 0.06330213 0.030822823 0.010096112][0.15769108 0.16989757 0.17990556 0.18884331 0.19695958 0.20316918 0.20373613 0.19522768 0.17507827 0.14654967 0.11128883 0.076106489 0.044406544 0.02069605 0.0059052557][0.084861785 0.0944041 0.10311656 0.1111443 0.11862125 0.12445843 0.1262036 0.1215193 0.10855733 0.090115465 0.067428604 0.0451093 0.025127729 0.010463759 0.0016728404][0.03805013 0.0439453 0.049751181 0.054819174 0.059547223 0.06303025 0.06400343 0.061366618 0.05397376 0.043971263 0.031957906 0.020407874 0.010182609 0.0028229987 -0.0013280855][0.012615638 0.015667258 0.018833268 0.021329634 0.023586217 0.024957824 0.025077444 0.023487074 0.019775702 0.015303593 0.010177579 0.0054763081 0.0014243077 -0.0013809297 -0.0028030123][0.00080745458 0.0020209949 0.0033279674 0.0043516243 0.0052532246 0.0056937719 0.0056653908 0.0049447026 0.0035601587 0.0020429844 0.00039220671 -0.0010246921 -0.0022048773 -0.0029472886 -0.0032639906][-0.0028027855 -0.0025317003 -0.0022204043 -0.0019405737 -0.0017002828 -0.0015872907 -0.0015886911 -0.0017497677 -0.002041013 -0.0023702797 -0.0027053112 -0.0029716706 -0.0031805488 -0.0032990677 -0.0033355714]]...]
INFO - root - 2017-12-10 01:16:49.655394: step 77110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:30m:04s remains)
INFO - root - 2017-12-10 01:16:58.277232: step 77120, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 59h:33m:44s remains)
INFO - root - 2017-12-10 01:17:06.699896: step 77130, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 60h:56m:40s remains)
INFO - root - 2017-12-10 01:17:15.323119: step 77140, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 60h:26m:32s remains)
INFO - root - 2017-12-10 01:17:24.059556: step 77150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 62h:11m:41s remains)
INFO - root - 2017-12-10 01:17:32.644954: step 77160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:59m:16s remains)
INFO - root - 2017-12-10 01:17:41.058318: step 77170, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:37m:40s remains)
INFO - root - 2017-12-10 01:17:49.823813: step 77180, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 64h:12m:03s remains)
INFO - root - 2017-12-10 01:17:58.361967: step 77190, loss = 0.89, batch loss = 0.68 (10.1 examples/sec; 0.789 sec/batch; 55h:58m:17s remains)
INFO - root - 2017-12-10 01:18:06.967470: step 77200, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:43m:32s remains)
2017-12-10 01:18:07.886350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033848083 -0.0033948254 -0.0034022909 -0.0034080164 -0.0034117403 -0.0034128956 -0.003412917 -0.0034129689 -0.0034126216 -0.0034123668 -0.0034123249 -0.0034123573 -0.0034125356 -0.0034124611 -0.0034119212][-0.003387958 -0.003397946 -0.0034046283 -0.0034086623 -0.0034122872 -0.0034139212 -0.0034142814 -0.0034143256 -0.0034134251 -0.0034126968 -0.0034117384 -0.0034111403 -0.0034112851 -0.0034109848 -0.0034101359][-0.0033819107 -0.0033914687 -0.0033983008 -0.0034032767 -0.0034080839 -0.0034112157 -0.0034122877 -0.0034129124 -0.0034124118 -0.0034117473 -0.0034108015 -0.0034102241 -0.0034103929 -0.0034102146 -0.0034093184][-0.0033750969 -0.0033821713 -0.0033889746 -0.0033965083 -0.0034032012 -0.0034079384 -0.00341001 -0.0034107144 -0.0034107841 -0.0034101417 -0.0034092653 -0.0034089051 -0.0034093938 -0.00340941 -0.0034086471][-0.0033759519 -0.003378425 -0.0033833745 -0.0033915183 -0.0033992224 -0.0034044336 -0.0034060709 -0.0034064609 -0.0034075323 -0.0034079079 -0.0034082972 -0.0034086062 -0.0034091291 -0.0034089633 -0.0034075025][-0.0033837559 -0.0033818642 -0.0033843494 -0.0033901106 -0.0033957737 -0.0033995311 -0.0033993626 -0.0033977339 -0.0033988494 -0.0034019426 -0.0034051777 -0.0034073223 -0.0034085573 -0.003407497 -0.0034033861][-0.0033949292 -0.0033912458 -0.0033910645 -0.003392702 -0.0033933627 -0.0033916875 -0.0033868814 -0.0033828418 -0.0033843918 -0.0033908044 -0.0033984813 -0.0034045489 -0.0034072325 -0.0034044541 -0.0033953676][-0.0034055472 -0.0034019852 -0.0034000904 -0.0033976748 -0.0033928819 -0.0033849035 -0.0033746045 -0.0033658771 -0.0033666687 -0.0033773184 -0.0033905734 -0.003400984 -0.0034050262 -0.0034000871 -0.003385795][-0.0034116826 -0.0034090865 -0.0034070339 -0.00340357 -0.0033974659 -0.0033861613 -0.0033717353 -0.0033602042 -0.0033601038 -0.0033714795 -0.0033861061 -0.0033979886 -0.0034026674 -0.0033975991 -0.0033816216][-0.0034129142 -0.0034117606 -0.0034110055 -0.0034086488 -0.0034030974 -0.0033935905 -0.0033818819 -0.0033721293 -0.0033717239 -0.0033792169 -0.0033899832 -0.0033994806 -0.0034036273 -0.0034000373 -0.0033862467][-0.0034129934 -0.0034120081 -0.0034119133 -0.0034109973 -0.0034081109 -0.0034021398 -0.0033950421 -0.0033887422 -0.0033877247 -0.0033913245 -0.003397078 -0.0034034862 -0.0034069256 -0.0034054562 -0.0033968976][-0.0034117212 -0.0034108348 -0.0034111268 -0.0034113107 -0.0034105463 -0.0034081992 -0.003405032 -0.0034019661 -0.0034012138 -0.0034021602 -0.0034045726 -0.0034074667 -0.0034095831 -0.0034095095 -0.0034057959][-0.0034106304 -0.0034094385 -0.0034097172 -0.0034104518 -0.0034107813 -0.0034107708 -0.0034098516 -0.0034090059 -0.0034086546 -0.0034083233 -0.0034088171 -0.0034100146 -0.0034115142 -0.0034120332 -0.0034111156][-0.0034097787 -0.0034084758 -0.0034085861 -0.0034093016 -0.0034099843 -0.0034104169 -0.0034104779 -0.0034107573 -0.0034109643 -0.0034106218 -0.0034106786 -0.0034110411 -0.0034113342 -0.0034116469 -0.0034121121][-0.0034092865 -0.0034077365 -0.0034076988 -0.003408062 -0.0034085682 -0.003408924 -0.0034091379 -0.0034096469 -0.0034101056 -0.0034102085 -0.0034102895 -0.0034104839 -0.0034106823 -0.0034107338 -0.0034111366]]...]
INFO - root - 2017-12-10 01:18:16.433174: step 77210, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 62h:01m:29s remains)
INFO - root - 2017-12-10 01:18:25.108311: step 77220, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:54m:56s remains)
INFO - root - 2017-12-10 01:18:33.824593: step 77230, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 61h:08m:44s remains)
INFO - root - 2017-12-10 01:18:42.456483: step 77240, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:27m:17s remains)
INFO - root - 2017-12-10 01:18:51.190449: step 77250, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 62h:50m:08s remains)
INFO - root - 2017-12-10 01:18:59.847874: step 77260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:54m:29s remains)
INFO - root - 2017-12-10 01:19:08.258440: step 77270, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:12m:38s remains)
INFO - root - 2017-12-10 01:19:17.087955: step 77280, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:36m:57s remains)
INFO - root - 2017-12-10 01:19:25.737374: step 77290, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:57m:21s remains)
INFO - root - 2017-12-10 01:19:34.488985: step 77300, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:22m:59s remains)
2017-12-10 01:19:35.444353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0023434605 -0.0010985716 0.000501564 0.0023446183 0.0042173211 0.0057095159 0.0063705491 0.0065388689 0.0063721035 0.0058747316 0.0051242225 0.0039328104 0.00245795 0.00085986569 -0.0006617005][-0.0020088046 -0.00043964293 0.0018083251 0.0044890717 0.0073927967 0.0098955277 0.011636122 0.012892939 0.013576521 0.0136791 0.012940578 0.0112565 0.0086688418 0.0054806145 0.0023042674][-0.0014931428 0.00074589555 0.0043079564 0.0088386741 0.013740091 0.018114015 0.021401932 0.023962973 0.025425026 0.025970627 0.025338547 0.023068018 0.019131213 0.013912829 0.008393663][0.00021982705 0.0043901028 0.010715761 0.018833105 0.02790731 0.03630491 0.042888544 0.047770415 0.050376751 0.050787624 0.04844144 0.043455247 0.036002461 0.026598955 0.017029261][0.0033593143 0.010498002 0.021210907 0.034510497 0.048876364 0.062209897 0.072572663 0.079602979 0.082563773 0.082137085 0.077789836 0.06946297 0.057565946 0.043180175 0.028650662][0.0078700725 0.018296741 0.033317715 0.05149433 0.07071466 0.087998658 0.10067731 0.1081773 0.11006926 0.10749438 0.10016103 0.088574775 0.073124327 0.05515641 0.037238318][0.012285195 0.02462828 0.041917104 0.062482208 0.083742768 0.10241629 0.11539029 0.12202661 0.12198748 0.11671458 0.10652202 0.092423752 0.075108565 0.056019053 0.037670575][0.014103115 0.025680358 0.041658878 0.060636796 0.08023423 0.097327106 0.10898673 0.11441129 0.11328993 0.1066763 0.095236816 0.080406711 0.063365765 0.045772448 0.029679084][0.011662488 0.020471295 0.032599833 0.047221661 0.062673949 0.076543637 0.086208239 0.090779707 0.0897698 0.08369641 0.073170207 0.059781119 0.045166295 0.030945072 0.018675029][0.0063484246 0.011675141 0.019207718 0.028595148 0.038994197 0.048811987 0.056026533 0.05955752 0.058913346 0.054318629 0.046216816 0.036048923 0.025525367 0.015987741 0.008256821][0.0012203928 0.0037442667 0.0074763121 0.012418 0.018287726 0.024159353 0.028693004 0.031027297 0.03071695 0.02783343 0.022761377 0.016547889 0.010462184 0.0053184573 0.001451615][-0.0020841088 -0.0011678902 0.00026668492 0.0022977532 0.0048588477 0.0075350758 0.00968818 0.010809928 0.010628209 0.0091617079 0.0066937795 0.00381469 0.0011982073 -0.00081412937 -0.0021594632][-0.0032672507 -0.0030932925 -0.0027559984 -0.0022062452 -0.00145585 -0.00064180093 1.8160092e-05 0.00035925955 0.00029772753 -0.00014793221 -0.00088705937 -0.0017199547 -0.0024270939 -0.0029134622 -0.0031869228][-0.0033835983 -0.003373682 -0.0033386599 -0.0032632342 -0.0031494636 -0.0030266135 -0.0029394829 -0.00290756 -0.0029370368 -0.0030186255 -0.0031363657 -0.003252354 -0.0033269324 -0.0033576542 -0.0033617218][-0.0033825755 -0.0033816092 -0.003380897 -0.0033787815 -0.0033736918 -0.0033676263 -0.0033640121 -0.0033628575 -0.0033641935 -0.0033671281 -0.0033721281 -0.0033767493 -0.0033786206 -0.0033776783 -0.003374669]]...]
INFO - root - 2017-12-10 01:19:43.901357: step 77310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 59h:34m:22s remains)
INFO - root - 2017-12-10 01:19:52.694217: step 77320, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:46m:47s remains)
INFO - root - 2017-12-10 01:20:01.214194: step 77330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:27m:07s remains)
INFO - root - 2017-12-10 01:20:09.861966: step 77340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 61h:44m:14s remains)
INFO - root - 2017-12-10 01:20:18.306853: step 77350, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 58h:18m:08s remains)
INFO - root - 2017-12-10 01:20:26.840489: step 77360, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 62h:01m:39s remains)
INFO - root - 2017-12-10 01:20:35.191653: step 77370, loss = 0.89, batch loss = 0.68 (11.6 examples/sec; 0.693 sec/batch; 49h:04m:40s remains)
INFO - root - 2017-12-10 01:20:43.666399: step 77380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:44m:42s remains)
INFO - root - 2017-12-10 01:20:52.231787: step 77390, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 60h:22m:06s remains)
INFO - root - 2017-12-10 01:21:00.802065: step 77400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:15m:05s remains)
2017-12-10 01:21:01.734584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033918056 -0.0033850432 -0.0033799345 -0.0033764418 -0.0033742194 -0.0033731968 -0.0033727761 -0.0033727288 -0.0033726355 -0.0033722946 -0.0033722688 -0.0033726559 -0.0033731915 -0.0033737039 -0.0033738185][-0.0033914889 -0.0033846109 -0.0033793612 -0.0033757193 -0.0033732876 -0.0033719477 -0.0033713479 -0.0033712951 -0.0033712161 -0.0033709519 -0.0033710173 -0.0033715572 -0.003372201 -0.0033725195 -0.0033724429][-0.0033901667 -0.0033838691 -0.0033794933 -0.0033766038 -0.0033745335 -0.0033732792 -0.0033726585 -0.0033725461 -0.0033722469 -0.0033717761 -0.0033716939 -0.0033720664 -0.0033725856 -0.0033726511 -0.0033723752][-0.0033872873 -0.0033821047 -0.0033793007 -0.0033777673 -0.0033766076 -0.0033758571 -0.0033754753 -0.0033753195 -0.0033745661 -0.0033736541 -0.0033730117 -0.0033727936 -0.0033728261 -0.0033725256 -0.0033720424][-0.0033840411 -0.003380175 -0.0033789549 -0.0033788264 -0.0033789431 -0.0033791244 -0.0033793049 -0.0033791899 -0.0033779659 -0.0033764192 -0.0033749177 -0.0033738308 -0.0033731365 -0.0033723649 -0.0033717041][-0.0033808982 -0.0033781182 -0.0033783438 -0.0033797319 -0.0033814155 -0.0033829473 -0.0033839433 -0.0033840048 -0.0033824618 -0.0033801356 -0.0033774346 -0.0033751612 -0.0033736108 -0.0033721768 -0.0033712711][-0.0033781063 -0.0033760692 -0.003377368 -0.0033800788 -0.0033832728 -0.0033862151 -0.0033881783 -0.0033887031 -0.0033869985 -0.0033840868 -0.0033803661 -0.0033770865 -0.0033745191 -0.0033724238 -0.0033712585][-0.0033764183 -0.0033747207 -0.0033765547 -0.003380032 -0.003384365 -0.0033884428 -0.0033910691 -0.003391851 -0.003390227 -0.0033871657 -0.003382979 -0.0033789186 -0.0033754741 -0.0033728981 -0.0033715547][-0.0033755605 -0.0033739517 -0.0033759309 -0.0033794502 -0.0033839995 -0.0033884379 -0.0033914626 -0.0033924384 -0.0033912321 -0.0033885231 -0.0033845687 -0.0033805158 -0.0033771326 -0.0033746988 -0.0033734525][-0.0033750266 -0.0033733477 -0.0033751545 -0.0033782492 -0.0033823745 -0.0033863725 -0.0033890451 -0.00339007 -0.0033894328 -0.0033875238 -0.0033846206 -0.0033817254 -0.0033792886 -0.0033775582 -0.0033764695][-0.0033747277 -0.0033728862 -0.0033742199 -0.0033765284 -0.0033797859 -0.0033829038 -0.003384948 -0.0033859236 -0.0033861306 -0.0033855834 -0.0033843555 -0.0033832416 -0.0033822595 -0.0033815117 -0.0033808234][-0.0033747973 -0.0033727842 -0.0033736622 -0.0033750795 -0.0033772739 -0.0033793447 -0.0033808164 -0.0033816695 -0.0033824048 -0.0033831038 -0.0033835478 -0.0033842654 -0.0033848139 -0.003385267 -0.0033851683][-0.0033753235 -0.0033728755 -0.0033734413 -0.003374208 -0.0033754515 -0.0033765875 -0.0033773454 -0.003377842 -0.003378642 -0.0033799941 -0.0033815454 -0.003383778 -0.0033859557 -0.0033877494 -0.0033883769][-0.0033756457 -0.0033730145 -0.0033732816 -0.0033735777 -0.0033741209 -0.0033745163 -0.0033747151 -0.0033750152 -0.0033758935 -0.0033774457 -0.0033794986 -0.0033825776 -0.0033857238 -0.003388247 -0.0033893327][-0.0033761156 -0.0033732902 -0.0033733216 -0.0033733582 -0.00337349 -0.0033735156 -0.0033734858 -0.0033736862 -0.0033744476 -0.00337588 -0.003377897 -0.0033809736 -0.0033840965 -0.0033863932 -0.0033874845]]...]
INFO - root - 2017-12-10 01:21:10.181607: step 77410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:12m:06s remains)
INFO - root - 2017-12-10 01:21:18.831300: step 77420, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 64h:10m:12s remains)
INFO - root - 2017-12-10 01:21:27.387855: step 77430, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:28m:10s remains)
INFO - root - 2017-12-10 01:21:36.041562: step 77440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:17m:02s remains)
INFO - root - 2017-12-10 01:21:44.489399: step 77450, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:54m:29s remains)
INFO - root - 2017-12-10 01:21:52.883405: step 77460, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 60h:24m:52s remains)
INFO - root - 2017-12-10 01:22:01.462526: step 77470, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:40m:05s remains)
INFO - root - 2017-12-10 01:22:09.998743: step 77480, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:02m:25s remains)
INFO - root - 2017-12-10 01:22:18.597890: step 77490, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:36m:54s remains)
INFO - root - 2017-12-10 01:22:27.192149: step 77500, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:32m:47s remains)
2017-12-10 01:22:28.120419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0032712354 -0.002921914 -0.0025497768 -0.0021518529 -0.001724261 -0.0013271333 -0.001152402 -0.0014515747 -0.0018227969 -0.0022424581 -0.0026936922 -0.0031056334 -0.0033252663 -0.0033653236 -0.0033691511][-0.003039747 -0.0025237398 -0.0019365468 -0.0012969838 -0.00064466847 -0.00017291377 -1.2560282e-05 -0.00043730764 -0.00097458065 -0.0016181353 -0.0023213262 -0.002936339 -0.0032779486 -0.0033614307 -0.0033688047][-0.0023583295 -0.0017498926 -0.00097773527 8.76328e-06 0.0011839166 0.0021571892 0.0027016208 0.0024287847 0.0017253116 0.00057648891 -0.00086624688 -0.002171238 -0.0030214346 -0.003322592 -0.0033680103][0.002462521 0.0034038338 0.004723927 0.0064952681 0.0086316466 0.010352584 0.011178922 0.010460186 0.008625106 0.0058978964 0.002609004 -0.0003615499 -0.0024143266 -0.0032201088 -0.003362505][0.014676096 0.016144667 0.018039946 0.020485971 0.023227395 0.024983052 0.025019458 0.022600584 0.018289736 0.012853645 0.0069618616 0.001897997 -0.0015859606 -0.0030797962 -0.0033526795][0.034060888 0.035837028 0.037567981 0.039556105 0.041413434 0.041580565 0.039250668 0.033850733 0.026280217 0.017952846 0.0097948033 0.0032114529 -0.0011411312 -0.0030058706 -0.0033429095][0.054564137 0.055514716 0.055533491 0.05535708 0.054670636 0.052024774 0.04677302 0.038641881 0.028756255 0.018884277 0.0099623417 0.0031558641 -0.001179877 -0.0030081689 -0.0033362519][0.068527542 0.067382663 0.064383633 0.061097234 0.057385709 0.052140683 0.044883482 0.035581846 0.025316611 0.015773706 0.007721548 0.001910731 -0.0016157539 -0.0030720537 -0.0033300952][0.0709934 0.0669775 0.060954668 0.055073608 0.049278181 0.042760123 0.035179637 0.026624652 0.017919485 0.010325944 0.0043021766 0.00017727888 -0.0022089747 -0.00316777 -0.003334068][0.061468564 0.05518619 0.047484227 0.040614888 0.034412205 0.028281437 0.021959692 0.015547542 0.009520323 0.0045813741 0.00090540922 -0.0014650642 -0.0027614571 -0.0032594507 -0.0033427046][0.04414922 0.037244 0.029776441 0.023647761 0.018532839 0.013984583 0.0097764069 0.0059318263 0.0026409335 0.00014051609 -0.0015837052 -0.0026160085 -0.0031389138 -0.0033240393 -0.0033524034][0.02560617 0.01961752 0.013849713 0.0095412591 0.0062864041 0.0037227811 0.0016233462 -7.9709105e-05 -0.0013880474 -0.0022936279 -0.0028605859 -0.0031689019 -0.0033105887 -0.0033558337 -0.0033617171][0.011067774 0.0069502862 0.0034380595 0.0011270146 -0.0003910407 -0.0014157251 -0.0021389471 -0.0026449305 -0.0029835792 -0.0031883409 -0.0032977767 -0.0033464665 -0.0033643162 -0.003367855 -0.003367462][0.0023312478 0.00014774292 -0.0014603981 -0.0023436 -0.0028184266 -0.0030746872 -0.0032178483 -0.0032970826 -0.0033421377 -0.0033650731 -0.0033739805 -0.0033748709 -0.0033743782 -0.0033728923 -0.0033722192][-0.0016518554 -0.0024920215 -0.0030042888 -0.0032192185 -0.0033034394 -0.0033356731 -0.0033501068 -0.0033583203 -0.0033675237 -0.0033742764 -0.0033768357 -0.0033761493 -0.0033745633 -0.0033730459 -0.0033727759]]...]
INFO - root - 2017-12-10 01:22:36.679276: step 77510, loss = 0.90, batch loss = 0.69 (8.6 examples/sec; 0.932 sec/batch; 66h:02m:10s remains)
INFO - root - 2017-12-10 01:22:45.355397: step 77520, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 59h:35m:22s remains)
INFO - root - 2017-12-10 01:22:53.882627: step 77530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:53m:14s remains)
INFO - root - 2017-12-10 01:23:02.591640: step 77540, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 64h:53m:06s remains)
INFO - root - 2017-12-10 01:23:11.272478: step 77550, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:25m:34s remains)
INFO - root - 2017-12-10 01:23:20.093329: step 77560, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:56m:07s remains)
INFO - root - 2017-12-10 01:23:28.862200: step 77570, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 60h:42m:33s remains)
INFO - root - 2017-12-10 01:23:37.572263: step 77580, loss = 0.89, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 64h:06m:25s remains)
INFO - root - 2017-12-10 01:23:46.122114: step 77590, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 61h:14m:28s remains)
INFO - root - 2017-12-10 01:23:54.990433: step 77600, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:01m:20s remains)
2017-12-10 01:23:55.869381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034207392 -0.0034204905 -0.0034203758 -0.0034199855 -0.0034191636 -0.0034175294 -0.0034156668 -0.0034137876 -0.0034122921 -0.0034113994 -0.0034113196 -0.003411467 -0.00341205 -0.0034127713 -0.0034136558][-0.0034172728 -0.0034172465 -0.0034173443 -0.0034169983 -0.0034160314 -0.00341449 -0.003412859 -0.0034111077 -0.0034096933 -0.0034087738 -0.0034084595 -0.0034084979 -0.0034091184 -0.0034101252 -0.0034116465][-0.0034131953 -0.0034133694 -0.0034139566 -0.0034138258 -0.0034128048 -0.0034114739 -0.0034101908 -0.0034085999 -0.0034068634 -0.0034057349 -0.0034051891 -0.0034048506 -0.003405055 -0.0034060164 -0.0034078467][-0.0034094378 -0.0034099119 -0.0034110304 -0.0034112062 -0.0034104893 -0.0034096327 -0.0034084348 -0.0034062781 -0.0034039165 -0.0034022247 -0.0034010769 -0.0034005446 -0.0034007158 -0.0034014904 -0.0034034583][-0.0034065004 -0.00340755 -0.003409262 -0.0034100097 -0.0034101119 -0.0034098669 -0.0034083738 -0.0034052464 -0.0034020143 -0.003399367 -0.0033976254 -0.0033967525 -0.0033969348 -0.0033977856 -0.0033997998][-0.0034047943 -0.0034063312 -0.0034086388 -0.0034103114 -0.0034115114 -0.0034113654 -0.0034090017 -0.0034049086 -0.0034006627 -0.0033970405 -0.0033945818 -0.0033932508 -0.0033931748 -0.0033939858 -0.0033959122][-0.0034041088 -0.0034060194 -0.0034089794 -0.0034115878 -0.0034133478 -0.0034129298 -0.0034097149 -0.003404676 -0.0033993842 -0.003394925 -0.0033922254 -0.0033907031 -0.0033903646 -0.00339088 -0.003392607][-0.0034042737 -0.0034065514 -0.0034099598 -0.003413276 -0.0034151585 -0.0034143985 -0.00341079 -0.0034048362 -0.0033987784 -0.0033940317 -0.003391227 -0.0033897171 -0.0033892447 -0.0033893262 -0.0033905106][-0.0034053705 -0.0034077291 -0.0034112216 -0.0034145399 -0.0034163853 -0.0034155129 -0.003411775 -0.0034057116 -0.0033994943 -0.0033948508 -0.0033923688 -0.0033908261 -0.0033898226 -0.0033891078 -0.0033894056][-0.0034072855 -0.003409375 -0.003412446 -0.0034152514 -0.00341655 -0.0034152081 -0.0034113806 -0.0034060921 -0.0034007598 -0.0033970312 -0.0033950605 -0.0033937348 -0.0033924684 -0.0033912463 -0.0033908235][-0.0034099098 -0.00341168 -0.0034140323 -0.00341608 -0.0034165143 -0.0034142795 -0.0034105915 -0.003406523 -0.0034026485 -0.0033999747 -0.0033981735 -0.0033968203 -0.0033955814 -0.0033940405 -0.0033935378][-0.0034124271 -0.0034135687 -0.0034149815 -0.0034160938 -0.0034155888 -0.0034127505 -0.0034094667 -0.0034066841 -0.0034042571 -0.0034021372 -0.0034000808 -0.003398316 -0.0033966119 -0.0033947793 -0.0033943078][-0.0034148064 -0.003415005 -0.0034152244 -0.0034153466 -0.0034142211 -0.0034113687 -0.003408724 -0.0034067843 -0.0034049517 -0.0034028632 -0.0034001183 -0.0033975893 -0.0033948708 -0.0033923269 -0.0033915304][-0.0034158342 -0.0034149524 -0.0034140071 -0.0034130169 -0.0034114716 -0.0034090641 -0.0034067596 -0.0034050345 -0.0034029747 -0.0034003921 -0.0033969956 -0.0033934526 -0.0033899471 -0.0033870628 -0.0033859676][-0.0034134365 -0.0034113636 -0.0034092788 -0.0034074916 -0.0034056257 -0.0034035295 -0.0034014701 -0.0033995179 -0.0033968375 -0.0033933872 -0.0033889355 -0.003384304 -0.0033806036 -0.0033784262 -0.0033778632]]...]
INFO - root - 2017-12-10 01:24:04.349502: step 77610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:49m:24s remains)
INFO - root - 2017-12-10 01:24:13.090885: step 77620, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:35m:20s remains)
INFO - root - 2017-12-10 01:24:21.627357: step 77630, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:53m:09s remains)
INFO - root - 2017-12-10 01:24:30.221550: step 77640, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 61h:12m:36s remains)
INFO - root - 2017-12-10 01:24:38.816244: step 77650, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 60h:49m:48s remains)
INFO - root - 2017-12-10 01:24:47.506447: step 77660, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:48m:01s remains)
INFO - root - 2017-12-10 01:24:55.927652: step 77670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:27m:08s remains)
INFO - root - 2017-12-10 01:25:04.402061: step 77680, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 62h:56m:09s remains)
INFO - root - 2017-12-10 01:25:13.044737: step 77690, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 60h:48m:02s remains)
INFO - root - 2017-12-10 01:25:21.704750: step 77700, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:01m:54s remains)
2017-12-10 01:25:22.603368: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026949571 0.020141851 0.0126439 0.0062363725 0.0015095228 -0.0010134894 -0.0019085162 -0.0018309719 -0.0015252006 -0.0011040524 -0.00076495879 -0.00075519853 -0.0011449188 -0.0016754939 -0.0021302893][0.025848707 0.019771006 0.012478242 0.0062766923 0.0016129918 -0.00099552213 -0.0020404032 -0.0020127099 -0.0017937891 -0.0016626302 -0.001540895 -0.0014476681 -0.0016702621 -0.0020043252 -0.0022785962][0.024897952 0.019266022 0.012491124 0.0067109726 0.0023251812 -0.00030384632 -0.0015140948 -0.0016314113 -0.0014944379 -0.0013322851 -0.0012607134 -0.0013704468 -0.0016331305 -0.0018304507 -0.0020003878][0.024389999 0.019327031 0.013111457 0.00793416 0.004070553 0.0016980355 0.00067969668 0.00092892605 0.0014957434 0.0018938694 0.0018406415 0.0013954418 0.00081258058 0.00032469095 -0.0002004113][0.024087245 0.019562665 0.01391222 0.0093903262 0.0062208604 0.0045733908 0.0043833787 0.0054525863 0.0068451753 0.0077965725 0.0079215569 0.0074117882 0.0062823649 0.0047826935 0.0032523961][0.022930644 0.019270698 0.014514254 0.01090933 0.0086913314 0.0079807732 0.0086016692 0.010380804 0.012460537 0.013907634 0.014131797 0.013352331 0.011624215 0.0094349412 0.0071108462][0.020425951 0.017735245 0.01380834 0.010999118 0.009585862 0.00950822 0.010556962 0.012618592 0.01489432 0.016507624 0.016923085 0.016199365 0.014363814 0.011801557 0.0091468738][0.017478647 0.01579128 0.012604317 0.010362804 0.0092822295 0.0092684962 0.009980645 0.011460319 0.013067357 0.014426842 0.014948796 0.014552291 0.013216032 0.011109284 0.0088151535][0.014584873 0.013807829 0.0112313 0.009262546 0.0081514167 0.0076919012 0.0076705879 0.0082608713 0.0090387771 0.0097738588 0.010046604 0.009763686 0.0089071821 0.0075447662 0.0061136903][0.012518382 0.012259414 0.0099331075 0.0078427419 0.00621524 0.0050549172 0.0043124557 0.0042084455 0.00443352 0.0047022328 0.0046836426 0.0043803519 0.0037927835 0.0030267045 0.0023517252][0.011429746 0.011401955 0.0090272166 0.0064268969 0.0040457426 0.0022474283 0.0010002416 0.00052702217 0.00054562907 0.00064689363 0.0005505702 0.0002430859 -0.00016810908 -0.00057245721 -0.00083746156][0.010642885 0.010519516 0.0078514228 0.0047899913 0.001955705 -0.00010542222 -0.0013900953 -0.0019172046 -0.0019710853 -0.0019402263 -0.0020008073 -0.0021505179 -0.0023220717 -0.0024796273 -0.0025896481][0.0096899541 0.0092045339 0.006258267 0.0028916269 -2.0042062e-06 -0.0017932315 -0.0026696092 -0.0029917154 -0.0030298068 -0.0030218954 -0.0030390283 -0.0030891928 -0.0031391762 -0.0031846224 -0.0032234115][0.0075072506 0.0068548536 0.0040468462 0.00089376396 -0.0015362017 -0.0027677417 -0.0031873416 -0.0033191405 -0.0033354652 -0.0033378711 -0.0033409121 -0.0033486392 -0.0033530148 -0.0033584256 -0.0033698413][0.0046282923 0.0039061515 0.0015670012 -0.0008813648 -0.0025504844 -0.0032057152 -0.0033464481 -0.0033807196 -0.0033833226 -0.003384765 -0.0033853105 -0.0033855885 -0.0033846376 -0.0033851289 -0.003386341]]...]
INFO - root - 2017-12-10 01:25:31.208652: step 77710, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 62h:39m:24s remains)
INFO - root - 2017-12-10 01:25:40.046940: step 77720, loss = 0.89, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 63h:43m:41s remains)
INFO - root - 2017-12-10 01:25:48.460352: step 77730, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:51m:57s remains)
INFO - root - 2017-12-10 01:25:57.127785: step 77740, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 64h:16m:24s remains)
INFO - root - 2017-12-10 01:26:05.939431: step 77750, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:50m:01s remains)
INFO - root - 2017-12-10 01:26:14.783076: step 77760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:41m:24s remains)
INFO - root - 2017-12-10 01:26:23.676258: step 77770, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 64h:23m:03s remains)
INFO - root - 2017-12-10 01:26:32.239347: step 77780, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 62h:55m:46s remains)
INFO - root - 2017-12-10 01:26:41.011868: step 77790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:34m:32s remains)
INFO - root - 2017-12-10 01:26:49.530631: step 77800, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 59h:52m:36s remains)
2017-12-10 01:26:50.434398: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024377862 0.023185771 0.022847043 0.024724422 0.028099688 0.030684637 0.031166343 0.028983323 0.025369613 0.020903898 0.016243096 0.011085023 0.0061748931 0.0021803027 -0.00032575056][0.041448448 0.039965943 0.039489571 0.042139534 0.046959262 0.050930165 0.0520206 0.049445212 0.044460773 0.037481576 0.029584771 0.020657768 0.01216148 0.0050701536 0.00045539043][0.062067062 0.061059795 0.061227933 0.065164775 0.071822554 0.077854306 0.080927975 0.0797521 0.074807405 0.0660939 0.054962806 0.040949572 0.026395746 0.01331443 0.0041578161][0.085160241 0.086049236 0.088353649 0.09542726 0.10553215 0.11471872 0.12049069 0.12114365 0.11658034 0.10586252 0.090332322 0.069825694 0.047797021 0.027114443 0.01153836][0.1051457 0.11033464 0.11762831 0.13055794 0.14694218 0.16267148 0.17459774 0.17979185 0.17688134 0.16399682 0.14213365 0.11205388 0.078896426 0.047221582 0.022347959][0.11527167 0.12564927 0.13940826 0.16031569 0.18537642 0.20980534 0.22996917 0.24191807 0.24282134 0.22949272 0.20206176 0.16269849 0.11781575 0.073787764 0.037583213][0.11008982 0.12620485 0.1464943 0.1750312 0.20910671 0.24324901 0.27267981 0.2918933 0.2972357 0.28502309 0.254632 0.20894159 0.15476781 0.10026001 0.053675193][0.091945514 0.11063869 0.13582833 0.17083132 0.21284002 0.25585735 0.29406139 0.32018495 0.32966307 0.31897455 0.28771186 0.2392495 0.18032843 0.11985757 0.066412367][0.068321966 0.086596861 0.11292367 0.15085177 0.19800352 0.24787471 0.29279912 0.32396463 0.336401 0.32742286 0.2967875 0.24848756 0.18876122 0.12683816 0.071513027][0.043418624 0.059988022 0.085309938 0.12229191 0.16989487 0.22205448 0.27022642 0.30420253 0.31810474 0.31079254 0.282322 0.23698418 0.18014248 0.12090757 0.067830391][0.024039457 0.036800738 0.058486868 0.091283143 0.13483803 0.18363087 0.22967687 0.26296049 0.27728039 0.27170116 0.24672559 0.20706904 0.15675996 0.10409953 0.057128191][0.012483373 0.021397574 0.03756864 0.06306462 0.0984273 0.13939911 0.17894915 0.20752648 0.21966355 0.21507761 0.19458759 0.16223371 0.12131074 0.078866974 0.041537434][0.006628491 0.012369338 0.02330629 0.041129433 0.066131577 0.095986418 0.12558293 0.14714931 0.15582106 0.15108007 0.13476005 0.11044553 0.080592677 0.050335109 0.024562202][0.0019048697 0.005842641 0.012761509 0.02387437 0.039609931 0.058697008 0.077608064 0.091335937 0.096425757 0.092070676 0.079896681 0.062893845 0.043478705 0.025021708 0.010255468][-0.0014119374 0.00052846665 0.0044348724 0.010828132 0.019513881 0.02987398 0.040136121 0.047376003 0.049525362 0.046058193 0.038346548 0.028372569 0.017609563 0.0082362853 0.0014969439]]...]
INFO - root - 2017-12-10 01:26:58.936149: step 77810, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:47m:03s remains)
INFO - root - 2017-12-10 01:27:07.532686: step 77820, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:03m:49s remains)
INFO - root - 2017-12-10 01:27:16.074286: step 77830, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 63h:22m:24s remains)
INFO - root - 2017-12-10 01:27:24.866950: step 77840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:58m:07s remains)
INFO - root - 2017-12-10 01:27:33.795070: step 77850, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 63h:04m:12s remains)
INFO - root - 2017-12-10 01:27:42.581155: step 77860, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 63h:30m:57s remains)
INFO - root - 2017-12-10 01:27:51.220473: step 77870, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:22m:25s remains)
INFO - root - 2017-12-10 01:27:59.816925: step 77880, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:13m:34s remains)
INFO - root - 2017-12-10 01:28:08.604641: step 77890, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:30m:09s remains)
INFO - root - 2017-12-10 01:28:17.257821: step 77900, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 60h:07m:20s remains)
2017-12-10 01:28:18.136390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033820882 -0.0033796504 -0.0033794802 -0.0033794933 -0.0033795338 -0.0033796546 -0.0033798311 -0.0033802669 -0.003381033 -0.0033820218 -0.0033836153 -0.0033856363 -0.0033878658 -0.0033898673 -0.0033910838][-0.003380531 -0.0033779286 -0.0033779328 -0.0033782148 -0.0033784611 -0.0033787587 -0.0033790537 -0.0033794648 -0.0033801496 -0.0033813256 -0.0033832395 -0.0033858009 -0.0033887213 -0.0033915383 -0.003393603][-0.0033808153 -0.0033784856 -0.0033789251 -0.0033797598 -0.0033804958 -0.0033812374 -0.0033819326 -0.0033823366 -0.0033828563 -0.0033838288 -0.003385491 -0.0033878288 -0.0033906507 -0.0033935162 -0.0033956678][-0.0033811433 -0.0033790325 -0.0033798502 -0.0033812569 -0.0033825564 -0.0033839406 -0.0033852342 -0.0033859569 -0.0033864172 -0.0033871674 -0.0033883592 -0.0033900023 -0.0033921672 -0.0033943844 -0.0033960859][-0.0033813943 -0.0033794546 -0.0033807182 -0.0033828572 -0.0033851576 -0.0033876868 -0.0033899855 -0.0033913574 -0.0033918815 -0.0033920114 -0.0033922386 -0.0033927006 -0.0033936149 -0.0033946822 -0.0033954787][-0.0033816714 -0.0033798716 -0.0033814609 -0.003384047 -0.0033871082 -0.0033905446 -0.0033937735 -0.0033958575 -0.0033968515 -0.0033968575 -0.0033961136 -0.003395274 -0.0033948733 -0.0033948985 -0.0033949788][-0.0033817941 -0.0033801743 -0.0033821319 -0.0033850195 -0.0033884947 -0.0033923557 -0.0033959264 -0.0033982766 -0.0033996229 -0.0033995577 -0.0033982391 -0.0033962955 -0.0033946214 -0.0033935453 -0.0033927616][-0.0033817613 -0.0033802078 -0.0033823517 -0.0033854779 -0.0033891615 -0.0033931478 -0.0033966706 -0.0033990303 -0.0034004746 -0.0034004375 -0.0033986354 -0.0033958489 -0.0033931953 -0.0033911308 -0.003389443][-0.0033816351 -0.0033800469 -0.0033821363 -0.0033852477 -0.003388931 -0.0033927981 -0.0033960089 -0.0033981877 -0.0033994648 -0.0033990461 -0.0033966491 -0.003393596 -0.0033907604 -0.0033883739 -0.0033861876][-0.0033813869 -0.0033797191 -0.0033817152 -0.0033844612 -0.0033878291 -0.0033914314 -0.0033945695 -0.0033966957 -0.0033977116 -0.0033970077 -0.0033944428 -0.003391369 -0.0033884915 -0.0033859322 -0.0033835128][-0.0033811042 -0.0033791638 -0.0033808385 -0.0033830393 -0.0033858411 -0.0033889064 -0.0033916121 -0.0033935516 -0.0033945669 -0.0033942147 -0.0033921755 -0.0033895564 -0.003387006 -0.0033847138 -0.003382754][-0.0033808553 -0.0033786935 -0.003379975 -0.0033816635 -0.0033837648 -0.0033860861 -0.003388223 -0.0033897376 -0.0033906398 -0.0033905436 -0.0033893543 -0.0033875729 -0.003385826 -0.0033843617 -0.0033833762][-0.0033809454 -0.0033784553 -0.0033793061 -0.0033804046 -0.0033818444 -0.0033834681 -0.003385036 -0.0033861522 -0.0033868318 -0.0033869676 -0.0033863462 -0.0033853673 -0.0033844297 -0.0033838423 -0.0033839801][-0.0033812972 -0.0033785147 -0.0033791298 -0.0033797713 -0.0033806069 -0.0033815389 -0.0033824421 -0.003383148 -0.0033836633 -0.003383935 -0.0033837932 -0.0033835208 -0.0033834407 -0.0033836977 -0.0033847832][-0.0033817447 -0.0033788136 -0.0033792851 -0.0033796921 -0.0033802136 -0.0033807785 -0.0033813138 -0.0033817361 -0.003382083 -0.0033823657 -0.0033824993 -0.0033826302 -0.0033830877 -0.0033839268 -0.0033854672]]...]
INFO - root - 2017-12-10 01:28:26.556092: step 77910, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:01m:32s remains)
INFO - root - 2017-12-10 01:28:35.333641: step 77920, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:39m:04s remains)
INFO - root - 2017-12-10 01:28:43.763073: step 77930, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:01m:48s remains)
INFO - root - 2017-12-10 01:28:52.511755: step 77940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:07m:00s remains)
INFO - root - 2017-12-10 01:29:01.125033: step 77950, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:54m:15s remains)
INFO - root - 2017-12-10 01:29:09.755559: step 77960, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:42m:28s remains)
INFO - root - 2017-12-10 01:29:18.205079: step 77970, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 58h:40m:52s remains)
INFO - root - 2017-12-10 01:29:26.742531: step 77980, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:31m:06s remains)
INFO - root - 2017-12-10 01:29:35.403251: step 77990, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 61h:50m:22s remains)
INFO - root - 2017-12-10 01:29:44.110232: step 78000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:18m:33s remains)
2017-12-10 01:29:45.040394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033992187 -0.0033995963 -0.0034024038 -0.0034053775 -0.0034063414 -0.0034037228 -0.003399129 -0.0033943602 -0.0033918931 -0.0033920333 -0.0033929686 -0.003394078 -0.003395295 -0.0033963062 -0.0033968815][-0.0033988904 -0.0033993789 -0.0034027153 -0.0034059624 -0.0034057952 -0.0033999216 -0.00339185 -0.0033863003 -0.003385667 -0.0033883068 -0.0033909127 -0.0033927865 -0.0033945153 -0.003395594 -0.0033962636][-0.0033994964 -0.0034001714 -0.0034036424 -0.0034063915 -0.0034037598 -0.0033933446 -0.0033791026 -0.0033729118 -0.0033765158 -0.0033839152 -0.0033894982 -0.0033927236 -0.0033949998 -0.0033963528 -0.0033969537][-0.0033999393 -0.0034010571 -0.003404483 -0.0034063475 -0.003401032 -0.0033857992 -0.0033655888 -0.0033575012 -0.0033659418 -0.0033781284 -0.0033875781 -0.0033930372 -0.0033958843 -0.0033971048 -0.00339754][-0.003400275 -0.0034017423 -0.0034051056 -0.0034066564 -0.0034001081 -0.0033828616 -0.003360721 -0.0033512993 -0.0033594256 -0.0033740595 -0.0033864703 -0.0033936042 -0.0033969663 -0.003397919 -0.0033979742][-0.0034004166 -0.0034019235 -0.003405249 -0.0034068697 -0.0034006406 -0.0033845322 -0.0033648713 -0.0033551767 -0.0033601336 -0.0033744448 -0.0033870933 -0.0033945066 -0.0033978848 -0.0033985428 -0.0033983842][-0.0034003025 -0.0034016161 -0.0034045381 -0.003406472 -0.0034016103 -0.0033886961 -0.0033730594 -0.0033638568 -0.0033662412 -0.0033782704 -0.0033898389 -0.0033961774 -0.003398669 -0.0033990364 -0.0033987209][-0.003400204 -0.003401171 -0.0034036257 -0.0034054711 -0.0034024364 -0.0033934133 -0.0033820381 -0.0033741461 -0.0033748818 -0.003383785 -0.0033928151 -0.00339786 -0.0033993497 -0.003399305 -0.0033989707][-0.0034000908 -0.0034005155 -0.003402702 -0.0034044536 -0.0034034313 -0.0033978606 -0.0033905872 -0.0033842435 -0.0033838113 -0.0033888638 -0.0033947038 -0.0033985018 -0.0033996273 -0.0033993809 -0.0033989975][-0.0033997684 -0.0033996138 -0.0034014918 -0.00340312 -0.0034030396 -0.0034001025 -0.0033959514 -0.0033917485 -0.0033906868 -0.0033930412 -0.0033961707 -0.0033986545 -0.0033993884 -0.0033992988 -0.0033990117][-0.0033993572 -0.0033987539 -0.0034002049 -0.0034013318 -0.0034012636 -0.0033998042 -0.003397976 -0.0033958147 -0.0033950903 -0.0033958901 -0.0033976042 -0.003398862 -0.0033991116 -0.003399102 -0.0033990059][-0.003399279 -0.0033984384 -0.0033993435 -0.0034000534 -0.0033999702 -0.0033993651 -0.0033987686 -0.0033977712 -0.0033971821 -0.0033976487 -0.003398454 -0.0033988818 -0.0033988676 -0.0033989944 -0.0033990317][-0.0033994154 -0.0033983721 -0.0033989414 -0.0033993113 -0.003399136 -0.0033988534 -0.0033987877 -0.0033983861 -0.0033979267 -0.0033982303 -0.0033985716 -0.0033987663 -0.0033988131 -0.0033990331 -0.0033991325][-0.0033995954 -0.0033984459 -0.0033988913 -0.0033989826 -0.0033987688 -0.00339871 -0.0033988296 -0.0033985549 -0.0033981886 -0.0033983176 -0.0033985924 -0.0033987937 -0.0033988659 -0.0033990506 -0.0033991889][-0.0033997924 -0.0033986073 -0.0033989546 -0.0033989602 -0.0033987202 -0.003398671 -0.0033987788 -0.003398648 -0.0033984843 -0.0033985996 -0.0033988173 -0.0033988652 -0.0033989258 -0.0033990263 -0.003399136]]...]
INFO - root - 2017-12-10 01:29:53.430831: step 78010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:33m:10s remains)
INFO - root - 2017-12-10 01:30:02.019821: step 78020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:10m:59s remains)
INFO - root - 2017-12-10 01:30:10.467890: step 78030, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 62h:30m:26s remains)
INFO - root - 2017-12-10 01:30:19.109324: step 78040, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:00m:40s remains)
INFO - root - 2017-12-10 01:30:27.894372: step 78050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 59h:40m:53s remains)
INFO - root - 2017-12-10 01:30:36.590542: step 78060, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 60h:34m:29s remains)
INFO - root - 2017-12-10 01:30:45.254216: step 78070, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:47m:24s remains)
INFO - root - 2017-12-10 01:30:53.800962: step 78080, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 59h:56m:41s remains)
INFO - root - 2017-12-10 01:31:02.419782: step 78090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:28m:52s remains)
INFO - root - 2017-12-10 01:31:11.103512: step 78100, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 61h:53m:15s remains)
2017-12-10 01:31:12.056020: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12558356 0.12309347 0.12041391 0.11913632 0.11866299 0.11851591 0.11857259 0.11822879 0.11847952 0.11854625 0.11840986 0.11708898 0.11417907 0.10845012 0.09978646][0.1439288 0.14283492 0.14121255 0.141043 0.14106525 0.14095587 0.13977651 0.13712989 0.13426951 0.13100541 0.12797761 0.12415974 0.11973448 0.11380393 0.1061276][0.15199898 0.1528908 0.15279217 0.15431775 0.15530582 0.15564755 0.15378277 0.14941381 0.14376234 0.13729519 0.13098067 0.12416574 0.11749104 0.11032818 0.10245454][0.15388533 0.15710644 0.15932027 0.16286212 0.16551305 0.16666768 0.16399227 0.15790306 0.14966279 0.14030713 0.13059494 0.12044564 0.1109447 0.10173485 0.092717364][0.15083776 0.15752354 0.16270928 0.16896561 0.17362246 0.1755323 0.17235988 0.16444969 0.15292373 0.13988113 0.12611476 0.11197355 0.098834209 0.087013379 0.076471768][0.14329052 0.15410006 0.16331947 0.17276075 0.17953573 0.18225054 0.17818166 0.16785681 0.152494 0.13495865 0.11631732 0.097617693 0.080863632 0.066868223 0.055490479][0.1287072 0.14447547 0.15826844 0.17131059 0.18044658 0.18399186 0.17900653 0.16622847 0.14701895 0.12491478 0.10153929 0.078935459 0.059541292 0.044410776 0.033315994][0.10750743 0.12712139 0.14484322 0.16102998 0.17240117 0.17706159 0.17175539 0.15729961 0.135157 0.10949367 0.082944088 0.058497459 0.038658909 0.024493674 0.01531611][0.082655579 0.10377396 0.12352629 0.14127994 0.15390459 0.15929852 0.15392587 0.13878055 0.11560191 0.088887326 0.061981238 0.038688488 0.021197528 0.010017057 0.0038338306][0.057985526 0.077082731 0.095852837 0.11299724 0.12528907 0.13041444 0.12520985 0.11073399 0.089008614 0.064687625 0.041285519 0.022305753 0.0092480555 0.0020297568 -0.0012094914][0.037078883 0.051566303 0.0664469 0.080354236 0.090412706 0.094525158 0.089914173 0.077486359 0.059711024 0.0406301 0.023290129 0.010337499 0.0023253539 -0.0014127023 -0.0026831396][0.021004988 0.030651869 0.04104314 0.050979484 0.058290958 0.061150461 0.057374816 0.047908291 0.035002246 0.021765411 0.010620076 0.0030566 -0.0010069197 -0.0025818795 -0.0029671963][0.0093460185 0.01500652 0.021432424 0.027740419 0.032478489 0.034147605 0.0314303 0.025090639 0.016860856 0.0088405889 0.0026417018 -0.001067288 -0.0027081168 -0.003207444 -0.0033165482][0.0021685914 0.0050288532 0.0083538853 0.011742194 0.014396835 0.015262587 0.013714386 0.010236291 0.0059095239 0.0018444455 -0.0010617629 -0.0025607464 -0.0031132991 -0.0032769784 -0.0033157445][-0.0015235792 -0.00037657074 0.00098353741 0.0023898494 0.0035884019 0.0040543703 0.0035132223 0.0022187291 0.00060366536 -0.00097178877 -0.0020654071 -0.0026154907 -0.0028686565 -0.0030089435 -0.0030628762]]...]
INFO - root - 2017-12-10 01:31:20.475085: step 78110, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 58h:18m:49s remains)
INFO - root - 2017-12-10 01:31:29.116632: step 78120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:54m:30s remains)
INFO - root - 2017-12-10 01:31:37.629776: step 78130, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:30m:47s remains)
INFO - root - 2017-12-10 01:31:46.032863: step 78140, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 58h:58m:48s remains)
INFO - root - 2017-12-10 01:31:54.728816: step 78150, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.911 sec/batch; 64h:23m:05s remains)
INFO - root - 2017-12-10 01:32:03.397152: step 78160, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 62h:22m:49s remains)
INFO - root - 2017-12-10 01:32:12.090073: step 78170, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:00m:09s remains)
INFO - root - 2017-12-10 01:32:20.789026: step 78180, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 53h:58m:28s remains)
INFO - root - 2017-12-10 01:32:29.499102: step 78190, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 61h:03m:38s remains)
INFO - root - 2017-12-10 01:32:38.065816: step 78200, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 61h:08m:19s remains)
2017-12-10 01:32:39.000675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033771105 -0.0033745575 -0.0033741528 -0.0033742248 -0.0033743714 -0.0033747235 -0.0033751565 -0.0033755503 -0.00337571 -0.0033756865 -0.0033755994 -0.0033755132 -0.003375357 -0.0033750469 -0.0033748315][-0.0033748262 -0.0033719956 -0.0033715477 -0.0033716874 -0.0033719288 -0.0033724173 -0.0033730152 -0.0033735682 -0.0033738571 -0.0033739305 -0.0033737486 -0.0033734369 -0.003372998 -0.0033724124 -0.003372011][-0.0033752241 -0.0033723179 -0.0033719046 -0.0033721065 -0.0033724701 -0.0033729533 -0.0033735808 -0.0033742914 -0.003374726 -0.0033748378 -0.0033745568 -0.0033741773 -0.00337357 -0.0033727721 -0.0033722322][-0.0033753614 -0.0033723742 -0.0033718981 -0.0033721656 -0.0033727456 -0.0033734047 -0.0033742087 -0.0033750234 -0.0033756359 -0.0033758609 -0.0033757209 -0.0033751656 -0.0033743735 -0.0033733232 -0.0033725456][-0.00337565 -0.0033726115 -0.0033721789 -0.0033724438 -0.0033731486 -0.0033739777 -0.0033749256 -0.0033757826 -0.0033764958 -0.0033768143 -0.0033766525 -0.0033759295 -0.0033750518 -0.0033738841 -0.0033729409][-0.0033760977 -0.003373222 -0.0033729621 -0.0033733689 -0.0033741933 -0.0033752834 -0.0033766245 -0.0033778003 -0.0033787382 -0.0033791014 -0.0033787787 -0.0033777049 -0.0033763289 -0.0033748189 -0.0033734832][-0.003376629 -0.0033743235 -0.0033747256 -0.0033758329 -0.0033770145 -0.0033781028 -0.0033794525 -0.003380819 -0.0033817019 -0.0033817946 -0.0033812814 -0.0033799098 -0.0033779147 -0.00337573 -0.0033740073][-0.0033773719 -0.003375977 -0.0033774381 -0.0033796306 -0.0033818253 -0.0033838132 -0.0033858188 -0.0033872682 -0.0033878982 -0.0033876614 -0.0033863655 -0.0033837662 -0.0033803317 -0.0033771717 -0.0033749007][-0.0033782225 -0.0033778748 -0.00338083 -0.0033847878 -0.0033887175 -0.0033923388 -0.0033952289 -0.0033968629 -0.0033971155 -0.0033961383 -0.0033935844 -0.0033894465 -0.0033844828 -0.0033798532 -0.0033763146][-0.0033788309 -0.0033793116 -0.0033833256 -0.00338841 -0.0033936386 -0.0033983758 -0.0034020094 -0.0034041605 -0.0034044278 -0.0034029081 -0.0033995186 -0.0033944696 -0.0033885979 -0.0033828402 -0.0033780178][-0.0033786746 -0.0033794937 -0.0033839671 -0.0033895411 -0.003395295 -0.0034003607 -0.0034040967 -0.0034062089 -0.0034063433 -0.0034044837 -0.0034008489 -0.0033958722 -0.0033901068 -0.0033841941 -0.0033790579][-0.0033779952 -0.0033784327 -0.0033823859 -0.0033872526 -0.0033922207 -0.0033965937 -0.0033997623 -0.0034015409 -0.003401624 -0.0034001004 -0.0033971765 -0.0033932431 -0.0033885047 -0.0033834632 -0.0033790704][-0.0033771645 -0.0033764564 -0.0033791203 -0.0033825717 -0.0033861352 -0.003389356 -0.0033919837 -0.0033937132 -0.0033941991 -0.00339352 -0.0033916652 -0.0033889159 -0.003385447 -0.003381728 -0.0033783154][-0.003376144 -0.0033744536 -0.003376038 -0.0033780858 -0.0033803007 -0.0033823769 -0.0033840446 -0.0033853338 -0.0033858886 -0.0033857496 -0.0033847408 -0.0033831098 -0.0033809897 -0.0033786355 -0.0033763805][-0.0033754255 -0.0033729258 -0.0033734792 -0.0033744238 -0.0033754825 -0.0033764353 -0.00337725 -0.0033779277 -0.0033783168 -0.0033783719 -0.0033780267 -0.0033773724 -0.0033763989 -0.0033753039 -0.0033742068]]...]
INFO - root - 2017-12-10 01:32:47.505878: step 78210, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 62h:32m:14s remains)
INFO - root - 2017-12-10 01:32:56.127377: step 78220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:49m:55s remains)
INFO - root - 2017-12-10 01:33:04.564006: step 78230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:10m:33s remains)
INFO - root - 2017-12-10 01:33:13.163035: step 78240, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 59h:18m:10s remains)
INFO - root - 2017-12-10 01:33:21.645309: step 78250, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 58h:50m:36s remains)
INFO - root - 2017-12-10 01:33:30.198590: step 78260, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:40m:57s remains)
INFO - root - 2017-12-10 01:33:38.880882: step 78270, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 60h:42m:21s remains)
INFO - root - 2017-12-10 01:33:47.495190: step 78280, loss = 0.91, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 61h:21m:05s remains)
INFO - root - 2017-12-10 01:33:56.014356: step 78290, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 62h:20m:33s remains)
INFO - root - 2017-12-10 01:34:04.680144: step 78300, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:40m:38s remains)
2017-12-10 01:34:05.565917: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07356102 0.093255624 0.11518618 0.13718323 0.15444152 0.16266197 0.15911007 0.14473559 0.1238576 0.10080779 0.080430433 0.065090813 0.054273769 0.046150796 0.039542988][0.0835771 0.10627908 0.13261113 0.16050318 0.18378773 0.19807471 0.19887924 0.18744701 0.16784829 0.1445719 0.12275818 0.10506717 0.091880374 0.080478914 0.07020165][0.089636691 0.11365947 0.14394425 0.17830376 0.21001516 0.23428458 0.24414743 0.23997259 0.2242883 0.2025082 0.17986421 0.15956594 0.14314885 0.12866271 0.11520626][0.087311156 0.11278164 0.14694591 0.187714 0.22870077 0.26454327 0.28655365 0.29361212 0.28663036 0.27126935 0.25190106 0.23167579 0.21266168 0.19420406 0.17589855][0.077493422 0.10404728 0.14170802 0.18886778 0.23860139 0.2859073 0.32058051 0.340287 0.34403282 0.33721438 0.32334736 0.30538639 0.28565362 0.26463017 0.24288106][0.061017133 0.08636491 0.12558992 0.17725702 0.23434912 0.29080024 0.33629853 0.36724189 0.38115662 0.3828178 0.37445948 0.36006352 0.34107855 0.31987074 0.29796875][0.045838118 0.068438485 0.10673656 0.16053316 0.22310238 0.28656432 0.34037334 0.378988 0.3986991 0.40405419 0.39662164 0.38275781 0.36352098 0.34321472 0.32400393][0.034245472 0.054673802 0.0913509 0.14493752 0.20929258 0.27574539 0.33361211 0.37558636 0.3973105 0.40152431 0.39109319 0.37473968 0.35361189 0.33358929 0.31775281][0.026379522 0.044785123 0.0787375 0.12935521 0.19076639 0.25554547 0.31176171 0.3532691 0.37352508 0.37502337 0.36144489 0.34171468 0.31843665 0.29798204 0.28535357][0.02237219 0.037211575 0.066020243 0.10971979 0.16322395 0.22031237 0.269468 0.30636424 0.32228839 0.32133135 0.30601424 0.28595486 0.26413891 0.24661934 0.23878716][0.020159582 0.030825967 0.052487232 0.086401626 0.12935883 0.175669 0.2158637 0.24529541 0.25722241 0.25382531 0.23797531 0.21915694 0.20053983 0.18833843 0.18639329][0.018189982 0.026151514 0.0413206 0.064467631 0.0948251 0.12782475 0.15697324 0.17840941 0.18677416 0.18358541 0.17080592 0.15603353 0.14277 0.13730083 0.14210546][0.015669378 0.021227291 0.030947087 0.045486197 0.0653074 0.086531445 0.10541607 0.11954518 0.12535574 0.12350421 0.11577497 0.10746463 0.10148338 0.10305515 0.11433683][0.016680298 0.020007731 0.024666671 0.031526811 0.041858368 0.053725883 0.065005235 0.073933244 0.078753978 0.079455949 0.078440733 0.0777077 0.080307983 0.089677058 0.10663321][0.025920339 0.029347485 0.031720437 0.033307254 0.036176357 0.040006496 0.044612553 0.049784962 0.054567549 0.05836333 0.06362059 0.071145177 0.0815037 0.095847018 0.11514956]]...]
INFO - root - 2017-12-10 01:34:14.036464: step 78310, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.814 sec/batch; 57h:28m:17s remains)
INFO - root - 2017-12-10 01:34:22.555772: step 78320, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 59h:03m:19s remains)
INFO - root - 2017-12-10 01:34:31.061659: step 78330, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:56m:11s remains)
INFO - root - 2017-12-10 01:34:39.802277: step 78340, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 63h:01m:57s remains)
INFO - root - 2017-12-10 01:34:48.476417: step 78350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:00m:52s remains)
INFO - root - 2017-12-10 01:34:57.109366: step 78360, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:52m:14s remains)
INFO - root - 2017-12-10 01:35:05.839903: step 78370, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:26m:09s remains)
INFO - root - 2017-12-10 01:35:14.431266: step 78380, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 60h:24m:15s remains)
INFO - root - 2017-12-10 01:35:22.966837: step 78390, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 63h:53m:42s remains)
INFO - root - 2017-12-10 01:35:31.564435: step 78400, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:48m:58s remains)
2017-12-10 01:35:32.500815: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074011154 0.0859913 0.096694566 0.10441592 0.10884058 0.11014694 0.10913049 0.10667685 0.10278803 0.097838625 0.091729507 0.085540786 0.079825334 0.07512264 0.07115531][0.083378404 0.0955604 0.10590663 0.11300317 0.11674771 0.11755311 0.11618483 0.11335327 0.10908955 0.10358084 0.096809857 0.089850768 0.083338976 0.078004405 0.073620722][0.093020722 0.104743 0.11407228 0.12010325 0.12295513 0.12321241 0.12161 0.11867817 0.11433785 0.10862355 0.10162885 0.094257213 0.087255858 0.081528276 0.076828949][0.10161354 0.11243457 0.12051756 0.1255247 0.12764546 0.12746184 0.12566581 0.12263086 0.11825819 0.11253916 0.10561442 0.098243371 0.09118484 0.085373305 0.0806205][0.10720212 0.11675605 0.12351279 0.12764958 0.12924477 0.12873866 0.1267949 0.12364101 0.11925818 0.11364964 0.10710543 0.10025556 0.093755156 0.088312224 0.083818048][0.10800035 0.11603384 0.12148686 0.12477285 0.12591743 0.12520808 0.12317966 0.11998468 0.11572931 0.11044862 0.10460328 0.098679185 0.093178026 0.088541463 0.084667154][0.10327882 0.10969384 0.11398388 0.11660945 0.11746644 0.11669602 0.11474203 0.11172461 0.10783894 0.10317937 0.098284647 0.093494035 0.089122444 0.085401334 0.082250334][0.094562188 0.099600807 0.10296716 0.10511024 0.10588036 0.1052877 0.10353517 0.10079442 0.097421356 0.093557216 0.089685589 0.086045742 0.082744114 0.079862669 0.07740318][0.083772935 0.087728836 0.090444192 0.092329741 0.093133532 0.092778578 0.091373213 0.089122236 0.086380228 0.083297566 0.080277696 0.0775267 0.074994951 0.072788909 0.07093098][0.072320335 0.075343415 0.077457942 0.0791752 0.080117218 0.08014445 0.079233654 0.07750544 0.075321332 0.07292109 0.070583634 0.068403825 0.066339992 0.064572357 0.063133016][0.060390785 0.062677182 0.064313658 0.065844312 0.066873558 0.067238353 0.066846848 0.065715037 0.064083725 0.062171109 0.060187481 0.058251288 0.056457397 0.054954592 0.053828344][0.047441572 0.049120616 0.050379567 0.051699322 0.052722685 0.053301953 0.053300343 0.052664321 0.051519677 0.050068274 0.048432007 0.046728093 0.045084782 0.043691013 0.042743471][0.033704069 0.034811176 0.03570535 0.036739048 0.037626393 0.038237363 0.038429357 0.03811321 0.037334893 0.036274344 0.034987088 0.033578947 0.032205112 0.031058779 0.030309094][0.020558504 0.021195324 0.021772427 0.022477984 0.023145838 0.023668915 0.023932733 0.023835165 0.023358317 0.022638777 0.02170499 0.020647073 0.019615477 0.018782487 0.018271428][0.0099025695 0.01021782 0.010571449 0.011001531 0.011425819 0.011779882 0.011989492 0.011998958 0.011742663 0.011310552 0.010722671 0.01003396 0.0093635311 0.0088464227 0.0085601816]]...]
INFO - root - 2017-12-10 01:35:40.881116: step 78410, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:11m:59s remains)
INFO - root - 2017-12-10 01:35:49.592259: step 78420, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:44m:26s remains)
INFO - root - 2017-12-10 01:35:58.219373: step 78430, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:53m:03s remains)
INFO - root - 2017-12-10 01:36:06.933722: step 78440, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:38m:08s remains)
INFO - root - 2017-12-10 01:36:15.521315: step 78450, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:49m:43s remains)
INFO - root - 2017-12-10 01:36:24.157542: step 78460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:27m:03s remains)
INFO - root - 2017-12-10 01:36:32.838120: step 78470, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 60h:00m:57s remains)
INFO - root - 2017-12-10 01:36:41.510770: step 78480, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:49m:33s remains)
INFO - root - 2017-12-10 01:36:50.181904: step 78490, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 61h:13m:37s remains)
INFO - root - 2017-12-10 01:36:58.791535: step 78500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:09m:47s remains)
2017-12-10 01:36:59.756570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020220778 -0.00086109806 0.00055153319 0.0020510366 0.0033523471 0.0042121103 0.0045531085 0.0044989483 0.00426191 0.0039968817 0.0037419961 0.0034023777 0.0028484364 0.0019634173 0.00079155038][-0.0013867659 0.0003460343 0.0025303129 0.004922959 0.0071262633 0.0087927654 0.0097287511 0.010071195 0.010064458 0.0099158566 0.0096578328 0.0091589754 0.0082214791 0.0066696731 0.0045897309][-0.00015752134 0.0025251477 0.0060289837 0.0099243335 0.013567533 0.016394373 0.018127594 0.018871926 0.018927628 0.018632848 0.018138008 0.017313629 0.015800802 0.013337705 0.010046659][0.0019909225 0.0061234338 0.011468664 0.017409811 0.022931566 0.027170161 0.0297758 0.030885274 0.030854383 0.030088527 0.028892336 0.027183872 0.024584558 0.020808317 0.016021151][0.0046619643 0.010315734 0.01748801 0.025292249 0.032420196 0.037800979 0.040992405 0.0422035 0.041847616 0.040446997 0.038413543 0.035728063 0.0320482 0.027028129 0.020904951][0.0069924658 0.013668103 0.022006154 0.030947803 0.039014775 0.044967625 0.048344921 0.049353261 0.048453435 0.046252508 0.043286186 0.039698508 0.035214256 0.029458622 0.02266839][0.0080535691 0.014813703 0.023264358 0.032348018 0.040602375 0.046700191 0.050102148 0.050953228 0.049625866 0.046774969 0.0430184 0.038678374 0.033664569 0.027663138 0.020900056][0.0074532777 0.013457168 0.021052424 0.029406523 0.037268497 0.04325207 0.046740171 0.047744293 0.046448421 0.043389693 0.039181359 0.034369342 0.029075706 0.02315017 0.016837457][0.0054765288 0.01015132 0.016202897 0.023113195 0.0299561 0.035453167 0.038899031 0.040163688 0.039221961 0.036411602 0.032251179 0.027443692 0.022331746 0.016934989 0.011527827][0.0027293793 0.0058405595 0.0099999178 0.014993405 0.020248313 0.024766535 0.027836291 0.029179068 0.028626719 0.026338905 0.022708138 0.018487634 0.014157033 0.0098722372 0.0058657434][-4.8978487e-05 0.0016456342 0.0040226877 0.0070422729 0.010439379 0.013547461 0.015794996 0.016848128 0.01650648 0.014846552 0.012136791 0.0090600066 0.00605762 0.0033156939 0.00097131147][-0.0020322127 -0.0013212578 -0.00026542926 0.0011944103 0.0029676326 0.0046736058 0.0059503214 0.0065662181 0.0063451054 0.0053126989 0.0036634633 0.0018850293 0.00028609321 -0.0010343357 -0.0020298297][-0.0031177234 -0.0028994114 -0.002531382 -0.0019805224 -0.0012676031 -0.00055514695 -2.2064429e-05 0.00021799048 7.4081356e-05 -0.00042723585 -0.0011767466 -0.0019275403 -0.0025272355 -0.0029477156 -0.0032028873][-0.0034046252 -0.003373327 -0.0032983262 -0.0031539516 -0.0029429574 -0.002717094 -0.0025490886 -0.0024825146 -0.0025331846 -0.0026926412 -0.0029149589 -0.0031255032 -0.0032718778 -0.0033513035 -0.0033858262][-0.0034139964 -0.0034118483 -0.0034027174 -0.0033782283 -0.003340482 -0.0032954181 -0.0032615657 -0.0032475519 -0.0032553896 -0.0032839181 -0.0033237149 -0.0033635304 -0.0033891534 -0.0034001865 -0.0034026888]]...]
INFO - root - 2017-12-10 01:37:08.263359: step 78510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:12m:52s remains)
INFO - root - 2017-12-10 01:37:16.860972: step 78520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:47m:25s remains)
INFO - root - 2017-12-10 01:37:25.353118: step 78530, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 58h:21m:38s remains)
INFO - root - 2017-12-10 01:37:33.897797: step 78540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:12m:25s remains)
INFO - root - 2017-12-10 01:37:42.472627: step 78550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:15m:55s remains)
INFO - root - 2017-12-10 01:37:51.086624: step 78560, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:29m:46s remains)
INFO - root - 2017-12-10 01:37:59.758592: step 78570, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:14m:29s remains)
INFO - root - 2017-12-10 01:38:08.416907: step 78580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 62h:53m:12s remains)
INFO - root - 2017-12-10 01:38:16.968434: step 78590, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 61h:00m:10s remains)
INFO - root - 2017-12-10 01:38:25.608389: step 78600, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:33m:11s remains)
2017-12-10 01:38:26.446305: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076167628 0.077420287 0.07802736 0.077668272 0.076773249 0.074787259 0.07167612 0.069001615 0.066348374 0.064083524 0.061500348 0.059964955 0.059203818 0.058191244 0.057794977][0.084988132 0.086070649 0.086358361 0.086031042 0.0853753 0.083835587 0.081583343 0.079711057 0.078062043 0.07658311 0.074957639 0.074123904 0.0733862 0.072254576 0.071679048][0.089040734 0.089400358 0.088846229 0.08830858 0.087716214 0.086829208 0.085947014 0.0855244 0.08576262 0.086046129 0.086302228 0.086583905 0.08606717 0.084742993 0.083461881][0.089506619 0.088936724 0.087479137 0.086307436 0.08523795 0.084603466 0.084500946 0.085603848 0.087529771 0.089725576 0.091875151 0.093469739 0.093905687 0.092867129 0.091606222][0.087835811 0.086484134 0.083947867 0.0816721 0.079908982 0.078868486 0.078951985 0.080635712 0.083993517 0.08774446 0.0916792 0.094805971 0.096673794 0.096930921 0.096137665][0.083403371 0.081581861 0.077911332 0.074839808 0.072303593 0.070712432 0.070524216 0.072467312 0.076669462 0.081448138 0.0866497 0.091288954 0.094803758 0.096550062 0.096852079][0.076947056 0.07459721 0.070138805 0.066365905 0.06311851 0.061192334 0.061146766 0.063226 0.067546353 0.073070556 0.0791021 0.085174508 0.090048246 0.0932892 0.094781987][0.069555327 0.066806257 0.061872676 0.058018796 0.05490585 0.052997854 0.053084783 0.055305816 0.059611596 0.064605631 0.070639312 0.077133194 0.082981572 0.087470129 0.090499543][0.061473221 0.05900991 0.054685712 0.05112163 0.048320558 0.046958093 0.047427624 0.049664289 0.05339073 0.057785105 0.063249156 0.069552943 0.07544709 0.080645643 0.084507637][0.053486388 0.051706709 0.048005857 0.045282427 0.043363836 0.042548358 0.043069303 0.045270786 0.048608959 0.052264005 0.056700788 0.061872542 0.067222372 0.072096355 0.075948067][0.045349184 0.044581067 0.042020295 0.040258102 0.039180428 0.038924295 0.039585415 0.041374825 0.043976456 0.046761692 0.050009292 0.053862449 0.057914026 0.061637223 0.065027416][0.037557889 0.037573382 0.036293719 0.035695642 0.03566055 0.036106762 0.036989231 0.038263 0.039778575 0.041423615 0.043198325 0.045453653 0.047926143 0.050530024 0.053177238][0.030393662 0.031291265 0.031353887 0.031975139 0.033117171 0.0342429 0.035197996 0.035737913 0.036021076 0.036107484 0.036283202 0.037020929 0.03818981 0.039853923 0.042008854][0.023787126 0.025313307 0.026464423 0.028280331 0.03036868 0.0322637 0.033443492 0.033627275 0.033091731 0.032124538 0.03123709 0.030774856 0.030779468 0.031398237 0.032731071][0.017320232 0.018971294 0.020736063 0.023270812 0.026116187 0.02853887 0.029938748 0.030105727 0.029243296 0.027750889 0.026158029 0.024760241 0.023808755 0.023331689 0.023642961]]...]
INFO - root - 2017-12-10 01:38:35.016866: step 78610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:38m:50s remains)
INFO - root - 2017-12-10 01:38:43.691475: step 78620, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:44m:08s remains)
INFO - root - 2017-12-10 01:38:52.209212: step 78630, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:15m:39s remains)
INFO - root - 2017-12-10 01:39:00.823969: step 78640, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 60h:43m:00s remains)
INFO - root - 2017-12-10 01:39:09.369476: step 78650, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:58m:21s remains)
INFO - root - 2017-12-10 01:39:17.982355: step 78660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:27m:02s remains)
INFO - root - 2017-12-10 01:39:26.750076: step 78670, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 59h:32m:36s remains)
INFO - root - 2017-12-10 01:39:35.462353: step 78680, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 61h:07m:25s remains)
INFO - root - 2017-12-10 01:39:43.997073: step 78690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:28m:55s remains)
INFO - root - 2017-12-10 01:39:52.662466: step 78700, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 62h:21m:56s remains)
2017-12-10 01:39:53.618395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033655954 -0.0033630736 -0.0033639453 -0.003365244 -0.0033666135 -0.0033678452 -0.003369201 -0.0033700168 -0.0033699849 -0.0033695141 -0.0033689169 -0.0033682315 -0.0033672736 -0.003366204 -0.0033654685][-0.0033643057 -0.0033618205 -0.00336287 -0.0033646678 -0.0033667183 -0.0033687875 -0.0033704259 -0.003371198 -0.0033706652 -0.0033699654 -0.00336923 -0.0033682031 -0.00336706 -0.0033656897 -0.0033647721][-0.0033664305 -0.0033640969 -0.0033652673 -0.0033672517 -0.00336969 -0.0033721954 -0.0033739228 -0.0033744348 -0.003373659 -0.0033723575 -0.0033712338 -0.0033698701 -0.0033685502 -0.0033669532 -0.0033657339][-0.0033697211 -0.0033676215 -0.0033687695 -0.0033707747 -0.0033731852 -0.0033755796 -0.0033769603 -0.0033772432 -0.0033769128 -0.0033759084 -0.0033747437 -0.0033731074 -0.0033714809 -0.0033693176 -0.0033673071][-0.003374465 -0.0033722769 -0.0033732795 -0.0033753894 -0.0033769936 -0.0033780879 -0.0033779964 -0.0033776166 -0.0033773875 -0.0033766106 -0.0033760748 -0.003375107 -0.0033740778 -0.0033721484 -0.0033696333][-0.0033803841 -0.0033781708 -0.0033784208 -0.0033796516 -0.003379839 -0.0033792863 -0.0033775466 -0.0033757929 -0.0033752772 -0.0033751212 -0.0033757105 -0.0033756536 -0.0033753999 -0.0033740064 -0.0033714275][-0.0033856281 -0.0033834649 -0.003382941 -0.0033832821 -0.0033824486 -0.0033807359 -0.0033775102 -0.0033746473 -0.0033742674 -0.0033749095 -0.0033768285 -0.0033778779 -0.0033779922 -0.0033768709 -0.0033740902][-0.00338971 -0.0033875038 -0.0033865832 -0.0033864696 -0.0033849429 -0.00338231 -0.0033783028 -0.003374744 -0.0033741407 -0.0033752571 -0.003378344 -0.0033804935 -0.0033806381 -0.00337936 -0.0033762597][-0.0033917676 -0.0033894379 -0.0033882814 -0.0033879199 -0.0033860896 -0.0033835035 -0.0033797817 -0.0033760425 -0.0033751659 -0.0033766762 -0.003380466 -0.0033829613 -0.0033828579 -0.00338099 -0.0033771985][-0.0033910964 -0.0033888619 -0.0033874169 -0.00338696 -0.0033856854 -0.0033837226 -0.0033805622 -0.003377486 -0.0033772013 -0.0033789915 -0.0033822628 -0.0033840481 -0.0033835724 -0.0033811948 -0.0033770017][-0.0033874847 -0.0033853834 -0.0033845264 -0.0033843061 -0.0033838137 -0.0033833452 -0.0033817217 -0.0033797976 -0.0033796781 -0.0033809331 -0.0033830726 -0.0033835424 -0.0033823398 -0.0033795375 -0.0033753519][-0.0033816698 -0.003379572 -0.00337923 -0.0033796697 -0.0033807126 -0.003381805 -0.0033817759 -0.0033812036 -0.0033815647 -0.0033823375 -0.0033833126 -0.0033828092 -0.0033805566 -0.0033771722 -0.0033729027][-0.0033763221 -0.0033738341 -0.0033738129 -0.0033747763 -0.00337634 -0.0033781119 -0.0033789512 -0.0033794227 -0.0033800402 -0.0033801273 -0.0033802197 -0.0033789726 -0.0033765479 -0.0033731572 -0.0033691975][-0.0033715183 -0.0033687707 -0.0033688582 -0.0033695661 -0.0033707472 -0.0033723975 -0.0033737367 -0.0033747118 -0.0033755789 -0.0033755009 -0.0033751347 -0.0033737703 -0.0033715246 -0.0033685446 -0.0033654084][-0.003367322 -0.0033643714 -0.0033644957 -0.0033648133 -0.0033653593 -0.0033664228 -0.0033674396 -0.0033683802 -0.0033690713 -0.0033688815 -0.0033683882 -0.003367441 -0.0033659963 -0.0033640803 -0.0033621662]]...]
INFO - root - 2017-12-10 01:40:02.191705: step 78710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:36m:35s remains)
INFO - root - 2017-12-10 01:40:10.887670: step 78720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:39m:46s remains)
INFO - root - 2017-12-10 01:40:19.550704: step 78730, loss = 0.89, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 54h:41m:14s remains)
INFO - root - 2017-12-10 01:40:28.316298: step 78740, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 63h:48m:18s remains)
INFO - root - 2017-12-10 01:40:37.021985: step 78750, loss = 0.90, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 61h:42m:27s remains)
INFO - root - 2017-12-10 01:40:45.701519: step 78760, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:10m:50s remains)
INFO - root - 2017-12-10 01:40:54.589267: step 78770, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 62h:02m:51s remains)
INFO - root - 2017-12-10 01:41:03.391860: step 78780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:44m:55s remains)
INFO - root - 2017-12-10 01:41:11.896840: step 78790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:42m:11s remains)
INFO - root - 2017-12-10 01:41:20.467105: step 78800, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:27m:44s remains)
2017-12-10 01:41:21.403552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028117814 0.0038576543 0.0053949328 0.0066668689 0.007391206 0.0073094014 0.0064871209 0.0052303663 0.00415083 0.0033335548 0.0027420393 0.0022854267 0.0020865046 0.0021639883 0.0027489031][0.0065591726 0.00855432 0.011169065 0.013054239 0.013676768 0.01264726 0.010456066 0.0077174371 0.0055727307 0.0042965608 0.0035363624 0.002887984 0.0022813466 0.0017292255 0.0013449385][0.011771619 0.015470836 0.019976774 0.023281496 0.024186544 0.022111326 0.017935865 0.01284945 0.0088205766 0.00630608 0.0050004376 0.0041399896 0.003348103 0.002528378 0.0016090747][0.018596837 0.025331827 0.033253979 0.039268125 0.040899388 0.0372916 0.029955672 0.021231554 0.014123134 0.0093889739 0.0069996854 0.005448713 0.0041483892 0.0029822297 0.0017437881][0.028310062 0.03899727 0.051250439 0.060701981 0.063395508 0.058065671 0.046795022 0.033402592 0.022077832 0.014241541 0.00990441 0.0071201269 0.0049404977 0.0030713507 0.001369742][0.0386668 0.053910926 0.070737794 0.083677232 0.087531768 0.080748975 0.065628141 0.047328085 0.031100592 0.019522516 0.012797533 0.0087052053 0.0059051327 0.0036842779 0.0017426696][0.048230153 0.067690171 0.088461258 0.10438865 0.10948817 0.10187921 0.08382687 0.061324321 0.040549386 0.025160212 0.015641149 0.010006652 0.0066340547 0.0043187952 0.0024569344][0.059982561 0.081944957 0.10455443 0.12179877 0.12745795 0.11965837 0.10031794 0.075491406 0.051625371 0.03308269 0.020873625 0.013430241 0.0091663888 0.006540034 0.0046187732][0.074093856 0.09663111 0.11882244 0.13550435 0.1410369 0.13368425 0.11470268 0.089399628 0.063845471 0.042932369 0.028288463 0.019030046 0.013791932 0.010439971 0.0080975546][0.088302307 0.10969397 0.12968285 0.14447376 0.14953619 0.14346741 0.12686388 0.10372615 0.078855358 0.056947712 0.040352222 0.029135665 0.022497077 0.018190188 0.015162027][0.10050996 0.1192744 0.1359425 0.14801234 0.15214233 0.14743572 0.13417119 0.11498611 0.093202636 0.072646745 0.055957343 0.04394583 0.036457621 0.031479117 0.027886407][0.10996316 0.1249236 0.13763936 0.14692061 0.15018184 0.14694384 0.13737932 0.12285568 0.10551281 0.088192 0.073298842 0.061899014 0.054335255 0.04930798 0.045555532][0.11651585 0.12742057 0.13625322 0.14280987 0.14520332 0.14307529 0.13671689 0.12656899 0.11397395 0.10091456 0.089123443 0.079637691 0.072972924 0.068536371 0.064985596][0.11901382 0.12633798 0.13203067 0.13663003 0.1386411 0.1377022 0.13383648 0.12720494 0.11872315 0.10963643 0.10105167 0.093993478 0.088895306 0.085512079 0.082704633][0.11795983 0.12222962 0.12553775 0.12872173 0.13062629 0.13083915 0.12923197 0.12566017 0.12065855 0.11508199 0.10950406 0.10477786 0.10109758 0.098602869 0.096521594]]...]
INFO - root - 2017-12-10 01:41:29.956576: step 78810, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:01m:10s remains)
INFO - root - 2017-12-10 01:41:38.599268: step 78820, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:56m:09s remains)
INFO - root - 2017-12-10 01:41:47.248796: step 78830, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 51h:55m:44s remains)
INFO - root - 2017-12-10 01:41:55.948737: step 78840, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:11m:36s remains)
INFO - root - 2017-12-10 01:42:04.543303: step 78850, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 58h:41m:37s remains)
INFO - root - 2017-12-10 01:42:13.068281: step 78860, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:23m:59s remains)
INFO - root - 2017-12-10 01:42:21.810483: step 78870, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:58m:31s remains)
INFO - root - 2017-12-10 01:42:30.593410: step 78880, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 62h:10m:34s remains)
INFO - root - 2017-12-10 01:42:39.212355: step 78890, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 60h:20m:42s remains)
INFO - root - 2017-12-10 01:42:47.844208: step 78900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:28m:07s remains)
2017-12-10 01:42:48.807992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033962983 -0.0033957723 -0.0033964412 -0.0033974757 -0.0033982359 -0.0033988731 -0.0033997232 -0.0034006159 -0.003402154 -0.0034031752 -0.0034037917 -0.0034038825 -0.0034037617 -0.0034037416 -0.0034031582][-0.0033938645 -0.0033929856 -0.0033934696 -0.0033943625 -0.0033949842 -0.0033952352 -0.0033956207 -0.0033960119 -0.0033972519 -0.0033979593 -0.0033984904 -0.0033987234 -0.0033991078 -0.00339967 -0.00339969][-0.0033897073 -0.0033883217 -0.0033887285 -0.0033894263 -0.003389976 -0.0033899527 -0.0033900246 -0.0033901483 -0.0033908556 -0.0033913495 -0.0033919122 -0.0033923401 -0.0033930687 -0.0033939264 -0.0033944158][-0.0033854665 -0.0033836435 -0.0033836926 -0.0033843343 -0.0033849378 -0.0033849371 -0.0033851622 -0.0033851536 -0.0033853739 -0.0033854423 -0.0033857902 -0.003386209 -0.0033868498 -0.0033877937 -0.0033884025][-0.0033832202 -0.003381053 -0.0033809098 -0.0033812427 -0.0033818306 -0.0033818148 -0.0033821652 -0.003382158 -0.003381772 -0.0033812884 -0.0033812509 -0.0033816018 -0.0033821473 -0.0033828816 -0.003383304][-0.0033829359 -0.0033811617 -0.0033815091 -0.0033820707 -0.0033829026 -0.0033831764 -0.0033834653 -0.0033830784 -0.0033818246 -0.0033805103 -0.0033798064 -0.0033797652 -0.0033800602 -0.0033805619 -0.0033808111][-0.0033842351 -0.0033829422 -0.0033838851 -0.0033849988 -0.0033862167 -0.0033864931 -0.0033863543 -0.0033852616 -0.0033830502 -0.0033808302 -0.0033796243 -0.0033792655 -0.0033794744 -0.0033798052 -0.0033798609][-0.0033859063 -0.0033852244 -0.0033866956 -0.003388304 -0.0033897923 -0.0033899953 -0.0033894193 -0.003387463 -0.0033845271 -0.0033817063 -0.0033800385 -0.0033794537 -0.0033795089 -0.0033797692 -0.0033798986][-0.0033865925 -0.0033863503 -0.0033882232 -0.0033902728 -0.0033918514 -0.0033918992 -0.0033910994 -0.0033886575 -0.0033855324 -0.0033825659 -0.0033807284 -0.0033798625 -0.0033797084 -0.0033798702 -0.0033800234][-0.0033860942 -0.0033859184 -0.0033879792 -0.0033901187 -0.0033915979 -0.003391396 -0.0033903401 -0.0033877548 -0.0033850092 -0.00338235 -0.0033806881 -0.0033800213 -0.0033797715 -0.003379704 -0.0033798043][-0.0033848349 -0.0033843827 -0.0033864363 -0.0033883967 -0.0033894735 -0.0033889611 -0.0033879231 -0.0033857331 -0.0033836209 -0.0033818115 -0.0033806802 -0.0033803512 -0.0033801137 -0.0033799121 -0.0033799584][-0.003383969 -0.0033829843 -0.0033846234 -0.003386091 -0.003386715 -0.0033861431 -0.0033854465 -0.0033839187 -0.0033825103 -0.0033814535 -0.0033808893 -0.0033809412 -0.003380924 -0.0033807773 -0.0033808933][-0.0033857706 -0.003384474 -0.0033854407 -0.003386271 -0.0033865196 -0.0033861417 -0.0033858262 -0.0033849168 -0.003384169 -0.0033836362 -0.0033832975 -0.0033834118 -0.0033834598 -0.0033833564 -0.0033833957][-0.0033877837 -0.0033859471 -0.0033863515 -0.0033866919 -0.0033867511 -0.0033865715 -0.0033866181 -0.003386267 -0.0033859212 -0.0033858023 -0.0033856933 -0.0033858551 -0.0033859327 -0.0033858032 -0.0033857708][-0.003388586 -0.003386412 -0.0033864505 -0.0033866253 -0.0033865822 -0.003386623 -0.0033868398 -0.0033867587 -0.0033866479 -0.0033865629 -0.0033866609 -0.0033868246 -0.0033868356 -0.0033867434 -0.00338681]]...]
INFO - root - 2017-12-10 01:42:57.276186: step 78910, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 59h:12m:57s remains)
INFO - root - 2017-12-10 01:43:05.901088: step 78920, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 62h:51m:36s remains)
INFO - root - 2017-12-10 01:43:14.705276: step 78930, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:08m:09s remains)
INFO - root - 2017-12-10 01:43:23.419606: step 78940, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 63h:10m:57s remains)
INFO - root - 2017-12-10 01:43:32.042981: step 78950, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:46m:40s remains)
INFO - root - 2017-12-10 01:43:40.827842: step 78960, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 60h:59m:53s remains)
INFO - root - 2017-12-10 01:43:49.574308: step 78970, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 60h:28m:38s remains)
INFO - root - 2017-12-10 01:43:58.312921: step 78980, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 61h:34m:25s remains)
INFO - root - 2017-12-10 01:44:06.841505: step 78990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:45m:16s remains)
INFO - root - 2017-12-10 01:44:15.623573: step 79000, loss = 0.90, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 62h:16m:11s remains)
2017-12-10 01:44:16.517723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0022676331 -0.0020973317 -0.0021158105 -0.0021432922 -0.0023158784 -0.0025995215 -0.0029410142 -0.0032223426 -0.0033640938 -0.0033866051 -0.0033894312 -0.0033887825 -0.0033883 -0.003387674 -0.0033874922][-0.001635879 -0.0014120603 -0.0014220688 -0.0014122801 -0.0016003102 -0.0020489334 -0.002651026 -0.0031003973 -0.0033274069 -0.0033797938 -0.0033883508 -0.0033877615 -0.0033871152 -0.0033860267 -0.0033856344][-0.00088717183 -0.00057046232 -0.00070142047 -0.00077670114 -0.0010159204 -0.0015312015 -0.0022761934 -0.0029030899 -0.003278726 -0.0033704119 -0.0033887189 -0.0033883436 -0.003387423 -0.0033858491 -0.0033851322][-0.00033856463 6.5100379e-05 -4.1177263e-05 -0.00014601974 -0.00048220297 -0.0010998617 -0.0019954904 -0.0027386695 -0.0032115858 -0.0033566824 -0.0033890621 -0.0033886274 -0.0033877264 -0.0033860963 -0.003385206][-0.00012692041 0.0003185533 0.00017252099 0.00010082661 -0.00020297407 -0.00085969293 -0.0018332932 -0.0026307586 -0.0031677056 -0.0033454555 -0.0033885501 -0.0033882523 -0.003387494 -0.0033862323 -0.0033854563][-0.00022445456 0.00018022722 7.9501187e-05 7.3729316e-05 -0.00026015192 -0.00091589568 -0.0018617305 -0.0026374939 -0.0031623219 -0.0033430082 -0.0033879452 -0.0033877047 -0.0033872575 -0.0033861278 -0.0033854994][-0.00060437224 -0.00025111507 -0.00031999382 -0.00026767096 -0.000556645 -0.001182053 -0.0020739804 -0.0027663102 -0.0032154236 -0.0033555836 -0.0033875636 -0.0033873473 -0.0033869534 -0.00338615 -0.0033856186][-0.0012280133 -0.00097587588 -0.00095706154 -0.00086440193 -0.0010784892 -0.0016055128 -0.0023631062 -0.0029399088 -0.0032848129 -0.0033719763 -0.0033866894 -0.0033872579 -0.0033866491 -0.0033858037 -0.0033854225][-0.0020348281 -0.0018507834 -0.0017773082 -0.0016560927 -0.0017635667 -0.0021450489 -0.0026984746 -0.0031151951 -0.00334207 -0.0033820949 -0.0033857706 -0.0033865219 -0.0033862861 -0.0033853415 -0.0033850577][-0.0027542105 -0.0026304023 -0.0025257384 -0.0024028008 -0.0024562282 -0.0026982925 -0.0030330953 -0.0032667988 -0.0033711512 -0.0033838868 -0.0033853431 -0.0033862623 -0.0033859245 -0.0033850817 -0.0033849066][-0.0031881491 -0.0031102996 -0.0030154677 -0.0029206327 -0.0029618666 -0.0031019391 -0.0032711392 -0.0033559958 -0.0033816886 -0.0033840148 -0.0033853955 -0.0033862169 -0.0033858558 -0.0033852877 -0.003384755][-0.0033655844 -0.0033327043 -0.0032678158 -0.0032148175 -0.0032371674 -0.0033035621 -0.00336528 -0.0033804828 -0.0033825976 -0.0033837571 -0.0033848048 -0.0033857967 -0.0033854421 -0.0033850216 -0.0033845913][-0.0034047216 -0.0033965111 -0.0033758406 -0.0033534134 -0.003353589 -0.0033691279 -0.0033844614 -0.0033830549 -0.0033827869 -0.0033838497 -0.0033846057 -0.0033855489 -0.0033851149 -0.003385131 -0.0033849876][-0.0034091871 -0.0034068248 -0.0034036427 -0.0033984811 -0.0033949015 -0.0033908465 -0.003387911 -0.0033844842 -0.0033839506 -0.0033843853 -0.0033847089 -0.0033854498 -0.0033851296 -0.0033851406 -0.0033850253][-0.0034050895 -0.0034026091 -0.0034014639 -0.0033995502 -0.0033968317 -0.0033924824 -0.0033882463 -0.0033853515 -0.0033844842 -0.0033848227 -0.0033851475 -0.003385603 -0.0033851992 -0.0033852691 -0.0033853224]]...]
INFO - root - 2017-12-10 01:44:25.037123: step 79010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:22m:25s remains)
INFO - root - 2017-12-10 01:44:33.680549: step 79020, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:15m:59s remains)
INFO - root - 2017-12-10 01:44:42.296944: step 79030, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:36m:58s remains)
INFO - root - 2017-12-10 01:44:50.772386: step 79040, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.856 sec/batch; 60h:14m:11s remains)
INFO - root - 2017-12-10 01:44:59.335497: step 79050, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.842 sec/batch; 59h:18m:18s remains)
INFO - root - 2017-12-10 01:45:07.939002: step 79060, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:03m:12s remains)
INFO - root - 2017-12-10 01:45:16.851535: step 79070, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 63h:24m:30s remains)
INFO - root - 2017-12-10 01:45:25.490080: step 79080, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 58h:55m:51s remains)
INFO - root - 2017-12-10 01:45:34.023028: step 79090, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 59h:11m:01s remains)
INFO - root - 2017-12-10 01:45:42.484461: step 79100, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:20m:14s remains)
2017-12-10 01:45:43.381680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0019901413 -0.00063380622 0.0020693594 0.0057014646 0.0095192557 0.012668198 0.014589639 0.015090873 0.013806241 0.011351007 0.0081662368 0.0051414194 0.00258065 0.00027098041 -0.0015642516][0.00064924988 0.0026888729 0.0064907381 0.01165911 0.017190112 0.021871744 0.024662582 0.025469957 0.023729462 0.019904379 0.01475958 0.0096888989 0.0054479921 0.0017578702 -0.0010146429][0.0068523856 0.010888409 0.017223049 0.025389533 0.033845052 0.040824618 0.045031987 0.046511076 0.044357006 0.038498633 0.030088278 0.021249974 0.013442781 0.006512532 0.0011556114][0.017032836 0.025243081 0.036321424 0.049468223 0.062320441 0.072437525 0.078452557 0.080662385 0.078029022 0.069580287 0.056773659 0.042398594 0.028803045 0.016307399 0.00618863][0.029227072 0.04340528 0.061251543 0.081228167 0.099891908 0.11413527 0.12266168 0.12587816 0.12258166 0.11107686 0.093179606 0.072070174 0.050911497 0.030955678 0.014365764][0.040409938 0.06095919 0.086156011 0.11357917 0.13872302 0.1578055 0.16963227 0.17457111 0.17125845 0.15735832 0.1348736 0.10699167 0.077494413 0.049026903 0.024929944][0.047737014 0.073095225 0.10396675 0.13717516 0.16756131 0.19091007 0.20602936 0.21308179 0.2105345 0.19581519 0.17069639 0.13787475 0.101572 0.065844238 0.035227604][0.049247995 0.076215386 0.10927843 0.14487267 0.17771441 0.20350109 0.2209121 0.229805 0.2284856 0.21450268 0.18922688 0.15468825 0.11520366 0.075718544 0.041598573][0.044773359 0.069926478 0.10109082 0.13494062 0.16668664 0.19230111 0.2102042 0.21998484 0.21988761 0.20778693 0.18458663 0.15181075 0.11360646 0.075050704 0.041619886][0.035654966 0.056311823 0.082286686 0.11088192 0.13820018 0.16080648 0.17709886 0.18642406 0.18705821 0.17736264 0.1578853 0.12994787 0.09714549 0.063940547 0.035174206][0.024603151 0.03960494 0.058743946 0.080114625 0.100898 0.11845846 0.13132131 0.13878949 0.13949178 0.13227315 0.11749147 0.0963024 0.071502849 0.046449833 0.024801511][0.014036343 0.023655664 0.036083262 0.050115224 0.063938633 0.075770386 0.084441066 0.0893508 0.089591287 0.084554128 0.0745216 0.060431309 0.044171918 0.02785722 0.013865309][0.0055216076 0.010881416 0.017926898 0.025964689 0.033942219 0.040785071 0.045688856 0.04822265 0.047930442 0.044619162 0.038594015 0.030536542 0.021499082 0.012590699 0.0050928714][3.1642849e-05 0.0025072054 0.0058509996 0.0097163245 0.01358071 0.016878104 0.019160528 0.020166894 0.01971635 0.017839033 0.014779165 0.010966253 0.0068699177 0.0029611632 -0.00020785723][-0.0026016869 -0.0017958566 -0.0006073087 0.00082002976 0.0022761116 0.003521485 0.0043506371 0.0046314113 0.0043244036 0.0035167348 0.0023450188 0.001003016 -0.00036362535 -0.0016013652 -0.0025487172]]...]
INFO - root - 2017-12-10 01:45:51.901544: step 79110, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.874 sec/batch; 61h:32m:42s remains)
INFO - root - 2017-12-10 01:46:00.587582: step 79120, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 60h:41m:37s remains)
INFO - root - 2017-12-10 01:46:09.177087: step 79130, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 61h:56m:46s remains)
INFO - root - 2017-12-10 01:46:17.900249: step 79140, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:04m:07s remains)
INFO - root - 2017-12-10 01:46:26.551438: step 79150, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 62h:27m:06s remains)
INFO - root - 2017-12-10 01:46:35.258005: step 79160, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:08m:17s remains)
INFO - root - 2017-12-10 01:46:43.919227: step 79170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:46m:04s remains)
INFO - root - 2017-12-10 01:46:52.638183: step 79180, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:46m:57s remains)
INFO - root - 2017-12-10 01:47:01.154877: step 79190, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 63h:02m:33s remains)
INFO - root - 2017-12-10 01:47:09.994619: step 79200, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.942 sec/batch; 66h:17m:07s remains)
2017-12-10 01:47:10.915535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034012066 -0.0034005968 -0.003400767 -0.0034011661 -0.0034017398 -0.0034022136 -0.0034024473 -0.0034023221 -0.0034014832 -0.0034005048 -0.0033994166 -0.0033984645 -0.0033977956 -0.0033978005 -0.0033983211][-0.0033994487 -0.003398791 -0.0033990885 -0.0033996191 -0.0033997314 -0.0033933646 -0.0033788355 -0.0033624338 -0.0033564928 -0.0033615408 -0.003371109 -0.0033805489 -0.0033879306 -0.0033928535 -0.0033953849][-0.0033992077 -0.0033986114 -0.0033991174 -0.0033999064 -0.0033981225 -0.0033692846 -0.0033097249 -0.0032446366 -0.0032241896 -0.0032450499 -0.0032843214 -0.0033264745 -0.0033616491 -0.0033846328 -0.003393647][-0.003398529 -0.0033980557 -0.0033987816 -0.0033996701 -0.0033939413 -0.0033262845 -0.0031925729 -0.0030489345 -0.0030037933 -0.0030461992 -0.0031293544 -0.003226079 -0.0033120746 -0.0033693158 -0.0033906621][-0.0033974347 -0.003397255 -0.003398316 -0.0033994503 -0.0033893206 -0.0032810476 -0.0030723007 -0.0028516296 -0.0027796975 -0.0028359063 -0.0029568905 -0.0031090737 -0.0032527766 -0.0033504658 -0.0033868311][-0.0033967586 -0.0033967542 -0.003397929 -0.0033992028 -0.0033857808 -0.0032536746 -0.0030057465 -0.0027484628 -0.0026651309 -0.0027254082 -0.0028621678 -0.0030427002 -0.0032177514 -0.00333872 -0.0033838754][-0.0033966254 -0.0033965704 -0.0033975556 -0.003398702 -0.0033849336 -0.0032575871 -0.0030247103 -0.0027894885 -0.0027174277 -0.002774223 -0.0028994142 -0.0030656299 -0.0032275196 -0.0033397709 -0.003383236][-0.0033969693 -0.00339675 -0.0033973763 -0.0033981432 -0.0033875566 -0.0032954137 -0.0031317826 -0.0029732878 -0.0029293546 -0.0029724461 -0.003060411 -0.0031737881 -0.0032815952 -0.0033553324 -0.0033857191][-0.0033973341 -0.0033967032 -0.0033967646 -0.0033971195 -0.0033912596 -0.0033444993 -0.0032628586 -0.003188632 -0.0031695231 -0.0031915242 -0.0032352712 -0.0032896847 -0.0033400515 -0.0033745533 -0.0033903515][-0.0033970957 -0.0033959681 -0.003395438 -0.0033955015 -0.0033936633 -0.0033795775 -0.0033548607 -0.0033333334 -0.0033279457 -0.0033341041 -0.0033473275 -0.0033637676 -0.0033788993 -0.0033894456 -0.0033950789][-0.0033966419 -0.0033953923 -0.0033946482 -0.0033945779 -0.003394648 -0.0033931434 -0.0033895327 -0.0033868169 -0.0033865427 -0.0033874905 -0.0033896558 -0.0033924542 -0.0033946906 -0.0033963374 -0.0033975828][-0.0033964375 -0.0033956247 -0.003395244 -0.0033952266 -0.0033957225 -0.0033964675 -0.0033966934 -0.003397445 -0.0033983248 -0.0033989532 -0.0033990175 -0.0033986412 -0.0033981961 -0.0033982187 -0.0033984608][-0.0033967108 -0.0033963078 -0.0033962897 -0.0033961115 -0.003396349 -0.0033970417 -0.0033976198 -0.0033984706 -0.0033993709 -0.0033997972 -0.0033995574 -0.0033990429 -0.0033985137 -0.0033984599 -0.00339865][-0.003397383 -0.003397119 -0.0033972599 -0.0033971604 -0.0033972901 -0.0033976333 -0.0033979113 -0.0033981579 -0.0033985591 -0.0033987076 -0.0033984859 -0.0033981767 -0.0033979598 -0.0033981265 -0.0033984696][-0.0033979896 -0.0033978128 -0.0033980533 -0.0033980417 -0.003398089 -0.0033981681 -0.0033981623 -0.0033980426 -0.0033980212 -0.00339803 -0.0033978922 -0.0033977136 -0.0033976682 -0.0033978666 -0.0033981788]]...]
INFO - root - 2017-12-10 01:47:19.397800: step 79210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:44m:32s remains)
INFO - root - 2017-12-10 01:47:28.033907: step 79220, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:50m:04s remains)
INFO - root - 2017-12-10 01:47:36.621405: step 79230, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.850 sec/batch; 59h:49m:22s remains)
INFO - root - 2017-12-10 01:47:45.122314: step 79240, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:31m:55s remains)
INFO - root - 2017-12-10 01:47:53.696868: step 79250, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:08m:36s remains)
INFO - root - 2017-12-10 01:48:02.198600: step 79260, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 59h:44m:03s remains)
INFO - root - 2017-12-10 01:48:10.886824: step 79270, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 59h:29m:35s remains)
INFO - root - 2017-12-10 01:48:19.617025: step 79280, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 62h:05m:11s remains)
INFO - root - 2017-12-10 01:48:28.292855: step 79290, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:24m:45s remains)
INFO - root - 2017-12-10 01:48:36.974799: step 79300, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 58h:02m:42s remains)
2017-12-10 01:48:37.962549: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019608026 0.019029073 0.017724156 0.016374182 0.015058454 0.013950463 0.013270617 0.013175707 0.012685621 0.012281355 0.010874822 0.0088834427 0.0075645344 0.0081833815 0.011267101][0.018489588 0.017968744 0.01678768 0.015631711 0.014688825 0.013894182 0.013355162 0.013180021 0.012904553 0.01247458 0.01138173 0.0095612742 0.0078649893 0.0085091032 0.01163331][0.016500052 0.015408542 0.013868414 0.013143433 0.012210459 0.01188598 0.011924832 0.01219202 0.012590289 0.012511112 0.011834645 0.010326274 0.0086252922 0.0093011009 0.012687834][0.014319452 0.012831775 0.011366557 0.010551021 0.0097875856 0.010382142 0.01068636 0.011609162 0.01237287 0.012731373 0.012805883 0.011494687 0.01020029 0.011017747 0.014638478][0.012553813 0.01111585 0.010027906 0.0095670214 0.0094584245 0.010410724 0.011055552 0.012384605 0.013146905 0.01427966 0.014917729 0.01419656 0.013146861 0.014097955 0.017530454][0.011135865 0.0099669835 0.0094256513 0.009644038 0.010305329 0.011882815 0.013063272 0.014200663 0.014767604 0.015949683 0.016236478 0.016222535 0.015270375 0.016542485 0.019859068][0.010221253 0.0094157383 0.0092241643 0.0099031553 0.011131598 0.0129009 0.014186485 0.015262712 0.015974091 0.016800687 0.016752521 0.016473077 0.016014643 0.017754251 0.02116443][0.010164935 0.0098796785 0.0099578276 0.010837575 0.012168226 0.013943085 0.015142977 0.015998987 0.016486006 0.016898416 0.016523195 0.016391374 0.016681211 0.018880889 0.022180948][0.010297476 0.010560013 0.011073674 0.012151343 0.01351672 0.014995437 0.015818464 0.016322546 0.016358377 0.016268488 0.01578786 0.01580767 0.016642129 0.018739928 0.021740247][0.010541034 0.011187778 0.011928647 0.013128003 0.01446536 0.015770659 0.016216252 0.016234515 0.015745915 0.015063684 0.014328111 0.014186108 0.015049944 0.016969221 0.019860487][0.010409156 0.011123425 0.011966577 0.013021123 0.014187701 0.015251771 0.015715484 0.015407223 0.014593683 0.013487356 0.012499178 0.012135033 0.012756841 0.014320202 0.016915817][0.0097128386 0.010399189 0.011133964 0.011916971 0.012766374 0.013409264 0.013572617 0.013216108 0.012312135 0.011093233 0.010066985 0.0096184164 0.010025002 0.011368987 0.013699822][0.0086614145 0.00911674 0.0096339313 0.010071037 0.010514311 0.01065772 0.010401279 0.0099900793 0.0091489684 0.0081451582 0.0073045269 0.0069747837 0.0073759928 0.0086084586 0.010654786][0.0073901797 0.0074610533 0.0075758956 0.0076640444 0.007858241 0.0078911455 0.0075460803 0.0071632741 0.0064785974 0.0057693264 0.0051837442 0.0050380444 0.0054839235 0.0065697813 0.0082402453][0.0061554192 0.0059417533 0.0057264129 0.0055726878 0.0055608144 0.005490941 0.005210585 0.0049279695 0.0045087449 0.0041309902 0.0038698448 0.0039316528 0.0044186478 0.0053349426 0.0066069374]]...]
INFO - root - 2017-12-10 01:48:46.469064: step 79310, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:23m:59s remains)
INFO - root - 2017-12-10 01:48:55.128690: step 79320, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:05m:49s remains)
INFO - root - 2017-12-10 01:49:03.717217: step 79330, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:53m:50s remains)
INFO - root - 2017-12-10 01:49:12.190440: step 79340, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:27m:55s remains)
INFO - root - 2017-12-10 01:49:20.878227: step 79350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:27m:38s remains)
INFO - root - 2017-12-10 01:49:29.349513: step 79360, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 58h:21m:07s remains)
INFO - root - 2017-12-10 01:49:37.854518: step 79370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:43m:53s remains)
INFO - root - 2017-12-10 01:49:46.475849: step 79380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:02m:17s remains)
INFO - root - 2017-12-10 01:49:55.054809: step 79390, loss = 0.89, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 53h:42m:58s remains)
INFO - root - 2017-12-10 01:50:03.696448: step 79400, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 61h:37m:19s remains)
2017-12-10 01:50:04.587114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034128069 -0.0034100458 -0.0034078956 -0.0034063798 -0.0034058986 -0.0034065102 -0.0034076881 -0.0034091768 -0.0034108546 -0.0034126048 -0.0034143543 -0.0034158642 -0.0034169231 -0.0034174779 -0.003417087][-0.0034128998 -0.0034100811 -0.0034079561 -0.0034066155 -0.0034064478 -0.0034072804 -0.0034085861 -0.0034099384 -0.0034111028 -0.003411995 -0.0034127727 -0.0034134726 -0.0034141196 -0.0034147934 -0.0034148174][-0.0034123447 -0.0034097962 -0.0034081531 -0.0034073652 -0.0034077102 -0.0034088651 -0.0034104683 -0.0034118793 -0.0034126658 -0.0034127233 -0.0034122753 -0.003411686 -0.0034114032 -0.0034117044 -0.0034118437][-0.0034106788 -0.0034086287 -0.0034078218 -0.0034077528 -0.0034086639 -0.0034101421 -0.0034119193 -0.0034135 -0.0034141648 -0.0034135161 -0.0034119906 -0.0034101552 -0.0034088236 -0.0034082178 -0.0034080257][-0.0034079053 -0.0034065237 -0.00340665 -0.0034075745 -0.0034090385 -0.0034108777 -0.0034130716 -0.0034147592 -0.0034152307 -0.0034138064 -0.0034115941 -0.0034088755 -0.0034063878 -0.0034046355 -0.0034038923][-0.0034043135 -0.0034034955 -0.0034046446 -0.0034064786 -0.0034087631 -0.0034110895 -0.0034137489 -0.0034156728 -0.0034159764 -0.0034141997 -0.0034114758 -0.0034079247 -0.0034042727 -0.003401404 -0.0033999535][-0.0034019088 -0.003401435 -0.0034032885 -0.0034057975 -0.0034085326 -0.0034111978 -0.0034139219 -0.003415833 -0.0034159995 -0.0034139419 -0.0034107973 -0.0034066157 -0.00340228 -0.0033987027 -0.003396624][-0.0034021703 -0.003401848 -0.003403649 -0.0034060529 -0.0034085272 -0.00341088 -0.0034132244 -0.0034146721 -0.0034145268 -0.0034122483 -0.0034089761 -0.0034047274 -0.003400421 -0.003396634 -0.0033941562][-0.0034043512 -0.0034037216 -0.0034048664 -0.0034064671 -0.0034080381 -0.003409462 -0.0034108285 -0.0034113778 -0.0034107536 -0.0034085175 -0.003405652 -0.0034022022 -0.0033985376 -0.0033950915 -0.0033925273][-0.0034061789 -0.0034049638 -0.00340522 -0.0034056916 -0.0034062185 -0.0034068348 -0.00340732 -0.0034071121 -0.0034061354 -0.0034043456 -0.0034022548 -0.003399668 -0.003396766 -0.0033938133 -0.0033912729][-0.0034065696 -0.003404987 -0.0034044066 -0.0034037908 -0.0034033679 -0.0034032906 -0.0034033256 -0.0034027861 -0.0034017696 -0.0034004375 -0.0033990559 -0.0033973022 -0.00339522 -0.0033929173 -0.003390613][-0.003405557 -0.0034041714 -0.0034030604 -0.003401831 -0.0034007682 -0.0034001421 -0.0033995325 -0.0033986713 -0.0033976596 -0.0033966843 -0.0033958808 -0.0033950256 -0.0033939441 -0.0033925658 -0.0033909446][-0.0034056844 -0.0034048429 -0.0034036536 -0.0034020427 -0.0034003933 -0.0033989402 -0.003397482 -0.0033960091 -0.0033947569 -0.0033938715 -0.0033934668 -0.0033933576 -0.0033932633 -0.0033929842 -0.0033925][-0.0034076769 -0.0034074439 -0.0034066651 -0.0034048939 -0.003402713 -0.0034002981 -0.0033979132 -0.00339573 -0.0033939949 -0.0033928379 -0.0033924829 -0.0033929127 -0.0033936531 -0.0033946771 -0.0033955581][-0.0034106383 -0.0034109866 -0.0034109966 -0.0034096334 -0.0034073999 -0.0034046012 -0.0034016476 -0.0033987989 -0.0033965223 -0.00339488 -0.0033942002 -0.0033946331 -0.0033958624 -0.0033977882 -0.0033997896]]...]
INFO - root - 2017-12-10 01:50:13.136381: step 79410, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 61h:01m:44s remains)
INFO - root - 2017-12-10 01:50:21.729102: step 79420, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 63h:26m:30s remains)
INFO - root - 2017-12-10 01:50:30.537984: step 79430, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 64h:23m:12s remains)
INFO - root - 2017-12-10 01:50:38.979898: step 79440, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 59h:46m:43s remains)
INFO - root - 2017-12-10 01:50:47.460051: step 79450, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:28m:52s remains)
INFO - root - 2017-12-10 01:50:55.987968: step 79460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:48m:34s remains)
INFO - root - 2017-12-10 01:51:04.634641: step 79470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:28m:22s remains)
INFO - root - 2017-12-10 01:51:13.412171: step 79480, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 60h:54m:28s remains)
INFO - root - 2017-12-10 01:51:22.031212: step 79490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:46m:00s remains)
INFO - root - 2017-12-10 01:51:30.599950: step 79500, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:35m:00s remains)
2017-12-10 01:51:31.470423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033720336 -0.003370025 -0.0033699484 -0.0033701451 -0.0033701132 -0.00337004 -0.0033700189 -0.0033699665 -0.0033698438 -0.0033696867 -0.0033696517 -0.0033696904 -0.0033697383 -0.0033698021 -0.0033698026][-0.0033702017 -0.0033681667 -0.0033683279 -0.0033688133 -0.0033689232 -0.0033688827 -0.0033688359 -0.0033686107 -0.00336818 -0.0033679961 -0.0033679928 -0.0033679164 -0.0033678645 -0.003367852 -0.0033678229][-0.00337113 -0.00336926 -0.0033697407 -0.0033707381 -0.0033713423 -0.0033715526 -0.003371449 -0.0033709523 -0.00337003 -0.0033692552 -0.0033688152 -0.0033683002 -0.0033679376 -0.0033678317 -0.0033678177][-0.0033715621 -0.00337002 -0.0033710741 -0.0033728848 -0.0033743808 -0.0033751351 -0.0033750879 -0.0033741102 -0.0033724778 -0.0033707868 -0.0033695849 -0.0033684573 -0.0033677716 -0.0033675579 -0.0033676345][-0.003371777 -0.0033705675 -0.0033723458 -0.0033752962 -0.0033779752 -0.0033796637 -0.0033797603 -0.0033782315 -0.0033757805 -0.0033731563 -0.0033708527 -0.0033689272 -0.0033678566 -0.003367496 -0.0033676177][-0.0033717828 -0.0033708429 -0.0033732126 -0.003377252 -0.00338115 -0.0033838809 -0.0033841419 -0.0033821845 -0.0033790285 -0.0033755645 -0.0033724259 -0.0033697388 -0.0033682003 -0.003367587 -0.0033675227][-0.0033715107 -0.0033710042 -0.0033738662 -0.0033788092 -0.0033837431 -0.0033872239 -0.0033877625 -0.00338526 -0.0033812 -0.003377036 -0.0033734597 -0.0033704487 -0.0033685779 -0.0033677123 -0.0033675455][-0.0033712978 -0.0033708704 -0.0033740401 -0.0033794024 -0.003384822 -0.0033887916 -0.0033896244 -0.0033867653 -0.0033819973 -0.0033776655 -0.0033740052 -0.0033709176 -0.0033689004 -0.0033677854 -0.0033675407][-0.0033709048 -0.0033703248 -0.0033733957 -0.0033784495 -0.0033836449 -0.0033878889 -0.0033892388 -0.0033870679 -0.0033829536 -0.0033783154 -0.0033743621 -0.0033711968 -0.0033690459 -0.0033678312 -0.0033675276][-0.0033705737 -0.0033696939 -0.0033722227 -0.0033765354 -0.0033811049 -0.0033850728 -0.0033868591 -0.0033859084 -0.0033829538 -0.0033786483 -0.0033746557 -0.0033715533 -0.0033693372 -0.0033679435 -0.0033673968][-0.0033702494 -0.003369044 -0.0033709002 -0.0033740087 -0.0033775216 -0.0033807927 -0.0033825871 -0.0033825175 -0.0033807491 -0.0033774127 -0.00337421 -0.0033715442 -0.003369472 -0.0033679439 -0.00336716][-0.003369957 -0.0033683842 -0.0033695274 -0.0033713339 -0.0033736522 -0.0033759878 -0.0033774644 -0.003377819 -0.0033770117 -0.0033751037 -0.0033729512 -0.003370909 -0.0033691674 -0.00336783 -0.0033669684][-0.0033699635 -0.0033678662 -0.0033683442 -0.003369024 -0.0033702191 -0.0033716543 -0.0033726774 -0.0033730625 -0.0033728734 -0.0033719854 -0.0033706601 -0.003369251 -0.0033679819 -0.0033669479 -0.0033662445][-0.0033698468 -0.0033673102 -0.0033673502 -0.0033675267 -0.0033680385 -0.0033686839 -0.0033691104 -0.0033693037 -0.0033692308 -0.0033688135 -0.003368065 -0.0033672077 -0.0033664356 -0.0033657942 -0.0033653947][-0.0033697041 -0.0033669574 -0.0033667956 -0.0033668261 -0.0033670452 -0.0033671765 -0.0033671313 -0.0033670512 -0.0033669244 -0.00336664 -0.0033661984 -0.0033657977 -0.0033654773 -0.0033651919 -0.0033650203]]...]
INFO - root - 2017-12-10 01:51:39.977291: step 79510, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 58h:51m:25s remains)
INFO - root - 2017-12-10 01:51:48.539251: step 79520, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 59h:14m:57s remains)
INFO - root - 2017-12-10 01:51:57.117467: step 79530, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 59h:07m:16s remains)
INFO - root - 2017-12-10 01:52:05.619414: step 79540, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:00m:28s remains)
INFO - root - 2017-12-10 01:52:14.253534: step 79550, loss = 0.90, batch loss = 0.69 (8.5 examples/sec; 0.940 sec/batch; 66h:00m:59s remains)
INFO - root - 2017-12-10 01:52:22.801022: step 79560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:54m:15s remains)
INFO - root - 2017-12-10 01:52:31.409859: step 79570, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:09m:49s remains)
INFO - root - 2017-12-10 01:52:39.956684: step 79580, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:43m:03s remains)
INFO - root - 2017-12-10 01:52:48.598028: step 79590, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:48m:36s remains)
INFO - root - 2017-12-10 01:52:56.923941: step 79600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:24m:12s remains)
2017-12-10 01:52:57.918094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033893208 -0.0033131307 -0.0028821283 -0.0016565258 0.00055660657 0.0034025412 0.0060315011 0.0075253546 0.0073882947 0.0057848245 0.0034223825 0.00099309324 -0.001023331 -0.0024037764 -0.0031161895][-0.0033847035 -0.0032434696 -0.0025645834 -0.00071356934 0.0026541383 0.00712426 0.011483034 0.01425628 0.014499405 0.012225008 0.0084193787 0.0042567719 0.00072319643 -0.0016799809 -0.0028928507][-0.0033552507 -0.003076877 -0.0019189308 0.0010487831 0.0063542714 0.013461821 0.020639457 0.025671918 0.026917361 0.024024762 0.018164709 0.011136056 0.004773031 0.0002028509 -0.0022445035][-0.0032106836 -0.0026368941 -0.00056839618 0.0042837383 0.012516724 0.023250783 0.034000419 0.041713037 0.044042177 0.040320046 0.031926822 0.021275403 0.011132257 0.0034313824 -0.0010045983][-0.0026548556 -0.0015330932 0.0020362486 0.0096979076 0.021952847 0.037344579 0.052407745 0.063126296 0.06639719 0.061275072 0.049457725 0.034152169 0.019267621 0.0076586371 0.00069416384][-0.0015395734 0.00041306531 0.0059592593 0.016912688 0.033434179 0.053349163 0.072262056 0.085402526 0.08922568 0.082630038 0.067488015 0.047597524 0.027923221 0.012281671 0.0026032741][0.00010340731 0.0029625802 0.010458646 0.024350569 0.044307448 0.067434117 0.088611312 0.10266799 0.10605962 0.097888671 0.080203868 0.057104468 0.034126356 0.015676528 0.0040474627][0.00188106 0.0054375986 0.014178835 0.029718803 0.051325191 0.075646408 0.0972449 0.11086359 0.11316237 0.10357323 0.084375046 0.059876882 0.035772629 0.016548881 0.0044227969][0.00308676 0.0068488894 0.015653135 0.030986508 0.052016214 0.07540831 0.095864728 0.10829074 0.10951436 0.09916617 0.0797354 0.055703618 0.032623533 0.014637068 0.0035346227][0.0031233106 0.0064538568 0.014030254 0.027316967 0.045688536 0.066274084 0.084326446 0.0951288 0.095676988 0.085654035 0.067632288 0.046045754 0.025950361 0.010816886 0.001826056][0.0016780745 0.0042057512 0.0098177558 0.019866968 0.034136847 0.050569512 0.065293819 0.0742142 0.074543059 0.065962248 0.05090566 0.03340859 0.017690755 0.0063440865 -4.5194291e-05][-0.00011414406 0.0014407902 0.0048732692 0.011330683 0.020965863 0.032569703 0.043359213 0.050111521 0.050466362 0.044097751 0.033017572 0.020486498 0.00966051 0.002249399 -0.0016317365][-0.00180386 -0.00091260392 0.000882176 0.0043568388 0.0098457932 0.016818145 0.023605959 0.028033065 0.028387176 0.024381235 0.017405577 0.0097128972 0.0033495745 -0.00072542927 -0.002664448][-0.0029613627 -0.0025588698 -0.0017616937 -0.00021301722 0.0023240133 0.005705148 0.0091504632 0.011478874 0.011701695 0.0096360361 0.0060735289 0.0022745088 -0.0007006852 -0.0024502957 -0.0031842035][-0.0033695796 -0.0032880851 -0.0030582005 -0.0025413553 -0.0016312327 -0.00038283784 0.00089905108 0.001757445 0.0018163247 0.0010148643 -0.00031281146 -0.0016627122 -0.0026497319 -0.0031726218 -0.0033612461]]...]
INFO - root - 2017-12-10 01:53:06.338061: step 79610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:43m:59s remains)
INFO - root - 2017-12-10 01:53:14.950441: step 79620, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:19m:42s remains)
INFO - root - 2017-12-10 01:53:23.496038: step 79630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:12m:45s remains)
INFO - root - 2017-12-10 01:53:32.014446: step 79640, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:42m:15s remains)
INFO - root - 2017-12-10 01:53:40.734441: step 79650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 60h:35m:48s remains)
INFO - root - 2017-12-10 01:53:49.385802: step 79660, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:20m:51s remains)
INFO - root - 2017-12-10 01:53:58.106136: step 79670, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:01m:03s remains)
INFO - root - 2017-12-10 01:54:06.734904: step 79680, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:19m:45s remains)
INFO - root - 2017-12-10 01:54:15.596266: step 79690, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.963 sec/batch; 67h:39m:20s remains)
INFO - root - 2017-12-10 01:54:24.156755: step 79700, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 60h:19m:31s remains)
2017-12-10 01:54:25.136265: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032379892 0.033458404 0.0339006 0.033321682 0.032570988 0.031642959 0.030609308 0.029800178 0.029239258 0.028697485 0.02782459 0.027402207 0.027329588 0.02781867 0.028027052][0.0333536 0.034849558 0.035594735 0.035044819 0.034047049 0.0329209 0.032136727 0.031769715 0.031764463 0.031785108 0.031748071 0.031657811 0.031574972 0.031359021 0.030788543][0.029544344 0.031695608 0.032963276 0.033087812 0.032595485 0.031526662 0.030604573 0.030044081 0.029868009 0.029973229 0.030191274 0.030561732 0.030998854 0.031061525 0.03094844][0.023618102 0.026651731 0.029007463 0.030247005 0.030459536 0.029976597 0.029469492 0.028903035 0.028509041 0.028298398 0.0282168 0.028312745 0.02845232 0.028590376 0.028693216][0.016477147 0.020357167 0.023740111 0.025964441 0.027034484 0.027203942 0.026996648 0.026426841 0.026013453 0.025849812 0.025926782 0.026225355 0.026507722 0.026702072 0.026592553][0.0093952008 0.012929143 0.016431501 0.019474866 0.021247223 0.02187172 0.02176859 0.021580778 0.021598866 0.021666646 0.021980487 0.022632159 0.023249218 0.02370974 0.023884885][0.0053866785 0.0073712086 0.0097135529 0.012037516 0.013829853 0.014896723 0.015313018 0.015579171 0.015972853 0.016366452 0.016691191 0.017136259 0.017702794 0.018152902 0.018364294][0.003072887 0.0039871223 0.0050144261 0.0060956636 0.006979499 0.0075586047 0.0079917 0.0084049441 0.0088878032 0.0093921674 0.0098781353 0.010371165 0.010958538 0.011504859 0.011660688][0.0012046068 0.0018560395 0.0022354855 0.0024685489 0.0023986946 0.0021969688 0.0020318923 0.0020421774 0.0022867273 0.0026381656 0.0030892717 0.003577959 0.0041820062 0.004725893 0.0049782656][-0.00091587426 -0.00054870383 -0.0001168754 4.1870633e-05 -9.4563467e-05 -0.0004281213 -0.00085998676 -0.0012122304 -0.0014261303 -0.0014341285 -0.0012201266 -0.00082088239 -0.00016291253 0.00041193748 0.00064669945][-0.000754019 -0.00096776197 -0.0010619222 -0.0011766038 -0.0014198548 -0.0017931537 -0.0022254549 -0.0026248554 -0.0029578509 -0.0031430442 -0.0031258967 -0.0028771202 -0.0023928133 -0.0018383926 -0.0015170852][0.002125757 0.0014713805 0.00066805934 -0.0001994248 -0.00094605749 -0.0016040266 -0.0021698086 -0.0026101675 -0.0029159384 -0.003104229 -0.0032069345 -0.0031264254 -0.0028603235 -0.0026792397 -0.0026521925][0.002526507 0.002853995 0.0028781991 0.0024444384 0.0015959179 0.00050963205 -0.00061845942 -0.0016679692 -0.0024322534 -0.0029058021 -0.0031078353 -0.0031068786 -0.0028271822 -0.0027793301 -0.0027384008][0.005127972 0.005250846 0.0051732687 0.0047959639 0.0039841961 0.0027683808 0.0013756698 -8.2121929e-05 -0.0013127725 -0.0021707984 -0.0026576789 -0.0027011449 -0.0023964173 -0.0021705483 -0.0018638551][0.010948505 0.011034182 0.010561904 0.00968388 0.0080937184 0.006048766 0.0037883029 0.0017045808 -5.3301454e-05 -0.0013620399 -0.0020745408 -0.0022746767 -0.0020073508 -0.0013816648 -0.00056989165]]...]
INFO - root - 2017-12-10 01:54:33.632536: step 79710, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:12m:15s remains)
INFO - root - 2017-12-10 01:54:42.382242: step 79720, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 63h:37m:56s remains)
INFO - root - 2017-12-10 01:54:51.074660: step 79730, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 63h:39m:01s remains)
INFO - root - 2017-12-10 01:54:59.610973: step 79740, loss = 0.91, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 60h:35m:47s remains)
INFO - root - 2017-12-10 01:55:08.260880: step 79750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:27m:57s remains)
INFO - root - 2017-12-10 01:55:16.834694: step 79760, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:17m:46s remains)
INFO - root - 2017-12-10 01:55:25.425813: step 79770, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 59h:16m:59s remains)
INFO - root - 2017-12-10 01:55:34.112178: step 79780, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 62h:07m:36s remains)
INFO - root - 2017-12-10 01:55:42.898877: step 79790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:50m:06s remains)
INFO - root - 2017-12-10 01:55:51.079883: step 79800, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:12m:44s remains)
2017-12-10 01:55:52.053275: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.083276965 0.085911505 0.08984641 0.094472 0.099738218 0.10321114 0.10374382 0.098437272 0.087873995 0.07237833 0.054401513 0.037343226 0.024395449 0.016767204 0.012521531][0.1092565 0.11117803 0.11556078 0.12129912 0.12930562 0.13642381 0.13994551 0.13574642 0.12299718 0.10214876 0.076149121 0.050984029 0.030765029 0.018435121 0.012028232][0.13636354 0.13855876 0.14379188 0.15058358 0.1603359 0.16972373 0.17539473 0.1730963 0.16000666 0.13589846 0.10445387 0.072100006 0.04399861 0.024406075 0.013334852][0.16099127 0.16424865 0.17126372 0.18051064 0.19320074 0.20567711 0.21338841 0.21211518 0.19781424 0.1711119 0.13559556 0.0983037 0.064926051 0.039032817 0.022221304][0.18177019 0.18666939 0.19414367 0.2047303 0.21936394 0.23461662 0.245372 0.2459321 0.23108391 0.20176485 0.16175446 0.11966507 0.081618913 0.051106777 0.03055569][0.19295809 0.20153847 0.21118063 0.22306065 0.23829873 0.25473303 0.26744798 0.2703748 0.25681952 0.22780071 0.18610409 0.14037576 0.0971418 0.060787521 0.03667612][0.18810527 0.20301224 0.21658255 0.23053679 0.24612464 0.26156506 0.27358302 0.27695966 0.26564175 0.24048254 0.20268868 0.15972249 0.11675542 0.077633373 0.048604388][0.17288435 0.19391689 0.21274602 0.23071276 0.24747482 0.26237151 0.27315572 0.27624446 0.2663945 0.24395743 0.21075201 0.17257965 0.13338402 0.0962424 0.066759124][0.15237191 0.17770539 0.20119682 0.22382094 0.24337713 0.25991094 0.27125826 0.27469426 0.26629022 0.24676958 0.21722336 0.18327837 0.14803664 0.11445129 0.086800791][0.1309752 0.15804939 0.18451127 0.21062331 0.23336779 0.25247872 0.26553163 0.27095675 0.26485834 0.24865934 0.22316736 0.19340943 0.16264558 0.13386627 0.10966226][0.10637645 0.13407041 0.16375902 0.19383235 0.22100104 0.24424426 0.25989422 0.26771292 0.26511249 0.2532675 0.23296335 0.20845526 0.18250631 0.15766872 0.13552403][0.081300549 0.10784956 0.13954842 0.17387933 0.20678383 0.23491034 0.25417116 0.26419902 0.26334837 0.2532151 0.23630571 0.21629424 0.19531761 0.1762542 0.15780449][0.064475693 0.087293886 0.11813721 0.1543545 0.19199224 0.22634061 0.25138423 0.26610053 0.26900974 0.26110938 0.24616037 0.22743472 0.20800942 0.19124605 0.17586157][0.053338803 0.07241796 0.10060795 0.13580653 0.17544691 0.21334365 0.24360664 0.2640495 0.27326974 0.27074778 0.26058671 0.24457431 0.22618918 0.20980382 0.19490577][0.046455625 0.063320294 0.089177042 0.12258114 0.16232282 0.20125319 0.2340275 0.25758541 0.27125189 0.27383777 0.26917824 0.25769106 0.24244027 0.22779049 0.21384704]]...]
INFO - root - 2017-12-10 01:56:00.526865: step 79810, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 62h:53m:35s remains)
INFO - root - 2017-12-10 01:56:09.202603: step 79820, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 61h:10m:40s remains)
INFO - root - 2017-12-10 01:56:17.940847: step 79830, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.921 sec/batch; 64h:37m:27s remains)
INFO - root - 2017-12-10 01:56:26.387402: step 79840, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 58h:25m:51s remains)
INFO - root - 2017-12-10 01:56:35.006121: step 79850, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 60h:06m:51s remains)
INFO - root - 2017-12-10 01:56:43.616769: step 79860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:49m:37s remains)
INFO - root - 2017-12-10 01:56:52.345764: step 79870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:16m:54s remains)
INFO - root - 2017-12-10 01:57:01.035091: step 79880, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:18m:24s remains)
INFO - root - 2017-12-10 01:57:09.677486: step 79890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:51m:05s remains)
INFO - root - 2017-12-10 01:57:18.196897: step 79900, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 62h:22m:36s remains)
2017-12-10 01:57:19.093934: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026320809 0.026080079 0.023748869 0.020276165 0.016205579 0.012250163 0.008932597 0.0063506244 0.0042138956 0.0020537202 3.8282713e-05 -0.0016309683 -0.0027506617 -0.0032377273 -0.0033505629][0.020177891 0.021767473 0.022210466 0.021959478 0.020940492 0.019110663 0.0165824 0.013597662 0.010294988 0.0066555096 0.0030366185 8.6964574e-06 -0.0020285847 -0.0030002333 -0.0033089258][0.014129011 0.017789697 0.021704817 0.025636394 0.028743727 0.03004103 0.029083187 0.026076203 0.021403728 0.015571699 0.0092813531 0.0037420865 -0.00023909681 -0.0023955205 -0.0032001426][0.010306685 0.016076319 0.023728063 0.032454662 0.040530752 0.045809738 0.047024962 0.044140205 0.037667729 0.028749628 0.018698396 0.00954202 0.0026378252 -0.00138477 -0.0030139654][0.0087224655 0.01627635 0.027312197 0.040783178 0.054230213 0.06426242 0.06848944 0.06619975 0.057845552 0.045186024 0.030448651 0.01672212 0.0061330432 -0.00019912305 -0.0028106323][0.0086527374 0.017472833 0.031031156 0.048181783 0.066018336 0.080207996 0.087341063 0.085946411 0.076194108 0.060182467 0.041139707 0.023141721 0.0091564367 0.00075931405 -0.0026717125][0.0087307617 0.018148053 0.033049338 0.052311536 0.072800338 0.089617737 0.098682374 0.097938448 0.08723852 0.068893038 0.046941206 0.026239073 0.010371698 0.0010211568 -0.0026766183][0.0080497 0.017243253 0.032049887 0.051518366 0.0725533 0.090141371 0.09987437 0.099314943 0.088070877 0.068667844 0.045779109 0.0247233 0.0091891624 0.00042495388 -0.0028336847][0.0062449258 0.014395641 0.027754731 0.045608968 0.065082364 0.081554838 0.090708271 0.089960232 0.078830406 0.060017742 0.038438931 0.019419093 0.00619852 -0.00073638884 -0.0030584391][0.0038305328 0.010253188 0.021024656 0.035770148 0.052017841 0.065820016 0.073310845 0.0721087 0.061925247 0.045503803 0.027459798 0.012417234 0.0027216047 -0.0018926846 -0.0032325839][0.001385709 0.005774905 0.013353975 0.024078287 0.036060184 0.046254363 0.051538464 0.049990047 0.041620024 0.02899945 0.01590753 0.0057537677 -0.00020293193 -0.0027046935 -0.0033155077][-0.0005196936 0.0019854067 0.0064815087 0.013148766 0.020751577 0.02721023 0.030342547 0.028810581 0.022846213 0.014531793 0.0065175425 0.00084850495 -0.0020971764 -0.0031390588 -0.0033445675][-0.0019078812 -0.00068070763 0.0015436288 0.0049388129 0.0089051789 0.012298949 0.013836183 0.0126934 0.0091574509 0.0046282439 0.00063561392 -0.0018813747 -0.0029917171 -0.0033039718 -0.0033568146][-0.0028824252 -0.0023731817 -0.0014934477 -0.00014952454 0.0014638791 0.002870641 0.0034966206 0.0029315383 0.001356642 -0.00054486189 -0.00209005 -0.0029573473 -0.0032741034 -0.0033492495 -0.0033673835][-0.0033267166 -0.0032059045 -0.0029665341 -0.0025834045 -0.0021120512 -0.0017009052 -0.0015187497 -0.001692109 -0.0021565044 -0.0026906971 -0.0030825471 -0.0032814932 -0.0033443749 -0.0033655511 -0.0033772239]]...]
INFO - root - 2017-12-10 01:57:27.516357: step 79910, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 58h:52m:24s remains)
INFO - root - 2017-12-10 01:57:35.947190: step 79920, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 57h:32m:24s remains)
INFO - root - 2017-12-10 01:57:44.737146: step 79930, loss = 0.90, batch loss = 0.69 (8.0 examples/sec; 1.003 sec/batch; 70h:20m:53s remains)
INFO - root - 2017-12-10 01:57:53.469784: step 79940, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:30m:19s remains)
INFO - root - 2017-12-10 01:58:02.123745: step 79950, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 58h:57m:25s remains)
INFO - root - 2017-12-10 01:58:10.601595: step 79960, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 58h:24m:54s remains)
INFO - root - 2017-12-10 01:58:19.242435: step 79970, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:37m:42s remains)
INFO - root - 2017-12-10 01:58:27.837586: step 79980, loss = 0.89, batch loss = 0.68 (9.8 examples/sec; 0.812 sec/batch; 56h:58m:13s remains)
INFO - root - 2017-12-10 01:58:36.442884: step 79990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:52m:45s remains)
INFO - root - 2017-12-10 01:58:44.995219: step 80000, loss = 0.89, batch loss = 0.69 (10.6 examples/sec; 0.758 sec/batch; 53h:07m:56s remains)
2017-12-10 01:58:45.857904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034115533 -0.0033971618 -0.003279265 -0.0028365189 -0.0021397583 -0.0013844983 -0.00081461784 -0.00060301577 -0.00074086525 -0.0011189349 -0.0016241705 -0.0021899389 -0.0026978441 -0.0030717717 -0.003286046][-0.0034099263 -0.0033815587 -0.003200046 -0.0026178879 -0.0016857788 -0.0006621636 0.00013256422 0.00050362642 0.00040360005 -8.4353611e-05 -0.00078822416 -0.0015701966 -0.0023011982 -0.002862958 -0.0032021843][-0.0033917641 -0.00333915 -0.0030414704 -0.0022771745 -0.00098917843 0.00058235112 0.0019883772 0.0029195494 0.0031516852 0.0026418257 0.0015552219 0.00017482624 -0.001196991 -0.0022909443 -0.0029713432][-0.0033004486 -0.003187574 -0.0027054502 -0.0015868477 0.00033123093 0.0028298276 0.0052869525 0.0071552871 0.0079588154 0.0074933115 0.0058723539 0.0035007948 0.000982885 -0.0011080967 -0.0024631808][-0.0030413214 -0.0027339212 -0.0017675087 0.00024719234 0.0035547607 0.0077240616 0.011764096 0.014749289 0.015933093 0.015016599 0.012218354 0.0082544908 0.0040960833 0.00062922505 -0.0016708109][-0.0025404694 -0.0017679407 0.00011814525 0.0036257824 0.008827622 0.014929609 0.020518567 0.024337908 0.025456591 0.02362874 0.019260177 0.013418404 0.0074498588 0.0025221382 -0.000779337][-0.0018155687 -0.00030441117 0.0028519614 0.008071729 0.015048811 0.022598375 0.0289744 0.032897841 0.03344724 0.030531606 0.02471631 0.017317537 0.0099216281 0.0038924955 -0.00013774075][-0.00098729762 0.0012980017 0.0056150323 0.012086676 0.020016329 0.027986342 0.034229618 0.037599932 0.037366856 0.033625685 0.027013889 0.018851859 0.010773365 0.0042612273 -2.4337787e-05][-0.00029865513 0.0024585591 0.0072639687 0.013985157 0.021732846 0.02912345 0.034627561 0.03728494 0.036490772 0.032413226 0.025716022 0.017627543 0.009719356 0.0034762488 -0.000497164][-0.00016092323 0.0025557179 0.007060824 0.013110152 0.01983913 0.026059378 0.030544804 0.03253153 0.031532384 0.027618671 0.021400083 0.014032781 0.0070624752 0.0018053625 -0.001349024][-0.00053578336 0.0017526413 0.005444577 0.010268032 0.015532903 0.020305425 0.023623221 0.024917008 0.023798753 0.020312071 0.015004937 0.008975232 0.0036122787 -0.00015307916 -0.0022348212][-0.0011704247 0.00050926139 0.0031597258 0.0065641645 0.01021102 0.013422513 0.015508279 0.016070593 0.014878469 0.0120004 0.0079952348 0.0037941949 0.00038590981 -0.0017958073 -0.0028867482][-0.0019494402 -0.00084671122 0.00083582872 0.0029038189 0.0050189 0.0067509664 0.0077084862 0.00767323 0.0065665869 0.0045539 0.0020950951 -0.00020111864 -0.0018615772 -0.0028143835 -0.003239501][-0.0026882843 -0.002067653 -0.0011682827 -0.00014566863 0.00081087393 0.0014771188 0.0016845765 0.0013897049 0.00063172146 -0.00042252149 -0.0015199597 -0.00241004 -0.0029739374 -0.0032602448 -0.0033725959][-0.0032102235 -0.0029522697 -0.0025760806 -0.0021765237 -0.0018601703 -0.0017230205 -0.0017941567 -0.0020321463 -0.0023696132 -0.0027235688 -0.0030246449 -0.0032287596 -0.0033390773 -0.0033875022 -0.0034042946]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-relu-biaes-from-scratch/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 01:58:54.955962: step 80010, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:50m:23s remains)
INFO - root - 2017-12-10 01:59:03.605549: step 80020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:31m:25s remains)
INFO - root - 2017-12-10 01:59:12.145929: step 80030, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:36m:42s remains)
INFO - root - 2017-12-10 01:59:20.589190: step 80040, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 61h:06m:29s remains)
INFO - root - 2017-12-10 01:59:29.180779: step 80050, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:16m:02s remains)
INFO - root - 2017-12-10 01:59:37.603727: step 80060, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 58h:48m:01s remains)
INFO - root - 2017-12-10 01:59:46.266737: step 80070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:10m:52s remains)
INFO - root - 2017-12-10 01:59:54.940493: step 80080, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 59h:46m:14s remains)
INFO - root - 2017-12-10 02:00:03.627305: step 80090, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:19m:25s remains)
INFO - root - 2017-12-10 02:00:12.285371: step 80100, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:16m:33s remains)
2017-12-10 02:00:13.119166: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40636241 0.39336178 0.3824344 0.37594697 0.36673066 0.35647005 0.34711817 0.34052306 0.33450183 0.33278921 0.33161902 0.33052671 0.32659525 0.320409 0.31242371][0.48246205 0.47048041 0.45877004 0.45064715 0.43918189 0.42688543 0.41482696 0.40599859 0.39937505 0.39647225 0.39402136 0.39167768 0.38670325 0.37846118 0.36531514][0.56032312 0.55438721 0.54499125 0.53538507 0.52086449 0.50551534 0.49019256 0.4776994 0.46823713 0.46338105 0.45884547 0.4534159 0.44446954 0.4319151 0.41353807][0.63836879 0.64115584 0.63513881 0.62365085 0.60644549 0.58751714 0.56801593 0.55088329 0.53705633 0.52821904 0.51977146 0.51005465 0.49637109 0.47898835 0.45573333][0.69897622 0.71148235 0.70806479 0.6954931 0.67509711 0.65302318 0.6297664 0.60787815 0.58936816 0.57618606 0.56381649 0.55008334 0.53246135 0.5113582 0.48451][0.72771293 0.75150955 0.752567 0.73875672 0.71508908 0.68934727 0.66206205 0.63643795 0.61392015 0.598456 0.58471525 0.56875014 0.54980582 0.52735579 0.49935222][0.72862387 0.75875294 0.76154 0.74639034 0.71964532 0.69092137 0.66040581 0.63185829 0.60688317 0.59117758 0.57884794 0.5648396 0.54886031 0.52860385 0.50289673][0.69983131 0.73167062 0.73470259 0.71995139 0.69311017 0.66182172 0.62870324 0.59913021 0.57392436 0.55969393 0.55101776 0.54280144 0.53300816 0.51769084 0.49670005][0.644241 0.674228 0.67526412 0.65997422 0.6328398 0.60174423 0.56887949 0.53954941 0.51651877 0.50642955 0.50426382 0.50430024 0.50323272 0.49627075 0.48206517][0.57133132 0.59663206 0.59439629 0.579582 0.553635 0.52399945 0.49286324 0.466668 0.44806206 0.44300908 0.44830006 0.45811439 0.4669075 0.46879566 0.46179062][0.49846309 0.51794434 0.51270169 0.50004447 0.477534 0.45037633 0.42152452 0.39858466 0.38443622 0.38402808 0.39567804 0.41390979 0.43143043 0.44113722 0.44075158][0.4352679 0.45127133 0.44579276 0.43686303 0.41953221 0.39781871 0.37385356 0.35512865 0.3448261 0.34764138 0.36280841 0.38546702 0.4075039 0.42147136 0.42528704][0.39835346 0.41315657 0.40833208 0.40243715 0.38977382 0.372125 0.35259211 0.3385359 0.33218268 0.33784637 0.35464931 0.37860498 0.40116471 0.41490215 0.41901866][0.38003728 0.3936922 0.38980439 0.38741225 0.37985054 0.36795831 0.355219 0.3470777 0.34615541 0.35471806 0.37233362 0.39513317 0.41476136 0.4250659 0.42533234][0.3829087 0.39543945 0.39094451 0.38944817 0.38512018 0.37659705 0.36992249 0.36808574 0.37252092 0.38370916 0.40154988 0.42192474 0.43644869 0.4409655 0.43562222]]...]
INFO - root - 2017-12-10 02:00:21.653253: step 80110, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:59m:51s remains)
INFO - root - 2017-12-10 02:00:30.276079: step 80120, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 59h:24m:12s remains)
INFO - root - 2017-12-10 02:00:39.093693: step 80130, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 63h:34m:52s remains)
INFO - root - 2017-12-10 02:00:47.500363: step 80140, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 57h:43m:23s remains)
INFO - root - 2017-12-10 02:00:56.098271: step 80150, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:34m:28s remains)
INFO - root - 2017-12-10 02:01:04.691530: step 80160, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 58h:03m:09s remains)
INFO - root - 2017-12-10 02:01:13.396927: step 80170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:35m:45s remains)
INFO - root - 2017-12-10 02:01:22.305510: step 80180, loss = 0.89, batch loss = 0.69 (8.5 examples/sec; 0.941 sec/batch; 65h:56m:36s remains)
INFO - root - 2017-12-10 02:01:30.986957: step 80190, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 62h:03m:13s remains)
INFO - root - 2017-12-10 02:01:39.667638: step 80200, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 63h:12m:25s remains)
2017-12-10 02:01:40.601638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033932673 -0.0033952575 -0.0033987742 -0.0034016655 -0.0034029922 -0.0034031027 -0.0034020697 -0.003400411 -0.0033992378 -0.0033981658 -0.0033974461 -0.0033956552 -0.0033927949 -0.0033890817 -0.003384483][-0.0033869916 -0.0033896167 -0.0033935909 -0.0033972966 -0.0033998229 -0.0034016233 -0.0034024771 -0.0034023444 -0.0034021214 -0.0034017807 -0.0034011176 -0.0033988922 -0.0033948694 -0.0033901222 -0.0033847962][-0.0033814847 -0.0033839059 -0.003388417 -0.0033928938 -0.0033966813 -0.0034003016 -0.0034033183 -0.0034051179 -0.0034060443 -0.0034060986 -0.0034049638 -0.0034019991 -0.0033972182 -0.0033915911 -0.0033856418][-0.0033766534 -0.003378626 -0.0033837552 -0.00338925 -0.0033942817 -0.003399255 -0.0034038592 -0.0034067659 -0.0034080206 -0.0034076255 -0.0034056611 -0.0034018576 -0.0033964922 -0.0033904864 -0.0033843298][-0.0033732676 -0.003374621 -0.0033799687 -0.0033860221 -0.0033920621 -0.0033979625 -0.0034034045 -0.0034069666 -0.0034080935 -0.003407069 -0.0034042543 -0.0033995877 -0.0033937704 -0.0033875245 -0.0033814546][-0.003371096 -0.0033715824 -0.0033768022 -0.0033829508 -0.0033894798 -0.0033958654 -0.0034015235 -0.003405472 -0.0034063919 -0.0034047603 -0.0034010755 -0.0033959432 -0.0033902263 -0.0033841964 -0.0033785529][-0.0033692212 -0.0033688007 -0.0033732587 -0.003379321 -0.0033861978 -0.0033926226 -0.0033982331 -0.003402302 -0.0034031963 -0.0034010073 -0.0033966922 -0.0033917676 -0.0033865916 -0.0033812127 -0.0033762087][-0.0033675744 -0.0033663718 -0.0033697411 -0.0033751046 -0.0033816125 -0.0033876023 -0.0033928172 -0.0033966308 -0.0033973234 -0.0033951916 -0.0033910426 -0.0033865152 -0.0033820663 -0.0033776348 -0.0033736313][-0.0033663292 -0.00336465 -0.0033669816 -0.003371041 -0.003376208 -0.0033810316 -0.003385131 -0.0033883692 -0.0033893581 -0.0033879043 -0.0033844812 -0.0033807824 -0.0033772029 -0.0033736133 -0.0033706443][-0.0033658722 -0.0033635765 -0.0033650356 -0.0033676778 -0.0033712555 -0.003374733 -0.0033774185 -0.0033797389 -0.003380972 -0.0033803661 -0.0033780995 -0.0033753891 -0.0033725547 -0.0033699987 -0.0033680706][-0.0033658177 -0.0033628664 -0.0033636054 -0.0033651888 -0.003367557 -0.0033697702 -0.0033712157 -0.0033723814 -0.0033733416 -0.0033732932 -0.0033722152 -0.0033706648 -0.0033689255 -0.003367339 -0.0033662322][-0.0033653947 -0.0033620347 -0.0033622119 -0.0033630205 -0.0033643085 -0.0033655406 -0.0033662389 -0.0033668384 -0.0033674138 -0.0033673283 -0.0033665725 -0.0033656934 -0.003365028 -0.0033644955 -0.0033642615][-0.0033650866 -0.0033615767 -0.003361553 -0.0033616871 -0.0033620631 -0.0033625078 -0.0033626966 -0.0033629863 -0.0033630647 -0.0033627423 -0.0033621229 -0.0033615967 -0.0033614514 -0.0033615106 -0.0033619753][-0.0033645676 -0.0033611588 -0.0033612903 -0.0033612621 -0.0033611739 -0.0033611008 -0.0033609322 -0.0033608337 -0.0033605769 -0.0033601387 -0.0033596088 -0.0033591981 -0.0033591723 -0.0033595304 -0.0033603646][-0.0033643222 -0.0033611287 -0.0033615157 -0.0033617867 -0.0033618067 -0.0033615874 -0.0033612293 -0.0033607485 -0.0033602216 -0.003359562 -0.0033589555 -0.0033586519 -0.0033588305 -0.0033593671 -0.0033600717]]...]
INFO - root - 2017-12-10 02:01:48.807901: step 80210, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:47m:48s remains)
INFO - root - 2017-12-10 02:01:57.339152: step 80220, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:52m:50s remains)
INFO - root - 2017-12-10 02:02:06.012748: step 80230, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:51m:50s remains)
INFO - root - 2017-12-10 02:02:14.466302: step 80240, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 52h:47m:31s remains)
INFO - root - 2017-12-10 02:02:22.840089: step 80250, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:52m:19s remains)
INFO - root - 2017-12-10 02:02:31.364856: step 80260, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:06m:05s remains)
INFO - root - 2017-12-10 02:02:39.828320: step 80270, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:49m:15s remains)
INFO - root - 2017-12-10 02:02:48.285593: step 80280, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 57h:49m:15s remains)
INFO - root - 2017-12-10 02:02:56.960446: step 80290, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:17m:29s remains)
INFO - root - 2017-12-10 02:03:05.524598: step 80300, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:55m:35s remains)
2017-12-10 02:03:06.437589: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0046239235 0.004704305 0.0037904594 0.0021785249 0.00035365624 -0.001071472 -0.0018616585 -0.0021882085 -0.00245152 -0.002672099 -0.0028180571 -0.0029849955 -0.0031536191 -0.0033229028 -0.0033942405][0.010606526 0.01085452 0.0091787614 0.00630664 0.0029499384 0.00035395194 -0.0012434132 -0.0019241802 -0.002330122 -0.0025462515 -0.0026431109 -0.0027971091 -0.0030211294 -0.0032634849 -0.0033821203][0.018589696 0.019199254 0.016847024 0.012684384 0.0077343667 0.0037843522 0.0010613438 -0.000285842 -0.0011075777 -0.0015186688 -0.001762449 -0.0020802356 -0.0025357439 -0.0030264729 -0.0033149365][0.027059129 0.02817039 0.025306609 0.020274404 0.014252786 0.0093287658 0.005660058 0.0035356663 0.0020150931 0.0010869659 0.00041143736 -0.00035017938 -0.0014144504 -0.00247449 -0.0031502952][0.034214549 0.036180608 0.033351514 0.028014088 0.021572288 0.016185155 0.011832848 0.0089489361 0.0065469006 0.0049501061 0.0036482008 0.0022092958 0.00029993802 -0.001596308 -0.0028884648][0.038563725 0.041357283 0.039070453 0.034250889 0.028300835 0.023147853 0.01855655 0.014996883 0.011594962 0.0090595093 0.0068644546 0.0046258979 0.0018507133 -0.00080738566 -0.0026570957][0.039142244 0.042689808 0.041163914 0.037299279 0.032477092 0.028163223 0.02389748 0.020037569 0.015855361 0.012419757 0.0093217 0.0063133342 0.0028538469 -0.00032717804 -0.0025358016][0.036132481 0.040157013 0.039417751 0.036637034 0.033077449 0.029825272 0.026199088 0.022421023 0.017874397 0.013818158 0.010044247 0.0065363105 0.0028459334 -0.00036722096 -0.0025630014][0.030301506 0.034351964 0.03414591 0.032293934 0.029873917 0.027584512 0.02471054 0.021304186 0.016902449 0.012709223 0.0087684225 0.0052840738 0.0019054695 -0.0008704206 -0.0027267421][0.022742065 0.026422301 0.026490599 0.025287131 0.023671964 0.022138659 0.019996321 0.017157404 0.013338752 0.0095927557 0.0060702348 0.0030635495 0.00040049013 -0.0016413843 -0.0029588507][0.014835931 0.017777624 0.017936228 0.017161183 0.016092351 0.015081627 0.013593158 0.011485632 0.00861955 0.0057012681 0.0029659991 0.00070500304 -0.0011429398 -0.0024363119 -0.0031857139][0.0078037996 0.0099172313 0.010056682 0.0095470995 0.00886419 0.0082329838 0.0073122149 0.0059377523 0.0040508611 0.0020816394 0.00025001517 -0.0012146023 -0.0023048287 -0.0029875026 -0.0033254798][0.0025785633 0.003885451 0.0039919196 0.0036927334 0.0032701746 0.0028880781 0.0023579353 0.0015641835 0.00047321734 -0.00066701905 -0.0017054994 -0.0024878003 -0.0030025267 -0.0032768673 -0.0033804534][-0.00072066789 -8.0676982e-05 -3.4397934e-05 -0.00019376865 -0.00041430863 -0.00061706407 -0.00088364049 -0.0012771159 -0.0018140648 -0.0023612534 -0.0028308453 -0.003138789 -0.0033084294 -0.0033772357 -0.0033928792][-0.0024896157 -0.0022495259 -0.002227908 -0.0022890726 -0.00237979 -0.0024697308 -0.0025850607 -0.0027447238 -0.002942163 -0.0031278187 -0.0032768724 -0.0033536849 -0.003385995 -0.0033937308 -0.0033948335]]...]
INFO - root - 2017-12-10 02:03:14.833532: step 80310, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:02m:52s remains)
INFO - root - 2017-12-10 02:03:23.466203: step 80320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 61h:09m:15s remains)
INFO - root - 2017-12-10 02:03:32.094947: step 80330, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 58h:53m:51s remains)
INFO - root - 2017-12-10 02:03:40.811103: step 80340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:36m:30s remains)
INFO - root - 2017-12-10 02:03:49.453394: step 80350, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:01m:34s remains)
INFO - root - 2017-12-10 02:03:57.914645: step 80360, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:10m:37s remains)
INFO - root - 2017-12-10 02:04:06.470790: step 80370, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:27m:45s remains)
INFO - root - 2017-12-10 02:04:15.291751: step 80380, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:01m:53s remains)
INFO - root - 2017-12-10 02:04:24.093112: step 80390, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:06m:06s remains)
INFO - root - 2017-12-10 02:04:32.802016: step 80400, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 61h:55m:46s remains)
2017-12-10 02:04:33.641925: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0033465263 0.010098552 0.01944357 0.029024614 0.036184993 0.03944305 0.038011342 0.032381948 0.024537344 0.016358988 0.0098369056 0.0052226493 0.0022226356 0.00059015048 0.00010312605][0.0067654718 0.015977444 0.028741194 0.041541081 0.051245112 0.056185078 0.055221763 0.048724178 0.03862679 0.0268972 0.016638406 0.0091162063 0.0043227812 0.0018412995 0.0016723035][0.0097238226 0.020724626 0.036071666 0.052132834 0.065038875 0.072690204 0.073471047 0.067797527 0.05690733 0.042593949 0.028690072 0.017015914 0.0090767378 0.004582759 0.0040818891][0.011252771 0.023244914 0.040230896 0.058938537 0.075259686 0.086738035 0.090746731 0.08784271 0.077599458 0.062015258 0.044781659 0.02861944 0.016480953 0.0090060476 0.0074293455][0.011028085 0.022498537 0.039811648 0.060174368 0.079783037 0.095889866 0.10468678 0.10620638 0.098238632 0.08317066 0.063834526 0.04388538 0.027574468 0.016778383 0.013748584][0.010420012 0.020309597 0.036109615 0.056374956 0.078388408 0.098906644 0.11313704 0.12061182 0.11712413 0.1044312 0.084386624 0.061810493 0.041783512 0.027828315 0.022920672][0.0090416688 0.016651707 0.029733805 0.048528191 0.071797237 0.096314326 0.11642654 0.1305038 0.13250667 0.12404729 0.10501684 0.081148647 0.058798589 0.041477844 0.03455729][0.0074317921 0.012949724 0.022816651 0.038970068 0.061525211 0.088309541 0.11332645 0.13368395 0.14189906 0.13863659 0.12231292 0.099082507 0.075576358 0.0558036 0.047074337][0.0054795062 0.0095696822 0.016768377 0.029681688 0.0497832 0.07643076 0.10371362 0.12849152 0.14248714 0.14452818 0.13213737 0.11095446 0.087770261 0.067222521 0.057860281][0.003724749 0.0064996919 0.011469766 0.0213335 0.037955251 0.061880097 0.088574313 0.11509964 0.13308068 0.13973852 0.13185255 0.11395922 0.092893034 0.073801666 0.064857908][0.0019852549 0.0039697196 0.007276223 0.014150955 0.026710972 0.0463718 0.070261046 0.095852233 0.11541526 0.12511992 0.12153094 0.10783382 0.090333 0.074495718 0.067041449][0.00020143366 0.0015478234 0.0036298775 0.0080049736 0.016478289 0.031125112 0.0505479 0.072833136 0.0914274 0.1021508 0.10194856 0.092908531 0.080121979 0.069122218 0.0647036][-0.001439306 -0.00048212009 0.00086463778 0.0035494084 0.0088004274 0.018533552 0.032520935 0.049690105 0.065134265 0.075080067 0.07713715 0.072527766 0.065051079 0.059460096 0.058939353][-0.00262347 -0.002148279 -0.0014072568 4.5986613e-05 0.0030221993 0.008926319 0.018016156 0.029675702 0.040762693 0.048503742 0.05129052 0.04992298 0.047136258 0.046530958 0.05018327][-0.0030619104 -0.0029124077 -0.0026589287 -0.0020729443 -0.00068818638 0.0023964227 0.0077233268 0.014732916 0.021655142 0.026797179 0.029210856 0.029635007 0.029905753 0.032650732 0.039136566]]...]
INFO - root - 2017-12-10 02:04:42.022230: step 80410, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:31m:56s remains)
INFO - root - 2017-12-10 02:04:50.787102: step 80420, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 60h:59m:15s remains)
INFO - root - 2017-12-10 02:04:59.420185: step 80430, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 58h:48m:04s remains)
INFO - root - 2017-12-10 02:05:08.228455: step 80440, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.925 sec/batch; 64h:44m:14s remains)
INFO - root - 2017-12-10 02:05:16.696779: step 80450, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:55m:37s remains)
INFO - root - 2017-12-10 02:05:25.359848: step 80460, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:46m:56s remains)
INFO - root - 2017-12-10 02:05:33.868753: step 80470, loss = 0.91, batch loss = 0.70 (9.6 examples/sec; 0.834 sec/batch; 58h:24m:09s remains)
INFO - root - 2017-12-10 02:05:42.359817: step 80480, loss = 0.90, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 60h:31m:27s remains)
INFO - root - 2017-12-10 02:05:50.961009: step 80490, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:08m:59s remains)
INFO - root - 2017-12-10 02:05:59.557480: step 80500, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:32m:32s remains)
2017-12-10 02:06:00.485058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003404089 -0.0034008813 -0.0033985276 -0.0033982522 -0.0033999805 -0.0034031821 -0.0034075349 -0.0034091952 -0.0034084031 -0.0034044744 -0.0033986461 -0.0033915818 -0.0033847808 -0.0033795002 -0.00337626][-0.003401414 -0.0033962929 -0.0033913702 -0.0033902114 -0.0033942072 -0.0033992659 -0.0034043083 -0.0034075079 -0.0034077982 -0.0034051063 -0.0034006548 -0.0033946903 -0.003388339 -0.0033828553 -0.0033786565][-0.0033971039 -0.003389637 -0.0033820714 -0.0033782253 -0.0033807587 -0.003389037 -0.003400034 -0.0034064921 -0.003408117 -0.0034069384 -0.0034041174 -0.0033999351 -0.0033950584 -0.0033897434 -0.0033845459][-0.0033915655 -0.0033796926 -0.0033677407 -0.0033609907 -0.0033614372 -0.003372466 -0.0033881955 -0.0034005106 -0.0034063014 -0.0034070194 -0.0034060022 -0.0034038231 -0.0034010592 -0.0033966652 -0.0033910223][-0.0033872572 -0.00336933 -0.0033500309 -0.003337048 -0.0033353264 -0.00334971 -0.003371641 -0.0033912365 -0.0034031821 -0.0034067461 -0.0034075009 -0.0034069878 -0.0034057156 -0.0034022632 -0.0033966482][-0.0033866826 -0.0033653064 -0.0033371439 -0.0033155333 -0.0033117598 -0.0033280717 -0.0033559536 -0.0033806646 -0.003398488 -0.0034051104 -0.0034076471 -0.00340851 -0.0034082055 -0.0034058471 -0.0034009188][-0.0033891227 -0.0033665861 -0.0033332699 -0.0033058573 -0.0032999916 -0.0033177091 -0.0033470623 -0.0033733342 -0.0033928349 -0.0034004601 -0.0034042604 -0.003406852 -0.0034078236 -0.0034065915 -0.0034025041][-0.0033915308 -0.0033723407 -0.0033406764 -0.0033101593 -0.0033027269 -0.0033191741 -0.003346469 -0.0033702515 -0.0033873117 -0.0033941397 -0.0033987437 -0.0034024108 -0.0034045186 -0.0034044234 -0.0034008455][-0.0033942 -0.0033807938 -0.0033566444 -0.0033310582 -0.0033212407 -0.0033326978 -0.0033523482 -0.0033695744 -0.0033817566 -0.0033872181 -0.0033925823 -0.0033971202 -0.0033998254 -0.0034003323 -0.0033974051][-0.0033932887 -0.0033875834 -0.0033743107 -0.0033580039 -0.0033497058 -0.003354897 -0.0033649327 -0.0033727835 -0.0033782769 -0.0033821135 -0.0033871429 -0.0033919201 -0.0033952647 -0.0033959001 -0.0033939565][-0.0033884058 -0.0033876242 -0.0033842898 -0.0033779896 -0.0033730657 -0.0033728939 -0.0033745614 -0.0033750779 -0.0033759619 -0.0033782 -0.0033820393 -0.0033862388 -0.0033893618 -0.0033903455 -0.0033893867][-0.0033813422 -0.0033819589 -0.0033829324 -0.003382571 -0.0033810013 -0.0033791072 -0.0033771144 -0.0033743698 -0.0033733868 -0.0033745335 -0.0033770078 -0.0033802872 -0.0033827506 -0.0033835636 -0.0033830379][-0.0033755687 -0.0033752462 -0.0033766546 -0.0033779414 -0.0033780241 -0.0033767282 -0.0033745994 -0.0033725249 -0.0033715942 -0.0033716359 -0.0033728539 -0.0033746958 -0.0033763116 -0.0033768716 -0.0033767368][-0.0033724869 -0.0033709158 -0.0033713079 -0.0033721845 -0.00337268 -0.003372363 -0.0033714117 -0.0033703898 -0.0033696152 -0.0033692967 -0.0033698832 -0.0033708096 -0.0033717055 -0.0033721731 -0.0033721582][-0.0033718345 -0.0033693486 -0.003368753 -0.0033688596 -0.0033691833 -0.0033691772 -0.0033687449 -0.0033681516 -0.0033676841 -0.0033674513 -0.0033676387 -0.0033681765 -0.0033687905 -0.0033694105 -0.0033696268]]...]
INFO - root - 2017-12-10 02:06:08.799895: step 80510, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:44m:38s remains)
INFO - root - 2017-12-10 02:06:17.275655: step 80520, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:49m:18s remains)
INFO - root - 2017-12-10 02:06:25.930055: step 80530, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:07m:37s remains)
INFO - root - 2017-12-10 02:06:34.666990: step 80540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:43m:18s remains)
INFO - root - 2017-12-10 02:06:43.208976: step 80550, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:43m:17s remains)
INFO - root - 2017-12-10 02:06:51.645951: step 80560, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:22m:41s remains)
INFO - root - 2017-12-10 02:07:00.201152: step 80570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:33m:16s remains)
INFO - root - 2017-12-10 02:07:08.986212: step 80580, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:42m:23s remains)
INFO - root - 2017-12-10 02:07:17.666976: step 80590, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:02m:46s remains)
INFO - root - 2017-12-10 02:07:26.261453: step 80600, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 61h:37m:39s remains)
2017-12-10 02:07:27.231587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034121398 -0.0034132958 -0.0034113079 -0.0034058336 -0.0033980431 -0.0033896875 -0.0033824067 -0.00337767 -0.0033755235 -0.0033751645 -0.0033756273 -0.0033764807 -0.0033772318 -0.003377497 -0.0033775074][-0.0034121207 -0.0034132912 -0.0034112989 -0.0034057805 -0.0033977809 -0.0033890435 -0.0033814339 -0.003376544 -0.0033743663 -0.0033739638 -0.0033744683 -0.0033754301 -0.0033761794 -0.0033764641 -0.0033765088][-0.0034129156 -0.0034137485 -0.0034115813 -0.0034058907 -0.0033976161 -0.0033887525 -0.0033813035 -0.0033767209 -0.0033747284 -0.0033743805 -0.0033749861 -0.0033759216 -0.0033764946 -0.0033765626 -0.0033764679][-0.0034137031 -0.0034140057 -0.0034114711 -0.0034054494 -0.0033968114 -0.0033878018 -0.0033806446 -0.0033765188 -0.0033748811 -0.003374903 -0.0033757624 -0.003376693 -0.0033770676 -0.0033769519 -0.0033767025][-0.0034143783 -0.0034141077 -0.0034111841 -0.0034047717 -0.0033959029 -0.0033867403 -0.0033796646 -0.00337599 -0.0033746625 -0.0033750045 -0.003376116 -0.0033771459 -0.003377489 -0.0033773035 -0.0033769608][-0.003414511 -0.0034137522 -0.0034105494 -0.003403859 -0.0033948217 -0.0033855559 -0.0033785047 -0.0033750238 -0.0033738678 -0.0033743947 -0.0033757826 -0.0033769964 -0.003377507 -0.0033774448 -0.0033771433][-0.0034135801 -0.0034126865 -0.0034095733 -0.0034029223 -0.0033939849 -0.0033846183 -0.0033774686 -0.0033738937 -0.0033728089 -0.0033734525 -0.0033749915 -0.0033764062 -0.003377167 -0.0033773491 -0.0033771386][-0.0034112143 -0.0034102767 -0.0034074078 -0.0034012015 -0.0033928712 -0.0033840467 -0.0033770048 -0.0033732678 -0.003372164 -0.003372862 -0.0033743866 -0.003375842 -0.0033767412 -0.0033770942 -0.0033770122][-0.0034077284 -0.0034067587 -0.003404134 -0.003398651 -0.0033913513 -0.0033834958 -0.0033769996 -0.0033733302 -0.0033721302 -0.0033726343 -0.0033739859 -0.0033754369 -0.00337644 -0.0033768893 -0.0033768779][-0.0034027323 -0.00340171 -0.0033995972 -0.0033951669 -0.0033891529 -0.0033826237 -0.0033771074 -0.0033739004 -0.0033727277 -0.0033728788 -0.0033738064 -0.0033750741 -0.0033760956 -0.0033766397 -0.0033766904][-0.0033969304 -0.0033957558 -0.0033942121 -0.0033909371 -0.0033864109 -0.0033813668 -0.0033770746 -0.0033745067 -0.0033735724 -0.0033736099 -0.0033741198 -0.0033750264 -0.0033758674 -0.0033763926 -0.0033764855][-0.0033909942 -0.0033896875 -0.0033886037 -0.00338638 -0.0033833093 -0.0033798062 -0.0033767703 -0.0033749291 -0.0033742937 -0.0033743747 -0.0033746697 -0.0033753135 -0.0033759142 -0.0033762984 -0.0033763784][-0.0033859857 -0.003384528 -0.003383691 -0.0033822064 -0.003380287 -0.0033781929 -0.003376316 -0.003375147 -0.0033747079 -0.0033748415 -0.0033750713 -0.0033755738 -0.0033760488 -0.0033763314 -0.0033763584][-0.0033820546 -0.0033803673 -0.0033797694 -0.0033787768 -0.0033776755 -0.0033766425 -0.0033756937 -0.0033750685 -0.0033748331 -0.003375008 -0.003375202 -0.0033756127 -0.0033760821 -0.0033763489 -0.0033763628][-0.0033791631 -0.0033774618 -0.0033770767 -0.0033764627 -0.0033758441 -0.0033753996 -0.0033750143 -0.0033746895 -0.0033745784 -0.0033748255 -0.0033750823 -0.0033755058 -0.0033759899 -0.0033762916 -0.0033763226]]...]
INFO - root - 2017-12-10 02:07:35.497652: step 80610, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 62h:56m:33s remains)
INFO - root - 2017-12-10 02:07:44.247543: step 80620, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 62h:05m:10s remains)
INFO - root - 2017-12-10 02:07:52.796860: step 80630, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:34m:27s remains)
INFO - root - 2017-12-10 02:08:01.652667: step 80640, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:34m:36s remains)
INFO - root - 2017-12-10 02:08:10.093737: step 80650, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:11m:27s remains)
INFO - root - 2017-12-10 02:08:18.626182: step 80660, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:10m:31s remains)
INFO - root - 2017-12-10 02:08:27.157220: step 80670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:43m:54s remains)
INFO - root - 2017-12-10 02:08:35.773362: step 80680, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:53m:07s remains)
INFO - root - 2017-12-10 02:08:44.462417: step 80690, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 62h:48m:36s remains)
INFO - root - 2017-12-10 02:08:53.163471: step 80700, loss = 0.91, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 61h:38m:50s remains)
2017-12-10 02:08:54.170694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033953 -0.0033940733 -0.0033942489 -0.0033941334 -0.0033937939 -0.0033939562 -0.0033946594 -0.0033954447 -0.0033961467 -0.0033969891 -0.0033976852 -0.0033986615 -0.003400259 -0.003401577 -0.0034023847][-0.0033948806 -0.0033935562 -0.0033939863 -0.0033943972 -0.0033947525 -0.0033954629 -0.0033964571 -0.0033974284 -0.0033986478 -0.0033996834 -0.0034000059 -0.0034002494 -0.0034011002 -0.0034017989 -0.0034021062][-0.0033959209 -0.0033947963 -0.0033953129 -0.0033960077 -0.0033969111 -0.0033980508 -0.0033993809 -0.003400679 -0.0034021144 -0.0034031745 -0.003403092 -0.003402662 -0.0034026825 -0.0034026497 -0.0034023137][-0.0033966415 -0.0033959446 -0.0033966021 -0.0033975104 -0.0033986634 -0.0034001761 -0.0034020615 -0.0034037037 -0.0034052371 -0.0034062213 -0.0034057163 -0.0034044851 -0.0034036234 -0.0034026827 -0.0034017633][-0.0033968887 -0.0033964242 -0.003397468 -0.0033985134 -0.0033998403 -0.0034017593 -0.0034041079 -0.0034061528 -0.0034078218 -0.0034085158 -0.0034075016 -0.00340527 -0.0034033929 -0.0034018557 -0.003400485][-0.0033966377 -0.0033964526 -0.0033977744 -0.0033989565 -0.003400194 -0.0034021789 -0.0034046948 -0.0034069209 -0.003408897 -0.0034098616 -0.0034085368 -0.0034055773 -0.0034028522 -0.0034008198 -0.0033991067][-0.0033963055 -0.0033961784 -0.0033977523 -0.0033992063 -0.0034003814 -0.003402136 -0.0034041523 -0.0034059079 -0.0034081219 -0.0034093717 -0.0034079608 -0.0034050043 -0.0034019116 -0.0033995209 -0.0033976333][-0.0033958079 -0.0033953174 -0.0033966231 -0.003397994 -0.0033993481 -0.0034010254 -0.0034024152 -0.0034033828 -0.0034051971 -0.0034064283 -0.0034055097 -0.0034030969 -0.0034004047 -0.0033979954 -0.0033961942][-0.0033952964 -0.0033941141 -0.0033948582 -0.0033959493 -0.0033973206 -0.0033988184 -0.0033999318 -0.0034008571 -0.0034016897 -0.0034023677 -0.0034017458 -0.0034000115 -0.0033981143 -0.0033961567 -0.0033945544][-0.0033946442 -0.0033930226 -0.0033933967 -0.0033942966 -0.0033954005 -0.0033966319 -0.0033976426 -0.003398438 -0.0033985809 -0.0033987006 -0.0033980801 -0.0033967998 -0.0033954049 -0.0033941164 -0.0033931169][-0.003393684 -0.0033919234 -0.0033924105 -0.0033931055 -0.0033938873 -0.003394657 -0.0033952387 -0.0033956687 -0.0033954433 -0.0033953993 -0.0033951297 -0.0033944126 -0.0033932577 -0.0033923332 -0.0033917066][-0.0033929409 -0.0033910857 -0.0033915155 -0.003391986 -0.0033925283 -0.003392827 -0.0033928675 -0.0033929406 -0.0033925364 -0.0033923481 -0.003392549 -0.0033924885 -0.0033917502 -0.0033911578 -0.0033910433][-0.0033923932 -0.0033904631 -0.0033908957 -0.0033912181 -0.0033914833 -0.0033915159 -0.0033914321 -0.0033911394 -0.00339034 -0.0033900538 -0.003390562 -0.0033910549 -0.0033908787 -0.0033909688 -0.0033913567][-0.0033924868 -0.0033902857 -0.0033907548 -0.0033910796 -0.0033912014 -0.0033911907 -0.0033909893 -0.0033905457 -0.0033897117 -0.0033892852 -0.0033897702 -0.0033903487 -0.0033905406 -0.0033911206 -0.0033917772][-0.003392624 -0.0033902139 -0.0033906614 -0.0033910607 -0.0033912489 -0.0033912424 -0.0033910589 -0.0033907597 -0.0033902503 -0.003389735 -0.003389894 -0.0033902486 -0.0033905341 -0.0033912736 -0.0033919662]]...]
INFO - root - 2017-12-10 02:09:02.371123: step 80710, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 60h:05m:26s remains)
INFO - root - 2017-12-10 02:09:10.956843: step 80720, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:07m:01s remains)
INFO - root - 2017-12-10 02:09:19.560183: step 80730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:15m:34s remains)
INFO - root - 2017-12-10 02:09:28.195205: step 80740, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 58h:16m:32s remains)
INFO - root - 2017-12-10 02:09:36.661977: step 80750, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:50m:18s remains)
INFO - root - 2017-12-10 02:09:45.241074: step 80760, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 59h:05m:02s remains)
INFO - root - 2017-12-10 02:09:53.739488: step 80770, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 58h:51m:29s remains)
INFO - root - 2017-12-10 02:10:02.240660: step 80780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 58h:51m:07s remains)
INFO - root - 2017-12-10 02:10:10.859310: step 80790, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:48m:43s remains)
INFO - root - 2017-12-10 02:10:19.486419: step 80800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:58m:12s remains)
2017-12-10 02:10:20.361352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033729726 -0.0033707719 -0.00337062 -0.003370598 -0.0033705863 -0.0033705668 -0.0033705868 -0.0033706995 -0.0033708119 -0.0033709051 -0.003370858 -0.0033708985 -0.0033707703 -0.0033706503 -0.0033704839][-0.0033718094 -0.0033693605 -0.0033691241 -0.0033691316 -0.0033691702 -0.0033692224 -0.0033693209 -0.0033694746 -0.0033695765 -0.0033696741 -0.0033696746 -0.0033696918 -0.0033696 -0.0033694615 -0.0033692929][-0.0033725156 -0.0033698324 -0.0033694883 -0.003369455 -0.0033694936 -0.0033695814 -0.0033697574 -0.0033699451 -0.0033700112 -0.0033699959 -0.0033699083 -0.003369791 -0.0033696031 -0.003369363 -0.0033691486][-0.0033727211 -0.0033699023 -0.003369502 -0.0033694652 -0.0033695425 -0.0033696636 -0.0033699058 -0.0033700985 -0.0033701509 -0.0033700538 -0.0033698583 -0.0033696517 -0.003369397 -0.0033691046 -0.003368855][-0.0033720061 -0.0033690042 -0.0033686005 -0.0033685013 -0.0033685178 -0.0033686 -0.0033688196 -0.0033690687 -0.0033692184 -0.0033692191 -0.0033691453 -0.0033690659 -0.0033689365 -0.0033687104 -0.0033685002][-0.0033704569 -0.0033673351 -0.0033670086 -0.003366902 -0.0033668431 -0.00336683 -0.00336698 -0.0033672652 -0.003367553 -0.0033677826 -0.0033679882 -0.0033682087 -0.0033683791 -0.0033683334 -0.0033682103][-0.0033686738 -0.0033655516 -0.003365316 -0.0033651537 -0.0033649544 -0.0033648238 -0.0033649593 -0.0033653707 -0.0033658973 -0.0033664659 -0.0033670315 -0.0033675705 -0.0033679989 -0.0033680936 -0.0033680343][-0.0033672568 -0.0033642079 -0.0033640265 -0.0033637807 -0.0033634126 -0.0033631295 -0.0033631392 -0.0033636352 -0.0033644889 -0.0033654599 -0.0033664242 -0.0033672755 -0.0033678862 -0.0033681102 -0.0033680981][-0.003366393 -0.0033636582 -0.0033636231 -0.0033633164 -0.0033627604 -0.0033623178 -0.0033621525 -0.003362661 -0.0033637013 -0.0033649502 -0.0033662522 -0.0033673448 -0.0033680559 -0.0033683537 -0.0033683812][-0.0033666857 -0.0033641462 -0.0033642077 -0.0033638375 -0.0033631558 -0.0033627353 -0.003362586 -0.0033630535 -0.0033640089 -0.0033653267 -0.003366726 -0.0033678287 -0.0033685216 -0.0033688312 -0.003368875][-0.0033681989 -0.0033657351 -0.0033658866 -0.0033655213 -0.0033648387 -0.003364512 -0.0033645062 -0.0033649118 -0.0033656573 -0.0033667274 -0.0033678571 -0.0033687125 -0.0033691854 -0.003369373 -0.0033693805][-0.003370137 -0.0033677628 -0.0033678818 -0.0033675032 -0.0033669064 -0.0033667022 -0.0033667637 -0.0033670398 -0.003367515 -0.0033682438 -0.0033689975 -0.0033694869 -0.0033696825 -0.003369736 -0.0033697123][-0.0033715044 -0.0033691281 -0.0033692564 -0.0033689742 -0.0033685525 -0.0033684156 -0.0033684233 -0.0033685239 -0.0033687255 -0.0033691111 -0.0033695181 -0.0033697227 -0.0033697637 -0.0033697931 -0.0033697758][-0.0033722213 -0.0033697654 -0.0033699113 -0.0033697747 -0.0033695225 -0.0033694324 -0.0033693977 -0.0033693661 -0.0033693467 -0.0033694555 -0.0033696152 -0.0033696315 -0.0033695963 -0.0033696166 -0.0033696236][-0.0033725244 -0.0033698443 -0.0033699654 -0.0033699202 -0.0033697998 -0.0033697537 -0.0033697479 -0.0033696694 -0.0033695444 -0.0033694962 -0.003369523 -0.0033694769 -0.0033694278 -0.0033694543 -0.0033694813]]...]
INFO - root - 2017-12-10 02:10:28.697583: step 80810, loss = 0.90, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 52h:01m:23s remains)
INFO - root - 2017-12-10 02:10:37.353394: step 80820, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 62h:27m:07s remains)
INFO - root - 2017-12-10 02:10:46.004725: step 80830, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:49m:51s remains)
INFO - root - 2017-12-10 02:10:54.535359: step 80840, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 58h:10m:39s remains)
INFO - root - 2017-12-10 02:11:02.916339: step 80850, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 59h:22m:49s remains)
INFO - root - 2017-12-10 02:11:11.343373: step 80860, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 58h:21m:34s remains)
INFO - root - 2017-12-10 02:11:19.782514: step 80870, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:05m:48s remains)
INFO - root - 2017-12-10 02:11:28.282857: step 80880, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:24m:55s remains)
INFO - root - 2017-12-10 02:11:36.848495: step 80890, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 57h:47m:29s remains)
INFO - root - 2017-12-10 02:11:45.459715: step 80900, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:00m:52s remains)
2017-12-10 02:11:46.411195: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0049405182 0.0032900108 0.001791198 0.00052125868 -0.0003842588 -0.00098108337 -0.0013609158 -0.0018247255 -0.002195091 -0.0025896661 -0.002699872 -0.0028527584 -0.0030577895 -0.0031842878 -0.0031179406][0.0062571969 0.0041082241 0.0022069162 0.00072127697 -0.00023131771 -0.00082672527 -0.0011544055 -0.0015290715 -0.0018114459 -0.002101518 -0.0021861368 -0.0024804503 -0.0027220433 -0.0028777507 -0.0027426512][0.0085409218 0.0066607622 0.0047671096 0.0029686687 0.0015161794 0.00032706908 -0.000463794 -0.0010899419 -0.0014276989 -0.001657876 -0.0017229322 -0.0018053822 -0.0018333432 -0.0018173286 -0.0013918097][0.010578988 0.0092251673 0.0078521539 0.0064080358 0.0049694162 0.0034023968 0.0021997273 0.0012006459 0.00058943196 0.00010216376 -0.00018250267 -0.0004017225 -0.00026609702 9.0652844e-05 0.00080298516][0.01111273 0.010233579 0.00945705 0.0086645661 0.00776131 0.0065331711 0.0054667061 0.0045086322 0.003764618 0.0029509643 0.0022772665 0.0017299729 0.0016991866 0.002041342 0.0026644927][0.010604457 0.0099897468 0.0095068617 0.0090566408 0.0084982291 0.0077396668 0.0069335466 0.0063532665 0.0058341818 0.0050743455 0.0042567765 0.0035805812 0.0034263122 0.0036588188 0.0040090419][0.0097462749 0.0095305005 0.0094639026 0.0092613054 0.0089493357 0.008290654 0.0074677076 0.0068124058 0.0061208429 0.0053478535 0.0044171717 0.0036716571 0.0034248719 0.0036069376 0.0038300829][0.0077188537 0.007804906 0.007992818 0.00802444 0.0079447832 0.0075910734 0.007097479 0.0065807467 0.0060263742 0.0050657419 0.0038927465 0.002906983 0.0024229533 0.0023447543 0.0023234419][0.0041762535 0.0043886686 0.0047057709 0.0048287734 0.0048275553 0.0046742344 0.0045630028 0.0045336522 0.0045648105 0.0039951419 0.003063625 0.0020692998 0.0013730775 0.00086141261 0.00040335837][0.0010993362 0.0011380513 0.001305647 0.0014918924 0.0017066298 0.0016811511 0.001638368 0.0016190058 0.0016693592 0.001469299 0.0010148825 0.00054390379 0.00013684505 -0.00045321719 -0.0011513012][-0.00086636934 -0.00076268683 -0.00057201879 -0.00063292985 -0.00084350328 -0.00113915 -0.0013682318 -0.0014527866 -0.0015936181 -0.0017688407 -0.0020476738 -0.0020560529 -0.0019669912 -0.0020806054 -0.002416586][-0.0016615675 -0.001433624 -0.0012366434 -0.001275877 -0.001508829 -0.0017814753 -0.0019714625 -0.0021285135 -0.0023279903 -0.0025015352 -0.0026517548 -0.0028047506 -0.0028423395 -0.0029277308 -0.0030535338][-0.002210611 -0.0019873318 -0.0017330477 -0.0016222012 -0.001683312 -0.0017171875 -0.0016305295 -0.0016490587 -0.001717406 -0.0017972107 -0.0018112981 -0.0019061009 -0.0020641112 -0.0023323745 -0.00261001][-0.0020640064 -0.0017269376 -0.0012596843 -0.0010424806 -0.00092782383 -0.0012055892 -0.0014371912 -0.0016391573 -0.0017652824 -0.0017650642 -0.0015316628 -0.0013157083 -0.0011959497 -0.0012579842 -0.0015221596][-0.0017518012 -0.001209473 -0.00048919464 -0.00016624178 -5.8717327e-05 -0.00047814823 -0.00086237513 -0.0013949135 -0.0016641741 -0.0016899453 -0.0015135431 -0.0012492964 -0.001237897 -0.0011477354 -0.0012788344]]...]
INFO - root - 2017-12-10 02:11:54.977516: step 80910, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 62h:44m:28s remains)
INFO - root - 2017-12-10 02:12:03.473066: step 80920, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:42m:03s remains)
INFO - root - 2017-12-10 02:12:12.128970: step 80930, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 60h:33m:34s remains)
INFO - root - 2017-12-10 02:12:20.734068: step 80940, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:00m:17s remains)
INFO - root - 2017-12-10 02:12:29.171952: step 80950, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:24m:27s remains)
INFO - root - 2017-12-10 02:12:37.932517: step 80960, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 61h:23m:25s remains)
INFO - root - 2017-12-10 02:12:46.475642: step 80970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:25m:19s remains)
INFO - root - 2017-12-10 02:12:54.903765: step 80980, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 57h:53m:03s remains)
INFO - root - 2017-12-10 02:13:03.534751: step 80990, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:50m:06s remains)
INFO - root - 2017-12-10 02:13:12.155554: step 81000, loss = 0.91, batch loss = 0.70 (9.7 examples/sec; 0.821 sec/batch; 57h:21m:31s remains)
2017-12-10 02:13:13.070879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030652073 -0.0030387328 -0.0030163061 -0.0029968151 -0.0029803037 -0.0029645935 -0.0029487724 -0.0029325155 -0.0029148117 -0.0028956551 -0.0028777805 -0.0028658407 -0.0028610057 -0.0028581107 -0.0028522294][-0.0030691717 -0.0030371111 -0.0030075391 -0.0029817517 -0.0029610482 -0.0029440455 -0.002928941 -0.0029146615 -0.0029002284 -0.0028850774 -0.002871044 -0.002861829 -0.0028581119 -0.0028555035 -0.0028491965][-0.0030821082 -0.0030441119 -0.0030083647 -0.0029771172 -0.0029525529 -0.0029345886 -0.0029213293 -0.0029110075 -0.0029020873 -0.002893209 -0.0028848401 -0.0028797158 -0.0028782045 -0.002876353 -0.0028706645][-0.003095845 -0.0030528684 -0.00301017 -0.0029714943 -0.0029419656 -0.0029226877 -0.0029110997 -0.0029043525 -0.0029002172 -0.0028977813 -0.0028967629 -0.0028984786 -0.0029018309 -0.0029042014 -0.002903722][-0.0031163124 -0.0030695493 -0.0030205138 -0.0029741742 -0.0029374021 -0.0029128944 -0.0028995732 -0.0028944828 -0.0028954374 -0.0029007392 -0.0029085041 -0.002917659 -0.0029272689 -0.0029362976 -0.0029443151][-0.0031521197 -0.0031055948 -0.0030526535 -0.0029993425 -0.0029548865 -0.0029238327 -0.0029059355 -0.0028994437 -0.0029024968 -0.0029140725 -0.0029308926 -0.0029490464 -0.0029671667 -0.0029850574 -0.00300404][-0.0032084147 -0.0031660004 -0.0031134987 -0.003056617 -0.0030066164 -0.0029703425 -0.0029485046 -0.0029402073 -0.0029447176 -0.002961386 -0.002985107 -0.0030108681 -0.003036669 -0.0030619979 -0.0030893851][-0.0032753684 -0.003240935 -0.0031957282 -0.003144305 -0.0030960098 -0.0030589271 -0.0030359633 -0.0030274084 -0.0030329218 -0.003052061 -0.0030796151 -0.0031091305 -0.0031380085 -0.0031660362 -0.0031959207][-0.0033359881 -0.003311639 -0.0032792538 -0.0032409567 -0.0032030088 -0.0031725927 -0.0031535048 -0.0031468514 -0.003153001 -0.0031708675 -0.0031954716 -0.0032207952 -0.0032449525 -0.0032679478 -0.0032916397][-0.0033767349 -0.0033625965 -0.0033446576 -0.0033224656 -0.003299722 -0.0032800373 -0.0032670407 -0.0032631787 -0.0032684561 -0.0032816243 -0.003298643 -0.0033158404 -0.0033311748 -0.0033449773 -0.0033588351][-0.0033970936 -0.0033907825 -0.0033835655 -0.0033742788 -0.0033641136 -0.0033544302 -0.003347653 -0.0033458339 -0.0033492863 -0.0033566146 -0.0033655481 -0.0033741181 -0.0033812281 -0.0033867157 -0.0033916074][-0.0034050611 -0.0034030678 -0.0034013116 -0.0033985195 -0.0033950186 -0.0033915376 -0.0033888561 -0.0033882253 -0.003389644 -0.0033917432 -0.0033947697 -0.0033976792 -0.0033998294 -0.0034010718 -0.003401225][-0.0034061731 -0.0034054879 -0.0034058092 -0.0034059598 -0.0034057095 -0.0034049589 -0.0034039204 -0.003403262 -0.003402486 -0.0034017793 -0.0034016094 -0.0034018473 -0.0034022862 -0.003402279 -0.0034014513][-0.0034046818 -0.0034041042 -0.003404547 -0.0034049382 -0.0034050755 -0.0034042916 -0.0034031433 -0.0034018906 -0.003400248 -0.0033981404 -0.0033958822 -0.0033951295 -0.0033950647 -0.003395173 -0.0033949646][-0.0034022136 -0.0034011547 -0.0034012853 -0.0034016273 -0.0034016648 -0.0034011658 -0.0034001314 -0.0033987456 -0.0033965823 -0.0033941176 -0.0033917434 -0.0033900824 -0.0033890272 -0.0033894563 -0.0033900929]]...]
INFO - root - 2017-12-10 02:13:21.477661: step 81010, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:33m:31s remains)
INFO - root - 2017-12-10 02:13:30.131241: step 81020, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 60h:41m:35s remains)
INFO - root - 2017-12-10 02:13:38.806232: step 81030, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 63h:57m:33s remains)
INFO - root - 2017-12-10 02:13:47.479949: step 81040, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 61h:57m:49s remains)
INFO - root - 2017-12-10 02:13:56.009661: step 81050, loss = 0.90, batch loss = 0.70 (10.1 examples/sec; 0.791 sec/batch; 55h:14m:00s remains)
INFO - root - 2017-12-10 02:14:04.733315: step 81060, loss = 0.90, batch loss = 0.69 (8.3 examples/sec; 0.964 sec/batch; 67h:20m:48s remains)
INFO - root - 2017-12-10 02:14:13.512523: step 81070, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 60h:23m:10s remains)
INFO - root - 2017-12-10 02:14:22.406448: step 81080, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.988 sec/batch; 68h:58m:12s remains)
INFO - root - 2017-12-10 02:14:31.156757: step 81090, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 62h:15m:45s remains)
INFO - root - 2017-12-10 02:14:39.915023: step 81100, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 60h:54m:59s remains)
2017-12-10 02:14:40.801861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033693847 -0.003366641 -0.0033667856 -0.0033670291 -0.0033673006 -0.0033675185 -0.003367699 -0.0033677849 -0.0033676233 -0.0033673632 -0.0033671798 -0.0033669209 -0.0033666838 -0.0033665171 -0.0033665083][-0.0033662615 -0.0033633637 -0.0033639108 -0.0033646917 -0.003365586 -0.003366401 -0.0033669998 -0.0033671516 -0.0033666964 -0.0033658422 -0.0033649816 -0.0033639607 -0.0033630333 -0.0033624666 -0.0033623255][-0.0033654948 -0.0033628526 -0.0033640023 -0.0033657886 -0.0033677923 -0.003369533 -0.0033706666 -0.0033709214 -0.0033700848 -0.0033682515 -0.00336606 -0.0033638317 -0.0033619725 -0.0033608226 -0.0033603469][-0.0033644068 -0.0033621253 -0.0033640885 -0.0033671712 -0.0033706506 -0.003373838 -0.0033760238 -0.0033767978 -0.0033758448 -0.0033730424 -0.0033693272 -0.0033655467 -0.003362108 -0.0033597827 -0.0033586896][-0.0033644636 -0.003362478 -0.0033651472 -0.0033694666 -0.0033740574 -0.0033783372 -0.0033815836 -0.0033831378 -0.0033821794 -0.0033787803 -0.003373998 -0.0033689351 -0.0033642615 -0.0033608219 -0.003358756][-0.0033649579 -0.0033633569 -0.00336659 -0.0033716345 -0.0033770264 -0.0033821615 -0.0033863164 -0.003388728 -0.0033880712 -0.0033842586 -0.0033785261 -0.0033724299 -0.003366814 -0.003362356 -0.0033594109][-0.0033650349 -0.0033636189 -0.0033671991 -0.003372509 -0.0033783263 -0.00338402 -0.0033888714 -0.0033919043 -0.0033914112 -0.0033874132 -0.0033814337 -0.0033749731 -0.0033689267 -0.003363878 -0.0033604307][-0.0033647604 -0.0033635104 -0.0033669113 -0.0033719291 -0.0033774758 -0.0033833012 -0.0033884845 -0.0033916819 -0.0033914119 -0.003387705 -0.0033821762 -0.003376306 -0.0033708818 -0.0033662776 -0.003363054][-0.0033643784 -0.00336312 -0.0033660131 -0.0033703793 -0.0033751132 -0.003380161 -0.0033850339 -0.00338811 -0.0033881923 -0.0033854903 -0.0033815284 -0.0033772604 -0.0033732385 -0.0033699584 -0.0033675053][-0.0033638827 -0.003362213 -0.0033645849 -0.0033681036 -0.0033718846 -0.0033759419 -0.0033798218 -0.0033824467 -0.0033830414 -0.003381782 -0.0033797668 -0.0033776152 -0.0033758213 -0.0033744434 -0.0033735053][-0.0033633569 -0.0033611858 -0.0033629944 -0.0033654007 -0.0033681439 -0.0033711242 -0.0033738541 -0.0033757351 -0.0033766243 -0.0033766902 -0.0033763936 -0.003376496 -0.0033772716 -0.0033782325 -0.0033794562][-0.0033629779 -0.0033603632 -0.0033614151 -0.0033627071 -0.0033641532 -0.0033657716 -0.0033672852 -0.0033684184 -0.003369417 -0.0033704538 -0.003371902 -0.0033741815 -0.0033775012 -0.0033808979 -0.0033840542][-0.0033632696 -0.0033599637 -0.00336042 -0.0033608002 -0.0033611271 -0.0033616191 -0.0033621928 -0.0033627737 -0.0033636831 -0.0033653169 -0.003367953 -0.0033718352 -0.0033767845 -0.0033817317 -0.0033861024][-0.0033636913 -0.0033599099 -0.0033601457 -0.003360152 -0.0033601229 -0.0033601427 -0.0033602393 -0.0033604458 -0.0033611183 -0.0033628161 -0.0033657264 -0.0033700911 -0.0033754783 -0.0033810048 -0.0033859981][-0.0033640694 -0.0033600847 -0.0033602237 -0.0033601425 -0.0033600067 -0.0033598773 -0.0033597823 -0.0033597916 -0.0033602004 -0.0033616051 -0.0033642347 -0.0033682731 -0.0033733221 -0.0033784013 -0.0033833864]]...]
INFO - root - 2017-12-10 02:14:49.277170: step 81110, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 57h:59m:11s remains)
INFO - root - 2017-12-10 02:14:57.998018: step 81120, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 61h:00m:59s remains)
INFO - root - 2017-12-10 02:15:06.636302: step 81130, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:30m:34s remains)
INFO - root - 2017-12-10 02:15:15.261636: step 81140, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 59h:58m:07s remains)
INFO - root - 2017-12-10 02:15:23.842783: step 81150, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 62h:58m:10s remains)
INFO - root - 2017-12-10 02:15:32.572638: step 81160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 61h:38m:37s remains)
INFO - root - 2017-12-10 02:15:41.147721: step 81170, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:53m:54s remains)
INFO - root - 2017-12-10 02:15:49.780466: step 81180, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:49m:58s remains)
INFO - root - 2017-12-10 02:15:58.432632: step 81190, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:17m:52s remains)
INFO - root - 2017-12-10 02:16:07.208349: step 81200, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:45m:19s remains)
2017-12-10 02:16:08.183887: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010259469 0.013023706 0.015377121 0.017301442 0.018619211 0.019321775 0.019621668 0.019245645 0.018261198 0.016366556 0.013769202 0.010293739 0.006303234 0.0023867197 -0.00083403219][0.01421486 0.017161081 0.0195142 0.021303965 0.022434896 0.023280662 0.023794515 0.023511721 0.022451574 0.020301355 0.017397584 0.013416627 0.0086432556 0.0037617607 -0.00023052469][0.015792206 0.018278962 0.020405032 0.022052389 0.023122866 0.024055677 0.024742937 0.024784625 0.024002524 0.021942697 0.018841283 0.01456663 0.0096463561 0.0046185236 0.00035825139][0.014642915 0.017097734 0.019133156 0.020782083 0.02189805 0.022810854 0.023644751 0.024009999 0.02354487 0.02179843 0.019014146 0.014955262 0.010136605 0.0050719883 0.00069837621][0.011422758 0.014070522 0.01641985 0.018225873 0.019380808 0.020299293 0.021162111 0.021588337 0.021295 0.019854123 0.017460985 0.013953026 0.0096720988 0.004929374 0.0006938919][0.0067763207 0.00945766 0.01194475 0.014147429 0.015716329 0.016771089 0.017594106 0.017953862 0.017653368 0.016278749 0.014129277 0.01117894 0.0075607616 0.0035226329 -7.7478355e-05][0.0023351589 0.0045344988 0.006800008 0.00900127 0.01071371 0.012054775 0.013024868 0.013381131 0.013039554 0.011808455 0.010018427 0.00764166 0.0047377795 0.0015570906 -0.0011723519][-0.00060632033 0.000743876 0.0022647448 0.0039800955 0.0055127954 0.0068486286 0.0077990144 0.0081546046 0.0078845816 0.006919181 0.0055708 0.0038378637 0.0017484326 -0.00045103393 -0.0022074659][-0.002199078 -0.0015442466 -0.00077709346 8.6700777e-05 0.00095335231 0.0018046971 0.0024216638 0.0026143889 0.002359885 0.0017904977 0.0010729965 0.00016215141 -0.00097263069 -0.002127138 -0.0029472061][-0.0030698474 -0.0028281056 -0.0025402033 -0.00225233 -0.001963126 -0.0016755997 -0.0014480038 -0.001409336 -0.0015650918 -0.001805737 -0.0020491453 -0.002340382 -0.0027154891 -0.0030831785 -0.0033035616][-0.0032431306 -0.0032597545 -0.0032823002 -0.003267732 -0.0032397853 -0.0032100456 -0.0031943806 -0.0032102326 -0.0032501179 -0.0032833861 -0.0033005483 -0.0033185706 -0.003344812 -0.0033657167 -0.00337501][-0.0031792491 -0.0032764517 -0.003358518 -0.0033781792 -0.0033792986 -0.003379504 -0.0033807317 -0.0033835419 -0.0033850882 -0.0033852609 -0.0033841103 -0.0033824136 -0.003381026 -0.0033800958 -0.0033794895][-0.0031550785 -0.0032680884 -0.0033591092 -0.0033777065 -0.0033791743 -0.0033797633 -0.0033811219 -0.0033839764 -0.0033870456 -0.0033891483 -0.0033889394 -0.0033878239 -0.003386033 -0.0033843643 -0.0033830302][-0.0032403085 -0.003328189 -0.0033724403 -0.0033800947 -0.0033806728 -0.0033818749 -0.0033827904 -0.0033849033 -0.0033873003 -0.0033898356 -0.0033909814 -0.0033902081 -0.0033884977 -0.0033869729 -0.0033858172][-0.0033114885 -0.0033621872 -0.0033790285 -0.0033814227 -0.0033821776 -0.0033831298 -0.0033843482 -0.0033860859 -0.0033875918 -0.0033894912 -0.00339085 -0.003390606 -0.00338922 -0.0033877434 -0.0033868118]]...]
INFO - root - 2017-12-10 02:16:16.641617: step 81210, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 62h:21m:53s remains)
INFO - root - 2017-12-10 02:16:25.161435: step 81220, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:57m:38s remains)
INFO - root - 2017-12-10 02:16:33.797648: step 81230, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:05m:25s remains)
INFO - root - 2017-12-10 02:16:42.405703: step 81240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:25m:35s remains)
INFO - root - 2017-12-10 02:16:50.903866: step 81250, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.732 sec/batch; 51h:04m:18s remains)
INFO - root - 2017-12-10 02:16:59.672354: step 81260, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 61h:54m:14s remains)
INFO - root - 2017-12-10 02:17:08.262426: step 81270, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 62h:15m:23s remains)
INFO - root - 2017-12-10 02:17:16.860753: step 81280, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:17m:15s remains)
INFO - root - 2017-12-10 02:17:25.424753: step 81290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:37m:58s remains)
INFO - root - 2017-12-10 02:17:34.033216: step 81300, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 61h:02m:15s remains)
2017-12-10 02:17:34.912550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015006828 0.00043750764 0.0036498827 0.0076454952 0.011738523 0.014775574 0.016111048 0.01520706 0.012197562 0.008313383 0.0045824787 0.0016756293 -0.00047573727 -0.0017942655 -0.0024912814][0.0015478393 0.0051626423 0.010637031 0.017416131 0.02437626 0.02988082 0.032439742 0.031081794 0.026028054 0.018960193 0.01169828 0.005763921 0.0015811611 -0.00077751116 -0.0018423898][0.0065085683 0.012466753 0.021162363 0.031801485 0.042590689 0.051107265 0.055060558 0.053038564 0.045207024 0.033988405 0.022060314 0.011812169 0.0043167472 3.8503902e-05 -0.0016996171][0.012581453 0.021925462 0.034788288 0.0498584 0.06454441 0.07547792 0.079680376 0.075881846 0.064612366 0.048943069 0.032293528 0.017838702 0.0072488375 0.001117964 -0.0013808876][0.019116178 0.032018445 0.049021531 0.068144664 0.085920818 0.098262548 0.10181253 0.095515 0.080754235 0.061173126 0.040718947 0.022898428 0.0099131633 0.0024244159 -0.00070078811][0.023910763 0.040143728 0.060851932 0.08320196 0.10291132 0.11547938 0.11760578 0.10872223 0.091095641 0.068823971 0.04617298 0.026598822 0.0124208 0.0042878725 0.00085090683][0.025534989 0.043712217 0.066860206 0.091413863 0.11218605 0.12430279 0.12492453 0.11417484 0.095106952 0.072011322 0.049136959 0.029566979 0.015425747 0.0073609957 0.003817965][0.024970217 0.043351918 0.066947326 0.091836 0.11251848 0.1239436 0.12369815 0.11250834 0.093923591 0.072202936 0.05129607 0.033782709 0.021203216 0.014021091 0.01063293][0.022896621 0.040094387 0.062237114 0.085667022 0.10514502 0.11581837 0.11566727 0.10575095 0.089643016 0.0712508 0.053912479 0.039801437 0.029831579 0.024273803 0.021402445][0.019579954 0.034731776 0.054175016 0.0748116 0.092129745 0.10190448 0.1025352 0.09524215 0.083189271 0.069585711 0.056967411 0.046978351 0.0399856 0.036006924 0.033482224][0.014943864 0.027524196 0.04365151 0.060899794 0.0756551 0.084509432 0.086278073 0.082138255 0.074748822 0.066422395 0.058807336 0.052976903 0.048857372 0.046174012 0.043558814][0.00972324 0.019188266 0.03151466 0.044953007 0.056843929 0.064615346 0.067511767 0.066540629 0.063705564 0.06047786 0.057630133 0.055564962 0.0537561 0.051763635 0.048552305][0.0050559817 0.011404744 0.019845819 0.029284058 0.038025945 0.044417754 0.048000243 0.049521513 0.050270192 0.050963245 0.051684584 0.052294429 0.051994178 0.050280668 0.046529938][0.0013841263 0.0051746471 0.010329468 0.016228903 0.021905238 0.026478266 0.029846312 0.032536428 0.035227969 0.038111545 0.040837158 0.04287022 0.043327343 0.041761819 0.037952978][-0.0010909564 0.00089856214 0.0036694251 0.00693213 0.010216292 0.013113698 0.015657471 0.018218774 0.021139169 0.024379173 0.027451105 0.029721843 0.030403143 0.029150343 0.025962679]]...]
INFO - root - 2017-12-10 02:17:43.298178: step 81310, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 61h:56m:31s remains)
INFO - root - 2017-12-10 02:17:51.899888: step 81320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 60h:42m:12s remains)
INFO - root - 2017-12-10 02:18:00.527609: step 81330, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.870 sec/batch; 60h:42m:04s remains)
INFO - root - 2017-12-10 02:18:09.244686: step 81340, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 60h:53m:14s remains)
INFO - root - 2017-12-10 02:18:17.936675: step 81350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 59h:48m:30s remains)
INFO - root - 2017-12-10 02:18:26.423529: step 81360, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:42m:32s remains)
INFO - root - 2017-12-10 02:18:35.079999: step 81370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:05m:16s remains)
INFO - root - 2017-12-10 02:18:43.606564: step 81380, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 59h:36m:32s remains)
INFO - root - 2017-12-10 02:18:52.165792: step 81390, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 62h:15m:11s remains)
INFO - root - 2017-12-10 02:19:00.831946: step 81400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:51m:29s remains)
2017-12-10 02:19:01.733757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033960226 -0.0033954626 -0.0033951532 -0.00339351 -0.0033903688 -0.0033873012 -0.0033862344 -0.0033876481 -0.003389948 -0.0033916852 -0.003392962 -0.0033934333 -0.0033933232 -0.0033931434 -0.0033930307][-0.0033965388 -0.0033964885 -0.0033963716 -0.0033932987 -0.0033873764 -0.0033821848 -0.0033806441 -0.003383178 -0.0033872377 -0.0033902447 -0.0033923411 -0.0033934582 -0.003393685 -0.0033935534 -0.0033934442][-0.0033973218 -0.0033972124 -0.0033965656 -0.0033898097 -0.00337923 -0.0033707267 -0.0033689023 -0.0033743607 -0.003381955 -0.0033881529 -0.0033920091 -0.0033936356 -0.0033941269 -0.0033940179 -0.0033938696][-0.0033973015 -0.0033971651 -0.0033953737 -0.0033836472 -0.0033666398 -0.0033528942 -0.0033501307 -0.0033597073 -0.0033731645 -0.0033841464 -0.0033906908 -0.0033933909 -0.0033942619 -0.0033941178 -0.0033938687][-0.003397054 -0.0033965262 -0.0033932936 -0.0033769025 -0.003353199 -0.0033338156 -0.0033296247 -0.0033430336 -0.0033627772 -0.0033792914 -0.0033891895 -0.0033933236 -0.0033946163 -0.0033943022 -0.0033939113][-0.0033966105 -0.0033956286 -0.0033913846 -0.0033727456 -0.0033455123 -0.0033224963 -0.003316296 -0.0033303509 -0.0033541033 -0.0033751929 -0.0033879566 -0.0033934538 -0.0033950196 -0.0033945434 -0.0033939476][-0.0033960012 -0.0033946233 -0.0033904149 -0.0033730809 -0.0033467838 -0.0033234016 -0.0033152832 -0.0033269329 -0.0033506453 -0.0033732641 -0.0033874682 -0.0033936081 -0.0033953032 -0.003394787 -0.0033939455][-0.003395377 -0.0033938827 -0.0033908295 -0.0033781268 -0.0033570186 -0.00333726 -0.0033282803 -0.0033356987 -0.0033547524 -0.0033747081 -0.0033880416 -0.0033937618 -0.0033953504 -0.0033948233 -0.0033938538][-0.0033949525 -0.0033935749 -0.0033924344 -0.0033855175 -0.0033719288 -0.0033579355 -0.0033496195 -0.0033530961 -0.0033652827 -0.0033794774 -0.0033898619 -0.0033941923 -0.0033951984 -0.0033946084 -0.0033936983][-0.0033946165 -0.0033936396 -0.0033942566 -0.0033920235 -0.0033850386 -0.0033766124 -0.0033700713 -0.0033705458 -0.0033766793 -0.0033847627 -0.0033914635 -0.0033943059 -0.0033947376 -0.0033941986 -0.0033934107][-0.0033942584 -0.0033933634 -0.003394878 -0.0033954093 -0.0033931478 -0.003388939 -0.0033841867 -0.0033828444 -0.0033849333 -0.0033887047 -0.0033922377 -0.0033939 -0.0033940605 -0.0033936123 -0.0033930768][-0.0033939895 -0.0033929807 -0.0033942983 -0.0033955814 -0.0033956172 -0.0033938612 -0.0033907907 -0.0033891872 -0.0033893825 -0.0033908398 -0.00339255 -0.0033934053 -0.0033934866 -0.0033931769 -0.0033928058][-0.0033938061 -0.0033926824 -0.0033933211 -0.0033942587 -0.0033948931 -0.0033945707 -0.0033930282 -0.0033916044 -0.00339105 -0.0033915497 -0.0033923464 -0.0033928545 -0.003392936 -0.003392779 -0.0033925672][-0.003393674 -0.003392488 -0.003392647 -0.0033930112 -0.0033934936 -0.0033937334 -0.00339312 -0.0033920687 -0.0033914126 -0.0033915141 -0.0033918561 -0.0033922342 -0.0033923974 -0.0033923907 -0.0033923096][-0.0033936107 -0.0033924396 -0.0033923748 -0.003392383 -0.0033924899 -0.0033927576 -0.003392606 -0.0033919862 -0.003391491 -0.0033913956 -0.0033915204 -0.0033918121 -0.003392047 -0.0033921218 -0.0033921027]]...]
INFO - root - 2017-12-10 02:19:10.340138: step 81410, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:33m:04s remains)
INFO - root - 2017-12-10 02:19:18.864581: step 81420, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:08m:08s remains)
INFO - root - 2017-12-10 02:19:27.584514: step 81430, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:09m:03s remains)
INFO - root - 2017-12-10 02:19:36.240089: step 81440, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:32m:22s remains)
INFO - root - 2017-12-10 02:19:44.822706: step 81450, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:18m:23s remains)
INFO - root - 2017-12-10 02:19:53.390206: step 81460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:25m:30s remains)
INFO - root - 2017-12-10 02:20:02.053085: step 81470, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 57h:04m:30s remains)
INFO - root - 2017-12-10 02:20:10.667819: step 81480, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 62h:18m:30s remains)
INFO - root - 2017-12-10 02:20:19.355013: step 81490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:06m:22s remains)
INFO - root - 2017-12-10 02:20:27.890110: step 81500, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:20m:21s remains)
2017-12-10 02:20:28.899016: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0029570672 0.0040844278 0.005188914 0.0059476551 0.0062512765 0.0060100472 0.0056593316 0.0057201376 0.00637615 0.007386297 0.0083065648 0.0086578531 0.00808734 0.0063500032 0.0037170581][0.007582048 0.010979589 0.01394272 0.01577667 0.016317312 0.015715292 0.014788911 0.014255945 0.014497228 0.015379909 0.0162776 0.016484989 0.015459517 0.012731453 0.008492169][0.013700062 0.019834073 0.025624231 0.029609846 0.030928357 0.030474583 0.029498978 0.028610473 0.028059212 0.027869442 0.027447345 0.026165813 0.023485472 0.019032994 0.013217786][0.019241128 0.027885875 0.035906766 0.04175825 0.044392116 0.044707615 0.043978933 0.042953964 0.041912396 0.04069113 0.0386425 0.035612866 0.031182555 0.02491343 0.017274888][0.023189586 0.033580828 0.043034978 0.049888816 0.053190291 0.053955913 0.05348593 0.052565038 0.051265988 0.049192831 0.045903616 0.0416974 0.036115125 0.028702674 0.019952489][0.024875397 0.036146678 0.0464308 0.053813566 0.057561781 0.058415975 0.05776279 0.05647869 0.054520477 0.051735282 0.04768819 0.042855114 0.03674731 0.028968664 0.019979337][0.024987433 0.03637689 0.04719789 0.055221997 0.059379734 0.060193621 0.059003264 0.056667376 0.053217918 0.04889321 0.043697782 0.038379896 0.032394931 0.025089504 0.016798805][0.022687593 0.03333424 0.044048056 0.052543379 0.057153527 0.058101237 0.056635492 0.053502213 0.0485315 0.04251669 0.036221318 0.030367659 0.024577424 0.018126165 0.011286479][0.017562913 0.026453571 0.035744194 0.043555312 0.048236653 0.049568012 0.048209362 0.044821005 0.039478552 0.03295745 0.026389811 0.020507731 0.015265428 0.01004881 0.005114208][0.010640186 0.016721025 0.023479458 0.029572116 0.033479404 0.034809988 0.0339148 0.031195411 0.026781186 0.021332106 0.015918536 0.011137414 0.0071125375 0.0034854871 0.00047608768][0.0040793112 0.0073034409 0.010977097 0.014465202 0.016853955 0.017700294 0.017022474 0.015205262 0.012447642 0.0090965647 0.0058358368 0.0030688413 0.00087318732 -0.00091686891 -0.0022407421][-0.00038406882 0.00087303878 0.0023215052 0.003735068 0.004684709 0.0049409093 0.004515673 0.0035869388 0.0023018958 0.00085786381 -0.00042645657 -0.0014450329 -0.0021904742 -0.0027521178 -0.0031281926][-0.0028045913 -0.0024461483 -0.0020574373 -0.0016707926 -0.0013973338 -0.0013362446 -0.0015248501 -0.0018860213 -0.0023074211 -0.0027011526 -0.0029963567 -0.003187865 -0.003287405 -0.0033347632 -0.0033567925][-0.0033581085 -0.0033158949 -0.0032675718 -0.0032209794 -0.0031885025 -0.0031855446 -0.0032132163 -0.0032678442 -0.0033213231 -0.0033538835 -0.0033646252 -0.0033686224 -0.0033686552 -0.0033675078 -0.0033675006][-0.0033946855 -0.0033904582 -0.0033862356 -0.0033816039 -0.0033772716 -0.0033727679 -0.0033692268 -0.0033679132 -0.003368333 -0.0033695383 -0.0033709458 -0.0033718336 -0.0033720257 -0.0033719251 -0.0033722317]]...]
INFO - root - 2017-12-10 02:20:37.277705: step 81510, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 58h:53m:49s remains)
INFO - root - 2017-12-10 02:20:45.632174: step 81520, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:40m:13s remains)
INFO - root - 2017-12-10 02:20:54.259930: step 81530, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:35m:28s remains)
INFO - root - 2017-12-10 02:21:02.893962: step 81540, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 62h:49m:22s remains)
INFO - root - 2017-12-10 02:21:11.695473: step 81550, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:01m:39s remains)
INFO - root - 2017-12-10 02:21:20.227263: step 81560, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:08m:40s remains)
INFO - root - 2017-12-10 02:21:28.929975: step 81570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 59h:59m:01s remains)
INFO - root - 2017-12-10 02:21:37.509597: step 81580, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 61h:52m:59s remains)
INFO - root - 2017-12-10 02:21:46.150217: step 81590, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:50m:00s remains)
INFO - root - 2017-12-10 02:21:54.906242: step 81600, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:51m:35s remains)
2017-12-10 02:21:55.750347: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41995949 0.39028448 0.36359754 0.3431249 0.32824036 0.32163012 0.31817356 0.3132503 0.30477768 0.29329088 0.27986613 0.26595289 0.25161105 0.23617986 0.21682607][0.40266857 0.37019086 0.34191513 0.32232475 0.30940074 0.30702817 0.30836087 0.30749995 0.30194902 0.29118079 0.27613863 0.25853062 0.23952125 0.21931417 0.19527498][0.3804687 0.34578145 0.31610468 0.29699656 0.28678802 0.28823873 0.29388946 0.29745805 0.29485804 0.28470141 0.26741663 0.24556363 0.22138101 0.19561672 0.16700242][0.36219263 0.325877 0.29482424 0.27637005 0.26829845 0.27297702 0.28293681 0.29107413 0.29227111 0.28313038 0.26354182 0.23667115 0.20602466 0.17405501 0.14079955][0.34622034 0.31154132 0.28101739 0.26273271 0.25593612 0.26264173 0.27530602 0.28641894 0.29041088 0.28289321 0.26175383 0.23042144 0.19361217 0.15593746 0.11824559][0.33450267 0.30253631 0.27257243 0.25537664 0.24978316 0.25637639 0.26959404 0.28205469 0.28833467 0.28203031 0.259646 0.22478588 0.18305458 0.14087711 0.10023268][0.32371631 0.29836166 0.27173457 0.25546941 0.24921158 0.25419825 0.26609483 0.27791449 0.28477544 0.27889964 0.25629744 0.21916325 0.17418787 0.12878114 0.086019896][0.31233948 0.29468676 0.27260143 0.258497 0.25232047 0.25443465 0.26310045 0.27367046 0.28054687 0.27503625 0.25238815 0.21468093 0.1681162 0.12068603 0.077084005][0.29664549 0.28834867 0.27275214 0.26209208 0.25637358 0.25612187 0.26075605 0.26839334 0.2742103 0.26859495 0.2474595 0.21067436 0.16430563 0.11678392 0.073260009][0.27286831 0.274215 0.26696971 0.26144513 0.25826892 0.25770897 0.25971469 0.26359764 0.26653075 0.26010138 0.239819 0.20472075 0.16013969 0.11424731 0.071804963][0.240851 0.250365 0.2515395 0.25199986 0.2527262 0.25319234 0.25486085 0.25760528 0.25943634 0.25198275 0.23241375 0.1989605 0.15607396 0.11171538 0.070624277][0.2043805 0.22128104 0.23144905 0.23887646 0.24412642 0.2474528 0.24994588 0.25231889 0.25314122 0.24487624 0.22621718 0.19384997 0.15239033 0.1094375 0.068988107][0.16739309 0.18985204 0.20794632 0.22216067 0.23265572 0.23897821 0.24304521 0.24595279 0.24638432 0.23824562 0.22023807 0.18987474 0.15082292 0.10986943 0.070874438][0.13407817 0.15958542 0.1827684 0.20178692 0.21702911 0.22736238 0.23407125 0.23775166 0.23804739 0.230928 0.21460015 0.18673526 0.15104775 0.11372387 0.0776869][0.11138813 0.13718921 0.16263354 0.18445948 0.20194609 0.21419927 0.22378474 0.22895753 0.22982454 0.22392689 0.20946938 0.18513219 0.15393063 0.12150804 0.089993708]]...]
INFO - root - 2017-12-10 02:22:04.309157: step 81610, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 62h:16m:40s remains)
INFO - root - 2017-12-10 02:22:12.885519: step 81620, loss = 0.88, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 60h:48m:40s remains)
INFO - root - 2017-12-10 02:22:21.530499: step 81630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:38m:07s remains)
INFO - root - 2017-12-10 02:22:30.160596: step 81640, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 61h:37m:45s remains)
INFO - root - 2017-12-10 02:22:38.976369: step 81650, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 62h:03m:06s remains)
INFO - root - 2017-12-10 02:22:47.476796: step 81660, loss = 0.91, batch loss = 0.70 (8.9 examples/sec; 0.898 sec/batch; 62h:32m:31s remains)
INFO - root - 2017-12-10 02:22:56.103517: step 81670, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 60h:28m:12s remains)
INFO - root - 2017-12-10 02:23:04.876066: step 81680, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:26m:16s remains)
INFO - root - 2017-12-10 02:23:13.548734: step 81690, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:38m:04s remains)
INFO - root - 2017-12-10 02:23:22.189070: step 81700, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:26m:27s remains)
2017-12-10 02:23:23.164991: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0066855764 0.0054495744 0.0044128131 0.0038203949 0.0036423586 0.0038983484 0.0043601794 0.0046828678 0.0045937859 0.0041144444 0.0034978276 0.0029881157 0.0026511771 0.002509509 0.0024032828][0.0047827475 0.0034920748 0.0025527992 0.0021607748 0.0022309653 0.0026314233 0.0030208945 0.0031215379 0.0027869001 0.0021296835 0.0014009012 0.00086483522 0.00060030469 0.00057270774 0.00060414593][0.0025084422 0.0013637289 0.00071866182 0.0006819244 0.0011158457 0.0017518706 0.002183432 0.0021745942 0.0016406837 0.00075536384 -0.00015685963 -0.00077799917 -0.0010000211 -0.00089658261 -0.00067450106][0.0004755652 -0.00029147789 -0.00046084216 2.358132e-05 0.00094511523 0.0019110551 0.0024543374 0.0023592694 0.0016186137 0.00046334974 -0.00069199386 -0.0014827233 -0.0017508166 -0.0015816756 -0.0012210868][-0.0011749826 -0.0014488436 -0.0010802748 -2.120994e-05 0.001432738 0.0028035596 0.0035586713 0.0034514389 0.0025210402 0.0010697262 -0.00039083976 -0.0014478397 -0.0018743025 -0.0017455429 -0.0013460496][-0.0022673102 -0.0021347483 -0.0013704826 0.00013423269 0.0020451809 0.0038144414 0.004826081 0.0047731856 0.003716544 0.0020092723 0.00024979608 -0.0011011807 -0.0017670664 -0.0018108507 -0.0015231044][-0.0028551358 -0.0025351113 -0.001650808 -2.6087277e-05 0.0020532736 0.004062023 0.0053358888 0.0054663206 0.0044396985 0.0026109598 0.00063500437 -0.00094323838 -0.0018112927 -0.0020385219 -0.001919672][-0.0030986206 -0.0028083147 -0.0020725427 -0.00068426249 0.0011831021 0.0030968606 0.0044471696 0.0047681564 0.0039366037 0.0022318771 0.00029847422 -0.0012739694 -0.002177431 -0.002493544 -0.0025000959][-0.0032072279 -0.003028193 -0.0025657208 -0.0016483006 -0.00031882781 0.0011658284 0.0023351677 0.0027372364 0.0021673825 0.00082358229 -0.00073741213 -0.0019905344 -0.0026974399 -0.0029591345 -0.003002343][-0.0032847023 -0.0032028537 -0.0029833242 -0.002513661 -0.0017668218 -0.00085820444 -7.1844552e-05 0.00024266751 -9.9222874e-05 -0.00096608954 -0.0019572056 -0.0027120132 -0.0031121387 -0.0032522825 -0.0032789889][-0.0033394485 -0.00330967 -0.0032366794 -0.0030634413 -0.0027481895 -0.0023209834 -0.0019181438 -0.0017543675 -0.0019372441 -0.0023792521 -0.0028460808 -0.0031641291 -0.0033112662 -0.0033576326 -0.0033662757][-0.003381032 -0.0033688687 -0.0033482802 -0.0033013434 -0.0032062053 -0.0030641602 -0.0029190569 -0.0028603794 -0.0029320475 -0.00309292 -0.0032480997 -0.0033380869 -0.0033713623 -0.0033785044 -0.0033792034][-0.0033903816 -0.0033886698 -0.0033875222 -0.0033821734 -0.00336545 -0.0033359767 -0.0033032123 -0.0032895706 -0.00330588 -0.00333943 -0.0033671346 -0.0033790306 -0.0033818563 -0.0033822479 -0.003381582][-0.0033937325 -0.0033932265 -0.0033945777 -0.0033957637 -0.0033949167 -0.0033923171 -0.0033881967 -0.003385111 -0.0033836213 -0.003384168 -0.0033853136 -0.0033861399 -0.0033862714 -0.0033856612 -0.0033839608][-0.0033957635 -0.003395393 -0.0033963642 -0.0033973183 -0.0033971835 -0.0033959146 -0.0033939141 -0.0033916894 -0.0033900121 -0.003389643 -0.0033898947 -0.0033902007 -0.003389712 -0.003389176 -0.0033879073]]...]
INFO - root - 2017-12-10 02:23:31.677836: step 81710, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:17m:47s remains)
INFO - root - 2017-12-10 02:23:39.996725: step 81720, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:47m:39s remains)
INFO - root - 2017-12-10 02:23:48.489437: step 81730, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 58h:27m:53s remains)
INFO - root - 2017-12-10 02:23:56.953996: step 81740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 59h:57m:27s remains)
INFO - root - 2017-12-10 02:24:05.589178: step 81750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:28m:21s remains)
INFO - root - 2017-12-10 02:24:14.102958: step 81760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 59h:50m:24s remains)
INFO - root - 2017-12-10 02:24:22.691264: step 81770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:21m:20s remains)
INFO - root - 2017-12-10 02:24:31.242555: step 81780, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 59h:28m:28s remains)
INFO - root - 2017-12-10 02:24:40.055147: step 81790, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:33m:37s remains)
INFO - root - 2017-12-10 02:24:48.734908: step 81800, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:21m:36s remains)
2017-12-10 02:24:49.609326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003397468 -0.0033965223 -0.0033970182 -0.0033973453 -0.0033977288 -0.0033978557 -0.0033979376 -0.0033980617 -0.0033984187 -0.0033985812 -0.0033986913 -0.0033987795 -0.0033980648 -0.0033962755 -0.0033942221][-0.0033956131 -0.0033945739 -0.0033949742 -0.0033948154 -0.0033941856 -0.0033931648 -0.0033927576 -0.0033932577 -0.0033942843 -0.0033949325 -0.0033955837 -0.0033959269 -0.0033953923 -0.0033931716 -0.0033901865][-0.0033954168 -0.0033936212 -0.0033924168 -0.003390264 -0.0033872398 -0.0033840088 -0.003381693 -0.0033810707 -0.0033825438 -0.0033855613 -0.0033887418 -0.0033903487 -0.0033899255 -0.0033864712 -0.0033818712][-0.0033943427 -0.0033903231 -0.0033841666 -0.00337329 -0.0033585792 -0.0033435991 -0.0033338317 -0.0033335956 -0.0033430308 -0.0033566759 -0.0033702052 -0.0033786679 -0.0033821994 -0.0033812714 -0.0033774055][-0.0033920112 -0.0033820793 -0.0033626533 -0.0033293949 -0.0032857968 -0.0032426154 -0.0032142457 -0.0032124403 -0.0032392866 -0.003280981 -0.0033223685 -0.0033510439 -0.0033654652 -0.0033686103 -0.0033675407][-0.0033755193 -0.0033508642 -0.0033072454 -0.0032388209 -0.0031550054 -0.0030763089 -0.0030281437 -0.0030272382 -0.0030749675 -0.00314654 -0.0032191593 -0.0032743001 -0.0033058845 -0.0033173959 -0.0033181116][-0.0033537697 -0.0033092955 -0.0032359266 -0.0031272587 -0.0030023679 -0.0028943478 -0.0028354358 -0.0028413194 -0.0029084766 -0.0030033647 -0.0030978585 -0.0031685419 -0.0032081991 -0.0032227091 -0.0032218911][-0.0033330799 -0.0032686568 -0.0031672767 -0.0030286047 -0.0028823018 -0.0027683889 -0.0027172633 -0.0027348949 -0.0028103513 -0.0029111768 -0.0030077889 -0.0030781305 -0.003116936 -0.0031309405 -0.0031272816][-0.003329542 -0.0032533559 -0.0031377561 -0.0029925629 -0.0028546653 -0.0027610166 -0.0027308539 -0.0027585886 -0.0028260825 -0.0029096135 -0.0029869308 -0.0030424446 -0.0030739398 -0.0030871902 -0.003085247][-0.0033514674 -0.0032842373 -0.0031822545 -0.0030617379 -0.0029557887 -0.0028937096 -0.0028843579 -0.0029143209 -0.002964345 -0.003019168 -0.0030669547 -0.0030995435 -0.0031167448 -0.0031245458 -0.0031232722][-0.0033788807 -0.0033349835 -0.0032674337 -0.0031887772 -0.003124442 -0.0030938156 -0.0030983728 -0.0031243241 -0.0031563311 -0.0031863016 -0.0032098868 -0.0032241638 -0.0032302486 -0.0032316672 -0.0032285375][-0.0033969148 -0.0033758173 -0.0033439347 -0.0033071695 -0.0032793158 -0.0032701723 -0.0032789637 -0.003296233 -0.0033132685 -0.0033271883 -0.0033366182 -0.0033409765 -0.0033410266 -0.0033390033 -0.003335481][-0.003405964 -0.003397655 -0.0033869229 -0.0033751661 -0.0033673393 -0.0033660824 -0.0033709158 -0.0033775815 -0.0033838849 -0.0033891709 -0.0033925569 -0.0033941439 -0.0033939269 -0.0033928144 -0.0033910652][-0.0034106907 -0.0034074765 -0.0034038189 -0.0033999458 -0.0033980003 -0.0033980254 -0.0033995016 -0.0034013754 -0.0034035572 -0.0034055242 -0.0034067645 -0.003407941 -0.0034087144 -0.0034088911 -0.003408323][-0.0034125606 -0.0034117703 -0.0034106816 -0.0034090313 -0.0034081757 -0.0034083819 -0.0034091182 -0.0034098406 -0.0034108954 -0.0034117834 -0.0034121191 -0.0034129464 -0.0034137219 -0.0034142295 -0.0034141033]]...]
INFO - root - 2017-12-10 02:24:58.234357: step 81810, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:25m:47s remains)
INFO - root - 2017-12-10 02:25:06.840962: step 81820, loss = 0.89, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 55h:50m:45s remains)
INFO - root - 2017-12-10 02:25:15.451884: step 81830, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:08m:56s remains)
INFO - root - 2017-12-10 02:25:24.027489: step 81840, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:09m:56s remains)
INFO - root - 2017-12-10 02:25:32.628764: step 81850, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:50m:49s remains)
INFO - root - 2017-12-10 02:25:41.109016: step 81860, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:25m:25s remains)
INFO - root - 2017-12-10 02:25:49.797474: step 81870, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 58h:14m:14s remains)
INFO - root - 2017-12-10 02:25:58.501266: step 81880, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:10m:44s remains)
INFO - root - 2017-12-10 02:26:07.244951: step 81890, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:27m:21s remains)
INFO - root - 2017-12-10 02:26:15.972250: step 81900, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 60h:40m:11s remains)
2017-12-10 02:26:16.850161: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.02642514 0.025489094 0.024635546 0.024213606 0.023864573 0.023668181 0.023371371 0.022739811 0.021503976 0.019723967 0.017253548 0.014030922 0.010137151 0.006222127 0.0026828998][0.0312464 0.029695159 0.028253997 0.027152125 0.026362035 0.025927676 0.025404772 0.024638359 0.023347357 0.021392053 0.018628543 0.015093496 0.010932107 0.0067025712 0.0028748557][0.029522849 0.028055815 0.026627855 0.025369482 0.02444079 0.023865651 0.02339652 0.022826497 0.021728583 0.020012174 0.017481446 0.014198814 0.010257212 0.0062495619 0.0026343332][0.026049528 0.025039693 0.024088249 0.023074942 0.022178778 0.021432282 0.020820271 0.020261897 0.019370427 0.018089043 0.015980193 0.0130827 0.0094428565 0.0056726467 0.0022645879][0.021498507 0.02093442 0.020600058 0.020194093 0.019684399 0.018975291 0.018266244 0.017613718 0.016801359 0.015765442 0.014087053 0.011712923 0.0085544288 0.0051332945 0.0019277993][0.01638836 0.016114801 0.016188556 0.016415741 0.016582137 0.016487578 0.016230455 0.015768334 0.015017897 0.013986237 0.012418484 0.010268018 0.0074778339 0.0044507273 0.0015777324][0.011343938 0.011041131 0.01119934 0.011844146 0.012684654 0.013417261 0.014009492 0.014273955 0.014020631 0.01315021 0.011558976 0.009319419 0.0065085366 0.0035900816 0.00095069758][0.0075973263 0.0069267368 0.006860842 0.0075686453 0.0087050432 0.0098797949 0.011002447 0.011814035 0.012083445 0.01164437 0.010346053 0.0082706762 0.0055848435 0.0028162084 0.00036369404][0.0058629354 0.0046224585 0.003991182 0.0041426765 0.0048296582 0.0057696691 0.0068096193 0.0076887896 0.0082052676 0.0081642382 0.0073574325 0.00581343 0.0036971995 0.0014628314 -0.00050027017][0.005612215 0.0036181018 0.0020501085 0.0013068949 0.0011808763 0.0014560826 0.0020020537 0.0025995139 0.0030616128 0.003205152 0.0028778512 0.0020635126 0.0008418595 -0.00048713968 -0.0016731457][0.0056662969 0.0031342104 0.0008362222 -0.00063295569 -0.0014012814 -0.0015968332 -0.0014306537 -0.0011411526 -0.000873921 -0.00073871785 -0.00083045824 -0.0011726785 -0.0016989375 -0.0022457468 -0.0027356674][0.0050332826 0.0023143792 -0.00024053082 -0.0018838542 -0.00277297 -0.0030825795 -0.0030932731 -0.0029832404 -0.0028487814 -0.0027524217 -0.0027373764 -0.0028068724 -0.0029420021 -0.0031009805 -0.0032459786][0.0039517004 0.0014823505 -0.00086709508 -0.0023346194 -0.0030959866 -0.0033444199 -0.0033989572 -0.0034018778 -0.0033991602 -0.0033933751 -0.0033912191 -0.0033901606 -0.0033922638 -0.0033956417 -0.0034007228][0.0022388271 0.00024052244 -0.0015956265 -0.002713596 -0.0032475749 -0.0033853124 -0.0034007304 -0.0034042192 -0.00340708 -0.0034083908 -0.0034100311 -0.0034094115 -0.0034094325 -0.0034095526 -0.0034099685][0.0012895262 -0.00045731035 -0.002024675 -0.0029194639 -0.0032971434 -0.003394041 -0.0034008666 -0.0034033905 -0.0034054834 -0.0034077154 -0.0034092453 -0.0034090623 -0.0034097985 -0.0034108111 -0.0034123147]]...]
INFO - root - 2017-12-10 02:26:25.377690: step 81910, loss = 0.91, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 59h:32m:05s remains)
INFO - root - 2017-12-10 02:26:33.913434: step 81920, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.734 sec/batch; 51h:06m:23s remains)
INFO - root - 2017-12-10 02:26:42.625715: step 81930, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 58h:22m:58s remains)
INFO - root - 2017-12-10 02:26:51.341364: step 81940, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:09m:42s remains)
INFO - root - 2017-12-10 02:26:59.969005: step 81950, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:20m:08s remains)
INFO - root - 2017-12-10 02:27:08.521215: step 81960, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:33m:54s remains)
INFO - root - 2017-12-10 02:27:17.222329: step 81970, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 60h:29m:54s remains)
INFO - root - 2017-12-10 02:27:25.770959: step 81980, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 59h:10m:08s remains)
INFO - root - 2017-12-10 02:27:34.425596: step 81990, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:01m:41s remains)
INFO - root - 2017-12-10 02:27:42.973205: step 82000, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 61h:47m:46s remains)
2017-12-10 02:27:43.896686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033638873 -0.003361437 -0.0033617213 -0.0033622978 -0.0033627891 -0.0033632203 -0.0033633397 -0.00336323 -0.0033628049 -0.0033622128 -0.00336163 -0.0033612372 -0.003361058 -0.003360942 -0.0033608198][-0.0033624324 -0.0033602039 -0.0033609816 -0.0033622175 -0.0033634296 -0.0033645229 -0.0033649185 -0.0033645637 -0.0033636147 -0.0033622631 -0.0033608454 -0.0033596745 -0.0033589548 -0.0033585243 -0.0033582796][-0.0033643253 -0.003362624 -0.0033643025 -0.003366801 -0.003369336 -0.0033714303 -0.0033721754 -0.0033713821 -0.0033693975 -0.0033666177 -0.0033637742 -0.0033614226 -0.00335988 -0.0033589837 -0.0033584945][-0.003366339 -0.0033653886 -0.0033684026 -0.0033725342 -0.0033767379 -0.0033800176 -0.0033810721 -0.0033798283 -0.0033765112 -0.0033719745 -0.0033674664 -0.0033636303 -0.0033609634 -0.0033594074 -0.0033585532][-0.0033678229 -0.0033677754 -0.0033722685 -0.0033780411 -0.0033837075 -0.0033879459 -0.0033892768 -0.0033874726 -0.0033830549 -0.0033770187 -0.0033709968 -0.003365915 -0.0033623772 -0.0033602305 -0.003358932][-0.0033692904 -0.0033704129 -0.0033763854 -0.0033834754 -0.0033902221 -0.0033949658 -0.0033964391 -0.0033937253 -0.0033881369 -0.0033808036 -0.003373489 -0.0033675353 -0.0033633714 -0.0033608617 -0.0033593921][-0.0033704909 -0.0033723314 -0.0033791647 -0.0033869802 -0.0033941376 -0.0033992399 -0.0034008154 -0.0033975712 -0.0033911192 -0.0033831184 -0.00337529 -0.0033686804 -0.0033639767 -0.0033612647 -0.0033598531][-0.0033708536 -0.0033730229 -0.0033800523 -0.0033877972 -0.0033946589 -0.003399427 -0.00340077 -0.003397438 -0.0033909187 -0.0033831769 -0.0033755198 -0.0033689633 -0.0033642394 -0.003361552 -0.0033603264][-0.0033701251 -0.0033721374 -0.0033785761 -0.0033850968 -0.0033907059 -0.0033944035 -0.0033952065 -0.0033921658 -0.0033865848 -0.0033800146 -0.0033735635 -0.0033681928 -0.0033642298 -0.0033619795 -0.0033609963][-0.0033688555 -0.0033701782 -0.0033754362 -0.0033805305 -0.0033847303 -0.003387203 -0.0033873543 -0.0033847846 -0.00338039 -0.0033754527 -0.0033708126 -0.0033671511 -0.0033644631 -0.0033629609 -0.0033622673][-0.0033678422 -0.0033679616 -0.0033718334 -0.0033755351 -0.003378412 -0.003379869 -0.003379738 -0.0033778748 -0.0033748285 -0.0033714368 -0.0033684832 -0.0033661672 -0.0033645122 -0.0033634929 -0.00336303][-0.003366461 -0.0033654249 -0.0033678936 -0.0033701386 -0.0033719051 -0.0033729135 -0.0033728969 -0.0033718783 -0.0033701272 -0.0033681679 -0.0033665486 -0.0033652536 -0.0033642722 -0.0033635588 -0.0033633013][-0.0033650857 -0.0033631106 -0.0033644389 -0.0033654794 -0.003366367 -0.0033669844 -0.0033670682 -0.0033666359 -0.0033658275 -0.0033650061 -0.0033643239 -0.0033637551 -0.0033632324 -0.0033627669 -0.0033626109][-0.0033642785 -0.0033615381 -0.0033621632 -0.0033624782 -0.0033628028 -0.0033631637 -0.003363281 -0.0033632049 -0.0033629988 -0.0033628151 -0.0033626349 -0.0033625064 -0.0033623285 -0.0033620384 -0.0033618449][-0.0033641928 -0.0033610135 -0.0033611222 -0.0033611129 -0.003361149 -0.0033612377 -0.0033612687 -0.003361298 -0.0033613534 -0.0033614649 -0.0033614659 -0.0033615415 -0.0033616126 -0.0033614831 -0.0033613194]]...]
INFO - root - 2017-12-10 02:27:52.417317: step 82010, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:37m:12s remains)
INFO - root - 2017-12-10 02:28:00.981571: step 82020, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:17m:36s remains)
INFO - root - 2017-12-10 02:28:09.570336: step 82030, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 58h:36m:01s remains)
INFO - root - 2017-12-10 02:28:18.284461: step 82040, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 59h:56m:08s remains)
INFO - root - 2017-12-10 02:28:26.944293: step 82050, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:33m:52s remains)
INFO - root - 2017-12-10 02:28:35.399784: step 82060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:17m:21s remains)
INFO - root - 2017-12-10 02:28:44.023401: step 82070, loss = 0.91, batch loss = 0.70 (9.0 examples/sec; 0.887 sec/batch; 61h:42m:23s remains)
INFO - root - 2017-12-10 02:28:52.689749: step 82080, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 60h:18m:12s remains)
INFO - root - 2017-12-10 02:29:01.269752: step 82090, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 61h:38m:27s remains)
INFO - root - 2017-12-10 02:29:09.656191: step 82100, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 57h:20m:53s remains)
2017-12-10 02:29:10.492455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034094197 -0.0034095484 -0.0034108325 -0.003412199 -0.0034130337 -0.00341228 -0.0034094681 -0.0034044257 -0.0033986566 -0.00339563 -0.0033959828 -0.0033995933 -0.0034042778 -0.0034085037 -0.0034113904][-0.0034072797 -0.0034074415 -0.0034088322 -0.0034102753 -0.0034116292 -0.0034116574 -0.0034099482 -0.0034067393 -0.003403197 -0.0034014117 -0.0034020061 -0.0034046907 -0.0034076893 -0.0034103522 -0.0034124348][-0.0034071624 -0.0034072369 -0.0034085328 -0.003409941 -0.0034114956 -0.0034124444 -0.0034122069 -0.0034106283 -0.0034087612 -0.0034080227 -0.0034080914 -0.0034096034 -0.003411527 -0.0034128162 -0.0034139392][-0.0034067263 -0.0034066939 -0.0034079743 -0.003409243 -0.0034108055 -0.0034122078 -0.0034127671 -0.0034127715 -0.0034123508 -0.0034126041 -0.0034126972 -0.003413209 -0.0034139489 -0.00341448 -0.0034150232][-0.0034059675 -0.0034056916 -0.0034068122 -0.0034078655 -0.0034092457 -0.0034108202 -0.0034119331 -0.003412744 -0.0034129196 -0.0034134148 -0.0034138702 -0.0034142225 -0.0034144358 -0.0034146868 -0.0034149818][-0.0034050646 -0.0034044369 -0.0034053163 -0.003406133 -0.0034070669 -0.0034084141 -0.0034097433 -0.0034108222 -0.0034113224 -0.0034120763 -0.0034128793 -0.0034134237 -0.0034133862 -0.0034134991 -0.0034138679][-0.0034042466 -0.0034033766 -0.0034036888 -0.0034040052 -0.0034047246 -0.0034058306 -0.0034069156 -0.0034079065 -0.0034084632 -0.0034093284 -0.0034100274 -0.0034106045 -0.0034108004 -0.0034112416 -0.0034119615][-0.0034039235 -0.0034028061 -0.0034027621 -0.0034026129 -0.0034030618 -0.0034037412 -0.0034041929 -0.0034047188 -0.0034052921 -0.0034059607 -0.0034066462 -0.0034073994 -0.0034078739 -0.0034085994 -0.0034096595][-0.0034038653 -0.0034026534 -0.0034023668 -0.0034020883 -0.0034023363 -0.003402693 -0.0034030061 -0.0034032646 -0.0034032494 -0.0034035803 -0.0034041919 -0.0034046518 -0.0034051447 -0.003406001 -0.0034070795][-0.0034039707 -0.0034026492 -0.003402299 -0.0034020522 -0.0034021265 -0.0034025819 -0.0034029298 -0.0034028487 -0.0034027444 -0.0034029388 -0.0034031412 -0.0034030834 -0.003403313 -0.0034039961 -0.0034047256][-0.0034037419 -0.0034025486 -0.0034025211 -0.003402601 -0.0034028518 -0.0034031584 -0.0034031561 -0.0034029323 -0.0034027132 -0.0034025218 -0.0034024429 -0.0034022955 -0.0034024855 -0.0034028152 -0.003403075][-0.0034033151 -0.0034022643 -0.0034024089 -0.0034027183 -0.0034031488 -0.003403418 -0.0034032068 -0.003402841 -0.0034026064 -0.003402238 -0.0034021339 -0.0034021051 -0.0034021677 -0.0034022199 -0.0034022303][-0.0034027228 -0.0034017828 -0.0034020538 -0.0034023514 -0.0034025097 -0.0034025032 -0.0034023046 -0.0034020222 -0.0034020017 -0.0034018944 -0.0034017712 -0.0034017644 -0.0034018126 -0.0034017544 -0.0034016506][-0.0034023908 -0.0034012196 -0.0034013165 -0.0034014808 -0.003401512 -0.0034014804 -0.0034014287 -0.0034012026 -0.0034011181 -0.0034010492 -0.00340099 -0.00340101 -0.0034010531 -0.0034010655 -0.0034010429][-0.0034026292 -0.0034013663 -0.0034012573 -0.0034012296 -0.0034011318 -0.0034010212 -0.0034009521 -0.0034008359 -0.0034008096 -0.0034007686 -0.0034007181 -0.0034007465 -0.0034008226 -0.0034009051 -0.003401032]]...]
INFO - root - 2017-12-10 02:29:18.928785: step 82110, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 59h:49m:46s remains)
INFO - root - 2017-12-10 02:29:27.507566: step 82120, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:17m:33s remains)
INFO - root - 2017-12-10 02:29:35.967951: step 82130, loss = 0.89, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:55m:57s remains)
INFO - root - 2017-12-10 02:29:44.501226: step 82140, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:45m:45s remains)
INFO - root - 2017-12-10 02:29:53.061755: step 82150, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 62h:14m:28s remains)
INFO - root - 2017-12-10 02:30:01.605307: step 82160, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:05m:53s remains)
INFO - root - 2017-12-10 02:30:10.350267: step 82170, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 58h:10m:47s remains)
INFO - root - 2017-12-10 02:30:18.989607: step 82180, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 63h:03m:55s remains)
INFO - root - 2017-12-10 02:30:27.660529: step 82190, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:44m:44s remains)
INFO - root - 2017-12-10 02:30:36.322252: step 82200, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:33m:52s remains)
2017-12-10 02:30:37.228238: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071051225 0.089511886 0.10754646 0.12384556 0.13559617 0.14435738 0.14867917 0.14989559 0.14824693 0.14529884 0.14225361 0.1405919 0.14019603 0.14039934 0.14226376][0.077458344 0.0966003 0.11494052 0.13073976 0.14243215 0.15025555 0.15347217 0.15417013 0.15338585 0.15144335 0.14950027 0.14939821 0.15094577 0.15256563 0.1549942][0.076533519 0.094893806 0.11189832 0.12625991 0.13678482 0.14304845 0.14577888 0.14646319 0.14636581 0.14556481 0.1457426 0.14811063 0.15098591 0.15510626 0.15956193][0.069494128 0.087631963 0.1042181 0.11785074 0.12824984 0.13433883 0.13709487 0.1374591 0.13767305 0.13832586 0.13958703 0.1430634 0.14700866 0.15311055 0.15924682][0.057846691 0.074880518 0.091525435 0.10529011 0.11596248 0.12294193 0.12684964 0.12858476 0.1294672 0.13104063 0.13314614 0.13666409 0.14118679 0.14783762 0.15433709][0.044851761 0.059415668 0.074125089 0.087720938 0.099421643 0.10721496 0.11195283 0.11549783 0.11834577 0.12162253 0.1245428 0.12864296 0.13343649 0.13951579 0.14614074][0.0324154 0.044074219 0.056171983 0.067814469 0.078500472 0.087170333 0.09287861 0.097173877 0.10118368 0.10626975 0.1108641 0.11566696 0.12094329 0.12728438 0.13369033][0.021425707 0.030054027 0.039086144 0.048187796 0.057142522 0.064230189 0.069227584 0.073680095 0.07790149 0.083189726 0.088122278 0.094416171 0.10073581 0.1077742 0.11493283][0.011061256 0.017041482 0.023331191 0.02934072 0.035631165 0.040884566 0.044695925 0.047389016 0.050369717 0.054794904 0.059601188 0.066150725 0.073310994 0.08207915 0.090722881][0.003161513 0.0062881438 0.0097053545 0.01323509 0.017177504 0.020402217 0.022964498 0.02456056 0.026341885 0.028874395 0.032511313 0.038234264 0.045266271 0.054325338 0.063743681][-0.001326513 -0.00017071981 0.0012194531 0.0029537089 0.0049407491 0.0067280876 0.008484818 0.0094875582 0.010599521 0.011750879 0.014017235 0.017932909 0.023524383 0.031338863 0.040016152][-0.0028341406 -0.0025733183 -0.002385101 -0.0020765876 -0.0013820704 -0.0005200447 0.00019181427 0.0009021603 0.0018217752 0.0025700254 0.0037741361 0.0058715139 0.0095054619 0.014518831 0.020712739][-0.0030615577 -0.0030097573 -0.0029586642 -0.0030519287 -0.0030539129 -0.0028823793 -0.0027097112 -0.0023503709 -0.0021016374 -0.0016183387 -0.00091412175 2.5094487e-06 0.0017731807 0.0040979506 0.00759115][-0.0032286295 -0.0032660794 -0.0032542215 -0.0032690316 -0.0032767619 -0.003165371 -0.0030651386 -0.0030076578 -0.0030232011 -0.0027648821 -0.0025456736 -0.0023693929 -0.0018671531 -0.0010858127 0.00027917232][-0.00328966 -0.00329907 -0.0033210269 -0.0032875098 -0.0033029886 -0.0032462904 -0.0031684372 -0.0031143259 -0.0030835159 -0.0029880516 -0.00295717 -0.0028798957 -0.0027945871 -0.0027503746 -0.0024669792]]...]
INFO - root - 2017-12-10 02:30:45.837365: step 82210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:40m:25s remains)
INFO - root - 2017-12-10 02:30:54.485425: step 82220, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 59h:39m:02s remains)
INFO - root - 2017-12-10 02:31:03.071098: step 82230, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:10m:39s remains)
INFO - root - 2017-12-10 02:31:11.878413: step 82240, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:15m:10s remains)
INFO - root - 2017-12-10 02:31:20.511746: step 82250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:30m:10s remains)
INFO - root - 2017-12-10 02:31:29.043858: step 82260, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:05m:15s remains)
INFO - root - 2017-12-10 02:31:37.707736: step 82270, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:32m:19s remains)
INFO - root - 2017-12-10 02:31:46.274578: step 82280, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 61h:54m:29s remains)
INFO - root - 2017-12-10 02:31:54.794024: step 82290, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 60h:45m:53s remains)
INFO - root - 2017-12-10 02:32:03.336584: step 82300, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 58h:44m:22s remains)
2017-12-10 02:32:04.264423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00015713321 0.00061820424 0.0009529891 0.00082149566 0.00036244793 -0.00031619635 -0.0010507256 -0.001758042 -0.0023797248 -0.002844583 -0.0031445432 -0.0033074422 -0.0033785945 -0.0034009402 -0.0034044853][0.0016997599 0.00266931 0.0030353602 0.002683555 0.001867586 0.00077340775 -0.00035067229 -0.0013616441 -0.0021791924 -0.0027542044 -0.0031115387 -0.0032988109 -0.0033771403 -0.0034006964 -0.0034043607][0.0063230824 0.0075299656 0.0078496179 0.0071769524 0.0058203582 0.0039871335 0.0020288054 0.00021244143 -0.0012631312 -0.0023000478 -0.0029255224 -0.0032371429 -0.0033612079 -0.0033973691 -0.0034035451][0.014751153 0.016231665 0.016465178 0.01538418 0.0133458 0.010502158 0.0072595356 0.0040125744 0.00119754 -0.00091100996 -0.0022601923 -0.0029756485 -0.0032817316 -0.0033795217 -0.0034007877][0.026296468 0.02807801 0.028240295 0.026796004 0.024091911 0.020144865 0.01537573 0.010283911 0.0055628596 0.0017657566 -0.00083813258 -0.0023390902 -0.0030550216 -0.003321711 -0.0033931434][0.038508907 0.040597819 0.040822227 0.039259166 0.036148861 0.031265847 0.025035577 0.018040368 0.011210673 0.0054090489 0.0012118279 -0.0013527363 -0.0026712092 -0.0032108924 -0.0033723991][0.048112046 0.050455946 0.050929762 0.049634617 0.046600398 0.04130758 0.034159273 0.025711799 0.017051248 0.00933309 0.0035041727 -0.00020552706 -0.002203593 -0.0030662487 -0.0033405283][0.052437451 0.054919362 0.055722341 0.054979816 0.052476253 0.04742619 0.040135652 0.03105831 0.021327702 0.012307681 0.0052739503 0.00068190531 -0.0018453443 -0.0029550856 -0.0033121603][0.050571531 0.053012487 0.054090679 0.053919766 0.0520854 0.047692847 0.040916365 0.032103375 0.022358781 0.013106359 0.0057625864 0.00091467029 -0.0017612912 -0.0029304866 -0.0033015707][0.04348154 0.045589659 0.0466529 0.0467617 0.045416638 0.04178955 0.035949588 0.028180206 0.019503402 0.01122911 0.0046579083 0.0003408941 -0.0020067832 -0.0030061072 -0.0033126695][0.033009082 0.034558933 0.035340961 0.035383735 0.034245189 0.03133354 0.026689654 0.020559233 0.013790235 0.0074276766 0.0024540981 -0.00075360411 -0.0024459842 -0.0031357943 -0.0033367747][0.021548955 0.02249703 0.022899935 0.022721082 0.021661917 0.019428587 0.016094239 0.011860706 0.0073284311 0.0031953084 6.1470782e-05 -0.0018947074 -0.0028828208 -0.0032616041 -0.0033644759][0.011360049 0.011821317 0.011922905 0.011591098 0.01067625 0.0091123 0.0070108408 0.0045198537 0.0019908014 -0.00020549912 -0.0017933926 -0.0027364003 -0.0031849188 -0.0033438017 -0.0033833859][0.003929425 0.0040987432 0.0040528625 0.0037208234 0.0030563965 0.0021071152 0.00098805944 -0.00021036342 -0.0013309084 -0.002235915 -0.0028475942 -0.0031871402 -0.003336862 -0.0033844667 -0.0033951069][-0.00050712842 -0.00046634069 -0.00053332094 -0.00074973353 -0.0011156453 -0.0015694691 -0.0020360993 -0.0024744971 -0.0028403741 -0.0031076181 -0.0032722494 -0.0033563916 -0.0033902964 -0.0033992678 -0.0034008194]]...]
INFO - root - 2017-12-10 02:32:12.719641: step 82310, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:53m:54s remains)
INFO - root - 2017-12-10 02:32:21.270351: step 82320, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 60h:07m:16s remains)
INFO - root - 2017-12-10 02:32:29.685105: step 82330, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:07m:16s remains)
INFO - root - 2017-12-10 02:32:38.270378: step 82340, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 60h:57m:42s remains)
INFO - root - 2017-12-10 02:32:46.741481: step 82350, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 58h:26m:26s remains)
INFO - root - 2017-12-10 02:32:55.041768: step 82360, loss = 0.88, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 60h:10m:06s remains)
INFO - root - 2017-12-10 02:33:03.746121: step 82370, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 62h:42m:06s remains)
INFO - root - 2017-12-10 02:33:12.417083: step 82380, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:06m:13s remains)
INFO - root - 2017-12-10 02:33:20.965590: step 82390, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:04m:22s remains)
INFO - root - 2017-12-10 02:33:29.488281: step 82400, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:17m:46s remains)
2017-12-10 02:33:30.315814: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076754026 0.075405777 0.073539585 0.071186259 0.068427823 0.065593824 0.062418483 0.059185006 0.056151737 0.053289138 0.050591718 0.047744658 0.044254832 0.040465955 0.036638368][0.086612575 0.086603381 0.085339509 0.08300063 0.079822659 0.076320618 0.072297573 0.06829121 0.064690441 0.061636269 0.058688477 0.055384148 0.051057491 0.046360653 0.041608669][0.0971226 0.099102691 0.098901443 0.096772149 0.093012564 0.088469811 0.083138824 0.077909738 0.073407456 0.069923505 0.066756949 0.063043416 0.058084685 0.052501258 0.046690002][0.10719436 0.11149012 0.11248824 0.11056251 0.10635977 0.10090055 0.09452989 0.088148847 0.082518175 0.07849592 0.075046904 0.071044542 0.065274693 0.058690906 0.051805034][0.11526884 0.12191147 0.12425953 0.12268299 0.11822616 0.11194081 0.10456555 0.097178087 0.090687715 0.086160131 0.082424872 0.078112379 0.071806051 0.064465508 0.056610681][0.12013076 0.12872271 0.13198009 0.1305784 0.12599155 0.11930142 0.11145736 0.10336474 0.096074536 0.0911053 0.087184116 0.082679756 0.076124161 0.068383045 0.060000181][0.12167151 0.13173199 0.13565205 0.13448684 0.12993202 0.12325239 0.11525713 0.10682094 0.099316433 0.094186813 0.090148315 0.085544571 0.078855164 0.070955381 0.06216912][0.12008727 0.13113463 0.13559698 0.13482958 0.13058053 0.12421981 0.11656387 0.10838597 0.10096835 0.095798522 0.091685764 0.08683794 0.079906486 0.071774274 0.062823854][0.11590394 0.12740682 0.13224654 0.13205861 0.12860569 0.12309434 0.11637172 0.10894429 0.10176264 0.09648779 0.092006728 0.086834729 0.07974638 0.071494877 0.062546276][0.10911377 0.12053677 0.12566163 0.12628594 0.12382758 0.1194936 0.11398056 0.10744114 0.10072219 0.095383391 0.090420365 0.084635511 0.0773022 0.069099255 0.060458086][0.099344894 0.11027342 0.115577 0.11706208 0.11578659 0.11273368 0.10858897 0.10310283 0.096893206 0.091475233 0.086265296 0.080275789 0.072983272 0.0651906 0.057001233][0.087715328 0.097790755 0.10305155 0.10520373 0.10495315 0.10314422 0.10022265 0.095954269 0.090720363 0.085554823 0.080264665 0.074213669 0.067117564 0.059702858 0.052128449][0.075336181 0.084324941 0.089442164 0.092161521 0.092811935 0.091973707 0.090041794 0.08672221 0.082173049 0.077407219 0.072361365 0.066568188 0.060016222 0.053298596 0.046601932][0.062887676 0.070574813 0.075261265 0.078367032 0.079784632 0.07988096 0.078825869 0.07632184 0.072620727 0.068424821 0.063760348 0.058409154 0.052659452 0.046880778 0.041246891][0.051347531 0.05765808 0.061611585 0.064655162 0.066467769 0.06711854 0.066764906 0.06513039 0.062374756 0.058846042 0.054701649 0.049897455 0.04492797 0.040105607 0.035628982]]...]
INFO - root - 2017-12-10 02:33:38.790393: step 82410, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:08m:59s remains)
INFO - root - 2017-12-10 02:33:47.474686: step 82420, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:13m:36s remains)
INFO - root - 2017-12-10 02:33:55.989484: step 82430, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 59h:02m:49s remains)
INFO - root - 2017-12-10 02:34:04.707746: step 82440, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:29m:47s remains)
INFO - root - 2017-12-10 02:34:13.459246: step 82450, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 61h:55m:16s remains)
INFO - root - 2017-12-10 02:34:22.006183: step 82460, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:42m:31s remains)
INFO - root - 2017-12-10 02:34:30.807436: step 82470, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:00m:49s remains)
INFO - root - 2017-12-10 02:34:39.612660: step 82480, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 62h:37m:43s remains)
INFO - root - 2017-12-10 02:34:48.420035: step 82490, loss = 0.89, batch loss = 0.68 (8.3 examples/sec; 0.963 sec/batch; 66h:51m:31s remains)
INFO - root - 2017-12-10 02:34:57.187813: step 82500, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 60h:34m:02s remains)
2017-12-10 02:34:58.135123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034003844 -0.0033989127 -0.0033986219 -0.0033985774 -0.003398512 -0.0033985891 -0.0033986387 -0.0033988135 -0.0033991858 -0.0033993761 -0.0033994566 -0.003399472 -0.0033993949 -0.0033993213 -0.0033993393][-0.0033991877 -0.0033974939 -0.0033971879 -0.0033972207 -0.0033972135 -0.0033973185 -0.0033973488 -0.0033975458 -0.0033978641 -0.0033981337 -0.0033983565 -0.0033983691 -0.0033983306 -0.0033983109 -0.0033982398][-0.0033989195 -0.0033971672 -0.0033970666 -0.0033969835 -0.0033970289 -0.0033973048 -0.0033974049 -0.0033974249 -0.0033976671 -0.003398122 -0.0033983476 -0.0033983046 -0.003398333 -0.0033984054 -0.0033983805][-0.0033986946 -0.0033969032 -0.0033969744 -0.0033970317 -0.0033971625 -0.00339747 -0.003397444 -0.0033974105 -0.0033977111 -0.0033982089 -0.0033983055 -0.0033981451 -0.0033981611 -0.0033982941 -0.0033983421][-0.003398956 -0.0033968014 -0.0033968717 -0.0033971118 -0.0033973409 -0.0033976422 -0.0033975509 -0.003397502 -0.0033978622 -0.0033983025 -0.0033982294 -0.0033980689 -0.0033980771 -0.0033982454 -0.0033982897][-0.0033996452 -0.0033970599 -0.0033968398 -0.0033969637 -0.0033973467 -0.0033977325 -0.003397793 -0.0033977367 -0.0033980955 -0.0033984212 -0.0033985381 -0.0033984783 -0.0033983886 -0.0033984145 -0.0033982536][-0.0034003649 -0.0033977123 -0.0033972745 -0.0033969795 -0.0033973814 -0.0033979204 -0.0033983104 -0.0033984405 -0.0033989053 -0.0033995502 -0.0034001083 -0.0033999998 -0.0033994874 -0.0033991234 -0.0033985898][-0.0034009656 -0.00339843 -0.0033979297 -0.0033973963 -0.0033976901 -0.0033984233 -0.0033990841 -0.00339913 -0.0033998303 -0.0034015239 -0.00340269 -0.0034022843 -0.0034010387 -0.0034000948 -0.0033990429][-0.0034013444 -0.0033987239 -0.0033982904 -0.003397807 -0.0033979036 -0.0033989174 -0.003399828 -0.0034001535 -0.0034019086 -0.0034044245 -0.0034056592 -0.0034048462 -0.0034027351 -0.003400963 -0.0033994345][-0.0034019053 -0.0033993451 -0.0033988131 -0.0033984086 -0.0033983872 -0.00339923 -0.0034006219 -0.003402066 -0.0034048213 -0.0034073235 -0.0034083547 -0.0034070795 -0.0034042797 -0.0034017346 -0.00339984][-0.0034030438 -0.0034009323 -0.0034003714 -0.003399696 -0.0033997553 -0.0034002981 -0.0034021342 -0.003404543 -0.0034074772 -0.0034096073 -0.0034100087 -0.0034081466 -0.0034048588 -0.0034020673 -0.003400153][-0.0034047847 -0.0034034313 -0.0034025405 -0.003401655 -0.0034016396 -0.0034021186 -0.0034035849 -0.0034060625 -0.003408785 -0.0034104523 -0.0034102253 -0.0034079084 -0.0034045822 -0.0034020455 -0.0034002189][-0.0034067293 -0.0034057223 -0.0034051212 -0.0034038597 -0.0034035102 -0.0034034743 -0.0034040865 -0.0034059859 -0.0034082211 -0.0034093459 -0.003408751 -0.0034064841 -0.0034036897 -0.00340138 -0.0033997954][-0.003408046 -0.0034072311 -0.0034065819 -0.0034053484 -0.0034045216 -0.0034038781 -0.003403726 -0.0034052567 -0.0034068776 -0.0034074602 -0.0034065649 -0.0034045794 -0.0034023135 -0.0034004059 -0.0033991765][-0.0034102064 -0.0034088972 -0.0034077868 -0.0034065489 -0.003405421 -0.0034042641 -0.0034034546 -0.0034042897 -0.0034053545 -0.003405483 -0.0034046231 -0.003403042 -0.003401174 -0.0033996296 -0.0033986482]]...]
INFO - root - 2017-12-10 02:35:06.764587: step 82510, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 62h:24m:47s remains)
INFO - root - 2017-12-10 02:35:15.546971: step 82520, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:05m:43s remains)
INFO - root - 2017-12-10 02:35:24.247184: step 82530, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 60h:03m:38s remains)
INFO - root - 2017-12-10 02:35:32.988664: step 82540, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:26m:51s remains)
INFO - root - 2017-12-10 02:35:41.607244: step 82550, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 61h:47m:21s remains)
INFO - root - 2017-12-10 02:35:50.127924: step 82560, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 59h:52m:24s remains)
INFO - root - 2017-12-10 02:35:58.796986: step 82570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:32m:48s remains)
INFO - root - 2017-12-10 02:36:07.605948: step 82580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:37m:48s remains)
INFO - root - 2017-12-10 02:36:16.237404: step 82590, loss = 0.89, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 57h:43m:26s remains)
INFO - root - 2017-12-10 02:36:24.916760: step 82600, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 59h:37m:55s remains)
2017-12-10 02:36:25.837795: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0090620248 0.017201979 0.026769731 0.03747575 0.04663882 0.052837938 0.053438023 0.051252011 0.04643584 0.040553357 0.034227673 0.029239567 0.025727397 0.026403209 0.032895274][0.017086523 0.02802697 0.041832484 0.058254495 0.073291183 0.085211717 0.089125082 0.088034868 0.080932505 0.070916027 0.058934953 0.04791715 0.040564731 0.039720338 0.047858067][0.027749846 0.042534851 0.061897412 0.085789517 0.10852174 0.12764651 0.13669309 0.13743664 0.12864506 0.11383949 0.095434614 0.077653758 0.064556792 0.059651539 0.066648908][0.041948058 0.062754527 0.090958327 0.12603937 0.16006756 0.18901232 0.20436493 0.20698845 0.19514473 0.17280287 0.14464851 0.11657251 0.095032863 0.083450362 0.085974053][0.056482151 0.084819064 0.12321307 0.17035411 0.21675719 0.2555128 0.27718452 0.28070527 0.26507986 0.23450637 0.19569451 0.1563137 0.12483872 0.10480087 0.10052403][0.067612588 0.10314887 0.15064983 0.20805873 0.264028 0.31024957 0.3364237 0.33998829 0.32129133 0.28337675 0.2354558 0.18587615 0.14513449 0.11645564 0.10396957][0.073709123 0.11444603 0.16776384 0.23130289 0.29220223 0.34107134 0.36791223 0.36999175 0.34841898 0.30533013 0.25176209 0.19581789 0.14909065 0.11420661 0.095217161][0.071247868 0.11425965 0.16958907 0.23458369 0.29619822 0.34440789 0.3696433 0.36905777 0.34504694 0.29902947 0.24283913 0.1847474 0.13661064 0.100334 0.078721888][0.062264085 0.10333674 0.15608522 0.21811441 0.276635 0.32194138 0.34435683 0.34166956 0.31671789 0.27102929 0.21606039 0.16022207 0.11529376 0.082170218 0.062168881][0.050178792 0.085532971 0.13130449 0.18536055 0.236782 0.27673149 0.29565838 0.29197961 0.26853338 0.22727394 0.17841376 0.13045342 0.093661956 0.068241648 0.053571478][0.038065381 0.065724239 0.10224529 0.14533107 0.18682735 0.21940736 0.23416534 0.23026671 0.20996171 0.17601006 0.13706587 0.10094737 0.075778641 0.060500003 0.053032152][0.027739126 0.047642719 0.07423164 0.10574756 0.13651145 0.16038619 0.17058337 0.16639279 0.15001202 0.12462863 0.097498767 0.075302809 0.063152082 0.059009083 0.059259593][0.018174596 0.031771712 0.049727827 0.070639372 0.090881392 0.10636777 0.11231349 0.10821366 0.096078582 0.079350032 0.064024895 0.055185437 0.055137109 0.060532875 0.067150876][0.0090600979 0.017230105 0.027842509 0.040060397 0.051603582 0.060122266 0.062793374 0.059522659 0.052034907 0.043336183 0.038158037 0.03987116 0.048676692 0.060505405 0.071000889][0.0020037543 0.0059355954 0.011007917 0.016672388 0.021804059 0.025460318 0.026297716 0.024602387 0.021319358 0.018960701 0.020834878 0.028794242 0.041695654 0.055446409 0.066428661]]...]
INFO - root - 2017-12-10 02:36:34.260614: step 82610, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 60h:06m:06s remains)
INFO - root - 2017-12-10 02:36:42.862038: step 82620, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 58h:11m:33s remains)
INFO - root - 2017-12-10 02:36:51.459392: step 82630, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:39m:22s remains)
INFO - root - 2017-12-10 02:37:00.134134: step 82640, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:17m:13s remains)
INFO - root - 2017-12-10 02:37:08.848764: step 82650, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 61h:33m:36s remains)
INFO - root - 2017-12-10 02:37:17.351247: step 82660, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:09m:21s remains)
INFO - root - 2017-12-10 02:37:26.021427: step 82670, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:51m:06s remains)
INFO - root - 2017-12-10 02:37:34.634753: step 82680, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:39m:08s remains)
INFO - root - 2017-12-10 02:37:43.211247: step 82690, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 58h:43m:13s remains)
INFO - root - 2017-12-10 02:37:51.947310: step 82700, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 58h:15m:52s remains)
2017-12-10 02:37:52.859965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033767552 -0.003375005 -0.0033750664 -0.0033751831 -0.0033751456 -0.0033750683 -0.0033749158 -0.0033747491 -0.0033745351 -0.0033741877 -0.0033738632 -0.0033736385 -0.0033734925 -0.0033735496 -0.0033737523][-0.0033756767 -0.0033737386 -0.0033737603 -0.0033738611 -0.0033737961 -0.003373642 -0.0033733647 -0.0033731016 -0.0033728038 -0.0033724813 -0.0033721789 -0.0033720438 -0.0033720389 -0.0033721561 -0.0033724024][-0.0033759819 -0.0033739139 -0.0033738981 -0.0033739696 -0.003373852 -0.0033735556 -0.0033730743 -0.0033726334 -0.0033722282 -0.0033718988 -0.0033716236 -0.0033716243 -0.0033718187 -0.0033720764 -0.0033723775][-0.0033761805 -0.0033740345 -0.0033739887 -0.0033740425 -0.003373903 -0.0033734571 -0.0033727686 -0.0033720762 -0.0033715181 -0.0033711544 -0.0033709039 -0.0033710774 -0.0033715635 -0.0033720117 -0.0033723884][-0.0033763463 -0.0033740595 -0.0033739861 -0.0033740327 -0.0033737745 -0.0033731502 -0.0033722876 -0.0033713996 -0.0033706971 -0.0033702671 -0.0033701188 -0.0033704585 -0.0033711996 -0.0033718548 -0.0033723384][-0.0033764928 -0.0033740676 -0.0033739656 -0.0033740066 -0.0033736511 -0.003372841 -0.0033718059 -0.0033707921 -0.0033699796 -0.0033694955 -0.0033694298 -0.0033699544 -0.0033708634 -0.00337164 -0.0033722217][-0.0033765074 -0.0033740343 -0.0033738951 -0.0033739137 -0.0033735181 -0.0033726711 -0.0033714972 -0.0033703821 -0.0033695055 -0.0033690012 -0.0033690219 -0.0033696888 -0.0033706473 -0.0033713977 -0.0033719991][-0.0033764383 -0.0033739991 -0.0033738394 -0.0033738539 -0.0033735156 -0.0033727319 -0.0033716275 -0.0033704024 -0.00336937 -0.003368899 -0.0033690291 -0.0033696538 -0.0033704997 -0.0033711514 -0.003371679][-0.0033762876 -0.0033739246 -0.0033738003 -0.0033737984 -0.0033734459 -0.00337283 -0.0033719274 -0.0033708604 -0.0033698829 -0.0033694061 -0.0033694848 -0.0033699493 -0.0033705768 -0.0033710357 -0.0033714122][-0.0033761603 -0.0033738443 -0.0033737756 -0.0033737849 -0.0033735025 -0.003373119 -0.0033725179 -0.0033718178 -0.003371109 -0.0033706103 -0.0033704308 -0.0033705579 -0.0033708746 -0.0033710848 -0.0033712685][-0.0033759826 -0.0033737391 -0.0033737826 -0.0033737484 -0.0033736178 -0.0033734534 -0.0033731463 -0.0033727437 -0.0033723281 -0.0033719731 -0.0033716252 -0.0033713784 -0.0033713698 -0.0033713263 -0.0033712978][-0.0033758858 -0.0033737137 -0.0033738283 -0.0033737761 -0.0033737184 -0.0033737535 -0.0033736536 -0.0033734115 -0.0033731668 -0.0033728948 -0.0033725107 -0.0033720871 -0.0033718327 -0.0033716506 -0.0033714846][-0.0033759759 -0.0033736988 -0.0033738569 -0.0033738059 -0.00337376 -0.0033737877 -0.0033737125 -0.0033735288 -0.003373347 -0.0033731267 -0.0033728187 -0.003372442 -0.0033721942 -0.0033720061 -0.0033718068][-0.0033760867 -0.0033736948 -0.0033738534 -0.0033738133 -0.0033737686 -0.0033737435 -0.0033736571 -0.003373534 -0.003373404 -0.0033732345 -0.0033730199 -0.0033727565 -0.0033725591 -0.0033723773 -0.0033721847][-0.0033762124 -0.0033737428 -0.0033738534 -0.0033738376 -0.0033738117 -0.0033737745 -0.0033736909 -0.0033735752 -0.0033734662 -0.0033733165 -0.0033731516 -0.0033729565 -0.0033728003 -0.0033726422 -0.0033724853]]...]
INFO - root - 2017-12-10 02:38:01.385980: step 82710, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 62h:09m:47s remains)
INFO - root - 2017-12-10 02:38:10.150921: step 82720, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 60h:40m:26s remains)
INFO - root - 2017-12-10 02:38:18.985061: step 82730, loss = 0.90, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 63h:31m:08s remains)
INFO - root - 2017-12-10 02:38:27.651969: step 82740, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:01m:01s remains)
INFO - root - 2017-12-10 02:38:36.421576: step 82750, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.988 sec/batch; 68h:33m:51s remains)
INFO - root - 2017-12-10 02:38:44.955151: step 82760, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 59h:59m:05s remains)
INFO - root - 2017-12-10 02:38:53.663723: step 82770, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:22m:47s remains)
INFO - root - 2017-12-10 02:39:02.438224: step 82780, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 60h:53m:51s remains)
INFO - root - 2017-12-10 02:39:11.221688: step 82790, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 61h:37m:12s remains)
INFO - root - 2017-12-10 02:39:19.846870: step 82800, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:47m:05s remains)
2017-12-10 02:39:20.825544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033908379 -0.0033892808 -0.0033890477 -0.0033890102 -0.0033889916 -0.0033890281 -0.0033890312 -0.0033890579 -0.0033890589 -0.0033889231 -0.003388769 -0.0033886244 -0.0033885296 -0.0033885117 -0.0033885124][-0.0033896023 -0.0033879131 -0.0033876831 -0.0033876614 -0.0033876514 -0.0033876537 -0.0033876617 -0.0033876898 -0.0033877143 -0.0033877247 -0.0033876938 -0.0033876519 -0.0033876267 -0.0033876386 -0.0033876642][-0.0033898654 -0.0033880863 -0.0033878973 -0.0033879154 -0.0033879208 -0.0033878651 -0.0033878318 -0.0033878097 -0.003387755 -0.0033877189 -0.0033876782 -0.0033876295 -0.0033875948 -0.0033876165 -0.0033876707][-0.0033902016 -0.003388515 -0.0033884721 -0.0033885874 -0.0033886873 -0.0033886489 -0.0033885508 -0.0033884072 -0.0033881632 -0.0033879313 -0.0033877501 -0.0033875888 -0.0033875017 -0.0033875199 -0.0033875853][-0.0033906256 -0.0033891622 -0.0033894146 -0.0033897737 -0.0033901618 -0.003390363 -0.0033904018 -0.0033901874 -0.0033895632 -0.0033888943 -0.0033883359 -0.0033878442 -0.0033875012 -0.0033873774 -0.0033874044][-0.0033911329 -0.0033899529 -0.0033907068 -0.0033916887 -0.0033925883 -0.0033932622 -0.0033935362 -0.003393231 -0.0033921055 -0.0033907075 -0.0033894691 -0.0033884398 -0.0033876249 -0.0033872288 -0.00338717][-0.0033915848 -0.0033907711 -0.0033920631 -0.0033938144 -0.0033953809 -0.0033965644 -0.0033970184 -0.0033965313 -0.0033948445 -0.0033927616 -0.0033908274 -0.0033891562 -0.003387918 -0.0033872374 -0.0033870263][-0.0033918973 -0.0033916449 -0.0033935155 -0.0033960254 -0.0033984124 -0.0034001982 -0.0034007123 -0.0033999127 -0.0033977202 -0.0033948743 -0.0033922091 -0.0033898957 -0.0033881394 -0.0033871166 -0.0033867955][-0.0033920109 -0.0033924223 -0.0033950659 -0.0033983397 -0.0034013507 -0.0034037114 -0.0034046832 -0.003403934 -0.0034013656 -0.0033978843 -0.003394291 -0.003391054 -0.0033884922 -0.0033869436 -0.0033864018][-0.0033917495 -0.0033925117 -0.0033955264 -0.0033992571 -0.0034027251 -0.0034056213 -0.0034071626 -0.003406873 -0.0034044161 -0.0034006082 -0.003396458 -0.0033925667 -0.0033893103 -0.0033872297 -0.0033863741][-0.0033912312 -0.0033919176 -0.0033948263 -0.003398373 -0.0034018238 -0.0034048681 -0.0034067144 -0.0034067985 -0.0034049181 -0.0034016347 -0.0033976769 -0.0033937376 -0.0033903436 -0.0033880305 -0.0033868812][-0.0033905318 -0.0033907036 -0.0033930042 -0.0033958224 -0.0033987476 -0.0034013975 -0.003403211 -0.0034036245 -0.00340258 -0.003400364 -0.003397238 -0.0033939655 -0.0033910458 -0.0033889338 -0.0033876293][-0.0033898221 -0.003389223 -0.0033907411 -0.0033925232 -0.0033945877 -0.003396542 -0.0033980031 -0.0033985807 -0.0033982003 -0.0033969635 -0.0033949551 -0.0033926996 -0.0033905292 -0.0033888759 -0.0033878116][-0.0033892784 -0.0033879622 -0.0033887373 -0.0033896023 -0.003390745 -0.0033918847 -0.0033928228 -0.0033932608 -0.0033931686 -0.0033926049 -0.0033915332 -0.0033902663 -0.0033890288 -0.0033880938 -0.0033874982][-0.0033890314 -0.0033871902 -0.0033873769 -0.0033876209 -0.0033880379 -0.0033884819 -0.0033888547 -0.0033890861 -0.0033891455 -0.0033889897 -0.0033886272 -0.003388162 -0.0033877427 -0.0033874412 -0.0033872565]]...]
INFO - root - 2017-12-10 02:39:29.340452: step 82810, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 60h:55m:57s remains)
INFO - root - 2017-12-10 02:39:37.953018: step 82820, loss = 0.91, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 58h:30m:21s remains)
INFO - root - 2017-12-10 02:39:46.493317: step 82830, loss = 0.89, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 53h:58m:30s remains)
INFO - root - 2017-12-10 02:39:55.147834: step 82840, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 57h:42m:06s remains)
INFO - root - 2017-12-10 02:40:03.651864: step 82850, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:33m:18s remains)
INFO - root - 2017-12-10 02:40:12.261034: step 82860, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 58h:28m:46s remains)
INFO - root - 2017-12-10 02:40:20.926751: step 82870, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 59h:49m:17s remains)
INFO - root - 2017-12-10 02:40:29.678788: step 82880, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 61h:18m:09s remains)
INFO - root - 2017-12-10 02:40:38.320096: step 82890, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:14m:11s remains)
INFO - root - 2017-12-10 02:40:46.863674: step 82900, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 58h:46m:07s remains)
2017-12-10 02:40:47.782372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033646703 -0.0033616021 -0.003361007 -0.0033607159 -0.0033604333 -0.0033603227 -0.0033605818 -0.0033608165 -0.0033609495 -0.003360963 -0.0033611218 -0.0033613981 -0.003361668 -0.003362 -0.0033620915][-0.0033623516 -0.0033588484 -0.0033580016 -0.0033575513 -0.0033572679 -0.003357203 -0.0033574644 -0.0033576866 -0.0033578633 -0.0033580114 -0.0033582528 -0.0033585744 -0.003358982 -0.0033594305 -0.00335977][-0.00336231 -0.0033584877 -0.003357419 -0.0033568018 -0.0033564128 -0.0033563026 -0.0033564684 -0.0033566379 -0.0033567024 -0.0033567972 -0.0033569213 -0.0033571972 -0.0033576507 -0.0033581811 -0.0033587755][-0.0033625127 -0.0033585478 -0.0033573254 -0.0033565154 -0.0033559375 -0.0033556034 -0.0033555618 -0.0033556004 -0.0033555056 -0.0033555233 -0.0033555282 -0.0033557822 -0.0033562002 -0.0033568053 -0.0033576754][-0.0033630487 -0.003359162 -0.003358078 -0.0033572526 -0.0033566081 -0.0033560677 -0.0033558055 -0.0033555615 -0.0033552151 -0.0033550968 -0.0033550137 -0.0033551487 -0.0033555212 -0.0033562325 -0.0033573888][-0.0033639052 -0.0033602193 -0.003359383 -0.0033586188 -0.0033580575 -0.0033574605 -0.0033569597 -0.0033564302 -0.0033557285 -0.0033553876 -0.0033551487 -0.003355182 -0.0033555231 -0.003356209 -0.0033574894][-0.0033646303 -0.0033612943 -0.0033607483 -0.0033601038 -0.0033595928 -0.0033589017 -0.0033581078 -0.0033572756 -0.0033564034 -0.0033560575 -0.003355759 -0.0033556549 -0.003355897 -0.0033565208 -0.0033577152][-0.0033649881 -0.0033618826 -0.0033615471 -0.0033611413 -0.0033607357 -0.0033598959 -0.003358908 -0.0033580572 -0.0033573005 -0.0033570374 -0.003356613 -0.003356331 -0.0033564535 -0.0033570367 -0.0033580593][-0.0033648289 -0.0033620077 -0.0033619474 -0.0033617311 -0.0033613604 -0.003360515 -0.0033597329 -0.0033591134 -0.003358532 -0.0033582654 -0.0033579045 -0.0033576288 -0.0033576258 -0.0033579634 -0.0033586081][-0.0033642172 -0.0033615949 -0.0033616826 -0.0033614412 -0.0033611574 -0.0033606153 -0.0033601974 -0.0033598908 -0.0033595858 -0.0033594568 -0.0033592102 -0.003358874 -0.0033586966 -0.0033587061 -0.00335886][-0.0033632573 -0.0033605716 -0.003360799 -0.0033606659 -0.003360502 -0.0033601618 -0.0033599406 -0.0033597911 -0.0033597536 -0.0033598361 -0.0033596426 -0.0033592512 -0.0033589972 -0.0033588789 -0.0033587753][-0.0033625111 -0.0033598049 -0.0033600002 -0.00335989 -0.0033597697 -0.0033595597 -0.0033594228 -0.0033594319 -0.0033595851 -0.0033597236 -0.0033595697 -0.0033592067 -0.0033589895 -0.0033588803 -0.0033587][-0.0033622065 -0.0033592365 -0.0033594319 -0.0033593688 -0.003359298 -0.003359169 -0.0033591294 -0.0033592533 -0.0033594589 -0.0033595627 -0.0033594456 -0.0033592444 -0.0033590994 -0.003358952 -0.0033587439][-0.0033620934 -0.0033588628 -0.0033590281 -0.0033590079 -0.0033590018 -0.0033589385 -0.0033589425 -0.0033590531 -0.0033592007 -0.0033592579 -0.0033592028 -0.0033591145 -0.003359044 -0.0033589334 -0.0033588039][-0.003362196 -0.0033588125 -0.0033589168 -0.0033589262 -0.0033589301 -0.0033589206 -0.00335893 -0.0033589816 -0.0033590351 -0.003359051 -0.0033590284 -0.0033590086 -0.0033589909 -0.0033589411 -0.0033588845]]...]
INFO - root - 2017-12-10 02:40:56.098570: step 82910, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 59h:57m:05s remains)
INFO - root - 2017-12-10 02:41:04.671429: step 82920, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 60h:07m:16s remains)
INFO - root - 2017-12-10 02:41:13.433626: step 82930, loss = 0.89, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:13m:55s remains)
INFO - root - 2017-12-10 02:41:21.987116: step 82940, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 61h:17m:27s remains)
INFO - root - 2017-12-10 02:41:30.739565: step 82950, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 59h:16m:43s remains)
INFO - root - 2017-12-10 02:41:39.207821: step 82960, loss = 0.90, batch loss = 0.69 (11.1 examples/sec; 0.723 sec/batch; 50h:05m:23s remains)
INFO - root - 2017-12-10 02:41:47.777438: step 82970, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:14m:35s remains)
INFO - root - 2017-12-10 02:41:56.309974: step 82980, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 57h:40m:50s remains)
INFO - root - 2017-12-10 02:42:04.937282: step 82990, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:40m:14s remains)
INFO - root - 2017-12-10 02:42:13.578940: step 83000, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:13m:54s remains)
2017-12-10 02:42:14.536850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033904132 -0.0033938184 -0.0033994638 -0.0034048548 -0.0034086667 -0.0034090795 -0.00340656 -0.0033972438 -0.0033632093 -0.00333312 -0.00332423 -0.0033560852 -0.0033682415 -0.0033320819 -0.0032354177][-0.0033880121 -0.0033901823 -0.0033948391 -0.0034001037 -0.0034042075 -0.0034060185 -0.0034065719 -0.0034050171 -0.0033938019 -0.0033824295 -0.0033773927 -0.0033867261 -0.0033767328 -0.0033028293 -0.0031521479][-0.0033874917 -0.0033882663 -0.0033912677 -0.0033955278 -0.0033993328 -0.0034021162 -0.0034041638 -0.0034057796 -0.0034057747 -0.0034044737 -0.0034030427 -0.0034023742 -0.0033817715 -0.003285456 -0.0030924866][-0.0033872612 -0.0033868919 -0.0033881653 -0.0033904405 -0.0033925853 -0.0033957884 -0.0033984955 -0.00340177 -0.0034060541 -0.0034091668 -0.0034098923 -0.0034069607 -0.0033849995 -0.003285764 -0.0030772614][-0.0033872139 -0.0033862996 -0.0033860111 -0.0033864051 -0.0033864642 -0.0033897059 -0.0033923991 -0.0033967881 -0.003402666 -0.0034078297 -0.0034094139 -0.0034076793 -0.0033883783 -0.0033037562 -0.0031106486][-0.0033874011 -0.0033864779 -0.003385708 -0.0033848521 -0.0033836875 -0.0033857245 -0.0033879639 -0.0033932393 -0.0033996513 -0.0034058029 -0.0034089009 -0.00340829 -0.0033937122 -0.003331559 -0.0031780265][-0.0033877182 -0.0033871843 -0.0033867904 -0.0033857962 -0.0033837021 -0.0033847776 -0.0033865329 -0.0033915897 -0.0033981055 -0.0034051081 -0.0034088942 -0.0034099622 -0.0034004427 -0.0033603134 -0.0032543535][-0.0033881261 -0.0033880109 -0.0033884388 -0.0033880202 -0.0033868745 -0.0033868079 -0.0033874654 -0.0033916596 -0.0033978089 -0.0034049465 -0.0034092877 -0.0034107438 -0.003405754 -0.0033834456 -0.003320354][-0.0033884495 -0.00338904 -0.0033903415 -0.0033911259 -0.0033913415 -0.0033909825 -0.0033910058 -0.0033937523 -0.0033988378 -0.0034053361 -0.0034097207 -0.00341134 -0.0034096378 -0.0033992776 -0.0033671225][-0.0033884342 -0.0033896721 -0.0033920791 -0.0033940834 -0.0033956002 -0.0033954412 -0.0033949513 -0.0033964606 -0.0034002755 -0.0034056788 -0.0034095584 -0.0034112572 -0.0034114809 -0.0034077482 -0.0033944519][-0.0033880589 -0.0033896496 -0.0033930065 -0.0033962785 -0.0033988685 -0.0033993011 -0.0033989176 -0.0033995607 -0.0034018746 -0.00340585 -0.0034088267 -0.0034103158 -0.0034109105 -0.0034102523 -0.0034061063][-0.003387385 -0.0033892603 -0.0033931192 -0.0033969516 -0.0033999227 -0.0034011167 -0.0034012317 -0.003401424 -0.0034022823 -0.0034047212 -0.0034068555 -0.0034077831 -0.0034080977 -0.0034087952 -0.0034090013][-0.0033863846 -0.0033882144 -0.0033919238 -0.0033956673 -0.0033987362 -0.0034002785 -0.0034006478 -0.0034006108 -0.0034009309 -0.0034020746 -0.0034033535 -0.0034037863 -0.0034043705 -0.0034057815 -0.0034070243][-0.0033857257 -0.0033870083 -0.003389986 -0.0033928677 -0.0033954273 -0.0033969672 -0.0033976263 -0.0033976694 -0.0033977521 -0.003398445 -0.0033994138 -0.0033999425 -0.0034008308 -0.0034022036 -0.0034032823][-0.0033849545 -0.0033856048 -0.0033876544 -0.0033896961 -0.0033915062 -0.0033927369 -0.0033934468 -0.0033935572 -0.0033935779 -0.0033941383 -0.0033951905 -0.0033958883 -0.0033968657 -0.0033978755 -0.0033990373]]...]
INFO - root - 2017-12-10 02:42:22.868397: step 83010, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 60h:40m:50s remains)
INFO - root - 2017-12-10 02:42:31.516683: step 83020, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:21m:38s remains)
INFO - root - 2017-12-10 02:42:40.190997: step 83030, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 58h:47m:45s remains)
INFO - root - 2017-12-10 02:42:48.747113: step 83040, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 58h:45m:54s remains)
INFO - root - 2017-12-10 02:42:57.450644: step 83050, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 59h:11m:07s remains)
INFO - root - 2017-12-10 02:43:06.009887: step 83060, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:35m:48s remains)
INFO - root - 2017-12-10 02:43:14.300720: step 83070, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:04m:36s remains)
INFO - root - 2017-12-10 02:43:22.798115: step 83080, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:27m:06s remains)
INFO - root - 2017-12-10 02:43:31.433848: step 83090, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 59h:51m:20s remains)
INFO - root - 2017-12-10 02:43:39.992187: step 83100, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:12m:43s remains)
2017-12-10 02:43:40.901984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033467121 -0.0033431628 -0.0033428527 -0.0033427794 -0.0033426904 -0.0033427526 -0.0033428106 -0.0033428771 -0.003342971 -0.0033430569 -0.0033431363 -0.0033431889 -0.0033432376 -0.0033432466 -0.0033432937][-0.0033445819 -0.0033407884 -0.0033405004 -0.0033403819 -0.0033401796 -0.003340079 -0.0033400389 -0.0033400352 -0.0033401225 -0.0033402902 -0.0033404017 -0.0033404499 -0.0033404764 -0.0033405377 -0.0033406543][-0.0033448015 -0.0033410345 -0.0033408287 -0.0033405593 -0.0033401498 -0.0033397239 -0.0033394485 -0.0033392732 -0.0033392995 -0.003339567 -0.0033398317 -0.0033400478 -0.00334015 -0.0033403074 -0.00334055][-0.0033450832 -0.0033413102 -0.0033411044 -0.0033407307 -0.0033401109 -0.0033394645 -0.0033389144 -0.0033385279 -0.0033385111 -0.003338859 -0.0033392191 -0.0033396245 -0.0033398559 -0.0033401561 -0.0033405807][-0.0033454392 -0.0033417912 -0.0033416364 -0.0033412725 -0.0033405453 -0.0033395321 -0.0033386105 -0.0033379262 -0.0033377125 -0.0033380019 -0.0033384252 -0.0033390343 -0.0033395675 -0.0033400303 -0.0033405821][-0.0033460655 -0.0033423542 -0.0033421938 -0.0033419221 -0.003341127 -0.0033397914 -0.003338316 -0.0033371386 -0.0033366028 -0.00333682 -0.0033374482 -0.0033383977 -0.0033393952 -0.0033402457 -0.0033409512][-0.0033464695 -0.0033427561 -0.0033426669 -0.0033426383 -0.003341913 -0.0033403633 -0.00333848 -0.0033366964 -0.0033358315 -0.0033357514 -0.0033364231 -0.0033376752 -0.0033390399 -0.0033402734 -0.0033412639][-0.0033467491 -0.0033430508 -0.0033431873 -0.0033437468 -0.0033435572 -0.0033424834 -0.0033408985 -0.0033390319 -0.0033375057 -0.0033365935 -0.0033366962 -0.0033377169 -0.0033391567 -0.0033405859 -0.0033417509][-0.0033470253 -0.0033433011 -0.0033435263 -0.0033447021 -0.0033453787 -0.0033451652 -0.0033442131 -0.0033427605 -0.0033410185 -0.003339397 -0.0033388881 -0.0033394247 -0.0033406194 -0.0033419977 -0.0033430988][-0.0033471098 -0.0033435081 -0.0033440227 -0.0033455468 -0.0033468192 -0.0033471952 -0.0033466835 -0.0033455095 -0.0033438983 -0.0033419142 -0.0033409952 -0.0033411824 -0.0033421039 -0.0033433414 -0.0033443645][-0.0033470294 -0.0033434138 -0.0033442343 -0.0033458879 -0.0033472301 -0.0033479778 -0.0033480769 -0.0033473 -0.003345842 -0.0033438907 -0.0033427631 -0.0033426401 -0.0033431838 -0.0033441042 -0.0033449025][-0.0033472388 -0.0033435188 -0.0033443402 -0.0033458297 -0.0033471789 -0.0033481536 -0.0033486038 -0.0033482076 -0.0033470236 -0.0033453703 -0.0033442727 -0.0033438739 -0.0033440688 -0.0033445912 -0.0033450937][-0.0033475887 -0.0033436164 -0.0033442366 -0.0033453424 -0.0033464353 -0.0033472707 -0.0033476902 -0.0033475764 -0.0033469072 -0.0033457614 -0.0033450061 -0.0033447021 -0.00334474 -0.0033449866 -0.0033452166][-0.0033477985 -0.0033436576 -0.0033442571 -0.0033448532 -0.0033453582 -0.0033457845 -0.0033460276 -0.0033458835 -0.0033454127 -0.0033448762 -0.0033446252 -0.0033445845 -0.0033447223 -0.0033449747 -0.0033451139][-0.0033479843 -0.0033436806 -0.0033441305 -0.0033443663 -0.0033444993 -0.0033445829 -0.0033446662 -0.0033445873 -0.00334434 -0.003344168 -0.0033442066 -0.0033442846 -0.003344422 -0.0033445116 -0.0033444804]]...]
INFO - root - 2017-12-10 02:43:49.285037: step 83110, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:04m:08s remains)
INFO - root - 2017-12-10 02:43:57.836119: step 83120, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:21m:43s remains)
INFO - root - 2017-12-10 02:44:06.488770: step 83130, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 61h:05m:45s remains)
INFO - root - 2017-12-10 02:44:15.133550: step 83140, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 58h:24m:20s remains)
INFO - root - 2017-12-10 02:44:23.759577: step 83150, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 59h:55m:42s remains)
INFO - root - 2017-12-10 02:44:32.335850: step 83160, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 59h:14m:51s remains)
INFO - root - 2017-12-10 02:44:40.777249: step 83170, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 61h:40m:30s remains)
INFO - root - 2017-12-10 02:44:49.395211: step 83180, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:06m:59s remains)
INFO - root - 2017-12-10 02:44:58.027992: step 83190, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:28m:00s remains)
INFO - root - 2017-12-10 02:45:06.815185: step 83200, loss = 0.90, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 59h:56m:00s remains)
2017-12-10 02:45:07.784959: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012255356 0.020664426 0.030543884 0.040166054 0.047679674 0.0527579 0.055482496 0.05610076 0.055300947 0.053689718 0.051598791 0.048903752 0.045901414 0.042398687 0.037969638][0.018437231 0.031107618 0.046382554 0.062006548 0.074635774 0.083177246 0.08743652 0.088409588 0.087272272 0.085025877 0.082272448 0.078916565 0.074993439 0.069858231 0.063069046][0.028081665 0.04738671 0.07070411 0.094930224 0.11496603 0.12883013 0.13580988 0.13740818 0.13555323 0.13139737 0.12578271 0.11888132 0.11116896 0.10248923 0.092039689][0.039414812 0.065538704 0.097243309 0.12997542 0.15708327 0.17613056 0.18594402 0.18853354 0.18602468 0.18020269 0.17200555 0.16127662 0.14912456 0.13576995 0.12068079][0.049736269 0.080886625 0.11822367 0.15664259 0.18843059 0.21064532 0.22225174 0.22567554 0.22292911 0.21551514 0.20460436 0.19019783 0.17417143 0.15716653 0.13889512][0.055175554 0.08862485 0.12840541 0.16887709 0.2024802 0.22572312 0.23779838 0.24102925 0.23720211 0.22786632 0.21425049 0.19664223 0.17752263 0.15805297 0.13840792][0.054664902 0.087808 0.12715386 0.16733848 0.20106779 0.22471286 0.23677287 0.23890457 0.23280407 0.22005966 0.20281619 0.18177584 0.16012178 0.13952355 0.12021436][0.049617235 0.080516294 0.11755081 0.15582748 0.18874089 0.21221368 0.22383918 0.224447 0.21538784 0.19848672 0.1770107 0.15279995 0.12962484 0.10908598 0.09120068][0.04186295 0.069056317 0.10230498 0.13761626 0.16853468 0.19038226 0.20026489 0.19835556 0.18591283 0.16513921 0.14058325 0.115162 0.092604905 0.073990569 0.059043195][0.033061791 0.055202626 0.082869753 0.11323863 0.1401801 0.15855391 0.16508602 0.15994307 0.1448199 0.12236175 0.097788475 0.074501082 0.055642545 0.04133838 0.03065055][0.024376156 0.040955648 0.061947539 0.085323744 0.10604119 0.11946774 0.1221101 0.11438513 0.0985549 0.077692412 0.05653337 0.038236883 0.024975957 0.016089274 0.010192067][0.016269911 0.027788648 0.042309277 0.0581946 0.071826361 0.079602234 0.0787994 0.0702664 0.056355979 0.040108774 0.025032006 0.01333186 0.00601946 0.0020457332 -8.8693341e-05][0.0087704277 0.016119311 0.025220949 0.034851097 0.042630479 0.046123561 0.043838531 0.036650207 0.026678516 0.01619041 0.0073434142 0.0013852485 -0.0016353033 -0.0027822321 -0.0031812598][0.0026009337 0.0067863893 0.011810031 0.016870286 0.020554252 0.021684039 0.01968088 0.015177507 0.00956426 0.0041693905 0.0001352001 -0.0021985825 -0.0031057184 -0.0032930376 -0.0033164395][-0.0011690499 0.00075345649 0.0030667915 0.0052750176 0.0066662552 0.0068670129 0.0057094656 0.0035579496 0.00110597 -0.001022002 -0.002387234 -0.0030743193 -0.0032858229 -0.0033150809 -0.0033231573]]...]
INFO - root - 2017-12-10 02:45:16.276378: step 83210, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:37m:24s remains)
INFO - root - 2017-12-10 02:45:24.924681: step 83220, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 58h:29m:14s remains)
INFO - root - 2017-12-10 02:45:33.602208: step 83230, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 61h:20m:07s remains)
INFO - root - 2017-12-10 02:45:42.010213: step 83240, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:00m:22s remains)
INFO - root - 2017-12-10 02:45:50.703651: step 83250, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:07m:01s remains)
INFO - root - 2017-12-10 02:45:59.319874: step 83260, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:38m:51s remains)
INFO - root - 2017-12-10 02:46:07.878073: step 83270, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:38m:56s remains)
INFO - root - 2017-12-10 02:46:16.506573: step 83280, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 60h:25m:54s remains)
INFO - root - 2017-12-10 02:46:24.894504: step 83290, loss = 0.90, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 56h:39m:40s remains)
INFO - root - 2017-12-10 02:46:33.354772: step 83300, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 58h:20m:16s remains)
2017-12-10 02:46:34.246706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033774194 -0.0033754904 -0.003375367 -0.00337548 -0.0033756259 -0.0033757687 -0.0033758602 -0.0033759449 -0.0033759971 -0.0033758848 -0.0033757559 -0.0033756548 -0.0033755659 -0.0033755254 -0.003375435][-0.0033757542 -0.0033736357 -0.0033735209 -0.0033737223 -0.0033739607 -0.0033741472 -0.0033742553 -0.0033743496 -0.0033743922 -0.0033743349 -0.0033742441 -0.0033742047 -0.0033742224 -0.0033742157 -0.0033741251][-0.0033758644 -0.0033735188 -0.003373326 -0.003373483 -0.0033737104 -0.0033738704 -0.0033739652 -0.0033740222 -0.0033740296 -0.0033739985 -0.0033739381 -0.0033739826 -0.0033741414 -0.0033742271 -0.0033741945][-0.0033757172 -0.0033731482 -0.0033728003 -0.0033728089 -0.0033729314 -0.0033730657 -0.0033731351 -0.0033731407 -0.0033731021 -0.003373082 -0.0033731146 -0.003373326 -0.0033736792 -0.0033739388 -0.0033740662][-0.0033756159 -0.0033729346 -0.003372476 -0.0033723293 -0.0033723679 -0.0033724583 -0.003372439 -0.0033723125 -0.003372187 -0.0033721433 -0.0033722222 -0.0033725153 -0.0033730289 -0.0033734867 -0.0033738259][-0.0033756413 -0.0033728115 -0.0033723197 -0.0033721486 -0.0033721307 -0.0033722031 -0.0033721963 -0.0033720082 -0.0033717807 -0.0033716359 -0.0033716597 -0.0033719395 -0.003372537 -0.0033731253 -0.0033736152][-0.003375564 -0.0033726839 -0.0033721607 -0.0033719572 -0.0033718669 -0.0033718629 -0.0033718394 -0.0033716131 -0.003371313 -0.0033711067 -0.0033711179 -0.0033714233 -0.00337207 -0.0033727442 -0.0033733412][-0.0033755628 -0.0033727323 -0.0033721803 -0.0033718657 -0.003371676 -0.0033716105 -0.0033715332 -0.0033712236 -0.0033708941 -0.0033706983 -0.0033707132 -0.0033710422 -0.0033717202 -0.0033724161 -0.0033730573][-0.0033755573 -0.0033728385 -0.0033723854 -0.0033720208 -0.003371818 -0.0033717728 -0.003371675 -0.0033713239 -0.0033710198 -0.003370872 -0.0033709216 -0.003371232 -0.0033718029 -0.0033723854 -0.0033729456][-0.0033756294 -0.0033729698 -0.0033727002 -0.0033724417 -0.0033723123 -0.0033722974 -0.003372283 -0.00337208 -0.0033718613 -0.0033717041 -0.0033716836 -0.0033718727 -0.0033722494 -0.0033726345 -0.0033730427][-0.0033755812 -0.0033730194 -0.0033728853 -0.0033727027 -0.0033726206 -0.0033726012 -0.0033726324 -0.0033725784 -0.0033724853 -0.0033723954 -0.0033723894 -0.0033725114 -0.0033727244 -0.0033729426 -0.0033731768][-0.0033755822 -0.0033731346 -0.0033730431 -0.0033728431 -0.0033727637 -0.0033727055 -0.0033727 -0.0033727235 -0.0033727488 -0.0033727456 -0.0033727603 -0.0033728709 -0.003373021 -0.0033731575 -0.0033732969][-0.0033757444 -0.0033732206 -0.0033731812 -0.003373008 -0.0033729493 -0.0033729062 -0.0033729132 -0.0033729787 -0.0033730569 -0.0033730892 -0.0033731319 -0.0033732329 -0.0033733321 -0.0033733903 -0.0033734578][-0.0033759545 -0.0033733412 -0.0033733808 -0.0033732757 -0.0033732187 -0.0033731717 -0.0033731784 -0.0033732511 -0.0033733151 -0.0033733626 -0.0033734206 -0.0033734981 -0.0033735405 -0.0033735461 -0.0033735703][-0.003376147 -0.0033735076 -0.0033735523 -0.0033735083 -0.0033734792 -0.0033734557 -0.0033734657 -0.0033735118 -0.0033735451 -0.003373561 -0.0033735768 -0.0033736017 -0.0033736096 -0.0033736024 -0.0033736089]]...]
INFO - root - 2017-12-10 02:46:42.595270: step 83310, loss = 0.88, batch loss = 0.67 (9.7 examples/sec; 0.824 sec/batch; 57h:01m:50s remains)
INFO - root - 2017-12-10 02:46:51.284130: step 83320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 58h:06m:10s remains)
INFO - root - 2017-12-10 02:46:59.936908: step 83330, loss = 0.88, batch loss = 0.67 (9.1 examples/sec; 0.879 sec/batch; 60h:49m:35s remains)
INFO - root - 2017-12-10 02:47:08.504588: step 83340, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 60h:39m:43s remains)
INFO - root - 2017-12-10 02:47:17.195005: step 83350, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:15m:08s remains)
INFO - root - 2017-12-10 02:47:25.940042: step 83360, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.849 sec/batch; 58h:45m:50s remains)
INFO - root - 2017-12-10 02:47:34.343100: step 83370, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 58h:45m:14s remains)
INFO - root - 2017-12-10 02:47:42.923741: step 83380, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:19m:06s remains)
INFO - root - 2017-12-10 02:47:51.541565: step 83390, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 59h:12m:24s remains)
INFO - root - 2017-12-10 02:48:00.135906: step 83400, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 58h:07m:15s remains)
2017-12-10 02:48:01.050093: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35420793 0.34463108 0.33481371 0.32360354 0.31104651 0.29814422 0.28367364 0.27064592 0.25835827 0.24727139 0.23744872 0.23053607 0.22559611 0.22016726 0.21524361][0.36190227 0.35881966 0.35456178 0.34744108 0.33736846 0.32583025 0.311085 0.29636994 0.28132612 0.2675454 0.25473297 0.24428852 0.23697726 0.22949751 0.22285405][0.36506817 0.3700431 0.37277105 0.37090424 0.36464393 0.3543084 0.33953112 0.32315579 0.30524364 0.28886077 0.27202222 0.25777784 0.24677369 0.23677561 0.22811617][0.36869755 0.38300878 0.3940202 0.3984538 0.39665303 0.38829735 0.37314984 0.35484606 0.33419073 0.31432581 0.29376471 0.27512926 0.25974533 0.24623661 0.23491408][0.37252197 0.39556849 0.41404852 0.42589575 0.42986366 0.42424634 0.4104493 0.39096123 0.36744514 0.3434442 0.3176291 0.29365024 0.27338359 0.2558654 0.24146254][0.37857035 0.40907905 0.43273145 0.4485727 0.4561137 0.4541693 0.44318798 0.42427838 0.40010583 0.37351668 0.34335232 0.313854 0.28781658 0.26595467 0.24804875][0.38721547 0.42368683 0.45092702 0.46937495 0.47854254 0.47752985 0.46803534 0.45067182 0.42682123 0.39912367 0.36706865 0.3337929 0.30326971 0.27655369 0.25480023][0.39691457 0.43617389 0.46343082 0.48242781 0.491815 0.49097905 0.48187733 0.46512362 0.44181252 0.4139955 0.38198298 0.34772906 0.31495824 0.28548163 0.26044834][0.40203485 0.44334254 0.47021741 0.48846215 0.4972471 0.49636403 0.48763868 0.47145551 0.4490484 0.42156288 0.39022204 0.35622638 0.32320288 0.29225037 0.26501343][0.40298364 0.44349697 0.46782991 0.48417664 0.49218389 0.49175149 0.48427469 0.46974859 0.44916144 0.42308009 0.39313957 0.36026636 0.327796 0.29670954 0.2683498][0.39937395 0.43794873 0.458924 0.47202471 0.47845355 0.47841689 0.47234181 0.4602066 0.44234368 0.41905814 0.39183691 0.36112791 0.32987085 0.29942957 0.27067715][0.39060044 0.42645711 0.44357783 0.45375124 0.45874879 0.45871007 0.45347232 0.44300842 0.42782739 0.40738559 0.38327482 0.35562348 0.32700726 0.29843298 0.27047181][0.37142834 0.40423352 0.41870973 0.42715549 0.43167353 0.43213764 0.42825854 0.41948023 0.40674624 0.3892099 0.36797857 0.3436394 0.31792602 0.29254022 0.26698536][0.34569481 0.37570423 0.3884027 0.39558369 0.3999339 0.40131122 0.39928108 0.39269334 0.38258234 0.36826813 0.35012847 0.3289054 0.30624568 0.28371263 0.26099145][0.31759891 0.34370324 0.35368335 0.35914734 0.36237547 0.36405307 0.36341015 0.3594856 0.35285175 0.34202668 0.32766137 0.31018078 0.29091093 0.27170983 0.25241327]]...]
INFO - root - 2017-12-10 02:48:09.536288: step 83410, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 58h:06m:10s remains)
INFO - root - 2017-12-10 02:48:18.030813: step 83420, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 58h:30m:45s remains)
INFO - root - 2017-12-10 02:48:26.472979: step 83430, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 59h:50m:54s remains)
INFO - root - 2017-12-10 02:48:34.916044: step 83440, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 50h:24m:55s remains)
INFO - root - 2017-12-10 02:48:43.555691: step 83450, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 58h:40m:14s remains)
INFO - root - 2017-12-10 02:48:52.154621: step 83460, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 60h:48m:33s remains)
INFO - root - 2017-12-10 02:49:00.605697: step 83470, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:19m:33s remains)
INFO - root - 2017-12-10 02:49:09.161394: step 83480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:09m:27s remains)
INFO - root - 2017-12-10 02:49:17.761433: step 83490, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:20m:50s remains)
INFO - root - 2017-12-10 02:49:26.324171: step 83500, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 57h:23m:54s remains)
2017-12-10 02:49:27.265119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.003377459 -0.0033756285 -0.0033755128 -0.0033756185 -0.0033755505 -0.00337545 -0.0033753384 -0.00337517 -0.0033749687 -0.0033746448 -0.0033743661 -0.0033741116 -0.0033740264 -0.0033740052 -0.0033739929][-0.0033759389 -0.0033740704 -0.0033739733 -0.0033740934 -0.0033740238 -0.0033739835 -0.0033738613 -0.0033737316 -0.0033735458 -0.0033733635 -0.0033731582 -0.0033729158 -0.0033728876 -0.0033729069 -0.0033728983][-0.0033758674 -0.0033738532 -0.0033737675 -0.0033738529 -0.0033738585 -0.0033738369 -0.0033737796 -0.0033736655 -0.0033734757 -0.0033733824 -0.0033731204 -0.0033728587 -0.0033728098 -0.0033728289 -0.0033728655][-0.0033757954 -0.0033736785 -0.0033735754 -0.00337364 -0.0033737447 -0.0033738709 -0.0033739419 -0.0033738269 -0.0033736962 -0.0033735558 -0.0033732189 -0.0033729142 -0.0033728348 -0.0033728981 -0.0033728916][-0.0033755088 -0.0033730753 -0.0033729984 -0.0033731111 -0.0033733246 -0.0033736469 -0.0033738012 -0.0033737826 -0.0033737486 -0.0033736504 -0.003373309 -0.0033729482 -0.0033728785 -0.0033728948 -0.0033728029][-0.0033747121 -0.0033718396 -0.0033717514 -0.003371954 -0.0033722806 -0.0033727798 -0.00337302 -0.0033730527 -0.0033731747 -0.0033733048 -0.0033730678 -0.0033726855 -0.0033724604 -0.0033724254 -0.0033722685][-0.0033741568 -0.0033709877 -0.0033706948 -0.0033708818 -0.0033710541 -0.0033713568 -0.0033714771 -0.0033717565 -0.003372103 -0.0033723249 -0.00337231 -0.0033719253 -0.0033716455 -0.0033714534 -0.0033711831][-0.0033747915 -0.0033714634 -0.003371092 -0.0033713442 -0.0033713724 -0.0033713346 -0.0033711225 -0.003371649 -0.0033720871 -0.0033723381 -0.0033725009 -0.0033720536 -0.0033717242 -0.0033712597 -0.0033706636][-0.0033764387 -0.0033733463 -0.00337306 -0.0033734846 -0.0033736136 -0.0033736059 -0.0033734962 -0.0033736047 -0.0033738629 -0.003374167 -0.0033743009 -0.0033737076 -0.0033729887 -0.003371998 -0.0033709728][-0.0033782765 -0.0033751153 -0.0033750897 -0.0033757458 -0.0033760141 -0.0033761866 -0.0033762518 -0.0033761358 -0.0033760588 -0.0033761286 -0.0033760983 -0.003375347 -0.0033742411 -0.0033729237 -0.0033716534][-0.0033800893 -0.0033765959 -0.0033765021 -0.0033772322 -0.0033776844 -0.0033779324 -0.0033780052 -0.0033778225 -0.0033775994 -0.0033775792 -0.0033774204 -0.003376605 -0.0033753994 -0.0033739593 -0.0033725637][-0.0033821042 -0.0033779542 -0.0033775871 -0.0033781386 -0.0033786485 -0.0033788839 -0.003378913 -0.0033787435 -0.0033784653 -0.0033783847 -0.0033781142 -0.0033773133 -0.0033761873 -0.0033747707 -0.0033734138][-0.0033838763 -0.0033787068 -0.00337772 -0.0033777778 -0.0033782024 -0.0033784057 -0.0033784057 -0.0033782294 -0.0033779438 -0.0033777165 -0.003377228 -0.0033764029 -0.0033753738 -0.0033742147 -0.0033730702][-0.0033848758 -0.0033786127 -0.0033767042 -0.0033760814 -0.0033762213 -0.0033762441 -0.0033762765 -0.0033761084 -0.0033758744 -0.0033756434 -0.0033751281 -0.0033744129 -0.0033736315 -0.0033728546 -0.0033720776][-0.0033853755 -0.0033782723 -0.0033758741 -0.0033745037 -0.0033743812 -0.0033742231 -0.0033742965 -0.0033742085 -0.0033742131 -0.0033742457 -0.0033740336 -0.0033735281 -0.0033729763 -0.0033723409 -0.0033716925]]...]
INFO - root - 2017-12-10 02:49:35.794229: step 83510, loss = 0.90, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 53h:03m:52s remains)
INFO - root - 2017-12-10 02:49:44.472997: step 83520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:07m:04s remains)
INFO - root - 2017-12-10 02:49:53.149871: step 83530, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 59h:47m:37s remains)
INFO - root - 2017-12-10 02:50:01.838022: step 83540, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:35m:30s remains)
INFO - root - 2017-12-10 02:50:10.379221: step 83550, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 59h:37m:26s remains)
INFO - root - 2017-12-10 02:50:18.994072: step 83560, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:36m:59s remains)
INFO - root - 2017-12-10 02:50:27.432906: step 83570, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:26m:00s remains)
INFO - root - 2017-12-10 02:50:36.031016: step 83580, loss = 0.89, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 58h:52m:39s remains)
INFO - root - 2017-12-10 02:50:44.721763: step 83590, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:34m:25s remains)
INFO - root - 2017-12-10 02:50:53.373434: step 83600, loss = 0.88, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 59h:47m:22s remains)
2017-12-10 02:50:54.255965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034013886 -0.0033998278 -0.0033997516 -0.0033998436 -0.0033999886 -0.0034000804 -0.0033999949 -0.003399946 -0.0033999549 -0.0033998701 -0.0033998892 -0.0033999784 -0.0034001872 -0.0034005444 -0.0034011123][-0.0034001225 -0.0033982564 -0.003398035 -0.0033982317 -0.0033984517 -0.0033985497 -0.0033984126 -0.003398265 -0.0033980941 -0.0033979618 -0.0033979968 -0.0033981381 -0.0033985772 -0.0033991784 -0.0033999579][-0.0033999106 -0.0033979104 -0.0033975083 -0.0033978163 -0.0033981109 -0.0033982436 -0.0033980967 -0.0033978694 -0.0033975889 -0.003397367 -0.0033973809 -0.0033976322 -0.0033981975 -0.0033990222 -0.0033998375][-0.0033994091 -0.0033972792 -0.00339691 -0.003397279 -0.0033975218 -0.0033976329 -0.0033975097 -0.0033973833 -0.0033971572 -0.0033969302 -0.0033970356 -0.0033974943 -0.0033982585 -0.003399191 -0.0033997446][-0.0033990536 -0.0033967677 -0.0033964803 -0.0033969753 -0.0033972845 -0.0033973923 -0.0033972117 -0.003397062 -0.0033969563 -0.0033969078 -0.0033971905 -0.003397885 -0.0033988969 -0.0033998198 -0.0034004254][-0.0033988522 -0.0033963881 -0.0033961651 -0.003396732 -0.0033971178 -0.0033973514 -0.0033972953 -0.0033972394 -0.00339721 -0.0033972822 -0.0033977288 -0.0033987428 -0.0034001414 -0.0034012911 -0.0034022653][-0.0033987819 -0.0033962096 -0.0033958929 -0.0033964012 -0.0033968524 -0.00339712 -0.0033972347 -0.0033975558 -0.0033978773 -0.003398218 -0.0033988254 -0.0033999344 -0.0034016687 -0.0034032543 -0.0034046897][-0.003398414 -0.00339579 -0.0033955427 -0.0033960615 -0.0033966687 -0.0033970645 -0.0033973393 -0.0033980277 -0.0033986578 -0.003399303 -0.003400147 -0.0034015 -0.0034035142 -0.0034052392 -0.0034071135][-0.0033979465 -0.0033951397 -0.0033949357 -0.0033954671 -0.0033963341 -0.0033970289 -0.0033975209 -0.0033985181 -0.0033994608 -0.0034004077 -0.0034015048 -0.0034031556 -0.0034055158 -0.0034076357 -0.0034097098][-0.0033976636 -0.0033949157 -0.0033946056 -0.0033949695 -0.0033957462 -0.0033966426 -0.0033975451 -0.0033989635 -0.0034002685 -0.0034013786 -0.0034027286 -0.0034045335 -0.0034068632 -0.0034091368 -0.0034110907][-0.0033971926 -0.0033945721 -0.0033942577 -0.0033944873 -0.0033952382 -0.0033963441 -0.0033975369 -0.0033991092 -0.0034005565 -0.0034017204 -0.0034029784 -0.0034044404 -0.0034062525 -0.0034083086 -0.0034102646][-0.003397038 -0.00339446 -0.0033942463 -0.003394441 -0.0033949232 -0.0033960403 -0.0033972925 -0.0033988378 -0.0034002387 -0.0034014769 -0.003402499 -0.0034033915 -0.0034044427 -0.0034057589 -0.0034075291][-0.0033973204 -0.0033946428 -0.0033943818 -0.0033944864 -0.0033948645 -0.0033958184 -0.0033970445 -0.003398634 -0.0034001248 -0.003401299 -0.003401987 -0.0034022275 -0.0034025311 -0.0034030373 -0.0034041347][-0.0033978738 -0.003395342 -0.0033950894 -0.0033951311 -0.0033954433 -0.0033962282 -0.0033972994 -0.0033987323 -0.0034002659 -0.003401377 -0.0034016105 -0.0034012247 -0.0034008462 -0.0034007926 -0.0034011833][-0.0033983232 -0.003395888 -0.003395699 -0.0033957758 -0.0033961425 -0.0033968878 -0.0033977928 -0.0033991055 -0.0034005421 -0.0034014843 -0.0034014867 -0.0034007393 -0.0033999102 -0.0033993265 -0.0033990594]]...]
INFO - root - 2017-12-10 02:51:02.819026: step 83610, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 58h:35m:22s remains)
INFO - root - 2017-12-10 02:51:11.443930: step 83620, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 61h:33m:01s remains)
INFO - root - 2017-12-10 02:51:20.102405: step 83630, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 58h:38m:26s remains)
INFO - root - 2017-12-10 02:51:28.496279: step 83640, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 57h:37m:02s remains)
INFO - root - 2017-12-10 02:51:36.826909: step 83650, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:10m:33s remains)
INFO - root - 2017-12-10 02:51:45.358226: step 83660, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 59h:48m:50s remains)
INFO - root - 2017-12-10 02:51:53.836573: step 83670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 58h:56m:07s remains)
INFO - root - 2017-12-10 02:52:02.339745: step 83680, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 58h:52m:39s remains)
INFO - root - 2017-12-10 02:52:10.969021: step 83690, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 62h:22m:26s remains)
INFO - root - 2017-12-10 02:52:19.801575: step 83700, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 60h:45m:40s remains)
2017-12-10 02:52:20.809175: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014646616 0.016272103 0.017440178 0.017466471 0.0163532 0.014214713 0.011823766 0.0095939375 0.0077199833 0.0056886133 0.0033032957 0.00086935633 -0.0011020629 -0.0023974907 -0.0030592436][0.015048845 0.017977284 0.020963376 0.022805821 0.023102773 0.021822972 0.019680506 0.017212776 0.014518198 0.011185922 0.0071990015 0.0032425935 9.7643351e-05 -0.0019109361 -0.0029123286][0.014740711 0.018763505 0.023481779 0.027290216 0.029407633 0.029661605 0.028599964 0.026756991 0.023812136 0.019403795 0.013643576 0.0076784566 0.0026677789 -0.00070895161 -0.0025022496][0.015710298 0.020425471 0.026529608 0.032180004 0.036380902 0.0386824 0.039228726 0.038224891 0.035032596 0.029479085 0.021822389 0.013665754 0.0064650439 0.0013011585 -0.0017029826][0.018471621 0.023283092 0.030155042 0.0373004 0.043558139 0.048143324 0.050815649 0.051137175 0.048140958 0.041628331 0.032035284 0.021393439 0.01155501 0.004134397 -0.0004861427][0.023478437 0.027473535 0.033967067 0.041547332 0.048996914 0.055259511 0.059751026 0.061562881 0.059456453 0.052946355 0.042284518 0.029711852 0.017447649 0.0076698093 0.0011678843][0.030608477 0.033167291 0.038093358 0.044626072 0.051735729 0.058458216 0.064084716 0.067332916 0.066633508 0.060945354 0.05014478 0.036528613 0.022604663 0.010993316 0.0028439604][0.037820805 0.038567632 0.041008916 0.045354318 0.050987195 0.0572597 0.063391291 0.067876868 0.068762943 0.064350493 0.054119442 0.040377688 0.025782723 0.013189491 0.0040172907][0.043128841 0.041777514 0.041139975 0.042757582 0.0464913 0.051961958 0.058284137 0.0637176 0.065885924 0.0626373 0.053341534 0.04026483 0.026026918 0.013502534 0.0042251339][0.044963177 0.041499652 0.037775874 0.036764082 0.038754132 0.043340839 0.049468208 0.055190716 0.05791888 0.055474032 0.047355656 0.035675772 0.022862637 0.011573245 0.0032616861][0.041593276 0.036883686 0.031185666 0.028280215 0.028768333 0.032256503 0.037592623 0.042765219 0.045365311 0.043476854 0.036814753 0.027213963 0.016816694 0.0078323586 0.0014086843][0.03357619 0.028420476 0.022135217 0.018422876 0.017927591 0.020265982 0.024298109 0.028313296 0.030366892 0.028961623 0.024013342 0.016981686 0.0095972549 0.0034712933 -0.00065470859][0.02319457 0.018458864 0.012882894 0.0093493471 0.0083739646 0.0096224817 0.012158806 0.014749523 0.016053913 0.015124289 0.01198692 0.0076577421 0.0033044412 -9.0606278e-05 -0.002193111][0.012490928 0.0089462269 0.0050655045 0.0025465346 0.0017068929 0.0022556402 0.0035831963 0.0049223034 0.0055561466 0.005018997 0.0033857494 0.001249417 -0.00076041464 -0.0021947557 -0.0029963676][0.0037968296 0.001745323 -0.00027144304 -0.0015271986 -0.001947999 -0.0017159795 -0.001148449 -0.00061374204 -0.00037741102 -0.00059378403 -0.0012058537 -0.0019761093 -0.0026490204 -0.0030787508 -0.0032934253]]...]
INFO - root - 2017-12-10 02:52:29.210352: step 83710, loss = 0.90, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 51h:56m:22s remains)
INFO - root - 2017-12-10 02:52:37.960653: step 83720, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 62h:38m:25s remains)
INFO - root - 2017-12-10 02:52:46.704767: step 83730, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 58h:47m:35s remains)
INFO - root - 2017-12-10 02:52:55.543630: step 83740, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:07m:46s remains)
INFO - root - 2017-12-10 02:53:04.101256: step 83750, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 59h:55m:07s remains)
INFO - root - 2017-12-10 02:53:12.721718: step 83760, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 60h:03m:49s remains)
INFO - root - 2017-12-10 02:53:21.293722: step 83770, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 59h:42m:48s remains)
INFO - root - 2017-12-10 02:53:29.908641: step 83780, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:22m:15s remains)
INFO - root - 2017-12-10 02:53:38.473549: step 83790, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.903 sec/batch; 62h:24m:08s remains)
INFO - root - 2017-12-10 02:53:47.189476: step 83800, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 62h:26m:13s remains)
2017-12-10 02:53:48.102390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033883131 -0.0033877622 -0.0033743358 -0.0033223727 -0.0032179719 -0.00309203 -0.0030227767 -0.0030296738 -0.0030950694 -0.0031974635 -0.0032922388 -0.0033604973 -0.0033819124 -0.0033886223 -0.0033918831][-0.0033893224 -0.0033871306 -0.0033634966 -0.0032763619 -0.0030944366 -0.0028549202 -0.0026864903 -0.0026716173 -0.002799754 -0.0030083375 -0.0031960022 -0.0033280724 -0.0033766029 -0.0033892656 -0.0033932992][-0.0033899944 -0.0033824034 -0.0033355367 -0.0031930022 -0.0029127973 -0.0025343988 -0.0021999564 -0.0020509618 -0.002182974 -0.0025482951 -0.0029578502 -0.0032491356 -0.0033638575 -0.0033896884 -0.0033948305][-0.0033890619 -0.0033723521 -0.0032787269 -0.0030046485 -0.0024734589 -0.0017768759 -0.0011840591 -0.00094979932 -0.0012005854 -0.001807361 -0.0025136517 -0.0030528847 -0.0033176003 -0.0033867969 -0.0033942054][-0.00338796 -0.0033611427 -0.0032072852 -0.0027496587 -0.0018534472 -0.00067966734 0.00032281736 0.00071619195 0.00031500729 -0.00065440196 -0.0018245147 -0.0027319889 -0.00321763 -0.0033711903 -0.0033901059][-0.0033874861 -0.0033561769 -0.0031607021 -0.0025495191 -0.0013211386 0.00030453736 0.0016845802 0.0022168497 0.0017316721 0.00051479531 -0.0010093576 -0.0022829152 -0.0030543811 -0.0033414129 -0.003386307][-0.0033862938 -0.0033561965 -0.0031562124 -0.0025052736 -0.0011578177 0.00067041139 0.0022547601 0.0028982379 0.0024225919 0.0011409877 -0.00050298055 -0.0019576282 -0.0029055933 -0.0033061036 -0.0033828272][-0.0033850097 -0.0033600153 -0.0031896487 -0.0026235026 -0.001423293 0.00026465207 0.0018210264 0.0025711295 0.0022638992 0.0011310952 -0.00042168354 -0.0018694672 -0.0028618935 -0.0032933129 -0.0033791012][-0.0033827431 -0.0033678669 -0.0032559014 -0.0028606553 -0.0019744188 -0.000634111 0.00073676812 0.0015756877 0.0015329381 0.00067445659 -0.00067253737 -0.0020069296 -0.0029303972 -0.00331269 -0.003380799][-0.0033788406 -0.0033731442 -0.00331995 -0.0031030946 -0.0025558821 -0.0016143752 -0.00049331109 0.00036869268 0.00055607571 -3.2265438e-05 -0.001155324 -0.0022980075 -0.0030604436 -0.0033426171 -0.003381582][-0.0033761857 -0.0033743342 -0.0033561664 -0.0032624877 -0.0029859128 -0.0024281945 -0.0016467265 -0.00092529389 -0.00065181032 -0.0010160548 -0.0018461243 -0.0026841606 -0.0032025857 -0.0033686173 -0.0033850013][-0.0033797445 -0.0033788432 -0.0033730213 -0.0033403046 -0.0032322146 -0.0029792909 -0.0025681949 -0.0021290849 -0.0019199623 -0.0021018379 -0.0025776513 -0.0030446674 -0.0033073956 -0.0033793796 -0.0033851843][-0.0033790637 -0.003379755 -0.0033814504 -0.0033773957 -0.0033519573 -0.0032785314 -0.0031411459 -0.0029769507 -0.0028877442 -0.0029455442 -0.0031177639 -0.0032810918 -0.0033629944 -0.0033819287 -0.0033839603][-0.0033777736 -0.0033776555 -0.0033794788 -0.0033813529 -0.0033811978 -0.0033735007 -0.0033527822 -0.0033225941 -0.0033038831 -0.0033104226 -0.0033369858 -0.0033645388 -0.0033774194 -0.0033806008 -0.0033828644][-0.0033769386 -0.0033758485 -0.0033769996 -0.0033786031 -0.0033802658 -0.0033811196 -0.0033814013 -0.0033803252 -0.0033794332 -0.0033783864 -0.0033779165 -0.0033778213 -0.0033779598 -0.0033788548 -0.0033802302]]...]
INFO - root - 2017-12-10 02:53:56.551510: step 83810, loss = 0.90, batch loss = 0.70 (10.8 examples/sec; 0.738 sec/batch; 51h:00m:21s remains)
INFO - root - 2017-12-10 02:54:05.253111: step 83820, loss = 0.90, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 62h:58m:17s remains)
INFO - root - 2017-12-10 02:54:13.984644: step 83830, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 60h:47m:29s remains)
INFO - root - 2017-12-10 02:54:22.620294: step 83840, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 59h:20m:37s remains)
INFO - root - 2017-12-10 02:54:31.260247: step 83850, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 59h:03m:30s remains)
INFO - root - 2017-12-10 02:54:39.975011: step 83860, loss = 0.89, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 61h:20m:23s remains)
INFO - root - 2017-12-10 02:54:48.431136: step 83870, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 58h:17m:24s remains)
INFO - root - 2017-12-10 02:54:57.117342: step 83880, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 62h:01m:33s remains)
INFO - root - 2017-12-10 02:55:05.713098: step 83890, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 57h:44m:55s remains)
INFO - root - 2017-12-10 02:55:14.078323: step 83900, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.811 sec/batch; 56h:00m:43s remains)
2017-12-10 02:55:14.945204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033643241 -0.0033616731 -0.0033610193 -0.0033616452 -0.0033623301 -0.0033630198 -0.0033637616 -0.0033645371 -0.0033651947 -0.0033652841 -0.0033653306 -0.0033654254 -0.0033652685 -0.0033647961 -0.0033644824][-0.0033685048 -0.0033661951 -0.0033653115 -0.0033655937 -0.00336592 -0.0033664019 -0.0033669269 -0.0033674676 -0.0033680166 -0.0033680848 -0.0033680373 -0.0033679036 -0.0033678273 -0.003367628 -0.0033673719][-0.0033793424 -0.0033778087 -0.0033771016 -0.0033774581 -0.0033778683 -0.003378062 -0.0033781542 -0.0033784173 -0.0033787042 -0.0033786455 -0.0033784718 -0.0033781254 -0.0033777063 -0.00337727 -0.0033771354][-0.0033924275 -0.0033915732 -0.0033913481 -0.0033918421 -0.0033921073 -0.0033921972 -0.0033921942 -0.0033919949 -0.0033917525 -0.0033913434 -0.0033911136 -0.0033908896 -0.0033906328 -0.0033901285 -0.0033893401][-0.0034039484 -0.0034037118 -0.0034043314 -0.0034054576 -0.003406083 -0.0034065328 -0.0034067654 -0.0034065235 -0.0034060944 -0.0034057423 -0.0034056555 -0.0034053794 -0.0034047426 -0.003403852 -0.0034024476][-0.0034107538 -0.0034107582 -0.0034118085 -0.0034130283 -0.0034138823 -0.0034145375 -0.0034149545 -0.0034149317 -0.0034145399 -0.003414294 -0.003414162 -0.00341398 -0.0034134835 -0.0034125894 -0.0034112066][-0.0034113573 -0.0034114763 -0.0034127317 -0.0034141182 -0.003415189 -0.0034160933 -0.0034167529 -0.0034167496 -0.0034163476 -0.0034161806 -0.0034161168 -0.0034158882 -0.0034152325 -0.0034140528 -0.0034124821][-0.0034081303 -0.0034079917 -0.0034091542 -0.0034104602 -0.0034115277 -0.0034124809 -0.003413076 -0.003413 -0.0034124269 -0.0034117212 -0.0034114318 -0.0034115342 -0.0034113624 -0.0034104842 -0.0034088225][-0.0034056052 -0.0034047391 -0.0034056092 -0.003406374 -0.0034070376 -0.0034078443 -0.0034081435 -0.0034077179 -0.0034067992 -0.0034058515 -0.0034058627 -0.0034065796 -0.0034071694 -0.0034069619 -0.0034057153][-0.0034048248 -0.0034044026 -0.0034056802 -0.0034068301 -0.0034077945 -0.0034085941 -0.0034088392 -0.0034085046 -0.0034079249 -0.0034072397 -0.0034069759 -0.0034070658 -0.0034069358 -0.0034063333 -0.0034052406][-0.0034038331 -0.0034035344 -0.0034050022 -0.003406334 -0.0034072013 -0.0034077782 -0.0034079752 -0.0034077943 -0.0034072872 -0.0034069389 -0.0034064993 -0.0034062751 -0.0034058581 -0.0034051356 -0.0034042418][-0.0034039677 -0.0034033479 -0.00340422 -0.003405266 -0.0034058818 -0.0034060664 -0.0034061351 -0.0034060874 -0.0034057004 -0.0034053808 -0.0034050662 -0.0034048245 -0.0034042764 -0.0034035183 -0.0034026941][-0.0034037081 -0.0034029642 -0.0034038108 -0.0034046534 -0.0034052485 -0.0034054827 -0.0034053572 -0.0034051822 -0.0034049477 -0.0034046215 -0.0034042622 -0.0034040082 -0.0034037365 -0.0034032159 -0.0034026532][-0.0034036296 -0.0034025079 -0.00340279 -0.0034031533 -0.0034033738 -0.0034032308 -0.0034029076 -0.0034024941 -0.0034021228 -0.0034018278 -0.0034016327 -0.0034015891 -0.0034015351 -0.0034012406 -0.0034009055][-0.003403252 -0.0034022771 -0.0034023842 -0.0034027686 -0.0034030224 -0.0034028846 -0.0034025472 -0.0034021141 -0.0034015703 -0.0034011744 -0.0034010315 -0.0034010701 -0.003401187 -0.0034013274 -0.0034012944]]...]
INFO - root - 2017-12-10 02:55:23.547069: step 83910, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:02m:32s remains)
INFO - root - 2017-12-10 02:55:32.173833: step 83920, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:05m:40s remains)
INFO - root - 2017-12-10 02:55:40.919632: step 83930, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 60h:55m:37s remains)
INFO - root - 2017-12-10 02:55:49.615935: step 83940, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:18m:35s remains)
INFO - root - 2017-12-10 02:55:58.132408: step 83950, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 61h:41m:58s remains)
INFO - root - 2017-12-10 02:56:06.867859: step 83960, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:06m:45s remains)
INFO - root - 2017-12-10 02:56:15.478911: step 83970, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 50h:53m:08s remains)
INFO - root - 2017-12-10 02:56:24.193911: step 83980, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 59h:36m:47s remains)
INFO - root - 2017-12-10 02:56:32.871299: step 83990, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 61h:30m:09s remains)
INFO - root - 2017-12-10 02:56:41.692798: step 84000, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 59h:33m:30s remains)
2017-12-10 02:56:42.632069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033928733 -0.0033918247 -0.0033924163 -0.0033928694 -0.0033928342 -0.0033926896 -0.003390183 -0.0033809468 -0.0033723596 -0.0033739647 -0.003379181 -0.0033846679 -0.0033886482 -0.0033904889 -0.0033906952][-0.0033894205 -0.003388287 -0.0033890773 -0.0033898053 -0.003390027 -0.0033899331 -0.0033791508 -0.003356572 -0.0033178267 -0.0033000244 -0.0033054713 -0.0033326012 -0.0033589189 -0.0033770925 -0.0033854498][-0.00338912 -0.0033879098 -0.0033887953 -0.003382928 -0.0033327092 -0.0032074458 -0.0030303439 -0.0029152397 -0.002905112 -0.0030067768 -0.003132205 -0.0032507768 -0.0033283762 -0.0033685034 -0.0033842123][-0.0033894978 -0.003387898 -0.0033630566 -0.0032052775 -0.0027618846 -0.0019771978 -0.0010906488 -0.00057482347 -0.00069571473 -0.0013306616 -0.0021127684 -0.0027489986 -0.0031339792 -0.0033124285 -0.0033733915][-0.0033898531 -0.0033850658 -0.003259419 -0.0026980042 -0.0013173772 0.00089187291 0.003257477 0.0046917694 0.0044963118 0.0028839565 0.000694799 -0.0012048839 -0.0024421765 -0.0030662888 -0.0033074275][-0.0033417156 -0.0032919094 -0.0029571636 -0.0017443646 0.00099420524 0.0051407348 0.0095185479 0.01233709 0.012350576 0.0096534584 0.0055579934 0.0017096333 -0.0010034505 -0.0024910744 -0.0031292888][-0.0032346547 -0.0030824174 -0.0024916823 -0.0006862951 0.0031719345 0.0089012347 0.015004076 0.019169938 0.019666951 0.016296124 0.010620366 0.004916952 0.00067035877 -0.0017812841 -0.0028970737][-0.003166426 -0.0028936374 -0.0021501665 -0.00011308957 0.004083436 0.010285185 0.017014433 0.021798382 0.022704221 0.019266043 0.013040388 0.0065250807 0.0015413549 -0.0014017129 -0.002770853][-0.0032039678 -0.0029353497 -0.0022360699 -0.00047371257 0.0031334118 0.0084986063 0.014417246 0.018711774 0.019649442 0.016720012 0.011258683 0.0054599829 0.0010028596 -0.0016302676 -0.0028485854][-0.0032821216 -0.0030942739 -0.0026209974 -0.0014660093 0.0009620355 0.00464865 0.0088030435 0.011849417 0.012510771 0.010442786 0.0066130236 0.0025782317 -0.00048152846 -0.0022572349 -0.0030563259][-0.0033513836 -0.0032590008 -0.0030184214 -0.0024408041 -0.0011743652 0.00080404035 0.0030802598 0.00475124 0.0050746808 0.0038944774 0.0017869866 -0.00037650787 -0.0019701989 -0.0028643238 -0.0032493309][-0.0033871129 -0.0033579685 -0.0032733146 -0.0030632252 -0.0025632863 -0.001750177 -0.00079481932 -0.0001007372 -2.3518223e-06 -0.00053521083 -0.0014235412 -0.0022920293 -0.0029009932 -0.0032257745 -0.0033573268][-0.0033952529 -0.0033918372 -0.0033756313 -0.0033274903 -0.0031919063 -0.0029566446 -0.0026752092 -0.0024743455 -0.0024601435 -0.002629498 -0.0028890059 -0.0031274247 -0.0032846425 -0.0033636466 -0.0033939995][-0.003393719 -0.0033933949 -0.003394126 -0.0033907404 -0.0033720967 -0.0033337257 -0.0032859121 -0.0032503845 -0.0032483854 -0.0032778503 -0.0033205769 -0.0033582293 -0.0033821752 -0.0033944359 -0.0033996031][-0.003392095 -0.0033916382 -0.00339243 -0.0033930847 -0.0033936929 -0.0033935616 -0.0033928319 -0.0033908749 -0.003388671 -0.003388305 -0.0033894004 -0.0033916081 -0.003394007 -0.0033964312 -0.0033976445]]...]
INFO - root - 2017-12-10 02:56:51.255673: step 84010, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 57h:37m:23s remains)
INFO - root - 2017-12-10 02:56:59.683039: step 84020, loss = 0.90, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 56h:39m:31s remains)
INFO - root - 2017-12-10 02:57:08.060550: step 84030, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:04m:12s remains)
INFO - root - 2017-12-10 02:57:16.795848: step 84040, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 58h:05m:17s remains)
INFO - root - 2017-12-10 02:57:25.482647: step 84050, loss = 0.89, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 61h:07m:32s remains)
INFO - root - 2017-12-10 02:57:34.168412: step 84060, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 58h:50m:36s remains)
INFO - root - 2017-12-10 02:57:42.929328: step 84070, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 61h:20m:50s remains)
INFO - root - 2017-12-10 02:57:51.610233: step 84080, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:07m:49s remains)
INFO - root - 2017-12-10 02:58:00.094402: step 84090, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 57h:15m:11s remains)
INFO - root - 2017-12-10 02:58:08.644362: step 84100, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:09m:14s remains)
2017-12-10 02:58:09.528839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3198294 0.33582798 0.35229003 0.36531615 0.37295133 0.37845239 0.38330334 0.3893266 0.393956 0.39592054 0.39252537 0.38372192 0.37060657 0.35347065 0.33234093][0.38481164 0.40718344 0.42954233 0.44892663 0.46131614 0.46819696 0.47164357 0.47472328 0.47568244 0.47354564 0.46643108 0.45617133 0.44400617 0.42970416 0.41119674][0.44304505 0.46927255 0.49518344 0.5187552 0.53489959 0.54338151 0.5447374 0.5438652 0.54043293 0.53499269 0.526362 0.51655382 0.50739241 0.49727887 0.48383552][0.48603877 0.51445723 0.54169291 0.56710833 0.58538866 0.59499651 0.59543866 0.59170258 0.58389336 0.57459873 0.56417853 0.55494887 0.5492416 0.54436791 0.53753954][0.50975055 0.53880292 0.5640977 0.58954221 0.60945749 0.61978328 0.61939985 0.61325884 0.60229111 0.59028494 0.57870907 0.56956488 0.5665893 0.5662809 0.56584764][0.51436067 0.54255247 0.56273335 0.58591962 0.60503024 0.61509919 0.61343491 0.60482407 0.59122759 0.57678771 0.56378448 0.55498266 0.55458641 0.558424 0.56377715][0.49333104 0.51855981 0.53365463 0.55262148 0.56829536 0.57600933 0.57302523 0.56327057 0.54795378 0.53237861 0.51949954 0.51252544 0.51424778 0.5208621 0.53067][0.4465034 0.46781626 0.47845107 0.49350786 0.5062142 0.51189107 0.50775003 0.49696213 0.48045081 0.46429375 0.45156929 0.44522348 0.448286 0.45695946 0.47038937][0.37610328 0.3934817 0.39992124 0.41110432 0.42084497 0.42490068 0.42026284 0.40957439 0.39331019 0.37765026 0.36469543 0.35821465 0.36059052 0.36959642 0.38350239][0.29281121 0.30497393 0.30770171 0.31480595 0.32108563 0.32297516 0.31777334 0.30765092 0.29272673 0.27900189 0.26740772 0.26152435 0.26315698 0.27055678 0.28231153][0.205907 0.21319908 0.21398082 0.21845013 0.22175513 0.22160533 0.21619388 0.20726402 0.19466276 0.18359859 0.17428204 0.16975142 0.17067257 0.17597683 0.18470643][0.12770201 0.13170989 0.13204125 0.13503069 0.13694002 0.13657103 0.13258617 0.12552382 0.11605357 0.10807907 0.10132023 0.09804251 0.098352917 0.10170177 0.10704546][0.068660706 0.070278496 0.070765242 0.073037937 0.074892335 0.074990228 0.072715089 0.068137147 0.062000565 0.056827657 0.051664159 0.049020994 0.048525069 0.050257038 0.053083714][0.029639347 0.029787173 0.029963376 0.031241188 0.03226449 0.032292176 0.030950772 0.028418435 0.025017649 0.021814194 0.018815504 0.017377252 0.016949037 0.017421 0.018516358][0.0075823516 0.0071750851 0.007259 0.0078815371 0.0083398642 0.0083116768 0.0076109208 0.0063628964 0.0048152776 0.003302071 0.0021209803 0.0014019168 0.0012483301 0.0015019861 0.0018507114]]...]
INFO - root - 2017-12-10 02:58:18.125561: step 84110, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 57h:28m:26s remains)
INFO - root - 2017-12-10 02:58:26.575718: step 84120, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 58h:41m:10s remains)
INFO - root - 2017-12-10 02:58:35.405145: step 84130, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 61h:20m:26s remains)
INFO - root - 2017-12-10 02:58:44.030540: step 84140, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 57h:53m:31s remains)
INFO - root - 2017-12-10 02:58:52.657161: step 84150, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 60h:19m:54s remains)
INFO - root - 2017-12-10 02:59:01.305013: step 84160, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 60h:51m:09s remains)
INFO - root - 2017-12-10 02:59:10.101242: step 84170, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 60h:36m:35s remains)
INFO - root - 2017-12-10 02:59:18.571102: step 84180, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 59h:15m:52s remains)
INFO - root - 2017-12-10 02:59:27.290233: step 84190, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:14m:41s remains)
INFO - root - 2017-12-10 02:59:35.772803: step 84200, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:07m:43s remains)
2017-12-10 02:59:36.668289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00338707 -0.0033884156 -0.00339114 -0.0033944347 -0.0033969786 -0.0033987258 -0.0033975411 -0.003395072 -0.0033907308 -0.0033868398 -0.0033849089 -0.0033852898 -0.0033886884 -0.0033936156 -0.0033985903][-0.0033786765 -0.0033812441 -0.0033846132 -0.0033879257 -0.0033897297 -0.0033921872 -0.0033914854 -0.003390654 -0.0033878679 -0.0033854926 -0.0033843904 -0.0033851031 -0.0033885147 -0.0033932133 -0.0033974594][-0.0033714692 -0.0033733866 -0.0033769757 -0.003381554 -0.0033849853 -0.0033875753 -0.0033872926 -0.0033873939 -0.0033861178 -0.0033848234 -0.0033848423 -0.0033861115 -0.0033897024 -0.0033945504 -0.0033985556][-0.0033672578 -0.003366505 -0.0033680813 -0.0033713214 -0.003375385 -0.0033800728 -0.003383148 -0.0033846349 -0.0033845364 -0.0033841836 -0.00338539 -0.0033875399 -0.0033916323 -0.0033969462 -0.0034009025][-0.0033656429 -0.0033612289 -0.0033611427 -0.0033620712 -0.0033647304 -0.0033695071 -0.0033749996 -0.0033793929 -0.0033821515 -0.003383345 -0.0033858521 -0.0033893711 -0.0033943844 -0.0034001595 -0.0034043023][-0.0033584936 -0.0033538858 -0.0033528758 -0.0033516274 -0.0033540886 -0.0033587061 -0.0033649758 -0.0033718555 -0.0033777631 -0.0033818192 -0.003386179 -0.003390877 -0.0033967751 -0.0034031782 -0.003407856][-0.0033505603 -0.0033419048 -0.0033412983 -0.0033410771 -0.003344127 -0.0033484427 -0.0033566351 -0.0033648531 -0.0033727961 -0.0033801165 -0.0033864463 -0.0033924892 -0.0033992557 -0.0034058471 -0.0034106348][-0.0033437265 -0.0033337933 -0.0033327062 -0.0033313734 -0.0033348545 -0.0033416271 -0.0033519571 -0.0033621779 -0.0033713058 -0.0033803824 -0.0033876277 -0.0033939008 -0.0034004808 -0.0034070192 -0.0034121387][-0.0033329448 -0.0033238854 -0.003325429 -0.0033266507 -0.003330023 -0.0033379518 -0.0033503321 -0.0033634212 -0.0033742781 -0.0033831398 -0.0033901718 -0.0033958987 -0.0034013041 -0.0034065512 -0.0034107035][-0.0033214986 -0.0033113908 -0.003314697 -0.0033178215 -0.003324331 -0.0033362031 -0.003350283 -0.0033664037 -0.0033787773 -0.0033874304 -0.0033931583 -0.0033977556 -0.0034019377 -0.0034055137 -0.0034079766][-0.0033139542 -0.0033009169 -0.0033042629 -0.0033090808 -0.003318602 -0.0033336154 -0.0033503035 -0.0033688904 -0.003382561 -0.0033904328 -0.0033945176 -0.0033977663 -0.0034005984 -0.0034026857 -0.0034041735][-0.003311079 -0.0032951036 -0.0033005734 -0.003305384 -0.0033181319 -0.0033345958 -0.0033523049 -0.0033713344 -0.0033845708 -0.003391583 -0.0033938799 -0.0033955981 -0.0033973847 -0.0033985067 -0.003399221][-0.0033075681 -0.0032913014 -0.0032981369 -0.0033044945 -0.0033192064 -0.0033359714 -0.0033545515 -0.0033727589 -0.0033853236 -0.0033914237 -0.0033922638 -0.0033923003 -0.0033928466 -0.0033931094 -0.0033931294][-0.0033028303 -0.0032889219 -0.0032957832 -0.003302943 -0.0033181799 -0.0033349032 -0.0033538959 -0.0033706732 -0.0033823929 -0.0033885343 -0.0033891774 -0.0033882614 -0.003388193 -0.0033874833 -0.0033869406][-0.0033005436 -0.0032887356 -0.0032946672 -0.0033016899 -0.0033159021 -0.0033324051 -0.0033515948 -0.0033675814 -0.0033785771 -0.0033849478 -0.0033855888 -0.0033843082 -0.0033836246 -0.003382416 -0.0033813699]]...]
INFO - root - 2017-12-10 02:59:45.227426: step 84210, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 59h:30m:58s remains)
INFO - root - 2017-12-10 02:59:53.780701: step 84220, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:27m:26s remains)
INFO - root - 2017-12-10 03:00:02.655270: step 84230, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 60h:57m:48s remains)
INFO - root - 2017-12-10 03:00:11.455814: step 84240, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 61h:24m:21s remains)
INFO - root - 2017-12-10 03:00:20.008066: step 84250, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:13m:03s remains)
INFO - root - 2017-12-10 03:00:28.593699: step 84260, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 60h:17m:50s remains)
INFO - root - 2017-12-10 03:00:37.311945: step 84270, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 61h:23m:59s remains)
INFO - root - 2017-12-10 03:00:45.650689: step 84280, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 60h:13m:55s remains)
INFO - root - 2017-12-10 03:00:54.386445: step 84290, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 60h:50m:14s remains)
INFO - root - 2017-12-10 03:01:02.982495: step 84300, loss = 0.90, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 61h:17m:14s remains)
2017-12-10 03:01:03.937519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00036890223 -0.0004512507 -0.00060205953 -0.00073442329 -0.00084752054 -0.00094958371 -0.0010325962 -0.0011225722 -0.0011634929 -0.0012376059 -0.0013419273 -0.0014819787 -0.0016720624 -0.0019330182 -0.00225221][0.00036924263 0.00030707195 0.00019809371 8.6731277e-05 -2.2001565e-05 -0.00012124609 -0.00019763247 -0.00028615491 -0.00037259632 -0.00050348043 -0.00066826656 -0.00088338158 -0.0011581643 -0.0014897845 -0.0018851833][0.00031961012 0.00026722904 0.00017641252 8.9227222e-05 1.1196593e-05 -4.6828296e-05 -8.6133135e-05 -0.00014689402 -0.00021096109 -0.00033235666 -0.00050323317 -0.00074198982 -0.0010415984 -0.0013969454 -0.0018167901][-0.00011036173 -0.0001521029 -0.00022532488 -0.00028567831 -0.00033399742 -0.00036180741 -0.00037438795 -0.00038963277 -0.00041800481 -0.00050562737 -0.00064898306 -0.00086769438 -0.0011466199 -0.0014891092 -0.0018915293][-0.00074140378 -0.000768553 -0.00082673994 -0.00086224335 -0.00088220718 -0.00088325003 -0.00087194843 -0.00086151366 -0.00087304646 -0.00094075222 -0.0010663534 -0.0012586205 -0.0015033197 -0.0018058625 -0.0021517712][-0.0014478536 -0.0014549203 -0.0014945175 -0.0015117308 -0.0015063882 -0.0014803975 -0.0014468823 -0.0014198176 -0.0014187323 -0.0014693346 -0.0015771667 -0.0017409908 -0.0019538614 -0.0022119982 -0.0024944113][-0.0020927736 -0.002072009 -0.0020806736 -0.0020725895 -0.0020465804 -0.0020117126 -0.0019776393 -0.001952112 -0.0019503564 -0.0019909563 -0.002077057 -0.0022101277 -0.0023842063 -0.0025923727 -0.0028089706][-0.0027419669 -0.0027205385 -0.0027054846 -0.0026716581 -0.0026235292 -0.0025723411 -0.0025248562 -0.002490341 -0.0024754349 -0.0024928947 -0.0025467975 -0.0026385318 -0.0027640117 -0.0029115332 -0.0030578515][-0.0031443627 -0.0031340253 -0.0031232755 -0.0030986872 -0.0030654545 -0.0030286331 -0.0029946615 -0.0029678906 -0.0029526097 -0.0029556022 -0.002980195 -0.0030295567 -0.0030974415 -0.0031745397 -0.0032485216][-0.0033709237 -0.0033678224 -0.0033639248 -0.0033560891 -0.0033434608 -0.0033270235 -0.0033112583 -0.0032969648 -0.0032854413 -0.00327995 -0.0032835235 -0.003297661 -0.0033189191 -0.003342083 -0.0033642794][-0.003389939 -0.0033882055 -0.0033886763 -0.003390193 -0.0033914549 -0.0033914531 -0.003392041 -0.0033925106 -0.0033929574 -0.0033936468 -0.0033944647 -0.0033962636 -0.0033985197 -0.0034008087 -0.0034036541][-0.0033972613 -0.0033960559 -0.0033959567 -0.0033967234 -0.0033973516 -0.0033972918 -0.0033976096 -0.0033982173 -0.0033992922 -0.0034004964 -0.0034021691 -0.0034033703 -0.0034045218 -0.0034057049 -0.0034072087][-0.0034030546 -0.0034019246 -0.0034011239 -0.00340142 -0.0034018534 -0.0034016797 -0.0034018506 -0.0034022145 -0.0034032119 -0.003404462 -0.0034062543 -0.0034075999 -0.0034084118 -0.0034087924 -0.0034092504][-0.0034075906 -0.0034063181 -0.0034050487 -0.003403553 -0.0034023619 -0.0034023579 -0.0034025582 -0.00340305 -0.0034039512 -0.0034062308 -0.0034082895 -0.0034089594 -0.0034094239 -0.0034095962 -0.0034098304][-0.0034066716 -0.0034057486 -0.0034050839 -0.0034039048 -0.0034025889 -0.003402425 -0.0034019146 -0.0034019952 -0.0034027707 -0.0034053011 -0.0034073484 -0.0034085014 -0.0034094898 -0.0034096325 -0.003409886]]...]
INFO - root - 2017-12-10 03:01:12.601944: step 84310, loss = 0.90, batch loss = 0.69 (8.1 examples/sec; 0.984 sec/batch; 67h:51m:36s remains)
INFO - root - 2017-12-10 03:01:21.282330: step 84320, loss = 0.89, batch loss = 0.68 (8.8 examples/sec; 0.910 sec/batch; 62h:42m:43s remains)
INFO - root - 2017-12-10 03:01:29.967413: step 84330, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 59h:58m:39s remains)
INFO - root - 2017-12-10 03:01:38.631220: step 84340, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:04m:33s remains)
INFO - root - 2017-12-10 03:01:47.209422: step 84350, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 59h:36m:13s remains)
INFO - root - 2017-12-10 03:01:55.979531: step 84360, loss = 0.91, batch loss = 0.70 (8.9 examples/sec; 0.896 sec/batch; 61h:43m:38s remains)
INFO - root - 2017-12-10 03:02:04.615966: step 84370, loss = 0.89, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 57h:51m:10s remains)
INFO - root - 2017-12-10 03:02:12.983618: step 84380, loss = 0.90, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 57h:56m:39s remains)
INFO - root - 2017-12-10 03:02:21.548298: step 84390, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 58h:18m:48s remains)
INFO - root - 2017-12-10 03:02:30.238812: step 84400, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:01m:31s remains)
2017-12-10 03:02:31.040079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033831554 -0.003380229 -0.0033780264 -0.0033757268 -0.0033732734 -0.0033704326 -0.0033676438 -0.0033650608 -0.0033626952 -0.0033607595 -0.0033591397 -0.003357636 -0.0033555878 -0.0033531908 -0.0033508826][-0.003371954 -0.0033687835 -0.003366905 -0.0033651083 -0.0033633206 -0.0033610791 -0.0033587443 -0.0033563147 -0.003354101 -0.0033523194 -0.0033509673 -0.0033500013 -0.0033488884 -0.0033475091 -0.0033462355][-0.0033618549 -0.0033586367 -0.0033576123 -0.0033566724 -0.0033556835 -0.0033541145 -0.0033522097 -0.0033500146 -0.0033482092 -0.0033467475 -0.003346022 -0.0033459265 -0.0033460241 -0.003346019 -0.0033458441][-0.0033546044 -0.0033515003 -0.0033514595 -0.0033512791 -0.0033507985 -0.0033495873 -0.0033481598 -0.003346167 -0.0033444529 -0.0033431626 -0.0033430865 -0.0033440678 -0.0033456904 -0.0033474702 -0.0033487203][-0.0033498851 -0.0033474336 -0.0033484921 -0.0033494877 -0.00334977 -0.003348931 -0.0033475487 -0.0033451975 -0.003343032 -0.0033416119 -0.003342147 -0.0033444532 -0.0033480437 -0.0033518947 -0.0033546898][-0.0033479095 -0.0033461913 -0.0033483603 -0.0033506323 -0.0033521662 -0.0033520511 -0.0033506746 -0.0033475051 -0.0033442511 -0.0033423251 -0.0033431507 -0.0033464136 -0.0033516476 -0.003357583 -0.0033619555][-0.0033466145 -0.0033456078 -0.0033491193 -0.0033526537 -0.0033551548 -0.0033556786 -0.0033543473 -0.0033505466 -0.0033464569 -0.0033439894 -0.0033449081 -0.0033492418 -0.0033558621 -0.0033634536 -0.0033693551][-0.003345575 -0.0033450113 -0.0033498702 -0.0033547569 -0.0033581518 -0.0033589934 -0.0033573227 -0.0033529354 -0.0033486583 -0.0033460637 -0.003347289 -0.0033522642 -0.0033597848 -0.0033684331 -0.0033754087][-0.0033437165 -0.0033434741 -0.0033492909 -0.0033552574 -0.0033594081 -0.0033608256 -0.0033592815 -0.0033548728 -0.00335037 -0.0033479067 -0.003349714 -0.0033553282 -0.0033629537 -0.0033715747 -0.0033787545][-0.0033424126 -0.0033418355 -0.003347287 -0.0033531203 -0.0033574631 -0.0033595269 -0.0033586982 -0.0033551492 -0.0033514639 -0.0033498129 -0.0033523683 -0.0033584929 -0.0033657015 -0.0033730571 -0.0033795082][-0.0033421258 -0.0033404038 -0.003344615 -0.0033491396 -0.0033527412 -0.0033548565 -0.0033548004 -0.0033525464 -0.0033504062 -0.0033504216 -0.0033544349 -0.0033611087 -0.003367394 -0.0033733367 -0.0033786816][-0.0033429477 -0.003339651 -0.0033418334 -0.0033445861 -0.0033468462 -0.0033482173 -0.0033486858 -0.0033478257 -0.0033475992 -0.0033496579 -0.0033554621 -0.0033628561 -0.0033685777 -0.0033733321 -0.0033774571][-0.0033438534 -0.0033390096 -0.0033392187 -0.0033400015 -0.0033410019 -0.003341784 -0.0033423328 -0.0033424555 -0.0033441423 -0.0033482171 -0.0033555583 -0.0033635057 -0.0033690506 -0.0033733465 -0.0033766294][-0.0033454737 -0.0033393044 -0.0033375821 -0.0033369134 -0.003337075 -0.0033377698 -0.003338472 -0.0033392366 -0.0033421204 -0.0033478655 -0.0033563646 -0.0033649183 -0.0033706923 -0.0033749363 -0.0033781081][-0.0033468991 -0.0033399074 -0.0033369851 -0.0033354277 -0.0033349353 -0.0033354377 -0.0033364734 -0.0033380578 -0.0033418778 -0.0033486439 -0.0033575052 -0.0033660417 -0.0033720268 -0.0033764534 -0.0033797391]]...]
INFO - root - 2017-12-10 03:02:39.701531: step 84410, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:07m:05s remains)
INFO - root - 2017-12-10 03:02:48.459667: step 84420, loss = 0.89, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 61h:53m:06s remains)
INFO - root - 2017-12-10 03:02:57.058913: step 84430, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 57h:46m:45s remains)
INFO - root - 2017-12-10 03:03:05.714151: step 84440, loss = 0.90, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 61h:37m:39s remains)
INFO - root - 2017-12-10 03:03:14.365913: step 84450, loss = 0.90, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 52h:44m:18s remains)
INFO - root - 2017-12-10 03:03:23.146345: step 84460, loss = 0.90, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 60h:54m:13s remains)
INFO - root - 2017-12-10 03:03:31.774313: step 84470, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 58h:14m:26s remains)
INFO - root - 2017-12-10 03:03:40.274416: step 84480, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 59h:44m:38s remains)
INFO - root - 2017-12-10 03:03:48.818349: step 84490, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 58h:51m:09s remains)
INFO - root - 2017-12-10 03:03:57.381750: step 84500, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 57h:26m:51s remains)
2017-12-10 03:03:58.392155: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45769152 0.44450966 0.4329744 0.42523286 0.42105934 0.41812828 0.41610035 0.41214737 0.40791065 0.40284041 0.39391914 0.38376123 0.3721042 0.36134076 0.34883514][0.453235 0.44354072 0.4357118 0.43245986 0.43319538 0.43454406 0.43713045 0.43650097 0.4343363 0.4297412 0.41901451 0.40465608 0.38764513 0.37241304 0.35644096][0.43802208 0.4321129 0.42837679 0.42940938 0.4346242 0.44138953 0.44902131 0.45264271 0.45303932 0.44878736 0.43632939 0.41769654 0.39561009 0.37498686 0.35518289][0.41967773 0.41870484 0.42025152 0.42533717 0.43423721 0.44451576 0.45617646 0.46371329 0.46596685 0.46234119 0.4495312 0.42859802 0.40263224 0.37725484 0.35409424][0.40212375 0.40541911 0.41026202 0.41945058 0.43160328 0.44382152 0.45735076 0.4680565 0.47210228 0.46887487 0.45535126 0.43352807 0.40564764 0.37704656 0.35152507][0.38704085 0.39357758 0.40034008 0.410625 0.42288518 0.4359237 0.45003325 0.46104586 0.46613938 0.46422985 0.45232826 0.4311333 0.40369555 0.37492904 0.34876403][0.37033844 0.37819007 0.38452134 0.39450476 0.40527275 0.41741925 0.43083602 0.44213948 0.44806781 0.44782805 0.43845966 0.41972819 0.39468691 0.36777663 0.34319696][0.34868431 0.35731798 0.36262503 0.37063745 0.3785851 0.3883374 0.39906934 0.40992349 0.41641662 0.41770113 0.41139019 0.39635164 0.37533307 0.35162961 0.32991982][0.31942236 0.32849202 0.33277947 0.33890772 0.34419236 0.35080555 0.35849121 0.36731482 0.37301856 0.37545094 0.3719328 0.36137971 0.34564212 0.32677552 0.30949706][0.29130906 0.29890224 0.30104989 0.3052229 0.30818939 0.31160468 0.31598145 0.32255206 0.32712889 0.32936066 0.32695875 0.32004729 0.30907676 0.29569259 0.28349331][0.26197451 0.26838204 0.2690593 0.271007 0.27215606 0.27378649 0.27620727 0.28042078 0.28423765 0.28646335 0.28571746 0.28116727 0.27394825 0.26545319 0.25749785][0.23339218 0.23919164 0.23895749 0.23964857 0.24019396 0.24069622 0.24216546 0.24555571 0.24859047 0.25052944 0.25003693 0.24696344 0.24181478 0.23611046 0.2315643][0.20874858 0.21432155 0.21402714 0.21431048 0.21466231 0.21475981 0.2156457 0.21823305 0.22041756 0.22181413 0.22118615 0.21854384 0.21445794 0.21041559 0.20812133][0.18870088 0.19380867 0.19341864 0.193994 0.19487023 0.1955331 0.1968355 0.19895956 0.20070101 0.2017788 0.20099792 0.19862981 0.19526683 0.19234845 0.19157238][0.17506908 0.17944929 0.17885645 0.17948055 0.18066247 0.18165208 0.18314703 0.18552749 0.18725066 0.18816485 0.18744428 0.18541294 0.1827316 0.18068786 0.18089524]]...]
INFO - root - 2017-12-10 03:04:06.898662: step 84510, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 58h:23m:01s remains)
INFO - root - 2017-12-10 03:04:15.470162: step 84520, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 59h:51m:16s remains)
INFO - root - 2017-12-10 03:04:23.965708: step 84530, loss = 0.89, batch loss = 0.68 (9.5 examples/sec; 0.838 sec/batch; 57h:42m:33s remains)
INFO - root - 2017-12-10 03:04:32.376686: step 84540, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 58h:15m:52s remains)
INFO - root - 2017-12-10 03:04:41.008912: step 84550, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 59h:47m:29s remains)
INFO - root - 2017-12-10 03:04:49.457902: step 84560, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.847 sec/batch; 58h:18m:32s remains)
INFO - root - 2017-12-10 03:04:58.133059: step 84570, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 59h:34m:58s remains)
INFO - root - 2017-12-10 03:05:06.521505: step 84580, loss = 0.90, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 59h:54m:36s remains)
INFO - root - 2017-12-10 03:05:15.087325: step 84590, loss = 0.89, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 58h:30m:32s remains)
INFO - root - 2017-12-10 03:05:23.708794: step 84600, loss = 0.89, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 60h:33m:20s remains)
2017-12-10 03:05:24.665798: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12669872 0.11588281 0.10661192 0.10132895 0.09909489 0.097322352 0.091991149 0.081456147 0.06597209 0.048500508 0.031583216 0.01769389 0.0077400748 0.002140858 0.00029842788][0.13219358 0.11926522 0.10884021 0.10352197 0.10217272 0.10207263 0.098802768 0.09022674 0.076022446 0.058732722 0.040921863 0.025241559 0.013026433 0.0053991862 0.0021654631][0.12925188 0.11551126 0.10512025 0.10052535 0.10062162 0.10237937 0.10145626 0.095498838 0.08362478 0.067600764 0.049676709 0.032756373 0.018577166 0.0088389944 0.0037789124][0.12250882 0.10891969 0.099408776 0.096206449 0.098292761 0.10220321 0.10359601 0.099919893 0.090029493 0.075056262 0.05694817 0.038836736 0.022926496 0.011422189 0.0046742791][0.11410858 0.1018 0.093584441 0.091722347 0.095304407 0.10065249 0.10366925 0.10167952 0.09330193 0.079172567 0.061058834 0.042213414 0.025163423 0.012448648 0.0044926312][0.10554036 0.09501265 0.087922834 0.086826108 0.090842225 0.096404955 0.099730045 0.098201096 0.090463057 0.076869167 0.059163876 0.040628277 0.02382219 0.011273885 0.0032749183][0.097479038 0.088280916 0.081469841 0.0800074 0.083154641 0.087689705 0.09009257 0.087983854 0.080266081 0.067367263 0.050982922 0.034162771 0.019233251 0.0083490536 0.0015664983][0.089604504 0.08042904 0.072461911 0.06943012 0.070725836 0.073519059 0.074502416 0.071703732 0.064284958 0.052771628 0.038777955 0.024882559 0.013001554 0.004669385 -0.00025398214][0.081287704 0.070644893 0.060285054 0.054743197 0.05360629 0.054460842 0.05417081 0.051179 0.044879306 0.035833865 0.025280925 0.015180558 0.006906894 0.0013595812 -0.0016814305][0.072246596 0.059563965 0.046472188 0.038482122 0.035248708 0.034493372 0.033373915 0.030700041 0.026042799 0.019848451 0.012946215 0.0066796038 0.0018392203 -0.0011679665 -0.0026381642][0.062793531 0.048235096 0.033042911 0.023414318 0.018914487 0.017428685 0.016247988 0.014335431 0.011461396 0.0078865131 0.0041146204 0.00087848608 -0.0014201018 -0.0026447561 -0.0031321486][0.054637071 0.038907316 0.022632303 0.012254063 0.0072903233 0.0056712618 0.0047873976 0.0038159282 0.002506885 0.00094113941 -0.00064857677 -0.0019450614 -0.00276955 -0.0031529712 -0.00327071][0.048636463 0.032489091 0.016112704 0.0057413056 0.00084596756 -0.00062387437 -0.0011260677 -0.0014754608 -0.0018673217 -0.0023076446 -0.0027345568 -0.0030563253 -0.003224924 -0.0032803037 -0.0032800138][0.042926643 0.027504938 0.012182489 0.0026150371 -0.0017513277 -0.0029106929 -0.0031139934 -0.0031669338 -0.0031995208 -0.0032403409 -0.0032793791 -0.0032977392 -0.003299091 -0.0032921368 -0.0032658328][0.036819316 0.023214605 0.009766601 0.0014414822 -0.0022715956 -0.0032038905 -0.0033015087 -0.0033123158 -0.0033166739 -0.003318792 -0.0033178581 -0.0033124192 -0.0033068922 -0.0032945059 -0.0032429826]]...]
INFO - root - 2017-12-10 03:05:33.120783: step 84610, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 58h:51m:43s remains)
INFO - root - 2017-12-10 03:05:41.539957: step 84620, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:13m:48s remains)
INFO - root - 2017-12-10 03:05:50.050149: step 84630, loss = 0.90, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 59h:17m:46s remains)
INFO - root - 2017-12-10 03:05:58.648758: step 84640, loss = 0.89, batch loss = 0.68 (9.1 examples/sec; 0.882 sec/batch; 60h:45m:08s remains)
INFO - root - 2017-12-10 03:06:07.370333: step 84650, loss = 0.89, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 59h:18m:07s remains)
INFO - root - 2017-12-10 03:06:15.859613: step 84660, loss = 0.90, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-10 03:06:24.560251: step 84670, loss = 0.90, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 58h:37m:37s remains)
