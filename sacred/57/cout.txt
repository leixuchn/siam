INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "57"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 07:12:01.416428: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:12:01.416467: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:12:01.416473: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:12:01.416477: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:12:01.416481: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:12:02.005634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 07:12:02.005673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 07:12:02.005679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 07:12:02.005692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 07:12:05.259309: step 0, loss = 2.03, batch loss = 1.97 (3.3 examples/sec; 2.389 sec/batch; 220h:37m:25s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 07:12:08.472530: step 10, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:41s remains)
INFO - root - 2017-12-05 07:12:10.691400: step 20, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-05 07:12:12.992451: step 30, loss = 2.04, batch loss = 1.98 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:38s remains)
INFO - root - 2017-12-05 07:12:15.139669: step 40, loss = 2.03, batch loss = 1.97 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:39s remains)
INFO - root - 2017-12-05 07:12:17.294225: step 50, loss = 2.05, batch loss = 1.99 (33.6 examples/sec; 0.238 sec/batch; 21h:59m:13s remains)
INFO - root - 2017-12-05 07:12:19.463127: step 60, loss = 2.03, batch loss = 1.97 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:58s remains)
INFO - root - 2017-12-05 07:12:21.598656: step 70, loss = 2.03, batch loss = 1.97 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:53s remains)
INFO - root - 2017-12-05 07:12:23.747003: step 80, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:20s remains)
INFO - root - 2017-12-05 07:12:25.891159: step 90, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:33s remains)
INFO - root - 2017-12-05 07:12:28.030897: step 100, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:01s remains)
INFO - root - 2017-12-05 07:12:30.222187: step 110, loss = 2.04, batch loss = 1.98 (38.8 examples/sec; 0.206 sec/batch; 19h:03m:24s remains)
INFO - root - 2017-12-05 07:12:32.360527: step 120, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 21h:51m:04s remains)
INFO - root - 2017-12-05 07:12:34.489031: step 130, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-05 07:12:36.601511: step 140, loss = 2.04, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:26s remains)
INFO - root - 2017-12-05 07:12:38.719398: step 150, loss = 2.05, batch loss = 1.99 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:19s remains)
INFO - root - 2017-12-05 07:12:40.861035: step 160, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:48s remains)
INFO - root - 2017-12-05 07:12:42.994179: step 170, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:03s remains)
INFO - root - 2017-12-05 07:12:45.131791: step 180, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:36s remains)
INFO - root - 2017-12-05 07:12:47.263606: step 190, loss = 2.03, batch loss = 1.97 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:30s remains)
INFO - root - 2017-12-05 07:12:49.399302: step 200, loss = 2.03, batch loss = 1.97 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:05s remains)
INFO - root - 2017-12-05 07:12:51.630933: step 210, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-05 07:12:53.786725: step 220, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:28s remains)
INFO - root - 2017-12-05 07:12:55.905694: step 230, loss = 2.05, batch loss = 1.99 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:37s remains)
INFO - root - 2017-12-05 07:12:58.031486: step 240, loss = 2.09, batch loss = 2.03 (38.4 examples/sec; 0.208 sec/batch; 19h:12m:58s remains)
INFO - root - 2017-12-05 07:13:00.150714: step 250, loss = 2.06, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:59s remains)
INFO - root - 2017-12-05 07:13:02.295115: step 260, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:33s remains)
INFO - root - 2017-12-05 07:13:04.434404: step 270, loss = 2.07, batch loss = 2.02 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:49s remains)
INFO - root - 2017-12-05 07:13:06.581791: step 280, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:46s remains)
INFO - root - 2017-12-05 07:13:08.749369: step 290, loss = 2.04, batch loss = 1.98 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:41s remains)
INFO - root - 2017-12-05 07:13:10.923504: step 300, loss = 2.05, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-05 07:13:13.151190: step 310, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:08s remains)
INFO - root - 2017-12-05 07:13:15.293675: step 320, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:45s remains)
INFO - root - 2017-12-05 07:13:17.439189: step 330, loss = 2.06, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:33s remains)
INFO - root - 2017-12-05 07:13:19.570913: step 340, loss = 2.05, batch loss = 1.99 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:19s remains)
INFO - root - 2017-12-05 07:13:21.715551: step 350, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-05 07:13:23.848518: step 360, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:41s remains)
INFO - root - 2017-12-05 07:13:26.002589: step 370, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:04s remains)
INFO - root - 2017-12-05 07:13:28.140948: step 380, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:26s remains)
INFO - root - 2017-12-05 07:13:30.312587: step 390, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:45s remains)
INFO - root - 2017-12-05 07:13:32.459201: step 400, loss = 2.04, batch loss = 1.98 (37.1 examples/sec; 0.215 sec/batch; 19h:52m:23s remains)
INFO - root - 2017-12-05 07:13:34.652120: step 410, loss = 2.04, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:33s remains)
INFO - root - 2017-12-05 07:13:36.800703: step 420, loss = 2.06, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:37s remains)
INFO - root - 2017-12-05 07:13:38.967696: step 430, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:54s remains)
INFO - root - 2017-12-05 07:13:41.095056: step 440, loss = 2.05, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-05 07:13:43.331164: step 450, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-05 07:13:45.480817: step 460, loss = 2.01, batch loss = 1.95 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:28s remains)
INFO - root - 2017-12-05 07:13:47.639648: step 470, loss = 2.03, batch loss = 1.97 (38.2 examples/sec; 0.209 sec/batch; 19h:17m:27s remains)
INFO - root - 2017-12-05 07:13:49.763785: step 480, loss = 2.09, batch loss = 2.03 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:57s remains)
INFO - root - 2017-12-05 07:13:51.940042: step 490, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:02s remains)
INFO - root - 2017-12-05 07:13:54.112596: step 500, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:29s remains)
INFO - root - 2017-12-05 07:13:56.331104: step 510, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.214 sec/batch; 19h:41m:47s remains)
INFO - root - 2017-12-05 07:13:58.466071: step 520, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:28s remains)
INFO - root - 2017-12-05 07:14:00.643115: step 530, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:34s remains)
INFO - root - 2017-12-05 07:14:02.781348: step 540, loss = 2.03, batch loss = 1.97 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:36s remains)
INFO - root - 2017-12-05 07:14:04.917588: step 550, loss = 2.01, batch loss = 1.95 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:56s remains)
INFO - root - 2017-12-05 07:14:07.076858: step 560, loss = 2.05, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:04s remains)
INFO - root - 2017-12-05 07:14:09.240932: step 570, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.218 sec/batch; 20h:03m:53s remains)
INFO - root - 2017-12-05 07:14:11.421759: step 580, loss = 2.06, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:47s remains)
INFO - root - 2017-12-05 07:14:13.574270: step 590, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:15s remains)
INFO - root - 2017-12-05 07:14:15.748090: step 600, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:56s remains)
INFO - root - 2017-12-05 07:14:18.003394: step 610, loss = 2.07, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:30s remains)
INFO - root - 2017-12-05 07:14:20.170431: step 620, loss = 2.04, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:09s remains)
INFO - root - 2017-12-05 07:14:22.352359: step 630, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:27s remains)
INFO - root - 2017-12-05 07:14:24.513463: step 640, loss = 2.04, batch loss = 1.98 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:31s remains)
INFO - root - 2017-12-05 07:14:26.673834: step 650, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-05 07:14:28.822181: step 660, loss = 2.03, batch loss = 1.97 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:40s remains)
INFO - root - 2017-12-05 07:14:31.019310: step 670, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:12s remains)
INFO - root - 2017-12-05 07:14:33.179308: step 680, loss = 2.05, batch loss = 1.99 (38.2 examples/sec; 0.210 sec/batch; 19h:19m:04s remains)
INFO - root - 2017-12-05 07:14:35.364448: step 690, loss = 2.03, batch loss = 1.97 (36.6 examples/sec; 0.218 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-05 07:14:37.519926: step 700, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:17s remains)
INFO - root - 2017-12-05 07:14:39.783958: step 710, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:49s remains)
INFO - root - 2017-12-05 07:14:41.949788: step 720, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:05s remains)
INFO - root - 2017-12-05 07:14:44.113365: step 730, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:05s remains)
INFO - root - 2017-12-05 07:14:46.261460: step 740, loss = 2.03, batch loss = 1.98 (37.8 examples/sec; 0.211 sec/batch; 19h:29m:07s remains)
INFO - root - 2017-12-05 07:14:48.421477: step 750, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:59s remains)
INFO - root - 2017-12-05 07:14:50.577584: step 760, loss = 2.04, batch loss = 1.98 (37.8 examples/sec; 0.212 sec/batch; 19h:29m:31s remains)
INFO - root - 2017-12-05 07:14:52.798670: step 770, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:11s remains)
INFO - root - 2017-12-05 07:14:54.971190: step 780, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:40s remains)
INFO - root - 2017-12-05 07:14:57.118504: step 790, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:27s remains)
INFO - root - 2017-12-05 07:14:59.255197: step 800, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:21s remains)
INFO - root - 2017-12-05 07:15:01.457702: step 810, loss = 2.05, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:52s remains)
INFO - root - 2017-12-05 07:15:03.612014: step 820, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:40s remains)
INFO - root - 2017-12-05 07:15:05.763273: step 830, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:37s remains)
INFO - root - 2017-12-05 07:15:07.932559: step 840, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:23s remains)
INFO - root - 2017-12-05 07:15:10.111460: step 850, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-05 07:15:12.314169: step 860, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:01s remains)
INFO - root - 2017-12-05 07:15:14.465081: step 870, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:24s remains)
INFO - root - 2017-12-05 07:15:16.637402: step 880, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:42s remains)
INFO - root - 2017-12-05 07:15:18.820754: step 890, loss = 2.00, batch loss = 1.94 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-05 07:15:20.998567: step 900, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:34s remains)
INFO - root - 2017-12-05 07:15:23.231133: step 910, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:18s remains)
INFO - root - 2017-12-05 07:15:25.434367: step 920, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.215 sec/batch; 19h:50m:40s remains)
INFO - root - 2017-12-05 07:15:27.597539: step 930, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:29s remains)
INFO - root - 2017-12-05 07:15:29.765886: step 940, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:57s remains)
INFO - root - 2017-12-05 07:15:31.931993: step 950, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-05 07:15:34.115776: step 960, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:36s remains)
INFO - root - 2017-12-05 07:15:36.287549: step 970, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:38s remains)
INFO - root - 2017-12-05 07:15:38.480770: step 980, loss = 2.04, batch loss = 1.98 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:39s remains)
INFO - root - 2017-12-05 07:15:40.648661: step 990, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:21s remains)
INFO - root - 2017-12-05 07:15:42.818295: step 1000, loss = 2.04, batch loss = 1.98 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:51s remains)
INFO - root - 2017-12-05 07:15:45.041773: step 1010, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:38s remains)
INFO - root - 2017-12-05 07:15:47.210313: step 1020, loss = 2.04, batch loss = 1.98 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:41s remains)
INFO - root - 2017-12-05 07:15:49.380695: step 1030, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:15s remains)
INFO - root - 2017-12-05 07:15:51.612261: step 1040, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:50s remains)
INFO - root - 2017-12-05 07:15:53.773553: step 1050, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:24s remains)
INFO - root - 2017-12-05 07:15:55.971172: step 1060, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:40s remains)
INFO - root - 2017-12-05 07:15:58.154068: step 1070, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:28s remains)
INFO - root - 2017-12-05 07:16:00.331843: step 1080, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:12s remains)
INFO - root - 2017-12-05 07:16:02.566724: step 1090, loss = 2.02, batch loss = 1.96 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:44s remains)
INFO - root - 2017-12-05 07:16:04.763024: step 1100, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:26s remains)
INFO - root - 2017-12-05 07:16:07.026978: step 1110, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:09s remains)
INFO - root - 2017-12-05 07:16:09.197898: step 1120, loss = 2.02, batch loss = 1.96 (36.8 examples/sec; 0.218 sec/batch; 20h:01m:37s remains)
INFO - root - 2017-12-05 07:16:11.353596: step 1130, loss = 2.05, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:53s remains)
INFO - root - 2017-12-05 07:16:13.539917: step 1140, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:17s remains)
INFO - root - 2017-12-05 07:16:15.718595: step 1150, loss = 2.03, batch loss = 1.97 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:29s remains)
INFO - root - 2017-12-05 07:16:17.898227: step 1160, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:09s remains)
INFO - root - 2017-12-05 07:16:20.083024: step 1170, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:16s remains)
INFO - root - 2017-12-05 07:16:22.330654: step 1180, loss = 2.09, batch loss = 2.03 (35.6 examples/sec; 0.224 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-05 07:16:24.526497: step 1190, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:35s remains)
INFO - root - 2017-12-05 07:16:26.689416: step 1200, loss = 2.08, batch loss = 2.03 (38.0 examples/sec; 0.210 sec/batch; 19h:21m:12s remains)
INFO - root - 2017-12-05 07:16:28.929459: step 1210, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:50s remains)
INFO - root - 2017-12-05 07:16:31.097323: step 1220, loss = 2.04, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:02s remains)
INFO - root - 2017-12-05 07:16:33.300383: step 1230, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 20h:34m:05s remains)
INFO - root - 2017-12-05 07:16:35.467288: step 1240, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-05 07:16:37.642586: step 1250, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 20h:05m:14s remains)
INFO - root - 2017-12-05 07:16:39.851310: step 1260, loss = 2.03, batch loss = 1.97 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:17s remains)
INFO - root - 2017-12-05 07:16:42.064118: step 1270, loss = 2.03, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:43s remains)
INFO - root - 2017-12-05 07:16:44.981181: step 1280, loss = 2.05, batch loss = 1.99 (21.4 examples/sec; 0.374 sec/batch; 34h:24m:54s remains)
INFO - root - 2017-12-05 07:16:48.767772: step 1290, loss = 2.04, batch loss = 1.99 (21.6 examples/sec; 0.370 sec/batch; 34h:00m:51s remains)
INFO - root - 2017-12-05 07:16:52.579272: step 1300, loss = 2.08, batch loss = 2.02 (19.6 examples/sec; 0.409 sec/batch; 37h:38m:07s remains)
INFO - root - 2017-12-05 07:16:56.455445: step 1310, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.374 sec/batch; 34h:23m:40s remains)
INFO - root - 2017-12-05 07:17:00.242567: step 1320, loss = 2.07, batch loss = 2.01 (21.5 examples/sec; 0.372 sec/batch; 34h:14m:38s remains)
INFO - root - 2017-12-05 07:17:03.985611: step 1330, loss = 2.06, batch loss = 2.01 (21.4 examples/sec; 0.374 sec/batch; 34h:24m:24s remains)
INFO - root - 2017-12-05 07:17:07.774821: step 1340, loss = 2.03, batch loss = 1.97 (22.3 examples/sec; 0.359 sec/batch; 33h:01m:09s remains)
INFO - root - 2017-12-05 07:17:11.574523: step 1350, loss = 2.05, batch loss = 1.99 (21.4 examples/sec; 0.374 sec/batch; 34h:24m:29s remains)
INFO - root - 2017-12-05 07:17:15.308230: step 1360, loss = 2.06, batch loss = 2.00 (21.0 examples/sec; 0.380 sec/batch; 34h:59m:12s remains)
INFO - root - 2017-12-05 07:17:18.999850: step 1370, loss = 2.07, batch loss = 2.01 (21.3 examples/sec; 0.376 sec/batch; 34h:37m:23s remains)
INFO - root - 2017-12-05 07:17:22.726004: step 1380, loss = 2.07, batch loss = 2.01 (22.2 examples/sec; 0.361 sec/batch; 33h:11m:28s remains)
INFO - root - 2017-12-05 07:17:26.441897: step 1390, loss = 2.03, batch loss = 1.97 (21.7 examples/sec; 0.369 sec/batch; 33h:57m:39s remains)
INFO - root - 2017-12-05 07:17:30.085340: step 1400, loss = 2.06, batch loss = 2.01 (21.3 examples/sec; 0.376 sec/batch; 34h:33m:26s remains)
INFO - root - 2017-12-05 07:17:33.952503: step 1410, loss = 2.07, batch loss = 2.02 (21.1 examples/sec; 0.379 sec/batch; 34h:52m:03s remains)
INFO - root - 2017-12-05 07:17:37.719185: step 1420, loss = 2.06, batch loss = 2.00 (21.5 examples/sec; 0.372 sec/batch; 34h:10m:43s remains)
INFO - root - 2017-12-05 07:17:41.450779: step 1430, loss = 2.07, batch loss = 2.01 (21.2 examples/sec; 0.377 sec/batch; 34h:42m:43s remains)
INFO - root - 2017-12-05 07:17:45.265337: step 1440, loss = 2.06, batch loss = 2.00 (21.0 examples/sec; 0.381 sec/batch; 35h:00m:28s remains)
INFO - root - 2017-12-05 07:17:49.096892: step 1450, loss = 2.04, batch loss = 1.98 (21.1 examples/sec; 0.379 sec/batch; 34h:53m:18s remains)
INFO - root - 2017-12-05 07:17:52.843680: step 1460, loss = 2.03, batch loss = 1.97 (20.7 examples/sec; 0.386 sec/batch; 35h:29m:41s remains)
INFO - root - 2017-12-05 07:17:56.583851: step 1470, loss = 2.05, batch loss = 1.99 (20.9 examples/sec; 0.383 sec/batch; 35h:13m:46s remains)
INFO - root - 2017-12-05 07:18:00.450192: step 1480, loss = 2.06, batch loss = 2.01 (20.4 examples/sec; 0.392 sec/batch; 36h:01m:15s remains)
INFO - root - 2017-12-05 07:18:04.309602: step 1490, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.390 sec/batch; 35h:51m:10s remains)
INFO - root - 2017-12-05 07:18:08.146789: step 1500, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.388 sec/batch; 35h:41m:18s remains)
INFO - root - 2017-12-05 07:18:12.093695: step 1510, loss = 2.05, batch loss = 1.99 (20.0 examples/sec; 0.400 sec/batch; 36h:48m:48s remains)
INFO - root - 2017-12-05 07:18:15.987065: step 1520, loss = 2.07, batch loss = 2.01 (20.7 examples/sec; 0.386 sec/batch; 35h:31m:37s remains)
INFO - root - 2017-12-05 07:18:19.877022: step 1530, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.396 sec/batch; 36h:25m:24s remains)
INFO - root - 2017-12-05 07:18:23.673358: step 1540, loss = 2.07, batch loss = 2.01 (24.1 examples/sec; 0.332 sec/batch; 30h:33m:06s remains)
INFO - root - 2017-12-05 07:18:27.483886: step 1550, loss = 2.07, batch loss = 2.01 (21.0 examples/sec; 0.382 sec/batch; 35h:04m:21s remains)
INFO - root - 2017-12-05 07:18:31.362473: step 1560, loss = 2.07, batch loss = 2.01 (21.6 examples/sec; 0.371 sec/batch; 34h:04m:14s remains)
INFO - root - 2017-12-05 07:18:35.230517: step 1570, loss = 2.05, batch loss = 1.99 (20.7 examples/sec; 0.386 sec/batch; 35h:29m:52s remains)
INFO - root - 2017-12-05 07:18:39.085133: step 1580, loss = 2.04, batch loss = 1.98 (22.1 examples/sec; 0.362 sec/batch; 33h:18m:41s remains)
INFO - root - 2017-12-05 07:18:42.877168: step 1590, loss = 2.07, batch loss = 2.01 (21.0 examples/sec; 0.382 sec/batch; 35h:05m:51s remains)
INFO - root - 2017-12-05 07:18:46.752120: step 1600, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.416 sec/batch; 38h:14m:35s remains)
INFO - root - 2017-12-05 07:18:50.622731: step 1610, loss = 2.06, batch loss = 2.00 (23.2 examples/sec; 0.345 sec/batch; 31h:44m:22s remains)
INFO - root - 2017-12-05 07:18:54.462884: step 1620, loss = 2.04, batch loss = 1.98 (21.6 examples/sec; 0.370 sec/batch; 33h:58m:07s remains)
INFO - root - 2017-12-05 07:18:58.292364: step 1630, loss = 2.10, batch loss = 2.04 (20.0 examples/sec; 0.400 sec/batch; 36h:45m:21s remains)
INFO - root - 2017-12-05 07:19:02.050479: step 1640, loss = 2.05, batch loss = 2.00 (21.5 examples/sec; 0.372 sec/batch; 34h:13m:27s remains)
INFO - root - 2017-12-05 07:19:05.838927: step 1650, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.375 sec/batch; 34h:25m:37s remains)
INFO - root - 2017-12-05 07:19:09.709057: step 1660, loss = 2.05, batch loss = 1.99 (19.7 examples/sec; 0.405 sec/batch; 37h:15m:17s remains)
INFO - root - 2017-12-05 07:19:13.666935: step 1670, loss = 2.05, batch loss = 2.00 (19.6 examples/sec; 0.407 sec/batch; 37h:25m:41s remains)
INFO - root - 2017-12-05 07:19:17.483300: step 1680, loss = 2.06, batch loss = 2.00 (20.7 examples/sec; 0.386 sec/batch; 35h:27m:29s remains)
INFO - root - 2017-12-05 07:19:21.377270: step 1690, loss = 2.06, batch loss = 2.00 (21.5 examples/sec; 0.372 sec/batch; 34h:10m:55s remains)
INFO - root - 2017-12-05 07:19:25.225111: step 1700, loss = 2.05, batch loss = 1.99 (20.4 examples/sec; 0.393 sec/batch; 36h:04m:27s remains)
INFO - root - 2017-12-05 07:19:29.250132: step 1710, loss = 2.05, batch loss = 2.00 (20.8 examples/sec; 0.384 sec/batch; 35h:19m:32s remains)
INFO - root - 2017-12-05 07:19:33.125662: step 1720, loss = 2.02, batch loss = 1.97 (23.7 examples/sec; 0.338 sec/batch; 31h:03m:43s remains)
INFO - root - 2017-12-05 07:19:37.036831: step 1730, loss = 2.04, batch loss = 1.98 (20.3 examples/sec; 0.394 sec/batch; 36h:09m:41s remains)
INFO - root - 2017-12-05 07:19:40.912605: step 1740, loss = 2.06, batch loss = 2.00 (20.7 examples/sec; 0.387 sec/batch; 35h:33m:51s remains)
INFO - root - 2017-12-05 07:19:44.774615: step 1750, loss = 2.07, batch loss = 2.01 (21.1 examples/sec; 0.380 sec/batch; 34h:52m:18s remains)
INFO - root - 2017-12-05 07:19:48.638298: step 1760, loss = 2.05, batch loss = 1.99 (20.6 examples/sec; 0.389 sec/batch; 35h:42m:15s remains)
INFO - root - 2017-12-05 07:19:52.531395: step 1770, loss = 2.06, batch loss = 2.00 (19.8 examples/sec; 0.404 sec/batch; 37h:08m:25s remains)
INFO - root - 2017-12-05 07:19:56.280299: step 1780, loss = 2.05, batch loss = 2.00 (22.4 examples/sec; 0.357 sec/batch; 32h:50m:31s remains)
INFO - root - 2017-12-05 07:20:00.102226: step 1790, loss = 2.05, batch loss = 1.99 (21.3 examples/sec; 0.376 sec/batch; 34h:31m:15s remains)
INFO - root - 2017-12-05 07:20:03.809525: step 1800, loss = 2.07, batch loss = 2.01 (21.3 examples/sec; 0.375 sec/batch; 34h:28m:20s remains)
INFO - root - 2017-12-05 07:20:07.728965: step 1810, loss = 2.05, batch loss = 1.99 (20.7 examples/sec; 0.386 sec/batch; 35h:28m:44s remains)
INFO - root - 2017-12-05 07:20:11.463335: step 1820, loss = 2.04, batch loss = 1.98 (20.6 examples/sec; 0.389 sec/batch; 35h:44m:48s remains)
INFO - root - 2017-12-05 07:20:15.344556: step 1830, loss = 2.03, batch loss = 1.97 (21.3 examples/sec; 0.376 sec/batch; 34h:29m:31s remains)
INFO - root - 2017-12-05 07:20:19.204350: step 1840, loss = 2.04, batch loss = 1.99 (20.6 examples/sec; 0.389 sec/batch; 35h:41m:22s remains)
INFO - root - 2017-12-05 07:20:23.055656: step 1850, loss = 2.05, batch loss = 1.99 (20.5 examples/sec; 0.391 sec/batch; 35h:53m:41s remains)
INFO - root - 2017-12-05 07:20:26.968531: step 1860, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.400 sec/batch; 36h:45m:33s remains)
INFO - root - 2017-12-05 07:20:30.909764: step 1870, loss = 2.04, batch loss = 1.99 (20.7 examples/sec; 0.387 sec/batch; 35h:33m:03s remains)
INFO - root - 2017-12-05 07:20:34.772932: step 1880, loss = 2.04, batch loss = 1.99 (20.8 examples/sec; 0.385 sec/batch; 35h:21m:21s remains)
INFO - root - 2017-12-05 07:20:38.682771: step 1890, loss = 2.08, batch loss = 2.03 (20.3 examples/sec; 0.394 sec/batch; 36h:10m:54s remains)
INFO - root - 2017-12-05 07:20:42.549509: step 1900, loss = 2.03, batch loss = 1.97 (20.6 examples/sec; 0.389 sec/batch; 35h:43m:34s remains)
INFO - root - 2017-12-05 07:20:46.482452: step 1910, loss = 2.05, batch loss = 1.99 (19.8 examples/sec; 0.405 sec/batch; 37h:11m:04s remains)
INFO - root - 2017-12-05 07:20:50.310692: step 1920, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.390 sec/batch; 35h:47m:25s remains)
INFO - root - 2017-12-05 07:20:54.099890: step 1930, loss = 2.02, batch loss = 1.97 (20.3 examples/sec; 0.393 sec/batch; 36h:06m:17s remains)
INFO - root - 2017-12-05 07:20:57.995953: step 1940, loss = 2.06, batch loss = 2.00 (20.3 examples/sec; 0.393 sec/batch; 36h:06m:11s remains)
INFO - root - 2017-12-05 07:21:01.919497: step 1950, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.390 sec/batch; 35h:46m:01s remains)
INFO - root - 2017-12-05 07:21:05.719947: step 1960, loss = 2.04, batch loss = 1.98 (28.1 examples/sec; 0.285 sec/batch; 26h:07m:42s remains)
INFO - root - 2017-12-05 07:21:09.429204: step 1970, loss = 2.07, batch loss = 2.01 (21.2 examples/sec; 0.377 sec/batch; 34h:35m:27s remains)
INFO - root - 2017-12-05 07:21:13.275278: step 1980, loss = 2.05, batch loss = 1.99 (20.8 examples/sec; 0.384 sec/batch; 35h:15m:22s remains)
INFO - root - 2017-12-05 07:21:17.128231: step 1990, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.395 sec/batch; 36h:18m:11s remains)
INFO - root - 2017-12-05 07:21:21.011918: step 2000, loss = 2.04, batch loss = 1.98 (20.3 examples/sec; 0.393 sec/batch; 36h:06m:29s remains)
INFO - root - 2017-12-05 07:21:24.964955: step 2010, loss = 2.08, batch loss = 2.02 (20.5 examples/sec; 0.390 sec/batch; 35h:45m:43s remains)
INFO - root - 2017-12-05 07:21:28.798211: step 2020, loss = 2.07, batch loss = 2.01 (21.4 examples/sec; 0.375 sec/batch; 34h:23m:11s remains)
INFO - root - 2017-12-05 07:21:32.663122: step 2030, loss = 2.06, batch loss = 2.00 (21.0 examples/sec; 0.381 sec/batch; 34h:56m:59s remains)
INFO - root - 2017-12-05 07:21:36.582733: step 2040, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.393 sec/batch; 36h:06m:15s remains)
INFO - root - 2017-12-05 07:21:40.377066: step 2050, loss = 2.05, batch loss = 1.99 (20.3 examples/sec; 0.394 sec/batch; 36h:11m:56s remains)
INFO - root - 2017-12-05 07:21:44.324774: step 2060, loss = 2.05, batch loss = 1.99 (20.9 examples/sec; 0.382 sec/batch; 35h:05m:19s remains)
INFO - root - 2017-12-05 07:21:48.226787: step 2070, loss = 2.02, batch loss = 1.96 (20.2 examples/sec; 0.396 sec/batch; 36h:22m:45s remains)
INFO - root - 2017-12-05 07:21:52.102712: step 2080, loss = 2.06, batch loss = 2.00 (20.9 examples/sec; 0.382 sec/batch; 35h:03m:51s remains)
INFO - root - 2017-12-05 07:21:55.918732: step 2090, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.388 sec/batch; 35h:37m:03s remains)
INFO - root - 2017-12-05 07:21:59.794940: step 2100, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.393 sec/batch; 36h:03m:02s remains)
INFO - root - 2017-12-05 07:22:03.738816: step 2110, loss = 2.05, batch loss = 1.99 (21.0 examples/sec; 0.381 sec/batch; 35h:00m:24s remains)
INFO - root - 2017-12-05 07:22:07.648429: step 2120, loss = 2.06, batch loss = 2.00 (20.4 examples/sec; 0.392 sec/batch; 35h:56m:12s remains)
INFO - root - 2017-12-05 07:22:11.544594: step 2130, loss = 2.06, batch loss = 2.00 (20.7 examples/sec; 0.387 sec/batch; 35h:32m:39s remains)
INFO - root - 2017-12-05 07:22:15.385623: step 2140, loss = 2.07, batch loss = 2.01 (20.5 examples/sec; 0.390 sec/batch; 35h:49m:33s remains)
INFO - root - 2017-12-05 07:22:19.237509: step 2150, loss = 2.06, batch loss = 2.00 (21.0 examples/sec; 0.381 sec/batch; 34h:59m:05s remains)
INFO - root - 2017-12-05 07:22:23.130037: step 2160, loss = 2.04, batch loss = 1.98 (20.7 examples/sec; 0.387 sec/batch; 35h:29m:44s remains)
INFO - root - 2017-12-05 07:22:26.911319: step 2170, loss = 2.05, batch loss = 2.00 (21.3 examples/sec; 0.375 sec/batch; 34h:25m:54s remains)
INFO - root - 2017-12-05 07:22:30.771155: step 2180, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.399 sec/batch; 36h:33m:52s remains)
INFO - root - 2017-12-05 07:22:34.586689: step 2190, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.388 sec/batch; 35h:33m:55s remains)
INFO - root - 2017-12-05 07:22:38.447524: step 2200, loss = 2.06, batch loss = 2.00 (20.8 examples/sec; 0.385 sec/batch; 35h:20m:13s remains)
INFO - root - 2017-12-05 07:22:42.428612: step 2210, loss = 2.03, batch loss = 1.98 (20.5 examples/sec; 0.390 sec/batch; 35h:47m:33s remains)
INFO - root - 2017-12-05 07:22:46.237599: step 2220, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.375 sec/batch; 34h:22m:22s remains)
INFO - root - 2017-12-05 07:22:50.103586: step 2230, loss = 2.06, batch loss = 2.00 (20.3 examples/sec; 0.393 sec/batch; 36h:04m:03s remains)
INFO - root - 2017-12-05 07:22:53.941643: step 2240, loss = 2.04, batch loss = 1.98 (20.0 examples/sec; 0.400 sec/batch; 36h:42m:55s remains)
INFO - root - 2017-12-05 07:22:57.798070: step 2250, loss = 2.05, batch loss = 1.99 (20.3 examples/sec; 0.394 sec/batch; 36h:08m:03s remains)
INFO - root - 2017-12-05 07:23:01.572057: step 2260, loss = 2.03, batch loss = 1.97 (21.4 examples/sec; 0.375 sec/batch; 34h:21m:49s remains)
INFO - root - 2017-12-05 07:23:05.450922: step 2270, loss = 2.07, batch loss = 2.01 (20.8 examples/sec; 0.384 sec/batch; 35h:12m:05s remains)
INFO - root - 2017-12-05 07:23:09.356806: step 2280, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.391 sec/batch; 35h:50m:54s remains)
INFO - root - 2017-12-05 07:23:13.238799: step 2290, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.396 sec/batch; 36h:21m:31s remains)
INFO - root - 2017-12-05 07:23:17.159534: step 2300, loss = 2.06, batch loss = 2.00 (20.8 examples/sec; 0.384 sec/batch; 35h:12m:27s remains)
INFO - root - 2017-12-05 07:23:21.109587: step 2310, loss = 2.09, batch loss = 2.03 (21.7 examples/sec; 0.369 sec/batch; 33h:51m:56s remains)
INFO - root - 2017-12-05 07:23:24.935868: step 2320, loss = 2.06, batch loss = 2.01 (20.8 examples/sec; 0.385 sec/batch; 35h:16m:43s remains)
INFO - root - 2017-12-05 07:23:28.768959: step 2330, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.395 sec/batch; 36h:16m:05s remains)
INFO - root - 2017-12-05 07:23:32.635560: step 2340, loss = 2.07, batch loss = 2.02 (20.1 examples/sec; 0.398 sec/batch; 36h:30m:18s remains)
INFO - root - 2017-12-05 07:23:36.555422: step 2350, loss = 2.05, batch loss = 1.99 (19.9 examples/sec; 0.402 sec/batch; 36h:49m:46s remains)
INFO - root - 2017-12-05 07:23:40.413925: step 2360, loss = 2.06, batch loss = 2.00 (20.8 examples/sec; 0.385 sec/batch; 35h:18m:23s remains)
INFO - root - 2017-12-05 07:23:44.311071: step 2370, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.396 sec/batch; 36h:16m:25s remains)
INFO - root - 2017-12-05 07:23:48.064484: step 2380, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.389 sec/batch; 35h:39m:08s remains)
INFO - root - 2017-12-05 07:23:51.888371: step 2390, loss = 2.05, batch loss = 1.99 (20.6 examples/sec; 0.389 sec/batch; 35h:40m:24s remains)
INFO - root - 2017-12-05 07:23:55.713183: step 2400, loss = 2.03, batch loss = 1.98 (20.7 examples/sec; 0.387 sec/batch; 35h:30m:26s remains)
INFO - root - 2017-12-05 07:23:59.596537: step 2410, loss = 2.04, batch loss = 1.98 (21.1 examples/sec; 0.378 sec/batch; 34h:41m:16s remains)
INFO - root - 2017-12-05 07:24:03.382076: step 2420, loss = 2.04, batch loss = 1.98 (20.1 examples/sec; 0.399 sec/batch; 36h:33m:04s remains)
INFO - root - 2017-12-05 07:24:07.177961: step 2430, loss = 2.04, batch loss = 1.99 (21.0 examples/sec; 0.381 sec/batch; 34h:58m:03s remains)
INFO - root - 2017-12-05 07:24:10.937959: step 2440, loss = 2.07, batch loss = 2.01 (20.5 examples/sec; 0.391 sec/batch; 35h:51m:48s remains)
INFO - root - 2017-12-05 07:24:14.739302: step 2450, loss = 2.04, batch loss = 1.99 (21.3 examples/sec; 0.376 sec/batch; 34h:28m:06s remains)
INFO - root - 2017-12-05 07:24:18.557455: step 2460, loss = 2.06, batch loss = 2.00 (21.3 examples/sec; 0.375 sec/batch; 34h:24m:36s remains)
INFO - root - 2017-12-05 07:24:22.391025: step 2470, loss = 2.07, batch loss = 2.01 (21.0 examples/sec; 0.382 sec/batch; 35h:00m:09s remains)
INFO - root - 2017-12-05 07:24:26.157251: step 2480, loss = 2.04, batch loss = 1.98 (22.1 examples/sec; 0.362 sec/batch; 33h:08m:29s remains)
INFO - root - 2017-12-05 07:24:29.967515: step 2490, loss = 2.02, batch loss = 1.97 (21.5 examples/sec; 0.373 sec/batch; 34h:08m:53s remains)
INFO - root - 2017-12-05 07:24:33.788555: step 2500, loss = 2.04, batch loss = 1.98 (20.6 examples/sec; 0.389 sec/batch; 35h:38m:10s remains)
INFO - root - 2017-12-05 07:24:37.667664: step 2510, loss = 2.06, batch loss = 2.00 (20.8 examples/sec; 0.385 sec/batch; 35h:16m:03s remains)
INFO - root - 2017-12-05 07:24:41.399233: step 2520, loss = 2.02, batch loss = 1.96 (21.2 examples/sec; 0.378 sec/batch; 34h:40m:06s remains)
INFO - root - 2017-12-05 07:24:45.219894: step 2530, loss = 2.05, batch loss = 1.99 (21.1 examples/sec; 0.379 sec/batch; 34h:42m:50s remains)
INFO - root - 2017-12-05 07:24:49.154195: step 2540, loss = 2.04, batch loss = 1.98 (19.6 examples/sec; 0.408 sec/batch; 37h:24m:07s remains)
INFO - root - 2017-12-05 07:24:53.020384: step 2550, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.374 sec/batch; 34h:18m:52s remains)
INFO - root - 2017-12-05 07:24:56.849424: step 2560, loss = 2.02, batch loss = 1.96 (20.7 examples/sec; 0.386 sec/batch; 35h:23m:00s remains)
INFO - root - 2017-12-05 07:25:00.644806: step 2570, loss = 2.06, batch loss = 2.00 (20.9 examples/sec; 0.383 sec/batch; 35h:07m:23s remains)
INFO - root - 2017-12-05 07:25:04.451011: step 2580, loss = 2.03, batch loss = 1.97 (21.3 examples/sec; 0.376 sec/batch; 34h:29m:11s remains)
INFO - root - 2017-12-05 07:25:08.311376: step 2590, loss = 2.08, batch loss = 2.02 (20.5 examples/sec; 0.390 sec/batch; 35h:45m:22s remains)
INFO - root - 2017-12-05 07:25:12.071396: step 2600, loss = 2.04, batch loss = 1.98 (20.6 examples/sec; 0.388 sec/batch; 35h:33m:46s remains)
INFO - root - 2017-12-05 07:25:16.021173: step 2610, loss = 2.03, batch loss = 1.98 (20.8 examples/sec; 0.385 sec/batch; 35h:15m:36s remains)
INFO - root - 2017-12-05 07:25:19.898616: step 2620, loss = 2.03, batch loss = 1.98 (21.4 examples/sec; 0.373 sec/batch; 34h:13m:28s remains)
INFO - root - 2017-12-05 07:25:23.664106: step 2630, loss = 2.04, batch loss = 1.99 (22.5 examples/sec; 0.355 sec/batch; 32h:31m:29s remains)
INFO - root - 2017-12-05 07:25:27.563168: step 2640, loss = 2.06, batch loss = 2.00 (21.6 examples/sec; 0.370 sec/batch; 33h:52m:52s remains)
INFO - root - 2017-12-05 07:25:31.480633: step 2650, loss = 2.04, batch loss = 1.98 (20.6 examples/sec; 0.389 sec/batch; 35h:36m:22s remains)
INFO - root - 2017-12-05 07:25:35.323797: step 2660, loss = 2.04, batch loss = 1.98 (21.7 examples/sec; 0.369 sec/batch; 33h:46m:23s remains)
INFO - root - 2017-12-05 07:25:39.204501: step 2670, loss = 2.06, batch loss = 2.00 (20.7 examples/sec; 0.386 sec/batch; 35h:21m:33s remains)
INFO - root - 2017-12-05 07:25:42.973237: step 2680, loss = 2.07, batch loss = 2.01 (21.6 examples/sec; 0.371 sec/batch; 33h:57m:33s remains)
INFO - root - 2017-12-05 07:25:46.770681: step 2690, loss = 2.05, batch loss = 2.00 (21.5 examples/sec; 0.372 sec/batch; 34h:03m:08s remains)
INFO - root - 2017-12-05 07:25:50.564595: step 2700, loss = 2.06, batch loss = 2.00 (20.1 examples/sec; 0.398 sec/batch; 36h:26m:49s remains)
INFO - root - 2017-12-05 07:25:54.527668: step 2710, loss = 2.06, batch loss = 2.00 (19.9 examples/sec; 0.403 sec/batch; 36h:55m:11s remains)
INFO - root - 2017-12-05 07:25:58.402179: step 2720, loss = 2.03, batch loss = 1.97 (20.8 examples/sec; 0.385 sec/batch; 35h:15m:36s remains)
INFO - root - 2017-12-05 07:26:02.171826: step 2730, loss = 2.10, batch loss = 2.04 (21.3 examples/sec; 0.376 sec/batch; 34h:25m:29s remains)
INFO - root - 2017-12-05 07:26:06.027441: step 2740, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 0.378 sec/batch; 34h:38m:26s remains)
INFO - root - 2017-12-05 07:26:09.758932: step 2750, loss = 2.07, batch loss = 2.01 (20.5 examples/sec; 0.391 sec/batch; 35h:48m:22s remains)
INFO - root - 2017-12-05 07:26:13.573373: step 2760, loss = 2.03, batch loss = 1.98 (19.9 examples/sec; 0.402 sec/batch; 36h:50m:24s remains)
INFO - root - 2017-12-05 07:26:17.380584: step 2770, loss = 2.05, batch loss = 1.99 (21.3 examples/sec; 0.375 sec/batch; 34h:20m:03s remains)
INFO - root - 2017-12-05 07:26:21.206510: step 2780, loss = 2.06, batch loss = 2.00 (20.4 examples/sec; 0.392 sec/batch; 35h:56m:51s remains)
INFO - root - 2017-12-05 07:26:25.026553: step 2790, loss = 2.01, batch loss = 1.96 (21.1 examples/sec; 0.380 sec/batch; 34h:47m:45s remains)
INFO - root - 2017-12-05 07:26:28.893210: step 2800, loss = 2.05, batch loss = 1.99 (21.5 examples/sec; 0.372 sec/batch; 34h:05m:59s remains)
INFO - root - 2017-12-05 07:26:32.833998: step 2810, loss = 2.03, batch loss = 1.97 (20.6 examples/sec; 0.388 sec/batch; 35h:29m:26s remains)
INFO - root - 2017-12-05 07:26:36.645092: step 2820, loss = 2.03, batch loss = 1.97 (21.3 examples/sec; 0.375 sec/batch; 34h:20m:34s remains)
INFO - root - 2017-12-05 07:26:40.402333: step 2830, loss = 2.05, batch loss = 2.00 (21.0 examples/sec; 0.380 sec/batch; 34h:49m:14s remains)
INFO - root - 2017-12-05 07:26:44.218682: step 2840, loss = 2.04, batch loss = 1.99 (21.3 examples/sec; 0.376 sec/batch; 34h:23m:13s remains)
INFO - root - 2017-12-05 07:26:48.075443: step 2850, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.401 sec/batch; 36h:42m:39s remains)
INFO - root - 2017-12-05 07:26:51.929697: step 2860, loss = 2.03, batch loss = 1.97 (21.0 examples/sec; 0.381 sec/batch; 34h:50m:39s remains)
INFO - root - 2017-12-05 07:26:55.649879: step 2870, loss = 2.06, batch loss = 2.00 (21.1 examples/sec; 0.379 sec/batch; 34h:39m:44s remains)
INFO - root - 2017-12-05 07:26:59.488700: step 2880, loss = 2.05, batch loss = 1.99 (21.3 examples/sec; 0.376 sec/batch; 34h:26m:57s remains)
INFO - root - 2017-12-05 07:27:03.411500: step 2890, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.401 sec/batch; 36h:40m:51s remains)
INFO - root - 2017-12-05 07:27:07.278466: step 2900, loss = 2.05, batch loss = 1.99 (20.6 examples/sec; 0.388 sec/batch; 35h:29m:13s remains)
INFO - root - 2017-12-05 07:27:11.179564: step 2910, loss = 2.05, batch loss = 1.99 (21.3 examples/sec; 0.375 sec/batch; 34h:21m:12s remains)
INFO - root - 2017-12-05 07:27:14.938322: step 2920, loss = 2.05, batch loss = 1.99 (22.6 examples/sec; 0.355 sec/batch; 32h:28m:41s remains)
INFO - root - 2017-12-05 07:27:18.876238: step 2930, loss = 2.08, batch loss = 2.03 (20.2 examples/sec; 0.397 sec/batch; 36h:19m:31s remains)
INFO - root - 2017-12-05 07:27:22.721045: step 2940, loss = 2.05, batch loss = 1.99 (20.5 examples/sec; 0.389 sec/batch; 35h:38m:34s remains)
INFO - root - 2017-12-05 07:27:26.578541: step 2950, loss = 2.05, batch loss = 2.00 (20.7 examples/sec; 0.387 sec/batch; 35h:24m:56s remains)
INFO - root - 2017-12-05 07:27:30.370697: step 2960, loss = 2.08, batch loss = 2.02 (20.8 examples/sec; 0.384 sec/batch; 35h:09m:20s remains)
INFO - root - 2017-12-05 07:27:34.257083: step 2970, loss = 2.04, batch loss = 1.98 (19.7 examples/sec; 0.406 sec/batch; 37h:07m:56s remains)
INFO - root - 2017-12-05 07:27:38.030663: step 2980, loss = 2.09, batch loss = 2.03 (22.2 examples/sec; 0.360 sec/batch; 32h:55m:58s remains)
INFO - root - 2017-12-05 07:27:41.913770: step 2990, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:32m:22s remains)
INFO - root - 2017-12-05 07:27:45.770952: step 3000, loss = 2.05, batch loss = 1.99 (20.4 examples/sec; 0.392 sec/batch; 35h:52m:28s remains)
INFO - root - 2017-12-05 07:27:49.772953: step 3010, loss = 2.05, batch loss = 1.99 (20.6 examples/sec; 0.388 sec/batch; 35h:31m:12s remains)
INFO - root - 2017-12-05 07:27:53.570208: step 3020, loss = 2.04, batch loss = 1.98 (22.4 examples/sec; 0.357 sec/batch; 32h:38m:15s remains)
INFO - root - 2017-12-05 07:27:57.481460: step 3030, loss = 2.05, batch loss = 1.99 (20.0 examples/sec; 0.400 sec/batch; 36h:36m:29s remains)
INFO - root - 2017-12-05 07:28:01.317478: step 3040, loss = 2.06, batch loss = 2.01 (22.1 examples/sec; 0.361 sec/batch; 33h:03m:31s remains)
INFO - root - 2017-12-05 07:28:05.196935: step 3050, loss = 2.07, batch loss = 2.01 (21.5 examples/sec; 0.373 sec/batch; 34h:07m:01s remains)
INFO - root - 2017-12-05 07:28:09.005591: step 3060, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.375 sec/batch; 34h:17m:12s remains)
INFO - root - 2017-12-05 07:28:12.798697: step 3070, loss = 2.04, batch loss = 1.98 (20.9 examples/sec; 0.383 sec/batch; 35h:03m:11s remains)
INFO - root - 2017-12-05 07:28:16.588564: step 3080, loss = 2.01, batch loss = 1.95 (21.1 examples/sec; 0.378 sec/batch; 34h:37m:14s remains)
INFO - root - 2017-12-05 07:28:20.355111: step 3090, loss = 2.07, batch loss = 2.01 (23.3 examples/sec; 0.344 sec/batch; 31h:27m:50s remains)
INFO - root - 2017-12-05 07:28:24.164506: step 3100, loss = 2.03, batch loss = 1.98 (20.8 examples/sec; 0.385 sec/batch; 35h:14m:28s remains)
INFO - root - 2017-12-05 07:28:28.121405: step 3110, loss = 2.04, batch loss = 1.98 (20.5 examples/sec; 0.391 sec/batch; 35h:44m:35s remains)
INFO - root - 2017-12-05 07:28:31.893982: step 3120, loss = 2.08, batch loss = 2.02 (20.7 examples/sec; 0.386 sec/batch; 35h:19m:19s remains)
INFO - root - 2017-12-05 07:28:35.682999: step 3130, loss = 2.05, batch loss = 1.99 (20.4 examples/sec; 0.392 sec/batch; 35h:51m:11s remains)
INFO - root - 2017-12-05 07:28:39.547768: step 3140, loss = 2.07, batch loss = 2.01 (20.9 examples/sec; 0.383 sec/batch; 35h:04m:30s remains)
INFO - root - 2017-12-05 07:28:43.375663: step 3150, loss = 2.04, batch loss = 1.98 (21.5 examples/sec; 0.372 sec/batch; 34h:02m:49s remains)
INFO - root - 2017-12-05 07:28:47.308173: step 3160, loss = 2.03, batch loss = 1.97 (20.8 examples/sec; 0.384 sec/batch; 35h:09m:56s remains)
INFO - root - 2017-12-05 07:28:50.993633: step 3170, loss = 2.03, batch loss = 1.97 (21.3 examples/sec; 0.376 sec/batch; 34h:21m:46s remains)
INFO - root - 2017-12-05 07:28:54.766790: step 3180, loss = 2.05, batch loss = 1.99 (20.7 examples/sec; 0.386 sec/batch; 35h:17m:42s remains)
INFO - root - 2017-12-05 07:28:58.601389: step 3190, loss = 2.05, batch loss = 1.99 (21.4 examples/sec; 0.374 sec/batch; 34h:12m:18s remains)
INFO - root - 2017-12-05 07:29:02.435729: step 3200, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 0.377 sec/batch; 34h:30m:53s remains)
INFO - root - 2017-12-05 07:29:06.360285: step 3210, loss = 2.06, batch loss = 2.00 (21.4 examples/sec; 0.374 sec/batch; 34h:13m:42s remains)
INFO - root - 2017-12-05 07:29:10.131822: step 3220, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.418 sec/batch; 38h:13m:18s remains)
INFO - root - 2017-12-05 07:29:13.939894: step 3230, loss = 2.03, batch loss = 1.97 (20.7 examples/sec; 0.386 sec/batch; 35h:16m:02s remains)
INFO - root - 2017-12-05 07:29:17.776325: step 3240, loss = 2.07, batch loss = 2.01 (21.0 examples/sec; 0.380 sec/batch; 34h:45m:46s remains)
INFO - root - 2017-12-05 07:29:21.653303: step 3250, loss = 2.02, batch loss = 1.96 (20.5 examples/sec; 0.390 sec/batch; 35h:42m:35s remains)
INFO - root - 2017-12-05 07:29:25.427999: step 3260, loss = 2.02, batch loss = 1.96 (21.0 examples/sec; 0.381 sec/batch; 34h:53m:17s remains)
INFO - root - 2017-12-05 07:29:29.241164: step 3270, loss = 2.05, batch loss = 1.99 (21.4 examples/sec; 0.374 sec/batch; 34h:14m:40s remains)
INFO - root - 2017-12-05 07:29:33.009979: step 3280, loss = 2.05, batch loss = 1.99 (20.6 examples/sec; 0.388 sec/batch; 35h:27m:26s remains)
INFO - root - 2017-12-05 07:29:36.836522: step 3290, loss = 2.04, batch loss = 1.98 (21.1 examples/sec; 0.379 sec/batch; 34h:39m:08s remains)
INFO - root - 2017-12-05 07:29:40.624619: step 3300, loss = 2.04, batch loss = 1.98 (20.6 examples/sec; 0.388 sec/batch; 35h:31m:30s remains)
INFO - root - 2017-12-05 07:29:44.599840: step 3310, loss = 2.02, batch loss = 1.97 (21.1 examples/sec; 0.379 sec/batch; 34h:40m:12s remains)
INFO - root - 2017-12-05 07:29:48.297756: step 3320, loss = 2.06, batch loss = 2.00 (21.3 examples/sec; 0.376 sec/batch; 34h:22m:06s remains)
INFO - root - 2017-12-05 07:29:52.111140: step 3330, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.395 sec/batch; 36h:04m:29s remains)
INFO - root - 2017-12-05 07:29:55.995461: step 3340, loss = 2.01, batch loss = 1.95 (21.2 examples/sec; 0.377 sec/batch; 34h:26m:24s remains)
INFO - root - 2017-12-05 07:29:59.893411: step 3350, loss = 2.05, batch loss = 1.99 (20.2 examples/sec; 0.396 sec/batch; 36h:10m:14s remains)
INFO - root - 2017-12-05 07:30:03.719820: step 3360, loss = 2.01, batch loss = 1.95 (21.8 examples/sec; 0.367 sec/batch; 33h:33m:19s remains)
INFO - root - 2017-12-05 07:30:07.558118: step 3370, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 0.377 sec/batch; 34h:27m:30s remains)
INFO - root - 2017-12-05 07:30:11.377997: step 3380, loss = 2.06, batch loss = 2.00 (21.0 examples/sec; 0.382 sec/batch; 34h:53m:25s remains)
INFO - root - 2017-12-05 07:30:15.273801: step 3390, loss = 2.06, batch loss = 2.00 (20.4 examples/sec; 0.391 sec/batch; 35h:46m:15s remains)
INFO - root - 2017-12-05 07:30:19.087999: step 3400, loss = 2.05, batch loss = 1.99 (20.0 examples/sec; 0.399 sec/batch; 36h:30m:34s remains)
INFO - root - 2017-12-05 07:30:23.025396: step 3410, loss = 2.06, batch loss = 2.00 (20.3 examples/sec; 0.395 sec/batch; 36h:05m:08s remains)
INFO - root - 2017-12-05 07:30:26.831168: step 3420, loss = 2.04, batch loss = 1.99 (20.9 examples/sec; 0.383 sec/batch; 35h:01m:34s remains)
INFO - root - 2017-12-05 07:30:30.726936: step 3430, loss = 2.03, batch loss = 1.97 (20.6 examples/sec; 0.389 sec/batch; 35h:32m:55s remains)
INFO - root - 2017-12-05 07:30:34.552966: step 3440, loss = 2.03, batch loss = 1.98 (21.2 examples/sec; 0.377 sec/batch; 34h:29m:24s remains)
INFO - root - 2017-12-05 07:30:38.332787: step 3450, loss = 2.05, batch loss = 1.99 (21.1 examples/sec; 0.379 sec/batch; 34h:36m:12s remains)
INFO - root - 2017-12-05 07:30:42.166822: step 3460, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.395 sec/batch; 36h:07m:34s remains)
INFO - root - 2017-12-05 07:30:45.983027: step 3470, loss = 2.05, batch loss = 1.99 (21.9 examples/sec; 0.365 sec/batch; 33h:19m:28s remains)
INFO - root - 2017-12-05 07:30:49.823983: step 3480, loss = 2.08, batch loss = 2.02 (20.8 examples/sec; 0.385 sec/batch; 35h:10m:41s remains)
INFO - root - 2017-12-05 07:30:53.646138: step 3490, loss = 2.09, batch loss = 2.03 (21.5 examples/sec; 0.373 sec/batch; 34h:02m:49s remains)
INFO - root - 2017-12-05 07:30:57.448491: step 3500, loss = 2.04, batch loss = 1.99 (20.8 examples/sec; 0.385 sec/batch; 35h:13m:40s remains)
INFO - root - 2017-12-05 07:31:01.282261: step 3510, loss = 2.03, batch loss = 1.98 (22.2 examples/sec; 0.360 sec/batch; 32h:56m:25s remains)
INFO - root - 2017-12-05 07:31:05.016942: step 3520, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.393 sec/batch; 35h:56m:35s remains)
INFO - root - 2017-12-05 07:31:08.912895: step 3530, loss = 2.05, batch loss = 1.99 (21.1 examples/sec; 0.379 sec/batch; 34h:35m:46s remains)
INFO - root - 2017-12-05 07:31:12.804324: step 3540, loss = 2.06, batch loss = 2.00 (20.0 examples/sec; 0.400 sec/batch; 36h:32m:55s remains)
INFO - root - 2017-12-05 07:31:16.698692: step 3550, loss = 2.07, batch loss = 2.01 (20.6 examples/sec; 0.389 sec/batch; 35h:30m:01s remains)
INFO - root - 2017-12-05 07:31:20.623313: step 3560, loss = 2.06, batch loss = 2.01 (19.5 examples/sec; 0.410 sec/batch; 37h:27m:12s remains)
INFO - root - 2017-12-05 07:31:24.487581: step 3570, loss = 2.05, batch loss = 1.99 (21.9 examples/sec; 0.365 sec/batch; 33h:23m:37s remains)
INFO - root - 2017-12-05 07:31:28.385334: step 3580, loss = 2.05, batch loss = 1.99 (21.2 examples/sec; 0.377 sec/batch; 34h:26m:34s remains)
INFO - root - 2017-12-05 07:31:32.291741: step 3590, loss = 2.04, batch loss = 1.98 (20.8 examples/sec; 0.384 sec/batch; 35h:03m:26s remains)
INFO - root - 2017-12-05 07:31:36.135940: step 3600, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 0.378 sec/batch; 34h:33m:25s remains)
INFO - root - 2017-12-05 07:31:40.132542: step 3610, loss = 2.04, batch loss = 1.98 (20.3 examples/sec; 0.394 sec/batch; 35h:57m:00s remains)
INFO - root - 2017-12-05 07:31:44.022515: step 3620, loss = 2.05, batch loss = 1.99 (21.9 examples/sec; 0.365 sec/batch; 33h:19m:18s remains)
INFO - root - 2017-12-05 07:31:47.789969: step 3630, loss = 2.06, batch loss = 2.00 (21.3 examples/sec; 0.376 sec/batch; 34h:21m:57s remains)
