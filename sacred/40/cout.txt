INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "40"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
sdfah Tensor("siamese_fc/conv5/def/offset2/BiasAdd:0", shape=(8, 8, 8, 72), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
sdfah Tensor("siamese_fc_1/conv5/def/offset2/BiasAdd:0", shape=(8, 22, 22, 72), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 06:50:22.345155: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:50:22.345193: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:50:22.345199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:50:22.345204: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:50:22.345208: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:50:22.911742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 06:50:22.911781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 06:50:22.911788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 06:50:22.911796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 06:50:25.871480: step 0, loss = 1.11, batch loss = 1.01 (3.5 examples/sec; 2.271 sec/batch; 209h:43m:41s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 06:50:28.424209: step 10, loss = 1.06, batch loss = 0.97 (44.0 examples/sec; 0.182 sec/batch; 16h:48m:02s remains)
INFO - root - 2017-12-03 06:50:30.315862: step 20, loss = 1.14, batch loss = 1.05 (43.2 examples/sec; 0.185 sec/batch; 17h:05m:15s remains)
INFO - root - 2017-12-03 06:50:32.194716: step 30, loss = 1.04, batch loss = 0.94 (43.3 examples/sec; 0.185 sec/batch; 17h:02m:59s remains)
INFO - root - 2017-12-03 06:50:34.069190: step 40, loss = 1.08, batch loss = 0.98 (42.5 examples/sec; 0.188 sec/batch; 17h:24m:06s remains)
INFO - root - 2017-12-03 06:50:35.961273: step 50, loss = 1.04, batch loss = 0.95 (42.0 examples/sec; 0.190 sec/batch; 17h:35m:09s remains)
INFO - root - 2017-12-03 06:50:37.849822: step 60, loss = 1.11, batch loss = 1.01 (42.5 examples/sec; 0.188 sec/batch; 17h:22m:23s remains)
INFO - root - 2017-12-03 06:50:39.727376: step 70, loss = 0.94, batch loss = 0.85 (42.1 examples/sec; 0.190 sec/batch; 17h:33m:43s remains)
INFO - root - 2017-12-03 06:50:41.671008: step 80, loss = 1.15, batch loss = 1.05 (41.9 examples/sec; 0.191 sec/batch; 17h:38m:55s remains)
INFO - root - 2017-12-03 06:50:43.553648: step 90, loss = 1.11, batch loss = 1.01 (42.9 examples/sec; 0.187 sec/batch; 17h:14m:14s remains)
INFO - root - 2017-12-03 06:50:45.426548: step 100, loss = 1.02, batch loss = 0.93 (43.1 examples/sec; 0.185 sec/batch; 17h:07m:13s remains)
INFO - root - 2017-12-03 06:50:47.409184: step 110, loss = 1.07, batch loss = 0.97 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:19s remains)
INFO - root - 2017-12-03 06:50:49.288428: step 120, loss = 1.04, batch loss = 0.94 (42.3 examples/sec; 0.189 sec/batch; 17h:28m:01s remains)
INFO - root - 2017-12-03 06:50:51.201473: step 130, loss = 0.95, batch loss = 0.86 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:51s remains)
INFO - root - 2017-12-03 06:50:53.093241: step 140, loss = 1.02, batch loss = 0.93 (42.0 examples/sec; 0.191 sec/batch; 17h:36m:14s remains)
INFO - root - 2017-12-03 06:50:55.008689: step 150, loss = 1.06, batch loss = 0.96 (41.4 examples/sec; 0.193 sec/batch; 17h:51m:29s remains)
INFO - root - 2017-12-03 06:50:56.899354: step 160, loss = 1.11, batch loss = 1.01 (41.0 examples/sec; 0.195 sec/batch; 18h:01m:17s remains)
INFO - root - 2017-12-03 06:50:58.793685: step 170, loss = 1.07, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:33s remains)
INFO - root - 2017-12-03 06:51:00.665216: step 180, loss = 1.00, batch loss = 0.90 (43.3 examples/sec; 0.185 sec/batch; 17h:03m:08s remains)
INFO - root - 2017-12-03 06:51:02.553372: step 190, loss = 0.84, batch loss = 0.74 (43.4 examples/sec; 0.184 sec/batch; 17h:00m:35s remains)
INFO - root - 2017-12-03 06:51:04.454727: step 200, loss = 0.92, batch loss = 0.82 (43.7 examples/sec; 0.183 sec/batch; 16h:53m:12s remains)
INFO - root - 2017-12-03 06:51:06.390366: step 210, loss = 1.30, batch loss = 1.20 (42.6 examples/sec; 0.188 sec/batch; 17h:20m:37s remains)
INFO - root - 2017-12-03 06:51:08.302273: step 220, loss = 1.10, batch loss = 1.00 (41.7 examples/sec; 0.192 sec/batch; 17h:41m:46s remains)
INFO - root - 2017-12-03 06:51:10.219760: step 230, loss = 1.22, batch loss = 1.13 (41.2 examples/sec; 0.194 sec/batch; 17h:54m:38s remains)
INFO - root - 2017-12-03 06:51:12.119287: step 240, loss = 1.09, batch loss = 0.99 (42.1 examples/sec; 0.190 sec/batch; 17h:32m:12s remains)
INFO - root - 2017-12-03 06:51:14.031592: step 250, loss = 1.05, batch loss = 0.95 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:57s remains)
INFO - root - 2017-12-03 06:51:15.970043: step 260, loss = 1.02, batch loss = 0.92 (41.5 examples/sec; 0.193 sec/batch; 17h:48m:19s remains)
INFO - root - 2017-12-03 06:51:17.852097: step 270, loss = 1.01, batch loss = 0.92 (41.1 examples/sec; 0.194 sec/batch; 17h:56m:48s remains)
INFO - root - 2017-12-03 06:51:19.747021: step 280, loss = 1.01, batch loss = 0.91 (43.0 examples/sec; 0.186 sec/batch; 17h:10m:51s remains)
INFO - root - 2017-12-03 06:51:21.664078: step 290, loss = 1.01, batch loss = 0.91 (40.8 examples/sec; 0.196 sec/batch; 18h:05m:36s remains)
INFO - root - 2017-12-03 06:51:23.570242: step 300, loss = 1.14, batch loss = 1.04 (41.5 examples/sec; 0.193 sec/batch; 17h:46m:39s remains)
INFO - root - 2017-12-03 06:51:25.582180: step 310, loss = 1.24, batch loss = 1.14 (41.2 examples/sec; 0.194 sec/batch; 17h:53m:54s remains)
INFO - root - 2017-12-03 06:51:27.473122: step 320, loss = 1.09, batch loss = 0.99 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:34s remains)
INFO - root - 2017-12-03 06:51:29.380016: step 330, loss = 1.06, batch loss = 0.96 (40.9 examples/sec; 0.196 sec/batch; 18h:02m:28s remains)
INFO - root - 2017-12-03 06:51:31.257014: step 340, loss = 1.17, batch loss = 1.07 (43.7 examples/sec; 0.183 sec/batch; 16h:54m:00s remains)
INFO - root - 2017-12-03 06:51:33.139467: step 350, loss = 1.21, batch loss = 1.11 (43.0 examples/sec; 0.186 sec/batch; 17h:09m:03s remains)
INFO - root - 2017-12-03 06:51:35.014696: step 360, loss = 1.21, batch loss = 1.11 (42.5 examples/sec; 0.188 sec/batch; 17h:22m:10s remains)
INFO - root - 2017-12-03 06:51:36.918835: step 370, loss = 1.13, batch loss = 1.03 (42.7 examples/sec; 0.187 sec/batch; 17h:15m:55s remains)
INFO - root - 2017-12-03 06:51:38.839801: step 380, loss = 1.20, batch loss = 1.10 (42.6 examples/sec; 0.188 sec/batch; 17h:19m:59s remains)
INFO - root - 2017-12-03 06:51:40.747426: step 390, loss = 1.04, batch loss = 0.95 (40.9 examples/sec; 0.196 sec/batch; 18h:02m:23s remains)
INFO - root - 2017-12-03 06:51:42.653213: step 400, loss = 1.11, batch loss = 1.01 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:39s remains)
INFO - root - 2017-12-03 06:51:44.616841: step 410, loss = 1.12, batch loss = 1.02 (41.1 examples/sec; 0.195 sec/batch; 17h:57m:00s remains)
INFO - root - 2017-12-03 06:51:46.569191: step 420, loss = 1.27, batch loss = 1.17 (42.9 examples/sec; 0.186 sec/batch; 17h:11m:31s remains)
INFO - root - 2017-12-03 06:51:48.475945: step 430, loss = 1.33, batch loss = 1.24 (43.3 examples/sec; 0.185 sec/batch; 17h:03m:37s remains)
INFO - root - 2017-12-03 06:51:50.398583: step 440, loss = 1.20, batch loss = 1.11 (42.3 examples/sec; 0.189 sec/batch; 17h:27m:52s remains)
INFO - root - 2017-12-03 06:51:52.306771: step 450, loss = 1.05, batch loss = 0.95 (41.9 examples/sec; 0.191 sec/batch; 17h:36m:21s remains)
INFO - root - 2017-12-03 06:51:54.217325: step 460, loss = 0.97, batch loss = 0.87 (41.4 examples/sec; 0.193 sec/batch; 17h:49m:52s remains)
INFO - root - 2017-12-03 06:51:56.112586: step 470, loss = 1.05, batch loss = 0.95 (42.6 examples/sec; 0.188 sec/batch; 17h:18m:46s remains)
INFO - root - 2017-12-03 06:51:58.031306: step 480, loss = 1.15, batch loss = 1.05 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:23s remains)
INFO - root - 2017-12-03 06:51:59.964788: step 490, loss = 1.07, batch loss = 0.97 (41.3 examples/sec; 0.194 sec/batch; 17h:51m:38s remains)
INFO - root - 2017-12-03 06:52:01.847264: step 500, loss = 1.18, batch loss = 1.08 (41.3 examples/sec; 0.194 sec/batch; 17h:51m:44s remains)
INFO - root - 2017-12-03 06:52:03.800448: step 510, loss = 0.98, batch loss = 0.89 (42.9 examples/sec; 0.186 sec/batch; 17h:11m:12s remains)
INFO - root - 2017-12-03 06:52:05.714633: step 520, loss = 1.21, batch loss = 1.11 (41.8 examples/sec; 0.191 sec/batch; 17h:38m:50s remains)
INFO - root - 2017-12-03 06:52:07.630485: step 530, loss = 0.95, batch loss = 0.85 (43.4 examples/sec; 0.184 sec/batch; 16h:59m:50s remains)
INFO - root - 2017-12-03 06:52:09.553703: step 540, loss = 1.16, batch loss = 1.07 (41.0 examples/sec; 0.195 sec/batch; 17h:58m:55s remains)
INFO - root - 2017-12-03 06:52:11.508831: step 550, loss = 1.17, batch loss = 1.07 (41.6 examples/sec; 0.192 sec/batch; 17h:42m:48s remains)
INFO - root - 2017-12-03 06:52:13.423839: step 560, loss = 1.03, batch loss = 0.93 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-03 06:52:15.323627: step 570, loss = 1.13, batch loss = 1.04 (43.5 examples/sec; 0.184 sec/batch; 16h:58m:04s remains)
INFO - root - 2017-12-03 06:52:17.243864: step 580, loss = 1.30, batch loss = 1.20 (41.0 examples/sec; 0.195 sec/batch; 17h:59m:57s remains)
INFO - root - 2017-12-03 06:52:19.134720: step 590, loss = 1.04, batch loss = 0.94 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:11s remains)
INFO - root - 2017-12-03 06:52:21.054226: step 600, loss = 1.11, batch loss = 1.01 (42.2 examples/sec; 0.190 sec/batch; 17h:28m:44s remains)
INFO - root - 2017-12-03 06:52:23.090833: step 610, loss = 1.33, batch loss = 1.23 (41.8 examples/sec; 0.192 sec/batch; 17h:39m:35s remains)
INFO - root - 2017-12-03 06:52:24.992207: step 620, loss = 1.14, batch loss = 1.04 (43.2 examples/sec; 0.185 sec/batch; 17h:04m:29s remains)
INFO - root - 2017-12-03 06:52:26.926594: step 630, loss = 1.21, batch loss = 1.11 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:28s remains)
INFO - root - 2017-12-03 06:52:28.860383: step 640, loss = 1.08, batch loss = 0.98 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:35s remains)
INFO - root - 2017-12-03 06:52:30.755614: step 650, loss = 1.23, batch loss = 1.13 (43.6 examples/sec; 0.183 sec/batch; 16h:54m:33s remains)
INFO - root - 2017-12-03 06:52:32.657455: step 660, loss = 1.08, batch loss = 0.97 (42.9 examples/sec; 0.187 sec/batch; 17h:11m:29s remains)
INFO - root - 2017-12-03 06:52:34.568743: step 670, loss = 0.84, batch loss = 0.74 (42.3 examples/sec; 0.189 sec/batch; 17h:25m:31s remains)
INFO - root - 2017-12-03 06:52:36.477895: step 680, loss = 1.16, batch loss = 1.06 (41.0 examples/sec; 0.195 sec/batch; 17h:59m:13s remains)
INFO - root - 2017-12-03 06:52:38.382250: step 690, loss = 0.97, batch loss = 0.87 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:26s remains)
INFO - root - 2017-12-03 06:52:40.316080: step 700, loss = 1.03, batch loss = 0.93 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:48s remains)
INFO - root - 2017-12-03 06:52:42.292481: step 710, loss = 1.04, batch loss = 0.93 (41.9 examples/sec; 0.191 sec/batch; 17h:36m:06s remains)
INFO - root - 2017-12-03 06:52:44.174566: step 720, loss = 1.08, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 17h:12m:57s remains)
INFO - root - 2017-12-03 06:52:46.092611: step 730, loss = 0.94, batch loss = 0.83 (40.7 examples/sec; 0.197 sec/batch; 18h:07m:53s remains)
INFO - root - 2017-12-03 06:52:48.020304: step 740, loss = 0.92, batch loss = 0.81 (41.0 examples/sec; 0.195 sec/batch; 17h:58m:44s remains)
INFO - root - 2017-12-03 06:52:49.940714: step 750, loss = 0.96, batch loss = 0.85 (42.0 examples/sec; 0.190 sec/batch; 17h:32m:37s remains)
INFO - root - 2017-12-03 06:52:51.853991: step 760, loss = 1.21, batch loss = 1.10 (40.5 examples/sec; 0.197 sec/batch; 18h:10m:56s remains)
INFO - root - 2017-12-03 06:52:53.759218: step 770, loss = 1.23, batch loss = 1.12 (42.8 examples/sec; 0.187 sec/batch; 17h:12m:28s remains)
INFO - root - 2017-12-03 06:52:55.660616: step 780, loss = 0.92, batch loss = 0.81 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:11s remains)
INFO - root - 2017-12-03 06:52:57.563193: step 790, loss = 1.02, batch loss = 0.90 (39.9 examples/sec; 0.200 sec/batch; 18h:27m:50s remains)
INFO - root - 2017-12-03 06:52:59.476910: step 800, loss = 1.11, batch loss = 0.99 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:03s remains)
INFO - root - 2017-12-03 06:53:01.447142: step 810, loss = 1.57, batch loss = 1.45 (41.7 examples/sec; 0.192 sec/batch; 17h:40m:38s remains)
INFO - root - 2017-12-03 06:53:03.366739: step 820, loss = 0.99, batch loss = 0.88 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:15s remains)
INFO - root - 2017-12-03 06:53:05.277914: step 830, loss = 0.95, batch loss = 0.83 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:35s remains)
INFO - root - 2017-12-03 06:53:07.193609: step 840, loss = 0.85, batch loss = 0.72 (43.4 examples/sec; 0.184 sec/batch; 16h:58m:14s remains)
INFO - root - 2017-12-03 06:53:09.076178: step 850, loss = 0.99, batch loss = 0.87 (42.2 examples/sec; 0.189 sec/batch; 17h:27m:06s remains)
INFO - root - 2017-12-03 06:53:11.013780: step 860, loss = 1.21, batch loss = 1.08 (40.1 examples/sec; 0.200 sec/batch; 18h:23m:13s remains)
INFO - root - 2017-12-03 06:53:12.894241: step 870, loss = 1.74, batch loss = 1.61 (43.1 examples/sec; 0.185 sec/batch; 17h:04m:59s remains)
INFO - root - 2017-12-03 06:53:14.796289: step 880, loss = 1.21, batch loss = 1.02 (43.1 examples/sec; 0.186 sec/batch; 17h:05m:17s remains)
INFO - root - 2017-12-03 06:53:16.690235: step 890, loss = 0.70, batch loss = 0.51 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:29s remains)
INFO - root - 2017-12-03 06:53:18.616664: step 900, loss = 0.99, batch loss = 0.80 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:40s remains)
INFO - root - 2017-12-03 06:53:20.593836: step 910, loss = 0.93, batch loss = 0.73 (42.6 examples/sec; 0.188 sec/batch; 17h:18m:26s remains)
INFO - root - 2017-12-03 06:53:22.473358: step 920, loss = 1.49, batch loss = 1.29 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:39s remains)
INFO - root - 2017-12-03 06:53:24.394998: step 930, loss = 1.00, batch loss = 0.78 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:44s remains)
INFO - root - 2017-12-03 06:53:26.314607: step 940, loss = 1.25, batch loss = 1.03 (40.9 examples/sec; 0.196 sec/batch; 18h:01m:43s remains)
INFO - root - 2017-12-03 06:53:28.241239: step 950, loss = 1.34, batch loss = 1.12 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:48s remains)
INFO - root - 2017-12-03 06:53:30.152864: step 960, loss = 1.96, batch loss = 1.73 (42.0 examples/sec; 0.190 sec/batch; 17h:32m:11s remains)
INFO - root - 2017-12-03 06:53:32.054963: step 970, loss = 3.27, batch loss = 2.89 (41.1 examples/sec; 0.195 sec/batch; 17h:55m:17s remains)
INFO - root - 2017-12-03 06:53:33.934201: step 980, loss = 5.06, batch loss = 4.66 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:38s remains)
INFO - root - 2017-12-03 06:53:35.817687: step 990, loss = 4.82, batch loss = 4.41 (43.7 examples/sec; 0.183 sec/batch; 16h:51m:46s remains)
INFO - root - 2017-12-03 06:53:37.712077: step 1000, loss = 5.78, batch loss = 5.37 (42.5 examples/sec; 0.188 sec/batch; 17h:20m:52s remains)
INFO - root - 2017-12-03 06:53:39.654188: step 1010, loss = 7.45, batch loss = 7.03 (42.6 examples/sec; 0.188 sec/batch; 17h:16m:56s remains)
INFO - root - 2017-12-03 06:53:41.564951: step 1020, loss = 12.36, batch loss = 11.94 (40.8 examples/sec; 0.196 sec/batch; 18h:02m:54s remains)
INFO - root - 2017-12-03 06:53:43.446585: step 1030, loss = 17.15, batch loss = 16.68 (41.4 examples/sec; 0.193 sec/batch; 17h:48m:14s remains)
INFO - root - 2017-12-03 06:53:45.314918: step 1040, loss = 58.94, batch loss = 57.73 (41.4 examples/sec; 0.193 sec/batch; 17h:48m:16s remains)
INFO - root - 2017-12-03 06:53:47.192091: step 1050, loss = 496.04, batch loss = 343.44 (41.4 examples/sec; 0.193 sec/batch; 17h:47m:39s remains)
INFO - root - 2017-12-03 06:53:49.076320: step 1060, loss = 1390.71, batch loss = 934.43 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:26s remains)
INFO - root - 2017-12-03 06:53:50.908801: step 1070, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:41m:46s remains)
INFO - root - 2017-12-03 06:53:52.738282: step 1080, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:53m:44s remains)
INFO - root - 2017-12-03 06:53:54.602456: step 1090, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:40m:25s remains)
INFO - root - 2017-12-03 06:53:56.428463: step 1100, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:50m:05s remains)
INFO - root - 2017-12-03 06:53:58.360991: step 1110, loss = nan, batch loss = 2.22 (44.5 examples/sec; 0.180 sec/batch; 16h:32m:54s remains)
INFO - root - 2017-12-03 06:54:00.177841: step 1120, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:13s remains)
INFO - root - 2017-12-03 06:54:02.005300: step 1130, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:54m:10s remains)
INFO - root - 2017-12-03 06:54:03.837676: step 1140, loss = nan, batch loss = 2.22 (42.8 examples/sec; 0.187 sec/batch; 17h:11m:06s remains)
INFO - root - 2017-12-03 06:54:05.662005: step 1150, loss = nan, batch loss = 2.22 (46.6 examples/sec; 0.172 sec/batch; 15h:48m:20s remains)
INFO - root - 2017-12-03 06:54:07.484197: step 1160, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 17h:01m:08s remains)
INFO - root - 2017-12-03 06:54:09.293690: step 1170, loss = nan, batch loss = 2.22 (45.4 examples/sec; 0.176 sec/batch; 16h:14m:02s remains)
INFO - root - 2017-12-03 06:54:11.088755: step 1180, loss = nan, batch loss = 2.22 (45.2 examples/sec; 0.177 sec/batch; 16h:17m:08s remains)
INFO - root - 2017-12-03 06:54:12.897318: step 1190, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.180 sec/batch; 16h:36m:18s remains)
INFO - root - 2017-12-03 06:54:14.715947: step 1200, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:40m:49s remains)
INFO - root - 2017-12-03 06:54:16.594671: step 1210, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:36m:39s remains)
INFO - root - 2017-12-03 06:54:18.395741: step 1220, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 17h:01m:03s remains)
INFO - root - 2017-12-03 06:54:20.217029: step 1230, loss = nan, batch loss = 2.22 (45.3 examples/sec; 0.177 sec/batch; 16h:15m:10s remains)
INFO - root - 2017-12-03 06:54:22.041816: step 1240, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:51m:03s remains)
INFO - root - 2017-12-03 06:54:23.852727: step 1250, loss = nan, batch loss = 2.22 (42.9 examples/sec; 0.186 sec/batch; 17h:08m:55s remains)
INFO - root - 2017-12-03 06:54:25.666271: step 1260, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:39s remains)
INFO - root - 2017-12-03 06:54:27.493895: step 1270, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:23s remains)
INFO - root - 2017-12-03 06:54:29.312596: step 1280, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:51m:51s remains)
INFO - root - 2017-12-03 06:54:31.145137: step 1290, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:50m:27s remains)
INFO - root - 2017-12-03 06:54:32.956054: step 1300, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:25s remains)
INFO - root - 2017-12-03 06:54:34.852612: step 1310, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:45m:45s remains)
INFO - root - 2017-12-03 06:54:36.678429: step 1320, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:57s remains)
INFO - root - 2017-12-03 06:54:38.496918: step 1330, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:47m:45s remains)
INFO - root - 2017-12-03 06:54:40.313396: step 1340, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:18s remains)
INFO - root - 2017-12-03 06:54:42.124592: step 1350, loss = nan, batch loss = 2.22 (44.9 examples/sec; 0.178 sec/batch; 16h:23m:58s remains)
INFO - root - 2017-12-03 06:54:43.959565: step 1360, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:44m:41s remains)
INFO - root - 2017-12-03 06:54:45.775001: step 1370, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:48s remains)
INFO - root - 2017-12-03 06:54:47.614164: step 1380, loss = nan, batch loss = 2.22 (42.0 examples/sec; 0.190 sec/batch; 17h:30m:43s remains)
INFO - root - 2017-12-03 06:54:49.420606: step 1390, loss = nan, batch loss = 2.22 (44.7 examples/sec; 0.179 sec/batch; 16h:27m:40s remains)
INFO - root - 2017-12-03 06:54:51.239541: step 1400, loss = nan, batch loss = 2.22 (45.1 examples/sec; 0.177 sec/batch; 16h:18m:56s remains)
INFO - root - 2017-12-03 06:54:53.128805: step 1410, loss = nan, batch loss = 2.22 (44.8 examples/sec; 0.179 sec/batch; 16h:25m:39s remains)
INFO - root - 2017-12-03 06:54:54.969212: step 1420, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:22s remains)
INFO - root - 2017-12-03 06:54:56.770460: step 1430, loss = nan, batch loss = 2.22 (44.6 examples/sec; 0.180 sec/batch; 16h:30m:34s remains)
INFO - root - 2017-12-03 06:54:58.565015: step 1440, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:34m:20s remains)
INFO - root - 2017-12-03 06:55:00.383362: step 1450, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.180 sec/batch; 16h:35m:40s remains)
INFO - root - 2017-12-03 06:55:02.192284: step 1460, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:40m:40s remains)
INFO - root - 2017-12-03 06:55:04.026822: step 1470, loss = nan, batch loss = 2.22 (44.7 examples/sec; 0.179 sec/batch; 16h:26m:28s remains)
INFO - root - 2017-12-03 06:55:05.827069: step 1480, loss = nan, batch loss = 2.22 (44.6 examples/sec; 0.180 sec/batch; 16h:30m:41s remains)
INFO - root - 2017-12-03 06:55:07.632035: step 1490, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:36m:42s remains)
INFO - root - 2017-12-03 06:55:09.445134: step 1500, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.182 sec/batch; 16h:46m:33s remains)
INFO - root - 2017-12-03 06:55:11.362822: step 1510, loss = nan, batch loss = 2.22 (43.0 examples/sec; 0.186 sec/batch; 17h:07m:12s remains)
INFO - root - 2017-12-03 06:55:13.191275: step 1520, loss = nan, batch loss = 2.22 (43.0 examples/sec; 0.186 sec/batch; 17h:06m:15s remains)
INFO - root - 2017-12-03 06:55:15.025408: step 1530, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:53m:06s remains)
INFO - root - 2017-12-03 06:55:16.840750: step 1540, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:02s remains)
INFO - root - 2017-12-03 06:55:18.672414: step 1550, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:04s remains)
INFO - root - 2017-12-03 06:55:20.482928: step 1560, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.182 sec/batch; 16h:41m:26s remains)
