INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "110"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 09:32:10.212182: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:32:10.212384: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:32:10.212470: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:32:10.212526: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:32:10.212577: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:32:12.746907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-06 09:32:12.747104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 09:32:12.747172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 09:32:12.747236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/def1/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/def1/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/def1/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/def1/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-10000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-10000
float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/biases:0' shape=(72,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 09:32:19.502073: step 0, loss = 2.06, batch loss = 2.00 (1.9 examples/sec; 4.228 sec/batch; 390h:30m:22s remains)
2017-12-06 09:32:19.973749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3318892 -4.3312826 -4.3332443 -4.3304062 -4.3215461 -4.3098941 -4.2928324 -4.26872 -4.2433066 -4.232192 -4.240541 -4.2591524 -4.2854357 -4.3105545 -4.3252025][-4.3278079 -4.3281016 -4.3287868 -4.3200941 -4.3015537 -4.2776704 -4.2447653 -4.2030759 -4.1655912 -4.1543021 -4.1736956 -4.2073665 -4.250659 -4.2892437 -4.3135252][-4.3231854 -4.3233519 -4.3214641 -4.305532 -4.2761045 -4.2374392 -4.1833172 -4.1148939 -4.0546627 -4.0413351 -4.0767345 -4.1315451 -4.200675 -4.261776 -4.3007293][-4.3120327 -4.3120332 -4.3099589 -4.29084 -4.2544208 -4.200592 -4.1234593 -4.0215054 -3.9306355 -3.9203081 -3.9795842 -4.0566635 -4.1525044 -4.2355647 -4.2898235][-4.2951059 -4.2961769 -4.2955713 -4.2732997 -4.2293682 -4.1578941 -4.0484614 -3.9015408 -3.7728243 -3.7819438 -3.8805225 -3.9872804 -4.1089745 -4.211833 -4.27919][-4.2773843 -4.2795248 -4.2789159 -4.2503848 -4.1970205 -4.108429 -3.9686267 -3.776643 -3.6275415 -3.6860456 -3.8301115 -3.9610367 -4.0936356 -4.20293 -4.2742305][-4.26244 -4.2639089 -4.2617679 -4.2290111 -4.1701407 -4.0782113 -3.9330826 -3.7407818 -3.6261454 -3.7288609 -3.8779464 -4.0005593 -4.1174593 -4.2154212 -4.2808771][-4.2536755 -4.2516766 -4.2486849 -4.221138 -4.1737785 -4.0994425 -3.9845827 -3.8465471 -3.7957613 -3.8920851 -3.9995515 -4.0836344 -4.1642785 -4.2394195 -4.2927585][-4.2435212 -4.2367172 -4.2342715 -4.2168531 -4.1861553 -4.1338835 -4.0588098 -3.9780173 -3.9629917 -4.0318985 -4.1003633 -4.1479445 -4.1981764 -4.2556753 -4.30059][-4.2315617 -4.2235246 -4.2229004 -4.2140532 -4.1947985 -4.1627731 -4.1183 -4.0730095 -4.0708532 -4.1167722 -4.1579189 -4.1814351 -4.2154388 -4.2641244 -4.3063283][-4.2260337 -4.220809 -4.2206774 -4.215766 -4.202332 -4.1850891 -4.1625085 -4.138236 -4.1413021 -4.1683712 -4.1891665 -4.1993566 -4.2281094 -4.2744966 -4.3148808][-4.2308855 -4.2309055 -4.2314286 -4.2289252 -4.2214837 -4.2172976 -4.2108226 -4.1965952 -4.1962905 -4.2046866 -4.2082014 -4.2101932 -4.2358923 -4.2825737 -4.3213644][-4.2485652 -4.2542057 -4.2583313 -4.2605634 -4.2593293 -4.2612939 -4.258729 -4.2423568 -4.230999 -4.2206292 -4.2082052 -4.2052164 -4.23191 -4.281105 -4.3211913][-4.2602124 -4.2712297 -4.28131 -4.2889395 -4.2896423 -4.290957 -4.2822289 -4.2555771 -4.2307096 -4.2096434 -4.1932344 -4.1931763 -4.2240615 -4.2760696 -4.3178291][-4.2568903 -4.2730441 -4.2876763 -4.2981043 -4.3005428 -4.2984715 -4.2816796 -4.245338 -4.2102 -4.186738 -4.1754136 -4.1813288 -4.2163706 -4.2700953 -4.3124595]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 09:32:22.943292: step 10, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 21h:04m:01s remains)
INFO - root - 2017-12-06 09:32:25.393000: step 20, loss = 2.06, batch loss = 2.01 (27.7 examples/sec; 0.289 sec/batch; 26h:43m:12s remains)
INFO - root - 2017-12-06 09:32:27.704772: step 30, loss = 2.07, batch loss = 2.01 (30.4 examples/sec; 0.263 sec/batch; 24h:18m:40s remains)
INFO - root - 2017-12-06 09:32:29.947437: step 40, loss = 2.09, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:59s remains)
INFO - root - 2017-12-06 09:32:32.153440: step 50, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:28m:35s remains)
INFO - root - 2017-12-06 09:32:34.490524: step 60, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:55s remains)
INFO - root - 2017-12-06 09:32:36.956130: step 70, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:41s remains)
INFO - root - 2017-12-06 09:32:39.166354: step 80, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:14s remains)
INFO - root - 2017-12-06 09:32:41.357206: step 90, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-06 09:32:43.735095: step 100, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 39h:07m:56s remains)
2017-12-06 09:32:44.127438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493544 -4.2234426 -4.2055187 -4.1949153 -4.1914988 -4.1966572 -4.2100697 -4.22001 -4.2259288 -4.2319856 -4.2326689 -4.2306752 -4.2268796 -4.2072587 -4.1840553][-4.25462 -4.2237 -4.1952 -4.1693945 -4.1508856 -4.148561 -4.1637945 -4.1800022 -4.1886182 -4.1949272 -4.1982512 -4.2016439 -4.2017336 -4.185925 -4.1678672][-4.2606282 -4.2240191 -4.1861906 -4.1493464 -4.121758 -4.1150103 -4.1310973 -4.1522274 -4.1648884 -4.1750088 -4.1838546 -4.1882305 -4.1877322 -4.1745014 -4.159349][-4.2635341 -4.2240767 -4.1838727 -4.1441207 -4.1157575 -4.1067286 -4.1197023 -4.1423006 -4.1554809 -4.1701517 -4.18526 -4.1887603 -4.1859217 -4.1746807 -4.1602845][-4.2621551 -4.22186 -4.1820807 -4.1436553 -4.1161518 -4.1037121 -4.1101856 -4.1279922 -4.1375041 -4.1532683 -4.1742954 -4.179409 -4.1773 -4.17065 -4.1596513][-4.2568936 -4.216424 -4.1777778 -4.1409168 -4.1122851 -4.0929327 -4.0904546 -4.0973306 -4.0959878 -4.1076603 -4.1350627 -4.1487021 -4.1536059 -4.154654 -4.1513658][-4.2507992 -4.2094 -4.1702709 -4.1313562 -4.1003404 -4.076962 -4.07026 -4.0679832 -4.0539794 -4.0565557 -4.0886674 -4.1124263 -4.1260781 -4.1350546 -4.1407895][-4.2484169 -4.20672 -4.1671157 -4.1265955 -4.0946579 -4.0731311 -4.0681624 -4.0616803 -4.0385265 -4.0351253 -4.0667334 -4.0916576 -4.1053395 -4.1198778 -4.1329865][-4.24989 -4.210062 -4.1713791 -4.1301756 -4.096478 -4.0785618 -4.0778193 -4.0715747 -4.0503736 -4.0496006 -4.0760961 -4.0919724 -4.0970254 -4.1092815 -4.1230516][-4.2568278 -4.2185807 -4.1807318 -4.1394553 -4.1047206 -4.0904818 -4.0908985 -4.084938 -4.0703344 -4.0783334 -4.1014476 -4.1080022 -4.1019869 -4.1069341 -4.117692][-4.2681575 -4.2323947 -4.1961503 -4.1580672 -4.1259675 -4.1163406 -4.1181097 -4.1122541 -4.1018114 -4.1142917 -4.1340704 -4.1357431 -4.1239805 -4.1238294 -4.1293287][-4.2807803 -4.24787 -4.2133708 -4.1784892 -4.1515107 -4.147512 -4.152473 -4.1504779 -4.1449924 -4.1559267 -4.1700878 -4.1705642 -4.1594038 -4.1580811 -4.1580811][-4.2949963 -4.2656469 -4.2343307 -4.2032261 -4.1794772 -4.1765 -4.1840606 -4.1895919 -4.1929269 -4.2020879 -4.2093177 -4.207808 -4.1997228 -4.19799 -4.1959267][-4.3123026 -4.290483 -4.267611 -4.2440414 -4.2243581 -4.219255 -4.2254605 -4.2354875 -4.2444553 -4.2517991 -4.25477 -4.2513313 -4.2447152 -4.2423453 -4.2412505][-4.3278427 -4.3144641 -4.3008709 -4.2869244 -4.274734 -4.2705736 -4.2745161 -4.2826562 -4.289896 -4.2936921 -4.2939038 -4.29066 -4.2862363 -4.2848911 -4.2858477]]...]
INFO - root - 2017-12-06 09:32:46.500516: step 110, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:37s remains)
INFO - root - 2017-12-06 09:32:48.703329: step 120, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:58s remains)
INFO - root - 2017-12-06 09:32:50.862229: step 130, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:09s remains)
INFO - root - 2017-12-06 09:32:53.146221: step 140, loss = 2.08, batch loss = 2.02 (27.0 examples/sec; 0.297 sec/batch; 27h:24m:00s remains)
INFO - root - 2017-12-06 09:32:55.424304: step 150, loss = 2.12, batch loss = 2.06 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:27s remains)
INFO - root - 2017-12-06 09:32:57.634690: step 160, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 21h:38m:02s remains)
INFO - root - 2017-12-06 09:32:59.913856: step 170, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:31s remains)
INFO - root - 2017-12-06 09:33:02.100975: step 180, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:24m:11s remains)
INFO - root - 2017-12-06 09:33:04.382056: step 190, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:24s remains)
INFO - root - 2017-12-06 09:33:06.577869: step 200, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.233 sec/batch; 21h:28m:31s remains)
2017-12-06 09:33:06.957583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2721519 -4.278326 -4.27602 -4.2711105 -4.2647572 -4.257597 -4.2514529 -4.2511816 -4.2495718 -4.2461166 -4.2484078 -4.2552562 -4.2575364 -4.2591128 -4.2619791][-4.2668824 -4.26673 -4.2641611 -4.2563658 -4.2455926 -4.2345343 -4.2278328 -4.2298326 -4.2327552 -4.2328534 -4.2354846 -4.2411327 -4.243763 -4.2450967 -4.2497263][-4.2493534 -4.2434731 -4.2471013 -4.2488704 -4.2379236 -4.2246 -4.2174535 -4.2204542 -4.2295275 -4.2412667 -4.247848 -4.2444472 -4.2402716 -4.2377706 -4.241282][-4.2292161 -4.2177224 -4.2259197 -4.238471 -4.2341485 -4.220365 -4.20597 -4.2015376 -4.2165742 -4.2455354 -4.2615919 -4.2523289 -4.237442 -4.2277374 -4.2269773][-4.217165 -4.2008672 -4.2075005 -4.2264161 -4.2271085 -4.2131481 -4.1934958 -4.1767159 -4.1862478 -4.2328687 -4.2630787 -4.2527447 -4.229476 -4.2105761 -4.204793][-4.2104244 -4.1867838 -4.1880407 -4.20925 -4.217298 -4.1972852 -4.1595612 -4.1127739 -4.1128807 -4.1863317 -4.2366214 -4.2369294 -4.2115617 -4.1913519 -4.1858506][-4.2040377 -4.1723523 -4.1636992 -4.1882911 -4.2005839 -4.1716232 -4.098002 -3.9969015 -3.9789717 -4.0942097 -4.1821609 -4.205009 -4.1845374 -4.1600304 -4.16061][-4.1896152 -4.1486616 -4.1312561 -4.1597972 -4.1819916 -4.1519284 -4.0481491 -3.8894873 -3.8465543 -4.0013585 -4.1295428 -4.1793017 -4.1636705 -4.129261 -4.1377878][-4.1713958 -4.1404877 -4.1291075 -4.1633372 -4.192688 -4.1752105 -4.0907841 -3.9588487 -3.9201264 -4.037549 -4.1495709 -4.2036514 -4.194994 -4.1607547 -4.1600738][-4.1703768 -4.1572781 -4.1580477 -4.1933103 -4.226645 -4.2246456 -4.1742949 -4.0931969 -4.0681586 -4.1318369 -4.2061219 -4.2482224 -4.2450948 -4.2177029 -4.2075934][-4.1865377 -4.1843772 -4.1902041 -4.2194123 -4.24717 -4.2509174 -4.2189693 -4.1676178 -4.1503158 -4.1851444 -4.2353635 -4.2688713 -4.2664909 -4.2462559 -4.2320604][-4.1953344 -4.1921439 -4.1998377 -4.2246771 -4.2463384 -4.2561097 -4.2346296 -4.1975994 -4.1814547 -4.1988416 -4.2320652 -4.259335 -4.258677 -4.2480807 -4.2358][-4.2023025 -4.1969056 -4.2061076 -4.22708 -4.2419052 -4.2490788 -4.2351236 -4.2088652 -4.1997733 -4.2114096 -4.2340369 -4.2563329 -4.2575541 -4.2516847 -4.2444153][-4.2143135 -4.2112889 -4.2242146 -4.2450757 -4.254797 -4.2581959 -4.252635 -4.2357426 -4.2313285 -4.2368455 -4.2506824 -4.2688055 -4.2711825 -4.2687993 -4.2624726][-4.2277713 -4.2244668 -4.2373896 -4.256382 -4.2644548 -4.2683191 -4.2682796 -4.2588263 -4.2551003 -4.2557392 -4.2618637 -4.2731805 -4.2765923 -4.2750678 -4.26733]]...]
INFO - root - 2017-12-06 09:33:09.193011: step 210, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:02s remains)
INFO - root - 2017-12-06 09:33:11.403979: step 220, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:10s remains)
INFO - root - 2017-12-06 09:33:13.595139: step 230, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:14s remains)
INFO - root - 2017-12-06 09:33:15.786654: step 240, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:41s remains)
INFO - root - 2017-12-06 09:33:18.003083: step 250, loss = 2.08, batch loss = 2.03 (33.4 examples/sec; 0.239 sec/batch; 22h:04m:24s remains)
INFO - root - 2017-12-06 09:33:20.185977: step 260, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 21h:22m:58s remains)
INFO - root - 2017-12-06 09:33:22.354167: step 270, loss = 2.10, batch loss = 2.04 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:50s remains)
INFO - root - 2017-12-06 09:33:24.563869: step 280, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:23s remains)
INFO - root - 2017-12-06 09:33:26.783353: step 290, loss = 2.09, batch loss = 2.03 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:02s remains)
INFO - root - 2017-12-06 09:33:29.010188: step 300, loss = 2.09, batch loss = 2.04 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:13s remains)
2017-12-06 09:33:29.308443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1225405 -4.1150379 -4.1278667 -4.1562519 -4.1922073 -4.2194438 -4.2287669 -4.2225137 -4.205431 -4.1799035 -4.1690807 -4.1752787 -4.178267 -4.1773176 -4.1823473][-4.1957588 -4.1886134 -4.1811185 -4.1829925 -4.2011909 -4.2282643 -4.2468476 -4.2486377 -4.24236 -4.2309885 -4.2264075 -4.2268939 -4.21819 -4.2031422 -4.1968813][-4.2606549 -4.2534895 -4.2305784 -4.2071943 -4.2068691 -4.2318964 -4.2574558 -4.2684865 -4.2741332 -4.274188 -4.2678709 -4.2579751 -4.2368507 -4.2059889 -4.1890435][-4.2877312 -4.2791567 -4.246623 -4.2073221 -4.1913705 -4.2108245 -4.2398682 -4.2607121 -4.2801995 -4.2887068 -4.2763667 -4.2517357 -4.2187166 -4.1783829 -4.158607][-4.2761359 -4.2666941 -4.2313533 -4.1894183 -4.1671767 -4.1786418 -4.2035465 -4.23101 -4.2596169 -4.271122 -4.252665 -4.2148042 -4.1706018 -4.1275158 -4.1139197][-4.2382669 -4.2315478 -4.198956 -4.1664872 -4.1520844 -4.1577411 -4.1732306 -4.1972203 -4.22474 -4.2319527 -4.2087007 -4.1627727 -4.1141047 -4.0767088 -4.0750861][-4.207418 -4.2001858 -4.1736627 -4.1551418 -4.1492062 -4.1507726 -4.15864 -4.175632 -4.1938992 -4.1912656 -4.1614265 -4.1118832 -4.0603213 -4.0294967 -4.0424886][-4.1899753 -4.1812453 -4.1617918 -4.1519918 -4.1501818 -4.1481781 -4.1479149 -4.1576996 -4.165288 -4.1552186 -4.1251974 -4.0797157 -4.0298476 -4.0035663 -4.0300088][-4.1846728 -4.1811657 -4.178247 -4.1777377 -4.1794014 -4.1733613 -4.1645212 -4.1636429 -4.1622734 -4.1513028 -4.1281109 -4.0939708 -4.0541482 -4.0356374 -4.0642395][-4.1845241 -4.1938419 -4.2103486 -4.2192445 -4.2245293 -4.2164445 -4.2008877 -4.1908045 -4.1846104 -4.1744289 -4.1600456 -4.1454916 -4.1228037 -4.110919 -4.1326771][-4.1715536 -4.1949968 -4.2256961 -4.2458382 -4.26015 -4.2536564 -4.2333012 -4.2173896 -4.2037783 -4.1916409 -4.1878724 -4.1945534 -4.1893277 -4.1860542 -4.2015867][-4.14899 -4.1826339 -4.2209454 -4.2516885 -4.2764039 -4.275033 -4.2564607 -4.2412128 -4.2231956 -4.2049413 -4.2059565 -4.2246323 -4.2315297 -4.2350879 -4.2488484][-4.14361 -4.1773963 -4.2113709 -4.2430086 -4.2712321 -4.2761807 -4.2689075 -4.2606554 -4.2410984 -4.2207747 -4.2227597 -4.2432213 -4.2513475 -4.2541356 -4.263885][-4.1765623 -4.1958094 -4.2142515 -4.235724 -4.2597136 -4.2683549 -4.2697296 -4.2650657 -4.2452688 -4.228229 -4.2315078 -4.2487335 -4.2547545 -4.2528529 -4.2550349][-4.2260685 -4.229291 -4.2337217 -4.2431288 -4.2585192 -4.2675991 -4.2744946 -4.2728319 -4.2585468 -4.2482209 -4.2511716 -4.2598367 -4.2604871 -4.2508473 -4.2409763]]...]
INFO - root - 2017-12-06 09:33:31.509353: step 310, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:36m:14s remains)
INFO - root - 2017-12-06 09:33:33.734071: step 320, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:24s remains)
INFO - root - 2017-12-06 09:33:35.939679: step 330, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:31s remains)
INFO - root - 2017-12-06 09:33:38.189585: step 340, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:17s remains)
INFO - root - 2017-12-06 09:33:40.340119: step 350, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:38s remains)
INFO - root - 2017-12-06 09:33:42.506837: step 360, loss = 2.10, batch loss = 2.04 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:38s remains)
INFO - root - 2017-12-06 09:33:44.685394: step 370, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:04s remains)
INFO - root - 2017-12-06 09:33:46.854941: step 380, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:35s remains)
INFO - root - 2017-12-06 09:33:48.994601: step 390, loss = 2.06, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:24s remains)
INFO - root - 2017-12-06 09:33:51.173989: step 400, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:45s remains)
2017-12-06 09:33:51.538262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2759356 -4.2721405 -4.2648563 -4.2576985 -4.2514863 -4.248054 -4.2507052 -4.259274 -4.2702689 -4.2747955 -4.2720118 -4.2660279 -4.2563114 -4.246819 -4.2390537][-4.2832103 -4.2871842 -4.2835665 -4.2750511 -4.2663083 -4.2601457 -4.2629704 -4.2759752 -4.2940545 -4.3060746 -4.3094468 -4.3031259 -4.2843165 -4.2583747 -4.2323437][-4.2797241 -4.2863388 -4.283246 -4.269721 -4.2577085 -4.2494965 -4.24924 -4.2630849 -4.2870045 -4.310462 -4.3286586 -4.3331347 -4.3159647 -4.2804251 -4.2352114][-4.2763615 -4.2788849 -4.2695332 -4.2458048 -4.2276249 -4.2162089 -4.2077146 -4.2146697 -4.2388663 -4.2729697 -4.3074594 -4.3273039 -4.3225579 -4.292459 -4.2426744][-4.2804284 -4.2725959 -4.2506986 -4.213305 -4.1857796 -4.1686673 -4.1494408 -4.1426754 -4.16016 -4.2022796 -4.2546754 -4.2941017 -4.3079448 -4.2913294 -4.2456946][-4.2794337 -4.2626319 -4.2324286 -4.1852188 -4.1473608 -4.1218491 -4.0891476 -4.0625443 -4.0685 -4.1181183 -4.1908965 -4.2522011 -4.2875953 -4.2911811 -4.2587361][-4.2799807 -4.26061 -4.2279487 -4.176394 -4.125937 -4.084 -4.0279126 -3.9687426 -3.954531 -4.0139389 -4.1099911 -4.1920323 -4.2467303 -4.27399 -4.2691269][-4.28536 -4.2679162 -4.2400923 -4.1941433 -4.1397276 -4.0870008 -4.014739 -3.9239302 -3.8795223 -3.9357183 -4.0399466 -4.1325326 -4.1992426 -4.2461405 -4.2691555][-4.2879639 -4.2758021 -4.2596436 -4.2289486 -4.1844621 -4.1389542 -4.0761786 -3.9918871 -3.938081 -3.964479 -4.0361338 -4.10685 -4.1670971 -4.220377 -4.2612753][-4.2929707 -4.2873235 -4.2822876 -4.267797 -4.2412615 -4.2123704 -4.1690121 -4.1072726 -4.062336 -4.0647125 -4.0968432 -4.13516 -4.1743655 -4.21734 -4.2579074][-4.29653 -4.29696 -4.2988887 -4.2968278 -4.2871265 -4.2743144 -4.2521424 -4.2163877 -4.1856437 -4.1765566 -4.1835909 -4.2001948 -4.2236161 -4.2505569 -4.2761831][-4.3003416 -4.3054948 -4.3095527 -4.311635 -4.3112826 -4.3102646 -4.3057318 -4.2927713 -4.2771349 -4.2683029 -4.2649555 -4.268249 -4.2799582 -4.2923136 -4.3026552][-4.3067374 -4.3144341 -4.3191781 -4.3201408 -4.3210583 -4.3245559 -4.3296537 -4.331521 -4.3285413 -4.3242245 -4.3189154 -4.3165317 -4.3181043 -4.3188715 -4.3185439][-4.3147182 -4.3200631 -4.3243723 -4.3244495 -4.3228273 -4.3242755 -4.3292866 -4.3346343 -4.3372808 -4.3375134 -4.3364124 -4.3339772 -4.3315177 -4.3274484 -4.3239031][-4.3199964 -4.3214006 -4.32379 -4.3237648 -4.3214192 -4.3210688 -4.3230958 -4.3254061 -4.3264365 -4.3274355 -4.3289132 -4.329555 -4.329289 -4.326827 -4.3237448]]...]
INFO - root - 2017-12-06 09:33:53.734399: step 410, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:37s remains)
INFO - root - 2017-12-06 09:33:55.940434: step 420, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:55s remains)
INFO - root - 2017-12-06 09:33:58.138772: step 430, loss = 2.10, batch loss = 2.04 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:10s remains)
INFO - root - 2017-12-06 09:34:00.360920: step 440, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:22s remains)
INFO - root - 2017-12-06 09:34:02.564213: step 450, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:08s remains)
INFO - root - 2017-12-06 09:34:04.766320: step 460, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:12s remains)
INFO - root - 2017-12-06 09:34:06.971348: step 470, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:20m:52s remains)
INFO - root - 2017-12-06 09:34:09.168520: step 480, loss = 2.08, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-06 09:34:11.400372: step 490, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-06 09:34:13.624959: step 500, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:21s remains)
2017-12-06 09:34:14.004667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2278481 -4.2305984 -4.2353539 -4.2275953 -4.2127376 -4.2044048 -4.2014227 -4.2004213 -4.2015343 -4.2012992 -4.1978488 -4.1867156 -4.1680264 -4.1461115 -4.127965][-4.18959 -4.1919065 -4.1955047 -4.1856012 -4.1698403 -4.1618958 -4.1626663 -4.1721787 -4.1878738 -4.1935434 -4.18645 -4.1709867 -4.1489091 -4.1254134 -4.1062641][-4.1583672 -4.15927 -4.1585684 -4.1455026 -4.131156 -4.1295147 -4.1422734 -4.1671543 -4.1913996 -4.1948042 -4.178494 -4.1572447 -4.13628 -4.1163082 -4.0993781][-4.1419806 -4.1378903 -4.1256695 -4.1053052 -4.0925503 -4.1007776 -4.12845 -4.1679478 -4.1979914 -4.1941719 -4.1667757 -4.1389828 -4.1163673 -4.1008253 -4.0944109][-4.1369786 -4.1267586 -4.1000757 -4.0721731 -4.0591793 -4.0714822 -4.1020503 -4.1457353 -4.1761007 -4.1665492 -4.1354694 -4.1089993 -4.0924449 -4.0911646 -4.1037521][-4.1395526 -4.1246433 -4.08898 -4.0526309 -4.02872 -4.02929 -4.0472441 -4.0822496 -4.1114731 -4.0993667 -4.072052 -4.0605311 -4.0624385 -4.0816016 -4.11201][-4.1510024 -4.1395431 -4.1041718 -4.0530467 -4.002027 -3.9646592 -3.9480417 -3.9668746 -4.0056314 -4.0097709 -3.9962435 -4.0090728 -4.031538 -4.064508 -4.1057715][-4.1547518 -4.1526723 -4.1261296 -4.071002 -3.9982154 -3.9138036 -3.8417079 -3.8378444 -3.9010129 -3.9418817 -3.9573917 -3.992837 -4.0246787 -4.0558414 -4.0940089][-4.1425691 -4.138772 -4.1185884 -4.0757484 -4.0123105 -3.9208865 -3.8254371 -3.8064299 -3.8803539 -3.9402852 -3.9771426 -4.0224996 -4.0501528 -4.0667696 -4.0914321][-4.1278653 -4.1162472 -4.0993543 -4.0760345 -4.0413694 -3.9816601 -3.9153295 -3.9042 -3.9514921 -3.9917166 -4.0264826 -4.0714455 -4.0940056 -4.0960441 -4.1007857][-4.116991 -4.1113186 -4.1068621 -4.1022091 -4.0891032 -4.0567818 -4.0217471 -4.0177875 -4.0367503 -4.0470214 -4.0650826 -4.0998306 -4.1184354 -4.1138191 -4.106751][-4.1259456 -4.1356082 -4.1419234 -4.1425457 -4.1337233 -4.1126456 -4.0910187 -4.0867352 -4.0885105 -4.0755563 -4.0729928 -4.0947108 -4.1137505 -4.1143284 -4.1113729][-4.1548448 -4.1696405 -4.1780257 -4.1740441 -4.1621032 -4.1430988 -4.1235375 -4.1131096 -4.1065283 -4.0870538 -4.0759048 -4.0884233 -4.109242 -4.1221519 -4.1340389][-4.1822977 -4.1926413 -4.19706 -4.1880045 -4.1743374 -4.1581926 -4.141211 -4.1312432 -4.1253948 -4.1119084 -4.1049628 -4.1130738 -4.1294556 -4.1477957 -4.1677856][-4.1930308 -4.2011104 -4.2037539 -4.193934 -4.1810336 -4.1674857 -4.1541085 -4.1515441 -4.1542897 -4.152173 -4.1496377 -4.1481333 -4.1529508 -4.1669731 -4.1840258]]...]
INFO - root - 2017-12-06 09:34:16.175935: step 510, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:21s remains)
INFO - root - 2017-12-06 09:34:18.362628: step 520, loss = 2.10, batch loss = 2.04 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:56s remains)
INFO - root - 2017-12-06 09:34:20.549931: step 530, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:57s remains)
INFO - root - 2017-12-06 09:34:22.732872: step 540, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.214 sec/batch; 19h:41m:23s remains)
INFO - root - 2017-12-06 09:34:24.906173: step 550, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:18s remains)
INFO - root - 2017-12-06 09:34:27.104485: step 560, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 21h:18m:18s remains)
INFO - root - 2017-12-06 09:34:29.267841: step 570, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:59s remains)
INFO - root - 2017-12-06 09:34:31.473168: step 580, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:10s remains)
INFO - root - 2017-12-06 09:34:33.633448: step 590, loss = 2.09, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:00s remains)
INFO - root - 2017-12-06 09:34:35.795250: step 600, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:22s remains)
2017-12-06 09:34:36.149132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2594671 -4.2622924 -4.2510529 -4.2346482 -4.2262716 -4.2230415 -4.2453618 -4.2795105 -4.3015704 -4.3067827 -4.3040462 -4.2923055 -4.2691917 -4.2565227 -4.2614446][-4.2592254 -4.2654543 -4.2646351 -4.2538619 -4.2473526 -4.2450094 -4.2666841 -4.2961349 -4.3084517 -4.306838 -4.298604 -4.2873383 -4.2681732 -4.2617021 -4.2728243][-4.2354808 -4.2393341 -4.2465372 -4.2450624 -4.2397361 -4.2361693 -4.2554255 -4.2778749 -4.2803135 -4.2762122 -4.2743063 -4.2700486 -4.255 -4.2513967 -4.265409][-4.2034807 -4.2057843 -4.2150817 -4.2134242 -4.1986237 -4.1858068 -4.1983495 -4.2134194 -4.2137022 -4.2180667 -4.2314639 -4.2382846 -4.229342 -4.229012 -4.2455335][-4.1864395 -4.191165 -4.1930985 -4.1797419 -4.1516361 -4.1276388 -4.13418 -4.1522965 -4.1648307 -4.1837959 -4.2045708 -4.2086639 -4.1934934 -4.1866226 -4.1973028][-4.1774645 -4.179225 -4.1649814 -4.1283245 -4.0853844 -4.0522823 -4.0592227 -4.0948038 -4.1291909 -4.1569681 -4.1755443 -4.1716824 -4.1425543 -4.1225324 -4.1253934][-4.1714211 -4.1682324 -4.1379147 -4.0819821 -4.0239472 -3.9858174 -3.9949467 -4.0459437 -4.0926676 -4.115047 -4.1223278 -4.1125674 -4.0737066 -4.0508533 -4.062993][-4.1666589 -4.1635051 -4.1310582 -4.07187 -4.0147419 -3.9778285 -3.9832816 -4.0262375 -4.0604773 -4.0619397 -4.0524349 -4.0396547 -4.0036168 -3.9958475 -4.0295162][-4.1776462 -4.1753235 -4.1504283 -4.1039829 -4.0582232 -4.0266137 -4.0220881 -4.0452929 -4.0579486 -4.0443172 -4.0236087 -4.0068436 -3.9757357 -3.9817679 -4.0300274][-4.2045012 -4.201201 -4.1850729 -4.1550646 -4.1242723 -4.102037 -4.091742 -4.0984507 -4.0982666 -4.0779653 -4.0552764 -4.0361304 -4.0080223 -4.0157309 -4.0623608][-4.2229342 -4.2147841 -4.2041826 -4.19092 -4.1794147 -4.1738281 -4.1694961 -4.1696277 -4.16397 -4.1435971 -4.1215091 -4.1017456 -4.0787411 -4.082469 -4.11838][-4.2267656 -4.2128286 -4.2020092 -4.1992555 -4.2045903 -4.2177849 -4.2265177 -4.2299895 -4.2263079 -4.2111511 -4.1936526 -4.1779413 -4.1611638 -4.160696 -4.1816077][-4.2259893 -4.209167 -4.1974483 -4.2021866 -4.2194147 -4.2426276 -4.2579985 -4.2653613 -4.2648726 -4.2553048 -4.2441621 -4.2336125 -4.2204123 -4.2165513 -4.227108][-4.22195 -4.2036157 -4.1923518 -4.2027216 -4.2259684 -4.2505655 -4.2668872 -4.275948 -4.277288 -4.2712731 -4.2643819 -4.2576361 -4.2458239 -4.239325 -4.2448225][-4.2316155 -4.2121634 -4.2013693 -4.2132335 -4.2349868 -4.2559967 -4.2694254 -4.2752557 -4.2732835 -4.2654119 -4.2574396 -4.2527456 -4.2456536 -4.242878 -4.250967]]...]
INFO - root - 2017-12-06 09:34:38.322343: step 610, loss = 2.10, batch loss = 2.04 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:30s remains)
INFO - root - 2017-12-06 09:34:40.545893: step 620, loss = 2.06, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:39s remains)
INFO - root - 2017-12-06 09:34:42.750171: step 630, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:03s remains)
INFO - root - 2017-12-06 09:34:44.975531: step 640, loss = 2.09, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:50s remains)
INFO - root - 2017-12-06 09:34:47.194049: step 650, loss = 2.10, batch loss = 2.04 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:05s remains)
INFO - root - 2017-12-06 09:34:49.367626: step 660, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:35s remains)
INFO - root - 2017-12-06 09:34:51.505088: step 670, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:31s remains)
INFO - root - 2017-12-06 09:34:53.677865: step 680, loss = 2.10, batch loss = 2.04 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:30s remains)
INFO - root - 2017-12-06 09:34:55.843989: step 690, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-06 09:34:58.044074: step 700, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:03s remains)
2017-12-06 09:34:58.443094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2213311 -4.2156487 -4.2107396 -4.20606 -4.2028546 -4.1999769 -4.1961188 -4.1912918 -4.1889868 -4.1897988 -4.1918097 -4.189549 -4.1790147 -4.1650014 -4.1495805][-4.2519226 -4.2470136 -4.2426004 -4.2382889 -4.2354927 -4.23204 -4.226491 -4.2185893 -4.2129855 -4.2116594 -4.2113743 -4.2075429 -4.1971087 -4.1858597 -4.1717076][-4.2636151 -4.2582517 -4.2533507 -4.2506518 -4.2504554 -4.2496796 -4.244803 -4.2371902 -4.2324452 -4.2335529 -4.234921 -4.2331262 -4.2244229 -4.2148213 -4.200079][-4.2533035 -4.2441258 -4.2362442 -4.2355332 -4.2394214 -4.2426476 -4.2389793 -4.2322288 -4.231987 -4.2413912 -4.2500281 -4.253778 -4.2495613 -4.2405529 -4.2233114][-4.2262554 -4.2089992 -4.1963887 -4.1978717 -4.2064376 -4.2119412 -4.2062263 -4.1978474 -4.2034707 -4.2244825 -4.2461014 -4.260304 -4.2644081 -4.2585955 -4.2411814][-4.1975336 -4.1738482 -4.1575384 -4.1596742 -4.1691413 -4.1709447 -4.1566381 -4.1420226 -4.1504712 -4.1800046 -4.2138395 -4.2412581 -4.2575574 -4.2598262 -4.2468681][-4.186183 -4.1645985 -4.1498475 -4.1510139 -4.1561232 -4.1479497 -4.1220231 -4.0978742 -4.1038933 -4.1364689 -4.1767511 -4.2129278 -4.237586 -4.2476459 -4.2410231][-4.1872792 -4.1745367 -4.1676545 -4.1717629 -4.1756644 -4.1642332 -4.1363187 -4.108942 -4.1084571 -4.1329479 -4.1669035 -4.19903 -4.2218447 -4.232749 -4.2290549][-4.1986771 -4.1926427 -4.1912084 -4.1976061 -4.2022352 -4.193758 -4.1730523 -4.1534982 -4.1534667 -4.1702485 -4.1938882 -4.2160392 -4.2293386 -4.2325044 -4.224184][-4.22003 -4.2151465 -4.21152 -4.2125664 -4.2127609 -4.2051888 -4.19203 -4.1838975 -4.1899996 -4.2053685 -4.2244258 -4.2417994 -4.2491055 -4.2447381 -4.2296319][-4.24669 -4.2390532 -4.2293277 -4.2219706 -4.2138033 -4.2012215 -4.1884036 -4.1865973 -4.1973867 -4.2142539 -4.2324724 -4.2499275 -4.2577925 -4.2524829 -4.2349744][-4.2617059 -4.2512436 -4.2382646 -4.22825 -4.2190466 -4.2067847 -4.1943994 -4.1926513 -4.2020831 -4.2173214 -4.2333865 -4.2491784 -4.2566485 -4.2514715 -4.2348642][-4.2545466 -4.2442956 -4.2341032 -4.2282138 -4.2240434 -4.2179561 -4.2104177 -4.208725 -4.2144146 -4.2239876 -4.2336431 -4.2434464 -4.2474527 -4.2411852 -4.2249966][-4.238049 -4.2287784 -4.2207718 -4.2176242 -4.2163758 -4.2170939 -4.2178078 -4.2194796 -4.2229648 -4.2269769 -4.2304635 -4.2341967 -4.2348561 -4.2273288 -4.2120047][-4.2161741 -4.2074323 -4.1998386 -4.1970291 -4.1973262 -4.2011151 -4.2063055 -4.210535 -4.2132783 -4.2142925 -4.2147837 -4.2157192 -4.2147841 -4.2073045 -4.1948094]]...]
INFO - root - 2017-12-06 09:35:00.653371: step 710, loss = 2.08, batch loss = 2.03 (38.9 examples/sec; 0.205 sec/batch; 18h:56m:08s remains)
INFO - root - 2017-12-06 09:35:02.836317: step 720, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:16s remains)
INFO - root - 2017-12-06 09:35:05.032260: step 730, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:30s remains)
INFO - root - 2017-12-06 09:35:07.215341: step 740, loss = 2.10, batch loss = 2.04 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-06 09:35:09.369788: step 750, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:21s remains)
INFO - root - 2017-12-06 09:35:11.549579: step 760, loss = 2.11, batch loss = 2.05 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:11s remains)
INFO - root - 2017-12-06 09:35:13.747412: step 770, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-06 09:35:15.908571: step 780, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:20s remains)
INFO - root - 2017-12-06 09:35:18.064972: step 790, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:28s remains)
INFO - root - 2017-12-06 09:35:20.334925: step 800, loss = 2.11, batch loss = 2.05 (37.1 examples/sec; 0.215 sec/batch; 19h:50m:52s remains)
2017-12-06 09:35:20.648304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2931061 -4.2891169 -4.2907772 -4.2940564 -4.2852459 -4.2659106 -4.2460952 -4.2313967 -4.2269111 -4.2329187 -4.2391024 -4.24261 -4.2410173 -4.2321329 -4.2206135][-4.3024759 -4.296936 -4.2959824 -4.2979665 -4.2889314 -4.2701325 -4.248168 -4.2278376 -4.2169328 -4.2181025 -4.2221851 -4.2259479 -4.2239137 -4.2157655 -4.2046423][-4.295764 -4.2893524 -4.2876668 -4.2883759 -4.2779365 -4.2577462 -4.2337322 -4.2097311 -4.1951818 -4.1938987 -4.1976352 -4.2040024 -4.205441 -4.2023482 -4.1959105][-4.2714667 -4.2622027 -4.2589941 -4.2579136 -4.2474871 -4.227932 -4.20443 -4.1803379 -4.1644945 -4.1654272 -4.1739507 -4.186214 -4.1942854 -4.1985989 -4.1988306][-4.2366652 -4.2252426 -4.2200494 -4.2165747 -4.2069287 -4.1890121 -4.1673641 -4.1448536 -4.1312475 -4.137619 -4.1550446 -4.1744494 -4.1854496 -4.1941919 -4.2006526][-4.2018046 -4.187705 -4.1765962 -4.1655245 -4.1529202 -4.1342225 -4.1111693 -4.08791 -4.079628 -4.0994434 -4.1328907 -4.1612568 -4.1752172 -4.1872172 -4.1968508][-4.1774769 -4.1630263 -4.14315 -4.1203356 -4.0993767 -4.07475 -4.0426507 -4.0121517 -4.0109539 -4.049933 -4.1024837 -4.1401691 -4.1566048 -4.1641817 -4.1680775][-4.1728325 -4.1660895 -4.1453409 -4.1167541 -4.0884271 -4.055789 -4.0117941 -3.9691291 -3.9686518 -4.016541 -4.0755215 -4.1142187 -4.1285634 -4.1276178 -4.1202354][-4.1871033 -4.1870093 -4.1680689 -4.1389794 -4.1132789 -4.0887618 -4.053031 -4.015038 -4.0096884 -4.0454226 -4.0909142 -4.1196136 -4.1265373 -4.1186066 -4.1036773][-4.2138634 -4.2141042 -4.196322 -4.1692491 -4.1496592 -4.1358457 -4.1139979 -4.0870671 -4.0791459 -4.1016674 -4.1322126 -4.1501675 -4.14955 -4.1366358 -4.1187959][-4.2464614 -4.2464461 -4.2322845 -4.2097464 -4.1929617 -4.1819329 -4.1642046 -4.140543 -4.1317186 -4.1450262 -4.1648793 -4.1771312 -4.1729903 -4.1562862 -4.1374464][-4.2808981 -4.2812328 -4.2718482 -4.2552385 -4.2408752 -4.2303157 -4.2145648 -4.1944032 -4.1857815 -4.192555 -4.2046576 -4.21323 -4.2096105 -4.1944036 -4.1772819][-4.3064666 -4.3056607 -4.298759 -4.2880726 -4.2783546 -4.2720556 -4.2626452 -4.2511587 -4.24796 -4.2526927 -4.2590709 -4.262394 -4.2585392 -4.2477007 -4.23487][-4.3153753 -4.3141527 -4.3094912 -4.3025179 -4.2956166 -4.2921867 -4.2889509 -4.2863445 -4.2888346 -4.2927923 -4.2949958 -4.2949128 -4.2918587 -4.2852869 -4.2767763][-4.3169646 -4.3169804 -4.3144441 -4.3089218 -4.3023548 -4.2994757 -4.299552 -4.3019438 -4.3066659 -4.3094792 -4.3089523 -4.306047 -4.3025703 -4.2983665 -4.2923336]]...]
INFO - root - 2017-12-06 09:35:22.783971: step 810, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:58s remains)
INFO - root - 2017-12-06 09:35:24.979385: step 820, loss = 2.04, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:33s remains)
INFO - root - 2017-12-06 09:35:27.249541: step 830, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:30s remains)
INFO - root - 2017-12-06 09:35:29.473363: step 840, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 21h:03m:13s remains)
INFO - root - 2017-12-06 09:35:31.652431: step 850, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:28s remains)
INFO - root - 2017-12-06 09:35:33.827375: step 860, loss = 2.10, batch loss = 2.04 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:30s remains)
INFO - root - 2017-12-06 09:35:35.981845: step 870, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:30s remains)
INFO - root - 2017-12-06 09:35:38.126562: step 880, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:19s remains)
INFO - root - 2017-12-06 09:35:40.326704: step 890, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-06 09:35:42.470690: step 900, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 19h:02m:29s remains)
2017-12-06 09:35:42.831959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033657 -4.2966375 -4.2977133 -4.3035831 -4.3102775 -4.3186183 -4.3259764 -4.3302851 -4.3310246 -4.3275781 -4.3235722 -4.3202291 -4.3187737 -4.3184652 -4.3185897][-4.2834687 -4.27001 -4.2647209 -4.2674408 -4.2742467 -4.2873921 -4.30132 -4.3146191 -4.3255272 -4.3288293 -4.3280668 -4.3250031 -4.32203 -4.3186197 -4.3156686][-4.257051 -4.2364068 -4.223424 -4.2201519 -4.2260146 -4.2429709 -4.2602196 -4.2820449 -4.305213 -4.3188491 -4.3265738 -4.329896 -4.3304453 -4.3253589 -4.3186274][-4.22678 -4.1931615 -4.1667361 -4.1525259 -4.1541791 -4.17404 -4.1965642 -4.2291861 -4.2639203 -4.2870946 -4.3069572 -4.321763 -4.3315306 -4.3299885 -4.3215351][-4.1995964 -4.1524 -4.1085844 -4.0778546 -4.0665298 -4.0834856 -4.11153 -4.1595736 -4.2104988 -4.2439508 -4.2729411 -4.2989063 -4.3193 -4.3267803 -4.3211012][-4.1847196 -4.1288466 -4.0693874 -4.0173135 -3.9812996 -3.9769223 -4.0020704 -4.0684185 -4.1416135 -4.191668 -4.2319708 -4.267632 -4.2967944 -4.3141875 -4.3162785][-4.1829405 -4.12706 -4.0631452 -3.9932148 -3.9266388 -3.8843691 -3.8870051 -3.9639525 -4.0571609 -4.1237097 -4.1791725 -4.2275877 -4.2652612 -4.2918625 -4.30452][-4.1935163 -4.1445718 -4.0902238 -4.0247235 -3.9471054 -3.8727114 -3.8335221 -3.8843384 -3.9752014 -4.0517254 -4.1197896 -4.1833606 -4.2338758 -4.2687173 -4.2890329][-4.20773 -4.1705489 -4.1284947 -4.0774918 -4.0125628 -3.9393232 -3.8777809 -3.879349 -3.931685 -3.9977937 -4.068573 -4.1439333 -4.2092619 -4.2529712 -4.2774696][-4.2268391 -4.2039933 -4.1763906 -4.1372046 -4.0898142 -4.0364566 -3.9842012 -3.9630609 -3.9722166 -4.0068 -4.0619707 -4.1344166 -4.2047005 -4.2523241 -4.2780695][-4.2505331 -4.2393932 -4.2240272 -4.1961317 -4.164155 -4.1306105 -4.09677 -4.0787735 -4.0759459 -4.0834236 -4.1120453 -4.1669388 -4.2261658 -4.2676167 -4.289948][-4.2697535 -4.2621856 -4.254272 -4.2372212 -4.2183423 -4.2006636 -4.184978 -4.1780424 -4.1787634 -4.1785436 -4.1890678 -4.2216234 -4.2612405 -4.2902937 -4.307374][-4.2917886 -4.2848096 -4.2805505 -4.27333 -4.2646923 -4.2574782 -4.2525125 -4.2512536 -4.2538376 -4.2546449 -4.2590466 -4.2771 -4.2984929 -4.3145938 -4.3250184][-4.3113236 -4.3054261 -4.3023658 -4.2994022 -4.2967558 -4.2960143 -4.2956734 -4.2952261 -4.2982111 -4.3021293 -4.3038778 -4.311316 -4.3215814 -4.3302264 -4.3350916][-4.3226871 -4.3185639 -4.3162446 -4.31424 -4.3127761 -4.3142509 -4.3159304 -4.3159628 -4.3176346 -4.3205414 -4.3211832 -4.3228946 -4.3271189 -4.3313184 -4.3331971]]...]
INFO - root - 2017-12-06 09:35:44.963675: step 910, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:22s remains)
INFO - root - 2017-12-06 09:35:47.134724: step 920, loss = 2.10, batch loss = 2.05 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:41s remains)
INFO - root - 2017-12-06 09:35:49.283958: step 930, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-06 09:35:51.432126: step 940, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-06 09:35:53.581643: step 950, loss = 2.11, batch loss = 2.06 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:52s remains)
INFO - root - 2017-12-06 09:35:55.750259: step 960, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:41s remains)
INFO - root - 2017-12-06 09:35:57.932178: step 970, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:49s remains)
INFO - root - 2017-12-06 09:36:00.073933: step 980, loss = 2.09, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:15s remains)
INFO - root - 2017-12-06 09:36:02.253198: step 990, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:06s remains)
INFO - root - 2017-12-06 09:36:04.435842: step 1000, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:53s remains)
2017-12-06 09:36:04.795841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861161 -4.2814918 -4.281621 -4.2848444 -4.2912483 -4.2999725 -4.3102484 -4.3136997 -4.307919 -4.2968674 -4.2794666 -4.2704024 -4.269978 -4.2772069 -4.2907333][-4.2624288 -4.2567573 -4.2593069 -4.263217 -4.2694936 -4.2774959 -4.2865868 -4.2869215 -4.2760816 -4.2612748 -4.238513 -4.2269225 -4.2284536 -4.240694 -4.2592669][-4.2527146 -4.2459474 -4.2482805 -4.2484035 -4.248405 -4.2498231 -4.2512403 -4.246099 -4.23572 -4.2292671 -4.2128954 -4.2032657 -4.2069774 -4.2209611 -4.2384214][-4.2435832 -4.2316532 -4.2262592 -4.2208619 -4.21281 -4.1990738 -4.1820469 -4.1681175 -4.169939 -4.1868563 -4.1887288 -4.1884236 -4.1949229 -4.2074051 -4.21838][-4.2228308 -4.20345 -4.190052 -4.1804757 -4.1643648 -4.12786 -4.0809283 -4.0530553 -4.0729389 -4.1231351 -4.1491194 -4.1621184 -4.1766291 -4.192111 -4.1995468][-4.211688 -4.1912637 -4.1760516 -4.166646 -4.1398892 -4.074688 -3.9879351 -3.9342415 -3.9689102 -4.0529222 -4.1054826 -4.1331205 -4.1592946 -4.184361 -4.1963515][-4.2162194 -4.198247 -4.1853766 -4.1815448 -4.1533766 -4.0775986 -3.9728413 -3.9051349 -3.9361 -4.0165315 -4.0659862 -4.0928121 -4.1246257 -4.1608782 -4.1884427][-4.2241983 -4.2091413 -4.2006316 -4.2016983 -4.1825337 -4.1248035 -4.0483675 -4.0002995 -4.01627 -4.0502796 -4.0626287 -4.0713849 -4.0978131 -4.1351137 -4.1731339][-4.2213111 -4.2077909 -4.1977148 -4.1961632 -4.1853023 -4.1513195 -4.1106892 -4.0870481 -4.0956926 -4.0966272 -4.08006 -4.072751 -4.09287 -4.1277471 -4.1678686][-4.2118697 -4.19834 -4.1890011 -4.187623 -4.18284 -4.1658816 -4.1524343 -4.1460729 -4.1529446 -4.1369429 -4.1024346 -4.0829744 -4.1018543 -4.1364222 -4.1751776][-4.2294016 -4.212369 -4.2034535 -4.2009077 -4.1962433 -4.1903467 -4.192791 -4.1984711 -4.2065492 -4.1853018 -4.1433859 -4.1187387 -4.1368256 -4.1684713 -4.1999187][-4.2767859 -4.2574434 -4.2465811 -4.2406712 -4.2348132 -4.2335186 -4.2425327 -4.2534895 -4.26203 -4.2450657 -4.2073317 -4.1840563 -4.197361 -4.2207918 -4.2418566][-4.3189626 -4.3030386 -4.2917213 -4.2855692 -4.2810626 -4.2850804 -4.2990127 -4.3105369 -4.3158669 -4.3026676 -4.2723026 -4.2512045 -4.2593694 -4.274704 -4.2848649][-4.3385491 -4.3301635 -4.3237953 -4.3214931 -4.3212919 -4.3261571 -4.3371725 -4.3450718 -4.3486271 -4.3410935 -4.3212991 -4.3065896 -4.3101678 -4.3186293 -4.3208685][-4.3420062 -4.338398 -4.3361654 -4.3368654 -4.339591 -4.3430262 -4.34689 -4.3496237 -4.3509607 -4.3474946 -4.337956 -4.3314614 -4.3341861 -4.3398604 -4.3408651]]...]
INFO - root - 2017-12-06 09:36:06.977663: step 1010, loss = 2.06, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:11s remains)
INFO - root - 2017-12-06 09:36:09.116527: step 1020, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:44s remains)
INFO - root - 2017-12-06 09:36:11.292485: step 1030, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-06 09:36:13.469272: step 1040, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:47s remains)
INFO - root - 2017-12-06 09:36:15.639971: step 1050, loss = 2.10, batch loss = 2.04 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:43s remains)
INFO - root - 2017-12-06 09:36:17.825133: step 1060, loss = 2.08, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:11s remains)
INFO - root - 2017-12-06 09:36:19.987662: step 1070, loss = 2.08, batch loss = 2.02 (38.5 examples/sec; 0.208 sec/batch; 19h:07m:18s remains)
INFO - root - 2017-12-06 09:36:22.162158: step 1080, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:02s remains)
INFO - root - 2017-12-06 09:36:24.328170: step 1090, loss = 2.06, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:24s remains)
INFO - root - 2017-12-06 09:36:26.474297: step 1100, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:27s remains)
2017-12-06 09:36:26.880122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1778378 -4.1478343 -4.1217742 -4.1197867 -4.1368742 -4.1673117 -4.2022734 -4.2336521 -4.2582979 -4.2791762 -4.2994742 -4.3104415 -4.3150644 -4.3201079 -4.3166375][-4.1818829 -4.1537933 -4.1190982 -4.0881233 -4.06842 -4.085845 -4.1361828 -4.1884546 -4.2276449 -4.2580853 -4.2821569 -4.294517 -4.3046484 -4.318193 -4.3234816][-4.1930623 -4.1674705 -4.1267729 -4.07529 -4.0259075 -4.02653 -4.083405 -4.1526523 -4.2051425 -4.2440462 -4.2700944 -4.282649 -4.2948132 -4.313334 -4.3230419][-4.2100143 -4.19145 -4.156373 -4.1066251 -4.0525455 -4.0388207 -4.082108 -4.1447892 -4.1984267 -4.2383151 -4.2638726 -4.2756882 -4.2879896 -4.3083072 -4.3200779][-4.212203 -4.195075 -4.1651707 -4.1271577 -4.0858622 -4.0702696 -4.0966864 -4.1449461 -4.1950717 -4.2308373 -4.2563806 -4.2703366 -4.2811642 -4.3007655 -4.3155041][-4.1927648 -4.1770039 -4.1485314 -4.1180372 -4.0844946 -4.0624876 -4.0684628 -4.0977821 -4.1480775 -4.1931286 -4.2282991 -4.2492294 -4.2648396 -4.287837 -4.308239][-4.1672635 -4.15227 -4.1292849 -4.1066413 -4.0736485 -4.0407548 -4.0234294 -4.0295749 -4.0761604 -4.1318774 -4.1784811 -4.21255 -4.2385378 -4.270112 -4.2971196][-4.1691675 -4.1513896 -4.1290903 -4.1118112 -4.0823131 -4.044023 -4.0130043 -4.0044117 -4.0400858 -4.0954161 -4.1458864 -4.1861086 -4.2168956 -4.2533841 -4.2853909][-4.2002969 -4.18001 -4.1571255 -4.1363029 -4.1088119 -4.0741143 -4.0470748 -4.0403233 -4.0635438 -4.1035872 -4.1456642 -4.1816621 -4.2122207 -4.2485619 -4.2827659][-4.2245183 -4.2078443 -4.1875091 -4.1611071 -4.1292343 -4.0973239 -4.0767703 -4.0745916 -4.0894737 -4.1163926 -4.1509986 -4.1852131 -4.2168036 -4.2535915 -4.2875538][-4.2204723 -4.2146878 -4.2039843 -4.1805544 -4.1452103 -4.1090307 -4.0857525 -4.0830741 -4.090878 -4.109457 -4.142087 -4.1802826 -4.2161531 -4.2562394 -4.29064][-4.2040792 -4.20834 -4.2102666 -4.1988392 -4.1678171 -4.1317029 -4.1055241 -4.0976682 -4.0960679 -4.1037951 -4.1324692 -4.1708837 -4.209682 -4.25294 -4.2873425][-4.2061391 -4.2144389 -4.2225294 -4.2197309 -4.2001443 -4.1719885 -4.1483245 -4.1335149 -4.1208906 -4.1182575 -4.1378508 -4.1691732 -4.2068534 -4.2503638 -4.28266][-4.2238188 -4.2289639 -4.2328329 -4.2331376 -4.2267413 -4.213068 -4.1986532 -4.1816421 -4.1597285 -4.1467152 -4.1566086 -4.1786551 -4.2138577 -4.2533746 -4.2810555][-4.2338762 -4.2342515 -4.23246 -4.2336297 -4.236876 -4.235002 -4.22957 -4.2145677 -4.1875238 -4.168498 -4.172658 -4.1882071 -4.218266 -4.2528567 -4.2763953]]...]
INFO - root - 2017-12-06 09:36:29.018635: step 1110, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:28s remains)
INFO - root - 2017-12-06 09:36:31.176791: step 1120, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:20s remains)
INFO - root - 2017-12-06 09:36:33.324316: step 1130, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:51s remains)
INFO - root - 2017-12-06 09:36:35.484737: step 1140, loss = 2.09, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:45s remains)
INFO - root - 2017-12-06 09:36:37.651266: step 1150, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:59s remains)
INFO - root - 2017-12-06 09:36:39.775919: step 1160, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:57s remains)
INFO - root - 2017-12-06 09:36:41.950766: step 1170, loss = 2.09, batch loss = 2.04 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:22s remains)
INFO - root - 2017-12-06 09:36:44.082372: step 1180, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:37s remains)
INFO - root - 2017-12-06 09:36:46.253034: step 1190, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:56s remains)
INFO - root - 2017-12-06 09:36:48.441715: step 1200, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:45s remains)
2017-12-06 09:36:48.746943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2562518 -4.2582207 -4.2730894 -4.2888856 -4.2963071 -4.2992511 -4.3013029 -4.3075461 -4.3156872 -4.3190055 -4.3149724 -4.3070664 -4.3005409 -4.2946835 -4.2890496][-4.2184935 -4.2167697 -4.2359333 -4.2644243 -4.2831984 -4.2928972 -4.2986488 -4.3082671 -4.3205557 -4.3280382 -4.3222528 -4.3053927 -4.2882833 -4.2765613 -4.2659779][-4.17996 -4.1742158 -4.193017 -4.2335806 -4.2631416 -4.2754135 -4.2812634 -4.2926459 -4.3098273 -4.3237019 -4.3225145 -4.30603 -4.2864952 -4.2704115 -4.2523851][-4.1657596 -4.1614313 -4.1796427 -4.2241921 -4.2527618 -4.2568955 -4.25507 -4.2640443 -4.2850909 -4.3044004 -4.3119392 -4.3054452 -4.2923689 -4.2764835 -4.25325][-4.1749187 -4.1766849 -4.1928439 -4.2312088 -4.2487307 -4.2327375 -4.2078824 -4.2087011 -4.2378507 -4.2670393 -4.2885094 -4.2986784 -4.3002553 -4.2896371 -4.2651396][-4.1823916 -4.190433 -4.2074895 -4.2348166 -4.2337494 -4.1889405 -4.1257277 -4.0992517 -4.1429291 -4.2002659 -4.2447448 -4.2762136 -4.2968607 -4.2998919 -4.2821426][-4.1899972 -4.2001176 -4.2124958 -4.223516 -4.2024994 -4.1321163 -4.0225639 -3.946794 -4.012167 -4.1191978 -4.1999316 -4.2514863 -4.2885628 -4.3055191 -4.2967734][-4.2018895 -4.2104163 -4.2173524 -4.2099032 -4.1748495 -4.0943427 -3.9659488 -3.8569839 -3.9361353 -4.0770488 -4.1763487 -4.2371054 -4.278151 -4.3039207 -4.3056831][-4.2054734 -4.2112684 -4.2161407 -4.2031593 -4.1742473 -4.119082 -4.0315189 -3.9550447 -4.0044608 -4.109982 -4.1872106 -4.2375703 -4.2712183 -4.2950706 -4.3052711][-4.2088265 -4.2119741 -4.217917 -4.2146659 -4.2030811 -4.1802597 -4.1433997 -4.1087437 -4.1256757 -4.1773376 -4.2190733 -4.2454681 -4.2632294 -4.2808151 -4.2953448][-4.2259111 -4.2257457 -4.2305555 -4.23502 -4.2349157 -4.2337966 -4.2306724 -4.2253819 -4.22851 -4.2426596 -4.2565732 -4.2597008 -4.2589808 -4.26424 -4.277565][-4.2594223 -4.2541885 -4.2532878 -4.2557096 -4.258678 -4.267951 -4.285583 -4.2984915 -4.2981758 -4.2899804 -4.2811265 -4.2712936 -4.2587032 -4.2530527 -4.2610221][-4.2963181 -4.2880988 -4.2826324 -4.2783985 -4.2771482 -4.28635 -4.3100724 -4.3269973 -4.3211546 -4.3018923 -4.2823939 -4.2693253 -4.2563953 -4.2499332 -4.2540827][-4.3111758 -4.3027034 -4.29918 -4.2936144 -4.2891078 -4.2927961 -4.3092489 -4.3204255 -4.3119187 -4.2887049 -4.2686782 -4.2578435 -4.2487969 -4.2487917 -4.2544813][-4.3087654 -4.3050385 -4.3047862 -4.3015447 -4.2987294 -4.2986355 -4.3037171 -4.3063331 -4.2971873 -4.2769403 -4.2626534 -4.2548656 -4.2467051 -4.2524261 -4.2663856]]...]
INFO - root - 2017-12-06 09:36:50.906101: step 1210, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:44s remains)
INFO - root - 2017-12-06 09:36:53.051520: step 1220, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:58s remains)
INFO - root - 2017-12-06 09:36:55.235521: step 1230, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:17s remains)
INFO - root - 2017-12-06 09:36:57.384801: step 1240, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:31s remains)
INFO - root - 2017-12-06 09:36:59.518704: step 1250, loss = 2.11, batch loss = 2.05 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:02s remains)
INFO - root - 2017-12-06 09:37:01.670202: step 1260, loss = 2.07, batch loss = 2.01 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:14s remains)
INFO - root - 2017-12-06 09:37:03.826516: step 1270, loss = 2.08, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:41s remains)
INFO - root - 2017-12-06 09:37:05.972389: step 1280, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-06 09:37:08.155884: step 1290, loss = 2.10, batch loss = 2.04 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:00s remains)
INFO - root - 2017-12-06 09:37:10.307419: step 1300, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:05s remains)
2017-12-06 09:37:10.623380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2106071 -4.2130532 -4.1980076 -4.188283 -4.1772385 -4.1504617 -4.14615 -4.1717181 -4.2036109 -4.2394457 -4.265738 -4.2807178 -4.2858949 -4.2849422 -4.2553654][-4.2088823 -4.2231054 -4.2151418 -4.20567 -4.1981068 -4.1743355 -4.154357 -4.1706986 -4.2075033 -4.2444773 -4.272707 -4.296628 -4.3008928 -4.298532 -4.2753148][-4.2089915 -4.2279058 -4.2238364 -4.2124567 -4.2056613 -4.1859832 -4.1552281 -4.1624546 -4.2022634 -4.241034 -4.2722254 -4.3025227 -4.3090363 -4.3056579 -4.2892523][-4.1978116 -4.2134461 -4.2101 -4.1941438 -4.1851721 -4.1710486 -4.1418967 -4.1400056 -4.1754026 -4.2187905 -4.2558093 -4.2874975 -4.2953563 -4.2912765 -4.2837696][-4.1548834 -4.1574464 -4.1508074 -4.1271615 -4.113379 -4.1040034 -4.0787992 -4.0662165 -4.0928612 -4.1396642 -4.1801081 -4.2159672 -4.2348709 -4.2368789 -4.2381392][-4.0958514 -4.0787096 -4.0655894 -4.0391283 -4.0234523 -4.0211124 -3.9936066 -3.955416 -3.9633243 -4.0136876 -4.0582452 -4.0993023 -4.1323705 -4.1459789 -4.1544533][-4.0461154 -4.0140719 -3.9949398 -3.9691195 -3.952095 -3.949461 -3.9125812 -3.8501635 -3.8384416 -3.9002266 -3.9564955 -3.9969146 -4.0344753 -4.0552087 -4.0702686][-4.0294509 -3.9936273 -3.9724276 -3.9521112 -3.9349861 -3.9256659 -3.8839865 -3.8214626 -3.8004937 -3.8579016 -3.9181023 -3.9542937 -3.9867663 -4.0077314 -4.0249934][-4.0381541 -4.0081019 -3.9896679 -3.9748049 -3.95986 -3.9484901 -3.9161966 -3.8743334 -3.8574691 -3.8943055 -3.9383259 -3.9606926 -3.978828 -3.9948463 -4.0085516][-4.0660987 -4.0464449 -4.0339017 -4.0249 -4.0159965 -4.0089488 -3.9876173 -3.9617636 -3.9499354 -3.9641213 -3.9812899 -3.9852409 -3.9893084 -3.99739 -4.003406][-4.1058421 -4.0964689 -4.0904617 -4.0863056 -4.0838943 -4.0846338 -4.0749946 -4.0582972 -4.0453758 -4.0412188 -4.03452 -4.0209351 -4.0103893 -4.0110545 -4.0139027][-4.1422429 -4.1369443 -4.1317968 -4.1283836 -4.1312852 -4.1401877 -4.137723 -4.1248507 -4.110806 -4.098063 -4.0819178 -4.0649524 -4.0516138 -4.0523262 -4.0567894][-4.1651511 -4.1574249 -4.1491594 -4.1448107 -4.1495509 -4.1635089 -4.1667376 -4.1585741 -4.1483369 -4.1358895 -4.1218228 -4.1096058 -4.0978894 -4.0983596 -4.1051655][-4.1831584 -4.1693387 -4.1543293 -4.1440082 -4.1462674 -4.1604223 -4.1669278 -4.1630888 -4.1571717 -4.1500459 -4.1422739 -4.1373506 -4.1312165 -4.1333618 -4.1415][-4.2080259 -4.1910639 -4.1709962 -4.1550083 -4.1524668 -4.1636457 -4.1705437 -4.1678534 -4.1634283 -4.1590624 -4.1537285 -4.1492476 -4.1450472 -4.1468925 -4.1546865]]...]
INFO - root - 2017-12-06 09:37:12.801391: step 1310, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:17m:38s remains)
INFO - root - 2017-12-06 09:37:15.010536: step 1320, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.221 sec/batch; 20h:18m:03s remains)
INFO - root - 2017-12-06 09:37:17.131225: step 1330, loss = 2.10, batch loss = 2.04 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:55s remains)
INFO - root - 2017-12-06 09:37:19.284387: step 1340, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:28s remains)
INFO - root - 2017-12-06 09:37:21.420090: step 1350, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:48s remains)
INFO - root - 2017-12-06 09:37:23.600101: step 1360, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:49s remains)
INFO - root - 2017-12-06 09:37:25.744005: step 1370, loss = 2.07, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:10s remains)
INFO - root - 2017-12-06 09:37:27.891641: step 1380, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:36s remains)
INFO - root - 2017-12-06 09:37:30.100185: step 1390, loss = 2.05, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:20s remains)
INFO - root - 2017-12-06 09:37:32.256932: step 1400, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:01s remains)
2017-12-06 09:37:32.624679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.298986 -4.2967329 -4.3014779 -4.30831 -4.3068428 -4.3050227 -4.3076839 -4.3175359 -4.3278093 -4.3368258 -4.341083 -4.3429523 -4.3396597 -4.330832 -4.3254404][-4.2657738 -4.2638292 -4.2735419 -4.2850342 -4.2840471 -4.2793956 -4.2796187 -4.2926412 -4.3079076 -4.3197474 -4.3268232 -4.3335929 -4.3350406 -4.3279848 -4.3226895][-4.2053528 -4.20159 -4.2171311 -4.2355018 -4.2383752 -4.2347865 -4.230619 -4.2455974 -4.2675295 -4.2870011 -4.299078 -4.313056 -4.3217444 -4.3222384 -4.3207483][-4.14418 -4.1377645 -4.1617355 -4.1889629 -4.1970525 -4.1946669 -4.1854744 -4.2026973 -4.2342381 -4.2642841 -4.2800803 -4.2969527 -4.310308 -4.3163848 -4.3193827][-4.094039 -4.0863595 -4.1224074 -4.1626811 -4.1750669 -4.166512 -4.1481886 -4.1661873 -4.2096496 -4.25046 -4.2684841 -4.2871652 -4.30424 -4.3129334 -4.3175][-4.0695391 -4.0620236 -4.10556 -4.1528649 -4.1598134 -4.1383448 -4.1077914 -4.1245322 -4.1773734 -4.2252913 -4.2479677 -4.2721195 -4.2959566 -4.308918 -4.3151579][-4.0768352 -4.0684919 -4.1099987 -4.1508284 -4.1492419 -4.1176395 -4.0786824 -4.0946407 -4.1535258 -4.2069173 -4.2355156 -4.2652283 -4.2906828 -4.3061228 -4.3133907][-4.115746 -4.1149964 -4.1515012 -4.1787562 -4.1719131 -4.13953 -4.1027865 -4.1188331 -4.1762691 -4.2245297 -4.2517104 -4.2767224 -4.2953877 -4.3075657 -4.3135824][-4.1606574 -4.1697946 -4.199295 -4.2131619 -4.2007327 -4.1712217 -4.1458545 -4.1657648 -4.2161436 -4.2520332 -4.2718544 -4.2875557 -4.2974577 -4.30728 -4.312582][-4.1811333 -4.1924496 -4.2200704 -4.2260842 -4.2099733 -4.1883106 -4.177341 -4.199894 -4.2413149 -4.2647319 -4.2748542 -4.2847233 -4.2940559 -4.3043 -4.3106022][-4.1869216 -4.1944327 -4.2148056 -4.2134476 -4.1981654 -4.1845641 -4.17985 -4.2021775 -4.2405195 -4.2595272 -4.2630949 -4.2730107 -4.2889152 -4.3030257 -4.3107643][-4.2102942 -4.2087531 -4.215085 -4.20187 -4.1799092 -4.1581626 -4.14868 -4.1724339 -4.2112389 -4.2318659 -4.2364655 -4.2521496 -4.27835 -4.298934 -4.30991][-4.2385411 -4.2320194 -4.2227163 -4.1974082 -4.1610274 -4.120481 -4.0967903 -4.1203547 -4.1641426 -4.19388 -4.20843 -4.2321138 -4.2658582 -4.2921948 -4.3075633][-4.2611485 -4.2506166 -4.2331452 -4.1983247 -4.1472521 -4.0916805 -4.0561328 -4.0778923 -4.1291761 -4.1713071 -4.1982594 -4.2291417 -4.2640529 -4.2911568 -4.3069062][-4.2720284 -4.2598863 -4.2375703 -4.1972461 -4.1383209 -4.0751715 -4.0388403 -4.0645332 -4.1225662 -4.1696477 -4.2009745 -4.2330952 -4.2653408 -4.2905407 -4.3054628]]...]
INFO - root - 2017-12-06 09:37:34.792471: step 1410, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-06 09:37:36.983964: step 1420, loss = 2.08, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:46s remains)
INFO - root - 2017-12-06 09:37:39.189693: step 1430, loss = 2.10, batch loss = 2.04 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:59s remains)
INFO - root - 2017-12-06 09:37:41.361740: step 1440, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:04s remains)
INFO - root - 2017-12-06 09:37:43.625329: step 1450, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.223 sec/batch; 20h:27m:44s remains)
INFO - root - 2017-12-06 09:37:45.853582: step 1460, loss = 2.11, batch loss = 2.05 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:10s remains)
INFO - root - 2017-12-06 09:37:48.001531: step 1470, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:40s remains)
INFO - root - 2017-12-06 09:37:50.134369: step 1480, loss = 2.09, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:14s remains)
INFO - root - 2017-12-06 09:37:52.295371: step 1490, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:07s remains)
INFO - root - 2017-12-06 09:37:54.485038: step 1500, loss = 2.11, batch loss = 2.05 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:44s remains)
2017-12-06 09:37:54.832504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2271643 -4.2152958 -4.1992893 -4.1819735 -4.1811404 -4.2088456 -4.2436728 -4.2771 -4.3059263 -4.31493 -4.3054347 -4.3021679 -4.2973361 -4.2940025 -4.2987485][-4.2119932 -4.1905985 -4.1652169 -4.138124 -4.13719 -4.1808758 -4.2256866 -4.2626977 -4.2926722 -4.2979441 -4.2850585 -4.2814531 -4.2766709 -4.2735472 -4.2806945][-4.2065387 -4.1795359 -4.1493444 -4.1122823 -4.1118455 -4.1700268 -4.2217159 -4.2595119 -4.2858105 -4.2845397 -4.265677 -4.26036 -4.2578874 -4.2598844 -4.2714319][-4.2068262 -4.1815176 -4.1461973 -4.0950727 -4.0917482 -4.1610522 -4.2171612 -4.2500582 -4.2704997 -4.2600679 -4.2400784 -4.2417164 -4.2467527 -4.2545352 -4.2690711][-4.2063689 -4.1841121 -4.1396046 -4.0711918 -4.0601106 -4.1359091 -4.1980414 -4.2289052 -4.2433968 -4.2258415 -4.209651 -4.2201447 -4.2324934 -4.2478294 -4.2669511][-4.1908379 -4.1715255 -4.1177998 -4.0343513 -4.01414 -4.0872445 -4.1549268 -4.1894851 -4.2037153 -4.1890988 -4.1778193 -4.1948824 -4.2144036 -4.2373662 -4.2597013][-4.1753359 -4.1561222 -4.0905528 -3.9854293 -3.9478922 -4.0213094 -4.0973043 -4.1403136 -4.1608276 -4.1482272 -4.1371226 -4.1575379 -4.1889052 -4.2216454 -4.2485271][-4.1522355 -4.1360469 -4.06784 -3.9533355 -3.9079947 -3.9836626 -4.0661955 -4.1184187 -4.1448708 -4.1320481 -4.1200061 -4.1395073 -4.1735988 -4.2085533 -4.24082][-4.1507568 -4.1461744 -4.0925426 -3.9929054 -3.9472184 -4.0064368 -4.0787649 -4.1276298 -4.1469207 -4.1311159 -4.1214776 -4.138792 -4.1694107 -4.2034779 -4.23707][-4.1669083 -4.1656523 -4.1237917 -4.0448236 -4.0050387 -4.045362 -4.0995178 -4.1373281 -4.1453266 -4.1257648 -4.11861 -4.1369085 -4.1658454 -4.2002821 -4.235393][-4.1967049 -4.1932049 -4.16073 -4.1035185 -4.0707231 -4.0923758 -4.1303511 -4.162715 -4.171174 -4.1554613 -4.1511559 -4.1681595 -4.1919804 -4.222 -4.2543][-4.2283058 -4.2232752 -4.2002697 -4.165225 -4.1439257 -4.1548696 -4.1813164 -4.2066236 -4.2141485 -4.2014446 -4.1990485 -4.21422 -4.234314 -4.2594142 -4.2851162][-4.2531276 -4.2507253 -4.2386951 -4.221755 -4.2118864 -4.21985 -4.2395139 -4.2564321 -4.2607393 -4.2508802 -4.246665 -4.2565689 -4.2722464 -4.2903476 -4.3078079][-4.2730408 -4.2708354 -4.264442 -4.2580094 -4.25618 -4.2637415 -4.2786956 -4.290668 -4.294096 -4.2895808 -4.2862253 -4.2903948 -4.2993402 -4.3090248 -4.3186922][-4.291759 -4.2858887 -4.2809072 -4.2783794 -4.28031 -4.2860131 -4.2955694 -4.3030682 -4.3074961 -4.30795 -4.3074775 -4.3084283 -4.3120236 -4.316689 -4.3235683]]...]
INFO - root - 2017-12-06 09:37:56.989671: step 1510, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:45s remains)
INFO - root - 2017-12-06 09:37:59.136592: step 1520, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:35s remains)
INFO - root - 2017-12-06 09:38:01.315449: step 1530, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-06 09:38:03.446983: step 1540, loss = 2.09, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:44s remains)
INFO - root - 2017-12-06 09:38:05.617228: step 1550, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:00s remains)
INFO - root - 2017-12-06 09:38:07.824699: step 1560, loss = 2.07, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 20h:48m:52s remains)
INFO - root - 2017-12-06 09:38:10.069622: step 1570, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:45s remains)
INFO - root - 2017-12-06 09:38:12.245190: step 1580, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:06s remains)
INFO - root - 2017-12-06 09:38:14.356148: step 1590, loss = 2.09, batch loss = 2.04 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:43s remains)
INFO - root - 2017-12-06 09:38:16.515373: step 1600, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:26s remains)
2017-12-06 09:38:16.868541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3114419 -4.320303 -4.3324256 -4.3382645 -4.3389697 -4.3389397 -4.3326769 -4.3269267 -4.3329186 -4.3377833 -4.344677 -4.3561139 -4.3671551 -4.3736148 -4.3694983][-4.2930231 -4.302609 -4.3168058 -4.32257 -4.3228974 -4.3240666 -4.3194361 -4.313457 -4.3234344 -4.3328595 -4.3451271 -4.3590465 -4.3692536 -4.3726993 -4.3636222][-4.2798409 -4.2909322 -4.3053102 -4.3100905 -4.30868 -4.31068 -4.3079505 -4.303576 -4.3164783 -4.3292179 -4.3468766 -4.3610692 -4.3666706 -4.3626628 -4.3456635][-4.2716279 -4.2839875 -4.2978144 -4.3000231 -4.2936811 -4.2908382 -4.2860942 -4.2827263 -4.2981348 -4.3179159 -4.3425908 -4.3613434 -4.3641477 -4.3551006 -4.3341455][-4.25937 -4.2650647 -4.2713633 -4.2677555 -4.2541666 -4.2447543 -4.2339711 -4.228333 -4.2476945 -4.2794976 -4.31823 -4.3466897 -4.3534231 -4.348237 -4.3297372][-4.2519579 -4.2432117 -4.2315559 -4.2160811 -4.1912074 -4.1716413 -4.14998 -4.1342292 -4.1647429 -4.2194152 -4.2735152 -4.3087888 -4.3237281 -4.3282695 -4.3171911][-4.2520123 -4.2330251 -4.2094984 -4.179132 -4.1368313 -4.0951004 -4.0411172 -4.0002966 -4.0459075 -4.1321988 -4.2061768 -4.2571378 -4.2877269 -4.3043866 -4.3040285][-4.25076 -4.2303104 -4.2043462 -4.1652265 -4.109849 -4.0452642 -3.9550717 -3.8846219 -3.9378719 -4.0437622 -4.13456 -4.2055197 -4.2506557 -4.2780313 -4.2896204][-4.2596145 -4.2513471 -4.2404928 -4.2134142 -4.1684113 -4.1094828 -4.0220885 -3.9515254 -3.9873848 -4.0645409 -4.1361022 -4.2004623 -4.2425442 -4.2666121 -4.2811255][-4.2639942 -4.2651858 -4.2686157 -4.2559781 -4.2225218 -4.1768332 -4.114409 -4.0647187 -4.084949 -4.1278567 -4.1726146 -4.2159338 -4.2435203 -4.2607856 -4.2726288][-4.24497 -4.248837 -4.2594843 -4.2544613 -4.2259469 -4.1904421 -4.1487794 -4.1169291 -4.1296644 -4.1547265 -4.18483 -4.2124233 -4.2313728 -4.2473097 -4.2583971][-4.2224751 -4.2277651 -4.2406578 -4.240684 -4.2195368 -4.1968513 -4.1714282 -4.1503434 -4.1598148 -4.1777496 -4.1968365 -4.2120562 -4.2251153 -4.2404885 -4.2508726][-4.2115917 -4.216085 -4.2264848 -4.22667 -4.2125778 -4.2008276 -4.1874228 -4.1740727 -4.183166 -4.19749 -4.2101645 -4.2197375 -4.231565 -4.248229 -4.2575407][-4.2065916 -4.2041607 -4.2093334 -4.2074676 -4.1984186 -4.1954074 -4.1895289 -4.1821375 -4.1933374 -4.20734 -4.2194185 -4.2290759 -4.2410135 -4.2555394 -4.2623014][-4.2108774 -4.2011867 -4.2013931 -4.200891 -4.19881 -4.201478 -4.1990995 -4.1939368 -4.2031054 -4.2147431 -4.2254448 -4.23494 -4.2466574 -4.2588239 -4.2638087]]...]
INFO - root - 2017-12-06 09:38:19.078418: step 1610, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:53s remains)
INFO - root - 2017-12-06 09:38:21.281273: step 1620, loss = 2.07, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:24s remains)
INFO - root - 2017-12-06 09:38:23.446698: step 1630, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:06s remains)
INFO - root - 2017-12-06 09:38:25.611539: step 1640, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:32s remains)
INFO - root - 2017-12-06 09:38:27.746754: step 1650, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:43m:22s remains)
INFO - root - 2017-12-06 09:38:29.922160: step 1660, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:15s remains)
INFO - root - 2017-12-06 09:38:32.097129: step 1670, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:27s remains)
INFO - root - 2017-12-06 09:38:34.248368: step 1680, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:17s remains)
INFO - root - 2017-12-06 09:38:36.376094: step 1690, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:38s remains)
INFO - root - 2017-12-06 09:38:38.528967: step 1700, loss = 2.08, batch loss = 2.02 (39.3 examples/sec; 0.204 sec/batch; 18h:42m:47s remains)
2017-12-06 09:38:38.885331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1536145 -4.1275682 -4.1536932 -4.215941 -4.2788067 -4.3251138 -4.3466954 -4.3409376 -4.3243465 -4.3074527 -4.305275 -4.3072686 -4.2999735 -4.2811112 -4.264184][-4.0956349 -4.0519428 -4.0852218 -4.1701331 -4.2535005 -4.3095655 -4.3327527 -4.3247285 -4.3146172 -4.3080482 -4.3119068 -4.3148808 -4.3029871 -4.2757025 -4.2536778][-4.0468984 -3.9916787 -4.033289 -4.1419415 -4.2381887 -4.292594 -4.3063731 -4.2963781 -4.3016319 -4.310915 -4.3216043 -4.3288331 -4.315228 -4.2807217 -4.2528896][-4.0513506 -4.0041294 -4.0495043 -4.1578507 -4.2477531 -4.2813721 -4.2711639 -4.2517438 -4.2669945 -4.2906837 -4.3176074 -4.3387485 -4.3273711 -4.2875419 -4.2527761][-4.1122603 -4.0937872 -4.1340389 -4.2134185 -4.2739191 -4.2753692 -4.2280889 -4.1935868 -4.2141929 -4.2555957 -4.3038349 -4.3397841 -4.3338122 -4.2915263 -4.2537475][-4.1751413 -4.1839371 -4.2153912 -4.2523212 -4.268405 -4.22487 -4.135972 -4.0884075 -4.1313758 -4.2078009 -4.2814121 -4.3266921 -4.3225832 -4.2810297 -4.2464948][-4.2261715 -4.2442245 -4.2561617 -4.251574 -4.2167115 -4.1146622 -3.9740617 -3.921766 -4.0112214 -4.1387739 -4.241972 -4.2930303 -4.2873883 -4.2507124 -4.2258096][-4.2602773 -4.2789154 -4.274919 -4.2387619 -4.1634922 -4.0082378 -3.8219261 -3.7787132 -3.9199176 -4.0849833 -4.204215 -4.2570972 -4.251544 -4.2170687 -4.2005892][-4.28758 -4.29988 -4.28555 -4.2371774 -4.1576104 -4.0142803 -3.864074 -3.8510191 -3.9793863 -4.1197176 -4.2165632 -4.250339 -4.2319603 -4.1941261 -4.1856127][-4.2983847 -4.3068433 -4.2905416 -4.2488847 -4.1907587 -4.0974569 -4.0224752 -4.0328031 -4.1178036 -4.2106509 -4.2683458 -4.2714787 -4.2349186 -4.195365 -4.1922631][-4.2979145 -4.3023725 -4.288506 -4.259789 -4.22475 -4.174777 -4.1450939 -4.16419 -4.220221 -4.2811494 -4.3122621 -4.2966847 -4.2538505 -4.2159081 -4.2135115][-4.30215 -4.30435 -4.2937956 -4.2778268 -4.2603016 -4.2314591 -4.2168288 -4.2349911 -4.2749043 -4.3150949 -4.3318996 -4.3148646 -4.27963 -4.247849 -4.2412806][-4.3137054 -4.312469 -4.3061595 -4.2999587 -4.2930312 -4.2761312 -4.26636 -4.2776985 -4.3027868 -4.3245978 -4.33065 -4.3152714 -4.2922125 -4.2696872 -4.2620831][-4.3235126 -4.3192091 -4.31406 -4.3116369 -4.3100033 -4.3034515 -4.3011394 -4.3071642 -4.3172917 -4.3235054 -4.3205118 -4.3083096 -4.295383 -4.2813144 -4.2773309][-4.3245664 -4.317821 -4.3144145 -4.3151627 -4.31801 -4.3183107 -4.3194995 -4.3206711 -4.3209486 -4.3172822 -4.3111582 -4.3036056 -4.29803 -4.2915568 -4.2899442]]...]
INFO - root - 2017-12-06 09:38:41.084361: step 1710, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:38s remains)
INFO - root - 2017-12-06 09:38:43.230583: step 1720, loss = 2.08, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:16s remains)
INFO - root - 2017-12-06 09:38:45.435618: step 1730, loss = 2.07, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:30s remains)
INFO - root - 2017-12-06 09:38:47.604171: step 1740, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:42s remains)
INFO - root - 2017-12-06 09:38:49.806978: step 1750, loss = 2.11, batch loss = 2.05 (35.8 examples/sec; 0.224 sec/batch; 20h:33m:05s remains)
INFO - root - 2017-12-06 09:38:51.905427: step 1760, loss = 2.09, batch loss = 2.04 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:44s remains)
INFO - root - 2017-12-06 09:38:54.114400: step 1770, loss = 2.08, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:49s remains)
INFO - root - 2017-12-06 09:38:56.260233: step 1780, loss = 2.11, batch loss = 2.05 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:44s remains)
INFO - root - 2017-12-06 09:38:58.489911: step 1790, loss = 2.07, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:05s remains)
INFO - root - 2017-12-06 09:39:00.615358: step 1800, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:26s remains)
2017-12-06 09:39:00.975676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.179069 -4.2163963 -4.2326145 -4.2165527 -4.1871462 -4.1705976 -4.149828 -4.092164 -4.059268 -4.0983191 -4.1599197 -4.2077479 -4.2312088 -4.2302561 -4.2211795][-4.1936278 -4.2331553 -4.2514281 -4.2348485 -4.2080307 -4.1908264 -4.1636467 -4.0937233 -4.0548792 -4.104167 -4.17496 -4.2285471 -4.2498021 -4.2468619 -4.2346134][-4.2125106 -4.2422652 -4.2547221 -4.2388287 -4.2185698 -4.2022114 -4.1712327 -4.09302 -4.0507741 -4.112093 -4.1892643 -4.2456083 -4.2659721 -4.2599835 -4.2443051][-4.2240043 -4.2363153 -4.2359228 -4.2194624 -4.2044697 -4.1917472 -4.1611938 -4.079659 -4.0380116 -4.109767 -4.187562 -4.2423277 -4.2612467 -4.2551208 -4.240891][-4.2227159 -4.2191744 -4.2049928 -4.1867509 -4.17412 -4.1639466 -4.129374 -4.0420012 -4.0026751 -4.0871763 -4.1672635 -4.2206273 -4.2384181 -4.2351794 -4.2256064][-4.2069263 -4.1893806 -4.1690068 -4.1552744 -4.1467061 -4.1341953 -4.0871835 -3.981864 -3.9394567 -4.0405121 -4.1301594 -4.1843371 -4.2037177 -4.2042832 -4.1976523][-4.174963 -4.1530318 -4.1376829 -4.1356945 -4.1334319 -4.1104031 -4.0382547 -3.901233 -3.848114 -3.9775765 -4.0882277 -4.1474857 -4.1690702 -4.1709013 -4.1667442][-4.1392956 -4.1293564 -4.1292233 -4.13915 -4.1374483 -4.0995464 -4.0015049 -3.8324258 -3.7740662 -3.9336786 -4.06937 -4.1415591 -4.1680169 -4.1671686 -4.1579285][-4.1229925 -4.1307607 -4.1471605 -4.1635995 -4.1598163 -4.1137242 -4.0141635 -3.8510737 -3.8075793 -3.9682345 -4.1055427 -4.17726 -4.1998644 -4.191699 -4.1717591][-4.1372461 -4.1582417 -4.1850944 -4.2022309 -4.1988225 -4.158926 -4.0814424 -3.9630485 -3.9419305 -4.0629492 -4.16522 -4.2172413 -4.2296457 -4.2143297 -4.1896639][-4.1722908 -4.2011733 -4.2295303 -4.2461762 -4.2416978 -4.2099304 -4.1573792 -4.0788426 -4.0639811 -4.1398153 -4.2067327 -4.2413955 -4.2436996 -4.219079 -4.1947536][-4.2112622 -4.235302 -4.2585106 -4.275012 -4.2696242 -4.2418742 -4.2071981 -4.1530375 -4.1344171 -4.1757922 -4.2170415 -4.2417226 -4.2403741 -4.2108274 -4.1848545][-4.2330108 -4.2411375 -4.2551246 -4.273664 -4.2706556 -4.2473683 -4.2247157 -4.1869326 -4.1646857 -4.1860504 -4.2126045 -4.22872 -4.2268171 -4.19438 -4.1651134][-4.2347026 -4.2232175 -4.2279921 -4.2496309 -4.2515035 -4.2338076 -4.2188563 -4.1939359 -4.174974 -4.1901932 -4.2106051 -4.2196646 -4.2117162 -4.1738367 -4.1432042][-4.2322955 -4.2028284 -4.1944618 -4.2127633 -4.2173491 -4.1999621 -4.1913319 -4.1811433 -4.1732149 -4.1949639 -4.216054 -4.2209044 -4.2050533 -4.161201 -4.1313143]]...]
INFO - root - 2017-12-06 09:39:03.123865: step 1810, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:45s remains)
INFO - root - 2017-12-06 09:39:05.306990: step 1820, loss = 2.09, batch loss = 2.03 (33.6 examples/sec; 0.238 sec/batch; 21h:51m:22s remains)
INFO - root - 2017-12-06 09:39:07.491488: step 1830, loss = 2.05, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:50s remains)
INFO - root - 2017-12-06 09:39:09.659302: step 1840, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:24s remains)
INFO - root - 2017-12-06 09:39:11.826313: step 1850, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:16s remains)
INFO - root - 2017-12-06 09:39:13.987823: step 1860, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:10s remains)
INFO - root - 2017-12-06 09:39:16.186310: step 1870, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:07s remains)
INFO - root - 2017-12-06 09:39:18.346688: step 1880, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:14m:31s remains)
INFO - root - 2017-12-06 09:39:20.486236: step 1890, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:00s remains)
INFO - root - 2017-12-06 09:39:22.639633: step 1900, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:31s remains)
2017-12-06 09:39:23.051159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2412024 -4.2208352 -4.2130322 -4.2232456 -4.242744 -4.2585316 -4.2790589 -4.2934928 -4.3092675 -4.3004179 -4.2720737 -4.260097 -4.2514958 -4.2453394 -4.2549229][-4.2164259 -4.1785088 -4.1585679 -4.1649265 -4.1941605 -4.2264795 -4.2544289 -4.2736034 -4.2977138 -4.2893753 -4.260006 -4.25542 -4.2518826 -4.2453732 -4.2511926][-4.1940379 -4.1449375 -4.108036 -4.1001096 -4.1312413 -4.1854892 -4.2288189 -4.2542849 -4.2839284 -4.2786417 -4.2505126 -4.2556152 -4.26192 -4.2568707 -4.2573][-4.1679697 -4.1137543 -4.0593014 -4.0345359 -4.0592556 -4.1281357 -4.1871057 -4.2236609 -4.2600136 -4.2624445 -4.2466793 -4.2582493 -4.2671967 -4.2612338 -4.2572975][-4.1374006 -4.0901108 -4.0320287 -3.9877362 -3.9999096 -4.0640588 -4.1243973 -4.1754212 -4.2288418 -4.2460585 -4.2442064 -4.2587051 -4.2669191 -4.2560935 -4.24975][-4.0977855 -4.0641212 -4.0130396 -3.9595757 -3.9520712 -3.9961882 -4.0482912 -4.1048088 -4.1668277 -4.1955304 -4.2088976 -4.2345366 -4.2485948 -4.24555 -4.24512][-4.0586581 -4.0343385 -3.9932535 -3.9455013 -3.9289773 -3.9516771 -3.9882483 -4.0385604 -4.1053009 -4.1377745 -4.1551046 -4.1908402 -4.2168875 -4.2287521 -4.2407331][-4.039958 -4.02059 -3.9899511 -3.9545724 -3.9383972 -3.9428339 -3.9578664 -3.9888041 -4.0435238 -4.0736213 -4.0959916 -4.1409383 -4.1803179 -4.203887 -4.22497][-4.0367303 -4.01597 -3.9898367 -3.9592977 -3.9439726 -3.9400873 -3.9390724 -3.952322 -3.9891055 -4.0141144 -4.04198 -4.0972757 -4.1445994 -4.1739173 -4.201324][-4.0629039 -4.0403557 -4.0166879 -3.9881408 -3.9681873 -3.954988 -3.9454691 -3.9480684 -3.9693902 -3.9839065 -4.0081954 -4.0642767 -4.1138229 -4.1472182 -4.1781106][-4.1220589 -4.1014791 -4.0823722 -4.0597839 -4.0412421 -4.0270977 -4.0179248 -4.0167279 -4.0262141 -4.0232954 -4.0300565 -4.0758796 -4.1186452 -4.1483078 -4.1738992][-4.1945977 -4.1797829 -4.1653719 -4.1489358 -4.1354094 -4.1255045 -4.1185694 -4.11557 -4.1170683 -4.1034136 -4.0949836 -4.1209645 -4.1468663 -4.1677608 -4.1867261][-4.2608376 -4.2493925 -4.2402511 -4.22984 -4.2206593 -4.2146673 -4.2104192 -4.2070479 -4.2058783 -4.192996 -4.1807628 -4.19018 -4.2006021 -4.2094684 -4.2204838][-4.3026052 -4.2951841 -4.2910895 -4.2859573 -4.28145 -4.2790322 -4.2772679 -4.2754912 -4.2740445 -4.2648263 -4.2536182 -4.2532883 -4.2533169 -4.25392 -4.2594285][-4.3161449 -4.3125906 -4.3124881 -4.3111038 -4.309598 -4.3094172 -4.3095717 -4.309341 -4.3087506 -4.3035364 -4.2954836 -4.2923784 -4.2893505 -4.2880425 -4.290853]]...]
INFO - root - 2017-12-06 09:39:25.214098: step 1910, loss = 2.07, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:18s remains)
INFO - root - 2017-12-06 09:39:27.349578: step 1920, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:39s remains)
INFO - root - 2017-12-06 09:39:29.519327: step 1930, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:03s remains)
INFO - root - 2017-12-06 09:39:31.651544: step 1940, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:58s remains)
INFO - root - 2017-12-06 09:39:33.795553: step 1950, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:31s remains)
INFO - root - 2017-12-06 09:39:35.964808: step 1960, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:22s remains)
INFO - root - 2017-12-06 09:39:38.140929: step 1970, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.215 sec/batch; 19h:42m:56s remains)
INFO - root - 2017-12-06 09:39:40.309750: step 1980, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:25m:05s remains)
INFO - root - 2017-12-06 09:39:42.443505: step 1990, loss = 2.10, batch loss = 2.04 (37.1 examples/sec; 0.215 sec/batch; 19h:46m:58s remains)
INFO - root - 2017-12-06 09:39:44.652973: step 2000, loss = 2.09, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:17s remains)
2017-12-06 09:39:44.988145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3354597 -4.3339992 -4.3355584 -4.3390083 -4.341598 -4.3432422 -4.3467069 -4.3495841 -4.3502693 -4.349237 -4.3502636 -4.3530912 -4.3545046 -4.35467 -4.3511038][-4.33099 -4.3275394 -4.3285737 -4.3313737 -4.3331661 -4.3356981 -4.3415675 -4.3426704 -4.3368616 -4.3280525 -4.3256483 -4.3298373 -4.3364081 -4.3424826 -4.343895][-4.3218927 -4.3132405 -4.3105345 -4.310883 -4.3121772 -4.3161268 -4.3225675 -4.3179212 -4.30237 -4.2847567 -4.2777262 -4.28125 -4.2946982 -4.3116188 -4.3234677][-4.3195271 -4.3065004 -4.2996178 -4.2942047 -4.2892356 -4.2885079 -4.2898397 -4.2748637 -4.246243 -4.2156286 -4.1980824 -4.197886 -4.221293 -4.2548795 -4.2844019][-4.3157511 -4.2969408 -4.2840033 -4.2690706 -4.2581115 -4.252459 -4.2492485 -4.2241721 -4.1823797 -4.1386271 -4.1051869 -4.1014132 -4.1352615 -4.1895819 -4.242228][-4.2987113 -4.2723026 -4.2547789 -4.2316985 -4.2163444 -4.2091317 -4.2019463 -4.1648822 -4.1104417 -4.0580029 -4.0187173 -4.0216265 -4.0697417 -4.1437869 -4.2146735][-4.267293 -4.2335858 -4.2084804 -4.1755219 -4.1554661 -4.1506252 -4.1409435 -4.0949664 -4.0374107 -3.9977338 -3.9773796 -3.9972456 -4.0567179 -4.1366196 -4.211103][-4.222909 -4.1806636 -4.142868 -4.0931787 -4.0646777 -4.0677238 -4.0736461 -4.0321465 -3.984956 -3.9766269 -3.9865215 -4.0216908 -4.082036 -4.1567159 -4.2240796][-4.1725349 -4.1190882 -4.0576429 -3.9827764 -3.9582343 -3.9882543 -4.023222 -3.9992111 -3.9756384 -3.9955046 -4.0270166 -4.06974 -4.127326 -4.1928482 -4.2486629][-4.1433115 -4.0813231 -4.0018644 -3.9127471 -3.9064217 -3.9689312 -4.0340939 -4.0371904 -4.0332141 -4.0593705 -4.0943995 -4.1373582 -4.1864738 -4.2365685 -4.2751522][-4.1589222 -4.1051216 -4.0427318 -3.979022 -3.9892473 -4.0523138 -4.1184592 -4.1337695 -4.1386375 -4.1570082 -4.1807289 -4.2104597 -4.2422361 -4.2743454 -4.2966452][-4.2123919 -4.1775641 -4.1456084 -4.1196671 -4.1345334 -4.1765814 -4.2241096 -4.234848 -4.23649 -4.2431374 -4.2530708 -4.2665482 -4.2823982 -4.3011079 -4.31253][-4.2673049 -4.2475271 -4.2344851 -4.2295961 -4.2438173 -4.2691488 -4.2961016 -4.2976575 -4.2944613 -4.2943993 -4.2965612 -4.3004065 -4.3079629 -4.3181019 -4.32299][-4.2977695 -4.2868314 -4.2830443 -4.2842269 -4.2932315 -4.3082023 -4.3210397 -4.3187904 -4.3153744 -4.314415 -4.3148651 -4.31547 -4.3194427 -4.3251176 -4.3272877][-4.3111763 -4.3045592 -4.3046045 -4.3076515 -4.3136153 -4.3217716 -4.3263769 -4.3240805 -4.3223519 -4.3217931 -4.3215446 -4.3217483 -4.3244519 -4.3275266 -4.3283782]]...]
INFO - root - 2017-12-06 09:39:47.131362: step 2010, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:16s remains)
INFO - root - 2017-12-06 09:39:49.344367: step 2020, loss = 2.10, batch loss = 2.04 (34.2 examples/sec; 0.234 sec/batch; 21h:26m:52s remains)
INFO - root - 2017-12-06 09:39:51.547934: step 2030, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:08s remains)
INFO - root - 2017-12-06 09:39:53.716317: step 2040, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:56s remains)
INFO - root - 2017-12-06 09:39:55.879439: step 2050, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:04s remains)
INFO - root - 2017-12-06 09:39:58.022706: step 2060, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:42s remains)
INFO - root - 2017-12-06 09:40:00.169558: step 2070, loss = 2.08, batch loss = 2.02 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:19s remains)
INFO - root - 2017-12-06 09:40:02.372528: step 2080, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:26s remains)
INFO - root - 2017-12-06 09:40:04.604690: step 2090, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:44s remains)
INFO - root - 2017-12-06 09:40:06.781019: step 2100, loss = 2.09, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:46s remains)
2017-12-06 09:40:07.137471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1427293 -4.1540241 -4.1681724 -4.1931205 -4.2144051 -4.2250814 -4.2370338 -4.248723 -4.2627878 -4.2821493 -4.3063889 -4.3189597 -4.323874 -4.3184977 -4.3034687][-4.1572595 -4.1703391 -4.1874533 -4.2108197 -4.2311416 -4.2409964 -4.2514911 -4.2641735 -4.2851648 -4.3132586 -4.3412957 -4.3527737 -4.3532104 -4.3431883 -4.3225803][-4.1736917 -4.1764712 -4.1935167 -4.2153106 -4.2310529 -4.2355471 -4.2388115 -4.246408 -4.2689133 -4.3070717 -4.3433619 -4.3577051 -4.3617754 -4.3516607 -4.3280916][-4.1843143 -4.1773925 -4.1912866 -4.2107296 -4.2205758 -4.2116728 -4.2005029 -4.1966724 -4.2201281 -4.2723289 -4.317687 -4.34061 -4.3521013 -4.3451304 -4.3230309][-4.2006707 -4.1912341 -4.1984353 -4.209919 -4.2048616 -4.1698847 -4.1354828 -4.1228132 -4.1501565 -4.2193274 -4.2773056 -4.3117771 -4.3339243 -4.3389726 -4.3236203][-4.2181349 -4.2185769 -4.2225838 -4.2232051 -4.1999345 -4.1351109 -4.0661697 -4.0401516 -4.076561 -4.1599579 -4.2336769 -4.2850246 -4.3187203 -4.3358107 -4.328793][-4.2367959 -4.2483349 -4.2536607 -4.247026 -4.2112446 -4.1289287 -4.0298133 -3.9818139 -4.0175805 -4.1072149 -4.193327 -4.2623258 -4.3067122 -4.330934 -4.3294597][-4.2475739 -4.262311 -4.2661219 -4.2578511 -4.2183833 -4.1370673 -4.0324912 -3.9715977 -3.9979513 -4.0819426 -4.1666126 -4.2444549 -4.2961826 -4.3180232 -4.314518][-4.239491 -4.2543635 -4.2570591 -4.2481761 -4.2126184 -4.1456141 -4.0530639 -3.9876397 -3.9996018 -4.0717788 -4.1535749 -4.2334323 -4.287662 -4.3033328 -4.2924013][-4.2289143 -4.2455873 -4.2483177 -4.2385659 -4.2075434 -4.15397 -4.0790329 -4.0217919 -4.0272527 -4.0826545 -4.1502576 -4.2262039 -4.2778692 -4.2912216 -4.2792377][-4.2375345 -4.2519774 -4.2504721 -4.2386608 -4.21346 -4.1724787 -4.1113753 -4.0637431 -4.070785 -4.1144209 -4.1653752 -4.2250094 -4.264564 -4.2758365 -4.2712955][-4.2559423 -4.2624588 -4.2554283 -4.2431097 -4.2247076 -4.196794 -4.1511936 -4.11515 -4.1221266 -4.1571741 -4.1927228 -4.2303023 -4.2493329 -4.2517781 -4.2524753][-4.2804403 -4.2822952 -4.2722206 -4.2601786 -4.246233 -4.2261033 -4.1942635 -4.172256 -4.1802568 -4.2087479 -4.2289367 -4.2404385 -4.2319474 -4.2150583 -4.2121716][-4.2844748 -4.289403 -4.2858391 -4.2815704 -4.2765312 -4.2664852 -4.2476048 -4.2379179 -4.2454147 -4.2631841 -4.2686877 -4.2577767 -4.2225928 -4.18546 -4.1772509][-4.2688656 -4.2748117 -4.2784181 -4.28482 -4.2912288 -4.2937794 -4.2888517 -4.2879515 -4.293808 -4.301394 -4.295217 -4.2694454 -4.2195649 -4.1716928 -4.158349]]...]
INFO - root - 2017-12-06 09:40:09.288692: step 2110, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:46s remains)
INFO - root - 2017-12-06 09:40:11.484419: step 2120, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:46s remains)
INFO - root - 2017-12-06 09:40:13.679298: step 2130, loss = 2.05, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:07s remains)
INFO - root - 2017-12-06 09:40:15.838344: step 2140, loss = 2.09, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:58s remains)
INFO - root - 2017-12-06 09:40:18.019327: step 2150, loss = 2.09, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:20s remains)
INFO - root - 2017-12-06 09:40:20.244613: step 2160, loss = 2.08, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:52s remains)
INFO - root - 2017-12-06 09:40:22.470600: step 2170, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:18s remains)
INFO - root - 2017-12-06 09:40:24.645209: step 2180, loss = 2.09, batch loss = 2.03 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:37s remains)
INFO - root - 2017-12-06 09:40:26.819360: step 2190, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:35s remains)
INFO - root - 2017-12-06 09:40:28.937826: step 2200, loss = 2.07, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:12m:48s remains)
2017-12-06 09:40:29.247279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3014774 -4.2979364 -4.29113 -4.2833381 -4.2720861 -4.2512274 -4.2200732 -4.1805649 -4.1696739 -4.1840739 -4.1950374 -4.2065945 -4.222755 -4.2370648 -4.2483892][-4.2918158 -4.28558 -4.2816472 -4.278388 -4.2681346 -4.2449932 -4.2117176 -4.1695542 -4.1598 -4.1852422 -4.2065105 -4.2258348 -4.2492118 -4.2708812 -4.2814541][-4.2782588 -4.2721314 -4.2728629 -4.2743373 -4.2609315 -4.23328 -4.1959314 -4.1515489 -4.1475124 -4.1839685 -4.2212062 -4.2540736 -4.2830677 -4.3070326 -4.3122363][-4.2694497 -4.26666 -4.273272 -4.2760286 -4.2541871 -4.2144876 -4.1618285 -4.1106133 -4.1123595 -4.1632109 -4.222167 -4.2734094 -4.3104353 -4.3346896 -4.3332605][-4.2671781 -4.2676935 -4.2777104 -4.2768188 -4.2438579 -4.1879673 -4.110786 -4.0451403 -4.051722 -4.1228676 -4.2109537 -4.2843094 -4.3320756 -4.3542919 -4.3457003][-4.2673931 -4.272747 -4.2847767 -4.2780628 -4.2343841 -4.16016 -4.0557356 -3.9701626 -3.9796126 -4.0755849 -4.1937485 -4.2853765 -4.3389654 -4.3554959 -4.3378286][-4.2694244 -4.2812076 -4.2935739 -4.2823496 -4.2323833 -4.1464224 -4.016737 -3.9037511 -3.9028354 -4.0160823 -4.1573706 -4.2668886 -4.3275776 -4.3404078 -4.31753][-4.2794566 -4.2955542 -4.3043623 -4.2877874 -4.2369671 -4.1495123 -4.0084548 -3.871366 -3.8484719 -3.9631817 -4.116127 -4.2391467 -4.3090153 -4.3225989 -4.297153][-4.2988744 -4.3146744 -4.3174143 -4.2955847 -4.2465811 -4.16777 -4.0343204 -3.8921728 -3.8498917 -3.949482 -4.0971208 -4.2221923 -4.2948756 -4.31158 -4.2867522][-4.3205395 -4.3333268 -4.3315825 -4.3081903 -4.2640958 -4.1968312 -4.0868487 -3.9614692 -3.9117823 -3.9818735 -4.1070032 -4.2196155 -4.2856274 -4.3035054 -4.2831697][-4.3374963 -4.3438258 -4.3390417 -4.3179717 -4.2813969 -4.2279415 -4.1472859 -4.0497866 -4.00112 -4.0399394 -4.1344852 -4.2260375 -4.2787938 -4.2960677 -4.2819133][-4.3471889 -4.3446417 -4.3351841 -4.3170524 -4.2900934 -4.2514381 -4.1953855 -4.1274467 -4.0897923 -4.1083193 -4.1727972 -4.24037 -4.2782421 -4.2919621 -4.2816529][-4.347446 -4.3363056 -4.3234425 -4.3083267 -4.287941 -4.2590127 -4.2201428 -4.1794567 -4.1614795 -4.1761079 -4.2211809 -4.2674561 -4.2906346 -4.298315 -4.2885981][-4.3418131 -4.325037 -4.3109431 -4.2986684 -4.2830968 -4.2607703 -4.2337966 -4.211432 -4.2073121 -4.2235913 -4.2578721 -4.2882152 -4.3015809 -4.3060627 -4.2987728][-4.3324723 -4.3144612 -4.3001366 -4.2891054 -4.2773108 -4.2607136 -4.2421594 -4.2296929 -4.2296886 -4.2423487 -4.2667933 -4.2871375 -4.2973332 -4.3033824 -4.3024664]]...]
INFO - root - 2017-12-06 09:40:31.394338: step 2210, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-06 09:40:33.544507: step 2220, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:09s remains)
INFO - root - 2017-12-06 09:40:35.744072: step 2230, loss = 2.10, batch loss = 2.04 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:32s remains)
INFO - root - 2017-12-06 09:40:37.928105: step 2240, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:29s remains)
INFO - root - 2017-12-06 09:40:40.080565: step 2250, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-06 09:40:42.219234: step 2260, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:01s remains)
INFO - root - 2017-12-06 09:40:44.385140: step 2270, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.215 sec/batch; 19h:41m:38s remains)
INFO - root - 2017-12-06 09:40:46.532304: step 2280, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.209 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-06 09:40:48.706458: step 2290, loss = 2.09, batch loss = 2.03 (33.1 examples/sec; 0.242 sec/batch; 22h:10m:10s remains)
INFO - root - 2017-12-06 09:40:50.883677: step 2300, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:25s remains)
2017-12-06 09:40:51.187538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2733326 -4.2695413 -4.2554407 -4.2349119 -4.219028 -4.2062807 -4.1998577 -4.2008886 -4.1985569 -4.1904073 -4.1677389 -4.1363382 -4.1328077 -4.1638007 -4.1992145][-4.2530479 -4.2475634 -4.2328358 -4.2149205 -4.2019114 -4.1914964 -4.1820593 -4.1773033 -4.1662636 -4.1578574 -4.1462092 -4.1297693 -4.1398411 -4.1741729 -4.2076359][-4.2437367 -4.2324271 -4.2141609 -4.1985879 -4.18564 -4.1671467 -4.1479225 -4.1327195 -4.1085811 -4.0990691 -4.115212 -4.1352887 -4.1628542 -4.2023239 -4.237977][-4.2409554 -4.2205348 -4.1973681 -4.1778545 -4.1593857 -4.1362033 -4.1118484 -4.0881858 -4.0573907 -4.0600109 -4.1001863 -4.1450987 -4.1823545 -4.2241 -4.2614145][-4.2375164 -4.2164435 -4.1915894 -4.1680193 -4.14434 -4.1172261 -4.090611 -4.0689287 -4.0537653 -4.0749617 -4.1209292 -4.1678839 -4.2058778 -4.242672 -4.27499][-4.248641 -4.2304854 -4.2044196 -4.1729546 -4.1356068 -4.0942655 -4.0554142 -4.0424523 -4.0644727 -4.1110897 -4.1572957 -4.1995974 -4.2357483 -4.2649026 -4.2875896][-4.2585135 -4.2485013 -4.2206984 -4.1753831 -4.1130323 -4.0376015 -3.9690914 -3.9610462 -4.0320868 -4.1150465 -4.1741385 -4.2247849 -4.2657452 -4.2875896 -4.2959151][-4.2498822 -4.2439346 -4.2147508 -4.1558719 -4.0699954 -3.9653726 -3.8740716 -3.8848019 -4.0068846 -4.1194029 -4.1869264 -4.2449641 -4.2868829 -4.302866 -4.3017511][-4.2146792 -4.2081161 -4.1824045 -4.1300015 -4.0566716 -3.9773474 -3.91947 -3.9506917 -4.063921 -4.1581883 -4.2150331 -4.2640862 -4.2960148 -4.3047147 -4.3022547][-4.1553707 -4.1596327 -4.1585217 -4.1437669 -4.1146836 -4.0806952 -4.0646086 -4.0971022 -4.1632051 -4.2151127 -4.2506428 -4.2792277 -4.2944684 -4.299098 -4.302763][-4.131289 -4.1556425 -4.1744432 -4.17995 -4.1713295 -4.1575036 -4.1581154 -4.1844258 -4.22012 -4.2495308 -4.2759013 -4.2927089 -4.2965541 -4.3002448 -4.308558][-4.1524167 -4.1798296 -4.1971254 -4.2019677 -4.2002072 -4.1935558 -4.2014866 -4.2216773 -4.2406325 -4.259975 -4.2834225 -4.2976441 -4.2989798 -4.3047004 -4.3138947][-4.1826615 -4.2033629 -4.2162385 -4.2197127 -4.2157011 -4.2102942 -4.21937 -4.2353849 -4.2478395 -4.262836 -4.2838674 -4.2978888 -4.3022966 -4.3103509 -4.3186932][-4.1992879 -4.2124772 -4.2218637 -4.2259297 -4.2231374 -4.2180033 -4.2233849 -4.2369509 -4.2524652 -4.26634 -4.2853441 -4.3016095 -4.3098984 -4.31683 -4.3213696][-4.1972561 -4.2089596 -4.2188964 -4.2222638 -4.220325 -4.2225084 -4.2308278 -4.2434063 -4.2603092 -4.2747536 -4.2927489 -4.309526 -4.3168025 -4.3200817 -4.3212409]]...]
INFO - root - 2017-12-06 09:40:53.336962: step 2310, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:51s remains)
INFO - root - 2017-12-06 09:40:55.478141: step 2320, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:29s remains)
INFO - root - 2017-12-06 09:40:57.628261: step 2330, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:09s remains)
INFO - root - 2017-12-06 09:40:59.774647: step 2340, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:26s remains)
INFO - root - 2017-12-06 09:41:01.919030: step 2350, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-06 09:41:04.119021: step 2360, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:05s remains)
INFO - root - 2017-12-06 09:41:06.269709: step 2370, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:41m:15s remains)
INFO - root - 2017-12-06 09:41:08.413508: step 2380, loss = 2.05, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:22s remains)
INFO - root - 2017-12-06 09:41:10.579041: step 2390, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:38s remains)
INFO - root - 2017-12-06 09:41:12.729254: step 2400, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:48m:49s remains)
2017-12-06 09:41:13.137136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.218256 -4.2326112 -4.247858 -4.2523403 -4.2636509 -4.27737 -4.2705531 -4.24871 -4.2387609 -4.2534332 -4.2716942 -4.2872653 -4.3037667 -4.3158889 -4.3214207][-4.2091351 -4.23189 -4.2540174 -4.2678127 -4.2796049 -4.2890654 -4.2765942 -4.2504954 -4.2372861 -4.24695 -4.2610674 -4.2812657 -4.3063116 -4.319716 -4.3239884][-4.19738 -4.2259426 -4.2577729 -4.2820854 -4.2965989 -4.2992225 -4.2781105 -4.2477026 -4.2330804 -4.2369165 -4.2462564 -4.268425 -4.2963948 -4.310956 -4.3167663][-4.2062473 -4.2327819 -4.2633796 -4.2854438 -4.2933488 -4.2835717 -4.2570386 -4.2290068 -4.21845 -4.2225862 -4.2342482 -4.2573528 -4.2833366 -4.2995052 -4.3107381][-4.2258139 -4.2470579 -4.2680035 -4.2774491 -4.268538 -4.2430229 -4.2091613 -4.1830025 -4.1783895 -4.1927867 -4.2157292 -4.2425461 -4.2693796 -4.2919173 -4.3097167][-4.2249522 -4.2392054 -4.24623 -4.2372518 -4.2104793 -4.169311 -4.1211572 -4.0873284 -4.0896487 -4.1263723 -4.1718283 -4.2101502 -4.248806 -4.284852 -4.3107829][-4.1943769 -4.2002068 -4.1936932 -4.1710696 -4.1348567 -4.0795951 -4.011292 -3.9567626 -3.9610567 -4.0236282 -4.091279 -4.1486673 -4.2088666 -4.2638383 -4.3005157][-4.1488237 -4.1523652 -4.1422815 -4.1157374 -4.0827465 -4.0306935 -3.9565389 -3.8849301 -3.8808455 -3.9473095 -4.0191374 -4.089725 -4.1652541 -4.2333236 -4.280633][-4.1262026 -4.1343603 -4.1325774 -4.1172028 -4.0994387 -4.0680413 -4.0128174 -3.9514096 -3.9393473 -3.9766128 -4.026679 -4.0883541 -4.1576619 -4.2200985 -4.268537][-4.1493421 -4.1582146 -4.1647964 -4.16229 -4.1573763 -4.1438484 -4.1104188 -4.0677929 -4.0529752 -4.0640817 -4.0889668 -4.1298165 -4.1788321 -4.226768 -4.2686644][-4.2075372 -4.2132263 -4.220108 -4.2206712 -4.2196841 -4.2141542 -4.1971726 -4.1723394 -4.1594381 -4.158711 -4.1668034 -4.186748 -4.21478 -4.2472744 -4.2785921][-4.2657971 -4.2683525 -4.2715726 -4.2717109 -4.2721 -4.270051 -4.2639136 -4.2539167 -4.2466936 -4.2429123 -4.2415161 -4.2468681 -4.2582731 -4.2758384 -4.2941346][-4.3002272 -4.3013191 -4.3024821 -4.3025994 -4.30457 -4.3072004 -4.308394 -4.3061104 -4.301887 -4.2971606 -4.2927418 -4.294095 -4.2992244 -4.3059077 -4.3133025][-4.3213758 -4.3216 -4.3216538 -4.3211994 -4.3233962 -4.3279214 -4.331605 -4.3325086 -4.3302183 -4.3261256 -4.3221054 -4.322782 -4.3259473 -4.3279581 -4.3287134][-4.3304615 -4.3294411 -4.329124 -4.3281603 -4.3286161 -4.330821 -4.3326573 -4.3330288 -4.3317542 -4.3303156 -4.3294954 -4.3310385 -4.3338423 -4.3349948 -4.3345528]]...]
INFO - root - 2017-12-06 09:41:15.280386: step 2410, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:28s remains)
INFO - root - 2017-12-06 09:41:17.412019: step 2420, loss = 2.09, batch loss = 2.04 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:19s remains)
INFO - root - 2017-12-06 09:41:19.573108: step 2430, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:27s remains)
INFO - root - 2017-12-06 09:41:21.732517: step 2440, loss = 2.05, batch loss = 1.99 (38.0 examples/sec; 0.210 sec/batch; 19h:16m:38s remains)
INFO - root - 2017-12-06 09:41:23.898072: step 2450, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:11s remains)
INFO - root - 2017-12-06 09:41:26.081020: step 2460, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:00s remains)
INFO - root - 2017-12-06 09:41:28.245193: step 2470, loss = 2.08, batch loss = 2.03 (38.0 examples/sec; 0.211 sec/batch; 19h:18m:47s remains)
INFO - root - 2017-12-06 09:41:30.381184: step 2480, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:47s remains)
INFO - root - 2017-12-06 09:41:32.521854: step 2490, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:00s remains)
INFO - root - 2017-12-06 09:41:34.658820: step 2500, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:08s remains)
2017-12-06 09:41:35.009297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1565347 -4.1428332 -4.1455579 -4.1623 -4.181016 -4.2090988 -4.2367024 -4.25733 -4.2725921 -4.2761374 -4.2579212 -4.2264104 -4.1836214 -4.1393957 -4.1024461][-4.1281343 -4.1256495 -4.1382313 -4.1590509 -4.1808224 -4.2064323 -4.2280231 -4.2397709 -4.25208 -4.2653503 -4.2632613 -4.2472348 -4.2104144 -4.1608372 -4.108108][-4.131474 -4.1436977 -4.1623449 -4.1756859 -4.1911626 -4.2051039 -4.2096186 -4.2020164 -4.2039714 -4.2265444 -4.2420468 -4.2465396 -4.2236891 -4.179882 -4.1235247][-4.1573439 -4.1798768 -4.1963692 -4.2019224 -4.2059574 -4.2035427 -4.1801496 -4.1381721 -4.1218357 -4.1577296 -4.2021194 -4.2281609 -4.2179642 -4.1836133 -4.1372876][-4.1587343 -4.1816225 -4.1968837 -4.202055 -4.2016616 -4.1881113 -4.139029 -4.0601563 -4.0269308 -4.0783458 -4.1524758 -4.1955285 -4.2021904 -4.1821485 -4.1518788][-4.1601911 -4.1735539 -4.1822085 -4.1787562 -4.1726007 -4.1443224 -4.0674434 -3.957629 -3.9194386 -3.9926262 -4.0958414 -4.1601458 -4.1836843 -4.1770449 -4.1598558][-4.1437893 -4.1407037 -4.1400504 -4.1288071 -4.1077108 -4.0558219 -3.955406 -3.8268623 -3.8125086 -3.9242678 -4.0500765 -4.1325636 -4.1636877 -4.1693325 -4.1635633][-4.1307559 -4.1180696 -4.0996208 -4.0730495 -4.03912 -3.9778078 -3.8782883 -3.7656436 -3.7860506 -3.921149 -4.0500288 -4.1344957 -4.1635985 -4.1716552 -4.1740251][-4.1274271 -4.11568 -4.0867357 -4.0563064 -4.0335608 -3.9983726 -3.9446781 -3.8884926 -3.9159479 -4.0186758 -4.1124811 -4.1695266 -4.1887851 -4.1953578 -4.200181][-4.1279478 -4.1236405 -4.1012168 -4.081986 -4.0763135 -4.0673113 -4.0531173 -4.0335031 -4.0571022 -4.1225295 -4.1768341 -4.2017603 -4.2069497 -4.2147985 -4.2217116][-4.1277447 -4.1271739 -4.1134529 -4.1165547 -4.132431 -4.1397052 -4.1473188 -4.1422944 -4.1570692 -4.1983228 -4.2289 -4.2315278 -4.2218332 -4.2203264 -4.2233729][-4.129 -4.1326852 -4.13105 -4.1516533 -4.1786041 -4.1878977 -4.1991587 -4.2060452 -4.2159314 -4.237391 -4.250803 -4.2416739 -4.2236814 -4.2084031 -4.2051349][-4.1141195 -4.1316419 -4.1491642 -4.1792064 -4.2020893 -4.2009897 -4.2040434 -4.2191176 -4.2253847 -4.2367435 -4.2422919 -4.2298098 -4.2091956 -4.1863961 -4.1751122][-4.1035018 -4.1314492 -4.1588678 -4.1800184 -4.1836782 -4.167068 -4.1631227 -4.1843119 -4.1927428 -4.2017307 -4.2056456 -4.1953688 -4.1806726 -4.1532412 -4.1313915][-4.1120663 -4.1383247 -4.1664419 -4.1744375 -4.1515145 -4.1113958 -4.1006842 -4.1282082 -4.1456857 -4.1532617 -4.1523838 -4.1419687 -4.1319089 -4.1077995 -4.0826683]]...]
INFO - root - 2017-12-06 09:41:37.173234: step 2510, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:37s remains)
INFO - root - 2017-12-06 09:41:39.364322: step 2520, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 20h:51m:18s remains)
INFO - root - 2017-12-06 09:41:41.523425: step 2530, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:12s remains)
INFO - root - 2017-12-06 09:41:43.673634: step 2540, loss = 2.07, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:10m:30s remains)
INFO - root - 2017-12-06 09:41:45.825858: step 2550, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.214 sec/batch; 19h:34m:09s remains)
INFO - root - 2017-12-06 09:41:47.990399: step 2560, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:02s remains)
INFO - root - 2017-12-06 09:41:50.123097: step 2570, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:39m:21s remains)
INFO - root - 2017-12-06 09:41:52.278807: step 2580, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-06 09:41:54.435040: step 2590, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:03s remains)
INFO - root - 2017-12-06 09:41:56.580185: step 2600, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:25s remains)
2017-12-06 09:41:56.887443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3527651 -4.357945 -4.3626676 -4.3644657 -4.3630633 -4.3611507 -4.3603 -4.3595452 -4.3589711 -4.3583412 -4.3570261 -4.3553767 -4.3522811 -4.3500662 -4.3502345][-4.3137021 -4.3226514 -4.335391 -4.345202 -4.3498926 -4.350996 -4.3509364 -4.3500595 -4.3498912 -4.349751 -4.3487282 -4.3459697 -4.339211 -4.3321753 -4.3282781][-4.2533116 -4.2620358 -4.2798023 -4.2964163 -4.30775 -4.3138885 -4.3172016 -4.3190174 -4.3224831 -4.3273058 -4.3310227 -4.3312011 -4.3232703 -4.3110538 -4.3014245][-4.174346 -4.1797314 -4.202867 -4.227756 -4.2478013 -4.260294 -4.2655268 -4.2691164 -4.2772379 -4.2885914 -4.2982616 -4.3023238 -4.2939925 -4.2787123 -4.2668457][-4.1017208 -4.1010752 -4.1251011 -4.1545434 -4.1804366 -4.1962752 -4.2003574 -4.2043982 -4.2183633 -4.2380981 -4.255187 -4.26289 -4.25545 -4.2413092 -4.2320833][-4.0560827 -4.0386233 -4.0503421 -4.0732832 -4.0964241 -4.1074538 -4.1056447 -4.1103616 -4.133575 -4.1668983 -4.1941695 -4.2072606 -4.2043409 -4.1982603 -4.198401][-4.0669174 -4.0329523 -4.02412 -4.0283003 -4.0338969 -4.0262475 -4.0046811 -3.9996929 -4.0274348 -4.0739169 -4.1151552 -4.1392274 -4.1457939 -4.1511903 -4.1643724][-4.1345172 -4.1050916 -4.093133 -4.0851197 -4.072587 -4.0457454 -4.0054026 -3.9835434 -3.997952 -4.0365286 -4.0759292 -4.1025639 -4.1139007 -4.1251383 -4.1455774][-4.201344 -4.1874986 -4.1849694 -4.1800823 -4.1675682 -4.1422606 -4.1064768 -4.0831242 -4.0859838 -4.1081457 -4.13186 -4.1454453 -4.1466231 -4.1491303 -4.1640134][-4.2422 -4.2420092 -4.2507691 -4.2562447 -4.25445 -4.2418537 -4.2203007 -4.2047305 -4.2048483 -4.2167187 -4.2282772 -4.2317276 -4.2223411 -4.2129555 -4.21468][-4.2427015 -4.2502704 -4.2671771 -4.2811012 -4.2890234 -4.2897649 -4.2825942 -4.2761536 -4.2768497 -4.2821851 -4.2865148 -4.2844982 -4.2686329 -4.2513762 -4.2438645][-4.1948595 -4.2030878 -4.2249455 -4.2464123 -4.2638211 -4.2749081 -4.2781563 -4.280797 -4.2842407 -4.2880173 -4.290616 -4.2857876 -4.2626204 -4.2350779 -4.2211714][-4.1173673 -4.1146708 -4.1316013 -4.1548395 -4.1782637 -4.1966705 -4.2065029 -4.2158203 -4.2243333 -4.2313128 -4.2368555 -4.2304749 -4.1989417 -4.1605697 -4.1432328][-4.0520992 -4.0351858 -4.0428095 -4.0619969 -4.0846977 -4.10368 -4.1134038 -4.122808 -4.1321592 -4.141293 -4.1500044 -4.1448135 -4.1102777 -4.0665483 -4.05034][-4.0649176 -4.0428448 -4.0455351 -4.0599093 -4.0776486 -4.0916228 -4.0962172 -4.10102 -4.107203 -4.1146655 -4.1236463 -4.1206656 -4.0926571 -4.05757 -4.0478306]]...]
INFO - root - 2017-12-06 09:41:59.104367: step 2610, loss = 2.09, batch loss = 2.04 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:23s remains)
INFO - root - 2017-12-06 09:42:01.294578: step 2620, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:28s remains)
INFO - root - 2017-12-06 09:42:03.468967: step 2630, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:58s remains)
INFO - root - 2017-12-06 09:42:05.602327: step 2640, loss = 2.09, batch loss = 2.03 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:33s remains)
INFO - root - 2017-12-06 09:42:07.745768: step 2650, loss = 2.09, batch loss = 2.03 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:14s remains)
INFO - root - 2017-12-06 09:42:09.882812: step 2660, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:54s remains)
INFO - root - 2017-12-06 09:42:12.067889: step 2670, loss = 2.11, batch loss = 2.05 (34.1 examples/sec; 0.235 sec/batch; 21h:29m:07s remains)
INFO - root - 2017-12-06 09:42:14.192814: step 2680, loss = 2.10, batch loss = 2.04 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:54s remains)
INFO - root - 2017-12-06 09:42:16.392169: step 2690, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:56s remains)
INFO - root - 2017-12-06 09:42:18.559830: step 2700, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:38m:26s remains)
2017-12-06 09:42:18.911748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464809 -4.2592125 -4.2711954 -4.2828403 -4.2869816 -4.2829971 -4.2801023 -4.2808676 -4.2834072 -4.2839236 -4.2830076 -4.2809486 -4.2795835 -4.2786355 -4.2762694][-4.2791162 -4.284575 -4.2837052 -4.2825642 -4.2785106 -4.26879 -4.2618365 -4.2609992 -4.2646775 -4.2682433 -4.2720203 -4.2766337 -4.2823367 -4.2860894 -4.2859406][-4.2981663 -4.2964869 -4.2850523 -4.2736773 -4.2612882 -4.2466569 -4.2358246 -4.2348251 -4.24152 -4.2478662 -4.2556376 -4.265389 -4.2753692 -4.282259 -4.2825875][-4.2836413 -4.2786264 -4.2611523 -4.2420411 -4.2234054 -4.2069893 -4.1974177 -4.1998959 -4.2120709 -4.2222548 -4.2316437 -4.2435241 -4.2567906 -4.2649956 -4.2618852][-4.2457614 -4.2344751 -4.2112684 -4.1888275 -4.1702991 -4.1551876 -4.1463895 -4.1502695 -4.1676073 -4.1871963 -4.2039771 -4.2190108 -4.2338934 -4.2409678 -4.2327471][-4.187202 -4.1692715 -4.1396022 -4.1138163 -4.0957551 -4.0806475 -4.07097 -4.0741453 -4.0982795 -4.1364379 -4.1702785 -4.1928682 -4.2088704 -4.2148824 -4.2038198][-4.1238794 -4.096364 -4.0589328 -4.028935 -4.0088515 -3.990202 -3.9727094 -3.9683745 -3.9994211 -4.0590153 -4.1147761 -4.1518049 -4.1731019 -4.1798477 -4.1708021][-4.1141191 -4.0845861 -4.0513625 -4.0277429 -4.0117173 -3.9934697 -3.9737091 -3.9671862 -3.997947 -4.0595732 -4.116375 -4.1509285 -4.1653218 -4.1652327 -4.1517143][-4.150538 -4.1304893 -4.1121111 -4.1018963 -4.0983458 -4.0894637 -4.0784025 -4.0772777 -4.1008029 -4.1439252 -4.1839681 -4.2071929 -4.2126346 -4.2067895 -4.1884589][-4.2068558 -4.1954031 -4.1857362 -4.1788077 -4.1769981 -4.17148 -4.1667309 -4.1708775 -4.1885266 -4.2167125 -4.2429667 -4.2561436 -4.2540536 -4.2458558 -4.2296376][-4.2501621 -4.2450781 -4.2352366 -4.2227092 -4.2168608 -4.2137117 -4.2141871 -4.220705 -4.2305717 -4.2463374 -4.26145 -4.2642927 -4.2574229 -4.2514663 -4.2416062][-4.2460008 -4.2452497 -4.2322264 -4.213943 -4.205 -4.2034259 -4.2059183 -4.2116275 -4.215692 -4.2255011 -4.2348824 -4.2331891 -4.2294555 -4.2317662 -4.23252][-4.2366891 -4.2318249 -4.2132134 -4.1900325 -4.1784954 -4.1759214 -4.1779146 -4.1807623 -4.1801639 -4.186008 -4.193346 -4.197104 -4.2045789 -4.2177734 -4.2295861][-4.2343121 -4.2236242 -4.1995234 -4.1751666 -4.16378 -4.16174 -4.1638627 -4.1653914 -4.1636457 -4.1691337 -4.1784973 -4.18729 -4.2009487 -4.2197561 -4.2353835][-4.2214756 -4.20881 -4.1884947 -4.1702571 -4.1637306 -4.1668177 -4.1734858 -4.177135 -4.1751261 -4.1795034 -4.1896057 -4.200994 -4.2164207 -4.23555 -4.2505069]]...]
INFO - root - 2017-12-06 09:42:21.154973: step 2710, loss = 2.13, batch loss = 2.07 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:14s remains)
INFO - root - 2017-12-06 09:42:23.337453: step 2720, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:47s remains)
INFO - root - 2017-12-06 09:42:25.486836: step 2730, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:53s remains)
INFO - root - 2017-12-06 09:42:27.646792: step 2740, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:55s remains)
INFO - root - 2017-12-06 09:42:29.793239: step 2750, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:12m:48s remains)
INFO - root - 2017-12-06 09:42:31.904836: step 2760, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:10m:06s remains)
INFO - root - 2017-12-06 09:42:34.050029: step 2770, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 19h:56m:01s remains)
INFO - root - 2017-12-06 09:42:36.207456: step 2780, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:54s remains)
INFO - root - 2017-12-06 09:42:38.404254: step 2790, loss = 2.10, batch loss = 2.04 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:36s remains)
INFO - root - 2017-12-06 09:42:40.598624: step 2800, loss = 2.07, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:35s remains)
2017-12-06 09:42:40.999867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3367052 -4.3378353 -4.3369207 -4.3356748 -4.33394 -4.3323417 -4.3305674 -4.3280063 -4.3263512 -4.3264227 -4.327507 -4.3295259 -4.3324857 -4.3350687 -4.3370352][-4.3372359 -4.3369927 -4.3349876 -4.332653 -4.33053 -4.329226 -4.3269343 -4.3227291 -4.3205848 -4.320498 -4.3214717 -4.3236728 -4.3273368 -4.3303461 -4.3323526][-4.3298044 -4.3279309 -4.3256788 -4.3250704 -4.325316 -4.3265271 -4.3249888 -4.3196378 -4.3170633 -4.3185992 -4.3209572 -4.3232317 -4.3263311 -4.3280745 -4.3281465][-4.30256 -4.297771 -4.2966671 -4.29994 -4.3038173 -4.308989 -4.3102388 -4.3060918 -4.3048344 -4.3091941 -4.3160534 -4.3206773 -4.3231492 -4.3229127 -4.3202209][-4.2567935 -4.248939 -4.2500339 -4.25815 -4.2670035 -4.2760315 -4.2819157 -4.2813344 -4.283865 -4.2943907 -4.3076982 -4.3149757 -4.3160167 -4.3126392 -4.3066678][-4.2219372 -4.21095 -4.2120647 -4.2207766 -4.2306323 -4.2381015 -4.2415409 -4.2393923 -4.2470765 -4.268312 -4.2891593 -4.2993793 -4.2979813 -4.2914891 -4.2837944][-4.1848097 -4.1711826 -4.1694636 -4.1708694 -4.1741147 -4.1756463 -4.1700811 -4.1549764 -4.1694627 -4.2059188 -4.2363472 -4.2541342 -4.2582779 -4.2535706 -4.2488441][-4.1481538 -4.1285996 -4.118392 -4.1043496 -4.0908947 -4.0766516 -4.0485191 -4.00935 -4.0330772 -4.0884008 -4.1345019 -4.1681213 -4.1862226 -4.1936426 -4.2015753][-4.1228418 -4.09869 -4.0779943 -4.0513587 -4.0270233 -3.9996867 -3.9465399 -3.8845172 -3.90829 -3.9719014 -4.0269027 -4.0753551 -4.1102452 -4.135973 -4.1600995][-4.1431384 -4.1280308 -4.1110616 -4.0873847 -4.07298 -4.0604486 -4.0216684 -3.9730735 -3.9823942 -4.0234571 -4.0563145 -4.0900726 -4.1146274 -4.1348958 -4.1560097][-4.1933789 -4.1884165 -4.1805968 -4.1673222 -4.1627297 -4.1626635 -4.143115 -4.115181 -4.1163578 -4.1386232 -4.1555176 -4.1732225 -4.182992 -4.1874413 -4.1936007][-4.246644 -4.2457371 -4.244555 -4.2388473 -4.2383981 -4.241807 -4.2329903 -4.2160964 -4.2134504 -4.2268157 -4.2392397 -4.2524147 -4.2561235 -4.2515221 -4.2455211][-4.2856555 -4.286046 -4.2876129 -4.28647 -4.2885079 -4.2915263 -4.2887721 -4.27521 -4.2685308 -4.2760577 -4.2863703 -4.2979136 -4.299118 -4.2923889 -4.28419][-4.3043613 -4.3052692 -4.307147 -4.3090358 -4.313343 -4.316813 -4.3169155 -4.3075256 -4.3005991 -4.303895 -4.310874 -4.3198018 -4.3222508 -4.3183961 -4.3123131][-4.3142715 -4.3151703 -4.3171163 -4.3190222 -4.3221765 -4.3247528 -4.3239107 -4.3185534 -4.3143144 -4.31594 -4.3192558 -4.323267 -4.3249307 -4.3256006 -4.3248935]]...]
INFO - root - 2017-12-06 09:42:43.151789: step 2810, loss = 2.09, batch loss = 2.03 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:58s remains)
INFO - root - 2017-12-06 09:42:45.278216: step 2820, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:11s remains)
INFO - root - 2017-12-06 09:42:47.439328: step 2830, loss = 2.07, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:26s remains)
INFO - root - 2017-12-06 09:42:49.582911: step 2840, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 20h:00m:16s remains)
INFO - root - 2017-12-06 09:42:51.724358: step 2850, loss = 2.07, batch loss = 2.02 (37.5 examples/sec; 0.214 sec/batch; 19h:33m:32s remains)
INFO - root - 2017-12-06 09:42:53.913136: step 2860, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:54s remains)
INFO - root - 2017-12-06 09:42:56.044024: step 2870, loss = 2.10, batch loss = 2.05 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 09:42:58.223273: step 2880, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:40s remains)
INFO - root - 2017-12-06 09:43:00.423290: step 2890, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:32s remains)
INFO - root - 2017-12-06 09:43:02.573012: step 2900, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:20s remains)
2017-12-06 09:43:02.917576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3211017 -4.3189106 -4.3204088 -4.3209696 -4.3161297 -4.3040195 -4.2881646 -4.2804489 -4.2885251 -4.3073783 -4.325561 -4.33676 -4.3393507 -4.3356581 -4.3300271][-4.3220057 -4.3217592 -4.3262467 -4.3308349 -4.3314304 -4.3271761 -4.3226991 -4.3243685 -4.3338718 -4.3457332 -4.3527474 -4.3519325 -4.3448606 -4.335494 -4.3271794][-4.3261948 -4.32726 -4.3317728 -4.3350778 -4.3349452 -4.331542 -4.3301063 -4.3339462 -4.3437505 -4.355186 -4.36147 -4.3589363 -4.3500066 -4.3389521 -4.3289471][-4.3291378 -4.3288984 -4.328866 -4.3243904 -4.3149085 -4.3016968 -4.2908988 -4.2889647 -4.3016858 -4.3232636 -4.3413582 -4.3485546 -4.3480229 -4.3433475 -4.3354192][-4.3272405 -4.3226156 -4.3139219 -4.2967119 -4.2705569 -4.2365789 -4.2043238 -4.19249 -4.2160058 -4.2599611 -4.3010111 -4.3282008 -4.3426433 -4.3471804 -4.3420286][-4.320076 -4.309063 -4.2892628 -4.2536693 -4.1988316 -4.1278496 -4.0595231 -4.0342441 -4.081903 -4.1636877 -4.2393403 -4.2948775 -4.328733 -4.3438587 -4.341733][-4.3116426 -4.293807 -4.2620873 -4.2024865 -4.1084661 -3.9854922 -3.8678834 -3.8278542 -3.912874 -4.0487809 -4.1680627 -4.2562966 -4.3101697 -4.3350892 -4.3359962][-4.3076963 -4.2863269 -4.246624 -4.1679549 -4.0404615 -3.8710327 -3.7102194 -3.6596212 -3.7823629 -3.9650116 -4.1177998 -4.2275181 -4.2937164 -4.324326 -4.3282528][-4.3128748 -4.2952862 -4.2601504 -4.1830177 -4.0552282 -3.8890471 -3.7391732 -3.6977587 -3.8127885 -3.9806483 -4.1227217 -4.2255745 -4.2895708 -4.3183718 -4.3224516][-4.3231273 -4.3123641 -4.2898545 -4.2359948 -4.1468148 -4.0352464 -3.9399145 -3.9135072 -3.9799304 -4.0838041 -4.1803255 -4.2548862 -4.3023357 -4.321764 -4.3219562][-4.3194742 -4.3104186 -4.2998052 -4.2754569 -4.2341828 -4.1826568 -4.1406736 -4.1293488 -4.15885 -4.2062349 -4.2558885 -4.297914 -4.3245692 -4.3322725 -4.3263631][-4.2841744 -4.2684665 -4.2659883 -4.271884 -4.2754459 -4.271718 -4.2667947 -4.2685194 -4.2794762 -4.2931986 -4.3103523 -4.3287683 -4.3412409 -4.3413482 -4.3314934][-4.2049413 -4.170795 -4.1770029 -4.2186575 -4.2658648 -4.30157 -4.3236175 -4.3356733 -4.338944 -4.3360195 -4.3356481 -4.3408794 -4.3462296 -4.3435411 -4.3328624][-4.0709391 -4.0000687 -4.0138841 -4.1028814 -4.2005615 -4.2755871 -4.3236303 -4.3486719 -4.3540235 -4.3470616 -4.3389654 -4.3387842 -4.3412251 -4.338244 -4.3295307][-3.90638 -3.7894194 -3.8122916 -3.957221 -4.1077409 -4.2205019 -4.2931185 -4.3319316 -4.3444805 -4.3390789 -4.3304329 -4.3298607 -4.3321543 -4.3299165 -4.3240767]]...]
INFO - root - 2017-12-06 09:43:05.056411: step 2910, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:27s remains)
INFO - root - 2017-12-06 09:43:07.191340: step 2920, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:38m:13s remains)
INFO - root - 2017-12-06 09:43:09.358538: step 2930, loss = 2.11, batch loss = 2.05 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:46s remains)
INFO - root - 2017-12-06 09:43:11.531998: step 2940, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:59s remains)
INFO - root - 2017-12-06 09:43:13.714600: step 2950, loss = 2.10, batch loss = 2.04 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:29s remains)
INFO - root - 2017-12-06 09:43:15.879432: step 2960, loss = 2.12, batch loss = 2.07 (38.0 examples/sec; 0.210 sec/batch; 19h:15m:14s remains)
INFO - root - 2017-12-06 09:43:18.048189: step 2970, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:38s remains)
INFO - root - 2017-12-06 09:43:20.213228: step 2980, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 20h:28m:03s remains)
INFO - root - 2017-12-06 09:43:22.402949: step 2990, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:55s remains)
INFO - root - 2017-12-06 09:43:24.597875: step 3000, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:37m:53s remains)
2017-12-06 09:43:24.931872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2401276 -4.235148 -4.2405038 -4.2599359 -4.2811952 -4.2955132 -4.2961435 -4.2868228 -4.266119 -4.2352281 -4.2183647 -4.2190385 -4.2369614 -4.2667589 -4.299984][-4.2139926 -4.2113032 -4.2243094 -4.2495189 -4.2725782 -4.2833385 -4.2781973 -4.2632718 -4.240952 -4.2051454 -4.1771779 -4.1720972 -4.18912 -4.2223406 -4.26134][-4.1773143 -4.1745377 -4.1920342 -4.2204838 -4.2412305 -4.24471 -4.2336164 -4.2149997 -4.1926513 -4.1544471 -4.1173611 -4.1133475 -4.1369991 -4.1782665 -4.2246408][-4.1327949 -4.1242466 -4.1421685 -4.1695604 -4.184876 -4.1806498 -4.1657243 -4.1479192 -4.1327114 -4.0955024 -4.0526223 -4.0513268 -4.086832 -4.1396341 -4.1911082][-4.0810313 -4.0580821 -4.0649629 -4.0830593 -4.0906634 -4.0833216 -4.0708904 -4.0658541 -4.0684538 -4.038475 -3.9896317 -3.9910183 -4.0321741 -4.0900421 -4.1470494][-4.0297208 -3.9916308 -3.9822671 -3.9824769 -3.9778938 -3.9666517 -3.9638338 -3.9792509 -4.00134 -3.98199 -3.9382038 -3.9375367 -3.9753435 -4.0317025 -4.0929823][-3.9913971 -3.9428251 -3.9179683 -3.8972666 -3.8797991 -3.8667493 -3.8742032 -3.9031 -3.937042 -3.9353046 -3.9059894 -3.9028971 -3.9341946 -3.985141 -4.0461626][-3.9643834 -3.910696 -3.877502 -3.8463764 -3.8263807 -3.8204732 -3.8412156 -3.8769317 -3.9107151 -3.9189527 -3.9001331 -3.889771 -3.9098878 -3.952523 -4.0102563][-3.9689391 -3.9193516 -3.8888896 -3.8604875 -3.8438585 -3.8434176 -3.8668618 -3.8969247 -3.9174662 -3.919493 -3.903048 -3.8930521 -3.9088497 -3.9491014 -4.0059872][-4.0142622 -3.9722815 -3.9484479 -3.9284551 -3.9153967 -3.9159856 -3.93151 -3.9482262 -3.954632 -3.9509754 -3.9413757 -3.9388974 -3.9566164 -3.9966898 -4.0498257][-4.0839424 -4.0555515 -4.0415936 -4.0306959 -4.023428 -4.0213761 -4.0274205 -4.0358334 -4.0379763 -4.0344095 -4.0306096 -4.0322437 -4.0480342 -4.0817108 -4.1243587][-4.1576185 -4.1430879 -4.1382556 -4.1353312 -4.1334934 -4.1314683 -4.132761 -4.1377916 -4.141118 -4.1406808 -4.139173 -4.1401 -4.1498685 -4.172998 -4.201685][-4.2218909 -4.216043 -4.21569 -4.2160454 -4.2171135 -4.2160487 -4.2153206 -4.2171335 -4.2187228 -4.2194457 -4.2185183 -4.2189341 -4.2252741 -4.2389493 -4.2561684][-4.2652626 -4.2617455 -4.2619128 -4.2626495 -4.2641339 -4.2643046 -4.2637162 -4.2639089 -4.2643909 -4.2653651 -4.2658687 -4.2672472 -4.2716818 -4.2795205 -4.2880325][-4.290164 -4.2876334 -4.2873883 -4.2880216 -4.2889729 -4.2894478 -4.289361 -4.2893124 -4.2897339 -4.2911062 -4.2929621 -4.2949929 -4.2975039 -4.3005834 -4.3033843]]...]
INFO - root - 2017-12-06 09:43:27.073905: step 3010, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:09s remains)
INFO - root - 2017-12-06 09:43:29.223541: step 3020, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:13s remains)
INFO - root - 2017-12-06 09:43:31.356814: step 3030, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:17s remains)
INFO - root - 2017-12-06 09:43:33.625215: step 3040, loss = 2.07, batch loss = 2.01 (32.0 examples/sec; 0.250 sec/batch; 22h:53m:58s remains)
INFO - root - 2017-12-06 09:43:35.796345: step 3050, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:01s remains)
INFO - root - 2017-12-06 09:43:37.944271: step 3060, loss = 2.09, batch loss = 2.03 (34.4 examples/sec; 0.232 sec/batch; 21h:16m:18s remains)
INFO - root - 2017-12-06 09:43:40.127205: step 3070, loss = 2.09, batch loss = 2.03 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:02s remains)
INFO - root - 2017-12-06 09:43:42.265675: step 3080, loss = 2.09, batch loss = 2.03 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:50s remains)
INFO - root - 2017-12-06 09:43:44.435274: step 3090, loss = 2.11, batch loss = 2.05 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:28s remains)
INFO - root - 2017-12-06 09:43:46.572119: step 3100, loss = 2.08, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:21m:57s remains)
2017-12-06 09:43:46.935314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2552972 -4.2301779 -4.200253 -4.1905289 -4.199667 -4.2013049 -4.1822081 -4.1601162 -4.1629825 -4.1923957 -4.2226653 -4.2278409 -4.2044072 -4.1770272 -4.1797085][-4.2294884 -4.2147141 -4.1969838 -4.1959071 -4.207953 -4.2092967 -4.1934876 -4.1783814 -4.187438 -4.2213092 -4.253613 -4.2607369 -4.2407885 -4.2135224 -4.2070675][-4.2260189 -4.2282691 -4.2297168 -4.2379389 -4.2474713 -4.2459173 -4.2344108 -4.2279162 -4.2388916 -4.2674766 -4.2905817 -4.2922707 -4.2740879 -4.2492371 -4.2330003][-4.2406487 -4.2519808 -4.2625165 -4.27247 -4.2746525 -4.2643137 -4.2537494 -4.2541018 -4.2666984 -4.2866116 -4.2972741 -4.2932692 -4.2795272 -4.2627726 -4.2467194][-4.2430716 -4.2504382 -4.2600126 -4.266108 -4.2571888 -4.2334828 -4.2185597 -4.2255087 -4.2448497 -4.2624335 -4.2665792 -4.2614722 -4.2565556 -4.2509456 -4.2435918][-4.2155747 -4.2091432 -4.2137513 -4.2133369 -4.1892242 -4.1458597 -4.1199403 -4.1354914 -4.17299 -4.2027893 -4.2118187 -4.21111 -4.2136059 -4.2178307 -4.2229853][-4.1689329 -4.1466174 -4.1445012 -4.1378169 -4.1007719 -4.0394039 -4.0033503 -4.0311441 -4.0941119 -4.1483445 -4.1738396 -4.18253 -4.194912 -4.2069616 -4.2181807][-4.1459265 -4.1210995 -4.1201859 -4.1162386 -4.0824523 -4.0247903 -3.9914312 -4.0221357 -4.0938034 -4.1583905 -4.192049 -4.2062631 -4.2194448 -4.2294059 -4.23605][-4.1623693 -4.1523938 -4.162024 -4.1672173 -4.1493516 -4.1129971 -4.091208 -4.1125751 -4.1660228 -4.2157884 -4.2420969 -4.2496948 -4.2514753 -4.251997 -4.2519774][-4.1937704 -4.2006779 -4.2197709 -4.2334146 -4.2304478 -4.2141023 -4.2032804 -4.2144408 -4.2446966 -4.2720118 -4.2837009 -4.2799783 -4.2704287 -4.2609162 -4.2547431][-4.2270555 -4.2428832 -4.2641196 -4.279067 -4.2827578 -4.2777753 -4.2729445 -4.2763715 -4.2901354 -4.3020248 -4.3030324 -4.2928915 -4.277329 -4.2630968 -4.2542787][-4.2585397 -4.2737331 -4.2895131 -4.3003664 -4.304533 -4.3033538 -4.2999306 -4.2987919 -4.3040519 -4.3072047 -4.3026032 -4.2910986 -4.27657 -4.2651267 -4.2591624][-4.2837286 -4.2915254 -4.2954016 -4.2968664 -4.2961307 -4.2929449 -4.2861037 -4.2791996 -4.2793841 -4.2829056 -4.2826471 -4.2781754 -4.2724466 -4.2690339 -4.2678237][-4.2856789 -4.2838058 -4.2734551 -4.2649026 -4.2596755 -4.2545576 -4.24693 -4.2383246 -4.2382741 -4.2488561 -4.2591977 -4.262701 -4.2628913 -4.2643485 -4.2646976][-4.2657027 -4.2577424 -4.2379465 -4.2228475 -4.2191482 -4.2178659 -4.2134752 -4.2070079 -4.2097163 -4.228579 -4.2494078 -4.2585793 -4.2592349 -4.2586956 -4.2556019]]...]
INFO - root - 2017-12-06 09:43:49.107866: step 3110, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:01s remains)
INFO - root - 2017-12-06 09:43:51.292642: step 3120, loss = 2.09, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:53s remains)
INFO - root - 2017-12-06 09:43:53.447714: step 3130, loss = 2.09, batch loss = 2.03 (38.7 examples/sec; 0.206 sec/batch; 18h:53m:29s remains)
INFO - root - 2017-12-06 09:43:55.600327: step 3140, loss = 2.10, batch loss = 2.04 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:04s remains)
INFO - root - 2017-12-06 09:43:57.807075: step 3150, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:14s remains)
INFO - root - 2017-12-06 09:44:00.029578: step 3160, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:21s remains)
INFO - root - 2017-12-06 09:44:02.126667: step 3170, loss = 2.08, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:46s remains)
INFO - root - 2017-12-06 09:44:04.273262: step 3180, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:27s remains)
INFO - root - 2017-12-06 09:44:06.450181: step 3190, loss = 2.09, batch loss = 2.04 (35.6 examples/sec; 0.225 sec/batch; 20h:33m:38s remains)
INFO - root - 2017-12-06 09:44:08.663625: step 3200, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:23s remains)
2017-12-06 09:44:09.081032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3293066 -4.318038 -4.3126106 -4.3152137 -4.3246808 -4.3359284 -4.3370328 -4.3355427 -4.337616 -4.3411365 -4.3453445 -4.3488922 -4.3517947 -4.3522015 -4.346683][-4.3055935 -4.2900152 -4.2816248 -4.2831025 -4.2947254 -4.3101897 -4.3101335 -4.3074093 -4.3083844 -4.3119392 -4.3162389 -4.3211122 -4.3287592 -4.3336492 -4.3321037][-4.2781472 -4.2563357 -4.2430835 -4.2393055 -4.2491236 -4.2642226 -4.2557149 -4.2478747 -4.2495141 -4.2552104 -4.2638793 -4.2745771 -4.2904572 -4.301342 -4.3071165][-4.2472868 -4.217175 -4.1978664 -4.186657 -4.189301 -4.1980281 -4.177496 -4.1650414 -4.16759 -4.176043 -4.1889806 -4.206768 -4.2342172 -4.2583952 -4.2774596][-4.2197485 -4.1754384 -4.1440349 -4.1223741 -4.1112723 -4.1135969 -4.0892859 -4.0778008 -4.0810304 -4.0870228 -4.0980988 -4.1228886 -4.1690378 -4.21589 -4.251492][-4.197608 -4.1372943 -4.0916023 -4.0582223 -4.03498 -4.0256987 -4.0010529 -3.9905689 -3.9848733 -3.9768517 -3.9816585 -4.018322 -4.09276 -4.1716008 -4.2272515][-4.164793 -4.0910015 -4.0338125 -3.9960184 -3.972858 -3.9611287 -3.9376225 -3.9233551 -3.9087033 -3.8851285 -3.8819814 -3.9324546 -4.0350227 -4.141623 -4.212049][-4.1374173 -4.0641174 -4.0072556 -3.9682794 -3.9489672 -3.9376311 -3.9118421 -3.8961544 -3.8848376 -3.8655331 -3.8627346 -3.9169185 -4.0255828 -4.1372862 -4.2098494][-4.1364851 -4.0795522 -4.0337157 -3.9991574 -3.980747 -3.964119 -3.9393692 -3.9315524 -3.9331744 -3.92757 -3.9350538 -3.981822 -4.06883 -4.1628604 -4.2272558][-4.1574221 -4.1187286 -4.0872355 -4.062181 -4.0453224 -4.0292511 -4.0148559 -4.0192409 -4.0293422 -4.033783 -4.0468421 -4.0819726 -4.1403427 -4.2078981 -4.2569885][-4.1981211 -4.1738567 -4.1546412 -4.142694 -4.1287169 -4.1156158 -4.1119814 -4.1240807 -4.1388264 -4.1478634 -4.1600456 -4.1837726 -4.2227907 -4.2666025 -4.2970223][-4.2341766 -4.2200522 -4.209744 -4.2070112 -4.2009945 -4.193665 -4.1950727 -4.2124925 -4.2294574 -4.2369928 -4.246223 -4.2636352 -4.290679 -4.3148651 -4.3296251][-4.2613478 -4.2578998 -4.2573342 -4.2598891 -4.2578487 -4.2554097 -4.2568917 -4.2687039 -4.2812138 -4.28573 -4.2928004 -4.3068695 -4.3260331 -4.3381948 -4.344193][-4.2910037 -4.2916522 -4.2948184 -4.2987461 -4.2993217 -4.2993221 -4.30073 -4.306673 -4.3133836 -4.3152461 -4.320075 -4.3294835 -4.3405437 -4.3465915 -4.3483176][-4.3148007 -4.3156915 -4.3177495 -4.3200431 -4.3206978 -4.3204942 -4.3207021 -4.3223724 -4.324491 -4.3258171 -4.3290906 -4.3352504 -4.3417921 -4.3458643 -4.3471065]]...]
INFO - root - 2017-12-06 09:44:11.320897: step 3210, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:04s remains)
INFO - root - 2017-12-06 09:44:13.461114: step 3220, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:38s remains)
INFO - root - 2017-12-06 09:44:15.608899: step 3230, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:01s remains)
INFO - root - 2017-12-06 09:44:17.753121: step 3240, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:37s remains)
INFO - root - 2017-12-06 09:44:19.920448: step 3250, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:45m:22s remains)
INFO - root - 2017-12-06 09:44:22.045531: step 3260, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:17s remains)
INFO - root - 2017-12-06 09:44:24.284884: step 3270, loss = 2.10, batch loss = 2.04 (36.3 examples/sec; 0.221 sec/batch; 20h:10m:18s remains)
INFO - root - 2017-12-06 09:44:26.434217: step 3280, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:55s remains)
INFO - root - 2017-12-06 09:44:28.566080: step 3290, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:38s remains)
INFO - root - 2017-12-06 09:44:30.715205: step 3300, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:21s remains)
2017-12-06 09:44:31.053137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2242579 -4.2405353 -4.2773466 -4.310432 -4.3340168 -4.3498015 -4.3522024 -4.3375568 -4.3106766 -4.2801442 -4.2549448 -4.2388339 -4.2433968 -4.2652192 -4.2770286][-4.21978 -4.2449622 -4.28244 -4.3128009 -4.3321257 -4.3462439 -4.3532987 -4.349791 -4.3380475 -4.3218694 -4.3054862 -4.29288 -4.29072 -4.2935605 -4.283886][-4.2117643 -4.2422752 -4.27787 -4.303896 -4.318181 -4.3268251 -4.3293257 -4.3285742 -4.3297062 -4.3325763 -4.331264 -4.3270473 -4.3224277 -4.3126407 -4.2874675][-4.2239361 -4.2488194 -4.2717237 -4.2859979 -4.2893109 -4.2856135 -4.2775993 -4.2739835 -4.2831078 -4.3040328 -4.3209195 -4.33149 -4.33397 -4.3233361 -4.2947474][-4.2478189 -4.2606373 -4.2655592 -4.2609525 -4.2438464 -4.2182879 -4.1896873 -4.1748872 -4.190589 -4.2312469 -4.27165 -4.3053136 -4.3257012 -4.3267636 -4.3063374][-4.2658043 -4.2696424 -4.2601905 -4.2348132 -4.1917591 -4.137567 -4.0793729 -4.0455923 -4.06861 -4.133625 -4.199255 -4.2557817 -4.2959766 -4.312902 -4.3047624][-4.2768593 -4.28208 -4.2640071 -4.2200775 -4.1520715 -4.0677266 -3.9737148 -3.9165831 -3.9473729 -4.0394158 -4.1299477 -4.2057405 -4.2596664 -4.2886863 -4.2939558][-4.2898588 -4.2988944 -4.2793236 -4.2280583 -4.1482029 -4.0459328 -3.9264896 -3.8500443 -3.8837776 -3.9901745 -4.09363 -4.1775045 -4.2365289 -4.2717133 -4.2898021][-4.3132238 -4.3217025 -4.3039556 -4.2568521 -4.18329 -4.0909734 -3.9837084 -3.9177065 -3.9481783 -4.0379305 -4.1255236 -4.19627 -4.2451067 -4.2777257 -4.3011384][-4.3384776 -4.3414564 -4.3228207 -4.2824526 -4.2261262 -4.1606722 -4.0897384 -4.0539112 -4.08014 -4.1383967 -4.1930151 -4.2386866 -4.2711434 -4.2974291 -4.3206038][-4.3494558 -4.3468051 -4.3263769 -4.292737 -4.2570453 -4.2210789 -4.1862745 -4.1772523 -4.1982064 -4.2277021 -4.2504439 -4.2699623 -4.2872934 -4.3090639 -4.3315678][-4.34199 -4.333539 -4.3111873 -4.2865548 -4.2730083 -4.2628279 -4.2553611 -4.2633109 -4.2774582 -4.2836876 -4.2806907 -4.2767382 -4.280611 -4.2978892 -4.3198056][-4.323885 -4.3093381 -4.285172 -4.2696915 -4.2733016 -4.2811666 -4.2906389 -4.3070383 -4.3159137 -4.3071856 -4.2869959 -4.2671542 -4.2614946 -4.275136 -4.2962308][-4.2979217 -4.2841229 -4.2644196 -4.2574663 -4.2703915 -4.2877836 -4.3036876 -4.3204823 -4.3257151 -4.3108664 -4.2827492 -4.2556081 -4.2451344 -4.2562637 -4.2770271][-4.2785664 -4.2729096 -4.2644734 -4.2662048 -4.2830472 -4.3015194 -4.3140492 -4.3230124 -4.3216238 -4.3034239 -4.274961 -4.250144 -4.2418857 -4.2531881 -4.2756529]]...]
INFO - root - 2017-12-06 09:44:33.198877: step 3310, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-06 09:44:35.419038: step 3320, loss = 2.11, batch loss = 2.05 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:24s remains)
INFO - root - 2017-12-06 09:44:37.606029: step 3330, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:48s remains)
INFO - root - 2017-12-06 09:44:39.828989: step 3340, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-06 09:44:41.961192: step 3350, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.214 sec/batch; 19h:31m:44s remains)
INFO - root - 2017-12-06 09:44:44.114445: step 3360, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:06m:48s remains)
INFO - root - 2017-12-06 09:44:46.297539: step 3370, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-06 09:44:48.448199: step 3380, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:51s remains)
INFO - root - 2017-12-06 09:44:50.650824: step 3390, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:26s remains)
INFO - root - 2017-12-06 09:44:52.834675: step 3400, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:09s remains)
2017-12-06 09:44:53.347591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1150556 -4.1471648 -4.1786838 -4.1987071 -4.2065215 -4.2172227 -4.2244334 -4.2239752 -4.2252793 -4.2291579 -4.2322264 -4.2281494 -4.2254748 -4.2239127 -4.2204275][-4.114975 -4.1578813 -4.1977506 -4.2272696 -4.2376852 -4.2499413 -4.2563887 -4.2485828 -4.2414179 -4.2395759 -4.2406836 -4.2371011 -4.232419 -4.2233596 -4.209][-4.1449051 -4.1910434 -4.2283087 -4.2530518 -4.2634244 -4.274066 -4.2762885 -4.2629428 -4.248867 -4.245502 -4.2507234 -4.2512364 -4.2447085 -4.2253127 -4.2015047][-4.1871057 -4.2277055 -4.2555809 -4.2699175 -4.2746429 -4.2761407 -4.2689519 -4.2510591 -4.2379522 -4.2420797 -4.2533979 -4.2571292 -4.2514739 -4.2291775 -4.2061639][-4.2177248 -4.2483845 -4.2661295 -4.2699118 -4.2657204 -4.2539115 -4.2298484 -4.2040958 -4.1938524 -4.2086635 -4.2270937 -4.2353497 -4.2348018 -4.2186866 -4.2091575][-4.2164941 -4.2414117 -4.2483363 -4.24452 -4.2313261 -4.203474 -4.1573567 -4.1177921 -4.1114116 -4.1402588 -4.172864 -4.1944675 -4.2071791 -4.2102227 -4.22183][-4.1817007 -4.19677 -4.1964417 -4.1896911 -4.1686821 -4.1258764 -4.0576296 -3.9977045 -4.0038905 -4.065948 -4.1291838 -4.177043 -4.2126069 -4.2343268 -4.2561297][-4.1613817 -4.1609974 -4.1556892 -4.1465511 -4.1182923 -4.063961 -3.9829171 -3.917676 -3.9519041 -4.0515866 -4.1412678 -4.2044306 -4.2514606 -4.2743025 -4.2904015][-4.1712904 -4.1608739 -4.1553516 -4.147882 -4.1246467 -4.0850616 -4.0353889 -4.0063953 -4.0474277 -4.1301966 -4.2011266 -4.2450418 -4.2754622 -4.285387 -4.2889137][-4.1903257 -4.1785579 -4.1791639 -4.1810031 -4.1719718 -4.1592717 -4.1448569 -4.1396089 -4.1677823 -4.2177649 -4.2575178 -4.2705941 -4.271832 -4.2645426 -4.2572408][-4.2030425 -4.1932774 -4.2016921 -4.2146511 -4.2152529 -4.2153683 -4.2147412 -4.2162228 -4.2342443 -4.2629843 -4.2783556 -4.2642164 -4.2396846 -4.21838 -4.206409][-4.194643 -4.1847553 -4.196424 -4.218257 -4.2299047 -4.2341681 -4.2344851 -4.2370715 -4.2487082 -4.2683597 -4.272552 -4.2445483 -4.2053723 -4.1769047 -4.1623859][-4.1781325 -4.1643085 -4.1726437 -4.1949172 -4.2095037 -4.2132578 -4.2151308 -4.2198629 -4.230176 -4.2473907 -4.2498937 -4.2203627 -4.1772342 -4.1493955 -4.141263][-4.1725817 -4.1588836 -4.1610546 -4.1749768 -4.1849151 -4.1834731 -4.182189 -4.1871896 -4.1998782 -4.2171779 -4.2226667 -4.2006755 -4.1631913 -4.14284 -4.1471481][-4.1911716 -4.1829057 -4.1803417 -4.1827955 -4.1822395 -4.1720695 -4.1620116 -4.1611133 -4.1722264 -4.18934 -4.1976004 -4.1863823 -4.1629949 -4.1529536 -4.1653533]]...]
INFO - root - 2017-12-06 09:44:55.508115: step 3410, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:02s remains)
INFO - root - 2017-12-06 09:44:57.655220: step 3420, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-06 09:44:59.783806: step 3430, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.209 sec/batch; 19h:08m:43s remains)
INFO - root - 2017-12-06 09:45:01.915319: step 3440, loss = 2.06, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:08m:20s remains)
INFO - root - 2017-12-06 09:45:04.091595: step 3450, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:56s remains)
INFO - root - 2017-12-06 09:45:06.233460: step 3460, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:02s remains)
INFO - root - 2017-12-06 09:45:08.410905: step 3470, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 21h:20m:54s remains)
INFO - root - 2017-12-06 09:45:10.598514: step 3480, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 09:45:12.737494: step 3490, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:04m:54s remains)
INFO - root - 2017-12-06 09:45:14.861698: step 3500, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.221 sec/batch; 20h:13m:53s remains)
2017-12-06 09:45:15.237345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2056336 -4.2110887 -4.2069159 -4.2026596 -4.2124028 -4.2242804 -4.2281575 -4.2263446 -4.2196069 -4.2107692 -4.2211943 -4.2176147 -4.2003942 -4.1850262 -4.1617694][-4.1725364 -4.1713157 -4.1613121 -4.158577 -4.1742768 -4.1891627 -4.1920395 -4.1892858 -4.182466 -4.1780977 -4.1983862 -4.1986728 -4.1790729 -4.1586256 -4.1308594][-4.1305127 -4.1222825 -4.113359 -4.1138139 -4.1361246 -4.1518478 -4.1480303 -4.1434646 -4.142653 -4.1483364 -4.1754179 -4.1804457 -4.1601944 -4.1343241 -4.1026034][-4.1089997 -4.0973587 -4.0925226 -4.0967298 -4.1197782 -4.13092 -4.1199274 -4.1150031 -4.1239419 -4.1366448 -4.1634884 -4.1704845 -4.1494341 -4.1149249 -4.0754538][-4.1277757 -4.106926 -4.1020265 -4.1009951 -4.1179075 -4.1264153 -4.1142006 -4.1115136 -4.1303411 -4.1468735 -4.1708584 -4.1789045 -4.1604042 -4.1237988 -4.0835452][-4.148829 -4.1210551 -4.1116114 -4.0997672 -4.1066823 -4.1094427 -4.096766 -4.1022553 -4.1332297 -4.1571383 -4.1818576 -4.1921964 -4.1790018 -4.1459441 -4.1034384][-4.151073 -4.1240773 -4.1094837 -4.088994 -4.0894518 -4.0824981 -4.0639915 -4.0719857 -4.10864 -4.1408682 -4.1696949 -4.185204 -4.1777277 -4.144484 -4.098177][-4.1500945 -4.125319 -4.1117597 -4.0910864 -4.0922322 -4.0821476 -4.0597949 -4.0640683 -4.0958996 -4.1223125 -4.1417561 -4.1561785 -4.1525006 -4.1208348 -4.0794978][-4.1483417 -4.1319304 -4.1288686 -4.1215115 -4.1335249 -4.1295209 -4.1121187 -4.1110659 -4.1270261 -4.1370368 -4.1391487 -4.1449709 -4.1417289 -4.1188979 -4.0909328][-4.154747 -4.1477184 -4.1565919 -4.1642265 -4.1870384 -4.1945987 -4.1856737 -4.1818261 -4.1859522 -4.1852121 -4.17906 -4.1764746 -4.1711621 -4.1549139 -4.1397943][-4.168016 -4.167366 -4.1830716 -4.196094 -4.2223272 -4.2354541 -4.2338438 -4.233007 -4.2353415 -4.2333312 -4.2266617 -4.2186503 -4.20846 -4.193469 -4.18601][-4.192215 -4.1973338 -4.2139144 -4.2254553 -4.2520781 -4.2670579 -4.2707272 -4.2734847 -4.2766471 -4.2763391 -4.2704415 -4.2619953 -4.2504244 -4.2384095 -4.2341056][-4.2148933 -4.2237816 -4.2395349 -4.2515879 -4.2749453 -4.2900581 -4.2964754 -4.2981162 -4.2989178 -4.2992067 -4.2971077 -4.29487 -4.2892733 -4.2841229 -4.2831945][-4.2191033 -4.2264524 -4.2379727 -4.2496586 -4.2712693 -4.2866335 -4.2955089 -4.2977276 -4.2989206 -4.3015995 -4.3050671 -4.3084927 -4.3085027 -4.3080673 -4.309948][-4.2076483 -4.2116108 -4.2218757 -4.2362471 -4.2594056 -4.2788424 -4.2910008 -4.2921367 -4.2905817 -4.2936606 -4.3007607 -4.3082466 -4.31349 -4.3179288 -4.3218584]]...]
INFO - root - 2017-12-06 09:45:17.389590: step 3510, loss = 2.09, batch loss = 2.04 (37.1 examples/sec; 0.216 sec/batch; 19h:42m:35s remains)
INFO - root - 2017-12-06 09:45:19.531817: step 3520, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:49s remains)
INFO - root - 2017-12-06 09:45:21.693351: step 3530, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:06m:56s remains)
INFO - root - 2017-12-06 09:45:23.873402: step 3540, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:09m:49s remains)
INFO - root - 2017-12-06 09:45:26.045863: step 3550, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:41m:34s remains)
INFO - root - 2017-12-06 09:45:28.240938: step 3560, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:29s remains)
INFO - root - 2017-12-06 09:45:30.445519: step 3570, loss = 2.09, batch loss = 2.04 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:36s remains)
INFO - root - 2017-12-06 09:45:32.588598: step 3580, loss = 2.06, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:56s remains)
INFO - root - 2017-12-06 09:45:34.761679: step 3590, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:31s remains)
INFO - root - 2017-12-06 09:45:36.923374: step 3600, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:49m:45s remains)
2017-12-06 09:45:37.330668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2684221 -4.2749062 -4.2722597 -4.2623935 -4.2412968 -4.2029696 -4.1649475 -4.1427336 -4.1346893 -4.1386466 -4.1632648 -4.208199 -4.2494678 -4.277174 -4.29189][-4.2934256 -4.2968678 -4.2904816 -4.2773943 -4.2556682 -4.2204866 -4.1920152 -4.1711698 -4.1576629 -4.160697 -4.18165 -4.2192163 -4.2532206 -4.27706 -4.2916923][-4.2872391 -4.2893634 -4.2797341 -4.2635098 -4.2440434 -4.212841 -4.1915913 -4.1759095 -4.167037 -4.1757522 -4.1934867 -4.2169695 -4.2376776 -4.2527127 -4.26907][-4.256515 -4.2582345 -4.2445817 -4.2246184 -4.2049375 -4.175096 -4.1543293 -4.1456146 -4.1453061 -4.1580868 -4.1759515 -4.1933031 -4.2041826 -4.2132883 -4.2335148][-4.233614 -4.2276354 -4.1988654 -4.167418 -4.1405873 -4.1083512 -4.0847306 -4.0814552 -4.0921531 -4.1159449 -4.1413341 -4.1599317 -4.1627774 -4.1678152 -4.1943364][-4.2097006 -4.1923838 -4.1513286 -4.1087408 -4.072948 -4.0339575 -4.0038929 -4.00462 -4.024713 -4.066 -4.1063752 -4.1249714 -4.1229086 -4.1290984 -4.1629248][-4.1801891 -4.1553206 -4.1114764 -4.0671215 -4.0306854 -3.9883754 -3.9531903 -3.9577277 -3.9849117 -4.0404491 -4.0918479 -4.10215 -4.0951643 -4.1037226 -4.1455231][-4.1736856 -4.1461186 -4.1077957 -4.0772915 -4.0584712 -4.0315156 -4.0079808 -4.0172162 -4.0404425 -4.0892711 -4.1291432 -4.1210856 -4.1039395 -4.1090641 -4.1499209][-4.1852221 -4.1669922 -4.1466193 -4.1359878 -4.1346307 -4.1235647 -4.1112385 -4.1183114 -4.1302052 -4.1609287 -4.185483 -4.1685066 -4.1489849 -4.1545329 -4.1950374][-4.2082138 -4.2018375 -4.1974039 -4.2041655 -4.215445 -4.21668 -4.2114277 -4.2149706 -4.2165837 -4.2277465 -4.2385974 -4.2218442 -4.210186 -4.2196622 -4.2553492][-4.2379746 -4.2361917 -4.2397881 -4.2545 -4.2719417 -4.2779331 -4.2756729 -4.2755933 -4.27238 -4.2703977 -4.2691545 -4.2575622 -4.2581749 -4.2704687 -4.2983][-4.2538528 -4.2496367 -4.2538257 -4.2701716 -4.2890024 -4.2977829 -4.2945395 -4.2890911 -4.2839751 -4.2799759 -4.2757239 -4.272934 -4.2835503 -4.2995467 -4.3222113][-4.2578907 -4.2480874 -4.2465205 -4.2588015 -4.2760062 -4.2868586 -4.282845 -4.2753997 -4.2728009 -4.27365 -4.2747731 -4.2787085 -4.2898006 -4.306325 -4.3233881][-4.2579522 -4.2405419 -4.2313452 -4.23477 -4.2480388 -4.2572646 -4.2534065 -4.2498827 -4.2562895 -4.2656322 -4.2784882 -4.2884212 -4.2982268 -4.3121424 -4.3226886][-4.2657681 -4.24375 -4.2268105 -4.2192178 -4.2208061 -4.221797 -4.217792 -4.2198596 -4.2368684 -4.2529597 -4.2723851 -4.2882366 -4.3007159 -4.3192463 -4.3281517]]...]
INFO - root - 2017-12-06 09:45:39.521624: step 3610, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:18s remains)
INFO - root - 2017-12-06 09:45:41.662376: step 3620, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:50m:28s remains)
INFO - root - 2017-12-06 09:45:43.880608: step 3630, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:36m:50s remains)
INFO - root - 2017-12-06 09:45:46.030703: step 3640, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:35s remains)
INFO - root - 2017-12-06 09:45:48.192334: step 3650, loss = 2.10, batch loss = 2.04 (35.0 examples/sec; 0.228 sec/batch; 20h:52m:12s remains)
INFO - root - 2017-12-06 09:45:50.348744: step 3660, loss = 2.11, batch loss = 2.06 (37.8 examples/sec; 0.211 sec/batch; 19h:18m:35s remains)
INFO - root - 2017-12-06 09:45:52.555818: step 3670, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 20h:51m:07s remains)
INFO - root - 2017-12-06 09:45:54.709924: step 3680, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:26s remains)
INFO - root - 2017-12-06 09:45:56.894473: step 3690, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:15s remains)
INFO - root - 2017-12-06 09:45:59.061577: step 3700, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:34s remains)
2017-12-06 09:45:59.384514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.298346 -4.2902765 -4.2640495 -4.2349796 -4.2171273 -4.2112 -4.214169 -4.2175074 -4.20603 -4.1766529 -4.1582689 -4.1664925 -4.185493 -4.1980929 -4.2001953][-4.2668548 -4.2574759 -4.2347918 -4.2083664 -4.1950717 -4.1925507 -4.1975803 -4.1973724 -4.1745138 -4.138299 -4.1206 -4.1366811 -4.1679721 -4.1931343 -4.1981158][-4.2217083 -4.2121258 -4.1926336 -4.1742358 -4.1709528 -4.1785092 -4.1856117 -4.1800346 -4.1541748 -4.1230764 -4.1186118 -4.14626 -4.1826777 -4.20793 -4.2061377][-4.1709003 -4.161633 -4.146935 -4.1399946 -4.1486049 -4.1658435 -4.1758108 -4.1682215 -4.1488504 -4.1363449 -4.1518955 -4.187396 -4.2211957 -4.2323389 -4.2131958][-4.131249 -4.1187372 -4.107554 -4.1063142 -4.1219311 -4.1409855 -4.1478853 -4.143693 -4.1375594 -4.1459317 -4.1790786 -4.2190566 -4.248826 -4.2442856 -4.2076592][-4.1071062 -4.0889788 -4.0723691 -4.0656013 -4.0748072 -4.0819192 -4.0745173 -4.0725117 -4.0842886 -4.1124468 -4.158206 -4.1953444 -4.2195253 -4.2140431 -4.1815162][-4.0661283 -4.0404677 -4.0135541 -3.9967294 -3.9955947 -3.984545 -3.9590092 -3.9612517 -3.9963417 -4.046041 -4.1016393 -4.1392217 -4.1610918 -4.1622629 -4.1462135][-3.9879644 -3.939137 -3.8857622 -3.8516409 -3.8441138 -3.8214791 -3.7882726 -3.8063529 -3.8706238 -3.9445243 -4.0148506 -4.0614824 -4.088099 -4.1022673 -4.108479][-3.9727192 -3.9169741 -3.8551967 -3.817996 -3.813066 -3.8007355 -3.786248 -3.8123732 -3.8735938 -3.9384449 -3.9972048 -4.0383067 -4.058641 -4.0719776 -4.0869255][-4.0556664 -4.0155258 -3.9753072 -3.9557617 -3.9565449 -3.9551775 -3.9539075 -3.9711206 -4.0011268 -4.0319166 -4.062048 -4.0832844 -4.0891738 -4.0908661 -4.0976567][-4.151207 -4.1275754 -4.1082449 -4.0997529 -4.1002831 -4.1023989 -4.1042013 -4.1096082 -4.1161547 -4.1237841 -4.13435 -4.1397414 -4.1370168 -4.1338744 -4.1341324][-4.2217622 -4.2052178 -4.1932983 -4.1880512 -4.1863356 -4.1863828 -4.1861925 -4.1862345 -4.1850138 -4.184082 -4.1848817 -4.1815348 -4.1783543 -4.1788592 -4.181057][-4.2689862 -4.257678 -4.2482858 -4.2422218 -4.2393384 -4.2375789 -4.2364597 -4.235405 -4.2320619 -4.2274132 -4.2247124 -4.2226992 -4.22231 -4.2267742 -4.2343016][-4.2967672 -4.2900343 -4.283895 -4.2785597 -4.2746072 -4.2711139 -4.2682734 -4.2656965 -4.2623258 -4.258091 -4.2551694 -4.2556019 -4.2581635 -4.264173 -4.2723913][-4.3232718 -4.3191175 -4.3143 -4.3085189 -4.3033433 -4.2992287 -4.2959614 -4.2933941 -4.2899456 -4.2855568 -4.2817755 -4.2811689 -4.2828674 -4.2867355 -4.2924976]]...]
INFO - root - 2017-12-06 09:46:01.540555: step 3710, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:07m:55s remains)
INFO - root - 2017-12-06 09:46:03.746392: step 3720, loss = 2.08, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-06 09:46:05.948021: step 3730, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-06 09:46:08.046919: step 3740, loss = 2.10, batch loss = 2.04 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:33s remains)
INFO - root - 2017-12-06 09:46:10.234301: step 3750, loss = 2.10, batch loss = 2.04 (36.4 examples/sec; 0.219 sec/batch; 20h:02m:37s remains)
INFO - root - 2017-12-06 09:46:12.465188: step 3760, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 20h:40m:46s remains)
INFO - root - 2017-12-06 09:46:14.652664: step 3770, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:43m:47s remains)
INFO - root - 2017-12-06 09:46:16.782044: step 3780, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:09m:17s remains)
INFO - root - 2017-12-06 09:46:18.950822: step 3790, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:28s remains)
INFO - root - 2017-12-06 09:46:21.101744: step 3800, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:25m:02s remains)
2017-12-06 09:46:21.445698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1505837 -4.0941944 -4.0657673 -4.1107388 -4.1932549 -4.2383261 -4.2399788 -4.2256184 -4.2093458 -4.2042546 -4.21417 -4.2135296 -4.2134867 -4.2382364 -4.2731194][-4.1470857 -4.0906591 -4.0737863 -4.128437 -4.2057052 -4.2414665 -4.2403784 -4.2267852 -4.2113614 -4.2124262 -4.2360311 -4.2487946 -4.2585969 -4.2765827 -4.2959104][-4.1831179 -4.1360054 -4.128685 -4.178844 -4.2357426 -4.2551236 -4.2392735 -4.2159677 -4.1951914 -4.2040749 -4.2351265 -4.2576814 -4.2794456 -4.2967443 -4.3076577][-4.2374339 -4.1972136 -4.191956 -4.2292242 -4.26599 -4.2660012 -4.224328 -4.1772289 -4.1539178 -4.180274 -4.2237377 -4.2581491 -4.2836871 -4.2976394 -4.3016891][-4.2810955 -4.2536864 -4.2485547 -4.2706852 -4.2852964 -4.2562146 -4.173532 -4.0942793 -4.0819387 -4.1389589 -4.1998734 -4.2508759 -4.2819023 -4.2939005 -4.2921133][-4.2884426 -4.2738848 -4.270299 -4.2788749 -4.2738948 -4.2093282 -4.078402 -3.9695048 -3.9898899 -4.0851531 -4.1650152 -4.2334752 -4.2767487 -4.29469 -4.2925835][-4.2832026 -4.2726884 -4.2653189 -4.2579031 -4.2232656 -4.113472 -3.9272528 -3.8116941 -3.9030082 -4.039567 -4.1382918 -4.2187934 -4.2748847 -4.3008051 -4.2985797][-4.280684 -4.2715383 -4.257133 -4.2274809 -4.1546474 -3.9972067 -3.7661374 -3.6936555 -3.8615637 -4.0280976 -4.1383362 -4.2220387 -4.2787666 -4.30551 -4.3040419][-4.27833 -4.2656054 -4.2401576 -4.1864839 -4.0928068 -3.927573 -3.7317915 -3.7366071 -3.9138525 -4.0635114 -4.1631918 -4.2380681 -4.2869682 -4.3093596 -4.3085284][-4.2784586 -4.2592859 -4.2243118 -4.1615729 -4.0742617 -3.9442387 -3.8346591 -3.8891726 -4.0264277 -4.1337504 -4.2112589 -4.2693839 -4.3036633 -4.3137779 -4.3119793][-4.2842073 -4.2635283 -4.2253723 -4.1716552 -4.100563 -4.0125675 -3.9709125 -4.0368214 -4.1360917 -4.2116294 -4.2665625 -4.3042955 -4.3188868 -4.317193 -4.3131618][-4.2937679 -4.2776394 -4.2483106 -4.2139134 -4.1647887 -4.1126628 -4.1049294 -4.1579676 -4.2252131 -4.2767339 -4.3132629 -4.3317022 -4.3288279 -4.3189597 -4.31413][-4.305953 -4.2956586 -4.2794304 -4.2648921 -4.2397556 -4.2160769 -4.2212157 -4.2577605 -4.2982731 -4.3267627 -4.34332 -4.3439116 -4.3314648 -4.3201985 -4.3163519][-4.3139405 -4.3094635 -4.3054123 -4.3056111 -4.2989645 -4.2936158 -4.300745 -4.3194261 -4.3381834 -4.3475437 -4.3504148 -4.3442445 -4.33091 -4.3211937 -4.3168082][-4.3166981 -4.3165112 -4.31788 -4.3231759 -4.3267121 -4.3294482 -4.3324022 -4.3366809 -4.3416452 -4.3423266 -4.340116 -4.3335438 -4.3231373 -4.3157496 -4.3107171]]...]
INFO - root - 2017-12-06 09:46:23.633372: step 3810, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:33m:25s remains)
INFO - root - 2017-12-06 09:46:25.786737: step 3820, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:47m:54s remains)
INFO - root - 2017-12-06 09:46:27.939828: step 3830, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:57s remains)
INFO - root - 2017-12-06 09:46:30.081599: step 3840, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:15m:13s remains)
INFO - root - 2017-12-06 09:46:32.235511: step 3850, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:25s remains)
INFO - root - 2017-12-06 09:46:34.380068: step 3860, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:47s remains)
INFO - root - 2017-12-06 09:46:36.541894: step 3870, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:08s remains)
INFO - root - 2017-12-06 09:46:38.701382: step 3880, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:05m:25s remains)
INFO - root - 2017-12-06 09:46:40.927856: step 3890, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 19h:57m:13s remains)
INFO - root - 2017-12-06 09:46:43.097815: step 3900, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:33s remains)
2017-12-06 09:46:43.614574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2794924 -4.2851672 -4.2919168 -4.2973018 -4.2986317 -4.2878418 -4.2674513 -4.2544193 -4.2640891 -4.2884607 -4.309618 -4.3218513 -4.3245444 -4.3155951 -4.3005567][-4.3003874 -4.3071241 -4.31006 -4.307734 -4.2976713 -4.2759604 -4.2481256 -4.2323389 -4.2435408 -4.272243 -4.297226 -4.3114281 -4.3181343 -4.3118258 -4.2970719][-4.3112364 -4.3168068 -4.3150921 -4.3077159 -4.291358 -4.2627769 -4.2286792 -4.21064 -4.2247772 -4.2599268 -4.2905941 -4.3080311 -4.3169632 -4.3140769 -4.3019128][-4.3056803 -4.3093848 -4.3037958 -4.2927337 -4.2715783 -4.2363091 -4.1961913 -4.1794934 -4.2014141 -4.24662 -4.2860241 -4.3094273 -4.3194542 -4.3178926 -4.3082967][-4.2931595 -4.2975125 -4.2897787 -4.2721252 -4.242331 -4.1967225 -4.1482835 -4.1360383 -4.172266 -4.2320342 -4.2829585 -4.3124709 -4.3253708 -4.3250604 -4.3171339][-4.2859135 -4.2934813 -4.2843323 -4.2591066 -4.2191334 -4.1609468 -4.0999804 -4.0862188 -4.1320157 -4.2055287 -4.2685709 -4.3058796 -4.325943 -4.3324718 -4.3285794][-4.2807651 -4.2915506 -4.2832289 -4.2569079 -4.21551 -4.1556878 -4.0869493 -4.0609345 -4.1015334 -4.1786437 -4.248323 -4.2926216 -4.3196273 -4.3338609 -4.3342295][-4.2800021 -4.2895203 -4.2843213 -4.2666306 -4.2408237 -4.1981449 -4.1359906 -4.0979342 -4.1196356 -4.1842341 -4.2503834 -4.297905 -4.3261166 -4.3406458 -4.3389792][-4.2849541 -4.2904625 -4.2893162 -4.2828135 -4.2736354 -4.2505574 -4.2028913 -4.1617718 -4.1673355 -4.2144337 -4.2714891 -4.3155823 -4.3383503 -4.3465109 -4.3393559][-4.2760243 -4.2839031 -4.2896557 -4.2911353 -4.29046 -4.2770715 -4.2398725 -4.2022352 -4.2023873 -4.2354603 -4.2800546 -4.3157506 -4.3312783 -4.3326197 -4.3211145][-4.2650809 -4.2817779 -4.2960062 -4.3011837 -4.3002653 -4.2880578 -4.2575893 -4.2279668 -4.2278075 -4.2497697 -4.278585 -4.3045497 -4.3161006 -4.3152971 -4.3044138][-4.2603531 -4.2819314 -4.2988124 -4.305522 -4.3042254 -4.2935472 -4.2689495 -4.2452974 -4.2422652 -4.2528377 -4.2689266 -4.286921 -4.2979374 -4.2999973 -4.2951088][-4.2576356 -4.27973 -4.2938776 -4.2980666 -4.2959652 -4.285748 -4.2637172 -4.2430997 -4.2349796 -4.2356973 -4.2428417 -4.2549987 -4.2686162 -4.2800241 -4.2847776][-4.25452 -4.2701416 -4.278235 -4.2794614 -4.276638 -4.2673707 -4.2480793 -4.2282653 -4.2147231 -4.2065578 -4.2063761 -4.2134104 -4.2277241 -4.2476268 -4.2666821][-4.2562914 -4.2655044 -4.26905 -4.2693481 -4.2678528 -4.2612748 -4.2460074 -4.2263641 -4.2040348 -4.1834836 -4.172574 -4.175035 -4.1925282 -4.2214928 -4.25348]]...]
INFO - root - 2017-12-06 09:46:45.780823: step 3910, loss = 2.10, batch loss = 2.04 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:25s remains)
INFO - root - 2017-12-06 09:46:48.008214: step 3920, loss = 2.10, batch loss = 2.04 (35.3 examples/sec; 0.227 sec/batch; 20h:40m:48s remains)
INFO - root - 2017-12-06 09:46:50.144661: step 3930, loss = 2.11, batch loss = 2.05 (38.5 examples/sec; 0.208 sec/batch; 18h:58m:46s remains)
INFO - root - 2017-12-06 09:46:52.277577: step 3940, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:38m:19s remains)
INFO - root - 2017-12-06 09:46:54.444774: step 3950, loss = 2.10, batch loss = 2.04 (36.3 examples/sec; 0.220 sec/batch; 20h:05m:49s remains)
INFO - root - 2017-12-06 09:46:56.603418: step 3960, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 19h:55m:50s remains)
INFO - root - 2017-12-06 09:46:58.777147: step 3970, loss = 2.07, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:20s remains)
INFO - root - 2017-12-06 09:47:00.903846: step 3980, loss = 2.13, batch loss = 2.07 (36.2 examples/sec; 0.221 sec/batch; 20h:09m:20s remains)
INFO - root - 2017-12-06 09:47:03.043487: step 3990, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:20m:24s remains)
INFO - root - 2017-12-06 09:47:05.199270: step 4000, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.221 sec/batch; 20h:11m:54s remains)
2017-12-06 09:47:05.591165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2868185 -4.2925735 -4.2873282 -4.2735806 -4.2597365 -4.2485003 -4.2377057 -4.2330956 -4.2408371 -4.2640314 -4.2822146 -4.2910862 -4.2963696 -4.3022757 -4.3040013][-4.252748 -4.2606149 -4.2578506 -4.2412062 -4.2168684 -4.1910648 -4.1645632 -4.1463914 -4.1531787 -4.1875672 -4.217649 -4.2319 -4.2419329 -4.2587414 -4.2689185][-4.22282 -4.23198 -4.2290425 -4.2066064 -4.1664815 -4.1144896 -4.0630565 -4.0285645 -4.0383019 -4.0897212 -4.1356082 -4.1585402 -4.1758385 -4.2069678 -4.2303791][-4.1916704 -4.2025018 -4.2044296 -4.1767788 -4.1152277 -4.0316439 -3.9427359 -3.8850718 -3.9104152 -4.000596 -4.078033 -4.1148095 -4.1418381 -4.1818256 -4.2111187][-4.150394 -4.1612353 -4.1747537 -4.1524925 -4.0784664 -3.9586184 -3.8079257 -3.716661 -3.779952 -3.9296081 -4.0502176 -4.1103983 -4.1503663 -4.1929922 -4.2164183][-4.0915527 -4.1009064 -4.1278992 -4.1202064 -4.0469003 -3.8992448 -3.6791394 -3.5378313 -3.6644883 -3.8835068 -4.0433764 -4.1292348 -4.1776533 -4.2163444 -4.229938][-4.0335817 -4.0400114 -4.0818663 -4.0939188 -4.036756 -3.8930161 -3.6524107 -3.4776235 -3.6404243 -3.8867414 -4.0588069 -4.1530132 -4.2038121 -4.2332134 -4.23505][-4.0109444 -4.0099773 -4.0579662 -4.0927591 -4.0694466 -3.9685662 -3.7927909 -3.6647429 -3.7664509 -3.949532 -4.0852051 -4.1684666 -4.209177 -4.2257628 -4.218133][-4.026917 -4.0161333 -4.0629759 -4.1157403 -4.1276746 -4.0789995 -3.9863679 -3.9205408 -3.95524 -4.0465465 -4.1258407 -4.1818051 -4.2057786 -4.20694 -4.1935105][-4.0612211 -4.0458083 -4.0899925 -4.1497526 -4.1831169 -4.1633663 -4.1201363 -4.0919995 -4.0912719 -4.1164079 -4.1523743 -4.188252 -4.1977262 -4.1902862 -4.17701][-4.0883226 -4.0753107 -4.1207962 -4.1760316 -4.2070403 -4.1929507 -4.168375 -4.1541686 -4.134624 -4.1243153 -4.1402421 -4.1734691 -4.191298 -4.1951737 -4.1873531][-4.1069379 -4.0972514 -4.1402731 -4.1846423 -4.2004013 -4.186861 -4.1699591 -4.1526995 -4.1301403 -4.1143794 -4.1312985 -4.1724367 -4.2081761 -4.2295241 -4.2285147][-4.140017 -4.1355953 -4.1701107 -4.2001586 -4.2044387 -4.1914053 -4.1769047 -4.1585288 -4.1429319 -4.138154 -4.1607575 -4.2006464 -4.2406445 -4.27126 -4.2742782][-4.1845965 -4.1834316 -4.2082567 -4.22812 -4.2225456 -4.204711 -4.1896372 -4.17467 -4.1648498 -4.1731677 -4.2012773 -4.2329855 -4.2726045 -4.3060117 -4.30664][-4.2332282 -4.2323189 -4.2500677 -4.26297 -4.251152 -4.2275329 -4.2107754 -4.1978664 -4.1891861 -4.1985903 -4.2235923 -4.2488408 -4.2826924 -4.3105655 -4.3071718]]...]
INFO - root - 2017-12-06 09:47:07.795451: step 4010, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.234 sec/batch; 21h:22m:35s remains)
INFO - root - 2017-12-06 09:47:09.944643: step 4020, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:48m:41s remains)
INFO - root - 2017-12-06 09:47:12.091771: step 4030, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:21s remains)
INFO - root - 2017-12-06 09:47:14.238849: step 4040, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 18h:56m:45s remains)
INFO - root - 2017-12-06 09:47:16.444029: step 4050, loss = 2.06, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:33m:59s remains)
INFO - root - 2017-12-06 09:47:18.601733: step 4060, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:14m:51s remains)
INFO - root - 2017-12-06 09:47:20.785432: step 4070, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:44s remains)
INFO - root - 2017-12-06 09:47:22.920083: step 4080, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:23m:23s remains)
INFO - root - 2017-12-06 09:47:25.071140: step 4090, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 19h:50m:35s remains)
INFO - root - 2017-12-06 09:47:27.246872: step 4100, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:13m:50s remains)
2017-12-06 09:47:27.593140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3213539 -4.3217874 -4.3182631 -4.3182116 -4.3249264 -4.3292737 -4.3262343 -4.3271661 -4.3378038 -4.3424945 -4.3384571 -4.3339224 -4.3309531 -4.32872 -4.3315229][-4.2993917 -4.2932854 -4.2881207 -4.293644 -4.3062716 -4.3136978 -4.3114066 -4.3131771 -4.3228235 -4.3256335 -4.3206978 -4.31617 -4.3127227 -4.3127708 -4.3200016][-4.267643 -4.2628407 -4.2614079 -4.2714462 -4.2857156 -4.2920136 -4.2864518 -4.2851386 -4.2925634 -4.2939091 -4.2898154 -4.2904487 -4.2924604 -4.2988963 -4.3081431][-4.2421541 -4.2451057 -4.2483816 -4.258069 -4.267118 -4.2644038 -4.2465868 -4.2354918 -4.2419767 -4.2482929 -4.2526636 -4.2662539 -4.2769637 -4.2879124 -4.2929015][-4.2438579 -4.25083 -4.2533603 -4.2564483 -4.2554774 -4.2319117 -4.1901994 -4.16595 -4.1822181 -4.2057967 -4.2304339 -4.2609243 -4.27937 -4.2846661 -4.278604][-4.2538829 -4.2533808 -4.2458487 -4.235765 -4.2157774 -4.1633697 -4.0903969 -4.0560913 -4.1029749 -4.1662822 -4.22055 -4.2603254 -4.2757483 -4.2713203 -4.2546616][-4.2533708 -4.2420025 -4.2245846 -4.2020845 -4.1620221 -4.0817113 -3.9806581 -3.9511425 -4.0452452 -4.1483936 -4.21699 -4.247458 -4.2507849 -4.2370567 -4.2141356][-4.2480445 -4.2310119 -4.2134361 -4.1913848 -4.1482787 -4.0734391 -3.9940057 -3.9933548 -4.0927835 -4.1886516 -4.2395921 -4.246788 -4.2332473 -4.2096968 -4.1892414][-4.246511 -4.2350087 -4.2248263 -4.2059321 -4.1727552 -4.1266189 -4.0923004 -4.1144629 -4.1834764 -4.2402067 -4.2593241 -4.2395005 -4.207839 -4.1843266 -4.1829791][-4.2313328 -4.2294626 -4.2274694 -4.214396 -4.1920104 -4.1661325 -4.1580291 -4.1875911 -4.2290034 -4.2502756 -4.2402763 -4.1991243 -4.1604481 -4.1551585 -4.1824527][-4.2176437 -4.2231278 -4.2248015 -4.2176938 -4.2010007 -4.1839781 -4.1869926 -4.2117586 -4.2350845 -4.2356653 -4.2059584 -4.1567144 -4.1291242 -4.1495209 -4.1970119][-4.1959891 -4.2039418 -4.2075653 -4.2075758 -4.196044 -4.1835179 -4.1931429 -4.2150455 -4.2303576 -4.2219448 -4.183084 -4.1388621 -4.129384 -4.1620827 -4.2106323][-4.1553197 -4.1605129 -4.1653638 -4.17298 -4.1689496 -4.1617622 -4.1763363 -4.1943026 -4.202899 -4.1924534 -4.1606984 -4.1335731 -4.1352658 -4.1633244 -4.2037959][-4.107594 -4.1048083 -4.1071954 -4.1201763 -4.1263194 -4.1280942 -4.1458206 -4.1614971 -4.1683741 -4.1668639 -4.1501312 -4.1342216 -4.1346064 -4.1509991 -4.1820965][-4.0852141 -4.0738482 -4.0722818 -4.0853066 -4.0940795 -4.1019635 -4.1227274 -4.1379619 -4.1473522 -4.1531167 -4.1442881 -4.1305838 -4.1275878 -4.1385336 -4.1665297]]...]
INFO - root - 2017-12-06 09:47:29.760178: step 4110, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:19m:12s remains)
INFO - root - 2017-12-06 09:47:31.899718: step 4120, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:06s remains)
INFO - root - 2017-12-06 09:47:34.092691: step 4130, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:25m:51s remains)
INFO - root - 2017-12-06 09:47:36.297518: step 4140, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:45s remains)
INFO - root - 2017-12-06 09:47:38.521455: step 4150, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:33s remains)
INFO - root - 2017-12-06 09:47:40.680180: step 4160, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.226 sec/batch; 20h:39m:26s remains)
INFO - root - 2017-12-06 09:47:42.854862: step 4170, loss = 2.11, batch loss = 2.05 (38.0 examples/sec; 0.211 sec/batch; 19h:13m:15s remains)
INFO - root - 2017-12-06 09:47:45.024541: step 4180, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:13s remains)
INFO - root - 2017-12-06 09:47:47.163583: step 4190, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:47s remains)
INFO - root - 2017-12-06 09:47:49.328606: step 4200, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:19m:22s remains)
2017-12-06 09:47:49.646553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2775226 -4.267344 -4.2679157 -4.2719693 -4.2749925 -4.2733035 -4.2683482 -4.2587957 -4.2480569 -4.2364845 -4.2263193 -4.222259 -4.23211 -4.2554269 -4.2827582][-4.2550511 -4.2471266 -4.2457867 -4.2436709 -4.2367058 -4.2229333 -4.2068176 -4.18861 -4.1700397 -4.151227 -4.1352682 -4.1282835 -4.1416149 -4.1754923 -4.2172132][-4.250031 -4.2480679 -4.2428908 -4.2289968 -4.2064419 -4.1770959 -4.1486597 -4.1231489 -4.1022792 -4.0802703 -4.061368 -4.0550194 -4.0730038 -4.115283 -4.1661243][-4.2430067 -4.245223 -4.2373967 -4.2138948 -4.1801066 -4.141458 -4.1075478 -4.08487 -4.0723548 -4.0586948 -4.0470366 -4.0483642 -4.0709896 -4.1095085 -4.1555147][-4.2100239 -4.2170687 -4.2119069 -4.1876488 -4.1526523 -4.1147947 -4.0849881 -4.074163 -4.0777707 -4.0810595 -4.0847354 -4.0951176 -4.11557 -4.1423545 -4.1739416][-4.155035 -4.1686091 -4.1688905 -4.1471839 -4.1151295 -4.0849586 -4.0675945 -4.0730791 -4.0916643 -4.1104512 -4.1276503 -4.1438365 -4.1607389 -4.175508 -4.1909394][-4.104382 -4.1219397 -4.124279 -4.1053677 -4.0804024 -4.0597782 -4.0522728 -4.06794 -4.0958095 -4.121902 -4.1436043 -4.1607137 -4.1778688 -4.192193 -4.2032223][-4.0805387 -4.102334 -4.1065788 -4.094707 -4.0838552 -4.0773811 -4.0775747 -4.0961137 -4.1219749 -4.142168 -4.1566296 -4.1709042 -4.1911459 -4.2121429 -4.2283673][-4.0898967 -4.1164761 -4.1289325 -4.13108 -4.1365843 -4.1396341 -4.1419172 -4.1534433 -4.1647787 -4.1710935 -4.1748247 -4.184248 -4.2060785 -4.2322283 -4.2560205][-4.1196465 -4.1468277 -4.1673918 -4.1807327 -4.1929054 -4.1998119 -4.2018127 -4.2041965 -4.1984935 -4.1876392 -4.1780272 -4.179101 -4.1993628 -4.2298927 -4.2627816][-4.1459122 -4.168736 -4.1914821 -4.2095265 -4.2269063 -4.2403488 -4.2467108 -4.2431483 -4.2232251 -4.194612 -4.1698589 -4.159162 -4.1747193 -4.20906 -4.2514839][-4.1732779 -4.1909723 -4.2113638 -4.2319 -4.25308 -4.2723289 -4.2817688 -4.2745132 -4.2467771 -4.2069626 -4.1704311 -4.1514378 -4.1631179 -4.1984315 -4.2435861][-4.2206779 -4.231256 -4.2465868 -4.2645583 -4.2827282 -4.2987576 -4.3063784 -4.29915 -4.2753496 -4.2403502 -4.2075891 -4.1907945 -4.1996627 -4.2252035 -4.2553043][-4.2732491 -4.2773762 -4.2859936 -4.2966275 -4.3065448 -4.3154254 -4.3197064 -4.3155122 -4.3008337 -4.2804456 -4.2598977 -4.2464223 -4.245986 -4.2519021 -4.2560935][-4.3196769 -4.3194551 -4.3220716 -4.3250046 -4.3262091 -4.3271155 -4.3266668 -4.3220606 -4.3111014 -4.2990379 -4.2860508 -4.2712173 -4.2586141 -4.2457576 -4.2281575]]...]
INFO - root - 2017-12-06 09:47:51.787989: step 4210, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:21m:57s remains)
INFO - root - 2017-12-06 09:47:53.931298: step 4220, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-06 09:47:56.090221: step 4230, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:33m:47s remains)
INFO - root - 2017-12-06 09:47:58.243985: step 4240, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-06 09:48:00.389209: step 4250, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.210 sec/batch; 19h:10m:45s remains)
INFO - root - 2017-12-06 09:48:02.556123: step 4260, loss = 2.10, batch loss = 2.04 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:36s remains)
INFO - root - 2017-12-06 09:48:04.775974: step 4270, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:40m:22s remains)
INFO - root - 2017-12-06 09:48:06.968281: step 4280, loss = 2.07, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:02s remains)
INFO - root - 2017-12-06 09:48:09.160195: step 4290, loss = 2.09, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:07m:50s remains)
INFO - root - 2017-12-06 09:48:11.331203: step 4300, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:21s remains)
2017-12-06 09:48:11.714635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1710491 -4.1706829 -4.1711178 -4.1637897 -4.1449523 -4.119452 -4.0985093 -4.0968084 -4.1165142 -4.1402431 -4.1611505 -4.1752419 -4.1792245 -4.1747732 -4.1659956][-4.1588187 -4.1616006 -4.1625404 -4.1502123 -4.1201229 -4.0820632 -4.0560236 -4.0605359 -4.0914292 -4.1234031 -4.1503005 -4.1695347 -4.1774096 -4.1736083 -4.1628122][-4.1537189 -4.1572714 -4.1546044 -4.1325345 -4.0892835 -4.0412383 -4.0143466 -4.0270057 -4.0690031 -4.1107721 -4.144815 -4.1679916 -4.1769533 -4.1724048 -4.1575418][-4.1549549 -4.1562529 -4.1467223 -4.1140251 -4.0593071 -4.0067449 -3.9829373 -4.0048208 -4.0569234 -4.1079292 -4.1487904 -4.1737747 -4.1809478 -4.17297 -4.1506157][-4.1626105 -4.1560092 -4.1364779 -4.0936694 -4.0312905 -3.97983 -3.96093 -3.9933753 -4.0570717 -4.1168675 -4.1638594 -4.1902466 -4.195446 -4.1803055 -4.1453753][-4.1756539 -4.1576595 -4.1273308 -4.0761924 -4.0099559 -3.9614801 -3.9451661 -3.9876504 -4.0622244 -4.1295452 -4.1823759 -4.2111912 -4.214582 -4.1893291 -4.1378808][-4.1823573 -4.1550465 -4.1186342 -4.0632362 -3.9948637 -3.9450309 -3.9260077 -3.9778304 -4.0642433 -4.1394777 -4.1967511 -4.2286787 -4.2312727 -4.1986728 -4.1350441][-4.1770024 -4.14581 -4.112298 -4.0625405 -3.9980285 -3.944201 -3.9204407 -3.977062 -4.0698185 -4.1501951 -4.2089515 -4.2432308 -4.2467752 -4.2130322 -4.1468735][-4.1588249 -4.1294975 -4.1067333 -4.0750632 -4.0276651 -3.9802392 -3.9600875 -4.0130792 -4.1004968 -4.1777558 -4.2305412 -4.2609997 -4.262753 -4.2316546 -4.1714048][-4.1394768 -4.1157703 -4.1059489 -4.0941882 -4.0693154 -4.0371366 -4.028286 -4.0768538 -4.1530433 -4.2188368 -4.2580619 -4.2777467 -4.2740717 -4.2469053 -4.195991][-4.1221581 -4.1073022 -4.1120052 -4.1200247 -4.1161866 -4.0999513 -4.1003003 -4.1421013 -4.2028704 -4.2513523 -4.2766919 -4.2865887 -4.2791324 -4.2568226 -4.2136436][-4.105391 -4.1021833 -4.1224308 -4.1481147 -4.1597652 -4.1553354 -4.1598883 -4.1916823 -4.235671 -4.269815 -4.2867785 -4.2914615 -4.2827458 -4.2622538 -4.2198925][-4.0977411 -4.105258 -4.1364923 -4.1722794 -4.1933017 -4.1977067 -4.2040882 -4.2251616 -4.2548862 -4.2804227 -4.2939715 -4.2967048 -4.2861443 -4.2622952 -4.2164025][-4.0998898 -4.115675 -4.1501603 -4.1859436 -4.2080731 -4.2170687 -4.2244 -4.2363605 -4.2564406 -4.278141 -4.2910719 -4.294858 -4.2844477 -4.2570572 -4.2114711][-4.1069117 -4.1292124 -4.1628833 -4.1949472 -4.2126565 -4.2202559 -4.2233558 -4.22261 -4.23307 -4.2537866 -4.2693973 -4.2785482 -4.2718191 -4.2444119 -4.2042584]]...]
INFO - root - 2017-12-06 09:48:13.916305: step 4310, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:55s remains)
INFO - root - 2017-12-06 09:48:16.042740: step 4320, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:12m:16s remains)
INFO - root - 2017-12-06 09:48:18.181057: step 4330, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:45s remains)
INFO - root - 2017-12-06 09:48:20.400304: step 4340, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:34s remains)
INFO - root - 2017-12-06 09:48:22.577544: step 4350, loss = 2.08, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:14m:53s remains)
INFO - root - 2017-12-06 09:48:24.807869: step 4360, loss = 2.10, batch loss = 2.04 (39.2 examples/sec; 0.204 sec/batch; 18h:34m:48s remains)
INFO - root - 2017-12-06 09:48:26.907384: step 4370, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:03m:50s remains)
INFO - root - 2017-12-06 09:48:29.082162: step 4380, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 20h:23m:06s remains)
INFO - root - 2017-12-06 09:48:31.321400: step 4390, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 21h:41m:36s remains)
INFO - root - 2017-12-06 09:48:33.559843: step 4400, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:09s remains)
2017-12-06 09:48:33.957587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2178617 -4.2390161 -4.2522678 -4.2420158 -4.2298694 -4.2205219 -4.2165966 -4.2097034 -4.1985312 -4.1885781 -4.188386 -4.2005768 -4.2234955 -4.2477512 -4.25326][-4.2221036 -4.2452822 -4.2543569 -4.2374835 -4.2276578 -4.2276006 -4.2334242 -4.2298794 -4.2209778 -4.2156682 -4.2195492 -4.2309189 -4.2402649 -4.2420607 -4.2334733][-4.2075267 -4.2286754 -4.2326159 -4.212059 -4.2042842 -4.20924 -4.2208266 -4.2290783 -4.2315097 -4.2306747 -4.2333775 -4.2407303 -4.235002 -4.2192426 -4.2033329][-4.148746 -4.1697264 -4.1756344 -4.1582851 -4.1515951 -4.1568947 -4.1675124 -4.1810756 -4.193882 -4.1969514 -4.1972075 -4.1964951 -4.1809616 -4.1597538 -4.1451235][-4.0673141 -4.0877805 -4.0999207 -4.093298 -4.096117 -4.10641 -4.113595 -4.1231747 -4.1394148 -4.1436129 -4.1368027 -4.1284285 -4.1119256 -4.0902867 -4.0780678][-4.0144849 -4.0386114 -4.0530348 -4.0522242 -4.0625744 -4.0769506 -4.0747647 -4.0703363 -4.0798612 -4.0822859 -4.072001 -4.0613408 -4.0516915 -4.0396996 -4.0370417][-3.9949126 -4.0196776 -4.03019 -4.0207372 -4.0249519 -4.0316062 -4.0078497 -3.977922 -3.9818985 -4.0023842 -4.0048208 -4.0024695 -4.004458 -3.9990857 -4.0063415][-3.9859476 -4.0040536 -4.0026584 -3.9783072 -3.964077 -3.9434345 -3.8864481 -3.8227379 -3.8316097 -3.8931956 -3.9225008 -3.9353514 -3.9534073 -3.9611 -3.9828672][-4.023078 -4.0196209 -4.0034719 -3.9684579 -3.9411077 -3.9015851 -3.8323293 -3.7647576 -3.7872763 -3.8691666 -3.910552 -3.9332154 -3.9672508 -3.99382 -4.0319304][-4.075675 -4.0573735 -4.0381784 -4.0122004 -3.9923875 -3.9617085 -3.9151158 -3.8793514 -3.9065993 -3.9641888 -3.9959486 -4.017581 -4.0552535 -4.0866542 -4.12325][-4.1198297 -4.1035233 -4.0897861 -4.0734873 -4.060708 -4.0462937 -4.02608 -4.0152035 -4.0370617 -4.0691757 -4.0902019 -4.1070266 -4.1374621 -4.1609426 -4.1883922][-4.1633892 -4.1555266 -4.1490173 -4.13898 -4.1287923 -4.1227403 -4.1180167 -4.1166434 -4.1269722 -4.1410904 -4.1541653 -4.1681108 -4.1896877 -4.2018819 -4.2163982][-4.2001686 -4.1943316 -4.1908679 -4.1842356 -4.1772385 -4.1775303 -4.1804013 -4.17842 -4.1777778 -4.1810427 -4.1878796 -4.1956282 -4.204349 -4.2039418 -4.2071567][-4.2347145 -4.2284555 -4.2250118 -4.2203469 -4.2173553 -4.2209611 -4.2242513 -4.21917 -4.2119617 -4.2077794 -4.2072663 -4.2049184 -4.1974354 -4.1815338 -4.1726203][-4.26767 -4.2633948 -4.2607059 -4.2574897 -4.2560415 -4.2576265 -4.2592778 -4.2562747 -4.2513037 -4.2480536 -4.2455697 -4.2368317 -4.2180157 -4.1884751 -4.1615372]]...]
INFO - root - 2017-12-06 09:48:36.196515: step 4410, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:12s remains)
INFO - root - 2017-12-06 09:48:38.363747: step 4420, loss = 2.07, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:34s remains)
INFO - root - 2017-12-06 09:48:40.525156: step 4430, loss = 2.06, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:12m:24s remains)
INFO - root - 2017-12-06 09:48:42.706824: step 4440, loss = 2.08, batch loss = 2.03 (37.5 examples/sec; 0.214 sec/batch; 19h:27m:48s remains)
INFO - root - 2017-12-06 09:48:44.914146: step 4450, loss = 2.09, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 21h:08m:07s remains)
INFO - root - 2017-12-06 09:48:47.132358: step 4460, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.214 sec/batch; 19h:27m:42s remains)
INFO - root - 2017-12-06 09:48:49.271192: step 4470, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:17s remains)
INFO - root - 2017-12-06 09:48:51.472501: step 4480, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.219 sec/batch; 19h:59m:58s remains)
INFO - root - 2017-12-06 09:48:53.743075: step 4490, loss = 2.09, batch loss = 2.04 (35.4 examples/sec; 0.226 sec/batch; 20h:35m:36s remains)
INFO - root - 2017-12-06 09:48:55.978665: step 4500, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:09s remains)
2017-12-06 09:48:56.378906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2986956 -4.2899642 -4.2877159 -4.2886648 -4.2836614 -4.2803164 -4.2723122 -4.2616978 -4.2673273 -4.2770476 -4.2804394 -4.2891855 -4.3040814 -4.3156009 -4.3201957][-4.3004928 -4.2891722 -4.2825174 -4.2784896 -4.2678781 -4.2563877 -4.2348075 -4.2133279 -4.2231979 -4.243083 -4.2534909 -4.2718039 -4.2973003 -4.3148313 -4.3205333][-4.307569 -4.2944283 -4.2840638 -4.2698064 -4.2485847 -4.2261629 -4.1861448 -4.1520486 -4.1707373 -4.2068162 -4.2330952 -4.2639031 -4.2980423 -4.3188815 -4.3250971][-4.3101096 -4.2987766 -4.2851348 -4.2616649 -4.2332835 -4.1981459 -4.1341338 -4.0898614 -4.1246819 -4.1824608 -4.2239256 -4.2618275 -4.2986097 -4.3210506 -4.3289638][-4.3092947 -4.3013744 -4.2858038 -4.2533 -4.2159147 -4.1666222 -4.0771995 -4.0306258 -4.0883679 -4.1670504 -4.2200623 -4.2656384 -4.3038735 -4.3243866 -4.3313026][-4.3055682 -4.3012528 -4.2838826 -4.2399654 -4.1854582 -4.1093321 -3.9914808 -3.9511418 -4.0434246 -4.1429996 -4.2074575 -4.2628169 -4.3066239 -4.3300657 -4.3340588][-4.302568 -4.30211 -4.2842617 -4.23151 -4.1589608 -4.0457077 -3.8848705 -3.8488843 -3.9897714 -4.1190548 -4.1940169 -4.2565913 -4.3062863 -4.3355403 -4.339417][-4.3004255 -4.308496 -4.2944531 -4.2461944 -4.1712465 -4.0460982 -3.8660591 -3.8237202 -3.9834704 -4.1257453 -4.200511 -4.2608137 -4.307322 -4.3354211 -4.3398852][-4.2935467 -4.3097181 -4.3046117 -4.2702131 -4.2131376 -4.1137195 -3.9772854 -3.9424758 -4.0599108 -4.1701221 -4.2260928 -4.2756848 -4.3111515 -4.3321896 -4.3342953][-4.283884 -4.2968373 -4.29487 -4.2732797 -4.2373433 -4.1739278 -4.09612 -4.0861406 -4.1595263 -4.2267003 -4.2552376 -4.2861538 -4.3091097 -4.3233304 -4.3231463][-4.2705073 -4.2722259 -4.2701168 -4.2582569 -4.2378287 -4.1995158 -4.1593342 -4.1689591 -4.2234564 -4.268086 -4.2811427 -4.2917094 -4.3007221 -4.3090072 -4.3097858][-4.258841 -4.2551689 -4.2510347 -4.2406087 -4.2278361 -4.2067723 -4.1891723 -4.207283 -4.2507253 -4.2828412 -4.2898593 -4.2847261 -4.2825379 -4.287539 -4.2902918][-4.2522807 -4.2529831 -4.2496557 -4.2359076 -4.2245326 -4.2163153 -4.2171297 -4.2362895 -4.2623553 -4.2782984 -4.2798696 -4.2684712 -4.2616296 -4.2660642 -4.26886][-4.2529941 -4.25992 -4.2593293 -4.2436519 -4.2337532 -4.2349153 -4.2437196 -4.2604623 -4.2712965 -4.2744908 -4.2714872 -4.2594509 -4.2505736 -4.2514391 -4.2514114][-4.2622976 -4.2740183 -4.2763028 -4.2607031 -4.2492604 -4.2494125 -4.2552819 -4.2638555 -4.2668071 -4.2661657 -4.2633505 -4.2583447 -4.2522326 -4.2500105 -4.2454286]]...]
INFO - root - 2017-12-06 09:48:58.534663: step 4510, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:55m:29s remains)
INFO - root - 2017-12-06 09:49:00.704552: step 4520, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:54s remains)
INFO - root - 2017-12-06 09:49:02.863665: step 4530, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:24m:07s remains)
INFO - root - 2017-12-06 09:49:05.089845: step 4540, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:54m:16s remains)
INFO - root - 2017-12-06 09:49:07.247200: step 4550, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-06 09:49:09.435871: step 4560, loss = 2.10, batch loss = 2.04 (35.8 examples/sec; 0.224 sec/batch; 20h:22m:48s remains)
INFO - root - 2017-12-06 09:49:11.614114: step 4570, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:14m:31s remains)
INFO - root - 2017-12-06 09:49:13.755094: step 4580, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:05m:59s remains)
INFO - root - 2017-12-06 09:49:15.949425: step 4590, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:59s remains)
INFO - root - 2017-12-06 09:49:18.110377: step 4600, loss = 2.10, batch loss = 2.04 (37.5 examples/sec; 0.214 sec/batch; 19h:26m:57s remains)
2017-12-06 09:49:18.487942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3127241 -4.2982593 -4.2868657 -4.2825141 -4.2824931 -4.2817035 -4.2811966 -4.2880516 -4.299603 -4.3085194 -4.3106289 -4.3089833 -4.3078852 -4.3059783 -4.2996173][-4.2999 -4.2803812 -4.2646818 -4.2596946 -4.2596965 -4.2562094 -4.2514482 -4.2584677 -4.274478 -4.2884722 -4.2929654 -4.2926536 -4.2915368 -4.2883549 -4.278986][-4.2850757 -4.2616415 -4.2419066 -4.2349262 -4.2350931 -4.2296972 -4.2216182 -4.228075 -4.2505932 -4.2723789 -4.2817893 -4.2853465 -4.2850657 -4.2781525 -4.2642894][-4.2719703 -4.2462535 -4.2221904 -4.2094579 -4.2070861 -4.1982517 -4.1837921 -4.1850471 -4.2142463 -4.2487164 -4.26852 -4.2810764 -4.2872877 -4.2811651 -4.2636609][-4.2659817 -4.2396016 -4.2118583 -4.1914496 -4.1803803 -4.1627512 -4.133038 -4.1219835 -4.1576424 -4.2090354 -4.2430968 -4.2702155 -4.2916493 -4.2953668 -4.2792459][-4.2671156 -4.2423859 -4.2124929 -4.1850786 -4.1608534 -4.1250324 -4.0672216 -4.0305996 -4.0701218 -4.1454144 -4.2011309 -4.2452412 -4.2846212 -4.3038921 -4.2974606][-4.2736607 -4.2527013 -4.2241435 -4.1946144 -4.1631055 -4.1081586 -4.0133982 -3.935451 -3.9675817 -4.0654736 -4.1467686 -4.2087107 -4.2627692 -4.2973075 -4.3050795][-4.2849021 -4.2691903 -4.2448707 -4.21867 -4.1903696 -4.1343884 -4.0255661 -3.914649 -3.9159698 -4.0079432 -4.0999403 -4.1731825 -4.2342057 -4.2759295 -4.2940445][-4.294354 -4.28495 -4.2670069 -4.246254 -4.2232275 -4.1793222 -4.0897455 -3.9869049 -3.9594462 -4.0130844 -4.0844207 -4.1504474 -4.2056813 -4.2419071 -4.2622309][-4.2977958 -4.2930117 -4.2819257 -4.2666054 -4.2488089 -4.2189012 -4.1567 -4.0820804 -4.0517449 -4.0734305 -4.1136975 -4.1566167 -4.19371 -4.2153769 -4.2302761][-4.2902341 -4.2837358 -4.2760458 -4.2678347 -4.2589865 -4.2443838 -4.2088943 -4.1652184 -4.1448545 -4.1514711 -4.1683507 -4.1846533 -4.1979394 -4.2056708 -4.2131763][-4.2796588 -4.2699614 -4.2641072 -4.2614665 -4.2607617 -4.258215 -4.2435193 -4.2257094 -4.2191515 -4.2211432 -4.2237687 -4.2216282 -4.2159977 -4.2136841 -4.2174683][-4.2774177 -4.2673755 -4.2638435 -4.2652993 -4.2664523 -4.2677011 -4.2628713 -4.2597308 -4.2643204 -4.26734 -4.2648335 -4.2545271 -4.240489 -4.234242 -4.2369871][-4.2837029 -4.2756581 -4.2749643 -4.2792435 -4.280159 -4.2785954 -4.2745767 -4.2760119 -4.2851095 -4.2902813 -4.2874751 -4.2760339 -4.2610073 -4.2547503 -4.2593622][-4.2955918 -4.289289 -4.2896509 -4.2950597 -4.296474 -4.2918296 -4.284667 -4.2845731 -4.292479 -4.2988997 -4.2986789 -4.2889066 -4.2751155 -4.2692943 -4.2750716]]...]
INFO - root - 2017-12-06 09:49:20.661870: step 4610, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:40m:03s remains)
INFO - root - 2017-12-06 09:49:22.831421: step 4620, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:23m:55s remains)
INFO - root - 2017-12-06 09:49:25.003556: step 4630, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-06 09:49:27.152042: step 4640, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:30m:25s remains)
INFO - root - 2017-12-06 09:49:29.306964: step 4650, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:52s remains)
INFO - root - 2017-12-06 09:49:31.463912: step 4660, loss = 2.10, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:14s remains)
INFO - root - 2017-12-06 09:49:33.679234: step 4670, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:23m:55s remains)
INFO - root - 2017-12-06 09:49:35.830480: step 4680, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:03s remains)
INFO - root - 2017-12-06 09:49:37.961836: step 4690, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:41s remains)
INFO - root - 2017-12-06 09:49:40.183512: step 4700, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:21m:24s remains)
2017-12-06 09:49:40.592244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2295895 -4.2510667 -4.2658787 -4.2626176 -4.2375789 -4.2007141 -4.1868482 -4.2123728 -4.2535987 -4.28899 -4.3086147 -4.3085876 -4.2942138 -4.2640324 -4.2138944][-4.2340789 -4.2671366 -4.28765 -4.276433 -4.2442064 -4.1936555 -4.1671405 -4.1885471 -4.2353258 -4.2779856 -4.3018713 -4.3050637 -4.2935905 -4.2628684 -4.2073035][-4.2421055 -4.2827797 -4.3009591 -4.2854939 -4.2550926 -4.1999326 -4.16431 -4.1792097 -4.2255578 -4.2710767 -4.30136 -4.3125186 -4.3037271 -4.2732038 -4.2135296][-4.2465591 -4.286139 -4.302424 -4.2868915 -4.2608933 -4.2080889 -4.1653261 -4.1722188 -4.2127881 -4.2618585 -4.3007007 -4.3245797 -4.3237572 -4.2967162 -4.2369623][-4.2409558 -4.2765231 -4.2950449 -4.281754 -4.2565918 -4.2012482 -4.1448479 -4.1306448 -4.1606212 -4.2197738 -4.2763991 -4.3156476 -4.3253865 -4.30467 -4.2515922][-4.2305808 -4.2690339 -4.2923269 -4.2811069 -4.2516775 -4.1877408 -4.1058273 -4.0467234 -4.0470443 -4.131711 -4.2238684 -4.2843194 -4.3087292 -4.300323 -4.2580338][-4.216558 -4.2630234 -4.2926397 -4.2857251 -4.2561789 -4.187902 -4.0825162 -3.9762006 -3.9358313 -4.0507607 -4.1823769 -4.2626824 -4.2990303 -4.3021069 -4.2726951][-4.2089419 -4.2601457 -4.2936821 -4.2920556 -4.2566319 -4.1876984 -4.0857768 -3.9775488 -3.9278533 -4.035018 -4.1701169 -4.2561703 -4.296474 -4.306284 -4.2862039][-4.2155395 -4.2684388 -4.3030071 -4.3007565 -4.2657285 -4.2072296 -4.1336179 -4.0618196 -4.0265789 -4.0996766 -4.2047892 -4.2762866 -4.3083177 -4.3176069 -4.3017626][-4.2414331 -4.2902083 -4.3195457 -4.3177905 -4.29171 -4.2507033 -4.2028174 -4.15712 -4.1255488 -4.1677227 -4.2449846 -4.29861 -4.3161783 -4.3217969 -4.31174][-4.2712531 -4.3120589 -4.3348355 -4.3361964 -4.3219714 -4.2934065 -4.2572923 -4.2169027 -4.1802654 -4.202702 -4.2661905 -4.3113594 -4.3266563 -4.3317351 -4.3247933][-4.2928462 -4.3227787 -4.338686 -4.3427458 -4.3359432 -4.3163667 -4.2836542 -4.2447405 -4.2048969 -4.2153769 -4.2737556 -4.31894 -4.3385987 -4.343163 -4.3383408][-4.3042297 -4.327148 -4.3414063 -4.3476954 -4.3439393 -4.3334794 -4.3089032 -4.27237 -4.2316179 -4.2321706 -4.2823186 -4.32703 -4.3496361 -4.3548074 -4.3519239][-4.3113389 -4.3307514 -4.3445082 -4.3514915 -4.3491988 -4.343595 -4.3253193 -4.29464 -4.2603412 -4.2574949 -4.2954478 -4.3332019 -4.354012 -4.3597107 -4.3586388][-4.3186321 -4.3348751 -4.3459163 -4.3512855 -4.3513255 -4.3474388 -4.3307586 -4.3057222 -4.281436 -4.2774825 -4.3012352 -4.328969 -4.344974 -4.349061 -4.3479972]]...]
INFO - root - 2017-12-06 09:49:42.731349: step 4710, loss = 2.08, batch loss = 2.02 (39.2 examples/sec; 0.204 sec/batch; 18h:33m:31s remains)
INFO - root - 2017-12-06 09:49:44.885265: step 4720, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:44m:44s remains)
INFO - root - 2017-12-06 09:49:47.058601: step 4730, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:27s remains)
INFO - root - 2017-12-06 09:49:49.244529: step 4740, loss = 2.07, batch loss = 2.02 (35.0 examples/sec; 0.228 sec/batch; 20h:47m:35s remains)
INFO - root - 2017-12-06 09:49:51.431717: step 4750, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:46m:21s remains)
INFO - root - 2017-12-06 09:49:53.639291: step 4760, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:04m:57s remains)
INFO - root - 2017-12-06 09:49:55.860311: step 4770, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:23m:43s remains)
INFO - root - 2017-12-06 09:49:58.025846: step 4780, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:37s remains)
INFO - root - 2017-12-06 09:50:00.190655: step 4790, loss = 2.08, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:59s remains)
INFO - root - 2017-12-06 09:50:02.365703: step 4800, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:16m:12s remains)
2017-12-06 09:50:02.715207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2075863 -4.2035241 -4.2023807 -4.2054758 -4.2107291 -4.2159414 -4.222363 -4.2322645 -4.2416782 -4.2475567 -4.2509875 -4.2552905 -4.2654133 -4.28219 -4.2978745][-4.2111526 -4.2050829 -4.2056866 -4.2134461 -4.2244449 -4.2335057 -4.2408242 -4.2496023 -4.2578497 -4.2621508 -4.2645206 -4.2703943 -4.2808852 -4.2948995 -4.3064718][-4.2265792 -4.2199612 -4.2206688 -4.228888 -4.2397819 -4.2468314 -4.2516389 -4.2586546 -4.2655673 -4.2679167 -4.2666917 -4.2686415 -4.2756219 -4.2859273 -4.2941675][-4.2436242 -4.2351313 -4.2356453 -4.2430229 -4.2491746 -4.2494435 -4.247261 -4.250803 -4.2573028 -4.2599449 -4.2564945 -4.2520761 -4.2537293 -4.2606215 -4.2664738][-4.2521815 -4.237834 -4.2337823 -4.2371893 -4.235074 -4.2209287 -4.2050986 -4.203033 -4.2133436 -4.2226925 -4.2220216 -4.2159743 -4.2173777 -4.2236772 -4.2285738][-4.2476864 -4.2263865 -4.2145638 -4.209929 -4.1936865 -4.1568489 -4.1198382 -4.1149735 -4.1407156 -4.1673813 -4.1762919 -4.1759682 -4.1843781 -4.1952767 -4.2006068][-4.2269859 -4.2053494 -4.189815 -4.1782355 -4.1461644 -4.0812316 -4.0154343 -4.0086918 -4.0609403 -4.1130385 -4.1373205 -4.1476722 -4.1663027 -4.1832571 -4.1892462][-4.1928959 -4.1746407 -4.1585045 -4.1485138 -4.1134443 -4.0318632 -3.944546 -3.9395425 -4.0144467 -4.0844316 -4.1188927 -4.1371527 -4.160893 -4.1815453 -4.1884708][-4.1581063 -4.1444063 -4.1327853 -4.1366982 -4.1212378 -4.0569296 -3.9851134 -3.9875457 -4.0581594 -4.1197615 -4.14884 -4.1625328 -4.1797996 -4.1953955 -4.1992321][-4.1243653 -4.1132364 -4.1107974 -4.13427 -4.1445041 -4.1123409 -4.0709467 -4.0773244 -4.1275482 -4.17052 -4.1910815 -4.19845 -4.205605 -4.2129111 -4.2127113][-4.1033297 -4.0915117 -4.101964 -4.1422186 -4.1689911 -4.1600695 -4.1403937 -4.1461515 -4.1790528 -4.2098913 -4.2272382 -4.2335172 -4.23635 -4.2377405 -4.2355556][-4.1016569 -4.0910273 -4.113153 -4.1585855 -4.1881623 -4.191072 -4.1861548 -4.1937714 -4.2173152 -4.2400765 -4.25604 -4.2647748 -4.2687874 -4.2679749 -4.263617][-4.1254082 -4.1181469 -4.1418242 -4.1768866 -4.1976628 -4.2026272 -4.2059298 -4.2167888 -4.2369552 -4.2557774 -4.2719035 -4.2837524 -4.2899556 -4.288991 -4.2818575][-4.1690769 -4.1612406 -4.1766768 -4.196918 -4.208467 -4.2123618 -4.218966 -4.231544 -4.2483568 -4.2622867 -4.2763972 -4.2901411 -4.2970905 -4.295146 -4.2867103][-4.1984363 -4.1915126 -4.2029142 -4.2151012 -4.2225833 -4.2284365 -4.2387571 -4.2503657 -4.2585645 -4.2621317 -4.268518 -4.2802329 -4.2854671 -4.2794909 -4.26874]]...]
INFO - root - 2017-12-06 09:50:04.897085: step 4810, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:19s remains)
INFO - root - 2017-12-06 09:50:07.113186: step 4820, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 19h:53m:39s remains)
INFO - root - 2017-12-06 09:50:09.255817: step 4830, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:24m:45s remains)
INFO - root - 2017-12-06 09:50:11.411420: step 4840, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 19h:06m:37s remains)
INFO - root - 2017-12-06 09:50:13.548431: step 4850, loss = 2.07, batch loss = 2.02 (39.1 examples/sec; 0.205 sec/batch; 18h:38m:10s remains)
INFO - root - 2017-12-06 09:50:15.720925: step 4860, loss = 2.11, batch loss = 2.05 (35.8 examples/sec; 0.224 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-06 09:50:17.859627: step 4870, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:19s remains)
INFO - root - 2017-12-06 09:50:20.047580: step 4880, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:34s remains)
INFO - root - 2017-12-06 09:50:22.277657: step 4890, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:53s remains)
INFO - root - 2017-12-06 09:50:24.434454: step 4900, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:07s remains)
2017-12-06 09:50:24.786475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2450471 -4.2260861 -4.2052646 -4.1967936 -4.2001 -4.2086234 -4.2185092 -4.2293453 -4.2419133 -4.2553596 -4.2571797 -4.2448282 -4.2286139 -4.218379 -4.2167864][-4.2649331 -4.252409 -4.2336721 -4.2250686 -4.2277403 -4.2350249 -4.2411251 -4.2498789 -4.2645783 -4.2813396 -4.285265 -4.2757368 -4.262291 -4.2495861 -4.2412062][-4.2725677 -4.2642775 -4.2450452 -4.2329431 -4.2338247 -4.2390079 -4.2401819 -4.2482505 -4.2668781 -4.2873416 -4.2946563 -4.290575 -4.2822909 -4.27012 -4.257287][-4.26225 -4.2496428 -4.226882 -4.2116117 -4.20918 -4.2127466 -4.2085605 -4.2127714 -4.2325759 -4.254889 -4.2655654 -4.268352 -4.2681737 -4.2621412 -4.2499828][-4.2400537 -4.2188144 -4.1918421 -4.1736388 -4.1704121 -4.1720467 -4.1613832 -4.1584897 -4.1727438 -4.191761 -4.2034307 -4.214519 -4.2265735 -4.2351875 -4.2356997][-4.2196369 -4.1933002 -4.1647038 -4.148355 -4.1471672 -4.1482739 -4.1354337 -4.1293025 -4.1363597 -4.143662 -4.1482468 -4.1639404 -4.1861367 -4.2084389 -4.2231355][-4.1990366 -4.1715083 -4.1497636 -4.1417065 -4.1430821 -4.1403041 -4.1241536 -4.1165819 -4.1169162 -4.1072822 -4.0996895 -4.12081 -4.1528506 -4.1835113 -4.2074738][-4.2050967 -4.1761189 -4.1542 -4.1482797 -4.1473551 -4.1390209 -4.1189747 -4.1105103 -4.10489 -4.0832896 -4.0670352 -4.0901523 -4.1284022 -4.1655335 -4.1978612][-4.2453594 -4.2170882 -4.1898828 -4.177588 -4.1687717 -4.1583886 -4.1446867 -4.140799 -4.1356244 -4.1142244 -4.0964313 -4.1124215 -4.1414952 -4.1723166 -4.2020698][-4.2782 -4.2562752 -4.2304649 -4.2125163 -4.2004838 -4.1963892 -4.1968446 -4.2039156 -4.2033672 -4.1871524 -4.1697783 -4.1751161 -4.1909127 -4.208178 -4.2248707][-4.2734804 -4.2580709 -4.237834 -4.219347 -4.2095585 -4.212296 -4.2257733 -4.24745 -4.2592583 -4.2513719 -4.2356496 -4.2331128 -4.2398529 -4.2457705 -4.2505078][-4.2584224 -4.2408738 -4.2188153 -4.1963596 -4.1872978 -4.196775 -4.2208085 -4.2555761 -4.2796736 -4.2823181 -4.2748051 -4.2717686 -4.2726874 -4.2711658 -4.2694745][-4.26649 -4.2446809 -4.2118955 -4.1747532 -4.1546211 -4.1606936 -4.1874409 -4.2274137 -4.2598238 -4.2731757 -4.2748652 -4.2762394 -4.275177 -4.2722673 -4.268342][-4.2783165 -4.2547989 -4.2148919 -4.1624532 -4.124198 -4.1165452 -4.1347365 -4.1683307 -4.2014537 -4.2206278 -4.2312031 -4.2427011 -4.24717 -4.2464342 -4.2433019][-4.2918611 -4.2690587 -4.2302122 -4.1788211 -4.1345048 -4.1087513 -4.1035838 -4.1181788 -4.1415243 -4.1556726 -4.1691732 -4.1908727 -4.2057018 -4.2133245 -4.2189355]]...]
INFO - root - 2017-12-06 09:50:26.921615: step 4910, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:15m:50s remains)
INFO - root - 2017-12-06 09:50:29.071042: step 4920, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:55m:48s remains)
INFO - root - 2017-12-06 09:50:31.218426: step 4930, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:10m:57s remains)
INFO - root - 2017-12-06 09:50:33.399538: step 4940, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:50s remains)
INFO - root - 2017-12-06 09:50:35.624069: step 4950, loss = 2.06, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:54m:29s remains)
INFO - root - 2017-12-06 09:50:37.793111: step 4960, loss = 2.10, batch loss = 2.05 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:08s remains)
INFO - root - 2017-12-06 09:50:39.944886: step 4970, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:38m:07s remains)
INFO - root - 2017-12-06 09:50:42.135257: step 4980, loss = 2.11, batch loss = 2.05 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:58s remains)
INFO - root - 2017-12-06 09:50:44.300856: step 4990, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:05m:12s remains)
INFO - root - 2017-12-06 09:50:46.462468: step 5000, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 18h:47m:18s remains)
2017-12-06 09:50:46.798854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2288451 -4.2349048 -4.231154 -4.2078643 -4.1641941 -4.1353331 -4.1355672 -4.1470246 -4.1594825 -4.1853065 -4.2199144 -4.2513742 -4.2761016 -4.2903504 -4.2802911][-4.2275791 -4.238863 -4.2342591 -4.2102022 -4.1710777 -4.1467175 -4.1463346 -4.1454148 -4.1505075 -4.1727848 -4.2070365 -4.2448506 -4.2724881 -4.2868152 -4.2825871][-4.205399 -4.2321715 -4.2347536 -4.2105908 -4.1791382 -4.1564403 -4.142221 -4.1280046 -4.1328883 -4.1608987 -4.1984892 -4.2440009 -4.2752404 -4.2904325 -4.2908][-4.1809592 -4.2317891 -4.247261 -4.2244806 -4.2 -4.1785879 -4.1363716 -4.0950828 -4.1063643 -4.1501408 -4.1870885 -4.228332 -4.2545962 -4.2679586 -4.277091][-4.1909451 -4.2504721 -4.271523 -4.2458754 -4.21896 -4.1848531 -4.0992703 -4.0205636 -4.0536504 -4.1232767 -4.1659088 -4.2004471 -4.2202296 -4.2414813 -4.2623367][-4.2122331 -4.2617216 -4.2770329 -4.2416668 -4.2012467 -4.1399016 -3.99098 -3.8722751 -3.9521737 -4.0673242 -4.1324391 -4.1683273 -4.1865425 -4.21853 -4.2534304][-4.2220373 -4.2585979 -4.2664309 -4.2236848 -4.1670961 -4.0715561 -3.8610241 -3.7119851 -3.8613579 -4.0285416 -4.10966 -4.1340008 -4.1410217 -4.1812668 -4.2312922][-4.2289829 -4.254622 -4.2550335 -4.2073092 -4.1560802 -4.0705009 -3.8762031 -3.7538052 -3.9190133 -4.0746646 -4.1326017 -4.1232128 -4.0992184 -4.1319995 -4.1876831][-4.2444477 -4.2635717 -4.2623653 -4.2170386 -4.1818552 -4.1327066 -4.0060606 -3.9358754 -4.0610929 -4.1671147 -4.1918249 -4.156395 -4.114923 -4.1407447 -4.1911798][-4.2694421 -4.274363 -4.2687654 -4.2337222 -4.209044 -4.1850181 -4.1133065 -4.0802288 -4.1669574 -4.2284303 -4.2255883 -4.1756186 -4.1317339 -4.1578608 -4.2043796][-4.2768741 -4.2740316 -4.2702103 -4.2468224 -4.2303085 -4.2218456 -4.1872492 -4.1738267 -4.2269726 -4.2549367 -4.2375822 -4.1922884 -4.1587181 -4.1805229 -4.2177892][-4.2872214 -4.2814341 -4.2793913 -4.2649922 -4.2533264 -4.2511559 -4.2380757 -4.2316694 -4.2621312 -4.2755361 -4.2622313 -4.2363729 -4.2064347 -4.2087874 -4.2230153][-4.3071027 -4.2994251 -4.2951117 -4.2796426 -4.2673383 -4.2668381 -4.2658048 -4.264812 -4.2809076 -4.2910576 -4.287591 -4.2786736 -4.2504835 -4.2341552 -4.2331228][-4.3179874 -4.3123817 -4.3078218 -4.2921019 -4.2756 -4.2710085 -4.2706227 -4.2690663 -4.2783294 -4.291008 -4.2955809 -4.2975273 -4.2810931 -4.2624726 -4.2560472][-4.3216791 -4.3169823 -4.315393 -4.3062525 -4.2918506 -4.2850642 -4.2838945 -4.2821126 -4.2854514 -4.2969551 -4.3032146 -4.3042536 -4.2953424 -4.2795167 -4.2705989]]...]
INFO - root - 2017-12-06 09:50:48.980832: step 5010, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:02m:49s remains)
INFO - root - 2017-12-06 09:50:51.105393: step 5020, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:38m:44s remains)
INFO - root - 2017-12-06 09:50:53.319640: step 5030, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:37s remains)
INFO - root - 2017-12-06 09:50:55.484322: step 5040, loss = 2.09, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:32m:15s remains)
INFO - root - 2017-12-06 09:50:57.653487: step 5050, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:44m:42s remains)
INFO - root - 2017-12-06 09:50:59.849787: step 5060, loss = 2.06, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:54m:06s remains)
INFO - root - 2017-12-06 09:51:01.978430: step 5070, loss = 2.10, batch loss = 2.04 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:58s remains)
INFO - root - 2017-12-06 09:51:04.101088: step 5080, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:22m:26s remains)
INFO - root - 2017-12-06 09:51:06.296230: step 5090, loss = 2.10, batch loss = 2.04 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:40s remains)
INFO - root - 2017-12-06 09:51:08.464375: step 5100, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:03m:24s remains)
2017-12-06 09:51:08.874796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1919312 -4.2169127 -4.2342639 -4.2233987 -4.2130027 -4.2298832 -4.249917 -4.2583437 -4.2606554 -4.2623882 -4.2714529 -4.286284 -4.2827673 -4.2537394 -4.2251096][-4.1606531 -4.1830196 -4.20515 -4.2028279 -4.198009 -4.2095776 -4.2260551 -4.2354207 -4.2386041 -4.24367 -4.2571206 -4.2744188 -4.2727618 -4.240725 -4.2077465][-4.1971269 -4.21225 -4.2261543 -4.2213116 -4.2168903 -4.2192106 -4.2244644 -4.2309437 -4.2377691 -4.2506113 -4.2702322 -4.2900872 -4.286757 -4.2548189 -4.2260995][-4.252902 -4.2578878 -4.2580442 -4.245975 -4.2348523 -4.22699 -4.221034 -4.2196283 -4.2310944 -4.2524271 -4.2784491 -4.2990375 -4.2928391 -4.2655697 -4.2496614][-4.2898769 -4.2895789 -4.2781997 -4.2558503 -4.2332253 -4.209353 -4.185998 -4.17226 -4.1859159 -4.2143359 -4.2449551 -4.268693 -4.2660837 -4.2481246 -4.2464175][-4.278049 -4.2782583 -4.261744 -4.2287116 -4.1851969 -4.1365528 -4.0911074 -4.0635295 -4.0865989 -4.1344347 -4.176548 -4.2095971 -4.2201219 -4.2204847 -4.232172][-4.2341623 -4.2321382 -4.2101979 -4.1657672 -4.1029906 -4.0301819 -3.9601934 -3.919723 -3.962219 -4.0454836 -4.111155 -4.1558409 -4.1796913 -4.1944776 -4.2144947][-4.1975908 -4.1925907 -4.168993 -4.1209536 -4.0519829 -3.9729834 -3.8975549 -3.8606524 -3.9218574 -4.0300546 -4.1098604 -4.1545367 -4.1783566 -4.1923165 -4.2081404][-4.2017894 -4.1936765 -4.1753764 -4.1384683 -4.0872574 -4.0336924 -3.9854388 -3.968348 -4.0204773 -4.1047025 -4.1658411 -4.196228 -4.2047439 -4.2025528 -4.2062197][-4.21876 -4.2134376 -4.2059779 -4.1837907 -4.15443 -4.1261044 -4.0999813 -4.093019 -4.1249576 -4.1770954 -4.2167668 -4.2337728 -4.227437 -4.2078362 -4.1992793][-4.2345738 -4.2358031 -4.2370205 -4.2270441 -4.2121859 -4.1992083 -4.18757 -4.1886 -4.2067709 -4.2333856 -4.25181 -4.2553382 -4.2419314 -4.2151566 -4.1987195][-4.2482328 -4.2532845 -4.2595181 -4.2587819 -4.2549119 -4.2516479 -4.2492414 -4.25377 -4.2624693 -4.2707181 -4.2723241 -4.267117 -4.2516055 -4.2237277 -4.2018323][-4.2722597 -4.2769208 -4.2823882 -4.2847538 -4.2845798 -4.2841358 -4.2853241 -4.28741 -4.2881289 -4.2862868 -4.2814426 -4.2741828 -4.2591915 -4.2340145 -4.20964][-4.3074007 -4.30881 -4.3074389 -4.3063097 -4.3057094 -4.3053832 -4.3050508 -4.304781 -4.302577 -4.2980595 -4.2927618 -4.2863526 -4.2741327 -4.2550316 -4.2352071][-4.3233728 -4.3222785 -4.3176026 -4.3140416 -4.3133879 -4.3149972 -4.3172441 -4.3184662 -4.3173413 -4.3141394 -4.3100524 -4.3058357 -4.2986512 -4.2881427 -4.2757349]]...]
INFO - root - 2017-12-06 09:51:11.019996: step 5110, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:54s remains)
INFO - root - 2017-12-06 09:51:13.133039: step 5120, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:10m:12s remains)
INFO - root - 2017-12-06 09:51:15.269272: step 5130, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:37s remains)
INFO - root - 2017-12-06 09:51:17.449067: step 5140, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:31s remains)
INFO - root - 2017-12-06 09:51:19.600196: step 5150, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 18h:55m:13s remains)
INFO - root - 2017-12-06 09:51:21.752986: step 5160, loss = 2.10, batch loss = 2.04 (37.5 examples/sec; 0.213 sec/batch; 19h:24m:16s remains)
INFO - root - 2017-12-06 09:51:23.929350: step 5170, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:42s remains)
INFO - root - 2017-12-06 09:51:26.062941: step 5180, loss = 2.08, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:46s remains)
INFO - root - 2017-12-06 09:51:28.245722: step 5190, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:08s remains)
INFO - root - 2017-12-06 09:51:30.365754: step 5200, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:09m:09s remains)
2017-12-06 09:51:30.699680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3049717 -4.2937107 -4.2844386 -4.2760649 -4.2731986 -4.2591796 -4.2375484 -4.2212381 -4.2248788 -4.2457666 -4.2624087 -4.2687416 -4.2588458 -4.2536974 -4.2386594][-4.3040237 -4.2930431 -4.2868505 -4.2832341 -4.2863388 -4.2755241 -4.2508321 -4.2246366 -4.2209973 -4.2357979 -4.2485352 -4.251349 -4.2386651 -4.2329111 -4.2238793][-4.3039217 -4.2932415 -4.2863779 -4.2827783 -4.2867708 -4.2752786 -4.2452359 -4.2134871 -4.2079635 -4.2202959 -4.2331829 -4.2359996 -4.2214451 -4.2125559 -4.2091565][-4.3026652 -4.2911787 -4.2808752 -4.27392 -4.2756424 -4.2641754 -4.2275972 -4.1894531 -4.1867647 -4.2064152 -4.2251291 -4.2294893 -4.21467 -4.20397 -4.2025123][-4.3001604 -4.2889423 -4.2773008 -4.2661676 -4.260931 -4.2454329 -4.203228 -4.159687 -4.1646433 -4.1966033 -4.2259164 -4.2318311 -4.217895 -4.2083149 -4.2073908][-4.29041 -4.2782784 -4.261941 -4.2452335 -4.2339125 -4.2186146 -4.1806364 -4.1359081 -4.14397 -4.1875157 -4.2240005 -4.228826 -4.2176065 -4.2132721 -4.2129445][-4.2671824 -4.2509656 -4.2288442 -4.2075329 -4.1963696 -4.1868806 -4.1609077 -4.1206393 -4.1209679 -4.1684179 -4.2112913 -4.2169137 -4.2063103 -4.2030082 -4.1982994][-4.24562 -4.2239928 -4.1969128 -4.1747675 -4.1669497 -4.1670356 -4.1559882 -4.1236954 -4.1130576 -4.1529651 -4.1978168 -4.2050095 -4.1939521 -4.1870956 -4.1790328][-4.2334161 -4.2108922 -4.1869116 -4.1689396 -4.164341 -4.1690569 -4.1692 -4.1519833 -4.1367373 -4.1620502 -4.2011003 -4.2120991 -4.2065058 -4.2048936 -4.2029238][-4.2344561 -4.2224965 -4.2096448 -4.2020392 -4.2021728 -4.206161 -4.2113504 -4.2089467 -4.2003031 -4.2149229 -4.2432837 -4.2528477 -4.2499065 -4.2511492 -4.2479954][-4.2438874 -4.2472439 -4.2442527 -4.2438803 -4.2500491 -4.2553844 -4.2640486 -4.27099 -4.2691922 -4.2768645 -4.2927051 -4.2983031 -4.2953706 -4.2931733 -4.2852187][-4.2512164 -4.2616453 -4.264483 -4.2705669 -4.2835879 -4.29169 -4.302237 -4.3103924 -4.3095012 -4.3120837 -4.3190165 -4.321032 -4.3189669 -4.31495 -4.3039026][-4.2669826 -4.2704215 -4.2726088 -4.2841163 -4.3013492 -4.3098373 -4.3178663 -4.3236284 -4.3215051 -4.3186297 -4.3190894 -4.3194332 -4.3197708 -4.3176341 -4.3076568][-4.2844739 -4.278965 -4.279067 -4.2946095 -4.3129749 -4.3169489 -4.3172865 -4.3201637 -4.3181696 -4.3137827 -4.3111033 -4.3093095 -4.3092017 -4.3073044 -4.2979007][-4.2997603 -4.2886448 -4.2843804 -4.2952061 -4.3081427 -4.3092613 -4.3062711 -4.3087344 -4.3099523 -4.308094 -4.3043356 -4.3010507 -4.2987924 -4.2950439 -4.2881489]]...]
INFO - root - 2017-12-06 09:51:32.880963: step 5210, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:05s remains)
INFO - root - 2017-12-06 09:51:35.024056: step 5220, loss = 2.09, batch loss = 2.04 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:16s remains)
INFO - root - 2017-12-06 09:51:37.167835: step 5230, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 19h:53m:41s remains)
INFO - root - 2017-12-06 09:51:39.326363: step 5240, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:31m:03s remains)
INFO - root - 2017-12-06 09:51:41.484600: step 5250, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-06 09:51:43.644209: step 5260, loss = 2.11, batch loss = 2.05 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:48s remains)
INFO - root - 2017-12-06 09:51:45.810100: step 5270, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:29m:09s remains)
INFO - root - 2017-12-06 09:51:47.952616: step 5280, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 19h:46m:15s remains)
INFO - root - 2017-12-06 09:51:50.114487: step 5290, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:15m:32s remains)
INFO - root - 2017-12-06 09:51:52.233524: step 5300, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:54s remains)
2017-12-06 09:51:52.545833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.322331 -4.3282938 -4.3233452 -4.3113828 -4.296371 -4.2885079 -4.2830462 -4.2826867 -4.2863083 -4.2850966 -4.2882462 -4.300262 -4.312233 -4.3214445 -4.3250256][-4.2931519 -4.3041716 -4.3003969 -4.2852488 -4.2633228 -4.248714 -4.2406616 -4.2429013 -4.2487617 -4.24487 -4.2458048 -4.2602744 -4.2755818 -4.2866073 -4.2876649][-4.2511806 -4.2675195 -4.2669683 -4.2491307 -4.2173109 -4.1936946 -4.1846333 -4.1968775 -4.2114687 -4.2078409 -4.2059298 -4.2195191 -4.2334352 -4.2428513 -4.2392426][-4.2034249 -4.2247028 -4.2296143 -4.2105417 -4.1666741 -4.1300564 -4.1209388 -4.1460752 -4.1741323 -4.1754003 -4.1728148 -4.1836143 -4.195682 -4.2033987 -4.1954913][-4.1652994 -4.1877112 -4.1948366 -4.1745529 -4.1194806 -4.0694919 -4.0594 -4.0952678 -4.13828 -4.1514812 -4.1537476 -4.1628275 -4.1729817 -4.178328 -4.1684132][-4.1461315 -4.1669679 -4.1707344 -4.1423469 -4.0705233 -4.0036173 -3.9887166 -4.0341506 -4.0936775 -4.1269455 -4.1409411 -4.1522222 -4.161427 -4.1628036 -4.1511388][-4.1546545 -4.1756124 -4.1752663 -4.1372719 -4.0506234 -3.9655509 -3.945601 -3.9991789 -4.0707526 -4.118772 -4.142663 -4.15593 -4.1635532 -4.1607685 -4.1495094][-4.1678572 -4.1898761 -4.1896749 -4.1503992 -4.0658979 -3.9846106 -3.9712541 -4.0260978 -4.0928922 -4.1375637 -4.1579943 -4.1652679 -4.1692295 -4.1684747 -4.1658769][-4.1653962 -4.1893764 -4.1923981 -4.1622729 -4.0947409 -4.0326209 -4.030158 -4.0777521 -4.1270428 -4.1551933 -4.1623626 -4.1612687 -4.1620154 -4.1670032 -4.1753979][-4.1578546 -4.1829205 -4.1916995 -4.1737447 -4.1246676 -4.0807886 -4.0829449 -4.11872 -4.1507735 -4.1623797 -4.1596723 -4.1549921 -4.1564403 -4.1664529 -4.1817918][-4.1490741 -4.1725903 -4.1856689 -4.1794057 -4.1480951 -4.1189861 -4.1221108 -4.14859 -4.1687336 -4.1693044 -4.1622925 -4.1588573 -4.16368 -4.1757097 -4.1919041][-4.1393919 -4.158618 -4.1735835 -4.1772041 -4.1607962 -4.1443157 -4.15098 -4.174552 -4.1877947 -4.1809821 -4.1716051 -4.170063 -4.1783285 -4.1914577 -4.2037072][-4.1399961 -4.1521821 -4.16595 -4.1740355 -4.1663766 -4.1577396 -4.1668019 -4.1883025 -4.1979146 -4.1880832 -4.1791945 -4.1793604 -4.1920047 -4.2056503 -4.2120576][-4.1463327 -4.153336 -4.1638331 -4.174396 -4.1738172 -4.1718478 -4.1814704 -4.1987381 -4.2048421 -4.1953897 -4.1883802 -4.190464 -4.2041454 -4.2151256 -4.2167544][-4.154541 -4.1597967 -4.1694784 -4.18236 -4.1861625 -4.1878657 -4.1940022 -4.2041559 -4.2069559 -4.1997108 -4.1959295 -4.200068 -4.2134261 -4.2214122 -4.2207236]]...]
INFO - root - 2017-12-06 09:51:54.705380: step 5310, loss = 2.12, batch loss = 2.06 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:49s remains)
INFO - root - 2017-12-06 09:51:56.833984: step 5320, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:58s remains)
INFO - root - 2017-12-06 09:51:58.977635: step 5330, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 18h:46m:24s remains)
INFO - root - 2017-12-06 09:52:01.172376: step 5340, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-06 09:52:03.345247: step 5350, loss = 2.09, batch loss = 2.03 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:18s remains)
INFO - root - 2017-12-06 09:52:05.529070: step 5360, loss = 2.09, batch loss = 2.04 (34.9 examples/sec; 0.229 sec/batch; 20h:49m:22s remains)
INFO - root - 2017-12-06 09:52:07.698775: step 5370, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-06 09:52:09.895908: step 5380, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:36m:19s remains)
INFO - root - 2017-12-06 09:52:12.068702: step 5390, loss = 2.09, batch loss = 2.04 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-06 09:52:14.222180: step 5400, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:49m:18s remains)
2017-12-06 09:52:14.621165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3008127 -4.3047228 -4.3088479 -4.3128233 -4.3158007 -4.3168459 -4.3179388 -4.3207583 -4.3172803 -4.3105583 -4.3043637 -4.3002424 -4.3031631 -4.3090515 -4.3125715][-4.2906427 -4.3006182 -4.31285 -4.3232851 -4.3298216 -4.3344355 -4.3392034 -4.3456068 -4.3421454 -4.3308611 -4.318862 -4.3064356 -4.3013477 -4.3021541 -4.304831][-4.2803144 -4.2957082 -4.3115926 -4.3234878 -4.3306174 -4.335063 -4.34126 -4.350831 -4.3502426 -4.3354983 -4.3156629 -4.2920303 -4.2778807 -4.276123 -4.2797394][-4.2656121 -4.2812476 -4.2960496 -4.3017225 -4.3015585 -4.3023944 -4.3079839 -4.3204641 -4.3264146 -4.3140516 -4.2934666 -4.2649927 -4.2502356 -4.2506475 -4.2584677][-4.2519965 -4.261435 -4.2732577 -4.2714725 -4.2587805 -4.2473097 -4.2477407 -4.266798 -4.2862234 -4.2863684 -4.2731175 -4.2501259 -4.2424421 -4.2476869 -4.2592087][-4.2394829 -4.2362518 -4.2343659 -4.2167463 -4.1861229 -4.1536317 -4.1451592 -4.1792932 -4.222898 -4.2456322 -4.2492981 -4.2393255 -4.2450771 -4.2605996 -4.2732987][-4.2320442 -4.2178264 -4.2001472 -4.1681023 -4.1190667 -4.0566058 -4.0298023 -4.0729961 -4.1381826 -4.186532 -4.2119384 -4.2199607 -4.2419891 -4.2694159 -4.2869124][-4.2190046 -4.2007265 -4.1700749 -4.11998 -4.0474114 -3.9502389 -3.890363 -3.9299605 -4.0148578 -4.0913582 -4.1472683 -4.1809812 -4.2154622 -4.2514362 -4.2757616][-4.2124028 -4.2008224 -4.1718554 -4.1195459 -4.0442615 -3.9404097 -3.8639731 -3.8874121 -3.9697897 -4.0491371 -4.1171956 -4.160717 -4.192996 -4.2263327 -4.2506876][-4.22227 -4.2273326 -4.2186503 -4.1914573 -4.1484427 -4.0773373 -4.0132909 -4.0103846 -4.0501156 -4.094523 -4.1400304 -4.1699286 -4.1886306 -4.2106552 -4.2308993][-4.2267704 -4.24216 -4.2470889 -4.2413669 -4.2269573 -4.1884818 -4.1407118 -4.1199327 -4.1214085 -4.1296172 -4.1451321 -4.1544833 -4.16109 -4.1785927 -4.2010317][-4.2169671 -4.2362509 -4.249999 -4.255723 -4.2551813 -4.235024 -4.2026491 -4.18119 -4.1652522 -4.1532812 -4.1486931 -4.1401181 -4.1382294 -4.1557684 -4.1846709][-4.2223878 -4.2401667 -4.2563672 -4.2648506 -4.2694488 -4.2605739 -4.2418075 -4.2297425 -4.2129068 -4.1933093 -4.1817803 -4.1687765 -4.1657591 -4.1822581 -4.2111292][-4.2381272 -4.2474637 -4.2591267 -4.2667489 -4.2765341 -4.27757 -4.2713284 -4.268342 -4.2586393 -4.2419157 -4.228735 -4.2179117 -4.2162337 -4.227138 -4.2479196][-4.2555065 -4.2614913 -4.2715411 -4.2778478 -4.286839 -4.291409 -4.2902374 -4.2910323 -4.287159 -4.2768927 -4.2641163 -4.2549644 -4.2551661 -4.263463 -4.2781639]]...]
INFO - root - 2017-12-06 09:52:16.787133: step 5410, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:22m:11s remains)
INFO - root - 2017-12-06 09:52:18.962474: step 5420, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:55s remains)
INFO - root - 2017-12-06 09:52:21.148268: step 5430, loss = 2.07, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:42s remains)
INFO - root - 2017-12-06 09:52:23.298262: step 5440, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:18s remains)
INFO - root - 2017-12-06 09:52:25.435633: step 5450, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:48s remains)
INFO - root - 2017-12-06 09:52:27.588852: step 5460, loss = 2.10, batch loss = 2.04 (36.3 examples/sec; 0.221 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-06 09:52:29.740645: step 5470, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:11m:17s remains)
INFO - root - 2017-12-06 09:52:31.902846: step 5480, loss = 2.10, batch loss = 2.04 (37.1 examples/sec; 0.216 sec/batch; 19h:35m:31s remains)
INFO - root - 2017-12-06 09:52:34.052516: step 5490, loss = 2.09, batch loss = 2.04 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:21s remains)
INFO - root - 2017-12-06 09:52:36.218837: step 5500, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.214 sec/batch; 19h:27m:28s remains)
2017-12-06 09:52:36.614054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219939 -4.2233057 -4.2091503 -4.2041745 -4.207016 -4.21343 -4.2294788 -4.2424006 -4.2508516 -4.2539988 -4.2447805 -4.2334871 -4.2357736 -4.2336779 -4.2461915][-4.2111382 -4.2163119 -4.2035222 -4.1998596 -4.2058229 -4.2123218 -4.2242241 -4.2302837 -4.2356019 -4.237421 -4.2265983 -4.2160068 -4.2242818 -4.2279477 -4.2450171][-4.1852059 -4.1924777 -4.1895037 -4.1896439 -4.2036877 -4.21709 -4.2296185 -4.23567 -4.2407188 -4.239543 -4.221242 -4.2053642 -4.2201471 -4.2312889 -4.2507887][-4.1678467 -4.1678987 -4.1693463 -4.173665 -4.1897764 -4.2076416 -4.2222772 -4.2316866 -4.23827 -4.2355189 -4.2100477 -4.1873474 -4.2060752 -4.226634 -4.251183][-4.1620445 -4.1556029 -4.1553173 -4.1610231 -4.1682372 -4.1819735 -4.1961884 -4.2130675 -4.2228589 -4.2217317 -4.1910744 -4.1569252 -4.1644263 -4.1909814 -4.2271533][-4.146934 -4.1371326 -4.1376657 -4.1428771 -4.1344595 -4.1280875 -4.1315842 -4.1538286 -4.1711669 -4.1777077 -4.1567655 -4.1164637 -4.1119828 -4.1361256 -4.1805711][-4.122416 -4.0986109 -4.0928235 -4.090589 -4.0539441 -4.0039515 -3.9867787 -4.0385418 -4.0906882 -4.1220059 -4.1186795 -4.0814915 -4.0720654 -4.0982094 -4.1459365][-4.0902176 -4.0478005 -4.0262623 -4.0068874 -3.9368865 -3.8205037 -3.7559972 -3.8556094 -3.9711177 -4.047956 -4.0694289 -4.0472903 -4.0454359 -4.0821457 -4.1365714][-4.0720844 -4.0283971 -4.0064454 -3.9911351 -3.9179263 -3.770844 -3.6472123 -3.7452488 -3.8970563 -3.9975588 -4.0350442 -4.0321665 -4.0513787 -4.1001477 -4.1540031][-4.0828166 -4.0649328 -4.0674167 -4.0798211 -4.0480886 -3.9617329 -3.8688741 -3.8983383 -3.9972177 -4.0679812 -4.0977736 -4.0983934 -4.1197233 -4.1594806 -4.1963754][-4.1051726 -4.1073222 -4.1317439 -4.1585875 -4.1588387 -4.1251268 -4.076941 -4.0713367 -4.1181235 -4.1547976 -4.1658988 -4.1572676 -4.1705065 -4.1984248 -4.2237821][-4.1437526 -4.15728 -4.1835332 -4.2047358 -4.2156796 -4.2065377 -4.184453 -4.1710448 -4.1812105 -4.1905823 -4.1794162 -4.1606221 -4.1696358 -4.1991224 -4.22628][-4.1910982 -4.2128735 -4.2334232 -4.2470512 -4.2569427 -4.2548347 -4.2422328 -4.2254457 -4.211647 -4.1962552 -4.1593528 -4.1247082 -4.1307135 -4.1689157 -4.2086821][-4.220294 -4.2444816 -4.2554331 -4.2608471 -4.2656221 -4.2658386 -4.2569089 -4.2394433 -4.2172446 -4.1901455 -4.1422796 -4.104116 -4.1158447 -4.1607666 -4.199513][-4.2178788 -4.2349272 -4.2418942 -4.2441134 -4.2480659 -4.2521372 -4.2488856 -4.2377214 -4.2218046 -4.2013712 -4.172215 -4.1542211 -4.1743636 -4.2117052 -4.231607]]...]
INFO - root - 2017-12-06 09:52:38.834765: step 5510, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:00m:36s remains)
INFO - root - 2017-12-06 09:52:40.985938: step 5520, loss = 2.10, batch loss = 2.04 (38.0 examples/sec; 0.210 sec/batch; 19h:06m:18s remains)
INFO - root - 2017-12-06 09:52:43.179441: step 5530, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:21s remains)
INFO - root - 2017-12-06 09:52:45.330352: step 5540, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-06 09:52:47.551030: step 5550, loss = 2.09, batch loss = 2.04 (36.1 examples/sec; 0.222 sec/batch; 20h:08m:59s remains)
INFO - root - 2017-12-06 09:52:49.684465: step 5560, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:04s remains)
INFO - root - 2017-12-06 09:52:51.829750: step 5570, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 18h:53m:44s remains)
INFO - root - 2017-12-06 09:52:53.985472: step 5580, loss = 2.10, batch loss = 2.04 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:30s remains)
INFO - root - 2017-12-06 09:52:56.151844: step 5590, loss = 2.07, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:13s remains)
INFO - root - 2017-12-06 09:52:58.294494: step 5600, loss = 2.07, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:46m:23s remains)
2017-12-06 09:52:58.671185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2642765 -4.258523 -4.251009 -4.2386751 -4.225184 -4.2228346 -4.2286024 -4.2370148 -4.247077 -4.2608395 -4.2760882 -4.2850533 -4.2852659 -4.284368 -4.2866411][-4.2416477 -4.2345366 -4.2297521 -4.2164044 -4.1984816 -4.1914153 -4.1886706 -4.1869507 -4.1945558 -4.2168221 -4.2449155 -4.2634878 -4.2687049 -4.2668476 -4.2663155][-4.227365 -4.2197657 -4.2190466 -4.2051878 -4.1809044 -4.1613693 -4.1404347 -4.1263037 -4.1356673 -4.1701684 -4.212841 -4.2438393 -4.2561755 -4.2538648 -4.2487316][-4.218132 -4.2102733 -4.2160821 -4.2056236 -4.173768 -4.1342335 -4.0897255 -4.0601869 -4.0680785 -4.1145725 -4.1731439 -4.2173347 -4.2395883 -4.2422643 -4.2352481][-4.2118359 -4.2042317 -4.21745 -4.2122273 -4.1746469 -4.1162229 -4.0477934 -3.9986734 -3.9998765 -4.0552826 -4.1308346 -4.1899648 -4.2240319 -4.2335267 -4.226923][-4.1954393 -4.1899319 -4.2086787 -4.2110887 -4.1742826 -4.1093173 -4.0261602 -3.9531107 -3.9342949 -3.9919035 -4.0871887 -4.1624103 -4.206017 -4.2210903 -4.2196259][-4.1711488 -4.1699605 -4.1950574 -4.2062612 -4.1800575 -4.1218333 -4.0290828 -3.9168653 -3.8587027 -3.9136715 -4.0294347 -4.1183767 -4.1696529 -4.1920547 -4.2005491][-4.1513863 -4.15133 -4.1794233 -4.1991215 -4.1846466 -4.1358795 -4.0412993 -3.9012299 -3.8152473 -3.8769934 -4.0041013 -4.0942345 -4.1424408 -4.1653523 -4.1775055][-4.1584558 -4.1555448 -4.1856661 -4.2132649 -4.2104282 -4.1759481 -4.0981989 -3.9740686 -3.8997097 -3.9530516 -4.0544848 -4.114131 -4.1401739 -4.150949 -4.1584363][-4.1912174 -4.1828027 -4.2067719 -4.2350407 -4.2373924 -4.216249 -4.1623774 -4.0704503 -4.0121779 -4.0389223 -4.1027746 -4.1344581 -4.1416612 -4.1432815 -4.149313][-4.2407651 -4.2272854 -4.2414107 -4.2629433 -4.2682528 -4.2585983 -4.2287054 -4.166079 -4.1168561 -4.1192594 -4.1529717 -4.1722708 -4.1743317 -4.1729655 -4.1786795][-4.2940478 -4.2811685 -4.286541 -4.2990351 -4.3026967 -4.2999082 -4.2842383 -4.2420359 -4.2026949 -4.1958346 -4.2147818 -4.2293897 -4.2315822 -4.2307396 -4.2347832][-4.326458 -4.3189249 -4.3226051 -4.3285518 -4.3292689 -4.3284039 -4.3188405 -4.289784 -4.2603 -4.2536759 -4.266263 -4.27876 -4.2835922 -4.2847037 -4.2857723][-4.335258 -4.3342862 -4.3391294 -4.3429933 -4.343472 -4.3437676 -4.3373218 -4.31892 -4.298048 -4.2919273 -4.2966242 -4.3065295 -4.315443 -4.3198595 -4.3189626][-4.3298774 -4.33074 -4.3351703 -4.337873 -4.3376007 -4.3373842 -4.3342452 -4.3264437 -4.3150492 -4.3091483 -4.310039 -4.3185949 -4.3284936 -4.334105 -4.3320718]]...]
INFO - root - 2017-12-06 09:53:00.858943: step 5610, loss = 2.10, batch loss = 2.04 (35.8 examples/sec; 0.223 sec/batch; 20h:16m:17s remains)
INFO - root - 2017-12-06 09:53:03.029703: step 5620, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-06 09:53:05.215986: step 5630, loss = 2.11, batch loss = 2.05 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:13s remains)
INFO - root - 2017-12-06 09:53:07.412030: step 5640, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:31m:08s remains)
INFO - root - 2017-12-06 09:53:09.619755: step 5650, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:36m:56s remains)
INFO - root - 2017-12-06 09:53:11.815826: step 5660, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.215 sec/batch; 19h:29m:07s remains)
INFO - root - 2017-12-06 09:53:13.987949: step 5670, loss = 2.10, batch loss = 2.04 (38.8 examples/sec; 0.206 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-06 09:53:16.161635: step 5680, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-06 09:53:18.329142: step 5690, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:43s remains)
INFO - root - 2017-12-06 09:53:20.498104: step 5700, loss = 2.06, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:32m:14s remains)
2017-12-06 09:53:20.835977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281311 -4.2654395 -4.2360969 -4.2124252 -4.1878471 -4.1818123 -4.1903696 -4.1845374 -4.1792336 -4.175241 -4.1639233 -4.1453657 -4.1319532 -4.1129193 -4.0969529][-4.2798615 -4.2606893 -4.2226415 -4.1864152 -4.1476307 -4.1286893 -4.1287622 -4.1173139 -4.1067419 -4.0980444 -4.0895524 -4.0799532 -4.0691352 -4.0421453 -4.019763][-4.281147 -4.2597017 -4.214994 -4.1676388 -4.1169777 -4.0868425 -4.0793324 -4.0649843 -4.0497456 -4.0344987 -4.0262661 -4.0248113 -4.015902 -3.9884622 -3.9702568][-4.2827997 -4.261591 -4.2154927 -4.1641254 -4.111608 -4.0785623 -4.0681677 -4.0546556 -4.0352821 -4.0187159 -4.0113573 -4.0095491 -3.9971781 -3.9743793 -3.9622927][-4.2841458 -4.2666507 -4.2251811 -4.1801848 -4.137455 -4.1100059 -4.1029444 -4.095366 -4.0793076 -4.0671854 -4.0601192 -4.0512252 -4.0304761 -4.0073447 -3.9948106][-4.2831831 -4.2719984 -4.2409062 -4.2096906 -4.18353 -4.1663122 -4.1642237 -4.1633396 -4.1547995 -4.1473026 -4.1376457 -4.1215754 -4.0964937 -4.0720792 -4.058044][-4.2795186 -4.2735033 -4.2523522 -4.230566 -4.2133818 -4.2017546 -4.20428 -4.2111869 -4.2113061 -4.207777 -4.1968026 -4.1801772 -4.1595912 -4.1391721 -4.1273456][-4.275269 -4.2711058 -4.2531829 -4.2325726 -4.2152848 -4.2037053 -4.2089772 -4.2214284 -4.225904 -4.2225127 -4.2129831 -4.2028522 -4.1943274 -4.1851096 -4.1811528][-4.2711864 -4.2649269 -4.2424741 -4.2172742 -4.1962457 -4.1837125 -4.1910439 -4.2046356 -4.2080665 -4.2028027 -4.1965785 -4.195642 -4.1997943 -4.203999 -4.2107382][-4.2677555 -4.2571006 -4.2275753 -4.1986246 -4.1755972 -4.1629195 -4.1704903 -4.17974 -4.1775708 -4.1715488 -4.1689067 -4.1755028 -4.1894326 -4.2044597 -4.2185373][-4.26517 -4.251821 -4.2184224 -4.1887107 -4.1640425 -4.14916 -4.1546621 -4.1601162 -4.1548553 -4.1517811 -4.1533222 -4.1636844 -4.1823196 -4.2028813 -4.2194214][-4.2635789 -4.2508245 -4.2185636 -4.1898556 -4.1639528 -4.1463079 -4.1500578 -4.1562805 -4.1535544 -4.1546054 -4.1582346 -4.1685829 -4.1882882 -4.2094526 -4.2241621][-4.2648377 -4.25512 -4.2266107 -4.2001352 -4.1758523 -4.1582093 -4.1611385 -4.1704369 -4.1718216 -4.1745811 -4.1775036 -4.1864996 -4.2041831 -4.2208643 -4.23082][-4.2737904 -4.2683258 -4.2439876 -4.21941 -4.1982284 -4.182035 -4.1839972 -4.1956859 -4.200295 -4.2034698 -4.205194 -4.2110538 -4.2227368 -4.2328992 -4.2375097][-4.2853985 -4.2851658 -4.2655969 -4.2431793 -4.2252893 -4.2109823 -4.2115579 -4.2236381 -4.2293811 -4.2322111 -4.2322 -4.2342029 -4.2392135 -4.2436185 -4.244319]]...]
INFO - root - 2017-12-06 09:53:22.980349: step 5710, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:34m:46s remains)
INFO - root - 2017-12-06 09:53:25.159360: step 5720, loss = 2.06, batch loss = 2.00 (38.5 examples/sec; 0.208 sec/batch; 18h:53m:05s remains)
INFO - root - 2017-12-06 09:53:27.303538: step 5730, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:25m:41s remains)
INFO - root - 2017-12-06 09:53:29.483181: step 5740, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:31s remains)
INFO - root - 2017-12-06 09:53:31.655004: step 5750, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:08s remains)
INFO - root - 2017-12-06 09:53:33.828532: step 5760, loss = 2.11, batch loss = 2.05 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-06 09:53:35.960453: step 5770, loss = 2.08, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:22m:27s remains)
INFO - root - 2017-12-06 09:53:38.121402: step 5780, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 19h:42m:41s remains)
INFO - root - 2017-12-06 09:53:40.271409: step 5790, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-06 09:53:42.460984: step 5800, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:06m:34s remains)
2017-12-06 09:53:42.836886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297884 -4.291883 -4.272017 -4.2651792 -4.2738986 -4.2858438 -4.2951317 -4.2935677 -4.2813749 -4.2529373 -4.227313 -4.2209387 -4.2256794 -4.2279539 -4.2150769][-4.2979922 -4.2876835 -4.2706194 -4.269537 -4.2795196 -4.2914672 -4.3023772 -4.3022361 -4.2895265 -4.2592473 -4.2304263 -4.2209826 -4.2231011 -4.2282152 -4.2217426][-4.2777009 -4.2608414 -4.2489595 -4.2518063 -4.2628098 -4.2750177 -4.2875485 -4.2936058 -4.28454 -4.2518816 -4.2139192 -4.1909342 -4.1886306 -4.2034192 -4.2114615][-4.241591 -4.2198415 -4.2149367 -4.2216539 -4.2315035 -4.2391315 -4.2469945 -4.2585688 -4.2617311 -4.23411 -4.1932526 -4.1636186 -4.161303 -4.1829572 -4.2022376][-4.2088213 -4.1821346 -4.1813006 -4.1928406 -4.1997046 -4.19207 -4.1809559 -4.19192 -4.2126646 -4.2075963 -4.1855106 -4.15987 -4.1566944 -4.1772823 -4.1951647][-4.1888952 -4.15976 -4.159514 -4.1696329 -4.1613164 -4.1186314 -4.0671234 -4.0723186 -4.1237841 -4.1563692 -4.1671052 -4.1606517 -4.1561217 -4.1739745 -4.186307][-4.1779275 -4.1420393 -4.1353831 -4.1317005 -4.0986185 -4.0116372 -3.902231 -3.9066417 -4.0120645 -4.0901504 -4.1319604 -4.1413941 -4.1416163 -4.16206 -4.1715851][-4.1728272 -4.133307 -4.1197076 -4.1029024 -4.0518451 -3.9390635 -3.7886155 -3.7885244 -3.9348209 -4.0382547 -4.0955606 -4.1142454 -4.1198587 -4.1424 -4.1541395][-4.1708913 -4.136939 -4.1276078 -4.1151285 -4.0785432 -3.9973197 -3.882534 -3.874212 -3.9813321 -4.0579023 -4.097518 -4.1101513 -4.1183548 -4.1429176 -4.157155][-4.1667719 -4.1403446 -4.1419067 -4.1452684 -4.136632 -4.1027379 -4.0402737 -4.0285387 -4.0833611 -4.1167569 -4.1284394 -4.1320038 -4.1454439 -4.170042 -4.1813459][-4.1706085 -4.1511602 -4.161715 -4.1748266 -4.1793928 -4.1722512 -4.1429949 -4.1359444 -4.1595116 -4.1650825 -4.1620321 -4.165009 -4.1833858 -4.2091675 -4.2165523][-4.1890635 -4.1799283 -4.1913257 -4.2024407 -4.2072978 -4.2066703 -4.19141 -4.1895523 -4.2036791 -4.1992903 -4.1959467 -4.2032094 -4.2235231 -4.2471304 -4.2523003][-4.2235575 -4.2192335 -4.2252426 -4.2277927 -4.2275681 -4.2256026 -4.2171888 -4.2188506 -4.2298126 -4.2266908 -4.22762 -4.2402086 -4.260179 -4.2796125 -4.2840729][-4.2625813 -4.2568221 -4.2566605 -4.2538872 -4.2509131 -4.2475457 -4.2440419 -4.2493415 -4.2606459 -4.2623472 -4.2662687 -4.2798691 -4.29485 -4.3092189 -4.3147759][-4.2932868 -4.2859159 -4.2838726 -4.2813907 -4.2794886 -4.27895 -4.2806015 -4.2871151 -4.2988062 -4.3023987 -4.3069029 -4.3167391 -4.3236895 -4.3308792 -4.3364491]]...]
INFO - root - 2017-12-06 09:53:45.034666: step 5810, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:28m:09s remains)
INFO - root - 2017-12-06 09:53:47.223203: step 5820, loss = 2.10, batch loss = 2.04 (37.8 examples/sec; 0.212 sec/batch; 19h:13m:42s remains)
INFO - root - 2017-12-06 09:53:49.396879: step 5830, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:59s remains)
INFO - root - 2017-12-06 09:53:51.548120: step 5840, loss = 2.10, batch loss = 2.05 (36.3 examples/sec; 0.221 sec/batch; 20h:00m:51s remains)
INFO - root - 2017-12-06 09:53:53.706193: step 5850, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:53m:15s remains)
INFO - root - 2017-12-06 09:53:55.801723: step 5860, loss = 2.08, batch loss = 2.02 (39.0 examples/sec; 0.205 sec/batch; 18h:36m:57s remains)
INFO - root - 2017-12-06 09:53:57.966045: step 5870, loss = 2.07, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:15s remains)
INFO - root - 2017-12-06 09:54:00.121262: step 5880, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-06 09:54:02.266461: step 5890, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.218 sec/batch; 19h:48m:53s remains)
INFO - root - 2017-12-06 09:54:04.415875: step 5900, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.215 sec/batch; 19h:29m:00s remains)
2017-12-06 09:54:04.833893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2733231 -4.2856827 -4.305378 -4.315742 -4.3169751 -4.3105626 -4.2987552 -4.2885814 -4.2908597 -4.2978563 -4.3058405 -4.307075 -4.300663 -4.2869129 -4.2730975][-4.2302041 -4.2450275 -4.2730465 -4.2894893 -4.2928824 -4.286839 -4.2710147 -4.2563496 -4.2585926 -4.2697639 -4.2833071 -4.2851892 -4.275372 -4.25331 -4.228313][-4.1994033 -4.212069 -4.2410517 -4.2620621 -4.2681079 -4.262054 -4.2402239 -4.2186041 -4.2170815 -4.2314353 -4.25077 -4.2568326 -4.2506838 -4.2264333 -4.1929374][-4.1785021 -4.1874 -4.2117653 -4.2354236 -4.24385 -4.2371087 -4.2076306 -4.1777229 -4.1740704 -4.1926746 -4.2194939 -4.2294722 -4.2321119 -4.2092195 -4.1696343][-4.1664829 -4.17543 -4.1967754 -4.213263 -4.2143927 -4.199368 -4.1622372 -4.124495 -4.12262 -4.1501784 -4.1863809 -4.2016191 -4.2134695 -4.1997585 -4.1592][-4.157485 -4.1687531 -4.1831012 -4.1865096 -4.1733766 -4.1444583 -4.097043 -4.0536275 -4.0610828 -4.1035352 -4.1552763 -4.1824551 -4.2067146 -4.2028604 -4.164135][-4.1598778 -4.1745977 -4.1813316 -4.168745 -4.1355948 -4.0833297 -4.0120492 -3.965085 -3.9867535 -4.0454497 -4.1185279 -4.1673455 -4.2024803 -4.2068748 -4.1738472][-4.174747 -4.1882143 -4.1916957 -4.1694236 -4.1198792 -4.0468464 -3.9578857 -3.9101303 -3.9368651 -4.0005126 -4.0863886 -4.1556382 -4.2006006 -4.2118683 -4.1880021][-4.1887655 -4.2014041 -4.2093272 -4.1946006 -4.1510005 -4.0816231 -4.0041952 -3.9663765 -3.9869244 -4.0299039 -4.0970459 -4.1708245 -4.2198114 -4.226953 -4.20255][-4.1890941 -4.2017479 -4.2163839 -4.2183609 -4.19539 -4.1485319 -4.0991554 -4.0718975 -4.0779958 -4.093049 -4.1315513 -4.1933918 -4.235652 -4.2359571 -4.2036762][-4.1894879 -4.1991577 -4.2185483 -4.2343259 -4.2302279 -4.2066069 -4.1793184 -4.1582789 -4.1505313 -4.1497469 -4.1703477 -4.217412 -4.2508082 -4.2471571 -4.2073722][-4.20064 -4.2074528 -4.2268014 -4.2502947 -4.2610774 -4.2581964 -4.2501783 -4.2328882 -4.2171803 -4.209619 -4.2183604 -4.248323 -4.2693429 -4.264286 -4.2244244][-4.2316923 -4.2363219 -4.2511377 -4.2707467 -4.2841759 -4.2933741 -4.2959323 -4.2853241 -4.271183 -4.2661085 -4.2709208 -4.2875462 -4.2986312 -4.2942638 -4.2593551][-4.2712059 -4.2783914 -4.2926178 -4.3039775 -4.3090405 -4.3162479 -4.3189535 -4.3144031 -4.3067813 -4.3094563 -4.3160954 -4.3248835 -4.3282013 -4.3190222 -4.2904353][-4.3059521 -4.3157816 -4.3302016 -4.3368807 -4.3346338 -4.3341928 -4.33187 -4.327776 -4.3266859 -4.3338189 -4.3395872 -4.3439436 -4.3416066 -4.3329115 -4.3134589]]...]
INFO - root - 2017-12-06 09:54:07.010919: step 5910, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:06m:15s remains)
INFO - root - 2017-12-06 09:54:09.224307: step 5920, loss = 2.08, batch loss = 2.03 (36.6 examples/sec; 0.218 sec/batch; 19h:48m:18s remains)
INFO - root - 2017-12-06 09:54:11.419948: step 5930, loss = 2.10, batch loss = 2.04 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:18s remains)
INFO - root - 2017-12-06 09:54:13.569072: step 5940, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:54s remains)
INFO - root - 2017-12-06 09:54:15.729040: step 5950, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-06 09:54:17.879665: step 5960, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:22s remains)
INFO - root - 2017-12-06 09:54:20.039199: step 5970, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:06m:58s remains)
INFO - root - 2017-12-06 09:54:22.199511: step 5980, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:15m:22s remains)
INFO - root - 2017-12-06 09:54:24.393889: step 5990, loss = 2.08, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:19s remains)
INFO - root - 2017-12-06 09:54:26.547210: step 6000, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.218 sec/batch; 19h:43m:41s remains)
2017-12-06 09:54:26.886957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1658196 -4.1736846 -4.2014437 -4.2351303 -4.2552867 -4.2590013 -4.2672763 -4.2829571 -4.2936721 -4.2967391 -4.2919912 -4.2684484 -4.2378769 -4.2071295 -4.1866932][-4.1262741 -4.1342692 -4.165513 -4.2073216 -4.2331786 -4.241415 -4.2520313 -4.2702551 -4.2844105 -4.2875824 -4.2809405 -4.254602 -4.2205257 -4.1899481 -4.1785431][-4.1081328 -4.1149707 -4.1461844 -4.1867747 -4.2089639 -4.2151132 -4.2257748 -4.248313 -4.27011 -4.2794418 -4.2719817 -4.2407689 -4.2018337 -4.1746163 -4.1723938][-4.1170692 -4.1220694 -4.146503 -4.1791053 -4.1927552 -4.1902008 -4.2023959 -4.2314143 -4.2622266 -4.2753606 -4.267715 -4.2344952 -4.1912684 -4.1657052 -4.171495][-4.153358 -4.1542192 -4.1653342 -4.1782351 -4.1748285 -4.1615057 -4.1716752 -4.2004752 -4.231039 -4.2447834 -4.2446747 -4.2229142 -4.1880131 -4.1651869 -4.1725068][-4.2028227 -4.2002935 -4.1977625 -4.1931338 -4.1701236 -4.1385169 -4.1351171 -4.1553926 -4.1765294 -4.1849089 -4.1984377 -4.2048535 -4.1938558 -4.179975 -4.1904187][-4.225338 -4.22395 -4.220633 -4.216444 -4.1875782 -4.1408339 -4.1189609 -4.1215181 -4.1202765 -4.1109452 -4.13473 -4.177392 -4.2005954 -4.2119703 -4.2337847][-4.2013235 -4.2126403 -4.2220335 -4.2347946 -4.217916 -4.16455 -4.1243887 -4.1015148 -4.0651717 -4.0258765 -4.0509934 -4.1244745 -4.1842351 -4.2260184 -4.2642155][-4.1535096 -4.1862917 -4.2167511 -4.2527542 -4.2565207 -4.2107525 -4.1601915 -4.108633 -4.0319347 -3.9599509 -3.979239 -4.06675 -4.152071 -4.2182984 -4.2675562][-4.0976825 -4.1566925 -4.2124562 -4.2676945 -4.292295 -4.2664547 -4.2145219 -4.1449065 -4.0437531 -3.9575987 -3.9693384 -4.0559807 -4.1459856 -4.2161736 -4.2616129][-4.045423 -4.1314969 -4.2065363 -4.2729082 -4.3091011 -4.2990184 -4.252111 -4.1821933 -4.0873871 -4.0084181 -4.0117865 -4.0818543 -4.1606641 -4.2219248 -4.25855][-4.0037351 -4.1069341 -4.1837387 -4.2502589 -4.2926497 -4.2956014 -4.261816 -4.2057219 -4.1329451 -4.0747814 -4.074831 -4.1204219 -4.1772475 -4.2234421 -4.2497497][-3.9890022 -4.085228 -4.1478643 -4.207159 -4.2550173 -4.2682428 -4.2499909 -4.2126036 -4.1631856 -4.1273251 -4.1271086 -4.1497211 -4.1832743 -4.2131534 -4.2290721][-4.0132027 -4.0906744 -4.1329956 -4.1809931 -4.2288585 -4.2454138 -4.2389317 -4.21759 -4.1875882 -4.165751 -4.1631632 -4.1698718 -4.1874084 -4.2051783 -4.2151518][-4.0597186 -4.1154928 -4.1422377 -4.1788177 -4.2175684 -4.2290616 -4.2276125 -4.2161937 -4.2015672 -4.1936927 -4.1950326 -4.1971927 -4.2088881 -4.2224116 -4.2304096]]...]
INFO - root - 2017-12-06 09:54:29.106687: step 6010, loss = 2.10, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:04m:04s remains)
INFO - root - 2017-12-06 09:54:31.245246: step 6020, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:24m:10s remains)
INFO - root - 2017-12-06 09:54:33.404240: step 6030, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:15s remains)
INFO - root - 2017-12-06 09:54:35.550869: step 6040, loss = 2.07, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:03s remains)
INFO - root - 2017-12-06 09:54:37.688691: step 6050, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.215 sec/batch; 19h:32m:03s remains)
INFO - root - 2017-12-06 09:54:39.828925: step 6060, loss = 2.09, batch loss = 2.03 (38.1 examples/sec; 0.210 sec/batch; 19h:03m:27s remains)
INFO - root - 2017-12-06 09:54:41.995737: step 6070, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:59s remains)
INFO - root - 2017-12-06 09:54:44.151525: step 6080, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.211 sec/batch; 19h:10m:18s remains)
INFO - root - 2017-12-06 09:54:46.294457: step 6090, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:57s remains)
INFO - root - 2017-12-06 09:54:48.486484: step 6100, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:59s remains)
2017-12-06 09:54:48.821137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055959 -4.305892 -4.3052588 -4.3061271 -4.3083177 -4.3103104 -4.3124437 -4.3146949 -4.3183937 -4.3214788 -4.3211236 -4.3170896 -4.3124223 -4.3116546 -4.3132524][-4.3275719 -4.3274765 -4.3287172 -4.3312092 -4.3341928 -4.3373647 -4.3409886 -4.3439517 -4.3462782 -4.347065 -4.3471231 -4.346808 -4.3452654 -4.3429852 -4.3410587][-4.3409457 -4.3383379 -4.3366103 -4.3351336 -4.3338809 -4.3339782 -4.3357983 -4.3378348 -4.3390656 -4.34058 -4.344893 -4.3512573 -4.3553281 -4.3552604 -4.35403][-4.3318624 -4.32441 -4.3165011 -4.30897 -4.3015409 -4.2963967 -4.2935262 -4.2931895 -4.2964759 -4.3030128 -4.3143382 -4.3284278 -4.3400068 -4.3461967 -4.3491268][-4.2949991 -4.2838273 -4.2704029 -4.2569885 -4.2446909 -4.2355943 -4.2286544 -4.2257967 -4.23178 -4.2441678 -4.2637644 -4.2886715 -4.30839 -4.319458 -4.3258905][-4.2521257 -4.2402339 -4.2220497 -4.2016706 -4.1828766 -4.1679921 -4.1555748 -4.148901 -4.1561022 -4.1714921 -4.1962671 -4.229485 -4.2562633 -4.2728729 -4.286027][-4.21423 -4.2070422 -4.189404 -4.1653357 -4.1395669 -4.1171145 -4.0978422 -4.0879259 -4.0974722 -4.116087 -4.1442003 -4.1798782 -4.204216 -4.2174187 -4.2329316][-4.1945496 -4.1940951 -4.1807375 -4.1583996 -4.1302171 -4.1003661 -4.0717831 -4.0577979 -4.068399 -4.0886106 -4.1172957 -4.149929 -4.1675549 -4.1729879 -4.1835394][-4.2051134 -4.2117624 -4.2058454 -4.1903939 -4.1664162 -4.1369991 -4.108501 -4.096354 -4.1054831 -4.1219063 -4.1438928 -4.1640477 -4.1677318 -4.1623278 -4.1644597][-4.2315054 -4.2446103 -4.24716 -4.241426 -4.2255516 -4.2044697 -4.1850042 -4.1781516 -4.1847372 -4.1965017 -4.2121387 -4.2222724 -4.2165051 -4.2028723 -4.1948528][-4.260272 -4.27424 -4.2820716 -4.2841473 -4.276319 -4.2646742 -4.2543449 -4.249052 -4.2509041 -4.25861 -4.2697449 -4.2757554 -4.2687087 -4.2546291 -4.2434635][-4.2885237 -4.2984514 -4.3070025 -4.3124533 -4.3112111 -4.3071361 -4.3026829 -4.2966409 -4.2924933 -4.2945318 -4.3002334 -4.3040085 -4.3004479 -4.2911777 -4.2823324][-4.311676 -4.3181782 -4.3235497 -4.326376 -4.3242693 -4.3207493 -4.3175268 -4.3102903 -4.3037877 -4.3035011 -4.3061342 -4.3068457 -4.3044038 -4.3001895 -4.296495][-4.3119912 -4.3178544 -4.3211374 -4.3205996 -4.3152032 -4.307508 -4.3001385 -4.2889662 -4.279479 -4.2782645 -4.28109 -4.2821312 -4.2812495 -4.2815104 -4.2851424][-4.2976894 -4.3045206 -4.3083434 -4.3087106 -4.3054156 -4.2979064 -4.2892756 -4.2771082 -4.2663317 -4.2633376 -4.264348 -4.2642636 -4.2632613 -4.2660561 -4.2752905]]...]
INFO - root - 2017-12-06 09:54:50.960412: step 6110, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 19h:59m:44s remains)
INFO - root - 2017-12-06 09:54:53.114381: step 6120, loss = 2.11, batch loss = 2.05 (33.8 examples/sec; 0.237 sec/batch; 21h:28m:18s remains)
INFO - root - 2017-12-06 09:54:55.325676: step 6130, loss = 2.06, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:39m:45s remains)
INFO - root - 2017-12-06 09:54:57.473596: step 6140, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:09m:30s remains)
INFO - root - 2017-12-06 09:54:59.623350: step 6150, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.211 sec/batch; 19h:09m:52s remains)
INFO - root - 2017-12-06 09:55:01.807625: step 6160, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:02m:55s remains)
INFO - root - 2017-12-06 09:55:03.980362: step 6170, loss = 2.09, batch loss = 2.04 (36.9 examples/sec; 0.217 sec/batch; 19h:39m:44s remains)
INFO - root - 2017-12-06 09:55:06.158864: step 6180, loss = 2.10, batch loss = 2.04 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:38s remains)
INFO - root - 2017-12-06 09:55:08.318678: step 6190, loss = 2.10, batch loss = 2.05 (36.5 examples/sec; 0.219 sec/batch; 19h:51m:35s remains)
INFO - root - 2017-12-06 09:55:10.495079: step 6200, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:05s remains)
2017-12-06 09:55:10.897808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1653814 -4.1564312 -4.1604934 -4.1834574 -4.205626 -4.2088041 -4.2021804 -4.1838331 -4.165206 -4.1601853 -4.1721826 -4.1859145 -4.1878233 -4.1790705 -4.1685314][-4.1612606 -4.1611118 -4.1681604 -4.1872025 -4.2028208 -4.1976991 -4.1810274 -4.1606817 -4.1450405 -4.1401105 -4.1477313 -4.1637511 -4.1763139 -4.1835923 -4.1869826][-4.166924 -4.1654782 -4.168129 -4.1851115 -4.1956344 -4.1800361 -4.1561651 -4.14279 -4.1427412 -4.1394362 -4.1312928 -4.1304264 -4.1422696 -4.168644 -4.1883826][-4.1841135 -4.1731057 -4.1675911 -4.1790423 -4.1840444 -4.16138 -4.1302385 -4.128839 -4.1436033 -4.1387496 -4.1083093 -4.0866246 -4.0956693 -4.1381865 -4.1738415][-4.1944051 -4.174201 -4.1633945 -4.1769633 -4.1839204 -4.1613917 -4.1290684 -4.1324768 -4.1468215 -4.1353836 -4.0869827 -4.0502434 -4.0575824 -4.1069412 -4.1527047][-4.1975732 -4.1699514 -4.1542187 -4.1698041 -4.1810551 -4.165915 -4.1442885 -4.1447935 -4.1455917 -4.1253047 -4.0835547 -4.0568027 -4.0593 -4.0953221 -4.139307][-4.1985464 -4.1704855 -4.1535935 -4.14963 -4.1395578 -4.1222525 -4.1119304 -4.11584 -4.1020842 -4.0831904 -4.073278 -4.0837317 -4.094543 -4.1168504 -4.1496673][-4.2076349 -4.1800032 -4.1515293 -4.1202369 -4.0874877 -4.0695939 -4.0670218 -4.0665359 -4.0333366 -4.0039616 -4.0313878 -4.0858526 -4.1196818 -4.1410637 -4.1610565][-4.2061143 -4.1773424 -4.1374187 -4.09158 -4.0511951 -4.0410891 -4.0460172 -4.0380054 -3.99976 -3.9739509 -4.0211959 -4.0969515 -4.1388526 -4.1536956 -4.1551766][-4.1903086 -4.162806 -4.1194787 -4.0720711 -4.0423217 -4.0512543 -4.0682936 -4.065475 -4.0446243 -4.0361676 -4.0812516 -4.139946 -4.1618328 -4.1602707 -4.1477351][-4.1970353 -4.1751022 -4.1368604 -4.1001043 -4.0863471 -4.1029649 -4.1245451 -4.1273389 -4.1184568 -4.1166782 -4.1448565 -4.1788354 -4.1828604 -4.1718292 -4.157207][-4.234817 -4.2233934 -4.1968122 -4.1745563 -4.1698213 -4.1825542 -4.1974869 -4.20179 -4.197742 -4.1968851 -4.209053 -4.2210236 -4.2105842 -4.1967492 -4.185822][-4.2851133 -4.2771239 -4.2559905 -4.2418284 -4.2428355 -4.2522488 -4.2639475 -4.2698956 -4.2701473 -4.2702179 -4.2708511 -4.2658114 -4.2472644 -4.2356324 -4.2324724][-4.3059845 -4.3021078 -4.2880855 -4.2792139 -4.2822986 -4.2907548 -4.3014393 -4.30877 -4.3077035 -4.3031454 -4.2958784 -4.2863431 -4.2718325 -4.2674985 -4.2727065][-4.3043966 -4.3038898 -4.2986341 -4.2943511 -4.2960896 -4.3006954 -4.3035464 -4.3038335 -4.2977257 -4.290647 -4.2838006 -4.2810864 -4.2788615 -4.2834387 -4.2929649]]...]
INFO - root - 2017-12-06 09:55:13.045555: step 6210, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:44s remains)
INFO - root - 2017-12-06 09:55:15.197264: step 6220, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-06 09:55:17.357814: step 6230, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:06m:40s remains)
INFO - root - 2017-12-06 09:55:19.503315: step 6240, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:55s remains)
INFO - root - 2017-12-06 09:55:21.668320: step 6250, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:00s remains)
INFO - root - 2017-12-06 09:55:23.873995: step 6260, loss = 2.10, batch loss = 2.05 (37.1 examples/sec; 0.216 sec/batch; 19h:32m:32s remains)
INFO - root - 2017-12-06 09:55:26.066945: step 6270, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-06 09:55:28.282059: step 6280, loss = 2.08, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:21s remains)
INFO - root - 2017-12-06 09:55:30.440144: step 6290, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:13s remains)
INFO - root - 2017-12-06 09:55:32.569841: step 6300, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:10m:04s remains)
2017-12-06 09:55:32.935863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3097272 -4.2889476 -4.2679362 -4.2559953 -4.2573066 -4.2645087 -4.2676659 -4.2587605 -4.2539206 -4.2511454 -4.2428513 -4.2539673 -4.2722912 -4.2842383 -4.2903109][-4.3163276 -4.302001 -4.28216 -4.2652879 -4.2599978 -4.2650938 -4.270906 -4.2714391 -4.2772775 -4.2835879 -4.282495 -4.2903056 -4.2981672 -4.2956085 -4.2867341][-4.321938 -4.3106775 -4.2913766 -4.2690835 -4.2538671 -4.2524438 -4.2594047 -4.2707796 -4.28892 -4.3046036 -4.3084068 -4.3122625 -4.3082614 -4.2894564 -4.2678394][-4.3228045 -4.309164 -4.2856712 -4.2554269 -4.2287407 -4.2166395 -4.2187567 -4.2338672 -4.2607918 -4.2892866 -4.3036194 -4.3102446 -4.2967453 -4.260973 -4.2269921][-4.3154716 -4.2969484 -4.2642417 -4.2211933 -4.1797085 -4.1497293 -4.1354094 -4.1377745 -4.1635456 -4.2109885 -4.2478089 -4.265646 -4.2496696 -4.2072825 -4.1682863][-4.2929616 -4.2648244 -4.2188187 -4.15901 -4.0976582 -4.0394511 -3.9927738 -3.9581118 -3.9703405 -4.0480776 -4.1186261 -4.15901 -4.1618953 -4.1353674 -4.11219][-4.2627788 -4.225369 -4.1713333 -4.1032763 -4.0295691 -3.9495265 -3.8704944 -3.786128 -3.7666113 -3.8764386 -3.9851165 -4.0570593 -4.0943942 -4.0987697 -4.100019][-4.2386928 -4.2017388 -4.1538758 -4.0964742 -4.0336804 -3.9682922 -3.8998473 -3.8134797 -3.7867036 -3.8909743 -3.9915509 -4.0652819 -4.1222515 -4.1434622 -4.1529641][-4.2209067 -4.1898394 -4.1607413 -4.132493 -4.1008105 -4.0732393 -4.0437961 -3.9937673 -3.9816585 -4.0539131 -4.1132603 -4.1622863 -4.2112589 -4.2271562 -4.2265897][-4.2118258 -4.1894221 -4.1780519 -4.1732178 -4.1680193 -4.1663003 -4.1629386 -4.1388168 -4.1385303 -4.1846395 -4.2133622 -4.242384 -4.2738481 -4.27669 -4.267725][-4.2157793 -4.2062736 -4.2100668 -4.218205 -4.2266541 -4.2335024 -4.239614 -4.2265525 -4.224854 -4.2487011 -4.2624726 -4.282423 -4.3032808 -4.2938337 -4.276762][-4.2282748 -4.2329254 -4.2500272 -4.2655849 -4.2778778 -4.2850027 -4.2943621 -4.2869086 -4.2797523 -4.28646 -4.2907186 -4.2952023 -4.3008752 -4.28377 -4.2633772][-4.2413383 -4.2529974 -4.27601 -4.292943 -4.3031554 -4.310801 -4.3246074 -4.3238034 -4.3134937 -4.3082166 -4.2991152 -4.2853217 -4.2745209 -4.2540874 -4.23678][-4.2550406 -4.2685452 -4.288147 -4.3004165 -4.3056784 -4.3095622 -4.3196554 -4.3206644 -4.3134532 -4.306191 -4.2896204 -4.2640338 -4.2473178 -4.2315207 -4.2222643][-4.266984 -4.2787938 -4.2905164 -4.2979026 -4.2988334 -4.2987294 -4.3041382 -4.305232 -4.3038263 -4.3005195 -4.2871037 -4.2656975 -4.2526932 -4.2435293 -4.2392673]]...]
INFO - root - 2017-12-06 09:55:35.104358: step 6310, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:19s remains)
INFO - root - 2017-12-06 09:55:37.235558: step 6320, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:26m:00s remains)
INFO - root - 2017-12-06 09:55:39.410028: step 6330, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:11m:48s remains)
INFO - root - 2017-12-06 09:55:41.588496: step 6340, loss = 2.11, batch loss = 2.05 (36.4 examples/sec; 0.220 sec/batch; 19h:54m:38s remains)
INFO - root - 2017-12-06 09:55:43.731485: step 6350, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:43s remains)
INFO - root - 2017-12-06 09:55:45.918367: step 6360, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 18h:59m:58s remains)
INFO - root - 2017-12-06 09:55:48.099537: step 6370, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:59m:42s remains)
INFO - root - 2017-12-06 09:55:50.270028: step 6380, loss = 2.07, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:28m:49s remains)
INFO - root - 2017-12-06 09:55:52.409735: step 6390, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:46s remains)
INFO - root - 2017-12-06 09:55:54.626058: step 6400, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:14m:33s remains)
2017-12-06 09:55:55.015867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2303271 -4.2324243 -4.2324553 -4.2228084 -4.2118368 -4.202909 -4.1983213 -4.1938214 -4.20302 -4.2195659 -4.2361245 -4.2486458 -4.2460737 -4.2255168 -4.1991954][-4.2377539 -4.2305241 -4.2231665 -4.2073455 -4.1934805 -4.1859422 -4.1841989 -4.1848307 -4.197329 -4.2176237 -4.2379231 -4.2530594 -4.2565289 -4.2408962 -4.2178221][-4.2320437 -4.2219253 -4.2142782 -4.1948137 -4.1763163 -4.1659503 -4.164351 -4.1683345 -4.1836619 -4.2064505 -4.2270122 -4.242044 -4.2499332 -4.2391977 -4.2188945][-4.2269659 -4.2177668 -4.2127304 -4.1904368 -4.1636925 -4.1417027 -4.1369605 -4.148242 -4.1727281 -4.199645 -4.2168784 -4.2261972 -4.2307081 -4.223628 -4.2078519][-4.2256894 -4.2191949 -4.2146373 -4.1878533 -4.1506925 -4.1153522 -4.1040754 -4.123219 -4.1597342 -4.19364 -4.2044387 -4.20354 -4.1983638 -4.1932535 -4.1836562][-4.2170634 -4.2165604 -4.2131314 -4.1852837 -4.1427836 -4.099967 -4.0810103 -4.1049085 -4.1490774 -4.1823525 -4.1824589 -4.1751394 -4.1644492 -4.1628194 -4.1638575][-4.2001176 -4.2006559 -4.1995754 -4.1783562 -4.1362529 -4.0919704 -4.066442 -4.0903611 -4.1369524 -4.1685772 -4.1661406 -4.1633792 -4.1581984 -4.1661696 -4.1798973][-4.1830907 -4.1777139 -4.1775818 -4.1655197 -4.1291137 -4.0881891 -4.0597272 -4.0802364 -4.1246328 -4.1584945 -4.1595364 -4.1696081 -4.1812363 -4.2022862 -4.2202125][-4.1756897 -4.162993 -4.162118 -4.15907 -4.1323128 -4.0994463 -4.0712285 -4.0832138 -4.1216393 -4.1548252 -4.1613703 -4.1821589 -4.2098188 -4.2407274 -4.2593336][-4.1877818 -4.1693397 -4.1636496 -4.1668625 -4.1549664 -4.1331258 -4.1082053 -4.1100159 -4.1388454 -4.1672969 -4.1809354 -4.207768 -4.2379055 -4.2647 -4.27528][-4.1986322 -4.1774592 -4.1677933 -4.174768 -4.1776271 -4.1702843 -4.1528506 -4.1475668 -4.16376 -4.1866026 -4.2037745 -4.226923 -4.2495418 -4.2641349 -4.2613478][-4.213613 -4.1930647 -4.1801085 -4.1858182 -4.1947446 -4.1962461 -4.1860275 -4.1801271 -4.1910458 -4.2092061 -4.2223167 -4.23238 -4.2384367 -4.2407875 -4.2293997][-4.2295094 -4.2137718 -4.199492 -4.1991153 -4.2063022 -4.2123156 -4.2089691 -4.20823 -4.2190933 -4.2323623 -4.2371254 -4.2279773 -4.2157574 -4.2108645 -4.1988425][-4.2417092 -4.2273216 -4.2116508 -4.2034011 -4.20492 -4.2133694 -4.2170043 -4.2223396 -4.2338471 -4.2436652 -4.2389708 -4.2184863 -4.1964364 -4.1906276 -4.1845889][-4.2552147 -4.2409749 -4.2256112 -4.212297 -4.2068648 -4.2126908 -4.218812 -4.22643 -4.2380786 -4.2453456 -4.2346373 -4.2135553 -4.1981487 -4.2000589 -4.2027555]]...]
INFO - root - 2017-12-06 09:55:57.168620: step 6410, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:00m:12s remains)
INFO - root - 2017-12-06 09:55:59.358340: step 6420, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:29m:19s remains)
INFO - root - 2017-12-06 09:56:01.537083: step 6430, loss = 2.08, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:32s remains)
INFO - root - 2017-12-06 09:56:03.685612: step 6440, loss = 2.09, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 19h:41m:37s remains)
INFO - root - 2017-12-06 09:56:05.869772: step 6450, loss = 2.06, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:37s remains)
INFO - root - 2017-12-06 09:56:08.022526: step 6460, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:43m:53s remains)
INFO - root - 2017-12-06 09:56:10.173666: step 6470, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 18h:59m:46s remains)
INFO - root - 2017-12-06 09:56:12.380698: step 6480, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:07m:49s remains)
INFO - root - 2017-12-06 09:56:14.613041: step 6490, loss = 2.08, batch loss = 2.02 (31.0 examples/sec; 0.258 sec/batch; 23h:22m:23s remains)
INFO - root - 2017-12-06 09:56:16.754800: step 6500, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:19s remains)
2017-12-06 09:56:17.167054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3373427 -4.3383956 -4.3369808 -4.3287773 -4.3088508 -4.2827187 -4.2600226 -4.2296853 -4.1965437 -4.1785851 -4.2000732 -4.2512379 -4.2933378 -4.3257561 -4.34406][-4.3470726 -4.3494272 -4.3478837 -4.3330879 -4.3061666 -4.2743306 -4.246141 -4.2179165 -4.192945 -4.1846571 -4.2093658 -4.2561145 -4.2991047 -4.3297505 -4.3458104][-4.3494372 -4.3499646 -4.3465848 -4.3249612 -4.2885194 -4.2505608 -4.2189746 -4.1921549 -4.1761065 -4.1837912 -4.2141142 -4.2519917 -4.292625 -4.3245425 -4.3416066][-4.3496938 -4.3482842 -4.3392196 -4.3096943 -4.2640872 -4.2213659 -4.1859164 -4.1561 -4.1405735 -4.1564231 -4.1935577 -4.2291036 -4.2766504 -4.3150449 -4.3366518][-4.3494263 -4.3445687 -4.3269596 -4.289031 -4.2363338 -4.1914344 -4.1516104 -4.1107011 -4.0862627 -4.10659 -4.1521363 -4.1957974 -4.2583466 -4.3042307 -4.3305268][-4.3507152 -4.3426552 -4.3152332 -4.2642374 -4.2013316 -4.1494136 -4.0992889 -4.0492234 -4.0268936 -4.0584192 -4.1162972 -4.17572 -4.2510972 -4.3012962 -4.3291974][-4.3527889 -4.3430595 -4.3076777 -4.23732 -4.1568294 -4.0910506 -4.0244446 -3.9775422 -3.9799287 -4.0295396 -4.10355 -4.17987 -4.2584987 -4.3083239 -4.3325524][-4.3475928 -4.3328567 -4.2868624 -4.1983733 -4.0960402 -4.0034027 -3.9205112 -3.9008398 -3.9438879 -4.0203733 -4.1099496 -4.1946664 -4.2663307 -4.3128424 -4.3298244][-4.3390613 -4.3147345 -4.2549167 -4.147162 -4.0128794 -3.8855104 -3.8070915 -3.8347151 -3.915652 -4.0159492 -4.12319 -4.2122068 -4.2709136 -4.3050137 -4.3123994][-4.3240037 -4.2885695 -4.2165332 -4.0998497 -3.9492664 -3.803906 -3.7436359 -3.8037353 -3.8979268 -4.0069447 -4.1256847 -4.216578 -4.2672844 -4.2908235 -4.2877369][-4.306006 -4.2597642 -4.1796255 -4.0700965 -3.9401939 -3.8200073 -3.7833495 -3.8476572 -3.9356956 -4.0375595 -4.1485395 -4.2303977 -4.2704434 -4.2856507 -4.27654][-4.2877989 -4.2332325 -4.150794 -4.0580349 -3.9661067 -3.8890269 -3.8793356 -3.9364207 -4.0085626 -4.0912228 -4.1817493 -4.2477221 -4.277616 -4.284462 -4.2730923][-4.2749114 -4.2197204 -4.1469064 -4.0759306 -4.0152426 -3.9721313 -3.9812133 -4.0282154 -4.0840244 -4.1449184 -4.2134571 -4.2628646 -4.2844515 -4.2871289 -4.2766185][-4.2721033 -4.22519 -4.1680622 -4.1156054 -4.0746241 -4.0521784 -4.0664916 -4.1054206 -4.1496158 -4.1981549 -4.2473836 -4.2804842 -4.292943 -4.2945175 -4.2884312][-4.2828574 -4.2484307 -4.2053094 -4.1662703 -4.1406264 -4.1279478 -4.1398244 -4.1689577 -4.2067389 -4.2464457 -4.28098 -4.2998633 -4.3046722 -4.3052454 -4.3025718]]...]
INFO - root - 2017-12-06 09:56:19.338467: step 6510, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-06 09:56:21.515022: step 6520, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.210 sec/batch; 18h:59m:11s remains)
INFO - root - 2017-12-06 09:56:23.666970: step 6530, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:35m:32s remains)
INFO - root - 2017-12-06 09:56:25.896378: step 6540, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:05m:33s remains)
INFO - root - 2017-12-06 09:56:28.050809: step 6550, loss = 2.11, batch loss = 2.06 (38.3 examples/sec; 0.209 sec/batch; 18h:55m:17s remains)
INFO - root - 2017-12-06 09:56:30.180898: step 6560, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 19h:58m:05s remains)
INFO - root - 2017-12-06 09:56:32.322956: step 6570, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:34s remains)
INFO - root - 2017-12-06 09:56:34.461625: step 6580, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:53m:56s remains)
INFO - root - 2017-12-06 09:56:36.626621: step 6590, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:20m:41s remains)
INFO - root - 2017-12-06 09:56:38.829540: step 6600, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:53m:11s remains)
2017-12-06 09:56:39.209538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889357 -4.2777715 -4.26022 -4.2411628 -4.2331252 -4.2278628 -4.2097154 -4.1852612 -4.1699972 -4.160975 -4.1494455 -4.1428885 -4.1445494 -4.1575885 -4.1554432][-4.2894454 -4.2743683 -4.2541666 -4.2353692 -4.2266703 -4.2234049 -4.2143674 -4.1975245 -4.1825328 -4.1735058 -4.1579566 -4.1534638 -4.1639757 -4.1780019 -4.1710458][-4.29101 -4.2749467 -4.2567568 -4.2392654 -4.229372 -4.2241454 -4.2151308 -4.2024846 -4.1948757 -4.1874743 -4.1713476 -4.1696591 -4.1857023 -4.1928091 -4.1787806][-4.2921476 -4.2726316 -4.2505426 -4.2301297 -4.22003 -4.2165122 -4.2083344 -4.2010593 -4.1994562 -4.1968288 -4.1865644 -4.1822867 -4.1926308 -4.1974192 -4.1831622][-4.2889681 -4.2645459 -4.2369151 -4.2148938 -4.2051368 -4.2037687 -4.1913195 -4.1786337 -4.1774511 -4.1815391 -4.1818557 -4.1735091 -4.1774654 -4.1849828 -4.1841464][-4.2789168 -4.2467747 -4.2133331 -4.1926961 -4.1863461 -4.1791244 -4.1505833 -4.1294084 -4.1362934 -4.1529636 -4.1673293 -4.16372 -4.1623693 -4.1708846 -4.178617][-4.2645822 -4.2215109 -4.1812186 -4.158349 -4.1475554 -4.1261845 -4.0819864 -4.058115 -4.0793672 -4.116323 -4.1473951 -4.1540785 -4.151967 -4.1597857 -4.1720519][-4.2526565 -4.2039576 -4.1580691 -4.129662 -4.1120558 -4.079565 -4.0267258 -4.0024228 -4.0290089 -4.0744519 -4.1132526 -4.1277065 -4.13063 -4.1400466 -4.1524067][-4.2442746 -4.19711 -4.1563783 -4.1325259 -4.1178341 -4.0891628 -4.0373158 -4.0099149 -4.0320563 -4.0733509 -4.1106057 -4.1282463 -4.1316676 -4.1324496 -4.1365252][-4.2439055 -4.2063613 -4.1791754 -4.1641078 -4.1551871 -4.1357841 -4.0906396 -4.0624847 -4.0780907 -4.112184 -4.1465578 -4.1616435 -4.1565857 -4.1399069 -4.1248384][-4.250576 -4.2228947 -4.2037387 -4.1925311 -4.182847 -4.1678491 -4.1344361 -4.1136804 -4.1264324 -4.1544657 -4.1864429 -4.1990409 -4.1857877 -4.1538396 -4.1241016][-4.2560115 -4.2336049 -4.2181034 -4.2079797 -4.1973691 -4.1848984 -4.1663814 -4.1551714 -4.1605392 -4.1764374 -4.1981435 -4.2067618 -4.1924105 -4.1571703 -4.1251717][-4.2612014 -4.2426186 -4.2286582 -4.2182374 -4.2104983 -4.2026424 -4.1934004 -4.1861262 -4.1840873 -4.1851659 -4.1900978 -4.1883068 -4.1738954 -4.142664 -4.1197472][-4.2677035 -4.2505164 -4.2357888 -4.2233367 -4.21554 -4.2100282 -4.2034469 -4.1975527 -4.1946654 -4.1905293 -4.1857424 -4.1783834 -4.1668506 -4.143786 -4.1286168][-4.2795582 -4.2611909 -4.2433524 -4.230864 -4.2251172 -4.222106 -4.216434 -4.2145705 -4.2133608 -4.2094421 -4.202714 -4.1950221 -4.1858878 -4.1684794 -4.1560812]]...]
INFO - root - 2017-12-06 09:56:41.337066: step 6610, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:23m:38s remains)
INFO - root - 2017-12-06 09:56:43.469206: step 6620, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:22m:07s remains)
INFO - root - 2017-12-06 09:56:45.662937: step 6630, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:10m:53s remains)
INFO - root - 2017-12-06 09:56:47.800127: step 6640, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:03m:04s remains)
INFO - root - 2017-12-06 09:56:49.949974: step 6650, loss = 2.09, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:39s remains)
INFO - root - 2017-12-06 09:56:52.124911: step 6660, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:45m:03s remains)
INFO - root - 2017-12-06 09:56:54.307749: step 6670, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 19h:00m:35s remains)
INFO - root - 2017-12-06 09:56:56.423065: step 6680, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:26m:53s remains)
INFO - root - 2017-12-06 09:56:58.626635: step 6690, loss = 2.10, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:01m:31s remains)
INFO - root - 2017-12-06 09:57:00.786126: step 6700, loss = 2.10, batch loss = 2.04 (37.5 examples/sec; 0.213 sec/batch; 19h:17m:11s remains)
2017-12-06 09:57:01.149381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1933832 -4.2139978 -4.2287092 -4.2196646 -4.1885953 -4.1474652 -4.1020555 -4.0718784 -4.0863934 -4.142323 -4.1914606 -4.2279954 -4.2548819 -4.2673106 -4.2595987][-4.1838927 -4.1996121 -4.21667 -4.222445 -4.2062387 -4.1685877 -4.1163011 -4.0690479 -4.0649853 -4.1105962 -4.1576858 -4.1989093 -4.2302513 -4.2520113 -4.2646294][-4.1844087 -4.2011018 -4.2186871 -4.235168 -4.2347832 -4.2036366 -4.1445265 -4.0864434 -4.0649261 -4.0919471 -4.1264434 -4.1610742 -4.1876054 -4.2109327 -4.2340112][-4.1731968 -4.2022681 -4.2303858 -4.2538748 -4.2569661 -4.2261 -4.1700897 -4.1155868 -4.0820785 -4.0899382 -4.1077375 -4.128407 -4.1481647 -4.170403 -4.198103][-4.1399155 -4.1760416 -4.2198834 -4.2555552 -4.2597547 -4.2318535 -4.181613 -4.1296039 -4.0971727 -4.10131 -4.1097493 -4.1197948 -4.1362109 -4.157197 -4.1840959][-4.1028118 -4.1324282 -4.1856742 -4.2312756 -4.2374806 -4.2070618 -4.1507111 -4.0924721 -4.0692611 -4.0867286 -4.1101866 -4.1310954 -4.1529675 -4.1710272 -4.191381][-4.0773406 -4.0897 -4.1420112 -4.1946893 -4.2065654 -4.1753421 -4.10711 -4.0343933 -4.0109305 -4.0475683 -4.1001325 -4.1435184 -4.1743321 -4.1905985 -4.2064295][-4.0626435 -4.0618482 -4.1049781 -4.1529441 -4.1684008 -4.1396117 -4.0741272 -4.0021229 -3.9769046 -4.0162268 -4.0817857 -4.1396174 -4.1790814 -4.1968417 -4.2097397][-4.0599375 -4.0549793 -4.0868735 -4.1231475 -4.1362953 -4.1146021 -4.0665565 -4.0147729 -3.9967086 -4.0292511 -4.0825067 -4.1345944 -4.1683288 -4.1803088 -4.1911163][-4.0643497 -4.0620317 -4.0878992 -4.1201305 -4.135849 -4.1261883 -4.0948892 -4.0589328 -4.047009 -4.0719442 -4.11101 -4.151145 -4.172616 -4.17067 -4.1734381][-4.0607347 -4.0692458 -4.0980797 -4.1300116 -4.1457644 -4.1412153 -4.1230865 -4.1018357 -4.0990949 -4.1186643 -4.1482444 -4.1783419 -4.1899 -4.1779032 -4.1713543][-4.0653043 -4.0864944 -4.1209359 -4.1501865 -4.1595697 -4.1528106 -4.1443472 -4.1373377 -4.1438117 -4.1630788 -4.1873875 -4.2113795 -4.2191877 -4.2036572 -4.1879258][-4.1119156 -4.1354237 -4.1640215 -4.1849012 -4.1890678 -4.1820374 -4.1791868 -4.1810412 -4.1908722 -4.2080793 -4.2309437 -4.2526178 -4.2596178 -4.2458534 -4.225894][-4.1868014 -4.2003064 -4.2162623 -4.2291694 -4.2339287 -4.2309756 -4.2310019 -4.232779 -4.2406831 -4.2543645 -4.2744088 -4.2940655 -4.300653 -4.2909517 -4.2734151][-4.2487211 -4.2544432 -4.2633619 -4.2739353 -4.2805524 -4.279573 -4.2772093 -4.2774916 -4.2834959 -4.2934217 -4.3088064 -4.323554 -4.32949 -4.3239021 -4.3113828]]...]
INFO - root - 2017-12-06 09:57:03.317088: step 6710, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:17m:39s remains)
INFO - root - 2017-12-06 09:57:05.493689: step 6720, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:43m:53s remains)
INFO - root - 2017-12-06 09:57:07.621628: step 6730, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:25m:46s remains)
INFO - root - 2017-12-06 09:57:09.791319: step 6740, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:09m:47s remains)
INFO - root - 2017-12-06 09:57:11.954059: step 6750, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:23m:50s remains)
INFO - root - 2017-12-06 09:57:14.205662: step 6760, loss = 2.10, batch loss = 2.04 (36.6 examples/sec; 0.219 sec/batch; 19h:46m:28s remains)
INFO - root - 2017-12-06 09:57:16.368628: step 6770, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:09m:59s remains)
INFO - root - 2017-12-06 09:57:18.514492: step 6780, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-06 09:57:20.666565: step 6790, loss = 2.10, batch loss = 2.04 (38.2 examples/sec; 0.209 sec/batch; 18h:56m:02s remains)
INFO - root - 2017-12-06 09:57:22.804077: step 6800, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:30m:40s remains)
2017-12-06 09:57:23.244066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2819781 -4.2797461 -4.2781339 -4.272274 -4.2596745 -4.2530937 -4.2269831 -4.1913619 -4.1908708 -4.2377963 -4.28342 -4.2930484 -4.2852068 -4.2784162 -4.2686629][-4.2843032 -4.2759147 -4.2688303 -4.258523 -4.2478428 -4.2534251 -4.23235 -4.1927123 -4.1877403 -4.2315021 -4.2789741 -4.2862782 -4.2706676 -4.2635827 -4.2560587][-4.2885876 -4.2753921 -4.2633176 -4.24643 -4.2311144 -4.2400537 -4.2244964 -4.1869621 -4.1754022 -4.2137184 -4.2607388 -4.2702928 -4.2546353 -4.2471189 -4.24501][-4.2975326 -4.2832193 -4.2701759 -4.245749 -4.2206745 -4.2224321 -4.2044988 -4.16298 -4.1447968 -4.181808 -4.2343097 -4.25378 -4.2496758 -4.2460251 -4.2490568][-4.304482 -4.2900538 -4.2771463 -4.2481151 -4.2135196 -4.1974564 -4.161593 -4.1043839 -4.0796151 -4.1275749 -4.1966228 -4.2324338 -4.243228 -4.24697 -4.2548032][-4.3067884 -4.2970886 -4.2860746 -4.256804 -4.215672 -4.18102 -4.1212273 -4.0341043 -3.9945259 -4.0594878 -4.1582804 -4.218152 -4.2465019 -4.2542982 -4.2636604][-4.3004365 -4.29627 -4.2911906 -4.2705083 -4.2344284 -4.1909919 -4.1102767 -3.9921579 -3.928714 -4.0026259 -4.1281519 -4.2096162 -4.2485161 -4.2636466 -4.2765121][-4.2933307 -4.2935705 -4.2983141 -4.2888145 -4.2609963 -4.2175527 -4.1331487 -4.0089993 -3.9324875 -3.9976099 -4.1248846 -4.2115912 -4.2583904 -4.2814989 -4.2961011][-4.2819862 -4.2852745 -4.2956743 -4.2946839 -4.278192 -4.2448692 -4.1728945 -4.0700936 -4.0024471 -4.048017 -4.1490545 -4.2221794 -4.2649865 -4.2907767 -4.3031654][-4.2674289 -4.2743788 -4.2882843 -4.2941995 -4.2907376 -4.2684011 -4.211247 -4.1368423 -4.0878639 -4.1159258 -4.1853342 -4.2386417 -4.2696562 -4.2899551 -4.2984862][-4.2613897 -4.2713065 -4.2883883 -4.298779 -4.3017063 -4.2869005 -4.2426639 -4.1872234 -4.1537862 -4.1718655 -4.2193966 -4.25429 -4.2704191 -4.2820382 -4.2867889][-4.2659044 -4.2746058 -4.291646 -4.3003116 -4.2979436 -4.2844296 -4.2490597 -4.2096357 -4.1896892 -4.2064552 -4.2424078 -4.264401 -4.2706103 -4.2723656 -4.2720456][-4.263104 -4.2682338 -4.2841825 -4.2905684 -4.2823243 -4.2656655 -4.2364912 -4.2124028 -4.2042918 -4.2187424 -4.241457 -4.2535305 -4.259654 -4.2583323 -4.2530823][-4.2504697 -4.24896 -4.2618718 -4.2705112 -4.2660193 -4.2505174 -4.2280121 -4.215817 -4.2157536 -4.2213798 -4.2267866 -4.2310219 -4.2413411 -4.2436891 -4.2383118][-4.2343197 -4.2254734 -4.2399497 -4.2517257 -4.2530651 -4.2410464 -4.2249894 -4.2208028 -4.2240391 -4.219111 -4.2084169 -4.2133994 -4.2306924 -4.23818 -4.233263]]...]
INFO - root - 2017-12-06 09:57:25.452696: step 6810, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:04m:43s remains)
INFO - root - 2017-12-06 09:57:27.673550: step 6820, loss = 2.08, batch loss = 2.02 (30.6 examples/sec; 0.262 sec/batch; 23h:40m:33s remains)
INFO - root - 2017-12-06 09:57:29.854207: step 6830, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:39m:56s remains)
INFO - root - 2017-12-06 09:57:32.022934: step 6840, loss = 2.08, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-06 09:57:34.175464: step 6850, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 19h:55m:08s remains)
INFO - root - 2017-12-06 09:57:36.378406: step 6860, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:13m:32s remains)
INFO - root - 2017-12-06 09:57:38.571616: step 6870, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:02m:58s remains)
INFO - root - 2017-12-06 09:57:40.732270: step 6880, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 18h:45m:54s remains)
INFO - root - 2017-12-06 09:57:42.894821: step 6890, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 18h:58m:28s remains)
INFO - root - 2017-12-06 09:57:45.013844: step 6900, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:32m:03s remains)
2017-12-06 09:57:45.323246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462468 -4.2209592 -4.2068596 -4.1986208 -4.1985011 -4.1897211 -4.1782207 -4.16672 -4.1840439 -4.2216115 -4.2438679 -4.2471795 -4.2459569 -4.2401524 -4.2190828][-4.2432103 -4.2145987 -4.1962152 -4.1893635 -4.1938615 -4.1775746 -4.1494889 -4.1292262 -4.15342 -4.2005649 -4.2275991 -4.2305117 -4.2297068 -4.2283797 -4.210175][-4.2451725 -4.2137952 -4.1925654 -4.1920185 -4.2038064 -4.19052 -4.1556549 -4.1281934 -4.1509366 -4.197474 -4.2231631 -4.2312922 -4.2339525 -4.2371149 -4.2196641][-4.2481642 -4.2152762 -4.1974163 -4.2002478 -4.2129755 -4.2077413 -4.1687536 -4.1301322 -4.1468525 -4.1899214 -4.2162437 -4.233644 -4.242826 -4.2492504 -4.2336283][-4.2469559 -4.2184267 -4.2040319 -4.2020826 -4.2067852 -4.2028418 -4.1573167 -4.1017189 -4.1092472 -4.1590042 -4.1963978 -4.2222166 -4.241118 -4.2545247 -4.2412186][-4.2435656 -4.2212706 -4.2050791 -4.1904936 -4.1767845 -4.1579919 -4.0944076 -4.0072818 -4.003993 -4.0894365 -4.1609497 -4.20586 -4.2384086 -4.2597351 -4.2547884][-4.245213 -4.2245789 -4.2049751 -4.1775765 -4.1433954 -4.0946007 -3.9888136 -3.8461835 -3.85291 -4.0031676 -4.1248856 -4.1935024 -4.2456412 -4.27268 -4.2675185][-4.2470713 -4.225018 -4.2026834 -4.173038 -4.1368103 -4.072947 -3.9311249 -3.7559416 -3.7890441 -3.9739382 -4.114399 -4.1932445 -4.2547412 -4.2852855 -4.2771025][-4.2479358 -4.2227178 -4.19774 -4.1746693 -4.150599 -4.1034074 -3.9982979 -3.8884342 -3.9238515 -4.0523252 -4.1527615 -4.2107258 -4.254776 -4.2794724 -4.2738395][-4.2506113 -4.2227931 -4.1946769 -4.173244 -4.1538882 -4.12384 -4.069932 -4.0221448 -4.0506892 -4.129087 -4.1936984 -4.2284136 -4.2497692 -4.2599854 -4.2531147][-4.2597556 -4.2344656 -4.212276 -4.1962323 -4.178216 -4.1502547 -4.120873 -4.0959682 -4.1143408 -4.1669545 -4.2102237 -4.2344761 -4.2462773 -4.2455997 -4.2369914][-4.2677536 -4.2498436 -4.2366467 -4.2324352 -4.2219324 -4.19853 -4.181077 -4.16632 -4.1772852 -4.2100816 -4.2345185 -4.2498155 -4.2557726 -4.2510428 -4.2450929][-4.269486 -4.256587 -4.2491937 -4.252604 -4.2468472 -4.2305789 -4.2227197 -4.215991 -4.22981 -4.2488627 -4.2622223 -4.2715297 -4.2730379 -4.2670345 -4.2620997][-4.2650723 -4.2561522 -4.25381 -4.2571297 -4.2554736 -4.2472992 -4.2440133 -4.245409 -4.2632203 -4.2770839 -4.281764 -4.2891588 -4.2920051 -4.2906923 -4.2870936][-4.2609882 -4.2552419 -4.2562909 -4.25927 -4.2580752 -4.2540417 -4.2507644 -4.2521005 -4.2662368 -4.2781897 -4.2877731 -4.3002305 -4.3090105 -4.3136582 -4.314477]]...]
INFO - root - 2017-12-06 09:57:47.537498: step 6910, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:22m:02s remains)
INFO - root - 2017-12-06 09:57:49.683993: step 6920, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.210 sec/batch; 19h:01m:47s remains)
INFO - root - 2017-12-06 09:57:51.865380: step 6930, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:14m:30s remains)
INFO - root - 2017-12-06 09:57:54.003234: step 6940, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:24m:59s remains)
INFO - root - 2017-12-06 09:57:56.161914: step 6950, loss = 2.10, batch loss = 2.04 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:51s remains)
INFO - root - 2017-12-06 09:57:58.369894: step 6960, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:04m:19s remains)
INFO - root - 2017-12-06 09:58:00.513593: step 6970, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-06 09:58:02.653383: step 6980, loss = 2.08, batch loss = 2.02 (39.1 examples/sec; 0.205 sec/batch; 18h:30m:54s remains)
INFO - root - 2017-12-06 09:58:04.807873: step 6990, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:11m:45s remains)
INFO - root - 2017-12-06 09:58:06.985169: step 7000, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:10m:19s remains)
2017-12-06 09:58:07.315942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1716022 -4.1652384 -4.1484628 -4.1386414 -4.1352024 -4.1547685 -4.1743484 -4.1784115 -4.1624351 -4.1483812 -4.1555357 -4.1681013 -4.1678553 -4.1616135 -4.1719232][-4.1553817 -4.1487355 -4.1348324 -4.1321898 -4.1336884 -4.1486053 -4.1616383 -4.1681523 -4.1549687 -4.1498866 -4.1516585 -4.1564388 -4.1531849 -4.1451459 -4.1583948][-4.1422696 -4.135118 -4.1236205 -4.1210074 -4.11732 -4.1281376 -4.1376004 -4.1461129 -4.1400709 -4.1508551 -4.1455774 -4.1316233 -4.1222458 -4.1179609 -4.1344314][-4.1429138 -4.1386943 -4.1315207 -4.1256247 -4.1136551 -4.114223 -4.1126213 -4.114512 -4.1219935 -4.1468992 -4.1401587 -4.1157026 -4.1024404 -4.1017842 -4.1210842][-4.14803 -4.1523032 -4.1482196 -4.1392875 -4.1250563 -4.1113739 -4.091639 -4.0895457 -4.1141858 -4.1522903 -4.1501341 -4.1288795 -4.1196847 -4.1246285 -4.1375036][-4.1438584 -4.1560359 -4.1583462 -4.1507883 -4.1357436 -4.1067209 -4.057292 -4.0575824 -4.10925 -4.1624055 -4.1693869 -4.1602292 -4.1564226 -4.1608915 -4.1680536][-4.1166663 -4.1354632 -4.15055 -4.1450667 -4.1225 -4.0712156 -3.9919739 -4.0055957 -4.0970387 -4.1671348 -4.1868315 -4.1893144 -4.1847081 -4.1812377 -4.1806054][-4.0683546 -4.0959096 -4.1205988 -4.1161466 -4.09151 -4.0208559 -3.92497 -3.9644804 -4.0887051 -4.1673532 -4.1883016 -4.1943984 -4.1884108 -4.1779356 -4.1810713][-4.0440536 -4.0763545 -4.1033206 -4.1006317 -4.0823522 -4.027638 -3.9653106 -4.017838 -4.1174555 -4.1709542 -4.1782231 -4.179729 -4.1707387 -4.1607885 -4.1731148][-4.0570583 -4.0904274 -4.1135144 -4.1159606 -4.1096859 -4.0813522 -4.0586367 -4.10031 -4.1601577 -4.1840844 -4.176682 -4.1720905 -4.1601787 -4.1495767 -4.1635389][-4.0855565 -4.1209431 -4.1359425 -4.1383896 -4.1386428 -4.1244216 -4.1168056 -4.14207 -4.1753817 -4.184978 -4.1755033 -4.1665072 -4.1475253 -4.1294646 -4.1398864][-4.1317124 -4.164629 -4.169807 -4.1659322 -4.1620922 -4.147892 -4.1372728 -4.1545191 -4.1771126 -4.1811924 -4.1763043 -4.1684494 -4.1451449 -4.1235628 -4.1337557][-4.1569252 -4.1774168 -4.1770358 -4.1769609 -4.175375 -4.1622825 -4.1535468 -4.1716938 -4.1944618 -4.1962252 -4.1922469 -4.1829686 -4.1583061 -4.1416626 -4.1558948][-4.159389 -4.1707335 -4.168438 -4.1739941 -4.177916 -4.1677036 -4.1598053 -4.1784716 -4.1995354 -4.2007842 -4.1939545 -4.1790004 -4.1544957 -4.1427212 -4.1624889][-4.1474991 -4.1540284 -4.1574869 -4.1701117 -4.1785154 -4.167769 -4.1517415 -4.1594005 -4.1731482 -4.1778331 -4.1746316 -4.1560769 -4.1279812 -4.1190786 -4.1430469]]...]
INFO - root - 2017-12-06 09:58:09.489088: step 7010, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 20h:05m:19s remains)
INFO - root - 2017-12-06 09:58:11.654592: step 7020, loss = 2.09, batch loss = 2.03 (38.0 examples/sec; 0.210 sec/batch; 19h:00m:37s remains)
INFO - root - 2017-12-06 09:58:13.820686: step 7030, loss = 2.06, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-06 09:58:15.997754: step 7040, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:03m:21s remains)
INFO - root - 2017-12-06 09:58:18.129268: step 7050, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.221 sec/batch; 20h:00m:49s remains)
INFO - root - 2017-12-06 09:58:20.299263: step 7060, loss = 2.08, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:02m:11s remains)
INFO - root - 2017-12-06 09:58:22.490444: step 7070, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:27m:41s remains)
INFO - root - 2017-12-06 09:58:24.651337: step 7080, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:21m:32s remains)
INFO - root - 2017-12-06 09:58:26.801021: step 7090, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:32m:21s remains)
INFO - root - 2017-12-06 09:58:28.978913: step 7100, loss = 2.11, batch loss = 2.05 (37.1 examples/sec; 0.216 sec/batch; 19h:29m:22s remains)
2017-12-06 09:58:29.337472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2340178 -4.2288265 -4.2155771 -4.1856618 -4.1550751 -4.1412992 -4.1398144 -4.1460466 -4.14155 -4.1389356 -4.155858 -4.1868038 -4.2080088 -4.2023687 -4.1893][-4.2178531 -4.2197328 -4.2117553 -4.1840935 -4.1493506 -4.1272383 -4.1288738 -4.1447949 -4.145484 -4.1404524 -4.1520462 -4.1809592 -4.2018604 -4.1951394 -4.1801682][-4.1978936 -4.2059412 -4.2068944 -4.1857166 -4.1535306 -4.1316886 -4.1407185 -4.1673317 -4.1771951 -4.1738563 -4.179348 -4.1964903 -4.208025 -4.1966758 -4.1816688][-4.1740417 -4.18698 -4.1973867 -4.1836238 -4.1557808 -4.1371059 -4.1538887 -4.1911559 -4.2081614 -4.2094564 -4.2144804 -4.2226686 -4.2229338 -4.2052913 -4.1907253][-4.158783 -4.173193 -4.1895781 -4.1834641 -4.1615124 -4.1419339 -4.1545086 -4.1875834 -4.2048717 -4.213665 -4.2286239 -4.2381682 -4.2334619 -4.2096033 -4.1936812][-4.1636248 -4.1764774 -4.1931715 -4.1908069 -4.1745753 -4.149405 -4.1413131 -4.14681 -4.1458654 -4.1634502 -4.2034907 -4.2333059 -4.2325263 -4.2090988 -4.1919518][-4.1850452 -4.1931472 -4.2067013 -4.2051883 -4.1865492 -4.1503525 -4.1132307 -4.0734906 -4.0385861 -4.0641975 -4.1402645 -4.2034206 -4.2171259 -4.1987543 -4.1852469][-4.2174268 -4.2246447 -4.230978 -4.2275376 -4.2059 -4.1611991 -4.097333 -4.0098872 -3.929811 -3.9498935 -4.05844 -4.1539636 -4.1853657 -4.1777487 -4.1806388][-4.2462196 -4.2574778 -4.2575622 -4.253087 -4.2339048 -4.1887493 -4.11421 -4.0071034 -3.9043241 -3.9048862 -4.0094624 -4.109623 -4.1518173 -4.1590214 -4.1806593][-4.2487907 -4.2684054 -4.2677836 -4.2637777 -4.2531223 -4.2239304 -4.1672449 -4.0869293 -4.0104694 -3.9957061 -4.0468149 -4.10073 -4.1252956 -4.1358504 -4.167326][-4.2119117 -4.2385588 -4.2442718 -4.2442904 -4.2446613 -4.2358265 -4.2086596 -4.1703906 -4.1319695 -4.1141119 -4.1173038 -4.1195173 -4.1127734 -4.11289 -4.1408706][-4.1689715 -4.1910191 -4.19902 -4.207541 -4.2178259 -4.2225695 -4.2155514 -4.20989 -4.2040915 -4.1996627 -4.190927 -4.168992 -4.139358 -4.1175804 -4.1244884][-4.1384048 -4.150218 -4.1625018 -4.1804852 -4.1947722 -4.2007275 -4.2000437 -4.2094307 -4.2240891 -4.2397161 -4.240602 -4.2217216 -4.1873326 -4.1522202 -4.1384115][-4.1379442 -4.1381412 -4.1558423 -4.1823382 -4.1968207 -4.1955385 -4.1872926 -4.1899481 -4.2064605 -4.2362423 -4.2521949 -4.2448783 -4.2194133 -4.1890745 -4.1732683][-4.1692171 -4.1661196 -4.1893878 -4.2194366 -4.2288327 -4.2163396 -4.1960778 -4.1841307 -4.1928291 -4.2200832 -4.2381158 -4.2355986 -4.2183356 -4.2023191 -4.1963849]]...]
INFO - root - 2017-12-06 09:58:31.478681: step 7110, loss = 2.12, batch loss = 2.07 (37.6 examples/sec; 0.213 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-06 09:58:33.664140: step 7120, loss = 2.05, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:33m:29s remains)
INFO - root - 2017-12-06 09:58:35.857135: step 7130, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:29m:18s remains)
INFO - root - 2017-12-06 09:58:38.028972: step 7140, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.215 sec/batch; 19h:23m:54s remains)
INFO - root - 2017-12-06 09:58:40.228353: step 7150, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:16m:38s remains)
INFO - root - 2017-12-06 09:58:42.411770: step 7160, loss = 2.08, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 19h:45m:53s remains)
INFO - root - 2017-12-06 09:58:44.645052: step 7170, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:35m:18s remains)
INFO - root - 2017-12-06 09:58:46.789778: step 7180, loss = 2.09, batch loss = 2.03 (38.4 examples/sec; 0.208 sec/batch; 18h:48m:37s remains)
INFO - root - 2017-12-06 09:58:49.159412: step 7190, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:31m:47s remains)
INFO - root - 2017-12-06 09:58:51.341775: step 7200, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 20h:03m:04s remains)
2017-12-06 09:58:51.750954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597036 -4.2640872 -4.2780004 -4.2791767 -4.2760019 -4.2781649 -4.272264 -4.26472 -4.2655859 -4.2648664 -4.25689 -4.2469749 -4.2461252 -4.2570243 -4.2706084][-4.2972465 -4.2958241 -4.3008804 -4.292315 -4.28127 -4.2801943 -4.2715 -4.2597613 -4.2570429 -4.2573323 -4.25033 -4.2399645 -4.2346067 -4.2403574 -4.2495031][-4.3226705 -4.3188109 -4.3105769 -4.2932177 -4.2761955 -4.2674246 -4.2521496 -4.2375712 -4.2386966 -4.2517295 -4.2574673 -4.2514987 -4.2400708 -4.2367387 -4.2402492][-4.3218341 -4.3227654 -4.3142571 -4.2901392 -4.2614512 -4.2339053 -4.20286 -4.1852889 -4.2009945 -4.2419825 -4.2753797 -4.2836943 -4.2728353 -4.2608614 -4.2562718][-4.3007741 -4.3091445 -4.3021426 -4.269485 -4.2242565 -4.1688023 -4.1104479 -4.0804181 -4.1100621 -4.1849666 -4.2507234 -4.2849841 -4.2924175 -4.2876873 -4.2839127][-4.2896848 -4.3030138 -4.2910414 -4.2408218 -4.1720257 -4.08341 -3.9784834 -3.9130177 -3.9554214 -4.0745053 -4.1797633 -4.2456241 -4.2855635 -4.302835 -4.3068471][-4.2805624 -4.2998977 -4.2882824 -4.2302155 -4.1481428 -4.0373564 -3.8882723 -3.7719557 -3.8127244 -3.9676888 -4.10507 -4.1945748 -4.2601724 -4.2951822 -4.3033223][-4.2686486 -4.2995081 -4.3016338 -4.2545547 -4.1850004 -4.0860043 -3.9413717 -3.8071318 -3.8189483 -3.9519672 -4.0824151 -4.1756816 -4.247972 -4.2866573 -4.2922645][-4.2611814 -4.3019152 -4.3179111 -4.290833 -4.2440648 -4.18095 -4.08125 -3.9797029 -3.9646308 -4.0355206 -4.1217871 -4.190908 -4.2458639 -4.2744875 -4.2737732][-4.2748919 -4.3149247 -4.336751 -4.3260846 -4.2965 -4.2592292 -4.2037816 -4.1471376 -4.1252732 -4.1506066 -4.1932778 -4.2311425 -4.2601609 -4.26932 -4.2578692][-4.3126717 -4.338222 -4.3525772 -4.3462958 -4.3277173 -4.3100057 -4.2863216 -4.2609386 -4.2455888 -4.2541947 -4.2726684 -4.2848682 -4.2891741 -4.2812648 -4.2580328][-4.3373995 -4.3470883 -4.3502083 -4.3452706 -4.3334742 -4.325387 -4.3193321 -4.3164415 -4.3163004 -4.3251872 -4.3346772 -4.3328743 -4.3205957 -4.2991791 -4.270123][-4.3429041 -4.3447952 -4.3388562 -4.3354878 -4.3339305 -4.33463 -4.3387461 -4.3454437 -4.3498445 -4.3585386 -4.3643508 -4.3608079 -4.3454046 -4.3196592 -4.2909431][-4.3476081 -4.3479991 -4.3404989 -4.3387384 -4.3411088 -4.3444552 -4.3493176 -4.3569422 -4.361413 -4.3670325 -4.3712773 -4.3709645 -4.3593469 -4.3365 -4.3130908][-4.3493586 -4.3495226 -4.3427773 -4.3401284 -4.3400846 -4.3398881 -4.3422866 -4.3475423 -4.351016 -4.3570166 -4.3643513 -4.3672652 -4.3609767 -4.3492856 -4.3394613]]...]
INFO - root - 2017-12-06 09:58:55.253842: step 7210, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:41m:06s remains)
INFO - root - 2017-12-06 09:58:58.483295: step 7220, loss = 2.10, batch loss = 2.04 (23.9 examples/sec; 0.335 sec/batch; 30h:16m:52s remains)
INFO - root - 2017-12-06 09:59:02.511813: step 7230, loss = 2.10, batch loss = 2.04 (18.7 examples/sec; 0.429 sec/batch; 38h:43m:44s remains)
INFO - root - 2017-12-06 09:59:06.740948: step 7240, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.419 sec/batch; 37h:49m:16s remains)
INFO - root - 2017-12-06 09:59:10.960480: step 7250, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.410 sec/batch; 37h:05m:00s remains)
INFO - root - 2017-12-06 09:59:15.258271: step 7260, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:23m:23s remains)
INFO - root - 2017-12-06 09:59:19.587826: step 7270, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.431 sec/batch; 38h:55m:21s remains)
INFO - root - 2017-12-06 09:59:24.017395: step 7280, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 42h:06m:56s remains)
INFO - root - 2017-12-06 09:59:28.364415: step 7290, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 38h:48m:14s remains)
INFO - root - 2017-12-06 09:59:32.531030: step 7300, loss = 2.10, batch loss = 2.04 (20.2 examples/sec; 0.396 sec/batch; 35h:46m:38s remains)
2017-12-06 09:59:33.047925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3196893 -4.32031 -4.3135891 -4.3025913 -4.2854409 -4.26596 -4.2285528 -4.2158427 -4.2038174 -4.1944284 -4.2206621 -4.2555037 -4.2851849 -4.2974238 -4.2908149][-4.3178682 -4.3207092 -4.3112888 -4.2960105 -4.2786241 -4.2574759 -4.2123928 -4.1942892 -4.183291 -4.1779065 -4.20162 -4.2312541 -4.2524204 -4.2598224 -4.252069][-4.3198938 -4.3213968 -4.31078 -4.2919197 -4.2720132 -4.2499571 -4.2041078 -4.1787987 -4.1721597 -4.1707916 -4.1841984 -4.2027912 -4.2172008 -4.2254663 -4.2195711][-4.3213763 -4.3205428 -4.3109264 -4.2904491 -4.2714868 -4.2497091 -4.2058206 -4.1759663 -4.1751561 -4.1818786 -4.1856618 -4.1921434 -4.2043562 -4.2169604 -4.2177358][-4.3149915 -4.3124537 -4.3034525 -4.283361 -4.2675943 -4.2475109 -4.2002087 -4.1608291 -4.1647291 -4.1781206 -4.1816759 -4.1832538 -4.1923137 -4.2068505 -4.2152171][-4.3039517 -4.3011603 -4.2913785 -4.2713022 -4.2568612 -4.2306795 -4.17568 -4.1211867 -4.1330543 -4.161118 -4.1780682 -4.1839194 -4.1875286 -4.1942377 -4.2017031][-4.2894344 -4.2928028 -4.2873287 -4.26668 -4.2473254 -4.2080727 -4.1364012 -4.0594344 -4.0840797 -4.141469 -4.1801128 -4.19477 -4.197886 -4.1991944 -4.2021675][-4.2846651 -4.2946515 -4.2891846 -4.2601829 -4.2273927 -4.16964 -4.0672083 -3.9516428 -3.9914036 -4.087986 -4.1497412 -4.1757717 -4.190227 -4.1994386 -4.2023449][-4.2924089 -4.3030643 -4.2910705 -4.2540555 -4.2099867 -4.1373167 -4.0101395 -3.8650117 -3.913311 -4.0360293 -4.1107745 -4.1483617 -4.1798611 -4.200037 -4.2086248][-4.3055911 -4.315731 -4.3032422 -4.2685194 -4.2286429 -4.1708355 -4.0717835 -3.9568086 -3.9865122 -4.0739632 -4.124928 -4.1523609 -4.182301 -4.2062769 -4.219593][-4.3133821 -4.320601 -4.3130713 -4.2880392 -4.2593207 -4.2197371 -4.1547246 -4.0736241 -4.0832148 -4.1328435 -4.161221 -4.1753922 -4.1918874 -4.2128649 -4.2285371][-4.3066177 -4.3112431 -4.3128433 -4.3048592 -4.2913222 -4.264225 -4.2212887 -4.1657991 -4.1613641 -4.1844392 -4.1982265 -4.2070618 -4.2174416 -4.2331729 -4.2471433][-4.2887373 -4.2922406 -4.301734 -4.3082981 -4.3089662 -4.2955413 -4.2710838 -4.2346888 -4.22383 -4.2311282 -4.2345996 -4.2349777 -4.2379389 -4.2471371 -4.2570367][-4.2724276 -4.2776732 -4.2910132 -4.3012223 -4.3079829 -4.3067846 -4.2971578 -4.2748318 -4.2651186 -4.267251 -4.2639871 -4.256783 -4.2516551 -4.2507539 -4.2521114][-4.2722578 -4.2778192 -4.2881594 -4.2928367 -4.2978649 -4.2983146 -4.2923403 -4.2762747 -4.2690639 -4.2721806 -4.2684956 -4.2603559 -4.2548437 -4.2490788 -4.2424941]]...]
INFO - root - 2017-12-06 09:59:37.349608: step 7310, loss = 2.11, batch loss = 2.05 (18.7 examples/sec; 0.429 sec/batch; 38h:44m:05s remains)
INFO - root - 2017-12-06 09:59:41.386240: step 7320, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.414 sec/batch; 37h:25m:38s remains)
INFO - root - 2017-12-06 09:59:45.685097: step 7330, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.448 sec/batch; 40h:29m:55s remains)
INFO - root - 2017-12-06 09:59:50.005948: step 7340, loss = 2.11, batch loss = 2.05 (18.1 examples/sec; 0.441 sec/batch; 39h:51m:21s remains)
INFO - root - 2017-12-06 09:59:54.250610: step 7350, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.422 sec/batch; 38h:08m:30s remains)
INFO - root - 2017-12-06 09:59:58.499601: step 7360, loss = 2.09, batch loss = 2.04 (18.4 examples/sec; 0.434 sec/batch; 39h:14m:28s remains)
INFO - root - 2017-12-06 10:00:02.706833: step 7370, loss = 2.08, batch loss = 2.02 (19.4 examples/sec; 0.413 sec/batch; 37h:16m:55s remains)
INFO - root - 2017-12-06 10:00:06.914565: step 7380, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 38h:41m:47s remains)
INFO - root - 2017-12-06 10:00:11.134900: step 7390, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 37h:58m:53s remains)
INFO - root - 2017-12-06 10:00:15.391361: step 7400, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 38h:41m:45s remains)
2017-12-06 10:00:15.887244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083634 -4.3231325 -4.3257051 -4.3063092 -4.2803926 -4.24061 -4.1939898 -4.1422377 -4.1019373 -4.1174183 -4.1696324 -4.2243013 -4.2617254 -4.27586 -4.2786484][-4.3255434 -4.3345032 -4.325521 -4.3011975 -4.278192 -4.2441993 -4.1997933 -4.1454573 -4.10584 -4.1258645 -4.17421 -4.2207313 -4.2536535 -4.2682095 -4.2791343][-4.3128729 -4.3195047 -4.3074303 -4.2880645 -4.2764583 -4.2609029 -4.2276011 -4.1740713 -4.1357403 -4.1516609 -4.18981 -4.2216558 -4.2505112 -4.2717676 -4.2889118][-4.3006411 -4.3039794 -4.2937942 -4.2814856 -4.2792826 -4.2794924 -4.2510357 -4.1961536 -4.1593342 -4.1723847 -4.2037606 -4.2292104 -4.2564616 -4.2799354 -4.2983379][-4.3022 -4.297358 -4.286994 -4.2744231 -4.2705765 -4.2689166 -4.23338 -4.1724463 -4.13932 -4.163599 -4.197824 -4.2263575 -4.2569818 -4.2831397 -4.3023829][-4.3087454 -4.2982349 -4.2880735 -4.2722631 -4.2537222 -4.22633 -4.1640677 -4.0801778 -4.0559368 -4.114861 -4.1740246 -4.2129803 -4.2491112 -4.2806587 -4.3013344][-4.3083844 -4.3005314 -4.2953625 -4.2802877 -4.242033 -4.1758914 -4.0634022 -3.933053 -3.9204097 -4.0307779 -4.1298246 -4.1909146 -4.2382727 -4.2773495 -4.3010845][-4.299716 -4.2980876 -4.30027 -4.2845297 -4.2291718 -4.1309118 -3.977519 -3.8048956 -3.7968459 -3.9459915 -4.0781579 -4.1655068 -4.228169 -4.2755947 -4.3015642][-4.29389 -4.2948895 -4.2992315 -4.2822065 -4.2271781 -4.1346965 -3.998394 -3.8481765 -3.8216174 -3.9357123 -4.0581779 -4.1519585 -4.2222629 -4.2731051 -4.3011947][-4.2915 -4.2925453 -4.2950525 -4.2805958 -4.2389703 -4.1748629 -4.0858207 -3.9837773 -3.9450078 -3.9967256 -4.0795403 -4.1589146 -4.2251625 -4.2749763 -4.3016777][-4.29459 -4.2931485 -4.2920041 -4.2789717 -4.2476511 -4.2055316 -4.1538463 -4.0907335 -4.0571666 -4.0781264 -4.1287718 -4.1880088 -4.2432604 -4.284409 -4.3049197][-4.2989545 -4.2960925 -4.2923203 -4.2814703 -4.2591715 -4.2331319 -4.2015591 -4.1622353 -4.1417723 -4.1530576 -4.1837897 -4.2264795 -4.2682428 -4.29653 -4.309094][-4.3028164 -4.2995729 -4.2951632 -4.2855649 -4.2687917 -4.2511573 -4.2277603 -4.1994619 -4.187377 -4.1957846 -4.221025 -4.2555494 -4.2859106 -4.3037753 -4.30965][-4.3009453 -4.2962394 -4.2926135 -4.2860012 -4.27256 -4.2563329 -4.2358065 -4.2125769 -4.2046518 -4.2124925 -4.2374291 -4.2695603 -4.291688 -4.3023 -4.3050041][-4.2942533 -4.2894063 -4.2862158 -4.2825665 -4.2737107 -4.26029 -4.2436781 -4.2252684 -4.2194204 -4.2283912 -4.2509289 -4.2752523 -4.2904491 -4.2969546 -4.2982359]]...]
INFO - root - 2017-12-06 10:00:20.143362: step 7410, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.421 sec/batch; 38h:03m:34s remains)
INFO - root - 2017-12-06 10:00:24.143273: step 7420, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 38h:51m:46s remains)
INFO - root - 2017-12-06 10:00:28.513836: step 7430, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.441 sec/batch; 39h:47m:53s remains)
INFO - root - 2017-12-06 10:00:32.841904: step 7440, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:06m:08s remains)
INFO - root - 2017-12-06 10:00:37.173334: step 7450, loss = 2.10, batch loss = 2.05 (18.6 examples/sec; 0.430 sec/batch; 38h:50m:01s remains)
INFO - root - 2017-12-06 10:00:41.487002: step 7460, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:42m:19s remains)
INFO - root - 2017-12-06 10:00:45.822685: step 7470, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:31m:39s remains)
INFO - root - 2017-12-06 10:00:50.215956: step 7480, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 40h:20m:45s remains)
INFO - root - 2017-12-06 10:00:54.594090: step 7490, loss = 2.06, batch loss = 2.01 (19.2 examples/sec; 0.416 sec/batch; 37h:33m:09s remains)
INFO - root - 2017-12-06 10:00:58.945914: step 7500, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.429 sec/batch; 38h:43m:48s remains)
2017-12-06 10:00:59.517661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3260145 -4.3220387 -4.3249941 -4.33235 -4.3288207 -4.3151836 -4.301621 -4.3010788 -4.3096271 -4.3154769 -4.319077 -4.3218026 -4.3242264 -4.3161364 -4.3001847][-4.3291397 -4.3257937 -4.3257828 -4.3276167 -4.3128042 -4.2866187 -4.2633777 -4.2628756 -4.2757468 -4.2846651 -4.2931876 -4.3048997 -4.3101544 -4.2948408 -4.2691751][-4.3337126 -4.33051 -4.3251958 -4.3153934 -4.2863235 -4.2473521 -4.2117682 -4.2112961 -4.2359715 -4.2578044 -4.275022 -4.2930503 -4.2984438 -4.2753348 -4.2394495][-4.3399367 -4.3378139 -4.3281627 -4.304 -4.2601905 -4.204731 -4.154511 -4.1568427 -4.2023797 -4.2460008 -4.2717834 -4.2934356 -4.2965283 -4.2636862 -4.2154222][-4.3430238 -4.3425355 -4.3295493 -4.2954278 -4.2379131 -4.1659312 -4.101337 -4.1027718 -4.1660748 -4.228199 -4.2623606 -4.28435 -4.2828393 -4.245131 -4.1939855][-4.3379498 -4.3346772 -4.3147869 -4.2735248 -4.2083111 -4.1198874 -4.0376506 -4.0346746 -4.1123147 -4.1930342 -4.2406864 -4.2672148 -4.26534 -4.2271638 -4.1817226][-4.3247218 -4.314115 -4.2872429 -4.2434278 -4.1759162 -4.0792503 -3.9763918 -3.9607339 -4.0577126 -4.1627483 -4.2228632 -4.2539907 -4.2509527 -4.2165842 -4.1864543][-4.2977343 -4.2795773 -4.2481322 -4.2080789 -4.14839 -4.0492568 -3.9202754 -3.8783073 -3.9915323 -4.1195016 -4.1899457 -4.21949 -4.2140651 -4.1859083 -4.1767][-4.2553449 -4.2321796 -4.2011476 -4.1738515 -4.1395216 -4.0658226 -3.9561896 -3.9075837 -3.9929221 -4.1013389 -4.1619391 -4.1848536 -4.1763291 -4.1517658 -4.1538172][-4.2187915 -4.1996136 -4.1791248 -4.1705575 -4.168292 -4.1344872 -4.0702033 -4.0380378 -4.0816607 -4.1424332 -4.1802444 -4.1955118 -4.1794605 -4.1472235 -4.1450067][-4.2058496 -4.1912727 -4.1742792 -4.1739058 -4.1883 -4.1799784 -4.1450238 -4.1226583 -4.1448541 -4.1764746 -4.19869 -4.2114706 -4.1934905 -4.1558056 -4.1485171][-4.2030444 -4.1883645 -4.1749258 -4.1776352 -4.1965103 -4.2046647 -4.1910205 -4.1782861 -4.1919985 -4.206821 -4.2185616 -4.22847 -4.2133069 -4.1813679 -4.1727848][-4.212172 -4.2007289 -4.19384 -4.1997743 -4.2180848 -4.2329192 -4.2320905 -4.2273026 -4.2362914 -4.2464967 -4.255538 -4.2643948 -4.2571325 -4.236299 -4.2260971][-4.2337818 -4.2275591 -4.2247434 -4.2303324 -4.2463479 -4.2624707 -4.2697344 -4.2703552 -4.2762446 -4.284338 -4.2917013 -4.2976384 -4.2933135 -4.2806726 -4.2721658][-4.2624135 -4.2582936 -4.256968 -4.2598324 -4.2722335 -4.2883635 -4.2983675 -4.3027368 -4.3075628 -4.3130426 -4.317359 -4.320406 -4.3173866 -4.31166 -4.3061223]]...]
INFO - root - 2017-12-06 10:01:03.819670: step 7510, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 38h:35m:55s remains)
INFO - root - 2017-12-06 10:01:07.875352: step 7520, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.416 sec/batch; 37h:31m:46s remains)
INFO - root - 2017-12-06 10:01:12.051278: step 7530, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:53m:34s remains)
INFO - root - 2017-12-06 10:01:16.368469: step 7540, loss = 2.09, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 38h:15m:48s remains)
INFO - root - 2017-12-06 10:01:20.835765: step 7550, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:06m:25s remains)
INFO - root - 2017-12-06 10:01:25.179233: step 7560, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 40h:58m:16s remains)
INFO - root - 2017-12-06 10:01:29.516322: step 7570, loss = 2.11, batch loss = 2.05 (18.8 examples/sec; 0.426 sec/batch; 38h:29m:27s remains)
INFO - root - 2017-12-06 10:01:33.812413: step 7580, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.431 sec/batch; 38h:53m:55s remains)
INFO - root - 2017-12-06 10:01:38.274986: step 7590, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.448 sec/batch; 40h:23m:30s remains)
INFO - root - 2017-12-06 10:01:42.603450: step 7600, loss = 2.06, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 39h:44m:47s remains)
2017-12-06 10:01:43.140062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893686 -4.2743459 -4.2609458 -4.2536697 -4.2479024 -4.2422614 -4.2396822 -4.2448664 -4.2563667 -4.2625809 -4.2705016 -4.2813921 -4.2988386 -4.3150215 -4.3162041][-4.2622228 -4.2417836 -4.2224736 -4.2099566 -4.1953754 -4.1821036 -4.1733427 -4.1781721 -4.1940575 -4.2062321 -4.2225962 -4.2433252 -4.2711291 -4.2938533 -4.2973323][-4.241468 -4.2205715 -4.2010965 -4.1887441 -4.1676979 -4.1415262 -4.1192365 -4.1203513 -4.1399121 -4.1562805 -4.1804647 -4.2078428 -4.2428112 -4.2694526 -4.2737746][-4.2283835 -4.2096872 -4.1927223 -4.180356 -4.1515274 -4.1098328 -4.077599 -4.0802402 -4.1034813 -4.1186795 -4.1435957 -4.1688867 -4.2023225 -4.2312031 -4.2397861][-4.2229481 -4.2053938 -4.188498 -4.1718616 -4.132906 -4.0795131 -4.0432067 -4.0558619 -4.0861936 -4.0985112 -4.1154432 -4.1256895 -4.1463919 -4.1711907 -4.1899157][-4.2208943 -4.2023783 -4.1829495 -4.1554184 -4.1004572 -4.0300822 -3.9904447 -4.0266252 -4.0753417 -4.0937119 -4.102345 -4.0942035 -4.0977612 -4.1194143 -4.1501908][-4.2186422 -4.1977944 -4.175756 -4.1375952 -4.0612869 -3.9578333 -3.9080935 -3.9757903 -4.0577221 -4.0923071 -4.0989728 -4.07941 -4.0682821 -4.0900478 -4.1335387][-4.2192125 -4.1995687 -4.1793861 -4.1397524 -4.0545115 -3.929121 -3.8632197 -3.9425986 -4.0413165 -4.0854568 -4.0910339 -4.0665007 -4.0467114 -4.0684271 -4.121139][-4.2274594 -4.2170491 -4.2033648 -4.172101 -4.1063223 -4.0065732 -3.9428492 -3.989629 -4.0619712 -4.0961447 -4.1028204 -4.0853376 -4.0674996 -4.0811162 -4.1244245][-4.2370753 -4.2349024 -4.2254624 -4.2038393 -4.1626978 -4.1018729 -4.0560665 -4.0725908 -4.102942 -4.117115 -4.1243944 -4.1237893 -4.12 -4.1285963 -4.1537738][-4.243506 -4.2432594 -4.2360997 -4.2207718 -4.1941743 -4.1593237 -4.1325521 -4.1401916 -4.1456733 -4.1409817 -4.1443529 -4.1539464 -4.1631823 -4.1698303 -4.1831045][-4.2430182 -4.2389479 -4.232091 -4.2201166 -4.2013922 -4.1825843 -4.1736 -4.1857905 -4.1858897 -4.1755395 -4.1746006 -4.1837792 -4.1953411 -4.1989613 -4.2055483][-4.2372255 -4.2248597 -4.2117319 -4.1968613 -4.18006 -4.1721187 -4.1789975 -4.2034435 -4.211184 -4.2026887 -4.1995645 -4.2066588 -4.2170787 -4.2188 -4.2204895][-4.2376285 -4.2195268 -4.1976023 -4.1754189 -4.1555376 -4.15167 -4.1637888 -4.1977582 -4.219532 -4.2163911 -4.2099595 -4.2151666 -4.2241216 -4.2266655 -4.2241206][-4.242753 -4.2233143 -4.1958308 -4.1682816 -4.1410336 -4.1333127 -4.1466966 -4.1839004 -4.2123442 -4.2154098 -4.20823 -4.2086096 -4.2158651 -4.2212853 -4.2196665]]...]
INFO - root - 2017-12-06 10:01:47.508330: step 7610, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:15m:34s remains)
INFO - root - 2017-12-06 10:01:51.631154: step 7620, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:39m:19s remains)
INFO - root - 2017-12-06 10:01:55.963960: step 7630, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 37h:54m:14s remains)
INFO - root - 2017-12-06 10:02:00.298419: step 7640, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:40m:52s remains)
INFO - root - 2017-12-06 10:02:04.650132: step 7650, loss = 2.09, batch loss = 2.03 (19.1 examples/sec; 0.420 sec/batch; 37h:53m:10s remains)
INFO - root - 2017-12-06 10:02:08.968628: step 7660, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:15m:56s remains)
INFO - root - 2017-12-06 10:02:13.414668: step 7670, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 39h:29m:02s remains)
INFO - root - 2017-12-06 10:02:17.792292: step 7680, loss = 2.08, batch loss = 2.03 (19.3 examples/sec; 0.415 sec/batch; 37h:25m:49s remains)
INFO - root - 2017-12-06 10:02:22.038741: step 7690, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.421 sec/batch; 37h:59m:50s remains)
INFO - root - 2017-12-06 10:02:26.279065: step 7700, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.426 sec/batch; 38h:23m:50s remains)
2017-12-06 10:02:26.810634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26812 -4.2725139 -4.2616534 -4.2457328 -4.2360125 -4.2426414 -4.2679863 -4.3020377 -4.3298631 -4.3447094 -4.3506379 -4.3430142 -4.3215003 -4.2969775 -4.2804413][-4.2029991 -4.2203226 -4.212254 -4.1967945 -4.18847 -4.1961017 -4.2246661 -4.2612457 -4.2899613 -4.3021989 -4.3082557 -4.3044343 -4.2838697 -4.2560186 -4.2382483][-4.15057 -4.1822014 -4.17888 -4.167347 -4.1653228 -4.1749907 -4.2019 -4.23321 -4.2563477 -4.2667379 -4.2743278 -4.2746243 -4.25797 -4.2323189 -4.2179093][-4.1456995 -4.1797891 -4.1793985 -4.1758451 -4.1788974 -4.1843257 -4.1962252 -4.2102523 -4.223042 -4.2316604 -4.2423916 -4.2495852 -4.2435369 -4.2302175 -4.225791][-4.1996875 -4.2210703 -4.2216258 -4.2214742 -4.2156835 -4.2003832 -4.1857224 -4.1780267 -4.1859193 -4.1944876 -4.2067962 -4.2222662 -4.23264 -4.2384911 -4.246985][-4.2524495 -4.2539597 -4.2457719 -4.2384167 -4.2170172 -4.1796241 -4.14445 -4.1265435 -4.1323085 -4.1386056 -4.1502233 -4.1683636 -4.1874423 -4.2086449 -4.2354331][-4.25895 -4.2468519 -4.2221961 -4.1980405 -4.1533642 -4.1017 -4.0610304 -4.0474806 -4.0592813 -4.0663528 -4.0790668 -4.096705 -4.1189609 -4.1560059 -4.2020545][-4.2551231 -4.2325883 -4.1947141 -4.1502867 -4.083951 -4.0264297 -3.9949718 -3.9943774 -4.0150185 -4.0275583 -4.0395889 -4.0538344 -4.0823431 -4.1317172 -4.1871686][-4.25799 -4.2296448 -4.1840014 -4.1291008 -4.0560184 -4.0054712 -3.9898193 -3.9952555 -4.022212 -4.0448117 -4.0593414 -4.0715876 -4.102653 -4.158535 -4.2109423][-4.2576232 -4.2269149 -4.1851516 -4.1396708 -4.0831451 -4.0476837 -4.0462022 -4.0609112 -4.09435 -4.1254363 -4.1434479 -4.1562557 -4.1822619 -4.222507 -4.2565546][-4.2499108 -4.2251616 -4.1976943 -4.1743093 -4.1470962 -4.1337657 -4.144599 -4.1664562 -4.1994128 -4.2256203 -4.2376471 -4.2446408 -4.2583227 -4.2792258 -4.2952003][-4.2376266 -4.2265062 -4.2202635 -4.2212186 -4.2191825 -4.2232194 -4.2388744 -4.2589569 -4.2812638 -4.2926488 -4.2955961 -4.2949557 -4.2992034 -4.3094921 -4.3159719][-4.2202806 -4.2278624 -4.2431526 -4.2615538 -4.2720356 -4.2816668 -4.2915816 -4.3023548 -4.313242 -4.3148193 -4.3119726 -4.3099918 -4.3134151 -4.3199382 -4.32337][-4.1858897 -4.2065392 -4.2393928 -4.2727551 -4.2907748 -4.2994461 -4.3039184 -4.3071976 -4.3119345 -4.3117437 -4.309721 -4.3107448 -4.3142567 -4.3173709 -4.3190455][-4.1431842 -4.1697655 -4.2135768 -4.2563753 -4.2806025 -4.291254 -4.2963562 -4.2975545 -4.3000073 -4.2997909 -4.3000278 -4.3024974 -4.3050227 -4.3073521 -4.309226]]...]
INFO - root - 2017-12-06 10:02:31.158151: step 7710, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.422 sec/batch; 38h:06m:25s remains)
INFO - root - 2017-12-06 10:02:35.275453: step 7720, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 38h:38m:41s remains)
INFO - root - 2017-12-06 10:02:39.580529: step 7730, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 38h:16m:14s remains)
INFO - root - 2017-12-06 10:02:43.879413: step 7740, loss = 2.10, batch loss = 2.04 (17.7 examples/sec; 0.451 sec/batch; 40h:39m:46s remains)
INFO - root - 2017-12-06 10:02:48.394051: step 7750, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 38h:48m:48s remains)
INFO - root - 2017-12-06 10:02:52.806699: step 7760, loss = 2.10, batch loss = 2.05 (17.5 examples/sec; 0.456 sec/batch; 41h:10m:06s remains)
INFO - root - 2017-12-06 10:02:57.153611: step 7770, loss = 2.06, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:24m:13s remains)
INFO - root - 2017-12-06 10:03:01.495947: step 7780, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:04m:06s remains)
INFO - root - 2017-12-06 10:03:05.822248: step 7790, loss = 2.10, batch loss = 2.04 (17.8 examples/sec; 0.449 sec/batch; 40h:31m:04s remains)
INFO - root - 2017-12-06 10:03:10.290864: step 7800, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.431 sec/batch; 38h:50m:13s remains)
2017-12-06 10:03:10.768970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3128371 -4.2926164 -4.2823925 -4.2767987 -4.2698135 -4.2662163 -4.2692509 -4.2713451 -4.2742248 -4.2775726 -4.2847157 -4.29499 -4.3076916 -4.3156071 -4.3169684][-4.2951627 -4.2640128 -4.2513876 -4.2490506 -4.2422609 -4.2377019 -4.24121 -4.2435937 -4.2452679 -4.2460785 -4.2531285 -4.263823 -4.2770677 -4.2879639 -4.2938237][-4.2634935 -4.2259941 -4.2127309 -4.2193365 -4.2180662 -4.2167315 -4.2216654 -4.2243404 -4.2258344 -4.2248373 -4.2307906 -4.2404923 -4.25199 -4.2602835 -4.2667232][-4.2314372 -4.1928372 -4.1825676 -4.1993122 -4.2058949 -4.2041183 -4.2006588 -4.1984024 -4.2031918 -4.2108736 -4.2216506 -4.2333584 -4.2429595 -4.2460666 -4.2489324][-4.2307463 -4.1992607 -4.18815 -4.1999702 -4.2025352 -4.1900826 -4.1691217 -4.1548157 -4.1704292 -4.19687 -4.2199717 -4.2365446 -4.245306 -4.2479725 -4.2528362][-4.2329321 -4.2080359 -4.1958923 -4.1987128 -4.1934958 -4.1684322 -4.1281843 -4.0946517 -4.1201329 -4.1732903 -4.2114286 -4.2360225 -4.2465863 -4.2523994 -4.2614317][-4.2153559 -4.1972184 -4.1806984 -4.1738014 -4.1559143 -4.1107664 -4.036891 -3.9638095 -3.9968286 -4.0957808 -4.1627398 -4.2033005 -4.2241635 -4.2369976 -4.2509427][-4.2052693 -4.1934385 -4.1757021 -4.1560478 -4.1165237 -4.0383573 -3.9179893 -3.786674 -3.8167448 -3.9742672 -4.0845237 -4.1464624 -4.179882 -4.19827 -4.2137671][-4.2175913 -4.215446 -4.204721 -4.1841121 -4.1409035 -4.0612187 -3.9410613 -3.8104553 -3.8240762 -3.9699607 -4.0814681 -4.1419034 -4.1751661 -4.1905808 -4.19713][-4.2311106 -4.2365141 -4.2392197 -4.2321081 -4.2045135 -4.1487603 -4.0694284 -3.9854839 -3.9866002 -4.0799179 -4.159225 -4.1985903 -4.2153153 -4.2202325 -4.2193446][-4.2471952 -4.2570534 -4.2655249 -4.2638621 -4.2485895 -4.210547 -4.1574726 -4.1056328 -4.1038632 -4.1624274 -4.2204905 -4.2503724 -4.2613082 -4.2622581 -4.2598543][-4.2618532 -4.2787776 -4.290781 -4.2907047 -4.2769527 -4.2467756 -4.2085047 -4.178503 -4.1783638 -4.2107654 -4.2493229 -4.2705984 -4.2780318 -4.2802315 -4.279686][-4.2704358 -4.2970781 -4.3130574 -4.314364 -4.2997694 -4.2718234 -4.243609 -4.2277937 -4.2308111 -4.2476645 -4.26912 -4.2805767 -4.2843394 -4.284894 -4.2815771][-4.2683187 -4.2969065 -4.3138814 -4.3159008 -4.30219 -4.2799664 -4.2624674 -4.2590427 -4.2667422 -4.2766304 -4.2879591 -4.2939062 -4.2948637 -4.2927713 -4.2872038][-4.2611508 -4.2915697 -4.310915 -4.3149433 -4.3040094 -4.287694 -4.2767396 -4.277246 -4.2850728 -4.2910228 -4.2953482 -4.2976789 -4.2985106 -4.298069 -4.2949338]]...]
INFO - root - 2017-12-06 10:03:15.143857: step 7810, loss = 2.06, batch loss = 2.01 (17.6 examples/sec; 0.455 sec/batch; 41h:03m:37s remains)
INFO - root - 2017-12-06 10:03:19.231194: step 7820, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 41h:27m:07s remains)
INFO - root - 2017-12-06 10:03:23.715822: step 7830, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 39h:15m:30s remains)
INFO - root - 2017-12-06 10:03:28.066205: step 7840, loss = 2.05, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 40h:02m:44s remains)
INFO - root - 2017-12-06 10:03:32.414753: step 7850, loss = 2.09, batch loss = 2.04 (17.9 examples/sec; 0.447 sec/batch; 40h:21m:01s remains)
INFO - root - 2017-12-06 10:03:36.840267: step 7860, loss = 2.06, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:16m:29s remains)
INFO - root - 2017-12-06 10:03:41.238468: step 7870, loss = 2.07, batch loss = 2.02 (19.1 examples/sec; 0.420 sec/batch; 37h:51m:04s remains)
INFO - root - 2017-12-06 10:03:45.615168: step 7880, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.427 sec/batch; 38h:27m:41s remains)
INFO - root - 2017-12-06 10:03:49.997476: step 7890, loss = 2.10, batch loss = 2.04 (18.7 examples/sec; 0.427 sec/batch; 38h:30m:51s remains)
INFO - root - 2017-12-06 10:03:54.371287: step 7900, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.439 sec/batch; 39h:37m:05s remains)
2017-12-06 10:03:54.850724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703776 -4.26429 -4.2643981 -4.2744184 -4.2812009 -4.2821703 -4.2830772 -4.2826691 -4.2845917 -4.2920675 -4.296329 -4.297163 -4.2914057 -4.283792 -4.2839284][-4.2224278 -4.2116046 -4.2166638 -4.2353806 -4.2475677 -4.2483363 -4.2490335 -4.2504969 -4.2544127 -4.2654214 -4.2752771 -4.2810035 -4.275176 -4.2622037 -4.2563906][-4.1949363 -4.1830688 -4.1959968 -4.2218494 -4.2360768 -4.23292 -4.2297282 -4.2291903 -4.2335896 -4.2484264 -4.2629623 -4.2721691 -4.2661786 -4.24839 -4.2379127][-4.1838107 -4.1740251 -4.1900358 -4.2170043 -4.2302456 -4.2238135 -4.2172213 -4.2141995 -4.2197266 -4.2367454 -4.2561612 -4.2685766 -4.2637038 -4.2443709 -4.2327447][-4.1827884 -4.1778321 -4.1933417 -4.2149529 -4.2259183 -4.2172704 -4.2091794 -4.2041779 -4.2067132 -4.2208614 -4.2427163 -4.2568831 -4.2537341 -4.2396226 -4.2325883][-4.1907897 -4.1917644 -4.2034397 -4.2156606 -4.2215815 -4.2118106 -4.2028518 -4.1966238 -4.1947246 -4.203649 -4.2249207 -4.2403674 -4.24088 -4.236402 -4.2381377][-4.1958194 -4.2007632 -4.2087965 -4.2127261 -4.2129927 -4.2025928 -4.194026 -4.1873059 -4.1828222 -4.1865005 -4.2037354 -4.2161617 -4.2191534 -4.2242827 -4.2347231][-4.1892962 -4.1942587 -4.2032266 -4.2061644 -4.2086954 -4.2031589 -4.1997824 -4.1961918 -4.1876984 -4.1852083 -4.1927309 -4.1986628 -4.1998935 -4.2086039 -4.222508][-4.174202 -4.1772218 -4.1908965 -4.1993117 -4.2085242 -4.2120209 -4.2155504 -4.2132521 -4.1986933 -4.1889119 -4.1869578 -4.1844835 -4.1798511 -4.1872215 -4.2012844][-4.1591606 -4.1624827 -4.1833887 -4.1996503 -4.2160745 -4.2255206 -4.2310648 -4.2238708 -4.200407 -4.1833534 -4.1747284 -4.1665926 -4.1561575 -4.15895 -4.173502][-4.1432972 -4.1490951 -4.1772704 -4.1985154 -4.2177329 -4.2270265 -4.2298532 -4.2172365 -4.1904078 -4.1729593 -4.1633883 -4.155159 -4.1397 -4.1359272 -4.1499314][-4.1267066 -4.1339593 -4.1651649 -4.1887884 -4.2082939 -4.2157607 -4.2132158 -4.1968651 -4.1702352 -4.1524787 -4.1435294 -4.1399965 -4.129137 -4.1237931 -4.1353769][-4.1155915 -4.1215477 -4.1506152 -4.1749253 -4.1949162 -4.202631 -4.1962376 -4.1782537 -4.1545129 -4.1382742 -4.1321063 -4.1357236 -4.1347332 -4.1321888 -4.1393566][-4.1219983 -4.1247272 -4.1467295 -4.1665068 -4.1853571 -4.1945939 -4.1869559 -4.1684093 -4.1502666 -4.1395135 -4.1383181 -4.146903 -4.1540203 -4.1549377 -4.1579108][-4.1521292 -4.1548972 -4.1705875 -4.1835666 -4.1965075 -4.2041473 -4.1982274 -4.1810422 -4.1651349 -4.1571827 -4.1601834 -4.1689525 -4.1776409 -4.1800928 -4.18307]]...]
INFO - root - 2017-12-06 10:03:59.150148: step 7910, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 38h:07m:40s remains)
INFO - root - 2017-12-06 10:04:03.254558: step 7920, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.429 sec/batch; 38h:43m:11s remains)
INFO - root - 2017-12-06 10:04:07.569032: step 7930, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:38m:25s remains)
INFO - root - 2017-12-06 10:04:12.040474: step 7940, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 39h:57m:29s remains)
INFO - root - 2017-12-06 10:04:16.370976: step 7950, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.423 sec/batch; 38h:09m:09s remains)
INFO - root - 2017-12-06 10:04:20.739091: step 7960, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:19m:28s remains)
INFO - root - 2017-12-06 10:04:25.138588: step 7970, loss = 2.10, batch loss = 2.04 (19.2 examples/sec; 0.417 sec/batch; 37h:33m:40s remains)
INFO - root - 2017-12-06 10:04:29.450979: step 7980, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 39h:30m:45s remains)
INFO - root - 2017-12-06 10:04:33.830148: step 7990, loss = 2.10, batch loss = 2.04 (17.9 examples/sec; 0.448 sec/batch; 40h:22m:56s remains)
INFO - root - 2017-12-06 10:04:38.247261: step 8000, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:34m:14s remains)
2017-12-06 10:04:38.653779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2319093 -4.2234497 -4.222538 -4.223208 -4.2157235 -4.2083988 -4.2079811 -4.2129931 -4.2252517 -4.2386465 -4.246758 -4.2468019 -4.243084 -4.2388439 -4.23609][-4.2125955 -4.1983409 -4.1956992 -4.1934981 -4.1840434 -4.1818862 -4.18947 -4.1967344 -4.2099204 -4.2279873 -4.2440453 -4.24971 -4.2459111 -4.2365823 -4.2272792][-4.197885 -4.1826482 -4.1770482 -4.165729 -4.1480145 -4.1401749 -4.1413956 -4.1503243 -4.1764126 -4.2110367 -4.239944 -4.2524767 -4.250577 -4.2403975 -4.2297311][-4.1835151 -4.170826 -4.160409 -4.1401663 -4.112154 -4.0895419 -4.0750008 -4.0836692 -4.12773 -4.1829624 -4.224493 -4.246706 -4.2511892 -4.2460003 -4.2379274][-4.1758795 -4.1640429 -4.147974 -4.1207256 -4.0831461 -4.0411758 -4.0073919 -4.014739 -4.0767016 -4.1502004 -4.2001328 -4.2299919 -4.2426677 -4.245038 -4.242394][-4.1687055 -4.154038 -4.1313071 -4.0997462 -4.054266 -3.9927082 -3.9380584 -3.9448464 -4.0243411 -4.1131091 -4.169735 -4.2054992 -4.2254958 -4.2340121 -4.2359691][-4.1655393 -4.1447091 -4.1132 -4.0726094 -4.0156941 -3.9327655 -3.8544977 -3.8584836 -3.9549685 -4.0578322 -4.1253004 -4.1725187 -4.2054138 -4.2227297 -4.2325606][-4.1668296 -4.141 -4.1014752 -4.0518265 -3.988987 -3.9043622 -3.8245559 -3.8267121 -3.9179568 -4.0135775 -4.0831647 -4.1401033 -4.1872473 -4.216548 -4.2360148][-4.167026 -4.1409078 -4.100935 -4.0517645 -4.0010333 -3.9447184 -3.8980207 -3.899478 -3.9514568 -4.0116868 -4.0668612 -4.1196513 -4.1671791 -4.2018805 -4.227087][-4.1621218 -4.13689 -4.0992837 -4.0550427 -4.0193219 -3.9964881 -3.9816236 -3.9812577 -4.0012207 -4.0323596 -4.0700507 -4.1094208 -4.1476955 -4.1807547 -4.2078695][-4.1564784 -4.1282907 -4.0888858 -4.0465431 -4.0221243 -4.0241413 -4.0299644 -4.0324759 -4.039772 -4.0568247 -4.0826306 -4.1103544 -4.14253 -4.1730056 -4.1963658][-4.161623 -4.1287489 -4.0853248 -4.0459886 -4.0367379 -4.0602756 -4.0804658 -4.0849247 -4.083735 -4.0902085 -4.1070094 -4.1250148 -4.1518283 -4.1730342 -4.186142][-4.1829109 -4.1500463 -4.1076822 -4.07435 -4.0789537 -4.1144409 -4.1363726 -4.135541 -4.1247454 -4.12196 -4.1292338 -4.1425161 -4.1632452 -4.1718297 -4.1724877][-4.21565 -4.188508 -4.1522932 -4.1259155 -4.1374049 -4.1717014 -4.1856632 -4.1764541 -4.1548223 -4.1427665 -4.14865 -4.1670284 -4.1886597 -4.1889186 -4.1785874][-4.2525148 -4.2334995 -4.2053642 -4.1881566 -4.2039614 -4.2306004 -4.2324634 -4.2130833 -4.1828685 -4.1662273 -4.1742272 -4.2006335 -4.2253528 -4.2221518 -4.2061405]]...]
INFO - root - 2017-12-06 10:04:42.973002: step 8010, loss = 2.07, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:45m:19s remains)
INFO - root - 2017-12-06 10:04:47.159068: step 8020, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 39h:11m:55s remains)
INFO - root - 2017-12-06 10:04:51.511150: step 8030, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 38h:51m:21s remains)
INFO - root - 2017-12-06 10:04:55.872650: step 8040, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 38h:43m:37s remains)
INFO - root - 2017-12-06 10:05:00.216067: step 8050, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.431 sec/batch; 38h:53m:00s remains)
INFO - root - 2017-12-06 10:05:04.561900: step 8060, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:51m:09s remains)
INFO - root - 2017-12-06 10:05:09.015285: step 8070, loss = 2.10, batch loss = 2.04 (17.9 examples/sec; 0.448 sec/batch; 40h:20m:24s remains)
INFO - root - 2017-12-06 10:05:13.370403: step 8080, loss = 2.09, batch loss = 2.04 (17.4 examples/sec; 0.459 sec/batch; 41h:19m:24s remains)
INFO - root - 2017-12-06 10:05:17.743399: step 8090, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.429 sec/batch; 38h:37m:34s remains)
INFO - root - 2017-12-06 10:05:22.149046: step 8100, loss = 2.09, batch loss = 2.03 (18.1 examples/sec; 0.442 sec/batch; 39h:49m:28s remains)
2017-12-06 10:05:22.611565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1940441 -4.2304826 -4.2534938 -4.2607431 -4.2609892 -4.2609024 -4.2601986 -4.2451363 -4.2333889 -4.2362719 -4.2412667 -4.2444911 -4.2509384 -4.2503753 -4.2436075][-4.1800542 -4.2161098 -4.2355237 -4.2454553 -4.2514453 -4.257113 -4.2626438 -4.2474818 -4.2387795 -4.247592 -4.2529659 -4.2513885 -4.2517033 -4.2491527 -4.2412724][-4.162178 -4.1957049 -4.2145286 -4.224936 -4.2331977 -4.2452621 -4.2569585 -4.2455835 -4.2398086 -4.2567844 -4.2645941 -4.2611041 -4.2593846 -4.2562366 -4.2470508][-4.1655011 -4.1914639 -4.2111664 -4.2195096 -4.2265615 -4.2363057 -4.2423515 -4.2279034 -4.229125 -4.2571807 -4.2747936 -4.2780337 -4.2771959 -4.2737236 -4.2643976][-4.17463 -4.198318 -4.2192197 -4.2248111 -4.230021 -4.2265329 -4.210186 -4.1828871 -4.1908364 -4.235939 -4.2744169 -4.2942433 -4.2964983 -4.2892575 -4.2743754][-4.1636448 -4.1885648 -4.2146406 -4.2265544 -4.2244563 -4.1979251 -4.1470318 -4.1040077 -4.1222529 -4.1929469 -4.2553177 -4.2908196 -4.2983894 -4.2876387 -4.269136][-4.1369996 -4.1690955 -4.2102523 -4.2312255 -4.215734 -4.1573009 -4.0588436 -3.985203 -4.0177197 -4.1209364 -4.2103109 -4.2592025 -4.2730813 -4.2630892 -4.2461843][-4.0935974 -4.1425529 -4.2094512 -4.2400289 -4.2144265 -4.1281815 -3.9846387 -3.8637154 -3.903435 -4.048974 -4.1669412 -4.2243981 -4.2379622 -4.2199945 -4.1963897][-4.0342116 -4.1043296 -4.1987405 -4.2408953 -4.2180514 -4.131897 -3.9853065 -3.8600137 -3.9080255 -4.0533414 -4.159647 -4.2042284 -4.1998115 -4.1603527 -4.1208792][-3.99479 -4.0733004 -4.179316 -4.2293043 -4.2213545 -4.1669168 -4.0692024 -3.9931607 -4.03541 -4.1327991 -4.1932116 -4.2061005 -4.1738129 -4.1065669 -4.0496411][-4.0071821 -4.0770841 -4.1735125 -4.2219853 -4.2233043 -4.1983862 -4.1471524 -4.1102285 -4.1478944 -4.2092328 -4.2379603 -4.2278547 -4.1743922 -4.0899129 -4.0243855][-4.0438223 -4.1040826 -4.1798735 -4.22237 -4.2287393 -4.2246566 -4.2023487 -4.1893725 -4.2233944 -4.2654066 -4.2774725 -4.2542014 -4.1959543 -4.1162162 -4.0582962][-4.0834627 -4.1428456 -4.1983819 -4.2276087 -4.2320247 -4.2382059 -4.234643 -4.234024 -4.2617159 -4.2926974 -4.2996259 -4.2775106 -4.2302756 -4.1689072 -4.1236587][-4.1210117 -4.1774521 -4.21 -4.2214737 -4.2156577 -4.2188287 -4.2255821 -4.240859 -4.2717304 -4.299757 -4.3035722 -4.2873325 -4.2565222 -4.2206726 -4.191236][-4.1413093 -4.1856856 -4.2023044 -4.2024145 -4.1894774 -4.1915112 -4.1998663 -4.22242 -4.2568493 -4.2879405 -4.2959847 -4.287219 -4.2729816 -4.2589359 -4.2451262]]...]
INFO - root - 2017-12-06 10:05:26.906111: step 8110, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.419 sec/batch; 37h:46m:21s remains)
INFO - root - 2017-12-06 10:05:30.894953: step 8120, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 38h:54m:18s remains)
INFO - root - 2017-12-06 10:05:35.219303: step 8130, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:00m:54s remains)
INFO - root - 2017-12-06 10:05:39.490357: step 8140, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 39h:52m:05s remains)
INFO - root - 2017-12-06 10:05:43.807926: step 8150, loss = 2.10, batch loss = 2.04 (19.1 examples/sec; 0.419 sec/batch; 37h:43m:05s remains)
INFO - root - 2017-12-06 10:05:48.068329: step 8160, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:50m:37s remains)
INFO - root - 2017-12-06 10:05:52.317939: step 8170, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.433 sec/batch; 38h:59m:08s remains)
INFO - root - 2017-12-06 10:05:56.642356: step 8180, loss = 2.10, batch loss = 2.04 (17.7 examples/sec; 0.451 sec/batch; 40h:37m:54s remains)
INFO - root - 2017-12-06 10:06:01.134491: step 8190, loss = 2.07, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 39h:56m:12s remains)
INFO - root - 2017-12-06 10:06:05.612257: step 8200, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:16m:32s remains)
2017-12-06 10:06:06.074036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165603 -4.3267355 -4.3245277 -4.3059773 -4.2771344 -4.2414165 -4.2133594 -4.2266569 -4.2569194 -4.25922 -4.2523746 -4.2568908 -4.2752552 -4.2995267 -4.3251257][-4.3177319 -4.3268261 -4.3210368 -4.2930541 -4.252131 -4.2111826 -4.18163 -4.1991153 -4.234376 -4.2413492 -4.2372823 -4.243834 -4.2633009 -4.2896371 -4.3155904][-4.3175111 -4.3214 -4.309679 -4.2738438 -4.2240291 -4.1795759 -4.152874 -4.1807165 -4.2266731 -4.2454238 -4.2500992 -4.259686 -4.2781477 -4.3001266 -4.3191733][-4.3140593 -4.3120327 -4.2942581 -4.2545166 -4.1987948 -4.150671 -4.1270881 -4.1676936 -4.2262096 -4.2580686 -4.2737775 -4.2885013 -4.3057313 -4.3212771 -4.3307962][-4.3100562 -4.3061523 -4.2864523 -4.244555 -4.1851768 -4.1335449 -4.1079521 -4.1492605 -4.2116442 -4.2533436 -4.281034 -4.3024611 -4.320116 -4.332087 -4.3368392][-4.3069086 -4.3039784 -4.2861018 -4.2429361 -4.1804748 -4.1223536 -4.0847111 -4.1133924 -4.1722622 -4.2219615 -4.2622876 -4.2918935 -4.3137641 -4.3262649 -4.3301735][-4.3077168 -4.3065896 -4.2893691 -4.2427645 -4.1743946 -4.1000943 -4.0332794 -4.0345902 -4.085535 -4.1437531 -4.2015018 -4.2474289 -4.2824879 -4.3046708 -4.3130779][-4.3125811 -4.3143115 -4.2959847 -4.2455454 -4.1676531 -4.0679054 -3.9598894 -3.9293656 -3.9765968 -4.048368 -4.1260858 -4.1912394 -4.2408686 -4.2723408 -4.2864995][-4.315321 -4.3194175 -4.2992187 -4.2454829 -4.1594887 -4.0372891 -3.8985457 -3.8550794 -3.911283 -3.9944966 -4.0815148 -4.1543188 -4.2087221 -4.2408209 -4.2555561][-4.3110676 -4.317349 -4.299088 -4.2461238 -4.1639442 -4.0457244 -3.9154136 -3.8898685 -3.9593475 -4.0403242 -4.1152306 -4.1738768 -4.2139192 -4.233089 -4.239243][-4.3050928 -4.3122635 -4.2988858 -4.2582979 -4.1974626 -4.10665 -4.0103669 -4.0078564 -4.0755687 -4.1407471 -4.1936431 -4.2276745 -4.2448511 -4.2451568 -4.2397861][-4.3021173 -4.3097863 -4.3027577 -4.2767878 -4.240509 -4.1802387 -4.1168504 -4.1272731 -4.1831946 -4.2286787 -4.2598095 -4.2742739 -4.2761478 -4.2652159 -4.2530727][-4.3033118 -4.3107457 -4.3071718 -4.2917595 -4.2725987 -4.23509 -4.1969609 -4.210145 -4.2508483 -4.2785387 -4.2921991 -4.2953877 -4.2902837 -4.2765532 -4.2645507][-4.3071537 -4.313972 -4.3125772 -4.3018808 -4.2907677 -4.2692032 -4.248426 -4.2623696 -4.2936554 -4.3074617 -4.3076224 -4.3019772 -4.2925034 -4.2792306 -4.2696528][-4.3113995 -4.3169651 -4.31611 -4.3018 -4.2879925 -4.2717285 -4.2584286 -4.2737913 -4.3059449 -4.3181667 -4.3160238 -4.3075004 -4.2957387 -4.2821717 -4.2734408]]...]
INFO - root - 2017-12-06 10:06:10.478319: step 8210, loss = 2.07, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:46m:30s remains)
INFO - root - 2017-12-06 10:06:14.553662: step 8220, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.422 sec/batch; 38h:02m:06s remains)
INFO - root - 2017-12-06 10:06:19.006851: step 8230, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:35m:35s remains)
INFO - root - 2017-12-06 10:06:23.332441: step 8240, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 38h:53m:43s remains)
INFO - root - 2017-12-06 10:06:27.668503: step 8250, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 37h:59m:00s remains)
INFO - root - 2017-12-06 10:06:32.023009: step 8260, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.420 sec/batch; 37h:47m:53s remains)
INFO - root - 2017-12-06 10:06:36.360168: step 8270, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 39h:22m:45s remains)
INFO - root - 2017-12-06 10:06:40.746413: step 8280, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.424 sec/batch; 38h:13m:45s remains)
INFO - root - 2017-12-06 10:06:45.104693: step 8290, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 40h:51m:07s remains)
INFO - root - 2017-12-06 10:06:49.435821: step 8300, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 37h:59m:02s remains)
2017-12-06 10:06:49.892523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3565559 -4.3598852 -4.3610973 -4.3589215 -4.3554559 -4.3537397 -4.352767 -4.350059 -4.3469677 -4.3483663 -4.3541532 -4.3600974 -4.3624825 -4.3601904 -4.3546238][-4.3494968 -4.35218 -4.3525453 -4.3489423 -4.344255 -4.3419595 -4.3383889 -4.3305736 -4.323782 -4.3252525 -4.3320827 -4.3365917 -4.3364863 -4.3331909 -4.32614][-4.3318396 -4.3275561 -4.3227534 -4.3154454 -4.3086724 -4.3039117 -4.2926617 -4.274159 -4.2615743 -4.2629018 -4.2700782 -4.2734437 -4.2741842 -4.2752 -4.2742004][-4.2962956 -4.2789974 -4.2655053 -4.2531505 -4.2419944 -4.231298 -4.2102227 -4.1757174 -4.1530962 -4.1560688 -4.1683931 -4.175621 -4.1838803 -4.1954556 -4.20597][-4.2528257 -4.2209167 -4.1968656 -4.1800265 -4.1643119 -4.1480308 -4.1202259 -4.0690756 -4.0326815 -4.0439978 -4.0759854 -4.095314 -4.1128249 -4.1339712 -4.1529851][-4.2145281 -4.1698289 -4.1386418 -4.1216311 -4.106236 -4.0820947 -4.0451484 -3.9729283 -3.9153495 -3.9435904 -4.0089421 -4.0496135 -4.0777855 -4.1044855 -4.1268578][-4.2008591 -4.1534233 -4.1211152 -4.1044092 -4.083375 -4.0458913 -3.9984391 -3.911489 -3.8409469 -3.8922133 -3.9883232 -4.0463777 -4.0755434 -4.0989361 -4.1238079][-4.2153172 -4.1778054 -4.1491475 -4.1293659 -4.10062 -4.0579615 -4.0106883 -3.9372919 -3.8826027 -3.9400864 -4.0355968 -4.0913711 -4.110558 -4.1276374 -4.1509342][-4.2377839 -4.2105174 -4.184782 -4.1586194 -4.1263146 -4.0913563 -4.0573316 -4.0115581 -3.9820824 -4.0297771 -4.1053319 -4.1492844 -4.1624541 -4.1780739 -4.1969657][-4.2499528 -4.2275205 -4.2048492 -4.1799669 -4.1546688 -4.1325893 -4.1124911 -4.0888791 -4.0740023 -4.1035342 -4.1543641 -4.1890569 -4.2011805 -4.2174168 -4.2319231][-4.2592897 -4.2425032 -4.2278967 -4.21234 -4.1963105 -4.184834 -4.1748261 -4.1616473 -4.1523194 -4.164269 -4.1942582 -4.2222834 -4.2369833 -4.2540989 -4.2642827][-4.2825856 -4.2714219 -4.261744 -4.2533813 -4.2443013 -4.2404237 -4.2391849 -4.2346258 -4.2288136 -4.2278519 -4.2386775 -4.2538066 -4.2653031 -4.2814465 -4.2912564][-4.3026094 -4.2965922 -4.2930927 -4.2940416 -4.2969384 -4.3019695 -4.3073759 -4.3076754 -4.3005223 -4.2886043 -4.2817817 -4.2800288 -4.283947 -4.297308 -4.3105912][-4.3241339 -4.3214364 -4.3213959 -4.3262815 -4.3339496 -4.3430667 -4.3512936 -4.3534188 -4.346323 -4.3318048 -4.3192997 -4.3094544 -4.3089056 -4.3195171 -4.3330951][-4.3413763 -4.3397856 -4.3394985 -4.3433781 -4.3511968 -4.3604541 -4.3686247 -4.3724136 -4.3692651 -4.3610306 -4.3514919 -4.34129 -4.3377705 -4.3440776 -4.353488]]...]
INFO - root - 2017-12-06 10:06:54.179861: step 8310, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 38h:16m:28s remains)
INFO - root - 2017-12-06 10:06:58.237152: step 8320, loss = 2.06, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 38h:06m:06s remains)
INFO - root - 2017-12-06 10:07:02.548357: step 8330, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.434 sec/batch; 39h:02m:30s remains)
INFO - root - 2017-12-06 10:07:06.904710: step 8340, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:37m:25s remains)
INFO - root - 2017-12-06 10:07:11.162550: step 8350, loss = 2.12, batch loss = 2.06 (18.7 examples/sec; 0.428 sec/batch; 38h:30m:46s remains)
INFO - root - 2017-12-06 10:07:15.355618: step 8360, loss = 2.06, batch loss = 2.01 (19.4 examples/sec; 0.412 sec/batch; 37h:05m:41s remains)
INFO - root - 2017-12-06 10:07:19.644546: step 8370, loss = 2.08, batch loss = 2.03 (18.1 examples/sec; 0.441 sec/batch; 39h:42m:42s remains)
INFO - root - 2017-12-06 10:07:23.815507: step 8380, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.416 sec/batch; 37h:26m:36s remains)
INFO - root - 2017-12-06 10:07:28.040706: step 8390, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 38h:25m:11s remains)
INFO - root - 2017-12-06 10:07:32.354881: step 8400, loss = 2.08, batch loss = 2.03 (17.8 examples/sec; 0.450 sec/batch; 40h:29m:39s remains)
2017-12-06 10:07:32.874901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3396292 -4.3437591 -4.3466 -4.3330426 -4.2963562 -4.2398071 -4.1885657 -4.1646733 -4.1789017 -4.219183 -4.2669506 -4.2981086 -4.3084393 -4.2992678 -4.2817826][-4.3441758 -4.35196 -4.3607883 -4.3550272 -4.3282976 -4.2827849 -4.232091 -4.1961317 -4.1881719 -4.2069712 -4.2436452 -4.2772379 -4.2988739 -4.3018684 -4.2911682][-4.353457 -4.3646421 -4.3766518 -4.3743854 -4.3541365 -4.3174973 -4.2655139 -4.2145877 -4.1807818 -4.1751013 -4.2027025 -4.2428041 -4.2793918 -4.3009491 -4.30546][-4.35893 -4.3709254 -4.3826861 -4.3805289 -4.3616614 -4.3289275 -4.27456 -4.2095633 -4.1530333 -4.1246305 -4.145288 -4.1934271 -4.2473145 -4.2938371 -4.3205919][-4.3607826 -4.3715053 -4.380764 -4.3774409 -4.3590574 -4.3274045 -4.2719932 -4.1950445 -4.1188354 -4.0690336 -4.085927 -4.1422486 -4.2110953 -4.2779665 -4.3249407][-4.3604112 -4.3691092 -4.3737068 -4.3658423 -4.342989 -4.3057342 -4.2445135 -4.1542768 -4.058042 -3.9912636 -4.0153303 -4.0882349 -4.1738386 -4.25714 -4.3192182][-4.3592296 -4.3639803 -4.3629994 -4.3480086 -4.3138494 -4.2642579 -4.1917577 -4.0867019 -3.9688342 -3.8892412 -3.9313514 -4.0301204 -4.1356497 -4.2329211 -4.3082142][-4.357018 -4.3555384 -4.3461814 -4.3199034 -4.2721663 -4.2110434 -4.1314449 -4.0224633 -3.8969798 -3.8182449 -3.8773417 -3.9932485 -4.1088071 -4.2137904 -4.2969084][-4.3524556 -4.3436995 -4.3248892 -4.2871995 -4.2292 -4.1621132 -4.08489 -3.99017 -3.8863118 -3.8360043 -3.8982577 -4.0047569 -4.1143689 -4.2164783 -4.2979064][-4.3465776 -4.3325868 -4.3086987 -4.2673364 -4.2086344 -4.1431923 -4.0785632 -4.0108347 -3.9446082 -3.9290693 -3.9858072 -4.0706086 -4.162477 -4.2486477 -4.3152051][-4.3431768 -4.3279438 -4.3047223 -4.2688103 -4.2195363 -4.1644115 -4.1151633 -4.0733414 -4.0403676 -4.0477495 -4.0928087 -4.1563449 -4.228302 -4.294466 -4.3400035][-4.3409162 -4.3276095 -4.3102098 -4.2864747 -4.2532272 -4.213932 -4.1809144 -4.1600003 -4.1515932 -4.1691494 -4.2021966 -4.2468691 -4.2964206 -4.3393731 -4.36191][-4.3395677 -4.329999 -4.3212113 -4.3103023 -4.2933774 -4.2716293 -4.2549253 -4.248755 -4.2541294 -4.2738671 -4.2978253 -4.3261275 -4.3538766 -4.3725781 -4.3724957][-4.338109 -4.3314781 -4.3294382 -4.3281331 -4.32443 -4.3190942 -4.3164659 -4.3196731 -4.329421 -4.3450565 -4.3597126 -4.3736563 -4.3805437 -4.3790846 -4.3643384][-4.3366017 -4.33263 -4.3344884 -4.3386388 -4.3438015 -4.3486452 -4.3539882 -4.3606453 -4.3687019 -4.3770046 -4.3819051 -4.381557 -4.3717413 -4.3575397 -4.3347721]]...]
INFO - root - 2017-12-06 10:07:37.124765: step 8410, loss = 2.07, batch loss = 2.01 (19.7 examples/sec; 0.406 sec/batch; 36h:34m:55s remains)
INFO - root - 2017-12-06 10:07:41.091732: step 8420, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.451 sec/batch; 40h:37m:54s remains)
INFO - root - 2017-12-06 10:07:45.368796: step 8430, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.431 sec/batch; 38h:46m:37s remains)
INFO - root - 2017-12-06 10:07:49.770255: step 8440, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.465 sec/batch; 41h:54m:04s remains)
INFO - root - 2017-12-06 10:07:54.050213: step 8450, loss = 2.05, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:41m:03s remains)
INFO - root - 2017-12-06 10:07:58.376441: step 8460, loss = 2.10, batch loss = 2.04 (18.0 examples/sec; 0.445 sec/batch; 40h:04m:46s remains)
INFO - root - 2017-12-06 10:08:02.683051: step 8470, loss = 2.06, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 39h:26m:39s remains)
INFO - root - 2017-12-06 10:08:07.037048: step 8480, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 38h:55m:52s remains)
INFO - root - 2017-12-06 10:08:11.441676: step 8490, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 38h:09m:37s remains)
INFO - root - 2017-12-06 10:08:16.336669: step 8500, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 0.526 sec/batch; 47h:20m:50s remains)
2017-12-06 10:08:16.885565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856127 -4.1789823 -4.1685147 -4.1753912 -4.2034841 -4.2129416 -4.2121882 -4.2073526 -4.1817827 -4.1599011 -4.1624656 -4.1705952 -4.1717391 -4.1693969 -4.1671782][-4.2051144 -4.196485 -4.1822696 -4.1900644 -4.2198877 -4.2351637 -4.2402635 -4.2341208 -4.2022419 -4.1648607 -4.164979 -4.183619 -4.2010355 -4.2158866 -4.2189403][-4.2277842 -4.214035 -4.1952243 -4.1996284 -4.22809 -4.2451148 -4.2535739 -4.2510796 -4.2235851 -4.1825833 -4.1762815 -4.1957927 -4.2178955 -4.2397423 -4.2488537][-4.2412825 -4.22302 -4.1969481 -4.1903267 -4.2051673 -4.2200675 -4.22968 -4.2281032 -4.2147737 -4.1863456 -4.1818995 -4.19874 -4.2178016 -4.2368741 -4.2514219][-4.237505 -4.2176743 -4.1864157 -4.172246 -4.1775541 -4.1867824 -4.1886277 -4.1829157 -4.1761289 -4.1644769 -4.1736274 -4.1947908 -4.2119813 -4.2262421 -4.2386742][-4.2140894 -4.1918588 -4.1608157 -4.147316 -4.155252 -4.1612868 -4.1539888 -4.1406894 -4.1340909 -4.136837 -4.1582 -4.1844478 -4.2030535 -4.2178655 -4.2317386][-4.1710563 -4.145225 -4.1240282 -4.1237588 -4.1387892 -4.1448603 -4.13279 -4.1150866 -4.1124721 -4.125731 -4.1539488 -4.1863461 -4.2056441 -4.2187223 -4.2353826][-4.14523 -4.1170506 -4.10716 -4.1213408 -4.1416693 -4.1539512 -4.1458368 -4.1267037 -4.1201305 -4.1294422 -4.1540861 -4.1854391 -4.20068 -4.2071033 -4.2243042][-4.1418185 -4.1160693 -4.1164265 -4.1434216 -4.1701326 -4.186348 -4.1836357 -4.1638751 -4.1482043 -4.1394844 -4.1435232 -4.1642656 -4.175869 -4.1824908 -4.2014966][-4.1487837 -4.133822 -4.1513944 -4.1874757 -4.2128558 -4.2216744 -4.2146258 -4.1917028 -4.1639185 -4.1379442 -4.1245131 -4.1311755 -4.14432 -4.1570172 -4.1823549][-4.1596937 -4.1569023 -4.1880016 -4.2283564 -4.2464414 -4.2366347 -4.2209063 -4.2020769 -4.1693258 -4.1359158 -4.1109333 -4.1041374 -4.1216946 -4.1471167 -4.1787105][-4.1711783 -4.1717858 -4.1982856 -4.2340932 -4.2402034 -4.2152495 -4.2010846 -4.2015481 -4.1852436 -4.1633162 -4.1402607 -4.1265097 -4.13553 -4.1597028 -4.1896276][-4.1654186 -4.1644635 -4.183764 -4.2124872 -4.2115426 -4.1848092 -4.1810279 -4.2017326 -4.2087564 -4.2122 -4.198348 -4.1858978 -4.18646 -4.2001967 -4.2168531][-4.1308355 -4.1309819 -4.1523046 -4.1850123 -4.1918559 -4.1758056 -4.173749 -4.1951904 -4.2163792 -4.2362518 -4.2318683 -4.2227235 -4.2190423 -4.2257271 -4.2290692][-4.0936079 -4.0980611 -4.1250257 -4.1618295 -4.18031 -4.1707783 -4.1592731 -4.1700997 -4.1968727 -4.2246556 -4.2244205 -4.2202296 -4.2164035 -4.2185664 -4.2130809]]...]
INFO - root - 2017-12-06 10:08:21.989027: step 8510, loss = 2.11, batch loss = 2.06 (15.8 examples/sec; 0.506 sec/batch; 45h:30m:05s remains)
INFO - root - 2017-12-06 10:08:26.937698: step 8520, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 0.498 sec/batch; 44h:50m:00s remains)
INFO - root - 2017-12-06 10:08:31.782828: step 8530, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:19m:22s remains)
INFO - root - 2017-12-06 10:08:36.019702: step 8540, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.436 sec/batch; 39h:16m:00s remains)
INFO - root - 2017-12-06 10:08:40.267284: step 8550, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:17m:05s remains)
INFO - root - 2017-12-06 10:08:44.538541: step 8560, loss = 2.07, batch loss = 2.02 (18.4 examples/sec; 0.434 sec/batch; 39h:04m:33s remains)
INFO - root - 2017-12-06 10:08:48.838095: step 8570, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.416 sec/batch; 37h:28m:11s remains)
INFO - root - 2017-12-06 10:08:53.168569: step 8580, loss = 2.10, batch loss = 2.04 (17.0 examples/sec; 0.471 sec/batch; 42h:22m:11s remains)
INFO - root - 2017-12-06 10:08:57.531986: step 8590, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 39h:50m:52s remains)
INFO - root - 2017-12-06 10:09:01.851723: step 8600, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:31m:17s remains)
2017-12-06 10:09:02.359008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893929 -4.2860708 -4.2856278 -4.285769 -4.2852693 -4.2827978 -4.2788606 -4.2751975 -4.273067 -4.2725186 -4.2736979 -4.2752914 -4.2749043 -4.2702923 -4.2624884][-4.2859297 -4.2822022 -4.2824583 -4.2848959 -4.287364 -4.2856874 -4.2794323 -4.2724667 -4.2677274 -4.2665133 -4.2689295 -4.272243 -4.2724633 -4.2663345 -4.2562613][-4.2946444 -4.2887573 -4.2877688 -4.2917709 -4.2965546 -4.294416 -4.2857585 -4.2758904 -4.2711506 -4.2721119 -4.276413 -4.2805233 -4.2791281 -4.2708683 -4.259788][-4.2874713 -4.2792344 -4.2770677 -4.282414 -4.2880721 -4.2840433 -4.2705636 -4.257637 -4.2590442 -4.2700267 -4.2788882 -4.2820392 -4.277595 -4.2668128 -4.2573175][-4.2629228 -4.2538033 -4.2525659 -4.2593579 -4.2620935 -4.2497854 -4.2251959 -4.2036481 -4.2138586 -4.2414556 -4.2606778 -4.2675424 -4.26399 -4.2539978 -4.2485738][-4.2089396 -4.1987977 -4.20288 -4.2176738 -4.2214541 -4.19558 -4.1454258 -4.096664 -4.10559 -4.1559486 -4.1967788 -4.2177749 -4.2238107 -4.2213931 -4.22459][-4.1499152 -4.137167 -4.1420989 -4.1628256 -4.1716232 -4.1337767 -4.0511141 -3.96097 -3.9617796 -4.0383348 -4.1051803 -4.1460009 -4.1668873 -4.1749182 -4.1857638][-4.1327882 -4.116538 -4.11703 -4.135541 -4.143693 -4.1020994 -4.0123534 -3.9146743 -3.9106727 -3.9929869 -4.0674682 -4.1135254 -4.1378717 -4.147614 -4.1578851][-4.1422892 -4.1266122 -4.12657 -4.1445031 -4.1503429 -4.1159525 -4.0516062 -3.9881527 -3.9886608 -4.0476623 -4.1019974 -4.13772 -4.1520185 -4.1532412 -4.1552715][-4.1553774 -4.1427016 -4.1422949 -4.1588826 -4.1642442 -4.138978 -4.0989895 -4.0660992 -4.0717926 -4.1084814 -4.1427288 -4.1696305 -4.1771336 -4.1714797 -4.1639061][-4.1524453 -4.1426749 -4.1412559 -4.15434 -4.1630154 -4.1507883 -4.1331534 -4.1200576 -4.1281853 -4.1503057 -4.1757865 -4.199265 -4.2068911 -4.2011085 -4.1888175][-4.1643906 -4.1533947 -4.1499906 -4.1562853 -4.1675024 -4.1690497 -4.1671596 -4.1664958 -4.1752248 -4.1897569 -4.2122493 -4.232141 -4.2373171 -4.2305183 -4.2157068][-4.1957912 -4.1857157 -4.1797481 -4.1803455 -4.1911082 -4.1999955 -4.2044144 -4.2060728 -4.2106037 -4.2191415 -4.23756 -4.2514582 -4.2532668 -4.2481127 -4.2369089][-4.2365217 -4.2263803 -4.2209239 -4.2210827 -4.2300568 -4.236196 -4.2350616 -4.23238 -4.2319202 -4.2359166 -4.2476726 -4.2549548 -4.2547965 -4.2509003 -4.2422328][-4.2716112 -4.2638307 -4.258606 -4.2577186 -4.2637105 -4.2652593 -4.2580981 -4.2513218 -4.2454963 -4.2410507 -4.243413 -4.24478 -4.2432747 -4.2398214 -4.2334838]]...]
INFO - root - 2017-12-06 10:09:06.633630: step 8610, loss = 2.09, batch loss = 2.03 (20.2 examples/sec; 0.396 sec/batch; 35h:39m:58s remains)
INFO - root - 2017-12-06 10:09:10.735064: step 8620, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:13m:50s remains)
INFO - root - 2017-12-06 10:09:14.953833: step 8630, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 38h:38m:29s remains)
INFO - root - 2017-12-06 10:09:19.264819: step 8640, loss = 2.06, batch loss = 2.00 (21.1 examples/sec; 0.380 sec/batch; 34h:09m:55s remains)
INFO - root - 2017-12-06 10:09:23.561846: step 8650, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.429 sec/batch; 38h:33m:31s remains)
INFO - root - 2017-12-06 10:09:27.909387: step 8660, loss = 2.07, batch loss = 2.02 (17.8 examples/sec; 0.449 sec/batch; 40h:24m:09s remains)
INFO - root - 2017-12-06 10:09:32.226371: step 8670, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.443 sec/batch; 39h:53m:29s remains)
INFO - root - 2017-12-06 10:09:36.530360: step 8680, loss = 2.07, batch loss = 2.02 (19.4 examples/sec; 0.413 sec/batch; 37h:07m:20s remains)
INFO - root - 2017-12-06 10:09:40.892378: step 8690, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:13m:09s remains)
INFO - root - 2017-12-06 10:09:45.184883: step 8700, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.418 sec/batch; 37h:37m:58s remains)
2017-12-06 10:09:45.766448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3047695 -4.2978644 -4.2955389 -4.3011985 -4.3133945 -4.3261552 -4.3277483 -4.3236194 -4.3142071 -4.3038011 -4.2954826 -4.2956238 -4.3032117 -4.3108277 -4.3162584][-4.2938805 -4.285399 -4.2815528 -4.2829113 -4.2916923 -4.308105 -4.3100991 -4.3052464 -4.2950997 -4.2871752 -4.2831569 -4.28528 -4.2941771 -4.301815 -4.3061728][-4.2831807 -4.2724 -4.2648048 -4.2555237 -4.2555108 -4.2691236 -4.270133 -4.2624326 -4.2514849 -4.2485895 -4.2524896 -4.2552524 -4.2680855 -4.2826853 -4.2921109][-4.2650585 -4.2542114 -4.2437482 -4.2282405 -4.2212582 -4.230248 -4.2274022 -4.2120819 -4.1977816 -4.2008514 -4.2172332 -4.2256646 -4.2412071 -4.2610192 -4.2805233][-4.2484012 -4.24032 -4.2288108 -4.2158008 -4.2054486 -4.1988735 -4.1818008 -4.1485648 -4.1269073 -4.1430831 -4.1815119 -4.2084193 -4.2319503 -4.2561369 -4.2829123][-4.2377286 -4.2310176 -4.2178745 -4.2051687 -4.187531 -4.159936 -4.1155891 -4.057909 -4.0359564 -4.0736456 -4.1336131 -4.1795006 -4.2169218 -4.2498169 -4.2822018][-4.2305832 -4.2193189 -4.1994252 -4.180778 -4.1518111 -4.0963464 -4.01053 -3.9242356 -3.9211545 -3.9900188 -4.0664182 -4.1336584 -4.1880879 -4.2326941 -4.2715154][-4.2287474 -4.2099457 -4.1825132 -4.1571326 -4.1083012 -4.0119615 -3.8767769 -3.7722566 -3.8104773 -3.9197211 -4.0168529 -4.1002073 -4.1650963 -4.2181311 -4.2621412][-4.2313423 -4.2111497 -4.180758 -4.1483722 -4.0876479 -3.9779768 -3.827013 -3.7369823 -3.8170204 -3.9452689 -4.0391 -4.1123109 -4.1648011 -4.2139807 -4.259984][-4.2278914 -4.209671 -4.1828122 -4.1536331 -4.1003647 -4.0122676 -3.9004896 -3.8485627 -3.9326329 -4.0403962 -4.1103959 -4.1588078 -4.1908951 -4.2285314 -4.2676725][-4.2332287 -4.2166667 -4.1946893 -4.1721253 -4.1371059 -4.0884261 -4.0288129 -4.0083437 -4.0695477 -4.1366229 -4.1776748 -4.206346 -4.2266059 -4.2536592 -4.2839184][-4.24857 -4.2371755 -4.2201638 -4.2062287 -4.1912146 -4.17121 -4.1500187 -4.1468372 -4.1841965 -4.2166924 -4.2387505 -4.2589178 -4.2708325 -4.2885284 -4.3112135][-4.2662587 -4.2578783 -4.2462444 -4.2419109 -4.2433071 -4.2391615 -4.2399521 -4.2465849 -4.2657785 -4.27754 -4.289278 -4.3040166 -4.3089652 -4.3194456 -4.3362637][-4.2900987 -4.2819943 -4.2732873 -4.2774143 -4.2908697 -4.2968121 -4.3037744 -4.311708 -4.3185067 -4.3189182 -4.3223877 -4.3309264 -4.3313637 -4.33622 -4.3478222][-4.2999735 -4.2930136 -4.2870317 -4.2910514 -4.3074961 -4.32083 -4.3313069 -4.3355474 -4.3340087 -4.3302383 -4.3311329 -4.33293 -4.3303018 -4.3318682 -4.3412123]]...]
INFO - root - 2017-12-06 10:09:50.072749: step 8710, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:05m:20s remains)
INFO - root - 2017-12-06 10:09:54.168782: step 8720, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:26m:47s remains)
INFO - root - 2017-12-06 10:09:58.942248: step 8730, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 0.500 sec/batch; 44h:56m:24s remains)
INFO - root - 2017-12-06 10:10:03.972760: step 8740, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.505 sec/batch; 45h:24m:26s remains)
INFO - root - 2017-12-06 10:10:09.010473: step 8750, loss = 2.09, batch loss = 2.03 (16.3 examples/sec; 0.491 sec/batch; 44h:09m:30s remains)
INFO - root - 2017-12-06 10:10:14.145884: step 8760, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 0.533 sec/batch; 47h:56m:52s remains)
INFO - root - 2017-12-06 10:10:19.292110: step 8770, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 46h:23m:46s remains)
INFO - root - 2017-12-06 10:10:24.500896: step 8780, loss = 2.10, batch loss = 2.04 (15.3 examples/sec; 0.522 sec/batch; 46h:55m:03s remains)
INFO - root - 2017-12-06 10:10:29.868931: step 8790, loss = 2.05, batch loss = 1.99 (14.7 examples/sec; 0.545 sec/batch; 49h:00m:56s remains)
INFO - root - 2017-12-06 10:10:35.198374: step 8800, loss = 2.10, batch loss = 2.04 (14.5 examples/sec; 0.552 sec/batch; 49h:35m:23s remains)
2017-12-06 10:10:35.786534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2115369 -4.2172308 -4.2168064 -4.2163858 -4.204843 -4.1900177 -4.1855044 -4.2003322 -4.2248511 -4.2482667 -4.26905 -4.2793307 -4.2897048 -4.296639 -4.2889676][-4.2014771 -4.2175403 -4.2195683 -4.2169533 -4.2066417 -4.1936831 -4.1899757 -4.2021604 -4.2239394 -4.2464833 -4.263629 -4.2733431 -4.2839723 -4.2862682 -4.2727251][-4.2116404 -4.2326503 -4.2309151 -4.2168794 -4.1999388 -4.1850433 -4.1774368 -4.1816077 -4.2017746 -4.2283363 -4.2483191 -4.2615943 -4.271419 -4.2680321 -4.2494373][-4.2387419 -4.2528319 -4.2392111 -4.2086692 -4.1751456 -4.1462317 -4.126564 -4.1251698 -4.152926 -4.1949472 -4.2268538 -4.2484045 -4.2562876 -4.2491794 -4.2300568][-4.276504 -4.2777672 -4.2474947 -4.198772 -4.1424284 -4.0840344 -4.0401673 -4.0371823 -4.0876765 -4.1585994 -4.2080021 -4.2383747 -4.2466488 -4.2391024 -4.2191153][-4.3095088 -4.3001871 -4.25743 -4.2009106 -4.1249409 -4.0255995 -3.9368787 -3.9302835 -4.0180922 -4.1271873 -4.1961226 -4.233139 -4.2408304 -4.23244 -4.2137446][-4.3215351 -4.3044734 -4.2614112 -4.2081614 -4.1207037 -3.98304 -3.8433743 -3.834868 -3.9691052 -4.1132851 -4.1942983 -4.2317038 -4.2381382 -4.2324471 -4.2191978][-4.3169494 -4.3038645 -4.2723575 -4.2283959 -4.1410551 -3.9888437 -3.8301768 -3.8233144 -3.9769802 -4.1250892 -4.2043457 -4.2384977 -4.2446809 -4.2405162 -4.2301769][-4.2994246 -4.3013988 -4.2916422 -4.263927 -4.1896124 -4.0607929 -3.9331105 -3.9280684 -4.0459919 -4.1604095 -4.221581 -4.2463479 -4.2488418 -4.2465663 -4.240859][-4.2758007 -4.2920623 -4.3027544 -4.2978668 -4.2506623 -4.1643329 -4.0822186 -4.0728931 -4.1341224 -4.2018967 -4.241683 -4.2532005 -4.25011 -4.2510934 -4.2535868][-4.2518649 -4.2775164 -4.3079786 -4.3276534 -4.3099589 -4.2607174 -4.2081304 -4.1884694 -4.2073183 -4.2423248 -4.2667389 -4.2671428 -4.2579594 -4.2605252 -4.2672362][-4.2405076 -4.2710781 -4.3137784 -4.3490677 -4.3541508 -4.3289905 -4.288558 -4.2591276 -4.2573709 -4.2773328 -4.2930341 -4.2853646 -4.27237 -4.2707753 -4.2752337][-4.244956 -4.2752757 -4.3167734 -4.3545909 -4.3698707 -4.3574715 -4.3255639 -4.29434 -4.2855825 -4.2986641 -4.307723 -4.29608 -4.2802215 -4.2725768 -4.2716541][-4.2616949 -4.2850142 -4.3156137 -4.3453827 -4.3616652 -4.3550291 -4.3287148 -4.297895 -4.2847214 -4.2929077 -4.2991242 -4.2872829 -4.2685266 -4.2560611 -4.25422][-4.2795763 -4.2951021 -4.3137441 -4.3326468 -4.3435802 -4.3351054 -4.3089027 -4.2737937 -4.2558889 -4.2604208 -4.2672586 -4.2594147 -4.2389073 -4.2234397 -4.2209964]]...]
INFO - root - 2017-12-06 10:10:41.128692: step 8810, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 46h:26m:00s remains)
INFO - root - 2017-12-06 10:10:46.251017: step 8820, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 58h:16m:30s remains)
INFO - root - 2017-12-06 10:10:51.579547: step 8830, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 46h:24m:59s remains)
INFO - root - 2017-12-06 10:10:56.813619: step 8840, loss = 2.09, batch loss = 2.04 (15.2 examples/sec; 0.527 sec/batch; 47h:24m:45s remains)
INFO - root - 2017-12-06 10:11:02.042045: step 8850, loss = 2.10, batch loss = 2.04 (14.8 examples/sec; 0.539 sec/batch; 48h:27m:09s remains)
INFO - root - 2017-12-06 10:11:07.256444: step 8860, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 0.511 sec/batch; 45h:57m:19s remains)
INFO - root - 2017-12-06 10:11:12.506050: step 8870, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 0.509 sec/batch; 45h:45m:23s remains)
INFO - root - 2017-12-06 10:11:17.844374: step 8880, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.515 sec/batch; 46h:19m:55s remains)
INFO - root - 2017-12-06 10:11:23.145557: step 8890, loss = 2.09, batch loss = 2.04 (14.9 examples/sec; 0.536 sec/batch; 48h:13m:23s remains)
INFO - root - 2017-12-06 10:11:28.385924: step 8900, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 0.513 sec/batch; 46h:07m:15s remains)
2017-12-06 10:11:28.957185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1912003 -4.1843376 -4.1804333 -4.197269 -4.2166247 -4.2217507 -4.2270947 -4.2323623 -4.2338371 -4.2379918 -4.2369022 -4.2279868 -4.2053351 -4.1724648 -4.1556468][-4.1256928 -4.13412 -4.1455121 -4.1777592 -4.2114267 -4.2300568 -4.2402806 -4.2441878 -4.2432327 -4.2469416 -4.2432828 -4.22524 -4.1882153 -4.1406989 -4.1199708][-4.0576596 -4.0886207 -4.1297112 -4.18399 -4.22884 -4.2521935 -4.2548394 -4.2458334 -4.2335668 -4.2252288 -4.2119942 -4.1847944 -4.1399922 -4.0884175 -4.0702453][-3.9872766 -4.0465989 -4.1239586 -4.1972585 -4.2429256 -4.2552814 -4.2384181 -4.2102542 -4.1898556 -4.1715622 -4.1574211 -4.1345882 -4.0953145 -4.0547543 -4.0471668][-3.9755764 -4.0502882 -4.1439676 -4.2080369 -4.2328024 -4.2214823 -4.1759424 -4.1243806 -4.1028824 -4.0957818 -4.0967789 -4.0929832 -4.0786963 -4.06674 -4.0729618][-4.0592871 -4.1244893 -4.1957269 -4.2235441 -4.2095518 -4.1654806 -4.089715 -4.0137777 -4.0034251 -4.0325942 -4.064023 -4.0877771 -4.1031818 -4.116374 -4.1237679][-4.1296883 -4.1743422 -4.2089896 -4.1952395 -4.1414442 -4.06861 -3.966598 -3.8704922 -3.891659 -3.9767804 -4.0475044 -4.0965161 -4.1341381 -4.1523466 -4.1460381][-4.1554208 -4.1769 -4.1755252 -4.1206021 -4.0320969 -3.9341629 -3.815732 -3.7114806 -3.7842374 -3.9257667 -4.0242224 -4.0913224 -4.1403422 -4.1509871 -4.1281514][-4.1715817 -4.1717191 -4.1397018 -4.0577164 -3.9534914 -3.8575308 -3.7656403 -3.6984656 -3.7989414 -3.9502854 -4.0450559 -4.1061344 -4.1517582 -4.153245 -4.1227593][-4.1725492 -4.1617079 -4.1175876 -4.0342641 -3.9441087 -3.8779111 -3.8332491 -3.8123064 -3.8949919 -4.0110393 -4.0835071 -4.1280885 -4.1642218 -4.1627021 -4.1378584][-4.1677895 -4.1507778 -4.1044693 -4.0333171 -3.9652557 -3.9277685 -3.9187307 -3.9251497 -3.9843826 -4.063704 -4.1148634 -4.1433787 -4.1700177 -4.1697035 -4.1539359][-4.1846042 -4.1666946 -4.1247144 -4.0726194 -4.0297937 -4.0165281 -4.0278025 -4.045927 -4.0844593 -4.1330891 -4.1642585 -4.1777945 -4.192874 -4.1936741 -4.1856737][-4.2241926 -4.2074871 -4.1769276 -4.146584 -4.1258116 -4.1231418 -4.135457 -4.1507006 -4.1717958 -4.1984315 -4.2171464 -4.2248073 -4.2329788 -4.2362719 -4.2343893][-4.2605033 -4.2468853 -4.2285733 -4.2160296 -4.2096829 -4.2117763 -4.2208586 -4.232626 -4.2436495 -4.2558951 -4.2663994 -4.2697196 -4.2732968 -4.2758965 -4.2747536][-4.2863531 -4.2760873 -4.2655206 -4.2610316 -4.26052 -4.2632122 -4.2694335 -4.2780495 -4.2841492 -4.2884636 -4.291997 -4.2928767 -4.2941761 -4.2951088 -4.2938561]]...]
INFO - root - 2017-12-06 10:11:33.915084: step 8910, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 0.532 sec/batch; 47h:47m:45s remains)
INFO - root - 2017-12-06 10:11:39.321140: step 8920, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 0.550 sec/batch; 49h:25m:51s remains)
INFO - root - 2017-12-06 10:11:44.640007: step 8930, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 0.518 sec/batch; 46h:33m:06s remains)
INFO - root - 2017-12-06 10:11:49.747924: step 8940, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 0.519 sec/batch; 46h:38m:29s remains)
INFO - root - 2017-12-06 10:11:54.826282: step 8950, loss = 2.09, batch loss = 2.04 (16.4 examples/sec; 0.488 sec/batch; 43h:53m:27s remains)
INFO - root - 2017-12-06 10:11:59.917668: step 8960, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 0.512 sec/batch; 46h:01m:24s remains)
INFO - root - 2017-12-06 10:12:04.971709: step 8970, loss = 2.08, batch loss = 2.03 (15.8 examples/sec; 0.505 sec/batch; 45h:24m:31s remains)
INFO - root - 2017-12-06 10:12:10.102185: step 8980, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 0.540 sec/batch; 48h:32m:47s remains)
INFO - root - 2017-12-06 10:12:15.214831: step 8990, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.517 sec/batch; 46h:30m:03s remains)
INFO - root - 2017-12-06 10:12:20.442759: step 9000, loss = 2.10, batch loss = 2.04 (14.9 examples/sec; 0.538 sec/batch; 48h:20m:41s remains)
2017-12-06 10:12:20.943070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1599636 -4.16444 -4.1573896 -4.1384263 -4.1273093 -4.150599 -4.1904774 -4.2152562 -4.2274237 -4.2192326 -4.1961389 -4.1894031 -4.2010093 -4.20947 -4.2037983][-4.1921697 -4.19299 -4.1800976 -4.1617775 -4.1512432 -4.1680574 -4.2020845 -4.228981 -4.2450523 -4.23443 -4.2004905 -4.1802597 -4.1806684 -4.1831288 -4.1821213][-4.2048855 -4.1985245 -4.1851997 -4.1726484 -4.1678729 -4.1803837 -4.2061214 -4.2333541 -4.2546148 -4.2474232 -4.2084165 -4.1765809 -4.1714253 -4.1731 -4.1777787][-4.2032905 -4.1899333 -4.1751542 -4.1653738 -4.166399 -4.1777372 -4.1980062 -4.2273369 -4.2553978 -4.2544093 -4.2103124 -4.1653104 -4.1572504 -4.1655374 -4.1797695][-4.1995726 -4.1752267 -4.1545639 -4.1470003 -4.1476765 -4.1478457 -4.1563578 -4.1893115 -4.2301431 -4.2426209 -4.2023315 -4.1504426 -4.1403871 -4.1566381 -4.17967][-4.2062893 -4.1686196 -4.1371231 -4.1287584 -4.120214 -4.0996985 -4.0872684 -4.12555 -4.1889482 -4.2236657 -4.1977277 -4.1467209 -4.127883 -4.1425276 -4.1703959][-4.2058568 -4.1596041 -4.12546 -4.1135397 -4.0908327 -4.0397658 -3.988862 -4.0247884 -4.1169934 -4.1804204 -4.1795964 -4.1444039 -4.1225166 -4.1312485 -4.1601224][-4.2094359 -4.1661758 -4.1328611 -4.1130538 -4.0791411 -4.0068011 -3.9208372 -3.940721 -4.0502448 -4.1398878 -4.1682358 -4.1550655 -4.139389 -4.1478806 -4.176836][-4.2237272 -4.1827855 -4.1507926 -4.1267071 -4.1021132 -4.051096 -3.9816165 -3.984467 -4.0640092 -4.1470089 -4.1857171 -4.1876278 -4.17916 -4.1877451 -4.211318][-4.2422814 -4.204987 -4.1746278 -4.1522808 -4.1413951 -4.1227193 -4.0819397 -4.0718789 -4.1173072 -4.1840954 -4.224215 -4.2323222 -4.2264395 -4.2321849 -4.249712][-4.2642789 -4.2336144 -4.2060361 -4.1836238 -4.178278 -4.1801639 -4.1611066 -4.1473875 -4.1724052 -4.2247195 -4.2664165 -4.2794433 -4.2720408 -4.2709112 -4.2801242][-4.2854118 -4.2635288 -4.2423744 -4.2246127 -4.2226157 -4.2322984 -4.2252965 -4.2103243 -4.2216868 -4.2605214 -4.2985458 -4.3091054 -4.2935414 -4.2812419 -4.2812934][-4.3032527 -4.2902803 -4.2764959 -4.2659106 -4.2663684 -4.2753649 -4.2706027 -4.251544 -4.2544637 -4.2814093 -4.3082929 -4.3118382 -4.2928386 -4.2745476 -4.2686024][-4.3068967 -4.3023291 -4.2960596 -4.2929587 -4.2956262 -4.3010836 -4.2962732 -4.2770834 -4.2759213 -4.2962775 -4.3154488 -4.3171768 -4.2960982 -4.2696238 -4.2551923][-4.2936344 -4.2963586 -4.2988143 -4.3028274 -4.3090711 -4.3147449 -4.3120914 -4.2992029 -4.2973065 -4.30993 -4.3227262 -4.3223591 -4.299211 -4.2637186 -4.241612]]...]
INFO - root - 2017-12-06 10:12:25.883299: step 9010, loss = 2.09, batch loss = 2.03 (16.0 examples/sec; 0.500 sec/batch; 44h:54m:47s remains)
INFO - root - 2017-12-06 10:12:31.140888: step 9020, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 0.534 sec/batch; 47h:59m:30s remains)
INFO - root - 2017-12-06 10:12:36.453641: step 9030, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.501 sec/batch; 45h:01m:01s remains)
INFO - root - 2017-12-06 10:12:41.589412: step 9040, loss = 2.10, batch loss = 2.04 (15.7 examples/sec; 0.510 sec/batch; 45h:51m:53s remains)
INFO - root - 2017-12-06 10:12:46.747243: step 9050, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 0.513 sec/batch; 46h:03m:06s remains)
INFO - root - 2017-12-06 10:12:52.037991: step 9060, loss = 2.11, batch loss = 2.05 (14.7 examples/sec; 0.543 sec/batch; 48h:46m:54s remains)
INFO - root - 2017-12-06 10:12:57.363162: step 9070, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 0.532 sec/batch; 47h:48m:08s remains)
INFO - root - 2017-12-06 10:13:02.642807: step 9080, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 0.521 sec/batch; 46h:48m:01s remains)
INFO - root - 2017-12-06 10:13:07.683965: step 9090, loss = 2.09, batch loss = 2.03 (16.1 examples/sec; 0.497 sec/batch; 44h:36m:32s remains)
INFO - root - 2017-12-06 10:13:12.839883: step 9100, loss = 2.09, batch loss = 2.04 (15.0 examples/sec; 0.532 sec/batch; 47h:48m:48s remains)
2017-12-06 10:13:13.454365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3115153 -4.301528 -4.2739573 -4.2379603 -4.2174582 -4.2132277 -4.2206821 -4.2284684 -4.2353168 -4.2357054 -4.2393818 -4.2561693 -4.2754736 -4.2870445 -4.2772632][-4.3098907 -4.2977638 -4.2696705 -4.2363067 -4.2170339 -4.2117105 -4.2251196 -4.23832 -4.2454619 -4.2421803 -4.240263 -4.2520695 -4.2710366 -4.2845321 -4.2752676][-4.3101873 -4.2941804 -4.2620645 -4.2255616 -4.2019176 -4.1957116 -4.2129636 -4.234282 -4.2493043 -4.2520652 -4.2468014 -4.2502389 -4.2613869 -4.2726746 -4.2651753][-4.3124404 -4.2931886 -4.2579851 -4.2153597 -4.1840925 -4.1759834 -4.1919045 -4.2198839 -4.2448368 -4.2549143 -4.2522397 -4.2485085 -4.2489324 -4.2539597 -4.2484879][-4.3158092 -4.2945037 -4.2541943 -4.2041659 -4.1602459 -4.14248 -4.1523323 -4.1822586 -4.2194066 -4.243279 -4.2496939 -4.2453971 -4.2416778 -4.237781 -4.2321186][-4.3208241 -4.3010712 -4.258657 -4.1998892 -4.1385493 -4.0968575 -4.0868988 -4.1075177 -4.1579375 -4.2108474 -4.2413397 -4.2495441 -4.2457418 -4.2359133 -4.2282472][-4.3255596 -4.3112664 -4.2732272 -4.2109561 -4.1315241 -4.0560694 -4.0076294 -3.9996805 -4.0655026 -4.1564236 -4.2217131 -4.252583 -4.2576594 -4.2489891 -4.2412462][-4.3291054 -4.3204346 -4.291368 -4.2334309 -4.1493449 -4.0508351 -3.9557631 -3.9001322 -3.9703298 -4.0946088 -4.1916628 -4.247333 -4.2698708 -4.2705493 -4.2647295][-4.3330054 -4.3279529 -4.3068881 -4.2589297 -4.1849275 -4.09424 -3.9933717 -3.9220493 -3.9678152 -4.0738668 -4.16894 -4.2336645 -4.2712951 -4.2843022 -4.2828841][-4.3368454 -4.3336797 -4.31621 -4.2768388 -4.2195024 -4.1599226 -4.0945492 -4.0500174 -4.0659389 -4.1157417 -4.1705518 -4.2219763 -4.2633724 -4.2834058 -4.287199][-4.3383908 -4.3357177 -4.3186569 -4.281085 -4.2343645 -4.2021265 -4.171948 -4.1569066 -4.1630683 -4.1738768 -4.19194 -4.2237797 -4.2593145 -4.2793574 -4.2834349][-4.3373055 -4.3339639 -4.3164678 -4.2783084 -4.2369351 -4.2189479 -4.2073936 -4.2068858 -4.2120156 -4.213429 -4.2216134 -4.2401943 -4.2647748 -4.2754712 -4.2726536][-4.3348045 -4.3301044 -4.3092923 -4.26944 -4.231452 -4.2153511 -4.2077165 -4.20763 -4.2153273 -4.2255907 -4.2427797 -4.2586694 -4.2719469 -4.2699261 -4.2560883][-4.3328428 -4.3272543 -4.3034334 -4.2637534 -4.2286377 -4.2102194 -4.2003565 -4.202168 -4.220325 -4.2425613 -4.2675543 -4.2830629 -4.2880135 -4.2760696 -4.2521725][-4.3329535 -4.3289723 -4.3063726 -4.2717862 -4.24414 -4.2274637 -4.218605 -4.2266736 -4.2539568 -4.2785912 -4.2971873 -4.3061285 -4.3051772 -4.2906508 -4.2643]]...]
INFO - root - 2017-12-06 10:13:18.530746: step 9110, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 0.537 sec/batch; 48h:13m:49s remains)
INFO - root - 2017-12-06 10:13:23.866091: step 9120, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 43h:18m:18s remains)
INFO - root - 2017-12-06 10:13:29.100136: step 9130, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 0.519 sec/batch; 46h:35m:49s remains)
INFO - root - 2017-12-06 10:13:34.227450: step 9140, loss = 2.10, batch loss = 2.04 (16.1 examples/sec; 0.498 sec/batch; 44h:43m:33s remains)
INFO - root - 2017-12-06 10:13:39.528550: step 9150, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 0.517 sec/batch; 46h:26m:44s remains)
INFO - root - 2017-12-06 10:13:44.733073: step 9160, loss = 2.06, batch loss = 2.00 (15.7 examples/sec; 0.511 sec/batch; 45h:52m:21s remains)
INFO - root - 2017-12-06 10:13:50.050917: step 9170, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 0.528 sec/batch; 47h:24m:01s remains)
INFO - root - 2017-12-06 10:13:55.437321: step 9180, loss = 2.11, batch loss = 2.06 (14.8 examples/sec; 0.540 sec/batch; 48h:31m:06s remains)
INFO - root - 2017-12-06 10:14:00.637159: step 9190, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 0.531 sec/batch; 47h:43m:40s remains)
INFO - root - 2017-12-06 10:14:05.931569: step 9200, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 0.542 sec/batch; 48h:41m:53s remains)
2017-12-06 10:14:06.473699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2500467 -4.2621288 -4.2649746 -4.2612491 -4.2523007 -4.2439761 -4.2383218 -4.2158251 -4.1970191 -4.2015204 -4.224113 -4.25058 -4.278224 -4.2934504 -4.2941256][-4.25237 -4.2601094 -4.2630329 -4.2648268 -4.2657905 -4.2732835 -4.2820692 -4.2684283 -4.2508197 -4.2492514 -4.2614579 -4.274838 -4.2924013 -4.3026972 -4.3029203][-4.2448397 -4.2514763 -4.2518826 -4.2540541 -4.2591906 -4.274642 -4.2911148 -4.29224 -4.2888045 -4.29227 -4.300849 -4.3065691 -4.314219 -4.3133874 -4.3072338][-4.230433 -4.2386718 -4.23414 -4.2301769 -4.2273211 -4.23242 -4.2424831 -4.2568297 -4.2815266 -4.3057737 -4.3243089 -4.333559 -4.3370442 -4.3292212 -4.3165421][-4.1915731 -4.20502 -4.1997576 -4.1879339 -4.1696668 -4.1547904 -4.1493587 -4.1630249 -4.2132831 -4.2673326 -4.3062415 -4.3277779 -4.3363652 -4.3317552 -4.3214035][-4.141819 -4.1564388 -4.155766 -4.139214 -4.1044016 -4.0635638 -4.0334487 -4.0302863 -4.093307 -4.1789522 -4.2411122 -4.2816725 -4.3048406 -4.3130684 -4.308135][-4.1172228 -4.1287665 -4.1306667 -4.11082 -4.0579662 -3.98558 -3.9172857 -3.879751 -3.9329855 -4.04552 -4.1398721 -4.2033033 -4.2485285 -4.2755303 -4.277801][-4.1201544 -4.1306696 -4.1281748 -4.110436 -4.0529652 -3.95998 -3.8497505 -3.7562294 -3.767591 -3.8876483 -4.0169473 -4.110095 -4.1809211 -4.229115 -4.2442012][-4.1339126 -4.1420603 -4.1344018 -4.1217518 -4.082655 -4.0048671 -3.88747 -3.7620838 -3.7337365 -3.8236046 -3.952774 -4.0557623 -4.1382618 -4.1976242 -4.2250295][-4.1760731 -4.1751041 -4.1646061 -4.1584711 -4.1402526 -4.0908155 -4.0011973 -3.9005394 -3.8719153 -3.9267874 -4.0193377 -4.0967703 -4.1634936 -4.2162542 -4.2440052][-4.2368608 -4.2292562 -4.2197447 -4.2189493 -4.2139635 -4.1898227 -4.1346316 -4.0681415 -4.0490465 -4.0800476 -4.137538 -4.1875873 -4.2324882 -4.2725024 -4.2914553][-4.2879481 -4.2809715 -4.2755795 -4.2765489 -4.2789087 -4.2714558 -4.2449574 -4.2074413 -4.1925688 -4.2050819 -4.2370462 -4.2695851 -4.2958031 -4.31885 -4.3254032][-4.3128452 -4.3099513 -4.3088708 -4.3112555 -4.3153877 -4.3154707 -4.3057652 -4.289299 -4.2801237 -4.2845507 -4.300724 -4.3165264 -4.329236 -4.338367 -4.3369474][-4.3203897 -4.3193207 -4.3208108 -4.3239274 -4.3283091 -4.3308454 -4.3288264 -4.3230891 -4.3188891 -4.3206673 -4.3282118 -4.3341174 -4.3389144 -4.3410373 -4.3378577][-4.3242407 -4.3227777 -4.3240657 -4.3256564 -4.3275471 -4.3285537 -4.3281212 -4.32714 -4.3269677 -4.3287668 -4.3325262 -4.3350668 -4.3368306 -4.3380885 -4.3369131]]...]
INFO - root - 2017-12-06 10:14:11.376035: step 9210, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 0.515 sec/batch; 46h:17m:16s remains)
INFO - root - 2017-12-06 10:14:16.794551: step 9220, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 0.514 sec/batch; 46h:07m:08s remains)
INFO - root - 2017-12-06 10:14:22.157512: step 9230, loss = 2.10, batch loss = 2.05 (15.6 examples/sec; 0.513 sec/batch; 46h:02m:27s remains)
INFO - root - 2017-12-06 10:14:27.398587: step 9240, loss = 2.11, batch loss = 2.05 (14.3 examples/sec; 0.561 sec/batch; 50h:20m:29s remains)
INFO - root - 2017-12-06 10:14:31.728627: step 9250, loss = 2.08, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 37h:11m:15s remains)
INFO - root - 2017-12-06 10:14:35.998134: step 9260, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:30m:50s remains)
INFO - root - 2017-12-06 10:14:40.499207: step 9270, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 37h:39m:05s remains)
INFO - root - 2017-12-06 10:14:44.932131: step 9280, loss = 2.11, batch loss = 2.05 (18.5 examples/sec; 0.434 sec/batch; 38h:55m:43s remains)
INFO - root - 2017-12-06 10:14:49.290331: step 9290, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:13m:05s remains)
INFO - root - 2017-12-06 10:14:53.596233: step 9300, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:29m:49s remains)
2017-12-06 10:14:54.140679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367382 -4.2351604 -4.2297206 -4.2155876 -4.1919742 -4.1755543 -4.1704783 -4.1783862 -4.2095394 -4.2481055 -4.2714434 -4.2697783 -4.2476196 -4.2128353 -4.1839581][-4.2294264 -4.2207737 -4.2092762 -4.1882272 -4.1558471 -4.1364017 -4.1369362 -4.1528468 -4.1900687 -4.235436 -4.26261 -4.2629395 -4.2449837 -4.2129903 -4.1863914][-4.2054749 -4.1894279 -4.1727648 -4.1479359 -4.1115646 -4.0941067 -4.1032453 -4.1271939 -4.1664519 -4.2138329 -4.2412434 -4.2422471 -4.2281876 -4.2005019 -4.1798358][-4.1783242 -4.1569967 -4.137742 -4.1139569 -4.0808182 -4.0700817 -4.0871663 -4.1145396 -4.1487761 -4.1908031 -4.2152824 -4.216763 -4.2045293 -4.1818218 -4.169385][-4.1636496 -4.143126 -4.1247826 -4.1043749 -4.0791488 -4.075829 -4.09493 -4.11728 -4.1427612 -4.177351 -4.1995993 -4.2024584 -4.1951427 -4.1784277 -4.1700573][-4.1514087 -4.1363974 -4.121151 -4.1033888 -4.0842113 -4.0808229 -4.0933189 -4.1065292 -4.1252389 -4.15732 -4.1793108 -4.1851177 -4.184463 -4.1740103 -4.1687455][-4.1254854 -4.115169 -4.0971041 -4.07683 -4.0597363 -4.0549703 -4.0635614 -4.0743055 -4.0948477 -4.13139 -4.1575823 -4.168931 -4.1735663 -4.16754 -4.1642284][-4.0889664 -4.0856948 -4.0685625 -4.0438976 -4.0249629 -4.0203772 -4.0320454 -4.0485744 -4.0771537 -4.1178913 -4.1465168 -4.160645 -4.1657524 -4.1613493 -4.1606245][-4.0655208 -4.0693159 -4.0590582 -4.0362854 -4.0155897 -4.0086708 -4.01982 -4.0426083 -4.0798779 -4.1224771 -4.1501732 -4.1633987 -4.1659951 -4.1613817 -4.1625824][-4.074687 -4.0847063 -4.0833049 -4.067831 -4.0456462 -4.0349565 -4.0416694 -4.0649 -4.1062012 -4.1488314 -4.1761642 -4.1881776 -4.1865063 -4.1773682 -4.1745195][-4.0882282 -4.0993404 -4.1009789 -4.0891027 -4.0657873 -4.0508575 -4.0553732 -4.0790663 -4.1218834 -4.1652212 -4.1956887 -4.2116375 -4.2093725 -4.1954808 -4.1880441][-4.0908093 -4.1037455 -4.1070962 -4.0978241 -4.0768723 -4.0617423 -4.0660496 -4.0874968 -4.1258383 -4.1685219 -4.2029004 -4.2238216 -4.22361 -4.2081165 -4.1968551][-4.0828114 -4.0945725 -4.0976496 -4.0915465 -4.0765557 -4.0658774 -4.0669241 -4.0790505 -4.1099586 -4.1521921 -4.18928 -4.2139187 -4.2186742 -4.206491 -4.1956077][-4.0768824 -4.0843062 -4.0826411 -4.0739703 -4.0611691 -4.0526371 -4.0479503 -4.0485892 -4.0717082 -4.1120477 -4.1496992 -4.1797104 -4.1928778 -4.1883535 -4.1833334][-4.085413 -4.0891261 -4.084374 -4.0733137 -4.061173 -4.0549574 -4.0503564 -4.0476274 -4.0663371 -4.102633 -4.1364207 -4.1653771 -4.1796594 -4.1770239 -4.1727905]]...]
INFO - root - 2017-12-06 10:14:58.247583: step 9310, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.430 sec/batch; 38h:36m:59s remains)
INFO - root - 2017-12-06 10:15:02.655450: step 9320, loss = 2.07, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:18m:53s remains)
INFO - root - 2017-12-06 10:15:06.980647: step 9330, loss = 2.09, batch loss = 2.04 (18.7 examples/sec; 0.428 sec/batch; 38h:26m:22s remains)
INFO - root - 2017-12-06 10:15:11.371801: step 9340, loss = 2.10, batch loss = 2.04 (17.7 examples/sec; 0.452 sec/batch; 40h:33m:57s remains)
INFO - root - 2017-12-06 10:15:15.643989: step 9350, loss = 2.07, batch loss = 2.01 (21.5 examples/sec; 0.371 sec/batch; 33h:20m:15s remains)
INFO - root - 2017-12-06 10:15:19.969265: step 9360, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 39h:43m:15s remains)
INFO - root - 2017-12-06 10:15:24.303155: step 9370, loss = 2.11, batch loss = 2.05 (19.0 examples/sec; 0.421 sec/batch; 37h:48m:07s remains)
INFO - root - 2017-12-06 10:15:28.654054: step 9380, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:11m:17s remains)
INFO - root - 2017-12-06 10:15:32.982012: step 9390, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 37h:58m:48s remains)
INFO - root - 2017-12-06 10:15:37.403788: step 9400, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 39h:54m:43s remains)
2017-12-06 10:15:37.944529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2125196 -4.212975 -4.20828 -4.2018228 -4.204587 -4.2172337 -4.2293968 -4.2295933 -4.2158995 -4.1667047 -4.1201096 -4.1282244 -4.1597018 -4.1922307 -4.2235079][-4.2221651 -4.2290435 -4.2344484 -4.2380977 -4.2419939 -4.2553377 -4.2615347 -4.2562261 -4.2406774 -4.1840272 -4.13559 -4.1458225 -4.1799622 -4.21173 -4.2366533][-4.2133689 -4.2248449 -4.2432942 -4.259491 -4.2661409 -4.271924 -4.2733264 -4.268229 -4.253612 -4.1970258 -4.1506524 -4.1623082 -4.1975965 -4.2279177 -4.2477126][-4.227314 -4.2299275 -4.2563181 -4.2806492 -4.2816043 -4.2754269 -4.2725539 -4.2679229 -4.2530074 -4.1954851 -4.1476874 -4.1582322 -4.1942973 -4.2218542 -4.2392955][-4.2523379 -4.2401471 -4.2585688 -4.2771292 -4.2705607 -4.2596912 -4.2579184 -4.2521534 -4.2377019 -4.1849675 -4.1386352 -4.1558409 -4.1958866 -4.2176447 -4.2308149][-4.2636 -4.2444234 -4.2480755 -4.2461033 -4.2362318 -4.231163 -4.2277055 -4.2140722 -4.1973443 -4.153091 -4.1116376 -4.1389809 -4.1852541 -4.2045856 -4.2154455][-4.2470088 -4.2185082 -4.1986752 -4.1672988 -4.1454329 -4.1445241 -4.1411605 -4.1208324 -4.1045065 -4.0737872 -4.0441475 -4.0841279 -4.1353564 -4.1559258 -4.17225][-4.1940885 -4.1513109 -4.105659 -4.036634 -3.9909539 -3.989018 -3.9855909 -3.9591174 -3.9430144 -3.9250402 -3.917064 -3.9851415 -4.0610285 -4.0966668 -4.129035][-4.1371827 -4.0878763 -4.0337334 -3.9471097 -3.8832436 -3.8760777 -3.8728504 -3.84352 -3.825953 -3.82058 -3.8286207 -3.9139261 -4.0067225 -4.0569897 -4.1057863][-4.0852513 -4.0491161 -4.0122724 -3.9399953 -3.8829918 -3.8792439 -3.88679 -3.8592718 -3.8357115 -3.843236 -3.863857 -3.9404101 -4.024291 -4.0746865 -4.12586][-4.0608025 -4.0436416 -4.0286646 -3.9797871 -3.9352005 -3.9311197 -3.9403813 -3.9103551 -3.8876173 -3.9129484 -3.9542921 -4.0215769 -4.092279 -4.140337 -4.1862812][-4.0957532 -4.0904045 -4.0860996 -4.06115 -4.0331545 -4.0277376 -4.0331321 -4.0083218 -3.9961946 -4.0242643 -4.0679455 -4.1237512 -4.1811113 -4.2214656 -4.2564797][-4.1604462 -4.1651845 -4.1674504 -4.1615157 -4.1524849 -4.1467605 -4.1427484 -4.12263 -4.1144986 -4.1277494 -4.154614 -4.19856 -4.2450318 -4.2786345 -4.30457][-4.2209673 -4.2280259 -4.2355533 -4.2400408 -4.2414494 -4.2367587 -4.2302084 -4.215827 -4.2049766 -4.1974497 -4.2028294 -4.2368517 -4.2752428 -4.3036823 -4.3233986][-4.26127 -4.2634211 -4.2688794 -4.2741685 -4.2798119 -4.2809005 -4.2779403 -4.2679915 -4.2516165 -4.2239943 -4.2092228 -4.2373514 -4.271234 -4.2986808 -4.3190861]]...]
INFO - root - 2017-12-06 10:15:42.071951: step 9410, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 38h:25m:12s remains)
INFO - root - 2017-12-06 10:15:46.574586: step 9420, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 39h:15m:11s remains)
INFO - root - 2017-12-06 10:15:50.971182: step 9430, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.427 sec/batch; 38h:17m:15s remains)
INFO - root - 2017-12-06 10:15:55.373891: step 9440, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 38h:24m:47s remains)
INFO - root - 2017-12-06 10:15:59.768444: step 9450, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 39h:08m:39s remains)
INFO - root - 2017-12-06 10:16:04.133297: step 9460, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.440 sec/batch; 39h:26m:18s remains)
INFO - root - 2017-12-06 10:16:08.387916: step 9470, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 37h:59m:08s remains)
INFO - root - 2017-12-06 10:16:12.755343: step 9480, loss = 2.10, batch loss = 2.04 (19.5 examples/sec; 0.410 sec/batch; 36h:45m:11s remains)
INFO - root - 2017-12-06 10:16:17.017677: step 9490, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 38h:45m:07s remains)
INFO - root - 2017-12-06 10:16:21.457463: step 9500, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:22m:53s remains)
2017-12-06 10:16:21.949637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1849604 -4.2194352 -4.2477069 -4.242157 -4.195488 -4.1173348 -4.0691037 -4.0719995 -4.133544 -4.2211838 -4.2777839 -4.2984838 -4.303062 -4.3061924 -4.3011789][-4.201406 -4.2333922 -4.2580547 -4.2542868 -4.2059083 -4.1267996 -4.0767632 -4.0643129 -4.10786 -4.1821327 -4.2418003 -4.2780805 -4.3010445 -4.3135052 -4.3154569][-4.2209134 -4.2422223 -4.2571335 -4.2512507 -4.2059097 -4.1327572 -4.0828352 -4.0572581 -4.0844674 -4.146399 -4.2059937 -4.2568936 -4.2953582 -4.3157907 -4.3203034][-4.2314215 -4.2430959 -4.2449713 -4.2284021 -4.1846156 -4.1264954 -4.0823851 -4.0605006 -4.082818 -4.1328807 -4.1834745 -4.23709 -4.2812061 -4.3033276 -4.30938][-4.2156868 -4.2229519 -4.2121964 -4.1798925 -4.134635 -4.0910625 -4.0629444 -4.05363 -4.0757957 -4.1199865 -4.1609669 -4.2096767 -4.2517424 -4.27403 -4.2848892][-4.1832361 -4.1882634 -4.1618237 -4.1103625 -4.0622115 -4.032196 -4.0224152 -4.0187249 -4.035954 -4.0811133 -4.1251097 -4.1740618 -4.211585 -4.2330036 -4.2534318][-4.1586819 -4.1635284 -4.1292686 -4.0725679 -4.0274024 -4.0115952 -4.0159645 -4.0156269 -4.0252433 -4.0580392 -4.1021347 -4.1568589 -4.1959543 -4.2166395 -4.23778][-4.1664596 -4.1756978 -4.1529098 -4.1051064 -4.0569534 -4.0409389 -4.0533271 -4.0620937 -4.070528 -4.0923247 -4.1323175 -4.1842508 -4.2192655 -4.2313194 -4.2426109][-4.1949859 -4.2059259 -4.196754 -4.1677217 -4.1204877 -4.0970902 -4.1079745 -4.1166315 -4.1250424 -4.1407108 -4.1756358 -4.2235074 -4.2539382 -4.2601657 -4.2605348][-4.2287774 -4.2408342 -4.2458653 -4.23583 -4.2025971 -4.1843495 -4.1946039 -4.1973405 -4.1977916 -4.2017779 -4.2262893 -4.2648082 -4.2885036 -4.2900023 -4.2840157][-4.2502379 -4.2641029 -4.2760382 -4.2755895 -4.256609 -4.2480483 -4.259706 -4.2655067 -4.2641659 -4.2621489 -4.2729664 -4.2944474 -4.308517 -4.3065929 -4.2990766][-4.261322 -4.2738304 -4.2859974 -4.2880859 -4.2768946 -4.2746859 -4.2881 -4.297636 -4.2965136 -4.2947254 -4.2992964 -4.3064008 -4.3091416 -4.3046117 -4.29886][-4.2677755 -4.2752109 -4.2848749 -4.2867 -4.2770834 -4.2751923 -4.2874269 -4.2963538 -4.2961869 -4.2950788 -4.2999721 -4.3027792 -4.3018203 -4.2983756 -4.2953434][-4.2783279 -4.280417 -4.2859125 -4.28497 -4.2752938 -4.2714543 -4.2794743 -4.286859 -4.2884078 -4.288393 -4.2909288 -4.2932 -4.2934651 -4.29322 -4.2924447][-4.2851949 -4.2842493 -4.2862687 -4.28602 -4.2812805 -4.2783875 -4.2814026 -4.2855134 -4.287663 -4.2883158 -4.2894311 -4.2911091 -4.2918725 -4.2925954 -4.29309]]...]
INFO - root - 2017-12-06 10:16:26.022299: step 9510, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:36m:56s remains)
INFO - root - 2017-12-06 10:16:30.486632: step 9520, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 38h:03m:31s remains)
INFO - root - 2017-12-06 10:16:34.869085: step 9530, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 39h:22m:28s remains)
INFO - root - 2017-12-06 10:16:39.251459: step 9540, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:00m:53s remains)
INFO - root - 2017-12-06 10:16:43.662826: step 9550, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:21m:15s remains)
INFO - root - 2017-12-06 10:16:47.994151: step 9560, loss = 2.11, batch loss = 2.05 (17.7 examples/sec; 0.453 sec/batch; 40h:36m:40s remains)
INFO - root - 2017-12-06 10:16:52.396527: step 9570, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.453 sec/batch; 40h:36m:21s remains)
INFO - root - 2017-12-06 10:16:56.823364: step 9580, loss = 2.05, batch loss = 1.99 (18.2 examples/sec; 0.439 sec/batch; 39h:23m:43s remains)
INFO - root - 2017-12-06 10:17:01.231617: step 9590, loss = 2.09, batch loss = 2.03 (17.3 examples/sec; 0.462 sec/batch; 41h:24m:58s remains)
INFO - root - 2017-12-06 10:17:05.565573: step 9600, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:08m:40s remains)
2017-12-06 10:17:06.141076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3175125 -4.3167715 -4.3102064 -4.2979503 -4.2803664 -4.2576208 -4.24121 -4.229888 -4.2192755 -4.20077 -4.1823654 -4.1603913 -4.132195 -4.1106725 -4.1090007][-4.3097506 -4.3114495 -4.3072195 -4.2938023 -4.2721915 -4.2408376 -4.2197242 -4.2124715 -4.2046075 -4.1877332 -4.1750226 -4.156981 -4.1209607 -4.0866704 -4.0784516][-4.2882609 -4.2931819 -4.2913127 -4.2781181 -4.2557759 -4.2224483 -4.204114 -4.2054815 -4.2042923 -4.1918073 -4.1881671 -4.1750464 -4.1358228 -4.0973654 -4.0917411][-4.2536864 -4.2621622 -4.2681684 -4.2584267 -4.2391973 -4.21205 -4.2043538 -4.2124023 -4.2159743 -4.2095466 -4.2141614 -4.2087622 -4.1789055 -4.1504126 -4.1539187][-4.2042618 -4.2166219 -4.2347836 -4.2322664 -4.2195363 -4.2040296 -4.20824 -4.2237988 -4.2328277 -4.2319908 -4.2418952 -4.243175 -4.2228131 -4.2042437 -4.2104888][-4.1518707 -4.1680636 -4.197494 -4.2045846 -4.19454 -4.1846166 -4.1958442 -4.2146664 -4.2278552 -4.2329531 -4.2499557 -4.2595115 -4.2504272 -4.2427874 -4.2540588][-4.1129255 -4.1350427 -4.1690044 -4.1804786 -4.1684256 -4.1539035 -4.1607161 -4.1772342 -4.1945915 -4.2114224 -4.2396674 -4.2574768 -4.260777 -4.264575 -4.2783661][-4.1225581 -4.1374598 -4.159122 -4.164598 -4.1456971 -4.1190019 -4.1175518 -4.1304479 -4.1520557 -4.1799736 -4.2167916 -4.2434192 -4.25496 -4.2656751 -4.2838607][-4.150558 -4.1520448 -4.1496544 -4.1348667 -4.1002617 -4.0586319 -4.0488672 -4.0636377 -4.0949459 -4.1343155 -4.17865 -4.2153368 -4.2376308 -4.2560716 -4.2790294][-4.16567 -4.1534834 -4.1269917 -4.0893521 -4.0401258 -3.9951916 -3.9862022 -4.0106344 -4.0561767 -4.1053386 -4.1534123 -4.1951976 -4.225625 -4.248723 -4.2709818][-4.1734471 -4.1523447 -4.1119084 -4.0728588 -4.0329547 -4.0106621 -4.012887 -4.0374575 -4.0831771 -4.1291757 -4.1671119 -4.20162 -4.2298861 -4.2517643 -4.2660694][-4.1782947 -4.1547327 -4.1172915 -4.0942106 -4.0791693 -4.0799828 -4.0899448 -4.1078396 -4.1449223 -4.184464 -4.2131062 -4.2357278 -4.2544456 -4.2685318 -4.2720871][-4.191175 -4.1682291 -4.144 -4.1388063 -4.1439223 -4.1590562 -4.1769972 -4.190804 -4.2176604 -4.2504191 -4.2722878 -4.2861571 -4.2942486 -4.297996 -4.292119][-4.2242517 -4.2065306 -4.1965504 -4.2044754 -4.221705 -4.2432604 -4.2615428 -4.2723742 -4.2906737 -4.3128285 -4.3270674 -4.3323197 -4.3288679 -4.3194323 -4.303287][-4.2650309 -4.254694 -4.2571583 -4.2731576 -4.2935309 -4.3128009 -4.3234029 -4.3266263 -4.3364892 -4.34881 -4.355432 -4.3540626 -4.341362 -4.3215532 -4.2969227]]...]
INFO - root - 2017-12-06 10:17:10.181650: step 9610, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.421 sec/batch; 37h:45m:39s remains)
INFO - root - 2017-12-06 10:17:14.439917: step 9620, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 37h:07m:21s remains)
INFO - root - 2017-12-06 10:17:18.896965: step 9630, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:36m:58s remains)
INFO - root - 2017-12-06 10:17:23.213932: step 9640, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:01m:18s remains)
INFO - root - 2017-12-06 10:17:27.654885: step 9650, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 40h:32m:53s remains)
INFO - root - 2017-12-06 10:17:31.970402: step 9660, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.422 sec/batch; 37h:52m:21s remains)
INFO - root - 2017-12-06 10:17:36.354025: step 9670, loss = 2.09, batch loss = 2.03 (18.1 examples/sec; 0.442 sec/batch; 39h:36m:55s remains)
INFO - root - 2017-12-06 10:17:40.740583: step 9680, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 39h:20m:59s remains)
INFO - root - 2017-12-06 10:17:45.114209: step 9690, loss = 2.11, batch loss = 2.05 (19.5 examples/sec; 0.411 sec/batch; 36h:52m:54s remains)
INFO - root - 2017-12-06 10:17:49.537644: step 9700, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:29m:24s remains)
2017-12-06 10:17:50.024757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2007003 -4.2227163 -4.2359204 -4.236691 -4.2367454 -4.2327456 -4.2203646 -4.2198071 -4.2374744 -4.25361 -4.2700977 -4.2813077 -4.2660785 -4.2280536 -4.2182703][-4.1885552 -4.208427 -4.2203026 -4.2207713 -4.2211785 -4.2189512 -4.2064815 -4.2015538 -4.2297397 -4.2596965 -4.2770228 -4.2820287 -4.2595773 -4.2144165 -4.1985035][-4.1868844 -4.2079825 -4.21849 -4.2171912 -4.2126217 -4.204977 -4.1902189 -4.1863675 -4.2253351 -4.2679253 -4.2878056 -4.2880859 -4.2618432 -4.2143035 -4.1969657][-4.1947317 -4.2170663 -4.227962 -4.2301178 -4.2242713 -4.2045555 -4.17662 -4.1714287 -4.2186465 -4.2717462 -4.294136 -4.29484 -4.2677679 -4.223196 -4.2012167][-4.2178535 -4.235198 -4.2391882 -4.2401085 -4.2310209 -4.1994596 -4.1495848 -4.1403041 -4.2051725 -4.2691231 -4.2962861 -4.2995381 -4.276721 -4.2392712 -4.2142386][-4.2366729 -4.2481508 -4.2412343 -4.2337646 -4.2201614 -4.1794543 -4.1039934 -4.0816517 -4.1687918 -4.2562747 -4.2928958 -4.2986465 -4.28223 -4.2566347 -4.2330012][-4.234951 -4.242815 -4.2298045 -4.2129493 -4.1930118 -4.1416774 -4.0354939 -3.9785836 -4.0970111 -4.2226405 -4.2756534 -4.283699 -4.2692041 -4.2522907 -4.233633][-4.2225704 -4.2310534 -4.2189708 -4.2033434 -4.1820164 -4.1319318 -4.0163646 -3.9220014 -4.0458207 -4.1893692 -4.2544022 -4.2653737 -4.2536521 -4.2398391 -4.2244067][-4.2208171 -4.2295685 -4.2234445 -4.2190886 -4.2091742 -4.1775556 -4.09761 -4.0279326 -4.0953884 -4.2053137 -4.2585821 -4.2689881 -4.2634912 -4.2544818 -4.2396755][-4.2276106 -4.2379251 -4.2384872 -4.2426805 -4.245554 -4.2354174 -4.1913304 -4.1496844 -4.172123 -4.2318506 -4.2643886 -4.2705727 -4.2677574 -4.2617116 -4.2529383][-4.2369819 -4.244504 -4.2474804 -4.2561216 -4.26486 -4.2628632 -4.2337174 -4.2034187 -4.2067132 -4.2318239 -4.2485614 -4.2542562 -4.2535081 -4.2498221 -4.2450047][-4.2449441 -4.2512345 -4.2511587 -4.2585206 -4.267055 -4.2611294 -4.2316365 -4.2010932 -4.2001004 -4.2158232 -4.2287455 -4.2367635 -4.2383428 -4.23781 -4.2334714][-4.2667084 -4.2735209 -4.2705379 -4.2749605 -4.2771835 -4.263854 -4.230886 -4.2004089 -4.1967459 -4.2080479 -4.223134 -4.2391629 -4.2482247 -4.2493095 -4.2439113][-4.2912045 -4.2974329 -4.2922196 -4.291337 -4.2863173 -4.2727561 -4.2499237 -4.2279177 -4.2215228 -4.2260876 -4.2409072 -4.2584953 -4.2724161 -4.2713928 -4.2653813][-4.3091822 -4.3125324 -4.3076992 -4.3027272 -4.2951946 -4.287148 -4.2754159 -4.2626195 -4.2555256 -4.2535419 -4.2613373 -4.2758837 -4.2887917 -4.2889605 -4.28445]]...]
INFO - root - 2017-12-06 10:17:54.209792: step 9710, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:10m:09s remains)
INFO - root - 2017-12-06 10:17:58.704914: step 9720, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 40h:05m:19s remains)
INFO - root - 2017-12-06 10:18:03.094713: step 9730, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.450 sec/batch; 40h:21m:28s remains)
INFO - root - 2017-12-06 10:18:07.453005: step 9740, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:27m:18s remains)
INFO - root - 2017-12-06 10:18:11.855275: step 9750, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:22m:13s remains)
INFO - root - 2017-12-06 10:18:16.333496: step 9760, loss = 2.04, batch loss = 1.99 (14.3 examples/sec; 0.558 sec/batch; 50h:02m:58s remains)
INFO - root - 2017-12-06 10:18:21.712084: step 9770, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 0.546 sec/batch; 48h:57m:53s remains)
INFO - root - 2017-12-06 10:18:26.804733: step 9780, loss = 2.06, batch loss = 2.00 (16.0 examples/sec; 0.499 sec/batch; 44h:43m:19s remains)
INFO - root - 2017-12-06 10:18:31.970712: step 9790, loss = 2.06, batch loss = 2.01 (15.6 examples/sec; 0.512 sec/batch; 45h:54m:23s remains)
INFO - root - 2017-12-06 10:18:37.167207: step 9800, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 0.540 sec/batch; 48h:21m:59s remains)
2017-12-06 10:18:37.668427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.331285 -4.3314095 -4.3307047 -4.3295732 -4.3284678 -4.3269939 -4.3257837 -4.3261266 -4.3277736 -4.3273292 -4.3209867 -4.3067651 -4.2972746 -4.2889929 -4.2771392][-4.3185973 -4.3160152 -4.3119965 -4.3075452 -4.3042884 -4.3031683 -4.3051486 -4.3100343 -4.3164539 -4.3197551 -4.3136005 -4.2994652 -4.292017 -4.2805347 -4.2633381][-4.2913327 -4.2861419 -4.2784638 -4.2696815 -4.2640204 -4.2644181 -4.2714419 -4.2827263 -4.2948895 -4.303587 -4.3003798 -4.2893305 -4.2835441 -4.26759 -4.2452307][-4.2750678 -4.26758 -4.2552123 -4.2413731 -4.2330232 -4.2354779 -4.2483072 -4.2658405 -4.2834687 -4.2974157 -4.2974763 -4.2908978 -4.2878065 -4.2710342 -4.2471609][-4.2631373 -4.2528954 -4.2349777 -4.2150331 -4.2030606 -4.204062 -4.2186007 -4.23996 -4.2618628 -4.281137 -4.2871456 -4.2865629 -4.2857862 -4.2704906 -4.2485914][-4.2406425 -4.2275057 -4.2034297 -4.1744847 -4.1549544 -4.151123 -4.1645446 -4.1860447 -4.2089825 -4.2331724 -4.24913 -4.2593274 -4.2660761 -4.256639 -4.238853][-4.2069774 -4.1929874 -4.1653657 -4.1278548 -4.0971489 -4.0845175 -4.0917735 -4.106946 -4.1245909 -4.1516724 -4.1790361 -4.2016325 -4.218327 -4.2186074 -4.2098737][-4.1969676 -4.1873636 -4.1650314 -4.13023 -4.0952296 -4.0723219 -4.0621018 -4.0567722 -4.0587311 -4.083293 -4.1207805 -4.1540642 -4.176753 -4.1861138 -4.1869817][-4.2282557 -4.222456 -4.2100158 -4.1891003 -4.1642852 -4.142839 -4.1227012 -4.1004353 -4.0871058 -4.1000991 -4.1307254 -4.1593971 -4.1777935 -4.1852837 -4.1866512][-4.2475672 -4.2440081 -4.2402883 -4.2335043 -4.224431 -4.2140832 -4.1965475 -4.1709328 -4.1505518 -4.1487451 -4.1619477 -4.1786494 -4.1901989 -4.1922903 -4.1897345][-4.2420187 -4.2407784 -4.2420363 -4.2424359 -4.2423244 -4.2408981 -4.232235 -4.2134771 -4.1978173 -4.191535 -4.1952372 -4.2011962 -4.2044864 -4.1987419 -4.19081][-4.2415724 -4.2411122 -4.2435946 -4.2435107 -4.2419472 -4.2399249 -4.2357712 -4.2260222 -4.218771 -4.2155571 -4.2191916 -4.2205434 -4.2170205 -4.2051582 -4.1932311][-4.2580667 -4.257268 -4.2569432 -4.2521257 -4.2445364 -4.2370267 -4.2328095 -4.2286677 -4.2271652 -4.2273121 -4.2314458 -4.2291231 -4.2208652 -4.2072382 -4.1953373][-4.2833023 -4.2829227 -4.280066 -4.2712646 -4.2581348 -4.2440982 -4.2346721 -4.2301526 -4.2315593 -4.2325974 -4.2350173 -4.2280545 -4.2172074 -4.2050338 -4.1952109][-4.3087182 -4.3094964 -4.306663 -4.29733 -4.2827406 -4.2642679 -4.2484031 -4.2393751 -4.2387319 -4.2383995 -4.2382755 -4.2302451 -4.2194676 -4.2086191 -4.2000165]]...]
INFO - root - 2017-12-06 10:18:42.948985: step 9810, loss = 2.10, batch loss = 2.04 (15.9 examples/sec; 0.502 sec/batch; 45h:01m:56s remains)
INFO - root - 2017-12-06 10:18:48.210282: step 9820, loss = 2.05, batch loss = 1.99 (15.8 examples/sec; 0.508 sec/batch; 45h:30m:53s remains)
INFO - root - 2017-12-06 10:18:53.365821: step 9830, loss = 2.05, batch loss = 1.99 (15.5 examples/sec; 0.515 sec/batch; 46h:10m:47s remains)
INFO - root - 2017-12-06 10:18:58.577870: step 9840, loss = 2.09, batch loss = 2.04 (15.1 examples/sec; 0.530 sec/batch; 47h:31m:08s remains)
INFO - root - 2017-12-06 10:19:03.789970: step 9850, loss = 2.06, batch loss = 2.01 (14.7 examples/sec; 0.542 sec/batch; 48h:36m:46s remains)
INFO - root - 2017-12-06 10:19:09.141955: step 9860, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 0.537 sec/batch; 48h:07m:56s remains)
INFO - root - 2017-12-06 10:19:14.389676: step 9870, loss = 2.11, batch loss = 2.06 (14.5 examples/sec; 0.553 sec/batch; 49h:35m:31s remains)
INFO - root - 2017-12-06 10:19:19.600346: step 9880, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 0.528 sec/batch; 47h:19m:24s remains)
INFO - root - 2017-12-06 10:19:24.828979: step 9890, loss = 2.05, batch loss = 2.00 (14.8 examples/sec; 0.539 sec/batch; 48h:19m:57s remains)
INFO - root - 2017-12-06 10:19:29.840636: step 9900, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.472 sec/batch; 42h:17m:14s remains)
2017-12-06 10:19:30.412207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3081646 -4.3047028 -4.301333 -4.3000927 -4.30497 -4.3125305 -4.3143139 -4.3109789 -4.3087296 -4.3151951 -4.3251534 -4.329958 -4.3326759 -4.330111 -4.3224149][-4.2970128 -4.2898145 -4.2836943 -4.2813869 -4.2850618 -4.2943654 -4.2981291 -4.2930188 -4.2890429 -4.299675 -4.3154111 -4.32326 -4.3288007 -4.328557 -4.3203764][-4.2867141 -4.2727151 -4.2626953 -4.2561584 -4.2557588 -4.2633557 -4.2629461 -4.2549553 -4.2519932 -4.2672963 -4.2888222 -4.3025231 -4.3146577 -4.3195648 -4.3150253][-4.2764044 -4.2575459 -4.2429361 -4.2317243 -4.2291074 -4.2320876 -4.2248678 -4.2098346 -4.2064571 -4.2226272 -4.2454305 -4.2650089 -4.2879786 -4.3018112 -4.3027577][-4.2628875 -4.2399015 -4.2180238 -4.2021012 -4.1990256 -4.1990943 -4.1833887 -4.15732 -4.1521311 -4.1704364 -4.19734 -4.2241011 -4.2558689 -4.2768559 -4.28467][-4.2467031 -4.2180042 -4.185554 -4.165719 -4.1598053 -4.1525526 -4.1209846 -4.0827613 -4.0832539 -4.1152525 -4.1519322 -4.1887851 -4.2297869 -4.2564855 -4.26963][-4.2334805 -4.199234 -4.1584477 -4.1314883 -4.11543 -4.09269 -4.0411997 -3.9959955 -4.0121779 -4.0651708 -4.1158814 -4.1671348 -4.2196469 -4.2495251 -4.2637048][-4.23326 -4.1984415 -4.1529369 -4.1166229 -4.08524 -4.0402727 -3.9690104 -3.9272544 -3.9698131 -4.0452728 -4.1107569 -4.1726775 -4.2300539 -4.2610927 -4.2727][-4.2427368 -4.2123814 -4.170464 -4.13307 -4.097425 -4.04601 -3.9724514 -3.9429624 -3.9933298 -4.06795 -4.1298933 -4.1836276 -4.235837 -4.2674336 -4.2796912][-4.2488151 -4.22607 -4.1926 -4.1620584 -4.1367669 -4.097487 -4.0405092 -4.0223594 -4.0544629 -4.105463 -4.1491456 -4.187891 -4.2328911 -4.2609825 -4.27412][-4.2529864 -4.2391534 -4.2151661 -4.1944356 -4.18235 -4.1576338 -4.1170344 -4.0985937 -4.1064997 -4.1333861 -4.1623573 -4.1923904 -4.2302742 -4.2499261 -4.2608643][-4.2664528 -4.2597613 -4.2474909 -4.2349811 -4.2295246 -4.215138 -4.1852179 -4.1634812 -4.1589637 -4.1699176 -4.183392 -4.2019825 -4.2285833 -4.238658 -4.2464523][-4.291131 -4.2910247 -4.2886729 -4.2794452 -4.2711654 -4.2570195 -4.2335062 -4.2160029 -4.2084374 -4.2099876 -4.2128115 -4.2214093 -4.2382298 -4.2444897 -4.2475748][-4.31627 -4.3208652 -4.3213172 -4.3114104 -4.2996831 -4.2834435 -4.2638168 -4.2494373 -4.237998 -4.2330446 -4.2324529 -4.2413645 -4.2574325 -4.2641253 -4.266366][-4.3207669 -4.3270183 -4.3282228 -4.3210888 -4.3118267 -4.2983584 -4.28251 -4.2693224 -4.2553158 -4.2468181 -4.2457824 -4.256917 -4.2747655 -4.2849855 -4.2879925]]...]
INFO - root - 2017-12-06 10:19:35.658017: step 9910, loss = 2.11, batch loss = 2.05 (15.0 examples/sec; 0.534 sec/batch; 47h:53m:07s remains)
INFO - root - 2017-12-06 10:19:40.742956: step 9920, loss = 2.10, batch loss = 2.04 (15.7 examples/sec; 0.509 sec/batch; 45h:35m:52s remains)
INFO - root - 2017-12-06 10:19:45.876742: step 9930, loss = 2.09, batch loss = 2.03 (16.0 examples/sec; 0.500 sec/batch; 44h:46m:09s remains)
INFO - root - 2017-12-06 10:19:50.991841: step 9940, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 0.527 sec/batch; 47h:10m:59s remains)
INFO - root - 2017-12-06 10:19:56.219709: step 9950, loss = 2.10, batch loss = 2.05 (15.0 examples/sec; 0.535 sec/batch; 47h:53m:38s remains)
INFO - root - 2017-12-06 10:20:01.457184: step 9960, loss = 2.09, batch loss = 2.03 (15.7 examples/sec; 0.509 sec/batch; 45h:38m:29s remains)
INFO - root - 2017-12-06 10:20:06.640245: step 9970, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 0.494 sec/batch; 44h:17m:50s remains)
INFO - root - 2017-12-06 10:20:11.733445: step 9980, loss = 2.06, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 42h:54m:33s remains)
INFO - root - 2017-12-06 10:20:16.982809: step 9990, loss = 2.08, batch loss = 2.03 (16.6 examples/sec; 0.483 sec/batch; 43h:14m:10s remains)
INFO - root - 2017-12-06 10:20:21.737239: step 10000, loss = 2.10, batch loss = 2.04 (18.7 examples/sec; 0.428 sec/batch; 38h:17m:48s remains)
2017-12-06 10:20:22.256997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2953453 -4.2970929 -4.2982574 -4.2970695 -4.2968392 -4.2942281 -4.290267 -4.2913961 -4.3020358 -4.3108792 -4.3148766 -4.3139195 -4.3096852 -4.3050256 -4.299655][-4.2822905 -4.2844596 -4.2848244 -4.2804027 -4.2768397 -4.2702775 -4.2605886 -4.2609153 -4.2769852 -4.2903628 -4.2971978 -4.3008513 -4.3009648 -4.2983418 -4.2930846][-4.2643108 -4.2620444 -4.25512 -4.2426229 -4.23309 -4.2201242 -4.2016525 -4.2006693 -4.2216058 -4.24218 -4.2531495 -4.2633195 -4.2712951 -4.2761817 -4.2757263][-4.2500787 -4.2434878 -4.2263508 -4.2012596 -4.1824317 -4.1597414 -4.1336122 -4.127954 -4.1545873 -4.182271 -4.1995978 -4.21481 -4.2287984 -4.2412529 -4.2492642][-4.2406144 -4.2320194 -4.20746 -4.1710992 -4.14167 -4.1058922 -4.0640941 -4.0443263 -4.0767679 -4.1186066 -4.1499443 -4.1737227 -4.1936383 -4.2130995 -4.2303333][-4.2292924 -4.2183342 -4.188025 -4.1405058 -4.0969844 -4.0397391 -3.9667842 -3.9230695 -3.9702914 -4.0490055 -4.1141138 -4.1563921 -4.1831779 -4.2038708 -4.2234511][-4.2195053 -4.2095747 -4.1776133 -4.1228881 -4.0611196 -3.9770818 -3.8598924 -3.7652969 -3.8176093 -3.9476621 -4.0582247 -4.1291842 -4.1686888 -4.1951637 -4.2181554][-4.2141533 -4.2099543 -4.1849923 -4.1326532 -4.059166 -3.9533613 -3.8023875 -3.6623564 -3.7021685 -3.8639739 -4.0053639 -4.1032066 -4.1611061 -4.1955585 -4.219573][-4.2108316 -4.2124104 -4.1984539 -4.1649547 -4.1085134 -4.0252395 -3.9080496 -3.8016896 -3.8226802 -3.9399502 -4.052949 -4.1377344 -4.1900511 -4.215754 -4.2296619][-4.2090306 -4.211174 -4.2022085 -4.1814985 -4.1514106 -4.106348 -4.0346732 -3.9710128 -3.983562 -4.05809 -4.135448 -4.1957417 -4.2342749 -4.2484245 -4.251296][-4.2118654 -4.2133508 -4.2068739 -4.1948166 -4.1805773 -4.1635504 -4.1273642 -4.0985284 -4.1137829 -4.1667781 -4.2202792 -4.2617078 -4.2877536 -4.2915087 -4.283679][-4.2271824 -4.2334867 -4.2362237 -4.2337403 -4.2322288 -4.2310009 -4.2136936 -4.1994205 -4.2102222 -4.2447419 -4.2796493 -4.3062592 -4.3219972 -4.3211904 -4.3090696][-4.2484713 -4.2571158 -4.2657847 -4.2705045 -4.275948 -4.2780027 -4.2689819 -4.2616796 -4.26753 -4.2874961 -4.3076549 -4.3236961 -4.3335953 -4.3307285 -4.3180337][-4.2724695 -4.2804317 -4.2886877 -4.2932577 -4.2971959 -4.2978868 -4.2938366 -4.2913685 -4.2957625 -4.3101349 -4.3228731 -4.3320851 -4.3373313 -4.3321233 -4.3200865][-4.285378 -4.289988 -4.2946362 -4.2971478 -4.2994752 -4.2980289 -4.2939491 -4.2927251 -4.2964654 -4.3064356 -4.3156676 -4.3220348 -4.3254547 -4.3214016 -4.3126507]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 10:20:27.257427: step 10010, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 38h:16m:13s remains)
INFO - root - 2017-12-06 10:20:31.561440: step 10020, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.435 sec/batch; 38h:56m:31s remains)
INFO - root - 2017-12-06 10:20:35.884253: step 10030, loss = 2.07, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 39h:48m:39s remains)
INFO - root - 2017-12-06 10:20:40.154198: step 10040, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 39h:10m:12s remains)
INFO - root - 2017-12-06 10:20:44.484874: step 10050, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 37h:31m:01s remains)
INFO - root - 2017-12-06 10:20:48.862440: step 10060, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 39h:06m:43s remains)
INFO - root - 2017-12-06 10:20:53.222225: step 10070, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.439 sec/batch; 39h:21m:17s remains)
INFO - root - 2017-12-06 10:20:57.666041: step 10080, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 39h:42m:44s remains)
INFO - root - 2017-12-06 10:21:02.151123: step 10090, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:17m:46s remains)
INFO - root - 2017-12-06 10:21:06.242316: step 10100, loss = 2.09, batch loss = 2.04 (19.0 examples/sec; 0.421 sec/batch; 37h:39m:45s remains)
2017-12-06 10:21:06.697734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29203 -4.2728009 -4.2719374 -4.2856622 -4.2983618 -4.3015466 -4.2936306 -4.2866917 -4.2864075 -4.2933083 -4.3009357 -4.2945027 -4.2828412 -4.2792773 -4.2808766][-4.2798529 -4.2531419 -4.2534533 -4.2673388 -4.2801061 -4.2853508 -4.2781405 -4.2692661 -4.2645183 -4.2704515 -4.2778444 -4.2698541 -4.2535043 -4.2484608 -4.2534027][-4.2810555 -4.2524223 -4.2520118 -4.263504 -4.2712064 -4.274797 -4.2693429 -4.2634697 -4.2600474 -4.2627082 -4.2606149 -4.2434177 -4.2246966 -4.2236409 -4.2331953][-4.2849112 -4.253984 -4.2492113 -4.2576957 -4.258729 -4.2551665 -4.2507463 -4.2545733 -4.2587705 -4.2599964 -4.2467561 -4.2247696 -4.2103605 -4.2104883 -4.2186937][-4.2746377 -4.2394314 -4.2213883 -4.2211833 -4.2160234 -4.1997104 -4.1908326 -4.2098522 -4.2343884 -4.2466578 -4.2363563 -4.22259 -4.2171855 -4.2147212 -4.2187848][-4.2456703 -4.2035823 -4.1771045 -4.1703587 -4.1591778 -4.1269975 -4.10378 -4.1300631 -4.1744952 -4.2090273 -4.2225332 -4.2274771 -4.2322631 -4.2266054 -4.2237449][-4.2057576 -4.1588283 -4.1296434 -4.1189327 -4.1026525 -4.0500746 -3.9971647 -4.01975 -4.0801525 -4.138514 -4.18398 -4.2095766 -4.2237649 -4.223556 -4.2205143][-4.1710052 -4.1177406 -4.08517 -4.07049 -4.0532832 -3.9973025 -3.9296579 -3.9489076 -4.0149441 -4.0829415 -4.144218 -4.1753721 -4.1960568 -4.20732 -4.2134485][-4.1609778 -4.1069679 -4.0762067 -4.062573 -4.0517163 -4.0184255 -3.9713714 -3.9945662 -4.0509524 -4.1030293 -4.1429858 -4.1564527 -4.1746058 -4.1967697 -4.2137823][-4.1769743 -4.1282134 -4.101223 -4.0898747 -4.0832663 -4.0749712 -4.0585585 -4.0839434 -4.1240973 -4.1515007 -4.1649051 -4.162539 -4.1735663 -4.2026176 -4.2261057][-4.2035141 -4.1643991 -4.1432176 -4.1359839 -4.1334348 -4.1387062 -4.1377368 -4.1569667 -4.1815882 -4.1915569 -4.18987 -4.1869421 -4.1952772 -4.2177405 -4.238625][-4.2355571 -4.2093139 -4.1970544 -4.1951389 -4.1949449 -4.1996121 -4.2012076 -4.21208 -4.2240262 -4.2260165 -4.2214718 -4.2203822 -4.2267723 -4.2438779 -4.2585497][-4.2608733 -4.2473269 -4.2442603 -4.2477479 -4.2471519 -4.2500734 -4.253212 -4.2570996 -4.2599554 -4.2559962 -4.2517323 -4.2501683 -4.2549396 -4.2706742 -4.2826352][-4.2679029 -4.2613673 -4.2620292 -4.2661614 -4.2669635 -4.2704434 -4.2745056 -4.2750769 -4.2731066 -4.2681594 -4.2645278 -4.265202 -4.2706 -4.2824259 -4.2891908][-4.2758608 -4.2700319 -4.2689576 -4.2715793 -4.2737403 -4.2772961 -4.2813234 -4.2812915 -4.27773 -4.2723403 -4.2701297 -4.2744265 -4.2814274 -4.2901225 -4.2945695]]...]
INFO - root - 2017-12-06 10:21:11.063792: step 10110, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:19m:28s remains)
INFO - root - 2017-12-06 10:21:15.446377: step 10120, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.445 sec/batch; 39h:52m:01s remains)
INFO - root - 2017-12-06 10:21:19.966267: step 10130, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:28m:11s remains)
INFO - root - 2017-12-06 10:21:24.396232: step 10140, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.456 sec/batch; 40h:47m:30s remains)
INFO - root - 2017-12-06 10:21:28.723076: step 10150, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 38h:55m:33s remains)
INFO - root - 2017-12-06 10:21:33.155056: step 10160, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.422 sec/batch; 37h:44m:56s remains)
INFO - root - 2017-12-06 10:21:37.577972: step 10170, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 38h:10m:28s remains)
INFO - root - 2017-12-06 10:21:41.845178: step 10180, loss = 2.09, batch loss = 2.03 (17.9 examples/sec; 0.446 sec/batch; 39h:55m:50s remains)
INFO - root - 2017-12-06 10:21:46.239719: step 10190, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 39h:37m:55s remains)
INFO - root - 2017-12-06 10:21:50.213624: step 10200, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 37h:02m:57s remains)
2017-12-06 10:21:50.741864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3245587 -4.3188109 -4.3120313 -4.3050866 -4.2999759 -4.291512 -4.2811122 -4.268888 -4.2642145 -4.2589817 -4.2521448 -4.2561126 -4.2699218 -4.2935076 -4.3146296][-4.3156853 -4.3086104 -4.3010354 -4.2905607 -4.2819519 -4.2731009 -4.2606344 -4.2426491 -4.2351527 -4.2310758 -4.22348 -4.2291508 -4.24925 -4.2813978 -4.3085876][-4.3010263 -4.2893057 -4.2779517 -4.2613635 -4.2497196 -4.2400126 -4.2221088 -4.1965775 -4.1903725 -4.1939211 -4.190361 -4.2008944 -4.2269764 -4.2679448 -4.3024988][-4.2732205 -4.25698 -4.2411366 -4.2209487 -4.2090883 -4.1958022 -4.1656227 -4.130702 -4.1286454 -4.1474319 -4.154974 -4.1732883 -4.204278 -4.2521272 -4.2931771][-4.2403193 -4.217175 -4.1948371 -4.1735249 -4.1646709 -4.1544442 -4.1133547 -4.061965 -4.0588789 -4.0961895 -4.1217341 -4.1510968 -4.1859069 -4.2394361 -4.2857885][-4.2034874 -4.1709561 -4.1397572 -4.1148629 -4.1096306 -4.1065464 -4.0554481 -3.9800234 -3.9671044 -4.0272121 -4.0805712 -4.1271257 -4.1700268 -4.2295237 -4.2820268][-4.1788392 -4.1386437 -4.0983181 -4.0651112 -4.0615454 -4.062726 -4.0093842 -3.9232605 -3.9040015 -3.9771962 -4.0548544 -4.1171679 -4.1700044 -4.2341113 -4.2874718][-4.1723576 -4.1308637 -4.0899329 -4.0552168 -4.053679 -4.0578375 -4.0145259 -3.9408872 -3.9303193 -3.9955475 -4.0661497 -4.1245694 -4.1798854 -4.2443485 -4.2931609][-4.1804252 -4.1474886 -4.1189485 -4.0943222 -4.0966406 -4.1049862 -4.0733104 -4.016645 -4.0134668 -4.0573487 -4.1016755 -4.1408858 -4.1893196 -4.2504897 -4.2958059][-4.1850061 -4.16469 -4.149826 -4.1381612 -4.1453624 -4.158082 -4.136826 -4.0915222 -4.0891767 -4.1142807 -4.1356626 -4.1602054 -4.1999259 -4.2548356 -4.2968659][-4.17521 -4.16299 -4.1536303 -4.148984 -4.1615214 -4.1792517 -4.1675506 -4.1350613 -4.1328311 -4.1481357 -4.1579714 -4.1763535 -4.2109237 -4.2591171 -4.2970362][-4.1574159 -4.1502366 -4.1454635 -4.1458073 -4.1604786 -4.1796203 -4.1752543 -4.1549697 -4.1560912 -4.165709 -4.1684656 -4.1846414 -4.2162161 -4.260345 -4.2959023][-4.1578035 -4.1543727 -4.1553731 -4.1614108 -4.1760321 -4.1904473 -4.18919 -4.1759858 -4.17726 -4.1822896 -4.1817341 -4.1960325 -4.2250438 -4.2657037 -4.2981987][-4.196187 -4.1912007 -4.1948304 -4.2047448 -4.2179961 -4.2266164 -4.2242236 -4.2131929 -4.21292 -4.2148442 -4.2139015 -4.2254796 -4.2491255 -4.2821808 -4.3078117][-4.2490711 -4.2434382 -4.2466497 -4.2543492 -4.2627234 -4.2666554 -4.2651281 -4.2575769 -4.2584257 -4.2593212 -4.2598934 -4.2673783 -4.28196 -4.3038936 -4.32038]]...]
INFO - root - 2017-12-06 10:21:55.169662: step 10210, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:03m:03s remains)
INFO - root - 2017-12-06 10:21:59.443040: step 10220, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.446 sec/batch; 39h:55m:55s remains)
INFO - root - 2017-12-06 10:22:03.864769: step 10230, loss = 2.12, batch loss = 2.06 (17.5 examples/sec; 0.458 sec/batch; 40h:59m:15s remains)
INFO - root - 2017-12-06 10:22:08.272447: step 10240, loss = 2.08, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 40h:59m:04s remains)
INFO - root - 2017-12-06 10:22:12.745797: step 10250, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 38h:49m:33s remains)
INFO - root - 2017-12-06 10:22:17.126674: step 10260, loss = 2.09, batch loss = 2.03 (17.6 examples/sec; 0.454 sec/batch; 40h:36m:29s remains)
INFO - root - 2017-12-06 10:22:21.480965: step 10270, loss = 2.07, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:23m:26s remains)
INFO - root - 2017-12-06 10:22:25.865854: step 10280, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.431 sec/batch; 38h:35m:19s remains)
INFO - root - 2017-12-06 10:22:30.211322: step 10290, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:15m:46s remains)
INFO - root - 2017-12-06 10:22:34.333105: step 10300, loss = 2.08, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:21m:14s remains)
2017-12-06 10:22:34.862377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2407436 -4.2480111 -4.255651 -4.2613306 -4.2622185 -4.2620192 -4.261209 -4.2603922 -4.2558236 -4.2451758 -4.2365251 -4.2355452 -4.2489214 -4.2753692 -4.3024354][-4.2359138 -4.2413688 -4.2474637 -4.2559481 -4.2593217 -4.26037 -4.2607803 -4.2589207 -4.2531018 -4.24301 -4.2385941 -4.2432294 -4.2585611 -4.2839866 -4.3098741][-4.2386675 -4.24251 -4.2474213 -4.255939 -4.2577777 -4.2550883 -4.2536755 -4.2546825 -4.2542582 -4.2517619 -4.2541766 -4.2614636 -4.2742577 -4.2947145 -4.31531][-4.2332034 -4.234632 -4.2375689 -4.2457004 -4.2451005 -4.23644 -4.2273655 -4.2293177 -4.2423673 -4.2542343 -4.264122 -4.2718911 -4.278193 -4.2924237 -4.3090248][-4.2190142 -4.2200027 -4.2233238 -4.2312536 -4.2288594 -4.210495 -4.1909728 -4.1941824 -4.2234678 -4.2542605 -4.2722826 -4.2765703 -4.2720246 -4.2776976 -4.2899556][-4.189187 -4.1992631 -4.2087975 -4.2128391 -4.2001204 -4.1649418 -4.1276164 -4.1298771 -4.1798172 -4.2367849 -4.2686467 -4.2714405 -4.2583308 -4.2565112 -4.2692947][-4.1463103 -4.1726985 -4.1948948 -4.1990786 -4.172555 -4.1102409 -4.0423927 -4.037828 -4.1102929 -4.1929603 -4.2420115 -4.2506428 -4.2378445 -4.2371278 -4.25594][-4.1052432 -4.1503739 -4.1926694 -4.2065673 -4.1746297 -4.0982752 -4.0071707 -3.9857435 -4.0586591 -4.1479158 -4.2084918 -4.2283454 -4.2275229 -4.2383785 -4.2645044][-4.0964403 -4.1528597 -4.2113919 -4.2363267 -4.2140455 -4.1524453 -4.0788846 -4.0507979 -4.0900874 -4.1492872 -4.201385 -4.2276583 -4.2418904 -4.2656207 -4.2947583][-4.1296577 -4.1826658 -4.2379971 -4.2631121 -4.2491164 -4.2072577 -4.1610374 -4.1389647 -4.1534944 -4.1835461 -4.2202907 -4.2447691 -4.2641158 -4.2935586 -4.3221498][-4.1725788 -4.2156086 -4.263052 -4.2872014 -4.2765088 -4.2451963 -4.2167549 -4.2007337 -4.1996455 -4.2061839 -4.2255516 -4.2448316 -4.2677054 -4.2997985 -4.3284822][-4.1951079 -4.2341127 -4.2799454 -4.3059764 -4.3005462 -4.2780566 -4.257566 -4.2404757 -4.2275443 -4.2177019 -4.2229795 -4.2400603 -4.2660928 -4.2983689 -4.3261404][-4.1982632 -4.2369218 -4.28609 -4.3176913 -4.3195615 -4.3059506 -4.2939382 -4.2803578 -4.2639427 -4.2492776 -4.247138 -4.2598934 -4.2814817 -4.30661 -4.3291326][-4.1911674 -4.2255569 -4.2764244 -4.3140922 -4.3246779 -4.3198342 -4.3140082 -4.3055735 -4.293324 -4.2810445 -4.2760906 -4.2815719 -4.2945843 -4.3115473 -4.3297281][-4.1869497 -4.2176952 -4.267292 -4.3048968 -4.3216376 -4.3215523 -4.3175473 -4.31255 -4.3031216 -4.2926273 -4.2893872 -4.2900381 -4.2948589 -4.3070154 -4.3238044]]...]
INFO - root - 2017-12-06 10:22:39.221223: step 10310, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.418 sec/batch; 37h:21m:55s remains)
INFO - root - 2017-12-06 10:22:43.618641: step 10320, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 41h:27m:34s remains)
INFO - root - 2017-12-06 10:22:47.949173: step 10330, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.417 sec/batch; 37h:16m:49s remains)
INFO - root - 2017-12-06 10:22:52.370693: step 10340, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.425 sec/batch; 38h:03m:10s remains)
INFO - root - 2017-12-06 10:22:56.826731: step 10350, loss = 2.11, batch loss = 2.05 (17.5 examples/sec; 0.457 sec/batch; 40h:56m:18s remains)
INFO - root - 2017-12-06 10:23:01.139131: step 10360, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 40h:02m:16s remains)
INFO - root - 2017-12-06 10:23:05.566110: step 10370, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 39h:47m:20s remains)
INFO - root - 2017-12-06 10:23:09.939945: step 10380, loss = 2.07, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 38h:58m:20s remains)
INFO - root - 2017-12-06 10:23:14.304336: step 10390, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:00m:56s remains)
INFO - root - 2017-12-06 10:23:18.337079: step 10400, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.418 sec/batch; 37h:22m:59s remains)
2017-12-06 10:23:18.837119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333464 -4.33315 -4.329145 -4.3157821 -4.2949996 -4.2769928 -4.2668433 -4.2731466 -4.2948313 -4.3149529 -4.3259273 -4.3291535 -4.3289118 -4.3277497 -4.3263707][-4.3361483 -4.3352981 -4.328342 -4.3111091 -4.2873454 -4.2679763 -4.2601657 -4.2716556 -4.2973809 -4.3189611 -4.3308735 -4.3336811 -4.3313389 -4.3284011 -4.3260884][-4.3382545 -4.3349104 -4.3223267 -4.29921 -4.2703028 -4.2479739 -4.242651 -4.2596812 -4.2908888 -4.3160343 -4.3299055 -4.3344359 -4.3327274 -4.3296461 -4.327508][-4.3315873 -4.3242455 -4.3044763 -4.2721882 -4.2363009 -4.2132564 -4.2132769 -4.238338 -4.2770786 -4.3091588 -4.327076 -4.3335829 -4.3335257 -4.3310533 -4.3294134][-4.3093357 -4.2967715 -4.2655334 -4.2219539 -4.1802626 -4.1590552 -4.1674705 -4.2034979 -4.2507472 -4.2925892 -4.3181062 -4.3285966 -4.3318205 -4.3313942 -4.3308845][-4.2703729 -4.2495289 -4.2049193 -4.1499791 -4.1057725 -4.0884266 -4.1065516 -4.1506476 -4.2013335 -4.2524796 -4.29049 -4.3115149 -4.323091 -4.3273144 -4.329329][-4.220118 -4.1894274 -4.1327777 -4.0709195 -4.0304155 -4.0218215 -4.0494289 -4.0941806 -4.1370831 -4.1922836 -4.2441196 -4.2795253 -4.3036761 -4.3166122 -4.3226657][-4.1813216 -4.1426983 -4.0812922 -4.0226259 -3.9927435 -3.993495 -4.026969 -4.0645804 -4.089653 -4.1381259 -4.19767 -4.2442546 -4.2817407 -4.3046384 -4.3161917][-4.1658196 -4.1264024 -4.0729156 -4.0266247 -4.0077963 -4.0122437 -4.0420957 -4.0678096 -4.076828 -4.113575 -4.1708608 -4.2216949 -4.2671437 -4.2960625 -4.3113751][-4.1703439 -4.1355286 -4.094748 -4.0630913 -4.0525675 -4.0558558 -4.0774589 -4.0916586 -4.0906172 -4.1188741 -4.169692 -4.2176876 -4.2637239 -4.2938533 -4.309587][-4.1878724 -4.159451 -4.1327572 -4.1148624 -4.1101708 -4.1117811 -4.1257634 -4.1319051 -4.1267605 -4.148212 -4.1878462 -4.2277026 -4.2677288 -4.2932048 -4.3064189][-4.2160621 -4.1969743 -4.1828656 -4.1765118 -4.1774211 -4.1798511 -4.1875262 -4.1880622 -4.1814671 -4.195363 -4.2220526 -4.250484 -4.2785406 -4.2958288 -4.3036284][-4.2443995 -4.2322516 -4.2266474 -4.228538 -4.2340035 -4.2377863 -4.2418017 -4.2397847 -4.2333541 -4.2399716 -4.2567143 -4.2762885 -4.2932181 -4.3015213 -4.3032188][-4.2672873 -4.259305 -4.2577024 -4.2629881 -4.2700977 -4.2742438 -4.2764254 -4.2743635 -4.2684183 -4.2694821 -4.2795973 -4.2940645 -4.3038054 -4.3058276 -4.30376][-4.2874017 -4.2811146 -4.2804523 -4.2847576 -4.2897272 -4.292541 -4.2930703 -4.2906613 -4.2856708 -4.2851214 -4.2913246 -4.3010755 -4.3060117 -4.3053212 -4.3024669]]...]
INFO - root - 2017-12-06 10:23:23.281214: step 10410, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.455 sec/batch; 40h:40m:35s remains)
INFO - root - 2017-12-06 10:23:27.675415: step 10420, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 39h:49m:48s remains)
INFO - root - 2017-12-06 10:23:32.065613: step 10430, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 38h:43m:10s remains)
INFO - root - 2017-12-06 10:23:36.433667: step 10440, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.436 sec/batch; 38h:59m:44s remains)
INFO - root - 2017-12-06 10:23:40.889396: step 10450, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.426 sec/batch; 38h:09m:13s remains)
INFO - root - 2017-12-06 10:23:45.203446: step 10460, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 37h:52m:57s remains)
INFO - root - 2017-12-06 10:23:49.610583: step 10470, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 37h:40m:18s remains)
INFO - root - 2017-12-06 10:23:53.968491: step 10480, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 38h:42m:59s remains)
INFO - root - 2017-12-06 10:23:58.413235: step 10490, loss = 2.08, batch loss = 2.03 (18.0 examples/sec; 0.444 sec/batch; 39h:40m:15s remains)
INFO - root - 2017-12-06 10:24:02.510934: step 10500, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:00m:49s remains)
2017-12-06 10:24:03.050846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30711 -4.2943258 -4.2831316 -4.2762184 -4.278326 -4.2864256 -4.2993078 -4.3100219 -4.3115163 -4.310627 -4.3110375 -4.3180428 -4.3263745 -4.3224716 -4.2966332][-4.295464 -4.2804379 -4.269475 -4.2645345 -4.27223 -4.2863193 -4.30458 -4.3177581 -4.317246 -4.3119979 -4.3098097 -4.3157787 -4.3206744 -4.3096776 -4.2780042][-4.2736516 -4.2594953 -4.2507625 -4.2486687 -4.2603297 -4.2792282 -4.3020926 -4.3160272 -4.3135757 -4.3063126 -4.3042517 -4.3102088 -4.3110204 -4.2930083 -4.2575974][-4.2468805 -4.240613 -4.2385774 -4.2402616 -4.2537313 -4.274467 -4.2971635 -4.3087525 -4.3036513 -4.2955346 -4.2930608 -4.2970362 -4.2931509 -4.271893 -4.2380633][-4.2104774 -4.2182064 -4.2255011 -4.23519 -4.2541184 -4.2781081 -4.2966256 -4.2986908 -4.2855015 -4.2754688 -4.2719717 -4.2722158 -4.2640438 -4.24249 -4.2174163][-4.17358 -4.1946487 -4.21056 -4.229651 -4.2554312 -4.280386 -4.2884049 -4.2714224 -4.245739 -4.2367024 -4.235918 -4.2346668 -4.2270603 -4.2131019 -4.2052264][-4.1604762 -4.1841378 -4.2008052 -4.2235775 -4.2486787 -4.2649283 -4.2562213 -4.217535 -4.1842413 -4.1826696 -4.1906471 -4.1962175 -4.1996307 -4.2018938 -4.2128663][-4.1770434 -4.1905875 -4.2001953 -4.2169528 -4.2333345 -4.2357631 -4.2079 -4.1535473 -4.1236415 -4.1370449 -4.1562119 -4.174408 -4.1933789 -4.2112551 -4.23295][-4.2063704 -4.2049603 -4.2057028 -4.2123842 -4.2188196 -4.2087059 -4.1680207 -4.1089182 -4.0946722 -4.1294107 -4.1603918 -4.1882968 -4.2148652 -4.2367787 -4.2582769][-4.2243366 -4.2152286 -4.2116532 -4.2136555 -4.2141728 -4.1981111 -4.1576881 -4.1097074 -4.1204433 -4.1705294 -4.2029319 -4.2280788 -4.2496471 -4.2666111 -4.2837276][-4.2273669 -4.21874 -4.2155404 -4.21852 -4.2201843 -4.21026 -4.1855159 -4.1576104 -4.1791596 -4.2223372 -4.2442193 -4.2599049 -4.2717824 -4.2829733 -4.2960663][-4.2298222 -4.2224808 -4.2216454 -4.2288065 -4.2371569 -4.2391572 -4.2297592 -4.2148271 -4.2284589 -4.2517118 -4.2606363 -4.2679353 -4.2720022 -4.2780437 -4.2880907][-4.2388048 -4.2344556 -4.2345204 -4.2437334 -4.2541537 -4.2605729 -4.2569051 -4.247149 -4.2494717 -4.2579861 -4.2601624 -4.264267 -4.263896 -4.2641063 -4.2679434][-4.2559495 -4.2541838 -4.2519135 -4.2553983 -4.2608504 -4.2651277 -4.2606263 -4.2558866 -4.2541652 -4.2558856 -4.2580366 -4.2600241 -4.2530494 -4.2460837 -4.2421384][-4.2642508 -4.2624731 -4.2557197 -4.2551303 -4.2571278 -4.2601447 -4.2548265 -4.2547379 -4.2524133 -4.2522783 -4.2576947 -4.2563443 -4.2434931 -4.2293067 -4.219018]]...]
INFO - root - 2017-12-06 10:24:07.345668: step 10510, loss = 2.12, batch loss = 2.06 (18.5 examples/sec; 0.433 sec/batch; 38h:43m:02s remains)
INFO - root - 2017-12-06 10:24:11.610021: step 10520, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 38h:24m:22s remains)
INFO - root - 2017-12-06 10:24:15.791018: step 10530, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:11m:49s remains)
INFO - root - 2017-12-06 10:24:20.219223: step 10540, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 38h:45m:43s remains)
INFO - root - 2017-12-06 10:24:24.484770: step 10550, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.451 sec/batch; 40h:20m:06s remains)
INFO - root - 2017-12-06 10:24:28.803567: step 10560, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:00m:31s remains)
INFO - root - 2017-12-06 10:24:33.175295: step 10570, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 38h:28m:17s remains)
INFO - root - 2017-12-06 10:24:37.522102: step 10580, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 41h:22m:10s remains)
INFO - root - 2017-12-06 10:24:41.934178: step 10590, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 38h:58m:12s remains)
INFO - root - 2017-12-06 10:24:46.018110: step 10600, loss = 2.11, batch loss = 2.05 (18.7 examples/sec; 0.428 sec/batch; 38h:15m:19s remains)
2017-12-06 10:24:46.527523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1852274 -4.1159124 -4.0412536 -3.9994338 -4.0430374 -4.1014686 -4.1275558 -4.1326189 -4.1198559 -4.1153545 -4.1453652 -4.1793814 -4.2064309 -4.221045 -4.21672][-4.1864915 -4.1265864 -4.0682268 -4.0406432 -4.0787654 -4.1161189 -4.1209149 -4.1123648 -4.0955749 -4.0999279 -4.1505432 -4.1991754 -4.2301745 -4.2472563 -4.2483482][-4.2089972 -4.164361 -4.12678 -4.1163 -4.1473808 -4.1677089 -4.1579876 -4.1450338 -4.1303554 -4.1383204 -4.1839046 -4.2286615 -4.2542129 -4.2650132 -4.2657518][-4.2330985 -4.1997871 -4.1766362 -4.1792564 -4.2029943 -4.2126122 -4.2043915 -4.2041988 -4.2028904 -4.203948 -4.22257 -4.2476335 -4.2634745 -4.272788 -4.2762718][-4.2440081 -4.2178631 -4.1994328 -4.2002273 -4.2096109 -4.2055082 -4.1980538 -4.2145581 -4.2364268 -4.2419186 -4.2401848 -4.2434354 -4.248322 -4.2581563 -4.2710819][-4.2424879 -4.21385 -4.1870756 -4.173667 -4.1649251 -4.146862 -4.1353312 -4.1573191 -4.1980004 -4.214869 -4.209 -4.2048893 -4.2112355 -4.2356157 -4.2675295][-4.2292323 -4.1926374 -4.1551962 -4.1209536 -4.0911512 -4.0611534 -4.0405059 -4.0537481 -4.1046038 -4.1305609 -4.12842 -4.1357412 -4.1636853 -4.2141232 -4.2655249][-4.2055173 -4.1609087 -4.1154552 -4.0718474 -4.0308981 -3.9935491 -3.961616 -3.9589188 -4.0106306 -4.0407763 -4.0376582 -4.0676069 -4.1337705 -4.2084394 -4.267189][-4.1897168 -4.1410065 -4.0929351 -4.0475292 -4.0092864 -3.9802926 -3.9495893 -3.9354618 -3.9731102 -3.9922543 -3.9865243 -4.0394258 -4.1365528 -4.2194862 -4.2690892][-4.1919322 -4.1486897 -4.1054993 -4.0646 -4.0375695 -4.0208507 -4.0081358 -3.9996338 -4.019362 -4.0231676 -4.0180655 -4.0745149 -4.172071 -4.2446828 -4.2711668][-4.2028346 -4.1702332 -4.138907 -4.1117568 -4.1001277 -4.098084 -4.09743 -4.0932708 -4.1068411 -4.1115456 -4.1123343 -4.1527233 -4.2221537 -4.2677593 -4.2675605][-4.2155671 -4.1911325 -4.1723061 -4.1614523 -4.1641374 -4.1714764 -4.1740322 -4.167038 -4.1777563 -4.1898456 -4.2008424 -4.2258883 -4.2606268 -4.2793107 -4.26396][-4.2305107 -4.2124691 -4.2005954 -4.1986513 -4.2091222 -4.2220211 -4.226635 -4.2220888 -4.2302914 -4.2428117 -4.2566648 -4.272645 -4.2838573 -4.283154 -4.2615242][-4.2405081 -4.2265215 -4.2204661 -4.2251849 -4.2415781 -4.2563019 -4.2609181 -4.2604451 -4.266397 -4.274766 -4.2837648 -4.2932973 -4.295156 -4.2867541 -4.2636456][-4.2411857 -4.227109 -4.2219744 -4.2302604 -4.2512383 -4.2656431 -4.2690926 -4.2729754 -4.2798491 -4.2871294 -4.2934456 -4.2998505 -4.2965989 -4.2846308 -4.263453]]...]
INFO - root - 2017-12-06 10:24:50.837671: step 10610, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.433 sec/batch; 38h:44m:17s remains)
INFO - root - 2017-12-06 10:24:55.171460: step 10620, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:03m:26s remains)
INFO - root - 2017-12-06 10:24:59.520489: step 10630, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 40h:55m:45s remains)
INFO - root - 2017-12-06 10:25:03.902165: step 10640, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.443 sec/batch; 39h:38m:54s remains)
INFO - root - 2017-12-06 10:25:08.196906: step 10650, loss = 2.09, batch loss = 2.04 (18.1 examples/sec; 0.441 sec/batch; 39h:27m:42s remains)
INFO - root - 2017-12-06 10:25:12.482587: step 10660, loss = 2.09, batch loss = 2.03 (19.6 examples/sec; 0.409 sec/batch; 36h:33m:23s remains)
INFO - root - 2017-12-06 10:25:16.831243: step 10670, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.434 sec/batch; 38h:48m:10s remains)
INFO - root - 2017-12-06 10:25:21.136616: step 10680, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 37h:52m:00s remains)
INFO - root - 2017-12-06 10:25:25.417829: step 10690, loss = 2.09, batch loss = 2.03 (19.1 examples/sec; 0.418 sec/batch; 37h:23m:44s remains)
INFO - root - 2017-12-06 10:25:29.393691: step 10700, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 40h:37m:30s remains)
2017-12-06 10:25:29.887139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.181479 -4.20403 -4.2276459 -4.2463717 -4.2594962 -4.2657433 -4.2673721 -4.2666645 -4.2622247 -4.254858 -4.247314 -4.2432785 -4.2421961 -4.2416878 -4.2427492][-4.2002378 -4.222661 -4.2434192 -4.2587876 -4.2680798 -4.2702041 -4.2684836 -4.2656426 -4.2604814 -4.2526212 -4.243803 -4.2379107 -4.2355819 -4.2352 -4.2367282][-4.2252483 -4.242815 -4.2571959 -4.265893 -4.2689919 -4.2663918 -4.2615423 -4.25691 -4.2512689 -4.2438049 -4.2353063 -4.229485 -4.2271457 -4.2274971 -4.2303381][-4.2418537 -4.2504177 -4.25535 -4.2562256 -4.2534981 -4.2475986 -4.2415419 -4.2371612 -4.2339611 -4.230092 -4.2250223 -4.2218714 -4.2217064 -4.2234859 -4.2267613][-4.2513709 -4.2490826 -4.2439284 -4.2362356 -4.2264404 -4.2171564 -4.211256 -4.2102327 -4.2133288 -4.2168078 -4.2186685 -4.2207065 -4.2243781 -4.2279019 -4.2305584][-4.254571 -4.2449126 -4.2327037 -4.2181745 -4.2034698 -4.1925159 -4.1884651 -4.1924157 -4.2023993 -4.2132425 -4.2217007 -4.2281747 -4.234241 -4.2384281 -4.2399426][-4.2553649 -4.24402 -4.2309508 -4.2154131 -4.2008233 -4.1916618 -4.1909142 -4.198103 -4.2105083 -4.2230263 -4.2322831 -4.2383623 -4.2430854 -4.2463884 -4.2479057][-4.2555962 -4.2465668 -4.2361484 -4.2234907 -4.2120233 -4.2057762 -4.2072511 -4.214736 -4.2248158 -4.2338848 -4.2394681 -4.2422023 -4.2442222 -4.2460647 -4.2478447][-4.2566781 -4.2496748 -4.2422915 -4.2328839 -4.2242222 -4.2201328 -4.2223883 -4.2282672 -4.2341228 -4.2383714 -4.2401714 -4.2402496 -4.2402539 -4.2407537 -4.2418633][-4.259779 -4.2540383 -4.2492132 -4.2425532 -4.2360396 -4.2329698 -4.2346077 -4.2377548 -4.2396188 -4.2402811 -4.2399778 -4.2389092 -4.2381287 -4.2377949 -4.2380424][-4.2634258 -4.258666 -4.2558231 -4.2514563 -4.2468114 -4.244473 -4.2451687 -4.246201 -4.2457528 -4.2448883 -4.2438622 -4.2423534 -4.2409949 -4.240273 -4.2400131][-4.265059 -4.2609177 -4.259305 -4.2567558 -4.2538147 -4.2522349 -4.2524266 -4.252635 -4.25171 -4.2506104 -4.2494693 -4.2476254 -4.2456212 -4.2443185 -4.2437143][-4.2637472 -4.2600474 -4.2591457 -4.257895 -4.2562652 -4.2552609 -4.2550941 -4.2549963 -4.2541447 -4.2531881 -4.2523565 -4.2507405 -4.2488246 -4.2474065 -4.246419][-4.2602043 -4.2566285 -4.255806 -4.2550073 -4.2539525 -4.2532482 -4.2528648 -4.2525554 -4.2517085 -4.2506213 -4.250042 -4.2489729 -4.2477779 -4.246789 -4.2459378][-4.2556529 -4.2527018 -4.2524533 -4.2523637 -4.2519741 -4.2514858 -4.2508297 -4.2499266 -4.2482157 -4.245966 -4.2442932 -4.2425971 -4.2410636 -4.2398982 -4.2394543]]...]
INFO - root - 2017-12-06 10:25:34.218919: step 10710, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:01m:37s remains)
INFO - root - 2017-12-06 10:25:38.512121: step 10720, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.449 sec/batch; 40h:09m:02s remains)
INFO - root - 2017-12-06 10:25:42.781472: step 10730, loss = 2.09, batch loss = 2.04 (18.7 examples/sec; 0.429 sec/batch; 38h:18m:35s remains)
INFO - root - 2017-12-06 10:25:47.134351: step 10740, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.434 sec/batch; 38h:49m:06s remains)
INFO - root - 2017-12-06 10:25:51.433171: step 10750, loss = 2.07, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 37h:43m:16s remains)
INFO - root - 2017-12-06 10:25:55.805047: step 10760, loss = 2.07, batch loss = 2.02 (18.4 examples/sec; 0.434 sec/batch; 38h:47m:23s remains)
INFO - root - 2017-12-06 10:26:00.214794: step 10770, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 38h:14m:43s remains)
INFO - root - 2017-12-06 10:26:04.517137: step 10780, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.422 sec/batch; 37h:42m:49s remains)
INFO - root - 2017-12-06 10:26:08.905645: step 10790, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:24m:37s remains)
INFO - root - 2017-12-06 10:26:12.769030: step 10800, loss = 2.07, batch loss = 2.01 (19.5 examples/sec; 0.411 sec/batch; 36h:45m:07s remains)
2017-12-06 10:26:13.272130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.320251 -4.3184438 -4.3171062 -4.3158355 -4.3125515 -4.3072515 -4.3009796 -4.2926207 -4.2861915 -4.28419 -4.2804766 -4.2695622 -4.2538114 -4.2361679 -4.2113571][-4.3119254 -4.3075013 -4.3031816 -4.2967768 -4.2874188 -4.2768335 -4.2659454 -4.2549057 -4.2515955 -4.2547975 -4.2578216 -4.2504449 -4.2336564 -4.2074823 -4.1727972][-4.3049722 -4.2979703 -4.289978 -4.2751508 -4.2606087 -4.2492604 -4.2373667 -4.2270374 -4.2284937 -4.2379293 -4.2474494 -4.245677 -4.2249255 -4.1899486 -4.1499505][-4.2896194 -4.2789779 -4.2658892 -4.2472119 -4.2341218 -4.2268963 -4.2153029 -4.20639 -4.2115669 -4.2248306 -4.2381525 -4.2424164 -4.2254677 -4.1937981 -4.16055][-4.2743092 -4.2628064 -4.2471232 -4.2273078 -4.2133527 -4.199645 -4.1793418 -4.1680632 -4.1752353 -4.191474 -4.2108274 -4.2278128 -4.22841 -4.212316 -4.1908226][-4.2705746 -4.2592783 -4.2389607 -4.2132549 -4.189579 -4.15899 -4.1193519 -4.0976028 -4.1116786 -4.1391273 -4.17048 -4.2008281 -4.2156658 -4.2158027 -4.2065234][-4.279911 -4.2697558 -4.2400832 -4.1992574 -4.1544342 -4.0931873 -4.0171714 -3.9827378 -4.0215483 -4.0740972 -4.1187291 -4.1576481 -4.1866341 -4.2056308 -4.2113881][-4.2906909 -4.2805004 -4.2420416 -4.1836271 -4.1130919 -4.0246739 -3.920203 -3.889801 -3.9704289 -4.0482135 -4.0923538 -4.1323833 -4.1757193 -4.2113304 -4.2331748][-4.28956 -4.2775598 -4.2370024 -4.1717434 -4.0963335 -4.0224242 -3.9517126 -3.9480848 -4.0277424 -4.090898 -4.1167426 -4.1462951 -4.1921678 -4.2327194 -4.2619934][-4.2702932 -4.2636776 -4.2324181 -4.1801004 -4.1270547 -4.098578 -4.0831261 -4.0922871 -4.1316586 -4.1608233 -4.1696467 -4.1906638 -4.2281408 -4.2634764 -4.2886233][-4.2261863 -4.2282929 -4.2185717 -4.1978359 -4.17629 -4.1751881 -4.1845832 -4.1956344 -4.2060537 -4.2098432 -4.2089162 -4.2236695 -4.2499423 -4.2758307 -4.2946978][-4.17909 -4.1982193 -4.2128983 -4.2158284 -4.2105627 -4.21209 -4.2231474 -4.233953 -4.2345734 -4.2263832 -4.222043 -4.2291889 -4.24728 -4.2688212 -4.2839189][-4.1650968 -4.1977892 -4.217845 -4.2245774 -4.2219748 -4.2221251 -4.23352 -4.2450023 -4.2416835 -4.2303009 -4.2208786 -4.22069 -4.2352242 -4.2594042 -4.2766695][-4.182333 -4.2112341 -4.2232666 -4.22347 -4.2159123 -4.2157574 -4.2276344 -4.2412271 -4.2391896 -4.2321773 -4.2263036 -4.2256107 -4.2425466 -4.2716136 -4.2909546][-4.2016473 -4.21469 -4.2180395 -4.2154474 -4.2080989 -4.2107773 -4.2272005 -4.2454181 -4.2504897 -4.251277 -4.252171 -4.25469 -4.2721858 -4.2975597 -4.3128815]]...]
INFO - root - 2017-12-06 10:26:17.603053: step 10810, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 37h:53m:10s remains)
INFO - root - 2017-12-06 10:26:21.848388: step 10820, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.423 sec/batch; 37h:45m:53s remains)
INFO - root - 2017-12-06 10:26:26.057772: step 10830, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 38h:33m:42s remains)
INFO - root - 2017-12-06 10:26:30.350959: step 10840, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.429 sec/batch; 38h:20m:12s remains)
INFO - root - 2017-12-06 10:26:34.746470: step 10850, loss = 2.11, batch loss = 2.06 (19.7 examples/sec; 0.407 sec/batch; 36h:21m:54s remains)
INFO - root - 2017-12-06 10:26:38.948620: step 10860, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.419 sec/batch; 37h:24m:25s remains)
INFO - root - 2017-12-06 10:26:43.243654: step 10870, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.436 sec/batch; 38h:56m:40s remains)
INFO - root - 2017-12-06 10:26:47.551557: step 10880, loss = 2.09, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 37h:54m:33s remains)
INFO - root - 2017-12-06 10:26:51.883165: step 10890, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.425 sec/batch; 38h:00m:04s remains)
INFO - root - 2017-12-06 10:26:55.918324: step 10900, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 38h:16m:23s remains)
2017-12-06 10:26:56.403413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2629752 -4.2474527 -4.230607 -4.2384667 -4.2741866 -4.3185039 -4.3416252 -4.3480649 -4.3476849 -4.3305836 -4.3019586 -4.2669263 -4.2316022 -4.2028713 -4.1974196][-4.2373738 -4.2146726 -4.1953464 -4.2017612 -4.2375512 -4.2800632 -4.3047462 -4.3193889 -4.3286042 -4.3198838 -4.2951922 -4.2600141 -4.2209048 -4.1911793 -4.1937194][-4.2163215 -4.1900167 -4.1751022 -4.1817474 -4.2125688 -4.2468472 -4.2676105 -4.2877541 -4.3049059 -4.3027163 -4.282351 -4.2516589 -4.2177238 -4.1927309 -4.2008224][-4.1942611 -4.1712618 -4.1680651 -4.1819143 -4.2068305 -4.2334814 -4.2513685 -4.270072 -4.2847309 -4.2837005 -4.26476 -4.2395096 -4.2144036 -4.1976562 -4.2091546][-4.161706 -4.1474714 -4.1561732 -4.1785464 -4.20059 -4.2199831 -4.2401114 -4.2601833 -4.2658639 -4.2568941 -4.2363648 -4.2227387 -4.2124691 -4.2027135 -4.215414][-4.1424503 -4.136827 -4.1548772 -4.178 -4.1959214 -4.2104011 -4.2316623 -4.246181 -4.2348876 -4.2074542 -4.182992 -4.1830826 -4.1920056 -4.1918545 -4.2069154][-4.1554165 -4.1523395 -4.1688318 -4.1870122 -4.20109 -4.21362 -4.2314134 -4.2350974 -4.201539 -4.1471062 -4.1144176 -4.128686 -4.1541839 -4.1630898 -4.1794934][-4.1679287 -4.1692982 -4.1805263 -4.1998029 -4.2170267 -4.2288632 -4.239037 -4.2311468 -4.1808405 -4.1026816 -4.0562015 -4.0818696 -4.1281128 -4.1452327 -4.1619315][-4.1640821 -4.1772652 -4.1905851 -4.21402 -4.240128 -4.2496243 -4.2489176 -4.2341747 -4.1814475 -4.0937486 -4.0407929 -4.0734873 -4.132266 -4.1532931 -4.168314][-4.1668439 -4.1910343 -4.2084575 -4.230906 -4.2533531 -4.2566805 -4.2504683 -4.2378497 -4.1991405 -4.1290956 -4.0911965 -4.1259956 -4.1757908 -4.1867194 -4.1936035][-4.1778016 -4.2055931 -4.2229757 -4.241909 -4.2523265 -4.2499967 -4.2436771 -4.2382779 -4.2160978 -4.1764932 -4.1613812 -4.19421 -4.226366 -4.2201757 -4.2166061][-4.1868262 -4.2144375 -4.2326159 -4.2490072 -4.2492938 -4.2435975 -4.2412825 -4.2385569 -4.223177 -4.201014 -4.2026768 -4.2366562 -4.257 -4.236321 -4.220037][-4.1947784 -4.2248406 -4.2469258 -4.2616091 -4.2607989 -4.2535725 -4.2487969 -4.243093 -4.2263408 -4.209475 -4.2173119 -4.2491531 -4.2577362 -4.221035 -4.1907306][-4.1994448 -4.2291236 -4.2556114 -4.27052 -4.26922 -4.2605343 -4.2520809 -4.240777 -4.2199197 -4.204217 -4.2165151 -4.2429738 -4.239748 -4.1886821 -4.1453986][-4.2152057 -4.2420268 -4.2704029 -4.2807107 -4.2742519 -4.2625976 -4.2567654 -4.2449636 -4.2215691 -4.2060089 -4.2163334 -4.2324548 -4.2193427 -4.1624737 -4.11547]]...]
INFO - root - 2017-12-06 10:27:00.645063: step 10910, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 39h:20m:33s remains)
INFO - root - 2017-12-06 10:27:04.867675: step 10920, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 38h:25m:03s remains)
INFO - root - 2017-12-06 10:27:09.204893: step 10930, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 38h:57m:18s remains)
INFO - root - 2017-12-06 10:27:13.609739: step 10940, loss = 2.09, batch loss = 2.04 (17.7 examples/sec; 0.453 sec/batch; 40h:27m:27s remains)
INFO - root - 2017-12-06 10:27:17.943665: step 10950, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 37h:41m:14s remains)
INFO - root - 2017-12-06 10:27:22.231672: step 10960, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 38h:34m:57s remains)
INFO - root - 2017-12-06 10:27:26.638103: step 10970, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.429 sec/batch; 38h:17m:39s remains)
INFO - root - 2017-12-06 10:27:30.964537: step 10980, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 39h:12m:25s remains)
INFO - root - 2017-12-06 10:27:35.327706: step 10990, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 41h:39m:01s remains)
INFO - root - 2017-12-06 10:27:39.307179: step 11000, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.422 sec/batch; 37h:43m:04s remains)
2017-12-06 10:27:39.843211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3025546 -4.2994313 -4.3006244 -4.3011346 -4.2988753 -4.295362 -4.2952251 -4.3101592 -4.3252616 -4.3309207 -4.3295584 -4.3274794 -4.3247437 -4.3200316 -4.3137674][-4.2785106 -4.2732725 -4.2732244 -4.2722464 -4.26903 -4.2620964 -4.2587943 -4.2794342 -4.3032341 -4.3113956 -4.3094125 -4.3055644 -4.2995057 -4.2917953 -4.2854638][-4.2745476 -4.2710409 -4.2697935 -4.2618537 -4.2517262 -4.2352114 -4.2220807 -4.2480588 -4.2857461 -4.3009853 -4.3025937 -4.2996902 -4.29075 -4.2795544 -4.2742248][-4.274159 -4.2726316 -4.267622 -4.2529392 -4.23341 -4.2040672 -4.1785359 -4.205976 -4.2537942 -4.2744522 -4.2821436 -4.2836637 -4.2759895 -4.2650743 -4.2605338][-4.2719073 -4.270412 -4.2657909 -4.247673 -4.2195287 -4.1769352 -4.1340513 -4.1519589 -4.2021494 -4.23213 -4.2554 -4.2704072 -4.2723746 -4.2692013 -4.267715][-4.271214 -4.2708831 -4.2664895 -4.243474 -4.2080064 -4.1554694 -4.0947709 -4.0973425 -4.1398935 -4.1736259 -4.2166572 -4.250217 -4.2654 -4.2745719 -4.2810178][-4.27402 -4.2719693 -4.2637835 -4.2329969 -4.1939054 -4.13547 -4.0657115 -4.0509372 -4.068891 -4.0914969 -4.1516914 -4.2059793 -4.2383165 -4.2668724 -4.2866917][-4.2713776 -4.263679 -4.2458315 -4.2040586 -4.1619692 -4.102457 -4.0390363 -4.02253 -4.02303 -4.0361252 -4.1100116 -4.180088 -4.2203 -4.2592554 -4.2828054][-4.2642412 -4.2519369 -4.22595 -4.1801996 -4.1390395 -4.0831523 -4.0353513 -4.0411248 -4.0568519 -4.07654 -4.1418171 -4.1999006 -4.2258959 -4.2572818 -4.2777915][-4.2686186 -4.2599907 -4.2345381 -4.1927557 -4.1563148 -4.10715 -4.0727348 -4.0966983 -4.1335058 -4.1611919 -4.2079859 -4.2430286 -4.24669 -4.2551479 -4.2672491][-4.2683368 -4.2623806 -4.238801 -4.2004428 -4.1712108 -4.1328564 -4.1056609 -4.1364913 -4.185709 -4.215632 -4.2468071 -4.26351 -4.2534137 -4.2437043 -4.2498131][-4.26204 -4.256671 -4.2362728 -4.1989484 -4.17256 -4.145412 -4.1266856 -4.1586928 -4.2098923 -4.23777 -4.25393 -4.2568107 -4.2428789 -4.2264028 -4.2281108][-4.2547183 -4.2535663 -4.2454028 -4.2176957 -4.1978784 -4.1765327 -4.1549683 -4.1753545 -4.2189159 -4.2404256 -4.2505322 -4.2515841 -4.2415791 -4.2261868 -4.2263656][-4.2478685 -4.2496967 -4.25475 -4.2443514 -4.2330112 -4.2178779 -4.1939411 -4.1996841 -4.2264156 -4.239 -4.2471948 -4.2492571 -4.242064 -4.2312436 -4.23595][-4.2544727 -4.257102 -4.2690349 -4.2731543 -4.269877 -4.2599363 -4.2413077 -4.2420607 -4.2554922 -4.2605281 -4.2661438 -4.2672453 -4.2592359 -4.2509689 -4.257771]]...]
INFO - root - 2017-12-06 10:27:44.157139: step 11010, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 38h:35m:21s remains)
INFO - root - 2017-12-06 10:27:48.592782: step 11020, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 37h:57m:01s remains)
INFO - root - 2017-12-06 10:27:52.984237: step 11030, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.404 sec/batch; 36h:03m:04s remains)
INFO - root - 2017-12-06 10:27:57.361376: step 11040, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.438 sec/batch; 39h:07m:12s remains)
INFO - root - 2017-12-06 10:28:01.650383: step 11050, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 38h:25m:48s remains)
INFO - root - 2017-12-06 10:28:05.950976: step 11060, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 38h:32m:27s remains)
INFO - root - 2017-12-06 10:28:10.174243: step 11070, loss = 2.07, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 37h:23m:26s remains)
INFO - root - 2017-12-06 10:28:14.427859: step 11080, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.425 sec/batch; 37h:56m:58s remains)
INFO - root - 2017-12-06 10:28:18.693592: step 11090, loss = 2.12, batch loss = 2.06 (19.5 examples/sec; 0.410 sec/batch; 36h:38m:40s remains)
INFO - root - 2017-12-06 10:28:22.663092: step 11100, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 37h:50m:14s remains)
2017-12-06 10:28:23.239545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2477736 -4.2546315 -4.2492881 -4.2475176 -4.2653723 -4.2815247 -4.2861457 -4.260725 -4.2061415 -4.1583242 -4.1644006 -4.1935987 -4.2109323 -4.2141881 -4.196507][-4.2485528 -4.2567344 -4.2546706 -4.2532153 -4.2687254 -4.2861128 -4.2900057 -4.2671142 -4.2230616 -4.1883879 -4.19446 -4.2153444 -4.2237005 -4.2155409 -4.1900196][-4.2702274 -4.2756834 -4.271091 -4.2664943 -4.27775 -4.2961321 -4.2978711 -4.2725472 -4.2345271 -4.2124333 -4.2238483 -4.2432771 -4.251359 -4.2438059 -4.2183065][-4.2857218 -4.2851925 -4.2800894 -4.2782588 -4.2872753 -4.3033142 -4.2997041 -4.26485 -4.2285933 -4.221395 -4.2437496 -4.2682915 -4.2846289 -4.28246 -4.2595057][-4.2931924 -4.2870708 -4.282681 -4.2827406 -4.2890177 -4.2997317 -4.2855024 -4.2342925 -4.1920276 -4.2003822 -4.2405095 -4.2776318 -4.3066125 -4.3149028 -4.300055][-4.2916389 -4.2761931 -4.2693062 -4.2674274 -4.2676239 -4.2689872 -4.2365446 -4.163053 -4.1089654 -4.1330376 -4.1982951 -4.2549191 -4.298244 -4.3183842 -4.3188357][-4.2670813 -4.2410989 -4.2287536 -4.2245269 -4.2173734 -4.200418 -4.1412992 -4.0378113 -3.9674461 -4.0201244 -4.1252308 -4.2048922 -4.2609024 -4.2915773 -4.302505][-4.2184672 -4.188983 -4.1747475 -4.170794 -4.1556125 -4.1172953 -4.0330787 -3.9096158 -3.8354325 -3.9245272 -4.066278 -4.1608663 -4.2187281 -4.2497778 -4.2617035][-4.1749935 -4.156076 -4.1530714 -4.1592264 -4.1473422 -4.1109481 -4.0388923 -3.9407897 -3.8924341 -3.9761877 -4.0978165 -4.1700163 -4.2028522 -4.2185888 -4.2230678][-4.1840773 -4.1828489 -4.1921811 -4.2074137 -4.2071438 -4.186645 -4.1387787 -4.0745182 -4.0458016 -4.1006136 -4.1761203 -4.2158165 -4.22413 -4.2261658 -4.2252016][-4.2210231 -4.2287478 -4.2385335 -4.2562919 -4.2647805 -4.252923 -4.217442 -4.1703577 -4.1462154 -4.1755967 -4.219512 -4.2440639 -4.2465582 -4.2461247 -4.2495375][-4.2633152 -4.2672858 -4.2666874 -4.2793555 -4.2900615 -4.28219 -4.2543912 -4.2174478 -4.1971903 -4.2116418 -4.2392206 -4.2571926 -4.2611146 -4.26343 -4.2690639][-4.2857909 -4.2826343 -4.2739019 -4.2811055 -4.2893224 -4.2850466 -4.2664194 -4.2409868 -4.2274232 -4.2341175 -4.250309 -4.2624388 -4.2660146 -4.2684679 -4.2753119][-4.260088 -4.2528038 -4.2406359 -4.243784 -4.2508659 -4.248507 -4.2356029 -4.2194748 -4.2106986 -4.2142906 -4.2244844 -4.2327809 -4.23429 -4.2356458 -4.2410984][-4.2300434 -4.2251606 -4.2118869 -4.2122808 -4.2191186 -4.2185855 -4.2106223 -4.201436 -4.1967258 -4.1984644 -4.2054663 -4.2111144 -4.2102494 -4.2083926 -4.2105527]]...]
INFO - root - 2017-12-06 10:28:27.579663: step 11110, loss = 2.11, batch loss = 2.06 (18.4 examples/sec; 0.435 sec/batch; 38h:48m:22s remains)
INFO - root - 2017-12-06 10:28:31.869667: step 11120, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 37h:54m:41s remains)
INFO - root - 2017-12-06 10:28:36.211430: step 11130, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.445 sec/batch; 39h:41m:49s remains)
INFO - root - 2017-12-06 10:28:40.501766: step 11140, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.420 sec/batch; 37h:31m:27s remains)
INFO - root - 2017-12-06 10:28:44.761243: step 11150, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 39h:41m:19s remains)
INFO - root - 2017-12-06 10:28:49.079627: step 11160, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.446 sec/batch; 39h:46m:11s remains)
INFO - root - 2017-12-06 10:28:53.492680: step 11170, loss = 2.08, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 39h:45m:13s remains)
INFO - root - 2017-12-06 10:28:57.873154: step 11180, loss = 2.11, batch loss = 2.05 (18.7 examples/sec; 0.428 sec/batch; 38h:11m:45s remains)
INFO - root - 2017-12-06 10:29:02.345317: step 11190, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 37h:57m:51s remains)
INFO - root - 2017-12-06 10:29:06.215371: step 11200, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.422 sec/batch; 37h:41m:20s remains)
2017-12-06 10:29:06.790456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1397848 -4.150106 -4.1674976 -4.1879263 -4.1998477 -4.2088542 -4.2197862 -4.2278571 -4.2330165 -4.2337928 -4.2377052 -4.2432842 -4.2318192 -4.2215695 -4.2104897][-4.1747274 -4.1821723 -4.1979132 -4.2153983 -4.2224903 -4.2250247 -4.2300873 -4.2346082 -4.2308416 -4.2200484 -4.2151017 -4.2155261 -4.2010989 -4.1905265 -4.1789021][-4.1857891 -4.1992855 -4.2218776 -4.2407947 -4.2454739 -4.2404509 -4.2369394 -4.2295351 -4.2102504 -4.1901712 -4.179338 -4.1791706 -4.1692615 -4.1638527 -4.1619697][-4.1935596 -4.2112904 -4.2319274 -4.2469697 -4.24985 -4.2386103 -4.2208371 -4.2010603 -4.1769686 -4.1562591 -4.1508603 -4.1595044 -4.1631255 -4.1737514 -4.1856441][-4.1888661 -4.2017493 -4.2165761 -4.2295461 -4.2287273 -4.2126975 -4.187686 -4.1684632 -4.1545653 -4.1460037 -4.154582 -4.1723642 -4.1837997 -4.19821 -4.2092018][-4.169672 -4.1768341 -4.1867738 -4.1984224 -4.1956844 -4.1760864 -4.1509085 -4.1403894 -4.1377726 -4.1353288 -4.1486368 -4.1743603 -4.1887035 -4.2006216 -4.2062631][-4.1448669 -4.1489911 -4.155622 -4.161314 -4.1533494 -4.1291561 -4.0988507 -4.0923357 -4.1000309 -4.1030469 -4.1209626 -4.1565809 -4.17379 -4.18387 -4.1902308][-4.1405187 -4.1259685 -4.1024733 -4.0819216 -4.0743876 -4.0633478 -4.0360417 -4.0387688 -4.0667553 -4.0885253 -4.1155877 -4.1563306 -4.1704593 -4.177299 -4.1806583][-4.1225605 -4.0846949 -4.0275259 -3.9833417 -4.0091753 -4.0528951 -4.0604744 -4.0708985 -4.1000628 -4.1227841 -4.14072 -4.1678872 -4.1746292 -4.1756625 -4.1745629][-4.0896406 -4.0636845 -4.0285816 -4.0125027 -4.0602264 -4.1231918 -4.1456189 -4.1444969 -4.1551924 -4.166419 -4.17194 -4.185235 -4.1868505 -4.1833329 -4.1804285][-4.0997477 -4.1020226 -4.1059852 -4.119597 -4.1530914 -4.1933031 -4.2046504 -4.1920638 -4.1920838 -4.2028565 -4.2071562 -4.2177958 -4.2161107 -4.2061563 -4.2020478][-4.1338658 -4.1448541 -4.162343 -4.17746 -4.19682 -4.2135339 -4.2143307 -4.2060366 -4.2143865 -4.2332158 -4.2452145 -4.2568345 -4.2565122 -4.2440329 -4.2330532][-4.1450596 -4.1570048 -4.1794028 -4.1908593 -4.1966085 -4.2010403 -4.1995959 -4.2017064 -4.2242775 -4.2515717 -4.2729354 -4.2882853 -4.2882357 -4.274796 -4.261477][-4.1443605 -4.1583509 -4.1836843 -4.1906934 -4.1865559 -4.1844983 -4.18345 -4.1950779 -4.2298388 -4.2664356 -4.2903953 -4.3034744 -4.2996864 -4.2828321 -4.27041][-4.1405835 -4.15176 -4.17042 -4.172936 -4.1622343 -4.161664 -4.174273 -4.1978989 -4.241425 -4.2774363 -4.2939692 -4.299283 -4.291399 -4.2725544 -4.263308]]...]
INFO - root - 2017-12-06 10:29:11.096310: step 11210, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:09m:13s remains)
INFO - root - 2017-12-06 10:29:15.387493: step 11220, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.417 sec/batch; 37h:13m:38s remains)
INFO - root - 2017-12-06 10:29:19.657405: step 11230, loss = 2.10, batch loss = 2.04 (19.1 examples/sec; 0.418 sec/batch; 37h:16m:54s remains)
INFO - root - 2017-12-06 10:29:23.953978: step 11240, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 38h:02m:17s remains)
INFO - root - 2017-12-06 10:29:28.255106: step 11250, loss = 2.05, batch loss = 1.99 (19.6 examples/sec; 0.409 sec/batch; 36h:29m:09s remains)
INFO - root - 2017-12-06 10:29:32.570224: step 11260, loss = 2.08, batch loss = 2.03 (19.2 examples/sec; 0.417 sec/batch; 37h:10m:39s remains)
INFO - root - 2017-12-06 10:29:36.871678: step 11270, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 38h:33m:59s remains)
INFO - root - 2017-12-06 10:29:41.271156: step 11280, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.429 sec/batch; 38h:16m:10s remains)
INFO - root - 2017-12-06 10:29:45.611138: step 11290, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 39h:17m:59s remains)
INFO - root - 2017-12-06 10:29:49.670261: step 11300, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.456 sec/batch; 40h:39m:55s remains)
2017-12-06 10:29:50.123461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1931949 -4.204843 -4.2002778 -4.186367 -4.1731853 -4.1791749 -4.1910763 -4.191752 -4.1864228 -4.178525 -4.1678433 -4.1630821 -4.1762581 -4.197669 -4.2281594][-4.1866264 -4.2069583 -4.2113466 -4.1954131 -4.1704435 -4.1660767 -4.178925 -4.1859865 -4.1845279 -4.1756063 -4.1656208 -4.1630793 -4.1799917 -4.20479 -4.2327709][-4.1715636 -4.1996794 -4.2152214 -4.2009296 -4.1668205 -4.1528983 -4.1644125 -4.1774144 -4.1818628 -4.1696043 -4.1553583 -4.1550035 -4.1787343 -4.2080975 -4.2329373][-4.1493483 -4.1829505 -4.2071605 -4.1998534 -4.16488 -4.1362085 -4.134172 -4.1568408 -4.1783185 -4.1690664 -4.1510062 -4.1516023 -4.1791635 -4.2091384 -4.2286386][-4.139607 -4.1750407 -4.204206 -4.2004142 -4.1579947 -4.1101389 -4.0900106 -4.1262016 -4.1723795 -4.1749773 -4.1559715 -4.1517529 -4.1730428 -4.1978617 -4.2140741][-4.155046 -4.1835575 -4.2048907 -4.1910286 -4.1298494 -4.0532575 -4.0205784 -4.0745239 -4.1510053 -4.17155 -4.1552806 -4.14801 -4.1603131 -4.1774697 -4.1943579][-4.1804919 -4.194973 -4.20096 -4.1709433 -4.083271 -3.9709222 -3.9248867 -4.0030031 -4.1094108 -4.1482863 -4.1382051 -4.1385584 -4.1484842 -4.1596303 -4.1804748][-4.2013612 -4.2053452 -4.199203 -4.1535873 -4.0517349 -3.915916 -3.8569934 -3.9578648 -4.0859928 -4.1354218 -4.132133 -4.1396513 -4.1551118 -4.1663723 -4.1858535][-4.2120495 -4.2119875 -4.2019167 -4.1601186 -4.0782967 -3.9710479 -3.9268153 -4.00597 -4.1096554 -4.1478791 -4.1440558 -4.1512628 -4.1670852 -4.1819735 -4.1990962][-4.2176728 -4.2167692 -4.2122693 -4.1859131 -4.1369581 -4.0732489 -4.0494561 -4.0927229 -4.1503911 -4.1704726 -4.165483 -4.1652765 -4.17437 -4.1888161 -4.204339][-4.2103119 -4.20812 -4.2089987 -4.1997623 -4.1793194 -4.1463 -4.1304784 -4.1488714 -4.1805873 -4.1969333 -4.1978035 -4.1911354 -4.1899872 -4.1973462 -4.205195][-4.1965284 -4.1882949 -4.1910987 -4.1945639 -4.1939359 -4.1820979 -4.1696849 -4.1720643 -4.1925139 -4.2111878 -4.2161345 -4.2069426 -4.196578 -4.1932087 -4.1924005][-4.1936107 -4.1814713 -4.1846604 -4.1924839 -4.1971478 -4.1897745 -4.1764231 -4.1736226 -4.1895385 -4.2105732 -4.218987 -4.2095509 -4.1892114 -4.1740828 -4.167479][-4.2052712 -4.1932917 -4.1933579 -4.1991992 -4.2012072 -4.1896925 -4.1754131 -4.1751318 -4.1938992 -4.2156472 -4.2264323 -4.2173014 -4.1923976 -4.1683969 -4.1561294][-4.2204804 -4.2164254 -4.217824 -4.2203231 -4.2168875 -4.2014818 -4.1858015 -4.1872873 -4.2067327 -4.2249031 -4.234046 -4.2248349 -4.2025685 -4.1796412 -4.1639118]]...]
INFO - root - 2017-12-06 10:29:54.459336: step 11310, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.434 sec/batch; 38h:40m:55s remains)
INFO - root - 2017-12-06 10:29:58.756599: step 11320, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 37h:30m:08s remains)
INFO - root - 2017-12-06 10:30:03.067554: step 11330, loss = 2.04, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 41h:28m:09s remains)
INFO - root - 2017-12-06 10:30:07.325386: step 11340, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 37h:29m:49s remains)
INFO - root - 2017-12-06 10:30:11.664537: step 11350, loss = 2.10, batch loss = 2.04 (18.5 examples/sec; 0.433 sec/batch; 38h:35m:11s remains)
INFO - root - 2017-12-06 10:30:15.971868: step 11360, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 37h:57m:13s remains)
INFO - root - 2017-12-06 10:30:20.294898: step 11370, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:10m:01s remains)
INFO - root - 2017-12-06 10:30:24.749507: step 11380, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.441 sec/batch; 39h:18m:46s remains)
INFO - root - 2017-12-06 10:30:28.982280: step 11390, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 37h:48m:54s remains)
INFO - root - 2017-12-06 10:30:33.081377: step 11400, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 38h:50m:47s remains)
2017-12-06 10:30:33.590933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3045244 -4.3062754 -4.3081145 -4.3118572 -4.3165283 -4.3147016 -4.3066826 -4.2965136 -4.2912064 -4.292891 -4.3031483 -4.3215628 -4.3321171 -4.3342714 -4.3329558][-4.2761827 -4.2757463 -4.274797 -4.2780032 -4.2860956 -4.2875772 -4.2795181 -4.2683058 -4.2634315 -4.2655616 -4.2770925 -4.3003583 -4.3179417 -4.3236723 -4.3222938][-4.2424712 -4.2402515 -4.2371492 -4.2397523 -4.2467108 -4.2460461 -4.234231 -4.2219944 -4.2211523 -4.2249584 -4.2334061 -4.258419 -4.2834692 -4.298655 -4.3038116][-4.1931472 -4.1878486 -4.1790004 -4.1764836 -4.1818042 -4.1775775 -4.1593647 -4.1434183 -4.1531157 -4.1688147 -4.17591 -4.1970015 -4.2276731 -4.2529736 -4.2699194][-4.1405945 -4.1257238 -4.1020994 -4.08043 -4.0735297 -4.0634041 -4.0407376 -4.0311646 -4.0651927 -4.1044393 -4.1191983 -4.1370759 -4.1694789 -4.200326 -4.2287126][-4.1143441 -4.0922194 -4.0540013 -3.9982166 -3.9473794 -3.8976774 -3.8505695 -3.8575509 -3.9460282 -4.033349 -4.0685883 -4.0881348 -4.1218605 -4.1581078 -4.195581][-4.1108322 -4.0893621 -4.0524268 -3.9798069 -3.8821592 -3.7631865 -3.6495633 -3.66044 -3.8160577 -3.9646986 -4.0369487 -4.0717077 -4.1084609 -4.1482768 -4.1877918][-4.1144357 -4.0989943 -4.081028 -4.0233078 -3.9196959 -3.7657616 -3.594635 -3.5870223 -3.7714455 -3.9493341 -4.0459247 -4.0935421 -4.1339431 -4.1728368 -4.2040849][-4.1084471 -4.1057081 -4.1148877 -4.0887437 -4.0146136 -3.8930464 -3.7534268 -3.7329021 -3.8647065 -4.00334 -4.0835776 -4.1276388 -4.1664453 -4.2027106 -4.2262673][-4.1016855 -4.1140103 -4.1434689 -4.1491632 -4.107861 -4.0206852 -3.9209635 -3.8962963 -3.97647 -4.0710268 -4.1277823 -4.1621327 -4.1982007 -4.2298694 -4.2447591][-4.0826774 -4.1060858 -4.1527181 -4.185864 -4.1746483 -4.118813 -4.0461893 -4.020947 -4.066545 -4.1264529 -4.1663866 -4.1950173 -4.22941 -4.2534552 -4.2610307][-4.1037755 -4.1252594 -4.1713958 -4.2134485 -4.2219868 -4.1935172 -4.1441078 -4.1198869 -4.1437616 -4.1821613 -4.2101207 -4.2358413 -4.2670426 -4.2834964 -4.2833762][-4.1660872 -4.1820607 -4.217309 -4.2509441 -4.2625346 -4.2517948 -4.2233567 -4.2059107 -4.218977 -4.2430606 -4.2588015 -4.27794 -4.299428 -4.3075533 -4.3044333][-4.2286549 -4.2393312 -4.2631154 -4.2846475 -4.2929368 -4.290781 -4.27381 -4.2604852 -4.2681041 -4.2831287 -4.2910829 -4.3040609 -4.3180652 -4.32177 -4.320159][-4.2716112 -4.278347 -4.2942328 -4.307169 -4.3115635 -4.3135734 -4.3053403 -4.2944694 -4.2961946 -4.3051305 -4.3124776 -4.3222132 -4.3320422 -4.3346376 -4.3333111]]...]
INFO - root - 2017-12-06 10:30:37.853268: step 11410, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 37h:49m:13s remains)
INFO - root - 2017-12-06 10:30:42.154112: step 11420, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 37h:59m:32s remains)
INFO - root - 2017-12-06 10:30:46.515311: step 11430, loss = 2.07, batch loss = 2.02 (18.2 examples/sec; 0.441 sec/batch; 39h:18m:24s remains)
INFO - root - 2017-12-06 10:30:50.798695: step 11440, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.409 sec/batch; 36h:30m:14s remains)
INFO - root - 2017-12-06 10:30:55.130066: step 11450, loss = 2.04, batch loss = 1.99 (19.4 examples/sec; 0.413 sec/batch; 36h:47m:32s remains)
INFO - root - 2017-12-06 10:30:59.496102: step 11460, loss = 2.07, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:04m:28s remains)
INFO - root - 2017-12-06 10:31:03.781973: step 11470, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.438 sec/batch; 39h:05m:52s remains)
INFO - root - 2017-12-06 10:31:08.096582: step 11480, loss = 2.10, batch loss = 2.04 (19.2 examples/sec; 0.417 sec/batch; 37h:08m:50s remains)
INFO - root - 2017-12-06 10:31:12.292867: step 11490, loss = 2.11, batch loss = 2.05 (27.4 examples/sec; 0.292 sec/batch; 26h:04m:09s remains)
INFO - root - 2017-12-06 10:31:16.456321: step 11500, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 37h:48m:27s remains)
2017-12-06 10:31:16.939558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2605119 -4.2481842 -4.2344766 -4.2140622 -4.1833267 -4.1532207 -4.1507564 -4.165236 -4.1850653 -4.2159224 -4.2456603 -4.2604637 -4.2662082 -4.2792044 -4.28545][-4.2531567 -4.2397809 -4.2250147 -4.2035542 -4.1665926 -4.1263142 -4.1172686 -4.1315818 -4.1546626 -4.1927481 -4.2339721 -4.2570462 -4.2675314 -4.2822404 -4.2887383][-4.2414384 -4.2322135 -4.2192683 -4.2008686 -4.1625991 -4.1119576 -4.0953126 -4.1107841 -4.1368837 -4.1790771 -4.2302856 -4.262722 -4.2777395 -4.2910085 -4.2973795][-4.2229061 -4.2212667 -4.2121606 -4.1990962 -4.1625319 -4.1035414 -4.0766368 -4.0916557 -4.1205349 -4.1619468 -4.2187419 -4.2623568 -4.2847929 -4.2963104 -4.3045468][-4.2108316 -4.2124066 -4.2088294 -4.1971626 -4.1595421 -4.0949259 -4.0594964 -4.0792141 -4.1186643 -4.1583295 -4.2120323 -4.2583871 -4.2846961 -4.2936559 -4.3052249][-4.1965289 -4.2019773 -4.2062511 -4.1954155 -4.152606 -4.0789981 -4.0351019 -4.0650768 -4.1282072 -4.1725497 -4.2210584 -4.2603493 -4.2832246 -4.2876244 -4.29836][-4.1635623 -4.1790805 -4.1967583 -4.1906209 -4.1425786 -4.0538807 -3.9869061 -4.0175924 -4.1124487 -4.1767936 -4.2278037 -4.2666059 -4.2856493 -4.2848525 -4.2897611][-4.1047015 -4.1372766 -4.17467 -4.1809897 -4.1339741 -4.0235205 -3.9137371 -3.928206 -4.0551462 -4.1523151 -4.2189507 -4.2701077 -4.2953777 -4.291779 -4.2889972][-4.0263863 -4.0745368 -4.1343937 -4.1593857 -4.1229086 -3.9938369 -3.8272164 -3.8023858 -3.9574759 -4.1022816 -4.1987982 -4.268291 -4.3019876 -4.3006368 -4.2931328][-3.9673429 -4.0239925 -4.0974469 -4.1432867 -4.132338 -4.0121346 -3.8114913 -3.7322214 -3.8806512 -4.0530448 -4.1703162 -4.2526026 -4.2960935 -4.3031354 -4.2978735][-3.9861336 -4.0337534 -4.1005383 -4.1514959 -4.1646886 -4.0812559 -3.9118257 -3.8107679 -3.9019849 -4.0503321 -4.1624174 -4.2414813 -4.2867408 -4.300878 -4.2996974][-4.0527387 -4.0868835 -4.1361928 -4.1790457 -4.1983271 -4.1545954 -4.0445929 -3.9553931 -3.9908903 -4.0886979 -4.1776094 -4.2436419 -4.2824192 -4.2997642 -4.3021665][-4.1268363 -4.1466746 -4.176754 -4.2085257 -4.2314782 -4.215497 -4.1531348 -4.0874987 -4.0922666 -4.1474333 -4.2122836 -4.2645283 -4.2926269 -4.3048983 -4.3066368][-4.2026906 -4.2101655 -4.2279439 -4.2488751 -4.2716694 -4.2717543 -4.2397366 -4.19419 -4.1828866 -4.2110643 -4.2541881 -4.288517 -4.3051147 -4.3099241 -4.3088508][-4.2552485 -4.256403 -4.2665458 -4.2794671 -4.2986736 -4.3062696 -4.291204 -4.2616668 -4.2439 -4.2551308 -4.2813363 -4.3020296 -4.3109879 -4.3122711 -4.3086119]]...]
INFO - root - 2017-12-06 10:31:21.187324: step 11510, loss = 2.08, batch loss = 2.02 (19.3 examples/sec; 0.416 sec/batch; 37h:02m:59s remains)
INFO - root - 2017-12-06 10:31:25.504930: step 11520, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 38h:29m:53s remains)
INFO - root - 2017-12-06 10:31:29.846953: step 11530, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 38h:29m:30s remains)
INFO - root - 2017-12-06 10:31:34.146360: step 11540, loss = 2.08, batch loss = 2.03 (19.1 examples/sec; 0.419 sec/batch; 37h:23m:46s remains)
INFO - root - 2017-12-06 10:31:38.470658: step 11550, loss = 2.07, batch loss = 2.02 (19.2 examples/sec; 0.417 sec/batch; 37h:11m:46s remains)
INFO - root - 2017-12-06 10:31:42.752779: step 11560, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 37h:36m:55s remains)
INFO - root - 2017-12-06 10:31:47.105080: step 11570, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 38h:29m:37s remains)
INFO - root - 2017-12-06 10:31:51.413017: step 11580, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.425 sec/batch; 37h:55m:05s remains)
INFO - root - 2017-12-06 10:31:55.585186: step 11590, loss = 2.07, batch loss = 2.01 (27.7 examples/sec; 0.289 sec/batch; 25h:45m:07s remains)
INFO - root - 2017-12-06 10:31:59.720824: step 11600, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.423 sec/batch; 37h:43m:42s remains)
2017-12-06 10:32:00.268601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3277378 -4.3240466 -4.3245225 -4.3278742 -4.3329725 -4.3388004 -4.3442359 -4.348381 -4.3530931 -4.3600631 -4.3633833 -4.3621869 -4.3601871 -4.3586826 -4.3582964][-4.3036432 -4.296896 -4.2927785 -4.2931561 -4.29564 -4.3027844 -4.3120823 -4.3182349 -4.3246455 -4.3354573 -4.343102 -4.3455639 -4.3467388 -4.3481665 -4.3502851][-4.2713723 -4.2601724 -4.2541828 -4.2536507 -4.2521377 -4.2600102 -4.2726121 -4.2788515 -4.2864995 -4.3014646 -4.3142276 -4.322247 -4.3292761 -4.3369536 -4.3440423][-4.2252955 -4.2054844 -4.2005277 -4.2038231 -4.2048678 -4.2151394 -4.2289276 -4.2353849 -4.2446175 -4.2612643 -4.2766447 -4.2896051 -4.2988276 -4.3092508 -4.3230329][-4.1823578 -4.151433 -4.1439829 -4.1517715 -4.1562362 -4.1642694 -4.1766229 -4.18512 -4.1983128 -4.219378 -4.2397394 -4.2566795 -4.2646685 -4.2760763 -4.2962441][-4.1503687 -4.1122475 -4.0968847 -4.0987821 -4.0977755 -4.0960574 -4.1033306 -4.1134739 -4.133554 -4.1645827 -4.1939869 -4.2182941 -4.2311206 -4.2451448 -4.2688065][-4.1329827 -4.0922151 -4.0688181 -4.0533819 -4.0361214 -4.0171375 -4.0067487 -4.0073338 -4.03531 -4.0852652 -4.1313763 -4.1726937 -4.2017269 -4.2216277 -4.2471557][-4.1339507 -4.0952997 -4.0679913 -4.0372486 -4.0012932 -3.9582386 -3.913547 -3.8903191 -3.9247897 -3.9936647 -4.0612955 -4.1222262 -4.1678834 -4.1966491 -4.225534][-4.1440763 -4.1135206 -4.0927529 -4.06124 -4.0188923 -3.9658916 -3.9007819 -3.8617749 -3.8926432 -3.9562087 -4.0257559 -4.0898643 -4.1442466 -4.1822209 -4.2137256][-4.1539812 -4.1291413 -4.1171513 -4.094933 -4.0583944 -4.0113888 -3.9538124 -3.9203143 -3.93739 -3.9806085 -4.0345292 -4.0908203 -4.1453 -4.1845036 -4.2139196][-4.1682224 -4.1429629 -4.13432 -4.1217456 -4.0967088 -4.0607128 -4.017364 -3.9919317 -3.9977751 -4.022974 -4.0628009 -4.1107373 -4.15712 -4.19333 -4.2216682][-4.1940126 -4.1696005 -4.1614146 -4.154614 -4.1380048 -4.1132822 -4.0824771 -4.0638418 -4.0637093 -4.0772724 -4.1053309 -4.1426611 -4.1791463 -4.212492 -4.2401643][-4.2322335 -4.2140484 -4.2105718 -4.208652 -4.1984529 -4.1824288 -4.1593461 -4.1433129 -4.137711 -4.1437688 -4.1621284 -4.187789 -4.21524 -4.2448449 -4.2706432][-4.27602 -4.26496 -4.265841 -4.2677794 -4.2629986 -4.2531443 -4.237083 -4.2236314 -4.2170124 -4.2184515 -4.2275453 -4.2420788 -4.2603793 -4.2824488 -4.303122][-4.3093095 -4.3047271 -4.3066096 -4.3083844 -4.3056493 -4.3009105 -4.291533 -4.2834673 -4.2805352 -4.2809606 -4.2842584 -4.2913861 -4.3012414 -4.3149219 -4.3282385]]...]
INFO - root - 2017-12-06 10:32:04.615784: step 11610, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 37h:46m:34s remains)
INFO - root - 2017-12-06 10:32:08.875451: step 11620, loss = 2.10, batch loss = 2.05 (19.0 examples/sec; 0.420 sec/batch; 37h:28m:17s remains)
INFO - root - 2017-12-06 10:32:13.184221: step 11630, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.429 sec/batch; 38h:15m:50s remains)
INFO - root - 2017-12-06 10:32:17.494202: step 11640, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 38h:48m:20s remains)
INFO - root - 2017-12-06 10:32:21.761440: step 11650, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 39h:40m:29s remains)
INFO - root - 2017-12-06 10:32:26.078982: step 11660, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 38h:39m:59s remains)
INFO - root - 2017-12-06 10:32:30.395797: step 11670, loss = 2.11, batch loss = 2.06 (18.2 examples/sec; 0.440 sec/batch; 39h:15m:02s remains)
INFO - root - 2017-12-06 10:32:34.664521: step 11680, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:24m:20s remains)
INFO - root - 2017-12-06 10:32:38.716159: step 11690, loss = 2.10, batch loss = 2.04 (22.8 examples/sec; 0.351 sec/batch; 31h:16m:40s remains)
INFO - root - 2017-12-06 10:32:43.029694: step 11700, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.436 sec/batch; 38h:48m:35s remains)
2017-12-06 10:32:43.571380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.169095 -4.1751623 -4.19838 -4.2153697 -4.221004 -4.217443 -4.1907239 -4.163991 -4.1649857 -4.1782713 -4.1790257 -4.1558414 -4.1390915 -4.152885 -4.18824][-4.1586571 -4.1572266 -4.1774282 -4.1965909 -4.2048082 -4.2040215 -4.1835523 -4.1679244 -4.1777225 -4.1970434 -4.2075467 -4.1893449 -4.169457 -4.1770997 -4.2013021][-4.150466 -4.1482053 -4.1733518 -4.1982365 -4.2070289 -4.2053657 -4.1906428 -4.1807165 -4.1908855 -4.2119455 -4.2279959 -4.2129846 -4.1920214 -4.1955009 -4.2115369][-4.1396217 -4.1371174 -4.1666212 -4.1979008 -4.2092037 -4.2017741 -4.1839876 -4.176569 -4.1901522 -4.2144742 -4.2351589 -4.2281785 -4.21494 -4.217082 -4.2266045][-4.1281166 -4.1242132 -4.1512613 -4.1787057 -4.1829472 -4.1645017 -4.1389852 -4.1352506 -4.1637063 -4.1977825 -4.2236643 -4.2248626 -4.2243724 -4.2338476 -4.2413063][-4.0961795 -4.0882158 -4.1118431 -4.135375 -4.1286907 -4.0869622 -4.0447731 -4.0500383 -4.1049094 -4.1590161 -4.1950016 -4.2026992 -4.2117529 -4.2287197 -4.2374105][-4.0560374 -4.0409927 -4.06253 -4.0815711 -4.0562382 -3.9821672 -3.9166679 -3.9424725 -4.0364122 -4.1166015 -4.1634645 -4.1765709 -4.189518 -4.2065349 -4.20874][-4.05495 -4.0369539 -4.0505209 -4.0573874 -4.0130134 -3.9130871 -3.8307829 -3.8795879 -4.0053449 -4.0998473 -4.1481476 -4.1617727 -4.1700616 -4.1777883 -4.1705289][-4.0993042 -4.0881977 -4.1006579 -4.1017294 -4.0587368 -3.9694333 -3.902982 -3.9504411 -4.059175 -4.131803 -4.1639042 -4.170929 -4.1707263 -4.1648636 -4.1472306][-4.1422381 -4.1426415 -4.1630154 -4.1690946 -4.1381783 -4.0761147 -4.0334239 -4.0704336 -4.1447787 -4.1893992 -4.2047982 -4.2024517 -4.1958761 -4.1851263 -4.1658087][-4.1790047 -4.1881647 -4.2143335 -4.2267776 -4.2076168 -4.1646538 -4.1331754 -4.15033 -4.1901922 -4.2169108 -4.2258735 -4.219873 -4.2155237 -4.2109647 -4.200985][-4.2071586 -4.218708 -4.2425976 -4.2576523 -4.2442861 -4.2108068 -4.1795421 -4.1787148 -4.1923189 -4.2064338 -4.2151942 -4.2165575 -4.2198443 -4.226006 -4.2305937][-4.2442927 -4.2483783 -4.2609768 -4.2664151 -4.2497516 -4.2159953 -4.1844897 -4.175797 -4.1783528 -4.1892529 -4.2061086 -4.2174306 -4.2278709 -4.2405925 -4.2523675][-4.2746634 -4.266396 -4.2626929 -4.2546716 -4.2297568 -4.1926751 -4.1631279 -4.1563663 -4.1591978 -4.1753321 -4.2022977 -4.22081 -4.2323608 -4.2427526 -4.2523265][-4.2802982 -4.2613506 -4.2477765 -4.2358274 -4.210813 -4.1763792 -4.1494813 -4.143446 -4.1457806 -4.166533 -4.2001343 -4.2197971 -4.2256889 -4.2260971 -4.2265229]]...]
INFO - root - 2017-12-06 10:32:47.938857: step 11710, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:03m:26s remains)
INFO - root - 2017-12-06 10:32:52.196922: step 11720, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 38h:20m:03s remains)
INFO - root - 2017-12-06 10:32:56.473711: step 11730, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.415 sec/batch; 36h:57m:38s remains)
INFO - root - 2017-12-06 10:33:00.681031: step 11740, loss = 2.08, batch loss = 2.02 (19.6 examples/sec; 0.408 sec/batch; 36h:18m:47s remains)
INFO - root - 2017-12-06 10:33:05.047780: step 11750, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 38h:50m:52s remains)
INFO - root - 2017-12-06 10:33:09.292769: step 11760, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 36h:57m:17s remains)
INFO - root - 2017-12-06 10:33:13.631733: step 11770, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.438 sec/batch; 38h:59m:10s remains)
