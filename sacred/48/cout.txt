INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "48"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 06:10:11.773245: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:10:11.773286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:10:11.773293: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:10:11.773297: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:10:11.773301: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:10:12.327162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 06:10:12.327243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 06:10:12.327268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 06:10:12.327287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 06:10:16.079361: step 0, loss = 2.03, batch loss = 1.97 (3.0 examples/sec; 2.630 sec/batch; 242h:55m:48s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 06:10:19.246392: step 10, loss = 2.05, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:48m:38s remains)
INFO - root - 2017-12-05 06:10:21.479218: step 20, loss = 2.07, batch loss = 2.01 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:43s remains)
INFO - root - 2017-12-05 06:10:23.763023: step 30, loss = 2.04, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:44s remains)
INFO - root - 2017-12-05 06:10:25.928833: step 40, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:55s remains)
INFO - root - 2017-12-05 06:10:28.071461: step 50, loss = 2.05, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:54s remains)
INFO - root - 2017-12-05 06:10:30.189189: step 60, loss = 2.04, batch loss = 1.98 (38.2 examples/sec; 0.209 sec/batch; 19h:20m:37s remains)
INFO - root - 2017-12-05 06:10:32.378757: step 70, loss = 2.04, batch loss = 1.98 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:30s remains)
INFO - root - 2017-12-05 06:10:34.492416: step 80, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:40s remains)
INFO - root - 2017-12-05 06:10:36.734961: step 90, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:39m:01s remains)
INFO - root - 2017-12-05 06:10:38.911991: step 100, loss = 2.07, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:22s remains)
INFO - root - 2017-12-05 06:10:41.156162: step 110, loss = 2.05, batch loss = 1.99 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:18s remains)
INFO - root - 2017-12-05 06:10:43.314805: step 120, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.236 sec/batch; 21h:49m:21s remains)
INFO - root - 2017-12-05 06:10:45.505974: step 130, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:24m:26s remains)
INFO - root - 2017-12-05 06:10:47.635205: step 140, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:04s remains)
INFO - root - 2017-12-05 06:10:49.774027: step 150, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:49s remains)
INFO - root - 2017-12-05 06:10:51.920815: step 160, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:32s remains)
INFO - root - 2017-12-05 06:10:54.036146: step 170, loss = 2.06, batch loss = 2.00 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:44s remains)
INFO - root - 2017-12-05 06:10:56.203125: step 180, loss = 2.05, batch loss = 2.00 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:38s remains)
INFO - root - 2017-12-05 06:10:58.371249: step 190, loss = 2.04, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:50s remains)
INFO - root - 2017-12-05 06:11:00.506356: step 200, loss = 2.03, batch loss = 1.97 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:43s remains)
INFO - root - 2017-12-05 06:11:02.697022: step 210, loss = 2.07, batch loss = 2.01 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:35s remains)
INFO - root - 2017-12-05 06:11:04.852184: step 220, loss = 2.06, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:17s remains)
INFO - root - 2017-12-05 06:11:06.994485: step 230, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:13s remains)
INFO - root - 2017-12-05 06:11:09.143572: step 240, loss = 2.10, batch loss = 2.04 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:33s remains)
INFO - root - 2017-12-05 06:11:11.297971: step 250, loss = 2.07, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 21h:34m:41s remains)
INFO - root - 2017-12-05 06:11:13.433776: step 260, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:05s remains)
INFO - root - 2017-12-05 06:11:15.572971: step 270, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:47s remains)
INFO - root - 2017-12-05 06:11:17.711733: step 280, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:27s remains)
INFO - root - 2017-12-05 06:11:19.854012: step 290, loss = 2.05, batch loss = 1.99 (38.4 examples/sec; 0.209 sec/batch; 19h:14m:54s remains)
INFO - root - 2017-12-05 06:11:22.000009: step 300, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:48s remains)
INFO - root - 2017-12-05 06:11:24.203795: step 310, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:02s remains)
INFO - root - 2017-12-05 06:11:26.338867: step 320, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-05 06:11:28.481947: step 330, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:10s remains)
INFO - root - 2017-12-05 06:11:30.619949: step 340, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 19h:08m:30s remains)
INFO - root - 2017-12-05 06:11:32.769416: step 350, loss = 2.08, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:07s remains)
INFO - root - 2017-12-05 06:11:34.915598: step 360, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:17s remains)
INFO - root - 2017-12-05 06:11:37.057639: step 370, loss = 2.10, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:57s remains)
INFO - root - 2017-12-05 06:11:39.215081: step 380, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:21s remains)
INFO - root - 2017-12-05 06:11:41.363886: step 390, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.209 sec/batch; 19h:14m:37s remains)
INFO - root - 2017-12-05 06:11:43.525934: step 400, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:13s remains)
INFO - root - 2017-12-05 06:11:45.745981: step 410, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:13s remains)
INFO - root - 2017-12-05 06:11:47.872307: step 420, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:44s remains)
INFO - root - 2017-12-05 06:11:50.022135: step 430, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-05 06:11:52.149247: step 440, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:06s remains)
INFO - root - 2017-12-05 06:11:54.321624: step 450, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:54s remains)
INFO - root - 2017-12-05 06:11:56.495027: step 460, loss = 2.01, batch loss = 1.96 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:11s remains)
INFO - root - 2017-12-05 06:11:58.645465: step 470, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:28s remains)
INFO - root - 2017-12-05 06:12:00.811147: step 480, loss = 2.09, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:28s remains)
INFO - root - 2017-12-05 06:12:02.970055: step 490, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-05 06:12:05.100697: step 500, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:04s remains)
INFO - root - 2017-12-05 06:12:07.326413: step 510, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-05 06:12:09.501778: step 520, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:29s remains)
INFO - root - 2017-12-05 06:12:11.686038: step 530, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:54s remains)
INFO - root - 2017-12-05 06:12:13.865171: step 540, loss = 2.03, batch loss = 1.97 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:24s remains)
INFO - root - 2017-12-05 06:12:16.022225: step 550, loss = 2.02, batch loss = 1.96 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:53s remains)
INFO - root - 2017-12-05 06:12:18.175045: step 560, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-05 06:12:20.333483: step 570, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-05 06:12:22.501126: step 580, loss = 2.06, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:17s remains)
INFO - root - 2017-12-05 06:12:24.639409: step 590, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:28s remains)
INFO - root - 2017-12-05 06:12:26.811506: step 600, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:41s remains)
INFO - root - 2017-12-05 06:12:29.049118: step 610, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:53s remains)
INFO - root - 2017-12-05 06:12:31.198029: step 620, loss = 2.05, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:15s remains)
INFO - root - 2017-12-05 06:12:33.360954: step 630, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:37s remains)
INFO - root - 2017-12-05 06:12:35.528753: step 640, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:42s remains)
INFO - root - 2017-12-05 06:12:37.672379: step 650, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:16s remains)
INFO - root - 2017-12-05 06:12:39.849111: step 660, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:10s remains)
INFO - root - 2017-12-05 06:12:42.021075: step 670, loss = 2.04, batch loss = 1.98 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-05 06:12:44.171964: step 680, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:59s remains)
INFO - root - 2017-12-05 06:12:46.351674: step 690, loss = 2.03, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:04s remains)
INFO - root - 2017-12-05 06:12:48.527899: step 700, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:32s remains)
INFO - root - 2017-12-05 06:12:50.732147: step 710, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:00s remains)
INFO - root - 2017-12-05 06:12:52.887128: step 720, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:32s remains)
INFO - root - 2017-12-05 06:12:55.105095: step 730, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:13s remains)
INFO - root - 2017-12-05 06:12:57.283906: step 740, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-05 06:12:59.429018: step 750, loss = 2.03, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:09s remains)
INFO - root - 2017-12-05 06:13:01.595221: step 760, loss = 2.04, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:11s remains)
INFO - root - 2017-12-05 06:13:03.734172: step 770, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-05 06:13:05.882606: step 780, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:17s remains)
INFO - root - 2017-12-05 06:13:08.055810: step 790, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:46s remains)
INFO - root - 2017-12-05 06:13:10.218987: step 800, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:26s remains)
INFO - root - 2017-12-05 06:13:12.442093: step 810, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:49s remains)
INFO - root - 2017-12-05 06:13:14.592317: step 820, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:45s remains)
INFO - root - 2017-12-05 06:13:16.761614: step 830, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:33s remains)
INFO - root - 2017-12-05 06:13:18.957104: step 840, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:08s remains)
INFO - root - 2017-12-05 06:13:21.141552: step 850, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:44s remains)
INFO - root - 2017-12-05 06:13:23.316888: step 860, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:08s remains)
INFO - root - 2017-12-05 06:13:25.510012: step 870, loss = 2.06, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:49s remains)
INFO - root - 2017-12-05 06:13:27.651153: step 880, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:19s remains)
INFO - root - 2017-12-05 06:13:29.803921: step 890, loss = 2.02, batch loss = 1.96 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:11s remains)
INFO - root - 2017-12-05 06:13:31.968977: step 900, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:27s remains)
INFO - root - 2017-12-05 06:13:34.221939: step 910, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:48s remains)
INFO - root - 2017-12-05 06:13:36.373605: step 920, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-05 06:13:38.539259: step 930, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:41s remains)
INFO - root - 2017-12-05 06:13:40.767183: step 940, loss = 2.07, batch loss = 2.02 (33.3 examples/sec; 0.240 sec/batch; 22h:08m:20s remains)
INFO - root - 2017-12-05 06:13:42.949520: step 950, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:09s remains)
INFO - root - 2017-12-05 06:13:45.125448: step 960, loss = 2.09, batch loss = 2.04 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:32s remains)
INFO - root - 2017-12-05 06:13:47.302664: step 970, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:00s remains)
INFO - root - 2017-12-05 06:13:49.488018: step 980, loss = 2.04, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:05s remains)
INFO - root - 2017-12-05 06:13:51.655787: step 990, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:52s remains)
INFO - root - 2017-12-05 06:13:53.833324: step 1000, loss = 2.04, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-05 06:13:56.085945: step 1010, loss = 2.06, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:39s remains)
INFO - root - 2017-12-05 06:13:58.243361: step 1020, loss = 2.04, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-05 06:14:00.427264: step 1030, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:57s remains)
INFO - root - 2017-12-05 06:14:02.576556: step 1040, loss = 2.10, batch loss = 2.04 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:29s remains)
INFO - root - 2017-12-05 06:14:04.734574: step 1050, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:17s remains)
INFO - root - 2017-12-05 06:14:06.909869: step 1060, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:59s remains)
INFO - root - 2017-12-05 06:14:09.079310: step 1070, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:00s remains)
INFO - root - 2017-12-05 06:14:11.272139: step 1080, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:36s remains)
INFO - root - 2017-12-05 06:14:13.440087: step 1090, loss = 2.03, batch loss = 1.97 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:59s remains)
INFO - root - 2017-12-05 06:14:15.605440: step 1100, loss = 2.05, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:11s remains)
INFO - root - 2017-12-05 06:14:17.852682: step 1110, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:41s remains)
INFO - root - 2017-12-05 06:14:20.023667: step 1120, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:42s remains)
INFO - root - 2017-12-05 06:14:22.197540: step 1130, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-05 06:14:24.405086: step 1140, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-05 06:14:26.593689: step 1150, loss = 2.04, batch loss = 1.99 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:55s remains)
INFO - root - 2017-12-05 06:14:28.763009: step 1160, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.218 sec/batch; 20h:05m:52s remains)
INFO - root - 2017-12-05 06:14:30.916585: step 1170, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:46s remains)
INFO - root - 2017-12-05 06:14:33.069634: step 1180, loss = 2.09, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:49s remains)
INFO - root - 2017-12-05 06:14:35.238280: step 1190, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:03s remains)
INFO - root - 2017-12-05 06:14:37.398496: step 1200, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-05 06:14:39.614050: step 1210, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-05 06:14:41.766360: step 1220, loss = 2.05, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:58s remains)
INFO - root - 2017-12-05 06:14:43.925660: step 1230, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:24s remains)
INFO - root - 2017-12-05 06:14:46.087665: step 1240, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:23m:25s remains)
INFO - root - 2017-12-05 06:14:48.300655: step 1250, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:13s remains)
INFO - root - 2017-12-05 06:14:50.470949: step 1260, loss = 2.04, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:45s remains)
INFO - root - 2017-12-05 06:14:52.636518: step 1270, loss = 2.04, batch loss = 1.98 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:14s remains)
INFO - root - 2017-12-05 06:14:54.872167: step 1280, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 21h:48m:18s remains)
INFO - root - 2017-12-05 06:14:57.058472: step 1290, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:03s remains)
INFO - root - 2017-12-05 06:14:59.232193: step 1300, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:12s remains)
INFO - root - 2017-12-05 06:15:01.509023: step 1310, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:39s remains)
INFO - root - 2017-12-05 06:15:03.668495: step 1320, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:40s remains)
INFO - root - 2017-12-05 06:15:05.838742: step 1330, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:49s remains)
INFO - root - 2017-12-05 06:15:07.997504: step 1340, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.224 sec/batch; 20h:34m:52s remains)
INFO - root - 2017-12-05 06:15:10.212277: step 1350, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:34s remains)
INFO - root - 2017-12-05 06:15:12.410554: step 1360, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:06s remains)
INFO - root - 2017-12-05 06:15:14.581067: step 1370, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:39s remains)
INFO - root - 2017-12-05 06:15:16.769372: step 1380, loss = 2.08, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:52s remains)
INFO - root - 2017-12-05 06:15:18.983098: step 1390, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:46m:27s remains)
INFO - root - 2017-12-05 06:15:21.203566: step 1400, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 21h:16m:55s remains)
INFO - root - 2017-12-05 06:15:23.451383: step 1410, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:29s remains)
INFO - root - 2017-12-05 06:15:25.637057: step 1420, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:21m:44s remains)
INFO - root - 2017-12-05 06:15:27.775283: step 1430, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:05s remains)
INFO - root - 2017-12-05 06:15:29.984986: step 1440, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:21s remains)
INFO - root - 2017-12-05 06:15:32.139253: step 1450, loss = 2.04, batch loss = 1.98 (38.1 examples/sec; 0.210 sec/batch; 19h:17m:27s remains)
INFO - root - 2017-12-05 06:15:34.333421: step 1460, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:46s remains)
INFO - root - 2017-12-05 06:15:36.501553: step 1470, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-05 06:15:38.666847: step 1480, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:15s remains)
INFO - root - 2017-12-05 06:15:40.851692: step 1490, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:49m:24s remains)
INFO - root - 2017-12-05 06:15:43.031080: step 1500, loss = 2.07, batch loss = 2.01 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:14s remains)
INFO - root - 2017-12-05 06:15:45.277272: step 1510, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:35s remains)
INFO - root - 2017-12-05 06:15:47.494244: step 1520, loss = 2.07, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:00s remains)
INFO - root - 2017-12-05 06:15:49.657266: step 1530, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:29s remains)
INFO - root - 2017-12-05 06:15:51.857455: step 1540, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:17m:03s remains)
INFO - root - 2017-12-05 06:15:54.039845: step 1550, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:01s remains)
INFO - root - 2017-12-05 06:15:56.271125: step 1560, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:53s remains)
INFO - root - 2017-12-05 06:15:58.467289: step 1570, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:53s remains)
INFO - root - 2017-12-05 06:16:00.655207: step 1580, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:09s remains)
INFO - root - 2017-12-05 06:16:02.849239: step 1590, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:05s remains)
INFO - root - 2017-12-05 06:16:05.055121: step 1600, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:38s remains)
INFO - root - 2017-12-05 06:16:07.306843: step 1610, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:43m:26s remains)
INFO - root - 2017-12-05 06:16:09.469766: step 1620, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:36s remains)
INFO - root - 2017-12-05 06:16:11.663194: step 1630, loss = 2.11, batch loss = 2.05 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:00s remains)
INFO - root - 2017-12-05 06:16:13.864333: step 1640, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:29s remains)
INFO - root - 2017-12-05 06:16:16.032493: step 1650, loss = 2.07, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:12s remains)
INFO - root - 2017-12-05 06:16:18.242654: step 1660, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:40s remains)
INFO - root - 2017-12-05 06:16:20.401418: step 1670, loss = 2.05, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:19s remains)
INFO - root - 2017-12-05 06:16:22.549274: step 1680, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-05 06:16:24.742590: step 1690, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-05 06:16:26.978227: step 1700, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:31s remains)
INFO - root - 2017-12-05 06:16:29.275487: step 1710, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:02s remains)
INFO - root - 2017-12-05 06:16:31.462267: step 1720, loss = 2.05, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:48m:18s remains)
INFO - root - 2017-12-05 06:16:33.651258: step 1730, loss = 2.04, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:10s remains)
INFO - root - 2017-12-05 06:16:35.818228: step 1740, loss = 2.07, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:39s remains)
INFO - root - 2017-12-05 06:16:38.026003: step 1750, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:50s remains)
INFO - root - 2017-12-05 06:16:40.185679: step 1760, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:39s remains)
INFO - root - 2017-12-05 06:16:42.401910: step 1770, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:42s remains)
INFO - root - 2017-12-05 06:16:44.577238: step 1780, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:13s remains)
INFO - root - 2017-12-05 06:16:46.745975: step 1790, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 20h:04m:05s remains)
INFO - root - 2017-12-05 06:16:48.934375: step 1800, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:40s remains)
INFO - root - 2017-12-05 06:16:51.169322: step 1810, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:35s remains)
INFO - root - 2017-12-05 06:16:53.332833: step 1820, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:22s remains)
INFO - root - 2017-12-05 06:16:55.507305: step 1830, loss = 2.04, batch loss = 1.98 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-05 06:16:57.648026: step 1840, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:43s remains)
INFO - root - 2017-12-05 06:16:59.830038: step 1850, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:44s remains)
INFO - root - 2017-12-05 06:17:02.000544: step 1860, loss = 2.07, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:27s remains)
INFO - root - 2017-12-05 06:17:04.173109: step 1870, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:31m:06s remains)
INFO - root - 2017-12-05 06:17:06.379709: step 1880, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 21h:00m:17s remains)
INFO - root - 2017-12-05 06:17:08.588743: step 1890, loss = 2.09, batch loss = 2.04 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:59s remains)
INFO - root - 2017-12-05 06:17:10.754450: step 1900, loss = 2.03, batch loss = 1.97 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:28s remains)
INFO - root - 2017-12-05 06:17:13.027753: step 1910, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-05 06:17:15.210011: step 1920, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:01s remains)
INFO - root - 2017-12-05 06:17:17.400833: step 1930, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:43s remains)
INFO - root - 2017-12-05 06:17:19.556952: step 1940, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:43s remains)
INFO - root - 2017-12-05 06:17:21.767687: step 1950, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:46m:56s remains)
INFO - root - 2017-12-05 06:17:23.976284: step 1960, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:57s remains)
INFO - root - 2017-12-05 06:17:26.188541: step 1970, loss = 2.07, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:04m:58s remains)
INFO - root - 2017-12-05 06:17:28.362355: step 1980, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:50s remains)
INFO - root - 2017-12-05 06:17:30.539365: step 1990, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.209 sec/batch; 19h:13m:54s remains)
INFO - root - 2017-12-05 06:17:32.737135: step 2000, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:01s remains)
INFO - root - 2017-12-05 06:17:34.983813: step 2010, loss = 2.11, batch loss = 2.05 (36.1 examples/sec; 0.221 sec/batch; 20h:19m:17s remains)
INFO - root - 2017-12-05 06:17:37.168069: step 2020, loss = 2.09, batch loss = 2.03 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:21s remains)
INFO - root - 2017-12-05 06:17:39.379893: step 2030, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 21h:25m:37s remains)
INFO - root - 2017-12-05 06:17:41.562861: step 2040, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:11s remains)
INFO - root - 2017-12-05 06:17:43.760193: step 2050, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:42s remains)
INFO - root - 2017-12-05 06:17:45.922578: step 2060, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:52s remains)
INFO - root - 2017-12-05 06:17:48.100775: step 2070, loss = 2.03, batch loss = 1.97 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:50s remains)
INFO - root - 2017-12-05 06:17:50.295112: step 2080, loss = 2.06, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 20h:03m:08s remains)
INFO - root - 2017-12-05 06:17:52.456402: step 2090, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:30s remains)
INFO - root - 2017-12-05 06:17:54.647350: step 2100, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:10s remains)
INFO - root - 2017-12-05 06:17:56.893018: step 2110, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:59s remains)
INFO - root - 2017-12-05 06:17:59.079122: step 2120, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:10s remains)
INFO - root - 2017-12-05 06:18:01.256022: step 2130, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:16s remains)
INFO - root - 2017-12-05 06:18:03.418812: step 2140, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 21h:01m:04s remains)
INFO - root - 2017-12-05 06:18:05.621268: step 2150, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:23s remains)
INFO - root - 2017-12-05 06:18:07.791906: step 2160, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:35m:59s remains)
INFO - root - 2017-12-05 06:18:09.961596: step 2170, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:43s remains)
INFO - root - 2017-12-05 06:18:12.155531: step 2180, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:18s remains)
INFO - root - 2017-12-05 06:18:14.355690: step 2190, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:54s remains)
INFO - root - 2017-12-05 06:18:16.516355: step 2200, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:38s remains)
INFO - root - 2017-12-05 06:18:18.801405: step 2210, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:54s remains)
INFO - root - 2017-12-05 06:18:20.970361: step 2220, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 20h:30m:55s remains)
INFO - root - 2017-12-05 06:18:23.156335: step 2230, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:19s remains)
INFO - root - 2017-12-05 06:18:25.377833: step 2240, loss = 2.04, batch loss = 1.98 (36.1 examples/sec; 0.221 sec/batch; 20h:19m:01s remains)
INFO - root - 2017-12-05 06:18:27.568642: step 2250, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.211 sec/batch; 19h:23m:56s remains)
INFO - root - 2017-12-05 06:18:29.757853: step 2260, loss = 2.04, batch loss = 1.98 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:36s remains)
INFO - root - 2017-12-05 06:18:31.934334: step 2270, loss = 2.05, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:16s remains)
INFO - root - 2017-12-05 06:18:34.158585: step 2280, loss = 2.07, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:59s remains)
INFO - root - 2017-12-05 06:18:36.331481: step 2290, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:53s remains)
INFO - root - 2017-12-05 06:18:38.514901: step 2300, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:01s remains)
INFO - root - 2017-12-05 06:18:40.852025: step 2310, loss = 2.07, batch loss = 2.01 (31.9 examples/sec; 0.250 sec/batch; 22h:58m:24s remains)
INFO - root - 2017-12-05 06:18:42.992045: step 2320, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:13s remains)
INFO - root - 2017-12-05 06:18:45.206210: step 2330, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:15s remains)
INFO - root - 2017-12-05 06:18:47.381017: step 2340, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-05 06:18:49.533899: step 2350, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:39m:07s remains)
INFO - root - 2017-12-05 06:18:51.725463: step 2360, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:25s remains)
INFO - root - 2017-12-05 06:18:53.915329: step 2370, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-05 06:18:56.116088: step 2380, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:39s remains)
INFO - root - 2017-12-05 06:18:58.292468: step 2390, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:18m:53s remains)
INFO - root - 2017-12-05 06:19:00.468127: step 2400, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:22s remains)
INFO - root - 2017-12-05 06:19:02.680089: step 2410, loss = 2.04, batch loss = 1.99 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:42s remains)
INFO - root - 2017-12-05 06:19:04.883336: step 2420, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-05 06:19:07.084848: step 2430, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:23s remains)
INFO - root - 2017-12-05 06:19:09.303136: step 2440, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:11s remains)
INFO - root - 2017-12-05 06:19:11.468135: step 2450, loss = 2.06, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:01s remains)
INFO - root - 2017-12-05 06:19:13.670494: step 2460, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:02s remains)
INFO - root - 2017-12-05 06:19:15.829295: step 2470, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:22s remains)
INFO - root - 2017-12-05 06:19:18.013262: step 2480, loss = 2.06, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:13s remains)
INFO - root - 2017-12-05 06:19:20.219551: step 2490, loss = 2.03, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:58s remains)
INFO - root - 2017-12-05 06:19:22.403867: step 2500, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:31s remains)
INFO - root - 2017-12-05 06:19:24.664215: step 2510, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-05 06:19:26.838448: step 2520, loss = 2.03, batch loss = 1.97 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:34s remains)
INFO - root - 2017-12-05 06:19:29.040297: step 2530, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.215 sec/batch; 19h:44m:59s remains)
INFO - root - 2017-12-05 06:19:31.217845: step 2540, loss = 2.04, batch loss = 1.98 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:47s remains)
INFO - root - 2017-12-05 06:19:33.396588: step 2550, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:53s remains)
INFO - root - 2017-12-05 06:19:35.592187: step 2560, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:58s remains)
INFO - root - 2017-12-05 06:19:37.760321: step 2570, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:20s remains)
INFO - root - 2017-12-05 06:19:39.958996: step 2580, loss = 2.04, batch loss = 1.98 (36.1 examples/sec; 0.221 sec/batch; 20h:17m:35s remains)
INFO - root - 2017-12-05 06:19:42.124868: step 2590, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:12s remains)
INFO - root - 2017-12-05 06:19:44.311395: step 2600, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:40s remains)
INFO - root - 2017-12-05 06:19:46.551004: step 2610, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:40s remains)
INFO - root - 2017-12-05 06:19:48.705416: step 2620, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:27s remains)
INFO - root - 2017-12-05 06:19:50.898054: step 2630, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-05 06:19:53.190457: step 2640, loss = 2.06, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:00s remains)
INFO - root - 2017-12-05 06:19:55.411331: step 2650, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:17s remains)
INFO - root - 2017-12-05 06:19:57.627921: step 2660, loss = 2.04, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:42s remains)
INFO - root - 2017-12-05 06:19:59.817158: step 2670, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:37s remains)
INFO - root - 2017-12-05 06:20:01.996631: step 2680, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:24s remains)
INFO - root - 2017-12-05 06:20:04.158579: step 2690, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:35s remains)
INFO - root - 2017-12-05 06:20:06.354504: step 2700, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:21s remains)
INFO - root - 2017-12-05 06:20:08.616628: step 2710, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:49s remains)
INFO - root - 2017-12-05 06:20:10.811198: step 2720, loss = 2.04, batch loss = 1.98 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:03s remains)
INFO - root - 2017-12-05 06:20:12.992951: step 2730, loss = 2.11, batch loss = 2.05 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:50s remains)
INFO - root - 2017-12-05 06:20:15.167636: step 2740, loss = 2.07, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:39m:06s remains)
INFO - root - 2017-12-05 06:20:17.346815: step 2750, loss = 2.08, batch loss = 2.03 (36.8 examples/sec; 0.218 sec/batch; 19h:55m:21s remains)
INFO - root - 2017-12-05 06:20:19.539175: step 2760, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:44s remains)
INFO - root - 2017-12-05 06:20:21.752515: step 2770, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:18m:17s remains)
INFO - root - 2017-12-05 06:20:23.958731: step 2780, loss = 2.06, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-05 06:20:26.131660: step 2790, loss = 2.02, batch loss = 1.97 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:55s remains)
INFO - root - 2017-12-05 06:20:28.340409: step 2800, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:41s remains)
INFO - root - 2017-12-05 06:20:30.621732: step 2810, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:39s remains)
INFO - root - 2017-12-05 06:20:32.809375: step 2820, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:46s remains)
INFO - root - 2017-12-05 06:20:34.963386: step 2830, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.211 sec/batch; 19h:21m:23s remains)
INFO - root - 2017-12-05 06:20:37.150686: step 2840, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:29s remains)
INFO - root - 2017-12-05 06:20:39.345208: step 2850, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 21h:42m:14s remains)
INFO - root - 2017-12-05 06:20:41.558515: step 2860, loss = 2.04, batch loss = 1.98 (36.3 examples/sec; 0.221 sec/batch; 20h:11m:35s remains)
INFO - root - 2017-12-05 06:20:43.729972: step 2870, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:29s remains)
INFO - root - 2017-12-05 06:20:45.952942: step 2880, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:51s remains)
INFO - root - 2017-12-05 06:20:48.144566: step 2890, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:54s remains)
INFO - root - 2017-12-05 06:20:50.347957: step 2900, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:10m:59s remains)
INFO - root - 2017-12-05 06:20:52.597395: step 2910, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-05 06:20:54.773529: step 2920, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:48s remains)
INFO - root - 2017-12-05 06:20:56.953258: step 2930, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:57s remains)
INFO - root - 2017-12-05 06:20:59.143157: step 2940, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:44s remains)
INFO - root - 2017-12-05 06:21:01.326501: step 2950, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-05 06:21:03.512856: step 2960, loss = 2.10, batch loss = 2.04 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:23s remains)
INFO - root - 2017-12-05 06:21:05.680667: step 2970, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:39s remains)
INFO - root - 2017-12-05 06:21:07.870121: step 2980, loss = 2.10, batch loss = 2.04 (35.8 examples/sec; 0.224 sec/batch; 20h:28m:18s remains)
INFO - root - 2017-12-05 06:21:10.068510: step 2990, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-05 06:21:12.245538: step 3000, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-05 06:21:14.525374: step 3010, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:20s remains)
INFO - root - 2017-12-05 06:21:16.700145: step 3020, loss = 2.04, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:57s remains)
INFO - root - 2017-12-05 06:21:18.920376: step 3030, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 21h:06m:55s remains)
INFO - root - 2017-12-05 06:21:21.087064: step 3040, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:53s remains)
INFO - root - 2017-12-05 06:21:23.283649: step 3050, loss = 2.08, batch loss = 2.03 (34.7 examples/sec; 0.230 sec/batch; 21h:05m:03s remains)
INFO - root - 2017-12-05 06:21:25.507960: step 3060, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:28s remains)
INFO - root - 2017-12-05 06:21:27.672972: step 3070, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-05 06:21:29.842529: step 3080, loss = 2.03, batch loss = 1.97 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:49s remains)
INFO - root - 2017-12-05 06:21:32.028166: step 3090, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:38s remains)
INFO - root - 2017-12-05 06:21:34.229897: step 3100, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:50s remains)
INFO - root - 2017-12-05 06:21:36.518391: step 3110, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:10s remains)
INFO - root - 2017-12-05 06:21:38.692671: step 3120, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:28m:03s remains)
INFO - root - 2017-12-05 06:21:40.887132: step 3130, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 20h:11m:09s remains)
INFO - root - 2017-12-05 06:21:43.070653: step 3140, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:08s remains)
INFO - root - 2017-12-05 06:21:45.288269: step 3150, loss = 2.05, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:07s remains)
INFO - root - 2017-12-05 06:21:47.479879: step 3160, loss = 2.04, batch loss = 1.98 (33.7 examples/sec; 0.237 sec/batch; 21h:42m:51s remains)
INFO - root - 2017-12-05 06:21:49.702932: step 3170, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:51s remains)
INFO - root - 2017-12-05 06:21:51.890744: step 3180, loss = 2.05, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:37s remains)
INFO - root - 2017-12-05 06:21:54.070086: step 3190, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:53s remains)
INFO - root - 2017-12-05 06:21:56.257082: step 3200, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:47m:31s remains)
INFO - root - 2017-12-05 06:21:58.526560: step 3210, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.214 sec/batch; 19h:31m:43s remains)
INFO - root - 2017-12-05 06:22:00.718837: step 3220, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:29s remains)
INFO - root - 2017-12-05 06:22:02.880836: step 3230, loss = 2.03, batch loss = 1.97 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:16s remains)
INFO - root - 2017-12-05 06:22:05.050903: step 3240, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:09m:38s remains)
INFO - root - 2017-12-05 06:22:07.221956: step 3250, loss = 2.03, batch loss = 1.97 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-05 06:22:09.432573: step 3260, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:11s remains)
INFO - root - 2017-12-05 06:22:11.613061: step 3270, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:03s remains)
INFO - root - 2017-12-05 06:22:13.772324: step 3280, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:26m:06s remains)
INFO - root - 2017-12-05 06:22:15.971238: step 3290, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:49s remains)
INFO - root - 2017-12-05 06:22:18.138403: step 3300, loss = 2.04, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:53s remains)
INFO - root - 2017-12-05 06:22:20.375189: step 3310, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.214 sec/batch; 19h:31m:46s remains)
INFO - root - 2017-12-05 06:22:22.560556: step 3320, loss = 2.07, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:57s remains)
INFO - root - 2017-12-05 06:22:24.747247: step 3330, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:37s remains)
INFO - root - 2017-12-05 06:22:26.911564: step 3340, loss = 2.03, batch loss = 1.97 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:58s remains)
INFO - root - 2017-12-05 06:22:29.099178: step 3350, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:04s remains)
INFO - root - 2017-12-05 06:22:31.277464: step 3360, loss = 2.03, batch loss = 1.97 (36.3 examples/sec; 0.220 sec/batch; 20h:08m:20s remains)
INFO - root - 2017-12-05 06:22:33.442819: step 3370, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:59m:27s remains)
INFO - root - 2017-12-05 06:22:35.645024: step 3380, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:50s remains)
INFO - root - 2017-12-05 06:22:37.849957: step 3390, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:04s remains)
INFO - root - 2017-12-05 06:22:40.057252: step 3400, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:03s remains)
INFO - root - 2017-12-05 06:22:42.292270: step 3410, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:41s remains)
INFO - root - 2017-12-05 06:22:44.507560: step 3420, loss = 2.06, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:21s remains)
INFO - root - 2017-12-05 06:22:46.680542: step 3430, loss = 2.05, batch loss = 1.99 (37.8 examples/sec; 0.212 sec/batch; 19h:21m:12s remains)
INFO - root - 2017-12-05 06:22:48.856320: step 3440, loss = 2.04, batch loss = 1.98 (36.3 examples/sec; 0.221 sec/batch; 20h:09m:23s remains)
INFO - root - 2017-12-05 06:22:51.047994: step 3450, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-05 06:22:53.235316: step 3460, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:17s remains)
INFO - root - 2017-12-05 06:22:55.449064: step 3470, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.214 sec/batch; 19h:30m:53s remains)
INFO - root - 2017-12-05 06:22:57.637810: step 3480, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:31s remains)
INFO - root - 2017-12-05 06:22:59.841964: step 3490, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:44s remains)
INFO - root - 2017-12-05 06:23:01.998786: step 3500, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-05 06:23:04.280800: step 3510, loss = 2.05, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:12s remains)
INFO - root - 2017-12-05 06:23:06.475952: step 3520, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:45s remains)
INFO - root - 2017-12-05 06:23:08.644799: step 3530, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:25s remains)
INFO - root - 2017-12-05 06:23:10.843190: step 3540, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:41m:08s remains)
INFO - root - 2017-12-05 06:23:13.075917: step 3550, loss = 2.09, batch loss = 2.03 (33.2 examples/sec; 0.241 sec/batch; 21h:59m:16s remains)
INFO - root - 2017-12-05 06:23:15.271222: step 3560, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:34s remains)
INFO - root - 2017-12-05 06:23:17.441862: step 3570, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:46s remains)
INFO - root - 2017-12-05 06:23:19.609406: step 3580, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:21s remains)
INFO - root - 2017-12-05 06:23:21.797075: step 3590, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:51s remains)
INFO - root - 2017-12-05 06:23:24.021026: step 3600, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:51s remains)
INFO - root - 2017-12-05 06:23:26.251547: step 3610, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:35m:27s remains)
INFO - root - 2017-12-05 06:23:28.432679: step 3620, loss = 2.06, batch loss = 2.01 (35.6 examples/sec; 0.224 sec/batch; 20h:30m:04s remains)
INFO - root - 2017-12-05 06:23:30.605808: step 3630, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:21s remains)
INFO - root - 2017-12-05 06:23:32.771746: step 3640, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:18m:46s remains)
INFO - root - 2017-12-05 06:23:34.992630: step 3650, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 20h:33m:37s remains)
INFO - root - 2017-12-05 06:23:37.158421: step 3660, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-05 06:23:39.329794: step 3670, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:06s remains)
INFO - root - 2017-12-05 06:23:41.518082: step 3680, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:58s remains)
INFO - root - 2017-12-05 06:23:43.689864: step 3690, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:03s remains)
INFO - root - 2017-12-05 06:23:45.853253: step 3700, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-05 06:23:48.089448: step 3710, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:43m:29s remains)
INFO - root - 2017-12-05 06:23:50.285491: step 3720, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:57s remains)
INFO - root - 2017-12-05 06:23:52.476665: step 3730, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:42m:09s remains)
INFO - root - 2017-12-05 06:23:54.653631: step 3740, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:43m:10s remains)
INFO - root - 2017-12-05 06:23:56.845536: step 3750, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:20m:05s remains)
INFO - root - 2017-12-05 06:23:59.037137: step 3760, loss = 2.05, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:02m:29s remains)
INFO - root - 2017-12-05 06:24:01.223154: step 3770, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:20s remains)
INFO - root - 2017-12-05 06:24:03.398863: step 3780, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:40m:04s remains)
INFO - root - 2017-12-05 06:24:05.608709: step 3790, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 21h:02m:17s remains)
INFO - root - 2017-12-05 06:24:07.753477: step 3800, loss = 2.05, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:05s remains)
INFO - root - 2017-12-05 06:24:10.056848: step 3810, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:00s remains)
INFO - root - 2017-12-05 06:24:12.237107: step 3820, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-05 06:24:14.414517: step 3830, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:03s remains)
INFO - root - 2017-12-05 06:24:16.589069: step 3840, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:32s remains)
INFO - root - 2017-12-05 06:24:18.729288: step 3850, loss = 2.04, batch loss = 1.98 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:46s remains)
INFO - root - 2017-12-05 06:24:20.898117: step 3860, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:26m:23s remains)
INFO - root - 2017-12-05 06:24:23.074824: step 3870, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:46s remains)
INFO - root - 2017-12-05 06:24:25.262229: step 3880, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:17m:03s remains)
INFO - root - 2017-12-05 06:24:27.435829: step 3890, loss = 2.04, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-05 06:24:29.624391: step 3900, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:43s remains)
INFO - root - 2017-12-05 06:24:31.876437: step 3910, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:08s remains)
INFO - root - 2017-12-05 06:24:34.080327: step 3920, loss = 2.04, batch loss = 1.98 (36.4 examples/sec; 0.220 sec/batch; 20h:02m:48s remains)
INFO - root - 2017-12-05 06:24:36.251171: step 3930, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.215 sec/batch; 19h:39m:16s remains)
INFO - root - 2017-12-05 06:24:38.437326: step 3940, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-05 06:24:40.621585: step 3950, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-05 06:24:42.832682: step 3960, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:05s remains)
INFO - root - 2017-12-05 06:24:45.038087: step 3970, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:56s remains)
INFO - root - 2017-12-05 06:24:47.228854: step 3980, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:56m:42s remains)
INFO - root - 2017-12-05 06:24:49.389737: step 3990, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 20h:41m:07s remains)
INFO - root - 2017-12-05 06:24:51.580673: step 4000, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:08s remains)
INFO - root - 2017-12-05 06:24:53.854230: step 4010, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:02m:47s remains)
INFO - root - 2017-12-05 06:24:56.035279: step 4020, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:54s remains)
INFO - root - 2017-12-05 06:24:58.204106: step 4030, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:03m:55s remains)
INFO - root - 2017-12-05 06:25:00.390305: step 4040, loss = 2.03, batch loss = 1.97 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:19s remains)
INFO - root - 2017-12-05 06:25:02.566746: step 4050, loss = 2.05, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:25s remains)
INFO - root - 2017-12-05 06:25:04.749766: step 4060, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:50m:53s remains)
INFO - root - 2017-12-05 06:25:06.933481: step 4070, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:23m:20s remains)
INFO - root - 2017-12-05 06:25:09.136908: step 4080, loss = 2.03, batch loss = 1.98 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:54s remains)
INFO - root - 2017-12-05 06:25:11.318880: step 4090, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:02s remains)
INFO - root - 2017-12-05 06:25:13.497756: step 4100, loss = 2.09, batch loss = 2.03 (37.5 examples/sec; 0.214 sec/batch; 19h:28m:37s remains)
INFO - root - 2017-12-05 06:25:15.789041: step 4110, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.215 sec/batch; 19h:34m:19s remains)
INFO - root - 2017-12-05 06:25:17.990440: step 4120, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 20h:34m:47s remains)
INFO - root - 2017-12-05 06:25:20.198742: step 4130, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:04s remains)
