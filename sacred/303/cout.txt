INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "303"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-16 12:28:26.959993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:28:26.960028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:28:26.960034: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:28:26.960038: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:28:26.960042: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:28:28.032895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 12:28:28.032932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 12:28:28.032939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 12:28:28.032946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
[u'siamese_fc/conv1/weights:0',
 u'siamese_fc/conv1/BatchNorm/beta:0',
 u'siamese_fc/conv1/BatchNorm/gamma:0',
 u'siamese_fc/conv1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b1/weights:0',
 u'siamese_fc/conv2/b1/BatchNorm/beta:0',
 u'siamese_fc/conv2/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b2/weights:0',
 u'siamese_fc/conv2/b2/BatchNorm/beta:0',
 u'siamese_fc/conv2/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv3/weights:0',
 u'siamese_fc/conv3/BatchNorm/beta:0',
 u'siamese_fc/conv3/BatchNorm/gamma:0',
 u'siamese_fc/conv3/BatchNorm/moving_mean:0',
 u'siamese_fc/conv3/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b1/weights:0',
 u'siamese_fc/conv4/b1/BatchNorm/beta:0',
 u'siamese_fc/conv4/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b2/weights:0',
 u'siamese_fc/conv4/b2/BatchNorm/beta:0',
 u'siamese_fc/conv4/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv5/def/offset1/weights:0',
 u'siamese_fc/conv5/def/b1/weights:0',
 u'detection/biases:0',
 u'global_step:0',
 u'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0',
 u'OptimizeLoss/detection/biases/Momentum:0']
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 12:28:33.826880: step 0, loss = 2.28, batch loss = 2.23 (2.0 examples/sec; 4.039 sec/batch; 373h:01m:41s remains)
2017-12-16 12:28:34.802198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288049 -4.4287915 -4.4287682 -4.4287634 -4.4287696 -4.4287715 -4.4287672 -4.4287643 -4.4287596 -4.4287515 -4.4287567 -4.4287744 -4.4287896 -4.4288025 -4.4288216][-4.4288363 -4.4288282 -4.4288139 -4.428813 -4.4288163 -4.428812 -4.4288006 -4.428793 -4.4287868 -4.4287848 -4.4287953 -4.4288135 -4.428822 -4.4288249 -4.428833][-4.4288616 -4.4288568 -4.4288487 -4.4288459 -4.4288416 -4.4288311 -4.4288144 -4.4288058 -4.4288039 -4.4288092 -4.428823 -4.4288387 -4.4288425 -4.42884 -4.4288397][-4.428874 -4.4288683 -4.4288583 -4.4288449 -4.4288292 -4.4288096 -4.4287858 -4.4287786 -4.4287848 -4.4287977 -4.4288158 -4.4288311 -4.4288349 -4.4288316 -4.4288254][-4.4288583 -4.4288497 -4.428833 -4.4288049 -4.4287744 -4.4287405 -4.4287033 -4.4286971 -4.4287138 -4.4287357 -4.428762 -4.4287844 -4.4287977 -4.4288025 -4.4287977][-4.428813 -4.4288044 -4.428781 -4.4287391 -4.4286938 -4.4286466 -4.4286017 -4.4286022 -4.428628 -4.4286561 -4.4286909 -4.4287276 -4.4287577 -4.4287777 -4.428782][-4.4287462 -4.4287434 -4.42872 -4.4286761 -4.4286342 -4.4285936 -4.4285626 -4.4285746 -4.4286008 -4.4286242 -4.4286561 -4.428699 -4.4287443 -4.4287806 -4.428793][-4.4286995 -4.4287062 -4.4286904 -4.4286585 -4.42863 -4.4286056 -4.428596 -4.428618 -4.4286413 -4.4286566 -4.42868 -4.4287195 -4.4287691 -4.4288082 -4.4288225][-4.4287081 -4.4287195 -4.4287095 -4.4286866 -4.4286661 -4.4286556 -4.4286623 -4.42869 -4.42871 -4.4287176 -4.4287329 -4.4287653 -4.4288087 -4.42884 -4.4288507][-4.4287429 -4.4287534 -4.4287457 -4.4287281 -4.4287095 -4.4287028 -4.4287171 -4.4287443 -4.4287586 -4.4287605 -4.428772 -4.4288006 -4.4288368 -4.4288597 -4.4288626][-4.4287782 -4.428781 -4.4287705 -4.4287524 -4.4287319 -4.4287219 -4.4287376 -4.4287643 -4.4287739 -4.4287744 -4.4287872 -4.4288149 -4.4288464 -4.428864 -4.4288645][-4.428792 -4.42879 -4.4287767 -4.4287577 -4.4287338 -4.428719 -4.4287324 -4.4287577 -4.4287691 -4.4287753 -4.4287939 -4.428822 -4.4288478 -4.4288621 -4.4288645][-4.4287906 -4.4287887 -4.4287763 -4.4287572 -4.4287329 -4.4287152 -4.4287233 -4.4287486 -4.4287672 -4.4287825 -4.4288058 -4.4288325 -4.4288507 -4.4288611 -4.4288635][-4.4287825 -4.4287825 -4.4287744 -4.4287596 -4.4287391 -4.4287205 -4.4287238 -4.4287515 -4.4287791 -4.4287992 -4.4288211 -4.4288411 -4.42885 -4.428854 -4.4288568][-4.4287806 -4.4287848 -4.4287839 -4.4287748 -4.4287596 -4.4287419 -4.4287419 -4.428772 -4.4288044 -4.4288239 -4.4288387 -4.4288464 -4.4288459 -4.4288435 -4.4288435]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 12:28:38.649048: step 10, loss = 1.23, batch loss = 1.17 (28.6 examples/sec; 0.280 sec/batch; 25h:50m:24s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5
INFO - root - 2017-12-16 12:28:41.494440: step 20, loss = 0.48, batch loss = 0.42 (28.7 examples/sec; 0.279 sec/batch; 25h:45m:35s remains)
INFO - root - 2017-12-16 12:28:44.293575: step 30, loss = 0.45, batch loss = 0.40 (29.0 examples/sec; 0.276 sec/batch; 25h:31m:02s remains)
INFO - root - 2017-12-16 12:28:47.108352: step 40, loss = 0.49, batch loss = 0.43 (28.8 examples/sec; 0.278 sec/batch; 25h:39m:39s remains)
INFO - root - 2017-12-16 12:28:49.923620: step 50, loss = 0.49, batch loss = 0.43 (27.5 examples/sec; 0.290 sec/batch; 26h:49m:12s remains)
INFO - root - 2017-12-16 12:28:52.759642: step 60, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 25h:42m:45s remains)
INFO - root - 2017-12-16 12:28:55.621454: step 70, loss = 0.40, batch loss = 0.34 (27.5 examples/sec; 0.291 sec/batch; 26h:52m:02s remains)
INFO - root - 2017-12-16 12:28:58.484979: step 80, loss = 0.42, batch loss = 0.36 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:52s remains)
INFO - root - 2017-12-16 12:29:01.297133: step 90, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:40s remains)
INFO - root - 2017-12-16 12:29:04.098812: step 100, loss = 0.37, batch loss = 0.31 (28.8 examples/sec; 0.278 sec/batch; 25h:38m:12s remains)
2017-12-16 12:29:04.824316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9640024 -1.9687698 -1.9932933 -1.9422777 -1.9814465 -2.0551434 -2.1737552 -2.3057232 -2.4787238 -2.604177 -2.7271996 -2.8515694 -2.8745308 -2.8210044 -2.7590714][-1.6318805 -1.707417 -1.7694137 -1.7445161 -1.8199186 -1.9427953 -2.0795329 -2.2137747 -2.3856142 -2.5087328 -2.625834 -2.702415 -2.6795506 -2.5783966 -2.5100861][-1.3692937 -1.4434783 -1.5789886 -1.6097488 -1.7060938 -1.8417568 -1.9662883 -2.1083846 -2.2901824 -2.4231429 -2.5600772 -2.6125917 -2.5554171 -2.4187725 -2.3438475][-1.1747022 -1.2172265 -1.3897746 -1.4202483 -1.4887762 -1.614291 -1.7255666 -1.8749545 -2.0523312 -2.1926291 -2.3791058 -2.4776006 -2.4687781 -2.3731079 -2.3234177][-1.0355425 -1.0507078 -1.2340128 -1.2618158 -1.2954571 -1.3885677 -1.4677737 -1.599076 -1.7741427 -1.9026551 -2.1155519 -2.2615769 -2.3124113 -2.2851233 -2.2923706][-0.97828245 -0.97691059 -1.1795056 -1.20615 -1.2207792 -1.3046823 -1.373239 -1.4640617 -1.5832243 -1.6698742 -1.875222 -2.0629752 -2.1836843 -2.2172797 -2.2661052][-0.98372674 -0.96271062 -1.1708946 -1.2332523 -1.2714889 -1.3435402 -1.381007 -1.4268568 -1.4850109 -1.5036297 -1.6637537 -1.862756 -2.0366423 -2.1249597 -2.2300828][-1.0054197 -0.9866457 -1.1769118 -1.2550936 -1.3104832 -1.3352706 -1.3075304 -1.2908103 -1.2956951 -1.2705617 -1.3822796 -1.5973499 -1.8468821 -2.0225754 -2.2108879][-1.0395186 -1.0420635 -1.1886425 -1.2312737 -1.2426963 -1.1916993 -1.092206 -1.0220795 -0.99444294 -0.94516468 -1.0625236 -1.3132942 -1.6314466 -1.90253 -2.1853][-1.1984365 -1.2297664 -1.325556 -1.3066926 -1.2451501 -1.1019814 -0.91851044 -0.78788304 -0.73712373 -0.70805192 -0.88975835 -1.2031128 -1.5809369 -1.9259324 -2.2832253][-1.8309817 -1.8658252 -1.9118474 -1.8221142 -1.6725223 -1.4511898 -1.2123723 -1.0483415 -0.998332 -1.0223155 -1.2710757 -1.6324375 -2.0151522 -2.3468943 -2.6665831][-2.5101285 -2.5397549 -2.5415297 -2.3948002 -2.18642 -1.9342904 -1.6980793 -1.5409842 -1.5103197 -1.5831428 -1.8917825 -2.2897499 -2.6523516 -2.9209538 -3.1468117][-3.1568694 -3.1797829 -3.1776738 -3.0239756 -2.8086538 -2.570611 -2.3730013 -2.252944 -2.2465925 -2.3452151 -2.6472549 -3.0199075 -3.3416848 -3.5383816 -3.6590667][-3.7389951 -3.7560384 -3.7733922 -3.6546361 -3.4740803 -3.2832284 -3.1474228 -3.0757127 -3.0890584 -3.1839767 -3.4351993 -3.7364995 -3.9827139 -4.1151509 -4.1557965][-4.2919445 -4.3565655 -4.403049 -4.3195238 -4.1804762 -4.0365148 -3.9452145 -3.9166081 -3.9546282 -4.048 -4.2215686 -4.410789 -4.5498042 -4.59796 -4.5726123]]...]
INFO - root - 2017-12-16 12:29:07.639872: step 110, loss = 0.40, batch loss = 0.34 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:05s remains)
INFO - root - 2017-12-16 12:29:10.477214: step 120, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 25h:48m:59s remains)
INFO - root - 2017-12-16 12:29:13.362287: step 130, loss = 0.42, batch loss = 0.36 (27.8 examples/sec; 0.288 sec/batch; 26h:36m:07s remains)
INFO - root - 2017-12-16 12:29:16.212461: step 140, loss = 0.45, batch loss = 0.39 (28.9 examples/sec; 0.277 sec/batch; 25h:32m:12s remains)
INFO - root - 2017-12-16 12:29:19.077501: step 150, loss = 0.39, batch loss = 0.33 (28.5 examples/sec; 0.280 sec/batch; 25h:52m:30s remains)
INFO - root - 2017-12-16 12:29:21.876862: step 160, loss = 0.36, batch loss = 0.30 (29.7 examples/sec; 0.269 sec/batch; 24h:51m:52s remains)
INFO - root - 2017-12-16 12:29:24.751139: step 170, loss = 0.51, batch loss = 0.45 (28.8 examples/sec; 0.278 sec/batch; 25h:39m:19s remains)
INFO - root - 2017-12-16 12:29:27.613915: step 180, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 26h:07m:14s remains)
INFO - root - 2017-12-16 12:29:30.523158: step 190, loss = 0.47, batch loss = 0.41 (28.3 examples/sec; 0.283 sec/batch; 26h:07m:52s remains)
INFO - root - 2017-12-16 12:29:33.371219: step 200, loss = 0.34, batch loss = 0.29 (28.3 examples/sec; 0.283 sec/batch; 26h:05m:53s remains)
2017-12-16 12:29:34.029864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9061503 -3.2544289 -3.2844353 -3.0763066 -2.6317163 -1.8804188 -0.99239588 -0.22625494 0.15613842 0.16597748 -0.33676767 -1.3332775 -2.4722595 -3.3638005 -3.900147][-2.7028329 -2.9545038 -2.9795904 -2.7879577 -2.3587053 -1.6187081 -0.73375607 0.024717808 0.39853191 0.35366488 -0.21410894 -1.2500036 -2.4097257 -3.3300481 -3.8709338][-2.2753057 -2.4384274 -2.4340789 -2.2924857 -1.8932047 -1.1885026 -0.36542368 0.29366112 0.56989813 0.44626236 -0.21565962 -1.2898417 -2.4683273 -3.4028397 -3.9719353][-1.7143714 -1.7805071 -1.7669671 -1.650825 -1.2946219 -0.64425421 0.098929882 0.62166309 0.78714037 0.56370211 -0.18530083 -1.3292258 -2.5209594 -3.4449069 -4.0288706][-1.0082181 -1.0588377 -1.0613768 -1.0089221 -0.72789884 -0.16398144 0.47858667 0.87981892 0.9071641 0.5901227 -0.23475456 -1.4081028 -2.5951414 -3.4942226 -4.0607367][-0.41413927 -0.47401643 -0.5169158 -0.53341365 -0.29082346 0.19506121 0.75657034 1.0787139 1.0526943 0.66167259 -0.21231222 -1.4159439 -2.6201034 -3.5166736 -4.0951633][-0.20704222 -0.25618935 -0.29874039 -0.31805038 -0.073582649 0.41876221 0.9654355 1.2660546 1.209137 0.78506327 -0.12858629 -1.3734701 -2.6119075 -3.5482953 -4.1344948][-0.24930286 -0.32726717 -0.368196 -0.38206816 -0.0872798 0.49536514 1.0982556 1.4308305 1.394455 0.97173929 -0.012533665 -1.3194923 -2.614768 -3.6041796 -4.2429013][-0.43621492 -0.59593105 -0.67119694 -0.68097425 -0.3425436 0.33827972 1.0421953 1.4439483 1.4424992 1.0250387 0.044958591 -1.3035247 -2.6394482 -3.6874914 -4.3817768][-0.6552937 -0.92969394 -1.0955942 -1.1301761 -0.79470682 -0.071285725 0.71488476 1.2442436 1.3483329 0.9781146 0.0010213852 -1.3534079 -2.7321949 -3.8211012 -4.5515361][-1.37205 -1.704438 -1.9129589 -1.9250491 -1.5523582 -0.80711341 -0.023429394 0.54589128 0.72547817 0.39398384 -0.541888 -1.8510847 -3.1651692 -4.1886368 -4.8506708][-2.0307252 -2.3713796 -2.5720367 -2.5546949 -2.1864989 -1.4686692 -0.72128606 -0.18955326 -0.017082691 -0.31915855 -1.2135849 -2.4662366 -3.69795 -4.6310763 -5.166152][-2.4833043 -2.848402 -3.0741868 -3.0475821 -2.7086987 -2.0699842 -1.4118836 -0.96097827 -0.83748889 -1.1249018 -1.9550138 -3.093013 -4.2083549 -5.0097723 -5.4251285][-2.9610105 -3.3180323 -3.5465212 -3.5423775 -3.2621436 -2.7194762 -2.164413 -1.8177254 -1.7901664 -2.1126842 -2.8370938 -3.8095009 -4.7341394 -5.37201 -5.596869][-3.4773595 -3.7993832 -4.0097718 -3.9842448 -3.6908894 -3.196281 -2.7216687 -2.4363036 -2.443156 -2.8075747 -3.5103073 -4.3693833 -5.1219854 -5.5803232 -5.6430554]]...]
INFO - root - 2017-12-16 12:29:36.868043: step 210, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 26h:20m:07s remains)
INFO - root - 2017-12-16 12:29:39.763112: step 220, loss = 0.32, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 26h:12m:19s remains)
INFO - root - 2017-12-16 12:29:42.568191: step 230, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.286 sec/batch; 26h:24m:25s remains)
INFO - root - 2017-12-16 12:29:45.437357: step 240, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.277 sec/batch; 25h:34m:29s remains)
INFO - root - 2017-12-16 12:29:48.283951: step 250, loss = 0.40, batch loss = 0.34 (29.1 examples/sec; 0.275 sec/batch; 25h:23m:58s remains)
INFO - root - 2017-12-16 12:29:51.167026: step 260, loss = 0.28, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 27h:06m:08s remains)
INFO - root - 2017-12-16 12:29:54.044884: step 270, loss = 0.32, batch loss = 0.27 (27.8 examples/sec; 0.288 sec/batch; 26h:35m:33s remains)
INFO - root - 2017-12-16 12:29:56.903245: step 280, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:12s remains)
INFO - root - 2017-12-16 12:29:59.755629: step 290, loss = 0.44, batch loss = 0.38 (28.0 examples/sec; 0.286 sec/batch; 26h:21m:58s remains)
INFO - root - 2017-12-16 12:30:02.606151: step 300, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 26h:40m:40s remains)
2017-12-16 12:30:03.250215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1397283 -0.58880186 -0.10584688 0.023685455 -0.14629984 -0.6079607 -1.0586193 -1.2954528 -1.3305051 -1.1091843 -0.86848855 -0.94581985 -1.2582185 -1.796783 -2.6332521][-1.0107653 -0.44070625 0.052706718 0.088930607 -0.27325821 -0.80066419 -1.3864717 -1.7143514 -1.6012747 -1.2534704 -0.892781 -0.94988203 -1.1966248 -1.6623721 -2.294188][-1.1155369 -0.64040685 -0.28027344 -0.23778629 -0.66926 -1.2801907 -1.8493314 -2.1156344 -2.0979958 -1.7341297 -1.3869393 -1.4301047 -1.6869416 -2.1370506 -2.7490678][-1.4117517 -1.0038078 -0.75083756 -0.748909 -1.0751023 -1.5899038 -2.1428502 -2.4372869 -2.4715152 -2.2274356 -2.0459416 -2.1591179 -2.4761634 -2.87676 -3.3906741][-1.6469519 -1.367471 -1.0770211 -1.0459225 -1.2696979 -1.6478255 -2.0409222 -2.3920329 -2.5197906 -2.4304428 -2.3971913 -2.6736844 -3.1167886 -3.6192303 -4.083353][-1.7118523 -1.5261395 -1.3088427 -1.1715119 -1.1904564 -1.3828924 -1.5954545 -1.8075511 -1.962743 -2.0650926 -2.2833335 -2.7785711 -3.4439976 -4.1059422 -4.7050304][-1.7896092 -1.6853659 -1.46176 -1.2015643 -1.0843008 -1.0413058 -1.0939577 -1.1557658 -1.2277303 -1.3087795 -1.6391926 -2.3653741 -3.2458916 -4.1030321 -4.8142991][-1.8017845 -1.7801988 -1.6125491 -1.3647077 -1.1368985 -0.95012832 -0.78592467 -0.63628626 -0.61991334 -0.6861546 -1.0424476 -1.8307586 -2.8287914 -3.8438342 -4.6654539][-1.8220348 -1.853507 -1.7777972 -1.6063609 -1.3471112 -1.0581899 -0.76028395 -0.44980836 -0.30195951 -0.33794689 -0.73973823 -1.5436468 -2.5204902 -3.4415505 -4.2194552][-1.8449512 -1.8931649 -1.8936698 -1.8412409 -1.6821334 -1.4113541 -1.045212 -0.65127277 -0.41173744 -0.38058233 -0.74158859 -1.4691055 -2.3203077 -3.1488667 -3.8455987][-2.0278137 -2.0869274 -2.151067 -2.1998777 -2.1429098 -1.9829988 -1.7013438 -1.3669598 -1.042897 -0.94746947 -1.1682048 -1.7699387 -2.4944868 -3.1601171 -3.701911][-2.4330633 -2.4333529 -2.531095 -2.6207867 -2.6507406 -2.6196451 -2.4541292 -2.1932385 -1.9277692 -1.7682214 -1.8362927 -2.2656593 -2.8650918 -3.4286785 -3.825983][-3.1387208 -3.1209869 -3.1850243 -3.3139465 -3.4625697 -3.5611219 -3.5424576 -3.3420892 -3.0583041 -2.8005569 -2.7226963 -2.9441426 -3.35251 -3.8005133 -4.1263161][-3.600534 -3.5370502 -3.6161265 -3.8021808 -4.0485153 -4.266449 -4.35764 -4.2105527 -3.8601882 -3.4896007 -3.2812719 -3.3163924 -3.6102362 -4.0363388 -4.346251][-4.0567193 -3.9712667 -4.0497851 -4.2898164 -4.5715356 -4.8378692 -4.9378657 -4.8130512 -4.4290118 -3.9708433 -3.5926552 -3.4781013 -3.6581779 -4.00951 -4.3907232]]...]
INFO - root - 2017-12-16 12:30:06.147923: step 310, loss = 0.34, batch loss = 0.28 (26.5 examples/sec; 0.301 sec/batch; 27h:49m:11s remains)
INFO - root - 2017-12-16 12:30:09.064659: step 320, loss = 0.27, batch loss = 0.21 (27.1 examples/sec; 0.295 sec/batch; 27h:14m:43s remains)
INFO - root - 2017-12-16 12:30:11.998485: step 330, loss = 0.38, batch loss = 0.32 (27.4 examples/sec; 0.292 sec/batch; 26h:53m:48s remains)
INFO - root - 2017-12-16 12:30:14.882526: step 340, loss = 0.34, batch loss = 0.28 (27.2 examples/sec; 0.294 sec/batch; 27h:05m:23s remains)
INFO - root - 2017-12-16 12:30:17.825965: step 350, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 26h:59m:01s remains)
INFO - root - 2017-12-16 12:30:20.759421: step 360, loss = 0.35, batch loss = 0.29 (26.4 examples/sec; 0.303 sec/batch; 27h:55m:00s remains)
INFO - root - 2017-12-16 12:30:23.666206: step 370, loss = 0.41, batch loss = 0.35 (26.5 examples/sec; 0.302 sec/batch; 27h:54m:12s remains)
INFO - root - 2017-12-16 12:30:26.568688: step 380, loss = 0.41, batch loss = 0.35 (27.9 examples/sec; 0.287 sec/batch; 26h:27m:31s remains)
INFO - root - 2017-12-16 12:30:29.444492: step 390, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 25h:38m:43s remains)
INFO - root - 2017-12-16 12:30:32.333073: step 400, loss = 0.23, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 25h:59m:36s remains)
2017-12-16 12:30:32.929129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2106023 -1.5607717 -1.7064478 -1.7153172 -1.7061367 -1.7264366 -1.8127053 -2.0266857 -2.3472178 -2.6084728 -2.777914 -3.06032 -3.4822397 -3.9199173 -4.2040362][-1.4289074 -1.8881271 -2.0609593 -1.974426 -1.8016489 -1.7307591 -1.7692592 -1.8623753 -2.0637312 -2.2820241 -2.5524406 -3.0056834 -3.5829544 -4.0469723 -4.2466216][-1.7968137 -2.3266411 -2.4040253 -2.1668746 -1.7932572 -1.486877 -1.2748566 -1.2069302 -1.2905862 -1.4946117 -1.9329231 -2.6362123 -3.3542442 -3.9763861 -4.251348][-2.3038349 -2.9074407 -2.9153609 -2.3947008 -1.6944518 -0.98267126 -0.45274234 -0.16591501 -0.17840242 -0.37268019 -0.85849643 -1.7605457 -2.8697233 -3.7862186 -4.2427788][-2.6555524 -3.2587323 -2.9914844 -2.115104 -1.0567899 -0.082175732 0.75686216 1.1252475 1.1268148 0.79830837 0.062942028 -1.1495397 -2.5219679 -3.6918869 -4.2838354][-2.8292348 -3.3349586 -3.0145164 -1.7114339 -0.16601181 1.1067786 1.9308348 2.3725271 2.3331356 1.8662801 0.84692717 -0.71676636 -2.4943128 -3.8570585 -4.4726272][-2.8892822 -3.1782765 -2.6315863 -1.2429829 0.49010849 1.8372445 2.5741396 2.7376513 2.6258302 2.1221232 1.0375123 -0.72434187 -2.651979 -4.1023116 -4.7196755][-2.6349883 -2.7832019 -2.0776989 -0.72283912 0.81428576 2.0650449 2.7772293 2.873251 2.4584012 1.7428985 0.50385189 -1.2751358 -3.1273236 -4.5479789 -5.0515556][-2.2714851 -2.2549114 -1.6580017 -0.44463348 0.776947 1.7059026 2.1449051 2.1057305 1.6439452 0.81313848 -0.3906064 -2.0223918 -3.7163992 -4.9782944 -5.3892684][-2.0315187 -1.7582912 -1.2801991 -0.47092652 0.47555542 1.0302157 1.2669988 1.0385542 0.44820309 -0.39515877 -1.492393 -2.8886428 -4.3110366 -5.4112921 -5.6831994][-2.3756633 -2.176513 -1.7527266 -1.2371526 -0.64403534 -0.080281258 0.066955566 -0.1420989 -0.82954359 -1.7285857 -2.720778 -3.7769778 -4.8852158 -5.7618818 -6.0036216][-2.9790115 -2.6768165 -2.359432 -1.8927243 -1.4545124 -1.1937654 -0.99165893 -1.1483324 -1.7497871 -2.6255198 -3.6125875 -4.5447421 -5.3274627 -5.9012432 -6.036973][-3.7485008 -3.2399621 -2.8279526 -2.51269 -2.1688251 -1.9774642 -1.9661555 -2.1874235 -2.7048159 -3.4747496 -4.3355036 -5.1283813 -5.6959472 -5.8468976 -5.7169352][-4.124507 -3.540499 -3.0410576 -2.6479406 -2.4050186 -2.2089446 -2.208883 -2.544909 -3.1704183 -3.9994843 -4.8441281 -5.5115595 -5.8595943 -5.7531343 -5.3691139][-4.1806779 -3.391979 -2.86407 -2.5473702 -2.3355267 -2.2048347 -2.2108421 -2.555028 -3.2420111 -4.14716 -5.0855613 -5.635457 -5.7282391 -5.4754953 -5.0324278]]...]
INFO - root - 2017-12-16 12:30:35.779713: step 410, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 26h:32m:19s remains)
INFO - root - 2017-12-16 12:30:38.699942: step 420, loss = 0.45, batch loss = 0.39 (27.6 examples/sec; 0.290 sec/batch; 26h:43m:44s remains)
INFO - root - 2017-12-16 12:30:41.555627: step 430, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 26h:32m:52s remains)
INFO - root - 2017-12-16 12:30:44.451007: step 440, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 25h:24m:57s remains)
INFO - root - 2017-12-16 12:30:47.404232: step 450, loss = 0.27, batch loss = 0.21 (26.6 examples/sec; 0.301 sec/batch; 27h:45m:37s remains)
INFO - root - 2017-12-16 12:30:50.319487: step 460, loss = 0.38, batch loss = 0.32 (28.4 examples/sec; 0.282 sec/batch; 26h:00m:53s remains)
INFO - root - 2017-12-16 12:30:53.231335: step 470, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 25h:46m:27s remains)
INFO - root - 2017-12-16 12:30:56.150783: step 480, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 26h:27m:45s remains)
INFO - root - 2017-12-16 12:30:59.045146: step 490, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.291 sec/batch; 26h:49m:40s remains)
INFO - root - 2017-12-16 12:31:01.968721: step 500, loss = 0.31, batch loss = 0.25 (25.4 examples/sec; 0.315 sec/batch; 29h:03m:06s remains)
2017-12-16 12:31:02.609379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8210387 -3.1639581 -3.3443298 -3.3598888 -3.3243394 -3.1993012 -3.2689111 -3.3181763 -3.21244 -3.1473517 -3.692697 -4.6698289 -5.371532 -5.8945751 -6.01625][-3.5212235 -3.8121636 -3.8823743 -3.8074183 -3.8074002 -3.8069735 -3.9645329 -3.9951837 -4.08122 -4.13581 -4.8895884 -5.9834971 -6.6138163 -7.0011473 -6.9970431][-4.2186017 -4.4098172 -4.2486935 -3.9763908 -3.6971676 -3.583468 -3.5901976 -3.6086152 -3.79912 -4.00036 -5.0833969 -6.528419 -7.3969173 -7.769124 -7.6445112][-4.4930038 -4.6312451 -4.3281589 -3.6991229 -2.9929917 -2.6835914 -2.6217282 -2.7232063 -2.9358921 -3.2399454 -4.4283524 -6.0540667 -7.0998259 -7.7261167 -7.7598228][-4.1243391 -4.1473384 -3.6745362 -2.7613654 -1.7146437 -1.0798924 -0.69646764 -0.77616286 -1.287847 -2.0406342 -3.5485358 -5.4682007 -6.7224116 -7.4581046 -7.3921776][-3.1570089 -2.9321096 -2.2673709 -1.1026175 0.30951548 1.3080039 1.8612323 1.7189641 0.98621655 -0.12537813 -2.2223914 -4.5594597 -5.9649935 -6.768044 -6.9844475][-2.3749378 -2.0460207 -1.0260231 0.5937438 2.3493018 3.6168795 4.3573027 4.2219915 3.3253884 1.9686346 -0.30676365 -2.8497634 -4.7002854 -5.8857775 -6.318244][-1.8565726 -1.494235 -0.59047556 0.8706007 2.639349 4.2484379 5.2033787 5.1653261 4.164691 2.6042638 0.23935032 -2.1314263 -3.6711869 -4.7712464 -5.444808][-1.9001215 -1.8023393 -1.2596836 -0.12877941 1.4155626 2.8897886 3.9038754 4.0984087 3.3557148 1.9585667 -0.24037695 -2.4383492 -3.7831361 -4.5984044 -5.1184511][-2.4942183 -2.5957856 -2.2560632 -1.6547537 -0.67170811 0.31330776 1.3735304 1.6446753 1.2279358 0.34736919 -1.3165278 -2.9813452 -4.1762371 -4.8044586 -5.1722565][-3.4711995 -3.7072084 -3.7642694 -3.3889239 -2.7059965 -1.8916955 -1.1446147 -0.89668536 -0.97674084 -1.5863037 -2.743691 -3.8858392 -4.6217303 -5.2209263 -5.6068325][-4.3299422 -4.7717352 -5.031075 -4.9624777 -4.7484417 -4.1625752 -3.6226976 -3.2913828 -3.2924294 -3.7286916 -4.3688679 -5.1577191 -5.6876974 -6.0331044 -6.3429585][-5.0451617 -5.47263 -5.8649745 -6.0990229 -6.1272488 -5.9032063 -5.5422826 -5.1870756 -5.0909014 -5.113749 -5.4095769 -5.8050165 -6.1106982 -6.4769526 -6.7491055][-4.9954338 -5.3094587 -5.6523623 -5.9082141 -6.0624347 -6.1321025 -6.0860605 -5.8947153 -5.7091932 -5.5935154 -5.5806284 -5.6694937 -5.8838587 -6.1572433 -6.3746939][-4.8993354 -5.1353712 -5.3893833 -5.6193395 -5.8277588 -5.9375205 -6.0148525 -6.0054741 -5.9459405 -5.8328028 -5.7468505 -5.7052908 -5.7172537 -5.8580246 -5.9050808]]...]
INFO - root - 2017-12-16 12:31:05.546306: step 510, loss = 0.27, batch loss = 0.22 (26.8 examples/sec; 0.299 sec/batch; 27h:33m:29s remains)
INFO - root - 2017-12-16 12:31:08.441872: step 520, loss = 0.38, batch loss = 0.32 (28.8 examples/sec; 0.278 sec/batch; 25h:35m:25s remains)
INFO - root - 2017-12-16 12:31:11.345513: step 530, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.279 sec/batch; 25h:46m:03s remains)
INFO - root - 2017-12-16 12:31:14.271852: step 540, loss = 0.29, batch loss = 0.24 (27.8 examples/sec; 0.287 sec/batch; 26h:30m:01s remains)
INFO - root - 2017-12-16 12:31:17.158345: step 550, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 26h:30m:56s remains)
INFO - root - 2017-12-16 12:31:20.045729: step 560, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 26h:09m:39s remains)
INFO - root - 2017-12-16 12:31:22.913599: step 570, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 25h:15m:11s remains)
INFO - root - 2017-12-16 12:31:25.859063: step 580, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 26h:31m:35s remains)
INFO - root - 2017-12-16 12:31:28.690087: step 590, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 25h:18m:01s remains)
INFO - root - 2017-12-16 12:31:31.629630: step 600, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:26s remains)
2017-12-16 12:31:32.255076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22531557 -0.17120886 -0.21711111 -0.44560814 -0.80301046 -1.191077 -1.6190603 -1.8189459 -2.0090501 -2.1119568 -2.7057271 -3.5602734 -4.5935907 -5.5894547 -6.4145832][0.70477819 0.50706673 0.44845533 0.13831568 -0.25944042 -0.78187323 -1.3597145 -1.7391844 -1.9753153 -2.1130023 -2.8321872 -3.8496571 -4.9547191 -5.9683723 -6.753665][1.2865772 1.1633677 0.93336248 0.54287434 0.16562033 -0.24438572 -0.72933912 -1.1526432 -1.4968643 -1.7851975 -2.6535368 -3.7946153 -4.9969797 -5.9533877 -6.6438785][1.8567371 1.7859139 1.6998858 1.321558 0.92304182 0.48690414 -0.0048508644 -0.36676931 -0.63431168 -1.0451081 -1.9993956 -3.1743426 -4.3399811 -5.3386564 -6.0208006][2.4256229 2.4616418 2.3655047 1.9983749 1.6630554 1.2951074 0.95267773 0.64052629 0.41094637 0.076172352 -0.73783994 -1.7859521 -2.9031262 -3.9196103 -4.6898379][2.2151732 2.3674335 2.516963 2.333282 2.2704611 2.0431542 1.7517195 1.5648298 1.4514561 1.2706628 0.6081872 -0.32552576 -1.3824322 -2.5101783 -3.4840033][1.5866942 2.0743594 2.4347315 2.5990629 2.7949872 2.7262683 2.5586219 2.3419142 2.2267165 2.087451 1.5399661 0.78876448 -0.24364805 -1.4205623 -2.513824][0.67734051 1.2354035 1.7143712 2.1148672 2.508831 2.6813631 2.8078609 2.7997923 2.8006687 2.8061104 2.3318977 1.5505123 0.54545641 -0.665215 -1.821162][-0.32798338 0.19722605 0.66505337 1.1839805 1.6069579 1.9760771 2.3212194 2.5545545 2.7545705 2.9012794 2.5157743 1.7902694 0.750617 -0.45082903 -1.6272697][-1.0837097 -0.69662786 -0.29708147 0.17671156 0.54539061 0.98006821 1.3821859 1.7053981 1.980794 2.1646891 1.7753539 1.0201511 0.049893856 -1.0609674 -2.113138][-2.2301452 -2.1310143 -1.903656 -1.4596415 -1.0334458 -0.51355815 -0.083071232 0.30368471 0.56927633 0.710835 0.27252817 -0.49966478 -1.4143424 -2.3573458 -3.1940081][-3.5893877 -3.8770235 -3.8327198 -3.5452762 -3.0932186 -2.5124121 -2.0208404 -1.586102 -1.3950984 -1.4121819 -1.9739723 -2.7299628 -3.5229037 -4.2591796 -4.7915468][-4.6694069 -5.1683817 -5.3204713 -5.1822257 -4.7668285 -4.3118777 -3.9028666 -3.5669479 -3.4140315 -3.4677866 -3.9563153 -4.6139855 -5.2986794 -5.8578119 -6.1374679][-4.7626433 -5.3428454 -5.5784168 -5.5849676 -5.3395996 -5.09528 -4.8616486 -4.7421188 -4.8271465 -5.0151339 -5.4830337 -6.0059772 -6.4840164 -6.8951397 -7.0437984][-4.8662643 -5.3051538 -5.5950832 -5.7512593 -5.6914411 -5.6190467 -5.5518003 -5.5977397 -5.7728815 -6.0487223 -6.4710445 -6.88109 -7.2666454 -7.50797 -7.5402932]]...]
INFO - root - 2017-12-16 12:31:35.088849: step 610, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 26h:19m:18s remains)
INFO - root - 2017-12-16 12:31:38.018194: step 620, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:29s remains)
INFO - root - 2017-12-16 12:31:40.929808: step 630, loss = 0.23, batch loss = 0.17 (26.0 examples/sec; 0.307 sec/batch; 28h:20m:07s remains)
INFO - root - 2017-12-16 12:31:43.854977: step 640, loss = 0.31, batch loss = 0.25 (25.5 examples/sec; 0.314 sec/batch; 28h:55m:51s remains)
INFO - root - 2017-12-16 12:31:46.768190: step 650, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 26h:16m:02s remains)
INFO - root - 2017-12-16 12:31:49.693036: step 660, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 26h:04m:10s remains)
INFO - root - 2017-12-16 12:31:52.608460: step 670, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:08s remains)
INFO - root - 2017-12-16 12:31:55.572939: step 680, loss = 0.36, batch loss = 0.30 (27.1 examples/sec; 0.295 sec/batch; 27h:11m:22s remains)
INFO - root - 2017-12-16 12:31:58.467298: step 690, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 26h:04m:15s remains)
INFO - root - 2017-12-16 12:32:01.347686: step 700, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 25h:56m:02s remains)
2017-12-16 12:32:01.969185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2700388 -1.6893187 -2.3736641 -2.8766248 -3.1209769 -3.0364459 -2.9988112 -2.6874337 -2.4049659 -2.0883181 -2.0251856 -2.4377809 -3.1904471 -4.3294482 -5.3234739][-0.9034946 -1.6116123 -2.6054368 -3.3630135 -3.7625847 -3.8142562 -3.7227328 -3.3713348 -3.0974307 -2.60856 -2.3508823 -2.4824436 -3.1436949 -4.1126757 -4.869997][-0.59298134 -1.4187098 -2.5722084 -3.4329162 -3.9054539 -4.2011905 -4.1811218 -3.8114197 -3.4034848 -2.8237464 -2.5204365 -2.6669679 -3.2360194 -3.9885309 -4.6173534][-0.62503624 -1.3677423 -2.4183469 -3.1416688 -3.5558786 -3.6270392 -3.3244386 -3.0336432 -2.7017632 -2.4046664 -2.2203102 -2.5026364 -3.239599 -4.0951838 -4.8006821][-0.68922424 -1.4229023 -2.2383952 -2.7316036 -2.8794122 -2.5536122 -1.9050052 -1.2601647 -0.83971453 -0.876863 -1.0776207 -1.8043149 -2.8623543 -4.0521517 -4.8156333][-0.95769691 -1.686095 -2.2690952 -2.4316816 -2.1976991 -1.4392977 -0.47265983 0.3991766 0.91591883 0.86963367 0.37573051 -0.75078106 -2.0867727 -3.5042748 -4.410615][-1.3908551 -1.9120579 -2.2579827 -2.3411188 -1.90007 -0.87619853 0.27362728 1.2832031 1.8349409 1.8485441 1.3294368 0.11290979 -1.4200718 -2.9524603 -3.9376757][-1.3735051 -1.9230831 -2.2145345 -2.3368204 -1.8403561 -0.84296417 0.41103029 1.4178572 2.0266795 2.1657743 1.521616 0.213202 -1.2874806 -2.589272 -3.4894753][-1.6074972 -2.1662416 -2.574223 -2.5953879 -2.0172033 -1.0244546 0.13535976 1.0122476 1.6039462 1.6362462 0.99192095 -0.12031937 -1.3194778 -2.3646941 -3.2257605][-1.9434342 -2.5958543 -3.0458584 -3.1327217 -2.6497276 -1.9558032 -1.0251951 -0.02554512 0.52970505 0.65808773 0.26464224 -0.54022384 -1.4906452 -2.3891559 -3.2462721][-2.7825537 -3.4779062 -3.8823528 -3.9069829 -3.6491346 -3.1363585 -2.2932558 -1.5793669 -1.0475416 -0.96606851 -1.1923966 -1.6881583 -2.3205018 -3.0516114 -3.7419059][-3.8164885 -4.3257709 -4.6481657 -4.5733886 -4.2596579 -3.9175973 -3.4224215 -3.0314462 -2.8507514 -2.8621659 -2.9826543 -3.2676551 -3.5419626 -3.9937935 -4.4241934][-4.7048936 -4.9390531 -5.05803 -4.9246063 -4.6419964 -4.4104605 -4.1370411 -3.9316447 -3.8743017 -3.9399981 -4.0182581 -4.2245474 -4.3920932 -4.6565428 -4.8832541][-5.1584921 -5.2982969 -5.1831422 -4.9311833 -4.7623115 -4.4873085 -4.2910666 -4.2485332 -4.2846694 -4.3761034 -4.5237622 -4.6680031 -4.7795658 -4.9503789 -4.9318342][-5.2320476 -5.3043766 -5.1876569 -4.9344125 -4.713203 -4.5261884 -4.4306474 -4.4487586 -4.559495 -4.6501846 -4.7387896 -4.8315573 -4.8035879 -4.8161936 -4.73898]]...]
INFO - root - 2017-12-16 12:32:04.840184: step 710, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 26h:52m:11s remains)
INFO - root - 2017-12-16 12:32:07.755083: step 720, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 27h:21m:51s remains)
INFO - root - 2017-12-16 12:32:10.689030: step 730, loss = 0.32, batch loss = 0.26 (26.5 examples/sec; 0.302 sec/batch; 27h:49m:46s remains)
INFO - root - 2017-12-16 12:32:13.606698: step 740, loss = 0.29, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 26h:08m:32s remains)
INFO - root - 2017-12-16 12:32:16.507562: step 750, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 26h:32m:50s remains)
INFO - root - 2017-12-16 12:32:19.455567: step 760, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 26h:30m:58s remains)
INFO - root - 2017-12-16 12:32:22.363780: step 770, loss = 0.28, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 26h:11m:56s remains)
INFO - root - 2017-12-16 12:32:25.342345: step 780, loss = 0.31, batch loss = 0.25 (27.1 examples/sec; 0.295 sec/batch; 27h:12m:05s remains)
INFO - root - 2017-12-16 12:32:28.246191: step 790, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:46m:13s remains)
INFO - root - 2017-12-16 12:32:31.104062: step 800, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 27h:04m:20s remains)
2017-12-16 12:32:31.767240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0819993 -3.7425482 -4.0711122 -4.3772759 -4.4029927 -4.2620821 -4.1129518 -3.6619825 -3.3155017 -2.8633711 -2.9080584 -3.4814694 -4.1975894 -4.8540859 -5.2202778][-2.8130145 -3.2680082 -3.4265869 -3.5020056 -3.2405734 -2.9063449 -2.4614623 -2.0291929 -1.9014995 -1.8659732 -2.3754768 -2.992362 -3.7119415 -4.2787638 -4.6181121][-2.1554649 -2.3526464 -2.5128284 -2.5127845 -1.9893727 -1.3533905 -1.0058768 -0.78329873 -0.88016343 -1.0301478 -1.7302933 -2.6938984 -3.4391155 -3.8456659 -4.1400084][-1.5710278 -1.5277402 -1.392297 -1.1213408 -0.73265481 -0.32591343 -0.0082230568 0.17610741 0.062140942 -0.35431814 -1.2131317 -2.2819223 -3.2427497 -3.7657073 -3.9582124][-0.63593483 -0.43751669 -0.274518 0.10760975 0.60128784 0.89642906 1.0761442 0.97209692 0.65985537 0.27444029 -0.77404904 -2.0220032 -3.0186253 -3.7359886 -4.0344014][-0.083227634 0.35535574 0.65367365 1.0880957 1.7298331 2.2720618 2.3013096 1.8723106 1.4466438 0.89055157 -0.23141003 -1.6334279 -2.799742 -3.5911391 -4.0296774][0.34717178 1.0375643 1.3567996 1.7652292 2.4086413 3.0555573 3.365005 2.9988637 2.3622098 1.5354052 0.286067 -1.1848226 -2.3188217 -3.2281821 -3.8035283][1.0425515 1.4298282 1.6005354 1.9967237 2.6080279 3.3801279 3.7178221 3.3750906 2.8782234 2.2268105 0.98014927 -0.60815096 -1.9022231 -2.9621859 -3.6420858][1.1886992 1.4210329 1.2339196 1.3504009 1.7896004 2.555748 3.0954032 3.0048008 2.7766886 2.3193722 1.2243142 -0.23824596 -1.6186702 -2.76974 -3.5261455][1.0663729 0.9130497 0.59225845 0.31865311 0.4478817 1.0963993 1.53087 1.6709919 1.7039442 1.6151538 0.89145088 -0.39839554 -1.5372355 -2.6663966 -3.4904749][-0.46740794 -0.57923317 -0.93353891 -1.0700686 -1.0503376 -0.87673354 -0.66332793 -0.59306931 -0.44837022 -0.26274347 -0.611629 -1.3929005 -2.2054551 -3.0716891 -3.6271148][-2.6685758 -2.7898736 -3.0313573 -3.215261 -3.1574574 -2.8976872 -2.9537642 -3.0019102 -3.0452085 -2.8601608 -3.0245681 -3.4170365 -3.790967 -4.1871166 -4.4276705][-4.90388 -4.9435482 -5.1186185 -5.2243991 -5.2665067 -5.1358676 -5.1139131 -5.0349469 -5.0117507 -4.9512186 -5.1014385 -5.3047805 -5.4794216 -5.6330013 -5.6144743][-6.4442787 -6.3863077 -6.322741 -6.3597708 -6.408926 -6.4247465 -6.4067612 -6.2166543 -6.1539559 -6.0571694 -6.2501144 -6.4807348 -6.6559944 -6.7125225 -6.5548983][-7.3732615 -7.1882696 -7.1303053 -7.0847483 -7.0187063 -7.0313931 -7.0850425 -6.9488358 -6.8676949 -6.6834574 -6.78137 -6.9931593 -7.1723061 -7.2122068 -7.0278091]]...]
INFO - root - 2017-12-16 12:32:34.668848: step 810, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 26h:58m:16s remains)
INFO - root - 2017-12-16 12:32:37.541334: step 820, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 26h:46m:30s remains)
INFO - root - 2017-12-16 12:32:40.443829: step 830, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.289 sec/batch; 26h:39m:52s remains)
INFO - root - 2017-12-16 12:32:43.296613: step 840, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:46m:14s remains)
INFO - root - 2017-12-16 12:32:46.219795: step 850, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.290 sec/batch; 26h:45m:43s remains)
INFO - root - 2017-12-16 12:32:49.113038: step 860, loss = 0.46, batch loss = 0.40 (27.0 examples/sec; 0.296 sec/batch; 27h:17m:50s remains)
INFO - root - 2017-12-16 12:32:51.989200: step 870, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:16s remains)
INFO - root - 2017-12-16 12:32:54.849136: step 880, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 25h:41m:32s remains)
INFO - root - 2017-12-16 12:32:57.762752: step 890, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 26h:16m:55s remains)
INFO - root - 2017-12-16 12:33:00.645919: step 900, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 26h:43m:04s remains)
2017-12-16 12:33:01.269550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4764445 -2.3617067 -2.4328594 -2.5953116 -2.7470689 -2.7057624 -2.5462971 -2.7915683 -3.179606 -3.496815 -3.9030476 -4.4912314 -5.0801311 -5.6172442 -6.0616512][-2.5464439 -1.9480186 -1.5698063 -1.6590152 -2.1841624 -2.5092759 -2.5359771 -2.4878404 -2.7413282 -3.1399002 -3.7672915 -4.5370049 -5.0096493 -5.4423237 -5.6788788][-2.0765808 -1.3423831 -0.81137252 -0.83197737 -1.4929364 -2.3247166 -2.5714598 -2.4769506 -2.3749764 -2.6117868 -3.1330795 -3.9497242 -4.7617259 -5.1008725 -5.1839838][-1.5024319 -0.72352314 -0.32858515 -0.38936758 -0.9793303 -1.646544 -1.8431702 -1.8781879 -1.8659675 -1.8812594 -2.2594256 -3.0525668 -3.9782283 -4.5934639 -4.6627259][-1.0753419 -0.39303446 -0.21670532 -0.49773598 -0.99932981 -1.3108141 -1.3208728 -1.0936787 -1.0884159 -1.1981068 -1.6729236 -2.5039237 -3.4707646 -4.2106295 -4.2421989][-0.95534611 -0.76682949 -0.69631958 -0.89532685 -1.0579731 -0.81243181 -0.43737459 -0.036550045 -0.018389702 -0.317482 -1.1433053 -2.2847927 -3.3295543 -4.1289883 -4.2456317][-1.0053129 -1.3026705 -1.6090338 -1.6739719 -1.2447281 -0.4088254 0.50514364 1.1020584 1.2031112 0.69241571 -0.46663547 -2.0831532 -3.380878 -4.2136045 -4.4015703][-1.6579762 -2.324723 -2.6721933 -2.6084871 -1.8552146 -0.50405955 0.967793 1.8131185 1.9554715 1.3419161 -0.028004169 -1.9324324 -3.5027165 -4.5200562 -4.7013445][-2.499923 -3.2943084 -3.5497403 -3.1963334 -2.2191341 -0.6299243 1.0366874 2.2132215 2.4281425 1.7857471 0.22937584 -1.8002758 -3.5978839 -4.7452207 -5.0270667][-3.2041035 -4.213378 -4.6420732 -4.0562172 -2.7834415 -1.06447 0.56915426 1.7667756 2.2274432 1.9260249 0.48151159 -1.5306265 -3.5609403 -5.029799 -5.452971][-4.1817327 -5.3239594 -5.5808053 -4.9833889 -3.5874219 -2.0100787 -0.47357273 0.7236414 1.3007588 1.2695179 0.30658674 -1.6133575 -3.6034732 -5.2075973 -5.8016376][-4.8604631 -6.0496378 -6.3679576 -5.6899428 -4.4244347 -2.8293447 -1.3163249 -0.17318773 0.47929907 0.50457907 -0.21744156 -1.7303059 -3.536696 -5.0624523 -5.8272696][-5.6337624 -6.7490826 -7.105092 -6.4886813 -5.2337 -3.7462511 -2.4784555 -1.4157898 -0.82253051 -0.81636977 -1.5408282 -2.7703054 -4.1318793 -5.2087388 -5.8264055][-6.0434179 -7.0137529 -7.2349291 -6.6949139 -5.43866 -4.0872183 -2.9060564 -2.2556899 -1.9494278 -1.8909249 -2.5148458 -3.4718132 -4.4151721 -5.22549 -5.7579813][-6.0785856 -7.023077 -7.2028837 -6.5856338 -5.471055 -4.251081 -3.3033237 -2.8295348 -2.753747 -2.9360757 -3.4091797 -4.0411062 -4.5737529 -5.1220837 -5.5338163]]...]
INFO - root - 2017-12-16 12:33:04.146911: step 910, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 25h:57m:23s remains)
INFO - root - 2017-12-16 12:33:07.043775: step 920, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 26h:37m:46s remains)
INFO - root - 2017-12-16 12:33:10.001214: step 930, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 27h:04m:36s remains)
INFO - root - 2017-12-16 12:33:12.923829: step 940, loss = 0.44, batch loss = 0.38 (27.2 examples/sec; 0.295 sec/batch; 27h:07m:44s remains)
INFO - root - 2017-12-16 12:33:15.835500: step 950, loss = 0.33, batch loss = 0.28 (28.2 examples/sec; 0.283 sec/batch; 26h:05m:07s remains)
INFO - root - 2017-12-16 12:33:18.777150: step 960, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 26h:51m:54s remains)
INFO - root - 2017-12-16 12:33:21.683239: step 970, loss = 0.25, batch loss = 0.19 (25.9 examples/sec; 0.309 sec/batch; 28h:24m:59s remains)
INFO - root - 2017-12-16 12:33:24.611152: step 980, loss = 0.31, batch loss = 0.25 (27.2 examples/sec; 0.294 sec/batch; 27h:04m:26s remains)
INFO - root - 2017-12-16 12:33:27.450382: step 990, loss = 0.27, batch loss = 0.21 (26.7 examples/sec; 0.300 sec/batch; 27h:36m:51s remains)
